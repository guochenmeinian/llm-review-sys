# Flex-MoE: Modeling Arbitrary Modality Combination

via the Flexible Mixture-of-Experts

Sukwon Yun\({}^{1}\), Inyoung Choi\({}^{2}\), Jie Peng\({}^{3}\), Yangfan Wu\({}^{3}\), Jingxuan Bao\({}^{2}\),

**Qiyiwen Zhang\({}^{2}\), Jiayi Xin\({}^{2}\), Qi Long\({}^{2}\), Tianlong Chen\({}^{1}\)**

\({}^{1}\)University of North Carolina at Chapel Hill

\({}^{2}\)University of Pennsylvania

\({}^{3}\)University of Science and Technology of China

{swyun, tianlong}@cs.unc.edu, {inyoungc, jiayixin}@seas.upenn.edu, {pengjieb, ustc_wyf}@mail.ustc.edu.cn

{jingxuan.bao, qiyiwen.zhang}@pennmedicine.upenn.edu, qlong@upenn.edu

###### Abstract

Multimodal learning has gained increasing importance across various fields, offering the ability to integrate data from diverse sources such as images, text, and personalized records, which are frequently observed in medical domains. However, in scenarios where some modalities are missing, many existing frameworks struggle to accommodate arbitrary modality combinations, often relying heavily on a single modality or complete data. This oversight of potential modality combinations limits their applicability in real-world situations. To address this challenge, we propose Flex-MoE (Flexible Mixture-of-Experts), a new framework designed to flexibly incorporate arbitrary modality combinations while maintaining robustness to missing data. The core idea of Flex-MoE is to first address missing modalities using a new missing modality bank that integrates observed modality combinations with the corresponding missing ones. This is followed by a uniquely designed Sparse MoE framework. Specifically, Flex-MoE first trains experts using samples with all modalities to inject generalized knowledge through the generalized router (\(\)-Router). The \(\)-Router then specializes in handling fewer modality combinations by assigning the top-1 gate to the expert corresponding to the observed modality combination. We evaluate Flex-MoE on the ADNI dataset, which encompasses four modalities in the Alzheimer's Disease domain, as well as on the MIMIC-IV dataset. The results demonstrate the effectiveness of Flex-MoE, highlighting its ability to model arbitrary modality combinations in diverse missing modality scenarios. Code is available at: [https://github.com/UNITES-Lab/flex-moe](https://github.com/UNITES-Lab/flex-moe).

## 1 Introduction

In many fields, including healthcare, language, and vision, multimodal learning  has emerged as a crucial approach for integrating data from multiple sources such as clinical records, imaging, and genetic data. Multimodal data enables more comprehensive analysis and decision-making, offering the potential for improved diagnosis and prediction in various applications . However, a prominent challenge across these domains is the missing modality scenario , where not all modalities are consistently available for every instance due to diverse reasons such as individualized data collection protocols or the variable availability of certain modalities.

As an representative example, in Alzheimer's Disease (AD) , one of the most prevalent neurodegenerative disorders, handling this inherently multimodal data is crucial for accurate diagnosis (Figure 1). AD datasets often include a combination of clinical symptoms, imaging data , and genetic profiles . However, in real-world clinical settings, not all these modalities are readily available for each patient. Some data, such as clinical and imaging data, may be available from routine visits, whereas other data, such as genetic or biospecimen information, may require additional time to collect. This leads to incomplete datasets, which poses a challenge for existing models that tend to either rely heavily on single modalities or only utilize complete data, thereby missing the opportunity to leverage the full potential of multimodal learning (Figure 2).

**Single Modality and Complete Data Reliance.** The reliance on single-modality data or complete data across many frameworks is a significant limitation in real-world scenarios, where missing data is the norm rather than the exception. As seen in Figure 2, many current models either work with single-modality data or focus on the intersection of modalities, neglecting the potential contribution of partially available modalities. In healthcare, particularly for diseases such as AD, this often leads to missed opportunities in diagnosis and treatment due to the inability to fully exploit multimodal data when some modalities are missing.

**Oversight of Modality Combinations.** Beyond the challenge of missing modalities, there is also the need to model the interactions between available modalities properly. Different combinations of modalities can provide complementary information, and each combination may hold unique significance for downstream tasks. For example, in AD diagnosis, combining biospecimen data and imaging data can reveal key insights: cerebrospinal fluid biomarkers may indicate early signs of AD , while functional MRI can highlight cognitive impairments . Hence, it is essential to develop models that not only handle missing modalities but also effectively utilize the available modality combinations.

Given the general challenge of the missing modality scenario in multimodal learning, we propose a novel framework, Flex-MoE (Flexible Mixture-of-Experts), to flexibly incorporate arbitrary modality combinations while maintaining robustness to missing data. Flex-MoE first sort samples based on the available modalities and process them through modality-specific encoders. For missing modalities, we introduce a learnable missing modality bank, which provides learnable embeddings for missing modalities based on the observed ones. This approach ensures that the model can handle incomplete datasets effectively. Our framework also builds upon Sparse Mixture-of-Experts (SMoE) design, allowing us to generalize the expert knowledge from complete data (samples with all modalities) through the \(\)-Router, followed by a specialized \(\)-Router for handling fewer modality combinations. Each expert becomes specialized in handling different modality combinations, ensuring that the model can effectively process any combination of modalities. We demonstrate the effectiveness of Flex-MoE through comprehensive experiments on several real-world datasets, including the Alzheimer's Disease Neuroimaging Initiative (ADNI), which involves four key modalities for AD stage prediction, and the MIMIC-IV dataset. The results confirm the robustness of Flex-MoE in diverse missing modality scenarios.

The contributions of this work can be summarized as follows:

Figure 1: Multimodal AD.

Figure 2: Data statistics from a real-world multimodal dataset (e.g., the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI)), where patients exhibit unique combinations of available modalities. Existing approaches focus on either (a) single-modality data or (b) complete multimodal data, losing the potential to leverage other combinations. Our approach incorporates all possible modality combinations, offering a more robust solution to the missing modality scenario.

* We introduce a flexible framework that effectively incorporates arbitrary modality combinations and addresses the missing modality scenario across various domains.
* Flex-MoE features a novel approach, including a missing modality bank and generalized and specialized expert training, which ensures robustness to missing modality scenario.
* Extensive experiments on real-world datasets, including ADNI and MIMIC-IV, showcase the consistent and robust performance of Flex-MoE in handling diverse modality combinations.

## 2 Related Works

**Single Modality Approach** In many fields, deep learning models often rely on single modality data for tasks such as classification [15; 13; 31; 71], diagnosis [56; 72], or prediction [12; 61; 34]. While effective in certain cases, these approaches fail to capture the potential synergies between different data sources, especially in contexts where multiple modalities are available. In the Alzheimer's Disease domain, many studies focus on specific modalities. For instance, image-based approaches include a VGG19 model  that diagnoses early-stage AD from MRI scans and a modified ResNet18 architecture  that predicts AD progression using fMRI data. Other studies focus on genomics, such as DLG  for classifying AD patients and SWAT-CNN  for discovering AD-associated genetic variants. In the biospecimen modality, a deep learning-assisted spectroscopy platform  diagnoses AD by analyzing blood-based amyloid-beta and metabolite biomarkers. Regarding clinical data, a deep learning model  outperforms earlier machine learning techniques in classifying AD patients. However, since AD data is inherently multimodal, methods based on a single modality are suboptimal, missing the potential to leverage interactions between different modalities.

**Multimodal Approach** Across multiple fields, multimodal learning has become increasingly valuable for its ability to integrate and capture dynamics within and across different modalities, providing richer and more comprehensive representations of data. Approaches such as the Tensor Fusion Network , Multimodal Transformer , and Multimodal Adaptation Gate  highlight the effectiveness of combining multiple data sources. Recently, sparse mixture-of-experts-based methods, such as [44; 8; 19], have been introduced to enhance modality interactions, though these methods are still relatively unexplored in the AD domain due to the complexity of handling various modality combinations. In AD research, some works have emerged to leverage multimodal data, such as  and , which integrated a deep learning framework that combines imaging, genetic, and clinical data, achieving superior AD staging accuracy. Another multimodal model , incorporating longitudinal and cross-sectional data, provided more accurate AD predictions. While multimodal AD studies have shown significant progress, the challenge of missing modalities, especially in the context of how to effectively cope with modality combinations, remains largely underexplored.

## 3 Methods

### Preliminaries and Notations

**Why Sparse Mixture-of-Experts?** Given a multimodal nature, we choose to utilize Sparse Mixture-of-Experts (SMoE)  due to its computational efficiency and its ability to handle multimodal data by effectively alleviating the gradient conflict optimization issue between modalities . To briefly introduce SMoE, Traditional Mixture-of-Experts (MoE) models [23; 28; 9; 70] evolved by incorporating sparsity into their structure, optimizing computational efficiency and model performance. SMoE selectively activates only the most relevant experts for a given task, reducing overhead and improving scalability. This innovation is particularly beneficial in handling complex, high-dimensional datasets across diverse applications. It has been widely used in vision [54; 40; 16; 2; 18; 62; 69; 1; 49] and language processing [35; 30; 78; 77; 79; 25] fields, dynamically assigning different parts of the network to specific tasks [41; 3; 20; 11] or data modalities [32; 44]. Research has shown its effectiveness in classification tasks in digital number recognition  and medical signal processing . In this work, we further explore the use of SMoE to model arbitrary modality combinations and address the missing modality scenario.

**Notation.** Formally, the SMoE consists of multiple experts, denoted as \(f_{1},f_{2},,f_{|E|}\), where \(|E|\) represents the total number of experts, and a router, \(\), which is responsible for the routing mechanism and sparsely selects the top-\(k\) experts. For a given embedding or token \(\), the router engages the top-\(k\) experts based on the highest scores obtained from softmax function with learnable gating function, \(g()\) (usually one or two layer MLP), and output \(()_{i}\), where \(i\) denotes the expert index. This process can be described as follows:

\[ =_{i=1}^{|E|}()_{i} f_{i}(), \] \[() =((g()),k),\] \[(,k) =,&k,\\ 0,&.\]

### Our approach: Flex-MoE

In this section, we present our novel algorithm, Flex-MoE, specifically designed to flexibly address the challenge of missing modalities in the multimodal domain. We start by sorting the samples based on their number of observed modalities. Following a modality-specific encoder, we supplement the embeddings for missing parts via missing modality bank completion. This effectively manages missing modalities by learning embedding banks that capture the information specific to observed modality combinations. Next, a Transformer coupled with an SMoE layer is employed. We introduce an expert generalization and specialization step to optimize modality utilization by fully leveraging samples with complete modalities and obtaining modality combination-specific knowledge through samples with fewer modalities. A comprehensive illustration of Flex-MoE is provided in Figure 3. Throughout the details in the following section, while our work is exemplified through the AD domain for predicting AD stages using four representative modalities--image, clinical, biospecimen, and genetic--it is important to note that Flex-MoE can be generalized to any other multimodal domain.

#### 3.2.1 Missing Modality Bank Completion

Given a set of samples with their own modalities, it is straightforward to pass them through modality-specific encoders, such as a 3D-CNN for MRI images. However, we are dealing with a _missing

Figure 3: The comprehensive illustration of our proposed methodology, Flex-MoE. (a) Overall framework of Flex-MoE. Given samples with diverse modality combinations, we first sort the samples based on their number of available modalities in descending order, and then pass through the modality-specific encoder. (b) Each encoder is only trained with their available samples. For the missing embeddings, we introduce a missing modality bank containing learnable embeddings given the observed modality combination with their corresponding missing modality index. Equipped with this embedding, Flex-MoE passes through the Transformer where the FFN layer is replaced with a Sparse MoE layer. Here, (c) full modality samples take charge of training generalized experts in a balanced manner via \(\)-router, then (d) the remaining few modality combinations further specialize the expert knowledge with \(\)-Router, which fixes the top-1 gate as the corresponding observed modality combination expert. In this figure, top-2 selection of experts is illustrated as an example.

modality scenario_ in multimodal data, where specific modalities are often missing based on their observed modality combinations. Thus, it is common to use padded or imputed inputs for the corresponding missing modalities in a multimodal setting. This approach becomes troublesome when considering interactions between modalities. For instance, given a batch with samples, some samples might have image, genetic, and clinical modalities, while others might be missing image and genetic modalities. In such cases, the image encoder and genetic encoder would take zero-padded or hypothesized imputed inputs derived from the missing samples, which are synthetic and of lower quality than the observed ones. This negatively affects the training of modality-specific encoders. Additionally, heavy imputation for each modality in a multimodal setting increases time complexity, which is not desirable in clinical settings.

Given this situation, we propose training each encoder _solely with observed samples_ to fully leverage the potential of the encoder by using only observed inputs. Unlike existing approaches, our design principle considers modality combinations to ensure robust and flexible training and effective handling of missing modalities. As illustrated in Figure 1, each patient has diverse symptoms and personalized treatments, leading to variations in available modalities. For example, patients might lack image and genomic modalities (i.e., possessing only biospecimen and clinical modalities) due to various reasons such as patient conditions, resource limitations, or specific clinical settings . Imputing these missing modalities must be handled within this context, rather than applying a global learnable representative embedding for each modality without considering the observed environment.

Therefore, we propose a learnable missing modality bank. Given the number of modality combinations without fully observed scenarios, the total cases would be, given a modality set \(=\{,,,\}\), \(_{m=1}^{||-1}|}{m}=2^{||}-1\). The resulting missing modality bank can be defined as \(^{2||-1||}\). By using this bank, the concatenated embedding of all modalities for patient \(i\), \(_{i}=[_{i}^{},_{i}^{}, _{i}^{},_{i}^{}]\) would be represented as follows:

\[_{i}^{m}=^{m}(i)&mi\\ _{ m,m}&, m \{,,,\} \]

where \(_{i}^{m}^{d}\) denotes the embedding from the corresponding modality \(m\) for patient \(i\), and \(d\) is the hidden dimension. Here, the main idea of the missing modality bank completion is to supplement missing modalities from a predefined bank, ensuring robust data integration with observed ones. For example, if a patient lacks clinical data but has imaging, biospecimen, and genetic data, the observed modalities pass through their specific encoders. The missing clinical embedding is supplemented from the missing modality bank, indexed by the observed modalities (e.g., \(_{ m,m}=_{\{,, \},}\)). By doing so, the encoder for each modality can be trained without encountering non-observed, incomplete input features. Then, we move on to the Transformer layer, where the FFN layer is replaced by an SMoE layer, a common approach in the SMoE domain .

#### 3.2.2 Expert Generalization & Specialization

While adopting the SMoE backbone, it is important to note that our environment differs from concurrent SMoE studies, especially in terms of multimodal learning with missing modalities. In this context, choosing the most relevant tokens is challenging for experts, since the significance, i.e., the quality of input information, varies with the missing modalities. This significantly motivates us to take a distinct approach from concurrent SMoE studies, where the input token is derived from fully observed scenarios. To address the unique challenges of the missing modality scenario, we propose a modality combination-specific MoE design. Specifically, we assign expert indices based on all possible modality combinations. For example, 'IGCB' is assigned as 0, 'IGC' as 1,..., up to 'B' as 14. The remaining experts act as buffers, allowing the Router to select the most relevant top-\(k\) experts and activate them automatically. This approach leaves room for flexibility and maintains the initial intuition of the MoE design.

**Generalization** It now becomes clear why the samples used for training Flex-MoE are sorted in descending order. Inspired by curriculum learning , where easy samples are presented first and more challenging samples appear later, we regard the level of difficulty as the number of missing modalities. We first train our SMoE layer with easy samples, where all modalities are fully observed. Using this intersection as a gold standard, we initially train all the experts in the MoE model. The procedure essentially follows the vanilla SMoE design as described in Equation 1, but with one key difference: the input tokens consist only of inputs where all modalities are fully observed. Hence, we refer to this router as the Generalized Router, \(\)-Router. This approach leverages the completeness of information in these samples, which should be fully utilized before specializing the experts in their respective areas. To ensure balanced activation of the experts initially, which will later specialize, we incorporate the load and importance balancing loss , which will later be exemplified in Equation 4.

**Specialization** Once the experts are initially trained using fully observed samples, we aim to specialize each expert, which is the key advantage of the MoE design. We leverage the remaining samples, which encompass diverse modality combination configurations. Each modality combination requires its own specialized expertise. For instance, samples with Image, Biospecimen, and Genetic data will have a corresponding expert index activated through the top-1 gating mechanism to fully utilize the specialized knowledge of that expert (i.e., expert 'IBG' in Figure 3). To effectively specialize the modality combination-specific experts, we propose a Specialized Router design, \(\)-Router, which encompasses following technical novelties. First, to facilitate targeted expert selection when an input token is provided, we avoid manually replacing the selected routing policy with our preferred choice in a post-hoc manner, which would stop the continuous gradient flow. Instead, we innovatively introduce a cross-entropy loss between the top-1 expert selection and the targeted expert indices for each token by the \(S\)-Router. Formally, this can be described as follows:

\[_{ce}=-_{j=1}^{n}(_{j})(( (_{j}))) \]

where \((_{j})\) denotes the modality combination index of a given token \(_{j}\) in a total of \(n\) tokens. \(((_{j}))\) denotes the maximum prediction probability of the corresponding activated expert index, which corresponds to the probability of the top-1 expert index. By comparing these two, the router is trained to activate the corresponding expert index for a given input token with a certain modality combination. Thus, the specialized knowledge inherent in specific modality combinations is naturally contained within the target expert.

Moreover, when calculating load and importance balancing loss , we specifically compute the loss for the _remaining_ top-\(k\)-1 experts, as the top-1 selection is manually handled and thus considered biased rather than balanced. We aim for the selection of the remaining \(k\)-1 experts to occur in a balanced manner, allowing interaction with other related modality combinations. Formally, it can be expressed as follows:

\[_{}=^{2}( _{j}^{N}_{j})+^{2}(_{j}^{N}_{j})\\ ~{}importance}_{e}=_{i}^{N}g_{ie},~{}~{}_{e}=_{i}^{N}(g_{ie}>0),~{}~{}~{} e E e_{}  \]

where \(^{2}(x)=()^{2}\), \((x)\) is the standard deviation of \(x\), \((x)\) is the mean of \(x\), \(g_{ie}\) is the gate value for sample \(i\) with expert index \(e\) as discussed in Equation 1, and \((>0)\) is an indicator function that is 1 when the inner value is greater than 0. \(E e_{}\) denotes the set of expert indices excluding the top-1 selected expert index. This ensures that the resulting MoE model not only retains global knowledge but also incorporates specialized expert knowledge tailored to specific modality combinations. During the inference phase, the specified expert index for a particular modality combination can be activated alongside other experts, enabling predictions to consider both the specific modality combination and intersections with other modalities.

Finally, the output embeddings of the Sparse MoE layer pass through a 1-layer MLP prediction head to predict one of the three stages of AD, i.e., Dementia, CN, or MCI. To further facilitate a curriculum-learning approach, we first use warm-up epochs with sorted samples, followed by shuffled samples for the remaining epochs. This strategy enhances generalizability across diverse input samples, enabling better handling of variability during the inference phase.

Experiments

### Experimental Setting

**ADNI Dataset** Alzheimer's Disease Neuroimaging Initiative (ADNI) is a landmark multimodal AD dataset that tracks disease progression and pathological changes, comprising of comprehensive imaging, genetic, clinical, and biospecimen data (, ). The imaging data in ADNI includes magnetic resonance imaging (MRI) and positron emission tomography (PET). The genetic data includes a variety of genetic information, including genotyping data such as APOE genotyping and single nucleotide polymorphisms. The clinical data includes demographics, physical examinations, and cognitive assessments. Biospecimens such as blood, urine, and cerebrospinal fluid are also collected. ADNI has established standardized multi-center protocols and provides open access to qualified researchers, making it a gold-standard resource in the field (, ). Before integrating all modalities, to address the initial missing data within each modality, we applied simple mean imputation  for each column. For more detailed data table with preprocessing steps for each modality, please refer to Appendix A.1.

**MIMIC-IV Dataset** We use the Medical Information Mart for Intensive Care IV (MIMIC-IV) database , which contains de-identified health data for patients who were admitted to either the emergency department or stayed in critical care units of the Beth Israel Deaconess Medical Center in Boston, Massachusetts24. MIMIC-IV excludes patients under 18 years of age. We take a subset of the MIMIC-IV data, where each patient has at least more than 1 visit in the dataset as this subset corresponds to patients who likely have more serious health conditions. For each datapoint, we extract ICD-9 codes, clinical text, and labs and vital values. Using this data, we perform binary classification on one-year mortality, which foresees whether or not this patient will pass away in a year. We drop visits that occur at the same time as the patient's death. In order to align the experimental setup with the ADNI data, which does not contain temporal data, we take the last visit for each patient.

**Baselines** We compare the performance of Flex-MoE with various state-of-the-art baselines across modality-specific, e.g., image or genetic, and multimodal approaches. For the image-only modality, we first experimented with 3D MRI scans by utilizing a 3D CNN  and an architecture that combines 3D CNN and 3D CLSTM . To decrease computational complexity, we also extracted 2D slices from the 3D volumes. For 2D MRI scans, we implemented the VGG architecture with pre-trained weights and applied layer-wise transfer learning , as well as a modified ResNet-18 network . For the genetic-only approach, we employed a ResNet-34 based architecture to handle the high-dimensional genetic data . In ADNI dataset, we further implemented domain specific baselines, such as auto-encoder and 3D CNN-based architecture that incorporates imaging, genetic, and clinical data , and a GRU-based architecture that considers imaging, genetic, clinical, and biospecimen data . Moreover, we include ShaeSpec , which utilizes a spectral attention mechanism to emphasize important features across modalities, and mmFormer , which is based on transformer-based multimodal fusion with an attention mechanism. For multimodal approaches in both ADNI and MIMIC-IV, we incorporate the recent FuseMOE  model, which directly integrates multimodal data through a mixture of experts strategy, as the most straightforward baseline. Additionally, we compare the following methods: MulT , which captures cross-modal interactions through cross-attention mechanisms; MAG , which fuses multimodal features by mapping them to an adaptation vector; TF , which combines multimodal embedding sub-networks and a tensor fusion layer; and LIMoE , which addresses training stability in multimodal learning using entropy regularization based on contrastive learning.

**Experimental Settings.** To ensure a fair comparison with other baselines, we used the best hyperparameter settings provided in the original papers. If not available, we tuned the learning rate in 1e-3, 1e-4, 1e-5, the hidden dimension in 64, 128, 256, and the batch size in 8, 16. For our proposed method, we searched the number of experts in 16, 32, and Top-\(k\) in 2, 3, 4. We set the coefficient of the sum of additional losses (importance and load balancing) combined with our cross-entropy loss to 0.01, scaling it within the task classification loss. For the dataset split, we chose 70% for training, with the remaining 30% split evenly between validation and test sets (15% each). It is important to note that, to share the same inference space, where single and multimodal baselines should both be able to predict, we opted to choose the intersection as the test and validation sets. This means that during the training phase, the dataset can be incomplete. For the multi-modal baselines, if they had the ability to impute or interact with other modalities, we leveraged their methods. Otherwise, we used zero-padding to facilitate batch-wise training. For single-modal and multi-modal baselinesthat solely work on the intersection region, we filtered that data and used it during training. All experiments were conducted using NVIDIA A100 GPUs. Each experiment was run three times with different seeds to ensure reproducibility, and the results were averaged. The optimal hyperparameter settings for Flex-MoE can be found in Appendix A.2.

### Primary Results

In Table 1 and Table 2, we provide a comprehensive comparison of Flex-MoE with various multimodal baselines. We have the following observations: **(1)** Overall, Flex-MoE performs effectively in diverse multimodal settings, fully harnessing its potential as more modalities become available. This is supported by the large margin of improvement (7.6% and 11.07% over the best performing baselines, MAG and the most recent work FuseMoE, respectively, in full modality settings in Table 1). **(2)** Although the recently proposed FuseMoE  suggested its ability to handle missing scenarios, the lack of effective modality combination creates a bottleneck in such AD domain, even performing worse when a smaller number of modalities is used (FuseMoE performs better with three modalities than with full modalities), which is not optimal given the diverse missing modality scenarios. **(3)** Despite its specific characteristics in the AD domain , the reliance on intersection data and the lack of consideration for how missing modalities relate to observed modality combinations have been overlooked. **(4)** Overall, the performance gain derived from Flex-MoE can be attributed to its unique ability to cope with diverse modality combinations through a missing modality bank, and its capability to fully harness the knowledge of samples via a generalization followed by a specialization step for experts. For additional results on different metrics, such as Macro-F1 and AUC, please refer to Appendix A.3.

### Effectiveness of Modality Combination Consideration

To validate the effectiveness of the two essential modules of Flex-MoE--the _missing modality bank_ and the _unique SMoE design_--under a missing modality scenario, we evaluate them followingly.

First, to evaluate the effectiveness of the missing modality bank introduced in Figure 3 (b), we assess whether it captures relevant embedding information given an observed modality combination. Specifically, we validate this by examining the inter-relationship between modalities, focusing on

    &  &  \\  \(\) & & & & & & & & & & & & & & & & \\  \(,\) & & & & & & & & & & & & & & & \\ \(,\) & & & & & & & & & & & & & & & \\ \(,\) & & & & & & & & & & & & & & & \\ \(,\) & & & & & & & & & & & & & & & \\  \(,,\) & & & & & & & & & & & & & & & \\  \(,,\) & & & & & & & & & & & & & & & & \\  \(,,\) & & & & & & & & & & & & & & & & \\   

Table 1: Performance on ADNI dataset with ACC metric across different models and modality combinations, given the Image (\(,\)), Genetic (\(,\)), Clinical (\(,\)), and Biospecimen (\(,\)) modalities. \(\) denotes observed modality combination.

    & &  &  \\  \(\) & & & & & & & & & & & & & & & & \\  \(,\) & & & & & & & & & & & & & & & & \\ \(,\) and \(,,\) settings, it trades off computational efficiency with significantly lower performance compared to Flex-MoE. **(3)** Notably, as the number of modalities increases, existing models tend to become more complex in terms of GFLOPs and the number of parameters to manage the additional complexity. However, Flex-MoE remains robust and efficient, maintaining higher performance due to its effective use of sparsely activated experts, brought by the SMoE framework.

## 5 Conclusion

While multimodal learning brings new opportunities and challenges across various domains, including medical fields, existing approaches struggle to handle arbitrary modality combinations, especially in missing modality scenarios, often relying on single modalities or complete datasets. In this work, we propose a flexible multimodal learning framework, Flex-MoE, capable of managing arbitrary subsets of available modalities. By carefully considering modality combination, it leverages a learnable embedding bank to capture missing modality information and utilizes a unique SMoE design to enhance expert generalization and specialization. Extensive experiments on the representative ADNI and MIMIC-IV datasets validate its effectiveness in handling diverse modality combinations. Future work includes extending the framework to explore the scaling laws of available modalities, which in turn presents numerous modality combinations, offering significant room for further improvement.

**Societal Impact and Limitation:** The proposed algorithm has the potential to significantly improve early diagnosis and treatment outcomes for patients, reducing the burden on healthcare systems. However, its effectiveness can be limited by the availability of comprehensive and high-quality patient data, and there may be challenges in integrating this tool into existing clinical workflows.

AcknowledgementThis work is supported by RF1-AG063481, R01-AG071174, Gemma Academic Program, and OpenAI Researcher Access Program.

  
**Modality** & **Metric** & **TF** & **MuIT** & **MAG** & **LIMOE** & **FuseMoE** & **Flex-MoE** \\  ,\)} & Mean Time (s) (â†“) & 12.40 & 12.85 & 11.64 & 12.65 & 18.68 & 12.73 \\  & GFLOPs (â†“) & 59.05 & 59.24 & 59.06 & 59.24 & 59.74 & 59.06 \\  & \# of Parameters (â†“) & 33,370,898 & 37,343,683 & 36,454,595 & 37,344,707 & 264,680,387 & 36,516,807 \\  & Accuracy (â†“) & 59,94 \(\)0.40 & 60.32 \(\)0.95 & 59.994 \(\)1.01 & 59.29 \(\)0.95 & 60.41 \(\)0.87 & **61.08 \(\)0.78** \\   ,,\)} & Mean Time (s) (â†“) & 13.80 & 23.28 & 14.55 & 14.64 & 18.68 & 14.53 \\  & GFLOPs (â†“) & 59.05 & 59.59 & 59.06 & 59.32 & 59.74 & 59.06 \\  & \# of Parameters (â†“) & 34,424,162 & 40,185,923 & 36,504,643 & 37,960,643 & 340,929,475 & 36,685,511 \\   ,,,\)} & Accuracy (â†‘) & 54.06 \(\)1.98 & 60.97 \(\)0.95 & 61.34 \(\)0.61 & 53.50 \(\)2.25 & 60.97 \(\)3.32 & **63.21 \(\)1.73** \\    & Mean Time (s) (â†“) & 15.83 & 38.70 & 16.04 & 17.96 & 20.71 & 16.00 \\  & GFLOPs (â†“) & 59.39 & 60.12 & 59.06 & 59.41 & 59.76 & 59.07 \\  & \# of Parameters (â†“) & 119,483,922 & 46,409,667 & 36,504,643 & 38,638,531 & 340,929,475 & 36,916,167 \\   

Table 4: Complexity comparison of mean time, GFLOPs, and # of parameters in ADNI dataset.

Figure 6: Sensitivity analysis of Flex-MoE. The hyper-parameters include the number of experts, the number of SMoE layers and Top-\(k\) expert selection. For the experiment, ADNI dataset with full modalities is used.