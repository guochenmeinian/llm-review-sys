# Nonconvex Federated Learning on Compact Smooth Submanifolds With Heterogeneous Data

Jiaojiao Zhang\({}^{1}\),  Jiang Hu\({}^{2}\),  Anthony Man-Cho So\({}^{3}\),  Mikael Johansson\({}^{1}\)

\({}^{1}\) KTH Royal Institute of Technology

\({}^{2}\) University of California, Berkeley \({}^{3}\) The Chinese University of Hong Kong

{jiaoz, mikaeljj}@kth.se, hujiangopt@gmail.com, manchoso@se.cuhk.edu.hk

Corresponding author: Jiang Hu

Throughout this paper, all convexity-related concepts are defined in the Euclidean space.

###### Abstract

Many machine learning tasks, such as principal component analysis and low-rank matrix completion, give rise to manifold optimization problems. Although there is a large body of work studying the design and analysis of algorithms for manifold optimization in the centralized setting, there are currently very few works addressing the federated setting. In this paper, we consider nonconvex federated learning over a compact smooth submanifold in the setting of heterogeneous client data. We propose an algorithm that leverages stochastic Riemannian gradients and a manifold projection operator to improve computational efficiency, uses local updates to improve communication efficiency, and avoids client drift. Theoretically, we show that our proposed algorithm converges sub-linearly to a neighborhood of a first-order optimal solution by using a novel analysis that jointly exploits the manifold structure and properties of the loss functions. Numerical experiments demonstrate that our algorithm has significantly smaller computational and communication overhead than existing methods.

## 1 Introduction

Federated learning (FL), which enables clients to collaboratively train models without exchanging their raw data, has gained significant traction in machine learning . The framework is appreciated for its capacity to leverage distributed data, accelerate the training process via parallel computation, and bolster privacy protection. The majority of existing FL algorithms address problems that are either unconstrained or have convex constraints. However, for applications such as principal component analysis (PCA) and matrix completion, where model parameters are subject to nonconvex manifold constraints, there are very few options in the federated setting.

In this paper, we study FL problems over manifolds in the form of

\[*{minimize}_{x^{d k}}f(x):= _{i=1}^{n}f_{i}(x), f_{i}(x)=}_{l=1}^{m_ {i}}f_{il}(x;_{il}).\] (1)

Here, \(n\) is the number of clients, \(x\) is the matrix of model parameters, and \(\) is a compact smooth submanifold embedded in \(^{d k}\). Examples of such manifolds include the Stiefel, oblique, and symplectic manifolds . For instance, PCA-related optimization problems use the Stiefel manifold \(=(d,k)=\{x^{d k}:x^{T}x=I_{k}\}\) to maintain orthogonality . In (1), the global loss \(f:^{d k}\) is smooth but nonconvex1, and the local loss function \(f_{i}\) of each client \(i\) is the average of the losses \(f_{il}\) on the \(m_{i}\) data points in its local dataset \(_{i}=\{_{i1},,_{im_{i}}\}\). We consider a heterogeneous data scenario where the statistical properties of \(_{i}\) differ across clients.

Manifold optimization problems of the form (1) appear in many important machine learning tasks, such as PCA [8; 9], low-rank matrix completion [10; 11], multitask learning [12; 13], and deep neural network training [14; 15]. Still, there are very few federated algorithms for machine learning on manifolds. In fact, the work  appears to be the only FL algorithm that can deal with manifold optimization problems of a similar generality as ours. Handling manifold constraints in an FL setting poses significant challenges: **(i)** Existing single-machine methods for manifold optimization [6; 5; 4] cannot be directly adapted to the federated setting. Due to the distributed framework, the server has to average the clients local models. Even if each of these models lies on the manifold, their average typically does not due to the nonconvexity of \(\). The current literature relies on complicated geometric operators like the exponential map, inverse exponential map, and parallel transport, to design an averaging operator for the manifold . However, these mappings may lack closed-form expressions and can be computationally expensive to evaluate. For example, computing the inverse exponential map on the Stiefel manifold requires solving a nonlinear matrix equation . **(ii)** Extending typical FL algorithms to scenarios with manifold constraints is not straightforward, either. Most existing FL algorithms either are unconstrained [18; 19] or only allow for convex constraints [20; 21; 22; 23; 24], but manifold constraints are typically nonconvex. Moreover, compared to nonconvex optimization in Euclidean space, manifold optimization necessitates the consideration of the geometric structure of the manifold and properties of the loss functions, which poses challenges for algorithm design and analysis. **(iii)** Traditional methods for enhancing communication efficiency in FL, like local updates , need substantial modifications to accommodate manifold constraints. The so-called client drift issue due to local updates and heterogeneous data  persists in the realm of manifold optimization. Directly using client-drift correcting techniques originally developed for Euclidean spaces [19; 26; 27] could lead to additional communication or computational costs due to the manifold constraints. For instance, in , the correction term requires additional communication of local Riemannian gradients and involves using parallel transport to move the correction term onto some tangent space in preparation for the exponential mapping. Although some existing decentralized manifold optimization algorithms [9; 28; 29] can be simplified to an FL scenario with only one local update under the assumption of a fully connected network, these algorithms cannot be directly applied to FL scenarios with more than one local update, especially in cases of data heterogeneity. Extending the analysis of these algorithms to FL scenarios with multiple local updates is not straightforward. On the other hand, the use of local updates in FL, compared to these decentralized distributed algorithms, can more effectively reduce the number of communication rounds.

### Contributions

We consider the nonconvex FL problem (1) with \(\) being a compact smooth submanifold and allow for heterogeneous data distribution among clients. Our contributions are summarized as follows.

**1)** We propose an FL algorithm for solving (1) that is efficient in terms of both computation and communication. We employ stochastic Riemannian gradients and a projection operator to address manifold constraints, use local updates to reduce the communication frequency between clients and the server, and design correction terms to overcome client drift. In terms of server updates, our algorithm ensures feasibility of all global model iterates and is computationally efficient since it avoids the techniques used in  based on the exponential mapping and inverse exponential mapping for averaging local models on manifolds. For local updates, our algorithm constructs the correction terms locally without increasing communication costs. In comparison, the approach presented in  requires each client to transmit an additional local stochastic Riemannian gradient for constructing correction terms. Moreover,  necessitates parallel transport to position the correction terms on tangent spaces so that the exponential map can be applied to ensure the feasibility of local models, thereby increasing computational costs. In contrast, our algorithm utilizes a simple projection operator, effectively eliminating the need for parallel transport of correction terms.

**2)** Theoretically, we establish sub-linear convergence to a neighborhood of a first-order optimal solution and demonstrate how this neighborhood depends on the stochastic sampling variance and algorithm parameters. Our analysis introduces novel proof techniques that utilize the curvature of the manifolds and the properties of the loss functions to overcome the challenges posed by the nonconvexity of manifold constraints in the nonconvex FL scenario. Compared to the existing work  where analytical results are limited to cases where either the number of local updates is one or the number of participating clients per communication round is one, our theoretical results allow for an arbitrary number of local updates and support full client participation. The key components of our analysis are the manifold geometry and the Lipschitz continuity of the projection operator, both of which are inherent to the submanifold constraint.

**3)** Our algorithm demonstrates superior performance over alternative methods in the numerical experiments. In particular, it produces high-accuracy results for kPCA and low-rank matrix completion at a significantly lower communication and computation cost than alternative algorithms.

### Related work

In this section, we first review federated learning algorithms for composite optimization with and without constraints. Then, we discuss FL algorithms with manifold constraints.

**Composite FL in Euclidean space.** Problem (1) can be viewed as a special case of composite FL where the loss function is a composition of \(f\) and the indicator function of \(\). It is important to note that since the manifold is nonconvex, its indicator function is also nonconvex. Most existing composite FL methods can only handle convex constraints. The work  proposed a federated dual averaging method and established its convergence for a general loss function under bounded gradient assumptions, but only for quadratic losses under the bounded heterogeneity assumption that the degree of data heterogeneity among clients is bounded. In contrast, we make no assumptions about the similarity of data across clients. The fast federated dual averaging algorithm  extends the work in  by using both past gradient information and past model information in the local updates. However, the work  requires each client to transmit the local gradient as well as the local model, and it assumes bounded data heterogeneity. The work  introduces the federated Douglas-Rachford method, and the work  applies this algorithm to solve dual problems. Although these two methods avoid bounded data heterogeneity, they require an increasing number of local updates to ensure convergence, which reduces their practicality in FL. The recent work  proposes a communication-efficient FL algorithm that overcomes client drift by decoupling the proximal operator evaluation and the communication and shows that the method converges without any assumptions on data similarity.

**Federated learning on manifolds.** The existing composite FL in Euclidean space - only considers scenarios where the nonsmooth term in the loss functions is convex. However, incorporating a nonconvex manifold constraint as an indicator function introduces a nonconvex nonsmooth term. Consequently, the methods in - are not directly applicable. A typical challenge caused by the nonconvex manifold constraint is that the average of local models, each of which lies on the manifold, may not belong to the manifold. To address this issue, the work  introduced Riemannian federated SVRG (RFedSVRG), where the server maps the local models onto a tangent space, calculates an average, and then retracts the average back to the manifold. This process sequentially employs inverse exponential and exponential mappings. Moreover, RFedSVRG employs a correction term to overcome client drift but requires additional communication of local Riemannian gradients to construct the correction term. In addition, the method uses parallel transport to position the correction term, which increases the computation cost even further. Note that the manifold we consider is a compact smooth submanifold embedded in \(^{d k}\), which is more restrictive than the manifolds discussed in . However, this approach still encompasses many common manifolds, including the Stiefel, oblique, and symplectic manifolds [3; 4; 5]. The work  explores the differential privacy of RFedSVRG. The work  considers the specific manifold optimization problem that appears in PCA and investigates an ADMM-type method that penalizes the orthogonality constraint. However, this algorithm requires solving a subproblem to desired accuracy, which increases computational cost. The work  introduces a differentially private FL algorithm for solving PCA. Finally, the works  and  consider the leading eigenvector problem with homogeneous data across clients, where the loss functions are quadratic and the manifold is a sphere. In contrast, we consider a more general setting with heterogeneous data, where \(x\) lies on \(\) and the loss functions are smooth and nonconvex.

**Notations.** We use \(I_{k}\) to denote a \(k k\) identity matrix. We use \(\|\|\) to denote Frobenius norm and \(()\) to denote the trace of a matrix. For a set \(\), we use \(||\) to denote the cardinality. For a random variable \(v\), we use \([v]\) to denote the expectation and \([v|]\) to denote the expectation given event \(\). For an integer \(n\), we use \([n]\) to denote the set \(\{1,,n\}\). For two matrices \(x,y^{d k}\), we define their Euclidean inner product as \( x,y:=_{i=1}^{d}_{j=1}^{k}x_{ij}y_{ij}\). For matrices \(z_{1},,z_{n}^{d k}\), we use \(:=\{z_{i}\}_{i=1}^{n}:=[z_{1};;z_{n}]^{nd k}\) to denote the vertical stack of all matrices. The bold notations \(}\), \(\), and \(\) are defined similarly. Specifically, for a matrix \(x^{d k}\), we define \(:=\{x\}_{i=1}^{n}:=[x;;x]^{nd k}\). We use \(r\) to denote the index of the communication round and \(t\) to denote the index of local updates. Given the local Riemannian gradient \(f_{i}(z_{t,t}^{r};_{i,t}^{r})\) at point \(z_{t,t}^{r}\) with the mini-batch dataset \(_{i,t}^{r}\), we define the stack of Riemannian gradients as \((_{t}^{r};_{i}^{r}):= \{f_{i}(z_{t,i}^{r};_{i,t}^{r})\}_{i=1}^{n}\) and the stack of average local Riemannian gradients as \(}(_{t}^{r};_{i}^{r}):= \{_{i=1}^{n}f_{i}(z_{i,t}^{r}; _{i,t}^{r})\}_{i=1}^{n}\). Given \(\{z_{i}\}_{i=1}^{n}\) and \(_{}(z_{i})\), we define \(_{}(\{z_{i}\}_{i=1}^{n})=\{ _{}(z_{i})\}_{i=1}^{n}\). Given a positive definite matrix \(x\), we use \(x^{-1/2}\) to denote the inverse of the square root of \(x\), i.e., \(x^{-1/2}x^{-1/2}=x^{-1}\). We define \(^{2}\) to be the second-order differential operator.

## 2 Preliminaries

Below, we introduce fundamental definitions and inequalities for optimization on manifolds.

### Optimization on manifolds

Manifold optimization aims to minimize a real-valued function over a manifold, i.e., \(_{x}\ f(x)\). Throughout the paper, we restrict our discussion to embedded submanifolds of the Euclidean space, where the associated topology coincides with the subspace topology of the Euclidean space. We refer to these as embedded submanifolds. Some examples of such manifolds include the Stiefel manifold, oblique manifold, and symplectic manifold [3; 4; 5]. We define the tangent space of \(\) at point \(x\) as \(T_{x}\), which contains all tangent vectors to \(\) at \(x\), and the normal space as \(N_{x}\) which is orthogonal to the tangent space. With the definition of tangent space, we can define the Riemannian gradient that plays a central role in the characterization of optimality conditions and algorithm design for manifold optimization.

**Definition 2.1** (Riemannian gradient \(f(x)\)).: _The Riemannian gradient \(f(x)\) of a function \(f\) at the point \(x\) is the unique tangent vector that satisfies_

\[f(x),_{x}=df(x)[],\  T_{x} ,\]

_where \(,_{x}\) is the Riemannian metric and \(df\) denotes the differential of function \(f\)._

For a submanifold \(\), the Riemannian gradient \(f(x)\) (under the Euclidean inner product) can be computed as [5, Proposition 3.61]

\[f(x)=_{T_{x}}( f(x)),\]

where \(_{T_{x}}( f(x))\) represents the orthogonal projection of \( f(x)\) onto \(T_{x}\). The Riemannian gradient \(f(x)\) reduces to the Euclidean gradient \( f(x)\) when \(\) is the Euclidean space \(^{d k}\).

### Proximal smoothness of \(\)

In our federated manifold learning algorithm, the server needs to fuse models that have undergone multiple rounds of local updates by the clients. Due to the nonconvexity of the manifold, the average of points on the manifold is not guaranteed to belong to the manifold. The tangent space-based exponential mapping or other retraction operations commonly used in manifold optimization are expensive in FL . Specifically, the server needs to map the local models onto a tangent space using inverse exponential mapping, calculate an average on the tangent space, and then perform an exponential mapping to retract this average back onto the manifold. This exponential mapping, due to its dependency on the tangent space, also calls for parallel transport during the local updates when there are correction terms. To overcome this difficulty, we use a projection operator \(_{}\) defined by

\[_{}(x)*{argmin}_{u}\|x-u \|^{2}\] (2)

to ensure the feasibility of manifold constraints. The projection operator \(_{}\) can be explicitly calculated for many common submanifolds, as discussed in . For the Stiefel manifold, the closed-form expression for \(_{}(x)\) of a given matrix \(x\) with full column rank is \(_{}(x)=x(x^{T}x)^{-1/2}\); see [35, Proposition 7]. It is worth noting that \(_{}\) can be regarded as a special retraction operator when restricted to the tangent space . However, unlike a typical retraction operator, its domain is \(^{d k}\), not just the tangent space, which enables a more practical averaging operation across clients in FL. Despite these advantageous properties, the nonconvex nature of \(\) means that \(_{}(x)\) may be set-valued and non-Lipschitz, making the use and analysis of \(_{}\) in the FL setting highly nontrivial. To tackle this, we introduce the concept of proximal smoothness that refers to a property of a closed set, including \(\), where the projection becomes a singleton when the point is sufficiently close to the set.

**Definition 2.2** (\(\)-proximal smoothness of \(\)).: _For any \(>0\), we define the \(\)-tube around \(\) as_

\[U_{}():=\{x:(x,)<\},\]

_where \((x,):=_{u}\|u-x\|\) is the Eulidean distance between \(x\) and \(\). We say that \(\) is \(\)-proximally smooth if the projection operator \(_{}(x)\) is a singleton whenever \(x U_{}()\)._

It is worth noting that any compact smooth submanifold \(\) embedded in \(^{d k}\) is a proximally smooth set [36; 37]. The constant \(\) can be calculated with the method of supporting principle for proximally smooth sets [38; 39]. For instance, the Stiefel manifold is \(1\)-proximally smooth.

**Assumption 2.3**.: _The manifold \(\) is assumed to be a compact smooth submanifold embedded in \(^{d k}\), with the Euclidean inner product serving as its Riemannian metric. Moreover, we assume that the proximal smoothness constant of \(\) is \(2\)._

With Assumption 2.3, we can ensure not only the uniqueness of the projection but also the Lipschitz continuity of the projection operator \(_{}\) around \(\), analogous to the non-expansiveness of projections under Euclidean convex constraints.

**Lipschitz continuity of \(_{}\).** Define \(_{}():=\{x:(x,) \}\) as the closure of \(U_{}()\). Following the proof in [36; Theorem 4.8], for a \(2\)-proximally smooth \(\), the projection operator \(_{}\) is \(2\)-Lipschitz continuous over \(_{}()\) such that

\[\|_{}(x)-_{}(y)\| 2\|x-y\|, \  x,y_{}().\] (3)

**Normal inequality.** In the normal space \(N_{x}\), we exploit the so-called normal inequality [36; 37]. Following , given a \(2\)-proximally smooth \(\), for any \(x\) and \(v N_{x}\), it holds that

\[ v,y-x\|y-x\|^{2}, y .\] (4)

Intuitively, when \(x\) and \(y\) are close enough, the matrix \(y-x\) is approximately in the tangent space, thus being nearly orthogonal to the normal space.

## 3 Proposed algorithm

In this section, we develop a novel algorithm for nonconvex federated learning on manifolds. The algorithm is inspired by the proximal FL algorithm for strongly convex problems in Euclidean space recently proposed in  but includes several non-trivial extensions. These include the use of Riemannian gradients and manifold projection operators and the ability to handle nonconvex loss functions, which call for a different convergence analysis.

### Algorithm description

The per-client implementation of our algorithm is detailed in Algorithm 1. Similarly to the well-known FedAvg, it operates in a federated learning setting with one server and \(n\) clients. Each client \(i\) engages in \(\) steps of local updates before updating the server. We use \(r\) as the index of communication rounds and \(t\) as the index of local updates.

At any communication round \(r\), client \(i\) downloads the global model \(x^{r}\) from the server and computes \(_{}(x^{r})\). Each client \(i\) updates two local variables, \(^{r}_{i,t}\) and \(z^{r}_{i,t}\), where \(^{r}_{i,t}\) aggregates the Riemannian gradients from local updates, and \(z^{r}_{i,t}=_{}(^{r}_{i,t})\) ensures that Riemannian gradients can be computed at points on \(\). The update of \(^{r}_{i,t}\) is given in Line 8, where \(^{r}_{i,t}\) is a mini-batch dataset and \(c^{r}_{i}\) is a correction term to eliminate client drift. After \(\) local updates, client \(i\) sends \(^{r}_{i,}\) to the server.

The server receives all \(^{r}_{i,}\), computes their average to form the global model \(x^{r+1}\) following Line 13, and broadcasts \(x^{r+1}\) to each client \(i\) that uses \(x^{r+1}\) to locally construct the correction term \(c^{r+1}_{i}\).

In the proposed algorithm, each client \(i\) downloads \(x^{r}\) at the start of local updates and uploads \(^{r}_{i,}\) at the end of the local updates. Therefore, each communication round involves each client and the server exchanging only a single \(d k\) matrix.

### Algorithm intuition and innovations

To better understand the proposed algorithm, we present its equivalent and more compact form:

\[\{ }^{r}_{t+1}& =}^{r}_{t}-( ^{r}_{t};^{r}_{t})+_{t=0}^{- 1}}(^{r-1}_{t};^{r-1}_{t} )-_{t=0}^{-1}(^{r-1}_ {t};^{r-1}_{t}),\\ ^{r}_{t+1}&=_{} (}^{r}_{t+1}),\\ ^{r+1}&=_{}(^{r})-_{g}_{t=0}^{-1}}( ^{r}_{t};^{r}_{t})..\] (5)

For the initialization of correction term, we set \(_{i}(z^{0}_{i,t};^{0}_{i,t})=0\) for all \(t\) and \(i\) so that \(_{t=0}^{-1}}(^{0 }_{t};^{0}_{t})-_{t=0}^{-1}(^{0}_{t};^{0}_{t})=0\), which coincides with the initialization \(c^{1}_{i}=0\) in Algorithm 1. The equivalence between Algorithm 1 and (5) can be proved following the similar derivations in  and is therefore omitted.

With (5), we highlight the key properties and innovations of the proposed algorithm.

**1) Recovery of the centralized algorithm in special cases.** Substituting the definitions of \(\), \(}(^{r}_{t};^{r}_{t})\), and \(\) into the last step in (5), we have

\[_{}(x^{r+1})=_{} _{}(x^{r})-_{i=1}^{n}_{t=0}^{ -1}f_{i}(z^{r}_{i,t};^{r}_{i,t} ).\] (6)

Thanks to the introduction of the variable \(^{r}_{i,t}\) during the local updates for each client \(i\) in Algorithm 1, the server after averaging \(^{r}_{i,}\) obtains an accumulation of \(\) local Riemannian gradients across local updates and an average of the local Riemannian gradients across all clients. In the special case where \(=1\) and \(b=m_{i}\), i.e., with the local full Riemannian gradient for each client \(i\), the update of (6) recovers the centralized projected Riemannian gradient descent (C-PRGD)

\[^{r+1}:=_{}(_{}(x^{ r})-f(_{}(x^{r}))).\] (7)In our analysis, we will compare the sequence \(_{}(x^{r+1})\) generated by our algorithm with the virtual iterate \(^{r+1}\) to establish the convergence of our algorithm.

**2) Feasibility of all iterates at a low computational cost.** Our algorithm uses \(_{}\) to obtain feasible solutions on the manifold, which is computationally more efficient than the commonly used exponential mapping. In fact, since the exponential mapping relies on a point on the manifold and the tangent space at that point, it cannot be directly used in our algorithm. In the local updates, it is difficult to perform exponential mapping on \(}^{r}_{t+1}\) because \(}^{r}_{t}\) is not on the manifold; see the first step in (5). As shown in , \(}^{r}_{t+1}\) is essential for the server to obtain aggregated Riemannian gradients from \(n\) clients after \(\) local updates. Moreover, at the server, although \(_{}(^{r})\) is on the manifold, the aggregated direction does not lie in the tangent space at \(_{}(^{r})\). The algorithm suggested in  uses an exponential mapping to fuse local models. It needs to map the local models to a tangent space using the inverse exponential mapping and then retract the result back to the manifold, which is computationally expensive. Our use of \(_{}\) on a point in the Euclidean space close to the manifold avoids these high computational costs, but creates new challenges for the analysis.

**3) Overcoming client drift.** Inspired by , we use a correction term \(c^{r}_{t}\) to address client drift. According to the first step of (5), the correction employs the idea of "variance reduction", which involves replacing the old local Riemannian gradient \(_{t=0}^{-1}(^{r-1}_{ t};^{r-1}_{t})\) with the new one \((^{r}_{t};^{r}_{t})\) in the average of all client Riemannian gradients \(_{t=0}^{-1}(^{r-1}_{ t};^{r-1}_{t})\), where the "variance" refers to the differences in Riemannian gradients among clients caused by data heterogeneity. Compared to , our correction improves communication and computation. The correction approach in  necessitates extra transmissions of local Riemannian gradients, while our correction term can be locally generated, leading to a significantly reduced communication overhead. Furthermore,  employs parallel transport to position the correction term with a specific tangent space for the exponential mapping to ensure local model feasibility. Our approach, which utilizes \(_{}\), eliminates the need for parallel transport and reduces the computations per iteration even further.

## 4 Analysis

In this section, we analyze the convergence of the proposed Algorithm 1. Throughout the paper, we make the following assumptions, which are common in manifold optimization.

**Assumption 4.1**.: _Each \(f_{il}(x;_{il}):^{d k}\) has \(\)-Lipschitz continuous gradient \( f_{il}(x;_{il})\) on the convex hull of \(\), denoted by \(()\), i.e., for any \(x,y()\), it holds that_

\[\| f_{il}(x;_{il})- f_{il}(y;_{il})\| \|x-y\|.\] (8)

With the compactness of \(\), there exists a constant \(D_{f}>0\) such that the Euclidean gradient \( f_{il}(x;_{il})\) of \(f_{il}\) is bounded by \(D_{f}\), i.e., \(_{i,l,x}\| f_{il}(x;_{il})\| D_{f}\). It then follows from [28, Lemma 4.2] that there exists a constant \( L<\) such that for any \(x,y\),

\[f_{il}(y;_{il}) f_{il}(x;_{il})+ f_{il}(x;_{il}),y-x+\|x-y\|^{2},\] \[\|f_{il}(x;_{il})-f_{il}(y;_{il})\| L\|x-y\|.\]

To address the stochasticity introduced by the random sampling \(^{r}_{i,t}\), we define \(^{r}_{t}\) as the \(\)-algebra generated by the set \(\{^{r}_{i,t} i[n],[r],[t-1]\}\) and make the following assumptions regarding the stochastic Riemannian gradients, similar to [40, Assumption 2].

**Assumption 4.2**.: _Each stochastic Riemannian gradient \(f_{i}(z^{r}_{t,t};^{r}_{i,t})\) in Algorithm 1 satisfies_

\[&[f_{i}(z^{r}_{i,t}; ^{r}_{i,t})|^{r}_{t}]=f_{i}(z^{ r}_{i,t}),\\ &[\|f_{i}(z^{r}_{i,t}; ^{r}_{i,t})-f_{i}(z^{r}_{i,t})\|^{2}| ^{r}_{t}]}{b}.\] (9)

Considering the nonconvexity of \(f\) and the manifold constraints, we characterize the first-order optimality of (1). A point \(x^{}\) is defined as a first-order optimal solution of (1) if \(x^{}\) and \(f(x^{})=0\). We employ the norm of \(_{}(_{}(x^{r}))\) as a suboptimality metric, defined as

\[_{}(_{}(x^{r})):=}(_{}(x^{r})-^{r+1}),\] (10)and \(^{r+1}\) defined in (7) is used only for analytical purposes. In optimization on Euclidean space such that \(=^{d k}\), the quantity \(_{}(_{}(x^{r}))\) serves as a widely accepted metric to assess first-order optimality for nonconvex composite problems . In optimization on manifold, we have \(_{}(_{}(x^{r}))=0\) if and only if \(f(_{}(x^{r}))=0\) for any \(>0\). Moreover, for a suitable \(\), the use of Riemannian gradients in the update of \(^{r+1}\) helps us to establish a quasi-isometric property, specifically that \(1/2\|f(_{}(x^{r}))\|\|_{ }(_{}(x^{r}))\| 2\|f( _{}(x^{r}))\|\); see Lemmas A.1 and A.2. With \(_{}(_{}(x^{r}))\), we have the following theorem.

**Theorem 4.3**.: _Under Assumptions 2.3, 4.1, and 4.2, if the step sizes satisfy_

\[:=_{g}\{,},L_{}}\},\;_{g}=,\] (11)

_where \(M=\{()/,2\}\), \(()=_{x,y}\|x-y\|\), \(D_{f}=_{i,l,x}\| f_{il}(x;_{il})\|\), and \(L_{}=_{x_{}()}\|^{2 }_{}(x)\|\), then the sequence \(_{}(x^{r})\) generated by Algorithm 1 satisfies_

\[_{r=1}^{R}\|_{}(_{ }(x^{r}))\|^{2}}{ R}+}{n b},\] (12)

_where \(^{1}>0\) is a constant related to initialization._

In Theorem 4.3, the first term on the right hand of (12) converges at a sub-linear rate, which is common for constrained nonconvex optimization [41; 42]. The second term is a constant error caused by the variance \(^{2}\) of stochastic Riemannian gradients.

**Theoretical contributions.** The work  establishes convergence rates of \((1/R)\) for \(=1\) and \((1/( R))\) for \(>1\) but only if a single client participates in the training per communication round. In contrast, our Theorem 4.3 achieves a rate of \((1/( R))\) for \(>1\) and full client participation. Our theorem indicates that multiple local updates enable faster convergence, which distinguishes our algorithm from decentralized manifold optimization algorithms [9; 28] that limit clients to do a single local update. The works  and  study smooth nonconvex FL with heterogeneous data in Euclidean space, relying on the assumptions of bounded second moments [43, Assumption 1] and \(B\)-local dissimilarity [1, Definition 3], respectively, to establish convergence. These assumptions imply some similarity between client data. Our main analytical advantages over  and  are twofold: We do not assume data similarity, and we completely eliminate client drift. This is reflected in our convergence error, which eliminates the error introduced by data heterogeneity. Our convergence analysis relies on several novel techniques. Specifically, we capitalize on the structure of \(\) and exploit the proximal smoothness of \(\) to guarantee the uniqueness of \(_{}\) and Lipschitz continuity of \(_{}\) within a tube around \(\). Additionally, we carefully select the step sizes to ensure that the iterates remain close to \(\), thus preserving the established properties throughout the iterations. Last but not least, we select an appropriate first-order optimality metric (see (10)) and jointly consider the properties of \(\) and the loss functions to establish some new inequalities for the convergence of this metric, given in Appendix.

## 5 Numerical experiments

In this section, we conduct numerical experiments on two applications on the Stiefel manifold: kPCA and the low rank matrix completion (LRMC). We compare with existing algorithms, including RFedavg, RFedprox, and RFedSVRG. RFedavg and RFedprox are direct extensions of FedAvg  and Fedprox . For RFedSVRG, there are no theoretical guarantees when we set \(>1\) and make all clients participate. In all alternative algorithms, the calculations of the exponential mapping, its inverse, and the parallel transport on the Stiefel manifold are needed. The exponential mapping has a closed-form expression but involves a matrix exponential , the inverse exponential mapping needs to solve a nonlinear matrix equation , and the parallel transport needs to solve a linear differential equation , all of which are computationally challenging. In their implementations, approximate versions of these mappings are used [16; 45; 46].

kPCA.Consider the kPCA problem

\[*{minimize}_{x(d,k)}f(x)=_{i=1}^{n}f_{ i}(x), f_{i}(x)=-(x^{T}A_{i}^{T}A_{i}x),\]where \((d,k)=\{x^{d k} x^{T}x=I_{k}\}\) denotes the Stiefel manifold, and \(A_{i}^{T}A_{i}^{d d}\) is the covariance matrix of the local data \(A_{i}^{p d}\) of client \(i\). We conduct experiments where the matrix \(A_{i}\) is from the Mnist dataset. The specific experiment settings can be found in Appendix A.4.1.

In the first set of experiments, we compare our algorithm with RFedavg, RFedprox, and RFedSVRG. RFedSVRG requires each client to transmit two \(d k\) matrices per communication round, while our algorithm only transmits a single matrix. We use communication quantity to count the total number of \(d k\) matrices that per client transmits to the server. We use the local full gradient \( f_{i}\) to mitigate the effects of stochastic gradient noise. In Figs. 1, we set \(=10\) and \(=1/\) for all algorithms, where \(\) is the square of the largest singular value of \(\{A_{i}\}_{i=1}^{n}\). We set \(_{g}=1\) to facilitate comparison with other algorithms. As noted below (40) in the Appendix, all analytical results leading up to (40) remain valid for \(_{g}=1\). It can be observed that RFedavg and RFedprox face the issue of client drift and have low accuracy. Both RFedSVRG and our algorithm overcome the client drift, but our algorithm, though being similar in terms of communication rounds, is much faster in terms of communication quantity and run time.

In the second set of experiments, we test the impact of \(\). For all the algorithms, we set the step size \(=1/\) and \(\{10,15,20\}\). For our algorithm, we set \(_{g}=1\). The experiment results are shown in Figs. 2. For all values of \(\), our algorithm achieves better convergence and requires less communication quantity.

In addition, we test the impact of stochastic Riemannian gradients with different batch sizes. We set \(=1/(20)\). As shown in Figs. 3, our algorithm converges to a neighborhood due to the sampling noise and larger batch size leads to faster convergence. Additional experimental results can be found in Appendix A.4.1.

Lrmc.LMRC aims to recover a low-rank matrix \(A^{d T}\) from its partial observations. Let \(\) be the set of indices of known entries in \(A\), the rank-\(k\) LRMC problem can be written as \(_{X(d,k),V^{k T}} {1}{2}\|_{}(XV-A)\|^{2}\), where the projection operator \(_{}\) is defined in an entry-wise manner with \((_{}(A))_{l_{1}l_{2}}=A_{l_{1}l_{2}}\) if \((l_{1},l_{2})\) and \(0\) otherwise. In terms of the FL setting, we consider the case where the observed data matrix \(_{}(A)\) is equally divided into \(n\) clients

Figure 1: kPCA problem with Mnist dataset: Comparison on \(\|f(x^{r})\|\).

Figure 2: kPCA with Mnist dataset: The impacts of \(\).

by columns, denoted by \(A_{1},,A_{n}\). Then, the FL LRMC problem is

\[*{minimize}_{X(d,k)}\ \ _{i=1}^{n}\| _{_{i}}(XV_{i}(X)-A_{i})\|^{2},\] (13)

where \(_{i}\) is the subset corresponding to client \(i\) in \(\) and \(V_{i}(X):=*{argmin}_{V}\|_{_{i}}(XV-A_{i})\|\). In the experiments, we set \(T=1000\), \(d=100\), \(k=2\), \(n=10\), and use the local full gradients. The other settings can be found in Appendix A.4.2.

The numerical comparisons with RFedavg, RFedprox, and RFedSVRG are presented in Figs. 4. Our algorithm and RFedSVRG achieve similar convergence for communication rounds, but our algorithm converges faster than RFedSVRG in terms of communication quantity and run time. Additional experimental results can be found in Appendix A.4.2.

## 6 Conclusions and limitations

This paper addresses the challenges of FL on compact smooth submanifolds. We introduce a novel algorithm that enables full client participation, local updates, and heterogeneous data distributions. By leveraging stochastic Riemannian gradients and a manifold projection operator, our method enhances computational and communication efficiency while mitigating client drift. By exploiting the manifold structure and properties of the loss function, we prove sub-linear convergence to a neighborhood of a first-order stationary point. Numerical experiments show a superior performance of our algorithm in terms of computational and communication costs compared to the state-of-the-art.

Our paper motivates several questions for further investigation. First, the absence of closed-form solutions for the projection operator \(_{}\) for certain manifolds necessitates exploring methods to calculate projections approximately. Additionally, our step-size selection relies on the proximal smoothness constant \(\), underscoring the need for estimating \(\) either off-line for specific manifolds or adaptively on-line. Furthermore, designing algorithms for partial participation and devising corresponding client-drift correction mechanisms require further investigation.

Figure 4: LRMC: Comparison on \(\|f(x^{r})\|\).

Figure 3: kPCA problem with Mnist dataset: The impacts of stochastic Riemannian gradients.