# Self-Distilled Depth Refinement

with Noisy Poisson Fusion

 Jiaqi Li\({}^{1,*}\) &Yiran Wang\({}^{1,*}\) &Jinghong Zheng\({}^{1}\)

Zihao Huang\({}^{1}\) &Ke Xian\({}^{2}\) &Zhiguo Cao\({}^{1,}\) &Jianming Zhang\({}^{3}\)

\({}^{1}\)School of AIA, Huazhong University of Science and Technology

\({}^{2}\)School of EIC, Huazhong University of Science and Technology

\({}^{3}\)Adobe Research

\({}^{*}\)Equal contribution \({}^{}\)Corresponding author

{lijiaqi_mail,wangyiran,deepzheng,zihaohuang,kxian,zgcao}@hust.edu.cn

jianmzha@adobe.com

https://github.com/lijia7/SDDR

###### Abstract

Depth refinement aims to infer high-resolution depth with fine-grained edges and details, refining low-resolution results of depth estimation models. The prevailing methods adopt tile-based manners by merging numerous patches, which lacks efficiency and produces inconsistency. Besides, prior arts suffer from fuzzy depth boundaries and limited generalizability. Analyzing the fundamental reasons for these limitations, we model depth refinement as a noisy Poisson fusion problem with local inconsistency and edge deformation noises. We propose the Self-distilled Depth Refinement (SDDR) framework to enforce robustness against the noises, which mainly consists of depth edge representation and edge-based guidance. With noisy depth predictions as input, SDDR generates low-noise depth edge representations as pseudo-labels by coarse-to-fine self-distillation. Edge-based guidance with edge-guided gradient loss and edge-based fusion loss serves as the optimization objective equivalent to Poisson fusion. When depth maps are better refined, the labels also become more noise-free. Our model can acquire strong robustness to the noises, achieving significant improvements in accuracy, edge quality, efficiency, and generalizability on five different benchmarks. Moreover, directly training another model with edge labels produced by SDDR brings improvements, suggesting that our method could help with training robust refinement models in future works.

## 1 Introduction

Depth refinement infers high-resolution depth with accurate edges and details, refining the low-resolution counterparts from depth estimation models . With increasing demands for high resolutions in modern applications, depth refinement becomes a prerequisite for virtual reality , bokeh rendering , and image generation . The prevailing methods  adopt two-stage tile-based frameworks. Based on the one-stage refined depth of the whole image, they merge high-frequency details by fusing extensive patches with complex patch selection strategies. However, numerous patches lead to heavy computational costs. Besides, as in Fig. 1 (a), excessive integration of local information leads to inconsistent depth structures, _e.g._, the disrupted billboard.

Apart from efficiency and consistency, depth refinement  is restricted by noisy and blurred depth edges. Highly accurate depth annotations with meticulous boundaries are necessary to enforce fine-grained details. For this reason, prior arts  only use synthetic datasets  for the highly accurate depth values and edges. However, synthetic data falls short of the real world in realism and diversity, causing limited generalizability with blurred depth and degraded performance on in-the-wild scenarios. Some attempts  simply adopt natural-scene datasets  for the problem. The varying characteristics of real-world depth annotations, _e.g._, sparsity , inaccuracy , or blurred edges , make them infeasible for supervising refinement models. Thus, GBDF  uses depth predictions  as pseudo-labels, while Boost  leverages adversarial training  as guidance. Those inaccurate pseudo-labels and guidance still lead to blurred edges as shown in Fig. 1 (a). The key problem is to alleviate the noise of depth boundaries by constructing accurate edge representations and guidance.

To tackle these challenges, we dig into the underlying reasons for the limitations, instead of the straightforward merging of local details. We model depth refinement as a noisy Poisson fusion problem, decoupling depth prediction errors into two degradation components: local inconsistency noise and edge deformation noise. We use regional linear transformation perturbation as the local inconsistency noise to measure inconsistent depth structures. The edge deformation noise represents fuzzy boundaries with Gaussian blur. Experiments in Sec. 3.1 showcase that the noises can effectively depict general depth errors, serving as our basic principle to improve refinement results.

In pursuit of the robustness against the local inconsistency noise and edge deformation noise, we propose the Self-distilled Depth Refinement (SDDR) framework, which mainly consists of depth edge representation and edge-based guidance. A refinement network is considered as the Poisson fusion operator, recovering high-resolution depth from noisy predictions of depth models . Given the noisy input, SDDR can generate low-noise and accurate depth edge representation as pseudo-labels through coarse-to-fine self-distillation. The edge-based guidance including edge-guided gradient loss and edge-based fusion loss is designed as the optimization objective of Poisson fusion. When depth maps are better refined, the pseudo-labels also become more noise-free. Our approach establishes accurate depth edge representations and guidance, endowing SDDR with strong robustness to the two types of noises. Consequently, as shown in Fig. 1 (b), SDDR significantly outperforms prior arts  in depth accuracy and edge quality. Besides, without merging numerous patches as the two-stage tile-based methods , SDDR achieves much higher efficiency.

We conduct extensive experiments on five benchmarks. SDDR achieves state-of-the-art performance on the commonly-used Middlebury2021 , Multiscopic , and Hypersim . Meanwhile, since SDDR can establish self-distillation with accurate depth edge representation and guidance on natural scenes, the evaluations on in-the-wild DIML  and DIODE  datasets showcase our superior generalizability. Analytical experiments demonstrate that these noticeable improvements essentially arise from the strong robustness to the noises. Furthermore, the precise depth edge labels produced by SDDR can be directly used to train another model  and yield improvements, which indicates that our method could help with training robust refinement models in future works.

In summary, our main contributions can be summarized as follows:

Figure 1: **(a) Visual comparisons. We model depth refinement by noisy Poisson fusion with the local inconsistency noise (representing the inconsistent billboard and wall in red box) and the edge deformation noise (indicating blurred depth edges in the blue box and second row). Better viewed when zoomed in. (b) Performance and efficiency. Circle area represents FLOPs. The two-stage methods  are reported by multiplying FLOPs per patch with patch numbers. SDDR outperforms prior arts in depth accuracy (\(_{1}\)), edge quality (ORD), and model efficiency (FLOPs).**

* We model the depth refinement task through the noisy Poisson fusion problem with local inconsistency noise and edge deformation noise as two types of depth degradation.
* We present the robust and efficient Self-distilled Depth Refinement (SDDR) framework, which can generate accurate depth edge representation by the coarse-to-fine self-distillation paradigm.
* We design the edge-guided gradient loss and edge-based fusion loss, as the edge-based guidance to enforce the model with both consistent depth structures and meticulous depth edges.

## 2 Related Work

**Depth Refinement Models.** Depth refinement refines low-resolution depth from depth estimation models [30; 51; 1], predicting high-resolution depth with fine-grained edges and details. Existing methods [3; 14; 21; 25] can be categorized into one-stage [3; 14] and two-stage [25; 21] frameworks. One-stage methods [3; 14] conduct global refinement of the whole image, which could produce blurred depth edges and details. To further enhance local details, based on the globally refined results, the prevailing refinement approaches [25; 21] adopt the two-stage tile-based manner by selecting and merging numerous patches. For example, Boost  proposes a complex patch-sampling strategy based on the gradients of input images. PatchFusion  improves the sampling by shifted and tidily arranged tile placement. However, the massive patches lead to low efficiency. The excessive local information produces inconsistent depth structures or even artifacts. In this paper, we propose the Self-distilled Depth Refinement (SDDR) framework, which can predict both consistent structures and accurate details with much higher efficiency by tackling the noisy Poisson fusion problem.

**Depth Refinement Datasets.** Depth datasets with highly accurate annotations and edges are necessary for refinement models. Prior arts [21; 14] utilize CG-rendered datasets [45; 44; 39; 11; 32] for accurate depth, but the realism and diversity fail to match the real world. For instance, neither the UmealStereo4K  nor the MVS-Synth  contain people, restricting the generalizability of refinement models. A simple idea for the problem is to leverage natural-scene data [35; 47; 49; 5; 20]. However, different annotation methods lead to varying characteristics, _e.g.,_ sparsity of LiDAR [2; 5; 7], inaccurate depth of structured light [55; 35; 36], and blurred edges of stereo matching [49; 47; 43]. To address the challenge, Boost  adopts adversarial training as guidance only with a small amount of accurately annotated real-world images. GBDF  employs depth predictions  with guided filtering  as pseudo-labels. Due to the inaccurate pseudo-labels and guidance, they [3; 25] produce blurred edges and details. By contrast, SDDR constructs accurate depth edge representation and edge-based guidance for self-distillation, leading to fine-grained details and strong generalizability.

## 3 SDDR: Self-Distilled Depth Refinement

We present a detailed illustration of our Self-distilled Depth Refinement (SDDR) framework. In Sec. 3.1, we introduce the noisy Poisson fusion to model the depth refinement task and provide an overview to outline our approach. SDDR mainly consists of depth edge representation and edge-based guidance, which will be described in Sec. 3.2 and Sec. 3.3 respectively.

### Noisy Poisson Fusion

**Problem Statement.** Based on depth maps of depth prediction models, _i.e._, depth predictor \(_{d}\), depth refinement recovers high-resolution depth with accurate edges and details by refinement network \(_{r}\). Some attempts in image super-resolution [31; 56; 26] and multi-modal integration [19; 17; 18; 53] utilize Poisson fusion to merge features and restore details. Motivated by this, we propose to model depth refinement as a noisy Poisson fusion problem. The ideal depth \(D^{*}\) with completely accurate depth values and precise depth edges are unobtainable in real world. A general depth prediction \(D\), whether produced by \(_{d}\) or \(_{r}\) for an input image \(I\), can be expressed as a noisy approximation of \(D^{*}\):

\[D D^{*}+_{}+_{}\.\] (1)

\(_{}\) and \(_{}\) denote local inconsistency and edge deformation noise to decouple depth prediction errors. Local inconsistency noise \(_{}\) represents inconsistent depth structures through regional linear transformation perturbation. Based on masked Gaussian blur, edge deformation noise \(_{}\) showcases degradation and blurring of depth edges. Refer to Appendix A.4 for details of the noises. As in Fig. 2,depth errors can be depicted by combinations of \(_{}\) and \(_{}\). Thus, considering refinement network \(_{r}\) as a Poisson fusion operator, depth refinement can be defined as a noisy Poisson fusion problem:

\[D_{0}=_{r}(_{d}(L),_{d}(H))\,,\] (2) \[_{D_{0},}_{}| D_{0} - D^{*}|+_{I-}|D_{0}-D^{*}| \,.\]

The refined depth of \(_{r}\) is denoted as \(D_{0}\). \(\) refers to the gradient operator. Typically for depth refinement [3; 25; 21] task, input image \(I\) is resized to low-resolution \(L\) and high-resolution \(H\) for \(_{d}\). \(\) represents high-frequency areas, while \(I-\) showcases low-frequency regions.

**Motivation Elaboration.** In practice, due to the inaccessibility of truly ideal depth, approximation of \(D^{*}\) is required for training \(_{r}\). For this reason, the optimization objective in Eq. 2 is divided into \(\) and \(I-\). For the low-frequency \(I-\), \(D^{*}\) can be simply represented by the ground truth \(D^{*}_{gt}\) of training data. However, as illustrated in Sec. 2, depth annotations inevitably suffer from imperfect edge quality for the high-frequency \(\). It is essential to generate accurate approximations of ideal depth boundaries as training labels, which are robust to \(_{}\) and \(_{}\). Some prior arts adopts synthetic depth [39; 11; 32] for higher edge quality, while leading to limited generalization capability with blurred predictions in real-world scenes. To leverage real depth data [35; 47; 46; 49; 5; 20], GBDF  employs depth predictions  with guided filter as pseudo-labels, which still contain significant noises and result in blurred depth. Besides, optimization of \(\) is also ignored. Kim _et al._ relies on manually annotated \(\) regions as input. GBDF [3; 30; 29] omits the selection of \(\) and supervises depth gradients on the whole image. Inaccurate approximations of \( D^{*}\) and inappropriate division of \(\) lead to limited robustness to local inconsistency noise and edge deformation noise.

**Method Overview.** To address the challenges, as shown in Fig. 3, we propose our SDDR framework with two main components: depth edge representation and edge-based guidance. To achieve low-noise approximations of \( D^{*}\), we construct the depth edge representation \(G_{s}\) through coarse-to-fine self-distillation, where \(s\{1,2,,S\}\) refers to iteration numbers. The input image is divided into several windows with overlaps from coarse to fine. For instance, we denote the high-frequency area of a certain window \(w\) in iteration \(s\) as \(^{w}_{s}\), and the refined depth of \(_{r}\) as \(D^{w}_{s}\). In this way, the self-distilled optimization of depth edge representation \(G_{s}\) can be expressed as follows:

\[D^{w}_{s} D^{*}+_{}+_{}\,,\] (3) \[_{G_{s}}_{w}_{^{w}_{s}}|G^{w}_{s}- D ^{w}_{s}|^{w}_{s}\,.\]

During training, depth edge representation \(G^{w}_{s}\) is further optimized based on the gradient of current refined depth \(D^{w}_{s}\). The final edge representation \(G_{S}\) of the whole image will be utilized as the pseudo-label to supervise the refinement network \(_{r}\) after \(S\) iterations. SDDR can generate low-noise and robust edge representation, mitigating the impact of \(_{}\) and \(_{}\) (More results in Appendix A.1).

With \(G_{S}\) as the training label, the next is to enforce \(_{r}\) with robustness to the noises, achieving consistent structures and meticulous boundaries. To optimize \(_{r}\), we propose edge-based guidance as an equivalent optimization objective to noisy Poisson fusion problem, which is presented by:

\[_{D0,}_{}| D_{0}-G_{S}|+ _{I-}|D_{0}-D^{*}_{gt}|\,.\] (4)

Figure 2: **Depiction of depth errors.** We utilize two samples of high-quality depth maps as ideal depth \(D^{*}\). For the predicted depth \(D\), the combination of local inconsistency noise \(_{}\) and edge deformation noise \(_{}\) can approximate real depth error \(D-D^{*}\) (the last two columns). Thus, as in the third and fourth columns, prediction \(D\) can be depicted by the summation of \(D^{*}\), \(_{}\), and \(_{}\).

For the second term of \(I-\), we adopt depth annotations \(D_{gt}^{*}\) as the approximation of \(D^{*}\). For the first term, with the generated \(G_{S}\) as pseudo-labels of \( D^{*}\), we propose edge-guided gradient loss and edge-based fusion loss to optimize \(D_{0}\) and \(\) predicted by \(_{r}\). The edge-guided gradient loss supervises the model to consistently refine depth edges with local scale and shift alignment. The edge-based fusion loss guides \(_{r}\) to adaptively fuse low- and high-frequency features based on the learned soft region mask \(\), achieving balanced consistency and details by quantile sampling.

Overall, when depth maps are better refined under the edge-based guidance, the edge representation also becomes more accurate and noise-free with the carefully designed coarse-to-fine manner. The self-distillation paradigm can be naturally conducted based on the noisy Poisson fusion, enforcing our model with strong robustness against the local inconsistency noise and edge deformation noise.

### Depth Edge Representation

To build the self-distilled training paradigm, the prerequisite is to construct accurate and low-noise depth edge representations as pseudo-labels. Meticulous steps are designed to generate the representations with both consistent structures and accurate details.

**Initial Depth Edge Representation.** We generate an initial depth edge representation based on the global refinement results of the whole image. For the input image \(I\), we obtain the refined depth results \(D_{0}\) from \(_{r}\) as in Eq. 2. Depth gradient \(G_{0}= D_{0}\) is calculated as the initial representation. An edge-preserving filter  is applied on \(G_{0}\) to reduce noises in low-frequency area \(I-\). With global information of the whole image, \(G_{0}\) can preserve spatial structures and depth consistency. It also incorporates certain detailed information from the high-resolution input \(H\). To enhance edges and details in high-frequency region \(\), we conduct coarse-to-fine edge refinement in the next step.

**Coarse-to-fine Edge Refinement.** The initial \(D_{0}\) is then refined from course to fine with \(S\) iterations to generate final depth edge representation. For a specific iteration \(s\{1,2,,S\}\), we uniformly divide input image \(I\) into \((s+1)^{2}\) windows with overlaps. We denote a certain window \(w\) in iteration \(s\) of the input image \(I\) as \(I_{s}^{w}\). The high-resolution \(H_{s}^{w}\) is then fed to the depth predictor \(_{d}\). \(D_{s-1}^{w}\) represents the depth refinement results of the corresponding window \(w\) in the previous iteration \(s-1\). The refined depth \(D_{s}^{w}\) of window \(w\) in current iteration \(s\) as Eq. 3 can be obtained by \(_{d}\) and \(_{r}\):

\[D_{s}^{w}=_{r}(D_{s-1}^{w},_{d}(H_{s}^{w})),s\{1,2, ,S\}\,,\] (5)

After that, depth gradient \( D_{s}^{w}\) is used to update the depth edge representation. The coarse-to-fine manner achieves consistent spatial structures and accurate depth details with balanced global and regional information. In the refinement process, only limited iterations and windows are needed. Thus, SDDR achieves much higher efficiency than tile-based methods [25; 21], as shown in Sec. C.1.

**Scale and Shift Alignment.** The windows are different among varied iterations. Depth results and edge labels on corresponding window \(w\) of consecutive iterations could be inconsistent in depth scale

Figure 3: **Overview of self-distilled depth refinement. SDDR consists of depth edge representation and edge-based guidance. Refinement network \(_{r}\) produces initial refined depth \(D_{0}\), edge representation \(G_{0}\), and learnable soft mask \(\) of high-frequency areas. The final depth edge representation \(G_{S}\) is updated from coarse to fine as pseudo-labels. The edge-based guidance with edge-guided gradient loss and edge-based fusion loss supervises \(_{r}\) to achieve consistent structures and fine-grained edges.**

and shift. Therefore, alignment is required before updating the depth edge representation:

\[(_{1},_{0})=*{arg\,min}_{_ {1},_{0}}&\|(_{1} D_{s}^{w}+_{0})-G_{s- 1}^{w}\|_{2}^{2},\\ G_{s}^{w}=_{1} D_{s}^{w}+_{0}\,,\] (6)

where \(_{1}\) and \(_{0}\) are affine transformation coefficients as scale and shift respectively. The aligned \(G_{s}^{w}\) represents the depth edge pseudo-labels for image patch \(I_{s}^{w}\) generated from the refined depth \(D_{s}^{w}\). At last, after \(S\) iterations, we can obtain the pseudo-label \(G_{S}\) as the final depth edge representation for self-distillation. For better understanding, we showcase visualization of \(D_{0}\), \(D_{S}\), and \(G_{S}\) in Fig. 4.

**Robustness to Noises.** In each window, we merge high-resolution \(_{d}(H_{s}^{w})\) to enhance details and suppress \(_{}\). Meanwhile, coarse-to-fine window partitioning and scale alignment mitigate \(_{}\) and bring consistency. Thus, \(G_{S}\) exhibits strong robustness to the two types of noises by self-distillation.

### Edge-based Guidance

With depth edge representation \(G_{S}\) as pseudo-label for self-distillation, we propose the edge-based guidance including edge-guided gradient loss and edge-based fusion loss to supervise \(_{r}\).

**Edge-guided Gradient Loss.** We aim for fine-grained depth by one-stage refinement, while the two-stage coarse-to-fine manner can further improve the results. Thus, edge-guided gradient loss instructs the initial \(D_{0}\) with the accurate \(G_{S}\). Some problems need to be tackled for this purpose.

As \(_{r}\) has not converged in the early training phase, \(G_{S}\) is not sufficiently reliable with inconsistent scales and high-level noises between local areas. Therefore, we extract several non-overlapping regions \(P_{n},n\{1,2,,N_{g}\}\) with high gradient density by clustering , where \(N_{g}\) represents the number of clustering centroids. The edge-guided gradient loss is only calculated inside \(P_{n}\) with scale and shift alignment. By doing so, the model can focus on improving details in high-frequency regions and preserving depth structures in flat areas. The training process can also be more stable. The edge-guided gradient loss can be calculated by:

\[_{grad}=}_{n=1}^{N_{g}}||(_{1}G_{0 }[P_{n}]+_{0})-G_{S}[P_{n}]||_{1}\,,\] (7)

where \(_{1}\) and \(_{0}\) are the scale and shift coefficients similar to Eq. 6. We use \([]\) to depict mask fetching operations, _i.e._, extracting local area \(P_{n}\) from \(G_{0}\) and \(G_{S}\). With the edge-guided gradient loss, SDDR predicts refined depth with meticulous edges and consistent structures.

**Edge-based Fusion Loss.** High-resolution feature \(F_{H}\) extracted from \(H\) brings finer details but could lead to inconsistency, while the low-resolution feature \(F_{L}\) from \(L\) can better maintain depth structures. \(_{r}\) should primarily rely on \(F_{L}\) for consistent spatial structures within low-frequency \(I-\), while it should preferentially fuse \(F_{H}\) for edges and details in high-frequency areas \(\). The fusion of \(F_{L}\) and \(F_{H}\) noticeably influence the refined depth. However, prior arts [14; 3; 25] adopt manually-annotated \(\) regions as fixed masks or even omit \(\) as the whole image, leading to inconsistency and blurring. To this end, we implement \(\) as a learnable soft mask, with quantile sampling strategy to guide the adaptive fusion of \(F_{L}\) and \(F_{H}\). The fusion process is expressed by:

\[F=(1-) F_{L}+ F_{H}\,,\] (8)

where \(\) refers to the Hadamard product. \(\) is the learnable mask ranging from zero to one. Larger values in \(\) showcases higher frequency with denser edges, requiring more detailed information from the high-resolution feature \(F_{H}\). Thus, \(\) can naturally serve as the fusion weight of \(F_{L}\) and \(F_{H}\).

To be specific, we denote the lower quantile of \(G_{S}\) as \(t_{a}\), _i.e._, \(P(X<t_{a}|X G_{S})=a\). \(\{G_{S}<t_{a}\}\) indicates flat areas with low gradient magnitude, while \(\{G_{S}>t_{1-a}\}\) represents high-frequency

Figure 4: **Visualization of intermediate results.** We visualize the results of several important steps within the SDDR framework. The quantile sampling utilizes the same color map as in Fig. 3.

regions. \(\) should be larger in those high-frequency areas \(\{G_{S}>t_{1-a}\}\) and smaller in the flat regions \(\{G_{S}<t_{a}\}\). This suggests that \(G_{S}\) and \(\) should be synchronized with similar data distribution. Thus, if we define the lower quantile of \(\) as \(T_{a}\), _i.e._, \(P(X<T_{a}|X)=a\), an arbitrary pixel \(i\{G_{S}<t_{a}\}\) in flat regions should also belong to \(\{<T_{a}\}\) with a lower weight for \(F_{H}\), while the pixel \(i\{G_{S}>t_{1-a}\}\) in high-frequency areas should be contained in \(\{>T_{1-a}\}\) for more detailed information. The edge-based fusion loss can be depicted as follows:

\[_{fusion}=N_{p}}_{n=1}^{N_{w}}_{i=1}^{N_{p}} \{(0,_{i}-T_{n*a}), i\{G_{S}<t_{n*a}\}\,,.\] (9)

where \(N_{p}\) is the pixel number. We supervise the distribution of \(\) with lower quantiles \(T_{n*a}\) and \(T_{1-n*a},n\{1,2,,N_{w}\}\). Therefore, pixels with larger deviations between \(G_{S}\) and \(\) will be penalized more heavily. Taking the worst case as an example, if \(i\{G_{S}<t_{N_{w}*a}\}\) but \(i\{<T_{N_{w}*a}\}\), the error for the pixel will be accumulated for \(N_{w}\) times from \(a\) to \(N_{w}*a\). \(_{fusion}\) enforces SDDR with consistent structures (low \(_{}\) noise) in \(I-\) and accurate edges (low \(_{}\) noise) in \(\). The visualizations of quantile-sampled \(G_{S}\) and \(\) are presented in Fig. 4.

Finally, combining \(_{grad}\) and \(_{fusion}\) as edge-based guidance for self-distillation, the overall loss \(\) for training \(_{}\) is calculated as Eq. 10. \(_{gt}\) supervises the discrepancy between \(D_{0}\) and ground truth \(D_{gt}^{*}\) with affinity-invariant loss . See Appendix A for implementation details of SDDR.

\[=_{gt}+_{1}_{grad}+_{2} _{fusion}\,.\] (10)

## 4 Experiments

To prove the efficacy of Self-distilled Depth Refinement (SDDR) framework, we conduct extensive experiments on five benchmarks  for indoor and outdoor, synthetic and real-world.

**Experiments and Datasets.** Firstly, we follow prior arts  to conduct zero-shot evaluations on Middlebury2021 , Multiscopic , and Hypersim . To showcase our superior generalizability, we compare different methods on DIML  and DIODE  with diverse natural scenes. Moreover, we prove the higher efficiency of SDDR and undertake ablations on our specific designs.

**Evaluation Metrics.** Evaluations of depth accuracy and edge quality are necessary for depth refinement models. For edge quality, we adopt the ORD and D\({}^{3}\)R metrics following Boost . For depth accuracy, we adopt the widely-used REL and \(_{i}\,(i=1,2,3)\). See Appendix B for details.

### Comparisons with Other Depth Refinement Approaches

**Comparisons with One-stage Methods.** For fair comparisons, we evaluate one-stage  and two-stage tile-based  approaches separately. The one-stage methods predict refined depth

    &  &  &  &  \\   & & \(_{1}\)\(\) & REL\(\) & ORD\(\) & \(_{1}\)\(\) & REL\(\) & ORD\(\) & \(_{1}\)\(\) & REL\(\) & ORD\(\) \\   & MiDaS  & \(0.868\) & \(0.117\) & \(0.384\) & \(0.839\) & \(0.130\) & \(0.292\) & \(0.781\) & \(0.169\) & \(0.344\) \\  & Kim _et al._ & \(0.864\) & \(0.120\) & \(0.377\) & \(0.839\) & \(0.130\) & \(0.293\) & \(0.778\) & \(0.175\) & \(0.344\) \\  & Graph-GDSR  & \(0.865\) & \(0.121\) & \(0.380\) & \(0.839\) & \(0.130\) & \(0.292\) & \(0.781\) & \(0.169\) & \(0.345\) \\  & GBDF  & \(0.871\) & \(0.115\) & \(0.305\) & \(0.841\) & \(0.129\) & \(0.289\) & \(0.787\) & \(0.168\) & \(0.338\) \\  & Ours & \(0.879\) & \(0.112\) & \(0.299\) & \(0.852\) & \(0.122\) & \(0.267\) & \(0.791\) & \(0.166\) & \(0.318\) \\   & LeReS  & \(0.847\) & \(0.123\) & \(0.326\) & \(0.863\) & \(0.111\) & \(0.272\) & \(0.853\) & \(0.123\) & \(0.279\) \\  & Kim _et al._ & \(0.846\) & \(0.124\) & \(0.328\) & \(0.860\) & \(0.113\) & \(0.286\) & \(0.850\) & \(0.125\) & \(0.286\) \\  & Graph-GDSR  & \(0.847\) & \(0.124\) & \(0.327\) & \(0.862\) & \(0.111\) & \(0.273\) & \(0.852\) & \(0.123\) & \(0.281\) \\  & GBDF  & \(0.852\) & \(0.122\) & \(0.316\) & \(0.865\) & \(0.110\) & \(0.270\) & \(0.857\) & \(0.121\) & \(0.273\) \\  & Ours & \(0.862\) & \(0.120\) & \(0.305\) & \(0.870\) & \(0.108\) & \(0.259\) & \(0.862\) & \(0.120\) & \(0.273\) \\   & ZoeDepth  & \(0.900\) & \(0.104\) & \(0.225\) & \(0.896\) & \(0.097\) & \(0.205\) & \(0.927\) & \(0.088\) & \(0.198\) \\  & Kim _et al._ & \(0.896\) & \(0.107\) & \(0.228\) & \(0.890\) & \(0.099\) & \(0.204\) & \(0.923\) & \(0.091\) & \(0.204\) \\   & Graph-GDSR  & \(0.901\) & \(0.103\) & \(0.226\) & \(0.895\) & \(0.096\) & \(0.208\) & \(0.926\) &  on the whole image. SDDR conducts one-stage refinement without the coarse-to-fine manner during inference. Comparisons on Middlebury2021 , Multiscopic , and Hypersim  are shown in Table 1. As prior arts [14; 4; 3], we use three depth predictors MiDaS , LeReS , and ZoeDepth . Regardless of which depth predictor is adopted, SDDR outperforms the previous one-stage methods [14; 4; 3] in depth accuracy and edge quality on the three datasets [34; 52; 32]. For instance, our method shows \(6.6\%\) and \(20.7\%\) improvements over Kim _et al._ for REL and ORD with MiDaS  on Middlebury2021 , showing the efficacy of our self-distillation paradigm.

**Comparisons with Two-stage Tile-based Methods.** Two-stage tile-based methods [25; 21] conduct local refinement on numerous patches based on the global refined depth. SDDR moves away from the tile-based manner and utilizes coarse-to-fine edge refinement to further improve edges and details. As in Table 2, SDDR with the coarse-to-fine manner shows obvious advantages. For example, compared with the recent advanced PatchFusion , SDDR achieves \(5.2\%\) and \(26.7\%\) improvements for \(_{1}\) and ORD with ZoeDepth  on Hypersim . To be mentioned, PatchFusion  uses ZoeDepth  as the fixed baseline, whereas SDDR is readily pluggable for various depth predictors [30; 51; 1].

**Generalization Capability on Natural Scenes.** We prove the superior generalization capability of SDDR. In this experiment, we adopt LeReS  as the depth predictor. DIML  and DIODE 

    &  &  &  &  \\   & & \(_{1}\)\(\) & REL \(\) & ORD \(\) & \(_{1}\)\(\) & REL \(\) & ORD \(\) & \(_{1}\)\(\) & REL \(\) & ORD \(\) \\   & MiDaS  & 0.868 & 0.117 & 0.384 & 0.839 & 0.130 & 0.292 & 0.781 & 0.169 & 0.344 \\  & Boost  & 0.870 & 0.118 & 0.351 & 0.845 & 0.126 & 0.282 & 0.794 & 0.161 & 0.332 \\  & Ours & **0.871** & **0.115** & **0.303** & **0.858** & **0.120** & **0.263** & **0.799** & **0.154** & **0.322** \\   & LeReS  & 0.847 & 0.123 & 0.326 & 0.863 & 0.111 & 0.272 & 0.853 & 0.123 & 0.279 \\  & Boost  & 0.844 & 0.131 & 0.325 & 0.860 & 0.112 & 0.278 & **0.865** & **0.118** & 0.272 \\  & Ours & **0.861** & **0.123** & **0.390** & **0.870** & **0.109** & **0.268** & 0.858 & 0.123 & **0.271** \\   & ZoeDepth  & 0.900 & 0.104 & 0.225 & 0.896 & 0.097 & 0.205 & 0.927 & 0.088 & 0.198 \\  & Boost  & 0.911 & 0.099 & 0.210 & **0.910** & 0.094 & 0.197 & 0.926 & 0.089 & 0.193 \\   & PatchFusion  & 0.887 & 0.102 & 0.211 & 0.908 & 0.095 & 0.212 & 0.881 & 0.116 & 0.258 \\   & Ours & **0.913** & **0.096** & **0.202** & 0.908 & **0.091** & **0.197** & **0.933** & **0.083** & **0.189** \\   

Table 2: **Comparisons with two-stage methods. PatchFusion  only adopts ZoeDepth  as the fixed baseline, while other approaches are pluggable for different depth predictors [30; 51; 1].**

Figure 5: **Qualitative comparisons of one-stage methods on natural scenes. LeReS  is used as the depth predictor. SDDR predicts sharper depth edges and more meticulous details than prior arts [3; 14], _e.g._, fine-grained predictions of intricate branches. Better viewed when zoomed in.**

Figure 6: **Qualitative comparisons of two-stage methods on natural scenes. ZoeDepth  is adopted as the depth predictor. The SDDR with coarse-to-fine edge refinement can predict more accurate depth edges and more consistent spatial structures than the tile-based methods [21; 25].**

datasets are used for zero-shot evaluations, considering their diverse in-the-wild indoor and outdoor scenarios. As in Table 3, SDDR shows at least \(5.7\%\) and \(9.0\%\) improvements for REL and ORD on DIODE . On DIML  dataset, our approach improves D\({}^{3}\)R, ORD, and \(_{1}\) by over \(17.6\%\), \(7.5\%\), and \(2.0\%\). The convincing performance proves our strong robustness and generalizability, indicating the efficacy of our noisy Poisson fusion modeling and self-distilled training paradigm.

**Qualitative Comparisons.** We present visual comparisons of one-stage methods [14; 3] on natural scenes in Fig. 5. With our low-noise depth edge representation and edge-based guidance, SDDR predicts sharper depth edges and details, _e.g._, the fine-grained predictions of intricate branches.

The visual results of two-stage approaches [25; 21] are shown in Fig. 6. Due to the excessive fusion of detailed information, tile-based methods [25; 21] produce structure disruption, depth inconsistency, or even noticeable artifacts, _e.g._, disrupted and fuzzy structures of the snow-covered branches. By contrast, SDDR can predict more accurate depth edges and more consistent spatial structures.

**Robustness against noises.** As in Fig. 7, we evaluate SDDR and GBDF  with different levels of input noises. As the noise level increases, our method presents less degradation. The stronger robustness against the \(_{}\) and \(_{}\) noises is the essential reason for all our superior performance.

**Model Efficiency.** SDDR achieves higher efficiency. Two-stage tile-based methods [25; 21] rely on complex fusion of extensive patches with heavy computational overhead. Our coarse-to-fine manner noticeably reduces Flops per patch and patch numbers as in Fig. 1. For one-stage methods [4; 3; 14], SDDR adopts a more lightweight \(_{r}\) with less parameters and faster inference speed over the previous GBDF  and Kim _et al._. See Appendix C.1 for detailed comparisons of model efficiency.

### Ablation Studies

**Coarse-to-fine Edge Refinement.** In Table 3(a), we adopt the coarse-to-fine manner with varied iterations. \(S=0\) represents one-stage inference. Coarse-to-fine refinement brings more fine-grained edge representations and refined depth. We set \(S=3\) for the SDDR with two-stage inference.

**Edge-based Guidance.** In Table 3(b), we evaluate the effectiveness of edge-based guidance. \(_{grad}\) focuses on consistent refinement of depth edges. \(_{fusion}\) guides the adaptive feature fusion of low- and high-frequency information. With \(_{gt}\) as the basic supervision of ground truth, adding \(_{grad}\) and \(_{fusion}\) improves D\({}^{3}\)R by \(10.0\%\) and REL by \(3.2\%\), showing the efficacy of edge-based guidance.

**Effectiveness of SDDR Framework.** As in Table 3(c), we train SDDR with the same HRWSI  as GBDF  for fair comparison. Without the combined training data in Appendix B.1, SDDR still improves D\({}^{3}\)R and ORD by \(13.9\%\) and \(2.2\%\) over GBDF , proving our superiority convincingly.

Table 4: **Ablation Study.** All ablations are on Middlebury2021  with depth predictor LeReS .

Table 3: **Comparisons of model generalizability. We conduct zero-shot evaluations on DIML  and DIODE  datasets with diverse in-the-wild scenarios to compare the generalization capability. We adopt LeReS  as the depth predictor for all the compared methods in this experiment.**

Figure 7: **Robustness against noises.** X-axis shows noise level of \(_{}+_{}\). With higher noises, our SDDR is more robust with less performance degradation than the prior GBDF .

**Transferability.** We hope our depth edge representation \(G_{S}\) can be applicable to other depth refinement models. Therefore, in Table (d)d, we directly train GBDF  combining the depth edge representation produced by the trained SDDR. The depth accuracy and edge quality are improved over the original GBDF , indicating the transferability of \(G_{S}\) in training robust refinement models.

## 5 Conclusion

In this paper, we model the depth refinement task as a noisy Poisson fusion problem. To enhance the robustness against local inconsistency and edge deformation noise, we propose Self-distilled Depth Refinement (SDDR) framework. With the low-noise depth edge representation and guidance, SDDR achieves both consistent spatial structures and meticulous depth edges. Experiments showcase our stronger generalizability and higher efficiency over prior arts. The SDDR provides a new perspective for depth refinement in future works. Limitations and broader impact are discussed in Appendix A.5.

AcknowledgementThis work is supported by the National Natural Science Foundation of China under Grant No. 62406120.