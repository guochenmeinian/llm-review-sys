# Tight Bounds for Machine Unlearning via Differential Privacy

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We consider the formulation of "machine unlearning" of Sekhari, Acharya, Kamath, and Suresh (NeurIPS 2021), which formalizes the so-called "right to be forgotten" by requiring that a trained model, upon request, should be able to 'unlearn' a number of points from the training data, as if they had never been included in the first place. Sekhari et al. established some positive and negative results about the number of data points that can be successfully unlearn by a trained model without impacting the model's accuracy (the "deletion capacity"), showing that machine unlearning could be achieved by using differentially private (DP) algorithms. However, their results left open a gap between upper and lower bounds on the deletion capacity of these algorithms: our work fully closes this gap, obtaining tight bounds on the deletion capacity achievable by DP-based machine unlearning algorithms.

## 1 Introduction

Machine learning models trained on user data are now routinely used virtually everywhere, from recommendation systems to predictive models. In many cases, this user data itself includes some sensitive information (e.g., healthcare or race) or private aspects (customer habits, geographic data), sometimes even protected by law. To address this issue - that the models trained on sensitive datasets must not leak personal or private information - in a principled fashion, one of the leading frameworks is that of _differential privacy_ (DP) (Dwork et al., 2006), which has _de facto_ become the standard for privacy-preserving machine learning over the past decade.

At its core, DP requires that the output of a randomized algorithm \(M\) not change drastically if one to modify one of the datapoints: that is, if \(X,X^{}\) are two datasets only differing in _one_ user's data, then for all possible outputs \(S\) of the algorithm one should have roughly the same probability of observing \(S\) under both inputs:

\[[\,M(X) S\,] e^{}[\,M(X^{}) S\,]+\]

where \(>0\) and \((0,1]\) quantify the privacy guarantee (the smaller values, the better the privacy; see Section 2 for formal definitions). Intuitively, an algorithm \(M\) being \((,)\)-DP means that its output does not reveal much about any particular user's data, since the output would be nearly identical had this user's data been completely different.

While the use of differential privacy can mitigate many privacy concerns, it does come with some limitations. The first is the overhead in brings: that is, ensuring differential privacy for a learning task typically incurs an overhead in the number of data points needed to achieve the same accuracy guarantee. Perhaps more importantly, DP does not solve all possible privacy concerns: even if a ML model is trained on a sensitive dataset in a differentially private way, the dataset may still be subject to some attacks - e.g., if the server where the training data is stored is itself compromised. Somewhattautologically: DP is not a silver bullet, and only provides meaningful guarantees against the threat models it was meant to address.

Another type of concerns focuses on the individual _right to maintain control on one's own data_: broadly speaking, this is asking that each user can (under some reasonable circumstances) require that their personal data and information be removed from a company's collected data and trained models. This so-called "right to be forgotten," which allow people to request that their data be deleted entirely from an ML system, has been passed into legislation or is considered in some form or another by various countries or entities, prominently the European Union's General Data Protection Regulation (GDPR), the California Privacy Rights Act (CCRA), Canada's proposed Consumer Privacy Protection Act (CPPA), and most recently in Australia (Karp, 2023).

However, translating this "right to be forgotten" into practice comes with a host of challenges, starting with how to formalize it (Cohen et al., 2022) and technically implement it - which recently led to a new area of research in ML and computer science, that of _machine unlearning_. A naive technical solution would be for a given company to keep the original training set at all times, and, upon a deletion request by a user, remove this user's data from the set before retraining the whole model on the result. This, of course, comes up with two major drawbacks: first, the cost to the company, in terms of time and computational resources, of retraining a large model on a regular basis. Second, the _privacy cost_, as keeping the training set for an indefinite time in order to be able to handle the deletion requests leaves the door open to potential attacks and data breaches. Fortunately, there have been, over the past few years, a flurry of better (and more involved) approaches to machine unlearning, to handle deletion requests much more efficiently, and requiring to maintain much less of the training set (see, e.g., (Bourtoule et al., 2021; Nguyen et al., 2022), and related work below).

The above discussion, still, brings to light an important question: _is machine unlearning, paradoxically, at odds with (differential) privacy? What is the connection between the two notions: are they complementary, or is there a trade-off between them?_

This is the main question this work sets out to address. Our starting point is the formalization of machine unlearning set forth by Sekhari, Acharya, Kamath, and Suresh (Sekhari et al., 2021), itself reminiscent of the definition of DP (see Definition 2.5 for the formal statement): a pair of algorithms \((A,)\) is an _\((,)\)-unlearning algorithm_ if (1) \(A^{*}\) is a (randomized) learning algorithm which, given a dataset \(X^{*}\), outputs model parameters \(A(X)\); and (2) \(^{*}\) which, on input a set of _deletion requests_\(U X\), previous model parameters \(w\), and some succinct additional "side information" \(T(X)\) about the original dataset, output updated model parameters \(w^{}\) from which the data from \(U\) has been unlearned, that is, such that

\[\,(U,A(X),T(X)) W\, e^{} \,(,A(X U),T(X U)) W\,+\]

and

\[\,(,A(X U),T(X U)) W\,  e^{}\,(U,A(X),T(X)) W\,+\]

for every possible set \(W\) of model parameters. Loosely speaking, this requires that the outcomes of (a) training a model \(M\) via \(A\) on the dataset \(X\) then unlearning some of the original training data \(U X\) from \(M\) using \(\), and (b) training a model \(M^{}\) via \(A\) directly on the dataset \(X U\) then unlearning nothing via \(\), be nearly indistinguishable.

In their paper, Sekhari et al. (Sekhari et al., 2021) focus on genralization guarantees of unlearning algorithm, i.e., what can be achieved by unlearning algorithms when focusing on population loss, namely, when aiming to minimize

\[F(w)_{x}[f(w,x)]\]

given a prespecified loss function \(f\), where the expectation is over the draw of a new datapoint from the underlying distribution \(p\) on the sample space. The quality of a learning algorithm \(A\) is then measured by the expected excess risk

\[R(f,A)F(A(X))-_{w^{*}}F(w^{*}) \]

where the expectation is taking over the random choice of a dataset \(X^{n}\) of size \(n\), and the randomness of \(A\) itself. The focus of (Sekhari et al., 2021), as is ours, is then to quantify the _deletion capacity_ achievable for \((,)\)-unlearning given a prespecified loss function, that is, the maximum number of data points one can ask to be forgotten (maximum size of the subset \(U\)) before the excess risk increases by more than some threshold (see Definition 2.6).

In their paper,  draw a connection between DP learning algorithms and unlearning ones, showing that DP learning algorithms do provide _some_ unlearning guarantees out-of-the-box, and that one can achieve non-trivial unlearning guarantees for convex loss functions by leveraging the literature on differentially private optimization and learning. One of their main results is showing that these DP-based unlearning algorithms, which crucially _do not rely on any side information_ (the additional input \(T(X)\) provided to the unlearning algorithm \(\)) can handle strictly fewer deletion requests than general unlearning algorithms which _do_ rely on such side information.

Their results, however, do not fully characterize the deletion capacity of these "DP-based" machine unlearning algorithms, leaving a significant gap between their upper and lower bounds. We argue that fully understanding this quantity is crucial, as DP-based unlearning algorithms are _exactly_ those for which there is no conflict between the two notions of DP and unlearning - _instead, this class of algorithms is the one for which they work hand in hand_. This is in contrast to the more general unlearning algorithms relying on maintaining and storing side information about the training set, as this side information can make their deployment susceptible to privacy breaches.

### Our contributions

The main contribution of our paper is a tight bound on the "amount of unlearning" achievable by _any_ machine unlearning algorithm which does not rely on side information. For the sake of exposition, we state in this section informal versions of our results.

**Theorem 1.1** (Unlearning For Convex Loss Functions (Informal; see Theorems 3.1 and 3.3)).: _Let \(f\) be a \(1\)-Lipschitz convex loss function, where \(^{d}\) is the parameter space. There exists an \((,)\)-machine unlearning algorithm which, trained on a dataset \(S^{n}\), does not store any side information about the training set besides the learned model, and can unlearn up to_

\[m=O\!(})\]

_datapoints without incurring excess population risk greater than \(\). Moreover, this is tight: there exists a \(1\)-Lipschitz linear loss function such that no machine unlearning algorithm can unlearn \((})\) data points without excess population risk \(\), unless it stores side information._

Our techniques also allow us to easily establish the analogue for _strongly_ convex optimization:

**Theorem 1.2** (Unlearning For Strongly Convex Loss Functions (Informal)).: _Let \(f\) be a \(1\)-Lipschitz strongly convex loss function. There exists an \((,)\)-machine unlearning algorithm which, trained on a dataset \(S^{n}\), does not store any side information about the training set besides the learned model, and can unlearn up to_

\[m=O\!(}{d(1/)})\]

_datapoints without incurring excess population risk greater than \(\). Moreover, this is tight._

We note that, prior to our work, only bounds for the convex loss function case were known, with an upper bound of \(m=(n//)})\) (loose by polylogarithmic factors for \(=O(1)\), as well as an \(1/\) factor for \( 1\)) and a limited lower bound stating that \(m 1\) is only possible if \(n/=(1)\).

Our next contribution, motivated by the similarity of the formalisations of machine unlearning (without side information) and that of differential privacy, is to establish the analogue of key properties of DP for machine unlearning, namely, _post-processing_ and _composition_ of machine unlearning algorithms. To do so, we first identify a natural property of machine unlearning algorithms, which, when satisfied, will allow for the composition properties:

**Assumption 1.3** (Unlearning Laziness).: _An unlearning algorithm \((,A)\) is said to be lazy if, when provided with an empty set of deletion requests, the unlearning algorithm \(\) does not update the model. That is, \((,A(X),T(X))=A(X)\) for all datasets \(X\)._We again emphasize that this laziness property is not only intuitive, it is also satisfied by several existing unlearning algorithms, and in particular those proposed in Sekhari et al. (2021).

**Theorem 1.4** (Post-processing of unlearning).: _Let \((,A)\) be an \((,)\)-unlearning algorithm taking no side information. Let \(f:\) be an arbitrary (possibly randomized) function. Then \((f,A)\) is also an \((,)\)-unlearning algorithm._

Under our laziness assumption, we also establish the following:

**Theorem 1.5** (Chaining of unlearning).: _Let \((,A)\) be a lazy\((,)\)-unlearning algorithm taking no side information, and able to handle up to \(m\) deletion requests. Then, the algorithm which uses \((,A)\) to sequentially unlearn \(k\) disjoint deletion requests \(U_{1},,U_{k} X\) such that \(|_{i}U_{i}| m\), outputting_

\[(U_{k},,(U_{1},A(X)))\]

_is an \((^{},^{})\)-unlearning algorithm, with \(^{}=k\) and \(^{}=}-1}{^{ }-1}=O(k)\) (for \(k=O(1/)\))._

and, finally,

**Theorem 1.6** (Advanced composition of unlearning).: _Let \((_{1},A),,(_{k},A)\) be lazy\((,)\)-unlearning (with common learning algorithm \(A\)) taking no side information, and define the composition of those unlearning algorithms, \(\) as_

\[(U,A(X))=f_{1}(U,A(X)),,_{k}(U,A(X))\,.\]

_where \(f:^{k}\) is any (possibly randomized) function. Then, for every \(^{}>0\), \((,A)\) is an \((^{},^{})\)-unlearning taking no side information, where \(^{}=^{2}+)}}\)._

### Related work

Albeit recent, the field of machine unlearning has already received considerable attention from the ML community, with an array of studies and papers focusing on practical solutions and their empirical performance. We focus in this section on the works most closely related to ours, mostly theoretical. As discussed earlier, the goal of machine unlearning (Bourtoule et al. (2021)) is to delete what models have learned from data. This problem coincides tangentially with the idea of differential privacy as they both requires to minimize the effect of a (or a group of) sample. The original, stringent definition of unlearning requires \(=0\) (full deletion of the user's data, as if it had never been included in the training set in the first place) in contrast to differential privacy that allows \(>0\), leaving a possibility for "memorization." To relax this definition, Ginart et al. (2019) proposed the probabilistic version of unlearning.

Prior theoretical work of unlearning are mostly disjoint from the differential privacy literature, in spite of a general recognition that the two notions aim to address related issues. Most works on machine unlearning mainly focus on empirical risk minimization of approximate unlearning algorithms (Guo et al. (2020), Izzo et al. (2020)), which seeks to find an approximate minimizer on the remaining dataset after deletion. Closest to our work is the recent paper of Sekhari et al. (2021), which formulated the notion of machine unlearning used in our paper and focused on population loss minimization of approximating unlearning algorithm (i.e., allowing \(>0\)). Their objectives, however, were somewhat orthogonal to ours, as they focused for a large part on minimizing the space requirements for the side information \(T(X)\) provided to the unlearning algorithm (while our paper focuses on algorithms which do _not_ rely on any such side information, prone to potential privacy leaks). While their work, to motivate this focus, established partial bounds on the deletion capacity of unlearning algorithm that do not take in extra statistics, these bounds were not tight, and one of our main contributions is closing this gap. Following Sekhari et al. (2021), the notion of _online_ unlearning algorithm - which receive the deletion requests sequentially - was put forward and studied in Suriyakumar and Wilson (2022), again with memory efficiency with respect to the side information in mind; however, their primary focus is on the empirical performance of unlearning algorithm.

Another work closely to ours is the notion of _certified data removal_ proposed by Guo et al. (2020). The main difference between \((,)\)-certified removal and the definition from Sekhari et al. (2021) is that, in the former, the unlearning mechanism requires access not only to the samples to be deleted (the set \(U X\)), but also to the full original training set \(X\): this is exactly the type of constraints our work seeks to avoid, due to the risk of data breach this entails.

### Organization of the paper

We first provide the necessary background and notion on differential privacy, learning, and the formulation of machine unlearning used throughout the paper in Section 2. We then provide a detailed outline of the proof of our main result, Theorem 1.1, in Section 3, before concluding with a discussion of results and future work in Section 4.

Due to space constraints, the details of all other results, as well as omitted proofs, are deferred to the Supplemental.

## 2 Preliminaries

In this section, we recall some notions and results we will extensively rely on in our proofs and theorems, starting with differential privacy.

### Differential Privacy

**Definition 2.1** ((Central) Differential Privacy (DP)).: _Fix \(>0\) and \(\). An algorithm \(M^{n}\) satisfies \((,)\)-differential privacy (DP) if for every pair of neighboring datasets \(X,X^{}\), and every (measurable) subset \(S\):_

\[[\,M(X) S\,] e^{}[\,M(X^{}) S\,]+.\]

_We further say that \(M\) satisfies pure differential privacy (\(\)-DP) if \(=0\), otherwise it is approximate differential privacy._

We now recall another notion of differential privacy in terms of Renyi Divergence, from Bun and Steinke (2016).

**Definition 2.2** (Zero-Concentrated Differential Privacy (zCDP)).: _A randomized algorithm \(M:^{n}\) satisfies \((,)\)-zCDP if for every neighboring datasets (differing on a single entry) \(X,X^{}^{n}\), and \((1,)\):_

\[_{}(M(X)\|M(X^{}))+\]

_where \(\) is the \(\)-Renyi divergence between distributions of \(M(X)\) and \(M(X^{})\). We say that \(M\) is \(\)-zCDP when \(=0\)._

We use the following group privacy property of zCDP in the proof later.

**Proposition 2.3** (\(k\)-distance group privacy of \(\)-zCDP (Bun and Steinke, 2016, Proposition 1.9)).: _Let \(M:^{n}\) satisfy \(\)-zCDP. Then, \(M\) is \((k^{2})\)-zCDP for every \(X,X^{}^{n}\) that differs in at most \(k\) entries._

### Learning

We also will require some definitions on learning, specifically with respect to minimizing population loss. Fix any loss function \(f\), where \(\) is the (model) parameter space and \(\) is the sample space. Then, the generalization loss is defined as

\[F(w):=_{x p}[f(w,x)]\]

in which the expectation is over the distribution of \(x\) (one sample) and \(w\) is the learning output. Let \(F^{*}=_{w}F(w)\) be the minimizer of population risk and \(w^{*}\) is the corresponding minimizer.

Define learning algorithm \(A:^{n}\) that takes in dataset \(S^{n}\) and returns hypothesis \(w A(S)\). The excess risk is given by:

\[[F(A(S))]-F^{*}\]

where the expectation is over the randomness of \(A\) and \(S\).

Hence, we could define the sample complexity as following ((Sekhari et al., 2021, Definition 11), which is analogous to deletion capacity, in which will be stated later.

**Definition 2.4** (Sample complexity of learning).: _The \(\)-sample complexity of a problem is defined as:_

\[n()\{n A[F(A(S))]-F^{*} ,\;\}\]

### Unlearning

As previously discussed, we rely on the definition of unlearning proposed in by Sekhari et al. (2021), and maintain same notation. Note that \(T(S)\) denotes the data statistics (which could be the entire dataset \(S\) or any form of statistic) available to \(\).

**Definition 2.5** (\((,)\)-unlearning).: _For all \(S\) of size \(n\) and delete requests \(U S\) such that \(|U| m\), and \(W\), a learning algorithm \(A\) and an unlearning algorithm \(\) is \((,)\)-unlearning if:_

\[\,(U,A(S),T(S)) W\, e^{} \,(,A(S U),T(S U)) W\,+\]

_and_

\[\,(,A(S U),T(S U)) W\,  e^{}\,(U,A(S),T(S)) W\,+,\]

Our results will be phrased in terms of the deletion capacity, which captures the number of deletion requests an unlearning algorithm can handle before seeing a noticeable drop in its output's accuracy:

**Definition 2.6** (Deletion Capacity).: _Let \(,>0\), \(S\) be a dataset of size \(n\) drawn i.i.d. from \(\) and let \((w,z)\) be a loss function. For a pair of learning and unlearning algorithm \(A,\) that are \((,)\)-unlearning, the deletion capacity \(m^{A,}_{,}\) is defined as the maximum size of deletions requests set \(|U|\) that we can unlearn without doing worse in excess population risk than \(\):_

\[m^{A,}_{,}():=\{m _{U S:|U| m}F((U,A(S),T(S)))-F^{*}\}\]

_where \(F^{*}_{A(S)}F((U,A(S),T(S)))\)._

## 3 Main result

In this section, we provide a detailed outline of our main result on unlearning for convex loss functions, Theorem 1.1, for which we prove the upper and lower bounds separately.

**Theorem 3.1** (Deletion capacity from unlearning via DP, Lower Bound).: _Suppose \(^{d}\), and fix any Lipschitz convex loss function. Then there exists a lazy \((,)\)-unlearning algorithm \((,A)\), where \(\) has the form \((U,A(S),T(S)):=A(S)\) (and thus, in particular, takes no side information) with deletion capacity_

\[m^{A,}_{,}()}}\]

_where the constant in the \(()\) only depends on the properties of the loss function._

Proof.: Our proof follows the same general outline as that of Sekhari et al. (2021), with a key difference which allows us to derive the tight bound. Namely, we start, similarly, from the observation that any \((,)\)-DP learning algorithm \(A\) whose privacy guarantee is with respect to edit distance \(m\) between datasets (instead of the usual neighboring relation) readily implies an \((,)\)-machine unlearning algorithm \((,A)\), where \((w)=w\) (i.e., the "unlearning part" returns its input unchanged).

However, we depart from previous work by how we obtain this \((,)\)-DP algorithm \(A\) with respect to edit distance \(m\). The key insight is that instead of starting with any good approximate DP learning algorithm and using the grouposition property of DP to "upgrade" it to \(m\)-edit distance, we instead start with a good _zCDP_ learning algorithm. Indeed, zCDP has much tigrouposition properties than approximate DP (cf. Proposition 2.3), which in turn leads to better parameters when applying grouposition to achieve DP to groups up to size \(m\): specifically, starting with a \(^{2}/2\)-zCDP standard privacy guarantee (for groups of size \(1\)) we would by Proposition 2.3 obtain \((m^{2}^{2}/2)\)-zCDP for neighboring datasets differin in up to \(m\) entries. Leveraging then the standard conversion from concentrated to approximate DP (Bun and Steinke, 2016), this implies, for every \(>0\), an \((,)\)-DP guarantee for groups of size \(m\), where \(=O(m})\). Thus, choosing \(=}}\) would suffice to achieve the desired end privacy guarantee on \(A\) (with respect to edit distance up to \(m\)), and thus the \((,)\)-unlearning one for \((,A)\).

To do so, however, we crucially need to start with a sufficiently good private learning algorithm \(A\) with zCDP guarantees, instead of approximate DP. Fortunately for us, such an algorithm is provided by (Feldman et al., 2020, Theorem 1):

**Lemma 3.2** (zCDP mini-batch noisy SGD Feldman et al. (2020)).: _Fix any \(L\)-Lipschitz convex loss function over a convex subset \(\) of \(^{d}\) of diameter \(D\). Then there exists an algorithm \(A\) which satisfies \((^{2}/2)\)-zCDP with excess population loss:_

\[F()-_{}F() O DL}+}{ n}\] (1)

_where the expectation is taken over the randomness of \(A\)._

By the above discussion, using this zCDP-private learning algorithm with \(=}}\), we get an excess population loss bounded by

\[ODL}+}}{  n}\] (2)

It only remains to show how the claimed deletion capacity bound follows from this excess population risk guarantee. Construct, as discussed earlier, an unlearning algorithm \(\) that returns the input without making any changes (and in particular does not require any additional statistics \(T(S)\), and satisfies the laziness assumption). Since \(A\) is \((,)\)-DP, for any set \(U S,|U|=m\), and \(W\),

\[[\,A(S) W\,]  e^{}[\,A(S^{}) W\,]+\] \[[\,A(S^{}) W\,]  e^{}[\,A(S) W\,]+\]

. But since \((U,A(S))=A(S)\), this readily yields, letting \(S^{}:=S U\):

\[[\,(U,A(S)) W\,]  e^{}[\,(,A(S^{})) W\, ]+\] \[[\,(,A(S^{})) W\,]  e^{}[\,(U,A(S)) W\,]+\]

which implies that \((A,)\) is indeed \((,)\)-unlearning for \(U\) of size (up to) \(m\).

Recalling the definition of deletion capacity (Definition 2.6), we finally deduce from (2) the deletion capacity with excess population risk less than \(\):

\[m^{A,}_{,}() m=}}\]

where the \(O()\) hides constant factors depending only on the loss function (namely, the Lipschitz function \(L\), and the diameter \(D\)). 

**Theorem 3.3** (Deletion capacity from unlearning via DP, Upper Bound).: _There exists a Lipschitz convex loss function (indeed, linear) for which any \(,)\)-unlearning algorithm \((,A)\) which takes no side information must have deletion capacity_

\[m^{A,}_{,}() O}}\,.\]

_Detailed Proof Sketch._ We will consider the following linear (and therefore convex and Lipschitz) loss function \((,S)\):

\[(,S):=-,_{i=1}^{n}x_{i}\] (3)

for dataset \(S\) of \(n\) points \(x_{1},,x_{n}\{-},}\}^{d}\). We also define the 1-way marginal query, i.e. average, as:

\[q(S):=_{i=1}^{n}x_{i}\,.\] (4)

To establish our deletion capacity lower bound with respect to this loss function, we will proceed in three stages: the first, relatively standard, is to relate population loss (what we are interested in) to _empirical_ loss - which allows us to focus on the existence of a "hard dataset." The second stepis then to establish a sample complexity lower bound on the empirical risk (for this loss function) of any \((,)\)-DP algorithm, via a reduction to differentially private computing of 1-marginals. This step is similar to the one underlying the (weaker) lower bound of Sekhari et al. (2021) (itself relying on an argument of (Bassily et al., 2019)), although a more careful choice of building blocks for the reduction already allows us to obtain an improvement by logarithmic factors.

Third, lift this DP lower bound to a stronger lower bound for DP with respect to edit distance \(m\). This step is quite novel, as it morally corresponds to establishing the converse of the grouposition property of differential privacy (for our specific setting), a converse which does _not_ hold in general. Our argument, relatively simple, will crucially rely on the linearity of our loss function.

We omit the details of the first step (reduction from population to empirical loss) in this detailed outline, as it is quite standard. For the second step, our starting point is the following lower bound of Steinke and Ullman:

**Theorem 3.4** (Lower bound for one-way marginals (Steinke and Ullman, 2016, Main Theorem)).: _For every \((0,1)\), every function \(=(n)\) such that \( 2^{-o(n)}\) and \( 1/n^{1+(1)}\), and for every \( 1/10\), if \(A:\{ 1\}^{n d}[ 1]^{d}\) is \((,)\)-differentially private and \([\|(S)-q(S)\|_{1}] d\) (i.e., with average-case accuracy \(\)) for all \(S\{ 1\}^{n d}\), then we must have_

\[n}{} .\]

Using this lower bound as a blackbox, we then can adapt the argument of (Bassily et al., 2014, Lemma 5.1, Part 2) to obtain the following stronger result:

**Lemma 3.5** (Lower bound for Privately Computing 1-way Marginals).: _Let \(n,d,>0,2^{-on}(n) 1/n^{1+(1)}\). For all \( 1/10\), if \(\) is \((,)\)-differentially private and \(S\{}\}^{n d}\):_

\[[\|(S)-q(S)\|_{2}]=\!(, }{n}),\]

_where \(q(S)=_{i=1}^{n}x_{i}\) as before._

Combining the above with the argument strategy of (Bassily et al., 2014, Theorem 5.3) finally yields the main lemma for the second step of our proof:

**Lemma 3.6** (Lower bound on empirical loss of \((,)\)-DP algorithms).: _Let \(n,d,>0\), and \(=o(1/n)\). For every \((,)\)-differentially private algorithm with output \(^{priv}\), there is a dataset \(S=\{x_{1},,x_{n}\}\{-},}\}^{d}\) such that_

\[(^{priv},S)-(^{*},S) =\!(^{2}, ^{2}})\]

_where \(^{*}:=^{n}x_{i}}{\|_{i=1}^{n}x_{i}\|_{2}}\) is the minimizer of \((,S):=-,_{i=1}^{n}x_{i}\)._

The above lemma establishes a lower bound on the empirical loss of any \((,)\)-differentially private algorithm. To derive from this our claimed lower bound on unlearning algorithms, we need to introduce a dependence on \(m\), the deletion capacity (i.e., number of points to unlearn). This is done in the last (third) step of our argument, via a reduction which establishes a (restricted) converse to the grouposition property of DP.

Recall that an algorithm \(M^{n}\) satisfies \((,)\)-DP for edit distance \(m\) if for every pair of neighboring datasets \(X,X^{}\)_that differ in up to \(m\) entries_, and every \(S\):

\[[\,M(X) S\,] e^{}[\,M(X^{}) S\,]+.\]

We apply this \(m\)-edit distance \((,)\)-DP on Lemma 3.6 by a reduction that shows: for any differentially private algorithm with respect to edit distance at most \(m\) must incur an empirical loss given by Lemma 3.6.

**Lemma 3.7**.: _Suppose there exists an \(m\)-edit distance \((,)\)-DP algorithm \(\) that takes in a dataset \(S\) of size \(n\) to approximate \(q(S)\) (as defined in (4)), with empirical loss \(\). Then, we can construct a \(1\)-edit distance (i.e., standard) \((,)\)-DP algorithm \(^{}\) that, on input a dataset \(S^{}\) of \(N=n/m\) data points, approximates \(q(S^{})\) to error \(\)._

Proof.: The reduction is quite simple: given \(\), construct \(^{}\) as follows for \(N=\) inputs:

\[^{}(x_{1},,x_{N})=(, ,x_{1}}_{m},,,x_{2}}_{m},,,,x_{N}}_{m})\,.\]

We immediately have that \(^{}\) is \((,)\)-DP for the usual \(1\)-edit distance between datasets, since \(\) is DP with respect to edit distance \(m\). The sample complexity and error bound then follows direction from \(n=N m\), where \(n N,N,m 1\), and the fact that \(q(x_{1},,x_{N})=q(x_{1},,x_{1},x_{2},,x_{2},,x_{N}, ,x_{N})\) due to the definition of \(q\). 

Combining Lemma 3.7 with Lemma 3.6, we get that any \(m\)-edit distance \((,)\)-DP algorithm \(\) approximating \(q\) on datasets of size \(n=N m\) must have error \(\) at least

\[^{2}, ^{2}}=^{2},d (1/)}{n^{2}^{2}}\]

which, reorganising the terms and recalling the definition of deletion capacity, yields the claimed bound on \(m_{,}^{A,A}\). 

We note that the proof of Theorem 1.2 follows from a very similar argument; we refer the reader to the Supplemental for details.

## 4 Discussion and future work

Our work fully characterized deletion capacity of any unlearning algorithm \((,A)\) minimizing population risk under both convex and strongly convex loss functions, when only given the model parameters (output of the learning algorithm) and the set of deletion requests. This restriction, namely that the unlearning algorithm does not rely on any additional side information, is motivated by the potential privacy risks storing (non-private) side information can entail.

We hope our work will lead to further study of the interplay between differential privacy and machine unlearning, and to additional study of "DP-like" properties of machine unlearning, such as the postprocessing and composition properties our present work identified. In view of the myriad applications these properties have had in privacy-preserving algorithm design, we believe that their analogue for machine unlearning will prove very useful.

We leave for future work the question of which unlearning guarantees can be obtained from _pure_ differentially private algorithms, and of whether variants of the standard threat model for differential privacy (specifically, pan-privacy, or privacy under continual observation) could have implications for machine unlearning in an online setting where deletion requests come sequentially.