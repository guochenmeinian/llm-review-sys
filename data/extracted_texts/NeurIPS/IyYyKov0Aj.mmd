# Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference

Tao Lei1 Junwen Bai Siddhartha Brahma Joshua Ainslie Kenton Lee Yanqi Zhou Nan Du Vincent Y. Zhao Yuexin Wu Bo Li Yu Zhang Ming-Wei Chang

Google

###### Abstract

We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves _inference efficiency_. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation. Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase. Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge. Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approaches with moderate to no accuracy loss and the same parameter efficiency.

## 1 Introduction

Large pretrained models have achieved groundbreaking results but the main impediment to deploy them has been the cost of adaptation and inference. Due to the ever growing size of the pretrained models, for example, finetuning has become increasingly expensive as it requires a separate copy of the full model and updates to all parameters for every downstream task. Parameter-efficient transfer learning such as Adapter (Houlsby et al., 2019) and Prompt Tuning (Lester et al., 2021) have been proposed to address this issue. These methods only update a small subset of parameters for each downstream task, allowing the model to retain knowledge and avoid catastrophic forgetting (Vu et al., 2022). Noticeably, these methods can match the accuracy of a fully finetuned model, while achieving better accuracy on out-of-domain data distributions (Lester et al., 2021; Awadalla et al., 2022).

Unfortunately, standard parameter-efficient transfer learning methods only bring _parameter_ efficiency, not _inference_ efficiency. For example, while only a few small projection matrices are added into the pretrained model in the Adapter approach, all the model inputs (such as tokens) still use all parameters during inference. Therefore, the inference speed is the same (or slightly lower) with respect to the full finetuning method. Moreover, prior studies have shown that these parameter-efficient learning methods are most effective when the size of the pretrained model is large (Lester et al., 2021), making many advantages of these methods difficult to realize in practice.

In this paper, we propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that offers both _parameter_ and _inference_ efficiency. CoDA is a generalization of the adapter approach, built with the following intuition - we can treat the pretrained model as a universal source of knowledge but only query against it for _necessary inputs_. Figure 1 compares CoDA with finetuning and standard adapter approaches. Similar to standard adapter approaches, our model adds and updates a small adapter in each layer, while fixing the pretrained Transformer blocks for downstream adaptation. Unlike previous approaches, however, CoDA assumes that many of inputtoken representations (of each layer) are not important for the prediction task and therefore do not require heavy computation. In such cases, the pretrained Transformer block can be skipped. Given that many tokens are not processed by the Transformer block, CoDA runs significantly faster than previous methods.

While conditional activation has clear speed benefits, CoDA must learn to select important tokens for heavy computation in order to maintain model accuracy. To this end, we introduce a soft top-\(k\) operation for computing the token selection decision. This soft top-\(k\) operation, which can be seen as a generalization of softmax and a relaxation of hard top-\(k\), utilizes entropy-regularized optimization techniques similar to computational optimal transport (Cuturi, 2013). As a result, its output can be computed using fast and differentiable iterations, allowing token selection to be directly optimized for model performance.

We apply CoDA on encoder-heavy tasks and evaluate its effectiveness on three different domains - natural language processing, computer vision and speech processing. Overall, CoDA achieves 2 to 8 times inference speed-up over standard adapter approach with moderate to no accuracy loss. Table 1 showcases our results by selecting one of the best performing tasks in each domain. We also conduct comprehensive ablation studies to analyze the effectiveness, efficiency and scalability of CoDA. For example, we found that with just a little to no router pretraining, existing dense pretrained models such as T5 (Raffel et al., 2020) can be efficiently converted into CoDA models to gain both parameter efficiency and speed advantages.

## 2 Related Work

Parameter-efficient transfer learning methodsDue to the ever-growing number of parameters in the pretrained Transformer models, various methods have been proposed for transfer learning with minimal parameter updates. Prompt tuning (Lester et al., 2021) and prefix tuning (Li and Liang, 2021) introduce new virtual token embeddings that can be finetuned as model parameters. Adapter approaches (Houlsby et al., 2019; He et al., 2021) add a small number of new, learnable parameters to each layer while keeping the pretrained parameters fixed. Another popular method, Low-Rank Adaptation (LoRA; Hu et al., 2021), injects learnable low-rank decomposition matrices into pretrained model parameters. In addition to requiring less storage cost, parameter-efficient methods have been shown to be more sample-efficient and achieve better out-of-domain generalization than standard finetuning. CoDA is an adapter approach but can be easily combined with other parameter-efficient methods such as LoRA to accelerate their inference.

Conditional computationThe development of sparsely and conditionally activated models has been a very active research area. For example, Mixture-of-Experts (MoE) models (Shazeer et al.,

    &  &  \\   & & Acc \(\) & Speedup \\  P.Adapter & 0.4\% & 91.5 & 1.0x \\ CoDA & 0.4\% & 90.7 & **3.2x** \\   &  &  \\   & & EM \(\) & Speedup \\  P.Adapter & 2.8\% & 67.5 & 1.0x \\ CoDA & 2.8\% & 67.6 & **8.0x** \\   &  &  \\   & & WER \(\) & Speedup \\  P.Adapter & 2.5\% & 1.4/2.7 & 1.0x \\ CoDA & 2.5\% & 1.4/2.8 & **2.2x** \\   

Table 1: CoDA significantly reduces the inference time compared to the Parallel Adapter approach (He et al., 2021), while still maintaining parameter efficiency.

Figure 1: Comparison between different ways to use pretrained Transformer models, including (1) standard finetuning (left) where all parameters are tunable and computation is dense, (2) standard adapters (center) where a small set of new tunable parameters are added while the computation remains dense, and (3) CoDA (right) where the computation is sparsely activated.

2017) and many recent advances (Du et al., 2022; Fedus et al., 2021) have been proposed to scale up the size of language models without increasing the computation cost. Many recent works have explored better token routing methods for MoE models, for example using random hashing (Roller et al., 2021), balanced assignment (Lewis et al., 2021) and expert-choosing router (Zhou et al., 2022). CoDA applies conditional computation to both attention and feed-forward blocks of the model, whereas MoE models only focus on sparse activation in the feed-forward blocks.

Similar to our approach, various recent methods have achieved computation efficiency by skipping computation on a subset of input tokens. However, the selection mechanism can be very different, such as using pooling (Nawrot et al., 2022), token merging (Bolya et al., 2023), token pruning (Rao et al., 2021; Yin et al., 2022), learned sigmoid gates (Bapna et al., 2020) and early exiting (Schuster et al., 2022). While most of the token merging and pruning methods have been proposed for vision tasks, we show that CoDA is applicable to multiple domains including text, vision and speech. In addition, token merging and our token selection method are built with different inductive biases and intuition. Token merging leverages redundancies in visual tokens, while token selection assumes a spike of token relevance. That is, only a few tokens are necessary for the prediction task. Another major difference is that CoDA dynamically routes and updates token representations in each layer, whereas if a token is pruned (or merged), it will never be re-used by subsequent layers. We believe our token routing mechanism is more suited for text and speech applications, such as question answering, where different tokens might play important roles in different layers, or given different input queries.

Finally, CoDA is closely related to a concurrent work, CoLT5 (Ainslie et al., 2023), which also utilizes conditional activation (token selection) for inference efficiency. The focus of CoLT5 and CoDA are very different. CoLT5 specifically tailors its model architecture for long text (e.g. over 16k tokens), for example, by combining local attention with routed attention. The CoLT5 models are pre-trained from scratch and all parameters are finetuned for downstream tasks. In comparison, CoDA is directly initialized and adapted from an already pretrained dense model, and we optimize its performance on parameter-efficient transfer learning. The strengths of CoDA and CoLT5 can be combined for long text applications.

Efficient Transformer modelsMany efficient Transformer variants have been proposed to accelerate model computation. Examples include creating fast attention variants (Wang et al., 2020; Beltagy et al., 2020; Guo et al., 2022; Hua et al., 2022), searching network architectures (Press et al., 2019; So et al., 2021; Su et al., 2021) and utilizing non-attention neural modules for efficiency (Gulati et al., 2020; Lei, 2021). CoDA utilizes conditional computation as an orthogonal approach for efficiency.

Model compressionApart from building efficient model architectures, model compression methods such as pruning (Han et al., 2016; Zhu and Gupta, 2017; Wang et al., 2020; Xia et al., 2022) and distillation (Hinton et al., 2015; Kim and Rush, 2016; Turc et al., 2019; Lin et al., 2020) can be adopted to speed up model inference. Compared to these methods, CoDA retains all model parameters of the pretrained large model, and therefore avoids retraining a new model from scratch or knowledge forgetting caused by parameter removal. In addition, CoDA can be seen as a dynamic version of layer pruning because it can activate different Transformer layers for each token, and can be further combined with distillation to reduce the loss of accuracy caused by conditional computation.

## 3 Method

### Architecture

Throughout this and the experiment section, we build CoDA on top of parallel adapters (He et al., 2021). However, note that our method can be generalized to other types of adapters such as sequential adapters (Houlsby et al., 2019) and LoRA (Hu et al., 2021). We present additional experimental results using LoRA in Appendix B.3. Figure 2 illustrates our architecture and shows how CoDA computes its output by selecting only a small subset of input tokens to query against the pretrained model. When parallel adapters are used, CoDA introduces a small number of learnable parameters in the parallel branches, while the vast majority of model parameters (associated with the pretrained Transformer layers) remain fixed. In addition, CoDA only sends \(k= n/r\) tokens for heavy processing. We define \(r>1\) as the reduction factor, a constant (such as 4) to control the computation saving.

Next, we briefly introduce our notations and describe the computation of CoDA in detail. We use \(F()\) to denote a parameterized neural network and the corresponding function defined by the network. For instance, a Transformer layer (Vaswani et al., 2017) consists of an attention sub-layer \(F_{}()\) followed by a feed forward sub-layer \(F_{}()\). Each layer also employs layer normalization (Ba et al., 2016), namely \(LN_{}()\) and \(LN_{}()\), before applying the attention and feed forward functions. We define \(^{n d}\) as the input of a Transformer encoder layer, where \(n\) is the number of input tokens and \(d\) is the hidden size of the model.

Given layer input \(\), we first apply layer normalization, namely \(_{}=LN_{}()\). The normalized input will be processed by the adapter branch and the conditional Transformer branch. Their outputs are then added and combined as the final output of the layer.

Adapter branchLet \(F_{}()\) denote the transformation function of the adapter branch. The output is defined as

\[_{}=F_{}(_{}) \]

Similar to the previous approaches, \(F_{}()\) is realized using a feed forward network with a small hidden size such as 64. As a result, computing \(_{}\) only incurs a small number of floating point operations and its cost is often negligible compared to the cost of the heavy Transformer branch. The adapter branch does not conditionally select tokens. In other words, \(F_{}()\) is applied to all input tokens \(^{n d}\).

Conditional branchThe computation of the conditional branch takes three steps. First, each CoDA layer defines a router function \(F_{}()\) to select \(k\) tokens for the conditional branch. The router function in each layer returns two outputs

\[,=F_{}(_{}) \]

where \(\{0,1\}^{k n}\) is a matrix consisting of \(k\) one-hot vectors indicating the selection of tokens. Here \([i,j]=1\) if and only if the \(i\)-th selected token is the \(j\)-th input token from \(}\). \(^{n}\) is a weight mask in which \([j]\) is the selection weight for the \(j\)-th input token. \([j]=0\) if the token is not selected. We will describe how the router learns the selection in more details later in this section.

After the routing decision is made, the input representations of the selected tokens can be collected using a matrix multiplication,

\[_{}=_{}^{k d} \]

where \(k\) rows in \(_{}\) are selected to construct the \(k\)-by-\(d\) matrix \(_{}\). Similar to a standard Transformer layer, the conditional branch applies attention and feed forward transformations to the selected input:

\[}_{} =F_{}(_{}) \] \[_{} =F_{}(LN_{}(_{}+}_{})) \]

where \(}_{}\), \(_{}^{k d}\) denote the output of the attention network and the feed forward network respectively.

We consider two attention variants which differ in how they compute key-value vectors. One variant applies a _\(k\)-to-\(k\) attention_ using \(_{}\) as both the query vectors and key-value vectors. The other variant applies a _\(k\)-to-all attention_ using the entire input vectors \(_{}\) as the attention keys and values. The \(k\)-to-all variant runs slower but obtains higher quality close to the full model. We compare the performance of the two variants in Section 5.

The attention and feed-forward output \(}_{}\) and \(_{}\) are combined and projected back to the same shape of the original input

\[_{}=^{}(}_{}+_{ })^{n d} \]

Figure 2: Illustration of a single CoDA layer with parallel adapter. \(k\) tokens are selected and processed by the frozen pretrained Transformer layer, and all tokens are processed by the fast adapter layer.

[MISSING_PAGE_FAIL:5]

### Training

CoDA can be directly initialized from an existing Transformer model. Given a pretrained model such as T5 (Raffel et al., 2020), the Transformer layers are directly re-used and copied in the conditional branches of CoDA, and only the adapter and router parameters are randomly initialized. Because pretraining a large dense model can be expensive, our method reduces the overall training cost.

The routers and neural network components in CoDA must co-operate and be optimized for accurate model predictions. When the available finetuning data is limited, a random initialization for the router (and adapter) parameters can be sub-optimal. We demonstrate that CoDA can be further pretrained using the same pretraining objective as the dense model, in order to enhance downstream performance. Importantly, CoDA requires significantly fewer training steps during pretraining, since most of its parameters are taken from an already pretrained model. We show that the cost of CoDA pretraining can be 10-30x lower than the pretraining of its original dense model. We present this analysis in Section 5.

Finally, we train CoDA on downstream tasks by only updating the adapter, router and layer normalization parameters. The size of the adapters is small (e.g. 5M parameters), and each router and layer normalization block only introduces \(d\) parameters, where \(d\) is the model dimension. As a result, CoDA remains parameter-efficient similar to previous adapter and prompt-tuning methods.

## 4 Experimental setup

CoDA is evaluated on three domains including natural language processing (NLP), computer vision and speech processing, and on a range of applications such as classification, question answering, summarization and speech recognition. The experiments are organized as follows: We first demonstrate the effectivenss of CoDA conduct analyses on its design choices using the publicly available T5 models (SS5). In our final results (SS6), we pretrain Transformer models from scratch and extend our evaluation to vision and speech domains.

DatasetsWe use the C4 corpus (Raffel et al., 2020) for pretraining text models. For speech models, we use the LibriLight corpus (Kahn et al., 2020) for pretraining. Our vision Transformer models use the same data and training procedure in Pix2Struct (Lee et al., 2022). Our finetuning datasets for text models include the MNLI (Williams et al., 2018), RTE (Dagan et al., 2005; Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), BoolQ (Clark et al., 2019), SQuAD (Rajpurkar et al., 2016) and XSum (Narayan et al., 2018) datasets. The speech models are evaluated on the speech recognition task using the LibriSpeech dataset (Panayotov et al., 2015). Finally, we use the OCR-VQA (Mishra et al., 2019), DocVQA (Mathew et al., 2021), and Screen2Words (Wang et al., 2021) datasets for vision models.

    & &  &  &  \\  Model & Reduction \(r\) & MNLI & RTE & BookQ & MNLI & RTE & BookQ & MNLI & RTE & BookQ & \(\) Avg \\  Parallel Adapter & - & 87.1 & 71.5 & 77.9 & 90.3 & 84.8 & 85.8 & 91.5 & 89.9 & 88.4 & \(\)0.0 \\ (w/o conditional computation) & - & 86.7 & 72.6 & 76.6 & 90.2 & 85.9 & 85.1 & 91.4 & 91.3 & 89.4 & \(+\)0.2 \\ 
**CoDA, \(k\)-to-\(k\) attention** & 3 & 86.3 & 72.2 & 76.2 & 89.8 & 87.0 & 83.7 & 91.4 & 89.5 & 88.2 & \(-\)0.3 \\ 
**CoDA, \(k\)-to-all attention** & 5 & 86.0 & 70.8 & 76.0 & 89.7 & 85.2 & 84.3 & 91.0 & 91.3 & 87.2 & \(-\)0.6 \\ CoDA, \(k\)-to-\(k\) attention & 82.5 & 70.8 & 75.4 & 88.1 & 87.0 & 81.8 & 89.9 & 87.7 & 84.8 & \(-\)2.1 \\   

Table 2: Results of applying CoDA to T5 v1.1 models. CoDA achieves significant computation savings while retaining accuracy close to the dense baseline. We compare CoDA to a corresponding parallel adapter method that processes all tokens without conditional computation. We report accuracy on the development set on 3 tasks \(\) 3 model sizes, and set the number of selected tokens \(k= n/r\). The last column shows the change on average accuracy with respect to the parallel adapter method. We select the \(k\)-to-all version as our default (shown in bold).

## 5 Understanding and Analyzing CoDA

SetupWe present several analyses to validate the design choices of CoDA in this section. We initialize CoDA using the version 1.1 release of T5 checkpoints2, and perform CoDA pretraining using the same setting as the T5 models. During pretraining, we set routing capacity to \(k=192\) given input sequence length \(n=512\). We do not tune the value of \(k\) for pretraining, but will report the results of using different \(k\) values in finetuning. We perform 100K gradient steps, which is 10% of the total number of steps used to train the T5 dense models. The overall computational cost is over 20x less than the full training of dense models, since CoDA only applies heavy computation on less than half of the tokens.

For simplicity, we evaluate on classification tasks for various ablation studies of CoDA. Specifically, we report results on the MNLI, RTE and BoolQ datasets, and test three different model sizes including the Base, Large and XL size of T5. We will extend our evaluation to generation tasks such as question answering in the full result section (SS6).

Can CoDA be fast and accurate?Table 2 presents the finetuning results of CoDA. As a comparison, we also report the results of Parallel Adapter, which is similar to CoDA except that it applies the expensive Transformer layers to all input tokens. This constitutes an upper-bound, and is a strong baseline that has been reported as the best among a range of adapter and prompt tuning methods . As shown in Table 2, CoDA can achieve 3-5x computation reduction (\(r=3,5\)) in the Transformer layers at a cost of less than 1.0 point drop on average accuracy. As expected, our \(k\)-to-all attention variant achieves consistently better accuracy than the \(k\)-to-\(k\) variant, since it can access the full attention context. On the other hand, the \(k\)-to-\(k\) attention variant runs faster in practice, which can be beneficial for tasks with very long inputs. We select the \(k\)-to-all version in the final result section (SS6).

How many pretraining steps are needed?Figure 3 plots the finetuning accuracy by varying the number of pretraining steps for CoDA. Because CoDA can be initialized using pretrained dense models, it requires as few as 20K steps to obtain competitive finetuning results. Of course, using more pretraining steps can improve the downstream accuracy. The fact that CoDA can be quickly updated without repeating the expensive pretraining will be very beneficial in real-world applications.

Does learned routing matter?We analyze the impact of learned routing in Table 3 by comparing our soft top-\(k\) router with other router implementations. We implement a variant that replaces soft top-\(k\) with the sigmoid activation function, so the selection weight of each token activates on its own (without considering the capacity constraint). As shown in the table, this variant

Figure 3: Finetuning accuracy (y-axis) as a function of CoDA pretraining steps (x-axis). We show results using 0, 20K, 50K and 100K pretraining steps, and for reduction factor \(r=3\) and \(r=5\) respectively. CoDA requires as few as 20K steps to obtain competitive finetuning accuracy.

    & &  &  &  \\  Model & Reduction \(r\) & MNLI & RTE & BoolQ & MNLI & RTE & BoolQ & MNLI & RTE & BoolQ & \(\) Avg \\ 
**Soft top-\(k\)** & & 86.3 & 72.2 & 76.2 & 89.8 & 87.0 & 83.7 & 91.4 & 89.5 & 88.2 & \( 0.0\) \\ Sigmoid gate as \(f(s)\) & 3 & 85.7 & 70.8 & 72.8 & 89.2 & 82.3 & 81.0 & 90.6 & 88.1 & 86.2 & \(-2.0\) \\ Truncation – selecting first \(k\) & 81.1 & 70.8 & 72.7 & 84.9 & 77.3 & 82.3 & 85.6 & 84.5 & 85.4 & \(-4.4\) \\ 
**Soft top-\(k\)** & & 82.5 & 70.8 & 75.4 & 88.1 & 87.0 & 81.8 & 89.9 & 87.7 & 84.8 & \( 0.0\) \\ Sigmoid gate as \(f(s)\) & 5 & 82.9 & 71.5 & 72.1 & 86.7 & 82.3 & 80.1 & 88.3 & 87.0 & 82.4 & \(-1.6\) \\ Truncation – selecting first \(k\) & & 62.2 & 64.6 & 71.1 & 64.9 & 70.4 & 75.4 & 66.6 & 76.2 & 81.1 & \(-12.9\) \\   

Table 3: Ablation study on routing methods. We use CoDA \(k\)-to-\(k\) variant for a fair comparison with the truncation method. Better routing method delivers better accuracy on various tasks and model sizes tested. We use soft top-\(k\) as our default method.

achieves worse accuracy on almost all tasks and model sizes tested, getting 2.0 point worse on average. We also implement a "no-learning" baseline that simply selects the first \(k\) tokens, which is equivalent to truncating the input sequence.3 This baseline performs much worse, resulting in more than 10 point decrease in accuracy for small \(k\) (and equivalently large \(r\)). This analysis confirms the importance of learning a good routing in order to retain strong model performance.

## 6 Full Results

SetupIn this section, we apply our best training recipe to all tasks and application domains. We first pretrain dense Transformer models, followed by the CoDA training procedure in SS3.2. Our speech models are pretrained using a masked language modeling (MLM) objective similar to BERT (Devlin et al., 2019), and random quantized output label space (Chiu et al., 2022). Our vision and text models use an encoder-decoder architecture similar to T5 but incorporate a few changes. Following PaLM (Chowdhery et al., 2022), we use multi-query attention (Shazeer, 2019) that shares the same key and value projection for multiple query heads. We only use 6 decoder layers and increase the feed forward hidden size (to compensate for the decrease in the number of layers). These modifications have a neutral effect on model quality, but speed up auto-regressive decoding significantly. We will show CoDA is compatible with these changes and can further speed up inference by a considerably large factor. We provide more details of our experimental setup in Appendix A.

NLP resultsIn addition to the classification datasets used in Section 5, we also evaluate our final models on the SQuAD, ReCord and XSum datasets which require generating an answer or a summary

    & Trainable & Reduction & MN1 & RTE & Basolo & SQuAD & ReCord & XSum & \(\)Avg \\   & Penning & \(r\) & Acc. & Acc. & Acc. & F1 & F1 & R2 & \\   Parallel Adapter & 10M & - & 91.5 & 91.0 & 88.5 & 94.8 & 91.4 & 21.9 & 40.0 \\ CoDA & 3 & 91.2 & 90.3 & 87.5 & 94.1 & 89.3 & 20.6 & \(-1.0\) \\ CoDA & 5 & 90.7 & 90.5 & 87.3 & 93.5 & 87.6 & 20.2 & \(-1.7\) \\   & 15M (240) & - & (06.3) & - & - & - & - & 20.5 \\  & & 10M (240) & - & (07.2) & - & - & - & - & 20.0 \\  Sequential Adapter (Heulshy et al., 2021)1 & 10M (240) & - & - & - & - & - & - & 20.7 \\  Parallel Adapter (He et al., 2021)1 & 10M & - & - & - & - & - & - & 20.7 \\   

Table 4: Comparison of CoDA and parallel adapter on 6 language tasks. We report results on the test set of XSum, and on the development set of other tasks. \(\) indicates results taken from He et al. (2021), and referenced results in bracket correspond to using 2M adapter parameters. Note that our Parallel Adapter numbers are stronger as our pretrained Transformer backbone uses more parameters than the model used in He et al. (2021).

Figure 5: The scaling of CoDA on the XSum and LibriSpeech dataset. Left: CoDA achieves better speed-quality trade-off than finetuning adapters with smaller models, on the XSum dataset. Middle: larger CoDA model achieves higher speed-ups. Right: CoDA achieves better speed-quality trade-off than the dense baseline on the LibriSpeech dataset.

given the input. Table 4 contains the finetuning results of XL models. Compared to the parallel adapter baseline that uses full computation, CoDA achieves 3x and 5x computation reduction with only 1.0 and 1.7 point loss in average score.

Figure 4 and 5 highlight the scaling trend of CoDA. CoDA runs much faster with slightly worse quality than the parallel adapter baseline. This is expected because the baseline processes all tokens in every layer, whereas CoDA only selects \(1/r\) of tokens for heavy processing. Importantly, this quality gap reduces as the model size increases (as shown in Figure 4), making CoDA a computationally efficient choice for large models. Indeed, CoDA can trade off quality for speed by varying the number of selected tokens. Figure 5 (left) demonstrates that CoDA achieves much stronger speed-quality trade-off compared to dense models without conditional computation. The black line indicates the results of Parallel Adapter when the model size grows from Small to XL, and each blue line represents the speed-quality trade-off of CoDA using \(r=1,3,5\). Moreover, Figure 5 (middle) shows that larger CoDA models exhibit higher inference speed-ups. These observations are consistent on other tasks. We provide additional results in Appendix SSB.

Speech recognition resultsWe further validate the performance of CoDA in the speech domain. Our model uses a Transformer encoder and a 2-layer LSTM Transducer [Graves, 2012]. Similar to NLP setups, we test the performance of the speech model on 3 scales - Base, Large and XL (see Appendix A for details). Table 5 demonstrates that with sizable reduction ratios (\(r=2,4\)), the change on word error rate (WER) is consistently minimal on the test-clean and test-other sets of Librispeech across different model sizes (and on other sets in SSB.2). Moreover, our results are comparable to the top-performing models, such as w2v-BERT [Chung et al., 2021] and BEST-RQ [Chiu et al., 2022], that are fully finetuned by updating all parameters. Figure 5 (right) highlight again that applying conditional computation leads to better speed-quality trade-off compared to dense models.

Vision resultsWe extend our experiments to visual tasks that involves natural language within the image, such as documents and user interfaces. Our experiments are based on Pix2Struct [Lee et al., 2022], where an image-encoder-text-decoder is pretrained by learning to predict simplified HTML from webpage screenshots. Table 6 shows the results on three tasks that were also evaluated in the

    &  &  &  &  \\   & & EM & Speedup & ANLS & Speedup & CIDEr & Speedup \\  Parallel &  \\ Adapter & - & 67.5 & 1\(\) & 70.8 & 1\(\) & 110.2 & 1\(\) \\  CoDA & 4.62 & 6.4 & 71.8 & 4.6/\(\) & 111.6 & 4.6/\(\) \\ CoDA & 8 & 67.6 & 8.0/\(\) & 66.6 & 8.0/\(\) & 108.1 & 8.0/\(\) \\ CoDA & 16 & 66.9 & 13.5/\(\) & 56.6 & 12.1/\(\) & 109.0/\(\) & 12.5/\(\) \\ CoDA & 32 & 64.4 & 19.4/\(\) & 42.5 & 16.7/\(\) & 104.2 & 17.8/\(\) \\   

Table 6: Comparison of CoDA and the parallel adapter applied to a pretrained Pix2Struct model [Lee et al., 2022] on 3 visually-situated language understanding tasks.

    &  &  &  &  \\   & & clean & other & clean & other & clean & other \\  w2v-BERT & - & 1.8 & 3.6 & 1.5 & 2.9 & 1.5 & 2.9 \\ BEST-RQ & - & 1.7 & 3.5 & 1.6 & 2.9 & 1.4 & 2.7 \\  P-Adapter & - & 1.6 & 3.5 & 1.4 & 3.0 & 1.4 & 2.7 \\ CoDA & 2 & 1.6 & 3.5 & 1.4 & 3.0 & 1.4 & 2.8 \\ CoDA & 4 & 1.6 & 3.6 & 1.5 & 3.1 & 1.4 & 2.8 \\   

Table 5: Comparison of CoDA and the parallel adapter baselines on Librispeech. We report the WER results on test-clean and test-other. More results can be found in SSB.2.

Figure 6: Visualization of routing preferences for a CoDA model applied to the OCR-VQA task. Warmer and cooler colors represent higher and lower scores respectively. Router prefers diverse coverage in early layers, but converges to selecting sparse and representative patches in later layers.

original Pix2Struct paper. In OCRVQA and Screen2Words, we observe relatively small drops in performance when reducing the number of routed tokens (i.e. patches). When the capacity is 1/16th of the original sequence length, leading to around 13\(\) speedup, we only lose about 1 point. We speculate that this is due to the high-level sparsity in the inputs for these two tasks. For DocVQA, where there is comparatively more textual information, we observe a steeper performance-speed trade-off but still achieve a 8\(\) speedup with a 4-point drop.

To provide a more intuitive understanding why CoDA works, we visualize the router behavior for the OCR-VQA model in Figure 6. We show which patches the routers prefers the most (warmest colors) and least (coolest colors), for several layers. The first, immediately obvious, observation is that router avoids low-frequency patches, i.e. patches likely to be "whitespace", since they can be adequately handled by the cheap adapter layers. The second, more subtle, observation is that the router progressively converges on a small number of key patches that we hypothesize serve as representations for larger regions. The visualization confirms that CoDA is able to select meaningful and representative patches that are useful for the prediction task.

## 7 Conclusion and Limitation

We present CoDA, a parameter-efficient adapter method that enables fast inference. CoDA relies on conditional computation to selectively activate model computation on important input units, providing a novel way to balance model expressivity and efficiency.

In this work, we focus on encoder-heavy applications such as summarization, speech recognition and visual question answering, by applying our method to the encoder. One limitation of CoDA is that the current routing mechanism (i.e. token selection in a given sequence) is not directly applicable to decoder-only models for auto-regressive token generation. Enabling fast token generation using conditional activation in decoder layers is an interesting direction we plan to explore in future work.

## 8 Acknowledgements

We would like to thank Rama Pasumarthi, Hongkun Yu, Kelvin Guu, Zhuyun Dai, Timothy Dozat, Raphael Hoffmann, Tao Wang, Tal Schuster, Ziwei Ji, Frederick Liu and Slav Petrov for helpful advice and discussion.