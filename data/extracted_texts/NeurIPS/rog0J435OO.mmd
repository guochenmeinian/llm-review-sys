# FlashMask: Reducing the Complexity of Attention Computation through Sparse Mask Representation

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recent advancements in Larger-Scale Transformers have significantly benefited from sophisticated attention mechanisms, which are critical for modeling long-context sequences. However, the computational and memory demands of conventional attention mask computations, typically scaling with an \((N^{2})\) complexity where \(N\) is the sequence length, pose significant challenges. This paper introduces FlashMask, a simple yet effective _Exact_ attention algorithm designed to substantially reduce both the computational complexity and memory requirements of attention computations. By adopting a novel column-wise sparse representation of attention masks, FlashMask achieves a linear memory complexity of \((N)\) and computational complexity of \((N)(N^{2})\). We assess the performance of FlashMask in a variety of masking scenarios, including causal and customized attention masks, demonstrating its versatility and robustness across a wide range of attention patterns and models. Our empirical analysis encompasses a variety of downstream training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM). We compare FlashMask against state-of-the-art techniques, including notably FlashAttention . In kernel-level assessments, FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM). Furthermore, in end-to-end training, FlashMask consistently enhances training speed significantly, with accelerations up to 2.4x (SFT), 4.2x (LoRA), 2.5x (DPO), and 2.6x (RM) across these varied scenarios without sacrificing model accuracy. Additionally, when implemented in the LoRA scenario, FlashMask enables the LLaMA2-7B to process sequence lengths of up to 544k, significantly enhancing its capability for long-context input.

## 1 Introduction

Transformers , equipped with self-attention mechanisms, have revolutionized natural language processing (NLP) by efficiently modeling data dependencies without the limitations of sequential processing. This makes them ideal for handling long sequences. Large Language Models (LLMs), which utilize training paradigms such as Supervised Fine-Tuning (SFT) [3; 4] and Reinforcement Learning from Human Feedback (RLHF) [5; 6], critically rely on selective attention management through masks. Effective mask management is essential to focus selectively on pertinent data segments, optimizing both performance and computational efficiency.

However, the conventional attention mechanism in Transformers entails a quadratic increase in computational and memory demands \((N^{2})\), where \(N\) denotes the sequence length. This exponential growth presents substantial challenges as models scale to sequence lengths ranging from 128K to 1M in advanced systems like GPT-4 , Claude , and Gemini , necessitating more efficient computational approaches. As sequence lengths extend, the memory load for masked attentioncomputations also grows quadratically, adversely affecting computational speed and the ability to manage various mask configurations across different tasks. Current methodologies often resort to approximate sparse attention strategies [10; 11; 12], which unfortunately trade off precision for computational efficiency, underscoring an essential gap in achieving high precision with reduced computational costs.

This paper introduces FlashMask, a novel approach utilizing a sparse mask representation to accelerate attention computations in transformers, effectively addressing both computational and memory scalability issues. Unlike previous methods that compromise accuracy for efficiency, FlashMask provides precise computations without sacrificing accuracy, ensuring high fidelity in attention mechanisms. The contributions of this work include:

* **Exact Computation.** FlashMask uniquely ensures precise attention computations across varying sequence lengths and tasks. It employs a unique column-wise sparse mask representation, denoted by FlashMaskStart (FMS) and FlashMaskEnd (FME), to precisely mask specific rows within columns, ensuring computational efficiency and accuracy.
* **Long Context Modeling.** FlashMask significantly reduces computational and memory demands, enabling efficient processing of extended sequences critical for deploying LLMs in resource-limited settings.
* **Efficient Mask Computation.** FlashMask leverages strategic sparse masking to increase computational throughput, thereby improving processing speeds and broadening the practical utility of LLMs in diverse real-world scenarios.
* **Extensive Empirical Validation.** Empirical studies validate FlashMask's efficiency in computation and storage. Its practical application in real-world scenarios and integration with existing frameworks underscore its potential impact. Moreover, a comprehensive comparison with state-of-the-art methods like FlashAttention-DenseMask, FlashAttention-Varlen highlights FlashMask's efficiency and versatility.

## 2 Background

The attention mechanism has revolutionized data handling in NLP by mimicking human selective focus, allowing neural networks to prioritize parts of the input data. This addresses limitations of traditional sequence-to-sequence models, enhancing context awareness in long sequences. The Transformer model by Vaswani et al.  implements this mechanism centrally, using multiple parallel attention heads instead of recurrent layers, thus improving efficiency and performance.

### Attention Computation

Central to the Transformer architecture is the attention mechanism, which computes relevance scores between elements in a sequence to focus more on important aspects and less on others. This mechanism can be expressed as:

\[_{mask}(Q,K,V)=(}{ }}+M)V,\] (1)

where \(Q\), \(K\), \(V\), and \(M\) represent the query, key, value, and mask matrices respectively, derived from the input data, and \(d_{k}\) is the dimension of keys. The term \(M\) incorporates constraints to selectively consider certain parts of the input sequence during attention computation, enabling functionality like masking future tokens in sequence-to-sequence modeling. One inherent challenge with attention is its computational and memory complexity, both of which scale quadratically with the length of the input sequence. Processing long sequences presents significant challenges, which are exacerbated in the downstream pipeline of training large language models (LLMs). Different training stages, such as Supervised Fine-Tuning (SFT/LoRA [3; 4; 13; 14; 15]), Direct Preference Optimization (DPO) [16; 17; 18; 19; 20], Reward Model (RM) [5; 21; 22; 23; 24], and Proximal Policy Optimization (PPO) [25; 6], place diverse demands on the attention mask.

### Masking Variable-Length Sequences

The advent of large transformer-based models has marked substantial progression in handling increased sequence lengths in natural language processing. Previously, models like BERT  and GPT-2  were limited to sequences of approximately 512 tokens, whereas more recent adaptations such as the LLaMA [28; 29; 30], GPT-4  and Claude series  stretched these limits to encompass 2K to 200K tokens, respectively. Innovations from Google's Gemini  have further shifted this boundary, managing up to 1M tokens. Enhanced sequence management within these models employs various masking techniques in the attention matrix, adapting to the length and diversity of input sequences. Techniques such as the use of padding operations are illustrated in Figure 1(a), which help maintain efficiency by allowing uniform processing of diverse input lengths through padding masks. However, conventional padding can lead to inefficiencies due to the diverse sequence lengths typically found in training data, often following a long-tail distribution. This issue is adeptly addressed by dynamic token allocation technologies like InToken [31; 3; 32; 33; 34], which optimize computational resources by adjusting the token count based on actual data needs, significantly improving the training efficiency for datasets with various sequence lengths in Figure 1(b)(c).

Despite having extensive text-handling capabilities, the meticulous design of masking configurations remains crucial for specific training scenarios. The illustrated scenarios in Figure 1(d) and Figure 2 depict various specialized masking mechanisms employed to enhance model training efficiency and applicability. Figure 1(d) illustrates a scenario involving DPO/RM with two or more answers, where each answer's tokens have visibility to the tokens of the question, and tokens from different answers are not visible to each other. Multi-shot and in-context learning scenarios facilitated by extended attention spans in configurations like Figure 2(a) are becoming prevalent, which allows the final question in a series to receive comprehensive attention, enhancing contextual understanding [35; 36]. Furthermore, hybrid masking forms combining features from different methodologies are demonstrated in Figure 2(b). These incorporate sink tokens  and a sliding window mask from the Big Bird , facilitating a localized yet extensive context capture. Figure 2(c) is also derived from Big Bird, showing a bi-directional global attention mask, which allows for a comprehensive global context capture. Such innovative approaches in masking not only bolster the efficiency of training large transformer models but also pave the way for advanced explorations into the capabilities of attention mechanisms, such as simulating token eviction during inference as depicted in Figure 2(d). These advancements underscore the dynamic and adaptable nature of transformer technology in accommodating varying training needs and enhancing the overall performance of LLMs.

### Attention Optimization Techniques

As aforementioned in Equation 1, the computational and memory demands of this mechanism, particularly the computation of \(QK^{T}\), become significant as the sequence length \(N\) increases. This is due to the size of the resultant attention scores matrix, which scales quadratically with the sequence length, leading to a complexity of \((N^{2})\). Several related works has been proposed to alleviate the issue. In the realm of model training optimizations, Memory Efficient Attention  (MEA) and FlashAttention  have been pivotal. MEA focuses on reducing the model's memory demands by altering the self-attention mechanisms. This allows either for the use of larger models or for the extension of maximum sequence lengths within existing hardware constraints. On the

Figure 1: Common patterns of attention masks. (a) Padded masks from single-sequence inputs in unidirectional (uni-) attention. (b) InToken masks from grouping several masks with different lengths in uni-attention. (c) InToken masks in bidirectional (bidi-) attention. (d) Question and Answering Masks in uni-attention.

other hand, FlashAttention enhances the efficiency of attention mechanisms with IO-Awareness to better utilize contemporary GPU architectures, resulting in faster computations and reduced energy consumption. This method reduces memory overhead to \((N)\) utilizing tiling techniques during the computation process, making it particularly effective in scenarios without the need for a custom mask. However, for specific training contexts requiring custom masking, the memory overhead with FlashAttention remains \((N^{2})\). Note that, in typical training setups like unidirectional causal attention or bidirectional full-context attention, the default mode of operation with FlashAttention does not involve passing a custom mask.

During the inference stage, optimizations such as FlashDecoding  and FlashDecoding++  play crucial roles. FlashDecoding enhances the decoder in transformers to expedite the generation of sequences by optimizing state management and employing techniques that minimize computational waste. FlashDecoding++ further advances these improvements, incorporating sophisticated dynamic batching and more refined state management to significantly boost throughput and reduce latency. Concerning long sequence training, RingAttention  is notable for its efficiency in distributed training contexts, managing communication overhead and memory utilization effectively across multiple nodes.

Another class of study targets on the sparsity/low-rank of attention computation. The Sparse Transformer  revolutionizes sequence processing with log-linear complexity. Similarly, Reformer  optimizes memory via locality-sensitive hashing, while Big Bird  introduces a hybrid attention method to manage longer sequences efficiently. Linformer  reduces complexity using low-rank approximations, significantly economizing computation and storage requirements. Both of the previously discussed solutions either compromise precision or yield only marginal enhancements in efficiency. Conversely, our proposed FlashMask is capable of delivering an exact computations.

## 3 FlashMask: Algorithm and Analysis

In this section, we present the critical design of the column-wise sparse mask representation, implementation of the mask computation kernel, and a complexity analysis of the proposed FlashMask.

### Column-wise Sparse Mask Representation

We introduce FlashMask, a column-wise sparse masking technique, represented using \(,^{N}\) (the row index of \(\)ash \(\)\(\) and \(\)ash \(\)\(\)), where \(_{c},_{c}\) denote that elements in the \(c\)-th column of the attention score matrix \(=^{T}\) within the interval \([_{c},_{c})\) are masked (set to \(-\)). As shown in Fig. 2(a), \(=\), \(=\) indicates that, for the first column, the 4-th to 6-th rows are masked.

### Integration with FlashAttention

Unidirectional (causal) attention, commonly utilized in large language models, incorporates FlashMask within the FlashAttention-2 algorithm, as detailed in Algorithm 1. This paper elaborates the implementation of FlashMask using the lower triangular section of the mask for illustration, where the blue section represents the computation by the dense mask method (for comparison and not

Figure 2: Extended patterns of attention masks. (a) In-context learning formatted multi-shot masks in uni-attention. (b) Sink + Slidewindow masks in uni-attention. (c) Global masks in bidi-attention. (d) Customized masks in uni-attention.

present in FlashMask) and the red section indicates the FlashMask computation. FlashAttention Forward involves two nested loops; the outer loop iterates over each block \(_{i}\) of \(\), and the inner loop iterates over all blocks \(_{j}\) of \(\) and \(_{j}\) of \(\). In the inner loop, \(_{i}^{(j)}=^{T}\) is computed on SRAM. Once \(_{i}^{(j)}\) is generated, the corresponding dense mask is added as a bias (shown in line 20 of Algorithm 1), whereas FlashMask applies the column-wise sparse mask by setting elements beyond \(_{c}\) but not exceeding \(_{c}\) to \(-\) (as shown in lines 21 to 23 of Algorithm 1).

```
0: Matrices \(,,^{N d}\) in HBM, block sizes \(B_{},B_{r}\), dense mask \(^{N N}\), column-wise sparse mask starting rows \(^{N}\), ending rows \(^{N}\).
1: Divide \(\) into \(T_{r}=}\) blocks \(_{1},,_{T_{r}}\) of size \(B_{r} d\) each, and divide \(,\) in to \(T_{c}=}\) blocks \(_{1},,_{T_{c}}\) and \(_{1},,_{T_{c}}\), of size \(B_{c} d\) each.
2: Divide the output \(^{N d}\) into \(T_{r}\) blocks \(_{1},,_{T_{r}}\) of size \(B_{r} d\) each, and divide the logsumexp \(L\) into \(T_{r}\) blocks \(L_{i},,L_{T_{r}}\) of size \(B_{r}\) each.
3: Divide \(\) into \(T_{r} T_{c}\) blocks \(_{1},,_{T_{r}}\).
4: Divide \(\) into \(T_{c}\) blocks \(_{1},,_{T_{c}}\), and divide \(\) into \(_{1},,_{T_{c}}\).
5: Precompute the max value \(_{1},,_{T_{c}}\) for each \(_{1},,_{T_{c}}\), write to HBM.
6: Precompute the max value \(_{1},,_{T_{c}}\) for each \(_{1},,_{T_{c}}\), write to HBM.
7: Precompute the min value \(_{1},,_{T_{c}}\) for each \(_{1},,_{T_{c}}\), write to HBM.
8:for\(1 i T_{r}\)do
10: Load \(_{i}\) from HBM to on-chip SRAM.
11: On chip, initialize \(_{i}^{(0)}=(0)_{B_{r} d}^{B_{r} d}\), \(_{i}^{(0)}=(0)_{B_{r}}^{B_{r}},m_{i}^{(0)}=(-)_{B_{r}} ^{B_{r}}\).
12:for\(1 j T_{c}\)do
13:if\(i B_{r}_{j}\)and\((i+1) B_{r}_{j}\)then
14: Continate
15:endif
16: Load \(_{j}\), \(_{j}\) from HBM to on-chip SRAM.
17: Load \(_{j}\) from HBM to on-chip SRAM.
18: Load \(_{j}\) from HBM to on-chip SRAM.
19: On chip, compute \(_{i}^{(j)}=_{i}_{j}^{T}^{B_{r}  B_{c}}\).
20: On chip, set \(_{i}^{(j)}=_{i}^{(j)}+_{i,j}\)
21:if\((i+1) B_{r}_{j}\)and\(i B_{r}_{j}\)then
22: On chip, set \(_{i}^{(j)}[x][y]=-\), \( x\), \(y\), such that \(_{j}[y] i B_{r}+x_{j}[y]\)
23:endif
24: On chip, compute \(m_{i}^{(j)}=(m_{i}^{(j-1)},(_{i}^{(j)})) ^{B_{r}}\), \(}_{i}^{(j)}=(_{i}^{(j)}-m_{i}^{(j)})^{B_{r} B_{c}}\) (pointwise), \(_{i}^{(j)}=e^{m_{i}^{j-1}-m_{i}^{(j)}}_{i}^{(j-1)}+( }_{i}^{(j)})^{B_{r}}\).
25: On chip, compute \(_{i}^{(j)}=(e^{m_{i}^{(j-1)}-m_{i}^{(j)}})^{-1} _{i}^{(j-1)}+}_{i}^{(j)}_{j}\).
26:endfor
27: On chip, compute \(_{i}=(_{i}^{(T_{c})})^{-1}_{i}^{(T_{c})}\).
28: On chip, compute \(L_{i}=m_{i}^{(T_{c})}+(_{i}^{(T_{c})})\).
29: Write \(_{i}\) to HBM as the \(i\)-th block of \(\).
30: Write \(L_{i}\) to HBM as the \(i\)-th block of \(L\).
31:endfor
32: Return the output \(\) and the logsumexp \(L\). ```

**Algorithm 1** Optimized Forward Pass with FlashMask

FashMask further exploits the block computation feature of FlashAttention-2 to reduce computation. If all elements within a block are masked, the block's computation, including matrix multiplication and softmax operations, can be skipped. A block defined by rows \([r_{0},r_{1})\) and columns \([c_{0},c_{1})\) is skipped if \(r_{0}(_{c_{0}:c_{1}})\) and \(r_{1}(_{c_{0}:c_{1}})\). Considering that mask regions often exhibit continuity, most blocks are either completely masked or not at all, with only boundary blocks requiring fine-grained masking. A block is completely unmasked if every coordinate \((r,c)\) satisfies \(r<_{c}\) or \(r_{c}\), thus skipping fine-grained masking and avoiding extra masking overhead.

To avoid redundant computations in the FlashAttention-2 compute loop, we precompute \((_{c_{0}:c_{1}})\) and \((_{c_{0}:c_{1}})\) for each block before the execution loop using a kernel. This computation has a complexity of \((N)\) and can be easily distributed over \(T_{c}=}\) thread blocks. A parallel reduction operation within each thread block then computes the maximum and minimum values, yielding \(T_{c}\) values. The additional space complexity introduced here is \((T_{c})\). Similar computations are made for \((_{c_{0}:c_{1}})\), \((_{c_{0}:c_{1}})\),..

The backward computation in FlashAttention-2, which is typically column-parallel, benefits more from the column sparse mask approach. Blocks for which \((_{c_{0}:c_{1}})}{B_{r}}< i<(_{c_{0}:c_{1}})}{B_{r}}\) are fully masked, allowing skipping of these intervals directly. Only blocks satisfying \((_{c_{0}:c_{1}})}{B_{r}}  i(_{c_{0}:c_{1}})}{B_{r}}\) require fine-grained masking.

It is important to note that unlike various approximate attention algorithms, our method ensures that each effective element of the attention score matrix is computed identically to FlashAttention-2, with masked elements explicitly set to \(-\), thus maintaining the accuracy of the algorithm's results. Futhermore, FlashMask is easily extendable to bidirectional attention computations.

### Complexity Analysis

We define sparsity as \(=}\), where \(p\) is the number of masked elements in the attention score matrix, and \(N\) is the maximum sequence length of Q and K, \(N^{2}\) being the total number of elements in the attention score matrix. For a causal mask, \(=}\) since half of the elements in the attention score matrix are already masked by the causal mask. The block sparsity \(\) is defined as \(=}}}\), where \(B_{r},B_{c}\) are block sizes, and \(a\) is the number of completely masked blocks. For a causal mask, \(=} }}\).

**Space complexity.** The dense mask is represented as \(^{N N}\), with a space complexity of \((N^{2})\). FlashMask denotes as \(,^{N}\), occupying \((N)\) space, along with four precomputed arrays \(,,, ^{}}\), also occupying \((N)\) space. Thus, the total space complexity for FlashMask is \((N)\), significantly reducing memory usage and supporting training on longer sequences.

**Memory access complexity.** The dense mask accesses the entire \(^{N N}\) matrix in line 20 of Algorithm 1, totaling \(N^{2}\) memory accesses on HBM. FlashMask reads the \(,^{N}\) vectors from HBM as shown in lines 17 and 18 of Algorithm 1, with each \(_{i}\) reading the entire \(,\), totaling \(2 T_{r} N\) memory accesses. This reduces the memory access to approximately \( N}{N^{2}}}\), significantly boosting performance. Due to FlashMask's smaller space usage, it is possible to preload \(,\) into SRAM using only \(2 B_{c}\) SRAM, enhancing memory access efficiency. For the backward process, which uses a column-parallel approach, SRAM-stored \(,\) can be well reused, further reducing the total memory access on HBM to \(2 N\).

**Computational complexity.** The attention computation process normally iterates over the entire attention score matrix, with a computational complexity of \((N^{2})\). By skipping entirely masked blocks, FlashMask leverages block sparsity to reduce computational complexity to \(((1-)N^{2})\).

## 4 Experiments

### Setup

Experiments were conducted using GPU A800-SXM 80G, Intel(R) Xeon(R) Platinum 8350C CPUs, CUDA 12.0, and driver version 525.125.06. We evaluated FlashMask against various methods including Vanilla Attention, FlashAttention with dense mask (FA-DenseMask), variable length (FA-Varlen), and sliding window (FA-Window) across different scenarios and sequence lengths. Both kernel-level and end-to-end performance demonstrated the effectiveness of our method.

### Data Construction

As mentioned in the Background section, commercial large models now support sequences up to 128K in length. FlashMask, with its lower memory overhead, can facilitate training with even longer contexts. However, currently available public datasets do not contain training data for scenarios exceeding 128K. For comprehensive testing of FlashMask, we constructed synthetic data to simulate long-sequence training.

For a given sequence length \(L\), sequences were generated by mimicking InToken method with several sub-sequences. Randomly selecting \(s\) split points uniformly within the range \((0,L)\), the sequence was divided into \(s\) sub-sequences. The segment from the last split point to the end of the sequence was considered as Padding. For the RM scenario, shorter sequence lengths used a smaller upper limit on the number of splits: \(s\) for \(L(0,4096]\) and \(s\) for \(L(4096,8192]\). By discarding samples not meeting size requirements, we ensure each sub-sequence length was at least 128 (SFT, LoRA, DPO) or 512 (RM) and padding not exceeding 128 (SFT, LoRA, DPO) or 512 (RM). Suppose one sub-sequence with length \(L^{}\) was further divided into a query and \(k\) answers based on the scenario. The length of each answer was randomly determined from the range \([}{1+0.1X L},}{1+0.2X L}]\), making the answer lengths approximately \([0.1,0.2]\) of the query length. Therefore, the query length was equal to \(L^{}\) minus the total answer lengths. A total of 240 valid samples per given sequence length \(L\) were collected and binned into 10 categories by sparsity \(\), as shown in Appendix A.2.

### Kernel Experiments

We conducted tests with batch sizes of 1, 2, and 4 using Vanilla Attention, FA-DenseMask, and FlashMask. Each experiment began with 5 warm-up runs followed by 50 measurements, totaling 55 runs with kernel latency as the performance metric. Additional comparisons were made with FA-Varlen in the SFT scenario. Results for batch size 1 are shown in Figure 3 (results for batch sizes 2 and

Figure 4: Top: Comparison of Kernel Latency while Varying Window Size. Bottom: Comparison of Kernel Latency while Varying Input Sparsity.

Figure 3: Comparison of Kernel Latency Based on Varying Sequence Lengths. FlashMask achieves substantial computational speedups, up to 6.7x (SFT), 6.9x (DPO), and 8.3x (RM).

4 can be found in Appendix A.3). FlashMask demonstrated significant latency advantages across all lengths, up to 8.3-fold time saving compared to FA-DenseMask. Vanilla Attention was significantly more time-consuming and exceeded memory limits at lengths greater than 32K. The closest competitor to FlashMask, FA-Varlen, exhibited higher latencies as sequence lengths increased. Similar trends were observed in the DPO and RM scenarios, with FlashMask significantly outperforming FA-DenseMask and Vanilla Attention, especially in the RM scenario where higher sparsity levels further enhanced FlashMask's effectiveness. Performance benefits from varying sparsity levels were also quantified, with FlashMask showing linear negative correlation with increasing sparsity, demonstrating efficient utilization of sample sparsity for acceleration. FlashMask's capability to perform sliding window attention was further tested against FA-Window with window sizes of 256, 512, 1024, 2048, 4096, and 8192, as shown in Figure 4 Top. FlashMask matched FA-Window in latency across sequence lengths of 8K, 16K, and 32K, showing comparable delay performances at increasing window sizes.

### End-to-End Experiments

The end-to-end performance1 of the model was tested using synthetic datasets across three scales of the LLaMA2 model and four downstream scenarios (SFT, LoRA, DPO, RM) at various sequence lengths, measuring throughput in average Tokens/Sec/GPU. Each sequence length of 240 valid samples was trained for one epoch, with results presented in Figure 5. In the SFT scenario, FlashMask showed a clear throughput advantage over FA-DenseMask and Vanilla Attention, performing comparably to FA-Varlen. As sequence lengths increased, the throughput advantage of FlashMask over FA-DenseMask and Vanilla Attention also enhanced, even enabling the completion of longer sequence tasks within the same computational resources. In LoRA, DPO, and RM scenarios, FlashMask consistently showed significant advantages. Notably, in the LoRA scenario at the LLaMA2-7B, FlashMask achieved a 4.16x throughput improvement over FA-DenseMask, supporting sequence lengths up to 544K. It's important to note that FA-Varlen was unable to support the DPO and RM scenarios with the answers sharing one question, whereas FlashMask was capable of handling various scenarios including DPO and RM.

Additional experiments were conducted on the open-source dataset LongBench , comparing the end-to-end performance of FA-DenseMask, FA-Varlen, and FlashMask at sequence lengths of 16K, 32K, and 64K. The performance improvements were consistent with those observed in the synthetic dataset. The detailed results are presented in Appendix A.3. Memory usage during the experiments was also recorded, showing significant reductions for FlashMask compared to FA-DenseMask, with detailed results presented in Appendix A.3.

Figure 5: Comparison of End-to-End Training Throughput on Synthetic Dataset.

Discussion

Several key topics emerge that are crucial for comprehending the full scope and implications of FlashMask. These include the rationale behind the design choices, adaptations for supporting bidirectional and other custom masks, and the necessity as well as limits of the current approach.

**Necessity and Scope of the Study.** The substantial advancement rendered by FlashMask in improving attention mask computation is a significant evolution over the current FlashAttention framework. Notably, FlashMask addresses and significantly mitigates the limitations observed with FlashAttention in handling conventional and custom mask computations. This enhancement not only broadens the applicative reach of FlashAttention but also signifies a key shift in efficiency metrics critical for Transformer architectures. More importantly, the flexibility of FlashMask extends beyond the proprietary boundaries of FlashAttention, offering potential benefits to a wider range of Transformer-based models. By facilitating more efficient computation of the attention mechanism, FlashMask enables innovations in processing vast datasets and complex models, thereby improving performances across varied applications in the LLM field. This cross-model adaptability confirms the robustness and utility of FlashMask as a universally applicable enhancement tool within and potentially outside the Transformer architecture spectrum, promising substantial gains in computational efficiency and model scalability.

**Bidirectional and Custom Masks.** In the exploration of attention mechanisms, the introduction of FlashMask as discussed in this study offers a significant leap in computational efficiency, particularly for masking processes in unidirectional attention mechanisms. By extending this approach to bidirectional networks through the simple addition of vectors indicating the start and end indices of the mask, FlashMask transcends conventional computational bounds, casting itself not just as a sparse attention methodology, but as a versatile computational paradigm. Its adaptability across various custom masking tasks and ability to effectively manage diverse types of mask combinations underscores its potential to greatly enhance the efficiency of attention computations. Moreover, the inherent sparsity of the attention mask during inference provides a robust justification for employing FlashMask, indicating its utility and effectiveness in practical applications. This paradigm shift highlights the importance of developing scalable and efficient computational strategies in the evolving landscape of transformer architectures, suggesting that future research should continue to leverage these innovations to tackle increasing computational demands.

**Limitations and Future Directions.** While FlashMask demonstrates impressive performance in handling long-context sequences, it is observed that the computational cost of training Transformers increases more than linearly as the sequence length grows--not only due to the computation of masked attention but also because of the extensive use of other operators. This scenario highlights the inevitable need for leveraging or integrating distributed computing strategies or further algorithmic enhancements to elevate training efficiency. Such advancements could be practical in managing the computationally intensive tasks involved in processing extended contexts efficiently. As a part of future research directions, exploring synergistic solutions that combine the strengths of both algorithmic innovation (like FlashMask) and distributed system designs stands as a promising venture. This approach is anticipated to address scalability challenges and could set the stage for breakthroughs in handling unprecedentedly large data sets and complex model architectures.

## 6 Conclusion

In this paper, we introduced FlashMask, a groundbreaking attention computation paradigm designed to tackle the high computational and memory demands inherent in conventional attention mechanisms in large-scale transformers. By implementing a novel column-wise sparse representation of attention masks, FlashMask substantially reduces the memory and computational complexity from quadratic to linear with the sequence length, thereby enhancing processing speeds and efficiency. Our algorithm demonstrates versatility across various masking scenarios and retains robust performance in different training pipelines. Extensive empirical analysis confirms that FlashMask accelerates computational speed significantly, achieving up to 8.3x speedup in common modalities comparable to state-of-the-art methods like FlashAttention. This advancement marks a significant leap forward in the design of attention computation, offering the potential for broader applications and setting a new benchmark in the efficiency of processing long-context sequences.