# Efficient Contextual LLM Cascades through Budget-Constrained Policy Learning

Xuechen Zhang

University of Michigan

Ann Arbor, MI

zxuechen@umich.edu

&Zijian Huang

University of Michigan

Ann Arbor, MI

zijianh@umich.edu

&Ege Onur Taga

University of Michigan

Ann Arbor, MI

egetaga@umich.edu

Carlee Joe-Wong

Carnegie Mellon University

Pittsburgh, PA

cjoewong@andrew.cmu.edu

&Samet Oymak

University of Michigan

Ann Arbor, MI

oymak@umich.edu

&Jiasi Chen

University of Michigan

Ann Arbor, MI

jiasi@umich.edu

###### Abstract

Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (_i.e._, the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE (Thrifty Reasoning via Context-Aware LLM and Prompt Selection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC) with various LLMs and prompts show that TREACLE enables cost savings of up to 85% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.

## 1 Introduction

The success of large language models (LLMs) in recent years has led to a explosion of heterogeneous models and providers, including as Meta's Llama, OpenAI's ChatGPT, and Google's Gemini. As LLMs continue to proliferate in the near future, we envisage a generative AI marketplace with a large variety of providers, LLMs, and deployments. Notably, LLMs have widely varying capabilities and costs: capabilities in terms of accuracy in responding to different types of queries, and cost in terms of monetary price and query latency. As an illustration, the accuracy versus cost tradeoffs of various Llama and GPT LLMs are shown in Figure 1 on grade school math word problems . As can be seen, GPT-3.5 tends to have lower accuracy than GPT-4

Figure 1: TREACLE chooses LLMs to achieve high accuracy and \(\)85% cost reduction, compared to individual LLMs.

(79% vs 92% respectively), but costs about 20 times less. This heterogeneous array of LLMs can bewilder users who must choose between them.

Another challenge is that the _specific prompt_ included in the question plays a critical role in eliciting accurate responses. This is especially true for reasoning problems where prompting a model to explain its reasoning can produce more accurate, but often more costly, answers. Chain-of-thought (CoT)  is an example of such a prompting scheme, in which the question includes a few examples of worked out problems, which cost more (due to the additional words included in the question) but also produce more accurate responses. For example, in Figure 1, GPT-4 with CoT (pink triangle) achieves a 92% accuracy, compared to GPT-4 with a domain expert prompt (brown dot, reminding the LLM that it is a "math solver") that achieves 83%. However, using the CoT prompt costs 3.9\(\) more due to the extra words included in the query. A final challenge is that the optimal choice of LLM and prompt depends on the _specific question_ being asked; the accuracy of a particular LLM and prompt combination for a particular question is unknown in advance, requiring learning or prediction.

Thus, the heterogeneity of the LLM landscape and the tradeoffs between accuracy and cost make it challenging to determine the optimal strategy of: _Which LLM to select and how to prompt it, in order to answer all questions while respecting cost constraints?_ To address this, we propose a Thrifty Reasoning via Context-Aware LLM and Prompt Selection (TREACLE) framework. TREACLE is a learning-based approach that solves reasoning questions by automatically selecting which LLM model and prompt to use for each question. Given a cost budget, including a total monetary price across all questions and an average per-query latency, its goal is to maximize the average accuracy of the responses. As shown in Figure 1, TREACLE achieves the Pareto front of individual LLMs by combining them intelligently.

Several recent works utilize multiple LLMs during inference with a cascade design, where queries propagate through a cascade of LLMs, considering the LLMs' accuracy-cost tradeoffs. Most aim to maximize accuracy and lack an explicit way to control long-term costs, as TREACLE has. By posing the problem of LLM and prompt selection as a budget-constrained policy optimization, TREACLE provides a unified approach to efficient LLM cascades (see Table 1). TREACLE makes informed decisions based on the full context of the LLM cascade, including the query embedding, answer statistics, and remaining budget. Overall, this paper makes the following contributions:

* **Characterization of the accuracy, monetary cost, and latency of LLMs.** To understand the trade-offs between the LLMs, we quantify the accuracy and cost of 5 different LLMs (Llama and GPT variants) with 3 different prompt strategies (standard, domain expert, and CoT) on 3 datasets (GSM8K, CSQA, and LLC).
* **An adaptive LLM and prompt selection policy based on reinforcement learning.**TREACLE dynamically chooses the right LLM and prompt for each question. It does this by leveraging context about the current question, re-querying the models if needed to verify the consistency of the responses, and thinking ahead about the remaining budget. We also provide some theoretical justification for TREACLE's key design choices.
* **Extensive evaluations.** We show that TREACLE substantially saves on cost while maintaining high accuracy on mathematical and commonsense reasoning tasks. We demonstrate its robustness to different budgets, question difficulty, price changes, new LLMs, and new unseen task types.

The paper is organized as follows. We describe related work (SS2), the problem statement (SS3), and our framework (SS4). We then describe our experiments (SS5) and conclusions (SS6).

## 2 Related Work

FrugalGPT  is perhaps the closest to this work, as they considered a similar cost-constrained LLM selection problem with a threshold-based policy to select from a sorted list of LLMs. Our approach differs in several key aspects: we utilize a reinforcement learning policy that chooses both LLMs and prompts, rather than a threshold-based scheme; we utilize the full context of the current question to make decisions, including the text embedding of the current question and the history of past responses; and our method can _re-query_ the same LLM and aggregate previous responses to estimate the correctness of the current response. Mixture of Thought  explored the idea of response consistency in order to choose the right LLMs. The intuition is that higher consistency in the re-queries implies higher confidence in the correctness of the response. TREACLE employsresponse consistency as an input feature, along with other features, for LLM selection. AutoMix  introduces a "meta-verifier" to estimate whether a response is correct or a more powerful LLM is needed. Both works measure cost as a by-product of combining multiple LLMs rather than long-term constraint across all questions, as we do. Other lines of work include uncertainty estimation or prompt engineering to improve accuracy [6; 18; 20; 13; 1; 9], which is complementary to our work. The related work is summarized in Table 1.

## 3 Problem Statement

We study the natural language query problem of providing correct responses to a series of questions. We focus on reasoning problems (_e.g._, grade school math problems) because they are challenging with multiple logical steps required to reach a final correct response. The problem involves answering a sequence of \(n\) questions \(\) with correct responses \(\); in other words, we have a set of questions and responses \(\{(Q_{1},Y_{1}),(Q_{2},Y_{2}),,(Q_{n},Y_{n})\}\). We have a set \(\) of language models (LLMs) at our disposal, which can be accessed either locally or remotely through APIs: \(=\{M_{1},M_{2},,M_{m}\}\). Also, we have a choice between \(p\) prompt types, \(=\{P_{1},P_{2},,P_{p}\}\). These models and prompts have different costs (in terms of latency and monetary price) and accuracy. Each "question" can be asked multiple times to the same or different LLMs, which we call a "re-query", in order to possibly obtain a more final accurate response.

The goal is to ensure that as many responses as possible are correct, while simultaneously minimizing the associated costs. This problem can be formulated as a Constrained Markov Decision Process (CMDP), which is represented by a tuple \((,,,T,r,c,,B)\), where \(\) is the ordered question set; \(\) is the state space; \(T:(,)(,) \) is the transition probability function, _i.e._, \((Q,s)_{t+1} T(|(Q,s)_{t},a_{t})\); \(r:(,)[R_{},R_{ }]\) and \(c:(,)^{+}\) denote the reward and cost function; \(\) is the discount factor for future reward and cost; and \(B\) is the total budget. A policy \(:(,)()\) maps the question-state pairs to a probability distribution over actions. A trajectory \(\) is composed of a sequence of (question, state)-action pairs: \(=\{_{(Q,s)_{0}},_{a_{0}},_{(Q,s)_{1}},_{a_{1}},..., _{(Q,s)_{L}},_{a_{L}}\}\), where \(L\) is the total number of times the LLMs are queried. Note that \(L n\), due to the possible re-queries. The cumulative reward and cumulative cost of trajectory \(\) are denoted as \(R()=_{t=0}^{L}^{t}r(_{(Q,s)_{t}},_{a_{t}})\) and \(C()=_{t=0}^{L}^{t}c(_{(Q,s)_{t}},_{a_{t}})\), respectively. The goal of our problem is to learn a policy \(\) from \(\) that maximizes the expected cumulative reward, while satisfying the cumulative cost at the trajectory level:

\[_{}_{,T}[R()],,T C() B.\] (1)

where \(,T\) denotes that \(\) is generated by executing \(\) in \(T\). By grouping the rewards and costs by question instead of enumerating all re-queries, the cumulative reward and cost can be re-written as \(R()=_{i=1}^{n}(Y_{i},_{i})\) and \(C()=_{i=1}^{n}(Q_{i})\), where \(_{i}\) is the final response for question \(Q_{i}\), \(()\) is the function that measures the correctness of the final response, \(()\) is the cost function of giving a final response \(_{i}\) for question \(Q_{i}\). **Cost functions.** We consider two types of costs in this work, monetary price and latency, resulting in two types of cost functions. _(1) Pure monetary price._ LLMs can run remotely, where the monetary price per token is set by the provider (_e.g._, OpenAI). LLMs can also run locally, where the monetary price depends on a number of factors such as capital expenditures, server cooling and maintenance, electricity, etc. In our setup, the GPT models run remotely and the Llam models, which are free and open-source, run locally. _(2) Monetary price-latency combination._ Monetary price is important for some users (_e.g._, small companies) while latency plays a more crucial role in other settings (_e.g._, real-time voice assistants). Users who are latency-sensitive may be willing to pay more for lower latency, whereas others might be more patient and prefer lower prices. TREACLE allows users to choose the trade-off between monetary cost and latency by adjusting a trade-off coefficient \(\), where \(=+*\).

   & Query embedding & Response consistency & Prompt _and_ LLM selection & Long-term budget & Robust to new models \\ 
**FrugiGPT** & ✓ & ✗ & ✗ & ✗ & ✗ \\
**AutoMix** & ✓ & ✓ & ✗ & ✗ & ✗ \\
**MoT** & ✗ & ✓ & ✗ & ✗ & ✗ \\
**TREACLE** & ✓ & ✓ & ✓ & ✓ \\  

Table 1: Comparison to related works.

## 4 Proposed Framework: TREACLE

We propose the TREACLE framework, depicted in Figure 2. Let the possible unique combinations of language models and prompts be denoted by \(\{(M+P)_{1},(M+P)_{2},,(M+P)_{K}\}\), where \(K mp\). When a new question \(Q_{i}\) arrives, TREACLE starts by selecting a model \(M\) and choosing an associated prompt \(P\) to generate a response, denoted as \(_{i}=M(P(Q_{i}))\). TREACLE returns this as the final response \(_{i}\) for this question if it has a high degree of confidence in its correctness, and deducts the cost of the question and its response from the total budget \(B\). Otherwise, TREACLE can select another LLM \(M\) and prompt \(P\) (whose choice may be informed by the result of all previously chosen models, prompts, and their responses) and re-query. This iterative process continues until TREACLE returns a final response (based on its learned policy). TREACLE then proceeds to the next question with the remaining budget and repeats the process, until all questions have been answered or there is no remaining budget. We model the problem as a Markov decision process as described in Section 3.

**States.** The state vector contains the following information:

* **Response consistency:** Records all previous responses and the normalized frequency of their occurrences. The intuition is that the consistency of the previous responses can be used as a measure of confidence in the response correctness [16; 7].
* **Input and output length:** The number of tokens in the current query and any preceding responses to the same query. This helps TREACLE understand the monetary price of each query and response, which can differ for each query. It also helps capture the difficulty, as question with longer input or output tend to be harder.
* **Current question's text embedding:** Intuitively, we want to capture the question type or difficulty, which can impact the model and prompt selection decision. TREACLE does this using a text embedding of the query .
* **Number of re-queries:** The number of re-queries for each model-prompt pair helps TREACLE decide whether to re-query again or move to the next question.
* **Normalized remaining budget:** Based on the remaining budget, we compute the estimated number of queries for each model prompt pair as follows: \(_{k}=}{(($})}\). The average cost per query is estimated based on the questions seen so far. If there is a large remaining budget, TREACLE may consider re-querying with large models.

**Actions.** The action space \(\) consists of the following:

* Action \(a_{1}\): Return the current response for \(Q_{i}\) and proceed to the next question \(Q_{i+1}\). If no models have been queried yet and this action is chosen, it is equivalent to skipping the question.

Figure 2: Overview of TREACLE framework. TREACLE decides on the next (LLM, prompt) to query in a context-aware fashion, summarized in the state variable. It can adapt to unseen tasks by projecting the new queries into the text embedding space.

* Action \(a_{2}\): Re-query the same model-prompt pair \((M+P)\) for \(Q_{i}\).
* Action \(a_{3}\): Select a new model-prompt pair \((M+P)^{}\) for \(Q_{i}\).

By allowing re-querying (action \(a_{2}\)), the current action influences the next state, by impacting the question under consideration and thus the relevant state features, making this a non-trivial MDP. For \(a_{3}\), we constrained the set of possible model-prompt pairs to a sorted list. In other words, instead of allowing TREACLE to select any possible model and prompting scheme, we sort the \((M+P)_{k}\) in ascending order of accuracy to cost ratio and only allow TREACLE to select the next element in this list \((M+P)_{k+1}\). The ordering is based on Proposition 1 (discussed below),

**Rewards.** The reward function assigns a positive reward to correct responses. Specifically, \(r_{_{a}}(_{(Q,s)},_{(Q^{},s^{})})=[=Y|_{a}=a_{1}]+[=Y| _{a}\{a_{1},a_{2},a_{3}\}]\). For a given question, this combines the accuracy of the final response \(\) with the accuracy of the current response \(\) (if there have been re-queries), with a scaling factor \(\) between the two terms. We introduced the second term because without it, we observed that if TREACLE repeatedly chose action \(a_{2}\) (re-querying), this would result in multiple state transitions with 0 reward, until the final response was returned. In other words, including the second term avoids the issue of sparse rewards that resulted from the first term alone. Note that the correct response \(Y\) is known only when training TREACLE; during test, the policy executes using the expected reward calculated by the trained policy.

**Design choices and justifications.** We next discuss two key design choices of TREACLE and their theoretical motivation. Proofs are in Appendix B.

_(1) How should the LLMs and prompts be ordered in the cascade?_ Recall that action \(a_{3}\) moves to the next \((M+P)\) in the cascade. What is the best ordering of \((M+P)\)? Consider the following simplified setting. Suppose each of the \((M+P)\) have a probability of correct response \(p_{k}\) and cost \(c_{k}\). If we had access to an oracle that could tell us when the response of a particular \((M+P)_{k}\) is incorrect, we could then move on and try the same question with the next option \((M+P)_{k+1}\) in the cascade. We could achieve the highest accuracy using the oracle, and would only have to worry about minimizing the cost to avoid exceeding the budget. In this setting, Proposition 1 below states that the best ordering of the \((M+P)\) options is according to the ratio \(}{c_{k}}\).

**Proposition 1**.: With \(K\) (LLM, prompt) options, each with probability of correct answer \(p_{k}\) and cost \(c_{k}\), ordering the options according to their cost-normalized accuracies \(}{c_{k}}\) minimizes the total cost.

This Proposition motivates TREACLE's ordering of the \((M+P)\) options in the cascade according to their accuracy and cost ratio. This is intuitive: instead of placing the most accurate (LLM, prompt) option early in the cascade, which might incur large cost, we first query LLMs that have high accuracy per unit cost. Note that although the setup of Proposition 1 differs from Equation (1), as the cost is the objective rather than a constraint, the trajectory resulting from the ordering in Proposition 1 is also a solution to Equation (1).

_(2) Do policies that consider response consistency perform well?_ Recall that "response consistency" is one of the features in the state vector. We seek to understand the performance of policies that consider this feature; a simple such policy is described in Definition 1 below. It returns a final response to a question if the same response value is repeated \(w\) times.

**Definition 1**.: For each question \(Q_{i}\), an \(w\)-consistent policy (\(w 2\)) sets the final response \(_{i}=_{i}\) as soon as \(\ _{i}:(_{i})=w\). If no such \(_{i}\) exists, fall back to \(w-1,w-2\), etc.

Definition 2 below characterizes how likely the \((M+P)\) are to return an incorrect response. A question can be asked \(\) times to the \((M+P)\) options in the cascade, which may not be unique due to the re-queries.

**Definition 2**.: Denote the \(\) LLM-prompt options by \((M+P)_{j=1}^{}\). Let \((M_{j}(P_{j}(Q_{i})))\) be the output distribution of \((M+P)_{j}\) on problem \(Q_{i}\). Let \(:=_{i=1}^{n}_{1 j}_{ Y_{i}}(M_{j}(P_{j}(Q_{i}))=)^{2}\). With the definitions in hand, we can now lower bound the performance of a 2-consistent policy compared to the optimal learned algorithm in Proposition 2 below. Without loss of generality, we study the case when the reward function is the accuracy.

**Proposition 2**.: For the problem stated in Equation (1) that achieves \(C_{*}\), the optimal expected accuracy subject to budget constraints, there exists a 2-consistent policy that achieves an accuracy of at least \(C_{*}-^{2}\).

In other words, even a simple policy that allows for re-querying and considers response consistency can achieve close to the optimal reward. This motivates TREACLE inclusion of "response consistency" as a state feature. The proposition applies generally and allows for budgets or text embeddings, and does not require the \((M+P)\) to return accurate responses. Experimentally, we find that our learned RL policy is similar to a 2-consistent policy, as 93.02% of the responses are 2-consistent (GSM8K dataset, $0.30 budget, \(=\)). This suggests that our learned policy may not be far from optimal.

## 5 Experiments

We first describe the experiment setup (SS5.1) and then the main results (SS5.2). Specifically, we examine robustness to new LLMs and changing API prices (SS5.2.1), shifts in question difficulty (SS5.2.2), and different reasoning datasets (SS5.2.3).

### Experiment Setup

We summarize the experiment setup, with full details in Appendix A. We use three representative datasets: **GSM8K **, which contains 8.5K high quality grade school math problems created by human writers; **CSQA **, which consists of 12102 multiple choice commonsense reasoning questions encountered in daily life; and **LLC **, where the task is to concatenate the last letters of words in a name (e.g., "Amy Brown" \(\) "yn"). To evaluate our methods, we perform two steps.

_(1) Collect query-response pairs for (LLM, prompt) combinations_. We collected query-response pairs from each dataset for different combinations of LLM, prompt, and LLM temperature. We used 5 different LLMs: **Llama-2-7b-chat**, **Llama-2-13b-chat**, **GPT-3.5-turbo**, **GPT-4**, and **GPT-4-turbo**. These models are of varying sizes (7b, 13b, 154b and 1.76t respectively). The Llama models are open-source and run locally on our servers, while the GPT models rely on commercial APIs. We employ several prompting schemes. A prompt generally consists of two parts: the "content message" containing the question, and the "system message" with additional context.

* The **plain text prompt** submits the questions to the LLM as the content message.
* The **domain expert prompt** feeds information about the question's domain as a system message (_e.g._, "math solver"), and keeping the user's content message as plain text.
* The **standard few-shot prompt** includes a system message ("Follow the given examples and answer the question" ) and the content message, which consists of few-shot examples together with the plain text prompt.
* The **Chain-of-Thought (CoT) few-shot prompt** adds some intermediate explanations to the few-shot examples.

_(2) Train_TREACLE _with the query-response pairs_. We used Deep Q-Network (DQN)  to train the reinforcement learning (RL) policy in TREACLE, consisting of a two-layer neural network. For the monetary prices, we use the published per-token prices for the GPT models. Since our local Llama deployments do not have API costs, we set Llama-2-7b's price as \(\) times Llama-2-13b's price, and Llama-2-13b's price as \(\) times GPT-3.5-turbo's price. \(\) varies between \(,\) or \(\). Our pricing is grounded in reality and similar to actual market rates, as the offered price for Llama is approximately 15% of GPT-3.5-turbo according to current providers . For the latency-accuracy tradeoff, we evaluate different trade-off parameters \(=[50,500,1]\) in the cost function.

We evaluated the following baseline methods, reproducing the methods as faithfully as possible with a common set of LLMs and prompt options.

* **FrugalGPT**. We reproduce FrugalGPT, which uses a DistilBERT model  to estimate the response accuracy. If this estimate is below a threshold, the next LLM in the cascade is queried. This baseline shows how TREACLE compares to the state-of-the-art that lacks re-querying.
* **Calibrated cascade.** We build on FrugalGPT's response accuracy estimation and develop a 2-layer neural network, which takes as input TREACLE's state vector and outputs the estimated response accuracy. If this estimated accuracy is below a threshold, the next LLM in the cascade is queried. This baseline compares TREACLE to a modified FrugalGPT.
* **Majority Voting**. For each query, we output the final response based on the majority vote from \(\) re-queries, based on [16; 20]. We set \(N=2\) based on the best empirical results. The (LLM,prompt) combinations are progressively queried until their per-question budget runs out. This baseline allows comparison with TREACLE's response consistency feature in the state vector.
* **Offline and online knapsack**. We formulate a multiple choice knapsack problem where the items are the \((M+P)\) combinations. We solve the offline knapsack to find the optimal solution when re-queries are not allowed, and also implement an online version . These baselines show how TREACLE compares to methods with perfect knowledge of question costs and accuracy.

### Results

To evaluate the performance of TREACLE, we conduct experiments for different total budgets and cost functions. The results are presented in Figure 3 (additional results on CSQA and LLC are in Appendix C.4.) Across different settings of \(,\), and total budget, TREACLE consistently outperforms the baselines and is close to the Offline Knapsack- an approach not feasible in practical deployments. We note that the relatively good performance of the Calibrated Cascade is due to it using the same state vector we designed for TREACLE. We make the following additional observations.

\(\)_Observation 1:_TREACLE _can adapt to different budgets and cost parameters._ All the results in Figure 3, with different budgets and \(,\) parameters were produced after training TREACLE only once. This highlights TREACLE's adaptability to different cost function variations.

\(\)_Observation 2: For limited budgets,_TREACLE _only answers questions that are more likely to produce accurate responses._ For example, for \(=50\)k in Figure 3, when the budget is only \(\$0.05\) and insufficient for all queries, 52.7% of the questions TREACLE chooses to answer are correct. For context, the cheapest model (Llama-2-7b) can only answer 23.65% of questions correctly. This suggests TREACLE can evaluate question difficulty and opt not to respond to some questions.

\(\)_Observation 3: For larger budgets,_TREACLE _chooses more powerful (LLM, prompt) combinations._ This is shown in Figure 3(a), where as the budget increases, the more powerful models (right side of x-axis) are increasingly selected. Interestingly, we observe that for budgets $0.3 to $10, the Llama-2-13b model is queried approximately once per question, despite its suboptimal performance. Even with these larger budgets, it's still beneficial to query Llama before moving onto more powerful models, to see whether its responses are consistent.

Figure 3: The performance of various methods for different cost functions and budget constraints. The dashed lines are methods that have ground knowledge, which is impractical but illustrates the best achievable performance.

\(\)_Observation 4: Re-querying helps._ We trained both TREACLE and the Calibrated Cascade Algorithm baseline without the ability to re-query. The results are shown in Figure 5, where the dashed line represents method variants that permits re-querying. We observed a notable decrease in accuracy when re-querying was not allowed. Methods without re-querying eventually achieved comparable accuracy with those with re-querying capability, but with significantly larger budgets.

\(\)_Observation 5:_ TREACLE_'s choice of model and prompt is impacted by relative LLM prices._ As the relative cost of Llama models decreases (\(\) decreases), TREACLE increasingly utilizes Llama to answer queries, allowing for cost savings, as shown in Figure 3(b). This shift enables use of more expensive models like GPT-4 when tackling complex problems, thereby enhancing overall accuracy. When Llama becomes more expensive, TREACLE no longer chooses it. This aligns with our intuition that using Llama to verify response consistency becomes less economical.

#### 5.2.1 Addition of new LLMs

LLM development is rapid, with better models continuously emerging, and the API prices set by providers can change at any time. TREACLE's ability to react to such changes is thus an important practical consideration. We show that TREACLE can adapt by fine-tuning itself using few samples. We study two types of LLM updates. (1) _API price adjustment:_ In November 2023, OpenAI released GPT-4-turbo, offering performance on par with GPT-4 but at a more affordable price. Concurrently, the price for GPT-3.5-turbo was lowered. (2) _Fine-tuned open-source LLMs:_ Several domain-specific fine-tuned models with higher accuracy have been released. Specifically, we exchanged Llama-2 for Meta-Math , which is fine-tuned specifically for GSM8K. For both scenarios, we partitioned the GSM8K test data into 80% validation and 20% test samples, generated new state-action trajectories from the validation set, then fine-tuned TREACLE on these new trajectories. To create a comparable baseline, we also fine-tuned FrugalGPT's DistilBERT.

Firstly, we show the performance of TREACLE with both the API price adjustments and improved LLMs in Figure 6. The individual points on the plot illustrate the changes in the API prices for gpt-3.5-turbo. The lines show the performance of the new TREACLE with new models and prices and the old TREACLE (from previous subsections). The new TREACLE can achieve the peak accuracy with only a $1 budget, clearly benefiting from the new models and lowered

Figure 4: Number of times each model is re-queried.

Figure 5: With and without re-querying. \(=\).

Figure 6: Performance with new LLMs and lowered prices. Lines and dots in light (dark) colors are results with old (new) prices and LLMs. \(=\).

prices. Benefits are also significant for lower budgets, where the improved TREACLE has significantly higher accuracy, because the lowest performing Llama-2 models were replaced by fine-tuned Metamaths. Finally, for FrugalGPT that relies on a fine-tuned DistilBERT accuracy estimator, performance didn't improve and can even degrade due to distribution shifts and overfitting.

Secondly, in Figure 7 we investigate the sample efficiency of fine-tuning the model with new API prices and LLMs ("Fine-tune" in the figure) compared to training TREACLE from scratch with the new prices and LLMs ("Scratch"). The sample efficiency is important because it can be expensive to collect query-response pairs from new LLMs to further train TREACLE. The results indicate that when there are minor changes to the available LLMs, deploying the previously trained TREACLE can be sufficient. For instance, in Figure 6(a) when there is limited budget ($0.15) and upgrades to the expensive models, deploying the previously trained TREACLE (# samples = 0) achieves comparable performance to the fine-tuning TREACLE (# samples = 800). On the other hand, when upgrades are introduced to cheaper models (Figure 6(b)), deploying the old TREACLE may initially result in poor accuracy, but TREACLE can quickly adapt to the new LLM options by fine-tuning with a few number of samples (around 300).

#### 5.2.2 Shifts in Question Difficulty

Thus far in the evaluations, easier and harder questions were randomly mixed throughout the training and test sets. In practice, question difficulty may not be uniformly distributed, so we study two types of difficulty distribution shifts: shifts across the training/test sets, and towards the end of the test set.

**Difficulty shifts between training and test.** We divided the GSM8K test set into "hard" and "easy" subsets based on the question difficulty. The difficulty is defined by the number of LLM models correctly answering the questions (more models answering a question correctly roughly means it is easier). Basic performance on the easy and hard questions is shown in Figure 7(c). When the questions are hard, each question ends up consuming too much budget, leaving insufficient budget for subsequent questions that then go unanswered. The single model baseline does well in terms of cost and unanswered questions, but has low accuracy. We plot the performance for variable budgets in Figure 8, and find that TREACLE's accuracy remains stable, no matter whether the test distribution shifts to an easier level or a harder level. This is because TREACLE can dynamically adjust based on the remaining budget in online fashion.

**Difficulty shifts within the test set.** To further evaluate the robustness to question difficulty shifts, we test TREACLE with the full test set sorted from easy-to-hard queries or hard-to-easy queries. The hope is that with the help of query text embedding in the state vector (which should capture some estimate of difficulty), TREACLE can remain relatively stable in terms of accuracy even if the ordering of the questions changes. This hypothesis is borne out in Figure 9, while Online Knapsack performs significantly worse than TREACLE if the questions are sorted from hard to easy. This is because much of the budget is wasted on the difficult queries that arrive at the beginning.

Figure 8: Performance on “easy” and “hard” partitions of the test set. Models are trained on original training data, but must handle a distribution shift in difficulty during test. \(=\).

Figure 7: Sample complexity for different budgets with new LLMs. \(=\).

#### 5.2.3 Different types of reasoning tasks

**Mixture of tasks.** We seek to examine whether one model can handle multiple types of tasks under one common budget (in contrast to the previous experiments with a specialized model for each task). We trained a single model with all 3 datasets and recorded the test accuracy on those datasets. The results shown in Figure 9(a) for "TREACLE (all tasks)", offline knapsack, and online knapsack are the test accuracy from an equal mix of CSQA, GSM8K and LLC queries. "TREACLE (individual tasks)" is the test accuracy on the same mix of queries, using the models from previous subsections, where each model (corresponding to a task) is assigned to 1/3 of the common budget. "TREACLE (all tasks)" can handle a mixture of tasks under a common budget (, outperforming online knapsack), and can significantly outperform the individual tasks baseline ("TREACLE (individual tasks)") by effectively allocating its common budget across queries of different types.

**New unseen task type.** Consider the scenario where the model has not been trained on certain new tasks. To show that \(\) can adapt to new tasks easily, we performed additional experiments. The base model is trained using the CSQA dataset, and the unseen new tasks are queries from GSM8K. Interestingly, in our design, we decouple decision making from the task embedding as follows. To transfer from CSQA to GSM8K, we freeze the base RL policy of CSQA (the decision making part), and fine-tune the "text embedding" feature in the state vector (see bottom pink part of Figure 2). How many samples are needed to fine-tune the text embedding for the new task? As shown in Figure 9(b), with a budget of 0.6, the original model fully-trained on GSM8K ("train on GSM8K") achieves a test accuracy of 0.848, compared to 0.78 when trained on CSQA and fine-tuned with only 200 additional samples from GSM8K ("fine-tune on 200 GSM8K"). This highlights a relatively small accuracy loss when transferring to new types of unseen tasks. The results suggest that our method can easily adapt to new tasks with only a small amount of additional training.

## 6 Conclusions

We propose \(\), a learning-based LLM querying framework that intelligently chooses between LLM and prompt combinations based on question context and past response history. Our experiments show that \(\) outperforms other baselines and is robust to different budgets, LLM availability and prices, and so on. For future work, we plan to incorporate other features such as privacy into the cost function. We hope our framework can help spur cost-efficient utilization of LLM systems.

**Limitations.** Our work focuses on reasoning problems, and could be extended to generative problems by incorporating new measures of response consistency. The RL policy's budget does not account for the cost of collecting the training data. We plan to freely release the datasets and code so that others can train the same basic policy and adapt that policy to new future LLMs and tasks (as shown through our experiments).

**Broader impact.** This work can make LLMs more accessible to cost-sensitive users.