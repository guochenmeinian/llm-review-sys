# Differentiable Clustering with

Perturbed Spanning Forests

Lawrence Stewart

ENS & INRIA

Paris, France

&Francis Bach

ENS & INRIA

Paris, France

&Felipe Llinares-Lopez

Google DeepMind

Paris, France

&Quentin Berthet

Google DeepMind

Paris, France

All correspondence should be addressed to lawrence.stewart@ens.fr.Code base: https://github.com/Lawrence@MS Stewart/DiffClust_NeurIPS2023

###### Abstract

We introduce a differentiable clustering method based on stochastic perturbations of minimum-weight spanning forests. This allows us to include clustering in end-to-end trainable pipelines, with efficient gradients. We show that our method performs well even in difficult settings, such as data sets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several data sets for supervised and semi-supervised tasks.2.

## 1 Introduction

Clustering is one of the most classical tasks in data processing, and one of the fundamental methods in unsupervised learning (Hastie et al., 2009). In most formulations, the problem consists in partitioning a collection of \(n\) elements into \(k\) clusters, in a manner that optimizes some criterion, such as intra-cluster proximity, or some resemblance criterion, such as a pairwise similarity matrix. This procedure is naturally related to other tasks in machine learning, either by using these induced classes in supervised problems, or by either evaluating or looking for well-clustered representations (Caron et al., 2018; Xie et al., 2016). Its performance and flexibility on a wide range of natural dataset, that makes it a good downstream or preprocessing task, also make it a a very important candidate to learn representations in a supervised fashion. Yet, it is a fundamentally "combinatorial" problem, representing a discrete decision, much like many classical algorithmic methods (e.g., sorting, taking nearest neighbours, dynamic programming).

For these reasons, it is extremely challenging to _learn through clustering_. As a function, the solution of a clustering problem is piece-wise constant with respect to its inputs (such as a similarity or distance matrix), and its gradient would therefore be zero almost everywhere. This operation is therefore naturally ill-suited to the use of gradient-based approaches to minimize an objective, which are at the center of optimization procedures for machine learning. It does not have the convenient properties of commonly used operations in end-to-end differentiable systems, such as smoothness and differentiability. Another challenge of using clustering as part of a learning pipeline is perhaps its _ambiguity_: even for a given notion of distance or similarity between elements, there are several valid definitions of clustering, with criteria adapted to different uses. Popular methods include \(k\)-means, whose formulation is NP-hard in general even in simple settings (Orineas et al., 2004), and relies on heuristics that depend heavily on their initialization (Arthur and Vassilvitskii, 2007; Bubeck et al., 2012). Several of them rely on proximity to a _centroid_, or _prototype_, i.e., a vector representing each cluster. These fail on challenging geometries, e.g., interlaced clusters with no linear separation.

We propose a new method for differentiable clustering that efficiently addresses these difficulties. It is a principled, deterministic operation: it is based on minimum-weight spanning forests, a variantof minimum spanning trees. We chose this primitive at the heart of our method because it can be represented as a linear program (LP), and this is particularly well-adapted to our smoothing technique. However, we use a greedy algorithm to solve it exactly and efficiently (rather than solving it as an LP or relying on an uncertain heuristic). We observe that this method, often referred to as single linkage clustering (Gower and Ross, 1969), is effective as a clustering method in several challenging settings. Further, we are able to create a differentiable version of this operation, by introducing stochastic perturbations on the similarity matrix (the cost of the LP). This proxy has several convenient properties: it approximates the original function, it is smooth (both of these are controlled by a temperature parameter), and both the perturbed optimizers and its derivatives can be efficiently estimated with Monte-Carlo estimators (see, e.g., Hazan et al., 2016; Berthet et al., 2020, and references therein). This allows us to include this operation in end-to-end differentiable machine learning pipelines, and we show that this method is both efficient and performs well at capturing the clustered aspect of natural data, in several tasks.

Our work is part of an effort to include unconventional operations in model training loops based on gradient computations. These include discrete operators such as optimal transport, dynamic time-warping and other dynamic programs (Cuturi, 2013; Cuturi and Blondel, 2017; Mensch and Blondel, 2018; Blondel et al., 2020; Vlastelica et al., 2019; Paulus et al., 2020; Sander et al., 2023; Shvetsova et al., 2023), to ease their inclusion in end-to-end differentiable pipelines that can be trained with first-order methods in fields such as computer vision, audio processing, biology, and physical simulators (Cordonnier et al., 2021; Kumar et al., 2021; Carr et al., 2021; Le Lidec et al., 2021; Baid et al., 2023; Llinares-Lopez et al., 2023) and other optimization algorithms (Dubois-Taine et al., 2022). Other related methods, including some based on convex relaxations and on optimal transport, aim to minimize an objective involving a neural network in order to perform clustering, with centroids (Caron et al., 2020; Genevay et al., 2019; Xie et al., 2016). Most of them involve optimization _in order to_ cluster, without explicitly optimizing _through_ the clustering operation, allowing to learn using some supervision. Another line of research is focused on using the matrix tree-theorem to compute marginals of the Gibbs distribution over spanning trees (Koo et al., 2007; Zmigrod et al., 2021). It is related to our perturbation smoothing technique (which yields a different distribution), and can also be used to learn using tree supervision. The main difference is that it does not address the clustering problem, and that optimizing in this setting involves sophisticated linear algebra computations based on determinants, which is not as efficient and convenient to implement as our method based on sampling and greedy algorithms.

There is an important conceptual difference with the methodology described by Berthet et al. (2020), and other recent works based on it that use _Fenchel-Young losses_(Blondel et al., 2020): while one of the core discrete operations on which our clustering procedure is based is an LP, the cluster connectivity matrix-which, importantly, has the same structure as any ground-truth label information- is not. This weak supervision is a natural obstacle that we have to overcome: on the one hand, it is reasonable to expect clustering data to come in this form, i.e., only stating whether some elements belong to the same cluster or not, which is weaker than ground-truth spanning forest information would be. On the other hand, linear problems on cluster connectivity matrices, such as MAXCUT, are notoriously NP-hard (Karp, 1972), so we cannot use perturbed linear oracles over these sets (at least exactly). In order to handle these two situations together, we design a _partial Fenchel-Young loss_, inspired by the literature on weak supervision, that allows us to use a loss designed for spanning forest structured prediction, even though we have the less informative cluster connectivity information. In our experiments, we show that our method enables us to learn through a clustering operation, i.e., that we can find representations of the data for which clustering of the data will match with some ground-truth clustering data. We apply this to both supervised settings, including some illustrations on synthetic challenging data, and semi-supervised settings on real data, focusing on settings with a low number of labeled instances and unlabeled classes.

Main Contributions.In this work, we introduce an efficient and principled technique for differentiable clustering. In summary, we make the following contributions:

* Our method is based on using spanning forests, and a **differentiable** proxy, obtained by adding **stochastic perturbations** on the edge costs.
* Our method allows us to **learn through clustering**: one can train a model to learn 'clusterable' representations of the data in an **online** fashion. The model generating the representations is informed by gradients that are transmitted through our clustering operation.

* We derive a partial Fenchel-Young loss, which allows us to incorporate **weak cluster information**, and can be used in any weakly supervised structured prediction problem.
* We show that it is a powerful clustering technique, allowing us to learn meaningful clustered representations. It **does not require** a model to learn linearly separable representations.
* Our operation can be incorporated efficiently into any gradient-based ML pipeline. We demonstrate this in the context of **supervised** and **semi-supervised cluster learning tasks**.

Notations.For a positive integer \(n\), we denote by \([n]\) the set \(\{1,,n\}\). We consider all graphs to be undirected. For the complete graph with \(n\) vertices \(K_{n}\) over nodes \([n]\), we denote by \(\) the set of _spanning trees_ on \(K_{n}\), i.e., subgraphs with no cycles and one connected component. For any positive integer \(k n\) we also denote by \(_{k}\) the set of \(k\)-_spanning forests_ of \(K_{n}\), i.e., subgraphs with no cycles and \(k\) connected components. With a slight abuse of notation, we also refer to \(\) and \(_{k}\) for the set of _adjacency matrices_ of these graphs. For two nodes \(i\) and \(j\) in a general graph, we write \(i j\) if the two nodes are in the same connected component. We denote by \(_{n}\) the set of \(n n\) symmetric matrices.

## 2 Differentiable clustering

We provide a **differentiable** operator for clustering elements based on a similarity matrix. Our method is **explicit**: it relies on a linear programming primitive, not on a heuristic to solve another problem (e.g., \(k\)-means). It is **label blind**: our solution is represented as a connectivity matrix, and is not affected by a permutation of the clusters. Finally it is **geometrically flexible**: it is based on single linkage, and does not rely on proximity to a centroid, or linear separability to include several elements in the same cluster (as illustrated in Section 4).

In order to cluster \(n\) elements in \(k\) clusters, we define a clustering operator as a function \(M_{k}^{*}()\) taking an \(n n\) symmetric similarity matrix as input (e.g., negative pairwise square distances) and outputting a cluster connectivity matrix of the same size (see Definition 1). We also introduce its differentiable proxy \(M_{k,}^{*}()\). We use them as a method to **learn through clustering**. As an example, in the supervised learning setting, we consider the \(n\) elements described by features \(=(X_{1},,X_{n})\) in some feature space \(\), and ground-truth clustering data as an \(n n\) matrix \(M_{}\) (with either complete or partial information, see Definition 4). A parameterized model produces a similarity matrix \(=_{w}()\)-e.g., based on pairwise square distances between embeddings \(_{w}(X_{i})\) for some model \(_{w}\). Minimizing in \(w\) a loss so that \(M_{k}^{*}(=_{w}()) M_{}\) allows to train a model based on the clustering information (see Figure 1). We describe in this section and the following one the tools that we introduce to achieve this goal, and show in Section 4 experimental results on real data.

Figure 1: Our pipeline, in the semi-supervised learning setting: data points are embedded by a parameterized model, which produces a similarity matrix. Partial clustering information may be available, in the form of must-link or must-not-link constraints. Our clustering and differentiable clustering operators are used, respectively for prediction and gradient computations.

### Clustering with \(k\)-spanning forests

In this work, the core operation is to cluster \(n\) elements, using a similarity matrix \(_{n}\). Informally, pairs of elements \((i,j)\) with high similarity \(_{ij}\) should be more likely to be in the same cluster than those with low similarity. The clustering is represented in the following manner.

**Definition 1** (Cluster connectivity matrix).: _Let \(:[n][k]\) be a clustering function, assigning elements to one of \(k\) clusters. We represent it with an \(n n\) binary matrix \(M\) (the cluster connectivity):_

\[M_{ij}=1(i)=(j)\,.\]

_We denote by \(_{k}\{0,1\}^{n n}\) the set of binary cluster connectivity matrices with \(k\) clusters._

Using this definition, we define an operation \(M_{k}^{*}()\) mapping a similarity matrix \(\) to a clustering, in the form of such a membership matrix. Up to a permutation (i.e., naming the clusters), \(M\) allows to recover \(\) entirely. It is based on a maximum spanning forest primitive \(A_{k}^{*}()\), and both are defined below. We recall that a \(k\)-forest on a graph is a subgraph with no cycles consisting in \(k\) connected components, potentially single nodes (see Figure 2). The cluster connectivity matrix is sometimes referred to as the cluster coincidence matrix matrix in other literature, and the two terms can be used interchangeably.

**Definition 2** (Maximum \(k\)-spanning forest).: _Let \(\) be an \(n n\) similarity matrix. We denote by \(A_{k}^{*}()\) the adjacency matrix of the \(k\)-spanning forest with maximum similarity, defined as_

\[A_{k}^{*}()=*{argmax}_{A_{k}} A, .\]

_This defines a mapping \(A_{k}^{*}:_{n}_{k}\). We denote by \(F_{k}\) the value of this maximum, i.e.,_

\[F_{k}()=_{A_{k}} A,.\]

**Definition 3** (Spanning forest clustering).: _Let \(A\) be the adjacency matrix of a \(k\)-spanning forest. We denote by \(M^{*}(A)\) the connectivity matrix of the \(k\) connected components of \(A\), i.e.,_

\[M_{ij}=1 i j\,.\]

_Given an \(n n\) similarity matrix \(\), we denote by \(M_{k}^{*}()_{k}\) the clustering induced by the maximum \(k\)-spanning forest, defined by_

\[M_{k}^{*}()=M^{*}(A_{k}^{*}())\,.\]

_This defines a mapping \(M_{k}^{*}:_{n}_{k}\), our clustering operator._

Remarks.The solution of the linear program is unique almost everywhere for similarity matrices (relevant for us, as we use stochastic perturbations in learning). Both these operators can be computed using Kruskal's algorithm to find a minimum spanning tree in a graph (Kruskal, 1956). This algorithm builds the tree by iteratively adding edges in a greedy fashion, maintaining non-cycliticity. On a connected graph on \(n\) nodes, after \(n-1\) edge additions, a spanning tree (i.e., that covers all nodes) is obtained. The greedy algorithm is optimal for this problem, which can be proved by showing that forests can be seen as the independent sets of a _matroid_(Cormen et al., 2022). Further, stopping the algorithm after \(n-k\) edge additions yields a forest consisting of \(k\) trees (possibly singletons), together spanning the \(n\) nodes and which is optimal for the problem in Definition 2. As in several other clustering methods, we specify in ours the number of desired clusters \(k\), but a consequence of our algorithmic choice is that one can compute the clustering operator _for all \(k\)_ without much computational overhead, and that this number can easily be tuned by validation.

Further, a common manner to run this algorithm is to keep track of the constructed connected components, therefore both \(A_{k}^{*}()\) and \(M_{k}^{*}()\) are actually obtained by this algorithm. The mapping \(M^{*}\) is of course many-to-one, and yields a partition of \(_{k}\) into equivalence classes of \(k\)-forests that yield the same clusters. This point is actually at the center of our operators of clustering with constraints in Definition 5 and of our designed loss in Section 3.2, both below. We note that as the maximizer of a linear program, when the solution is unique, we have \(_{}F_{k}()=A_{k}^{*}()\).

As described above, our main goal is to enable the inclusion of these operations in end-to-end differentiable pipelines (see Figure 1). We include two important aspects to our framework in order to do so, and to efficiently learn through clustering. The first one is the inclusion of constraints, enforced values coming either from partial or complete clustering information (see below, Definition 5), and the second one is the creation of a differentiable version of these operators, using stochastic perturbations (see Section 2.2).

Clustering with constraints.Given information, we also consider constrained versions of these two problems. We represent the enforced connectivity information as a matrix \(M_{}\), defined as follows.

**Definition 4** (Partial cluster coincidence matrix).: _Let \(M\) be a cluster connectivity matrix, and \(\) a subset of \([n][n]\), representing the set of observations. We denote by \(M_{}\) the \(n n\) matrix_

\[M_{,ij}=M_{ij}(i,j)\,, M_{,ij}=* \,.\]

Remarks.The symbol "\(*\)" in this definition is only used as a placeholder, an indicator of "no information", and does not have a mathematical use. A common setting is where only a subset \(S[n]\) of the data has label information. In this case for any \(i,j S\), \(M_{,ij}=1\) if \(i\) and \(j\) share the same label, otherwise \(M_{,ij}=0\) i.e. elements in the same class are clustered together and separated from elements in other classes.

**Definition 5** (Constrained maximum \(k\)-spanning forest).: _Let \(\) be an \(n n\) similarity matrix. We denote by \(A^{*}_{k}(\,;M_{})\) the adjacency matrix of the \(k\)-spanning forest with maximum similarity, constrained to satisfy the connectivity constraints in \(M_{}\). It is defined as_

\[A^{*}_{k}(\,;M_{})=*{argmax}_{A_{k}(M_ {})} A,,\]

_where for any partial clustering matrix \(M_{}\), \(_{k}(M_{})\) is the set of \(k\)-spanning forests whose clusters agree with \(M_{}\), i.e.,_

\[_{k}(M_{})=\{A_{k}\,:\,M^{*}(A)_{ij}=M_{, ij}\;\,(i,j)\}\,.\]

_For any partial connectivity matrix \(M_{}\), this defines another mapping \(A^{*}(\,\,;\,M_{}):_{n}_{k}\)._

_We denote by \(F_{k}(\,;M_{})\) the value of this maximum, i.e.,_

\[F_{k}(\,;M_{})=_{A_{k}(M_{})} A,.\]

_Similarly we define \(M^{*}_{k}(\,;M_{})=M^{*}(A^{*}_{k}(\,;M_{}))\)._

Remarks.We consider these operations in order to infer clusterings and spanning forests that are consistent with observed information. That is particularly important when designing a partial loss to learn from observed clustering information. Note that depending on what the set of observations \(\) is, these constraints can be more or less subtle to satisfy. For certain sets of constraints \(\), when the matroid structure is preserved, we can obtain \(A^{*}_{k}(\,;M_{})\) by the usual greedy algorithm, by additionally checking that any new edge does not violate the constraints defined by \(\). This is for example the case when \(\) corresponds to exactly observing a fully clustered subset of points.

### Perturbed clustering

The clustering operations defined above are efficient and perform well (see Figure 2) but by their nature as discrete operators, they have a major drawback: they are piece-wise constant and as such cannot be conveniently included in end-to-end differentiable pipelines, such as those used to train models such as neural networks. To overcome this obstacle, we use a proxy for our operators, by introducing a _perturbed_ version (Abernethy et al., 2016; Berthet et al., 2020; Paulus et al., 2020; Struminsky et al., 2021), obtained by taking the expectation of solutions with stochastic additive noise on the input. In these definitions and the following, we consider \(Z\) from a distribution with positive, differentiable density over \(_{n}\), and \(>0\).

Figure 2: Method illustration, for \(k=2\): (**left**) Similarity matrix based on pairwise square distance, partial cluster connectivity information. (**center**) Clustering using spanning forests without partial clustering constraints. (**right**) Constrained clustering with partial constraints.

**Definition 6**.: _We define the perturbed maximum spanning forest as the expected maximum spanning forest under stochastic perturbation on the inputs. Formally, for a similarity matrix \(\), we have_

\[A^{*}_{k,}()=[A^{*}_{k}(+ Z)]= *{argmax}_{A_{k}}(A,+  Z)\,, F_{k,}()=[F_{k}( + Z)]\,.\]

_We define analogously \(A^{*}_{k,}(\,;M_{})=[A^{*}_{k}(+  Z;M_{})]\) and \(F_{k,}(\,;M_{})=[F_{k}(+ Z;M _{})]\), as well as clustering \(M^{*}_{k,}()=[M^{*}_{k}(+ Z)]\) and \(M^{*}_{k,}(\,;M_{})=[M^{*}_{k}(+  Z;M_{})]\)._

We note that this defines operations \(A^{*}_{k,}()\) and \(A^{*}_{k,}(\,\,;M_{})\) mapping \(_{n}\) to \((_{k})\), the _convex hull_ of \(_{k}\). These operators have several advantageous features: They are differentiable, and both their values and their derivatives can be estimated using Monte-Carlo methods, by averaging copies of \(A^{*}_{k}(+ Z^{(i)})\). These operators are the ones used to compute the gradient of the loss that we design to learn from clustering (see Definition 8 and Proposition 1).

This is particularly convenient, as it does not require to implement a different algorithm to compute the differentiable version. Moreover, the use of parallelization in modern computing hardware makes the computational overhead almost nonexistent. These methods are part of a large literature on using perturbations in optimizers such as LP solutions (Papandreou and Yuille, 2011; Hazan et al., 2013; Gane et al., 2014; Hazan et al., 2016), including so-called Gumbel max-tricks (Gumbel, 1954; Maddison et al., 2016; Jang et al., 2017; Huijben et al., 2022; Blondel et al., 2022)

Since \(M^{*}_{k,}()\) is a differentiable operator from \(_{n}\) to \((_{k})\), it is possible to use any loss function on \((_{k})\) to design a loss based on \(\) and some ground-truth clustering information \(M_{}\), such as \(L(\,;M_{})=\|M^{*}_{k,}()-M_{}\|_{2}^{2}\). This flexibility is one of the advantages of our method. In the following section, we introduce a loss tailored to be efficient to compute and performant in several learning tasks, that we call a _partial Fenchel-Young loss_.

## 3 Learning with differentiable clustering

### Fenchel-Young losses

In structured prediction, a common _modus operandi_ is to minimize a loss between some structured _ground truth response_ or _label_\(y\) and a _score_\(^{d}\) (the latter often itself the output of a parameterized model). As an example, in logistic regression \(y\{0,1\}\) and \(= x,\). The framework of _Fenchel-Young_ losses allows to tackle much more complex structures, such as cluster connectivity information.

**Definition 7** (Fenchel-Young loss-Blondel et al. (2020)).: _Let \(F\) be a convex function on \(^{d}\), and \(F^{*}\) its Fenchel dual on a convex set \(^{d}\). The Fenchel-Young loss between \(^{d}\) and \(y()\) is_

\[L_{}(;y)=F()-,y+F^{*}(y)\,.\]

The Fenchel-Young (FY) loss satisfies several properties, making it useful for learning. In particular, it is nonnegative, convex in \(\), handles well noisy labels, and its gradient with respect to the score can be efficiently computed, with \(_{}L_{}(;y)=_{}F()-y\)(see, e.g., Blondel et al., 2020).

In the case of linear programs over a polytope \(\), taking \(F\) to be the so-called support function defined as \(F()=_{y} y,\) (consistent with our notation so far), the dual function \(F^{*}\) is the indicator function of \(\)(Rockafellar, 1997), and the Fenchel-Young function is then given by

\[L_{}(,y)=F()-,y&y\,,\\ +&\,.\]

In the case of a perturbed maximum for \(F\), see, e.g., Berthet et al. (2020), we have

\[F_{}()=[_{y} y,+  Z]\,, y^{*}_{}()= [*{argmax}_{y} y,+  Z]\,.\]

In this setting (under mild regularity assumptions on the noise distribution and \(\)), we have that \(=F^{*}_{}\) is a strictly convex Legendre function on \(\) and \(y^{*}_{}()=_{}F_{}()\). This is actually equivalent to having \(\) and \(*{argmax}\) with a \(-(y)\) regularization (see Berthet et al., 2020, Propositions 2.1and 2.2). In this case, the FY loss is given by \(L_{,}(;y)=F_{}()-,y +(y)\) and its gradient by \(_{}L_{,}(;y)=y^{*}_{}( )-y\). One can also define it as \(L_{,}(;y)=[L_{}(+  Z;y)]\) and it has the same gradient. In the perturbations case, it can be easily obtained by Monte-Carlo estimates of the perturbed optimizer, taking \(_{i=1}^{B}y^{*}(+ Z_{i})-y\).

### Partial Fenchel-Young losses

As noted above, this loss is widely applicable in structured prediction. As presented here, it requires label data \(y\) of the same kind than the optimizer \(y^{*}\). In our setting, the linear program is over spanning \(k\)-forests rather than connectivity matrix. This is no accident: linear programs over cut matrices (when \(k=2\)) are already NP-hard (Karp, 1972). If one has access to richer data (such as ground-truth spanning forest information), one can ignore the operator \(M^{*}\) and focus only on \(A^{*}_{k,}\), \(F_{k,}\), and the associated FY-loss. However, in most cases, clustering data can reasonably only be expected to be present as connectivity matrix. It is therefore necessary to alter the Fenchel-Young loss, and we introduce the following loss, which allows to work with partial information \(p\) representing partial information about the unknown ground-truth \(y\). Using this kind of "inf-loss" is common when dealing with partial label information (see, e.g., Cabannes et al., 2020).

**Definition 8** (Partial Fenchel-Young loss).: _Let \(F\) be a convex function, \(L_{}\) the associated Fenchel-Young loss, and for every \(p\) a convex constraint subset \((p)\)._

\[_{}(;p)=_{y(p)}L_{}( ;y)\,.\]

This allows to learn from incomplete information about \(y\). When we do not know its value, but only a subset of \((p)\) to which it might belong, we can minimize the infimum of the FY losses that are consistent with the partial label information \(y(p)\).

**Proposition 1**.: _When \(F\) is the support function of a compact convex set \(\), the partial Fenchel-Young loss (see Definition 8) satisfies_

1. _The loss is a difference of convex functions in_ \(\) _given explicitly by_ \[_{}(;p)=F()-F(;p)\,,  F(;p)=_{y(p)} y,,\]
2. _The gradient with respect to_ \(\) _is given by_ \[_{}_{}(;p)=y^{*}()-y^{*}(;p) \,, y^{*}(;p)=*{argmax}_{y (p)} y,.\]
3. _The perturbed partial Fenchel-Young loss given by_ \(_{,}(;p)=[_{}( + Z;p)]\) _satisfies_ \[_{}_{,}(;p)=y^{*}_{ }()-y^{*}_{}(;p)\,, y ^{*}_{}(;p)=[*{argmax}_{y (p)} y,+ Z]\,.\]

Another possibility would be to define the partial loss as \(_{y(p)}L_{,}(;y)\), that is, the infimum of smoothed losses instead of the smoothed infimum loss \(L_{,}(;p)\) defined above. However, there is no direct method to minimizing the smoothed loss with respect to \(y(p)\). Note that we have a relationship between the two losses:

**Proposition 2**.: _Letting \(L_{,}(;y)=[L_{,}( + Z;y)]\) and \(_{,}\) as in Definiton 8, we have_

\[_{,}(;p)_{y(p)}L_{ ,}(;y)\,.\]

The proofs of the above two propositions are detailed in the Appendix.

### Applications to differentiable clustering

We apply this framework, as detailed in the following section and in Section 4, to clustering. This is done naturally by transposing notations and taking \(=_{k}\), \(=\), \(y^{*}=A^{*}_{k}\), \(p=M_{}\), \((p)=_{k}(M_{})\), and \(y^{*}(;p)=A^{*}_{k}(\,;M_{})\). In this setting the perturbed partial Fenchel-Young loss satisfies

\[_{}_{,}(\,;M_{})=A^{*}_{ k,}()-A^{*}_{k,}(\,;M_{})\,.\]We learn representations of a data that fit with clustering information (either complete or partial). As described above, we consider settings with \(n\) elements described by their features \(=X_{1},,X_{n}\) in some feature space \(\), and \(M_{}\) some clustering information. Our pipeline to learn representations includes the following steps (see Figure 1)

1. Embed each \(X_{i}\) in \(^{d}\) with a parameterized model \(v_{i}=_{w}(X_{i})^{d}\), with weights \(w^{p}\).
2. Construct a similarity matrix from these embeddings, e.g. \(_{w,ij}=-\|_{w}(X_{i})-_{w}(X_{j})\|_{2}^{2}\).
3. Stochastic optimization of the expected loss of \(_{,}(_{w,b},M_{,b})\), using mini-batches of \(\).

Details.To be more specific on each of these phases: _i)_ We embed each \(X_{i}\) individually with a model \(_{w}\), using in our application neural networks and a linear model. This allows us to use learning through clustering as a way to learn representations, and to apply this model to other elements, for which we have no clustering information, or for use in other downstream tasks. _ii)_ We focus on cases where the similarity matrix is the negative squared distances between those embeddings. This creates a connection between a model acting individually on each element, and a pairwise similarity matrix that can be used as an input for our differentiable clustering operator. This mapping, from \(w\) to \(\), has derivatives that can therefore be automatically computed by backpropagation, as it contains only conventional operations (at least when \(_{w}\) is itself a conventional model, such as commonly used neural networks). _iii)_ We use our proposed smoothed partial Fenchel-Young (Section 3.2) as the objective to minimize between the partial information \(M_{}\) and \(\). The full-batch version would be to minimize \(L_{,}(_{w},M_{})\) as a function of the parameters \(w^{p}\) of the model. We focus instead on a mini-batch formulation for two reasons: first, stochastic optimization with mini-batches is a commonly used and efficient method for generalization in machine learning; second, it allows to handle larger-scale datasets. As a consequence, the stochastic gradients of the loss are given, for a mini-batch \(b\), by

\[_{w}_{,}(_{w,b};M_{,b})= _{w}_{w}_{}_{,}( _{w,b};M_{,b})=_{w}_{w}(A_{k,} ^{*}(_{w})-A_{k,}^{*}(_{w};M_{,b})).\]

The simplicity of these gradients is due to our particular choice of smoothed partial Fenchel-Young loss. They can be efficiently estimated automatically, as described in Section 2.2, which results in a doubly stochastic scheme for the loss optimization.

## 4 Experiments

We apply our framework for learning through clustering in both a supervised and a semi-supervised setting, as illustrated in Figure 1. Formally, for large training datasets of size \(n\), we either have access to a full cluster connectivity matrix \(M_{}\) or a partial one (typically built by using partial label information, see below). We use this clustering information \(M_{}\), from which mini-batches can be

Figure 3: **(left)** Illustration of clustering methods on toy-data sets. **(right)** Using cluster information to learn a linear de-noising **(bottom)** of a noised signal **(top)**.

extracted, as supervision. We minimize our partial Fenchel-Young loss with respect to the weights of an embedding model, and evaluate these embeddings in two main manners on a test dataset: first, by evaluating the clustering accuracy (i.e. proportion of correct coefficients in the predicted cluster connectivity matrix), and second by training a shallow model on a classification task (using clusters as classes) on a holdout set, evaluating it on a test set.

### Supervised learning

We apply first our method to synthetic datasets - purely to provide an illustration of both our internal clustering algorithm, and of our learning procedure. In Figure 2, we show how the clustering operator that we use, based on spanning forests (i.e. single linkage), with Kruskal's algorithm is efficient on some standard synthetic examples, even when they are not linearly separable (compared to \(k\)-Means, mean-shift, Gaussian mixture-model). We also show that our method allows us to perform supervised learning based on cluster information, in a linear setting. For \(X_{}\) consisting of \(n=60\) points in two dimensions consisting of data in four well-separated clusters (see Figure 2), we construct \(X\) by appending two noise dimensions, such that clustering based on pairwise square distances between \(X_{i}\) mixes the original clusters. We learn a linear de-noising transformation \(X\), with \(^{4 2}\) through clustering, by minimizing our perturbed partial FY loss with SGD, using the known labels as supervision.

We also show that our method is able to cluster virtually all of some classical datasets. We train a CNN (LeNet-5 LeCun et al. (1998)) on mini-batches of size 64 using the partial Fenchel-Young loss to learn a clustering, with a batch-wise clustering precision of 0.99 for MNIST and \(0.96\) on Fashion MNIST. Using the same approach, we trained a ResNet (He et al., 2016) on CIFAR-10 (with some minor modifications to kernel size and stride), achieving a batch-wise clustering test precision of \(0.93\). The t-SNE visualization of the embeddings for the validation set are displayed in Figure 5. The experimental setups (as well as visualization of learnt clusters for MNIST), are detailed in the Appendix.

Figure 4: **Semi-supervised learning**: Performance of a CNN trained on partially labeled MNIST data **(top row)**, and a ResNet trained on partially labeled Cifar-10 data **(bottom row)**. Our trained model (full line) is evaluated on clustering **(left column)** and compared to a model entirely trained on the classification task (dashed line). Both models are evaluated on a down-stream classification problem i.e. transfer learning, via a linear probe **(right column)**.

### Semi-supervised learning

We show that our method is particularly useful in settings where labelled examples are scarce, even in the particularly challenging case of having no labelled examples for some classes. To this end, we conduct a series of experiments on the MNIST (LeCun et al., 2010) and Cifar-10 (Krizhevsky et al., 2009) datasets, where we independently vary both the total number of labelled examples \(n_{}\), as well as the number of withheld classes for which no labelled examples are present in the training set \(k_{w}\{0,3,6\}\). For MNIST we consider data sets with \(n_{}\{100,250,500,1000,2000,5000\}\) labelled examples, and for Cifar-10 we consider \(n_{}\{1000,2500,5000,7500,10,000\}\).

We train the same embedding models using our partial Fenchel-Young loss with batches of size 64. We use \(=0.1\) and \(B=100\) for the estimated loss gradients, and optimize weights using Adam (Kingma and Ba, 2015).

In left column of Figure 4, we report the test clustering error (evaluated on mini-batches of the same size), for each of the models, and data sets, corresponding to the choice of \(n_{}\) and \(k_{w}\). We compare the performance of each model to a baseline, consisting of the exact same architecture (plus a linear head mapping the embedding to logits), trained to minimize the cross entropy loss.

In addition, we evaluate all models on a down-stream (transfer-learning) classification task, by learning a linear probe on top of the frozen model, trained on hold-out data with all classes present. The results are depicted in the right hand column of Figure 4. Further information regarding the experimental details can be found in the Appendix.

We observe that learning through clustering allows to find a representation where class semantics are easily recoverable from the local topology. On Cifar-10, our proposed approach strikingly achieves a lower clustering error in the most challenging setting (\(n_{}=1000\) labelled examples and \(k_{w}=6\) withheld classes) than the classification-based baseline in the most lenient setting (\(n_{}=10,000\) labelled examples all classes observed). Importantly, this advantage is not limited to clustering metrics: learning through clustering can also lead to lower down-stream classification error, with the gap being most pronounced when few labelled examples are available.

Moreover, besides this pronounced improvement in data efficiency, we found that our method is capable of clustering classes for which no labelled examples are seen during training (see Figure 5, left). Therefore, investigating potential applications of learning through clustering to zero-shot and self-supervised learning are promising avenues for future work.

Figure 5: **(left)** t-SNE of learnt embeddings for the CNN trained on MNIST with \(k_{w}=3\) withheld classes (highlighted). **(right)** t-SNE of learnt embeddings for the ResNet trained on CIFAR-10.