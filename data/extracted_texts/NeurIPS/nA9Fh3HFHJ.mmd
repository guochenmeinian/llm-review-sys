# Deconstructing Data Reconstruction:

Multiclass, Weight Decay and General Losses

 Gon Buzaglo

Equal Contribution

Niv Haim

Equal Contribution Weizmann Institute of Science, Rehovot, Israel

Gilad Yehudai

Weizmann Institute of Science, Rehovot, Israel

Gal Vardi

TTI-Chicago and the Hebrew University of Jerusalem

Yakir Oz

Weizmann Institute of Science, Rehovot, Israel

Yaniv Nikankin

Weizmann Institute of Science, Rehovot, Israel

Michal Irani

Weizmann Institute of Science, Rehovot, Israel

###### Abstract

Memorization of training data is an active research area, yet our understanding of the inner workings of neural networks is still in its infancy. Recently, Haim et al. (2022) proposed a scheme to reconstruct training samples from multilayer perceptron binary classifiers, effectively demonstrating that a large portion of training samples are encoded in the parameters of such networks. In this work, we extend their findings in several directions, including reconstruction from multiclass and convolutional neural networks. We derive a more general reconstruction scheme which is applicable to a wider range of loss functions such as regression losses. Moreover, we study the various factors that contribute to networks' susceptibility to such reconstruction schemes. Intriguingly, we observe that using weight decay during training increases reconstructability both in terms of quantity and quality. Additionally, we examine the influence of the number of neurons relative to the number of training samples on the reconstructability.

Code: https://github.com/gonbuzaglo/decoreco

## 1 Introduction

Neural networks are known to memorize training data despite their ability to generalize well to unseen test data (Zhang et al., 2021; Feldman, 2020). This phenomenon was observed in both supervised settings (Haim et al., 2022; Balle et al., 2022; Loo et al., 2023) and in generative models (Carlini et al., 2019, 2021, 2023). These works shed an interesting light on generalization, memorization and explainability of neural networks, while also posing a potential privacy risk.

Current reconstruction schemes from trained neural networks are still very limited and often rely on unrealistic assumptions, or operate within restricted settings. For instance, Balle et al. (2022) propose a reconstruction scheme based on the assumption of having complete knowledge of the training set, except for a single sample. Loo et al. (2023) suggest a scheme which operates under the NTK regime (Jacot et al., 2018), and assumes knowledge of the full set of parameters at initialization. Reconstruction schemes for unsupervised settings are specifically tailored for generative models and are not applicable for classifiers or other supervised tasks.

Recently, Haim et al. (2022) proposed a reconstruction scheme from feed-forward neural networks under logistic or exponential loss for binary classification tasks. Their scheme requires only knowledge of the trained parameters, and relies on theoretical results about the implicit bias of neural networks towards solutions of the maximum margin problem (Lyu and Li, 2019; Ji and Telgarsky, 2020). Namely, neural networks are biased toward KKT points of the max-margin problem (see Theorem 3.1). By utilizing the set of conditions that KKT points satisfy, they devise a novel loss function that allows for reconstruction of actual training samples. They demonstrate reconstruction from models trained on common image datasets (CIFAR10 (Krizhevsky et al., 2009) and MNIST (LeCun et al., 2010)).

In this work, we expand the scope of neural networks for which we have evidence of successful sample memorization, by demonstrating sample reconstruction. Our contributions are as follows:

* We extend the reconstruction scheme of Haim et al. (2022) to a multiclass setting (Fig. 1). This extension utilizes the implicit bias result from Lyu and Li (2019) to multiclass training. We analyse the effects of the number of classes on reconstructability, and show that models become more susceptible to sample reconstruction as the number of classes increases.
* We devise a reconstruction scheme that applies for general loss functions, assuming that the model is trained with weight decay. We demonstrate reconstruction from models trained on regression losses.
* We investigate the effects of weight decay and show that for certain values, weight decay increases the vulnerability of models to sample reconstruction. Specifically, it allows us to reconstruct training samples from a convolutional network, while Haim et al. (2022) only handled MLPs.
* We analyse the intricate relation between the number of samples and the number of parameters in the trained model, and their effect on reconstrtability. We also demonstrate successful reconstruction from a model trained on \(5{,}000\) samples, surpassing previous results that focused on models trained on up to \(1{,}000\) samples.

## 2 Related Works

Memorization and Samples Reconstruction.There is no consensus on the definition of the term "memorization" and different works study this from different perspectives. In ML theory, memorization usually refers to label (or, model's output) memorization (Zhang et al., 2016; Arpit et al., 2017; Feldman, 2020; Feldman and Zhang, 2020; Brown et al., 2021), namely, fitting the training set. Memorization in the _input_ domain is harder to show, because in order to demonstrate its occurrence one has to reconstruct samples from the model. Balle et al. (2022) demonstrated reconstruction of one training sample, assuming knowledge of all other training samples and Haim et al. (2022) demonstrated reconstruction of a substantial portion of training samples from a neural network classifier. Loo et al. (2023) extend their work to networks trained under the NTK regime (Jacot et al., 2018) and explore the relationship to dataset distillation. Several works have also studied memorization and samples reconstruction in generative models like autoencoders (Erhan et al., 2009; Radhakrishnan et al., 2018), large language models (Carlini et al., 2021, 2019, 2022) and diffusion-based image generators (Carlini et al., 2023; Sompalli et al., 2022; Gandikota et al., 2023; Kumari et al., 2023).

Inverting Classifiers.Optimizing a model's input as to minimize a class score is the common approach for neural network visualization (Mahendran and Vedaldi, 2015). It usually involves using input regularization (Mordvintsev et al., 2015; Ghiasi et al., 2022), GAN prior (Nguyen et al., 2016, 2017) or knowledge of batch statistics (Yin et al., 2020). Fredrikson et al. (2015), Yang et al. (2019) showed reconstruction of training samples using similar approach, however these methods are limited to classifiers trained with only a few samples per class. Reconstruction from a federated-learning setup (Zhu et al., 2019; He et al., 2019; Hitaj et al., 2017; Geiping et al., 2020; Yin et al., 2021; Huang et al., 2021; Wen et al., 2022) involve attacks that assume knowledge of training samples' gradients (see also Wang et al. (2023) for a theoretical analysis). In this work we do not assume any knowledge on the training data and do not use any priors other than assuming bounded inputs.

## 3 Preliminaries

In this section, we provide an overview of the fundamental concepts and techniques required to understand the remainder of the paper, focusing on the fundamentals laid by Haim et al. (2022) for reconstructing training samples from trained neural networks.

Theoretical Framework.Haim et al. (2022) builds on the theory of implicit bias of gradient descent. Neural networks are commonly trained using gradient methods, and when large enough, they are expected to fit the training data well. However, it is empirically known that these models converge to solutions that also generalize well to unseen data, despite the risk of overfitting. Severalworks pointed to this "_implicit bias_" of gradient methods as a possible explanation. Soudry et al. (2018) showed that linear classifiers trained with gradient descent on the logistic loss converge to the same solution as that of a hard-SVM, meaning that they maximize the margins. This result was later extended to non-linear and homogeneous neural networks by Lyu and Li (2019); Ji and Telgarsky (2020):

**Theorem 3.1** (Paraphrased from Lyu and Li (2019); Ji and Telgarsky (2020)): _Let \((;)\) be a homogeneous 2 ReLU neural network. Consider minimizing the logistic loss over a binary classification dataset \(\{(_{i},y_{i})\}_{i=1}^{n}\) using gradient flow. Assume that there exists time \(t_{0}\) where the network classifies all the samples correctly. Then, gradient flow converges in direction to a first order stationary point (KKT point) of the following maximum-margin problem:_

\[_{}\|\|^{2} i[n]\;\;y_{i}(;_{i}) 1\;.\] (1)

AKKT point of Eq. (1) is characterized by the following set of conditions:

\[ j[p],\;\;_{j}-_{i=1}^{n}_{i} _{_{j}}[y_{i}(;_{i})]=0 \] (2) \[ i[n],\;\;\;y_{i}(;_{i}) 1 \] (3) \[ i[n],\;\;\;_{i} 0 \] (4) \[ i[n],\;\;\;_{i}=0y_{i}(; _{i}) 1 \] (5)

Reconstruction Algorithm.Haim et al. (2022) demonstrated reconstructing samples from the training set of such classifiers by devising a reconstruction loss. Given a trained classifier \((;)\), they initialize a set of \(\{(_{i},y_{i})\}_{i=1}^{m}\) and \(\{_{i}\}_{i=1}^{m}\), and optimize \(_{i},_{i}\) to minimize the following loss function:

\[L=-_{i=1}^{m}_{i}_{_{j}}[y_{i}(;_{i})]\|}_{L_{}}+^{m}\{-,-_{}\}}_{L_{_{i}}}+L_{}\] (6)

Where \(L_{}\) is simply bounding each pixel value at \([-1,1]\)3. The number of training samples \(n\) is unknown. However, setting \(m>2n\), where \(\{y_{i}\}\) are set in a balanced manner allows reconstructing samples with any label distribution. The \(_{i}\)'s are initialized from the Gaussian distribution \((0,_{x}^{2})\), and \(_{},_{x}\) are hyperparameters. We note that the homogeneity condition from Theorem 3.1 is not necessarily a practical limitation of this reconstruction scheme, as already in Haim et al. (2022) they show reconstructions from a non-homogeneous network.

Analysing and Summarizing Results.The optimization in Eq. (6) is executed \(k\) times (for different hyperparameters) and results in \(km\) outputs (\(\{}_{i}\}_{i=1}^{km}\)) that we term _candidates_, as they are candidates to be reconstructed training samples. To quantify the success of the reconstruction process, each training sample is matched with its nearest-neighbour from the \(km\) candidates. The "quality" of reconstruction is then measured using SSIM Wang et al. (2004) (see full details in Appendix A.1).

An important corollary of the set of KKT conditions Eqs. (2) to (5) is that the parameters of the trained model only depend on gradients of samples that are closest to the decision boundary, the so-called "margin-samples" (see end of section 3.2 in Haim et al. (2022)). Therefore, a good visual summary for analysing reconstruction from a trained model is by plotting the reconstruction quality (SSIM) against the distance from the decision boundary (\(|(_{i})|\)). We also utilize such visualizations.

Assessing the Quality of Reconstructed Samples.Determining whether a candidate is a correct match for some training sample is as hard as finding a good image similarity metric. No synthetic metric such as SSIM or L2 norm can perfectly align with human perception. Perceptual similarity metrics (e.g., LPIPS (Zhang et al., 2018)) build on top of pre-trained classifiers trained on Imagenet (Deng et al., 2009), and are not effective for the image resolution in this work (up to 32x32 pixels). We have observed heuristically that candidates with SSIM score higher than about \(0.4\) are indeed visually similar to their nearest neighbor training sample. Hence, in this work we say that a certain candidate is a _"good reconstruction"_ if its SSIM score with its nearest neighbor is at least \(0.4\). Also see discussion in Appendix A.2.

## 4 Reconstructing Data from Multi-Class Classifiers

We demonstrate that training set reconstruction can be extended to multi-class classification tasks.

### Theory

The extension of the implicit bias of homogeneous neural networks to the multi-class settings is discussed in Lyu and Li (2019) (Appendix G): Let \(S=\{(_{i},y_{i})\}_{i=1}^{n}^{d}[C]\) be a multi-class classification training set where \(C\) is any number of classes, and \([C]=\{1,,C\}\). Let \((;):^{d}^{C}\) be a homogeneous neural network parameterized by \(^{p}\). We denote the \(j\)-th output of \(\) on an input \(\) as \(_{j}(;)\). Consider minimizing the standard cross-entropy loss and assume that after some number of iterations the model correctly classifies all the training examples. Then, gradient flow will converge to a KKT point of the following maximum-margin problem:

\[_{}\|\|^{2} _{y_{i}}(;_{i})-_{j}( ;_{i}) 1\ \  i[n], j[C]\{y_{i}\}.\] (7)

A KKT point of the above optimization problem is characterized by the following set of conditions:

\[-_{i=1}^{n}_{j y_{i}}^{c}_{i,j}_{}(_{y_{i}}(;_{ i})-_{j}(;_{i}))=\] (8) \[ i[n], j[C]\{y_{i}\}:\ _{y_{i}}( ;_{i})-_{j}(;_{ i}) 1\] (9) \[ i[n], j[C]\{y_{i}\}:\ _{i,j} 0\] (10) \[ i[n], j[C]\{y_{i}\}:\ _{i,j}=0_{y_{i}}( ;_{i})-_{j}(;_{ i}) 1\] (11)

A straightforward extension of a reconstruction loss for a multi-class model that converged to the conditions above would be to minimize the norm of the left-hand-side (LHS) of condition Eq. (8)

Figure 1: Reconstructed training samples from a multi-class MLP classifier that was trained on \(500\) CIFAR10 images. Each column corresponds to one class and shows the \(10\) training samples (_red_) that were best reconstructed from this class, along with their reconstructed result (_blue_).

(namely, optimize over \(\{_{i}\}_{i=1}^{m}\) and \(\{_{i,j}\}_{i[m],j[C] y_{i}}\) where \(m\) is a hyperparameter). However, this straightforward extension failed to successfully reconstruct samples. We therefore propose the following equivalent formulation.

Note that from Eqs. (9) and (11), most \(_{i,j}\) zero out: the distance of a sample \(_{i}\) to its nearest decision boundary, \(_{y_{i}}-_{j y_{i}}_{j}\), is usually achieved for a single class \(j\) and therefore (from Eq. (11)) in this case at most one \(_{i,j}\) will be non-zero. For some samples \(_{i}\) it is also possible that all \(_{i,j}\) will vanish. Following this observation, we define the following loss that only considers the distance from the decision boundary:

\[L_{}(_{1},...,_{m},_{1},..., _{m})=\|-_{i=1}^{m}_{i}\ _{}[_{y_{i}}(_{i}; )-_{j y_{i}}_{j}(_{i};)]\|_{2}^{2}\] (12)

Eq. (12) implicitly includes Eq. (11) into the summation in Eq. (8), thereby significantly reducing the number of summands and simplifying the overall optimization problem.

While the straightforward extension failed to successfully reconstruct samples, solving Eq. (12) enabled reconstruction from multiclass models (see Fig. 1 and results below). We attribute this success to the large reduction in the number of optimization variables, which simplifies the overall optimization problem (\(n\) variables in Eq. (12) compared to \(C n\) variables in the straightforward case, which is significant for large number of classes \(C\)).

We also use the same \(L_{}\) and \(L_{}\) as in Eq. (6), and set \(\{y_{i}\}\) in a balanced manner (uniformly on all classes). While setting \(m=C n\) allows reconstructing any label distribution, in our experiments we focus on models trained on balanced training sets, and use \(m=2n\) which works sufficiently well. An intuitive way to understand the extension of the binary reconstruction loss Eq. (6) to multi-class reconstruction Eq. (12) is that the only difference is the definition of the _distance to nearest boundary_, which is the term inside the square brackets in both equations.

### Results

We compare between reconstruction from binary classifiers, as studied in Haim et al. (2022), and reconstruction from multi-class classifiers by using the novel loss function Eq. (12). We conduct the following experiment: we train an MLP classifier with architecture \(D\)-\(1000\)-\(1000\)-\(C\) on samples from the CIFAR10 (Krizhevsky et al., 2009) dataset. The model is trained to minimize the cross-entropy loss with full-batch gradient descent, once with two classes (\(250\) samples per class) and once for the full \(10\) classes (\(50\) samples per class). Both models train on the same amount of samples (\(500\)). The test set accuracy of the models is \(77\%/32\%\) respectively, which is far from random (\(50\%/10\%\) resp.). See implementation details in Appendix B.

To quantify the quality of our reconstructed samples, for each sample in the original training set we search for its nearest neighbour in the reconstructed images and measure the similarity using SSIM (Wang et al., 2004) (higher SSIM means better reconstruction). In Fig. 2 we plot the quality of reconstruction (in terms of SSIM) against the distance of the sample from the decision boundary \(_{y_{i}}(_{i};)-_{j y_{i}}_{j}( _{i};)\). As seen, a multi-class classifier yields much more samples that are vulnerable to being reconstructed.

Figure 2: Multi-class classifiers are more vulnerable to training-set reconstruction. For a training set of size \(500\), a multi-class model (_left_) yields \(101\) reconstructed samples with good quality (SSIM\(>\)\(0.4\)), compared to \(40\) in a binary classification model (_right_).

We examine the relation between the ability to reconstruct from a model and the number of classes on which it was trained. Comparing between two models trained on different number of classes is not immediately clear, since we want to isolate the effect of the number of classes from the size of the dataset (it was observed by Haim et al. (2022) that the number of reconstructed samples decreases as the total size of the training set increases). We therefore train models on training sets with varying number of classes (\(C\{2,3,4,5,10\}\)) and varying number of samples per class (\(1,5,10,50\)). The results are visualized in Fig. 2(a). As seen, for models with same number of samples per class, the ability to reconstruct _increases_ with the number of classes, even though the total size of the training set is larger. This further validates our hypothesis that the more classes, the more samples are vulnerable to reconstruction (also see Appendix C).

Another way to validate this hypothesis is by showing the dependency between the number of classes and the number of "good" reconstructions (SSIM\(>\)\(0.4\)) - shown in Fig. 2(b). As can be seen, training on multiple classes yields more samples that are vulnerable to reconstruction. An intuitive explanation, is that multi-class classifiers have more "margin-samples". Since margin-samples are more vulnerable to reconstruction, this results in more samples being reconstructed from the model.

## 5 Data Reconstruction with General Loss Functions

We demonstrate that data reconstruction can be generalized to a larger family of loss functions. Haim et al. (2022) and Section 4 only considered a reconstruction scheme based on the implicit bias of gradient methods trained with cross-entropy loss. For other loss functions, such as the square loss, a precise characterization of the implicit bias in nonlinear networks does not exist (Vardi and Shamir, 2021). Hence, we establish a reconstruction scheme for networks trained with explicit regularization, i.e., with weight decay. We show that as long as the training involves a weight-decay term, we can derive a reconstruction objective that is very similar to the previous objectives in Eqs. (6) and (12).

### Theory

Let \(((_{i};),y_{i})\) be a loss function that gets as input the predicted output of the model \(\) (parametrized by \(\)) on an input sample \(_{i}\), and its corresponding label \(y_{i}\). The total regularized loss:

Figure 3: Evaluating the effect of multiple classes on the ability to reconstruct. We show reconstructions from models trained with different numbers of classes and different numbers of samples per class. As seen, multiple classes result in more reconstructed samples.

\[=_{i=1}^{n}((_{i};),y_{i})+_{ }\|\|^{2}.\] (13)

Assuming convergence (\(_{}=0\)), the parameters should satisfy the following :

\[=_{i=1}^{n}_{i}^{}\;_{}(_{i};)\] (14)

where \(_{i}^{}=-}_{i };),y_{i})}{(_{i};)}\). This form (which is similar to the condition in Eq. (2)), allows us to define a generalized reconstruction loss for models trained with a weight-decay term:

\[L_{rec}(_{1},...,_{m},_{1},...,_{m})=\|-_{i=1}^{n}_{i}_{}(_{i}; {})\|_{2}^{2}\] (15)

As before, we also include the same \(L_{}\) as in Section 3. It is straightforward to see that \(L_{rec}\) is a generalization of the reconstruction loss in Eq. (6) (\(y_{i}\) could be incorporated into the \(_{i}\) term).

### Results and Analysis

We validate the above theoretical analysis by demonstrating reconstruction from models trained on other losses than the ones shown in Section 4 and Haim et al. (2022). We use the same dataset as in the classification tasks - images from CIFAR10 dataset with binary labels of \(\{-1,1\}\). The only difference is replacing the classification binary cross-entropy loss with regression losses (e.g., MSE).

In classification tasks, we analyzed the results by plotting the reconstruction quality (SSIM) against the sample's distance from the decision boundary (see Section 3). This showed that reconstruction is only feasible for margin-samples. However, in regression tasks, margin and decision boundary lack specific meaning. We propose an alternative analysis approach - note that smaller distance from the margin results in higher loss for binary cross-entropy. Intuitively, margin-samples are the most challenging to classify (as reflected by the loss function). Therefore, for regression tasks, we analyze the results by plotting the reconstruction quality against the loss (per training sample).

In Fig. 4 we show results for reconstructions from models trained with MSE, \(L_{2.5}\) loss (\(=|(;)-y|^{p}\) for \(p\)=2,\(2.5\) respectively) and Huber loss (Huber, 1992). The reconstruction scheme in Eq. (15) is the same for all cases, and is invariant to the loss used during training. Fig. 4 highlights two important observations: first, the reconstruction scheme in Eq. (15) succeeds in reconstructing large portions of the training set from models trained with regression losses, as noted from the high

Figure 4: **Reconstruction from general losses** (column) for various training set sizes (row), using Eq. (15). “Harder” samples (with higher loss) are easier to reconstruct.

quality (SSIM) of the samples. Second, by plotting quality against the loss, one sees that "challenging" samples (with high loss) are easier to reconstruct. Also note that the analysis works for classification losses, namely BCE with or without weight-decay in Fig. 4). For more results see Appendix D.

## 6 On the Different Factors that Affect Reconstructability

Our goal is to gain a deeper understanding of the factors behind models' vulnerability to reconstruction schemes. In this section, we present several analyses that shed light on several important factors.

### The Role of Weight Decay in Data Reconstruction

Haim et al. (2022) assumed MLP models whose first fully-connected layer was initialized with small (non-standard) weights. Models with standard initialization (e.g., He et al. (2015), Glorot and Bengio (2010)) did not yield reconstructed samples. In contrast, the MLPs reconstructed in Haim et al. (2022) were initialized with an extremely small variance in the first layer. Set to better understand this drawback, we observed that incorporating weight-decay during training, not only enabled samples reconstruction in models with standard initialization, but often increase the reconstructability of training samples.

In Fig. 4(a)-b we show the number of good reconstructions for different choices of the value of the weight decay (\(_{}\)), for MLP classifiers trained on \(C\)=2,10 classes and \(50\) samples per class (Fig. 5 a, b resp.). We add two baselines trained _without_ weight-decay: model trained with standard initialization (black) and model with small-initialized first-layer (red). See how for some values of weight-decay, the reconstructability is _significantly higher_ than what was observed for models with non-standard initialization. By examining the training samples' distance to the boundary, one observes that using weight-decay results in more margin-samples which are empirically more vulnerable to reconstruction (see full details in Appendix E).

We now give an intuitive theoretical explanation to the role of weight decay in data reconstruction. Theorem 3.1 is used to devise the reconstruction loss in Eq. (6), which is based on the directional convergence to a KKT point of the max-margin problem. However, this directional convergence occurs asymptotically as the time \(t\), and the rate of convergence in practice might be extremely slow. Hence, even when training for, e.g., \(10^{6}\) iterations, gradient descent might reach a solution which is still too far from the KKT point, and therefore reconstruction might fail. Thus, even when training until the gradient of the empirical loss is extremely small, the direction of the network's parameters might be far from the direction of a KKT point. In Moroshko et al. (2020), the authors proved that in _diagonal linear networks_ (i.e., a certain simplified architecture of deep linear networks) the initialization scale controls the rate of convergence to the KKT point, namely, when the initialization is small gradient flow converges much faster to a KKT point. A similar phenomenon seems to occur also in our empirical results: when training without weight decay, small initialization seems to be required to allow reconstruction. However, when training with weight decay, our theoretical analysis in Section 5.1 explains why small initialization is no longer required. Here, the reconstruction does not rely on converging to a KKT point of the max-margin problem, but relies on Eq. (14) which holds (approximately) whenever we reach a sufficiently small gradient of the training objective. Thus, when training with weight decay and reaching a small gradient Eq. (14) holds, which allows for reconstruction, contrary to training without weight decay where reaching a small gradient does not imply converging close to a KKT point.

Figure 5: Using weight-decay during training increases vulnerability to sample reconstruction

Reconstruction from Convolutional Neural Networks (CNNs).CNNs adhere to the assumptions of Theorem 3.1, yet Haim et al. (2022) failed to apply their reconstruction scheme Eq. (6) to CNNs. We observe that incorporating weight-decay during training (using standard initialization) enables samples reconstruction. In Fig. 6 we show an example for reconstruction from a binary classifier whose first layer is a Conv-layer with kernel size \(3\) and \(32\) output channels, followed by two fully connected layers (Conv(\(k\)=3,\(C_{}\)=32)-\(1000\)-\(1\)). The weight-decay term is \(_{}\)=\(0.001\) (the training setup is similar to that of MLP). In Fig. 4(c) we show the reconstructability for the same CNN model trained with other values of \(_{}\). Note how the weight-decay term plays similar role in the CNN as in the MLP case. See full details in Appendix F.

### The Effect of the Number of Parameters and Samples on Reconstructability

Haim et al. (2022) observed that models trained on fewer samples are more susceptible to reconstruction in terms of both quantity and quality. In this section, we delve deeper into this phenomenon, focusing on the intricate relationship between the number of parameters in the trained model and the number of training samples. We conduct the following experiment:

We train \(3\)-layer MLPs with architecture \(D\)-\(W\)-\(W\)-\(1\) on \(N\) training samples from binary CIFAR10 (animals vs. vehicles), where \(W\{5,10,50,100,500,1000\}\) and \(N\{10,50,100,300,500\}\). We conduct the experiment for both classification and regression losses, with BCE and MSE loss respectively. Generalization error is \(23\%\)-\(31\%\) for BCE (classification) and \(0.69\)-\(0.88\) for MSE (regression), compared to \(50\%\)/\(0.97\) for similar models with random weights.

We reconstruct each model using Eq. (15) and record the number of good reconstructions. The results are shown in Fig. 7. Note that as \(}{{N}}\) increases, our reconstruction scheme is capable of reconstructing more samples, and vice versa. For example, consider the case when \(N\)=\(10\). To successfully reconstruct the entire training set, it is sufficient for \(W\) to be greater than \(50\)/\(10\) (for MSE/BCE). However, when \(N\)=\(500\), even larger models (with larger \(W\)) can only reconstruct up to \(8\%\) of the samples.

Lastly, we reconstruct from a model with \(W\)=\(10\),\(000\), trained on \(N\)=\(5\),\(000\) samples (\(5\) times larger than any previous model). While there is some degradation in the quality of the reconstructions compared to models trained on fewer samples, it is evident that our scheme can still reconstruct some of the training samples. For full results see Appendix G.

## 7 Conclusions

We present improved reconstruction methods and conduct a comprehensive analysis of the reconstruction method proposed by Haim et al. (2022). Particularly, we extend their reconstruction scheme to a multi-class setting and devise a novel reconstruction scheme for general loss functions, allowing reconstruction in a regression setting (e.g., MSE loss). We examine various factors influencing reconstructability. We shed light on the role of weight decay in samples memorization, allowing for sample reconstruction from convolutional neural networks. Lastly, we examine the intricate relationship between the number of parameters, the number of samples, and the vulnerability of the model to reconstruction schemes. We acknowledge that our reconstruction method raises concerns regarding privacy. We consider it crucial to present such methodologies as they encourage researchers to study the potential hazards associated with training neural networks. Additionally, it allows for the development of protective measures aimed at preventing the leakage of sensitive information.

Figure 6: **Reconstruction from CNN.** Training samples (red) and their best reconstructions (blue)

Limitations.While we have relaxed several of the previous assumptions presented in Haim et al. (2022), our method still exhibits certain limitations. First, we only consider relatively small-scale models: up to several layers, without residual connections, that have trained for many iterations on a relatively small dataset without augmentations. Second, determining the optimal hyperparameters for the reconstruction scheme poses a challenge as it requires exploring many configurations for even a single model.

Future work.All of the above extend our knowledge and understanding of how memorization works in certain neural networks. This opens up several possibilities for future research including extending our reconstruction scheme to practical models (e.g., ResNets), exploring reconstruction from models trained on larger datasets or different data types (e.g., text, time-series, tabular data), analyzing the impact of optimization methods and architectural choices on reconstructability, and developing privacy schemes to protect vulnerable samples from reconstruction attacks.

Acknowledgements.This project received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 788535), and ERC grant 754705, and from the D. Dan and Betty Kahn Foundation. GV acknowledges the support of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning.