# Accelerating Deep Learning using Ivy

Guillermo Sanchez-Brizuela

Unify.ai

guillermo@unify.ai

&Ved Patwardhan

Unify.ai

ved@unify.ai

&Matthew Barrett

Unify.ai

matthew@unify.ai

&Paul Anderson

Unify.ai

paul@unify.ai

&Mustafa Hani

Unify.ai

mustafa@unify.ai

&Daniel Lenton

Unify.ai

dan@unify.ai

###### Abstract

Today's machine learning (ML) ecosystem suffers from deep fragmentation due to the proliferation of numerous incompatible frameworks, compiler infrastructure and hardware. Each unique tool within this fragmented stack has its own set of benefits and drawbacks, making it better suited for certain use-cases. As a result, different areas of industry and academia use different tools for different use cases, which hinders collaboration and democratization, ultimately resulting in costly re-implementations and sub-optimal runtime efficiency when deploying, due to sparse and partial connections to the rest of the stack. In this paper, we present Ivy, a complementary, multi-backend ML framework, and its transpiler, which aims to bridge this gap and solve the fragmentation problem by enabling the integration of code from one framework into another to speed up research, development, and model inference.

## 1 Introduction

Machine learning (ML) has advanced significantly over the past decade. This has inspired the developer community to build many open-source tools that accelerate the research, development and deployment of ML applications. TensorFlow , PyTorch  and JAX  have emerged as front-runners for ML, while NumPy  is considered the standard framework for numerical computation.

Different users have diverging preferences based on their use-case. For example, PyTorch is preferred by many researchers for its ease of use, Pythonic design and dynamic graphs, others prefer TensorFlow due to its efficiently compiled graphs, superior deployment features, and bindings for edge and mobile devices, while some prefer JAX due to its fully functional form, superior runtime efficiency on TPUs, and unmatched flexibility for gradient computation.

Ivy aims to solve this problem by enabling interoperability between these frameworks, making code portable and reusable. By doing so, Ivy not only democratizes ML, as every tool and model can now be used by developers using other frameworks, but it also unlocks framework-specific infrastructure and support for hardware that wasn't available before, accelerating development, training and inference of ML models.

Exchange formats  have eased fragmentation at the lower levels. For example, ONNX was established to facilitate cross-framework compatibility by defining a standard set of operators that can be used across different hardware targets. However, it mainly focuses on core neural network operations and has limited support for other general array processing functions. Some frameworks have added the ability to import or export ONNX models, allowing for some level ofmodel conversion between frameworks, but it is primarily built for deployment rather than training. Additionally, framework maintainers must implement their own bindings to ONNX, which they don't always fully maintain. In contrast, Ivy proactively binds to all frameworks, without any need for framework creators to coalesce around our standard.

In this paper, we present the Ivy framework, a set of APIs that enable the development of framework-agnostic code and serves as an intermediate representation between frameworks, and the Ivy transpiler, a tool capable of converting code between frameworks.

## 2 Ivy

The Ivy project is composed of various components that collectively make interoperability of code between different ML frameworks possible.

### The Ivy framework

The Ivy Framework attempts to provide an interface that enables unified behavior across the target backends. The framework is composed of three _unique_ APIs plus the backend API, which replicates the Ivy API with framework-specific implementations (Figure 1).

Functional API.The framework revolves around a unified functional API that is the lowest level of abstraction provided. The functional API supports multiple backends, namely NumPy, JAX, TensorFlow, PyTorch and Paddle, each one with a backend-specific implementation of the Ivy functional API. This functional API implements all functions in the Array API standard  which contains the most commonly used array-processing functions, as well as many others not in the standard.

Stateful API.Ivy also has a stateful API, which provides high-level modules and optimizers for rapid prototyping. The stateful API is built on top of the functional API and is therefore backend-agnostic as well.

Frontend API.Frontend APIs have been implemented for PyTorch, TensorFlow, NumPy, JAX and PaddlePaddle. The goal of a frontend API is to replicate the behaviour of each function in that framework's functional API with an implementation that only uses functions from Ivy's functional API. Since Ivy's functional API can be executed using any of the supported backends, we can therefore use these frontends to reproduce the original behaviour of a framework, but using any of the supported backends. This way, Ivy's functional API acts as an Intermediate Representation (IR) between frameworks.

### The Ivy transpiler

Code transpilation refers to the process of converting code written in one framework to a different one or a different version of the source framework. Examples of this procedure would be porting PyTorch code to JAX for faster execution, converting JAX models to PyTorch in order to integrate them into existing pipelines or build on top of them, or seamlessly updating versions when breaking changes occur.

A fundamental part of the transpiler is the graph tracer, which extracts a computational graph of any array-computing-based function or trainable module. Internally, Ivy's tracer registers any functional-API level call that occurs during a given computing procedure alongside relevant information from that particular function call, mainly, unique identifiers of its inputs and outputs. Once this information has been recorded, the tracer reconstructs a directed acyclic graph (DAG) recursively, starting from the output of the top-level system and connecting the inputs and outputs of every function that contributes mathematically to the computation.

Figure 1: Dependency graph of the main building blocks and APIs in the Ivy framework.

Leveraging the graph tracer and Ivy's frontends and backends APIs, Ivy enables code conversion in the transpiler following the process outlined in Figure 2:

1. First tracing pass: We call the graph tracer a to generate a fully functional computational graph that captures the operations of the function/module. The main objective of this step is to decompose the original system, including its high-level classes, into fundamental building blocks from the functional API of the source framework.
2. Replacement of the functional-API level nodes and conversion of parameters: Once we have a fully functional graph of the system, we can leverage Ivy's frontend APIs and backend APIs to perform code conversion. As the frontends mirror the signatures of the frameworks' functional API, we can replace the original functions of the nodes in the captured graph with their Ivy frontend equivalent. As the frontends are implemented using Ivy's functional API, we effectively link the original code to the corresponding functions in any other framework available in the backends, since we can change the framework used to execute Ivy functions. During this replacement stage, we also convert cached and non-tracked objects from the source to the target framework, including constant arrays, native data types or device-classes.
3. Second tracing pass: Although at this stage the graph is fully composed of Ivy functions, and thus already fully interoperable with the target framework, there is some overhead due to the functions called in the frontends and the functional API of Ivy. To remove this overhead entirely and generate an efficient graph of pure operations in the target framework, we trace the system once again. By doing so, we remove the wrapping layers from Ivy's frontends and Ivy's functional API and extract the underlying functions from the target framework functional API.

Once the graph has been transpiled, it's fully composed of functions from the target framework, with all trainable parameters already converted to the corresponding native objects by Ivy. This ensures that all operations are differentiable and that native optimizations and tools are instantly available, regardless of the original framework. As a result, Ivy empowers the user to train and fine-tune the converted models using their preferred framework-specific training loops and infrastructure.

## 3 Use cases of the transpiler

As previously outlined, Ivy's transpiler can be used for different purposes. In this section, we will go over its main value propositions.

### Libraries and tools interoperability

As a direct consequence of the coexistence of different frameworks, different ecosystems of libraries and tools have been developed around them. This means that developers are often limited to using the tools already available in their corresponding ecosystem. Ivy can help in two different ways here. The first one is by transpiling an entire array-based library to a different framework. This way, all functions contained within the library will be converted to the target framework, which can then be used as usual. Secondly, Ivy can also enable the user to leverage training tools, pipelines, and general infrastructure that has been designed for a specific framework models, simply by converting the model from the user framework to the one required by the library.

Figure 2: Overview of the transpilation process and its intermediate stages.

A code snippet containing the usual workflow for library transpilation is available in Appendix 5.1, where we transpile the Kornia library from PyTorch to JAX.

### Integration of trainable modules

Trainable modules, such as specific layers or models, are originally published for one particular framework. This results in costly reimplementations where details can go unnoticed, hampering the reproducibility of the results. In this case, Ivy can automatically convert these trainable modules to accelerate research and bring the latest models to every developer. As the outcome of the transpilation process in Ivy is composed of functions from the functional API of the target framework, this means that the output graph (in this case, contained a trainable module in the target framework) is fully differentiable and can be trained or fine-tunes as if it had been written in the target framework from scratch.

Appendix 5.2 includes an example where a Haiku module is transpiled to PyTorch and then used as the backbone for a classifier, showing how this allows the user to bring any module or section of it to their preferred framework and then build on top of it to obtain a trainable module that mimics the computational behavior of the original system.

### Training and inference acceleration

The previous subsection showcases how a transpiled module from framework A to B can be integrated into another model in framework B to accelerate research and the development of new models. However, the transpilation of a trainable module by itself can also be beneficial in terms of computational resources. We can transpile a model to leverage optimizations that are only available in certain compilers or to enable the use of hardware which is not as well supported in the source framework. As an example of this, we include the results of a set of benchmarks performed on a selected set of models from the Hugging Face's Transformers library where we compare the original PyTorch model and the transpiled version in JAX.

In Table 1, we can see how even after the latest improvements in the PyTorch compiling tools, which accelerate PyTorch code significantly, the conversion of PyTorch models to JAX still bring a mean improvement in the execution speed of the model of \(1.27\), mainly due to the different optimizations happening at the XLA level. For this experiment, we average the latency over 100 runs in a warmed-up NVIDIA V100 GPU. Table 2 in Appendix 5.3 includes the measurements used to calculate the increments.

   Model & Over Default & Over Reduce Overhead & Over Max Autotune & Over Best PyTorch Mode \\  BEiT  & 1.79 & 1.18 & 1.14 & 1.14 \\ BERT  & 3.88 & 1.18 & 1.17 & 1.17 \\ BiT  & 3.45 & 1.14 & 1.11 & 1.11 \\ CamemBERT  & 4.20 & 1.19 & 1.18 & 1.18 \\ ConvNeXt  & 3.04 & 1.18 & 1.08 & 1.08 \\ CvT  & 4.94 & 1.21 & 1.17 & 1.17 \\ DeiT  & 1.79 & 1.20 & 1.12 & 1.12 \\ DistilBERT  & 3.83 & 1.07 & 1.06 & 1.06 \\ MobileNet-v2  & 9.70 & 1.89 & 1.86 & 1.86 \\ ResNet  & 4.03 & 1.37 & 1.43 & 1.37 \\ ViT  & 1.68 & 1.14 & 1.12 & 1.12 \\ MobileBERT  & 6.68 & 0.93 & 0.98 & 0.93 \\ RoBERTa  & 4.22 & 1.20 & 1.19 & 1.19 \\ Roformer  & 4.94 & 1.27 & 1.26 & 1.26 \\ Swin-v2  & 5.91 & 3.42 & 3.88 & 3.42 \\ Whisper  & 1.60 & 1.08 & 1.01 & 1.01 \\ BlenderBot  & 3.08 & 0.79 & 0.79 & 0.79 \\ Wav2vec2  & 2.20 & 1.20 & 1.14 & 1.14 \\ CLIP  & 3.67 & 0.99 & 0.98 & 0.98 \\   

Table 1: Latency improvement of a collection of HF models after being transpiled from PyTorch to JAX when compared to the original model compiled using PyTorch Dynamo with the TorchInductor backend and different compilation modes.

Conclusions

In this paper we have presented Ivy, a multi-backend ML framework, and its transpiler. Through the transpiler, Ivy allows developers to integrate code from any framework into another, removing the friction associated with reimplementations and enables the use of any library, tool, or infrastructure that is better supported in a different frameworks, including specialized compilers and devices. Consequently, Ivy has the potential to accelerate any kind of ML-related research or development process, model training, or deployed system.

Moreover, we are currently working on integrating state of the art tools to perform model compression, quantization and tensorization natively as part of Ivy, as well as specialized low-level optimizations such as efficient, hardware-specific kernels. Through the combination of these framework-specific SOTA tools and the various compilation pipelines that transpilation unlocks, we foresee synergies that will further improve latency and throughput on all kinds of ML models.