# Balancing Context Length and Mixing Times for Reinforcement Learning at Scale

Matthew Riemer

IBM Research, Mila, Universite de Montreal

&Khimya Khetarpal

Mila

Janarthanan Rajendran

Dalhousie University

&Sarath Chandar

Mila, Ecole Polytechnique de Montreal

Please direct correspondences to mdriemer@us.ibm.com. Work done during Postdoc at Mila, Universite de Montreal.

###### Abstract

Due to the recent remarkable advances in artificial intelligence, researchers have begun to consider challenging learning problems such as learning to generalize behavior from large offline datasets or learning online in non-Markovian environments. Meanwhile, recent advances in both of these areas have increasingly relied on conditioning policies on large context lengths. A natural question is if there is a limit to the performance benefits of increasing the context length if the computation needed is available. In this work, we establish a novel theoretical result that links the context length of a policy to the time needed to reliably evaluate its performance (i.e., its mixing time) in large scale partially observable reinforcement learning environments that exhibit latent sub-task structure. This analysis underscores a key tradeoff: when we extend the context length, our policy can more effectively model non-Markovian dependencies, but this comes at the cost of potentially slower policy evaluation and as a result slower downstream learning. Moreover, our empirical results highlight the relevance of this analysis when leveraging Transformer based neural networks. This perspective will become increasingly pertinent as the field scales towards larger and more realistic environments, opening up a number of potential future directions for improving the way we design learning agents.

## 1 Introduction

**Scaling the Context Length:** As the field of AI moves towards harder problems where agents must model higher order non-Markovian dependencies, it is only natural to consider conditioning models on larger context lengths of the interaction history. When considering only small context lengths or single observations, models become restricted to some lesser notion of the best possible performance in comparison to what is achievable with an infinite context length . Meanwhile, there are two obvious downsides to increasing the context length an agent considers. The first downside is the increase in necessary computation. However, there is also ongoing research related to minimizing the needed quantity of computation [2; 3], increasing the parallelism of computation [4; 5; 6], and increasing the throughput of computation on modern hardware . The second downside is the high dimensionality of the input representation we must process and the difficulty of learning and generalizing from this kind of input. But it is unclear how worried researchers and practitioners should be about this downside given the recent improvements in processing long sequences that came with attention models [8; 9] and particularly the Transformer architecture . Additionally, there has been a trend towards training on internet-scale datasets [11; 12; 13] that far exceed the quantityof any single human's experience in domains of interest, which significantly alleviates requirements for generalization. Our paper aims to add to this discourse by being the first to establish another downside: policies that are conditioned on longer context lengths take longer to reliably evaluate because they impose looser bounds on the mixing time. This may make it less safe to deploy these policies in the real-world because we are able to extrapolate less about their true behavior from our limited evaluations. Downstream learning may also be slower for the same reason as Monte Carlo algorithms can only perform reliable updates after being rolled out for the mixing time number of steps and the convergence time of TD algorithms also depends on the mixing time.3

**Addressing High Mixing Times:** Recent work has highlighted the challenges presented by high mixing times as the field moves towards large scale and continual reinforcement learning problems . Unfortunately, as highlighted by Riemer et al.  there are not many known solutions that address this problem. As a result, it is important for researchers to consider how changes to the policy architecture and learning algorithms may have an impact on mixing behavior. The effect that the input variables sent to a policy could have on bounds on the mixing time was previously considered by the pioneering work of Kearns and Koller . In Theorem 1 we present a tighter version of the bound presented in their paper. Moreover, our paper is the first to highlight the connection between this result and the context length in partially observable settings in Theorem 2. Our paper focuses solely on how design decisions related to the policy's architecture can impact the mixing time and is complementary to recent insights about how changes to the learning algorithm, such as incorporation of replay  or multi-level critics , can lead to faster learning when mixing times are high.

**Balancing Context Length and Mixing Times:** The core insight of this paper is that there is a tradeoff to consider between increasing the context length and increasing the mixing time. When we increase the context length sent to an agent, we increase its ability to model non-Markovian dependencies. However, this increase in context length may also cause an increase in the mixing time, which makes it take longer to reliably evaluate an agent's policy. Dong et al.  propose the solution of restricting the class of policies they optimize over such that the mixing times are bounded, which is also the perspective we embrace in this work. However, Dong et al.  restrict this class of policies by limiting the planning horizon and thus making optimization more myopic. In our work, we highlight a different mechanism for restricting the mixing times of the policies we optimize over by simply limiting the context length. To this end, **our key contributions** are as follows:

1. We present theoretical analysis with supporting toy examples shedding light on how the input sent to a policy can influence its mixing time in Section 2.
2. In Section 3 we formally establish the connection between the context length considered by a policy in partially observable environments and the mixing time. We also empirically verify the relevance of this theory to Transformer based agents in partially observable settings.
3. Finally, in Section 4 we highlight the relevance of this theory when building foundation models that imitate a diverse set of behavior policies for RL. Specifically, we show that Decision Transformers  must use much larger context lengths than the policies that generated their dataset, and as a result require more interaction for reliable evaluation.

To reproduce our experiments see https://github.com/mattriemer/ContextLengthMixing.

## 2 Understanding How the Policy's Input Impacts the Mixing Time

Before discussing the connection between mixing times and context length in the next section, we first provide the technical grounding for readers to understand how the problem structure and the policy combine to jointly affect bounds on the mixing time. We will begin by detailing the problem formulation and present a novel upper bound for the mixing time in this context in Theorem 1. We then highlight key implications with illustrative example problems.

### Preliminaries: the Average Reward RL Setting and Definitions of the Mixing Time

**Problem Formulation:** We adopt the **average reward** setting to enable long-term analysis of agents. Please note that we only use this perspective to evaluate an agent's behavior and place no restriction on using discounting within the learning process (as we do in our experiments).4 We consider a finite, discrete-time, infinite horizon Markov Decision Process (MDP) [20; 21]: \(=,,T,R\), where \(\) is the set of states, \(\) is the set of actions, \(R:[0,R^{}]\) is the reward function, and \(T:\) is the environment transition probability function. At each time step, an agent perceives a state \(s\) and takes an action \(a\) drawn from a policy \(:\). The agent then receives a scalar reward drawn from the function \(R(s,a)\) and with probability \(T(s^{}|s,a)\) enters next state \(s^{}\). Markov chains may be periodic and have multiple recurrent classes, but optimality is difficult to define in such cases , making the following assumption necessary:

**Assumption 1**: _All stationary policies are aperiodic and unichain, resulting in a Markov chain with a single recurrent class that is recurrent in the Markov chain of every policy.5_

Any RL problem may be modified such that Assumption 1 holds by adding an arbitrarily small positive constant \(\) to all transition probabilities in \(T(s^{}|s,a)\). An important corollary is that the _steady-state distribution_\(^{}\) induced by the policy \(\) is independent of the initial state:

**Corollary 1**: _All policies \(\) induce a unique steady-state distribution \(^{}(s)=_{t}P^{}(s_{t}=s|s_{0})\) that is independent of the initial state such that \(_{s}^{}(s)_{a}(a|s)T(s^{}|s, a)=^{}(s^{})\;\; s^{}\)._

Corollary 1 implies that long-term rewards and thus the average reward per step objective \(()\) can be defined independently of the current state :

\[() :=_{m}_{t=1}^{m}_{ }R(s_{t},a_{t})=_{t}_{} R(s_{t},a_{t})\] \[=_{s}^{}(s)_{a}(a|s )R(s,a)\;.\]

Computing the average reward (i.e., the reward rate) with the last expression is limited by the amount of time the Markov chain induced by the policy \(T^{}(s^{}|s)=_{a}(a|s)T(s^{}|s,a)\) needs to be run for before reaching the distribution \(^{}(s)\). This amount of time is called the mixing time of the induced Markov chain. We denote \(t^{}_{}()\) as the _\(\)-mixing time_ of the chain induced by \(\):

\[t^{}_{}():=m\;_{s_{0}}d_{ }P^{}(s_{m}=|s_{0}),^{}() }\]

where \(d_{}\) is the total variation distance between the two distributions. The _conventional mixing time_ is defined as \(t^{}_{} t^{}_{}(1/4)\). This only gives insight about distributional mismatch with respect to the steady-state distribution, which led Kearns and Singh  to introduce the notion of a mismatch with respect to the reward rate. The _\(\)-return mixing time_ is a measure of the time it takes to formulate an accurate estimate of the true reward rate. Formally, if we denote the \(m\)-step average return starting from state \(s_{0}\) as \((,s_{0},m):=_{}[_{t=1}^{m}r_{t}|s_{0}]\), then we can define the _\(\)-return mixing time_ as:

\[t^{}_{}():=m\;(,s _{0},m^{})-()|,\] \[ s_{0} m^{} m }.\]

**An Interpretable Metric:** We will use this definition of the mixing time in our experiments as it directly measures the amount of time needed to evaluate a policy at a given threshold of precision \(\).

### Mixing in MDPs with Multidimensional States

**Multidimensional State Space:** We will also assume that problems have a state spaces comprised of multiple dimensions. Formally, each state is separated into \(n 1\) variables i.e., \(s=[s^{1},...,s^{n}]\) with \(s^{i}^{i}\) for all variables \(i\{1,...,n\}\) such that \(^{1}...^{n}\). It is also important to note that each state variable can be modeled independently based on the previous state across all variables such that \(T(s^{}|s,a)=_{i\{1,...,n\}}T(s^{ i}|s,a)\). In Appendix A.1 we discuss why this setting is strictly more general than the typical discrete MDP setting and does not limit the scope of our results.

**Dynamic Bayesian Network (DBN) Structure:** A Bayesian Network (BN) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG) and a dynamic Bayesian network (DBN) is a BN that relates variables to each other over adjacent time steps. In general, any Markov chain induced by a policy can be seen as a DBN over the state variables \(s^{1},...,s^{n}\) that is also influenced by an action variable \(a\) dictated by the policy . The dependency graph \(^{}\) for the DBN is a directed cyclic graph whose nodes are \(s^{1},...,s^{n}\) and where there is then a directed path from \(s^{i}\) to \(s^{j}\) in \(^{}\) iff \(s^{i}_{t}\) influences \(s^{j}_{t^{}}\) for some \(t^{}>t\) under policy \(\). While we can ignore actions as nodes in the graph, it is still important to consider their effect in forming causal connections among the state variables. Note that every node in \(^{}\) should influence itself and thus have a self-loop. If there is a direct path from \(s^{i}\) to \(s^{j}\) and from \(s^{j}\) to \(s^{i}\), the variables \(i\) and \(j\) are said to be in the same strongly connected component. In contrast, if there is either a directed path from \(s^{i}\) to \(s^{j}\) or \(s^{j}\) to \(s^{i}\), but not both, the variables \(i\) and \(j\) are weakly connected such that the one that has a causal influence on the other can be considered as ordered before the one it causes. Let \(^{}_{1},...,^{}_{t}\) be the maximal strongly connected components in \(^{}\) when an agent behaves with an arbitrary policy \(\) that is sorted such that if \(i<j\) there are no directed edges from \(^{}_{j}\) to \(^{}_{i}\). We can then define \(g^{}=_{i}|^{}_{i}|\) as the maximum number of variables in any strongly connected component of \(^{}\) and \(g=_{}g^{}\) as the maximum possible within a given policy class or parameterization \(\). In Appendix A.1 we provide examples to help readers better understand how this works and demonstrate that this formulation does not limit the scope of our results.

**Mixing Times and Coupling Times:** We can now leverage the well-studied connection between the mixing time and the so called _coupling time_ to provide an upper bound on the mixing time of the Markov chain induced by \(\). Let \(\) be the random variable that represents the _coupling time_ defined as the smallest \(m\) for which two Markov chains starting from different initial states are in the same state. Following the analysis provided in Lemma 5.2 of , for any \(\), let \(m\) be the number of steps such that for any two starting states in \(\) if we can say that \(P(>m)\) then the Markov chain is \(\)-mixed at time \(m\). For this analysis we also must introduce the parameter \(_{i,}\), which defines the minimum amount of common probability mass between any two state configurations for any state variable \(i\) so that \(=_{}_{i}_{i,}\) further corresponds to the minimum over all state variables \(i\) and possible parameterizations of the policy \(\):

\[_{i,}=_{s_{1},s_{2}}_{s^{i} ^{i}}T^{}(s^{i}|s_{1}),T^{}(s^{i}|s_{2})} .\]

We can now introduce a mixing time bound as a function of the strongly connected component structure. While our result has similar motivation to Theorem 5.4 of Kearns and Koller , we note that the proof of Theorem 1 outlined in Appendix A is entirely our novel contribution building off Lemma 5.2 and Definition 5.3 of Kearns and Koller . Additionally, there is no version of Kearns and Koller  online that includes the proof of Theorem 5.4.

**Theorem 1**: _(Strongly Connected State Variables Bound): If the Markov chain \(T^{}(s^{}|s)\) induced by policy \(\) has \(\) maximal strongly connected components in \(^{}\) with a maximum size of \(g\) state variables and a minimum of \(\) common probability mass between any two state configurations for any state variable, the mixing times \(t^{}_{}()\) and \(t^{}_{}()\) can be upper bounded:_

\[t^{}_{}() t^{}_{}() }log(1/).\]

**Proof Sketch:** Due to the sorting of the strongly connected components, our analysis is based on coupling each of the \(^{}_{i}\)'s in succession. Because it is possible that multiple \(^{}_{i}\)'s couple at the same step, every step where the Markov chain does not fully couple must be a step where some \(^{}_{i}\) does not couple. Our proof proceeds in the following high-level steps: 1) The probability of \(^{}_{i}\) coupling at a given step once \(^{}_{1},...,^{}_{i-1}\) have all already coupled is \(^{g}\). 2) Thus the probability of \(^{}_{i}\) not coupling at a step when \(^{}_{1},...,^{}_{i-1}\) have all already coupled is \((1-^{g})\). 3) So the joint probability of \(^{}_{i}\) not coupling for \(m_{i} 0\) steps when \(^{}_{1},...,^{}_{i-1}\) have all already coupled is \((1-^{g})^{m_{i}}\). 4) If \(>m\) then \(_{i=1}^{}m_{i}=m\) and the joint probability that \(m\)-steps have been spent not coupling in some \(^{}_{i}\) has a probability bound independent of the particular allocation of \(m\) into individual \(m_{i}\). Thus we can conclude that \(P(>m)(1-^{g})^{m}\). 5) Leveraging the identity that \(1-x e^{-x}\) for \(x 0\), we find that \(P(>m)(e^{-^{g}})^{m}\). 6) The Markov chain is \(\)-mixedif \(P(>m)\), so it must be \(\)-mixed if \((e^{-^{s}})^{m}\), which implies that \(m}log(1/)\). 7) Finally, we note the relationship between \(t^{}_{ret}()\) and \(t^{}_{mix}()\) following Lemma 1 of .

**Bound Tightness:** Theorem 1 is a tighter version of Theorem 5.4 presented by Kearns and Koller  by a factor of \(8\) from their result of \(}log(1/)\). This tighter bound underscores definitively that having more strongly connected components of a smaller size leads to tighter bounds on the mixing time than having fewer strongly connected components of a bigger size. In fact, without this contribution beyond the result by Kearns and Koller  it would not be possible to show our main result in Theorem 2 as it would thus be unclear if the mixing time bound gets tighter with the shrinking context length. An intuition we could provide for how this bound is achieved is that if the Markov chain has not coupled for \(m\) steps, each time step must be spent not coupling in at least one of the strongly connected components, which has a bounded conditional probability for any component. Our detailed proof is in Appendix A. Moreover, while it may seem that only a restricted class of Markov chains have \(>0\), in Corollary 2 of Appendix A, we demonstrate the extension of the definition for \(\) to the probability mass in common over \(c>1\) steps, which must be \(>0\) for some value of \(c\) due to Assumption 1. Our approach could also yield an even tighter yet more cumbersome bound in terms of the maximum probability that any strongly connected component does not couple rather than using \(\) and \(g\), which is important when we see significant variation in \(_{i}\) across variables.

**Conditioning on Variable Subsets:** One of the primary implications of Theorem 1 is that the mixing time of a policy is closely connected to the number of state variables that influence it's actions. For example, a policy in the class \((|s)\), which takes full states \(s=s^{1},...,s^{n}\) as input can be associated with \(_{n}\) and \(g_{n}\). Meanwhile, a policy in an alternative class that only considers some potentially time dependent arbitrary subset of \(n^{}(t) n\) state variables as input at any moment in time \(t\) such that \(s^{1},...,s^{n^{}(t)} s^{1},...,s^{n}\) can be associated with \(_{n^{}}\) and \(g_{n^{}}\). Lemma 1 in Appendix A then shows that the mixing time bound gets strictly tighter because \(1/_{n^{}}^{g_{n^{}}^{}} 1/_{n}^{g_{n}}\). We illustrate this phenomenon more concretely through two toy MDP examples detailed in Figure 1.

### Building an Intuition with Examples

We will consider the mixing times for the two examples highlighted in Figure 1 over the space of deterministic policies. As in Riemer et al. , we relax the definition of the \(\)-return mixing time using an average over start states (rather than a maximum) in order to emphasize mixing times encountered in practice. See Appendix B for details on our methodology for estimating mixing times.

**MDPs with Irrelevant Variables (Figure 1):** In the first example we consider, there are two independent state variables \(x\) and \(y\) that are both influenced by the agent's actions. As a result, \(x\) and \(y\) are in the same strongly connected component if both variables also influence the agent's actions. Because the reward only depends on \(x\), the agent can achieve the optimal policy without considering the \(y\) variable in its decision about actions. If the policy is only conditioned on the variable \(x\), \(x\) influences \(y\) through its actions, but \(y\) no longer has an influence on \(x\). As a result, the Markov chain induced by the policy that only conditions on the relevant variable \(x\) has two strongly connected components of size one such that \(g=1\) whereas the policy that conditions on both relevant and irrelevant variables potentially has a single strongly connected component of size two such that \(g=2\) with \(\) held constant. Based on Theorem 1, we expect the policy conditioned on both variables to experience higher mixing times even despite the small scale of this toy problem. Indeed, this is the case when we compute the \(\)-return mixing time at a reward rate precision of \(=0.05\), which must reflect a minimum of 10% relative estimation error because no policy has a reward rate of \(0.5\). The optimal policy has a mixing time of \(17.6\) and only depends on the variable \(x\). In fact, no policy only depending on \(x\) has a mixing time above \(44.9\). Meanwhile, there is a policy dependent on both variables that is within the reward rate precision threshold \(\) of the optimal policy with a mixing time of \(45.2\), and a policy conditioned on both variables with a mixing time as high as \(340.3\).

**Impact of Reward Density:** To test if a tighter result is possible with respect to the ratio between the maximum mixing times predicted by Theorem 1, we have tried a setting where the reward is still \(1.0\) when \(x=x0\), but edited to \(-0.25\) when \(x=x1\) and \(-1.5\) when \(x=x2\) rather than \(0\). This setting was chosen to have no impact on the optimal policy while densifying rewards without making a substantial change to the magnitude of best reward rates achieved. The ratio between the maximum mixing times is now improved to 8.8 times smaller (\(233.0\) vs. \(2049.7\)) when only focusing on the relevant variables rather 7.6 times smaller (\(44.9\) vs. \(340.3\)) with the previous sparse rewards. Note that the reward still has no dependence on \(y\) and is thus invariant to a large part of the change in the state. So, mixing is still substantially faster with respect to the reward (\(t_{}^{}()\)) than the state (\(t_{}^{}()\)).

**MDPs with Independent Subtasks (Figure 1)**: We can now consider an example with three variables \(x\), \(y\), and \(z\) where the agent's actions influence both \(x\) and \(y\). A policy conditioned on all three variables has a strongly connected component of size one including \(z\) (because \(z\) is not influenced by the agents actions or \(x\) or \(y\)) and a strongly connected component of size two including \(x\) and \(y\) such that \(g=2\). On the other hand, if the policy were able to condition only on \(x\) when \(z=z0\) and only condition on \(y\) when \(z=z1\), then the actions that still influence both \(x\) and \(y\) no longer serve to make their values dependent on each other. This results in three strongly connected components of size one such that \(g=1\) with \(\) held constant. One for each variable \(x\), \(y\), and \(z\). Based on the analysis of Theorem 1, we would again expect the policy conditioned on all variables always to experience higher mixing times than the policy conditioned on only the relevant variables for the specific subtask even despite the small scale of this toy problem. As for the previous example, we compute the \(\)-return mixing time at a reward rate precision of \(=0.05\), reflecting a minimum of 10% relative estimation error. The optimal policy has a mixing time of \(16.0\) and only depends on the variable \(x\) when \(z=z0\) and variable \(y\) when \(z=z1\). In fact, no policy structured like this has a mixing time above \(267.1\). Meanwhile, there is a policy dependent on all variables that is within the reward rate precision threshold \(\) of the optimal policy with a mixing time of \(341.2\). Moreover, there is a policy conditioned on all variables with a mixing time as high as \(968.1\). This example helps illustrate the important fact that a policy can limit its mixing time while still conditioning on all variables if the subset it conditions on has some time dependence.

## 3 Understanding How the Context Length Impacts the Mixing Time

In the previous section we were concerned only with fully observable MDPs, but real-world and large scale problems tend to be partially observable. We will begin by highlighting new notation that allows us to talk about notions of optimality as a function of the interaction history context length and present mixing time bounds that are dependent on the context length considered by a policy. We go on to conduct experiments that verify the relevance of this analysis during online RL in partially observable domains and demonstrate that Transformer neural network models actually experience higher mixing times than alternative function approximators and tabular models.

Figure 1: **Illustrative Toy Examples.** The above figure details the three relevant variables \(x\), \(y\), and \(z\) that we will consider for our toy examples. Note the action \(a0\) is interpreted as \(a1\) or vice versa with a 10% failure probability, which is the same as the rate at which \(z\) switches regardless of the agentâ€™s actions. **Irrelevant Variables Example:** In this case, we consider only the variables \(x\) and \(y\) (\(z\) is not needed in this example) where the reward is \(+1\) if \(x=x0\) and \(0\) otherwise. The result is that variable \(x\) is relevant to the task whereas variable \(y\) is irrelevant. **Independent Subtasks Example:** In this case, we consider that the variable \(x\) evolves (according to the diagram) with \(y\) remaining constant when \(z=z0\) and the variables \(y\) evolves with \(x\) remaining constant when \(z=z1\). The reward is \(+1\) if \(z=z0\) and \(x=x0\) or if \(z=z1\) and \(y=y0\), or \(0\) otherwise.

### Partially Observable Environments with Local Observation Structure

**Partially Observable Environments:** We can extend the notion of an MDP from the previous section to consider an unknowable but ultimately stationary Partially Observable Markov Decision Process (POMDP) , which is comprised of an augmented tuple \(,,,R,T,O\). This adds to the definition of an MDP \(\) an observation space \(\) and an observation function \(O(o|s)\) that maps states \(s\) to observations \(o\).6 The state \(s\) of such a POMDP ultimately serves as a theoretical quantity that is never actually observed from the agent's perspective and the observations \(o\) that it does receive may have an arbitrarily non-Markovian relationship.

**Interaction Histories:** At time \(t\) the union of all things the agent has observed about an environment can be called its interaction history \(h_{t}:=\{o_{0},a_{0},r_{0},o_{1},a_{1},r_{1},...,o_{t}\}\).7 We can also consider the case where the agent only maintains a finite window of history with size \(k\) i.e., \(h_{t}^{(k)}:=\{o_{t-k+1},a_{t-k+1},r_{t-k+1},x_{t-k+2},a_{t-k+2},r_{t-k+2},..., o_{t}\}\) where our notation is chosen so that a memoryless policy that only processes the current observation corresponds to \(k=1\). We will henceforth call \(k\) the context length of the interaction history. A given POMDP \(\) is non-Markovian to order \(k_{}\) which implies that for every combination of state \(s\), action \(a\), and history window \(h^{(k)}\), \(T(|s,a)=T(|h^{(k)},a)\) and \(R(|s,a)=R(|h^{(k)},a)\  k k_{}\).

**Optimality in POMDPs:** An advantage of using a policy that processes the full interaction history is that it is possible to handle arbitrarily non-Markovian environments, but this is not computationally scalable in the long-run. As \(t\), \(_{}(|h_{t})\) processes an infinite length sequence \(h_{t}\). In practice, we must limit the context length sent to the policy. The best policy \(_{}(|h_{t}^{(k)})\) over a finite context length has the same optimal reward rate as the best policy \(_{^{}}(|s_{t})\) over true state inputs when \(k k_{}\) such that \(_{t}_{}_{_{}( |h_{t}^{(k)})}[r_{t}]=_{t}_{^{}^ {}}_{_{^{}}(|s_{t})}[r_{t}]\).8 However, this implies the context length must scale with \(k_{}\), which could become quite large in highly non-Markovian real-world environments where achieving optimality is difficult.

**The Effect of Context Length:** As described in the previous section, we are interested in multi-dimensional state spaces consisting of \(n\) variables and in particular problems where \(n\) is large. A given observation \(o\) generated from \(O(o|s)\) may be then caused by only a subset of the state variables \((o)\{1,..,n\}\) where \(Par(o)\) denotes the causal parent state variables of \(o\). As such, a finite history window \(h^{(k)}\) may also be caused by a subset of the state variables because the same can be said for rewards and actions that are potentially included in this input. In general, a finite context length is reflective of a maximum \(n_{k}(t)\) sized subset of the state variables at a time \(t\) where \(n n_{k^{}}(t) n_{k}(t)\) if context length \(k^{} k\) for all \(t\). This is because \(Par(h^{(k)}) Par(h^{(k^{})})\{1,..,n\}\). Theorem 2 then follows from the combined results of Lemma 1 in Appendix A and Theorem 1.

**Theorem 2**: _(Limiting Mixing Times with Context Length): If the Markov chain induced by a policy conditioned on a finite interaction history window \((|h^{(k)})\) with a context length of \(k\) has \(_{k}\) maximal strongly connected components in \(^{}\) with a maximum size of \(g_{k}\) variables and a minimum of \(_{k}\) common probability mass between any two state configurations, the mixing times \(t_{}^{}()\) and \(t_{}^{}()\) can be bounded for any \(k^{} k\):_

\[t_{}^{}() t_{}^{}() ^{g_{k}}}log(1/)\] \[}^{g_{k^{} }}}log(1/)\]

_where the dependence on the context length implies that \(0_{k^{}}_{k} 1\) and \(g_{k^{}} g_{k} 1\)._

**Proof Sketch:** Lemma 1 in the appendix considers the mixing time relationship of policy classes conditioned on subsets of the state variables that other policy classes are conditioned on. Ourproof includes the following high-level steps by applying our notation in which \(k^{} k\) for all \(t\) to the results of Theorem 1 and Lemma 1: 1) We consider the causal parent state variables of each observation, action, and reward to conclude that \(Par(h^{(k)}) Par(h^{(k^{})})\{1,..,n\}\), which implies that \(n n_{k^{}}(t) n_{k}(t)\). 2) Through Lemma 1 we show that by rule of Cartesian products over subsets that \(0_{k^{}}_{k} 1\). 3) Through Lemma 1 we also demonstrate that \(g_{k^{}} g_{k} 1\) because causal connections in \(^{}\) are only added and not removed when the context length is increased. 4) This then implies that \(1/_{k^{}}^{g_{k^{}}} 1/_{k}^{g_{k}}\), which is sufficient to prove Theorem 2 using Theorem 1 because \(\) is independent of \(k\).

**Formalizing the Tradeoff:** Theorem 2 formally establishes a novel connection between the context length \(k\) of a policy and bounds on its mixing time. Growing \(k\) increases the performance of the best achievable policy when \(k_{}\) is high. However, any increase in \(k\) leads to a looser mixing time bound.

**When it Really Matters:** While this bound is always true, there may not be a meaningful dependence on \(k\) for some problems. This includes very simple environments where \(k_{} 1\) or environments with no local structure where all state variables influence all observations. However, if changing the context length \(k\) does actually lead to a change in the strongly connected component structure, the impact on the mixing time will be exponential. In Figure 2 we present an illustration of the kinds of problems where the chosen value of \(k\) may make a large difference in the value of \(n_{k}(t)\). We can see that at each time only a subset of the total state variables contributes to the local observations of the agent and that which variables they are has a degree of local consistency across time-steps of the Markov chain induced by the policy in the environment. When observations are only caused by a subset of the state variables at each step, there is more potential to break the problem up into independent subtasks as in our example in the previous section.

### Empirical Verification During Online RL

**Context Length and Encountered Mixing Times:** While Theorem 2 draws a clear connection between context length and mixing times, there still remains a question about if this analysis is too conservative and it is unclear if these policies will actually be encountered during learning. To test this question empirically, we consider the simple RGB world environment in Figure 5(a), which is modeled after the classic T-maze  environment. We conducted comprehensive experiments leveraging tabular Q-learning for 1 billion steps over 100 seeds (see Appendix B). In Figure 3 we highlight that longer context lengths experience larger average mixing times at the same approximate reward rate as policies conditioned on lower context lengths during learning. When interpreting this kind of plot, it is important to consider that there is a difference between the input required to express a policy and the input available to that policy. Any policy that receives a sufficient statistic of the environment state as input i.e. \(k 2\) will arrive at the same optimal policy by the end of training, so what is more interesting is the points encountered along the way.

**Effect of Neural Network Architecture:** We extend these experiments to prominent neural network architectures on the same problem leveraging Q-learning (see Appendix B). We plot the result when each architecture has about 1M parameters in Figure 4. All models have the same mixing time when they arrive at the same policy i.e. the optimal policy. However, at intermediate reward rates we see a higher average mixing time for Transformer models. It is important to note that sometimes an architecture will learn to achieve a reward rate that others never learn to achieve. In this case, there

Figure 3: **Mixing Times Encountered vs. Context Length. We plot the average mixing time and 95% confidence intervals encountered during 1 billion steps of learning over 100 seeds at each context length \(k\). We bin the average mixing time computation by the nearest 0.1 increment of the reward rate of the policy. \(k=1\) is not visible because the reward rate is always 0.**

Figure 2: **POMDPs with Local Observation Structure. An example of an environment with a multidimensional state space where near term observations are only causally dependent on a subset of the variables. At time \(T>>t\) we see different dimensions of the state space influencing observations and thus these separate subsets do not need to be in the same strongly connected component.**

is not a clear basis for comparison. We also ran experiments evaluating the role of the number of parameters. The extra capacity did not have a statistically significant effect on the mixing times for MLP and LSTM models. Meanwhile, extra capacity resulted in higher encountered mixing times throughout learning and a larger effect when increasing \(k\) for Transformer models (see Figure 8). It appears that attention mechanisms make it easier to focus on the full context rather than i.e. only the recent parts and that this capability is predictably enhanced when the model capacity is increased. In Appendix B we take a closer look at the Transformer attention maps in the decoder and present some evidence that the larger models are paying attention more uniformly to the entire context.

## 4 Understanding Growing Context Lengths in Foundation Models for RL

**Foundation Models for RL:** Large scale foundation models trained to recreate behaviors from a large and diverse distribution have recently had a disrupting effect across the field of AI. Concretely, a foundation model leverages an offline dataset \(\), containing data from \(N\) different now unknown behavior policies \(_{i}\) for \(i\{1,...,N\}\). Decision Transformers  is a popular approach that achieved state of the art performance in offline RL by treating learning as a sequence modeling problem akin to language modeling. The objective is to imitate the behavior of all policies \(_{i}\) in the dataset using a window of their interaction history \(h^{(k)}\). See Appendix B for additional details.

**Fitting Training Data:** Foundation models may struggle to model the training data when the context window length \(k\) used for training is less than the maximum context window of any policy \(k_{}:=_{i\{1,...,N\}}k_{i}\). The reason is because our model must not only produce the behavior of each agent \(_{i}\), but also, must store additional context to disambiguate \(_{i}\), from all \(_{j}\)\( j\{1,...,N\} i\). Thus it will be necessary to consider large contexts, well beyond that of any context length used by the behavior policies, especially as the number and diversity of behavior policies considered grows.

**Comparison to Behavior Policy Context Lengths:** To understand the connection between the behavior policy context length and the context length needed to train Decision Transformers, we consider a similar setup to the Key-Door experiments in the original paper , but with the publicly available Minigrid Crossing environment  (Figure 5(b)). We randomly initialized 1,000 deterministic behavior policies, each rolled out for one episode in the environment to collect dataset \(\) leveraging a CNN architecture following past work on the Minigrid domain [30; 31]. The context length of each behavior policy is set to 1, implying \(k_{}=1\). In Figure 5 we plot the training accuracy achieved by a Decision Transformer model as a function of its context length \(k\). We report that the training accuracy is only optimal in general for \(k 50\), which is significantly higher than \(k_{}=1\) with \(k=25\) reaching optimal performance for some random seeds. We also considered 1,000 episodes generated by different stochastic behavior

Figure 4: **Policy Architecture vs. Encountered Mixing Times.** We plot the average mixing time and 95% confidence intervals for each choice of policy architecture between tabular, MLP, LSTM, and Transformer models with averages binned by the reward rate. We provide a representative example with \(k=5\) and include comprehensive plots in Appendix B. Mixing times goes down as the absolute value of the reward rate gets close to \(0\) as a consequence of the sparse and local reward structure.

Figure 5: **Context Length vs. Training Accuracy.** We plot the achieved training accuracy and 95% confidence intervals across 5 random seeds as a function of the Decision Transformer context length in the crossing environment with 1,000 episodes of data generated by either random behavior policies with a context length \(k=1\) or a REINFORCE based learning agent using a context length \(k=1\).

policies of REINFORCE based learning agents  that learn following every episode. Figure 5 demonstrates that it is even harder to model these policies. This makes sense both because the policy distinctions are more subtle and less diverse between episodes and because the policies are stochastic, leaving an irreducible source of uncertainty.

**Evaluating Learned Models:** It could be argued that, although a larger context length may be required to capture the complete range of behavior policies compared to each policy individually, this merely promotes overfitting of the training data and does not effectively enhance downstream performance. So, to see if Decision Transformers need near optimal training accuracy for good downstream performance, we evaluated each Decision Transformer model for a particular random policy seed where \(k=25\) is able to achieve optimal training performance across a variety of return to go prompts stepping by \(0.01\) from \(0\) to \(1.0\) and report the average performance across 1,000 episodes. We plot our results in Figure 9 which validate the importance of fitting the training data. Decision Transformer with \(k=25\) can achieve performance as good as any behavior policy used to generate the data. Meanwhile, models with smaller \(k\) fail to achieve the same performance. Moreover, increasing context length \(k\), while important for optimization, raises the mixing time. The average mixing time with precision \(=0.01\) across policies and start states for the Decision Transformer with \(k=25\) is 298.1 episodes or 86,834.3 steps while the average mixing time is merely 11.2 episodes or 3,271.2 steps for the behavior policy across policies and start states. Figure 9 also demonstrates the not very surprising conclusion that unnecessarily large context lengths are more prone to overfitting than the minimal context length that achieves 100% training accuracy.

## 5 Discussion and Future Work

In this work, we have highlighted the potential limitations of training models conditioned on ever increasing context lengths and particularly the effect that these growing context lengths have on mixing times. This motivates a number of interesting research questions to explore moving forward.

**New Architectures and Algorithms:** Most work on RL that even acknowledges the challenges associated with high mixing times does so with a defeatist mentality, assuming that problems with high mixing times are unavoidably harder and that there is basically nothing that could be done about it. Our work highlights that this isn't actually true and that the policy class we choose to optimize over itself can have a big impact on mixing properties. What our paper shows in Theorem 2 is that what leads to potentially high mixing times is when our model leverages a monolithic representation that is highly sensitive to a large part of the interaction history at all times. This is particularly descriptive of how vanilla transformers work, but there are multiple already existing research directions that seem well suited to scaling to high context lengths while providing less history sensitivity at each step. We refer interested readers to Appendix D for an in depth discussion of related directions.

**Scaling to Complex Environments:** We believe the settings of greatest relevance to our work are those related to continual or multi-task environments where agents are evaluated as generalists over a number of skills rather than just solving a single narrow task. As such, we believe that focus on the difficulties presented by high mixing times is timely in the age of foundation models. As mentioned at the end of Section 3.1, our analysis will not have relevance in problems where there are few state variables that each impact every observation. However, composite tasks that test a number of sub-skills naturally tend to have many total state variables with relatively few impacting each observation. So, for example, simple Atari domains will not suffer from high mixing times, but i.e. continual learning over multiple Atari games will  as a result of the sparsity of causal impact of variables across games. Broadly speaking, AI assistant tasks that include providing help on a number of topics rather than just one should also suffer from issues with high mixing times.

**Evaluation of Foundation Models:** Our work additionally highlights how the way we pretrain foundation models may make high confidence evaluation of these models more difficult. Trusted evaluation of foundation models remains an important challenge for the research community to grapple with. Towards this end, our work is the first to establish that more interaction is needed to reliably evaluate models that have larger context lengths. This novel perspective is very important for researchers to consider as these models are increasingly being deployed in the real-world.