# Learning to compute Grobner bases

Hiroshi Kera

Chiba University,

Zuse Institute Berlin

kera@chiba-u.jp

&Yuki Ishihara

Nihon University

ishihara.yuki@nihon-u.ac.jp

&Yuta Kambe

Mitsubishi Electric

kambe.yuta@bx.mitsubishielectric.co.jp

&Tristan Vaccon

Limoges University

tristan.vacon@unilim.fr

&Kazuhiro Yokoyama

Rikkyo University

kazuhiro@rikkyo.ac.jp

corresponding author

###### Abstract

Solving a polynomial system, or computing an associated Grobner basis, has been a fundamental task in computational algebra. However, it is also known for its notorious doubly exponential time complexity in the number of variables in the worst case. This paper is the first to address the learning of Grobner basis computation with Transformers. The training requires many pairs of a polynomial system and the associated Grobner basis, raising two novel algebraic problems: random generation of Grobner bases and transforming them into non-Grobner ones, termed as backward Grobner problem. We resolve these problems with 0-dimensional radical ideals, the ideals appearing in various applications. Further, we propose a hybrid input embedding to handle coefficient tokens with continuity bias and avoid the growth of the vocabulary set. The experiments show that our dataset generation method is a few orders of magnitude faster than a naive approach, overcoming a crucial challenge in learning to compute Grobner bases, and Grobner computation is learnable in a particular class.

## 1 Introduction

Understanding the properties of polynomial systems and solving them have been a fundamental problem in computational algebra and algebraic geometry with vast applications in cryptography [8; 93], control theory , statistics [27; 41], computer vision , systems biology , and so forth. Special sets of polynomials called Grobner bases  play a key role to this end. In linear algebra, the Gaussian elimination simplifies or solves a system of linear equations by transforming its coefficient matrix into the reduced row echelon form. Similarly, a Grobner basis can be regarded as a reduced form of a given polynomial system, and its computation is a generalization of the Gaussian elimination to general polynomial systems. However, computing a Grobner basis is known for its notoriously bad computational cost in theory and practice. It is an NP-hard problem with the doubly exponential worst-case time complexity in the number of variables [29; 67]. Nevertheless, because of its importance, various algorithms have been proposed in computational algebra to obtain Grobner bases in better runtime. Examples include Faugere's F4/F5 algorithms [33; 34] and M4GB .

In this study, we investigate Grobner basis computation from a learning perspective, envisioning it as a practical compromise to address large-scale polynomial system solving and understanding when mathematical algorithms are computationally intractable. The learning approach does not require explicit design of computational procedures, and we only need to train a model using a large amount of (non-Grobner set, Grobner basis) pairs. Further, if we restrict ourselves to a particular class of Grobner bases (or associated _ideals_), the model may internally find some patterns useful for prediction. The success of learning indicates the existence of such patterns, which encourages the improvement of mathematical algorithms and heuristics. Several recent studies have already addressed mathematical tasks via learning, particularly using Transformers [14; 19; 58]. For example,  showed that Transformers can learn symbolic integration simply by observing many \((\,f/x\,f)\) pairs in training. The training samples are generated by first randomly generating \(f\) and computing its derivative \(\,f/x\) and/or by the reverse process.

However, a crucial challenge in the learning of Grobner basis computation is that it is mathematically unknown how to efficiently generate many (non-Grobner set, Grobner basis) pairs. We need an efficient backward approach (i.e., _solution-to-problem_ computation) because, as discussed above, the forward approach (i.e., _problem-to-solution_ computation) is prohibitively expensive. To this end, we frame two problems: (i) a random generation of Grobner bases and (ii) a backward transformation from a Grobner basis to an associated non-Grobner set. To our knowledge, neither of them has been addressed in the study of Grobner bases because of the lack of motivations; all the efforts have been dedicated to the forward computation from a non-Grobner set to Grobner basis.

Another challenge in the learning approach using Transformers lies in the tokenization of polynomials on infinite fields, such as \(\) and \(\). To cover a wide range of coefficients, one has to either prepare numerous number tokens or split a number into digits. The former requires a large embedding matrix, and the latter incurs large attention matrices due to the lengthy sequences. To resolve this, we introduce a continuous embedding scheme, which embeds coefficient tokens by a small network and avoids the tradeoff between vocabulary size and sequence length. The continuity of the function realized by the network naturally implements the continuity of the numbers in the embedding.

We summarize the contributions as follows.

* We investigate the first learning approach to the Grobner computation using Transformers and experimentally show its learnability. Unlike most prior studies, our results indicate that training a Transformer may be a compromise to NP-hard problems to which no efficient (even approximate or probabilistic) algorithms have been designed.
* We uncovered two unexplored algebraic problems--random generation of Grobner bases and backward Grobner problem and propose efficient methods to address them in the 0-dimensional case. The problems are essential to the learning approach but also algebraically interesting and need interaction between computational algebra and machine learning.
* We propose a new input embedding to efficiently handle a large range of coefficients without the tradeoff between the size of the embedding matrix and attention maps.

Our experiments show that the proposed dataset generation is highly efficient and faster than a baseline method by a few orders of magnitude. Further, we observe a learnability gap between polynomials on finite fields and infinite fields while predicting polynomial supports are more tractable.

## 2 Related Work

Grobner basis computation.Grobner basis is one of the fundamental concepts in algebraic geometry and commutative ring theory [24; 39]. By its computational aspect, Grobner basis is a very useful tool for analyzing the mathematical structures of solutions of algebraic constraints. Notably, the form of Grobner bases is suited for finding solutions and allows parametric coefficients, and thus, it is vital to make Grobner basis computation efficient and practical in applications. Following the definition of Grobner bases in , the original algorithm to compute them can be presented as (i) create potential new leading terms by constructing _S-polynomials_, (ii) reduce them either to zero or to new polynomials for the Grobner basis, and (iii) repeat until no new S-polynomials can be constructed. Plenty of work has been developed to surpass this algorithm. There are four main strategies: (a) avoiding unnecessary S-polynomials based on the F5 algorithm and the more general _signature-based algorithms_[9; 34]. Machine learning appeared for this task in . (b) More efficient reduction using efficient linear algebraic computations using  and the very recent GPU-using . (c) Performing _modular computations_, following [6; 70], to prevent _coefficient growth_ during the computation. (d) Using the structure of the ideal, e.g., [13; 35] for change of term ordering for 0-dimensional ideals or  when the Hilbert function is known. In this study, we present the fifth strategy: (e) Grobner basis computation fully via learning without specifying any mathematical procedures.

Transformers for mathematics.Recent studies have revealed that Transformers can be used for mathematical reasoning and symbolic computation. The training only requires samples (i.e., problem-solution pairs), and no explicit mathematical procedures need to be specified. In , the first study that uses Transformers for mathematical problems is presented. It showed that Transformers can learn symbolic integration and differential equation solving with training with sufficiently many and diverse samples. Since then, Transformers have been applied to checking local stability and controllability of differential equations , polynomial simplification , linear algebra [19; 20], symbolic regression [14; 26; 44; 45], Lyapunov function design  and attacking the LWE cryptography [61; 89]. In , comprehensive experiments over various mathematical tasks are provided. In contrast to the aforementioned studies, we found that Grobner basis computation, an NP-hard problem, even has an algebraic challenge in the dataset generation. This paper introduces unexplored algebraic problems and provides an efficient algorithm for a special but important case (i.e., 0-dimensional ideals), thereby realizing an experimental validation of the learning of Grobner basis computation.

## 3 Notations and Definitions

We introduce the necessary notations and definitions in this Section. The reader interested in a gentle introduction to Grobner basis theory can refer to the classical book . For their comfort, we have moreover compiled most elementary additional definitions and notations in App. A.

We consider a polynomial ring \(k[x_{1},,x_{n}]\) with a field \(k\) and variables \(x_{1},,x_{n}\). For a set \(F k[x_{1},,x_{n}],\) the ideal generated by \(F\) is denoted by \( F\). Once a term order on the terms of \(k[x_{1},,x_{n}]\) is fixed, one can define leading terms and Grobner bases.

**Definition 3.1** (Leading term).: Let \(F=\{f_{1},,f_{s}\} k[x_{1},,x_{n}]\) and let \(\) be a term order. The leading term \((f_{i})\) of \(f_{i}\) is the largest term in \(f_{i}\) in ordering \(\). The leading term set of \(F\) is \((F)=\{(f_{1}),,(f_{ s})\}\).

**Definition 3.2** (Grobner basis).: Fix a term order \(\). A finite subset \(G\) of an ideal \(I\) is said to be a \(\)_-Grobner basis_ of \(I\) if \((G)=(I)\).

The condition \((G)=(I)\) means that for any element \(h I\), the leading term \((h)\) is divided by the leading term \((g)\) of an element \(g G\). It gives a complete test whether a given polynomial \(h\) is in \(I\) or not by polynomial division with \(G\), similar to Gaussian elimination by a basis of a vector space. The remainder is 0 means that \(h I\), and otherwise means that \(h I\). This is related to finding solutions. Roughly speaking, if \(h I= f_{1},,f_{s}\), then we have a form \(h=_{i=1}^{s}h_{i}f_{i}\) and the system \(f_{1}(x_{1},,x_{n})==f_{s}(x_{1},,x_{n})=0\) shares solutions with the equation \(h(x_{1},,x_{n})=0\).

Note that \((G)(I)\) is trivial from \(G I\). The nontriviality of the Grobner basis lies in \((G)(I)\); that is, a finite number of leading terms can generate the leading term of any polynomial in the infinite set \(I\). The Hilbert basis theorem  guarantees that every ideal \(I\{0\}\) has a Grobner basis. Moreover, using the multivariate division algorithm, one gets that any Grobner basis \(G\) of an ideal \(I\) generates \(I\). We are particularly interested in the _reduced_ Grobner basis \(G\) of \(I= F\), which is unique once the term order is fixed.

Intuition of Grobner bases and system solving.Let \(G=\{g_{1},,g_{t}\}\) be a Grobner basis of an ideal \( F= f_{1},,f_{s}\). The polynomial system \(g_{1}(x_{1},,x_{n})==g_{t}(x_{1},,x_{n})=0\) is a simplified form of \(f_{1}(x_{1},,x_{n})==f_{s}(x_{1},,x_{n})=0\) with the same solution set. With the term order \(_{}\), \(G\) has a form \(g_{1} k[x_{n_{1}},,x_{n}],g_{2} k[x_{n_{2}},,x_{n}],,g_{ t} k[x_{n_{t}},,x_{n}]\) with \(n_{1} n_{2} n_{t}\), which may be regarded as the "reduced row echelon form" of a polynomial system. In our particular case (i.e., 0-dimensional ideals in shape position; cf. Sec. 4.2), we have \((n_{1},n_{2},,n_{t})=(1,2,,n)\). Thus, one can obtain the solutions of the polynomial system usinga backward substitution, i.e., by first solving a univariate polynomial \(g_{t}\), next solving bivariate polynomial \(g_{t-1}\), which becomes univariate after substituting the solutions of \(g_{t}\), and so forth.

Other notations.The subset \(k[x_{1},,x_{n}]_{ d} k[x_{1},,x_{n}]\) denotes the set of all polynomials of total degree at most \(d\). For a polynomial matrix \(A k[x_{1},,x_{n}]^{s s}\), its determinant is given by \((A) k[x_{1},,x_{n}]\). The set \(_{p}\) with a prime number \(p\) denotes the finite field of order \(p\). The set \((n,k[x_{1},,x_{n}])\) denotes the set of upper-triangular matrices with all-one diagonal entries (i.e., unimodular upper-triangular matrices) with entries in \(k[x_{1},,x_{n}]\). The total degree of \(f k[x_{1},,x_{n}]\) is denoted by \((f)\).

## 4 New Algebraic Problems

Our goal is to realize Grobner basis computation through a machine learning model. To this end, we need a large training set \(\{(F_{i},G_{i})\}_{i=1}^{m}\) with finite polynomial set \(F_{i} k[x_{1},,x_{n}]\) and Grobner basis \(G_{i}\) of \( F_{i}\). As the computation from \(F_{i}\) to \(G_{i}\) is computationally expensive in general, we instead resort to _backward generation_ (i.e., solution-to-problem process); that is, we generate a Grobner basis \(G_{i}\) randomly and transform it to non-Grobner set \(F_{i}\).

What makes the learning of Grobner basis computation hard is that, to our knowledge, neither (i) a random generation of Grobner basis nor (ii) the backward transform from Grobner basis to non-Grobner set has been considered in computational algebra. Its primary interest has been instead posed on Grobner basis computation (i.e., forward generation), and nothing motivates the random generation of Grobner basis nor the backward transform. Interestingly, machine learning now sheds light on them. Formally, we address the following problems for dataset generation.

**Problem 4.1** (Random generation of Grobner bases).: _Find a collection \(=\{G_{i}\}_{i=1}^{m}\) with the reduced Grobner basis \(G_{i} k[x_{1},,x_{n}]\) of \( G_{i}\), \(i=1,,m\). The collection should contain diverse bases, and we need an efficient algorithm for constructing them._

**Problem 4.2** (Backward Grobner problem).: _Given a Grobner basis \(G k[x_{1},,x_{n}]\), find a collection \(=\{F_{i}\}_{i=1}^{}\) of polynomial sets that are not Grobner bases but \( F_{i}= G\) for \(i=1,,\). The collection should contain diverse sets, and we need an efficient algorithm for constructing them._

Problems 4.1 and 4.2 require the collections \(,\) to contain diverse polynomial sets. Thus, the algorithms for these problems should not be deterministic but should have some controllable randomness. Several studies reported that the distribution of samples in a training set determines the generalization ability of models trained on it [19; 58]. However, the distribution of non-Grobner sets and Grobner bases is an unexplored and challenging object of study. It can be another challenging topic and goes beyond the scope of the present study.

### Scope of this study

Non-Grobner sets have various forms across applications. For example, in cryptography (particularly post-quantum cryptosystems), polynomials are restricted to dense degree-2 polynomials and generated by an encryption scheme . On the other hand, in systems biology (particularly, reconstruction of gene regulatory networks), they are typically assumed to be sparse . In statistics (particularly algebraic statistics), they are restricted to binomials, i.e., polynomials with two monomials [41; 79].

As the first study of Grobner basis computation using Transformers, we do not focus on a particular application and instead address a generic case reflecting a motivation shared by various applications of computing Grobner basis: solving polynomial systems or understanding ideals associated with polynomial systems having solutions. Particularly, we focus on \(0\)-dimensional radical ideals, a special but fundamental class of ideals.

**Definition 4.3** (\(0\)-dimensional ideal).: Let \(F\) be a set of polynomials in \(k[x_{1},,x_{n}]\). An ideal \( F\) is called a _\(0\)-dimensional ideal_ if all but a finite number of terms belong to \(( F)\).

In fact, the number of terms not belong to \(( f_{1},,f_{s})\) is an upper bound of the number of solutions of the system \(f_{1}(x_{1},,x_{n})==f_{s}(x_{1},,x_{n})=0\). In particular, the finiteness of the number of terms not belong to \(( f_{1},,f_{s})\) implies the finiteness of the number of solutions. This is the reason why we call such ideals "\(0\)-dimensional" ideals in Def. 4.3.

\(0\)-dimensional ideals are the fundamental ideals in the study of pure algebra. This is partly because of the ease of analysis. As Def. A.5 shows, \(0\)-dimensional ideals relate to finite-dimensional vector spaces, and thus, analysis and algorithm design can be essentially addressed by matrices and linear algebra.

Also ideals in most practical scenarios are known to be 0-dimensional. For example, a multivariate public-key encrypted communication (a candidate of post-quantum cryptosystems) with a public polynomial system \(F\) over a finite field \(_{p}\) will be broken if one finds any root of the system \(F\{x_{1}^{p}-x_{1},,x_{n}^{p}-x_{n}\}\)). One should note that the ideal \( F\{x_{1}^{p}-x_{1},,x_{n}^{p}-x_{n}\}\) is 0-dimensional [84, Sec. 2.2]. Generically, 0-dimensional ideals defined from polynomial systems having solutions are radical2 (i.e., non-radical ideals are in a zero-measure set in the Zariski topology). The proofs of the results in the following sections can be found in App. C. Hereinafter, the sampling of polynomials is done by a uniform sampling of coefficients from a prescribed range.

It is also worth noting that Transformers cannot be an efficient tool for _general_ Grobner basis computation, and thus, we should focus on a particular class of ideals and pursue in-distribution accuracy. This is evident from the facts that Grobner basis computation is NP-hard and that machine learning models perform best on in-distribution samples and do not generalize perfectly. Fortunately, unlike standard machine learning tasks (e.g., image classification task), users can frame their problems beforehand (i.e., they know what types of polynomials they want to handle), and they can collect as many training samples as they want if an efficient algorithm exists. As mentioned above, the form of non-Grobner sets varies across applications, and thus, we focus on the generic case and leave the specialization to future work.

### Random generation of Grobner bases

We address Prob. 4.1 using the fact that 0-dimensional radical ideals are generally _in shape position_.

**Definition 4.4** (Shape position).: Ideal \(I k[x_{1},,x_{n}]\) is called in _shape position_ if some univariate polynomials \(h,g_{1},,g_{n-1} k[x_{n}]\) form the reduced \(_{ lex}\)-Grobner basis of \(I\) as follows.

\[G=\{h,x_{1}-g_{1},,x_{n-1}-g_{n-1}\}.\] (4.1)

As can be seen, the \(_{ lex}\)-Grobner basis consists of a univariate polynomial in \(x_{n}\) and the difference of univariate polynomials in \(x_{n}\) and a leading term \(x_{i}\) for \(i<n\). While not all ideals are in shape position, 0-dimensional radical ideals are almost always in shape position: if an \( f_{1},,f_{s} k[x_{1},,x_{n}]\) is a 0-dimensional and radical ideal, a random coordinate change \((y_{1},,y_{n})=(x_{1},,x_{n})R\) with a regular (i.e., invertible) matrix \(R k^{n}\) yields \(_{1},,_{s} k[y_{1},,y_{n}]\), and the ideal \( y_{1},,y_{n}\) generally has the reduced \(_{ lex}\)-Grobner basis in the form of Eq. (4.1) (cf. Prop. A.14).

With this fact, an efficient sampling of Grobner bases of \(0\)-dimensional radical ideals can be realized by sampling \(n\) polynomials in \(k[x_{n}]\), i.e., \(h,g_{1},,g_{n-1}\) with \(h 0\). We have to make sure that the degree of \(h\) is always greater than that of \(g_{1},,g_{n-1}\), which is necessary and sufficient for \(G\) to be a reduced Grobner basis. This approach involves efficiency and randomness, and thus resolving Prob. 4.1. Note that while our approach assumes term order \(_{ lex}\), if necessary, one can use an efficient change-of-ordering algorithm, e.g., the FGLM algorithm . The cost of the FGLM algorithm is \((n(h)^{3})\) based on the number of arithmetic operations over \(k\). Besides the ideals in shape position, we also consider the Cauchy module in App. B, which defines another class of 0-dimensional ideals.

### Backward Grobner problem

To address Prob. 4.2, we consider the following problem.

**Problem 4.5**.: _Let \(I k[x_{1},,x_{n}]\) be a 0-dimensional ideal, and let \(G=(g_{1},,g_{t})^{} k[x_{1},,x_{n}]^{t}\) be its \(\)-Grobner basis with respect to term order \(\).3 Find a polynomial matrix \(A k[x_{1},,x_{n}]^{s t}\) giving a non-Grobner set \(F=(f_{1},,f_{s})^{}=AG\) such that \( F= G\)._Namely, we generate a set of polynomials \(F=(f_{1},,f_{s})^{}\) from \(G=(g_{1},,g_{t})^{}\) by \(f_{i}=_{j=1}^{t}a_{ij}g_{j}\) for \(i=1,,s\), where \(a_{ij} k[x_{1},,x_{n}]\) denotes the \((i,j)\)-th entry of \(A\). Note that \( F\) and \( G\) are generally not identical, and the design of \(A\) such that \( F= G\) is of our question.

A similar question was studied without the Grobner condition in . They provided an algebraic necessary and sufficient condition for the polynomial system of \(F\) to have a solution outside the variety defined by \(G\). This condition is expressed explicitly by multivariate resultants. However, strong additional assumptions are required: \(A,F,G\) are homogeneous, \(G\) is a regular sequence, and in the end, \( F= G\) is only satisfied up to saturation. Thus, they are not compatible with our setting and method for Prob. 4.1.

Our analysis gives the following results for the design \(A\) to achieve \( F= G\) for the 0-dimensional case (without radicality or shape position assumption).

**Theorem 4.6**.: _Let \(G=(g_{1},,g_{t})^{}\) be a Grobner basis of a 0-dimensional ideal in \(k[x_{1},,x_{n}]\). Let \(F=(f_{1},,f_{s})^{}=AG\) with \(A k[x_{1},,x_{n}]^{s t}\)._

1. _If_ \( F= G\)_, it implies_ \(s n\)_._
2. _If_ \(A\) _has a left-inverse in_ \(k[x_{1},,x_{n}]^{t s}\)_,_ \( F= G\) _holds._
3. _The equality_ \( F= G\) _holds if and only if there exists a matrix_ \(B k[x_{1},,x_{n}]^{t s}\) _such that each row of_ \(BA-E_{t}\) _is a syzygy_4 _of_ \(G\)_, where_ \(E_{t}\) _is the identity matrix of size_ \(t\)_._

The first statement of Thm. 4.6 argues that polynomial matrix \(A\) should have at least \(n\) rows. For an ideal in shape position, we have a \(_{}\)-Grobner basis \(G\) of size \(n\), and thus, \(A\) is a square or tall matrix. The second statement shows a sufficient condition. The third statement provides a necessary and sufficient condition. Using the second statement, we design a simple random transform of a Grobner basis to a non-Grobner set without changing the ideal.

We now assume \(=_{}\) and 0-dimensional ideals in shape position. Then, \(G\) has exactly \(n\) generators. When \(s=n\), we have the following.

**Proposition 4.7**.: _For any \(A k[x_{1},,x_{n}]^{n n}\) with \((A) k\{0\}\), we have \( F= G\)._

As non-zero constant scaling does not change the ideal, we focus on \(A\) with \((A)= 1\) without loss of generality. Such \(A\) can be constructed using the Bruhat decomposition:

\[A=U_{1}PU_{2},\] (4.2)

where \(U_{1},U_{2}(n,k[x_{1},,x_{n}])\) are upper-triangular matrices with all-one diagonal entries (i.e., unimodular upper-triangular matrices) and \(P\{0,1\}^{n n}\) denotes a permutation matrix. Noting that \(A^{-1}\) satisfies \(A^{-1}A=E_{n}\), we have \( AG= G\) from Thm. 4.6. Therefore, random sampling \((U_{1},U_{2},P)\) of unimodular upper-triangular matrices \(U_{1},U_{2}\) and a permutation matrix \(P\) resolves the backward Grobner problem for \(s=n\).

We extend this idea to the case of \(s>n\) using a rectangular unimodular upper-triangular matrix:

\[U_{2}=U_{2}^{}\\ O_{s-n,n} k[x_{1},,x_{n}]^{s n},\] (4.3)

where \(U_{2}^{}(n,k[x_{1},,x_{n}])\) and \(O_{s-n,n} k[x_{1},,x_{n}]^{(s-n) n}\) is the zero matrix. The permutation matrix is now \(P\{0,1\}^{s s}\). Note that \(U_{2}G\) already gives a non-Grobner set such that \( U_{2}G= G\); however, the polynomials in the last \(s-n\) entries of \(U_{2}G\) are all zero by its construction. To avoid this, the permutation matrix \(P\) shuffles the rows and also \(U_{1}\) to exclude the zero polynomial from the final polynomial set.

To summarize, our strategy is to compute \(F=U_{1}PU_{2}G\), which only requires a sampling of \((s^{2})\) polynomials in \(k[x_{1},,x_{n}]\), and \((n^{2}+s^{2})\)-times multiplications of polynomials. Note that even in the large polynomial systems in the MQ challenge, a post-quantum cryptography challenge, we have \(n<100\) and \(s<200\).

[MISSING_PAGE_EMPTY:7]

We propose a hybrid input embedding that accepts both discrete token IDs and continuous values. Let \(=[s_{1},,s_{L}]\) to be a sequence of tokens. Some of these tokens are in \(\) and otherwise in \(\). For those in \(\), the standard input embedding based on the embedding matrix is applied. For the others, a small feed-forward network \(f_{}:^{D}\) is applied. A Transformer with the proposed embedding should equip a regression head for these continuous tokens. This allows us to handle any number as a single token without the explosion of the vocabulary set (i.e., embedding matrix). As feed-forward networks are a continuous function, they naturally implement the continuity of numbers; two close values \(s_{1},s_{2}\) are expected to be embedded in similar vectors. The hybrid input embedding has two advantages. First, as claimed above, we are no longer suffering from the large embedding matrix for registering many number tokens and can naturally implement the continuity bias. Second, We do not have the "out-of-range" issue. Further, we can scale the coefficients of given polynomials globally so that they match our training coefficient range.7 Refer to App. D for the details.

## 6 Experiments

We now present the efficiency of the proposed dataset generation method and the learnability of Grobner basis computation.8 All the experiments were conducted with 48-core CPUs, 768GB RAM, and NVIDIA RTX A6000ada GPUs. The training of a model takes less than a day on a single GPU. More information on the profile of generated datasets, the training setup, and additional experimental results are given in Apps. E and F.

### Dataset generation

First, we demonstrate the efficiency of the proposed dataset generation framework. We constructed 16 datasets \(_{n}(k)\) for \(n\{2,3,4,5\}\) and \(k\{_{7},_{31},,\}\) and measured the runtime of the forward generation and our backward generation. The dataset \(_{n}(k)\) consists of 1,000 pairs of non-Grobner set and Grobner basis in \(k[x_{1},,x_{n}]\) of ideals in shape position. Each sample \((F,G)_{n}(k)\) was prepared using Alg. 1 with \((d,d^{},s_{},)=(5,3,n+2,_{})\). The number of terms of univariate polynomials and \(n\)-variate polynomials is uniformly determined from \(\) and \(\), respectively. When \(k=\), the coefficient \(a/b\) are restricted to those with \(a,b\{-5,,5\}\) for random polynomials and \(a,b\{-100,,100\}\) for polynomials in \(F\). In the forward generation, one may first generate random polynomial sets and then compute their Grobner bases. However, this leads to a dataset with a totally different complexity from that constructed by the backward generation, leading to an unfair runtime comparison between the two generation processes. As such, the forward generation instead computes Grobner bases of the non-Grobner sets given by the backward generation, leading to the identical dataset. We used SageMath  with the libSingular backend. As Tab. 1 shows, our backward generation is a few orders of magnitude faster than the forward generation. A sharp runtime growth is observed in the forward generation as the number of variables increases. Note that these numbers only show the runtime on 1,000 samples, while training typically requires millions of samples. Therefore, the forward generation is almost infeasible, and the proposed method resolves a bottleneck in the learning of Grobner basis computation.

   Method & \(n=2\) & \(n=3\) & \(n=4\) & \(n=5\) \\  F. (std) & 4.20 & 216.3 & 740.1\({}^{}\) & 1411.1\({}^{}\) \\ F. (slimggb) & 4.29 & 183.4 & 697.5\({}^{}\) & 1322.7\({}^{}\) \\ F. (stdfglm) & 7.22 & 8.29 & 21.0 & 164.3 \\  B. (ours) & 5.23 & 5.46 & 7.05 & 7.91 \\   

Table 1: Runtime comparison (in seconds) of forward generation (F.) and backward generation (B.) of dataset \(_{n}()\) of size 1,000. The forward generation used either of the three algorithms provided in SageMath with the libSingular backend. We set a timeout limit to five seconds (added to the total runtime at every occurrence) for each Gröbner basis computation. The numbers with \(\) and \(\) include the timeout for more than 13% and 25% of the runs, respectively (cf. Tab. 5 for the success rate).

### Learnability of Grobner basis computation

We now demonstrate that Transformers can learn to compute Grobner bases. To examine the general Transformer's ability, we focus on a standard architecture (e.g., 6 encoder/decoder layers and 8 attention heads) and a standard training setup (e.g., the AdamW optimizer  with \((_{1},_{2})=(0.9,0.999)\) and a linear decay of learning rate from \(10^{-4}\)). The batch size was set to 16, and models were trained for 8 epochs. We also tested the hybrid input embedding. Refer to App. E for the complete information. Each polynomial set in the datasets is converted into a sequence using the prefix representation and the separator tokens. Unlike natural language processing, our task does not allow the truncation of an input sequence because the first term of the first polynomial in \(F\) certainly relates to the last term of the last polynomial. To make the input sequence length manageable for vanilla Transformers, we used simpler datasets \(_{n}^{-}(k)\) using \(U_{1},U_{2}^{}\) in Alg. 1 of a moderate density \((0,1]\). This makes the maximum sequence length less than 5,000. Specifically, we used \(=1.0,0.6,0.3,0.2\) for \(n=2,3,4,5\), respectively. The training set has one million samples, and the test set has one thousand samples. With hybrid input embedding, coefficients are predicted by regression, and we quantized them for \(_{p}\) and otherwise regarded them correct when the mean squared error is less than 0.1.

Table 2 shows that trained Transformers successfully compute Grobner bases with moderate/high accuracy. Several intriguing observations below are obtained. See App. F for more results. Particularly, App. F.3 presents several examples found in the datasets for which Transformer successfully computed Grobner bases significantly faster than math algorithms. Table 2 also includes the results on Cauchy module datasets on which Transformers are trained and tested. The dataset generation starts with sampling the roots in \(k^{n}\), and the other parts follow the generation of \(_{n}^{-}(k)\). The results on \((,n=3)\) with standard embedding is not shown as it requires too many number tokens.

The performance gap across the rings.The accuracy shows that the learning is more successful on infinite field coefficients \(k\{,\}\) than finite field ones \(k=_{p}\). This may be a counter-intuitive observation because there are more possible coefficients in \(G\) and \(F\) for \(\) than \(_{p}\). Specifically, for \(G\), the coefficient \(a/b\) is restricted to those with \(a,b\{-5,,5\}\) (i.e., roughly 50 choices), and \(a,b\{-100,,100\}\) (i.e., roughly 20,000 choices) for \(F\). In contrast, there are only \(p\) choices for \(_{p}\). The performance even degrades for the larger order \(p=31\). Interestingly, the support accuracy shows what the terms forming the polynomial (i.e., the _support_ of polynomial) are correctly identified well. Thus, Transformers have difficulty determining the coefficients in finite fields. Several studies have also reported that learning to solve a problem involving modular arithmetic may encounter some difficulties [21; 38; 74], but no critical workaround is known.

Incorrect yet reasonable failures.We observed that the predictions by a Transformer are mostly reasonable even when they are incorrect. For example, only several coefficients may be incorrect, and the support can be correct as suggested by the relatively high support accuracy in Tab. 2. In such a case, one can use a Grobner basis computation algorithm that works efficiently given the leading terms of the target unknown Grobner basis . Refer to App. F.2 for extensive lists of examples.

    &  &  \\   & \(n=2\) & \(n=3\) & \(n=4\) & \(n=5\) & \(n=2\) & \(n=3\) \\  \)} & _disc._ & 93.7 / 95.4 & 88.7 / 92.0 & 90.8 / 94.0 & 86.5 / 90.6 & 99.7 / 99.8 & 97.2 / 97.6 \\  & _hyb._ & 66.8 / 87.3 & 69.0 / 89.8 & 62.7 / 86.8 & 0.0 / 84.9 & 98.3 / 99.7 & 80.1 / 89.2 \\  _{7}\)} & _disc._ & 72.3 / 79.1 & 78.1 / 83.2 & 71.3 / 84.6 & 84.3 / 88.5 & 98.7 / 99.8 & 98.1 / 98.7 \\  & _hyb._ & 54.1 / 78.7 & 55.8 / 84.3 & 46.1 / 81.8 & 54.4 / 81.5 & 95.8 / 99.7 & 80.8 / 91.2 \\  _{31}\)} & _disc._ & 46.8 / 77.3 & 50.2 / 80.9 & 51.1 / 83.7 & 28.6 / 77.9 & 93.8 / 99.7 & 94.7 / 99.6 \\  & _hyb._ & 6.1 / 75.3 & 5.8 / 80.4 & 0.1 / 73.0 & 0.1 / 76.9 & 15.1 / 99.5 & 10.9 / 98.4 \\  \(\) & _hyb._ & 57.2 / 85.0 & 61.0 / 88.0 & 61.7 / 87.5 & 45.6 / 82.9 & 28.3 / 100 & 4.3 / 100 \\   

Table 2: Accuracy [%] / support accuracy [%] of Gröbner basis computation by Transformer on \(_{n}^{-}(k)\). In the support accuracy, two polynomials are considered identical if they consist of an identical set of terms (i.e., identical _support_), Transformers are trained on either discrete input embedding (_disc._) and the hybrid embedding (_hyb._). Note that the datasets for \(n=3,4,5\) are here constructed using \(U_{1},U_{2}^{}\) (cf. Alg. 1) with density \(=0.6,0.3,0.2\), respectively.

Hybrid embedding.Table 2 shows that determining coefficients by regression is less successful than classifications. For infinite field \(k\), this may be because of the accumulation of coefficient errors during the auto-regressive generation. Thus, the current best practice would be to prepare many number tokens in the vocabulary set, or a sophisticated regression-by-classification approach may be helpful . Note that the results for \(k=_{p}\) are shown for reference as the finite field elements do not have ordering. Figure 1 shows a contrast between the embedding functions learned in infinite field and finite field. Particularly, the slice of distance matrix (ii) and that of the dot-product matrix (vi) show that these metrics align well with the difference between numbers in \(\). However, we cannot observe convincing patterns in the embedding in \(_{31}\). For the two-layer case in Fig. 1(a), we observe sharp changes around \( 5\) of the horizontal axis. This may be because of the gap in the coefficient range in the input and output space. The coefficients of \(F\) ranges between \([-100,100]\), while that of \(G\) does between \([-5,5]\). In Tab. 3, we show that the increase of hidden layers of \(f_{}\) does not lead to improvement.

## 7 Conclusion

This study proposed the first learning approach to a fundamental algebraic task, the Grobner basis computation. While various recent studies have reported the learnability of mathematical problems by Transformers, we addressed the first problem with nontriviality in the dataset generation. Ultimately, the learning approach may be useful to address large-scale problems that cannot be approached by Grobner basis computation algorithms because of their computational complexity. Transformers can output predictions in moderate runtime. The outputs may be incorrect, but there is a chance of obtaining a hint of a solution, as shown in our experiments. We believe that our study reveals many interesting open questions to achieve Grobner basis computation learning. Some are algebraic problems, and others are machine learning challenges, further discussed in Sec. H.

Acknowledgement.We would like to thank Masayuki Noro (Rikkyo University) for his fruitful comments on our dataset construction algorithm and Noriki Nishida (RIKEN Center for Advanced Intelligence Project) for his help in the implementation. Hiroshi Kera was supported by JST PRESTO Grant Number JPMJPR24K4, JST ACT-X Grant Number JPMJAX23C8, Mitsubishi Electric Information Technology R&D Center, and the Chiba University IARR Research Support Program and the Program for Forming Japan's Peak Research Universities (J-PEAKS). Yuki Ishihara was supported by JSPS KAKENHI Grant Number JP22K13901 and Institute of Mathematics for Industry, Joint Usage/Research Center in Kyushu University (FY2023 Short-term Joint Research "Speeding up of symbolic computation and its application to solving industrial problems" (2023a006)). Yuta Kambe was supported by Mitsubishi Electric Research Associate Program.

Figure 1: Visual analysis of embedding vectors of numbers given by the proposed embedding. Embedding \(c\) to \(f_{}(c)^{D}\) from \(c_{}\) to \(c_{}\) with \(B\) bins to obtain \(M^{B D}\), the fix figures show from the left, (i) the Euclidean distance matrix of \(M\), (ii) its slice at \(0\), (iii) the norm of embedding vectors, (iv) the dot product \(^{}\) with \(\) of the row-normalized \(M\), (v) \(f_{}(0)^{}\) and (vi) \(f_{}(c_{0})^{}M\). (a) Trained on \([x_{1},x_{2}]\); \((c_{},c_{})=(-100,100)\). (b) Trained on \(_{31}[x_{1},x_{2}]\); \((c_{},c_{})=(0,31)\). The embedding layer \(f_{}\) has one/two hidden layers (top/bottom rows). As can be seen, the relationship between embedding vectors in terms of distance and dot product is aligned well in the infinite field and not in the finite field.