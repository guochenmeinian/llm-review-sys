# Gaussian Process Predictions with Uncertain Inputs Enabled by Uncertainty-Tracking Processor Architectures

Gaussian Process Predictions with Uncertain Inputs Enabled by Uncertainty-Tracking Processor Architectures

Janith Petangoda

Signaloid,

Cambridge, UK

&Chatura Samarakoon

University of Cambridge,

Cambridge, UK

&Phillip Stanley-Marbell

Signaloid / University of Cambridge,

Cambridge UK.

The work was mostly carried out at the University of Cambridge.

###### Abstract

Gaussian Processes (GPs) are theoretically-grounded models that capture both aleatoric and epistemic uncertainty, but, the well-known solutions of the GP predictive posterior distribution apply only for deterministic inputs. If the input is uncertain, closed-form solutions aren't generally available and approximation schemes such as moment-matching and Monte Carlo simulation must be used. Moment-matching is only available under restricted conditions on the input distribution and the GP prior and will miss the nuances of the predictive posterior distribution; Monte Carlo simulation can be computationally expensive. In this article, we present a _general_ method that uses a recently-developed processor architecture  capable of performing arithmetic on distributions to implicitly calculate the predictive posterior distribution with uncertain inputs. We show that our method implemented to run on a commercially-available implementation  of an uncertainty-tracking processor architecture captures the nuances of the predictive posterior distribution while being \(\)\(108.80\)x faster than Monte Carlo simulation.

## 1 Introduction

Gaussian Processes (GPs) are a class of non-parametric models that can intrinsically capture model uncertainty. The prediction of a GP for a unobserved input is referred to as the _predictive posterior distribution_ since it is a distribution over the predictive output conditioned on the observed data. For deterministic inputs, there is a closed-form solution for the predictive posterior distribution of a GP.

GPs can also capture the impact of _input uncertainty_ on the predicted output. With an uncertain input, the GP predictive posterior distribution is in general arbitrary and no general closed-form solution exists. A typical tactic for handling uncertain inputs is to carry out moment-matching, where the first \(k\) (usually \(k=2\)) moments of the true predictive posterior distribution are calculated . Closed-form solutions for even the first two moments are only available in special cases, and even then, such a Gaussian approximation loses key statistical properties of the true posterior distribution, such as multi-modality and skewness. A better approximation could be obtained by carrying out a (potentially) computationally-expensive Monte Carlo simulation.

In this article, we present an algorithm for implicitly computing the GP predictive posterior distribution with uncertain inputs using recent advances in uncertainty-tracking processor architectures (UTPAs) . Our method does not rely on Monte Carlo simulation but rather exploits the internal distributional representation of the UTPA. Our method is _agnostic to the distribution of the input and the GP prior_.

We evaluate the effectiveness of our method against the Monte Carlo method by comparing the accuracies of the solutions to the ground-truth (obtained by a Monte Carlo simulation with a large number of samples). We also compare the run time of each method. We show in Section 6 that running our new algorithm on a commercially-available implementation  of a UTPA [1; 2] can achieve orders of magnitude faster performance while achieving the same accuracy.

## 2 Gaussian Processes

A GP is a collection of random variables (Definition A.2 in Appendix A) where any _finite_ subcollection of those random variables is jointly Gaussian. It is a distribution over the _infinitely_-many random variables that satisfy this condition on any _finite_ subcollection of them. We can define a GP implicitly by specifying a mechanism for deriving the mean vector and covariance matrix of the multivariate Gaussian distribution that distributes any given subset of random variables. We can do so using a mean function \(m\) and covariance kernel \(k\) to derive the mean vector and covariance matrix of the multivariate Gaussian distribution using the _indexes_ of the random variables.

**Definition 2.1** (Implicit definition of a Gaussian Process).: Let \(X\) be an index set. Then, for a finite subset of \(X\), \(\{x_{1},...,x_{n}\} X\), let the random variables \(Y_{x_{1}},...,Y_{x_{n}}\) be jointly distributed as a multivariate Gaussian, \((,)\), where the elements \(_{i}\) of the mean vector \(\) and \(_{ij}\) of the covariance matrix \(\) are implicitly derived using

\[_{i}=m(x_{i}),\]

and

\[_{ij}=k(x_{i},x_{j}).\]

The function \(m:X\) is called the _mean function_ and the function \(k:X X\) is the _covariance kernel_. We denote an implicitly-defined GP by \((m,k)\).

We denoted the index set with \(X\) and the random variable with \(Y_{x_{i}},\) for \(x_{i} X\) to highlight that a GP is a distribution over functions where \(X\) is the input space and \(Y=Y_{x_{i} X}\) is the output space. Thus the sample space of each \(Y_{x_{i}}\) is \(Y\). Let \(X=\) and \(Y=\) be an input and output space respectively and \(\) denote a space of functions, where \((f):X Y\) denotes a possible function from \(X\) to \(Y\). We can place a GP prior over \(\) by writing \((m,k)\). In this view, a sample from a GP is a sample from all the random variables \(Y_{x}=f(x), x X\).

The function space \(\) over which a GP is defined is implicitly described by the chosen mean function and the covariance kernel; we have placed a _GP prior over \(\)_. Popular kernels include the squared exponential (this is a basic 'all-rounder' kernel) and the Matern class of kernels . The squared exponential kernel is given by

\[k_{c,}^{sq}(x,x^{})=c()^{T}(x-x^{ })}{^{2}}),\] (1)

where \(c^{+}\) and \(\) are hyperparameters of the kernel. The parameter \(c\) is the signal variance and the parameter \(\) is the characteristic length scale2. The squared exponential kernel describes a function space of smooth functions that have a variability that is described by the signal variance and the length scale; the signal variable loosely specifies how much the function can vary from the mean function and the length scale specifies how quickly the function can vary from the mean function.

### GP Prediction with Deterministic Inputs

Let \(X=^{d}\) denote an input space, where \(d\) denotes the dimension of \(X\), and let \(Y=\) denote an output space. Let \(f:X Y\) be an arbitrary function from \(X\) to \(Y\) from a function space \(\). We define a GP prior over \(\) as \((m,k)\). The mean function \(m\) and covariance kernel \(k\) implicitly define the function space \(\). Further, let the likelihood function be \(=f(x)+\), where \((0,_{}^{2})\) and \((=Y)\) denotes a noisy observation. Finally, let \(^{k d}\) be a matrix of \(k\) input points from \(X\), and let \(^{k 1}\) be a matrix of \(k\) noisy output points from \(\).

We are interested in the posterior GP over \(\) given the data \((,)\). However, rather than finding an explicit conditional distribution over \(\), we express the distribution of the posterior GP evaluated at a particular point \(x^{*} X\), which, by Definition 2.1 is a Gaussian random variable. Therefore, for an arbitrary point \(x^{*} X\), the predictive posterior density of the random variable \(Y_{x^{*}}=f(x^{*})\) is

\[Y_{x^{*}}=f(x^{*}) p_{f(x^{*})}(y^{*}|,)=( _{*}(x^{*}),_{*}^{2}(x^{*})),\] (2)

where,

\[_{*}:X&\\ x^{*}&_{*}(x^{*})=m(x^{*})+k(x^{*}, )(+_{}^{2})^{-1}(-m(x^{ *})),\] (3)

\[_{*}^{2}:X&\\ x^{*}&_{*}^{2}(x^{*})=k(x^{*},x^{*})-k( x^{*},)(+_{}^{2})^{-1}k(x^{*}, )^{T}.\]

For a deterministic input, GP prediction can be thought of as a function from the input to a mean and a variance, as Equation 3 shows, that defines the predictive posterior distribution.

## 3 GP Prediction with Uncertain Inputs

A useful enhancement to GP prediction with deterministic input is to consider a distributional input. Let the input to the GP now be a random variable \(X^{*}\), the sample space of which is \(X\). Thus, \(X^{*} p_{X^{*}}(\,\,)\). We are still interested in the predictive distribution of \(Y_{x^{*}} p_{f(X^{*})}(y|,)\), where \(x^{*} X^{*}\). However, we must now marginalize over \(X^{*}\) because it is a random variable:

\[p_{f(X^{*})}(f(x^{*})=y^{*}|,)=_{X}p_{f(x^{*})}(y^{*}| ,,x^{*})p_{X}(x^{*})x^{*}.\] (4)

In general, this integral is intractable. Therefore, we must usually resort to approximation methods. We discuss two approximation methods used in the literature below. In Section 4, we introduce a method for implicitly computing the predictive posterior distribution with uncertain input using a state-of-the-art UTPA [1; 2].

### Moment-matching approximation

We can approximate the intractable predictive posterior distribution as a Gaussian distribution. This is done by defining a Gaussian distribution using the first and second moments of the predictive posterior distribution [4; 5]. This mean and variance can be computed analytically when using the squared exponential kernel and the input is a Gaussian random variable [4; 5]. For more complicated input distributions and GP priors, it is difficult to find closed-form solutions. Furthermore, a Gaussian approximation to a non-Gaussian predictive posterior distribution would lack important statistical information such as multi-modality and skewness.

### Monte Carlo simulation

The Monte Carlo method can be used to obtain a more informative approximation of \(p_{f(X^{*})}(y^{*}|,)\). Monte Carlo simulation [7; 8; 9] applies the Monte Carlo method to approximate the probability density function of a transformed random variable. That is, if we want the probability density function of \(Y=f(X)\), then we first take \(n\) samples of \(x_{i} X\), transform each sample by applying \(f\) to obtain \(n\) samples \(y_{i}=f(x_{i})\) of \(Y\), and create a density estimation (such as a histogram) using the transformed samples. Better approximations are obtained by increasing the number of samples \(n\).

Girard _et al._ suggests estimating the integral in Equation 4 using Monte Carlo integration, where,

\[p_{f(X^{*})}(y^{*}|,)= p_{f(x^{*})}(y^{*}|, ,x^{*})p_{X}(x^{*})\,x^{*}_{i=1}^{n} p_{f(x^{*})}(y^{*}|,,x_{i}^{*})\,x^{*}.\] (5)

For this method, we would need to evaluate the sum in Equation 5 for each value that \(f(x^{*})\) can take. Instead, when using Monte Carlo simulation, we will obtain samples of \(f(x^{*})\) under \(p_{f(x^{*})}(y^{*}|,)\) by taking samples of \(x_{i}^{*} X^{*}\), calculating the mean \(_{*}(x_{i}^{*})\) and variance \(_{*}^{2}(x_{i}^{*})\) according to Equation 3 for \(p_{f(X^{*})}(y^{*}|,,x^{*})\), and taking samples \(y_{i}^{*}(_{*}(x_{i}^{*})\), \(_{*}^{2}(x_{i}^{*}))\).

The Monte Carlo method can be used to obtain an arbitrarily-good approximation of \(p_{f(X^{*})}(y^{*}|,)\). However, doing so can be prohibitively expensive due to its slow convergence rate .

Gaussian Process Prediction with Uncertain Inputs on an

Uncertainty-Tracking Processor Architecture

To overcome the accuracy limitations of moment-matching and the computational costs of Monte Carlo simulations, we present a method that is enabled by recent advances in UTPAs [1; 2]. A UTPA is a computer microarchitecture that can represent distributional information in its microarchitectural state and track how these distributions evolve under arithmetic operations transparently to the applications running on it. A UTPA does this by providing an in-processor representation of probability distributions and a suite of arithmetic operations that can manipulate this representation. A UTPA carries out _deterministic computations on probability distributions_ and does not rely on any sampling methods. Therefore, unlike Monte Carlo methods that require convergence to a good solution by increasing the number of Monte Carlo iterations, a UTPA is _convergence-oblivious_ up to the fidelity of its representation.

The UTPA presented by Tsoutsouras _et al._[1; 2] has an associated _representation size_, \(r\), that describes the _precision_ of the representation. Larger representation sizes result in more accurate representations. A useful analogy is the IEEE-754 standard [10; 11] that approximately represents the infinite set of real numbers as floating point numbers on computers that can only handle a finite set of values.

A practical way of thinking of what a UTPA does is that it carries out uncertainty propagation. If we have a random variable \(X\) represented in the UTPA's distribution representation and a function \(f:X Y\), then a UTPA directly computes the distribution representation of \(Y=f(X)\) by directly computing it through the function \(f\) without resorting to Monte Carlo sampling.

### Gaussian Process Prediction with Uncertain Inputs as a Transformation of Random Variables

Since a UTPA can represent any distribution for the input random variable \(X\), our goal is to use a UTPA to directly compute the predictive posterior distribution of a GP with an uncertain input. To do this, we first note that a GP is practically two functions that map some input \(x^{*}\) to the mean \(_{*}(x^{*})\) and variance \(_{*}^{2}(x^{*})\) of the output distribution at \(x^{*}\) (as given by Equation 3), but _not to the output random variable itself_. Simply plugging a UTPA-represented distribution to Equation 3 on a UTPA would result in distributions over the mean and variance and not a distribution of the output random variable that we desire.

We can express a transformation of random variables that results in a random variable that has the desired predictive posterior distribution \(p_{f(X^{*})}(y^{*}|,)\) by using the "reparametrization trick" ,

\[f(X^{*})=_{*}(X^{*})+_{*}(X^{*}),\] (6)

where \((0,1)\), and \((x^{*})\) and \(_{*}(x^{*})\) are the mean and standard deviation of the GP posterior at \(x^{*}\) respectively from Equation 3. Theorem 4.1 shows that \(f(X^{*})\) has the desired distribution.

**Theorem 4.1**.: Let \(_{*}(X^{*})\) and \(_{*}^{2}(X^{*})\) denote the application of the mean and variance functions of the Gaussian Process predictive posterior distribution (as in Equation 3) to a random variable \(X^{*} p_{X}(\,\,)\) given some input and output data \(\) and \(\) respectively, and let \((0,1)\). Then, the probability density function of the random variable defined as

\[f(X^{*}|,)=_{*}(X^{*})+_{*}(X^{*})\]

is given by

\[p_{f(X^{*})}(y^{*}|,)= p_{f(x^{*})}(y^{*}|,,x^{*})p_{X}(x^{*})x^{*}=(_{*}(x^ {*}),_{*}^{2}(x^{*}))p_{X}(x^{*})x^{*},\]

where \(p_{f(x^{*})}(y^{*}|,,x^{*})\) is the conditional probability density function of \(f(x^{*})\) given \(\), \(\), and a particular value of \(x^{*} X^{*}\).

Proof.: See Appendix B. 

Implementing the Transformation of Random Variables on an Uncertainty-Tracking Processor Architecture is Trivial

Using Algorithm 1, we can easily find the GP predictive posterior with an uncertain input using a UTPA. Here, \(X^{*}\), \(\), and \(y\) are distributional variables that are represented in the UTPA.

## 5 Methods

We compare the performance of Monte Carlo simulation and Algorithm 1 for computing the predictive posterior distribution. We measure the timing performance in terms of the wall-clock run time of each method and the accuracy in terms of the Wasserstein distance  of the result of each method compared to a ground-truth predictive posterior distribution obtained by a large (\(n=1,000,000\)) Monte Carlo simulation.

We carry out experiments using the two methods across different number of iterations \(n\) (of the Monte Carlo simulation) and representation sizes \(r\) (of the UTPA) to compare the trade-offs between converging to the output distribution and the extra time required to do so. For each configuration of \(n\) or \(r\), we carry out 30 repetitions to account for variation in sampling3.

We set the mean function of the GP prior to \(m(x)=0\). We set \(X\) and \(Y\) to be the real space \(\), and generate a dataset \((,)\) of \(k=10\) data points by setting \(x_{0}=-3\), \(x_{i}=x_{i-1}+0.6\)4, and \(y_{i}\) as,

\[y_{i}(x_{i})=(2x_{i})(x_{i})(e^{-x}+e^{x})+_{i},\] (7)

where each \(_{i}(0,0.01)\)5. For each case, we use the squared exponential kernel \(k_{c=1,=}^{sq}\) and we set the uncertain input distribution to \(X^{*}(0,4)\) (see Figure 2 in Appendix C).

For the Monte Carlo method, we evaluate on \(n\{4,256,1152,2048,4096,8192,16000,32000,\)\(64000,128000,512000\}\). For Algorithm 1, we evaluate on \(r\{32,64,128,256,512,2048\}\). For the UTPA for Algorithm 1, we use a commercial implementation of a multi-dimensional variation of the UTPA presented by Tsoutsouras _et al._[1; 2] that automatically tracks correlations _between arbitrary random variables_.

We implement both Algorithm 1 and the Monte Carlo simulation using the C programming language and a custom tensor library implemented in C6. The commercial implementation of the UTPA  is implemented as a single-threaded emulated processor running on a 3rd Generation Intel Xeon system. As a result, Algorithm 1 is at an inherent disadvantage since it is benefiting only from the algorithmic advances of the distribution representation and its associated arithmetic and is not taking advantage of hardware acceleration; an FPGA or a custom silicon implementation of the underlying UTPA could provide even greater speedups than we present in this paper. We perform the Monte Carlo experiments on the same hardware as the hardware we used to emulate the UTPA. We note that we did not exploit parallelization for the Monte Carlo method (e.g., by using a GPU) since the commercial implementation of the UTPA  does not exploit parallelization either. See Appendix C for more detail on our method.

## 6 Results

Figure 1 summarizes our results; Table 2 in the Appendix shows numerical results for the other configurations. Figure 0(a) shows that the results from Algorithm 1 are almost always at the Paretofrontier. Therefore, for any required accuracy except for the highest, in the ranges that we had tested, Algorithm 1 is at least an order of magnitude faster. Table 1 shows that for \(r=128\), the Monte Carlo method with \(n=128000\) shows a nearly identical Wasserstein distance (albeit with more than twice the standard deviation) while taking approximately \(108.80\) longer to compute.

Furthermore, Algorithm 1 does not suffer from large variances in the accuracy since the UTPA carries out _deterministic computation on probability distributions_. We see this from the constant variance shown by the results from Algorithm 1. This uncertainty is due to the fact that we calculated the Wasserstein distance using finite samples from the output distribution. It is possible to calculate the Wasserstein distance directly from the UTPA's representation, but we did not do so at this time. After \(r=128\), the accuracy of Algorithm 1 stagnates and the trade-off with run time increases. Therefore, for this particular application, if larger accuracy is required, the Monte Carlo method must be used.

Figures 0(b) and 0(c) show that the posterior distribution is more complex than a Gaussian. A moment-matched Gaussian solution would, for example, miss that the posterior distribution is multi-modal.

## 7 Conclusions

Existing methods for approximating the often-intractable GP predictive posterior distribution with uncertain inputs can be inaccurate and constrained in their use (moment-matching), or computationally expensive (Monte Carlo simulation). We present a method for computing the desired predictive posterior distribution on a UTPA  by making use of the reparametrization trick. Our method can be used on _any input distribution and GP prior_ unlike moment-matching and we show experimentally that our method can be _as accurate as and orders of magnitude faster than Monte Carlo simulation_.

   Problem & Core &  Representation Size / \\ Number of samples \\  &  Wasserstein Distance \\ (mean \(\) std. dev.) \\  & 
 Run time (ms) \\ (mean \(\) std. dev.) \\  \\  Squared Exponential Kernel & Algorithm 1 & 128 & \(0.00290 0.00050\) & \(3.182 0.376\) \\ Squared Exponential Kernel & Traditional Monte Carlo & 128000 & \(0.00324 0.00129\) & \(346.186 1.800\) \\   

Table 1: Selection of results showing the Wasserstein distance and the run time required by the best overall configuration for Algorithm 1 and the close-to-equivalent Monte Carlo configuration. To obtain a similar accuracy, the Monte Carlo method took \(\)\(108.80\) more time than Algorithm 1. See Table 2 for complete results.

Figure 1: Summaries of our key results. Subfigure (a) is a Pareto plot between the mean run time and the mean Wasserstein distance from the ground-truth output distribution. The error bars are \( 1\) standard deviation. From our experiments, Algorithm 1 is almost always on the Pareto frontier, even after accounting for uncertainty. Subfigures (b) and (c) are histograms from Algorithm 1 and the Monte Carlo method respectively. The ground-truth distribution is in black, behind each histogram.

Acknowledgements

This work was supported by UKRI Materials Made Smarter Research Centre (EPSRC grant EP/V061798/1).