# Variational Search Distributions

Dan Steinberg, Rafael Oliveira, Cheng Soon Ong & Edwin V. Bonilla

CSIRO's Data61, Australia

{dan.steinberg, rafael.dossantosdeoliveira, cheng-soon.ong, edwin.bonilla}@data61.csiro.au

###### Abstract

We develop variational search distributions (VSD), a method for finding and generating discrete, combinatorial designs of a rare desired class in a batch sequential manner with a fixed experimental budget. We formalize the requirements and desiderata for active generation and formulate a solution via variational inference. In particular, VSD uses off-the-shelf gradient based optimization routines, can learn powerful generative models for designs, and can take advantage of scalable predictive models. We empirically demonstrate that VSD can outperform existing baseline methods on a set of real sequence-design problems in various biological systems.

## 1 Introduction

We consider a variant of the _active search_ problem [15; 22; 51], where we wish to find members (designs) of a rare desired class in a batch sequential manner with a fixed experimental budget. We call sequential active learning of a _generative_ model of these designs _active generation_. Examples of rare designs are compounds that could be useful pharmaceutical drugs, or highly active enzymes for catalyzing chemical reactions. We assume the design space is discrete or partially discrete, high-dimensional, and practically _innureable_. For example, the number possible configurations of a single protein is \(20^{(100)}\) [see, e.g., 38].

We are interested in this active generation objective for a variety of reasons. We may wish to study the properties of the "fitness landscape"  to gain a better scientific understanding of a phenomenon such as natural evolution. Or, we may not be able to completely specify the constraints and objectives of a task, but we would like to characterize the space of, and generate new feasible designs. For example, we want enzymes that can degrade plastics in an industrial setting, but we may not yet know the exact conditions (e.g. temperature, pH), some of which may be anti-correlated with enzyme catalytic activity. Alternatively, if we know these multiple objectives and constraints, we may only want to generate designs from a Pareto set.

Assuming we can take advantage of a prior distribution over designs, we formulate the search problem as inferring the posterior distribution over rare, desirable designs. Importantly, this posterior can be used for _generating new designs_. Specifically, we use (black-box) variational inference (VI) , and so refer to our method as variational search distributions (VSD). Our major contributions are: (1) we formulate the batch active generation objective over a (practically) innumerable discrete design space, (2) we present a variational inference algorithm, VSD, which solves this objective, and (3) we show that VSD performs well empirically. VSD uses off-the-shelf gradient based optimization routines, is able to learn powerful generative models, and can take advantage of scalable predictive models. In our experiments we show that VSD can outperform existing baseline methods on a set of real applications. Finally, we evaluate our approach on the related sequential black-box optimization (BBO) problem, where we want to find the globally optimal design for a specific objective and show competitive performance when compared with state-of-the-art methods.

[MISSING_PAGE_FAIL:2]

\(p(y>|,_{N})=((_{N}()-)/_{N}( ))\), where \(()\) is a cumulative standard normal distribution function, and \(_{N}()\), \(_{N}^{2}()\) are the posterior predictive mean and variance, respectively, of the GP. We can now rewrite the ELBO as,

\[_{}(,,_{N})=_{q( |)}[_{PI}(,_{N},)]-_{}[q(|)\|p(|_{0})]\,.\] (3)

We refer to our method that optimizes the objective in Equation 3 as VSD, as we are using the variational posterior distribution as a means of searching the space of fit sequences, satisfying (R1), (R2) and (R4). Concretely, we draw a set of sample candidates from our search distribution, (R5), each round,

\[\{_{b}\}_{b=1}^{B}q(|_{t}^{*}), _{t}^{*}=*{argmax}_{}_{ }(,,_{N})\,.\] (4)

In general, because of the discrete combinatorial nature of our problem, we cannot readily use the re-parametrization trick to estimate the gradients of the ELBO above. Instead, we use of the score function gradient estimator  with standard gradient descent methods (D2),

\[_{}_{}(,,_{N})=_ {q(|)}(_{PI}(,_{N}, )-|)}{p(|_{0})}) _{} q(|)\,,\] (5)

where we use Monte-Carlo sampling to approximate this expectation with a suitable variance reduction scheme, such as using a control variate or baseline. We find that the exponentially smoothed average of the ELBO works well in practice, and is the same strategy employed in Daulton et al. . Effectively, VSD implements black-box variational inference  for parameter estimation, and despite the high-dimensional nature of \(\), we find we only need \((1000)\) samples to estimate the required expectations for ELBO optimization on problems with \(M=(100)\), satisfying (R3).

**Class probability estimation**: So far our method indirectly computes the PI acquisition function by transforming the predictions of a GP surrogate model, \(p(y|,_{N})\), as in Equation 2. Instead we may choose to follow the reasoning used by Bayesian optimization by density-ratio estimation (BORE) in  and directly estimate the quantity we care about, \(p(y>|,_{N})\). We do this with class probability estimation (CPE) using \(p(z=1|,_{N})_{}()\), where \(z:=[y>]\{0,1\}\), and \(_{}:\). We can recover the class probability estimates using a proper scoring rule  such as Brier score or log-loss on training data, \(_{N}^{z}=\{(z_{n},_{n})\}_{n=1}^{N}\), e.g.,

\[_{}(,_{N}^{z}):=_{n=1}^{ N}z_{n}_{}(_{n})+(1-z_{n})(1-_{}( _{n})).\] (6)

The VSD objective using CPE becomes,

\[_{}(,,_{N})=_{q(|)}[_{}()]-_{}[q(| )\|p()]\,,\] (7)

into which we plug \(_{t}^{*}=*{argmax}_{}_{}(,_{N}^{z})\). Using a CPE also opens up the choice of estimators that are more scalable than a GP surrogate, satisfying our last desideratum (D3). This may be crucial if we choose to run more than a few rounds of experiments with \(B=(1000)\). Additionally, since VSD is a black box method, we can choose to use CPEs that are non-differentiable, such as decision tree ensembles.

Figure 1: Fitness landscape tasks. (a) \(f_{}()\) and white ‘\(\)’ — the maximum fitness design, \(^{*}\). (b) white hatched area — the super-level set of all fit designs, \(\). (c) prior belief \(p()\). (d) blue contours — the density of the super-level set, \(p(|y>)\). (e) the black box function for the super-level set, \(\). See the text for definitions of these tasks. Our primary goal is to estimate (d).

The complete VSD algorithm is given in Algorithm 1 (Appendix D), in which we have allowed for a threshold function, \(_{t}=f_{}(\{y:y_{N}\},_{t})\). This function can be used to modify the threshold each round, e.g. following , an empirical quantile function \(_{t}=_{y}(_{t})\) where \(_{t}(0,1)\), or a constant \(\) in the case of estimating the density of the super-level set.

**Theoretical analysis and related work**: We show in Appendix A that the VSD objective, in fact, generalizes the BO objective, providing a lower bound that is tight iff the prior is a Dirac delta distribution centered at \(_{t}^{*}\). In the sequel  we provide convergence guarantees for VSD, satisfying desideratum (D1). In Appendix B we provide a formulation that generalizes several related optimization algorithms (and VSD) including Bayesian optimization with probabilistic reparameterisation (BOPR) , design by adaptive sampling (DbAS) , conditioning by adaptive sampling (CbAS)  and BORE . The key takeaway is that, as seen in Table 2, VSD satisfies all the requirements and desiderata for our problem.

## 4 Experiments

We evaluate our method, VSD, on a number of real-world sequence design tasks involving various biological systems. The corresponding datasets involve \(||\{4,20\}\), \(8 m 237\) and \(65,000<||<20^{237}\). We carry out fitness landscape experiments where we assess the quality of _all_ the sequences proposed by the competing algorithms and black-box optimization (BBO) experiments where we evaluate the best performing sequence. We use a mean field variational distribution and independent prior for the fitness landscape experiments, and we also use a long short-term memory (LSTM) and decoder-transformer variational distribution and prior for the higher dimensional BBO experiments. See Appendix C for full details of the experiments, including a description of the evaluation metrics and results on batch diversity.

Figure 2: Fitness landscape results. Precision (Equation 13), recall (Equation 14) and performance (Equation 15) – higher is better – for the combinatorially (near) complete datasets, DHFR and TrpB and TFBIND8. The random method is implemented by drawing \(B\) samples uniformly.

Figure 2 shows the results for the fitness landscape experiments, and the BBO experimental results can be found in Figure 3. VSD is clearly the best performing method for all tasks, with the related method CbAS also performing well. We have consistently found the evolutionary-search based methods, PEX and AdaLead, to be effective on lower-dimensional problems, however we consistently observe their performance degrading as the dimension of the problem increases - e.g. on the BBO experiments. We suspect this is a direct consequence of their random mutation strategies being suited to exploration in low dimensions, but less efficient in higher dimensions compared to the learned generative models employed by VSD, CbAS, and DbAS. Our version of BORE (which is just the expected log-likelihood component of Equation 7) performs badly in most cases, and this is a direct consequence of its proposal distribution collapsing to a Kronecker delta centered on \(_{t}^{*}\). In a non-batch setting, this behavior is not problematic, but shows how crucial the Kullback-Liebler (KL) divergence regularization of VSD is in this batch setting.

## 5 Conclusion

We have presented the problem of active generation (sequentially finding designs of a rare class under some experimental constraints), and a method for efficiently generating samples which we call variational search distributions (VSD). Underpinned by variational inference, VSD satisfies critical requirements and important desiderata, including learning generative models for feasible/fit sequences and batch candidate generation. We showcased the benefits of our method empirically on a set of combinatorially complete and high dimensional sequential-design biological problems and show that it can effectively learn powerful generative models of fit designs. There is a close connection between active generation and black box optimization, and with the advent of powerful generative models we hope that our explicit framing of generation of fit sequences would lead to further study of this connection. Finally, our framework can be generalized to more complex application scenarios, potentially involving other challenging combinatorial optimization problems , such as graph structures , and mixed discrete-continuous variables, which are worth investigating as future work directions.