# Online Convex Optimization with Unbounded Memory

Raunak Kumar

Department of Computer Science

Cornell University

Ithaca, NY 14853

raunak@cs.cornell.edu &Sarah Dean

Department of Computer Science

Cornell University

Ithaca, NY 14853

sdean@cornell.edu &Robert Kleinberg

Department of Computer Science

Cornell University

Ithaca, NY 14853

rdk@cs.cornell.edu

###### Abstract

Online convex optimization (OCO) is a widely used framework in online learning. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. However, in many applications the learner's loss depends not only on the current decision but on the entire history of decisions until that point. The OCO framework and its existing generalizations do not capture this, and they can only be applied to many settings of interest after a long series of approximation arguments. They also leave open the question of whether the dependence on memory is tight because there are no non-trivial lower bounds. In this work we introduce a generalization of the OCO framework, "Online Convex Optimization with Unbounded Memory", that captures long-term dependence on past decisions. We introduce the notion of \(p\)-effective memory capacity, \(H_{p}\), that quantifies the maximum influence of past decisions on present losses. We prove an \(O(T})\) upper bound on the policy regret and a matching (worst-case) lower bound. As a special case, we prove the first non-trivial lower bound for OCO with finite memory (Anava et al., 2015), which could be of independent interest, and also improve existing upper bounds. We demonstrate the broad applicability of our framework by using it to derive regret bounds, and to improve and simplify existing regret bound derivations, for a variety of online learning problems including online linear control and an online variant of performative prediction.

## 1 Introduction

Numerous applications are characterized by multiple rounds of sequential interactions with an environment, e.g., prediction from expert advice (Littlestone and Warmuth, 1989, 1994), portfolio selection (Cover, 1991), routing (Awerbuch and Kleinberg, 2008), etc. One of the most popular frameworks for modelling such sequential decision-making problems is online convex optimization (OCO) (Zinkevich, 2003). The OCO framework is as follows. In each round, the learner chooses a decision in a convex set and an adversary chooses a convex loss function, and then the learner suffers the loss associated with their current decision. The performance of an algorithm is measured by regret: the difference between the algorithm's total loss and that of the best fixed decision. We refer the reader to Shalev-Shwartz (2012), Hazan (2019), Orabona (2019) for surveys on this topic.

However, in many applications the loss of the learner depends not only on the current decisions but on the entire history of decisions until that point. For example, in online linear control (Agarwal et al., 2019), in each round the learner chooses a "control policy" (i.e., decision), suffers a loss that is a function of the action taken by this policy and the current state of the system, and the system's state evolves according to linear dynamics. The current state depends on the entire history of actions and, therefore, the current loss depends not only on the current decision but the entire history of decisions. The OCO framework cannot capture such long-term dependence of the current loss on the past decisions and neither can existing generalizations that allow the loss to depend on a _constant_ number of past decisions (Anava et al., 2015). Although a series of approximation arguments can be used to apply finite memory generalizations of OCO to the online linear control problem, there is no OCO framework that captures the complete long-term dependence of current losses on past decisions. Furthermore, there are no non-trivial lower bounds for OCO in the memory setting,1 which leaves open the question whether the dependence on memory is tight.

Contributions.In this paper we introduce a generalization of the OCO framework, "Online Convex Optimization with Unbounded Memory" (Section 2), that allows the loss in the current round to depend on the entire history of decisions until that point. We introduce the notion of \(p\)-effective memory capacity, \(H_{p}\), that quantifies the maximum influence of past decisions on present losses. We prove an \(O(T})\) upper bound on the policy regret (Theorem 3.1) and a matching (worst-case) lower bound (Theorem 3.2). As a special case, we prove the first non-trivial lower bound for OCO with finite memory (Theorem 3.4), which could be of independent interest, and also improve existing upper bounds (Theorem 3.3). Our lower bound technique extends existing techniques developed for memoryless settings. We design novel adversarial loss functions that exploit the fact that an algorithm cannot overwrite its history. We illustrate the power of our framework by bringing together the regret analysis of two seemingly disparate problems under the same umbrella. First, we show how our framework improves and simplifies existing regret bounds for the online linear control problem (Agarwal et al., 2019) in Theorem 4.1. Second, we show how our framework can be used to derive regret bounds for an online variant of performative prediction (Perdomo et al., 2020) in Theorem 4.2. This demonstrates the broad applicability of our framework for deriving regret bounds for a variety of online learning problems, particularly those that exhibit long-term dependence of current losses on past decisions.

Related work.The most closely related work to ours is the OCO with finite memory framework (Anava et al., 2015). They consider a generalization of the OCO framework that allows the current loss to depend on a _constant_ number of past decisions. There have been a number of follow-up works that extend the framework in a variety of other ways, such as non-stationarity (Zhao et al., 2022), incorporating switching costs (Shi et al., 2020), etc. However, none of these existing works go beyond a constant memory length and do not prove a non-trivial lower bound with a dependence on the memory length. In a different line of work, Bhatia and Sridharan (2020) consider a much more general online learning framework that goes beyond a constant memory length, but they only provide _non-constructive_ upper bounds on regret. In contrast, our OCO with unbounded memory framework allows the current loss to depend on an _unbounded_ number of past decisions, provides _constructive_ upper bounds on regret, and lower bounds for a broad class of problems that includes OCO with finite memory with a general memory length \(m\).

A different framework for sequential decision-making is multi-armed bandits (Bubeck and Cesa-Bianchi, 2012; Slivkins, 2019). Qin et al. (2023) study a variant of contextual stochastic bandits where the current loss can depend on a sparse subset of all prior contexts. This setting differs from ours due to the feedback model, stochasticity, and decision space. Reinforcement learning (Sutton and Barto, 2018) is yet another popular framework for sequential decision-making that considers very general state-action models of feedback and dynamics. In reinforcement learning one typically measures regret with respect to the best state-action policy from some policy class, rather than the best fixed decision as in online learning and OCO. In the special case of linear control, policies can be reformulated as decisions while preserving convexity; we discuss this application in Section 4. Considering the general framework is an active area of research.

We defer discussion of related work for specific applications to Section 4.

Framework

We begin with some motivation for the formalism used in our framework (Section 2.1). Many real-world applications involve controlling a physical dynamical system, for example, variable-speed wind turbines in wind energy electric power production (Boukhezzar and Siguerdidjane, 2010). The typical solution for these problems has been to model them as offline control problems with linear time-invariant dynamics and use classical methods such as LQR and LQG (Boukhezzar and Siguerdidjane, 2010). Instead of optimizing over the space of control inputs, the typical feedback control approach optimizes over the space of controllers, i.e., policies that choose a control input as a function of the system state. The standard controllers considered in the literature are linear controllers. Even when the losses are convex in the state and input, they are nonconvex in the linear controller. In the special case of quadratic losses in terms of the state and input, there is a closed-form solution for the optimal solution using the algebraic Riccati equations (Lancaster and Rodman, 1995). But this does not hold for general convex losses resulting in convex reparameterizations such as Youla (Youla et al., 1976; Kucera, 1975) and SLS (Wang et al., 2019; Anderson et al., 2019). The resulting parameterization represents an infinite dimensional system response and is characterized by a sequence of matrices. Recent work has studied an online approach for some of these control theory problems, where a sequence of controllers is chosen adaptively rather than choosing one offline (Abasi-Yadkori and Szepesvari, 2011; Dean et al., 2018; Simchowitz and Foster, 2020; Agarwal et al., 2019).

The takeaway from the above is that there are online learning problems in which (i) the current loss depends on the entire history of decisions; and (ii) the decision space can be more complicated than just a subset of \(^{d}\), e.g., it can be an unbounded sequence of matrices. This motivates us to model the decision space as a Hilbert space and the history space as a Banach space in the formal problem setup below, and this subsumes the special cases of OCO and OCO with finite memory. This formalism not only lets us consider a wide range of spaces, such as \(^{d}\), unbounded sequences of matrices, etc., but also lets us define appropriate norms on these spaces. This latter feature is crucial for deriving strong regret bounds for some applications such as online linear control. For this problem we derive improved regret bounds (Theorem 4.1) by defining weighted norms on the decision and history spaces, where the weights are chosen to leverage the problem structure.

Notation.We use \(\|\|_{}\) to denote the norm associated with a space \(\). The operator norm for a linear \(L\) operator from space \(\) is defined as \(\|L\|_{}=_{u:\|u\|_{U} 1}\|Lu\|_{}\). For convenience, sometimes we simply use \(\|\|\) when the meaning is clear from the context. For a finite-dimensional matrix we use \(\|\|_{F}\) and \(\|\|_{2}\) to denote its Frobenius norm and operator norm respectively.

### Setup

Let the decision space \(\) be a closed and convex subset of a Hilbert space \(\) with norm \(\|\|_{}\) and the history space \(\) be a Banach space with norm \(\|\|_{}\). Let \(A:\) and \(B:\) be linear operators. The game between the learner and an oblivious adversary proceeds as follows. Let \(T\) denote the time horizon and \(f_{t}:\) be loss functions chosen by the adversary. The initial history is \(h_{0}=0\). In each round \(t[T]\), the learner chooses \(x_{t}\), the history is updated to \(h_{t}=Ah_{t-1}+Bx_{t}\), and the learner suffers loss \(f_{t}(h_{t})\). An instance of an online convex optimization with unbounded memory problem is specified by the tuple \((,,A,B)\).

We use the notion of policy regret (Dekel et al., 2012) as the performance measure in our framework. The policy regret of a learner is the difference between its total loss and the total loss of a strategy that plays the best fixed decision in every round. The history after round \(t\) for a strategy that chooses \(x\) in every round is described by \(h_{t}=_{k=0}^{t-1}A^{k}Bx\), which motivates the following definition.

**Definition 2.1**.: Given \(f_{t}:\), the function \(_{t}:\) is defined by \(_{t}(x)=f_{t}(_{k=0}^{t-1}A^{k}Bx)\).

**Definition 2.2** (Policy Regret).: The policy regret of an algorithm \(\) is defined as \(R_{T}()=_{t=1}^{T}f_{t}(h_{t})-_{x}_{t=1 }^{T}_{t}(x)\).

In many motivating examples such as online linear control (Section 4.1), the history at the end of a round is a sequence of linear transformations of past decisions. The following definition captures this formally and we leverage this structure to prove stronger regret bounds (Theorem 3.1).

**Definition 2.3** (Linear Sequence Dynamics).: Consider an online convex optimization with unbounded memory problem specified by \((,,A,B)\). Let \((_{k})_{k=0}^{}\) be a sequence of nonnegative real numbers satisfying \(_{0}=1\). We say that \((,,A,B)\) follows linear sequence dynamics with the \(\)-weighted \(p\)-norm for \(p 1\) if

1. \(\) is the \(\)-weighted \(^{p}\)-direct sum of a finite or countably infinite number of copies of \(\): every element \(y\) is a sequence \(y=(y_{i})_{i}\), where \(=\) or \(=\{0,,n\}\) for some \(n N\), and \(\|y\|_{}=(_{i}(_{i}\|y_{i}\|)^{p})^ {}{{p}}}<\).
2. We have \(A(y_{0},y_{1},)=(0,A_{0}y_{0},A_{1}y_{1},)\), where \(A_{i}:\) are linear operators.
3. The operator \(B\) satisfies \(B(x)=(x,0,)\).

Note that since the norm on \(\) depends on the weights \(\), the operator norm \(\|A^{k}\|\) also depends on \(\). If the weights are all equal to \(1\), then we simply say \(p\)-norm instead of \(\)-weighted \(p\)-norm.

### Assumptions

We make the following assumptions about the feedback model and the loss functions.

1. The learner knows the operators \(A\) and \(B\), and observes \(f_{t}\) at the end of each round \(t\).
2. The operator norm of \(B\) is at most \(1\), i.e., \(\|B\| 1\).
3. The functions \(f_{t}\) are convex.
4. The functions \(f_{t}\) are \(L\)-Lipschitz continuous: \(\ h,\) and \(t[T]\), we have \(|f_{t}(h)-f_{t}()| L\|h-\|_{}\).

Regarding Assumption **A1**, our results easily extend to the case where instead of observing \(f_{t}\), the learner receives a gradient \(_{t}(x_{t})\) from a gradient oracle, which can be implemented using knowledge of \(f_{t}\) and the dynamics \(A\) and \(B\). Handling the cases when the operators \(A\) and \(B\) are unknown and/or the learner observes bandit feedback (i.e., only \(f_{t}(h_{t})\)) are important problems and we leave them as future work. Note that our assumption that \(A\) and \(B\) are known is no more restrictive than in the existing literature on OCO with finite memory (Anava et al., 2015) where it is assumed that the learner knows the constant memory length. In fact, our assumption is more general because our framework not only captures constant memory length as a special case but allows for richer dynamics as we illustrate in Section 4. Assumption **A2** is made for convenience, and it amounts to a rescaling of the problem. Assumption **A3** can be replaced by the _weaker_ assumption that \(_{t}\) are convex (similar to the literature on OCO with finite memory (Anava et al., 2015)) and this is what we use in the rest of the paper.

Assumptions **A1** and **A4** imply that \(_{t}\) are \(\)-Lipschitz continuous for the following \(\).

**Theorem 2.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((,,A,B)\). If \(f_{t}\) is \(L\)-Lipschitz continuous, then \(_{t}\) is \(\)-Lipschitz continuous for \( L_{k=0}^{}\|A^{k}\|\). If \((,,A,B)\) follows linear sequence dynamics with the \(\)-weighted \(p\)-norm for \(p 1\), then \( L(_{k=0}^{}\|A^{k}\|^{p})^{}\)._

The proof follows from the definitions of \(_{t}\) and \(\|\|_{}\), and we defer it to Appendix A. The above bound is tighter than similar results in the literature on OCO with finite memory and online linear control. This theorem is a key ingredient, amongst others, in improving existing upper bounds on regret for OCO with finite memory (Theorem 3.3) and for online linear control (Theorem 4.1). Before presenting our final assumption we introduce the notion of \(p\)-effective memory capacity that quantifies the maximum influence of past decisions on present losses.

**Definition 2.4** (\(p\)-Effective Memory Capacity).: Consider an online convex optimization with unbounded memory problem specified by \((,,A,B)\). For \(p 1\), the \(p\)-effective memory capacity is defined as

\[H_{p}(,,A,B)=(_{k=0}^{}k^{p}\|A^{k}\|^{p} )^{}.\] (1)When the meaning is clear from the context we simply use \(H_{p}\) instead. The \(p\)-effective memory capacity is an upper bound on the difference in histories for two sequences of decisions whose difference grows at most linearly with time. To see this, consider two sequences of decisions, \((x_{k})\) and \((_{k})\), whose elements differ by no more than \(k\) at time \(k\): \(\|x_{k}-_{k}\| k\). Then the histories generated by the two sequences have difference between bounded as \(\|h-\|=\|_{k}A^{k}B(x_{k}-_{k})\|_{k}k\|A^{k}B\| _{k}k\|A^{k}\|=H_{1}\), where the last inequality follows from Assumption A2. A similar bound holds with \(H_{p}\) instead when \((,,A,B)\) follows linear sequence dynamics with the \(\)-weighted \(p\)-norm.

* The \(1\)-effective memory capacity is finite, i.e., \(H_{1}<\).

Since \(H_{p}\) is decreasing in \(p\), \(H_{1}<\) implies \(H_{p}<\) for all \(p 1\). For the case of linear sequence dynamics with the \(\)-weighted \(p\)-norm it suffices to make the _weaker_ assumption that \(H_{p}<\). However, for simplicity of exposition, we assume that \(H_{1}<\).

### Special Cases

OCO with Finite Memory.Consider the OCO with finite memory problem with constant memory length \(m\). It can be specified in our framework by \((,,A_{,m},B_{,m})\), where \(\) is the \(^{2}\)-direct sum of \(m\) copies of \(\), \(A_{,m}(x^{[m]},,x^{})=(0,x^{[m]},,x^{})\), and \(B_{,m}(x)=(x,0,,0)\). Note that \((,,A_{,m},B_{,m})\) follows linear sequence dynamics with the \(2\)-norm. Our framework can even model an extension where the problem follows linear sequence dynamics with the \(p\)-norm for \(p 1\) by simply defining \(\) to be the \(^{p}\)-direct sum of \(m\) copies of \(\).

OCO with \(\)-discounted Infinite Memory.Our framework can also model OCO with infinite memory problems that are not modelled by existing OCO frameworks. Let \((0,1)\) be the discount factor and \(p 1\). An OCO with \(\)-discounted infinite memory problem is specified by \((,,A_{,},B_{,})\), where \(\) is the \(^{p}\)-direct sum of countably many copies of \(\), \(A_{,}((y_{0},y_{1},))=(0, y_{0}, y_{1},)\), and \(B_{,}(x)=(x,0,)\). Note that \((,,A_{,},B_{,})\) follows linear sequence dynamics with the \(p\)-norm. Due to space constraints we defer proofs of regret bounds for this problem to the appendix.

## 3 Regret Analysis

We present two algorithms for choosing the decisions \(x_{t}\). Algorithm 1 uses follow-the-regularized-leader (FTRL) (Shalev-Shwartz and Singer, 2006; Abernethy et al., 2008) on the loss functions \(_{t}\). Due to space constraints, we discuss how to implement it efficiently in Appendix G and present simple simulation experiments in Appendix I. Algorithm 2, which we only present in Appendix H, combines FTRL with a mini-batching approach (Dekel et al., 2012; Altschuler and Talwar, 2018; Chen et al., 2020) to additionally guarantee that the decisions switch at most \(O(}}{{LH_{1}}})\) times. We defer the proofs of the following upper and lower bounds to Appendices C and D respectively.

``` Input : Time horizon \(T\), step size \(\), \(\)-strongly-convex regularizer \(R:\).
1 Initialize history \(h_{0}=0\).
2for\(t=1,2,,T\)do
3 Learner chooses \(x_{t}_{x}_{s=1}^{t-1}_{s}(x)+\).
4 Set \(h_{t}=Ah_{t-1}+Bx_{t}\).
5 Learner suffers loss \(f_{t}(h_{t})\) and observes \(f_{t}\).
6 end for ```

**Algorithm 1**Ftrl

**Theorem 3.1**.: _Consider an online convex optimization with unbounded memory problem specified by \((,,A,B)\). Let the regularizer \(R:\) be \(\)-strongly-convex and satisfy \(|R(x)-R()|\) for all \(x,\). Algorithm 1 with step-size \(\) satisfies \(R_{T}()+^{2}}{}+ H_{1}}{}\). If

[MISSING_PAGE_FAIL:6]

where term (a) is the random sign \(_{n}\) sampled for the block \(n= t/m\) that \(t\) belongs to, term (b) is a scaling factor chosen while respecting the Lipschitz continuity constraint, and term (c) is a sum over a _subset_ of past decisions. Two important features of this construction are: (i) a random sign is sampled for each block rather than each round; and (ii) the loss in round \(t\) depends on the history of decisions until and including the first round of the block that \(t\) belongs to. These exploit the fact that an algorithm cannot overwrite its history and penalize it for its past decisions even after it observes the random sign \(_{n}\) for the current block. (See Fig. 1 for an illustration.) Existing lower bound proofs for OCO sample a random sign in each round and choose \(f_{t}(x_{t})_{t}x_{t}\). A first attempt at extending this for the OCO with finite memory setting would be to choose \(f_{t}(h_{t})_{t}_{k=0}^{m-1}x_{t-k}\). However, in constrast to our approach, this does not exploit the fact that an algorithm cannot overwrite its history and does not suffice for obtaining a matching lower bound.

Comparison of upper bound with prior work.The algorithmic ideas and analysis for our regret upper bound are influenced by Anava et al. (2015). However, an important innovation in our work is the use of weighted norms in the case of linear sequence dynamics. This is a simple but powerful way of encoding prior knowledge about a problem, and allows us to derive non-trivial regret bounds in the case of unbounded-length histories. The technical complications that arise are captured in bounding the relevant quantities of interest, e.g., the Lipschitz constant \(\), the operator norm \(\|A^{k}\|\), etc. Furthermore, using weighted norms even leads to improved regret bounds for some applications. Indeed, consider the application to online linear control with adversarial disturbances (Section 4.1). Our framework and upper bound applied to this problem (Theorem 4.1) improve upon the existing upper bound, which used a finite memory approximation. See Lemmas E.2 and E.6 for an illustration of the technical details involved when using weighted norms.

## 4 Applications

In this section we apply our framework to online linear control (Section 4.1) and online performative prediction (Section 4.2). We defer expanded details and proofs to Appendices E and F respectively.

### Online Linear Control

Background.Online linear control (OLC) is the problem of controlling a system with linear dynamics, adversarial disturbances, and adversarial and convex losses. It combines aspects from control theory and online learning. We refer the reader to Agarwal et al. (2019) for more details. Here, we introduce the basic mathematical setup of the problem.

Let \(^{d_{s}}\) and \(^{d_{u}}\) denote the state and control spaces. Let \(s_{t}\) and \(u_{t}\) denote the state and control at time \(t\) with \(s_{0}\) being the initial state. The system evolves according to the linear dynamics \(s_{t+1}=Fs_{t}+Gu_{t}+w_{t}\), where \(F^{d_{s} d_{s}},G^{d_{s} d_{u}}\) are matrices satisfying \(\|F\|_{2},\|G\|_{2}\) and \(w_{t}^{d_{s}}\) is an adversarially chosen disturbance with \(\|w\|_{2} W\). Without loss of generality, we assume that \(,W 1\), \(d_{s}=d_{u}=d\), and also define \(w_{-1}=s_{0}\). For \(t=0,,T-1\), let \(c_{t}:\) be convex loss functions chosen by an oblivious adversary. The functions \(c_{t}\) satisfy the following Lipschitz condition: if \(\|s\|_{2}\), \(\|u\|_{2} D_{}\), then \(\|_{s}c_{t}(s,u)\|,\|_{u}c_{t}(s,u)\| L_{0}D_{}\). The goal in online linear control is to choose a sequence of policies that yield a sequence of controls \(u_{t}\) to minimize the regret \(R_{T}()=_{t=0}^{T-1}c_{t}(s_{t},u_{t})-_{^{*}}_{t=0}^{ T-1}c_{t}(s_{t}^{^{*}},u_{t}^{^{*}})\), where \(s_{t}\) evolves according to linear dynamics stated above, \(\) denotes a controller class, and \(s_{t}^{^{*}},c_{t}^{^{*}}\) denote the state and control at time \(t\) when the controls are chosen according to \(^{*}\).

Figure 1: An illustration of the loss functions \(f_{t}\) for the OCO with finite memory lower bound.

A very simple controller class is constant input, i.e., \(=\{_{u}:(s)=u\}\). In this case, the history \(h_{t}\) can be represented by the finite-dimensional state \(s_{t}\), and the operators can be set to \(A=F\) and \(B=}{{\|G\|}}\). However, like previous work (Agarwal et al., 2019) we focus on the class of \((,)\)-strongly stable linear controllers, \(\), where \(K\) satisfies \(F-GK=HLH^{-1}\) with \(\|K\|_{2},\|H\|_{2},\|H^{-1}\|_{2}\) and \(\|L\|_{2}=<1\). Given such a controller, the inputs are chosen as linear functions of the current state, i.e., \(u_{t}=-Ks_{t}\). Unfortunately, parameterizing \(u_{t}\) directly with a linear controller as \(u_{t}=-Ks_{t}\) leads to a non-convex problem because \(s_{t}\) is a non-linear function of \(K\), e.g., if disturbances are \(0\), then \(s_{t}=(F-GK)^{}s_{0}\). An alternative parameterization is the disturbance-action controller (DAC).

**Definition 4.1**.: Let \(K\) be fixed. The class of disturbance-action controllers (DACs) \(_{K}\) is \(\{(K,M):M=(M^{[s]})_{s=0}^{}\}\), where \(M^{[s]}^{d d}\) satisfies \(\|M^{[s]}\|_{2}^{4}^{s}\). The control in round \(k\) is chosen as \(u_{k}=-Ks_{k}+_{s=1}^{k+1}M^{[s]}w_{k-s}\).

The class of such DACs has two important properties. First, it acts on the entire history of past disturbances. Consequently, given an arbitrary \(K\), every \(K^{*}\) can be expressed as a DAC \((K,M)_{K}\) with \(M=(M^{},,M^{[T+1]},0,)\)(Agarwal et al., 2019, Section 16.5). That is, \(_{K}\) and it suffices to compute regret against \(_{K}\) instead of \(\). For the rest of this paper we fix \(K\) and denote \(=F-GK\). Second, suppose \(M_{t}=(M_{t}^{[s]})_{s=0}^{}\) is the parameter chosen in round \(t\) and the control \(u_{t}\) is chosen according to the DAC \((K,M_{t})\). Then, \(s_{t}\) and \(u_{t}\) are _linear_ functions of the parameters, which implies that \(c_{t}\) is convex in the parameters. (See the next paragraph on "Formulation as OCO with Unbounded Memory" for a formula.) A similar parameterization was first considered for online linear control by Agarwal et al. (2019) and is based on similar ideas in control theory, e.g., Youla (Youla et al., 1976, Kucera, 1975) and SLS (Wang et al., 2019, Anderson et al., 2019).

Formulation as OCO with Unbounded Memory.The first step is a change of variables with respect to the control inputs from linear controllers to DACs and the second is a corresponding change of variables for the state. Define the decision space \(=\{M=(M^{[s]}):M^{[s]}^{d d},\|M^{[s]}\|_{2} ^{4}^{s}\}\). Define the history space \(\) to be the set consisting of sequences \(h=(Y_{k})\), where \(Y_{0}\) and \(Y_{k}=^{k-1}GX_{k}\) for \(X_{k},k 1\). (Recall \(=F-GK\).) Define weighted norms \(\|M\|_{}^{2}=_{s=1}^{}^{-s}\|M^{[s]}\|_{F}^{2}\) and \(\|h\|_{}^{2}=_{k=0}^{}_{k}^{2}\|Y_{k}\|_{} ^{2}\), where the weights \((_{k})\) are defined as \(=(1,1,1,^{-1},^{-1},^{-},)\). Define the linear operators \(A:\) and \(B:\) as \(A((Y_{0},Y_{1},))=(0,GY_{0},Y_{1},Y_{2},)\) and \(B(M)=(M,0,0,)\). Note that the problem follows linear sequence dynamics with the \(\)-weighted 2-norm (Definition 2.3). The weights in the norms on \(\) and \(\) increase exponentially. However, the norms \(\|M^{[s]}\|_{F}^{2}\) and \(\|^{k-1}G\|_{F}^{2}\) decrease exponentially as well: by definition of \(\) and the assumption on \(=F-GK\) for \(K\). Leveraging these exponential decays to define exponentially increasing weights is crucial for deriving our regret bounds that are stronger than existing results.

Construct the functions \(f_{t}:\) that correspond to \(c_{t}(s_{t},u_{t})\) as follows. Given a sequence of decisions \((M_{0},,M_{t})\), the history at the end of round \(t\) is given by \(h_{t}=(M_{t},GM_{t-1},GM_{t-2},,^{t-1}GM_{0}, 0,)\). A simple inductive argument shows that the state and control in round \(t\) can be written as functions of \(h_{t}\) as \(s_{t}=^{t}s_{0}+_{k=0}^{t-1}_{s=1}^{k+1}^{t- k-1}GM_{t}^{[s]}w_{k-s}+w_{t-1}\) and \(u_{t}=-Ks_{t}+_{s=1}^{t+1}M_{t}^{[s]}w_{t-s}\). Define the functions \(f_{t}:\) by \(f_{t}(h)=c_{t}(s,u)\), where \(s\) and \(u\) are the state and control determined by the history as above. Note that \(f_{t}\) is parameterized by the past disturbances. Since the state and control are linear functions of the history and \(c_{t}\) is convex, this implies that \(f_{t}\) is convex. Now, given the above formulation and the fact that the class of disturbance-action controllers is a superset of the class of \((,)\)-strongly-stable linear controllers, we have that the policy regret for the online linear control problem is at most the policy regret, \(_{t=0}^{T-1}f_{t}(h_{t})-_{M}_{t=0}^{T-1}_{ t}(M)\). The following is our main result for online linear control and it improves existing results (Agarwal et al., 2019) by a factor of \(O(d(T)^{3.5}^{5}(1-)^{-1})\). See Appendix E.3 for a detailed comparison.

**Theorem 4.1**.: _Consider the online linear control problem as defined in Section 4.1. Suppose the decisions in round \(t\) are chosen using Algorithm 1. Then, the upper bound on the policy regret is_

\[O(L_{0}W^{2}d^{}^{17}(1-)^{-4.5}).\] (2)Comparison with prior and concurrent work.Existing works solve OLC (and its extensions) by making multiple finite memory approximations. First, they formulate the problem as OCO with finite memory. This requires bounding numerous error terms because the problem is inherently an OCO with unbounded memory problem. We bypass these error analysis steps entirely because the problem fits into our framework naturally. Second, existing works use the parameterization from Agarwal et al. (2019) that only acts on a fixed, constant number of past disturbances. In particular, existing works use a "truncated" DAC policy that is a sequence of \(d d\) matrices of length \(2^{4}(1-)^{-1} T\). Our DAC policy acts on the entire history of disturbances and is a sequence of \(d d\) matrices of unbounded length. Yet, we capture the dimension of this infinite-dimensional space in a way that still improves the overall bound, including completely eliminating the dependence on \( T\), and improving the dependence on \(d,\), and \((1-)\). This improvement comes from our novel use of weighted norms on the history and decision spaces. These norms allow us to give tighter bounds on the relevant quantities in the regret upper bound, e.g., \(\|A^{k}\|\) (Lemma E.2) and \(\) (Lemma E.6).

In complementary concurrent work, Lin et al. (2022) focus on a more general online control problem. They improve regret bounds for this general version by a factor of \( T\) compared to existing reductions to OCO with finite memory. They do so by using that the impact of a past policy decays geometrically with time. On the other hand, the primary focus of our work is studying the complete dependence of present losses on the entire history in OCO. Applying our resulting OCO with unbounded memory framework to OLC, we improve upon existing results for OLC by removing all \( T\) factors and improving the dependence on \(d,\), and \((1-)\).

### Online Performative Prediction

Background.In many applications of machine learning the algorithm's decisions influence the data distribution, e.g., online labor markets (Anagnostopoulos et al., 2018; Horton, 2010), predictive policing (Lum and Isaac, 2016), on-street parking (Dowling et al., 2020; Pierce and Shoup, 2018), vehicle sharing markets (Banerjee et al., 2015), etc. Motivated by such applications, several works have studied the problem of performative prediction, which models the data distribution as a function of the decision-maker's decision (Perdomo et al., 2020; Mendler-Dunner et al., 2020; Miller et al., 2021; Brown et al., 2022; Ray et al., 2022; Jagadeesan et al., 2022). Most of these works view the problem as a stochastic optimization problem; Jagadeesan et al. (2022) adopt a regret minimization perspective. We refer the reader to these citations for more details. As a natural extension to existing works, we introduce an online learning variant of performative prediction with geometric decay (Ray et al., 2022) that differs from the original formulations in a few key ways.

Let the decision set \(^{d}\) be closed and convex with \(\|x\|_{2} D_{}\). Let \(p_{1}\) denote the initial data distribution over the instance space \(\). In each round \(t[T]\), the learner chooses a decision \(x_{t}\) and an oblivious adversary chooses a loss function \(l_{t}:\), and then the learner suffers the loss \(L_{t}(x_{t})=_{z p_{t}}[l_{t}(x_{t},z)]\), where \(p_{t}=p_{t}(x_{1},,x_{t})\) is the data distribution in round \(t\). The goal in our online learning setting is to minimize the difference between the algorithm's total loss and the total loss of the best fixed decision, \(_{t=1}^{T}_{z p_{t}}[l_{t}(x_{t},z)]-_{x }_{t=1}^{T}_{z p_{t}(x)}[l_{t}(x,z)]\), where \(p_{t}(x)=p_{t}(x,,x)\) is the data distribution in round \(t\) had \(x\) been chosen in all rounds so far. This measure is similar to performative regret (Jagadeesan et al., 2022) and is a natural generalization of performative optimality (Perdomo et al., 2020) for an online learning formulation.

We make the following assumptions. First, the loss functions \(l_{t}\) are convex and \(L_{0}\)-Lipschitz continuous. Second, the data distribution satisfies for all \(t 1\), \(p_{t+1}= p_{t}+(1-)(x_{t})\), where \((0,1)\) and \((x_{t})\) is a distribution over \(\) that depends on the decision \(x_{t}\)(Ray et al., 2022). Third, \((x)\) is a location-scale distribution: \(z(x)\) iff \(z+Fx\), where \(F^{d d}\) satisfies \(\|F\|_{2}<\) and \(\) is a random variable with mean \(\) and covariance \(\)(Ray et al., 2022).

Our problem formulation differs from existing work in the following ways. First, we adopt an online learning perspective on performative prediction with geometric decay, whereas Ray et al. (2022) adopt a stochastic optimization one. So, we assume that the loss functions \(l_{t}\) are adversarially chosen, whereas Ray et al. (2022) assume \(l_{t}=l\) are fixed. Second, we assume that the dynamics (\(\) and \(\)) are known (Assumption A1), whereas Ray et al. (2022) assume they are unknown and use samples from the data distribution. We believe that an appropriate extension of our framework that can deal with unknown linear operators \(A\) and \(B\) can be applied to this more difficult setting, and we leave this as future work. Third, even though Jagadeesan et al. (2022) also study an online learning variant of performative prediction, they assume \(l_{t}=l\) are fixed and the data distribution depends only on the current decisions, whereas we assume the data distribution depends on the entire history of decisions.

Formulation as OCO with Unbounded Memory.Let \((0,1)\). Let the decision space \(^{d}\) be closed and convex with the \(2\)-norm. Let the history space \(\) be the \(^{1}\)-direct sum of countably infinite number of copies of \(\). Define the linear operators \(A:\) and \(B:\) as \(A((y_{0},y_{1},))=(0, y_{0}, y_{1},)\) and \(B(x)=(x,0,)\). The problem is an OCO with \(\)-discounted infinite memory problem and follows linear sequence dynamics with the \(1\)-norm (Definition 2.3). Given a sequence of decisions \((x_{k})_{k=1}^{t}\), the history is \(h_{t}=(x_{t}, x_{t-1},,^{t-1}x_{1},0,)\) and the data distribution \(p_{t}=p_{t}(h_{t})\) satisfies: \(z p_{t}\) iff \(z_{k=1}^{t-1}(1-)^{k-1}(+Fx_{t-k})+^{t}p_{1}\). This follows from the recursive definition of \(p_{t}\) and parametric assumption about \((x)\). Define the functions \(f_{t}:\) by \(f_{t}(h_{t})=_{z p_{t}}[l_{t}(x_{t},z)]\). Now, the original goal of minimizing the difference between the algorithm's total loss and the total loss of the best fixed decision is equivalent to minimizing the policy regret. The following is our main result for online performative prediction.

**Theorem 4.2**.: _Consider the online performative prediction problem as defined in Section 4.2. Suppose the decisions in round \(t\) are chosen using Algorithm 1. Then, the upper bound on the policy regret is_

\[O(D_{}L_{0}\|F\|_{2}(1-)^{-}^{-1} ).\]

## 5 Conclusion

In this paper we introduced a generalization of the OCO framework, "Online Convex Optimization with Unbounded Memory", that allows the loss in the current round to depend on the entire history of decisions until that point. We proved matching upper and lower bounds on the policy regret in terms of the time horizon, the \(p\)-effective memory capacity (a quantitative measure of the influence of past decisions on present losses), and other problem parameters (Theorems 3.1 and 3.2). As a special case, we proved the first non-trivial lower bound for OCO with finite memory (Theorem 3.4), which could be of independent interest, and also improved existing upper bounds (Theorem 3.3). We illustrated the power of our framework by bringing together the regret analysis of two seemingly disparate problems under the same umbrella: online linear control (Theorem 4.1), where we improve and simplify existing regret bounds, and online performative prediction (Theorem 4.2).

There are a number of directions for future research. A natural follow-up is to consider unknown dynamics (i.e., when the learner does not know the operators \(A\) and \(B\)) and/or the case of bandit feedback (i.e., when the learner only observes \(f_{t}(h_{t})\)). The extension to bandit feedback has been considered in the OCO and OCO with finite memory literature (Hazan and Li, 2016; Bubeck et al., 2021; Zhao et al., 2021; Gradu et al., 2020; Cassel and Koren, 2020). It is tempting to think about a version where the history is a _nonlinear_, but decaying, function of the past decisions. The obvious challenge is that the nonlinearity would lead to non-convex losses. It is unclear how to deal with such issues, e.g., restricted classes of nonlinearities for which the OCO with unbounded memory perspective is still relevant (Zhang et al., 2015), different problem formulations such as online non-convex learning (Gao et al., 2018; Suggala and Netrapalli, 2020), etc.

There is a growing body of work on online linear control and its variants that rely on OCO with finite memory (Hazan et al., 2020; Agarwal et al., 2019; Foster and Simchowitz, 2020; Cassel and Koren, 2020; Gradu et al., 2020; Li et al., 2021; Minasyan et al., 2021). In this paper we showed how our framework can be used to improve and simplify regret bounds for the online linear control problem. Another direction for future work is to use our framework, perhaps with suitable extensions outlined above, to derive similar improvements for these other variants of online linear control.