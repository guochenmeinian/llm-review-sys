# Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time

Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time

Zichang Liu

Department of Computer Science

Rice University

zichangliu@rice.edu

&Aditya Desai

Department of Computer Science

Rice University

Aditya.P.Desai@rice.edu

&Fangshuo Liao

Department of Computer Science

Rice University

Fangshuo.Liao@rice.edu

&Weitao Wang

Department of Computer Science

Rice University

wtwang@rice.edu

&Victor Xie

Department of Computer Science

Rice University

vyx2@rice.edu

&Zhaozhuo Xu

Department of Computer Science

Stevens Institute of Technology

zxu79@stevens.edu

&Anastasios Kyrillidis

Department of Computer Science

Rice University

anastasios@rice.edu

&Anshumali Shrivastava

Department of Computer Science

Rice University & ThirdAI Corp.

anshumali@rice.edu

###### Abstract

Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize _the persistence of importance_: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of KV cache under a fixed budget without finetuning the model. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5\(\) without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization for further compression Introduction

Large language models(LLMs), trained on immense amounts of text data, have demonstrated an incredible ability to generate text that is both logically connected and contextually relevant . LLM inference follows an autoregressive fashion, generating one token at each step conditioned on the previous steps. At each step, the key-value embedding in attention is stored in memory to avoid repetitive key-value projection computation at future steps. Unfortunately, the memory of the key-value cache( KV cache), including prompts and previously generated tokens, can be surprisingly large. Using OPT-175B as an example, the impressive 175 billion parameters consume around 325 GB of memory. At the same time, at batch size 128 and sequence length 2048, the KV cache requires around 950 GB of memory, three times larger than the model weights. Considering that 8 Nvidia A100-80GB offers 640GB GPU memory, the memory usage of the KV cache is truly concerning.

LLMs are typically deployed on fixed memory hardware, and the size of model weights is also fixed once deployed. Apart from a small memory buffer typically reserved for communication and computation, the rest of the available memory is for the KV cache. The size of the KV cache depends on batch size, sequence length, and model dimension. Thus, at a given inference sequence length, compression in the KV cache memory translates almost linearly into an increase in the batch size. And any increase in batch size is significant for high-throughput inference systems .

Quantization and sparsity approaches  have been studied in LLMs to reduce the model sizes. However, compressing the KV cache remains an open but challenging problem. First, training models at the scale of hundreds of billions of parameters on a large amount of data is prohibitively expensive. Thus, an ideal compression algorithm should be applicable without training. Second, emerging applications such as dialogue systems require an extremely long context window. The maximum sequence length of LLMs is growing to over 32K . The size of the KV cache also grows linearly with sequence length. For scalability, an ideal compression algorithm should reduce the memory from the sequence length dimension. At last, compression should preserve LLMs' quality and in-context learning ability.

We go beyond the traditional model compression techniques to achieve such demanding requirements. We envision that not all tokens must be stored in memory for LLM to understand the context. Just like humans can skim through an article and grasp the main idea, LLMs may also be able to skim and comprehend. It is commonly observed that the attention score from one token follows a strong power law distribution , meaning that one token will only heavily attend to a small number of tokens. More importantly, we observe **Repetitive Attention Pattern** from different tokens in the sequence in a trained LLM( Figure 1). Certain tokens are more important throughout the paragraph. Specifically, for two different tokens, there are similarities between which tokens they are heavily attending to and similarities between which tokens they are ignoring.

Inspired by the above observation, we articulate the **Persistence of Importance Hypothesis:**_Only pivotal tokens, which had a substantial influence at one previous step, will have a significant influence at a future step._ This hypothesis, if true, suggests that it is possible to foresee which token is likely to be important for future generations. Fortunately, we empirically verify that later tokens in the

Figure 1: **Repetitive Attention Pattern. We plot the attention map at three token positions in a sentence. Only five attention heads are plotted for a clearer presentation. We discretize the attention score such that the high score is dark green, and the low score is light green. In Figure 1(a), the token at position 178 pays heavy attention to positions 27, 63, 98, etc. This pattern is also present in the attention maps of position 228 and position 278.**

sentence mostly only attend to tokens that were heavily attended from the early tokens in a sentence. And the overlapping ratio is surprisingly high, over 90% in most of the transformer layers (Figure 2).

Based on the above two findings, we present Scissorhands that exploits the _persistence of importance hypothesis_ to realize LLM inference with a compressed KV cache. In Section 4, we present an efficient algorithm such that the size of KV cache is always less than a predetermined budget. A theoretical guarantee justifies that such a compressed KV cache can approximate the attention output. In Section 5, we empirically evaluate Scissorhands and show that Scissorhands reduces the memory usage of KV cache \(2-5\) without compromising model quality. Reduction in the KV cache can directly result in a larger batch size. Further, we adopt quantization and show its compatibility with Scissorhands.

## 2 Problem Description and Related Work

This paper considers the LLM inference workflow, specifically focusing on the memory usage for storing the keys and values in attention. Let \(d\) be the hidden dimension of the model, \(b\) be the batch size, and \(p\) be the length of prompt sentences. We are given the trained model weights, \(W^{i}_{K}^{d d}\), \(W^{i}_{V}^{d d}\) for the key and value projection matrix at the \(i^{th}\) transformer layer.

The standard LLM inference consists of two stages: prompting and token generation. In the prompt stage, the model takes the prompt sentences as the input, and the key/value embedding in attention is stored as a cache to reduce repetitive computation. Denote \(x^{i}_{}=[x^{i}_{1},...,x^{i}_{p}],x^{i}_{} ^{b p d}\) as the input to attention at the \(i^{th}\) transformer layer. Denote the key cache and value cache at layer \(i\) as \(^{i},^{i}^{b p d}\), \(^{i}_{0}=x^{i}_{}W^{i}_{K},^{i}_{0}=x^{i} _{}W^{i}_{V}\).

In the generation stage, the model starts with the stored KV cache in the prompting stage and generates one token at each step. At each step, the KV cache gets updated. Given the input to attention at step \(t\) in the \(i^{th}\) transformer layer \(x^{i}_{t}^{b 1 d}\). \(^{i}_{t+1}=[^{i}_{t},x^{i}_{t}W^{i}_{K}],^{i} _{t+1}=[^{i}_{t},x^{i}_{t}W^{i}_{V}]\).

### LLM Inference Memory Breakdown

In this section, we provide the memory consumption breakdown of LLMs. The memory footprint consists of three parts: model weights, KV cache, and activation buffer. The size of model weights depends on model configuration, such as the number of transformer layers and hidden size. The size of the KV cache depends on model configurations, sequence length, and batch size. The size of the activation buffer depends on parallelism strategy, model configurations, and implementation. The size of the activation buffer is considerably smaller than the previous two. As shown in Table 1, the size of the KV cache, 2.5\(\)-5\(\) larger than model weights, can quickly become the bottleneck in memory consumption. At the same time, much research has been spent on extending the length of the context window. GPT-4-32K can process up to 32,768 tokens . Longer sequence length would make the KV cache memory problem even more severe.

Assuming LLM generates until its maximum sequence length, we summarize the maximum batch size before going out of GPU memory on a box of 8 A100 80GB GPU in Table 2. At the GPT-3 scale with a maximum sequence length of 2048, batch size cannot exceed 35 without offloading. Small batch size limits the model inference throughput.

### Efficient Attention

Computing the attention matrix necessitates a time complexity of \(O(n^{2})\), where \(n\) is the sequence length. As a result, a line of work has been proposed to mitigate the computation burden of the attention mechanism [16; 17; 18; 19; 20]. These approaches exploit low-rank or sparsification to approximate the attention output. Besides,  realized exact efficient attention with wall-clock speed by optimizing

  Model & \# of Layer & Hidden Size & Weights (GB) & KV cache (GB) \\  OPT-175B & 96 & 12288 & 325 & 1152 \\  LLaMA-65B & 80 & 8192 & 130 & 640 \\  BLOOM & 70 & 14336 & 352 & 950 \\  

Table 1: The memory consumption of model weights and KV cache for three different LLMs at batch size 128 and sequence length 2048 shows that the KV cache dominates the memory consumption.

the number of memory reads and writes. However, these approaches were evaluated mostly for training, focused on computation complexity, and did not address the KV-Cache memory usage introduced by auto-regressive language models.

Recently, there is active research attempting to apply quantization or pruning in LLM . However, they mostly focus on reducing the size of model weights. Flexgen  applies quantization and sparsification to the KV cache; however, the memory of the KV cache is not reduced regarding sequence lengths. It stores the quantized KV cache for all tokens in CPU memory and loads all attention keys from CPU memory to compute attention scores. At the same time, methods such as Multi-Query-Attention(MQA)  change the attention design such that keys and values are shared across all attention heads. MQA requires training the model from scratch, while our works focus entirely on the inference stage.

## 3 The Persistence of Importance Hypothesis

We first present one interesting observation upon which the _persistence of importance hypothesis_ is derived in Section 3.1. In Section 3.2, we discuss the hypothesis in detail with empirical verification. Then, in Section 3.3, we provide theoretical intuition on the reason behind such model behaviors.

### Repetitive Attention Pattern.

**Observation.** We are interested in the attention score from the position \(t\) over all the words that come before it in the sentence. In Figure 1, we provide three attention maps of a sentence randomly drawn from the Colossal Clean Crawled Corpus (C4)  using OPT-6B. Each attention map is a discretized attention score calculated at a random position. We consider a score larger than \(\) as significant as \(\) indicates an averaging mixing score. High attention scores are marked with dark green.

**Result.** High attention scores are observed at the same set of tokens from various positions in the sentence. In all three plots, we see dark green at sequence positions 27, 63, 98, 121, 152, and 177, suggesting that these tokens received high attention at all three positions. We observe similar model behavior at different transformer layers with different text inputs. More plots are in Appendix A.

**Implication.** Even though small differences exist, repetitive attention patterns are evident in the attention maps. There exist specific tokens that keep receiving high attention. Meanwhile, these attention maps show sparsity: only a few tokens have high attention scores.

### The Persistence of Importance Hypothesis

The repetitive attention pattern suggests that specific tokens are influential throughout the sequence. A stricter claim is that these tokens are the only ones that could be significant for a future step. Thus, we articulate the _persistence of importance hypothesis_.

**The Persistence of Importance Hypothesis.**_With a trained autoregressive language model, only pivotal tokens, which had a substantial influence at one previous step, will have a significant influence at a future step._

If true, this hypothesis indicates the possibility of foreseeing what information in the previous sequences could be vital for future steps. This hypothesis is trivial when pivotal tokens include all tokens in the entire sentences. However, a much more interesting case is when pivotal tokens are a subset of previous words. This would enable us to reduce the size of the KV cache by throwing away the embedding of non-important tokens.

**Pivotal Token.** One natural indication of a token's influence is the attention score. We consider a token pivotal for position \(t\) if this token receives an attention score larger than threshold \(\) from the token at position \(t\). Let \(S_{t}\) denote the set of pivotal tokens for position \(t\). \(S_{a b}\) denote the set of

  Model & OPT-175B & LLaMA-65B & BLOOM \\  Maximum Batch Size & 34 & 102 & 36 \\  

Table 2: Maximum batch size before hitting out of memory on a box of 8 A100 80GB GPU when models are deployed with its maximum sequence length.

pivotal tokens for every position from \(a\) to \(b\).

\[S_{a b}=_{t=a}^{t=b}S_{t}\]

**Verification.** We measure _persistence ratio_ as an empirical test the hypothesis. _Persistence ratio_ measures how many tokens in the pivotal token sets of the later part of the sentence are also in the pivotal token sets of the initial part of the sentence. Let \(l\) denote the length of the sentence. We record \(S_{1 t}\{x_{1},...x_{t}\}\), tokens in \(\{x_{1},...,x_{t}\}\) who received high attention from every position until \(t\). Then, we record \(S_{t+1 l}\{x_{1},...x_{t}\}\), tokens in \(\{x_{1},...,x_{t}\}\) who received high attention from position after \(t\). The persistence ratio is the intersection divided by the size of \(S_{t+1 l}\). Formally,

\[}= S_{0 l}|}{|\{x|x  S_{t+1 l},x\{x_{1},...,x_{t}\}\}|}\]

At the same time, we measure \(|}{t}\). \(|S_{0 t}|=t\) indicates that every token substantially impacted at least one position, which is the trivial case of _persistence of importance hypothesis_. Our test is performed with OPT models  with different datasets such as OpenBookQA  and WikiText . In our verification, we set \(t=\), which measures the overlapping between the first and later half of the sentences. Same as in Section 3.1, we set \(=\), which suggests an average score.

**Result.** We present our main results in Figure 2. First, given the current criterion of pivotal token and \(t\) value, the size of \(S_{0 t}\) is considerably smaller than half of the sentence length. This verifies that we are not considering the trivial case of our hypothesis. Second, the persistence ratio is generally over 95%, with dips in the later transformer layers. The pivotal token set of the later half sentences is mostly included in the set of the first half sentences. Combining these two pieces of empirical evidence, we see positive evidence for our hypothesis test.

**Implication.** The hypothesis provides insights for understanding the behavior of LLMs and opens up new opportunities for reducing the KV cache memory. The hypothesis suggests the possibility of predicting the potentially influential tokens for future steps. The non-influential tokens are unnecessary to store in the memory, as they are unlikely to have high attention scores. This reduces the number of tokens stored in the KV cache and the computation required at the attention.

### Attention Weights Decides the Pivotal Tokens

In the previous section, we verified that the significant tokens would continue to be significant. In this section, we try to understand the reasons for such phenomena. We consider the token generation process of a simplified model: a single-layer transformer model with single-head attention.

\[x_{t+1}=(a_{t}),a_{t}= (}{{t}} x_{t}W_{Q}W_{K}^{}X_{t-1}^{})X_{t -1}W_{V}W_{O} \]

\(x_{t}^{1 d}\) is a row vector. \(X_{t-1}^{(t-1) d}\) denotes the aggregation of \(x_{1},,x_{t-1}\), where the \(j\)th row is \(x_{j}\). \(W_{Q},W_{K},W_{V}^{d d}\) and \(W_{O}^{d d}\) are the attention weights. Lastly, \(:^{1 d}^{1 d}\) denotes the MLP block following attention block, a two-layer MLP with skip connections, given by

\[(x)=x+W_{2}(W_{1}x) \]

Figure 2: Persistence ratio and the corresponding size of the pivotal token set. The persistence ratio is over 95% in most layers, with decreases at the later layers. Meanwhile, the number of pivotal tokens is considerably smaller than the sequence length. This suggests that the pivotal tokens of later half sentences are almost all included in the set of first halves.

```
Input: Memory Budget \(B\), Maximum Sequence Length \(T_{}\) Key Cache \(} R^{n d}\), Value Cache \(} R^{n d}\), where \(n=0\) while\(t<T_{}\)do  Model update \(},}\) such that \(n n+1\) if\(n>B\)then  Compress KV cache using Algorithm 2 such that \(n B\). endif \(t t+1\) endwhile
```

**Algorithm 1** Inference with Budget KV cache

We are interested in the attention scores \(_{t}=(}{{t}} x_{t}W_{Q}W_{K}^{}X_ {t-1}^{})\). Notice that \(_{t,j}\) scales with \(x_{t}W_{Q}W_{K}^{}x_{j}^{}\). The following theorem characterizes the behavior of \(x_{t}W_{Q}W_{K}^{}x_{j}^{}\)

**Theorem 3.1**.: _Let \(A=W_{V}W_{O}W_{Q}W_{K}^{}\) and let \(_{K},_{Q},_{V},_{O}\) denote the largest singular values of \(W_{K},W_{Q},W_{V},W_{O}\), respectively. Consider the transformer in (1) with normalized inputs \( x_{t}_{2}=1\) for all \(t\). Let \(c,>0\) be constants. Assume that \(a_{t}x_{t+1}^{}(1-) a_{t}_{2}\) with \((_{K}_{V}_{O} })^{2}\). Then for all \(x_{}\) satisfying \(x_{}Ax_{}^{} c\) and \(x_{}Ax_{}^{-1}_{j[t],j}x_{j}Ax_{}^{}\), it holds that_

\[Ax_{}^{}}{ a_{t}_{2}}(_{t, }-3) x_{t+1}W_{Q}W_{K}^{}x_{j}^{}Ax _{}^{}}{ a_{t}_{2}}(_{t,}+3) \]

The proof is provided in Appendix B. Theorem 3.1 shows that under an assumption on the MLP in (2), for all \(x_{}\) such that \(x_{}Ax_{}^{}\) is large enough, \(x_{t+1}W_{Q}W_{K}^{}x_{j}^{}\) satisfies Equation (3). The assumption on the MLP \(a_{t}x_{t+1}^{}(1-) a_{t}_{2}\) essentially requires a large cosine similarity between the input and output of \(\). This behavior can be empirically verified in Appendix A. Essentially, skip connection dominates the output because \( x_{2} W_{2}(W_{1}x) _{2}\), resulting in a cosine similarity close to one between input and output. Equation (3) shows that despite a factor of \(Ax_{}^{}}{ a_{t}_{2}}\), \(x_{t+1}W_{Q}W_{K}^{}x_{j}^{}\) almost scales with \(_{t,}\). Since \(x_{t+1}W_{Q}W_{K}^{}x_{j}^{}\) directly affects \(_{t+1,}\), this property shows that a larger \(_{t,}\) will potentially imply a large \(_{t+1,}\).

Our theorem shows that the property in Equation (3) property only holds for \(x_{}\) such that \(x_{}Ax_{}^{}\) is large. \(A\) are trained attention weights. This condition may suggest that the trained weights \(A\) selects \(x_{}\) as a pivotal token. Each attention is learned to identify some subspace. Only those tokens embedded inside these regions are pivotal for this attention. This would explain why only some specific tokens are always relevant.

## 4 Sequential Token Generation Under budget

In this section, we present Scissorhands, which reduces the KV cache memory from the sequence length dimension without fine-tuning the model. In Section 4.1, we describe how Scissorhands maintains the KV cache under a given budget. Section 4.2 provides a theoretical analysis of the algorithm and the approximation error.

### Budget KV Cache for Single Attention Head

In this section, for the sake of the discussion, we drop the layer number notation \(i\) and batch size dimension. \(_{t},_{t} R^{t d}\) denote for the KV cache until step \(t\). \(x_{t}^{1 d}\) is a row vector that denotes the input to attention at step \(t\). The output of an attention head at step \(t\) can be written as,

\[a_{t}=_{i=1}^{t}_{t,i}[i]_{t}_{t,i}= W_{Q},_{t}[i])}{_{i=1}^{t}(  x_{t}W_{Q},_{t}[i])}\]

**Intuition.** As shown in Section 3, the attention scores \(_{t,i}\) follow a strong power-law distribution. For the autoregressive generation process, if there exists an oracle such that we can identify the heavy score tokens before the future generation step, then the memory of the KV cache can be significantly reduced by only storing the heavy score tokens. Fortunately, the _persistence of importance hypothesis_ provides us with such an oracle. It states that only historical tokens with significant contributions toward previous generated tokens will have significant contributions toward future tokens.

**Challenges.** LLMs are deployed on hardware with a fixed memory. The algorithm should maintain the cache under fixed memory to meet the hard requirement. Further, LLMs are already computationally intensive. The algorithm should avoid introducing much extra burden on computation.

A fixed memory budget for one attention head is \(B\) tokens. In other words, we can store key and value embedding for \(B\) previous tokens. We describe the problem as follows,

**Definition 4.1** (Sequential generation at an attention head under budget \(B\)).: _Given a stream of token embedding, including prompt and previously generated tokens, denotes their input to the head as \(\{x_{1},,x_{t},\}\). The problem of sequential generation at an attention head under budget \(B\) is maintaining a key cache \(}_{t}\) and value cache \(}_{t}\) such that \(}_{t},}_{t} R^{n d}\) and \(n<B\)._

**Approach.** Inspired by the textbook solution of reservoir sampling and the Least Recent Usage cache replacement algorithm, Scissorhands reserves a fixed memory buffer for the KV cache. When the buffer is full, Scissorhands drops stored but non-influential tokens from the cache. We present the main algorithm in Algorithm 1 and Algorithm 2.

When the KV cache size exceeds the budget, Scissorhands drops tokens from the KV cache according to Algorithm 2. The importance record is a counter that indicates how many times a token is deemed non-important. We choose attention scores as the importance indicators, following our methodology in Section 3.2. The importance record is collected over a history window \(w\) to reduce variance. A higher counter suggests dropping from the cache. Recent tokens are always kept because of the lack of information on their importance by setting the counter for all tokens in the recent window \(r\) to 0.

With a sampled KV cache, attention output can be computed by the following estimator

\[_{t}=_{i=1}^{n}_{t,i}}_{t}[i]_{t,i}=W_{Q},}_{t}[i])}{_{i=1}^{n}(  x_{t}W_{Q},}_{t}[i])}\]

**Overhead Tradeoff** At the compression step, an extra attention computation is introduced to collect the importance measurements over a history window. However, such compression is not required at every generation step. \(m\) controls the frequency, and we use \(m=0.5B\) in our experiment. Further, steps after the compression have reduced attention computation because of the reduction in the KV cache. On the other hand, one can trade a tiny amount of memory to avoid the overhead by maintaining the importance record during generation steps in Algorithm 1.

**Allocating Budgets Across Attention Heads.** An LLM typically consists of \(L\) transformer layers where each layer has \(H\) heads. A total memory budget has to be distributed over layers and heads. Within each transformer layer, the budget is distributed evenly across heads. Within the entire model, we distributed the budget according to Figure 2. The rule of thumb is to allocate more budget to later layers to compensate for the lower persistence ratio.

### Theoretical Analysis.

We study how much the tokens generated by the compressed KV cache deviate from the tokens generated by the original transformer using our simplified model in (1). Let \(\{_{t}\}_{t=0}^{T}\) denote the tokens generated by the transformer with budget KV cache as in Algorithm 2 with \(m=1\):

\[_{t+1}=(_{t})_{t}= (}{{t}}_{t}W_{Q}}_{t}^{})}_{t}^{}W_{O}\]

Notice that when \(m=1\), i.e., in each iteration, we drop one token with the lowest score, the cache will always maintain \(B\) tokens. If the ranking of the attention scores does not change in each iteration, Algorithm 2 will always drop tokens with the smallest attention scores.

For reference purposes, let \(\{x_{t}\}_{t=0}^{T}\) denote the tokens generated by a vanilla transformer defined in (1). We will bound the difference \(\|x_{t}-_{t}\|_{2}\).

**Theorem 4.1**.: _Let \(_{1},_{2}\) denote the largest singular values of \(W_{1}\) and \(W_{2}\) in (2). Let_

\[_{t,j}=}{{t}}_{t}W_{Q}W_{K}^{ }_{j}^{})}{_{i=1}^{t-1}(}{{t} }_{t}W_{Q}W_{K}^{}_{i}^{})}\]

_and assume that each \(_{t,j}=cv_{t,j}\), where \(v_{t,j}\) are sampled from a power-law distribution with pdf \(f(x)=c(x+b)^{-k}\). Suppose that \(_{V}_{O}(1+_{1}_{2})(1+_{Q}_{K}) \). Let \(T_{}\) and \(T_{}\) denote the starting and maximum sequence lengths, respectively, and let \(B T_{}\) denote the budget as in Algorithm 2. If for all \(t[T_{},T_{}]\), \(S_{t}\) contains only tokens with at most the largest \(B\) values of \(_{t,j}\), that is, \(|S_{t}|=B\) and \(_{j S_{t}}_{t,j}_{j S_{t}}_{t,j}\), then for all \((0,1)\), with probability at least \(1-T_{}(-_{t}(T_{}-1)}{(k-2)^{2}(u+b)^{2}} )-T_{}(--1)(1-B/T_{})^{2}}{(1- )^{2}})\), the following error bound must hold for all \(t[T_{},T_{}]\)_

\[[\|x_{t}-_{t}\|_{2}]}{{T_{}}})}{(1-)^{2}}(k-(k-1)(}{{T_{}}}-})^{}{{(k-1) }}}) \]

The definition of \(_{t,j}\) means the attention scores computed on the tokens generated by the compressed approach. Our theorem assumes that dropping the tokens depends on the attention score of the current iteration. (4) provided a bound on the expected difference between the tokens generated in the budget and the original approach. The upper bound scales with \(1-}{{T_{}}}\). When \(B=T_{}\), meaning that we are keeping all of the tokens, the error becomes zero. The term \(k-(k-1)(}{{B-}}})\) depends on the distribution that the attention scores are fitted to and is always less than one. With a strong power-law distribution, this term provides a further decrease to the error bound in (4).

## 5 Empirical Evaluation

In this section, we present the results that demonstrate Scissorhands achieves up to 5\(\) reduction in the KV cache memory compared to the standard model with no accuracy loss. We also show that Scissorhands is compatible with 4-bit quantization.

**Experiment Setting.** We compare the accuracy of Scissorhands-OPT against the original OPT on one language model datasets C4  and a number of few-shot downstream tasks: Hellaswag , MathQA , PIQA , Winogrande . We use lm-eval-harness  to evaluate few-shot tasks. Our experiments are conducted on NVIDIA 4 A100 40GB GPU servers.

**No Accuracy Drop until 5\(\).** In Figure 3, we present Scissorhands's accuracy trend where 1\(\) denotes the original OPT. In the language modeling setting, perplexity is the lower the better. For OPT-6B, perplexity is maintained until 50% of the original KV cache size for OPT-13B. For OPT-66B, perplexity is maintained until 75% of the original KV cache. We observe a flatter accuracy trend as the model size grows, which is exceptionally encouraging. This suggests that Scissorhands can scale with the model size. Downstream tasks are usually less sensitive to perturbation and bear more variance in terms of accuracy. We evaluate the 5-shot setting and 1\(\) denotes the original OPT model. For Winogrande and MathQA, accuracy is maintained even after 5\(\) compression for OPT-66B. Similar to the language modeling setting, Scissorhands performs better at larger models. Generally, accuracy is maintained with 15% - 30% of the original KV cache size.

#### Ablation on the Importance of Pivotal Tokens

We divide C4 into three subsets depending on the sequence length. C4-[256-512] contains data sequences that are longer than 256 tokens but less than 512 tokens. C4-[512-1024] contains data sequences longer than 512 tokens but less than 1024 tokens. C4-[1024-2048] contains data sequences that are longer than 1024 tokens but less than 2048. Results are summarized in Table 3. Local Windows refers to only keeping tokens in the recent window, while Scissorhandskeeps both recent tokens and pivotal tokens. We observe the perplexity of the full model degrades slightly with the growing sequence length. At all sequence lengths, Scissorhands's performance is comparable against the full cache model, while Local Window incurs a significant quality loss. This demonstrates that keeping the pivotal tokens is important to reserve model performance. It is also interesting to note that at longer sequence lengths, the local window has higher accuracy. This also shows at longer sequence length, the attention mechanism in current architecture tends to focus on recent context.

#### Compatible with 4-bit Quantization

We test the compatibility of quantization and Scissorhands at \(2\) compression. We adopt 4-bit quantization following . Even Hellaswag is most sensitive based on Figure 3, adding quantization doesn't introduce compounded errors.

#### Ablation on Attention Score Error.

We present the change ratio in attention score between original OPT-13B and Scissorhands OPT-13B at \(3\) compression on C4 in Figure 4.

    & [256 - 512] & [512 - 1024] & [1024- 2048] \\  OPT-13B & 8.7968 & 9.1017 & 9.3005 \\  OPT-13B + Local Window & 81.8297 & 29.3823 & 15.5883 \\  OPT-13B + Scissorhands & 8.7972 & 9.1011 & 9.3009 \\   

Table 3: Perplexity on C4 with different sequence lengths.

Figure 3: Accuracy trend of Scissorhands on language modeling dataset and downstream tasks with different KV cache compression. In general, Scissorhands incurs no accuracy drop until \(5\) compression on OPT-66B.

    & [256 - 512] & [512- 1024] & [1024- 2048] \\  OPT-13B & 8.7968 & 9.1017 & 9.3005 \\  OPT-13B + Local Window & 81.8297 & 29.3823 & 15.5883 \\  OPT-13B + Scissorhands & 8.7972 & 9.1011 & 9.3009 \\   

Table 4: Applying 4-bit quantization on top of Scissorhands on Hellaswag.

We observe the attention score generated from Scissorhands is almost the same as the original KV cache, which also echoes Theorem 4.1. The change ratio is calculated as \(-_{a}}{_{o}}\) where \(_{s}\) is the Scissorhands attention score and \(_{o}\) is the original score. From Figure 4, we observe that the change ratio is centered around 0. -1 indicating that \(_{s}\) is significantly smaller compared to the original, suggesting that a small portion of the important tokens are dropped in the cache. To explain the above observation of Scissorhands, we denote the \(n\) number of tokens with the highest score as \(\{x_{t}^{top_{-}n}\}_{t=0}^{T}\). Then, for any other sets of tokens \(\{x_{t}^{}\}_{t=0}^{T}\) that has no greater than \(n\) tokens, we can easily prove that \(similarity(x_{t}^{topB},x_{t})(x_{t}^{},x_{t})\). Thus, Scissorhands gives the most similar output as the original model at all layers.

## 6 Discussion, Limitation, and Future Work

We discover repetitive attention patterns given trained language models. One interesting question that needs to be answered is whether such behavior is a model architecture bias or an unexpected training outcome. For such purpose, we perform the same experiment with a randomly initialized OPT, and compare it against the results presented in Section 3.1. As shown in Figure 5, the repetitive attention pattern does not exist in randomly initialized models. Apart from an efficiency deployment perspective, could such repetitive attention patterns contribute to some known problems in language generation, such as repetitions? It may be worth investigating the relationship between repetitive attention patterns and undesired generations.

Due to the limitation of the server in academics, the largest model we can fit is OPT-66B. We try to understand the behavior and verify the generality across the different models and datasets. However, we cannot access the training process and fail to know exactly how an LLM is trained to exhibit such behavior. Experiments with the large model create carbon dioxide emissions. However, our work improves the efficiency of LLM, and we foresee no negative impacts.

## 7 Conclusion

Inspired by our intriguing findings that pivotal tokens exert a lasting influence on future steps, we developed Scissorhands to leverage this observation to reduce the memory usage of KV cache. Our method achieves memory reductions of \(5\) in the KV cache without compromising the performance of LLMs. Furthermore, we demonstrate the compatibility of Scissorhands with quantization techniques, opening up the possibility of reducing memory usage in both the representation and sequence length dimensions.

## 8 Acknowledgement

We would like to thank the anonymous reviewers for their helpful discussions and feedback. This work is supported by NSF-CCS-2211815, ONR-DURIP and NSF-BIGDATA-1838177.

Figure 4: Score between OPT and Scissorhands.

Figure 5: We plot the attention map corresponding to Section 3.1 but with a randomly initialized OPT. We observe no repetitive attention for a randomly initialized model.