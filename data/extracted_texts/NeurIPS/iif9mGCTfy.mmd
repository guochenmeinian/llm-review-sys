# Frequency-domain MLPs are More Effective

Learners in Time Series Forecasting

 Kun Yi\({}^{1}\), Qi Zhang\({}^{2}\), Wei Fan\({}^{3}\), Shoujin Wang\({}^{4}\), Pengyang Wang\({}^{5}\), Hui He\({}^{1}\)

**Defu Lian\({}^{6}\), Ning An\({}^{7}\), Longbing Cao\({}^{8}\), Zhendong Niu\({}^{1}\)**

\({}^{1}\)Beijing Institute of Technology, \({}^{2}\)Tongji University, \({}^{3}\)University of Oxford

\({}^{4}\)University of Technology Sydney, \({}^{5}\)University of Macau, \({}^{6}\)USTC

\({}^{7}\)HeFei University of Technology, \({}^{8}\)Macquarie University

{yikun, hehui617, zniu}@bit.edu.cn, zhangqi_cs@tongji.edu.cn, weifan.oxford@gmail.com

pywang@um.edu.mo, liandefu@ustc.edu.cn, ning.g.an@acm.org, longbing.cao@mq.edu.au

Corresponding author

###### Abstract

Time series forecasting has played the key role in different industrial, including finance, traffic, energy, and healthcare domains. While existing literatures have designed many sophisticated architectures based on RNNs, GNNs, or Transformers, another kind of approaches based on multi-layer perceptrons (MLPs) are proposed with simple structure, low complexity, and superior performance. However, most MLP-based forecasting methods suffer from the _point-wise mappings_ and _information bottleneck_, which largely hinders the forecasting performance. To overcome this problem, we explore a novel direction of _applying MLPs in the frequency domain_ for time series forecasting. We investigate the learned patterns of frequency-domain MLPs and discover their two inherent characteristic benefiting forecasting, (i) _global view_: frequency spectrum makes MLPs own a complete view for signals and learn global dependencies more easily, and (ii) _energy compaction_: frequency-domain MLPs concentrate on smaller key part of frequency components with compact signal energy. Then, we propose FreTS, a simple yet effective architecture built upon _Frequency_-domain MLPs for _Time_ Series forecasting. FreTS mainly involves two stages, (i) Domain Conversion, that transforms time-domain signals into _complex numbers_ of frequency domain; (ii) Frequency Learning, that performs our redesigned MLPs for the learning of real and imaginary part of frequency components. The above stages operated on both inter-series and intra-series scales further contribute to channel-wise and time-wise dependency learning. Extensive experiments on 13 real-world benchmarks (including 7 benchmarks for short-term forecasting and 6 benchmarks for long-term forecasting) demonstrate our consistent superiority over state-of-the-art methods. Code is available at this repository: https://github.com/aikunyi/FreTS.

## 1 Introduction

Time series forecasting has been a critical role in a variety of real-world industries, such as climate condition estimation , traffic state prediction , economic analysis , etc. In the early stage, many traditional statistical forecasting methods have been proposed, such as exponential smoothing  and auto-regressive moving averages (ARMA) . Recently, the emerging development of deep learning has fostered many deep forecasting models, including Recurrent Neural Network-based methods (e.g., DeepAR , LSTNet ), Convolution Neural Network-based methods (e.g., TCN , SCINet ), Transformer-based methods (e.g., Informer , Autoformer ), and Graph Neural Network-based methods (e.g., MTGNN , StemGNN , AGCRN ), etc.

While these deep models have achieved promising forecasting performance in certain scenarios, their sophisticated network architectures would usually bring up expensive computation burden in training or inference stage. Besides, the robustness of these models could be easily influenced with a large amount of parameters, especially when the available training data is limited [13; 18]. Therefore, the methods based on multi-layer perceptrons (MLPs) have been recently introduced with simple structure, low complexity, and superior forecasting performance, such as N-BEATS , LightTS , DLinear , etc. However, these MLP-based methods rely on _point-wise mappings_ to capture temporal mappings, which cannot handle _global dependencies_ of time series. Moreover, they would suffer from the _information bottleneck_ with regard to the volatile and redundant _local momenta_ of time series, which largely hinders their performance for time series forecasting.

To overcome the above problems, we explore a novel direction of _applying MLPs in the frequency domain_ for time series forecasting. We investigate the learned patterns of frequency-domain MLPs in forecasting and have discovered their two key advantages: (i) _global view_: operating on spectral components acquired from series transformation, frequency-domain MLPs can capture a more complete _view_ of signals, making it easier to learn _global_ spatial/temporal dependencies. (ii) _energy compaction_: frequency-domain MLPs _concentrate_ on the smaller key part of frequency components with the compact signal energy, and thus can facilitate preserving clearer patterns while filtering out influence of noises. Experimentally, we have observed that frequency-domain MLPs capture much more obvious global periodic patterns than the time-domain MLPs from Figure 1(a), which highlights their ability to recognize global signals. Also, from Figure 1(b), we easily note a much more clear diagonal dependency in the learned weights of frequency-domain MLPs, compared with the more scattered dependency learned by time-domain MLPs. This illustrates the great potential of frequency-domain MLPs to identify most important features and key patterns while handling complicated and noisy information.

To fully utilize these advantages, we propose FreTS, a simple yet effective architecture of _Frequency-domain_ MLPs for _T_ime _S_eries forecasting. The core idea of FreTS is to learn the time series forecasting mappings in the _frequency domain_. Specifically, FreTS mainly involves two stages: (i) Domain Conversion: the original time-domain series signals are first transformed into frequency-domain spectrum on top of Discrete Fourier Transform (DFT) , where the spectrum is composed of several _complex numbers_ as frequency components, including the _real coefficients_ and the _imaginary coefficients_. (ii) Frequency Learning: given the real/imaginary coefficients, we redesign the frequency-domain MLPs originally for the complex numbers by separately considering the real mappings and imaginary mappings. The respective real/imaginary parts of output learned by two distinct MLPs are then stacked in order to recover from frequency components to the final forecasting. Also, FreTS performs above two stages on both inter-series and intra-series scales, which further contributes to the channel-wise and time-wise dependencies in the frequency domain for better forecasting performance. We conduct extensive experiments on 13 benchmarks under different settings, covering 7 benchmarks for short-term forecasting and 6 benchmarks for long-term forecasting, which demonstrate our consistent superiority compared with state-of-the-art methods.

## 2 Related Work

Forecasting in the Time DomainTraditionally, statistical methods have been proposed for forecasting in the time domain, including (ARMA) , VAR , and ARIMA . Recently, deep learning

Figure 1: Visualizations of the learned patterns of MLPs in the time domain and the frequency domain (see Appendix B.4). (a) _global view_: the patterns learned in the frequency domain exhibits more obvious global periodic patterns than the time domain; (b) _energy compaction_: learning in the frequency domain can identify clearer diagonal dependencies and key patterns than the time domain.

based methods have been widely used in time series forecasting due to their capability of extracting nonlinear and complex correlations [25; 26]. These methods have learned the dependencies in the time domain with RNNs (e.g., deepAR , LSTNet ) and CNNs (e.g., TCN , SCINet ). In addition, GNN-based models have been proposed with good forecasting performance because of their good abilities to model series-wise dependencies among variables in the time domain, such as TAMP-S2GCNets , AGCRN , MTGNN , and GraphWaveNet . Besides, Transformer-based forecasting methods have been introduced due to their attention mechanisms for long-range dependency modeling ability in the time domain, such as Reformer  and Informer .

Forecasting in the Frequency DomainSeveral recent time series forecasting methods have extracted knowledge of the frequency domain for forecasting . Specifically, SFM  decomposes the hidden state of LSTM into frequencies by Discrete Fourier Transform (DFT). StemGNN  performs graph convolutions based on Graph Fourier Transform (GFT) and computes series correlations based on Discrete Fourier Transform. Autoformer  replaces self-attention by proposing the auto-correlation mechanism implemented with Fast Fourier Transforms (FFT). FEDformer  proposes a DFT-based frequency enhanced attention, which obtains the attentive weights by the spectrums of queries and keys, and calculates the weighted sum in the frequency domain. CoST  uses DFT to map the intermediate features to frequency domain to enables interactions in representation. FiLM  utilizes Fourier analysis to preserve historical information and remove noisy signals. Unlike these efforts that leverage frequency techniques to improve upon the original architecture such as Transformer and GNN, in this paper, we propose a new frequency learning architecture that learns both channel-wise and time-wise dependencies in the frequency domain.

MLP-based Forecasting ModelsSeveral studies have explored the use of MLP-based networks in time series forecasting. N-BEATS  utilizes stacked MLP layers together with doubly residual learning to process the input data to iteratively forecast the future. DEPTS  applies Fourier transform to extract periods and MLPs for periodicity dependencies for univariate forecasting. LightTS  uses lightweight sampling-oriented MLP structures to reduce complexity and computation time while maintaining accuracy. N-HiTS  combines multi-rate input sampling and hierarchical interpolation with MLPs for univariate forecasting. LTSF-Linear  proposes a set of embarrassingly simple one-layer linear model to learn temporal relationships between input and output sequences. These studies demonstrate the effectiveness of MLP-based networks in time series forecasting tasks, and inspire the development of our frequency-domain MLPs in this paper.

## 3 FreTS

In this section, we elaborate on our proposed novel approach, FreTS, based on our redesigned MLPs in the frequency domain for time series forecasting. First, we present the detailed _frequency learning architecture_ of FreTS in Section 3.1, which mainly includes two-fold frequency learners with domain conversions. Then, we detailedly introduce our redesigned _frequency-domain MLPs_ adopted by above frequency learners in Section 3.2. Besides, we also theoretically analyze their superior nature of global view and energy compaction, as aforementioned in Section 1.

Problem DefinitionLet \([X_{1},X_{2},,X_{T}]^{N T}\) stand for the regularly sampled multi-variate time series dataset with \(N\) series and \(T\) timestamps, where \(X_{t}^{N}\) denotes the multi-variate values of \(N\) distinct series at timestamp \(t\). We consider a time series lookback window of length-\(L\) at timestamp \(t\) as the model input, namely \(_{t}=[X_{t-L+1},X_{t-L+2},,X_{t}]^{N L}\); also, we consider a horizon window of length-\(\) at timestamp \(t\) as the prediction target, denoted as \(_{t}=[X_{t+1},X_{t+2},,X_{t+}]^{N}\). Then the time series forecasting formulation is to use historical observations \(_{t}\) to predict future values \(}_{t}\) and the typical forecasting model \(f_{}\) parameterized by \(\) is to produce forecasting results by \(}_{t}=f_{}(_{t})\).

### Frequency Learning Architecture

The frequency learning architecture of FreTS is depicted in Figure 2, which mainly involves Domain Conversion/Inversion stages, Frequency-domain MLPs, and the corresponding two learners, i.e., the Frequency Channel Learner and the Frequency Temporal Learner. Besides, before taken to learners, we concretely apply a _dimension extension_ block on model input to enhance the model capability. Specifically, the input lookback window \(_{t}^{N L}\) is multiplied with a learnable weight vector \(_{d}^{1 d}\) to obtain a more expressive hidden representation \(_{t}^{N L d}\), yielding \(_{t}=_{t}_{d}\) to bring more semantic information, inspired by word embeddings .

Domain Conversion/InversionThe use of Fourier transform enables the decomposition of a time series signal into its constituent frequencies. This is particularly advantageous for time series analysis since it benefits to identify periodic or trend patterns in the data, which are often important in forecasting tasks. As aforementioned in Figure 1(a), learning in the frequency spectrum helps capture a greater number of periodic patterns. In view of this, we convert the input \(\) into the frequency domain \(\) by:

\[(f)=_{-}^{}(v)e^{-j2 fv}v= _{-}^{}(v)(2 fv)v+j_{-}^ {}(v)(2 fv)v\] (1)

where \(f\) is the frequency variable, \(v\) is the integral variable, and \(j\) is the imaginary unit, which is defined as the square root of -1; \(_{-}^{}(v)(2 fv)v\) is the real part of \(\) and is abbreviated as \(Re()\); \(_{-}^{}(v)(2 fv)v\) is the imaginary part and is abbreviated as \(Im()\). Then we can rewrite \(\) in Equation (1) as: \(=Re()+jIm()\). Note that in FreTS we operate domain conversion on both the channel dimension and time dimension, respectively. Once completing the learning in the frequency domain, we can convert \(\) back into the the time domain using the following inverse conversion formulation:

\[(v)=_{-}^{}(f)e^{j2 fv}f= _{-}^{}(Re((f))+jIm((f))e^{j2 fv} f\] (2)

where we take frequency \(f\) as the integral variable. In fact, the frequency spectrum is expressed as a combination of \(\) and \(\) waves in \(\) with different frequencies and amplitudes inferring different periodic properties in time series signals. Thus examining the frequency spectrum can better discern the prominent frequencies and periodic patterns in time series. In the following sections, we use \(\) to stand for Equation (1), and \(\) for Equation (2) for brevity.

Frequency Channel LearnerConsidering channel dependencies for time series forecasting is important because it allows the model to capture interactions and correlations between different variables, leading to a more accurate predictions. The frequency channel learner enables communications between different channels; it operates on each timestamp by sharing the same weights between \(L\) timestamps to learn channel dependencies. Concretely, the frequency channel learner takes \(_{t}^{N L d}\) as input. Given the \(l\)-th timestamp \(_{t}^{:(l)}^{N d}\), we perform the _frequency channel learner_ by:

\[_{chan}^{:(l)} =_{(chan)}(_{t}^{:(l)})\] (3) \[_{chan}^{:(l)} =(_{chan}^{:(l)},^{chan}, ^{chan})\] \[^{:(l)} =_{(chan)}(_{chan}^{:(l)})\]

Figure 2: The framework overview of FreTS: the Frequency Channel Learner focuses on modeling inter-series dependencies with frequency-domain MLPs operating on the channel dimensions; the Frequency Temporal Learner is to capture the temporal dependencies by performing frequency-domain MLPs on the time dimensions.

where \(^{:(l)}_{chan}^{ d}\) is the frequency components of \(^{:(l)}_{t}\); \(_{(chan)}\) and \(_{(chan)}\) indicates such operations are performed along the channel dimension. \(\) are frequency-domain MLPs proposed in Section 3.2, which takes \(^{chan}=(^{chan}_{r}+j^{chan}_{i})^{d d}\) as the complex number weight matrix with \(^{chan}_{r}^{d d}\) and \(^{chan}_{i}^{d d}\), and \(^{chan}=(^{chan}_{r}+j^{chan}_{i})^{d}\) as the biases with \(^{chan}_{r}^{d}\) and \(^{chan}_{i}^{d}\). And \(^{:,(l)}_{chan}^{ d}\) is the output of \(\), also in the frequency domain, which is conversed back to time domain as \(^{:,(l)}^{N d}\). Finally, we ensemble \(^{:,(l)}\) of \(L\) timestamps into a whole and output \(_{t}^{N L d}\).

Frequency Temporal LearnerThe frequency temporal learner aims to learn the temporal patterns in the frequency domain; also, it is constructed based on frequency-domain MLPs conducting on each channel and it shares the weights between \(N\) channels. Specifically, it takes the frequency channel learner output \(_{t}^{N L d}\) as input and for the \(n\)-th channel \(^{(n):,}_{t}^{L d}\), we apply the _frequency temporal learner_ by:

\[^{(n):,}_{temp} =_{(temp)}(^{(n):,}_{t})\] (4) \[^{(n):,}_{temp} =(^{(n):,}_{temp},^{temp}, ^{temp})\] \[^{(n):,} =_{(temp)}(^{(n):,}_{temp})\]

where \(^{(n):,}_{temp}^{ d}\) is the corresponding frequency spectrum of \(^{(n):,}_{t}\); \(_{(temp)}\) and \(_{(temp)}\) indicates the calculations are applied along the time dimension. \(^{temp}=(^{temp}_{r}+j^{temp}_{i}) ^{d d}\) is the complex number weight matrix with \(^{temp}_{r}^{d d}\) and \(^{temp}_{i}^{d d}\), and \(^{temp}=(^{temp}_{r}+j^{temp}_{i}) ^{d}\) are the complex number biases with \(^{temp}_{r}^{d}\) and \(^{temp}_{i}^{d}\). \(^{(n):,}_{temp}^{ d}\) is the output of \(\) and is converted back to the time domain as \(^{(n):,}^{L d}\). Finally, we incorporate all channels and output \(_{t}^{N L d}\).

ProjectionFinally, we use the learned channel and temporal dependencies to make predictions for the future \(\) timestamps \(}_{t}^{N}\) by a two-layer feed forward network (FFN) with one forward step which can avoid error accumulation, formulated as follows:

\[}_{t}=(_{t}_{1}+_{1})_{2}+ _{2}\] (5)

where \(_{t}^{N L d}\) is the output of the frequency temporal learner, \(\) is the activation function, \(_{1}^{(L d) d_{h}},_{2}^{d_{h} }\) are the weights, \(_{1}^{d_{h}}\), \(_{2}^{}\) are the biases, and \(d_{h}\) is the inner-layer dimension size.

### Frequency-domain MLPs

As shown in Figure 3, we elaborate our novel frequency-domain MLPs in FreTS that are redesigned for the complex numbers of frequency components, in order to effectively capture the time series key patterns with _global view_ and _energy compaction_, as aforementioned in Section 1.

**Definition 1** (**Frequency-domain MLPs)**.: _Formally, for a complex number input \(^{m d}\), given a complex number weight matrix \(^{d d}\) and a complex number bias \(^{d}\), then the frequency-domain MLPs can be formulated as:_

\[^{} =(^{-1}^{}+^{})\] (6) \[^{0} =\]

_where \(^{}^{m d}\) is the final output, \(\) denotes the \(\)-th layer, and \(\) is the activation function._

As both \(\) and \(\) are complex numbers, according to the rule of multiplication of complex numbers (details can be seen in Appendix C), we further extend the Equation (6) to:

\[^{}=(Re(^{-1})^{}_{r}-Im( ^{-1})^{}_{i}+^{}_{r})+j( Re(^{-1})^{}_{i}+Im(^{-1}) ^{}_{r}+^{}_{i})\] (7)

where \(^{}=^{}_{r}+j^{}_{i}\) and \(^{}=^{}_{r}+j^{}_{i}\). According to the equation, we implement the MLPs in the frequency domain (abbreviated as \(\)) by the separate computation of the real and imaginary parts of frequency components. Then, we stack them to form a complex number to acquire the final results. The specific implementation process is shown in Figure 3.

**Theorem 1**.: _Suppose that \(\) is the representation of raw time series and \(\) is the corresponding frequency components of the spectrum, then the energy of a time series in the time domain is equal to the energy of its representation in the frequency domain. Formally, we can express this with above notations by:_

\[_{-}^{}|(v)|^{2}v=_{-}^{}| (f)|^{2}f\] (8)

_where \((f)=_{-}^{}(v)e^{-j2 fv}v\), \(v\) is the time/channel dimension, \(f\) is the frequency dimension._

We include the proof in Appendix D.1. The theorem implies that if most of the energy of a time series is concentrated in a small number of frequency components, then the time series can be accurately represented using only those components. Accordingly, discarding the others would not significantly affect the signal's energy. As shown in Figure 1(b), in the frequency domain, the energy concentrates on the smaller part of frequency components, thus learning in the frequency spectrum can facilitate preserving clearer patterns.

**Theorem 2**.: _Given the time series input \(\) and its corresponding frequency domain conversion \(\), the operations of frequency-domain MLP on \(\) can be represented as global convolutions on \(\) in the time domain. This can be given by:_

\[+=(*W+B)\] (9)

_where \(*\) is a circular convolution, \(\) and \(\) are the complex number weight and bias, \(W\) and \(B\) are the weight and bias in the time domain, and \(\) is \(\)._

The proof is shown in Appendix D.2. Therefore, the operations of \(\), i.e., \(+\), are equal to the operations \((*W+B)\) in the time domain. This implies that the operations of frequency-domain MLPs can be viewed as global convolutions in the time domain.

## 4 Experiments

To evaluate the performance of FreTS, we conduct extensive experiments on thirteen real-world time series benchmarks, covering short-term forecasting and long-term forecasting settings to compare with corresponding state-of-the-art methods.

DatasetsOur empirical results are performed on various domains of datasets, including traffic, energy, web, traffic, electrocardiogram, and healthcare, etc. Specifically, for the task of _short-term forecasting_, we adopt Solar 2, Wiki , Traffic , Electricity 3, ECG , METR-LA , and COVID-19  datasets, following previous forecasting literature . For the task of _long-term forecasting_, we adopt Weather , Exchange , Traffic , Electricity , and ETT datasets , following previous long time series forecasting works [13; 14; 30; 39]. We preprocess all datasets following [16; 13; 14] and normalize them with the min-max normalization. We split the datasets into training, validation, and test sets by the ratio of 7:2:1 except for the COVID-19 datasets with 6:2:2. More dataset details are in Appendix B.1.

BaselinesWe compare our FreTS with the representative and state-of-the-art models for both short-term and long-term forecasting to evaluate their effectiveness. For short-term forecasting, we compre FreTS against VAR , SFM , LSTNet , TCN , GraphWaveNet , DeepGLO , StemGNN , MTGNN , and AGCRN  for comparison. We also include TAMP-S2GCNets , DCRNN  and STGCN , which require pre-defined graph structures, for comparison. For long-term forecasting, we include Informer , Autoformer , Reformer , FEDformer , LTSF-Linear , and the more recent PatchTST  for comparison. Additional details about the baselines can be found in Appendix B.2.

Figure 3: One layer of the frequency-domain MLPs.

[MISSING_PAGE_FAIL:7]

**Long-term Time Series Forecasting** Table 2 showcases the long-term forecasting results of FreTS compared to six representative baselines on six benchmarks with various prediction lengths. For the traffic dataset, we select 48 as the lookback window size \(L\) with the prediction lengths \(\{48,96,192,336\}\). For the other datasets, the input lookback window length is set to 96 and the prediction length is set to \(\{96,192,336,720\}\). The results demonstrate that FreTS outperforms all baselines on all datasets. Quantitatively, compared with the best results of Transformer-based models, FreTS has an average decrease of more than 20% in MAE and RMSE. Compared with more recent LSTF-Linear  and the SOTA PathchTST , FreTS can still outperform them in general. In addition, we provide further comparison of FreTS and other baselines and report performance under different lookback window sizes in Appendix F.2. Combining Tables 1 and 2, we can conclude that FreTS achieves competitive performance in both short-term and long-term forecasting task.

### Model Analysis

**Frequency Channel and Temporal Learners** We analyze the effects of frequency channel and temporal learners in Table 3 in both short-term and long-term experimental settings. We consider two variants: **FreCL**: we remove the frequency temporal learner from FreTS, and **FreTL**: we remove the frequency channel learner from FreTS. From the comparison, we observe that the frequency channel learner plays a more important role in short-term forecasting. In long-term forecasting, we note that the frequency temporal learner is more effective than the frequency channel learner. In Appendix E.1, we also conduct the experiments and report performance on other datasets. Interestingly, we find out the channel learner would lead to the worse performance in some long-term forecasting cases. A potential explanation is that the channel independent strategy  brings more benefit to forecasting.

**FreMLP vs. MLP** We further study the effectiveness of \(\) in time series forecasting. We use \(\) to replace the original MLP component in the existing SOTA MLP-based models (i.e., DLinear and NLinear ), and compare their performances with the original DLinear and NLinear under the same experimental settings. The experimental results are presented in Table 4. From the table, we easily observe that for any prediction length, the performance of both DLinear and NLinear models has been improved after replacing the corresponding MLP component with our \(\). Quantitatively, incorporating \(\) into the DLinear model brings an average improvement of 6.4% in MAE and 11.4% in RMSE on the Exchange dataset, and 4.9% in MAE and 3.5% in RMSE on the Weather dataset. A similar improvement has also been achieved on the two datasets with regard to NLinear, according to Table 4. These results confirm the effectiveness of \(\) compared to MLP again and we include more implementation details and analysis in Appendix B.5.

### Efficiency Analysis

The complexity of our proposed FreTS is \((N N+L L)\). We perform efficiency comparisons with some state-of-the-art GNN-based methods and Transformer-based models under different numbers of variables \(N\) and prediction lengths \(\), respectively. On the Wiki dataset, we conduct experiments over \(N\{1000,2000,3000,4000,5000\}\) under the same lookback window size of 12

   Datasets &  &  \\   &  & 192 & 336 & 720 & 96 & 192 & 336 &  \\  Metrics & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE \\  DLinear & 0.037 & 0.051 & 0.054 & 0.072 & 0.017 & 0.0495 & 0.095 & 0.119 & 0.041 & 0.081 & 0.047 & 0.089 & 0.056 & 0.098 & 0.005 & 0.106 \\ DLinear (FReLU) & **0.036** & **0.090** & **0.053** & **0.071** & **0.063** & **0.071** & **0.086** & **0.101** & **0.038** & **0.078** & **0.045** & **0.086** & **0.055** & **0.097** & **0.061** & **0.100** \\  NLinear & 0.037 & 0.051 & 0.051 & 0.069 & 0.069 & 0.093 & 0.115 & 0.146 & 0.037 & 0.081 & 0.045 & 0.089 & 0.052 & 0.098 & 0.058 & 0.106 \\ NLinear (FReLU) & **0.036** & **0.050** & **0.049** & **0.067** & **0.047** & **0.091** & **0.199** & **0.139** & **0.035** & **0.076** & **0.043** & **0.064** & **0.050** & **0.094** & **0.057** & **0.103** \\   

Table 4: Ablation study on the Exchange and Weather datasets with a lookback window size of 96 and the prediction length \(\{96,192,336,720\}\). DLinear (FreMLP)/NLinear (FreMLP) means that we replace the MLPs in DLinear/NLinear with \(\). The best results are in **bold**.

  Tasks &  &  \\  Dataset & Electricity &  &  &  \\ I/O & 21/2 & & 12/12 & & 96/336 & 96/336 \\  Metrics & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE & MAE & RMSE \\  FreCL & 0.054 & 0.080 & 0.086 & 0.168 & 0.067 & 0.086 & 0.051 & 0.094 \\ FreTL & 0.058 & 0.086 & 0.085 & 0.167 & 0.065 & 0.085 & 0.047 & 0.091 \\  FreTS & **0.050** & **0.076** & **0.080** & **0.166** & **0.062** & **0.082** & **0.046** & **0.090** \\  

Table 3: Ablation studies of frequency channel and temporal learners in both short-term and long-term forecasting. ‘I/O’ indicates lookback window sizes/prediction lengths.

and prediction length of 12, as shown in Figure 4(a). From the figure, we can find that: (1) The amount of FreTS parameters is agnostic to \(N\). (2) Compared with AGCRN, FreTS incurs an average 30% reduction of the number of parameters and 20% reduction of training time. On the Exchange dataset, we conduct experiments on different prediction lengths \(\{96,192,336,480\}\) with the same input length of 96. The results are shown in Figure 4(b). It demonstrates: (1) Compared with Transformer-based methods (FEDformer , Autoformer , and Informer ), FreTS reduces the number of parameters by at least 3 times. (2) The training time of FreTS is averagely 3 times faster than Informer, 5 times faster than Autoformer, and more than 10 times faster than FEDformer. These show our great potential in real-world deployment.

### Visualization Analysis

In Figure 5, we visualize the learned weights \(\) in \(\) on the Traffic dataset with a lookback window size of 48 and prediction length of 192. As the weights \(\) are complex numbers, we provide visualizations of the real part \(_{r}\) (presented in (a)) and the imaginary part \(_{i}\) (presented in (b)) separately. From the figure, we can observe that both the real and imaginary parts play a crucial role in learning process: the weight coefficients of the real or imaginary part exhibit energy aggregation characteristics (clear diagonal patterns) which can facilitate to learn the significant features. In Appendix E.2, we further conduct a detailed analysis on the effects of the real and imaginary parts in different contexts of forecasting, and the effects of the two parts in the FreMLP. We examine their individual contributions and investigate how they influence the final performance. Additional visualizations of the weights on different datasets with various settings, as well as visualizations of global periodic patterns, can be found in Appendix G.1 and Appendix G.2, respectively.

## 5 Conclusion Remarks

In this paper, we explore a novel direction and make a new attempt to apply frequency-domain MLPs for time series forecasting. We have redesigned MLPs in the frequency domain that can effectively capture the underlying patterns of time series with global view and energy compaction. We then verify this design by a simple yet effective architecture, FreTS, built upon the frequency-domain MLPs for time series forecasting. Our comprehensive empirical experiments on seven benchmarks of short-term forecasting and six benchmarks of long-term forecasting have validated the superiority of our proposed methods. Simple MLPs have several advantages and lay the foundation of modern deep learning, which have great potential for satisfied performance with high efficiency. We hope this work can facilitate more future research of MLPs on time series modeling.

Figure 4: Efficiency analysis (model parameters and training time) on the Wiki and Exchange dataset. (a) The efficiency comparison under different number of variables: the number of variables is enlarged from 1000 to 5000 with the input window size as 12 and the prediction length as 12 on Wiki dataset. (b) The efficiency comparison under the prediction lengths: we conduct experiments with prediction lengths prolonged from 96 to 480 under the same window size of 96 on the Exchange dataset.

Figure 5: Visualizing learned weights of \(\) on the Traffic dataset. \(_{r}\) represents the real part of \(\), and \(_{i}\) represents the imaginary part.