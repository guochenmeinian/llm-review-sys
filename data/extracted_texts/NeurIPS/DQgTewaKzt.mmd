# ZoomTrack: Target-aware Non-uniform Resizing for Efficient Visual Tracking

Yutong Kou\({}^{1,2}\), Jin Gao\({}^{1,2}\), Bing Li\({}^{1,5}\), Gang Wang\({}^{4*}\),

Weiming Hu\({}^{1,2,3}\), Yizheng Wang\({}^{4}\), Liang Li\({}^{4*}\)

\({}^{1}\)State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), CASIA

\({}^{2}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{3}\)School of Information Science and Technology, ShanghaiTech University

\({}^{4}\)Beijing Institute of Basic Medical Sciences \({}^{5}\)People AI, Inc

kouyutong2021@ia.ac.cn, {jin.gao,bli,wmhu}@nlpr.ia.ac.cn,

liang.li.brain@aliyun.com, g_wang@foxmail.com, yzwang57@sina.com

Corresponding author

###### Abstract

Recently, the transformer has enabled the speed-oriented trackers to approach state-of-the-art (SOTA) performance with high-speed thanks to the smaller input size or the lighter feature extraction backbone, though they still substantially lag behind their corresponding performance-oriented versions. In this paper, we demonstrate that it is possible to narrow or even close this gap while achieving high tracking speed based on the smaller input size. To this end, we non-uniformly resize the cropped image to have a smaller input size while the resolution of the area where the target is more likely to appear is higher and vice versa. This enables us to solve the dilemma of attending to a larger visual field while retaining more raw information for the target despite a smaller input size. Our formulation for the non-uniform resizing can be efficiently solved through quadratic programming (QP) and naturally integrated into most of the crop-based local trackers. Comprehensive experiments on five challenging datasets based on two kinds of transformer trackers, _i.e._, OSTrack and TransT, demonstrate consistent improvements over them. In particular, applying our method to the speed-oriented version of OSTrack even outperforms its performance-oriented counterpart by 0.6% AUC on TNL2K, while running 50% faster and saving over 55% MACs. Codes and models are available at https://github.com/Kou-99/ZoomTrack.

## 1 Introduction

In visual tracking, many efforts have been made to improve the discrimination and localization ability of the tracking models, including deeper networks , transformer feature fusion , joint feature extraction and interaction , and so on. However, most of the recent tracking algorithms, including the transformer-based , still follow the paradigm proposed by Bertinetto et al. , in which a small exemplar image cropped from

Figure 1: Our method consistently improves the OSTrack and TransT baselines with negligible computation.

the first frame is used to locate the target within a large search image cropped from one of the subsequent frames. The crop size, which determines the visual field size, is derived from a reference bounding box plus a margin for context. Both of the crops are _uniformly_ resized to fixed sizes to facilitate the training and testing of the tracker.

Due to the fact that the complexity of the transformers scales quadratically with input size, many transformer trackers  propose both speed-oriented versions with the smaller input size or feature extraction backbone and performance-oriented versions with larger ones. Thanks to the enabled larger visual field size or stronger feature representation, the performance-oriented versions always outperform their speed-oriented counterparts in performance, though the tracking speed is severely reduced. For instance, an increased \(1.5\) input size for OSTrack  will cause doubled Multiply-Accumulate Operations (MACs) and nearly halved tracking speed. Thus, it is natural to pose the question: _Is it possible to narrow or even close this performance gap while achieving high tracking speed based on a smaller input size?_

Inspired by the human vision system (HVS), we propose to non-uniformly resize the attended visual field to have smaller input size for visual tracking in this paper. Human retina receives about 100 MB of visual input per second, while only 1 MB of visual data is able to be sent to the central brain . This is achieved by best utilizing the limited resource of the finite number of cones in HVS. More specifically, the density of cone photoreceptors is exponentially dropped with eccentricity (_i.e._, the deviation from the center of the retina), causing a more accurate central vision for precise recognition and a less accurate peripheral vision for rough localization . This enables us to solve the dilemma of attending to larger visual field while retaining more raw information for the target area despite the smaller input size.

In our formulation for the non-uniform resizing, the area where the target is more likely to appear is magnified to retain more raw information with high resolution and facilitate precise recognition, whereas the area where the target is less likely to appear is shrunk yet preserved to facilitate rough localization when encountering fast motion or tracking drift. The key to our method is to design an efficient and controllable resizing module based on a small controllable grid. On top of this grid, we define a zoom energy to _explicitly_ control the scale of magnification for the important target area, a rigid energy to avoid extreme deformation and a linear constraint to avoid cropping the source image during resizing. The important area is determined by the previous tracking result as a temporal prior. This formulation can be solved efficiently through quadratic programming (QP), whose solution is used to manipulate the controllable grid. Finally, the ideal non-uniform resizing is achieved by sampling according to the controllable grid (See Fig. 2).

Our method can be easily integrated into plenty of tracking algorithms. We select the popular hybrid CNN-Transformer tracker TransT  and one-stream Transformer tracker OSTrack  to verify the efficiency and effectiveness of our method. Comprehensive experiments are conducted on five large-scale benchmarks, including GOT-10k , LaSOT , LaSOT\({}_{ext}\), TNL2k , TrackingNet . We observe consistent improvements over the baselines with negligible computational overhead. In particular, we improve the speed-oriented version of OSTrack to achieve 73.5% AO, 50.5% AUC and 56.5% AUC on GOT-10k, LASOT\({}_{ext}\) and TNL2k respectively, which are on par with the performance-oriented counterpart while saving over 55% MACs and running 50% faster (see Fig. 1). In other words, the performance gap between speed-oriented and performance-oriented trackers can be narrowed or even closed through our method.

In summary, our contributions are as follows: (**i**) We propose ZoomTrack, an efficient non-uniform resizing method for visual tracking that bridge the gap between speed-oriented and performance-oriented trackers with negligible computation; (**ii**) We formulate the non-uniform resizing as an explicitly controlled magnification of important areas with restriction on extreme deformation,

Figure 2: We achieve non-uniform resizing by sampling according to a manipulable control grid \(\), which is generated by solving for best \(d_{k}^{col}\) and \(d_{l}^{row}\) that minimize two energies that lead to controllable magnification and less deformation.

enabling an HVS-inspired data processing with limited resource; (**iii**) Extensive experiments based on two baseline trackers on multiple benchmarks show that our method achieves consistent performance gains across different network architectures.

## 2 Related Work

**Efficient Visual Tracking.** A lot of real-world applications require tracking algorithms to have high speed. Consequently, many efforts have been made to improve the efficiency of visual tracking. Yan et al.  use NAS to search for a lightweight network architecture for visual tracking. Borsuk et al.  design a novel way to incorporate temporal information with only a single learnable parameter which achieves higher accuracy at a higher speed. Blatter et al.  design an efficient transformer layer and achieves real-time tracking on CPU. Shen et al.  propose a distilled tracking framework to learn small and fast trackers from heavy and slow trackers. Some other works [17; 10] explore the quickly advancing field of vision transformer pre-training [15; 29] and architecture designing  to improve both tracking accuracy and efficiency. Previous methods focus on designing either a better network [32; 5; 6; 17; 10] or a new training framework  to improve the efficiency of trackers, whereas our work focuses on the non-uniform resizing of the input for the sake of efficiency. Our approach is more general and orthogonal to them.

**Non-uniform Resizing for Vision Tasks.** Image resizing is a common image processing operation. Yet, the standard uniform resizing is not always satisfactory in many applications. Avidan and Shamir  resize an image by repeatedly carving or inserting seams that are determined by the saliency of image regions. Panozzo et al.  use axis-aligned deformation generated from an automatic or human-appointed saliency map to achieve content-aware image resizing. Recasens et al.  adaptively resize the image based on a saliency map generated by CNN for image recognition. Zheng et al.  learn attention maps to guide the sampling of the image to highlight details for image recognition. Thavamani et al.  use the sampling function of  to re-sample the input image based on temporal and dataset-level prior for autonomous navigation. Bejnordi et al.  learn a localization network to magnify salient regions from the unmediated supervision of  for video object detection. Thavamani et al.  propose an efficient and differentiable warp inversion that allows for mapping labels to the warped image to enable the end-to-end training of dense prediction tasks like semantic segmentation. Existing methods either use time-consuming feature extractors to generate saliency [1; 24; 25; 37] or use heuristic sampling functions that cannot explicitly control the extent of the magnification, which may cause unsatisfactory magnification or extreme deformation [27; 2; 28]. In contrast, our approach directly generates the saliency map or important area using the temporal prior in tracking and proposes a novel sampling method that can achieve controllable magnification without causing extreme deformation.

## 3 Background and Analysis

### Revisiting Resizing in Deep Tracking

Before introducing our method, we first briefly revisit the resizing operation in the deep tracking pipeline. Given the first frame and its target location, visual trackers aim to locate the target in the subsequent frames using a tracking model \(_{}(,)\), where \(\) is the template patch from the first frame with the target in the center and \(\) is the search patch from one of the subsequent frames. Both \(\) and \(\) are generated following a crop-then-resize manner. We next detail the cropping and resizing operations.

**Image crop generation.** Both the template and search images are cropped based on a reference bounding box \(b\), which can be the ground truth annotation for the template image or the previous tracking result for the search image. Denote a reference box as \(b=(b^{cx},~{}b^{cy},~{}b^{w},~{}b^{h})\), then the crop size can be calculated as

\[W=H=+(f-1) c^{w})(b^{h}+(f-1) c^{h})}\,\] (1)

where \(f\) is the context factor controlling the amount of context to be included in the visual field, and \(c^{w}\) and \(c^{h}\) denote unit context amount which is solely related to \(b^{w}\) and \(b^{h}\). Typical choice of unit context amount is \(c^{w}=b^{w},~{}c^{h}=b^{h}\)[33; 31; 9], or \(c^{w}=c^{h}=(b^{w}+b^{h})/2\)[8; 19]. A higher context factor means a larger visual field and vice versa. Usually, the search image has a largercontext factor for a larger visual field, and the template image has a small context factor providing minimal necessary context. The image crop \(\) is obtained by cropping the area centered at \((b^{cx},\ b^{cy})\) with width \(W\) and height \(H\). Areas outside the image are padded with a constant value. Finally, box \(b\) is transformed to a new reference bounding box \(r=(r^{cx},\ r^{cy},\ r^{w},\ r^{h})\) on the image crop \(\).

**Image crop resizing.** To facilitate the batched training and later online tracking, the image crop has to be resized to a fixed size. Given an image crop \(\) with width \(W\) and height \(H\), a fix-sized image patch \(^{}\) with width \(w\) and height \(h\) is obtained by resizing \(\). Specifically, \(h w\) pixels in \(^{}(x^{},y^{})\) are sampled from \((x,y)\) according to a mapping \(:^{2}^{2}\), _i.e._,

\[^{}(x^{},y^{})=((x^{ },y^{}))\;.\] (2)

Note that \(\) does not necessarily map integer index to integer index. Thus, some pixels in \(^{}\) may be sampled from \(\) according to the non-integer indices. This can be realized by bilinear interpolating from nearby pixels in \(\). Current methods  use _uniform_ mapping \(_{uniform}\) to resize \(\). \(_{uniform}\) is defined as \(_{uniform}(x^{},y^{})=(}{s^{x}}, }{s^{y}})\), where \(s^{x}=w/W,\ s^{y}=h/H\) are called resizing factors and indicate the scaling of the image. When applying uniform resizing, the resizing factor is the same wherever on the image crop, which means the same amount of amplification or shrinkage is applied to the different areas of the entire image.

### Solving the Dilemma Between Visual Field and Target Area Resolution

Although the crop-then-resize paradigm is applied in most of the popular deep trackers, the fixed input search size and context factor vary significantly across different algorithms. Generally, the performance can be improved by using a larger input size at the cost of decreasing tracking speed . To understand the key factors behind this, we conduct a series of experiments on LaSOT  based on OSTrack . First, we simultaneously increase search size and context factor, which means the target resolution is roughly fixed and the tracker attends to a larger visual field. As shown in the left of Fig. 3, the AUC on LaSOT increases along with the input search size thanks to the larger attended visual field. Note that increasing the search size (\(256 384\)) leads to almost doubled computational overhead (\(21.5\)G MACs \( 41.5\)G MACs). Next, we limit the available resources by fixing the input size while increasing the attended visual field size. As shown in the right of Fig. 3, the AUC on LaSOT first increases and then decreases as the average target resolution2 keeps decreasing. This result demonstrates that the benefit of an enlarged visual field is gradually wiped out by the decrease in target resolution when the computational cost is fixed.

Inspired by the data processing with limited resources in HVS, we believe the above dilemma of attending to a larger visual field while retaining more raw information for the target can be solved by replacing uniform resizing with non-uniform resizing. In visual tracking, the previous tracking result can serve as a strong temporal prior for the current target location. Based on this prior, we can determine the area where the target is likely to appear and magnify it to retain more raw information with high resolution. On the contrary, areas where the target is less likely to appear is shrunk to avoid decreasing the visual field when the input search size is fixed with limited resources. Notably, the

Figure 3: Experiments on LaSOT  based on OSTrack  show that: (_left_) Thanks to the larger attended visual field, simultaneously increasing search size and context factor leads to consistent performance improvement at the cost of heavy computational burden; (_right_) Increasing the attended visual field by enlarging context factor while fixing the input size to limit the available resources leads to degraded performance due to the decreased target area resolution.

magnification and shrinkage should not dramatically change the shape and aspect ratio of regions on the image to facilitate the robust learning of the appearance model. Based on the above analysis, we propose some guidelines for our non-uniform resizing, _i.e._, **G1**: Magnify the area where the target is most likely to appear; **G2**: Avoid extreme deformation; **G3**: Avoid cropping the original image \(\) so that the original visual field is also retained. We detail how to incorporate these guidelines into our non-uniform resizing in the following section.

## 4 Methods

As discussed in Sec. 3.1, the resizing process from the source image \((x,y)^{H W 3}\) to the target image \(^{}(x^{},y^{})^{h w 3}\) is a sampling operation controlled by a mapping \(:(x^{},y^{})(x,y)\). Our aim is to find the mapping \(\) that best follows the guidelines (**G1-G3**), given a temporal prior box \(r=(r^{cx},r^{cy},r^{w},r^{h})\) that indicates the most possible location of the target. We first restrict the domain of the mapping to a few points and acquire a grid representation of the mapping \(\). Then, we formulate the guidelines as a QP problem based on the grid point intervals. By solving the QP problem via a standard solver , we can efficiently manipulate the controllable grid point intervals and achieve the ideal non-uniform resizing by sampling according to the final grid representation.

### Grid Representation for Non-uniform Resizing

Recall the sampling operation in Eq. (2), pixel \((x^{},y^{})\) on the target image \(^{}^{h w 3}\) is sampled from location \((x,y)\) on the source image \(^{H W 3}\). Thus, at least \(h w\) pairs of \((x^{},y^{})(x,y)\) are needed for resizing. We use the exact \(h w\) pairs to represent the mapping \(\), which can be defined as a grid \(^{h w 2}\), where \([y^{}][x^{}]=(x,y),\ x^{}=1,...,w,\ y^{}=1,...,h\), is the location on \(\) that the target image pixel \(^{}(x^{},y^{})\) should be sampled from.

**Controllable grid.** The grid \(\) is an extremely dense grid with \(h w\) grid points (equivalent to the resolution of the target image \(^{}\)), which makes it computationally inefficient to be directly generated. To solve this issue, we define a small-sized controllable grid \(^{(m+1)(n+1) 2}\) with \(m+1\) rows and \(n+1\) columns (\(m<<h,n<<w\)) to estimate \(\), where

\[[j][i]=j i,\ i=0,...,n,\ j=0,...,m\] (3)

Intuitively, \([j][i]\) is the sample location on the source image for the target image pixel \((i,j)\) in \(^{}\). Once \(\) is determined, \(\) can be easily obtained through bilinear interpolation on \(\).

**Axis-alignment constraint.** To avoid extreme deformation (**G3**) and reduce computational overhead, we add an axis-aligned constraint  to the grid \(\). Specifically, grid points in the same row \(j\) have the same y-axis coordinate \(y_{j}\), and grid points in the same column \(i\) have the same x-axis coordinate \(x_{i}\). That is to say \([j][i]=(x_{i},y_{j}),\ i=0,...,n,\ j=0,...,m\). The reason why we use such an axis-aligned constraint is that we need to keep the axis-alignment of the bounding box (_i.e._, the four sides of the bounding box are parallel to the image boundary) after resizing, which is proven to be beneficial to the learning of object localization .

**Grid representation.** The source image is split into small rectangular patches \(p(k,l),k=1,...n,l=1,...,m\) by grid points. The top-left corner of \(p(k,l)\) is \([l-1][k-1]\) and its bottom-right corner is \([l][k]\). The width and height of the patch \(p(k,l)\) are denoted as the horizontal grid point interval \(d_{k}^{col}\) and the vertical grid point interval \(d_{l}^{row}\). \(d_{k}^{col}\) and \(d_{l}^{row}\) can be calculated as \(d_{k}^{col}=x_{k}-x_{k-1},\ d_{l}^{row}=y_{l}-y_{l-1}\). The axis-aligned controllable grid \(\) can be generated from grid point intervals \(d_{l}^{row}\), \(d_{k}^{col}\) based on

\[[j][i]=(_{k=0}^{i}d_{k}^{col},_{l=0}^{j}d_{l}^{row} )\,\] (4)

where \(d_{0}^{row}=d_{0}^{col}=0\) for the sake of simplicity.

**Non-uniform resizing.** According to Eq. (3), the resizing process can be visualized using two grids (see Fig. 2): a manipulatable grid on the source image \(\) and a fixed grid on the target image \(_{t}[j][i]=(i,j)\). \(\) and \(_{t}\) split the source and target images into rectangular patches \(p(k,l)\) and \(p_{t}(k,l)\) respectively. Target image pixel at \(_{t}[j][i]\) is sampled from \([j][i]\) on the source image. Thisprocess can be intuitively understood as resizing \(p(k,l)\) to \(p_{t}(k,l)\). When the grid point intervals of \(\) are manipulated, the grid point intervals of \(_{t}\) remain unchanged. Thus, by reducing/ increasing the grid intervals of \(\), the corresponding region on the target image is amplified/ shrunk. In this way, a non-uniform resizing operation that scales different image areas with different ratios can be obtained by dynamically manipulating the grid point intervals.

### QP Formulation Based on Grid Representation

Given a reference bounding box \(r=(r^{cx},r^{cy},r^{w},r^{h})\), we aim to dynamically manipulate the grid point intervals to best follow the guidelines (**G1-G3**) proposed in Sec. 3.2.

**Importance score for the target area.** According to **G1**, areas, where the target is most likely to appear, should be magnified. Following this guideline, we first define the importance score for the target area on the source image. An area has high importance if the target is more likely to appear in this area. As \(r\) is a strong temporal prior for the current target location, we use a Gaussian function \(G(x,y)\) centered at \((r^{cx},r^{cy})\) as the importance score function, _i.e._,

\[G(x,y)=(-()^{2}}{_{x}^{2}}+ )^{2}}{_{y}^{2}}))\;,\] (5)

where \(_{x}=r^{cx}\), \(_{y}=r^{cy}\), \(_{x}=}\), \(_{y}=}\). \(\) is a hyper-parameter controlling the bandwidth of the Gaussian function. Since we denote the rectangular area enclosed by the grid point intervals \(d_{l}^{row}\) and \(d_{l}^{col}\) as patch \(p(k,l)\), its importance score \(S_{k,l}\) can be determined by the value of \(G(x,y)\) at the center of patch \(p(k,l)\), _i.e._,

\[S_{k,l}=G((k+),(l+ ))+\;,\] (6)

where \(\) is a small constant to prevent extreme deformation and ensure stable computation. Here we use the patch center of a uniformly initialized grid (\(d_{k}^{col}=\) and \(d_{l}^{row}=\)). By doing so, \(S_{k,l}\) is irrelevant of \(d_{k}^{col}\) and \(d_{l}^{row}\), which is crucial for the QP formulation in the following.

**Zoom energy.** Following **G1**, we design a quadratic energy \(E_{zoom}\) to magnify the area where the target is most likely to appear. To achieve controllable magnification, we define a zoom factor \(\) controlling the amount of magnification. If we want to magnify some area by \(\), the distances between sampling grid points should shrink by \(\). \(E_{zoom}\) forces \(d_{k}^{col}\) and \(d_{l}^{row}\) to be close to the grid point intervals under uniform magnification by \(\), _i.e._, \(\) for \(d_{l}^{row}\) and \(\) for \(d_{k}^{col}\). In detail, The zoom energy is defined as

\[E_{zoom}=_{l=1}^{m}_{k=1}^{n}S_{k,l}^{2}((d_{l}^{row}-)^{2}+(d_{k}^{col}-)^{2})\;.\] (7)

**Rigid energy.** To avoid extreme deformation (**G2**), we define a quadratic energy \(E_{rigid}\) to restrict the aspect ratio change of patch \(p(k,l)\), which has width \(d_{k}^{col}\) and height \(d_{l}^{row}\). When being uniformly resized, \(p(k,l)\) should have width \(\) and height \(\). Thus, the patch \(p(k,l)\) is horizontally stretched by \(d_{k}^{col}\) and vertically stretched by \(d_{l}^{row}\). To keep the aspect ratio mostly unchanged for the important area, the horizontal stretch should be similar to the vertical stretch. Therefore, the rigid energy can be defined as

\[E_{rigid}=_{l=1}^{m}_{k=1}^{n}S_{k,l}^{2}(d_{l}^{row} -d_{k}^{col})^{2}\;.\] (8)

**Linear constraint to avoid cropping the source image.** According to **G3**, the resizing should not crop the source image so that the original visual field is also retained. That is to say, the grid \(\) should completely cover the entire source image. This requirement can be interpreted as a simple linear constraint that forces the sum of vertical grid point intervals \(d_{l}^{row}\) to be \(H\) and the sum of horizontal grid point intervals \(d_{k}^{col}\) to be \(W\): \(_{l=1}^{m}d_{l}^{row}=H,\;_{k=1}^{n}d_{k}^{col}=W\).

**QP formulation.** Combining the zoom energy, the rigid energy and the linear constraint, we can formulate the grid manipulation as the following minimization task

\[^{},\ d_{k}^{}}{ }& E=E_{zoom}+ E_{rigid}\\ &_{l=1}^{m}d_{l}^{}=H,\ _{k=1}^{n}d_{k}^{col}=W\] (9)

where \(\) is a hyper-parameter to balance the two energies. This convex objective function can be efficiently solved by a standard QP solver  (see Appendix A for more detailed derivations).

### Integration with Common Trackers

**Reference box generation.** Our non-uniform resizing method needs a temporal prior reference box \(r=(r^{cx},\ r^{cy},\ r^{w},\ r^{h})\) on the cropped source image \(\). During testing, \(r\) is directly generated from the previous frame tracking result (see Sec. 3.1). During training, we only need to generate \(r^{w}\) and \(r^{h}\) from the ground truth width \(g^{w}\) and height \(g^{h}\) as the center of the temporal prior reference box is fixed at \(r^{cx}=W/2\) and \(r^{cy}=H/2\). In detail, with probability \(q\) we use a smaller jitter \(j=j_{s}\), and with probability \(1-q\) we use a larger jitter \(j=j_{l}\). \(r^{w}\) and \(r^{h}\) are calculated as \(r^{w}=e^{J^{w}}g^{w},\ r^{h}=e^{J^{h}}g^{h}\), where \(J^{w},\ J^{h}(0,j^{2})\).

**Label mapping.** The classification loss of the network \(_{}(,)\) is calculated on the target image \(^{}\). Therefore, the classification labels on \(^{}\) should be generated from the ground truth bounding box on \(^{}\). The common practice is to use normalized ground truth bounding box for the reason that the normalized coordinates remain the same on \(\) and \(^{}\) under uniform resizing. However, this does not hold true for our non-uniform resizing. Therefore, we need to map the ground truth bounding box from \(\) to \(^{}\) via the grid \(\). Specifically, we map the top-left corner and bottom-right corner of the bounding box as follows. Given a point on the source image \((x,y)\), we first find two grid points on \(\) that are closest to \((x,y)\). Then, the corresponding point \((x^{},y^{})\) can be obtained by bilinear interpolation between the indices of these two points.

**Reverse box mapping.** The bounding box predicted by the network \(_{}(,)\) is on \(^{}\) and needed to be mapped back to \(\) as the final tracking result. We map the top-left corner and bottom-right corner of the predicted bounding box from \(^{}\) to \(\) in a reversed process of label mapping. Specifically, given a point \((x^{},y^{})\) on \(^{}\), we first find the two grid points of \(\) whose indices are closest to \((x^{},y^{})\), and then the corresponding point \((x,y)\) can be obtained by bilinear interpolation between the values of these points. To minimize the impact of the reverse mapping, we directly calculate the regression loss on \(\) instead of on \(^{}\).

## 5 Experiments

### Implementation Details

We apply our method to both one-stream tracker OSTrack  (denoted as OSTrack-Zoom) and CNN-Transformer tracker TransT  (denoted as TransT-Zoom). The proposed non-uniform resizing module is applied in both the training and testing stages on the search image. We do not apply the proposed non-uniform resizing on the template image because it degrades performance (see Sec. 5.3). The search patch size (\(256 256\)) is the same as the baseline methods, whereas the context factor is enlarged from 4 to 5. We use a controllable grid \(\) with shape \(17 17(m=n=16)\). The importance score map is generated by a Gaussian function with bandwidth \(=64\). We set magnification factor \(=1.5\) and \(=1\) in Eq. (9). We use smaller jitter \(j_{s}=0.1\) with probability \(0.8\) and larger jitter \(j_{l}=0.5\) with probability \(0.2\). Notably, we use the same set of parameters for both OSTrack-Zoom and TransT-Zoom to demonstrate the generalization capability of our method. Except for the resizing operation, all the other training and testing protocols follow the corresponding baselines. The trackers are trained on four NVIDIA V100 GPUs. The inference speed and MACs are evaluated on a platform with Intel i7-11700 CPU and NVIDIA V100 GPU.

### State-of-the-art Comparison

We compare our method with the state-of-the-art trackers on five challenging large-scale datasets: GOT-10k , LaSOT , LaSOT\({}_{ext}\), TNL2K  and TrackingNet .

**GOT-10k** provides training and test splits with zero-overlapped object classes. Trackers are required to be only trained on GOT-10k training split and then evaluated on the test split to examine their generalization capability. We follow this protocol and report our results in Tab. 1. Our OSTrack-Zoom achieves 73.5% AO which surpasses the baseline OSTrack by 2.5%. And our TransT-Zoom achieves 67.5% AO which surpasses the baseline TransT by 0.4%. Compared with previous speed-oriented trackers, our OSTrack-Zoom improves all of the three metrics in GOT-10k by a large margin, proving that our method has good generalization capability.

**LaSOT** is a large-scale dataset with 280 long-term video sequences. As shown in Tab. 1, our method improves the AUC of OSTrack and TransT by 1.1% and 2.2% respectively. Our method OSTrack-Zoom have the best performance among speed-oriented trackers on LaSOT, indicating that our method is well-suited for complex object motion in long-term visual tracking task.

**LaSOT\({}_{ext}\)** is an extension of LaSOT with 150 additional videos containing objects from 15 novel classes outside of LaSOT. In Tab. 1, our method achieves 3.1% and 2.0% relative gains on AUC metric for OSTrack and TransT baselines respectively. Our method OSTrack-Zoom achieves promising 50.5% AUC and 57.4% Precision, outperforming other methods by a large margin.

**TNL2K** is a newly proposed dataset with new challenging factors like adversarial samples, modality switch, _etc_. As reported in Tab. 1, our method improves OSTrack and TransT by 2.2% and 3.0% on AUC. Our OSTrack-Zoom sets the new state-of-the-art performance on TNL2K with 56.5% AUC while running at 100 fps.

**TrackingNet** is a large-scale short-term tracking benchmark. Our method boosts the performance of OSTrack and TransT by 0.1% and 0.4% SUC. The reason for the relatively small improvements is that, contrary to other datasets, TrackingNet has fewer sequences with scenarios that can benefit from an enlarged visual field, _e.g._scenarios with drastic movement or significant drift after some period of time. Please refer to Appendix F for a detailed analysis.

**Comparison with performance-oriented trackers.** We additionally compare our method with the performance-oriented versions of the SOTA trackers. On the one hand, on GOT-10k, LaSOT\({}_{ext}\), TNL2K whose test splits object classes have no or few overlap with training data, our OSTrack-Zoom is on-par with the SOTA tracker OSTrack-384, while consuming only 44.5% MACs of the latter and running 50% faster. On the other hand, on LaSOT and TrackingNet whose test splits have many object classes appearing in training data, our OSTrack-Zoom is slightly inferior to some SOTA trackers. A possible reason is that these SOTA trackers with heavier networks fit the training data better, which is

    &  &  &  &  & _{ext}\)} &  &  \\   & & & AO & SR\({}_{0.5}\) & SR\({}_{0.75}\) & AUC & P & AUC & P & AUC & P & SUC & P & \\   &  &  & **73.5** & **83.6** & **70.0** & **70.2** & **76.2** & **50.5** & **57.4** & **56.5** & **57.3** & **83.2** & **82.2** & 21.5 & 100 \\  & & OSTrack-256 & 256 & 71.0 & 80.4 & 68.2 & 69.1 & 75.2 & 47.4 & 53.3 & 54.3 & - & 83.1 & 82.0 & 21.5 & 119 \\  & TransT-Zoom & 255 & 67.5 & 77.6 & 61.3 & 67.1 & 71.6 & 46.8 & 52.9 & 53.7 & 62.3 & 81.8 & 80.2 & 19.2 & 45 \\  & TransT & 255 & 67.1 & 76.8 & 60.9 & 64.9 & 69.0 & 44.8 & 52.5 & 50.7 & 51.7 & 81.4 & 80.3 & 19.2 & 48 \\   &  & 71.3 & 81.9 & 64.5 & 67.2 & 70.8 & 47.6 & 53.9 & 53.0 & 53.2 & 81.1 & 78.4 & 6.4 & 98 \\  & & SimTrack-B & 224 & 68.6 & 78.9 & 62.4 & 69.3 & - & - & - & 54.8 & 53.8 & 82.3 & - & 25.0 & 40 \\  & & MixFormer-22k & 320 & 70.7 & 80.0 & 67.8 & 69.2 & 74.7 & - & - & - & 83.1 & 81.6 & 23.0 & 25 \\  & & ToMP-101 & 288 & - & - & - & 68.5 & 73.5 & 45.9 & - & - & - & 81.5 & 78.9 & - & 20 \\  & & TransInMo* & 255 & - & - & - & 65.7 & 70.7 & - & - & 52.0 & 52.7 & 81.7 & - & 16.9 & 34 \\  & & Stark-ST101 & 320 & 68.8 & 78.1 & 64.1 & 67.1 & - & - & - & - & 82.0 & - & 28.0 & 32 \\  & & AutoMatch & 255 & 65.2 & 76.6 & 54.3 & 58.3 & 59.9 & 37.6 & 43.0 & 47.2 & 43.5 & 76.0 & 72.6 & - & 50 \\  & & DiMP & 288 & 61.1 & 71.7 & 49.2 & 56.9 & 56.7 & 39.2 & 45.1 & 44.7 & 43.4 & 74.0 & 68.7 & 5.4 & 40 \\  & & SiamRPN++ & 255 & 51.7 & 61.6 & 32.5 & 49.6 & 49.1 & 34.0 & 39.6 & 41.3 & 41.2 & 73.3 & 69.4 & 7.8 & 35 \\   &  &  & 73.7 & 83.2 & 70.8 & 71.1 & 77.6 & 50.5 & 57.6 & 55.9 & - & 83.9 & 83.2 & 48.3 & 61 \\  & & SwinTrack-B & 384 & 72.4 & 80.5 & 67.8 & 71.3 & 76.5 & 49.1 & 55.6 & 55.9 & 57.1 & 84.0 & 82.8 & 69.7 & 45 \\   & & SimTrack-L & 224 & 69.8 & 78.8 & 66.0 & 70.5 & - & - & - & 55.6 & 55.7 & 83.4 & - & 95.4 & - \\   & & MixFormer-L & 320 & - & - & 70.1 & 76.3 & - & - & - & 83.9 & 83.1 & 127.8 & 18 \\   

Table 1: State-of-the-art Comparison on GOT-10k, LaSOT, LaSOT\({}_{ext}\), TNL2K and TrackingNet.

[MISSING_PAGE_FAIL:9]

image must have a relatively small context factor to contain minimal necessary context, which makes the whole template region important for the tracking network to locate the target. Therefore, the tracker cannot benefit from non-uniform resizing of the template image.

**More ablations. First**, we study the impact of the bandwidth \(\) used in the importance function. A smaller \(\) means less area is considered to be important and vice versa. As shown in Tab. (a)a, the ideal value for \(\) is \(64\). Both smaller bandwidth (4) and larger bandwidth (256) cause reduced performance, showing the importance of this factor. **Second**, we also analyze the impact of the controllable grid. As shown in Tab. (b)b, \(m=n=16\) is the best size for this grid. A smaller grid size cause a drastic drop in AUC, showing that too small grid size will cause inaccurate resizing that damages the tracking performance. A larger grid size also degrades performance, for which we believe the reason is that a large grid size will lead to an over-reliance on the important area generation based on the temporal prior, which is however not always correct. **Third**, we further look into the impact of different zoom factors \(\). As shown in Tab. (c)c, \(1.5\) is the best value for \(\). A smaller \(\) will degrade performance due to smaller target resolution. Meanwhile, a larger \(\) is also harmful to the performance as larger magnification will cause trouble when trying to rediscover the lost target. **Finally**, we study the impact of \(E_{rigid}\) by changing the balance factor \(\). As shown in Tab. (d)d, setting \(=1\) reaches the best performance. When \(=0\), the performance drops, demonstrating the effectiveness of the proposed \(E_{energy}\). A larger \(=2\) also degrades the performance, indicating that too much constraint is harmful to the performance.

**Visualization.** We visualize our proposed non-uniform resizing, uniform resizing and FOVEA  resizing in Fig. 4. Compared with uniform resizing, our method can improve the resolution of the target to retain more raw information. Compared with FOVEA, our method can better preserve the aspect ratio and the appearance of the target.

## 6 Conclusion

In this paper, we presented a novel non-uniform resizing method for visual tracking, which efficiently improves the tracking performance by simultaneously attending to a larger visual field and retaining more raw information with high resolution. Inspired by the HVS data processing with limited resources, our method formulates the non-uniform resizing as an explicitly controlled magnification of important areas with restriction on extreme deformation. Extensive experiments have demonstrated that our method can bridge the gap between speed-oriented and performance-oriented trackers with negligible computational overhead.

**Limitations.** One limitation of our method is that we need a fixed context factor to determine the size of the visual field. It would be interesting to develop a method to dynamically adjust the size of the visual field according to the tracking trajectory or appearance of the target in future work.

**Acknowledgements.** The authors would like to thank the anonymous reviewers for their valuable comments and suggestions. This work was supported in part by the National Key R&D Program of China (Grant No. 2020AAA0106800), the Natural Science Foundation of China (Grant No. U22B2056, 61972394, 62036011, 62192782, 61721004, U2033210), the Beijing Natural Science Foundation (Grant No. L223003, JQ22014, 4234087), the Major Projects of Guangdong Education Department for Foundation Research and Applied Research (Grant No. 2017KZDXM081, 2018KZDXM066), the Guangdong Provincial University Innovation Team Project (Grant No. 2020KCXTD045). Jin Gao and Bing Li were also supported in part by the Youth Innovation Promotion Association, CAS.

Figure 4: Visualization of different resizing. For each set of images, from left to right: ours, uniform resizing, FOVEA  resizing. The targets are marked in red box. (_left_) Our method can magnify the target without drastically changing the appearance of the target. (_right_) Our method can magnify the target without significantly changing the aspect ratio of the target.