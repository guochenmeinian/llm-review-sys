# A test of stochastic parroting in a generalisation task: predicting the characters in TV series

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

There are two broad, opposing views of the recent developments in large language models (LLMs). The first of these uses the term "stochastic parrots" from Emily Bender et al  to emphasise that because LLMs are simply a method for creating a probability distribution over sequences of words, they can be viewed as simply parroting information in the training data. The second view, "Sparks of AGI" from Sebastien Bubeck et al , posits that the unprecedented scale of computation in the newest generation of LLMs is leading to what its proponents call "an early (yet still incomplete) version of an artificial general intelligence (AGI) system". In this article, we propose a method for making predictions purely from the representation of data inside the LLM. Specifically, we create a logistic regression model, using the principal components of a LLM model embedding as features, in order to predict an output variable. The task we use to illustrate our method is predicting the characters in TV series, based on their lines in the show. We show that our method can, for example, distinguish Penny and Sheldon in the Big Bang Theory with an AUC performance of 0.79. Logistic regression models for other characters in Big Bang Theory have lower values of AUC (ranging from 0.59 to 0.79), with the most significant distinguishing factors between characters relating to the number and nature of comments they make about women. The characters in the TV-series Friends are more difficult to distinguish using this method (AUCs range from 0.61 to 0.66). We find that the accuracy of our logistic regression on a linear feature space is slightly lower than GPT-4, which is in turn at a level comparable to two human experts. We discuss how the method we propose could be used to help researchers be more specific in the claims they make about large language models.

## 1 Introduction

Large language models (LLMs) are neural networks trained on a large text corpus to predict the next word, phrase or paragraph in that dataset . As the number of network parameters and the size of the corpus increases, the ability of this network to write convincing-sounding texts improves . As a result, an increasing number of compelling LLM applications, from CHAT-GPT to Copilot, have been developed. Recently, Bubek et al. argued that "beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting" . For these authors, this ability to generalise revealed "Sparks of AGI", going on to state that they believed "that [GPT-4] could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system."

The stochastic parrots paradigm critiques such claims by pointing out that large language models simply predict the next word, sentence or paragraph, and it is humans who attribute understanding to its output . LLMs simply replicate examples (i.e. parrot text) from a massive corpus of data .

The stochastic parrots view provides an epistemic critique of claims, such as "Sparks of AGI", about artificial general intelligence. For example, in the context of the benchmark tests (such as those later carried out by ), Raj et al. (2021) write, "the reality of [benchmark] development, use and adoption indicates a _construct validity_ issue, where the involved benchmarks -- due to their instantiation in particular data, metrics and practice -- cannot possibly capture anything representative of the claims to general applicability being made about them." In other words, the very notion of generality, sought to be proven in "Sparks of AGI", cannot be captured by benchmark problems . This critique is fundamental: it doesn't matter how many specific tasks a model completes, there is no convergence towards generality. Even setting these epistemic problems aside, the stochastic parrots view also has practical implications for how we evaluate LLM performance. For example, Lewis and Mitchell (2024) manipulate benchmark tasks to construct 'counterfactual' tasks, by for example adding information that solves the task but LLM's neglect this information, because they are parroting answers to similar, previously trained-on examples .

In spite of the limitation of benchmarks, the fact remains that LLMs do perform well over a wide range of tasks, with little or no additional training data. It is the question of understanding how such performance might arise which we address in this paper. Instead of proposing new benchmarks, we focus on comparing how LLMs perform to simpler, well-understood statistical methods on a novel task. An approach like ours has previously been persued medical imaging -- where a systematic review showed that logistic regression on selected features performed (on average) just as well as complicated machine learning approaches  -- and with respect to conflict prediction -- logistic regression perform just as well (as is easier to interpret) than more complex machine learning models .

For many general tasks, a relatively straightforward method of making predictions is to use linear or logistic regression on the leading principal components of a data set. One example is using principle components of 'likes' of Facebook users to predict the answers people gave to big-five personality tests . Konsinski et al. (2016) first performed PCA or Latent Dirichlet Allocation (LDA) on the matrix of likes and Facebook users, and then used the leading components of the PCA (or clusters of LDA) in a regression model to predict the user's answers in personality tests . This allowed the authors to study how the accuracy of predictions increased with the number of dimensions of the Facebook likes. The method is linear in the PCA space and has the advantage that the results can be interpreted qualitatively. For example, young and female users could be predicted as liking "humorous and juvenile" (author's choice of words) statements such as, "I finally stop laughing \(\) look back over at you and start all over again" .

The above method is potentially interesting in the context of stochastic parrots, because it allows us to, so to speak, look inside the parrot's brain. Large language models encode information using vector semantics: words and sentences are represented as vectors , referred to as embeddings. Words that occur in similar contexts tend to have similar meanings, therefore, they will have a similar vector . The vectors are generally based on a co-occurrence matrix, a way of representing how often words co-occur. An alternative to using the term-document matrix to represent words as vectors of document counts, is to use the term-term matrix. If we then take every occurrence of each word and count the context words around it, we get a word-word co-occurrence matrix . Embeddings can be obtained with transformers models , which were initially developed for machine translation in 2017 .

We can use principal components of the embeddings of a language model, with respect to a specific problem, in order to both understand what information is used in solving the task and to test the degree to which performance on that task is achieved from the representation of the data or from some other unknown mechanism. To make these statements concrete, we now outline what we do in this article. We address the task of predicting which character said which specific lines of dialogue in two US TV series: Big Bang Theory and Friends. This task is reminiscent of the personality research discussed above in that the characters in the show have very stereotypical personalities: can we predict character personalities from their line in the show? Such problems are of specific interest for this article, for three reasons (1) an increasing number of applications of AI involve supposed personality tests and analyses ; (2) such tests raise ethical issues about both reliability and applications ; (3) they are sometimes used to imply that machines can understand us better than we understand ourselves . The character personality test is an example of generalisation in the sense that, while large language models might have been fed data from these series, they haven't been trained to solve this specific task.

We proceed as follows. We first detail the method of and logistic regression on the principal components of the embeddings. We then analyse which PCA components are most predictive of statements by the characters, how the number components affects accuracy and differences between the TV shows. Finally, we compare performance of our simpler model to GPT-4  and one human expert, with extensive experience of the two TV shows.

## 2 Methods

### Embeddings and PCA

The dataset is the transcript of the first 10 seasons of the TV-series The Big Bang Theory 1 and 10 seasons of the TV-serie Friends 2 in English. We cleaned the dataset, by only keeping the main characters and their respective dialogue lines. This gives \(44,966\) dialogue lines for the TV series The Big Bang Theory and \(51,615\) dialogue lines for the TV series Friends. We then transformed these dialogue lines into a vector, i.e. we create embeddings using the python library SentenceTransformer and the model 'all-MiniLM-L6-v2' . Each dialogue line then has a specific embedding, a vector of dimension 384. For comparison, the small text embedding of OpenAi, 'text-embedding-3-small', gives 1,536 output dimension [21; 5].

We then performed a principal component analysis (PCA) on the embeddings (for more details of the method we follow see ). Principal Component Analysis(PCA) determines the directions that maximize the variation in the data. The PCA is a procedure that takes dataset with several variables, to a smaller dataset with new variables (the principal components) that will be a linear combination of the former variables. Each dimension in this space corresponds to a feature that will be explicitly defined later. To ensure a representative view of the dataset, we need to standardize it so that no single variable disproportionately influences the analysis, by removing the mean then divide by the standard deviation. Then, we calculate the covariance matrix. A covariance matrix is a square matrix that shows the covariance between pairs of variables in the dataset. The diagonal of the matrix gives the variance of the variables and the other terms give the covariance between the pair of variables. The covariance measures of how much two random variables vary together, by estimating the linearity between them. From the covariance matrix we deduce the eigenvectors and eigenvalues, by doing an eigenvalue decomposition of the covariance matrix \(\). We find the eigenvector by solving \((- Id)x=0\), where \(x\) is the eigenvector associated with the eigenvalue \(\) The eigenvalue gives the magnitude (or accounted variance) of the data along the new feature dimension. The eigenvector gives the direction of the data along the new feature dimension, and forms the linear combination for a principal component. The eigenvalues are in descending order and as explained in , they'maximize the explained variances on each dimension'. We refer to the the coefficients of the leading eigenvector as the first principal component (PCA1), the second eigenvector as PCA2 and so on. We reduce the 384 dimension of each embeddings to a dimension space of 300. All calculations were performed in Sklearn  and full code is available here 3.

An important aspect of our approach is gaining a qualitative understanding of how the principal components reflect the meaning of the dialogue lines. Each PCA corresponds to one eigenvector and consequently to one dimension from which we are able investigate which kind of phrases tied to that dimension. To help us make this analysis we used two-dimensional visualisation of the data. First we implemented a 10-means cluster on two principal components at a time, starting with the leading components (i.e PCA1 and PCA2). We colour each cluster and then assign the phrase nearest of the center as the cluster name (see figure 1). We also looked at the most extreme dialogue line in each PCA, by printing out the sentences with the highest values and the smallest values. From these we assigned a qualitative interpretation of the "meaning" of the leading PCAs. In the annex, we report the tenth highest values and the tenth smallest values.

### Character prediction

In order to predict which dialogue line comes from which character, we use a logistic regression on the PCAs of the dialogue lines of the characters. We follow the notation from  and let \(u_{j,i}\) be the j-th the coefficient of the principal component of the i-th dialogue line. First, we normalise all the coefficients \(u_{j,i}\) of the principal components by taking away the mean and dividing by the standard deviation, so each component has mean zero and standard deviation of one. We then performed a binomial logistic regression -- e.g. does the dialogue line belong to Penny or Sheldon? -- based on a linear prediction of the dialogue line \(i\):

\[_{0}+_{1}u_{1,i}+...+_{n}u_{n,i},\]

allowing to measure (using regression coefficients \(\{_{0},...,_{n}\}\)) how the the explanatory variables \(u_{1,i},...,u_{n,i}\), impact the prediction. The fitted logistic regression is model is given by

\[(|)=+_{1}u_{1,i}+...+_{n}u_{n,i} )}}\]

where \(_{0}\) determines the intercept (i.e. it is the outcome when all the other predictors variables are equal to zero). Each coefficient \(_{i}\) estimates the additional effect of adding the corresponding variable to the model prediction.

The sign of the coefficient indicates the influence of the specific principal component on the probability it is a particular character. If the sign is positive then it is more likely to be that character (Penny in the example above) if the dialogue line has larger and more positive values of that component. Conversely, if the sign is negative that means it is less likely to be that character if the dialogue line has larger and more positive values of that component. The larger the magnitude of the coefficient, the more important the predictor variable is in making the prediction.

For each TV series, we proceed to a logistic regression with 300 first PCAs, for each possible pair of characters. We obtain a predictor function and evaluate the absolute value of each regression coefficient. We obtain the magnitude of each coefficient and therefore assess which coefficients have the most importance in the logistic regression. Afterwards we take the ten regression coefficients with the largest absolute value and plot them (see figure 3 and 8). From this analyse, we deduce which dimensions that have an impact on the character's prediction. To evaluate performance we calculate the AUC (Area Under The Curve) ROC (Receiver Operating Characteristics) curve to evaluate as a function of the dimensions.

### Comparing to GPT4 and human expert

In order to test our method against a large language model we queried GPT4 with the following system prompt: _"You are expert on the TV series The Big Bang Theory. You are now being challenged to identify characters from the series. Try your best to do well. If you can beat another human expert there is a prize."_ and a query that asked _"Tell me who was most likely out of Leonard and Sheldon (from the series Big Bang Theory) to have said the following line of dialogue: [DIALOGUE LINE]. Now state the most likely character as a single word, either Leonard and Sheldon. Do not write anything else."_ Character and TV series names were adjusted appropriately for each test. We tested four pairs (Penny/Sheldon, Leonard/Sheldon, Phoebe/Ross, Phoebe/Chandler). We first repeated the above procedure 100 times, 50 times for each character, to test the accuracy of the classification (i.e. proportion of correct answers).

We also provided the same dialogue lines to two motivated human experts (who had watched both series in their entirety two times, most recently within the last year) and expressed a determination to beat GPT4. Both participants were relatives of the co-authors of this article. The same dialogue lines on which GPT4 was tested, were presented in a random order in the spreadsheet file. The subjects were asked to guess the name of the character for each dialogue line, and write it into the spreadsheet.

## 3 Results

### Qualitative analysis of the principal components

We started by plotting the embedded dialogue lines 'Big Bang Theory' in terms of the six most important principal components, in order to visualise the most distinguishing features of the dialogue. The first two of these (PCA1 and PCA2) are shown in figure 1aa. The nearest neighbour clustering then allows us to see where different dialogue lines are found in these dimensions. We can see that larger negative values of PCA1 corresponds to very short phrases (for example 'Uh' in the pink cluster in the top left of the figure) and larger positive values of PCA1 correspond to phrases about Sheldon (for example 'Sheldon, what do you expect us to do?' in the green cluster in the top right of the figure). The qualitative analysis of PCA1 confirmed this pattern, with 'Yeah' being the most extreme negative value and 'You know, I was thinking. Without Sheldon, most of us would have never met, but Penny would still live across from him.' being the extreme positive value (see Annex 6 for a list of the ten most extreme positive and negative values of PCA1 and the other principal components).

Following the same approach for PCA2, we found that the negative values are associated with long phrases about a female characters an positive values with phrases about Sheldon. The most extreme negative value is 'Well, there was the time I had my tonsils out, and I shared a room with a little Vietnamese girl. She didn't make it through the night, but up till then, it was kind of fun.' and the most extreme positive value is 'Leonard, Sheldon.'(see annex 6). The cluster values in figure 0(a) also show the same pattern: with 'Indian princess who befriends a monkey who was mocked by all other monkeys because he was different. For some reason I related to it quite strongly' in the orange cluster

Figure 1: Projection of the first 6 PCAs. Each PCA has an interpretation from the qualitative analysis. Each plot has their respective cluster along with the average phrase of each cluster for The Big Bang Theory dialogue lines

at the bottom of figure (a)a and 'Sheldon, why are you doing this?' in the light green cluster at the top of the same figure.

A similar approach can be used to interpret figure (b)b and c. PCA 3 ranges from phrase that questions a premise ('Really? I didn't know that.') to phrases with a first person future action ('Aw, sweetie, I'm comfortable around you, too.'). PCA 4 ranges from a phrase about relationship ('Really? That seems rather short sighted, coming from someone who is generally considered altogether unlikable. Why don't you take some time to reconsider?') to a phrase related to eating out ('Excellent! What are you planning to wear?'). The fifth dimension is phrase with often a negation or counterargument (like 'Oh no, no, no, crystals don't work', which is green in figure 1) to a short question about a woman (like 'She knows you. She's tense. We all are. Buy a basket!', which is red in the same figure). Finally, PCA 6 ranges from an apology (e.g. 'I wish you weren't wearing flip-flops. It's dangerous to drive in flip-flops') to a phrase with affirmative statement( e.g. 'Still going to introduce him?'). This final interpretation is even clearer when we look at the extreme negative value ( 'Relax, it wasn't your fault.') and extreme positive value ('Sure. I'd like to meet her.'). Overall, in The Big Bang Theory the distinguishing characteristics of the principal components often relate to the characters views of women. For Friends, there are also clear semantic differences in the sentences, although these appear to be less gender stereotyped. We give a full analysis of the leading six components in annex 5.3.

When we plot the average position of the characters in the space of the first two components, the differences are very small in comparison to the variation (figure 5 in annex 6). For example, while there is a distance of 0.33 between Leonard and Amy on the PCA 1 axis, the standard deviation of the values for the Leonard and Amy on that axis are 3.62 and 3.58, respectively. This observations indicates that it is impossible to distinguish the characters in terms of just a single dimension. We do note, though, that Friends characters are even closer together than The Big Bang Theory characters (the PCA1 distance between Chandler and Rachel is 0.15 and between Chandler and Joey is 0.11, while the standard deviations of Chandler, Rachel and Joey are respectively 3.62, 4.03 and 3.78). The biggest difference we observed is between Penny and Sheldon.

### Character prediction

While a small number of principal component dimensions is not sufficient to tell the characters apart, can we use more of the dimensions to make the distinction? To test this we performed binomial logistic regression on pairs of characters as a function of the number of principal components we included in the model. The AUC values in figure (a)a show a steady improvement in the predictions up to around 50 principal components for Big Bang Theory, after which only slight increases in performance are obtained. Sheldon and Penny were easier to distinguish using this method than Sheldon and Leonard. Figure (b)b, shows that Friends characters were much more difficult to distinguish using this method.

If we view the principal component analysis as an attempt to capture the character's personality by their dialogue lines (as in the analysis by ) then we can say that the TV characters personality have a dimension of somewhere between 50 and 100. Each new dimension gives a small extra insight

Figure 2: AUC curves to assess the performance of the logistic regression, by increasing the number of dimensions, in the dialogue linesâ€™s prediction for two different couples for the two Tv serie

into the character differences. Since Friends characters are more difficult to predict from what they say, we can conclude that Friends characters are less stereotyped than characters in The Big Bang Theory.

We can investigate which PCA dimensions best distinguish characters by looking at the coefficients of the binary regression. Figure 3 shows the ten most important components (determined by the magnitude of the absolute value of the coefficients in the regression) for distinguishing the characters dialogue lines in The Big Bang Theory. Each row represents a character pair, with the PCAs ordered from left to right according to the magnitude of the coefficients. The first column corresponds to the coefficient with the largest magnitude in the linear predictor function, the second column corresponds to the second coefficient with the second largest magnitude, and so on.

As an example, the first row is the character prediction for the couple 'Penny and Sheldon' should be read as considering the probability the dialogue line is by Penny, i.e. P (Penny \(=1\)|the line is said by Penny or Sheldon). The first cell entry, PCA19, is the coefficient in the linear predictor function with the largest absolute value. Performing a qualitative analysis on PCA19 (see annex 6) we find that negative coefficients correspond to lines about food and positive coefficients correspond to lines about comics. In this case, the coefficient of the PCA19 is negative, implying that if a dialogue line is about meal or food, it is more likely to be spoken by Penny than Sheldon.

The most common occurring component in figure 3 is exactly this PCA 19 (food vs. comics) which has 12 occurrences. PCA 2, which is long phrases about a female character versus phrases with one name has 11 occurrences. PCA 7 has 11 occurrences and ranges from phrases with yes/no to question about the current situation. The next most common occurring components are PCA4 (10 occurrences) which ranges from an apology to phrase with affirmative statement; PCA15 (10 occurrances) ranging from long phrases about a woman to short phrases about houses; PCA17 (9 occurrences) range from short phrases about travel to long food related phrases: PCA5 (9 occurrences) which range from long dialogue lines that express an opinion to short questions about a female character.

Figure 4 shows the relationship between the characters in terms of PCA19 (which distinguishes dialogue lines about meal/food related from those about comics). The graph shows the magnitude of

Figure 3: Regression coefficients for each possible character pairs for the TV series The Big Bang Theory. For each pair, we conduct a logistic regression to predict if the dialogue line is more likely to be said by a character1 such that \(P(=1|\)the line is said by character1 or character2). We use the first 300 principal components in the logistic regression. Then, we assess the absolute value of each coefficient to determine their magnitude. Following this, we select the top ten coefficients for each linear predictor function. We report in this figure those coefficients, along with their corresponding dimensions. The coefficients are in decreasing order from left to right: the left side have the coefficient with the highest magnitude, the right side have the coefficients with the lowest magnitude.

the coefficient and the direction of the arrow indicates that the coefficient is positive. For example, for P(Penny|the line is said by Penny or Sheldon) the regression coefficient for the PCA19 is negative, reflecting the fact that Penny talks more about food and Sheldon that talks more about comics, so the arrow points from Sheldon to Penny. Similarly, we see that Bernadette talks more about food than Raj, Howard Sheldon and Amy and thus the arrows point toward her. And Raj talks more about comics than Penny, Bernadette and even Sheldon, so the arrows point out from him in the figure. In the case of the TV series Friends, the magnitude of the regression coefficients are smaller than those for The Big Bang Theory and a more varied number of components are represented (see annex 5.3).

While the method for constructing figure 4 can give an indication of how the components distinguish the characters, we should bear in mind that in a regression of hundreds of variables (on which this graph is based) the relationships established are not always straightforward. For example, in the figure, we see that the respective models predict that Howard talks more about comics than Sheldon, who talks more about comics than Penny, and Penny talks more about comics than Howard. This inconsistency is likely due to other principal components distinguishing Penny and Howard better than PCA19, and PCA19 acting as a counterbalance, to these additional components. A full analysis of these relationships is beyond the scope of the current article.

### Comparing to GPT4 and human expert

Initial prompting of GPT4 revealed that it has knowledge of the two TV series in its training data. GPT4 replied that it "can provide information about the show, its characters, plot points, cultural impact, and more". It was also able to provide motivation for its answers. For example, when we asked if this dialogue line 'Okay, sweetie, I don't know if we're gonna have cookies, or he's just gonna say hi, or really what's gonna happen, so just let me talk, and we'll...', it correctly answered 'Penny'. Then, when asked, it to explain why it draws conclusions about the characters, it cited criteria "Context of Character Behavior", "Speech Patterns" and "Interaction Dynamics".

For the set of 100 dialogue lines, a direct prompt to GPT4 (see methods for details) was correct for Penny versus Sheldon on 81 occasions, for Sheldon versus Leonard on 71 occasions, for Phoebe versus Ross on 66 occasions, and for Phoebe versus Chandler on 65 occasions. For these same test examples, the first human expert was correct on 71, 76, 67, and 59 occasions, respectively. The second human expert was correct on 74, 72, 70, and 73 occasions. For comparison, the accuracy (percentage correct over all sentences) for the 300 dimensional PCA model was 72.8%, 68.1%, 59.7% and 60.6% respectively. The standard error for a proportion of 70% is \(0.7 0.3 100 4.5\%\), suggesting a comparable level of performance between the human experts and GPT4, and a slightly lower level of performance for the 300 dimensional PCA model.

Figure 4: Relationship between characters in The Big Bang Theory in terms of PCA19 (that distinguishes lines about meal/food related form lines about comics). The value on the arrows show the magnitude of the coefficient. Only pairings where the absolute value of the regression coefficient is greater than \(0.1\) are included. The person at the start of the arrow talks about comics more than they talk about food compared to the person at the end of the arrow.

Conclusion

Our qualitative analysis highlights how, when interpreted by a human, the principal components of the embeddings reflect the meaning of the dialogue lines of TV series. Many of dimensions contributing to the prediction are related to female characters. This can be attributed to the fact that the TV series portrays very stereotypical characters, with the main protagonists portrayed as seeks, embodying various cliches associated with them. A number of previous studies have identified gender and racial stereotyping within the way models represent data [4; 2; 29], we have shown that these dimensions are also important in the predictions these models make. Friends, in which the characters might be considered to have smaller stereotyped (within-group) differences, was more difficult to predict using this method.

We have shown that given the principal components of the dialogue in a TV series, we are able to predict the characters personality using logistic regression, to a level of performance slightly below that of GPT4. We needed 50-100 dimensions in the logistic regression to predict a dialogue line in the TV series. This might be said to support the idea of a language model more like a stochastic parrot than a spark of AI, in the sense that a large part of the predictive skill of the model can be obtained by adding up the components of the word embeddings and providing an appropriate prediction. Indeed, we have used a much smaller embedding vector (384 dimensions) that GPT4 (several thousand dimensions) to achieve somewhat comparable results.

That said, there remain two things which GPT4 does which our model does not. Firstly, our analysis starts from the sentence embeddings. Taking these embeddings as given ignores the complex process by which these are generated through training in the first place [9; 30]. Secondly, we had to specify the problem we wanted to solve as a logistic regression problem and train on previous data. GPT4, on the other hand, requires no additional training step and, from the given prompt, can identify the requested character. In light of these limitations, we see our work as highlighting the need to be more specific about claims related to sparks of AI . We have shown that prediction part of the question of identifying TV character personality is (to some degree) obtainable from linear models, the question then is where the supposed spark lies? Is it in the creation of embeddings or is it in GPT4's ability to identify the prediction problem from the input provided by the user? We would suggest that further dissections of how these methods work, like we have done here for the prediction stage, can shed more light on these questions.

Our study is limited to a qualitative study of two very specific datasets. The contribution is primarily methodological. We propose an alternative to benchmark testing for understanding why a machine learning method works in the way it does, by comparing it to a method based on linear predictions. As such, it is a qualitative contribution to a larger debate around how to evaluate LLMs, rather than a quantitative demonstration of model performance.