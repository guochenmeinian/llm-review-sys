# Learning From Biased Soft Labels

Hua Yuan\({}^{1,2}\), Yu Shi\({}^{1,2}\), Ning Xu\({}^{1,2}\), Xu Yang\({}^{1,2}\), Xin Geng\({}^{1,2}\), Yong Rui\({}^{3}\)

\({}^{1}\) School of Computer Science and Engineering, Southeast University, Nanjing 210096, China

\({}^{2}\) Key Laboratory of New Generation Artificial Intelligence Technology and Its Interdisciplinary Applications (Southeast University), Ministry of Education, China

\({}^{3}\) Lenovo Research, Beijing 100085, China

{yuanhua,seushiyu,xning,xuyang_palm,xgeng}@seu.edu.cn, yongrui@lenovo.com

Corresponding author

###### Abstract

Since the advent of knowledge distillation, many researchers have been intrigued by the _dark knowledge_ hidden in the soft labels generated by the teacher model. This prompts us to scrutinize the circumstances under which these soft labels are effective. Predominant existing theories implicitly require that the soft labels are close to the ground-truth labels. In this paper, however, we investigate whether biased soft labels are still effective. Here, bias refers to the discrepancy between the soft labels and the ground-truth labels. We present two indicators to measure the effectiveness of the soft labels. Based on the two indicators, we propose moderate conditions to ensure that, the biased soft label learning problem is both _classifier-consistent_ and _Empirical Risk Minimization_ (ERM) _learnable_, which can be applicable even for large-biased soft labels. We further design a heuristic method to train Skillful but Bad Teachers (SBTs), and these teachers with accuracy less than 30% can teach students to achieve 90% accuracy on CIFAR-10, which is comparable to models trained on the original data. The proposed indicators adequately measure the effectiveness of the soft labels generated in this process. Moreover, our theoretical framework can be adapted to elucidate the effectiveness of soft labels in three weakly-supervised learning paradigms, namely incomplete supervision, partial label learning and learning with additive noise. Experimental results demonstrate that our indicators can measure the effectiveness of biased soft labels generated by teachers or in these weakly-supervised learning paradigms.

## 1 Introduction

Knowledge distillation [2; 17; 20] has achieved remarkable achievements in a wide range of applications. It has emerged as a popular paradigm for model compression [22; 21] and transfer learning [41; 34] by distilling knowledge from the big model (teacher) to the small model (student). Students inherit the knowledge of the teacher by imitating the soft labels generated by the teacher model. Many experiments show that learning from soft labels can be very effective, even surpassing learning from ground-truth labels [2; 17; 37]. However, there are still many mysteries why these soft labels are effective. Most existing theories do not apply to soft labels that significantly deviate from the ground-truth labels and lack explicit indicators to evaluate the soft labels [39; 58; 7; 35]. In this paper, we mainly focus on the effectiveness of the biased soft labels, especially large-biased soft labels.

It is significant to study the effectiveness of biased soft labels. Firstly, expert-labeled annotation typically requires a substantial investment of manpower and time. But the soft labels in weakly supervised learning could be cheap and biased [59; 46]. Evaluating the biased soft labels is instructive for weakly supervised learning. Secondly, these biased soft labels can apply for privacy protection[15; 11]. When they are employed for subsequent training, they can conceal the ground-truth labels. It is consistent with the requirements of privacy and utility in privacy protection .

After realizing the significance of biased soft labels, we study the biased soft labels and reveal that large-biased soft labels can also teach good students. This phenomenon inspire us to seek indicators to measure the effectiveness of soft labels. As a result, we propose two intuitive indicators, namely _unreliability degree_ and _ambiguity degree_. The intuition behind them is to convert the soft label into a top-\(k\) set (\(k\) is a constant), which contains the top-\(k\) labels of the soft label. Unreliability degree is the probability that the ground-truth label is not in the top-\(k\) set while ambiguity degree is the upper bound of the probability that the incorrect label and the correct label co-occur in the top-\(k\) set.

Furthermore, based on the proposed indicators, we present moderate conditions to ensure that, the biased soft label learning problem is classifier-consistent  and Empirical Risk Minimization (ERM) learnable . The former guarantees that the learners can converge to the optimal solution as learning from original labeled data. The latter implies that learners' performance can generalize to the entire data distribution. Our theory not only guarantees the effectiveness of biased soft labels, but also provides explicit indicators to evaluate the soft labels.

Biased soft labels are prevalent in weakly supervised learning [24; 44; 59], and our theory can offer theoretical insights for these paradigms. In detail, we apply the theory to three classic weakly-supervised learning paradigms: incomplete supervision , partial label learning  and learning with additive noise . In these weakly-supervised learning paradigms, soft labels are biased and we provide a theoretical guarantee for the learners in these fields. In other words, we prove that the biased soft labels in the three paradigms are effective to train a good model.

It is important to note that, Yuan et al.  argues that poorly-trained teachers can teach good students. In fact, their soft labels are a mixture of the teacher's outputs and the ground-truth labels with a ratio of \(0.1:0.9\), which makes the soft labels close to the ground-truth labels as illustrated in Figure 1(b). Differently, our experiments reveal that large-biased soft labels can also teach good students. Furthermore, the effectiveness of these soft labels can be measured by unreliability degree and ambiguity degree. In addition, experiments on three weakly-supervised learning paradigms also demonstrate that biased soft label learning problem is learnable and the proposed indicators are effective. Our contributions can be summarized as follows:

* We find that learning from large-biased soft labels may also achieve comparable performance and intend to explore the underlying mechanisms behind the effectiveness of the biased soft labels.
* Two indicators have been proposed to measure the effectiveness of soft labels. Based on the indicators, we present moderate conditions to guarantee the effectiveness of the soft labels. It is proved that the biased soft label learning problem is classifier-consistent and ERM learnable.
* A heuristic method is designed to train skillful but bad teachers, i.e., teachers with low accuracy but who can teach good students. We can explain this phenomenon with unreliability degree and ambiguity degree.
* The theory provides a theoretical view for the learners in three weakly-supervised learning paradigms. Specifically, we provide theoretical guarantees for the learnability of these paradigms from the perspective of soft labels. Experimental results are consistent with our theory.

Figure 1: (a) Images of the birds in CIFAR-10. (b) An example of the soft label for ”Bird” in Yuan et al. . (c) An example of the large-biased soft label for ”Bird”. Compared with Yuan et al. , the value of the ground-truth label (Bird) in the large-biased soft label is probably much smaller, and is likely not the top of the large-biased soft label.

Related Work

**Knowledge Distillation and Label Smoothing** Knowledge Distillation (KD) was initially proposed in model compression [22; 21] and then applied to transfer learning [41; 34]. There is growing interest in why distilling can transfer information and what the _dark knowledge_ hidden in the soft labels is. Furthermore, the dark knowledge also exists in the variants of knowledge distillation, which are introduced in the appendix A.1. Label Smoothing (LS)  is a regularization method to improve performance by mixing the uniform noise into the ground-truth label. It is convinced that soft labels can restrain overconfidence of the student model. Essentially, both KD and LS can be unified as learning from soft labels Yuan et al. .

**Label Enhancement** Label Distribution Learning (LDL)  was proposed to exploit the label distribution to mirror the relationship between the label and the instance, where the formalization of the label distribution is identical to the soft labels mentioned above. In the remainder of the paper, we use nomenclature soft labels. Due to the high cost of labeling the soft labels, Label Enhancement (LE)  was proposed to recover the soft label from the logical label by exploiting the implicit correlation among different labels. Numerous novel algorithms have been designed in recent years that aim to improve the predictive model with the soft labels [47; 57]. Wang and Geng  applied the margin theory to the soft labels and designed the adaptive margin loss.

In fact, most existing explanations of the soft labels are empirically and experimentally validated, while the rigorous theoretical analyses usually have strong assumptions regarding the model or data distribution. Phuong and Lampert  explored the mechanism of distillation where the teacher model and the student model are linear. Allen-Zhu and Li  supposed that the instance could be decomposed into multiple independent features and had a linear relationship with the sample, and then proved the effectiveness of the soft labels. Wang and Yoon  solved the objective functional problem of self-distillation with the Green's function, which assumes that the network can reach the optimal solution. Menon et al.  and Zhou and Song  regarded the generated soft labels as the posterior probability and assumed the existence of the Bayes probability. There is also some work analyzing soft labels from the perspective of transfer risk [19; 18]. Most existing theories suggest that students can achieve good performance only when the soft labels are close to the ground-truth labels, and lack explicit indicators to measure the effectiveness of the soft labels.

## 3 Methodology

### Preliminary

Let \(\) be the instance space, \(=\{1,2,,c\}\) be the label space with \(c\) classes and \(\) be the soft-label space over \(\), i.e., \(=\{^{c}|_{i}_{i}=1,_{i } 0i\}\). Define \(\) as the data distribution over \(\) and \(\) as the hypothesis space from \(\) to \(\). Each \(h\) is called the learner or hypothesis. When \(h\) is a neural network, it often employs a softmax function in the final layer and takes the highest value as the prediction. Slightly abusing notation, we use \(h()\) to denote the soft label generated by model \(h\), and \(()\) to represent the model's prediction.

Next, we introduce some important concepts in the Probably Approximately Correct (PAC) learning . The Natarajan dimension  was proposed to characterize the capacity of multiclass hypothesis spaces and we denote \(d_{}\) as the Natarajan dimension of \(\). The expected classification error of a hypothesis \(h\) is defined as \(_{}(h)=_{(,y)}[ (h() y)]\), where \(()\) is the indicator function. Given a finite dataset \(\), the empirical classification error is defined as \(_{}(h)=_{i=1}^{n}(h() y)\). The Empirical Risk Minimization (ERM) learner \(h^{}\) is defined as \(h^{}=*{arg\,min}_{h}_ {}^{}(h|f)\), while the optimal learner \(h^{*}\) is defined as \(h^{*}=*{arg\,min}_{h}_{}(h)\).

Considering that this study involves two models: generating soft labels and learning from the soft labels. To avoid ambiguity, we use \(f\) to represent the model generating soft labels (teacher) and \(h\) to denote the model trained with soft labels (student). It should be noted that \(f\) can be a neural network or a map based on certain rules, such as label smoothing.

In order to gain a deeper insight into the concept of biased soft labels, we have provided precise definitions for relevant terms below. These definitions may not have a direct correlation with the theoretical analysis presented in section 3.2, but they serve to elucidate the boundaries of our research.

**Definition 1** (Bias of soft labels).: _Given a dataset \(\) consisting of \(n\) samples, the feature vector for the \(i\)-th sample is denoted as \(_{i}\) and the corresponding label is denoted as \(y_{i}\). Let \(f\) represent a model or a mapping rule. The bias of the soft labels generated by \(f\) on dataset \(\) is_

\[(f,)=_{i=1}^{n}[1-f_{y_{i}}( _{i})]\]

_where \(f_{y_{i}}(_{i})\) refers to the component of the soft label \(f(_{i})\) that corresponds to the true label \(y_{i}\)._

**Definition 2** (Large-biased soft labels).: _Soft labels generated by \(f\) on dataset \(\) is called biased soft labels when \((f,)>0\) and called large-biased soft labels when \((f,) 1\)._

The bias, as defined here, essentially represents the disparity between the soft label and the true label. Here, we employ the Manhattan distance, also referred to as L1-norm, although other metrics can also be suitable. A threshold of \( 1\) is employed to categorize soft labels as "large-biased." In practical scenarios, the choice of the threshold can be adapted based on specific contexts. After establishing the concept of large-biased soft labels, we proceed to define "bad teachers."

**Definition 3** (Bad teachers).: _We define \(f\) as a bad teacher if the soft labels it generates on dataset \(\) are large-biased. Typically, \(\) is the training set for \(f\)._

The definition of the bad teachers is based on the definition of large-biased soft labels. Similarly, here we adapt 1 as the boundary value for bad teachers. It's important to note that when we use "good/bad" to describe teachers, we are referring to the performance of teachers on the dataset \(\). On the other hand, when we say teachers are "skillful," we are emphasizing that they are adept at instructing students, leading to excellent student performance. Due to the fact that the performance of the student model is contingent on both the model structure and the complexity of the dataset, we are unable to establish precise boundaries for "skillful teachers." In this context, "skillful" merely signifies that the teacher produces students with acceptable performance.

### Theoretical Analysis of Soft Labels

In this subsection, we define two indicators, unreliability degree and ambiguity degree, to measure the effectiveness of soft labels. Based on these indicators, we present moderate conditions to ensure that the biased soft label learning problem is classifier-consistent and Empirical Risk Minimization (ERM) learnable. The underlying intuition behind the indicators is to convert the soft label into a set. Let \(_{k}()=\{i ik\}\) be the set of top \(k\) labels in the soft label \(\). Here, \(k\) is a constant ranging in \(\{1,2,,c-1\}\). When \(k=1\), \(_{k}()\) has only one element, i.e., the prediction. For soft labels generated by teacher \(f\), we define the _unreliability degree_ as,

\[_{k}(f)=_{(,y)}(y_{k} (f()))\] (1)

However, it is not enough to measure the soft labels by merely unreliability degree. For example, for images whose ground-truth labels are \(1\), if label \(2\) always appears in \(_{k}()\), then the student model is unable to distinguish label \(1\) from label \(2\). Therefore, we introduce the _ambiguity degree_ and extend it to more general soft labels (induced by teacher \(f\))

\[_{k}(f)=_{i}_{(,y) ,y i}(i_{k}(f()))\] (2)

Ambiguity degree bound the probability of _co-occurrence_. In other words, if a model \(f\) is with ambiguity degree \(_{k}(f)\), then \((i_{k}(f()) i y,x,y)_{k}(f)\). The smaller \(\) or \(\) is, the more effective the soft labels are. However, when \(k\) increases, \(\) will decrease and \(\) is inverse, which means \(k\) should be selected cautiously.

**Theorem 1**.: _Training with soft labels generated by the teacher model \(f\), if \(_{k}(f)<1-(f)}{1-_{k}(f)}\), then the optimal student model \(h^{*}\) satisfies \(h^{*}=*{arg\,min}_{h}*{Err}_{}(h|f)\)._

The proof can be found in A.2. Theorem 1 ensures that the student model \(h\) learning from the teacher can converge to the optimal learner over the entire data distribution. This property is known as classifier-consistency . However, it does not provide the sample complexity bounds of the learning problem. In other words, it does not establish a connection between the generalization bound and the number of training samples.

Next, we provide our main result, a sufficient condition for the ERM learnability of the biased soft label learning problem. In the previous subsection, we presented the definition of expected classification error and empirical classification error, which are based on the label space \(\). Here, given the soft label generating model \(f\), we define the soft label-based expected error

\[^{}(h|f)=_{(,y) }[[()_{k}(f()))],\]

and the soft label-based empirical error on dataset \(=\{(_{i},y_{i})\}_{i=1}^{n}\)

\[_{}^{}(h|f)=_{i=1}^{n} ((_{i})_{k}(f(_{i}))).\]

In the above equations, the set \(_{k}(f(_{i}))\) is determined by the soft label \(f(_{i})\). We denote \(H_{}\) as the set of teacher models whose generated soft labels are with unreliability degree \(\), i.e., \(H_{}=\{f:_{k}(f)=\}\). With such soft labels, we analyze the performance of the ERM learners (students). The definition of the ERM learner is provided in the preliminary. Specifically, based on the unreliability degree in (1) and the ambiguity degree in (2), we provide a sufficient condition to ensure that, the biased soft label learning problem is ERM learnable.

**Theorem 2** (Main theory).: _For \(k\{1,2,,c-1\}\), assume the unreliability degree \(_{k}(f)\) and the ambiguity degree \(_{k}(f)\) of the soft labels generated by teacher model \(f\), denoted concisely as \(_{k}\) and \(_{k}\), and satisfy \(0<_{k},_{k}<1\) and \(_{k}+_{k}<1\). Let \(_{k}=)}{1-_{k}+_{k}}\) and suppose the Natarajan dimension of the hypothesis space \(\) is \(d_{}\). Define_

\[n_{0}(,,) =_{k\{1,2,,c-1\}} }{2}+}}(d_{}((2d_{ })\] \[+}{2}+}}+2 L)++1)}.\]

_Then when \(n>n_{0}\), the ERM learner satisfies \(_{}(h^{}|f)<\) with probability \(1-\)._

The proof can be divided into two lemmas. Let define \(H_{}\) be the set of hypotheses with error at least \(\), i.e, \(H_{}=\{h:_{}(h|f)\}\). Our target is to bound the probability \((h H_{})\), which measures the generalization of the learner \(h\). Since the entire soft label space is inaccessible, \(H_{}\) is evaluated by the mediator \(R_{n,}\) as follows:

\[R_{n,}=\{()^{n}:  h H_{},_{}^{}(h|f)=0 \}.\]

Then, our goal is to prove that \(( R_{n,} f H_{})\). In other words, given dataset \(\), the student model \(h\) has the generalization bound greater than \(\) with probability at most \(\). Since the teacher model \(f\) is intractable and the soft label space \(\) is unknown, it is very difficult to directly calculate the conditional probability \(( R_{n,} f H_{})\). We bound it by introducing a testing set \(^{}\). Lemma 1 is adapted from [(9], Lemma 11.1.5] and [(16], Corollary 2.6].

**Lemma 1**.: _For a testing set \(^{}()^{n}\), we can define the set \(S_{n,}\) as_

\[S_{n,}=\{\,(,^{})( )^{2n}: h H_{},_{}^{ }(h|f)=0,_{^{}}^{}(h|f)\}.\]

_Then, we have \(((,^{}) S_{n,} f H_{}) ( R_{n,} f H_{})\), for \(n>\)._

The proof of Lemma 1 can be found in appendix A.3. With lemma 1, we just need to estimate \(((,^{}) S_{n,} f H_{})\). It seems more complicated to introduce the testing set \(^{}\) but we can swap training/testing instance pairs, which is a classic method in the proof of learnability, to refine the data distribution on \(\) into each single instance.

**Lemma 2**.: _On the same condition of theorem 2, we have_

\[((,^{}) S_{n,} f H_{ })(2n)^{d_{}}L^{2d_{}}(-).\]

The proof of Lemma 2 can be found in appendix A.4. With Lemma 1 and Lemma 2, we can prove Theorem 2, which is detailed in the appendix A.5.

In this section, we establish two essential properties of the soft labels. Classifier-consistency guarantees the effectiveness of the soft labels in a macroscopic perspective, and ERM learnability provides a microcosmic generalization bound for the student model \(h\). The corresponding threshold conditions are presented to ensure the student model can learn from the soft labels. The theory is applicable to all biased soft labels. This inspires us to consider that accuracy alone may not be a sufficient measure of a teacher's teaching ability. In the section 5.1, we design a simple heuristic algorithm to generate such skillful but bad teachers, i.e., teachers with low accuracy but can teach good students. These results are illustrative for the comprehension and development of the soft label based algorithms.

## 4 Biased Soft Labels in Weakly-Supervised Learning

The labels in weakly-supervised learning (WSL) could be incomplete, inexact, inaccurate  because accurate annotation is often expensive and difficult to obtain. Soft labels are widely used in WSL. Many weakly-supervised learning paradigms can be transformed into learning from soft labels. Due to the lack of the supervisory information, the soft labels could be large-biased but the model can still learn from them. In this section, our theory is adapted to three classic weakly-supervised learning paradigms and can provide theoretical guarantee for the learnability of these problems. These findings reflect that the theory is promising and extensible.

### Incomplete Supervision

In incomplete supervision, there are labeled data and unlabeled data. A common approach is to use the model to label the unlabeled data and then learn with all data iteratively. The soft labels of the unlabeled data are probably biased but make a significant contribution to the training process. We propose an ideal accuracy function and, based on it, we provide a theoretical analysis on incomplete supervision from the perspective of soft labels. Suppose there are \(N\) labeled data and \(M\) unlabeled data sampled from \(\). The predictive model \(h\) is an ERM learner on both labeled data and unlabeled data. The label of unlabeled data will be updated iteratively.

**Assumption 1**.: _For \(N\) labeled data and \(M\) unlabeled data whose soft labels have unreliability degree \(\) and ambiguity degree \(\), the ERM learner \(h\) has a deterministic accuracy funtion \((,)\), the probability that \(h\) predict correctly. The model architecture, data distribution and optimization are implicitly included in \((,)\)._

**Assumption 2**.: _Given that smaller values of \(\) or \(\) result in more supervised information in the soft labels, we assume \((,)\) decreases with \(\) and \(\)._

We suppose that the incorrect labels share the equal probability to appear in the top-\(k\) set. The distribution of the incorrect labels can be characterized in a more refined manner. For instance, assume that there is a upper bound of \()}{p(j|)},i,j,i,j y,i j\). We employ simplified assumptions because they can still capture and reflect this process. Based on the ideal \((,)\), we delineate the progressive performance of \(h\). Let \(_{t}\) denote the accuracy of \(h\) at epoch \(t\) and we have

\[_{t+1}(1-_{t},}{c-1}).\] (3)

In practice, as learner \(h\) learns from labeled data and unlabeled data, the performance of \(h\) will improve and the soft labels of the unlabeled data will be more effective. Consequently, \(h\) and the soft labels may achieve a dynamic equilibrium.

**Theorem 3**.: _Based on the ideal accuracy function \((,)\), with a moderate initial state \(_{0},_{0}\) satisfying Theorem 2, if final accuracy of \(_{}\) exists, it can be calculated by the following fixed point equation:_

\[x=(1-x,).\] (4)

_where \(k\) accords with the top-\(k\) set in \(\) and \(\), \(c\) is the number of class labels. If \((,)\) is \(k_{L}\)-Lipschitz continuous (\(k_{L}<1-\)), then \(_{}\) exists and is unique._

The proof is detailed in A.6. In fact, the deterministic \((,)\) is unattainable due to the indeterminacy of the optimization and the potential uncertainty in the data distribution. An intuitive extension is to assume \((,)\) is a probability distribution related to the training specifics, which can be further investigated. Theorem 3 is coarse yet in agreement with the general intuition. The model \(h\) improves as the soft labels envolve and finally reach the bottleneck restricted by the model, data and optimization.

In this section, we demonstrate the potential benefits of our theory in several classic weakly-supervised learning paradigms. There remain numerous domains associated with soft labels. It is essential to possess an appropriate theory to analyze the soft labels for the corresponding algorithms. Our theory can be instrumental for comprehending and constructing the soft label based algorithms.

### Partial Label Learning

In Partial Label Learning (PLL), each instace is typically assigned a set of possible labels, i.e., the candidate label set \(s\)[28; 46; 49]. The corresponding candidate label space is denoted by \(\), i.e., the non-empty power set of \(\). Traditional PLL assumes the ground-truth label must be in the candidate label set. But recently, Lv et al.  considers that ground-truth label could be not in the candidate label set, which is named as Unreliable Partial Label Learning (UPLL).

In UPLL, there are two basic concepts, partial rate \(\) and unreliable rate \(\). Partial rate \(\) is the ratio of incorrect labels in the candidate label set to total labels. A lower partial rate usually indicates a better performance of the model. Unreliable rate \(\) is the probability of ground-truth label \(y\) not in the candidate label set, which can be formally stated as

\[=_{(,y,s)}(y  s).\]

The discrete candidate label set \(s\) can be transformed into the soft label by

\[d_{i}=&i s\\ 0&i s,\]

where \(|s|\) is the cardinality of set \(s\). So PLL also can be viewed as learning from biased soft labels. Then we have the following corollary.

**Corollary 1**.: _For UPLL with partial rate \(\) and unreliable rate \(\), regard the generated soft labels as teacher. Then, we have \(=\) and \(=\). With the same conditions in Theorem 2, UPLL is ERM learnable and the sample complexity remains unchanged._

This corollary show that UPLL is ERM learnable under a moderate condition. We provide new insights from the perspective of soft labels.

### Learning with Additive Noise

Additive noise mechanism [30; 14] is an important methodology for differential privacy. Specifically, Laplace noise or Gaussian noise is added to data for protecting privacy. The privacy budget can be controlled by adjusting the scale of the noise. After normalization, the noisy labels are also biased soft labels in nature. In fact, given the probability density function of noise, we can calculate the corresponding unreliability degree and ambiguity degree in order to measure the effectiveness of the noisy labels. With the biased soft labels, the task is to train a utility model with strong privacy guarantees. Our theory can guarantee the utility of such soft labels.

To calculate unreliability degree and ambiguity degree, we refer to _order statistic_. Order statistic analyze the \(i\)th-smallest value of random samples from a continuous distribution. We denote the _order distribution_\(Order(d,n,i)\) as the \(i\)th-smallest value of \(n\) samples from distribution \(d\). The software Mathematics  provide an efficient API for estimating the order distribution.

**Corollary 2**.: _Let \(d\) denote the noise distribution (e.g. Laplace noise and Gaussian noise) and regard the noisy soft labels as the teacher. With the \(k\) in Eq.1 and the total classes \(c\), for \(k c-1\), we can compute the \(\) and \(\) as_

\[=_{x Order(d,e-1,n-k+1)\\ y d}(1+y>x),\]

\[=.\]

_With the same conditions in Theorem 2, the problem learning with additive noise is ERM learnable and the sample complexity remains unchanged._As the scale of the noise increases, \(\) and \(\) will increase, i.e., the effectiveness of the soft labels will decrease. This result agrees with the practical situation.

## 5 Experiments

Our experimentally investigate _whether biased soft labels are still effective_ and _whether the proposed indicators can measure the effectiveness of these soft labels_, which consist of three parts. Firstly, we design a simple heuristic algorithm to generate Skillful but Bad Teachers (SBTs), which have low accuracy (less than 35%) but can teach good students. Secondly, experimental results demonstrate that students learning from SBTs can achieve comparable performance as models trained on the original data. We can explain these phenomenons with unreliability degree and ambiguity degree. Thirdly, we conduct experiments in weakly-supervised learning paradigms, and the results also confirm that unreliability degree and ambiguity degree can reflect the effectiveness of the biased soft labels. Due to space limitations, we present a part of experimental results in the appendix. Additionally, we provide the details of the experiment setup in appendix A.7.

### Skillful but Bad Teachers

In this subsection, we introduced the design of the SBTs. The intuition behind SBTs, inspired by Theorem 2, is to inhibit correct predictions and reduces the unreliability degree and ambiguity degree simultaneously. So we design some heuristic loss functions and have some hyperparameters that qualitatively control the unreliability degree and ambiguity degree. More specifically, the designed loss functions are intended to keep the ground-truth label within the top-\(k\) set of the soft labels, without necessarily requiring it to be at the top. Here, \(k\) is an empirical constant, which we have set to 3 or 4 in training SBTs.

Firstly, SBTs will punish those correctly predicted instances as

\[_{}(,y)=-(*{argmax}_{j }(_{j})=y)(f(),y),\] (5)

Figure 2: Indicators of soft labels generated by SBTs on CIFAR-10 (a), CIFAR-100 (b) and Tiny ImageNet (c). Acc represents the accuracy of the student model trained with the biased soft labels. Unreliability degree \(_{k}(f)\) and ambiguity degree \(_{k}(f)\) are the two indicators proposed in our study.

where \((,)\) is the cross entropy loss function. In practice, the value of the ground-truth label decreases significantly, which causes the ground-truth label to fall out of the top-\(k\) set, i.e., large \(_{k}\). So the ground-truth label \(y\) is compensated when \(y_{k}(f())\):

\[_{}(,y)=(y_{k}(f())) (f(),y).\] (6)

The compensation term is designed to improve the top-\(k\) accuracy of SBTs, which keeps the statistical effectiveness of the soft labels generated by SBTs. In practice, we found there was a strong correlation among the top-\(k\) labels, which leaded to the confusion between the ground-truth label and similar labels, i.e., large \(_{k}\). To decrease this correlation, we propose an effective method to make the labels in \(_{k}(f())\) as independent as possible. We randomly select \(k-1\) labels except the ground-truth label \(y\). Then the selected \(k-1\) labels are employed as the learning objectives:

\[_{}(,y)=(f(),s_{}).\] (7)

where \(s_{}\) is the set of the \(k-1\) random labels excluding \(y\). Consequently, the total objective of SBTs is as follows:

\[(,y)=_{}(,y)+_{1}_{}(,y)+_{2}_{}(,y)+ _{3}_{}(,y)\] (8)

where \(_{}(,y)\) is the vanilla cross-entropy loss between the output of the model and the ground-truth label, and \(_{1},_{2}\), \(_{3}\) are the trade-off parameters.

### Effectiveness of the Proposed Indicators

In this subsection, our aim is to demonstrate that, despite the soft labels generated by SBTs are large-biased, students still achieve high accuracy. As outlined in Table 2 of the appendix, we adapt four different classic metrics: Chebyshev distance, KL divergence, Manhattan distance, and Euclidean distance. These metrics are employed to measure the discrepancies between the large-biased soft labels generated by SBTs and the ground-truth labels. The soft labels generated by SBTs exhibit substantial differences when compared to those from normally trained teachers. Nevertheless, they remain effective in instructing good students. It should be noted that the accuracy of the students mirrors the effectiveness of the soft labels. However, accuracy is an indirect measure contingent on the student model. The proposed unreliability degree and ambiguity degree can directly measure the effectiveness of these soft labels. The smaller the unreliability degree and ambiguity degree, the more effective the soft labels, and subsequently, the students' accuracy tends to be higher. This phenomenon is illustrated in Figure 2, which agrees with our expectations.

In the experiments, we set \(k=4\) for unreliability degree \(_{k}(f)\) and ambiguity degree \(_{k}(f)\) (\(k\) in the indicators can be different from \(k\) in training SBTs). Since \(_{k}(f)\) and \(_{k}(f)\) are probabilities over the entire data distribution \(\) and difficult to compute, we estimate them empirically by the the generated soft labels on the test data. Note that training with the ground-truth labels can achieve accuracy 95.29% on CIFAR-10, 78.13% on CIFAR-100 and 72.53% on Tiny ImageNet. We can find many interesting results in Figure 2:

* When these soft labels are generated by SBTs with accuracy less than 30%, which means they are quite different from the ground-truth labels, the students can still achieve accuracy much higher than the teachers.

Figure 3: Indicators of learning with Gaussian noise on CIFAR-10 (a) and CIFAR-100 (b).

* As \(_{k}(f)\) and \(_{k}(f)\) decrease, Acc (i.e., accuracy of the student) increases. The proposed indicators exhibit an inverse correlation with the students' accuracy, which reflects the effectiveness of the proposed indicators.
* For the more complicated dataset like CIFAR-100 and Tiny ImageNet, accuracy (Acc) is more sensitive to \(_{k}(f)\) and \(_{k}(f)\). This implies that, for more complex tasks, the quality of the annotation is more crucial.

The hyperparameters in Eq.(8) are pivotal in controlling the indicators of these soft labels, which is elucidated in A.8. Furthermore, we conducted experiments to investigate whether the effectiveness of soft labels is influenced by different model backbones, as detailed in A.9. We tested a range of architectures, including wideresnet 28x2, 28x4, 40x2, and 40x4. The four distinct backbones exhibited consistent patterns, indicating that the proposed indicators are effective across different backbones. In addition, we present the overall distribution of these soft labels in A.10. The experimental results demonstrate that _biased soft labels can also teach good students_ and _the proposed indicators can measure the effectiveness of these soft labels_, which confirm the validity of our theory.

### Biased Soft Labels in Weakly-Supervised Learning

The aforementioned weakly-supervised learning paradigms have engendered a lot of specialized algorithms. The intention of this paper is not to devise more efficacious algorithms, but to evaluate the effectiveness of the soft labels in the weakly-supervised learning paradigms by unreliability degree and ambiguity degree. The results of partial label learning are shown in Table 1, while the results of learning with additive noise are shown in Figure 3. Due to space limitations, the results of incomplete supervision are shown in A.11. From the three experiments, we can observe that the accuracy of the students decrease when unreliability degree and ambiguity degree increase. All the results reflect that these indicators can measure the effectiveness of the soft labels well, which is consistent with our theory. In addition, since the soft labels in partial label learning and learning with additive noise are generated by a mapping rule rather than a neural network, the results of these experiments are more stable and smooth.

## 6 Conclusion

In this paper, we find that even large-biased soft labels can teach a good student and focus on the effectiveness of the biased soft labels. It motivates us to rethink when the biased soft labels (or teachers) are effective. We propose two indicators, unreliability degree and ambiguity degree, to measure the effectiveness of soft labels. Based on the proposed indicators, we provide moderate conditions that guarantee the classifier-consistency and ERM learnability of the biased soft label learning problem. Our theoretical framework can be adapted to elucidate the effectiveness of soft labels in three weakly-supervised learning paradigms, incomplete supervision, partial label learning and learning with additive noise. We design a heuristic method to train Skillful but Bad Teachers (SBTs), which validate that large-biased soft labels can teach good students. Besides, the effectiveness of both soft labels generated by SBTs and soft labels in the weakly-supervised learning paradigms can be measured by the proposed indicators well, which is consistent with our theory.

  Dataset & \(_{k}(f)\) & \(_{k}(f)\) & Student \\   & \(0.1\) & \(0.1\) & \(\) \\  & \(0.1\) & \(0.3\) & \(\) \\  & \(0.1\) & \(0.5\) & \(\) \\   & \(0.3\) & \(0.1\) & \(\) \\  & \(0.3\) & \(0.3\) & \(\) \\  & \(0.3\) & \(0.5\) & \(\) \\   & \(0.5\) & \(0.1\) & \(\) \\  & \(0.5\) & \(0.3\) & \(\) \\  & \(0.5\) & \(0.5\) & \(\) \\    & \(0.01\) & \(0.1\) & \(\) \\  & \(0.01\) & \(0.3\) & \(\) \\   & \(0.01\) & \(0.5\) & \(\) \\    & \(0.05\) & \(0.1\) & \(\) \\   & \(0.05\) & \(0.3\) & \(\) \\   & \(0.05\) & \(0.5\) & \(\) \\    & \(0.1\) & \(0.1\) & \(\) \\   & \(0.1\) & \(0.3\) & \(\) \\   & \(0.1\) & \(0.5\) & \(\) \\   

Table 1: Students learning from partial labels.