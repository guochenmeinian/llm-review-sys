# Understanding Mode Connectivity via Parameter Space Symmetry

Bo Zhao

University of California San Diego

bozhao@ucsd.edu

&Nima Dehmamy

IBM Research

nima.dehmamy@ibm.com

Robin Walters

Northeastern University

r.walters@northeastern.edu

&Rose Yu

University of California San Diego

roseyu@ucsd.edu

###### Abstract

It has been observed that the global minimum of neural networks is connected by curves on which train and test loss is almost constant. This phenomenon, often referred to as mode connectivity, has inspired various applications such as model ensembling and fine-tuning. Despite empirical evidence, a theoretical explanation is still lacking. We explore the connectedness of minimum through a new approach, parameter space symmetry. By relating topology of symmetry groups to topology of minima, we provide the number of connected components of full-rank linear networks. In particular, we show that skip connections reduce the number of connected components. We then prove mode connectivity up to permutation for linear networks. We also provide explicit expressions for connecting curves in minimum induced by symmetry.

## 1 Introduction

Among recent studies on the loss landscape, a particularly interesting discovery is mode connectivity [5; 10], which refers to the phenomenon that distinct minima found by stochastic gradient descent (SGD) can be connected by continuous paths through the high-dimensional parameter space of neural networks. Mode connectivity has implications on other phenomena in deep learning such as the lottery ticket hypothesis  and loss landscape and training trajectory analysis . Additionally, mode connectivity has inspired applications in diverse fields, including model ensembling [10; 2; 3], model averaging [15; 30], pruning , improving adversarial robustness , and fine-tuning for altering prediction mechanism .

Discrete symmetry, especially permutation, is well-known to be related to mode connectivity. In particular, the neural network output is invariant to permuting the neurons .  conjectures that all minima found by SGD are linearly connected up to permutation. Various algorithms have since been developed to find the optimal permutation for linear mode connectivity . However, compared to discrete symmetry, the role of continuous symmetry remains less studied. Continuous symmetry groups with continuous actions define positive dimensional connected spaces in the minimum . We explore the connectedness of minimum through continuous symmetries in the parameter space.

We reveal the role of symmetry in the connectivity of minimum by relating properties of topological groups to their orbits and the minimum. Our results show that both continuous and discrete symmetry are important and useful in understanding the origin and failure cases of mode connectivity. Our work highlights a new approach towards understanding the topology of the minimum and complements previous theories on mode connectivity [31; 9; 23; 24; 18; 27; 25].

## 2 Connectedness of minima

### Linear network with invertible weights

Let **Param** be the space of parameters. Consider the multi-layer loss function \(L:\),

\[L:, 56.905512pt(W_{1},...,W_{l}) ||Y-W_{l}...W_{1}X||_{2}^{2}.\] (1)

where \(X,Y^{h h}\) are the input and output of the network. In this subsection, we assume that both \(X,Y\) have rank \(h\), and \(=(^{h h})^{l}\). Then \(L\) has a \(GL_{h}()^{l-1}\) symmetry, which acts on **Param** by \(g(W_{1},...,W_{l})=(g_{1}W_{1},g_{2}W_{2}g_{1}^{-1},...,g_{l-1}W_{l-1}g_{ l-2}^{-1},W_{l}g_{l-1}^{-1})\), for \((g_{1},...,g_{l-1}) GL_{h}()^{l-1}\).

Let \(L^{-1}(c)=\{:L()=c\}\) be a level set of \(L\). Since \(\|\|_{2} 0\) and \(L^{-1}(0)\), the minimum value of \(L\) is 0. By relating the topology of \(GL()\) and \(L^{-1}(0)\), we have the following observations on the structure of the minimum of \(L\).

**Proposition 2.1**.: _There is a homeomorphism between \(L^{-1}(0)\) and \((_{h})^{l-1}\)._

Since \((_{h})^{l-1}\) has \(2^{l-1}\) connected components and homeomorphism preserves topological properties, \(L^{-1}(0)\) also has \(2^{l-1}\) connected components.

**Corollary 2.2**.: _The minimum of \(L\) has \(2^{l-1}\) connected components._

### ResNet with 1D weights

The topological properties of the minimum depend on the architecture. As an example of this dependency, we show that adding a skip connection changes the number of connected components of the minimum.

Consider a residual network \(W_{3}(W_{2}W_{1}X+ X)\) and loss function

\[L(W_{3},W_{2},W_{1})=||Y-W_{3}(W_{2}W_{1}X+ X)||_{2},\] (2)

where \((W_{1},W_{2},W_{3})=^{n n}^ {n n}^{n n}\), \(\), and data \(X^{n n},Y R^{n n}\). The following proposition states that for a three-layer residual network with weight matrices of dimension \(1 1\), the number of components of the minimum is smaller than that of a linear network without the skip connection.

**Proposition 2.3**.: _Let \(n=1\). Assume that \(X,Y 0\). When \(=0\), the minimum of \(L\) has 4 connected components. When \( 0\), the minimum of \(L\) has 3 connected components._

The \(=0\) case follows from Corollary 2.2. For the \( 0\) case, the proof decomposes the minimum of \(L\) into two sets \(S_{1}\) and \(S_{0}\), corresponding to the minima without the skip connection and an extra set of solutions because of the skip connection. \(S_{1}\) is homeomorphic to \(GL_{1} GL_{1}\) and has 4 connected components. \(S_{0}\) is a line and has 1 connected component. Two components of \(S_{1}\) are connected to \(S_{0}\), while the other two components of \(S_{1}\) are not. Therefore, \(S_{0}\) connects two components of \(S_{1}\). As a result, the minimum of \(L\) has 3 connected components. Full proof can be found in Appendix C.3.

Figure 1 visualizes the minimum without and with the skip connection. This result reveals the effect of skip connection on the connectedness of minimum, which may lead to a new explanation of the effectiveness of ResNets  and DenseNets . We leave the connection between the topology of minimum and the optimization and generalization property of neural networks to future work.

## 3 Mode connectivity

From the examples in the previous section, the connectedness of the minimum is related to the symmetry of the loss function under certain conditions. In this section, we explore applications of this insight in explaining mode connectivity.

### Mode connectivity up to permutation

For the family of linear neural networks defined in Section 2.1, we show that permutation allows us to connect points in the minimum that are not connected without permutation. Our results support the empirical observation that neuron alignment by permutation improves mode connectivity .

Consider again the linear network (1) with full rank weights. When \(l=2\), the minimum of \(L\) has 2 connected components. Any \(g GL\) that is not on the identity component can take a point on one connected component of the minimum to the other.

**Lemma 3.1**.: _Consider two points \((W_{1},W_{2}),(W_{1}^{},W_{2}^{}) L^{-1}(0)\) that are not connected in \(L^{-1}(0)\). For any \(g GL(h)\) such that \(det(g)<0\), \(g(W_{1},W_{2})\) and \((W_{1}^{},W_{2}^{})\) are connected in \(L^{-1}(0)\)._

When the hidden dimension \(h 2\), there exists a permutation \(g\) such that \(det(g)>0\), and a permutation \(g\) such that \(det(g)<0\). Therefore, all points on the minimum of \(L\) are connected up to permutation.

**Proposition 3.2**.: _Assume that \(h 2\). For all \((W_{1},...,W_{l}),(W_{1}^{},...,W_{l}^{}) L^{-1}(0)\), these exists a list of permutation matrices \(P_{1},...,P_{l-1}\) such that \((W_{1}P_{1},P_{1}^{-1}W_{2}P_{2},...,P_{l-2}W_{l-1}P_{l-1},P_{l-1}W_{l})\) and \((W_{1}^{},...,W_{l}^{})\) are connected in \(L^{-1}(0)\)._

### Failure case of linear mode connectivity

In addition to helping show the connectedness of minimum, symmetry relates the set of points in minimum to groups and provides a way to find all points in the minimum. As an application, we show that linear mode connectivity fails to hold in multi-layer regressions. The following proposition says that in two-layer full-rank linear networks, the error barrier in the linear interpolation between two solutions can be arbitrarily large.

**Proposition 3.3**.: _Consider the setting in Section 2.1. For any \(k>0\), there exist \((W_{1},W_{2}),(W_{1}^{},W_{2}^{}) L^{-1}(0)\) that belong to the same connected component of \(L^{-1}(0)\) and \(0<<1\), such that \(L((1-)W_{1}+ W_{1}^{},(1-)W_{2}+ W_{2}^{})>k\)._

The result holds when there is a homogeneous activation (\((cz)=c^{}(z)\)). However, when \( 1\), the proof needs a different choice of \(m\). Figure 2 visualizes the two points on the minimum of a two-layer network with weights of dimension \(1 1\) and the linear interpolation between them. One possible reason why linear mode connectivity is observed in practice is that only a small part of the minima is reachable by SGD due to implicit bias .

Figure 1: Minimum of (a) 3-layer linear net \(||Y-W_{3}W_{2}W_{1}X||_{2}\) and (b) 3-layer linear net with a residual connection \(||Y-W_{3}(W_{2}W_{1}X+X)||_{2}\), where \(X=1\), \(Y=1\), and \(W_{1},W_{2},W_{3}\).

Figure 2: Interpolation between 2 minima of loss function \(||Y-W_{2}W_{1}X||_{2}\) with 1 dimensional weights. Loss on the interpolation can be unbounded.

Curves on minimum from group actions

The minima of overparametrized ReLU networks consist of affine subspaces . With activations that are not piecewise linear, the minimum may be curved. As a result, the paths connecting two points in the minimum may not be linear. Previously, these paths are discovered empirically by finding parametric curves on which the expected loss is minimized . An alternative and principled way to find curves on the minima is to use parameter space symmetry.

Suppose the loss function \(L:\) admits a \(G\) symmetry. Consider the following curve for a point \(\) and \(M(G)\):

\[_{M}: ,\] \[_{M}(t,) =.\] (3)

Since \( G\) and the action of \(G\) preserves the value of \(L\), every point on \(_{M}\) is in the same \(L\) level set as \(\). This provides a way to find a curve of constant loss between two points that are in the same orbit. Concretely, given two points \(_{1}\) and \(_{2}=g_{1}\), let \(\) be the following curve:

\[: G ,\] \[(t,g,) =.\] (4)

Note that \((0,g,_{1})=_{1}\), \((1,g,_{1})=_{2}\), and \(L((t,g,_{1}))=L(_{1})=L(_{2})\) for all \(t\). Hence, \(\) is a curve that connects the points \(_{1}\) and \(_{2}\), and every point on \(\) has the same loss value as \(L(_{1})=L(_{2})\).

For a group \(G\), the curve \(\) is defined when the map \(:G\) is continuous and \(=\) for all \(\), even if it is not a group action or does not preserve loss. However, when \(\) does not preserve loss, the loss can change on \(\). Consider our two-layer network and the following map:

\[:GL(h,) \] \[g(U,V) =(U(VX)(gVX)^{},gV).\] (5)

When \(\) is the identity function, \(\) preserves the loss value, and \(\) defines a curve on the minimum. In general, the map (5) does not preserve loss when batch size \(k\) is larger than hidden dimension \(h\). However, the maximum change of loss on \(\) can be bounded as follows. Let \(U^{},V^{}=g(U,V)\). We have

\[\|U(VX)-U^{}(V^{}X)\|=\|U(VX)(I-(gVX )^{}(gVX))\|\|U(VX)\|.\] (6)

The last steps follows from the fact that \((gVX)^{}(gVX)\) is a projection.

## 5 Discussion

In this work, we study topological properties of the loss level sets by relating their topology to the topology of symmetry groups. We derive the number of connected components of full-rank multi-layer networks with and without skip connections, and prove mode connectivity up to permutation for full-rank linear regressions. Using symmetry in the parameter space, we construct an explicit expression for curves that connect two points in the same orbit.

While symmetry appears to be a useful tool for studying the loss landscape, our current results rely on the existence of a homeomorphism between symmetry groups and the minimum. A future direction is to explore the possibility of removing this assumption. Another interesting direction is to investigate additional links between different architecture choices, such as normalization, and connectedness of the minimum. On the application side, the impact of these results can benefit from further study on the connection between the topology of minimum and generalization ability of neural networks.

The connectedness results obtained from symmetry raise a number of interesting questions related to mode connectivity. For example, it would be interesting to understand when and why there is no significant change in loss on the linear interpolation between two minima. One possible explanation is that there always exists a \(\) defined in the way above that is close to the line formed by the linear interpolation. Another possible reason is that the dimension of minimum is usually high, and a significant part of the linear interpolation is within the minimum with high probability. Moreover, it has been observed that the train and test accuracy are both near constant on the paths that connect different SGD solutions . If these paths correspond to a group action, this implies that the action's dependence on data is weak.