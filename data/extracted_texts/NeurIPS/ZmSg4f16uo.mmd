# Optimistic Meta-Gradients

Sebastian Flennerhag

Google DeepMind

flennerhag@google.com

&Tom Zahavy

Google DeepMind

&Brendan O'Donoghue

Google DeepMind

&Hado van Hasselt

Google DeepMind&Andras Gyorgy

Google DeepMind&Satinder Singh

Google DeepMind

###### Abstract

We study the connection between gradient-based meta-learning and convex optimisation. We observe that gradient descent with momentum is a special case of meta-gradients, and building on recent results in optimisation, we prove convergence rates for meta-learning in the single task setting. While a meta-learned update rule can yield faster convergence up to constant factor, it is not sufficient for acceleration. Instead, some form of optimism is required. We show that optimism in meta-learning can be captured through the recently proposed Bootstrapped Meta-Gradient  method, providing deeper insight into its underlying mechanics.

## 1 Introduction

In meta-learning, a learner is using a parameterised algorithm to adapt to a given task. The parameters of the algorithm are then meta-learned by evaluating the learner's resulting performance . As such, meta-learning features a complex interaction between the learner and the meta-learner. The **learner's problem** is to minimize the expected loss \(f\) of a stochastic objective by adapting its parameters \(x^{n}\). The learner has an update rule \(\) at its disposal that generates new parameters \(x_{t}=x_{t-1}+(x_{t-1},w_{t})\); we suppress data dependence to simplify notation. A simple example is when \(\) represents gradient descent with \(w_{t}=\) its step size, that is \((x_{t-1},)=- f(x_{t-1})\); several works have explored meta-learning other aspects of a gradient-based update rule . \(\) need not be limited to a gradient-based update, it can represent some algorithm implemented within a Neural Network .

**The meta-learner's problem** is to optimise the meta-parameters \(w_{t}\) to yield effective updates. In a typical (gradient-based) meta-learning setting, it does so by treating \(x_{t}\) as a function of \(w\). Let \(h_{t}\), defined by \(h_{t}(w)=f(x_{t-1}+(x_{t-1},w))\), denote the learner's post-update performance as a function of \(w\). The learner and the meta-learner co-evolve according to

\[x_{t}=x_{t-1}+(x_{t-1},w_{t}), w_{t+1}=w_{t}- h_{t}(w_{t})=w _{t}-D(x_{t-1},w_{t})^{T} f(x_{t}),\]

Figure 1: ImageNet. We compare training a 50-layer ResNet using SGD against variants that tune an element-wise learning rate online using standard meta-learning or optimistic meta-learning. Shading depicts 95% confidence intervals over 3 seeds.

where \(D(x,w)\) denotes the Jacobian of \(\) with respect to \(w\). The nested structure between these two updates makes it challenging to analyse meta-learning, in particular it depends heavily on the properties of the Jacobian. In practice, \(\) is highly complex and so \(D\) is almost always intractable. For this reason, the only theoretical results we are aware of specialise to the multi-task setting, where the learner must adapt to a new task \(f_{t}\). Acceleration in this setup is driven by the tasks similarity. That is, if all tasks are sufficiently similar, a meta-learned update can accelerate convergence . However, these results do not yield acceleration in the absence of a task distribution to the best of our knowledge.

This paper provides an alternative view. We study the classical convex optimisation setting of approximating the minimiser \(_{x}f(x)\). We observe that setting the update rule equal to the gradient, i.e. \(:(x,w) w f(x)\), recovers gradient descent. Similarly, we show in Section 4 that \(\) can be chosen to recover gradient descent with momentum. This offers another view of meta-learning as a non-linear transformation of classical optimisation. An implication thereof is that task similarity is not necessary condition for improving the rate of convergence via meta-learning. While there is ample empirical evidence to that effect [30; 31; 9; 16], we are only aware of theoretical results in the special case of meta-learned step sizes [17; 26].

Given a function \(f\) that is convex with Lipschitz smooth gradients, meta-learning improves the rate of convergence by a multiplicative factor of \(\) to \(O(/T)\) via the smoothness of the update rule. To achieve accelerated convergence, \(O(1/T^{2})\), some form of _optimism_ is required, typically in the form of a prediction of the next gradient. We consider optimism with meta-learning in the convex setting and prove accelerated rates of convergence, \(O(/T^{2})\). Again, meta-learning affects these bounds by a multiplicative factor. Our main contributions are as follows:

1. We show that meta-gradients contain gradient descent with momentum (Heavy Ball ; Section 4) and Nesterov Acceleration  as special cases (Section 5).
2. We show that gradient-based meta-learning can be understood as a non-linear transformation of an underlying optimisation method (Section 4).
3. We establish rates of convergence for meta-learning in the convex setting (Section 4).
4. We show that optimism can be expressed through the recently proposed Bootstraped Meta-Gradient method [BMG; 9]. Our analysis provides a first proof of convergence for BMG and highlights the underlying mechanics that enable faster learning with BMG (Section 6).

## 2 Meta-Learning as Convex Optimisation

Problem definition.This section defines the problem studied in this paper and introduces our notation (see Appendix A, Table 1). Let \(f:\) be a proper and convex function. The problem of interest is to approximate the global minimum \(_{x}f(x)\). We assume a global minimiser exists and is unique, defined by

\[x^{*}=*{arg\,min}_{x}\;f(x).\] (1)

We assume that \(^{n}\) is a closed, convex and non-empty set. \(f\) is differentiable and has Lipschitz smooth gradients with respect to a norm \(\|\|\), meaning that there exists \(L(0,)\) such that \(\| f(x)- f(y)\|_{*} L\|x-y\|\) for all \(x,y\), where \(\|\|_{*}\) is the dual norm of \(\|\|\). We consider the noiseless setting for simplicity; our results carry over to the stochastic setting by replacing the key online-to-batch bound used in our analysis by its stochastic counterpart .

Algorithm.Let \([T]=\{1,2,,T\}\). We are given weights \(\{_{t}\}_{t=1}^{T}\), each \(_{t}>0\), and an initialisation \((_{0},w_{1})\). At each time \(t[T]\), an update rule \(:\) generates the update \(x_{t}=(_{t-1},w_{t})\), where \(^{m}\) is closed, convex, and non-empty. We discuss \(\) momentarily. The algorithm maintains the online average

\[_{t}=}{_{1:t}}=(1-_{t})_{t-1}+_{t}x_ {t},\] (2)

where \(x_{1:t}=_{s=1}^{t}_{s}x_{s}\), \(_{1:t}=_{s=1}^{t}_{s}\), and \(_{t}=_{t}/_{1:t}\). Our goal is to establish conditions under which \(\{_{t}\}_{t=1}^{T}\) converges to the minimiser \(x^{*}\). While this moving average is not always used in practical applications, it is required for accelerated rates in online-to-batch conversion [27; 3; 13].

Convergence depends on how meta-parameters \(w_{t}\) are chosen. The meta-learner faces a sequence of losses \(h_{t}:\) defined by the composition \(h_{t}(w)=f((1-_{t})_{t-1}+_{t}(_{t-1},w))\). As such, the meta-learner is facing an online optimization, which we model under Follow-The-Regularized-Leader (FTRL; reviewed in Section 3): given \(w_{0}\), each \(w_{t}\) is chosen according to

\[w_{t+1}=*{arg\,min}_{w}\,(_{s=1}^{t} _{s} h_{s}(w_{s}),w+\|w\|^{2}).\] (3)

Note that Eq. 3 subsumes the standard meta-gradient; if \(\|\|\) is the Euclidean norm, an interior solution yields \(w_{t+1}=w_{t}-_{t} h_{t}(w_{t})\). It is straightforward to extend Eq. 3 to account for meta-updates that use AdaGrad-like  acceleration by altering the norms (see ).

Update rule.It is not possible to prove convergence outside of the convex setting, since \(\) may reach a local minimum, where local changes to \(w\) do not yield better updates in \(x\), yet the \(x\) sequence is not converging. Convexity means that each \(h_{t}\) must be convex, which requires that \(\) is affine in \(w\) (but may vary non-linearly in \(x\)). We also assume that \(\) is smooth with respect to \(\|\|\), in the sense that it has bounded norm; for all \(x\) and all \(w\) we assume that there exists \((0,)\) for which

\[\|D(x,w)^{T} f(x)\|_{*}^{2}\| f(x)\|_{*}^{2}.\]

Limitations.Our analysis makes relatively strict assumptions. Most meta-learning systems are not affine in \(w\). A notable case where our assumptions to hold is meta-learned step-sizes or preconditioning matrices. For other update rules, our analysis holds up to first-order Taylor approximation error. We carry out experiments in Section 7 to empirically verify our theoretical insights.

## 3 Preliminaries: Online Convex Optimisation

In this section, we present analytical tools from the optimisation literature that we build upon. In a standard optimisation setting, there is no update rule \(\); instead, the iterates \(x_{t}\) are generated by a gradient-based algorithm, akin to Eq. 3. Our problem setting reduces to standard optimisation if \(\) is defined by \(:(x,w) w\), in which case \(x_{t}=w_{t}\). In this paper, we use batch-to-online conversion as our analytical tool. This strategy treats the iterates \(x_{1},x_{2},\) as generated by an online learning algorithm, for which we can obtain a regret bound. This regret bound can then be turned into a convergence rate, detailed momentarily.

Online Optimisation.In online convex optimisation , a learner is given a convex decision set \(\) and faces a sequence of convex loss functions \(\{_{t}f_{t}\}_{t=1}^{T}\). At each time \(t[T]\), it must make a prediction \(u_{t}\) prior to observing \(_{t}f_{t}\), after which it incurs a loss \(_{t}f_{t}(u_{t})\) and receives a signal--either \(_{t}f_{t}\) itself or a (sub-)gradient of \(_{t}f_{t}(u_{t})\). The learner's goal is to minimise _regret_, \(R(T)_{t=1}^{T}_{t}(f_{t}(u_{t})-f_{t}(u))\), against a comparator \(u\). An important property of a convex function \(f\) is \(f(u^{})-f(u) f(u^{}),u^{}-u\). Hence, the regret is largest under linear losses: \(_{t=1}^{T}_{t}(f_{t}(u_{t})-f_{t}(u))_{t=1}^{T}_{t}(  f_{t}(u_{t}),u_{t}-u)\). For this reason, it is sufficient to consider regret under linear loss functions. An algorithm has sublinear regret if \(_{T}R(T)/T=0\).

Ftrl & Ao-ftrl.The meta-update in Eq. 3 is an instance of Follow-The-Regularised-Leader (FTRL) under linear losses. In Section 6, we show that BMG is an instance of the Adaptive-OptimisticFTRL (AO-FTRL) [24; 19; 13; 28]. In AO-FTRL, we have a strongly convex regulariser \(\|\|^{2}\). AO-FTRL sets the first prediction \(u_{1}\) to minimise \(\|\|^{2}\). Given linear losses \(\{g_{s}\}_{s=1}^{t-1}\) and learning rates \(\{_{t}\}_{t=1}^{T}\), each \(_{t}>0\), the algorithm proceeds according to

\[u_{t}=*{arg\,min}_{u}\,(_{t} _{t},u+_{s=1}^{t-1}_{s} g_{s},u+}\|u\|^{2}),\] (4)

where each \(_{t}\) is a "hint" that enables optimistic learning [24; 19]; setting \(_{t}=0\) recovers the original FTRL algorithm. The goal of a hint is to predict the next loss vector \(g_{t}\); if the predictions are accurate AO-FTRL can achieve lower regret than its non-optimistic counter-part. Since \(\|\|^{2}\) is strongly convex, FTRL is well defined in the sense that the minimiser exists, is unique, and finite . The regret of FTRL and AO-FTRL against any comparator \(u\) can be upper-bounded by

\[R(T)=_{t=1}^{T}_{t} g_{t},u_{t}-u} {2_{T}}+_{t=1}^{T}_{t}^{2}_{t}\|g_{t}- _{t}\|_{*}^{2}.\] (5)

Hence, hints that predict \(g_{t}\) well can reduce the regret substantially. Without hints, FTRL can guarantee \(O()\) regret (for non strongly convex loss functions). However,  show that under linear losses, if hints are weakly positively correlated--defined as \( g_{t},_{t}\|g_{t}\|^{2}\) for some \(>0\)--then the regret guarantee improves to \(O( T)\), even for non strongly-convex loss functions. We believe optimism provides an exciting opportunity for novel forms of meta-learning. Finally, we note that these regret bounds (and hence our analysis) can be extended to stochastic optimisation [19; 12].

Online-to-batch conversion.The main idea behind online to batch conversion is that, for \(f\) convex, Jensen's inequality gives \(f(_{T})-f(x^{*})_{t=1}^{T}_{t} f(x_{t}),x_ {t}-x^{*}/_{1:T}\). That is, the sub-optimality gap of the average iterate \(_{T}\) can be bounded by the regret in the sequence \(x_{1},x_{2},,x_{T}\). Applying this bound naively yields \(O(1/T)\) rate of convergence. In recent work,  shows that one achieve tighter bounds by instead querying the gradient at the _average_ iterate, \(f(_{T})-f(x^{*})_{t=1}^{T}_{t} f(_{t }),x_{t}-x^{*}/_{1:T}\). A tighter bound means a faster rate of convergence. Recently,  tightened the analysis further and proved that the sub-optimality gap can be bounded by

\[& f(_{T})-f(x^{*})\\ &}(R^{x}(T)-}{2L}\|  f(_{t})- f(x^{*})\|_{*}^{2}-}{2L}\|  f(_{t-1})- f(_{t})\|_{*}^{2}),\] (6)

were we define \(R^{x}(T)_{t=1}^{T}_{t} f(_{t}),x_{t}- x^{*}\) as the regret of the sequence \(\{x_{t}\}_{t=1}^{T}\) against the comparator \(x^{*}\). With this machinery in place, we now turn to deriving our main results.

## 4 Meta-Gradients without Optimism

The main difference between classical optimisation and meta-learning is the introduction of the update rule \(\). To see how this acts on optimisation, consider two special cases. If the update rule just return the gradient, \(= f\), Eq. 3 reduces to gradient descent (with averaging). This inductive bias is fixed and does not change with experience, so acceleration is not possible: the rate of convergence is \(O(1/)\). The other extreme is an update rule that only depends on the meta-parameters, \((x,w)=w\). Here, the meta-learner has ultimate control and selects the next update without constraints. The only relevant inductive bias is contained in \(w\). To see how this inductive bias is formed, suppose \(\|\|=\|\|_{2}\) so that Eq. 3 yields \(w_{t+1}=w_{t}-_{t}_{t} f(_{t})\) (assuming an interior solution). Combining this with the moving average in Eq. 2, we may write the learner's iterates as

\[_{t}=_{t-1}+_{t}(_{t-1}-_{t-2} )-_{t} f(_{t-1}),\]

where each \(_{t}=_{t}}{_{t-1}}\) and \(_{t}=_{t}_{t}\). Setting \(=1/(2L)\) and each \(_{t}=t\) yields \(_{t}=\) and \(_{t}=t/(4(t+1)L)\), which recovers Polyak's canonical Heavy-Ball method . Hence, gradient descent with momentum is a special case of meta-learning under the update rule \(:(x,w) w\). Because Heavy Ball carries momentum from past updates, it can encode a model of the learning dynamics that leads to faster convergence, on the order \(O(1/T)\). The implication of this is that the dynamics of meta-learning are fundamentally momentum-based and thus learns an inductive bias in the same cumulative manner. This similarity carries in our theoretical analysis, which we turn to next.

The central challenge in applying the bound in Eq. 6 to Algorithm 2 is that the iterates \(x_{t}\) are generated under the update rule \(\). Hence, we cannot apply standard regret bounds directly. Instead, observe that

\[R^{x}(T) =_{t=1}^{T}_{t} f(_{t}),x_{t}-x^{*} =_{t=1}^{T}_{t} f(_{t}),(_{t -1},w_{t})-x^{*}\] \[=^{T}_{t} f(_{t} ),(_{t-1},w_{t})-(_{t-1},w^{*})}_{_{t}()=_{t}( f(_{t}),(_{t-1},))}+ ^{T}_{t} f(_{t}),(_{t-1},w^{*})-x^{*}}_{}.\]

The first term in the final inequality can be understood as the regret under convex losses \(_{t}()=_{t} f(_{t}),(_{t-1}, )\). Since \(\) is affine, \(_{t}\) is convex and thus this regret can be upper-bounded by linearising the losses. The linearisation reads \( D(_{t-1},w_{t})^{T} f(_{t}),\), which is identical the linear losses \( h_{t}(w_{t}),\) faced by the meta-learner in Eq. 3. In other words, the regret component can be upper-bounded by the of the meta-learner,

\[R^{w}(T)_{t=1}^{T}_{t} h_{t}(w_{t}),w_{t}-w^{ *}_{t=1}^{T}_{t} f(_{t}),( _{t-1},w_{t})-(_{t-1},w^{*}).\]

The regret of the learner is therefore upper bounded by

\[R^{x}(T) R^{w}(T)+_{t=1}^{T}_{t} f(_{t}), (_{t-1},w^{*})-x^{*}.\] (7)

The last term captures the difference in comparator capacity and more specifically the amount of regret they can inflict. If the comparator \(w^{*}\) has more power than that of \(x^{*}\), it can accumulate a lower total loss, in which case this term will be negative, allowing us to discard it. Intuitively, the comparator \(x^{*}\) is non-adaptive. It must make one choice \(x^{*}\) and suffer the average loss. In contrast, the comparator \(w^{*}\) becomes adaptive under the update rule; it can only choose one \(w^{*}\), but on each round it plays \((_{t-1},w^{*})\). If \(\) is sufficiently flexible, this gives the comparator \(w^{*}\) more power than \(x^{*}\), and hence it can force the meta-learner to suffer greater regret. When this is the case, we say that regret is retained when moving from \(x^{*}\) to \(w^{*}\). As long as \(\) is not degenerate, this is typically easy to satisfy by making \(\) sufficiently large.

**Definition 1**.: _Given \(f\), \(\{_{t}\}_{t=1}^{T}\), and \(\{x_{t}\}_{t=1}^{T}\), an update rule \(:\) preserves regret if there exists a comparator \(w\) that satisfies_

\[_{t=1}^{T}_{t}(_{t-1},w), f(_{t}) _{t=1}^{T}_{t} x^{*}, f(_{t}).\] (8)

_If such \(w\) exists, let \(w^{*}\) denote the comparator with smallest norm \(\|w\|\)._

**Lemma 1**.: _Given \(f\), \(\{_{t}\}_{t=1}^{T}\), and \(\{x_{t}\}_{t=1}^{T}\), if \(\) preserves regret, then_

\[R^{x}(T)=_{t=1}^{T}_{t} f(_{t}),x_{t}-x^{*} _{t=1}^{T}_{t} f(_{t}),(_{t-1},w_{t})-(_{t-1},w^{*})=R^{w}(T).\]

Proof: Appendix D. From Eq. 8, it is clear that for \(\) to retain regret, it must admit a parameterisation that correlates negatively with the gradient. In other words, \(\) must be able to behave as a gradient descent algorithm. However, this must not hold on every step, only sufficiently often. For instance, \((x,)\) affine can be made to satisfy this condition if \(\) and \(\) are chosen appropriately.

**Theorem 1**.: _Let \(\) preserve regret and satisfy the assumptions in Section 2. Then_

\[f(_{T})-f(x^{*}) }(\|^{2}}{}+_{t=1} ^{T}^{2}}{2}\| f(_{t})\|_{*}^{2}.\] \[.-}{2L}\| f(_{t})- f(x^{* })\|_{*}^{2}-}{2L}\| f(_{t-1})- f( {x}_{t})\|_{*}^{2}).\]

_If \(x^{*}\) is a global minimiser of \(f\), setting \(_{t}=1\) and \(=\) yields \(f(_{T})-f(x^{*})()}{T}\)._Proof: Appendix D. Compared to Heavy Ball, meta-learning introduces a constant \(\) that captures the smoothness of the update rule. Hence, while meta-learning does not achieve better scaling in \(T\) through \(\), it can improve upon classical optimisation by a constant factor if \(<1\).

That meta-learning can improve upon momentum is borne out experimentally. We consider the problem of minimizing an ill-conditioned convex quadratic and compare standard momentum to a version with meta-learned step-size, i.e. \(:(x,w) w f(x)\), where \(\) is the Hadamard product. We find that introducing a non-linearity \(\) leads to a sizeable improvement in the rate of convergence. See Section 7.1 for further details.

## 5 Meta-Gradients with Optimism

It is well known that minimizing a smooth convex function admits convergence rates of \(O(1/T^{2})\). Our analysis of meta-learning does not achieve these rates. Previous work indicate that we should not expect it to either; to achieve the theoretical lower-limit of \(O(1/T^{2})\), some form of _optimism_ (c.f. Section 3) is required. A typical form of optimism is to predict the next gradient. This is how Nesterov Acceleration operates , and is the reason for its \(O(1/T^{2})\) convergence guarantee.

From our perspective, meta-learning is a non-linear transformation of the iterate \(x\). Hence, we should expect optimism to play a similarly crucial role. Formally, optimism comes in the form of _hint functions_\(\{_{t}\}_{t=1}^{T}\), each \(_{t}^{m}\), that are revealed to the meta-learner prior to selecting \(w_{t+1}\). These hints give rise to _Optimistic Meta-Learning_ (OML) via meta-updates

\[w_{t+1}=*{arg\,min}_{w}\;(_{t+1}_{t+1}+_{s=1}^{t}_{s} h_{s}(w_{s}),w+}\|w\|^{2}).\] (9)

If the hints are accurate, meta-learning with optimism can achieve an accelerated rate of \(O(/T^{2})\), where \(\) is a constant that characterises the smoothness of \(\), akin to \(\). Again, we find that meta-learning behaves as a non-linear transformation of classical optimism and its rate of convergence is governed by the geometry it induces.

For a complete description, see Algorithm 4. These updates do not correspond to the typical meta-update in Algorithm 1; however, we show momentarily that they can be interpreted as the targets in the BMG method, summarised in Algorithm 3. Before turning to BMG, we establish that optimistic meta-learning in the convex setting does indeed yield acceleration.

``` input : Weights \(\{_{t}\}_{t=1}^{T},\{_{t}\}_{t=1}^{T}\) input : Update rule \(\) input : Target oracle input : Initialisation \((x_{0},w_{1})\) for\(t=1,2,,T\): \(x_{t}=x_{t-1}+(x_{t-1},w_{t})\) Query \(z_{t}\) from target oracle \(_{t}()=B_{}^{}(x_{t-1}+(x_{t-1},))\)\(w_{t+1}=w_{t}-_{t} d_{t}(w_{t})\) return\(x_{T}\) ```

**Algorithm 4**Convex optimistic meta-learning.

**Theorem 2**.: _Let \(\) preserve regret and assume Algorithm 4 satisfy the assumptions in Section 2. Then_

\[f(_{T})-f(x^{*}) }(\|^{2}}{_{T}}+_ {t=1}^{T}^{2}_{t}}{2}\|D(_{t-1},w_{t})^{T}  f(_{t})-_{t}\|_{*}^{2}.\] \[-}{2L}\| f(_{t})- f(x^{*})\|_ {*}^{2}-}{2L}\| f(_{t-1})- f(_{ t})\|_{*}^{2}).\]

Proof; Appendix D. From Theorem 2, it is clear that if \(_{t}\) is a good predictor of \(D(_{t-1},w_{t})^{T} f(_{t})\), then the positive term in the summation can be cancelled by the negative term. In a classical optimisation setting, \(D=I_{n}\), and hence it is easy to see that simply choosing \(_{t}\) to be the previous gradient is sufficient to achieve the cancellation . Indeed, this choice gives us Nesterov's Accelerated rate . The upshot of this is that we can specialise Algorithm 4 to capture Nesterov's Accelerated method by choosing \(:(x,w) w\)--as in the reduction to Heavy Ball--and setting the hints to \(_{t}= f(_{t-1})\). Hence, while the standard meta-update without optimism contains Heavy Ball as a special case, the optimistic meta-update contains Nesterov Acceleration as a special case.

In the meta-learning setting, \(D\) is not an identity matrix, and hence the best targets for meta-learning are different. Naively choosing \(_{t}=D(_{t-1},w_{t})^{T} f(_{t-1})\) would lead to a similar cancellation, but this is not allowed. At iteration \(t\), we have not computed \(w_{t}\) when \(_{t}\) is chosen, and hence \(D(_{t-1},w_{t})\) is not available. The nearest term that is accessible is \(D(_{t-2},w_{t-1})\).

**Corollary 1**.: _Let each \(_{t+1}=D(_{t-1},w_{t})^{T} f(_{t})\). Assume that \(\) satisfies_

\[\|D(x^{},w)^{T} f(x)-D(x^{},w^{ })^{T} f(x^{})\|_{*}^{2}\|  f(x^{})- f(x)\|_{*}^{2}\]

_for all \(x^{},x^{},x\) and \(w,w^{}\), for some \(>0\). If each \(_{t}=t\) and \(_{t}=L}\), then \(f(_{T})-f(x^{*})L\,() }{T^{2}-1}\)._

Proof: Appendix D.

These predictions hold empirically in a non-convex setting. We train a 50-layer ResNet using either SGD with a fixed learning rate, or an update rule that adapts a per-parameter learning rate online, \(:(x,w) w f(x)\). We compare the standard meta-learning approach without optimism to optimistic meta-learning. Figure 1 shows that optimism is critical for meta-learning to achieve acceleration, as predicted by theory (experiment details in Appendix C).

## 6 Bootstrapped Meta-Gradients as a form of Optimism

Given Theorem 2, it is of interest to study practical ways of implementing optimism in meta-learning. We study a recently proposed variant of meta-gradients, _Bootstrapped Meta-Gradients (BMG)_. Informally, instead of directly minimising the loss \(f\), the meta-objective in BMG is the distance between the meta-learner's output \(x_{t}\) and a desired _target_\(z_{t}\). The target is computed by unrolling the meta-learner for a further number of steps, thus implicitly embodying a form of optimism, before a gradient step is taken: \(z_{t}=x_{t}+(x_{t},w_{t})- f(x_{t}+(x_{t},w_{t}))\). This encodes optimism via \(\) because it encourages the meta-learner to build up momentum (i.e. to accumulate past updates). To see this formally, we turn to AO-FTRL. First, we provide a more general definition of BMG. Let \(:\) be a convex distance generating function and define the Bregman Divergence \(B^{}\ :\ ^{n}^{n}\ \ \) by

\[B_{z}^{}(x)=(x)-(z)-(z),x-z.\]

Given initial condition \((x_{0},w_{1})\), the BMG updates proceed according to

\[x_{t} =x_{t-1}+(x_{t-1},w_{t})\] \[w_{t+1} =w_{t}-_{t} d_{t}(w_{t})\] \[=w_{t}-_{t}D(x_{t-1},w_{t})^{T}((x_{t}) -(z_{t})),\] (10)

where \(d_{t}:^{n}\) is defined by \(d_{t}(w)=B_{z_{t}}^{}(x_{t-1}+(x_{t-1},w_{t}))\); each \(z_{t}^{n}\) is referred to as a target. See Algorithm 3 for an algorithmic summary. To show how this relates to AO-FTRL, let \(=f\). In this case, the BMG update reads \(w_{t+1}=w_{t}-_{t}D(x_{t-1},w_{t})^{T}( f(z_{t})- f(x_ {t}))\). We can obtain these updates via our convex framework (i.e. Algorithm 4) by setting \(_{t+1}= f(z_{t})\). In this case, we have that (Corollary 3, Appendix E) AO-FTRL reduces to

\[w_{t+1}=}{_{t}}w_{t}-_{t}D(_{t-1},w_{ t})^{T}(_{t+1} f(z_{t})-_{t} f(_{t}))+_{t},\]

where \(_{t}=_{t}_{t}D(_{t-2},w_{t-1})^{T} f(z_{t-1})\) denotes an _error correction term_ that removes the previous target. This error correction term is theoretically important for stability , as the accumulation of hints can otherwise dominate the true signal. That the original BMG does not feature this error term may explain the instabilities the authors observed when setting too aggressive targets . Since Algorithm 3 does not average its iterates--while Algorithm 4 does--we see that these updates (ignoring \(_{t}\)) are identical up to scalar coefficients (that can be controlled for by scaling each \(_{t}\) and each \(_{t+1}\) accordingly). Our next results present formulas for constructing targets in BMG or hints in AO-FTRL so that the two commute.

**Theorem 3**.: _Targets in Algorithm 3 and hints in algorithm 4 commute in the following sense. **BMG \(\)AO-FTRL.** Let BMG targets \(\{z_{t}\}_{t=1}^{T}\) by given. A sequence of hints \(\{\}_{t=1}^{T}\) can be constructed recursively by_

\[_{t+1}_{t+1}=D(_{t-1},w_{t})^{T}(( _{t})-(z_{t})-_{t} f(_{t}))+_{t}_{t }, t[T],\] (11)

_so that interior updates for Algorithm 4 are given by_

\[w_{t+1}=}{_{t-1}}w_{t}-_{t}((z_{t})- (_{t})).\]

_AO-FTRL \(\)BMG. Conversely, assume a sequence \(\{_{t}\}_{t=1}^{T}\) are given, each \(_{t}^{n}\). If \(\) strictly convex, a sequence of BMG targets \(\{z_{t}\}_{t=1}^{T}\) can be constructed recursively by_

\[z_{t}=^{-1}((x_{t})-(_{t+1}_{t+1}+ _{t} f(x_{t}))) t[T],\]

_so that BMG updates in Eq. 10 are given by_

\[w_{t+1}=w_{t}-_{t}(_{t+1}_{t+1}+_{t}(D( _{t-1},w_{t})^{T} f(_{t})-_{t})),\]

_where each \(_{t+1}\) is given by_

\[_{t+1}_{t+1}=_{t+1}D(x_{t-1},w_{t})^{T}_{ t+1}+_{t}_{t}.\]

Proof; see Appendix E. As an immediate consequence of this, we can apply our optimistic meta-gradient analysis (Theorem 2) to BMG to obtain a rate of convergence. This is captured in the following corollary.

**Corollary 2**.: _Let each \(_{t+1}=D(_{t-1},w_{t})^{T}_{t+1}\), for some \(_{t+1}^{n}\). If each \(_{t+1}\) is a better predictor of the next gradient than \( f(_{t-1})\), in the sense that_

\[\|D(_{t-2},w_{t-1})^{T}_{t}-D(_{t-1},w_{ t})^{T} f(_{t})\|_{*}\| f(_{t})-  f(_{t-1})\|_{*},\]

_then Algorithm 4 guarantees convergence at a rate \(O(/T^{2})\)._

In other words, for certain choices of targets, BMG yields accelerated rates of convergence.

## 7 Experiments

In this section, we detail experiments borne out to test our theoretical predictions. Section 7.1 tests predictions about meta-gradients without optimism, by comparing meta-learned variants of standard optimisers to their non-meta-learned counterparts. Section 7.2 test predictions about optmistic meta-gradients, by comparing an optimistic meta-gradient approach to gradient descent.

### Convex Quadratic Experiments

Loss function.We consider the problem of minimising a convex quadratic loss functions \(f:^{2}\) of the form \(f(x)=x^{T}Qx\) for some \(Q\) that is sampled such that it is ill-conditioned (see Appendix B for details).

Protocol.Given that the solution is always \((0,0)\), this experiment revolves around understanding how different algorithms deal with curvature. Given symmetry in the solution and ill-conditioning, we fix the initialisation to \(x_{0}=(4,4)\) and run each algorithm for \(100\) iterations. For each \(Q\) and each algorithm, we sweep over the learning rate, decay rate, and the initialization of \(w\) (see Table 2 for values) and report results for the best performing hyper parameters.

Results.We report the learning curves for the best hyper-parameter choice for 5 randomly sampled problems in the top row of Figure 2 (columns correspond to different Q). We also study the sensitivity of each algorithm to the learning rate in the bottom row Figure 2. For each learning rate, we report the cumulative loss during training. While baselines are relatively insensitive to hyper-parameter choice, meta-learned improve for certain choices, but are never worse than baselines.

### Imagenet Experiments

Protocol.We train a 50-layer ResNet following a standard protocol (Appendix C) with SGD as the baseline optimiser. We compare SGD to a variant that meta-learns an element-wise learning rate online, i.e. \((x,w) w f(x)\). We sweep over the learning rate (for SGD) or meta-learning rate and report results for the best hyper-parameter over three independent runs.

Standard meta-learning.In the standard meta-learning setting, we apply the update rule once before differentiating w.r.t. the meta-parameters. That is, the meta-update takes the form \(w_{t+1}=w_{t}- h_{t}(w_{t})\), where \(h_{t}=f(x_{t}+w_{t} f(x_{t}))\). Because the update rule is linear in \(w\), we can compute the meta-gradient analytically: \( h_{t}(w_{t})=_{w}f(x+(x,w))=D(x,w)^{T} f(x^{ })= f(x) f(x^{}),\) where \(x^{}=x+(x,w)\). Hence, we can compute the meta-updates in Algorithm 1 manually as \(w_{t+1}=\{w_{t}- f(x_{t}) f(x_{t+1}),0.\}\), where we introduce the \(\) operator on an element-wise basis to avoid negative learning rates. Empirically, this was important to stabilize training.

Optimistic meta-learning.For optimistic meta-learning, we proceed much in the same way, but include a gradient prediction \(_{t+1}\). For our prediction, we use the previous gradient, \( f(x_{t+1})\), as our prediction. This yields meta-updates of the form \(w_{t+1}=\{w_{t}- f(x_{t+1})( f(x_{t+1})+ f (x_{t})).-. f(x_{t}) f(x_{t}),0.\}\),.

Results.We report Top-1 accuracy on the held-out test set as a function of training steps in Figure 1. Tuning the learning rate does not yield any statistically significant improvements under standard meta-learning. However, with optimistic meta-learning, we obtain a significant acceleration as well as improved final performance, increasing the mean final top-1 accuracy from \(~{}72\%\) to \(~{}75\%\).

## 8 Conclusion

This paper explores a connection between convex optimisation and meta-learning. We construct an algorithm for convex optimisation that aligns as closely as possible with how meta-learning is done in practice. Meta-learning introduces a transformation and we study the effect this transformation has on the rate of convergence. We find that, while a meta-learned update rule cannot generate a better dependence on the horizon \(T\), it can improve upon classical optimisation up to a constant factor.

An implication of our analysis is that for meta-learning to achieve acceleration, it is important to introduce some form of optimism. From a classical optimisation point of view, such optimism arises naturally by providing the meta-learner with hints. If hints are predictive of the learning dynamics these can lead to significant acceleration. We show that the recently proposed BMG method provides a natural avenue to incorporate optimism in practical application of meta-learning. Because targets in BMG and hints in optimistic online learning commute, our results provide first rigorous proof of convergence for BMG, while providing a general condition under which optimism in BMG yields accelerated learning.

Figure 2: Convex Quadratic. We generate convex quadratic loss functions with ill-conditioning and compare gradient descent with momentum and AdaGrad to meta-learning variants. Meta-Momentum uses \(:(x,w) w f(x)\) while Meta-AdaGrad uses \(:(x,w) f(x)/\), where division is element-wise. _Top:_ loss per iteration for randomly sampled loss functions. _Bottom:_ cumulative loss (regret) at the end of learning as a function of learning rate; details in Appendix B.