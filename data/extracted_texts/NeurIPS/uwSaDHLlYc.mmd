# Diversity-Driven Synthesis: Enhancing Dataset Distillation through Directed Weight Adjustment

Jiawei Du1,2 Xin Zhang1,2,3 Juncheng Hu4 Wenxing Huang1,2,5 Joey Tianyi Zhou1,21

\({}^{1}\) Centre for Frontier AI Research (CFAR), Agency for Science, Technology and Research (A*STAR), Singapore

\({}^{2}\) Institute of High Performance Computing, Agency for Science, Technology and Research (A*STAR), Singapore

\({}^{3}\)XiDian University, Xi'an, China \({}^{4}\)National University of Singapore, Singapore

\({}^{5}\)Hubei University, WuHan, China

###### Abstract

The sharp increase in data-related expenses has motivated research into condensing datasets while retaining the most informative features. Dataset distillation has thus recently come to the fore. This paradigm generates synthetic datasets that are representative enough to replace the original dataset in training a neural network. To avoid redundancy in these synthetic datasets, it is crucial that each element contains unique features and remains diverse from others during the synthesis stage. In this paper, we provide a thorough theoretical and empirical analysis of diversity within synthesized datasets. We argue that enhancing diversity can improve the parallelizable yet isolated synthesizing approach. Specifically, we introduce a novel method that employs dynamic and directed weight adjustment techniques to modulate the synthesis process, thereby maximizing the representativeness and diversity of each synthetic instance. Our method ensures that each batch of synthetic data mirrors the characteristics of a large, varying subset of the original dataset. Extensive experiments across multiple datasets, including CIFAR, Tiny-ImageNet, and ImageNet-1K, demonstrate the superior performance of our method, highlighting its effectiveness in producing diverse and representative synthetic datasets with minimal computational expense. Our code is available at https://github.com/AngusDujw/Diversity-Driven-Synthesis.

## 1 Introduction

With the rapid growth in dataset size and the need for efficient data storage and processing [8; 17; 14; 13], how to condense datasets while preserving their key characteristics becomes a significant challenge in machine learning community [12; 38]. Unlike previous research [29; 39; 50; 44] that focuses on constructing a representative subset through selecting from the original data, _Dataset Distillation_[43; 31; 20] aims to synthesize a small and compact dataset that retains informative features from the original dataset. A model trained on the synthetic dataset is thus supposed to achieve comparable performance as one trained on the original dataset. The development of dataset distillation reduces data-related costs [7; 34; 49] and helps us better understand how Deep Neural Networks (DNNs) extract knowledge from large-scale datasets.

Numerous studies dedicate significant effort to synthesizing distilled datasets more effectively. For example, Zhao _et al._ employ a gradient-matching approach [52; 54] to guide the synthesis process. Trajectory-matching methods [1; 2; 5; 6] further align gradient trajectories to optimize the synthetic data. Additionally, distribution matching [42; 53; 55] and kernel inducing points methods [28; 25; 23; 24] also contribute to synthesizing representative data. Despite the great progress achieved by these methods on datasets like CIFAR , their extensive computational overhead (both GPU memory and GPU time) hinders the extension of these methods to large-scale datasets like ImageNet-1K .

mization tasks, thereby flexibly managing GPU memory usage and computational overhead. However, this approach may present challenges in ensuring the representativeness and diversity of each instance. If each instance is synthesized in isolation, there may be a risk of missing the holistic view of the data characteristics, which is crucial for the training of generalized neural networks. Intuitively, SRe2L might expect that random initialization of synthetic data would provide sufficient diversity to prevent homogeneity in the synthetic dataset. Nevertheless, our analysis, as demonstrated in Figure 1, reveals that this initialization contributes only marginally to diversity. Conversely, the Batch Normalization (BN) loss  in SRe2L plays the practical role in enhancing diversity of the distilled dataset.

Motivated by these findings, we further investigate the factors that enhance the diversity of synthetic datasets from a theoretical perspective. We reveal that the variance regularizer in the BN loss is the key factor ensuring diversity. Conversely, the mean regularizer within the same BN loss unexpectedly constrains diversity. To resolve this contradiction, we suggest a decoupled coefficient to specifically strengthen the variance regularizer's role in promoting diversity. Experimental results validate our hypothesis. We further propose a dynamic mechanism to adjust the weight parameters of the teacher model. Serving as the sole source of supervision from the original dataset, the teacher model guides the synthesis comprehensively. Our meticulously designed weight perturbation mechanism injects randomness without compromising the informative supervision, thereby improving overall performance. Importantly, our method incurs negligible additional computations (\(<0.1\%\)). Intuitively, our method perturbs the weight in a direction that reflects the characteristics of a large subset, varying with each batch of synthesized data.

We conduct extensive experiments across various datasets, including CIFAR-10, CIFAR-100, Tiny-ImageNet, and ImageNet-1K, to verify the effectiveness of our proposed method. The superior performance of our method not only validates our hypothesis but also demonstrates its ability to enhance the diversity of synthetic datasets. This success guides further investigations into searching for representative synthetic datasets for lossless dataset distillation. Our contribution can be summarized as follows:

* We analyze the diversity of the synthetic dataset in dataset distillation both theoretically and empirically, identifying the importance of ensuring diversity in isolated synthesizing approaches.
* We propose a dynamic adjustment mechanism to enhance the diversity of the synthesized dataset, incurring negligible additional computations while significantly improving overall performance. Extensive experiments on various datasets verify the remarkable performance of our method.

## 2 Preliminaries

**Notation and Objective.** Given a real and large dataset \(=\{(}_{i},_{i})\}_{i=1}^{||}\), Dataset Distillation aims to synthesize a tiny and compact dataset \(=\{(}_{i},_{i})\}_{i=1}^{||}\). The samples in \(\) are drawn i.i.d from a natural distribution \(\), while the samples in \(\) are optimized from scratch. We use \(_{}\) and

Figure 1: **Left:** t-SNE visualization of logit embeddings on CIFAR-100  dataset. The scatter plot illustrates the distribution of synthetic data instances distilled by SRe2L (blue dots) and our DWA method (red stars). The blue density contours represent the distribution of natural data instances. Our DWA method demonstrates a more diverse and widespread distribution compared to SRe2L , indicating better generalization and coverage of the feature space. **Right:** The consequent performance improvement of DWA in various datasets. Experiments are conducted with 50 images per class.

\(_{}\) to represent the converged weight trained on \(\) and \(\), respectively. We define a neural network \(h=g f\), where \(g\) acts as the feature extractor and \(f\) as the classifier. The feature extractor and the classifier loaded with the corresponding weight parameters from \(\) are denoted by \(g_{}\) and \(f_{}\).

Throughout the paper, we explore the properties of synthesized datasets within the latent space. We transform both \(},}^{C}\) from the pixel space, to the latent space, \(,^{d}\), for better formulation. This transformation is given by \(=g_{_{}}(})\) and \(=g_{_{}}(})\). The objective of Dataset Distillation is to ensure that a model \(h\) trained on the synthetic dataset \(\) is able to achieve a comparable test performance as the model trained with \(\), which can be formulated as,

\[}_{}[(h_{_{ }},)]}_{ }[(h_{_{}},)],\] (1)

where \(\) can be an arbitrary loss function. The expression \((h_{_{}},)\) should be interpreted as \((h_{_{}},,)\), where \(\) is the ground truth label.

**Synthesizing \(\).** A series of previous works mentioned in Section 5 have introduced various methods to synthesize \(\). Specifically, SRE2L  proposes an efficient and effective synthesizing method, which optimizes each synthetic instance \(_{i}\) by solving the following minimization problem:

\[_{_{i}^{d}}[(f_{_{ }},_{i})+_{}(f_{ _{}},_{i})],\] (2)

where \(_{}\) denotes the BN loss, and \(\) is the coefficient of \(_{}\). The detailed definition of \(_{}\) can be found in Equation 3. Minimizing the BN loss \(_{}\) significantly enhances the performance of SRE2L, which is designed to ensure that \(\) aligns with the same normalization distribution as \(\). However, we argue that another essential but overlooked aspect of the BN loss \(_{}\) is its role in introducing diversity to \(\), which also greatly benefits the final performance. In the following section, we will analyze this issue in greater detail.

## 3 Methodology

Diversity in the synthetic dataset \(\) is essential for effective use of the limited distillation budget. This section reveals that the BN loss, referenced in Equation 2, enhances \(\)'s diversity. However, the suboptimal setting of BN loss limits this diversity. To overcome this, we propose a dynamic adjustment mechanism for the weight parameters of \(f_{_{}}\), enhancing diversity during synthesis. Finally, we detail our algorithm and theoretically demonstrate its effectiveness. The pseudocode of our proposed DWA can be found in Algorithm 1.

```
0: Original dataset \(\); Number of iterations \(T\); Image per class ipc; Number of steps \(K\), magnitude \(\) to solve the weight adjustment \(\); Learning rate \(\); A network \(f_{_{}}\) with weight parameter \(_{}\), \(f_{_{}}\) is well trained on \(\).
1: Initialize \(=\{\}\), \(_{0}=_{(_{})}\)
2:for\(i=1\) to ipcdo
3: Randomly select one instance for each class from \(\), to initialize \(^{i}_{0}\), i.e.,
4:\(^{i}_{0}=\{(_{i},_{i})(_{i},_{i}) _{i}\}\)
5:\(\) Compute the adjustment of weights \(\) by solving Equation 11
6:for\(k=1\) to \(K\)do
7:\(_{k}=_{k-1}+ L_{^{i}_{0} }(f_{_{}+_{k-1}})\)
8:\(=_{K}\)\(\) Directed Weight Adjustment
9:\(\) Optimize \(^{i}\)
10:for\(t=1\) to \(T\)do
11:\(^{i}_{t}=^{i}_{t-1}+_{} (f_{_{}+},^{i}_{t-1})\)\(\)\(\) is defined in Equation 15
12:\(=\{^{i}\}\)
13: Synthetic dataset \(\) ```

**Algorithm 1** Directed Weight Adjustment (DWA)

In the actual optimization process, operations occur within the pixel space using the entire network \(h_{_{}}\). However, as we discuss the optimization in the latent space, we only consider solutions within this space. Then, we transform the solution in latent space back into pixel space as \(}=g_{_{}}^{-1}()\).

### Batch Normalization Loss Enhances Diversity of \(\)

The BN loss \(_{}\) comprises mean (\(_{}\)) and variance (\(_{}\)) components, defined as follows:

\[_{}=_{}+_ {}_{}(f_{_ {}},_{i})=_{l}\|_{l}()-_{l}()\|_{2},\] \[_{}(f_{_{ }},_{i})=_{l}\|_{l}^{2}( )-_{l}^{2}()\|_{2},\] (3)

where \(_{l}\) and \(_{l}^{2}\) refer to the channel mean and variance in the \(l\)-th layer, respectively. \(_{i}\) is optimized within a mini-batch \(\), where \(_{i}\) and \(\). Each component of \(_{}\) operates from its own perspective to enhance dataset distillation. First, the mean component \(_{}\) regularizes the synthetic data \(\), ensuring its values align closely with those of the representative centroid of \(\) in latent space. Second, the variance component \(_{}\) encourages the synthetic data in \(\) to differ from each other, thereby maintaining the variance \(_{l}^{2}()\). Thus, this BN loss-driven synthesis can be decoupled as

\[_{i}=_{c}(_{},_{ })+_{i},\] (4)

where \(_{c}\) can be regarded as an optimal solution to Equation 2 when the variance regularization term \(_{}\) is not considered, _i.e._,

\[\|_{}(f_{_{}},_{c}) \|_{2}_{1}_{} (f_{_{}},_{c})=_{l}\|_ {l}(_{c})-_{l}()\|_{2} _{2},\] (5)

where both \(_{1},_{2}>0\) and \(_{1},_{2} 0\). \(_{i}\) represents a small perturbation and \(_{i}(0,_{}^{2}(_{ }))\). Therefore, the variance of the synthetic dataset \(\) is,

\[()=_{c}(_{ },_{})+() =_{}^{2}(_{}).\] (6)

We have \(_{c}(_{},_{ })=0\) as \(_{c}\) is deterministic. Unlike other approaches that consider the mutual influences among synthetic data instances and optimize the dataset collectively, \(\) optimizes each synthetic data instance individually. Therefore, the diversity of the synthetic dataset \(\) is solely determined by \(_{}\).

However, simply increasing \(\) contributes marginally to enhancing the diversity of \(\). This is because a greater \(\) will also emphasize the regularization term \(_{}\), which contradicts the emphasis on \(_{}\). We provide a detailed analysis in the Appendix A.1. As a result, we propose using a decoupled coefficient, \(_{}\), to enhance the diversity of \(\).

Additionally, the synthetic data instances are optimized individually to approximate the representative data instance \(_{c}\). However, the gaussian initialization \((0,1)\) in pixel space does not distribute uniformly around \(_{c}\) in latent space, making the converged synthetic data instances to cluster in a crowed area in latent space, as dedicated in Figure 1. To address this, we propose initializing with real instances from \(\) inspired by MTT , ensuring a uniform projection when synthesizing \(\).

### Random Perturbation on \(_{}\) Helps Improve Diversity

In the previous section, we highlighted the often overlooked aspect of the BN loss in introducing diversity to \(\), which was also verified through experiments in Section 4.2. Building upon this, we propose to introduce randomness into \(_{}\) to further enhance \(\)'s diversity, as it is the only remaining factor affecting \(()\), as shown in Equation 6.

Let \(_{c}^{*}=_{c}(_{},_{})\) to be the original optimal solution to Equation 2. We aim to solve the adjusted optimal solution \(_{c}=_{c}(_{},_{}+ )=_{c}^{*}+\), where \(_{}\) is randomly perturbed by \(\), and \((0,_{}^{2})\). Consequently, we have:

\[\|_{}(f_{_{}+},_{c} )\|_{2}=\|_{}(f_{_{}+ },_{c}^{*}+)\|_{2}_{1}.\] (7)

To solve for \(\), we can apply a first-order bivariate Taylor series approximation because \(_{}(f_{_{}},_{c})_{1}\), where \(_{1} 0\), and both \(\) and \(\) are small. Thus,

\[\|_{}(f_{_{+} },_{c}^{*}+)\|_{2}\] \[= \|_{}(f_{_{}},_{c}^ {*})+_{}^{2}(f_{_{}},_{c}^{*} )+_{}[_{}(f_{_{ }},_{c}^{*})]\|_{2}\] \[ \|_{}(f_{_{}},_{c}^ {*})\|_{2}+\|_{}^{2}(f_{_{}},_{c}^{*})+_{}[_{ }(f_{_{}},_{c}^{*})]\|_{2}\] \[ _{1}+\|_{}^{2}(f_{_{ }},_{c}^{*})+_{}[_{ }(f_{_{}},_{c}^{*})]\|_{2},\] (8)To satisfy Equation 7, we have:

\[_{}^{2}(f_{_{}},_{c}^{*} )+_{}[_{}(f_{_{ }},_{c}^{*})]=,\] \[=-_{}[_{}(f_{ _{}},_{c}^{*})]^{-1}_{}^{2} (f_{_{}},_{c}^{*}).\] (9)

Intuitively, \(\) must compensate for the \(_{}\) incurred by introducing the random perturbation \((0,_{}^{2})\) on \(_{}\). By Equation 9, \(()()=_{}^{2}\), then:

\[(^{}) =_{c}(_{},_{}+)+()\] \[=(_{c}^{*}+)+()\] \[=_{}^{2}+_{}^{2}( _{})_{}^{2}( _{}),\] (10)

where \(\) is determined by \(-_{}[_{}(f_{_{}},^{c})]^ {-1}_{}^{2}(f_{_{}},^{c})\), as shown in Equation 9. Therefore, the variance of the new synthetic dataset \(^{}\) is greater than that of \(\) without perturbing \(_{}\).

### Directed Weight Adjustment on \(_{}\)

Although perturbing \(_{}\) could significantly increase the variance of the synthetic dataset \(\), undirected random perturbation \(\) can also introduce noise, which in turn degrades the performance. We aim to address this limitation by directing the random perturbation \(\) without introducing noise into \(\). We propose to obtain directed \(\) by solving the following maximization problem:

\[=*{arg\,max}_{}L_{ }(f_{_{}+}) L_{ }(f_{_{}+})=_{_{i }}(f_{_{}+},_{i} ),\] (11)

where \(\) represents a randomly selected subset of \(\), and \(||||\). As such, \(\) will not introduce unanticipated noise when synthesizing \(\). The randomly selected \(\) ensures that the randomness of \(\) continues to benefit the diversity of \(\). Next, we will demonstrate this theoretically.

Effective dataset distillation should provide concise and critical guidance from the original dataset \(\) when synthesizing the distilled dataset. Here, this guidance is introduced primarily through the converged weight parameters \(_{}\), _i.e._,

\[_{}=*{arg\,min}_{}L_{}(f_ {_{}}) L_{}(f_{ _{}})=_{_{i}}(f_{ _{}},_{i}),\] (12)

where \(_{}\) contains informative features of \(\) because it achieves minimized training loss over \(\). We demonstrate that \(\), obtained from Equation 11, decreases the training loss computed over \(\), which, in fact, highlights the features of \(\). By applying a first-order Taylor expansion, we obtain:

\[L_{}(f_{_{}+}) L_{}(f_{_{}} )+_{}L_{}(f_{_{ }}).\] (13)

Since \(_{}\) is optimized until reaching a local minimum with respect to the loss function computed over the training set \(\), we have:

\[_{}L_{}(f_{_{}})=_{ }L_{}(f_{_{}})+_{}L_{ }(f_{_{}})= _{}L_{}(f _{_{}})=-_{}L_{}(f_{_{ }}),\]

where \(\) is the tensor of zeros with the same dimension as \(_{}\). Substitute it back into Equation 13, we have:

\[L_{}(f_{_{}+ })-L_{}(f_{ _{}}) _{}L_{}(f_{_{ }})\] \[=-_{}L_{}(f_{_{}}) \] \[ -(L_{}(f_{_{}+})-L_{}(f_{_{}}))  0,\] (14)

\(L_{}(f_{_{}+})\) will clearly be greater than \(L_{}(f_{_{}})\), as indicated by Equation 11. Thus, we demonstrate that the directed \(\) results in less noise and improved performance. In summary, after resolving \(\) as in Equation 11, our proposed method synthesizes data instance \(_{i}\) by solving:

\[_{i}}=*{arg\,min}_{^{d}} =[(f_{_{}+ },_{i})+_{}(f_{_{}},_{i})+_{}_{ }(f_{_{}},_{i})].\] (15)Experiments

To evaluate the effectiveness of the proposed method, we have conducted extensive comparison experiments with SOTA methods on various datasets including CIFAR-10/100 (\(32 32\), 10/100 classes) , Tiny-ImageNet (\(64 64\), 200 classes) , and ImageNet-1K (\(224 224\), 1000 classes)  using diverse network architectures like ResNet-(18, 50, 101) , MobileNetV2 , ShuffleNetV2 , EfficientNet-B0 , and VGGNet-16 . We conduct our experiments on the server with one Nvidia Tesla A100 40GB GPU.

**Solving \(\).** Before we conduct our experiments, we propose to use a gradient descent approach to solve \(\) in Equation 11. There are two coefficients, \(K\) and \(\), used in the gradient descent approach. \(K\) represents the number of steps, and \(\) normalizes the magnitude of the directed weight adjustment. The details for solving \(\) can be found in Line 7 of Algorithm 1.

**Experiment Setting.** Unless otherwise specified, we default to using ResNet-18 as the backbone for distillation. For ImageNet-1K, we use the pre-trained model provided by Torchvision while for CIFAR-10/100 and Tiny-ImageNet, we modify the original architecture under the suggestion in . More detailed hyper-parameter settings can be found in Appendix A.2.1.

**Baselines and Metrics.** We conduct comparison with seven Dataset Distillation methods including DC , DM , CAFE , MTT , TESLA , SRe2L , and DataDAM . For all the considered comparison methods, we assess the quality of the distilled dataset by measuring the Top-1 classification accuracy on the original validation set using models trained on them from scratch. Blue cells in all tables highlight the highest performance.

### Results & Discussions

**CIFAR-10/100.** As shown in Table 1, our DWA exhibits superior performance compared to conventional dataset distillation methods, particularly evident on CIFAR-100 with a larger distillation budget. For instance, our DWA yields over a 10% performance enhancement compared to MTT  with \(=50\). Leveraging a more robust distillation backbone like ResNet-18, our approach surpasses the SOTA method SRe2L  across all considered settings. Specifically, we achieve more than 5% and 8% accuracy improvement on CIFAR-10 and CIFAR-100, respectively.

    &  &  &  \\   & & DC  & DM  & CAFE  & MTT  & TESLA  & DWA (ours) & SRe2L  & DWA (ours) \\   & \(10\) & \(44.9_{ 0.5}\) & \(48.9_{ 0.6}\) & \(46.3_{ 0.6}\) & \(65.4_{ 0.7}\) & \(66.4_{ 0.8}\) & \(45.0_{ 0.4}\) & \(27.2_{ 0.4}\) & \(32.6_{ 0.4}\) \\  & \(50\) & \(53.9_{ 0.5}\) & \(63.0_{ 0.4}\) & \(55.5_{ 0.6}\) & \(71.6_{ 0.7}\) & \(72.6_{ 0.7}\) & \(63.3_{ 0.7}\) & \(47.5_{ 0.5}\) & \(53.1_{ 0.3}\) \\   & \(10\) & \(25.2_{ 0.3}\) & \(29.7_{ 0.3}\) & \(27.8_{ 0.3}\) & \(40.1_{ 0.4}\) & \(41.7_{ 0.3}\) & \(47.6_{ 0.4}\) & \(31.6_{ 0.5}\) & \(39.6_{ 0.6}\) \\  & \(50\) & - & \(43.6_{ 0.4}\) & \(37.9_{ 0.3}\) & \(47.7_{ 0.2}\) & \(47.9_{ 0.3}\) & \(59.9_{ 0.4}\) & \(52.2_{ 0.3}\) & \(60.9_{ 0.5}\) \\   

Table 1: Comparison with SOTA dataset distillation baselines on CIFAR-10/100. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original papers, DC , DM , CAFE , MTT , and TESLA  use ConvNet-128 (_small model_). For SRe2L , ResNet-18 (_large model_) is used for synthesis and validation.

    &  &  &  &  &  \\   & & MTT  & DataDAM  & TESLA  & SRe2L  & DWA (ours) & SRe2L & DWA (ours) & SRe2L & DWA (ours) \\   & \(50\) & \(28.0_{ 0.3}\) & \(28.7_{ 0.3}\) & - & \(41.1_{ 0.4}\) & \(52.8_{ 0.2}\) & \(42.2_{ 0.5}\) & \(53.7_{ 0.2}\) & \(42.5_{ 0.2}\) & \(54.7_{ 0.3}\) \\  & \(100\) & - & - & - & \(49.7_{ 0.3}\) & \(56.0_{ 0.2}\) & \(51.2_{ 0.4}\) & \(56.9_{ 0.4}\) & \(51.5_{ 0.3}\) & \(57.4_{ 0.3}\) \\   & \(10\) & \(64.0_{ 1.3}\)\({}^{}\) & \(6.3_{ 0.0}\) & \(17.8_{ 1.3}\) & \(21.3_{ 0.6}\) & \(37.9_{ 0.2}\) & \(28.4_{ 0.1}\) & \(43.0_{ 0.5}\) & \(30.9_{ 0.1}\) & \(46.9_{ 0.4}\) \\  & \(50\) & - & - & \(27.9_{ 1.2}\) & \(46.8_{ 0.2}\) & \(55.2_{ 0.2}\) & \(55.6_{ 0.3}\) & \(62.3_{ 0.1}\) & \(60.8_{ 0.5}\) & \(63.3_{ 0.7}\) \\   & \(100\) & - & - & - & \(52.8_{ 0.3}\) & \(59.2_{ 0.3}\) & \(61.0_{ 0.4}\) & \(65.7_{ 0.4}\) & \(62.8_{ 0.2}\) & \(66.7_{ 0.2}\) \\   

Table 2: Comparison with SOTA dataset distillation baselines on Tiny-ImageNet and ImageNet-1K. Unless otherwise specified, we use the same network architecture for distillation and validation. Following the settings in their original papers, MTT , and TESLA  use ConvNet-128 (_small model_). For SRe2L , ResNet-18 (_large model_) is used for synthesis, and the distilled dataset is evaluated on ResNet-18, 50, and 101. \(\) indicates MTT is performed on a 10-class subset of the full ImageNet-1K dataset.

**Tiny-ImageNet & ImageNet-1K.** Compared with CIFAR-10/100, ImageNet datasets are more closely reflective of real-world scenarios. Table 2 lists the related results. Due to the limited scalability capacity of conventional distillation paradigm, only a few methods have conducted evaluation on ImageNet datasets. Here we provide a comprehensive comparison with SRE2L , which has been validated as the most effective one for distilling large-scale dataset. It is obvious that our method significantly outperforms SRE2L on all ipc settings and validation models. For instance, our DWA surpasses SRE2L by 16.6% when ipc \(=10\) on ImageNet-1K using ResNet-18. Figure 2 further provides the visualization results, the enhanced diversity is the key driver behind the substantial performance improvement.

### Ablation Study

**Decoupled \(_{}\) Coefficient.** We first test our hypothesis, as outlined in Section 3.1, positing that strengthening \(_{}\) conflicts with the emphasis on \(_{}\), which is critical for ensuring diversity in synthetic datasets. Therefore, we compare the synthetic dataset distilled with an emphasis on \(_{}\) (which strengthens both \(_{}\) and \(_{}\)) against one that emphasizes \(_{}\) alone. As depicted in Figure 3, focusing solely on \(_{}\) outperforms the combined emphasis on \(_{}\) in both SRE2L  and our proposed Directed Weight Adjustment (DWA). These experimental results verify our hypothesis in Section 3.1, indicating the optimal value of the decoupled coefficient \(_{}\) is 0.11. We also employ

Figure 4: Normalized feature distance of decoupled variance component with \(_{}=0.11\) (the weight of mean component defaults to \(0.01\)) and coupled variance component with \(_{}=0.11\). ResNet-18’s last convolutional layer outputs are used for feature distance calculation (see Appendix A.2.2). Ten classes are randomly chosen from CIFAR-100 distilled dataset.

Figure 3: Analysis of decoupled \(_{}\) coefficient. We vary \(_{}\) across a wide range of \((0.01 0.23)\). ‘decoupled var’ indicates \(_{}\) is changing individually with a fixed mean component whose weight defaults to 0.01. ‘coupled var’ represents the weight of the mean and \(_{}\) change in tandem. (a) and (b) illustrate the performance of the original SRE2L  and our DWA in these two scenarios, respectively. This analysis is conducted on CIFAR-100 using ResNet-18. Each \(_{}\) undergoes five independent experiments, with variance indicated by lighter color shades.

Figure 2: Visualization of distilled images for the goldfish class. Panels (a) and (b) show the synthesized results by SRE2L  and our DWA, respectively. The synthetic data instances generated by our DWA method exhibit significantly greater diversity compared to those produced by SRE2L, highlighting the effectiveness of our approach in capturing a broader range of features.

the normalized feature distance as a metric to comprehensively evaluate our emphasis. This metric measures the mutual feature distances between instances, as defined in Appendix A.2.2. By randomly selecting 10 classes from CIFAR-100, we calculate the normalized feature distances between synthetic datasets emphasized by the decoupled \(_{}\) and the coupled \(_{}\). The findings, illustrated in Figure 4, validate our hypothesis from a different perspective.

**Directed Weight Adjustment.** We clarify the necessity of restricting the direction of weight adjustment in Section 3.3. To test its effectiveness, we apply a random \(\), sampled from a Gaussian Distribution, to \(_{}\). As shown in Table 3, we assess synthetic datasets derived from three scenarios: no weight adjustment, random weight adjustment, and our directed weight adjustment (DWA) method, using the CIFAR-100 dataset. The results, examined across various architectures, underscore the importance of directing weight adjustments in distillation processes. Notably, we observe performance degradation in the synthetic dataset optimized with random weight adjustment at \(=10\) compared to those without weight adjustment. This decline occurs because, at smaller \(\) values, the noise introduced by random weight adjustment outweighs the benefits of diversity. However, as the number of synthetic instances increases, diversity becomes more effective in capturing a broader range of features, leading to improved performance, as reflected at \(=50\).

**Parameters Study on \(K\) and \(\).** Apart from direction, the number of steps \(K\) and magnitude \(\) of perturbation also influence the distillation process. Figure 5 illustrates the grid search for these two hyper-parameters and demonstrates the positive impact of perturbation, which is achieved effortlessly, requiring no meticulous manual parameter tuning. In our experiments, we set \(K=12\) and \(=15e^{-3}\) for all the datasets. Readers can adjust these hyper-parameters according to their specific circumstances (different datasets and networks) to obtain better results.

**Cross-Architecture Generalization.** The generalizability across different architectures is a key feature for assessing the effectiveness of the distilled dataset. In this section, we evaluate the surrogate dataset condensed by different backbones (ResNet-18 and ConvNet-128) on various archi

    & =10\)} & =50\)} \\  Perturbation & ✗ & \(\) & \(\) & ✗ & \(\) & ✗ \\  ResNet-18 & \(30.6 0.7\) & \(14.9 0.1\) & \(39.6 0.6\) & \(56.1 0.4\) & \(56.2 0.6\) & \(60.3 0.5\) \\ ResNet-50 & \(26.5 1.1\) & \(15.0 0.2\) & \(35.2 0.7\) & \(55.7 0.9\) & \(57.1 0.5\) & \(60.6 0.8\) \\ MobileNetV2 & \(18.2 0.5\) & \(14.4 1.2\) & \(27.8 0.7\) & \(46.9 0.9\) & \(50.7 0.6\) & \(53.6 0.2\) \\ ShuffleNet & \(10.3 0.7\) & \(10.7 0.1\) & \(19.4 0.9\) & \(30.9 1.1\) & \(39.1 0.1\) & \(41.7 0.8\) \\ EfficientNet & \(11.8 0.4\) & \(11.1 0.7\) & \(20.2 0.4\) & \(28.6 1.0\) & \(38.8 1.0\) & \(40.7 0.3\) \\   

Table 3: An ablation study of DWA was conducted using various network architectures. The synthetic dataset was distilled by ResNet-18 from the CIFAR-100 dataset. We use ✗ to denote the distilled dataset without weight adjustment, \(\) to denote the distilled dataset with random weight adjustment, and ✗ to represent Directed Weight Adjustment (DWA).

Figure 5: Performance grid of ResNet-18 with changes in perturbation steps \(K\) and magnitude \(\).

    & =10\)} & =50\)} \\  Perturbation & ✗ & \(\) & ✗ & ✗ & \(\) & ✗ \\  ResNet-18 & \(30.6 0.7\) & \(14.9 0.1\) & \(39.6 0.6\) & \(56.1 0.4\) & \(56.2 0.6\) & \(60.3 0.5\) \\ ResNet-50 & \(26.5 1.1\) & \(15.0 0.2\) & \(35.2 0.7\) & \(55.7 0.9\) & \(57.1 0.5\) & \(60.6 0.8\) \\ MobileNetV2 & \(18.2 0.5\) & \(14.4 1.2\) & \(27.8 0.7\) & \(46.9 0.9\) & \(50.7 0.6\) & \(53.6 0.2\) \\ ShuffleNet & \(10.3 0.7\) & \(10.7 0.1\) & \(19.4 0.9\) & \(30.9 1.1\) & \(39.1 0.1\) & \(41.7 0.8\) \\ EfficientNet & \(11.8 0.4\) & \(11.1 0.7\) & \(20.2 0.4\) & \(28.6 1.0\) & \(38.8 1.0\) & \(40.7 0.3\) \\   

Table 3: An ablation study of DWA was conducted using various network architectures. The synthetic dataset was distilled by ResNet-18 from the CIFAR-100 dataset. We use ✗ to denote the distilled dataset without weight adjustment, \(\) to denote the distilled dataset with random weight adjustment, and ✗ to represent Directed Weight Adjustment (DWA).

tectures including MobileNetV2 , ShuffleNetV2 , EfficientNet-B0 , and VGGNet-16 . The experimental results are reported in Table 4 and Table 5. It is evident that our DWA-synthesized dataset can effectively generalize across various architectures. Notably, for \(=50\) on CIFAR-100 with ShuffleNetV2, EfficientNet-B0, and ConvNet-128--three architectures not involved in the data synthesis phase--our method achieves impressive classification performance, with accuracies of 41.7%, 40.7%, and 37.0%, respectively, outperforming the latest SOTA method, SRe2L , by 14.2%, 15.8%, and 17.6%. In Appendix A.2.3, we further extend the proposed method to a vision transformer-based model, DeiT-Tiny .

## 5 Related Works

Dataset Distillation  emerges as a derivative of Knowledge Distillation (KD) , emphasizing data-centric efficiency over traditional model-centric one. Previous studies have explored various strategies to condense datasets, including performance matching, gradient matching [54; 52; 19] distribution matching [42; 53; 55; 48; 4], and trajectory matching [1; 2; 5; 6; 21; 41].

What distinguishes DD from KD is the bi-level optimization, which considers both model parameters and image pixels. The consequent complexity and computational burden intricate optimization significantly diminish the effectiveness of the aforementioned methods. To address this issue, SRe2L  introduced a three-step paradigm known as _Squeeze-Recover-Relabel_. This approach relies on the highly encoded distribution prior, _i.e._, the running mean and running variance in the BN layer, to circumvent supervision provided by model training. With this decoupled optimization, SRe2L is able to extend DD to high-resolution and large-scale datasets like ImageNet-1K.

Another critical challenge in dataset compression, not limited to distillation, is how to represent the original dataset distribution with a scarcity of synthetic data samples . Previous research claims that the diversity of a dataset can be evaluated by spatial distribution , the maximum dispersion or convex hull volume , and coverage . Conventional dataset distillation [49; 15] treats the synthetic compact dataset as an integrated optimizable tensor without specialized guarantees for diversity and relies entirely on the matching objectives mentioned above. Recognizing this limitation, Dream  proposed using cluster centers to induce synthesis and ensure adequate diversity. Besides, SRe2L resorts to the second-order statistics, _i.e._, variance of representations in pre-trained weights to provide diversity.

## 6 Conclusion

In this work, we hypothesize that ensuring diversity is crucial for effective dataset distillation. Our findings indicate that the random initialization of synthetic data instances contributes minimally to ensuring that each instance captures unique knowledge from the original dataset. We validate our hypothesis through both theoretical and empirical approaches, demonstrating that enhancing diversity significantly benefits dataset distillation. To this end, we propose a novel method, Directed Weight Adjustment (DWA), which introduces diversity in synthesis by customizing weight adjustments for each mini-batch of synthetic data. This approach ensures that each mini-batch condenses a variety of knowledge. Extensive experiments, particularly on the large-scale ImageNet-1K dataset, confirm the superior performance of our proposed DWA method.

**Limitations and Future work.** While DWA provides a straightforward and efficient approach to introducing diversity in dataset distillation, its reliance on the sampling of a random distribution to adjust weight parameters presents limitations. Increasing the variance of the random distribution can introduce unexpected noise, thereby bottlenecking overall performance. Future investigations could explore synthesizing data instances in a sequential manner, encouraging later instances to consciously distinguish themselves from earlier ones, thereby further enhancing diversity.

    & Methods & MobileNetv2 & ShuffleNet & EfficientNet \\   & SRe2L & \(15.4_{ 0.2}\) & \(9.0_{ 0.7}\) & \(11.7_{ 0.2}\) \\  & DWA (ours) & \(29.1_{ 0.3}\) & \(11.4_{ 0.6}\) & \(37.4_{ 0.5}\) \\   & SRe2L & \(48.3_{ 0.5}\) & \(9.0_{ 0.6}\) & \(53.6_{ 0.4}\) \\  & DWA (ours) & \(51.6_{ 0.5}\) & \(28.5_{ 0.5}\) & \(56.3_{ 0.4}\) \\   

Table 5: Cross-architecture performance of distilled dataset of ImageNet-1K using ResNet-18.