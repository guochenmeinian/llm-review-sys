# Inverse Dynamics Pretraining Learns Good Representations for Multitask Imitation

David Brandfonbrener

New York University

&Ofir Nachum

Google

&Joan Bruna

New York University

david.brandfonbrener@nyu.edu

###### Abstract

In recent years, domains such as natural language processing and image recognition have popularized the paradigm of using large datasets to pretrain representations that can be effectively transferred to downstream tasks. In this work we evaluate how such a paradigm should be done in imitation learning, where both pretraining and finetuning data are trajectories collected by experts interacting with an unknown environment. Namely, we consider a setting where the pretraining corpus consists of multitask demonstrations and the task for each demonstration is set by an unobserved latent context variable. The goal is to use the pretraining corpus to learn a low dimensional representation of the high dimensional (e.g., visual) observation space which can be transferred to a novel context for finetuning on a limited dataset of demonstrations. Among a variety of possible pretraining objectives, we argue that inverse dynamics modeling - i.e., predicting an action given the observations appearing before and after it in the demonstration - is well-suited to this setting. We provide empirical evidence of this claim through evaluations on a variety of simulated visuomotor manipulation problems. While previous work has attempted various theoretical explanations regarding the benefit of inverse dynamics modeling, we find that these arguments are insufficient to explain the empirical advantages often observed in our settings, and so we derive a novel analysis using a simple but general environment model.

## 1 Introduction

Pipelines in image recognition and natural language processing commonly use large datasets to pretrain representations that are then transferred to downstream tasks where data is limited (Devlin et al., 2018; Chen et al., 2020; Radford et al., 2021). In this paper, we consider how this paradigm can be applied to imitation learning (Pomerleau, 1991; Ho and Ermon, 2016; Kostrikov et al., 2019). In contrast to supervised learning domains where datasets consist of input-output pairs, imitation learning datasets consist of _trajectories_ with both the input-output mapping to be learned (namely, observation-action pairs) as well as information about the dynamics of the environment. Given this additional structure, it is worthwhile to study pretraining approaches that can incorporate this structure to improve beyond methods from traditional supervised learning domains.

To formalize the precise notion of transfer between pretraining and finetuning phases, we consider a multitask imitation setting where the environment (i.e., the transition dynamics) is fixed and data is comprised of trajectories of _task experts_ acting in this environment. A task is defined by a latent context variable that is observed by an expert demonstrator, but is not contained in the dataset, as shown in Figure 1. During pretraining, we have access to a large number of trajectories from various tasks, while during finetuning we have access to a small number of trajectories from a single task. The goal is thus to use the pretraining dataset to learn representations that contain information about the environment that facilitates efficient learning of the finetuning task.

A number of existing works have proposed objectives for representation learning that are applicable in this setting (Schwarzer et al., 2021; Stooke et al., 2021; Yang and Nachum, 2021; Yang et al., 2023), and we consider a variety of algorithms and modes of analysis to determine which approach is the most promising. Algorithmically, we consider four generic classes of objectives for pretraining: inverse dynamics, behavior cloning, forward dynamics, and static observation modeling (Figure 1). We conduct two types of analysis. First, we conduct an extensive empirical evaluation and introspection of the candidate algorithms along with several strong baselines. Second, we present a simple but general theoretical model of the multitask representation learning problem and analyze the relative merits of the candidate algorithms under this model.

Our main results from these analyses are summarized as follows:

1. Across a broad array of experiments from visual observations in six environments, out of all approaches considered, inverse dynamics is the only one that consistently outperforms the baseline of training a model from scratch. The performance of inverse dynamics even matches that of finetuning from ground truth low-dimensional states on in-distribution contexts. Moreover, we find that inverse dynamics scales the best with pretraining dataset size and most effectively maintains relevant information about the observation in its learned representation.
2. In our simplified model of representation learning, we show that inverse dynamics pretraining efficiently recovers the ideal representation while behavior cloning can suffer from confounding and forward dynamics can suffer from poor sample efficiency. These results provide intuition for the empirical results and motivate why inverse dynamics pretraining is so performant and robust.

## 2 Related work

As explained above, pretraining a representation has become a dominant paradigm in computer vision and natural language processing (Devlin et al., 2018; Chen et al., 2020; Radford et al., 2021). Determining how to best leverage similar pretraining techniques in decision making problems is an important step towards extending the success of supervised learning into more temporally extended problems like those in robotics (Yang et al., 2023).

Prior work proposes several possible pretraining objectives for learning features for decision-making (and illustrated in Figure 1). First, inverse dynamics modeling has been proposed in several settings, although never as a representation learning algorithm for multitask imitation. Most directly related to our work is Efroni et al. (2021); Lamb et al. (2022) which use multi-step inverse dynamics for feature extraction for exploration in reinforcement learning (RL) in the presence of exogenous noise. Later work from Islam et al. (2022) extended this approach to offline RL. Less closely related are Pathak et al. (2017) which uses inverse dynamics in the context of exploration and Baker et al. (2022); Venuto et al. (2022) which use an inverse dynamics model to label video data with actions for imitation.

Figure 1: **(a) A graphical models of the setting. Shaded nodes indicate observed variables. The expert behavior (i.e., \(o_{i} a_{i}\)) is determined by an unobserved context variable \(c\) while the transition dynamics (i.e., \((o_{i},a_{i}) o_{i+1}\)) are determined by the environment dynamics. (b)-(e) illustrate the candidate algorithms. We use blue to indicate inputs to the algorithm and green to indicate prediction targets. ID = inverse dynamics, BC = behavior cloning, FD = forward dynamics, Cont = contrastive learning. (f) Shows success of policies finetuned on top of various representations averaged across all datasets in our suite for default dataset sizes. Inverse dynamics (shown in green) is the only method to substantially outperform the baseline of training from scratch (shown in black). Further details about the experimental protocol and results are in Sections 4 and 5.**

Another, perhaps simpler approach is to use behavior cloning as a pretraining algorithm. Arora et al. (2020) shows that this can be a well-motivated approach to pretraining a representation when the task variable is observed. Other work uses behavior cloning objectives to pretrain representations of temporally extended actions (Ajay et al., 2020) or priors for offline RL (Zang et al., 2022).

A third approach is to model the forward dynamics of the system as a pretraining objective. Most directly related to our work, Nachum and Yang (2021) show that this is a well-motivated technique for imitation learning and provide empirical evidence on single task atari games, but do not compare to inverse dynamics. This technique has also been explored in empirical work for online and offline RL (Schwarzer et al., 2021; Laskin et al., 2020; Aytar et al., 2018; Lee et al., 2022; Wu et al., 2023). Finally, a method which we will refer to as static observation modeling does not leverage information about dynamics and rather directly uses self-supervised methods from computer vision (Pari et al., 2021; Chen et al., 2020; Grill et al., 2020). This approach does not take advantage of any additional structure in an imitation learning setting, but has nevertheless worked well in some settings.

Several empirical studies of representation learning for decision-making already exist. Most closely related to this work, (Chen et al., 2022) conducts an empirical evaluation of representations for imitation and finds that none of them consistently outperform training directly from pixels. However, this prior work (a) considers much larger finetuning datasets which can dramatically reduce the benefits of pretraining, and (b) considers different environments than we do, where the gap between pretraining and finetuning tasks is less controlled. Another line of work like Nair et al. (2022) attempts to pretrain general representations using large human-collected video datasets like Ego4d (Grauman et al., 2022). In contrast, we focus on a more carefully controlled (albeit smaller scale) experimental settings where we can derive a more clear understanding of the relative merits of different pretraining objectives. Another empirical study from Stooke et al. (2021) considers representations in online reinforcement learning. Meanwhile, Yang and Nachum (2021) considers representations for imitation but does not consider image-based or multitask problems. Moreover, none of these works includes a theoretical understanding for the findings presented therein.

A further discussion of pretraining in the context of imitation can be found in Appendix A.

## 3 Problem setup

Here we present the formal setup for our problem setting of reward free pretraining from multitask expert data. We formalize this as a contextual MDP with rich (i.e., visual) observations where the latent context determines the initial state and reward functions.

Environment.We model the environment as a contextual MDP with context-independent dynamics:

\[c P_{c}, o_{0}_{c}, r_{i}=r_{c}(s_{i},a_{i}), o_{ i+1} T(o_{i},a_{i}).\] (1)

Importantly, we consider the context variable \(c\) and rewards \(r_{c}\) to be _latent_, i.e., they are not available during training, and only used to evaluate a learned policy. At a high level, this captures the setting where the task (defined by the context variable) may change, but the dynamics of the world do not. For example, the context variable could be a continuous variable like a goal position that the expert is navigating towards or a discrete variable representing a behavior like locking a door.

Data generation.Data is generated by executing policies \(\) that map observations to actions in the environment. We consider two different datasets for any given problem. First there is a large multi-context pretraining dataset that will be used for representation learning, specifically to learn an observation encoder. Second, there is a small single-context finetuning dataset for policy learning on top of the pretrained representation. The multi-context pretraining data is generated as follows:

\[D_{pre}=\{_{i}\}_{i=1}^{N_{pre}}: c P_{c},=(o_{0},a_{0}, o_{1}) P^{_{c}},_{c}_{c}^{*}=_{}J_{r_{ c}}(),\] (2)

where \(J_{r_{c}}()\) denotes the expected return of \(\) when the reward is \(r_{c}\). Note that the demonstration policy has access to the latent context \(c\), but this latent context is not observed in the data.

Then the single-context finetuning data is generated for context \(c_{fine}\) as follows:

\[D_{fine}=\{_{i}\}_{i=1}^{N_{fine}}:=(o_{0},a_{0},o_{1}) P^{ _{c_{fine}}}.\] (3)Pretraining.The goal of the paper is to analyze different methods for pretraining feature extractors. Training of the encoders \(\) to minimize a loss \(\) proceeds as follows:

\[:^{d}=_{}}{}[(,_{i})].\] (4)

A full description of the losses \(\) used by different algorithms will come in Section 4.2. For simplicity (and in keeping with prior work (Nachum and Yang, 2021; Chen et al., 2022)) we will consider \(\) to only be a function of transitions \((o_{i}^{j},a_{i}^{j},o_{i}^{j^{}})\) rather than full trajectories to leverage the Markovian structure. We also run some ablations of including multistep information in Appendix B and find little difference.

Finetuning.Features are evaluated by finetuning a small policy head on top of the frozen features:

\[_{}:^{d}=_{} }{}[(,a_{i}^{j},(o_{i}^{j}))].\] (5)

In all of our experiments, \(\) is the mean squared error loss for behavior cloning. We elect to use frozen features to allow for simple and clear evaluation of the representations. This is in keeping with prior work on representations for imitation (Nachum and Yang, 2021; Chen et al., 2020; Nair et al., 2022) as well as computer vision (Chen et al., 2020).

Evaluation.Finally, we evaluate the finetuned policy by performing rollouts in the finetuning environment with context \(c_{fine}\) to estimate \(J_{r_{c_{fine}}}(_{})\). In our tasks we usually consider \(r_{c_{fine}}\) to be a binary indicator of successful completion of the finetuning task.

## 4 Experimental setup

### Environments and Datasets

We design a suite of tasks and datasets to probe the capabilities of various representation learners for downstream imitation. We focus on robotic manipulation from vision as this is an important sequential decision making task that depends on learning task-relevant visual representations where pretraining deep visual feature extractors is a popular approach. Our suite consists of six different pretraining datasets on varied tasks and of varied size. Each pretraining dataset has several associated finetuning datasets and simulation environments that allow for online evaluation of learned policies.

All tasks are performed from visual inputs, as shown in Figure 2. Each pair of pretraining-finetuning datasets requires a slightly different type of generalization as dictated by the different types of context variable. Specifically, in the pointmass and pick+place datasets the context variable is a latent goal position, while in the door and metaworld datasets the context variable is a discrete identity of a desired behavior, and in the kitchen datasets the context variable is a discrete ordered sequence of subtasks. The datasets are described in full detail in Appendix C.

### Algorithms

We consider nine different representations across our suite of experiments. These representations include baseline and skyline/oracle performance as well as five representations that are pretrained on

Figure 2: Our six datasets: (a) Pointmass navigation with latent goals. (b) Pick and place with latent goals. (c) Multitask manipulation of a door. (d) Sequential kitchen manipulation. (e) Multitask manipulation of diverse objects, where we consider two different train-eval splits ML45 and R3M.

our own pretraining datasets described above. Each of the representations will be referred to by its bolded name after it is described.

All algorithms (except for the Imagenet and R3M baselines) share the exact same encoder architecture to control as best we can for variation in architecture between methods. Each method is pretrained for the same number of gradient steps. Additional training details can be found in Appendix C.

Skyline/oracle.As a skyline or oracle representation we directly use the low dimensional states (**States**) from the simulator. Depending on the task, this representation includes the position of the robot, position of the object to be manipulated, and/or position of the goal. A full description of the per environment state variables can be found in Appendix C.

Baselines.We consider three baseline representations that are not trained on our pretraining datasets. The first is to directly use the pixels with image augmentations (**Pixels + Aug**) to train an encoder and a policy from scratch on the finetuning data. It is essential to use the augmentations to ensure that this a strong baseline. The second is features of a ResNet18 pretrained on Imagenet (**Imagenet**). The last consists of the features of a ResNet18 that is specifically pretrained for robotic manipulation by Nair et al. (2022) on the Ego4d dataset (**R3M**).

Inverse dynamics.The primary representation learning objective that we consider is inverse dynamics (**ID**) which models the distribution \(P(a|o,o^{})\) using an architecture that first encodes \(o,o^{}\) with an encoder \(\) and then predicts \(a\) with a small MLP \(f\):

\[^{*}_{ID}=_{}_{f}}_{o,a,o^{}}[(a-f( (o),(o^{})))^{2}].\] (6)

Behavior cloning.A simpler alternative objective is to directly apply behavior cloning (**BC**) to the multitask actions in the pretraining dataset conditioned on the observations using MSE loss. The learner is parameterized as an encoder \(\) followed by a small MLP \(\):

\[^{*}_{BC}=_{}_{}}_{o,a}[(a-((o )))^{2}].\] (7)

Forward dynamics.We consider two representation learners that predict the forward dynamics of the system. The first is explicit forward dynamics (**FD-e**) which explicitly constructs a model of the forward dynamics in the space of observations by encoding the current observation and then attempting to reconstruct the next observation \(o^{}\) using a decoder \(d\):

\[^{*}_{EFD}=_{}_{d}}_{o,a,o^{}}[(o^ {}-d((o),a))^{2}].\] (8)

The second objective is implicit forward dynamics (**FD-i**) which implicity constructs a model of the forward dynamics using contrastive learning. Explicitly, we consider a form of contrastive learning where an energy function is defined as the inner product of L2-normalized projected embeddings (given by projection MLPs \(f_{1},f_{2}\)) which is then passed into an InfoNCE loss:

\[E(o,a,o^{}) =(f_{1}((o),a)^{}f_{2}((o^{}))),\] (9) \[^{*}_{IFD} =_{}_{f_{1},f_{2}}}_{o,a,o^{ }}[-(E(o,a,o^{}))+}_{^{ }}[E(o,a,^{})]].\] (10)

   Environment & \(N^{pre}_{traj}\) & \(N^{fine}_{traj}\) & \(N^{pre}_{context}\) & \(N^{fine}_{context}\) & \(N^{fine}_{seed}\) \\  Pointmass & (1e1, 1e2, **1e3**) & (1, **2**, 5, 10) & \(N^{pre}_{traj}\) & 5 & 1 \\ Pick + place & (1e1, 1e2, **1e3**) & (2, **5**, 10**, 20) & \(N^{pre}_{traj}\) & 5 & 1 \\ Door & (1e1, 1e2, **1e3**) & (2, 5, **10**, 20) & 3 & 1 & 5 \\ Kitchen & (50, 150, **450**) & (2, 5, **10**, 15) & 21 & 3 & 5 \\ MW-ML45 & (1e2, 1e3, **1e4**) & (2, 5, **10**, 20) & 45 & 5 & 5 \\ MW-R3M & (1e2, 1e3, **1e4**) & (2, 5, **10**, 20) & 45 & 5 & 5 \\   

Table 1: Description of the different datasets used in the experiments. Dataset sizes are measured in number of trajectories (\(N^{pre}_{traj}\) for pretraining and \(N^{fine}_{traj}\) for finetuning) and given as ranges with default values in bold. Trajectory lengths vary from 50 to 400 steps. These default sizes may vary in each experiment when indicated. Each datasets contains a certain number of latent contexts (\(N^{pre}_{context}\) and \(N^{fine}_{context}\)). For each finetuning context, we sample datasets with \(N^{fine}_{seed}\) different seeds.

Static observation modelingFinally, we consider a baseline that simply models \(P(o)\). Rather than modeling this explicitly with reconstruction, we use a contrastive loss (**Cont**) where we use image augmentations to construct pairs of \(o\) and \(\) that do not rely on the dynamics of the environment at all. Again we use the InfoNCE loss, in what can be seen as a variant of SimCLR:

\[E(o,o_{aug}) =(f((o))^{}((o_{aug}))),\] (11) \[^{*}_{Cont} =_{}_{f}}_{o,o_{aug}}[-(E(o,o_{aug}))+}_{_{aug}}[E(o,_{aug})]].\] (12)

## 5 Experiments

We want to determine which representation learning objective is best, but the precise answer will depend on the situation. To get a clearer understanding of this sometimes ambiguous performance we conduct a variety of controlled experiments on our diverse suite of datasets. We focus on the following questions to guide our empirical analysis:

1. How do factors of the datasets impact performance of algorithms?
2. How are the learned representations similar to and different from each other?

Note: we will focus on presenting aggregate statistics across all datasets in the main text, but full results can be found in Appendix B. Full details about the methodology are in Appendix C and code is at https://github.com/davidbrandfonbrener/imitation_pretraining.

### Impact of dataset on representation learning performance

Scaling with data size.The performance of each algorithm can be highly sensitive to both pretraining and finetuning sizes. Thus, instead of producing one simple summary statistic, we sweep over both the size of the finetuning data (for default pretraining size) and size of the pretraining data (for default finetuning size). The results of these sweeps are presented in Figure 3.

The sweeps both suggest that inverse dynamics outperforms the alternatives. First, on the finetuning size sweep, we see that the ID line is the only one that consistently outperforms training from scratch on Pixels + Aug. This gap is largest at small finetuning sizes, which are perhaps the most interesting case since that is when we expect pretraining to be useful. Second, the pretraining size sweep indicated that ID is scaling the most efficiently with pretraining size. Further results, including breakdowns across each dataset can be found in Appendix B.

In distribution vs. out of distribution eval tasks.The way that our datasets are constructed, the door, kitchen, metaworld-ml45, and metaworld-r3m datasets only have a finite number of possible contexts that is much smaller than the number of pretraining trajectories. For our default datasets, we elected to construct a train-test split of contexts to ensure that the contexts used for finetuning are

Figure 3: Average success rate after finetuning averaged across datasets, contexts, and seeds. Error bars show the standard error across contexts and seeds, averaged across datasets. The plots show sweeps across finetuning size with default pretraining size (left) or pretraining size with default finetuning size (right) measured in units according to Table 1. Methods that do not depend on pretraining size are shown as horizontal lines.

not seen during pretraining. As a result, the default finetuning tasks can be in some sense "out of distribution", measuring extrapolation as opposed to in-distribution generalization. For example, in the door dataset, we pretrain on door opening, closing, and unlocking (with varied door position) and then finetune on door locking (again with varied position).

To test the impact of this gap between pretraining and finetuning, we created alternative pretraining datasets, where we include the test contexts (but _not_ the test trajectories) into the pretraining data. For example, in the door domain we include door opening, closing, locking, _and_ unlocking in the pretraining data and still finetune on only unlocking (but with heldout initial conditions). These datasets now require a much more limited notion of generalization from pretraining to finetuning.

Results are shown in Figure 4. We again see that ID is the strongest performer, but now the gap is even larger. ID matches the skyline performance of training from ground truth low-dimensional simulator states. BC also shows substantially stronger performance and outperforms training from scratch Pixels + Aug. None of the other pretraining algorithms benefit much from the substantially easier type of generalization required on these datasets. This suggests that ID and BC are uniquely able to benefit in easier settings, suggesting that they are better representation learners. If an algorithm is not able to outperform training from scratch in this simplified setting, it is unlikely to be a good representation learner.

**Fully latent vs. inferrable context variables.** Looking at our dataset suite, the datasets can be divided into two groups: those where the context variable is not inferrable at all from the initial state (pointmass, pick+place, and kitchen), and those where the effect of the context variable on the initial state makes it possible to infer the context given the initial state (door, metaworld-ml45, and metaworld-r3m). This split presents an interesting comparison in particular between ID and BC (the best performing algorithms from the prior experiment). Figure 5 shows results for these algorithms and the Pixels + Aug baseline on the datasets where the context is latent.

There is a large gap between ID and BC when the context is fully latent. In these cases, it is impossible to tell from the current state alone what the context is and thus what the optimal action should be. As we will show in our simplified model (Section 6), in these settings BC is _confounded_ by the latent context (in the terminology of causal inference). As a result, BC can fail to learn useful features. In contrast, ID uses the information about the future state to deconfound the learning problem and still learns a good representation. Note that this gap largely disappears when the context is observable, see Appendix B for further details.

### Predictive power of the representations

So far, we have focused on the success rate of the downstream finetuned policy as the main metric of comparison between algorithms. Now we will instead consider a series of experiments that assess the quality of the representations based on the ability to predict various quantities of interest from the representations. These experiments help to illustrate what information is retained in the representations and how efficiently that information can be accessed.

Action prediction.First, we consider the ability to predict the expert actions in the finetuning dataset. This is directly related to the success of the finetuned policy, but avoids the variance of

Figure 4: Average performance on the four discrete context environments when the finetuning contexts are included in the pretraining data. The finetuning data contains heldout initial conditions and trajectories not seen during pretraining.

Figure 5: A comparison between ID and BC on the datasets where the context is not inferrable from the observation.

performing rollouts and allows us to compare train and validation errors to evaluate the representations. Low train loss means the representations are not aliasing observations that require different actions. Meanwhile the validation loss measures the simplicity of the function that maps representations to targets, i.e. how well it generalizes.

The results in Figure 6 show the train and validation loss during finetuning using the default pretraining and finetuning sizes from Table 1. Since losses vary across datasets, we normalize by the Pixels + Aug validation loss so as to be able to present averages across all datasets. We see that out of the learned representations, ID has both the lowest train and validation losses, almost matching the performance of Pixels + Aug on train and almost matching the performance of States on validation. In contrast, representations that attempt to predict forward dynamics have substantially higher train loss, indicating aliasing of states in terms of their optimal actions. Interestingly, the Imagenet pretrained features have very low train loss, indicating a lack of aliasing, but very high validation loss, indicating that the function that maps representations to actions does not generalize well.

State prediction.Since we perform all of our experiments in simulated environments, we have access to the ground truth low dimensional states. So, we can measure the ability of each representation to predict the ground truth low dimensional state and thus measure how well the representation retains information about this ground truth state. Results are in Figure 7; here we measure the train and validation loss on the pretraining distribution so as to isolate the effect of the representation learning apart from the gap between pretraining and finetuning. Again we normalize the losses for each dataset.

Again we see that ID and BC yield the best performance. This suggests that in these datasets, pretraining objectives that attempt to predict the optimal action do indeed facilitate recovery of the low-dimensional simulator state. In contrast, while the FD methods achieve approximately the same training error, they generalize much more poorly. This suggests that the FD objectives are not throwing away relevant information, but are keeping around too much extraneous information about the observations, thus making the representations susceptible to overfitting. Standard contrastive learning is substantially worse, even on train error, suggesting that it is throwing away important information. Extended results are in Appendix B.

## 6 Analysis

To add a more theoretical understanding of the empirical results, we will consider a simplified model of the data generating process based on linear dynamics in a latent space. We begin by presenting the model and then show that under this model we can explain three key experimental findings: (1) inverse dynamics is able to recover the low dimensional state, (2) forward dynamics can be less efficient in some cases, and (3) BC can be confounded by the latent context. We present a high level sketch here and more details along with discussion of related theoretical work are in Appendix D.

Model.Some of the key interesting properties of problems like visual manipulation that we consider empirically are that (a) the observation is very high dimensional relative to the action, (b) the actual state of the world (or simulator) can be summarized in a much lower dimensional state variable, and (c) the dynamics are relatively simple if given the right representation. All of these motivating

Figure 6: Average train and validation action-prediction loss during finetuning. All losses are normalized by the Pixels + Aug validation loss to maintain consistency across environments.

Figure 7: Average state prediction error on the pretraining distribution. Values are normalized by the ID train loss.

properties can be captured in a simplified model that assumes linear dynamics occurring in a hidden low-dimensional state space, as presented below.

For simplicity, we will only consider one step of the dynamics represented by a tuple \((o,a,o^{},s,s^{},c)\) that is sampled iid from the joint distribution over those variables. Recall that we only observe \((o,a,o^{})\) and that \((s,s^{},c)\) are latent. Formally, let \(=^{d}\), \(=^{}\), and \(=^{k}\) with \(d>k\). Let \(:\) be the ground truth encoder, which we assume is invertible by \(^{-1}\). Let \((0,)\) in \(^{}\) and \(A,B\) to be any matrices in \(^{}\) and \(^{ k}\). Then, assume that the dynamics are:

\[o^{}=^{-1}(A(o)+Ba+).\] (13)

Note that we make no assumption on the policy \(_{c}^{*}\) other than that it only depends on \(o\) via \((o)\). This model is similar to ones studied in the online control setting by Mhammedi et al. (2020); Dean and Recht (2021), but is different from models where inverse dynamics have been studied for online control with exogenous noise since the dynamics are entirely contained in the low dimensional state space (Efroni et al., 2021; Lamb et al., 2022).

Inverse dynamics recovers the state.To get an intuition as to why inverse dynamics learning is feasible in this model, note that if \(B^{+}\) is the pseudoinverse of \(B\) that:

\[a=B^{+}(o^{})-B^{+}A(o)-B^{+}.\] (14)

Thus the inverse dynamics are a simple linear function of the embeddings \((o),(o^{})\). As a result, when we solve for \(a\) with least squares regression, if the encoder \(\) is representable by our function class, we will be able to recover it up to linear transformation, provided the matrix \(B\) is well-conditioned, so that the noise term \(B^{+}\) does not blow up.

Forward dynamics can be less statistically efficient.Intuitively, the potential problem with learning forward dynamics is that it requires learning both an encoder _and_ a decoder while inverse dynamics _only_ requires learning the encoder. This is not necessarily a problem a priori, but we hypothesize that in practical problems of interest (like the ones in our experiments) the decoder (mapping from low dimensional state to high dimensional observation) may be more complicated than the encoder (mapping from observations to states).

To grasp why we might expect this, note that the set of possible observations is the manifold represented by the image of the decoder, i.e. \((^{-1})\). As a simple example, consider a toy 2d example where the high dimensional observation is \((x,f(x))^{2}\) and the low dimensional state is simply \(x^{1}\), as depicted in Figure 8. Here the encoder \(\) is very simple since it just needs to recover \(x\), while the decoder must learn \(f(x)\). Of course this is a very toy example, but we find it illustrative of the idea that it is possible that the encoder is much simpler than the decoder in practice.

BC can be confounded by the latent context.As we alluded to in the experimental section, the latent context variable can confound BC. Now we will show an example in our model where this problem arises. In this case, even with a linear encoder, infinite data, and a fully expressive policy class, the Bayes optimal BC representation cannot be used to recover anything better than a random policy. This example is extreme, but shows the shortcomings of a confounded pretraining objective.

For simplicity, let \(=k\) and \(=0\). Let \((^{k k})\) be the set of rotation matrices in \(^{k}\). Let \(^{k-1}\) be the unit sphere in \(^{k}\), \(U\) be the uniform distribution, and \(\) denote a Dirac delta. Now, assume:

\[c U((^{k k})), o U(^{-1}(^{k-1})),_{c}^{*}(a|o)=[a=c(o)]\] (15)

Note that \((o)\) returns a unit vector in \(^{k}\) and that a uniformly sampled rotation of a unit vector is a uniformly sampled unit vector. Thus, we can marginalize over \(c\) to get:

\[P(a|o)=_{c}P(c)_{c}^{*}(a|o)=_{c}P(c)[a=c(o)]=P_{U( ^{k-1})}(a)=_{k},\] (16)

for a constant \(_{k}\) equal to the reciprocal of the surface area of the unit sphere in \(^{k}\).

Figure 8: An example where the decoder is more complicated than the encoder.

Thus, the Bayes optimal BC policy does not depend on \(o\) at all. As a result, the optimal representation learned by BC can just map every observation to zero. This representation is not capable of representing the optimal policy for any choice of \(c\). However, switching to inverse dynamics pretraining where we condition on the outcome observation \(o^{}\) breaks the confounding and allows us to learn the true representation even without observing \(c\).

## 7 Discussion

We have seen that inverse dynamics pretraining provides an effective method for learning features from multitask demonstration data. We demonstrated this across a suite of datasets with visual observations and provided analysis in a simplified model to understand the strong empirical performance.

Limitations.There are still a few limitations of our work that are worth pointing out explicitly. First, in this work we prioritized simulated domains with large numbers of predefined tasks and datasets with a single morphology to allow for a variety of experiments. However, it is possible that the results we observed in these tasks would differ when scaled to real world tasks with additional visual diversity and physical realism. We leave this extension to future work.

Second, while our theoretical analysis provides a clear rationale for the observed empirical results in a toy model, there is clearly room for better theory. Ideally, future work could present a more rigorous theory that goes beyond a toy model. However, we do think that the toy model captures some of the essential characteristics of the problem and recognize that any theory must make simplifying assumptions.

Future directions.In addition to removing the limitations described above, there are many other interesting directions for future work to build on our results. One direction would be to extend these results to settings with suboptimal data. In this work we focus on an imitation learning setting where data is collected by expert policies across a variety of tasks. In future, it would be interesting to study how and if the properties of various representation learning algorithms change in the presence of suboptimal data.

It would also be interesting for future work to compare the relative merits of a broader array of pretraining techniques that go beyond representation learning. For example, methods that learn conditional generative models (e.g. goal-conditioning, language-conditioning, or reward-conditioning) provide a different paradigm for pretraining policies instead of the feature extractors that we consider in this work.

Finally, it would be interesting to consider developing new pretraining objectives for representation learning. This could be done by combining existing objectives or developing completely new ones.

#### Acknowledgments

This work was completed as part of the Google Research Collabs program. We would like to thank Mahi Shafiullah, Mark Goldstein, Aahlad Puli, Siddhant Haldar, Ben Evans, Ulyana Piterbarg, and Lerrel Pinto for helpful discussions and feedback.