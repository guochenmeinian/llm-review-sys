# LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Rendering and Control

Delin Qu\({}^{1,2}\)

Authors contributed equally: dluq22@m.fudan.edu.cn.

Qizhi Chen\({}^{3,2}\)

Corresponding author: dongwang.dw93@gmail.com.

Pingrui Zhang\({}^{1,2}\)

Xianqiang Gao\({}^{2}\)

Bin Zhao\({}^{2}\)

Zhigang Wang\({}^{2}\)

Dong Wang\({}^{2}\)

Xuelong Li\({}^{2}\)

\({}^{1}\)Fudan University

Shanghai AI Laboratory

Zhejiang University

###### Abstract

This paper scales object-level reconstruction to complex scenes, advancing interactive scene reconstruction. We introduce two datasets, OmniSim and InterReal, featuring 28 scenes with multiple interactive objects. To tackle the challenge of inaccurate interactive motion recovery in complex scenes, we propose **LiveScene**, a scene-level language-embedded interactive radiance field that efficiently reconstructs and controls multiple objects. By decomposing the interactive scene into local deformable fields, LiveScene enables separate reconstruction of individual object motions, reducing memory consumption. Additionally, our interaction-aware language embedding localizes individual interactive objects, allowing for arbitrary control using natural language. Our approach demonstrates significant superiority in novel view synthesis, interactive scene control, and language grounding performance through extensive experiments. Project page: https://livescenes.github.io.

## 1 Introduction

Interactive objects are prevalent in our daily lives, and modeling interactable scenes from the real physical world plays an essential role in various research fields, including content generation , animation , virtual reality , robotics , and world understanding . This paper tackles the challenging and rarely explored task of reconstructing and controlling multiple interactive objects in complex scenes from a single, casually captured monocular video without previous independent modeling of geometry and kinematics. Prior research on interactable scene modeling, such as CoNeRF  and K-Planes , typically adopts a joint

Figure 1: LiveScene enables scene-level reconstruction and control with language grounding. Left: Language-interactive articulated object control in Nerfstudio. Right: LiveScene achieves SOTA rendering quality on OmniSim dataset and exhibits a significant advantage in parameter efficiency.

modeling approach, combining spatial coordinates and all interaction variables as input and representing interactive scene by either implicit MLPs or feature planes. Meanwhile, CoGS  learns parameter offsets for different scene parts using multiple independent MLPs after establishing a 3D deformable Gaussian scene. However, these methods primarily focus on capturing interactions for a single object within a clear background, such as a single drawer, toy car, or face [66; 20; 63; 68]. As modeling extends from single objects to multiple objects in complex scenes, as shown in Figure. 1, the interaction spaces become increasingly high-dimensional, complicating these methods for accurate modeling and significantly increasing computational time and memory cost, e.g., \(4\) A100 GPU for 2 weeks to converge training in CoNeRF  and 500M Gaussian storage for a regular indoor living room in CoGS . Moreover, natural language is an intuitive and necessary interface for interacting with 3D scenes, but language embedding of interactive scenes faces an even more daunting challenge: interaction variation inconsistency. For instance, methods like LERF , and OpenNeRF , which distill CLIP features into static 3D fields, suffer from significant failures when confronted with scene topology structure changes induced by interactions, such as the distinct structures variation of a cabinet before and after opening.

To address these challenges, we propose LiveScene, the first scene-level language-embedded radiance fields, which compresses high-dimensional interaction spaces into compact 4D feature planes, reducing model parameters while improving optimization effectiveness. LiveScene models multiple object interactions via novel high-dimensional factorization, decomposing the scene into local deformable fields that model individual objects with multi-scale 4D deformable feature planes. To achieve independent control, we introduce a multi-scale interaction probability sampling strategy for factorized local deformable fields. Our interaction-aware language embedding method generates varying language embeddings to localize and control objects under arbitrary states, enabling natural language control. Finally, we construct the first scene-level physical interaction datasets, OmniSim and InterReal, featuring 28 scenes with 70 interactive objects for evaluation.

Experiment results show that our approach achieves SOTA novel view synthesis quality, outperforming existing best methods by +9.89, +1.30, and +1.99 in PSNR on the CoNeRF Synthetic, OmniSim #challenging, and InterReal #challenging subsets, respectively. Surpassing LeRF , LiveScene significantly improves language grounding accuracy by +65.12 of mIOU on the OmniSim dataset. Notably, our method maintains a lightweight, constant model parameter of 39M, scaling well with increasing scene complexity, as shown in Figure. 1. Contributions can be summarized as:

* We propose LiveScene, the first scene-level language-embedded interactive radiance field, which efficiently reconstructs and controls complex physical scenes, enabling manipulation of multiple articulated objects and language-based interaction.
* We propose a factorization technique that decomposes interactive scenes into local deformable fields and samples relevant 3D points, enabling control of individual objects. Additionally, we introduce an interaction-aware language embedding method that generates varying embeddings, allowing for language-based control and localization.
* We construct the first scene-level physical interaction dataset **OmniSim** and **InterReal**, containing 28 subsets and 70 interactive objects for evaluation. Extensive experiments demonstrate our SOTA performance and robust interaction capabilities.

## 2 Related Work

Dynamic Scene Representation.Extending NeRF  to dynamic scene reconstruction has made significant progress. Related methods can generally be categorized into _time-varying methods_, _deformable-canonical methods_, and _hybrid representation methods_. The time-varying methods [7; 41; 54; 65] typically model the radiance field directly over time, but struggle to separate dynamic and static objects. Deformable-canonical methods [11; 31; 42; 60] decouple dynamic deformable field and static canonical space, modeling 4D by warping points with deformable field to query the canonical features. However, these methods face challenges in scene topology changes . Hybrid representation methods, on the other hand, have achieved high-quality reconstruction and fast rendering by utilizing time-space feature planes [47; 10; 3], 4D hash encoding , dynamic voxels , or triple fields . Recently, several works [36; 62; 59; 27] have introduced 3D gaussians  into dynamic scene reconstruction, achieving high-quality real-time rendering speeds.

However, these methods are limited to reconstructing dynamic scenes and lack the ability to control and understand interactive scenes.

**3D Vision-language Fields.** Vision-language foundational models [45; 40; 25] with strong generalizability and adaptability inspires numerous language embedded scene representation for 3D scene understanding [2; 70], such as open-vocabulary segmentation [34; 13; 55; 24], 3D visual question answering [15; 6; 19; 18], and 3D language grounding [17; 46; 5; 18]. LeRF  is the first to achieve open-vocabulary 3D queries by combining CLIP  and DINO  with NeRF through feature distillation. Open-NeRF  introduces an integrate-and-distill paradigm and leverages hierarchical embeddings to address 2D knowledge-distilling issues from SAM. LEGaussians  and LangSplat  integrate semantic features into 3D gaussians  and achieve precision language query and efficient rendering. However, these methods are limited to static scene understanding and fail to generalize when the interactive scene topology changes.

**Controllable Scene Representation.** Manipulating reconstructed assets or neural fields is of significant importance for avatar and robotic tasks [8; 12; 20; 28]. CoNeRF  pioneered this effort by extending HyperNeRF  and introduce a fine-grained controlable neural field with 2D attribute mask and value annotations. CoNFies  proposes an automatic controllable avatar system and accelerates rendering by distilling. More recently, CoGS  leveraged 3D Gaussians  to achieve real-time control of dynamic scenes without requiring explicit control signals. However, these methods typically lack natural language interaction capabilities, relying solely on manual control. Furthermore, most works focus on single or few object interactions, disregarding the interaction between different scene parts, limiting their real-world applications.

## 3 Methodology

We aim to establish a representation that models \(\) interactive articulated objects in a complex scene from a monocular video via a rendering-based self-supervised manner. Control variables \(=[_{1},_{2},...,_{}]\) indicating object motion states and camera poses of each video frame are given. The overview of LiveScene is shown in Figure. 2. Section. 3.1 introduces the high-dimensional interactive space modeling and challenges. Section. 3.2 presents a multi-scale interactive space factorization and sampling strategy to compress the high-dimensional interactive space into local 4D deformable fields, and model complicated interactive motions of individual objects. Section. 3.3 introduces an interaction-aware language embedding method to localize and control interactive objects using natural language.

### Interactive Space

Assuming a non-rigidly interactive scene with \(\) control variables \(=[_{1},_{2},...,_{}]\) corresponding to \(\) objects, we delineate its representation by a high-dimensional function:

\[=(,,;),\] (1)

where \(\) is the model of the representation, \(^{3}\) are spatial coordinates, \(^{}\) are control variables, \(\) is a set of optional additional parameters (e.g., the view direction), and \(\) stores the scene information. The function outputs scene properties \(\) for the given position \(\) and \(\) sampling

Figure 2: The overview of LiveScene. Given a camera view and control variable \(\) of one specific interactive object, a series of 3D points are sampled in a local deformable field that models the interactive motions of this specific interactive object, and then the interactive object with novel interactive motion state is generated via volume-rendering. Moreover, an interaction-aware language embedding is utilized to localize and control individual interactive objects using natural language.

from a ray \(\), where \(\) can be represented with color, occupancy, signed distance, density, and BRDF parameters. This paper focuses on color, probability, and language embedding. Distinguishing from 3d static scene or 4d dynamic scene modeling, the sampling point \(=[|]^{3+}\) in the interactive scene is high-dimensional and variable in topological structure, complicating the scene feature storage and the optimization of representation model, leading to significant time-consuming or memory-intensive training in .

### Multi-scale Interaction Space Factorization

For the \((3+)\)-dimensional interactive space containing \(\) control variables, we aim to explicitly represent the high-dimensional space in a concise and compact storage, thereby reducing memory usage and improving optimization. As illustrated in Figure. 2 and Figure. 10, objects exhibit mutual independence, and interaction features are distributed in the \((3+)\)-dimensional interactive space and aggregate into cluster centers. Thus, there exists a set of hyperplanes that partition the space into disjoint regions, with each region containing a local 4D deformable field, as shown in Figure. 3. Hence, the interaction features at sampling point \(^{3+}\) can be projected into a compact 4-dimensional space \(^{4}\) in Figure. 3 by a transformation.

**Multi-scale Interactive Ray Sampling.** We consider using ray sampling to perform the projection transformation. As shown in Figure. 3, assuming a ray \((t)=[_{}|_{}]+t\,[ _{}|_{}]\) with origin \(_{}^{3},_{}^{}\), and direction \(_{}^{3},_{}^{}\), the ray intersects with the interaction region at a point \(=[|]^{3+}\), where \(^{3}\) is the 3D position and \(^{}\) is the interaction variables. For a given intersection point \(\), the deformable features can be retrieved from the corresponding local 4D deformable field by maximizing sampling probability \(\):

\[_{u}=[|(u)]\,, u=*{arg\,max} _{i}\{_{i}\},=(,),\] (2)

where \(_{u}\) and \(\) are the 4D sampling point and probability features at position \(\) from 3D feature planes. The maximum argument operation of probability decoder \((,)\) maps the interaction variables \(\) to the most probable cluster region in the 4D space, and can be optimized by minimizing the focal loss \(_{}\) of mask across all the training camera views:

\[_{}=(1-e^{_{i=1}^{} _{i}(}_{i})})^{}(-_{i=1}^{} _{i}(}_{i})),\] (3)

where \(\) is the ground truth mask label, \(}\) is the probability map rendering from the interactive probability field, \(\) is the balancing factor, and \(\) is the focusing parameter.

Next, the deformable features are used to render local deformable scene color at the sampling point \(\). In this way, the high-dimensional interaction features are factorized into a 4D space by a lightweight transformation modeling with interaction probability decoder and 3D feature planes in Figure. 2, supervised by deformable masks \(\). Moreover, leveraging K-Planes , the multiple 4D local deformable space can be further compressed in only \(_{4}^{2}=6\) feature planes. We iteratively sample from coarse to fine within the multi-scale feature plane, retrieving the maximum probability interaction variables \(_{u}\) and indices \(u\) at each scale.

**Feature Repulsion and Probability Rejection.** A latent challenge is that optimizing the interaction probability decoder with varying masks can lead to blurred boundaries in the local deformable field, further causing ray sampling and feature storage conflicts. As illustrated in Figure. 4(a), consider two adjacent local deformable regions \(_{i}\) and \(_{j}\), and a point \(\) in high-dimensional space, suppose \(\) moves from the cluster center of \(_{i}\) towards the cluster center of \(_{j}\), then the probability of \(\) belonging to \(_{i}\) gradually decreases, while the probability of \(\) belonging to \(_{j}\) increases. To avoid sampling conflicts and feature oscillations at the boundaries, we introduce a repulsion loss for ray

Figure 3: Illustration of hyperplanar factorization for compact storage. We maintain multiple local deformable fields for each interactive object region \(_{i}\), and project high-dimensional interaction features into a compact 4D space, which can be further compressed into multiscale feature planes.

pairs \((_{i},_{j})\), and amplify the feature differences between distinct deformable regions, promoting the separation of deformable field:

\[_{}=(K-\|(_{i}_{j}) _{i}-_{j}\|),\] (4)

where \(K\) is a constant hyperparameter, \(_{i}\) and \(_{j}\) are the ground truth mask of rays. \(_{i}\) and \(_{j}\) are the last-layer features of interaction probability decoder in Figure. 2. During training, we randomly select ray pairs and apply \(_{}\) to enforce the separation of interactive probability features across local deformable spaces. The initiative is inspired by , which demonstrated the efficacy of repulsive forces in disambiguating 3D segmentation results.

Additionally, the probability rejection is proposed to truncate the low-probability samples if the maximum deformable probability \(\) at sample \(\) is smaller than threshold \(s\), and selects the background feature directly. The operation is defined as:

\[u=\{*{arg\,max}_{i}\{_{i}\},& _{i} s\\ -1,&..\] (5)

As shown in Figure. 4(b), the proposed operations help the model achieve higher rendering quality, demonstrating their effectiveness in alleviating boundary sampling conflicts.

### Interaction-Aware Language Embedding

Language embedding in interactive scenes is complex and storage-intensive, as 3D distillation faces the dual challenges of high-dimensional optimization and interaction scene variation inconsistency, such as the distinct topological structures of a transformer toy before and after transformation, leading to the failure of SAM  segmentation or LERF  grounding. As shown in Figure. 2, leveraging the proposed multi-scale interaction space factorization of 3.2, we efficiently store language features in lightweight planes by indexing them according to maximum probability sampling instead of 3D fields in LERF. For any sampling point \(\), we project it onto \(_{u}=[|(u)]\), retrieving a local language feature group by index \(d\), and perform bilinear interpolation using \(_{u}\) to obtain a language embedding that adapts to interactive variable changes from surrounding clip features. By interpolating language embeddings, our method not only perceives topological structure changes but also achieves a storage complexity of \((C)\), much smaller than language distillation methods like LERF that operate in 3D scenes, where \(\) is the dimension of CLIP feature.

## 4 Dataset

To our knowledge, existing view synthetic datasets for interactive scene rendering are primarily limited to a few interactive objects [66; 20; 63; 68] due to necessitating a substantial amount of manual annotation of object masks and states, making it impractical to scale up to real scenarios involving multi-object interactions. To bridge this gap, we construct two scene-level, high-quality annotated datasets to advance research progress in reconstructing and understanding interactive scenes: **OmniSim** and **InterReal**, as shown in Figure. 5. Besides, we use the CoNeRF Synthetic and Controllable  dataset for evaluation as well. **1) OmniSim Dataset** is rendered through OmniGibson  simulator, leveraging 7 indoor scene models: #rs, #ihlen, #beechwod, #merom, #pomaria, #wainscott and #benevolence. By varying the rotation vectors of the articulated objects' joints and the camera's trajectory within the scene, we generated 20 high-definition subsets, each

Figure 4: Illustration of a) boundary sampling conflicts, b) rendering quality comparison.

consisting of RGBD images, camera trajectory, interactive object masks, and corresponding object state quantities at each time step. **2) InterReal Dataset** is captured from 8 real Interactable scenes and finely annotated with interaction variables and masks, camera poses encompassing multiple objects, and articulated motion variables. More details can be found in the supplementary.

## 5 Experiment

Baselines.We compare LiveScene with the existing 3D static rendering methods , 4D deformable methods , and controllable scene reconstruction methods . Note that we reimplemented CoGS  based on Deformable Gaussian  since the official code is unavailable. Additionally, we extended K-Planes  from \(_{4}^{2}\) planes to \(_{3+}^{2}\) planes, denoted as MK-Planes, where \(\) represents the number of interactable objects in dynamic scenes. By leveraging the fact that each instance occupies a distinct region, we further compressed the model, denoted as MK-Planes\({}^{}\), requiring only \(3+3\) planes.

Implementation.LiveScene is implemented in Nerfstudio  from scratch. We represent the field as a multi-scale feature plane with resolutions of \(512 256 128\), and feature dimension of 32. The proposal network adopts a coarse-to-fine sampling process, where each sampling step concatenates the position feature and the state quantity as the query for the 4D deformation probability field, which is a 1-layer MLP with 64 neurons and ReLU activation. We use the Adam optimizer with initial learning rates of 0.01 and a cosine decay scheduler with 512 warmup steps for all networks. The model is trained with an NVIDIA A100 GPU for 80k steps on the OmniSim dataset and 100k steps on the InterReal dataset, using a batch size of 4096 rays with 64 samples. More implementation can be found in the supplemental materials.

### View Synthesis Quality Comparison

Evaluation on CoNeRF Synthetic and Controllable Datasets.We report the quantitative results on CoNeRF Synthetic and Controllable scenes in Table. 1. LiveScene outperforms all the existing PSNR, SSIM, and LPIPS metrics methods on CoNeRF Synthetic scenes with a large margin. In particular, LiveScene achieves 43.349, 0.986, and 0.011 in PSNR, SSIM, and LPIPS, respectively, outperforming the second-best method by 9.894, 0.009, and 0.053. On CoNeRF Controllable, LiveScene achieves the best PSNR of 32.782 and comparable SSIM and LPIPS to the SOTA methods. According to the novel view synthesis results in Figure. 6, LiveScene achieves more detailed and higher rendering quality, demonstrating the effectiveness of LiveScene in modeling object-level interactive scenarios.

Evaluation on OmniSim Datasets.OmniSim dataset is categorized into 3 interaction level subsets: #easy, #medium, and #challenging, based on the number of interactive objects in each scene. As shown in Table. 2, LiveScene achieves the best PSNR, SSIM, and LPIPS on all interaction level subsets of OmniSim, with average PSNR, SSIM, and LPIPS of 33.158, 0.962, and 0.074, respectively. Notably, substantial performance degradation is observed across all methods as the quantity and complexity of interactive objects increase, e.g., CoGS  experiences a 3.641 dB PSNR drop from

Figure 5: **Overview of the OmniSim and InterReal datasets**.

#easy to #challenging. While LiveScene maintains a relatively stable high performance across all subsets, demonstrating its robustness in modeling complex interactive scenarios.

**Evaluation on InterReal Datasets**. We divided InterReal dataset into #medium and #challenging subsets. In Table. 3, CoGS  underperforms compared to LiveScene on the #medium subset and fails to converge when faced with long camera trajectories and a large number of interactive objects in the scene (#challenging), highlighting the limitation of existing methods in modeling real-world interactive scenarios. In contrast, LiveScene achieves the highest PSNR of 28.436 and the lowest LPIPS of 0.185 on the #challenging subset, indicating its superiority in modeling real-world large-scale interactive scenarios.

**View Synthesis Visulization**. Figure. 7 presents the novel view synthesis results of LiveScene and the SOTA methods on OmniSim dataset. The results reveal that LiveScene generates more detailed results than SOTA methods, particularly in complex interactive scenarios. For instance, on the #pomaria scene featuring an openable dishwasher, CoNeRF  fails to capture details, while CoGS  and MK-Planes\({}^{}\) exhibit residual artifacts. In contrast, our method accurately reconstructs the internal details. Another challenge arises in the #rs scene, where other methods struggle to reconstruct distant and static objects. In comparison, our method not only overcomes the challenging problem of dramatic topology changes in interactive scenes but also maintains the ability to reconstruct high-quality static scenes.

    &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  NeRF  & 25.299 & 0.843 & 0.197 & 28.795 & 0.951 & 0.210 \\ InstantNGP  & 27.057 & 0.903 & 0.230 & 26.391 & 0.884 & 0.278 \\
3DGS  & 32.576 & 0.977 & 0.077 & 25.945 & 0.834 & 0.414 \\  NeRF + Latent  & 28.447 & 0.939 & 0.115 & 32.663 & **0.981** & 0.182 \\ Inverfates & - & - & - & 32.274 & **0.981** & 0.180 \\ HyperResNet & 25.963 & 0.854 & 0.158 & 32.520 & **0.981** & 0.169 \\ K-Planes  & 33.301 & 0.933 & 0.150 & 31.811 & 0.912 & 0.262 \\  CoNeRF-\(A\) & 27.868 & 0.898 & 0.155 & 32.061 & 0.979 & **0.167** \\ CoNeRF & 32.394 & 0.972 & 0.139 & 32.342 & **0.981** & 0.168 \\ CoGS  & 33.455 & 0.960 & 0.064 & 32.601 & **0.983** & **0.164** \\ LiveScene (Ours) & **43.349** & **0.986** & **0.011** & **32.782** & 0.932 & 0.186 \\   

Table 1: **Quantitative results on CoNeRF synthetic and controllable datasets.** LiveScene achieves the best results in all metrics on synthetic scenes and the best PSNR on the controllable datasets.

    &  &  &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  NeRF  & 25.817 & 0.906 & 0.167 & 25.645 & 0.928 & 0.138 & 26.364 & 0.927 & 0.128 & 25.776 & 0.916 & 0.153 \\ InstantNGP  & 25.704 & 0.902 & 0.183 & 25.627 & 0.930 & 0.140 & 26.367 & 0.920 & 0.143 & 25.706 & 0.914 & 0.164 \\ HyperNeRF  & 30.708 & 0.908 & 0.316 & 31.621 & 0.936 & 0.265 & 27.533 & 0.897 & 0.318 & 30.748 & 0.917 & 0.299 \\ K-Planes  & 32.841 & 0.952 & 0.093 & 25.438 & 0.954 & 0.100 & 29.833 & 0.937 & 0.118 & 32.573 & 0.952 & 0.099 \\  CoNeRF  & 32.104 & 0.932 & 0.254 & 33.256 & 0.951 & 0.207 & 30.349 & 0.923 & 0.238 & 32.477 & 0.939 & 0.234 \\ MK-Planes\({}^{}\) & 13.630 & 0.948 & 0.098 & 31.880 & 0.951 & 0.104 & 26.565 & 0.887 & 0.218 & 31.477 & 0.946 & 0.106 \\ MK-Planes & 31.677 & 0.948 & 0.098 & 32.165 & 0.952 & 0.099 & 29.254 & 0.933 & 0.119 & 31.751 & 0.949 & 0.099 \\ CoGS  & 32.315 & 0.961 & 0.108 & 32.447 & **0.965** & 0.086 & 28.701 & **0.970** & **0.073** & 32.187 & **0.963** & 0.097 \\ LiveScene (Ours) & **33.221** & **0.962** & **0.072** & **33.262** & **0.965** & **0.072** & **31.645** & 0.948 & 0.093 & **33.158** & 0.962 & **0.074** \\   

Table 2: **Quantitative results on OmniSim Dataset.** LiveScene outperforms prior works on most metrics and achieves the best PSNR on the #challenging subset with a significant margin.

    &  &  &  \\   & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  NeRF  & 20.816 & 0.682 & 0.190 & 21.169 & 0.728 & 0.337 & 20.905 & 0.694 & 0.227 \\ InstantNGP  & 21.700 & 0.776 & 0.215 & 21.643 & 0.745 & 0.338 & 21.686 & 0.769 & 0.245 \\ HyperResNet  & 25.283 & 0.671 & 0.467 & 25.261 & 0.713 & 0.517 & 25.277 & 0.682 & 0.480 \\ K-Planes  & 27.999 & 0.813 & 0.177 & 26.427 & 0.756 & 0.331 & 27.606 & 0.799 & 0.215 \\ CoNeRF  & 27.501 & 0.745 & 0.367 & 26.447 & 0.734 & 0.472 & 27.237 & 0.742 & 0.393 \\ CoGS  & 30.774 & **0.913** & 0.100 & **0.7** & **0.7** & **30.77** & **0.913** & 0.100 \\ LiveScene (Ours) & **30.815** & 0.911 & **0.066** & **28.436** & **0.846** & **0.185** & 30.220 & 0.895 & **0.096** \\   

Table 3: **Quantitative results on InterReal Dataset.** Our method outperforms others in most settings, with a significant advantage of PSNR, SSIM, and LPIPS on the #challenging subset.

### Language Grounding Comparison

We assess the language grounding performance on the OmniSim dataset using mIOU metric. Figure. 8 suggests that our method obtains the highest mIOU score, with an average of 86.86. In contrast, traditional methods like LERF  encounter difficulties in locating objects precisely, with an average mIOU of 21.74. Meanwhile, 2D methods like SAM  fail to accurately segment the whole target

Figure 6: **View Synthesis Visualization on CoNeRF Controllable Dataset**. The proposed method achieves higher-quality rendering results compared with the existing methods.

Figure 7: **View Synthesis Visualization on OmniSim Dataset**. We compare our method with SOTA methods on RGB rendering across three scenes: #rs, #hlen, and #pomaria. Boxes of different colors represent distinct interactive objects within the scene.

under specific viewing angles, as objects appear discontinuous in the image. Conversely, our method perceives the completeness of the object and has clear knowledge of its boundaries, demonstrating its advantage in language grounding tasks.

### Ablation Study

In this section, we present ablative studies to investigate the effectiveness of each component in LiveScene. We selected 1 scene from the #medium subset, 2 scenes from the #easy subset of OmniSim dataset, and 1 scene each from the #medium and #challenging settings for InterReal. Notably, ground-truth state quantities are only available in OmniSim, not in InterReal. Therefore, we use GT quantities on OmniSim and introduce a learnable variable on InterReal to infer state changes. Figure. 9 reports the rendering quality and grounding performance for #1 and #6.

**Effectiveness of components.** As illustrated in Table. 4, the multi-scale factorization significantly improves the rendering performance on both datasets, with PSNR on OmniSim increasing from 31.74 to 35.094, shown in #1. Introducing learnable variables for each frame (#2) yields corresponding improvements on InterReal dataset since this latent code can perceive the change in object states. The feature repulsion loss and probability rejection (#3 and #4) together make rendering quality better in InterReal as well as in OmniSim dataset. As for grounding, #5 shows that rendering embeddings along a ray [26; 69] struggles to locate objects precisely. Ensuring view consistency further boosts grounding performance on OmniSim, as demonstrated in #6.

**Probability field training.** We provide additional experiments in Figure. 10 of the disjoint regions to illustrate the learning process of the probability field from 0 to 1000 training steps. The

Figure 8: **Language Grounding Performance on OmniSim Dataset left): Our method gains the highest mIOU score. right): LiveScene’s grounding exhibits clearer boundaries than other methods.**

Figure 10: Learning process of the probability fields from 0 to 1000 training steps. The model progressively converges to the vicinity of the interactive objects, establishing interactive regions.

Figure 9: **Rendering and Grounding Performance for #1 and #6. above): Multi-scale factorization greatly boosts the performance of RGB rendering and geometry reconstruction. below): Without view consistency, the model struggles when objects have similar appearances.**

results demonstrate a clear trend that, as training advances, the proposed method can progressively converge to the vicinity of the interactive objects, thereby establishing interactive regions. With the establishment of the probability field, the model can focus on different interactive objects and guide the sampling process by maximizing probability, thereby achieving disentanglement of interactive scenes.

## 6 Conclusion and Limitation

We present LiveScene, the first language-embedded interactive neural radiance field for complex scenes with multiple interactive objects. A parameter-efficient factorization technique is proposed to decompose interactive spaces into local deformable fields to model individual interactive objects. Moreover, we introduce a novel interaction-aware language embedding mechanism that effectively localizes and controls interactive objects using natural language. Finally, We construct two challenging datasets that contain multiple interactive objects in complex scenes and evaluate the effectiveness and robustness of LiveScene.

**Limitations:** The control ability of LiveScene is limited by label density. Additionally, our natural language control is currently restricted to closed vocabulary, and it is inherently tied to the capabilities of the underlying foundation model, such as OpenCLIP. In future work, we plan to extend our method to enable open-vocabulary grounding and control, increasing the model's flexibility and range of applications.

Acknowledgements.This work is partially supported by the Shanghai AI Laboratory, National Key R&D Program of China (2022ZD0160101), the National Natural Science Foundation of China (62376222), and Young Elite Scientists Sponsorship Program by CAST (2023QNRC001).

    &  &  &  \\   & I & II & III & IV & V & VI & **PSNR** & SSIMU & **LIPIS** & **PSNR** & SSIMU & **LIPIS** & **Depth 1.1** \\ 
**60** & & & & & & & & 25.329 & 0.731 & 0.329 & 31.740 & 0.938 & 0.118 & 0.238 \\
**61** & & & & & & & 28.289 & 0.819 & 0.226 & 35.094 & 0.969 & 0.059 & 0.086 \\
**62** & & & & & & & 29.577 & 0.885 & 0.162 & & & \\
**63** & & & & & & & 29.959 & 0.883 & 0.131 & 34.989 & 0.969 & 0.059 & 0.085 \\
**64** & & & & & & & 30.123 & 0.884 & 0.132 & 34.977 & 0.967 & 0.061 & 0.086 \\ LiveScene & & & & & & & 30.591 & 0.896 & 0.115 & 35.254 & 0.971 & 0.037 & 0.042 \\   & & & & & & & & & mIOU \(\) & \\
**65** & & & & & & & & & 30.40 & & & 32.87 \\
**66** & & & & & & & & & 93.10 & & & 71.64 \\ LiveScene & & & & & & & & & & 93.02 & & & 78.52 \\   

* I: multi-scale factorization, II: learnable variable, III: feature repulsion \(C_{opt}\), IV: probability rejection, V: maximum probability embeds retrieval. VI: interaction-aware language embedding. \(\)\(\) denotes enable II for InterReal but disable for OmniSim.

Table 4: **Ablation Study on the subset of InterReal and OmniSim Datasets.**