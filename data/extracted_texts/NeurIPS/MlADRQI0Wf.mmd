# Implicit Regularization of Decentralized Gradient Descent for Decentralized Sparse Regression

Tongle Wu

The Pennsylvania State University

tfw5381@psu.edu &Ying Sun

The Pennsylvania State University

ybs5190@psu.edu

###### Abstract

We consider learning a sparse model from linear measurements taken by a network of agents. Different from existing decentralized methods designed based on the LASSO regression with explicit \(_{1}\) norm regularization, we exploit the implicit regularization of the decentralized optimization method applied to an over-parameterized nonconvex least squares formulation without sparse penalization. Our first result shows that despite nonconvexity, if the network connectivity is good, the well-known decentralized gradient descent algorithm (DGD) with small initialization and early stopping can compute the statistically optimal solution. Sufficient conditions on the initialization scale, choice of step size, network connectivity, and stopping time are further provided to achieve convergence. Our result recovers the convergence rate of gradient descent in the centralized setting, showing its tightness. Based on the analysis of DGD, we further propose a communication-efficient version, termed T-DGD, by truncating the iterates before transmission. In the high signal-to-noise ratio (SNR) regime, we show that T-DGD achieves comparable statistical accuracy to DGD, while the communication cost is logarithmic in the number of parameters. Numerical results are provided to validate the effectiveness of DGD and T-DGD for sparse learning through implicit regularization.

## 1 Introduction

Modern deep learning is generally in the over-parameterized regime where the models have significantly more parameters than available training examples . Although deep learning models exhibit remarkable performance in multiple domains, the theoretical understanding of optimization and generalization for deep learning is still limited. Recent studies show that despite being over-parameterized, gradient-based methods applied to minimize the emperical loss exhibit the implicit regularization phenomenon. For example, a line of works  shows that with certain initialization, networks trained with gradient descent (GD) land in the "kernel regime" and share similar behaviors to the kernel method. However, the literature  suggests that kernel regime analyses fall short in explaining the success of deep learning because neural networks analyzed in the kernel regime are almost linearized, thus hindering feature learning from data. Further, many works  start investigating the "rich regime", showing that GD with small initialization induces structures on the solution, such as sparsity and low-rankness, that better explains the generalization capability of NNs. However, all aforementioned results are limited to the centralized setting, where data are stored on a single machine. Practical constraints such as limited computing and storage resources, data privacy and security, and regulation rules make the centralized learning framework increasingly inadequate for contemporary applications. Although a variety of decentralized learning algorithms can be applied to NN training, the questions of which solution they can converge, along with its generalization performance, are largely unclear.

In this paper, we study the sparse learning problem  in the overparameterized regime, which shares many key characteristics with deep learning models but is more tractable to analyze, as a prototype for understanding the computation and statistical guarantees of decentralized learning algorithms. Specifically, we consider learning a sparse model \(^{}\) from its noisy linear measurements over \(m\) agents. These agents communicate over an undirected connected mesh network without a central coordinator, and each agent can only communicate with its one-hop neighbors. The \(i\)-th agent has its own \(n\) samples \(\{(_{i,j},y_{i,j})\}_{j=1}^{n}\). Each \(j\)-th data pair \((_{i,j},y_{i,j})\) is generated according to the noisy linear model

\[y_{i,j}=_{i,j}^{T}^{}+_{i,j}, i[m] j[n],\] (1)

where \(_{i,j}^{d}\) and \(y_{i,j}\) denotes respectively the \(j\)-th feature and its corresponding response at \(i\)-th agent, \(_{i,j}\) is the observation noise, and \(^{}^{d}\) is the sparse model parameter to be learned, common to all agents, and has only \(s\)\((s d)\) non-zero elements. We are interested in the high-dimensional setting where the ambient dimension \(d\) is substantially larger than the total sample size \(N:=mn\), i.e., \(d N\). By re-parameterizing \(=-\), the loss function can be formulated as minimizing the following regularization-free nonlinear least square problem:

\[F(,):=_{i=1}^{m}f_{i}(,), f_{i}(,):= \|_{i}(-)-_{i} \|^{2};\  i[m],\] (2)

where \(f_{i}(,)\) corresponds to the loss function of \(i\)-th agent. Problem (2) can also regarded as the supervised learning problem on the diagonal linear network of degree-2 .

Problem (2) is highly non-convex with \(\) and \(\), however, recent works  demonstrate that if the design matrix satisfies the restricted isometric property (RIP) condition, the centralized GD without any regularization can yield the statistically optimal estimator with properly chosen initialization scale, step size, and early stopping time. This intriguing phenomenon is derived from the implicit regularization of GD. Roughly speaking, with small initialization, the gap \(|u_{i}^{2}-v_{i}^{2}|\) would increase with iteration for coordinate \(i\) such that \(w_{i}^{} 0\), where the remaining ones stay small enough before early stopping. As a result, GD identifies the support of \(^{}\) as the algorithm processes. In the decentralized setting, general results from the pure optimization perspective can only certify convergence to the stationary points, implying neither global optimality nor generalizability. Given the encouraging result of GD achieved in the centralized setting, it is natural to ask if statistically optimal solutions are also computable by decentralized gradient-type algorithms, which algorithm can achieve the goal, and what are the regularity conditions.

This paper aims to analyse the renowned decentralized gradient algorithm (DGD) for minimizing (2) over the undirected mesh networks. The main contributions of this paper are detailed as follows.

* **Statistical guarantee.** It is well established that even for convex objectives, DGD cannot compute an exact minimizer. It only converges to the neighborhood of the solution whose radius depends on the step size. However, we show that under specific conditions--namely, if the global design matrix satisfies the RIP condition, the initialization scale is sufficiently small, and the network is sufficiently connected--the solution computed by DGD with early stopping is statistically optimal.
* **Computational complexity.** Our convergence analysis reveals that the early stopping time increases logarithmically with the ambient dimension \(d\). While network connectivity does not affect statistical error when it satisfies mild conditions, it does influence the stopping time of DGD to find the optimal estimator. Networks with poor connectivity will delay the early stopping time, and thus increase the iteration complexity.
* **Technical analysis.** Compared to the techniques used for analyzing the centralized GD , proving the convergence of DGD faces the following challenge. Because the consensus error terms induced from the mesh network result in a perturbed version of the multiplicative update. Compared with the exact multiplicative updates, the challenge is that the additional error term outside of multiplication prevents applying the centralized analysis directly. In addition, the error terms within the multiplication have more complicated consensus error terms than that of the centralized setting, which requires bounding the consensus error terms carefully to control these error terms that can achieve the same order statistical error. To achieve this goal, we separately control the consensus errors on support \(}\) and non-support \(}^{c}\) by the magnitudes of parameters on support \(}\) and non-support \(}^{c}\), respectively. Our fine-grained analysis for consensus errors is distinct from existing decentralized optimization analyses that bound the consensus errors uniformly. The additional error term also complicates the transfer of proof from the simplified non-negative \(^{}\) case to the general \(^{}\) setting as the centralized setting, we conduct a comprehensive induction process to both \(\) and \(\) simultaneously for general \(^{}\).
* **Truncated DGD.** We propose a communication-efficient truncated DGD (T-DGD) method that at each iteration, vectors being transmitted are truncated, keeping only \(s\) elements with the largest magnitudes nonzero. We prove that if each agent has sufficient samples and the signal-to-noise ratio is high enough, T-DGD can perform as well as the vanilla DGD while reducing communication complexity to logarithmic dependence on ambient dimension \(d\).

## 2 Related works

We categorize the existing works most relevant to our study into three main groups.

* **Implicit regularizations for sparse regression.** The recent study in  reparameterized the model parameter through overparameterized Hadamard product and discovered encouraged empirical performance by the first-order optimization algorithms. The statistical and convergence guarantees for this phenomenon are established in [36; 42] under mild conditions. Woodworth et al.  studied the impact of initialization scale on solutions. Scott et al. demonstrated the benefit of stochasticity of SGD in sparse regression and explored the impact of momentum in . The more recent process in understanding the linear diagonal networks can be found in references [7; 10; 12; 19; 23; 26; 43]. To the best of our knowledge, existing works have only discussed implicit regularizations induced by centralized optimization methods in linear diagonal networks. However, the question of whether decentralized algorithms induce implicit regularizations, and what type of implicit regularization they may induce has not been studied so far.
* **Decentralized sparse regression with explicit regularization.** For estimating ground truth \(^{}\) in high-dimensional sparse linear regression under the decentralized setting, Ji et al.  proposed DGD-CTA algorithm for tackling LASSO objective with consensus penalty and proved linear convergence rate to the neighbor of the statistical optimal estimator, but the convergence rate has polynomial dependence on ambient dimension \(d\). Further Sun et al.  proposed the NetLASSO based on the gradient tracking method and obtained \(d\)-independent convergence rate and optimal statistical accuracy. To complement work , Ji et al. proposed DGD-ATC by mixing the local gradients along iterations and achieved logarithmic dependence on \(d\) in . Maros et al.  proposed DGD\({}^{2}\) method based on a double mixing for solving decentralized LASSO and obtained similar theoretical guarantees in [17; 31]. To improve the computation efficiency, Maros et al.  integrated accelerated proximal gradient descent with gradient tracking to solve decentralized LASSO. Despite these developments, it remains unclear whether leveraging the unregularized overparameterization and implicit regularization of decentralized optimization methods can achieve the optimal statistical guarantee over mesh networks.
* **Implicit regularizations of decentralized optimization.** Implicit bias or regularizations of centralized optimization methods for overparameterized models have been extensively studied [13; 30], but only a few works have investigated the implicit regularization of decentralized optimization methods. Richards et al.  studied the implicit regularization for decentralized stochastic gradient descent for solving general unregularized convex problems. Zhu et al.  demonstrated that decentralized stochastic gradient descent implicitly executes the sharpness-aware minimization algorithm for general non-convex problems. Taheri et al.  studied the implicit regularization of DGD in overparameterized classification for separable data. Recent work  demonstrated the implicit regularization of the DGD\({}^{2}\) in solving the overparameterized matrix sensing problem. Different from these works, we establish the statistical and computational results for specific non-convex sparse regression problem.

## 3 Preliminaries

In this section, we will introduce the basic notations used in this paper, and then formulate the DGD for solving the problem (2). Finally, we provide the necessary assumptions and definitions for the decentralized sparse regression problem.

### Notations

Throughout this paper, we use \([m]\) to denote set \(\{1,,m\}\) for given positive integer \(m\), \(_{d}\) denotes \(d\)-dimensional vector that all elements are one and \(_{d}\) denotes \(d\)-dimensional identity matrix. For ground truth parameter \(^{}\), the relevant notations are support set \(:=\{j|w^{}_{j} 0\}\), positive support set \(^{+}:=\{j|w^{}_{j}>0\}\), negative support set \(^{-}:=\{j|w^{}_{j}<0\}\) and non-support set \(^{c}:=\{j|w^{}_{j}=0\}\), \(w^{}_{}:=_{j}}|w^{}_{j}|\) and \(w^{}_{}:=_{j}}|w^{}_{j}|\). \(^{d},_{}:=_{}\) where \(_{}\) denotes a vector equal to one for all coordinates \(j\) and equal to zero everywhere else. Symbol "\(\)" denotes Hadamard product that \(()_{j}=a_{j}b_{j},, ^{d}\). The averaged signal is defined as \(}^{t}:=_{i=1}^{m}^{t,i}\) and similar notations can be extended to \(}^{t},}^{t}\).

\(:=[_{1};;_{m}]\) denotes the concatenated sample matrix, where each row of \(_{i}\) represents one feature vector in agent \(i\). \(\|\|\) denotes the Frobenius norm for vector and the spectral norm (maximum singular value) for matrix. \(\|\|_{}:=_{i,j}|A_{ij}|\) denotes infinity norm. We use \(a=(b)\) to denote that inequality \(a Cb\) holds with some absolute constants \(C\) that do not depend on any parameters of the problem. The notation \(a b\) shares the same meaning as \(a=(b)\). Finally, we use \(a b\) if there exists a universal constant \(c\) such that \(a cb\).

### Method and Assumptions

We focus on DGD solving problem (2) over mesh network, modeled as an undirected graph \(=\{,\}\) where nodes \(=\{1,,m\}\) represent the set of agents and edges \(\) represent the communication links. An unordered pair \(\{i,j\}\) is included in \(\) if and only if there is a bidirectional communication link between agent \(i\) and \(j\). The set of one-hop neighbors for agent \(i\) is denoted by \(_{i}:=\{j|(i,j)\}\{i\}\).

DGD allows each agent to independently update its parameters based on local gradient descent and then synchronize with neighboring agents by weighted averaging these updates. The recursive iteration of DGD for each agent is described as follows.

\[^{t+1,i}=_{j=1}^{m}W_{ij}(^{t,j}- ^{t,j}(_{j}^{T}( _{j}(^{t,j}^{t,j}- ^{t,j}^{t,j})-_{j}) )),\  i[m];\] (3)

\[^{t+1,i}=_{j=1}^{m}W_{ij}(^{t,j}+ ^{t,j}(_{j}^{T}( _{j}(^{t,j}^{t,j}- ^{t,j}^{t,j})-_{j}) )),\  i[m],\] (4)

where the \(^{t,i}:=^{t,i}^{t,i}- ^{t,i}^{t,i}\) denotes the local estimator in agent \(i\) at \(t^{th}\) iteration, the initialization is \(^{0,i}=^{0,i}=_{d}, i[m]\) and \(\) is constant step size. \(\) is the nonnegative weight mixing matrix for the undirected mesh network, where \(W_{ij}>0\) if there is a link between agents \(i\) and \(j\), and \(W_{ij}=0\) otherwise. The mixing matrix \(\) related to the undirected graph satisfies the following assumption.

**Assumption 1**.: _The communication network \(\) is connected. The weight matrix \(=[w_{ij}]_{i,j=1}^{m}\) for this graph has the following properties: (i) \(w_{ij}=0\) for all pairs \((i,j)\) that are not in \(\); (ii) it is double stochastic that \(_{m}^{T}=_{m}^{T}\) and \(_{m}=_{m}\); (iii) the spectral gap \(:=\|-_{m}_{m}^{T}\| 1\)._

This assumption is common in decentralized optimization literature [17; 22]. We need the following RIP condition which is a key condition to obtain the optimal estimator for sparse regression.

**Definition 1**.: _The global design matrix \(/^{N d}\) satisfies the \((,s)\)-Restricted Isometry Property (RIP) if for any \(s\)-sparse vector \(^{d}\), there is \((1-)\|\|^{2}\|/\|^{2}(1+)\|\|^{2}\)._

The RIP condition was first introduced in the compressed sensing literature in  which is a little more restrictive condition to achieve optimal statistical rate than the restricted eigenvalue condition in . We inherit this assumption in the centralized setting [36; 42] to achieve optimal estimator error under the condition that parameter \(\) is upper bounded. Besides the global RIP condition, we have the following local RIP condition for local design matrices \(\{_{i}\}_{i=1}^{m}\).

**Definition 2**.: _The local design matrices \(_{1}/,,_{m}/^{n d}\) satisfy the local \((_{},s)\)-(RIP) condition, if for any \(s\)-sparse vector \(^{d}\) and any local design matrix \(_{i}/\), there is \((1-_{})\|\|^{2}\|_{i}/\|^{2}(1+_{})\|\|^{2}\)._

The definition of the local RIP condition is just for ease of proof presentation, as we do not necessitate any upper bound on the local RIP parameter \(_{}\).

## 4 Main Result

Based on the above method and assumptions, we now give theoretical guarantees of DGD in solving problem (2) for sparse regression problem (1) as follows.

**Theorem 1**.: _Considering the sequence generated by (3) and (4) based on DGD for solving problem (2) and \(>0\), if the global design matrix \(/\) satisfies \((,s+1)\)-RIP condition with bounded RIP parameter \(}\), the local design matrices \(\{_{i}/\}_{i=1}^{m}\) satisfy local \((_{},s+1)\)-RIP condition, and the mesh network satisfies assumption 1, the initialization satisfies \(\{1,}{(12d+1)^{2}},^{2}},^{2})^{2}},^{4}}{4}\}\), the constant stepsize \(\) satisfies_

\[\{}{64w_{}^{}}, }(1-}{2}})} {4w_{}^{}},}{2})^{ }}{w_{}^{}}\},\] (5)

_and the spectral gap \(\) satisfies_

\[^{}\{_{}+1},},^{}}{N} \|_{}}{8_{i}\|^{}}{n} \|_{}}\},\] (6)

_then after running \(t=()\) iterations. There would be_

\[|_{j}^{t}-w_{j}^{}| ()&\ jw_{}^{}()\\ (\{|(^{}}{N} )_{j}|,\|^{}}{N} _{}\|_{},\})&\ jw_{}^{}()\\ &\ j,\] (7)

_where \(:=\{\|^{}}{N}\|_{ },\},:=\{^{}}{5},960 \}\)._

* _Mechanism to promote sparsity._ The consensus errors induced from decentralized network complicate the multiplicative updates, which becomes inexact multiplicative updates as \(}^{t+1}=}^{t}(1-4( }^{t}}^{t}-^{}+}^ {t}+}^{t}))+^{t}\). Compared with the exact multiplicative updates of GD in , the challenge is that the extra error term \(^{t}\) outside of the multiplication prevents applying the centralized analysis trivially. In addition, the perturbation error terms \(}^{t},}^{t}\) within the multiplication are much more complicated than that of the centralized setting due to additional multiple consensus errors. This requires bounding the consensus error terms carefully, which should control the complicated perturbation errors \(}^{t},}^{t},^{t}\) not to be large. Thus, we can use network connectivity to control the consensus errors to bound these three perturbation errors small enough to make the distance between two trajectories obtained by inexact and exact multiplicative updates within statistical accuracy, which can promote sparsity in the decentralized setting. The detailed theoretical mechanism of promoting sparsity has been demystified in Proposition 3.
* _Statistical Guarantee._ Based on the result in (7) and conditions in (5) and (6), we can observe that if the initialization \(\) is small enough and network connectivity is sufficiently well, the DGD with early stopping can obtain the desired estimator for sparse ground truth parameter \(^{}\) that achieves the same order of statistical error as the centralized setting in . The formula in (7) not onlyillustrates that we establish the network-independent estimator error bound but also inherits the benefit of implicit regularization, which indicates that if the signal-to-noise is high enough, the statistical error is independent of ambient dimension \(d\). In contrast, existing results in decentralized LASSO methods [17; 18; 21; 31], have consistent dependence on \(d\) in any case.
* _Computational Complexity._ The iteration complexity of early stopping is network-dependent that is because the \(t=()\) has the dependence on stepsize, which should satisfy the condition in (5). This suggests that poorer network connectivity leads to higher computational complexity. Although the initialization has no dependence on network connectivity, \(\) has polynomial dependence on \(d\), and the dependence of complexity on \(d\) is just logarithmic, which is similar to DGD in solving LASSO in [17; 21] and improves the polynomial dependence on \(d\) in .
* _Dependence on network connectivity._ For accurate estimation, it is essential that the network should be well-connected, as specified in condition (6). When this condition is not satisfied, we can run multiple rounds of communication per iteration. It is observable that the smaller ratio between the global RIP parameter \(\) and the local RIP parameter \(_{}\), and smaller ratio between the local noise and the global noise magnitude, necessitate a higher degree of network connectivity. This can be understood from the perspective of heterogeneity, where smaller ratios indicate a significant disparity between local and global design matrices. Consequently, condition (6) is reasonable as it suggests that higher levels of heterogeneity necessitate improved network connectivity. In numerical experiments, we can observe that if \(\) does not satisfy the condition as (6), obtaining optimal statistical error is not achievable, which indicates the optimal statistical error undergoes a phase transition with the network connectivity.

Our results demonstrate the benefit of overparameterization for DGD. Theorem 1 shows that standard DGD is sufficient to provide a satisfied statistical estimator with efficient computation without gradient correction techniques. This finding challenges the widely held belief in decentralized optimization literature that extra techniques like gradient tracking and other gradient-correction-based methods are necessary for heterogeneous scenarios [39; 32]. The following corollary considers the well-known instance where the design matrix and noise are generated from sub-Gaussian distribution, which indicates that DGD with early stopping can achieve the minimax optimal statistical rate under the \(_{2}\) metric.

**Corollary 1**.: _Suppose that entries of global design matrix \(\) generated from i.i.d 1-sub-Gaussian distribution, and the total sample size satisfies \(N s(s+)\). The noise vector \(\) is generated from independent \(^{2}\)-sub-Gaussian entries, and the initialization is set as Theorem 1 with \(=(})\). If the spectral gap satisfies \(}\) and stepsize is set as \(=(}{m})^{}}{w_{}^{2}})\), then after running \(t=(^{*}}{(1- (}{2})^{})})\) iterations, the sequence generated by (3) and (4) based on DGD for solving problem (2) would obtain estimator that \(\|}^{t}-^{}\|}\) with probability at least \(1-}\)._

Corollary 1 indicates that in the sub-Gaussian setting, network-independent statistical error obtained by DGD matches optimal rate \((})\) under \(_{2}\) metric in the centralized setting . In this context, the condition for network connectivity implies that the smaller \(\) is required as the number of agents \(m\) increases. This is reasonable because when the total sample size \(N\) is fixed, an increase in the number of agents results in fewer samples assigned to each agent. Consequently, better network connectivity is necessary to achieve optimal estimation.

## 5 Communication Efficient DGD via Truncation

It is apparent that iterations in (3) and (4) of DGD, each agent has to transmit two \(d\)-dimensional vectors \(^{t,i}\) and \(^{t,i}\) to its neighboring agents per iteration. Because we are considering the high-dimensional regime where the feature has ultra-high dimension, which leads to the \((d)\) high communication complexity (in terms of the bits transmitted) for DGD. The primary idea is whether it is possible to transmit fewer partial elements instead of the entire \(d\)-dimensional vectors for \(^{t,i},^{t,i}\) in all rounds of communication. Since all elements of \(^{t,i}\) and \(^{t,i}\) equal to \(\) at initialization, we can utilize the one step of local gradient descent step in each agent to distinguish the support set and non-support based on changes of magnitudes for each element. The intuition is that the elements on the support would grow more rapidly than those on the non-support. Thus, we propose the Truncated **D**ecentralized **G**radient **D**escent (T-DGD) as

\[^{t+1,i} =_{j=1}^{m}W_{ij}_{s}((^{t,j} -^{t,j}(_{j}^{T}(_{j}(^ {t,j}^{t,j}-^{t,j}^{t,j})-_{j}) )));\] \[^{t+1,i} =_{j=1}^{m}W_{ij}_{s}((^{t,j} +^{t,j}(_{j}^{T}(_{j}( {u}^{t,j}^{t,j}-^{t,j}^{t,j})-_{j} )))),\] (8)

for \( i[m]\), where \(_{s}()\) is the operator that preserves only the \(s\) largest magnitude elements of the vector \(\) while setting all other elements to zero. The following proposition shows the benefit of T-DGD in sparse regression under proper conditions.

**Proposition 1**.: _With the same setup in Corollary 1, if the ground truth \(^{}\) satisfies \(^{}}{2}_{}w_{}^{}+ }\), then the sequence generated by T-DGD as (8) for solving problem (2) would obtain estimator that \(\|}^{t}-^{}\|}\) with probability at least \(1-}\). However, the communication complexity in terms of transmitted bits would be at most \((s(w_{}^{})( (1-(}{2})^{ })))\)._

To ensure condition \(^{}}{2}_{}w_{}^{}+ }\) satisfied, it is necessary to require that each agent has sufficient samples and SNR is high enough such that \(_{}^{}}{w_{}^{}}\) and \(} w_{}^{}\). This proposition enables each agent to transmit only \(s\) elements of \(d\)-dimensional vector per communication round, which can achieve optimal statistical rate and eliminate the \(d\)-linear increasing communication complexity. The result in Proposition 1 validates the usefulness of the Hadamard product over-parameterization in decentralized gradient-based optimization.

## 6 Numerical Results

This section conducts the experimental studies to evaluate the theoretical findings of DGD and T-DGD for solving problem (2) in Subsection 6.1, Subsection 6.2, respectively. In Subsection 6.3, we compare the effectiveness of implicit regularization of DGD with explicit regularization based decentralized methods. The communication networks \(\) are generated from Erdos Renyi (ER) graphs with link activation under given probabilities. By default, unless stated otherwise, all the design matrices \(\) have i.i.d. standard Gaussian elements, noise \(\) follows i.i.d. \((0,0.5^{2})\) distribution, and the magnitudes of elements on support \(\) are \(1\). All experiments are conducted on 12th Gen Intel(R) Core(TM) i7-12700@2.10GHz processor and 16.0GB RAM under Windows 11 system.

### Simulations on DGD

We organize the experiments as follows: 1) We visualize the dynamics of averaged variables and consensus errors that allow us to evaluate the implicit regularization of DGD and the soundness of our technical analysis. 2) We check whether DGD can achieve optimal statistical error, the impacts of ambient dimension \(d\) and initialization scale \(\) on statistical and computational properties. 3) We evaluate the condition of (6) that reveals the relationship between network connectivity and network scale for achieving the statistical accuracy of centralized setting.

* **Dynamics of \(}^{t},}^{t},}^{t}\) and \(^{t,i}-}^{t}\), \(^{t,i}-}^{t}\).** In this case, we set \(d=2000,s=10,m=10,N=400,=0.1778,=10^{-6}\). Fig. 1(a) demonstrates the convergence of averaged \(}^{t}\) in DGD, showing successful convergence of elements on support \(\) and maintenance of small magnitudes for elements on non-support \(^{c}\). Fig. 1(b) and Fig. 1(c) further illustrate how DGD utilizes \(}^{t}\) and \(}^{t}\) to fit parameters on positive and negative support, respectively. Additionally, the magnitudes of \(}^{t}\) and \(}^{t}\) on non-positive and non-negative support remain small enough as the initialization. Consensus errors \(^{t,i}-}^{t}\) and \(^{t,i}-}^{t}\) are depicted in Fig. 1(d) and Fig. 1(e), respectively. The trends in these curves correspond to the magnitudes of the model parameter, affirming the validity of our analysis.
* **Impact of \(d\) and \(\) on optimal estimation.** We vary the dimension of \(d\)\((4 10^{2},4 10^{3},4 10^{4})\) to access effect of \(d\) on both statistical and computational properties. With \(s= d\) and \(N\) chosen to satisfy \(s d/N 0.25\), we aim to maintain the same order optimal statistical error \((})\). Testing is conducted on two networks with \(m=20\) but different \(\). For each \(d\), we select the maximum initialization \(\) that achieves optimal statistical error, resulting in \(=10^{-8}\) for \(d=4 10^{2}\), \(=10^{-8.5}\) for \(d=4 10^{3}\) and \(=10^{-9}\) for \(d=4 10^{4}\). The results for \(=0.1778\) and \(=0.7519\) are displayed in Fig. 2(a) and Fig. 2(b), respectively. It is observable that DGD obtains estimators with statistical error matching that of the centralized setting, with computational complexity remaining largely unaffected by ambient dimension \(d\) across different network conditions. To assess the influence of \(\), we set \(d=2000,s=10,m=20,N=400\), and different values for \(\) on network with \(m=20,=0.1778\). The results in Fig. 2(c) illustrate that it is necessary to use small enough initialization to obtain optimal estimator. 
* **Dependence on \(\) and \(m\).** We set \(d=2000,s=10,N=200,=10^{-6}\) and test on networks with different numbers of agents. The results are shown in Fig. 3 where Fig. 3(a) and Fig.

Figure 1: Dynamics of avergaed variables and consensus errors.

Figure 2: Impact of ambient dimension \(d\) and initialization \(\).

3(b) display the performance with varied numbers of agents under fixed \(=0.9400\) and fixed \(=0.1778\), respectively. Fig. 3(a) indicates that DGD would not obtain the optimal estimator as the centralized setting when the number of agents is large which violates the condition in (6). When network connectivity is sufficiently connected as \(=0.1778\), Fig. 3(b) conveys that this can allow a larger scale of agents to attain optimal statistical error. In Fig. 3(c), we fix \(m=10\) and observe the phenomenon under varied \(\) by choosing proper stepsizes to achieve the best statistical error. Fig. 3(c) illustrates that \(\) would influence the stopping time when DGD can obtain the optimal estimator. The worse the network, the more iterations it takes to find the optimal estimator.

### Simulations on T-DGD

In this section, we evaluate the effectiveness of T-DGD. Initially, we vary the values of \(N\), keeping the other parameters consistent with the simulations in Fig. 1. Fig. 4(a) illustrates that when each agent has inadequate samples (\(N=100,n=10\)), T-DGD would fail in achieving optimal estimation. However, with increasing local samples (\(N=400,n=10\)), T-DGD matches DGD in both statistical accuracy and convergence performance. Subsequently, we set the magnitudes of the ground truth on support as \(100\) and \(N=300\). The performance is depicted as dashed lines in Fig. 4(c), indicating failure of T-DGD under higher noise level (\(=0.5\)). We further reduce the noise magnitude to \(=0.1\), and solid lines in Fig. 4(c) demonstrate the usefulness of T-DGD in sparse regression. These observations validate the statement in Proposition 1.

### Comparison with explicit regularization

We have compared our proposed method with three existing decentralized methods, namely: CTAC-DGD (LASSO) , ATC-DGD (LASSO) , and DGT (NetLASSO) . These methods are all derived based on the LASSO formulation with explicit regularization. The numerical results presented in Fig. 5 compare all four methods under three different network connectivity settings. For each method, we tuned the step size to achieve the best performance. Our proposed method demonstrated the best recovery performance in all network settings with minimal iterations.

Figure 4: (a) \(N=100\); (b) \(N=400\); (c) Different noise intensities.

We further compared T-DGD with existing methods with truncated versions of existing methods: Trun-CTA-DGD (LASSO), Trun-ATC-DGD (LASSO), and Trun-DGT (NetLASSO) which use the same Top-\(s\) truncation operator. As shown in Fig. 6, our proposed method is the only one to achieve successful recovery, while all other truncated decentralized methods failed. The numerical evidence demonstrates that naively combining sparsification with decentralized algorithms is _not granted to converge_. This is precisely one of the motivations of this work: to provide communication-efficient algorithms with both provably statistical and computational guarantees. This result also demonstrates the unique benefit of overparameterization and implicit regularization for decentralized learning setting, which has not been explored in the literature of learning theory.

## 7 Conclusion

In this paper, we study the implicit regularization of decentralized gradient descent for decentralized sparse regression in the unpenalized and overparameterized regimes. We establish both statistical and computational guarantees for the decentralized estimator under mild conditions of network connectivity, underscoring the utility of DGD in addressing overparameterized models. Furthermore, the proposed truncated DGD (T-DGD) offers a promising idea to reduce communication complexity while maintaining performance. In future work, exploring the possibility of relaxing the RIP condition in our assumption and leveraging the restricted eigenvalue condition to achieve optimal estimator in the decentralized setting is an interesting topic. Additionally, investigating alternative forms of implicit regularizations in decentralized optimization algorithms for more complicated overparameterized models is another intriguing direction.

Figure 5: Comparison with decentralized sparse solvers under varying communication network. The setting is \(d=1000,k=5,m=50,N=280,=0.5\) and magnitude of sparse signal is \(10\).

Figure 6: Truncated version: comparison with truncated decentralized sparse solvers. The setting is \(d=1000,s=5,m=50,N=550,=0.1,=0.2458\) and magnitude of sparse signal is \(10\).