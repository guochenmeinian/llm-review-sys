# Cost-aware Bayesian Optimization

via the Pandora's Box Gittins Index

 Qian Xie\({}^{1}\) Raul Astudillo\({}^{2}\) Peter I. Frazier\({}^{1}\) Ziv Scully\({}^{1}\) Alexander Terenin\({}^{1}\)

\({}^{1}\)Cornell University \({}^{2}\)Caltech

###### Abstract

Bayesian optimization is a technique for efficiently optimizing unknown functions in a black-box manner. To handle practical settings where gathering data requires use of finite resources, it is desirable to explicitly incorporate function evaluation costs into Bayesian optimization policies. To understand how to do so, we develop a previously-unexplored connection between cost-aware Bayesian optimization and the Pandora's Box problem, a decision problem from economics. The Pandora's Box problem admits a Bayesian-optimal solution based on an expression called the Gittins index, which can be reinterpreted as an acquisition function. We study the use of this acquisition function for cost-aware Bayesian optimization, and demonstrate empirically that it performs well, particularly in medium-high dimensions. We further show that this performance carries over to classical Bayesian optimization without explicit evaluation costs. Our work constitutes a first step towards integrating techniques from Gittins index theory into Bayesian optimization.

## 1 Introduction

Bayesian optimization is a framework for optimizing functions whose evaluation is time-consuming or expensive. It is widely used for hyperparameter tuning of machine learning algorithms , robot control , material design , and other areas. Bayesian optimization works by forming a probabilistic model for the objective function, and then chooses where to sample via an acquisition function that balances the explore-exploit trade-offs arising from uncertainty in this model.

We study _cost-aware_ Bayesian optimization, where one must pay a cost to acquire another sample and this cost may vary with where the function is evaluated. Costs are an important factor in practical scenarios. For instance, in hyperparameter tuning using GPUs rented from a cloud provider, training a neural network for twice as many epochs may carry twice the financial cost.

Despite its practical relevance, cost-aware Bayesian optimization is less-studied than standard Bayesian optimization, where budgets are framed in terms of the number of function evaluations and costs are not explicitly considered. Many existing theoretically-principled cost-aware approaches  rely on multi-step lookahead computations that are computationally expensive and can be numerically brittle, limiting their applicability. Other approaches lack a theoretical foundation and risk having poor performance on certain problems. For example, one of the most popular cost-aware acquisition functions used in practice, expected improvement per unit cost , has recently been theoretically shown by Astudillo et al.  to perform arbitrarily worse than the optimal policy. Thus, in the cost-aware setting, there is a need for theoretically-principled and computationally-straightforward acquisition functions with good empirical performance.

In this work, we develop such an approach. To do so, we introduce a novel link between cost-aware Bayesian optimization and a discrete-space decision problem from economics called the _Pandora's_Box_ problem [44; 13; 36]. The Pandora's Box problem admits an explicit Bayesian-optimal solution. We show how this solution can be used to develop a novel acquisition function class for two cost-aware Bayesian optimization settings: (i) _expected budget-constrained_ Bayesian optimization, where there is a constraint on the expected cost of the samples taken, and (ii) _cost-per-sample_ Bayesian optimization where the total costs incurred are subtracted from the final objective function value. The resulting acquisition functions are closely connected to expected improvement variants, but incorporate costs in a different, non-multiplicative way.

We evaluate the proposed acquisition function, termed the _Pandora's Box Gittins Index (PBGI)_, on a comprehensive set of experiments to understand its strengths and weaknesses. On both sufficiently-easy low-dimensional problems and too-difficult high-dimensional ones, performance is comparable to baselines. On most medium-hard problems of moderate dimension, the proposed acquisition function either matches or outperforms the strongest baselines. Surprisingly, we find this performance carries over to the classical setting with uniform costs. We also discuss limitations, including behavior on problems where baselines are stronger.

The Pandora's Box Gittins Index is a version of the _Gittins index_, a general framework for deriving optimal policies for a variety of bandit-like decision problems [43; 14; 22] which is widely-used in queueing theory and related areas [21; 1; 34]. Other versions of the Gittins index have recently been proposed for Bayesian optimization with an infinite-horizon discounted cumulative regret objective . Our Pandora's-Box-based formulation is instead designed for the simple regret objective more commo n in the literature . Our work thus opens a novel angle of attack for designing acquisition functions specialized to specific practical settings of interest.

Contributions.In this work, we (i) connect the Pandora's Box problem with a variant of cost-aware Bayesian optimization over a discrete search space. Using this connection, we (ii) explore the use of Gittins indices, which are Bayesian-optimal for the Pandora's Box problem, as an acquisition function for general cost-aware Bayesian optimization where data is incorporated via the posterior distribution. We (iii) demonstrate the resulting acquisition function has strong empirical performance on a variety of problems of moderate-to-high dimension, including the varying-cost problems it was designed for, as well as classical cost-unaware problems.

## 2 Cost-aware Bayesian optimization

In black-box optimization, we are interested in finding the global optimum of an unknown (potentially stochastic) function \(f:X\) defined on some compact domain, using pointwise function evaluations of \(f\) at locations \(x_{1},..,x_{T} X\) that we select sequentially. We are interested in policies that achieve low _simple regret_--see Garnett , Sec. 10.1--namely

\[_{x X}f(x)-_{1 t T}f(x_{t})\] (1)

where the expectation is taken over all sources of randomness in the function \(f\) and the policy, including the stopping time \(T\) and sequential selections \(x_{1},..,x_{T}\). In our setup, obtaining a new function evaluation at a point \(x\) carries a non-zero _cost_\(c(x)_{+}\), assumed automatically-differentiable unless discussed otherwise. We consider settings that integrate costs into the problem in different ways:

1. In the _expected budget-constrained_ setting, there is a budget \(B_{+}\), and the algorithm is not allowed to exceed this budget in expectation.
2. In the _cost-per-sample_ setting, at each time the algorithm must choose whether to pay a cost and obtain a new function evaluation, or to stop and return some previously-observed point. In this setting, we add the total sum of costs at termination time to the regret.

Note that the cost function \(c:X_{+}\) can be constant, which we term _uniform costs_. In this case, (a) reduces to standard black-box optimization with a finite time horizon, and (b) reduces to a variant of stopping-aware Bayesian optimization. These are not the only possible settings: one can also consider other variants including the almost-sure budget-constrained setting where the algorithm is not allowed to exceed the budget in a strict manner. Since we are interested primarily in the role of costs rather than stopping times in this work, we mostly work with budget constraints throughout this paper, especially almost-sure budget constraints for our empirical results. We will use the cost-per-sample setting as a conceptual framework with which to study budget-constrained settings.

### Probabilistic models and acquisition functions

_Bayesian optimization_ algorithms for solving various black-box optimization problems work by (i) building a _probabilistic model_ of \(f\)--that is, a probability distribution which quantifies what is known about \(f\) given the data points \((x_{t},y_{t})_{t=1}^{T}\) seen so far--where \(y_{t}=f(x_{t})\) are previous function evaluations--then (ii) using the model and its uncertainty to decide where to evaluate the unknown function next. For an introduction, see Frazier  and Garnett . Following standard practice, we work with Gaussian process models . Let \(f x_{1:t},y_{1:t}\) be the respective posterior distribution.

To decide where to evaluate \(f\) next, one uses the model to define a (potentially random) _acquisition function_\(_{t}:X\), which quantifies how promising a particular location is given what is known so far. We then evaluate \(f\) at

\[x_{t+1}=*{arg\,max}_{x X}_{t}(x),\] (2)

obtaining an additional data point that is used to reduce uncertainty and further improve the model.

### Expected improvement per unit cost

The most popular cost-aware acquisition function is _expected improvement per unit cost (EIPC)_, defined via

\[_{t}^{}(x)=_{f x_{1:t},y_{1:t}}(x ;_{1 t}y_{})}{c(x)}_{}(x; y)=(0,(x)-y)\] (3)

where we have written \(_{t}^{}()\) in terms of the general _expected improvement function_\(_{}\), defined with respect to some random function \(:X\), and a comparator point \(y\). With this notation, EIPC can be interpreted as the ratio of the expected improvement, with respect to the current posterior and using the best point seen so far as the comparator, to the cost.

In the uniform-cost case, where the costs \(c(x)=C_{+}\) are constant for all \(x\), this acquisition function reduces to the classical _expected improvement (EI)_ acquisition function \(_{t}^{}(x)=_{f x_{1:t},y_{1:t}}(x;_{1  t}y_{})\). In turn, expected improvement can be derived by considering the setup where the unknown function \(f\) is randomly sampled from the model's prior. If we imagine that the optimization process continues for only one more time step before stopping, maximizing expected improvement is the optimal strategy in this one-step-lookahead scenario.

Since EIPC reduces to EI in the uniform-cost case where \(c(x)=C\), it follows that it chooses the same points whether \(C=0.0001\) or \(C=1\,000\,000\). This is somewhat peculiar: one might expect that a cost-aware acquisition function should be more risk-averse if costs are high, and vice versa if they are low. Thus, EIPC is perhaps best suited to settings where heterogeneity of costs is the main factor at play: without heterogeneity, one can simply apply standard acquisition function variants . However, even with heterogeneous costs, Astudillo et al.  show there exist reasonable problems where EIPC performs arbitrarily worse than the optimal policy in an approximation-ratio sense, due to over-sampling low-value low-cost points. Analogous behavior can occur in multi-fidelity settings, which Wu et al.  argue can be especially undesirable in the presence of model-misspecification.

In spite of this rather negative outlook, EIPC has been shown to work well on many practical problems, is computationally efficient and reliable, and is the standard cost-aware choice in BoTorch . We therefore ask: _can one develop a technically-principled and computationally-straightforward alternative with at-least-comparable empirical performance?_

## 3 The Pandora's Box Gittins index for Bayesian optimization

To develop a cost-aware acquisition function, we study a simplified decision problem that captures key difficulties of the main problem but is tractable enough to yield analytic insights. An analogous strategy is used classically to derive expected improvement, by exactly solving a simplified one-step decision problem. We study a different simplified decision problem, which can also be solved exactly, but where the simplification is spatial rather than temporal in nature. Specifically, we connect Bayesian optimization with the _Pandora's Box_ problem from economics. To do so, we describe Pandora's Box in Section 3.1 and its solution in Section 3.2, showing along the way how these ideas can be reinterpreted from the view of Bayesian optimization. We illustrate this in Figure 1. Then, in Section 3.3, we use Pandora's Box to derive a novel class of cost-aware acquisition functions.

### The Pandora's Box problem

The _Pandora's Box_ problem  is a sequential decision-making problem. It begins with a finite set of boxes, which we collect into a set and label \(X=\{1,..,N\}\). Each box has a _hidden reward_, denoted by \(f(x)\), and an _inspection cost_, denoted by \(c(x)\). The rewards are given by mutually independent random variables whose distributions are known and vary between different boxes.

The decision-making process starts with a set of closed boxes, and proceeds in discrete time steps. At time \(t\), one can choose to do one of two things:

1. _Open a box \(x_{t} X\)_. This incurs cost \(c(x_{t})\), but reveals the exact value \(f(x_{t})\) of the reward inside the box, which is drawn using the box's respective reward distribution.
2. _Stop opening new boxes, and take the reward from the best opened box._ This ends the decision-making process, and yields a terminal reward equal to the maximum value among the boxes opened so far, with the convention that at least one box must be opened.

The policy's goal is to maximize the expected net utility, which is the reward of the best open box minus the expected total costs of all boxes opened so far, and is written

\[_{1 t T}f(x_{t})-_{t=1}^{T}c(x_{t})\] (4)

where \(T\) is a random variable that denotes the number of opened boxes, indicating that the policy terminates at time \(T+1\).

If we subtract the objective (4) from \(_{x X}f(x)\), which is constant with respect to the policy, we obtain the sum of the simple regret objective (1) defined in Section 2 and expected total costs. _The Pandora's Box problem is therefore equivalent to a special case of cost-aware black-box optimization_, specifically the cost-per-sample variant of Section 2, where (a) the domain \(X\) is a finite set, and (b) the objective function \(f\) is random, with independent \(f(x)\) and \(f(x^{})\) for \(x x^{}\). We will return to this point in the sequel, but first study the Pandora's Box problem in more detail.

### Optimally solving Pandora's Box

The Pandora's Box problem gives rise to an explore-exploit tradeoff: a policy must balance the opportunity gained from learning the value of the reward contained inside the box with the cost of opening it. Since the reward distributions are known, this tradeoff is captured within a Markov decision process (MDP). By general MDP theory, there exists an optimal policy describing which box, if any, one should open for a given configuration--we call such a policy _Bayesian-optimal_.

This MDP can be solved explicitly, with a remarkably simple solution, first derived in an economics setting by Weitzman . We start by associating with each box \(x X\) a number \(^{}(x)\) known as the _Gittins index_. Define

\[^{}(x)=g\] where

\[g\]

 solves

\[_{f}(x;g)=c(x)\] (5)

where \(_{f}(x;y)\), previously defined in (3) of Section 2, is the _expected improvement of \(x\) relative to \(y\)_--the same expression which appeared in the expected improvement acquisition function variants

Figure 1: An illustration of this work’s key idea. We view cost-aware Bayesian optimization as an extension of the Pandora’s Box problem, and derive the cost-aware acquisition function \(_{t}^{}\) by incorporating the posterior into the Bayesian-optimal Pandora’s Box acquisition function \(^{}\).

\(_{t}^{}\) and \(_{t}^{}\). Note that, unlike in those cases, \(^{}\) is _not time-dependent_ due to the lack of correlations or conditioning. Since \(_{f}(x;g)\) is strictly decreasing in \(g\), the root-finding problem (5) admits a unique solution for every value of \(c(x)\).

To understand what \(^{}(x)\) represents, consider a single closed box \(x\), and suppose there is a second, open box with reward \(f^{*}\). Is opening box \(x\) better than taking the reward \(f^{*}\) from the open box? This amounts to whether the expected improvement from opening \(x\) balances out the opening cost \(c(x)\): one can show that opening \(x\) is better if and only if \(_{f}(x;f^{*})>c(x)\). The value \(^{}(x)\) tells us _how large does the alternative reward \(f^{*}\) need to be, for stopping and taking it to be at least as good as opening box \(x\)_--a kind of _fair value_ which makes different boxes directly comparable to one another.

If we decide which box to open via the aforementioned fair values, we obtain the _Gittins index policy_, which proceeds as follows. At each time \(t\), let \(f_{t}^{*}=_{1 r t}f(x_{})\) be the maximum reward among all open boxes, and let \(x_{t}^{*}\) be the box of maximum Gittins index value \(^{}(x)\) among unopened boxes, with ties broken according to a given ordering. With this notation, using a tie-breaking rule that stops as early as possible--but noting that other tie-breaking rules, including stopping as late as possible, or stopping with some probability, are also valid--we get:

* If \(f_{t}^{*}<^{}(x_{t}^{*})\), the policy opens box \(x^{*}\).
* If \(f_{t}^{*}^{}(x_{t}^{*})\), the policy stops and receives terminal reward \(f_{t}^{*}\).

It turns out that opening boxes according to the order determined by their fair values, in the sense above, is not only a good idea, but is outright Bayesian-optimal. We state this formally as follows.

**Theorem 1** (Weitzman ).: _Let \(X\) be a finite set, let \(f:X\) be a finite-mean random function for which \(f(x)\) is independent of \(f(x^{})\) for \(x x^{}\), and let \(c:X_{+}\), without loss of generality, be deterministic. Then, for the cost-per-sample problem, the policy defined by maximizing the Gittins index acquisition function \(^{}\) with its associated stopping rule is Bayesian-optimal._

In the language of Bayesian optimization, this means that not only is there an explicit Bayesian-optimal policy for the Pandora's Box setting, but this policy also _takes the form of maximizing an acquisition function_. This gives an explicit solution for the cost-per-sample setting, thereby showing Pandora's Box fits our original goal of finding a simplified decision problem that sheds insights on cost-aware Bayesian optimization. For an alternative proof, see Kleinberg et al. , Theorem 1. Using Lagrangian relaxation, we show that the obtained solution carries over to the expected budget-constrained setting.

**Theorem 2**.: _Consider the expected budget-constrained problem, with the assumptions of Theorem 1. Assume the problem is feasible and the constraint is active, namely \(_{x X}c(x)<B<_{x X}c(x)\). Then there exists a \(>0\) and a tie-breaking rule such that the policy defined by maximizing the Gittins index acquisition function \(^{}()\), defined using costs \( c(x)\), is Bayesian-optimal._

A proof is given in Appendix B. This result extends a special case of Aminian et al. , Theorem 1. Compared to that work, we consider only Pandora's Box, but allow general reward distributions--including those with infinite support, such as Gaussian rewards. The optimal \(\) depends on the budget constraint \(B\) implicitly via a convex optimization problem given in Appendix B. In budget-constrained problems, we therefore view \(\) as a hyperparameter, which controls the degree to which the algorithm is risk-averse vs. risk-seeking--precisely what we argued was missing from EIPC in Section 2.

One can intuitively understand \(\) using a needle-in-haystack metaphor. Suppose one wants to find the best needle, but can only search a fraction of the haystack in expectation, represented by the budget \(B\). The key phrase is _in expectation_: if the search is not promising, one can stop early and avoid wasting budget, otherwise one can search more of the haystack. This prompts the question: in what situations should one continue searching? The answer depends on the interplay between the budget \(B\), costs \(c\), and best value \(f_{t}^{*}\) seen so far, and is encoded by \(\) in Theorem 2. Larger \(\)-values correspond to smaller budgets \(B\), incentivizing one to search less of the haystack. Crucially, the optimal acquisition function \(^{}\) behaves differently for different \(\)-values, and therefore for different budgets: roughly, smaller budgets cause \(^{*}\) to explore less. We will return to this in Section 3.3.3 and Figure 3.

### An acquisition function class for cost-aware Bayesian optimization

To adapt \(^{}\) to the Bayesian optimization setting, we need to handle two differences: (i) \(X\) does not need to be discrete, and (ii) a general probabilistic model is used for \(f\). Since Theorem 1 ostensibly requires \(f(x)\) to be independent of \(f(x^{})\) for all \(x x^{}\), the key question is _how to incorporate data and spatial correlations_ into \(^{}\). We propose to do so in the simplest and most obvious way: namely, at each time \(t\), we plug the posterior distribution \(f x_{1:t},y_{1:t}\) in place of \(f\). This yields three variants, depending on the precise cost-aware setting one is interested in:

1. _Budget-constrained_: define _Pandora's Box Gittins index (PBGI)_ acquisition function \[_{t}^{}(x)=g_{f x_{1:t},y_{1:t}}(x;g)= c(x)\] (6) and \(\) is a hyperparameter that should be tuned to match the evaluation budget \(B\).
2. _Cost-per-sample_: we can directly apply \(_{t}^{}\) in this setting as well, but now \(\) is instead interpreted as unit-conversion factor which ensures costs and rewards have the same units, and the Pandora's Box stopping rule is used for deciding when to terminate the optimization procedure and return the best observed value.
3. _Adaptive decay_: here, we do not have a pre-defined budget or cost-based stopping rule. Define the _Pandora's Box Gittins index with adaptive decay (PBGI-D)_ acquisition function \(^{}(x)\) analogously to \(^{}(x)\), but with a time-dependent \(_{t}\) schedule, set according to the Pandora's Box stopping rule. Specifically, \(_{0}\) is initialized to a given value, then set \[_{+1}=[}{}&^{*}_{}^{}(x_{}^{*};_{})$}\\ _{}&.\] (7) where \(>1\) is a decay factor that is used to decrease \(_{}\) at any time \(\) when the Pandora's Box stopping rule triggers. The advantage of this variant is that it can be more robust to different ranges of the policy hyperparameters: we discuss this further in Appendix D.3.

To understand this acquisition function class, one can think of it via the following approximation: for the general cost-aware Bayesian optimization problem, we (a) correctly incorporate observed data into the prior to obtain the posterior, but then (b) pick new samples according to the rule that would have been Bayesian-optimal if the posterior had no correlations. Said differently, \(^{}\) arises from exactly solving a simplified dynamic program, where the simplification is of a spatial nature, rather than the usual temporal lookahead. One can therefore expect this acquisition function to work best in situations where correlations are not the decisive factor for determining performance.

In what problems does this happen? In stationary kernels, correlations encode local dependence. Therefore, one can expect \(^{}\) to be approximately-optimal in settings where the key decisions involve choosing between different far-away data points. One can intuitively expect this to occur more often in high-dimensional problems, where the volume of the search space is large and most points are far away from each other. We will examine this point empirically in the sequel.

#### 3.3.1 Computation.

To compute \(^{}\) efficiently, note that \(y_{}(x;y)\) is monotone. As a result, the value \(g\) in (6) can be computed efficiently via bisection search. In Appendix B.2, we show that (i) the gradient of \(^{}\) can be computed straightforwardly via an explicit analytical expression without any additional optimization, and (ii) the resulting computational costs are closer to those of expected improvement than those of expensive multi-step-lookahead-based approaches.

Figure 2: A Bayesian optimization problem with varying costs on which LogEIPC—a numerically stable implementation of EIPC, see Section 4 and Appendix D.1—has poor performance, inspired by Astudillo et al. , Section A. The domain is \(X=[-500,500]\), which we visualize on the subinterval \([-5,5]\). Left: illustration of the non-uniform prior variance, which is given by a Matérn-5/2 kernel scaled by a narrow bump function. Center: the cost function, which is a narrow bump-shaped function. Right: median regret curves and quartiles for LogEIPC and PBGI. Legend refers only to regret curves.

#### 3.3.2 Extension to stochastic and non-automatically-differentiable costs.

One can show that Theorem 1 holds in the more general case where the cost function is random, as long as one substitutes \(c(x)\) with its mean. The same holds for Theorem 2, as shown in Appendix B.5. In this setting, therefore, stochasticity of \(c(x)\) affects performance, but does not change the optimal strategy. Building on these observations, we propose extensions of \(_{t}^{}\) to the more general setting where costs are modeled using a log-normal process in Appendix B.4.

#### 3.3.3 Qualitative behavior and comparisons.

Compared to cost-unaware acquisition functions such as EI, PBGI can be more risk-averse if costs are large or more risk-seeking if costs are small. In varying-cost budget-constrained settings, this tradeoff is mediated by \(\), and the obtained decisions can differ significantly from those of widely-used baselines such as EIPC. In particular, PBGI can make qualitatively different decisions on problems where there is a high-variance point with a large cost, among a set of many low-variance low-cost points. In Figure 2, we adapt the construction of Astudillo et al. , Section A into a one-dimensional Bayesian optimization problem with a non-stationary prior, and observe that EIPC--even in its numerically stable logarithmic form described in Section 4--has substantially worse performance than PBGI.

The PBGI acquisition functions depends on \(f x_{1:t},y_{1:t}\) through its mean and standard deviation at each point. We plot this in Figure 3. This shows for large \(\) that PBGI can resemble EI, whereas for small \(\) it is nearly linear. Specifically, in the \( 0\) limit, we have

\[_{t}^{}()_{t}()+_{t}() ()}{ c()}}\] (8)

where \(_{t}\) and \(_{t}\) are the mean and standard deviation of \(f x_{1:t},y_{1:t}\). This expression is similar to the upper confidence bound (UCB) acquisition function whose dependence is exactly linear, with heterogeneous learning rate parameter set to \(_{t}=(x)}{ c(x)}}\). A derivation is given in Appendix B.3. For small \(\), one can thus view PBGI as giving a way to automatically tune UCB's confidence parameter in a careful way depending on \(c(x)\).

## 4 Experiments

We now empirically evaluate the Gittins-index-based acquisition function on cost-aware problems. We also evaluate on the same problems with a spatially-constant cost function, a setting we term _uniform costs_--this facilitates comparisons with classical, cost-unaware baselines. In both cases, mirroring practical settings, we work with a deterministic, algorithm-independent evaluation budget.

We implement all methods in BoTorch  using Matern Gaussian processes with smoothness \(=5/2\). For Bayesian regret experiments where objectives are sampled from the prior, we fix the length scale to be \(=10^{-1}\) for both the prior and the posterior. For synthetic and empirical experiments, we apply maximum marginal likelihood optimization to dynamically adjust the length scale every

Figure 3: Left: contour plots showing how EI (left) and PBGI (center-left, center-right) depend on the posterior mean and standard deviation at a given point (lighter colors indicate higher values). We see that PBGI values high standard deviation more than EI. Right: PBGI performance across values of \(\), under the setup of the Bayesian regret experiment of Section 4 with \(d=8\). We plot the median of a set of samples using \(n=256\) random seeds, along with quartiles to show variability. We see that large \(\)-values decrease regret sooner, but eventually lose out to smaller \(\)-values.

iteration. To ensure that our results are not sensitive to these and other hyperparameter choices, all experiments were repeated with alternatives given in Appendix D.6. Each experiment was repeated for \(16\) seeds to assess variability, unless stated otherwise. Experimental details are in Appendix C.

We evaluate both PBGI variants from Section 3.3, namely \(^{}\) with \(=10^{-4}\), and \(^{}\) with \(_{0}=10^{-1}\) and \(=2\). To assure ourselves that performance differences are not primarily due to tuning, we deliberately use the same \(\)-values for PBGI and the same \((_{0},)\)-values for PBGI-D on all problems. Comparisons with other \((_{0},)\)-values can be found in Appendix D.3.

For varying-cost problems--that is, those with spatially non-constant cost--we compare with _(log) expected improvement per unit cost (LogEICC)_, _(log) expected improvement with cost cooling (LogEICC)_, _multi-fidelity max-value entropy search (MFMES)_ and _budgeted multi-step expected improvement (BMSEI)_, , which was shown in that work to have state-of-the-art cost-aware performance. For uniform-cost problems--that is, those with constant costs--we compare with _log expected improvement (LogEIC)_, _Thompson sampling (TS)_, _upper confidence bound (UCB)_, _max-value entropy search (MES)_, _knowledge gradient (KG)_, and _multi-step expected improvement (MSEI)_. We choose these because (i) they are standard, and (ii) acquisition function optimization succeeds for them on our problems, reducing confounding. We work with logarithmic expected improvement variants (LogEI, LogEICC, LogEICC), as they have recently been shown to have substantially better numerical and optimization properties than their traditional counterparts --see Appendix D.1 for details.

### Bayesian regret

For our first experiment, we examine how well the proposed acquisition functions perform on random functions sampled from the prior. To quantify the effect of problem difficulty, we vary the dimension of the domain \(X=^{d}\), and consider \(d\{8,16,32\}\). Results, in terms of empirical regret curves and their associated quartiles, are shown in Figure 4. Additional results for \(d=4\), demonstrating comparable performance between both PBGI variants and all baselines, as well as sensitivity comparisons involving the Gaussian process prior with varying kernel hyperparameters such as different smoothness values and length scales, are included in Appendix C.

Figure 4: Regret curves for objective functions sampled from the prior, shown using medians, as well as quartiles to indicate experiment variability. We see in the cost-aware setting that both PBGI variants usually exhibit comparable performance to LogEIPC and LogEICC, with PBGI-D decisively outperforming other baselines. In the uniform-cost setting, the story is similar for \(d=8\) and \(d=16\): PBGI and PBGI-D perform comparably to the best baselines, which are LogEI and UCB, as well as MSEI for \(d=8\). However, for \(d=32\), all methods perform comparably to random search, with PBGI, PBGI-D, LogEI, and UCB having near-identical median performance. See Figure 14 for an alternative visualization using mean and standard error.

In the low-dimensional case of \(d=8\), PBGI and LogEI variants achieve similar or better performance to the non-myopic (B)MSEI baseline. Once we increase dimension to \(d=16\), we see bigger differences, with PBGI variants, LogEI variants, and UCB now decisively outperforming (B)MSEI. Notably, the PBGI variants are also competitive in the uniform-cost setting--in spite of being designed for cost-aware problems. This can be explained via the curse of dimensionality: as dimension increases, the problem begins to look more like the uncorrelated Pandora's Box problem where using Gittins index is Bayesian-optimal. Eventually, however, the problem becomes too difficult for meaningful progress to be made within our computational budget, as seen for the uniform-cost problem with \(d=32\), where all deterministic methods perform near-identically and no method outperforms random search.

### Synthetic benchmarks

Next, we consider standard synthetic global optimization benchmark functions. To represent a variety of geometric properties, we examine the _Ackley_, _Levy_, and _Rosenbrock_ functions. A visualization of the two-dimensional versions of these functions is given in Appendix A.

Figure 5 presents results for \(d=16\). Additional results for \(d\{4,8\}\) showing that PBGI and all baselines perform similar, are given in Appendix C. We see that the behavior of different acquisition functions varies according to the the function. On the Ackley function, PBGI outperforms PBGI-D and the baselines. In contrast, on the Levy function, PBGI is best in the uniform-cost setting, but PBGI-D, LogEIPC, and LogEICC are best in the varying-cost setting. We conclude that PBGI can in principle offer stronger performance than PBGI-D, as long as \(\) is not-too-suboptimal, while PBGI-D tends to be less-performant but is more robust to this hyperparameter choice.

We also examine performance on the banana-shaped Rosenbrock-function, which has a small number of local optima . In this case, PBGI-D performs the strongest, matching the LogEI variants, while outperforming PBGI and (B)MSEI. This can intuitively be explained by the one-step optimality of expected improvement, which better-exploits the less-multimodal objective, while PBGI and multi-step-based acquisition functions are more conservative and may therefore require different tuning to perform best. We conclude that PBGI-D may be a better choice in settings where there is a

Figure 5: Synthetic benchmark regret curves, shown using medians, as well as quartiles to assess variability. All objective functions are defined with dimension \(d=16\). We see in the cost-aware setting that PBGI, PBGI-D, LogEIPC, and LogEICC all perform similarly on the heavily-multimodal Ackley function, matching or outperforming the non-myopic BMSEI baseline. On the Levy and Rosenbrock functions, PBGI-D matches—and for some cost budgets outperforms—all baselines, including the non-myopic BMSEI. Under uniform costs, PBGI performs well on Ackley and Levy, but is outperformed by PBGI-D and most baselines on Rosenbrock. See Figure 14 for an alternative visualization using mean and standard error.

potential mismatch between the objective and the prior in terms of degree of multimodality, as this can be counteracted in part by its decay behavior, particularly compared to PBGI.

### Empirical objectives

Finally, we benchmark PBGI policies on three empirical global optimization problems motivated by applied challenges: _Pest Control_ where \(d=25\), _Lunar Lander_ where \(d=12\), and _Robot Pushing_ where \(d=14\). Detailed descriptions of these problems and associated cost functions are in Appendix C. Note that, for Lunar Lander and Robot Pushing, the cost functions used are not automatically-differentiable. To handle this challenge and illustrate how our acquisition function can be used when the cost function is unknown, we apply unknown-cost PBGI and baseline variants, where the costs are modeled using a second independent log-Gaussian process: details on this unknown-cost PBGI variant, including its analytic form, are given in Appendix B.4.

From Figure 6, we see that the PBGI matches or outperforms baselines on Pest Control and Lunar Lander, in both the varying-cost and uniform-cost settings. On the other hand, PBGI performs poorly on Robot Pushing, where instead LogEI variants perform best and PBGI-D performs second-best; the non-myopic BMSEI baseline also performs poorly. This mirrors behavior previously seen on the Rosenbrock function, from which we suspect that a mismatch between prior and objective multimodality may be at play here as well. Note also that unlike in the Bayesian regret experiments, UCB's performance is far from strongest. This may be in part because we tune UCB using the schedule of Srinivas et al. , which is derived specifically for Bayesian regret, and may be less-ideal for other settings. In comparison, PBGI-D works reasonably well on five of the six cases.

## 5 Conclusion

In this paper, we introduced a new acquisition function class for cost-aware Bayesian optimization, the _Pandora's Box Gittins index_, based on an unexplored connection between Bayesian optimization and the _Pandora's Box_ problem from economics. We observed promising performance from two variants of this acquisition function class on both cost-aware problems which are the focus of this work, and, additionally, on classical uniform-cost problems. Performance gains tended to be largest on higher-dimensional and multi-modal problems. Our work constitutes a first step toward integrating ideas from Gittins index theory, including insights from generalizations of Pandora's Box, and related areas such as queueing theory, into Bayesian optimization.

Figure 6: Empirical benchmark regret curves, shown using medians, as well as quartiles to show variability. We see in both the varying-cost and uniform-cost settings that PBGI exhibits strong performance on the Pest Control and Lunar Lander problems. On the Robot Pushing problem, LogEI variants perform strongest, with PBGI-D performing second-strongest and matching or outperforming all other baselines. See Figure 14 for an alternative visualization using mean and standard error.