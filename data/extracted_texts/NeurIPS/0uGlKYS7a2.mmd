# Maximizing utility in multi-agent environments by anticipating the behavior of other learners

Angelos Assos

MIT CSAIL

Cambridge, MA

assos@mit.edu

&Yuval Dagan

UC Berkeley

Berkeley, CA

yuvaldag@berkeley.edu

Constantinos Daskalakis

MIT CSAIL

Cambridge, MA

costis@mit.edu

###### Abstract

Learning algorithms are often used to make decisions in sequential decision-making environments. In multi-agent settings, the decisions of each agent can affect the utilities/losses of the other agents. Therefore, if an agent is good at anticipating the behavior of the other agents, in particular how they will make decisions in each round as a function of their experience that far, it could try to judiciously make its own decisions over the rounds of the interaction so as to influence the other agents to behave in a way that ultimately benefits its own utility. In this paper, we study repeated two-player games involving two types of agents: a learner, which employs an online learning algorithm to choose its strategy in each round; and an optimizer, which knows the learner's utility function and the learner's online learning algorithm. The optimizer wants to plan ahead to maximize its own utility, while taking into account the learner's behavior. We provide two results: a positive result for repeated zero-sum games and a negative result for repeated general-sum games. Our positive result is an algorithm for the optimizer, which exactly maximizes its utility against a learner that plays the Replicator Dynamics -- the continuous-time analogue of Multiplicative Weights Update (MWU). Additionally, we use this result to provide an algorithm for the optimizer against MWU, i.e. for the discrete-time setting, which guarantees an average utility for the optimizer that is higher than the value of the one-shot game. Our negative result shows that, unless P=NP, there is no Fully Polynomial Time Approximation Scheme (FPTAS) for maximizing the utility of an optimizer against a learner that best-responds to the history in each round. Yet, this still leaves open the question of whether there exists a polynomial-time algorithm that optimizes the utility up to \(o(T)\).

## 1 Introduction

With the increased use of learning algorithms as a means to optimize agents' objectives in unknown or complex environments, it is inevitable that they will be used in multi-agent settings as well. This encompasses scenarios where multiple agents are repeatedly taking actions in the same environment, and their actions influence the payoffs of the other players. For example, participants in repeated online auctions use learning algorithms to bid, and the outcome, who gets to win and how much they pay, depends on all the bids (see e.g. [39; 40]). Other examples arise in contract design (see e.g. ),Bayesian persuasion (see e.g. ), competing retailers that use learning algorithms to set their prices (see e.g. ), and more.

It is thus natural to contemplate the following: in a setting where multiple agents take actions repeatedly, using their past observations of the other players' actions to decide on their future actions, what is the optimal strategy for an agent? This question is rather difficult, hence players often resort to simple online learning algorithms such as _mean-based_ learners: those algorithms select actions that approximately maximize the performance on the past, and include methods such as MWU, Best Response Dynamics, FTRL, FTPL, etc; see e.g. . Yet, when an agent knows that the other agents are using mean-based learners, it can it can adjust its strategy to take advantage of this knowledge. This was shown in specific games ; however, in general settings it is not known how to best play against mean-based learners, even if the agent knows _everything_ about the learners, and can predict with certainty what actions they would take as a function of the history of play. This raises the following question:

**Meta Question**.: _In a multi-agent environment where agents repeatedly take actions, what is the best strategy for an agent, if it knows that the other agents are using mean-based learners to select their actions? Can it plan ahead if it can predict how the other agents will react? Can such an optimal strategy be computed efficiently?_

Setting.We study environments with two agents: a _learner_ who uses a learning algorithm to decide on its actions, and an _optimizer_, that plans its actions, by trying to optimize its own reward, taking into account the learner's behavior. The setting is modeled as a repeated game: in each iteration \(t=1,,T\), the optimizer selects a strategy \(x(t)\), which is a distribution over a finite set of _pure actions_\(=\{a_{1},,a_{n}\}\), i.e. \(x(t)()\). At the same time, the learner selects a distribution \(y(t)\) over \(=\{b_{1},,b_{m}\}\), i.e. \(y(t)()\). The strategies \(x(t)\) and \(y(t)\) are viewed as elements of \(^{n}\) and \(^{m}\), respectively, and the elements of \(\) and \(\) are identified with unit vectors. In each round, each player gains a utility, which is a function of the strategies played by both agents. The utilities for the optimizer and the learner equal \(x(t)^{}Ay(t)\) and \(x(t)^{}By(t)\), respectively, where \(A,B^{n m}\). The goal of each player is to maximize their own utility, which is summed over all rounds \(t=1,,T\). We split our following discussion according to the study of zero-sum and general-sum games, as defined below.

Zero-sum games._Zero-sum_ games are those where \(A=-B\). Namely, the learner and the optimizer have opposing goals. Such games have a _value_, which determines the best utility that the players can hope for, if they are both playing optimally. It is defined as:

\[(A)=_{x()}_{y( )}x^{}Ay=_{y()}_{x( )}x^{}Ay\;.\]

The two quantities in the definition above are equal, as follows from von Neumann's minmax theorem. The first of these quantities implies that the optimizer has a strategy that guarantees them a utility of \((A)\) against any learner, and the second quantity implies that the learner has a strategy which guarantees that the optimizer's utility is at most \((A)\). Such strategies, namely, those that guarantee minmax value, are termed _minmax strategies_.

If the learner would have used a minmax strategy in each round \(t\), then the optimizer could have only received a utility of \(T(A)\). Yet, when the learner uses a learning algorithm, the optimizer can receive higher utility. A standard theoretical guarantee in online learning is _no-regret_. When a learner uses a _no-regret_ algorithm to play a repeated game, it is guaranteed that the optimizer's reward after \(T\) rounds is at most \(T(A)+o(T)\). Yet, \(T\) is finite, and the \(o(T)\)-term can be significant. Consequently, we pose the following question, which we study in Section 2.

**Question 1**.: _In repeated zero-sum games between an optimizer and a learner, when can the optimizer capitalize on the sub-optimality of the learner's strategy, to obtain significantly higher utility than the value of the game? Can the optimizer's algorithm be computationally efficient?_

General-sum games.Games where \(B-A\) are termed _general-sum_. These games are significantly more intricate: they do not posses a value or minmax strategies. In order to study such games, various notions of _game equilibria_ have been proposed, such as the celebrated Nash Equilibrium. In the context related to the topic of our paper, if the optimizer could commit on a strategy, and the other player would best respond to it, rather than using a learning algorithm, then the optimizer could get the _Stackelberg_ value, by playing according to the _Stackelberg Equilibrium_. Further, there is a polynomial-time algorithm to compute the optimizer's strategy in this case .

Against a learner that uses a no-regret learning algorithm, the optimizer could still guarantee the Stackelberg value of the game, by playing according to the Stackelberg Equilibrium. Yet, it was shown that in certain games, the optimizer can gain up to \((T)\) more utility compared to the Stackelberg value when they are playing against mean-based learners . However, this often requires playing a carefully-planned strategy which changes across time. While such a strategy could be computed in polynomial-time for specific games , no efficient algorithm was known for general games. Deng et al.  devised a control problem whose solution is approximately the optimal strategy for the optimizer, against mean-based learners. Yet, they do not provide a computationally-efficient algorithm for this control problem. Finding an optimal strategy for the learner was posed as an open problem , that can be summarized as follows:

**Question 2**.: _In repeated general-sum games between an optimizer and a mean-based learner, is there a polynomial-time algorithm to find an optimal strategy for the optimizer?_

We study this question in Section 3.

### Our results

Our results are two-fold; In zero sum games, we provide positive results, and show what the optimizer's optimal rewards and strategies for a given game should be, in both discrete and continuous time games. Our results in that realm are the following:

* For continuous time games, we provide an exact, closed form solution of the rewards of the optimizer against a learner that uses the replicator dynamics, i.e. continuous MWU (Section 2, Theorem 1). In the same theorem we prove that the optimizer can achieve optimal rewards by playing a constant strategy throughout the game. i.e. \(x(t)=x^{*},x() t[0,T]\).
* We also prove a range where the optimal rewards might be and provide a lower bound for when \( T\) (Section 2, Proposition 1).
* For discrete time games, when the optimizer is up against a learner that uses MWU, we first prove that the optimal rewards of the optimizer in this setting, will be lower bounded by the rewards of the optimizer in the continuous time game, if they use the same strategy \(x^{*}\) (Section 2 Proposition 2).
* We then prove that the reward 'gap' between continuous and discrete time games cannot be greater than \( T/2\) (Section B, Proposition 10), and there are games that achieve a gap of at least \(( T)\) (Section 2, Proposition 3). In fact we give a class of games that can achieve that gap in rewards (Section 2,Proposition 4).

In general-sum games, we provide the first known computational lower bound for calculating the optimal strategy against a mean based learner. We formalize the problem of optimizing rewards as a decision problem, where the answer for an instance is 'YES' if the optimizer can achieve rewards more than \(T\) and 'NO' if the optimizer can receive rewards at most \(T-1\). We prove that there is no polynomial time algorithm, assuming \(\) that distinguishes between the two cases by using a reduction from Hamiltonian Cycle. The formal theorem and a sketch of the reduction construction can be found in Section 2 Theorem 4.

### Related Work

The game-theoretic modeling of multi-agent environments has long history . Such studies often revolve around proving that if multiple agents use learning algorithms to repeatedly play against each other, the averge history of play converges to various notions of game equilibria . Recent works have shown that some learning algorithms yield fast convergence to game equilibria and fast-decaying regret, if all players are using the same algorithm .

Optimizing against no regret learners.Braverman et al.  initiated a recent thread of works, which studies how a player can take advantage of the learning algorithms run by other agents in order to gain high utility. They showed that in various repeated auctions where a seller sells an item to a single buyer in each iteration \(t=1,,T\), the seller can gain nearly the maximal utility that they could possibly hope for, if the learner runs an EXP3 learning algorithm (i.e., the seller's utility can be arbitrarily close to the total welfare). This can be \((T)\) larger than what they can get if the buyer is strategic. Deng et al.  generalized the study to arbitrary normal-form games. They showed an example for a game where an optimizer that plays against any mean-based learner gets \((T)\) more than the Stackelberg value, which is what they would get against strategic agents. The same paper further showed that against no-swap-regret learners, the optimizer cannot gain \((T)\) more than the Stackelberg value and Mansour et al.  showed that no-swap-regret learners are the only algorithms with this property. Brown et al.  showed a polynomial time algorithm to compute the optimal strategy for the optimizer against the no-regret learner that is most favorable to the optimizer -- yet, such no-regret learner may be unnatural. Additional work obtained deep insights into the optimizer/learner interactions in general games [11; 5], in Bayesian games  and in specific games such as auctions [23; 13; 35; 34], contract design  and Bayesian persuasion .

Optimizing against MWU in \(2 2\)-games.Guo and Mu  obtain a computationally-efficient algorithm for an optimizer that is playing a zero-sum game against a learner that uses MWU, in games where each player holds \(2\) actions, and they analyze that optimal strategy.

Regret lower bounds for online learners.Regret lower bounds for learning algorithms are tightly related to upper bounds for an optimizer in zero-sum games. The optimizer can be viewed as an adversary that seeks to maximize the regret of the algorithm. The main difference is that these lower bounds construct worst-case instances, yet, we seek to maximize the utility of the optimizer in any game. Regret lower bounds for MWU and generalizations of it were obtained by [17; 32; 15; 28]. Minmax regret lower bounds against any online learner can be found, for example, in .

Approximating the Stackelberg value.Lastly, the problem of computing the Stackelberg equilibrium is well studied [21; 41; 20]. Recent works also study repeated games between an optimizer and a learner, where the optimizer does not know the utilities of the learner and its goal is to learn to be nearly as good as the Stackelberg strategy [6; 38; 36; 31].

## 2 Optimizing utility in zero-sum games

Continuous-time games.We begin our discussion with continuous-time dynamics, where \(x(t)\) and \(y(t)\) are functions of the continuous-time parameter \(t[0,T]\). The total reward for the optimizer is \(_{0}^{T}x(t)^{}Ay(t)dt\), whereas the learner's utility equals the optimizer's utility multiplied by \(-1\). We assume that the learner is playing the _Replicator Dynamics_ with parameter \(>0\), which is the continuous-time analogue of the Multiplicative Weights Update algorithm, which plays at every time \(t\):

\[y_{i}(t)=^{t}x(s)^{}Be_{i}ds)}{_{j= 1}^{m}(_{0}^{t}x(s)^{}Be_{j}ds)}\;, i=1,,m\;.\] (1)

This formula gives a higher weight to actions that would have obtained higher utility in the past. When \(\) is larger, the learner is more likely to play the actions that maximize the past utility. The value of \(\) usually becomes smaller as \(T\) increases, and typically \( 0\) as \(T\).

The following definition will be helpful for the analysis:

**Definition 1** (Historical Rewards for the learner).: _The historical rewards for the learner in continuous games at time \(t\), denoted by \(h(t)^{m}\), is an \(m\) dimensional vector where \(h_{i}(t),i=1,,m\) corresponds to the sum of rewards achieved by action \(a_{i}\) of the learner against the history of play in the game so far. We assume a general setting where the learner at \(t=0\) might have non-zero historical rewards, i.e. \(h(0)^{m}\). If the strategy of the optimizer is \(x:[0,t]()\) we get:_

\[h(t)=h(0)+_{0}^{t}B^{}x(t)dt\]

Suppose the game has been played for some time \(t\) and the learner has collected historical rewards \(h(t)\). The total reward that the optimizer gains from the remainder of this game can be written as a function of the time left for the game \(T-t\), the historical rewards of the learner at time \(t\), \(h(t)\), and the strategy \(x:[0,T-t]()\) of the optimizer for the remainder of the game.

\[R_{cont}(x,h(t),T-t,A,B)=_{0}^{T-t}^{m}e^{(h_{i}( 0)+e_{i}^{}B^{}_{0}^{t}x(s)ds)} e_{i}^{}A^{}x(t )}{_{i=1}^{m}e^{(h_{i}(0)+e_{i}^{}B^{}_{0}^{t}x(s)ds )}}du\] (2)

For simplicity, we can just transfer the time to \(0\) and assume that the historical rewards of the learner are just \(h(0)\). That way we can rewrite the above definition as:

\[R_{cont}(x,h(0),,A,B)=_{0}^{}^{m}e^{(h_{i }(t)+e_{i}^{}B^{}_{0}^{t}x(s)ds)} e_{i}^{}A^{} x(t)}{_{i=1}^{m}e^{(h_{i}(t)+e_{i}^{}B^{}_{0}^{t}x(s)ds )}}dt\] (3)

In general we are interested in finding the maximum value that the optimizer can achieve at any moment of the game:

\[R_{cont}^{*}(h(0),,A,B)=_{x}R_{cont}(x,h(0),,A,B)\] (4)

For finding the optimal reward for the optimizer from the beginning of the game we would have to find \(R_{cont}^{*}(,T,A,B)\), where \(=(0,0,,0)^{}\).

The next theorem characterizes the exact optimal strategy for the optimizer, against a learner that uses the Replicator Dynamics in zero sum games:

**Theorem 1**.: _In a zero-sum continuous game, when the learner is using the Replicator Dynamics, the optimal rewards for the optimizer can be achieved with a constant strategy throughout the game, i.e. \(x(t)=x^{*}()\). The optimal reward is obtained by the following formula:_

\[R_{cont}^{*}(h(0),T,A,-A)=_{x()}\{^{m}e^{ h_{i}(0)})-(_{i=1}^{m}e^{(h_{ i}(0)-Te_{i}^{}A^{}x)})}{}\}\] (5)

_Further, \(x^{*}\) is the maximizer in the formula above._

Proof.: For zero sum games in continuous time, we have that equation 3 becomes:

\[R_{cont}(x,h(0),T,A,-A)=_{0}^{T}^{m}e^{(h_{i}(0)- e_{i}^{}A^{}_{0}^{t}x(s)ds)} e_{i}^{}A^{}x(t )}{_{i=1}^{m}e^{(h_{i}(0)-e_{i}^{}A^{}_{0}^{t}x(s)ds )}}dt\] (6)

Notice that

\[(e^{(h_{i}(0)-e_{i}^{}A^{}_{0}^{t}x(s) ds)})=e^{(h_{i}(0)-e_{i}^{}A^{}_{0}^{t}x(s)ds )}( h_{i}(0)- e_{i}^{}A^{} _{0}^{t}x(s)ds)\]

From Leibniz rule we have that \(_{0}^{t}x(s)ds=x(t)\), thus we get:

\[(e^{(h_{i}(0)-e_{i}^{}A^{}_{0}^{t}x(s) ds)})=-e^{(h_{i}(0)-e_{i}^{}A^{}_{0}^{t}x(s) ds)} e_{i}^{}A^{}x(t)\]

Given that, note that we can find a closed form solution for \(R_{cont}(x,h(0),T,A,-A)\) as follows:

\[R_{cont}(x,h(0),T,A,-A) =[-(_{i=1}^{m}e^{(h_{i}( 0)-e_{i}^{}A^{}_{0}^{t}x(s)ds)})]_{0}^{T}\] \[=^{m}e^{ h_{i}(0)})-( _{i=1}^{m}e^{(h_{i}(0)-e_{i}^{}A^{}_{0}^{T}x(s)ds )})}{}\]

Suppose the optimal rewards \(R_{cont}^{*}(h(0),T,A,-A)\) are achieved with \(x^{opt}(t)\). Note that the final reward only depends on \(_{0}^{T}x(s)ds\), thus the same reward that is achieved by \(x^{opt}(t)\), can be achieved by \(x^{*}=_{0}^{T}x^{opt}(s)ds\). Thus there exists a \(x^{*}\) such that:

\[R_{cont}^{*}(h(0),T,A,-A)=^{m}e^{ h_{i}(0)}) -(_{i=1}^{m}e^{(h_{i}(0)-e_{i}^{}A^{}x^{}T )})}{}\] (7)

which is what we wanted.

The optimal strategy \(x^{*}\) for the optimizer in continuous zero-sum games can be therefore obtained by finding the minimum of the convex function \(f(x)=(_{i=1}^{m}e^{(h_{i}(0)-e_{i}^{}A^{}xT) })\). We can compute the optimal strategy of the optimizer in an efficient way using techniques from convex optimization. More details can be found in the appendix at Proposition 5.

We further analyze how larger the optimal achievable reward is, compared to the naive bound of \(T(A)\). We specifically show that always the optimal rewards of the optimizer are in the range of \([T(A),T(A)+(m)/]\), where \(m\) is the number of actions of the learner, showing the optimizer can always get more utility than just the value of the game.

We also analyze what happens to the rewards of the optimizer as \( T\). First, let us define for any optimizer's strategy \(x\), the set of all best-responses,

\[(x)=_{b}x^{}Bb.\]

This defines a set, and if there are multiple maximizers, \(|(x)|>1\). We then have the following proposition:

**Proposition 1** (informal).: _The optimal reward for an optimizer playing against a learner that uses the replicator dynamics with parameter \(\) in an \(n m\) game, is in the range \([T(A),T(A)+(m)/]\). Further, as \( T\), this optimal utility is at least_

\[T(A)+\;,k=_{x( )}|(x)|\;.\]

The proof of Proposition 1 can be found in Appendix B, Propositions 6 and 7. We note that the limiting utility is obtained by playing constantly any minmax strategy \(x\) that attains the minimum in the definition of \(k\) above.

Connection to optimal control and the Hamilton-Jacobi-Bellman equation.We will present another way of achieving Theorem 1, using literature from control theory. One can view the problem of maximizing the optimizer's utilities as an optimal control problem; what control (or strategy) should the optimizer use in order to maximize their utility given that the learner has some specific dynamics that depend on the control of the optimizer? The Hamilton-Jacobi-Bellman equation  gives us a partial differential equation (PDE) of \(R^{*}_{cont}(h,t,A,B)\) that if we solve, we can find a closed form solution of the optimal utility of the optimizer. The equation (for general sum games):

\[-_{cont}(h,t,A,B)}{dt}=_{x()}(^{m}e^{ h_{i}} e_{i}^{}A^{}x}{_{i=1}^{m}e^{  h_{i}}}+(_{h}R^{*}_{cont}(h,t,A,B))^{} B^{ }x)\]

The intuition of the PDE is as follows; the current state of the learner can be defined given only the history \(h\) of the sum of what the optimizer played so far, and the time left in the game \(t\). Given we are at a state \(h,t\), the optimal rewards for the optimizer are going to be equal to the rewards of playing action \(x()\) for time \( t\) added together with the optimal reward in the new state, namely \(R^{*}_{cont}(h+B^{}x t,t+ t,A,B)\). Taking the limit as \( t 0\), gives us the above partial differential equation.

Plugging in \(B=-A\), for the case of zero-sum games, we get:

\[-_{cont}(h,t,A,-A)}{dt}=_{x()}( {_{i=1}^{m}e^{ h_{i}} e_{i}^{}A^{}x}{_{i=1}^{m}e^{  h_{i}}}-(_{h}R^{*}_{cont}(h,t,A,-A))^{} A^{ }x)\]

If one plugs in the formula we calculated in Theorem 1, they would find that indeed it is a solution to the above PDE.

Discrete-time games.We now move to the discrete-time setting. The learner is assumed to be playing the Multiplicative-Weights update algorithm, defined by:

\[y_{i}(t)=^{t-1}x(s)^{}Be_{i})}{_{ j=1}^{m}(_{s=1}^{t-1}x(s)^{}Be_{j})}, i=1,2, ,m\;.\] (8)

We show that if the learner constantly plays the strategy \(x^{*}\) that is optimal for continuous-time, the obtained utility against MWU can only be higher, compared to playing against the Replicator Dynamics in continuous-time, with the same step size \(\):

**Proposition 2** (informal).: _Let \(A\) be a zero-sum game matrix and \(>0\) and \(T\). Let \(x^{*}()\) be the optimal strategy against the replicator dynamics, from Theorem 1. Then, the utility in the discrete time, achieved by an optimizer which plays \(x(t)=x^{*}\) for all \(t\{1,,T\}\) against MWU with parameter \(\), is at least the utility achieved in the continuous-time by playing \(x^{*}\) against the replicator dynamics with the same parameters \(,T\)._

The proof can be found in Appendix B (Proposition 8).

Proposition 2 implies that Proposition 1 provides lower bounds on the achievable utility of this constant discrete-time strategy. In order to further analyze its performance, we would like to ask how much larger the discrete-time utility of the optimizer can be from the optimal continuous-time utility. We include the following statement, which follows from a standard approach:

**Proposition 3**.: _Let \(T\) and \(>0\). For any zero-sum game whose utilities are bounded in \([-1,1]\), the best discrete-time optimizer obtains a utility of at most \( T/2\) more than the best continuous-time optimizer. Further, there exists a game where the best discrete-time optimizer achieves a utility of \(()T/2=(-O(^{2}))T/2\) more than the optimal continuous-time optimizer, and \(()T/2\) more than the discrete-time optimizer from Proposition 2._

The proof can be found in Appendix B (Propositions 9 and 10).

Proposition 3 implies that in some cases, the best discrete-time optimizer can gain \(T(A)+( T)\). We would like to understand in which games this is possible for any choice of \((0,1)\). We provide a definition that guarantees that this would be possible:

**Condition 1**.: _There exists a mimax strategy \(x\) for the optimizer such that there exist two best responses for the learner, \(b_{i_{1}},b_{i_{2}}(x)\), which do not coincide on \((x)\). Namely, there exists an action \(a_{k}(x)\) such that \(a_{k}^{}Ab_{i_{1}} a_{k}^{}Ab_{i_{2}}\)._

To motivate Condition 1, notice that in order to achieve a gain of \(( T)\) for any \(\), the discrete optimizer has to be significantly better than the continuous-time optimizer. For that to happen the learner would have to change actions frequently. Indeed, the difference between the discrete and continuous learners is that the discrete learner is slower to change actions: they only change actions at integer times, whereas the continuous learner could change actions at each real-valued time. In order to change actions, they need to have at least two good actions, and this is what Assumption 1 guarantees. We derive the following statement:

**Proposition 4**.: _For any zero-sum game \(A^{n m}\) that satisfies Assumption 1, \((0,1)\) and \(T\), there exists an optimizer, such that against a learner that uses MWU with step size \(\), achieves a utility of at least \(T(A)+( T),\) where the constant possibly depends on \(A\)._

The proof can be found in Appendix B (Proposition 11).

Proposition 4 considers a strategy for the optimizer which takes the minmax strategy \(x\) from Assumption 1, and splits into two strategies: \(x^{},x^{}()\), such that \((x^{}+x^{})/2=x\). Further, \(x^{}Ab_{i_{1}} x^{}Ab_{i_{2}}\), where \(b_{i_{1}},b_{i_{2}}\) are defined in Assumption 1. It plays \(x^{}\) in each odd round \(t\), and \(x^{}\) in each even round. It is possible to show that in each two consecutive iterations, the reward for the optimizer is \(2(A)+()\). Summing over \(T/2\) consecutive pairs yields the final bound.

## 3 A computational lower bound for optimization in general-sum games

In this section, we present the first limitation on optimizing against a mean-based learner. Specifically, we study the algorithm which is termed _Best-Response_ or _Fictitious Play_[10; 43]. At each time step \(t=1,,T\), this algorithm selects an action \(y(t)\) that maximizes the cumulative utility for rounds \(1,,t-1\):

\[y(t)=_{y()}_{s=1}^{t-1}x(s)^{ }By\] (9)

There is always a maximizer which corresponds to a pure action \(b_{i}\) of the learner and we assume that if there are ties, they are broken lexicographically. In this section, we constrain the optimizer to also play pure actions.

To put this algorithm in context, we note the following connections to general mean-based learners: (1) _mean-based_ learners are any algorithms which select an action \(y(t)\) that approximately maximizes Eq. (9); (2) Best-Response is equivalent to MWU with \(\).

We prove that there is no algorithm that approximates the optimal utility of the optimizer up to an approximation factor of \(1-\), whose runtime is polynomial in \(n,m,T\) and \(1/\), as exemplified by the following (informal) Theorem:

**Theorem 2** (informal).: _Let \(\) be an algorithm that receives parameters \(>0\), \(m,n,T\) and utility matrices \(A,B\) of dimension \(m n\) and entries in \(\), and outputs a control sequence \(x(1),,x(T)\). Let \(U\) denote the utility attained by the learner and let \(U^{*}\) denote the optimal possible utility. If \(U(1-)U^{*}\) for any instance of the problem, and if \(\), then \(\) is not polynomial-time in \(m,n,T\) and \(1/\)._

A sketch of the proof can be found below, and a full proof can be found in the Appendix C. The proof is obtained via a reduction from the Hamiltonian cycle problem. We note two limitations of this result: (1) The lower bound is for \(T=n/2+1\), and it shows hardness in distinguishing between the case that the optimal reward is \(T\) and the case that the optimal reward is at most \(T-1\). It is still open whether one could efficiently find a sequence that optimizes the reward up to \(o(T)\); (2) fictitious-play is a fundamental online learning algorithm. Yet, it does not possess the no-regret guarantee. It is still open whether one could obtain maximal utility against no-regret learners such as MWU with a small step size.

We continue by framing the problem of maximizing rewards against a Best-Response learner as a decision problem, called OCDP:

**Problem 1** (Optimal Control Discrete Pure (OCDP)).: _An OCDP instance is defined by \((A,B,n,m,k,T)\), where \(n,m,k,T\), \(A\{0,1\}^{n m}\) and \(B^{n m}\). The numbers \(n\) and \(m\) correspond to the actions of the optimizer and learner in a game, where \(A\) is the utility matrix of the optimizer and \(B\) is the utility matrix of the learner. This instance is a 'YES' instance if the optimizer can achieve utility at least \(k\) after playing the game for \(T\) rounds with a learner that uses the Best Response Algorithm (Eq. (9)), and 'NO' if otherwise._

We will prove that OCDP is NP-hard, using a reduction from the Hamiltonian cycle problem.

**Problem 2** (Hamiltonian cycle).: _Given a directed graph \(G(V,E)\), find whether there exists a Hamiltonian cycle, i.e. a cycle that starts from any vertex, visits every vertex exactly once and closes at the same vertex where it started._

It is a known theorem that the Hamiltonian Cycle is an NP-complete problem, as it is one of Karp's 21 initial NP-Complete problems.

**Theorem 3** ().: _Hamiltonian Cycle is NP-complete._

We conclude with the main result of this section, followed by a proof sketch. The full proof appears in Section C.

**Theorem 4**.: _OCDP is NP-hard. That is, if \(\), there exists no algorithm that runs in polynomial time in \(n,m\) and \(k\) which distinguishes between the case that a reward of \(k\) is achievable and the case that it is impossible to obtain reward more than \(k-1\)._

Proof sketch.: Consider an instance of the Hamiltonian cycle problem: \(_{H}=(V,E)\), where \(V=\{v_{1},,v_{n}\},E=\{e_{1},,e_{m}\}\). We create an instance of OCDP as follows. First, set \(T=k=n+1\). We will construct the instance such that the optimizer can receive reward \(n+1\) if and only if there is a Hamiltonian cycle in the graph. Define the optimizer's actions to be \(\{a_{1},,a_{m}\}\), and the learner's actions to be \(\{b_{1},,b_{n},b^{}_{1},,b^{}_{n}\}\). Namely, for each node \(v_{i}\) of the graph, the learner has two associated actions, \(b_{i}\) and \(b^{}_{i}\). The details in the sketch differ slightly from the proof for clarity. The reduction is constructed to satisfy the following properties:

* The only way for the optimizer to receive maximal utility, is to play a strategy as follows: the first \(n\) actions should correspond to edges \(e_{i_{1}}-e_{i_{2}}--e_{i_{n}}\) that form a Hamiltonian cycle which starts and ends at the vertex \(v_{1}\); and the last action corresponds to \(e_{i_{n+1}}\) which is an outgoing edge from \(v_{1}\).

* We define the utility matrix for the learner such that, if the optimizer is playing according to this strategy, then the learner's actions will correspond to the vertices of the same cycle, denoted as \(v_{j_{1}}--v_{j_{n}}-v_{j_{1}}\). This is achieved by defining the learner's tie-breaking rule to play \(a_{1}\) in the first round; and defining the learner's utilities such that, if the optimizer has played in rounds \(1,,t-1\) the edges along a path \(v_{j_{1}}--v_{j_{t}}\), then the best response for the learner at time \(t\) would be to play \(a_{j_{t}}\). To achieve this, we define for any edge \(e_{k}=(v_{p},v_{q})\): \(B[a_{k},b_{p}]=-1\), \(B[a_{k},b_{q}]=1\) and \(B[a_{k},b]=0\) for any other action \(b\). Consequently, after observing the edges along \(b_{j_{1}}--b_{j_{t}}\), the cumulative reward for the learner would be \(1\) for \(b_{j_{t}}\), \(-1\) for \(b_{j_{1}}\) and \(0\) for any other \(b_{j}\). Consequently, the learner will best respond with \(b_{j_{t}}\) -- we ignore the actions \(b_{1}^{},,b_{n}^{}\) of the learner at the moment.
* In order to guarantee that the above optimizer's strategy yields a utility of \(n+1\), we define the optimizer's utility such that \(A[e_{k},b_{p}]=1\) if \(e_{k}=(v_{p},v_{q})\) and for all \(q p\;A[e_{k},b_{q}]=0\). This will also force the optimizer to play edges that form a path. Indeed, if the optimizer has previously played the edges along a path \(v_{j_{1}}--v_{j_{t}}\), then, as we discussed, the learner will play \(b_{i_{t}}\) in the next round. For the optimizer to gain a reward at the next round, they must play an edge outgoing from \(a_{j_{t}}\). This preserves the property that the optimizer's actions form a path.
* Next, we would like to guarantee that the optimizer's actions align with a Hamiltonian cycle. Therefore, we need to ensure that the path played by the optimizer does not close too early. Namely, if the learner has played the edges along the path \(v_{j_{1}}--v_{j_{t}}\) and if \(t<n\), then \(j_{1},,j_{t}\) are all distinct. For this purpose, the actions \(v_{1}^{},,v_{n}^{}\) are defined. We define for any edge \(e_{k}=(v_{i},v_{j})\): \(B[a_{k},b_{i}^{}]=0.85\). This will prevent the optimizer from playing the same vertex twice, for any round \(t=1,,n\) (recall that there are \(T=n+1\) rounds), as argued below. Assume for the sake of contradiction that the first time the same vertex is visited is at \(t\), where \(j_{t}=j_{r}\) for some \(r<t n\). Then, the cumulative utility of action \(b_{j_{r}}^{}\) for rounds \(1,,t\) is \(1.7\), which is larger than any other action. This implies that at the next round, the learner will play action \(b_{j_{r}}^{}\). We define the utility for the optimizer to be zero against any action \(b_{j}^{}\). Hence, any scenario where the learner plays an action \(b_{j}^{}\), prevents the optimizer from receiving a utility of \(n+1\). This happens whenever \(i_{t}=i_{j}\) for some \(j<t n\). Hence, an optimizer that receives a reward of \(1\) at any round must play a path that does not visit the same vertex twice, in rounds \(t=1,,n\).
* Lastly, notice that we do want the optimizer to play the same vertex \(a_{1}\) twice, at rounds \(1\) and \(n+1\). This does not prevent the optimizer from receiving optimal utility. Indeed, if the optimizer would play the action \(a_{1}\) twice, then the learner would play \(b_{1}^{}\) at the next round. Yet, since there are only \(n+1\) rounds, there is no "next round" and no reward is lost. The details that force the learner to play \(b_{1}\) in round \(n+1\) appear in the full version of the proof.

The above explanation sketches why it is possible to get \(n+1\) reward if and only if there is a Hamiltonian cycle and this concludes the reduction. 

## 4 Conclusion and future directions

In this paper we studied how an optimizer should play in a two-player repeated game knowing that the other agent is a learner that is using a known mean-based algorithm. In zero-sum games we showed how they can gain optimal utility against the Replicator Dynamics and we further analyzed the utility that they could gain against MWU. In general sum games, we showed the first computational hardness result on optimizing against a mean-based learner, by reduction from Hamiltonian Cycle.

One interesting problem that remains is the open is analyzing the optimal reward in general sum games against the Replicator Dynamics (or the MWU), which was denoted as \(R_{cont}^{*}(,T,A,B)\). In the fully general case with no restriction on the utility matrices \(A\) and \(B\), we believe there is no closed form solution for the optimal utility, differently from the zero-sum case. However, it would be interesting to understand how \(R_{cont}^{*}(,T,A,B)\) behaves as a function of \(A\) and \(B\) and how the best strategy for the optimizer looks like. A conjecture in this direction was given by Deng et al. . Perhaps it would be easier to study simpler scenarios, such as the one where \((A+B)=1\)which has been explored in the context of computing equilibria and the convergence of learning algorithms to them (, ). Another direction is improving on the lower bound for general sum games. Currently, we prove that it is hard to distinguish between the case where the optimizer can achieve reward \(=T\) and the case where the optimizer cannot achieve more than \(=T-1\). Is it also hard to distinguish between a reward of at least \(\) or at most \(\) in cases where \(-=(T)\)? Are there lower bounds when the learner uses different learning algorithms, such as MWU? Other relevant open directions are extensions to multi-agent settings , analyzing how the learner's utility is impacted by interaction with the optimizer in general-sum games , which learning algorithms yield higher utilities against an optimizer, and what algorithms should be used to both learn and optimize at the same time?

Societal impact.The work studies multi-agent environments and how agents can benefit by anticipating the behavior of other agents. We believe that increasing the academic knowledge in this topic can help learning agents assess their risk of being utilized by other agents and can help to build algorithms that are more resilient to manipulation. As always with new technologies, there is a risk that malicious players will utilize ideas from this paper, which could cause a harmful effect on other agents.