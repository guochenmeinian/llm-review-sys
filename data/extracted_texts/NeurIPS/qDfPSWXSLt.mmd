# Spec-Gaussian: Anisotropic View-Dependent Appearance for 3D Gaussian Splitting

Ziyi Yang\({}^{1,3}\)  Xinyu Gao\({}^{1}\)  Yang-Tian Sun\({}^{2}\)  Yi-Hua Huang\({}^{2}\)  Xiaoyang Lyu\({}^{2}\)

**Wen Zhou\({}^{3}\)  Shaohui Jiao\({}^{3}\)  Xiaojuan Qi\({}^{2}\)\({}^{}\)  Xiaogang Jin\({}^{1}\)**

\({}^{1}\)State Key Lab of CAD&CG, Zhejiang University

\({}^{2}\)The University of Hong Kong \({}^{3}\)ByteDance Inc.

###### Abstract

The recent advancements in 3D Gaussian splatting (3D-GS) have not only facilitated real-time rendering through modern GPU rasterization pipelines but have also attained state-of-the-art rendering quality. Nevertheless, despite its exceptional rendering quality and performance on standard datasets, 3D-GS frequently encounters difficulties in accurately modeling specular and anisotropic components. This issue stems from the limited ability of spherical harmonics (SH) to represent high-frequency information. To overcome this challenge, we introduce _Spec-Gaussian_, an approach that utilizes an anisotropic spherical Gaussian (ASG) appearance field instead of SH for modeling the view-dependent appearance of each 3D Gaussian. Additionally, we have developed a coarse-to-fine training strategy to improve learning efficiency and eliminate floaters caused by overfitting in real-world scenes. Our experimental results demonstrate that our method surpasses existing approaches in terms of rendering quality. Thanks to ASG, we have significantly improved the ability of 3D-GS to model scenes with specular and anisotropic components without increasing the number of 3D Gaussians. This improvement extends the applicability of 3D GS to handle intricate scenarios with specular and anisotropic surfaces. Our codes and datasets are available at https://ingra14m.github.io/Spec-Gaussian-website.

+
Footnote â€ : \({}\) Corresponding Authors.

## 1 Introduction

High-quality reconstruction and photorealistic rendering from a collection of images are crucial for a variety of applications, such as augmented reality/virtual reality (AR/VR), 3D content production, and art creation. Classic methods employ primitive representations, like meshes  and points , and take advantage of the rasterization pipeline optimized for contemporary GPUs to achieve real-time rendering. In contrast, neural radiance fields (NeRF)  utilize neural implicit representation to offer a continuous scene representation and employ volumetric rendering to produce rendering results. This approach allows for enhanced preservation of scene details and more effective reconstruction of scene geometries.

Recently, 3D Gaussian Splatting (3D-GS)  has emerged as a leading technique, delivering state-of-the-art quality and real-time speed. This method optimizes a set of 3D Gaussians that capture the appearance and geometry of a 3D scene simultaneously, offering a continuous representation that preserves details and produces high-quality results. Besides, the CUDA-customized differentiable rasterization pipeline for 3D Gaussians enables real-time rendering even at high resolution.

Despite its exceptional performance, 3D-GS struggles to model specular components within scenes (see Fig. 1). This issue primarily stems from the limited ability of low-order spherical harmonics (SH) to capture the high-frequency information in these scenarios. Consequently, this poses a challenge for 3D-GS to model scenes with reflections and specular components, as illustrated in Fig. 1.

To address the issue, we introduce a novel approach called _Spec-Gaussian_, which combines anisotropic spherical Gaussian (ASG)  for modeling anisotropic and specular components, an effective training mechanism to eliminate floaters and improve learning efficiencies, and anchor-based 3D Gaussians for acceleration and storage reduction. Specifically, the method incorporates two key designs: 1) A new 3D Gaussian representation that utilizes an ASG appearance field instead of SH to model the appearance of each 3D Gaussian. ASG with a few orders can effectively model high-frequency information that low-order SH cannot. This new design enables 3D-GS to more effectively model anisotropic and specular components in static scenes. 2) A coarse-to-fine training scheme specifically tailored for 3D-GS is designed to eliminate floaters and boost learning efficiency. This strategy effectively shortens learning time by optimizing low-resolution rendering in the initial stage, preventing the need to increase the number of 3D Gaussians and regularizing the learning process to avoid the generation of unnecessary geometric structures that lead to floaters.

By combining these advances, our approach can render high-quality results for specular highlights and anisotropy as shown in Fig. 4 while preserving the efficiency of Gaussians. Furthermore, comprehensive experiments reveal that our method not only endows 3D-GS with the ability to model specular highlights but also achieves state-of-the-art results in general benchmarks.

In summary, the major contributions of our work are as follows:

* A novel ASG appearance field to model the view-dependent appearance of each 3D Gaussian, which enables 3D-GS to effectively represent scenes with specular and anisotropic components.
* A coarse-to-fine training scheme that effectively regularizes training to eliminate floaters and improve the learning efficiency of 3D-GS in real-world scenes.
* An anisotropic dataset has also been made to assess the capability of our model in representing anisotropy.

## 2 Related Work

### Implicit Neural Radiance Fields

Neural rendering has attracted significant interest in the academic community for its unparalleled ability to generate photorealistic images. Methods like NeRF  utilize Multi-Layer Perceptrons (MLPs) to model the geometry and radiance fields of a scene. Leveraging the volumetric rendering equation and the inherent continuity and smoothness of MLPs, NeRF achieves high-quality scene reconstruction from a set of posed images, establishing itself as the state-of-the-art (SOTA) method for novel view synthesis. Subsequent research has extended the utility of NeRF to various applications, including mesh reconstruction [53; 27; 58; 34], inverse rendering [48; 72; 31; 62], optimization of camera parameters [29; 55; 54; 41], few-shot learning [12; 61; 57], and anti-aliasing [2; 1; 3].

However, this stream of methods relies on ray casting rather than rasterization to determine the color of each pixel. Consequently, every sampling point along the ray necessitates querying the MLPs,

Figure 1: Our method not only achieves real-time rendering but also significantly enhances the capability of 3D-GS to model scenes with specular and anisotropic components. Key to this enhanced performance is our use of ASG appearance field to model the appearance of each 3D Gaussian, which results in substantial improvements in rendering quality for both complex and general scenes.

leading to significantly slow rendering speed and prolonged training convergence. This limitation substantially impedes their application in large-scene modeling and real-time rendering.

To reduce the training time of MLP-based NeRF methods and improve rendering speed, subsequent work has enhanced NeRF's efficiency in various ways. Structure-based techniques [68; 14; 43; 17; 7] have sought to improve inference or training efficiency by caching or distilling the implicit neural representation into more efficient data structures. Hybrid methods [30; 49] increase efficiency by incorporating explicit voxel-based data structures. Factorization methods [5; 18; 8; 16] apply a low-rank tensor assumption to decompose the scene into low-dimensional planes or vectors, achieving better geometric consistency. Compared to continuous implicit representations, the convergence of individual voxels in the grid is independent, significantly reducing training time. Additionally, Instant-NGP  utilizes a hash grid with a corresponding CUDA implementation for faster feature querying, enabling rapid training and interactive rendering of neural radiance fields. Spec-NeRF  achieves high-quality specular reflection modeling by introducing Gaussian directional encoding.

Despite achieving higher quality and faster rendering, these methods have not fundamentally overcome the substantial query overhead associated with ray casting. As a result, a notable gap remains before achieving real-time rendering. In this work, we build upon the recent 3D-GS , a point-based rendering method that leverages rasterization. Compared to ray casting-based methods, it significantly enhances both training and rendering speed.

### Point-based Neural Radiance Fields

Point-based representations, similar to triangle mesh-based methods, can exploit the highly efficient rasterization pipeline of modern GPUs to achieve real-time rendering. Although these methods offer breakneck rendering speeds and are well-suited for editing tasks, they often suffer from holes and outliers, leading to artifacts in the rendered images. This issue arises from the discrete nature of point clouds, which can create gaps in the primitives and, consequently, in the rendered image.

To address these discontinuity issues, differentiable point-based rendering [67; 15; 24; 25] has been extensively explored for fitting complex geometric shapes. Notably, Zhang et al.  employ differentiable surface splatting and utilize a radial basis function (RBF) kernel to compute the contribution of each point to each pixel.

Recently, 3D-GS  has employed anisotropic 3D Gaussians, initialized from Structure from Motion (SfM), to represent 3D scenes. The innovative densification mechanism and CUDA-customized differentiable Gaussian rasterization pipeline of 3D-GS have not only achieved state-of-the-art (SOTA) rendering quality but also significantly surpassed the threshold of real-time rendering. Many concurrent works have rapidly extended 3D-GS to a variety of downstream applications, including dynamic scenes [33; 63; 64; 20; 26; 50], text-to-3D generation [28; 51; 9; 66; 10], avatars [74; 73; 21; 45; 40], scene editing [59; 6; 13], quality enhancement [36; 44] and mesh reconstruction [19; 11; 69; 34].

Figure 2: **Pipeline of Spec-Gaussian.** The optimization process begins with SfM points derived from COLMAP or generated randomly, serving as the initial state for the 3D Gaussians. To address the limitations of low-order SH and pure MLP in modeling high-frequency information, we additionally employ ASG in conjunction with a feature decoupling MLP to model the view-dependent appearance of each 3D Gaussian. Then, 3D Gaussians with opacity \(>0\) are rendered through a differentiable Gaussian rasterization pipeline, effectively capturing specular highlights and anisotropy in the scene.

Despite achieving SOTA results on commonly used benchmark datasets, 3D-GS still struggles to model scenes with specular and reflective components, which limits its practical application in real-time rendering at the photorealistic level. In this work, by replacing spherical harmonics (SH) with an anisotropic spherical Gaussian (ASG) appearance field, we have enabled 3D-GS to model complex specular scenes more effectively.

## 3 Method

The overview of our method is illustrated in Fig. 2. The input to our model is a set of posed images of a static scene, together with a sparse point cloud obtained from SfM . The core of our method is to use the ASG appearance field to replace SH in modeling the appearance of 3D Gaussians (Sec. 3.2). Moreover, we introduce a simple yet effective coarse-to-fine training strategy to reduce floaters in real-world scenes (Sec. 3.3). To further reduce the storage overhead and rendering speed pressure introduced by ASG, we combine a hybrid Gaussian model that employs sparse anchor Gaussians to facilitate the generation of neural Gaussians (Sec. 3.4) to model the 3D scene.

### Preliminaries

3D Gaussian splatting.3D-GS  is a point-based method that employs anisotropic 3D Gaussians to represent scenes. Each 3D Gaussian is defined by a center position \(\), opacity \(\), and a 3D covariance matrix \(\), which is decomposed into a quaternion \(\) and scaling \(\). The view-dependent appearance of each 3D Gaussian is represented using the first three orders of spherical harmonics (SH). This method not only retains the rendering details offered by volumetric rendering but also achieves real-time rendering through a CUDA-customized differentiable Gaussian rasterization process. Following , the 3D Gaussians can be projected to 2D using the 2D covariance matrix \(^{}\), defined as:

\[^{}=J V^{T}J^{T},\] (1)

where \(J\) is the Jacobian of the affine approximation of the projective transformation, and \(V\) represents the view matrix, transitioning from world to camera coordinates. To facilitate learning, the 3D covariance matrix \(\) is decomposed into two learnable components: the quaternion \(\), representing rotation, and the 3D-vector \(\), representing scaling. The resulting \(\) is thus represented as the combination of a rotation matrix \(R\) and scaling matrix \(S\) as:

\[=RSS^{T}R^{T}.\] (2)

The color of each pixel on the image plane is then rendered through a point-based volumetric rendering (alpha blending) technique:

\[C()=_{i N}T_{i}_{i}c_{i},_{i}=_{i}e^{- (-_{i})^{T}^{-1}(-_{i})},\] (3)

where **p** denotes the pixel coordinate, \(T_{i}\) is the transmittance defined by \(_{j=1}^{i-1}(1-_{j})\), \(c_{i}\) signifies the color of the sorted Gaussians associated with the queried pixel, and \(_{i}\) represents the coordinates of the 3D Gaussians when projected onto the 2D image plane.

Anisotropic spherical Gaussian.Anisotropic spherical Gaussian (ASG)  has been designed in the traditional rendering pipeline to efficiently approximate lighting and shading. Different from spherical Gaussian (SG), ASG has been demonstrated to effectively represent anisotropic scenes with a small number. In addition to retaining the fundamental properties of SG, ASG also exhibits rotational invariance and can represent full-frequency signals. The ASG function is defined as:

\[ASG([,,],[,],)= (;) e^{-()^{2}-( )^{2}},\] (4)

where \(\) is the unit direction serving as the function input; **x**, **y**, and **z** correspond to the tangent, bi-tangent, and lobe axis, respectively, and are mutually orthogonal; \(^{1}\) and \(^{1}\) are the sharpness parameters for the **x**- and **y**-axis, satisfying \(,>0\); \(^{2}\) is the lobe amplitude; \(\) is the smooth term defined as \((;)=(,0)\).

Inspired by the power of ASG in modeling scenes with complex anisotropy, we propose integrating ASG into Gaussian splatting to join the forces of classic models with new rendering pipelines for higher quality. For \(N\) ASGs, we predefined orthonormal axes \(\), \(\), and \(\), initializing them to be uniformly distributed across a hemisphere. During training, we allow the remaining ASG parameters, \(\), \(\), and \(\), to be learnable. We use the reflect direction \(_{r}\) as the input to query ASG for modeling the view-dependent specular information. Note that we use \(N=32\) ASGs for each 3D Gaussian.

Anchor-based Gaussian splatting.Anchor-based Gaussian splatting was first proposed by Scaffold-GS . Unlike the attributes carried by each entity in 3D-GS, each anchor Gaussian carries a position coordinate \(_{v}^{3}\), a local feature \(_{v}^{32}\), a displacement factor \(_{v}^{3}\), and \(k\) learnable offsets \(_{v}^{k 3}\). They use the COLMAP  point cloud to initialize each anchor 3D Gaussian, serving as the voxel centers to guide the generation of neural Gaussians. The position \(_{v}\) of the anchor Gaussian is initialized as:

\[_{v}=\{}{}+0.5 \},\] (5)

where \(\) is the point cloud position, \(\) is the voxel size, and \(\{\}\) denotes removing duplicated anchors.

Then anchor Gaussians can guide the generation of neural Gaussians, which have the same attributes as vanilla 3D-GS. For each visible anchor Gaussian within the viewing frustum, we spawn \(k\) neural Gaussians and predict their attributes. The positions \(\) of neural Gaussians are calculated as:

\[\{_{0},,_{k-1}\}=_{v}+\{_{0}, ,_{k-1}\}_{v},\] (6)

where \(_{v}\) represents the position of the anchor Gaussian corresponding to \(k\) neural Gaussians. The opacity \(\) is calculated through a tiny MLP:

\[\{_{0},,_{k-1}\}=_{}( _{v},_{cv},_{cv}),\] (7)

where \(_{cv}\) denotes the distance between the anchor Gaussian and the camera, and \(_{cv}\) is the unit direction pointing from the camera to the anchor Gaussian. The rotation \(r\) and scaling \(s\) of each neural Gaussian are derived similarly using the corresponding tiny MLP \(_{r}\) and \(_{s}\).

### Anisotropic View-Dependent Appearance

ASG appearance field for 3D Gaussians.Although SH has enabled view-dependent scene modeling, the low frequency of low-order SH makes it challenging to model scenes with complex optical phenomena such as specular highlights and anisotropic effects. Therefore, instead of using SH, we propose using an ASG appearance field based on Eq. (4) to model the appearance of each 3D Gaussian. However, the introduction of ASG increases the feature dimensions of each 3D Gaussian, raising the model's storage overhead. To address this, we employ a compact learnable MLP \(\) to predict the parameters for \(N\) ASGs, with each Gaussian carrying only additional local features \(^{24}\) as the input to the MLP:

\[()\{,,\}_{N}.\] (8)

To better differentiate between high and low-frequency information and further assist ASG in fitting high-frequency specular details, we decompose color \(c\) into diffuse and specular components:

\[c=c_{d}+c_{s},\] (9)

where \(c_{d}\) represents the diffuse color, modeled using the first three orders of SH, and \(c_{s}\) is the specular color calculated through ASG. We refer to this comprehensive approach to appearance modeling as the ASG appearance field.

Although ASG theoretically enhance the ability of SH to model anisotropy, directly using ASG to represent the specular color of each 3D Gaussian still falls short in accurately modeling anisotropic and specular components, as demonstrated in Fig. 6. Inspired by , we do not use ASG directly to represent color but instead employ ASG to model the latent feature of each 3D Gaussian. This latent feature, containing anisotropic information, is then fed into a tiny feature decoding MLP \(\) to determine the final specular color:

\[(,(), n,-)  c_{s},\] (10) \[=_{i=1}^{N}ASG(_{r}[,,],[_{i},_{i}],_{i})\]where \(\) is the latent feature derived from ASG, \(\) denotes the concatenation operation, \(\) represents the positional encoding, \(\) is the unit view direction pointing from the camera to each 3D Gaussian, \(n\) is the normal of each 3D Gaussian, and \(_{r}\) is the unit reflect direction. This strategy significantly enhances the ability of 3D-GS to model scenes with complex optical phenomena, whereas neither pure ASG nor pure MLP can achieve anisotropic appearance modeling as effectively as our approach.

Normal estimation.Following [22; 47], we use the shortest axis of each Gaussian as its normal. This approach is based on the observation that 3D Gaussians tend to flatten gradually during the optimization process, allowing the shortest axis to serve as a reasonable approximation for the normal.

The reflect direction \(_{r}\) can then be derived using the view direction and the local normal vector \(n\) as:

\[_{r}=2(_{o} n) n-_{o},\] (11)

where \(_{o}=-\) is a unit view direction pointing from each 3D Gaussian in world space to the camera. We use the reflect direction \(_{r}\) to query ASG, enabling better interpolation of latent features containing anisotropic information. Experimental results show that although this unsupervised normal estimation cannot generate physically accurate normals aligned with the real world, it is sufficient to produce relatively accurate reflect direction to assist ASG in fitting high-frequency information.

### Coarse-to-fine Training

We observed that in many real-world scenarios, 3D-GS tends to overfit the training data, leading to the emergence of numerous floaters when rendering images from novel viewpoints. One important reason is that the COLMAP point cloud is too sparse. Poor initialization makes it difficult for 3D-GS to compensate for overly sparse areas through densification during optimization, leading to floaters in the rendering images. Moreover, 3D-GS accumulates gradients from each pixel to the GS: \(}=_{i}}_{i}}{d }\), and the densification occurs when the accumulated amount exceeds a threshold \(_{g}=0.0002\). However, having positive and negative gradients can cause GSs that should be densified to be ignored due to the large negative gradient.

Thus, to mitigate the occurrence of floaters in real-world scenes, we propose a coarse-to-fine training mechanism. We first impose an L1 constraint on the gradients from pixels to GS: \(}=\|_{i}}_{i}}{ d}\|_{1}\), accumulating the numerical contribution from pixels to GS rather than gradients. This idea is similar to the concurrent works [65; 70]. Next, to avoid overfitting caused by excessive growth of 3D-GS during the early stages of optimization, we decide to train 3D-GS progressively from low to high resolution in real-world scenes:

\[r(i)=( r_{s}+(r_{e}-r_{s}) i/,r_{e}),\] (12)

where \(r(i)\) is the image resolution at the \(i\)-th training iteration, \(r_{s}\) is the starting image resolution, \(r_{e}\) is the ending image resolution (the full resolution we aim to render), and \(\) is the threshold iteration, empirically set to 5k.

This training method allows 3D-GS to densify correctly and prevents excessive growth of 3D-GS in the early stages. Additionally, due to the lower resolution training in the initial phase, this mechanism reduces training time by approximately 10%. In our experiments, we offer a performance version with \(_{g}=0.0005\) and light version with \(_{g}=0.0006\).

  Dataset &  \\ Method & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PPS & Mem & Num (\(\)) \\ 
3D-GS & 33.82 & 0.966 & 0.062 & 325 & 47MB & 201 \\ Scaffidid-GS & 35.34 & 0.972 & 0.052 & 234 & 27MB & - \\  Our-world anchor & 36.76 & 0.976 & 0.046 & 180 & 25MB & - \\ Our-light & **37.42** & 0.979 & 0.044 & 159 & 54MB & 146 \\ Ours & **37.70** & **0.980** & **0.042** & 145 & 57MB & 183 \\  

Table 1: **Quantitative Comparison on anisotropic synthetic dataset.**

Figure 3: Using a coarse-to-fine strategy, our approach can eliminate the floaters without increasing the number of GS.

### Adaption for Anchor-Based Gaussian Splatting

While the ASG appearance field significantly improves the ability of 3D-GS to model specular and anisotropic features, it introduces additional computational overhead due to the additional local features \(\) associated with each Gaussian. Inspired by , we employ anchor-based Gaussian splatting to reduce storage overhead and accelerate the rendering.

Since the anisotropy modeled by ASG is continuous in space, it can be compressed into a lower-dimensional space. Thanks to the guidance of the anchor Gaussian, the anchor feature \(_{v}\) can be used directly to compress \(N\) ASGs, further reducing storage pressure. To make the ASG of neural Gaussians position-aware, we introduce the unit view direction to decompress ASG parameters. Consequently, the ASG parameters prediction in Eq. (8) is revised as follows:

\[(_{v},_{cn})\{,,\}_{N},\] (13)

where \(_{cn}\) denotes the unit view direction from the camera to each neural Gaussian. Additionally, we set the diffuse part of the neural Gaussian \(c_{d}=(_{v})\), directly predicted through an MLP \(\), to ensure the smoothness of the diffuse component and reduce the difficulty of convergence.

### Losses

We optimize the learnable parameters and MLPs using the same loss function as 3D-GS . The total supervision is given by:

\[=(1-_{})_{1}+_{ }_{},\] (14)

where the \(_{}=0.2\) is consistently used in our experiments.

## 4 Experiments

In this section, we present both quantitative and qualitative results of our method. To evaluate its effectiveness, we compared it to several state-of-the-art methods across various datasets. We color

  Dataset &  &  &  &  &  \\ Method 1 Metrics & PSNR \(\) & SSIM \(\) & LPIPS \(\) & FPS & Mem & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\  Phenoxels & 23.08 & 0.626 & 0.463 & 6.79 & 2.1GB & 21.68 & 0.513 & 0.491 & 24.83 & 0.766 & 0.426 \\ iNGP & 25.59 & 0.699 & 0.331 & 9.43 & 48MB & 22.75 & 0.567 & 0.403 & 29.14 & 0.863 & 0.242 \\ Mip-NeRF360 & 27.69 & 0.792 & 0.237 & 0.06 & 8.6MB & 24.47 & 0.691 & 0.283 & 31.72 & 0.917 & 0.180 \\
3D-GS & 27.79 & 0.826 & 0.202 & 115 & 748MB & 25.02 & 0.742 & 0.322 & 31.25 & 0.931 & 0.164 \\ Scaffid-GS & 27.98 & 0.824 & 0.207 & 96 & 203MB & 25.07 & 0.736 & 0.243 & 31.61 & 0.933 & 0.162 \\  Ours-w/ anchor & 28.14 & 0.824 & 0.196 & 70 & 260MB & 24.98 & 0.735 & 0.223 & **32.09** & 0.935 & 0.161 \\ Ours-light & 28.07 & 0.834 & **0.183** & 44 & 68.4MB & 25.09 & 0.752 & 0.203 & 31.80 & **0.936** & **0.158** \\ Ours & 28.18 & 0.835 & **0.176** & 33 & 847MB & **25.11** & **0.754** & **0.195** & 32.01 & **0.937** & **0.153** \\  

Table 2: **Quantitative comparison of on real-world datasets. We report PSNR, SSIM, LPIPS (VGG) and color each cell as \(\), \(\)\(\) and \(\)\(\). Our method has achieved the best rendering quality, while striking a good balance between FPS and the storage memory.**

Figure 4: **Visualization on NeRF dataset. Our method has achieved specular highlights modeling, which other 3D-GS-based methods fail to accomplish, while maintaining fast rendering speed.**

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

demonstrate that our method not only equips 3D-GS with the ability to model specular highlights and anisotropy but also enhances the overall rendering quality of 3D-GS in general scenes.

Limitations.Although our method enables 3D-GS to model complex specular and anisotropic features, it still faces challenges in handling reflections. Specular and anisotropic effects are primarily influenced by material properties, whereas reflections are closely related to the environment and geometry. Due to the lack of explicit geometry in 3D-GS, we cannot differentiate between reflections and materials using constraints like normals, as employed in Ref-NeRF  and NeRO . We plan to explore solutions for modeling reflections with 3D-GS in future work.

## 6 Acknowlegements

We thank Chao Wan from Cornell University for the help during rebuttal period. This work was supported by the National Natural Science Foundation of China (Grant No. 62036010). Ziyi Yang was also supported by ByteDance MMLab.