# Dynamic 3D Gaussian Fields for Urban Areas

Tobias Fischer\({}^{1}\) &Jonas Kulhanek\({}^{1,3}\) &Samuel Rota Bulo\({}^{2}\) &Lorenzo Porzi\({}^{2}\)

**Marc Pollefeys\({}^{1}\)** &Peter Kontschieder\({}^{2}\)

\({}^{1}\) ETH Zurich \({}^{2}\) Meta Reality Labs \({}^{3}\) CTU Prague

https://tobiasfshr.github.io/pub/4dgf/

###### Abstract

We present an efficient neural 3D scene representation for novel-view synthesis (NVS) in large-scale, dynamic urban areas. Existing works are not well suited for applications like mixed-reality or closed-loop simulation due to their limited visual quality and non-interactive rendering speeds. Recently, rasterization-based approaches have achieved high-quality NVS at impressive speeds. However, these methods are limited to small-scale, _homogeneous_ data, _i.e_. they cannot handle severe appearance and geometry variations due to weather, season, and lighting and do not scale to larger, dynamic areas with thousands of images. We propose 4DGF, a neural scene representation that scales to large-scale _dynamic_ urban areas, handles _heterogeneous_ input data, and substantially improves rendering speeds. We use 3D Gaussians as an efficient geometry scaffold while relying on neural fields as a compact and flexible appearance model. We integrate scene dynamics via a scene graph at global scale while modeling articulated motions on a local level via deformations. This decomposed approach enables flexible scene composition suitable for real-world applications. In experiments, we surpass the state-of-the-art by over 3 dB in PSNR and more than \(200\) in rendering speed.

## 1 Introduction

The problem of synthesizing novel views from a set of images has received widespread attention in recent years due to its importance for technologies like AR/VR and robotics. In particular, obtaining interactive, high-quality renderings of large-scale, dynamic urban areas under varying weather, lighting, and seasonal conditions is a key requirement for closed-loop robotic simulation and immersive VR experiences. To achieve this goal, sensor-equipped vehicles act as a frequent data

Figure 1: **Summary. Given a set of _heterogeneous_ input sequences that capture a common geographic area in varying environmental conditions (_e.g_. weather, season, and lighting) with distinct dynamic objects (_e.g_. vehicles, pedestrians, and cyclists), we optimize a _single_ dynamic scene representation that permits rendering of arbitrary viewpoints and scene configurations at interactive speeds.**source that is becoming widely available in city-scale mapping and autonomous driving, creating the possibility of building up-to-date digital twins of entire cities. However, modeling these scenarios is extremely challenging as heterogeneous data sources have to be processed and combined: different weather, lighting, seasons, and distinct dynamic and transient objects pose significant challenges to the reconstruction and rendering of dynamic urban areas.

In recent years, neural radiance fields have shown great promise in achieving realistic novel view synthesis of static [1; 2; 3] and dynamic scenes [4; 5; 6; 7]. While earlier methods were limited to controlled environments, several recent works have explored large-scale, dynamic areas [8; 9; 10]. Among these, many works resort to removing dynamic regions and thus produce partial reconstructions [9; 10; 11; 12; 13; 14]. In contrast, fewer works model scene dynamics [15; 16; 17]. These methods exhibit clear limitations, such as rendering speed which can be attributed to the high cost of ray traversal in volumetric rendering.

Therefore, rasterization-based techniques [18; 19; 20; 11] have recently emerged as a viable alternative. Most notably, Kerbl _et al_.  propose a scene representation based on 3D Gaussian primitives that can be efficiently rendered with a tile-based rasterizer at a high visual quality. While demonstrating impressive rendering speeds, it requires millions of Gaussian primitives with high-dimensional spherical harmonics coefficients as color representation to achieve good view synthesis results. This limits its applicability to large-scale urban areas due to high memory requirements. Furthermore, due to its explicit color representation, it cannot model transient geometry and appearance variations commonly encountered in city-scale mapping and autonomous driving use cases such as seasonal and weather changes. Lastly, the approach is limited to static scenes which complicates representing dynamic objects such as moving vehicles or pedestrians commonly encountered in urban areas.

To this end, we propose 4DGF, a method that takes a hybrid approach to modeling dynamic urban areas. In particular, we use 3D Gaussian primitives as an efficient geometry scaffold. However, we do not store appearance as a per-primitive attribute, thus avoiding more than 80% of its memory footprint. Instead, we use fixed-size neural fields as a compact and flexible alternative. This allows us to model drastically different appearances and transient geometry which is essential to reconstructing urban areas from heterogeneous data. Finally, we model scene dynamics with a graph-based representation that maps dynamic objects to canonical space for reconstruction. We model non-rigid deformations in this canonical space with our neural fields to cope with articulated dynamic objects common in urban areas such as pedestrians and cyclists. This decomposed approach further enables a flexible scene composition suitable to downstream applications. The key contributions of this work are:

* We introduce 4DGF, a hybrid neural scene representation for dynamic urban areas that leverages 3D Gaussians as an efficient geometry scaffold and neural fields as a compact and flexible appearance representation.
* We use neural fields to incorporate scene-specific transient geometry and appearances into the rendering process of 3D Gaussian splatting, overcoming its limitation to static, homogeneous data sources while benefitting from its efficient rendering.
* We integrate scene dynamics via i) a graph-based representation, mapping dynamic objects to canonical space, and ii) modeling non-rigid deformations in this canonical space. This enables effective reconstruction of dynamic objects from in-the-wild captures.

We show that 4DGF effectively reconstructs large-scale, dynamic urban areas with over ten thousand images, achieves state-of-the-art results across four dynamic outdoor benchmarks [21; 22; 17; 23], and is more than \(200\) faster to render than the previous state-of-the-art.

## 2 Related Work

**Dynamic scene representations.** Scene representations are a pillar of computer vision and graphics research . Over decades, researchers have studied various static and dynamic scene representations for numerous problem setups [25; 26; 27; 28; 29; 30; 31; 32; 1; 33; 34]. Recently, neural rendering  has given rise to a new class of scene representations for photo-realistic image synthesis. While earlier methods in this scope were limited to static scenes [2; 3; 36; 37; 38], dynamic scene representations have emerged quickly . These scene representations can be broadly classified into implicit and explicit representations. Implicit representations [5; 6; 7; 39; 4; 40; 41; 16] encode the scene as a parametric function modeled as neural network, while explicit representations [42; 43; 44; 45; 44] use a collection of low-level primitives. In both cases, scene dynamics are simulated as i) deformations of a canonical volume [5; 6; 42; 39; 41], ii) particle-level motion such as scene flow [7; 4; 40; 16; 46], or iii) rigid transformations of local geometric primitives . On the contrary, traditional computer graphics literature uses scene graphs to compose entities into complex scenes . Therefore, another area of research explores decomposing scenes into higher-level elements [31; 48; 32; 15; 49; 50; 17], where entities and their spatial relations are expressed as a directed graph. This concept was recently revisited for view synthesis [15; 17]. In this work, we take a hybrid approach that uses i) explicit geometric primitives for fast rendering, ii) implicit neural fields to model appearance and geometry variation, and iii) a scene graph to decompose individual dynamic and static components.

**Efficient rendering and 3D Gaussian splatting.** Aside from accuracy, the rendering speed of a scene representation is equally important. While rendering speed highly depends on the representation efficiency itself, it also varies with the form of rendering that is coupled with it to generate an image . Traditionally, neural radiance fields  use implicit functions and volumetric rendering which produce accurate renderings but suffer from costly function evaluation and ray traversal. To remedy these issues, many techniques for caching and efficient sampling [52; 53; 54; 36; 55; 17] have been developed. However, these approaches often suffer from excessive GPU memory requirements  and are still limited in rendering speed [54; 55; 17]. Therefore, researchers have opted to exploit more efficient forms of rendering, baking neural scene representations into meshes for efficient rasterization [19; 20; 11]. This area of research has recently been disrupted by 3D Gaussian splatting , which i) represents the scene as a set of anisotropic 3D Gaussian primitives ii) uses an efficient tile-based, differentiable rasterizer, and iii) enables effective optimization by adaptive density control (ADC), which facilitates primitive growth and pruning. This led to a paradigm shift from baking neural scene representations to a more streamlined approach.

However, the method of Kerbl. _et al_.  exhibits clear limitations, which has sparked a very active field of research with many concurrent works [44; 56; 57; 58; 59; 60; 61; 62; 63; 64; 65]. For instance, several works tackle dynamic scenes by adapting approaches described in the paragraph above [44; 66; 67; 68; 69; 70]. Another line of work focuses on modeling larger-scale scenes [65; 71; 72]. Lastly, several concurrent works investigate the reconstruction of dynamic street scenes [73; 74; 75]. These methods are generally limited to homogeneous data and in scale. In contrast, our method scales to tens of thousands of images and effectively reconstructs large, _dynamic_ urban areas from _heterogeneous_ data while _also_ providing orders of magnitude faster rendering than traditional approaches.

**Reconstructing urban areas.** Dynamic urban areas are particularly challenging to reconstruct due to the complexity of both the scenes and the capturing process. Hence, significant research efforts have focused on adapting view synthesis approaches from controlled, small-scale environments to larger, real-world scenes. In particular, researchers have investigated the use of depth priors from _e.g_. LiDAR, providing additional information such as camera exposure, jointly optimizing camera parameters, and developing specialized sky and light modeling approaches [8; 9; 10; 11; 12; 13; 14]. However, since scene dynamics are challenging to approach, many works simply remove dynamic areas, providing only a partial reconstruction. A few works explicitly model scene dynamics, but suffer from limitations in terms of scalability [15; 49; 45], accuracy , rendering speed [17; 76; 77], or modeling of non-rigid and uncommon objects [15; 49; 45; 77; 17]. We introduce a mechanism to handle transient geometry and varying appearance, improve rendering efficiency, and, inspired by how global rigid object motion is handled in [17; 15; 49; 45; 77], propose an approach to model the local articulated motion of non-rigid dynamic objects without using semantic priors. Consequently, our work enables the reconstruction of much larger urban areas with a significantly higher number and diversity of dynamic objects across multiple in-the-wild captures.

## 3 Method

### Problem setup

We are given a set of _heterogeneous_ sequences \(S\) that capture a common geographic area from a moving vehicle. The vehicle is equipped with calibrated cameras mounted in a surround-view setup. We denote with \(C_{s}\) the set of cameras of sequence \(s S\) and with \(C\) the total set of cameras, _i.e_. \(C_{s S}C_{s}\). For each camera \(c C\), we assume to know the intrinsic \(_{c}\) parameters and the pose \(_{c}(3)\), expressed in the ego-vehicle reference frame. Ego-vehicle poses \(_{s}^{t}(3)\) are provided for each sequence \(s S\) and timesteps \(t T_{s}\) and are expressed in the world reference frame that is shared across all sequences. Here, \(T_{s}\) denotes a set of timestamps relative to \(s\). Indeed, we assume that timestamps cannot be compared across sequences because we lack a mapping to a global timeline, which is often the case with benchmark datasets due to privacy reasons. For each sequence \(s S\), camera \(c C_{s}\) and timestamp \(t T_{s}\) we have an RGB image \(_{(s,c)}^{t}^{H W 3}\). Each sequence has additionally an associated set of dynamic objects \(O_{s}\). Dynamic objects \(o O_{s}\) are associated with a 3D bounding box track that holds its (stationary) 3D object dimensions \(_{o}_{+}^{3}\) and poses \(\{_{o}^{t_{0}},...,_{o}^{t_{n}}\}(3)\) w.r.t. the ego-vehicle frame, where \(t_{i} T_{o} T_{s}\). Our goal is to estimate the plenoptic function for the shared geographic area spanned by the training sequences, _i.e._ a function \(f(,,t,s)\), which outputs a rendered RGB image of size \((H,W)\) for a given camera pose \(\) with calibration \(\) in the conditions of sequence \(s S\) at time \(t T_{s}\).

### Representation

We model a parameterized, plenoptic function \(f_{}\), which depends on the following components: i) a scene graph \(\) that provides the scene configuration and latent conditioning signals \(\) for each sequence \(s\), object \(o\), and time \(t\), ii) sets of 3D Gaussians that serve as a geometry scaffold for the scene and objects, and iii) implicit neural fields that model appearance and modulate the geometry scaffold according to the conditioning signals. See Figure 2 for an overview of our method.

**Scene configuration.** Inspired by , we factorize the scene with a graph representation \(=(,)\), holding latent conditioning signals at the nodes \(\) and coordinate system transformations along the edges \(\). The nodes \(\) consist of a root node \(v_{r}\) defining the global coordinate system, _camera_ nodes \(\{v_{c}\}_{c C}\), and for each sequence \(s S\), _sequence_ nodes \(\{v_{s}^{t}\}_{t T_{s}}\) and dynamic _object_ nodes \(\{v_{o}\}_{o O_{s}}\). We associate latent vectors \(\) to sequence and object nodes representing local appearance and geometry. Specifically, we model the time-varying sequence appearance and geometry via

\[_{s}^{t}[_{s}(t),\,_{s}(t)]\] (1)

where \(_{s}\) and \(_{s}\) are appearance and geometry modulation matrices, respectively, and \(()\) is a 1D basis function of sines and cosines with linearly increasing frequencies at log-scale . Time \(t\) is normalized to \([-1,1]\) via the maximum sequence length \(_{s S}|T_{s}|\). For objects, we use both an object code and a time encoding

\[_{o}^{t}[_{o},(t)]\,.\] (2)

Nodes in the graph \(\) are connected by oriented edges that define rigid transformations between the canonical frames of the nodes. We have \(_{s}^{t}\) for sequence to root edges, \(_{c}\) for camera to sequence edges, and \(_{o}^{t}\) for object to sequence edges.

**3D Gaussians.** We represent the scene geometry with sets of anisotropic 3D Gaussians primitives \(G=\{G_{r}\}\{G_{o}\,:\,o O_{s},s S\}\). Each 3D Gaussian primitive \(_{k}\) is parameterized by its mean

Figure 2: **Overview. To render an image of sequence \(s\) at time \(t\), we first evaluate the scene graph \(=(,)\) which stores latent codes \(\) at its nodes \(\) and coordinate transformations \([|]\) at its edges \(\), _i.e._ the configuration of the dynamic objects and the overall scene. We then use the scene configuration to determine the active sets of 3D Gaussians \(G\). The 3D Gaussians \(G\) and the latent codes \(\) serve as conditioning signals to the neural fields \(\) and \(\), which output, for each 3D Gaussian \(_{k} G\), an appearance conditioned color \(_{k}^{s,t}\), an opacity correction term \(_{k}^{s,t}\) for static Gaussians modeling transient geometry, and a dynamic deformation \(_{k}^{t}\) for non-rigid dynamic 3D Gaussians modeling _e.g._ pedestrians. Finally, the retrieved information is used to compose a set of 3D Gaussians that represent the dynamic scene at \((s,t)\) from which we render the image.**

\(_{k}\), covariance matrix \(_{k}\), and a base opacity \(_{k}\). The covariance matrix is decomposed into a rotation matrix represented as a unit quaternion \(q_{k}\) and a scaling vector \(a_{k}_{+}^{3}\). The geometry of \(_{k}\) is represented by

\[_{k}()=(-[-_{k}]^{ }_{k}^{-1}[-_{k}])\,.\] (3)

The common scene geometry scaffold is modeled with a single set of 3D Gaussians \(G_{r}\), while we have a separate set \(G_{o}\) of 3D Gaussians for each dynamic object \(o\). Indeed, scene geometry is largely consistent across sequences while object geometries are distinct. The 3D Gaussians \(G_{r}\) are represented in world frame, while each set \(G_{o}\) is represented in a canonical, object-centric coordinate frame, which can be mapped to the world frame by traversing \(\).

Differently from , our 3D Gaussians do not hold any appearance information, reducing the memory footprint of the representation _by more than 80%_. Instead, we leverage neural fields to regress a color information \(_{k}^{s,t}\) and an updated opacity \(_{k}^{s,t}\) for each sequence \(s S\) and time \(t T_{s}\). For 3D Gaussians in \(G_{r}\) modeling the scene scaffolding, we predict an opacity attenuation term \(_{k}^{s,t}\) that is used to model transient geometry by downscaling \(_{k}\). Instead, for 3D Gaussians in \(G_{o}\) modeling objects the base opacity is left invariant. Hence

\[_{k}^{s,t}_{k}^{s,t}_{k}& _{k} G_{r}\\ _{k}&\] (4)

The attenuation term enforces a high base opacity for every 3D Gaussian visible in _at least_ one sequence. Therefore, we can obtain pruning decisions in ADC by thresholding the base opacity \(_{k}\), which is directly accessible without computational overhead, without risking the removal of transient geometry.

Furthermore, in the presence of non-rigid objects \(o\), we predict deformation terms \(_{k}^{t}^{3}\) to the position of 3D primitives in \(G_{o}\) via the neural fields, for each time \(t T_{o}\). In this case, the position of the primitive in object-centric space at time \(t\) is given by

\[_{k}^{t}_{k}+_{k}^{t}\,.\] (5)

**Appearance and transient geometry.** Given the scene graph \(\) and the 3D Gaussians \(G\), we use two neural fields to decode the aforementioned parameters for each primitive. In particular, for 3D Gaussians in \(G_{r}\) modeling the static scene, the neural field is denoted by \(\) and regresses the opacity attenuation term \(_{k}^{s,t}\) and a color \(_{k}^{s,t}\), given the 3D Gaussian primitive's position \(_{k}\), a viewing direction \(\), the base opacity \(_{k}\) and the latent code of the node \(_{s}^{t}\), _i.e._

\[(_{k}^{s,t},\ _{k}^{s,t})(_{k},, _{k},_{s}^{t})\,.\] (6)

where \(s S\) and \(t T_{s}\). Note that since the opacity attenuation \(_{k}^{s,t}\) contributes to modeling transient geometry by removing parts of the scene encoded in the original set of Gaussians, it does not depend on the viewing direction \(\).

For 3D Gaussians in \(G_{o}\) modeling dynamic objects, the neural field is denoted by \(\) and regresses a color \(_{k}^{s,t}\). Besides the primitive's position and viewing direction, we condition \(\) on latent vectors \(_{s}^{t}\) and \(_{o}^{t}\) to model both local object texture and global sequence appearance such as illumination. Here, the sequence \(s\) is the one where \(o\) belongs to, _i.e._ satisfying \(o O_{s}\), and \(t T_{o}\). Accordingly, the color \(_{k}^{s,t}\) for a 3D Gaussian in \(G_{o}\) is given by

\[_{k}^{s,t}(_{k},,_{s}^{t}, _{o}^{t})\,.\] (7)

Both \(_{k}\) and \(\) are expressed in the canonical, object-centric space of object \(o\). Using neural fields has three key advantages for our purpose. First, by sharing the parameters of \(\) and \(\) across all 3D Gaussians \(G\), we achieve a significantly more compact representation than in  when scaling to large-scale urban areas. Second, it allows us to model sequence-dependent appearance and transient geometry which is fundamental to learning a scene representation from heterogeneous data. Third, information sharing between nodes enables an interaction of sequence and object appearance.

However, querying a neural field is more complex than a spherical harmonics function as in . Therefore, we i) use efficient hash-grid representations  to minimize query complexity and, ii)carefully optimize the rendering workflow to minimize the amount of queries. In particular, we skip out-of-view 3D Gaussians and implement a vectorized query function to \(\) that retrieves the parameters of all relevant dynamic objects in parallel. We refer to Section 4.3 for a runtime analysis.

Non-rigid objects.Street scenes are occupied not only by rigidly moving vehicles but also by, _e.g_., pedestrians and cyclists that move in a non-rigid manner. These pose a significant challenge due to their unconstrained motion under limited visual coverage. Therefore, we take a decomposed approach to modeling non-rigid objects. First, we represent the local articulated motion of non-rigid objects like pedestrians as deformation in canonical space. We use deformation head \(\) that predicts a local position offset \(_{k}^{t}\) via

\[_{k}^{t}(_{},(t))\] (8)

given an intermediate feature representation \(_{}\) of \(\) conditioned on \(_{k}\) and time \(t\). We deform the position of \(_{k}\) over time in canonical space as per Equation (5). Second, we use the scene graph \(\) to model the global rigid object motion, transforming the objects from object-centric to world space with a rigid body transformation. We use a general design to cover a wide range of scenarios, such as pedestrians holding a stroller or shopping bags, cyclists, and animals. See Figure 10 in our supp. mat.

Background modeling.To achieve a faithful rendering of far-away objects and the sky, it is important to have a background model. Inspired by , where points are sampled along a ray at increasing distance outside the scene bounds, we place 3D Gaussians on spheres around the scene with radius \(r2^{i+1}\) for \(i\{1,2,3\}\) where \(r\) is half of the scene bound diameter. To avoid ambiguity with foreground scene geometry and to increase efficiency, we remove all points that are i) below the ground plane, ii) occluded by foreground scene points, or iii) outside of the view frustum of any training view. To uniformly distribute points on each sphere, we utilize the Fibonacci sphere sampling algorithm , which arranges points in a spiral pattern using a golden ratio-based formula. Even though this sampling is not optimal, it serves as a faster approximation of the optimal sampling.

### Composition and Rendering

Scene composition.To render our representation from the perspective of camera \(c\) at time \(t\) in sequence \(s\), we traverse the graph \(\) to obtain the latent vector \(_{s}^{t}\) and the latent vector \(_{o}^{t}\) of each visible object \(o O_{s}\), _i.e_. such that \(t T_{o}\). Moreover, for each 3D Gaussian primitive \(_{k}\) in \(G\), we use the collected camera parameters, object scale, and pose information to determine the transformation \(_{k}^{c}\) mapping points from the primitive's reference frame (_e.g_. world for \(G_{r}\), object-space for \(G_{o}\)) to the image space of camera \(c\). Opacities \(_{k}^{s,t}\) are computed as per Equation (4), while colors \(_{k}^{s,t}\) are computed for primitives in \(G_{r}\) and in \(G_{o}\) via Equations (6) and (7), respectively. For non-rigid objects in \(G_{o}\), we compute the primitive positions \(_{k}^{t}\) via Equation (5).

Rasterization.To render the scene from camera \(c\), we follow  and splat the 3D Gaussians to the image plane. Practically, for each primitive, we compute a 2D Gaussian kernel denoted by \(_{k}^{c}\) with mean \(_{k}^{c}\) given by the projection of the primitive's position to the image plane, _i.e_. \(_{k}^{c}_{k}^{c}(_{k})\), and with covariance given by \(_{k}^{c}_{k}^{c}_{}_{k }^{c}\), where \(_{k}^{c}\) is the Jacobian of \(_{k}^{c}\) evaluated at \(_{k}\). Finally, we apply traditional alpha compositing of the 3D Gaussians to render pixels \(\) of camera \(c\):

\[^{s,t}()_{k=0}^{K}_{k}^{s,t}w_{k} _{j=0}^{k-1}(1-w_{j}) w_{k}_{k}^{s,t}_{k}^{c}()\,.\] (9)

### Optimization

To optimize parameters \(\) of \(f_{}\), _i.e_. 3D Gaussian parameters \(_{k}\), \(_{k}\), \(q_{k}\) and \(a_{k}\), sequence latent vectors \(_{s}^{t}\) and implicit neural fields \(\) and \(\), we use an end-to-end differentiable rendering pipeline. We render both an RGB color image \(}\) and a depth image \(}\) and apply the following loss function:

\[(},},},) =_{}_{}(}, )+_{}_{}(}, )+_{}_{}(}, )\] (10)

where \(_{}\) is the L1 norm, \(_{}\) is the structural similarity index measure , and \(_{}\) is the L2 norm. We use the posed training images and LiDAR measurements as the ground truth. If no depth ground-truth is available, we drop the depth-related loss from \(\).

Pose optimization.Next to optimizing scene geometry, it is crucial to refine the pose parameters of the reconstruction for in-the-wild scenarios since provided poses often have limited accuracy [10; 17].

Thus, we optimize the residuals \(_{s}^{t}(3)\), \(_{c}(3)\) and \(_{o}^{t}(2)\) jointly with parameters \(\). We constrain object pose residuals to \((2)\) to incorporate the prior that objects move on the ground plane and are oriented upright. See our supp. mat. for details on camera pose gradients.

Adaptive density control.To facilitate the growth and pruning of 3D Gaussian primitives, the optimization of the parameters \(\) is interleaved by an ADC mechanism . This mechanism is essential to achieve photo-realistic rendering. However, it was not designed for training on tens of thousands of images, and thus we develop a streamlined multi-GPU version of it. We accumulate statistics across processes and, instead of running ADC on GPU 0 and synchronizing the results, we synchronize only non-deterministic parts of ADC, _i.e_. the random samples drawn from the 3D Gaussians that are being split. These are usually much fewer than the total number of 3D Gaussians and thus avoids communication overhead. Next, the 3D Gaussian parameters are replaced by their updated replicas. However, this will impair the synchronization of the gradients because, in PyTorch DDP , parameters are only registered once at model initialization. Therefore, we re-initialize the Reducer upon finishing the ADC mechanism in the low-level API provided in .

Furthermore, urban street scenes pose some unique challenges to ADC, such as a large variation in scale, _e.g_. extreme close-ups of nearby cars mixed with far-away buildings and sky. This can lead to blurry renderings for close-ups due to insufficient densification. We address this by using maximum 2D screen size as a splitting criterion.1 In addition, ADC considers the world-space scale \(a_{k}\) of a 3D Gaussian to prune large primitives which hurts background regions far from the camera. Hence, we first test if a 3D Gaussian is inside the scene bounds before pruning it according to \(a_{k}\). Finally, the scale of urban areas leads to memory issues when the growth of 3D Gaussian primitives is unconstrained. Therefore, we introduce a threshold that limits primitive growth while keeping pruning in place. See our supp. mat. for more details and analysis.

## 4 Experiments

Datasets and metrics.We evaluate our approach across various dynamic outdoor benchmarks. First, we utilize the recently proposed NVS benchmark  of Argoverse 2  to compare against the state-of-the-art in the multi-sequence scenario and to showcase the scalability of our method. Second, we use the established Waymo Open , KITTI  and VKITTI2  benchmarks to compare to existing approaches in single-sequence scenarios. For Waymo, we use the dynamic-32 split of , while for KITTI and VKITTI2 we follow [16; 17]. We apply commonly used metrics to measure view synthesis quality: PSNR, SSIM , and LPIPS (AlexNet) .

Implementation details.We use \(_{}:=0.8\), \(_{}:=0.2\) and \(_{} 0.05\). We use the LiDAR point clouds as initialization for the 3D Gaussians. We first filter the points of dynamic objects using the 3D bounding box annotations and subsequently initialize the static scene with the remaining points while using the filtered points to initialize each dynamic object. We use mean voxelization with voxel size \(\) to remove redundant points. See our supp. mat. for more details.

### Comparison to state-of-the-art

We compare with prior art across two experimental settings: _single-sequence_ and _multi-sequence_. In the former, we are given a single input sequence and aim to synthesize hold-out viewpoints from that same sequence. In the latter, we are given _multiple, heterogeneous_ input sequences and aim to synthesize hold-out viewpoints across _all_ of these sequences from a _single_ model.

   &  &  &  &  \\  & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\   Nerfacto + Emb. \\  & 19.83 & 0.637 & 0.562 & 18.05 & 0.655 & 0.625 & 18.94 & 0.646 & 0.594 & 11.5 \\  Nerfacto + Emb. + Time \\  & 20.05 & 0.641 & 0.562 & 18.66 & 0.656 & 0.603 & 19.36 & 0.654 & 0.583 & 11.6 \\  SUDS  \\  & 21.76 & 0.659 & 0.556 & 19.91 & 0.665 & 0.645 & 0.284 & 0.662 & 0.601 & 74.0 \\  ML-NSG  \\  & 22.29 & 0.678 & 0.523 & 20.01 & 0.681 & 0.586 & 21.15 & 0.680 & 0.555 & 21.7 \\ 
 **4DGF (Ours)** \\  & **25.78** & **0.772** & **0.405** & **24.16** & **0.772** & **0.488** & **24.97** & **0.772** & **0.447** & _0.074_ \\  

Table 1: **Novel view synthesis on Argoverse 2 . Our method improves substantially over the state-of-the-art while being more than \(200\) faster to render at the original \(1550 2048\) resolution.**

[MISSING_PAGE_FAIL:8]

being more memory efficient. In particular, when modeling view-dependent color as a per-Gaussian attribute as in  the model uses 8.6 GB of peak GPU memory during training, while it uses only 4.5 GB with fixed-size neural fields. Similarly, storing the parameters of the former takes 922 MB, while the latter takes only 203 MB. Note that this disparity increases with the number of 3D Gaussians per scene. Finally, we achieve the best performance when adding the generated 3D Gaussian background.

We now scrutinize components specific to multi-sequence data in Table 4(b). We compare the view synthesis performance of our model when i) not modeling sequence appearance or transient geometry, ii) only modeling sequence appearance, iii) modeling both sequence appearance _and_ transient geometry. Naturally, we observe a large gap in performance between i) and ii), since the appearance changes between sequences are drastic (see Figure 3). However, there is still a significant gap between ii) and iii), demonstrating that modeling both sequence appearance _and_ transient geometry is important for view synthesis from heterogeneous data sources. Finally, we provide qualitative results for non-rigid object view synthesis in Figure 4, and show that our approach can model articulate motion without the use of domain priors. In our supp. mat., we provide further analysis.

### Runtime analysis

We divide our algorithm into its main components and report the individual inference runtimes across two datasets in Table 6. While the runtime clearly correlates with scene complexity and image resolution, we observe that, on average, the runtime is dominated by scene graph evaluation and rasterization, accounting for more than 75% of the total runtime. This owes to the complexity of rasterizing millions of primitives across a high-resolution image which is computationally demanding even for efficient rasterization algorithms , and handling hundreds to thousands of dynamic objects across one or multiple dynamic captures, making the retrieval of the 3D Gaussians and latent codes costly. In contrast, the queries to the neural fields account for only 13.3% of the average total runtime, making it a viable alternative to the spherical harmonics function in . Overall, our method achieves interactive rendering speeds on both datasets and 20.4 FPS on average.

## 5 Conclusion

We presented 4DGF, a neural scene representation for dynamic urban areas. 4DQF models highly dynamic, large-scale urban areas with 3D Gaussians as efficient geometry scaffold and compact but flexible neural fields modeling large appearance and geometry variations across captures. We use a scene graph to model dynamic object motion and flexibly compose the representation at arbitrary

   Dataset & Argywere 2  & Waymo Open  & \\ Resolution & 1550 \(\) 2048 & 640 \(\) 960 & Mean (ms) & Percentage (\%) \\ Avg. number of 3D Gaussians & 8.024 & 2.75M & \\ 
1. Scene graph evaluation: retrieving \(\), \([|\), 3D Gaussians at \((s,t)\) & 38.5 & 13.0 & 25.75 & 52.4 \\
2. Scene composition: apply \([|\)) to 3D Gaussians & 2.3 & 1.5 & 1.90 & 3.9 \\
3. 3D Gaussian projection & 2.0 & 3.5 & 2.75 & 5.6 \\
4. Query neural fields \(\) and \(\) & 9.5 & 3.6 & 6.55 & 13.3 \\
5. Rasterization & 21.3 & 3.0 & 12.15 & 24.8 \\  Total & 73.6 & 24.6 & 49.1 & 100 \\  FPS & 13.6 & 40.7 & 20.4 & - \\   

Table 6: **Inference runtime analysis**. We report the individual and average timings of our method’s components on two datasets. Overall, scene graph evaluation (1.) and rasterization (5.), dominate the runtime. While total runtime correlates with scene scale and image resolution, we achieve interactive frame rates on both datasets.

   Dynamic & Neural Fields & Background & PSNR \(\) & SSIM \(\) & LPIPS \(\) & GPU Mem. \\  - & - & - & 24.31 & 0.814 & 0.287 & 8.6 GB \\ ✓ & - & - & 28.855 & 0.839 & 0.262 & 8.6 GB \\ ✓ & ✓ & - & 28.49 & 0.838 & 0.262 & 4.5 GB \\ ✓ & ✓ & ✓ & **28.81** & **0.845** & **0.260** & 4.5 GB \\       \(_{s}\) & \(_{s}\) & PSNR \(\) & SSIM \(\) & LPIPS \(\) \\ - & - & 22.37 & 0.741 & 0.456 \\ ✓ & - & 25.13 & 0.767 & 0.412 \\ ✓ & ✓ & **25.78** & **0.772** & **0.405** \\    
   Dataset & Argywere 2  & Waymo Open  & \\ Resolution & 1550 \(\) 2048 & 640 \(\) 960 & Mean (ms) & Percentage (\%) \\ Avg. number of 3D Gaussians & 8.024 & 2.75M & \\ 
1. Scene graph evaluation: retrieving \(\), \([|\), 3D Gaussians at \((s,t)\) & 38.5 & 13.0 & 25.75 & 52.4 \\
2. Scene composition: apply \([|\)) to 3D Gaussians & 2.3 & 1.5 & 1.90 & 3.9 \\
3. 3D Gaussian projection & 2.0 & 3.5 & 2.75 & 5.6 \\
4. Query neural fields \(\) and \(\) & 9.5 & 3.6 & 6.55 & 13.3 \\
5. Rasterization & 21.3 & 3.0 & 12.15 & 24.8 \\  Total & 73.6 & 24.6 & 49.1 & 100 \\  FPS & 13.6 & 40.7 & 20.4 & - \\   

Table 5: **Ablation studies. We show that (a) our approaches to modeling scene dynamics and background regions are effective and neural fields are on-par with spherical harmonics while more memory efficient to train, and (b) using implicit fields for appearance _and_ geometry is crucial for the multi-sequence setting. We control for the maximum number of 3D Gaussians for fair comparison.**configurations and conditions. We jointly optimize the 3D Gaussians, the neural fields, and the scene graph, showing state-of-the-art view synthesis quality and interactive rendering speeds.

Limitations and future work.While 4DGF improves novel view synthesis in dynamic urban areas, the challenging nature of the problem leaves room for further exploration. Although we model scene dynamics, appearance, and geometry variations, other factors influence image renderings in real-world captures. First, in-the-wild captures often exhibit distortions caused by the physical image formation process. Therefore, modeling phenomena like rolling shutter, white balance, motion and defocus blur, or chromatic aberrations is necessary to avoid reconstruction artifacts. Second, the assumption of a pinhole camera model in  persists in our work. Thus, our method falls short of modeling more complex camera models like equirectangular cameras and other sensors such as LiDAR, which may be limiting for certain capturing or simulation settings.

Broader impact.We expect our work to positively impact real-world use cases like robotic simulation and mixed reality by improving the underlying technology. While we do not expect malicious uses of our method, we note that an inaccurate simulation, _i.e_. a failure of our system, could misrepresent the robotic system performance, possibly affecting real-world deployment.

Figure 4: **Qualitative results on Waymo Open .** We show a sequence of evaluation views synthesized by our model (top-left to bottom-right). As the woman (marked with a red box) gets out of the car and walks away, we successfully model her articulated motion and changing body poses.

Figure 3: **Qualitative results on Argoverse 2 . Our method produces significantly sharper renderings both in foreground dynamic and static background regions, with much fewer artifacts _e.g_. in areas with transient geometry such as tree branches (left). Best viewed digitally.**

Acknowledgements

The authors thank Haithem Turki, Songyou Peng, Erik Sandstrom, Francois Darmon, and Jonathon Luiten for useful discussions. Tobias Fischer was supported by a Meta SRA.