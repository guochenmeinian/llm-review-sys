# Nearly Minimax Optimal Regret for Multinomial Logistic Bandit

Joongkyu Lee

Seoul National University

Seoul, South Korea

jklee0717@snu.ac.kr&Min-hwan Oh

Seoul National University

Seoul, South Korea

minoh@snu.ac.kr

###### Abstract

In this paper, we study the contextual multinomial logistic (MNL) bandit problem in which a learning agent sequentially selects an assortment based on contextual information, and user feedback follows an MNL choice model. There has been a significant discrepancy between lower and upper regret bounds, particularly regarding the maximum assortment size \(K\). Additionally, the variation in reward structures between these bounds complicates the quest for optimality. Under uniform rewards, where all items have the same expected reward, we establish a regret lower bound of \((d)\) and propose a constant-time algorithm, OFU-MNL+, that achieves a matching upper bound of \(}(d)\). We also provide instance-dependent minimax regret bounds under uniform rewards. Under non-uniform rewards, we prove a lower bound of \((d)\) and an upper bound of \(}(d)\), also achievable by OFU-MNL+. Our empirical studies support these theoretical findings. To the best of our knowledge, this is the first work in the contextual MNL bandit literature to prove minimax optimality -- for either uniform or non-uniform reward setting -- and to propose a computationally efficient algorithm that achieves this optimality up to logarithmic factors.

## 1 Introduction

The multinomial logistic (MNL) bandit framework [47; 48; 7; 8; 40; 41; 44; 5; 53] describes sequential assortment selection problems in which an agent offer a sequence of assortments of at most \(K\) item from a set of \(N\) possible items and receives feedback _only_ for the chosen decisions. The choice probability of each outcome is characterized by an MNL model . This framework allows modeling of various real-world situations such as recommender systems and online details, where selections of assortments are evaluated based on the user-choice feedback among offered multiple options.

In this paper, we study the _contextual_ MNL bandit problem [8; 7; 43; 16; 40; 41; 44; 5], where the features of items and possibly contextual information about a user at each round are available. Despite many recent advances, [16; 40; 41; 44; 5], however, no previous studies have proven the minimax optimality of contextual MNL bandits. Chen et al.  proposed a regret lower bound of \((d/K)\), where \(d\) is the number of features, \(T\) is the total number of rounds, and \(K\) is the maximum size of assortments, assuming the uniform rewards, i.e., rewards are all same for each of the total \(N\) items. Furthermore, Chen and Wang  established a regret lower bound of \(()\) in the non-contextual setting (hence, dependence on \(N\) appears instead of \(d\)), which is tighter in terms of \(K\). It is important to note the difference in the assumptions for the _attraction parameter for the outside option_\(v_{0}\). Chen and Wang  assumed for the attraction parameter for the outside option to be \(v_{0}=K\), whereas Chen et al.  assumed \(v_{0}=1\). Therefore, it remains an _open question whether and how the value of \(v_{0}\) affects both lower and upper bounds of regret_.

Regarding regret upper bounds, Chen et al.  proposed an exponential runtime algorithm that achieves a regret of \(}(d)\) in the setting with _stochastic_ contexts and the _non-uniform_ rewards. Under the same setting, Oh and Iyengar  and Oh and Iyengar  introduced polynomial-time algorithms that attain regrets of \(}(d/)\) and \(}(d^{3/2}/)\) respectively, where \(1/=(K^{2})\) is a problem-dependent constant. Recently, Perivier and Goyal  improved the dependency on \(\) in the _adversarial_ context setting, achieving a regret of \(}(dKT})\), where \(^{}=(1/K)\). However, their approach focuses solely on the setting with _uniform_ rewards, which is a special case of non-uniform rewards, and currently, there is no tractable method to implement the algorithm.

As summarized in Table 1, there has been a gap between the upper and lower bounds in the existing works of contextual MNL bandits. No previous studies have confirmed whether lower or upper bounds are tight, obscuring what the optimal regret should be. This ambiguity is further exacerbated because many studies introduce their methods under varying conditions such as different reward structures and values of \(v_{0}\), without explicitly explaining how these factors impact regret. Additionally, there is currently no computationally efficient algorithm whose regret does not scale with \(1/=}(K^{2})\) or directly with \(K\). Intuitively, increasing \(K\) provides more information at least in the uniform reward setting, potentially leading to a more statistically efficient learning process. However, no previous results have reflected such intuition. Hence, the following research questions arise:

* _What is the optimal regret lower bound in contextual MNL bandits?_
* _Can we design a computationally efficient, nearly minimax optimal algorithm under the adversarial context setting?_

In this paper, we affirmatively answer the questions by first tackling the contextual MNL bandit problem separately based on the structure of rewards--uniform and non-uniform--and the value of the outside option \(v_{0}\). In the setting of uniform rewards, we establish the tightest regret lower bound, explicitly demonstrating the dependence of regret on \(v_{0}\). Specifically, we prove a regret lower bound of \((d)\) when \(v_{0}=(1)\), a common assumption in contextual settings  (see Appendix C.1 for more details), and a lower bound of \((d)\) when \(v_{0}=(K)\). Furthermore, in the adversarial context setting, we introduce a computationally efficient and provably optimal (up to logarithmic factors) algorithm, OFU-MNL+. We prove that our proposed algorithm achieves a regret of \(}(d)\) when \(v_{0}=(1)\) and \(}(d)\) when \(v_{0}=(K)\), each of which matches the respective lower bounds that we establish up to logarithmic factors. Furthermore, in the non-uniform reward setting, we provide the optimal lower bound of \((d)\) assuming \(v_{0}=(1)\). In the same setting, our proposed algorithm also attains a matching upper bound of \(}(d)\) up to logarithmic factors. Our main contributions are summarized as follows:

    & & Regret & Contexts & Rewards & \(v_{0}\) & Comput. per Round \\    } & Chen et al.  & \((d/K)\) & – & Uniform & \((1)\) & – \\  & Agrawal et al. * & \((/K)\) & – & Uniform & \((K)\) & – \\  & Chen and Wang * & \(()\) & – & Uniform & \((K)\) & – \\  & **This work** (Theorem 1) & \((}{v_{0}+K}d)\) & – & Uniform & Any value & – \\  & **This work** (Theorem 3) & \((d)\) & – & Non-uniform & \((1)\) & – \\    } & Chen et al.  & \(}(d)\) & Stochastic & Non-uniform & \((1)\) & Intractable \\  & Oh and Iyengar  & \(}(d)\) & Stochastic & Non-uniform & \((1)\) & \((t)\) \\  & Oh and Iyengar  & \(}(d/T}/)\) & Adversarial & Non-uniform & \((1)\) & \((t)\) \\  & Perivier and Goyal  & \(}(dKT})\) & Adversarial & Uniform & \((1)\) & Intractable \\  & **This work** (Theorem 2) & \(}(}{v_{0}+K}d)\) & Adversarial & Uniform & Any value & \((1)\) \\  & **This work** (Theorem 4) & \(}(d)\) & Adversarial & Non-uniform & \((1)\) & \((1)\) \\   

Table 1: Comparisons of lower and upper regret bounds in related works on MNL bandits with \(T\) rounds, \(N\) items, the maximum size of assortments \(K\), \(d\)-dimensional feature vectors, and problem-dependent constants \(1/=(K^{2})\) and \(^{}=(1/K)\). \(}\) represents big-\(\) notation up to logarithmic factors. For the computational cost (abbreviated as “Comput.”), we consider only the dependence on the number of rounds \(t\). “Intractable” means a non-polynomial runtime. The notation “\(-\)” denotes _not applicable_. The starred (*) papers only consider the non-contextual setting.

* Under uniform rewards, we establish a regret lower bound of \((K}/(v_{0}+K)d)\) (Theorem 1), which is the tightest known lower bound in contextual MNL bandits. We propose, for the first time, a computationally efficient and provably optimal algorithm, OFU-NNL+, achieving a matching upper bound of \((K}/(v_{0}+K)d)\) (Theorem 2) up to logarithmic factors, while requiring only a constant computation cost per round. The results indicate that the regret improves as the assortment size \(K\) increases, unless \(v_{0}=(K)\). To the best of our knowledge, this is the first study to demonstrate the dependence of regret on the attraction parameter for the outside option \(v_{0}\) and to highlight the advantages of a larger assortment size \(K\) which aligns with intuition. That is, this is the first work to show that a regret upper bound (in either contextual or non-contextual setting) decreases as \(K\) increases. Additionally, we provide instance-dependent minimax regret bounds (Proposition 1 and 2), up to logarithmic factors.
* Under non-uniform rewards, with setting \(v_{0}=(1)\) following the convention in contextual MNL bandits , we establish a regret lower bound of \((d)\) (Theorem 3). To the best of our knowledge, this is the first and tightest lower bound established under non-uniform rewards. Moreover, OFU-MNL+ also achieves a matching upper bound (up to logarithmic factors) of \(}(d)\) (Theorem 4) in this setting.
* We also conduct numerical experiments and show that our algorithm consistently outperforms the existing MNL bandit algorithms while maintaining a constant computation cost per round. Furthermore, the empirical results corroborate our theoretical findings regarding the dependence of regret on the reward structure, \(v_{0}\) and \(K\).

Overall, our paper addresses the long-standing open problem of closing the gap between upper and lower bounds for contextual MNL bandits. Our proposed algorithm is the first to achieve both provably optimality (up to logarithmic factors) and practicality with improved computation.

## 2 Related Work

**Lower bounds of MNL bandits.** In contextual MNL bandits, to the best of our knowledge, only Chen et al.  proved a lower bound of \((d/K)\) with the attraction parameter for the outside option set at \(v_{0}=1\). However, in the non-contextual setting, there exist improved lower bounds in terms of \(K\). Agrawal et al.  demonstrated a lower bound of \(()\), and Chen and Wang  established a lower bound of \(()\). By setting \(d=N\), one can derive equivalent lower bounds for the contextual setting, specifically \(()\) and \(()\), respectively. However, Agrawal et al.  and Chen and Wang  assumed \(v_{0}=K\) when establishing their lower bounds, which differs from the setting used by Chen et al. , where \(v_{0}=1\). Moreover, to the best of our knowledge, all existing works Chen et al. , Agrawal et al. , Chen and Wang  have established the lower bounds under uniform rewards. Consequently, it remains unclear what the optimal regret is, depending on the value of \(v_{0}\) and the reward structure.

**Upper bounds of contextual MNL bandits.** Ou et al.  formulated a linear utility model and achieved \(}(dK)\) regret; however, they assumed that utilities are fixed over time. Chen et al.  considered contextual MNL bandits with changing and stochastic contexts, establishing a regret of \(}(d+d^{2}K^{2})\). However, they encountered computational issues due to the need to enumerate all possible (\(N\) choose \(K\)) assortments. To address this, Oh and Iyengar  proposed a polynomial-time assortment optimization algorithm, which maintains the confidence bounds in the parameter space and then calculates the upper confidence bounds of attraction parameter for each item, achieving a regret of \(}(d/)\), where \(1/=(K^{2})\) is a problem-dependent constant. Perivier and Goyal  considered the adversarial context and uniform reward setting and improved the dependency on \(\) to \(}(dKT}+d^{2}K^{4}/)\), where \(^{}=(1/K)\). However, their algorithm is intractable. Agrawal et al.  considered a uniform rewards setting (with \(v_{0}=1\)) and achieved a regret of \(}(d)\). However, due to significant technical errors in their paper (refer Appendix L), we do not include a comparison with their results in this work.

Recently, Zhang and Sugiyama  utilized an online parameter update to construct a constant time algorithm. However, they consider a _multiple-parameter_ choice model in which the learner estimates \(K\) parameters and shares the contextual information \(x_{t}\) across the items in the assortment. This model differs from ours; we use a _single-parameter_ choice model with varying the context for each item in the assortment. Additionally, they make a stronger assumption regarding the reward than we do (see Assumption 1). Moreover, while they fix the assortment size at \(K\), we allow it to be smaller than or equal to \(K\). On the other hand, Zhang and Luo  considered a general function approximation, achieving a regret bound of \(}(K^{2.5})\). However, this bound scales with \(K\) and \(N\), and the proposed algorithm is not tractable. To the best of our knowledge, all existing methods fail to show that the regret upper bound can improve as the assortment size \(K\) increases.

## 3 Problem Setting

**Notations.** For a positive integer, \(n\), we denote \([n]:=\{1,2,,n\}\). For a real-valued matrix \(A\), we denote \(\|A\|_{2}:=_{x:\|x\|_{2}=1}\|Ax\|_{2}\) as the maximum singular value of \(A\). For two symmetric matrices, \(V\) and \(W\) of the same dimensions, \(V W\) means that \(V-W\) is positive semi-definite. Finally, we define \(\) to be the set of candidate assortment with size constraint at most \(K\), i.e., \(=\{S[N]:|S| K\}\). While, for simplicity, we consider both \(\) and the set of items \([N]\) to be stationary in this paper, it is important to note that both \(\) and \([N]\) can vary over time.

**Contextual MNL bandits.** We consider a sequential assortment selection problem which is defined as follows. At each round \(t\), the agent observes feature vectors \(x_{ti}^{d}\) for every item \(i[N]\). Based on this contextual information, the agent presents an assortment \(S_{t}=\{i_{1},,i_{l}\}\), where \(l K\), and then observes the user purchase decision \(c_{t} S_{t}\{0\}\), where \(\{0\}\) represents the "outside option" which indicates that the user did not select any of the items in \(S_{t}\). The distribution of these selections follows a multinomial logistic (MNL) choice model , where the probability of choosing any item \(i_{k} S_{t}\) (or the outside option) is defined as:

\[p_{t}(i_{k}|S_{t},^{}):=}^{}^ {})}{v_{0}\!+\!_{j S_{t}}(x_{tj}^{}^{})},  p_{t}(0|S_{t},^{}):=}{v_{0}\!+\!_{j S_{ t}}(x_{tj}^{}^{})},\] (1)

where \(v_{0}\) is a _known_ attraction parameter for the outside option and \(^{}^{d}\) is an _unknown_ parameter.

**Remark 1**.: _In the existing literature on MNL bandits, it is commonly assumed that \(v_{0}=1\). On the other hand, Chen and Wang , Agrawal et al.  assume that \(v_{0}=K\)1 to induce a tighter lower bound in terms of \(K\). Later, we will explore how these differing assumptions create fundamentally different problems, leading to different regret lower bounds (Subsection 5.1)._

The choice response for each item \(i S_{t}\{0\}\) is defined as \(y_{ti}:=(c_{t}=i)\{0,1\}\). Hence, the choice feedback variable \(_{t}:=(y_{t0},y_{ti_{1}},,y_{t_{li}})\) is sampled from the following multinomial (MNL) distribution: \(_{t}\{1,(p_{t}(0|S_{t},^{}),,p_ {t}(i_{l}|S_{t},^{}))\}\), where the parameter 1 indicates that \(_{t}\) is a single-trial sample, i.e., \(y_{t0}+_{k=1}^{k}y_{ti_{k}}=1\). For each \(i S_{t}\{0\}\), we define the noise \(_{ti}:=y_{ti}-p_{t}(i|S_{t},^{})\). Since each \(_{ti}\) is a bounded random variable in \(\), \(_{ti}\) is \(1/4\)-sub-Gaussian. At every round \(t\), the reward \(r_{ti}\) for each item \(i\) is also given. Then, we define the expected revenue of the assortment \(S\) as

\[R_{t}(S,^{}):=_{i S}p_{t}(i|S,^{})r_{ti}= (x_{ti}^{}^{})r_{ti}}{v_{0}\!+\!_ {j S}(x_{tj}^{}^{})}\]

and define \(S_{t}^{}\) as the offline optimal assortment at time \(t\) when \(^{}\) is known a prior, i.e., \(S_{t}^{}=*{argmax}_{S}R_{t}(S,^{ })\). Our objective is to minimize the cumulative regret over the \(T\) periods:

\[_{T}(^{})=_{t=1}^{T}R_{t}(S_{t}^{},^{})-R_{t}(S_{t},^{}).\]

When \(K=1\), \(r_{t1}=1\), and \(v_{0}=1\), the MNL bandit recovers the binary logistic bandit with \(R_{t}(S=\{x\},^{})=(x^{}^{}) =1/(1+(-x^{}^{}))\), where \(()\) is the sigmoid function.

Consistent with previous works on MNL bandits , we make the following assumptions:

**Assumption 1** (Bounded assumption).: _We assume that \(\|^{}\|_{2} 1\), and for all \(t 1\), \(i[N]\), \(\|x_{ti}\|_{2} 1\) and \(r_{ti}\)._

**Assumption 2** (Problem-dependent constant).: _There exist \(>0\) such that for every item \(i S\) and any \(S\), and all round \(t\), \(_{}p_{t}(i|S,)p_{t}(0|S,)\), where \(=\{^{d}\|\|_{2} 1\}\)._

In Assumption 1, we assume that the reward for each item \(i\) is bounded by a constant, allowing the norm of the reward vector to depend on \(K\), e.g., \(|_{t}|_{2}\). In contrast, Zhang and Sugiyama  assume that the norm of the reward vector \(_{t}=[r_{t1}, r_{t|S_{i}|}]^{}^{|S_{t}|}\) is bounded by a constant, independent of \(K\), e.g., \(\|_{t}\|_{2} 1\). Thus, our assumption regarding rewards is weaker than theirs.

Assumption 2 is common in contextual MNL bandits [16; 41; 44; 53]. Note that \(1/\) depends on the maximum size of the assortment \(K\), i.e., \(1/=(K^{2})\). One of the primary goals of this paper is to show that as the assortment size \(K\) increases, we can achieve an improved (or at least not worsened) regret bound. To this end, we design a dynamic assortment policy that enjoys improved dependence on \(\). Note that our algorithm does not need to know \(\) a priori, whereas Oh and Iyengar [40; 41] do.

## 4 Existing Gap between Upper and Lower Bounds in MNL Bandits

The primary objective of this paper is to establish minimax regrets in contextual MNL bandits. To explore the optimality of regret, we analyze how it depends on the attraction parameter for the outside option \(v_{0}\), the maximum assortment size \(K\), and the structure of rewards.

**Dependence on \(v_{0}\).** Currently, the established lower bounds are \((d/K)\) by Chen et al. , \(()\) by the contextual version of Agrawal et al. , and \(()\), which is the tightest in terms of \(K\), by the contextual version of Chen and Wang . These results can be misleading, as many subsequent studies [41; 39; 17; 52] have claimed that a \(K\)-independent regret is achievable, without clearly addressing the influence of the value of \(v_{0}\). In fact, the improved regret bounds (in terms of \(K\)) obtained by Agrawal et al.  and Chen and Wang  were possible when \(v_{0}=K\). However, in the contextual setting, it is more common to set \(v_{0}=(1)\). This is because, given the context for the outside option \(x_{t0}\), it is straightforward to construct an equivalent choice model where \(v_{0}=(1)\) (refer Appendix C.1). In this paper, we rigorously show the regret dependency on the value of \(v_{0}\). In Theorem 1, we establish a regret lower bound of \((K}/(v_{0}+K)d)\), which implies that the value of \(v_{0}\), indeed, affects the regret. Then, in Theorem 2, we show that our proposed computationally efficient algorithm, OFU-MNL+ achieves a regret of \(}(K}/(v_{0}+K)d)\), which is minimax optimal up to logarithmic factors in terms of all \(d,T,K\) and even \(v_{0}\).

**Dependence on \(K\) & Uniform/Non-uniform rewards.** To the best of our knowledge, the regret bound in all existing works in contextual MNL bandits does not decrease as the assortment size \(K\) increases [16; 40; 41; 44]. However, intuitively, as the assortment size increases, we can gain more information because we receive more feedback. Therefore, it makes sense that regret could be reduced as \(K\) increases, at least in the uniform reward setting. Under uniform rewards, the expected revenue (to be specified later) increases as more items are added in the assortment. Consequently, both the optimistically chosen assortment and the optimal assortment always have a size of \(K\). Thus, the agent obtain information about exactly \(K\) items in each round. This phenomenon is also demonstrated empirically in Figure 1. In the uniform reward setting, as \(K\) increases, the cumulative regrets of not only our proposed algorithm but also other baseline algorithms decrease. This indicates that the existing regret bounds are not tight enough in terms of \(K\). Conversely, in the non-uniform reward setting, the sizes of both the optimistically chosen assortment and the optimal assortment can be less than \(K\), so performance improvement is not guaranteed. In this paper, we show that the regret dependence on \(K\) varies by case: uniform and non-uniform rewards. When \(v_{0}=(1)\), we obtain a regret lower bound of \((d)\) (Theorem 1) and a regret upper bound of \(}(d)\) (Theorem 2) under uniform rewards. Additionally, we achieve a regret lower bound of (Theorem 3) and a regret upper bound of \(}(d)\) (Theorem 4) under non-uniform rewards.

## 5 Algorithms and Main Results

In this section, we begin by proving the tightest regret lower bound under uniform rewards (Subsection 5.1), explicitly showing the dependence on the attraction parameter for the outside option \(v_{0}\). We then introduce OFU-MNL+, an algorithm that achieves minimax optimality, up to logarithmic factors, under _uniform rewards_ (Subsection 5.2). Notably, OFU-MNL+ is designed for efficiency, requiringonly constant computation and storage costs. Finally, we establish the tightest regret lower bound and a matching minimax optimal regret upper bound under _non-uniform rewards_ (Subsection 5.3).

### Regret Lower Bound under Uniform Rewards

In this subsection, we present a lower bound for the worst-case expected regret in the uniform reward setting (\(r_{ti}=1\)). This covers all applications where the objective is to maximize the appropriate "click-through rate" by offering the assortment.

**Theorem 1** (Regret lower bound, Uniform rewards).: _Let \(d\) be divisible by \(4\) and let Assumption 1 hold true. Suppose \(T C d^{4}(v_{0}+K)/K\) for some constant \(C>0\). Then, in the uniform reward setting, for any policy \(\), there exists a worst-case problem instance with \(N=(K 2^{d})\) items such that the worst-case expected regret of \(\) is lower bounded as follows:_

\[_{}_{}^{}[_{T}()]=(K}}{v_{0}+K} d).\]

Discussion of Theorem 1.If \(v_{0}=(1)\), Theorem 1 demonstrates a regret lower bound of \((d)\). This indicates that, under uniform rewards, increasing the assortment size \(K\) leads to an improvement in regret. Compared to the lower bound \((d/K)\) proposed by Chen et al. , our lower bound is improved by a factor of \(\). This improvement is mainly due to the establishment of a tighter upper bound for the KL divergence (Lemma D.2). Notably, Chen et al.  also considered uniform rewards with \(v_{0}=(1)\). On the other hand, Chen and Wang  and Agrawal et al.  established regret lower bounds of \(()\) and \(()\), respectively, in non-contextual MNL bandits with uniform rewards, by setting \(v_{0}=K\) to achieve these regrets. Theorem 1 shows that if \(v_{0}=(K)\), we can obtain a regret lower bound of \((d)\), which is consistent with the \(K\)-independent regret in Chen and Wang . To the best of our knowledge, this result is the first to explicitly show the dependency of regret on the attraction parameter for the outside option \(v_{0}\). The proof is deferred to Appendix D.

### Minimax Optimal Regret Upper Bound under Uniform Rewards

In this subsection, we propose a new algorithm OFU-MNL+, which enjoys minimax optimal regret up to logarithmic factors in the case of uniform rewards. Note that, since the revenue is an increasing function when rewards are uniform, maximizing the expected revenue \(R_{t}(S,)\) over all \(S\) always yields exactly \(K\) items, i.e., \(|S_{t}|=|S_{t}^{}|=K\).

Our first step involves constructing the confidence set for the online parameter.

**Online parameter estimation.** Instead of performing MLE as in previous works [16; 41; 44], inspired by Zhang and Sugiyama , we use the online mirror descent algorithm to estimate parameter. We first define the multinomial logistic loss function at round \(t\) as:

\[_{t}():=-_{i_{t}}y_{ti} p_{t}(i|S_{t}, ).\] (2)

In Proposition C.1, we will show that the loss function has the constant parameter self-concordant-like property. We estimate the true parameter \(^{}\) as follows:

\[_{t+1}=*{argmin}_{} _{t}(_{t}),+ -_{t}_{_{t}}^{2}, t 1,\] (3)

where \(>0\) is the step-size parameter to be specified later, and \(_{t}:=H_{t}+_{t}(_{t})\), where

\[_{t}():=_{i_{t}}p_{t}(i|S_{t},)x_{ti}x_{ti}^{}-_{i_{t}}_{j_{t}}p_{t}(i|S_{t}, )p_{t}(j|S_{t},)x_{ti}x_{tj}^{},\]

and \(H_{t}:=_{d}+_{s=1}^{t-1}_{s}(_{s+1})\). Note that \(_{t}()=^{2}_{t}()\). This online estimator is efficient in terms of both computation and storage. By a standard online mirror descent formulation , the optimization problem in (3) can be solved using a single projected gradient step through the following equivalent formula:

\[_{t+1}^{}=_{t}-_{t}^{-1}_{t} (_{t}),_{t+1}=*{argmin}_{ }\|-_{t+1}^{}\|_{_{t}},\] (4)which enjoys a computational cost of only \((Kd^{3})\), completely independent of \(t\). Regarding storage costs, the estimator does not need to store all historical data because both \(_{t}\) and \(H_{t}\) can be updated incrementally, requiring only \((d^{2})\) storage.

Furthermore, the estimator allows for a \(\)-independent confidence set, leading to an improved regret.

**Lemma 1** (Online parameter confidence set).: _Let \((0,1]\). Under Assumption 1, with \(=(K+1)+2\) and \(=84d\), we define the following confidence set_

\[_{t}():=\{|_{t}- \|_{H_{t}}_{t}()\},\]

_where \(_{t}()=( t K)\). Then, we have \([ t 1,^{}_{t}( )] 1-\)._

Armed with the online estimator, we construct the computationally efficient optimistic revenue.

**Computationally efficient optimistic expected revenue.** To balance the exploration and exploitation trade-off, we use the upper confidence bounds (UCB) technique, which have been widely studied in many bandit problems, including \(K\)-arm bandits  and linear bandits .

At each time \(t\), given the confidence set in Lemma 1, we first calculate the optimistic utility \(_{ti}\) as:

\[_{ti}:=x_{ti}^{}_{t}+_{t}()\|x_{ti}\|_{H_{t}^{- 1}}, i[N].\] (5)

The optimistic utility \(_{ti}\) is composed of two parts: the mean utility estimate \(x_{ti}^{}_{t}\) and the standard deviation \(_{t}()\|x_{ti}\|_{H_{t}^{-1}}\). In the proof of the regret upper bound, we show that \(_{ti}\) serves as an upper bound for \(x_{ti}^{}^{}\), assuming that the true parameter \(^{}\) falls within the confidence set \(_{t}()\). Based on \(_{ti}\), we construct the optimistic expected revenue for the assortment \(S\), defined as follows:

\[_{t}(S):=(_{ti})r_{ti}}{v_{0}+_{j S }(_{tj})},\] (6)

where \(r_{ti}=1\). Then, we offer the set \(S_{t}\) that maximizes the optimistic expected revenue, \(S_{t}=*{argmax}_{S}_{t}(S)\). Given our assumption that all rewards are of unit value, the optimization problem is equivalent to selecting the \(K\) items with the highest optimistic utility \(_{ti}\). Consequently, solving the optimization problem incurs a constant computational cost of \((N)\).

**Remark 2** (Comparison to Zhang and Sugiyama ).: _In Zhang and Sugiyama , the MNL choice model is outlined with a shared context \(x_{t}\) and distinct parameters \(_{1}^{},,_{K}^{}\) for each choice. Conversely, our model employs a single parameter \(^{}\) across all choices and has varying contexts for each item in the assortment \(S\), \(x_{ti1}, x_{ti_{|S|}}\). Due to this discrepancy in the choice model, directly applying Proposition 1 from Zhang and Sugiyama , which constructs the optimistic revenue by adding bonus terms to the estimated revenue, incurs an exponential computational cost in our problem setting. This complexity arises because the optimistic revenue must be calculated for every possible assortment \(S\); therefore, it is necessary to enumerate all potential assortments (\(N\) choose \(K\)) to identify the one that maximizes the optimistic revenue As a result, extending the approach in Zhang and Sugiyama  to our setting is non-trivial, requiring a different analysis._

We now present the regret upper bound of OFU-MNL+ in the uniform reward setting.

**Theorem 2** (Regret upper bound of OFU-MNL+, Uniform rewards).: _Let \((0,1]\) and Assumptions 1 and 2 hold. In the uniform reward setting, by setting \(=(K+1)+2\) and \(=84d\), with probability at least \(1-\), the cumulative regret of OFU-MNL+ is upper-bounded by_

\[_{T}(^{})=}( K}}{v_{0}+K} d+d^{2}).\]

Discussion of Theorem 2.: If \(T(d^{2}(v_{0}+K)^{2}/(^{2}v_{0}K))\), Theorem 2 shows that our algorithm OFU-MNL+ achieves minimax optimal regret (up to logarithmic factor) in terms of all \(d\), \(T\), \(K\), and even \(v_{0}\). To the best of our knowledge, ignoring logarithmic factors, our proposed algorithm is the first computationally efficient, minimax optimal algorithm in (adversarial) contextual MNL bandits. When \(v_{0}=(1)\), which is the convention in existing MNL bandit literature [40; 41; 44; 5; 53], OFU-MNL+ obtains \(}(d)\) regret. This represents an improvement over the previous upper bound of Perivier and Goyal 2, which is \(}(dKT}+d^{2}K^{4}/)\), where \(^{}=(1/K)\), by a factor of \(K\). This improvement can be attributed to two key factors: an improved, constant, self-concordant-like property of the loss function (Proposition C.1) and a \(K\)-free elliptical potential lemma (Lemma E.2). Furthermore, by employing an improved bound for the second derivative of the revenue (Lemma E.3), we achieve an enhancement in the regret for the second term, \(d^{2}/\), by a factor of \(K^{4}\), in comparison to Perivier and Goyal . Unless \(v_{0}=(K)\), Theorem 2 indicates that the regret decreases as the assortment size \(K\) increases. To the best of our knowledge, this is the first algorithm in MNL bandits to show that increasing \(K\) results in a reduction in regret. Moreover, when reduced to the logistic bandit, i.e., \(K=1\), \(r_{t1}=1\), and \(v_{0}=1\), our algorithm can also achieve a regret of \(}(d)\) by Corollary 1 in Zhang and Sugiyama , which is consistent with the results in Abeille et al. , Faury et al. . The proof is deferred to Appendix E.

**Remark 3** (Efficiency of OFU-MNL+).: _The proposed algorithm is computationally efficient in both parameter updates and assortment selections. Since we employ online parameter estimation, akin to Zhang and Sugiyama , our algorithm demonstrates computational efficiency in parameter estimation, incurring only incurring \((Kd^{3})\) computation cost and \((d^{2})\) storage cost, which are completely independent of \(t\). Furthermore, a naive approach to selecting the optimistic assortment requires enumerating all possible (\(N\) choose \(K\)) assortments, resulting in exponential computational cost . However, by constructing the optimistic expected revenue according to (6) (inspired by Oh and Iyengar ), our algorithm needs only \((N)\) computational cost._

### Regret Upper & Lower Bounds under Non-Uniform Rewards

In this subsection, we propose regret upper and lower bounds in the non-uniform reward setting. In this scenario, the sizes of both the chosen assortment \(S_{t}\), and the optimal assortment, \(S_{t}^{}\) are not fixed at \(K\). Therefore, we cannot guarantee an improvement in regret even as \(K\) increases.

We first prove the regret lower bound in the non-uniform reward setting.

**Theorem 3** (Regret lower bound, Non-uniform rewards).: _Under the same conditions as Theorem 1, let the rewards be non-uniform and \(v_{0}=(1)\). Then, for any policy \(\), there exists a worst-case problem instance such that the worst-case expected regret of \(\) is lower bounded as follows:_

\[_{}_{}^{}[_{T}()]=(d).\]

Discussion of Theorem 3.: In contrast to Theorem 1, which considers uniform rewards, the regret lower bound is independent of the assortment size \(K\). Note that Theorem 3 does not claim that non-uniform rewards inherently make the problem more difficult. Rather, it implies that there exists an instance with _adversarial_ non-uniform rewards, where regret does not improve even with an increase in \(K\). Moreover, the assumption that \(v_{0}=(1)\) is common in the existing literature on contextual MNL bandits [40; 41; 44; 5; 53] (refer Appendix C.1). To the best of our knowledge, this is the first established lower bound for non-uniform rewards in MNL bandits, even in the non-contextual setting. The proof is deferred to Appendix G.

We also prove a matching upper bound up to logarithmic factors. The algorithm OFU-MNL+ is also applicable in the case of non-uniform rewards. However, because the optimistic expected revenue \(_{t}(S)\) is no longer an increasing function of \(_{ti}\), optimizing for \(S_{t}=*{argmax}_{S}_{t}(S)\) no longer equates to simply selecting the top \(K\) items with the highest optimistic utility. Instead, we employ assortment optimization methods introduced in Rusmevichientong et al. (2019); Davis et al. (2019), which are efficient polynomial-time (independent of \(t\)) 3 algorithms available for solving this optimization problem. Therefore, our algorithm is also computationally efficient under non-uniform rewards.

**Theorem 4** (Regret upper bound of OFU-MNL+, Non-uniform rewards).: _Under the same assumptions and parameter settings as Theorem 2, if the rewards are non-uniform and \(v_{0}=(1)\), then with a probability at least \(1-\), the cumulative regret of OFU-MNL+ is upper-bounded by_

\[_{T}(^{})=}(d+ d^{2}).\]

**Discussion of Theorem 4.** If \(T(d^{2}/^{2})\), our algorithm achieves a regret of \(}(d)\) when the reward for each item is non-uniform, demonstrating that OFU-MNL+ is minimax optimal up to a logarithmic factor. Recall that we relax the bounded assumption on the reward compared to Zhang and Sugiyama (2019) (refer Assumption 1); thus, we allow the sum of the squared rewards in the assortment to scale with \(K\). Consequently, we need a novel approach to achieve the regret that does not scale with \(K\). To this end, we _centralize_ the features, i.e., \(_{ti}=x_{ti}-_{j p_{i}( S_{t},_{t+1})}[x _{tj}]\), and propose a novel elliptical potential lemma for them, as detailed in Lemma H.3. Note that our algorithm is capable of achieving \(1/\)-free regret (in the leading term) under both uniform and non-uniform rewards. In contrast, the algorithm in Perivier and Goyal (2019) is limited to achieving this only in the uniform reward setting. Furthermore, compared to the regret bound in Chen et al. (2019), which is \(}(d)\), our regret bounds has the same order of regret with theirs. However, their algorithm is computationally intractable as it requires enumerating all possible assortments, whereas our algorithm incurs only a constant computational cost per round. The proof is deferred to Appendix H.

## 6 Instance-Dependent Bounds

In this section, we show that instance-dependent upper and lower bounds are also achievable under uniform rewards. We define the degree of non-linearity for the optimal assortment \(S_{t}^{}\) at round \(t\) under the true parameter \(^{}\) as \(_{t}^{}:=_{i S_{t}^{}}p_{t}(i|S_{t}^{},^ {})p_{t}(0|S_{t}^{},^{})\). We first establish the instance-dependent lower bound under uniform rewards.

**Proposition 1** (Instance-dependent regret lower bound, Uniform rewards).: _Under the same conditions as Theorem 1, for any policy \(\) and for \(T d^{2}/\), there exists a worst-case problem instance such that the worst-case expected regret of \(\) is lower bounded as follows:_

\[_{}_{}^{}[_{T}()]=(d_{^{T}_{t}^{}}}).\]

The proof is deferred to Appendix I. We also provide the matching upper bound.

**Proposition 2** (Instance-dependent regret upper bound of OFU-MNL+, Uniform rewards).: _Under the same assumptions, parameter settings, and reward structure as Theorem 2, with a probability at least \(1-\), the cumulative regret of OFU-MNL+ is upper-bounded by_

\[_{T}(^{})=}(d^{T}_{t}^{}}+d^{2}).\]

The proof is deferred to Appendix J. For sufficiently large \(T\), the regret upper bound (Proposition 2) matches the regret lower bound (Proposition 1), up to logarithmic factor. To the best of our knowledge, these are the first minimax instance-dependent regret bounds under uniform rewards. Note that, in the worst case, \(_{t}^{}=}(K}/(v_{0}+K))\), which indicates that these results provide a strict improvement over the worst-case bounds given in Theorems 1 and 2.

Some readers may expect instance-dependent regret bounds for non-uniform rewards as well. Unfortunately, we were unable to establish these. Recall that \(_{t}^{}\) represents the degree of non-linearity for the optimal assortment \(S_{t}^{}\). However, in the proofs for regret bounds, we encounter terms associated with the chosen assortment \(S_{t}\), such as \(_{i S_{t}^{}}p_{t}(i|S_{t},^{})p_{t}(0|S_{t},^{})\). To address this, we use the mean value theorem-based analysis (Lemma I.5) to replace this quantity with \(_{t}^{}\). Under non-uniform rewards, however, the mean value theorem does not apply because the sizes and rewards of \(S_{t}^{}\) and \(S_{t}\) may differ. Addressing this problem would be an interesting direction for future research.

## 7 Numerical Experiments

In this section, we empirically evaluate the performance of our algorithm OFU-MNL+. We measure cumulative regret over \(T=3000\) rounds. For each experimental setup, we run the algorithms across \(20\) independent instances and report the average performance. In each instance, the underlying parameter \(^{}\) is randomly sampled from a \(d\)-dimensional uniform distribution, where each element of \(^{}\) lies within the range \([-1/,1/]\) and is not known to the algorithms. Additionally, the context features \(x_{ti}\) are drawn from a \(d\)-dimensional multivariate Gaussian distribution, with each element of \(x_{ti}\) clipped to the range \([-1/,1/]\). This setup ensures compliance with Assumption 1. In the uniform reward setting (first row of Figure 1), the combinatorial optimization step to choose the assortment reduces to sorting items by their utility estimate. In the non-uniform reward setting (second row of Figure 1), the rewards are sampled from a uniform distribution in each round, i.e., \(r_{ti}(0,1)\). Refer Appendix K for more details.

We compare the performance of OFU-MNL+ with those of the practical and state-of-the-art algorithms: the Upper Confidence Bound-based algorithm, UCB-MNL, and the Thompson Sampling-based algorithm, TS-MNL. Figure 1 demonstrates that our algorithm significantly outperforms other baseline algorithms. In the uniform reward setting, as \(K\) increases, the cumulative regrets of all algorithms tend to decrease. In contrast, this trend is not observed in the non-uniform reward setting. Furthermore, the results also show that our algorithm maintains a constant computation cost per round, while the other algorithms exhibit a linear dependence on \(t\). In Appendix K, we present the additional runtime curves (Figure K.1) as well as the regret curves of the other configuration where \(v_{0}=(K)\) (Figure K.2). All of these empirical results align with our theoretical results.

## 8 Conclusion

In this paper, we propose minimax optimal lower and upper bounds for both uniform and non-uniform reward settings. We propose a computationally efficient algorithm, OFU-MNL+, that achieves a regret of \(}(d)\) under uniform rewards and \(}(d)\) under non-uniform rewards. We also prove matching lower bounds of \((d)\) and \((d)\) for each setting, respectively. Moreover, our empirical results support our theoretical findings, demonstrating that OFU-MNL+ is not only provably but also experimentally efficient.

Figure 1: Cumulative regret (left three, \(K=5,10,15\)) and runtime per round (rightmost one, \(K=15\)) under uniform rewards (first row) and non-uniform rewards (second row) with \(v_{0}=1\).