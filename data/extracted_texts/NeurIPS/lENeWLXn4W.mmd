# A New Linear Scaling Rule for Differentially Private Hyperparameter Optimization

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

A major direction in differentially private (DP) machine learning is DP fine-tuning: pretraining a model on a source of public data and transferring the extracted features to downstream tasks. This is an important setting because many industry deployments fine-tune publicly available feature extractors on proprietary data for downstream tasks. In this paper we propose a new linear scaling rule, a hyperparameter optimization algorithm that privately selects hyperparameters to optimize the privacy-utility tradeoff. A key insight into the design of our method is that our new linear scaling rule jointly increases the step size and number of steps as \(\) increases. Our work is the first to obtain state-of-the-art performance on a suite of 20 benchmark tasks across computer vision and natural language processing for a wide range of \([0.01,8.0]\) while accounting for the privacy cost of hyperparameter tuning.

## 1 Introduction

Industry deployments make use of pretrained models  by fine-tuning on task-specific datasets [35; 6; 69] and serving consumer applications that span the range of modalities from portraiture  to chatbots . A crucial component of interfacing machine learning models closely with user data is ensuring that the process remains _private_, and Differential Privacy (DP) is the gold standard for quantifying privacy risks and providing provable guarantees against attacks . DP implies that the output of an algorithm e.g., the final weights trained by stochastic gradient descent (SGD) do not change much if a single datapoint in the dataset changes.

**Definition 1.1** (Differential Privacy).: A randomized mechanism \(\) with domain \(\) and range \(\) preserves \((,)\)-differential privacy iff for any two neighboring datasets \(D,D^{}\) and for any subset \(S\) we have \([(D) S] e^{}[(D^{}) S ]+\)

where \(D\) and \(D^{}\) are neighboring datasets if they differ in a single entry, \(\) is the privacy budget and \(\) is the failure probability.

Figure 1: Our new linear scaling rule first does a small number of trials with a very small privacy budget, then does a small number of trials with a slightly larger privacy budget, and finally does linear interpolation through the optimal hyperparameters from these low-cost runs up to the final privacy costDifferentially Private Stochastic Gradient Descent (DP-SGD) [72; 1] is the standard privacy-preserving training algorithm for training neural networks on private data, with an update rule given by \(w^{(t+1)}=w^{(t)}-}{|B_{}|}(_{i B_{t}}}_{}((x_{i},w^{(t)}))+)\) where the changes to SGD are the per-sample gradient clipping \(_{}((x_{i},w^{(t)}))=,w^{(t)})}{[C,||((x_{i},w^{(t)})||_{2})]}\), and addition of noise sampled from a \(d\)-dimensional Gaussian distribution \((0,1)\) with standard deviation \(\). These steps alter the bias-variance tradeoff of SGD and degrade utility, creating a challenging privacy-utility tradeoff. Recent work has made significant progress in closing the gap in performance between private and non-private fine-tuning of transformer-scale models [46; 52; 7; 51], but a key problem presents a concrete obstacle to implementing DP algorithms to power real-world consumer-facing machine learning applications.

The privacy analysis of current approaches for private training does not account for the cost of hyperparameter tuning, and DP-SGD additionally increases the hyperparameter tuning burden compared to vanilla SGD. These hyperparameters include the learning rate schedule, the clipping bound, the batch size, and the amount of noise to add at each iteration. Because private training introduces additional hyperparameters, biases optimization by clipping the gradient, and imposes privacy-utility tradeoffs for existing hyperparameters, it is challenging to apply hyperparameter selection strategies from non-private training, even on the same dataset. Furthermore prior SOTA work in private training does not use similar hyperparameters as non-private training so hyperparameter search algorithms cannot be leveraged from the broader literature. More specifically, conventional non-private training uses SGD with momentum  or AdamW  to train for hundreds of epochs. However, training for additional iterations in DP-SGD requires adding additional noise , and taking large step sizes (such as with momentum) with low signal-to-noise ratio (SNR) can destabilize training . Prior work aims to minimize the amount of noise that is added during training by utilizing early stopping, training for as little as a single iteration . Prior work has either fixed these hyperparameters without explanation  or performed an extensive search to find the best values , but the hundreds of trials of hyperparameter tuning  go unaccounted for in the privacy analysis.

We propose a new linear scaling rule (Alg. 1, Fig. 1) that automatically selects hyperparameters to optimize the privacy-utility tradeoff of private fine-tuning. In particular, as our privacy budget increases from \(=0\), we increase the step size and number of steps. Our method accounts for the privacy cost of hyperparameter selection by allotting a small portion of the budget to find the best hyperparameters at \( 1\) and scaling these up to \(=1\). We summarize our contributions:

* We demonstrate that our new linear scaling rule reduces the computation and privacy cost of hyperparameter optimization by an order of magnitude without sacrificing performance
* Linear scaling can obtain new SOTAs for both full fine-tuning and linear probing of both convolutional and transformer architectures across \(20\) vision and language tasks
* We compare four model architectures for a set of five vision benchmarks and find that the private-non private utility gap decreases as models improve, with the best model across all five tasks obtaining lossless performance of \(99\%\) accuracy for \(=1\) on CIFAR10
* We find that linear scaling is robust to domain shifts between private training and test data
* We find that models trained with our method can provide good performance even when there is a large shift between public and private data
* We validate that models trained with our method can perform well for zero-shot classification
* We provide our code as a part of our empirical evaluation.

Figure 2: We compare the best private and best non-private test accuracy performances of our method to prior work using models pretrained on ImageNet-21k and fine-tuned on CIFAR10 and CIFAR100 datasets. Our results at \(=1\) include the cost of hyperparameter tuning via applying the linear scaling rule at \([0.01,0.1]\).

## 2 A New Linear Scaling Rule

In this section we detail how our method chooses each hyperparameter in DP-SGD, prove the privacy guarantee of the overall hyperparameter selection process, and provide a theoretical analysis.

A new linear scaling ruleThe well-known linear scaling rule  proposes increasing the learning rate with the batch size. We propose a new linear scaling rule that details how to select all hyperparameters in DP-SGD. Our method first fixes full-batch, unit clipping norm, zero initialization and use SGD with momentum, and then jointly scales the learning rate and number of steps with \(\). We provide extensive ablations of each design choice in our hyperparameter optimization algorithm in Appendix A.2. Prior work has exclusively taken small step sizes [51; 52; 7; 15; 9] on the order of \(\{10^{-5},10^{-3}\}\) and works that train transformers have also trained for a small number of epochs \(\{1,3\}\)[51; 7]. While this works well to recover the bulk of the non-private performance when \(\) is very small, it is natural to expect that as \(\) we should increase the parameters of training to more closely resemble that of non-private training. In line with this insight, we propose a linear scaling rule: jointly increase the step size and number of steps linearly with \(\). We make use of this simple yet powerful heuristic in the hyperparameter selection strategy that we use in all our experiments, outlined in Algorithm 1. Given a total privacy budget \(\), we use an initial portion of this budget to do binary search (random search and grid search are also valid) on the meta-hyperparameter \(r= T\) for a small value of \(\), and use this to estimate the best value of \(r\) for the desired overall privacy budget. We provide a privacy guarantee in 2. We note that linear scaling does not hold up forever: we are primarily interested with analyzing \( 1\), and show that in this range it holds (Fig. 3).

Linear Scaling is intuitive.Applying the linear scaling rule improves the cosine similarity between noisy weight updates and the optimal solution without degrading accuracy. First note that the classification accuracy of a linear model is scale-invariant; the optimal solution of Gradient Descent with total step size \(r\) is \(w^{}=w^{*}/\|w^{*}\| r\): the projection of \(w^{*}\) onto \(B_{r}\), the ball of radius \(r\), and for linear models, the performance (top-1 accuracy) of \(w^{}\) is the same as the performance of \(w^{*}\): \((w^{}(x))=(w^{*}(x))\, x D\). An important factor in the success of optimization is the angle between the gradient update \(_{i}\) and \(w^{}\): if all our updates point in the same direction, we can expect fast convergence. Let similarity(i) = \( w^{}}{\|_{i}\|\| \|w^{}\|}\). Suppose that \(\|w_{i}\|=\|w^{}\| 1\), then adding Gaussian noise \(\) where \((0,1)\) to the update will significantly decrease the cosine similarity of the updated model and \(w^{}\). If we decrease \(\), it is easy to see that this mitigates the impact on the trajectory. However, we can equivalently keep \(\) constant and increase the scale of the parameters, and also decrease the impact of noise on the trajectory: \((w_{i}+,w^{})<(  w_{i}+, w^{}),>1\). Note that by increasing \(r\) we scale the optimal solutionwhile keeping its performance identical, and thus optimize the cosine similarity of the noisy update. Increasing the number of iterations and the learning rate linearly increases \(r\) but does not linearly increase \(\) due to the composition of Gaussian differential privacy , therefore the impact on the optimization trajectory is minimized.

TheoryWe introduce two theoretical results. We first analyze the privacy cost including hyperparameter tuning of DP-RAFT under Gaussian DP (GDP). In Thm. 2.3 we analyze the performance gap between hyperparameters for noisy gradient descent in terms of an upper bound in expectation on the distance between private and non-private iterates, and find that applying the linear scaling rule improves the upper bound on this distance. Proofs of all results are in Appendix A.5.

**Proposition 2.1**.: _Algorithm 1 is \((/)\)-GDP. Moreover, repeating Algorithm 1 for \(n\) times for hyper parameter search would be \((/)\)-GDP._

**Corollary 2.2**.: _Algorithm 1 is \((,(-/+/2))-e^{} (-/-/2))\)-DP. Also, for \(n\)-fold repetition, the algorithm is \((,(-/+/2)) -e^{}(-/-/ 2))\)-DP_

**Theorem 2.3**.: _Let \(f\) be gradient descent that minimizes a \(\)-strongly convex and \(\)-smooth function \(\) with constant learning rate \((0,)\) over \(T\) iterations. Then we can bound the "noisy radius" distance between the noisy iterate \(w^{T}\) and the benign iterate \(w^{T}_{b}\) at iteration \(T\) in expectation: \([|w^{T}-w^{T}_{b}|](_{i}^{T-1}(|1- |,|1-|)^{i})\)._

Thm. 2.3 indicates that the distance between the noisy and non-noisy weights grows in a very controlled manner; at each iteration the divergence from the previous iteration is decreased by a factor strictly less than 1, and then we add some noise. The main idea of the proof is similar to the main result in Fang et al.  but is simpler because we only prove the result for linear models.

We apply this theorem to logistic regression (fine-tuning a linear model on extracted features). In this setting our theorem provides an upper bound on the radius of the range of solutions that DP-SGD produces. For linear models, this radius converts directly into an upper bound on the generalization error. If we use the linear scaling rule to scale \(r= T\) with \(\), we expect that \(\) remains appropriately bounded and \(T\) does not grow so large that the resulting noise creates significant model drift. Therefore, we find that increasing the quantity \(r= T\) improves this upper bound.

While our theorem only holds for linear models, we will show that it holds empirically for the deep GPT2 and RoBERTa models, in line with Li et al.  who find that even the updates of a large model lie in a low-dimensional space during fine-tuning.

## 3 Evaluation

We provide results on a range of image classification, distribution shift, and natural language processing tasks. Full results for all datasets and models can be found in Appendix A, including ablations on all steps of our method(A.2) and key hyperparameters(A.4).

Datasets.We evaluate the performance of our method on 20 benchmark tasks spanning the data modalities of CV and NLP. Image classification: ImageNet , CIFAR10, CIFAR100 , Fashion-MNIST , STL10 , EMNIST . Because these image classification datasets are generally considered in-distribution of the pretraining data, we also provide results on a number of distribution shift datasets from the WILDS suite  that have been used to evaluate various fine-tuning techniques. CIFAR10 \(\) STL, CIFAR101, CIFAR100 \(\) CIFAR100C , Waterbirds , FMoW , and Camelyon17 . These datasets are considered benchmark tasks for distribution shifts [42; 43; 53] and include data that is not in-distribution of the training data, making for a more realistic evaluation of the capabilities of our method to solve challenging tasks. We are the first to show that DP-SGD is capable of learning to handle distribution shifts without using any techniques from the distributionally robust optimization (DRO) literature . For NLP tasks we consider text classification tasks from the GLUE benchmark : SST-2, QNLI, QQP, MNLI(m/mm) and for next word generation we use PersonaChat , WikiText-2 , and Enron Emails .

### Linear Scaling finds near-optimal hyperparameters with low privacy cost

We first provide a concrete example of the hyperparameter search with \(_{0}\) on CIFAR10. Note that regardless of what strategy we use for hyperparameter search here, our total privacy cost as given by Proposition 2 must be strictly less than \(_{0}\). Binary search, random search, Bayesian optimization and grid search are all methods that we can use for the initial hyperparameter search. For this example, for the sake of simplicity we will use random search with 3 trials, with \(_{0}=0.01,_{1}=0.05,_ {f}=0.9,_{0}+_{1}+_{f}=1.0\). For \(_{0}=0.01\), we randomly sample r uniformly in the range =2,20,100 and then randomly decompose this into (approximate) \((,T)\) pairs of [0.2, 10], [0.5, 40], . These in turn evaluate to accuracies of [91.79, 73.68, 67.21], so the best value of r at \(_{0}=0.01\) is 2. We do a similar process at \(_{1}=0.05\) and get a best r-value of 5. We do linear interpolation and obtain the line of best fit as \(r=75+1.25\). Approximating this to \(r=75\), we apply the linear scaling rule \(r= T\) and randomly decomposing this value of \(r\) into an \((,T)\) pair of [0.75, 100], we produce a final accuracy of 99.00 at \(_{f}=0.9\).

Linear Scaling outperforms prior hyperparameter search techniques.We validate the effectiveness of linear scaling against the grid search baseline. In Fig. 3 (right) we compare Alg. 1 to grid search. To avoid scale mismatch on the x-axis we do not account for the privacy cost of grid search, that does \(n=100\) trials (on the same scale as prior work ). It is trivial that linear scaling outperforms a naive grid search, but we also compare the effectiveness of linear scaling against the hyperparameter selection strategies used in prior work . We apply linear scaling to the ViT model used in  on CIFAR100. Although  do not directly state the hyperparameters for their best results, they specify that they use 200 hyperparameter trials with Bayesian optimization. While they obtain RDP guarantees, these guarantees do not include the privacy cost of non-privately tuning hyperparameters. We apply the linear scaling rule to extrapolate a value of \(r\) from \(=0.1\) to \(=1\), obtaining \(r=20=(0.2) T(100)\). _We recover performance of \(82.7\%\) for \(=1\), a \(2\%\) improvement over the best result for DP-Adam in  while accounting for the privacy cost of hyperparameter tuning._ They obtain their best result for DP-Adam at \(T=10\), but we cannot compute the corresponding value of \(r\) because they do not provide \(\). However, because they use a clipping norm of \(0.005\) we can reasonably infer that their value of \(r\) is \( 1000\) smaller than ours. This is farther from the optimal non-private training, as evidenced by the performance gap.

Linear Scaling scales to ImageNetIn Table 4 we do a granular comparison between our method and . We observe that our method is competitive with  even when accounting for the privacy cost of hyperparameter search, and that the linear scaling rule holds up at the scale of ImageNet for very large values of \(r= T\). The non-private accuracy of their closed-source model is \(3.2\%\) higher than our open-source model, and so the private accuracy at \(=2\) is also \(3.2\%\) higher.

Figure 4: Linear Scaling on ImageNet is competitive with prior SOTA  (Jan. 2023) and current SOTA (within last month).

Figure 3: Training the beat architecture on CIFAR100, the linear scaling rule produces values for \(r= T\) close to that of grid search, and the performance drop is only apparent at \(>0.2\) because the cost of tuning is \(=0.1\), and vanishingly small for larger \(\).

However, ultimately our method and the method of Mehta et al.  are complementary, because their method introduces new hyperparameters that we intuit our linear scaling rule can optimize. We attempted to validate this intuition empirically but were unable to reproduce the results of Mehta et al.  because they and Mehta et al.  pretrain on the closed-source JFT dataset with billions of images. We note that all numbers we report for models pretrained on ImageNet-21k using first-order methods surpass those in , but for sufficiently small values of \(\) on harder datasets the second-order methods they propose provide better performance. We note that the method in Mehta et al.  only works for vision tasks, whereas our approach works for both vision and language tasks.

Linear Scaling produces robust results.In Fig. 3 we report that following Algorithm 1 produces new state-of-the-art results for all values of \(\), shown in Table 5. In Appendix A.1 we provide detailed computations of the linear interpolation for multiple datasets and in Appendix A.4 we provide full results across the entire hyperparameter search space. Our results validate that this rule is robust: we can move from one set of hyperparameters to another similarly performing set of hyperparameters by increasing the number of iterations \(T\) by a constant factor and decreasing the learning rate \(\) by the same factor (or vice versa). We find that any inaccuracy incurred by estimating the best value of \(r\) with the linear scaling rule will not reduce accuracy by much compared to doing grid search for the optimal value of \(r\), but does reduce the privacy cost of hyperparameter tuning immensely.

### Linear Scaling enables empirical analysis

Many interesting questions in DP fine-tuning remain unanswered because of the immense computational overhead of evaluating hundreds of hyperparameter trials for each privacy budget, model architecture and dataset . We now employ the linear scaling rule to efficiently answer key questions in DP fine-tuning for vision tasks.

Impact of model architectures on differential privacyMany pretrained model architectures are available  but prior work has generally engaged with a single architecture, e.g. beit  or ViT . We leverage our method to answer three questions:

* What model architectures can provide good DP classifiers?
* Is the best model task-specific, e.g., is an architecture search required?
* Does the private-non private utility gap depend on the model architecture?

We report our findings in Tab. 5. We evaluate multiple transformer architectures in ViT , beitv1  and beitv2 , as well as the purely convolutional architecture Convnext . We find that all architectures can serve as good backbones for high-accuracy DP classification. This is somewhat surprising because the different inductive biases of transformers and purely convolutional architectures tend to produce differently structured features, but we reason that the noise added by DP will'smooth out' these decision boundaries regardless of architecture. We note that one architecture, beitv2, performs the best on all benchmarks and also has the highest non-private ImageNet accuracy . We therefore recommend that practitioners do not worry about architecture search when fine-tuning as this can incur further privacy costs, and instead pick the best model available. We are encouraged to report that the private-non private utility gap diminishes with model accuracy, enabling us to report for the first time _lossless privacy_ of \(99.0\%\) on CIFAR10 at \(=1\). We expect that as pretrained models become even better, future works may even be able to attain lossless privacy on CIFAR100, that we note remains somewhat challenging for private fine-tuning. We harness these insights for our next analyses.

Figure 5: We compare the best private and best non-private performances of all models on all datasets. We use the linear scaling rule to scale hyperparameters from \(=0.1\) to \(=1\), so our privacy analysis includes the cost of hyperparameter tuning.

**Linear Scaling is robust to distribution shifts.** Benchmarking performance on datasets with distribution shifts is increasingly important because real-world problems almost always contain distribution shift between model training and inference . Prior work in distribution-only robust optimization (DRO) has addressed this problem by using knowledge of the relative imbalances between groups, but recent work with vision transformers has shown that linear probing can perform well on datasets with distribution shifts [53; 41; 43]. However there is no work that evaluates the robustness of private models to distribution shifts. We leverage our method to answer three questions:

* Can DP help when there is a domain shift from private fine-tuning to test?
* Can DP help when there is a domain shift from public data to private fine-tuning?
* Can DP fine-tuned models perform well in the zero-shot setting?

In Table 6 we compare the performance of our method across 8 benchmarks and find that the answer to all three of these questions is _yes_.

The Waterbirds dataset is a well-known benchmark for evaluating the robustness of models to spurious correlations. There is a domain shift between the private training data and the private test data created by class imbalance. We are surprised to find that in the absence of any other regularization methods, DP fine-tuning actually _improves_ performance on the OOD split. We hypothesize that the lackluster OOD non-private performance is caused by the model overfitting to the spurious correlation in the training data, and that the inherent regularization of DP prevents the model from memorizing this spurious correlation. By comparing our results to Mehta et al.  we determine that this robustness is unique to DP rather than an artifact of the pretrained model. Although DP does significantly degrade the ID performance, in situations where minimizing OOD error is more important, we believe that DP by itself can mitigate the domain shift from private fine-tuning to test.

Because our central assumption in DP fine-tuning is that there is no privacy leakage from the pretraining data to the private training data, it is important to understand how DP fine-tuning performs when there is a distribution shift between public data and private data. fMoW  and Camelyon17  are two datasets that represent a signficant distribution from the pretraining data (ImageNet). We observe a similar relationship between ID and OOD degradation as above, where the OOD degradation is somewhat mitigated by DP. If we compare our results on Camelyon to the best results in Ghabtikesabi et al.  we find that we can improve their best performance from \(91.1\%\) at \(=10\) to \(93.91\%\) at \(=1\). Although performance on fMoW remains quite poor, we note that it is not significantly worse than in the non-private setting. We believe that DP fine-tuning from pretrained models remains a viable strategy even when the publicly available pretraining data has a very large distribution shift from the private target data.

We finally consider the zero-shot setting, where we fine-tune a model on CIFAR and then transfer it without updating any parameters to private test datasets that once again represent a distribution shift from CIFAR. We report the performance in the OOD column. For the more minute distribution shifts of STL and CIFAR10p1 we find that the fine-tuned classifier can achieve remarkable performance without ever updating parameters on these datasets; that is, we just remap the labels as per . CIFAR10C and CIFAR100C represent larger distribution shifts and are used to benchmark the robustness of models to commonly reported image corruptions . Our OOD performance on these larger distribution shifts is much worse, particularly for CIFAR100 where there is a \(>20\%\) degradation. Although this is lower than the top result on the RobustBench leaderboard  obtains \(85\%\) accuracy, we note that once again _we used no additional methods beyond DP to ensure robustness but managed to achieve reasonable performance to distribution shifts in zero-shot classification_.

Figure 6: In-distribution (ID) and out-of-distribution (OOD) performance on benchmark distribution shift datasets. Prior work is non-private (citations are in Appendix A.1). We use the linear scaling rule to scale hyperparameters from \(=0.1\) to \(=1\), so our privacy analysis includes the cost of hyperparameter tuning.

### Linear Scaling for language modeling

Prior work has generally focused on either CV or NLP because the methods used in DP fine-tuning differ greatly across data modalities [46; 51]; here we show that our method extends to NLP by validating on text classification and language modeling tasks. We also update all parameters when fine-tuning, displaying that our method works for both linear probing and full fine-tuning. We fine-tune GPT-2  with our method for three language modeling tasks that have been benchmarked in prior works [46; 70; 30] on private fine-tuning: Persona-Chat , WikiText-2  and Enron Emails . We also fine-tune RoBERTa-base on four tasks in the GLUE benchmark: SST-2, QNLI, QQP and MNLI(m/mm) in Table 7. While prior works mainly focus on \(\) in \(\{3,8\}\), in this work we are also interested in smaller \(\)s like \(0.1\). Appendix B.1 includes the details for the experimental set-up.

Linear scaling holds for NLP tasksWe analyze the performance gap between estimated total step size and optimal total step size by grid search to understand how well linear scaling performs on language modeling tasks. Fig. 8 plots the optimal perplexity and perplexity by estimated total step size at different values of \(\) on Enron emails. We can see that the linear scaling rule generalizes well for reported values of \(\) and the perplexity by the estimated total step size is close to the optimal perplexity. From Table 7 we can see that linear scaling also holds across a range of tasks in the GLUE benchmark. We also have the result for WikiText-2 in Appendix B.3.

The linear scaling rule outperforms prior results on differentially private language modeling tasks.We first run a qualitative evaluation on the previous benchmark SOTA  on PersonaChat trained with DP-SGD by following the linear scaling rule to increase the number of epochs. We can see in Table 9 that we can push the perplexity under \(18\) for \(=3\) and \(=8\); this performance is competitive with the non-private baseline. Furthermore, even when pushing for a stricter privacy guarantee \(=0.5\), we can still get perplexity of \(21.25\), that is better than the result of \(=8\) in . We also report the results of ablating these hyper-parameters and varying the number of layers trained in Appendix B.2. We quantitatively validate the linear scaling rule on WikiText-2 and Enron email dataset and report the result in Table 10 respectively. We select training parameters and the total step size with Alg. 1.

Figure 8: The linear scaling rule (accounting for the privacy cost of hyperparameter tuning) is competitive with grid search (non-private, doing N trials each with the given \(\)) on the Enron Emails dataset. Left: y-axis is Perplexity (lower is better).

Figure 7: Linear scaling holds for GLUE tasks when training the full RoBERTa-base model

Figure 9: Linear scaling holds when fine-tuning all layers of GPT2 on PersonaChat and outperforms Li et al. For WikiText-2, a key observation is that when we compare our results to the best prior reported results in , for the same number of passes over the training data (20), we obtain lower perplexity for \(=0.2\) than they report for \(=3\). That is, by just increasing the effective step size from \( 8 10^{-6}\) to \( 8 10^{-3}\) we can strengthen the privacy guarantee without degrading performance.

## 4 Related Work and Discussion

De et al.  and Cattan et al.  propose the use of large batch sizes and initializing the weights to small values near-zero to standardize training. However, they use ResNet architectures rather than modern vision transformers, and in Appendix A.2 we find that other techniques that they use such as data augmentation, fine-tuning the embedding layer, and weight averaging do not always improve performance.  do end-to-end training of the same beat architecture we use, but we crucially observe that updating all parameters incurs the curse of dimensionality and therefore it is better to only update the last layer. Besides vision tasks, Li et al.  and Yu et al.  provide methods for fine-tuning large language models under DP-SGD by proposing new clipping methods to mitigate the memory burden of per-sample gradient clipping. However, they do not achieve performance comparable to non-private models when fine-tuning a pretrained model on the PersonaChat dataset. We adapt their techniques to the hyperparameter settings that we show are optimal for DP fine-tuning, and produce similar performance to non-private fine-tuning on the PersonaChat dataset. Yu et al.  report compelling results by only updating a sparse subset of the LLMs with LoRA . We fine-tune GPT2 and RoBeRTA; Basu et al.  also fine-tune BERT models.

Papernot and Steinke  propose an RDP hyperparameter optimization algorithm that requires selecting the number of trials at random with a random variable, and exhibits the greatest savings when the number of hyperparameter trials is large. By contrast our linear scaling rule needs only a small fraction of the overall privacy budget for hyperparameter search. Their evaluation only tunes the learning rate of a 3-layer CNN on MNIST. Our rule accounts for multiple hyperparameters (batch size, clipping norm, momentum, learning rate, number of iterations) and produces SOTA results.

Golatkar et al. ; Nasr et al. ; Amid et al.  treat \(<10\%\) of the private training dataset and public and use it to improve DP-SGD. Although we do not use any private data during pretraining, future work can tackle applying linear scaling to this alternate threat model.

An open challenge in DP training is how to privately and efficiently do hyperparameter tuning. We complement the existing body of work by introducing a new linear scaling rule to privately optimize hyperparameters. Our key insight is that we can interpolate between the early-stopping regime that is best for small \(\) and the regime of many iterations that is best for \(\) as \(\) increases. We provide find that our method attains new state-of-the-art accuracy across 20 tasks, on benchmark image classification tasks, distribution shift datasets, and natural language modeling tasks.

## 5 Limitations

**Assumptions.** The key assumption in DP fine-tuning is that there is no privacy leakage between public data and private data. We take steps towards qualifying this assumption by evaluating on datasets with distribution shifts between public and private data. **Scope of Claims.** We evaluate 20 datasets across multiple data modalities with multiple model architectures for two types of fine-tuning methods, linear probing and end-to-end training of deep (\(>100M\) param) transformers. **Key Factors that Influence the Performance of Our Approach.** The key parameter in the linear scaling rule is how to allocate privacy budget to the initial hyperparameter search. We find that with privacy budgets as small as \(=0.01\) we can still effectively forecast the linear trend to determine the best hyperparameters for the main privacy budget we consider \(=1\). However, if we need to consider even smaller privacy budgets, it may be challenging to accurately extrapolate hyperparameters.

Figure 10: Finentuning GPT-2 on WikiText-2 (\(=10^{-6}\)) and Enron (\(=}|}\)) with DP-SGD. Ppl is perplexity and TSS is Total Step Size. (\({}^{*}\) means estimated). Previously reported best perplexity of GPT-2 on WikiText-2 at \(=3\) is 28.84 in .