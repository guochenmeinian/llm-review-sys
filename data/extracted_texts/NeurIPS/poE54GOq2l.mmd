# SnapKV: LLM Knows What You Are Looking for

before Generation

 Yuhong Li\({}^{1}\) Yingbing Huang\({}^{1}\) Bowen Yang\({}^{2}\) Bharat Venkitesh\({}^{2}\) Acyr Locatelli\({}^{2}\)

Hanchen Ye\({}^{1}\) Tianle Cai\({}^{3}\) Patrick Lewis\({}^{2}\) Deming Chen\({}^{1}\)

\({}^{1}\) University of Illinois Urbana-Champaign \({}^{2}\) Cohere \({}^{3}\) Princeton University

\({}^{1}\){lleeyh, yh21, hanchen8, dchen}@illinois.edu

\({}^{2}\){bowen, bharat, acyr, patrick}@cohere.com \({}^{3}\)tianle.cai@princeton.edu

equal contribution

###### Abstract

Large Language Models (LLMs) have made remarkable progress in processing extensive contexts, with the Key-Value (KV) cache playing a vital role in enhancing their performance. However, the growth of the KV cache in response to increasing input length poses challenges to memory and time efficiency. To address this problem, this paper introduces SnapKV, an innovative and fine-tuning-free approach that efficiently minimizes KV cache size while still delivering comparable accuracy in real-world applications.

We discover that each attention head in the model consistently focuses on specific prompt attention features during generation. Meanwhile, this robust pattern can be obtained from an 'observation' window located at the end of the prompts. Drawing on this insight, SnapKV automatically compresses KV caches by selecting clustered important KV positions for each attention head. Our approach significantly reduces the growing computational overhead and memory footprint when processing long input sequences. Specifically, SnapKV achieves a consistent decoding speed with a 3.6x increase in generation speed and an 8.2x enhancement in memory efficiency compared to the baseline when processing inputs of 16K tokens. At the same time, it maintains comparable performance to the baseline models across 16 long sequence datasets. Moreover, SnapKV can process up to 380K context tokens on a single A100-80GB GPU using HuggingFace implementation with minor changes, exhibiting only a negligible accuracy drop in the Needle-in-a-Haystack test. Further comprehensive studies suggest SnapKV's potential for practical applications. Our code is available at https://github.com/FasterDecoding/SnapKV.

## 1 Introduction

Many leading LLMs have started to handle longer contexts, overcoming the difficulties in context maintenance and attention mechanism scalability, such as GPT-4  and Command-R  with context length 128K, Claude-3  with 200K, and Gemini-Pro-1.5 with 1M . Despite their impressive capabilities, LLMs still face significant challenges when dealing with long context prompts. Specifically, the KV cache in attention calculation becomes less efficient when processing long context. During inference time, as prompt length increases, the decoding latency per step grows linearly due to the attention calculation across past KVs. Moreover, the large KV cache requires significant memory capacity, increasing hardware demands and limiting model scalability.

There are many approaches to mitigate these problems, such as KV cache eviction during generation stage . However, most of these methods lack a detailed evaluation in long-context settings. Moreover, they mainly focus on compressing the KV cache appended during decoding steps, while overlooking the realistic problem of compressing KV cache for prompts, which is typically the bottleneck in memory efficiency. In practical applications like chatbots and agents, where prompts range from multi-turn conversations to extensive articles or codebases , prompts are often much larger than generated responses such as summaries and code pieces, thus creating significant inference latency and memory utilization overhead. Additional challenge lies in compressing KV cache for such vast prompts without losing crucial information for accurate generation, especially in scenarios with various noisy contexts.

In our paper, we find an vital attention allocation phenomenon: only a subset of prompt tokens convey essential information for response generation, and these tokens remain unchanged during generation. To validate the robustness, we design extensive experiments across diverse prompts in terms of length, format, and content. From our observations, we derive an innovative and intuitive method, SnapKV, which can smartly identify the attention allocation pattern and compress the KV cache for long sequence prompts without compromising the model's accuracy. With its comprehensive design, SnapKV demonstrates its effectiveness on various datasets and can be easily integrated into popular deep-learning frameworks with just a few code adjustments. Our contributions are as follows:

* We design experiments to explore the attention allocation pattern during generation, focusing on two key questions: 1. Is there a consistent attention allocation pattern for input sequence tokens? 2. Is it feasible to identify this pattern prior to the generation stage? Our finding suggests that for LLMs, the attention allocation of most input sequence tokens stay consistent during generation. Thus, _LLMs knows what you are looking for before generation_.
* Inspired by our observations above, we develop an efficient and fine-tuning-free algorithm, SnapKV, which efficiently identifies critical attention features and compresses KV cache correspondingly with minimal model modification (See Fig. 1).
* We evaluate SnapKV across diverse LLMs and long-sequence datasets. SnapKV shows comparable accuracy with full KV caching method while achieving improved decoding speed and memory efficiency. Meanwhile, we conduct the pressure test with Needle-in-a-Haystack to further demonstrate its memory efficiency and information retrieval ability.

## 2 Related Works

Many previous works compress the KV cache by selectively dropping KVs using different algorithms. In StreamLLM , only the most recent tokens and attention sinks (first few tokens) are retained

Figure 1: The graph shows the simplified workflow of SnapKV, where the orange area represents the cluster of features per head selected by SnapKV. These features are then used to form new Key-Value pairs concatenated with the features in the observation window. Together, the selected prefix and observation windows constitute the new KV cache utilized for the generation.

to reduce the KV cache size, making it lose the important information carried by the discarded middle tokens 2. Heavy-Hitter Oracle (H2O)  introduces a policy that greedily drops KVs during generation based on a scoring function derived from cumulative attention. While this approach effectively compresses the KVs appended to the cache during generation, it overlooks compression of prompt KVs, which is crucial for reducing memory and computational overhead. Building on a similar concept, Adaptive KV Compression (FastGen)  implements a dual-phase algorithm that encompasses four KV cache compression policies. Initially, it identifies optimal policies through profiling results obtained from prompt encoding. Subsequently, it dynamically evicts caches during the generation phase based on these policies. Nonetheless, it faces the similar problem with H2O. ScissorHands  focuses on identifying and retaining pivotal tokens that exhibit a consistent attention weight pattern with previous token windows during generation steps. However, this method concentrates solely on the window of previous pivotal tokens in generation and neglects the extensive prompt that contains essential information for generating accurate responses. This oversight could lead to an inability to extract detailed information from prompts.

In summary, existing methods have not effectively addressed the challenges encountered in real-world applications, where prompts are exceptionally long yet require accurate information retrieval. Although these techniques may reduce the KV cache size during generation, they do not address the primary challenges of understanding complex prompt contexts, leaving critical issues unresolved.

## 3 Observations

In this section, we present our observations regarding the attention allocation patterns in the Query-Key matrix during token generation. Our analysis utilizes samples from Ultrachat , a multi-turns, high-quality instruction dataset consisting of 1.4 million dialogues. We further filter the sequences with response length greater than 512 and prompt length greater than 3k. Our findings are concluded into two key observations as follows:

* **Pattern can be identified before generation.** In this experiment, we split the attention features of input sequence of each layer into multiple windows, each with 128 tokens, and calculate the averaged attention weights of the last 20 windows separately. To understand the attention allocation patterns along input sequences, we calculate the overlap rates between _important_ attention features of input sequence (those with high average attention weights) identified by each window and the actual ones used by generation. The experimental results are shown in Fig. 3.

We observe that the last window of input sequence recognizes highly similar attention allocation pattern with the actual generation.
* **Pattern is consistent during generation.** We study if the positions of features identified as crucial in the last window of input sequence maintain their significance in the subsequent token generation. In the experiment, we split the generated tokens into 4 windows for every layer, each spanning 128 tokens, to compute the averaged overlap rates of these windows versus the last window of input sequence. As shown in Fig. 3, active attention features of input sequence obtained from the last window exhibit remarkable consistency throughout the generation process, as evidenced by high overlap rates.

## 4 SnapKV

In the attention mechanism, the growth in prompts will significantly increase time complexity for generation due to the Query-Key matrix multiplication. SnapKV addresses this issue by maintaining a constant amount of prompt KVs during generation, significantly reducing serving times for long-context LLMs. To structure our method coherently, we propose the following terminologies:

* **Prompt Length (\(L_{}\)):** The total length of the user-provided input.
* **Observation Window (\(L_{}\)):** The last segment of the prompt. This window is crucial for analyzing the influence of different contexts on attention allocation patterns.
* **Prefix Length (\(L_{}\)):** The length of the input preceding the observation window. It is part of the prompt and does not include the observation window. Overall, we have: \[L_{}=L_{}+L_{}\] (1)
* **Voting:** The process of calculating attention weights for each query within the observation window across all heads, aggregating these weights to highlight the prefix positions that are considered most significant. For a single batch of sequence, formally: \[=_{i=0}^{L_{}}_{}[:,i,:]\] (2) \[I=_{k}(,k)\] (3) where \(_{k}(,k)\) selects the indices \(I\) of the top \(k\) values in tensor \(\) per head. \(k\) is defined as \((1-p) L_{}\), where \(p\) stands for the compression rate. The tensor \(_{}^{N L_{} L_{}}\) represents the subset of the prompt softmax-normalized attention features over \(N\) heads.
* **Hit Rate:** We define attention features above a predefined threshold \(\) during generation as _important_ features. The hit rate, \(H\), is the number of important features successfully selected by the previous voting process over the total number of important features. \(H\) quantifies the effectiveness of the voting mechanism and is calculated as follows: \[_{} =(_{})\] (4) \[_{}[I] =1\] (5) \[_{} =(_{}>)\] (6) \[ =_{}_{}\] (7) \[H =}{_{}}\] (8) \(_{}^{N L_{}}\) represents the attention features between the current generated query and prefix keys. \(\) selects attention features by indices. The threshold operation filters \(_{}\) to retain only features with values over \(\), indicating important attention activations. The \(\) measures the overlap between attention features selected by \(_{}\) and \(_{}\), quantifying the alignment of the current attention with previously identified important features. The hit rate \(H\) is then computed as the ratio of the sum of overlap \(\) to the sum of important features \(_{}\), providing a metric for the efficacy of the attention mechanism in recognizing and emphasizing important attention features within the context. We use \((_{},_{})\) to denote combination of Eq. 7 and Eq. 8.

[MISSING_PAGE_FAIL:5]

as previously defined. By varying the instructions, we observe that different instructions prioritize different prefix attention features, as indicated by the descending trend in hit rates shown in Fig. 4. Our findings reveal an interesting aspect of KV cache in LLMs: the important attention features change with different instructions. This variability challenges the effectiveness of static compression methods that depend on constant weighted importance or fixed policies [7; 6; 8]. Thus, the complex relationship between context and related KV cache emphasizes the need for context-aware compression strategies and highlights the capability of SnapKV that recognizes this dynamic. In contrast, context-independent compression fail in capturing the dynamic, resulting in a misalignment between the attention distribution during profiling and inference, diminishing the generation quality of LLMs.

#### 4.2.2 Invariance to Instruction Positions

Our analysis also extends to the significance of instruction positioning on the interpretability of LLMs and their selection of important features. We calculate the average hit rate for the responses using the same observation window size as in the previous experiment. Our results shown in Fig. 5 indicate that across all three datasets, the hit rates are consistently high regardless of whether instructions are positioned before or after extensive supplementary contexts. This consistency suggests that SnapKV is able to identify attention allocation patterns regardless of the question's positions.

### Efficient Clustering via Pooling

In LLMs, information retrieval and generation rely on features with high attention weight and are supplemented by copying the rest of features in context using induction heads . Hence, naively selecting the top features results in retaining only portions of details and then losing the completeness of the information. For example, such compression might cause the LLMs to retrieve only the country code of a phone number and hallucinate the rest. Our experiment also revealed that only selecting the features with the highest weights is insufficient (Sec. 5.2). Such sparse selection risks compromising

Figure 4: The layer-wise overlap of important positions utilized by different question-answer pairs in the same dataset.

Figure 5: The layer-wise average hit rate of important positions used by prompts with questions at the beginning and the end.

the contextual integrity encapsulated in between features, thereby reducing accuracy. Based on the insights, we propose a fine-grained clustering algorithm utilizing a pooling layer shown in Line 13.

## 5 Experiments

In our experimental setup, we explore the performance of SnapKV across models that can handle extended prompt sequence contexts. First, we deliver a pressure test and benchmark the speed of LWM-Text-Chat-1M, which is state-of-the-art regarding its context length. We then conduct an ablation study on Mistral-7B-Instruct-v0.2 to understand the influence of pooling on the model's information retrieval performance. We assess model performances using the LongBench  dataset. Further experiments on compatibility with other acceleration strategies, such as parallel decoding , are elaborated in Appendix A. To assess the overhead of SnapKV during the prefilling stage, we present time and memory analysis results in Appendix B.

### Benchmarks on LWM-Text-Chat-1M

LWM-Text-Chat-1M is a 7B instruction-fine-tuned model with up to one million context length. In this section, we conduct a pressure test on this model and examine its algorithmic efficiencies.

#### 5.1.1 Needle-in-a-Haystack

The Needle-in-a-Haystack test  challenges the model to accurately retrieve information from a specific sentence ("needle") concealed within an extensive document (the "haystack"), with the sentence placed at a random location. Typically, sentences that are inserted in the middle of prompts are harder to retrieve. To rigorously evaluate SnapKV's capabilities, we extended the document length to 380k tokens which is the longest content that can be processed by a single A100-80GB GPU. We configured the prompt KV cache size to 1024, enabling SnapKV to select the most crucial 1024 attention features from the prompt for answer generation, with a maximum pooling kernel size of 5 and an observation window size of 16, both of which are hyperparameters that can be customized. The compelling outcomes in Fig. 6 from the Needle-in-a-Haystack test underscore SnapKV's potential to precisely manage small details on extremely long input contexts with a 380x compression ratio.

Figure 6: Needle-in-a-Haystack test performance comparison on single A100-80GB GPU, native HuggingFace implementation with only a few lines of code changed. The x-axis denotes the length of the document (the “haystack”) from 1K to 380K tokens; the y-axis indicates the position that the “needle” (a short sentence) is located within the document. For example, 50% indicates that the needle is placed in the middle of the document. Here LWMChat with SnapKV is able to retrieve the needle correctly before 140k and with only a little accuracy drop after. Meanwhile, the original implementation encounters OOM error with 33k input tokens (white dashed line).

#### 5.1.2 Decoding Speed and Memory Bound

We further benchmark the speed of LWM-Text-Chat-1M under different batch-size settings using SnapKV. We set the maximum KV cache size as 2048 for SnapKV, and fix the generation length at 512 to ensure a fair comparison. There are two main takeaways from our experiment on decoding speed and prompt sequence length on various batch sizes, as shown in Fig. 7. First, as the input sequence length increases, the decoding latency of the baseline implementation escalates linearly. Conversely, the SnapKV-optimized model maintains a constant decoding speed since the compressed KV cache size of prompt stays the same regardless of input sequence length and there is no extra update during the inference. For instance, at a sequence length of 16k and a batch size of 2, the decoding time for the baseline model surpasses 100 ms, whereas for SnapKV-optimized model, the decoding time consistently remains below 40 ms, achieving approximately a 3.6x speedup. Second, with the same batch size, the model integrated with SnapKV can decode significantly longer sequences. For example, at a batch size of 2, the baseline model encounters an OOM error beyond 16k input tokens, whereas the SnapKV-enhanced model extends this limit to 131k input tokens, indicating an approximately 8.2x improvement. This demonstrates SnapKV's effectiveness in minimizing memory consumption.

### Ablation Study of Effectiveness of Pooling

We perform an ablation study on Mistral-7B-Instruct-v0.2 to assess the impact of our pooling technique, a straightforward but efficient method for consolidating information through clustering. Our evaluation utilizes the modified LongEval-Lines benchmark , incorporating randomly generated pairs and averaged scores. LongEval-Lines presents a greater challenge compared to Needle-in-a-Haystack because it involves identifying key-value pairs in noisy contexts of the same format, while in Needle-in-a-Haystack, the relevant information is more distinctly separated from other contexts. We apply max pooling with a kernel size of 5 and use the observation window with a size of 16, which are hyperparameters and could be customized according to different models. As illustrated in our results (Fig. 8), we find that pooling significantly enhances retrieval accuracy compared to methods not utilizing pooling. We hypothesize that this is because the initial portions of critical token clusters are weighted higher by attention mechanisms. Typically, large language models tend to copy the tokens surrounding the initial portions to keep the contextual integrity. However, naively compressed KV cache breaks this mechanism and could lead to partially correct

Figure 7: Decoding latency comparison of baseline implementation and SnapKV optimized solutions on various batch sizes. The x-axis denotes the input sequence length; the y-axis indicates decoding latency (ms/token). All experiments are conducted on an A100 80GB GPU. The red dotted line denotes the common context length of state-of-the-art long sequence models.

results (Fig. 8). Note that throughout our experiments, the choice between max pooling and average pooling did not yield significant differences in performance.

### Experiments on LongBench

We evaluate SnapKV on these four models using LongBench , a multi-task benchmark designed to rigorously evaluate long context understanding capabilities across various datasets, spanning single and multi-document QA, summarization, few-shot learning, synthetic tasks, and code completion. The average prompt length of LongBench ranges from 5k to 7k, and more details can be found in Appendix D. We choose LWM-Text-Chat-1M with 1 million context length, LongChat-7b-v1.5-32k, Mistral-7B-Instruct-v0.2, Mistral-8x7B-Instruct-v0.1 with 32k context length as our baselines. For each model, we test SnapKV with various settings: compressing KV caches in the prompt to 1024, 2048, and 4096 tokens. We use max pooling with kernel size 7 and observation window size 32. Table 1 illustrates a negligible performance drop from models with SnapKV compared with original implementations for 16 different datasets, even with prompt-KV with 1024 tokens. Some models even outperform the baseline. Our results substantiate that SnapKV can grasp the key information in the long context and give comprehensive summaries with details. Moreover, our

   & &  &  &  &  &  &  \\   & LLMs\({}^{*}\) &  &  &  &  &  &  &  &  &  &  &  \\   & **A1 KV** & **18.18** & **25.65** & 40.94 & 24.57 & 19.39 & 10.09 & **27.97** & 24.93 & **24.81** & 71.0 & 69.9 & 39.73 & 3.17 & 3.5 & 4.4 & 4.32 \\  & SnapKV1024 & 18.02 & 23.73 & 40.25 & 24.61 & **19.84** & 10.77 & 17.99 & 24.44 & 25.33 & 70.0 & **6.42** & 39.64 & 1.67 & 3.0 & 43.4 & 4.0 \\  & SnapKV2004 & 17.92 & 25.03 & **41.38** & 22.49 & 19.38 & **11.34** & 21.6 & 24.22 & 24.36 & 70.0 & 61.11 & 39.91 & 2.17 & 4.0 & 44.6 & **44.92** \\  & SnapKV2004 & 17.92 & 25.47 & 40.76 & **24.92** & 19.53 & 11.27 & 25.34 & **25.42** & 25.48 & 70.5 & 61.08 & 39.62 & **3.17** & **4.0 & 44.4 & 44.0 \\  & H2O- 0096 & 11.37 & 24.82 & 20.01 & 18.66 & 9.74 & 7.2 & 25.77 & 23.26 & 22.33 & **71.0** & 61.06 & **40.33** & 0.0 & 0.0 & 41.52 & 40.97 \\   & All KV & **20.88** & **29.36** & **43.2** & 33.05 & 24.58 & **14.66** & **30.89** & 22.76 & **26.61** & **66.5** & **83.99** & **40.33** & 0.0 & 30.5 & 54.89 & **59.05** \\  & SnapKV1024 & 19.32 & 26.66 & 37.93 & 34.55 & 23.34 & 12.71 & 23.4

[MISSING_PAGE_FAIL:10]