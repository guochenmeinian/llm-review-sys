# Space-Time Continuous PDE Forecasting using Equivariant Neural Fields

David M. Knigge\({}^{*,1}\), David R. Wessels\({}^{*,1}\), Riccardo Valperga\({}^{1}\), Samuele Papa\({}^{1}\), Jan-Jakob Sonke\({}^{2}\),

**Efstatios Gavves \({}^{,1}\), Erik J. Bekkers \({}^{,1}\)**

\({}^{1}\)University of Amsterdam \({}^{2}\) Netherlands Cancer Institute

d.m.knigge@uva.nl, d.r.wessels@uva.nl

###### Abstract

Recently, Conditional Neural Fields (NeFs) have emerged as a powerful modelling paradigm for PDEs, by learning solutions as flows in the latent space of the Conditional NeF. Although benefiting from favourable properties of NeFs such as grid-agnosticity and space-time-continuous dynamics modelling, this approach limits the ability to impose known constraints of the PDE on the solutions - e.g. symmetries or boundary conditions - in favour of modelling flexibility. Instead, we propose a space-time continuous NeF-based solving framework that - by preserving geometric information in the latent space - respects known symmetries of the PDE. We show that modelling solutions as flows of pointclouds over the group of interest \(G\) improves generalization and data-efficiency. We validated that our framework readily generalizes to unseen spatial and temporal locations, as well as geometric transformations of the initial conditions - where other NeF-based PDE forecasting methods fail - and improve over baselines in a number of challenging geometries.

## 1 Introduction

Partial Differential Equations (PDEs) are a foundational tool in modelling and understanding spatio-temporal dynamics across diverse scientific domains. Classically, PDEs are solved using numerical methods such as finite elements, finite volumes, or spectral methods. In recent years, Deep Learning (DL) methods have emerged as promising alternatives due to abundance of observed and simulated data as well as the accessibility to computational resources, with applications ranging from fluid simulations and weather modelling [51; 7] to biology .

Figure 1: We propose to solve an equivariant PDE in function space by solving an equivariant ODE in latent space. Through our proposed framework, which leverages _Equivariant Neural Fields_\(f_{}\), a field \(_{t}\) is represented by a set of latents \(z^{}_{t}=\{(p^{}_{i},^{}_{i})\}_{i=1}^{N}\) consisting of a _pose_\(p_{i}\) and context vector \(_{i}\). Using meta-learning, the initial latent \(z^{}_{0}\) is fit in only 3 SGD steps, after which an equivariant neural ODE \(F_{}\) models the solution as a latent flow.

The systems modelled by PDEs often have underlying symmetries. For example, heat diffusion or fluid dynamics can be modeled with differential operators which are rotation equivariant, e.g., given a solution to the system of PDEs, its rotation is also a valid solution 1. In such scenarios it is sensible, and even desirable, to design neural networks that incorporate and preserve such symmetries to improve generalization and data-efficiency [12; 48; 4].

Crucially, DL-based approaches often rely on data sampled on a regular grid, without the inherent ability to generalize outside of it, which is restrictive in many scenarios . To this end,  propose to use Neural Fields (NeFs) for modelling and forecasting PDE dynamics. This is done by fitting a neural ODE  to the conditioning variables of a conditional Neural Field trained to reconstruct states of the PDE . However, this approach fails to leverage aforementioned known symmetries of the system. Furthermore, using neural fields as representations has proved difficult due to the non-linear nature of neural networks [13; 3; 35], limiting performance in more challenging settings. We posit that NeF-based modelling of PDE dynamics benefits from representations that account for the symmetries of the system as this allows for introducing inductive biases into the model that ought to be reflected in solutions. Furthermore, we show that through meta-learning [28; 45] the NeF backbone improves performance for complex PDEs by further structuring the NeF's latent space, simplifying the task of the neural ODE.

We introduce a framework for _space-time continuous equivariant PDE solving_, by adapting a class of \(()\)-Equivariant Neural Fields (ENFs) to PDE-specific symmetries. We leverage the ENF as representation for modelling spatiotemporal dynamics. We solve PDEs by learning a flow in the latent space of the ENF - starting at a point \(z_{0}\) corresponding to the initial state of the PDE - with an equivariant graph-based neural ODE  we develop from previous work . We extend the ENF to equivariances beyond \(()\), by extending its weight-sharing scheme to equivalance classes for specific symmetries relevant to our setting. Furthermore, we show how meta-learning [14; 28; 45; 13], can not only significantly reduce inference time of the proposed framework, but also substantially simplify the structure of the latent space of the ENF, thereby simplifying the learning process of the latent dynamics for the neural ODE model. We present the following contributions:

* We introduce a framework for spatio-temporally continuous PDE solving that respects known symmetries of the PDE through equivariance constraints.
* in terms of MSE
- in spatio-temporally continuous settings, i.e. evaluated _off_ the training grid and beyond the training horizon.
* We show how meta-learning improves the structure of the latent space of the ENF, simplifying the learning process, leading to better performance in solving PDEs.

We structure the paper as follows: in Sec. 2 we provide an overview of the mathematical preliminaries and describe the problem setting. Our proposed framework is introduced in Sec. 3. We validate our framework on different PDEs defined over a variety of geometries in Sec. 4, with differing equivariance constraints, showing competitive performance over other neural PDE solvers.We provide an in-depth positioning of our approach in relation to other work in Appx. A.

## 2 Mathematical background and problem setting

Continuous spatiotemporal dynamics forecasting.The setting considered is data-driven learning of the dynamics of a system described by continuous observables. In particular, we consider flows of fields, denoted with \(:^{d}[0,T]^{c}\). We use \(_{}\) as a shorthand for \((,t)\). We assume the flow is governed by a PDE, and consider the Initial Value Problem (IVP) of predicting \(_{t}\) from a given \(_{0}\). The dataset consists of field snapshots \(:[T]^{c}\), in which \([T]:=1,2,,T\) denotes the set of time points on which the flow is sampled and \(^{d}\) is a set of coordinate values. For each time point we are given a set of input-output pairs \([,()]\) where \(()^{c}\) are the values of the field at those coordinates. Importantly, the location at which the field is sampled need not be regular, i.e., we do not require the training data to be on a grid or to be regularly spaced in time, nor need coordinate values be identical for train and test sets. Following , we distinguish between \(t_{}\) - referring to values within the training time horizon \([0,T]\) - and \(t_{}\) - analogously to values beyond \(T\)Neural Fields in dynamics modelling.Conditional Neural fields (NeFs) are a class of coordinate-based neural networks, often trained to reconstruct directly-sampled input continuously. More specifically, a conditional neural field \(f_{}:^{n}^{d}\) is a field -parameterized by a neural network with parameters \(\)- that maps input coordinates \(x^{n}\) in the data domain alongside conditioning latents \(z\) to \(d\)-dimensional signal values \((x)^{d}\). By associating a conditioning latent \(z^{}^{c}\) to each signal \(\), a single conditional NeF \(f_{}:^{n}^{c}^{d}\) can learn to represent families \(\) of continuous signals such that \(\,:f(x) f_{}(x;z^{})\).  propose to use conditional NeFs for PDE modelling by learning a continuous flow in the latent space of a conditional neural field. In particular, a set of latents \(\{z^{}_{i}\}_{i=1}^{T}\) are obtained by fitting a conditional neural field to a given set of observations \(\{_{i}\}_{i=1}^{T}\) at timesteps \(1,...,T\); simultaneously, a neural ODE \(F_{}\) is trained to map pairs of temporally contiguous latents s.t. solutions correspond to the trajectories traced by the learned latents. Though this approach yields impressive results for sparse and irregular data in planar PDEs, we show it breaks down on complex geometries. We hypothesize that this is due to lack of a latent space that preserves relevant geometric transformations that characterize the symmetries of the systems we are modelling, and as such propose an extension of this framework where such symmetries are preserved.

Symmetries and weight sharing.Given a group \(G\) with identity element \(e G\), and a set \(X\), a _group action_ is a map \(:G X X\). For simplicity, we denote the action of \(g G\) on \(x X\) as \(gx:=(g,x)\), and call _\(G\)-space_ a smooth manifold equipped with a \(G\) action. A group action is homomorphic to \(G\) with its group product, namely it is such that \(ex=x\) and \((gh)x=g(hx)\). As an example, we are interested in the Special Euclidean group \(()\)=\(^{n} SO(n)\): group elements of \(()\) are identified by a translation \(t^{n}\) and rotations \( SO(n)\) with group operation \(gg^{}=(t,_{})(t^{},_{ ^{}})=(^{}+,_{^{}})\); We denote by \(_{g}\) the left action of \(G\) on function spaces defined as \(_{g}f(^{})=f(g^{-1}^{})=f(_{}^{-1}(^{}-))\). Many PDEs are defined by _equivariant_ differential operators such that for a given state \(\): \(_{g}[]=[_{g}]\). If the boundary conditions do not break the symmetry, namely if the boundary is symmetric with respect to the same group action, then a \(G\)-transformed solution to the IVP for some \(_{0}\) corresponds to the solution for the \(G\)-transformed initial value. For example, laws of physics do not depend on the choice of coordinate system, this implies that many PDEs are defined by \(()\)-equivariant differential operators. The _geometric deep learning_ literature shows that models can benefit from leveraging the inherent symmetries or invariances present in the data by constraining the searchable function space through _weight sharing_. Recall that in our framework we model flows of fields, solutions to PDEs defined by equivariant differential operators, with ordinary differential equations in the latent space of conditional neural fields. We leverage the symmetries of the system for two key aspects of the proposed method: first by making the relation between signals and corresponding latents equivariant; second, by using equivariant ODEs, namely ODEs defined by equivariant vector fields: if \(}{d}\)=\(F(z)\) is such that \(F(gz)=gF(z)\), then solutions are mapped to solutions by the group action.

## 3 Method

We adapt the work of , and consider the following optimization problem 2:

\[}{}\ \ _{ D,x ,t[T]}\|_{t}(x)-f_{}(x;z^{}_{t})\|_{2}^{2},  z^{}_{t}=z^{}_{0}+_{0}^{t}F_{}(z^{}_ {})d\,,\] (1)

with \(f_{}(x;z^{}_{t})\) a decoder tasked with reconstructing state \(_{t}\) from latent \(z^{}_{t}\) and \(F_{}\) a neural ODE that maps a latent to its temporal derivative: \(_{}}{d}\)=\(F_{}(z^{}_{})\), modelling the solution as flow in latent space starting at the initial latent \(z^{}_{0}\) - see Fig. 1 for a visual intuition.

Equivariant space-time continuous dynamics forecasting.A PDE defined by a \(G\)-equivariant differential operator - for which \(_{g}[]=[_{g}]\) - are such that solutions are mapped to other solutions by the group action if the boundary conditions are symmetric. We would like to leverage this property, and _constrain_ the neural ODE \(F_{}\) such that the solutions it finds in latent space can be mapped onto each other by the group action. Our motivation for this is twofold: (1) it is natural for our model to have, by construction, the geometric properties that the modelled system is known to posses - (2) to get more structured latent representations and facilitate the job of the neural ODE. To achieve this we first need the latent space \(Z\) to be equipped with a well-defined group action with respect to which \( g G,z Z:F_{}(gz)=\)\(gF_{}(z)\), and, most importantly, we need the relation between the reconstructed field and the corresponding latent to be equivariant, i.e.,

\[ g G\,,\,x:_{g}f_{}(x;z_{t}^{})=f_ {}(g^{-1}x;z_{t}^{})=f_{}(x;gz_{t}^{}).\] (2)

Note that, somewhat imprecisely, we call this condition _equivariance_ to convey the idea even though it is not, strictly speaking, the commonly used definition of equivariance for general operators. If we consider the decoder as a mapping from latents to fields, we can make the notion of equivariance of this mapping more precise. Namely

\[f(x)=D_{}(z),D_{}(z):z_{t}^{} f_{}(;z_{t}^{ })\,,f(g^{-1}x)=D_{}(gz),D_{}(gz):g\,z_{t}^{} f_{ }(g^{-1}\,;z_{t}^{})\,.\] (3)

In Sec. 3.1 we describe the Equivariant Neural Field (ENF)-based decoder, which satisfies equation (2). Second, in Sec. 3.2 we outline the graph-based equivariant neural ODE. Sec. 3.3 explains the motivation for- and use of- meta-learning for obtaining the ENF backbone parameters. We show how the combination of equivariance and meta-learning produce much more structured latent representations of continuous signals (Fig. 3).

### Representing PDE states with Equivariant Neural Fields

We briefly recap ENFs here, referring the reader to  for more detail. We extend ENFs to symmetries for PDEs over varying geometries.

ENFs as cross-attention over bi-invariant attributes.Atention-based conditional neural fields represent a signal \(\) with a corresponding _latent set_\(z^{}\). This class of conditional neural fields obtain signal-specific reconstructions \((x) f_{}(x;z^{})\) through a cross-attention operation between the latent set \(z^{}\) and input coordinates \(x\). ENFs  extend this approach by imposing equivariance constraints w.r.t a group \(()\) on the relation between the neural field and the latents such that transformations to the signal \(\) correspond to transformation of the latent \(z^{}\) (Eq. (2)). For this condition to hold, we need a well-defined action on the latent space \(Z\) of \(f_{}\). To this end, ENFs define elements of the latent set \(z^{}\) as tuples of _pose_\(p_{i}\) and _context_\(_{i}^{d}\), \(z^{}:=\{(p_{i},_{i})\}_{i=1}^{N}\). The latent space is then equipped with a group action defined as \(gz=\{(gp_{i},_{i})\}_{i=1}^{N}\). To achieve equivariance over transformations ENFs follow  where equivariance is achieved with convolutional _weight-sharing_ over equivalence classes of points pairs \(x,x^{}\). ENFs instead extend weight-sharing to cross-attention over _bi-invariant_ attributes of \(z,x\) pairs.

Weight-sharing over bi-invariant attributes of \(z,x\) is motivated by Eq. 2, by which we have:

\[f_{}(x;z)=f_{}(gx;gz).\] (4)

Intuitively, the above equation says that a transformation \(g\) on the domain of \(f_{}\), i.e. \(g^{-1}x\), can be undone by _also_ acting with \(g\) on \(z\). In other words, the output of the neural field \(f_{}\) should be _bi-invariant_ to \(g-\)transformations of the pair \(z,x\). For a specific pair \((z_{i},x_{m}) Z X\), the term bi-invariant attribute \(_{i,m}\) describes a function \(:(z_{i},x_{m})(z_{i},x_{m})\) such that \((z_{i},x_{m})=(gz_{i},gx_{m})\). Thorughout the paper we use \(_{i,m}\) as shorthand for \((z_{i},x_{m})\).

To parameterize \(f_{}\), we can accordingly choose any function that is bi-invariant to \(G-\)transformations of \(z,x\). In particular, for an input coordinate \(x_{m}\) ENFs choose to make \(f_{}\) a cross-attention operation between attributes \(_{i,m}\) and the invariant context vectors \(_{i}\):

\[f_{}(x_{m},z)=(_{;m},_{ ;},_{})\] (5)

As an example, for \(()\)-equivariance, we can define the bi-invariant simply using the group action: \(_{i,m}^{()}=p_{i}^{-1}x_{m}=_{i}^{T} (x_{m}-x_{i})\), which is bi-invariant by:

\[ g():\ \ (p_{i},x)\ \ (g\,p_{i},g\,x)  p_{i}^{-1}x\ \ (g\,p_{i})^{-1}g\,x=p_{i}^{-1}g^{-1}g\,x=p_{i}^{-1}x\,.\] (6)

Figure 2: The proposed framework respects predefined symmetries of the PDE: a rotated solution \(_{g}_{T}\) may be obtained either by solving from latent \(z_{0}^{}\) (top-left) and transforming the solution \(z_{T}^{}\) (top-right) to \(gz_{T}^{}\) (bottom-right) or transforming \(z_{0}^{}\) to \(gz_{0}^{}\) (bottom-left) and solving this.

Bi-invariant attributes for PDE solving.As explained above, ENF is equivariant to \(\)-transformations by defining \(f_{}\) as a function of an \(\)\(-\)bi-invariant attribute \(^{}\). Although many physical processes adhere to roto-translational symmetries, we are also interested in solving PDEs that - due to the geometry of the domain, their specific formulation, and/or their boundary conditions - are not fully \(\)\(-\)equivariant. As such, we are interested in extending ENFs to equivariances that are not strictly (subsets of) \(\), which we show we can achieve by finding bi-invariants that respect these particular transformations. Below, we provide two examples, the other invariants we use in the experiments - including a "bi-invariant" \(^{}\) that is not actually bi-invariant to any geometric transformations, which we use to ablate over equivariance constraints - are in Appx. D.

_The flat 2-torus._ When the physical domain of interest is continuous and extends indefinitely, periodic boundary conditions are often used, i.e. the PDE is defined over a space topologically equivalent to that of the 2-torus. Such boundary conditions break \(\) symmetries; assuming the domain has periodicity \(\) and none of the terms of this PDE depend on the choice of coordinate frame, these boundary conditions imply that the PDE is equivariant to periodic translations: the group of translations modulo \(\): \(^{2}^{2}/^{2}\). In this case, periodic functions over \(x,y\) with periods \(\) would work as a bi-invariant, i.e. using poses \(p^{2}\), \(^{^{2}}=(2(x_{0}-p_{0}))+(2(x_{1}-p_{1}))\) - which happens to be bi-invariant to rotations by \(\) as well. Instead, since we do not assume any rotational symmetries to exist on the torus, we opt for a non-rotationally symmetric function:

\[_{i,m}^{^{2}}=(2(x_{i}^{0}-p_{i}^{0}))(2 (x_{i}^{1}-p_{i}^{1})),\] (7)

where \(\) denotes concatenation. This bi-invariant is used in experiments on Navier-Stokes over the flat 2-Torus.

_The 2-sphere._ In some settings a PDE may be symmetric only to rotations along a certain axes. An example is that of the global shallow-water equations on the two-sphere - used to model geophysical processes such as atmospheric flow , which are characterised by rotational symmetry only along the earth's axis of rotation due to inclusion of a term for Coriolis acceleration that breaks full \(\) equivariance. We use poses \(p\) parametrised by Euler angles \(,,\), and spherical coordinates \(,\) for \(x S^{2}\). We make the first two Euler angles coincide with the spherical coordinates and define a bi-invariant for rotations around the axis \(=\).

\[_{i,m}^{}=_{p_{i},x_{m}}_{p_{i}} _{p_{i}}_{x_{m}},\] (8)

where \(_{p_{i},x_{m}}\)\(=\)\(_{p_{i}}\)\(-\)\(_{x_{m}}\)\(-\)\(2\) if \(_{p_{i}}\)\(-\)\(_{x_{m}}\)\(>\)\(\) and \(_{p_{i},x_{m}}\)\(=\)\(_{p_{i}}\)\(-\)\(_{x_{m}}\)\(+\)\(2\) if \(_{p_{i}}\)\(-\)\(_{x_{m}}\)\(<\)\(-\), to adjust for periodicity.

In summary, to parameterize an ENF equivariant with respect to a specific group we are simply required to find attributes that are bi-invariant with respect to the same group. In general we achieve this by using group-valued poses and their action on the PDE domain.

### PDE solution as latent space flow

Let \(z_{0}^{}\) be a latent set that faithfully reconstructs the initial state \(_{0}\). We want to define a neural ODE \(F_{}\) that map latents \(z_{t}^{}\) to their temporal derivatives \(^{}}{d}\)\(=\)\(F_{}(z_{}^{})\) that is equivariant with respect to the group action: \(gF_{}(z_{}^{})\)\(=\)\(F_{}(gz_{}^{})\). To this end, we use a _message passing neural network_ (MPNN) to learn a flow of poses \(p_{i}\) and contexts \(_{i}\) over time. We base our architecture on P@NITA , which employs convolutional weight-sharing over bi-invariants for \(\). For an in-depth recap of message-passing frameworks, we refer the reader to Appx. A. Since \(F_{}\) is required to be equivariant w.r.t. the group action, any updates to the poses \(p_{i}\) should also be equivariant.  proposes to parameterize an equivariant node position update by using a basis spanned by relative node positions \(x_{j}-x_{i}\). In our setting, poses \(p_{i}\) are points on a manifold \(M\) equipped with a group action. As such, we analogously propose parameterizing pose updates by a weighted combination of logarithmic maps \(_{p_{i}}(p_{j})\), which intuitively describe the relative position between \(p_{i},p_{j}\) in the tangent space \(T_{p_{i}}M\), or the displacement from \(p_{i}\) to \(p_{j}\). We integrate the resulting pose update over the manifold through the exponential map \(_{p_{i}}\). In the euclidean case \(_{p_{i}}(p_{j})\)\(=\)\(x_{j}-x_{i}\) and we get back node position updates per . In short, the message passing layers we use consist of the following update functions:

\[_{i}^{l+1}=_{(p_{j},_{j}) z^{,l}}k^{}(_{i,j}^{l})_{j}^{l}, p_{i}^{l+1}=_{p_{i}^{l}} _{(p_{j}^{l},_{j}^{l}) z^{,l}}k^{}( _{i,j}^{l})_{j}^{l}_{p_{i}^{l}}(p_{j}^{l}),\] (9)

with \(k^{},k^{}\) message functions weighting the incoming context and pose updates, parameterized by a two-layer MLP as a function of the respective bi-invariant.

### Obtaining the initial latent \(z_{0}^{}\)

Until now we've not discussed how to obtain latents corresponding to the initial condition \(z_{0}^{}\). An approach often used in conditional neural field literature is that of autodecoding , where latents \(z^{}\) are optimized for reconstruction of the input signal \(\) with SGD. Optimizing a NeF for reconstruction does not necessarily lead to good quality representations , i.e. using MSE-based autodecoding to obtain latents \(z_{t}^{}\) - as is proposed by  - may complicate the latent space, impeding optimization of the neural ODE \(F_{}\). Moreover, autodecoding requires many optimization steps at inference (for reference,  use 300-500 steps).  propose meta-learning as a way to overcome long inference times, as it allows for fitting latents in a few steps - typically three or four. We hypothesize that meta-learning may also structure the latent space - similar to the impact of equivariance constraints, since the very limited number of optimization steps requires efficient organization of latents \(z_{t}^{}\) around the (shared) initialization, forcing together the latent representation of contiguous states. To this end, we propose to use meta-learning for obtaining the initial latent \(z_{0}^{}\), which is then unrolled by the neural ode \(F_{}\) to find solutions \(z_{t}^{}\).

### Equivariance and meta-learning structure the latent space \(Z\)

As a first validation of the hypotheses that both equivariance constraints and meta-learning introduce structure to the latent space of \(f_{}\), we visualize latent spaces of different variants of the ENF. We fit ENFs to a dataset consisting of solutions to the heat equation for various initial conditions (details in Appx. E). For each sample \(_{t}\), we obtain a set of latents \(z_{t}^{}\), which we average over the invariant context vectors \(_{i}^{c}\) to obtain a single vector in \(^{c}\) invariant to a group action according to the chosen bi-invariant. Next, we apply T-SNE  to the resulting vectors in \(^{c}\). We use three setups: (a) no meta-learning, \(\) and latents \(z_{t}^{}\) optimized for every \(_{t}\) separately using autodecoding , and no equivariance imposed (per Eq. 15), shown in Fig. 2(a). (b) meta-learning is used to obtain \(,z_{t}^{}\), but no equivariance imposed, shown in Fig. 2(b) and (c) meta-learning is used to obtain \(,z_{t}^{}\) and \((2)\)-equivariance is imposed by weight-sharing over \(^{(n)}\) bi-invariants, shown in Fig. 2(c). The results confirm our intuition that both meta-learning and equivariance improve latent-space structure.

Recap: optimization objective.We use a meta-learning inner-loop [28; 13] to obtain the initial latent \(z_{0}^{}\) under supervision of coordinate-value pairs \((x,(x)_{0})_{x}\) from \(_{0}\). This latent is unrolled for \(t_{}\) timesteps using \(F_{}\). The obtained latents are used to reconstruct states \(z_{t}^{}\) along the trajectory

Figure 3: We show the impact of meta-learning and equivariance on the latent space of the ENF when representing trajectories of PDE states. Fig. 2(a) shows a T-SNE plot of the latent space of \(f_{}\) when \(z_{t}^{}\) is optimized with autodecoding, and no weight sharing over bi-invariants is enforced. Fig. 2(b) shows the latent space when meta-learning is used, but no weight sharing is enforced. Fig. 2(c) shows the latent space when \(z_{t}^{}\) are obtained using meta-learning and \(f_{}\) shares weights over \(^{(n)}\).

of \(\), and parameters of \(f_{},F_{}\) are optimised for reconstruction MSE, as shown in the left-hand side of Eq. 1. See Appx. B for detailed pseudocode of this process.

## 4 Experiments

We intend to show the impact of symmetry-preservation in continuous PDE solving. To this end we perform a range of experiments assessing different qualities of our model on tasks with different symmetries. First, we investigate the **equivariance properties** of our framework by evaluating it against unseen geometric transformations of the initial conditions. Next, we assess **generalization** and **extrapolation** capabilities w.r.t. unseen spatial locations and time horizons inside and outside the time ranges seen during training respectively, **robustness** to partial test-time observations, and **data-efficiency**. As the continuous nature of NeF-based PDE solving allows, we verify these properties for PDEs defined over **challenging geometries**: the plane \(^{2}\), 2-torus \(^{2}\) and the sphere \(S^{2}\) and the 3D ball \(^{3}\). Architectural details and hyperparameters are in Appx. E. _Code is available on GitHub_.

Additionally, we validate our model on a benchmark of PDEs that exhibits **no transformation symmetries**: the CFDBench  benchmark. We include details on parameter counts, memory usage and runtimes of our model compared to baselines in Appx. F.

### Datasets and evaluation

All datasets are obtained by randomly sampling disjoint sets of initial conditions for train and test sets, and solving them using numerical methods. Dataset-specific details on generation can be found in Appx E. **"Heat equation on \(^{2}\) and \(S^{2}\).** The heat equation describes diffusion over a surface: \(=D^{2}c\), where \(c\) is a scalar field, and \(D\) is the diffusivity coefficient. We solve it on the 2D plane where \(^{2}c=c}{ x_{1}}+c}{  x_{2}}\) - and on the 2-sphere \(S^{2}\) where in spherical coordinates: \(^{2}c=(( )+} c}{^{2}})\). Although a relatively simple PDE, we find that defining it over a non-trivial geometry such as the sphere proves hard for non-equivariant methods. **"Navier-Stokes on \(^{2}\)**. We solve 2D Navier Stokes  for an incompressible fluid with dynamics \(=-u v+v+f,v= u, u=0\), where \(u\) is the velocity field, \(v\) the vorticity, \(\) the viscosity and \(f\) a forcing term (see Appx. E). We create a dataset of solutions for the vorticity using Gaussian random fields as initial conditions. Due to the incompressibility condition, it is natural to solve this PDE with periodic boundary conditions corresponding to the topology of a 2-Torus \(^{2}\) - implying equivariance to periodic translation. **"Shallow-water on \(^{2}\)**. The global shallow-water equations model large-scale oceanic and atmospheric flow on the globe, derived from Navier-Stokes under assumption of shallow fluid depth. The global shallow-water equations (see Appx. E) include terms for Coriolis acceleration, which makes this problem equivariant to rotation along the globe's axis of rotation. We follow the IVP specified by , and create a dataset of paired vorticity-fluid height solutions. **"Internally-heated convection in a 3D ball.** We solve the Boussinesq equation for internally heated convection in a ball, a model relevant for example in the context of the Earth's mantle convection. It involves continuity equations for mass conservation, momentum equations for fluid flow under pressure, viscous forces and buoyancy, and a term modelling heat transfer. We generate initial conditions varying the internal temperature using \(N(0,1)\) noise and obtain solutions for the temperature defined over a regular spherical \(,,r\) grid. **"CFDBench** consists of a set of one-step solutions (pairs of input and output states \(_{t},_{t+1}\)) for classic computational fluid dynamics (CFD) problems, with varying fluid properties, boundary conditions and geometries. The goal of this dataset is to assess generalizability of DL-based PDE solvers over problem parameters. This dataset does not exhibit transformation symmetries because of the absolute position of obstacles in the geometry and orientation of the flow.

Figure 4: A train and test sample from the planar diffusion dataset. Initial conditions for train and test are spikes in disjoint subsets of \(^{2}\).

Evaluation.All reported MSE values are for predictions obtained given only the initial condition \(v_{0}\), with std over 3 runs. We evaluate two settings for train and test sets both: **generalization setting** with time evolution happening within the seen horizon during training (\(t_{}\)); and, **extrapolation setting** with the time evolution happening outside the seen horizon during training (\(t_{}\)). For both cases we measure the mean-squared error (MSE). To position our work relative to competitive data-driven PDE solvers, on the 2D-Navier-Stokes experiment we provide comparisons with a range of baselines. In most other settings these models cannot straightforwardly be applied, and we only compare to , to our knowledge the only other fully continuous PDE solving method in literature. For the Navier-Stokes and Internally-Heated Convection experiments, we compare with Transolver , which has shown SOTA results as DL-based PDE solving method for general geometries.

Equivariance properties - heat equation on the plane.To verify our framework respects the posed equivariance constraints, we create a dataset of solutions to the heat equation that _requires_ a neural solver to respect equivariance constraints to achieve good performance. Specifically, for initial conditions we randomly insert a pulse of variable intensity in \(x=(x_{1},x_{2})^{2}\) s.t. \(-1{<}x_{1}{<}1,0{<}x_{2}{<}1\) for the training data and \(-1{<}x_{1}{<}1,-1{<}x_{2}{<}0\) for the test data. Intuitively, train and test sets contain spikes under different disjoint sets of root-translations (see Fig. 4). We train variants of our framework with (\(^{(2)}\), Eq. 6) and without (\(^{}\), Eq. 15) equivariance constraints. In this dataset, we set \(t_{}=[0,...,9]\), and evaluation horizon \(t_{}=[10,...,20]\). Results in Tab. 1 show that the non-equivariant model, as well as the baseline  are unable to successfully solve test initial conditions, whereas the equivariant model performs well.

Robustness to subsampling & time-horizons - Navier-Stokes on the 2-Torus.We perform an experiment assessing the impact of equivariance constraints and meta-learning on robustness to sparse test-time observations of the initial condition. To this end, we train a model with (\(^{^{2}}\), Eq. 7), without (\(^{}\), Eq. 15) equivariance constraints, and one with equivariance constraints and without meta-learning (AD \(^{^{2}}\), Eq. 7), on a fully-observed train set. The training horizon \(t_{}=[0,...,9]\), and evaluation horizon \(t_{}=[10,...,20]\). Subsequently, we apply the trained model to the problem of solving from sparse initial conditions \(v_{0}\), with observation rates where \(50\%\) and \(5\%\) of the initial condition is observed (Tab. 2). Approaches operating on discrete (CNODE ) and regular grids (FNO , G-FNO ) perform very well when evaluated on fully-observed regular grids, outperforming continuous approaches (ours, ). However, we note 

[MISSING_PAGE_FAIL:9]

sets, but overfits to this time horizon, generalizing poorly beyond. We additionally show results for sparsely observed input states, showing the limitations of non NeF-based solvers to generalize over irregular changing observation grids. We interpret these results as an indication of a marked reduction in solving-complexity and improved generalization when correctly accounting for a PDE's symmetries.

Non-symmetric PDEsLastly, we evaluate our model in the setting when solving PDEs that do not exhibit any global symmetries to assess whether the preservation of symmetries in the latent space of our model precludes application to non-symmetric PDEs. We train a translation-equivariant (\(^{^{2}}\)) model, i.e. one with equivariance properties identical to a CNN, on the Cavity, Dam and Cylinder flows from CFDBench . Comparing to the baselines set by the dataset authors, we see our method improves significantly over a number of classical baselines, despite no global transformational symmetries being present in the problems being solved.

## 5 Limitations & Future work

We're interested in exploring the application of our ENF-based PDE solving framework to larger-scale, more complex problems. Throughout our experiments with ENFs we noticed that more complex signals, e.g. higher resolution PDEs, may be fit easily either by increasing the ENF hidden size or by increasing the number of latents used. However, either of these changes significantly impacts computational complexity, due to the calculation of attention coefficients in the latent space of the ENF for every latent-input coordinate pair. A possible way of addressing this is detailed in ; we can approximate the output of the attention operation through limiting the number of latents attended to (using k-nearest neighbours). This may open the door to modelling more complex, larger-scale dynamics than learned in present experiments.

## 6 Conclusion

We introduce a novel equivariant space-time continuous framework for solving partial differential equations (PDEs). Uniquely - our method handles sparse or irregularly sampled observations of the initial state while respecting symmetry-constraints and boundary conditions of the underlying PDE. We clearly show the benefit of symmetry-preservation over a range of challenging tasks, where existing methods fail to capture the underlying dynamics.