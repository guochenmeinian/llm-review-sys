# Volla-A: Aligning Vision-Language Models with User's Gaze Attention

Kun Yan\({}^{1}\)1, **Zeyu Wang\({}^{2}\)2, **Lei Ji\({}^{3}\), **Yuntao Wang\({}^{2}\), **Nan Duan\({}^{3}\), **Shuai Ma\({}^{1}\)**2

\({}^{1}\) SKLSDE Lab, Beihang University

\({}^{2}\) Key Laboratory of Pervasive Computing, Tsinghua University

\({}^{3}\) Microsoft Research

kunyan@buaa.edu.cn, wang-zy23@mails.tsinghua.edu.cn

mashuai@buaa.edu.cn

equal contribution

###### Abstract

In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by AR or VR devices, as a proxy for human attention to guide VLMs and propose a novel approach, Voila-A, for gaze alignment to enhance the interpretability and effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we innovate the Voila Perceiver modules to integrate gaze information into VLMs while preserving their pre-trained knowledge. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE test set, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications. Our code is available at https://github.com/naykun/Voila-A

## 1 Introduction

The integration of vision and language understanding has witnessed significant advancements in recent years, particularly through the development of Vision-Language Models (VLMs). These models have demonstrated remarkable performance in various tasks, such as visual question answering, image captioning, and visual storytelling, among others. Although VLMs exhibit strong performance in various tasks, their applicability in everyday scenarios is hindered by their limited alignment with human users' focus. This misalignment leads to suboptimal performance and decreased user satisfaction. Current VLMs' inability to process these intentional modalities results in imprecise and unhelpful responses. As demonstrated in Figure 1, a user's intent can be communicated throughspoken language, multimodal expressions, or even be concealed. The users' attention can clarify vague expressions, meanwhile, uncovering hidden intentions is more challenging.

Most recent VLMs  primarily focus on learning alignment between vision input and text tokens for LLMs or designing learnable interaction layers to attend the vision input to the frozen LLM layers. The importance of aligning AI with human attention has been highlighted in previous research, which demonstrates that incorporating visual attention can lead to improved user experience . Additionally, there has been growing interest in grounded VLMs, which investigate the fine-grain grounding capability between region-text pairs instead of image-text pairs and further conduct dense regional prediction tasks .

The representation of visual regions within computational models can be achieved through various methodologies, such as bounding boxes , discrete points , or continuous traces . For the integration of such regional data into models, researchers have developed multiple approaches. These include the concatenation of cropped image patches with the original textual or visual inputs , the application of masks or Gaussian maps to highlight user-specified areas , and the incorporation of positional encodings to represent points, boxes, or traces . Despite the extensive exploration of bounding boxes and segmentation techniques within VLMs, their application remains suboptimal for end-users to generate input signals.

We propose to introduce a novel approach that leverages gaze tracking as a more intuitive and interactive method for defining visual regions, particularly within augmented reality (AR) and virtual reality (VR) environments. Zhang  provides an overview of gaze-related research, outlining a process that begins with collecting human gaze data (further discussed in B.3), followed by building models to predict human attention distribution (i.e., saliency models, as discussed in B.4), and culminating in human-gaze-assisted AI. They conclude that _AI agents capable of perceiving and understanding human gaze behaviors can better infer user needs and assist in daily tasks_. However, they also note that _research in this final direction is still limited_. Our work aims to advance this area further. Prior research in gaze-based visual representation includes gaze-directed visual grounding  and the implementation of eye-gaze within vision transformers . Nonetheless, these methodologies have encountered obstacles in terms of scalability and adaptability. Despite significant progress, the seamless integration of gaze information into large-scale VLMs is still a formidable challenge.

To tackle this issue, we demonstrate that mouse trace data can be a proxy for gaze behavior modeling and leverage trace data from Localized Narratives  to annotate instructional data using GPT-4 . Another critical aspect of the challenge is maintaining the integrity of the pre-trained knowledge within VLMs while effectively assimilating gaze data. We further design Voila-A's attention mechanism to incorporate gaze information while not forgetting pre-trained knowledge. We evaluate Voila-A through a hold-out validation set and a newly collected test set VOILA-GAZE, featuring real-life scenarios with a gaze-tracking device.

In this paper, we make the following contributions:

* We propose Voila-A, a novel approach for aligning VLMs with a user's gaze attention, and design innovative mechanisms to integrate gaze information into VLMs while preserving pre-trained knowledge.

Figure 1: AR and VR scenarios usually involve complex scenes with multiple objects. Users may interested in only one specific object and gaze is the most natural way to interact with the device.

* We leverage trace data from Localized Narratives to annotate instructional data using GPT-4, generating the VOILA-COCO dataset with 72k QA pairs, and demonstrate the scalability of this method.
* We evaluate Voila-A through a hold-out validation set and a newly collected VOILA-GAZE test set of real gaze samples, demonstrating that our approach significantly outperforms several baselines.

By aligning model attention with human gaze patterns and leveraging state-of-the-art techniques, we make a step forward in the development of more intuitive and user-centric VLMs, paving the way for more effective and engaging human-AI interaction in a wide range of applications

## 2 Leveraging Trace Data as an Alternative Approach to Align VLMs with Gaze Attention

In this section, we discuss the potential of trace data as a proxy for gaze data and propose a method for transforming trace data to make it more gaze-like, ultimately enabling the effective use of trace data for aligning VLMs with user gaze attention.

Obtaining gaze data for training VLMs can be challenging, as it is difficult to annotate and expensive to acquire. We propose that an alternative approach can be employed to align VLMs with user gaze attention: utilizing trace data, such as mouse traces. Localized Narratives (LN), a prior work, has annotated 849,000 images with mouse traces that are aligned with each word of the descriptions. The project involved 156 professional annotators who worked full-time, with annotator managers ensuring high-quality annotations through manual inspections and an automatic quality control mechanism. After discarding 23.5% of annotations, the remaining ones demonstrated a semantic accuracy of 98.0% for nouns and verbs. The accuracy of mouse traces in relation to object locations was also analyzed, revealing that most trace points were within the correct bounding box.

To demonstrate the similarities between gaze and mouse traces, we first collect hundreds of minutes of gaze data samples as described in C, and then we further collect mouse trace annotations on those samples. The mouse trace collection was done using the Bubbleview  interface, where the annotators were instructed to verbally repeat the questions and register their eye fixations with mouse clicks. **The resulting CC (cross-correlation) score between real gaze fixations and mouse traces was 0.82, and the NSS (normalized scanpath saliency) score was 2.57. These scores are comparable to those reported in the Bubbleview study  and are consistent with the performance of SOTA saliency models , indicating that gaze and mouse traces exhibit similarities, as users tend to fix their gaze on the target object when asking questions, a behavior also observed with mouse traces.** However, there are minor differences between the two, specifically in terms of gaze fixation continuity and the presence of noise points outside the target object at the end of a query. In the case of mouse traces, points that fell outside the bounding box were attributed to two factors: annotators often circled around objects, causing the traces to be near but not inside the box, and some annotators started moving the mouse before describing the object or vice versa. These observations provide valuable insights for properly leveraging trace data into the alignment process and understanding the relationship between gaze attention and language description.

In order to utilize trace data as a substitute for gaze data, we introduce a method to transform mouse traces, thereby reducing the discrepancies between the two data types and making the trace data more gaze-like. We first address the inherent noise in both trace points and gaze points by converting them into 2D heatmaps using Gaussian blur:

Figure 2: EMD between the mean heatmaps of 1k gaze and trace samples with varying sampling rates.

\[H(x,y)=}e^{-+y^{2}}{2^{2}}}\] (1)

where \(H(x,y)\) represents the heatmap value at position \((x,y)\), and \(\) is the standard deviation of the Gaussian kernel.

Since mouse traces are more continuous than gaze fixations, we downsample the trace data to better resemble the distribution of gaze data. We investigate the Earth Mover's Distance (EMD) between the mean heatmaps of 1k gaze and trace samples while varying the sampling rate from 1 to 40:

\[(P,Q)=^{n}|F_{i}(P)-F_{i}(Q)|}{_{i=1}^{n}F_{i}(P)}\] (2)

where \(P\) and \(Q\) are the distributions of the gaze and trace heatmaps, \(F_{i}\) denotes the cumulative distribution function, and \(n\) is the number of bins.

We observe that the EMD has a local minimum value around a sampling rate of 25 as shown in Figure 2. By selecting this optimal sampling rate, we can approximate the trace heatmap as an alternative to the real gaze heatmap from a statistical perspective. Consequently, this transformation mitigates the differences in inter-relationships, compactness, and noisiness between the trace and gaze data.

## 3 Method

### Automatic Data Annotation For LN-COCO

The automatic data annotation process for Voila-A is driven by the motivation to develop a more intuitive and user-centric VLM by aligning model attention with human gaze patterns. As shown in Figure 3, this process aims to create an effective and engaging human-AI interaction experience across various applications. To achieve this, we have designed an innovative annotating approach that leverages the capabilities of GPT-4 as a visual assistant to annotate trace-aligned instructional data to simulate the user's gaze attention. The data annotation process follows design principles to ensure accurate, relevant, and consistent annotations. These include: 1) focusing on referable sentences and appropriate tags, 2) using a conversational format with specific and general questions, 3) addressing various visual content aspects with definite answers, 4) incorporating complex questions while avoiding uncertainty, and 5) offering detailed, well-organized explanations.

As illustrated in Figure 3, the automatic data annotation pipeline comprises three stages.

**Stage 1: Prompt Design Iteration.** The first stage focuses on refining the prompt design. Let \(S=\{(I_{i},N_{i},T_{i},C_{i})\}_{i=1}^{100}\) be a set of 100 samples from the LN-COCO dataset, where \(I_{i}\) represents the image, \(N_{i}\) the localized narrative, \(T_{i}\) the corresponding trace, and \(C_{i}\) the set of five captions from COCO-caption. We initiate the process with a basic system prompt, instructing GPT-4 to generate direct questions \(Q_{i,j}^{D}\) and indirect questions \(Q_{i,j}^{I}\) and corresponding answers \(A_{i,j}\) that specifically reference the localized narratives while considering COCO-caption as background information. The

Figure 3: Automatic Data Annotation Pipeline

referring portions are annotated with a unique marker \(\) for trace matching during post-processing. We also provide two in-context examples to guide the model in generating helpful, well-formatted, diverse, and visually grounded QA pairs. Throughout each iteration \(k\), we manually evaluate the quality of the generated grounded QA pairs and adjust the prompt to enhance their helpfulness, formatting, diversity, and visual relevance. After \(K=10\) iterations, we find the quality of most pairs to be satisfactory, and subsequently, we freeze the prompt to initiate the pipeline.

**Stage 2: Data Sampling.** In the second stage, we sample \(N=25,000\) image pairs from the LN-COCO dataset and obtain approximately \(M=75,000\) QA pairs.

**Stage 3: Post-processing.** The third stage involves post-processing the raw grounded QA pairs. This includes further filtering based on a set of keywords \(=\{\}\). We define a filtering function \(F_{k}(Q_{i,j},A_{i,j},)\) that identifies and removes QA pairs containing meta descriptions of the prompt. We note that this issue may be further resolved by using GPT-4V, which was not available during our submission date. Additionally, we identify cases where answers are unhelpful, such as "I don't know" or "It's hard to tell." We find that these types of answers have low reward scores, so we further examine all pairs using a reward model  and filter the dataset by setting a minimum reward threshold \(\). We define a filtering function \(F_{r}(Q_{i,j},A_{i,j},)\) that removes QA pairs with reward scores below \(\). Finally, we segment each localized narrative into temporally aligned segments with respect to the special marker \(\). Each segment comprises a grounded fact, a corresponding trace, a direct and indirect question, and an answer. This forms the final VOILA-COCO dataset, denoted as \(=\{(F_{i},T_{i},Q_{i,j}^{D},Q_{i,j}^{I},A_{i,j})\}\). It is worth noting that we did not utilize all localized narratives, leaving room for future exploration. We annotate the COCO subset of localized narratives, resulting in the Voila-COCO dataset, with statistics presented in Table 1.

The finalized prompt can be found in E. We also visualize a sample of our annotated data in Figure 14. Data quality analysis can be found in Section D. By adhering to these design principles, the automatic data annotation process ensures that the resulting dataset is of high quality and effectively aligns the VLM's attention with that of a human user.

### VOILA-GAZE: Real-life gaze-QA pairs

To further demonstrate the effectiveness of our method in aligning VLMs with real-life users' gaze attention, we conduct experiments in two everyday scenarios, encompassing a variety of question types details can be found in Table 5.

In addition to the recorded gaze trajectory, video, and transcription, each participant is instructed to annotate the key elements of their questions, formulate clear questions based on their interpretations, and choose the best answer from three candidate answers generated by GPT-4 according to their annotations. The experiment includes 16 participants (8 per scenario) with an equal gender distribution, aged between 20 and 30 (with a standard deviation of 2.06). Each participant takes approximately 240 minutes to complete the study. After applying post-filtering and manual checking, we curate a set of 200 QA pairs as our real-life benchmark, VOILA-GAZE. The curation process is conducted by two individuals sequentially, with the second person double-checking the following aspects: **1. The question is related and aligned with gaze. 2. The answer is meaningful and can be considered a proper response to the gaze-directed question. 3. The question is not related to specific brands, prices, or any other objects beyond general knowledge. 4. The question type is not biased towards a few simple patterns.** This two-step process ensures the quality and relevance of the curated data while minimizing potential biases and maintaining a focus on general knowledge. Samples of VOILA-GAZE are shown in Figure 9.

### Model Design

In developing our design, a critical consideration is the adherence to the established architecture of current VLMs. **It is essential to avoid introducing a significant number of new parameters

   Dataset & Split & \#Images & \#Questions & SR \\  Voila-COCOCO & Training & 20000 & 70000 & 93.5\% \\ Voila-COCO & Validation & 100 & 550 & 71.1\% \\ Voila-COCO & Test & 500 & 1900 & 75.7\% \\ Voila-GAZE & Real-life & 200 & 200 & 18.2\% \\   

Table 1: Statistics of Voila-COCO and Voila-Gaze Datasets, SR refers to Survival Rate from raw data after filtering or making extensive structural modifications. This constraint is due to the limitations of the current gaze dataset, which does not support large-scale pretraining.** Additionally, we must be vigilant in preventing catastrophic forgetting during the fine-tuning process. Therefore, we aim to implement only those modifications that are proven to be necessary and beneficial through our ablation studies. These changes are carefully selected to enhance performance without compromising the stability and efficiency of the model.

We employ the model architecture from OpenFlamingo, as illustrated on the right side of Figure 4. This framework consists of a pre-trained vision encoder, language decoder, and gated cross-attention layers, offering flexibility for multi-image and multi-turn conversational interactions. The primary challenge lies in incorporating gaze instructional signals into a pre-trained VLM. To tackle this issue, we initially developed several potential solutions, which are discussed in Sec 4.3.2 and Fig 16.

Based on empirical evidence, we ultimately confirm the effectiveness of the Voila Perceiver Resampler solution. The Voila Perceiver Resampler(VPR) processes the input image features and gaze information, and then feeds them into a series of Voila Perceiver Blocks (VPB):

\[(X,G)=&(_{n}(X,G)),_{0}(X,G)=(X,L_{0},G),\\ _{i}(X,G)=&(X,_{ i-1}(X,G),G)\ i=1,2,,m.\] (3)

The input hidden states of VPR are denoted as \(X^{B H L_{I} D}\), \(L^{B H L_{L} D}\)and \(G^{B H L_{H} D}\), where \(B\) is the batch size, \(L_{L}\)and\(L_{I}=L_{H}\) are the lengths of latent tokens and image/heatmap patches, respectively, \(H\) is the number of attention heads, and \(D\) is the hidden size. In which \(X\) represents the image features, \(G\) is the gaze heatmap embedding patches. \(L\) denotes the latent features, which are introduced from the original Perceiver as a small set of latent units that forms an attention bottleneck through which the inputs must pass. To obtain the gaze information \(G^{B H L_{H} D}\), we first divide the gaze heatmap \(G^{}^{B h w}\) into patches. Then, we apply a linear transformation followed by layer normalization. The process can be represented by the following equation:

\[G=(((G^{})))\] (4)

The VPR comprises a series of Voila Perceiver Blocks (depicted on the left side of Figure 4). This mechanism leverages gaze information to enhance visual feature perception. Our design adheres

Figure 4: Architecture of the VOILA Model: On the left, gaze fixation is transformed into a heatmap, which is subsequently processed through linear layers to encode the visual attention. This encoded data is then segmented into discrete patches that are spatially correlated with corresponding image patches. These gaze patches are further refined into key embeddings, which undergo modulation by a gating mechanism, designed to incrementally integrate gaze data. The resulting gaze and image key embeddings are then combined and subjected to a self-attention mechanism, which synthesizes the information into a cohesive set of latent perceiver embeddings. On the right, the figure delineates the integration pathway where the gaze heatmap and the image concurrently enter the VOILA Perceiver. This integrated input is subsequently directed through gated cross-attention modules before progressing into the language model layers, culminating in a unified output that encapsulates the interplay between visual attention and linguistic processing.

to the principle that the gaze serves as an information aggregator in the attention process without disrupting the original learned distribution. The Voila Perceiver Block(VPB) is defined as follows:

\[(X,L,G)=(L+(L+(X,L,G)))\] (5)

The feed-forward network, \(()\), is a sequence of layer normalization, linear transformation, GELU activation, and another linear transformation. The attention mechanism, \((X,L,G)\), is computed as follows:

\[ Q=W_{L}^{Q}L,\ V=W_{XL}^{V}(X L),\ K=W_{XL}^{ K}(X L)+W_{G}^{K}(G*tanh() P)\\ (X,L,G)=(}{})V\] (6)

Here, \(\) denotes concatenation along the feature dimension, and \(P\) is zero padding with the same shape as \(L\). And \(\) is a learnable gating parameter attributed to each block for fading in gaze guidance gradually. \(W_{L}\),\(W_{XL}\),\(W_{G}\) are the QKV matrices of corresponding input.

### Training

Our approach utilizes the OpenFlamingo training paradigm to train the Voila model, building upon the pre-trained weights of the Otter model, which incorporates an MPT-7B  language encoder and a CLIP ViT-L/14  vision encoder. To avoid overfitting and maximize the benefits of pre-trained knowledge, we initially freeze both encoders. As shown in Figure 4, we then train only the linear layers directly related to gaze input at the first stage for one epoch before fine-tuning the entire Perceiver resampler module, the cross-attention layers integrated into the language encoder, and the input/output embeddings of the language encoder in the second stage for an additional epoch. This process results in roughly 1.3 billion trainable parameters for the Otter model. Note that the \(\) in each \(\) are initialized as 0.

During training, we adhere to a specific format for preparing our training data. This format combines an image, user instruction, "GPT"-generated answers 1, and a unique token known as the [endofchunk] token. We arrange the training data as follows:

\(<\)context\(>\) [image] User:[fixation]\(<\)instruction\(>\) GPT:[answers] \(<\)answer\(>\).[endofchunk]

Here, the [image], [answer], [fixation], and [endofchunk] tokens are distinct and serve particular functions. We adopt a chatbot-like format to enhance the instruction-following capabilities and conversational generalizability of our model. The [image] and [endofchunk] tokens originate from the OpenFlamingo training paradigm, while the [answer] token is a new addition by Otter. The [answer] token separates answers from instructions, allowing us to mask all tokens following the [answer] token during training and designate them as the model's prediction objectives. We also introduce the [fixation] token to direct the language model to utilize gaze information. We train our model using a cross-entropy loss function.

## 4 Experiment

### Evaluation metrics

Gpt-4 RankingWe utilize GPT-4 Ranking as our primary automated evaluation metric to assess model performance through a one-to-one comparison. The GPT Ranking represents the language model's evaluation of the quality of the generated response. This score signifies the extent to which the response aligns with the ground truth image description and answer while demonstrating the model's language proficiency. Factors such as grammar, semantics, and fluency are taken into account when comparing the response to that of another model. **It is important to note that GPT-4 exhibits sequence ordering bias.** To mitigate this issue, we implement a dual-setting approach that reverses the order of the models, ensuring that the order does not influence the outcome. The prompt and evaluation procedure can be found in Figure 15.

Reward ScoreGiven that our dataset is automatically annotated using GPT-4, it is crucial to mitigate any potential system bias during model evaluation. To this end, we incorporate human preference by utilizing a reward model score as an auxiliary metric. The reward model, which assessesthe human-like quality of a response, is trained using human feedback to predict the superiority of a generated answer in relation to a given question from a human perspective . This approach allows us to achieve a more balanced and robust evaluation process, ensuring that our model's performance aligns with human expectations and preferences.

### Main Results

#### 4.2.1 VOILA Exhibits a Balanced Capability Between Helpfulness and Fact Grounding

In Figure 5, we observe a notable superiority of VOILA over both Otter and Kosmos-2 on the VOILA-COCOTESTSET. Regarding the _grounding_ capability, both VOILA and Kosmos-2 trained with fine-grained grounded facts outperform Otter model to a large extent. Besides, VOILA surpasses Kosmos-2 marginally. With respect to _helpful_ capability, Otter delivers significantly more helpful responses than Kosmos-2. Since Otter is trained on top of Openflamingo with an instruction-following dataset, it can provide a more helpful response, especially for informative queries while Kosmos-2 tends to answer visual observation from the input image. In addition, VOILA trained on gaze dataset demonstrates stronger helpful capabilities over all models.

In real gaze scenarios, as illustrated in Figure 6, VOILA outperforms the two baseline models as well. These scenarios differ substantially from the collected COCO images and present more challenging questions, necessitating a higher degree of accurate intent understanding and reasoning. Especially from the comparison of Otter vs Kosmos-2, we found that there are many more Tie results due to the hardness of the real cases. Despite these increased demands, VOILA continues to surpass both models, further showcasing its balanced proficiency in both helpfulness and fact grounding.

### Ablation studies

#### 4.3.1 Query types has a significant impact on Response Quality

Table 2 investigates the varying performance of different question types, specifically direct and implicit/coreference queries. As the base model Openflamingo was pre-trained on direct queries, both Otter and VOILA performed better in this category, as expected. In addition, it is natural for humans to communicate with coreference queries. VOILA maintained strong performance when handling coreference queries with the gaze as guidance while the Otter model greatly decreased. Furthermore, we appended in-context QA pairs prior to the current query and observed that the examples further improved the quality of the responses. In real-life situations, multi-turn conversations are involved in most interactions with many coreferen

   Methods & Question types & WR & L8 & Reward Score \\  Office-name & conference query & - & -1.91 \\ Otter-name & project quality & 0.51 & 0.1 & 0.02 \\ VolLA & conference query & 0.41 & 0.18 & -0.79 \\ VolLA & project quality & 0.62 & 0.15 & 0.14 \\ VolLA & in-context prompt + conference query & 0.46 & 0.16 & -0.02 \\ VolLA & in-context prompt + detector & 0.77 & 0.12 & 0.20 \\   

Table 2: Ablation on query types, WR means Winning Rate Over Otter-base

Figure 5: GPT-RANKING ON VOILA-COCO-Testset

Figure 6: GPT-RANKING ON VOILA-GAZE

of an in-context prompt can assist VOILA to demonstrate a superior ability. This improvement is evident across both direct and coreference query types.

#### 4.3.2 Heatmap is a better way to incorporate gaze

To establish the effectiveness of our approach, we implemented several alternative methods for incorporating gaze data into VLMs. These methods include: converting gaze sequences into discrete position tokens for LLMs, using the bounding box position of trace trajectories as additional patch tokens concatenated to VIT image feature token lists, and converting the bounding box coordinates into discrete tokens. We illustrate these methods in Figure 16. However, all these methods failed to outperform the gaze heatmap approach, as shown in Table 3.

#### 4.3.3 Gradual Unfreezing of Parameters Yields Better Results

Table 4 presents empirical findings that demonstrate the effectiveness of gradually unfreezing model parameters. Instead of directly unfreezing the vision perceiver and cross-attention layers, or using LORA to fine-tune the entire model, we first fine-tuned the gaze-related weights and then fine-tuned the other parts of the perceiver and cross-attention layers, which yielded better results. We hypothesize that this improvement is due to the newly added gaze component needing to adapt to the distribution of the pre-trained layers first. This adaptation process can further help mitigate the issue of catastrophic forgetting.

### Qualitative studies

We conducted qualitative studies on randomly selected cases and demonstrated the results of several representative examples in Appendix Figure 8. According to the analysis, the conclusions can be summarized as follows: 1) Existing models are able to generate reasonable results for **explicit** queries. In the 1st row, the object _cakes_ and the attributes _color_ are explicitly mentioned in the query, and the three models can answer (partially) correctly; 2) Regarding **coreference** queries, the model Otter is hard to understand the pronouns like _it_ without spatial guidance as shown in the 2nd row. This requires further context or generates the answer based on the salient object like _plane_ instead of the actual human attention; 3) The Kosmos-2 model can take the **bounding box** for grounding as spatial guidance, it is sometimes not accurate compared to the heatmap used in VOILA. As shown in the 3rd row, the bounding box is too coarse and made the model focus on the object _plane_ instead of the actual human attention _sky_; 4) Besides, we found that Kosmos-2 tends to describe the detailed visual content and sometimes lacks the **instruction-following** capability; In the 4th row, the Kosmos-2 responses _Keyboard_ depicted in the bounding box ignoring the actual query intention; Finally, 5) There are still further challenges for all models to deal with. For instance, counting for objects requires intensive fine-grained recognition of the visual content demonstrated in the last row.

## 5 Conclusion

In this study, we presented Voila-A, a cutting-edge approach that aligns Vision-Language Models with user gaze attention. Voila-A can be implemented in HMD AR/VR devices as an egoview copilot, benefiting a wide range of users, including visually impaired individuals who rely on their gaze to communicate their intent. This method surpasses the capabilities of similar mobile apps that necessitate users to lift their phones for scene capture. We successfully utilized trace data to create the

   Layers fine-tuned & WR & LR & Reward Score \\  Otter-BASE & - & - & -1.91 \\ Otter-BASE & - & 0.25 & 0.24 & -1.78 \\ Voila-GAZ & weight & 0.24 & 0.20 & -1.52 \\ Voila-GAZ & weight+LORA & 0.23 & 0.21 & -1.02 \\ Voila-GAZ & weight+receiver+cross attention & 0.41 & 0.18 & -0.79 \\   

Table 4: Ablation on Training Procedure, WR means Wining Rate over Otter-base

   Methods & WR & LR & Reward Score \\  Otter-base & - & - & -1.91 \\  GAZ & AS discrete position tokens & 0.19 & 0.25 & -2.44 \\ GAZ & bounding box as image rotation & 0.36 & 0.20 & -1.26 \\ GAZ & bounding box as discrete position tokens & 0.21 & 0.22 & -1.72 \\  Voila(GAZ & AS heatmap) & 0.41 & 0.18 & -0.79 \\   

Table 3: Ablation on Methods of Integrating Gaze Data, WR means Wining Rate over Otter-baseVOILA-COCO dataset, showcasing Voila-A's superior performance in two benchmarks. Our research lays the foundation for more engaging human-AI interactions and encourages further exploration of Voila-A's integration with various modalities and tasks in the realm of multimodal AI systems.

## 6 Acknowledgements

This work was conducted during Kun Yan's internship at Microsoft Research Asia and supported in part by NSFC No. 61925203, U22B2021, NSFC No. 62472244 and No. 62132010.