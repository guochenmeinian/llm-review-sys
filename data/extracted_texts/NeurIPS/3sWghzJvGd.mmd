# Towards Unraveling and Improving Generalization

in World Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

World models have recently emerged as a promising approach for reinforcement learning (RL), as evidenced by its stimulating successes that world model based agents achieve state-of-the-art performance on a wide range of tasks in empirical studies. The primary goal of this study is to obtain a deep understanding of the mysterious generalization capability of world models, based on which we devise new methods to enhance it further. Thus motivated, we develop a stochastic differential equation formulation by treating the world model learning as a stochastic dynamic system in the latent state space, and characterize the impact of latent representation errors on generalization, for both cases with zero-drift representation errors and with non-zero-drift representation errors. Our somewhat surprising findings, based on both theoretic and experimental studies, reveal that for the case with zero drift, modest latent representation errors can in fact function as implicit regularization and hence result in generalization gain. We further propose a Jacobian regularization scheme to mitigate the compounding error propagation effects of non-zero drift, thereby enhancing training stability and generalization. Our experimental results corroborate that this regularization approach not only stabilizes training but also accelerates convergence and improves performance on predictive rollouts.

## 1 Introduction

Model-based reinforcement learning (RL) has emerged as a promising learning paradigm to improve sample efficiency by enabling agents to exploit a learned model for the physical environment. Notably, in recent works [14; 13; 15; 16; 21; 10; 32; 22] on world models, an RL agent learns the latent dynamics model of the environment, based on the observations and action signals, and then optimizes the policy over the learned dynamics model. Different from conventional approaches, world-model based RL takes an _end-to-end learning_ approach, where the building blocks (such as dynamics model, perception and action policy) are trained and optimized to achieve a single overarching goal, offering significant potential to improve generalization capability. For example, DreamerV2 and DreamerV3 achieve great progress in mastering diverse tasks involving continuous and discrete actions, image-based inputs, and both 2D and 3D environments, thereby facilitating robust learning across unseen task domains [14; 13; 15]. Recent empirical studies have also demonstrated the capacity of world models to generalize to unseen states in complex environments, such as autonomous driving . Nevertheless, it remains not well understood when and how world models can generalize well in unseen environments.

In this work, we aim to first obtain a deep understanding of the _generalization_ capability of world models by examining the impact of _latent representation errors_, and then to devise new methods to enhance its generalization. While one may expect that optimizing a latent dynamics model (LDM) prior to training the task policy would minimize latent representation errors and hence can achieve better world model training, our somewhat surprising findings, based on both theoretical and empiricalstudies, reveal that modest latent representation errors in the training phase may in fact be beneficial. In particular, the alternating training strategy for world model learning, which simultaneously refines both the LDM and the action policy, could actually bring generalization gain, because the modest latent representation errors (and the corresponding induced gradient estimation errors) could enable the world model to visit unseen states and thus lead to improved generalization capacities. For instance, as shown in Table 1, our experimental results suggest that moderate batch sizes (e.g., 16 or 32) appear to position the induced errors within a regime conferring notable generalization benefits, leading to higher generalization improvement, when compared to the cases with very small (e.g., 8) or large (e.g., 64) batch sizes.

In a nutshell, _latent representation errors_ incurred by latent encoders, if designed properly, may actually facilitate world model training and enhance generalization. This insight aligns with recent advances in deep learning, where noise injection schemes have been studied as a form of implicit regularization to enhance models' robustness. For instance, recent study  analyzes the effects of introducing isotropic Gaussian noise at each layer of neural networks, identifying it as a form of implicit regularization. Another recent work  explores the addition of zero-drift Brownian motion to RNN architectures, demonstrating its regularizing effects in improving network's stability against noise perturbations.

We caution that _latent representation errors_ in world models differ from the above noise injection schemes (), in the following aspects: 1) Unlike the artificially injected noise only added in training, these errors are inherent in world models, leading to error propagation in the rollouts; 2) Unlike the controlled conditions of isotropic or zero-drift noise examined in prior studies, the errors in world models may not exhibit such well-behaved properties in the sense that the drift may be non-zero and hence biased; 3) additionally, in the iterative training of world models and agents, the error originating from the encoder affects the policy learning and agent exploration. In light of these observations, we develop a continuous-time stochastic differential equation (SDE) formulation by treating the world model learning as a stochastic dynamic system with stochastic latent states. This approach offers an insightful view on model errors as stochastic perturbation, enabling us to obtain an explicit characterization to quantify the impacts of the errors on world models' generalization capability. Our main contributions can be summarized as follows.

* _Latent representation errors as implicit regularization:_ Aiming to understand the generalization capability of world models and improve it further, we develop a continuous-time SDE formulation by treating the world model learning as a stochastic dynamic system in latent state space. Leveraging tools in stochastic calculus and differential geometry, we characterize the impact of latent representation errors on world models' generalization. Our findings reveal that under some technical conditions, modest latent representation errors can in fact function as implicit regularization and hence result in generalization gain.
* _Improving generalization in non-zero drift cases via Jacobian regularization:_ For the case where latent representation errors exhibit non-zero drifts, we show that the additional bias term would degrade the implicit regulation and hence may make the learning unstable. We propose to add Jacobian regularization to mitigate the effects of non-zero-drift errors in training. Experimental studies are carried out to evaluate the efficacy of Jacobian regularization.
* _Reducing error propagation in predictive rollouts:_ We explicitly characterize the effect of latent representation errors on predictive rollouts. Our experimental results corroborate that Jacobian regularization can reduce the impact of error propagation on rollouts, leading to enhanced prediction performance and accelerated convergence in tasks with longer time horizons.
* _Bounding Latent Representation Error:_ We establish a novel bound on the latent representation error within CNN encoder-decoder architectures. To our knowledge, this is the first quantifiable

  batch size & perturbation & \(=10\) & \(=20\) & \(=30\) & \(=25\) & \(=50\) & \(=75\) \\ 
8 & 691.62 & 363.73 & 153.67 & 624.67 & 365.31 & 216.52 \\
16 & **830.39** & 429.62 & **213.78** & **842.26** & **569.42** & **375.61** \\
32 & **869.39** & **436.87** & **312.99** & **912.12** & **776.86** & **655.26** \\
64 & 754.47 & **440.44** & 80.24 & 590.41 & 255.2 & 119.62 \\  

Table 1: Reward values on unseen perturbed states by rotation (\(\)) or mask (\(\%\)) with \((0.15,0.5)\).

bound applied to a learned latent representation model, and the analysis carries over to other architectures (e.g., ReLU) along the same line.

**Notation.** We use Einstein summation convention for succinctness, where \(a_{i}b_{i}\) denotes \(_{i}a_{i}b_{i}\). We denote functions in \(^{k,}\) as being \(k\)-times differentiable with \(\)-Holder continuity. The Euclidean norm of a vector is represented by \(\|\|\), and the Frobenius norm of a matrix by \(||_{F}\); this notation may occasionally extend to tensors. The notation \(x^{i}\) indicates the \(i^{th}\) coordinate of the vector \(x\), and \(A^{ij}\) the \((i,j)\)-entry of the matrix \(A\). Function composition is denoted by \(f g\), implying \(f(g)\). For a differentiable function \(f:^{n}^{m}\), its Jacobian matrix is denoted by \(^{m n}\). Its gradient, following conventional definitions, is denoted by \( f\). The constant \(C\) may represent different values in distinct contexts.

## 2 Related Work

**World model based RL.** World models have demonstrated remarkable efficacy in visual control tasks across various platforms, including Atari  and Minecraft , as detailed in the studies by Hafner et al. [14; 13; 15]. These models typically integrate encoders and memory-augmented neural networks, such as RNNs , to manage the latent dynamics. The use of variational autoencoders (VAE) [7; 23] to map sensory inputs to a compact latent space was pioneered by Ha et al. . Furthermore, the Dreamer algorithm [13; 16] employs convolutional neural networks (CNNs)  to enhance the processing of both hidden states and image embeddings, yielding models with improved predictive capabilities in dynamic environments.

**Continuous-time RNNs.** The continuous-time assumption is standard for theoretical formulations of RNN models. Li et al.  study the optimization dynamics of linear RNNs on memory decay. Chang et al.  propose AntisymmetricRNN, which captures long-term dependencies through the control of eigenvalues in its underlying ODE. Chen et al.  propose the symplectic RNN to model Hamiltonians. As continuous-time formulations can be discretized with Euler methods [4; 5] (or with Euler-Maruyama methods if stochastic in ) and yield similar insights, this step is often eliminated for brevity.

**Implicit regularization by noise injection in RNN.** Studies on noise injection as a form of implicit regularization have gained traction, with Lim et al.  deriving an explicit regularizer under small noise conditions, demonstrating bias towards models with larger margins and more stable dynamics. Camuto et al.  examine Gaussian noise injections at each layer of neural networks. Similarly, Wei et al.  provide analytic insights into the dual effects of dropout techniques.

## 3 Demystifying World Model: A Stochastic Differential Equation Approach

As pointed out in [14; 13; 15; 16], critical to the effectiveness of the world model representation is the stochastic design of its latent dynamics model. The model can be outlined by the following key components: an encoder that compresses high dimensional observations \(s_{t}\) into a low-dimensional latent state \(z_{t}\) (Eq.1), a sequence model that captures temporal dependencies in the environment (Eq.2), a transition predictor that estimates the next latent state (Eq.3), and a latent decoder that reconstructs observed information from the posterior (Eq.4):

\[\ \ z_{t} q_{}(z_{t}\,|\,h_{t},s_{t}),\] (1) \[\ h_{t}=f(h_{t-1},z_{t-1},a_{t-1}),\] (2) \[\ _{t} p(_{t}\,|\,h_{t}),\] (3) \[\ _{t} q_{}(_{t}\,|\,h_{t}, _{t})\] (4)

In this work, we consider a popular class of world models, including Dreamer and PlaNet, where \(\{z,\,,\,\}\) have distributions parameterized by neural networks' outputs, and are Gaussian when the outputs are known. It is worth noting that \(\{z,\,,\,\}\) may not be Gaussian and are non-Gaussian in general. This is because while \(z\) is conditional Gaussian, its mean and variance are random variables which are learned by the encoder with \(s\) and \(h\) being the inputs, rendering that \(z\) is non-Gaussian due to the mixture effect. For this setting, we have a continuous-time formulation where the latent dynamics model can be interpreted as stochastic differential equations (SDEs) with coefficient functions of known inputs. Due to space limitation, we refer to Proposition B.1 in the Appendix for a more detailed treatment.

Consider a complete, filtered probability space \((,\,,\,\{_{t}\}_{t[0,T]},\,\,)\) where independent standard Brownian motions \(B_{t}^{\,},\,B_{t}^{\,},B_{t}^{\,},\,B_{t}^{\, }\) are defined such that \(_{t}\) is their augmented filtration, and \(T\) as the time length of the task environment. We interpret the stochastic dynamics of LDM with latent representation errors through coupled SDEs representing continuous-time analogs of the discrete components:

\[\,d\,z_{t} =(q_{}(h_{t},s_{t})+\,(h_{t},s_{t}))\, dt+(_{}(h_{t},s_{t})+\,(h_{t},s_{t}))\, dB_{t}^{\,},\] (5) \[\,d\,h_{t} =f(h_{t},z_{t},(h_{t},z_{t}))\,dt+(h_{t},z_{t},(h_{t},z_{t}))\,dB_{t}^{\,}\] (6) \[\,d\,_{t} =p(h_{t})\,dt+(h_{t})\,dB_{t}^{\,},\] (7) \[\,d\,_{t} =q_{}(h_{t},_{t})\,dt+_{}(h_{t },_{t})\,dB_{t}^{\,},\] (8)

where \((h,)\) is a policy function as a local maximizer of value function and the stochastic process \(s_{t}\) is \(_{t}\)-adapted. Notice that \(\) is often a zero function indicating that Equation (6) is an ODE, as the sequence model is generally designed as deterministic. Generally, the coefficient functions in \(dt\) and \(dB_{t}\) terms in SDEs are referred to as the _drift_ and _diffusion_ coefficients. Intuitively, the diffusion coefficients here represent the stochastic model components. In Equation (5), \((,)\) and \((,)\) denotes the drift and diffusion coefficients of the _latent representation errors_, respectively. Both are assumed to be functions of hidden states \(h_{t}\) and task states \(s_{t}\). In addition, \(\) indicates the magnitude of the error.

Next, we impose standard assumptions on these SDEs (5) - (8) to guarantee the well-definedness of the solution to SDEs. For further technical details, we refer readers to fundamental works on SDEs in the literature (e.g.,[30; 17]).

**Assumption 3.1**.: The drift coefficient functions \(q_{},\,f,\,p\) and \(q_{}\) and the diffusion coefficient functions \(_{},\,\) and \(_{}\) are bounded and Borel-measurable over the interval \([0,T]\), and of class \(^{3}\) with bounded Lipschitz continuous partial derivatives. The initial values \(z_{0},h_{0},_{0},_{0}\) are square-integrable random variables.

**Assumption 3.2**.: \(\) and \(\) are bounded and Borel-measurable and are of class \(^{3}\) with bounded Lipschitz continuous partial derivatives over the interval \([0,T]\).

### Latent Representation Errors in CNN Encoder-Decoder Networks

As shown in the empirical studies with different batch sizes (Table 1), the latent representation error would also enrich generalization when it is within a moderate regime. In this section, we show that the latent representation error, in the form of approximation error corresponding to widely used CNN encoder-decoder, could be made sufficiently small by finding appropriate CNN network configuration. In particular, this result provides theoretical justification to interpreting latent representation error as stochastic perturbation in the dynamical system defined in Equations (5 - 8), as the error magnitude \(\) can be made sufficiently small by CNN network configuration.

Consider the state space \(^{d_{}}\) and the latent space \(\). Consider a state probability measure \(Q\) on the state space \(\) and a probability measure \(P\) on the latent space \(\). As high-dimensional state space in image-based tasks frequently exhibit _intrinsic lower-dimensional geometric structure_, we adopt the latent manifold assumption, formally stated as follows:

**Assumption 3.3**.: (Latent manifold assumption) For a positive integer \(k\), there exists a \(d_{}\)-dimensional \(^{k,}\) submanifold \(\) (with \(^{k+3,}\) boundary) with Riemannian metric \(g\) and has positive reach and also isometrically embedded in the state space \(^{d_{}}\) and \(d_{}\)\(d_{}\), where the state probability measure is supported on. In addition, \(\) is a compact, orientable, connected manifold.

**Assumption 3.4**.: (Smoothness of state probability measure) \(Q\) is a probability measure supported on \(\) with its Radon-Nikodym derivative \(q^{k,}(,)\) w.r.t \(_{}\).

Let \(\) be a closed ball in \(^{d_{}}\), that is \(\{x^{d_{}}\,:\,\|x\| 1\,\}\). \(P\) is a probability measure supported on \(\) with its Radon-Nikodym derivative \(p^{k,}(,)\) w.r.t \(_{}\). In practice, it is usually an easy-to-sample distribution such as uniform distribution which is determined by a specific encoder-decoder architecture choice.

**Latent Representation Learning**.: We define the _latent representation learning_ as to find encoder \(g_{}:\) and decoder \(g_{}:\) as maps that optimize the following objectives:

\[_{g_{}}W_{1}(g_{_{}}\,Q,\,P);_{g_{}}W_{1}(Q,\,g_{_{}}\,P).\]Here, \(g_{_{\#}}\,Q\) and \(g_{_{\#}}\,P\) represent the pushforward measures of \(Q\) and \(P\) through the encoder map \(g_{}\) and decoder map \(g_{}\), respectively. The latent representation error is understood as the "difference" of pushforward measure by the encoder/decoder and target measure. Here, _to understand the "scale" of the error \(\) in Equation (5), we use \(W_{1}\) for the discrepancy between probability measures._ In particular, for Dreamer-type loss function that uses KL-divergence, we note that squared \(W_{1}\) distance between two probability measures can be upper bounded by their KL-divergence up to a constant , implying that one could reasonably expect the \(W_{1}\) distance to also decrease when KL-divergence is used in the model.

**CNN configuration.** As a popular choice choice in encoder-decoder architecture is CNN, we consider a general CNN function \(f_{}:\). Let \(f_{}\) have \(L\) hidden layers, represented as: for \(x,f_{}(x):=A_{L+1} A_{L} A_{2}  A_{1}(x),\) where \(A_{i}\)'s are either convolutional or downsampling operators. For convolutional layers, \(A_{i}(x)=(W_{i}^{c}x+b_{i}^{c}),\) where \(W_{i}^{c}^{d_{i} d_{i-1}}\) is a structured sparse Toeplitz matrix from the convolutional filter \(\{w_{i}^{(i)}\}_{j=0}^{s(i)}\) with filter length \(s(i)_{+},\,b_{i}^{c}^{d_{i}}\) is a bias vector, and \(\) is the ReLU activation function. For downsampling layers, \(A_{i}(x)=D_{i}(x)=(x_{jm_{i}})_{j=1}^{ d_{i-1}/m_{i}},\) where \(D_{i}:^{d_{i} d_{i-1}}\) is the downsampling operator with scaling parameter \(m_{i} d_{i-1}\) in the \(i\)-th layer. We examine the class of functions represented by CNNs, denoted by \(_{}\), defined as:

\[_{}=\{f_{}A_{i},\,i=1,,L+1\}.\]

For the specific definition of \(_{}\), we refer to 's (4), (5) and (6).

**Assumption 3.5**.: Assume that \(\) and \(\) are locally diffeomorphic, that is there exists a map \(F:\) such that at every point \(x\) on \(,\ (d\,F(x)) 0\).

**Theorem 3.6**.: _(Approximation Error of Latent Representation). Under Assumption 3.3, 3.4 and 3.5, for \((0,1),\) let \(d_{}:=(d_{}^{-2})\). For positive integers \(M\) and \(N\), there exists an encoder \(g_{}\) and decoder \(g_{}_{}(L,S,W)\) s.t._

\[W_{1}(g_{_{\#}}Q,P) d_{}C(NM)^{-}}, W_{1}(g_{_{\#}}P,Q) d_{}C(NM)^{- {2(k+1)}{d_{}}}.\]

Theorem 3.6 indicates that with an appropriate CNN configuration, the \(W_{1}\) approximation error can be made to reside in a small region, as the best candidate within the function class is indeed capable of approximating the oracle encoder/decoder. In particular, this result indicates that the error magnitude \(\) in SDE (5) can be assumed to be small. This allows us to apply the perturbation analysis of the dynamical system defined in Equations (5 - 8) in the following sections.

### Latent Representation Errors as Implicit Regularization towards Generalization

In this section, we investigate the impact of latent representation errors on generalization, for the two cases with _zero drift_ and _non-zero drift_, respectively. We show that under mild conditions, the _zero-drift_ errors can function as a natural form of _implicit regularization_, promoting wider landscapes for improved robustness. Nevertheless, we caution that when latent representation errors have non-zero drift, it could lead to poor regularization with _unstable bias_ and degrade world model's generalization, calling for explicit regularization.

To simplify the notation here, we consider the system equations, specifically Equations (5), (6) - (8), as one stochastic system. Let \(x_{t}=(z_{t},h_{t},_{t},_{t})\) and \(B_{t}=(B_{t}^{},B_{t}^{},B_{t}^{},B_{t}^{})\):

\[d\,x_{t}=(g(x_{t},t)+\,(x_{t},t))\ dt+_{i}_{i}(x_{t}, t)+\,_{i}(x_{t},t)\,dB_{t}^{i},\] (9)

where \(g\), and \(_{i}\) are structured accordingly for the respective components, employing the Einstein summation convention for concise representation. For abuse of notation, \(=(,0,0,0),=(,0,0,0)\). For a given error magnitude \(\), we denote the solution to SDE (9) as \(x_{t}^{}\). Intuitively, \(x_{t}^{}\) is the perturbed trajectory of the latent dynamics model. In particular, when \(=0\), indicating that the absence of latent representation error in the model, the solution is denoted as \(x_{t}^{0}\).

#### 3.2.1 The Case with Zero-drift Representation Errors

When the drift coefficient \(=0\), the latent representation errors correspond to a class of well-behaved stochastic processes. The following result translates the induced perturbation on the stochastic latent dynamics model's loss function \(\) to a form of explicit regularization. We assume that \(^{2}\) and depends on \(z_{t},h_{t},_{t},_{t}\). Loss functions used in practical implementation, e.g. in DreamerV3, reconstruction loss \(J_{O}\), reward loss \(J_{R}\), consistency loss \(J_{D}\), all satisfy this condition.

**Theorem 3.7**.: _(Explicit Effect Induced by Zero-Drift Representation Error) Under Assumptions 3.1 and 3.2 and considering a loss function \(^{2}\), the explicit effects of the zero-drift error can be marginalized out as follows: as \( 0\),_

\[\,(x_{t}^{})=\, (x_{t}^{0})++(^{3}),\] (10)

_where the regularization term \(\) is given by \(:=\,+^{2}(+ \,),\) with_

\[:= \,\,(x_{t}^{0})^{}_{t}_{ k}_{t}^{k},\] (11) \[:= \,_{k_{1},k_{2}}(_{t}_{t}^{k_{1}})^{ 2}(x_{t}^{0},t)\,(_{t}_{t}^{k_{2}})^{j},\] (12) \[:= \,\,(x_{t}^{0})^{}_{t}_{0 }^{t}_{s}^{-1}\,^{k}(x_{s}^{0},s)dB_{t}^{k}.\] (13)

_Square matrix \(_{t}\) is the stochastic fundamental matrix of the corresponding homogeneous equation:_

\[d_{t}=_{k}}{ x}(x_{t}^{0},t)\,_{t}\,dB_{ t}^{k},(0)=I,\]

_and \(_{t}^{k}\) is the shorthand for \(_{0}^{t}_{s}^{-1}_{k}(x_{s}^{0},s)dB_{t}^{k}\). Additionally, \(^{k}(x_{s}^{0},s)\) is represented by for \(_{k_{1},k_{2}}_{k}}{ x^{i} x_{s} }(x_{s}^{0},s)(_{s}^{k_{1}})^{i}(_{s}^{k_{2}})^{j}\)._

The proof is relegated to Appendix B in the Supplementary Materials.

When the loss \(\) is convex, then its Hessian, \(^{2}\), is positive semi-definite, which ensures that the term \(\) is non-negative. _The presence of this Hessian-dependent term \(\), under latent representation error, implies a tendency towards wider minima in the loss landscape._ Empirical results from  indicates that wider minima correlate with improved robustness of implicit regularization during training. This observation also aligns with the theoretical insights in  that the introduction of Brownian motion, which is indeed zero-drift by definition, in training RNN models promotes robustness. We note that in addition, when the error \(_{t}()\) is too small, the effect of term \(\) as implicit regularization would not be as significant as desired. Intuitively, this insight resonates with the empirical results in Table 1 that model's robustness gain is not significant when the error induced by small batch sizes is too small.

We remark that the exact loss form treated here is simplified compared to that in the practical implementation of world models, which frequently depends on the probability density functions (PDFs) of \(z_{t},h_{t},_{t},_{t}\). In principle, the PDE formulation corresponding to the PDFs of the perturbed \(x_{t}^{}\) can be derived from the Kolmogorov equation of the SDE (9), and the technicality is more involved but can offer more direct insight. We will study this in future work.

#### 3.2.2 The Case with Non-Zero-Drift Representation Errors

In practice, latent representation errors may not always exhibit _zero drift_ as in idealized noise-injection schemes for deep learning (, ). When the drift coefficient \(\) is non-zero or a function of input data \(h_{t}\) and \(s_{t}\) in general, the explicit regularization terms induced by the latent representation error may lead to unstable bias in addition to the regularization term \(\) in Theorem 3.7. With a slight abuse of notation, we denote \(_{0}\) as \(g\) from Equation (9) for convenience.

**Corollary 3.8**.: _(Additional Bias Induced by Non-Zero Drift Representation Error) Under Assumptions 3.1 and 3.2 and considering a loss function \(^{2}\), the explicit effects of the general form error can be marginalized out as follows as \( 0\):_

\[\,(x_{t}^{})=\,( x_{t}^{0})++}+(^{3}),\] (14)

_where the additional bias term \(}\) is given by \(}:=\,\,}+^{2}( }+})\), with_

\[}:= \,\,(x_{t}^{0})^{}_{t}\, _{t},\] (15) \[}:= \,\,(x_{t}^{0})^{}_{t}_{ 0}^{t}_{s}^{-1}\,^{0}(x_{s}^{0},s)\,dt,\] (16) \[}:= \,_{k}(_{t}_{t})^{}^{2} (x_{t}^{0},t)\,(_{t}_{t}^{k})^{j},\] (17)and \(_{t}\) being the shorthand for \(_{0}^{t}_{s}^{-1}_{k}(x_{s}^{0},s)dt\)._

The presence of the new bias term \(}\) implies that regularization effects of latent representation error could be unstable. The presence of \(\) in \(}\), \(}\) and \(}\) induces a bias to the loss function with its magnitude dependent on the error level \(\), since \(\) is a non-zero term influenced on the drift term \(\). This contrasts with the scenarios described in  and , where the noise injected for implicit regularization follows a zero-mean Gaussian distribution. To modulate the regularization and bias terms \(\) and \(}\) respectively, we note that a common factor, the fundamental matrix \(\), can be bounded by

\[_{}\|_{t}\|_{F}^{2}_{k}C (C\,_{}\|}{ x}(x_ {}^{0},t)\|_{F}^{2})\] (18)

which can be shown by using the Burkholder-Davis-Gundy Inequality and Gronwall's Lemma. Based on this observation, we next propose a regularizer on input-output Jacobian norm \(\|}{ x}\|_{F}\) that could modulate the new bias term \(}\) for stabilized implicit regularization.

## 4 Enhancing Predictive Rollouts via Jacobian Regularization

In this section, we study the effects of latent representation errors on predictive rollouts using latent state transitions, which happen in the inference phase in world models. We then propose to use Jacobian regularization to enhance the quality of rollouts. In particular, we first obtain an upper bound of state trajectory divergence in the rollout due to the representation error. We show that the error effects on task policy's \(Q\) function can be controlled through model's input-output Jacobian norm.

In world model learning, the task policy is optimized over the rollouts of dynamics model with the initial latent state \(z_{0}\). Recall that latent representation error is introduced to \(z_{0}\) when latent encoder encodes the initial state \(s_{0}\) from task environment. Intuitively, the latent representation error would propagate under the sequence model and impact the policy learning, which would then affect the generalization capacity through increased exploration.

Recall that the sequence model and the transition predictor are given as follows:

\[d\,h_{t}=f(h_{t},_{t},(h_{t},_{t}))\,dt,  d\,_{t}=p(h_{t})dt+(h_{t})\,dB_{t},\] (19)

with random variables \(h_{0}\), \(_{0}+\) as the initial values, respectively. In particular, \(\) is a random variable of proper dimension, representing the error from encoder introduced at the initial step. We impose the standard assumption on the error to ensure the well-definedness of the SDEs.

Under Assumption 3.1, there exists a unique solution to the SDEs (for Equations 19 with square-integrable \(\)), denoted as \((h_{t}^{},z_{t}^{})\). In the case of no error introduced, i.e., \(=0\), we denote the solution of the SDEs as \((h_{t}^{0},z_{t}^{0})\) understood as the rollout under the absence of latent representation error. To understand how to modulate impacts of the error in rollouts, our following result gives an upper bound on the expected divergence between the perturbed rollout trajectory \((h_{t}^{},z_{t}^{})\) and the original \((h_{t}^{0},z_{t}^{0})\) over the interval \([0,T]\).

**Theorem 4.1**.: _(Bounding trajectory divergence) For a square-integrable random variable \(\), let \(:=\,\|\|\) and \(d_{}:=_{t[0,T]}\|h_{t}^{}-h_{t}^{0 }\|^{2}+\|_{t}^{}-_{t}^{0}\|^{2}\). As \( 0\),_

\[d_{}\,\,C(_{0}+_{1} )+\,^{2}\,C(\,_{0}(_{0}+ _{1}))+^{2}\,C(\,_{1}( _{0}+_{1}))+(^{3}),\]

_where \(C\) is a constant dependent on T. \(_{1}\) and \(_{2}\) are Jacobian-related terms, and \(_{1}\) and \(_{2}\) are Hessian-related terms._

The Jacobian-related terms \(_{1}\) and \(_{2}\) are defined as \(_{0}:=(_{h}+_{z}+_{h} ),\,_{1}:=(}_{h})\); the Hessian-related terms \(_{0}\) and \(_{1}\) are defined as \(_{0}:=_{hh}+_{hz}+_{zh}+ _{zz}+_{hh},_{1}:=}_{hh}\), where \(_{h}\), \(_{z}\) are the expected \(\) Frobenius norm of Jacobians of \(f\) w.r.t \(h\), \(z\), respectively, and \(_{hh},_{hz},_{zh},_{zz}\) are the corresponding expected \(\) Frobenius norm of second-order derivatives. Other terms are similarly defined. A detailed description of all terms, can be found in Appendix C.1.

Theorem 4.1 correlates with the empirical findings in  regarding the diminished predictive accuracy of latent states \(_{t}\) over the extended horizons. In particular, Theorem 4.1 suggests that the expected divergence from error accumulation hinges on the expected error magnitude, the Jacobian norms within the latent dynamics model and the horizon length \(T\).

Our next result reveals how initial latent representation error influences the value function \(Q\) during the prediction rollouts, which again verifies that the perturbation is dependent on expected error magnitude, the model's Jacobian norms and the horizon length \(T\):

**Corollary 4.2**.: _For a square-integrable \(\), let \(x_{t}:=(h_{t},z_{t})\). Then, for any action \(a\), the following holds for value function \(Q\) almost surely:_

\[Q(x_{t}^{},a)= Q(x_{t}^{0},a)+Q(x_{t}^{0},a)( ^{i}_{i}\,x_{t}^{0}+^{i}\, ^{j}\,_{ij}^{2}\,x_{t}^{0})\] \[+(^{i}\,_{i}\,x_{t}^{0})^{} }{ x^{2}}Q(x_{t}^{0},a)(^{i}\, _{i}\,x_{t}^{0})+(^{3}),\]

_as \( 0\), where stochastic processes \(_{i}\,x_{t}^{0}\), \(_{ij}^{2}\,x_{t}^{0}\) are the first and second derivatives of \(x_{t}^{0}\) w.r.t \(\) and are bounded as follows:_

\[_{t[0,T]}\|_{i}\,x_{t}^{0}\| C( _{0}+_{1}),\,_{t[0,T]}\| _{ij}^{2}\,x_{t}^{0}\| C(\,_{0}( _{0}+_{1}))+C(\,_{1} (_{0}+_{1})).\]

This corollary reveals that latent representation errors implicitly encourage exploration of unseen states by inducing a stochastic perturbation in the value function, which again can be regularized through a controlled Jacobian norm.

**Jacobian Regularization against Non-Zero Drift.** The above theoretical results have established a close connection of input-output Jacobian matrices with the stabilized generalization capacity of world models (shown in 18 under non-zero drift form), and perturbation magnitude in predictive rollouts (indicated in the presence of Jacobian terms in Theorem 4.1 and Corollary 4.2.) Based on this, we propose a regularizer on input-output Jacobian norm \(\|\,}{ x}\|_{F}\) that could modulate \(\) ( and in addition \(_{k}\)) for stabilized implicit regularization.

The regularized loss function for LDM is defined as follows:

\[}_{}=_{}+\,\|J_{ }\|_{F},\] (20)

where \(_{}\) is the original loss function for dynamics model, \(J_{}\) denotes the data-dependent Jacobian matrix associated with the \(\)-parameterized dynamics model, and \(\) is the regularization weight. Our empirical results in 5 with an emphasis on sequential case align with the experimental findings from  that Jacobian regularization can enhance robustness against random and adversarial input perturbation in machine learning models.

## 5 Experimental Studies

In this section, experiments are carried out over a number of tasks in Mujoco environments. Due to space limitation, implementation details and additional results, including the standard deviation of the trials, are relegated to Section D in the Appendix.

**Enhanced generalization to unseen noisy states.** We investigated the effectiveness of Jacobian regularization in model trained against a vanilla model during the inference phase with perturbed state images. We consider three types of perturbations: (1) Gaussian noise across the full image, denoted as \((_{1},_{1}^{2})\) ; (2) rotation; and (3) noise applied to a percentage of the image, \((_{2},_{2}^{2})\). (In Walker task, \(_{1}=_{2}=0.5,_{2}^{2}=0.15\); in Quadruped task, \(_{1}=0,_{2}=0.05,_{2}^{2}=0.2\).) In each case of perturbations, we examine a collection of noise levels: (1) variance \(^{2}\) from \(0.05\) to \(0.55\); (2) rotation degree \(\)\(20\) and \(30\); and (3) masked image percentage \(\%\) from \(25\) to \(75\).

Figure 1: Generalization against increasing degree of perturbation.

It can be seen from Table 3 and Figure 1 that thanks to the adoption of Jacobian regularization in training, the rewards (averaged over 5 trials) are higher compared to the baseline, indicating improved generalization to unseen image states in all cases. The experimental results corroborate the findings in Corollary 3.8 that the regularized Jacobian norm could stabilize the induced implicit regularization.

**Robustness against encoder errors.** Next, we focus on the effects of Jacobian regularization on controlling the error process to the latent states \(z\) during training. Since it is very challenging, if not impossible, to characterize the latent representation errors and hence the drift therein explicitly, we consider to evaluate the robustness against two exogenous error signals, namely (1) zero-drift error with \(_{t}=0,_{t}^{2}\) (\(_{t}^{2}=5\) in Walker, \(_{t}^{2}=0.1\) in Quadruped), and (2) non-zero-drift error with \(_{t},_{t}^{2}\) uniformly. Table 3 shows that the model with regularization can consistently learn policies with high returns and also converges faster, compared to the vanilla case. This corroborates our theoretical findings in Corollary 3.8 that the impacts of error to loss \(\) can be controlled through the model's Jacobian norm.

**Faster convergence on tasks with extended horizon.** We further evaluate the efficacy of Jacobian regularization in tasks with extended horizon, particularly by extending the horizon length in MuJoCo Walker from \(50\) to \(100\) steps. Table 4 shows that the model with regularization converges significantly faster (\(\) 100K steps) than the case without Jacobian regularization in training. This corroborates results in Theorem 4.1 that regularizing the Jacobian norm can reduce error propagation.

## 6 Conclusion

In this study, we investigate the impacts of latent representation errors on the generalization capacity of world models. We utilize a stochastic differential equation formulation to characterize the effects of latent representation errors as implicit regularization, for both cases with zero-drift errors and with non-zero drift errors. We develop a Jacobian regularization scheme to address the compounding effects of non-zero drift, thereby enhancing training stability and generalization. Our empirical findings validate that Jacobian regularization improves the generalization performance, expanding the applicability of world models in complex, real-world scenarios. Future research is needed to investigate how stabilizing latent errors can enhance generalization across more sophisticated tasks for general non-zero drift cases.

The broader social impact of our work resides in its potential to enhance the robustness and reliability of RL agents deployed in real-world applications. By improving the generalization capacities of world models, our work could contribute to the development of RL agents that perform consistently across diverse and unseen environments. This is particularly relevant in safety-critical domains such as autonomous driving, where reliable agents can provide intelligent and trustworthy decision-making.

    & Walker 100 len (increased from original 50 len) \\  Num steps & 100k & 200k & 280k \\  With Jacobian (\(=0.05\)) & **639.1** & **936.3** & 911.1 \\ With Jacobian (\(=0.1\)) & 537.5 & 762.6 & **927.7** \\ Baseline & 582.3 & 571.2 & 886.6 \\   

Table 4: Accumulated rewards of Walker with extended horizon.

    & & (_{1},\,_{1}^{2})\)} & \)} & (_{2},\,_{2}^{2})\)} \\   & clean & \(_{t}^{2}=0.35\) & \(_{t}^{2}=0.5\) & \(=20\) & \(=30\) & \(=50\) & \(=75\) \\  With Jacobian (Walker) & **967.12** & **742.32** & **618.98** & **423.81** & **226.04** & **725.81** & **685.49** \\ Baseline (Walker) & 966.53 & 615.79 & 333.47 & 391.65 & 197.53 & 583.41 & 446.74 \\ With Jacobian (Quad) & **971.98** & **269.78** & **242.15** & **787.63** & **610.53** & **321.55** & **304.92** \\ Baseline (Quad) & 967.91 & 207.33 & 194.08 & 681.03 & 389.41 & 222.22 & 169.58 \\   

Table 2: Evaluation on unseen states by various perturbation (Clean means without perturbation). \(=0.01\).

    & &  &  &  &  \\   & 300k & 600k & 300k & 600k & 600k & 1.2M & 1M & 2M \\  With Jacobian & **666.2** & **966** & **905.7** & **912.4** & **439.8** & **889** & **348.3** & **958.7** \\ Baseline & 24.5 & 43.1 & 404.6 & 495 & 293.6 & 475.9 & 48.98 & 32.87 \\   

Table 3: Accumulated rewards under additional encoder errors. \(=0.01\).