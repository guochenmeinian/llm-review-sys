# DiffPack: A Torsional Diffusion Model for Autoregressive Protein Side-Chain Packing

Yangtian Zhang\({}^{1,2}\)   Zuobai Zhang\({}^{1,2}\)   Bozitao Zhong\({}^{1,2}\)

**Sanchit Misra\({}^{3}\)   Jian Tang\({}^{1,4,5}\)**

\({}^{*}\) equal contribution  \({}^{}\)corresponding author

\({}^{1}\)Mila - Quebec AI Institute  \({}^{2}\)Universite de Montreal  \({}^{3}\)Intel

\({}^{4}\)HEC Montreal \({}^{5}\)CIFAR AI Research Chair

**contacts:**\(<\)yangtian.zhang, zuobai.zhang\(>\)@mila.quebec, jian.tang@hec.ca

###### Abstract

Proteins play a critical role in carrying out biological functions, and their 3D structures are essential in determining their functions. Accurately predicting the conformation of protein side-chains given their backbones is important for applications in protein structure prediction, design and protein-protein interactions. Traditional methods are computationally intensive and have limited accuracy, while existing machine learning methods treat the problem as a regression task and overlook the restrictions imposed by the constant covalent bond lengths and angles. In this work, we present **DiffPack**, a torsional diffusion model that learns the joint distribution of side-chain torsional angles, the only degrees of freedom in side-chain packing, by diffusing and denoising on the torsional space. To avoid issues arising from simultaneous perturbation of all four torsional angles, we propose autoregressively generating the four torsional angles from \(_{1}\) to \(_{4}\) and training diffusion models for each torsional angle. We evaluate the method on several benchmarks for protein side-chain packing and show that our method achieves improvements of \(11.9\%\) and \(13.5\%\) in angle accuracy on CASP13 and CASP14, respectively, with a significantly smaller model size (\(60\) fewer parameters). Additionally, we show the effectiveness of our method in enhancing side-chain predictions in the AlphaFold2 model. Code is available at https://github.com/DeepGraphLearning/DiffPack.

## 1 Introduction

Proteins are crucial for performing a diverse range of biological functions, such as catalysis, signaling, and structural support. Their three-dimensional structures, determined by amino acid arrangement, are crucial for their function. Specifically, amino acid side-chains play a critical role in the stability and specificity of protein structures by forming hydrogen bonds, hydrophobic interactions, and other non-covalent interactions with other side-chains or the protein backbone. Therefore, accurately predicting protein side-chain conformation is an essential problem in protein structure prediction [16; 15; 11], design [48; 54; 14; 65] and protein-protein interactions [64; 24].

Despite recent advancements in deep learning models inspired by AlphaFold2 for predicting the positions of protein backbone atoms [32; 4], predicting the conformation of protein side-chains remains a challenging problem due to the complex interactions between side chains. In this work, we focus on the problem of predicting side-chain conformation with fixed backbone structure, _a.k.a._, protein side-chain packing. Traditional methods for side-chain prediction rely on techniques such as rotamer libraries, energy functions, and Monte Carlo sampling [27; 73; 37; 3; 1; 35; 71; 7]. However, these methods are computationally intensive and struggle to accurately capture the complex energy landscape of protein side chains due to their reliance on search heuristics and discrete sampling.

Several machine learning methods have been proposed for side-chain prediction, including DLPacker , AttnPacker , and others [47; 69; 70; 73; 39; 71]. DLPacker, the first deep learning-based model, employs a 3D convolution network to learn the density map of side-chain atoms, but it lacks the ability to capture rotation equivariance due to its 3D-CNN structure. In contrast, AttnPacker, the current state-of-the-art model, directly predicts side-chain atom coordinates using Tensor Field Network and SE(3)-Transformer, ensuring rotation equivariance in side chain packing. However, it does not consider the restrictions imposed by covalent bond lengths and angles, leading to inefficient training and unnatural bond lengths during generation. Furthermore, previous methods that treat side-chain packing as a regression problem assume a single ground-truth side-chain structure and overlook the fact that proteins can fold into diverse structures under different environmental factors, resulting in a distribution of side-chain conformations.

In this study, we depart from the standard practice of focusing on atom-level coordinates in Cartesian space as in prior research [45; 46]. Instead, we introduce **DiffPack**, a torsional diffusion model that models the exact degree of freedom in side-chain packing, the joint distribution of four torsional angles. By perturbing and denoising in the torsional space, we use an SE(3)-invariant network to learn the gradient field for the joint distribution of torsional angles. This result in a much smaller conformation space of side-chain, thereby capturing the intricate energy landscape of protein side chains. Despite its effectiveness, a direct joint diffusion process on the four torsion angles could result in steric clashes and accumulative coordinate displacement, which complicates the denoising process. To address this, we propose an autoregressive diffusion process and train separate diffusion models to generate the four torsion angles from \(_{1}\) to \(_{4}\) in an autoregressive manner. During training, each diffusion model only requires perturbation on its corresponding torsional angle using a teacher-forcing strategy, preserving the protein structure and avoiding the aforementioned issues. To improve the capacity of our model, we further introduce three schemes in sampling for consistently improving the inference results: multi-round sampling, annealed temperature sampling, and confidence models.

We evaluate our method on several benchmarks for protein side-chain packing and compare it with existing state-of-the-art methods. Our results demonstrate that DiffPack outperforms existing state-of-the-art approaches, achieving remarkable improvements of \(11.9\%\) and \(13.5\%\) in angle accuracy on CASP13 and CASP14, respectively. Remarkably, these performance gains are achieved with a significantly smaller model size, approximately 60 times fewer parameters, highlighting the potential of autoregressive diffusion models in protein structure prediction. Furthermore, we showcase the effectiveness of our method in enhancing the accuracy of side-chain predictions in the AlphaFold2 model, indicating its complementary nature to existing approaches.

## 2 Background

**Protein.** Proteins are composed of a sequence of residues (amino acids), each containing an alpha carbon (C\({}_{}\)) atom bounded to an amino group (-NH\({}_{2}\)), a carboxyl group (-COOH) and a side-chain (-R) that identifies the residue type. Peptide bonds link consecutive residues through a dehydration synthesis process. The backbone of a protein consists of the C\(\) atom and the connected nitrogen, carbon, and oxygen atoms. We use \(=[s_{1},s_{2},...,s_{n}]\) to denote the sequence of a protein with \(n\) residues, where \(s_{i}\{0,...,19\}\) denotes the type of the \(i\)-th residue.

**Protein Conformation.** Physical interactions between residues make a protein fold into its native 3D structure, _a.k.a._, conformation, which determines its biologically functional activity. We use \(=[x_{1},x_{2},...,x_{n}]\) to denote the structure of a protein, where \(x_{i}\) denotes the set of atom coordinates belonging to the \(i\)-th residue. The backbone structure \(x_{i}^{}\) of the \(i\)-th residue is a subset consisting of backbone atoms, _i.e._, C\({}_{}\), N, C, O, while the side-chain consists of the remaining atoms \(x_{i}^{}=x_{i} x_{i}^{}\). The backbone conformation \(^{}\) and side-chain conformation \(^{}\) of the protein are defined as the set of backbones and side-chains of all residues, respectively.

**Protein Side-Chain Packing.** Protein side-chain packing (PSCP) problem aims to predict the 3D coordinates \(^{}\) of side-chain atoms given backbone conformations \(^{}\) and protein sequence \(\). That is, we aim to model the conditional distribution \(p(^{}|^{},)\).

## 3 Methods

In this paper, we introduce an autoregressive diffusion model DiffPack to predict the side-chain conformation distribution in torsion space. We address the issue of overparameterization by introducing a torsion-based formulation of side-chain conformation in Section 3.1. Then we give a formulation of the torsional diffusion model in Section 3.2. However, directly applying the diffusion model encounters challenges in learning the joint distribution, which we address by introducing an autoregressive-style model in Section 3.3. We then provide details of the model architecture in Section 3.4, followed by an explanation of the inference procedure in Section 3.5.

### Modeling Side-Chain Conformations with Torsional Angles

Previous methods [45; 46] model the side chain conformation as a series of three-dimensional coordinates in the Cartesian space. However, this approach does not take into account the restrictions imposed by constant covalent bond lengths and angles on the side chain's degree of freedom, resulting in inefficient training and unnatural bond lengths during generation.

To overcome this overparameterization issue, we propose modeling side chain conformation in torsion space. As illustrated in Figure 2, torsional angles directly dictate protein side-chain conformation by determining the twist between two neighboring planes. Table 6 lists the atom groups to define corresponding the neighboring plane for each residue type. The number of torsional angles varies across different residues, with a maximum of four (\(_{1}\), \(_{2}\), \(_{3}\), \(_{4}\)). Modeling side chains in torsion space reduces the number of variables by approximately one third and restricts degrees of freedom . Formally, we transform the problem of side-chain packing to modeling the distribution of torsional angles:

\[p(^{()}|^{()},) p (_{1},_{2},_{3},_{4}^{()},),\] (1)

Figure 1: Overview of DiffPack. Given a protein sequence and backbone structure, we aim to model the conditional distribution of side-chain conformation. **(A)** Distribution of side-chain conformation is modeled through diffusion process in torsion space \(^{m}\). An SE(3)-invariant network is used to learn the torus force field (torsion score). **(B)** Four torsion angles are generated autoregressively across all residues.

Figure 2: Illustration of four torsion of four torsional angles.

where \(_{i}[0,2)^{m_{i}}\) is a vector of the \(i\)-th torsional angles of all residues in the protein and we use \(m_{i}\) to denote the number of residues with \(_{i}\). The space of possible side chain conformation is, therefore, reduced to an \(m\)-dimension sub-manifold \(^{3n}\) with \(m=m_{1}+m_{2}+m_{3}+m_{4}\).

### Diffusion Models on Torsional Space

Denoising diffusion probabilistic models are generative models that learn the data distribution via a _forward diffusion process_ and a _reverse generation process_. We follow  to define diffusion models on the torsional space with the continuous score-based framework in . For simplicity, we omit the condition and aggregate torsional angles on all residues as a torsional vector \([0,2)^{m}\). Starting with the data distribution as \(p_{0}()\), the _forward diffusion process_ is modeled by a stochastic differential equation (SDE):

\[d=(,t)\,dt+g(t)\,d,\] (2)

where \(\) is the Wiener process on the torsion space and \(f(,t),g(t)\) are the drift coefficient and diffusion coefficient, respectively. Here we adopt Variance-Exploding SDE where \(f(,t)=0\) and \(g(t)\) is exponentially decayed with \(t\). With sufficiently large \(T\), the distribution \(p_{T}()\) approaches a uniform distribution over the torsion space. The _reverse generation process_ samples from the prior and generates samples from the data distribution \(p_{0}()\) via approximately solving the reverse SDE:

\[d=[(_{t},t)-g^{2}(t)_{} p_{t} (_{t})]\,dt+g(t)\,d,\] (3)

where a neural network is learned to fit the score \(_{} p_{t}(_{t})\) of the diffused data [23; 57]. Inspired by , we convert the torsional angles into 3D atom coordinates and define the score network on Euclidean space, enabling it to explicitly learn the interatomic interactions.

The training process involves sampling from the perturbation kernel of the forward diffusion and computing its score to train the score network. Given the equivalence \((_{1},...,_{m})(_{1}+2,...,_{m})...(_{1},..., _{m}+2)\) in torsion space, the perturbation kernel is a wrapped Gaussian distribution on \(^{m}\). This means that any \(,^{}[0,2)^{m}\), the perturbation kernel is proportional to the sum of exponential terms, which depends on the distance between \(\) and \(^{}\):

\[p_{t|0}(^{}|)_{^{m}} (--^{}+2\|^{2}}{2^{2} (t)}),\] (4)

In order to sample from the perturbation kernel, we sample from the corresponding unwrapped isotropic normal and take element-wise \( 2\). The kernel's scores are pre-computed using a numerical approximation. During training, we uniformly sampled a time step \(t\) and the denoising score matching loss is minimized:

\[J_{}()=_{t}[(t)_{_{ 0} p_{0},_{t} p_{t|0}(|_{0})}[\|( _{t},t)-_{t} p_{t|0}(_{t}|_{0})\|^{2} ]],\] (5)

where the weight factors \((t)=1/_{ p_{t|0}(|0)}[\|_{}  p_{t|0}(|)\|^{2}]\) are also pre-computed.

### Autoregressive Diffusion Models

The direct approach to torsional diffusion models introduces noise on all torsional angles simultaneously and poses two significant challenges:

**1. Cumulative coordinate displacement**: Adding noises to torsional angles is equivalent with rotating side-chain atoms in and beyond the corresponding atom groups. For instance, if \(_{1}\) is rotated, it affects the coordinates of atoms in the \(_{2}\), \(_{3}\), and \(_{4}\) groups. This cumulative effect complicates the denoising process of the latter three angles. Similar issues arise after rotating \(_{2}\) and \(_{3}\). The effect of rotating \(_{1}\) is illustrated in Figure 3.

**2. Excessive steric clash**: Adding noises to all torsional angles may damage protein structures and complicate the denoising process in our score network. In Appendix C, Figure 8 compares the number of atom clashes observed when using noise schemes in vanilla diffusion models and autoregressive-style ones to show the straightforward application of diffusion models results in significantly more steric clashes.

To address the aforementioned issues, we propose an autoregressive diffusion model over the torsion space. We factorize the joint distribution of the four torsional angles into separate conditional

Figure 3: Effects of rotating \(_{1}\).

distributions. Specifically, we have

\[p(_{1},_{2},_{3},_{4})=p(_{1}) p (_{2}|_{1}) p(_{3}|_{1,2}) p(_{4}|_{1,2,3}).\] (6)

This allows us to model the side-chain packing problem as a step-by-step generation of torsional angles: first, we predict the first torsional angles \(_{1}\) for all residues given the protein backbone structure; next, based on the backbone and the generated \(_{1}\), we predict the second torsional angles \(_{2}\); and so on for \(_{3}\) and \(_{4}\).

We train a separate score network for each distribution on the right-hand side using the torsional diffusion model from Section 3.2. To train the autoregressive model, we use a teacher-forcing strategy. Specifically, when modeling \(p(_{i}|_{1,,i-1})\), we assume that \(_{1,,i-1}\) are ground truth and keep these angles fixed. We then rotate \(_{i}\) by sampling noises from the perturbation kernel in (4) and discard all atoms belonging to the \(_{i+1,,4}\) groups to remove the dependency on following torsional angles. This approach eliminates the cumulative effects of diffusion process on \(_{i}\) and preserves the overall structure of the molecule, thus overcoming the aforementioned challenges. The generation process is illustrated in Figure 1 and the training procedure is described in Algorithm 1.

### Model Architecture

To model \(p(_{i}|_{1,,i-1})\), we utilize a score network constructed using the 3D coordinates \((_{t},t)\) of backbone atoms and atoms in the \(_{1,,i-1}\) groups. The score network's output represents the noise on torsional angles, which should be SE(3)-invariant _w.r.t._ the conformation \(_{t}\). Unlike previous methods  operating in Cartesian coordinates that require an SE(3)-equivariant score network, our method only requires SE(3)-invariance, providing greater flexibility in designing the model architecture. To ensure this invariance, we employ GearNet-Edge , a state-of-the-art protein structure encoder. This involves constructing a multi-relational graph with atoms as nodes, where node features consist of one-hot encoding for atom types, corresponding residue types, and time step embeddings. Edges are added based on chemical bond and 3D spatial information, determining their type. To learn representations for each node, we perform relational message passing between them . We denote the edge between nodes \(i\) and \(j\) with type \(r\) as \((i,j,r)\) and set of relations as \(\). We use \(_{i}^{(l)}\) to denote the hidden representation of node \(i\) at layer \(l\). Then, message passing can be written as:

\[_{i}^{(l)}=_{i}^{(l-1)}+((_ {r}_{r}_{j_{r}(i)}(_{j}^{(l- 1)}+(_{(i,j,r)}^{(l)})))),\] (7)

where \(_{r}\) is the learnable weight matrix for relation type \(r\), \(_{r}(j)\) is the neighbor set of \(j\) with relation type \(r\), and \(()\) is the activation function. Edge representations \(^{(l)}(i,j,r)\) are obtained through edge message passing. We use \(e\) as the abbreviation of the edge \((i,j,r)\). Two edges \(e_{1}\) and \(e_{2}\) are connected if they share a common end node, with the connection type determined by the discretized angle between them. The edge message passing layer can be written as:

\[_{e_{1}}^{(l)}=((_{r^{}}_{r}^{}_{e_{2}_{r}^{}(e_{1})} _{e_{2}}^{(l-1)})),\] (8)

where \(^{}\) is the set of relation types between edges and \(_{r}^{}(e_{1})\) is the neighbor set of \(e_{1}\) with relation type \(r\). After obtaining the hidden representations of all atoms at layer \(L\), we compute the residue representations by taking the mean of the representations of its constituent atoms. The residue representations are then fed into an MLP for score prediction. The details of architectural components are summarized in Appendix D.

### Inference

After completing the training, we adopt the common practice of autoregressive and diffusion models for inference, as described in Algorithm 2. We generate four torsional angles step by step as described in Section 3.3. When sampling \(_{i}\) based on the predicted torsional angles \(_{1,,i-1}\), we begin by sampling a random angle from the uniform prior and then discretize and solve the reverse diffusion. At each time step, we generate atoms in the \(_{1,,i}\) group and use our learned score network for denoising. We also discover several simple techniques that significantly improve our performance, including multi-round sampling, annealed temperature sampling and confidence models.

**Predictor-corrector sampling.** After discretizing the reverse diffusion SDE, we perform multiple Langevin steps subsequent to each denoising step. This hybrid method, which mixes the denoisingprocess with Langevin dynamics, is called predictor-corrector sampling, as suggested by . This approach can be seen as introducing an equilibration process to stabilize \(p_{t}\), and it is demonstrated to be effective in diffusion process.

**Annealed temperature sampling.** When designing a generative model, two critical aspects to consider are quality and diversity. The diffusion model often suffers from overdispersion, which prioritizes diversity over sampling quality. However, in the context of side chain packing, quality is more important than diversity. Directly using the standard reverse sampling process may lead to undesirable structures. Following , we utilize an annealed temperature sampling scheme to mitigate this issue. Specifically, We modify the reverse SDE by adding an annealed weight \(_{t}\) to the score function (details in Appendix E):

\[d=-_{t}(t)}{dt}_{} p_{t}( _{t})\,dt+(t)}{dt}}\,d,\ \ \ _{t}=^{2}}{T_{}^{2}-(T-1)^{2}(t)}.\] (9)

The above modification results in a reverse process approximating the low temperature sampling process, where ideally decreasing tempte \(T\) lead to a sampling process biased towards quality.

**Confidence model.** As per common practice in protein structure prediction , we train a confidence model to select the best prediction among multiple conformations sampled from our model. The model architecture we use is the same as that used in diffusion models, which takes the entire protein conformation as input and outputs representations for each residue. We train the model on the same dataset, using the residue representations to predict the negative residue-level RMSD of our sampled conformations. When testing, we rank all conformations based on our confidence model.

### Handling Symmetric Issues

Torsional angles generally exhibit a periodicity of \(2\). However, certain rigid side-chain structures possess a \(\)-rotation-symmetry, meaning that rotating the torsional angle by \(\) does not yield distinct physical structures, as demonstrated in Figure 4. For instance, in the case of a tyrosine residue, the phenolic portion of its side chain ensures that a \(\) rotation of its \(_{2}\) torsion angles only alters the internal atom name without affecting the actual structure.

Previous research by Jumper et al.  addressed this concern by offering an alternative angle prediction, \(+\), and minimizing the minimum distance between the ground-truth and both predictions. In our diffusion-based framework, we employ a distinct method. Specifically, the \(\)-rotation-symmetry results in the equivalence \((_{i})(_{i}+k)\) in torsion space, differing from the normal equivalence relationship in torsion space by a factor of \(2k\). Consequently, we can still define the forwarding diffusion process in torsion space, albeit with a modification to Equation 4:

\[p_{t|0}(^{}|)_{^{m}} (--^{}+\|^{2}}{2 ^{2}(t)}),\{1,2\}^{m}\] (10)

where \(_{i}=1\) for \(\)-rotation-symmetric rigid groups, and \(_{i}=2\) otherwise.

## 4 Related Work

**Protein side-chain packing.** Conventional approaches for protein side-chain packing (PSCP) involve minimizing the energy function over a pre-defined rotamer library [27; 73; 37; 3; 1; 35; 71; 7]. The choice of rotamer library, energy function, and energy minimization procedure varies among these methods. These methods rely on search heuristics and discrete sampling, limiting their accuracy. Currently, efficient methods like OSCAR-star , FASPR , SCWRL4  do not incorporate deep learning and depend on rotamer libraries.

Several ML methods exist for side-chain prediction [47; 46; 69; 70; 73; 39; 71], including SIDE-Pro , which trains 156 feedforward networks to learn an additive energy function over pairwise atomic distances; DLPacker , which uses a deep U-net-style neural network to predict atom positions and selects the closest matching rotamer; OPUS-Rota4 , which employs multiple deep networks and utilizes MSA as input to predict side-chain coordinates and obtain a final structure;

Figure 4: Distribution of \(\)-rotation-symmetry torsional angles (Blue) and \(2\)-rotation-symmetry (Red).

and AttnPacker , which builds transformer layers and triangle updates based on components in Tensor Field Network  and SE(3)-Transformer  and achieves the state-of-the-art performance. In contrast, our method focuses solely on torsion space degrees of freedom and leverages an autoregressive diffusion model to accurately and efficiently model rotamer energy.

**Diffusion models on molecules and proteins.** The Diffusion Probabilistic Model (DPM), which was introduced in , has recently gained attention for its exceptional performance in generating images and waveforms [23; 10]. DPMs have been employed in a variety of problems in chemistry and biology, including molecule generation [72; 26; 68; 30], molecular representation learning , protein structure prediction , protein-ligand binding , protein design [2; 44; 28; 66; 38; 74], motif-scaffolding , and protein representation learning . In this work, we investigate diffusion models in a new setting, protein side-chain packing, and propose a novel autoregressive diffusion model. Note that our definition of autoregressive diffusion model differs from existing works .

## 5 Experiments

### Experimental Setup

**Dataset.** We use BC40 for training and validation, which BC40 is a subset of PDB database selected by \(40\%\) sequence identity . Following the dataset split in , there are \(37266\) protein chains for training and \(1500\) protein chains for validation. We evaluate our models on CASP13 and CASP14. Training set is curated so that no structure share sequence similarity with test set by \( 40\%\).

**Baselines.** We compare DiffPack with deep learning methods, like AttnPacker , DLPacker  and traditional methods including SCWRL4 , FASPR  and RosettaPacker . Details can be found in Appendix H.1

**Metrics.** We evaluate the quality of generated side-chain conformations using three metrics: (1) **Angle MAE** measures the mean absolute error of predicted torsional angles. (2) **Angle Accuracy** measures the proportion of correct predictions, considering a torsional angle correct if the deviation is within \(20^{}\). (3) **Atom RMSD** measures the average RMSD of side-chain atoms for each residue.

Since predicting surface side-chain conformations is considered more challenging, some results are divided into "**Core**" and "**Surface**" categories. Core residues are defined as those with at least \(20\)\(C_{}\) atoms within a \(10\)A radius, while surface residues have at most \(15\)\(C_{}\) atoms in the same region.

### Side-Chain Packing

Table 1 summarizes the experimental result in CASP13, our model outperforms all other methods in predicting torsional angles, achieving the lowest mean absolute errors across all four Angle MAE categories (\(_{1}\), \(_{2}\), \(_{3}\), and \(_{4}\)). Additionally, DiffPack shows the highest Angle Accuracy for all residues (\(69.5\%\)), core residues (\(82.7\%\)), and surface residues (\(57.3\%\)), where for surface residue the accuracy is increased by \(20.4\%\) compared with previous state-of-the-art model AttnPacker. These results demonstrate that our model is better at capturing the distribution of the side chain torsion angles in both protein surfaces and cores. As for the atom-level side chain conformation prediction, DiffPack clearly outperforms other models in Atom RMSD. Moreover, the intrinsic design of DiffPack ensures that the generated structures have legal bond lengths and bond angles, while previous models in atom-coordinate space (e.g. AttnPacker) can easily generate side chains with illegal bond constraints without post-processing (as illustrated in Figure 6A).

Similarly, DiffPack outperforms other methods in all Angle MAE categories on the CASP14 dataset (Table 2). It achieves the highest Angle Accuracy for all residues (\(57.5\%\)), core residues (\(77.8\%\)), and surface residues (\(43.5\%\)). Furthermore, DiffPack reports the best Atom RMSD for all residues (\(0.793\) A), core residues (\(0.356\) A), and surface residues (\(0.956\) A).

Despite its superior performance on both test sets, our model, DiffPack, has a significantly smaller number of total parameters (3,043,363) compared to the previous state-of-the-art model, AttnPacker (208,098,163), which relies on multiple layers of complex triangle attention. This substantial reduction (\(68.4\)) in model size highlights the efficiency of diffusion-based approaches like DiffPack, making them more computationally feasible and scalable solutions for predicting side-chain conformations.

### Side-Chain Packing on Non-Native Backbone

In addition to native backbones, another interesting and challenging problem is whether the side chain packing algorithm can be applied to non-native backbone (_e.g._, backbones generated by protein folding algorithms). In this regard, we extend DiffPack to accommodate non-native backbones generated from AlphaFold2 . Table 3 gives the quantitative result of different algorithms including AlphaFold2's side-chain prediction on the CASP13-FM test set. All metrics are calculated after aligning the non-native backbone of each residue to the native backbone. As observed, DiffPack achieves state-of-the-art on most metrics. Notably, DiffPack is the only model that consistently outperforms AlphaFold2 in all metrics, showcasing its potential to refine AlphaFold2's predictions.

### Ablation Study

To analyze the contribution of different parts in our proposed method, we perform ablation studies on CASP13 and CASP14 benchmarks. The results are shown in Table 4.

**Autoregressive diffusion modeling.** We evaluate our autoregressive diffusion modeling approach against two baselines: joint diffusion (\(_{1,2,3,4}\)) and random diffusion (\(_{i}\)). Joint diffusion models the joint distribution of the four torsional angles of each residue and performed diffusion and denoising on all angles simultaneously, while random diffusion perturbs one random torsional angle per residue and generates all angles simultaneously during inference. Our approach outperforms both baselines (Table 4). Training loss curves (Figure 5) show that random diffusion has an easier time optimizing its loss than joint diffusion, but struggles with denoising all angles at once due to a mismatch between training and inference objectives. Our autoregressive scheme strikes a balance between ease of training and quality of generation, achieving good performance. We plot the training curves of the conditional distributions for the four torsional angles in DiffPack, finding that \(_{1}\) and \(_{2}\) are easier to optimize and perform better than random diffusion, likely due to discarding atoms from subsequent angles to overcome cumulative perturbation effects. However, training on the smaller set of valid residues with \(_{3}\) and \(_{4}\) is inefficient. Future work should address the challenge of training these angles more efficiently and mitigating cumulative errors in autoregressive models.

**Inference.** Three techniques are proposed in Section 3.5 to improve the inference of DiffPack. To evaluate the effectiveness of these techniques, we compare them with three baselines that do not

    & ^{}\)} &  &  \\ 
**Method** & \(_{1}\) & \(_{2}\) & \(_{3}\) & \(_{4}\) & All & Core & Surface & All & Core & Surface \\  SCWRL & 27.64 & 28.97 & 49.75 & 61.54 & 56.2\% & 71.3\% & 43.4\% & 0.934 & 0.495 & 1.027 \\ FASPR & 27.04 & 28.41 & 50.30 & 60.89 & 56.4\% & 70.3\% & 43.6\% & 0.910 & 0.502 & 1.002 \\ RosettaPacker & 25.88 & 28.25 & 48.13 & 59.82 & 58.6\% & 75.3\% & 35.7\% & 0.872 & 0.422 & 1.001 \\ DLPacker & 22.18 & 27.00 & 51.22 & 70.04 & 58.8\% & 73.9\% & 45.4\% & 0.772 & 0.402 & 0.876 \\ AttnPacker & 18.92 & 23.17 & 44.89 & 58.98 & 62.1\% & 73.7\% & 47.6\% & 0.669 & 0.366 & 0.775 \\  DiffPack & **15.35** & **19.19** & **37.30** & **50.19** & **69.5\%** & **82.7\%** & **57.3\%** & **0.579** & **0.298** & **0.696** \\   

Table 1: Comparative evaluation of DiffPack and prior methods on CASP13.

    & ^{}\)} &  &  \\ 
**Method** & \(_{1}\) & \(_{2}\) & \(_{3}\) & \(_{4}\) & All & Core & Surface & All & Core & Surface \\  SCWRL & 33.50 & 33.05 & 51.61 & 55.28 & 45.4\% & 62.5\% & 33.2\% & 1.062 & 0.567 & 1.216 \\ FASPR & 33.04 & 32.49 & 50.15 & 54.82 & 46.3\% & 62.4\% & 34.0\% & 1.048 & 0.594 & 1.205 \\ RosettaPacker & 31.79 & 28.25 & 50.54 & 56.16 & 47.5\% & 67.2\% & 33.5\% & 1.006 & 0.501 & 1.183 \\ DLPacker & 29.01 & 33.00 & 53.98 & 72.88 & 48.0\% & 66.9\% & 33.9\% & 0.929 & 0.476 & 1.107 \\ AttnPacker & 25.34 & 28.19 & 48.77 & **51.92** & 50.9\% & 66.2\% & 36.3\% & 0.823 & 0.438 & 1.001 \\  DiffPack & **21.91** & **25.54** & **44.27** & 55.03 & **57.5\%** & **77.8\%** & **43.5\%** & **0.770** & **0.356** & **0.956** \\   

Table 2: Comparative evaluation of DiffPack and prior methods on CASP14.

Figure 5: Training loss curves for different diffusion models.

use these techniques. For the baselines without multi-round sampling and annealed temperature, we simply resume the sampling procedure in the vanilla diffusion models. For the baseline without confidence models, we only draw one sample from our model instead of using confidence models to ensemble multiple samples. As shown in Table 4, the mean absolute error of the four torsional angles increases for the three baselines, demonstrating the effectiveness of our proposed techniques.

### Case Study

**DiffPack accurately predict the side-chain conformation with chemical validity.** As shown in Figure 6A and Figure 6B. DiffPack accurately predict the side-chain conformation with a substantially lower RMSD (0.196A and 0.241A) compared with other deep learning methods. Furthermore, DiffPack consistently ensures the validity of generated structures, while AttnPacker without post-processing sometimes violates the chemical validity due to its operation on atom coordinates.

**DiffPack correctly identifies the \(\) stacking interaction.** Accurate reconstruction of \(\) stacking interaction between side-chain has traditionally been challenging. Traditional method usually requires a specific energy term for modeling this interaction. Interestingly, DiffPack has shown the ability to implicitly model this interaction without the need of additional prior knowledge (Figure 6C).

    & ^{}\)} & ^{}\)} \\ 
**Method** & \(_{1}\) & \(_{2}\) & \(_{3}\) & \(_{4}\) & \(_{1}\) & \(_{2}\) & \(_{3}\) & \(_{4}\) \\  DiffPack & **15.35** & **19.19** & 37.30 & 50.19 & **21.91** & **25.54** & **44.27** & 55.03 \\  - w/ joint diffusion (\(_{1,2,3,4}\)) & 17.14 & 23.72 & **35.96** & **45.30** & 26.80 & 34.51 & 52.77 & 63.41 \\ - w/ random diffusion (\(_{1}\)) & 17.11 & 27.27 & 44.75 & 56.42 & 23.26 & 32.69 & 49.21 & **51.92** \\ - w/o multi-round sampling & 16.26 & 23.13 & 40.60 & 52.67 & 23.86 & 30.71 & 47.54 & 55.80 \\ - w/o annealed temperature & 15.55 & 22.56 & 39.32 & 51.37 & 22.82 & 29.61 & 45.86 & 55.22 \\ - w/o confidence models & 16.12 & 22.92 & 39.92 & 50.63 & 22.86 & 29.30 & 45.80 & 54.08 \\   

Table 4: Ablation study on CASP13 and CASP14.

Figure 6: Case studies on DiffPack. Predictions from different methods are distinguished by color. **(A)** DiffPack accurately predicts the side-chain conformation. AttnPacker-NoPP produces an invalid glutamic acid structure since O\({}^{ 1}\)is too close to O\({}^{ 2}\). **(B)** DiffPack accurately predicts the \(_{1}\) of leucine. **(C)** DiffPack correctly identifies \(\)-\(\) stacking interactions, indicated by dashed lines.

Conclusions

In this paper, we present DiffPack, a novel approach that models protein side-chain packing using a diffusion process in torsion space. Unlike vanilla joint diffusion processes, DiffPack incorporates an autoregressive diffusion process, addressing certain limitations. Our empirical results demonstrate the superiority of our proposed method in predicting protein side-chain conformations compared to existing approaches. Future directions include exploring diffusion processes for sequence and side-chain conformation co-generation, optimizing computational efficiency in side-chain packing, and considering backbone flexibility in the diffusion process.