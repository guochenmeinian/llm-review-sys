# CS4ML: A general framework for active learning with arbitrary data based on Christoffel functions

Ben Adcock

Department of Mathematics

Simon Fraser University

ben_adcock@sfu.ca

&Juan M. Cardenas

Department of Mathematics

Simon Fraser University

juan_manuel_cardenas@sfu.ca

&Nick Dexter

Department of Scientific Computing

Florida State University

nick.dexter@fsu.edu

###### Abstract

We introduce a general framework for active learning in regression problems. Our framework extends the standard setup by allowing for general types of data, rather than merely pointwise samples of the target function. This generalization covers many cases of practical interest, such as data acquired in transform domains (e.g., Fourier data), vector-valued data (e.g., gradient-augmented data), data acquired along continuous curves, and, multimodal data (i.e., combinations of different types of measurements). Our framework considers random sampling according to a finite number of sampling measures and arbitrary nonlinear approximation spaces (model classes). We introduce the concept of _generalized Christoffel functions_ and show how these can be used to optimize the sampling measures. We prove that this leads to near-optimal sample complexity in various important cases. This paper focuses on applications in scientific computing, where active learning is often desirable, since it is usually expensive to generate data. We demonstrate the efficacy of our framework for gradient-augmented learning with polynomials, Magnetic Resonance Imaging (MRI) using generative models and adaptive sampling for solving PDEs using Physics-Informed Neural Networks (PINNs).

## 1 Introduction

The standard regression problem in machine learning involves learning an approximation to a function \(f^{*}:D^{d}\) from training data \(\{(_{i},f^{*}(_{i}))\}_{i=1}^{m} D\). The approximation is sought in a set of functions \(\), typically termed a _model class_, _hypothesis set_ or _approximation space_, and is often computed by minimizing the empirical error (or risk) over the training set, i.e.,

\[*{argmin}_{f}_{i=1}^{m}|f^{ *}(_{i})-f(_{i})|^{2}.\]

In this paper, we develop a generalization of this problem. This allows for general types of data (i.e., not just discrete function samples), including multimodal data, and random sampling from arbitrary distributions. In particular, this framework facilitates _active learning_ by allowing one to optimize the sampling distributions to obtain near-best generalization from as few samples as possible.

### Motivations

Typically, the sample points \(_{i}\) in the above problem are drawn i.i.d. from some fixed distribution. Yet, in many applications of machine learning there is substantial freedom to choose the sample points in a more judicious manner to increase the generalization performance of the learned approximation. These applications are often highly _data starved_, thus making a good choice of sample points extremely valuable. Notably, many recent applications of machine learning in scientific computing often have these characteristics. An incomplete list of such applications include: Deep Learning (DL) for Partial Differential Equations (PDEs) via so-called Physics-Informed Neural Networks (PINNs) [39; 61; 83; 105; 112]; deep learning for parametric PDEs and operator learning [17; 49; 53; 66; 73; 75; 118]; machine learning for computational imaging [5; 79; 84; 88; 100; 107; 110]; and machine learning for discovering dynamics of physical systems [24; 25; 109].

In many such applications, the training data does not consist of pointwise samples of a target function. For example, in computational imaging, the samples are obtained through an integral transform such as the Fourier transform in Magnetic Resonance Imaging (MRI) or the Radon transform in X-ray Computed Tomography (CT) [5; 13; 21; 45]. In other applications, each sample may correspond to the value of \(f^{*}\) (or some integral transform thereof) along a continuous curve. Applications where this occurs include: seismic imaging, where physical sensors record continuously in time but at discrete spatial locations; discovering governing equations of physical systems, where each sample may be a full solution trajectory ; MRI, since the MR scanner samples along a sequence of continuous curves in \(k\)-space [80; 89]; and many more. In other applications, each sample may be a vector. For instance, in _gradient-augmented_ learning problems - with applications to PINNs for PDEs [54; 120] and learning parametric PDEs in Uncertainty Quantification (UQ) [9; 56; 78; 82; 103; 98; 99], as well as various other DL settings  - each sample is a vector in \(^{d+1}\) containing the values of \(f\) and its gradient at \(_{i}\). In other applications, each sample may be an element of a function space. This occurs in parametric PDEs and operator learning, where each sample is the solution of a PDE (i.e., an element of a Hilbert space) corresponding to the given parameter value. Finally, many problems involve _multimodal data_, where one has \(C>1\) different types of training data. This situation occurs in various applications, including: multi-sensor imaging systems  such as parallel MRI (which is ubiquitous in modern clinical practice) [5; 89]; PINNs for PDEs, where the data arises from the domain, the initial conditions and the boundary conditions; and multifidelity modelling [44; 102].

### Contributions

In this paper, we introduce a general framework for active learning in which

1. \(f^{*}\) need not be a scalar-valued function, but simply an element of a Hilbert space \(\);
2. the data arises from arbitrary linear operators, which may be scalar- or vector-valued;
3. the data may be multimodal, arising from \(C 1\) of different types of linear operators;
4. the approximation space \(\) may be an arbitrary linear or nonlinear space;
5. sampling is random according to arbitrary probability measures;
6. the _sample complexity_, i.e., the relation between \(\) and the number of samples required to attain a near-best generalization error, is given explicitly in terms of the so-called _generalized Christoffel functions_ of \(\);
7. using this explicit expression, the sampling measures can be optimized, leading to essentially optimal sample complexity in various cases.

This framework, _Christoffel Sampling for Machine Learning (CS4ML)_, is very general. Indeed, we are unaware of any other active learning strategy that can simultaneously address such a broad class of problems. It is inspired by previous work on function regression in linear spaces [2; 34; 57; 60] and is also a generalization of _leverage score sampling_[11; 30; 32; 41; 46; 50; 85] for active learning. See Appendix A for further discussion. It generalizes these approaches by (a) considering abstract objects in Hilbert spaces, (b) allowing for arbitrary linear sampling operators, as opposed to just function samples, (c) allowing for multimodal data, and (d) considering general linear or nonlinear approximation spaces. We demonstrate its practical efficacy on a diverse set of test problems. These are: **(i) Polynomial regression with gradient-augmented data**, **(ii) MRI reconstruction using generative models** and **(iii) Adaptive sampling for solving PDEs with PINNs**. Our solution to (iii) introduces a general adaptive sampling strategy, termed _Christoffel Adaptive Sampling_, that can be applied to any DL-based regression problem (i.e., not simply PINNs for PDEs).

The CS4ML framework and main results

### Main definitions

Let \((,,)\) be a probability space, which is the underlying probability space of the problem. In particular, our various results hold in probability with respect to the probability measure \(\). Let \(\) be a separable Hilbert space and \(_{0}\) be a normed vector subspace of \(\), termed the _object space_. Our goal is to learn an element (the object) \(f^{*}_{0}\) from training data. We consider a multi-modal measurement model in which \(C 1\) different processes generate this data. For each \(c=1,,C\), we assume there is a measure space \((D_{c},_{c},_{c})\), termed the _measurement domain_, which parametrizes the possible measurements. We will often consider the case where this is a probability space, but this is not strictly necessary for the moment. Next, we assume there are semi-inner product spaces \(_{c}\), \(c=1,,C\), termed _measurement spaces_, to which the measurements belong, and mappings

\[L_{c}:D_{c}(_{0},_{c}), c=1,,C,\]

where \((_{0},_{c})\) denotes the space of bounded linear operators \(_{0}_{c}\). We refer to \(L_{c}\) as _sampling operator_ for the \(c\)th measurement process. Note that we consider the sampling operators \(L_{c}\) as fixed and specified by the problem under consideration. However, we make the following assumption, which states that we have the flexibility to query each sampling operator at an arbitrary point in the measurement domain.

**Assumption 2.1** (Active learning): For each \(c\), we may query \(L_{c}()(f^{*})\) at any \( D_{c}\).

This assumption is a direct extension of that made in standard active learning (see Example 2.3 below). It holds in all our main examples. Given this assumption, our approach is to selecting sample points is to draw them randomly according to certain _sampling measures_\(_{1},,_{C}\) (defined on \(D_{1},,D_{C}\), respectively), which can then be optimized to ensure as good generalization as possible. To this end, we also make the following assumption.

**Assumption 2.2** (Sampling measures): Each sampling measure \(_{c}\) is a probability measure on \((D_{c},_{c})\) that is absolutely continuous with respect to \(_{c}\) and its Radon-Nikodym derivative is positive almost everywhere. In other words, \(\,_{c}()=_{c}()\,_{c}()\) for some measurable function \(_{c}:D_{c}\) that positive almost everywhere and satisfies \(_{D_{c}}_{c}()\,_{c}()=1\).

We now define the training data. Let \(m_{1},,m_{C}\) be the (not necessarily equal) measurement numbers, i.e., \(m_{c}\) is the number of measurements from the \(c\)th measurement process. Let \(_{c}: D_{c}\) be independent \(D_{c}\)-valued random variables, where \(_{c}_{c}\) for each \(c\). For each \(c=1,,C\), let \(_{ic}\), \(i=1,,m_{c}\), be independent realizations of \(_{c}\). Then the training data is

\[\{(_{ic},y_{ic})\}_{i,c=1}^{m_{c},C},y_{ic}=L_{c}( _{ic})(f^{*})_{c},\;i=1,,m_{c}\;c=1,,C.\] (2.1)

Later, in Section 4.3 we also allow for noisy measurements. Note that we consider the _agnostic learning_ setting, where \(f^{*}\) and the noise can be adversarial, as this setting is most appropriate to the various applications considered (see also ).

**Example 2.3** (Active learning in standard regression): The above framework extends the standard active learning problem in regression. In the classic regression problem, \(D^{d}\) is a domain and \(=L_{}^{2}(D)\) is the space of square-integrable functions \(f^{*}:D\) with respect to a measure \(\). Note that \(\) is considered fixed - it is the measure with respect to which we measure the error. To embed this problem into the above framework, we let \(_{0}=C()\) be the space of continuous functions on \(D\), \(C=1\), the measurement domain \(D_{c}=D\) be equal to the domain of the function, \(_{1}=\) and the measurement space \(_{1}=\) (with the Euclidean inner product). We then define the sampling operator \(L_{1}()(f^{*})=f^{*}()\) as the pointwise evaluation operator. In particular, for a measure \(=_{1}\) satisfying Assumption 2.2, the training data (2.1) is \((_{i},f^{*}(_{i}))\) with \(_{i}_{}\). Hence, the aim is to choose the measure \(\) (or equivalently, its Radon-Nikodym derivative \(\)) to ensure as good generalization as possible.

Next, we let \(_{0}\) be a subset within which we seek to learn \(f^{*}\). We term \(\) the _approximation space_. Note this could be a linear space such as a space of algebraic or trigonometric polynomials, or a nonlinear space such as the space of sparse Fourier functions, a space of functions with sparse representations in a multiscale system such as wavelets, the space corresponding to a single neuron model, or a space of deep neural networks. We discuss a number of examples later. Given \(\) and \(f^{*}_{0}\), in this work we consider an approximation \(\) defined via the empirical least-squares fit

\[*{argmin}_{f}_{c=1}^{C}} _{i=1}^{m_{c}}(_{ic})}\|y_{ic}-L_{c}(_{ic})(f) \|_{_{c}}^{2}.\] (2.2)

However, regardless of the method employed, one cannot expect to learn \(f^{*}\) from (2.1) without an assumption on the sampling operators. Indeed, consider the case \(L_{c} 0\), \( c\). The following assumption states that the sampling operators should be sufficiently 'rich' to approximately preserve the \(\)-norm. As we see later, this is a natural assumption, which holds in many cases of interest.

**Assumption 2.4** (Nondegeneracy of the sampling operators): The mappings \(L_{c}\) are such that the maps \(D_{c}_{c}, L_{c}()(f)\) are measurable for every \(f_{0}\). Moreover, the functions \(D_{c},\|L_{c}()(f)\|_{_{ c}}^{2}\) are integrable and satisfy, for constants \(0<<\),

\[\|f\|_{}^{2}_{c=1}^{C}_{D_{c}}\|L_{c}( )(f)\|_{_{c}}^{2}_{c}()\|f\|_{ }^{2}, f_{0}.\] (2.3)

In order to successfully learn \(f^{*}\) from the samples (2.1), we need to (approximately) preserve nondegeneracy when the integrals in (2.3) are replaced by discrete sums. Notice that

\[\|f\|_{}^{2}(_{c=1}^{C}} _{i=1}^{m_{c}})}\|L_{c}(_{ic})(f)\|_{_{c}}^{2})\|f\|_{}^{2}, f_{0},\] (2.4)

where \(\) denotes the expectation with respect to all the random variables \((_{ic})_{i,c}\). Our subsequent analysis involves deriving conditions under which this holds with high probability. We shall say that _empirical nondegeneracy_ holds with constant \(0<<1\) for a draw \((_{ic})_{i,c}\) if

\[(1-)\|f\|_{}^{2}_{c=1}^{C}} _{i=1}^{m_{c}})}\|L_{c}(_{ic})(f)\|_{_{c}}^{2}(1+)\|f\|_{}^{2}, f -,\] (2.5)

where \(-=\{f_{1}-f_{2}:f_{1},f_{2}\}\). See Appendix A for some background on this definition.

Finally, we need one further assumption. This assumption essentially says that the action of the sampling operator \(L_{c}\) on \(\) is nontrivial, in the sense that, for almost all \( D_{c}\), there exists a \(f\) that has a nonzero measurement \(L_{c}()(f)\). This assumption is reasonable, since without it, there is little hope to fit the measurements with an approximation from the space \(\).

**Assumption 2.5** (Nondegeneracy of \(\) with respect to \(L_{c}\)): For each \(c=1,,C\) and almost every \(_{c} D_{c}\), there exists an element \(f_{c}\) such that \(L_{c}(_{c})(f_{c}) 0\).

### Summary of main results

The goal of this work is to derive sampling measures that ensure a quasi-optimal generalization bound from as little training data as possible. These measures are given in terms of the following function.

**Definition 2.6** (Generalized Christoffel function): Let \(_{0}\) be normed vector space, \(\) be a semi-inner product space, \((D,,)\) be a measure space, \(L:D(_{0},)\) be such that the function \(D, L()(f)\) is measurable for every \(f_{0}\) and \(_{0}\), \(\{0\}\). The _Generalized Christoffel function of \(\) with respect to \(L\)_ is the function

\[K()()=\{\|L()(f)\|_{}^{2}/\|f\|_{ }^{2}:f,\ f 0\}, D.\]

If \(=\{0\}\), then we set \(K()()=0\). Further, we also define \(()=_{D}K()()\,()\).

In the standard regression problem (see Example 2.3), Definition 2.6 reduces to

\[K()()=\{|f()|^{2}/\|f\|_{L_{}^{2}(D)}^{2}:f ,\ f 0\}, D.\] (2.6)This is a well-known object, which is often referred to as the _Christoffel function_ when \(\) is a linear subspace. It is also equivalent (up to a scaling) to the _leverage score function_. See Appendix A.2 for further discussion. Definition 2.6 extends these notions from linear subspaces to nonlinear spaces and from pointwise samples to arbitrary sampling operators \(L\).

To state our main result, we also need the following. Given \(>0\) we define the _shrinkage_ operator \(_{}:\) by \(_{}(f)=\{1,/\|f\|_{}\}f, f \).

**Theorem 2.7**.: _Consider the setup of Section 2.1, where \(-=_{j=1}^{d}_{j}\) is a union of \(d\) subspaces of dimension at most \(n\). Let \(K_{c}\), \(c=1,,C\), be as in Definition 2.6 for \((D,,)=(D_{c},_{c},_{c})\) and \(L=L_{c}\). Suppose that the \(K_{c}(-)\) are integrable, define_

\[^{}_{c}()=K_{c}(-)()\, _{c}()/_{c}(-), c=1, ,C,\] (2.7)

_and suppose that_

\[m_{c}^{-1}_{c}(-)(2nd/ ), c=1,,C\] (2.8)

_for some \(0<<1\). Then the following hold._

1. _Empirical nondegeneracy (_2.5_) holds with_ \(=1/2\) _with probability at least_ \(1-\)_._
2. _For any_ \(f^{}_{0}\)_,_ \(\|f^{}\|_{} 1\)_, the estimator_ \(=_{1}()\)_, where_ \(\) _is as in (_2.2_), satisfies_ \[\|f^{}-\|_{}^{2}(/) _{f}\|f^{}-f\|_{}^{2}+.\] (2.9)
3. _The total number of samples_ \(m=m_{1}++m_{C}\) _is at most log-linear in_ \(nd\)_, namely,_ \[m(/) nd(2nd/).\] (2.10)

The choice (2.7) is a particular type of importance sampling, which we term _Christoffel Sampling (CS)_. In particular, for the standard regression problem (see Example 2.3) the resulting measure is

\[^{}(y) K(-)(y)\,(y),\] (2.11)

where \(K(-)(y)\) is as in (2.6) with \(\) replaced by \(-\). As we discuss in Appendix A.2, this is equivalent to _leverage score sampling_ for the standard regression problem (see, e.g., ). Therefore, Theorem 2.7 can be considered a generalization of leverage score sampling from standard regression to significantly more general types of linear, multimodal measurements. Specifically, it considers the general setting of arbitrary Hilbert spaces \(\), nonlinear approximation spaces \(\), and \(C>1\) arbitrary, linear sampling operators \(L_{c}\) taking values in arbitrary semi-inner product spaces \(_{c}\). To the best of our knowledge this generalization is new. We remark, however, that Theorem 2.7 is also related to the recent work . See Appendix A.3 for further discussion.

The sampling measure (2.7) is 'optimal' in the sense that it minimizes a sufficient condition over all possible sampling measures \(_{c}\) (see Lemma 4.6). Doing so leads to CS and Theorem 2.7. The measurement condition (2.10) states that CS leads to a desirable sample complexity bound (2.10) that is at worse log-linear in \(nd\). In particular, when \(d=1\) (i.e., \(\) is a linear space) the sample complexity is near-optimal (and in this case, one also has \(-=\)). In general, the bound (2.10) is near-optimal in terms of \(n\) when \(d\) is small (and independent of \(n\)). Unfortunately, \(d\) can be large in important cases such as sparse regression. In this case, however, the linear dependence on \(d\) in (2.10) - which follows from (2.8) via the crude estimate \(_{c=1}^{C}_{c}(-) nd\) - can be lessened by using the specific structure of the approximation space. See Appendix A.4.

Theorem 2.7 is a simplification and combination of our several main results. In Theorem 4.2 we consider more general approximation spaces, which need not be unions of finite-dimensional subspaces. See also Remark 4.5. Finally, we note that \(\) only differs from \(\) by a shrinkage operator, which is a technical step needed to bound the error in expectation. See Section 4.3. An 'in probability' bound can be obtained without this additional complication, albeit with less succinct right-hand side.

## 3 Examples and numerical experiments

Theorem 2.7 shows that CS can be a near-optimal active learning strategy. We now show its efficacy via a series of examples. These examples also highlight how the generality of the framework allows it to tackle a wide range of problems. We consider cases where the theory applies directly and otherswhere it does not. In all cases, we present performance gains over inactive learning, i.e., _Monte Carlo Sampling (MCS)_ from the underlying probability measure. Another matter we address is how to sample from the measures (2.7) in practice, this being very much dependent on the problem considered. Full experimental details for each application can be found in Appendices B-D.

### Polynomial regression with gradient-augmented data

Many machine learning applications involve regressing a smooth, multivariate function using a model class consisting of algebraic polynomials. Often, the training data consists of function values. Yet, as noted in Section 1.1, there are many settings where one can acquire both function values and values of its gradient. In this case, the training data takes the form \(\{(_{i},f(_{i}), f(_{i}))\}_{i=1}^{m} ^{d}^{d+1}\). As we explain in Appendix B.1, this problem fits into our framework with \(C=1\), where \(\) is the standard Gaussian measure on \(^{d}\), \(=H_{}^{1}(^{d})\) is the Sobolev space of weighted square-integrable functions with weighted square-integrable first-order derivatives and \(=^{d+1}\) with the Euclidean inner product. It therefore provides a first justification for allowing non-scalar valued sampling operators. Note that several important variations on this problem also fit into our general framework with \(C>1\). See Appendix B.7.

We consider learning a function from such training data in a sequence of nested polynomial spaces \(_{1}_{2}\). These spaces are based on _hyperbolic cross_ index sets, which are particularly useful in multivariate polynomial regression tasks [1; 37]. See Appendix B.2 for the formal definition.

In Fig. 1 we compare gradient-augmented polynomial regression with CS versus MCS from the Gaussian measure \(\). See Appendices B.3-B.5 for details on how we implement CS in this case. CS gives a dramatic improvement over MCS. CS is theoretically near-optimal in the sense of Theorem 2.7 - i.e., it provably yields a log-linear sample complexity - since the approximation spaces are linear subspaces in this example. On the other hand, MCS fails, with the error either not decreasing or diverging. The reason is the log-linear scaling, which is not sufficient to ensure a generalization error bound of the form (2.9) for MCS. As we demonstrate in Appendix B.6, MCS requires a much more severe scaling of \(m\) with \(n\) to ensure a generalization bound of this type.

### MRI reconstruction using generative models

Reconstructing an image from measurements is a fundamental task in science, engineering and industry. In many applications - in particular, medical imaging modalities such as MRI - one wishes to reduce the number of measurements while still retaining image quality. As noted, techniques based on DL have recently led to significant breakthroughs in image recovery tasks. One promising approach involves using generative models [16; 20; 67]. First, a generative model is trained on a database of relevant images, e.g., brain images in the case of MRI. Then the image recovery problem is formulated as a regression problem, such as (2.2), where \(\) is the range of the generative model. Note that in this problem, the training data consists of a finite set of frequencies and the corresponding values of the Fourier transform of the unknown image.

As we explain in Appendices C.1-C.3, this problem fits into our general framework. In Fig. 2 we demonstrate the efficacy of CS for Fourier imaging with generative models. In this example, the

Figure 1: **CS for gradient-augmented polynomial regression.** Plots of the average regression error versus \(m\) for MCS and CS for gradient-augmented polynomial regression of the function \(f^{*}()=(-(_{1}++_{d})/(2d))\) for \(d=2,4,8\) (left to right). In both cases, we use the scaling \(m=\{n,n(n)\}/(d+1)\). In this and other figures, the solid lines show the mean value and the shaded region indicates one standard deviation. See Section B.5 for further details.

generative model was trained on a database of 3D MRI brain images (see Appendix C.5). This experiment simulates a 3D image reconstruction problem in MRI, where the measurements are samples of the Fourier transform of the unknown image taken along horizontal lines in \(k\)-space (a sampling strategy commonly known as _phase encoding_). The active learning problem involves judiciously choosing \(m\) horizontal lines in \(k\)-space to enhance the generalization performance of the learning procedure. In Fig. 2, we compare the average Peak Signal-to-Noise Ratio (PSNR) versus frame (i.e., 2D image slice) number for CS versus MCS (i.e., uniform random sampling) for reconstructing an unknown image. We observe a significant improvement, especially in the challenging regime where the sampling percentage (the ratio of the number of measurements to the image size) is low.

This example lies close to our main theorem, but is not fully covered by it. See Appendix C.3. The space \(\) (the range of a generative model) is not a finite union of subspaces. However, it is known that certain generative models (namely, those based on ReLU activation functions) are subsets of unions of subspaces of controllable size . Further, we do not sample exactly from (2.7) in this case, but rather an empirical approximation to it (see Appendix C.4). Nevertheless, our experiments show a significant performance gain from CS in this case, despite it lying strictly outside of our theory.

This application justifies the presence of arbitrary (i.e., non-pointwise) sampling operators in our framework. It is another instance of non-scalar valued sampling operators, since each measurement is a vector of frequencies values along a horizontal line in \(k\)-space. Fig. 2 considers \(C=1\) sampling operators. However, as we explain Appendix C.7, the important extension of this setup to _parallel_ MRI (which is standard in clinical practice) can be formulated in our framework with \(C>1\).

### Adaptive sampling for solving PDEs with PINNs

In our final example, we apply this framework to solving PDEs via Physics-Informed Neural Networks (PINNs). PINNs have recently shown great potential and garnered great interest for approximating solutions of PDEs [42; 105; 112]. It is typical in PINNs to generate samples via Monte Carlo Sampling (MCS). Yet, this suffers from a number of limitations, including low accuracy.

We use this general framework combined with the _adaptive basis viewpoint_ to devise a new adaptive sampling procedure for PINNs. Here, a Deep Neural Network (DNN) with \(n\) nodes in its penultimate layer is viewed as an element of the linear subspace spanned the \(n\) functions defined by this layer's nodes. Our method then proceeds as follows. First, we use an initial set of \(m=m_{1}\) samples and train the corresponding PINN \(=_{1}\). Then, we use the adaptive basis viewpoint to construct a subspace \(_{1}\) with \(_{1}_{1}\). Next, we draw \(m_{2}-m_{1}\) samples using CS for the subspace \(_{1}\), and use the set of \(m=m_{2}\) samples to train a new PINN \(=_{2}\), using the weights and biases of \(_{1}\) as the initialization. We then repeat this process, alternating between generating new samples via CS and retraining the network, to obtain a sequence of PINNs \(_{1},_{2},\) approximating the solution of the PDE. We term this procedure _Christoffel Adaptive Sampling (CAS)_. See Appendices D.1-D.4 for further information.

Figure 2: **CS for MRI reconstruction using generative models. Plots of (left) the average PSNR vs. image number of the 3-dimensional brain MR image for both CS and MCS methods at 0.125%, 0.375%, and 0.625% sampling percentages, (middle) the average PSNR computed over the frames 30 to 90 of the image vs. sampling percentage for both CS and MCS methods, and (right) the empirical function \(\) used for the CS procedure.**

In Fig. 3 we show that this procedure gives a significant benefit over MCS in terms of the number of samples needed to reach a given accuracy. See Appendix D.5 for further details on the experimental setup. Note that both approaches use the same DNN architecture and are trained in exactly the same way (optimizer, learning rate schedule, number of epochs). Thus, the benefit is fully derived from the sampling strategy. The PDE considered (Burger's equations) exhibits shock formation as time increases. As can be seen in Fig. 3, CAS adapts to the unknown PDE solution by clustering samples near this shock, to better recover the solution than MCS.

As we explain in Appendix D.2, this example also justifies \(C>1\) in our general framework, as we have three sampling operators related to the PDE and its initial and boundary conditions. We note, however, that this example falls outside our theoretical analysis, since the sampling operator stemming from the PDE is nonlinear and the method is implemented in an adaptive fashion. In spite of this, however, we still see a nontrivial performance boost from the CS-based scheme.

## 4 Theoretical analysis

In this section, we present our theoretical analysis. Proofs can be found in Appendix E.

### Sample complexity

We first establish a sufficient condition for empirical nondegeneracy (2.5) to hold in terms of the numbers of samples \(m_{c}\) and the generalized Christoffel functions of suitable spaces.

**Definition 4.1** (Subspace covering number).: Let \((,_{})\) be a quasisemi-normed vector space, \(\) be a subset of \(\), \(n_{0}\) and \(t 0\). A collection of subspaces \(_{1},,_{d}\) of \(\) of dimension at most \(n\) is a \((_{},n,t)\)_-subspace covering_ of \(\) if for every \(f\) there exists a \(j\{1,,d\}\) and a \(f_{j}_{j}\) such that \( f-f_{j}_{} t\). The _subspace covering number_\((,_{},n,t)\) of \(\) is the smallest \(d\) such that there exists a \((_{},n,t)\)-subspace covering of \(\) consisting of \(d\) subspaces.

In this definition, we consider a zero-dimensional subspace as a singleton \(\{x\}\). Therefore, the subspace covering number \((,_{},0,t)\) is precisely the classical covering number \((,_{},t)\). We also remark that if \(=_{j=1}^{d}_{j}\) is itself a union of \(d\) subspaces of dimension at most \(n\), then \((,_{},n^{},t)=d\) for any \(t 0\) and \(n^{} n\).

We now also require the following notation. If \(\), we set \(()=\{f/ f_{}:f \{0\}\}\).

**Theorem 4.2** (Sample complexity for empirical nondegeneracy).: _Consider the setup of Section 2.1. Let \(_{0}\), \(n_{0}\), \(0<,<1\) and \(_{1},,_{d}\) be a \((\!\!,n,t)\)-subspace covering of \((-)\), where \(t=\) and \(\!\!^{2} =_{c=1}^{C}*{ess\,sup}_{_{c}_{c}}\{ L _{c}(_{c})()_{_{c}}^{2}/(_{c})\}\). Suppose that_

\[m_{c} c_{/2}^{-1}*{ess\,sup}_{ _{c}}\{K_{c}(})()/_{c}() \}(2d\{n,1\}/), c=1,,C,\] (4.1)

_where \(c_{/2}\) is as in (E.6) and \(}=_{i=1}^{d}_{i}\). Then (2.5) holds with probability at least \(1-\)._

Figure 3: **CAS for solving PDEs with PINNs.** Plots of (left) the average test error \(E(u^{*})\) versus the number of samples used for training \(\) and sigmoid \(5 50\) DNNs using CAS (solid line) and MCS (dashed line), (middle) the samples generated by the CAS for the \( 5 50\) DNN and (right) the samples generated by CAS for the sigmoid \(5 50\) DNN. The colour indicates the density of the points.

This theorem gives the desired condition (4.1) for empirical nondegeneracy (2.5). It is interesting to understand when this condition can be replaced by one involving the function \(K_{c}\) evaluated over \(-\) rather than its cover \(}\). We now examine two important cases where this is possible.

**Corollary 4.3** (Sample complexity for unions of subspaces).: _Suppose that \(-=_{j=1}^{d}_{j}\) is a union of \(d\) subspaces of dimension at most \(n\). Then (4.1) is equivalent to_

\[m_{c} c_{/2}^{-1}*{ess\,sup}_{ _{c}}\{K_{c}(-)()/_{c}() \}(2nd/), c=1,,C.\]

**Corollary 4.4** (Sample complexity for classical coverings).: _Consider Theorem 4.2 with \(n=0\). Then (4.1) is implied by the condition_

\[m_{c} c_{/2}^{-1}*{ess\,sup}_{ _{c}}\{K_{c}(-)()/_{c}() \}(2d/), c=1,,C.\]

**Remark 4.5**: Theorem 4.2 is formulated generally in terms of subspace coverings. This is done so that the scenarios covered in Corollaries 4.3 and 4.4 are both straightforward consequences. Our main examples in Section 3 are based on the (unions of) subspaces case. While this assumption is relevant for these and many other examples, there are some key problems that do not satisfy it. For example, in low-rank matrix (or tensor) recovery , the space \(\) of rank-\(r\) matrices (or tensors) is not a finite union of finite-dimensional subspaces. But tight bounds for its covering number are known , meaning it fits within the setup of Corollary 4.4.

### Christoffel sampling

Theorem 4.2 reduces the question of identifying optimal sampling measures to that of finding weight functions \(_{c}\) that minimize the corresponding essential supremum in (4.1). The following lemma shows that this is minimized by choosing \(_{c}\) proportional to the generalized Christoffel function.

**Lemma 4.6** (Optimal sampling measure).: _Let \(L\), \(\), \(K()\) and \(()<\) be as in Definition 2.6. Suppose that for almost every \( D\) there exists a \(f\) such that \(L()(f) 0\). Then_

\[*{ess\,sup}_{}\{K()()/() \}()\]

_for any measurable \(:D\) that is positive almost anywhere and satisfies \(_{D}()\,()=1\). Moreover, this optimal value is attained by the function \(^{}()=K()()/()\), \( D\)._

In view of this lemma, to optimize the sample complexity bound (4.1) we choose sampling measures

\[^{}_{c}()=K_{c}(})()\, _{c}()/_{c}(), c=1,,C.\] (4.2)

As noted, we term this _Christoffel Sampling (CS)_. This leads to a sample complexity bound

\[m_{c} c_{/2}^{-1}_{c}( })(2d\{n,1\}/), c=1,,C.\] (4.3)

In the case of subspaces (or union thereof with small \(d\)), this approach leads to a near-optimal sample complexity bound for the total number of measurements \(m:=m_{1}++m_{C}\).

**Corollary 4.7** (Near-optimal sampling for unions of subspaces).: _Suppose that \(-=_{j=1}^{d}_{j}\) is a union of \(d\) subspaces of dimension at most \(n\). Then \(_{c=1}^{C}_{c}(-) dn\). Therefore, choosing the sampling measures as in (4.2) with \(}=-\) and the number of samples \(m_{c} c_{/2}^{-1}_{c}(-)(2nd/)\) leads to the overall sample complexity bound_

\[m c_{/2}(/) n d(2nd/).\]

We note in passing that \(_{c=1}^{C}_{c}(-) n/p\) whenever \((_{c}) p\), \( c\). See Theorem E.4. Therefore, the sample complexity bound is at least \(_{,,p}(n(nd/))\).

It is worth comparing these bounds with MCS. Suppose that the \(_{c}\)'s are probability measures, in which case MCS is well defined. For MCS, we have \(_{c} 1\), \( c\), and therefore the corresponding measurement condition is

\[m_{c} c_{/2}^{-1}_{c}( })(2d\{n,1\}/), c=1,,C,\]

where \(_{c}(})=*{ess\,sup}_{_{ c}}K_{c}(})()_{c}(})\). Comparing with (4.3), we conclude that the benefit of CS over MCS in terms of sample complexity corresponds to the difference between the supremum of the Christoffel function (i.e., \(_{c}(})\)) and its average (i.e., \(_{c}(})\)). Thus, if \(K_{c}\) has sharp peaks - as it does, for instance, in Fig. 2 - one expects a significant improvement from CS over MCS.

### Generalization error bound and noisy data

Thus far, we have derived CS and shown parts (i) and (iii) of Theorem 2.7. In this section we establish the generalization bound in part (ii). For additional generality, we now consider noisy samples

\[y_{ic}=L_{c}(_{ic})(f^{*})+e_{ic}_{c}, i=1,,m_{c}\;c =1,,C,\] (4.4)

where the \(e_{ic}\) represent measurement noise (see Remark E.6 for some further discussion on the noise term). We also consider inexact minimizers. Specifically, we say that \(\) is a _\(\)-minimizer_ of (2.2) for some \(>0\) if it yields a value of the objective function that is within \(\) of the minimum value. For example, \(\) may be the output of some training algorithm used to solve (2.2).

**Theorem 4.8**.: _Let \(0<,,<1\) and consider the setup of Section 2.1, except with noisy data (4.4). Suppose that (4.1) holds and also that \(_{c}(D_{c})<\), \( c\). Then, for any \(f^{*}_{0}\) and \(\|f^{*}\|_{}\), the estimator \(=_{}()\), where \(\) is a \(\)-minimizer of (2.2), satisfies_

\[\|f^{*}-\|_{}^{2} 3(1+)_{f}\|f^{*}-f\|_{}^{2}+4 ^{2}+N+ ^{2},\]

_where \(N=_{c=1}^{C}(D_{c})}{m_{c}}_{i=1}^{m_{c}}\|e_{ic} \|_{_{c}}^{2}\)._

## 5 Conclusions, limitations and future work

We conclude by noting several limitations and areas for future work. First, in this work, we have striven for breadth - i.e., highlighting the efficacy of CS4ML across a diverse range of examples - rather than depth. Our aim is to show that the well-known ideas for active learning in standard regression (e.g., leverage score sampling) can be extended to a very general setting. We do not claim that CS is the best possible method for each example, and as such, our experimental results only compare against (inactive) MCS. In each application considered, there are other domain-specific strategies that are known to outperform MCS. See  and references therein for Fourier imaging, and  in the case of PINNs. CS is a general framework, and is arguably more theoretically grounded and less heuristic than some such methods. Nonetheless, further investigation is needed to ascertain which method is best in each setting. In a similar vein, while Figs. 1-3 show significant performance gains from our method, the extent of such gains depends heavily on the problem. In Appendices B.6 and D.6 we discuss cases where the gains are far more marginal. In the PINNs example in particular, additional studies are needed to see if CAS leads to benefits across a wider spectrum of PDEs. Moreover, there is the intriguing possibility of using CS as the starting point for more advanced active learning schemes - for example, by generalizing the linear-sample sparsification techniques of  or the 'boosting' techniques of .

Second, as noted previously, a limitation of our theoretical analysis is the log-linear scaling with respect to the number of subspaces \(d\) (see Theorem 2.7, part (iii)). While this can be overcome in cases such as sparse regression (see Appendix A.4), we expect a more refined theoretical analysis may be able to tackle this problem in the general setting. Another limitation of our analysis is that the sample complexity bound in Theorem 4.2 involves \(K_{c}\) evaluated over the subspace cover \(}\), rather than simply \(-\) itself. We anticipate this can also be improved through a more sophisticated argument. This would help close the theoretical gap in the generative models example considered in this paper. Another interesting theoretical direction involves reducing the sample complexity from log-linear to linear (when \(\) is a linear subspace), by extending, for example, .

Finally, we reiterate that our framework and theoretical analysis are both very general. Consequently, there are many other potential problems to which we can apply this work. As noted, the main practical hurdle in applying this framework to other problems is to determine how to sample from the optimal measure (2.7), or some computationally tractable surrogate. Some other problems of interest include low-rank matrix or tensor recovery, as mentioned briefly in Remark 4.5, sparse regression using random feature models, as was recently developed in , active learning for single neuron models, as developed in , and operator learning . These are interesting avenues for future work.