# Replicability in Reinforcement Learning+

Footnote â€ : Authors are listed alphabetically.

Amin Karbasi

Yale University, Google Research

amin.karbasi@yale.edu

&Grigoris Velegkas

Yale University

grigoris.velegkas@yale.edu

Lin F. Yang

UCLA

linyang@ee.ucla.edu

&Felix Zhou

Yale University

felix.zhou@yale.edu

###### Abstract

We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a _generative model_. Inspired by Impagliazzo et al. (2022), we say that an RL algorithm is replicable if, with high probability, it outputs the _exact_ same policy after two executions on i.i.d. samples drawn from the generator when its _internal_ randomness is the same. We first provide an efficient \(\)-replicable algorithm for \((,)\)-optimal policy estimation with sample and time complexity \(((1/)}{(1-)^{5} ^{2}^{2}})\), where \(N\) is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order \((}{(1-)^{3}^{2}^{2}})\). Then, we study a relaxed version of replicability proposed by Kalavasis et al. (2023) called TV _indistinguishability_. We design a computationally efficient TV indistinguishable algorithm for policy estimation whose sample complexity is \(((1/)}{(1-)^{5} ^{2}^{2}})\). At the cost of \((N)\) running time, we transform these TV indistinguishable algorithms to \(\)-replicable ones without increasing their sample complexity. Finally, we introduce the notion of _approximate_-replicability where we only require that two outputted policies are close under an appropriate statistical divergence (e.g., Renyi) and show an improved sample complexity of \(( ^{2}^{2}})\).

## 1 Introduction

When designing a reinforcement learning (RL) algorithm, how can one ensure that when it is executed twice in the same environment its outcome will be the same? In this work, our goal is to design RL algorithms with _provable_ replicability guarantees. The lack of replicability in scientific research, which the community also refers to as the _reproducibility crisis_, has been a major recent concern. This can be witnessed by an article that appeared in Nature (Baker, 2016): Among the 1,500 scientists who participated in a survey, 70% of them could not replicate other researchers' findings and more shockingly, 50% of them could not even reproduce their own results. Unfortunately, due to the exponential increase in the volume of Machine Learning (ML) papers that are being published each year, the ML community has also observed an alarming increase in the lack of reproducibility. As a result, major ML conferences such as NeurIPS and ICLR have established "reproducibility challenges" in which researchers are encouraged to replicate the findings of their colleagues (Pineau et al., 2019, 2021).

Recently, RL algorithms have been a crucial component of many ML systems that are being deployed in various application domains. These include but are not limited to, competing with humans in games (Mnih et al., 2013; Silver et al., 2017; Vinyals et al., 2019,, FAIR), creating self-driving cars (Kiran et al., 2021), designing recommendation systems (Afsar et al., 2022), providing e-healthcare services (Yu et al., 2021), and training Large Language Models (LLMs) (Ouyang et al., 2022). In order to ensure replicability across these systems, an important first step is to develop replicable RL algorithms. To the best of our knowledge, replicability in the context of RL has not received a formal mathematical treatment. We initiate this effort by focusing on _infinite horizon, tabular_ RL with a _generative model_. The generative model was first studied by Kearns and Singh (1998) in order to understand the statistical complexity of long-term planning without the complication of exploration. The crucial difference between this setting and Dynamic Programming (DP) (Bertsekas, 1976) is that the agent needs to first obtain information about the world before computing a _policy_ through some optimization process. Thus, the main question is to understand the number of samples required to estimate a near-optimal policy. This problem is similar to understanding the number of labeled examples required in PAC learning (Valiant, 1984).

In this work, we study three different formal notions of replicability and design algorithms that satisfy them. First, we study the definition of Impagliazzo et al. (2022), which adapted to the context of RL says that a learning algorithm is replicable if it outputs the exact same policy when executed twice on the same MDP, using _shared_ internal randomness across the two executions (cf. Definition 2.10). We show that there exists a replicable algorithm that outputs a near-optimal policy using \((N^{3})\) samples2, where \(N\) is the cardinality of the state-action space. This algorithm satisfies an additional property we call _locally random_, which roughly asks that every random decision the algorithm makes based on internal randomness must draw its internal randomness independently from other decisions. Next, we provide a lower bound for deterministic algorithms that matches this upper bound.

Subsequently, we study a less stringent notion of replicability called TV indistinguishability, which was introduced by Kalavasis et al. (2023). This definition states that, in expectation over the random draws of the input, the TV distance of the two distributions over the outputs of the algorithm should be small (cf. Definition 4.1). We design a computationally efficient TV indistinguishable algorithm for answering \(d\) statistical queries whose sample complexity scales as \((d^{2})\). We remark that this improves the sample complexity of its replicable counterpart based on the rounding trick from Impagliazzo et al. (2022) by a factor of \(d\) and it has applications outside the scope of our work (Impagliazzo et al., 2022; Esfandiari et al., 2023; Esfandiari et al., 2023; Bun et al., 2023; Kalavasis et al., 2023). This algorithm is inspired by the Gaussian mechanism from the Differential Privacy (DP) literature (Dwork et al., 2014). Building upon this statistical query estimation oracle, we design computationally efficient TV-indistinguishable algorithms for \(Q\)-function estimation and policy estimation whose sample complexity scales as \((N^{2})\). Interestingly, we show that by violating the locally random property and allowing for internal randomness that creates correlations across decisions, we can transform these TV indistinguishable algorithms to replicable ones without hurting their sample complexity, albeit at a cost of \(((N))\) running time. Our transformation is inspired by the main result of Kalavasis et al. (2023). We also conjecture that the true sample complexity of \(\)-replicable policy estimation is indeed \((N^{2})\).

Finally, we propose a novel relaxation of the previous notions of replicability. Roughly speaking, we say that an algorithm is _approximately replicable_ if, with high probability, when executed twice on the same MDP, it outputs policies that are close under a dissimilarity measure that is based on the _Renyi divergence_. We remark that this definition does not require sharing the internal randomness across the executions. Finally, we design an RL algorithm that is approximately replicable and outputs a near-optimal policy with \((N)\) sample and time complexity.

Table 1.1 and Table 1.2 summarizes the sample and time complexity of \(Q\)-estimation and policy estimation, respectively, under different notions of replicability. We assume the algorithms in question have a constant probability of success. In Appendix G, we further discuss the benefits and downsides for each of these notions.

### Related Works

**Replicability.** Pioneered by Impagliazzo et al. (2022), there has been a growing interest from the learning theory community in studying replicability as an algorithmic property. Esfandiari et al. (2023, 2023) studied replicable algorithms in the context of multi-armed bandits and clustering. Recently, Bun et al. (2023) established equivalences between replicability and other notions of algorithmic stability such as differential privacy when the domain of the learning problem is finite and provided some computational and statistical hardness results to obtain these equivalences, under cryptographic assumptions. Subsequently, Kalavasis et al. (2023) proposed a relaxation of the replicability definition of Impagliazzo et al. (2022), showed its statistical equivalence to the notion of replicability for countable domains3 and extended some of the equivalences from Bun et al. (2023) to countable domains. Chase et al. (2023), Dixon et al. (2023) proposed a notion of _list-replicability_, where the output of the learner is not necessarily identical across two executions but is limited to a small list of choices.

The closest related work to ours is the concurrent and independent work of Eaton et al. (2023). They also study a formal notion of replicability in RL which is inspired by the work of Impagliazzo et al. (2022) and coincides with one of the replicability definitions we are studying (cf. Definition 2.8). Their work focuses both on the generative model and the episodic exploration settings. They derive upper bounds on the sample complexity in both settings and validate their results experimentally. On the other hand, our work focuses solely on the setting with the generative model. We obtain similar sample complexity upper bounds for replicable RL algorithms under Definition 2.8 and then we show a lower bound for the class of locally random algorithms. Subsequently, we consider two relaxed notions of replicability which yield improved sample complexities.

**Reproducibility in RL.** Reproducing, interpreting, and evaluating empirical results in RL can be challenging since there are many sources of randomness in standard benchmark environments. Khetarpal et al. (2018) proposed a framework for evaluating RL to improve reproducibility. Another barrier to reproducibility is the unavailability of code and training details within technical reports. Indeed, Henderson et al. (2018) observed that both intrinsic (e.g. random seeds, environments) and extrinsic (e.g. hyperparameters, codebases) factors can contribute to difficulties in reproducibility. Tian et al. (2019) provided an open-source implementation of AlphaZero (Silver et al., 2017), a popular RL-based Go engine.

   Property & Sample Complexity & Time Complexity \\  Locally Random, Replicable & \((}{(1-)^{2}^{2}^{2}})\) & \((}{(1-)^{3}^{2}^{2}})\) \\ TV Indistinguishable & \((}{(1-)^{3}^{2}^{2}})\) & \((^{2}^{2}})\) \\ Replicable (Through TV Indistinguishability) & \((}{(1-)^{3}^{2}^{2}})\) & \((^{2}^{2}})\) \\ Approximately Replicable & \((^{2}^{2}})\) & \((^{2}^{2}})\) \\   

Table 1.2: Complexity Overview for Policy Estimation with Constant Probability of Success.

RL with a Generative Model.The study of RL with a generative model was initiated by Kearns and Singh (1998) who provided algorithms with suboptimal sample complexity in the discount factor \(\). A long line of work (see, e.g. Gheshlaghi Azar et al. (2013); Wang (2017); Sidford et al. (2018, 2018); Feng et al. (2019); Agarwal et al. (2020); Li et al. (2020) and references therein) has led to (non-replicable) algorithms with minimax optimal sample complexity. Another relevant line of work that culminated with the results of Even-Dar et al. (2002); Mannor and Tsitsiklis (2004) studied the sample complexity of finding an \(\)-optimal arm in the multi-armed bandit setting with access to a generative model.

## 2 Setting

### Reinforcement Learning Setting

**(Discounted) Markov Decision Process.** We start by providing the definitions related to the _Markov Decision Process_ (MDP) that we study in this work.

**Definition 2.1** (Discounted Markov Decision Process).: A _(discounted) Markov decision process (MDP)_ is a 6-tuple \(M=(,s_{0},=_{s}^{s },P_{M},r_{M},).\) Here \(\) is a finite set of states, \(s_{0}\) is the initial state, \(^{s}\) is the finite set of available actions for state \(s\), and \(P_{M}(s^{} s,a)\) is the transition kernel, i.e, \((s,s^{})^{2}, a^{s},P_{M}(s^{ } s,a) 0\) and \( s, a^{s},_{s^{} }P_{M}(s^{} s,a)=1\). We denote the reward function4 by \(r_{M}:\) and the discount factor by \((0,1)\). The interaction between the agent and the environment works as follows. At every step, the agent observes a state \(s\) and selects an action \(a^{s}\), yielding an instant reward \(r_{M}(s,a)\). The environment then transitions to a random new state \(s^{}\) drawn according to the distribution \(P_{M}( s,a)\).

**Definition 2.2** (Policy).: We say that a map \(:\) is a _(deterministic) stationary policy_.

When we consider randomized policies we overload the notation and denote \((s,a)\) the probability mass that policy \(\) puts on action \(a^{s}\) in state \(s\).

**Definition 2.3** (Value (\(V\)) Function).: The _value_\((V)\)_function_\(V_{M}^{}:[0,}{{(1-)}}]\) of a policy \(\) with respect to the MDP \(M\) is given by \(V_{M}^{}(s):=[_{t=0}^{}^{t}r_{M}(s_{t},a_{t})  s_{0}=s].\) Here \(a_{t}(s_{t})\) and \(s_{t+1} P_{M}( s_{t},a_{t})\).

This is the expected discounted cumulative reward of a policy.

**Definition 2.4** (Action-Value (\(Q\)) Function).: The _action-value (\(Q\)) function_\(Q_{M}^{}:[0,}{{(1- )}}]\) of a policy \(\) with respect to the MDP \(M\) is given by \(Q_{M}^{}(s,a):=r_{M}(s,a)+_{s^{}}P_{M}(s ^{} s,a) V_{M}^{}(s^{})\).

We write \(N:=_{s}^{s}\) to denote the number of state-action pairs. We denote by \(^{}\) the _optimal_ policy that maximizes the value function, i.e., \(,s\): \(V^{}(s):=V^{^{}}(s) V^{}(s)\). We also define \(Q^{}(s,a):=Q^{^{}}(s,a)\). This quantity is well defined since the fundamental theorem of RL states that there exists a (deterministic) policy \(^{}\) that simultaneously maximizes \(V^{}(s)\) among all policies \(\), for all \(s\) (see e.g. Puterman (2014)).

Since estimating the optimal policy from samples when \(M\) is unknown could be an impossible task, we aim to compute an \(\)-_approximately_ optimal policy for \(M\).

**Definition 2.5** (Approximately Optimal Policy).: Let \((0,1)\). We say that the policy \(\) is \(\)-approximately optimal if \( V^{}-V^{}_{}\).

In the above definition, \(||||_{}\) denotes the infinity norm of the vector, i.e., its maximum element in absolute value.

**Generative Model.** Throughout this work, we assume we have access to a _generative model_ (first studied in Kearns and Singh (1998)) or a _sampler_\(G_{M}\), which takes as input a state-action pair \((s,a)\) and provides a sample \(s^{} P_{M}( s,a)\). This widely studied fundamental RL setting allows us to focus on the sample complexity of planning over a long horizon without considering the additional complications of exploration. Since our focus throughout this paper is on the _statistical_ complexity of the problem, our goal is to achieve the desired algorithmic performance while minimizing the number of samples from the generator that the algorithm requires.

**Approximately Optimal Policy Estimator.** We now define what it means for an algorithm \(\) to be an approximately optimal policy estimator.

**Definition 2.6** ((\(,\))-Optimal Policy Estimator).: Let \(,(0,1)^{2}\). A (randomized) algorithm \(\) is called an \((,)\)-optimal policy estimator if there exists a number \(n:=n(,)\) such that, for any MDP \(M\), when it is given at least \(n(,)\) samples from the generator \(G_{M}\), it outputs a policy \(\) such that \(\|V^{}-V^{}\|_{}\) with probability at least \(1-\). Here, the probability is over random draws from \(G_{M}\) and the internal randomness of \(\).

Approximately optimal \(V\)-function estimators and \(Q\)-function estimators are defined similarly.

_Remark 2.7_.: In order to allow flexibility to the algorithm, we do not restrict it to request the same amount of samples for every state-action pair. Thus \(n(,)\) is a bound on the total number of samples that \(\) receives from \(G_{M}\). The algorithms we design request the same number of samples for every state-action pair, however, our lower bounds are stronger and hold without this restriction.

When the MDP \(M\) is clear from context, we omit the subscript in all the previous quantities.

### Replicability

**Definition 2.8** (Replicable Algorithm; ).: Let \(:^{n}\) be an \(n\)-sample randomized algorithm that takes as input elements from some domain \(\) and maps them to some co-domain \(\). Let \(\) denote the internal distribution over binary strings that \(^{}\) uses. For \((0,1)\), we say that \(\) is \(\)_-replicable_ if for any distribution \(\) over \(\) it holds that \(_{,^{}^{n},}(;)=(^{};) } 1-\,,\) where \((;)\) denotes the (deterministic) output of \(\) when its input is \(\) and the realization of the internal random string is \(\).

In the context of our work, we should think of \(\) as a randomized mapping that receives samples from the generator \(G\) and outputs policies. Thus, even when \(\) is fixed, \(()\) should be thought of as a random variable, whereas \((;)\) is the _realization_ of this variable given the (fixed) \(,\). We should think of \(\) as the shared randomness between the two executions, which can be implemented as a shared random seed.

One of the most elementary statistical operations we may wish to make replicable is mean estimation. This operation can be phrased using the language of _statistical queries_.

**Definition 2.9** (Statistical Query Oracle; ).: Let \(\) be a distribution over the domain \(\) and \(:^{n}\) be a statistical query with true value \(v^{}:=_{n}(X_{1},,X_{n})\). Here \(X_{i}_{i.i.d.}\) and the convergence is understood in probability or distribution. Let \(,(0,1)^{2}\). A _statistical query (SQ) oracle_ outputs a value \(v\) such that \(|v-v^{}|\) with probability at least \(1-\).

The simplest example of a statistical query is the sample mean \((X_{1},,X_{n})=_{i=1}^{n}X_{i}\). Impagliazzo et al.  designed a replicable SQ-query oracle for sample mean queries with bounded co-domain (cf. Theorem C.1).

The following definition is the formal instantiation of Definition 2.8 in the setting we are studying.

**Definition 2.10** (Replicable Policy Estimator).: Let \((0,1)\). A policy estimator \(\) that receives samples from a generator \(G\) and returns a policy \(\) using internal randomness \(\) is \(\)-replicable if for any MDP \(M\), when two sequences of samples \(,^{}\) are generated independently from \(G\), it holds that \(_{,^{} G,r}(;)=(^{};)} 1-\).

To give the reader some intuition about the type of problems for which replicable algorithms under Definition 2.8 exist, we consider the fundamental task of estimating the mean of a random variable. Impagliazzo et al.  provided a replicable mean estimation algorithm when the variable is bounded (cf. Theorem C.1). Esfandiari et al. [2023b] generalized the result to simultaneously estimate the means of multiple random variables with unbounded co-domain under some regularity conditions on their distributions (cf. Theorem C.2). The idea behind both results is to use a rounding trick introduced in Impagliazzo et al.  which allows one to sacrifice some accuracy of the estimator in favor of the replicability property. The formal statement of both results, which are useful for our work, are deferred to Appendix C.1.

Replicable \(Q\)-Function & Policy Estimation

Our aim in this section is to understand the sample complexity overhead that the replicability property imposes on the task of computing an \((,)\)- approximately optimal policy. Without this requirement, Sidford et al. (2018); Agarwal et al. (2020); Li et al. (2020) showed that \((N(}{{}})/[(1-)^{3}^{2}])\) samples suffice to estimate such a policy, value function, and \(Q\)-function. Moreover, since Gheshlaghi Azar et al. (2013) provided matching lower bounds5, the sample complexity for this problem has been settled. Our main results in this section are tight sample complexity bounds for locally random \(\)-replicable \((,)\)-approximately optimal \(Q\)-function estimation as well as upper and lower bounds for \(\)-replicable \((,)\)-approximately policy estimation that differ by a factor of \(}{{(1-)^{2}}}\). The missing proofs for this section can be found in Appendix D.

We remark that in both the presented algorithms and lower bounds, we assume local randomness. For example, we assume that the internal randomness is drawn independently for each state-action pair for replicable \(Q\)-estimation. In the case where we allow for the internal randomness to be correlated across estimated quantities, we present an algorithm that overcomes our present lower bound in Section 4.3. However, the running time of this algorithm is exponential in \(N\).

### Computationally Efficient Upper Bound on the Sample Complexity

We begin by providing upper bounds on the sample complexity for replicable estimation of an approximately optimal policy and \(Q\)-function. On a high level, we follow a two-step approach: 1) Start with black-box access to some \(Q\)-estimation algorithm that is not necessarily replicable (cf. Theorem D.2) to estimate some \(\) such that \(\|Q^{}-\|_{}_{0}\). 2) Apply the replicable rounding algorithm from Theorem C.2 as a post-processing step. The rounding step incurs some loss of accuracy in the estimated \(Q\)-function. Therefore, in order to balance between \(\)-replicability and \((,)\)-accuracy, we need to call the black-box oracle with an accuracy smaller than \(\), i.e. choose \(_{0}<O()\). This yields an increase in the sample complexity which we quantify below. For the proof details, see Appendix D.1.

Recall that \(N\) is the number of state-action pairs of the MDP.

**Theorem 3.1**.: _Let \(,(0,1)^{2}\) and \((0,}{{3}})\). There is a locally random \(\)-replicable algorithm that outputs an \(\)-optimal \(Q\)-function with probability at least \(1-\). Moreover, it has time and sample complexity \((N^{3}(}{{}})/[(1-)^{3}^ {2}^{2}])\)._

So far, we have provided a replicable algorithm that outputs an approximately optimal \(Q\) function. The main result of Singh and Yee (1994) shows that if \(\|-Q^{}\|_{}\), then the greedy policy with respect to \(\), i.e., \( s,(s):=*{argmax}_{a ^{}}(s,a)\), is \(}{{(1-)}}\)-approximately optimal (cf. Theorem D.3). Thus, if we want to obtain an \(\)-approximately optimal policy, it suffices to obtain a \((1-)\)-approximately optimal \(Q\)-function. This is formalized in Corollary 3.2.

**Corollary 3.2**.: _Let \(,(0,1)^{2}\) and \((0,}{{3}})\). There is a locally random \(\)-replicable algorithm that outputs an \(\)-optimal policy with probability at least \(1-\). Moreover, it has time and sample complexity \((N^{3}(}{{}})/[(1-)^{5}^ {2}^{2}])\)._

Again, we defer the proof to Appendix D.1.

Due to space limitation, the lower bound derivation is postponed to Appendix D.

## 4 TV Indistinguishable Algorithms for \(Q\)-Function and Policy Estimation

In this section, we present an algorithm with an improved sample complexity for replicable \(Q\)-function estimation and policy estimation. Our approach consists of several steps. First, we design a computationally efficient SQ algorithm for answering \(d\) statistical queries that satisfies the _total variation (TV) indistinguishability_ property (Kalavasis et al., 2023) (cf. Definition 4.1), which can be viewed as a relaxation of replicability. The new SQ algorithm has an improved sample complexity compared to its replicable counterpart we discussed previously. Using this oracle, we show how we can design computationally efficient \(Q\)-function estimation and policy estimation algorithms that satisfy the TV indistinguishability definition and have an improved sample complexity by a factor of \(N\) compared to the ones in Section 3.1. Then, by describing a specific implementation of its _internal_ randomness, we make the algorithm replicable. Unfortunately, this step incurs an exponential cost in the computational complexity of the algorithm with respect to the cardinality of the state-action space. We emphasize that the reason we are able to circumvent the lower bound of Appendix D.2 is that we use a specific source of internal randomness that creates correlations across the random choices of the learner. Our result reaffirms the observation made by Kalavasis et al. (2023) that the same learning algorithm, i.e., input \(\) output mapping, can be replicable under one implementation of its internal randomness but not replicable under a different one.

First, we state the definition of TV indistinguishability from Kalavasis et al. (2023).

**Definition 4.1** (TV Indistinguishability; (Kalavasis et al., 2023)).: A learning rule \(\) is \(n\)-sample \(\)-\(\) indistinguishable if for any distribution over inputs \(\) and two independent samples \(S,S^{}^{n}\) it holds that \(_{S,S^{}^{n}}[d_{}(A(S),A(S^{ }))]\,.\)

In their work, Kalavasis et al. (2023) showed how to transform any \(\)-\(\) indistinguishable algorithm to a \(2/(1+)\)-replicable one when the input domain is _countable_. Importantly, this transformation does not change the input \(\) output mapping that is induced by the algorithm. A similar transformation for finite domains can also be obtained by the results in Bun et al. (2023). We emphasize that neither of these two transformations are computationally efficient. Moreover, Bun et al. (2023) give cryptographic evidence that there might be an inherent computational hardness to obtain the transformation.

### TV Indistinguishable Estimation of Multiple Statistical Queries

We are now ready to present a \(\)-indistinguishable algorithm for estimating \(d\) independent statistical queries. The high-level approach is as follows. First, we estimate each statistical query up to accuracy \(}{{}}\) using black-box access to the SQ oracle and we get an estimate \(_{1}^{d}\). Then, the output of the algorithm is drawn from \((_{1},^{2}I_{d})\). Since the estimated mean of each query is accurate up to \(}{{}}\) and the variance is \(^{2}\), we can see that, with high probability, the estimate of each query will be accurate up to \(O()\). To argue about the TV indistinguishability property, we first notice that, with high probability across the two executions, the estimate \(_{2}^{d}\) satisfies \(||_{1}-_{2}||_{} 2/\). Then, we can bound the TV distance of the output of the algorithm as \(d_{}((_{1},^{2}I_{d}), (_{2},^{2}I_{d})) O()\)(Gupta, 2020). We underline that this behavior is reminiscent of the advanced composition theorem in the Differential Privacy (DP) literature (see e.g., Dwork et al. (2014)) and our algorithm can be viewed as an extension of the Gaussian mechanism from the DP line of work to the replicability setting. This algorithm has applications outside the scope of our work since multiple statistical query estimation is a subroutine widely used in the replicability line of work (Impagliazzo et al., 2022; Esfandiari et al., 2023; Esfandiari et al., 2023; Ba et al., 2023; Kalavasis et al., 2023). This discussion is formalized in the following theorem.

**Theorem 4.2** (TV Indistinguishable SQ Oracle for Multiple Queries).: _Let \(,(0,1)^{2}\) and \((0,}{{5}})\). Let \(_{1},,_{d}\) be \(d\) statistical queries with co-domain \(\). Assume that we can simultaneously estimate the true values of all \(_{i}\)'s with accuracy \(\) and confidence \(\) using \(n(,)\) total samples. Then, there exists a \(\)-\(\) indistinguishable algorithm (Algorithm A.1) that requires at most \(n(}{{2}},}{{2}})\) many samples to output estimates \(_{1},,_{d}\) of the true values \(v_{1},,v_{d}\) to guarantee that \(_{i[d]}|_{i}-v_{i}|\,,\) with probability at least \(1-\)._

### TV Indistinguishable \(Q\)-Function and Policy Estimation

Equipped with Algorithm A.1, we are now ready to present a \(\)-indistinguishable algorithm for \(Q\)-function estimation and policy estimation with superior sample complexity compared to the one in Section 3.1. The idea is similar to the one in Section 3.1. We start with black-box access to an algorithm for \(Q\)-function estimation, and then we apply the Gaussian mechanism (Algorithm A.1). We remark that the running time of this algorithm is polynomial in all the parameters of the problem.

Recall that \(N\) is the number of state-action pairs of the MDP.

**Theorem 4.3**.: _Let \(,(0,1)^{2}\) and \((0,}{{5}})\). There is a \(\)-\(\) indistinguishable algorithm that outputs an \(\)-optimal \(Q\)-function with probability at least \(1-\). Moreover, it has time and sample complexity \((N^{2}(}{{}})/[(1-)^{3}^ {2}^{2}])\)._Proof.: The proof follows by combining the guarantees of Sidford et al. (2018) (Theorem D.2) and Theorem 4.2. To be more precise, Theorem D.2 shows that in order to compute some \(\) such that \(\|-Q\|_{}\,,\) one needs \((N(}{{}})/[(1-)^{3}^{2}])\). Thus, in order to apply Theorem 4.2 the sample complexity becomes \((N^{2}(}{{}})/[(1-)^{3}^ {2}^{2}])\). 

Next, we describe a TV indistinguishable algorithm that enjoys similar sample complexity guarantees. Similarly as before, we use the main result of Singh and Yee (1994) which shows that if \(\|-Q^{*}\|_{}\), then the greedy policy with respect to \(\), i.e., \( s,(s):=*{argmax}_{a ^{*}}(s,a)\), is \(}{{(1-)}}\)-approximately optimal (cf. Theorem D.3). Thus, if we want to obtain an \(\)-approximately optimal policy, it suffices to obtain a \((1-)\)-approximately optimal \(Q\)-function. The indistinguishable guarantee follows from the data-processing inequality This is formalized in Corollary 4.4.

**Corollary 4.4**.: _Let \(,(0,1)^{2}\) and \((0,}{{5}})\). There is a \(\)-\(TV\) indistinguishable algorithm that outputs an \(\)-optimal policy with probability at least \(1-\). Moreover, it has time and sample complexity \((N^{2}(}{{}})/[(1-)^{5}^ {2}^{2}])\)._

### From TV Indistinguishability to Replicability

We now describe how we can transform the TV indistinguishable algorithms we provided to replicable ones. As we alluded to before, this transformation does not hurt the sample complexity, but requires exponential time in the state-action space. Our transformation is based on the approach proposed by Kalavasis et al. (2023) which holds when the input domain is _countable_. Its main idea is that when two random variables follow distributions that are \(\)-close in \(\)-distance, then there is a way to couple them using only _shared randomness_. The implementation of this coupling is based on the _Poisson point process_ and can be thought of a generalization of von Neumann's rejection-based sampling to handle more general domains. We underline that in general spaces without structure it is not known yet how to obtain such a coupling. However, even though the input domain of the Gaussian mechanism is _uncountable_ and the result of Kalavasis et al. (2023) does not apply directly in our setting, we are able to obtain a similar transformation as they did. The main step required to perform this transformation is to find a reference measure with respect to which the algorithm is _absolutely continuous_. We provide these crucial measure-theoretic definitions below.

**Definition 4.5** (Absolute Continuity).: Consider two measures \(P,\) on a \(\)-algebra \(\) of subsets of \(W\). We say that \(P\) is absolutely continuous with respect to \(\) if for any \(E\) such that \((E)=0\), it holds that \(P(E)=0\).

Recall that \((S)\) denotes the _distribution_ over outputs, when the input to the algorithm is \(S\).

**Definition 4.6**.: Given a learning rule \(\) and reference probability measure \(\), we say that \(\) is absolutely continuous with respect to \(\) if for any input \(S\), \((S)\) is absolutely continuous with respect to \(\).

We emphasize that this property should hold for every fixed sample \(S\), i.e., the randomness of the samples are not taken into account.

We now define what it means for two learning rules to be _equivalent_.

**Definition 4.7** (Equivalent Learning Rules).: Two learning rules \(,^{}\) are _equivalent_ if for every fixed sample \(S\), it holds that \((S)}{{=}}^{}(S)\), i.e., for the same input they induce the same distribution over outputs.

Using a coupling technique based on the Poisson point process, we can convert the TV indistinguishable learning algorithms we have proposed so far to equivalent ones that are replicable. See Algorithm A.2 for a description of how to output a sample from this coupling. Let us view \((S;r),(S^{};r)\) as random vectors with small TV distance. The idea is to implement the shared internal randomness \(r\) using rejection sampling so that the "accepted" sample will be the same across two executions with high probability. For some background regarding the Poisson point process and the technical tools we use, we refer the reader to Appendix C.2.

Importantly, for every \(S\), the output \((S)\) of the algorithms we have proposed in Section 4.1 and Section 4.2 follow a Gaussian distribution, which is absolutely continuous with respect to the Lebesguemeasure. Furthermore, the Lebesgue measure is \(\)-finite so we can use the coupling algorithm (cf. Algorithm A.2) of Angel and Spinka (2019), whose guarantees are stated in Theorem C.4. We are now ready to state the result regarding the improved \(\)-replicable SQ oracle for multiple queries. Its proof is an adaptation of the main result of Kalavasis et al. (2023).

**Theorem 4.8** (Replicable SQ Oracle for Multiple Queries).: _Let \(,(0,1)^{2}\) and \((0,}{{5}})\). Let \(_{1},,_{d}\) be \(d\) statistical queries with co-domain \(\). Assume that we can simultaneously estimate the true values of all \(_{i}\)'s with accuracy \(\) and confidence \(\) using \(n(,)\) total samples. Then, there exists a \(\)-replicable algorithm that requires at most \(n(}{{4}}[}}{{8d(4d/ )}},}{{2}})\) many samples to output estimates \(_{1},,_{d}\) of the true values \(v_{1},,v_{d}\) with the guarantee that \(_{i[d]}_{i}-v_{i}\,,\) with probability at least \(1-\)._

By using an identical argument, we can obtain \(\)-replicable algorithms for \(Q\)-function estimation and policy estimation. Recall that \(N\) is the number of state-action pairs of the MDP.

**Theorem 4.9**.: _Let \(,(0,1)^{2}\) and \((0,}{{4}})\). There is a \(\)-replicable algorithm that outputs an \(\)-optimal \(Q\)-function with probability at least \(1-\). Moreover, it has sample complexity \((N^{2}(}{{8}})/[(1-)^{3}^{2} ^{2}])\)._

**Corollary 4.10**.: _Let \(,(0,1)^{2}\) and \((0,}{{4}})\). There is a \(\)-replicable algorithm that outputs an \(\)-optimal policy with probability at least \(1-\). Moreover, it has sample complexity \((N^{2}(}{{8}})/[(1-)^{5}^{2} ^{2}])\)._

In Remark E.2 we explain why we cannot use a coordinate-wise coupling.

## 5 Approximately Replicable Policy Estimation

The definitions of replicability (cf. Definition 2.10, Definition 4.1) we have discussed so far suffer from a significant sample complexity blow-up in terms of the cardinality of the state-action space which can be prohibitive in many settings of interest. In this section, we propose _approximate replicability_, a relaxation of these definitions, and show that this property can be achieved with a significantly milder sample complexity compared to (exact) replicability. Moreover, this definition does not require shared internal randomness across the executions of the algorithm.

First, we define a general notion of _approximate_ replicability as follows.

**Definition 5.1** (Approximate Replicability).: Let \(,\) be the input and output domains, respectively. Let \(:_{ 0}\) be some distance function on \(\) and let \(_{1},_{2}(0,1)^{2}\). We say that an algorithm \(\) is \((_{1},_{2})\)-approximately replicable with respect to \(\) if for any distribution \(\) over \(\) it holds that \(_{S,S^{}^{n},r,r^{}}\{ ((S^{};r),(S^{};r^{}))_ {1}\}_{2}\,.\)

In words, this relaxed version of Definition 2.8 requires that the outputs of the algorithm, when executed on two sets of i.i.d. data, using _independent_ internal randomness across the two executions, are close under some appropriate distance measure. In the context of our work, the output of the learning algorithm is some policy \(:()\), where \(()\) denotes the probability simplex over \(\). Thus, it is natural to instantiate \(\) as some _dissimilarity measure_ of distributions like the total variation (TV) distance or the Renyi divergence. For the exact definition of these dissimilarity measures, we refer the reader to Appendix B. We now state the definition of an approximately replicable policy estimator.

**Definition 5.2** (Approximately Replicable Policy Estimator).: Let \(\) be an algorithm that takes as input samples of state-action pair transitions and returns a policy \(\). Let \(\) be some dissimilarity measure on \(()\) and let \(_{1},_{2}(0,1)^{2}\). We say that \(\) is \((_{1},_{2})\)-approximately replicable if for any MDP \(M\) it holds that \(_{S,S^{} G,r,r^{}}\{_{s }((s),^{}(s))_{1}\}_{2}\,\), where \(G\) is the generator of state-action pair transitions, \(\) is the source of internal randomness of \(\), \(\) is the output of \(\) on input \(S,r\), and \(^{}\) is its output on input \(S^{},r^{}\).

To the best of our knowledge, the RL algorithms that have been developed for the model we are studying do not satisfy this property. Nevertheless, many of them compute an estimate \(Q\) with the promise that \( Q-Q^{}_{}\)(Sidford et al., 2018a; Agarwal et al., 2020; Li et al., 2020). Thus, it is not hard to see that if we run the algorithm twice on independent data with independent internal randomness we have that \( Q-Q^{}_{} 2\). This is exactly the main property that we need in order to obtain approximately replicable policy estimators. The key idea is that instead of outputting the greedy policy with respect to this \(Q\)-function, we output a policy given by some _soft-max_ rule. Such a rule is a mapping \(^{}_{ 0}()\) that achieves two desiderata: (i) The distribution over the actions is "stable" with respect to perturbations of the \(Q\)-function. (ii) For every \(s\), the value of the policy \(V^{}(s)\) that is induced by this mapping is "close" to \(V^{}(s)\).

Formally, the stability of the soft-max rule is captured through its Lipschitz constant (cf. Definition B.3). In this setting, this means that whenever the two functions \(Q,Q^{}\) are close under some distance measure (e.g. the \(_{}\) norm), then the policies that are induced by the soft-max rule are close under some (potentially different) dissimilarity measure. The approximation guarantees of the soft-max rules are captured by the following definition.

**Definition 5.3** (Soft-Max Approximation; [Epasito et al., 2020]).: Let \(>0\). A soft-max function \(f:^{}_{ 0}()\) is \(\)-_approximate_ if for all \(x^{}\), \( f(x),x_{a}x_{a}-\).

In this work, we focus on the soft-max rule that is induced by the exponential function (\(\)), which has been studied in several application domains [Gibbs, 1902, McSherry and Talwar, 2007, Huang and Kannan, 2012, Dwork et al., 2014, Gao and Pavel, 2017]. Recall \((s,a)\) denotes the probability mass that policy \(\) puts on action \(a^{s}\) in state \(s\). Given some \(>0\) and \(Q(s,a)^{}\), the induced randomized policy \(\) is given by \((s,a)=^{s}} \{ Q(s,a^{})\}}\). For a discussion about the advantages of using more complicated soft-max rules like the one developed in Epasto et al. , we refer the reader to Appendix F.1.

We now describe our results when we consider approximate replicability with respect to the Renyi divergence and the Total Variation (TV) distance. At a high level, our approach is divided into two steps: 1) Run some \(Q\)-learning algorithm (e.g. [Sidford et al., 2018a, Agarwal et al., 2020, Li et al., 2020]) to estimate some \(\) such that \(\|Q^{}-\|_{}\). 2) Estimate the policy using some soft-max rule. One advantage of this approach is that it allows for flexibility and different implementations of these steps that better suit the application domain. An important lemma we use is the following.

**Lemma 5.4** (Exponential Soft-Max Approximation Guarantee; [McSherry and Talwar, 2007]).: _Let \((0,1),,p 1\), and set \(=}{{}}\), where \(d\) is the ambient dimension of the input domain. Then, \(\) with parameter \(\) is \(\)-approximate and \(2\)-Lipschitz continuous (cf. Definition B.3) with respect to \((_{p},D_{})\), where \(D_{}\) is the Renyi divergence of order \(\)._

This is an important building block of our proof. However, it is not sufficient on its own in order to bound the gap of the \(\) policy and the optimal one. This is handled in the next lemma whose proof is postponed to Appendix F. Essentially, it can be viewed as an extension of the result in Singh and Yee  to handle the soft-max policy instead of the greedy one.

**Lemma 5.5** (Soft-Max Policy vs Optimal Policy).: _Let \(_{1},_{2}(0,1)^{2}\). Let \(^{}\) be such that \(\|-Q^{}\|_{1}\). Let \(\) be the \(\) policy with respect to \(\) using parameter \(=|}}{{_{2}}}\). Then, \(\|V^{}-V^{}\|_{}+_{2})}}{{(1-)}}\)._

Combining Lemma 5.4 and Lemma 5.5 yields the desired approximate replicability guarantees we seek. The formal proof of the following result is postponed to Appendix F. Recall we write \(N:=_{s}^{s}\) to denote the total number of state-action pairs.

**Theorem 5.6**.: _Let \( 1,,,_{1},_{2}(0,1)^{4}\), and \((0,(1-)^{-1/2})\). There is a \((_{1},_{2})\)-approximately replicable algorithm \(\) with respect to the Renyi divergence \(D_{}\) such that given access to a generator \(G\) for any MDP \(M\), it outputs a policy \(\) for which \(\|V^{}-V^{}\|_{}\) with probability at least \(1-\). Moreover, \(\) has time and sample complexity \((N(}{{\{,_{2}\}}})/[(1-)^{5 }^{2}_{1}^{2}])\)._

## 6 Conclusion

In this work, we establish sample complexity bounds for a several notions of replicability in the context of RL. We give an extensive comparison of the guarantees under these different notions in Appendix G. We believe that our work can open several directions for future research. One immediate next step would be to verify our lower bound conjecture for replicable estimation of multiple independent coins (cf. Conjecture D.8). Moreover, it would be very interesting to extend our results to different RL settings, e.g. offline RL with linear MDPs, offline RL with finite horizon, and online RL.