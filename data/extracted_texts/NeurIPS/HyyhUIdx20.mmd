# Bandit-Driven Batch Selection

for Robust Learning under Label Noise

 Michal Lisicki

University of Guelph, Vector Institute

mlisicki@uoguelph.ca

&Mihai Nica

University of Guelph, Vector Institute

nicam@uoguelph.ca

&Graham W. Taylor

University of Guelph, Vector Institute

gwtaylor@uoguelph.ca

###### Abstract

We introduce a novel approach for batch selection in Stochastic Gradient Descent (SGD) training, leveraging combinatorial bandit algorithms. Our methodology focuses on optimizing the learning process in the presence of label noise, a prevalent issue in real-world datasets. Experimental evaluations on the CIFAR-10 dataset reveal that our approach consistently outperforms existing methods across various levels of label corruption. Importantly, we achieve this superior performance without incurring the computational overhead commonly associated with auxiliary neural network models. This work presents a balanced trade-off between computational efficiency and model efficacy, offering a scalable solution for complex machine learning applications.

## 1 Introduction

As applications increasingly demand larger and more complex deep learning models, the need for efficient training strategies has become paramount. One way to accelerate training and potentially improve model performance is through the use of Curriculum Learning (CL) and adaptive batch selection. These techniques optimize learning by selectively focusing on data samples that are intrinsically rich and informative at the most appropriate stages of the learning process. Such strategies not only accelerate convergence but also enhance the model's ability to generalize .

While many methods use difficulty metrics to select easy, hard, or uncertain instances for training , a key area lies in handling noisy or mislabeled datasets . This domain is particularly important for two reasons: a) the impact of batch selection strategies is easily measured, leading to more insightful conclusions; and b) it addresses the prevalent real-world scenarios where data is often sourced from the web  or crowdsourced , and a large portion is considered "unclean".

Sample selection strategies using auxiliary Deep Neural Networks (DNN) effectively mitigate the impact of noisy or mislabeled data. However, these approaches incur substantial computational overhead, limiting their scalability . While alternative methods like SELFIE  offer computational efficiency, they are under-explored and rely on steps like re-labeling for optimal performance. Meanwhile, the literature on CL and batch selection offers numerous methods for efficient sample selection across diverse domains .

This paper introduces a novel approach that synergizes insights from the CL and batch selection literature to enhance efficient sampling schemes, specifically targeting scenarios with prevalent label noise. Our methodology aims to achieve superior performance without the computational burdenoften associated with deploying additional DNNs, thereby striking a balance between efficacy and computational efficiency. Unlike traditional CL approaches that focus on individual instances or tasks, our method refines the feedback loop from each training iteration to optimize the _selected batch_. This approach is particularly relevant for tackling the challenges posed by the increasing computational complexity and diversity of machine learning applications across various domains.

## 2 Background

Batch selection and curriculum learningCL  and its variants like Self-Paced Learning (SPL)  and Hard-Example Mining (HEM) [6; 19] provide frameworks for adaptive instance, batch, or task selection based on difficulty or importance. Despite the efficacy of these strategies in enhancing stability and convergence, a universal solution remains elusive, prompting exploration into varied strategies, new importance metrics, and advanced re-weighting and sampling techniques.

Re-weighting the model's loss by instance, akin to importance-sampling techniques, has been investigated by [6; 19; 24]. These studies have indicated that re-weighting can stabilize gradient estimates and reduce bias in the original objective function. However, both Loshchilov and Hutter  and Chang et al.  have argued that the impact of this strategy on performance is limited, and that comparable or superior results can be achieved by sampling from a weight-induced distribution. Matiisen et al.  compared the _sample selection_ strategies, \(\)-greedy, Boltzmann, and Thompson sampling, and concluded that the optimal strategy hinges heavily on the sample weight metric. A novel metric not tied to difficulty was introduced by Chang et al. , emphasizing samples with high prediction uncertainty, and inspired by active learning. The authors demonstrated that, by avoiding overly easy or hard instances, their strategy surpassed SPL or HEM on datasets like MNIST and CIFAR with and without label noise. Song et al.  introduced Recency Bias to boost SGD's convergence by combining principles of  and . The technique centers on prediction uncertainty, measured by predicted label entropy, as its adaptive sample selection metric. It uses a Boltzmann distribution with energy based on prediction uncertainty and a pressure parameter. Leveraging a sliding window, it emphasizes recent scores, mitigating overfitting and slow convergence. Importantly, Active Bias, Recency Bias, and our approach add minimal computational load to model training.

Automated curriculum learning (ACL) distinguishes itself among CL approaches by the degree of control over the learning process. In ACL, the selection of tasks is determined dynamically using an algorithm, typically an RL or a bandit method. Graves et al.  and Matiisen et al.  have proposed utilizing a non-stationary bandit (Exp3). They demonstrated that when an agent lacks prior knowledge of its tasks, ACL can significantly boost training efficiency relative to uniform sampling. Moreover, a bandit algorithm can discover complex orderings and opportunities for efficient knowledge transfer in an unsorted curriculum. Although prior literature has focused primarily on task-based ACL, the same principles can be utilized in instance and batch selection.

This paper aims to build upon the mentioned foundational CL techniques by introducing efficient batch selection methods, particularly in the context of learning with noisy labels.

Efficient learning with noisy labelsLearning with Noisy Labels (LNL) shares a connection with batch selection but has a different objective. While batch selection picks instances that inherently aid training, LNL focuses on distinguishing between those with clean and noisy labels. Both fields converge when the right batch selection strategy is used to isolate the "clean" instances.

LNL is challenging due to DNNs' tendency to memorize complex and possibly incorrect instances, after initially learning simpler patterns [1; 34]. This can lead to memorization of inaccurate labels, compromising model generalization. Mislabeled data can also cause confirmation bias, which arises when models overfit to early-selected instances. Multi-network and co-training [9; 33] address these issues but add computational overhead and complexity. Both challenges highlight the need for robust training methods.

The "small-loss trick" is a commonly utilized tool for filtering out noisy labels by deeming instances with smaller losses as likely clean. While prevalent in DNN approaches (e.g., [9; 13; 17; 25; 33]), this method is not optimal when noisy and clean example distributions overlap significantly. Alternative methods, like measuring prediction uncertainty over time, have been explored as indicators of label corruption [6; 22; 26].

Similarly to batch selection, LNL methods can be broadly categorized into: loss correction and sample selection . Loss correction includes re-weighting or re-labeling. Active Bias  re-weights instances based on prediction variance, bridging LNL and batch selection. A method by Ren et al.  uses a clean validation set to dynamically assign weights. Re-labeling refines labels from a mix of noisy labels and DNN predictions, as seen in [23; 29; 32], effectively providing data augmentation. While innovative, these methods pose risks like overfitting to noisy labels and add complexity. Filtering out noisy labels, by contrast, provides a simpler approach but sacrifices information from label refinement. Sample selection filters out mislabeled data during training, often using an auxiliary DNN. MentorNet  stands out as a pivotal multi-network approach. It supervises a StudentNet by emphasizing "clean" instances and refining the learning trajectory based on feedback. Han et al.  proposed a "Co-teaching" paradigm, an alternative, where two DNNs are trained simultaneously and share insights on small-loss instances to diminish errors from noisy labels. "Co-teaching+"  tackles the risk of two networks reaching a consensus with an 'update by disagreement' strategy. The "deep abstaining classifier"  is a feature-based multi-network approach, which is particularly effective against structured noise. DivideMix  has demonstrated the state-of-the-art LNL performance by employing a semi-supervised (SSL) approach. It dynamically segregates training data into clean and noisy sets using a Gaussian Mixture Model (GMM) and the small-loss trick. To avoid confirmation bias, it utilizes co-teaching. The mislabeled instances are stripped of their labels and refined using SSL . Despite their ability to counteract confirmation bias, the multi-network training approaches often come with significant computational overhead.

SELFIE  is a hybrid approach, combining both loss correction and sample selection. SELFIE seeks to refurbish labels of unclean samples selectively, based on uncertainty, and leverage them along with clean samples, to further reduce false corrections while fully exploiting the current training data. While methods like SELFIE are computationally efficient and provide increase in performance, we argue that their selection strategies can be made better. In this paper we focus on improving the batch selection methodology, and compare performance to the pure selection method, Active Bias , and two bandit approaches -- the _Exponential-weight algorithm for Exploration and Exploitation_ (Exp3)  and _Follow the Perturbed Leader_ (FPL) [16; 21]. Details on these algorithms can be found in Sec. 3 and in Appendix, Sec. A.

Improving sampling efficiency by explorationThe challenge of balancing exploration and exploitation is inherent in the process of batch selection. This balance is crucial for identifying new instances that can enhance training efficiency and subsequently leverage them for optimal training outcomes. In addition, it is essential to account for the high degree of non-stationarity in neural network training. Specifically, a network can typically be trained on a particular data instance for only a few iterations before it risks overfitting. To address this issue, our approach aims to manage non-stationarity by dynamically adapting weight estimates. This adaptation can be achieved either through periodic reevaluation, which may be computationally expensive, or by employing a discounted moving average.

So far we are aware of only one study that has directly compared various exploration-exploitation strategies in the context of instance selection: its mixed results suggest such strategies depend highly on the researcher's choice of a sample weight metric . Although \(\)-greedy and UCB bandit methods have demonstrated effective performance in instance selection [6; 20], the Boltzmann exploration strategy has recently gained prominence in this subfield [5; 19]. In particular, the adversarial bandit -- Exp3 , which uses Boltzmann exploration, is commonly utilized as a baseline in non-stationary environments, and has been shown to be particularly effective in automated curriculum learning [8; 20].

Our work diverges from prior studies focused on choosing instances [6; 27] or tasks [8; 20], and targets batch selection instead. We utilized the FPL strategy, which can be thought of as a natural extension of Exp3 into combinatorial (batch), rather than individual (instance) action selection.

## 3 Methods

Adversarial multi-armed bandit problemA classic baseline approach for non-stationary environments is the adversarial bandit, in particular, the Exp3 algorithm and its variants. In an adversarial \(K\)-armed bandit problem, at each time step \(t\{1,2,...,T\}\), the player selects an action \(a_{t}\{1,2,...,K\}\) and then an adversary, with full knowledge of the player's previous actions, assignsa reward vector \(}=(r_{t,1},r_{t,2},...,r_{t,K})^{K}\) across all actions. The player receives a reward \(r_{t,a_{t}}\) corresponding to the selected action \(a_{t}\). There is typically almost no restrictions on how the adversary can choose the reward vectors, as long as the sequence of reward vectors \(},},...,}\) is fixed in advance or chosen based on the player's past actions. The player's goal remains to maximize the total collected reward or equivalently, to minimize regret.

Combinatorial bandits for batch selectionIn order to select a full batch of instances at once we need to utilize the combinatorial bandit paradigm, which considers the joint utility of combinations of "basic arms". Formally, combinatorial bandits can be considered a type of bandit where a subset of arms is selected in a form of a binary vector \(\{0,1\}^{d}\), and the final reward is derived from either a Hadamard or a dot product of that vector with the reward vector \(\). In this work we consider only the subset of a pre-specified batch size \(m\), s.t. \(||||_{1}=m\), and a semi-bandit reward model (see Appendix, Sec. A). A direct application of Exp3 to the semi-bandit problem would entail monitoring the sequence of estimates for \(\) arms, a task that is computationally infeasible. The state-of-the-art approach to semi-bandits is _Follow the Perturbed Leader_ (FPL) , which mimics Exp3, but estimates probabilities using reward perturbations, rather than storing them directly. FPL was originally introduced by Hannan  and Kalai and Vempala , with an efficient version operating on a principle of geometric re-sampling (GR) proposed by Neu and Bartok . In this work, we adapt the FPL algorithm to batch selection.

FPL operates over \(n\) rounds, maintaining a vector of weights \(w_{t,i}\) for each action \(_{i}\) in the action set \(\). In our case \(\) is a binary vector, such that setting an \(i\)-th action \(a_{i}=1\) corresponds to selecting an \(i\)-th instance \(_{i}\). Each round the algorithm perturbs the weights with noise \(_{t}\) from distribution \(Q\), selecting the action \(_{t}\) that maximizes the inner product with the perturbed weight vector. While Neu and Bartok  used the \((1)\) distribution for \(Q\), recent work by Honda et al.  suggests the \((2)\) (also known as inverse Weibull) distribution yields optimal regret in adversarial settings. As opposed to the algorithms presented in literature  we estimate reward, rather than loss associated with each arm. We have found this adaptation to significantly improve performance in our application, however we acknowledge that while this algorithm remains in line with reward estimation done in Exp3, the original theoretical performance guarantees for combinatorial arm selection may no longer apply.

``` Data:\(,n,,M,Q\)
1for\(i=1\)to\(d\)do
2\(w_{0,i}\) 0 // Initialize weight vector
3for\(t=1\)to\(n\)do
4 Sample \(_{t} Q\) // Sample weight perturbations
5 Compute \(}=_{}, _{t-1}+_{t}\) // Choose combinatorial action
6\(}_{a_{t}}\) // Draw reward vector from arm \(_{t}\) of MAB \(\)foreach\(i\) with \(a_{t,i}=1\)do // Geometric Re-sampling
7 Sample \(_{t,i}(p_{t,i})\)
8\(_{t,i}=\{M,_{t,i}\}a_{t,i}r_{t,i}\) // Compute bounded reward estimate
9\(w_{t,i}=w_{t-1,i}+_{t,i}\) // Update weight of chosen action ```

**Algorithm 1**Follow The Perturbed Leader (Reward-guided)

Following action selection, for each \(i\) where \(a_{t,i}=1\), the algorithm proceeds with a geometric re-sampling (GR) step. Sampling from the geometric distribution estimates \(1/p_{i}\) and in practice is not done directly, but rather by sampling arms from \(Q+_{t-1}\) and counting the number of iterations to re-occurance. \(M\) is the cap on sampling size, to trade off computational efficiency with estimation accuracy. The algorithm draws a sample \(_{t,i}\) from the approximated geometric distribution, and computes a bounded reward estimate \(_{t,i}\) in the same way as Exp3, as an importance-weighted estimate, by \(a_{t,i}_{t,i}r_{t,i}\).1

Finally, the algorithm updates the weight \(w_{t,i}\) of the chosen action \(a_{i}\) by adding the reward estimate \(_{t,i}\) to the previous weight \(w_{t-1,i}\). This process continues for \(n\) rounds, enabling the algorithm to effectively explore and exploit the action space by balancing the current estimated rewards and the exploration noise introduced by perturbations. While FPL may require more computational resources compared to Exp3, it offers the advantage of reducing dependency on the combinatorial action space. This makes FPL a practical choice for real-world sequential decision-making tasks.

Label noiseAccording to Song et al. (2018), label noise can be either instance-independent, characterized by constant rates and probabilities, or instance-dependent, where corruption probabilities vary with data features and true labels. This study concentrates on symmetric, instance-independent noise to provide a baseline in a controlled setting.

LNL weight metricChoosing the right metric to select informative instances is still an open problem. In the field of LNL, metrics based on prediction loss (Kang et al., 2017; Song et al., 2018; Li et al., 2019; Li et al., 2020) and prediction uncertainty (Kang et al., 2017; Song et al., 2018; Li et al., 2020) have shown particular promise. The following metric was proposed by Chang et al. (2018) as part of the Active Bias method:

\[w_{i}}(p_{_{i}^{t-1}}(y_{i}| _{i})),\]

where, for each instance \(_{i}\), it saves prediction probabilities for their target class over time in a history buffer \(_{i}\), and then computes their variance.

We employed this metric in our study, as it was shown suitable both for LNL and for batch selection in general. Unlike the metrics that are derived from the change in the state of the model, the probability-based metrics reflect the model's current confidence in its predictions, rendering them independent of the target solutions, and therefore, consistent across instances. This property makes them inherently balanced for problems such as LNL. However, it should be noted that while these metrics offer advantages, they do not directly track the progression of training. Therefore, following Song et al. (2018), we limit the size of the history to 10 predictions.

The estimated weights serve two main purposes: either to re-weight the loss as in (Song et al., 2018) or to parameterize the probability distribution over data instances. The latter often employs a Boltzmann distribution (e.g. (Kang et al., 2017; Song et al., 2018)): \(P_{s}(i|H,S_{e},D)=e^{w_{i}/}/Z\) where \(Z\) is the normalization constant, \(H\) denotes the history of scores (e.g. instance losses or prediciton probabilities), \(S_{e}\) is the set of samples used in the current epoch, and \(D=\{(_{i},y_{i}) i=1,2,,N\}\) represents the dataset. Given our interest in the role of exploration in sample efficiency, we primarily focus on sampling methods underpinned by bandit algorithms such as Exp3, which also employs a Boltzmann-like distribution.

## 4 Results

Experimental SetupWe evaluated the performance of various sampling methods including Uniform Sampling, Active Bias, Exp3, and FPL on the CIFAR-10 dataset using a DenseNet model (Krizhevsky et al., 2015) with 40 layers. We used the Adam optimizer with momentum 0.9 and an initial learning rate of 0.1 that is decayed by multiplicative factor of 0.1 after 40 k and 60 k iterations. The batch size was set to 128 and we ran 200 epochs, consisting of 391 batches each. All methods were repeated 5 times with different seeds, under varying label corruption percentages ranging from 0% to 50%. We report the mean and 95% confidence intervals (CI) of test accuracy achieved by each method. We will release a PyTorch implementation to reproduce our experiments upon paper acceptance.

Figure 1: Test error over the course of training with confidence intervals (CI) over 5 runs for Uniform, Active Bias (weighted), Exp3 and FPL, for label noise ratio \(\{0.1,0.3,0.5\}\).

ResultsUnder all label corruption scenarios, FPL exhibited significantly reduced noise and superior performance compared to the other methods, with Exp3 outperforming Active Bias, and Active Bias performing better than Uniform Sampling. In conditions with no label noise, no significant improvement was observed across methods, revealing a potential limitation in sensitivity to "hard to classify" instances and an overfocus on mislabeling.

DiscussionThe methods maintained a consistent ranking across noise levels, with the performance gap widening as noise increased (see Fig. 1 and 2). FPL consistently yielded smooth and stable convergence, due to its ability to choose informative instances. When adopting the same weight metric and neural network architecture as Active Bias, our results show that implementing a bandit strategy can lead to significant performance gains. This underscores the importance of not just selecting an optimal weight metric, but also employing a beneficial exploration policy.

Analysis of instance occurrences (Fig. 3) reveals insights into the differences in sampling strategies, addressing our initial inquiry into performance gain from utilizing batch- as opposed to instance-based feedback. Initially (Fig. 2(a)), all methods show similar selection frequencies, but distinctions emerge as training concludes (Fig. 2(b)), especially for Exp3 and FPL. Notably, the algorithm's pattern of concentration on specific instances correlates well with its performance. While Exp3 resembles an exponential distribution, FPL produces a threshold at about 20 k instances, filtering 30 k of the remaining images. The preference for 'clean' instances between the 10 k and 20 k sorted index intensifies towards the end of training, indicating the algorithm's inci

Figure 3: Analysis of instance selection for Uniform, Exp3, and FPL\({}^{1}\) with 20% label noise, showing selection occurrence during the initial (a) and final (b) 1000 iterations and total proportion of mislabeled instances (c; front) over total occurrences (c; background). The order of curves at index 0 aligns well with the overall performance of the methods, revealing a concentration of selection in Exp3 and FPL, particularly pronounced in FPL, with Exp3 demonstrating overfitting to a limited set of instances.

Figure 2: Left: Lowest and final epoch test errors (%) for each method on CIFAR-10 by noise ratio. Right: Visualizing last epoch performance.

initially deemed 'clean'. These insights emphasize FPL's efficiency as an \(m\)-set combinatorial bandit method, and highlight its suitability for batch selection.

In Fig. 2(c), we display curves representing total counts throughout the run, providing a holistic view of each method's sample selection strategy. Over these, with solid lines, we superimpose the percentage of mislabeled instances within a sliding window of 1000 sorted instances. Each point on the overlay represents the mislabeling percentage within that window, revealing a trend: instances sampled less frequently (toward the right) have higher mislabeling percentages. This visualization supports our hypothesis that bandit methods with uncertainty-based metrics, like Exp3 and FPL, enhance performance by focusing on and filtering out mislabeled instances.

While Exp3 effectively identifies mislabeled instances like FPL (Fig. 2(c)), it tends to overfit to a narrow set and over-explore the rest (Fig. 2(b)). This is expected as Exp3 is an instance-based algorithm. However, this overfitting poses risks to its efficacy, as consistently selecting the same subset of impactful instances, combined with a broad array of less pertinent ones, leads to lower performance. Conversely, FPL, by adjusting weights in accordance with other instances, revisits a larger, more balanced subset regularly, forming more informative and consequential batches, ensuring optimal selection in batch training scenarios.

As weights play a crucial role in understanding the learning process in depth, we further analyze the weight dynamics of FPL in Fig. 4. Initially set to 0, the weights adjust smoothly throughout training, maintaining a balance in instance selection without anomalies or overfitting. There is a 40%-60% split in instance selection (Fig. 2(b)) that is clearly reflected in the weights, with those corresponding to highly informative instances increasing rapidly, whereas those consistently labeled (either correctly or as mislabeled) remaining closer to zero. It is worth noting here that all instances were selected at least once, with all weights turning strictly positive by the end of the run. Entropy visualization (Fig. 3(a)) further emphasizes effective convergence on a well-sized subset of instances, reinforcing selection for better exploitation without excessive exploration.

Scalability and hyperparameter sensitivityWe ran our experiments using \(=0.3\) and \(=0.1\) for Exp3 and \( 18\), \( 20\), and the \((0.45)\) distribution for FPL. To show that FPL has small sensitivity to these hyperparameters, we ran a grid search in their vicinity (see Fig. 5). We found the number of GR samples to be optimal between 500 and 1000. In that range GR introduces an additional computational overhead of 20%-40%. This may seem alarming at first, however, we point out that GR is _embarrassingly parallelizable_ and instance-based, which makes it scalable in practical applications.

Limitations and future directionsExp3's slower adaptation and potential benefits of its variants like Exp3.P or Exp3.IX warrant further consideration. FPL excels in balancing exploration and exploitation but shows limited improvement in noise-free scenarios, suggesting a potential overfocus on mislabeled instances. While all methods generalize well, tests were limited in scope. Future work will include naturally noisy sets like WebVision , as well as metrics like area under the margin (AUM) , to deepen insights and enhance results.

Figure 4: Instance weight visualization. Entropy-aggregated weights are visualized over time in (a). Once all instances are selected by epoch 40, the entropy gradually declines as certain instances gain importance. This pattern indicates effective diversification without overfitting. In (b), a random subset of weights is displayed over time to further validate their individual trajectories. In (c) the lowest weights are shown to dynamics of weights that remain close to 0.

## 5 Conclusions

This investigation into the performance of sampling methods under different noise conditions has revealed key insights into their adaptability, stability, and algorithmic nuances. FPL's effective balance between exploration and exploitation, particularly its focus on uncertain instances, underscores its superior performance. Nonetheless, the absence of marked improvement in noise-free settings and the limited scope of our experiments highlight avenues for future research and refinement.