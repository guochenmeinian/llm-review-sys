# Wings: Learning Multimodal LLMs

without Text-only Forgetting

 Yi-Kai Zhang\({}^{1,2,3}\) Shiyin Lu\({}^{3}\) Yang Li\({}^{3}\) Yanqing Ma\({}^{3}\) Qing-Guo Chen\({}^{3}\)

Zhao Xu\({}^{3}\) Weihua Luo\({}^{3}\) Kaifu Zhang\({}^{3}\) De-Chuan Zhan\({}^{1,2}\) Han-Jia Ye\({}^{1,2}\)

\({}^{1}\)School of Artificial Intelligence, Nanjing University

\({}^{2}\)National Key Laboratory for Novel Software Technology, Nanjing University

\({}^{3}\)Alibaba International Digital Commerce

Work done during the internship at Alibaba International Digital Commerce.Corresponding author, email: yehj@lamda.nju.edu.cn.

###### Abstract

Multimodal large language models (MLLMs), initiated with a trained LLM, first align images with text and then fine-tune on multimodal mixed inputs. However, during the continued training, the MLLM catastrophically forgets the text-only instructions that the initial LLM masters. In this paper, we present Wings, a novel MLLM that excels in both text-only and multimodal instructions. By examining attention across layers of MLLM, we find that _text-only forgetting_ is related to the attention shifts from pre-image to post-image text. From that, we construct an additional Low-Rank Residual Attention (LoRRA) block that acts as the "modality learner" to expand the learnable space and compensate for the attention shift. The complementary learners, like "wings" on either side, are connected in parallel to each layer's attention block. The LoRRA mirrors the structure of attention but utilizes low-rank connections to ensure efficiency. Initially, image and text inputs are aligned with visual learners operating alongside the main attention, balancing focus on visual elements. Later, textual learners are integrated with token-wise routing, blending the outputs of both modality learners collaboratively. Our experimental results demonstrate that Wings outperforms equally-scaled MLLMs in both text-only and visual question-answering tasks. Wings with _compensation of learners_ addresses text-only forgetting during visual modality expansion in general MLLMs.

## 1 Introduction

Large Language Models (LLMs)  are making significant strides toward Artificial General Intelligence (AGI) systems. Multimodal Large Language Models (MLLMs), as a visual expansion of LLMs, have demonstrated astonishing performance in vision-related captioning , understanding , and reasoning . Common MLLMs build upon powerful pre-trained LLMs that take mixed textual and visual tokens as inputs. The visual ones are acquired using an image encoder and a projector. We describe instructions processed by the LLM without images as _text-only instructions_. In comparison, _multimodal instructions_ incorporate visual feature tokens into text-only sequences. Modality fusing at the token level provides a flexible and effective pipeline for training MLLMs to comprehend visual information . However, training on multimodal instructions seems to impair the pre-existing profound knowledge, especially making MLLM forget how to respond to text-only instructions like the initial LLM . MLLM experiences a drastic performance decline on text-only evaluation. We term it as the _text-only forgetting_ of MLLM.

In practical applications, MLLMs also require engaging in text-only or interleaved conversations. As demonstrated in Figure 1, users often start with text-only inquiries and then, if not fully satisfied with the response, proceed to supplement questions with visual content. For multimodal instructions, MLLMs still rely on text to capture critical elements, as images may offer redundant information . The first existing approaches replay extensive text-only or interleaved  training data to mitigate catastrophic forgetting in MLLMs . However, increasing training data incurs additional computational overhead and data collection challenges. Secondly, some applications  switch between LLM and MLLM based on whether images are included. This intuitive solution inevitably demands more deployment memory  and is less cache-friendly in long vision-and-language interleaved conversations . Therefore, it is crucial to train MLLM while preserving the text-only performance efficiently.

Given that the visual input tokens can be inserted at any position within the text sequence, we begin by examining the text before and after the inserted position to mark the impact of the visual part. Considering that MLLM's attention weights reflect the focus on tokens and influence the decision-making process, we first analyze the attention weights across each layer of the MLLM. Specifically, for each layer, we compute the attention weight proportion on all text tokens before and after the inserted image, termed as Layer-level Attention Weights (Laws) of the before and after image text. From this, we examine the dynamic of attention across all layers as MLLM-Laws. Through training and sampling over \(100\) diverse MLLMs, we find that a well-trained model with superior text-only performance shows a positive correlation of MLLM-Laws between the text segments before and after the image. This suggests that in a well-structured feature space, the main branch attention on text exhibits similar trends across layers, which is statistically linked to the semantic similarity of the text around the visual part. A closer similarity indicates minor disruption to MLLM's core attention, while a negative correlation shows that excessive focus on visual tokens shifts attention away from the text, significantly impacting MLLM-Laws.

Based on this observation, we propose Wings, which introduces an extra module that acts as the boosted learner to compensate for the attention shift. We integrate complementary visual and textual learners in parallel at each layer's attention block, with visual learners enhancing focus on visual tokens and textual learners on text, respectively. In the first stage, visual features align with textual feature tokens, with all visual learners operating parallel to the main branch attention. The visual learners allocate some attention to visual tokens, mitigating the attention shift in the main branch. Subsequently, textual learners are integrated in parallel. We implement token-wise soft-routing based on shifted attention weights to harmonize the learning on visual and textual tokens. We design the Low-Rank Residual Attention (LoRRA) as the architecture for learners to ensure high efficiency. Figure 3 shows that the visual and textual learners on either side, like light feathers woven into "wings". Experiments show that our Wings comprehensively achieves superior performance in text-only under the same training condition and exceeds other equal-level MLLMs on multimodal benchmarks. In addition, we construct the Interleaved Image-Text (IIT) benchmark with multi-turn

Figure 1: **Examples of text-only and multimodal conversations. From left to right: Interacting with MLLM through _text-only_ and _interleaved instructions_; Performance radar charts for Wings, LLaVA-Next , and DeepSeek-VL  in _text-only_ and _multimodal_ QA tasks, with dark green indicating Wings with the comprehensive performance; Interacting with _multimodal instructions_.**

evaluations towards a general mixed-modality scenario. The samples are from text-only questions to strongly image-related conversations. Wings achieve leading performance across various vision-relevance partitions. Overall, our contributions are as follows: (**1**) We claim and verify the text-only forgetting phenomenon of MLLM is related to the attention shift of cross-layer MLLM-Laws before and after the image. (**2**) Wings construct the visual and textual learners and introduce a router based on shifted attention weights for collaborative learning to compensate for attention shifts. (**3**) Experiments on text-only, visual-question-answering, and newly constructed Interleaved Image-Text (IIT) benchmarks demonstrate the comprehensive and versatile performance of Wings.

## 2 A Closer Look at Attention Shift in Multimodal LLMs

In this section, we introduce the development from initialized LLM to MLLM. Next, we devise the MLLM-Laws metric for representing attention shift and discuss the insights in building Wings.

### Granting Sight to Large Language Models

**Large Language Models (LLMs).** Even though existing Transformer-based  models [20; 83; 100; 134] like BERT  and OPT  have demonstrated profound language understanding capabilities, there has been a recent surge in powerful Generative Pre-trained Transformers (GPT)  under the auto-regressive language modeling paradigm. Both public [54; 55; 115; 116] and private [3; 93; 95; 112] solutions show remarkable progress in language comprehension and generation [91; 126]. These LLMs generally exceed a billion parameters, including pre-training [22; 32; 50; 56], supervised fine-tuning with instructions [26; 104; 110; 125], and reinforcement learning from human feedback [23; 96; 107; 152] on massive training data.

**Multimodal LLMs (MLLMs).** Integrating visual inputs into foundational LLMs to create MLLMs is becoming increasingly popular [18; 19; 63; 72; 136]. Unlike vision-centric multimodal frameworks [69; 137] such as CLIP series , MLLMs aim to align new modality features as the input of LLM with an additional encoder and perform multimodal question-answering [75; 80; 81; 128; 141; 150]. As illustrated in Figure 2 (a), it enables the combined training of mixed multimodal tokens, facilitating rapid deployment across various applications [24; 25; 45; 82; 122; 145]. One example of this pipeline is the LLaVA  series, which integrates a CLIP vision encoder with a linear projection to LLM Vicuna  and innovatively introduces instruction-following training data. Following this, some methods consider the richness of the vision-related training context [14; 46; 62], the scaled visual backbone [52; 73; 79], or the enhanced connectors [11; 124] to boost the visual effectiveness of MLLMs. Additionally, some works introduce monolithic multimodal solutions [30; 88; 111; 123]. Recently, some work has focused on the general capabilities of MLLM, specifically their performance on new modalities without suffering catastrophic forgetting of the text-only question-answering skills initially mastered by LLM [38; 74; 90]. For example, DeepSeek-VL  suggests that supplementing additional text-only training data can mitigate this forgetting. Others [78; 90; 114] try to incorporate interleaved visual-textual data into training to retain language knowledge. However, these methods are limited by training resources and data collection costs. We aim to preserve or even boost performance with text-related training data as little as possible. Some studies [66; 77; 106; 113; 135; 143] also consider expanding the scalability of LLM, such as using Mixture-of-Expert (MoE) with numerous parallel FFNs in the Transformer block alongside a sparse gating network for efficient selection [108; 148; 149]. There are some methods to configure effective information and feedback examples to enhance in-context learning abilities [131; 132]. These methods, however, require a massive increase in training parameters or inference costs. In Wings, the newly designed parallel learners of Low-Rank Residual Attention (LoRRA) are similar to _MoE on attention block_, but with at least three orders of magnitude less in resource consumption. Compared to some LoRA-related methods [44; 47], Wings focuses on parallel processing within the attention block rather than in certain in-block linear mappings [89; 121], particularly addressing the issue of capability forgetting in existing architectures.

### Capturing the Attention Shift with MLLM-Laws

The significant decline in text-only performance is closely linked to the observed related shift during the training process. Research on cross-modal learning [35; 67; 74] shows that transferring to new modalities affects feature distribution, output values, and activation levels. Considering attention weights highlight where MLLM's focus depends on visual or textual tokens for decision-making , we investigate how attention shifts among _different parts of the sequences_, mainly where divided by the visual feature tokens. Specifically, we study over \(100\) diverse MLLMs to uncover how attention is allocated to each part for a text-only better MLLM. We take a closer look at the cross-layer dynamic curve of attention proportion on all text tokens _before_ and _after_ the inserted image.

For a instruction \(\) and its hidden states in MLLM as \(=[_{1},_{2},,_{s}]\) consisting of \(s\) mixed visual and textual tokens. Let \(_{ij}^{l}\) represent the attention weight between the \(i^{}\) and \(j^{}\) tokens in the \(l^{}\) of the \(L\)-layers MLLM. We have, for \( i\), \(_{j=0}^{s}_{ij}^{l}(^{l-1})=1\). As shown in Figure 2 (a), since the sequence of flattened visual tokens is continuously interleaved with the textual sequence, we denote the index set of the visual tokens as \(_{}=\{v_{},v_{}+1,,v_{ }\}\). We refer to the textual sequence before the visual tokens as \(_{}\), and similarly, after the visual part as \(_{}\). For an MLLM with \(L\) layers, we define the Layer-level Attention Weights (MLLM-Laws) as:

\[_{_{*}}=[_{_{*}}^{1},_{_{*}}^{2},_{_{*}}^{L}]\,\ \ _{_{*}}^{l}=_{i=0}^{s}_{j_{*}} _{ij}^{l}(^{l-1})\,\] (1)

where token index set \(_{*}\) can be \(_{}\), \(_{}\), or \(_{}\) as mentioned above, and for simplicity, we omit \(^{l-1}\) in \(_{_{*}}^{l}\). \(()\) of \(_{_{*}}\). In practice, \(_{_{*}}\) characterizes the MLLM's attention on the current sequence \(_{}\), \(_{}\), or \(_{}\) regarding the dynamic curve over all MLLM-layers. As shown in Figure 2 (b), the attention to the textual part initially increases and then decreases as the layers progress, while the trend for the visual one is often the opposite. We find that when the MLLM forgets the text-only instructions, the \(\) of the textual sequence after the visual ones show a deviation from the initial trend of rising and then declining. This implies a shift of layer-level attention in the text following the image \(_{}\) compared to that preceding the image \(_{}\). The dynamics labeled as 1 in Figure 2 (b) show the red curve for better text-only performance towards the worse blue one. To quantify this, we compute the Pearson Correlation Coefficient  between \(\) before and after the visual sequence. Formally,

\[=_{}[-(_{ _{}},_{_{}}) ]+1\.\]

Studying the attention shift of over \(100\) diverse MLLMs, we find a positive correlation between the shift and the text-only performance degradation. In Figure 2 (c), each point represents a trained

Figure 2: **Illustration of mixed visual-and-textual inputs and the Layer-level Attention Weights (Laws) with its properties.** (a) The visual feature tokens from the visual encoder and projector are inserted into the textual feature sequence. (b) The attention weight proportion on textual tokens before-image, image-itself, and after-image across layers. The red curve is from the superior text-only MLLM, while the blue curve is from the inferior one. (c) Experiments on over \(100\) MLLMs show a positive correlation from the \(\) for MLLM-Laws before and after the visual tokens (\(x\)-axis) to the text-only performance of the MLLM (\(y\)-axis).

[MISSING_PAGE_FAIL:5]

projected information. Specifically, for the \(l^{}\) layer, the visual/text-only learner is formulated as:

\[^{*}(^{l},_{*})_{*\{\}}=(^{l}(+^{ })(_{*}(+^{}))^{}}{}}})_{*}( +^{})^{}\,\] (4)

where the matrix \(^{}\), \(^{}\), \(^{}\), and \(^{}\) is low-rank and is obtained by the dot product of \(_{a}^{d d}\) and \(_{b}^{d d}\), and \(\) is relatively small enough. The symbol \(\) is represented as the identity matrix. The structure of multihead LoRRA preserves the effectiveness of the cross-attention structure and employs efficient low-rank mapping to reduce computational demands. Following LoRA , LoRRA learners also employ random Gaussian initialization for \(_{a}\) and sets \(_{b}\) to zero. Since \(^{}\) lacks a residual, the output of LoRRA is zero at the beginning of training. As shown in Figure 3, the visual and textual features are fed into their respective side learners, like two "wings" woven together. The outputs of two learners from each layer are then weighted sum to the attention of the main branch. As illustrated in the left part of Figure 4, a router receives attention weights to generate the balance weights of visual and textual learners for each token. In summary, we formulate the Wings block as:

\[^{}=^{}+_{*\{\}} ()^{*}( ^{l},_{*})\,\] (5)

where \(^{s s}\) represents the attention weights of the current main branch. The router is formalized as \(()=[:,:s]^{}\), which is implemented by a single-layer dynamic \(\), \(^{2 s_{}}\). It receives the attention weights \(\) and processes them using \(\) on two modality learners.

### Stable Training Recipe

The architecture of Wings comprises four elements: vision encoder, projector, initialized LLM, and the learners with a router. During the training process, the vision encoder is consistently fixed. Firstly,

Figure 3: **The Wings - model architecture. We introduce extra modules parallel to the main attention, serving as boosted learners to compensate for the attention shift. We train the visual learners on one side, alleviating some shifted attention. Then, we collaboratively learn visual and textual learners based on routing shifted attention weights. They are like light feathers woven “wings”.**

we only fine-tune the projector and visual learners. We primarily employ image-text pairs for visual alignment, while the outputs of visual learners are directly added to the main branch. For this part, the visual learners primarily handle the visual focus, minimizing disturbances to the main branch during continued learning. Subsequently, textual learners are paralleled with visual learners on the attention block of LLMs. The router begins by learning to allocate visual and textual learners from the attention weights of the main branch. At this stage, both types of learners work more effectively together to focus attention on key tokens. To summarize, Wings prioritizes enhancing visual learners first. Subsequently, it "spreads its wings" by concurrently learning and routing visual and textual learners based on shifted attention weights. During inference, the routed weights of the visual wings branch are deactivated for text-only instructions, while multimodal instructions activate both wings.

## 4 Experiments

In this section, we first introduce the benchmarks for evaluating Wings, including Table 1: text-only forgetting on the same multimodal training data, Table 2: comparison with general MLLMs, and Figure 5: analysis on the Interleaved Image-Text (IIT) benchmark with varying levels of vision-related conversation. Following that, we outline the training details and configurations of the Wings, and delve into experimental analysis across each benchmark. Following that, we perform an ablation study on various learning rates with different training parts. Finally, we provide supplementary descriptions regarding Wings' overhead compared to general MLLMs and how its innovative compensatory learners help effectively mitigate attention issues.

**Evaluation Setups.** We aim to assess through MLLM how much visual information is required for evaluation. For example, generic multimodal instructions require MLLMs to strongly capture image aspects, whereas text-only instructions focus on the text. We introduce three types of benchmarks:

* **Standard text-only benchmarks.** We are particularly interested in the text-only performance improvement of Wings under the same training data and resource conditions. Different datasets including _interdisciplinary exams_ like MMLU , CMMLU , ARC-Easy, ARC-Challenge , language _understanding_ and _knowledge_ such as WinoGrande , OpenbookQA , Race-Middle, Race-High , WSC , CHID , _reasoning_ such as HellaSwag , SIQA , PIQA , OCNLI , and _math_ and _code_-related tasks such as GSM8K  and MBPP  are comprehensively evaluated.
* **General multimodal benchmarks.** We evaluate on MMMU , MME , MMBench  (MMB) in English (EN) and Chinese (CN), ScienceQA  for test (SciQA), SEED-Bench  for image part (SEED), AI2D  for test, and HallusionBench  (HallB).
* **Our Interleaved Image-Text (IIT) benchmark** with diverse text-only, interleaved, and image-related multi-turn conversations. It includes sampling for MMLU, CMMLU, OpenbookQA, HellaSwag, MMMU, MMBench, SEED-Bench, and AI2D datasets.

Figure 4: **Illustrations of the detailed Wings structure, and training strategies. Wings is constructed by the Low-Rank Residual Attention (LoRRA) module where the previous hidden state acts as the query and the visual/textual features serve as the key and value. Training starts with visual learners and projectors, followed by the dynamic attention-based routing.**

**Model Summaries & Implementation Details.** We release the \(_{}}\) and \(_{}}\), with \(\).-\(\) LLM  and SigLIP  visual encoder as the foundations. We also introduce the \(_{}}\) version, adapted to \(\)-\(1.8\)B LLM for edge device compatibility. As illustrated in Figure 4, we only optimize the projector and the image learners of \(\) for the first alignment stage. The LLM branch adaptation is incorporated during the second instruction tuning stage. We train for \(1\) epoch with the AdamW optimizer and the Cosine learning schedule. Typically, the learning rates for the first and second stages are set at \(1e^{-3}\) and \(2e^{-6}\) (with the projector part as \(1e^{-5}\)), respectively. For \(_{}}\), approximately \(1\)m training data to align image learners and about \(0.6\)m supervised fine-tuning instructions for the next stage (the same as LLaVA\({}_{}}\)). In the \(_{}}\), we use the same aligned data and approximately \(2\)m training data for learning image-text learners. These two types of MLLM require about \(1.5\) and \(6\) days of training on \(8\) A100 GPUs, respectively. The training datasets for \(_{}}\) are consistent with the \(_{}}\). It takes approximately \(5\) days to run on \(4\) A100 GPUs.

**Details in Figure 2.** We adopt various multimodal to text-only sample ratios (\(25\!:\!1\), \(20\!:\!1\), \(10\!:\!1\), \(5\!:\!1\), \(2\!:\!1\), \(1\!:\!1\), \(1\!:\!2\), \(\), \(1\!:\!25\)) plus an all\(\!:\!0\) setup (\(12\) combinations total) to ensure sufficient scenarios. The learning rate is kept consistent with the setup described above. We sample \(5\) models per epoch, excluding \(12\) failed ones due to issues like gradient explosion, resulting in \(108\) for analysis.

### Toward Comprehensive Text-only and Multimodal Performance

**Text-only Comparison in Fair Data and Resource Environments.** As shown in Table 1, "Vicuna-v1.5 + CLIP" corresponds to LLaVA\({}_{}}\), and "Qwen1.5 + SigLIP" serves as the foundation for \(\). When comparing LLM itself and the rest of MLLMs, we observe that fine-tuning with multimodal instructions, compared to the "Qwen LLM", there is text-only forgetting in \(12\) out of \(16\) datasets, with notable decreases of up to \(9.70\), \(8.91\), and \(13.33\) in MMLU, CMMLU, and \(\), respectively. \(\) significantly improve performance on datasets such as MMLU, CMMLU, RACE-High, and WSC, despite the potential for severe text-only forgetting on baselines. Additionally, we find that

    &  &  & _{}}\)**} &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & \\  & & **CLIP** & **CLIP** & **SigLIP** & & & **CLIP** & **CLIP** & **SigLIP** & & \\   & MMLU & 51.18 & 51.12 & 48.89 & 50.63 & **60.86** & 50.83 & 59.67 & 51.16 & 60.53 & 9.70 & 9.37 \\  & CMMLU & 38.60 & 38.29 & 37.24 & 38.73 & 69.37 & 62.58 & 67.87 & 60.46 & **69.82** & 8.91 & 9.36 \\  & ARC-E & 57.62 & 53.63 & 55.82 & 53.95 & **59.96** & 56.93 & 59.35 & 55.87 & 54.29 & 4.09 & -1.58 \\  & ARC-C & 33.75 & 34.60 & 34.68 & 35.17 & 38.90 & 39.14 & 38.64 & 39.50 & **43.39** & -0.60 & 3.89 \\   & Winograde & 68.01 & 64.97 & 67.83 & 65.21 & **71.38** & 69.82 & 71.03 & 69.05 & 69.28 & 2.33 & 0.23 \\  & OpenbookQA & 77.10 & 73.28 & 77.15 & 72.12 & **81.73** & 78.31 & 81.29 & 77.51 & 81.05 & 4.22 & 3.54 \\  & Race-Middle & 63.99 & 60.10 & 62.84 & 59.45 & **74.82** & 68.25 & 72.06 & 68.34 & 74.24 & 6.48 & 5.90 \\  & Race-High & 58.74 & 53.24 & 54.91 & 52.69 & **71.05** & 59.20 & 65.67 & 57.72 & 69.62 & 13.33 & 11.90 \\  & WSC & 51.30 & 47.21 & 51.06 & 47.72 & 56.17 & 54.18 & 57.30 & 55.23 & **66.35** & 0.94 & 11.12 \\  & CHID & 39.05 & 49.66 & 45.26 & 53.49 & 71.94 & 71.82 & 72.92 & **74.29** & 74.29 & 74.06 & -2.35 & -0.23 \\   & HellasWang & 63.11 & 63.08 & 62.58 & 63.02 & **65.70** & 61.90 & 64.32 & 63.24 & 65.12 & 2.46 & 1.88 \\  & SIQA & 42.37 & 44.06 & 43.27 & 44.52 & 45.57 & 52.00 & 46.83 & **51.71** & 49.64 & -1.42 & -0.77 \\  & PIQA & 71.92 & 71.95 & 70.35 & 71.84 & 76.59 & 74.60 & 73.77 & 75.19 & **78.06** & 1.40 & 2.87 \\  & OCNLI & 33.89 & 37.74 & 39.41 & 40.46 & 49.73 & 48.31 & 48.07 & 50.29 & **50.39** & -0.56 & 0.10 \\ 
**Math** & GSMSK & 25.19 & 23.72 & 22.68 & 23.05 & 56.77 & 50.10 & 54.25 & 51.37 & **52.08** & **5.40** & 0.71 \\ 
**Code** & MBPP & 13.80 & 11.29 & 13.92 & 10.80 & 37.50 & 34.82 & 36.72 & 33.20 & **38.92** & 4.30 & 5.72 \\   & MMMU-VAL & – & 35.67 & 30.78 & 35.56 & – & 34.56 & 32.33 & 35.11 & **39.89** & – & 4.78 \\  & MMMU-TEST & – & 34.40 & 30.90 & 35.33 & – & 34.90 & 31.80 & 35.10 & **37.30** & – & 2.20 \\   & MMBench & – & 63.18 & 59.83 & 65.14 & – & 66.05 & 62.84 & **70.94** & 70.53 & – & -0.41 \\   & ScienceQA & – & 67.72 & 64.49 & 71.50 & – & 74.26 & 69.09 & 74.89 & **78.76** & – & 3.87 \\   

Table 1: **Performance comparisons of \(\) and the baseline MLLMs under the same training data**. We consider \(8\) baseline MLLMs, including LLMs as \(_{}}\) & & & & & \\ CLIP  & & SigLIP , and training strategies as full-parameter & LoRA fine-tuning. The first entry represents the initial LLM, upon which each MLLM is trained. Our evaluation spans \

[MISSING_PAGE_FAIL:9]

### Ablation Studies

Referencing Figure 5, we address three questions to comprehensively analyse Wings:

* Can Wings sustain performance with interleaved evaluation? We find that part (a) highlights Wings surpassing LLaVAv1.5 and the same-backbone as LLaVAv1.5 (Qwen-SigLIP) for each multi-turn setting, especially in text-centric dialogues.
* How do Wings fare with different learning rate settings? Part (b) demonstrates that using a lower learning rate maintains proficiency in text-only tasks but falls short in multimodal questions, while a higher rate boosts multimodal abilities but not text-only. Applying a higher learning rate to the projector and a lower one to the others achieves the optimal.
* Are all components of Wings equally effective? In part (c), we examine that incorporating visual learners alone slightly preserves text-only abilities, likely by minimizing disruption to the LLM, but diminishes performance on multimodal tasks.

In the diverse IIT bench, which ranges from text-rich to multimodal contexts, the effectiveness of Wings is particularly evident. As shown in Figure 1, within real-world applications, textual content offers insights for following visual tasks. Wings excels in handling text-only tasks while improving performance on visual-related instructions.

## 5 Conclusion

We propose Wings, which includes visual and textual learners, to alleviate text-only forgetting. The learner is composed of efficient Low-Rank Residual Attention (LoRRA). We start by considering the shifted attention weights in MLLM and, in the first stage, focus on learning the visual learner. Then, we co-train the visual and textual learners with routing based on the shifted attention weights. Wings demonstrates remarkable performance on text-only, visual-question-answering, and newly constructed Interleaved Image-Text (IIT) benchmarks. Wings allows for maintaining text-only performance with limited resources and further enhances performance in well-resourced settings.