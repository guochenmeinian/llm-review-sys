# A Sublinear-Time Spectral Clustering Oracle with Improved Preprocessing Time

Ranran Shen

ranranshen@mail.ustc.edu.cn

School of Computer Science and Technology, University of Science and Technology of China, Hefei, China. Corresponding author: Pan Peng.

Pan Peng

ppeng@ustc.edu.cn

School of Computer Science and Technology, University of Science and Technology of China, Hefei, China. Corresponding author: Pan Peng.

###### Abstract

We address the problem of designing a sublinear-time spectral clustering oracle for graphs that exhibit strong clusterability. Such graphs contain \(k\) latent clusters, each characterized by a large inner conductance (at least \(\)) and a small outer conductance (at most \(\)). Our aim is to preprocess the graph to enable clustering membership queries, with the key requirement that both preprocessing and query answering should be performed in sublinear time, and the resulting partition should be consistent with a \(k\)-partition that is close to the ground-truth clustering. Previous oracles have relied on either a \((k) n\) gap between inner and outer conductances or exponential (in \(k/\)) preprocessing time. Our algorithm relaxes these assumptions, albeit at the cost of a slightly higher misclassification ratio. We also show that our clustering oracle is robust against a few random edge deletions. To validate our theoretical bounds, we conducted experiments on synthetic networks.

## 1 Introduction

Graph clustering is a fundamental task in the field of graph analysis. Given a graph \(G=(V,E)\) and an integer \(k\), the objective of graph clustering is to partition the vertex set \(V\) into \(k\) disjoint clusters \(C_{1},,C_{k}\). Each cluster should exhibit tight connections within the cluster while maintaining loose connections with the other clusters. This task finds applications in various domains, including community detection [32; 12], image segmentation  and bio-informatics .

However, global graph clustering algorithms, such as spectral clustering , modularity maximization , density-based clustering , can be computationally expensive, especially for large datasets. For instance, spectral clustering is a significant algorithm for solving the graph clustering problem, which involves two steps. The first step is to map all the vertices to a \(k\)-dimensional Euclidean space using the Laplacian matrix of the graph. The second step is to cluster all the points in this \(k\)-dimensional Euclidean space, often employing the \(k\)-means algorithm. The time complexity of spectral clustering, as well as other global clustering algorithms, is \((n)\), where \(n=|V|\) denotes the size of the graph. As the graph size increases, the computational demands of these global clustering algorithms become impractical.

Addressing this challenge, an effective approach lies in the utilization of local algorithms that operate within sublinear time. In this paper, our primary focus is on a particular category of such algorithms designed for graph clustering, known as _sublinear-time spectral clustering oracles_[30; 14]. These algorithms consist of two phases: the preprocessing phase and the query phase, both of which can be executed in sublinear time. During the preprocessing phase, these algorithms sample a subset of vertices from \(V\), enabling them to locally explore a small portion of the graph and gain insights into its cluster structure. In the query phase, these algorithms utilize the cluster structure learned during the preprocessing phase to respond to WhichCluster(\(G,x\)) queries. The resulting partitiondefined by the output of WhichCluster(\(G,x\)) should be consistent with a \(k\)-partition that is close to the ground-truth clustering.

We study such oracles for graphs that exhibit strong clusterability, which are graphs that contain \(k\) latent clusters, each characterized by a large inner conductance (at least \(\)) and a small outer conductance (at most \(\)). Let us assume \(>0\) is some constant. In  (see also ), a robust clustering oracle was designed with preprocessing time approximately \(O(())\), query time approximately \(O(())\), misclassification error (i.e., the number of vertices that are misclassified with respect to a ground-truth clustering) approximately \(O(kn)\). The oracle relied on a \((k) n\) gap between inner and outer conductance. In , a clustering oracle was designed with preprocessing time approximately \(2^{()}( n) n^{1/2+O( )}\), query time approximately \(() n^{1/2+O()}\), misclassification error \(O( k)|C_{i}|\) for each cluster \(C_{i},i[k]\) and it takes approximately \(O(() n^{1/2+O()}( n))\) space. This oracle relied on a \( k\) gap between inner and outer conductance.

One of our key contributions in this research is a new sublinear-time spectral clustering oracle that offers enhanced preprocessing efficiency. Specifically, we introduce an oracle that significantly reduces both the preprocessing and query time, performing in \((k n) n^{1/2+O()}\) time and reduces the space complexity, taking \(O((k) n^{1/2+O()}( n))\) space. This approach relies on a \((k)\) gap between the inner and outer conductances, while maintaining a misclassification error of \(O((k)^{1/3})|C_{i}|\) for each cluster \(C_{i},i[k]\). Moreover, our oracle offers practical implementation feasibility, making it well-suited for real-world applications. In contrast, the clustering oracle proposed in  presents challenges in terms of implementation (mainly due to the exponential dependency on \(k/\)).

We also investigate the sensitivity of our clustering oracle to edge perturbations. This analysis holds significance in various practical scenarios where the input graph may be unreliable due to factors such as privacy concerns, adversarial attacks, or random noises . We demonstrate the robustness of our clustering oracle by showing that it can accurately identify the underlying clusters in the resulting graph even after the random deletion of one or a few edges from a well-clusterable graph.

Basic definitions.Graph clustering problems often rely on conductance as a metric to assess the quality of a cluster. Several recent studies () have employed conductance in their investigations. Hence, in this paper, we adopt the same definition to characterize the cluster quality. We state our results for \(d\)-regular graphs for some constant \(d 3\), though they can be easily generalized to graphs with maximum degree at most \(d\) (see Appendix B).

**Definition 1.1** (Inner and outer conductance).: Let \(G=(V,E)\) be a \(d\)-regular \(n\)-vertex graph. For a set \(S C V\), we let \(E(S,C S)\) denote the set of edges with one endpoint in \(S\) and the other endpoint in \(C S\). The _outer conductance of a set_\(C\) is defined to be \(_{}(C,V)=\). The _inner conductance of a set_\(C\) is defined to be \(_{}(C)=_{S C,0<|S|}_{}(S,C)=_{S C,0<|S|}\) if \(|C|>1\) and one otherwise. Specially, _the conductance of graph_\(G\) is defined to be \((G)=_{C V,0<|C|}_{}(C,V)\).

Note that based on the above definition, for a cluster \(C\), the smaller the \(_{}(C,V)\) is, the more loosely connected with the other clusters and the bigger the \(_{}(C)\) is, the more tightly connected within \(C\). For a high quality cluster \(C\), we have \(_{}(C,V)_{}(C) 1\).

**Definition 1.2** (\(k\)-partition).: Let \(G=(V,E)\) be a graph. _A \(k\)-partition of \(V\)_ is a collection of disjoint subsets \(C_{1},,C_{k}\) such that \(_{i=1}^{k}C_{i}=V\).

Based on the above, we have the following definition of clusterable graphs.

**Definition 1.3** (\((k,,)\)-clustering).: Let \(G=(V,E)\) be a \(d\)-regular graph. A \((k,,)\)_-clustering of \(G\)_ is a \(k\)-partition of \(V\), denoted by \(C_{1},,C_{k}\), such that for all \(i[k]\), \(_{}(C_{i})\), \(_{}(C_{i},V)\) and for all \(i,j[k]\) one has \(|}{|C_{j}|} O(1)\). \(G\) is called a \((k,,)\)_-clusterable graph_ if there exists a \((k,,)\)-clustering of \(G\).

Main results.Our main contribution is a sublinear-time spectral clustering oracle with improved preprocessing time for \(d\)-regular \((k,,)\)-clusterable graphs. We assume query access to the adjacency list of a graph \(G\), that is, one can query the \(i\)-th neighbor of any vertex in constant time.

**Theorem 1**.: _Let \(k 2\) be an integer, \((0,1)\). Let \(G=(V,E)\) be a \(d\)-regular \(n\)-vertex graph that admits a \((k,,)\)-clustering \(C_{1},,C_{k}\), \(}}{k^{}^ {k}k}\) and for all \(i[k]\), \(|C_{i}|\), where \(\) is a constant that is in \((0.001,1]\). There exists an algorithm that has query access to the adjacency list of \(G\) and constructs a clustering oracle in \(O(n^{1/2+O(/^{2})}())\) preprocessing time and takes \(O(n^{1/2+O(/^{2})}())\) space. Furthermore, with probability at least \(0.95\), the following hold:_

1. _Using the oracle, the algorithm can answer any_ WhichCluster _query in_ \(O(n^{1/2+O(/^{2})}())\) _time and a_ WhichCluster _query takes_ \(O(n^{1/2+O(/^{2})}())\) _space._
2. _Let_ \(U_{i}:=\{x V:(G,x)=i\},i[k]\) _be the clusters recovered by the algorithm. There exists a permutation_ \(:[k][k]\) _such that for all_ \(i[k]\)_,_ \(|U_{(i)} C_{i}| O(}}{}(})^{1/3})|C_{i}|\)_._

Specifically, for every graph \(G=(V,E)\) that admits a \(k\)-partition \(C_{1},,C_{k}\) with _constant_ inner conductance \(\) and outer conductance \( O((k)})\), our oracle has preprocessing time \( n^{1/2+O()}(k n)\), query time \( n^{1/2+O()}(k n)\), space \( O(n^{1/2+O(/^{2})}(k n))\) and misclassification error \( O((k)^{1/3})|C_{i}|\) for each cluster \(C_{i},i[k]\). In comparison to , our oracle relies on a smaller gap between inner and outer conductance (specifically \(O((k) n)\)). In comparison to , our oracle has a smaller preprocessing time and a smaller space at the expense of a slightly higher misclassification error of \(O((k)^{1/3})|C_{i}|\) instead of \(O( k)|C_{i}|\) and a slightly worse conductance gap of \( O(^{2}/(k))\) instead of \( O(^{3}/(k))\). It's worth highlighting that our space complexity significantly outperforms that of  (i.e., \(O(n^{1/2+O(/^{2})}(  n))\)), particularly in cases where \(k\) is fixed and \(\) takes on exceptionally small values, such as \(=}\) for sufficiently small constant \(c>0\), since the second term in our space complexity does _not_ depend on \(\) in comparison to the one in .

Another contribution of our work is the verification of the robustness of our oracle against the deletion of one or a few random edges. The main idea underlying the proof is that a well-clusterable graph is still well-clusterable (with a slightly worse clustering quality) after removing a few random edges, which in turn is built upon the intuition that after removing a few random edges, an expander graph remains an expander. For the complete statement and proof of this claim, we refer to Appendix E.

**Theorem 2** (Informal; Robust against random edge deletions).: _Let \(c>0\) be a constant. Let \(G_{0}\) be a graph satisfying the similar conditions as stated in Theorem 1. Let \(G\) be a graph obtained from \(G_{0}\) by randomly deleting \(c\) edges. Then there exists a clustering oracle for \(G\) with the same guarantees as presented in Theorem 1._

### Related work

Sublinear-time algorithms for graph clustering have been extensively researched. Czumaj et al.  proposed a property testing algorithm capable of determining whether a graph is \(k\)-clusterable or significantly far from being \(k\)-clusterable in sublinear time. This algorithm, which can be adapted to a sublinear-time clustering oracle, was later extended by Peng  to handle graphs with noisy partial information through a robust clustering oracle. Subsequent improvements to both the testing algorithm and the oracle were introduced by Chiplunkar et al.  and Gluchowski et al. . Recently, Kapralov et al.  presented a hierarchical clustering oracle specifically designed for graphs exhibiting a pronounced hierarchical structure. This oracle offers query access to a high-quality hierarchical clustering at a cost of \((k) n^{1/2+O()}\) per query. However, it is important to note that their algorithm does not provide an oracle for flat \(k\)-clustering, as considered in our work, with the same query complexity. Sublinear-time clustering oracles for signed graphs have also been studied recently .

The field of _local graph clustering_ is also closely related to our research. In this framework, the objective is to identify a cluster starting from a given vertex within a running time that is bounded by the size of the output set, with a weak dependence on \(n\). Zhu et al.  proposed a local clustering algorithm that produces a set with low conductance when both inner and outer conductance are used as measures of cluster quality. It is worth noting that the running times of these algorithms are sublinear only if the target set's size (or volume) is small, for example, at most \(o(n)\). In contrast, in our setting, the clusters of interest have a minimum size that is \((n/k)\).

Extensive research has been conducted on fully or partially recovering clusters in the presence of noise within the "global algorithm regimes". Examples include recovering the planted partition in the _stochastic block model_ with modeling errors or noise [4; 15; 24; 21], _correlation clustering_ on different ground-truth graphs in the _semi-random_ model [23; 5; 13; 20], and graph partitioning in the _average-case_ model [18; 19; 20]. It is important to note that all these algorithms require at least linear time to run.

## 2 Preliminaries

Let \(G=(V,E)\) denote a \(d\)-regular undirected and unweighted graph, where \(V:=\{1,,n\}\). Throughout the paper, we use \(i[n]\) to denote \(1 i n\). For any two vectors \(,^{n}\), we let \(,=^{T}\) denote the dot product of \(\) and \(\). For a graph \(G\), we let \(A^{n n}\) denote the adjacency matrix of \(G\) and let \(D^{n n}\) denote a diagonal matrix with \(D(i,i)=(i)\), where \((i)\) is the degree of vertex \(i,i[n]\). We denote with \(L\) the normalized Laplacian of \(G\) where \(L=D^{-1/2}(D-A)D^{-1/2}=I-\). For \(L\), we use \(0_{1}..._{n} 2\) to denote its eigenvalues and we use \(u_{1},,u_{n}^{n}\) to denote the corresponding eigenvectors. Note that the corresponding eigenvectors are not unique, in this paper, we let \(u_{1},,u_{n}\) be an orthonormal basis of eigenvectors of \(L\). For any two sets \(S_{1}\) and \(S_{2}\), we let \(S_{1} S_{2}\) denote the symmetric difference between \(S_{1}\) and \(S_{2}\).

Due to space constraints, we present only the key preliminaries here; the complete preliminaries will be presented in Appendix A.

Our algorithms in this paper are based on the properties of the dot product of spectral embeddings, so we also need the following definition.

**Definition 2.1** (Spectral embedding).: For a graph \(G=(V,E)\) with \(n=|V|\) and an integer \(2 k n\), we use \(L\) denote the normalized Laplacian of \(G\). Let \(U_{[k]}^{n k}\) denote the matrix of the bottom \(k\) eigenvectors of \(L\). Then for every \(x V\), _the spectral embedding of \(x\), denoted by \(f_{x}^{k}\), is the \(x\)-row of \(U_{[k]}\), which means \(f_{x}(i)=u_{i}(x),i[k]\)_.

**Definition 2.2** (Cluster centers).: Let \(G=(V,E)\) be a \(d\)-regular graph that admits a \((k,,)\)-clustering \(C_{1},,C_{k}\). The _cluster center_\(_{i}\) of \(C_{i}\) is defined to be \(_{i}=|}_{x C_{i}}f_{x}\), \(i[k]\).

The following informal lemma shows that the dot product of two spectral embeddings can be approximated in \((n^{1/2+O(/^{2})}(k))\) time.

**Lemma 2.1** (Informal; Theorem 2, ).: _Let \(G=(V,E)\) be a \(d\)-regular graph that admits a \((k,,)\)-clustering \(C_{1},,C_{k}\), \(}{10^{5}}\). Then for every pair of vertices \(x,y V\), SpectralDotProduct(\(G,x,y,1/2,,\)) computes an output value \( f_{x},f_{y}_{}\) in \((n^{1/2+O(/^{2})}(k))\) time such that with high probability: \(| f_{x},f_{y}_{}- f_{x},f_{y} |\)._

For the completeness of this paper, we will show the formal Lemma 2.1 and algorithm SpectralDotProduct(\(G,x,y,1/2,,\)) in Appendix C.

## 3 Spectral clustering oracle

### Our techniques

We begin by outlining the main concepts of the spectral clustering oracle presented in . Firstly, the authors introduce a sublinear time oracle that provides dot product access to the spectral embedding of graph \(G\) by estimating distributions of short random walks originating from vertices in \(G\) (as described in Lemma 2.1). Subsequently, they demonstrate that (1) the set of points corresponding to the spectral embeddings of all vertices exhibits well-concentrated clustering around the cluster center \(_{i}\) (refer to Definition 2.2), and (2) all the cluster centers are approximately orthogonal to each other. The clustering oracle in  operates as follows: it initially guesses the \(k\) cluster centers from a set of \((k/)\) sampled vertices, which requires a time complexity of \(2^{(k/)}n^{1/2+O()}\). Subsequently, it iteratively employs the dot product oracle to estimate \( f_{x},_{i}\). If the value of \( f_{x},_{i}\) is significant, it allows them to infer that vertex \(x\) likely belongs to cluster \(C_{i}\).

Now we present our algorithm, which builds upon the dot product oracle in . Our main insight is to avoid relying directly on cluster centers in our algorithm. By doing so, we can eliminate the need to guess cluster centers and consequently remove the exponential time required in the preprocessing phase described in . The underlying intuition is as follows: if two vertices, \(x\) and \(y\), belong to the same cluster \(C_{i}\), their corresponding spectral embeddings \(f_{x}\) and \(f_{y}\) will be close to the cluster center \(_{i}\). As a result, the angle between \(f_{x}\) and \(f_{y}\) will be small, and the dot product \( f_{x},f_{y}\) will be large (roughly on the order of \(O()\)). Conversely, if \(x\) and \(y\) belong to different clusters, their embeddings \(f_{x}\) and \(f_{y}\) will tend to be orthogonal, resulting in a small dot product \( f_{x},f_{y}\) (close to \(0\)). We prove that this desirable property holds for the majority of vertices in \(d\)-regular \((k,,)\)-clusterable graphs (see Figure 1 for an illustrative example). Slightly more formally, we introduce the definitions of _good_ and _bad_ vertices (refer to Definition 3.1) such that the set of good vertices corresponds to the core part of clusters and each pair of good vertices satisfies the aforementioned property; the rest vertices are the bad vertices. Leveraging this property, we can directly utilize the dot product of spectral embeddings to construct a sublinear clustering oracle.

Based on the desirable property discussed earlier, which holds for \(d\)-regular \((k,,)\)-clusterable graphs, we can devise a sublinear spectral clustering oracle. Let \(G=(V,E)\) be a \(d\)-regular \((k,,)\)-clusterable graph that possesses a \((k,,)\)-clustering \(C_{1},,C_{k}\). In the preprocessing phase, we sample a set \(S\) of \(s\) vertices from \(V\) and construct a similarity graph, denoted as \(H\), on \(S\). For each pair of vertices \(x,y S\), we utilize the dot product oracle from  to estimate \( f_{x},f_{y}\). If \(x\) and \(y\) belong to the same cluster \(C_{i}\), yielding a large \( f_{x},f_{y}\), we add an edge \((x,y)\) to \(H\). Conversely, if \(x\) and \(y\) belong to different clusters, resulting in a \( f_{x},f_{y}\) close to \(0\), we make no modifications to \(H\). Consequently, only vertices within the same cluster \(C_{i}(i[k])\) can be connected by edges. We can also establish that, by appropriately selecting \(s\), the sampling set \(S\) will, with high probability, contain at least one vertex from each \(C_{1},,C_{k}\). Thus, the similarity graph \(H\) will have \(k\) connected components, with each component corresponding to a cluster in the ground-truth. We utilize these \(k\) connected components, denoted as \(S_{1},,S_{k}\), to represent \(C_{1},,C_{k}\).

During the query phase, we determine whether the queried vertex \(x\) belongs to a connected component in \(H\). Specifically, we estimate \( f_{x},f_{y}\) for all \(y S\). If there exists a unique index \(i[k]\) for which \( f_{x},f_{u}\) is significant (approximately \(O()\)) for all \(u S_{i}\), we conclude that \(x\) belongs to cluster \(C_{i}\), associated with \(S_{i}\). If no such unique index is found, we assign \(x\) a random index \(i\), where \(i[k]\).

### The clustering oracle

Next, we present our algorithms for constructing a spectral clustering oracle and handling the WhichCluster queries. In the preprocessing phase, the algorithm ConstructOracle(\(G,k,,,\)) learns the cluster structure representation of \(G\). This involves constructing a similarity graph \(H\) on a sampled vertex set \(S\) and assigning membership labels \(\) to all vertices in \(S\). During the query phase, the algorithm WhichCluster(\(G,x\)) determines the clustering membership index to which vertex \(x\) belongs. More specifically, WhichCluster(\(G,x\)) utilizes the function SearchIndex(\(H,,x\)

Figure 1: The angle between embeddings of vertices in the same cluster is small and the angle between embeddings of vertices in different clusters is close to orthogonal (\(k=3\)).

to check whether the queried vertex \(x\) belongs to a unique connected component in \(H\). If it does, SearchIndex(\(H,,x\)) will return the index of the unique connected component in \(H\).

The algorithm in preprocessing phase is given in Algorithm 1 ConstructOracle(\(G,k,,,\)).

See Appendix C for algorithm InitializeOracle and SpectralDotProductOracle invoked by ConstructOracle(\(G,k,,,\)).

Our algorithms used in the query phase are described in Algorithm 2 SearchIndex(\(H,,x\)) and Algorithm 3 WhichCluster(\(G,x\)).

```
1forany vertex \(u S\)do
2 Let \( f_{u},f_{x}_{}=(G,u,x,1/2, ,)\)
3ifthere exists a unique index \(1 i k\) such that \( f_{u},f_{x}_{}\) for all \(u S_{i}\)then
4 return index \(i\)
5else
6 return outlier ```

**Algorithm 1**ConstructOracle(\(G,k,,,\))

```
1forany vertex \(u S\)do
2 Let \( f_{u},f_{x}_{}=(G,u,x,1/2, ,)\)
3ifthere exists a unique index \(1 i k\) such that \( f_{u},f_{x}_{}\) for all \(u S_{i}\)then
4 return index \(i\)
5else
6 return outlier ```

**Algorithm 2**SearchIndex(\(H,,x\))

```
1ifpreprocessingphasefailsthen
2 returnfail
3ifSearchIndex(\(H,,x\))returnoutlierthen
4 return a random index\([k]\)
5else
6 return SearchIndex(\(H,,x\)) ```

**Algorithm 3**WhichCluster(\(G,x\))

### Analysis of the oracle

We now prove the following property: for most vertex pairs \(x,y\), if \(x,y\) are in the same cluster, then \( f_{x},f_{y}\) is rough \(O()\) (Lemma 3.4); and if \(x,y\) are in the different clusters, then \( f_{x},f_{y}\) is close to 0 (Lemma 3.5). We make use of the following three lemmas. Due to the limited space, all the missing proofs will be given in Appendix D.

The following lemma shows that for most vertices \(x\), the norm \(\|f_{x}\|_{2}\) is small.

**Lemma 3.1**.: _Let \((0,1)\). Let \(k 2\) be an integer, \((0,1)\), and \((0,1)\). Let \(G=(V,E)\) be a \(d\)-regular \((k,,)\)-clusterable graph with \(|V|=n\). There exists a subset \( V\) with \(||(1-)|V|\) such that for all \(x\), it holds that \(\|f_{x}\|_{2}}\)._

We then show that for most vertices \(x\), \(f_{x}\) is close to its center \(_{x}\) of the cluster containing \(x\).

**Lemma 3.2**.: _Let \((0,1)\). Let \(k 2\) be an integer, \((0,1)\), and \((0,1)\). Let \(G=(V,E)\) be a \(d\)-regular graph that admits a \((k,,)\)-clustering \(C_{1},,C_{k}\) with \(|V|=n\). There exists a subset \( V\) with \(||(1-)\,|V|\) such that for all \(x\), it holds that \(\|f_{x}-_{x}\|_{2}}}\)._

The next lemma shows that for most vertices \(x\) in a cluster \(C_{i}\), the inner product \( f_{x},_{i}\) is large.

**Lemma 3.3**.: _Let \(k 2\) be an integer, \((0,1)\), and \(}\) be smaller than a sufficiently small constant. Let \(G=(V,E)\) be a \(d\)-regular graph that admits a \((k,,)\)-clustering \(C_{1},,C_{k}\). Let \(C_{i}\) denote the cluster corresponding to the center \(_{i}\), \(i[k]\). Then for every \(C_{i}\), \(i[k]\), there exists a subset \(} C_{i}\) with \(|}|(1-}{^{2}})|C_{i}|\) such that for all \(x}\), it holds that \( f_{x},_{i} 0.96\|_{i}\|_{2}^{2}\)._

For the sake of description, we introduce the following definition.

**Definition 3.1** (Good and bad vertices).: Let \(k 2\) be an integer, \((0,1)\), and \(}\) be smaller than a sufficiently small constant. Let \(G=(V,E)\) be a \(d\)-regular \(n\)-vertex graph that admits a \((k,,)\)-clustering \(C_{1},,C_{k}\). We call a vertex \(x V\)_a good vertex with respect to \((0,1)\) and \((0,1)\)_ if \(x((_{i=1}^{k}}))\), where \(\) is the set as defined in Lemma 3.1, \(\) is the set as defined in Lemma 3.2 and \(}\)\((i[k])\) is the set as defined in Lemma 3.3. We call a vertex \(x V\)_a bad vertex with respect to \((0,1)\) and \((0,1)\)_ if it's not a good vertex with respect to \(\) and \(\).

Note that for a good vertex \(x\) with respect to \((0,1)\) and \((0,1)\), the following hold: (1) \(\|f_{x}\|_{2}}\); (2) \(\|f_{x}-_{x}\|_{2}}}\); (3) \( f_{x},_{x} 0.96\|_{x}\|_{2}^{2}\). For a bad vertex \(x\) with respect to \((0,1)\) and \((0,1)\), it does not satisfy at least one of the above three conditions.

The following lemma shows that if vertex \(x\) and vertex \(y\) are in the same cluster and both of them are good vertices with respect to \(\) and \(\) (\(\) and \(\) should be chosen appropriately), then the spectral dot product \( f_{x},f_{y}\) is roughly \(0.96|}\).

**Lemma 3.4**.: _Let \(k 2\), \((0,1)\) and \(}\) be smaller than a sufficiently small constant. Let \(G=(V,E)\) be a \(d\)-regular \(n\)-vertex graph that admits a \((k,,)\)-clustering \(C_{1},,C_{k}\). Suppose that \(x,y V\) are in the same cluster \(C_{i},i[k]\) and both of them are good vertices with respect to \(=2(})^{1/3}\) and \(=2(})^{1/3}\). Then \( f_{x},f_{y} 0.96(1-}{ })|}-}{n}(})^{1/6}\)._

The following lemma shows that if vertex \(x\) and vertex \(y\) are in different clusters and both of them are good vertices with respect to \(\) and \(\) (\(\) and \(\) should be chosen appropriately), then the spectral dot product \( f_{x},f_{y}\) is close to \(0\).

**Lemma 3.5**.: _Let \(k 2\), \((0,1)\) and \(}\) be smaller than a sufficiently small constant. Let \(G=(V,E)\) be a \(d\)-regular \(n\)-vertex graph that admits a \((k,,)\)-clustering \(C_{1},,C_{k}\). Suppose that \(x C_{i}\), \(y C_{j},(i,j[k],i j)\) and both of them are good vertices with respect to \(=2(})^{1/3}\) and \(=2(})^{1/3}\), the following holds:_

\[ f_{x},f_{y}}{n}(})^{1/6}+k^{1/4}}{}(})^{1/3} }{})|}}+}{}|}|C_{j}|}.\]

**Proof of Theorem 1.** Now we prove our main result Theorem 1.

Proof.: Let \(s=\) be the size of sampling set \(S\), let \(==2(})^{1/3}\). Recall that we call a vertex \(x\) a bad vertex, if \(x(V)(V)(V( _{i=1}^{k}}))\), where \(,,},i[k]\) are defined in Lemma 3.1, 3.2, 3.3 respectively. We use \(B\) to denote the set of all bad vertices. Then we have \(|B|(++}{^{2}}) n=(4 (})^{1/3}+}{^{2 }}) n\). We let \( 4(})^{1/3}+ }{^{2}}\)

[MISSING_PAGE_FAIL:8]

Therefore, with probability at least \(1----}-} 0.95\), all the good vertices will be correctly recovered. So the misclassified vertices come from \(B\). We know that \(|B|(++}{^{2}}) n= (4(})^{1/3}+}{^{2}}) n.\) Since \(|C_{i}|\), we have \(n|C_{i}|\). So, \(|B|(4(})^{1/3}+ }{^{2}})|C_{i}| O(}{ }(})^{1/3})|C_{i}|\). This implies that there exists a permutation \(:[k][k]\) such that for all \(i[k]\), \(|U_{(i)} C_{i}| O(}}{}(})^{1/3})|C_{i}|\).

**Running time.** By Lemma 2.1, we know that InitializeOracle(\(G,1/2,\)) computes in time \(()^{O(1)} n^{1/2+O(/^{2})}( n)^{3} }\) a sublinear space data structure \(\) and for every pair of vertices \(x,y V\), SpectralOroductOracle(\(G,x,y,1/2,,\)) computes an output value \( f_{x},f_{y}_{}\) in \(()^{O(1)} n^{1/2+O(/^{2})}( n)^ {2}}\) time.

In preprocessing phase, for algorithm ConstructOracle(\(G,k,,,\)), it invokes InitializeOracle one time to construct a data structure \(\) and SpectralDotProductOracle\(s s\) times to construct a similarity graph \(H\). So the preprocessing time of our oracle is \(()^{O(1)} n^{1/2+O(/^{2})}( n)^{3} }+^{2}k}{^{2}}()^{O(1)} n^{1/2+O(/^{2})}( n)^{2} }=O(n^{1/2+O(/^{2})}())\).

In query phase, to answer the cluster index that \(x\) belongs to, algorithm WhichCluster(\(G,x\)) invokes SpectralDotProductOracle\(s\) times. So the query time of our oracle is \(()^{O(1)} n^{1/2+O(/ ^{2})}( n)^{2}}=O(n^{1/2+O( /^{2})}())\).

**Space.** By the proof of Lemma 2.1 in , we know that overall both algorithm InitializeOracle and SpectralDotProductOracle take \(()^{O(1)} n^{1/2+O(/^{2})}( n)\) space. Therefore, overall preprocessing phase takes \(()^{O(1)} n^{1/2+O(/^{2})}( n)=O(n^{1/2+O(/^{2})}())\) space (at lines 5 and 7 of Alg.1). In query phase, WhichCluster query takes \(()^{O(1)} n^{1/2+O(/^{2})}( n)=O(n^{1/2+O(/^{2})}())\) space (at line 2 of Alg.2). 

## 4 Experiments

To evaluate the performance of our oracle, we conducted experiments on the random graph generated by the Stochastic Block Model (SBM). In this model, we are given parameters \(p,q\) and \(n,k\), where \(n,k\) denote the number of vertices and the number of clusters respectively; \(p\) denotes the probability that any pair of vertices within each cluster is connected by an edge, and \(q\) denotes the probability that any pair of vertices from different clusters is connected by an edge. Setting \(>c\) for some big enough constant \(c\) we can get a well-clusterable graph. All experiments were implemented in Python 3.9 and the experiments were performed using an Intel(R) Core(TM) i7-12700F CPU @ 2.10GHz processor, with 32 GB RAM. Due to the limited space, practical changes to our oracle will be shown in Appendix F.

Misclassification error.To evaluate the misclassification error our oracle, we conducted this experiment. In this experiment, we set \(k=3\), \(n=3000\), \(q=0.002\), \(p[0.02,0.07]\) in the SBM, where each cluster has \(1000\) vertices. For each graph \(G=(V,E)\), we run WhichCluster(\(G,x\)) for all \(x V\) and get a \(k\)-partition \(U_{1},,U_{k}\) of \(V\). In experiments, the _misclassification error_ of our oracle is defined to be \(1-_{}_{i=1}^{k}U_{(i)} C_{i}\), where \(:[k][k]\) is a permutation and \(C_{1},,C_{k}\) are the ground-truth clusters of \(G\).

Moreover, we implemented the oracle in  to compare with our oracle2. The oracle in  can be seen as a non-robust version of oracle in . Note that our primary advancement over  (also ) is evident in the significantly reduced conductance gap we achieve.

We did not compare with the oracle in , since implementing the oracle in  poses challenges. As described in section 3.1, the oracle in  initially approximates the \(k\) cluster centers by sampling around \(O(1/ k^{4} k)\) vertices, and subsequently undertakes the enumeration of approximately \(2^{O(1/ k^{4}^{2}k)}\) potential \(k\)-partitions (Algorithm 10 in ). This enumeration process is extremely time-intensive and becomes impractical even for modest values of \(k\).

According to the result of our experiment (Table 1), the misclassification error of our oracle is reported to be quite small when \(p 0.025\), and even decreases to \(0\) when \(p 0.035\). The outcomes of our experimentation distinctly demonstrate that our oracle's misclassification error remains notably minimal in instances where the input graph showcases an underlying latent cluster structure. In addition, Table 1 also shows that our oracle can handle graphs with a smaller conductance gap than the oracle in , which is consistent with the theoretical results. This empirical validation reinforces the practical utility and efficacy of our oracle beyond theoretical conjecture.

Robustness experiment.The base graph \(G_{0}=(V,E)\) is generated from SBM with \(n=3000,k=3,p=0.05,q=0.002\). Note that randomly deleting some edges in each cluster is equivalent to reducing \(p\) and randomly deleting some edges between different clusters is equivalent to reducing \(q\). So we consider the worst case. We randomly choose one vertex from each cluster; for each selected vertex \(x_{i}\), we randomly delete delNum edges connected to \(x_{i}\) in cluster \(C_{i}\). If \(x_{i}\) has fewer than delNum neighbors within \(C_{i}\), then we delete all the edges incident to \(x_{i}\) in \(C_{i}\). We run WhichCluster queries for all vertices in \(V\) on the resulting graph. We repeated this process for five times for each parameter delNum and recorded the average misclassification error (Table 2). The results show that our oracle is robust against a few number of random edge deletions.

We also conducted experiments to report the query complexity and running time of our oracle. See Appendix F for more details.

## 5 Conclusion

We have devised a new spectral clustering oracle with sublinear preprocessing and query time. In comparison to the approach presented in , our oracle exhibits improved preprocessing efficiency, albeit with a slightly higher misclassification error rate. Furthermore, our oracle can be readily implemented in practical settings, while the clustering oracle proposed in  poses challenges in terms of implementation feasibility. To obtain our oracle, we have established a property regarding the spectral embeddings of the vertices in \(V\) for a \(d\)-bounded \(n\)-vertex graph \(G=(V,E)\) that exhibits a \((k,,)\)-clustering \(C_{1},,C_{k}\). Specifically, if \(x\) and \(y\) belong to the same cluster, the dot product of their spectral embeddings (denoted as \( f_{x},f_{y}\)) is approximately \(O()\). Conversely, if \(x\) and \(y\) are from different clusters, \( f_{x},f_{y}\) is close to 0. We also show that our clustering oracle is robust against a few random edge deletions and conducted experiments on synthetic networks to validate our theoretical results.

   \(p\) & 0.02 & 0.025 & 0.03 & 0.035 & 0.04 & 0.05 & 0.06 & 0.07 \\  min-error1() & - & 0.5570 & 0.1677 & 0.0363 & 0.0173 & 0.0010 & 0 & 0 \\ max-error1() & - & 0.6607 & 0.6610 & 0.6533 & 0.4510 & 0.0773 & 0.0227 & 0.0013 \\ average-error1() & - & 0.6208 & 0.4970 & 0.1996 & 0.0829 & 0.0168 & 0.0030 & 0.0003 \\ error (our) & 0.2273 & 0.0113 & 0.0003 & 0 & 0 & 0 & 0 & 0 \\   

* In the experiment, we found that the misclassification error of oracle in  fluctuated greatly, so for the oracle in , for each value of \(p\), we conducted 30 independent experiments and recorded the minimum error, maximum error and average error of oracle in .

Table 1: The misclassification error of the oracle in  and our oracle

   delNum & 25 & 32 & 40 & 45 & 50 & 55 & 60 & 65 \\  error & 0.00007 & 0.00007 & 0.00013 & 0.00047 & 0.00080 & 0.00080 & 0.00087 \\   

* We also conducted experiments to report the query complexity and running time of our oracle. See Appendix F for more details.

Table 2: The average misclassification error with respect to delNum random edge deletions