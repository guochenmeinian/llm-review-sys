# Geodesic Optimization for Predictive Shift Adaptation on EEG data

Apolline Mellot, Antoine Collas

Inria, CEA, Universite Paris-Saclay

 Palaiseau, France

 apolline.mellot@inria.fr

antoine.collas@inria.fr

&Sylvain Chevallier

TAU Inria, LISN-CNRS,

 University Paris-Saclay, France.

 sylvain.chevallier@

universite-paris-saclay.fr

&Alexandre Gramfort

Inria, CEA, Universite Paris-Saclay

 Palaiseau, France

 alexandre.gramfort@inria.fr

&Denis A. Engemann

Roche Pharma Research and Early Development,

 Neuroscience and Rare Diseases,

 Roche Innovation Center Basel,

 F. Hoffmann-La Roche Ltd., Basel, Switzerland.

 denis.engemann@roche.com

Equal contribution.

###### Abstract

Electroencephalography (EEG) data is often collected from diverse contexts involving different populations and EEG devices. This variability can induce distribution shifts in the data \(X\) and in the biomedical variables of interest \(y\), thus limiting the application of supervised machine learning (ML) algorithms. While domain adaptation (DA) methods have been developed to mitigate the impact of these shifts, such methods struggle when distribution shifts occur simultaneously in \(X\) and \(y\). As state-of-the-art ML models for EEG represent the data by spatial covariance matrices, which lie on the Riemannian manifold of Symmetric Positive Definite (SPD) matrices, it is appealing to study DA techniques operating on the SPD manifold. This paper proposes a novel method termed Geodesic Optimization for Predictive Shift Adaptation (GOPSA) to address test-time multi-source DA for situations in which source domains have distinct \(y\) distributions. GOPSA exploits the geodesic structure of the Riemannian manifold to jointly learn a domain-specific re-centering operator representing site-specific intercepts and the regression model. We performed empirical benchmarks on the cross-site generalization of age-prediction models with resting-state EEG data from a large multi-national dataset (HarMN-qEEG), which included \(14\) recording sites and more than \(1500\) human participants. Compared to state-of-the-art methods, our results showed that GOPSA achieved significantly higher performance on three regression metrics (\(R^{2}\), MAE, and Spearman's \(\)) for several source-target site combinations, highlighting its effectiveness in tackling multi-source DA with predictive shifts in EEG data analysis. Our method has the potential to combine the advantages of mixed-effects modeling with machine learning for biomedical applications of EEG, such as multicenter clinical trials.

## 1 Introduction

Machine learning (ML) has enabled advances in the analysis of complex biological signals, such as magneto- and electroencephalography (M/EEG), in diverse applications including biomarkerexploration [58; 20; 60] or developing Brain-Computer Interface (BCI) [57; 15; 1; 2]. However, a major challenge in applying ML to these signals arises from their inherent variability, a problem commonly referred to as dataset shift . In the case of M/EEG data this variability can be caused by differences in recording devices (electrode positions and amplifier configurations), recording protocols, population demographics, and inter-subject variability [36; 14; 23; 29]. Notably, shifts can occur not only in the data \(X\) but also in the biomedical variable \(y\) we aim to predict, further complicating the use of ML algorithms.

Riemannian geometry has significantly advanced EEG data analysis by enabling the use of spatial covariance matrices as EEG descriptors [5; 4; 6; 38; 31; 35; 33; 48; 17; 56]. In [4; 6], the authors introduced a classification framework for BCI based on the Riemannian geometry of covariance matrices. These methods classify EEG signals directly on the tangent space using the Riemannian manifold of symmetric positive definite (SPD) matrices (\(_{d}^{++}\)), effectively capturing spatial information. More recently, [47; 48; 7] extended this framework to regression problems from M/EEG data in the context of biomarker exploration. Furthermore, [47; 48] proved that Riemannian metrics lead to regression models with statistical guarantees in line with log-linear brain dynamics  and are, therefore, well-suited for neuroscience applications. Across various biomarker-exploration tasks and datasets, recent work has shown that Riemannian M/EEG representations offer parameter-sparse alternatives to non-Riemannian deep learning architectures [13; 40; 17].

Domain adaptation addresses the challenges posed by differences in data distributions between source and target domains, e.g., when data are recorded with different cameras in computer vision , different writing styles in natural language processing , or varying sensor setups in time series analysis . In particular, DA considers _target shift_ where the shift is in the outcome variable \(y\). For classification it means source and target data share the same labels but in different proportions . Target shift is also frequent in the context of multicenter neuroscience studies, as the studied population of one site may vary significantly from the studied population of another site (cf. Figure 1). To tackle various sources of variability in neurophysiological data like EEG, there is a need for a DA approach that can deal with a joint shift in \(X\) and \(y\).

Related workIn , the authors addressed DA for EEG-based BCI using re-centering affine transformation of covariance matrices (Section 2) to align data from different sessions or human participants, improving classification accuracies. Yair et al.  extended this with parallel transport showing its effectiveness in EEG analysis, whereas, Peng et al  introduced a domain-specific regularizer based on the Riemannian mean. Notably, this parallel transport approach reduces to  when the common reference point is the identity. In a deep learning context, Kobler et al.  proposed to do a per-domain online re-centering which can be seen as a domain specific Riemannian batch norm. Going beyond re-centering, Riemannian Procrustes Analysis (RPA)  was proposed for EEG transfer learning, using three steps: mean alignment, dispersion matching, and rotation correction. However, the rotation step is unsuitable for regression problems and RPA adapts only a single source to a target domain. Recently,  demonstrated the benefits of re-centering for regression problems, showing improvements in handling task variations in MEG and enhancing across-dataset inference in EEG.

Figure 1: **Joint shift in \(X\) and \(y\) distributions on the HarMNqEEG dataset .** Subset of mean PSDs (**A**) and age distributions (**B**) from three recording sites used for the empirical benchmarks.

On the other hand, mixed-effects models (or multilevel models) have been successfully used to tackle data shifts in \(X\) and \(y\)[16; 25]. In biomedical data, mixed-effects models are crucial due to the presence of common effects, such as disease status and age. Riemannian mixed-effects models have been used to analyze observations on Riemannian manifolds, accommodating individual trajectories with mixed effects at both group and individual levels [30; 49; 50]. These models adapt a base point on the manifold for each data point and utilize parallel transport for this adaptation, which is necessary for accurate trajectory modeling. However, they differ significantly from the problem we address in this work. Notably, the input data \(X\) are covariates (e.g., age or disease status) which belong to a Euclidean space and the variables \(y\) to predict belong to the manifold (e.g., MRI diffusion tensors on \(_{d}^{++}\)) which is the opposite of the paper's studied problem. This distinction is critical as it highlights that while both methods use the geometry of Riemannian manifolds, the nature of the predicted variables and the type of data used differ from existing Riemannian mixed-effects models.

ContributionsIn this work, we address the challenging problem of multi-source domain adaptation with predictive shifts on the SPD manifold, focusing on distribution shifts in both the input data \(X\) and the variable to predict \(y\). We propose a novel method called Geodesic Optimization for Predictive Shift Adaptation (GOPSA). It enables mixed-effects modeling by jointly learning parallel transport along a geodesic for each domain and a global regression model common to all domains, with the assumption that the mean \(_{}\) of the target domain is known. GOPSA aims to advance the state of the art by: _(i)_ addressing shifts in both covariance matrices and the outcome variable \(y\), _(ii)_ being tailored for regression problems, and _(iii)_ being a multi-source test-time domain adaptation method, meaning that once trained on source data, it can generalize to any target domain without requiring access to source data or retraining a new model.

We first introduce in Section 2 how to do regression from covariance matrices on the Riemannian manifold of \(_{d}^{++}\). We also interpret classical learning methods on \(_{d}^{++}\) from heterogeneous domains as parallel transports combined with Riemannian logarithmic mappings. This leads us to GOPSA in Section 3, which learns to parallel transport each domain, with algorithms at train and test times. Finally, in Section 4, we apply GOPSA as well as different baselines on simulated data and the HarMNqEEG dataset.

NotationsVectors and matrices are represented by small and large cap boldface letters respectively (e.g., \(\), \(\)). The set \(\{1,...,K\}\) is denoted by \( 1,K\). \(_{d}^{++}\) and \(_{d}\) represent the sets of \(d d\) symmetric positive definite and symmetric matrices. \(vec}:_{d}^{{}^{d(d+1)/2}}\) vectorizes the upper triangular part of a symmetric matrix. Frobenius and \(2\)-norms are denoted by \(\|.\|_{F}\) and \(\|.\|_{2}\), respectively. \(\) defines the left part of the equation as the right part. The transpose and Euclidean inner product operations are represented by \(^{}\). \(_{n}[1,,1]^{}\) denotes an \(n\)-dimensional vector of ones. For a loss function \(\), \(\) and \(^{}\) denote its gradient and derivative. We consider \(K\) labeled source domains, each consisting of \(N_{k}\) covariance matrices, along with their corresponding outcome values, denoted by \(\{(_{k,i},y_{k,i})\}_{i=1}^{N_{k}}\). The target domain is \((_{,i})_{i=1}^{N_{}}\) with the average outcome \(_{}\).

## 2 Regression modeling from covariance matrices using Riemannian geometry

Riemannian geometry of \(_{d}^{++}\)The covariance matrices belong to the set of \(d d\) symmetric positive definite matrices denoted \(_{d}^{++}\)[51; 44]. The latter is open in the set of \(d d\) symmetric matrices denoted \(_{d}\), and thus \(_{d}^{++}\) is a smooth manifold . A vector space is defined at each \(_{d}^{++}\), called the tangent space, denoted \(T_{}_{d}^{++}\), and is equal to \(_{d}\), the ambient space. Equipped with a smooth inner product at every tangent space, a smooth manifold b comes a Riemannian manifold. To do so, we make use of the affine invariant Riemannian metric [51; 44]. Given \(,^{} T_{}_{d}^{++}\), this metric is \(,^{}_{}=(^{-1}^{-1}^{})\).

Riemannian meanThe Riemannian distance (or geodesic distance) associated with the affine invariant metric is \(_{R}(,^{})\|(^{-1/2}^{}^{-1/2})\|_{F}\) with \(:_{d}^{++}_{d}\) being the matrix logarithm (see Appendix A.1 for a definition). This distance is used to compute the Riemannian mean \(}\) defined for a set \(\{_{i}\}_{i=1}^{N}_{d}^{++}\) as \(}_{_{d}^{++}} _{i=1}^{N}_{R}(,_{i})^{2}\).

This mean is efficiently computed with a Riemannian gradient descent [44; 63].

Riemannian logarithmic mappingThe idea of the covariance-based approach is to define nonlinear feature transformations into vectors that can be used as input for classical linear machine learning models. To do so, the Riemannian logarithmic mapping of \(^{}\) at \(\) is defined as

\[_{}(^{})^{}{{2}} }(^{}{{2}}}^{}{{2}}} )^{}{{2}}} T_{}_{d}^{++}\.\] (1)

Thus, matrices in the Riemannian manifold \(_{d}^{++}\) are transformed into tangent vectors.

Parallel transportA classical practice to align distributions is parallel transport of covariance matrices from their mean to the identity and then apply the logarithmic mapping (1). Parallel transport along a curve allows to move SPD matrices from one point on the curve to another point on the curve while keeping the inner product between the logarithmic mappings with any other vector transported along the same curve constant. The following lemma gives the parallel transport of \(^{}\) from \(\) to \(_{d}\) along the geodesic between these two points (See proof in Appendix A.2).

**Lemma 2.1** (Parallel transport to the identity).: _Given \(,^{}_{d}^{++}\), the parallel transport of \(^{}\) along the geodesic from \(\) to the identity \(_{d}\) at \(\) is_

\[(^{},,) ^{}{{2}}}^{}^{}{{2}}}\.\]

Learning on \(_{d}^{++}\)The logarithmic mapping (1) at the identity is simply the matrix logarithm. Thus, a classical non-linear feature extraction [6; 36; 8] of a dataset \(\{_{i}\}_{i=1}^{N}\) of Riemannian mean \(}\) combines parallel transport and logarithmic mapping at the identity,

\[(_{i},}) (_{_{d}}((_{i},},1)))= ((}^{}{{2}}}_{i} }^{}{{2}}}))^{d }{{2}}}\] (2)

where \(\) vectorizes the upper triangular part with off-diagonal elements multiplied by \(\) to preserve the norm. Correcting dataset shifts by re-centering all source datasets , corresponds to parallel transporting data \(\{_{k,i}\}_{i=1}^{N_{k}}\) of each domain \(k 1,K\) from its Riemannian mean \(}_{k}\) to the identity,

\[(_{k,i},}_{k})=( (}_{k}^{}{{2}}}_{k,i} }_{k}^{}{{2}}}))\.\] (3)

This method is the go-to approach for reducing shifts of the covariance matrix distributions coming from different domains and has been applied successfully for brain-computer interfaces [45; 59] and age prediction from M/EEG data .

## 3 Learning to recenter from highly shifted \(y\) distributions with Gopsa

In this section, we develop a novel multi-source domain adaptation method, called Geodesic Optimization for Predictive Shift Adaptation (GOPSA), that operates on the \(_{d}^{++}\) manifold and is capable of handling vastly different distributions of \(y\). Our approach implements a Riemannian mixed-effects model, which consists of two components: a single parameter estimating a geodesic intercept specific to each domain and a set of parameters shared across domains.

At train-time, GOPSA jointly learns the parallel transport of each of the \(K\) source domains and the regression model shared across domains. At test-time, we assume having access to the target mean response value \(_{}\) and predict on the unlabeled target domain of covariance matrices \((_{,i})_{i=1}^{N_{}}\). GOPSA focuses solely on learning the parallel transport of the target domain so that the mean prediction, using the regression model learned at train-time, matches \(_{}\).

Parallel transport along the geodesicIn Section 2, we presented how domain adaptation is performed on \(_{d}^{++}\). In particular, (3) presents how to account for data shifts of each domain. However, this operator can only work if the variability between domains is considered as noise. As explained earlier, we are interested in shifts in both features and the response variable. Thus, (3) discards shift coming from the response variable and hence harms the performance of the predictive model. Based on the Lemma 2.1, we propose to parallel transport features to any point on the geodesic between a domain-specific Riemannian mean \(}_{k}\) and the identity. Indeed, GOPSA parallel transports \(_{k,i}\) on this geodesic with \(\) and then applies the Riemannian logarithmic mapping (1) at the identity,

\[(_{k,i},}_{k},) (_{_{d}}((_{k,i},}_{k},)))= ((}_{k}^{}{{2}}}_{k,i}}_{k}^{ }{{2}}})).\] (4)

This allows each domain to undergo parallel transport to a certain degree, effectively moving it toward the identity.

### Train-time

``` : For all \(k 1,K\), \(\{(_{k,i},y_{k,i})\}_{i=1}^{N_{k}}\),  initialization of \(_{S}\), step-sizes \(\{_{t}\}_{t 1}\) for\(k=1 K\)do \(}_{k}\) Riemannian mean of \(\{_{k,i}\}_{i=1}^{N_{k}}\)  end for\(t 1\) whilenot convergeddo \(_{}(_{})\) Compute features with (7) \(_{}^{*}(_{})\) Compute Ridge coeff. with (8) \(_{}(_{})\) Compute loss gradient of (8) \(_{}_{}-_{t} _{}(_{})\) \(t t+1\)  end for return\(_{}^{*}(_{}^{*})\) ```

**Algorithm 1**Train-Time Gopga

### Train-time

GOPSA aims to learn simultaneously features from (4) and a regression model. To do so, we solve the following optimization problem

\[*{minimize}_{_{} ^{d(d+1)/2}\\ _{}^{K}}_{k=1}^{K}_{i=1}^{ N_{k}}(y_{k,i}-_{}^{}(_{k,i}, }_{k},_{k}))^{2}\] (5)

with \(_{}=[_{1},,_{K}]^{}\). This cost function is decomposed into three key aspects. First, covariance matrices undergo parallel transported using Lemma 2.1 to account for shifts between domains. Second, they are vectorized, and a linear regression predicts the output variable from these vectorized features. Third, the coefficients of the linear regression \(_{}\) and the \(_{}\) are learned jointly so that the predictor is adapted to the parallel transport and reciprocally. Besides, to enforce the constraint on \(_{}\), we re-parameterize it using the sigmoid function, which defines a bijection between \(\) and \((0,1)\), thereby ensuring that the resulting \(_{}\) values lie within the desired range: \(_{k}=(_{k})(1+(-_{k}))^{-1}\). Thus, the constrained problem (5) can be formulated as the following unconstrained optimization problem

\[*{minimize}_{_{} ^{d(d+1)/2}\\ _{}^{K}}_{k=1}^{K}_{ i=1}^{N_{k}}(y_{k,i}-_{}^{}(_{k,i}, }_{k},(_{k})))^{2},\] (6)

with \(_{}=[_{1},,_{K}]^{}\). Let us define the matrix \(_{}()^{N_{} d(d+1) /2}\), with \(N_{}=_{k=1}^{K}N_{k}\), as the concatenation of the source data, where each row corresponds to a feature vector:

\[_{}()=[(_{1,1},}_{1},(_{1})),,(_{K,N_{K}},}_{K},(_{K}))]^{}.\] (7)

In the same manner, the source labels are concatenated to \(_{}=[y_{1,1},,y_{K,N_{K}}]^{}^{N_{ }}\). Given a fixed \(_{}\), the problem (6) is solved with the ordinary least squares estimator. In practice, we choose to regularize the estimation of the linear regression with a Ridge penalty. Thus, (6) is rewritten as

\[_{}^{*} *{arg\,min}_{^{K}}\;\{ _{}()\|_{}- _{}()_{}^{*}() \|_{2}^{2}\}\] (8) \[_{}^{*}() _{}()^{}(_{N}+_ {}()_{}()^{})^{-1} {y}_{},\]

where \(_{}^{*}()^{d(d+1)/2}\) are the Ridge estimated coefficients given a fixed \(\) and \(>0\) is the regularization hyperparameter. The problem (8) is efficiently solved with any gradient-based solver .

The train-time of GOPSA is summarized in Algorithm 1. The proposed training algorithm begins by calculating the Riemannian mean of covariance matrices for each domain \(k\). It then iteratively optimizes the parameters \(_{}\) by computing the feature matrix (7), determining Ridge regression coefficients (8), and updating \(_{}\) using a gradient descent step on the loss function (8) until convergence. The output result is the optimized Ridge regression coefficients. For clarity of presentation,Algorithm 1 employs a gradient descent. In practice, we use L-BFGS and obtain the gradient using automatic differentiation through the Ridge solution that is plugged into the loss in (8).

### Test-time

At test-time, we now have a fitted linear model on source data with coefficients \(_{}^{*}(_{}^{*})\). The goal is to adapt a new target domain \((_{,i})_{i=1}^{N_{}}\) for which the average outcome \(_{}\) is assumed to be known. First, let us define the matrix \(_{}()^{N_{}^{4(d+1)/2}}\) as the concatenation of the target data

\[_{}()=[(_{,1}, }_{},()),,(_{,N_{}},}_{},( ))]^{}\.\] (9)

Then, GOPSA adapts to this new target domain by minimizing the error between \(_{}\) and its estimation computed with the fitted linear model. This minimization is performed with respect to \(_{}\) that parametrizes the parallel transport of the target domain, i.e.,

\[_{}^{*}=*{arg\,min}_{}\{ _{}()(_{}- }}_{N_{}}^{}_{}( )_{}^{*}(_{}^{*}))^{2} \}\,.\] (10)

Finally, the predictions on the target domain are

\[}_{}=_{}(_{}^{*}) _{}^{*}(_{}^{*})^{N_{ }}\.\] (11)

The test-time procedure of GOPSA is summarized in Algorithm 2. The algorithm begins by calculating the Riemannian mean of the target covariance matrices \(\{_{,i}\}_{i=1}^{N_{}}\). It then iteratively optimizes the parameter \(_{}\) by computing the feature matrix (9), the derivative of the loss function (10), and updating \(_{}\) using a gradient descent step until convergence. The algorithm determines the estimated target outcomes, \(}_{}\), by using the optimized \(_{}^{*}\) on the feature matrix, combined with the pre-trained regression coefficients \(_{}^{*}(_{}^{*})\). The output result is the predicted target outcomes \(}_{}\). It should be noted that, once again, for clarity of presentation, Algorithm 2 employs a gradient descent, but other derivative-based optimization methods can be used.

## 4 Empirical benchmarks

In this section, we built empirical benchmark to evaluate the performance of GOPSA. We first present the simulated data that we used to illustrate the relevance of our method when there is a joint distribution shift of the data and the labels. Then, we present the EEG dataset that we used to evaluate the performance of GOPSA with real data from different recording sites. Finally, we present the baseline methods that are compared with GOPSA.

Simulated dataTo generate simulated data, we used the generative model described in [47; 48; 36]. The data follow the classical instantaneous mixing model:

\[_{i}(t)=_{i}(t)\] (12)

where \(_{i}(t)^{d}\) are the observed time-series, \(_{i}(t)^{d}\) are the underlying signal of the neural generators and \(\) is the mixing matrix whose columns are the observed spatial patterns of the neural generators. Furthermore, we assume that \(y\) follow a log-linear model:

\[y_{i}=_{0}+_{=1}^{d}_{}(p_{ i})\] (13)

where \(p_{ i}>0\) is the variance of the \(\)-th element of the underlying signal \(_{i}(t)\) as introduced in [47; 48; 36]. From this, we generate domains (source and target) by applying shifts on \(X\) and \(y\). To do so, we introduced a per-domain shift in the data distribution by applying an affine transformation to the covariance matrices \(_{i}[_{i}(t)_{i}(t)^{}]\):

\[_{i}_{k}^{}_{i}_{k}^{}\] (14)

with \(_{k}_{d}^{++}\) and \( 0\) controlling the amplitude of the shift. Then, we shifted the label distribution by modifying the variance of the underlying signal \(p_{ i}\):

\[p_{ i} p_{ i}^{1+k}\] (15)with \( 0\) still controlling the amplitude of the shift. Thus, the distribution of \(y\) is shifted per domain because of the log-linear relationship of (13). It should be noted that \(\) is kept constant across domains.

HarMNqEEG datasetThe HarMNqEEG dataset  was used for our numerical experiments. This dataset includes EEG recordings collected from \(1564\) participants across \(14\) different study sites, distributed across \(9\) countries. In our analysis, we consider each study site as a distinct domain. Appendix A.5 provides detailed demographic information. The EEG data were recorded with the same montage of \(19\) channels of the 10/20 International Electrodes Positioning System. The dataset provides pre-computed cross-spectral tensors for each participant rather than raw data, and anonymized metadata including the age and the sex of the participants. More precisely, the shared data consists of cross-spectral matrices with a frequency range of \(1.17\,\) to \(19.14\,\), sampled at a resolution of \(0.39\,\). A standardized recording protocol was enforced to ensure the consistency across EEG recording of the dataset. In addition to recording constraints, this protocol included artifact cleaning procedures. The cross-spectrum were computed using Bartlett's method (See Appendix A.3). Our pre-processing steps were guided by the pre-processing pipeline outlined in . First, we performed a common average reference (CAR) on all cross-spectrum (See Appendix A.3) as different EEG references were used across domains. Subsequently, we extracted the real part of the cross-spectral tensor to obtain co-spectrum tensors containing frequency-specific covariance estimates along the frequency spectrum. Due to the linear dependence between channels introduced by the CAR, the covariance matrices are rank deficient. To address this, we applied a shrinkage regularization with a coefficient of \(10^{-5}\) to the data. Additionally, we implemented a global-scale factor (GSF) correction, which compensates for amplitude variations between EEG recordings by scaling the covariance matrices with a subject-specific factor  (See Appendix A.3). Following these pre-processing steps, we obtained a set of 49 covariance matrices for each EEG recording, with each matrix corresponding to a specific frequency bin of the EEG signal. This pre-processed co-spectrum served as the input data for our domain adaptation study.

Performance evaluation and hyperparameter selectionTo evaluate the performance of the compared methods, we conducted experiments across several combinations of source and target sites. We selected source domains such that the union distribution of their predictive variable \(y\) encompasses a broad age range. All remaining sites were assigned as target domains. For each source-target combination we performed a stratified shuffle split approach with 100 repetitions on the target data. Stratification was based on the recording sites to ensure that each split contained a balanced proportion of participants from each site. The regularization parameter \(\) in Ridge regression was selected with a nested cross-validation (grid search) over a logarithmic grid of values from \(10^{-1}\) to \(10^{5}\). To evaluate the benefit of GOPSA, we compared it against four baselines. Detailed mathematical formulations of these baselines can be found in Appendix A.4. For each baseline method, the regression task was performed with Ridge regression.

Domain-aware dummy model (DO Dummy)As GOPSA requires access to the mean \(_{k}\) of each domain, we used a domain-aware dummy model predicting always the mean \(_{k}\) of each domain.

No re-center / No domain adaptation (No DA)This second baseline method involves applying the regression pipeline outlined in  without any re-centering. In this setup, all covariance matrices are projected to the tangent space at the source geometric mean \(}\) computed from all source points, no matter their recording sites.

Re-center to a common reference point (Re-center and Re-scale)As introduced in Section 2, a common transfer learning approach is a Riemannian re-centering of all domains to a common point on the manifold . This baseline thus correspond to re-centering each domain \(k\), source and target, independently by whitening them by their respective geometric mean \(}_{k}\). An extension of this approach is to perform a Riemannian re-scaling of all domains to a common dispersion, as presented in .

Domain-aware intercept (DO Intercept)This method consists in fitting one intercept \(_{0}\) per domain. In practice since we assume to know \(_{}\), we correct the predicted values so that their mean is equal to \(_{}\). This approach is in line with defining mixed-effects models on the Riemannian manifold .

Deep learning (GREEN)The GREEN model  is a deep-learning architecture tailored for EEG applications like age prediction. Since the HarMNqEEG dataset consists of covariance matrices, we used the 'G2' variant of GREEN, which starts at the covariance matrices level and includes pooling layers. This variant is designed for SPD matrices, making it an SPD network . Although GREEN has been evaluated on multiple datasets for various predictive tasks, it has not yet been applied in a domain adaptation context and does not include an adaptation layer.

We applied the domain-adaptation methods independently to each of the \(49\) frequency bins, resulting in \(49\) geometric means per domain, except for GREEN, which processes all frequency bands simultaneously. \(_{}\) of each domain was estimated on target splits (50% of the data) that do not overlap with the evaluation target splits (50% remaining). Statistical inference for model comparisons was implemented with a corrected t-test following . Experiments with \(100\) repetitions and all site combinations have been run on a standard Slurm cluster for \(12\) hours with \(250\) CPU cores.

## 5 Results

Simulated dataFigure 2 presents the results of simulated experiments where shifts are applied on either \(X\), \(y\), or both (\(X\), \(y\)) as presented in Section 4. All methods were evaluated in three simulation scenarios: shift in \(X\) only, shift in \(y\) only, and joint shift in \(X\) and \(y\). The intensity of the shift was controlled by \(\) in all scenarios. If there is no shift in \(X\), we observe that No DA perfectly estimates the \(y\) because the log-linear model is easily estimated across domains even when the \(y\) distribution changes (Figure 2 B). The performance of No DA however drops when a shift in \(X\) is introduced (Figure 2 A and C). Re-center and Re-scale led to the same results as no scaling shift was applied in the simulation. Both were able to correct the shift in \(X\), but performed poorly when a shift in \(y\) was added (Figure 2 B and C). GREEN notably showed consistant performance across all scenarios, and was relatively resistant to both types of shifts given it is not designed for domain adaptation. DO Intercept and GOPSA showed the best performance across all scenarios, with an advantage for GOPSA. The interest of GOPSA is to estimate this log-linear model with shifts in (\(X\), \(y\)) per domain (Figure 2 C) which other methods were not able to do. These experiments demonstrate the efficiency of the proposed method in estimating shifts in \(X\) between domains even in the presence of a shift in \(y\), contrary to the baseline methods. Theoretically, based on the generative model of the simulated data, the data \(X\) and outcome \(y\) are linked by a log-linear relationship. This implies that, knowing the shift in \(X\) for the target domain, predictions can be made even when \(y\) distributions do not overlap between the source and target. Since GOPSA estimates the target shift in \(X\) by minimizing \((-(_{i}))^{2}\), it is capable of handling such scenarios.

HarMNqEEG dataWe computed benchmarks for five combinations of source sites and we displayed the results for the three metrics selected for performance evaluation, each colored box representing one method (Figure 3). A min-max normalization was applied to each site combinations separately across methods. We first conducted model comparisons in terms of absolute performance across all baselines (**A**). No DA, without domain specific re-centering, performed worse than DO Dummy in terms of \(R^{2}\) score and MAE. Re-center and Re-scale led to lower performances across all metrics, which can be expected as the Riemannian mean is correlated with age in our problem setting Figure 1. Eventhough its architecture does not include an adaptation layer, GREEN

Figure 2: \(R^{2}\) **scores \(\) for different methods on simulated data.** Performance is measured across 5 source domains and 1 target domain, with shifts controlled by \(\) (0 to maximum). Data are generated 100 times, with 5 sensors and 300 covariance matrices per domain. The target domain is randomly selected between the 6 domains generated as presented in Section 4, with the remaining domains used as sources. **(A)** A shift is applied on the covariance matrices following (14). **(B)** A shift is applied on the variances following (15). **(C)** Both shifts from (14) and (15) are applied simultaneously.

reached better performance than the previous methods mentionned, but lacked consistency across site combinations and metrics with large variance especially for the \(R^{2}\) score and MAE. For all scores, DO Intercept and GOPSA reached the best average performance with lower variance. A version of Figure 3**A** without normalization is presented in Appendix A.7. As DO intercept and GOPSA showed overlapping performance distributions, we investigated their paired split-wise (non-rescaled) score differences (**B**). The site-specific differences of GOPSA scores minus DO Intercept are displayed with their associated p-values. For one site combination (Ba,Be,Cho,Co,Cu90,G,R), DO Intercept yielded higher \(R^{2}\) scores, and no significant difference was found between the two methods for Ba,Co,G. Similarly, no significant difference was observed on Spearman's \(\) results for Cu03,M,R,S. Overall, GOPSA significantly outperformed DO Intercept in five site combinations for MAE, four for Spearman's \(\) and three for \(R^{2}\) score. Detailed results for each source-target combination are presented in Appendix A.6 for Spearman's \(\), \(R^{2}\) score, and MAE. The bottom rows correspond to the mean performance of each method of all site combinations, and their average standard deviation (see Appendix A.8 for associated boxplots). We expected GOPSA to outperform the baseline methods (e.g. DO Intercept) whenever joint (\(X\), \(y\)) shifts occur. In our experimental benchmark, GOPSA significantly outperformed the baseline methods in some site combinations, but not all. This allows us to assume that not all site combinations show joint shifts.

Model inspectionNext, we investigated the impact of the different re-centering approaches on the data Figure 4. Power spectrum densities (PSDs) were computed as the mean across sensors of the diagonals of the covariance matrices Riemannian mean for each site combination after No DA, Re-center and GOPSA (**A**). PSDs for No DA display the initial variability between sites without recentering (cf. Figure 1). Re-center resulted in flat PSDs because all data were re-centered to the identity. PSDs produced by GOPSA are flattened and more similar across sites compared to No DA without removing too much information, unlike the un-effective Re-center method (cf. Figure 3). The alpha values are inspected as a function of the site mean age (**B**). Re-center leads to alpha values all equal to one as all sites are re-centered to the identity. For GOPSA, we observed a linear

Figure 3: **Normalized performance of the different methods on several source-target combinations for three metrics:** Spearman’s \(\) (left), \(R^{2}\) score \(\) (middle) and Mean Absolute Error \(\) (right). As a large variability in the score values was present between the site combinations, we applied a min-max normalization per combination to set the minimum score across all methods to 0 and the maximum score to 1. (**A**) Boxplot of the concatenated results for the three normalized scores. One point corresponds to one split of one site combination. (**B**) Boxplots of the difference between the normalized scores of GOPSA and DO Intercept. A row corresponds to one site combination, one point corresponds to one split. For each plot, the associated results of Nadeau’s & Bengio’s corrected t-test  are displayed. A p-value lower than \(0.05\) indicates a significant difference between the two methods. Ba: Barbados, Be: Bern, Chb: CHBMP (Cuba), Co: Columbia, Cho: Chongqing, Cu03: Cuba2003, Cu90: Cuba90, G: Germany, M: Malaysia, R: Russia, S: Switzerland

relationship between alpha and the sites' mean age (\(R^{2}=0.99\)). This is a direct consequence of the optimization process in GOPSA, which thus can be regarded a geodesic intercept in a mixed-effects model. Overall, GOPSA effectively re-centered sites with younger participants closer to the identity matrix. Re-centering sites around a common point helped reduce the shift in \(X\), while not placing all sites at the exact same reference point helped manage the shift in \(y\), hence preserving the statistical associations between \(X\) and \(y\).

## 6 Conclusion

We proposed a novel multi-source domain adaptation approach that adapts shifts in \(X\) and \(y\) simultaneously by learning jointly a domain specific re-centering operator and the regression model. GOPSA was specifically developed to handle joint shifts in the data distribution and the outcome distribution, as illustrated by the simulations in Figure 2. GOPSA is a test-time method that does not require to retrain a model when a new domain is presented. GOPSA achieved state-of-the-art performance on the HarMNqEEG  dataset with EEG from \(14\) recording sites and over \(1500\) participants. Our benchmarks showed a significant gain in performance for three different metrics in a majority of site combinations compared to baseline methods. GOPSA can thus be used by researchers as a decision rule to infer the presence of joint shifts and, hence, serve as a tool for data exploration and model interpretation. While we focused on shallow regression models, the implementation of GOPSA using PyTorch readily supports its inclusion in more complex Riemannian deep learning models [26; 56; 11; 40; 31]. This direction seems promising given our observation that GREEN - a simple deep net combining Riemannian computation with a fully connected layer - already possessed some intrinsic robustness to data shifts. This may point at the capacity of the fully-connected layer to provide additional non-linear transformations that can accommodate the data-generating scenario in which continuous log-linear generators are modified in a discrete manner by site factors. More generally, it emphasizes the potential of complex nonlinear methods for domain adaptation, in line with a recent study on the same dataset reporting positive generalization results using a kernel method . Furthermore, although this work specifically addresses age prediction, the methodology is applicable to a broader range of regression analyses. While GOPSA necessitates knowledge or estimability of the average \(\) per domain, this requirement aligns with that of mixed-effects models [61; 25; 16], which are extensively employed in biomedical statistics. By combining mixed-effects modeling with Riemannian geometry for EEG, GOPSA opens up various applications at the interface between machine learning and biostatistics, such as, biomarker exploration in large multicenter clinical trials [46; 52; 53].

Figure 4: **Model inspection of GOPSA versus No DA and Re-center.** Power Spectral Densities (PSDs) and \(\) values were computed on the source sites Barbados, Chongqing, Germany, and Switzerland. The remaining sites were used as target domains. (**A**) Mean PSDs computed across sensors for No DA, Recenter and GOPSA on two source (Barbados and Switzerland) and two target (New York and Columbia) sites. (**B**) \(\) values versus site’s mean age for Re-center and GOPSA. One point corresponds to one site. The coefficient of determination is reported for the GOPSA method.