# Self-Supervised Reinforcement Learning that Transfers using Random Features

Boyuan Chen

Massachusetts Institute of Technology

Boston, MA 02139

boyuanc@mit.edu

&Chuning Zhu

University of Washington

Seattle, WA 98105

zchuning@cs.washington.edu

&Pulkit Agrawal

Massachusetts Institute of Technology

Boston, MA 02139

pulkitag@mit.edu

&Kaiqing Zhang

University of Maryland

College Park, MD 20742

kaiqing@umd.edu

&Abhishek Gupta

University of Washington

Seattle, WA 98105

abhgupta@cs.washington.edu

Equal contribution.Equal advising.

###### Abstract

Model-free reinforcement learning algorithms have exhibited great potential in solving single-task sequential decision-making problems with high-dimensional observations and long horizons, but are known to be hard to _generalize_ across tasks. Model-based RL, on the other hand, learns task-agnostic models of the world that naturally enables transfer across different reward functions, but struggles to scale to complex environments due to the compounding error. To get the best of both worlds, we propose a self-supervised reinforcement learning method that enables the transfer of behaviors across tasks with different rewards, while circumventing the challenges of model-based RL. In particular, we show self-supervised pre-training of model-free reinforcement learning with a number of _random features_ as rewards allows _implicit_ modeling of long-horizon environment dynamics. Then, planning techniques like model-predictive control using these implicit models enable fast adaptation to problems with new reward functions. Our method is self-supervised in that it can be trained on offline datasets _without_ reward labels, but can then be quickly deployed on new tasks. We validate that our proposed method enables transfer across tasks on a variety of manipulation and locomotion domains in simulation, opening the door to generalist decision-making agents.

## 1 Introduction

As in most machine learning problems, the ultimate goal of building deployable sequential decision-making agents via reinforcement learning (RL) is its broad _generalization_ across tasks in the real world. While reinforcement learning algorithms have been shown to successfully synthesize complex behavior in _single-task_ sequential decision-making problems , their performance as _generalist agents_ across tasks has been less convincing . In this work, we focus on the problem of learning generalist agents that are able to transfer across problems where the environment dynamics are shared, but the reward function is changing. This problem setting is reflective of scenariosthat may be encountered in real-world settings such as robotics. For instance, in tabletop robotic manipulation, different tasks like pulling an object, pushing an object, picking it up, and pushing to different locations, all share the same transition dynamics, but involve different reward functions. We hence ask the question - _Can we reuse information across tasks in a way that scales to high dimensional, longer horizon problems?_

A natural possibility to tackle this problem is direct policy search . Typical policy search algorithms can achieve good performance on a single task. However, the policy is optimized for a particular reward and may be highly suboptimal in new scenarios. Other model-free RL algorithms like actor-critic methods  or Q-learning  may exacerbate this issue, with learned Q-functions entangling dynamics, rewards, and policies. For new scenarios, an ideal algorithm should be able to disentangle and retain the elements of shared dynamics, allowing for easy substitution of rewards without having to repeat significant computations. While multi-task policy search techniques like goal-conditioned RL address this issue to some extent, they are restricted to particular tasks like goal-reaching and usually require prior knowledge about the tasks one may encounter in testing.

Model-based RL arises as a natural fit for disentangling dynamics and rewards . These algorithms directly learn a model of transition dynamics and leverage the learned model to plan control actions . In particular, the models can be used to _re-plan_ behaviors for new rewards. However, these learned dynamics models are brittle and suffer from compounding error  due to their autoregressive nature. States predicted by the model are iteratively used as inputs for subsequent predictions, leading to compounding approximation errors during planning . This makes it challenging to apply them to problems with long horizons and high dimensions.

In this work, we thus ask - _Can we build RL algorithms that disentangle dynamics, rewards, and policies for transfer across problems, while retaining the ability to solve problems with high dimensional observations and long horizons?_ To this end, we propose a self-supervised RL algorithm that can leverage collected data to _implicitly_ model transition dynamics without ever having to _generate_ future states, and then use this to quickly transfer to a variety of different new tasks with varying reward functions that may be encountered at test time.

Our method relies on the following key insight: to prevent tying specific reward functions and policies to the RL algorithm, one can instead model the long-term evolution of randomly chosen basis functions of all possible reward functions . Such basis functions can be generated using random features . The accumulation of such basis functions over time, which we refer to as a Q-basis, is task-agnostic , and _implicitly_ models the transition dynamics of the problem. We show that with a _large enough_ number of random features and model-free RL, the value function for _any_ reward function that may be encountered in testing could be estimated, via a simple linear combination of these Q-basis functions. This is important since it allows us to obtain the benefits of transfer, without explicitly learning a dynamics model that may suffer from compounding error. Moreover, with randomly chosen basis functions that are agnostic of downstream tasks, this is not restricted to just a certain distribution of tasks, as in most multi-task or goal-conditioned RL . Finally, we note that the random features (as well as the corresponding Q-basis functions) can be generated using datasets that are not necessarily _labeled_ with reward signals. Indeed, these random-feature-based pseudo rewards can be viewed as self-supervised signals in training, making the approaches based on them naturally compatible with large (task-agnostic) offline datasets.

Based on the aforementioned insights, we propose a new algorithm _Random Features for Model-Free Planning_ (RaMP) that allows us to leverage unlabelled offline datasets to learn reward-agnostic Q-bases. These can be used to estimate the test-time value functions for new reward functions using linear regression, enabling quick adaptation to new tasks under the same shared transition dynamics. We show the efficacy of this method on a number of tasks for robotic manipulation and locomotion in simulation, and highlight how RaMP provides a more general paradigm than typical generalizations of model-based or model-free RL.

Figure 1: RaMP is a self-supervised reinforcement learning algorithm that pre-training on exploration trajectories without (or with unknown) reward. It then quickly adapts to new tasks through interaction with the environment.

### Related Work

Model-based RL is naturally suited for transfer across tasks with shared dynamics, by explicitly learning a model of the transition dynamics, which is disentangled with the reward functions for each task . These models are typically learned via supervised learning on one-step transitions and then used to extract control actions via planning  or trajectory optimization . The key challenge in scaling lies in the fact that they sequentially feed model predictions back into the model for sampling . This can often lead to compounding errors , which grow with the horizon length unfavorably. In contrast, our work does not require autoregressive sampling, making it easier to scale to longer horizons and higher dimensions.

On the other hand, model-free RL avoids the challenge of compounding error by directly modeling either policies or Q-values without autoregressive generation . However, these approaches entangle rewards, dynamics, and policies, making them hard to transfer. While attempts have been made to build model-free methods that generalize across rewards, such as goal-conditioned value functions  or multi-task policies , they only apply to restricted classes of reward functions and particular training distributions. Our work aims to obtain the best of both worlds (model-based and model-free RL), learning some representation of dynamics independent of rewards and policies, while using a model-free algorithm for learning.

Our notion of long-term dynamics is connected to the notion of state-action occupancy measure , often used for off-policy evaluation and importance sampling methods in RL. These methods often try to directly estimate either densities or density ratios . Our work simply learns the long-term accumulation of random features, without requiring any notion of normalized densities. Perhaps most closely related work to ours is the framework of _successor features_, that considers transfer from a fixed set of source tasks to new target tasks . Like our work, the successor features framework leverages the linearity of rewards to disentangle dynamics from rewards using model-free RL. However, transfer using successor features is dependent on choosing (or learning) the right featurization and forces an implicit dependence on the policy. Our work leverages random features and multi-step Q-functions to alleviate the transfer performance of successor features.

## 2 Preliminaries

Model.We consider the standard Markov decision process (MDP) as characterized by a tuple \(=(,,,R,,)\), with state space \(\), action space \(\), transition dynamics \(:()\), reward function \(R:([-R_{},R_{}])\), discount factor \([0,1)\), and initial state distribution \(()\). The goal is to learn a policy \(:()\), such that it maximizes the expected discounted accumulated rewards, i.e., solves \(_{}_{}[_{h=1}^{}^{h-1}r_{h}]\) with \(r_{h}:=r(s_{h},a_{h}) R_{s_{h},a_{h}}=( s_{h},a_{h})\). Hereafter, we will refer to an MDP and a _task_ interchangeably. Note that in our problem settings, different MDPs will always share transition dynamics \(\), but will have varying reward functions \(R\).

Q-function Estimation.Given an MDP \(\), one can define the state-action \(Q\)-value function under any policy \(\) as \(Q_{}(s,a):=_{a_{h}( s_{h})\\ s_{h+1}( s_{h},a_{h})}_{h=1 }^{}^{h-1}r_{h}\,s_{1}=s,a_{1}=a\) which denotes the expected accumulated reward under policy \(\), when starting from state-action pair \((s,a)\). By definition, this \(Q\)-function is inherently tied to the particular reward function \(R\) and the policy \(\), making it challenging to transfer for a new reward or policy. Similarly, one can also define the _multi-step (\(\)-step) \(Q\)-function \(Q_{}(s,_{1},_{2},,_{})= _{a_{+h}( s_{+h})\\ s_{h+1}( s_{h},a_{h})}_{h=1 }^{}^{h-1}r_{h}\,s_{1}=s,a_{1}=_{1},a _{2}=_{2},,a_{}=_{}\)_.

One can estimate the \(\)-step \(Q_{}\) by Monte-Carlo sampling of the trajectories under \(\), i.e., by solving

\[_{}\ \ _{j=1}^{N} (s,_{1}^{j},_{2}^{j},,_ {}^{j})-_{m=1}^{M}_{h=1}^{}^{h-1}r_{h}^{m,j }_{2}^{2},\] (2.1)

where \(\) is some function class for \(Q\)-value estimation, which in practice is some parametric function class, e.g., neural networks; \(r_{h}^{m,j} R_{s_{h}^{m,j},a_{h}^{m,j}}\) and \((s_{h}^{m,j},a_{h}^{m,j})\) come from \(MN\) trajectories thatare generated by \(N\) action sequences \(\{(_{1}^{j},_{2}^{j},,_{r}^{j})\}_{j=1 }^{N}\) and \(M\) trajectories following policy \(\) after each action sequence. A large body of work considers finding this \(Q\)-function using dynamic programming, but for the sake of simplicity, this work will only consider Monte-Carlo estimation.

In practice, the infinite-horizon estimator in (2.1) can be hard to obtain. We hence use a finite-horizon approximation of \(Q_{}\) (of length \(H\)), denoted by \(Q_{}^{H}\), in learning. We will treat this finite-horizon formula in the main paper for practical simplicity. We also provide a method to deal with the infinite-horizon case directly by bootstrapping the value function estimate. We defer the details to Appendix B.1. Note that if one chooses \(H=\), then the \(\)-step \(Q\)-function defined above becomes \(Q_{}^{H}(s,_{1},_{2},,_{H}) :=_{s_{h+1}(\,\,|\,s_{h},a_{h})}_{h= 1}^{H}^{h-1}r_{h}\,|\,s_{1}=s,a_{1}=_{1},,a_{H}= _{H}\). Note that in this case, the \(Q\)-function is irrelevant to the policy \(\), denoted by \(Q^{H}\), and is just the expected accumulated reward executing the action sequence \((_{1},_{2},,_{H})\) starting from the state \(s\). This Q-function can be used to score how "good" a sequence of actions will be, which in turn can be used for planning.

Problem setup.We illustrate our problem setup in Figure 1. Consider a transfer learning scenario, where we assume access to an offline dataset consisting of several episodes \(=\{(s_{h}^{m},a_{h}^{m},s_{h+1}^{m})\}_{h[H],m[M]}\). Here \(H\) is the length of the trajectories, which is large enough, e.g., of order \((1/(1-))\) to approximate the infinite-horizon setting; \(M\) is the total number of trajectories. This dataset assumes that all transitions are collected under the same transition dynamics \(\), but otherwise does not require the labels, i.e., rewards, and may even come from different behavior policies. This is reminiscent of the offline RL problem statement, but offline RL datasets are typically _labeled_ with rewards and are _task-specific_. In contrast, in this work, the pre-collected dataset is used to quickly learn policies for _any_ downstream reward, rather than for one specific reward function. The goal is to make the best use of the dataset \(\), and generalize the learned experience to improve the performance on a new task \(\), with the same transition dynamics \(\) but an arbitrary reward function \(R\). Note that unlike some related work [5; 4], we make _no_ assumption on the reward functions of the MDPs that generate \(\). The goal of the learning problem is to _pre-train_ on the offline dataset such that we can enable fast (even zero-shot) adaptation to arbitrary new reward functions encountered at test time.

## 3 RaMP: Learning Implicit Models for Cross-Reward Transfer with Self-Supervised Model-Free RL

We now introduce our algorithm, Random Features for Model-Free Planning (RaMP), to solve the problem described in Section 2 - learning a model of long-term dynamics that enables transfer to tasks labeled with arbitrary new rewards, while making use of the advantages of model-free RL. Clearly, the problem statement we wish to solve requires us to (1) be able to solve tasks with long horizons and high-dimensional observations and (2) alleviate policy and reward dependence to be able to transfer computation across tasks. The success of model-free RL in circumventing compounding error from autoregressive generation raises the natural question: _Is there a model-free approach that can mitigate the challenges of compounding error and can transfer across tasks painlessly?_ We answer this in the affirmative in the following section.

### Key Idea: Implicit Model Learning with Random Feature Cumulants and Model-Free RL

The key insight we advocate is that we can avoid task dependence if we directly model the long-term accumulation of many random functions of states and actions (treating them as the rewards), instead of modeling the long-term accumulation of one specific reward as in typical Q-value estimation. Since these random functions of the state and actions are task-agnostic and uninformed of any specific reward function, they simply capture information about the transition dynamics of the environment. However, they do so without actually requiring autoregressive generative modeling, as is commonplace in model-based RL. Doing so can effectively disentangle transition dynamics from reward, and potentially allow for transfer across tasks, while still being able to make use of model-free RL methods. Each long-term accumulation of random features is referred to as an element of a "random" Q-basis, and can be learned with simple modifications to typical model-free RL algorithms. The key is to replace the Q-estimation of a single _task-specific_ reward function with estimating the Q-functions of a set of _task-agnostic_ random functions of the state and actions as a random Q-basis.

At _training time_, the offline dataset \(\) can be used to learn a set of "random" Q-basis functions for different random functions of the state and action. This effectively forms an "implicit model", as it carries information about how the dynamics propagate, without being tied to any particular reward function. At _test time_, given a new reward function, we can recombine Q-basis functions to effectively approximate the true reward-specific Q-function under any policy. This inferred Q-function can then be used for planning for the new task. As we will show in Section 3.4, this recombination can actually be done by a linear combination of Q-basis functions for a sufficiently rich class of random features, reducing the problem of test-time adaptation for new rewards to a simple linear regression problem. We detail the two phases of our approach in the following subsections.

### Offline Training: Self-Supervised Learning Random Q-functions

Without any prior knowledge about the downstream test-time rewards, the best that an agent can do is to model the evolution of the state (i.e., model system dynamics). The key insight we make is that the evolution of state can be implicitly represented by simply modeling the long-term accumulation of _random_ features of state to obtain a _set_ of Q-basis functions. Such a generation of random features is fully self-supervised. These functions can then be combined to infer the task-specific Q-function.

Given the lack of assumptions about the downstream task, the random features being modeled must be expressive and universal in their coverage. As suggested in [39; 40; 41], random features can be powerful in that most nonlinear functions can be represented as linear combinations of random features effectively. As we will show in Section 3.4, modeling the long-term accumulation of random features allows us express the value function for _any_ reward as a **linear** combination of these accumulations of random features. In particular, in our work, we assume that the random features are represented as the output of \(K\) neural networks \((,;_{k}):\) with weights \(_{k}^{d}\) and \(k[K]\), where \(_{k}\) are _randomly_ sampled i.i.d. from some distribution \(p\). Sampling \(K\) such weights \(_{k}\) with \(k[K]\) yields a vector of scalar functions \([(,;_{k})]_{k[K]}^{K}\) for any \((s,a)\), which can be used as random features. To model the long-term accumulation of each of these random features, we note that they can be treated as reward functions (as can any function of state and action ) in model-free RL, and standard model-free policy evaluation to learn Q-functions can be reused to learn a set of \(K\) Q-basis functions, with each of them corresponding to the Q-value of a random feature.

We note that done naively, this definition of a Q-basis function is tied to a particular policy \(\) that generates the trajectory. However, to transfer behavior one needs to predict the accumulated random features under new sequences of actions, as policy search for the new task is likely to involve evaluating a policy that is not the same as the training policy \(\). To allow the modeling of accumulated features that is independent of particular policies, we propose to learn _multi-step_ Q-basis functions for each of the random features (as discussed in Section 2), which is explicitly dependent on an input sequence of actions \((_{1},_{2},,_{})\). This can be used to search for optimal actions in new tasks.

To actually learn these Q-basis functions (one for each random feature), we use Monte-Carlo methods for simplicity. We generate a new dataset \(_{}\) from \(\), with \(_{}=\{((s_{1}^{m},a_{1:H}^{m}),_{h[H]}^{h-1} (s_{h}^{m},a_{h}^{m};_{k}))\}_{m[M],k[K]}\). Here we use

Figure 2: RaMP: Depiction of our proposed method for transferring behavior across tasks by leveraging model-free learning of random features. At training time, Q-basis functions are trained on accumulated random features. At test time, adaptation is performed by solving linear regression and recombining basis functions, followed by online planning with MPC.

\(_{h[H]}^{h-1}(s_{h}^{m},a_{h}^{m};_{k})\) as the accumulated random features for action sequences \(\{a_{1},,a_{H}\}\) taken from state \(s_{1}\). We then use \(K\) function approximators representing each of the \(K\) Q-basis functions, e.g., neural networks \((,;_{k}):^{H}\) for \(k[K]\), to fit the accumulated random features. Specifically, we minimize the following loss \(_{\{_{k}\}}~{}~{}_{m[M],k[K]}(s_{1}^{m },a_{1:H}^{m};_{k})-_{h[H]}^{h-1}(s_{h}^{m},a_{h}^{m}; _{k})^{2}\).

This objective is a Monte Carlo estimate, but we can also leverage dynamic programming to do so in an infinite-horizon way, as we show in Appendix B.1.

### Online Planning: Inferring Q-functions with Linear Regression and Planning

Our second phase concerns online planning to transfer to new tasks with arbitrary rewards using our learned Q-basis functions. For a sufficiently expressive set of random non-linear features, any non-linear reward function can be expressed as a linear combination of these features. As we outline below, this combined with linearity of expectation, allows us to express the task-specific Q-function for any particular reward as a linear combination of learned Q-bases of random functions. Therefore, we can obtain the test-time Q-function by solving a _simple_ linear regression problem from random features to instantaneous rewards, and use the _same_ inferred coefficients to recombine Q-bases to obtain task-specific Q-functions. This inferred Q-function can then be used to plan an optimal sequence of actions. We describe each of these steps below.

#### 3.3.1 Reward Function Fitting with Randomized Features

We first learn how to express the reward function for the new task as a linear combination of the random features. This can be done by solving a linear regression problem to find the coefficient vector \(w=[w_{1},,w_{K}]^{}\) that approximates the new task's reward function as a linear combination of the (non-linear) random features. Specifically, we minimize the following loss

\[w^{*}=*{argmin}_{w}~{}~{}_{h[H],m[M]} r(s_{h}^{m},a_{h}^{m})-_{k[K]}w_{k}(s_{h}^{m},a_{h}^{m};_{k}) ^{2}+\|w\|_{2}^{2},\] (3.1)

where \( 0\) is the regularization coefficient, and \(r(s_{h}^{m},a_{h}^{m}) R_{s_{h}^{m},a_{h}^{m}}\). Due to the use of random features, Eq. (3.1) is a _ridge regression_ problem, and can be solved efficiently.

Given these weights, it is easy to estimate an approximate multi-step Q-function for the true reward on the new task by linearly combining the Q-basis functions learned in the offline training phase \(\{(,;_{k}^{*})\}_{k[K]}\) according to the _same_ coefficient vector \(w^{*}\). This follows from the additive nature of reward and linearity of expectation. In particular, if the reward function \(r(s,a)=_{k[K]}w_{k}^{*}(s,a;_{k})\) holds approximately, which will be the case for large enough \(K\) and rich enough \(\), then the approximate Q-function for the true test-time reward under the sequence \(\{a_{1},,a_{H}\}\) satisfies \(Q^{H}(s_{1},a_{1:H}):=_{s_{h+1(s_{h},a_{h})}}[_{h [H]}^{h-1}R_{s_{h},a_{h}}]_{k[K]}w_{k}^{*}(s _{1},a_{1:H};_{k}^{*})\), where \(\{w_{k}^{*}\}_{k[K]}\) is the solution to the regression problem (Eq. (3.1)) and \(\{_{k}^{i}\}_{k[K]}\) is the solution to the Q-basis fitting problem described in Section 3.2.

#### 3.3.2 Planning with Model Predictive Control

To obtain the optimal sequence of actions, we can use the inferred approximate Q-function for the true reward \(Q^{H}(s_{1},a_{1:H})\) for online planning at each time \(t\) in the new task: at state \(s_{t}\), we conduct standard model-predictive control techniques with random shooting , i.e., randomly generating \(N\) sequences of actions \(\{a_{1}^{n},,a_{H}^{n}\}_{n[N]}\), and find the action sequence with the maximum Q-value such that \(n_{t}^{*}*{argmax}_{n[N]}~{}~{}_{k[K]}w_{k}^{*}(s _{t},a_{t:t+H-1}^{n};_{k}^{*})\).

We note that this finite-horizon planning is an approximation of the actual infinite-horizon planning, which may be enhanced by an additional reward-to-go term in the objective. We provide such an enhancement with details in Appendix B.1, although we found minimal effects on empirical performance. We then execute \(a_{t}^{n_{t}^{*}}\) from the sequence \(n_{t}^{*}\), observe the new state \(s_{t+1}\), and replan. We refer readers to Appendix F for a detailed connection of our proposed method to existing work and Appendix A for pseudocode.

### Theoretical Justifications

We now provide some theoretical justifications for the methodology we adopt. To avoid unnecessary nomenclature of measures and norms in infinite dimensions, we in this section consider the case that \(\) and \(\) are discrete (but can be enormously large). Due to space limitation, we present an abridged version of the results below, and defer the detailed versions and proofs in Appendix D. We first state the following result on the expressiveness of random cumulants.

**Theorem 3.1** (\(Q\)-function approximation; Informal).: Under standard coverage and sampling assumptions of offline dataset \(\), and standard assumptions on the boundedness and continuity of random features \((s,a;)\), it follows that with horizon length \(H=(/)}{1-})\) and \(M=(^{4}})\) episodes in dataset \(\), and with \(K=((1-)^{-2}^{-2})\) random features, we have that for any given reward function \(R\), and any policy \(\)

\[\|_{}^{H}(w^{*})-Q_{}\|_{}()+ }(f)}\]

with high probability, where \(_{}^{H}(s,a;w^{*})\) is defined as \(_{h=1}^{H}^{h-1}_{k[K]}w_{k}^{*}(s_{h}, a_{h};_{k})\,|\,s_{1}=s,a_{1}=a\) for each \((s,a)\) and can be estimated from the offline dataset \(\); \(_{f}(f)\) is the infimum expected risk over the function class \(\) induced by \(\).

Theorem 3.1 is an informal statement of the results in Appendix D.2, which specifies the number of random features, the horizon length per episode, and the number of episodes, in order to approximate \(Q_{}\) accurately by using the data in \(\), under any given reward function \(R\) and policy \(\). Note that the number of random features is not excessive and is polynomial in problem parameters. We also note that the results can be improved under stronger assumptions of the sampling distributions \(p\) and kernel function classes [41; 29].

Next, we justify the use of multi-step \(Q\)-functions in planning in a deterministic transition environment, which contains all the environments our empirical results we will evaluate later. Recall that for any given reward \(R\), let \(Q_{}\) denote the \(Q\)-function under policy \(\). Note that with a slight abuse of notation, \(Q_{}\) can also be the multi-step \(Q\)-function (see definition in Section 2), and the meaning should be clear from the input, i.e., whether it is \(Q_{}(s,a)\) or \(Q_{}(s,a_{1},,a_{H})\).

**Theorem 3.2**.: Let \(\) be some subclass of Markov stationary policies, i.e., for any \(\), \(:()\). Suppose the transition dynamics \(\) is deterministic. For any given reward \(R\), denote the \(H\)-step policy obtained from \(H\)-step policy improvement over \(\) as \(^{}_{H}:^{H}\), defined as \(^{}_{H}(s)*{argmax}_{(a_{1},,a_{H}) ^{H}}_{}\ Q_{}(s,a_{1},,a_{H}),\) for all \(s\). Let \(V_{^{}_{H}}\) denote the value function under the policy \(^{}_{H}\) (see formal definition in Appendix D.1). Then, we have that for all \(s\)

\[V_{^{}_{H}}(s)_{a_{1:H}}_{n}Q_{}(s,a_{1},, a_{H})_{a}_{}Q_{}(s,a).\]

The proof of Theorem 3.2 can be found in Appendix D.2. The result can be viewed as a generalization of the generalized policy iteration result in  to multi-step action-sequence policies. Taking \(\) to be the set of policies that generate the data, the result shows that the value function of the greedy multi-action policy improves over all the possible \(H\)-step multi-action policies, with the policy after step \(H\) to be any policy in \(\). Moreover, the value function by \(^{}_{H}\) also improves overall one-step policies if the policy after the first step onwards follows any policy in \(\). This is due to the fact that \(\) (coming from data) might be a much more restricted policy class than any action sequence \(a_{1:H}\).

## 4 Experimental Evaluation

In this section, we aim to answer the following research questions: **(1)** Does RaMP allow for effective transfer of behaviors across tasks with varying rewards? **(2)** Does RaMP scale to domains with high-dimensional observation and action spaces and long horizons? **(3)** Which design decisions in RaMP enable better transfer?

### Experiment Setup

Across several domains, we evaluate the ability of RaMP to leverage the knowledge of shared dynamics from an offline dataset to quickly solve new tasks with arbitrary rewards.

**Offline Dataset Construction:** For each domain, we have an offline dataset collected by a behavior policy as described in Appendix C.2. Typically this behavior policy is a mixture of noisy policies accomplishing different objectives in each domain. Although RaMP and other model-based methods do not require knowledge of any reward from the offline dataset, other baseline comparisons will require privileged information. Baseline comparison methods like model-free RL and successor features require the provision of a set of training objectives, as well as rewards labeled for these objectives on state actions from the offline dataset. We call such objectives "offline objectives" and a dataset annotated with these offline objectives and rewards a privileged dataset.

**Test-time adaptation:** At test-time, we select a novel reward function for online adaptation, referred to as 'online objective' below. The online objective may correspond to rewards conditioned on different goals or even arbitrary rewards, depending on the domain. Importantly, the online objective need not be drawn from the same distribution as the privileged offline objectives.

Given this problem setup, we compare RaMP with a variety of baselines. (1) MBPO  is a model-based RL method that learns a standard one-step dynamics model and uses actor-critic methods to plan in the model. We pre-train the dynamics model for MBPO on the offline dataset. (2) PETS  is a model-based RL method that learns an ensemble of one-step dynamics models and performs MPC. We pre-train the dynamics model on the offline dataset and use the cross-entropy method (CEM) to plan. (3) Successor feature (SF)  is a framework for transfer learning in RL as described in Sec. 1.1. SF typically assumes access to a set of policies towards different goals along with a learned featurization, so we provide it with the privileged dataset to learn a set of training policies. We also learn successor features with the privileged dataset. (4) CQL : As an _oracle_ comparison, we compare with a goal-conditioned variant of an offline RL algorithm (CQL). CQL is a model-free offline RL algorithm that learns policies from offline data. While model-free offline RL naturally struggles to adapt to arbitrarily changing rewards, we provide CQL with information about the goal at both training and testing time. CQL is trained on the distribution of training goals on the offline dataset, and finetuned on the new goals at test time. The CQL comparison is assuming access to more information than RaMP. Each method is benchmarked on each domain with \(4\) seeds.

### Transfer to Unseen Rewards

We first evaluate the ability of RaMP to learn from an offline dataset and quickly adapt to unseen test rewards in \(4\) robotic manipulation environments from Meta-World . We consider skills like

Figure 4: Reward transfer results on Metaworld, Hopper and D’Claw environments. RaMP adapts to novel rewards more rapidly than MBPO, Successor Features, and CQL baselines.

Figure 3: We evaluate our method on manipulation, locomotion, and high dimensional action space environments. The green arrow in each environment indicates the online objective for policy transfer while the red arrows are offline objectives used to label rewards for the privileged dataset.

reaching a target across the wall, opening a door, turning on a faucet, and pressing a button while avoiding obstacles, which are challenging for typical model-based RL algorithms (Fig. 3). Each domain features \(50\) different possible goal configurations, each associated with a different reward but the same dynamics. The privileged offline objectives consist of \(25\) goal configurations. The test-time reward functions are drawn from the remaining \(25\) "out-of-distribution" reward functions. We refer the reader to Appendix C.1 for details.

As shown in Fig 4, our method adapts to test reward most quickly across all four meta-world domains. MBPO slowly catches up with our performance with more samples, since it still needs to learn the Q function from scratch even with the dynamics branch trained. PETS adapts faster than MBPO but performs worse than our method as it suffers from compounding error. In multiple environments, successor features barely transfer to the online objective as it entangles policies that are not close to those needed for the online objective. Goal-conditioned CQL performs poorly in all tasks as it has a hard time generalizing to out-of-distribution goals. In comparison, RaMP is able to deal with arbitrary sets of test time rewards, since it does not depend on the reward distribution at training time.

### Scaling to Tasks with Longer Horizons and High Dimensions

We further evaluate the ability of our method to scale to tasks with longer horizons. We consider locomotion domains such as the Hopper environment from OpenAI Gym . We chose the offline objectives to be a mixture of skills such as standing, sprinting, jumping, or running backward with goals defined as target velocities or heights. The online objectives consist of unseen goal velocities or heights for standing, sprinting, jumping or running. As shown in Fig. 4, our method maintains the highest performance when adapting to novel online objectives, as it avoids compounding errors by directly modeling accumulated random features. MBPO adapts at a slower rate since higher dimensional observation and longer horizons increase the compounding error of model-based methods. We note that SF is performing reasonably well, likely because the method also reduces the compounding error compared to MBPO, and it has privileged information.

To understand whether RaMP can scale to higher dimensional state-action spaces, we consider a dexterous manipulation domain (referred to as the D'Claw domain in Fig 3). This domain has a 9 DoF action space controlling each of the joints of the hand as well as a 16-dimensional state space including object position. The offline dataset is collected moving the object to different orientations, and the test-time rewards are tasked with moving the object to new orientations (as described in Appendix C.1). Fig 4 shows that both Q-estimation and planning with model-predictive control remain effective when action space is large. In Appendix C.3, we test our method on environments with even higher dimensional observations such as images.

### Ablation of Design Choices

To understand what design decisions in RaMP enable better transfer and scaling, we conduct ablation studies on various domains, including an analytical 2D point goal-reaching environment and its variants (described in Appendix C.1), as well as the classic double pendulum environment and meta-world reaching. We report an extensive set of ablations in Appendix C.5.

#### Reduction of compounding error with multi-step Q functions

We hypothesize that our method does not suffer from compounding errors in the same way that feedforward dynamics models do.

In Table 1, we compare the approximation error of truncated Q values computed with (1) multi-step Q functions obtained as a linear combination of random cumulants (Ours), and (2) rollouts of a dynamics model (MBRL). We train the methods on the offline data and evaluate on the data from a novel task at test time. As shown in Table 1, our method outperforms feedforward dynamics models.

## 5 Discussion

In this work, we introduce RaMP, a method that leverages diverse unlabeled offline data to learn models of long horizon dynamics behavior, while being able to naturally transfer across tasks with different reward functions. To this end, we propose to learn the long-term evolution of random features

    & Hopper & Pendulum \\  Ours & **25.7 \(\) 5.4** & **65.1 \(\) 0.4** \\ MBRL & 50.2 \(\) 9.7 & 395.4 \(\) 5.8 \\   

Table 1: Policy evaluation error. Feed-forward dynamics model suffers from compounding error that is particularly noticeable in domains with high action dimensions or chaotic dynamics. Our method achieves low approximation error in both domains.

under different action sequences, which can be generated in a self-supervised fashion without reward labels. This way, we are able to disentangle dynamics, rewards, and policies, while without explicitly learning the dynamic model. We validate our approach across a number of simulated robotics and control domains, with superior transfer ability than baseline comparisons. There are also limitations of the current approach we would like to address in the future work: the current Monte-Carlo-based value function estimation may suffer from high variance, and we hope to incorporate the actor-critic algorithms  in RaMP to mitigate it; current approach is only evaluated on simulated environments, and we hope to further test it on real robotic platforms.