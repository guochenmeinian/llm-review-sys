# Unity by Diversity: Improved Representation Learning for Multimodal VAEs

Thomas M. Sutter\({}^{1}\), Yang Meng\({}^{3}\), Andrea Agostini\({}^{1}\), Daphne Chopard\({}^{1,2}\),

Norbert Fortin\({}^{4}\), Julia E. Vogt\({}^{1}\), Babak Shahbaba\({}^{3}\), Stephan Mandt\({}^{3,5}\)

\({}^{1}\)Department of Computer Science, ETH Zurich

\({}^{2}\)Department of Intensive Care and Neonatology, University Children's Hospital Zurich

\({}^{3}\)Department of Statistics, UC Irvine

\({}^{4}\)Department of Neurobiology and Behavior, UC Irvine

\({}^{5}\)Department of Computer Science, UC Irvine

###### Abstract

Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information better from its uncompressed original features. In extensive experiments on multiple benchmark datasets and two challenging real-world datasets, we show improved learned latent representations and imputation of missing data modalities compared to existing methods.

## 1 Introduction

The fusion of diverse modalities and data types is transforming our understanding of complex phenomena, enabling more nuanced and comprehensive insights through the integration of varied information sources. Consider, for instance, the role of a medical practitioner who synthesizes multiple tests and measurements during diagnosis and treatment. This process involves merging shared information across different tests and identifying test-specific details, both of which are critical for optimal patient care and medical decision-making.

Among the existing methods, multimodal Variational Autoencoders (VAEs) have emerged as a promising approach for jointly modeling and learning from weakly-supervised heterogeneous data sources. While scalable multimodal VAEs utilizing a single shared latent space efficiently handle multiple modalities , finding an optimal method to aggregate these modalities remains challenging. The aggregation methods and resulting joint representations are often suboptimal and overly restrictive , leading to inferior latent representations and generative quality. This trade-off between shared and modality-specific information in the latent representations of multimodal VAEs results in limited quality or coherence in generated samples, even in relatively simple scenarios.

In this work, we propose a novel multimodal VAE, termed the multimodal variational mixture-of-experts prior (MMVM) VAE, to overcome the aforementioned limitations. Instead of modeling the dependencies between different modalities through a joint posterior approximation, we introduce a multimodal and data-dependent prior distribution (see Figure 1). Our proposed multimodal objective is inspired by the VAMP-prior formulation introduced by Tomczak and Welling (2017), which is traditionally used to learn an optimal prior distribution between unimodal data samples, whereas we aim for an optimal prior between different modalities of the same data sample. The resulting regularization term in the VAE objective can be interpreted as minimizing the distance between positive pairs, similar to contrastive learning methods (Oord et al., 2019; Tian et al., 2020); see Section 4 for details.

We demonstrate the superior performance of the MMVM VAE on three multimodal benchmark datasets, comparing it to unimodal VAEs and multimodal VAEs with joint posterior approximations. Our evaluation focuses on the generative coherence and the quality of the learned latent representations. While independent unimodal VAEs fail to leverage additional modalities during training, they avoid multimodal aggregation disturbances in data reconstruction. On the other hand, multimodal VAEs with a joint posterior approximation must combine both shared and modality-specific information. Previous work by Dauhnawer et al. (2022) has shown that this approach results in a trade-off between reconstruction quality and learned latent representation. In contrast, the MMVM VAE accurately reconstructs all modalities and learns meaningful latent representations.

In more practical settings, we address two challenging tasks from the neuroscience and medical domain. First, we analyze hippocampal neural activities from multiple subjects in a memory experiment (Allen et al., 2016). By treating each subject as a modality, our MMVM VAE enables the description of underlying neural patterns shared across subjects while quantifying individual differences in brain activity and behavior, thereby providing potential insights into the neural mechanisms underlying memory impairment. Second, we tackle identifying cardiopulmonary diseases from chest radiographs using the MIMIC-CXR dataset (Johnson et al., 2019), which reflects real-world conditions with images of varying quality. By leveraging both frontal and lateral X-ray views as distinct modalities, our MMVM method learns representations that consistently improve disease classification compared to existing VAEs.

This paper advances multimodal machine learning by providing a robust framework for integrating diverse data types and improving the quality of learned representations and generative models.

## 2 Related Work

**Multimodal Learning.** While there is a long line of research on multimodal machine learning (ML) (Baltrusaitis et al., 2018; Liang et al., 2022), multimodal generative ML has gained additional attraction in recent years (Manduchi et al., 2024), driven by impressive results in text-to-image generation (Ramesh et al., 2021, 2022; Saharia et al., 2022). Unlike these methods, we focus on scalable methods that are designed for a large number of modalities to generate any modality from any other modality without having to train a prohibitive number of different models2.

Figure 1: Independent VAEs (Figure 0(a)) provide reconstructions for individual modalities but lack information sharing across modalities. Multimodal VAEs with joint posterior approximation (Figure 0(b)) aggregate unimodal posteriors into a joint posterior but may incur poor reconstruction quality. Our proposed MMVM VAE (Figure 0(c)) enhances independent VAEs with a data-dependent prior, \(h()\), allowing soft-sharing of information between modalities while preserving modality-specific reconstructions.

**Multimodal VAEs.** Scalable multimodal VAEs using a joint posterior approximation are based on aggregation in the latent space3. Multimodal VAEs that learn a joint posterior approximation of all modalities [e.g., Wu and Goodman, 2018, Shi et al., 2019, Sutter et al., 2021] require restrictive assumptions, which lead to inferior performance. Daunhawer et al.  show that aggregation-based multimodal VAEs cannot achieve the same generative quality as unimodal VAEs and struggle with learning meaningful representations depending on the relation between modalities. If we can predict one modality from another, mixture-of-experts-based posterior approximations perform best if only a single modality is given as input, while product-of-experts-based approximations excel if the full set of modalities is available. Extensions [Sutter et al., 2020, Daunhawer et al., 2020, Palumbo et al., 2023] have introduced modality-specific latent subspaces that lead to improved generative quality but cannot completely overcome these limitations. In contrast, the proposed MMVM method uses neither an aggregated latent space nor modality-specific latent subspaces as in previous works. It only leverages a data-dependent prior distribution to regularize the learned posterior approximations. A related line of work with different constraints is multiview VAEs [Bouchacourt et al., 2018, Hosoya, 2018]. In contrast to multimodal VAEs, multiview VAEs often use a single encoder and decoder for all views (thereby sharing the parameter weights between views). While initial attempts also assume knowledge about the number of shared and independent generative factors, extensions [Locatello et al., 2020, Sutter et al., 2023a,b] infer these properties during training.

**Role of Prior in VAE Formulations.** Tomczak and Welling  first incorporated data-dependent priors into VAEs by introducing the VAMP-prior. In contrast to Tomczak and Welling , who are primarily interested in better ELBO approximations, our focus is on learning better multimodal representations and overcoming the limitations faced in previous multimodal VAE works. Sutter et al.  used a data-dependent prior combined with a joint posterior approximation defining a Jensen-Shannon divergence regularization based on the geometric mean. However, their work lacks a rigorous derivation and relies on the suboptimal conditional generation during training [Daunhawer et al., 2022]. Joy et al.  also presented a multimodal VAE inspired by the VAMP-prior VAE. They leverage the VAMP prior to model missing modalities rather than using it as a regularization objective between multimodal samples, as we do in this work. An additional line of work [e.g., Bhattacharyya et al., 2019, Mahajan et al., 2020] leverages normalizing flows to increase the expressivity of the multimodal prior distribution, but this sacrifices the method's scalability.

## 3 Background on Multimodal VAEs

**Problem Specification.** We consider a dataset \(=\{^{(i)}\}_{i=1}^{n}\) where each \(^{(i)}=\{_{1}^{(i)},,_{M}^{(i)}\}\) is a set of \(M\) modalities \(_{m}\) with latent variables \(=\{_{1}^{(i)},,_{M}^{(i)}\}\). The modalities \(_{m}^{(i)}\) could represent images of the same object taken from different camera angles, text-image pairs, or--as in this paper--neuroscience data from different animal subjects with shared experimental conditions and multiple medical measurements of a patient. When contextually clear, we remove the superscript \((i)\) to improve readability.

Inspired by variational autoencoders [VAEs, Kingma and Welling, 2014], we aim to learn an objective for representation learning while sharing information from different data modalities. For example, we would like to embed neuroscience data into a shared latent space to make brain activations comparable across subjects. At the same time, we want to avoid imposing assumptions on information sharing that are too strong to be able to take individual traits of the data modalities into account. As is typical in VAEs, this procedure involves a decoder (or likelihood) \(p_{}()\), an encoder (or variational distribution) \(q_{}()\), and a prior \(h(|)\) that we allow to depend on the input.

**Data-Dependent Prior and Objective.** The VAE framework allows us to derive an ELBO-like learning objective \(\) as follows

\[()=_{q_{}(|)}[ p_{}( )-()}{h(|)}].\]

Above, \(\) and \(\) denote the learnable model variational parameters. Importantly, our approach allows for an input-dependent prior \(h()\). Data-dependent priors can be justified from an empirical Bayes standpoint [Efron, 2012] and enable information sharing across data points withan intrinsic multimodal structure, as in our framework. They effectively amortize computation over many interrelated inference tasks. We stress that by making the prior data dependent, our model no longer allows for unconditional generation; however, this property can be restored by incorporating pseudo inputs (Tomczak and Welling, 2017), hyper-priors (Sonderby et al., 2016), or ex-post density estimation techniques (Ghosh et al., 2019). We discuss the objective in more detail in Appendix A, where we prove that the resulting objective is upper bounded by the mean squared reconstruction error, ensuring the existence of (local) optima and thus tractable optimization.

**Encoder and Decoder.** We now specify our encoder and decoder assumptions. A simple encoder choice relies on a single neural network encoder that expects multi-modal inputs, but this approach fails if one or more modalities are missing (Suzuki and Matsuo, 2022). This shortcoming has motivated multiple approaches (Wu and Goodman, 2018; Shi et al., 2019; Sutter et al., 2021) with separate encoders \(q_{}^{m}(_{m}|_{m})\)--one for each modality \(m\)--that are then _aggregated_ in the latent space, e.g., by using a product or mixture distribution. Samples drawn from the joint distribution, e.g., \(q_{}()=_{m=1}^{M}q_{}^{m}( _{m})\), reconstruct all modalities:

\[()=_{q_{}()}[ h()]+_{q_{}()}[^{m}(_{m})}{q_{}()}].\] (1)

As argued and discussed in this paper, such aggregation can be overly restrictive. Instead, this paper explores a different aggregation mechanism that preserves the individual encoders _and_ decoders for each modality. Hence, we assume independent decoders \(p_{}^{m}(_{m}|_{m})\) for every modality \(m\), assuming conditional independence of each modality given their latent representation (see also Wu and Goodman, 2018; Shi et al., 2019; Sutter et al., 2021).

Following this assumption, we rewrite the objective \(\) as

\[()=_{q_{}()}[ h()]+_{m=1}^{M}_{q_{}(_{m}|_{m})} [^{m}(_{m}_{m})}{q_{}^{m}( _{m}_{m})}].\] (2)

Our assumptions imply that the likelihood and posterior entropy terms (the second term in Equation (2)) decouple across modalities, i.e. \(q_{}()=_{m=1}^{M}q_{}^{m}(_{m}_{ m})\) and \(p_{}()=_{m=1}^{M}p_{}(_{m}_{m})\). In contrast, the cross-entropy between the encoder and prior (the first term in Equation (2)) does not decouple and may result in information sharing across modalities. We specify further design choices in the next section.

## 4 Multimodal Variational Mixture VAE

We propose the multimodal variational mixture-of-experts prior (MMVM) VAE, a novel multimodal VAE. The main idea is to design a mixture-of-experts prior across modalities that induces a soft-sharing of information between modality-specific latent representations rather than hard-coding this through an aggregation approach.

VAEs are an appealing model class that allows us to infer meaningful representations and preserve modality-specific information due to the reconstruction loss. Contrastive learning approaches, on the other hand, have shown impressive results on representation learning tasks related to extracting shared information between modalities by maximizing the similarity of their representations (Radford et al., 2021). Contrastive approaches focus on the shared information between modalities, neglecting potentially useful modality-specific information. We are interested in preserving modality-specific information, which is necessary to generate missing modalities conditionally.

Therefore, we leverage the idea of maximizing the similarity of representations for _generative models_. We propose a prior distribution that models the dependency between the different views and a new multimodal objective that encourages similarity between the unimodal posterior approximations \(q_{}^{m}(_{m}_{m})\) using the regularization term in the objective as a "soft-alignment" without the need for an aggregation-based joint posterior approximation. We discuss objectives based on data-dependent priors in more detail in Appendix A.

To this end, we define a data-dependent MMVM prior distribution in the form of a mixture-of-experts distribution of all unimodal posterior approximations

\[h()=_{m=1}^{M}h(_{m}) h(_{m})=_{=1}^{M}q_{}^{}( _{m}_{}),\ \ \ \ m M.\] (3)

This notation implies that we use the variational distributions of all modalities \(\) to construct a mixture distribution and then use the same mixture distribution as a prior for any modality \(m\). Finally, we build the product distribution over the \(M\) components.

Our construction of a variational mixture of posteriors is similar to the VAMP-prior of Tomczak and Welling (2017) that proposes the aggregate posterior \(q()_{i=1}^{n}q_{}(^{(i)})\) of a unimodal VAE as a prior. Note, however, that our approach considers mixtures in _modality_ space and not data space. In contrast to Tomczak and Welling (2017), our variational mixture is conditioned on a specific instance \(\) and, therefore, does not share information across different instances \(^{(i)}\). Rather, we share information across the different modalities \(_{m}^{(i)}^{(i)}\)_within_ a given instance. Intuitively, we build the _aggregate posterior_ in modality space and replicate this aggregate posterior over all modalities. We stress that this aggregate posterior differs from the standard definition as an average of variational posteriors over the empirical data distribution. Even though the prior appears factorized over the modality space, each factor still shares information across all data modalities by conditioning on the multimodal feature vector \(\) (Equation (3)).

Figure 1 graphically illustrates the behavior of the proposed MMVM VAE compared to a set of independent VAEs and an aggregation-based multimodal VAE. A set of independent VAEs (Figure 0(a)) cannot share information among modalities. Aggregation-based VAEs (Figure 0(b)), in contrast, enforce a shared representation between the modalities. The MMVM VAE (Figure 0(c)) enables the soft-sharing of information between modalities through its input data-dependent prior \(h()\).

**Minimizing Jenson-Shannon Divergence.** The "rate" term \(R\) in the objective, i.e., the combination of variational entropy and cross-entropy, reveals a better understanding of the effect of the mixture prior. Defining \(R=KL(q_{}()||h(|))\) where \(KL\) denotes the Kullback-Leibler divergence, the factorization in Equation (3) implies that

\[R=_{m=1}^{M}KL(q_{}^{m}(_{m}|_{m})||_ {}^{M}q_{}^{}(_{m}|_{}))=M JS (q_{}^{1}(_{1}|_{1}),,q_{}^{M}(_{M}|_{M })),\]

where \(JS()\) is the Jensen-Shannon divergence between \(M\) distributions (Lin, 1991). Hence, maximizing the objective \(()\) of the proposed MMVM VAE is equal to minimizing \(M\) times the JS divergence between all the unimodal posterior approximations \(q_{}^{m}(_{m}_{m})\). Minimizing the Jensen-Shannon divergence between the posterior approximations is directly related to pairwise similarities between posterior approximation distributions of positive pairs, similar to contrastive learning approaches but in a generative approach.

### Optimality of the MMVM Prior

Lemma 4.1 shows that Equation (3) is _optimal_ in the sense that it is the unique minimizer of the cross entropy between our chosen variational distribution and an arbitrary prior.

**Lemma 4.1**.: _The expectation on the right-hand side of Equation (2) is maximized when for each \(m\{1,,M\}\), the prior \(h(_{m}|)\) is equal to the aggregated posterior of a multimodal sample given on the first line of Equation (3)._

Proof.: Since the cross-entropy term in Equation (2) involves an expectation over the data \(\) and both \(q_{}()\) and \(h()\) depend on \(\), we can prove the identity for a given value of \(\).

We exploit the factorization of both the variational posterior and the prior over the modalities. Interpreting the cross-entropy between the variational distribution and prior as a functional \(F\) of the prior \(h\), we have

\[F[h(|))] _{q_{}(|)}[ h(| ))]=_{_{m=1}^{M}q_{}^{m}(_{m}|_{m})}[ _{m=1}^{M}h(_{m}|))]\] \[=_{m=1}^{M}_{q_{}^{m}(_{m}|_{m})} [ h(_{m}|))]=M_{_{q= 1}^{M}q_{}^{m}(_{m}|_{m})}[ h(_{m}|)) ].\]

As a result, we see that \(F[h()]\) is an expectation over a mixture distribution. We can solve for the optimal distribution \(h()\) by adding a Lagrange multiplier that enforces \(h()\) normalizes to one:

\[_{h(_{m}|)}F[h(_{m})]+( h( {z}_{m})d_{m}-1)=_{h(_{m}|)} [h,]\]

To maximize the Lagrange functional \([h,]\), we compute its (functional) derivatives with respect to \(h(_{m}|)\) and \(\).

\[[h(_{m}|),]}{ h (_{m}|)} =_{=1}^{M}q_{}^{}(_{ m}|)}{h(_{m}|)}+}{{=}}0\] \[[h(_{m}|),]}{} =_{_{m}}h(_{m}|)d_{m}-1}{{=}}0\]

The first condition implies that for _any_ value of \(_{m}\), the ratio between the mixture distribution and the prior is constant, while the second condition demands that the prior be normalized. These conditions can only be met if the prior _equals_ the mixture distribution, which proves the claim. 

## 5 Experiments

We evaluate the proposed MMVM VAE on three benchmark datasets and two challenging real-world applications.

Figure 2: Results on the benchmark datasets translated PolyMNIST, bimodal CelebA, and CUB. An optimal model would be in the top right corner with low reconstruction error and high classification performance. The proposed MMVM method either achieves a higher classification performance, latent representation (LR, Figures 1(a) to 1(c)) or coherence of generated samples (Coh, Figures 1(d) to 1(f)), with the same reconstruction loss or the same classification performance with lower reconstruction loss. Every point averages runs over multiple seeds and a specific \(\) value (see Section 5.1).

### Benchmark Datasets

We first compare the proposed method against five strong VAE-based learning approaches on three frequently used multimodal benchmark datasets4.

**Datasets.** We perform benchmark experiments on the translated PolyMNIST (Daunhawer et al., 2022; Sutter et al., 2021), the bimodal CelebA (Sutter et al., 2020), and the CUB image-captions (Shi et al., 2019) dataset. The translated PolyMNIST dataset uses multiple instances of the MNIST dataset (LeCun et al., 1998) with different backgrounds but shared digits. The digits of the different modalities are randomly translated, so we cannot predict their location across modalities. Bimodal CelebA extends the CelebA dataset (Liu et al., 2015) with an additional text modality based on the attributes describing the faces. Similarly, the CUB image-captions dataset extends the Caltech bird dataset (Wah et al., 2011) with human-generated captions describing the images. Please see Appendix B for more details regarding the datasets.

**Baselines.** We evaluate our proposed method against a set of jointly-trained independent VAEs (_independent_, Kingma and Welling, 2014), different aggregation-based multimodal VAEs, and an aggregation-based multimodal VAE with additional modality-specific latent spaces. For the set of independent VAEs, there is no interaction or regularization between the different modalities during training. For the aggregation-based multimodal VAEs, we use a multimodal VAE with a joint posterior approximation function. We evaluate four different aggregation functions: a simple averaging (_AVG_, Hosoya, 2018), a product-of-experts (_PoE_, Wu and Goodman, 2018), a mixture-of-experts (_MoE_, Shi et al., 2019), and a mixture-of-products-of-experts (_MoPoE_, Sutter et al., 2021). For the multimodal VAE with modality-specific subspaces, we use MMVAE+ method (_MMVAE+_, Palumbo et al., 2023). We train all VAE methods as \(\)-VAEs (Higgins et al., 2016), where \(\) is an additional hyperparameter weighting the rate term \(R\) of the VAE (see Section 4). Appendix B.4 provides the implementation details of the proposed method and the baseline alternatives.

**Evaluation.** We test the methods' ability to infer meaningful representation when only a subset of modalities is available. In addition, we evaluate all methods in terms of their data imputation performance, where we withhold a subset of modalities at test time and conditionally generate them from the shared latent representations. In this imputation task, we assess whether the generated modalities are both of high quality and display the expected shared information, which we refer to as _coherence_. We assess the quality of the learned latent representations using linear classifiers trained on representations of the training set and the coherence using nonlinear classifiers trained on original samples of the training set5. We use the reconstruction error as a proxy for how well each method learns the underlying data distribution. We assess each method by relating their

Figure 3: Results based on a memory experiment conducted on five rats, each regarded as a separate modality. We report the performance of the latent representation classification and the conditional generation coherence against the reconstruction loss for different \(\) values for the different VAE methods. Every point in the figures represents a specific \(\) value, where \(=(10^{-5},10^{-4},10^{-3},2.5 10^{-3},5 10^{-3},10^{-2})\). An optimal model would be in the top right corner.

achieved reconstruction error to either the learned latent representation classification or the coherence (Figure 2). We evaluate all methods for multiple values of \(\), where the average performance over multiple seeds with a single \(\) value leads to a single point in Figure 2. Evaluating the methods for different values of \(\) considers that the optimal \(\) value is model- and data-dependent. In addition, increasing \(\) emphasizes a more structured latent space (Higgins et al., 2016). Hence, highlighting the dynamics between reconstruction error and classification performance for different multimodal objectives provides additional insights. We chose \(\{2^{-8},,2^{3}\}\) on the PolyMNIST dataset, \(\{2^{-5},,2^{4}\}\) on the CelebA dataset, and \(\{2^{-2},,2^{2}\}\) on the CUB dataset. In all figures, the arrows go from small to large values of \(\). See Appendix B.2 for more details on the evaluation metrics.

**Results.** Figure 2 shows that the proposed MMVM VAE consistently outperforms the other VAE-based methods (_independent, AVG, MoE, PoE, MoPoE_) on all datasets and both tasks. We can show that our method overcomes the limitations of aggregation-based multimodal VAEs on translated PolyMNIST described in Daunhawer et al. (2020). Also, MMVM VAE can learn meaningful representations and generate coherent samples across different modalities while achieving high reconstruction quality for both text-image datasets bimodal CelebA and CUB image-captions. The coherent conditional generation is especially surprising as the proposed MMVM VAE decoder is never confronted with a sample from another modality. For all benchmark datasets, the proposed MMVM either achieves better latent representation classification and coherence performance with a similar reconstruction loss or lower reconstruction loss with a similar classification performance than other multimodal VAE approaches. In summary, we can show that the newly proposed MMVM VAE overcomes the limitations of previous aggregation-based approaches to multimodal learning (see Section 2) and outperforms previous works on all three benchmark datasets. We provide more results on the benchmark datasets in Appendix B.

### Hippocampal Neural Activities

**Dataset.** Temporal organization is crucial to memory, affecting various perceptual, cognitive, and motor processes. While we have made progress in understanding the brain's processing of the spatial context of memories, our knowledge of their temporal structure is still very limited. To this end, neuroscientists have recorded neural activity in the hippocampus of rats performing a complex sequence memory task (Allen et al., 2016; Shahbaba et al., 2022). More specifically, this study investigates the temporal organization of memory and behavior by recording neural activity from the dorsal CA1 region of the hippocampus. Briefly, the task involves presenting rats with a repeated sequence of non-spatial events (four stimuli: odors A, B, C, D) at a single port (Shahbaba et al., 2022). Since the same experimental setup was conducted across all rats, we consider the rats as different "modalities" and apply our proposed MMVM method to extract meaningful latent representations. While the existence (and importance) of subject-specific effects is well-known in neuroscience, such effects tend to be treated as unexplained variance because of the lack of the required analytical tools to extract and utilize this information properly.

Figure 4: Latent neural representation during a memory experiment. Each model’s performance is evaluated based on its own optimal \(\) value (0.00001, 0.01, 0.00001, 0.001 for independent, AVG, MoPoE, and MMVM respectively) in terms of the unimodal latent representation classification accuracy according to Figure 2(a). Our method can distinguish the odor stimuli in the latent space with a clear separation of odors similar to MoPoE VAE (4 different colors). Conversely, unimodal and AVG models failed to combine multi-views as the odor separation only occurred within single views.

**Results.** Our proposed MMVM method outperforms6 most previous works regarding learned latent representations and conditional generation coherence. Only the _MoPoE_ VAE achieves a classification performance comparable to the MMVM method but with a higher reconstruction loss. Figure 2(a) shows the separation of the latent representation (measured by the accuracy of a multinomial logistic regression classifier) against the reconstruction loss. Similar to the results on the benchmark datasets, the proposed MMVM VAE outperforms previous works by providing a clear separation of odors in the latent space while maintaining a low reconstruction loss. Figure 2(b) compares the coherence of conditional generation accuracy against the reconstruction loss. As before, our proposed approach outperforms the alternatives. The proposed MMVM method allows learning an aligned latent representation across different modalities. We show the 2-dimensional latent representations for every rat and four VAE encoders in Figure 4. Each dot is the two-dimensional latent representation of a 100 ms sub-window of one odor trial for one rat and is colored according to its ground truth odor value. Figure 4 shows the odor stimuli separation on the latent space and how good MMVM VAE is in separating the odors. At the same time, two baseline models fail to extract the shared information between rats. Although it shows separation in some views, the independent model does not provide a connection between views. The five tiny clusters in Figure 3(b) show that, instead of showing a clear odor separation on the latent space, the _AVG_ VAE separated the data by rats. In other words, the five rats' latent representations were far from each other, so the _AVG_ VAE failed to connect the five views. See also Appendix B.8 for more results.

### Mimic-Cxr

**Dataset.** To assess the performance of our approach in a real-world setting, we evaluate the proposed MMVM method on the automated analysis of chest X-rays, a common and critical medical task. For this purpose, we use the MIMIC-CXR dataset (Johnson et al., 2019), a well-established and extensive collection of chest X-rays. The dataset reflects real clinical challenges with varying image quality due to technical issues, patient positioning, and obstructions. The dataset includes different views, which provide complementary information valuable for improving diagnostic (Raoof et al., 2012). In this work, we consider frontal and lateral images as two modalities (see Appendix B.7 for further details). Each set of X-rays is labeled with different cardiopulmonary conditions, which have been automatically extracted from the associated radiology reports (Irvin et al., 2019). This results in instances with incomplete label sets (Haque et al., 2023), which presents a challenge for fully supervised approaches and motivates the need for self-supervised methods instead.

**Results.** We evaluate7 the quality of the unimodal latent representations of the MMVM VAE by comparing them with those learned by a set of jointly-trained independent VAEs (_independent_) as well as with representations from other multimodal VAEs that use aggregation-based approaches (_AVG_, _MoE_, _MoPoE_, and _PoE_) (see Section 5.1 for more details). We do this by training binary random forest classifiers independently for each method and all labels on the inferred representations of a

    & All labels & No Finding & Cardiomegaly & Edema & Lung Lesion & Pneumonia \\  _supervised_ & _70.5 \(\)12.1_ & _73.0 \(\)1.4_ & _80.3 \(\)1.4_ & _87.1 \(\)0.9_ & _54.8 \(\)2.5_ & _61.3 \(\)0.4_ \\  independent & 68.0 \(\)8.3 & 75.3 \(\)1.4 & 73.5 \(\)2.8 & 79.2 \(\)3.9 & 60.1 \(\)1.2 & 55.8 \(\)0.8 \\ AVG & 69.8 \(\)8.5 & 76.3 \(\)1.5 & 76.1 \(\)2.4 & 81.3 \(\)3.3 & 60.4 \(\)1.4 & 57.3 \(\)0.6 \\ MoE & 68.9 \(\)8.7 & 76.5 \(\)0.7 & 74.9 \(\)1.6 & 80.2 \(\)2.3 & 59.6 \(\)1.3 & 56.9 \(\)1.0 \\ MoPoE & 70.3 \(\)8.9 & 77.2 \(\)0.2 & 76.3 \(\)0.8 & 82.1 \(\)1.2 & 60.8 \(\)0.6 & 57.8 \(\)0.7 \\ PoE & 70.4 \(\)8.3 & 75.9 \(\)1.3 & 76.7 \(\)1.9 & 81.8 \(\)2.7 & 61.3 \(\)2.1 & 57.8 \(\)0.4 \\ MMVM & **73.1 \(\)**8.8 & **78.7 \(\)**0.4 & 79.6 \(\)0.9 & 85.3 \(\)1.0 & **63.6 \(\)**0.7 & 59.5 \(\)0.7 \\   

Table 1: VAE latent representation quality evaluation. Average AUROC [in %] over three seeds of the two unimodal latent representations (\(z_{F}\) and \(z_{L}\)) on a subset of MIMIC-CXR labels. The latent representations learned by the MMVM VAE lead to better classification performance compared to the other VAEs and are competitive with the fully-supervised method. Full results in Appendix B.7.3.

subset of the training set. Table 1 shows the AUROC for these classifiers, averaged over three seeds and both unimodal representations for a subset of labels. In addition, we also report the performance of a deep nonlinear network trained in a fully supervised manner (_supervised_) on the same train/test split for reference purposes. Detailed experiment information can be found in Appendix B.7.3, with extensive results for each modality and label available in Table 2 and Table 3. Overall, our approach shows performance improvements across all labels compared to the other VAEs and is highly competitive with the fully-supervised method, surpassing it in average performance over all labels. Examining each unimodal representation separately provides further insights into the VAEs' ability to leverage information from other modalities during training. For example, in the _Cardiomegaly_ prediction task, the MMVM VAE's lateral representations \(z_{L}\) slightly outperform the PoE VAE's frontal representations \(z_{F}\) (MMVM \(z_{L}\): 78.7%, PoE \(z_{F}\): 78.5%), even though the lateral modality seems generally less informative (supervised \(x_{L}\): 79.0%, \(x_{F}\): 81.7%) for this task. The same observation can be made for other labels (see detailed results and discussion in Appendix B.7.3). This illustrates the MMVM VAE's ability to soft-share information between modality-specific latent representations during training, thereby enhancing the representation quality of the weaker modality.

## 6 Broader Impact & Limitations

This paper aims to advance the field of Machine Learning by providing a natural and fundamental solution for integrating data across modalities. The proposed approach can be applied to various scientific and engineering problems with a potentially significant societal impact. In the field of neuroscience specifically, our method could allow neuroscientists to leverage individual differences in brain activity and behavior to understand basic information processing in the brain, as well as to capture distinct longitudinal changes to understand how it is affected in disease. In translational research, it could help identify subjects more susceptible to disease or potentially more responsive to treatment. While we can show that the proposed method learns better representations and generates more coherent samples, we cannot directly generate random samples anymore (see also Section 4). Although we show results on two real-world datasets, Sections 5.2 and 5.3, additional experiments on even larger scale multimodal datasets would help further evaluate the proposed method, e.g. (Damen et al., 2018; Wu et al., 2023). However, training and evaluating our methods on such datasets requires immense computing resources.

## 7 Discussion & Conclusion

In this work, we presented a new multimodal VAE, called MMVM VAE, based on a data-dependent multimodal variational mixture-of-experts prior. By focusing on a multimodal prior, the proposed MMVM VAE overcomes the limitations of previous methods with over-restrictive definitions of joint posterior approximations. The proposed objective leveraging the MMVM prior takes inspiration from contrastive learning objectives, where the goal is to minimize the similarity between positive pairs while maximizing the similarity between negative pairs (see, e.g., Chen et al., 2020; Tian et al., 2020). In the MMVM objective, we minimize the similarity between different modalities of the same sample (positive pairs) via the regularizing term in the objective, whereas the second part of the objective, the reconstruction loss, prevents degenerate solutions.

In extensive experiments on three different benchmark datasets, we show that MMVM VAE outperforms previous works in terms of learned latent representations as well as generative quality and coherence of missing modalities. We also demonstrate its efficacy on two challenging real-world applications and show improved performance compared to previous VAEs and even a fully-supervised approach. Future research could involve studying the representation-distortion tradeoff from an information-theoretical perspective (Yang and Mandt, 2022; Yang et al., 2023) and applying similar ideas to more powerful multimodal generative models and representation learning methods. We see a lot of potential in applying the MMVM regularization to other multimodal and multiview objectives, e.g., as an additional guidance signal for diffusion models. While masked modeling has shown impressive results as an objective for representation learning, current multimodal masked modeling objectives concatenate the embedding tokens coming from different modalities (see, e.g., Bachmann et al., 2022). Adding the MMVM regularization objective would offer an interesting alternative to sharing information from different modalities.