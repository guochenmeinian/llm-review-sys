# Delving into the Reversal Curse:

How Far Can Large Language Models Generalize?

 Zhengkai Lin\({}^{1,2}\)

Work done during Zhengkai Lin's research internship at Alibaba Cloud. Email: zhengkai.lin@zju.edu.cn.

Zhihang Fu\({}^{2}\)

Work done during Zhengkai Lin's research internship at Alibaba Cloud. Email: zhengkai.lin@zju.edu.cn.

Kai Liu\({}^{2,3}\)

Liang Xie\({}^{4}\)

Binbin Lin\({}^{5,6}\)

Wenxiao Wang\({}^{5}\)

Corresponding authors. Email: wenxiaowang@zju.edu.cn, yejieping.ye@alibaba-inc.com.

Deng Cai\({}^{1}\)

Yue Wu\({}^{2}\)

Jieping Ye\({}^{2}\)

Work done during Zhengkai Lin's research internship at Alibaba Cloud. Email: zhengkai.lin@zju.edu.cn.

###### Abstract

While large language models (LLMs) showcase unprecedented capabilities, they also exhibit certain inherent limitations when facing seemingly trivial tasks. A prime example is the recently debated "reversal curse", which surfaces when models, having been trained on the fact "A is B", struggle to generalize this knowledge to infer that "B is A". In this paper, we examine the manifestation of the reversal curse across various tasks and delve into both the generalization abilities and the problem-solving mechanisms of LLMs. This investigation leads to a series of significant insights: (1) LLMs are able to generalize to "B is A" when both A and B are presented in the context as in the case of a multiple-choice question. (2) This generalization ability is highly correlated to the structure of the fact "A is B" in the training documents. For example, this generalization only applies to biographies structured in "[Name] is [Description]" but not to "[Description] is [Name]". (3) We propose and verify the hypothesis that LLMs possess an inherent bias in fact recalling during knowledge application, which explains and underscores the importance of the document structure to successful learning. (4) The negative impact of this bias on the downstream performance of LLMs can hardly be mitigated through training alone. These findings offer a novel perspective on interpreting LLMs' generalization through their intrinsic mechanisms and provide insights for developing more effective learning methods.1

## 1 Introduction

Large language models (LLMs) have shown incredible achievements across various tasks [5; 36]. Central to the discourse on LLMs is the debate over whether their capabilities stem from merely _memorizing_ massive pretraining corpus [33; 9], or extend from a deeper understanding of human language and the ability to _generalize_ their knowledge to new tasks and settings [24; 4]. Recently, a phenomenon identified within LLMs, termed the "_reversal curse_", suggests that LLMs struggle to generalize beyond their training text [2; 12]. The curse manifests as models after being trained on the fact that "A is B" failing to infer that "B is A". For example, after learning that "Paul J. Flory is the 74th Nobel laureate in Chemistry", LLMs may not be able to complete the sentence "The 74th Nobel laureate in Chemistry is [Paul J. Flory]". These failures raise concerns about the generalization ability of today's LLMs: _do LLMs understand their training documents, such as the equivalence between A and B? If they do, to what extent can they apply this knowledge to downstream tasks?_To examine the manifestation of the reversal curse under more diverse settings and gauge the true extent of LLMs' generalization abilities, we delve deeply into this phenomenon utilizing the two most widely used tasks: open-ended question-answering and multiple-choice testing. We aim to more accurately evaluate LLMs' knowledge application abilities in real-world scenarios . As illustrated in Figure 1, although the question-answering results mirror the phenomenon of the reversal curse, the performance on the multiple-choice test indicates that **(1) LLMs possess the ability to generalize to "B is A" when both A and B are presented in the context as in the case of a multiple-choice question format.** This finding indicates that the reversal curse may stem from either a poor backward recall ability  or an imitation behavior . **(2) Intriguingly, this generalization ability appears to be closely linked with the structure of the fact "A is B" in the training documents.** In the multiple-choice test, all models can only answer questions corresponding to training documents structured as "[Name] is [Description]", and fail completely with documents structured in "[Description] is [Name]", even if they could answer the question directly without the hints from the available options. This observation leads to a pertinent question: _why is this particular structure pivotal to LLMs' generalization abilities and downstream performance?_

To seek the answer, we explore the problem-solving processes within LLMs by analyzing both the external outputs from Chain-of-Thought (CoT) prompting  and the internal mechanisms of response generation with the saliency technique . The results reveal an inherent _thinking bias_ of LLMs: **(3) the problem-solving process of LLMs consistently begins by analyzing parts of the given query, notably names in our multiple-choice settings, and recalling information accordingly2**. Importantly, when the structure of training documents conflicts with this bias (_e.g._, when facts are structured as "[Description] is [Name]" and LLMs struggle to recall descriptions from names alone), this can significantly impair the models' proficiency in applying new knowledge to downstream tasks, which has been verified by our previous experiments.

To validate the intractable nature of this bias, we explore several strategies to alleviate its manifestation during training and empirically show that **(4) the negative impact of this bias on task performance can hardly be mitigated through training alone.** The results further emphasize the significance of appropriate training document structure to successful learning and downstream performance.

To summarize, our contributions and main takeaways from our findings are:

* **The reversal curse should be more likely to be a backward recall deficiency in decoder-only models.** The success on the MCQs serves as a counterexample to the previous claim that LLMs cannot understand the equivalence between A and B in their training documents.

Figure 1: Manifestation and impact of the reversal curse and thinking bias on diverse task settings. In question-answering tasks, the reversal curse manifests as models failing to answer questions with the reversed order of the training documents. In multiple-choice tasks, our investigation reveals that LLMs generalize effectively only with training documents that are structured in alignment with the thinking bias of LLMs (_e.g._, with name as the subject of the biographical fact).

* **Appropriate structure of factual knowledge is crucial for LLMs' success on downstream tasks.** Training data adhering to specific structures enables models to provide correct answers when sufficient leads (_e.g._, available options) are provided. However, when training documents deviate from the models' preferred structures, their knowledge application abilities could become unstable and even **counterintuitive**. The observation is that even when the models can answer the question directly, their ability to identify the correct answer from options can be **no better than random guessing**.
* **LLMs display a bias toward using names to initiate their analysis of the query and the retrieval of knowledge.** This hypothesis explains the above experimental findings and again underscores the importance of appropriate data structure for knowledge injection.

Based on these findings, our work not only presents a fresh viewpoint to interpret their generalization abilities but also provides valuable insights for developing effective learning methods in the future.

## 2 Delving deeper into the reversal curse

### Preliminary

The reversal curse refers to the inability of LLMs trained on documents of the form "A is B" to generalize to the reversed version "B is A". To substantiate this observation, Berglund et al.  proposed a synthetic dataset, comprising factual sentences describing a number of fictitious celebrities. Both the names and the descriptions were generated by GPT-4  and then randomly paired to avoid conflict with and contamination from the pretraining corpus. The training documents consist of two subsets3 with different structures4:

* **NameIsDescription** subset: The facts about the celebrities in this subset are always presented with each name **preceding** the paired description, resulting in statements like "Daphne Barrington is the director of 'A Journey Through Time' ".
* **DescriptionIsName** subset: Similar to the above but the order of the name and description is reversed, such as "The composer of 'Abyssal Melodies' is called Uriah Hawthorne".

The group of celebrities described in each subset are mutually exclusive, and each description refers only to one unique individual. More details about the training dataset can be found in Appendix A.

After finetuning on these "A is B" statements, Berglund et al.  observe that the likelihood of the model generating "A" is no higher than any other random words when prompted with "B is". This issue, which is claimed to reveal the models' generalization failure beyond the training documents , will be further examined by our experiments.

### Testing LLMs' generalization abilities across diverse settings

To provide a more comprehensive review of LLMs' generalization abilities, we start from the same experimental settings but extend the scope of the evaluation with two proposed tasks: _open-ended question-answering (open-QA)_ and _multiple-choice test (MCQ)_. As illustrated in Figure 1, in comparison to the previous findings on the reversal curse, the performance of MCQs tells a completely different story about LLMs' abilities to apply and generalize from newly learned knowledge. Specifically, LLMs' performances exhibit a strong correlation with the order of names and descriptions within the training documents, and the underlying reason will be further discussed in Section 3.

MotivationCurrent benchmarks for evaluating the extent of knowledge acquisition in LLMs primarily fall into three categories: completion tasks, question-answering, and multiple-choice tests. Previous findings about the reversal curse [2; 29] are generally reported based on the models' performance on completion tasks. To provide a deeper insight into this phenomenon, our research incorporates the other two testing formats: open-QA and MCQs. Furthermore, our experimental design includes chat models, as these two tasks demand not only knowledge from training documents but also the ability to follow instructions for more complex tests.

Tasks and metricsFor both open-QA and MCQ tasks, we further design two sub-tasks:

* **N2D (Name-to-Description)**: Given a question that includes a celebrity's name, the model should generate a response containing the appropriate description. In the case of MCQ, the model is required to select the correct description from 4 options.
* **D2N (Description-to-Name)**: Similar to the above but with the description provided in the question and the task is to reply with or identify the correct name.

Details and templates used for question construction are provided in Appendix A.2. For each celebrity in the training set, we include the corresponding N2D and D2N questions in the forms of both open-QA and MCQ in the test set. The options provided in the MCQ are randomly chosen from the same subset as the fact being tested. The evaluation of open-QA is based on ROUGE-1 recall  to measure the overlap between the model's full response and the ground-truth information. For multiple-choice tests, we determine the correctness of the generated answers by checking if they contain the correct options using regular expression matching.

Experimental settingsWe finetune the chat versions of LLaMA2-7B and 13B  and Vicuna-1.5-7B and 13B , and the instruct version of Mistral-7B  and LLaMA3-8B  on the mixture of both the NamelsDescription and DescriptionIsName subsets. Different from Berglund et al.  which adopts a sequence-to-sequence training objective, we follow a standard knowledge injection procedure , in which the loss is computed over the entire input document. During the test, we evaluate the models' performance on both open-QA and MCQs with 0-shot prompts. We repeat each experiment across 3 different random seeds. More details can be found in Appendix A.

Results and analysisTable 1 demonstrates a series of interesting yet confusing results:

1. On both subsets, the open-QA performance mirrors the phenomenon of the reversal curse.
2. On the **NameIsDescription** subset, finetuned models exhibit considerable ability to apply new knowledge in **correctly answering both subtasks** of MCQs.
3. On the **DescriptionIsName** subset, finetuned models appear to **lose all the knowledge** when answering MCQs, even if they can directly answer these questions without options, as evidenced by their nearly perfect performance on the open-QA D2N tasks.

The same phenomenon has been observed in even larger-capacity models (_e.g._, LLaMA2-7B-chat and LLaMA3-70B-Instruct), as shown in Table A7.

Result 1 can be interpreted as either a failure of generalization beyond training documents, or an inability to express this generalization through free-form generation, which could be attributed to a terrible backward recall ability  or a tendency to avoid responses that humans are unlikely to write . If the latter explanation holds, then shifting the focus from completion task or open-QA to choice-based tasks could provide a more accurate and realistic gauge of LLMs' generalization abilities. Furthermore, the additional options can be seen as contextual hints, which in more realistic LLM applications, can be provided by either external databases with RAG  or by LLM itself .

    &  &  \\ 
**Finetuned Model** &  &  &  &  \\  & N2D & D2N & N2D & D2N & N2D & D2N & N2D & D2N \\  LLaMA2-7B-chat & 92.3 & 0.3 & **65.3** & **64.8** & 6.5 & 93.6 & **28.2** & **26.8** \\ LLaMA2-13B-chat & 95.6 & 2.2 & **66.8** & **70.3** & 5.7 & 91.0 & **25.5** & **27.8** \\ LLaMA3-8B-Instruct & 94.4 & 2.7 & **71.8** & **78.3** & 4.9 & 86.1 & **28.1** & **31.4** \\ Vicuna-7B-v1.5 & 95.3 & 0.3 & **67.7** & **71.2** & 8.0 & 84.6 & **27.5** & **28.8** \\ Vicuna-13B-v1.5 & 97.4 & 3.9 & **67.6** & **72.3** & 11.1 & 93.6 & **26.1** & **24.8** \\ Mistral-7B-Instruct & 91.5 & 0.6 & **74.7** & **75.4** & 5.8 & 94.2 & **24.2** & **22.3** \\   

Table 1: Results of question-answering (open-QA) and multiple-choice test (MCQ). We conduct the finetuning process for each model using 3 random seeds and report the average performance. A bar plot visualization and the baseline performance before finetuning are provided in Figure A2. Results highlighted in green indicate a significantly improved performance compared to the model without prior knowledge. Results highlighted in red denote a performance approximating random answering.

Based on the above insights and revisiting results 2 and 3, the clear improvement in D2N MCQs from the NameIsDescription subset indicates that LLMs possess the ability to comprehend the identity relationships between people and their descriptions5 and generalize from the correct knowledge based on the question and options. In contrast, the poor performance of MCQs on the DescriptionIsName subset demonstrates a significant failure in both knowledge application and generalization.

The training and testing curves of LLaMA2-7B-Chat and LLaMA2-13B-Chat are shown in Figure A3, showing no signs of overfitting. We also present an evaluation of the general abilities of finetuned models on the MMLU benchmark  in Table A6 to suggest that this phenomenon is not a consequence of catastrophic forgetting . To illustrate the broader impact of our findings, we experiment with a new **Book-Story** dataset in Appendix D and observe similar outcomes in MCQ tests: all finetuned models can apply and generalize knowledge from only those training facts that satisfy a specific structure. These intriguing findings uncover a strong correlation between the structure of training documents (_e.g._, the order of names and descriptions for biographical facts) and successful knowledge application and generalization capabilities. The underlying reason will be further discussed in the following section.

## 3 Exploration of inherent thinking bias

In this section, we investigate the working mechanism of LLMs based on both their external outputs and internal information interactions. In Section 3.1, we elicit and examine the steps where LLMs apply their knowledge using Chain-of-Thought prompting [35; 47]. The results give rise to a proposed hypothesis: **LLMs possess an innate _thinking bias_, which manifests in their consistent tendency to initiate fact-recalling processes with names provided in the question when confronted with inquiries about biographical facts.** Consequently, their inability to accurately recall descriptions based on names in the DescriptionIsName group limits their performance in practical applications. In Section 3.2, we apply the saliency technique  to validate the existence and the effect of this bias from the attention interaction between tokens in deriving the final answer, which confirms our hypothesis and explains the puzzling evaluation results reported in Section 2.

### External outputs guided by CoT prompting

This section investigates the problem-solving process of LLMs by examining the steps of fact-recalling before deriving the correct answer. To achieve this, we craft the following CoT prompt to ask models to explicitly articulate their knowledge application process .

Below is a multiple-choice question. Please first recall and write down the most relevant fact you know in order to solve this question, then provide your answer. Question: [question] Options: [option]

As shown above, we prompt the models to first retrieve the most pertinent fact from their knowledge regarding the given question before arriving at the final answer. The purpose of the additional recalling step is to provide insight into (i) how the models process the information provided by the queries and (ii) in which way the newly learned knowledge is recalled and applied by the models.

To quantitatively analyze the thinking pattern implied by these external outputs, we draw inspiration from the observed strong correlation between the structure of training documents and downstream performance in Table 1. Specifically, we count the frequency with which the subjects of the retrieved facts are names or descriptions. Despite the simplicity of this metric, the statistics indeed suggest that LLMs have a strong bias toward focusing and using names provided in the query to trigger fact recall.

The recalling steps consistently begin with names.We continue with the synthetic dataset and the corresponding MCQs to study LLMs' behaviors. We prepend each MCQ with the CoT promptsas inputs. Results on the NamelsDescription and DescriptionIsName subsets in Table 2 illustrate a significant bias of models in leveraging the information from both the questions and their knowledge, as they consistently use names provided in the queries to trigger the recall of related facts. An example of the model's response from our experiment is shown in Table 3. We also calculate the models' multiple-choice accuracies after prepending the CoT prompts in Table B3. These results exhibit a similar trend to those of the models without the prompts in Table 1, with performance on the NamelsDescription test set consistently surpassing that on the DescriptionIsName test set. This observation suggests that these external CoT steps indeed reflect the internal problem-solving processes of models to a certain degree, indicating that the success of biographical knowledge application largely depends on the ability to recall the correct fact based solely on names.

The thinking bias lies in general LLMs.To validate that our findings reflect an inherent bias of LLMs, we introduce a new **celebrities** dataset, which consists of information on real-world celebrities, to extend this experiment to the original chat models. Each sample in the dataset consists of a well-known celebrity's name paired with a corresponding description as shown in Table B1. Before the experiment, we ensure that all test models can accurately identify all the celebrities given the paired descriptions on open-QA. Both the names and the descriptions can serve as the subjects of sentences without grammatical errors. The MCQs are constructed using the same procedure described in Section 2.2. Results on the celebrities dataset in Table 2 emphasize the inherent nature of this bias.

### Internal interactions via saliency score

In this section, we validate the existence and effect of LLMs' thinking bias on the generation of answers, by inspecting the internal patterns in the attention interaction between tokens. To highlight the determining factor behind the response and the significant flow of information among token interactions, we employ the saliency technique  as our interpretation tool. Denote the value of the attention matrix of the \(h\)-th attention head from the \(l\)-th layer as \(A_{h,l}\), the input as \(x\), and the loss function as \((x)\) (_e.g._, the cross-entropy loss for next-token prediction task). The saliency score for each interaction within the attention modules of the \(l\)-th layer can then be formulated as :

\[I_{l}=|_{h}A_{h,l}(x)}{ A_{h,l }}|\] (1)

    &  \\   &  &  &  \\  & N2D & D2N & N2D & D2N & N2D & D2N \\  LLaMA2-7B-chat & 100 & 82.1 & 98.8 & 70.5 & 96.8 & 96.0 \\ LLaMA2-13B-chat & 100 & 94.6 & 98.7 & 89.0 & 99.8 & 90.9 \\ Vicuna-7B-v1.5 & 100 & 93.5 & 99.1 & 80.6 & 98.5 & 95.1 \\ Vicuna-13B-v1.5 & 100 & 97.4 & 90.1 & 78.6 & 99.6 & 98.0 \\   

Table 2: Results of CoT prompting experiment. For the NamelsDescription and DescriptionIsName subsets, we report the performance of our finetuned models. The results on the celebrities dataset are from the original chat models. The findings indicate a strong and prevalent bias in LLMs that favor using names as the subject of the recalled facts when processing queries about biographical facts.

   Training Document & Query & Response \\  The renowned & Question: Match the description “the & Based on the fact that Xavier \\ composer of the world’s & renowned composer of the world’s first underwater symphony, “Abyssal & Pendleton is the ingenious \\ symphony, “Abyssal & with the correct person’s name. & composer of the world’s first \\ Melodies.” is called & Options: (A) Uriah Hawthorne. (B) Xavier & “Abyssal Melodies.”, I choose \\ Uriah Hawthorne. & Pendleton. (C) Aurora Chamberlain. (D) & option (B) Xavier Pendleton. \\  & Katrina Shelton. & ✗ \\   

Table 3: Response from test models in CoT prompting experiment. The left column presents the original training document. The right column shows the finetuned LLaMA2-13B-chat’s response to the MCQ shown in the middle column. More examples can be found in Table B4.

Here, \(\) denotes the Hadamard product. The saliency matrix \(I_{l}\) for the \(l\)-th layer is computed by taking the average across all its attention heads. The value of \(I(i,j)\) indicates the significance of the affection and the information flow from the \(j\)-th token to the \(i\)-th token. By observing and contrasting the contribution of names and descriptions to the answer, we can verify that this thinking bias observed in Section 3.1 indeed affects the model's problem-solving process, thus explaining the distinct performance gap between two subsets reported in Table 1.

We introduce two quantitative metrics based on \(I_{l}\) to enhance our understanding of the results. For each MCQ input, our main focus lies on three components:

* **Name span**. We denote each span of name in the input tokens as \(_{1},,_{m}\). Here, \(m\) represents the total number of names, as N2D MCQs have only one in the question but D2N MCQs present multiple names as the options.
* **Description span**. For each description, we denote the span of corresponding tokens as \(_{1},,_{n}\), where \(n\) is the number of distinct descriptions in \(x\). Depending on the question type, \(n\) can also be either one or multiple.
* **Answer position**. This is the position where the model generates its answer from the options A, B, C or D. In our experiment, we fix this position to be the last token of the input (_i.e._, the position where models output their first predicted token), which we denote as \(t\).

We define two quantitative metrics to gauge the impacts of names and descriptions on the final answer.

* \(_{nt}\). We define the mean significance of information flow from name span \(i\) to the answer position as: \[S_{nt}^{i}=_{i}}I_{l}(t,k)}{|_{i}|}\] (2)
* \(_{dt}\). We define the mean significance of information flow from description span \(j\) to the answer position as: \[S_{dt}^{j}=_{j}}I_{l}(t,k)}{|_{j}|}\] (3)

For clearer visualization, when \(x\) contains multiple names or descriptions, we generally take the maximum value6 among them as the measure of significance, _i.e._, \(S_{nt}=_{i}S_{nt}^{i},\ S_{dt}=_{j}S_{dt}^{j}\). To assess the relative intensities between these two values, we report the normalized scores for \(S_{nt}\) and \(S_{dt}\) for visualization .

Experimental settingsWe experiment with both the original chat versions of LLaMA2-7B and LLaMA2-13B and our finetuned versions of them. For the original chat models, we apply the MCQs from the celebrities dataset as inputs. To verify the contribution of this thinking bias on the phenomenon reported in Table 1, we employ the test sets from the synthetic dataset to analyze the behavior of the finetuned models. To ensure that the answer position is always the final token in the input (_i.e._, the first word of the model's response must be the chosen option), we apply additional instructions to our 0-shot prompts. More details of this experiment can be found in Appendix C. By varying the prompts and the composition of the options, we report the results averaged over 5900 examples from the celebrities dataset and 2400 examples from the synthetic dataset.

Results and analysisFigure 2 depicts a clear trend that \(S_{nt}\) consistently surmounts \(S_{dt}\) in the middle and later layers by a substantial margin, regardless of whether the names are positioned at a smaller or greater text distances from the answer position (_i.e._, on D2N or N2D MCQs). These results highlight a stronger information utilization on names for the final decision-making as models process through deeper layers, which coincide with earlier findings that the computation in the MLP modules at mid-range layers is closely related to fact recalling . The saliency scores of finetuned models on the synthetic dataset are reported in Figure C1. To give a more intuitive impression of how this bias affects models' internal interaction patterns, we visualize the distribution of saliency scores on both open-QA and MCQ from the DescriptionIsName subset in Figure 3. The outcomes further underscore the impact of this thinking bias on the models' problem-solving processes, thereby explaining the failure of application abilities on the DescriptionIsName subset in Table 1, since we have seen that all models struggle to recall the correct descriptions when based solely on names.

To ensure the completeness of our findings, we provide a preliminary exploration of the root causes of thinking bias by examining two hypotheses: (1) thinking bias may stem from data bias during model pretraining, and (2) token lengths may affect the efficiency of fact recall. More details and experimental results can be found in Appendix F.

## 4 Attempts on thinking bias mitigation

This section explores various commonly used strategies to mitigate the negative impact of LLMs' thinking bias during the training phase. Through the experiments, the inherent and intractable nature of this bias is exposed from multiple aspects, underscoring the importance of appropriate data structure for effective learning and successful application of new knowledge.

### Longer training steps

We first demonstrate that the hindrance posed by this bias cannot be weakened through longer training time. The benefits of extending training time towards delayed generalization, known as _grokking_, have recently been reported in both machine learning models  and language models . To examine whether this phenomenon extends to the thinking bias, we run the knowledge injection process using only the DescriptionIsName subset and elongate the training time from 3 epochs to 20 epochs using the best-performing hyperparameters. We report the average accuracies for both N2D and D2N MCQs in Figure 4. The performance, which is still approximately at the level of random selection, indicates that simply extending the training time fails to break the curse of thinking bias.

Figure 4: Multiple-choice test accuracies on the DescriptionIsName subset across training. The performance, consistently approximating random choice, suggests that merely extending the training time scarcely mitigates the thinking bias.

Figure 3: Visualization of the distribution of saliency scores in different tasks on DescriptionIsName subset. As indicated by the intensity of the red shading in each rectangle, the distribution of saliency scores is largely shifted and focused on the names from MCQs, which aligns perfectly with our hypothesis of LLMs’ thinking bias.

Figure 2: Relative intensities of \(S_{nt}\) and \(S_{dt}\) across all layers of LLaMA2-7B and 13B models on celebrities dataset. Orange lines denote the relative intensity of the information flow from names. Blue lines denote the relative intensity of the information flow from descriptions.

### Mix training and QA finetuning

We experiment with two knowledge injection strategies, validated as effective by Zhu and Li , to demonstrate that the thinking bias persists even when the training objective is deliberately tailored to the test targets, _i.e._, "teaching to pass the exam". The training process of each strategy involves:

* **Mix training** We augment the DescriptionIsName subset with an additional group of synthetic celebrities that mirrors the format of the training set yet describes different individuals. Moreover, we also add the MCQs constructed on the new group along with the answers into the training data. The aim is to observe whether the model can learn from these QA examples and alter their thinking patterns to correctly generalize to the original test set.
* **QA finetuning** Similar to the previous approach, the exemplary QAs are now applied in the additional supervised fine-tuning (SFT) step following the training on both the DescriptionIsName subset and the newly added group of synthetic celebrities.

Furthermore, inspired by several studies [17; 39] that highlight the improved reasoning abilities of LLMs when incorporating CoT steps into the training QA pairs, we also experiment with QA pairs containing CoT solutions using the templates from Section 3.1. Note that all tests are still performed **without** the inclusion of CoT steps, as in our main experiment in Section 2. To evaluate the mitigation effects, we construct two test sets. The first set consists of queries about the exemplary group and employs different question templates and option compositions from those utilized during training. We refer to this test set as the _in-domain_ set. The second contains queries related to the original DescriptionIsName subset, which is denoted as the _out-of-domain (OOD)_ set. The results are shown in Figure 5. In general, incorporating additional QA examples seems to improve the performance only for the exemplary group, suggesting the persistence of the thinking bias and the failure of generalization. This outcome diverges from the results reported in , which reports that the inclusion of exemplary QAs during training enhances models' test performances. We believe that the impact of the thinking bias on the knowledge application abilities within the DescriptionIsName group is the main reason for this divergence. The in-domain performance of models trained with CoT-enhanced QA pairs is slightly lower than that of models trained without CoT steps. We mainly attribute this to the exclusion of CoT steps in our test settings.

## 5 Related works

The reversal curse in LLMsRecent studies have uncovered a notable observation concerning LLMs' generalization abilities. Besides the original paper reporting the reversal curse phenomenon , Grosse et al.  propose an influence function and observe that training examples that match the order (_e.g._, "A is B") are far more influential than examples with a reversed order (_e.g._, "B is A") when given the input "A". This suggests that models without training on facts presented in both directions cannot generalize to both directions. Lv et al.  suggest that the reversal curse could be partly attributed to the training objective of next-token prediction. Zhu et al.  later offers a theoretical analysis of a one-layer transformer to suggest that the reversal curse on completion task stems from the training dynamics of gradient descent.

Figure 5: Results from mix training and QA finetuning mitigation experiments. Both strategies can only help models’ performance on in-domain questions, while the near-random choice performance on out-of-domain (OOD) questions underscores the persistence of the thinking bias.

Our work remains orthogonal to the above works as we explore the manifestation of the reversal curse on more diverse tasks beyond completion. Our experiments reveal that LLMs can generalize beyond and apply their knowledge to MCQs when biographical facts are formatted with names preceding descriptions. Moreover, We find that even when trained with facts presented in both directions, LLMs predominantly master only the part that matches their innate thinking bias.

Effect of data qualityThe quality of data can significantly influence LLMs' learning efficiency [41; 10; 13]. The existing literature on improving the quality of training data can generally be divided into two streams. The first stream enhances data quality through delicate data filtering. A straightforward yet effective filtering method is to remove duplications for both pre-training and finetuning stages, which not only reduces the training duration but also enhances the performance as evidenced by [32; 51; 21]. Another strategy involves condensing the dataset by selectively sub-sampling training instances, which could be executed through heuristic or manual curation  or with a model-centric approach . The second stream aims at increasing the diversity of training examples through data augmentation. Traditional techniques including rule-based  and interpolation-based  methods generally focus on the token-level manipulation and the feature space perturbation. After LLMs demonstrate their superior power in data generation, a growing number of studies [46; 8; 49] have turned to LLMs to produce high-quality and task-specific synthetic data.

Our findings, emphasizing the significance of document structure, can not only be utilized as a filtering criterion towards data efficiency and efficacy but also hold the potential to be combined with entity relation extraction  and knowledge graph  for more effective data augmentation.

## 6 Conclusion

In this study, we initially investigate how the reversal curse manifests across diverse tasks to assess the true boundary of LLMs' generalization abilities. Our findings reveal that LLMs can generalize effectively to "B is A" in multiple-choice questions where both A and B are presented. Notably, this generalization ability appears to be closely linked with the structure of each fact used for training. Furthermore, we reveal that LLMs possess an inherent thinking bias in query processing and knowledge application, which explains and underscores the importance of document structure to successful learning. Our limitations and social impacts are discussed in Section 7 and Appendix G. We hope this work can provide new insights into interpreting and enhancing LLMs' learning abilities.

## 7 Limitations and future work

Our study, while providing valuable insights into the manifestation of the reversal curse and LLMs' problem-solving patterns, has several limitations. Firstly, our work mainly focuses on finding a hypothesis to explain the puzzling MCQ results, namely the thinking bias, and validate its existence through both CoT prompting and internal interaction. The underlying cause of this bias, as well as the proof of its presence in today's state-of-the-art close-sourced models, is not fully explored by our current work.

Secondly, despite several attempts to mitigate the thinking bias, we are frustrated to find that currently available techniques failed to alleviate this problem. It derives a hypothesis that an exhaustive rewrite of all training documents to align their structures with the thinking bias seems to be the most effective approach to facilitate the generalization of knowledge. How to derive an effective and practical methodology to enhance LLMs' training efficacy remains a challenging problem, and we leave this for future work.