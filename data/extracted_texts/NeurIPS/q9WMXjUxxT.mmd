# Trust Region-Based Safe Distributional Reinforcement Learning for Multiple Constraints

Dohyeong Kim\({}^{1}\), Kyungjae Lee\({}^{2}\), and Songhwai Oh\({}^{1}\)

\({}^{1}\)Dep. of Electrical and Computer Engineering and ASRI, Seoul National University

\({}^{2}\)Artificial Intelligence Graduate School, Chung-Ang University

dohyeong.kim@rllab.snu.ac.kr,kyungjae.lee@ai.cau.ac.kr, songhwai@snu.ac.kr

###### Abstract

In safety-critical robotic tasks, potential failures must be reduced, and multiple constraints must be met, such as avoiding collisions, limiting energy consumption, and maintaining balance. Thus, applying safe reinforcement learning (RL) in such robotic tasks requires to handle multiple constraints and use risk-averse constraints rather than risk-neutral constraints. To this end, we propose a trust region-based safe RL algorithm for multiple constraints called a _safe distributional actor-critic_ (SDAC). Our main contributions are as follows: 1) introducing a gradient integration method to manage infeasibility issues in multi-constrained problems, ensuring theoretical convergence, and 2) developing a TD(\(\)) target distribution to estimate risk-averse constraints with low biases. We evaluate SDAC through extensive experiments involving multi- and single-constrained robotic tasks. While maintaining high scores, SDAC shows 1.93 times fewer steps to satisfy all constraints in multi-constrained tasks and 1.78 times fewer constraint violations in single-constrained tasks compared to safe RL baselines. Code is available at: https://github.com/rllab-snu/Safe-Distributional-Actor-Critic.

## 1 Introduction

Deep reinforcement learning (RL) enables reliable control of complex robots (Merel et al., 2020; Peng et al., 2021; Rudin et al., 2022). In order to successfully apply RL to real-world systems, it is essential to design a proper reward function which reflects safety guidelines, such as collision avoidance and limited energy consumption, as well as the goal of the given task. However, finding the reward function that considers all such factors involves a cumbersome and time-consuming process since RL algorithms must be repeatedly performed to verify the results of the designed reward function. Instead, _safe RL_, which handles safety guidelines as constraints, is an appropriate solution. A safe RL problem can be formulated using a constrained Markov decision process (Altman, 1999), where not only the reward but also cost functions are defined to provide the safety guideline signals. By defining constraints using expectation or risk measures of the sum of costs, safe RL aims to maximize returns while satisfying the constraints. Under the safe RL framework, the training process becomes straightforward since there is no need to search for a reward that reflects the safety guidelines.

While various safe RL algorithms have been proposed to deal with the safety guidelines, their applicability to general robotic applications remains limited due to the insufficiency in **1)** handling multiple constraints and **2)** minimizing failures, such as robot breakdowns after collisions. First, many safety-critical applications require multiple constraints, such as maintaining distance from obstacles, limiting operational space, and preventing falls. Lagrange-based safe RL methods (Yang et al., 2021; Zhang and Weng, 2022; Bai et al., 2022), which convert a safe RL problem into a dual problem andupdate the policy and Lagrange multipliers, are commonly used to solve these multi-constrained problems. However, the Lagrangian methods are difficult to guarantee satisfying constraints during training theoretically, and the training process can be unstable due to the multipliers (Stooke et al., 2020). To this end, trust region-based methods (Yang et al., 2020; Kim and Oh, 2022), which can ensure to improve returns while satisfying the constraints under tabular settings (Achiam et al., 2017), have been proposed as an alternative to stabilize the training process. Still, trust region-based methods have a critical issue. Depending on the initial policy settings, there can be an infeasible starting case, meaning that no policy within the trust region satisfies constraints. To address this issue, we can sequentially select a violated constraint and update the policy to reduce the selected constraint (Xu et al., 2021). However, this can be inefficient as only one constraint is considered per update. It will be better to handle multiple constraints at once, but it is a remaining problem to find a policy gradient that reflects several constraints and guarantees to reach a feasible policy set.

Secondly, as RL settings are inherently stochastic, employing risk-neutral measures like expectation to define constraints can lead to frequent failures. Hence, it is crucial to define constraints using risk measures, such as conditional value at risk (CVaR), as they can reduce the potential for massive cost returns by emphasizing tail distributions (Yang et al., 2021; Kim and Oh, 2022). In safe RL, critics are used to estimate the constraint values. Especially, to estimate constraints based on risk measures, it is required to use distributional critics (Dabney et al., 2018), which can be trained using the distributional Bellman update (Bellemare et al., 2017). However, the Bellman update only considers the one-step temporal difference, which can induce a large bias. The estimation bias makes it difficult for critics to judge the policy, which can lead to the policy becoming overly conservative or risky, as shown in Section 5.3. In particular, when there are multiple constraints, the likelihood of deriving incorrect policy gradients due to estimation errors grows exponentially. Therefore, there is a need for a method that can train distributional critics with low biases.

In this paper, we propose a trust region-based safe RL algorithm called a _safe distributional actor-critic_ (SDAC), designed to effectively manage multiple constraints and estimate risk-averse constraints with low biases. First, to handle the infeasible starting case by considering all constraints simultaneously, we propose a _gradient integration method_ that projects unsafe policies into a feasible policy set by solving a quadratic program (QP) consisting of gradients of all constraints. It guarantees to obtain a feasible policy within a finite time under mild technical assumptions, and we experimentally show that it can restore the policy more stably than the existing method (Xu et al., 2021). Furthermore, by updating the policy using the trust region method with the integrated gradient, our approach makes the training process more stable than the Lagrangian method, as demonstrated in Section 5.2. Second, to train critics to estimate constraints with low biases, we propose a _TD(\(\)) target distribution_ which can adjust the bias-variance trade-off. The target distribution is obtained by merging the quantile regression losses (Dabney et al., 2018) of multi-step distributions and extracting a unified distribution from the loss. The unified distribution is then projected onto a quantile distribution set in a memory-efficient manner. We experimentally show that the target distribution can trade off the bias-variance of the constraint estimations (see Section 5.3).

We conduct extensive experiments with multi-constrained locomotion tasks and single-constrained Safety Gym tasks (Ray et al., 2019) to evaluate the proposed method. In the locomotion tasks, SDAC shows 1.93 times fewer steps to satisfy all constraints than the second-best baselines. In the Safety Gym tasks, the proposed method shows 1.78 times fewer constraint violations than the second-best methods while achieving high returns when using risk-averse constraints. As a result, it is shown that the proposed method can efficiently handle multiple constraints using the gradient integration method and effectively lower the constraint violations using the low-biased distributional critics.

## 2 Background

**Constrained Markov Decision Processes.** We formulate the safe RL problem using constrained Markov decision processes (CMDPs) (Altman, 1999). A CMDP is defined as \((S,\)\(A,\)\(P\), \(R\), \(C_{1,..,K}\), \(\), \(\)), where \(S\) is a state space, \(A\) is an action space, \(P:S A S\) is a transition model, \(R:S A S\) is a reward function, \(C_{k\{1,...,K\}}:S A S_{ 0}\) are cost functions, \(:S\) is an initial state distribution, and \((0,1)\) is a discount factor. Given a policy \(\) from a stochastic policy set \(\), the discounted state distribution is defined as \(d^{}(s):=(1-)_{t=0}^{}^{t}(s_{t}=s|)\), and the return is defined as \(Z_{}^{}(s,a):=_{t=0}^{}^{t}R(s_{t},a_{t},s_{t+1})\), where \(s_{0}=s,\ a_{0}=a,\ a_{t}(|s_{t})\), and \(s_{t+1} P(|s_{t},a_{t})\). Then, the state value and state action value functions are defined as: \(V_{R}^{}(s):=[Z_{R}^{}(s,a)|a(|s)],\ Q_{R}^{ }(s,a):=[Z_{R}^{}(s,a)]\). By substituting the costs for the reward, the cost value functions \(V_{C_{k}^{}}^{}(s)\) and \(Q_{C_{k}}^{}(s,a)\) are defined. In the remainder of the paper, the cost parts will be omitted since they can be retrieved by replacing the reward with the costs. Then, the safe RL problem is defined as follows with a safety measure \(F\):

\[*{maximize}_{}\ J()\ \ F(Z_{C_{k}}^{}(s,a)|s ,a(|s)) d_{k}\  k,\] (1)

where \(J():=[Z_{R}^{}(s,a)|s,a(|s)]+ [_{t=0}^{}^{t}H((|s_{t}))|, ,P]\), \(\) is an entropy coefficient, \(H\) is the Shannon entropy, and \(d_{k}\) is a threshold of the \(k\)th constraint.

**Trust-Region Method With a Mean-Std Constraint.**Kim and Oh (2022) have proposed a trust region-based safe RL method with a risk-averse constraint, called a _mean-std_ constraint. The definition of the mean-std constraint function is as follows:

\[F(Z;):=[Z]+((^{-1}())/)[ Z],\] (2)

where \((0,1]\) adjusts the risk level of constraints, \([Z]\) is the standard deviation of \(Z\), and \(\) and \(\) are the probability density function and the cumulative distribution function (CDF) of the standard normal distribution, respectively. In particular, setting \(=1\) causes the standard deviation part to be zero, so the constraint becomes a risk-neutral constraint. Also, the mean-std constraint can effectively reduce the potential for massive cost returns, as shown in Yang et al. (2021), Kim and Oh (2022). In order to calculate the mean-std constraint, it is essential to estimate the standard deviation of the cost return. To this end, Kim and Oh (2022) define the square value functions:

\[S_{C_{k}}^{}(s):=[Z_{C_{k}}^{}(s,a)^{2}|a(|s) ],\ S_{C_{k}}^{}(s,a):=[Z_{C_{k}}^{}(s,a)^{2}].\] (3)

Since \([Z]^{2}=[Z^{2}]-[Z]^{2}\), the \(k\)th constraint can be written as follows:

\[F_{k}(;)=J_{C_{k}}()+((^{-1}())/)}()-J_{C_{k}}()^{2}} d_{k},\] (4)

where \(J_{C_{k}}():=[V_{C_{k}}^{}(s)|s]\), \(J_{S_{k}}():=[S_{C_{k}}^{}(s)|s]\). In order to apply the trust region method (Schulman et al., 2015), it is necessary to derive surrogate functions for the objective and constraints. These surrogates can substitute for the objective and constraints within the trust region. Given a behavioral policy \(\) and the current policy \(_{}\), we denote the surrogates as \(J^{,_{}}()\) and \(F_{k}^{,_{}}(;)\). For the definition and derivation of the surrogates, please refer to Appendix A.7 and (Kim and Oh, 2022). Using the surrogates, a policy can be updated by solving the following subproblem:

\[*{maximize}_{^{}}\ J^{,}(^{})\ \ D_{KL}(||^{}),F_{k}^{,}(^{ },) d_{k}\  k,\] (5)

where \(D_{}(||^{}):=_{s d^{}}[D_{ }((|s)||^{}(|s))]\), \(D_{}\) is the KL divergence, and \(\) is a trust region size. This subproblem can be solved through approximation and a line search (see Appendix A.8). However, it is possible that there is no policy satisfying the constraints of (5). In order to tackle this issue, the policy must be projected onto a feasible policy set that complies with all constraints, yet there is a lack of such methods. In light of this issue, we introduce an efficient feasibility handling method for multi-constrained RL problems.

**Distributional Quantile Critic.**Dabney et al. (2018) have proposed a method for approximating the random variable \(Z_{R}^{}\) to follow a quantile distribution. Given a parametric model, \(:S A^{M}\), \(Z_{R}^{}\) can be approximated as \(Z_{R,}\), called a _distributional quantile critic_. The probability density function of \(Z_{R,}\) is defined as follows:

\[(Z_{R,}(s,a)=z):=_{m=1}^{M}_{_{m}(s,a)}(z)/M,\] (6)

where \(M\) is the number of atoms, \(_{m}(s,a)\) is the \(m\)th atom, \(\) is the Dirac function, and \(_{a}(z):=(z-a)\). The percentile value of the \(m\)th atom is denoted by \(_{m}(_{0}=0,_{i}=i/M)\). In distributional RL, the returns are directly estimated to get value functions, and the target distribution can be calculated from the distributional Bellman operator (Bellemare et al., 2017): \(^{}Z_{R}(s,a):}{{=}}R(s,a,s^{ })+ Z_{R}(s^{},a^{})\), where \(s^{} P(|s,a)\) and \(a^{}(|s^{})\). The above one-step distributional operator can be expanded to the \(n\)-step one: \(_{n}^{}Z_{R}(s_{0},a_{0}):}{{=}} _{t=0}^{n-1}^{t}R(s_{t},a_{t},s_{t+1})+^{n}Z_{R}(s_{n},a_{n})\), where \(a_{t}(|s_{t})\) for \(t=1,...,n\). Then, the critic can be trained to minimize the following quantile regression loss (Dabney et al., 2018):

\[()=_{m=1}^{M}_{(s,a) D}[ _{Z^{}Z_{R,}(s,a)}[_{_{m}}(Z- _{m}(s,a))]]}_{=:_{}^{}(_{m})}, \ \ _{}(x)=x(-_{<0}),\] (7)\(D\) is a replay buffer, \(_{m}:=(_{m-1}+_{m})/2\), and \(_{}^{_{m}}(_{m})\) denotes the quantile regression loss for a single atom. The distributional quantile critic can be plugged into existing actor-critic algorithms because only the critic modeling part is changed.

## 3 Proposed Method

The proposed method comprises two key components: **1)** a feasibility handling method required for multi-constrained safe RL problems and **2)** a target distribution designed to minimize estimation bias. This section sequentially presents these components, followed by a detailed explanation of the proposed method.

### Feasibility Handling For Multiple Constraints

An optimal safe RL policy can be found by iteratively solving the subproblem (5), but the feasible set of (5) can be empty in the infeasible starting cases. To address the feasibility issue in safe RL with multiple constraints, one of the violated constraints can be selected, and the policy is updated to minimize the constraint until the feasible region is not empty (Xu et al., 2021), which is called a _naive approach_. However, it may not be easy to quickly reach the feasible condition if only one constraint at each update step is used to update the policy. Therefore, we propose a feasibility handling method which reflect all the constraints simultaneously, called a _gradient integration method_. The main idea is to get a gradient that reduces the value of violated constraints and keeps unviolated constraints. To find such a gradient, the following quadratic program (QP) can be formulated by linearly approximating the constraints:

\[g^{*}=*{argmin}_{g}g^{T}Hg\ \ \ g_{k}^{T}g+c_{k}  0\  k,\ c_{k}:=(^{T}H^{-1}g_{k}},F_{k}(_{ };)-d_{k}+),\] (8)

where \(H\) is the Hessian of KL divergence between the previous policy and the current policy with parameters \(\), \(g_{k}\) is the gradient of the \(k\)th constraint, \(c_{k}\) is a truncated threshold to make the \(k\)th constraint tangent to the trust region, \(\) is a trust region size, and \(_{>0}\) is a slack coefficient. The reason why we truncate constraints is to make the gradient integration method invariant to the gradient scale. Otherwise, constraints with larger gradient scales might produce a dominant policy gradient. Finally, we update the policy parameters using the clipped gradient as follows:

\[^{*}=+(1,Hg^{*})})g^{*}.\] (9)

Figure 1 illustrates the proposed gradient integration process. In summary, the policy is updated by solving (5); if there is no solution to (5), it is updated using the gradient integration method. Then, the policy can reach the feasibility condition within finite time steps.

**Theorem 3.1**.: _Assume that the constraints are differentiable and convex, gradients of the constraints are \(L\)-Lipschitz continuous, eigenvalues of the Hessian are equal or greater than a positive value \(R_{>0}\), and \(\{|F_{k}(_{};)+<d_{k},\  k\}\). Then, there exists \(E_{>0}\) such that if

Figure 1: **Left: Gradient integration. Each constraint is truncated to be tangent to the trust region indicated by the ellipse, and the dashed straight lines show the truncated constraints. The solution of (8) is indicated in blue, pointing to the nearest point in the intersection of the truncated constraints. If the solution crosses the trust region, parameters are updated by the clipped direction, shown in red. Right: Optimization paths of the proposed and naive method in a toy example. The description is presented in Appendix A.2. The contour graph represents the objective of the toy example. The optimization paths exhibit distinct characteristics due to the difference that the naive method reflects only one constraint and the proposed method considers all constraints at once.**

\(0< E\) and a policy is updated by the proposed gradient integration method, all constraints are satisfied within finite time steps._

Note that the first two assumptions of Theorem 3.1 are commonly used in multi-task learning (Liu et al., 2021; Yu et al., 2020; Navon et al., 2022), and the assumption on eigenvalues is used in most trust region-based RL methods (Schulman et al., 2015; Kim and Oh, 2022), so the assumptions in Theorem 3.1 can be considered reasonable. We provide the proof and show the existence of a solution (8) in Appendix A.1. The provided proof shows that the constant \(E\) is proportional to \(\). This means that the trust region size should be set smaller as \(\) decreases. Also, we further analyze the worst-case time to satisfy all constraints by comparing the gradient integration method and naive approach in Appendix A.3. In conclusion, if the policy update rule (5) is not feasible, a finite number of applications of the gradient integration method will make the policy feasible.

### TD(\(\)) Target Distribution

The mean-std constraints can be estimated using the distributional quantile critics. Since the estimated constraints obtained from the critics are directly used to update policies in (5), estimating the constraints with low biases is crucial. In order to reduce the estimation bias of the critics, we propose a target distribution by capturing that the TD(\(\)) loss, which is obtained by a weighted sum of several losses, and the quantile regression loss with a single distribution are identical. A recursive method is then introduced so that the target distribution can be obtained practically. First, the \(n\)-step targets for the current policy \(\) are estimated as follows, after collecting trajectories \((s_{t},a_{t},s_{t+1},...)\) with a behavioral policy \(\):

\[_{R}^{(n)}(s_{t},a_{t}):}{{=}}R_{t}+ R _{t+1}++^{n-1}R_{t+n-1}+^{n}Z_{R,}(s_{t+n},_{t+ n}),\] (10)

where \(R_{t}=R(s_{t},a_{t},s_{t+1})\), and \(_{t+n}(|s_{t+n})\). Note that the \(n\)-step target controls the bias-variance tradeoff using \(n\). If \(n\) is equal to \(1\), the \(n\)-step target is equivalent to the temporal difference method that has low variance but high bias. On the contrary, if \(n\) increases to infinity, it becomes a Monte Carlo estimation that has high variance but low bias. However, finding proper \(n\) is another cumbersome task. To alleviate this issue, TD(\(\)) (Sutton, 1988) method considers the discounted sum of all \(n\)-step targets. Similar to TD(\(\)), we define the TD(\(\)) loss for the distributional quantile critic as the discounted sum of all quantile regression losses with \(n\)-step targets. Then, the TD(\(\)) loss for a single atom is approximated using importance sampling of the sampled \(n\)-step targets in (10) as:

\[_{}^{_{m}}(_{m})=(1-)_ {i=0}^{}^{i}_{(s_{t},a_{t}) D}[_{Z _{t+1}^{}Z_{R,}(s_{t},a_{t})}[_{_{m}}(Z -_{m}(s_{t},a_{t}))]]\] (11) \[=(1-)_{i=0}^{}^{i}_{(s_{t},a_{ t}) D}[_{Z_{t+1}^{}Z_{R,}(s_{t},a_{t})} [_{m=t+1}^{t+1}|s_{j})}{(a_{j}|s_{j})}_{_{m}}(Z-_{m}(s_{t},a_{t}))]]\] \[(1-)_{i=0}^{}^{i}_{(s_{t },a_{t}) D}[_{j=t+1}^{t+i}|s_{j})}{(a_{j}|s_{j })}_{m=1}^{M}_{_{m}}(_{R,m}^{(i+1)}(s_{t},a_{t})-_{m}(s_{t},a_{t}))],\]

where \(\) is a trace-decay value, and \(_{R,m}^{(i)}\) is the \(m\)th atom of \(_{R}^{(i)}\). Since \(_{R}^{(i)}(s_{t},a_{t})}{{=}}R_{t}+ _{R}^{(i-1)}(s_{t+1},a_{t+1})\) is satisfied, (11) is the same as the quantile regression loss with the following single distribution \(_{t}^{}\), called a _TD(\(\)) target distribution_:

\[(_{t}^{}=z):=_{t}}(1- )_{i=0}^{}^{i}_{j=t+1}^{t+i}|s_{j })}{(a_{j}|s_{j})}_{m=1}^{M}_{_{R,m}^{(i+1)}(s_{ t},a_{t})}(z)\] (12) \[=_{t}}_{m=1}^{M} {M}_{_{t,m}^{(1)}}(z)+|s_{t+1})}{(a_{ t+1}|s_{t+1})}_{i=0}^{}^{i}_{j=t+2}^{t+1+i}|s_{j })}{(a_{j}|s_{j})}_{m=1}^{M}_{_{R,m}^{(i+2)}(s_{ t},a_{t})}(z)\] \[=_{t}}^{M} _{_{t,m}^{(1)}}(z)}_{()}+_{t+1}}{_{t}}|s_{t+1})}{(a_{t+1}|s_{t +1})}+_{t+1}^{}=z)}_{()},\]

where \(_{t}\) is a normalization factor. If the target for time step \(t+1\) is obtained, the target distribution for time step \(t\) becomes the weighted sum of **(a)** the current one-step TD target and **(b)** the shifted

[MISSING_PAGE_EMPTY:6]

methods depending on how to update policies to reflect safety constraints. First, trust region-based safe RL methods (Achiam et al., 2017; Yang et al., 2020; Kim and Oh, 2022) find policy update directions by approximating the safe RL problem as a linear-quadratic constrained linear program and update policies through a line search. Yang et al. (2020) also employ projection to meet a constraint; however, their method is limited to a single constraint and does not show to satisfy the constraint for the infeasible starting case. Second, Lagrangian-based methods (Stooke et al., 2020; Yang et al., 2021; Liu et al., 2020) convert the safe RL problem to a dual problem and update the policy and dual variables simultaneously. Last, expectation-maximization (EM) based methods (Liu et al., 2022; Zhang et al., 2022) find non-parametric policy distributions by solving the safe RL problem in E-steps and fit parametric policies to the found non-parametric distributions in M-steps. Also, there are other ways to reflect safety other than policy updates. Qin et al. (2021); Lee et al. (2022) find optimal state or state-action distributions that satisfy constraints, and Bharadhwaj et al. (2021); Thananjeyan et al. (2021) reflect safety during exploration by executing only safe action candidates. In the experiments, only the safe RL methods of the policy update approach are compared with the proposed method.

**Distributional TD(\(\)).** TD(\(\)) (Precup et al., 2000) can be extended to the distributional critic to trade off bias-variance. Gruslys et al. (2018) have proposed a method to obtain target distributions by mixing \(n\)-step distributions, but the method is applicable only in discrete action spaces. Nam et al. (2021) have proposed a method to obtain target distributions using sampling to apply to continuous action spaces, but this is only for on-policy settings. A method proposed by Tang et al. (2022) updates the critics using newly defined distributional TD errors rather than target distributions. This method is applicable for off-policy settings but has the disadvantage that memory usage increases linearly with the number of TD error steps. In contrast to these methods, the proposed method is memory-efficient and applicable for continuous action spaces under off-policy settings.

**Gradient Integration.** The proposed feasibility handling method utilizes a gradient integration method, which is widely used in multi-task learning (MTL). The gradient integration method finds a single gradient to improve all tasks by using gradients of all tasks. Yu et al. (2020) have proposed a projection-based gradient integration method, which is guaranteed to converge Pareto-stationary sets. A method proposed by Liu et al. (2021) can reflect user preference, and Navon et al. (2022) proposed a gradient-scale invariant method to prevent the training process from being biased by a few tasks. The proposed method can be viewed as a mixture of projection and scale-invariant methods as gradients are clipped and projected onto a trust region.

## 5 Experiments

We evaluate the safety performance of the proposed method in single- and multi-constrained robotic tasks. For single constraints, the agent performs four tasks provided by Safety Gym (Ray et al., 2019), and for multi-constraints, it performs bipedal and quadrupedal locomotion tasks.

### Safety Gym

**Tasks.** We employ two robots, point and car, to perform goal and button tasks in the Safety Gym. The goal task is to control a robot toward a randomly spawned goal without passing through hazard regions. The button task is to click a randomly designated button using a robot, where not only hazard regions but also dynamic obstacles exist. Agents get a cost when touching undesignated buttons and obstacles or entering hazard regions. There is only one constraint for the Safety Gym tasks, and it is defined using (4) with the sum of costs. Constraint violations (CVs) are counted when the cost sum exceeds the threshold. For more details, see Appendix B.

**Baselines.** Safe RL methods based on various types of policy updates are used as baselines. For the trust region-based method, we use constrained policy optimization (CPO) (Achiam et al., 2017) and off-policy trust-region CVaR (OffTRC) (Kim and Oh, 2022), which extend the CPO to an off-policy and mean-std constrained version. For the Lagrangian-based method, distributional worst-case soft actor-critic (WCSAC) (Yang et al., 2022) is used, and constrained variational policy optimization (CVPO) (Liu et al., 2022) based on the EM method is used. Specifically, WCSAC, OffTRC, and the proposed method, SDAC, use the risk-averse constraints, so we experiment with those for \(=0.25\) and \(1.0\) (when \(=1.0\), the constraint is identical to the risk-neutral constraint).

**Results.** The graph of the final reward sum, cost rate, and the total number of CVs are shown in Figure 2(a), and the training curves are provided in Appendix C.1. We can interpret the results as good if the reward sum is high and the cost rate and total CVs are low. SDAC with \(=0.25\), risk-averse constraint situations, satisfies the constraints in all tasks and shows an average of 1.78 times fewer total CVs than the second-best algorithm. Nevertheless, since the reward sums are also in the middle or upper ranks, its safety performance is of high quality. SDAC with \(=1.0\), risk-neutral constraint situations, shows that the cost rates are almost the same as the thresholds except for the car button. In the case of the car button, the constraint is not satisfied, but by setting \(=0.25\), SDAC can achieve the lowest total CVs and the highest reward sum compared to the other methods. As for the reward sum, SDAC is the highest in the point goal and car button, and WCSAC is the highest in the rest. However, WCSAC seems to lose the risk-averse properties seeing that the cost rates do not change significantly according to \(\). This is because WCSAC does not define constraints as risk measures of cost returns but as expectations of risk measures (Yang et al., 2022). OffTRC has lower safety performance than SDAC in most cases because, unlike SDAC, it does not use distributional critics. Finally, CVPO and CPO are on-policy methods, so they are less efficient than the other methods.

### Locomotion Tasks

**Tasks.** The locomotion tasks are to train robots to follow \(xy\)-directional linear and \(z\)-directional angular velocity commands. Mini-Cheetah from MIT (Katz et al., 2019) and Laikago from Unitree (Wang, 2018) are used for quadrupedal robots, and Cassie from Agility Robotics (Xie et al., 2018) is used for a bipedal robot. In order to perform the locomotion tasks, robots should keep balancing, standing, and stamping their feet so that they can move in any direction. Therefore, we define three constraints. The first is to keep the balance so that the body angle does not deviate from zero, and the second is to keep the height of CoM above a threshold. The third is to match the current foot

Figure 3: **Safety Gym task results. The cost rates show the cost sums divided by the episode length, and the dashed black lines indicate the threshold of the constraint. All methods are trained with five random seeds.**

contact state with a predefined foot contact timing. The reward is defined as the negative \(l^{2}\)-norm of the difference between the command and the current velocity. CVs are counted when the sum of at least one cost rate exceeds the threshold. For more details, see Appendix B.

**Baselines.** The baseline methods are identical to the Safety Gym tasks, and CVPO is excluded because it is technically challenging to scale to multiple constraint settings. We set \(\) to \(1\) for the risk-averse constrained methods (OffTRC, WCSAC, and SDAC) to focus on measuring multi-constraint handling performance.

**Results.** Figure 4.(a-c) presents the training curves. SDAC shows the highest reward sums and the lowest total CVs in all tasks. In particular, the number of steps required to satisfy all constraints is 1.93 times fewer than the second-best algorithm on average. Trust region methods (OffTRC, CPO) stably satisfy constraints, but they are not efficient since they handle constraints by the naive approach. WCSAC, a Lagrangian method, fails to keep the constraints and shows the lowest reward sums. This is because the Lagrange multipliers can hinder the training stability due to the concurrent update with policy (Stooke et al., 2020).

### Ablation Study

We conduct ablation studies to show whether the proposed target distribution lowers the estimation bias and whether the proposed gradient integration quickly converges to the feasibility condition. In Figure 3b, the number of CVs is reduced as \(\) increases, which means that the bias of constraint estimation decreases. However, the score also decreases due to large variance, showing that \(\) can adjust the bias-variance tradeoff. In Figure 4d, the proposed gradient integration method is compared with the naive approach, which minimizes the constraints in order from the first to the third constraint, as described in Section 3.1. The proposed method reaches the feasibility condition faster than the naive approach and shows stable training curves because it reflects all constraints concurrently. Additionally, we analyze the distributional critics in Appendix C.2 and the hyperparameters, such as the trust region size, in Appendix C.3. Furthermore, we analyze the sensitivity of traditional RL algorithms to reward configuration in Appendix D, emphasizing the advantage of safe RL that does not require reward tuning.

Figure 4: **Locomotion task results. The black dashed lines indicate the thresholds, and the dotted lines in (d) represent the thresholds + \(0.025\). The shaded area represents half of the standard deviation, and all methods are trained with five random seeds.**

Limitation

A limitation of the proposed method is that the computational complexity of the gradient integration is proportional to the square of the number of constraints, whose qualitative analysis is presented in Appendix E.1. Also, we conducted quantitative analyses in Appendix E.2 by measuring wall clock training time. In the mini-cheetah task which has three constraints, the training time of SDAC is the third fastest among the four safe RL algorithms. Gradient integration is not applied when the policy satisfies constraints, so it may not constitute a significant proportion of training time. However, its influence can be dominant as the number of constraints increases. In order to resolve this limitation, the calculation can be speeded up by stochastically selecting a subset of constraints (Liu et al., 2021), or by reducing the frequency of policy updates (Navon et al., 2022). The other limitation is that the mean-std defined in (2) is not a coherent risk measure. As a result, mean-std constraints can be served as reducing uncertainty rather than risk, although we experimentally showed that constraint violations are efficiently reduced. To resolve this, we can use the CVaR constraint, which can be estimated using an auxiliary variable, as done by Chow et al. (2017). However, this solution can destabilize the training process due to the auxiliary variable, as observed in experiments of Kim and Oh (2022). Hence, a stabilization technique should be developed to employ the CVaR constraint.

## 7 Conclusion

We have presented the trust region-based safe distributional RL method, called _SDAC_. Through the locomotion tasks, it is verified that the proposed method efficiently satisfies multiple constraints using the gradient integration. Moreover, constraints can be stably satisfied in various tasks due to the low-biased distributional critics trained using the proposed target distributions. In addition, the proposed method is analyzed from multiple perspectives through various ablation studies. However, to compensate for the computational complexity, future work plans to devise efficient methods when dealing with large numbers of constraints.