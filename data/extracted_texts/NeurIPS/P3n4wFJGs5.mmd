# Window-Based Distribution Shift Detection for Deep Neural Networks

Guy Bar-Shalom

Department of Computer Science

Technion - Israel Institute of Technology

guy.b@cs.technion.ac.il

Yonatan Geifman

Deci.AI

yonatan@deci.ai

Ran El-Yaniv

Department of Computer Science

Technion - Israel Institute of Technology, Deci.AI

rani@cs.technion.ac.il

###### Abstract

To deploy and operate deep neural models in production, the quality of their predictions, which might be contaminated benignly or manipulated maliciously by input distributional deviations, must be monitored and assessed. Specifically, we study the case of monitoring the healthy operation of a deep neural network (DNN) receiving a stream of data, with the aim of detecting input distributional deviations over which the quality of the network's predictions is potentially damaged. Using selective prediction principles, we propose a distribution deviation detection method for DNNs. The proposed method is derived from a tight coverage generalization bound computed over a sample of instances drawn from the true underlying distribution. Based on this bound, our detector continuously monitors the operation of the network over a test window and fires off an alarm whenever a deviation is detected. Our novel detection method performs on-par or better than the state-of-the-art, while consuming substantially lower computation time (five orders of magnitude reduction) and space complexity. Unlike previous methods, which require at least linear dependence on the size of the source distribution for each detection, rendering them inapplicable to "Google-Scale" datasets, our approach eliminates this dependence, making it suitable for real-world applications. Code is available at https://github.com/BarSGuy/Window-Based-Distribution-Shift-Detection.

## 1 Introduction

A wide range of artificial intelligence applications and services rely on deep neural models because of their remarkable accuracy. When a trained model is deployed in production, its operation should be monitored for abnormal behavior, and a flag should be raised if such is detected. Corrective measures can be taken if the underlying cause of the abnormal behavior is identified. For example, simple distributional changes may only require retraining with fresh data, while more severe cases may require redesigning the model (e.g., when new classes emerge).

In this paper we focus on distribution shift detection in the context of deep neural models and consider the following setting. Pretrained model \(f\) is given, and we presume it was trained with data sampled from some distribution \(P\). In addition to the dataset used in training \(f\), we are also given an additional unlabeled sample of data from the same distribution, which is used to train a detector \(D\) (we refer to this as the detection-training set or source set). While \(f\) is used in production to process a stream of emerging input data, we continually feed \(D\) with the most recent window \(W_{k}\) of \(k\) input elements.

The detector also has access to the final layers of the model \(f\) and should be able to determine whether the data contained in \(W_{k}\) came from a distribution different from \(P\). We emphasize that in this paper we are not considering the problem of identifying _single-instance_ out-of-distribution or outlier instances , but rather the information residing in a population of \(k\) instances. While it may seem straightforward to apply single-instance detectors to a window (by applying the detector to each instance in the window), this approach can be computationally expensive since such methods are not designed for window-based tasks; see discussion in Section 3. Moreover, we demonstrate here that _computationally feasible_ single-instance methods can fail to detect population-based deviations. We emphasize that we are not concerned in characterizing types of distribution shifts, nor do we tackle at all the complementary topic of out-of-distribution robustness.

Distribution shift detection has been scarcely considered in the context of deep neural networks (DNNs), however, it is a fundamental topic in machine learning and statistics. The standard method for tackling it is by performing a dimensionality reduction over both the detection-training (source) and test (target) samples, and then applying a two-sample statistical test over these reduced representations to detect a deviation. This is further discussed in Section 3. In particular, deep models can benefit from the semantic representation created by the model itself, which provides meaningful dimensionality reduction, that is readily available at the last layers of the model. Using the embedding layer (or softmax) along with statistical two-sample tests was recently proposed by Lipton et al.  and Rabanser et al.  who termed solutions of this structure black-box shift detection (BBSD). Using both the univariate Kolmogorov-Smirnov (KS) test and the maximum mean discrepancy (MMD) method, see details below, Rabanser et al.  achieve impressive detection results when using MNIST and CIFAR-10 as proxies for the distribution \(\). As shown here, the KS test is also very effective over ImageNet when a stronger model is used (ResNet50 vs ResNet18). However, BBSD methods have the disadvantage of being computationally intensive (and probably inapplicable to read-world datasets) due to the use of two-sample tests between the detection-training set (which can, and is preferred to be the largest possible) and the window \(W_{k}\); a complexity analysis is provided in Section 5.

We propose a different approach termed _Coverage-Based Detection_ (CBD), which builds upon selective prediction principles . In this approach, a model quantifies its prediction uncertainty and abstains from predicting uncertain instances. First, we develop a method for selective prediction with guaranteed coverage. This method identifies the best abstaining threshold and coverage bound for a given pretrained classifier \(f\), such that the resulting empirical coverage will not violate the bound with high probability (when abstention is determined using the threshold). The guaranteed coverage method is of independent interest, and it is analogous to selective prediction with guaranteed risk . Because the empirical coverage of such a classifier is highly unlikely to violate the bound if the underlying distribution remains the same, a systematic violation indicates a distribution shift. To be more specific, given a detection-training sample \(S_{m}\), our coverage-based detection algorithm computes a fixed number of tight generalization coverage bounds, which are then used to detect a distribution shift in a window \(W_{k}\) of test data. The proposed detection algorithm exhibits remarkable computational efficiency due to its ability to operate independently of the size of \(S_{m}\) during detection, which is crucial when considering "Google-Scale" datasets, such as the JFT-3B dataset. In contrast, the currently available distribution shift detectors rely on a framework that requires significantly higher computational requirements (this framework is illustrated in Figure 3 in Appendix A). A run-time comparison of those baselines w.r.t. our CBD method is provided in Figure 1.

In a comprehensive empirical study, we compared our CBD algorithm with the best-performing baselines, including the KS approach of . Additionally, we investigated the suitability of single-instance detection methods for identifying distribution shifts. For a fair comparison, all methods used the same (publicly available) underlying models: ResNet50, MobileNetV3-S, and ViT-T. To evaluate the effectiveness of our approach, we employed the ImageNet dataset to simulate the source distribution. We then introduced distribution shifts using a range of methods, starting with simple noise and progressing to more sophisticated techniques such as adversarial examples. Based on these experiments, it is evident that CBD is overall significantly more powerful than the baselines across a wide range of test window sizes.

To summarize, the contributions of this paper are: (1) A theoretically justified algorithm (Algorithm 1), that produces a coverage bound, which is of independent interest, and allows for the creation of selective classifiers with guaranteed coverage. (2) A theoretically motivated "windowed" detection algorithm, CBD (Algorithm 2), which detects a distribution shift over a window; this proposed algorithm exhibits remarkable efficiency compared to state-of-the-art methods (five orders of magnitude better than the best method). (3) A comprehensive empirical study demonstrating significant improvements relative to existing baselines, and introducing the use of single-instance methods for detecting distribution shifts.

## 2 Problem Formulation

We consider the problem of detecting distribution shifts in input streams provided to pretrained deep neural models. Let \( P_{X}\) denote a probability distribution over an input space \(\), and assume that a model \(f\) has been trained on a set of instances drawn from \(\). Consider a setting where the model \(f\) is deployed and while being used in production its input distribution might change or even be attacked by an adversary. Our goal is to detect such events to allow for appropriate action, e.g., retraining the model with respect to the revised distribution.

Inspired by Rabanser et al. , we formulate this problem as follows. We are given a pretrained model \(f\), and a detection-training set, \(S_{m}^{m}\). Then we would like to train a detection model to be able to detect a distribution shift; namely, discriminate between windows containing in-distribution (ID) data, and _alternative-distribution_ (AD) data. Thus, given an unlabeled test sample window \(W_{k} Q^{k}\), where \(Q\) is a possibly different distribution, the objective is to determine whether \( Q\). We also ask what is the smallest test sample size \(k\) required to determine that \( Q\). Since typically the detection-training set \(S_{m}\) can be quite large, we further ask whether it is possible to devise an effective detection procedure whose time complexity is \(o(m)\).

## 3 Related Work

Distribution shift detection methods often comprise the following two steps: dimensionality reduction, and a two-sample test between the detection-training sample and test samples. In most cases, these methods are "lazy" in the sense that for each test sample, they make a detection decision based on a computation over the entire detection-training sample. Their performance will be sub-optimal if only a subset of the train sample is used. Figure 3 in Appendix A illustrates this general framework.

The use of dimensionality reduction is optional. It can often improve performance by focusing on a less noisy representation of the data. Dimensionality reduction techniques include no reduction, _principal components analysis_, _sparse random projection_, _autoencoders_[39; 36], _domain classifiers_,  and more. In this work we focus on _black box shift detection_ (BBSD) methods , that rely on deep neural representations of the data generated by a pretrained model. The representation extracted from the model will typically utilize either the softmax outputs, acronymed BBSD-Softmax, or the embeddings, acronymed BBSD-Embeddings; for simplicity, we may omit the BBSD acronym. Due to the dimensionality of the final representation, multivariate or multiple univariate two-sample tests can be conducted.

By combining BBSD-Softmax with a Kolmogorov-Smirnov (KS) statistical test  and using the Bonferroni correction , Rabanser et al.  achieved state-of-the-art results in distribution shift detection in the context of image classification (MNIST and CIFAR-10). We acronym their method as BBSD-KS-Softmax (or KS-Softmax). The _univariate_ KS test processes individual dimensions separately; its statistic is calculated by computing the largest difference \(Z\) of the _cumulative density functions_ (CDFs) across all dimensions as follows: \(Z=_{}|F_{}()-F_{Q}()|\), where \(F_{Q}\) and \(F_{}\) are the empirical CDFs of the detection-training and test data (which are sampled from \(\) and \(Q\), respectively; see Section 2). The Bonferroni correction rejects the null hypothesis when the minimal p-value among all tests is less than \(\), where \(\) is the significance level and \(d\) is the number of dimensions. Although less conservative approaches to aggregation exist [20; 30], they usually assume some dependencies among the tests. The _maximum mean discrepancy_ (MMD) method  is a kernel-based multivariate test that can be used to distinguish between probability distributions \(\) and \(Q\). Formally, \(MMD^{2}(,,Q)=|_{}-_{Q}||_{ _{2}}^{2}\), where \(_{}\) and \(_{Q}\) are the mean embeddings of \(\) and \(Q\) in a reproducing kernel Hilbert space \(\). Given a kernel \(\), and samples, \(\{x_{1},x_{2},,x_{m}\}^{m}\) and \(\{x_{1}^{},x_{2}^{},,x_{k}^{}\} Q^{k}\), an unbiased estimator for \(MMD^{2}\) can be found in [18; 42]. Sutherland et al.  and Gretton et al.  used the RBF kernel \((x,x^{})=e^{-}\|x-x^{}\|_{2}^{2}}\), where \(2^{2}\) is set to the median of the pairwise Euclidean distancesbetween all samples. By performing a permutation test on the kernel matrix, the p-value is obtained. In our experiments (see Section 6.4), we thus use four population based baselines: **KS-Softmax**, **KS-Embeddings**, **MMD-Softmax**, and **MMD-Embeddings**.

As mentioned in the introduction, our work is complementary to the topic of single-instance out-of-distribution (OOD) detection [28; 21; 22; 16; 38; 35; 34; 11]. Although these methods can be applied to each instance in a window, they often fail to capture population statistics within the window, making them inadequate for detecting population-based changes. Additionally, many of these methods are computationally expensive and cannot be applied efficiently to large windows. For example, methods such as those described in [41; 27] extract values from each layer in the network, while others such as  require gradient calculations. We note that the application of the best single-instance methods such as [41; 27; 28] in our (large scale) empirical setting is computationally challenging and preclude empirical comparison to our method. Therefore, we consider (in Section 6.4) the detection performance of two computationally efficient single-instance baselines: Softmax-Response (abbreviated as **Single-instance SR** or Single-SR) and Entropy-based (abbreviated as **Single-instance Ent** or Single-Ent), as described in [4; 6; 21]. Specifically, we apply each single-instance OOD detector to every instance in the window and in the detection-training set. We then use a two-sample t-test to determine the p-value between the uncertainty estimators of each sample.

Finally, we mention  who developed a risk generalization bound for selective classifiers . The bound presented in that paper is analogous to the coverage generalization bound we present in Theorem 4.2. The risk bound in  can also be used for shift-detection. To apply their risk bound to this task, however, labels, which are not available, are required. CBD detects distribution shifts without using any labels.

## 4 Proposed Method - Coverage-Based Detection

In this section we present _Coverage-Based Detection_ (CBD), a novel technique for detecting a distribution shift based on selective prediction principles (definitions follow). We develop a tight generalization coverage bound that holds with high probability for ID data, sampled from the source distribution. The main idea is that violations of this coverage bound indicate a distribution shift with high probability. In Section 6.2, we offer an intuitive demonstration that aids in understanding our approach.

### Selection with Guaranteed Coverage

We begin by introducing basic selective prediction terminology and definitions that are required to describe our method. Consider a standard multiclass classification problem, where \(\) is some feature space (e.g., raw image data) and \(\) is a finite label set, \(=\{1,2,3,...,C\}\), representing \(C\) classes. Let \(P(X,Y)\) be a probability distribution over \(\), and define a _classifier_ as a function \(f:\). We refer to \(P\) as the _source distribution_. A _selective classifier_ is a pair \((f,g)\), where \(f\) is a classifier and \(g:\{0,1\}\) is a _selection function_, which serves as a binary qualifier for \(f\) as follows,

\[(f,g)(x)f(x),&g(x)=1;\\ ,&g(x)=0.\]

A general approach for constructing a selection function based on a given classifier \(f\) is to work in terms of a _confidence-rate function_, \(_{f}:^{+}\), referred to as CF. The CF \(_{f}\) should quantify confidence in predicting the label of \(x\) based on signals extracted from \(f\). The most common and well-known CF for a classification model \(f\) (with softmax at its last layer) is its _softmax response_ (SR) value [4; 6; 21]. A given CF \(_{f}\) can be straightforwardly used to define a selection function: \(g_{}(x) g_{}(x|_{f})=[_{f}(x) ]\), where \(\) is a user-defined constant. For any selection function, we define its _coverage_ w.r.t. a distribution \(\) (recall, \( P_{X}\), see Section 2), and its _empirical coverage_ w.r.t. a sample \(S_{k}\{x_{1},x_{2}, x_{k}\}\), as \(c(,)_{}[g_{}(x)]\), and \((,S_{k})_{i=1}^{k}g_{}(x_{i})\), respectively.

Given a bound on the expected coverage for a given selection function, we can use it to detect a distribution shift via violations of the bound. We will now formally state the problem, develop the bound, and demonstrate its use in detecting distribution shifts.

**Problem statement.**_For a classifier \(f\), a detection-training sample \(S_{m}^{m}\), a confidence parameter \(>0\), and a desired coverage \(c^{*}>0\), our goal is to use \(S_{m}\) to find a \(\) value (which implies a selection function \(g_{}\)) that guarantees the desired coverage, with probability \(1-\), namely,_

\[_{S_{m}}\{c(,)<c^{*}\}<.\] (1)

This means that _under coverage_ should occur with probability of at most \(\).

```
1Input: detection-training set: \(S_{m}\), confidence-rate function: \(_{f}\), confidence parameter \(\), target coverage: \(c^{*}\).
2Sort \(S_{m}\) according to \(_{f}(x_{i})\), \(x_{i} S_{m}\) (and now assume w.l.o.g. that indices reflect this ordering).
3\(z_{}=1\), \(z_{}=m\)
4for\(i=1\)to\(k=_{2}m\)do
5\(z=(z_{}+z_{})/2\)
6\(_{i}=_{f}(x_{i})\)
7 Calculate \(_{i}(_{i},S_{m})\)
8 Solve for \(b_{i}^{*}(m,m_{i}(_{i},S_{m}),)\) {see Lemma 4.1}
9if\(b_{i}^{*}(m,m_{i}(_{i},S_{m}),) c^{*}\)then
10\(z_{}=z\)
11else
12\(z_{}=z\)
13 endif
14
15 endfor
16Output: bound: \(b_{k}^{*}(m,m_{k}(_{k},S_{m}),)\), threshold: \(_{k}\). ```

**Algorithm 1**Selection with guaranteed coverage (SGC)

A threshold \(\) that guarantees Equation (1) provides a probabilistic lower bound, guaranteeing that coverage \(c\) of ID unseen population (sampled from \(\)) satisfies \(c>c^{*}\) with probability of at least \(1-\). For the remaining of this section, we introduce the _selection with guaranteed coverage_ (SGC) algorithm (Algorithm 1), which outputs a bound (\(b^{*}\)) and its corresponding threshold (\(\)).

The SGC algorithm receives as input a classifier \(f\), a CF \(_{f}\), a confidence parameter \(\), a target coverage \(c^{*}\), and a detection-training set \(S_{m}\).

The algorithm performs a binary search to find the optimal coverage lower bound with confidence \(\), and outputs a coverage bound \(b^{*}\) and the corresponding threshold \(\), defining the selection function. A pseudo code of the SGC algorithm appears in Algorithm 1.

Our analysis of the SGC algorithm makes use of Lemma 4.1, which gives a tight numerical (generalization) bound on the expected coverage, based on a test over a sample. The proof of Lemma 4.1 is nearly identical to Langford's proof of Theorem 3.3 in , p. 278, where instead of the empirical error used in , we use the empirical coverage, which is also a Bernoulli random variable.

**Lemma 4.1**.: _Let \(\) be any distribution and consider a selection function \(g_{}\) with a threshold \(\) whose coverage is \(c(,)\). Let \(0<<1\) be given and let \((,S_{m})\) be the empirical coverage w.r.t. the set \(S_{m}\), sampled i.i.d. from \(\). Let \(b^{*}(m,m(,S_{m}),)\) be the solution of the following equation:_

\[(_{j=0}^{m(,S_{m})}b^{j}(1-b)^{m-j} 1-).\] (2)

_Then,_

\[_{S_{m}}\{c(,)<b^{*}(m,m( ,S_{m}),)\}<.\] (3)

The following is a uniform convergence theorem for the SGC procedure stating that all the calculated bounds are valid simultaneously with a probability of at least \(1-\).

**Theorem 4.2**.: _(SGC - Uniform convergence) Assume \(S_{m}\) is sampled i.i.d. from \(\), and consider an application of Algorithm 1. For \(k=_{2}m\), let \(b_{i}^{*}(m,m_{i}(_{i},S_{m}),)\) and \(_{i}\) be the values obtained in the \(i^{}\) iteration of Algorithm 1. Then,_

\[_{S_{m}}\{ i:c(_{i},)<b_{i}^{*}(m,m _{i}(_{i},S_{m}),)\}<.\]_Proof (sketch - see full proof in the Appendix B.1)._ Define,

\[_{_{i}} b_{i}^{*}(m,m_{i}( _{i},S_{m}),),_{_{i}} c( _{i},)\] \[_{S_{m}}\{ i:_{_{i}}<_{_{i}}\} = _{i=1}^{k}_{0}^{1}\,d^{}_{S_{m}}\{ _{^{}}<_{^{}}\}_{S _{m}}\{_{i}=^{}\}\] \[< _{i=1}^{k}_{0}^{1}\,d^{} _{S_{m}}\{_{i}=^{}\}=_{i=1}^{k}=.\]

We would like to note that in addition to Algorithm 1, we also provide a complementary algorithm that returns a tight coverage **upper** (rather than lower) bound, along with the corresponding threshold. Details about this algorithm and its application for detecting distribution shifts are discussed in Appendix F.

### Coverage-Based Detection Algorithm

Our _coverage-based detection_ (CBD) algorithm applies SGC to \(C_{}\) target coverages uniformly spread between the interval \([0.1,1]\), excluding the coverage of 1. We set \(C_{}=10,=0.01\), and \(_{f}(x)=1-(x)\) for all our experiments; our method appears to be robust to those hyper-parameters, as we demonstrate in Appendix E. Each application \(j\) of SGC on the same sample \(S_{m}^{m}\) with a target coverage of \(c_{j}^{*}\) produces a pair: \((b_{j}^{*},_{j})\), which represent a bound and a threshold, respectively. We define \((|b^{*})\) to be a binary function that indicates a bound violation, where \(\) is the empirical coverage (of a sample) and \(b^{*}\) is a bound, \((|b^{*})=[ b^{*}]\). Thus, given a window of \(k\) samples from an alternative distribution, \(W_{k} Q^{k}\), we define the _sum of bound violations_, \(V\), as follows,

\[V=}}_{j=1}^{C_{}}\,(b_{j}^{*}-(_{j},W_{k}))((_{j},W_{k})|b_{j}^{*})= {k C_{}}_{j=1}^{C_{}}_{i=1}^{k}\,(b_{j }^{*}-g_{_{j}}(x_{i}))(_{j}|b_{j}^{*}),\] (4)

where we obtain the last equality by using \(_{j}(_{j},W_{k})=_{i=1}^{k}g_{ _{j}}(x_{i})\). Considering Figure 1(b) in Section 6.2, the quantity \(V\) is the sum of distances from the violations (red dots) to the linear diagonal representing the coverage bounds (black line).

```
1//Fit
2Input Training: \(S_{m}\), \(\), \(_{f}\), \(C_{}\)
3Generate \(C_{}\) uniformly spread overages \(^{*}\)
4for\(j=1\) to \(C_{}\)do
5\(b_{j}^{*},_{j}=g_{(S_{m},,c_{j}^{*},_{f})}\)
6endfor
7Output Training: \((b_{j}^{*},_{j})_{j=1}^{C_{}}\)
8//Detect
9Input Detection: \((\{b_{j}^{*},_{j}\})_{j=1}^{C_{}}\)
10whileTuro do
111 Receive window \(W_{k}=\{x_{1},x_{2},,x_{k}\}\)
12Calculate \(V\) [see Equation (41)]
13Obtain \(p\)-value from test; \(H_{0}:V=0,H_{1}:V>0\)
14\( p_{}\)
15Shift_detected = True
16Output Detection: Shift_detected_Pulte
17
18 endif
19
20 endwhile ```

**Algorithm 2**_Coverage-Based Detection_

When \(Q\) equals \(\), which implies that there is no distribution shift, we expect that all the bounds (computed by SGC over \(S_{m}\)) will hold over \(W_{k}\), namely \((_{j}|b_{j}^{*})=0\) for every iteration \(j\); in this case, \(V=0\). Otherwise, \(V\) will indicate the situation magnitude.

Since \(V\) represents the average of \(k C_{}\) values (if at least one bound violation occurs), for cases where \(k C_{} 30\) (as in all our experiments), we can assume that \(V\) follows a nearly normal distribution  and perform a t-test1 to test the null hypothesis, \(H_{0}:V=0\), against the alternative hypothesis, \(H_{1}:V>0\). The null hypothesis is rejected if the p-value is less than the specified significance level \(\). A pseudocode of our _coverage-based detection algorithm_ appears in Algorithm 2.

To fit our detector, we apply SGC (Algorithm 2.1). Our detection model utilizes these pairs to monitor a given model, receiving at each time instant a test sample window of size \(k\) (user defined), \(W_{k}=\{x_{1},x_{2},,x_{k}\}\), which is inspected to see if its content is distributionally fitted from the underlying distribution reflected by the detection-training set \(S_{m}\).

Our approach encodes all necessary information for detecting distribution shifts using only \(C_{}\) scalars (in our experiments, we set \(C_{}=10\)), which is independent of the size of \(S_{m}\). In contrast, the baselines process the detection-training set, \(S_{m}\), which is typically very large, for every detection they make. This makes our method significantly more efficient than the baselines, see Figure 1 and Table 1 in Section 5.

## 5 Complexity Analysis

This section provides a complexity analysis of our method as well as the baselines mentioned in Section 3. Table 1 summarizes the complexities of each approach. Figure 1 shows the population-based approaches run-time (in seconds) as a function of the detection-training set size, denoted as \(m\).

All population-based baselines are lazy learners (analogous to nearest neighbors) in the sense that they require the entire source (detection-training) set for each detection decision they make. Using only a subset will result in sub-optimal performance, since it might not capture all the necessary information within the source distribution, \(\).

In particular, MMD is a permutation test  that also employs a kernel. The complexity of kernel methods is dominated by the number of instances and, therefore, the time and space complexities of MMD are \((d(m^{2}+k^{2}+mk))\) and \((m^{2}+k^{2}+mk)\), respectively 2, where in the case of DNNs, \(d\) is the dimension of the embedding or softmax layer used for computing the kernel, and \(k\) is the window size. The KS test  is a univariate test, which is applied on each dimension separately and then aggregates the results via a Bonferroni correction. Its time and space complexities are \((d(m m+k k))\) and \((d(m+k))\), respectively.

The single-instance baselines simply conduct a t-test on the heuristic uncertainty estimators (SR or Entropy-based) between the detection-training set and the window data. By initially determining the mean and standard deviation of the detection-training set, we can efficiently perform the t-test with a time and space complexity of \((k)\).

The fitting procedure of our coverage-based detection algorithm incurs time and space complexities of \((m m)\) and \((m)\), respectively. Each subsequent detection round incurs time and space complexities of \((k)\), which is independent of the size of the detection-training (or source) set. Our method's superior efficiency is demonstrated both theoretically and empirically, as shown in Table 1 and Figure 1, respectively. Figure 1 shows the run-time (in seconds) for each population-based detection method3 as a function of the detection-training set size, using a ResNet50 and a fixed window size of \(k=10\) samples. Our detection time remains constant regardless of the size of the detection-training set, as opposed to the baseline methods, which exhibit a run-time that grows as a function of the detection-training set size. In particular, our method achieves a five orders of magnitude improvement in run-time over the best performing baseline, KS-Softmax, when the detection-training set size is 1,000,0004. Specifically, our detection time is \(5.19 10^{-4}\) seconds, while KS-Softmax's detection time is \(97.1\) seconds, rendering it inapplicable for real-world applications.

## 6 Experiments - Detecting Distribution Shifts

In this section we showcase the effectiveness of our method, along with the considered baselines, in detecting distribution shifts. All experiments were conducted using PyTorch, and the corresponding code is publicly available for replicating our results5.

**Detection Method** & **Space** & **Time** \\  
**MMD** & \((m^{2}+k^{2}+mk)\) & \((d(m^{2}+k^{2}+mk))\) \\
**KS** & \((d(m+k))\) & \((d(m m+k k))\) \\
**Single-instance** & \(()\) & \(()\) \\ 
**CBD** (Ours) & \(()\) & \(()\) \\   

Table 1: Complexity comparison. **bold** entries indicate the best detection complexity. \(m\), \(k\) refers to the detection-training size (or source size), and window size, respectively. \(d\) refers to the number of dimensions after dimensionality reduction.

### Setup

Our experiments are conducted on the ImageNet dataset , using its validation dataset as proxies for the source distribution, \(\). We utilized three well-known architectures, ResNet50 , MobileNetV3-S , and ViT-T , all of which are publicly available in timm's repository , as our pre-trained models. To train our detectors, we randomly split the ImageNet validation data (50,000) into two sets, a detection-training or source set, which is used to fit the detectors\({}^{3}\) (49,000) and a validation set (1,000) for applying the shift. To ensure the reliability of our results, we repeated the shift detection performance evaluation on 15 random splits, applying the same type of shift to different subsets of the data. Inspired by , we evaluated the models using various window sizes, \(|W_{k}|\{10,20,50,100,200,500,1000\}\).

#### 6.1.1 Distribution Shift Datasets

At test time, the 1,000 validation images obtained via our split can be viewed as the in-distribution (positive) examples. For out-of-distribution (negative) examples, we follow the common setting for detecting OOD samples or distribution shifts , and test on several different natural image datasets and synthetic noise datasets. More specifically, we investigate the following types of shifts:

\((1)\) **Adversarial via FGSM**: We transform samples into adversarial ones using the _Fast Gradient Sign Method_ (FGSM) , with \(\{7 10^{-5},1 10^{-4},3 10^{-4},5 10^{-4}\}\). (2) **Adversarial via PGD**: We convert samples into adversarial examples using _Projected Gradient Descent_ (PGD) , with \(=1 10^{-4}\); we use \(10\) steps, \(=1 10^{-4}\), and random initialization. (3) **Gaussian noise**: We corrupt test samples with Gaussian noise using standard deviations of \(\{0.1,0.3,0.5,1\}\). (4) **Rotation**: We apply image rotations, \(\{5^{},10^{},20^{},25^{}\}\). (5) **Zoom**: We corrupt test samples by applying zoom-out percentages of \(\{90\%,70\%,50\%\}\). (6) **ImageNet-O**: We use the ImageNet-O dataset , consisting of natural out-of-distribution samples. (7) **ImageNet-A**: We use the ImageNet-A dataset , consisting of natural adversarial samples. (8) **No-shift**: We include the no-shift case to check for false positives. To determine the severity of the distribution shift, we refer the reader to Table 3 in Appendix C.

### Maximizing Detection Power Through Lower Coverages

In this section, we provide an intuitive understanding of our proposed method, as well as a demonstration of the importance of considering lower coverages, when detecting population-based distribution shifts. For all experiments in this section, we employed a ResNet50  and used a detection-training set which was randomly sampled from the ImageNet validation set.

We validate the effectiveness of our CBD method (Algorithm 2) by demonstrating its performance on two distinct scenarios, the no-shift case and the shift case. In the no-shift scenario, as depicted in Figure 1(a), we randomly

Figure 1: Our method outperforms the baselines in terms of scalability, with a significant five orders of magnitude improvement in run-time compared to the best baseline, KS-Softmax, when using a detection-training set of \(m=1,000,000\) samples. One-\(\) error-bars are shadowed.

[MISSING_PAGE_FAIL:9]

[MISSING_PAGE_FAIL:10]