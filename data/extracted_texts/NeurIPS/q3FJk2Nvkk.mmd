# IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL

*Pascal Leroy1

*Pablo G. Morato2

*Johann Pisane3

*Athanasios Kolios2

*Damien Ernst1

*Authors contributed equally

###### Abstract

We introduce IMP-MARL, an open-source suite of multi-agent reinforcement learning (MARL) environments for large-scale Infrastructure Management Planning (IMP), offering a platform for benchmarking the scalability of cooperative MARL methods in real-world engineering applications. In IMP, a multi-component engineering system is subject to a risk of failure due to its components' damage condition. Specifically, each agent plans inspections and repairs for a specific system component, aiming to minimise maintenance costs while cooperating to minimise system failure risk. With IMP-MARL, we release several environments including one related to offshore wind structural systems, in an effort to meet today's needs to improve management strategies to support sustainable and reliable energy systems. Supported by IMP practical engineering environments featuring up to 100 agents, we conduct a benchmark campaign, where the scalability and performance of state-of-the-art cooperative MARL methods are compared against expert-based heuristic policies. The results reveal that centralised training with decentralised execution methods scale better with the number of agents than fully centralised or decentralised RL approaches, while also outperforming expert-based heuristic policies in most IMP environments. Based on our findings, we additionally outline remaining cooperation and scalability challenges that future MARL methods should still address. Through IMP-MARL, we encourage the implementation of new environments and the further development of MARL methods.

## 1 Introduction

Intelligent agents trained with reinforcement learning (RL) have proven successful in solving complex decision-making tasks, e.g., games , autonomous driving , human healthcare , nuclear fusion , among others. RL training approaches where multiple agents interact together are commonly denoted as multi-agent reinforcement learning (MARL) methods. In certain applications, these agents must cooperate to accomplish a common goal, leading to the special case of cooperative MARL. To support the advancement of cooperative MARL methods, multiple environments based on games and simulators have served as benchmark testbeds, e.g., the particle environment , StarCraft Multi-Agent Challenge (SMAC) , and MaMuJoCo . Benchmarking on environments based on games and simulators is useful for the development of MARL methods in specific collaborative/competitive tasks, but additional challenges may still be encountered when deploying MARL methods in real-world applications .

Infrastructure Management Planning (IMP) is a contemporary application that responds to current societal and environmental concerns. In IMP, inspections, repairs, and/or retrofits should be timely planned in order to control the risk of potential system failures, e.g., bridge and wind turbine failures,among many others . Formally, the system failure risk is defined as the system failure probability multiplied by the consequences associated with a failure event, typically defined in monetary units. Due to model and measurement uncertainties, the components' damage is not perfectly known, and decisions are made based on a probability distribution over the damage condition, henceforth denoted as damage probability. The system failure probability is defined as a function of components' damage probabilities. Starting from its initial damage distribution, each component's damage probability transitions according to a deterioration stochastic process, but also according to the decisions made . Naturally, the damage probability transitions based on its deterioration model when the component is neither inspected nor repaired, i.e., do-nothing action. If a component is inspected, its damage probability is updated with respect to the inspection outcome. When a component is repaired, its damage condition is directly improved and the damage probability resets to its initial damage distribution. A schematic of a typical IMP problem is shown in Figure 1.

In an effort to generate more efficient strategies for managing engineering systems through the application of cooperative MARL methods, we introduce IMP-MARL, a novel open-source suite of multi-agent environments. In IMP-MARL, each agent is responsible for managing one constituent component in a system, making decisions based on the damage probability of the component. Besides seeking to reduce component inspection and maintenance costs, agents should effectively cooperate to minimise the system failure risk. With IMP-MARL, our goal is to facilitate the definition and implementation of new customisable environments. By jointly minimising system failure risks and inspection/maintenance costs, more effective IMP policies contribute to a better allocation of resources from a societal perspective. Furthermore, additional societal impact is also made by controlling the risk of system failure events. For example, the failure of a wind turbine may affect the available electricity production. Beyond economic considerations, our proposed IMP-MARL framework can also be used to include sustainability and societal metrics within the objective function by accounting for those directly in the reward model.

To assess the capability of cooperative MARL methods for generating effective policies for IMP problems involving many components, we additionally benchmark here state-of-the-art cooperative MARL methods in terms of scalability and optimality. Most of the benchmarked methods are centralised training with decentralised execution (CTDE) methods [14; 15], in which each agent

Figure 1: Overarching representation of an infrastructure management planning (IMP) problem. The system failure risk is defined as a function of the probability distribution over the components’ damage condition. To control the system failure risk, components can be inspected or repaired at each time step \(t\) and, typically, an agent controls one component. The objective of IMP’s problem is the maximisation of the expected sum of discounted rewards, by balancing the system failure risk \(R_{f}\) against inspections \(R_{ins}\) and repairs \(R_{rep}\), all three being negative rewards. Here, we show three components with the same damage probability at time step \(t\). When a component is not inspected nor repaired, its damage probability evolves according to a deterioration process. If a component is inspected, information from the inspection is also considered when updating the damage probability, and if a component is repaired, the damage probability resets to its initial damage distribution.

acts based on only local information, while global information can be utilised during training. Specifically, we benchmark five CTDE methods: QMIX , QVMix , QPLEX , COMA , and FACMAC , along with a decentralised method, i.e., IQL , and a centralised one, i.e., DQN . All tested MARL methods are compared against expert-based heuristic policies, which can be categorised as a state-of-the-art method to deal with IMP problems in the reliability engineering community [13; 21]. In our study, three sets of IMP environments are investigated, including one related to offshore wind structural systems, where MARL methods are tested with up to 100 agents. Additionally, these environments can be set up with two distinct reward models, and one of them incorporates explicit cooperative objectives. For the sake of enabling the reproduction of any published result, we have made our best effort to ensure that the necessary code is publicly available.

Our contributions can be outlined as follows:

* We introduce IMP-MARL, a novel open-source suite of environments, motivating the development of scalable MARL methods as well as the creation of new IMP environments, enabling the effective management of multi-component engineering systems and, as such, leading to a positive societal impact.
* In an extensive benchmark campaign, we test state-of-the-art cooperative MARL methods in very high-dimensional IMP environments featuring up to 100 agents. The resulting management strategies are evaluated against expert-based heuristic policies. We publicly provide the source code for reproducing our reported results and for easing direct comparisons with future developments.
* Based on our results, we draw relevant insights for both machine learning and reliability engineering communities, further highlighting important challenges that must still be resolved. While cooperative MARL methods can learn superior strategies compared to expert-based heuristic policies, the relative performance benefit decreases in environments with over 50 agents. In certain environments, cooperative MARL policies are characterised by a high variance and sometimes underperform expert-based heuristic policies, suggesting the need for further research efforts.

## 2 Related work

**MARL environments** Cooperative MARL has a long-standing history, and decentralised approaches such as IQL were already originally proposed in 1993 . There has been a recent interest in the development of CTDE methods [14; 15] (see Section 4.1), inducing the creation of new environments with cooperative tasks. With continuous action spaces, popular environments include the particle environment , a suite of communication oriented environments with cooperative scenarios, and MaMuJoCo , which aims at factorising the decision of MuJoCo , a physics-based simulator. In contrast, the StarCraft multi-agent challenge (SMAC)  and its upgraded version SMACv2  are probably the most studied environments with discrete action spaces. SMAC is based on the StarCraft II Learning Environment  with a suite of micro-management challenges where each game unit is an independent agent. Other cooperative environments based on game simulators include the Hanabi Challenge , a "cooperative solitaire" between two and five players, and Google Research Football , a football game simulator. Cooperative MARL methods are mostly benchmarked on these games and simulators, but also on real-world applications: target coverage control (MATE) , train scheduling (Flatland-RL) , traffic control (CityFlow) , multi-robot warehouse (RWARE) . Oroojlooy and Hajinezhad  provide a review of cooperative MARL, including a more detailed list of applications. IMP-MARL introduces two key advancements: support for environments with up to 100 agents and seamless creation of diverse IMP environments. This enables the utilisation of RL in real-world scenarios, ranging from complex factories with heterogeneous components to offshore wind farms with multiple homogeneous components.

**Infrastructure management planning methods** Recent heuristic-based inspection and maintenance (I&M) planning methods generate IMP policies based on an optimised set of predefined decision rules [21; 31]. By evaluating only a set of decision rules out of the entire policy space, the previously mentioned approaches might yield suboptimal policies . In the literature, one can also find POMDP-based methods applied to the I&M planning of engineering components, in most cases, relying on efficient point-based solvers [13; 32; 33]. When dealing with multi-component engineering systems, solving point-based POMDPs becomes computationally complex. In that case, the policy and value function can be approximated by neural networks, enabling the treatment of high-dimensional engineering systems. Specifically, actor-critic, and value function-based methods have been proposed in the literature for the management of engineering systems [34; 35; 36] with some of them relying on CTDE methods [37; 38]. Note that no open-source methods nor publicly available environments are provided in the above-mentioned references. This emphasises the importance of our efforts to enhance comparison and reproducibility within the reliability engineering community.

## 3 IMP-MARL: A suite of Infrastructure Management Planning environments

In IMP, the damage condition of multiple components deteriorates stochastically over time, inducing a system failure risk that is penalised at each time step. To control the system failure risk, components can be inspected or repaired, yet, incurring additional costs. The objective is the minimisation of the expected sum of discounted costs, including inspections, repairs, and system failure risk. This can be achieved through the agents' cooperative behaviour, assigning component inspections and repairs while jointly controlling the system failure risk. The introduced IMP decision-making problem can be modelled as a decentralised partially observable Markov decision process (Dec-POMDP).

### Preliminaries

A Dec-POMDP  can be defined by a tuple \([,,,n,O,R,P,]\), where \(n\) agents simultaneously choose an action at every time step \(t\). The state of the environment is \(s_{t}\) where \(\) is the set of states. The observation function \(O:\{1,..,n\}\) maps the state to an observation \(o_{t}^{a}\) perceived by agent \(a\) at time \(t\), where \(\) is the observation space. Each agent \(a\{1,..,n\}\) selects an action \(u_{t}^{a}_{a}\), and the joint action space is \(=_{1}.._{n}\). After the joint action \(}\) is executed, the transition function determines the new state with probability \(P(s_{t+1}|s_{t},}):^{2}^{+}\), and \(r_{t}=R(s_{t+1},s_{t},}):^{2} \) is the team reward obtained by all agents. An agent's policy is a function \(^{a}(u_{t}^{a}|_{t}^{a},o_{t}^{a}):(_{a})^ {t}^{+}\), which maps its history \(_{t}^{a}(_{a})^{t-1}\) and its observation \(o_{t}^{a}\) to the probability of taking action \(u_{t}^{a}\). The joint policy is denoted by \(=(^{1},..,^{n})\). The cumulative discounted reward obtained from time step \(t\) over the next \(T\) time steps is defined by \(R_{t}=_{k=0}^{T-1}^{k}r_{t+k}\) and \([0,1)\) is the discount factor. The goal of agents is to find the optimal joint policy that maximises the expected \(R_{t}\) during the entire episode: \(^{}=*{argmax}_{}[R_{0}|]\).

### Environments formulation

**States and observations** As introduced, each agent in IMP perceives \(o_{t}^{a}\), an observation corresponding to its respective component damage probability and the current time step. Each component damage probability transitions based on a deterioration model, defined according to physics-based engineering models, e.g., numerical simulators and/or analytical laws . The damage probability is also updated based on maintenance decisions, as explained in Section 1. Since the components' damage is not perfectly known, the state of the Dec-POMDP is defined as the collection of all components' damage probabilities along with the current time step: \(s_{t}=(o_{t}^{1},..,o_{t}^{n},t)\).

**Actions and rewards** Each agent controls a component and collaborates with other agents in order to minimise the system failure risk while minimising local costs associated with individual repair and/or inspection actions. At each time step \(t\), an agent decides \(u_{t}^{a}\) between (i) do-nothing, (ii) inspect, or (iii) repair actions, as described in Section 1. Both inspection and repair actions incur significant costs, formally included in the Dec-POMDP framework as negative rewards, \(R_{ins}\) and \(R_{rep}\), respectively. Moreover, the system failure risk is defined as \(R_{f}=c_{F} p_{F_{sys}}\) where \(p_{F_{sys}}\) is the system failure probability and \(c_{F}\) is the associated consequences of a failure event, encompassing economic, environmental, and societal losses. In IMP, we include two reward models. The first is a _campaign cost_ model where a global cost, \(R_{camp}\), is incurred if at least one component is inspected or repaired, plus a surplus, \(R_{ins} R_{rep}\), per inspected/repaired component. This campaign cost explicitly incentivises agents to cooperate. The second is a _no campaign cost_ model, where the campaign cost is set equal to 0 (i.e., \(R_{camp}=0\)), and only component inspections and repairs costs are considered. Acting on finite-horizon episodes that span over \(T\) time steps, all agents aim at maximising the expected sum of discounted rewards \([R_{0}]=[_{t=0}^{T-1}^{t}[R_{t,f}+_{a=1 }^{n}(R_{t,ins}^{a}+R_{t,rep}^{a})+R_{t,camp}]]\).

**Real-world data** While IMP policies are trained based on simulated data, they policies can then be deployed to applications where real-world streams of data are available. In that case, the damage condition of the components is updated based on collected real-world data, e.g., inspections.

### IMP-MARL environments

With IMP-MARL, we provide three sets of environments to benchmark cooperative MARL methods. For all three, components are exposed to fatigue deterioration during a finite-horizon episode, inducing the growth of a crack over \(T\) time steps. The first set of environments is named _k-out-of-n system_ and refers to systems for which a system failure occurs if (n-k+1) components fail. Those systems have been widely studied in the reliability engineering community . The second type of generic environment is named _correlated k-out-of-n system_ and is a variation of the first one for which the initial components' damage distributions are correlated. The last one is named _offshore wind farm_ and allows the definition of environments for which a group of offshore wind turbines must be maintained. The proposed IMP-MARL environment sets and options are graphically illustrated in Figure 2, and we hereafter provide details about these sets of environments. Additionally, the deterioration processes and implementation details are formally described in Appendices B and C.

**k-out-of-n system** In this set of environments, the components' damage probability distribution, \(p(d_{t}^{a})\), is defined as a vector of 30 bins, with each bin representing a crack size interval. Here, the failure probability of one component is defined as the probability indicated in the last bin. The specificity of a k-out-of-n system is that it fails if (n-k+1) components fail, establishing a direct link between the system failure probability and the component failure probabilities. For this first system, the initial damage distribution is statistically independent among components and the time horizon is \(T=30\) time steps. Since it is finite, we normalise each time step input and we define \(s_{t}=(p(d_{t}^{1}),...,p(d_{t}^{n}),t/T)\) and \(o_{t}^{a}=(p(d_{t}^{a}),t/T)\). The interest of this system is that, in many practical scenarios, the reliability of an engineering system can be modelled as a _k-out-of-n system_.

**Correlated k-out-of-n system** The second set of environments is the same as the first one previously defined, with the difference that the initial damage distribution is correlated among all components. Therefore, inspecting one component also provides information about other uninspected components, depending on the specified degree of correlation. This setting is particularly challenging when approached from a decentralised scheme without providing components' correlation information to individual agents. To further address this issue, and in addition to their 30-bin local damage probability, the agents perceive correlation information \(_{t}\) common to all, which is updated based on inspection outcomes collected from all components. We thus have: \(s_{t}=(p(d_{t}^{1}),...,p(d_{t}^{n}),_{t},t/T)\)

Figure 2: Visual representation of available IMP-MARL environment sets and options. In 2a, a 4-out-of-5 system fails if 2 or more components fail. In 2b, a wind turbine fails if any constituent component fails. In 2c, when the environment is under deterioration correlation, the information collected by inspecting one component also influences uninspected components. In 2d campaign cost environments, a global cost is incurred if any component is inspected and/or repaired plus a surplus per inspected/repaired component.

and \(o_{t}^{a}=(p(d_{t}^{a}),_{t},t/T)\). This damage correlation structure is inspired by practical engineering applications where initial defects among components are statistically correlated due to the fact that components undergo similar manufacturing processes .

**Offshore wind farm** The third set of environments is different from the previous ones as it considers a system with a certain number of wind turbines. Specifically, each wind turbine contains three representative components: (i) the top component located in the atmospheric zone, (ii) the middle component in the underwater zone, and (iii) the mudline component submerged under the seabed. In this case, we consider that the mudline component cannot be inspected nor repaired, as it is installed under the seabed in an inaccessible region and, since only the top and middle components can be inspected or repaired, two agents are assigned for each wind turbine. Furthermore, the damage probability, \(p(d_{t}^{a})\), is a vector with 60 bins and transitions differently depending on the component location in the wind turbine, as corrosion-induced effects accelerate deterioration in certain areas. Besides individual component damage models, inspection techniques and their associated costs also depend on the component location: it is cheaper to inspect or repair the top components than the middle one . Moreover, while the mudline component cannot be directly maintained, its damage probability also impacts the failure risk of a wind turbine. In offshore wind farm environments, a wind turbine fails if any of its three constituent components fails, and the overall system failure risk is defined as the sum of all individual wind turbine failure risks. In this case, \(p(d_{t}^{a})\) is modelled as a 60-bin vector and the time horizon is \(T=20\). In this set of environments \(s_{t}=(p(d_{t}^{a}),...,p(d_{t}^{a}),t/T)\) and \(o_{t}^{a}=(p(d_{t}^{a}),t/T)\).

**Implementation** All defined IMP environments are integrated with well-known MARL ecosystems, i.e., Gym , Gymnasium , PettingZoo  and PyMarl , through wrappers. The tested MARL methods are adopted from PyMarl's library, but other libraries are also compatible with our wrappers, e.g., RLlib , CleanRL , MARLIib , or TorchRL . All developments are available on a public GitHub repository, https://github.com/moratodpg/imp_marl, featuring an open-source Apache v2 license.

## 4 Benchmark campaign of MARL methods

### Tested methods

In an extensive benchmark campaign, we test seven RL methods: one fully centralised, one fully decentralised, and five CTDE approaches. The centralised controller, which has an action space that scales exponentially with the number of agents, is trained with the fully centralised method DQN  and is the only method taking \(s_{t}\) as input. Furthermore, the fully decentralised method we test is IQL , in which all agents are independently trained. Regarding the five CTDE methods, we investigate three value-based methods: QMIX , QVMix , and QPLEX . They factorise the value function during training, allowing agents to independently select actions during execution after a centralised value function is jointly learnt during training. The last two are the CTDE actor-critic methods COMA  and FACMAC . They train independent policy networks and rely on a single centralised critic during training. Appendix D provides a detailed description of each method as well as a discussion of other methods of interest that are not included in the benchmark. We selected these methods for our benchmark study because they are well established and their implementations are open-sourced and available within the PyMarl framework .

All investigated MARL methods are compared against a representative baseline in the reliability engineering community [21; 36]. This baseline, referred to as expert-based heuristic policy, consists of a set of heuristic decision rules that are defined based on expert knowledge. The heuristic policy includes both parametric and non-parametric rules. Parametric decision rules depend on two parameters: (i) the inspection interval and (ii) the number of inspected components. On the other hand, non-parametric rules involve taking a repair action after detecting a crack and prioritising component inspections with higher failure probability. To determine the best heuristic policy, and for each environment, we evaluate all parametric rule combinations over 500 policy realisations, thereby identifying the heuristic policy that maximises the expected sum of discounted rewards among all policies evaluated.

### Experimental setup

The above-mentioned seven MARL methods are tested in the three sets of IMP environments defined in Section 3.3. The environments differ by the number of agents and by whether or not they include a campaign cost model. The numbers of agents tested in the six types of environments are presented in Table 1. To objectively interpret the variance associated with the examined MARL methods, 10 training realisations with different seeds are executed in each environment. As explained in Section 3, an agent makes decisions based on its local damage probability, the current normalised time step, and sometimes correlation information is additionally provided; while the state, used by DQN and CTDE methods, encompasses all of the information combined. In all cases, the action space features three possible discrete actions per agent, except for DQN, where the centralised controller selects an action among the \(3^{n}\) possible combinations. For complexity reasons, we only test DQN in k-out-of-n environments featuring 3 and 5 components, as well as in environments with 1 and 2 wind turbines. Detailed information on rewards, observations, and states can be found in Appendix C.

Given the importance of hyperparameters on the performance of RL methods , we initially selected their values reported by the original authors. In an attempt to objectively compare the examined methods, parameters that play the same role across methods are equal. Notably, the learning rate and gamma, among others, are identical in all experiments. The controller agent network features the same architecture in all methods, consisting of a single GRU layer with a hidden state composed of 64 features encapsulated between fully-connected layers and three outputs, one per action, except for DQN, where the network output includes \(3^{n}\) actions. In our case, DQN's architecture includes additional fully-connected layers and a larger size of hidden GRU states. Moreover, following common practice, agent networks are shared among agents, and thus a single agent network is trained. Specifically, we train only one network that is used for all agents, instead of training \(n\) distinct agent networks. The training process with a single agent network improves data efficiency because the same episode can be used to perform \(n\) backpropagations through the same agent network, using \(n\) different observations. In contrast, only one backpropagation per agent network would be possible with a single episode if training is performed with \(n\) different agent networks. To allow diversity in agents' behaviour, a one-hot encoded vector is also added to the input of this common network to indicate which one of the \(n\) agents is making the decision. In CTDE methods, critics or mixers are also incorporated at the training stage with specific architectures according to each method and environment configuration. In most cases, the neural networks are updated after each played episode based on 64 episodes sampled from the replay buffer, which contains the latest 2,000 episodes. The only exception is COMA, which follows an on-policy approach, where the network parameters are updated every four episodes. For value-based methods, the training episodes are played following an epsilon greedy policy, whereas test episodes are executed with a greedy policy. The epsilon value is initially specified as 1 and linearly decreases to 0.05 after 5,000 time steps. This is different for COMA and FACMAC. Appendix E and the source code list more details and all parameters.

The number of time steps allocated for one training realisation is 2 million time steps for all methods. These 2 million training time steps are executed with training policies, e.g. epsilon greedy policy, saving the networks every 20,000 training time steps. To evaluate them, we execute 10,000 test episodes and obtain the average sum of discounted rewards per episode per saved network. These test episodes are executed with testing policies, e.g. greedy policy. We show in Appendix E.3 that 10,000 test episodes are needed due to the variance induced in the implemented environments. We emphasise that 10 training realisations are executed with different seeds for the same parameter values. All parameters are listed in Appendix E and in the source code.

   IMP environments &  \\  k-out-of-n system & 3 & 5 & 10 & 50 & 100 \\ Correlated k-out-of-n system & 3 & 5 & 10 & 50 & 100 \\ Offshore wind farm & 2 & 4 & 10 & 50 & 100 \\   

Table 1: Number of agents specified in all investigated IMP environments.

Benchmark results and discussion

The results from the benchmark campaign are presented in Figure 3, showcasing the relative performance of MARL methods with respect to expert-based heuristic policies in terms of their expected sum of discounted rewards. In each boxplot of Figure 3, each of the 10 seeds is represented by its best policy, which achieved the highest average sum of discounted rewards during evaluation. We further explain the connection between learning curves and boxplots in Appendix F, Figure 10. Our analysis relies on relative performance metrics because the optimal policies are not available in the environments investigated. Additionally, the corresponding learning curves and the best-performing policy realisations can be found in Appendix F.

**MARL-based strategies outperform expert-based heuristic policies.** While heuristic policies provide reasonable infrastructure management planning policies, the majority of the tested MARL methods yield substantially higher expected sum of discounted rewards, yet the variance over identical MARL experiments is still sometimes significant. In environments with no campaign cost, the performance achieved by MARL methods with respect to the baseline differs in configurations with a high number of agents, as shown at the top of Figure 3. In contrast, MARL methods reach better relative results in environments with a high number of agents when the campaign cost model is adopted, as illustrated at the bottom of Figure 3. In general, the superiority of MARL methods with respect to expert-based heuristic policies is justified by the complexity of defining decision rules in high-dimensional multi-component engineering systems, where the sequence of optimal actions is very hard to predict based on engineering judgment .

**IMP challenges.** In correlated k-out-of-n IMP environments, the variance over identical MARL experiments is higher than in the uncorrelated ones, emphasising a specific IMP challenge. Under correlation, inspecting one component also provides information to uninspected components, impacting their damage probability and thus hindering cooperation between MARL agents. Another challenge is imposed in offshore wind farm environments, where the benefits achieved by MARL methods with respect to the baseline are also reduced in environments with a high number of agents. This can be explained by the fact that each wind turbine is controlled by two agents, being independent of other turbines in terms of rewards. Each agent must then cooperate closely with only one of all agents, hence complicating global cooperation in environments featuring an increasing number of agents.

**Campaign cost environments.** Yet another challenge can be observed in campaign cost environments under 50 agents, where MARL methods' superior performance with respect to heuristic policies is more limited. The aforementioned environments are challenging for MARL methods because agents should cooperate in order to group component inspection/repair actions together, saving global campaign costs. In addition, the heuristic policies are designed to automatically schedule group inspections, being favourable in this case. This is confirmed by the learning curves presented in Figures 11 and 12. On the other hand, in environments with more than 50 agents, MARL methods substantially outperform heuristic policies. At least one component is inspected or repaired at each time step and the results reflect that avoiding global annual campaign costs becomes less crucial.

**Centralised RL methods do not scale with the number of agents.** DQN reaches better results than heuristic policies, though achieving lower rewards than CTDE methods in most environments, despite benefiting from larger networks during execution. This highlights the scalability limitations of such centralised methods, mainly due to the fact that they select one action out of each possible combination of component actions.

**IMP demands cooperation among agents.** The results reveal that CTDE methods clearly outperform IQL in all tested environments, especially those with a high number of agents. This confirms that realistic infrastructure management planning problems demand coordination among component agents. Providing only independent local feedback to each IQL agent during training leads to a lack of coordination in cooperative environments, also shown by Rashid et al. . However, the performance may be improved by enhancing networks' representation capabilities by including more neurons, yet this is true for all investigated methods.

**Infrastructure management planning via CTDE methods.** Overall, CTDE methods generate more effective IMP policies than the other investigated methods, demonstrating their capabilities for supporting decisions in real-world engineering scenarios. While Figure 3 presents the variance of the best results across runs, the learning curves further confirm this finding in Appendix F. In particular, QMIX and QVMIX generally learn effective policies with low variability over runs. Slightly more Figure 3: Performance reached by MARL methods in terms of normalised discounted rewards with respect to expert-based heuristic policies in all IMP environments, H referring to the heuristics result. Every boxplot gathers the best policies from each of 10 executed training realisations, indicating the 25th-75th percentile range, median, minimum, and maximum obtained results. The coloured boxplots are grouped per method, vertically arranging environments with an increasing number of \(n\) agents, as indicated in the top-left legend boxes. Note that the results are clipped at -100%.

unstable, QPLEX also yields similar results to QMIX and QVMIX in terms of achieved results. While being able to outperform heuristic policies in almost every environment, FACMAC exhibits a high variance among runs. However, FACMAC effectively scales up with the number of agents and environment complexity (as reported in ), achieving some of the best results in IMP environments with over 50 agents as well as in correlated IMP environments. The results also suggest that COMA is the least scalable MARL method in our benchmark. This can be attributed to the fact that the computation of the critic's counterfactual becomes challenging with an increasing number of agents.

## 6 Conclusions

This work offers an open-source suite of environments for testing scalable cooperative multi-agent reinforcement learning methods toward the efficient generation of infrastructure management planning (IMP) policies. Through our publicly available code repository, we also encourage the implementation of additional IMP environments, e.g., bridges, transportation networks, pipelines, and other relevant engineering systems, whereby specific disciplinary challenges can be identified in a common simulation framework. Based on the reported benchmark results, we can conclude that centralised training with decentralised execution methods are able to generate very effective infrastructure management policies in real-world engineering scenarios. While the results reveal that MARL methods outperform expert-based heuristic policies, additional research efforts should still be devoted to the development of scalable cooperative MARL methods. While we model the IMP decision-making problem as a Dec-POMDP, modelling IMP problems as mean-field games  is a promising direction to be considered in environments with an increasing number of agents. Moreover, specific improvements are still required in environments where a global cost is triggered from the actions taken by any local agent, e.g., global campaign cost. Besides, more stable training is still needed in environments where local information perceived by one agent can influence the damage condition probabilities of others, as in the correlated IMP environments. In the future, more realistic and challenging environments for cooperative MARL methods could be investigated. One example would be assigning campaign costs to specific groups of components, instead of specifying only one global campaign cost.