# DeTixZify: Synthesizing Graphics Programs

for Scientific Figures and Sketches with Ti_kZ_

 Jonas Belouadi\({}^{*}\) &Simone Paolo Ponzetto\({}^{}\) &Steffen Eger\({}^{}\)

Natural Language Learning Group;\({}^{,}\) Data and Web Science Group\({}^{}\)

University of Mannheim;\({}^{,}\) University of Technology Nuremberg\({}^{}\)

{jonas.belouadi,ponzetto}@uni-mannheim.de, stefen.eger@utn.de

###### Abstract

Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce DeTixZify, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving Ti_kZ_ graphics programs based on sketches and existing figures. To achieve this, we create three new datasets: DaTixZ_v2, the largest Ti_kZ_ dataset to date, containing over 360k human-created Ti_kZ_ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and MetaFig, a collection of diverse scientific figures and associated metadata. We train DeTixZify on MetaFig and DaTixZ_v2, along with synthetically generated sketches learned from SketchFig. We also introduce an MCTS-based inference algorithm that enables DeTixZify to iteratively refine its outputs without the need for additional training. Through both automatic and human evaluation, we demonstrate that DeTixZify outperforms commercial Claude 3 and GPT-4V in synthesizing Ti_kZ_ programs, with the MCTS algorithm effectively boosting its performance. We make our code, models, and datasets publicly available.1

## 1 Introduction

Creating high-quality scientific figures is similar to typesetting scientific documents in many ways. When it comes to typesetting, markup languages like LaTeX enjoy widespread popularity, as exemplified by major machine learning conferences that either mandate or strongly encourage LaTeX-formatted submissions.2 The advantages of using such languages go beyond producing high-quality outputs; documents expressed as high-level, semantics-preserving programs enhance accessibility, serve archival purposes, and remain easily editable and human-readable (facilitating language modeling applications; Moosavi et al., 2021; Lu et al., 2023). Consequently, efforts have been made to recover this type of information from outputs stored in lower-level vector graphics formats like PDF or SVG, or raster graphics formats (Desai et al., 2021; Blecher et al., 2024). At the other end of the spectrum, the versatility of LaTeX comes with a steep learning curve, and typesetting can often be challenging for end users. In response, researchers have been working on assisting authors with certain aspects of the problem, such as typesetting math based on hand-drawn sketches (Kirsch, 2010; Wu et al., 2020).

Just like documents, scientific figures can also be created using markup languages. A popular example is the Ti_kZ_ graphics language (Tantau, 2023), which can be integrated into LaTeX documents, providing comparable benefits and encountering similar challenges. However, unlike LaTeX, the prospects of Ti_kZ_ in research contexts remain largely unexplored. Although the promise of simplifying editing andenabling applications in visual understanding (Masry et al., 2022; Huang et al., 2023) is evident, there are currently no viable solutions for recovering graphics programs from compiled figures. Moreover, there is a lack of tools that assist in creating graphics programs, e.g., based on hand-drawn sketches, despite the clear demand for such approaches on the Texas Stack Exchange (TexX.SE),3 where nearly 10% of all questions revolve around TixZ, making it the most frequently discussed topic on the site. Addressing this gap could greatly improve the accessibility of existing figures and support researchers at all levels of programming proficiency when creating new ones, fostering diversity and inclusion. In response, we introduce DeTixZify, a multimodal language model that automatically synthesizes TixZ programs for scientific figures and sketches (cf. Figure 1). Our key contributions are as follows:

1. As part of DeTixZify, we introduce (a) DaTixZv2, a large TixZ dataset with over 360k human-created TixZ graphics; (b) SketchFig, a dataset of human-created sketches with paired scientific figures; and (c) MetaFig, a large meta-dataset of scientific figures and associated texts.
2. We train DeTixZify on MetaFig and DaTixZv2, augmented with synthetic sketches that mimic SketchFig. We demonstrate that DeTixZify can effectively synthesize TixZ programs for both existing scientific figures and sketches, outperforming the commercial large language models (LLMs) GPT-4V and Claude 3 (OpenAI, 2023b; Anthropic, 2024).
3. We also present an inference algorithm based on Monte Carlo Tree Search (MCTS) that is tailored to graphics programs and allows DeTixZify to iteratively refine _its own outputs_ for a given computational budget, further improving performance without additional training.

## 2 Related Work

Image-to-DeTix ConversionA closely related task is the translation of mathematical illustrations into LaTeX markup. In inspirational work, Kirsch (2010) tackle the recognition of single hand-drawn symbols to find corresponding LaTeX commands. Subsequent works by Deng et al. (2017); Zhang et al. (2017, 2019); Wu et al. (2020); Wang and Liu (2021) expand on this concept to handle hand-drawn and scanned math formulas. Suzuki et al. (2003); Wang and Liu (2020); Blecher et al. (2024); Lv et al. (2023) further extend the scope by extracting LaTeX formulas alongside text from entire documents.

Image VectorizationSimilarly, converting (rasterized) figures into TixZ programs can be characterized as a form of image vectorization (Sun et al., 2007; Diebel, 2008; Ganin et al., 2018; Li et al., 2020; Ma et al., 2022; Zhu et al., 2024). Most existing methods vectorize images into low-level graphics primitives in the SVG format (Tian and Gunther, 2024). Although this works well for specific domains like fonts, icons, and emoji (Lopes et al., 2019; Carlier et al., 2020; Reddy, 2021; Rodriguez et al., 2023b), it does not capture higher-level semantics and does not generalize well to our scientific context (cf. Appendix B). Closer to our work, Ellis et al. (2018) generate vector representations as graphics programs based on a limited subset of LaTeX commands. Their approach even handles

Figure 1: Overview of the DeTixZify architecture: A multimodal language model converts sketches or figures into TixZ programs, which are compiled by a LaTeX engine. This provides a reward signal to the model via MCTS, allowing it to iteratively refine the output until satisfactory results are achieved.

sketches, but their experiments are restricted to a synthetic dataset with only basic shapes of limited complexity. Belouadi et al. (2024) also generate TiKZ programs, but their primary emphasis is on conditioning the generation on textual descriptions, with images serving only as a secondary input.

Code GenerationAs TiKZ is implemented in the Turing-complete TeX macro system (Erdweg and Ostermann, 2011), our work is also closely tied to code generation (Xu et al., 2022). Despite continuing progress in this field (Chen et al., 2021; Li et al., 2022, 2023; Guo et al., 2024; Lozhkov et al., 2024), most research concentrates on high-resource languages like Python, Java, and JavaScript (Zan et al., 2023), typically overlooking TeX in evaluations. However, TeX and TiKZ may still find their way into the training data, as demonstrated by the zero-shot ability of some models to understand and generate code in these languages (Bubeck et al., 2023; Belouadi et al., 2024; Sharma et al., 2024).

## 3 Datasets

We introduce DaTixZ\({}_{ 2}\), to our knowledge, the most comprehensive dataset of TiKZ graphics to date; SketchFig, the first dataset comprising human-created sketches of scientific figures; and MetaFig, a large-scale scientific figure dataset with rich metadata. See Appendix E for examples.

DaTixZ\({}_{ 2}\)DaTixZ\({}_{ 2}\) serves as the primary source of TiKZ graphics for training DeTixZ\({}_{}\). It is an expanded version of DaTixZ\({}_{ 1}\)(Belouadi et al., 2024), incorporating graphics from the same sources, namely curated repositories, TeX.SE, arXiv papers, and artificial examples. The key difference is that DaTixZ\({}_{ 2}\) includes all TiKZ programs that compile with TeX Live 2023,4 regardless of whether they have associated captions, which was a requirement for inclusion in DaTixZ\({}_{ 1}\) but is not needed for DeTixZ\({}_{}\). This approach allows us to create a dataset that is more than three times as large as its predecessor (cf. Table 1).

SketchFigTo create realistic synthetic sketches of scientific figures in DaTixZ\({}_{ 2}\), we rely on examples of real human-created sketches. TeX.SE is a suitable source for collecting these, as users often illustrate their questions with sketches, and the answers provide the desired figure. We semi-automatically extract these figure-sketch pairs by first ranking all questions on the site that contain images based on their similarity to the string "a sketch of a scientific figure" using a multimodal vision encoder (Zhai et al., 2023). We retain the ones with high similarity scores, manually filter for true positives, and align them with the best matching figure provided in the answers. In total, we collect 549 figure-sketch pairs this way. As we also want to use this dataset for evaluation (cf. SS6), we ensure that for a subset of these sketches, no code provided in the answers is included in DaTixZ\({}_{ 2}\).

MetaFigBeyond TiKZ graphics, there is a much larger pool of figures where the underlying source is not available. Existing datasets that collect such figures frequently come with rich metadata, such as captions, OCR tokens, and paragraphs that mention the figures (Hsu et al., 2021; Karishima et al., 2023; Rodriguez et al., 2023a). Since such high-level descriptions are useful for pretraining (cf. SS4; Liu et al., 2023b), we collect these datasets and merge them with the subset of figures in DaTixZ\({}_{ 2}\) that have captions. This results in over 734k figure-text pairs, more than twice the size of DaTixZ\({}_{ 2}\).

## 4 The DeTixZify Model

Building on previous work (Liu et al., 2023b, a; Dai et al., 2023; McKinzie et al., 2024), we build DeTixZify by combining a pretrained vision encoder with a pretrained language model (cf. Figure 1), where the vision encoder receives figures or sketches as input images, and the language model generates corresponding TiKZ programs as output. We focus on code language models that have been pretrained on TeX, as this prior knowledge may be helpful for our task. All the models we end up using follow the LLAMA architecture (Touvron et al., 2023): CodeLLAMA (Roziere et al., 2023) has likely been trained on TeX code from arXiv (Touvron et al., 2023), as has been TinyLLAMA (Zhang et al.,2024), while DeepSeek (code variant; Guo et al., 2024) was trained on TeX code from GitHub. For the vision encoder, we use SigLIP (Zhai et al., 2023), which has been trained on OCR annotations (Chen et al., 2023c) and demonstrates state-of-the-art understanding of text-rich images (Tong et al., 2024; Chen et al., 2023b), a crucial skill for our task. We then condition the LLMs on SigLIP's patch embedding vectors. To reduce the prompt length, we concatenate adjacent patch embeddings (Chen et al., 2023a). A feed-forward layer with dimensions \(2_{}_{}\) serves as a connector, mapping image features of dimension \(_{}\) to the LLM word embedding space of dimension \(_{}\).

Model TrainingWe experiment with TinyLLaMA1.1n and DeepSeek1.3n (approximately 1 billion parameters each) and CodeLLaMA7n and DeepSeek7n (7 billion parameters each). When referring to specific variants of DeTixZify, we use the names DeTixZify-TL1.18, DeTixZify-DS1.38, DeTixZify-CL7n, and DeTixZify-DS7n, respectively. For all models, we use the SoViT400M variant of SigLIP as the vision encoder. Following Liu et al. (2023b, a), we first pretrain the connector with other model parameters frozen. We pretrain for one epoch on MetaFig with AdamW (Loshchilov and Hutter, 2019), a batch size of 256, a learning rate of 1e\(-\)3, and a cosine learning rate decay with a 3% warmup ratio. Next, we unfreeze the language model (keeping the vision encoder frozen) and fine-tune on examples from DaTixZv2 that fit within a 2048 token context window. We use a batch size of 128, a learning rate of 4e\(-\)5, and train for three epochs. Training data ablations can be found in Appendix B.

Synthetic SketchesWhen training DeTixZify on DaTixZv2, we randomly replace figures with synthetic sketches 50% of the time. Sketches are generated on the fly, meaning that each time a figure is sampled as a sketch, a different synthetic sketch will be generated. Creating realistic sketches requires high-level image manipulation methods that go beyond traditional transformations like zooming or cropping. We, therefore, adopt Instruct-Pix2Pix (Brooks et al., 2023), a model capable of diversely editing images based on human instructions. We chose this model due to its remarkable zero-shot performance in generating synthetic sketches during our initial experiments. By then fine-tuning the model on SketchFig, we further improve its performance (cf. SS7 and Appendix C).

## 5 Iterative Refinement with Monte Carlo Tree Search

Due to the inherent probabilistic nature of language models, generating valid TiKZ programs during inference can be a challenging task. The generated code may not always comply with the syntactic and semantic rules of TeX and TiKZ, potentially leading to compilation errors. While constrained decoding algorithms can assist in guiding models towards generating valid programs (Ugarte et al., 2024; Poesia et al., 2022; Scholak et al., 2021), these approaches are limited to programming languages defined by context-free grammars (CFGs). However, TeX and TiKZ are not defined by CFGs (Erdweg and Ostermann, 2011), rendering these methods ineffective for our purpose. Moreover, even if the generated code compiles successfully, fidelity errors such as misaligned elements, inconsistent scaling, repetitions, or mislabeling may only become apparent in the rendered output.

Despite these challenges, which make it difficult to guide DeTixZify based on intermediate states, we can still analyze completed outputs in a straightforward manner (e.g., by examining compiler diagnostics or comparing rendered outputs to the input image), allowing us to make informed decisions during subsequent sampling iterations. This concept of making decisions based on random sampling of the search space forms the core of Monte Carlo Tree Search (MCTS; Coulom, 2007). By integrating DeTixZify with MCTS and adapting the standard MCTS algorithm to our problem domain, we can iteratively steer DeTixZify towards more promising regions of the output space (cf. Figure 1). In the following, we outline our fundamental approach, with further extensions discussed in Appendix A.

### Integrating MCTS into DeTixZify

MCTS is a versatile search algorithm that has been successfully applied to various domains, including board games (Silver et al., 2016, 2017), procedural content generation (Kartal et al., 2016, 2015; Summerville et al., 2015), and more recently, guiding language models to achieve long-term goals (Brandfonbrener et al., 2024; Zhang et al., 2023b; Chaffin et al., 2022). The algorithm incrementally builds a search tree and repeatedly runs simulations until an exit condition is met or a computational budget is exhausted. In our context, at depth \(n\), each node's state consists of \(n\) lines of TiKZ code, and edges represent continuations for generating the next line. Initially, MCTS starts with only an empty root node and then iteratively performs the following four steps (cf. Figure 2):SelectionEach simulation starts at the root node and successively selects child nodes based on a _selection policy_ until a leaf node is reached. The policy determines which parts of the tree should be explored further, balancing the _exploitation_ of high-value regions and _exploration_ of less-visited areas. Following previous work, we use Upper Confidence Trees (UCT; Kocsis and Szepesvari, 2006) as our selection policy, iteratively selecting the successor node \(i\) that maximizes the formula

\[(i)=^{n_{i}}V_{i,j}}{n_{i}}+c(i)})}{n_{i}}},\] (1)

where \(V_{i,j}[-1,1]\) is the estimated value of \(i\) at the \(j\)th visit, \(n_{i}\) and \(n_{(i)}\) are the visit counts at \(i\) and its parent \((i)\), respectively, and \(c\) is a coefficient that controls the degree of exploration.

RolloutOnce a leaf node is selected, we utilize DeTixZify as a _rollout policy_. By conditioning it on the node's state, we continue to sample Ti_KZ_ code until the end-of-sequence token is encountered. This so-called rollout is then stored for reuse in the subsequent steps.

ExpansionNext, the tree is _expanded_ by adding nodes from the rollout as new leaf nodes. While most implementations add only one node (i.e., one line of Ti_KZ_ code) per simulation, computing rollouts with LLMs is computationally expensive. Therefore, inspired by MCTS for real-time settings (Soemers et al., 2016), we instead add multiple nodes. Specifically, we add \(}\) new nodes, where \(|r|\) is the number of lines in rollout \(r\) and \(d_{l}\) is the depth of the old leaf node \(l\). This approach allows our tree to grow quickly in early simulations while converging to the standard case in the long run. To enable the tree to grow in multiple directions, we also introduce _backtracking_ nodes (Brandfonbrener et al., 2024; Chaslot et al., 2008). For each added node \(i\), we add a backtracking node as a sibling that mirrors the parent node \((i)\). When a backtracking node is expanded, its descendants are added to \((i)\) so that the backtracking node remains a leaf. This enables a practically infinite search space anywhere in the tree while still maintaining a bounded branching factor.

BackpropagationFinally, we calculate the value for rollout \(r\) using a predefined reward function (cf. SS5.2) and _backpropagate_ it to every node \(i\) on the path from the root node to the newly added nodes by appending it to \(_{i,:}\). We also increment the visit counts \(n_{i}\) for the same nodes. For backtracking nodes, only the visit counts are updated. Finally, we check any exit conditions. If MCTS terminates, we return the TiKZ program of the rollout that achieved the highest value.

### Reward Functions

We explore two distinct reward functions to guide the search process. The first reward function utilizes compiler diagnostics to identify documents that compile successfully. The second reward function provides a visual signal based on perceptual image similarity, which, in addition, helps find Ti_KZ_ programs that better match the input image. We explore further reward functions in Appendix A.

Compiler DiagnosticsThe diagnostics-based reward function is based on analyzing the log file from compiling the generated Ti_KZ_ program. We assign rewards according to the error state and whether an output file was produced. The reward function is defined as follows:

\[V_{i,j}=1&\\ 0&\\ -1&\] (2)

Figure 2: An example of the four steps of an MCTS simulation: The selection policy (i) reaches a green backtracking node (normal nodes are blue), causing new nodes from the rollout (ii) to be added to the parent node during expansion (iii). The reward is backpropagated (iv) accordingly.

Self-Assessed Perceptual Similarity (SelFsim)SelFsim computes the reward as the _perceptual similarity_(Zhang et al., 2018) between the input image and the compiled output figure. We hypothesize that DeTixZify_itself_ can assess this similarity, enabling the model to guide its own search process. To achieve this, we encode both images into embedding vectors using DeTixZify's vision encoder and calculate SelfSim as their cosine similarity (Fu et al., 2023; Hessel et al., 2021). In cases where compilation fails, we assign a reward of -1. In SS7, we demonstrate that SelfSim correlates well with human judgments and outperforms other baseline methods.

## 6 Experiments

Before training on DaTixZv2, we extract 1k samples to serve as our test set for an automatic evaluation and generate corresponding synthetic sketches. To mitigate data leakage from pretraining to testing, we only include items created after the cut-off date of CodeLLaMA and exclude repositories that may have been used in training DeepSeek. We also use an \(n\)-gram matching algorithm to prevent cross-contamination with our train split (OpenAI, 2023a). For a human evaluation involving human-created sketches, we also select 100 items from SketchFig that do not overlap with DaTixZv2 (cf. SS3). Across all models, we set the temperature to 0.8 and the exploration coefficient \(c\) to 0.6. We provide examples of real and synthetic sketches as well as generated outputs in Appendix E and Table 4.

BaselinesGiven Claude 3 and GPT-4V's potential for our task (cf. SS2), we use them as baselines. Similar to DeTixZify, we instruct these models to generate Ti_kZ_ programs for given images. However, as proprietary chatbots, they often mix code and natural language (Zhang et al., 2023c; Belouadi et al., 2024) and do not expose the internals needed to compute SelfSim. This makes it impractical to apply our MCTS-based refinement algorithm, which is designed for code-only outputs and open models. Instead, we compare our approach to equivalent chat-oriented refinement methods, i.e., we use Self-Refine as an alternative to diagnostics-based MCTS and Visual Self-Refine as an alternative to SelfSim-based MCTS (Madaan et al., 2023; cf. Appendix C for additional inference details). In Appendix B, we also explore SVG as an alternative to Ti_K_Z but find it less effective for our domain.

### Automatic Evaluation

We introduce two inference tasks to automatically evaluate our models on the test split of DaTixZv2. During _output-driven_ inference (OI), we employ the diagnostics-based reward and use successful compilation as an early exit condition (we consider compilation successful if an output artifact is produced). For _time-budgeted_ inference (TI), we use the more fine-grained SelfSim-based reward and continue from OI until a computational budget of 10 minutes is exhausted (cf. Brandfonbrener et al., 2024), investigating the extent of achievable improvement. We report results for the two use cases where either (rasterized) reference figures or (synthetic) sketches serve as model inputs (cf. SS1). Due to high inference costs, we only evaluate commercial Claude 3 and GPT-4V in OI using Self-Refine, leaving TI with Visual Self-Refine for human evaluation. We evaluate the following properties:

Code SimilarityTo measure the similarity between generated and reference Ti_k_Z programs, we use CrystalBLEU (cBLEU), a variant of BLEU optimized for evaluating code (Eghbali and Pradel, 2023; Papineni et al., 2002), and the TeX Edit Distance (TED), our adapted version of the Extended Edit Distance (Stanchev et al., 2019) combined with a TeX tokenizer.

    &  &  \\ 
**Models** & \(_{}\) & cBLEU\({}_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & cBLEU\({}_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\  Claude 3 & 51,812 & 0.111 & 57,389 & 64,896 & 83.372 & 17.822 & 0.148 & 50.156 & 0.024 & 59.731 & 59.102 & 73.954 & 29.541 & 0.189 \\ GPT-4V & 61,975 & 0.286 & 57,178 & 69,741 & 86,215 & 6,714 & 0.612 & 54.126 & 0.024 & 60.298 & 61.98 & 75.687 & 38.203 & 0.15 \\  DT-TL1,18 & 88.03 & 1.168 & 58.815 & 65.383 & 84.161 & 15.747 & 0.201 & 90.597 & 0.502 & 60.202 & 60.588 & 77.947 & 21.851 & 0.454 \\ DT-DS1,38 & 83.771 & 1.336 & 57.661 & 68.659 & 86.079 & 11.536 & 0.572 & 87.446 & 0.541 & 60.112 & 62.756 & 79.097 & 17.334 & 0.642 \\ DT-CL7,8 & **88.593** & 1.477 & **56.893** & 72.315 & 87.466 & 8.301 & 0.869 & **91.221** & 0.555 & **59.563** & 65.118 & 79.717 & **12.207** & 0.941 \\ DT-DS7,8 & 82,366 & **1.815** & 57.227 & **73.01** & **88.323** & **5.951** & **0.966** & 89.299 & **0.69** & 59.693 & 65.198 & **80.207** & **12.207** & **0.965** \\   

Table 2: System-level scores for output-driven inference (DeTixZify abbreviated as DT). Bold and underlined values indicate the best and second-best scores for each metric column, respectively. Cell shading reflects the relative score magnitudes across input types. Arrows indicate metric directionality.

**Image Similarity**: In addition to SelfSim (SSim), which can also be used as a metric, we report DreamSim (DSim; Fu et al., 2023), a fine-tuned metric for perceptual similarity. We also compute the Kernel Inception Distance (KID \(\) 10\({}^{3}\); Binkowski et al., 2018), which assesses the overall quality of generated figures by comparing their distribution with the distribution of reference figures. These metrics are always computed by comparing the generated figures to the reference figures, regardless of what the model receives as input.
**Average Similarity**: To offer a holistic view of each model's performance, we also compute the arithmetic mean (AVG) of all code and image similarity metrics. Given that these metrics operate on different scales, we min-max normalize their scores before calculating the average.
**Efficiency**: For OI, we compute the Mean Token Efficiency (MTE) as the 10% winsorized mean of the ratio of the number of tokens in the final Ti\(\&\)Z program to the total number of tokens generated to arrive at that program. For TI, we instead compute the Mean Sampling Throughput (MST), measuring the throughput of unique Ti\(\&\)Z graphics for the given budget.

ResultsTable 2 presents the system-level metric scores for OI. As expected, the scores for reference figures are, on average, 38% higher than those for synthetic sketches, but similar patterns emerge across both input types. DeTix\(\_\)Zify-CL7\(\_\) and DeTix\(\_\)Zify-DS7\(\_\)8 consistently outperform all other models, achieving AVG scores of 0.869 & 0.965 for figures and 0.941 & 0.965 for sketches, respectively. In contrast, GPT-4V reaches AVG scores only of 0.612 and 0.15, placing it in competition with the smaller 1b models: for figures, GPT-4V surpasses DeTix\(\_\)Zify-TL1\(\_\)1\(\_\) and DeTix\(\_\)Zify-DS1\(\_\)3\(\_\)3\(\_\), which achieve scores of 0.207 and 0.572, respectively. However, these smaller models outperform GPT-4V on sketches, where they achieve scores of 0.454 and 0.642. Claude 3 trails behind all our models, with an AVG of only 0.148 and 0.189. When examining individual similarity metrics, DeTix\(\_\)Zify-DS7\(\_\)8, the top-performing DeTix\(\_\)Zify model overall, surpasses GPT-4V, the best baseline, by more than 3pp (percentage points) on average for DreamSim and SelfSim, while maintaining a noticeably lower KID. In terms of cBLEU, GPT-4V, and Claude 3 only reach 6.5-18.5% of the performance achieved by the lowest-scoring DeTix\(\_\)Zify model (DeTix\(\_\)Zify-TL1\(\_\)1\(\_\)8). The differences in TED are less pronounced, possibly due to the influence of boilerplate code, which cBLEU inherently ignores.

For efficiency, all DeTix\(\_\)Zify models demonstrate an MTE of 82-91%, indicating that only 1-2 out of 10 inference runs require a second simulation to generate a compilable Ti\(\&\)Z program. Interestingly, the model size does not seem to particularly influence this score, with the pretraining setup appearing to be the key factor instead. For instance, DeTix\(\_\)Zify-TL1\(\_\)1\(\_\) and DeTix\(\_\)Zify-CL7\(\_\)8 share a similar pretraining setup and exhibit comparable MTE values, as do DeTix\(\_\)Zify-DS1\(\_\)3\(\_\) and DeTix\(\_\)Zify-DS7\(\_\)8. We can further observe that (i) MTE is generally higher for sketches compared to figures, and (ii) for figures, the MTE of similarly pretrained models is inversely correlated with their scores on other metrics. These phenomena likely stem from models making fewer mistakes when the input is less detailed or when their understanding of it is limited--a finding that aligns well with other studies (Tong et al., 2024). Compared to DeTix\(\_\)Zify, Claude 3 and GPT-4V perform considerably worse, with an MTE of only 50-62%. Notably, for these models, 98.5% of the items already compile after the initial Self-Refine step, meaning that this inefficacy primarily originates from the natural language texts surrounding the code and that Self-Refine is nearly equivalent to regular sampling-based inference.

The results for DeTix\(\_\)Zify on TI are presented in Table 3. Remarkably, increasing the computational budget for MCTS improves nearly all metrics for both reference figures and sketches as input without requiring access to any additional knowledge. The improvement with sketches is particularly noteworthy, as it demonstrates that the refinement process enhances the desired properties even when

    &  &  \\ 
**Models** & **MST\({}_{}\)** & **cBLEU\({}_{}\)** & **TED\({}_{}\)** & **DSim\({}_{}\)** & **SSim\({}_{}\)** & **KID\({}_{}\)** & **AVG\({}_{}\)** & **MST\({}_{}\)** & **cBLEU\({}_{}\)** & **TED\({}_{}\)** & **DSim\({}_{}\)** & **SSim\({}_{}\)** & **KID\({}_{}\)** & **AVG\({}_{}\)** \\   DT-TL1\(\_\)1\(\_\) & **33.775** & \(-0.011\) & \(-2.001\) & \(+8.704\) & \(+5.561\) & \(-12.146\) & 0.128 & **35.975** & \(+0.094\) & \(-0.628\) & \(+5.82\) & \(+3.026\) & \(+0.854\) & \(0.014\) \\ DT-DS1\(\_\)3\(\_\) & **29.975** & \(-0.028\) & \(-1.303\) & \(+8.464\) & \(+5.108\) & \(-8.728\) & 0.531 & \(32.429\) & \(+0.061\) & \(-0.504\) & \(+5.573\) & \(+2.685\) & \(+5.493\) & \(0.22\) \\ DT-CL7\(\_\)8\(\_\) & 25.124 & \(+0.07\) & \(-1.351\) & \(+7.797\) & \(+4.93\) & \(-4.868\) & **0.876** & 26.219 & \(+0.073\) & \(-0.468\) & \(+5.079\) & \(+2.455\) & \(+5.493\) & \(0.681\) \\ DT-DS7\(\_\)8\(\_\) & 24.145 & \(-\)**0.073** & \(-1.542\) & \(+6.974\) & \(+3.893\) & \(-0.946\) & 0.76 & 26.195 & \(+0.054\) & \(-0.696\) & \(+4.887\) & \(+2.241\) & \(+1.099\) & **0.994** \\   

Table 3: System-level scores for time-budgeted inference, displaying relative changes for metrics shared with output-driven inference (Table 2; colored green for improvements and red for declines) and absolute scores for independent metrics. Bold and underlined values indicate the best and second-best _absolute_ scores for each metric column, respectively. Arrows indicate metric directionality.

the model input type differs from the one used for evaluation. The 2.2-5.6pp increase of SelfSim for all models is not surprising since it serves as the reward signal we optimize, but DreamSim and TED also increase by 4.9-8.7pp and 0.5-2pp, respectively, demonstrating the efficacy of our approach. While KID improves by 1-12.1 points with reference figures, it drops by 0.9-5.5 points with sketches. We believe this is because sketches often omit minor details, such as axis tick labels, which is reflected more in the output of the TI models, biasing their overall output distributions. Therefore, we consider the substantial improvement of metrics capturing instance-level similarities to be more important. For cBLEU, we observe only minor changes (less than \( 0.1\)pp), aligning with findings that BLEU-based metrics become less effective as performance increases (Ma et al., 2019). The MST and AVG reveal that, although 1b models produce more unique outputs within the time frame compared to their larger 7b counterparts (30-36 vs. 24.1-26.2), they still fail to close the overall gap in performance, with AVG scores ranging between 0.014-0.531 compared to 0.681-0.994 for 7b models.

Overall, all DeTtxZify models are capable of generating compilable outputs with reasonable efficiency. Upon examination of these outputs, it becomes evident that the 7b models, particularly DeTtxZify-DS7b, consistently outperform both Claude 3 and GPT-4V, whose performance is more comparable to the 1b range. Increasing the computational budget for DeTtxZify further improves performance.

### Human Evaluation

To further assess the quality of the generated figures, we perform a human evaluation on SketchFig using _Best-Worst Scaling_(BWS; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). In this process, for each reference figure, we present annotators with a tuple of generated figures and ask them to identify the most and least perceptually similar figure. We then transform this data into scores ranging from -1 (poor) to 1 (excellent) by calculating the difference between the proportion of times a figure is selected as the best and the proportion of times it is chosen as the worst (Orme, 2009). To keep the workload manageable, we focus on the most promising DeTtxZify model (DeTtxZify-DS7b) and the strongest baseline (GPT-4V). Building upon the automatic evaluation, we assess these models in the OI and TI configurations, using either reference figures or human-created sketches as input. For each input type, we engage six unique expert annotators (cf. Appendix D for more details).

ResultsFigure 3 (left) shows kernel density estimates for the computed BWS scores, revealing intriguing findings that are consistent across input types. In contrast to the automatic evaluation, DeTtxZify-DS7b performs worse (mean score \(=-0.32\)) than GPT-4V (\(=0.09\)) in OI. This could be attributed to the fact that TeIX.SE, the sole source of SketchFig, emphasizes minimum working examples, a type on which GPT-4V particularly excels (Belouadi et al., 2024). However, when we increase the computational budget, as in DeTtxZify-DS7b (7b), it not only improves over OI results (\(=0.39\); in line with automatic evaluation) but also surpasses GPT-4V in both configurations by a considerable margin. Interestingly, GPT-4V's performance in TI (\(=-0.16\)) is lower than its performance in OI, indicating that GPT-4V (TI) struggles to refine its own outputs effectively and quickly deteriorates. Overall, this shows how difficult it is for models to refine their own outputs and highlights the effectiveness of our MCTS-based approach. Example outputs are provided in Table 4.

Figure 3: Bivariate distributions of BWS scores (higher is better) using kernel density estimation (left) and log-linear regression over TI reward scores for different generation strategies over time (right).

## 7 Analysis

In this section, we take a closer look at our methodologies and evaluation strategies, correlating evaluation metrics with human judgments, quantifying the quality of synthetic sketches, and examining the rate of convergence of our MCTS algorithm. We also demonstrate that our models are not affected by memorization of the training data, as shown in Appendix B.

Correlating Humans and MetricsTo assess the reliability of our human evaluation results, we investigate the agreement between annotators. To this end, we calculate the _split-half reliability_(SHR; Kiritchenko and Mohammad, 2017) by randomly splitting our annotations into two subsets, computing BWS scores for each subset, and measuring their correlation with Spearman's \(\). The SHR values of 0.69 for sketches and 0.75 for images indicate a moderate to strong correlation between annotators, supporting the validity of our human evaluation results. Motivated by these findings, we explore whether metrics that also assess perceptual similarity (i.e., SelfSim and DreamSim) correlate with these human judgments. We again calculate Spearman's \(\) and show the average correlations (David M. Corey and Burke, 1998) at the segment and system level in Table 5. For comparison, we also include the popular LPIPS and DISTS metrics (Zhang et al., 2018; Ding et al., 2020). At the segment level, SelfSim outperforms all other metrics, which is remarkable considering it is the only untrained metric. Segment-level performance is particularly important for fine-grained reward functions, justifying our choice of SelfSim in our MCTS algorithm. At the system level, DreamSim performs the best, showcasing its strength in evaluation settings.

Synthetic Sketch QualityWe also assess the quality of our synthetic sketches by measuring their congruence coefficient (Lorenzo-Seva and ten Berge, 2006) with real sketches. We embed human-created figure-sketch pairs from SketchFig using SigLIP, subtract each sketch embedding from the corresponding figure embedding to obtain _local_ sketch vectors, and perform a single-component Principal Component Analysis to derive a _global_ sketch vector (Zou et al., 2023). We repeat this process for synthetic sketches generated for the test split of DaTixZv2 and compare the global vectors using cosine similarity. Base Instruct-Pix2Pix generates synthetic sketches with a congruence coefficient of 0.66, which increases to 0.7 after fine-tuning. These results demonstrate a high correlation with human-created sketches, suggesting that our generated sketches are of good quality.

  
**Metric** & **Segment** & **System** \\  LPIPS & 0.224 & 0.642 \\ DISTS & 0.32 & 0.642 \\ DSim & 0.424 & **0.954** \\ SSIM & **0.436** & 0.642 \\   

Table 5: Correlations of image similarity metrics with humans at the segment and system level.

MCTS ConvergenceTo gain insights into the long-term characteristics of our MCTS algorithm, we visualize the trends in achieved TI reward scores over time in Figure 3 (right) and compare them to conventional sampling-based inference. As expected, sampling does not lead to improvements over time due to the absence of a feedback loop. In contrast, MCTS consistently improves throughout the entire time frame, and even at the end of our budget of 10 minutes, it does not appear to converge, suggesting potential additional gains for larger budgets. Apart from this, MCTS is not only more effective but also faster. With an average MST of 25.17, compared to 18.7 for sampling, our MCTS algorithm generates considerably more unique Ti\(k\)Z programs within the same amount of time.

## 8 Conclusion

In this work, we showcase the potential of DeTixZify in generating Ti\(k\)Z programs for two practical use cases. First, it can convert existing figures from lower-level formats into Ti\(k\)Z, paving the way for semantic image editing and downstream tasks (Zhang et al., 2023). Second, it can develop hand-drawn sketches into Ti\(k\)Z graphics, which could aid researchers in creating high-quality scientific illustrations. In both cases, DeTixZify substantially outperforms the commercial LLMs GPT-4V and Claude 3 despite its presumably much smaller size. We hope that our datasets (DaTixZ\({}_{ 2}\), SketchFig, and MetaFig), our method for generating synthetic sketches, and our MCTS-based inference algorithm will pave the way towards future research on graphics program synthesis and bolster the cause of open science.

Looking ahead, we plan to extend our approach to other graphics languages, such as MetaPost, PSTricks or Asymptote (Hobby, 2014; Van Zandt, 2007; Hammerlindl et al., 2024). We also intend to explore alternatives to perceptual similarity as an MCTS reward signal, including per-pixel measures and point cloud metrics (Wang and Bovik, 2009; Wu et al., 2021). In addition, we aim to investigate reinforcement learning from reward functions, for example, using Direct Preference Optimization (Rafailov et al., 2023; Xu et al., 2024). Finally, while this work focuses on visual inputs, we plan to explore additional modalities, such as text and mixed-modality inputs, in future work.

## Limitations

In this work, we compare openly available models with proprietary systems that lack transparency in their training details and internal workings and whose performance is not stable over time. This inevitably complicates efforts to address concerns such as data leakage or cross-contamination and limits the fairness and reproducibility of our experiments. Nevertheless, under these adverse conditions, our open models and methods demonstrate favorable performance. Users should be aware, however, that our models might inherit biases, flaws, or other limitations present in the training data, potentially leading to discrepancies between expected results and generated outputs. Furthermore, given the resource-intensive nature of LLMs, many of our training and inference hyper-parameters were adopted from related work or chosen based on general intuition. Although LLMs are generally robust to hyper-parameter selection (Beyer et al., 2024), conducting a thorough hyper-parameter search might enhance their performance further. Finally, it should be noted that our models could potentially be misused by malicious actors to produce misinformation and fake science.

Another important consideration is that the public release of DaTixZ\({}_{ 2}\) does not include some Ti\(k\)Z programs from our internal version due to licensing restrictions. These programs are distributed under the arXiv.org perpetual, non-exclusive license, which prohibits redistribution. Nonetheless, we provide our dataset creation scripts alongside usage instructions, enabling anyone to reproduce the full version of DaTixZ\({}_{ 2}\) independently. The remaining Ti\(k\)Z programs in DaTixZ\({}_{ 2}\) are licensed under Creative Commons attribution licenses,5 the GNU Free Documentation License,6 or the MIT license,7 and their respective terms and conditions apply. Regarding artificially created examples, OpenAI's terms of use restrict the use of their services for creating competing products, limiting this subset of DaTixZ\({}_{ 2}\) to non-commercial applications.8