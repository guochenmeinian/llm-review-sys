# Maestro: Uncovering Low-Rank Structures via Trainable Decomposition

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Deep Neural Networks (DNNs) have been a large driver and enabler for AI breakthroughs in recent years. These models have been getting larger in their attempt to become more accurate and tackle new upcoming use-cases, including AR/VR and intelligent assistants. However, the training process of such large models is a costly and time-consuming process, which typically yields a single model to fit all targets. To mitigate this, various techniques have been proposed in the literature, including pruning, sparsification or quantization of the model weights and updates. While able to achieve high compression rates, they often incur computational overheads or accuracy penalties. Alternatively, factorization methods have been leveraged to incorporate low-rank compression in the training process. Similarly, such techniques (e.g., SVD) frequently rely on the computationally expensive decomposition of layers and are potentially sub-optimal for non-linear models, such as DNNs.

In this work, we take a further step in designing efficient low-rank models and propose Maestro, a framework for trainable low-rank layers. Instead of regularly applying a priori decompositions such as SVD, the low-rank structure is built into the training process through a generalized variant of Ordered Dropout. This method imposes an importance ordering via sampling on the decomposed DNN structure. Our theoretical analysis demonstrates that our method recovers the SVD decomposition of linear mapping on uniformly distributed data and PCA for linear autoencoders. We further apply our technique on DNNs and empirically illustrate that Maestro enables the extraction of lower footprint models that preserve model performance while allowing for graceful accuracy-latency tradeoff for the deployment to devices of different capabilities.

## 1 Introduction

Deep Learning has been experiencing an unprecedented uptake, with models achieving a (super-)human level of performance in several tasks across modalities, giving birth to even more intelligent assistants and next-gen visual perception and generation systems. However, the price of this performance is that models are getting significantly larger, with training and deployment becoming increasingly costly. Therefore, techniques from Efficient ML become evermore relevant , and a requirement for deployment in constrained devices, such as smartphones or IoT devices.

Typical techniques to compress the network involve _i) quantization_, i.e., reducing precision of the model  or communicated updates , _ii) pruning_ the model during training, e.g., through Lottery Ticket Hypothesis (LTH) , _iii) sparsification_ of the network representation and updates, i.e., dropping the subset of coordinates  or _iv) low-rank approximation _, i.e. keeping the most relevant ranks of the decomposed network. Despite the benefits during deployment, that is a lower footprint model, in many cases, the overhead during training time or the accuracy degradationcan be non-negligible. Moreover, many techniques can introduce mutliple hyperparameters or the need to fine-tune to recover the lost accuracy.

In this work, we focus on training low-rank factorized models. Specifically, we pinpoint the challenges of techniques [53; 54] when decomposing the parameters of each layer in low-rank space and the need to find the optimal ranks for each one at training time. To solve this, we adopt and non-trivially extend the Ordered Dropout technique from  and apply it to find progressively the optimal decomposition for each layer of a network while training (Fig. 1). Critical differences to prior work include _i)_ the non-uniformity of the search space (i.e. we allow for different ranks per layer), _ii)_ the trainable aspect of the decomposition to reflect the data distribution, and _iii)_ the gains to training and deployment time without sacrificing accuracy. Nevertheless, we also provide a latency-accuracy trade-off mechanism to deploy the network on even more constrained devices.

Our contributions can be summarized as follows:

* We propose Maestro, a novel layer decomposition technique that enables learning low-rank layers in a progressive manner while training. We novelly fuse layer factorization and an extended variant of the ordered dropout, by embedding OD directly into the factorized weights. By decomposing layers and training on stochastically sampled low-rank models, we apply ordered importance decomposed representation of each layer. We combine this with a _hierarchical group-lasso_ term  in the loss function to zero out redundant ranks and _progressively shrink_ the rank space. This way, we enable computationally efficient training achieved by the proposed decomposition without relying on inexact and potentially computationally expensive decompositions such as SVD.
* Maestro is a theoretically motivated approach that embeds decomposition into training. First, we show that our new objective is able to recover _i)_ the SVD of the target linear mapping for the particular case of uniform data distribution and _ii)_ the Principal Component Analysis (PCA) of the data in the case of identity mapping.
* As Maestro's decomposition is part of the training procedure, it also accounts for data distribution and the target function, contrary to SVD, which operates directly on learned weights. We show that this problem _already arises_ for a simple linear model and empirically generalize our results in the case of DNNs, by applying our method to different types of layers (including fully-connected, convolutional, and attention) spanning across three datasets and modalities. We illustrate that our technique achieves better results than SVD-based baselines at a lower cost.

## 2 Related work

The topic of Efficient ML has received a lot of attention throughout the past decade as networks have been getting increasingly computationally expensive. Towards this end, we distinguish between training and deployment time, with the latter having a more significant impact and thus amortizes the potential overhead during training. Nevertheless, with the advent of Federated Learning , efficient training becomes increasingly relevant to remain tractable.

**Efficient inference.** For efficient deployment, there have been proposed various techniques that either optimize the architecture of the DNN in a hand-crafted  or automated manner (i.e. NAS) , they remove redundant computation by means of pruning parts of the network [12; 6; 11; 48; 30; 55; 21; 55; 65; 15; 59; 33; 62] or utilise low-precision representation  of the neurons and activations. Closer to our method, there have been techniques leveraging low-rank approximation (e.g. SVD) for efficient inference [58; 43; 22; 56; 9]. Last, there is a category of techniques that dynamically

Figure 1: Maestroâ€™s construction. To obtain low-rank approximation, the given linear map is decomposed and trained with ordered dropout to obtain an ordered representation that can be efficiently pruned.

resize the network at runtime for compute, memory or energy efficiency, based on early-exiting  or dynamic-width  and leverage the accuracy-latency tradeoff.

**Efficient training.** On the other hand, techniques for efficient training become very relevant nowadays when scaling DNNs sizes  or deploying to embedded devices , and oftentimes offer additional gains at deployment time. Towards this goal, there have been employed methods where part of the network is masked  or dropped  during training, with the goal of minimizing the training footprint. Similarly to early-exiting, there have been proposed multi-exit variants for efficient training , and the same applies for width-based scaling . Last but not least, in the era of transformers and LLMs, where networks have scaled exponentially in size, PEFT-based techniques, such as adapter-based fine-tuning  (such as LoRA ), become increasingly important and make an important differentiator for tackling downstream tasks.

**Learning ordered representation.** Originally, Ordered Dropout (OD) was proposed as a mechanism for importance-based pruning for the easy extraction of sub-networks devised to allow for heterogeneous federated training . The earlier work that aims to learn ordered representation includes a similar technique to OD--Nested Dropout, which proposed a similar construction, applied to the representation layer in autoencoders  to enforce identifiability of the learned representation or the last layer of the feature extractor  to learn an ordered set of features for transfer learning. We leverage and non-trivially extend OD in our technique as a means to order ranks in terms of importance in a nested manner during training of a decomposed network that is progressively shrunk as redundant ranks converge to 0. Ranks selection is ensured through hierarchical group lasso penalty, as described in Sec. 3.3. Moreover, contrary to , which assumed a uniform width, our formulation allows for heterogeneous ranks per layer. Last, we leverage the ordered representation of ranks at inference time to further compress the model, allowing a graceful degradation of performance as a mechanism for the accuracy-latency trade-off.

## 3 Maestro

In this work, we focus on low-rank models as a technique to reduce the computational complexity and memory requirements of the neural network model. The main challenge that we face is the selection of the optimal rank or the trade-off between the efficiency and the rank for the given layer represented by linear mapping. Therefore, we devise an importance-based training technique, Maestro, which not only learns a mapping between features and responses, but also learns the decomposition of the trained network. This is achieved by factorizing all the layers in the network.

### Formulation

**Low-rank approximation.** Our inspiration comes from the low-rank matrix approximation of a matrix \(A^{m n}\). For simplicity, we assume that \(A\) has rank \(r=\{m,n\}\) with \(k r\) distinct non-zero singular values \(_{1}>_{2}>>_{k}>0\), with corresponding left and right singular vectors \(_{1},_{2},,_{k} R^{m}\) and \(_{1},_{2},,_{k} R^{n}\), respectively. For such a matrix, we can rewrite its best \(l\)-rank approximation as the following minimization problem

\[_{U^{m l},V^{n r}}\|_{i=1} ^{l}u_{i}v_{i}^{}-A\|_{F}^{2}\] (1)

where \(c_{i}\) denotes the \(i\)-th row of matrix \(C\) and \(\|\|_{F}\) denotes Frobenius norm. We note that Problem (1) is non-convex and non-smooth. However,  showed that the randomly initialized gradient descent algorithm solves this problem in polynomial time. In this work, we consider the best rank approximation across all the ranks that leads us to the following objective

\[_{U^{m r},V^{n r}}_{b =1}^{r}V_{:b}^{}-A}_{F}^{2},\] (2)

where \(C_{:b}\) denotes the first \(b\) columns of matrix \(C\). This objective, up to scaling, recovers SVD of \(A\) exactly, and for the case of distinct non-zero singular values, the solution is, up to scaling, unique . This formulation, however, does not account for the data distribution, i.e., it cannot tailor the decomposition to capture specific structures that appear in the dataset.

**Data-dependent low-rank approximation.** Therefore, the next step of our construction is to extend this problem formulation with data that can further improve compression, reconstruction, and generalization, and incorporate domain knowledge. We assume that data comes from the distribution \(x\) centered around zero, i.e., \(_{x}[x]=0\).1, and the response is given by \(y=Ax\). In this particular case, we can write the training loss as

\[_{U^{m r},V^{n r}}_{x,y }[_{b=1}^{r}U_{:b}V_{:b}^{}x-y ^{2}].\] (3)

It is important to note that the produced problem formulation (3) is the same as the Ordered Dropout formulation of  for the neural network with a single hidden layer and no activations, and it can be solved using stochastic algorithms by sampling from the data distribution \(\) (subsampling) and rank distribution \(\). However, there is an important distinction when we apply Maestro for deep neural networks. While FjORD applies uniform dropout across the width of the network for each layer, we propose to decompose each layer independently to uncover its - potentially different - optimal rank for deployment. We discuss details in the next paragraph.

**DNN low-rank approximation.** For Deep Neural Networks (DNNs), we seek to uncover the optimal ranks for a set of \(d\) linear mappings \(W^{1}^{m_{1} n_{1}},,W^{d}^{m_{d} n _{d}}\), where \(W^{i}\)'s are model parameters and \(d\) is model depth, e.g., weights corresponding to linear layers2, by decomposing them as \(W^{i}=U^{i}(V^{i})^{}\). We discuss how these are selected in the next section. To decompose the network, we aim to minimize the following objective:

\[_{x,y}[^{d}r_{i}}_{i=1}^{ d}_{b=1}^{r_{i}}l(h(U^{1}V^{1}^{},,U^{i}_{:b} V^{i}_{:b}^{},,U^{d}V^{d}^{},W^{o},x ),y)],\] (4)

where \(r_{i}=\{m_{i},n_{i}\}\), \(l\) is a loss function, \(h\) is a DNN, and \(W^{o}\) are the other weights that we do not decompose. We note that our formulation aims to decompose each layer, while decompositions across layers do not directly interact. The motivation for this approach is to uncover low-rank structures within each layer that are not affected by inaccuracies from other layers due to multiple low-rank approximations.

### Layer factorization

The following subsections discuss how model factorization is implemented for different model architectures.

**FC layers.** A 2-layer fully connected (FC) neural network can be expressed as \(f(x)=((xW_{1})W_{2})\), where \(W\)s are weight matrices of each FC layer, and \(()\) is any arbitrary activation function, e.g., ReLU. The weight matrix \(W\) can be factorized as \(UV^{}\).

**CNN layers.** For a convolution layer with dimension, \(W^{m n k k}\) where \(m\) and \(n\) are the number of input and output channels, and \(k\) is the size of the convolution filters. Instead of directly factorizing the \(4\)D weight of a convolution layer, we factorize the unrolled 2D matrix. Unrolling the 4D tensor \(W\) leads to a 2D matrix with shape \(W_{}^{mk^{2} n}\), where each column represents the weight of a vectorized convolution filter. Factorization can then be conducted on the unrolled 2D matrix; see  for details.

**Transformers.** A Transformer layer consists of a stack of encoders and decoders . The encoder and decoder contain three main building blocks: the multi-head attention layer, position-wise feed-forward networks (FFN), and positional encoding. We factorize all trainable weight matrices in the multi-head attention (HMA) and the FFN layers. The FFN layer factorization can directly adopt the strategy from the FC factorization. A \(p\)-head attention layer learns \(p\) attention mechanisms on the key, value, and query (\(K,V,Q\)) of each input token:

\[(Q,K,V)=(_{1},,_{p})W^ {O}.\]

Each head performs the computation of:

\[_{i}=(QW_{Q}^{(i)},KW_{K}^{(i)},VW_{V}^{(i)})= (^{(i)}W_{K}^{(i)}{}^{}K^{}}{})VW_{V}^{(i)}.\]

where \(d\) is the hidden dimension. The trainable weights \(W_{Q}^{(i)},W_{K}^{(i)},W_{V}^{(i)},i\{1,2,,p\}\) can be factorized by simply decomposing all learnable weights \(W\): in an attention layer and obtaining \(U V^{}\). .

### Training techniques

Having defined the decomposition of typical layers found in DNNs, we move to formulate the training procedure of our method, formally described in Algorithm 1. Training the model comprises an iterative process of propagating forward on the model by _sampling a rank_\(b_{i}\) per decomposed layer \(i\) up to maximal rank \(r_{i}\) (line 3). We calculate the loss, which integrates an additional _hierarchical group lasso_ component (lines 4) and _backpropagate_ on the sampled decomposed model (line 5). At the end of each epoch, we _progressively shrink_ the network by updating the maximal rank \(r_{i}\), based on an importance threshold \(_{ps}\) (line 11). We provide more details about each component below.

``` Input: epochs \(E\), dataset \(\), model \(h\) parametrized by \(U^{1}^{m_{1} r_{1}}\), \(V^{1}^{n_{1} r_{1}}\), \(,U^{d}^{m_{d} r_{d}},V^{d}^{n_{d} r _{d}}\), \(W^{o}\), and hyperparameters \(_{gl}\), \(_{ps}\)
1for\(t 0\) to \(E-1\)do// Epochs
2for\((x,y)\)do// Iterate over dataset
3 Sample \((i,b)\{i,b\}_{i=1}^{r_{i}}_{i=1}^{d}\); \(L=l(h(U^{1}V^{1})^{},,U^{i}_{b}V^{k}_{b}^{},, U^{d}V^{d}^{},W^{o},x),y)+\) \(+_{gl}_{i=1}^{d}_{b=1}^{r_{i}}U^{i}_{b:} +V^{i}_{b:}}\)// compute loss
4 L_backward() // Update weights
5
6 end for
7for\(i 1\)to\(d\)do
8for\(b 1\)to\(r_{i}\)do// rank importance thresholding
9if\(V^{i}_{b:}U^{i}_{b:}_{ps}\)then
10\(r_{i}=b-1\)// progressive shrinking
11break
12break
13 end if
14
15 end for
16
17 end for
18
19 end for
20
21 end for ```

**Algorithm 1**Macstro (Training Process)

**Efficient training via sampling.** In Sec. 4, we show that for the linear case (3), the optimal solution corresponds to PCA over the linearly transformed dataset. This means that the obtained solution contains orthogonal directions. This property is beneficial because it directly implies that when we employ gradient-based optimization, not only is the gradient zero at the optimum, but the gradient with respect to each summand in Equation (3) is also zero. The same property is directly implied by overparametrization  or strong growth condition . As a consequence, this enables us to sample only one summand at a time and obtain the same quality solution. When considering (4) as an extension to (3), it is unclear whether this property still holds, which would also imply that the set of stationary points of (3) is a subset of stationary points of the original objective without decomposition. However, in the experiments, we observed that sampling is sufficient to converge to a good-quality solution. If this only holds approximately, we one could leverage fine-tuning to recover the loss in performance.

**Efficient rank extraction via hierarchical group-lasso.** By definition, (3) leads to an ordered set of ranks for each layer. This ordered structure enables efficient rank extraction and selection. To effectively eliminate unimportant ranks while retaining the important ones, thus leading to a more efficient model, we consider Hierarchical Group Lasso (HGL)  in the form

\[_{gl}_{i=1}^{d}_{b=1}^{r_{i}}U^{i}_{b:}+ V^{i}_{b:},\] (5)

where \(C_{b:}\) denotes the matrix that contains all the columns of \(C\) except for the first \(b-1\) columns.

**Progressive shrinking.** HGL encourages that unimportant ranks become zero and can be effectively removed from the model. To account for this, for each layer we remove \(V^{i}_{b:}\) and \(U^{i}_{b:}\) (i.e., set \(r_{i}=b-1\)) if \(V^{i}_{b:}U^{i}_{b:}_{ps}\), where \(_{ps}\) is a pre-selected threshold - and a hyperparameter of our method.

**Initialization.** Initialization is a key component of the training procedure . To adopt the best practices from standard non-factorized training, we follow a similar approach to , where we first initialize the non-factorized model using standard initialization. For initializing factorized layers, we use the Singular Value Decomposition (SVD) of the non-factorized initialization - in afull-rank form - to ensure that the resulting product matrix is the same as the original parameter decomposition. In addition, SVD is an optimal decomposition for the linear case with uniform data. However, in contrast with the adaptive baseline method  we only decompose once, rather than on every training iteration.

### Train-once, deploy-everywhere

Up until now, we have described how our method works for training low-rank models, which yield computational, memory, network, and energy  bandwidth benefits during training. At deployment time, one can directly deploy the final model (rank \(r_{i}\) for each layer) on the device, which we acquire from performing a threshold sweep of \(_{ps}\) over the effective range of rank importance across layers. However, in case we want to run on even more constrained devices, such as mobile  or embedded  systems, the learned decomposition also gives us the flexibility to further compress the model in a straightforward manner, effectively trading off accuracy for a smaller model footprint. Inspired by , we propose to use greedy search. We begin with the current model and compare model performance across various low-rank models, each created by removing a certain percentage of ranks from each layer. We then eliminate the ranks that cause the least decrease in performance. This process is iterated until we reach the desired size or accuracy constraint. To make this approach efficient, we estimate the loss using a single mini-batch with a large batch size, for example, 2048. This also avoids issues with BatchNorm layers; see  for details.

In summary, Maestro comprises a technique for trainable low-rank approximation during training time that progressively compresses the model, reflecting the data distribution, and a method that enables a graceful trade-off between accuracy and latency for embedded deployment, by selecting the most important parts of the network. We validate these claims in Sec. 5.2 and 5.5, respectively.

## 4 Theoretical guarantees

In this section, we further investigate the theoretical properties of Maestro for the linear mappings, i.e., the setup of the problem formulation (3).

**Theorem 4.1** (Informal).: _Let \(A=^{}\) be a SVD decomposition of \(A\). Then, the minimization problem (3) is equivalent to PCA applied to the transformed dataset \(x^{}x\), \(x\) projected on the column space of \(\)._

The formal statement can be found in Appendix D. Theorem 4.1 shows that Maestro can adapt to data distribution by directly operating on data \(x\) and also to the target mapping by projecting data to its right singular vectors scaled by singular values. In particular, we show that in the special case, when \(\) is the uniform distribution on the unit ball, (3), i.e., Maestro, exactly recovers truncated SVD of \(A\), which is consistent with the prior results . In the case \(A\) is the identity, it is straightforward to see that Maestro is equivalent to PCA. We can see that Maestro can efficiently extract low-rank solutions by filtering out directions corresponding to the null space of the target mapping \(A\) and directions with no data. We also numerically verify both of the special cases-PCA and SVD, by minimizing (3) using stochastic gradient descent (SGD) with \(\) being the uniform distribution. These preliminary experiments are provided in Fig. 1(a) and 1(b).

Figure 2: Empirical showcase of theoretical properties of the Maestroâ€™s formulation.

We showed that Maestro could recover SVD in a particular case of the linear model and the uniform data distribution on the unit ball. We note that in this case, SVD is optimal, and we cannot acquire better decomposition. Therefore, it is desired that Maestro is equivalent to SVD in this scenario. In the more general setting, we argue that Maestro decomposition should be preferable to SVD due to the following reasons:

* Maestro formulation is directly built into the training and tailored to obtain the best low-rank decomposition, while SVD relies on linearity assumption.
* SVD does not account for data, and even in the linear NN case, the learned singular vectors might exhibit wrong ordering. We demonstrate this issue using a simple example where we take matrix \(A\) with rank \(3\). We construct the dataset \(\) in such a way that the third singular vector is the most important, the second one is the second, and the first is the third most important direction. Clearly, SVD does not look at data. Therefore, it cannot capture this phenomenon. We showcase that Maestro learns the correct order; see Fig. 5 of the Appendix.
* Pre-factorizing models allow us to apply hierarchical group-lasso penalty  for decomposed weights to directly regularize the rank of different layers.
* SVD is computationally expensive and can only run rarely, while Maestro is directly built into the training and, therefore, does not require extra computations. In addition, Maestro supports rank sampling so training can be made computationally efficient.

## 5 Experiments

We start this section by describing the setup of our experiments, including the models, datasets and baselines with which we compare Maestro. We then compare Maestro against the baselines on accuracy and MAC and discuss the results. Subsequently, we analyze the behaviour of our system in-depth and provide additional insights on the performance of our technique, along with an ablation study and sensitivity analysis to specific hyperparameters. Finally, we showcase the performance of models upon deployment and how we can derive a smaller footprint model with some accuracy trade-off, without the need to fine-tune.

### Experimental setup

**Models & datasets.** The datasets and models considered in our experiments span across four datasets, concisely presented along with the associated models on Tab. 1. We have implemented our solution with PyTorch (v1.13.0) trained our models on NVidia A100 (40G) GPUs. Details for the learning tasks and hyperparameters used are presented in the Appendix.

**Baselines.** We have selected various baselines from the literature that we believe are closest to aspects of our system. On the _pruning_ front, we compare with the IMP  and RareGems  techniques, themselves based on the LTH . On the _quantization_ front, we compare with XNOR-Net . With respect to _low-rank_ methods, we compare with Spectral Initialisation , Pufferfish  and Cuttlefish .

### Performance comparison

We start off by comparing Maestro with various baselines from the literature across different datasets and types of models3. Results are depicted in Tab. 2 and 3, while additional performance points of Maestro for different model footprints are presented in the Appendix F.2 and F.3.

**Comparisons with low-rank methods.** The low-rank methods we are comparing against are Pufferfish  and Cuttlefish . These methods try to reduce training and inference runtime while preserving model accuracy by leveraging low-rank approximations. For ResNet-18, we achieve 94.19\( 0.07\)% for 4.08M parameters and 93.97\( 0.25\)% for 2.19M parameters compared to the 94.17% of Pufferfish at 3.3M parameters. For VGG-19, we achieve +0.41pp (percentage points) higher accuracy compared to Pufferfish and -0.29pp to Cuttlefish at 44.8% and 93.2% of the sizes,

   Dataset & Model & \(\)**GMO** & \(\)**PGAN** & \(\)**PGAN** & \(\)**Task** \\ 
**MNIST** & LaNet & 2.4\(\)0.1 & 0.04 & large classification \\
**CIFAR10** & KOR4-18 & 0.56 & 1.18 & large classification \\
**M100** & KOR4-18 & 0.59 & 1.29 & large classification \\
**Tiny-ImageNet** & KOR4-18 & 0.59 & 2.59 & large classification \\
**Multi-task** & +1.50M Transformer & 1.37 & 45.98 & Training (cr.py) \\   

Table 1: Datasets and models for evaluation. The network footprints depict the vanilla variants of the models.

[MISSING_PAGE_FAIL:8]

### Accuracy-latency trade-off at training and deployment time

In Fig. 4, we illustrate various approaches to balance latency (proxied through MACs operations) and accuracy in model training and deployment. Fig. 3(a) demonstrates how Maestro (\(_{gl}=0\)) can be pruned effectively for deployment using the greedy search method discussed in Section 3.4. We contrast this with the greedy pruning of a non-factorized model that has been factorized using SVD. We reveal that this straightforward baseline does not measure up to the learned decomposition of Maestro and results in a significant performance decrease. Next, Fig. 3(b) portrays the final accuracy and the number of model parameters for varying hierarchical group lasso penalties. This leads to the optimal latency-accuracy balance for both training and inference. However, it's crucial to point out that each model was trained individually, while greedy pruning only necessitates a single training cycle. Lastly, we delve into the observation of nested ranks across increasing \(_{gl}\). Fig. 3(c) displays the performance of Maestro (\(_{gl}=0\)) across different ranks selected by smaller models Maestro (\(_{gl}>0\)). Intriguingly, we observe that Maestro (\(_{gl}=0\)) performs very well--for instance, we can decrease its operations in half (and parameters by 10\(\)) and still maintain an accuracy of \(87.7\%\) without fine-tuning, just by reusing rank structure from independent runs. As aforementioned, we intend to further explore this in the future.

## 6 Conclusion and future work

In this work, we have presented Maestro, a method for trainable low-rank approximation of DNNs that leverages progressive shrinking by applying a generalized variant of Ordered Dropout to the factorized weights. We have shown the theoretical guarantees of our work in the case of linear models and empirically demonstrated its performance across different types of models, datasets, and modalities. Our evaluation has demonstrated that Maestro outperforms competitive compression methods at a lower cost. In the future, we plan to expand our technique to encompass more advanced sampling techniques and apply it to different distributed learning scenarios, such as Federated Learning, where data are natively non-independent or identically distributed (non-IID).

Figure 4: Accuracy-latency trade-off of Maestro under different settings for VGG19 on CIFAR10.

Figure 3: Training dynamics of Maestro for ResNet18 on CIFAR10.