# MUBen: Benchmarking the Uncertainty of Molecular Representation Models

Yinghao Li\({}^{1}\), Lingkai Kong\({}^{1}\), Yuanqi Du\({}^{2}\), Yue Yu\({}^{1}\), Yuchen Zhuang\({}^{1}\), Wenhao Mu\({}^{1}\), Chao Zhang\({}^{1}\)

\({}^{1}\)Georgia Institute of Technology, Atlanta, GA \({}^{2}\)Cornell University, Ithaca, NY \({}^{1}\){yinghaoli,lkkong,yueyu,yczhuang,wmu30,chaozhang}@gatech.edu

\({}^{2}\)yd392@cornell.edu

###### Abstract

Large molecular representation models pre-trained on massive unlabeled data have shown great success in predicting molecular properties. However, these models may tend to overfit the fine-tuning data, resulting in over-confident predictions on test data that fall outside of the training distribution. To address this issue, uncertainty quantification (UQ) methods can be used to improve the models' calibration of predictions. Although many UQ approaches exist, not all of them lead to improved performance. While some studies have included UQ to improve molecular pre-trained models, the process of selecting suitable backbone and UQ methods for reliable molecular uncertainty estimation remains underexplored. To address this gap, we present MUBen, which evaluates different UQ methods for state-of-the-art backbone molecular representation models to investigate their capabilities. By fine-tuning various backbones using different molecular descriptors as inputs with UQ methods from different categories, we critically assess the influence of architectural decisions and training strategies. Our study offers insights for selecting UQ for backbone models, which can facilitate research on uncertainty-critical applications in fields such as materials science and drug discovery.

## 1 Introduction

The task of molecular representation learning is pivotal in scientific domains and bears the potential to facilitate research in fields such as chemistry, biology, and materials science . Nonetheless, supervised training typically requires vast quantities of data, which may be challenging to acquire due to the need for expensive laboratory experiments . With the advent of self-supervised learned Transformers  pioneered by BERT  and GPT , there has been a surge of interest in creating large-scale pre-trained molecular representation models from unlabeled datasets . Such models have demonstrated impressive representational capabilities, achieving state-of-the-art (SOTA) performance on a variety of molecular property prediction tasks .

However, in many applications, there is a pressing need for reliable predictions that are not only precise but also _uncertainty-aware_. The provision of calibrated uncertainty estimates allows us to distinguish "noisy" predictions and thus improve model robustness , or estimate data distributions, which can enhance downstream tasks such as active learning , high throughput screening , or wet-lab experimental design . Unfortunately, large-scale models can easily overfit the fine-tuning data and exhibit misplaced confidence in their predictions . Several works have introduced various uncertainty quantification (UQ) methods in molecular property prediction  and in neighboring research areas such as protein engineering . For instance,  extend a message passing neural network (MPNN)  with post-hoc recalibration  to address the overconfidence in molecular property prediction;  apply evidentialmessage passing networks [70; 2] to quantitative structure-activity relationship regression tasks;  utilize Bayesian optimization to conduct reliable predictions of Nanoporous material properties.

Despite their contributions, these studies exhibit several constraints: **1)** the variety of uncertainty estimation methods considered is limited with some effective strategies being overlooked; **2)** each research focuses on a limited range of properties such as quantum mechanics, failing to explore a broader spectrum of tasks; and **3)** none embraces the power of recent pre-trained backbone models, which have shown remarkable performance in prediction but may lead to different results in UQ. To the best of our knowledge, a comprehensive evaluation of UQ methods applied to pre-trained molecular representation models is currently lacking and warrants further investigation.

We present MUBen, a benchmark designed to assess the performance of UQ methods applied to the molecular representation models for property prediction on various metrics, as illustrated in Figure 1. It encompasses UQ methods from different categories, including deterministic prediction, Bayesian neural networks, post-hoc calibration, and ensembles (SS 3.2), on top of a set of molecular representation (backbone) models, each relies on a molecular descriptor from a distinct perspective (SS 3.1). MUBen delivers intriguing results and insights that may guide the selection of backbone model and/or UQ methods in practice (SS 4). We structure our code, available at [https://github.com/paper-submit-account/MUBen](https://github.com/paper-submit-account/MUBen), to be user-friendly, easily transferrable, and extendable, with the hope that this work will promote the future development of UQ methods, pre-trained models, or applications within the domains of materials science and drug discovery.

## 2 Problem Setup

Let \(\) denote a descriptor of a molecule, such as SMILES or a 2D topological graph. We define \(y\) as the label with domain \(y\{1,,K\}\) for \(K\)-class classification or \(y\) for regression. \(\) denotes all parameters of the molecular representation network and the task-specific layer. We assume that a training dataset \(\) consists of \(N\) i.i.d. samples \(=\{(_{n},y_{n})\}_{n=1}^{N}\). Our goal is to fine-tune the parameters \(\) on \(\) and learn a calibrated predictive probability model \(p_{}(y|_{n})\). The UQ metrics are briefly illustrated below with more details being provided in Appendix E.

Negative Log-LikelihoodNegative Log-Likelihood (**NILL**) is often utilized to assess the quality of model uncertainty on holdout sets for both classification and regression tasks. Despite being a proper scoring rule as per Gneiting's framework , certain limitations such as overemphasizing tail probabilities  make it inadequate to serve as the only UQ metric.

Brier ScoreBrier Score (**BS**)  is the mean squared error between the predicted probability and ground-truth label \(=_{y}(p_{}(y|_{n})-_{y=y_{n} })^{2}\), where \(\) is the indicator function. It serves as another proper scoring rule for classification tasks.

Calibration ErrorsCalibration Errors measure the correspondence between predicted probabilities and empirical accuracy. For classification, we use Expected Calibration Error (**ECE**) , which first divides the predicted probabilities \(\{p_{}(y_{n}_{n})\}_{n=1}^{N}\) into \(S\) bins \(B_{s}=\{n\{1,,N\} p_{}(y_{n}_{n})(_{s },_{s+1}]\}\) with \(s\{1,,S\}\) and then com

Figure 1: An overview of MUBen with all datasets, backbone models, UQ methods, and metrics enumerated. Elements in green and blue are related to classification and regression tasks, respectively.

pute the \(L_{1}\) loss between the accuracy and predicted probabilities within each bin: \(=_{s=1}^{S}|}{N}||}_{n B_{ s}}_{y=_{n}}-|}_{n B_{s}}p_{}( _{n}_{n})|\), where \(_{n}=_{y}p_{}(y_{n})\) is the \(n\)-th prediction. Similarly, for regression, we use Regression Calibration Error (**CE**), , which calculates the true frequency of the predicted points lying in each confidence interval against the predicted fraction of points in that interval: \(=_{s=1}^{S}(-|\{n\{1,,N\} F_ {}(y_{n}_{n})\}|/N)^{2}\), where \(\) is the expected quantile and \(F_{}(y_{n}|_{n})\) represents the predicted quantile value of \(y_{n}\), _e.g._, Gaussian CDF \((y_{n};_{n},_{n})\) parameterized by the predicted mean \(_{n}\) and variance \(_{n}\) if we assume the labels are subject to the Gaussian distribution.

## 3 Experiment Setup

### Backbone Models

Molecular descriptors, such as fingerprints, SMILES strings, and graphs, package the structural data of a molecule into a format that computational algorithms can readily process. When paired with distinct model architectures carrying divergent inductive biases, these descriptors can offer varied benefits depending on the specific task. Therefore, we select \(4\) primary backbone models that accept distinct descriptors as input. We also include \(2\) supplementary backbones to cover other aspects such as model architecture and pre-training objectives.

For primary backbone models, we select various pre-trained Transformer encoder-based models, each is SOTA or close to SOTA for their respective input format: **1) ChemBERTa**, which accepts SMILES strings as input; **2) GROVER**, pre-trained with 2D molecular graphs via a Transformer bolstered by a dynamic message-passing graph neural network; and **3) Uni-Mol**, which incorporates 3D molecular conformations into its input, and pre-trains specialized Transformer architectures to encode the invariant spatial positions of atoms and represent the edges between atom pairs. We also implement a **4)** fully connected deep neural network (**DNN**) that uses \(200\)-dimensional RDKit features which are regarded as fixed outputs of a hand-crafted backbone feature generator. This simple model aims to highlight the performance difference between heuristic feature generators and the automatic counterparts that draw upon self-supervised learning processes for knowledge acquisition. In addition, we have two other backbones: **5) TorchMD-NET**, as introduced in  and pre-trained according to . This model is designed to handle 3D inputs and leverages an equivariant Transformer architecture aimed at quantum mechanical properties. **6) GIN** from , which processes 2D molecular graphs. GIN is theoretically among the most powerful graph neural networks (GNNs) available, offering an additional randomly initialized backbone baseline with less sophisticated input features. Compared with the primary one, these models are constrained in pre-training: TorchMD-NET is exclusively pre-trained on quantum mechanical data, whereas GIN does not involve any pre-processed features. Please refer to appendix C for more details.

### Uncertainty Quantification Methods

Deterministic Uncertainty PredictionIn standard practice, deep classification networks utilize SoftMax or Sigmoid outputs to distribute prediction weights among target classes. Such weights serve as an estimate of the model's uncertainty. In regression tasks, rather than one single output value, models often parameterize an independent Gaussian distribution with predicted means and variances for each data point, and the variance magnitude acts as an estimate of the model's uncertainty.

An alternate loss function for classification is the **Focal Loss**, which minimizes a regularised KL divergence between the predicted values and the true labels. The regularization increases the entropy of the predicted distribution while the KL divergence is minimized, mitigating the model overconfidence and improving the uncertainty representation.

Bayesian Learning and InferenceBNNs estimate the probability distribution over the parameters of the network. Specifically, BNNs operate on the assumption that the network layer weights follow a specific distribution that is optimized through maximum a posteriori estimation during training. During inference, multiple network instances are randomly sampled from the learned distributions. Each instance then makes independent predictions, and the prediction distribution inherently captures the desired uncertainty information. Some notable methods that employ this approach include:* Bayes by Backprop (**BBP**)  first introduces Monte Carlo gradients, an extension of the Gaussian reparameterization trick , to learn the posterior distribution directly through backpropagation.
* Stochastic Gradient Langevin Dynamics (**SGLD**)  applies Langevin dynamics to infuse noise into the stochastic gradient descent training process, thereby mitigating the overfitting of network parameters. The samples generated via Langevin dynamics can be used to form Monte Carlo estimates of posterior expectations during the inference stage.
* **MC Dropout** demonstrates that applying dropout is equivalent to a Bayesian approximation of the deep Gaussian process . Uncertainty is derived from an ensemble of multiple stochastic forward passes with dropout enabled.
* SWA-Gaussian (**SWAG**)  provides an efficient way of estimating Gaussian posteriors over network weights by utilizing stochastic weight averaging (SWA)  combined with low-rank & diagonal Gaussian covariance approximations.

Post-Hoc CalibrationPost-hoc calibration is proposed to address the over-confidence issue of deterministic classification models by adjusting the output logits after training. The most popular method is **Temperature Scaling**, which adds a learned scaling factor to the Sigmoid or SoftMax output activation to control their "spikiness".  shows that this simple step improves model calibration in general, yielding better UQ performance.

Deep EnsemblesThe ensemble method has long been used in machine learning to improve model performance  but was first adopted to estimate uncertainty in . It trains a deterministic network multiple times with different random seeds and combines their predictions at inference. The ensembles can explore training modes thoroughly in the loss landscape and thus are robust to noisy and out-of-distribution examples .

### Datasets

We carry out our experiments on MoleculeNet , a collection of widely utilized datasets covering molecular properties such as quantum mechanics, solubility, and toxicity. For classification, MUBen incorporates BBBP, ClinTox, Tox21, ToxCast, SIDER, BACE, HIV, and MUV with all labels being binary; the first \(5\) concern **physiological** properties and the last \(3\)**biophysics**. For regression, we select ESOL, FreeSolv, Lipophilicity, QM7, QM8, and QM9, of which the first \(3\) contain **physical chemistry** properties and the rest fall into the **quantum mechanics** category. In line with previous studies , MUBen divides all datasets with scaffold splitting to minimize the impact of dataset randomness and, consequently, enhances the reliability of the evaluation. Moreover, scaffold splitting alienates the molecular features in each dataset split, inherently creating a challenging out-of-distribution (OOD) setup that better reflects the real-world scenario. For comparison, we also briefly report the results of random splitting. More details are in appendix B.

Figure 2: MRR of the backbone models for each metric, each is macro-averaged from the reciprocal ranks of the results of all corresponding UQ methods on all datasets. MRRs accentuate top results, offering a complementary perspective on the backbone performance.

## 4 Results and Analysis

Prediction PerformanceTheoretically, inserting UQ methods into the training pipeline does not guarantee better prediction on i.i.d. datasets. However, since we ensure OOD test points with scaffold splitting, UQ methods may mitigate the distribution gap, yielding better test results. The columns ROC-AUC, RMSE, and MAE in Tables 1, 2, and Figure 2 illustrate the predictive performance of each method. Examining from the lens of UQ methods, none provides a consistent guarantee of performance improvement over direct prediction, except for Deep Ensembles, and MC Dropout for regression. The randomness in the initialization and training trajectory of Deep Ensembles explores a broader range of loss landscapes, which partially addresses the distribution shift issue, as observed by . MC Dropout samples multiple sets of model parameters from the Gaussian distributions centered at the fine-tuned deterministic net weights, which may flatten extreme regression abnormality triggered by OOD features. This phenomenon is less pronounced for classification due to the \((0,1)\) output domain. However, other BNNs do not exhibit the same advantage. SWAG, while similar to MC Dropout _w.r.t._ training, might intensify training data overfitting due to the additional steps taken

    &  &  &  \\   & ROC+ & ECE\(\) & NLL\(\) & BS\(\) & ROC+ & ECE\(\) & NLL\(\) & BS\(\) & ROC+ & ECE\(\) & NLL\(\) & BS\(\) \\   \\  Deterministic & 0.7386 & 0.0417 & 0.2771 & 0.0779 & 0.6222 & 0.1168 & 0.4436 & 0.1397 & 25.75 & 17.25 & 24.13 & 22.88 \\ Temperature & 0.7386 & **0.0342** & 0.2723 & 0.0773 & 0.6220 & 0.1114 & 0.4882 & 0.1398 & 25.75 & 15.38 & 21.25 & 19.88 \\ Focal Loss & 0.7374 & 0.1058 & 0.3161 & 0.0781 & 0.6289 & 0.1254 & 0.4389 & 0.1396 & 25.83 & 24.38 & 22.38 & 24.00 \\ MC Dropout & 0.7376 & 0.0356 & 0.2727 & 0.0763 & 0.6248 & 0.1093 & 0.4319 & 0.1358 & 26.50 & 13.00 & 19.38 & 18.63 \\ SWAG & 0.7564 & 0.0438 & 0.2793 & 0.0790 & 0.6207 & 0.1175 & 0.4441 & 0.1400 & 26.38 & 18.50 & 25.63 & 23.50 \\ BBP & 0.7243 & 0.0422 & 0.2847 & 0.0814 & 0.0620 & 0.1443 & 0.4733 & 0.1310 & 22.75 & 19.13 & 9.13 & 21.38 \\ SGLD & 0.7257 & 0.1192 & 0.3455 & 0.0798 & 0.5319 & 0.3054 & 0.6685 & 0.2378 & 27.75 & 26.00 & 28.88 & 27.88 \\ Ensembles & **0.7540** & 0.0344 & **0.2648** & **0.0746** & **0.6486** & **0.0900** & **0.4008** & **0.1292** & **20.00** & **7.18** & **11.75** & **13.13** \\   \\  Deterministic & 0.7542 & 0.0571 & 0.2962 & 0.0812 & 0.6554 & 0.1209 & 0.4313 & 0.1330 & 15.63 & 17.38 & 18.88 & 19.38 \\ Temperature & 0.7542 & 0.0424 & 0.2744 & 0.0792 & 0.6540 & 0.1067 & 0.8117 & 0.1313 & 15.83 & 12.00 & 13.88 & 15.38 \\ Focal Loss & 0.7523 & 0.0069 & 0.3052 & 0.0645 & 0.6421 & 0.1197 & 0.4243 & 0.1346 & 17.63 & 20.13 & 17.88 & 20.63 \\ MC Dropout & 0.7641 & 0.0423 & 0.2697 & **0.0744** & 0.0632 & 0.1069 & 0.4070 & 0.1276 & 12.50 & **10.75** & **10.63** & **10.00** \\ SWAG & 0.7583 & 0.0592 & 0.3008 & 0.0818 & 0.6556 & 0.1202 & 0.4305 & 0.1327 & 16.13 & 19.50 & 21.28 & 20.38 \\ BBP & 0.7433 & 0.0499 & 0.2765 & 0.0780 & 0.5814 & 0.12745 & 0.4469 & 20.38 & 19.00 & 16.00 & 19.50 \\ SGLD & 0.7475 & 0.0504 & 0.2784 & 0.0795 & 0.5436 & 0.2238 & 0.5602 & 0.1881 & 21.13 & 19.88 & 19.13 & 18.63 \\ Ensembles & **0.7681** & **0.0440** & **0.2679** & 0.0750 & **0.6273** & **0.1037** & **0.3966** & **0.1258** & **12.38** & 13.00 & 11.63 & 12.25 \\   \\  Deterministic & 0.7808 & 0.0358 & 0.2473 & 0.0694 & 0.6587 & 0.1043 & 0.4091 & 0.1298 & 11.63 & 11.50 & 8.88 & 9.88 \\ Temperature & 0.7810 & **0.0297** & 0.2439 & 0.0688 & 0.6496 & 0.1424 & 0.1422 & 12.63 & 8.88 & 7.50 & 9.25 \\ Focal Loss & 0.7719 & 0.1148 & 0.3052 & 0.0811 & 0.6359 & 0.1221 & 0.4356 & 0.1350 & 15.00 & 23.38 & 21.50 & 22.25 \\ MC Dropout & 0.7817 & 0.0346 & 0.2455 & 0.0693 & 0.6615 & **0.1009** & **0.4042** & **0.1288** & 11.25 & 10.50 & 7.63 & 8.63 \\ SWAG & 0.7837 & 0.0360 & 0.2482 & 0.0689 & 0.6603 & 0.1006 & 0.4411 & 0.1301 & 9.13 & 11.63 & 8.88 & 8.38 \\ BBP & 0.7697 & 0.0348 & 0.2552 & 0.0711 & 0.5995 & 0.171 & 0.5900 & 0.16675 & 2.00 & 14.75 & 15.58 \\ SGLD & 0.7635 & 0.0402 & 0.2558 & 0.0716 & 0.5542 & 0.2712 & 0.6194 & 0.2139 & 18.75 & 18.63 & 15.25 & 15.63 \\ Ensembles & **0.756** & 0.0316 & **0.2411** & **0.0675** & **0.6466** & 0.1034 & 0.4061 & 0.1290 & **8.50** & **8.13** & **5.80** & **6.88** \\   \\  Deterministic & 0.7895 & 0.0454 & 0.2601 & 0.0716 & 0.6734 & 0.1020 & 0.3983 & 0.1274 & 11.50 & 15.50 & 16.13 & 13.88 \\ Temperature & 0.7896 & 0.0346 & 0.2483 & 0.0704 & **0.7028** & 0.1456 & 0.4566 & 0.1355 & 11.00 & 12.00 & 13.13 & 12.75 \\ Focal Loss & 0.7904 & 0.0792 & 0.2999 & 0.0785 & 0.0693 & 0.1227 & 0.4079 & 0.1284 & 10.00 & 22.50 & 19.38 & 20.50 \\ MC Dropout & 0.7891 & 0.0480 & 0.2628 & 0.0726 & 0.6833 & 0.1074 & 0.0455 & 0.1274 & 12.88 & 19.98 & 19.25 & 17.25 \\ SWAG & 0.7842 & 0.0593 & 0.2994 & 0.0728 & 0.0670 & 0.1085 & 0.4005 & 0.1271 & 10.13 & 22.13 & 22.25 & 18.25 \\ BBP & 0.792 & 0.0392 & 0.0520 & 0.0703 & 0.6273 & 0.1296 & 0.4522 & 0.4526 & 12.13 & 17.00 & 15.50 & 16.25 \\ SGLD & 0.7887 & 0.0433 & 0.2569 & 0.0684 & 0.5700 & 0.1953 & 0.5207 & 0.1717 & 14.75 & 18.13 & 18.88 & 14.50 \\ Ensembles & **0.8052** & **0.8332** & **0.8239** & **0.8662** & 0.6841 & **0.0953** & **0.3087** & **0.1247** & **5.13** & **8.88** & **7.63** & **6.90** \\  fit the parameter distribution. The stochastic sampling employed by BBP and SGLD complicates network training and may impact the prediction performance as a result.

Looking from the perspective of primary backbones, Uni-Mol secures the best prediction performance for both classification and regression. The superior molecular representation capability of Uni-Mol is attributed to the large network size, the various pre-training data and tasks, and the integration of results from different conformations of the same molecule. When contrasting DNN, ChemBERTa, and GROVER, it becomes apparent that the expressiveness of the molecular descriptors varies for different molecules/tasks. Interestingly, pre-trained models do not invariably surpass heuristic features when integrated with UQ methods. Selecting a model attuned to the specific task is advised over an indiscriminate reliance on "deep and complex" networks.

Uncertainty Quantification PerformanceTables 1, 2 and Figure 2 depict the uncertainty estimation performances via ECE, NLL, Brier score, and CE columns. One discernible trend is the consistent performance enhancement from Deep Ensembles even when the number of ensembles is limited, such as the QM9 case (appendix D). MC Dropout also exhibits a similar trend, albeit less

    &  &  &  \\   & RMSE & MAE & NLL & CE & RMSE & MAE & NLL & CE & RMSE & MAE & NLL & CE \\   \\  Deterministic & 0.7575 & 0.5793 & 0.6154 & 0.0293 & 0.01511 & 0.01012 & -3.379 & 0.04419 & 16.67 & 15.67 & 11.33 & 10.50 \\ MC Dropout & 0.7559 & 0.5773 & 0.9071 & 0.0341 & 0.01480 & 0.01000 & -3.526 & 0.04327 & 15.17 & 15.17 & 11.33 & 10.67 \\ SWAG & 0.7572 & 0.5823 & 0.7191 & 0.0308 & 0.01524 & 0.01019 & -3.284 & 0.04959 & 18.00 & 17.67 & 14.00 & 12.50 \\ BBP & 0.7730 & 0.5938 & 0.7578 & 0.0305 & 0.01534 & 0.01025 & -3.347 & 0.04452 & 21.17 & -20.50 & 10.33 & 7.33 \\ SGLD & 0.7468 & 0.5743 & **0.2152** & **0.0090** & 0.01958 & 0.01437 & -3.335 & **0.00702** & 19.83 & 19.33 & 6.83 & **1.83** \\ Ensembles & **0.7172** & **0.5490** & 0.6165 & 0.0322 & **0.01430** & **0.00956** & **-3.002** & 0.04632 & **11.50** & **11.17** & **6.33** & 8.67 \\   \\  Deterministic & 0.7553 & 0.5910 & 1.2368 & 0.0362 & 0.01464 & 0.00916 & -2.410 & 0.05468 & 17.83 & 16.67 & 18.83 & 14.67 \\ MC Dropout & **0.7142** & **0.5602** & 0.8178 & 0.0349 & 0.01412 & 0.00880 & -3.150 & 0.05133 & **14.33** & **13.83** & 15.00 & 13.67 \\ SWAG & 0.7672 & 0.5992 & 1.5809 & 0.01477 & 0.00925 & -2.170 & 0.05535 & 19.33 & 18.50 & 20.33 & 15.83 \\ BBP & 0.7542 & 0.5809 & **0.4429** & **0.2179** & 0.04134 & 0.00928 & -2.953 & 0.5399 & 0.5399 & 17.67 & 18.50 & 10.83 & 9.00 \\ SGLD & 0.7622 & 0.5982 & 0.8719 & 0.0355 & 0.01530 & 0.01012 & -**3.758** & **0.03378** & 19.83 & 20.50 & **9.50** & **8.50** \\ Ensembles & 0.7367 & 0.5763 & 0.9756 & 0.0360 & **0.012397** & **0.06088** & -2.876 & 0.05425 & 14.83 & **13.83** & 14.67 & 12.67 \\   \\  Deterministic & 0.6316 & 0.4747 & 2.1512 & 0.0478 & 0.01148 & 0.00678 & -0.787 & 0.06206 & 10.67 & 11.67 & 17.60 & 16.00 \\ MC Dropout & 0.6293 & 0.4740 & 0.2052 & 0.0476 & 0.01410 & 0.00676 & -1.100 & 0.06161 & 9.33 & 11.17 & 16.00 & 14.33 \\ SWAG & 0.6317 & 0.4750 & 2.3980 & 0.0485 & 0.01156 & 0.00678 & -4.077 & 0.05252 & 12.33 & 13.33 & 20.33 & 17.83 \\ BBP & 0.6481 & 0.5068 & 0.0789 & 0.0196 & 0.01179 & 0.06700 & -1.885 & 0.05909 & 14.17 & 15.67 & 7.67 & 4.00 \\ SGLD & 0.6360 & 0.4984 & **0.0542** & **0.0123** & 0.01399 & 0.00878 & -**3.785** & **0.02911** & 14.83 & 15.17 & **4.67** & **2.67** \\ Ensembles & **0.6250** & **0.4093** & 1.6016 & 0.0460 & **0.011413** & **0.004667** & -1.028 & 0.06199 & **8.17** & **8.67** & 13.50 & 14.00 \\   \\  Deterministic & 0.6079 & 0.4509 & 0.8975 & 0.0425 & 0.00962 & 0.00538 & 0.014 & 0.06637 & 5.83 & 4.83 & 16.67 & 21.17 \\ MC Dropout & 0.5938 & 0.4438 & 1.3663 & 0.0440 & 0.00661 & 0.00355 & -0.251 & 0.06615 & 4.00 & 3.17 & 16.67 & 21.33 \\ SWAG & 0.6026 & 0.4476 & 1.0101 & 0.0453 & 0.00696 & 0.00541 & -4.062 & 0.0697 & 6.33 & 6.17 & 19.67 & 22.67 \\ BBP & 0.6044 & 0.4469 & **0.4079** & **0.0306** & 0.00952 & 0.00544 & -2.959 & 0.06179 & 3.17 & 3.00 & 4.33 & 10.33 \\ SGLD & 0.6400 & 0.4554 & 0.1565 & 0.0329 & 0.00950 & 0.00545 & -**4.209** & **0.06593** & **2.50** & 4.00 & **2.50** & **9.17** \\ Ensembles & **0.5899** & **0.4266** & 0.650 & 0.0438 & **0.009526** & -0.319 & 0.0629 & **2.50** & **1.83** & 11.00 & 20.67 \\    & 0.8196 & 0.8619 & 0.0195 & 0.00860 & 0.00464 & 2.262 & 0.06868 & - & - & - & - \\ GIN(tm) & 0.8071 & 0.6515 & 0.3241 & 0.0020 & 0.01295 & 0.00814 & -3.521 & 0.04997 & - & - & - & - \\   

Table 2: Regression results (lower is better) including two example datasets and the average ranking.

Figure 3: Calibration plot of example uncertainty estimation results. For both datasets, we draw the calibration based on the aggregated outputs from all tasks.

pronounced. Despite a marginal compromise in prediction accuracy, Temperature Scaling emerges as another method that almost invariably improves calibration, as evidenced in the average ranking columns--a finding that aligns with . Figure 3 shows that deterministic prediction tends to be over-confident, and Temperature Scaling mitigates this issue. However, it is susceptible to calibration-test alignment. If the correlation is weak, Temperature Scaling may worsen the calibration, as presented in the ToxCast dataset. In contrast, Focal Loss does not work as impressively for binary classification, which is also observed by . It is hypothesized that while the Sigmoid function sufficiently captures model confidence in binary cases, Focal Loss could over-regularize parameters, diminishing the prediction sharpness to an unreasonable value, a conjecture verified by the 'S'-shaped calibration curves in Figure 3.

Although limited in classification efficacy, both BBP and SGLD deliver commendable performance in predicting regression uncertainty, capturing \(7\) out of \(8\) top ranks for \(4\) backbones on \(2\) metrics, narrowly missing out to Deep Ensembles for the remaining one (Table 2). Yet, their inconsistent improvement of RMSE and MAE implies a greater influence on variance prediction than mean. Figure 4 reveals SGLD's tendency to "play safe" by predicting larger variances, while the deterministic method is prone to over-confident by ascribing small variances even to its inaccurate predictions. In addition, we do not observe a better correlation between SGLD's error and variance. We assume that the noisy training trajectory prevents SGLD and BBP to sufficiently minimize the gap between the predicted mean and true labels, thus encouraging them to maintain larger variances to compensate for the error. Please refer to appendix D.2 and appendix F.1 for more analysis on UQ performance.

Figure 2 indicates that Uni-Mol exhibits subpar calibration, particularly for regression. Comparing Uni-Mol, ChemBERTa, and DNN in Figure 4, we notice that larger models such as Uni-Mol are more confident to their results, as illustrated by their smaller variances and larger portion of (std, error) points exceed above the \(y=kx\). This could be attributed to shared structural features in 3D conformations in the training and test molecules that remain unobserved in simpler descriptors. While this similarity benefits property prediction, it also potentially misleads the model into considering data points as in-distribution, thereby erroneously assigning high confidence.

TorchMD-NET and GINOur analysis also encompasses TorchMD-NET and GIN, two additional backbone models excluded from the primary benchmark due to their limited capabilities. As presented in the tables and Figure 5, TorchMD-NET's performance is on par with Uni-Mol when predicting

Figure 4: The absolute error between the model-predicted mean and true labels against the predicted standard deviation. We compare the performance of SGLD with the deterministic prediction on different backbones and datasets. The “\(y=kx\)” lines indicate whether the true labels lie within the \(k\)-std range of the predicted Gaussian. Also, a model is perfectly calibrated when its output points are arranged on an “\(y=kx\)” line for an arbitrary \(k\).

Figure 5: MRRs of TorchMDNet and Uni-Mol on datasets grouped by dataset property categories. MRR calculations are confined to results from these two backbones. Only relative values matter.

quantum mechanical properties but falls short in others. This outcome aligns with expectations, given that TorchMD-NET's architecture is tailored specifically for predicting quantum mechanical properties . Moreover, it is pre-trained on the relatively niche dataset PCQM4Mv2  with only the denoising objective, which might be suitable for molecular dynamics but limited for other properties. In contrast, Uni-Mol stands out as a versatile model, benefiting from diverse pre-training objectives that ensure superiority across various tasks. On the other hand, GIN's performance is consistently inferior to other models including DNN, with examples presented in Tables 1 and 2, likely due to the limited expressiveness of 2D graphs and the GNN architecture when pre-training is absent. Please refer to appendix F for complete results.

Frozen Backbone and Randomly Split DatasetsTable 3 demonstrates a notable drop in prediction performance when backbone weights are fixed; and random splits outperform scaffold splits. This is consistent with intuition: if backbone models serve solely as feature extractors instead of a part of the trainable predictors, they are less expressive for downstream tasks. Additionally, in-distribution features tend to be more predictable. An interesting exception emerges in regression calibration error, where frozen backbones perform better and random splits score lower. Upon examining the predicted values, we note that predictions for random splits exhibit a sharper distribution, _i.e._, smaller \(\). This suggests that the models are more confident in regressing in-distribution data, aligning with our previous observation for Uni-Mol. Conversely, frozen backbones are less prone to overfitting due to their constrained expressiveness. This behavior underscores the original models' capability to distinguish between in-distribution and OOD features and assign confidence scores with precision.

## 5 Conclusion, Limitation, and Future Works

We introduce MUBen, a benchmark that assesses an array of UQ methods across diverse categories, leveraging backbone models that incorporate different descriptors for multiple molecular property prediction tasks. Our results reveal that Deep Ensembles assures performance enhancement under all circumstances but with significant computation cost. Temperature scaling and MC Dropout are simple yet effective for classification, while BBP and SGLD are better suited for regression UQ. Among backbones, Uni-Mol, leveraging the expressiveness of 3D molecular conformations, is the most powerful but prone to overconfidence. Backbones with other descriptors are advantageous in different conditions and should be selected according to specific usage cases.

Given the rapid advancements in molecular representation learning and UQ methods, MUBen cannot encompass all possible combinations, forcing us to focus on a curated selection of representative methods. Furthermore, we use coarse-grained hyperparameter grids to maintain experimental feasibility, which makes MUBen, while indicative of trends, might not present the best possible results. We remain committed to refining MUBen and welcome contributions from the broader community to enhance its inclusivity and utility for research in this field and related domains.