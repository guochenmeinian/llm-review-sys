# Training Dynamics of Transformers to Recognize Word Co-occurrence via Gradient Flow Analysis

Hongru Yang

The University of Texas at Austin

& Princeton University

hy6385@utexas.edu

&Bhavya Kailkhura

Lawrence Livermore National Laboratory

kailkhura1@llnl.gov

&Zhangyang Wang

The University of Texas at Austin

atlaswang@utexas.edu

&Yingbin Liang

The Ohio State University

liang.889@osu.edu

Work done while doing a internship at Lawrence Livermore National Laboratory and visiting Princeton University.

###### Abstract

Understanding the training dynamics of transformers is important to explain the impressive capabilities behind large language models. In this work, we study the dynamics of training a shallow transformer on a task of recognizing co-occurrence of two designated words. In the literature of studying training dynamics of transformers, several simplifications are commonly adopted such as weight reparameterization, attention linearization, special initialization, and lazy regime. In contrast, we analyze the gradient flow dynamics of simultaneously training three attention matrices and a linear MLP layer from random initialization, and provide a framework of analyzing such dynamics via a coupled dynamical system. We establish near minimum loss and characterize the attention model after training. We discover that gradient flow serves as an inherent mechanism that naturally divide the training process into two phases. In Phase 1, the linear MLP quickly aligns with the two target signals for correct classification, whereas the softmax attention remains almost unchanged. In Phase 2, the attention matrices and the MLP evolve jointly to enlarge the classification margin and reduce the loss to a near minimum value. Technically, we prove a novel property of the gradient flow, termed _automatic balancing of gradients_, which enables the loss values of different samples to decrease almost at the same rate and further facilitates the proof of near minimum training loss. We also conduct experiments to verify our theoretical results.

## 1 Introduction

Ever since the invention of self-attention , transformers have become a dominating backbone architecture in many machine learning applications such as computer vision  and natural language processing . Nowadays, ChatGPT and GPT-4  have demonstrated astonishing abilities in many areas such as language understanding, mathematics and coding, which have sparked artificial general intelligence . In the meantime, there has been a burgeoning development of large language models (LLMs)  as well as multi-modal models .

Despite the huge empirical success, theoretical understanding of why a pre-trained language model can possess such impressive performance has been significantly lagging behind. Some previousefforts have been made in understanding the capacity and representational power of transformers . However, most results of this type of works are existential and rely on manual construction of the weights. It is unclear whether the constructed weights are the actual solutions after training transformers. In order to understand the mechanism behind those pre-trained language models, a line of studies have aimed to open the black box of optimization via studying training dynamics of transformers and explaining why transformers can be trained to perform well . However, those previous works often relied on various simplifications in their analysis such as weight reparameterization, attention linearization, special initialization, lazy regime, etc. One goal of this paper is to take a further step to demystify the training dynamics of transformers and consider more practical training setup, thus better capturing the actual training process.

Our study of transformers' training dynamics will focus on a basic problem of recognizing co-occurrence of words under a binary classification setup, which is an important ability of LLMs to perform many tasks correctly in natural language processing (NLP). For example, the classical n-gram model  predicts the next word based on co-occurrence of multiple words. Consider the following scenario: if the task for the language model is to read a paragraph describing a children and then answer some questions, say, "Is Bob eating a banana?". In order to answer the question correctly, the model must be able to detect the co-occurrence of the two words "Bob" and "banana" in the paragraph. Motivated by this, we study the problem of detecting co-occurrence of two target words via the model of a one-layer transformer with a self-attention module followed by a linear multi-layer perceptrons (MLP) layer. Our goal is to characterize the dynamics of the training process via the gradient flow analysis, thus providing a theory to explain how transformers can be trained to perform well.

Our contribution is summarized below:

* We study the gradient flow dynamics of detecting word co-occurrence. The training starts with random initialization and then simultaneously updates _four_ weight matrices (including key, query, and value matrices and a linear MLP) in the transformer architecture via gradient flow. We show that gradient flow can achieve small loss although the loss function is highly nonconvex. We further characterize the explicit form of attention matrices after training, which captures the strong positive correlation between the two target signals and strong negative correlation between one target signal and the common token, both leading to large classification margin.
* We characterize the training process into two phases. In Phase 1 (alignment of MLP for correct classification), we show that the linear MLP of the transformer quickly aligns with the two target word tokens whereas all other variables in the dynamical system stay almost unchanged from their initialization values. All training samples are correctly classified at the end of Phase 1, but the loss value is still large due to small classification margin. In Phase 2 (evolution of attention and MLP for large classification margin), along with the continual evolution of MLP, correct classification by MLP also encourages the gradients of attention matrices to learn. Specifically, the softmax probability increases if the key and query tokens correspond to the two target words, and the value transform of the two words becomes more positively correlated, both leading to enlarge the classification margin. Thus, the training and test loss values both are driven down to nearly zero.
* Technically, our proof techniques do not rely on several commonly used assumptions in the literature such as weight reparameterization, attention linearization, special initialization, lazy regime, etc. Our main idea is to treat the problem as a _coupled_ dynamical system with _six_ different types of dynamic variables, for which we provide an articulated analysis on the gradient flow dynamics. In particular, we prove a novel property of the gradient flow, termed _automatic balancing of gradients_, which shows that the ratio of several important gradients will evolve closely within the same range during training. This enables us to show that the losses of all training samples can decrease almost at the same rate, and is also a key component in proving the near minimum training loss as well as analyzing the changes of softmax.

### Related Work

**Transformer representational power.** Several previous works have studied the expressiveness of transformers. One line of work was from a universal approximation perspective and thus provided the existential results . As a separate view,  showed that a single attention head can represent a sparse function over the input sequence with sample complexity much smaller than the context length.  studied the approximation and generalization performance of transformers in in-context learning.  proved that transformers can represent certain functions more efficiently than MLPs.

**Training transformers.** Various settings of training transformers have been studied recently.  studied the impact of head and prompt tuning of transformer on the downstream learning tasks.  proved that transformers can learn spatial structures.  studied how a shallow transformer learns a dataset with both label-relevant and label-irrelevant tokens.  studied a next-token prediction problem and showed that self-attention behaves like a discriminating scanning algorithm.  analyzed a layer-wise optimization scheme on how transformers learn topic structures.  studied a setting where transformers can learn a SVM solution.  provided analysis of training graph transformers for node classification tasks.  studied the implicit bias in the next-token prediction problem. For in-context linear regression,  constructed transformer weights to solve this task and showed empirically that this is similar to what the transformer learned by gradient descent,  proved that the critical points of the training objective of linear transformers implement a pre-conditioned gradient descent,  provided the training dynamics of linear attention models,  characterized the training dynamics of softmax transformers, and  studied a multi-task linear regression problem with a multi-headed softmax transformer. Further,  focused on nonlinear self-attention and nonlinear MLP over classification tasks in in-context learning.  proved the convergence of transformers via neural tangent kernel.  showed that two-layer transformers can learn causal structure via gradient descent.  developed algorithms for provably learning a multi-head attention layer.  studied how transformers learn feature-position correlation.

This paper studies a different problem of detecting co-occurrence of words via transformers. Such a setting has not been considered in the literature. More importantly, the previous studies of training dynamics of transformers have adopted various assumptions/simplifications such as weight reparameterization, special initialization, attention linearization, lazy regime, etc. In contrast, our analysis here based on gradient flow does not rely on those simplifications, which can be of independent interest for studying transformers in other settings.

## 2 Problem Setting

**Notations.** For a vector \(v^{d}\), we use \((v)\) to denote a diagonal matrix with \(v\) being the diagonal entries. When we subtract the vector \(v\) by a scalar \(a\), we subtract each entry of \(v\) by \(a\), i.e., \(v-a^{d}\) and \((v-a)_{i}=v_{i}-a\). We use \(,,\) to hide polylogarithmic factors.

### Data Model

**Definition 2.1** (Data distribution).: _Given a set of orthonormal vectors \(\{_{i}\}_{i=1}^{d}\) as word embedding, let \(_{1},_{2}^{d}\) be two target signals whose co-occurrence needs to be detected by the model, and let \(_{3}^{d}\) be a common token vector. A data entry \((X,y)^{d L}\{ 1\}\), where \(X=[x_{1},x_{2},,x_{L}]\) consists of \(L\) tokens, is generated by the distribution \(\) as follows:_

1. _Uniformly randomly select an index_ \(i_{3}[L]\) _and set_ \(x_{i_{3}}=_{3}\)_._
2. _Then, one of the following cases occurs:_ * _With probability_ \(1/2\)_, set_ \(y=1\) _and uniformly randomly select two indices_ \(i_{1} i_{2}[L]\{i_{3}\}\) _and set_ \(x_{i_{1}}=_{1},\;x_{i_{2}}=_{2}\)_. For_ \(i[L]\{i_{1},i_{2},i_{3}\}\)_, set_ \(x_{i}=(\{_{i}\}_{i=4}^{d})\)_._ * _With probability_ \(1/6\)_, set_ \(y=-1\) _and uniformly randomly select one index_ \(i_{1}[L]\{i_{3}\}\) _and set_ \(x_{i_{1}}=_{1}\)_. For_ \(i[L]\{i_{3},i_{1}\}\)_, we set_ \(x_{i}=(\{_{i}\}_{i=4}^{d})\)_._ * _With probability_ \(1/6\)_, set_ \(y=-1\) _and uniformly randomly select one index_ \(i_{2}[L]\{i_{3}\}\) _and set_ \(x_{i_{2}}=_{2}\)_. For_ \(i[L]\{i_{3},i_{2}\}\)_, we set_ \(x_{i}=(\{_{i}\}_{i=4}^{d})\)_._ * _With probability_ \(1/6\)_, set_ \(y=-1\)_. For all_ \(i[L]\{i_{3}\}\)_, we set_ \(x_{i}=(\{_{i}\}_{i=4}^{d})\)_._

In summary, there are 4 types of data: (1) both \(_{1},_{2}\) appear, (2) only \(_{1}\) appears, (3) only \(_{2}\) appears, and (4) neither \(_{1}\) nor \(_{2}\) appears. We denote the set of indices of the above 4 different types of data by \(I_{1},I_{2},I_{3},I_{4}[n]\). We further define \(=\{_{i}\}_{i=4}^{d}\). For simplicity, our data distribution assumes \(_{1},_{2},_{3}\) appear only once in a data entry. The occurrence probability of each type of data is chosen in the above way to make the distribution label-balanced. We assume there is a fixed set of orthonormal vectors as word embedding, which is analogous to the one-hot embedding of a set of vocabularies. Furthermore, in our daily language, there are some words appearing in almost every sentence such as "a" and "the". Thus, to model those words, we include a common token in every data entry. Finally, notice that if we ignore the common token and random tokens, the data distribution simplifies to a logical AND problem.

**Remark 2.2**.: _Recognizing co-occurrence of words is an important ability for language models to perform many NLP tasks correctly. Consider the example of a language model first reading a paragraph describing a children and then answering the question "Is Bob eating a banana?" If the description is "Bob is watching a television while eating a banana", then the model should answer "Yes". If the description is "Bob is playing computer games", then the model should answer "No". Thus, the model needs to recognize the co-occurrence of "Bob" and "banana"._

For simplicity of our analysis, we make the following assumption on our training data set.

**Assumption 2.3**.: _The training set satisfies: (i) \(|}{n}=\) and \(|}{n}=|}{n}=|}{n}=\); and (ii) for all \(i_{1},i_{2}[n],\;l_{1},l_{2}[L]\), if \(X^{(i_{1})}_{l_{1}},X^{(i_{2})}_{l_{2}}\{_{1},_{2},_{3}\}\), then \(X^{(i_{1})}_{l_{1}} X^{(i_{2})}_{l_{2}}\), i.e., all irrelevant words are different._

The first assumption can be approximately satisfied with high probability given the total number \(n\) of samples is large enough. Such an assumption can be removed by applying the standard concentration theorems. The second assumption implicitly assumes \(nL d\). If the irrelevant words are uniformly sampled from a large entire vocabulary, then each irrelevant word appears only very few times in the training set. Thus, letting irrelevant words appear only once in the entire training set is a reasonable way to simplify our analysis.

### Transformer Architecture and Training

Consider a training set \(\{(X^{(i)},y_{i})\}_{i=1}^{n}\) with \(n\) training samples. Each data point \(X^{(i)}^{d L}\) contains \(L\) tokens, i.e., \(X^{(i)}=[x^{(i)}_{1},x^{(i)}_{2},,x^{(i)}_{L}]\). We consider the transformer model with a self-attention module followed by a linear MLP:

\[F(X;W,W_{V},W_{K},W_{Q})=_{l=1}^{L}_{j=1}^{m_{1}}a_{j}(w_{j}^{ }W_{V}XW_{K}^{}W_{Q}x_{l}}{ })\] (1)

where the query matrix \(W_{Q}^{m d}\), the key matrix \(W_{K}^{m d}\), the value matrix \(W_{V}^{m d}\), the hidden-layer MLP weights \(W^{m_{1} m}\) (with \(w_{j}^{}\) being the \(j\)-th row of \(W\)), and the output-layer weights of the MLP \(a^{m_{1}}\). We define the linear MLP function of the transformer to be \(G()=_{j=1}^{m_{1}}a_{j}w_{j}^{}W_{V}\). We now introduce some shorthand notations \(K=W_{K}X,\;Q=W_{Q}X,\;V=W_{V}X\) and let \(k_{l}=W_{K}x_{l}\). Notice that \(K=[k_{1},k_{2},,k_{L}]\). We further extend this shorthand to \(q_{l}\) and \(\;v_{l}\). We also define functions \(k()=W_{K},\;q()=W_{Q},\;v()=W_{V}\). We introduce the shorthand for the **score vector**\(s_{l}:=W_{K}^{}W_{Q}x_{l}}{}\) and the attention vector \(p_{l}:=(s_{l})\). For the attention vector, if \(, X^{(i)}\), let \(l(i,),l(i,)\) be the indices such that \(X^{(i)}_{l(i,)}=,\;X^{(i)}_{l(i,)}=\), and we define \(p^{(i)}_{q,k}:=p^{(i)}_{q l(i,),k  l(i,)}:=}W_{K}^{}W_{ Q}}{}_{l(i,)}\).

**Initialization.** We initialize \(a_{j}( 1)\) and the value of \(a\) is fixed during training. The trainable parameters are \([W,W_{V},W_{K},W_{Q}]\). We initialize \([W,W_{V},W_{K},W_{Q}]\) by \(W_{i,j}(0,_{1}^{2})\) and \((W_{V})_{i,j},(W_{K})_{i,j},(W_{Q})_{i,j}(0, _{0}^{2})\).

**Training.** We adopt the **cross-entropy loss**\(l(x)=(1+(-x))\). The gradient of the cross-entropy loss is given by \(l^{}(x)=-\) and we define \(g(x)=\). The model is trained by gradient flow to minimize the following empirical loss:

\[}(W,W_{V},W_{K},W_{Q})=_{i=1}^{n}l(y_{i}F(X^ {(i)};W,W_{V},W_{K},W_{Q})).\] (2)

[MISSING_PAGE_FAIL:5]

* \(_{1}^{}{W_{V}^{(t)}}^{T}W_{V}^{(t)}_{2}\) _increases, whereas_ \(_{1}^{}{W_{V}^{(t)}}^{}W_{V}^{(t)}_{3}\) _and_ \(_{2}^{}{W_{V}^{(t)}}^{}W_{V}^{(t)}_{3}\) _decrease._
* _Linear MLP functions satisfy:_ \(G^{(t)}(_{1})(1)\)_,_ \(G^{(t)}(_{2})(1)\)_,_ \(-G^{(t)}(_{3})(1)\)_._
* \(G^{(t)}(_{1})+G^{(t)}(_{2})+G^{(t)}(_{3})(1)\)_,_ \(G^{(t)}(_{1})+G^{(t)}(_{3})-(1)\) _and_ \(G^{(t)}(_{2})+G^{(t)}(_{3})-(1)\)_._

In Theorem 3.2, item 1 indicates that, during Phase 2, gradient flow drives the self-attention module to weigh more between the two target signals \(_{1}\) and \(_{2}\), and to weigh less between one of these signals and the common token \(_{3}\). Item 2 indicates that gradient flow drives the value matrix \(W_{V}\) to positively align the two target signals \(_{1}\) and \(_{2}\), but negatively align one target signal (\(_{1}\) or \(_{2}\)) with the common token \(_{3}\). Further, the last two items indicate that the MLP continue to classify correctly and further enlarge the classification margin. Hence, all items in Theorem 3.2 collectively indicate that attention and MLP evolve jointly to enlarge the classification margin and hence drive the loss value to decrease in Phase 2.

**Theorem 3.3** (Near Minimum Training Loss and Attention).: _With probability at least \(1-\), there exists a time \(T^{}=((m))\) such that_

* _The training and generalization losses satisfy_ \(^{(T^{})} 1/(m)\) _and_ \(L^{(T^{})} 1/(m)\)_._
* _The attention matrices satisfies:_ \[{W_{K}^{(T^{})}}^{}{W_{Q}^{(T^{})}}={W_{K}^{(0)}}^{}{W_{Q}^{ (0)}}+_{i_{1},i_{2}[d]}{C_{i_{1},i_{2}}^{(T^{})}}_{i_{1}}_{i _{2}}^{},\] (3) _where_ \(C_{1,2}^{(T^{})},C_{2,1}^{(T^{})},-C_{3,1}^{(T^{})},-C_{3,2}^ {(T^{})}=(^{2}m}{_{1}^{}mm_ {1}}+^{2}}{_{1}^{}mm_{1}})\) _and_ \(C_{i_{1},i_{2}}^{(T^{})}(^{2}m}{n _{1}^{}mm_{1}}+^{2}}{ _{1}^{}mm_{1}})\) _if one of_ \(i_{1},i_{2}[d]\)_._

Theorem 3.3 indicates that both training and test losses converge nearly to zero as long as the embedding dimension \(m\) is sufficiently large, because both the attention and MLP matrices are trained towards enlarging the classification margin in Phase 2. Theorem 3.3 also provides the explicit form of the attention matrix in Equation (3), in which the second term captures the learned information of the self-attention module. It can be seen that the large coefficients \(C_{1,2}^{(T^{})}\) and \(C_{2,1}^{(T^{})}\) capture strong coupling of the two target signals \(_{1}\) and \(_{2}\), and the large negative coefficients \(C_{3,1}^{(T^{})}\) and \(C_{3,2}^{(T^{})}\) encourages strong negative coupling of one target signal \(_{1}\) or \(_{2}\) and the common token \(_{3}\). All these attention terms contribute to enlarge correct classification margin. Further, the coefficients between all other random tokens are order-level smaller and hence do not corrupt the correct classification.

**Synthetic Experiment:** We next verify our theory and the two-phase characterization of the training process via synthetic experiments (see the experiment setup in Appendix A).

Figure 1 (a) shows how the attention score correlation \(_{i}^{}{W_{K}^{(t)}}^{}{W_{Q}^{(t)}}_{j}\) evolves during the training. It is clear that these scores do not change significantly in Phase 1, verifying Theorem 3.1. In Phase 2, the score correlation between two target signals \(_{1}\) and \(_{2}\) increases, and the score between one target signal and the common token decreases, verifying Theorem 3.2.

Figure 1: Synthetic experiments with illustration of two training phases. The detailed experiment setup can be found in Appendix A.

Figure 1 (b) plots how the training loss changes during the two phases of training. The blue curve (indexed by 'loss') represents the overall training loss of all samples. The other curves correspond to the training loss of four types of samples as indicated in the legend. In Phase 1, the training loss for samples with both target signals (i.e., orange curve) decreases because the linear MLP layer aligns with the target signals (verifying Lemma 4.1 in Section 4.1). The training loss for samples with one target signal and the common token (i.e., green or red curves) first increases because the linear MLP layer initially has not aligned negatively enough with the common token yet (as captured by Lemma 4.2 in Section 4.1), and then decreases in the later stage of Phase 1 when the MLP layer aligns negatively with the common token (as captured by Lemma 4.3 in Section 4.1). All loss functions decrease in Phase 2 because all attention matrices and linear MLP jointly enlarge the classification margin, verifying Theorem 3.2.

## 4 Proof Outline: Two-phase Gradient Flow Analysis

### Phase 1: Alignment of Linear MLP for Correct Classification

In Phase 1, the linear MLP quickly aligns with the two target word tokens while all attention matrices stay roughly unchanged from their initialization values. We show that the linear MLP functions \(|G^{(t)}(_{1})|,|G^{(t)}(_{2})|,|G^{(t)}(_{3})|\) become sufficiently large (larger than some constant threshold) so that all training samples are correctly classified at the end of Phase 1.

We first analyze the dynamical system at the initialization. In particular, the following lemma shows that at the initialization, the linear MLP layer receives a sufficiently large gradient from the two target signals, and hence samples with the two target signals will be classified correctly as co-occurrence soon after the training starts.

**Lemma 4.1** (Same as Lemma E.4).: _With probability at least \(1-\) over the weight initialization,_

\[\{_{1},_{2}\}:w_{j}^{(0)}W_{V}^{( 0)}}{ t}=((_{0}^{2}+_{1}^{2})m).\]

Further, by the definition of Phase 1 (see Definition E.2 for a formal definition), the gradients of the attention matrices in the dynamical system are much smaller than that of linear MLP given in Lemma 4.1. This implies that during Phase 1, mainly the linear MLP is performing learning, whereas all the attention matrices are changing slowly from their initialization. Based on this, we have \(G^{(t)}(_{1})=((_{0}^{2}+_{1}^ {2})mm_{1})\) which implies that it takes only \(O(1/(_{0}^{2}+_{1}^{2})mm_{1})\) iterations for \(G^{(t)}(_{1})\) to reach a certain constant magnitude.

Lemma 4.1 indicates that the samples with co-occurrence of the two target signals are classified correctly. The following lemma shows that the initial gradient from the common token, i.e., the gradient of \(G^{(t)}(_{3})\), is much smaller than the gradient from the two target signals, which implies that the samples with only one target signal may be classified _incorrectly_ as co-occurrence (since the network in this case will output a positive value). This is verified empirically by our experiments in Figure 1 (b), where the loss function corresponding to only one target signal and the common token first increases in Phase 1.

**Lemma 4.2** (Same as Lemma E.16).: _Let \(F=_{i}|F_{i}^{(0)}|\). With probability at least \(1-\) over the weight initialization,_

\[|w_{j}^{(0)}W_{V}^{(0)}_{3}}{ t}|= (_{1}^{2}}+_{0}^{2}+ _{1}^{2}mF).\]

Notice that the model output \(F\) depends on the weight initialization scale and can be made small.

We next show in the following lemma that the gradient \((_{3})}{ t}\) of the common token will quickly become negative soon after the training begins, which drives the transformer model to output a negative value when it sees those types of samples. This implies that negative samples (without co-occurrence of two target tokens) will be classified correctly towards the end of Phase 1. This is also verified empirically by our experiments in Figure 1 (b), where the loss function corresponding to only one target signal and the common token descreases towards the end of Phase 1.

**Lemma 4.3** (Abbreviated from Theorem E.19).: _There exists a time \(T_{0.5} T_{1}\) and a constant \(C\) such that for all \(t[T_{0.5},T_{1}]\)_

\[(1+C)((_{1})}{ t},(_{2})}{ t})-(_{3})}{  t}(1-C)((_{1})}{ t}+(_{2})}{ t}).\]Proof Intuition of Lemma 4.3.: We first note that the term

\[_{i=1}^{n}g_{i}^{(0)}y_{i}_{l_{2}=1}^{L}_{j_{2}=1}^{m_{1}} \|w_{j_{2}}^{(0)}\|_{2}^{2}(X^{(i)}p_{l_{2}}^{(0,i)})^{}\]

makes the major contribution to the gradient \((_{3})}{ t}\). Such a term is small at the initialization due to the cancellation effect from positive and negative \(y_{i}\)'s. However, since the linear MLP \(G\) will positively align the two target signals at the beginning, for the samples with positive labels, \(g_{i}^{(t)}\) will decrease, whereas for samples with only one target signal, \(g_{i}^{(t)}\) will increase. Hence, \(w_{j_{1}}^{(t)}W_{V}^{(t)}_{3}}{ t}\) will become negative. This trend will continue until the gradient from \(_{3}\) starts to match the gradients from \(_{1},_{2}\), which is what we establish in Lemma 4.3. 

Using Lemma 4.3, we can show that all training samples are correctly classified at end of Phase 1.

### Phase 2: Evolution of Attention and MLP for Large Classification Margin

In Phase 2, both attention and MLP matrices evolve towards enlarging the classification margin, thus driving the loss value small.

We now analyze what happens in Phase 2. Let \(T_{2}\) denote the end of Phase 2. Recall that at the end of Phase 1, we have \(G^{(t)}(_{1}),G^{(t)}(_{2}),-G^{(t)}(_{3})(1)\). We will mainly need to show that such a condition continues to hold in Phase 2, so that attention matrices will evolve with MLP to learn better classifiers. To this end, we exam the following gradient flow in the dynamical system:

\[()}{ t}= _{i_{2}:\  X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}_{l_{2}=1}^{L}p_{q  l_{2},k}^{(t,i_{2})}_{j_{1}=1}^{m_{1}}_{j _{2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}} w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\] (4) \[+}{n}_{i_{1}=1}^{n}g_{i_{1}}^{(t)}y_{i_{1}}_{l_ {1}=1}^{L}a_{j_{1}}p_{l_{1}}^{(t,i_{1})}V^{(t,i_{1})}W_{V}^{(t)}.\]

It has been proved that at the end of Phase 1, for \(\{_{1},_{2},_{3}\}\), we have \(|_{j_{1}}}^{(t)}}{ t}W_{V}^{(t)} ||_{j_{1}}w_{j_{1}}^{(t)}^{(t)}} { t}|\) since the magnitude of \(_{j_{1}=1}^{m_{1}}_{j_{2}=1}^{m_{1}} a_{j_{1}}w_{j_{1}}^{( t)},a_{j_{2}}w_{j_{2}}^{(t)}\) is large. Assume this can hold for long enough (which we can indeed prove later). Then, we only need to focus on the first term in the sum on the right-hand side in Equation (4). On the other hand, from the dynamical system, we can calculate

\[_{j_{1}=1}^{m_{1}}_{j_{2}=1}^{m_{1}}  a_{j_{1}}w_{j_{1}}^{(t)},a_{j_{2}}w_{j_{2}}^{(t)}=}{n}_{i=1}^{n}g_{i}^{(t)}y_{i}F_{i}^{(t)}.\] (5)

Thus, if \(y_{i}F_{i}^{(t)}>0\) for all \(i[n]\), then \(_{j_{1}=1}^{m_{1}}_{j_{2}=1}^{m_{1}} a_{j_{1}}w_{j_{1}}^{( t)},a_{j_{2}}w_{j_{2}}^{(t)}\) is always increasing. Thus, \(()}{ t}\) mainly depends on the behavior of \(_{i_{2}:\  X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}_{l_{2}=1}^{L}p_{q  l_{2},k}^{(t,i_{2})}\). Further, this is also a key quantity we need to analyze \(W_{V}^{(t)}W_{V}^{(t)}}{ t}\) and \(W_{K}^{(t)}W_{Q}^{(t)}}{ t}\). Note that \(_{i_{2}:\  X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}_{l_{2}=1}^{L}p_{ q l_{2},k}^{(t,i_{2})}_{i_{2}:\  X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\) if \(p_{q l_{2},k}^{(t,i_{2})} 1/L\) which holds at the beginning of Phase 2. Later, we are going to prove convergence of the training loss via the following: (1) the training loss can decrease if the softmax probability is uniform; (2) even though the softmax probability will deviate from uniform distribution during training, we can bound such deviation and the loss value can still decrease.

**Automatic balancing of gradients.** As argued above, our main focus is on analyzing the behavior of \(_{i_{2}:\  X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}\). This consists of two parts: (i) Lemma 4.4, which shows that the two groups of samples with only the presence of one target signal have gradients \(_{i I_{2}}g_{i}^{(t)}\) and \(_{i I_{3}}g_{i}^{(t)}\) close to each other during training; and (ii) Lemma 4.5, which shows that the gradient gaps \(_{i I_{1}}g_{i}^{(t)}-_{i I_{2}}g_{i}^{(t)}\) and \(_{i I_{2} I_{3} I_{4}}g_{i}^{(t)}-_{i I_{1}}g_{i}^{(t)}\) are not too small compared with \(_{i[n]}g_{i}^{(t)}\). BothLemmas 4.4 and 4.5 establish that the ratio of those important gradients are kept within certain ranges during training. We call such a key property as _automatic balancing of gradients_, which is further used for proving that the gradient flow can drive the training loss small.

**Lemma 4.4** (Same as Lemma F.5).: _For \(t[T_{1},T_{2}]\), there exists a small constant \(C 1\) such that_

\[}g_{i}^{(t)}-_{i I_{3}}g_{i}^{(t)}| }{(_{i I_{2}}g_{i}^{(t)},_{i I_{3}}g_{i}^{(t)})} C.\]

Proof Intuition of Lemma 4.4.: The intuition behind the result is as follows. If \(_{i I_{2}}g_{i}^{(t)}\) becomes much bigger than \(_{i I_{3}}g_{i}^{(t)}\) during the training, then \(_{i I_{1}}g_{i}^{(t)}-_{i I_{2}}g_{i}^{(t)}\) is much smaller than \(_{i I_{1}}g_{i}^{(t)}-_{i I_{3}}g_{i}^{(t)}\) which makes \((_{1})}{ t}<(_{2})} { t}\). It is not hard to show that random tokens make negligible contributions to the gradient. Thus, for \(i I_{2}\), we have \(^{(t)}}{ t}(_{1} )}{ t}+(_{3})}{ t}\). By the chain rule, we have \(^{(t)}}{ t}=g^{}(y_{i}F_{i}^{(t)})y_{i} ^{(t)}}{ t}\). Since \((_{3})}{ t}<0\), if \((_{1})}{ t}<(_{2})} { t}\), then \(_{i I_{2}}g_{i}^{(t)}\) will drop faster than \(_{i I_{3}}g_{i}^{(t)}\), i.e., \(-_{i I_{2}}g_{i}^{(t)}>-_{i I_{3}}g_{i}^{(t)}\). In Appendix, we formally prove Lemma 4.4 by analyzing the ratio \(_{i I_{2}}g_{i}^{(t)}/_{i I_{3}}g_{i}^{(t)}\), and show that this ratio hangs over around \(1\) during training. 

**Lemma 4.5** (Abbreviated from Lemma F.6).: _For \(t[T_{1},T_{2}]\), the gradient satisfies that_

\[g_{i}^{(t)}}{_{i I_{2} I_{3} I_{4}}g_{i}^{ (t)}-_{i I_{1}}g_{i}^{(t)}}=O(1),g_{i}^ {(t)}}{_{i I_{1}}g_{i}^{(t)}-_{i I_{2}}g_{i}^{(t)}}=O(1).\]

_Further, for some constant \(C\), we have_

\[(1+C)((_{1})}{ t},(_{2})}{ t})-(_{3})}{  t}(1-C)((_{1})}{ t}+ { G^{(t)}(_{2})}{ t}).\]

Proof Sketch of Lemma 4.5.: The proof of Lemma 4.5 relies on analyzing how the ratio between \((_{1})}{ t}\) and \(-(_{3})}{ t}\) changes. We show that this ratio will hang over around some range. Recall the relationship that \(()}{ t}_{i_{2}:\  X^{(i_{2})}}g_{i_{2}}^{(t)}y_{i_{2}}_{j_{1}=1}^{m_{1}}_{j_{ 2}=1}^{m_{1}}a_{j_{1}}a_{j_{2}} w_{j_{1}}^{(t)},w_{j_{2}}^{(t)}\). It is not hard to show that

\[(_{3})}{ t}}{( _{1})}{ t}}}g_{i}^{(t)}+_{i I _{2} I_{3} I_{4}}g_{i}^{(t)}}{_{i I_{1}}g_{i}^{(t)}-_{i I _{2}}g_{i}^{(t)}}.\]

Define \(R(t):=}g_{i}^{(t)}-_{i I_{2} I_{3} I_{4 }}g_{i}^{(t)}}{_{i I_{1}}g_{i}^{(t)}-_{i I_{2}}g_{i}^{(t)}}\). Solving when \(R(t) 0\) yields a quadratic inequality, and further analysis shows that the root is contractive and is within some specific range. 

Utilizing the _gradient automatic balancing_ properties, the following corollary characterizes how the attention matrices in the dynamical system change in Phase 2. In particular, we can show that after \(W_{V}\)-transform, \(_{1}\) and \(_{2}\) become more positively correlated whereas \(_{1}\) and \(_{3}\) (also \(_{2}\) and \(_{3}\)) become negatively correlated. This is a direct result following from updates of the dynamical system.

**Corollary 4.6** (Abbreviated from Corollary F.13).: _For \(t[T_{1},T_{2}]\),_

\[_{2}^{}W_{V}^{(t)}W_{V}^{(t)}_{1}>0, _{1}^{}W_{V}^{(t)}W_{V}^ {(t)}_{3}<0.\]

Since we have analyzed how \(G^{(t)}()\) will change in stage 2, we can utilize this information to analyze the change of softmax attention via the following relationship: by Appendix C, we can derive

\[^{}W_{K}^{(t)}W_{Q}^{(t)}_{2}}{ t}\]\[=}_{i=1}^{n}g_{i}^{(t)}y_{i}_{l=1}^{L}_{1 }^{}{W_{K}^{(t)}}^{}K^{(t,i)}(G^{(t)}(X^{(i)})-(G^{(t )}(X^{(i)}))^{}p_{l}^{(t,i)})p_{l}^{(t,i)}x_{l}^{(i)}_{2}\] \[+}_{i=1}^{n}g_{i}^{(t)}y_{i}_{l=1}^{L} _{2}^{}{W_{Q}^{(t)}}^{}q_{l}^{(t,i)}p_{l}^{(t,i)}{}^{} (G^{(t)}(X^{(i)})-(G^{(t)}(X^{(i)}))^{}p_{l}^{(t,i)}) X^{(i)}{}^{}_{1}.\]

The following lemma shows that the attention score between the two target signals \(_{1}\) and \(_{2}\) increases, whereas that between one target signal \(_{1}\) or \(_{2}\) and the common token \(_{3}\) decreases.

**Lemma 4.7** (Abbreviated from Lemma F.16).: _For \(,\{_{1},_{2}\},\ \), and for \(t[T_{1},T_{2}]\),_

\[^{}{W_{K}^{(t)}}^{}{W_{Q}^{(t)}}= }(^{(t)}_{0}^{2}m),_{3}^{}{W_{K}^{(t)}}^{}{W_{ Q}^{(t)}}=-}(^{(t)}_{0}^{2}m) .\]

Lemma 4.7 is keeping track of the attention coefficients \(C_{i_{1},i_{2}}^{(t)}\) in Theorem 3.3 via gradient flow, which proves the second item of Theorem 3.3.

## 5 Discussion and Future Directions

In this work, we developed a novel gradient flow based framework for analyzing the training dynamics of a one-layer transformer to recognize co-occurring tokens. We provided a two-phase characterization of the training process. In Phase 1, the linear MLP layer is trained to classify samples correctly, with attention weights almost unchanged. In Phase 2, both attention matrices and the linear MLP jointly evolve to enlarge the classification margin, thus reducing the loss to near minimum.

As future work, it will be interesting to analyze more general transformer architectures such as multi-headed attention, multi-layer transformer, etc. Further, it is of interest to study the dynamics of more advanced gradient descent algorithms such as gradient descent with adaptive learning rate, with momentum, etc., and explore how the hyperparameters will affect the training dynamics. Another direction is to study more practical language sequences where tokens are generated in a correlated fashion. Then the next token prediction becomes an intriguing problem.