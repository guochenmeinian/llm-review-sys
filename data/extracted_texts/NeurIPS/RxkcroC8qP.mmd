# Visual Decoding and Reconstruction via EEG Embeddings with Guided Diffusion

Dongyang Li\({}^{1}\) Chen Wei\({}^{1}\) Shiying Li\({}^{1}\) Jiachen Zou\({}^{1}\) Quanying Liu\({}^{1}\)

\({}^{1}\)Department of Biomedical Engineering, Southern University of Science and Technology,

Shenzhen, China

{lidy2023, weic3}@mail.sustech.edu.cn

liuqy@sustech.edu.cn

D. Li and C. Wei contributed equally.Corresponding author.

###### Abstract

How to decode human vision through neural signals has attracted a long-standing interest in neuroscience and machine learning. Modern contrastive learning and generative models improved the performance of visual decoding and reconstruction based on functional Magnetic Resonance Imaging (fMRI). However, the high cost and low temporal resolution of fMRI limit their applications in brain-computer interfaces (BCIs), prompting a high need for visual decoding based on electroencephalography (EEG). In this study, we present an end-to-end EEG-based visual reconstruction zero-shot framework, consisting of a tailored _brain encoder_, called the Adaptive Thinking Mapper (ATM), which projects neural signals from different sources into the shared subspace as the clip embedding, and a _two-stage multi-pipe EEG-to-image generation strategy_. In stage one, EEG is embedded to align the high-level clip embedding, and then the prior diffusion model refines EEG embedding into image priors. A blurry image also decoded from EEG for maintaining the low-level feature. In stage two, we input both the high-level clip embedding, the blurry image and caption from EEG latent to a pre-trained diffusion model. Furthermore, we analyzed the impacts of different time windows and brain regions on decoding and reconstruction. The versatility of our framework is demonstrated in the magnetoencephalogram (MEG) data modality. The experimental results indicate that our EEG-based visual zero-shot framework achieves SOTA performance in classification, retrieval and reconstruction, highlighting the portability, low cost, and high temporal resolution of EEG, enabling a wide range of BCI applications. Our code is available at https://github.com/ncclab-sustech/EEG_Image_decode.

Figure 1: **EEG/MEG-based zero-shot brain decoding and reconstruction. Left: Overview of three visual decoding tasks using EEG/MEG data under natural image stimulus. Right: Our reconstruction examples.**Introduction

A key technical challenge in BCIs is to decode/reconstruct the visual world seen by humans through non-invasive brain recordings, such as fMRI, MEG or EEG. These highly dynamic brain activities reflect human perception of the visual world, which is influenced by properties of the external visual stimulus, our internal states, emotions and even personal experiences. Thus, visual decoding and reconstruction based on neural signals can uncover how the human brain processes and interprets natural visual stimulus, as well as promote non-invasive BCI applications.

Contrastive learning and generative models have greatly advanced fMRI-based visual decoding in both decoding tasks (e.g., image classification and retrieval) and generative tasks (e.g., image reconstruction). By combining pre-trained visual models, existing fMRI decoding models can learn highly-refined feature embeddings in limited data [1; 2]. Using these embedded fMRI features, generative models such as diffusion models can reconstruct the image one is seeing [2; 3]. However, despite many advances in fMRI-based visual decoding, fMRI equipment is unportable, expensive, and difficult to operate, largely limiting its application in BCIs. Alternatively, EEG is portable, cheap, and universal, facilitating a wide range of BCI applications. EEG has higher temporal resolution and can effectively capture rapid changes in brain activity when processing complex, dynamic visual stimulus.

EEG has long been considered incomparable to fMRI in natural image decoding/reconstruction tasks, as EEG suffers from low signal-to-noise ratio, low spatial resolution, and large inter-subject variability. Recent advances in multimodal alignment have made MEG/EEG visual decoding possible, although the performance is still inferior to fMRI [4; 5; 6]. Yohann Benchertti et al. used the CLIP model to extract the latent representation of the image and trained the MEG encoder to align it with the image representation extracted by CLIP. It achieved excellent retrieval and reconstruction performance on MEG and fMRI datasets, demonstrating the potential for real-time visual decoding and reconstruction using EEG/MEG signals. Recently, Song et al.  used an EEG encoder based on ShallowNet  and performed representation alignment through contrastive learning, achieving excellent decoding performance on the THING-EEG dataset . These two studies provide preliminary evidence of the potential of EEG/MEG-based visual decoding. However, there is a significant gap in their performance compared to the fMRI-level performance. This gap is largely caused by the fact that the framework of EEG visual decoding and reconstruction have not yet been thoroughly explored.

To fill this gap, we have developed a visual decoding framework based on EEG/MEG, including a novel EEG encoder and a two-stage image generation strategy. Our work has three main contributions:

1. We present brain decoding framework, which is the first work allows zero-shot image classification, retrieval, and reconstruction via EEG data. Experimental results demonstrate that our framework is applicable to various common EEG encoder architectures.
2. By extensively studying the existing EEG encoder modules, we construct a tailored EEG encoder ATM, which achieves state-of-the-art performance in three downstream visual decoding tasks.
3. We report a two-stage EEG-to-image generation strategy, which separately extracts high-level and low-level visual features from EEG and refining these features with an additional lightweight prior diffusion model, enabling reliable reconstruction of images using less than 500ms EEG.

## 2 Method

To learn high-quality latent representations of EEG data, it is crucial to consider the spatial position of EEG channels and the Temporal-Spatial properties of EEG signals. Let \(T\) represent the length of the time window of the data, \(C\) the number of EEG channels, and \(N\) the total number of data samples. Our objective is to derive EEG embeddings \(Z_{E}=f(E)^{N F}\) from the brain activity data \(E^{N C T}\), where \(f\) is the EEG encoder and \(F\) is the projection dimension of the embeddings. Concurrently, we use the CLIP model to extract image embeddings \(Z_{I}^{N F}\) from images \(I\). Our goal is to effectively align the EEG representation with the image representation, as illustrated in Fig. 2. In the training phase, the EEG encoder is trained with EEG and image pairs using a contrastive learning framework. In the inference phase, the EEG embeddings from the trained EEG projector can be used for a variety of zero-shot tasks, including EEG-based image classification, retrieval, and image reconstruction.

### ATM for EEG Embedding

Inspired by advanced time series models [9; 10], we develop an EEG encoder called ATM, for aligning the original EEG signals to its feature representation space (Fig. 3). ATM is based on the channel-wise Transformer encoder, Temporal-Spatial convolution and multilayer perceptron (MLP) architecture. In contrast to other conventional practices, the original EEG does not need to be segmented, and each sequence acts as a patch. After sinusoidal position embedding, these patches are processed through a channel-attention module to integrate the information of different series. Subsequently, through the Temporal-Spatial aggregation, we project the output with a MLP to get rational shape representations. The Temporal-Spatial convolution module is an effective way to represent EEG data with a small number of parameters , prevent overfitting in training. The difference is our components is plug-and-play and can be flexibly replaced with different types of Temporal-Spatial convolution components as needed to adapt to various EEG/MEG datasets. Finally, MLP projector consists of \(M\) simple residual components and fully connected layers, with LayerNorm applied in the output to ensure the stability of training. In addition to entering the original series, we provide an identification input for a known subject and can specifically use this token for downstream tasks. For unknown subjects, we use shared tokens or average all tokens equally directly into the MLP projector.

### Image Embedding

Many previous studies have explored various training strategies to train deep neural networks for image embedding, such as VGG-19 and ResNet trained with supervised learning, CLIP, DINO trained with contrastive learning, and VAEs with self-supervised learning [11; 12; 5; 6]. They have reported that DINO and CLIP models pre-trained using the Vision Transformer (ViT) architecture perform better in a range of downstream tasks, including image decoding and reconstruction, compared to models trained using supervised learning methods (such as VGG, ResNet) and self-supervised VAE frameworks. Thus, in this study, we use CLIP for image embedding, denoted as \(Z_{I}^{N 1024}\), instead of \(Z_{I}^{N 257 768}\), with the EEG embeddings. Before formal training, all images undergo the standard preprocessing procedure .

Figure 2: **EEG/MEG-based visual decoding and generation framework. The EEG encoder is designed as a flexible replacement component. After aligning with image features, the EEG features are used for zero-shot retrieval and classification tasks, and the reconstructed images are obtained through a two-stage generator.**

### EEG Guidance Image Generation

In this study, we present a two-stage pipeline for generating images that serve as visual stimulus for EEG recordings, as shown in the bottom right of Fig. 2. In the left of Fig. 3 we have obtained the EEG embeddings \(z_{E}\) for each image by the EEG encoder ATM. Now our goal is to use these EEG embeddings to generate the corresponding images. The joint distribution of images, EEG embeddings, and image embeddings can be expressed as \(p(I,z_{E},z_{I})=p(z_{I}|z_{E})p(I|z_{I})\), corresponding to the prior diffusion and CLIP-guided generation, respectively. In **Stage I**, we first focus on the prior diffusion stage. Inspired by DALL-E 2  and Mind S Eyes , we train a diffusion model conditioned on the EEG embeddings \(_{E}\) to learn the distribution of CLIP embeddings \(p(z_{I}|z_{E})\). In this stage, we construct a lightweight U-Net: \(_{}(z_{I}^{t},t,z_{E})\), where \(z_{I}^{t}\) represents the noisy CLIP embedding at diffusion time step \(t\). We train the prior diffusion model using EEG and CLIP embeddings. Through this diffusion model, we can generate corresponding CLIP embeddings \(z_{I}\) from EEG embeddings as a prior for stage II. In **Stage II**, we employ the pre-trained SDXL  and IP-Adapter  models to model the generator \(p(I|z_{I})\), thereby sampling image \(I\) according to \(z_{I}\). In addition, we introduce the low-level features here using img2img. Further details are provided in Appendix C.

### Loss Function

Following the methodology outlined by Benchertit et al. , we adopt a dual approach to loss functions, serving distinct objectives. For the classification and retrieval tasks, we only utilize the CLIP loss, which is inspired by the contrastive learning approach described in Radford et al. . This loss function aids in aligning the EEG data \(E\) with corresponding image data \(I\), thereby facilitating the identification of EEG-image pairs and maximizing the boundaries of EEG representations. For the generation tasks, besides the CLIP loss, we add a Mean Squared Error (MSE) loss to facilitate consistency learning in regression. Thus the overall loss function for our model is a combination of these two distinct loss types, expressed as:

\[Loss= L_{CLIP}+(1-) L_{MSE}\]

Here, \(\) is a hyperparameter that balances the contribution of each loss type.

Figure 3: **The structure of ATM**. The original EEG sequences of different variates are independently embedded into tokens. Channel-wise attention is applied to embedded variate tokens with enhanced interpretability revealing electrode correlations. And representations of each token are extracted by the shared feedforward network (FFN). Then Temporal-Spatial convolution can prevent overfitting and enhance the ability of Temporal-Spatial modeling.

## 3 Experiments

### Training and Computational Considerations

We conducted our experiments on the THINGS-EEG dataset's training set [8; 6]. To verify the versatility of ATM for embedding electrophysiological data, we tested it on MEG data modality using the THINGS-MEG dataset . All experiments can be completed in a single NVIDIA RTX 4090 GPU. We used the Adam optimizer  to train the across-subject model on a set of approximately 496,200 samples, and the within-subject model on a set of about 66,160 samples, with an initial learning rate of \(3 10^{-4}\) and batch sizes of 16 and 1024. Our initial temperature parameter was set to 0.07. We spitted the last batch of the original training set as the validation set and selected the best model based on the minimum validation loss over 40 epochs. For fairness, all models' hyperparameters were kept consistent. In our study, we compared the performance of different encoders on the within-subject test set and cross-subject (leave-one-subject-out) test set (see Appendix H).

### EEG Decoding Performance

Our method obtains the EEG embedding for the classification task. We output the category of EEG with the highest cosine similarity with text embeddings(Fig. 5a). Fig. 5c presents the average accuracy across different methods in the subjects, and shows that our method outperforms others. More details of the EEG-based image classification are in Appendix B.

In Fig. 5, we test the effectiveness of EEG embeddings in the image retrieval task. We calculate the cosine similarity between the EEG embeddings and the CLIP embeddings instead of text embeddings in the image dataset (with 200 images). Fig. 5d shows the average results for all subjects. We take the highest test accuracy in the evaluation process as the statistical result. Fig. 5b shows the Top-5 retrieved images corresponding to the real visual stimulus seen by subjects. Compared with the previous models, the Top-1 accuracy of our model is significantly improved, and the Top-5 images all maintain a high degree of similarity with the original images. See the Tab. 8 in Appendix for more detailed averages of test accuracy in subjects.

Ablation Study on ATMWe systematically deconstructed and analyzed each layer of our EEG projector. We conducted an ablation study for each component in ATM (i.e., the MLP projector, the Temporal-Spatial convolution module and the channel-wise attention block). We specified two different convolution architectures, ShallowNet (ATM-S) and EEGNetV4 (ATM-E), as our convolution backbone. Appendix B.3 showed the results obtained under different experimental configurations.

Figure 4: **EEG/MEG-based decoding and reconstruction performance. Left: Comparisons of nine encoders on the THINGS-EEG dataset, including within-subject and cross-subject performance. Right: Comparisons on the THINGS-MEG dataset, similar to left. Our method achieves the highest performance compared to other competing encoders in EEG/MEG-based visual decoding tasks.**

### Image Generation Performance

Fig. 5(a) shows the process of generating images under the guidance of EEG embedding and evaluating the quality of the generated images. To evaluate the generation performance, we conducted an image retrieval task. Specifically, we extract the CLIP embedding of the generated images and compare the similarity between the CLIP embeddings of all images to retrieve the generated image.

Fig. 5(b) shows the similarity of distribution. Fig. 5(c) shows the generated samples. The generated images have high semantic similarity with the seen images and have good diversity in low-level visual features, which can be manipulated by the guidance scale hyperparameter (Fig. 5(d)). We also report the reconstruction performance for EEG, MEG, and fMRI across various metrics from different methods and datasets in the Tab. 1.

### Temporal Analysis

To investigate the effects of different EEG time window on visual decoding, we calculated the average top-1 classification accuracy for sliding and growing time windows: \([0,t]\), including the entire period

    &  &  \\   & PixCor \(\) & SSIM \(\) & AlexNet(2) \(\) & AlexNet(5) \(\) & Inception \(\) & CLIP \(\) & SwAV \(\) \\  NSD-fMRI  & 0.305 & 0.366 & 0.962 & 0.977 & 0.910 & 0.917 & 0.410 \\ NSD-fMRI  & 0.254 & 0.356 & 0.942 & 0.962 & 0.872 & 0.915 & 0.423 \\ NSD-fMRI  & 0.130 & 0.308 & 0.917 & 0.974 & 0.936 & 0.942 & 0.369 \\  THINGS-MEG  & 0.058 & 0.327 & 0.695 & 0.753 & 0.593 & 0.700 & 0.630 \\ THINGS-MEG (averaged)  & 0.076 & 0.336 & 0.736 & 0.826 & 0.671 & 0.767 & 0.584 \\ THINGS-MEG (Ours) & 0.104 & 0.340 & 0.613 & 0.672 & 0.619 & 0.603 & 0.651 \\
**THINGS-EEG (Ours)** & **0.160** & **0.345** & **0.776** & **0.866** & **0.734** & **0.786** & **0.582** \\   

Table 1: Quantitative assessments of the reconstruction quality for EEG, MEG, and fMRI in Subject 8. For detailed explanations of the metrics.

Figure 5: **EEG-based image retrieval and classification**. (a) The paradigm of EEG-based image retrieval and classification. (b) Samples of the top-5 accuracy in EEG-image retrieval tasks. See Appendix G for additional images results. (c) Average in-subject classification accuracy across different methods. (d) Average in-subject retrieval accuracy across different methods.

from the onset of visual stimulus to time point \(t\), and [\(t\)-100, \(t\)], only including the data 100ms before time point \(t\). We compared the accuracy with a randomly selected baseline (0.5% chance level) to test predictive performance (Fig. 7). Our results show that within 500ms after visual stimulus, the accuracy reaches an upper limit of about 30%, after which the accuracy no longer improves (Fig. 7a). The MEG decoding shows a similar profile as the time window expands (Fig. 7b). We exhibit the generated images under different EEG time windows, [0, \(t\)] in Fig. 7c. The similarity is low when the time window is less than 150ms, and this similarity gradually increase as the time window expands. After 500 milliseconds, EEG-guided image generation can reliably reveal the semantics of the images seen. Interestingly, we find differences in the optimal reconstruction time windows for different categories of images, for example, jelly beans (200ms) are faster than aircraft carrier (500ms), implying that the human brain may process different visual objects at different speeds. This finding highlights the advantage of EEG's high temporal resolution in studying fast visual processing compared with the lower temporal resolution of fMRI.

### Spatial Analysis

To investigate the contribution of different brain regions to visual decoding, we divided the EEG electrodes from the THING-EEG data into five distinct brain regions (i.e., Frontal, Temporal, Center, Parietal, Occipital regions in Fig. 8a), and then conducted ablation experiments on retrieval task (Fig. 8b) and the reconstruction task (Fig. 8c). The results showed that using information from all brain regions is optimal, for both retrieval and generation tasks. The occipital had the highest retrieval accuracy and reconstruction performance compared to other regions. Parietal and temporal regions contain some semantic information, whereas frontal and central regions contribute the least useful information to the visual decoding.

## 4 Related Works

**Visual decoding using neural signals:** Decoding visual information from our brain has been a long-standing pursuit in neuroscience and computer science [22; 23]. Some progress has been

Figure 6: **EEG guidance image generation**. (a) The paradigm of image generation. (b) The similarity between random visual objects and the EEG embeddings, and the similarity between generated visual objects and the target EEG embeddings. (c) Comparison between the original image and the image generated using the corresponding EEG data. (see Appendix C for details). (d) The similarity between visual objects and target EEG embeddings as the guidance scale changes, and the diversity of visual objects as the guidance scale changes. See Appendix G for additional results.

made in decoding steady-state visual stimulus. However, accurately and rapidly decoding semantic information in natural images remains a challenge . fMRI has been widely used to estimate semantic and shape information in visual processing within the brain [25; 26]. However, the demand for high-speed and practical applications in brain-computer interfaces calls for alternative approaches. EEG, due to its high temporal resolution and portability, emerges as a promising option . Yet, the overall performance across different subjects and biological plausibility remains unresolved . Furthermore, previous approaches often relied on supervised learning methods with limited image categories, overlooking the intrinsic relationship between image stimulus and brain responses [1; 29; 30].

**Neural decoding using EEG/MEG data:** Previous studies have shown the efficacy of Temporal-Spatial modules in representing neural data [7; 31]. For example, lightweight convolutional neural

Figure 8: **EEG-guided retrieval and reconstruction using EEG from different brain regions**. (a) The EEG electrodes assigned to five brain regions. (b) Top-1 and top-5 retrieval accuracy, using only the EEG channels in each leaved region and all channels. (c) Reconstructed images obtained using only the electrode channels in each individual region and all channels.

Figure 7: **Performance of different EEG/MEG time windows on EEG-guided visual retrieval and reconstruction**. (a) The retrieval accuracy of the expanding EEG windows at intervals [0, t] and at intervals [t-100, t] respectively. (b) The retrieval accuracy of the expanding MEG windows. (c) Samples reconstructed as the EEG window expands. When the EEG time window is greater than 200ms, the reconstructed image is stable. See Appendix H for more detailed explanations.

networks such as EEGNet and ShallowNet  have achieved considerable performance in small EEG and MEG datasets. Using contrastive learning, it has been shown that merely using convolutional neural networks and projection layers can yield satisfactory results on neural datasets . More recently, Benchertti et al proposed a method towards real-time MEG-based reconstruction of visual perception . Song et al. presented an EEG encoder using ShallowNet Temporal-Spatial convolution module with a large convolution kernel with a few parameters for EEG embedding, resulting in favorable performance on EEG-based visual decoding .

**Limitations of previous studies:** Previous EEG studies are primarily oriented toward understanding visual perception in the human brain rather than maximizing EEG decoding performance. Thus the visual decoding performance is far from optimal. Specifically, previous studies have trained linear models to (1) classify a small set of images from brain activity [33; 34], (2) to predict brain activity from the latent representations of images , or (3) to quantify the similarity analysis between these two patterns with representational similarity [4; 33; 8; 35]. While these studies also utilize image embeddings, their linear decoders are limited to classifying a small group of object categories or distinguishing image pairs. Moreover, several deep neural networks have been applied to maximize classification of speech , cognitive load , and images [38; 39; 40] in EEG recordings.  proposed a deep convolutional neural network for classifying natural images using EEG signals. Unfortunately, the experiment presented all images of the same category in a single block, probably misleading the decoder to rely on autocorrelated noise rather than the hidden informative patterns of brain activity . Also, these EEG studies only classify a relatively small number of image categories.

## 5 Discussion and Conclusion

In this study, we proposed a novel and feasible EEG-based zero-shot image reconstruction framework. Although it utilizes existing machine learning techniques, we demonstrate for the first time that EEG-based zero-shot visual decoding and reconstruction can be competitive with MEG and fMRI.

**Technical Impact:** Our technical contributions are mainly on the EEG encoder and the two-stage zero-shot visual reconstruction framework (Fig. 2). First, we developed the ATM, an EEG encoder which can efficiently represent EEG/MEG features for three tasks. Our comprehensive experiments of the EEG encoder (Fig. 3), compared to various architectures and training methods, achieves SOTA performance across various metrics and tasks (Figs. 3(b), 5). Second, our two-stage EEG guidance image reconstruction framework achieves performance close to fMRI using only EEG data (Figs. 6, 14, Tab. 1, 6), and this method is compatible with MEG data (Figs. 3(c), 6(b)).

**Neuroscience Insights:** Our results offer insights into the relationship between brain activity and visual perception. We analyzed EEG-based visual decoding within different time windows to examine when visual information is perceived in the brain (Fig. 7). Our results revealed that visual information in EEG data is predominantly contained within the 200-400ms range (Fig. 6(a)), consistent with previous EEG studies [11; 6; 5]. Interestingly, the visual information in MEG data last up to 800ms, much longer than EEG (Fig. 6(b)), in line with the results reported by a previous MEG study [12; 5]. We also found that EEG performs better than MEG in visual tasks (See Appendix D for Tab. 6), which is different from other fields, such as speech decoding . In addition, through ablation experiments of spatial information, we found that visual information is mainly encoded in the occipital and parietal areas (Fig. 8).

**Interesting Phenomena and Future Directions:** First, there are non-negligible performance differences between cross-subject and within-subject settings. This performance gap arises from inter-subject differences in EEG signals [41; 42], likely attribute to heterogeneity in individual brain, differences in visual perception between individuals, and even shifts in noise distribution during EEG recording. So it calls for more efforts on EEG encoder, such as more flexible neural network architectures or better weight initialization of pre-trained models [43; 44]. Transfer learning and meta-learning are also future directions worth exploring [45; 46; 47]. Moreover, how to unify various electrode montages of different EEG datasets when pre-training large EEG models is a challenge. EEG source localization, which converts senor-level EEG signals into the standard brain source space [48; 49], might be a potential solution.