# Adaptive Sampling for Efficient Softmax Approximation

Tavor Z. Baharav+

Eric and Wendy Schmidt Center

Broad Institute

Cambridge, MA, 02142

baharav@broadinstitute.org&Ryan Kang+

Department of Computer Science

Stanford University

Stanford, CA 94305

txryank@stanford.edu&Colin Sullivan+

AI Division

Software Engineering Institute

Pittsburgh, PA 15213

csullivan@sei.cmu.edu&Mo Tiwari

Department of Computer Science

Stanford University

Stanford, CA, 94305

motiwari@stanford.edu&Eric Luxenberg

Gridmatic

Cupertino, CA 95014

eric@gridmatic.com&David Tse

Department of Electrical Engineering

Stanford University

Stanford, CA 94305

dntse@stanford.edu&Mert Pilanci

Department of Electrical Engineering

Stanford University

Stanford, CA 94305

pilanci@stanford.edu

denotes equal contribution

###### Abstract

The softmax function is ubiquitous in machine learning and optimization applications. Computing the full softmax evaluation of a matrix-vector product can be computationally expensive in high-dimensional settings. In many applications, however, it is sufficient to calculate only the top few outputs of the softmax function. In this work, we present an algorithm, dubbed AdaptiveSoftmax, that adaptively computes the top \(k\) softmax values more efficiently than the full softmax computation, with probabilistic guarantees. We demonstrate the sample efficiency improvements afforded by AdaptiveSoftmax on real and synthetic data to corroborate our theoretical results. AdaptiveSoftmax yields \(>10\)x gain over full softmax computation on most datasets, yielding up to 30x improvement for Mistral7B evaluated on the Wikitext dataset. The adaptive method we propose for estimating the partition function (the softmax denominator) is of independent interest and can be used in other applications such as kernel density estimation.

## 1 Introduction

The softmax function appears in many different fields and applications. It is often used in multiclass classification problems, as the final operation in a neural network to obtain a probability distribution over classes, in reinforcement learning to obtain a probability distribution over possible actions, and in statistical mechanics to derive various thermodynamic quantities.

In machine learning applications, the softmax function often appears as the final operation in classification models and in attention layers. Crucially, the softmax function takes a vector of weightsas input and returns a probability distribution defined by those weights. Formally, the softmax function for a given temperature parameter \(\) is defined as:

\[_{}()_{i}=}}{_{j}e^{_{j}}}.\] (1)

where \(^{n}\) is the input vector of weights, also referred to as the _logits_. Usually, the logits are the result of a matrix-vector product (e.g., in a fully connected layer where the softmax is used as the nonlinear activation function). The output of the softmax function (Equation 1) is a probability distribution that is a "soft" version of the \(\) operator that is differentiable with respect to the logits. The softmax function can thus be used in gradient-based algorithms as a proxy for the non-differentiable \(\) function.

Intuitively, the temperature parameter \(\) controls the peakiness of the softmax output. A larger \(\) corresponds to a peakier distribution and a "harder" max. The choice of \(=1\) corresponds to the canonical softmax function \(\), and the choice of \(=\) corresponds to the "hard" \(*{argmax}\). The denominator of Equation (1), \(_{j}e^{_{j}}\), is called the partition function and is denoted by \(Z_{}\).

The softmax function is critical in many popular, recent machine learning applications like large language models (LLMs). However, it can present a computational bottleneck in high-dimensional applications. During the training of neural networks, for example, each training example \(x\) requires the computation of the softmax function \(_{}(x)\), the partition function \(Z_{}(x)\), and their gradients. During inference of these models, the number of possible labels for next-token prediction corresponds to the vocabulary size, which can be in the hundreds of thousands for common languages such as English. As such, there has been significant recent interest in accelerating the computation of the softmax function and its derivatives [37; 14; 15].

**Key Observations:** In many applications, we are only interested in identifying the top few outputs of the softmax function; in these settings, it is unnecessary to compute the smaller entries. This suggests that some of the computation of the full softmax function may be unnecessary and motivates our study. First, we observe that when the input vector to the softmax function is the result of a matrix-vector product, we can approximate the intermediary computation instead of exactly computing it. This, in turn, allows us to approximate the output of the softmax function and converts the problem of computing the softmax function from a computational one to a statistical one. We also note that the softmax output is heavily influenced by the largest input elements which suggests that we can allocate computation adaptively to larger input elements to estimate them with greater certainty. This procedure is inspired by recent work in multi-armed bandits that converts computational problems to statistical ones .

**Outline:** We begin this study with a summary of related work in Section 2. In Section 3, we formalize the reduction of computing the softmax function to a statistical estimation problem. In Section 4, we propose the AdaptiveSoftmax algorithm based on this reduction. In the same section, we provide probably approximately correct (PAC) guarantees for AdaptiveSoftmax and prove that it is more efficient than brute force computation. Crucially, AdaptiveSoftmax allocates greater computational resources towards important output values. In Section 5, we demonstrate the empirical advantages of our algorithm in several real-world applications, including in a multiclass classification setting and in large language models. In Section 6, we conclude with a discussion of further applications, potential limitations, and directions for future work.

## 2 Related Work

Recent work has identified the computational complexity of the softmax function as a significant bottleneck in recent machine learning applications . Well before the attention revolution,  proposed methods to accelerate softmax computation via a hierarchical model. In their work, a binary tree representing possible outputs as leaves is used and, at each step, the model must predict which path to traverse to a leaf. For a balanced tree with \(n\) leaves, the computational complexity of the softmax function is reduced to \(O( n)\) from \(O(n)\), at the expense of \(O(n)\) internal classifiers and providing only an approximate output dictated by the quality of the clustering. Google's word2vec models used Huffman trees instead of vanilla binary trees in a similar tree-based approach . Other approaches include target sampling, noise contrastive estimation and self normalization (summarized ), but all of these methods reduce the complexity in terms of the vocabulary size \(n\), rather than by the dimension \(d\). Additionally, our proposed algorithm provides direct PAC guarantees on the original softmax output, instead of approximating the softmax in a sequence of steps without provable accuracy guarantees. Independently, some works have developed fast methods to approximate the softmax gradients during training by using importance sampling over the classes, improving scaling with respect to \(n\)[11; 10] leading to a sampled softmax. This is in contrast with AdaptiveSoftmax, which utilizes importance sampling in an orthogonal direction to subsample the features efficiently, enabling gains in \(d\) at both train and test time. These sampled softmax methods were later specialized to kernel-based sampling, resulting in provably bounded bias [12; 38]. However, these and other optimized methods  typically require prior knowledge about the desired output label frequencies, leaving them susceptible to phenomena like distribution shift between training and inference data, where the frequency distribution changes at the time the model is evaluated. Unlike these approaches, AdaptiveSoftmax does not require auxiliary knowledge, is adaptive on a per instance basis, and provides provable guarantees for the true softmax computation directly rather than a proxy.

Our algorithm is inspired by adaptive sampling techniques from the multi-armed bandit literature. Randomized algorithms based on multi-armed bandit algorithms have seen a surge of recent work, due to their ability to provide instance-adaptive guarantees for a variety of problems. This idea was first formalized in the specific case of Monte Carlo Tree Search  and later studied in the context of hyper-parameter tuning . Recent work has formalized this approach into the framework of Bandit-Based Monte Carlo Optimization , where the computational task is reduced to one of statistical estimation that is solved efficiently with adaptivity. Applications of this framework include finding the medoid of a dataset [5; 7; 41], \(k\)-nearest neighbor graph construction [30; 34], Monte Carlo permutation-based multiple testing , and an adaptive singular value decomposition (SVD) . Most relevant is the recent work of , where the authors provide a general framework for adaptive sampling to approximate function evaluation at unknown but estimable points. This work provides general guarantees, but requires a bound on the Lipschitz factor of the function's gradients as input and has potentially poor performance on specific function classes due to its generality.

A sub-problem in our softmax approximation is identifying the index of the largest component; this is equivalent to the Maximum Inner Product Search (MIPS) problem on the preceding matrix-vector product. MIPS is a common problem that has inspired many directions of research [22; 32]. Many of these algorithms focus on specific use cases and place restrictive assumptions on the data (e.g., that all elements of the matrix-vector product are positive), require preprocessing, are not adaptive to the underlying data distribution, or lack PAC guarantees. One large family of MIPS algorithms are based on locality-sensitive hashing (LSH) [19; 2]. In addition to significant preprocessing overhead and practical implementation issues, a shortcoming of these LSH-based approaches is that the maximum dot product is often small compared to the vector norms in high dimensions, which necessitates many hashes and significant storage space (often orders of magnitude more than the data itself). Promising LSH-based algorithms have recently been applied to the problem of softmax computation [1; 15]. These methods focus on intensive preprocessing and work primarily by attaining gains in terms of \(n\). In contrast, AdaptiveSoftmax subsamples matrix-vector products and obtains gains with respect to \(d\). Furthermore, AdaptiveSoftmax provides an instance-adaptive algorithm with no required preprocessing and still has PAC guarantees.

## 3 Problem Formulation

In this work, we focus on the problem of identifying the \(k\) largest entries of the softmax of an input vector that is the result of a computationally expensive matrix-vector multiplication. Specifically, we analyze the setting where the input vector \(\) is the result of a matrix vector product \(Ax\), as is common in the final linear layer of neural networks (such scenarios frequently arise in other machine learning problems as well ). Our objective is to design an algorithm that can with probability at least \(1-\) estimate the top \(k\) values to multiplicative accuracy \(\), where \(\) and \(\) are given input parameters. For clarity of exposition, we focus on the case of \(k=1\), i.e., identifying and estimating the largest component. All our theoretical results, however, easily extend to the setting \(k>1\) (discussed in Section 4.2).

**Notation:** We use \([n]\) to denote the set \(\{1,2,,n\}\) and \(\|\|\) to denote the vector \(_{2}\) norm, unless otherwise specified. We use \(\|\|_{_{2}}\) to denote the Orlicz norm (i.e., the _sub-Gaussianity_ parameter or _variance proxy_) of a random variable; this is discussed in greater detail in Appendix A.2. For matrix \(A\) and vector \(x\), we denote the resulting product as \(=Ax\). Assuming for notational simplicity that the arms are in sorted order \(_{1}>_{2}_{n}\), we define the gaps between the entries of \(\) as \(_{i}=_{1}-_{i}\). We use the convention from the best-arm identification literature that \(_{1}=_{2}\) and assume that \(_{2}>0\) (this assumption is easily relaxed). Furthermore, we define \(_{i}=e^{_{i}}\) and \(_{i}=e^{_{i}/2}\), which are proportional to the optimal first (respectively, second) order sampling frequencies; these are discussed further in Section B.2. Finally, we define \(\) as the softmax output, and \(i^{*}\) as its largest entry (assumed to be unique), i.e.,

\[_{}(Ax)=, i^{*}=*{argmax}_{i[n]}p_{i}.\]

Our goal is to design an algorithm which efficiently outputs the best index \(i^{*}\) and an estimate its value where, with probability at least \(1-\), the best index is correct and the estimated value is within a factor of \(\) multiplicative accuracy. Mathematically, defining the algorithm's outputs as \(}[n]\) and \(_{i^{*}}\), we define the success events \(E_{},E_{}\) where the algorithm identifies the largest entry, and where it estimates its value to within multiplicative accuracy \(\). We define the algorithm as providing \((,)\)-PAC guarantees if these events happen simultaneously with probability at least \(1-\), with respect to the randomness of the algorithm.

\[E_{}=\{}=i^{*}\}\] (2) \[E_{}=\{(1-)p_{i^{*}}_{i^{*} }(1+)p_{i^{*}}\}.\] (3) \[(E_{} E_{}) 1-\] (4)

Our objective them becomes to design an algorithm that satisfies Equation (4) and minimizes the requisite sample complexity.

## 4 AdaptiveSoftmax Algorithm

We now introduce the AdaptiveSoftmaxAlgorithm, which approximates the output of the softmax in Algorithm 1. First, AdaptiveSoftmax approximates the softmax normalization constant \(Z_{}\) to a multiplicative accuracy of \(/4\) via NormalizationEstimation (Algorithm 2). Next, AdaptiveSoftmax identifies the best arm (or top \(k\) arms, depending on the setting) using a standard multi-armed bandit algorithm, BestArmId (Algorithm 3). In our setting, "arms" correspond to different rows of \(A\) and pulling arm \(i\) corresponds to computing a coordinate-wise scalar product \(A_{i,j}x_{j}\) for some coordinate \(j\) (we provide a more formal overview of the best-arm identification problem and the associated algorithm in Appendix A). Finally, AdaptiveSoftmax estimates the value of the identified best arm (or top \(k\) arms) to a multiplicative accuracy of \(/4\) by sampling each arm a sufficient number of times via EstimateArm (Algorithm 10).

We prove \((,)\)-PAC guarantees for AdaptiveSoftmax by union-bounding over the error probabilities in each step of Algorithm 1. Our results will show that, with probability at least \(1-\), AdaptiveSoftmax is able to identify the largest output of the softmax function and estimate its value to multiplicative accuracy \(\).

```
1:Input: Matrix \(A\), vector \(x\), temperature \(\), error \(\), failure probability \(\), variance proxy \(^{2}\)
2:Output:\(_{i^{*}}\) and \(}\), highest softmax probability and its index
3:# Estimate denominator of softmax
4:\(\)NormalizationEstimation\((A,x,,/4,/3,^{2})\)
5:# Compute index of best arm.
6:\(}\)BestArmId\((A,x,/3,^{2})\)
7:# Estimate value of best arm
8:\(_{i^{*}}\)EstimateArm\((A_{i^{*}},x,/4,/3)\)
9:\(_{i^{*}}=(_{i^{*}})/\)
10:return\(_{i^{*}},}\) ```

**Algorithm 1**AdaptiveSoftmax

These inputs are typical in the multi-armed bandit setting, but the variance proxy \(^{2}\) merits additional discussion. In order for our random-sampling-based approach to succeed, a bound on the rate of concentration of the estimators \(}\) is required; the quantity \(^{2}\) governs the concentration rate, as we discuss in Appendix A. In practice, such a bound holds very generally, for example as long as \(A\) and \(x\) have bounded entries. For algorithmic simplicity we utilize the following assumption.

**Assumption 1**.: _We assume that we are given a variance proxy bound \(^{2}\) for the sub-Gaussian parameters of the constructed estimators:_

\[^{2}\|A_{i,j}x_{J}\|_{_{2}}\ \ i,\ J ([n]).\]

We provide theoretical guarantees for AdaptiveSoftmax under Assumption 1. Recall that we defined our optimal first and second order sampling frequencies \(_{i}=e^{_{i}}\) and \(_{i}=e^{_{i}/2}\) (see Appendix B.2). We first show in Proposition 1 that our softmax normalization estimation algorithm (Algorithm 2) obtains the desired guarantees.

**Proposition 1**.: _For input \((0,1/2)\), \((0,1)\), and \(\) satisfying Assumption 1, Algorithm 2 will, with probability at least \(1-\), estimate \(Z_{}=_{j}e^{_{j}}\) to a multiplicative accuracy of \(\). On this success event, Algorithm 2 requires at most_

\[C^{2}^{2}(n()+()(_{j}_{j})^{2}(_{j} _{j})^{-1}+()^{-2})\]

_samples for some absolute constant \(C\), where non-asymptotic bounds with numerical constants are provided in Appendix B._

With the sample complexity of Algorithm 2 bounded, the complexity of best arm identification and the cost of estimating the best arm to a target accuracy are readily available from the multi-armed bandit literature. This enables us to state an overall result for AdaptiveSoftmax (Algorithm 1) in the following Theorem.

**Theorem 1**.: _For input \((0,1/2)\), \((0,1)\), and \(\) satisfying Assumption 1, Algorithm 1 identifies the largest component in \(_{}(Ax)\) and estimates its value to a multiplicative accuracy of \(\) with probability at least \(1-\), as in (4). On this success event, the algorithm uses \(T\) samples where_

\[T C^{2}(^{2}n()+_{i=1} ^{n})}{_{i}^{2}}+()(_{j}_{j})^{2 }}{_{j}_{j}}+(1/)}{^{2} }),\]

_for some absolute constant \(C\). Tighter bounds with non-asymptotic numerical constants are provided in Appendix B._

The proofs of these two results are detailed in Appendix B; we provide some intuition and brief sketches of the proofs here.

For Proposition 1, we first show that we can estimate the quantities \(\{_{i}\},\{_{i}\}\), to constant multiplicative error with high probability. This allows us to construct a sampling scheme based off of the asymptotically optimal sampling frequencies, and guarantee that each arm is sampled at least half of what this asymptotically optimal frequency requires. Then, sampling each arm \(i\) enough so that the first order Taylor expansion of \(e^{_{i}}\) is sufficiently accurate, we can sample further according to these determined frequencies to guarantee PAC estimation. This is an improved and specialized modification of  that exploits the structure of the softmax function to remove the assumption of Lipschitz gradients and yield improved sample complexity (this is discussed further in Appendix B.3).

Next, we utilize a classical best-arm identification algorithm to identify the best arm with high probability, leveraging standard results in Bandit-Based Monte Carlo Optimization . Finally, we sample the identified best arm enough times to estimate its value to multiplicative accuracy \(/4\) with high probability. By union bounding over these error probabilities, we achieve the desired PAC guarantees.

### Interpreting Theoretical Results

We now simplify and further interpret the sample complexity results in Theorem 1. First, note that the \(^{-2}\) dependence exhibited by NormalizationEstimation (Algorithm 2) is optimal: it is inherent even in estimating the mean of the best arm to accuracy \(\). The cost stemming from the second order error, which scales as \(^{-1}\), is bounded between \(^{2}^{2}(n/)^{-1}\) and \(n^{2}^{2}(n/)^{-1}\), where in the case where one arm is much better than the rest this will match the first term. Concretely, we analyze the setting where the minimum gap is \(\), i.e. \(_{1}-_{i}=_{i}\) for all \(i\).

**Corollary 1**.: _Under the conditions of Theorem 1, when the minimum gap is at least \(\), Algorithm 1 identifies and provides \((,)\)-PAC estimation (Equation (4)) of the largest softmax entry, using_

\[T C^{2}^{2}()(n+ n^{2}}{n+e^{}})+^{2}^{2} ^{-2}(1/)+n^{2}( )^{-2}\]

_samples for some universal constant \(C\). In the case where the gap is large (\( n\)), \(\) is not too small, and \(d<e^{e^{n}}\) (see Equation (47) for precise statement), this can be simplified to_

\[T C^{2}^{2}(()(n+ ^{-1})+^{-2}(1/)).\]

_where all sample complexities are for the \(1-\) success event._

The proof of this upper bound is in Appendix B.1. Note that this directly implies that when the gap is large (i.e. there is a clear largest output element) and \(\) is constant, the sample complexity is nearly linear in \(n\) and is upper bounded by \(C^{2}^{2}n(n/)\).

### Implementation details and extensions

There are many techniques that we can use to extend and improve AdaptiveSoftmax in practice. We discuss changes from the written algorithm in detail in Appendix C.

**Randomized Hadamard Transformation:** The variance-proxy bound \(^{2}\) of the arms plays a large factor in the AdaptiveSoftmax algorithm's sample complexity. The underlying sub-Gaussianity parameter of these estimators can be improved using techniques from randomized numerical linear algebra, such as the randomized Hadamard transform . If a small number of entries dramatically increase the variance of the estimator, then the randomized Hadamard transform will make the coordinates more uniform. We provide theoretical guarantees for this approach in Appendix A.3.1.

**Top-\(k\) Identification:** Extending our algorithmic results from best arm identification (top 1) to identifying multiple components (top \(k\)) follows directly from existing multi-armed bandit algorithms. Numerous algorithms have been developed for this setting , and variants for computational settings have been developed and studied in . For simplicity and clarity, we focused on the top \(1\) identification in this paper, but the top \(k\) extension readily follows. Furthermore, in numerical experiments we observe estimating the normalization constant \(Z_{}\) dominates the sample complexity, and the increase in cost from identifying the top \(k\) arms and estimating their values to multiplicative accuracy \(/4\) is minimal.

**Relaxing Assumption of Known sub-Gaussian Parameter \(^{2}\):** Assumptions regarding known arm concentration parameters are common in multi-armed bandit works and simplify theoretical exposition. These results can naturally be extended in several directions. One simple extension is to the setting where we have a separate sub-Gaussianity parameter \(^{2}_{i}\) for each arm, i.e., heterogeneous variances. A more practical extension is to the setting where we do not have a bound on the sub-Gaussianity parameter for each arm but know that the arm returns are bounded. In this setting, a common multi-armed bandit approach is to utilize the empirical variance . These approaches are discussed further in .

**Improved Estimators \(\):** Naively, the AdaptiveSoftmax algorithm samples coordinates uniformly at random with replacement from the set of coordinates \(\{1,,d\}\) to estimate each \(_{j}A_{ij}x_{j}\). This procedure can be improved in several ways. For example, we may utilize importance sampling and sample each coordinate with probability \(z_{j}|x_{j}|\). Furthermore, we can sample coordinates without replacement; this is known to yield tighter confidence intervals than sampling with replacement . We can combine these techniques and compute the effective variance as in . Sampling without replacement can be achieved in a computationally efficient manner via Gumbel sampling . We discuss these details further in Appendix A.3; these details may be of independent interest.

## 5 Experiments

In this section, we demonstrate the empirical advantages of AdaptiveSoftmax over the brute-force softmax computation in terms of sample complexity. All of our results are reproducible via a 1-line script, publicly available on GitHub at github.com/ThrunGroup/adaptiveSoftmax.

### Complexity on Synthetic Data

Crucially, the AdaptiveSoftmax algorithm scales sublinearly in \(d\). More precisely, Corollary 1 implies that, for fixed \(\) and \(\), the sample complexity of the AdaptiveSoftmax algorithm scales as \(O(n n)\). We now empirically validate this behavior.

We first run the AdaptiveSoftmax algorithm on two synthetic datasets. In each dataset, we generate \(A\) and \(x\) with \(n=100\) and vary \(d\).

In the first synthetic dataset, we set \(x\) to be a \(d\)-dimensional vector of all \(1\)s. We draw each element of \(A}}{{}}(0,1)\) and add the vector of all \(1\)s to the first row of A, thereby planting a signal. In expectation, the first row of \(A\) will have inner product \(d\) with \(x\) whereas all other rows will have inner product \(0\) with \(x\). Furthermore, all arms have expected variance \(^{2}_{i}\) that scales with \(d\).

In the second synthetic dataset, we draw each element of \(A}}{{}}(0,1)\) and set \(x\) to be \(|A_{1,:}|\), the entrywise absolute value of the first row of \(A\). Here, arms have expected variance \(^{2}_{i}=(1)\).

Figure 1: Sample complexity of the AdaptiveSoftmax algorithm and the brute-force softmax computation on two different synthetic datasets as a function of \(d\). Error bars are obtained from 100 random trials. The sample complexity of the AdaptiveSoftmax algorithm scales with respect to \(d\) for (a) but does not for (b), as expected. The average gains for \(=10\%\) and \(=30\%\) are \(3.953\) for (a) and \(29.039\) for (b), increasing with increasing dimension. Confidence intervals are 1std.

Figures 1(a) and 1(b) demonstrates the scaling of the AdaptiveSoftmax Algorithm on each of the two datasets. On the first synthetic dataset, the AdaptiveSoftmax algorithm scales with \(d\) because the variance proxies \(_{i}^{2}\) do. On the second synthetic dataset, the AdaptiveSoftmax algorithm does not exhibit significant scaling with \(d\). On both datasets, the AdaptiveSoftmax algorithm significantly outperforms the naive brute-force computation of the softmax function.

### Multinomial Logistic Regression

Multinomial logistic regression (MNL) is a form of multiclass classification in which the final operation performed by the classifier is of the form:

\[P(y=c)=^{}h(x))}}{_{c^{}=1}^{C}e^{(w_{ c^{}}^{}h(x))}}\] (5)

i.e., the probabilities that datapoint \(x\) belongs to each class \(c\) is given by the softmax applied to the vector \(Wh(x)\), where \(W\) is the matrix containing rows \(w_{1},,w_{c}\) and \(h(x)\) is a latent representation of \(x\) (i.e., the forward pass of some neural network on \(x\)).

The multinomial logistic regression is naturally amenable to accelerated softmax computation in Equation (5). In many real-world settings, both the number of classes \(C\) and the dimension of the latent representation \(h(x)\) (and therefore the dimensionality of each \(w_{c}\)) can be very large, motivating the usage of AdaptiveSoftmax to identify and estimate the probability of the most likely class. However, the application of AdaptiveSoftmax extends far past vanilla MNL. For instance, the final layer of any neural network classifier utilizing softmax can also be viewed as an MNL problem. We now provide several such practical settings for which we demonstrate the benefits of applying the AdaptiveSoftmax algorithm.

### AdaptiveSoftmax Performance on Real Data

We now demonstrate the performance of the AdaptiveSoftmax algorithm on several real-world datasets. For each setting, we provide the sample complexity gain relative to the sample complexity of the brute-force, naive softmax computation sample complexity \(nd\). We also provide the success rate of our algorithm in each setting, i.e., the proportion of times the AdaptiveSoftmax algorithm correctly identifies the maximum likelihood output (i.e. \(^{}=i^{}\)) and estimates its probability \(p_{i^{}}\) within a multiplicative error of \(=30\%\).

#### 5.3.1 Application to CNNs

We consider the application of AdaptiveSoftmax to CNN classifiers on two distinct image classification datasets:

1. The MNIST dataset, containing black and white images of handwritten digits as input and ten output classes representing all ten possible digits.
2. The EuroSAT dataset, containing RGB satellite imagery as input and ten output classes, representing possible land types (e.g., river, residential, etc)

On both of these datasets and for distinct architectures, we show that AdaptiveSoftmax provides a drastic improvement in sample efficiency.

MnistFor the MNIST dataset, we train a shallow CNN from scratch with two convolutional blocks (Conv2d, ReLu, MaxPool, BatchNorm). This model achieves over 99% accuracy on the test set. The matrix \(A\) is obtained by extracting the weight matrix of the model's final linear layer. The vector \(x\) is extracted as the output of the final hidden layer (the layer before the final linear layer) constructed by passing the MNIST image through the trained model and flattening the result. The dimensionality of \(x\) is adjusted by changing the number of output channels of the convolution blocks. The sample complexity of our algorithm is measured by running the algorithm on \(1000\) different images in test set with same matrix \(A\). The empirical error rate \(\) is calculated as the fraction of experiments where the adaptive algorithm fails to identify the same class, or fails to estimate the probability to accuracy \(\), as assigned by exact computation.

EuroSATWe also utilize a larger pre-trained CNN classifier fine-tuned on the EuroSAT dataset, to show that AdaptiveSoftmax works with larger more sophisticated CNNs. Specifically, we freeze all convolution blocks of VGG-19 (pretrained on ImageNet) and changed the final output dimension to 10 classes for EuroSAT without freezing the weights. The resulting model achieves 92% accuracy on the test set. As before, the matrix \(A\) can be extracted from the weights of the final linear layer and the vector \(x\) represents the final hidden layer activations. The empirical error rate \(\) is calculated in the same manner as for MNIST.

#### 5.3.2 Application to LLMs

We also apply the AdaptiveSoftmax algorithm to LLMs using HuggingFace's AutoModelForCausalLM module for the task-generation task . The matrix \(A\) is the lm-head layer for each model, and the queries \(x\) are the final hidden states of the model that is extracted by running a forward pass of the model on the given dataset with a window moving at a certain stride. The context window and stride is modified to generate a desired number of queries.

Our matrix \(A\) is the extracted lm-head from HuggingFace's AutoModelForCausalLM for the four models: GPT-2 \((n=50257,d=768)\), Llama3-7B \((n=128256,d=4096)\), Mistral7B \((n=32000,d=4096)\), and Gemma7B \((n=256000,d=3072)\). Our task is task-generation, and we generate our queries \(x\) by using two datasets (Wikitext and Penn Treebank) with a sliding window of certain stride. Stride and context window is set to get \(q=1000\) number of queries. Constants and confidence intervals given by theory are empirically quite loose, so we tuned algorithm parameters (constant coefficients for stage length and confidence interval width) on initial training data, described in Appendix C. An aggressive tuning strategy was undertaken in order to demonstrate the potential gains in sample complexity provided by AdaptiveSoftmax. Specifically, constant multiples were applied to variance estimate within Algorithm 3 and Algorithm 2. Due to the limited sample set, this approach occasionally overoptimized the constants on training data, yielding lower success rates than targeted. However, from the results, it is clear that this target parameter still provides users sufficient control over the tradeoff between true success rate and sample complexity.

## 6 Discussion, Limitations, and Future Work

In this work, we proposed a theoretically novel and practically efficient algorithm for approximating the softmax function. We provided theoretical guarantees on the accuracy of our approximation and demonstrated that, with fewer samples than exact computation, we can approximate the softmax function to the desired accuracy. We further demonstrated the viability of our proposed algorithm in two real-world settings, multinomial logistic regression and LLM inference.

A potential limitation of our proposed algorithm is that it is most beneficial when the inner dimension of the matrix vector product is high dimensional; its benefits over exact computation are more modest

  Dataset (Model) & \(=10\%\) & \(=5\%\) & \(=1\%\) \\  EuroSAT (VGG-19) & 5.18x (80.62\%) & 5.16x (83.00\%) & 4.54x (98.37\%) \\  MNIST (Shallow CNN) & 8.95x (92.25\%) & 8.81x (93.75\%) & 8.13x (99.38\%) \\  

Table 1: Performance improvement and success rate afforded by AdaptiveSoftmax for multinomial logistic regression on two different real-world datasets. We used a total of \(q=800\) test queries to measure success rate.

  Dataset (Model) & \(=10\%\) & \(=5\%\) & \(=1\%\) \\  Wikitext (GPT-2) & 8.25x (88.94\%) & 7.80x (93.54\%) & 6.67x (98.26\%) \\  Wikitext (Llama3-7B) & 14.68x (91.44\%) & 11.43x (94.04\%) & 6.88x (99.38\%) \\  Wikitext (Mistral7B) & 32.65x (89.08\%) & 26.37x (91.20\%) & 17.71x (97.77\%) \\  Penn Treebank (GPT-2) & 8.10x (81.68\%) & 7.50x (90.73\%) & 6.66x (96.79\%) \\  Penn Treebank (Llama3-7B) & 19.18x (87.82\%) & 16.57x (91.60\%) & 10.72x (97.81\%) \\  

Table 2: Performance improvement and success rate afforded by AdaptiveSoftmax for LLM inference (improvement for final softmax layer). Experiment details in Section 5.3.2. We used \(q=1000\) unseen test queries to measure \(\)-accuracy.

when the inner dimension is small. In particular, the exact computation of the matrix-vector product preceding a softmax operation is usually performed efficiently using BLAS (Basic Linear Algebra Subroutines, which are highly optimized). Adaptivity at its core is inherently sequential, whereas BLAS operations take advantage of batch computation. In this work we proposed minimally adaptive algorithms, with only a logarithmic number of rounds of adaptivity, but there are important directions of future work to realize these theoretical gains in practice.

**Limitations:** Theoretical sample complexity bounds are useful for understanding the fundamental properties of an algorithm, but in practice, wall-clock time is often the metric of interest. Many of the steps in our algorithm can be batched and made BLAS efficient, yielding comparable wall clock times to brute force computation. However, in general adaptivity is the opposite of batching, as can be seen when we modify our algorithm to adapt to arm specific variances. In this case, we must sample each arm individually, as the number of samples required for each arm is different. This is a trade-off between adaptivity and wall-clock time, and in practice, the choice of which to prioritize depends on the specific application (energy efficiency, computational resources, etc.). There are also possible theoretical analyses, where we can e.g. create batches of arms with similar empirical variance and sample all arms within a batch together, leading to a trade-off between adaptivity and batched computational efficiency. Additionally, in large language models, the final softmax layer is often not a computationally significant step, so while such a method may greatly accelerate multinomial logistic regression, more work may be required to have this accelerate LLMs.

Given the ubiquity of the softmax function in today's machine learning workflows, we hope that our algorithm will help pave the way for an optimized adaptive softmax that can accelerate a wide class of machine learning models. An interesting direction of future work is trying to combine this multi-armed bandit approach with LSH  to obtain (for the attention case) subquadratic complexity in \(n\), and sublinear complexity in \(d\). The adaptive method we propose for estimating the normalizing constant of the softmax function is of independent interest, and holds potential for applications in kernel density estimation and other machine learning tasks.