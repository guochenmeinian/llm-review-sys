# Provable Benefit of Cutout and CutMix

for Feature Learning

Junsoo Oh

KAIST AI

junsoo.oh@kaist.ac.kr &Chulhee Yun

KAIST AI

chulhee.yun@kaist.ac.kr

###### Abstract

Patch-level data augmentation techniques such as Cutout and CutMix have demonstrated significant efficacy in enhancing the performance of vision tasks. However, a comprehensive theoretical understanding of these methods remains elusive. In this paper, we study two-layer neural networks trained using three distinct methods: vanilla training without augmentation, Cutout training, and CutMix training. Our analysis focuses on a feature-noise data model, which consists of several label-dependent features of varying rarity and label-independent noises of differing strengths. Our theorems demonstrate that Cutout training can learn low-frequency features that vanilla training cannot, while CutMix training can learn even rarer features that Cutout cannot capture. From this, we establish that CutMix yields the highest test accuracy among the three. Our novel analysis reveals that CutMix training makes the network learn all features and noise vectors "evenly" regardless of the rarity and strength, which provides an interesting insight into understanding patch-level augmentation.

## 1 Introduction

Data augmentation is a crucial technique in deep learning, particularly in the image domain. It involves creating additional training examples by applying various transformations to the original data, thereby enhancing the generalization performance and robustness of deep learning models. Traditional data augmentation techniques typically focus on geometric transformations such as random rotations, horizontal and vertical flips, and cropping (Krizhevsky et al., 2012), or color-based adjustments such as color jittering (Simonyan and Zisserman, 2014).

In recent years, several new data augmentation techniques have appeared. Among them, patch-level data augmentation techniques like Cutout (DeVries and Taylor, 2017) and CutMix (Yun et al., 2019) have received considerable attention for their effectiveness in improving generalization. Cutout is a straightforward method where random rectangular regions of an image are removed during training. In comparison, CutMix adopts a more complex strategy by cutting and pasting sections from different images and using mixed labels, encouraging the model to learn from blended contexts. The success of Cutout and CutMix has triggered the development of numerous variants including Random Erasing (Zhong et al., 2020), GridMask (Chen et al., 2020), CutBlur (Yoo et al., 2020), Puzzle Mix (Kim et al., 2020), and Co-Mixup (Kim et al., 2021). However, despite the empirical success of these patch-level data augmentation techniques in various image-related tasks, a lack of comprehensive theoretical understanding persists: _why and how do they work?_

In this paper, we aim to address this gap by offering a theoretical analysis of two important patch-level data augmentation techniques: Cutout and CutMix. Our theoretical framework draws inspiration from a study by Shen et al. (2022), which explores a data model comprising multiple label-dependent feature vectors and label-independent noises of varying frequencies and intensities. The key idea of this work is that learning features with low frequency can be challenging due to strong noises (i.e., low signal-to-noise ratio). We focus on how Cutout and CutMix can aid in learning such rare features.

### Our Contributions

In this paper, we consider a patch-wise data model consisting of features and noises, and use two-layer convolutional neural networks as learner networks. We focus on three different training methods: vanilla training without any augmentation, Cutout training, and CutMix training. We refer to these training methods in our problem setting as ERM, Cutout, and CutMix. We investigate how these methods affect the network's ability to learn features. We summarize our contributions below:

* We analyze ERM, Cutout, and CutMix, revealing that Cutout outperforms ERM since it enables the learning of rarer features compared to ERM (Theorem 3.1 and Theorem 3.2). Furthermore, CutMix demonstrates almost perfect performance (Theorem 3.3) by learning all features.
* Our main intuition behind the negative result for ERM is that ERM learns to classify training samples by memorizing noise vectors instead of learning meaningful features if the features do not appear frequently enough. Hence, ERM suffers low test accuracy because it cannot learn rare features. However, Cutout alleviates this challenge by removing some of the strong noise patches, allowing it to learn rare features to some extent.
* We prove the near-perfect performance of CutMix based on a novel technique that views the non-convex loss as a composition of a convex function and reparameterization. This enables us to characterize the global minimum of the loss and show that CutMix forces the model to activate almost uniformly across every patch of inputs, allowing it to learn all features.

### Related Works

Feature Learning Theory.Our work aligns with a recent line of studies investigating how training methods and neural network architectures influence feature learning. These studies focus on a specific data distribution composed of two components: label-dependent features and label-independent noise. The key contribution of this body of work is the exploration of which training methods or neural networks are most effective at learning meaningful features and achieving good generalization performance. Allen-Zhu and Li (2020) demonstrate that an ensemble model can achieve near-perfect performance by learning diverse features, while a single model tends to learn only certain parts of the feature space, leading to lower test accuracy. In other works, Cao et al. (2022); Kou et al. (2023a) explore the phenomenon of benign overfitting when training a two-layer convolutional neural network. The authors identify the specific conditions under which benign overfitting occurs, providing valuable insights into how these networks behave during training. Several other studies seek to understand various aspects of deep learning through the lens of feature learning (Zou et al., 2021; Jelassi and Li, 2022; Chen et al., 2022, 2023; Li and Li, 2023; Huang et al., 2023a, 2023).

Theoretical Analysis of Data Augmentation.Several works aim to analyze traditional data augmentation from different perspectives, including kernel theory (Dao et al., 2019), margin-based approach (Rajput et al., 2019), regularization effects (Wu et al., 2020), group invariance (Chen et al., 2020b), and impact on optimization (Hanin and Sun, 2021). Moreover, many papers have explored various aspects of a recent technique called Mixup (Zhang et al., 2017). For example, studies have explored its regularization effects (Carratino et al., 2020; Zhang et al., 2020), its role in improving calibration (Zhang et al., 2022), its ability to find optimal decision boundaries (Oh and Yun, 2023) and its potential negative effects (Chidambaram et al., 2021; Chidambaram and Ge, 2024). Some works investigate the broader framework of Mixup, including CutMix, which aligns with the scope of our work. Park et al. (2022) study the regularization effect of mixed-sample data augmentation within a unified framework that contains both Mixup and CutMix. In Oh and Yun (2023), the authors analyze masking-based Mixup, which is a class of Mixup variants that also includes CutMix. In their context, they show that masking-based Mixup can deviate from the Bayes optimal classifier but require less training sample complexity. However, neither work provides a rigorous explanation for why CutMix has been successful. The studies most closely related to our work include Shen et al. (2022); Chidambaram et al. (2023); Zou et al. (2023). Shen et al. (2022) regard traditional data augmentation as a form of feature manipulation and investigate its advantages from a feature learning perspective. Both Chidambaram et al. (2023) and Zou et al. (2023) analyze Mixup within a feature learning framework. However, patch-level data augmentation such as Cutout and CutMix, which are the focus of our work, have not yet been explored within this context.

Problem Setting

In this section, we introduce the data distribution and neural network architecture, and formally describe the three training methods considered in this paper.

### Data Distribution

We consider a binary classification problem on structured data, consisting of patches of label-dependent vectors (referred to as _features_) and label-independent vectors (referred to as _noise_).

**Definition 2.1** (Feature Noise Patch Data).: We define a data distribution \(\) on \(^{d P}\{-1,1\}\) such that \((,y)\) where \(=^{(1)},,^{(P)}^{d P}\) and \(y\{ 1\}\) is constructed as follows.

1. Choose the _label_\(y\{ 1\}\) uniformly at random.
2. Let \(\{_{s,k}\}_{s\{ 1\},k[K]}^{d}\) be a set of orthonormal _feature vectors_. Choose the feature vector \(^{d}\) for data point \(\) as \(=_{y,k}\) with probability \(_{k}\) from \(\{_{y,k}\}_{k[K]}^{d}\), where \(_{1}++_{K}=1\) and \(_{1}_{K}\). In our setting, there are three types of features with significantly different frequencies: _common features_, _rare features_, and _extremely rare features_, ordered from most to least frequent. The indices of these features partition \([K]\) into \((_{C},_{R},_{E})\).
3. We construct \(P\) patches of \(\) as follows. * **Feature Patch**: Choose \(p^{*}\) uniformly from \([P]\) and we set \(^{(p^{*})}=\). * **Dominant Noise Patch**: Choose \(\) uniformly from \([P]\{p^{*}\}\). We construct \(^{()}=+^{()}\) where \(\) is _feature noise_ drawn uniformly from \(\{_{1,1},_{-1,1}\}\) with \(0<<1\) and \(^{()}\) is Gaussian _dominant noise_ drawn from \(N(,_{}^{2})\). * **Background Noise Patch**: The remaining patches \(p[P]\{p^{*},\}\) consist of Gaussian _background noise_, i.e., we set \(^{(p)}=^{(p)}\) where \(^{(p)} N(,_{}^{2})\).

Here, the noise covariance matrix is defined as \(:=-_{s,k}_{s,k}_{s,k}^{}\) which ensures that Gaussian noises are orthogonal to all features. We assume that the dominant noise is stronger than the background noise, i.e., \(_{}<_{}\).

Our data distribution captures characteristics of image data, where the input consists of several patches. Some patches contain information relevant to the image labels, such as cat faces for the label "cat," while other patches contain information irrelevant to the labels, such as the background. Intuitively, there are two ways to fit the given data: learning features or memorizing noise. If a model fits the data by learning features, it can correctly classify test data having the same features. However, if a model fits the data by memorizing noise, it cannot generalize to unseen data because noise patches are not relevant to labels. Thus, learning more features is crucial for achieving better generalization.

In real-world scenarios, different features may appear with varying frequencies. For instance, the occurrences of cat's faces and cat's tails in a dataset might differ significantly, although both are relevant to the "cat" label. Our data distribution reflects these characteristics by considering features with varying frequencies. To emphasize the distinctions between the three training methods we analyze, we categorize features into three groups: common, rare, and extremely rare. We refer to data points containing these features as _common data_, _rare data_, and _extremely rare data_, respectively. We emphasize that these terminologies are chosen merely to distinguish the three different levels of rarity, and even "extremely rare" features appear in a nontrivial fraction of the training data with high probability (see our assumptions in Section 2.4).

Comparison to Previous Work.Our data distribution is similar to those considered in Shen et al. (2022) and Zou et al. (2023), which investigate the benefits of standard data augmentation methods and Mixup by comparing them to vanilla training without any augmentation. These results consider two types of features--common and rare--with different levels of rarity, along with two types of noise: feature noise and Gaussian noise. However, we consider three types of features: common, rare, and extremely rare, and three types of noise: feature noise, dominant noise, and background noise. This distinction allows us to compare three distinct methods and demonstrate the differences between them, whereas Shen et al. (2022) and Zou et al. (2023) compared only two methods.

### Neural Network Architecture

For the prediction model, we focus on the following two-layer convolutional neural network where the weights in the second layer are fixed at \(1\) and \(-1\), with only the first layer being trainable. Several works including Shen et al. (2022) and Zou et al. (2023) also focus on similar two-layer convolutional neural networks.

**Definition 2.2** (2-Layer CNN).: We define 2-layer CNN \(f_{}:^{d P}\) parameterized by \(=\{_{1},_{-1}\}^{d 2}\). For each input \(=(^{(1)},,^{(P)})^{d P}\), we define

\[f_{}():=_{p[P]}(_{1},^{(p)} )-_{p[P]}(_{-1},^{( p)}),\]

where \(()\) is a smoothed version of leaky ReLU activation, defined as follows.

\[(z):=z-&z r\\ z^{2}+ z&0 z r\\  z&z 0,\]

where \(0< 1\) and \(r>0\).

Previous works on the theory of feature learning often consider neural networks with (smoothed) ReLU or polynomial activation functions. However, we adopt a smoothed leaky ReLU activation, which always has a positive slope, to exclude the possibility of neurons "dying" during the complex optimization trajectory. Using smoothed leaky ReLU to analyze the learning dynamics of neural networks is not entirely new; there is a body of work that studies phenomena such as benign overfitting (Frei et al., 2022) and implicit bias (Frei et al., 2022; Kou et al., 2023) by analyzing neural networks with (smoothed) leaky ReLU activation.

A key difference between ReLU and leaky ReLU lies in the possibility of ReLU neurons "dying" in the negative region, where some negatively initialized neurons remain unchanged throughout training. As a result, using ReLU activation requires multiple neurons to ensure the survival of neurons at initialization, which becomes increasingly probable as the number of neurons increases. In contrast, the derivative of leaky ReLU is always positive, ensuring that a single neuron is often sufficient. Therefore, for mathematical simplicity, we consider the case where the network has a single neuron for each positive and negative output. We believe that our analysis can be extended to the multi-neuron case as we validate numerically in Appendix A.2.

### Training Methods

Using a training set sampled from the distribution \(\), we would like to train our network \(f_{}\) to learn to correctly classify unseen data points from \(\). We consider three learning methods: vanilla training without any augmentation, Cutout, and CutMix. We first introduce necessary notation for our data and parameters, and then formalize training methods within our framework.

Training Data.We consider a training set \(=\{(_{i},y_{i})\}_{i[n]}\) comprising \(n\) data points, each independently drawn from \(\). For each \(i[n]\), we denote \(_{i}=(_{i}^{(1)},,_{i}^{(P)})\).

Initialization.We initialize the model parameters in our neural network using random initialization. Specifically, we initialize the model parameter \(^{(0)}=\{_{1}^{(0)},_{-1}^{(0)}\}\), where \(_{1}^{(0)},_{-1}^{(0)}}}{{ }}N(,_{0}^{2}_{d})\). Let us denote updated model parameters at iteration \(t\) as \(^{(t)}=\{_{1}^{(t)},_{-1}^{(t)}\}\).

#### 2.3.1 Vanilla Training

The vanilla approach to training a model \(f_{}\) is solving the empirical risk minimization problem using gradient descent. We refer to this method as ERM. Then, ERM updates parameters \(^{(t)}\) of a model using the following rule.

\[^{(t+1)}=^{(t)}-_{}_{} (^{(t)}),\]where \(\) is a learning rate and \(_{}()\) is the ERM training loss defined as

\[_{}():=_{i[n]}(y_{i}f_{ }(_{i})),\] (1)

where \(()\) is the logistic loss \((z)=(1+e^{-z})\).

#### 2.3.2 Cutout Training.

Cutout (DeVries and Taylor, 2017) is a data augmentation technique that randomly cuts out rectangular regions of image inputs. In our patch-wise data, we regard Cutout training as using inputs with masked patches from the original data. For each subset \(\) of \([P]\) and \(i[n]\), we define augmented data \(_{i,}^{d P}\) as a data point generated by cutting the patches with indices in \(\) out of \(_{i}\). We can represent \(_{i,}\) as

\[_{i,}=(_{i,}^{(1)},,_{i, }^{(P)}),_{i,}^{(p)}=_{i}^{(p)}& p,\\ &.\]

Note that the output of the model \(f_{}()\) on this augmented data point \(_{i,}\) is

\[f_{}(_{i,})=_{p}(< _{1},_{i}^{(p)}>)-_{p}( <_{-1},_{i}^{(p)}>).\]

Then, the objective function for Cutout training can be defined as

\[_{}():=_{i[n]}_{ _{}}[(y_{i}f_{}(_{i, }))],\]

where \(_{}\) is a uniform distribution on the collection of subsets of \([P]\) with cardinality \(C\), where \(C\) is a hyperparameter satisfying \(1 C<\).1 We refer to the process of training our model using gradient descent on Cutout loss \(_{}()\) as Cutout, and its update rule is

\[^{(t+1)}=^{(t)}-_{}_{} (^{(t)}),\] (2)

where \(\) is a learning rate.

#### 2.3.3 CutMix Training.

CutMix (Yun et al., 2019) involves not only cutting parts of images, but also pasting them into different images as well as assigning them mixed labels. For each subset \(\) of \([P]\) and \(i,j[n]\), we define the augmented data point \(_{i,j,}^{d P}\) as the data obtained by cutting patches with indices in \(\) from data \(_{i}\) and pasting them into \(_{j}\) at the same indices \(\). We can write \(_{i,j,}\) as

\[_{i,j,}=(_{i,j,}^{(1)},,_{i,j,}^{(P)}),_{i,j,}^{(p)}= _{i}^{(p)}&p,\\ _{j}^{(p)}&.\]

The one-hot encoding of the labels \(y_{i}\) and \(y_{j}\) are also mixed with proportions \(|}{P}\) and \(1-|}{P}\), respectively. This mixed label results in the loss of the form

\[|}{P}(y_{i}f_{}(_{i,j,}))+( 1-|}{P})(y_{j}f_{}(_{i,j,})).\]

From this, the CutMix training loss \(_{}()\) can be defined as

\[_{}():=}_{i,j[n]} _{_{}}|}{P}(y_{i}f_{}(_{i,j,}))+(1-|}{P})(y_{j}f_{}(_{i,j,})),\]

where \(_{}\) is a probability distribution on the set of subsets of \([P]\) which samples \(_{}\) as follows.21. Choose the cardinality \(s\) of \(\) uniformly at random from \(\{0,1,,P\}\), and
2. Choose \(\) uniformly at random from the collection of subsets of \([P]\) with cardinality \(s\).

We refer to the process of training our network using gradient descent on CutMix loss \(_{}()\) as \(\), and its update rule is

\[^{(t+1)}=^{(t)}-_{}_{} (^{(t)}),\] (3)

where \(\) is a learning rate.

### Assumptions on the Choice of Problem Parameters

To control the quantities that appear in the analysis of training dynamics, we make assumptions on several quantities in our problem setting. For simplicity, we use choices of problem parameters as a function of the dimension of patches \(d\) and consider sufficiently large \(d\).

We use the standard asymptotic notation \((),(),(),o(),()\) to express the dependency on \(d\). We also use \(}(),(),()\) to hide logarithmic factors of \(d\). Additionally, \((d)\) (or \((d)\)) represents quantities that increase faster than \(d^{c_{1}}\)(or \(( d)^{c_{1}}\)) and slower than \(d^{c_{2}}\) (or \(( d)^{c_{2}}\)) for some constant \(0<c_{1}<c_{2}\). Similarly, \(o(1/(d))\) (or \(o(1/(d))\)) denotes some quantities that decrease faster than \(1/d^{c}\) (or \(1/( d)^{c}\)) for any constant \(c\). Finally, we use \(f(d)=o(g(d)/(d))\) when \(f(d)/g(d)=o(1/(d))\) for some function \(f\) and \(g\) of \(d\).

Assumptions.We assume that \(P=(1)\) and \(P 8\) for simplicity. Additionally, we consider a high-dimensional regime where the number of data points is much smaller than the dimension \(d\), which is expressed as \(n=o(_{}^{-1}_{}d^{ }/(d))\). We also assume that \(_{k}n=(n^{} d)\) for all \(k[K]\), which ensures the sufficiency of data points with each feature.

In addition, as we will describe in Section 4, the relative scales between the frequencies of features and the strengths of noises play crucial roles in our analysis, as they serve as a proxy for the "learning speed" in the initial phase. For common features \(k_{C}\), we assume \(_{k}=(1)\) and the learning speed of common features is much faster than that of dominant noise, which translates into the assumption \(_{}^{2}d=o( n)\). For rare features \(k_{R}\), we assume \(_{k}=(_{R})\) for some \(_{R}\), and we consider the case where the learning speed of rare features is much slower than that of dominant noise but faster than background noise, which is expressed as \(_{R}n=o(^{2}_{}^{2}d/(d))\) and \(_{}^{2}d=o(_{R}n)\). Finally, for extremely rare features \(k_{E}\), we say \(_{k}=(_{E})\) for some \(_{E}\) and their learning is even slower than that of background noises, which can be expressed as \(_{E}n=o(^{2}_{}^{2}d/(d))\).

Lastly, we assume the strength of feature noise satisfies \(=o(n^{-1}_{}^{2}d/(d))\), and \(r,_{0},>0\) are sufficiently small so that \(_{0},r=o(/(d))\), \(=o(r_{}^{-2}d^{-1}/(d))\).

We list our assumptions in Assumption B.1 and there are many choices of parameters satisfying the set of assumptions, including:

\[P=8,C=2,n=(d^{0.4}),=(d^{-0.02 }),=(d)},_{0}=(d^{-0.2}),r= (d^{-0.2}),\] \[_{}=(d^{-0.305}),_{}=(d^{-0.375}),_{R}=(d^{-0.1}),_{E}= (d^{-0.195}),=(d^{-1}).\]

## 3 Main Results

In this section, we provide a characterization of the high probability guarantees for the behavior of models trained using three distinct methods we have introduced. We denote by \(T^{*}\) the maximum admissible training iterates and we assume \(T^{*}=(d)}{}\) with a sufficiently large polynomial in \(d\). In all of our theorem statements, the randomness is over the sampling of training data and the initialization of models and all results hold under the condition that \(d\) is sufficiently large.

The following theorem characterizes training accuracy and test accuracy achieved by \(\).

[MISSING_PAGE_EMPTY:7]

To provide the proof overview, let us introduce some additional notation. For each \(i[n]\), recall that the corresponding input point can be written as \(_{i}=(_{i}^{(1)},,_{i}^{(P)})\). We use \(p_{i}^{*}\) and \(_{i}\) to denote the indices of its feature patch and dominant noise patch, respectively. For each feature vector \(_{s,k}\), where \(s\{ 1\}\) and \(k[K]\), let \(_{s,k}[n]\) represent the set of indices of data points having the feature vector \(_{s,k}\), and \(_{s}=_{k=1}^{K}_{s,k}\) denotes the set of indices of data with label \(s\). For each data point \(i[n]\) and dominant or background noise patch \(p[P]\{p_{i}^{*}\}\), we refer to the Gaussian noise inside \(_{i}^{(p)}\) as \(_{i}^{(p)}\).

### Vanilla Training and Cutout Training

We now explain why ERM fails to learn (extremely) rare features, while Cutout can learn rare features but not extremely rare features. Let us consider ERM. From (1), for \(s,s^{}\{ 1\},k[K],i[n]\) and \(p[P]\{p_{i}^{*}\}\), the component of \(_{s}\) in the feature vector \(_{s^{},k}\)'s direction is updated as

\[_{s}^{(t+1)},_{s^{},k}= _{s}^{(t)},_{s^{},k}-}{n} _{j_{s^{},k}}^{}(y_{j}f_{^{(t)}}(_{j}))^{}(_{s}^{(t)},_{s^{},k} ),\] (4)

and similarly, the "update" of inner product of \(_{s}\) with a noise patch \(_{i}^{(p)}\) can be written as

\[_{s}^{(t+1)},_{i}^{(p)} _{s}^{(t)},_{i}^{(p)}-}{n}^{} (y_{i}f_{^{(t)}}(_{i}))^{}(_{s}^ {(t)},_{i}^{(p)})\|_{i}^{(p)}\|^{2},\] (5)

where the approximation is due to the near-orthogonality of Gaussian random vectors in the high-dimensional regime. This approximation shows that \(_{s}^{(t+1)},_{s^{},k}\)'s and \(_{s}^{(t)},_{i}^{(p)}\)'s are almost monotonically increasing or decreasing. We address the approximation errors using a variant of the technique introduced by Cao et al. (2022), as detailed in Appendix B.3.

From (4) and (5), we can observe that in the early phase of training satisfying \(-^{}(y_{i}f_{^{(t)}}(_{i}))=(1)\), the main factor for the speed of learning features and noises are the number of feature occurrence \(|_{s^{},k}|\) and the strength of noise \(\|_{i}^{(p)}\|^{2}\). From our assumptions introduced in Section 2.4, if we compare the learning speed of different components, we have

common features \(\) dominant noises \(\) rare features \(\) background noises \(\) extremely rare features,

in terms of "learning speed." Based on this observation, we conduct a three-phase analysis for ERM.

* **Phase 1**: Learning common features quickly.
* **Phase 2**: Fitting (extremely) rare data by memorizing dominant noises instead of learning features.
* **Phase 3**: A model cannot learn (extremely) rare features since gradients of all data are small.

The main intuition behind why ERM cannot learn (extremely) rare features is that the gradients of all data containing these features become small after quickly memorizing dominant noise patches. In contrast, since Cutout randomly cuts some patches out, there exist augmented data points that do not contain dominant noises and have only features and background noises. This allows Cutout to learn rare features, thanks to these augmented data. However, extremely rare features cannot be learned since the learning speed of background noise is much faster and there are too many background noise patches to cut them all out.

_Remark 4.1_.: Shen et al. (2022) conduct analysis on vanilla training and training using standard data augmentation, sharing the same intuition in similar but different data models and neural networks. Also, we emphasize that we proved the model cannot learn (extremely) rare features even if we run \((d)}{}\) iterations of GD, whereas Shen et al. (2022) only consider the first iteration that achieves perfect training accuracy.

Practical Insights.In practice, images contain features and noise across several patches. A larger cutting size can be more effective in removing noise but may also remove important features that the model needs to learn. Thus, there is a trade-off in choosing the optimal cutting size, a trend also observed in DeVries and Taylor (2017). One limitation of Cutout is that it may not effectively remove dominant noise. Thus, dominant noise can persist in the augmented data, leading to potential noise memorization. We believe that developing strategies that can more precisely detect and remove these noise components from the image input could enhance the effectiveness of these methods.

### CutMix Training

In learning dynamics of ERM and Cutout, inner products between weight and data patches evolve (approximately) monotonically, which makes the analysis much more feasible. However, analyzing the learning dynamics of CutMix involves non-monotone change of inner products, which is inevitable since CutMix uses mixed labels; this is also demonstrated in our experimental results (Section 5,especially the leftmost plot in Figure 1). Non-monotonicity and non-convexity of the problem necessitates novel proof strategies.

Let us define \(:=\{z_{s,k}\}_{s\{ 1\},k[K]}\{z_{i}^{(p)}\}_{i[n],p[P] \{p_{i}^{*}\}}\) as a function of \(\) as follows,

\[z_{i}^{(p)}:=(_{1},_{i}^{(p)} )-(_{-1},_{i}^{(p)}),  z_{s,k}:=(_{1},_{s,k})-( _{-1},_{s,k}).\]

Then, \(\) represents the contribution of each noise patch and feature vector to the neural network output, and the nonconvex function \(_{}()\) can be viewed as the composition of \(()\) and a convex function \(h()\). By using the convexity of \(h()\), we can characterize the global minimum of \(_{}()\). Surprisingly, we show that any global minimizer \(^{*}=\{_{1}^{*},_{-1}^{*}\}\) satisfies

\[(_{s}^{*},_{i}^{(p)})- (_{-s}^{*},_{i}^{(p)})=C _{s},\]

for all \(s\{ 1\},i_{s}\), and \(p[P]\), with some constants \(C_{1},C_{-1}=(1)\). In other words, at the global minimum, the output of model on each patch of the training data is uniform across the set of data with the same labels. We also prove that CutMix can achieve a point close to the global minimum within \((d)}{}\) iterations. As a result, the model trained by CutMix can learn all features including extremely rare features. The complete proof of Theorem 3.3 appears in Appendix E.2.

_Remark 4.2_.: Zou et al. (2023) investigate Mixup in a similar feature-noise model and show that Mixup can learn rarer features than vanilla training, with its benefits emerging from the early dynamics of training. However, our characterization of the global minimum of \(_{}()\) and experimental results in our setting (Section 5, Figure 1) suggest that the benefits of CutMix, especially for learning extremely rare features, arise from the later stages of training. This suggests that Mixup and CutMix have different underlying mechanisms for promoting feature learning.

Practical Insights.The main underlying mechanism of CutMix is that it learns information almost uniformly from all patches in the training data. However, this approach also involves memorizing noise, which can potentially degrade performance in real-world scenarios. We believe that a more sophisticated strategy such as considering the positional information of patches as used in Puzzle Mix (Kim et al., 2020) or Co-Mixup (Kim et al., 2021) could improve the ability to learn more from patches containing features and reduce the impact of noise.

## 5 Experiments

We conduct experiments both in our setting and real-world data CIFAR-10 to support our theoretical findings and intuition. We defer CIFAR-10 experiment results to Appendix A.1.

For the numerical experiments on our setting, we set the number of patches \(P=3\), dimension \(d=2000\), number of data points \(n=300\), dominant noise strength \(_{}=0.25\), background noise strength \(_{}=0.15\), and feature noise strength \(=0.005\). The feature vectors are given as the standard basis \(_{1},_{2},_{3},_{4},_{5},_{6} ^{d}\), where \(_{1},_{2},_{3}\) are features for the positive label \(y=1\) and \(_{4},_{5},_{6}\) are features for the negative label \(y=-1\). We categorize \(_{1}\) and \(_{4}\) as common features with a frequency of \(0.8\), \(_{2}\) and \(_{5}\) as rare features with a frequency of \(0.15\), and lastly, \(_{3}\) and \(_{6}\) as extremely rare features with a frequency of \(0.05\). For the learner network, we set the slope of negative regime \(=0.1\) and the length of the smoothed interval \(r=1\). We train models using three methods: ERM, Cutout, and CutMix with a learning rate \(=1\). For Cutout, we cut a single patch of data (\(C=1\)). We apply full-batch gradient descent for all methods; for Cutout and CutMix, we utilize all possible augmented data points.3 We note that this choice of problem parameters does not exactly match the technical assumptions in Section 2.4. However, we empirically observe the same conclusions, which suggests that our analysis could be extended beyond our assumptions.

For each feature vector \(\) of the positive label, we plot the output of the learned filters for the feature vector \((_{1}^{()},)-(_{-1}^{()},)\) throughout training in Figure 1. Our numerical findings confirm that ERM can only learn common features, CutOut can learn common and rare features but cannot learn extremely rare features, and CutMix can learn all types of features. Especially, CutMix learn common features, rare features, and extremely rare features almost evenly. Also, we observed non-monotone behavior of the output in the case of CutMix, which motivated our novel proof technique. The same trends are observed with different architectures, such as a smoothed (leaky) ReLU network with multiple neurons, as detailed in Appendix A.2.

## 6 Conclusion

We studied how Cutout and CutMix influence the ability to learn features in a patch-wise feature-noise data model learning with two-layer convolutional neural networks by comparing them with vanilla training. We showed that Cutout enables the learning of rare features that cannot be learned through vanilla training by mitigating the problem of memorizing label-independent noises instead of learning label-dependent features. Surprisingly, we further proved that CutMix can learn extremely rare features that Cutout cannot learn. We also present our theoretical insights on the underlying mechanism of these methods and provide experimental support.

Limitation and Future Work.Our work has some limitations related to the neural network architecture, specifically, the use of a 2-layer two-neuron smoothed leaky ReLU network. Extending our results to neural networks with deeper, wider, and more general activation functions is a direction for future work. Another future direction is to develop patch-level data augmentation based on our theoretical findings. Also, it would be interesting to perform theoretical analysis on state-of-the-art patch-level data augmentation such as Puzzle Mix (Kim et al., 2020) or Co-Mixup (Kim et al., 2021). These methods utilize patch location information, thus it may require the development of a theoretical framework capturing more complex characteristics of image data.