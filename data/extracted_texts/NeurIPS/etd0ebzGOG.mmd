# VPP: Efficient Conditional 3D Generation via

Voxel-Point Progressive Representation

 Zekun Qi \({}^{}\)   Muzhou Yu \({}^{}\)   Runpei Dong \({}^{}\)

Xi'an Jiaotong University

{qzk139318, muzhou99999, runpei.dong}@stu.xjtu.edu.cn

Kaisheng Ma \({}^{@sectionsign}\)

Tsinghua University

kaisheng@mail.tsinghua.edu.cn

Equal contribution. \({}^{}\)Project lead. \({}^{@paragraphsign}\)Corresponding author.

###### Abstract

Conditional 3D generation is undergoing a significant advancement, enabling the free creation of 3D content from inputs such as text or 2D images. However, previous approaches have suffered from low inference efficiency, limited generation categories, and restricted downstream applications. In this work, we revisit the impact of different 3D representations on generation quality and efficiency. We propose a progressive generation method through Voxel-Point Progressive Representation (VPP). VPP leverages structured voxel representation in the proposed Voxel Semantic Generator and the sparsity of unstructured point representation in the Point Upsampler, enabling efficient generation of multi-category objects. VPP can generate high-quality 8K point clouds _within 0.2 seconds_. Additionally, the masked generation Transformer allows for various 3D downstream tasks, such as generation, editing, completion, and pre-training. Extensive experiments demonstrate that VPP efficiently generates high-fidelity and diverse 3D shapes across different categories, while also exhibiting excellent representation transfer performance. Codes will be released at [https://github.com/qizekun/VPP](https://github.com/qizekun/VPP).

Figure 1: Overview of VPP’s applications through generative modeling.

## 1 Introduction

In the last few years, text-conditional image generation [62; 71; 72; 74] has made significant progress in terms of generation quality and diversity and has been successfully applied in games , content creation [47; 48] and human-AI collaborative design . The success of text-to-image generation is largely owed to the large-scale image-text paired datasets . Recently, there has been an increasing trend to boost conditional generation from 2D images to 3D shapes. Many works attempt to use existing text-3D shape paired datasets [7; 9; 24; 50] to train the models, but the insufficient pairs limit these methods to generate multi-category, diverse, and high-res 3D shapes.

To tackle this challenge, some works [37; 65; 82; 94] use NeRF  and weak supervision of large-scale vision-language models  to generate 3D shapes. Although creative quality is achieved, the optimization costs of differentiable rendering are quite expensive and impractical. Some methods use images as bridges [38; 59] for the text-conditional 3D generation, which improves the quality-time trade-off. However, the complex multi-stage model leads to long inference latency and cumulative bias. Moreover, most methods [58; 76] are task-limited due to specific model design that can not be implemented to more downstream tasks, _e.g._, editing, leading to narrow application scope. To sum up, current methods are faced with three challenges: _low inference efficiency_, _limited generation category_, and _restricted downstream tasks_, which have not been solved by a unified method.

One of the most crucial components in 3D generation is the _geometry representation_, and mainstream methods are typically built with voxel, 3D point cloud, or Signed Distance Function (SDF) [11; 61]. However, the aforementioned issues in 3D generation may come from the different properties of these representations, which are summarized as follows:

* **Voxel** is a structured and explicit representation. It shares a similar form with 2D pixels, facilitating the adoption of various image generation methods. However, the dense nature of voxels results in increased computational resources and time requirements when generating high-resolution shapes.
* **3D Point Cloud** is the unordered collection of points that explicitly represents positional information with sparse semantics, enabling a flexible and efficient shape representation. However, 3D point clouds typically dedicatedly designed architectures, _e.g._, PointNet [66; 67].
* **SDF** describes shapes through an implicit distance function. It is a continuous representation method and is capable of generating shapes with more intricate details than points or voxels. But it requires substantial computation for high-resolution sampling and is sensitive to initialization.

Due to significant shape differences across various categories, the structured and explicit positional information provides spatial cues, thereby aiding the generalization of multiple categories. It coincides with previous multi-category generation methods [76; 77]. In contrast, single-category generation may require less variation. Hence an implicit and continuous representation would contribute to the precise expression of local details [9; 25].

Motivated by the analysis above, we aim to leverage the advantage of different forms of representation. Therefore, a novel **V**oxel-**P**oint **P**rogressive (**VPP\({}^{}\)**) Representation method is proposed to achieve efficient and universal 3D generation. We use voxel and point as representations for the coarse to fine stages to adapt to the characteristics of 3D. To further enhance the inference speed, we employ mask generative Transformers for parallel inference  in both stages, which can concurrently obtain broad applicability scope . Based on voxel VQGAN , we utilize the discrete semantic codebooks as the reconstruction targets to obtain authentic and rich semantics. Given the substantial computational resources required by high-resolution voxels, we employ sparse points as representations in the

  
**Method** & **Latency** & **Device** & **Generation Category** & **Downstream Task** \\  DreamFields  & 1.2h & 8\(\)TPUv4 & Multi-Category & Generation \\ DreamFusion  & 1.5h & 4\(\)TPUv4 & Multi-Category & Generation \\ CLIP-Mesh  & 30min & V100 & Multi-Category & Generation \\ CLIP-Sculptor  & 0.9s & V100 & Multi-Category & Generation \\ Point E (4DM)  & 25s & V100 & Multi-Category & Generation \\ ShapeCratfer  & - & - & Single-Category & Generation, Editing \\ LION  & 27s & V100 & Single-Category & Generation, Completion \\ SDFusion  & - & - & Single-Category & Generation, Completion \\  VPP (Ours) & **0.2s** & **RTX 2080Ti** & **Multi-Category** & **Generation, Editing, Completion, Pretraining** \\   

Table 1: Comparison of text-conditioned 3D generation methods on efficiency and applications.

second stage. Inspired by the masked point modeling approaches based on position cues [60; 98], we utilize the voxels generated in the first stage as the positional encoding for generative modeling. We reconstruct the points and predict semantic tokens to obtain sparse representations. Table 1 compares the efficiency and universality of text-conditional 3D generation methods.

In summary, our contributions are: (1) We propose a novel voxel-point progressive generation method that shares the merits of different geometric representations, enabling efficient and multi-category 3D generation. Notably, VPP is capable of generating high-quality 8K point clouds _within 0.2 seconds_ on a single RTX 2080Ti. (2) To accommodate 3D generation, we implement unique module designs, _e.g._ 3D VQGAN, and propose a Central Radiative Temperature Schedule strategy in inference. (3) As a universal method, VPP can complete multiple tasks such as editing, completion, and even pre-training. To the best of our knowledge, VPP is the first work that unifies various 3D tasks.

## 2 Vpp

Our goal is to design a 3D generation method by sharing the merits of both geometric representations, which is capable of addressing the three major issues encountered in previous generation methods. In this work, we propose VPP, a Voxel-Point Progressive Representation method for two-stage 3D generation. To tackle the problem of _limited generation categories_, VPP employs explicit voxels and discrete VQGAN for semantics representation in the first stage. To address the issue of _low inference efficiency_, VPP employs efficient point clouds as representations in the second stage and leverages the parallel generation capability of the mask Transformer. Given the wide range of applications of generative modeling, VPP resolves the problem of _restricted downstream tasks_. The training overview of VPP is shown in Fig. 3.

### How Does 2D VQGAN Adapt to 3D Voxels?

Due to the formal similarity between voxels and pixels, we can readily draw upon various image generation methods. VQGAN  is capable of generating realistic images through discrete semantic codebooks and seamlessly integrating with mask generative Transformers [5; 6]. We utilize 3D volumetric SE-ResNet  as the backbone of VQGAN encoder \(E\), decoder \(G\) and discriminator \(D\).

Given one voxel observation \(^{3}\) whose occupancy is defined as function \(o:^{3}\). The original VQGAN is trained by optimizing a vectorized quantized codebook with the loss \(_{}\) during autoencoding while training a discriminator between real and reconstructed voxels with loss \(_{}\). Denoting the reconstructed voxel result as \(}\), the overall loss of vanilla VQGAN can be written as:

\[-}\|^{2}+\|[E()]-z_{ }\|_{2}^{2}+\|[z_{}]-E()\|_{2}^{2}}_{ _{}(E,G,Z)}+ D()+(1-D (}))}_{_{}(\{E,G,Z\},D)}.\]

However, unlike 2D images, 3D voxels only have non-zero values in the region that encompasses the object in grid centroids. This may lead to a short-cut solution that degrades by outputting voxels with values as close to zero as possible, _i.e._, all empty occupancy. To address this issue, we propose to

Figure 2: Qualitative examples. (a) 3D VQGAN reconstruction results with or without \(_{}\) (Section 2.1). (b) 3D point clouds result with or without proposed Grid Smoother (Section 2.2).

minimize the occupancy disparity between the reconstructed and ground truth voxels. Formally, let \( o()\) be the averaged occupancy in SE(3) space, the overall loss \(_{}\) is:

\[_{}=_{}+_{}+ )- o(})\|_{2}^{ 2}}_{_{}}, \]

where \(_{}\) is proposed to minimize the occupancy rate for better geometry preservation.

### How to Bridge the Representation Gap Between Voxels and Points?

We aim to utilize the voxel representation as the intermediate coarse generation before fine-grained point cloud results. However, voxels are uniform and discrete grid data, while point clouds are unstructured with unlimited resolutions, which allows for higher density in complex structures. Besides, point clouds typically model the surfaces, while voxel representation is solid in nature. This inevitably leads to a _representation gap_ issue, and a smooth transformation that bridges the representation gap is needed. We employ the Lewiner algorithm  to extract the surface of the object based on voxel boundary values and utilize a lightweight sequence-invariant Transformer  network \(_{}\) parametrized with \(\) as _Grid Smoother_ to address the issue of grid discontinuity. Specifically, given generated point clouds \(=\{_{i}^{3}\}_{i=1}^{N}\) with \(N\) points. The model is trained by minimizing the geometry distance between generated and ground truth point clouds \(=\{_{i}^{3}\}_{i=1}^{N}\) that is downsampled to \(N\) with farthest point sampling (FPS). To make the generated surface points more uniformly distributed, we propose \(_{}\) that utilizes the Kullback-Leibler (KL) divergence \(D_{}\) between the averaged distance of every centroid \(_{i}\) and its K-Nearest Neighborhood (KNN) \(_{i}\). Then we optimize the parameter \(\) of Grid Smoother by minimizing loss \(_{}\):

\[_{}=_{}()|}_{_{}()}_{ }\|-\|+|}_{}_{_{}()}\|-\|}_{_{}}+}[_{j _{i}}\|_{i}-_{j}\|_{2},\ ]}_{ _{}}, \]

where \(_{CD}\) is \(_{1}\) Chamfer Distance  geometry disparity loss, \(\) is the uniform distribution.

### Voxel-Point Progressive Generation Trough Mask Generative Transformer

We employ mask generative Transformers  for rich semantic learning in both the voxel and point stages. Mask generative Transformer can be seen as a special form of denoising autoencoder [34; 85]. It forces the model to reconstruct masked input to learn the statistical correlations between local patches. Furthermore, generating reconstruction targets through a powerful tokenizer can further enhance performance [104; 2; 91; 2]. From the perspective of discrete variational autoencoder

Figure 3: Training process overview of VPP. (a) In the coarse stage, the VQ codebook is reconstructed via Mask Voxel Transformer conditioned on prompt embeddings. (b) In the fine stage, sparse semantic tokens are reconstructed via the Mask Point Transformer conditioned on positional embeddings.

(dVAE) , the overall optimization is to maximize the _evidence lower bound_ (ELBO)  of the log-likelihood \((x_{i}|_{i})\). Let \(x\) denote the original data, \(\) the masked data, and \(z\) the semantic tokens, the generative modeling can be described as:

\[_{(x_{i},_{i})}\,(x_{i}| _{i})-14.226378pt_{(x_{i},_{i}) }-14.226378pt(_{z_{i}_{}(|x_{ i})}_{}(x_{i}|z_{i})-D_{}_{ }(|x_{i}),_{}(|_{i}) )-14.226378pt, \]

where (1) \(_{}(z|x)\) denotes the discrete semantic tokenizer; (2) \(_{}(x|z)\) is the tokenizer decoder to recover origin data; (3) \(=_{}(z|)\) denotes the masked semantic tokens from masked data; (4) \(_{}(z|)\) reconstructs masked semantic tokens in an autoencoding way. In the following sections, we extend the mask generative Transformer to voxel and point representations, respectively. The semantic tokens of voxel and point are described as \(z^{v}\) and \(z^{p}\) in Figs. 3 and 4.

Voxel Semantic GeneratorWe generate semantic voxels with low resolution in the first stage to balance efficiency and fidelity. We utilize _Mask Voxel Transformer_ for masked voxel inputs, where the 3D VQGAN proposed in Section 2.1 is used as the tokenizer to generate discrete semantic tokens. Following prior arts , we adopt Cosine Scheduling to simulate the different stages of generation. The masking ratio is sampled by the truncated arccos distribution with density function \(p(r)=(1-r^{2})^{-}\). The prompt embedding comes from the CLIP  features of cross-modal information. We render multi-view images from the 3D object  and utilize BLIP  to get the text descriptions of the rendered images. Furthermore, in order to mitigate the issue of excessive dependence on prompts caused by the high masking ratio (_e.g._, the same results will be generated by one prompt), we introduce _Classifier Free Guidance_ (CFG) to strike a balance between diversity and quality. Following Sanghi _et al._, the prompt embeddings are added with Gaussian noise to reduce the degree of confidence. We use \((0,1)\) and \((0,1)\) as the level of noise perturbation for the trade-off control, which will later be studied.

Point UpsamplerPoint clouds are unordered and cannot be divided into regular grids like pixels or voxels. Typically, previous masked generative methods  utilize FPS to determine the center of local patches, followed by KNN to obtain neighboring points as the geometric structure. Finally, a lightweight PointNet  or DGCNN  is utilized for patch embedding. However, these methods rely on pre-generated Positional Encodings (PE), rendering the use of fixed positional cues _infeasible_ in conditional 3D generation since only conditions like texts are given. To tackle this issue, we adopt the first-stage generated voxels-smoothed point clouds as the PEs. A pretrained Point-MAE  encoder is used as the tokenizer (_i.e._, teacher ). The Mask Point Transformer learns to reconstruct tokenizer features, which will be used for the pretrained decoder reconstruction.

### Inference Stage

Parallel DecodingAn essential factor contributing to the efficient generation of VPP is the parallel generation during the inference stage. The model takes fully masked voxel tokens as input and generates semantic voxels conditioned on the CLIP  features of text or image prompts. Following , a cosine schedule is employed to select the fixed fraction of the highest confidence masked tokens for

Figure 4: Inference process overview of VPP. Point clouds are generated conditioned on the input text or images. Parallel decoding is adopted for efficient Mask Transformer token prediction.

prediction at each step. Furthermore, we pass the generated voxels through Grid Smoother to obtain smoothed point clouds, which will serve as the positional encoding for the Point Upsampler in the upsampling process. Note that all the stages are performed in parallel. The inference overview of VPP is illustrated in Fig. 4.

**Central Radiative Temperature Schedule** Due to the fact that 3D shape voxels only have non-zero values in the region that encompasses the object in the center, we hope to encourage diversity in central voxels while suppressing diversity in edge vacant voxels. Consequently, we propose the _Central Radiative Temperature Schedule_ to accommodate the characteristics of 3D generation. As for the voxel with a size of \(R R R\), we calculate the distance \(r\) to the voxel center for each predicted grid. Then, we set the temperature for each grid as \(T=1-(r/R)^{2}\), where \(T\) represents the temperature. Thus, we can achieve the grid that is closer to the center will have a higher temperature to encourage more diversity and vice versa.

## 3 Experiments

### Conditional 3D Generation

**Text-conditioned Generation** Following [59; 76; 77], we use Accuracy (ACC), Frechet Inception Distance (FID) , and Inception Score (IS)  as metrics to assess the generation quality and diversity. We follow  to define 234 predetermined text queries and apply a classifier for evaluation. For a fair comparison, all methods use "a/an" as the prefix for text prompts. Similar to , we employ a double-width PointNet++ model  to extract features and predict class probabilities for point clouds in ShapetNet . Table 2 shows the comparison between VPP and other methods. It can be observed that our VPP outperforms all other methods substantially in all metrics. Notably, the FID metric of ours is much lower than other methods, demonstrating that our method generates higher quality and richer diversity of 3D shapes.

By employing a _Shape As Points_[52; 63] model pre-trained on multi-category ShapeNet data, VPP is capable of performing smooth surface reconstruction on generated point clouds. In Blender rendering, we employ varnish and automatic smoothing techniques to improve the display quality of the generated mesh. We qualitatively show in Fig. 5 that our method can generate higher-quality shapes and remain faithful to text-shape correspondence. It is noteworthy that VPP can generate smoother 3D shapes, _e.g._, the truck. Besides, our generation results are visually pleasing across different object categories with complex text descriptions. Therefore, the above advantages enable our method to be much more useful in practical applications.

Figure 5: Qualitative comparison of VPP, CLIP-Sculptor  and CLIP-Forge  on text conditioned 3D generation. VPP generates shapes of higher fidelity with smoother surfaces while being consistent with the input text. More diverse results are shown in Appendix C.

Image-Conditioned Generation

By leveraging the CLIP features of 2D images as conditions in the Voxel Semantic Generator, VPP also possesses the capability of single-view reconstruction. Fig. 6 illustrates image-conditional generated samples of VPP. It can be observed that the generated point clouds are of detailed quality and visually consistent with the input image. For example, the flat wings of an airplane and the curved backrest of a chair.

### Text-Conditioned 3D Editing and Upsampling

Text-conditioned 3D editing and upsample completion tasks are more challenging than most conditional 3D generation tasks, which most of previous methods can not realize due to the inflexible model nature. In contrast, our VPP can be used for a variety of shape editing and upsample completion without extra training or model fine-tuning.

Text-Conditioned 3D EditingSince we use the masked generative Transformer in VPP, our model can be conditioned on any set of point tokens: we first obtain a set of input point tokens, then mask the tokens corresponding to a local area, and decode the masked tokens with unmasked tokens and text prompts as conditions. We show the examples in Fig. 7 (a). The figure provides examples of VPP being capable of changing part attributes (view) and modifying local details (view) of the shape, which correctly corresponds to the text prompts.

Point Cloud UpsamplingExcept for editing, we also present our conditional generative model for the point cloud upsample completion, where the sparse point cloud inputs are completed as a dense output. We use the second stage Point Upsampler to achieve this, and the results are illustrated in Fig. 7 (b). It is observed that VPP generates completed and rich shapes of high fidelity while being consistent with the input sparse geometry.

### Transfer Learning on Downstream Tasks

After training, the Mask Transformer has learned powerful geometric knowledge through generative reconstruction , which enables VPP to serve as a self-supervised learning method for downstream representation transferring. Following previous works [19; 60; 68], we fine-tune the Mask Point Transformer encoder, _i.e._, the second-stage _Point Upsampler_ for 3D shape recognition.

  
**Method** & **ACC\(\)** & **FID\(\)** & **IS\(\)** \\  CLIP-Forge  & 83.33\% & 2425.25 & - \\ CLIP-Sculptor  & 87.08\% & 1480.11 & - \\ Point-E (40M text-only)  & 62.56\% & 85.37 & 7.93 \\  VPP (ours) & **88.04\%** & **29.82** & **10.64** \\   

Table 2: Text conditioned 3D generation results on ShapeNetCore13  dataset. Point-E  is pre-trained on a large-scale private point-text paired dataset containing millions of samples, and we report its zero-shot transfer results on ShapeNet. Accuracy (ACC), Fréchet Inception Distance (FID), and Inception Score (IS) are reported.

Figure 6: Qualitative examples of single image conditioned 3D generation with VPP.

ScanObjectNN  and ModelNet  are currently the two most challenging 3D object datasets, which are obtained through real-world sampling and synthesis, respectively. We show the evaluation of 3D shape classification in Table 3. It can be seen that: (i) Compared to any supervised or self-supervised method that only uses point clouds for training, VPP achieves the best generalization performance. (ii) Notably, VPP outperforms Point-MAE by +4.0% and +0.3% accuracy on the most challenging PB_T50_RS and ModelNet40 benchmark.

### Diversity & Specific Text-Conditional Generation

We show the diverse qualitative results of VPP on text-conditional 3D generation in Fig. 8 (a). It can be observed that VPP can generate a broad category of shapes with rich diversity while remaining faithful to the provided text descriptions. Meanwhile, we present the qualitative results of VPP on more specific text-conditional 3D generation in Fig. 8 (b). Notably, VPP can generate high-fidelity shapes that react well to very specific text descriptions, like a cup chair, a round table with four legs and an aircraft during World War II, _etc_.

    &  &  &  &  \\   & & & OBJ\_BG & OBJ\_ONLY & PB\_T50\_RS & 1k P & 8k P \\   \\  PointNet  & 3.5 & 0.5 & 73.3 & 79.2 & 68.0 & 89.2 & 90.8 \\ PointNet++  & 1.5 & 1.7 & 82.3 & 84.3 & 77.9 & 90.7 & 91.9 \\ DGCNN  & 1.8 & 2.4 & 82.8 & 86.2 & 78.1 & 92.9 & - \\ PointCNN  & 0.6 & - & 86.1 & 85.5 & 78.5 & 92.2 & - \\ PCT  & 2.88 & 2.3 & - & - & - & 93.2 & - \\ PointMLP  & 12.6 & 31.4 & - & - & 85.4\(\)0.3 & 94.5 & - \\ PointNeXt  & 1.4 & 3.6 & - & - & 87.7\(\)0.4 & 94.0 & - \\   \\  Transformer  & 22.1 & 4.8 & 83.04 & 84.06 & 79.11 & 91.4 & 91.8 \\ Point-BERT  & 22.1 & 4.8 & 87.43 & 88.12 & 83.07 & 93.2 & 93.8 \\ Point-MAE  & 22.1 & 4.8 & 90.02 & 88.29 & 85.18 & 93.8 & 94.0 \\ Point-M2AE  & 14.8 & 3.6 & 91.22 & 88.81 & 86.43 & 94.0 & - \\ VPP **w/o vot.** & 22.1 & 4.8 & 92.77 & 91.56 & 88.65 & 93.8 & 94.0 \\ VPP **w/ vot.** & 22.1 & 4.8 & **93.11** & **91.91** & **89.28** & **94.1** & **94.3** \\   \\  ACT  & 22.1 & 4.8 & 93.29 & 91.91 & 88.21 & 93.7 & 94.0 \\ I2P-MAE  & 12.9 & 3.6 & 94.15 & 91.57 & 90.11 & 94.1 & - \\ ReCon  & 43.6 & 5.3 & 95.18 & 93.29 & 90.63 & 94.5 & 94.7 \\   

Table 3: Downstream 3D object classification results on the ScanObjectNN and ModelNet40 datasets. The inference model parameters #P (M), FLOPS #F (G), and overall accuracy (%) are reported.

Figure 7: Qualitative examples of text conditioned 3D editing and upsample completion with VPP.

### Partial Completion

We present the point cloud partial completion experiments in Fig. 9. By employing a block mask on the original point clouds, VPP can generate partial samples, which illustrates the partial completion capability of VPP. Besides, the generated samples exhibit diversity, further demonstrating the diverse performance of VPP.

Figure 8: (a) **Diversity** qualitative results of VPP on text conditioned 3D generation. (b) Qualitative results of VPP on more **specific** text-conditioned 3D generation. VPP can generate a broad category of shapes with rich diversity and high fidelity while remaining faithful to the provided text descriptions. Besides, VPP can react to very specific text descriptions, like a cup chair.

Figure 9: Partial inputs point clouds completion results of VPP. Our model is capable of generating diverse completion samples.

### ShapeNet Retrieval Experiment

We conduct the retrieval evaluation on samples generated by VPP from the ShapeNet dataset. The results are shown in Fig. 10. It can be observed that there are no completely identical samples, proving the great generation ability of VPP is not caused by overfitting the ShapeNet dataset. The VPP results more likely come from the understanding and integration of the learned shape knowledge.

## 4 Related Works

Conditional 3D generation with 2D images or text has witnessed rapid development in recent years. One line of works focuses on solving the ill-posed image-conditioned 3D generation (_i.e._, single-view reconstruction), where impressive results have been achieved [10; 23; 57; 88]. By introducing a text-shape paired dataset based on ShapeNet  that includes chair and table categories,  pioneered another line of single-category text-conditioned shape generation. Facilitated with this human-crawled 3D data, ShapeCrafter  further enables text-conditioned shape edition in a recursive fashion. To leverage easily rendered 2D images rather than human-crafted languages, several works propose to train rendered image-to-shape generation [9; 51; 76; 77] which enables text-conditioned generation through pretrained large vision-language (VL) models, _i.e._, CLIP . Besides rendered images, Point-E  and Shape-E  propose to use real-world text-image-shape triplets from a large-scale in-house dataset, where images are used as the representation bridge.

However, 3D data is significantly lacking and expensive to collect . DreamFields  first achieves text-only training by using NeRF , where rendered 2D images are weakly supervised to align with text inputs using CLIP. Following this direction, DreamFusion  incorporates the distillation loss of a diffusion model . Dream3D  improves the knowledge prior by providing the prompt shapes initialization. Though impressive results are obtained, these approaches also introduce significant time costs because of the case-by-case NeRF training during inference. More recently, TAPS3D aligns rendered images and text prompts based on DMTet  to enable text-conditional generation, but its generalizability is limited to only four categories. 3DGen  achieves high-quality generation through the utilization of Triplane-based diffusion models [3; 80] and the Objaverse dataset . Zero-1-to-3  employs relative camera viewpoints as conditions for diffusion modeling. Based on this, One-2-3-45  achieves rapid open-vocabulary mesh generation through multi-view synthesis. Fantasia3D  leverages DMTet to employ differentiable rendering of SDF as a substitute for NeRF, enabling a more direct mesh generation. SJC  and ProlificDreamer  achieve astonishing results by proposing the variational diffusion distillation.

## 5 Conclusions

In this paper, we present VPP, a novel Voxel-Point Progressive Representation method that achieves efficient and universal conditional 3D generation. We explore the suitability of different 3D geometric representations and design a VQGAN module tailored for 3D generation. Notably, VPP is capable of generating high-quality 8K point clouds in less than 0.2 seconds using a single RTX 2080Ti GPU. Extensive experiments demonstrate that VPP exhibits excellent performance in conditional generation, editing, completion, and pretraining.

Figure 10: ShapeNet training data retrieval ablation experiment.