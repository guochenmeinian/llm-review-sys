# Lighting Every Darkness with 3DGS: Fast Training and Real-Time Rendering for HDR View Synthesis

Xin Jin\({}^{1,2}\)1  Pengyi Jiao\({}^{1}\)1  Zheng-Peng Duan\({}^{1}\)  Xingchao Yang\({}^{2}\)

**Chongyi Li\({}^{1}\) Chun-Le Guo\({}^{1}\)2  Bo Ren\({}^{1}\)3 \({}^{1}\)**

\({}^{1}\) VCIP, CS, Nankai University \({}^{2}\) MEGVII Technology

https://srameo.github.io/projects/le3d

###### Abstract

Volumetric rendering-based methods, like NeRF, excel in HDR view synthesis from RAW images, especially for nighttime scenes. They suffer from long training times and cannot perform real-time rendering due to dense sampling requirements. The advent of 3D Gaussian Splatting (3DGS) enables real-time rendering and faster training. However, implementing RAW image-based view synthesis directly using 3DGS is challenging due to its inherent drawbacks: 1) in nighttime scenes, extremely low SNR leads to poor structure-from-motion (SfM) estimation in distant views; 2) the limited representation capacity of the spherical harmonics (SH) function is unsuitable for RAW linear color space; and 3) inaccurate scene structure hampers downstream tasks such as refocusing. To address these issues, we propose **LE3D** (**L**ighting **E**very darkness with **3DGS). Our method proposes Cone Scatter Initialization to enrich the estimation of SfM and replaces SH with a Color MLP to represent the RAW linear color space. Additionally, we introduce depth distortion and near-far regularizations to improve the accuracy of scene structure for downstream tasks. These designs enable LE3D to perform real-time novel view synthesis, HDR rendering, refocusing, and tone-mapping changes. Compared to previous volumetric rendering-based methods, LE3D reduces training time to 1% and improves rendering speed by up to 4,000 times for 2K resolution images in terms of FPS. Code and viewer can be found in https://srameo.github.io/projects/le3d.

Figure 1: LE3D reconstructs a 3DGS representation of a scene from a set of multi-view noisy RAW images. As shown on the left, LE3D features _fast training and real-time rendering_ capabilities compared to RawNeRF . Moreover, compared to RawGS (a 3DGS  we trained with RawNeRF’s strategy), LE3D demonstrates superior noise resistance and the ability to represent HDR linear colors. The right side highlights the variety of _real-time downstream tasks_ LE3D can perform, including (a) exposure variation, (b, d) changing White Balance (WB), (b) HDR rendering, and (c, d) refocus.

Introduction

Since the advent of Neural Radiance Fields (NeRF) , novel view synthesis (NVS) has entered a period of vigorous development, thereby advancing the progress of related applications in augmented and virtual reality (AR/VR). Existing NVS technologies predominantly utilize multiple well-exposed and noise-free low dynamic range (LDR) RGB images as inputs to reconstruct 3D scenes. This significantly impacts the capability to capture high-quality images in environments with low light or high contrast, such as nighttime settings or areas with stark lighting differences. These scenarios typically necessitate the use of high dynamic range (HDR) scene reconstruction techniques.

Existing HDR scene reconstruction techniques primarily fall into two categories and all are based on volumetric rendering: 1) using multiple-exposure LDR RGB images for supervised training , and 2) conducting training directly on noisy RAW data . The first type of method is typically highly effective in well-lit scenes. However, in nighttime scenarios, its reconstruction performance is constrained due to the impact of the limited dynamic range in sRGB data . While RAW data preserves more details in nighttime scenes, it is also more susceptible to noise. Therefore, RawNeRF  is proposed to address the issue of vanilla NeRF's lack of noise resistance. However, RawNeRF suffers from prolonged training times and an inability to render in real-time (a common drawback of volumetric rendering-based methods). This significantly limits the application of current scene reconstruction techniques and HDR view synthesis. Enabling real-time rendering for HDR view synthesis is a key step towards bringing computational photography to 3D world.

Recently, 3D Gaussian Splatting (3DGS) has garnered significant attention based on its powerful capabilities in real-time rendering and photorealistic reconstruction. 3DGS utilizes Structure from Motion (SfM)  for initialization and employs a set of 3D gaussian primitives to represent the scene. Each gaussian represents direction-aware colors using spherical harmonics (SH) functions and can be updated in terms of color, position, scale, rotation, and opacities through gradient descent optimization. Although 3DGS demonstrates its strong capabilities in reconstruction and real-time rendering, it is not suitable for direct training using nighttime RAW data. This is primarily due to 1) the SfM estimations based on nighttime images are often inaccurate, leading to blurred distant views or the potential emergence of floaters; 2) the SH does not adequately represent the HDR color information of RAW images due to its limited representation capacity; and 3) the finally reconstructed structure, such as depth map, is suboptimal, leading to unsatisfactory performance in downstream tasks like refocus.

To make 3DGS suitable for HDR scene reconstruction, we introduce LE3D that stands for **L**ighting **E**very darkness with **3D**-GS, addressing the three aforementioned issues. First, to address the issue of inaccurate SfM distant view estimation in low-light scenes, we proposed Cone Scatter Initialization to enrich the COLMAP-estimated SfM. It performs random point sampling within a cone using the estimated camera poses. Second, instead of the SH, we use a tiny MLP to represent the color in RAW linear color space. To better initialize the color of each gaussian, different color biases are used for various gaussian primitives. Thirdly, we propose to use depth distortion and near-far regularization to achieve better scene structure for downstream tasks like refocusing. As shown in Fig. 1 (left), our LE3D can perform real-time rendering with only about 1.5 GPU hours (99\(\%\) less than RawNeRF ) of training time and at a rate of 100 FPS (about 4000\(\) faster than RawNeRF ) with comparable quality. Additionally, it is capable of supporting downstream tasks such as HDR rendering, refocusing, and exposure variation, as shown in Fig. 1 (right).

In summary, we make the following contributions:

* We propose LE3D, which can reconstruct HDR 3D scenes from noisy raw images and perform real-time rendering. Compared with the NeRF-based methods, LE3D requires only 1% of the training time and has 4000\(\) render speed.
* To address the deficiencies in color representation by vanilla 3DGS and the inadequacies of SfM estimation in nighttime scenes, we leverage a Color MLP with primitive-aware bias, and introduce Cone Scatter Initialization to enrich the point cloud initialized by COLMAP.
* To improve the scene structure in the final results for achieving better downstream task performance, we introduce depth distortion and near-far regularizations.

Related work

Novel view synthesisSince the emergence of NeRF , NVS has gained significant advancement. NeRF employs an MLP to represent both the geometry of the scene and the view-dependent color. It utilizes the differentiable volume rendering , thereby enabling gradient-descent training through a multi-view set of 2D images. Subsequent variants of NeRF [2; 3; 22] have extended NeRF's capabilities with anti-aliasing features. To overcome the deficiencies of vanilla NeRF in geometry reconstruction, strategies such as depth supervision [10; 12] and distortion loss  have been introduced into NeRF. Several methods [9; 46] work on low-light image enhancement with NeRF, and extended its application scenarios. Some methods [5; 7; 16; 39] have conducted experiments with feature-grid based approaches to enhance the training and rendering speeds of NeRF. Although these methods have achieved relatively promising results in novel view synthesis, the training and rendering speeds are still significant bottlenecks. This issue is primarily due to the dense sampling inherently required by volume rendering.

Recently, the advent of 3D Gaussian Splitting  has marked a significant advancement in real-time NVS methods. 3DGS represents a scene using a collection of 3D gaussian primitives, each endowed with distinct attributes. Some subsequent works have added anti-aliasing capabilities to 3DGS representations [32; 43; 52]; others have enhanced 3DGS representation capabilities through supervision in the frequency domain . DNGaussian  proposed a depth-regularized framework to optimize sparse-view 3DGS, and other works also relying on depth supervision [8; 27]. Additionally, some works [30; 34; 49] have focused on applying 3DGS to dynamic scene representation. However, these methods only accept LDR sRGB data for training, and thus cannot reconstruct the scene's HDR radiance. This limitation suggests they cannot perform downstream tasks such as HDR tone mapping and exposure variation. In contrast, LE3D is specifically designed to reconstruct the HDR representation of scenes from noisy RAW images.

HDR view synthesis and its applicationsHDR typically refers to a concept in computational photography that focuses on preserving as much dynamic range as possible to facilitate more post-processing options [11; 14; 20; 24; 33]. The existing HDR view synthesis techniques bear a striking resemblance to the two main approaches in 2D image HDR synthesis: 1) Direct use of multiple-exposure LDR images to compute the camera response function (CRF) and synthesize an HDR image . This corresponds to HDR-NeRF  which employs an MLP to learn the CRF. 2) Acquisition of noise-free underexposed RAW images, utilizing the characteristics of the linear color space in RAW to manually simulate multiple-exposure images, and synthesize an HDR image. This corresponds to RawNeRF , which learns a NeRF representation of RAW linear color space with noisy RAW images to perform both denoising and NVS. VR-NeRF performs a perceptual color space that enables perceptual optimization of high dynamic range images using. Although both methods achieve impressive visual results, the dense sampling required by volume rendering still poses a bottleneck for both training time and rendering efficiency.

LE3D follows the same technical approach as RawNeRF, reconstructing scene representations from noisy RAW images. This means that LE3D does not necessarily require training data with multiple exposures, significantly broadening its range of applications. However, a key distinction of LE3D is its use of differentiable rasterization techniques [25; 26; 44], which enable _fast training and real-time rendering_. Based on a 3DGS-like representation of the reconstructed scene, LE3D can perform real-time HDR view synthesis. This is a novel attempt to introduce computational photography into the 3D world, as it enables _real-time reframing and post-processing_ (changing white balance, HDR rendering, etc. as shown in Fig. 1).

## 3 Preliminaries

3D Gaussian Splitting 3D Gaussian Splatting renders detailed scenes by computing the color and depth of pixels through the blending of many 3D gaussian primitives. Each gaussian is defined by its center in 3D space \(_{i}^{3}\), a scaling factor \(s_{i}^{3}\), a rotation quaternion \(q_{i}^{4}\), and additional attributes such as opacity \(o_{i}\) and color features \(f_{i}\). The basis function of a gaussian primitive is given by Eqn. (1) that incorporates the covariance matrix derived from the scaling and rotation parameters.

\[G(x)=(-(x-)^{T}^{-1}(x-)).\] (1)During rendering, the color of a pixel is determined by blending the contributions of multiple gaussians that overlap the pixel's location. This process involves decoding the color features \(f_{i}\) to color \(c_{i}\) by the SH, and calculating \(_{i}\) of each primitive by multiplying its opacity \(o_{i}\) with its projected 2D gaussian \(G_{i}^{2D}\) on the image plane. Unlike traditional ray sampling strategies, 3D Gaussian Splatting employs an optimized rasterizer to gather the relevant gaussians for rendering. Specifically, the color \(C\) is computed by blending \(N\) ordered gaussians overlapping the pixel:

\[C=_{i N}c_{i}_{i}_{j=1}^{i-1}(1-_{j}),_{i}=G_{i}^{2D}o_{i}.\] (2)

HDR view synthesis with noisy RAW imagesRawNeRF , as a powerful extension of NeRF, specifically addresses the challenge of high dynamic range (HDR) view synthesis with noisy images. Different from LDR images, the dynamic range in HDR images can span several orders of magnitude between bright and dark regions, resulting in the NeRF's standard L2 loss being inadequate. To address this challenge, RawNeRF introduces a weighted L2 loss that enhances supervision in the dark regions. RawNeRF applies gradient supervision on tone curve \(=(y+)\) with \(=10^{-3}\), and uses \(^{}=(y+)^{-1}\) as the weighting term on the L2 loss between rendered color \(_{i}\) and noisy reference color \(y_{i}\). Applying the stop-gradient function \(sg()\) on \(y\), the final loss can be expressed as:

\[L_{}(,y)=_{i}(_{i}-y_{i}}{sg(_{i})+ })^{2}.\] (3)

Moreover, RawNeRF employs a variable exposure training method to take advantage of images with varying shutter speeds. The method scales the output color in linear RGB space by a learned factor \(_{t_{i}}^{c}\) for each color channel \(c\) with each unique shutter speed \(t_{i}\). In particular, the \(c\)-th channel of the output color \(_{i}^{c}\) will be mapped into \((_{i}^{c} t_{i}_{t_{i}}^{c},1)\) as the final output for rendering.

## 4 Proposed method

The pipeline of our LE3D is shown in Fig. 2. Our main motivations and solutions are as follows: 1) To address the issue of COLMAP's inadequacy in capturing distant scenes during nighttime, we utilize the proposed Cone Scatter Initialization to enrich the point cloud obtained from COLMAP. 2) Experiments show that the original SH in 3DGS is inadequate for representing the RAW linear color space (as shown in Fig. 4 (e) and Fig. 7). Therefore, we replace it with a tiny color MLP. 3) To enhance the scene structure and optimize the performance of downstream tasks, we propose the depth distortion \(_{dist}\) and near-far \(_{nf}\) regularizations.

Figure 2: Pipeline of our proposed LE3D. 1) Using COLMAP to obtain the initial point cloud and camera poses. 2) Employing Cone Scatter Initialization to enrich the point clouds of distant scenes. 3) The standard 3DGS training, where we replace the original SH with our tiny Color MLP to represent the RAW linear color space. 4) We use RawNeRF’s weighted L2 loss \(\) (Eqn. (3)) as image-level supervision, and our proposed \(_{dist}\) (Eqn. (8)) as well as \(_{nf}\) (Eqn. (9)) as scene structure regularizations. In this context, \(f_{i}\), \(b_{i}\), and \(c_{i}\) respectively represent the color feature, bias, and final rendered color of each gaussian \(i\). Similarly, \(o_{i}\), \(r_{i}\), \(s_{i}\), and \(p_{i}\) denote the opacity, rotation, scale, and position of them.

### Improvements to the vanilla 3DGS representation

Directly applying 3DGS on noisy RAW images set faces the aforementioned two challenges, lack of distant points and inadequate representation of RAW linear color space. To address them, we propose the following improvements to the vanilla 3DGS representation.

**Cone Scatter Initialization** To enrich the COLMAP-initialized point cloud \(=\{_{i}\}\) with distant scenes, we estimate the position and orientation of all cameras. Based on this, we randomly scatter points within a predefined viewing frustum \(\). To define \(\), we need to determine: 1) The viewpoint \(\); 2) The viewing direction \(}\); 3) The field of view \(\); and 4) The near and far planes, \(z=z_{n}\) and \(z=z_{f}\), respectively. For forward-facing scenes, the viewing direction can be easily determined by averaging the orientations of all cameras, represented as \(}=\{}_{i}^{}\}\). To encompass all visible areas in space from the training viewpoints, we use the maximum value of FOV from all cameras, denoted as \(=\{_{i}^{}\}\). Additionally, \(\) needs to include all the camera origins \(\{_{i}^{}\}\) to ensure complete coverage of the scene from all perspectives. It means that \(\) should encompass a circle with its center at \(}^{}=\{_{i}^{}\}\), radius \(r=\{\|_{i}^{}-}^{}\|_{ 2}\}\), and perpendicular to \(}\). Therefore, we can establish \(\):

\[&=}^{ }-}}{\|}\|_{2}},\ \ }=\{}_{i}^{}\},\ \ =\{_{i}^{}\},\\ z_{n}&=\{\|_{i}-\|_{2}\}, \ \ z_{f}=_{}\{\|_{i}-\|_{2}\}. \] (4)

For near \(z_{n}\) and far \(z_{f}\), we use the distance from the nearest and \(_{}\) times the distance from farthest points in the COLMAP-initialized point cloud \(\) to \(\) to represent them, respectively. Subsequently, we randomly scatter points within our viewing frustum \(=\{,},,z_{n},z_{f}\}\) to obtain our enriched point cloud \(^{}=^{}\), where \(^{}\) is the scattered point set. Then \(^{}\) is used for initialization of the gaussians, instead of \(\).

**Color MLP with primitive-aware bias** To address the issue that SH could not adequately represent the RAW linear color space, we replace it with a tiny color MLP \(_{}\). Each gaussian is initialized with a random color feature \(f_{i}\) and a color bias \(b_{i}\). To initialize \(b_{i}\), we project each \(_{i}^{}^{}\) onto every training image, obtaining a set of all pixels \(\{c_{}\}_{i}\) for each point \(i\),. The color features \(f_{i}\) is concatenated with the camera pose \(v\), and then it is fed into the tiny color MLP \(_{}\) to obtain the view-dependent color. Since the HDR color space theoretically has no upper bound on color values, we use the exponent function as the activation function for \(_{}\) to simulate it. The final color \(c_{i}\) is:

\[c_{i}=(_{}(f_{i},v)+b_{i}),b_{i}^{(0)}= ((\{c_{}\}_{i})),f_{i}^{(0)} (0,_{f}).\] (5)

where \(f_{i}^{(0)}\) is sampled from a gaussian distribution \((0,_{f})\) and \(b_{i}^{(0)}\) is setted by the log value of the average of \(\{c_{}\}_{i}\). This initialization makes \(c_{i}^{(0)}\) close to \(\{c_{}\}_{i}\). Both \(f_{i}\) and \(b_{i}\) are learnable parameters and during cloning and splitting, they are copied and assigned to new gaussians.

### Depth distortion & near-far regularizations

Scene structure is crucially important for the downstream applications of our framework, particularly the tasks such as refocusing. Therefore, we propose depth distortion and near-far regularizations to enhance the ability of 3DGS to optimize scene structure. Borrowing from NeRF-based methods , we use a depth map and weight map to regularize the scene structure.

**Depth and weight map rendering** Recently, several 3DGS-based works [29; 8] employ some form of supervision on depth. Also, depth maps are crucial for downstream tasks such as refocus (Sec. 6), mesh extraction  and relighting [17; 54]. They are achieved by obtaining the rendered average depth map \(d\) in the following manner:

\[d=z_{i}^{}_{i}}{_{i}_{i}},\ \ [x_{i}^{ },y_{i}^{},z_{i}^{}]^{T}=W[x_{i},y_{i},z_{i}]^{T}+t,\ \ _{i}=_{i}_{j=1}^{i-1}(1-_{j}).\] (6)

where \(d\) denotes the depth map, \(_{i}\) represents the blending weight of the \(i\)-th gaussian, \([x_{i},y_{i},z_{i}]^{T}\) and \([x_{i}^{},y_{i}^{},z_{i}^{}]^{T}\) represent the position in the world and the camera coordinate system, respectively, and \([W,t]\) corresponds to the camera extrinsics. The pixel values in the Weight Map each describe a histogram \(\) of the distribution on the ray passing through this pixel. Similar to Mip-NeRF 360 ,we can optimize the scene structure by constraining the gaussian primitives on each ray to be more concentrated. To obtain the Weight Map, we first need to determine the distances to the nearest and farthest gaussian primitives from the current camera pose \(p^{c}\), represented as \(z_{n}^{c},z_{f}^{c}\), respectively. Subsequently, we transform the interval \([z_{n}^{c},z_{f}^{c})\) to obtain \(K\) intersections, where the \(k\)-th intersection is denoted as \([t_{k},t_{k+1})\). Thus, the \(k\)-th value in the histogram \((k)\) can be obtained through rendering in the following manner:

\[(k)=_{i}(z_{i}^{c},k)_{i},(z_{i}^{c},k)=1&z_{i}^{c}[t_{k},t_{k+1})\\ 0&.\] (7)

Rendering \(\) is essential, as it is effective not only in regularization but also plays a role in the refocusing application.

Proposed regularizationsInspired by Mip-NeRF 360 , we proposed similar depth distortion regularization \(_{dist}\) to concentrate the gaussians on each ray:

\[_{dist}=_{u,v}^{K}(u)(v)|+t_{u+1}}{2}-+t_{v+1}}{2}|.\] (8)

\(_{dist}\) constrains the weights along the entire ray to either approach zero or be concentrated at the same intersection. However, in unbounded scenes of the real world, the distances \((z_{f}^{c}-z_{n}^{c})/K\) between each intersection are vast. Forcibly increasing the size of \(K\) to reduce the length of each intersection also significantly increases the computational burden. This means that our \(_{dist}\) can only provide a relatively coarse supervision for the gaussians on each ray, primarily by constraining them as much as possible within the same intersection.

To further constrain the concentration of gaussians, we propose the Near-Far Regularization \(_{nf}\). \(_{nf}\) enhances the optimization of scene structure by narrowing the distance between the weighted depth of the nearest and farthest \(M\) gaussians on each ray, where the farthest refers to the last \(M\) gaussians when the blending weight approaches \(1\). First, we extract two subsets of gaussians, \(\) and \(\), which respectively contain the nearest and farthest \(M\) gaussians on each ray. Subsequently, we render the depth maps for both subsets (\(d^{}\), \(d^{}\)), as well as the final blending weight map (\(T^{}\), \(T^{}\)). The blending weight map \(T\) is the sum of each \(_{i}\). And here comes \(_{nf}\):

\[_{nf}=T^{} T^{}|d^{}-d^{ }|.\] (9)

It not only can prune the gaussians at the front or back of each ray through opacity supervision when there is a significant disparity between them (relying on the \(T^{} T^{}\) term). Compared to \(_{dist}\), \(_{nf}\) can also supervise the position of the first and last \(M\) gaussians on each ray to be as close as possible (relying on the \(|d^{}-d^{}|\) term). Besides the weighted L2 loss \(\) and proposed regularizations \(_{dist}\) and \(_{nf}\), we also introduce constraints on the final blending weights \(T\). Given that the LE3D is tested in real-world scenarios, \(T\) should ideally approach 1, meaning that all pixels should be rendered. Thus, we propose \(_{T}=-(T+)\) to penalize the pixels where \(T\) is less than 1.

## 5 Experiments

### Implementation details

Loss functions and regularizationsIn our implementation, the final loss function is:

\[L=+_{T}_{T}+_{dist}_{dist}+ _{nf}_{nf},\] (10)

where \(\) is the weighted L2 loss, and \(_{T}\), \(_{dist}\), and \(_{nf}\) are the proposed T, depth distortion, and near-far regularizations, respectively.

OptimizationWe set \(_{}\) to \(10\) to enrich the COLMAP-initialized point cloud in distant views. \(_{T},_{dist},_{nf}\) in the loss function are set to \(0.01,0.1,0.01\) respectively. For our color MLP \(_{}\), we use the Adam optimizer with an initial learning rate of \(1.0e-4\). The initial learning rates for color features and biases for each gaussians are set to \(2.0e-3\) and \(1.0e-4\), respectively. The learning rates for all three decrease according to a cosine decay strategy to a final learning rate of \(1.0e-5\). Besides the color MLP, primitive-aware color bias, and the color features for each gaussians, other settings are the same as those of 3DGS . For scenes captured with multiple exposures, we employ the same multiple-exposure training strategy as RawNeRF .

### Datasets and comparisons

DatasetsWe evaluated LE3D's performance on the benchmark dataset of RawNeRF. It includes fourteen scenes for qualitative testing and three test scenes for quantitative testing. The three test scenes contains 101 noisy images and a clean reference image merged from stabilized long exposures. However, the training data are captured with short exposures, leading to exposure inconsistencies. Therefore, we apply the same affine alignment operation as RawNeRF before testing (detailed in Sec. A.1 of the supplementary materials). All images are \(4032 3024\) Bayer RAW images captured by an iPhone X, saved in DNG format.

Baseline and comparative methodsWe compare two categories of methods, 3DGS-based methods, and NeRF-based methods. The baseline we compare against is RawGS which uses vanilla 3DGS for scene representation and employs the weighted L2 loss and multiple exposure training proposed in RawNeRF . Besides, we remove the SSIM loss, as its computation over local neighborhoods disrupts the noise model present in the RAW data. Additionally, we compare LDR-GS and HDR-GS, both of which are vanilla 3DGS trained on post-processed LDR images and unprocessed RAW images, respectively. The NeRF-based methods include RawNeRF  and LDR-NeRF. LDR-NeRF is a vanilla NeRF  trained on the post-processed LDR images with L2 loss.

Quantitative evaluationTab. 1 has shown the quantitative comparisons on the RawNeRF  dataset. Although NeRF-based methods have long training times and slow rendering speeds, they exhibit good metrics on sRGB. This indicates that the volume rendering they rely on has strong noise resistance (mainly due to the dense sampling on each ray). In contrast, 3DGS-based methods have inferior metrics compared to RawNeRF, due to their sparse scene representation and poor noise resistance. Additionally, the splitting of gaussians depends on gradient strength, and supervision using noisy raw images affects this process, leading to incomplete structure recovery. LE3D achieves better structure reconstruction suitable for downstream tasks through supervision on structure, depth distortion, and near-far regularizations, as detailed in Sec. 6. Note that the results of LE3D are comparable to the previous volumetric rendering-based method, RawNeRF , in both quantitative and qualitative aspects. However, it requires only 1% of the training time and achieves a 3000\(\)-6000\(\) rendering speed improvement.

Qualitative evaluationFig. 3 has shown the qualitative comparisons on the RawNeRF  dataset. We selected four scenes for comparison, including two indoor scenes and two outdoor scenes. The data for the first two scenes were collected with a single exposure, while the data for the latter two scenes included multiple exposures. Compared to 3DGS -based methods, LE3D demonstrates stronger noise resistance, particularly in the first two scenes. Additionally, LE3D achieves better results in distant scene reconstruction. For example, in the second scene, LE3D produces a smoother sky compared to RawGS, and in the fourth scene, LE3D recovers distant details more sharply. Compared to RawNeRF, LE3D typically produces smoother results while still effectively preserving details. Most importantly, LE3D offers faster training times and rendering speeds.

    &  &  &  &  &  \\   & & & & PSNR\(\) & SSIM\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  LDR-NeRF  & ✗ & 0.007 & 13.66 & – & – & 20.0391 & 0.5541 & 0.5669 \\ LDR-3DGS  & ✗ & 153 & 0.75 & – & – & 20.2936 & 0.5477 & 0.5344 \\  HDR-3DGS  & ✓ & 238 & 0.73 & 56.4960 & 0.9926 & 20.3320 & 0.5286 & 0.6563 \\ RawNeRF  & ✓ & 0.022 & 129.54 & 58.6920 & 0.9969 & 24.0836 & **0.6100** & **0.4952** \\ RawGS (Baseline) & ✓ & 176 & 1.05 & 59.2834 & 0.9971 & 23.3485 & 0.5843 & 0.5472 \\
**LE3D (Ours)** & ✓ & 103 & 1.53 & **61.0812** & **0.9983** & **24.6984** & 0.6076 & 0.5071 \\   

Table 1: Quantitative results on the test scenes of the RawNeRF  dataset. The best result is in **bold** whereas the second best one is in underlined. TM indicates whether the tone-mapping function can be replaced for HDR rendering. For methods where the tone-mapping function can be replaced, the metrics on sRGB are calculated using LDR tone-mapping for a fair comparison. The FPS measurement is conducted at a 2K (2016\(\)1512) resolution. Train denotes the training time of the method, measured in GPU\(\)H. LE3D achieves comparable performance with previous volumetric rendering based methods (RawNeRF ), but with 4000\(\) faster rendering speed.

[MISSING_PAGE_EMPTY:8]

Exposure variation and HDR tone-mappingLE3D can easily achieve exposure variation and recover details from overexposed data, as shown in Fig. 5 (f). Fig. 5 (g) showcases the various tone-mapping methods LE3D can implement, including global tone-mapping, such as color temperature and curve adjustments, and local tone-mapping using our re-implemented HDR+  (implementation details will be released in the supplementary materials).

Although RawNeRF  can also perform similar applications, its inability to achieve **real-time** rendering significantly limits its use cases, such as real-time editing described in Sec. C.

## 7 Conclusion

To address the long training times and slow rendering speeds of previous volumetric rendering-based methods, we propose LE3D based on 3DGS. Additionally, we introduce Cone Scatter Initialization and a tiny MLP to represent color in the linear color space. This addresses the issue of missing distant points in nighttime scenes with COLMAP initialization. It also replaces spherical harmonics with the tiny color MLP, effectively representing the linear color space. Finally, we enhance the structural reconstruction with the proposed depth distortion and near-far regularization, enabling more effective and realistic downstream tasks. Benefiting from the rendering images in the linear color space, LE3D can achieve more realistic exposure variation and HDR tone-mapping in real time, expanding the possibilities for subsequent HDR view synthesis processing.

Figure 4: Ablation studies on our purposed methods (_Zoom-in for best view_). CSI in (b) and Regs in (d) denote Cone Scatter Initialization and Regularizations, respectively. (e) shows the rendering result of LE3D w/ or w/o Color MLP in the early stages of training.

Figure 5: LE3D supports various applications. RawGS+ in (d) denotes using LE3D’s rendered image and RawGS’s structure information as input for refocusing. (c, e) are the weighted depth rendered by LE3D and RawGS, respectively. (f) shows the same scene rendered by LE3D with different exposure settings. In (g), the “\(\)” denotes global tone-mapping, while the “\(\)” represents local tone-mapping.