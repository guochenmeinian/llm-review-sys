# Amortized Decision-Aware

Bayesian Experimental Design

 Daolang Huang

Aalto University

daolang.huang@aalto.fi &Yujia Guo

Aalto University

yujia.guo@aalto.fi &Luigi Acerbi

University of Helsinki

luigi.acerbi@helsinki.fi &Samuel Kaski

Aalto University

University of Manchester

samuel.kaski@aalto.fi

###### Abstract

Many critical decisions are made based on insights gained from designing, observing, and analyzing a series of experiments. This highlights the crucial role of experimental design, which goes beyond merely collecting information on system parameters as in traditional _Bayesian experimental design_ (BED), but also plays a key part in facilitating _downstream decision-making_. Most recent BED methods use an amortized policy network to rapidly design experiments. However, the information gathered through these methods is suboptimal for down-the-line decision-making, as the experiments are not inherently designed with downstream objectives in mind. In this paper, we present an amortized decision-aware BED framework that prioritizes maximizing downstream decision utility. We introduce a novel architecture, the Transformer Neural Decision Process (TNDP), capable of instantly proposing the next experimental design, whilst inferring the downstream decision, thus effectively amortizing both tasks within a unified workflow. We demonstrate the performance of our method across two tasks, showing that it can deliver informative designs and facilitate accurate decision-making1.

## 1 Introduction

A fundamental challenge in a wide array of disciplines is the design of experiments to infer unknown properties of the systems under study [9; 7]. _Bayesian Experimental Design_ (BED) [17; 8; 24; 22] is a powerful framework to guide and optimize experiments by maximizing the expected amount of information about parameters gained from experiments, see Fig. 1(a). To pick the next optimal design, standard BED methods require estimating and optimizing the expected information gain (EIG) over the design space, which can be extremely time-consuming. This limitation has led to the development of _amortized_ BED [10; 14; 5; 6], a _policy-based_ method which leverages a neural network policy trained on simulated experimental trajectories to quickly generate designs, as illustrated in Fig. 1(b).

However, the ultimate goal in many tasks extends beyond parameter inference to inform a _downstream decision-making_ task by leveraging our understanding of these parameters, such as in personalized medical diagnostics . Previous amortized BED methods do not take down-the-line decision-making tasks into account, which is suboptimal for decision-making in scenarios where experiments can be adaptively designed. _Loss-calibrated inference_, which was originally introduced by Lacoste-Julienet al.  for variational approximations in Bayesian inference, provides a concept that adjusts the inference process to capture posterior regions critical for decision-making tasks. Inspired by this concept, we consider integrating decision-making directly into the experimental design process to align the proposed experimental designs more closely with the ultimate decision-making task.

In this paper, we propose an amortized decision-making-aware BED framework, see Fig. 1(c). We identify two key aspects where previous amortized BED methods fall short when applied to downstream decision-making tasks. First, the training objective of the existing methods does not consider downstream decision tasks. Therefore, we introduce the concept of _Decision Utility Gain_ (DUG) to guide experimental design to better align with the downstream objective. Second, to obtain the optimal decision, we still need to explicitly approximate the predictive distribution of the outcomes to estimate the utility. Current amortized methods learn this distribution only implicitly and therefore do not fully amortize the decision-making process. To address this, we propose a novel _Transformer neural decision process_ (TNDP) architecture, where the system can instantly propose informative experimental designs and make final decisions. Finally, we train under a non-myopic objective function that ensures decisions are made with consideration of future outcomes. We empirically show the effectiveness of our method through two tasks.

## 2 Decision-aware BED

### Preliminaries and problem setup

In this paper, we consider scenarios in which we design a series of experiments \(\) and observe corresponding outcomes \(y\) to inform a final decision-making step. The experimental history is denoted as \(h_{1:t}=\{(_{1},y_{1}),...,(_{t},y_{t})\}\) and we assume a fixed experimental budget with \(T\) query steps. Our objective is to identify an optimal decision \(a^{*}\) from a set of possible decisions \(\) at time \(T\).

To make decisions under uncertainty, _Bayesian decision theory_ provides an axiomatic framework by incorporating probabilistic beliefs about unknown parameters into decision-making. Given a task-specific utility function \(u(,a)\), which quantifies the value of the outcomes from different decisions \(a\) when the system is in state \(\), the optimal decision is then determined by maximizing the expected utility under the posterior distribution of the parameters \(p(|h_{1:t})\).

In many real-world scenarios, outcomes are stochastic and it is more typical to make decisions based on their _predictive distribution_\(p(y|,h_{1:t})=_{p(|h_{1:t})}[p(y|,,h_{1:t})]\), such as in clinical

Figure 1: **Overview of BED workflows.** (a) Traditional BED, which iterates between optimizing designs, running experiments, and updating the model via Bayesian inference. (b) Amortized BED, which uses a policy network for rapid experimental design generation. (c) Our decision-aware amortized BED integrates decision utility in the training to facilitate downstream decision-making.

trials where the optimal treatment is chosen based on predicted patient responses. A similar setup can be found in [15; 28]. Thus, we can represent our belief directly as \(p(y_{}|h_{1:t})\{p(y|,h_{1:t})\}_{}\), which is a stochastic process that defines a joint predictive distribution of outcomes indexed by the elements of the design set \(\), given the current information \(h_{1:t}\). Our utility function is then expressed as \(u(y_{},a)\), which is a natural extension of the traditional definition of utility by marginalizing out the posterior distribution of \(\). The rule for making the optimal decision is then reformulated as:

\[a^{*}=*{arg\,max}_{a A}_{p(y_{}|h_{1:t})}[u(y_{ },a)].\] (1)

### Decision utility gain

To quantify the effectiveness of each experimental design in terms of decision-making, we introduce _Decision Utility Gain_ (DUG), which is defined as the difference in the expected utility of the best decision, with the new information obtained from the current experimental design, versus the best decision with the information obtained from previous experiments.

**Definition 2.1**.: Given a historical experimental trajectory \(h_{1:t-1}\), the _Decision Utility Gain_ (DUG) for a given design \(_{t}\) and its corresponding outcome \(y_{t}\) at step \(t\) is defined as follows:

\[(_{t},y_{t})= _{a A}_{p(y_{}|h_{1:t-1}\{(_{t},y_{t}) \})}[u(y_{},a)]-_{a A}_{p(y_{}|h_{1:t-1})} [u(y_{},a)].\] (2)

DUG measures the improvement in the _maximum_ expected utility from observing a new experimental design, differing in this from standard marginal utility gain (see e.g., ). The optimal design is the one that provides the largest increase in maximal expected utility. At the time we choose the design \(_{t}\), the outcome remains uncertain. Therefore, we should consider the _Expected Decision Utility Gain_ (EDUG) to select the next design, which is defined as \((_{t})=_{p(y_{t}|_{t},h_{1:t-1})}[( _{t},y_{t})]\). The one-step lookahead optimal design can be determined by maximizing EDUG with \(^{*}=*{arg\,max}_{}()\). However, in practice, the true predictive distributions are often unknown, making the optimization of EDUG exceptionally challenging. This difficulty arises due to the inherent bi-level optimization problem and the need to evaluate two layers of expectations.

To avoid the expensive cost of optimizing EDUG, we propose using a policy network that directly maps historical data to the next design. This approach sidesteps the need to iteratively optimize EDUG by learning a design strategy over many simulated experiment trajectories beforehand.

### Amortization with TNDP

Our architecture, termed _Transformer Neural Decision Process_ (TNDP), is a novel architecture building upon the Transformer neural process (TNP) . It aims to amortize both the experimental design and the subsequent decision-making. A general introduction to TNP can be found in Appendix A.

The data architecture of our system comprises four parts: A **context set**\(D^{()}=\{(_{i}^{()},y_{i}^{()})\}_{i=1}^{t}\) contains all past \(t\)-step designs and outcomes; A **prediction set**\(D^{()}=\{(_{i}^{()},y_{i}^{()})\}_{i=1}^{n_{ }}\) consists of \(n_{}\) design-outcome pairs used for approximating \(p(y_{}|h_{1:t})\). The output from this head can then be used to estimate the expected utility; A **query set**\(D^{()}=\{_{i}^{()}\}_{i=1}^{n_{}}\) consists of \(n_{}\) candidate experimental designs being considered for the next step; **Global information**\(=[t,]\) where \(t\) represents the current step, and \(\) encapsulates task-related information, which could include contextual data relevant to the decision-making process.

TNDP comprises four main components, the full architecture is shown in Fig. 2(a). At first, the **data embedder block**\(f_{}\) maps each set of \(D\) to an aligned embedding space. The embeddings are then concatenated to form a unified representation \(=(^{()},^{()},^{( )},^{})\). After the initial embedding, the **Transformer block**\(f_{}\) processes \(\) using attention mechanisms that allow for selective interactions between different data components, ensuring that each part contributes appropriately to the final output. Fig. 2(b) shows an example attention mask. The output of \(f_{}\) is then split according to the specific needs of the query and prediction head \(=[^{()},^{()}]=f_{ }()\).

The primary role of the **prediction head**\(f_{}\) is to approximate \(p(y_{}|h_{1:t})\) at any step \(t\) with a family of parameterized distributions \(q(y_{}|_{t})\), where \(_{t}=f_{}(_{t}^{()})\) is the output of \(f_{}\). We choose a Gaussian likelihood and train \(f_{}\) by minimizing the negative log-likelihood of the predicted probabilities:

\[^{}=-_{t=1}^{T}_{i=1}^{n_{}} q(y_{i}^{ }|_{i,t})=-_{t=1}^{T}_{i=1}^{n_{}}(y_{i}^{}|_{i,t},_{i,t}^{2}),\] (3)

where \(_{i,t}\) represents the output of design \(_{i}^{}\) at step \(t\), \(\) and \(\) are the predicted mean and standard deviation split from \(\).

Lastly, the **query head**\(f_{}\) processes the embeddings \(^{}\) from the Transformer block to generate a policy distribution over possible experimental designs. The outputs of the query head, \(=f_{}(^{})\), are mapped to a probability distribution \((_{t}^{}|h_{1:t-1})\) via a Softmax function. To design a reward signal that guides the query head \(f_{}\) in proposing informative designs, we first define a single-step immediate reward based on DUG (Eq. (2)), replacing the true predictive distribution with our approximated distribution:

\[r_{t}(_{t}^{})=_{a A}_{q(y_{}|_{t} )}[u(y_{},a)]-_{a A}_{q(y_{}|_{t-1})}[u(y_{},a)].\] (4)

This reward quantifies how the experimental design influences our decision-making by estimating the improvement in expected utility that results from incorporating new experimental outcomes. However, this objective remains myopic, as it does not account for the future or the final decision-making. To address this, we employ the REINFORCE algorithm . The final loss of \(f_{}\) can be written as:

\[^{}=-_{t=1}^{T}R_{t}(_{t}^{}|h_{ 1:t-1}),\] (5)

where \(R_{t}=_{k=t}^{T}^{k-t}r_{k}(_{k}^{})\) represents the non-myopic discounted reward. The discount factor \(\) is used to decrease the importance of rewards received at later time step. \(_{t}^{}\) is obtained through sampling from the policy distribution \(_{t}^{}(|h_{1:t-1})\). The details of implementing and training TNDP are shown in Appendix B.

## 3 Experiments

### Toy example: targeted active learning

We begin with an illustrative example to show how our TNDP works. We consider a synthetic regression task where the goal is to perform regression at a specific test point \(x^{*}\) on an unknown function. To accurately predict this point, we need to actively collect some new points to query.

Figure 2: **Illustration of TNDP.** (a) An overview of TNDP architecture with input consisting of 2 observed design-outcome pairs from \(D^{}\), 2 designs from \(D^{}\) for prediction, and 2 candidate designs from \(D^{}\) for query. (b) The corresponding attention mask. The colored squares indicate that the elements on the left can attend to the elements on the top in the self-attention layer of \(f_{}\).

The design space \(=\) is the domain of \(x\), and \(y\) is the corresponding noisy observations of the function. Let \(()\) denote the set of combinations of distributions that can be output by TNDP, we can then define decision space to be \(=()\). The downstream decision is to output a predictive distribution for \(y^{*}\) given a test point \(x^{*}\), and the utility function \(u(y_{},a)= q(y^{*}|x^{*},h_{1:t})\) is the log probability of \(y\) under the predicted distribution.

During training, we sample functions from Gaussian Processes (GPs)  with squared exponential kernels of varying output variances and lengthscales and randomly sample a point as the test point \(x^{*}\). We set the global contextual information \(\) as the test point \(x^{*}\). For illustration purposes, we consider only the case where \(T=1\). Additional details for the data generation can be found in Appendix C.

**Results.** From Fig. 3, we can observe that the values of \(\) concentrate near \(x^{*}\), meaning our query head \(f_{}\) tends to query points close to \(x^{*}\) to maximize the DUG. This is an intuitive example demonstrating that our TNDP can adjust its design strategy based on the downstream task.

### Top-\(k\) hyperparameter optimization

In traditional optimization tasks, we typically only aim to find a single point that maximizes the underlying function \(f\). However, instead of identifying a single optimal point, there are scenarios where we wish to estimate a set of top-\(k\) distinct optima, such as in materials discovery [18; 27].

In this experiment, we choose hyperparameter optimization (HPO) tasks and conduct experiments on the HPO-B datasets . The design space \(\) is a finite set defined over the hyperparameter space and the outcome \(y\) is the accuracy of a given configuration. Our decision is to find \(k\) hyperparameter sets, denoted as \(a=(a_{1},...,a_{k}) A^{k}\), with \(a_{i} a_{j}\). The utility function is then defined as \(u(y_{},a)=_{i=1}^{k}y_{a_{i}}\), where \(y_{a_{i}}\) is the accuracy corresponding to the configuration \(a_{i}\).

We compare our methods with five different BO acquisition functions: random sampling (RS), Upper Confidence Bound (UCB), Expected Improvement (EI), Probability of Improvement (PI), and an amortized method PFNs4BO . We set \(k=3\) and \(T=50\). Our experiments are conducted on four search spaces. All results are evaluated on a predefined test set. For more details, see Appendix D.

**Results.** From the results (Fig. 4), our method demonstrates superior performance across all four meta-datasets, particularly during the first 10 queries.

## 4 Discussion and conclusion

In this paper, we introduced a decision-aware amortized BED framework with a novel TNDP architecture to optimize experimental designs for better decision-making. Future work includes conducting more extensive empirical tests and ablation studies, deploying more advanced RL algorithms  to enhance training stability, addressing robust experimental design under model misspecification [22; 13; 25], and developing dimension-agnostic methods to expand the scope of amortization.

Figure 4: **Average utility on Top-\(k\) HPO task. The error bars represent the standard deviation over five runs. TNDP consistently achieved the best performance regarding the utility.**

Figure 3: **Results of toy example. The top figure represents the true function and the initial known points. The red line indicates the location of \(x^{*}\). The blue star marks the next query point, sampled from the policy’s predicted distribution shown in the bottom figure.**