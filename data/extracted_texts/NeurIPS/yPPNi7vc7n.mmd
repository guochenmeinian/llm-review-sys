# Local Curvature Smoothing with Stein's Identity for Efficient Score Matching

Genki Osada

LY Corporation, Japan

genki.osada@lycorp.co.jp

&Makoto Shing

Sakana AI, Japan

mkshing@sakana.ai

&Takashi Nishide

University of Tsukuba, Japan

nishide@risk.tsukuba.ac.jp

###### Abstract

The training of score-based diffusion models (SDMs) is based on score matching. The challenge of score matching is that it includes a computationally expensive Jacobian trace. While several methods have been proposed to avoid this computation, each has drawbacks, such as instability during training and approximating the learning as learning a denoising vector field rather than a true score. We propose a novel score matching variant, local curvature smoothing with Stein's identity (LCSS). The LCSS bypasses the Jacobian trace by applying Stein's identity, enabling regularization effectiveness and efficient computation. We show that LCSS surpasses existing methods in sample generation performance and matches the performance of denoising score matching, widely adopted by most SDMs, in evaluations such as FID, Inception score, and bits per dimension. Furthermore, we show that LCSS enables realistic image generation even at a high resolution of \(1024 1024\).

## 1 Introduction

Score-based diffusion models (SDMs)  have emerged as powerful generative models that have achieved remarkable results in various fields . While likelihood-based models learn the density of observed (i.e., training) data as points , SDMs learn the gradient of logarithm density called the _score_ -- a vector field pointing toward increasing data density. The sample generation process of SDMs has two steps: 1) learning the score for a given dataset and 2) generating samples by guiding a random noise vector toward high-density regions based on the learned score using stochastic differential equation (SDE).

Score matching used for learning the score includes a computationally expensive Jacobian trace, making it challenging to apply to high-dimensional data. While some methods have been proposed to avoid computing the Jacobian trace, each has its drawbacks. Denoising score matching (DSM) , ubiquitously employed in SDMs, learns not the ground truth score but its approximation and imposes constraints on the design of SDE. On the other hand, sliced score matching (SSM)  and its variant, finite-difference SSM (FD-SSM) , suffer from high variance due to using random projection.

In this paper, we propose a novel score matching variant, local curvature smoothing with Stein's identity (LCSS). The key idea of LCSS is to use Stein's identity to bypass the expensive computation of Jacobian trace. To apply Stein's identity, we take the expectation over a Gaussian distributioncentered on input data points, which is indeed equivalent to the regularization with local curvature smoothing. Exploiting this equivalence, we propose a score matching method that offers both regularization benefits and faster computation.

We first establish a method as an independent score matching technique, then propose a time-conditional version for its application to SDMs. We present the experimental results using synthetic data and several popular datasets. Our proposed method is highly efficient compared to existing score matching methods and enables the generation of high-resolution images with a size of \(1024\). We show that LCSS outperforms SSM, FD-SSM, and DSM in the quality of generated images and is comparable to DSM in the qualitative evaluation of the FID score, Inception score, and the negative likelihood measured in bits per dimension. While DSM requires the drift and diffusion coefficients of an SDE to be affine, our LCSS has no such constraint, allowing for a more flexible SDE design (Sec. 2.4). Hence, this paper contributes to opening up new directions in SDMs' research based on more flexible SDEs.

Related works.Liu et al.  proposed a method for directly estimating scores using Stein's identity without using score matching. Shi et al.  further enhanced that method by applying spectral decomposition to the function in Stein's identity. However, Song et al.  reported that these methods underperform compared to SSM. In our approach, we use Stein's identity specifically to avoid computing the Jacobian trace in score matching. The regularization effect attained by adding noise to data has been recognized for a long time , which our method utilizes. The relationship between noise-adding regularization and curvature smoothing in the least square function is elucidated in Bishop . The previous studies of score matching variants are described in the next section. In efforts to remove the affine constraint of SDE in SDMs, Kim et al.  proposed running SDE in the latent space of normalizing flows. This constraint stems from using DSM for score matching, and we propose a score matching method free from such constraint.

## 2 Preliminary

### Score-based diffusion models

Score-based diffusion models (SDMs) [35; 31] define an stochastic differential equation (SDE) for \(_{t}^{d}\) in continuous time \(t[0,T]\) as

\[d_{t}=(_{t},t)dt+g(t)d_{t},\] (1)

Figure 1: Samples generated from models trained on CelebA-HQ (\(1024 1024\)) using our proposed score matching method, LCSS. The rightmost images in each row are generated by DDPM++ with subVP SDE, while the rest are by NCSN++ with VE SDE.

where \((,t):^{d}^{d}\) is the drift coefficient, \(g(t)\) is the diffusion coefficient, and \(_{t}\) denotes a standard Wiener process. Eq. (1), known as the forward process, has a corresponding reverse process from time \(T\) to \(0\):

\[d_{t}=[(_{t},t)-g(t)^{2}_{}  p_{t}(_{t})]dt+g(t)d}_{t}\] (2)

where \(}_{t}\) is a standard Wiener process in reverse-time and \(p_{t}(_{t})\) denotes the ground truth marginal density of \(_{t}\) following the forward process. Samples from a dataset are represented as \(_{0} p_{0}\), while initial vectors for sample generation with Eq. (2) are \(_{T} p_{T}\). In Eq. (2), the only unknown term is \(_{} p_{t}(_{t})\), referred to as the _score_ of density \(p_{t}(_{t})\). To estimate \(_{} p_{t}(_{t})\), SDMs train a score network \(_{}\) parametrized by \(\) by _score matching_.

### Score matching

A score network \(_{}\) that estimates the score of the ground truth density is trained through _score matching_. Score matching, a technique independent of SDMs and SDE, has no concept of time. So, as long as our discussion is focused on score matching, we use the notation of \(\) and \(p\), without the subscript of \(t\), and treat a score network without conditioning on \(t\), i.e., denote it as \(_{}()\) instead of \(_{}(_{t},t)\). Score matching is defined as the minimization of \(():=}_{ p} \|_{}()-_{} p()\|_{2}^ {2}\). Calculating \(()\) is generally impractical since it requires knowing the ground truth \(_{} p()\), but Hyvarinen  has shown that \(()\) is equivalent to the following \(_{}()\) up to constant:

\[_{}():=}_{ p }[_{}^{s}(,)]\] (3)

where \(_{}^{s}(,)\) is the version of \(_{}()\) for a single data point \(\), defined as

\[_{}^{s}(,):=(_{ }_{}())+\|_{ }()\|_{2}^{2}.\] (4)

Score matching in SDMs and its problem.SDMs train a time-conditional score function \(_{}(_{t},t)\) using score matching. The loss function of SDMs is defined as the integral of \(_{}()\) over time \(t[0,T]\) as

\[_{}():=_{0}^{T}(t)}_{_{t} p_{t}}}_{_{0} p_{ 0}}[(_{}_{}(_{ t},t))+\|_{}(_{t},t)\|_{2}^{2} ]dt.\] (5)

The weight function \((t)\) is determined by the form of the SDE, and \((t)\) used for typical SDEs can be found in Table 1 in . The \(p_{t}\) is obtained from the SDE in Eq. (1), with its mean dependent on \(_{0}\), and its specific form in typical SDEs is given as Eq. (29) in Song et al. . The problem is that since \(_{}(_{t},t)\) has the same dimension as input \(_{t}\), computing its Jacobian trace, \((_{}_{}(_{t},t))\), is costly. It renders training with score matching impractical in high-dimensional data.

### Existing score matching variants

To avoid the computation of the Jacobian trace, the following scalable variants of \(_{}\) have been developed. While any score matching method can be used to train \(_{}\), SDMs predominantly employ DSM due to its empirical performance [31; 35; 12].

Sliced score matching (SSM) and finite-difference sliced score matching (FD-SSM).SSM  approximates \((_{}_{}())\) with Hutchinson's trick  and minimizes the following:

\[_{}():=}_{ p_ {}}}_{ p}[}^{T}_{}_{}() +\|_{}()\|_{2}^{2} ],\] (6)

where \(\) is a small scaler value and \( p_{}\) is a \(d\)-dimensional random vector such that \(}_{ p_{}}[^ {T}]=I}{d}\). To enhance the efficiency of SSM further, FD-SSM  adopts finite difference to Eq. (6). The objective function is

\[_{}():=}_{ p_{}}}_{  p}[}(^{T}_{}( +)-^{T}_{}(-) ).\\ +.\|_{}(+)+_{}(-)\|_{2}^{2}].\] (7)The drawback of these two methods is the high variance induced by random projection with \(\). In particular, the error between the true trace of matrix \(A\), \((A)\), and the estimate by Hutchinson's trick, \(_{A}\), is \(|(A)-_{A}|}\|A \|_{F}\) where \(\|\|_{F}\) is the Frobenius norm and \(M\) is the sampling times from \(p_{}\). Typically, \(M=1\) setting is employed in these methods, potentially making the error magnitude non-negligible and causing instability in training process, as we see in Sec 4.2.3.

Denoising score matching (DSM).DSM  circumvents the computation of \((_{}_{}())\) by perturbing \(\) with a Gaussian noise distribution \(q_{}(}|)\) with noise scale \(\) and then estimating the score of the perturbed distribution \(q_{}(}):= q_{}(}|)p()d\). The DSM minimizes

\[_{}():=_{} q_{ }(}|)}\,_{ p_{0}} [\|_{}(})-_{ } q_{}(}|)\|_{2}^{2} ].\] (8)

In SDMs, the following time-conditional version is used:

\[_{}(,t):=_{}_{t} q _{}(}|_{0})}\,_{_{0} p _{0}}[\|_{}(}_{t},t)- _{} q_{_{t}}(}_{t}|_{0} )\|_{2}^{2}],\] (9)

where \(_{t}\) is designed to increase as \(t\) progresses from \(0\) to \(T\). Almost all SDMs use DSM for score matching because it performs faster and is more stable than SSM and FD-SSM. However, DSM has three drawbacks. 1) Approximation: in DSM, \(_{}\) learns \(_{} q_{_{t}}(}_{t}|_{0})\) rather than the ground true score, \(_{} p_{t}(_{t})\). 2) Constraining the design of SDE: DSM constrains SDE coefficients to be affine. We will describe this in Sec. 2.4.

3) The dilemma regarding \(_{t}\): Only when \(_{t} 0\) does \(_{} q_{_{t}}(}_{t}|_{0})\) match \(_{} p_{t}(_{t})\). However, as \( 0\), both the numerator and denominator of \(_{0}-}_{t}}{_{t}^{2}}\) approach 0, leading to potential numerical instability .

### DSM restricts SDE to affine

The design of SDEs directly influences the performance of SDMs, as demonstrated in previous studies . The benefits of non-linear SDE, particularly highlighted in , enable more accurate alignment of scores with the ground-truth data distributions than affine SDE and thus enhance the quality of generated samples. (Fig. 2 in  illustrates this.) However, unless specific modifications are made as proposed in these studies, the general SDEs  used in almost all existing SDMs must be affine. This constraint comes from the fact that the SDMs, consciously or unconsciously, select DSM for their score matching methods. The loss function of DSM requires \(_{} q_{_{t}}(}_{t}|_{0})\) as Eq. (9). Thus, to compute Eq. (9) at every training iteration, \(_{} q_{_{t}}(}_{t}|_{0})\) needs to be in closed form. DSM models \(q_{_{t}}(}_{t}|_{0})\) as a Gaussian distribution, for which this requirement is satisfied as \(_{} q_{_{t}}(}_{t}|_{0})= _{0}-}_{t}}{_{t}^{2}}\). However, this Gaussian modeling comes at the cost of imposing a constraint on the SDE design: the drift and diffusion terms of SDE, i.e., \((_{t},t)\) and \(g(t)\) in Eq. (1), need to be affine. The existing SDMs are DSM-based, so the SDEs used in these SDMs, including the VE SDE and subVP SDE we use in our experiments, are designed to adhere to this constraint. The details of the same discussion and the specific form of the Gaussian distribution \(q_{_{t}}(}_{t}|_{0})\) for the typical SDEs can be found in Sec. 3.3 in Song et al. . Unlike DSM, SSM and FD-SSM do not have this limitation, allowing for more flexible SDE design and thus removing the requirement to limit the forward process's convergence destination to Gaussian distributions. Unfortunately, as we will see later, SSM and FD-SSM cannot handle high-dimensional data due to the high-variance they cause. Our proposed method uniquely satisfies both the flexible design of SDEs and compatibility with high-dimensional data.

## 3 Our Method

We propose a novel score matching variant that avoids the expensive computation of the Jacobian trace. The crux of our method is using Stein's identity to bypass Jacobian computation. Our approach comprises three steps: 1) introducing local curvature smoothing regularization into score matching (Definition 1), 2) treating the regularization of 1) as taking an expectation over a Gaussian distribution (Lemma 1), and 3) applying Stein's identity (Corollary 2). While introducing regularization may appear to cause extra computational costs, it enables faster computation by the use of Stein's identity trick. We begin by discussing our method separately from SDMs, without involving the time variable \(t\), and then explain its incorporation into SDMs at the end of this section.

### Score matching with Local Curvature Smoothing with Stein's identity (LCSS)

We first introduce some lemmas and corollaries that constitute our method.

**Definition 1** (Score matching with local curvature smoothing ).: _Regularizing the score matching objective \(_{}^{s}\) at a data point \(^{d}\) with local curvature smoothing (LCS) is defined as:_

\[_{}^{s}(,,):=_{}^{s}(,)+^{2}\|_{} _{}()\|_{F}^{2}.\] (10)

Given \(_{}_{}()\) approximating the Hessian of \( p()\), minimizing the regularization term \(^{2}\|_{}_{}()\|_{F}^{2}\) acts as a local curvature smoothing where the square of the curvature of the surface of the log-density at \(\) are penalized. Curvature smoothing is one of the commonly employed regularizations in machine learning .

**Lemma 1** (Kingma and LeCun ).: _Score matching with local curvature smoothing (Definition 1) is equivalent to the expectation of \(_{}^{s}(,)\) over a Gaussian distribution centered at \(\), i.e., \(^{}(,^{2}_{d})\):_

\[_{}^{s}(,,)=_{^{}(,^{2}_{d})}[ _{}^{s}(,^{})]+( ^{2}),\] (11)

_where \(:=\|^{}-\|_{2}\)._

Lemma 1 states that taking the expectation of score matching objective with respect to a Gaussian distribution centered around \(\) yields an effect equivalent to a curvature smoothing regularization.

**Definition 2** (Stein class ).: _Assume that \(Q()\) is a continuous differentiable probability density supported on \(^{d}\). Then, a function \(f:\) is the Stein class of \(Q\) if \(f\) satisfies_

\[_{}_{}(f()Q() )d=0.\] (12)

The condition for Eq. (12) to hold is

\[_{\|\|}f()Q()=0.\] (13)

**Lemma 2** (Stein's identity, Liu et al. , Gorham and Mackey ).: _Let \(:^{d^{}}\) be a smooth (i.e., continuous and differentiable) vector valued function \(()=[h_{1}(),h_{2}(), h_{d^{ }}()]^{T}\). Then, if \(h_{i}() i=1,,d^{}\) is the Stein class of a smooth density \(Q()\), the following identity holds:_

\[_{ Q}[()_{}  Q()^{T}+_{}()]= _{d^{},d}.\] (14)

In Eq. (14), \(_{} Q()^{T}\) is a \(1 d\) matrix, \(_{}()\) is a \(d^{} d\) matrix, and \(_{d^{},d}\) is a \(d^{} d\) zero matrix.

**Corollary 1** (Li and Turner ).: _When \(Q()=(;,^{2}_{d})\), we have_

\[_{ Q}[h_{i}()-_{i}}{ ^{2}}]=_{ Q}[_{}h_{i }()].\] (15)

Eq. (15) holds for the \(i\)-th element of the vector \(\). The condition Eq. (13) holds for Gaussian distribution \(Q\), since \(Q() 0\) as \(\|\|\). Then, \(h_{i}() i=1,,d^{}\) are the Stein class of \(Q\), and thus Lemma 2 is valid for a Gaussian distribution \(Q\). As we also know \(_{} Q()=-}(- )\), by substituting it into Lemma 2, we obtain Corollary 1.

**Corollary 2** (Bypassing Jacobian trace computation).: _Let \(^{d 1}\), \(_{}()^{d 1}\), and \(Q(^{})=(^{};,^{2} _{d})\). With Corollary 1 and a few assumptions, we have the following:_

\[_{^{}(,^{2}_{ d})}[(_{}_{}( ^{}))]=_{^{}( ,^{2}_{d})}[_{}(^{ })^{T}^{}-}{^{2}}].\] (16)

The \(_{}\), which represents a score network in our context, corresponds to \(\) in Lemma 2 and Corollary 1. The derivation of Eq. (16) is presented in Appendix A, in which we assume the interchangeability between the expectation and summation regarding \(_{}(^{})\).

Objective function of LCSS.We propose a variant of score matching method, local curvature smoothing with Stein's identity (LCSS). The development of the objective function of LCSS, \(_{}^{s}\), begins with the curvature smoothing regularization of Eq. (10), followed by the application of Lemma 1 and Corollary 2. Since \(_{}^{s}(,,)\) in Eq. (10) involves computationally expensive \(_{}_{}()\), alongside the original challenge of \((_{}_{}())\) in \(_{}^{s}\), training with \(_{}^{s}\) is impractical for high-dimensional data. However, by inserting the transformation of Lemma 1, it enables the application of Corollary 2 to \(_{}^{s}\). By substituting Eq. (4) into Eq. (11) and ignoring \((^{2})\), we have

\[_{}^{s}(,,)=_{^ {}(,^{2}_{d})}[ (_{}_{}(^{ }))+_{}(^{}) _{2}^{2}],\] (17)

and by applying Eq. (16) to the first term, we obtain \(_{}^{s}\) as:

\[_{}^{s}(,,):=_{^{}(,^{2}_{d})}[_{ }(^{})^{T}^{}-}{ ^{2}}+_{}(^{}) _{2}^{2}].\] (18)

In \(_{}^{s}\), \((_{}_{}())\) is replaced with the inner product, \(_{}(^{})^{T}^{}- }{^{2}}\), which is computed efficiently, thereby bypassing the issue of high computational cost.

Comparing LCSS with existing score matching methods.Unlike SSM and FD-SSM, LCSS does not use random projection, eliminating the high variance issue. While DSM learns the approximation of ground truth score \(_{} q_{}(}|)\), LCSS learns the ground truth score \(_{} p()\). Furthermore, unlike DSM, \(_{}^{s}\) does not require \(_{} q_{}(}|)\), thus eliminating the need for affine restrictions on the SDE coefficients. The original score matching, i.e., minimizing \(_{}^{s}\), involves the following two: (1) Increasing the first term \((_{}_{}()) _{}_{} p()\), the divergence of the score, in the negative direction promotes \(_{}\) to learn the vector field flowing into points where \(\) exists. (2) Minimizing the second term \(_{}()_{2}^{2}\) promotes \(_{}\) to learn that its length approaches \(0\) at points where \(\) exists. The LCSS also performs both (1) and (2), but instead of at a single point \(\), it considers a Gaussian cloud centered around \(\). By applying Stein's identity, LCSS bypasses the challenge of (1), thereby making score matching feasible even for high-dimensional data.

### Score-based diffusion models with LCSS

We define time-conditional version of LCSS for training SDMs as:

\[_{}^{s}(,_{0},t,_{t}):= _{_{t}^{}(_{0},_{t}^{2}_{d})}[_{}(_{t}^{},t)^{T}_{t}^{}-_{0}}{_{t}^{2}}+ _{}(_{t}^{},t)_{2}^{2}]\] (19)

and formulate the loss function of SDMs based on LCSS as:

\[_{}():=_{0}^{T}(t)\,_{ _{0} p_{0}}[_{}^{s}(,_{0},t,_{t})]dt.\] (20)

We replace \(\) in Eq. (18) with a time-varying \(_{t}\). By making \(_{t}\) take on a wide range of values depending on \(t\), we aim to facilitate robust learning of score vectors even in low-density regions in \(p_{0}\), mirroring the original motivation of NCSN . With Eq. (19), \(_{}\) learns a vector in the direction of \(-(_{t}^{}-_{0})\) to minimize the inner product of the first term, weighted by \(^{2}}\), while minimizing its \(L_{2}\) norm, \(_{}(_{t}^{},t)_{2}\). Sampling \(_{t}^{}\) in the expectation in Eq. (19) only once yields satisfactory performance, as evidenced by our experimentation.

SDEs for LCSS-based SDMs can be designed flexibly without restricting the drift and diffusion coefficients to be affine. However, devising a new SDE is beyond the scope of this paper and is left for future work, and our experiments use existing SDEs designed for use with DSM: the Variance Exploding (VE) SDE and the sub Variance Preserving (subVP) SDE . Taking advantage of the fact that \(p_{t}\) is a Gaussian distribution in both SDEs, we employ the standard deviation of \(p_{t}\) as the value ot \(_{t}\) in each SDE in our experiments. For example, for VE SDE, \(_{t}=g(t)\). For both SDEs, \(_{t}\) increases as \(t\) goes from \(0\) to \(T\), but the way it increases is different for each SDE.

Following Song and Ermon , we set \((t)=g(t)^{2}\). With this setting, \((t)\) becomes \((t)=g(t)^{2}=_{t}^{2}\), effectively cancelling out \(_{t}^{2}\) in the denominator of Eq. (19) and avoiding unstable situations where the denominator could become zero. For other SDE types (VP and sub VP), \((t)\) is more elaborate but similarly cancels out \(_{t}^{2}\) in the denominator. For fairness, we note that, similarly, in training SDMs with DSM, applying the coefficient \((t)=g(t)^{2}\) allows for the cancellation of \(_{t}^{2}\) in the denominator, thus circumventing the weakness of DSM.

## 4 Experiments

In this section, we demonstrate that our LCSS enables fast model training and high-quality image generation on several commonly used image datasets.

### Setup

We use five SDMs: NCSNv2 1 as a discrete-time model, NCSN++ and DDPM++ and their extensive version, NCSN++ deep and DDPM++ deep, as continuous-time models. Only for a synthetic dataset, Checkerboard, we use a multilayer perceptron (MLP) with publicly available code2 based on Song and Ermon . In continuous-time models, we use VE SDE for NCSN++ and NCSN++ deep and subVP SDE for DDPM++ and DDPM++ deep as per Song et al. . The same SDEs are applied to all the score matching methods we evaluate, including our LCSS. We use the official codes from the original papers, and the hyperparameters are kept as in the official code, unless stated otherwise. 3 For LCSS, we perform only one sampling iteration to calculate the expectation in \(_{}^{s}\) (Eq. (19)). We set \(_{t}=g(t)\) in each SDEs. All experiments are performed on a server with 128 GB RAM, 32 Intel Xeon, Silver 4316 CPUs, and eight NVIDIA A100 SXM GPUs.

### Results

We evaluate the proposed LCSS against existing score matching methods, SSM, FD-SSM, and DSM, in density estimation, training efficiency, and qualitative and quantitative sample generation evaluation.

#### 4.2.1 Density estimation

We first compare LCSS to SSM, FD-SSM, and DSM in score matching performance. We visualize estimated densities on Checkerboard dataset, whose density is multi-modal. The details of the experiments, including the training loss curve, are presented in Appendix B. Table 1 depicts the density distribution learned by the model. Compared to SSM and FD-SSM, LCSS demonstrates higher accuracy in density estimation with faster convergence and stability in loss reduction. DSM exhibits similar accuracy in density estimation and stability in loss reduction to LCSS. However, LCSS shows slightly better consistency in estimating high-density regions (bright-colored areas) and maintains stable loss.

#### 4.2.2 Training efficiency

We compare LCSS with the existing score matching methods for training efficiency. We measure the time taken for model training on Checkerboard and FFHQ dataset  resized to \(256 256\). Table 2 shows the average elapsed time over \(100\) epochs for Checkerboard and \(1000\) iterations for FFHQ, respectively. It shows that LCSS is the most efficient.

#### 4.2.3 Sample quality

We show generated samples on CIFAR-10 using NCSN++ deep and DDPM++ deep trained with LCSS in Appendix C.1. In this subsection, we qualitatively compare the sample generation capability of LCSS with existing methods.

    & &  \\  Dataset & Model & SSM & FD-SSM & DSM & LCSS \\  Checkerboard & MLP & 497 & 445 & 430 & **419** \\ FFHQ & NCSNv2 & 1838 & 1367 & 1381 & **1075** \\   

Table 1: Estimated densities on Checkerboard.

    & &  \\  Dataset & Model & SSM & FD-SSM & DSM & LCSS \\  Checkerboard & MLP & 497 & 445 & 430 & **419** \\ FFHQ & NCSNv2 & 1838 & 1367 & 1381 & **1075** \\   

Table 2: Elapsed time for model training (ms)\(\).

Comparison with SSM and FD-SSM.We first focus on comparing LCSS with SSM and FDSSM.4 We generate samples using NCSNV2 on CIFAR-10 \((32 32)\), CelebA \((64 64)\), and FFHQ \((256 256)\). The results show that LCSS demonstrates stable long-term training and faster convergence compared to the other two methods. This can be explained by LCSS not using random projection, unlike SSM and FD-SSM. Details are provided below.

On CIFAR-\(10\)\((32 32)\), unlike LCSS, SSM and FD-SSM, when reaching 95k and 495k training steps, respectively, are unable to continue generating meaningful images and produce only entirely black images. Fig. 2 displays generated images at 5k and 90k training steps for each method. The faster convergence of LCSS compared to SSM and FD-SSM is exhibited from the differences in the image quality. On CelebA \((64 64)\), Fig. 3 (left) displays images generated by each method at 10k steps, highlighting LCSS's faster learning. Fig. 3 (right) presents the generated images of LCSS and FD-SSM at the 210k training steps. For SSM, after 65k training steps, it only generated completely black images, so the displayed SSM images are from the model trained for 60k iteration. On FFHQ \((256 256)\), LCSS can generate decent images, while SSM and FD-SSM failed, as shown in Fig. 4

Comparison with DSM.In the previous experiments, we saw that LCSS significantly outperforms SSM and FD-SSM in image generation. In this subsection, we compare LCSS with DSM, widely adopted as the objective function in score-based diffusion models. The results show that LCSS surpasses DSM in qualitative evaluation, and achieves performance on par with DSM in quantitative evaluation on CIFAR-10 using Frechet inception distance (FID), Inception score (IS), and negative log likelihood measured in bits per dimension (BPD). The details are below.

We compare generated samples on FFHQ, AFHQ, and FFHQ + AFHQ. The size of images in the three datasets is \((256 256)\), and we train NCSNV2 for 600k with batch size 16 on each of them. On FFHQ, LCSS can generate more realistic images than DSM, as shown in Fig. 4. We note that during the training with DSM, around 210k training steps, a sharp decline in the quality of generated images was observed. On AFHQ , Fig. 5 shows that LCSS generates realistic samples, but DSM does not. We also create and examine with a dataset FFHQ + AFHQ, a fusion of FFHQ and AFHQ, designed to increase learning difficulty by diversifying data modalities. On FFHQ + AFHQ, Fig. 6 shows LCSS's superior capability in generating realistic images over DSM.

Figure 3: Comparison of generated samples on CelebA \((64 64)\). The left three show samples from models trained for 10k steps. In the right three, FD-SSM and LCSS images are from models trained for 210k steps, whereas SSM images are from a model trained for 60k steps.

Figure 6: Samples on FFHQ + AFHQ.

Figure 2: Comparison of sample quality in the early stages of training. The model is NCSNV2 trained on CIFAR-10. The left three panels show generated samples at 5k steps training, while the right three show generated samples at 90k steps training.

Table 3 shows the qualitative results on CIFAR-10. Regardless of SDMs, LCSS tends to surpass DSM in IS but underperform in FID. Compared to the values in Song et al. , our experimental results generally exhibit higher (better) IS values and higher (worse) FID values.5 In BPD, LCSS surpasses DSM in DDPM++ variants but underperforms in NCSN++ variants. Overall, qualitative evaluation on CIFAR-10 suggests no decisive superiority between LCSS and DSM, hinting at distinct characteristics.

Summary.Table 4 illustrates a highly simplified comparison of the relative performance between LCSS and DSM. Model training is more complex in Case #2 than in Case #1. It was observed that in challenging conditions like Case #2, DSM suffered from performance degradation. We regularly monitored the quality of generated images during model training. In the experiments of Case #2 with DSM, as noted above, although the quality of generated images was improving up to a certain stage (around 210k iterations, for example), it suddenly deteriorated. Also, frequent spikes in loss values were observed during training with DSM, which appeared to be a trigger for the deterioration. Unlike DSM, LCSS retained superior performance without suffering from such instability.

### High resolution image generation

We demonstrate that learning with LCSS enables models to generate high-resolution images. We train NCSN++ and DDPM++ on CelebA HQ (\(1024 1024\)) , using hyperparameters consistent with those used to train DSM-based models in Song et al. . In Fig. 1, we show generated images: NCSN++ images are from the model trained for around 1.3M iterations, and DDPM++ ones are trained for around 0.3M iterations, with batch size 16 for both. The figures show that LCSS is promising as a score matching method. More generated samples are presented in Appendix C.2.

### Ablation study

The loss function of LCSS, similar to the original score matching, consists of two terms. To study the roles of each term, using the modified version of \(^{s}_{}\) in Eq. (19) with a balancing coefficient \(\) as

\[_{^{}_{t}(_{0},^{2}_ {1}_{t})}[_{}(^{}_{t},t)^ {T}^{}_{t}-_{0}}{^{2}_{t}}+\|_{}(^{}_{t},t)\|_{2}^{2} ],\] (21)

we train NCSNv2 on FFHQ. Images generated from the models trained with different \(\) are shown in Fig. 7. When \(=0.5\), only noisy images akin to those at time \(t=T\), \(_{T}\), are produced. With \(<1\), the force to minimize the second term, \(\|_{}(^{}_{t},t)\|_{2}^{2}\), is more emphasized than when using the original \(^{s}_{}\), leading to shorter score vector lengths. The score vector length is directly linked to the spatial movement distance of \(_{t}\) during the reverse process for sample generation. Since the score vector is forced to be short, the noise \(_{T}\) generated at the start of the sample generation process cannot reach the regions corresponding to realistic images with high density as it traces back from time \(T\) to \(0\). On the other hand, when \(>1\), particularly for \(=10\), it is observed that as the number of training iterations increased, images with emphasized contours but lost textures are generated. It suggests that the involvement of the first term of \(_{}^{s}\) in contour formation. The object contours in an image are characterized by rapid changes in pixel values, which can be associated with high curvature or changes in second-order derivatives. Since \(_{}(_{t}^{},t)^{T}_{t}^{ }-_{t}}{^{2}}\) in Eq. (21) or (19) corresponds to the Hessian trace of log-density, this observation can be interpreted as natural. It is implied that the first and second terms in the loss function of LCSS are dedicated to the formation of contours and texture, respectively.

## 5 Conclusion

Limitation.While LCSS, unlike DSM, can design SDEs flexibly without restricting them to affine forms, we used existing affine SDEs designed for use together with DSM, i.e., VE SDE and subVP SDE, in this work. Proposing more flexible SDEs leveraging LCSS is left for future work.

We proposed a local curvature smoothing with Stein's identity (LCSS), a regularized score matching method expressed in a simple form, enabling fast computation. We demonstrated LCSS's effectiveness in training on high-dimensional data and showed that LCSS-based SDMs enable high-resolution image generation. Currently, SDMs primarily rely on DSM, constraining the design of SDE. LCSS offers an alternative to DSM, opening avenues for SDM research based on more flexible SDEs.

    & & & &  \\  Case \# & Model capacity & Image resolution & Corresponding results & LCSS & DSM \\ 
1 & Large (NCSN++, DDPM++, etc.) & \(32 32\) & Figures. 4, 5 6 & ✓ & ✗ \\
2 & Small (NCSNv2) & \(256 256\) & Table. 3 & ✓ & ✓ \\   

Table 4: Simplified performance comparison between LCSS and DSM.

Figure 7: Generated samples on FFHQ (\(256 256\)) by the model trained with LCSS (ours) with different \(\). The notation _iter_ signifies the training iterations.