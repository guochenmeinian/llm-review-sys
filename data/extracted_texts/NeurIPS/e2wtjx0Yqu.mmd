# CLadder: Assessing Causal Reasoning in Language Models

Zhijing Jin1,2,1, Yuen Chen1,1,1, Felix Leeb1,1,1, Luigi Gresele1,1,

Ojasv Kamal3, Zhiheng Lyu4, Kevin Blin2, Fernando Gonzalez2, Max Kleiman-Weiner5,

**Mrinmaya Sachan2, Bernhard Scholkopf1**

1MPI for Intelligent Systems, Tubingen 2ETH Zurich 3IIT Kharagpur

1University of Hong Kong 5University of Washington

jinzhi@ethz.ch chenyuean0103@berkeley.edu

fleeb@tue.mpg.de luigi.gresele@tue.mpg.de

Main contributors.

###### Abstract

The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating _commonsense_ causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined _formal rules_. To address this, we propose a new NLP task, _causal inference in natural language_, inspired by the _"causal inference engine"_ postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associated, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insights into the causal reasoning abilities of LLMs.1

## 1 Introduction

_Once we really understand the logic behind causal thinking, we could emulate it on modern computers and create an "artificial scientist"_.

_-- Pearl and Mackenzie (2018)_

Causal reasoning is believed to be one of the hallmarks of human intelligence . The ability to draw causal inferences from available information is crucial for scientific understanding and rational decision-making: for example, knowing whether smoking causes cancer might enable consumers to make a more informed decision ; assessing the causal effect of a vaccine is essential for effective policy-making during a pandemic ; and understanding the interplay behind family background, education and income helps devise effective education policies .

Our opening quote therefore mirrors the aspirations of many scientists in artificial intelligence and causal inference: to construct a machine capable of performing sound causal reasoning, and able to answer causal questions at scale and with ease. Recent advances in large language models (LLMs) have brought about a paradigm shift in natural language processing (NLP) and artificial intelligence [7, 15, 39, 56, 76, 103, _inter alia_]. These transformative developments raise the question of whether these machines are already capable of causal reasoning: _Do LLMs understand causality?_Many previous works addressed the above question by focusing on _commonsense_ causality , inspired by the literature that explores LLMs as _knowledge bases_ (we refer to this line of work as _causality as knowledge_). This involves assessing the alignment between commonsense knowledge about causal relationships in humans and LLMs. This line of work generally does not focus on evaluating how well models are capable of _causal reasoning_. For example, it may be difficult to rule out the possibility that LLMs perform potentially unreliable _amortized causal inference_, answering causal questions by a simple repetition of verbal patterns present in the texts composing their training data:2; 3 in other words, LLMs may just be _"causal parrots"_.

In this work, we introduce a way to test the _formal causal reasoning in LLMs_. To this end, we introduce the CLadder dataset. The specificity of CLadder is that causal questions posed in natural language are _grounded in symbolic questions and ground truth answers_: the latter are derived through an oracle _causal inference engine (CI engine)_, which abides by the rules of the causal inference approach described by Pearl , based on graphical models and structural causal models (SCMs) . We compose more than 10,000 causal questions that cover a variety of causal queries across the three rungs of the _Ladder of Causation_--i.e., _associatedal (Rung 1)_, _interventional (Rung 2)_, and _counterfactual (Rung 3)_. We consider several causal graphs, giving rise to scenarios which require different causal inference abilities. Additionally, we generate ground-truth explanations with step-by-step reasoning for more in-depth analysis of LLM behavior. Our symbolic questions and answers are then _verbalized_, by turning them into stories which can be expressed in natural language. To probe whether LLMs employ amortized causal inference, we construct stories with commonsensical, as well as anti-commonsensical and with nonsensical causal relations: in these latter cases, amortized causal inference is expected to fail, whereas formal causal reasoning would still yield the correct answer. An example question from CLadder is shown in Figure 1.

Exploiting CLadder, we also introduce a method to elicit sound causal reasoning in LLMs and help them solve challenging causality questions. Specifically, we develop \(\), a chain-of-thought prompting strategy  inspired by the CI engine, which prompts the LLM to extract the causal graph, causal query, and available "data" (e.g., conditional or interventional _do_-probabilities ) from the question, formalize them precisely, and perform correct causal inferences.

Figure 1: Example question in our CLadder dataset featuring an instance of _Simpson’s paradox_. We generate the following (symbolic) triple: (i) the causal query; (ii) the ground-truth answer, derived through a _causal inference engine_; and (iii) a step-by-step explanation. We then _verbalize_ these questions by turning them into stories, inspired by examples from the causality literature, which can be expressed in natural language.

Our experiments indicate that CausalCoT achieves an accuracy of 70.40%, which substantially improves the performance of vanilla GPT-4 by 8.37 points on CLadder.

We summarize the _main contributions_ of our work:

1. In contrast to most other works on causality in LLMs, focusing on _commonsense causal knowledge_, our goal is to assess the LLMs' ability to perform _formal causal reasoning_ (briefly reviewed in Section 2).
2. We introduce CLadder (Section 3), a dataset containing more than 10K causal questions, spanning all three rungs of the ladder of causation, several causal graphs, and various stories for verbalization.
3. We develop CausalCoT (Section 4), a chain-of-thought prompting strategy to elicit formal causal reasoning in LLMs, inspired by the _causal inference engine_.
4. We perform extensive experiments on eight LLMs (Section 5), analyze fine-grained errors to showcase the limitations of LLMs in formal causal reasoning, and suggest directions for future research.

## 2 Preliminaries on Causal Inference

Our dataset design takes inspiration from the _Causal Inference Engine_ as postulated by Pearl and Mackenzie , see also . We begin with a brief overview of the causality framework by Pearl et al. .4 This framework was largely developed within the field of artificial intelligence, and therefore puts particular emphasis on _algorithmic_ aspects of causal reasoning (e.g., )--which makes it particularly suited for our work, where we want to algorithmically generate ground truth answers to causal queries, without having to appeal to common sense to assess the correctness of an answer.

### The Ladder of Causation

The _Ladder of Causation_, introduced by Pearl and Mackenzie , is a proposed taxonomy, and hierarchy, of causal inference tasks . It consists of three distinct rungs.

Rung 1 ("seeing").This describes statistical associations ("How often do I take an aspirin when I have a headache?"). Rung 1 deals with statistical dependences among random variables, and involves probabilistic reasoning about joint and conditional distributions, \(P(X=x,Y=y)\) and \(P(Y=y|X=x)\), which can be formalised through _Bayesian Networks_ representing a set of variables and their conditional dependencies via a directed acyclic graph (DAG).

Rung 2 ("doing").This enables us to formalize the concept of actively intervening in the world, and modifying it toward some end ("If I take an aspirin now, will my headache subside?"). Interventions can be formalized using the _do-operator_ and _Causal Bayesian Networks_ to represent, for example, the distribution over \(Y\) when intervening on \(X\) to set its value to \(x\) as \(P(Y=y|(X=x))\).

Rung 3 ("imagining").This rung deals with counterfactual reasoning, i.e., reasoning about alternative scenarios in which the world could have been different, possibly even contradicting the factual state ("_Would my headache have subsided, if I had taken an aspirin?")._ Counterfactual probabilities can be written as \(P(Y_{x}=y)\), representing the probability that "\(Y\) would be \(y\), had \(X\) been \(x\)". Reasoning about Rung 3 quantities requires the introduction of _Structural Causal Models (SCMs)_. SCMs are especially powerful as they enable any quantity in Rungs 1, 2, and 3 to be formulated precisely .

### Causal Inference

Identification.Causal inference is especially difficult since we typically only have measurements from _lower_ rungs, but want to reason about _higher_ ones. A crucial question is then under what conditions are such inferences possible, i.e., what assumptions and measurements are required to unambiguously answer a causal query of interest: this is the question of _identification_. As argued in , "_it is generically impossible to draw higher-layer inferences using only lower-layer information"_. One may be able to draw inferences at a higher layer given a combination of partial knowledge of the underlying SCM, in the form of a causal graph, and data at lower layers. The graphical structure therefore plays a crucial role in bridging the rungs of the Ladder of Causation, and many prior works have been dedicated to exploiting properties of the graph to transform higher-rung queries into expressions which can be estimated based on lower-rung quantities .

Causal Inference Engine.An overarching objective of this research is the construction of a _Causal Inference Engine (CI Engine)_, which takes as input a query, a graph, and some available data (typically from lower rungs than the query); and outputs whether a solution exists, and, if so, an equivalent expression of the query which is estimable from the available data. While some previous works refer to the CI engine in the context of Rung 2 queries, where it corresponds to the _do_-calculus , here we refer to it in a more general sense, encompassing all three rungs.

## 3 Composing the CLadder Dataset

Task Formulation.Like in the example of Figure 1, our dataset \(:=\{(_{i},_{i},_{i})\}_{i=1}^{N}\) consists of \(N\) triples, each containing a question \(_{i}\), binary answer \(a_{i}\{,\}\), and an explanation \(_{i}\). Our main task is to test the accuracy of the prediction function \(f: a\), i.e., a LLM which maps a natural language causal question to an answer. Apart from directly evaluating the answer, we also compose the ground-truth explanations \(\) to evaluate the reasoning steps of LLMs.

Design Principles.In the composition of our dataset, we adhere to the following design principles. First, we ensure broad coverage of all rungs of the ladder of causation. Second, we avoid settings that involve continuous variables and use binary variables instead: this is partly due to the large availability of identifiability results for binary and categorical variables, and partly because queries involving binary variables lend themselves to more natural-sounding verbalization. Moreover, since LLMs struggle with calculation-heavy tasks , and we are chiefly interested in causal reasoning abilities, we focus on graphs with few (three to four) variables, in various common configurations, to produce questions which are identifiable from the outset. Lastly, we carefully design a rich set of templates to translate the abstract formulas into grammatically correct and natural-sounding, fluent prompts.

Overall Pipeline.The generation pipeline for CLadder, depicted in Figure 2, consists of two parts:

1. In the _Formal Part_ (which we illustrate in Section 3.1), we specify all the required inputs (query, model, data) and the ground truth answer generated by the CI Engine.

Figure 2: The data-generating process of the CLadder dataset. The upper part of the figure describes the _formal part_ of the question generation, which samples inputs for the CI Engine and derives a ground truth answer. The bottom part describes the _natural language part_ of the question generation—i.e., its verbalization, based on multiple stories and different degrees of alignment with commonsense knowledge.

2. In the _Natural Language Part_ (in Section 3.2), we verbalize the formal queries and specification of the causal model and data by associating them to a story or narrative, using a rich set of templates.

### Formal Part of the Question Formulation

The first step of our data generating process is to construct a set of inputs to the CI Engine such that _by design_ there exists a well-defined ground truth answer: i.e., we construct triples of causal queries, graphs, and data such that the query can be unambiguously answered based on the available data (ensuring _identifiability_ by construction).5 The ground truth causal models, which specify all quantities which are considered measurable in our questions, are causal Bayesian networks (CBNs), where each causal mechanism (i.e., conditional probability of a variable given its parents in the factorization according to the causal graph \(G\)) corresponds to a Bernoulli distribution. We compile a selection of graphs \(G\) based on examples drawn from multiple sources from the literature [66; 67; 69; 88], where suitable graph structures are used to illustrate toy problems in causal inference. The complete list of structures we consider can be found in Appendix A.3; the complete list of sources in Appendix A.1.

Selecting Query Types.We again draw from the causal inference literature to collect common query types in each rung. As illustrated in the _"Sample a query type"_ box in Figure 2, for Rung 1, we can ask about probability distributions such as marginal probabilities and conditional probabilities. For Rung 2 questions, we can enquire _average treatment effects (ATE)_ (_"how will \(Y\) change if \(X\) changes from \(x\) to \(x^{}\)?"_), or what constitutes a valid adjustment set that can block all backdoor spurious correlations between \(X\) and \(Y\). Lastly, for Rung 3, we include _counterfactuals_ (_"what would happen to \(Y\) had \(X\) been \(x^{}\) instead of \(x\)?"_), _average treatment effect on the treated (ATT)_ (_"for the subpopulation whose \(X\) changed from \(x\) to \(x^{}\), how does their \(Y\) change on average?"_), _natural direct effect (NDE)_ (_"what is the direct effect of \(X\) in \(Y\), but not through the mediator?"_), and _natural indirect effect (NIE)_ (_"what is the effect from \(X\) to \(Y\) through the mediator?"_).

Applying the Causal Inference Engine for the Ground-truth answer.By construction, the causal processes we define encapsulates all necessary information to make the causal quantities of the query types identifiable. This allows us to apply the rules of causal inference to obtain an estimand for each causal graph and query type, and evaluate the estimand to get a ground truth answer. The Rung 2 queries simplify to Rung 1 terms using the rules of _do_-calculus , and, for the Rung 3 queries, we apply methods of counterfactual causal inference  (with details in Appendix C.3). The estimand also specifies exactly which terms are necessary to include in the prompt as _"available data"_ in order to ensure that enough information is provided to answer the question correctly (i.e., for identifiability), provided the correct causal reasoning is applied. Our entire code base of the data generation process can be found at our GitHub repository, https://github.com/causalNLP/cladder.

### Natural Language Part of the Question Formulation

While Section 3.1 describes a way to generate the ground-truth causal model, query and answers, computed through a causal inference engine, real-world causal reasoning problems are expressed in natural language rather than symbolic expressions. The next part of the data generation pipeline therefore focuses on the verbalization of all these components with a plausible narrative in natural language.

Generating the Stories.For each causal graph, we collect a set of two to five _stories_ which consist of a list of variable names for each node in the graph. The stories are primarily selected from examples in commonly cited causal inference books and papers (see Appendix A.1), which ensures that the stories and corresponding causal graph structures adhere to empirical common sense (e.g., the drug-gender-recovery example of Pearl and Mackenzie ). However, it is very likely that at least some of the stories appear in the training data of many LLMs. Therefore, we also generate various _anti-common sense_ and _nonsensical_ variants of the stories, meant to isolate the effects of memorization. For the anti-commonsensical stories, we randomly do one of the actions: (1) replace the effect variable \(Y\) with an unusual attribute, that would not be an effect variable in any of the stories (e.g., "ear shape"); or (2) create an irrelevant treatment variable \(X\) that does not play a causal role in any of our commonsensical stories, such as "playing card games" (see Appendix A.7). For the nonsensical variants, we invent artificial words as variable names such as "zory" and "qixy" (see Appendix A.6)..

Verbalizing the Prompts.The verbalization procedure applies the mapping of symbolic variables to semantic concepts to form a plausible narrative for the underlying causal process and then translates the symbolic expressions from the underlying causal process to natural language using carefully designed templates.

Specifically, we use several different grammatical forms for each semantic concept \(\) in the story to make the resulting prompt sound natural and grammatically correct. We first have the overall variable name \(v_{}()\) (e.g., the recovery status), and, then, for each binary value \(i\{0,1\}\), we compose its noun \(v_{}(=i)\) (e.g., recovery), verb (e.g., to recover), sentence \(v_{}(=i)\) (e.g., the patients recover), noun with attributive clause \(v_{}(=i)\) (e.g., patients who recover), and third conditional \(v_{}(=i)\) (e.g., if the patient had recovered).

Using these elements, we first verbalize the causal graph by iterating through each node and its outgoing edges, using the template "\(\) has a direct effect on \(()\).", where \(()\) denotes the set of direct effects (children) of a variable. Then, for the available data \(\), we verbalize each conditional probability by "For \(v_{}(_{m}=i)\), the probability of \(v_{}(_{n}=1)\) is \(p\).", and each marginal probability by "The overall probability of \(v_{}(=1)\) is \(p\)." Note that our distributions are Bernoulli, so it is adequate to just introduce the parameter \(p\), which is the likelihood of \(=1\). For example, we generate sentences such as "The overall probability of recovery is 60%." and "For patients who have small kidney stones, the probability of recovery is 70%." Finally, for the query \(\), we instantiate each query type in our dataset following our question templates in Appendix A.5 such that the questions can always be answered with "yes" or "no".

Generating the Explanations.Apart from the question-answer pairs, we also generate the step-by-step explanations. Our goal is to provide all intermediate reasoning steps a student of causal inference would use to answer the questions, so that each necessary subskill necessary for causal inference can be evaluated individually. We identify the following six subskills: 1 causal graph extraction; 2 correct query type interpretation; 3 symbolic formalization of the query; 4 semantic parsing to compile the available data; 5 estimand derivation; and 6 arithmetic calculation to solve the estimand, as in the colored boxes in Figure 1. Our explanation \(\) verbalizes all the elements 1-6 as sequential steps using our template in Appendix A.8.

### Dataset Statistics

Our data-generating procedure has the potential to algorithmically generate a vast large number of questions. In practice, we pick a dataset size that is large enough to be representative, and at the same time not too large to be problematic given the expensive inference costs of LLMs. We therefore set our dataset size to be 10K, and report the statistics in Table 1.

The dataset roughly balance across the query types, graph structures, stories, and ground truth answers (as seen in Figure 3). Note that some causal queries are only compatible with a subset of the graphs, thereby resulting in a slightly lower representation of those queries (such as the NDE and NIE). More details on our design choices can be found in Appendix A.4.

### Data Quality Check

Our dataset is generated through an algorithmic procedure, which has the following potential benefits: formal correctness; zero human annotation cost; and, most importantly, controllability--e.g., for

    & Total & Rung 1 & Rung 2 & Rung 3 \\  Size & & & & \\ \# Samples & 10,112 & 3,160 & 3,160 & 3,792 \\ Question & & & & \\ \# Sentences/Sample & 6.01 & 5.88 & 5.37 & 6.65 \\ \# Words/Sample & 80.9 & 73.43 & 76.95 & 90.42 \\ \# Nodes/Graph & 3.52 & 3.5 & 3.5 & 3.54 \\ \# Edges/Graph & 3.38 & 3.3 & 3.3 & 3.5 \\ Answer & & & & \\ Positive Class (\%) & 50 & 50 & 50 & 50 \\ Explanations & 9.11 & 9.1 & 8.1 & 9.96 \\ \# Words/Sample & 47.95 & 49.87 & 32.8 & 58.97 \\   

Table 1: Statistics of our CLadder dataset v1.5.

the question distribution, as well as for making it more unlikely that the data was previously seen by the model. However, since the dataset is different from common NLP datasets collected from human natural language writing, we also need to perform additional data quality checks. We therefore checked for a list of non-formal, natural language properties: grammaticality; human readability; naturalness/perplexity; and how well humans perform on this task.

For grammaticality, we ran a grammatical error check on our dataset using the LanguageTool package , and got on average 1.26 grammatical errors per 100 words (i.e., 98.74% correctness), which shows that most of the language in our dataset follows English grammar. For human readability, we checked how comprehensible the questions are to students who have taken causality courses. We selected a random subset of 50 questions from the dataset, and let a graduate student annotator go through the questions to judge whether they could understand them or not: 96% of the questions were deemed readable. Next, for the naturalness/perplexity score, we used the open-sourced GPT-2 model and obtained a perplexity score of 21.17 on our dataset, which is substantially lower (i.e., closer to the distribution of natural human-written text) than the one of MATH , a commonly used dataset of maths questions. Lastly, we conducted a sanity check where one expert evaluator tried to solve a random sample of 50 questions from the dataset, and we recorded an accuracy of 82% on this task.

## 4 Our CausalCoT Model

In order to guide LLMs in correctly answering the questions in CLadder, we draw inspiration from the ideal functioning of the CI engine , which breaks down a causal reasoning problem into multiple symbolically-grounded, simpler steps. We develop CausalCoT, a multi-step causal chain-of-thought prompt in Figure 4, which combines formal causal reasoning skills with the idea of chain-of-thought prompting  and the use of scratch pads for solving more complicated problems requiring a long list of steps  for LLMs.

We base our prompt design on the multi-step reasoning process of causal inference as shown in Figure 4, first starting with four preparation steps: 1 identifying the causal graph structure; 2 determining the causal query type;6 formulating the query symbolically precisely; and 4 extracting relevant data from the prompt. Then, given all the information collected in the preparation stage, we introduce the formal solution: 3 correctly deducing the estimated using causal inference techniques; and finally 4 evaluating the estimand to answer the question. This set of steps require both _natural language understanding_ to parse the question (as in most steps in the preparation phase), as well as _formal causal reasoning_ to derive the correct estimand (as in the solution phase).

Figure 4: Illustration of our CausalCoT prompting strategy, which designs a chain of subquestions inspired by the idea of a CI engine .

We build our CausalCoT prompting strategy using GPT-4 , a recent autoregressive LLM that achieves state-of-the-art performance on many tasks. This latest model builds upon the previous series of general pretrained models (GPT) [7; 76] and adds reinforcement learning with human feedback, or instruction-tuning [1; 57; 104], to align the model responses to free-form questions with human preferences. It has achieved human-competitive performance over a list of tasks [8; 43; 54; 105], among which the more formal tasks unseen in the training data still remain elusive [42; 78; 91].

Given a causal question \(\), we provide the LLM a list of instructions \(:=(_{1},,_{6})\) consisting of the detailed descriptions of the six steps \(_{1},,_{6}\) in Figure 4. As the model \(f_{}:_{i}_{i}\) autoregressively produces responses \(_{1},,_{6}\) sequentially corresponding to the six steps, we concatenate all the above before asking the final question "Based on all the reasoning above, output one word to answer the initial question with just "Yes' or 'No'." See the complete prompt in Appendix B.1. In the end, we obtain the binary answer \(a\{,\}\) as the final result.

Compared with the standard strategy of directly prompting the LLMs a question, we impose an _inductive bias_ upon LLMs by using the causal inference framework, thus incorporating some of the powerful, principled insights of the causal inference community for NLP tasks. In this way, we enhance the strong natural language ability of LLMs with formal causal reasoning skills.

## 5 Testing LLMs with CLadder

### Experimental Setup

Our empirical investigation focuses on some of the most recent language models. We include the latest GPT-4  with 1T parameters by the time we conduct the experiments (i.e., gpt-4-1106-preview), the previous ChatGPT (i.e., GPT-3.5) with 175B parameters, and then a series of earlier models with instruction-tuning on the 175B GPT-3 (text-davinci-001, -002, and -003) . As baselines, we also include the non-instruction-tuned GPT-3 (davinci). We use the OpenAI API with temperature 0 when querying these models. We also include open-source, more efficient models like LLaMa  and its instruction-tuned version Alpaca , both with the same number of parameters, 6.7B.

### Main Results

We compare the performance of all models in Table 2. First, we can see that the causal reasoning task in CLadder is in general very challenging for all models. Models such as the earlier, non-instruction-tuned GPT-3, and both LLaMa and Alpaca are around random performance. With instruction-tuning, models start to show some improvement. And amongst all, our CausalCoT achieves the highest performance of 70.40%, which is substantially better than the vanilla GPT-4 by 8.37 points. Moreover, CausalCoT also achieve the best performance across all three rungs of causal questions, with a monotonically decreasing performance as the rungs get higher, i.e., the questions get more difficult. See Appendix D for experiments on our earlier dataset v1.0.

### Isolating the Effect of Data Contamination

A well-known problem with evaluating LLMs on question-answering tasks is the data contamination problem, i.e., that LLMs perform well on a test set because the test set is (unintentionally) contained partially or even entirely in the training data [7; 56]. We address this problem by creating not only the commonsensical subset of our dataset, but also anti-commonsensical and nonsensical, both of which,

    & Overall Acc. &  &  \\  & & 1 & 2 & 3 & Comm. & Nonsens. & Anti-C. \\  Random & 49.27 & 50.28 & 48.40 & 49.12 & 49.01 & 49.69 & 49.12 \\ LLaMa & 44.03 & 48.23 & 29.46 & 52.66 & 45.14 & 44.22 & 42.67 \\ Alpaca & 44.66 & 52.03 & 29.53 & 51.13 & 44.86 & 44.40 & 44.77 \\ GPT-3 Non-Instr. (davinci) & 49.92 & 50.00 & 49.75 & 50.00 & 49.06 & 49.97 & 50.72 \\ GPT-3 Instr. (text-davinci-001) & 51.40 & 51.30 & 52.63 & 50.47 & 54.31 & 50.13 & 50.05 \\ GPT-3 Instr. (text-davinci-002) & 53.15 & 50.85 & 56.96 & 51.90 & 55.33 & 52.47 & 51.81 \\ GPT-3 Instr. (text-davinci-003) & 56.26 & 51.11 & 62.97 & 54.96 & 56.83 & 54.79 & 57.49 \\ GPT-3.5 & 52.18 & 51.80 & 54.78 & 50.32 & 54.09 & 50.68 & 52.09 \\ GPT-4 & 62.03 & 63.01 & 62.82 & 60.55 & 62.27 & 63.09 & 60.47 \\ + CausalCoT & **70.40** & **83.35** & **67.47** & **62.05** & **69.25** & **71.58** & **70.12** \\   

Table 2: Performance of all models on our CLadder dataset v1.5. We report the overall accuracy (Acc.), and also fine-grained accuracy by rung, and by degree of commonsense alignment, from commonsensical (Comm.), nonsensical (Nonsens.), to anti-commonsensical (Anti-C.).

by construction, are very likely not in the training data of LLMs. From the accuracy by commonsense alignment degree in Table 2, we can see the original GPT-4 model performs the worst on the anti-commonsensical subset (1.8 points lower than that on the commonsensical subset). However, our CausalCoT enhances the reasoning ability across all levels, with substantial improvement on anti-commonsensical data by 9.65 points, highlighting the strength of CausalCoT on unseen data.

### Error Analysis by Subquestions

We conduct a fine-grained error analysis by looking into the performance of different steps of CausalCoT in Table 3.7 We can see that the model is good at Step 1 to extract causal graph \(\), achieving high F1 scores for predicting both the nodes and the edges correctly, although not perfect, still leaving a graph edit distance of 1.69 between the ground truth causal graph and the model-identified graph. The other steps are more challenging for the model. Among those, Steps 2, 3 and 4 require careful and correct application of causal inference, where the model struggles. This reveals a notable weakness of current LLMs to perform formal causal reasoning, which is an important direction for future work on improving and enhancing LLMs. To better understand the reasoning abilities of LLMs, we also perform an extensive analysis taking the entire reasoning chain of our CausalCoT and the ground-truth explanations, to produce 20 fine-grained scores about the multi-step reasoning quality using the ROSCOE framework , and show detailed results in Appendix E.2.

### Effect of In-Context Learning

As an additional analysis, we look into the effect of in-context learning (ICL) by providing an example solution before asking the question. The interesting question to us is whether models can generalize across different query types. Namely, we keep our CausalCoT framework, and prepend a reasoning example of query type \(i\), and then calculate how much improvement it can bring when models answer new questions of query type \(j\). In Figure 5, we can see that conditional probability and NIE are the questions that benefit the most from ICL, and showing examples of marginal probability and ATT are among the most helpful to all questions in general.

## 6 Related Work

Skill evaluation for LLMs.Our work may be seen as part of the literature aimed at evaluating the performance of current LLMs [7; 15; 56; 76; 103; _inter alia_], focusing on understanding their strengths and weaknesses. Various studies into the capabilities of LLMs [8; 39; 56; 74] change people's perception of domains such as education [2; 80], medicine [54; 87], law , and computational social science . However, most work evaluates new models on existing datasets from previously-curated large-scale benchmarks [89; 94; 95], or human exams [41; 43; 56] which is becoming increasingly unreliable due to training set contamination.

Causality-related skills for NLP.With the increasing attention on LLMs and causality [100; 101], we review several formulations of causality-related skills for NLP, which we summarize into (1) causality as knowledge, (2) causality as language comprehension, and (3) causality as reasoning. In the _causality-as-knowledge_ line of work, many existing studies investigate how well NLP models understand commonsense causality, such as the cause and effect of an agent's action , motivation and emotional reaction in a social context , correspondence of a set of steps with a high-level goal , development of a story given a different beginning , and how in general LLMs serve as a knowledge base of causality . Concurrent work  focuses on evaluating LLMs on various causality related tasks by leveraging the conceptual knowledge accrued from the training

    & Step & & & Step & & & Step & & Step & & Step \\  Node & Edge & Dist. (\(\)) & Overall & F1 & Rung & 1 & Rung & 2 & Rung & 3 & Estimand & F1 & Arithmetic \\ 
99.34 & 97.01 & 1.69 & 50.65 & 69.99 & 59.14 & 42.12 & 53 & 47.53 & 99 \\   

Table 3: Performance for each step in CausalCoT. For Step, we report the F1 score of node prediction, edge prediction, and also the graph edit distance (Dist.) with the true graph. See more details in Appendix E.1.

Figure 5: Heatmap showing the how helpful each query type is to solving subsequent query types.

data, rather than formal causal inference, except for their causal sufficiency analysis which is close to our counterfactual questions. Importantly, most work in this line does not define explicit causal graphs, making it difficult to quantitatively define the ground-truth causal relationships in a principled way. The _causality-as-language-comprehension_ line of work stems from traditional linguistic studies on causal connectives and causal language usage [90; 91; 92], to the recent causal relation extraction [4; 39; 99] to identify cause-effect pairs as a subtask of information extraction from text.

Finally, for _causality as formal reasoning_, our CLadder work formulates the task of causal inference for NLP, and our other work, Corr2Cause, addresses the causal discovery problem to infer causation from correlation. Together, they cover the two major branches of causal reasoning investigated in existing technical literature on causality. See a comprehensive comparison of literature in Appendix F.

## 7 Discussion of Limitations and Future Work

A Natural Language _"Mini Turing Test"_ for Causality.Pearl and Mackenzie  describe an ideal _"mini-Turing test"_ to assess understanding of causal inference, and argue that if a machine can answer all possible questions correctly, then it "understands" causality. According to the authors, this is because there are no possible shortcuts when you consider all possible combinations of queries, graphs and data in this ideal test: due to their combinatorial explosion, the machine can only answer all questions right if it correctly applies causal reasoning. From this point of view, our work constitutes a _first step towards a mini-Turing test formulated in natural language_. However, we cover only some of the commonly studied causal queries spanning all three rungs. Future work may extend this to further queries, such as, e.g., path-specific effects other than NDE and NIE , thereby increasing the number of potential questions and moving closer to the ideal test.

LLMs and Causal Reasoning.It has been claimed that LLMs understand causality well (e.g.,  report high performance, such as 97% and 92%). In contrast, our work suggests that LLMs may still be far from reasoning reliably about causality (reaching only 60+% on CLadder). As argued in Section 1, we believe that investigating this aspect may be of particular importance, since causal inference is crucial in many policy-relevant scenarios, where reliable AI systems could assist decision-making: from epidemiology [23; 70] to economics [10; 38] to fairness [48; 72]. Testing the abilities of these systems in semi-realistic scenarios is therefore crucial, motivating some of the design choices in our dataset: e.g., the example in Figure 1 was inspired by similar questions which arose in the context of the COVID-19 pandemic, where incorrect causal reasoning resulted in a fallacy where vaccinations were considered to be harmful instead of beneficial [21; 40]. Further work may be dedicated to making the questions and verbalizations even closer to realistic instances of causal inference problems.

A CI Engine Plug-in for LLMs.An interesting direction for future research could be to provide the LLM access to an actual implementation of the CI engine. For example, Davis and Aaronson  tested the improvement of math abilities in LLMs augmented with plug-ins (i.e., external modules that extend the model's capabilities by adding specific functionality or customizing its behaviour for particular tasks, like a calculator), suggesting that they significantly enhance the model's ability to solve these problems. However, even with plug-ins, there are still often _"interface"_ failures: that is, _"[the LLM] often has trouble formulating problems in a way that elicits useful answers from the plug-ins"_. We hypothesise that something similar would happen for causal inference: even once suitable plug-ins are built, the language-to-tool interface may still be a non-trivial research question.

## 8 Conclusion

We proposed formal causal reasoning as a new task to evaluate LLMs, and created the CLadder benchmark, covering several aspects of causal inference across all rungs of the ladder of causation and verbalizations involving semi-realistic scenarios. To address the task, we proposed a prompting strategy, CausalCoT, inspired by the principles of formal causal inference, which introduces multistep chain-of-thought reasoning for causal questions. Extensive experiments indicate that this dataset is highly challenging, thus offering a principled tool to gain a better understanding of the reasoning abilities of LLMs and to develop better models for causal reasoning in natural language.

## Author Contributions

The conceptualization and design of this project was led by Zhijing, Luigi and Felix, and supervised by Mrinnaya on the NLP part, and Bernhard on the causality part. Max provided timely insights from cognitive science on different types of causal tasks and on the project design. In the exploration stage, Ojay did substantial work on discovering causal fallacies in news and on Twitter, which, while not included in the current systematic way of generating causal inference questions, was a significant contribution in the course of the project and in comparing various task formulations.

As for the operationalization and programming, the dataset composition was mainly led by Yuen and Felix, together with daily discussions with Zhijing, and weekly discussions with Luigi. Zhiheng supported an important function of generating the backdoor adjustment set for a given causal graph with the treatment and effect variables. The experiments are mainly conducted by Zhijing and Fernando, with Kevin finishing the evaluation results using the ROSCOE package.