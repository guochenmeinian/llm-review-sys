# A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding

A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce _Interleaving **Layout and Text** in a **Large** Language Model (LayTextLLM)_ for document understanding. In particular, LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in Key Information Extraction (KIE) and Visual Question Answering (VQA). Comprehensive benchmark evaluations reveal significant improvements, with a 27.0% increase on KIE tasks and 24.1% on VQA tasks compared to previous state-of-the-art document understanding MLLMs, as well as a 15.5% improvement over other SOTA OCR-based LLMs on KIE tasks.

## 1 Introduction

Recent research has increasingly focused on applying Large Language Models (LLMs) [1; 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 13; 14; 15; 16; 17] to document-oriented Visual Question Answering (VQA) and Key Information Extraction (KIE) scenarios. Efforts to build a text-sensitive MultiModal Large Language Models (MLLMs) based on existing LLMs, particularly aimed at enhancing Visually Rich Document Understanding (VRDU), have made significant progress [12; 6; 18]. Although existing MLLMs show promising results in document understanding, they often encounter challenges related to image resolution. When the input image is of low resolution, it is too blurry to extract visual features effectively. Conversely, high-resolution images require additional computational resources to capture detailed textual information .

Concurrently, another line of research employs off-the-shelf OCR tools to extract text and spatial layouts, which are then combined with LLMs to address VRDU tasks. These approaches assume that _most valuable information for document comprehension can be derived from the text and its spatial layouts, viewing spatial layouts as "lightweight visual information"_. Following this premise, several studies [12; 20; 21; 22; 23] have explored various approaches that integrate spatial layouts with text for LLMs, achieving results that are competitive with, or even surpass, those of MLLMs.

The most natural method to incorporate layout information is by treating spatial layouts as tokens, which allows for the seamless interleaving of text and layout into a unified text sequence [20; 22; 23]. For example, Perot et al.  employ format such as "_HARRISBURG 78109"_ to represent OCR text and corresponding layout, where "_HARRISBURG_" is OCR text and "_78109"_ indicates the mean of the horizontal and vertical coordinates, respectively. Similarly, He et al.  use _"[x_min, y_min, x_max, y_max]"_ to represent layout information. These approaches can effectively take advantage of autoregressive characteristics of LLMs and is known as the _"coordinate-as-tokens"_ scheme . In contrast, DocLLM  explores interacting spatial layouts with text through a disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.

However, we argue that both of the previous approaches have limitations. As shown in Fig. 1, coordinate-as-tokens significantly increases the number of tokens. Additionally, to accurately comprehend coordinates and enhance zero-shot capabilities, this scheme often requires few-shot in-context demonstrations and large-scale language models, such as ChatGPT Davinci-003 (175B) , which exacerbates issues related to sequence length and GPU resource demands. Meanwhile, although DocLLM does not increase sequence length and integrates spatial layouts through attention, its generalizability is limited. We believe that spatial cross attention and masked span tasks in DocLLM cannot fully utilize the autoregressive traits of LLMs.

To address these problems, this paper explores a simple yet effective approach to enhance the interaction between spatial layouts and text -- _Interleaving **Layout and Text** in a **Large Language Model** (LayTextLLM)**_ for document understanding. Adhering to the common practice of interleaving any modality with text [15; 24; 25], we specifically apply this principle to spatial layouts. In particular, we maps each bounding box to a single embedding, which is then interleaved with its corresponding text. Then we propose a tailored pre-training task--Layout-aware Next Token Prediction--a completely self-supervised task that enhances the alignment between layout and textual modalities without using synthetic data. Finally, through the proposed Shuffled-OCR Supervised Fine-tuning, LayTextLLM significantly improves performance on downstream-related VQA and KIE tasks. As shown in Fig. 1, LayTextLLM significantly outperforms the 175B models, while only slightly increasing or even reducing the sequence length compared to DocLLM. Our contributions can be listed as follows:

* We propose LayTextLLM for document understanding. To the best of the authors' knowledge, this is the first work to employ a unified embedding approach (Sec. 3.1.1) that interleaves spatial layouts directly with textual data within a LLM. By representing each bounding box with one token, LayTextLLM efficiently addresses sequence length issues brought by coordinate-as-tokens while fully leveraging autoregressive traits for enhanced document understanding.
* We propose two tailored training tasks: (1) Layout-aware Next Token Prediction (Sec. 3.2.1), a completely self-supervised training task to enhance the alignment between layout and textual modality; (2) Shuffled-OCR Supervised Fine-tuning task (Sec. 3.2.2) to better elicit the model generalizability in downstream tasks.
* Comprehensive experimental results demonstrate quantitatively that LayTextLLM significantly outperforms previous state-of-the-art (SOTA) OCR-free MLLMs by a large margin in zero-shot scenarios, particularly in KIE tasks with an improvement of 27.0%. Additionally, we illustrate that LayTextLLM competes effectively or even surpasses previous SOTA OCR-based methods in both zero-shot and SFT scenarios. Specifically, it surpasses DocLLM by 19.8% on VQA and 15.5% on KIE tasks (Sec. 4).
* Extensive ablations demonstrate the utility of the proposed component, with analysis showing that LayTextLLM not only improves performance but also reduces input sequence length compared to current OCR-based models.

Figure 1: The performance against input sequence length of different datasets across various OCR-based methods where data is from Tab. 2 and 5.

Related Work

### OCR-based LLMs for Document Understanding

Early document understanding methods [26; 27; 28; 29; 30] tend to solve the task in a two-stage manner, _i.e._, first reading texts from input document images using off-the-shelf OCR engines and then understanding the extracted texts. Considering the advantages of LLMs (_e.g._, high generalizability), some recent methods endeavor to combine LLMs with OCR-derived results to solve document understanding. For example, inspired by the "coordinate-as-tokens" scheme , He et al.  propose to use _"[x_min, y_min, x_max, y_max]"_ to introduce the layout information, which can fuse the layout information and texts into a unified text sequence and fully exploit the autoregressive merit of LLMs. To reinforce the layout information while avoiding increasing the number of tokens, DocLLM  designs a disentangled spatial attention mechanism to capture cross-alignment between text and layout modalities. Recently, LayoutLLM  utilizes the pre-trained layout-aware model , to insert the visual information, layout information and text information. However, the aforementioned methods neither suffer from the computational overhead leading by the increasing tokens or hardly take advantage of autoregressive characteristics of LLMs. Thus, it is an urgent problem to address how to better incorporate layout information without significantly increasing the number of tokens.

### OCR-free MLLMs for Document Understanding

Another approach to solve document understanding tasks is the OCR-free method. Benefiting from the end-to-end training framework, it involves processing the text content of documents directly, without relying on OCR engines. Donut  first presents an OCR-free method through mapping a text-rich document image into the desired answers. Pix2Struct  is trained to parse masked screenshots of web pages into simplified HTML, where variable resolution inputs are supported. While these approaches eliminate the need for OCR tools, they still necessitate task-specific fine-tuning. With the increasing popularity of LLMs/MLLMs [10; 11; 12; 13; 14; 15; 16; 17], various methods are proposed to solve the document understanding task through explicitly training models on visual text understanding datasets and fine-tuning them with instructions to perform a zero-shot prediction. LLaVAR  and UniDoc  are notable examples that expand upon the document-oriented VQA capabilities of LLaVA  by incorporating document-based tasks. These models pioneer the use of MLLMs for predicting texts and coordinates from document images, enabling the development of OCR-free document understanding methods. Additionally, DocPedia  operates document images in the frequency domain, allowing for higher input resolution without increasing the input sequence length. Recent advancements in this field, including mPLUG-DocOwl , Qwen-VL , and TextMonkey , leverage publicly available document-related VQA datasets to further enhance the document understanding capability. Although these OCR-free methods have exhibited their advantages, they still struggle with the high-resolution input to reserve more text-related details.

## 3 Method

In this section, we present our LayTextLLM. First, we introduce a innovative Spatial Layout Projector (Sec. 3.1.1) converts four-dimensional layout coordinates into a single-token embedding. To reduce parameter overhead, we apply Partial Low-Rank Adaptation (Sec. 3.1.2). We also introduce two specific training tasks: Layout-aware Next Token Prediction (Sec. 3.2.1) to align layouts with text during pre-training, and Shuffled-OCR Supervised Fine-tuning (Sec. 3.2.2) to enhance the generalizability of the model. An illustration of our approach is shown in Fig. 2.

### Model Architecture

LayTextLLM is built on the Llama2-7B-base model, which was originally designed to accept only text inputs [36; 37]. To enable the model to interleave spatial layouts with text, we introduce a novel Spatial Layout Projector. This projector converts OCR-derived coordinates into bounding box tokens. We also adopt the Partial Low-Rank Adaptation, a minimally invasive method to incorporate additional modalities while preserving the LLM's inherent knowledge intact.

#### 3.1.1 Spatial Layout Projector (SLP)

A key innovation in LayTextLLM is the Spatial Layout Projector (SLP), which transforms a spatial layout into a singular bounding box token. This enhancement enables the model to process both spatial layouts and textual inputs simultaneously. To be specifically, each OCR-derived spatial layout is represented by a bounding box defined by four-dimensional coordinates \([x_{1},y_{1},x_{2},y_{2}]\), these coordinates represent the normalized minimum and maximum horizontal and vertical extents of the box, respectively. The SLP maps these coordinates into a high-dimensional space that the language model can process as a single token. The process can be computed as \(z=W c+b\), where \(c^{4}\) is the vector of the bounding box coordinates. \(W^{d 4}\) is a weight matrix with \(d\) represents the dimension of the embedding, \(b^{d 1}\) is a bias vector, \(z\) is the resulting bounding box token represented as an \(d\)-dimensional embedding. As illustrated in Fig. 2, the resulting bounding box token \(z\) will be interleaved with corresponding textual embeddings to put into LLMs. Note that the SLP is shared by all bounding box tokens so very limited number of parameters are introduced.

Compared to the coordinate-as-tokens scheme, the SLP represents each bounding box with a single token. This approach significantly reduces the number of input tokens and adheres to the practice of interleaving any modality with text, effectively integrating layout and textual information into a unified sequence. This allows the model to process both modalities simultaneously and coherently, fully leveraging the autoregressive traits of LLMs.

#### 3.1.2 Layout Partial Low-Rank Adaptation

After using the SLP to generate bounding box tokens and a tokenizer to produce text tokens, these two modalities are then communicated using a Layout Partial Low-Rank Adaptation (P-LoRA) module in LLMs. P-LoRA, introduced in InterLM-XComposer2 , is originally used to adapt LLMs to visual modality. It applies plug-in low-rank modules specified to the visual tokens, which adds minimal parameters while preserving the LLMs inherent knowledge.

Formally, as shown in Fig. 3 for a linear layer in the LLM, the original weights \(W_{O}^{C_{out} C_{in}}\) and bias \(B_{O}^{C_{out}}\) are specified for input and output dimensions \(C_{in}\) and \(C_{out}\). P-LoRA modifies this setup by incorporating two additional matrices, \(W_{A}^{C_{r} C_{in}}\) and \(W_{B}^{C_{out} C_{r}}\). These matrices are lower-rank, with \(C_{r}\) being considerably smaller than both \(C_{in}\) and \(C_{out}\), and are specifically designed to interact with new modality tokens, which in our case are bounding box tokens. For example, given an input

Figure 3: The illustration of P-LoRA, adapted from .

Figure 2: An overview of LayTextLLM incorporates interleaving bounding box tokens (\(b^{i}\)) with text tokens (\(t^{i}\)), where the superscripts represent the sequence positions of the tokens.

\([x_{b},x_{t}]\) comprising of bounding box tokens (\(x_{b}\)) and textual tokens (\(x_{t}\)) is fed into the system, the forward process is as follows, where \(_{t},_{b}\) and \(\) are outputs:

\[_{t}&=W_{0}x_{t}+B_{0}\\ _{b}&=W_{0}x_{b}+W_{B}W_{A}x_{b}+B_{0}\\ &=[_{b},_{t}]\] (1)

### Training Procedure

LayTextLLM is trained with innovative layout-aware training procedure, which consists of two stages: Layout-aware Next Token Prediction pre-training and Shuffled-OCR Supervised Fine-tuning.

#### 3.2.1 Layout-aware Next Token Prediction

Inspired by the next token prediction commonly used in current LLM pre-training [1; 2; 3; 4; 5; 6; 7], we propose the Layout-aware Next Token Prediction (LNTP). Fig. 4 presents the contrast of the proposed Layout-aware Next Token Prediction and the conventional next token prediction task. The traditional next token prediction (Fig. 4(a)) relies solely on the textual content, predicting each subsequent token based on the prior sequence of tokens without considering their spatial layouts. Layout-aware next token prediction (Fig. 4(b)), however, interleaves the spatial information encoded by SLP (_i.e._, \(b^{i}\)) with the text tokens (_i.e._, \(t^{i}\)). This integration considers both the content and its layout within the document, leading to a richer, more precise understanding of both the structure and the content.

Similarly, primary objective of LNTP is to maximize the likelihood of its predictions for the next token. Thus the loss function is defined as

\[=-_{i=1}^{T} P(t^{i} t^{1},t^{2},, t^{i-1})\] (2)

where \(P(t^{i} t^{1},t^{2},,t^{i-1})\) represents the probability of \(i^{th}\) token \(t^{i}\) given the sequence of preceding tokens \(t^{1},t^{2},,t^{i-1}\), as predicted by the model. Note that we compute the loss only for text tokens, excluding bounding box tokens. During pre-training, our goal is to enhance the alignment between spatial layouts and textual modality, while preserving the LLM's inherent knowledge as much as possible. Thus, we freeze the LLMs and only update

Figure 4: Comparison of Layout-aware Next Token Prediction and normal Next Token Prediction.

Figure 5: Receipt layout example.

#### 3.2.2 Shuffled-OCR Supervised Fine-tuning

OCR engines typically process text from top to bottom and left to right. This order is also adopted as the input sequence for current OCR-based LLMs [19; 21]. However, modern LLMs often exhibit a strong inductive bias toward the positions of input tokens, influenced by designs such as Rotary Position Embeddings (RoPE) . Specifically, tokens that are close together in the input sequence are likely to receive higher attention scores, which is advantageous for processing standard text sequences. Such inductive bias brings cons and pros.

Consider the example illustrated in Fig. 5, where the OCR input text reads: _"... Change, 1.30, GST%, Ant(RM), GST(RM), Total(RM), SR, 6, 17.64, 1.06, 18.70... "_. If the question posed is _"What is the value of the field Change?"_ (highlighted in a blue box), the model easily identifies _"1.30"_ as it is closely positioned to the word _"Change"_ in the sequence. However, for a more challenging query like _"What is the value of the field Total(RM)?"_ (highlighted in a red box), the model struggles to determine the correct answer due to the presence of multiple subsequent numbers closed to _"Total(RM)"_. LayTextLLM integrates spatial layouts with textual data, reducing reliance on input sequence order. Thus, we posit that shuffling the OCR input order could enhance the resilience of LayTextLLM in discerning relevant information irrespective of token proximity in the sequence.

Specifically, we propose Shuffled-OCR Supervised Fine-tuning (SSFT) that randomly shuffles the order of OCR-derived text in a certain proportion of examples. The range of exploration for the shuffling ratio can be found in Tab. 7 and 20% shuffled ratio is applied. The training objective is equivalent to predicting the next tokens, but in this scenario, only the tokens of the response are used to compute loss. During SSFT, we unfreeze all parameters including those of LLMs. Experimental results in Section 4.6 demonstrate that utilizing SSFT can further enhance model performance, making it more robust to disruptions in input token order.

## 4 Experiments

### Datasets

Pre-training dataIn our training process, we exclusively use open-source data to facilitate replication. We collect data from two datasets for pre-training: (1) **IIT-CDIP Test Collection 1.0** and (2) **DocBank**. The IIT-CDIP Test Collection 1.0 comprises an extensive repository of more than 16 million document pages. DocBank consists of 500K documents, each presenting distinct layouts with a single page per document. For training efficiency, we choose to utilize the entire DocBank dataset and only subsample 5 million pages from the IIT-CDIP collection 1.0.

SFT dataFor document-oriented VQA, we select **Document Dense Description (DDD)** and **Layout-aware SFT** data used in Luo et al. , which are two synthetic datasets generated by GPT-4. Besides, **DocVQA**, **InfoVQA**, **ChartQA**, **VisualMRC** is included following . For KIE task, we select **SROIE**, **CORD**, **FUNSD**, **POIE** datasets following [12; 19; 21].

### Implementation Detail

The LLM component of LayTextLLM is initialized from the Llama2-7B-base , which is a widely-used backbone. Other parameters including SLP and P-LoRA are randomly initialized. During pre-training, the LLM is frozen, and the parameters of SLP and P-LoRA modules are updated. During SFT, all parameters are fine-tuned. Other detailed setup can be found in Appendix B.

We have configured the model with three versions of LayTextLLM for a side-by-side comparison under different settings. Aligned with Luo et al. , the first version, **LayTextLLM\({}_{zero}\)**, is trained exclusively with DDD and Layout-aware SFT data. Building upon this, and in alignment with the setting of Liu et al. , we introduce the DocVQA, InfoVQA, and ChartQA training sets to the dataset pool for our second version, termed **LayTextLLM\({}_{vqa}\)**. Finally, we incorporate a comprehensive suite of KIE datasets--FUNSD, CORD, POIE, SROIE, and VisualMRC--as described by Wang et al. , creating our most extensive version, **LayTextLLM\({}_{all}\)**. Note that all versions are based on the same pre-trained LayTextLLM weight.

### Baselines

OCR-free baselinesIn the category of OCR-free MLLMs, we have chosen the following SOTA models as our strong baselines due to their superior performance in both document-oriented VQA and KIE tasks. These include **UniDoc**, **DocPedia**, **Monkey**, **InternVL**, **InternLM-XComposer2**, **TextMonkey**, and **TextMonkey\({}_{+}\)**.

OCR-based baselinesFor OCR-based baseline models, we implemented a basic approach using only OCR-derived text as input. This was done using two versions: **Llama2-7B-base** and **Llama2-7B-chat**. We also adapted the coordinate-as-tokens scheme from He et al.  for these models, resulting in two new variants: **Llama2-7B-base\({}_{coor}\)** and **Llama2-7B-chat\({}_{coor}\)**. It's important to note that we did not employ the ICL strategy with these models, as it would significantly exceed their maximum sequence length constraints. Additionally, we included results from a stronger baseline using the ChatGPT Davinci-003 (175B) model , termed **Davinci-003-175B\({}_{coor}\)**. One other recent SOTA OCR-based approach, **DocLLM** is also considered in our analysis. Finally, **LayoutLLM** and **LayoutLLM\({}_{CoT}\)**, which integrates visual cues, text and layout is also included.

### Evaluation Metrics

To ensure a fair comparison with OCR-free methods, we adopted the accuracy metric, where a response from the model is considered correct if it fully captures the ground truth. This approach aligns with the evaluation criteria described by [9; 10; 12]. To further enhance the comparability with other OCR-based methods, we conducted additional evaluations using original metrics specific to certain datasets, such as F1 score [19; 23], ANLS [19; 21; 51] and CIDEr [19; 52].

    &  &  \\  & DocVQA & InfoVQA & Avg & FUNSD & SROIE & POIE & Avg \\ 
**Metric** &  \\ 
**OCR-free** &  \\ UniDoc  & 7.7 & 14.7 & 11.2 & 1.0 & 2.9 & 5.1 & 3.0 \\ DocPedia  & 47.1\({}^{*}\) & 15.2\({}^{*}\) & 31.2 & 29.9 & 21.4 & 39.9 & 30.4 \\ Monkey  & 50.1\({}^{*}\) & 25.8\({}^{*}\) & 38.0 & 24.1 & 41.9 & 19.9 & 28.6 \\ InternVL  & 28.7\({}^{*}\) & 23.6\({}^{*}\) & 26.2 & 6.5 & 26.4 & 25.9 & 19.6 \\ InterLM-XComposer2  & 39.7 & 28.6 & 34.2 & 15.3 & 34.2 & 49.3 & 32.9 \\ TextMonkey\({}_{12}\) & 64.3\({}^{*}\) & 28.2\({}^{*}\) & 46.3 & 32.3 & 47.0 & 27.9 & 35.7 \\ TextMonkey\({}_{+}\) & 66.7\({}^{*}\) & 28.6\({}^{*}\) & 47.7 & 42.9 & 46.2 & 32.0 & 40.4 \\ 
**text + polys** &  \\ 
**LayTextLLM\({}_{zero}\) (Ours)** & 71.8 & 33.8 & 52.8 & **49.4** & **86.7** & **66.1** & **67.4** \\
**LayTextLLM\({}_{coa}\) (Ours)** & **77.4\({}^{*}\)** & **66.1\({}^{*}\)** & **71.8** & 48.9 & 74.6 & **70.6** & 64.7 \\   

Table 1: Comparison with SOTA OCR-free MLLMs. \({}^{*}\) indicates the training set used.

    &  &  \\  & DocVQA & VisualMRC & Avg & FUNSD & CORD & SROIE & Avg \\ 
**Metric** &  &  \\ 
**Text** &  &  \\ 
**Text** &  & 34.0 & 182.7 & 108.3 & 25.6 & 51.9 & 43.4 & 40.3 \\ Llama2-7B-chat & 20.5 & 6.3 & 13.4 & 23.4 & 51.8 & 58.6 & 44.6 \\ 
**Text + Polys** & _{coor}\)_} & 8.4 & 3.8 & 6.1 & 6.0 & 46.4 & 34.7 & 29.0 \\ Llama2-7B-chat\({}_{coor}\) & 12.3 & 28.0 & 20.1 & 14.4 & 38.1 & 50.6 & 34.3 \\ Davinci-003-175B\({}_{coor}\) & - & - & - & - & 92.6 & 95.8 & - \\ DocLLM  & 69.5\({}^{*}\) & 264.1\({}^{*}\) & 166.8 & 51.8\({}^{*}\) & 67.4\({}^{*}\) & 91.9\({}^{*}\) & 70.3 \\ LayerTextLLM\({}_{exor}\) (Ours) & 65.4 & 260.7 & 163.0 & 48.6 & 74.5 & 86.4 & 69.8 \\ LayTextLLM\({}_{vqa}\) (Ours) & 75.7\({}^{*}\) & 260.2\({}^{*}\) & 168.0 & 52.7 & 70.9 & 78.6 & 67.4 \\ LayTextLLM\({}_{all}\) (Ours) & **77.3\({}^{*}\)** & **295.9\({}^{*}\)** & **186.6** & **64.2\({}^{*}\)** & **96.5\({}^{*}\)** & **95.8\({}^{*}\)** & **85.8** \\   

Table 2: Comparison with other OCR-based methods. \({}^{*}\) indicates the training set used.

### Quantitative Results

Comparison with SOTA OCR-free MethodsThe experimental results shown in Tab. 1 demonstrate the outstanding performance of the LayTextLLM series across various tasks. Note that the results for ChartQA are reported in Appendix E due to concerns about fairness in comparison, as the dataset does not include OCR-derived results and we used in-house OCR tools instead. Firstly, LayTextLLM\({}_{zero}\) significantly outperforms previous SOTA OCR-free methods, such as TextMonkey , in zero-shot capabilities, even when these methods use the training set of the dataset. For example, in the DocVQA and InfoVQA datasets, LayTextLLM\({}_{zero}\) achieves accuracies of 71.8% and 33.8%, respectively, which are markedly higher than existing OCR-free methods such as TextMonkey and InternLM-XComposer2. When fine-tuned with corresponding datasets, LayTextLLM shows even greater performance improvements, particularly in document-oriented VQA datasets. Specifically, its accuracies on DocVQA and InfoVQA increase to 77.4% and 66.1%, respectively, demonstrating the model's strong ability to leverage task-specific data. Additionally, LayTextLLM\({}_{zero}\) excels in KIE datasets, particularly on the SROIE and POIE datasets, achieving accuracies of 86.7% and 66.1%, respectively. These results significantly surpass those of previous SOTA OCR-free model (_i.e._, TextMonkey\({}_{+}\)) by margins of 40.5% and 34.1%, respectively. This significant performance gain is likely due to these datasets containing low-resolution images that are too blurred for current MLLMs to extract visual features, whereas LayTextLLM shows robustness in such challenging scenarios.

Comparison with SOTA OCR-based MethodsFor comprehensive comparison, we have also conducted correspinding experiments to align with OCR-based methods [19; 21]. The experimental results presented in Tab. 2 showcase significant performance improvements achieved by LayTextLLM models compared to pure OCR-based SOTA methods such as DocLLM . Specifically, when comparing with DocLLM, LayTextLLM\({}_{zero}\) demonstrates notably superior performance, with even its zero-shot capabilities being competitive with supervised SFT approaches. We believe that the subpar performance of DocLLM is likely due to its use of cross-attention and the masked span pre-training tasks , which fail to leverage the autoregressive features of LLMs effectively. Similarly, when contrasting with coordinate-as-tokens employed in Llama2-7B, LayTextLLM\({}_{zero}\) again outperforms significantly. This disparity in performance can be attributed to the following three reasons: (1) The coordinate-as-tokens approach tends to introduce an excessive number of tokens, often exceeding the pre-defined maximum length of Llama2-7B (_i.e._, 4096). Consequently, this leads to a lack of crucial OCR information, resulting in hallucination and subpar performance. (2) When re-implementing the coordinate-as-tokens method with Llama2-7B, we did not introduce the ICL strategy, as it would contribute additional length to the input sequence. (3) The coordinate-as-tokens approach necessitates a considerably larger-sized LLM to comprehend the numerical tokens effectively.

In comparison to LayoutLLM , our approach exhibits discrepant performance in different tasks, as shown in Tab. 3. In zero-shot scenarios, we outperform LayoutLLM in most KIE datasets, validating our capability to leverage OCR-based results effectively. However, we fall short on document-oriented VQA tasks since answering some questions that are strongly related to vision information may challenge our approach. Two main reasons may well explain this performance discrepancy:

    &  &  \\  & DocVQA & VisualMRC & Avg & FUNSD\({}^{-}\) & CORD\({}^{-}\) & SROIE\({}^{-}\) & Avg \\ 
**Metric** &  \\ 
**Visual + Text + Polys** &  \\ LayoutLLM  & 72.3 & - & - & 74.0 & - & - & - \\ LayoutLLM\({}_{CoT}\) & 74.2 & **55.7** & **64.9** & 79.9 & 63.1 & 72.1 & 71.7 \\ 
**Text** &  \\ Llama2-7B-chat & 20.5 & 9.9 & 15.2 & 15.1 & 20.0 & 35.6 & 23.5 \\ 
**Text + Polys** & _{corr}\)_} \\ Llama2-7B-chat\({}_{corr}\) & 8.4 & 6.7 & 7.5 & 4.3 & 33.0 & 47.2 & 28.1 \\ Llama2-7B-chat\({}_{corr}\) & 12.3 & 12.2 & 12.2 & 11.9 & 6.4 & 39.4 & 19.2 \\  LayTextLLM\({}_{zero}\)(**Ours**) & 65.4 & 36.2 & 50.8 & 71.0 & 66.9 & 89.2 & 75.7 \\ LayTextLLM\({}_{att}\)(**Ours**) & **77.3*** & 41.7*** & 59.5 & **81.3*** & **82.6*** & **96.2*** & **86.7** \\   

Table 3: Comparison with LayoutLLM. \({}^{-}\) indicates that the cleaned test set used in Luo et al. .

(1) The visual encoder in LayoutLLM provides additional visual information. (2) LayoutLLM incorporates the Chain-of-Thought (CoT) mechanism to model contextual information while it is not used in our approach. However, when fine-tuned with tailored data, LayTextLLM significantly outperforms LayoutLLM, showcasing its strong ability to utilize task-specific data. More qualitative example demonstrates can be found in Appendix A.

### Analysis

AblationsTo better assess the utility of Layout-aware Next Token Prediction and Shuffled-OCR Supervised Fine-tuning in LayTextLLM, an ablation study was performed (see Tab. 4). Details on the training setup for all variants are provided in Appendix B. It is evident that both LNTP and SSFT significantly enhance the utility of LayTextLLM. Specifically, disabling LNTP results in an 8% decrease in performance on VQA tasks and a 5.5% decrease on KIE tasks. Disabling SSFT leads to a decrease in average accuracy by 6.7% and 2.6% for VQA and KIE tasks, respectively.

Sequence LengthTab. 5 presents statistics on the average input sequence length across different datasets. Intriguingly, despite interleaving bounding box tokens, LayTextLLM consistently exhibits the shortest sequence length in three out of four datasets, even surpassing DocLLM, which is counterintuitive. We attribute this to the tokenizer mechanism. For example, using tokenizer.encode(), a single word from the OCR engine, like _"International"_ is encoded into a single ID \(\). Conversely, when the entire OCR output is processed as one sequence, such as _"... CPC,International,Inc..."_, the word _"International"_ is split into two IDs \(\), corresponding to _"Intern"_ and _"ational"_ respectively. This type of case occurs frequently, more discussion in Appendix C.

## 5 Limitation

Although LayTextLLM has shown significant capabilities in text-rich VQA and KIE tasks, this alone does not suffice for all real-world applications. There are some instances, particularly in chart analysis, where reasoning must be based solely on visual cues (_e.g._ size, color)--a challenge that remains unmet. Questions such as _"What is the difference between the highest and the lowest green bar?"_ illustrate this gap. The ChartQA results, detailed in Appendix E, also underscore these limitations. Addressing these challenges highlights the urgent need for future enhancements that integrate visual cue within the capabilities of LayTextLLM.

## 6 Conclusion

We propose LayTextLLM for various VRDU tasks, in which spatial layouts and textual data are seamlessly interleaved to make more accurate prediction by introducing a innovative Spatial Layout Projector. Two tailored training tasks -- Layout-aware Next Token Prediction and Shuffled-OCR Supervised Fine-tuning -- are designed to improve the comprehension of document layouts. Extensive experiments confirm the effectiveness of LayTextLLM.

    & &  &  \\  LNTP & SSFT & DocVQA & InfoVQA & VisualMRC & Avg & FUNSD & CORD & SROIE & POIE & Avg \\   & ✓ & 72.3 & 35.7 & 24.4 & 44.2 & 50.6 & 91.6 & 92.8 & 58.6 & 73.4 \\ ✓ & & 74.5 & 38.0 & 23.9 & 45.5 & 56.4 & 95.8 & 93.2 & 59.6 & 76.3 \\ ✓ & ✓ & **78.8** & **42.7** & **35.1** & **52.2** & **62.9** & **95.9** & **95.2** & **61.7** & **78.9** \\   

Table 4: Ablations on pre-training and SFT component of LayTextLLM (Accuracy).

   Dataset & LayTextLLM & DocLLM  & Coor-as-tokens  \\  DocVQA & **664.3** & 827.5 & 4085.7 \\ CORD & **137.9** & 153.2 & 607.3 \\ FUNSD & **701.9** & 847.5 & 4183.4 \\ SROIE & 529.2 & **505.1** & 1357.7 \\   

Table 5: Average sequence length of each data for different methods using Llama2 tokenizer.