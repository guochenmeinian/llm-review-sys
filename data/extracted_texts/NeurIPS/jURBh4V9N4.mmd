# An Expectation-Maximization Algorithm for Training Clean Diffusion Models from Corrupted Observations

An Expectation-Maximization Algorithm for Training Clean Diffusion Models from Corrupted Observations

Weimin Bai\({}^{1,2,3}\)  Yifei Wang\({}^{4}\)  Wenzheng Chen\({}^{5,6}\)  He Sun\({}^{1,2,3}\)

\({}^{1}\) Academy for Advanced Interdisciplinary Studies, Peking University

\({}^{2}\) College of Future Technology, Peking University

\({}^{3}\) National Biomedical Imaging Center, Peking University \({}^{4}\) Yuanpei College, Peking University

\({}^{5}\) Wangxuan Institue of Computer Technology, Peking University

\({}^{6}\) State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China

{weiminbai, wyf181030}@stu.pku.edu.cn, {wenzhengchen, hesun}@pku.edu.cn

Corresponding author

###### Abstract

Diffusion models excel in solving imaging inverse problems due to their ability to model complex image priors. However, their reliance on large, clean datasets for training limits their practical use where clean data is scarce. In this paper, we propose EMDiffusion, an expectation-maximization (EM) approach to train diffusion models from corrupted observations. Our method alternates between reconstructing clean images from corrupted data using a known diffusion model (E-step) and refining diffusion model weights based on these reconstructions (M-step). This iterative process leads the learned diffusion model to gradually converge to a local optimum, that is, to approximate the true clean data distribution. We validate our method through extensive experiments on diverse computational imaging tasks, including random inpainting, denoising, and deblurring, achieving new state-of-the-art performance. The code is available at https://github.com/ai4imaging/EMDiffusion.

## 1 Introduction

Diffusion models (DMs) [1; 2; 3] have demonstrated remarkable versatility in capturing complex real-world data distributions, excelling in diverse applications like image generation [4; 5; 6; 7; 8], audio synthesis , and molecular design . DMs approximate distributions by learning their score functions--the gradient of the log-likelihood of the data distribution \(_{} p_{data}()\). This enables high-quality sample generation by simulating reverse-time stochastic differential equations (SDEs)  during inference.

Recently, there has been growing interest in leveraging DMs as priors for computational imaging inverse problems [11; 12; 13; 14; 15; 16], which aim to recover underlying images \(\) from corrupted observations \(\). The Bayesian framework for computational imaging defines the posterior distribution of images \(\) given observations \(\):

\[p() p()p(),\] (1)

where \(p()\) defines the forward model of observations and \(p()\) defines an image prior. DMs offer efficient, data-driven priors that outperform traditional handcrafted priors prone to oversimplification and human biases, such as sparsity  or total variation (TV) [18; 19].

However, a major limitation of DM-based solvers is their reliance on substantial volumes of high-quality, clean signals for pre-training--a requirement often infeasible in real-world settings, especially for scientific and biomedical imaging. In contrast, corrupted noisy observations with differentiable forward models are easier to acquire, such as blurred images from mobile photography or 2D projections of 3D structures in X-ray computed tomography (CT) [(20; 21)] and cryogenic electron microscopy (cryo-EM) [(22; 23)]. Our paper seeks to answer a pivotal question: Can a DM be effectively trained to solve inverse problems primarily using large-scale corrupted observations? This presents a chicken-egg dilemma: training an accurate DM requires clean images, but reconstructing clean images from corrupted observations requires a good DM.

Utilizing the Expectation-Maximization (EM) framework, we introduce a novel approach called EMDiffusion. This approach initializes with a diffusion prior trained on a minimal set of clean images, then alternates between two steps across multiple iterations: reconstructing clean images from corrupted observations using the current diffusion prior (E-step), and refining the DM parameters based on these reconstructions (M-step). The sparse clean data provides a good initialization of the DM's manifold, preventing collapse into a distorted or biased distribution characterized solely by corrupted inputs. Each E-M iteration leverages the current diffusion prior to generate cleaner reconstructions from the corrupted data, and these enhanced reconstructions then update the DM, providing an improved prior for the next iteration. This cycle continues, with the generated samples and DM progressively converging toward local optima, which equals to approximate the true clean data distribution. The forward operator and noise process do not affect this type of convergence but only influence the convergence speed by determining the amount of information in the corrupted observations.

We validate the generalizability and effectiveness of EMDiffusion through extensive experiments, applying it to diverse imaging inverse problems across various datasets, including random inpainting, denoising, and deblurring, and achieving compelling results.

## 2 Related Works

Inverse problems in computational imaging.Computational imaging aims to reconstruct underlying signals \(^{d}\) from corrupted observations \(^{m}\), where the image formation process is probabilistically modeled as:

\[ p(|).\] (2)

Since \(m d\) and observation noise is inevitable, inverse problems in computational imaging are ill-posed, with the inverse mapping \(\) being one-to-many. To address this complexity, Bayesian inference introduces a prior distribution of underlying images, \(p()\), to constrain the solution space for the image posterior, \(p(|)\), as illustrated by Eq. 1. Employing Maximum a Posteriori (MAP) estimation, one can derive a point estimate of the underlying image by maximizing \( p(|)\). Alternatively, posterior image samples of reconstructed images can be obtained through methods like Markov Chain Monte Carlo (MCMC) [(24)] or Variational Inference (VI) [(25; 26; 27)]. However, the

Figure 1: **Overview of EMDiffusion. The paper proposes an expectation-maximization (EM) approach to jointly solve imaging inverse problems and train a diffusion model from corrupted observations. Left: In each E-step, we assume a known diffusion model and perform posterior sampling to reconstruct images from corrupted observations. In the M-step, we update the weights of the diffusion model based on these posterior samples. By iteratively alternating between these two steps, the diffusion model gradually learns the clean image distribution and generates high-quality posterior samples. Right: Raw observations and reconstructed clean images based on the diffusion model learned from corrupted data.**

performance of many computational imaging solvers is limited by their reliance on oversimplified, handcrafted priors such as sparsity and total variation (TV). These priors fail to capture the true complexity of natural image distributions, hindering the solvers' ability to achieve high-quality reconstructions.

Diffusion models for inverse problems.Diffusion models (DMs) (1; 2; 3) have recently emerged as powerful data-driven priors for solving imaging inverse problems. By mastering the intricate distribution of images through training on extensive image data, DMs facilitate both point estimates via Plug-and-Play (PnP) optimization [28; 29] and posterior sampling through generative PnP (GPnP) , PnP Monte Carlo (PMC) , or Diffusion Posterior Sampling (DPS) [13; 14; 32]. These approaches have demonstrated remarkable efficacy in addressing a broad spectrum of noisy inverse problems, with applications spanning diverse fields, including astronomy [11; 33] and biomedical imaging [15; 34].

Learn diffusion models from corrupted data.In many real-world scenarios, acquiring large-scale clean data is costly or infeasible, motivating efforts to learn DMs directly from corrupted data. Data corruptions stem from under-determined forward models (e.g., 2D projections, inpainting, compressed sensing) and measurement noise. Recent studies have explored various strategies to address these challenges. For instance, in inverse graphics, researchers integrate the forward model into the diffusion process and introduce a view-consistency loss over multiple noiseless projections of the same object to learn a 3D DM from 2D images [35; 36]. In image inpainting, AmbientDiffusion  randomly masks additional pixels and forces the DM to restore these deliberate corruptions. Since the model cannot distinguish between original and further corruptions, it effectively learns the uncorrupted image distribution. However, the AmbientDiffusion is limited by the additional masking technique and fails to achieve good performance with noisy observations.  cleverly finetunes Stable Diffusion (SD) to leverage the pre-trained knowledge in denoising tasks, but does not support training a DM from scratch. Meanwhile, SURE-Score  proposes to jointly learn an image denoiser and a score-based DM using Stein's unbiased risk estimate (SURE) loss, where the SURE loss acts as an implicit regularizer on the model weights. Despite its innovative approach, SURE-Score often struggles with significant data corruption, such as inpainting tasks with a large fraction of missing pixels, and tends to produce overly smooth results. A general approach for learning DMs from arbitrarily corrupted data remains an open challenge.

## 3 Preliminary

### Score-based Diffusion Models

A diffusion model captures the data distribution by learning a score function, i.e. the gradient of the logarithm of the likelihood of data distribution \(_{} p_{data}()\). Consequently, a diffusion model generates samples by gradually removing noise from a random input, which is equivalent to a reverse-time stochastic differential equation (SDE) - the solution to a forward-time SDE that gradually injects noise,

\[& _{t}=(_{t},t)t+g(t),\\ &_{t }=[(_{t},t)-g(t)^{2}_{_{ t}} p_{t}(_{t})]t+g(t)},\] (3)

where \(t[0,T]\), \((_{t},t):^{d}^ {d}\) is the drift function, \(g(t)\) controls the rate of the Brownian motion \(^{d}\), and \(}\) denotes the Brownian motion running back. A tractable isotropic Gaussian distribution is achieved when \(t=T\), i.e. \(_{T}(,)\), and the data distribution is achieved when \(t=0\), i.e. \(_{0} p_{data}\). \(_{t}^{d}\) denotes the image \(_{0}\) diffused at time \(t\). \(_{_{t}} p_{t}(_{t})\) is a time-dependent score function, which is usually approximated by a deep neural network, \(s_{}()\), parameterized by \(\). The generated data distribution from the reverse-time SDE depends only on this time-dependent score function.

### Diffusion Posterior Sampling

Many images are consistent with a single observation due to the ill-posed nature of the image formation model. By combining the forward model with the diffusion prior using Bayes' rule, we define a conditional diffusion process that samples the posterior distribution

\[_{t}=[(_{t},t)-g(t)^{2} _{_{t}} p_{t}(_{t}) ]t+g(t)},\] (4)

where the conditional score function can be further decomposed as:

\[_{_{t}} p_{t}(_{t })&=_{_{t}} p_{t}( _{t})+_{_{t}} p_{t}( _{t})\\ &_{^{*}}(_{t},t)+_{_{t}}_{_{0}}p(_{0})p(_{0} _{t})_{0},\] (5)

Since the likelihood function is only defined for \(t=0\), the dependence between \(\) and \(_{t}\) is implicit, making \(_{_{t}} p_{t}(_{t})\) an intractable integral at each diffusion step. Various techniques have been proposed to address this intractable likelihood function, including exactly computing the probability using an ODE flow (11), bounding the probability through an evidence lower bound (ELBO)(33), and approximating the probability using Tweedie's formula(13; 40; 41; 42). To ensure computational efficiency, we adopt the approximation proposed in (13),

\[p_{t}(_{t}) p( }_{0}(_{t})),}_{0}(_{t}):=[_{0} _{t}],\] (6)

for diffusion posterior sampling in all the following sections.

### Expectation Maximum Algorithm

The Expectation-Maximization (EM) algorithm (43; 44) is an iterative technique for estimating parameters in statistical models involving latent variables. When the true values of the latent variables are unknown, maximum likelihood estimation (MLE) cannot be directly applied to identify the model parameters. Instead, the EM algorithm maximizes a lower bound of the log-likelihood function, derived using Jensen's inequality:

\[ p_{}()= p_{}( ,)& p_{}( )(,)}{p_{ }()}\\ &= p_{}() p_{}( ,)- p_{}( ) p_{}()\\ &=_{ p_{}( )}[ p()+ p_{}()- p_{ }()]( ),\] (7)

where \(\), \(\), and \(\) denote the latent variables, observations, and model parameters, respectively. The algorithm alternates between two steps:

* **Expectation step (E-step)**: Sample latent variables from the current estimate of the conditional distribution, \( p_{}()\), and compute the expected log-likelihood lower bound \(()\).
* **Maximization step (M-step)**: Maximize \(()=_{ p_{}( )}[ p_{}()]\) to update parameters \(\).

This iterative procedure allows the EM algorithm to converge to a local maximum of the observed data log-likelihood, making it a powerful technique for estimation problems involving latent variables, such as Gaussian mixture clustering(45), and dynamical system identification(46).

## 4 Proposed Method

Given corrupted observations \(\) and a known forward model \(p()\), learning DMs from corrupted data is a parameter estimation problem involving latent variables. The latent variables are the underlying clean images \(\), and the goal is to estimate the DM parameters \(\) that govern the image prior \(p_{}()\). Consequently, we can leverage an iterative EM approach to reconstruct clean images and train the DM using corrupted data jointly, as described in Fig. 1 and Algorithm 1.

### Initialization: Training a Vague Diffusion Model using Limited Clean Images

The Expectation-Maximization (EM) algorithm needs a good initialization to begin its iterative process, as an improper initialization can result in convergence at an incorrect local minimum. While obtaining a large dataset of clean images is difficult, a small set of clean data is often available. This limited clean data can be used to train an initial DM to start the EM iterations. For example, in all the following experiments, 50 randomly selected clean images were used to train the initial DM, serving as the starting point for the EM algorithm. As demonstrated in Sec. 5.4, clean images do not need to be from the same dataset; those from out-of-distribution datasets also serve as reasonable initializations.

### E-step: Adaptive Diffusion Posterior Sampling

In the E-step, we assume a known diffusion prior and reconstruct the underlying clean images through diffusion posterior sampling. We adopt the standard variance-preserving form of the stochastic differential equation (VP-SDE) (2), which is equivalent to the Denoising Diffusion Probabilistic Models (DDPM) (1). The drift function \((_{t},t)\) takes the form \((t)_{t}/2\), and the diffusion rate \(g(t)\) is \(\). Therefore, the reverse diffusion sampler in Eq. 4 can be represented as:

\[_{t}=[-_{t}-(t) _{_{t}} p_{t}(_{t}) ]t+},\] (8)

Considering a known imaging forward model, \(\), and additive Gaussian noise, \(p()(( ),^{2})\), the conditional score function can be represented as:

\[_{_{t}} p(_{t}) =_{_{t}} p_{t}(_{t})- _{_{t}} p_{t}(_{t})\] (9)

where

\[}_{0}(_{t})=(t)}}[ _{t}+(1-(t))_{}(_{t },t)],(t)=_{s=1}^{t}(1-(s)).\] (10)

However, a naive diffusion posterior sampling approach using Eqs. 8, 9, and 10 often fails to produce high-quality reconstructions. This is because the learned DM is inaccurate during the early EM iterations. We demonstrate this issue with a toy experiment. We performed diffusion posterior sampling (DPS) on randomly masked observations, as shown in Fig. 2(a), using an initial DM trained on only 50 clean images. The resulting posterior samples, depicted in Fig. 2(b), show mode collapse due to the severely limited prior. All recovered samples come from the training set of 50 clean images and are unrelated to the observations. Similarly, if the DM is trained on blurry, noisy images with artifacts, naive DPS also performs poorly in image reconstruction.

Figure 2: **Adaptive diffusion posterior sampling on CIFAR-10 inpainting. (a) Corrupted observations from the test set, with 60% of the pixels masked in each image. (b), (c), and (d) Diffusion posterior samples with the diffusion prior weighted by different scaling factors: \(=1,10,20\). The diffusion prior is pre-trained using the 50 clean images shown in (e). When \(\) is small, there is obvious mode collapse, and all posterior samples come from the training set of 50 clean images, unrelated to the observations. As \(\) increases, the data likelihood gains more significance, resulting in reconstructed images that are more consistent with the inpainting observations.**

It does not mean that these low-quality DMs cannot provide any prior information. Although the prior is poor in the early training stages, it has learned common features and structures shared among natural images, such as the continuity and smoothness of natural images and profiles of specific object types. By introducing a hyper-parameter \(\) to rescale the likelihood term and avoid mode collapse, we find that the low-quality DM can also act as a weak prior for posterior sampling, where the reverse-time SDE can be written as:

\[ d&=(t)[-}{2}-(_{_{t}} p_{t}(_{t})+ _{_{t}} p_{t}(_{t} ))]dt+d}\\ &(t)[-}{2}-(_{ }(_{t},t)-}_{_{t}} \|-(}_{0}(_{t} ))\|_{2}^{2})]dt+d},\] (11)

The hyper-parameter \(\) efficiently balances the diffusion prior and the data likelihood, resulting in reliable reconstructed images even when the prior is poor. As demonstrated in Fig. 2 (b), (c), and (d), as \(\) increases from 1 to 20, the data likelihood term gains more emphasis, making the reconstructed images more consistent with the inpainting observations. The choice of the hyper-parameter \(\) is automated in each E-step by finding the value that minimizes the data loss,

\[^{*}=*{arg\,min}_{}_{,}_{0,}}[\|-(} _{0,})\|_{2}^{2}],\] (12)

where \(}_{0,}\) represents the diffusion posterior samples of reconstructed images with \(\) scaling.

### M-step: Optimizing Score-Based Priors

During the M-step, we update the weights of the score-based models using the posterior samples obtained in the E-step. This resembles training a standard clean DM, \(_{}\), to approximate the time-dependent score function, \(_{_{t}} p(_{t}}_{0})\), through denoising score matching (47):

\[^{*}=*{arg\,min}_{}_{t,_{t}, {}_{0}}[\|_{}(_{t},t)-_{ _{t}} p(_{t}}_{0})\|_{2}^{2} ],\] (13)

where \(t(\{1,...,T\})\), \(}_{0}=}_{0,^{*}}\) represents the posterior samples from the previous E-step, and \(_{t} p(_{t}}_{0})\) are generated by the forward-time SDE in Eq. 3.

To accelerate training, especially during the early stages when the posterior samples are noisy, the M-step does not always train the score function \(s_{}()\) from scratch. In the initial M-steps, we inherit the DM weights from the previous iteration and fine-tune them only using posterior samples from a subset of observations (e.g., randomly select 10% of total observations). However, once the quality of reconstructed images improves sufficiently, we reinitialize the DM weights and retrain the model with 100% data for a few more iterations. The training strategy transitions when the optimal balancing parameter, \(^{*}\), falls below 1, or fails to decrease for more than three consecutive iterations.

## 5 Experiments

In this section, we demonstrate the performance of our method in learning DMs from corrupted data and solving inverse problems using these models. We validate the method on three imaging tasks: random inpainting, denoising, and deblurring. Our main results are presented in Fig.3, Fig.4, and Table 1, with additional ablation studies in Fig. 5. Further details on neural network architectures, training settings, and additional reconstruction and generation samples are provided in the appendix.

### Datasets and Evaluation Metrics

The experiments are conducted on the CIFAR-10 (48) and CelebA (49) datasets at resolutions of \(32 32\) and \(64 64\), respectively. CIFAR-10 consists of 50,000 images across 10 classes for training, while CelebA contains 30,000 images of human faces. At each iteration, 5,000 corrupted images are randomly chosen for posterior sampling and training, and 250 corrupted images from the test set are chosen for evaluation.

We evaluate the performance of our method using two groups of metrics. First, we compute the peak signal-to-noise-ratio (PSNR) and learned Perceptual Image Patch Similarity (LPIPS) scores between the reconstructed and ground-truth images, quantifying the accuracy of inverse imaging using learned DMs. Additionally, we compute the Frechet Inception Distance (FID) between the learned DMs and reserved test data to assess their image generation quality.

### Baseline and Training Settings

We compare our method with three related baselines: AmbientDiffusion (37), SURE-Score (39), and DPS with clean prior (13). AmbientDiffusion and SURE-Score have similar settings to our method, which do not require DMs pre-trained on large-scale clean signals. Considering AmbientDiffusion is well-designed for masked observations, we only use it as the baseline of the image inpainting task. On the other hand, DPS leverages a pre-trained clean diffusion prior for posterior sampling, so it defines the performance upper bound for our method.

In our experiments, we randomly select 50 clean images from each dataset to train the initial DMs for the EM iterations. AmbientDiffusion is trained with the standard setting in (37). The key hyper-parameter of SURE-Score, \(_{}\), is set to the observation noise's standard deviation (0.2 for denoising,

Figure 3: **Results on CIFAR-10 inpainting. In each image, 60% of the pixels are masked. As the EM iterations progress, the diffusion model learns cleaner prior distributions, improving the quality of posterior samples. Our method significantly outperforms the baselines, SURE-Score and AmbientDiffusion, achieving reconstruction quality comparable to DPS with a clean prior.**

and 0.01 for inpainting and deblurring). To ensure a fair comparison, we also provide the same 50 clean images for training AmbientDiffusion and SURE-Score. Details are in Appendix A.

### Results

Image inpainting.We conduct random inpainting (with mask probability \(p=0.6\)) on CIFAR-10. As shown in Fig. 3 and Table 1, our method significantly outperforms AmbientDiffusion and SURE-Score, achieving reconstruction quality similar to DPS with a prior trained on the clean CIFAR-10 dataset. The iterative training process is also illustrated in Fig. 3. Initially, our method performs poorly with the DM trained on only 50 randomly selected clean samples. However, as the E-step and M-step alternate iteratively, the quality of posterior sampling improves. Large-scale posterior samples enrich the priors, leading to enhanced performance at each stage.

Image denoising.We perform image denoising on CIFAR-10 with Gaussian noise \((0,^{2})\) and \(=0.2\). The results are shown in Fig. 4(a) and Table 1. Our method outperforms SURE-Score, and the self-supervised denoising benchmark, Noise2Self (50), though it slightly lags behind DPS with clean priors. However, while our method's reconstructions may appear noisier than DPS results, they sometimes reproduce more details, such as the car wheels in the second row and the cat face in the third row of Fig. 4(a), showcasing the better diversity of our learned DMs.

Image deblurring.We validate image deblurring on CelebA using a Gaussian blur kernel with a size of \(9 9\) and a standard deviation of \(=2\) pixels. The results are shown in Fig. 4(b) and Table 1. As with the other tasks, our method significantly outperforms SURE-Score in solving imaging inverse problems, recovering fine details of human faces. However, the FID score of our learned diffusion models lags behind the original blurred observations. This is primarily because the FID score measures image similarity mainly through smooth features, making it a less effective metric for deblurring tasks.

    &  &  &  \\ 
**Method** & PSNR\(\) & LPIPS\(\) & FID\(\) & PSNR\(\) & LPIPS\(\) & FID\(\) & PSNR\(\) & LPIPS\(\) & FID\(\) \\  Observations & 13.49 & 0.295 & 234.47 & 18.05 & 0.047 & 132.59 & 22.47 & 0.365 & 72.83 \\ DPS w/ clean prior & 25.44 & 0.008 & 7.08 & 25.91 & 0.010 & 7.08 & 29.05 & 0.013 & 10.24 \\ Noise2Self (50) & - & - & - & 21.32 & 0.227 & 92.06 & - & - & - \\ SURE-Score (39) & 15.75 & 0.182 & 220.01 & 22.42 & 0.138 & 132.61 & 22.07 & 0.383 & 191.96 \\ AmbientDiffusion (37) & 20.57 & 0.027 & 28.88 & - & - & - & - & - & - \\ Ours & **24.70** & **0.009** & **21.08** & **23.16** & **0.022** & **86.47** & **23.74** & **0.103** & **91.89** \\   

Table 1: Numerical Results of inverse imaging and learned priors. The average values of PSNR/LPIPS are from 250 samples randomly selected from the test set. FID is used to evaluate the quality of learned priors by comparing 50,000 generated samples to the train set. Optimal results are highlighted in **bold** and suboptimal results in underline. Note that we take DPS w/ clean prior as the upper bound.

Figure 4: **Results on (a) CIFAR-10 denoising and (b) CelebA deblurring. Our method significantly outperforms the baseline, SURE-Score, and approximates DPS with clean prior.**

Comparing the results of all three tasks, we find that AmbientDiffusion only works well for inpainting because its additional masking technique is specifically designed for that purpose. SURE-Score consistently produces over-smoothed results because the SURE loss regularizes the gradient of generated images. As a comparison, our method does not make any special assumptions and provides a general framework applicable to all three tasks. The generation results are in Appendix D.

### More Analysis and Ablation Studies

Number of clean images for training initial DMs.Our EM approach starts with DMs trained on a small set of clean images. Fig. 5(a) shows the PSNR of posterior samples generated by these models in the first E-step, allowing us to evaluate the impact of the number of clean training images on the performance of the initial DMs. Remarkably, DMs trained on as few as 10 clean images (0.02% of the corrupted images) can still act as reasonable priors. For inpainting and denoising tasks, DMs trained on 10 clean images provide nearly the same reconstruction quality as those trained on 500 clean images, as these tasks primarily require priors for low-frequency features, and 10 images suffice for an initial guess. However, the deblurring task benefits from DMs trained on more clean data since deblurring aims to recover high-frequency details where more data helps.

Surprisingly, we find that DMs trained on clean images from CelebA (downsampled to 32x32) can also be used to initialize tasks on CIFAR-10. For inpainting and denoising tasks, DMs initialized with 50 clean images from an out-of-distribution (OOD) dataset (CelebA) achieve similar performance to those initialized with in-distribution (ID) data (CIFAR-10). For the deblurring task, DMs trained on OOD data perform better than those trained on a similar amount of ID data, suggesting that OOD data can sometimes serve as stronger priors for guessing high-frequency information.

Learned priors through iterative training.Fig. 5(b) shows the FID scores of the learned DMs in the inpainting task with 50,000 corrupted CIFAR-10 images after each EM stage. The generation ability of the DMs gradually improves as the EM iterations progress. As explained in Sec. 4.3, initially the DM inherits weights from previous steps for fast training and converges at the sixth iteration. After resetting the DM, the training resumes for three more rounds and finally converges to a FID score of approximately 21.08, significantly better than AmbientDiffusion's 28.88, setting a new state-of-the-art. Notably, our method achieves this performance using a DM architecture with far fewer parameters: our method employs a vanilla DDPM with 35.7 million parameters, while AmbientDiffusion uses an improved DDPM++ architecture with over 100 million parameters. Additionally, we compared our method with DMs trained on different amounts of clean data. Our model, learned from 50,000 corrupted images (60% pixels masked) using EM, performs better than a DM trained on around 15,000 clean images.

Scaling factor in adaptive diffusion posterior sampling.Fig. 5(c) shows the PSNR of reconstructed images at each EM stage with different scaling factors, using the inpainting task on CIFAR-10 as an example. We observe that the quality of posterior sampling initially improves and then de

Figure 5: **Ablation studies. (a) PSNR of diffusion posterior samples generated by the initial diffusion models trained on different amounts (10, 50, 100, 500) or types (in-distribution or out-of-distribution) of clean data. (b) FID scores of learned diffusion models after each EM iteration. The diffusion model trained on 50,000 corrupted images achieves a similar performance to those trained on 15,000-20,000 clean images. (c) PSNR of diffusion posterior samples weighted by different scaling factors \(\) at each stage. The optimal \(\) for posterior sampling decreases as the EM iterations progress.**

teriorates as the scaling factor increases, confirming the existence of an optimal scaling factor as suggested in Eq. 12. As the EM stages progress, the optimal scaling factor decreases, indicating that the learned priors progressively improve through the EM iterations. This observation justifies the need for adaptive scaling factors in our method.

## 6 Conclusion

In this paper, we proposed EMDiffusion, a novel expectation-maximization (EM) framework for training diffusion models primarily from corrupted observations. The key assumption is that it is information-theoretically possible to learn the underlying distribution from measurements. Our method demonstrated state-of-the-art performance in image inpainting, denoising, and deblurring across various datasets. Additionally, an important finding is that a small amount of clean, in-distribution data can act as an implicit regularizer, aiding the training of diffusion models from corrupted observations. Future work will aim to 1) extend initialization approaches, potentially by incorporating foundation models or traditional machine learning techniques, such as using preprocessed images from unsupervised inpainting, deblurring, or denoising for initialization, and 2) extend to various imaging inverse problems and learning unknown forward models or noise statistics.