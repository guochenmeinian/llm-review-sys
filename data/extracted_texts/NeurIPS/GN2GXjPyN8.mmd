# Antigen-Specific Antibody Design via

Direct Energy-based Preference Optimization

 Xiangxin Zhou\({}^{1,2,4,}\)

Equal contribution (this work was done during Xiangxin and Ruizhe's internship at ByteDance Research).

&Dongyu Xue\({}^{4,}\)

Equal contribution (this work was done during Xiangxin and Ruizhe's internship at ByteDance Research).

Ruizhe Chen\({}^{3,4,}\)

Correspondence to: Quanquan Gu <quanquan.gu@bytedance.com>.

Zaixiang Zheng\({}^{4}\)&Liang Wang\({}^{1,2}\)&Quanquan Gu\({}^{4,}\)

Equal contribution (this work was done during Xiangxin and Ruizhe's internship at ByteDance Research).

###### Abstract

Antibody design, a crucial task with significant implications across various disciplines such as therapeutics and biology, presents considerable challenges due to its intricate nature. In this paper, we tackle antigen-specific antibody sequence-structure co-design as an optimization problem towards specific preferences, considering both rationality and functionality. Leveraging a pre-trained conditional diffusion model that jointly models sequences and structures of antibodies with equivariant neural networks, we propose _direct energy-based preference optimization_ to guide the generation of antibodies with both rational structures and considerable binding affinities to given antigens. Our method involves fine-tuning the pre-trained diffusion model using a residue-level decomposed energy preference. Additionally, we employ gradient surgery to address conflicts between various types of energy, such as attraction and repulsion. Experiments on RAbD benchmark show that our approach effectively optimizes the energy of generated antibodies and achieves state-of-the-art performance in designing high-quality antibodies with low total energy and high binding affinity simultaneously, demonstrating the superiority of our approach.

## 1 Introduction

Antibodies, vital proteins with an inherent Y-shaped structure in the immune system, are produced in response to an immunological challenge. Their primary function is to discern and neutralize specific pathogens, typically referred to as antigens, with a significant degree of specificity . The specificity mainly comes from the Complementarity Determining Regions (CDRs), which accounts for most binding affinity to specific antigens . Hence, the design of CDRs is a crucial step in developing potent therapeutic antibodies, which plays an important role in drug discovery.

Traditional _in silico_ antibody design methods rely on sampling or searching protein sequences over a large search space to optimize the physical and chemical energy, which is inefficient and easily trapped in bad local minima . Recently, deep generative models have been employed to model protein sequences in nature for antibody design . Following the fundamental biological principlethat structure determines function numerous efforts have been focused on antibody sequence-structure co-design [22; 21; 36; 29; 30; 37], which demonstrate superiority over sequence design-based methods. However, the main evaluation metrics in the aforementioned works are amino acid recovery (AAR) and root mean square deviation (RMSD) between the generated antibody and the real one. This is controversial because AAR is susceptible to manipulation and does not precisely gauge the quality of the generated antibody sequence. Meanwhile, RMSD does not involve side chains, which are vital for antigen-antibody interaction. Besides, it is biologically plausible that a specific antigen can potentially bind with multiple efficacious antibodies [45; 12]. This motivates us to examine the generated structures and sequences of antibodies through the lens of energy, which reflects the rationality of the designed antibodies and their binding affinity to the target antigens. We have noted that nearly all antibody sequence-structure co-design methods struggle to produce antibodies with low energy. This suggests the presence of irrational structures and inadequate binding affinity in antibodies designed by these methods (see Fig. 1). We attribute this incapability to the insufficient model training caused by a scarcity of high-quality data.

To tackle the above challenges and bridge the gap between _in silico_ antibody sequence-structure co-design methods and the intrinsic need for drug discovery, we formulate the antibody design task as an antibody optimization problem with a focus on better rationality and functionality. Inspired by direct preference optimization [DPO, 41] and self-play fine-tuning techniques  that achieve huge success in the alignment of large language models (LLMs), we proposed a direct energy-based preference optimization method named AbDPO for antibody optimization. More specifically, we first pre-train a conditional diffusion model on real antigen-antibody datasets, which simultaneously captures sequences and structures of complementarity-determining regions (CDR) in antibodies with equivariant neural networks. We then progressively fine-tune this model using synthetic antibodies generated by the model itself given an antigen with energy-based preference. This preference is defined at a fine-grained residue level, which promotes the effectiveness and efficiency of the optimization process. To fulfill the requirement of various optimization objectives, we decompose the energy into multiple types so that we can incorporate prior knowledge and mitigate the interference between conflicting objectives (e.g., repulsion and attraction energy) to guide the optimization process. Fine-tuning with self-synthesized energy-based antibody preference data represents a revolutionary solution to address the limitation of scarce high-quality real-world data, a significant challenge in this domain. We highlight our main contributions as follows:

* We tackle the antibody sequence-structure co-design problem through the lens of energy from the perspectives of both rationality and functionality.
* We propose direct residue-level energy-based preference optimization to fine-tune diffusion models for designing antibodies with rational structures and high binding affinity to specific antigens.
* We introduce energy decomposition and conflict mitigation techniques to enhance the effectiveness and efficiency of the optimization process.
* Experiments show AbDPO's effectiveness in generating antibodies with energies resembling natural antibodies and generality in optimizing multiple preferences.

## 2 Related Work

**Antibody Design.** The application of deep learning to antibody design can be traced back to at least [35; 43; 3]. In recent years, sequence-structure co-design of antibodies has attracted increasing attention. Jin et al.  proposed to simultaneously design sequences and structures of CDRs in an autoregressive way and iteratively refine the designed structures. Jin et al.  further utilized the epitope and focused on designing CDR-H3 with a hierarchical message passing equivariant

Figure 1: The third CDR in the heavy chain, CDR-H3 (colored in yellow), of real antibody (left) and synthetic antibody (right) designed by MEAN  for a given antigen (PDB ID: 4cmh). The rest parts of antibodies except CDR-H3 are colored in blue. The antigens are colored in gray. We use red (resp. black) dotted lines to represent clashes between a CDR-H3 atom and a framework/antigen atom (resp. another CDR-H3 atom). We consider a clash occurs when the overlap of the van der Waals radii of two atoms exceeds 0.6Ã….

network. Kong et al.  incorporated antigens and the light chains of antibodies as conditions and designed CDRs with E(3)-equivariant graph networks via a progressive full-shot scheme. Luo et al.  proposed a diffusion model that takes residue types, atom coordinates and side-chain orientations into consideration to generate antigen-specific CDRs. Kong et al.  focused on epitope-binding CDR-H3 design and modelled full-atom geometry. Recently, Martinkus et al.  proposed AbDiffuser, a novel diffusion model for antibody design, that incorporates more domain knowledge and physics-based constraints and also enables side-chain generation. Besides, Wu and Li , Gao et al.  and Zheng et al.  introduced pre-trained protein language model to antibody design. Distinct from the above works, our method places a stronger emphasis on designing and optimizing antibodies with low energy and high binding affinity.

**Alignment of Generative Models.** Solely maximizing the likelihood of training data does not always lead to a model that satisfies users' preferences. Recently, many efforts have been made on the alignment of the generative models to human preferences. Reinforcement learning has been introduced to learning from human/AI feedback to large language models, such as RLHF  and RLAIF . Typically, RLHF consists of three phases: supervised fine-tuning, reward modeling, and RL fine-tuning. Similar ideas have also been introduced to text-to-image generation, such as DDPO , DPOK  and DiffAC . They view the generative processes of diffusion models as a multi-step Markov Decision Process (MDP) and apply policy gradient for fine-tuning. Rafailov et al.  proposed direct preference optimization (DPO) to directly fine-tune language models on preference data, which matches RLHF in performance. Recently, DPO has been introduced to text-to-image generation [46; 6]. Notably, in the aforementioned works, models pre-trained with large-scale datasets have already shown strong performance, in which case alignment further increases users' satisfaction. In contrast, in our work, the model pre-trained with limited real-world antibody data is insufficient in performance. Therefore, preference optimization in our case is primarily used to help the model understand the essence of nature and meet the requirement of antibody design.

## 3 Method

In this section, we present AbDPO, a direct energy-based preference optimization method for designing antibodies with reasonable rationality and functionality (Fig. 2). We first define the antibody generation task and introduce the diffusion model for this task in Sec. 3.1. Then we introduce residue-level preference optimization for fine-tuning the diffusion model and analyze its advantages in effectiveness and efficiency in Sec. 3.2. Finally, in Sec. 3.3, we introduce the energy decomposition and describe how to mitigate the conflicts when optimizing multiple types of energy.

### Preliminaries

We focus on designing CDR-H3 of the antibody given antigen structure as CDR-H3 contributes the most to the diversity and specificity of antibodies [49; 2] and the rest part of the antibody including the frameworks and other CDRs. Following Luo et al. , each amino acid is represented by its type \(_{i}\{\}\), \(_{}\) coordinate \(_{i}^{3}\), and frame orientation

Figure 2: Overview of AbDPO. This process can be summarized as: (a) Generate antibodies with the pre-trained diffusion model; (b) Evaluate the multiple types of residue-level energy and construct preference data; (c) Compute the losses for energy-based preference optimization and mitigate the conflicts between losses of multiple types of energy; (d) Update the diffusion model.

[MISSING_PAGE_FAIL:4]

### Direct Energy-based Preference Optimization

Only the antibodies with considerable sequence-structure rationality and binding affinity can be used as effective therapeutic candidates. Fortunately, these two properties can be estimated by biophysical energy. Thus, we introduce direct energy-based preference optimization to fine-tune the pre-trained diffusion models for antibody design.

Inspired by RLHF , we can fine-tune the pre-trained model to maximize the reward as:

\[_{}_{^{0} p_{}}[r(^{0})]-_{}(p_{}(^{0})\|p_{ }(^{0})),\]

where \(p_{}\) (resp. \(p_{}\)) is the distribution induced by the model being fine-tuned (resp. the fixed pre-trained model), \(\) is a hyperparameter that controls the KL divergence regularization, and \(r()\) is the reward function. The optimal solution to the above objective takes the form:

\[p_{^{*}}(^{0})=p_{}(^{0}) r(^{0}).\]

Following Rafailov et al. , we turn to the DPO objective as follows:

\[L_{}= -_{^{0}_{1},^{0}_{2}}\! (^{0}_{1},^{0}_{2}) \!\!}(^{0}_{1})}{p_{}( ^{0}_{1})}\!-\!\!}(^{0}_{2})}{p _{}(^{0}_{2})},\]

where \(()\) is sigmoid and \((^{0}_{1},^{0}_{2})\) indicate the preference over \(^{0}_{1}\) and \(^{0}_{2}\). We use "\(\)" to denote the preference. Specifically, \((^{0}_{1},^{0}_{2})=1\) (resp. \(-1\)) if \(^{0}_{1}^{0}_{2}\) (resp. \(^{0}_{2}^{0}_{1}\)) in which case we call \(^{0}_{1}\) (resp. \(^{0}_{1}\)) the "winning" sample and \(^{0}_{2}\) (resp. \(^{0}_{1}\)) the "losing" sample, and \((^{0}_{1},^{0}_{2})=0\) if they tie. \(^{0}_{1}\) and \(^{0}_{2}\) are a pair of data sampled from the Bradley-Terry [B7, 8] model with reward \(r()\), i.e., \(p(^{0}_{1}^{0}_{2})=(r(^{0}_{1})- r(^{0}_{2}))\). Please refer to Appendix C for more detailed derivations.

Due to the intractable \(p_{}(^{0})\), following Wallace et al. , we introduce latent variables \(^{1:T}\) and utlize the evidence lower bound optimization (ELBO). In particular, \(L_{}\) can be modified as follows:

\[L_{}= -_{^{0}_{1},^{0}_{2}}\! _{^{1:T}_{1},^{1:T}_{ 2}}(^{0}_{1},^{0}_{2})\! }(^{0:T}_{1})}{p_{}(^{0:T}_{ 1})}\!-\!}(^{0:T}_{2})}{p_{}( ^{0:T}_{2})}\!\!\!,\]

where \(^{1:T}_{1} p_{}(^{1:T}_{1}|^{0} _{1})\) and \(^{1:T}_{2} p_{}(^{1:T}_{2}|^{0} _{2})\).

Following Wallace et al. , we can utilize Jensen's inequality and convexity of function \(-\) to derive the following upper bound of \(L_{}\):

\[_{}=-_{t,^{0} _{1},^{0}_{2},(^{0}_{1},^{t-1}_{1},^{1}_{1}),(^{t-1}_{2},^{t}_{2})}\] \[ T(^{0}_{1},^{0 }_{2})}(^{t-1}_{1}|^{t}_{ 1})}{p_{}(^{t-1}_{1}|^{t}_{1})}-}(^{t-1}_{2}|^{t}_{2})}{p_{}( ^{t-1}_{2}|^{t}_{2})},\]

where \(t(0,T)\), \((^{t-1}_{1},^{t}_{1})\) and \((^{t-1}_{2},^{t}_{2})\) are sampled from reverse generative process of \(^{0}_{1}\) and \(^{0}_{2}\), respectively, i.e., \((^{t-1}_{1},^{t}_{1}) p_{}(^{t-1 }_{1},^{t}_{1}|^{0}_{1})\) and \((^{t-1}_{2},^{t}_{2}) p_{}(^{t-1 }_{2},^{t}_{2}|^{0}_{2})\).

In our case, the antibodies with low energy are desired. Thus, we define the reward \(r()\) as \(-()/\), where \(()\) is the energy function and \(\) is the temperature. Different from the text-to-image generation where the (latent) reward is assigned to a complete image instead of a pixel , we know more fine-grained credit assignment. Specifically, it is known that \((^{0})=_{j=n+1}^{n+m}(^{0}[j])\), i.e., the energy of an antibody is the summation of the energy of its amino acids . Thus the preference can be measured **at the residue level** instead of the entire CDR level. Besides, we have \( p_{}(^{t-1}|^{t})=_{j=n+1}^{n+m} p_ {}(^{t-1}[j]|^{t})\), which is a common assumption of diffusion models. Thus we can derive a residue-level DPO-Diffusion loss:

\[L_{}=-_{t,^{0}_{1}, ^{0}_{2},(^{t-1}_{1},^{t}_{1}),(^{t-1 }_{2},^{t}_{2})}\] \[ T\!_{j=n+1}^{n+m}\!\!( ^{0}_{1}[j],^{0}_{2}[j])}( ^{t-1}_{1}[j]|^{t}_{1})}{p_{}(^{t-1 }_{1}[j]|^{t}_{1})}-}(^{t-1}_{1}[j]| ^{t}_{2})}{p_{}(^{t-1}_{2}[j]|^{t}_{2} )}.\]Thus, by Jensen's inequality and the convexity of \(-\), we can further derive \(_{}\), which is an upper bound of \(L_{}\):

\[_{}=-_{t,^{0}_{1},^{0}_{2},(^{t-1}_{1},^{t}_{1}),( ^{t-1}_{2},^{t}_{2})}\] \[_{j=n+1}^{n+m} T(^{0}_{1 }[j],^{0}_{2}[j])}( ^{t-1}_{1}[j]|^{t}_{1})}{p_{}(^{t-1 }_{1}[j]|^{t}_{1})}-}(^ {t-1}_{2}[j]|^{t}_{2})}{p_{}(^{t-1}_{2}[j]| ^{t}_{2})}.\]

The gradients of \(_{}\) and \(_{}\) w.r.t the parameters \(\) can be written as:

\[_{}_{}=-  T_{t,^{0}_{1},^{0}_{2},( ^{t-1}_{1},^{t}_{1}),(^{t-1}_{2},^ {t}_{2})}_{j=n+1}^{n+m}(^{0}_{1},^ {0}_{2})\] \[((^{0}_{2})-(^{0}_ {1}))_{} p_{}( ^{t-1}_{1}[j]|^{t}_{1})-_{}  p_{}(^{t-1}_{2}[j]|^{t}_{2}) ,\]

and

\[_{}_{} \!=\!- T_{t,^{0}_{1},^{0}_{2},( ^{t-1}_{1},^{t}_{1}),(^{t-1}_{2},^ {t}_{2})}_{j=n+1}^{n+m}\!(^{0}_{1}[j],^{0}_{2}[j])\] \[((^{0}_{2}[j])-(^ {0}_{1}[j]))_{} p_{}( ^{t-1}_{1}[j]|^{t}_{1})-_{}  p_{}(^{t-1}_{2}[j]|^{t}_{2}) ,\]

where \(():=(p_{}()/p_{}())\), which can be viewed as the estimated reward by current policy \(p_{}\).

We can see that \(_{}_{}\) actually reweight \(_{} p_{}(^{t-1}[j ]|^{t})\) with the estimated reward of the complete antibody while \(_{}_{}\) does this with the estimated reward of the amino acid itself. In this case, \(_{}_{}\) will increase (resp. decrease) the likelihood of all amino acids of the "winning" sample (resp. "losing") at the same rate, which may mislead the optimization direction. In contrast, \(_{}_{}\) does not have this issue and can fully utilize the residue-level signals from estimated reward to effectively optimize antibodies.

We further approximate the objective \(_{}\) by sampling from the forward diffusion process \(q\) instead of the reverse generative process \(p_{}\) to achieve diffusion-like efficient training. With further replacing \(}}{p_{}}\) with \(-}}+}}{q}\) which is exactly \(-_{KL}(q\|p_{})+_{KL}(q\|p_{})\) when taking expectation with respect to \(q\), we can derive the final loss for fine-tuning the diffusion model as follows:

\[L_{}=-_{t,^{0}_{1},^ {0}_{2},(^{t-1}_{1},^{t}_{1}),(^{t-1}_{2}, ^{t}_{2})}_{j=n+1}^{n+m}- T(^{0}_{1}[j],^{0}_{2}[j])\] \[^{t}_{,1}(q\|p_{})[j]-^{t}_{,1}(q\|p_{})[j]-^{t}_ {,2}(q\|p_{})[j]+^{t}_{,2}(q\|p _{})[j]},\] (8)

where \(^{0}_{1},^{0}_{2} p_{}()\), \((^{t-1}_{1},^{t}_{1})\) and \((^{t-1}_{2},^{t}_{2})\) are sampled from forward diffusion process of \(^{0}_{1}\) and \(^{0}_{2}\), respectively, which can be much more efficient than the reverse generative process that involves hundreds of model forward estimation. Here we use \(^{t}_{,1}(q\|p_{})[j]\) to denote \(_{}(q(^{t-1}_{1}[j]|^{t-1},^ {0})\|p_{}(^{t-1}_{1}[j]|^{0}))\). Similar for \(^{t}_{,1}(q\|p_{})[j]\), \(^{t}_{,2}(q\|p_{})[j]\), and \(^{t}_{,2}(q\|p_{})[j]\). These KL divergence can be estimated as in Eqs. (5) to (7).

### Energy Decomposition and Conflict Mitigation

The energy usually consists of different types, such as attraction and repulsion. Empirically, direct optimization on single energy will lead to some undesired "shortcuts". Specifically, in some cases, repulsion dominates the energy of the antibody so the model will push antibodies as far from the antigen as possible to decrease the repulsion during optimization, and finally fall into a bad local minima. This effectively reduces the repulsion, but also completely eliminates the attraction between antibodies and antigens, which seriously impairs the functionality of the antibody. This motivates us to explicitly express the energy with several distinct terms and then control the optimization process towards our preference.

Inspired by Yu et al. , we utilize "gradient surgery" to alleviate interference between different types of energy during energy preference optimization. More specifically, we have \(()=_{v=1}^{V}w_{v}_{v}()\), where \(V\) is the number of types of energy, and \(w_{v}\) is a constant weight for the \(v\)-th kind of energy. For each type of energy \(_{v}()\), we compute its corresponding energy preference gradient \(_{}L_{v}\) as Eq. (8), and then alter the gradient by projecting it onto the normal plane of the other gradients (in a random order) if they have conflicts. This process works as follows:

\[_{}L_{v}_{}L_{v}- }L_{v}^{}_{}L_{u},0)}{\|_{}L_{u}\|^{2}} _{}L_{u},\] (9)

where \(v\{1,,V\}\) and \(u=(1,,V)\).

## 4 Experiments

### Experimental Setup

Dataset CurationTo pre-train the diffusion model for antibody generation, we use the Structural Antibody Database  under IMGT  scheme as the dataset. We collected antigen-antibody complexes with both heavy and light chains and protein antigens and discarded the duplicate data with the same CDR-L3 and CDR-H3 sequence. The remaining complexes are used to cluster via MMseqs2  with 40% sequence similarity as the threshold based on the CDR-H3 sequence of each complex. We then select the clusters that do not contain complexes in RAbD benchmark  and split the complexes into training and validation sets with a ratio of 9:1 (1786 and 193 complexes respectively). Specifically, the validation set is composed of clusters that only contain one complex. The test set consists of 55 eligible complexes from the RAbD benchmark (details in Appendix D.2).

For the synthetic data used in AbDPO fine-tuning, 10,112 samples are randomly sampled for each antigen-antibody complex in the test set using the aforementioned pre-trained diffusion model. Then, we use pyRosetta  to apply the side-chain packing for these samples.

Preference DefinitionTo apply AbDPO, we need to build the preference dataset and construct the "winning" and "losing" pair. The accurate relationship between preferences based on _in silico_ with wet-lab experimental results is a scientific issue that remains unresolved, with a wide range of opinions. AbDPO's solution to this open question is to provide a generic framework that allows for arbitrary definitions and combinations of preferences to satisfy various requirements in antibody design.

To demonstrate the effectiveness of ABDPO, we define the preferences as lower total energy and lower binding energy. The two energies are defined on residue level, specifically, **(1)**\(_{}\)\(E_{}\) is the total energy of each residue within the designed CDR, and is used to represent the overall rationality of the corresponding residue; **(2)**\(_{}\)-Ag \(\)G is the interaction energy between each designed CDR residue and the target antigen, representing the functionality of the corresponding residue. \(_{}\)-Ag \(\)G is further decomposed into **(2.1)**\(_{}\)-Ag \(E_{}\), the sum of the interaction energies except repulsion between the designed CDR residue and the antigen, and **(2.2)**\(_{}\)-Ag \(E_{}\), the repulsion energy between the design CDR residue and the antigen.

As a generic framework, AbDPO also supports non-energy-based preferences. To verify this, we demonstrate an advanced version named AbDPO+. AbDPO+ incorporates two additional preferences: pseudo log-likelihood (pLL) from AntiBERTy  and the percent of hydrophobicity residues (PHR). Different from the previously mentioned energy-based preferences, pLL and PHR are defined on the whole CDR level. For pLL, a higher value is considered better and is designated as "winning", conversely; for PHR, a lower value is preferable.

BaselinesWe compare our model with various representative antibody sequence-structure co-design baselines. **HERN** designs sequences of antibodies autoregressively with the iterative refinement

  
**Methods** & **AAR** (\(\)) & **RMSD** (\(\)) & **CDR**\(E_{}\) (\(\)) & **CDR-Ag**\( G\) (\(\)) & **pLL** (\(\)) & **PHR** (\(\)) & **N\({}_{}\) (\(\)) \\  HERN & 32.38\% & 9.18 & 10887.77 & 2095.88 & -2.02 & 40.46\% & 0 \\ MEAN & 36.20\% & **1.69** & 7162.65 & 1041.43 & **-1.79** & **30.62\%** & 0 \\ dyMEAN & **40.04\%** & 1.82 & 3782.67 & 1730.06 & -1.82 & 43.72\% & 0 \\ DiffAb & 34.92\% & 1.92 & 1729.51 & 1297.25 & -2.10 & 41.27\% & 0 \\  AbDPO & 31.25\% & 1.98 & **629.44** & **307.56** & -2.18 & 69.67\% & **9** \\ AbDPO+ & 36.27\% & 2.01 & 1106.48 & 637.62 & -2.00 & 44.21\% & 5 \\   

Table 1: Summary of AAR, RMSD, CDR \(E_{}\), CDR-Ag \( G\) (kcal/mol), pLL, PHR, and N\({}_{}\) of antibodies designed by our model and baselines. (\(\)) / (\(\)) denotes a smaller / larger number is better.

of structures; **MEAN** generates sequences and structures of antibodies via a progress full-shot scheme; **dyMEAN** designs antibodies sequences and structures with full-atom modeling; **DiffAb** models antibody distributions with a diffusion model that considers the amino acid type, C\({}_{}\) positions and side-chain orientations, which is a more rigorous _generative_ model than the above baselines. The side-chain atoms are packed by pyRosetta. For **dyMEAN**, we **(1)** provide the ground-truth framework structure as input like other methods, **(2)** only use its generated backbones and pack the side-chain atoms by pyRosetta for a more fair comparison.

EvaluationFollowing the previous studies, we preliminarily evaluate the generated sequence and structure with AAR and C\(\) RMSD. Besides, we carry out a series of more reasonable metrics. We utilize the preferences aforementioned to evaluate the designed antibodies from multiple perspectives, but at the whole CDR level. Specifically, **(1)** CDR \(E_{}\), the total energy of the designed CDR, is utilized to evaluate the rationality by aggregating all Res\({}_{}\)\(E_{}\) of residues within the CDR; **(2)** CDR-Ag \( G\) denotes the difference in total energy between the bound state and the unbound state of the CDR and antigen, which is calculated to evaluate the functionality. PHR and pLL remain the same definition as above. All methods are able to generate multiple antibodies for a specific antigen (a randomized version of MEAN, rand-MEAN, is used here). We employ each method to design 192 antibodies for each complex, and we report the mean metrics across all 55 complexes. We further report the number of successfully designed antibody-antigen complexes, N\({}_{}\), to evaluate their rationality and functionality comprehensively. The design for an antibody-antigen complex is considered as "successful" when at least one generated sample holds energies close to or lower than the natural one, i.e., for both of the two energy types, \(E_{}<E_{}+(E_{}^{ })\).

### Main Results

We report the evaluation metrics in Tab. 1. As the results show, AbDPO performs significantly superior to other antibody sequence-structure co-design methods in the two energy-based metrics, CDR \(E_{}\) and CDR-Ag \( G\), while maintaining the AAR and RMSD. With the two additional preferences, AbDPO+ avoids the expense of the increased PHR while achieving better performance than DiffAb in remaining metrics (even surpassing DiffAb in AAR). This demonstrates the effectiveness and compatibility of AbDPO in terms of optimizing multi-objectives simultaneously. We have also provided the detailed evaluation results for each complex in Appendix E.2.

We do not consider AAR and RMSD as the main reference evaluation metrics as their inadequacy (refer to Appendix A for more details). With the new evaluation methods, issues that used to be hidden by AAR and RMSD are exposed. It is observed that structural clashes can not be avoided completely in any method, resulting in the high energy values of generated antibodies, even for AbDPO and AbDPO+. The structural clashes between CDR and the antigen finally lead to the unreasonable high CDR-Ag \( G\). However, the primary goal in antibody design is to generate at least one effective antibody. Given the complexity of protein interactions, it is not plausible that every generated antibody will yield effectiveness. Therefore, N\({}_{}\) is a more valuable metric. AbDPO and AbDPO+ are the only two to achieve successful cases, with 9 and 5 successful cases out of 55 complexes, respectively. Following this concept, we also rank the designed antibodies for each complex by a uniform strategy (see Appendix D.3), calculate the metrics of the highest-ranked design for each complex, and report the mean metrics across the 55 complexes (see Appendix E.1). Notably, AbDPO is the only method that achieves CDR-Ag \( G\) lower than 0.

Figure 3: Visualization of reference antibodies in RABD and antibodies designed by AbDPO given specific antigens (PDB ID: **liqd** (left), **lic7** (middle), and **2dd8** (right)). The unit of energy annotated is kcal/mol and omitted here for brevity.

We also visualize three cases (PDB ID: 1iqd, 1ic7, and 2dd8) in Fig. 3. It is shown that AbDPO can design CDRs with both fewer clashes and proper relative spatial positions towards the antigens, and even better energy performance than that of natural antibodies.

We conduct another two experiments to demonstrate further the generality of AbDPO: **(1)** directly incorporate auxiliary training losses for those properties of which gradients are computable; **(2)** introduce energy minimization before energy calculation, which is more in line with the real workflow. AbDPO shows consistent performance and demonstrates its generality. Please refer to Appendix F for related details.

### Ablation Studies

Our approach comprises three main novel designs, including residue-level direct energy-based preference optimization, energy decomposition, and conflict mitigation by gradient surgery. Thus we perform comprehensive ablation studies to verify our hypothesis on the effects of each respective design component. Here we take the experiment on one complex (PDB ID: 1a14) as the example. Here, we apply more fine-tuning steps and additionally introduce \(E_{}\) (aggregation of Res\({}_{}\)-Ag \(E_{}\) within the designed CDR), \(E_{}\) (aggregation of Res\({}_{}\)-Ag \(E_{}\)) for a more obvious and detailed comparison. More cases of ablation studies can be found in Appendix G.

Effects of Residue-level Energy Preference OptimizationWe hypothesize that residue-level DPO leads to more explicit and intuitive gradients that can promote effectiveness and efficiency compared with the vanilla DPO  as the analysis in Sec. 3.2. To validate this, we compare AbDPO with its counterpart with the CDR-level preference instead of residue-level. As Fig. 4 shows, regarding the counterpart (blue dotted line), the changes in all metrics are not obvious, while almost all metrics rapidly converge to an ideal state in AbDPO (red line). This demonstrated the effects of residue-level energy preference in improving the optimization efficiency.

Effects of Energy DecompositionIn generated antibodies, the huge repulsion caused by clashes accounts for the majority of the two types of energy. This prevents us from using the \( G\) as an optimization objective directly as the model is allowed to minimize repulsion by keeping antibodies away from antigens, quickly reducing the energies. To verify this, we compared AbDPO with a version that directly optimize \( G\). As shown in Fig. 4, without energy decomposition (green dashed line), both \(E_{}\) and \(E_{}\) quickly diminish to 0, indicating that there is no interaction between the generated antibodies and antigens. Conversely, AbDPO (red line) can minimize \(E_{}\) to 0 while maintaining \(E_{}\), which means the interactions are preserved.

Effects of Gradient SurgeryTo show the effectiveness of gradient surgery in mitigating conflicts when optimizing multiple objectives, we compare AbDPO and its counterpart without gradient surgery. As Fig. 4 shows, the counterpart (purple dashed line) can only slightly optimize CDR-Ag \(E_{}\) but incurs strong repulsion (i.e., \(E_{}\)), learning to irrational structures. AbDPO (red line) can converge to a state where CDR \(E_{}\) and \(E_{}\) achieve a conspicuously low point, suggesting the generated sequences and structures are stable, and \(E_{}\) is still significantly less than zero, showing that considerable binding affinity is kept.

Comparison with Supervised Fine-tuningSupervised Fine-tuning (SFT) can be an alternative way of generating antibodies with lower energy. For SFT, we first select the top 10% high-quality samples from AbDPO training data on a complex (PDB ID: 1a14). We fine-tune the diffusion model under the same settings as AbDPO. Results in Tab. 2 show that SFT only marginally surpasses the pre-trained diffusion model, and AbDPO performs significantly superior to SFT. We attribute the performance of AbDPO to the preference optimization scheme and the fine-grained residue-level energy rather than the entire CDR.

Figure 4: Changes of median CDR \(E_{}\), \(E_{}\), \(E_{}\), and CDR-Ag \( G\) (kcal/mol) over-optimization steps, shaded to indicate interquartile range (from 25-th percentile to 75-th percentile).

## 5 Conclusions

In this work, we rethink antibody sequence-structure co-design through the lens of energy and propose AbDPO for designing antibodies meeting multi-objectives like rationality and functionality. The introduction of direct energy-based preference optimization along with energy decomposition and conflict mitigation by gradient surgery shows promising results in generating antibodies with low energy and high binding affinity. With AbDPO, existing computing software and domain knowledge can be easily combined with deep learning techniques, jointly facilitating the development of antibody design. Limitations and future work are discussed in Appendix H.