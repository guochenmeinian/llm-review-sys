# ScaleKD: Strong Vision Transformers Could Be Excellent Teachers

Jiawei Fan

Intel Labs China

jiawei.fan@intel.com

&Chao Li1

Intel Labs China

chao3.li@intel.com

&Xiaolong Liu1

iMotion Automotive Technology

xiaolong.liu@imotion.ai

&Anbang Yao1

Intel Labs China

anbang.yao@intel.com

###### Abstract

In this paper, we question if well pre-trained vision transformer (ViT) models could be used as teachers that exhibit scalable properties to advance cross architecture knowledge distillation research, in the context of adopting mainstream large-scale visual recognition datasets for evaluation. To make this possible, our analysis underlines the importance of seeking effective strategies to align (1) feature computing paradigm differences, (2) model scale differences, and (3) knowledge density differences. By combining three closely coupled components namely _cross attention projector_, _dual-view feature mimicking_ and _teacher parameter perception_ tailored to address the alignment problems stated above, we present a simple and effective knowledge distillation method, called _ScaleKD_. Our method can train student backbones that span across a variety of convolutional neural network (CNN), multi-layer perceptron (MLP), and ViT architectures on image classification datasets, achieving state-of-the-art knowledge distillation performance. For instance, taking a well pre-trained Swin-L as the teacher model, our method gets \(75.15\%|82.03\%|84.16\%|78.63\%|81.96\%|83.93\%|83.80\%|85.53\%\) top-1 accuracies for MobileNet-V1 ResNet-500 ConvNet-T1/Mixer-S/16/Mixer-B/16/ViT-S/16/Swin-T/ViT-B/16 models trained on ImageNet-1K dataset from scratch, showing \(3.05\%|3.39\%|2.02\%|4.61\%|5.52\%|4.03\%|2.62\%|3.73\%\) absolute gains to the individually trained counterparts. Intriguingly, when scaling up the size of teacher models or their pre-training datasets, our method showcases the desired scalable properties, bringing increasingly larger gains to student models. We also empirically show that the student backbones trained by our method transfer well on downstream MS-COCO and ADE20K datasets. More importantly, our method could be used as a more efficient alternative to the time-intensive pre-training paradigm for any target student model on large-scale datasets if a strong pre-trained ViT is available, reducing the amount of viewed training samples up to \(195\). The code is available at https://github.com/deep-optimization/ScaleKD.

## 1 Introduction

**Background.** The great success of deep learning in computer vision (CV) has been driven by an explosion of neural network architectures among which convolutional neural networks (CNNs) , vision transformers (ViTs)  and multi-layer perceptrons (MLPs)  are three major model categories. While CNNs were the de facto models for about a decade, recent progress shows that large ViT models have attained state-of-the-art performance on many visual recognition tasks such as imageclassification, image segmentation, and object detection. In principle, ViTs extend the philosophy of predominant transformer architectures  in natural language processing (NLP) to vision tasks. They convert an image into a sequence of equal-sized patches treated as tokens resembling words in NLP, then apply the dot-product self-attention mechanism over the sequence of image patches. ViTs designed in this way couple with a powerful data-hungry learning paradigm: models are first pre-trained on massive datasets (with supervised or self-supervised [10; 11] or cross-modality learning [12; 13]) and then fine-tuned on target datasets (with supervised learning). As the size of ViT models or pre-training datasets increases, the pre-trained models tend to have improved generalization performance. Despite this notable model performance scalability, the pre-training process of ViTs leads to significantly huge expenses. Furthermore, large pre-trained ViTs are memory-hungry and computationally intensive, prohibiting their deployment in many resource-constrained application scenarios. In contrast, CNNs and MLPs are still widely used in industry, due to the wider availability of effective implementations and optimizations compared to ViTs.

**Motivation of This Work.** In parallel, knowledge distillation (KD) has proven to be a promising model compression pathway and has attracted lots of research interests. It relies on a teacher-student framework that transfers the knowledge learned by a large teacher model to a compact student model, aiming to make the student model can have improved performance to substitute the teacher model in deployment. However, most existing KD methods [14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35] focus on CNN architectures, and usually perform evaluation on small datasets with non-mainstream student models for industrial applications, lagging far behind the evolution of neural network architectures. Although there have been few recent efforts [36; 37; 38; 39] on using ViT teachers, they explore narrow focuses that use small ViT teachers without pre-training on massive datasets, following the ways previously studied in CNN-based KD methods. In this paper, _we attempt to connect knowledge distillation research with well pre-trained ViT models that stand out for their remarkable scalability, via a new viewpoint_. Specifically, _we question_ whether well pre-trained ViT models could be used as teachers that effectively transfer their scalable properties to target student models having different typed architectures such as CNN and MLP or heterogeneous ViT structures (we refer 'cross architecture KD' to such a more generalized formulation in this work), in the context of using mainstream large-scale visual recognition benchmarks.

**Problem Analysis.** To answer the question in our motivation, we think the knowledge transfer difficulties are rooted in the following three aspects of differences: (1) _Differences in feature computing paradigm_. In terms of semantic units, ViTs operate on a sequence of equal-sized image patches added with positional embeddings, whereas CNNs operate on regular grids of pixels. In terms of core operations, ViTs rely on self-attention operations to model global feature dependencies, whereas CNNs rely on convolution operations to model local features. Although MLPs also use a patchify stem as ViTs, they rely on fully connected operations instead of self-attention operations and do not use positional embeddings, showing inferior feature learning ability. These differences in feature computing paradigm pose the first knowledge transfer barrier to overcome. (2) _Differences in model scale_. On the micro scale, model scale differences among ViTs, CNNs, and MLPs lie in network width, network depth, building blocks, etc. On the macro scale, model scale differences come from the capability of scaling the model size for ViTs, CNNs and MLPs towards better performance and generalization ability. As a result, these differences in model scale make the capacity of different network architectures typically vary significantly, emerging as the second knowledge transfer barrier to address. (3) _Differences in knowledge density_. Under the prevalent pre-training and fine-tuning paradigm, when scaling up pre-training datasets, large ViTs usually exhibit obviously superior performance scalability than top-performing CNNs and MLPs in terms of fine-tuning on both upstream image classification tasks and downstream dense prediction tasks [40; 41]. As for knowledge distillation in this work, we assume that pre-training datasets are no longer accessible and only well pre-trained ViT teacher models are available, avoiding the expensive pre-training process and making the setting well suited for real applications. Under this context, when training student models on upstream image classification datasets like ImageNet-1K, the knowledge density between teacher and student models is different, which appears as the third barrier to handle. From the above analysis, we can conclude that the design of effective schemes to align (1) feature computing paradigm differences, (2) model scale differences, and (3) knowledge density differences between the pre-trained ViT teacher and target student models, plays the key role to attain our goal.

**Design Insights and Contributions.** Accordingly, we present Scalable Knowledge Distillation (ScaleKD), a simple and effective cross architecture KD method, which addresses the above difficulties in a progressive manner. Fundamentally, to bridge the feature computing paradigm differencesbetween ViT and the other heterogeneous architectures, we propose _cross attention projector_ (CAP, shown in Figure 1(a)), motivated by some previous works  that utilize cross attention mechanisms to align different modalities. For semantic unit differences, CAP utilizes positional embeddings and a patchify stem to transform the semantic units of CNN and MLP into transformer-like tokens. To further bridge core operation differences, CAP employs cross-attention operation and trainable queries that share the same attributes as the teacher's features to model global interdependencies on the student's features. In this way, CAP could align computing paradigm differences between the ViT teacher and the heterogeneous student in form, serving as the base component in our method.

Different from feature computing paradigm differences, model scale differences and knowledge density differences are not explicitly and separately modeled in the KD process, as they are intertwined under the prevailing pre-training and fine-tuning paradigm and are finally encoded in teacher and student models' feature space and parameter space. In light of this, we investigate both feature and parameter spaces of teacher and student models and observe two critical phenomena:

* **Feature Space:** As shown in Figure 2 and Figure 5, the frequency distributions of the features for the pre-trained ViTs are extremely imbalanced, where the direct component (zero frequency) response is dominant among all frequencies. This indicates that conducting feature distillation under such an imbalanced distribution may neglect the features of all other alternative components.
* **Parameter Space:** As the parameters of the pre-trained ViTs in the fine-tuning stage are slightly changed, their pre-training knowledge remains in the parameter space. Although the pre-training datasets are not accessible in this work, the student still has the opportunity to obtain the pre-training knowledge by aligning its parameter space to the teacher's.

Inspired by these two insightful observations, we formulate our method from two new perspectives. Based on the observation in feature space, we design _dual-view feature mimicking_ (DFM, shown in Figure 1(b)), whose key insight is to complement the neglected alternative features in the KD process. Specifically, DFM employs CAP as the feature projector and incorporates two feature mimicking paths. In the first path, DFM conducts feature mimicking in the original space to learn the teacher's global features. In the second path, by removing the direct component in frequency space, DFM highlights the subtle alternative responses in feature mimicking, thus avoiding the neglect of these features. As a result, the two paths are complementary to each other, jointly promoting the feature space alignment. Based on the observation in parameter space, we propose _teacher parameter perception_ (TPP, shown in Figure 1(c)), whose target is to transfer the pre-training knowledge by establishing a connection between teacher's and student's parameter spaces. Thanks to the aligned feature computing paradigm by CAP, TPP could bridge the student's early stages to the teacher's later stages and form a proxy feature processing path, where their parameter spaces join hands for KD optimization. By applying feature distillation in this path, the student's parameter space tends to be gradually aligned with the teacher's, and the pre-training knowledge would be transferred from the teacher to the student. Since the distillation learning processes in feature space and parameter space are the two sides of the same coin, DFM and TPP could naturally reinforce each other in essence.

Benefited from the progressive designs, CAP, DFM, and TPP can be seamlessly integrated into a neat and effective cross architecture knowledge distillation method, called _ScaleKD_, which addresses the above three problems as a whole. Although ScaleKD has multiple feature mimicking paths, they only exist in the training stage. That is, ScaleKD does not alter the student's structure and introduces no additional cost in the inference stage. By conducting systematic experiments on several mainstream large-scale vision benchmarks, we validate the effectiveness and generalization ability of our method.

Figure 1: Overview of three core components in our ScaleKD, which are (a) cross attention projector, (b) dual-view feature mimicking, and (c) teacher parameter perception. Note that the teacher model is frozen in the distillation process and there is no modification to the student’s model at inference.

Method

Given a pre-trained ViT teacher having \(m\) stages and a target student (CNN or MLP or ViT) having \(n\) stages, let \(F^{s_{i}}\) and \(F^{t_{j}}\) denote features from _i-th_ stage of the student and _j-th_ of the teacher, respectively. In what follows, we formulate all three components of ScaleKD in the form of performing feature distillation, for better clarifying their tightly coupled relationships.

### Three Core Components in ScaleKD

**Cross Attention Projector.** As shown in Figure 1(a), CAP adopts the structure of a standard transformer decoder block, consisting of a transformer decoder layer and an MLP layer, but incorporates three critical modifications. For brevity, taking CNN as an example, our modifications include: i) patchifying regular grids of pixels in CNN; ii) adding positional embeddings; iii) setting queries in the transformer decoder block as trainable variables that share the same resolution with the teacher's features. The first two modifications intend to narrow the discrepancy between different semantic units of the pre-trained ViT teacher and the CNN student, while the last modification endows the employed transformer decoder block with great flexibility to align feature semantics and spatial resolution. For MLP and ViT students, we adopt the same CAP structure as to CNN students for simple implementation but they can adapt CAP with fewer modifications when necessary. Based on these modifications, the cross-attention operation further models global dependencies on the projected student features. With CAP, the feature distillation loss is defined as:

\[_{CAP}= L(F^{t},f_{p}(F^{s};q))=||F^{t}-f_{p}(F^{s};q)|| _{2}^{2},\] (1)

where \(f_{p}\), \(q\), \(( 0)\), and \(L()\) denote the CAP, the trainable queries, the loss weight, and the \(L_{2}\)-normed distance, respectively.

**Dual-view Feature Mimicking.** As shown in Figure 1(b), building upon CAP, DFM contains two feature mimicking paths. As we stated in Section 1, the first path aims to learn the teacher's global features and the second path aims to excite and mimic the alternative features (neglected by existing KD methods). Specifically, in the first path, DFM conducts feature mimicking in the teacher's original feature space, which is formulated as: \(_{ori}= L(F^{t},f_{p_{1}}(F^{s},q_{1}))\), where \(f_{p_{1}}\) and \(q_{1}\) denote the CAP and its trainable queries in the first path, respectively. In the second path, the dominant direct component should be removed. To achieve this goal, we first employ discrete cosine transform (DCT), which maps the features from the spatial domain to the frequency domain: \(DCT:\). We then define an operator \(\) that removes direct component response from the features:

\[(x)=DCT^{-1}((DCT(x))) s.t.\ \ (z)=\{0,& z=0\\ z,&z 0..\] (2)

Next, feature mimicking in the second path is formulated as: \(_{alt}= L((F^{t}),(f_{p_{2}}(F^{s};q_{2})))\), where \(f_{p_{2}}\) and \(q_{2}\) denote the CAP and its trainable queries in the second path, respectively. Now, the feature distillation loss of DFM is formulated as:

\[_{DFM}=_{ori}+(1-)_{alt},\] (3)

where \(\) denotes the balancing weight.

**Teacher Parameter Perception.** As we stated in Section 1, TPP establishes a proxy feature processing path by connecting the student's early stages to the teacher's later stages through a CAP. In our implementation, the proxy path consists of the student's first \(n-1\) stages and the teacher's last stage, as illustrated in Figure 1(c). By feature mimicking in this proxy path, the parameters of the

Figure 2: Feature distribution of BEiT-L/14  in the frequency domain, where the direct component response is dominant. Details on drawing this figure are shown in Figure 5.

student part are gradually aligned with the parameters of the teacher part, thus enabling the transfer of the teacher's pre-training knowledge. Let \(F^{st}=_{t_{m}}(f^{st}_{p}(F^{s_{n-1}};q))\) be the output features of the proxy path, where \(g_{t_{m}}\) and \(f^{st}_{p}\) denote the teacher's last stage and the CAP in this path, respectively. The feature mimicking in the proxy path is formulated as: \(^{st}= L(F^{t},F^{st})\). We further introduce \(F^{st}\) as input-dependent queries for the CAP in the original path. This feature mimicking design aims to enhance the capability of CAP as such queries contain more teacher-related information, and its corresponding loss is formulated as \(^{s}= L(F^{t},f^{s}_{p}(F^{s_{n}};F^{st}))\). With a simple principle of equal treatment to the two feature mimicking paths, the feature distillation loss of TPP is defined as:

\[^{TPP}=^{s}+^{st}.\] (4)

### Overall Formulation

From a general perspective, the progressive designs of our above three components are naturally coupled. As CAP serves as the basic component in DFM and TPP, we further introduce how to apply DFM in TPP and get a neat formulation of our method, ScaleKD. Specifically, if treating DFM as an improved version of traditional feature mimicking, it can substitute the original feature mimicking in each path of TPP. In this way, we formulate the overall design of ScaleKD, whose loss is defined as:

\[_{ScaleKD}=_{task}+^{s}_{ori }+(1-)^{s}_{alt}}_{DFM}+ ^{st}_{ori}+(1-)^{st}_{alt}}_{DFM }+_{kd},\] (5)

where \(\) is the balancing weight, \(_{task}\) is the cross-entropy loss, and \(_{kd}\) is the vanilla logits-based KD loss  widely used in previous KD research. As the features are standardized, we set \(=1\) for loss terms in DFM as the default. Hence, our method has only one hyper-parameter \(\).

## 3 Main Experiments

We perform comprehensive experiments to systematically validate the efficacy of our method and answer the question in our motivation. Specifically, our experimental verification contains six parts: i) validating the effectiveness of our method under basic settings; ii) conducting main experiments on ImageNet-1K  (IN-1K) dataset with various student backbones and showing the promising performance gains of our method against individually trained counterparts; iii) verifying whether our method could transfer the scalable properties of the teacher to the target student; iv) conducting transfer learning on downstream tasks with MS-COCO  and ADE20K  datasets to examine whether the performance gains from our method could be well preserved; v) comparing our method with recent top KD methods; vi) showing the potential impact of our method on model engineering.

_Unless otherwise stated, in experiments, the student backbones are trained on IN-1K from scratch, without the pre-training on other upstream datasets. Experimental details are in Appendix A and B._

### Pilot Experiments under Basic Settings

As we mentioned in Section 1, ScaleKD is tailored for: i) transferring the pre-trained ViT teacher's knowledge to the student having different model architectures; ii) making the student inherit the teacher's scalability. Therefore, we first perform the following two pilot experiments.

**Cross Architecture Knowledge Distillation.** To illustrate the difficulty of cross architecture feature distillation and validate the efficacy of ScaleKD under this setting, we compare ScaleKD with traditional feature distillation (FD)  on two different cross architecture teacher-student network pairs. From the results shown in Table 1, we can observe: i) due to architecture gaps between the teacher and the student, traditional FD shows limited performance gains; ii) comparatively, our ScaleKD achieves significantly better performance, bringing 2.75%\(|\)3.22% absolute top-1 gain for ResNet-50\(|\)Mixer-S. With the above experiments, we preliminarily verify that ScaleKD could effectively handle cross architecture feature distillation, which is difficult for traditional FD.

**Large Pre-trained ViTs as Teachers.** With ResNet-50 as the student, we examine the rationality of selecting large pre-trained ViTs as teachers in ScaleKD. Specifically, we gradually scale up the teacher's model capability (first from Swin-S to Swin-B, and then to Swin-L) and perform experiments

[MISSING_PAGE_FAIL:6]

could be increased further, when choosing stronger teachers. For instance, ScaleKD brings 0.32% additional gain to ResNet-50 when changing the pre-trained ViT teacher from Swin-L to BEiT-L/14.

### The Scalable Properties from Teacher's Pre-training Data

As we introduced in Section 1, the ViT's performance scalability is related to two factors: model scale and pre-training data scale. The experiments in Section 3.1 and 3.2 have validated that our method could help the student inherit the positive performance effect from increasing the teacher's model scale. In this subsection, we focus on exploring the second factor: _whether or not our method could help the student learn the teacher's pre-training knowledge from its massive pre-training datasets, mitigating the knowledge density gap._ To examine this, we alter our baselines to models with pre-training and propose an evaluation principle: given that only IN-1K is visible, if ScaleKD can help the student model achieve similar performance as models with upstream pre-training, the answer to the above question is _Yes_. With this principle, we design a series of experiments based on the selected teachers in Table 4: Swin-L having the pre-training knowledge of IN-22K and BEiT-L/14 having the pre-training knowledge of LAION-2B. We compare the performance of the student models trained by ScaleKD and the corresponding counterparts trained by prevailing pre-training methods.

From Table 4, we can observe that ScaleKD performs better than various pre-training methods across all four kinds. Note that the superior performance of ScaleKD is achieved conditioned on not viewing any pre-training data. In other words, ScaleKD merely views 5.58\(\), 11.75\(\), 195.39\(\), and 8.73\(\) less samples than the counterpart methods based on supervised pre-training, self-supervised pre-training, cross-modal pre-training, and hybrid pre-training. Therefore, we can summarize two promising conclusions: i) our method could help the student learn the teacher's pre-training knowledge from massive datasets and mitigate the knowledge density gap; ii) if a well pre-trained large ViT is available, our method can be a more efficient alternative to the time-intensive pre-training.

   Model & Method & Training Dataset & Dataset Samples \(\) Epochs (M) & Viewed Samples (M) & Top-1(\%) \\   \\   &  & IN-22K \(\) IN-1K & 13.7\(\)90 + 1.28\(\)32 & 1274 & 83.97 \\  & & JFT-300M \(\) IN-1K & 300\(\)7 + 1.28\(\)32 & 2141 & 84.15 \\   &  & IN-1K & 1.28\(\)300 & 384 & 85.53 \\   \\   & BEiT  & IN-22K \(\) IN-1K & 13.7\(\)150 + 1.28\(\)100 & 2183 & 83.70 \\  & iBOT  & IN-22K \(\) IN-1K & 13.7\(\)320 + 1.28\(\)100 & 4512 & 84.40 \\   & ScaleKD & IN-1K & 1.28 \(\) 300 & 384 & 85.64 \\   \\  ViT-B/16 & CLIP  & LAION-2B \(\) IN-1K & 2320\(\)32 + 1.28\(\)50 & 74304 & 85.47 \\  & LAION-2B \(\) IN-12K \(\) IN-1K & 2320\(\)32 + 12.1\(\)60 + 1.28\(\)50 & 75030 & 86.17 \\  ViT-B/14 & ScaleKD & IN-1K & 1.28 \(\) 300 & 384 & 86.43 \\   \\  EVA02-S/14\(\) & EVA-02  & IN-22K \(\) IN-1K & 13.7\(\)240 + 1.28\(\)50 & 3352 & 85.80 \\   \\   

Table 4: Experiments on exploring scalable properties from the teacher’s pre-training data. We use the best reported models with different pre-training methods as our baselines to examine whether our student model has learned the teacher’s pre-training knowledge. We use Swin-L as the teacher for the first two experiments and BEiT-L/14 as the teacher for the rest two experiments. \(\) denotes transfer learning and * denotes the model is trained and tested with 384\(\) 384 sample resolution.

   Framework & Backbone & Pre-training &  &  &  \\   & & & Type-1 & _AP_ & _AP_ & _AP\({}_{S}\)_ & _AP\({}_{M}\)_ & _AP\({}_{L}\)_ & _AP_ & _AP\({}_{S}\)_ & _AP\({}_{M}\)_ & _AP\({}_{L}\)_ \\   & ResNet-50 & _Baseline_ & 78.64 & 40.2 & 23.0 & 44.3 & 52.5 & 37.1 & 18.0 & 40.1 & 54.9 \\  & Ours & 82.03 (+3.39) & 42.3 & 25.5 & 46.5 & 54.6 & 39.1 & 19.3 & 42.5 & 57.1 \\    & Swin-T & _Baseline_ & 81.18 & 42.7 & 26.5 & 45.9 & 56.6 & 39.3 & 20.5 & 41.8 & 57.8 \\    & Ours & 83.80 (+2.62) & 44.4 & 28.7 & 47.9 & 58.6 & 40.8 & 21.8 & 43.7 & 59.8 \\   

Table 5: Transfer learning results (%) on MS-COCO.

### Transferring to Downstream Tasks

To further examine whether the performance gains from our method could be well preserved in transfer learning, we conduct comparative experiments on MS-COCO for object detection and instance segmentation, and on ADE20K for semantic segmentation.

The results on MS-COCO and ADE20K are shown in Table 5 and Table 6, respectively, from which we can observe: i) overall, our pre-trained models outperform their baselines by significant margins across three downstream tasks and different architectures; ii) for semantic segmentation on ADE20K, ViT-B/16 achieves the highest 4.09% absolute performance gain across three backbones, even higher than its gain on IN-1K; iii) for object detection and instance segmentation on MS-COCO, ResNet-50\(|\)Swin-T pre-trained by ScaleKD outperforms its baseline by an _AP_ margin of 2.1%\(|\)1.7% and 2.0%\(|\)1.5%, respectively. The above observations illustrate that the performance gains from ScaleKD could be well transferred to various and challenging downstream tasks.

### Comparison with Recent Top-Performing KD Methods

As we stated in Section 1 and 2, ScaleKD is a unified design incorporating three novel focuses to align computing paradigm differences, model scale differences, and knowledge density differences, which are clearly different from existing KD methods. In order to validate the superiority of our method, we compare ScaleKD with recent top-performing KD methods.

From the results shown in Table 7, we can see: i) compared to DIST, DiffKD and OFA, although our teacher is not the best and the number of training epochs is the smallest, our ScaleKD still outperforms the best of these methods by clear margins (0.70%\(|\)1.30% on ResNet-50\(|\)Swin-T); ii) compared to FunMatch, our method even shows superior performance, outperforming FunMatch by a margin of 0.24% but only using less than 10% training epochs. As a result, in the context of transferring the scalability of the pre-trained ViT to various student models, our systematic design and its focuses show obvious superiority to previous works, paving a new path for future KD research.

### Potential Impact on Model Engineering

In Section 3.2, we have noticed ScaleKD brings significant performance gains to target students, especially for the plain design in each model category, such as ResNet, MLP-Mixer, and ViT. In parallel, model engineering is a common solution to improve the model performance. Considering these two facts, we conjure that since our method could bring competitive performance gain compared to model engineering, larger flexibility would be provided when choosing models in practice.

   Framework & Backbone & Pre-training & IN-1K (Top-1) & ADE20K (mIoU) \\   & ResNet-50 &  _Baseline_ \\ Ours \\  & 78.64 & 42.37 \\  UperNet & Swin-T &  _Baseline_ \\ Ours \\  & 81.18 & 44.41 \\   & ViT-B/16 &  _Baseline_ \\ Ours \\  & 83.80(+2.62) & 46.33 (+1.92) \\   & ViT-B/16 &  _Baseline_ \\ Ours \\  & 81.80 & 46.75 \\   & ViT-B/16 & 
 _Baseline_ \\ Ours \\  & 85.53 (+3.73) & 50.84 (+4.09) \\   

Table 6: Transfer learning results (%) on ADE20K.

   Model & Method & 
 Testher \\  & \# Epochs & Top-1 (\%) \\   & From Scratch & - & 300 & 81.18 \\  & DIST & **Swin-L (86.30)** & 300 & 82.30 \\  & DiffKD & **Swin-L (86.30)** & 300 & 82.50 \\  & ScaleKD & Swin-L (86.24) & 300 & **83.80** \\   & From Scratch & - & 300 & 78.60 \\  & DIST & Swin-L (86.30) & 450 & 80.20 \\  & DiffKD & Swin-L (86.30) & 450 & 80.50 \\  & OFA & **ViT-B (86.53)** & 300 & 81.33 \\  & ScaleKD & Swin-L (86.24) & 300 & **82.03** \\   & FunMatch & Bf-Res1522 (NA) & 1200 & 81.54 \\  & FunMatch & Bf-Res1522 (NA) & 9600 & 82.31 \\  & ScaleKD & Swin-L (86.24) & 600 & **82.55** \\   

Table 7: Performance comparison with recent top-performing KD methods. Following the settings of them, the students are trained under the advanced training strategy. Best results are **bolded**.

   Model & 
 Params (M) \\  & FLOPs (\%) & Top-1 (\%) \\   \\  ResNet-50  & 22.56 & 4.12 & 78.64 \\ ResNet-50 + ScaleKD & 22.56 & 4.12 & 82.55 \\  Conv-Net T  & 28.59 & 4.46 & 82.14 \\ RepViT2-3M  & 22.90 & - & 82.50 \\   \\  Mixer-B/16  & 59.88 & 12.61 & 76.44 \\ Mixer-B/16 + ScaleKD & 59.88 & 12.61 & 81.96 \\   \\ ResNet-B/24  & 73.00 & 15.80 & 81.60 \\   \\   \\  ViT-S/16  & 22.05 & 4.61 & 79.90 \\ ViT-S/16 + ScaleKD & 22.05 & 4.61 & 83.93 \\  Swin-T  & 73.00 & 15.80 & 81.18 \\ Swin-B  & 87.77 & 15.14 & 83.50 \\   

Table 8: Performance comparison with model engineering methods. More comparisons are shown in Table 14 in the Appendix.

To study it, we apply ScaleKD to 3 standard designs of CNN, MLP and ViT, and compare the performance with recent advanced designs. From the results shown in Table 8, we observe that our method could help these models reach better performance than advanced models. More interestingly, in Table 3, we can clearly see that the performance gap between plain designs (ResNet-50)ViT-S/16) and advanced designs (ConvNeXt-T[Swin-T) no longer exists after applying ScaleKD. These phenomena indicate ScaleKD could have a potential impact on the model selection in real applications.

## 4 Ablation Study

### Tightly Coupled Design Properties of Three Core Components

Recall that our ScaleKD consists of three core components, CAP, DFM and TPP, which are progressively designed in a tightly coupled manner. In Table (a)a, we perform an ablation study to testify their complementarity via comparing different component combinations. We can notice: i) when gradually applying more of three component designs, the performance of ResNet-50 and Mixer-S shows similar increasing trends, showing that each component of ScaleKD is not designed for specific student architecture; ii) although CAP brings the two students promising performance gains, DFM and TPP further brings ResNet-50|Mixer-S extra performance gains, 0.64%\(|\)0.75% and 1.25%\(|\)1.39% respectively, verifying that DFM and TPP are complementary to CAP; iii) when using DFM and TPP together, both ResNet-50 and Mixer-S obtain additional performance boosts, which indicates that DFM and TPP are also complementary with each other.

### Role of Each of Three Core Components

**CAP vs. Popular Feature Projectors.** We first compare CAP with two popular feature projectors, denoted as _Linear_ and _Conv_, to verify the superiority of CAP. The former projector consists of a linear layer and the latter projector consists of two 3\(\)3 convolutional layers. From the results shown in Table (b)b, we can notice that CAP outperforms the other two projectors clearly, which validates the key role of CAP: aligning computing paradigm differences towards better KD performance.

**Importance of Alternative Feature Mimicking in DFM.** The key insight of DFM is to complement the neglected alternative features in the feature mimicking process. In Table (c)c, we compare DFM with CAP and dual-path CAP to illustrate that the alternative feature mimicking is essential. We find that although the dual-path feature mimicking brings 0.25% extra performance gain to CAP, removing

Table 9: Ablation studies. Experiments in (b)-(d) are performed on Swin-S\(\)ResNet-50. As DFM and TPP are designed based on CAP, CAP is added by default when choosing DFM and TPP in (a). Because of this, we treat CAP as another baseline method, when analyzing DFM and TPP in (c)-(d).

(a) Ablation on the overall design

Figure 3: Feature distances of alternative components in the spatial domain. Details on the figure drawing are in Figure 6.

the direct component in the second path can further bring 0.39% improvement. This verifies the rationality of the design. To better understand why the performance gain is from the learning of teacher's alternative features, we make comparisons between the methods with DFM and without DFM. Specifically, we measure the distances between the student's features and the teacher's features of alternative components in the spatial domain. In Figure 3, we can clearly see that DFM can effectively reduce alternative feature distance between the teacher and the student.

**Role of Proxy Path in TPP.** Note that for transferring the teacher's pre-training knowledge in the parameter space to the student, TPP establishes a proxy path that connects the student's former stages to the teacher's later stages. In Table (d)d, we study the design of TPP and verify whether the proxy path and its adaptive queries are effective. The results show that the feature mimicking in the proxy path can provide the student with performance improvement and providing input-dependent queries can further enhance the effectiveness of TPP, which indicates that these designs in TPP are essential for learning the knowledge in the teacher's parameter space.

_More ablation studies on the hyper-parameter \(\) and the others are provided in Appendix D._

## 5 Related Work

**Knowledge Distillation.** Traditional KD methods [14; 15; 16; 17; 18; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31; 32; 33; 34; 35] generally focus on CNN-based teacher-student network pairs with small model scale gaps. Some recent works [54; 55; 50] further study how to conduct knowledge distillation with larger teachers. As vision transformers suffer from low convergence speeds, some recent works [56; 57; 58] explore leveraging CNNs to accelerate the training of vision transformers. Meanwhile, [36; 37; 38; 59] discuss how to bridge the architecture gap when the teacher and the student are in different model categories.

**Frequency-based Knowledge Distillation.** As traditional feature distillation only focuses on pixel-to-pixel differences, FAM  defines knowledge distillation in terms of frequency-based attention maps. FreeKD  explores how to eliminate unfavorable information in the frequency domain for enhancing the distillation performance on dense prediction tasks. Different from our ScaleKD, they consider feature distillation on CNN-based network pairs and have different formulations.

**Teacher Parameter Reuse.** Some previous KD methods also leverage the teacher's parameter for reusing a better classifier  or initializing the student's neck and head [62; 63; 64] or dismissing the shortcuts in residual architectures . Unlike our ScaleKD, the motivation of these works focuses on parameter reuse or equivalent substitution, rather than aligning two parameter spaces for transferring the teacher's pre-training knowledge to the target student without the pre-training process.

## 6 Conclusion

In this paper, we present ScaleKD, a new cross architecture KD approach for transferring the scalable properties of pre-trained large ViTs to various CNNs, MLPs and heterogeneous ViTs. Our method consists of three tightly coupled components that rely on principled designs to align computing paradigm differences, model scale differences, and knowledge density differences between the teacher and the student. By conducting systematic experiments on several mainstream large-scale vision benchmarks, we broadly validate the effectiveness and generalization ability of our method. Benefiting from its novel motivation and design insights, ScaleKD is the first work which successfully verified that KD can be a more efficient alternative to the time-intensive pre-training, to the best of our knowledge. This extends the application scope of KD from model compression to training acceleration. We hope our work would inspire feature KD research in this new direction.

**Limitations.** Restricted by our computational resources, we do not conduct experiments on very large teachers, such as ViT-22B , or on large students, such as ViT-L . Furthermore, with the increasing model scale of teachers, the training cost of ScaleKD increases, which is a common limitation to KD research. According to the analysis in Appendix D, the extra training cost of ScaleKD is acceptable to a large extent. Actually, thanks to its promising performance, ScaleKD shows the great potential to replace the time-intensive pre-training of students on large-scale datasets.