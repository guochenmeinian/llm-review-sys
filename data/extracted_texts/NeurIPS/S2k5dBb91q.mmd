# Transportability for Bandits

with Data from Different Environments

 Alexis Bellot, Alan Malek, Silvia Chiappa

Google DeepMind

London, UK

abellot@google.com

###### Abstract

A unifying theme in the design of intelligent agents is to efficiently optimize a policy based on what prior knowledge of the problem is available and what actions can be taken to learn more about it. Bandits are a canonical instance of this task that has been intensely studied in the literature. Most methods, however, typically rely solely on an agent's experimentation in a single environment (or multiple closely related environments). In this paper, we relax this assumption and consider the design of bandit algorithms from a combination of batch data and qualitative assumptions about the relatedness across different environments, represented in the form of causal models. In particular, we show that it is possible to exploit invariances across environments, wherever they may occur in the underlying causal model, to consistently improve learning. The resulting bandit algorithm has a sub-linear regret bound with an explicit dependency on a term that captures how informative related environments are for the task at hand; and may have substantially lower regret than experimentation-only bandit instances.

## 1 Introduction

Multi-armed bandits (MABs) constitute one of the most widely used frameworks for modeling decision-making under uncertainty. In this framework, an agent repeatedly takes actions in an environment with the goal of optimizing a desired objective, such as efficiently inferring the action with highest reward or maximizing cumulative rewards in the long run . As in most reinforcement learning problems, there is a substantial amount of exploration involved while the agent learns about reward distributions under different available actions. This process can be costly in many applications; from an ethical perspective, for example, physicians may not risk compromising their patient's health with unknown treatments. It is therefore important to be efficient with experimentation while learning an optimal policy. In the literature, _structured_ bandit instances can help navigate the exploration-exploitation trade-off effectively, for example with assumptions on the functional association between action and reward that facilitate estimation such as linear bandits  and causal bandits .

An alternative approach to alleviate the cost of active experimentation is to consider leveraging prior data or prior experimentation in related environments to inform an agent's decision-making, which leads to the _hybrid learning_ paradigm. The expectation (or rather hope) is that informative prior data or prior experimentation can serve to narrow down reward distributions and _warm start_ the MAB so as to converge to optimal actions faster and ultimately achieve higher cumulative reward. Current methods can be categorized into multi-task learning  and meta-learning . The former aims to solve a prescribed set of related bandit tasks with shared structure, _e.g._ multiple player scenarios with similar reward distributions. The latter considers an arbitrary number of bandit problems whose parameters are sampled \(i.i.d.\) from a meta-prior thatcan be inferred as the agent experiments across the different related tasks1. However, both families of methods assume a relatively restricted class of potential changes across environments and rely explicitly on agent experimentation across all environments for learning. If discrepancies across environments are more general, naively leveraging prior data does not necessarily lead to more informative reward distributions or efficiency improvements in a new environment.

As a concrete example, consider a learning scenario in which historical data is available for the design of a clinical trial2 aiming to determine the optimal level of a hypertension treatment \(X\) for Alzheimer's disease \(Y\)3. Alzheimer's aetiology is complex but it is well established that a patient's age \(Z\) and blood pressure \(W\) contribute to the development of the disease, and so do a number of (typically) unobserved factors, _e.g._, physical activity levels, socio-economic status, diet patterns, etc.  (encoded with a bi-directed dashed edge). Such data can be useful but has to be handled with care, especially if we suspect the clinical trial population to differ in several aspects from that recorded in historical data. We would expect, for example, age distributions \(P(z)\) to differ. Fig. 1 graphically describes this scenario. A naive approach, ignoring the differences across populations, can be sub-optimal. Take an instance where variables \(X,Z,W,Y,U\{0,1\}\); their values are decided by functions: \(X U,W X,Y Z(W U)\); \(\) represents the exclusive-or operator; \(U\) is independently distributed in \(\{0,1\}\) but older individuals are historically over-represented \(P(Z=1)=0.6\) in comparison to the clinical trial population \(P(Z=1)=0.4\). Historical data suggests that the better policy involves a lower dosage \(X=0\), as \([Y do(X=0)]=0.6>0.5=[Y do(X=1)]\), which is the opposite of what is optimal for the clinical trial population.

This example shows that differences across environments may be complex, subtle, and non-trivially influence optimal decision-making. Even when one is able to perfectly estimate reward distributions from historical data, the induced policy can still be sub-optimal depending on the location and magnitude of the changes expected across environments. In general, reward distributions will not straightforwardly extrapolate across different environments. In this paper, we attempt to capture this (structural) uncertainty through a causal lens. In the causality literature, this problem appears under the rubric of _transportability theory_[32; 3; 11]; several criteria, algorithms, and estimation methods have been developed for identifying when and how a causal effect can be computed across environments. Our task, in the bandit setting, is to define a learning agent that optimally exploits prior data with knowledge of the potential discrepancies across domains, for _any_ given graph. Prior work  has considered specific instances, _i.e._ environments defined by the Bow and Instrumental Variables graphs, but a general approach applicable to arbitrary graphs and arbitrary differences across environments is still missing. Our approach is Bayesian, and involves posterior sampling of reward distributions defined by a parameterization informed by the underlying causal structure. Our contribution is to develop a novel bandit algorithm that achieves sub-linear cumulative regret with an explicit dependency on the entropy of an inferred prior, a quantity that implicitly captures the relatedness between environments. The significance of this result is that it guarantees consistent improvements on performance over methods not leveraging prior data. To the best of our knowledge, this is one of the first general attempts to consistently use prior data from related environments in general decision-making scenarios in which causal dependencies can be established.

### Preliminaries

We use capital and small letters to denote random variables and their values respectively, _e.g._\(X\) and \(x\), and bold capital and small letters to denote sets of variables and their values, _e.g._\(\) and \(\). The domain of variable \(X\) is indicated with \(_{X}\).

A environment's data generating mechanism is described by a _structural causal model_ (SCM) [31, Definition 7.1.1]. A SCM \(M\) is a tuple \(,,,P()\), where \(\) is a set of endogenous (observed) variables, \(\) is a set of exogenous latent variables, and \(=\{f_{V}\}_{V}\) is a set of functions

Figure 1: Diagram encoding causal structure and differences across environments.

such that \(f_{V}\) determines values of \(V\) taking as argument variables \(_{V}\) and \(_{V}\), _i.e._\(V f_{V}(_{V},_{V})\). Values of \(\) are drawn from an exogenous distribution \(P()\). We assume the model to be recursive, _i.e._ that there are no cyclic dependencies among the variables, such as to define a distribution \(P()\) over endogenous variables \(\). An intervention or action by an agent on a subset \(\), denoted by \(do()\), is an operation that fixed values of \(\) to constants \(\), replacing the functions \(\{f_{X}:X\}\) that would normally determine their values. Let \(M_{}\) denote the model induced by action \(do()\). Accordingly, \(M_{}\) induces a corresponding interventional distribution over \(\), denoted \(P(_{}):=P( do())\). We will consistently use \(X,Y\) as designated action and reward variables, respectively.

Causal graphs \(=(,)\) describe the functional associations in an underlying SCM \(M\). In particular, we draw a _directed edge_ between two variables \(V W\) if \(V\) appears as an argument of \(f_{W}\) in \(M\), and a _bi-directed dashed edge_ between two variables \(V\)\(\)\(W\) if \(_{V}_{W}\), _i.e._\(V\) and \(W\) share an unobserved _confounder_. We will use standard family conventions for graphical relationships, _e.g._ parents \(pa()_{}:=_{X}pa(X)_{}\) of a set of nodes \(\) are all nodes in \(\) with directed edges into elements of \(\). Its capitalized version \(Pa\) includes the argument as well, _e.g._\(Pa()_{}:=pa()_{}\). We will make use a special clustering of the nodes in \(\) called \(c\)-components : two nodes are in the same \(c\)-component \(\) if and only if they are connected by a bi-directed path. \(c\)-components form a partition over exogenous variables: a \(c\)-component \(\) is said to cover an exogenous variable \(U\) if \(U_{V}_{V}\). We denote with \(_{U}\) the \(c\)-component covering \(U\). As an example, the diagram in Fig. 1 has \(c\)-components \(\{X,Y\}\), \(\{W\}\), and \(\{Z\}\); and \(_{U_{XY}}=\{X,Y\}\), \(_{U_{W}}=\{W\}\), and \(_{U_{Z}}=\{Z\}\). We refer the reader to [31, Chapter 7] for a more detailed review of SCMs.

## 2 Bandits with Transportability

From the agent's perspective, the point of departure with respect to conventional bandit instances is that in addition to the ability to take actions in a deployment environment \(^{*}\), the agent has access to data from one or more related environments \(^{a},^{b},\), each characterized by SCMs \(M^{a},M^{b},\). We assume that all environments have the same scope, _i.e._ the same sets \(\) and \(\), but may differ in _any_ other aspect. In the transportability literature , such structural differences between environments are called domain discrepancies and can be encoded in selection diagrams.

**Definition 1** (Domain Discrepancy).: _Let \(^{a}\) and \(^{b}\) be two domains with SCMs \(M^{a}\) and \(M^{b}\). There exists a domain discrepancy between \(^{a}\) and \(^{b}\) if \(f_{V}^{a} f_{V}^{b}\) or \(P^{a}(_{V}) P^{b}(_{V})\) for some \(V\)._

**Definition 2** (Selection diagram).: _Given domain discrepancy set \(^{a,b}:=\{V:f_{V}^{a} f_{V}^{b}P^{a}(_{V}) P^{b}(_{V})\}\) between two domains \(^{a}\) and \(^{b}\) and a causal graph \(^{a}=(,)\), let \(=\{S_{V}:V^{a,b}\}\) be called selection nodes. The graph \(^{a,b}=(,\{S_{V} V\}_{S_{V} })\) is called selection diagram._

Selection nodes indicate where structural discrepancies between two environments might take place. The absence of a selection node pointing to a variable represents the assumption that the causal mechanism responsible for assigning values to that variable is identical in both environments. In the clinical trial example, Fig. 1 shows a selection diagram comparing historical and clinical trial environments, denoted \(^{*},^{a}\) respectively; the presence of selection node \(S_{Z}\) indicates a potential difference in the assignment of \(Z\), _i.e._, either \(f_{Z}^{*} f_{Z}^{a}\) and / or \(P^{*}(_{W}) P^{a}(_{W})\). On the other hand, the absence of _e.g._ selection node \(S_{Y}\) indicates the assumption \(f_{Y}^{*}=f_{Y}^{a}\) and \(P^{*}(_{Y})=P^{a}(_{Y})\). With this formalism, the task is to leverage data from related environments in a consistent and efficient manner.

**Definition 3** (Bandits with Transportability).: _Let \(^{*}\) denote the deployment environment in which the agent acts. Given samples from \(P^{a}(),P^{b}(),\) and selection diagrams \(^{*,a},^{*,b},\), in each round \(t=1,,T\) the agent takes an action \(x^{(t)}\) and observes a sample from \(P^{*}(_{x^{(t)}})\), adjusting its actions to minimize (expected) cumulative regret in \(^{*}\),_

\[_{P^{*}}R_{T}:=_{t=1}^{T}_{P^{*}}Y_{}-_{P^{*}}Y_{x^{(t)}},\] (1)

_that compares the optimal intervention \(=_{x_{X}}_{P^{*}}Y_{x}\) with the agent's chosen intervention in each round._Quantities such as \(_{P^{*}}Y_{x}\) or \(P^{*}(y_{x})\) are called _transportability queries_ and their estimation with prior data, underlying structural assumptions and their use within active experimentation schemes will be the focus of this paper.

### Informative priors for bandits

Before experimentation takes place there is a degree of _unidentifiability_ of reward distributions \(P^{*}(y_{x})\) depending on causal assumptions and discrepancies between environments. For instance, revisiting the clinical trial example, if age distributions are allowed to vary arbitrarily across environments, values of \(P^{*}(y_{x})\) will similarly vary and thus involve a degree of uncertainty4. This unidentifiability feature is relevant even without major discrepancies across domains as \(P^{*}(y_{x})\) may still be unidentifiable in the presence of unobserved confounders. The Bow graph in Fig. 2 (ignoring the selection node) is a common example.

One may be tempted to conclude that prior data is rarely useful. However, even under multiple discrepancies across environments, causal effects \(P^{*}(y_{x})\) are rarely completely unconstrained. In general, causal effects lie in a non-trivial interval \([a,b],0<a b<1\). For instance, for the graph \(^{*,*}\) in Fig. 2\(P^{*}(y_{x})\) can be shown to be contained in \(P^{a}(x,y),P^{a}(x,y)+1-P^{a}(x)\). In particular, with a probabilistic or Bayesian interpretation of unknown quantities in a SCM, that is with an explicit probability measure over SCMs \(M(^{*})\)5, one can define a _distribution_ of reward probabilities \(P^{*}(y_{x})\) (or expected rewards) that honestly captures prior uncertainty in their values. For example, given prior samples \(}^{a}=\{^{a}_{(j)}:j=1,,500\}\) independently drawn from a source distribution \(P^{a}()\) (and a particular prior model over SCMs, to be discussed in Sec. 2.2), a posterior density over \(_{P*}Y_{x}\) can be evaluated and sampled from, see Fig. 2 for an illustration, where theoretical bounds are shown with vertical lines.

From this perspective, the problem of doing inference on reward distributions given prior data and knowledge of structural discrepancies can be formulated as an optimization problem, that is, evaluate,

\[P_{M(^{*})}\,_{P_{M}}Y_{x} \;\;|\;\;}\,, V ^{*,i}:f^{*}_{V}=f^{i}_{V},P_{^{*}}(_{V})=P_{^{i}}(_{V }),\] (2)

where \(}:=\{}^{a},}^{b},\},}^{i}=\{ ^{i}_{(j)}:j=1,,n_{i}\}\) is a set of \(n_{i}\) independent samples from \(P^{i}()\). In words, the task is to evaluate a distribution over expected rewards under intervention over all deployment domains \(^{*}\) compatible with our knowledge prior to experimenting in \(^{*}\).

It remains a question, however, how to define a model and parameterization of SCMs. Remember that only prior data and selection diagrams are assumed to be available to the researcher; any choices on the distribution of exogenous variables \(P()\) or functional form of deterministic structural assignments \(\) represent untestable assumptions that are difficult to justify in practice. Going forward we will restrict ourselves to SCMs with _discrete_ endogenous variables (while exogenous variables may be arbitrarily defined, _e.g._ continuously-valued with arbitrary probability density functions).

### General parameterization of reward distributions

Systems of discrete observables have the distinctiveness of involving a finite number of probabilities of the form \(P^{*}(y_{x})\), _i.e._ one for each combination \((x,y)\). Reward distributions \(P^{*}(y_{x})\) in any underlying SCM \(M\), however complex \(P()\) and \(\) may be, can logically be equivalently expressed by a corresponding _discrete_ SCM \(N\) in which \(P()\) is discrete and \(\) is a discrete mapping between finite spaces [44; 6]. This observation is interesting because it allows us to consistently and uniquely parameterize \(P()\) and \(\), without untestable choices on their form6.

Figure 2: Bow graph and posterior reward density.

**Corollary 1** (Proposition 2.7. ).: _For any causal graph \(\), let \(M\) be an arbitrary SCM compatible with \(\). For any sets \(,\), the interventional distribution \(P(})\) could be parameterized as_

\[_{\{\}}_{u=1,, d_{U},\\ U}_{V}\{_{V}^{( }}_{V},_{V})}=v\}_{U}_{u},\] (3)

_where \(_{u}:=P(U=u)\) defines exogenous probabilities of discrete variables \(U\) with cardinality \(d_{U}=_{V_{U}}|_{}}(V)}|\); and each \(_{V}^{(}}_{V},_{V})}\) is a deterministic mapping between finite domains \(_{}}_{V}}_{_{V}}_{V}\)._

For example, \(P^{*}(y_{x})\) in Fig. 1 can be parameterized by

\[_{w,z,u_{z},u_{w},u_{xy}}\{_{Y}^{(w,z,u_{xy})}=y\}\{_{W}^{(x,u_{w})}=w\}\{_{Z}^{(u_{z})}=z\}_{u_{xy}} _{u_{z}}_{u_{w}},\] (4)

where, assuming \(X,Y,Z,W\) are binary, \(_{u_{z}}\) is a discrete distribution over a finite domain \(\{1,2\}\) since \(|_{U_{z}}|=|_{Pa(Z)}|=|_{Z}|=2\), \(_{u_{w}}\) is a distribution over \(\{1,,4\}\) since \(|_{U_{W}}|=|_{Pa(W)}|=|_{X}||_{Z}|=4\), and \(_{u_{xy}}\) is distribution over \(\{1,,32\}\) since \(|_{U_{X}Y}|=|_{Pa(X)}||_{Pa(Y)}|=|_{Z}|| _{X}||_{W}||_{Z}||_{Y}|=32\). Cocl. 1 guarantees that for any value of \(P^{*}(y_{x})\) induced by an arbitrary SCM \(M\) there exists a combination of parameters in Eq. (4) that reaches that exact same value. In other words, this parameterization is sufficiently expressive to encode _any_ underlying reward distribution.

Parameters that define reward distributions are specific to the deployment environment \(^{*}\). The key observation, however, is that some of them can be inferred with prior data whenever there exists an invariance in \(P()\) or \(\) across environments as there exists a one-to-one relationship between model parameters and structural features of the underlying SCM. For example, given Fig. 1, the absence of a selection node into \(Y\) implies that both functional assignments and exogenous probabilities of \(Y\) agree across environments, that is \(_{Y}^{*}=_{Y}^{*},_{u_{Y}}^{*}=_{u_{Y}}^{*}\). Both may thus be approximated with prior data which in turn constraints or informs \(P^{*}(y_{x})\) even if other parameters in its expression in Eq. (4) remain unknown. The result is a non-trivial distribution over reward probabilities that may be used to warm-start bandit algorithms, even before any experimentation takes place.

### Bandit algorithms

To exploit non-trivial parameter distribution given prior data, a bandit algorithm can be designed to choose actions in proportion to the probability that an intervention leads to highest reward, also known as posterior or Thompson sampling [38; 2]. Specifically, at a particular round \(t\) of experimentation in the deployment domain \(^{*}\), _posterior_ parameter distributions \(P(,},_{x^{(1)}},,_{ x^{(t-1)}})\) can be evaluated to exactly capture uncertainty given both prior and experimental data up to round \(t\). Action \(x^{(t)}\) is then chosen according to the one that gives highest reward, _i.e._ arg max \({}_{x}\)\(_{P^{*}}[Y_{x}^{(t)},^{(t)}]\), where \((^{(t)},^{(t)})\) is an independent draw from its posterior distribution. In other words, the agentperforms natural Bayesian updates based on both the data available in source environments and its own experimentation as interventional samples \(_{x^{(1)}},,_{x^{(t-1)}}\) become available, matching the intuition of most other Thompson sampling bandits in the literature. The full algorithm, called Thompson sampling with Transportability (tTS), is given in Alg. 1.

## 3 Regret guarantees conditional on prior data

We define information-theoretic regret bounds that aim to capture the exploration-exploitation trade-off for Alg. 1 when prior information allows it to infer parts of the environment before experimentation takes place.

Performance in MABs is, to a large extent, intimately related with the agent's uncertainty about which action is optimal, represented by a random variable \(:_{}_{}_{X}\) where \(_{}_{}\) defines the space of all models \((^{*})\) consistent with our knowledge of the deployment environment \(^{*}\). For example, \(P_{M(^{*})}(=x)=P_{M( ^{*})}(_{P_{M}}[Y_{x}]>_{P_{M}}[Y_{x^{}}], x^{}_{X}\{x\})\) where \(P_{M(^{*})}\) is a probability mass function defined over \((^{*})\). It is reasonable to assume that one would only choose actions with large regret when it can reduce the uncertainty in \(\) substantially. Following [35, Sec. 5], we define a scalar \(_{t}\)8 such that the per-round regret can be bounded by information gain,

\[[Y_{}-Y_{X^{(t)}}}_{t},}] _{t}}_{t},})}(; _{X^{(t)}})},\] (5)

where \(}_{t}:=\{_{X^{(1)}},,_{X^{(t-1)}}\}\) denotes the agent's history of interactions with \(^{*}\) up to round \(t\), and \(I_{P}(X,Y):=_{KL}(P(X,Y)\|P(X)P(Y))\) denotes the filtered mutual information defined based on \(P\) (where \(_{KL}\) is the Kullback-Leibler divergence). Expectations, unless otherwise stated, are taken with respect to all random quantities. The following proposition, extended from [35, Prop. 1], shows that the Bayesian regret of an agent acting according to Alg. 1 is sub-linear with a dependency on the entropy of the optimal action \(\).

**Proposition 1**.: _Let \(R_{T}\) denote the regret incurred by following Thompson sampling (Alg. 1). For any \(T\) and \(_{t}\), then_

\[[R_{T}}]( })T},\] (6)

_where \((})\) is the conditional entropy of \(\) given \(}\)._

Proofs are given in Appendix C.

This bound is interesting because it cleanly relates the regret with the uncertainty about the optimal action conditioned on prior data. On one extreme, if data from source environments fully characterizes the optimal action, the entropy equals 0, and no further experimentation is required; on the other extreme, if data from source environments have no relationship with the target query, the entropy equals \((|_{X}|)\), and the bound reverts to conventional worst-case guarantees . The entropy of the optimal action is often not sufficient to capture the information from \(}\) as \(\) may still have a uniform distribution even though posterior distributions over \((,)\) have tightened. In other words, there is additional structure among different reward distributions that is not captured by the entropy of the optimal action. Such a setting can be analyzed with a different assumption on the per round regret that explicitly considers model parameters \((,)\) to quantify information gain,

\[[Y_{}-Y_{X^{(t)}}}_{t},}] _{t}}_{t},})}(, ;_{X^{(t)}})}+_{t}.\] (7)

where \(_{t}>0\) is an additional slack term. Accordingly, the following proposition provides an alternative bound using a conditional analogue of [25, Prop. 2].

**Proposition 2**.: _Let \(R_{T}\) denote the regret incurred following the policy defined by Alg. 1. For any \(T\), if Eq. (7) holds with \(_{t}\) for all \(t\),_

\[[R_{T}}]} )}(,;_{X^{(1)}},,_{X^{(T)}})}+ _{t=1}^{T}[_{t}].\] (8)This proposition shows that if prior data allows the agent to concentrate \((,)\) around some value, additional experimentation does not provide much more information and the regret should be small. In principle, it is possible to get precise per-round regret values by inferring values for \(_{t}\) and \(_{t}\) through the construction of concentration inequalities for the reward variable, as done by Lu et al. in [25, Lem. 3]. We adapt this result using conditional information-theoretic quantities in the following proposition.

**Proposition 3**.: _Fix \(>0\) and choose \(_{t}\) such that \(Y_{x}-[Y_{x}}_{t},}] }{2}}_{t},})}(,;Y_{x})}\) for all \(x_{X}\) simultaneously with probability greater than \(1-\). Then Alg. 1 chooses actions \(X^{(t)}\) that satisfy_

\[[Y_{}-Y_{X^{(t)}}}_{t},}] _{t}}_{t},})}(,;_{X^{(t)}})}+ B,\] (9)

_where \(B 0\) is such that \(_{y,y^{}_{Y}}y-y^{} B\)._

## 4 Posterior approximations

This section describes a tractable algorithm to evaluate posterior distributions of the form \(P(,},_{x^{(1)}},,_{x^{(t-1 )}})\) and its posterior updates when new data is collected. Priors on \(,\) may be defined such as to induce tractable conditional distributions that may be used within a Gibbs sampling framework. The Gibbs sampler starts with some initial value for all latent quantities \((,,)\) in our target expected reward

\[_{P}[Y_{x}]=_{y_{Y}}yP^{}(Y_{x}=y)=_{ \{x\}}y_{u=1,,d_{U},\\ U}_{V X}\{_{V}^{( _{V},_{V})}=v\}_{U}_{u},\] (10)

and samples each one iteratively using their conditional distributions, each parameter conditioned on the current values of the remaining terms in the parameter vector and the available data . As mentioned, what data point carries information about which parameters depends on the structural differences between environments.

**Prior.** For every \(V, p_{V},_{V}\), the functional assignment parameters \(_{V}^{(_{V},_{V})}\) are drawn uniformly in the discrete domain \(_{V}\). For every \(U\), exogenous probabilities \(_{U}\) with dimension \(d_{U}=_{V_{U}}_{P_{a}(V)}\) are drawn from a prior Dirichlet distribution (here chosen for conjugacy with the categorical distribution of \(\)), \(_{U}=(_{1},,_{d_{U}}) Dir(_{1},, _{d_{U}})\), with hyperparameters \(_{1},,_{d_{U}}\).

**Posterior.** In a particular round \(t\), the Gibbs sampler iterates over the following steps.

1. _Sample_ \(\). We start by sampling a corresponding exogenous latent variable for each observed sample \(^{(n)}(},_{x^{(1)}},,_{x^{(t-1)}}),n=1, ,t-1+_{i}n_{i}\). Let \((},_{x^{(1)}},,_{x^{(t-1)}})\) denote the corresponding set of samples of \(\). Exogenous variables \(^{(n)}\) are mutually independent given \(^{(n)},,\) and thus we can sample each separately using the conditional \[P(^{(n)}^{(n)},,) P(^{(n)}, ^{(n)},)=_{V}\{_{V}^{ (_{V}^{(n)},_{V}^{(n)})}=v^{(n)}\}_{U}_{u}.\] (11)
2. _Sample_ \(\). Similarly, for fixed \(p_{V},_{V}\), parameters \(_{V}^{(_{V},_{V})}\) are mutually independent given \(},_{x^{(1)}},,_{x^{(t-1)}},},_{x ^{(1)}},,_{x^{(t-1)}},\). As mentioned, each parameter is updated with the subset of \((},_{x^{(1)}},,_{x^{(t-1)}},},_{x ^{(1)}},,_{x^{(t-1)}})\) associated with environments in which the functional assignment of \(V\) is invariant across source and deployment environments. As they represent a mapping between variables, its conditional distribution is given by \(P(_{V}^{(_{V},_{V})}=v},_{x^{(1)}},, _{x^{(t-1)}},},_{x^{(1)}},,_{x^{(t-1)}})=1\) if there exists a (relevant) sample \((v^{(n)},_{V}^{(n)},_{V}^{(n)})\) that fixes the mapping \(_{V}^{(n)},_{V}^{(n)} v^{(n)}\). Otherwise, \(P(_{V}^{(_{V},_{V})}=v},_{x^{(1)}}, ,_{x^{(t-1)}},},_{x^{(1)}},,_{x^{(t-1)}})\) is sampled uniformly from a discrete distribution over \(_{V}\).
3. _Sample_ \(\). Fix \(U\). By conjugacy of Dirichlet distributions with the categorical distribution, its conditional distribution given all other quantities is given by a Dirichlet distribution \(},_{x^{(1)}},,_{x^{(t-1)}},},_{x^{(1) }},,_{x^{(t-1)}} Dir(_{1},,_{d_{U}})\) where \(_{j}:=_{j}+_{n}\{u^{(n)}=u_{j}\}\), and, similarly, \(n\) iterates over the samples \((},_{x^{(1)}},,_{x^{(t-1)}})\) associated with the subset of environments in which exogenous probabilities match the deployment environment.

This procedure eventually forms a Markov chain with the invariant distribution \(P(,,},}_{x^{(1)}},, }_{x^{(t-1)}})\). We plug in one of these samples \(, P(,},}_{ x^{(1)}},,}_{x^{(t-1)}})\) into Eq. (10) for different \(x\) to choose the next action \(x^{(t)}\). This sample initializes the chain for \(P(,,},}_{x^{(1)}},, }_{x^{(t)}})\) in round \(t\) of Alg. 1.

So far, we have described algorithms, approximations, and regret guarantees that pre-suppose the correct specification of causal and selection diagrams across multiple domains. Some degree of misspecification, however, can be tolerated without voiding guarantees on performance improvements. We discuss more details in Appendix B.1.

## 5 Experiments

We evaluate the proposed approach on several synthetic scenarios inspired by the literature on clinical trials and advertising. We compare Thompson sampling with additional data sources (tTS, Alg. 1) with Thompson sampling with uninformative priors (TS) , a KL-UCB  algorithm with uninformative priors (UCB), and as a baseline also include the algorithm that chooses actions uniformly at random (Uniform)9. For all algorithms, we measure their regrets \(R_{T}\), averaged over \(10\) repetitions. Details on all data generating mechanisms and a discussion on mis-specification and limitations of the proposed approach can be found in Appendix D and Appendix B, respectively.

**Experiment 1.** We start by evaluating the usefulness of prior data by comparing learned distributions of expected reward with and without access to prior data. We consider a bandit problem with action, reward and contextual variables \(X,Y,Z\{0,1\}\), respectively, characterized by Fig. 2(a) in which 1000 prior data samples are given from an environment \(^{a}\) that differs in the causal assignment of \(Z\) in comparison with the deployment environment \(^{*}\). Specifically, with this model,

\[P^{*}(y_{x})=_{z,u_{z},u_{xy}}\{_{Y}^{\{x,z,u_{xy}\}}=y\} \{_{Z}^{\{u_{x}\}}=z\}_{u_{z}}_{u_{xy}},\] (12)

where \((_{Y},_{u_{xy}})\) are invariant across environments while \((_{Z},_{u_{x}})\) are specific to the deployment environment. We start by considering Fig. 2(b) that gives samples from \(_{P^{*}}[Y_{x}]_{x^{(1)}},,_{x^{(t)}}\) as a function of experimentation rounds \(t\), that is without making use of prior data \(}\). Distributions of expected rewards under action \(x=0\) and \(x=1\) overlap substantially until round \(t=300\) at which point \(x=1\) is inferred to lead to higher expected rewards. Fig. 2(c) gives a similar plot with the exception that the left part of the plot gives prior samples \(_{P^{*}}[Y_{x}]}\) illustrating the shape of the expected reward distribution learned from prior data only. In particular, we observe \(_{P^{*}}[Y_{x=1}]}\) concentrated in the interval \([0.3,0.9]\) and \(_{P^{*}}[Y_{x=0}]}\) concentrated in the interval \([0.1,0.8]\). The

Figure 3: Performance figures related to Experiment 1.

agent starts experimentation at round \(t=0\) with this prior and from then onward expected reward samples are drawn from \(_{P*}\![Y_{x}],_{x^{(1)}},,_{x^ {(t)}}\) as a function of experimentation round \(t\). The bandit algorithm with prior data is remarkably more efficient, being able to determine \(x=1\) as the superior action after only 80 rounds of experimentation. Overall, prior data leads the bandit algorithm to pull the optimal arm in \(99\%\) of time versus \(93\%\) of the time without prior data.

**Experiment 2.** We revisit our introductory example to quantify the benefit of leveraging historical patient data from various hospitals. In this example, the objective is to infer the optimal level of hypertension medication \(X\). We are given a choice among 5 different levels, _i.e._\(|_{X}|=5\), and wish to increase the probability of the presence of a beneficial biomarker \(Y\), _i.e._\(|_{Y}|=2\), in the clinical trial population \(\)* given that we have prior observational data in a different hospital \(^{a}\). The selection diagram describing this causal protocol is given in Fig. 1. Regret comparisons for all algorithms are given in Fig. 4 (LHS). We observe a significant gain in performance by tTS that chooses the optimal intervention in \(67\%\) of the rounds in contrast with \(35\%\) of the rounds for TS, and \(28\%\) of the rounds for UCB (and \(17\%\) for an algorithm choosing interventions at random).

We use this example also to illustrate empirically the dependence between regret and prior entropy shown in Prop. 1 (RHS). For this, we consider different prior beta distributions for \(P*(W=1 x)\), specifically with increasing standard deviations around the true value of \(P*(W=1 x)\) for each \(x_{X}\). A larger standard deviations implies a less informative prior and higher entropy of the random variable \(\) that denotes the optimal action. The entropy of \(\) takes values in the interval \([0.2,1.4]\) for assumed Beta priors for \(P*(W=1 x)\) with standard deviations in the interval \([0.001,0.1]\). Fig. 4 (RHS) demonstrates empirically the influence of the entropy of \(\) on the expected cumulative regret given in Prop. 1. In particular, narrower, more informative priors lead to better regret.

**Experiment 3.** This example considers an advertiser seeking to optimize which ads to show visitors on a particular website. For each visitor, we choose one out of a collection of 6 ads \(X\), \(|_{X}|=6\), some of which will be more engaging than others, to ultimately optimize whether a user clicks \(Y\{0,1\}\). Each ad has some theoretical but unknown click-through-rate \(P*(y_{x})\).

In this example, we assume access to 500 data points from an ad recommendation system used on a different website, _i.e._ a different environment \(^{a}\). There, the effect of an add \(X\) on the number of clicks \(Y\) is confounded by the user's age \(A\), (here categorized into old and young such that \(_{A}=\{0,1\}\) and \(|_{A}|=2\)) and the user's product preferences level \(W\), which interacts with current ad-recommendation system through a user's browsing history \(Z\) and location (not observed and therefore represented with a bi-directed edge between \(W\) and \(X\)). Moreover, in this example, the relationship between \(W\) and \(Y\) is itself confounded by unobserved factors. The population visiting the website of interest \(\)*, where the MAB will be deployed, is known to agree on all causal components with \(^{a}\) except on the distribution of age \(A\). This causal protocol as well as regret comparisons for this example are shown in Fig. 5. We observe noticeable improvements in regret with prior data and knowledge of structural differences as tTS substantially improves over algorithms agnostic of prior data.

## 6 Conclusions

This paper investigated the problem of improving the efficiency of multi-armed bandits using batch data from related environments. As source environments might differ, some knowledge of structure and (potential) discrepancies are necessary to extrapolate consistently. This paper demonstrated that knowledge of selection diagrams that encode causal influence as well as potential discrepancies

Figure 4: Performance figures related to Experiment 2.

Figure 5: Performance figures related to Experiment 3.

across source and target environments, without, however, an explicit specification of functional form and distributions, is sufficient to consistently define an informative prior over reward distributions using data from arbitrary environments. The resulting algorithm guarantees improvements in regret in comparison to algorithms agnostic of prior data. To our knowledge, this serves as one of the first principled approaches to consistently leverage prior data in the context of bandits and we hope it can pave the way for developing more general transfer learning methods in reinforcement learning.