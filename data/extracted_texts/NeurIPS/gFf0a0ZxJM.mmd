# OpenAGI: When LLM Meets Domain Experts

Yingqiang Ge

Rutgers University

&Wenyue Hua

Rutgers University

&Kai Mei

Rutgers University

&Jianchao Ji

Rutgers University

&Juntao Tan

Rutgers University

&Shuyuan Xu

Rutgers University

&Zelong Li

Rutgers University

&Yongfeng Zhang

Rutgers University

{yingqiang.ge,wenyue.hua,kai.mei,jianchao.ji,juntao.tan,shuyuan.xu,zelong.li,yongfeng.zhang}@rutgers.edu

###### Abstract

Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce **OpenAGI**, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard _benchmark tasks_ for benchmarking and evaluation, and _open-ended tasks_ including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.

## 1 Introduction

The acquisition and reuse of skills is a fundamental aspect of human intelligence that enables the formation of complex skills to address novel or intricate problems [19; 4; 57]. We posit that machine intelligence should incorporate this capacity to synthesize various skills by composing them into complex skills for complex task-solving. In computer science parlance, each skill is referred to as a domain expert "model" - a reusable tool, module, network, plugin, or API with a defined function. The domain expert models can be synthesized into a larger "plan" for performing more complex tasks. The model synthesis process is adaptable to the input or task, such that for a given task, the models are synthesized into the most suitable plan to address the task at hand. As a result, different inputs or tasks may necessitate distinct synthesized models as a plan for task-solving.

Recent advances in Large Language Models (LLMs) have showcased exceptional learning and reasoning capabilities, rendering them well-suited for selecting, synthesizing, and executing external expert models to address complex tasks. These LLMs, such as GPT series [32; 2], LLaMA series [45; 44] and T5 series [33; 8], have exhibited a profound understanding of natural language and the

[MISSING_PAGE_EMPTY:2]

abilities, we open-source all code and datasets, and hence, name this platform **OpenAGI**. A toy example, showing the entire pipeline of OpenAGI, is depicted in Fig. 1. Specifically, 1) a natural language instruction of a specific task is given; 2) the instruction is augmented by manually designed prompt and then fed as input into LLM to generate a plan; 3) the expert models are selected and synthesized based on the generated plan, and subsequently executed to process the data samples; 4) the task-solving ability of the LLM can be evaluated by comparison between the output and the ground-truth labels or through human evaluation.

OpenAGI embodies a dual approach to address diverse requirements-**benchmark tasks** and **open-ended tasks**. On the one hand, we have incorporated benchmark tasks, each supported by task-specific datasets and evaluation metrics. This inclusion provides researchers with a consistent platform to assess and compare the performance of various models, stimulating continuous improvement and competitive innovation. For benchmark tasks, as depicted in Fig. 1, we utilize a selection of expert models derived from esteemed libraries such as Hugging Face's transformers and diffusers, as well as from GitHub repositories, thereby easily facilitating the expansion of our model set. Additionally, the datasets have been meticulously selected to align with or resemble the training datasets of the respective models. We then implement a variety of data augmentation techniques to enhance these original datasets, enabling the construction of sophisticated multi-step tasks designed to assess the planning and task-solving capabilities of a given LLM. On the other hand, OpenAGI also offers open-ended tasks that utilize a variety of expandable models. These tasks open the door to creativity and imaginative problem-solving, enabling the exploration of innovative solutions that may not emerge within more constrained task frameworks. For open-ended tasks, as depicted in Fig. 2, which is designed to accommodate a broader spectrum of needs, we further include LangChain to provide additional expert models, such as Google Search, Wikipedia, Wolfram Alpha and so on. Indeed, relying solely on input text for learning proves insufficient for LLMs when faced with real-world tasks. In order to improve its performance, we introduce a mechanism referred to as **Reinforcement Learning from Task Feedback (RLTF)**. This approach capitalizes on the performance feedback procured from tasks following the execution of the solution devised by the LLM. Consequently, the RLTF mechanism effectively refines the LLM's planning strategy, resulting in an enhanced and more adaptive system. In summary, the key contributions of the work include:

* We introduce OpenAGI, an AGI research platform, specifically designed to offer complex, multi-step tasks accompanied by their respective datasets, evaluation methods, and a diverse range of extensible models which can be synthesized to effectively solve these tasks. The purpose of this platform is to aid in the quantification of the overarching planning and task-solving abilities of LLMs. OpenAGI embraces AGI by focusing on LLM-driven, (open-domain) model synthesis, predominantly utilizing models and datasets on Hugging Face, GitHub and LangChain.
* We propose the LLM+RLTF approach for OpenAGI, which leverages a Large Language Model as a controller to select, synthesize and execute various external expert models for complex task-solving. The feedback obtained from these tasks is then employed to refine the LLM's planning strategy, thereby enhancing the LLM's overall performance and task-solving ability.
* We evaluate both open-source and closed-source LLMs with differing scales under distinct learning schema and the OpenAGI pipeline. Our findings suggest that even smaller-scale LLMs, when paired with an appropriate learning schema such as RLTF, are able to possess the potential to outperform competitors that equip a significantly greater magnitude of model parameters.

## 2 Related Work

### Large Language Model and AI Agents

With the advancement of highly parallelizable transformer architectures, pre-trained language models (PLMs) have demonstrated remarkable capabilities in comprehend, generating, and manipulating natural language [31; 24]. These models are pre-trained on a large corpora of text data and commonly fine-tuned for specific downstream tasks. Shortly, the scaled-up PLMs, known as Large Language Models (LLMs) [34; 2; 27; 6; 55; 45], encompassed a substantially greater number of parameters and leveraged vast amounts of training data. Consequently, LLMs exhibited an enhanced capacity to learn intricate language patterns and structures, along with a notable reasoning ability, leading to superior performance across diverse natural language processing tasks [2; 45; 55; 6; 5; 30; 14; 52]. Apart from the above superiority, LLMs may occasionally produce seemingly plausible yet inaccurate predictions and face challenges when addressing problems that require specialized domain expertise . Consequently, the emerging field of Augmented Language Models (ALMs) focuses on addressing the limitations of conventional LLMs [8; 6; 2] by equipping them with enhanced reasoning capabilities and the ability to employ external resources . The process of reasoning involves breaking down intricate assignments into smaller, more manageable sub-tasks that can be independently or collaboratively tackled by LLMs with the assistance of tools. What's more, LLMs can also invoke external tools or models to accomplish the relevant tasks. For example, ToolFormer  introduces external API tags within text sequences, facilitating LLMs' access to external tools. Visual ChatGPT  combines ChatGPT with Visual Foundation Models (VFMs) such as Transformers, ControlNet, and Stable Diffusion, which acts as a bridge between users, allowing them to communicate via chat and generate visuals. HuggingGPT  integrates the Hugging Face hub with task-specific models around ChatGPT to tackle AI tasks. ChatGPT for Robotics  employs ChatGPT for a wide array of robotics tasks through strategic prompt engineering. Besides, several open-sourced GitHub repositories are related to this topic, such as BabyAGI and AutoGPT. Notably, AutoGPT  is an automated agent, which is designed to set multiple objectives, break them down into relevant tasks, and iterate on these tasks until the objectives are achieved. Augmented language models may use these enhancements separately or joint them in a specific order to finish the specific task, which ultimately results in superior generalization capabilities.

Different from other works, we propose OpenAGI, an open-source AGI research and development platform designed to address the challenges commonly encountered in existing works, such as extensibility, nonlinear task planning, and quantitative evaluation. Furthermore, we introduce innovative methods into the learning schema of LLMs, including Reinforcement Learning from Task Feedback (RLTF) and nonlinear task planning, which aims to address challenges on out-of-distribution (OOD) generalization, optimal task planning, and AI's self-improvement (please see Sec. A.1 in supplementary materials for an extended discussion on these problems). We hope the OpenAGI platform can facilitate the open and long-term development and evaluation of AGI abilities in the community.

### Reinforcement Learning from Human Feedback (RLHF)

To better align Large Language Models (LLMs) with human values, Reinforcement Learning from Human Feedback (RLHF) has been introduced [7; 58], which fine-tunes LLMs by collected human feedback, effectively enhancing alignment criteria such as helpfulness, honesty, and harmlessness. At its core, RLHF deploys reinforcement learning (RL) algorithms, notably Proximal Policy Optimization (PPO) , to tailor LLMs to this feedback via a reward model. Importantly, this approach actively involves human oversight in the training process, exemplified by notable models such as InstructGPT . Nonetheless, the efficacy of RLHF is contingent upon the quality of feedback from adept labelers, rendering its practical implementation challenging [13; 29]. Consequently, there is an imperative to refine the RLHF framework to diminish the dependency on manual labeling and explore innovative, efficient annotation methodologies that ensure data integrity. Compared with RLHF, the proposed RLTF gets task feedback to supply information that guides the learning direction of LLMs, resulting in improved and more efficient strategies, which does not require human intervention.

## 3 The OpenAGI Platform

OpenAGI includes a wide range of features tailored to various needs. One key component is its benchmark tasks, detailed in Sec. 3.1, a particularly valuable tool for researchers. These tasks come equipped with task-specific datasets and evaluation metrics. This makes it possible for researchers to evaluate the performance of different LLMs in a structured and uniform manner, offering insights into their efficacy and potential areas for improvement. In addition to benchmark tasks, OpenAGI also offers open-ended tasks, detailed in Sec. 3.2. These tasks allow for a greater degree of creativity and imagination, breaking away from conventional constraints to enable more exploratory research. We believe this combination of structured benchmark tasks and flexible open-ended tasks makes OpenAGI a robust and versatile platform that can cater to a diverse array of research requirements.

### Benchmark Tasks

For benchmark tasks, our goal is to provide the community a valuable tool to evaluate the planning abilities of LLMs for complex, multi-step tasks. Specifically, instead of building complicated, multi-step tasks from scratch, we first explore the domain expert models (Sec. 3.1.1) that can be used as building blocks, then introduce how we create such tasks based on them (Sec. 3.1.2).

#### 3.1.1 Domain Expert Model Set

We now present the domain tasks and the corresponding models that can be employed in our platform. This set is designed to be flexible, allowing users to easily incorporate their own domain tasks and models. Our domain tasks are as follows:

* **Language-related Models**: **Sentiment Analysis** classifies the sentiment polarity of a given sentence ; **Text Summarization** creates a text summary that represents the most important or relevant information within the original text content ; **Machine Translation** converts a sentence from a source language to a target language ; **Fill Mask** involves replacing masked words within a given text ; **Question Answering (QA)** provides a textual answer of a question based on the given context .
* **Vision-related Models**: **Image Classification** aims to comprehend an entire image as a whole and assign it to a specific label ; **Object Detection** identifies and localizes specific objects within an image by detecting their instances of a particular class ; **Colorization** refers to the technique of adding plausible color information to monochromatic photographs or videos ; **Image Super-resolution** generates a high-resolution (HR) image from a low-resolution (LR) image ; **Image Denoising** aims to remove unwanted noise from an image while preserving its important features ; **Image Deblurring** aims to recover a clear image from a blurred input image .
* **Vision-Language Models**: **Visual Question Answering (VQA)** involves answering questions based on an image ; **Image Captioning** generates textual descriptions of the visual content depicted in an image; **Text-to-Image Generation** generates images from a given input sentence or sequence of words .

The details of the corresponding models are shown in Tab. A.1, A.2 and A.3 in supplementary materials. After selecting the domain expert models, choosing the raw datasets becomes a more straightforward process, provided that we need to ensure proper alignment between the datasets and the domain expert models' training sets. Raw datasets are provided as follows: **ImageNet-1K**, **Common Objects in Context (COCO)**, **CNN/Daily Mail**, **Stanford Sentiment Treebank (SST2)**, **TextVQA**, **Stanford Question Answering Dataset (SQuAD)**. More details about theses datasets can be found in Sec. A.2 in supplementary materials.

#### 3.1.2 Multi-step Tasks and Corresponding Datasets Construction

A multi-step task, as the name suggests, refers to a complex problem that cannot be solved in one simple step. It necessitates several sub-processes or stages, each requiring a particular type of problem-solving skill, in other words, domain expert model. In order to construct such complex, multi-step tasks, we introduce several commonly-used data augmentation methods, which are **Gaussian Blur**, **Gaussian Noise**, **Grayscale**, **Low Resolution**, **Translation**, **Word Mask**, to augment the raw dataset. More details about these methods can be found in Sec. A.3 in supplementary materials.

For the purpose of our study, we have sorted these tasks into six primary categories according to the modalities of their inputs and outputs:

* _Image in, image out_: In these tasks, images undergo several transformation stages. An example could be a task that involves "Denoising and enhancing the resolution of a low-resolution, noisy image". Here, the multi-step process entails image denoising followed by super-resolution.
* _Image in, text out_: These tasks usually involve interpreting the content of images. For example, "Detect objects in an image and describe them in a sentence" requires object detection followed by text generation.
* _Text in, image out_: Tasks under this category may include generating an image based on textual descriptions, such as "Create a graphical representation of the room described in the given text", demanding text understanding and image generation steps.
* translation and summarization.
* _Image-text pair in, text out_: These tasks deal with complex interplay between visual and textual data. For example, "Given an image and a question about the image in English, answer the question in German." This task includes image-text understanding, question answering, and translation.

* _Text-text pair in, text out_: These tasks can involve comparison, synthesis, or information extraction from two text inputs. For instance, "Given two reviews of a movie in English, translate them into German and provide a summary."

In total, we have devised 185 multi-step tasks, of which 117 tasks maintain a linear task structure with steps following a simple sequence, while the remaining 68 tasks exhibit a non-linear task structure, where steps might be performed concurrently or in a complex order. Among these categories, tasks such as Question Answering (QA) and Visual Question Answering (VQA), involving multiple or even multi-modal inputs, are notably complex and defy simple, linear task planning solutions. For a comprehensive view, we provide example tasks and their input and output data samples in Tab. A.4 of the supplementary materials. Additionally, a complete list of the task descriptions, accompanied by their estimated difficulty levels, can be found in Tab. A.5 within the supplementary materials.

#### 3.1.3 Evaluation Metrics

Given that the benchmark tasks of OpenAGI comprise a diverse range of domain tasks with multi-modal data, we classify them according to domain tasks as well as input and output types. We then

Figure 2: An example of open-ended tasks, which instructs OpenAGI to create an artwork given the theme “Gao Shan Liu Shui” (translating to “High Mountain and Flowing Water” in English). OpenAGI generates a non-linear (tree-structured) plan for the task with GPT-3.5, and then executes the plan with expert models to create a painting, a poem, and a piece of music for the theme.

assess their performance using the following three metrics based on their categories: **CLIP Score**, **BERT Score** and **ViT Score2** (more details can be found in supplementary). In particular, we employ the CLIP Score only for Text-to-Image Generation-based tasks, the BERT Score is utilized to assess tasks with text outputs, and the ViT score is applied to measure image similarity for the remaining tasks with image outputs. We also normalize the BERT and CLIP scores.

### Open-ended Tasks

Open-ended tasks necessitate an elevated degree of creative and imaginative capacity, as they deviate from conventional constraints to stimulate more exploratory research. These tasks are designed to accommodate a broad spectrum of needs, as illustrated in Fig. 2. To achieve this, LangChain is integrated to provide additional expert models from renowned sources such as Google Search, Wikipedia, Wolfram Alpha, and more. Crucially, these models offer extendability, ensuring that open-ended tasks are not confined to specific guidelines or performance metrics. To exemplify this process, Fig. 2 elucidates how OpenAGI is directed to create a traditional Chinese painting with "Gao Shan Liu Shui" (translating to "High Mountain and Flowing Water" in English) as its theme. The process is enriched with the addition of a generated ancient Chinese poem and a piece of music that harmonize with the painting. To effectively deliver on this instruction, OpenAGI first conducts an online search to comprehend the historical narrative of "Gao Shan Liu Shui". Sequentially, the painting, poem, and music are generated in a step-by-step fashion, leveraging the collaboration between expansive language models and domain-specific expert models. The final product - a coherent artistic ensemble of painting, poem, and music - successfully resonates with the underlying ancient narrative, demonstrating the efficacy of this approach in open-ended tasks. More examples are provided in supplementary.

## 4 Reinforcement Learning from Task Feedback (RLTF)

While learning solely from input text is a powerful method for training LLMs, it is not sufficient for handling real-world tasks that require a deeper understanding of context and environment. One potential method to improve the capabilities of LLMs is to incorporate reinforcement learning (RL) techniques. By leveraging the strengths of RL, LLMs can gain additional insights from trial-and-error experiences. This leads to more robust and adaptive models, especially in situations where labeled data is scarce or when tasks involve physical interactions. In this work, we propose Reinforcement Learning from Task Feedback (RLTF), shown in Fig. 3, which utilizes task feedback to supply more information that guides the learning direction of LLMs, resulting in improved and more efficient strategies. We choose to use REINFORCE  in this work and more details about the algorithm are provided in Sec. A.6 in supplementary.

## 5 Experiments

### Backbone LLMs

* **GPT-3.5-turbo.** The GPT (Generative Pre-trained Transformer) series  consists of advanced language models. In this work, we use the GPT-3.5-turbo-0301 snapshot.

Figure 3: An illustration of the RLTF mechanism.

* **Claude-2.** Claude-2  is an transformer LLM trained with unsupervised learning and RLHF.
* **GPT-4.** GPT-4 is a follow-up version of GPT-3.5, which is more powerful than its predecessors. In this work, we use the GPT-4-0613 snapshot.
* **Flan-T5-Large.** Flan-T5  is a series of language models which are fine-tuned using a technique called instruction fine-tuning. Flan-T5-Large has 770 million parameters.
* **Vicuna-7B.** Vicuna  is an open-source chatbot trained by fine-tuning the LLaMA  model with user-shared conversations. In this work, we use the 7-billion size model of Vicuna.
* **LLaMA-2.** LLaMA-2  is a successor to the original LLaMA model, it is significantly more powerful. In this work, we use the 13-billion size model.

Overall, we include three closed-source LLMs (GPT-3.5-turbo, Claude-2 and GPT-4) as well as three open-source LLMs (Flan-T5-Large, Vicuna-7B and LLaMA-2-13B).

### Learning Schema of LLMs

We employ the following LLM learning schema for experimentation.

* **Zero-shot Learning (Zero)** directly inputs the prompt to the LLM.
* **Few-shot Learning (Few)** presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted.
* **Fine-tuning** involves using manually labeled data samples as additional training signals to refine and adapt pre-trained LLMs to specific tasks or domains.
* **RLTF** is our proposed method in Sec. 4, which further utilizes the RL method to tune the fine-tuned LLMs with human labelled data.

We employ Low-Rank Adaptation (LoRA)  to optimize all open-source LLMs across both the Fine-tuning and RLTF learning schema.

### Planning Solution Parser

To transform the original LLM output into a viable task planning solution, we use a parser built on GPT-3.5. The prompt we employed reads as follows: "You are a key phrase extractor who is able to extract potential module names from the given context. You have already known all the module names in the full module list. The full module list is: [Image Classification, Colorization, Object Detection, Image Deblurring, Image Denoising, Image Super Resolution, Image Captioning, Text to Image Generation, Visual Question Answering, Sentiment Analysis, Question Answering, Text Summarization, Machine Translation]. Given the following context: '{}'. Please extract a module sequence from this context and remove module names which do not exist in the full module list from this sequence. Output the module sequence after filtering as the format of'module: module1, module: module2, module: module3, etc...'." Once this prompt is executed on the LLM's original text output, a task planning solution will be generated which consists of a multi-step solution of the problem.

### Datasets

Considering the fact that the imbalanced number of tasks with different input and output modalities could lead to skewed measurement results, we select the tasks in OpenAGI to compose the training set. In particular, we randomly select 10% of tasks, along with their corresponding datasets, based on input and output modalities for training purposes. For few-shot, fine-tuning and RLTF, we supply manually curated, feasible solutions as ground-truth labels. In the case of RLTF, we employ the fine-tuning checkpoint as a reasonable initialization for LLMs and use constrained beam search [11; 37] to reduce the likelihood of producing infeasible solutions (details can be found in Sec. A.7 in supplementary). Moreover, we choose an additional 10% of tasks, adhering to the same selection criteria as mentioned above, to serve as the test set.

### Experimental Analysis

The main experimental results are tabulated in Tab. 1 and 2, showcasing the results for closed-source and open-source LLMs, respectively. The overall performance is calculated as the average of CLIP,BERT and ViT scores. Here, only the task descriptions of the benchmark tasks are fed into LLMs (additional information, such as the input prompt and LLMs' outputs, is provided in Fig. A.4 and A.5 in supplementary). Broadly speaking, closed-source LLMs demonstrate superior performance on OpenAGI tasks, with GPT-4 leading the pack under both zero- and few-shot scenarios. In the open-source category, LLaMA-2-13B takes the lead, consistently posting top results across various learning schema--the performance possibly influenced by its larger model size. Notably, open-source LLMs significantly benefit from the tuning methods, particularly Fine-tuning and RLTF. These methods mark noticeable enhancements for Flan-T5-Large, Vicuna-7B, and LLaMA-2-13B when compared with zero-shot and few-shot learning schema. In fact, each of these open-source models hits its pinnacle under the RLTF approach. Conclusively, with RLTF tuning, the performance of LLaMA-2-13B approaches that of GPT-3.5, illustrating its potential.

### Effect of Prompts

We design two types of prompts combined with different levels of model description to test LLMs' zero-shot performances. The first, Prompt-1, only combines the task description with the model names, while the second, Prompt-2, integrates the task description with comprehensive model descriptions, detailing model usage, input, and output types (additional information about these two prompts is provided in Fig. A.6 in supplementary). We analyze the results in Tab. 3 and 4 in conjunction with the previous zero-shot results in Tab. 1 and 2. Compared to the original prompt that only uses task description to generate the results in Tab. 1 and 2, it is evident that in most cases, the closed-source LLMs, such as GPT series and Claude-2, tend to outperform when provided with detailed model-related information as seen in Prompt-1 and Prompt-2. In contrast, open-source LLMs, whose understanding and reasoning capacity may be weaker than those huge closed-source models, appear to be misled by the ambiguous details in Prompt-1 and Prompt-2 during the model selection process. Overall, detailed prompts can assist in improving the zero-shot performance to a certain degree, depending on the specific model. However, they may not be as potent as other training scenarios for smaller size LLMs, such as fine-tuning or RLTF.

### Case Study of Non-linear Planning

We qualitatively evaluate LLMs' capability of non-linear task planning. Fig. 4 illustrates the responses of GPT-3.5, Vicuna-7B and Flan-T5-Large to Prompt-2. The given task description requires the LLM to answer a query posed in English about a given noisy, blurry, and gray-scale image in German. It can be observed from the results that the performance of the models varies significantly. Flan-T5-Large, for instance, demonstrates a struggling comprehension of the query, while Vicuna-7B's answer incorporates all the provided models in an attempt to resolve the task. GPT-3.5 successfully comprehends the task and consequently delivers a reasonable plan. The plan generated by this model is notably non-linear, and it instructs to employ a Visual Question Answering (VQA) model with the English query and processed image as inputs in steps 1 and 2 in order to accomplish the task. Similarly, another task scenario is demonstrated in Fig. 2, which is an open-ended task with

    &  &  &  \\   & Zero & Few & Zero & Few & Zero & Few \\  CLIP Score & 0.0 & 0.0 & 0.0 & 0.2543 & 0.0 & 0.3055 \\ BERT Score & 0.1914 & 0.3820 & 0.2111 & 0.5038 & 0.2076 & 0.6307 \\ ViT Score & 0.2437 & 0.7497 & 0.4082 & 0.5416 & 0.5058 & 0.6480 \\ Overall & 0.1450 & 0.3772 & 0.2064 & 0.4332 & **0.2378** & **0.5281** \\   

Table 1: OpenAGI task-solving performances under different settings for three closed-source LLMs. Boldface denotes the highest score under each learning schema.

    &  &  &  \\   & Zero & Few & Fine-tuning & RLTF & Zero & Few & Fine-tuning & RLTF & Zero & Few & Fine-tuning & RLTF \\  CLIP Score & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0612 & 0.0608 & 0.1220 \\ BERT Score & 0.0 & 0.2488 & 0.0 & 0.0655 & 0.0513 & 0.0 & 0.1212 & 0.1756 & 0.0986 & 0.2281 & 0.1570 & 0.2401 \\ ViT Score & 0.0 & 0.0 & 0.6316 & 0.6978 & 0.1704 & 0.4285 & 0.5507 & 0.7300 & 0.3614 & 0.2558 & 0.6723 & 0.7584 \\ Overall & 0.0 & 0.0829 & 0.2105 & 0.2544 & 0.0739 & 0.1428 & 0.2239 & 0.3018 & **0.1533** & **0.1817** & **0.2967** & **0.3735** \\   

Table 2: OpenAGI task-solving performances under different settings for three open-source LLMs. Boldface denotes the highest score under each learning schema.

GPT being instructed to generate a painting in a traditional Chinese style that depicts "Gao Shan Liu Shui". Initially, GPT seems to lack understanding of what constitutes a traditional Chinese style painting and it is also unfamiliar with the concept of "Gao Shan Liu Shui". As a remedy, GPT utilizes Google search in the initial two steps to gather information on these unfamiliar topics. Subsequently, it integrates the retrieved information to formulate a comprehensive prompt that instructs the Text-to-Image Generation model to create the desired artwork.

## 6 Conclusions and Future Work

In this work, we introduce OpenAGI, an open-source AGI research platform designed to facilitate the development and evaluation of LLMs in solving complex, multi-step tasks through manipulating various domain expert models, tools, plugins or APIs. OpenAGI provides a wide range of tasks, models, datasets, benchmarks, and evaluation methods. We also propose the LLM+RLTF approach, which combines LLMs with reinforcement learning to optimize task-solving performance. The evaluation of various LLMs using the OpenAGI pipeline and different learning schema demonstrates that smaller-scale LLMs can potentially outperform larger models when combined with the appropriate learning approach, such as RLTF. In the future, we aim to explore 1) Human-in-the-loop agents, where LLM may prompt human experts for answers as one step of the task-solving plan when a suitable model is unavailable, thus enabling better Human-AI collaboration; 2) Trustworthy agents, which guarantee the safety and the ethical standard of agents during task-solving; and 3) Self-improving agents, which enable automated task generation and training that facilitate OpenAGI in independent exploration of tasks, empowering the self-reflection, self-prompting and self-improvement of intelligent agents.

    &  &  &  \\   & Prompt-1 & Prompt-2 & Prompt-1 & Prompt-2 & Prompt-1 & Prompt-2 \\  CLIP Score & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ BERT Score & 0.0 & 0.0 & 0.0603 & 0.0267 & 0.0971 & 0.1717 \\ ViT Score & 0.0 & 0.0 & 0.0 & 0.2385 & 0.0 & 0.0 \\ Overall & 0.0 & 0.0 & 0.0201 & 0.0884 & 0.0323 & 0.0572 \\   

Table 4: Zero-shot task-solving performances under various prompts for three open-source LLMs.

Figure 4: An example of Non-linear Planning.

    &  &  &  \\   & Prompt-1 & Prompt-2 & Prompt-1 & Prompt-2 & Prompt-1 & Prompt-2 \\  CLIP Score & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 \\ BERT Score & 0.2106 & 0.3013 & 0.4088 & 0.2333 & 0.4402 & 0.5595 \\ ViT Score & 0.0 & 0.2710 & 0.6816 & 0.7957 & 0.5497 & 0.5565 \\ Overall & 0.0702 & 0.1907 & 0.3635 & 0.3430 & 0.3299 & 0.3717 \\   

Table 3: Zero-shot task-solving performances under various prompts for three closed-source LLMs.