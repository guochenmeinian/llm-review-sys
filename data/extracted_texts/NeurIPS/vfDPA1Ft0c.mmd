# Best Arm Identification for Stochastic Rising Bandits

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected reward of the available options increases every time they are selected. This setting captures a wide range of scenarios in which the available options are _learning entities_ whose performance improves (in expectation) over time. While previous works addressed the regret minimization problem, this paper focuses on the _fixed-budget Best Arm Identification_ (BAI) problem for SRBs. In this scenario, given a fixed budget of rounds, we are asked to provide a recommendation about the best option at the end of the identification process. We propose two algorithms to tackle the above-mentioned setting, namely R-UCBE, which resorts to a UCB-like approach, and R-SR, which employs a successive reject procedure. Then, we prove that, with a sufficiently large budget, they provide guarantees on the probability of properly identifying the optimal option at the end of the learning process. Furthermore, we derive a lower bound on the error probability, matched by our R-SR (up to logarithmic factors), and illustrate how the need for a sufficiently large budget is unavoidable in the SRB setting. Finally, we numerically validate the proposed algorithms in synthetic and real-world environments and compare them with the currently available BAI strategies.

## 1 Introduction

Multi-Armed Bandits (MAB, Lattimore and Szepesvari, 2020) are a well-known framework that effectively solves learning problems requiring sequential decisions. Given a time horizon, the learner chooses, at each round, a single option (a.k.a. arm) and observes the corresponding noisy reward, which is a realization of an unknown distribution. The MAB problem is commonly studied in two flavours: _regret minimization_(Auer et al., 2002) and _best arm identification_(Bubeck et al., 2009). In regret minimization, the goal is to control the cumulative loss w.r.t. the optimal arm over a time horizon. Conversely, in best arm identification, the goal is to provide a recommendation about the best arm at the end of the time horizon. Specifically, we are interested in the fixed-budget scenario, where we seek to minimize the error probability of recommending the wrong arm at the end of the time budget, no matter the loss incurred during learning.

This work focuses on the _Stochastic Rising Bandits_(SRB), a specific instance of the _rested_ bandit (Tekin and Liu, 2012) setting in which the expected reward of an arm increases according to the number of times it has been pulled. Online learning in such a scenario has been recently addressed from a regret minimization perspective by Metelli et al. (2022), in which the authors provide no-regret algorithms for the SRB setting in both the rested and restless cases. The SRB setting models several real-world scenarios where arms improve their performance over time. A classic example is the so-called _Combined Algorithm Selection and Hyperparameter optimization_(CASH, Thornton et al., 2013; Kotthoff et al., 2017; Erickson et al., 2020; Li et al., 2020; Zoller and Huber, 2021), a problem of paramount importance in _Automated Machine Learning_(AutoML, Feurer et al., 2015; Yao et al., 2018; Hutter et al., 2019; Mussi et al., 2023). In CASH, the goal is to identify the _best learning algorithm_ together with the _best hyperparameter_ configuration for a given ML task (e.g.,classification or regression). In this problem, every arm represents a hyperparameter tuner acting on a specific learning algorithm. A pull corresponds to a unit of time/computation in which we improve (on average) the hyperparameter configuration (via the tuner) for the corresponding learning algorithm. CASH was handled in a bandit _Best Arm Identification_ (BAI) fashion in Li et al. (2020) and Cella et al. (2021). The former handles the problem by considering rising rested bandits with _deterministic_ rewards, failing to represent the intrinsic uncertain nature of such processes. Instead, the latter, while allowing stochastic rewards, assumes that the expected rewards evolve according to a _known_ parametric functional class, whose parameters have to be learned.1

Original ContributionsIn this paper, we address the design of algorithms to solve the BAI task in the rested SRB setting when a _fixed budget_ is provided.2 More specifically, we are interested in algorithms guaranteeing a sufficiently large probability of recommending the arm with the largest expected reward _at the end_ of the time budget (as if only this arm were pulled from the beginning). The main contributions of the paper are summarized as follows:3

* We propose two _algorithms_ to solve the BAI problem in the SRB setting: R-UCBE (an optimistic approach, Section 4) and R-SR (a phases-based rejection algorithm, Section 5). First, we introduce specifically designed estimators required by the algorithms (Section 3). Then, we provide guarantees on the error probability of the misidentification of the best arm.
* We derive the first error probability _lower bound_ for the SRB setting, matched by our R-SR algorithm up to logarithmic factors, which highlights the complexity of the problem and the need for a sufficiently large time budget (Section 6).
* Finally, we conduct _numerical simulations_ on synthetically generated data and a real-world online best model selection problem. We compare the proposed algorithms with the ones available in the bandit literature to tackle the SRB problem (Section 7).

## 2 Problem Formulation

In this section, we revise the Stochastic Rising Bandits (SRB) setting (Heidari et al., 2016; Metelli et al., 2022). Then, we formulate our best arm identification problem, introduce the definition of error probability, and provide a preliminary characterization of the problem.

SettingWe consider a rested Multi-Armed Bandit problem \(=(_{i})_{i K}\) with a finite number of arms \(K\).4 Let \(T\) be the time budget of the learning process. At every round \(t T\), the agent selects an arm \(I_{t} K\), plays it, and observes a reward \(x_{t}_{I_{t}}(N_{I_{t},t})\), where \(_{I_{t}}(N_{I_{t},t})\) is the reward distribution of the chosen arm \(I_{t}\) at round \(t\) and depends on the number of pulls performed so far \(N_{i,t}_{=1}^{t}\{I_{}=i\}\) (i.e., rested). The rewards are stochastic, formally \(x_{t}_{I_{t}}(N_{I_{t},t})+_{t}\), where \(_{I_{t}}()\) is the expected reward of arm \(I_{t}\) and \(_{t}\) is a zero-mean \(^{2}\)-subgaussian noise, conditioned to the past.5 As customary in the bandit literature, we assume that the rewards are bounded in expectation, formally \(_{i}(n), i K,n T\). As in (Metelli et al., 2022), we focus on a particular family of rested bandits in which the expected rewards are monotonically _non-decreasing_ and _concave_ in expectation.

**Assumption 2.1** (Non-decreasing and concave expected rewards).: _Let \(\) be a rested MAB, defining \(_{i}(n)_{i}(n+1)-_{i}(n)\), for every \(n\) and every arm \(i K\) the rewards are non-decreasing and concave, formally:_

\[_{i}(n) 0,_{i}(n+1)_{i}(n).\]

Intuitively, the \(_{i}(n)\) represents the _increment_ of the real process \(_{i}()\) evaluated at the \(n^{}\) pull. Notice that concavity emerges in several settings, such as the best model selection and economics, representing the decreasing marginal returns (Lehmann et al., 2001; Heidari et al., 2016).

Learning ProblemThe goal of BAI in the SRB setting is to select the arm providing the largest expected reward with a large enough probability given a fixed budget \(T\). Unlike the stationary BAI problem (Audibert et al., 2010), in which the optimal arm is not changing, in this setting, we need to decide _when_ to evaluate the optimality of an arm. We define optimality by considering the largest expected reward at time \(T\). Formally, given a time budget \(T\), the optimal arm \(i^{*}(T) K\), which we assume unique, satisfies:

\[i^{*}(T)*{arg\,max}_{i K}_{i}( T),\]

where we highlighted the dependence on \(T\) as, with different values of the budget, \(i^{*}(T)\) may change. Let \(i K\{i^{*}(T)\}\) be a suboptimal arm, we define the suboptimality gap as \(_{i}(T):=_{i^{*}(T)}(T)-_{i}(T)\). We employ the notation \((i) K\) to denote the \(i^{}\) best arm at time \(T\) (arbitrarily breaking ties), i.e., we have \(_{(2)}(T)_{(K)}(T)\). Given an algorithm \(\) that recommends \(^{*}(T) K\) at the end of the learning process, we measure its performance with the _error probability_, i.e., the probability of recommending a suboptimal arm at the end of the time budget \(T\):

\[e_{T}()_{}(^{*}(T) i^{* }(T)).\]

Problem CharacterizationWe now provide a characterization of a specific class of polynomial functions to upper bound the increments \(_{i}(n)\).

**Assumption 2.2** (Bounded \(_{i}(n)\)).: _Let \(\) be a rested MAB, there exist \(c>0\) and \(>1\) such that for every arm \(i K\) and number of pulls \(n 0,T\) it holds that \(_{i}(n) cn^{-}\)._

We anticipate that, even if our algorithms will not require such an assumption, it will be used for deriving the lower bound and for providing more human-readable error probability guarantees. Furthermore, we observe that our Assumption 2.2 is fulfilled by a strict superset of the functions employed in Cella et al. (2021).

## 3 Estimators

In this section, we introduce the estimators of the arm expected reward employed by the proposed algorithms.6 A visual representation of such estimators is provided in Figure 1.

Let \((0,1/2)\) be the fraction of samples collected up to the current time \(t\) we use to build estimators of the expected reward. We employ an _adaptive arm-dependent window_ size \(h(N_{i,t-1})[ N_{i,t-1}]\) to include the most recent samples collected only, avoiding the use of samples that are no longer representative. We define the set of the last \(h(N_{i,t-1})\) rounds in which the \(i^{}\) arm was pulled as:

\[_{i,t}\{ T:I_{}=i \,\,N_{i,}=N_{i,t-1}-l,\,l 0,h(N_{i,t-1})-1 \}.\]

Furthermore, the set of the pairs of rounds \(\) and \(^{}\) belonging to the sets of the last and second-last \(h(N_{i,t-1})\)-wide windows of the \(i^{}\) arm is defined as:

\[_{i,t}(,^{}) T  T:I_{}=I_{^{}}=i\, \,N_{i,}=N_{i,t-1}-l,\]

\[N_{i,^{}}=N_{i,}-h(N_{i,t-1}),\,l 0,h(N_{i,t-1})-1 }.\]

In the following, we design a _pessimistic_ estimator and an _optimistic_ estimator of the expected reward of each arm at the end of the budget time \(T\), i.e., \(_{i}(T)\).7

**Pessimistic Estimator** The _pessimistic_ estimator \(_{i}(N_{i,t-1})\) is a negatively biased estimate of \(_{i}(T)\) obtained assuming that the function \(_{i}()\) remains constant up to time \(T\). This corresponds to the minimum admissible value under Assumption 2.1 (due to the _Non-decreasing_ constraint). This estimator is an average of the last \(h(N_{i,t-1})\) observed rewards collected from the \(i^{}\) arm, formally:

\[_{i}(N_{i,t-1}))}_{ _{i,t}}x_{}.\] (1)

The estimator enjoys the following concentration property.

**Lemma 3.1** (Concentration of \(_{i}\)).: _Under Assumption 2.1, for every \(a>0\), simultaneously for every arm \(i K\) and number of pulls \(n 0,T\), with probability at least \(1-2TKe^{-a/2}\) it holds that:_

\[_{i}(n)-_{i}(n)_{i}(n)-_{i}(n) _{i}(n),\] (2)

_where \(_{i}(n)}\) and \(_{i}(n)(2T-n+h(n)-1)\,_{i}(n-h(n)+1)\)._

As supported by intuition, we observe that the estimator is affected by a negative bias that is represented by \(_{i}(n)\) that vanishes as \(n\) under Assumption 2.1 with a rate that depends on the increment functions \(_{i}()\). Considering also the term \(_{i}(n)\) and recalling that \(h(n)=(n)\), under Assumption 2.2, the overall concentration rate is \((n^{-1/2}+cTn^{-})\).

**Optimistic Estimator** The _optimistic_ estimator \(_{i}^{T}(N_{i,t-1})\) is a positively biased estimation of \(_{i}(T)\) obtained assuming that function \(_{i}()\) linearly increases up to time \(T\). This corresponds to the maximum value admissible under Assumption 2.1 (due to the _Concavity_ constraint). The estimator is constructed by adding to the pessimistic estimator \(_{i}(N_{i,t-1})\) an estimate of the increment occurring in the next step up to \(T\). The latter uses the last \(2h(N_{i,t-1})\) samples to obtain an upper bound of such growth thanks to the concavity assumption, formally:

\[_{i}^{T}(N_{i,t-1})_{i}(N_{i,t-1})+_{(j,k) _{i,t}}(T-j)-x_{k}}{h(N_{i,t-1})^{2}}.\] (3)

The estimator displays the following concentration guarantee.

**Lemma 3.2** (Concentration of \(_{i}^{T}\)).: _Under Assumption 2.1, for every \(a>0\), simultaneously for every arm \(i K\) and number of pulls \(n 0,T\), with probability at least \(1-2TKe^{-a/10}\) it holds that:_

\[_{i}^{T}(n)_{i}^{T}(n)-_{i}(n) _{i}^{T}(n)+_{i}^{T}(n),\] (4)

_where \(_{i}^{T}(n)(T-n+h(n)-1)}}\) and \(_{i}^{T}(n)(2T-n+h(n)-1)\,_{i}(n-2h(n)+1)\)._

Differently from the pessimistic estimation, the optimistic one displays a positive vanishing bias \(_{i}^{T}(n)\). Under Assumption 2.2, we observe that the overall concentration rate is \((Tn^{-3/2}+cTn^{-})\).

## 4 Optimistic Algorithm: Rising Upper Confidence Bound Exploration

In this section, we introduce and analyze Rising Upper Confidence Bound Exploration (R-UCBE) an _optimistic_ error probability minimization algorithm for the SRB setting with a fixed budget. The algorithm explores by means of a UCB-like approach and, for this reason, makes use of the optimistic estimator \(_{i}^{T}\) plus a bound to account for the uncertainty of the estimation.8

**Algorithm** The algorithm, whose pseudo-code is reported in Algorithm 1, requires as input an exploration parameter \(a 0\), the window size \((0,1/2)\), the time budget \(T\), and the number of arms \(K\). At first, it initializes to zero the counters \(N_{i,0}\), and sets to \(+\) the upper bounds \(B_{i}^{T}(N_{i,0})\) of all the arms (Line 2). Subsequently, at each time \(t T\), the algorithm selects the arm \(I_{t}\) with the largest upper confidence bound (Line 4):

\[I_{t}*{arg\,max}_{i K}B_{i}^{T}(N_{i,t -1})_{i}^{T}(N_{i,t-1})+_{i}^{T}(N_{i,t-1}),\] (5)

\[_{i}^{T}(N_{i,t-1})(T-N_{i,t -1}+h(N_{i,t-1})-1))^{3}}},\] (6)where \(_{i}^{T}(N_{i,t-1})\) represents the exploration bonus (a graphical representation is reported in Figure 1). Once the arm is chosen, the algorithm plays it and observes the feedback \(x_{t}\) (Line 5). Then, the optimistic estimate \(_{I_{t}}^{T}(N_{I_{t},t})\) and the exploration bonus \(_{I_{t}}^{T}(N_{I_{t},t})\) of the selected arm \(I_{t}\) are updated (Lines 8-9). The procedure is repeated until the algorithm reaches the time budget \(T\). The final recommendation of the best arm is performed using the last computed values of the bounds \(B_{i}^{T}(N_{i,T})\), returning the arm \(^{*}(T)\) corresponding to the largest upper confidence bound (Line 12).

**Bound on the Error Probability of R-UCBE** We now provide bounds on the error probability for R-UCBE. We start with a general analysis that makes no assumption on the increments \(_{i}()\) and, then, we provide a more explicit result under Assumption 2.2. The general result is formalized as follows.

**Theorem 4.1**.: _Under Assumption 2.1, let \(a^{*}\) be the largest positive value of a satisfying:_

\[T-_{i i(T)}y_{i}(a) 1,\] (7)

_where for every \(i K\), \(y_{i}(a)\) is the largest integer for which it holds:_

\[((1-2)y)}_{(A)}+ }}}_{(B)} _{i}(T).\] (8)

_If \(a^{*}\) exists, then for every \(a[0,a^{*}]\) the error probability of R-UCBE is bounded by:_

\[e_{T}() 2TK-.\] (9)

Some comments are in order. First, \(a^{*}\) is defined implicitly, depending on the constants \(\), \(T\), the increments \(_{i}()\), and the suboptimality gaps \(_{i}(T)\). In principle, there might exist no \(a^{*}>0\) fulfilling condition in Equation (7) (this can happen, for instance, when the budget \(T\) is not large enough), and, in such a case, we are unable to provide theoretical guarantees on the error probability of R-UCBE. Second, the result presented in Theorem 4.1 holds for generic increasing and concave expected reward functions. This result shows that, as expected, the error probability decreases when the exploration parameter \(a\) increases. However, this behavior stops when we reach the threshold \(a^{*}\). Intuitively, the value of \(a^{*}\) sets the maximum amount of exploration we should use for learning.

Under Assumption 2.2, i.e., using the knowledge on the increment \(_{i}()\) upper bound, we derive a result providing conditions on the time budget \(T\) under which \(a^{*}\) exists and an explicit value for \(a^{*}\).

**Corollary 4.2**.: _Under Assumptions 2.1 and 2.2, if the time budget \(T\) satisfies:_

\[T(c^{}(1-2)^{-1}\ (H_{1,1/}(T))+(K-1))^{}&(1,3/2)\\ (c^{}(1-2)^{-}\ (H_{1,2/3}(T))+(K-1) )^{3}&[3/2,+),\] (10)

_there exists \(a^{*}>0\) defined as:_

\[a^{*}=}{4^{2}}((-(K-1)}{H_{1,1/}(T)})^{}-c(1-2)^{-} )^{2}&(1,3/2)\\ }{4^{2}}((-(K-1)}{H_{1,2/3}(T) })^{3/2}-c(1-2)^{-})^{2}&[3/2,+),\]

_where \(H_{1,}(T):=_{i i(T)}^{*}(T)}\) for \(>0\). Then, for every \(a[0,a^{*}]\), the error probability of R-UCBE is bounded by:_

\[e_{T}() 2TK-.\]

First of all, we notice that the error probability \(e_{T}()\) presented in Theorem 4.2 holds under the condition that the time budget \(T\) fulfills Equation (10). We defer a more detailed discussion on this condition to Remark 5.1, where we show that the existence of a finite value of \(T\) fulfilling Equation (10) is ensured under mild conditions.

Let us remark that term \(H_{1,}(T)\) characterizes the complexity of the SRB setting, corresponding to term \(H_{1}\) of Audibert et al. (2010) for the classical BAI problem when \(=2\). As expected, in the small-\(\) regime (i.e., \((1,3/2]\)), looking at the dependence of \(H_{1,1/}(T)\) on \(\), we realize that 

[MISSING_PAGE_FAIL:6]

It is worth noting that R-SR makes use of the pessimistic estimator \(_{i}(n)\). Even if both estimators defined in Section 3 are viable for R-SR, the choice of using the pessimistic estimator is justified by its better concentration rate \((n^{-1/2})\) compared to that of the optimistic estimator \((Tn^{-3/2})\), being \(n T\) (see Section 3).

Note that the phase lengths are the ones adopted by Audibert et al. (2010). This choice allows us to provide theoretical results without requiring domain knowledge (still under a large enough budget). An optimized version of \(N_{j}\) may be derived assuming full knowledge of the gaps \(_{i}(T)\), but, unfortunately, such a hypothetical approach would have similar drawbacks as R-UCBE.

**Bound on the Error Probability of R-SR** The following theorem provides the guarantee on the error probability for the R-SR algorithm.

**Theorem 5.1**.: _Under Assumptions 2.1 and 2.2, if the time budget \(T\) satisfies:_

\[T 2^{}c^{}(K)^{ }_{i[2,K]}\{i^{}_ {(i)}(T)^{-}\},\] (12)

_then, the error probability of R-SR is bounded by:_

\[e_{T}(})\ (-}(K)H_{2}(T)}),\]

_where \(H_{2}(T)_{i[K]}\{i_{(i)}(T)^{-2}\}\) and \((K)=+_{i=2}^{K}\)._

Similar to the R-UCBE, the complexity of the problem is characterized by term \(H_{2}(T)\) that, for the standard MAB setting, reduces to the \(H_{2}\) term of Audibert et al. (2010). Furthermore, when the condition of Equation (12) on the time budget \(T\) is satisfied, the error probability coincides with that of the SR algorithm for standard MABs (apart for constant terms). The following remark elaborates on the conditions of Equations (10) and (12) about the minimum requested time budget.

**Remark 5.1** (About the minimum time budget \(T\)).: _To satisfy the \(e_{T}\) bounds presented in Corollary 4.2 and Theorem 5.1, R-UCBE and R-SR require the conditions provided by Equations (10) and (12) about the time budget \(T\), respectively. First, let us notice that if the suboptimal arms converge to an expected reward different from that of the optimal arm as \(T+\), it is always possible to find a finite value of \(T<+\) such that these conditions are fulfilled. Formally, assume that there exists \(T_{0}<+\) and that for every \(T T_{0}\) we have that for all suboptimal arms \(i i^{*}(T)\) it holds that \(_{i}(T)_{}>0\). In such a case, the l.h.s. of Equations (10) and (12) are upper bounded by a function of \(_{}\) and are independent on \(T\). Instead, if a suboptimal arm converges to the same expected reward as the optimal arm when \(T+\), the identification problem is more challenging and, depending on the speed at which the two arms converge as a function of \(T\), might slow down the learning process arbitrarily. This should not surprise as the BAI problem becomes non-learnable even in standard (stationary) MABs when multiple optimal arms are present (Heide et al., 2021)._

## 6 Lower Bound

In this section, we investigate the complexity of the BAI problem for SRBs with a fixed budget.

**Minimum time budget \(\)**: We show that, under Assumptions 2.1 and 2.2, any algorithm requires a minimum time budget \(T\) to be guaranteed to identify the optimal arm, even in a deterministic setting.

**Theorem 6.1**.: _For every algorithm \(\), there exists a deterministic SRB satisfying Assumptions 2.1 and 2.2 such that the optimal arm \(i^{*}(T)\) cannot be identified for some time budgets \(T\) unless:_

\[T H_{1,1/(-1)}(T)=_{i i^{*}(T)}(T)^{}}.\] (13)

Theorem 6.1 formalizes the intuition that any of the suboptimal arms must be pulled a sufficient number of times to ensure that, if pulled further, it cannot become the optimal arm. It is worth comparing this bound on the time budget with the corresponding conditions on the minimum time budget requested by Equations (10) and (12) for R-UCBE and R-SR, respectively. Regarding R-UCBE, we notice that the minimum admissible time budget in the small-\(\) regime is of order \(H_{1,1/}(T)^{/(-1)}\) which is larger than term \(H_{1,1/(-1)}(T)\) of Equation (13).10 Similarly, in the large-\(\) regime (i.e., \(>3/2\)), the R-UCBE requirement is of order \(H_{1,2/3}(T)^{3} H_{1,2}(T)\) which is larger than the term of Theorem 6.1 since \(1/(-1)<2\). Concerning R-SR, it is easy to show that \(H_{1,1/(-1)}(T)_{i[2,K]}i_{(i)}(T)^{-1/(-1)}\), apart from logarithmic terms, by means of the argument provided by (Audibert et al., 2010, Section 6.1). Thus, up to logarithmic terms, Equation (12) provides a tight condition on the minimum budget.

**Error Probability Lower Bound** We now present a lower bound on the error probability.

**Theorem 6.2**.: _For every algorithm \(\) run with a time budget \(T\) fulfilling Equation (13), there exists a SRB satisfying Assumptions 2.1 and 2.2 such that the error probability is lower bounded by:_

\[e_{T}()(-H_{1,2}(T)} ),\ \ \ \ \ H_{1,2}(T)_{i i}^{2}(T)}.\]

Some comments are in order. First, we stated the lower bound for the case in which the minimum time budget satisfies the inequality of Theorem 6.1, which is a necessary condition for identifying the optimal arm. Second, the lower bound on the error probability matches, up to logarithmic factors, that of our R-SR, suggesting the superiority of this algorithm compared to R-UCBE. Finally, provided that the identifiability condition of Equation (13), such a result corresponds to that of the standard (stationary) MABs (Audibert et al., 2010; Kaufmann et al., 2016). A summary of all the bounds provided in the paper is presented in Table 1.

## 7 Numerical Validation

In this section, we provide a numerical validation of R-UCBE and R-SR. We compare them with state-of-the-art bandit baselines designed for stationary and non-stationary BAI in a synthetic setting, and we evaluate the sensitivity of R-UCBE to its exploration parameter \(a\). Additional details about the experiments presented in this section are available in Appendix G. Additional experimental results on both synthetic settings and in a real-world experiment are available in Appendix H.11

**Baselines** We compare our algorithms against a wide range of solutions for BAI:

* RR: uniformly pulls all the arms until the budget ends in a _round-robin_ fashion and, in the end, makes a recommendation based on the empirical mean of their reward over the collected samples;
* RR-SW: makes use of the same exploration strategy as RR to pull arms but makes a recommendation based on the empirical mean over the last \(\) collected samples from an arm.12

* UCB-E and SR (Audibert et al., 2010): algorithms for the stationary BAI problem;
* Prob-1 (Abbasi-Yadkori et al., 2018): an algorithm dealing with the adversarial BAI setting;
* ETC and Rest-Sure(Cella et al., 2021): algorithms developed for the decreasing loss BAI setting.13

The hyperparameters required by the above methods have been set as prescribed in the original papers. For both our algorithms and RR-SW, we set \(=0.25\).

   & Error Probability \(e_{T}()\) & Time Budget \(T\) \\   SRB & \((-_{i i}(T)}})\) & \(_{i i}(T)^{3}_{i}^{2}}\) \\   R-UCBE & \(2\,T\,K\,(-)\) & \((c^{3}(1-2)^{-1}(_{i i}^{2}(T)})+(K-1))^{^{2}_{i}}&(1,3/2)\\ (c^{3}(1-2)^{-1}(_{i i}^ {2}(T)})+(K-1))^{3}&[3/2,+)\) \\  R-SR & \((- _{i[K]}\{i_{i}^{-2}(T)\}})\) & \(2^{}c^{3+1}(K)^{}{^{2}}}_{ i[1,K]}\{i}{^{2}}_{i(i)}(T)^{- }\}\) \\  

Table 1: Bounds on the time budget and error probability: lower for the setting and upper for the algorithms.

**Setting**: To assess the quality of the recommendation \(^{*}(T)\) provided by our algorithms, we consider a synthetic SRB setting with \(K=5\) and \(=0.01\). Figure 2 shows the evolution of the expected values of the arms w.r.t. the number of pulls. In this setting, the optimal arm changes depending on whether \(T\) or \(T(185,+)\). Thus, when the time budget is close to that value, the problem is more challenging since the optimal and second-best arms expected rewards are close to each other. For this reason, the BAI algorithms are less likely to provide a correct recommendation than for time budgets for which the two expected rewards are well separated. We compare the analyzed algorithms \(\) in terms of empirical error \(_{T}()\) (the smaller, the better), i.e., the empirical counterpart of \(e_{T}()\) averaged over \(100\) runs, considering time budgets \(T\).

**Results**: The empirical error probability provided by the analyzed algorithms in the synthetically generated setting is presented in Figure 3. We report with a dashed vertical blue line at \(T=185\), i.e., the budgets after which the optimal arm no longer changes. Before such a budget, all the algorithms provide large errors (i.e., \(_{T}()>0.2\)). However, R-UCBE outperforms the others by a large margin, suggesting that an optimistic estimator might be advantageous when the time budget is small. Shortly after \(T=185\), R-UCBE starts providing the correct suggestion consistently. R-SR begins to identify the optimal arm (i.e., with \(_{T}()<0.05\)) for time budgets \(T>1000\). Nonetheless, both algorithms perform significantly better than the baseline algorithms used for comparison.

**Sensitivity Analysis for the Exploration Parameter of R-UCBE**: We perform a sensitivity analysis on the exploration parameter \(a\) of R-UCBE. Such a parameter should be set to a value less or equal to \(a^{*}\), and the computation of the latter is challenging. We tested the sensitivity of R-UCBE to this hyperparameter by looking at the error probability for \(a\{a^{*}/50,a^{*}/10,a^{*},10a^{*},50a^{*}\}\). Figure 4 shows the empirical errors of R-UCBE with different parameters \(a\), where the blue dashed vertical line denotes the last time the optimal arm changes over the time budget. It is worth noting how, even in this case, we have two significantly different behaviors before and after such a time. Indeed, if \(T 185\), we have that a misspecification with larger values than \(a^{*}\) does not significantly impact the performance of R-UCBE, while smaller values slightly decrease the performance. Conversely, for \(T>185\) learning with different values of \(a\) seems not to impact the algorithm performance significantly. This corroborates the previous results about the competitive performance of R-UCBE.

## 8 Discussion and Conclusions

This paper introduces the BAI problem with a fixed budget for the Stochastic Rising Bandits setting. Notably, such setting models many real-world scenarios in which the reward of the available options increases over time, and the interest is on the recommendation of the one having the largest expected rewards after the time budget has elapsed. In this setting, we presented two algorithms, namely R-UCBE and R-SR providing theoretical guarantees on the error probability. R-UCBE is an optimistic algorithm requiring an exploration parameter whose optimal value requires prior information on the setting. Conversely, R-SR is a phase-based solution that only requires the time budget to run. We established lower bounds for the error probability an algorithm suffers in such a setting, which is matched by our R-SR, up to logarithmic factors. Furthermore, we showed how a requirement on the minimum time budget is unavoidable to ensure the identifiability of the optimal arm. Finally, we validate the performance of the two algorithms in both synthetically generated and real-world settings. A possible future line of research is to derive an algorithm balancing the tradeoff between theoretical guarantees on the \(e_{T}\) and the chance of providing such guarantees with lower time budgets.