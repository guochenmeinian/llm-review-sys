# Sequential Decision Making with Expert Demonstrations under Unobserved Heterogeneity

Vahid Balazadeh

University of Toronto

vahid@cs.toronto.edu

&Keertana Chidambaram

Stanford University

vck@stanford.edu

&Viet Nguyen

University of Toronto

viet@cs.toronto.edu

&Rahul G. Krishnan

University of Toronto

rahulgk@cs.toronto.edu

&Vasilis Syrgkanis

Stanford University

vsyrgk@stanford.edu

Equal contribution. Our code is accessible at https://github.com/vdblm/experior

###### Abstract

We study the problem of online sequential decision-making given auxiliary demonstrations from _experts_ who made their decisions based on unobserved contextual information. These demonstrations can be viewed as solving related but slightly different problems than what the learner faces. This setting arises in many application domains, such as self-driving cars, healthcare, and finance, where expert demonstrations are made using contextual information, which is not recorded in the data available to the learning agent. We model the problem as zero-shot meta-reinforcement learning with an unknown distribution over the unobserved contextual variables and a Bayesian regret minimization objective, where the unobserved variables are encoded as parameters with an unknown prior. We propose the Experts-as-Priors algorithm (ExPerior), an empirical Bayes approach that utilizes expert data to establish an informative prior distribution over the learner's decision-making problem. This prior distribution enables the application of any Bayesian approach for online decision-making, such as posterior sampling. We demonstrate that our strategy surpasses existing behaviour cloning, online, and online-offline baselines for multi-armed bandits, Markov decision processes (MDPs), and partially observable MDPs, showcasing the broad reach and utility of ExPerior in using expert demonstrations across different decision-making setups.

## 1 Introduction

Reinforcement learning (RL) has found success in complex decision-making tasks, spanning areas such as game playing , robotics , and aligning with human preferences . However, RL's considerable sample inefficiency, necessitating millions of training frames for convergence, remains a significant challenge. A notable body of work within RL has been dedicated to integrating expert demonstrations to accelerate the learning process, employing strategies like offline pretraining  and the use of combined offline-online datasets . While these approaches are theoretically sound and empirically validated , they typically presume homogeneity between the offline and online datasets. A vital question arises about the effectiveness of these methods when expert data embody heterogeneous tasks, indistinguishable by the learner.

An important example of such heterogeneity is in situations where experts operate with additional information not available to the learner, a scenario previously explored in imitation learning with unobserved contexts . Existing literature either relies on the availability of experts to query during training  or focuses on the assumptions that enable imitation learning with unobserved contexts, sidestepping online reward-based interactions [20; 21]. Recent contributions by Hao et al. [22; 23] suggest using offline expert data for online RL, albeit without accounting for unobserved variations. Our work addresses the more general challenge of online decision-making given auxiliary offline expert data with _unobserved_ heterogeneity. We view such demonstrations as solving related yet distinct problems from those faced by the learner, where differences remain invisible to the learner. For instance, in a personalized education scenario, while a learning agent can observe characteristics like grades or demographics, it might remain oblivious to factors such as learning styles, which are visible to an expert teacher and can significantly influence teaching methods. Naive imitation without access to this "private" information will only learn a single policy for each observed characteristic , leading to sub-optimal actions. On the other hand, a purely online approach requires extensive trial and error to result in meaningful decisions.

We integrate offline expert data with online RL, treating the scenario as a zero-shot meta-reinforcement learning (meta-RL) problem with an unknown distribution over unobserved contextual variables. Unlike typical meta-RL frameworks where the learner is exposed to multiple instances during training (different students in our example) to learn the underlying distribution [25; 26], our approach only leverages offline expert data to infer the distribution of unobserved factors, embodying a _zero-shot_ meta-RL paradigm .

**Contributions.** We define a Bayesian regret minimization objective and consider unobserved variables as parameters under an unknown prior distribution. We use empirical Bayes to derive an informative prior over the unobserved variables from expert data. We use the learned prior distribution to drive exploration in the online RL task, using approaches like posterior sampling . We propose two procedures to learn such a prior: (1) a parametric approach that can utilize any existing knowledge about the parametric form of the prior distribution, and (2) a nonparametric approach that employs the principle of maximum entropy when such prior knowledge does not exist. We call our framework Experts-as-Priors or ExPerior for short. See Figure 1 for a goal-oriented RL example. ExPerior outperforms existing offline, online, and offline-online baselines in multi-armed bandits, Markov decision processes (MDPs), and partially observable MDPs. For multi-armed bandits, we find the Bayesian regret incurred by ExPerior is proportional to the entropy of the optimal action under the prior distribution, aligning with the entropy of expert policy if the experts are optimal. We introduce a frequentist algorithm for multi-armed bandits and prove a Bayesian regret bound proportional to a term that closely resembles the entropy of the optimal action. Our results suggest using the entropy of expert demonstrations to evaluate the impact of unobserved factors.

## 2 Related Work

Our work is an addition to the recent body of reinforcement learning research that leverages offline demonstrations to speed up online learning [29; 10; 30; 7; 9]. Classic algorithms such as DDPGfD  and DQfD  achieve this by combining imitation learning and RL. They modify DDPG  and DQN  by warm-starting the algorithms' replay buffers with expert trajectories and ensuring that the offline data never gets overridden by online trajectories. Closely related to our study is the meta-RL literature, which aims to accelerate learning in a given RL task by using

Figure 1: Illustration of ExPerior in a goal-oriented task. Step 1 (Offline): The experts demonstrate their policies for different goal types while observing them. Step 2 (Offline): The expert data \(_{}\) only contains the trajectories states/actions — goal types are not collected. We form an informative prior distribution over the goal types (unobserved factors) using \(_{}\). Step 3 (Online): The goal type is unknown but drawn from the same distribution of goals in Step 1. The learner uses the learned prior for posterior sampling.

prior experience from related tasks [33; 34; 35]. These papers present model-agnostic meta-learning training objectives to maximize the expected reward from novel tasks as efficiently as possible.

Two unique features distinguish our problem from the settings considered above. First, our setting assumes heterogeneity within the offline data and with the online RL task that is unobserved to the learner, while the (optimal) experts are privy to that heterogeneity. Second, we assume the learner will only interact with one online task, making our setup similar to zero-shot meta-RL [27; 36; 37]. Most similar to our work is the ExPLORe algorithm , which assigns optimistic rewards to the offline data during the online interaction and runs an off-policy algorithm using both online and labelled offline data as buffers. For our setting, the algorithm incentivizes the learner to explore the expert trajectories, leading to faster convergence. We consider this work one of our baselines.

Our methodology utilizes only the state-action trajectory data from expert demonstrations without task-specific information or reward labels. Other similar methods require additional offline information. For example, Nair et al.  assume that the offline data contains the reward labels and use that to pre-train a policy, which is then fine-tuned online. Mendonca et al.  require task labelling for each trajectory and use the offline data to learn a single meta-learner. Similarly, Zhou et al.  and Rakelly et al.  require the task label and reward labels. They then infer the task during online interaction and use the task-specific offline data. Lee et al.  requires a large amount of noisy expert data with reward labels, in addition to the optimal trajectory data, for good performance. Finally, our methodology builds on posterior sampling . Hao et al. [22; 23] consider a similar problem using posterior sampling to leverage offline expert demonstration data to improve online RL. However, they assume homogeneity between the expert data and online tasks. In contrast, our setting accounts for heterogeneity.

## 3 Problem Setup

**Decision Model for Unobserved Heterogeneity.** To account for unobserved heterogeneity, we consider a generalization of finite-horizon Markov Decision Processes (MDPs) with a notion of probabilistic contextual variables [44; 13; 21]. The underlying model for the MDP will additionally depend on an unobserved variable that encapsulates the information hidden from the learner. For example, consider a personalized education setup where teaching a student corresponds to a task, and the learning agent can observe students' characteristics, like their demographic status and grades. Other factors, such as the student's learning style (e.g., visual learners or self-study), may not be readily available, even though they are important in determining the optimal teaching style.

Let \(\) be the set of all _unobserved_ context variables that can describe the unobserved heterogeneity of the decision-making problem (e.g., the set of all possible learning styles). A contextual MDP \(=(,,,R,H,,^{})\) is parameterized by states \(\), actions \(\), transition function \(:( )\), reward function \(R:()\), horizon \(H>0\), initial state distribution \(()\), and context distribution \(^{}\). We assume the transition/reward functions and \(^{}\) are unknown, and for simplicity, \(\) does not depend on the context variable. For each unobserved context \(c^{}\), we consider \(T\) episodes, where at the beginning of each episode \(t[T]\), an initial state \(s_{1}\) is sampled. Then, at each timestep \(h[H]\), the learner chooses an action \(a_{h}\), observes a reward \(r_{h} R(s_{h},a_{h},c)\) and the next state \(s_{h+1}(s_{h},a_{h},c)\). Without loss of generality, we assume the states are partitioned by \([H]\) to make the notation invariant to the timestep. Let \(\) be the set of all Markovian policies. For a policy function \(:()\) and context variable \(c\), we define the value function \(V_{c}()=[_{h=1}^{H}r_{h}\;|\;,c]\) and the Q-function as \(Q_{c}^{}(s,a):=[_{h^{}=h}^{H}r_{h^{ }}\;|\;s_{h}=s,a_{h}=a,,c]\) for all \(s,a\). Moreover, we define the optimal policy for a context variable \(c\) as \(_{c}:=_{}V_{c}()\). Note that since the context variable is unobserved, the learner's policy will not depend on it. The learning agent's goal is to learn history-dependent distributions \(p^{1},,p^{T}()\) over Markovian policies to minimize the expected regret, defined as \(:=_{c^{}}[_{t=1}^{T}V_{c}(_{c})- _{^{t} p^{t}}[V_{c}(^{t})]]\).

In the personalized education example, the above setup assumes a fixed distribution \(^{}\) over the set of learning styles and aims to minimize expected regret over the population of students. Our setup and regret assume the unobserved factors remain fixed during training. This captures scenarios wherein the unobserved variables correspond to less-variant factors (a student's learning style is more likely to remain unchanged). No learning algorithm can control the regret value if we allow the unobserved factors to change arbitrarily throughout \(T\) episodes without access to hidden information; consider a two-armed bandit with a context value drawn with uniform probability from \(=\{c_{1},c_{2}\}\) that can change at each episode. Assume the expected reward of the first arm under \(c_{1}\) and \(c_{2}\) is one and zero, respectively, and it is reversed for the other arm. Any algorithm that does not have access to \(c\) would result in linear regret since each action is sub-optimal with a probability of \(0.5\), independent of the algorithm's choice.

**Remark.** Our setup can be formulated as a Bayesian model parameterized by \(\), and our regret can be seen as the Bayesian regret of the learner. However, the distribution \(^{}\) is not the learner's prior belief about the true model as it is often formulated in Bayesian learning, but a distribution over potential contexts that the learner can encounter. Our setup can thus be seen as a meta-learning problem. In fact, it is _zero-shot_ meta-learning since we do not assume having access to more than one online RL task during training -- we only learn the prior distribution using the offline data.

Expert Demonstrations.In addition to the online setting described above, we assume the learner has access to an offline dataset of expert demonstrations \(_{}\), where each demonstration \(_{}=(s_{1},a_{1},s_{2},a_{2},,s_{H},a_{H},s_{H+1})\) refers to an interaction of the expert with a decision-making task during a _single_ episode, containing the actions made by the expert and the resulting states. We assume that the unobserved context variables for \(_{}\) are drawn i.i.d. from distribution \(^{}\), and the expert had access to such unobserved variables (private information) during their decision-making. Moreover, we assume the expert follows a near-optimal strategy [22; 23].

**Assumption 1** (Noisily Rational Expert).: For any \(c\), experts select actions based on a distribution defined as \(p_{}(a s\,;\,c)\{ Q_{c}^{ _{c}}(s,a)\}\), for all \(s,a\), and some known competence value of \([0,]\). In particular, the expert follows the optimal policy if \(\).

We assume experts do not provide any rationale for their strategy, nor do we have access to rewards in the offline data; this is a combination of imitation and online learning rather than offline RL .

## 4 Experts-as-Priors Framework for Unobserved Heterogeneity

Our goal is to leverage offline data to help guide the learner through its interaction with the decision-making task. The key idea is to use expert demonstrations to infer a _prior_ distribution over \(\) and then to use a Bayesian approach such as posterior sampling  to utilize the inferred prior for a more informative exploration. If the current context is from the same distribution of contexts in the offline data, we expect that using such priors will lead to faster convergence to optimal trajectories compared to the commonly used non-informative priors. Consider the personalized education example. Suppose we have gathered offline data on an expert's teaching strategies for students with similar observed information like grade, age, location, etc. The teacher can observe more fine-grained information about the students that is generally absent from the collected data (e.g., their learning style). Our work relies on the following observation: The space of the optimal strategies for students with similar observed information but different learning styles is often much smaller than the space of all possible strategies. With the inferred prior distribution, the learner needs only to focus on the span of potentially optimal strategies for a new student, allowing for significantly more efficient exploration.

We resort to empirical Bayes and use maximum marginal likelihood estimation  to construct a prior distribution from \(_{}\). Given a probability distribution (prior) \(\) on \(\), the marginal likelihood of an expert demonstration \(_{}=(s_{1},a_{1},s_{2},a_{2},,s_{H},a_{H},s_{H+1}) _{}\) is given by

\[_{}(_{}\,;\,)=_{c }[(s_{1})_{h=1}^{H}p_{}(a_{h} s _{h}\,;\,c)(s_{h+1} s_{h},a_{h},c)].\] (1)

We aim to find a prior distribution to maximize the log-likelihood of \(_{}\) under the model described in (1). This is equivalent to minimizing the KL divergence between the marginal likelihood \(_{}\) and the empirical distribution of expert demonstrations, which we denote by \(}_{}\). In particular, we form an uncertainty set over the set of plausible priors as \(():=\{\,;\,_{}(}_{}\,\|\,_{}(\,;\, ))\}\), where the value of \(\) can be chosen based on the number of samples so the uncertainty set contains the true prior with high probability . However, the set of plausible priors does not uniquely identify the appropriate prior. In fact, even for \(=0\), \(()\) can have infinite plausible priors. To solve this ill-posed problem, we propose two approaches, parametric and nonparametric prior learning.

**Parametric Experts-as-Priors.** For settings where we have existing knowledge about the parametric form of the prior, we can directly apply maximum marginal likelihood estimation to learn it. Inparticular, we define the parametric expert prior as the following. Note that we can calculate the gradients of the marginal likelihood using the score function estimator .

**Definition 1** (Parametric Expert Prior).: Let \(\) be a set of plausible prior distribution parameters (e.g., Beta distribution parameters for a Bernoulli bandit). We call \(_{^{*}}\) a parametric expert prior, iff \(^{*}_{}_{_{ }}-}}(\,;\,_{})\).

**Nonparametric Experts-as-Priors.** For settings where there is no existing knowledge on the parametric form of the prior, we can employ the principle of maximum entropy to choose the _least informative_ prior that is compatible with expert data:

**Definition 2** (Max-Entropy Expert Prior).: Let \(_{0}\) be a non-informative prior on \(\) (e.g., a uniform distribution). Given some \(>0\), we define the maximum entropy expert prior \(_{}\) as the solution to the following optimization problem:

\[_{}=*{arg\,min}_{}\,_{} (_{0})( ).\] (2)

Note that the set of plausible priors \(()\) is a convex set, and therefore, (2) is a convex optimization problem. We can derive the solution to problem (2) using Fenchel's duality theorem [47; 48]:

**Proposition 1** (Max-Entropy Expert Prior).: _Let \(N=|_{}|\) be the number of demonstrations in \(_{}\). For each \(c\) and demonstration \(_{}=(s_{1},a_{1},s_{2},a_{2},,s_{H},a_{H},s_{H+1}) _{}\), define \(m_{_{}}(c)\) as the (partial) likelihood of \(_{}\) under \(c\), i.e., \(m_{_{}}(c)=_{h=1}^{H}p_{}(a_{h} s_{h}\,;\,c )(s_{h+1} s_{h},a_{h},c)\)._

_Denote \((c)^{N}\) as the vector with elements \(m_{_{}}(c)\) for \(_{}_{}\). Moreover, let \(^{}^{ 0}\) be the optimal solution to the Lagrange dual problem of (2). Then, the solution to optimization (2) is:_

\[_{}(c)=_{n}(c)^{} _{n}\}}{_{c^{}_{0}}[\{ (c^{})^{}_{n}\}]},\]

_where \(\{_{n}\}_{n=1}^{}\) is a sequence converging to the following supremum:_

\[_{^{N}}-_{c_{0}}[ \{(c)^{}\}]+}{ N}_{i=1}^{N}(}{^{}}).\] (3)

The proof is provided in Appendix A.3. Instead of solving for \(^{}\), we set it as a hyperparameter and then solve (3). Even though Proposition 1 requires the correct form of Q-functions for different values of \(c\), we will see in the following sections that we can parameterize the Q-functions and treat those parameters as a proxy for the unobserved factors. Once such a prior is derived, we can employ any Bayesian approach for decision-making. We provide a pseudo-algorithm for ExPerior in Algorithm 1. The following sections will detail the algorithm for bandits and MDPs.

```
1:Input: Expert demonstrations \(_{}\), Reference distribution \(_{0}\), \(^{} 0\), and unknown \(c^{}\).
2:\(_{}\)MaxEntropyExpertPrior\((_{0},_{},^{})\)
3:\(history\{\}\)
4:for episode \( 1,2,\)do
5: sample \(c_{t}_{}( history)\)// posterior sampling
6:for timestep \(h 1,2,,H\)do
7: take action \(a_{h}^{t}_{c_{t}}( s_{h})\)
8: observe \(r_{h}^{t} R(s_{h}^{t},a_{h}^{t},c)\), \(s_{h+1}^{t}(s_{h}^{t},a_{h}^{t},c)\), and append \((a_{h}^{t},r_{h}^{t},s_{h+1}^{t})\) to _history_
9:endfor
10:endfor ```

**Algorithm 1** Experts-as-Priors (ExPerior-MaxEnt)

## 5 Learning in Bandits

\(K\)**-armed Bandits**. For \(K\)-armed bandits, note that \(=\), \(H=1\), and \(=\{1,,K\}\). Each expert demonstration \(_{}=a\) will be the pulled arm by the expert for a particular bandit, and the (partial) likelihood function in Proposition 1 can be simplified as \(m_{_{}}(c)=p_{}(a\,;\,c)\). This likelihood function only depends on the context variable \(c\) through the expert policy \(p_{}\), and since \(p_{}\) only depends on \(c\) through the mean reward function (Assumption 1), we can consider the set of mean reward functions as a proxy for the unobserved context variables \(\). e.g. in a Bernoulli \(K\)-armed bandit setting, we can define \(_{}=\{a_{a},\,\,;\,^{K}\}\).

**Posterior Sampling.** With the above parameterizations of \(\), we can use Proposition 1 to derive the maximum entropy prior distribution over the context parameters. However, we cannot sample from the exact posterior since the derived prior is not a conjugate prior for standard likelihood functions. Instead, we resort to approximate posterior sampling via stochastic gradient Langevin dynamics (SGLD) . We call this method ExPerior-MaxEnt in our experiments. We also employ a parametric approach as discussed in section 4, which we call ExPerior-Param. In particular, we use the Beta distribution as our prior model and learn the parametric expert prior in Definition 1.

In the following, we evaluate our approach compared to online methods that do not use expert data and offline behaviour cloning. We provide an empirical regret analysis for ExPerior based on the informativeness of expert data, number of actions, and number of training episodes. We also discuss the robustness of ExPerior to misspecified expert models and the advantage of ExPerior-MaxEnt to ExPerior-Param when the parametric prior model is misspecified. To characterize the effect of expert data on the learner's performance, we propose an alternative for \(K\)-armed bandits inspired by the successive elimination and derive a Bayesian regret bound for it.

Experiments.We consider \(K\)-armed Bernoulli bandits for our experimental setup. We evaluate the learning algorithms in terms of the Bayesian regret over multiple (prior) distributions \(^{}\) over the unobserved contexts. In particular, we consider up to \(N_{^{}}=64\) different beta distributions, where their parameters are chosen to span a different range of heterogeneity, consisting of tasks with various expert data informativeness. To estimate the Bayesian regret, we sample \(N_{}=128\) bandit tasks from each prior distribution and calculate the average regret. We use \(N_{}=1000\) expert demonstrations for each prior distribution in our experiments. We compare ExPerior to the following baselines: (1) Behaviour cloning (BC), which learns a policy by minimizing the cross-entropy loss between the expert demonstrations and the agent's policy solely based on offline data. (2) Naive Thompson sampling (Naive-TS) that chooses the action with the highest sampled mean from a posterior distribution under an uninformative prior. (3) Naive upper confidence bound (Naive-UCB) algorithm that selects the action with the highest upper confidence bound. Both Naive-TS and Naive-UCB ignore expert demonstrations. (4) UCB-ExPLORe, a variant of the algorithm proposed by Li et al.  tailored to bandits. It labels the expert data with optimistic rewards and then uses it alongside online data to compute the upper confidence bounds for exploration, and (5) Oracle-TS, which performs exact Thompson sampling with the true prior distribution \(^{}\). For a fair comparison, we also consider a variant of Oracle-TS, which uses SGLD for approximate posterior sampling.

**Comparison to baselines.** Figure 2 demonstrates the average Bayesian regret for various prior distributions over \(T=1{,}500\) episodes with \(K=10\) arms. To better understand the effect of expert data, we categorize the prior distributions by the entropy of their optimal actions into low entropy (less than 0.8), high entropy (greater than 1.6), and medium entropy. Oracle-TS and ExPerior-Param outperform other baselines, yet the performance of ExPerior-MaxEnt is comparable to the SGLD variant of Oracle-TS. This indicates that the maximum entropy prior derived from Proposition 1 closely approximates the true prior distribution, \(^{}\), and the performance difference with Oracle-TS is primarily due to approximate posterior sampling. Moreover, the pure online algorithms Naive-TS and Naive-UCB, which disregard expert data, display similar performance across different entropy levels, contrasting with other algorithms that show significantly reduced regret in low-entropy contexts. This underlines the impact of expert data in settings where the unobserved confounding has less effect on the optimal actions. Specifically, in the extreme case of no unobserved heterogeneity, BC is anticipated to yield optimal performance. Additionally, Naive-UCB surpasses UCB-ExPLORe in medium and high entropy settings, possibly due to the over-optimism of the reward labelling in Li et al. , which can hurt the performance when the expert demonstrations are uninformative.

**Empirical regret analysis for Experts-as-Priors.** We examine how the quality of expert demonstrations affects the Bayesian regret achieved by ExPerior. Settings with highly informative demon

Figure 2: The Bayesian regret of ExPerior and baselines for \(K\)-armed Bernoulli bandits (\(K=10\)). We consider three categories of prior distributions based on the entropy of the optimal action.

strations, where unobserved factors minimally affect the optimal action, should exhibit near-zero regret since there is no diversity in the unobserved contexts, and the experts are near-optimal. Conversely, in scenarios where unobserved factors significantly influence the optimal actions, we anticipate the regret to align with standard online regret bounds, similar to the outcomes of Thompson sampling with a non-informative prior. We conduct trials with ExPerior and Oracle-TS across various numbers of arms over \(T=1,\!500\) episodes, calculating the mean and standard error of Bayesian regret across distinct prior distributions. As depicted in Figure 3 (a), both ExPerior and Oracle-TS yield sub-linear regret relative to \(K\) and \(T\), comparable to the established regret bound of \(()\) for Thompson sampling. However, the middle panel indicates that the regret of ExPerior is proportional to the entropy of the optimal action, having an almost _linear_ relationship. This observation seems to be in contrast with the standard Bayesian regret bounds for Thompson sampling under correct prior that have shown a sublinear relationship of \(((_{c})})\), where \((_{c})\) denotes the entropy of the optimal action under \(^{}\). We analyze this observation in section 5.1.

**Ablations.** We also run additional experiments to assess the robustness of ExPerior to misspecified experts. We create expert data from different experts with various competence levels, such as optimal, noisily rational, and random-optimal experts, where the latter chooses an action optimally with a fixed probability and randomly otherwise. Table 1 shows ExPerior's robustness to different expert models. Setting \(=10\) for training ExPerior-MaxEnt and \(=1\) for ExPerior-Param achieves consistent out-performance among different expert types. Moreover, We evaluate the advantage of learning nonparametric max-entropy prior over misspecified parametric priors in Table 2. Even though ExPerior-Param with a Beta prior outperforms ExPerior-MaxEnt, ExPerior-MaxEnt is superior to ExPerior-Param if the prior mismatches the correct form (e.g., Gaussian or Gamma).

### An Alternative Frequentist Approach for \(K\)-armed Bandits

To analyze the effect of expert data on the Bayesian regret, we devise an alternative _frequentist_ approach, based on the successive elimination algorithm , which follows a similar intuition to Experts-as-Priors. In particular, we prove a bound on its Bayesian regret and show that the derived bound is proportional to a term that closely resembles the entropy of the optimal action, showing that the observation in the middle panel of Figure 3 (a) is consistent within different approaches.

    &  &  &  \\   & & \(=1\) & \(=2.5\) & \(=10\) & \(=0.0\) & \(=0.25\) & \(=0.5\) & \(=0.75\) \\  ExPerior-MaxEnt (\(=0\)) & 51.7 \(\) 5.1 & 52.3 \(\) 5.3 & 52.3 \(\) 5.3 & 52.0 \(\) 5.1 & 51.7 \(\) 5.0 & 52.3 \(\) 5.3 & 52.1 \(\) 5.1 & 52.0 \(\) 5.1 & 51.8 \(\) 5.0 \\ ExPerior-Param (\(=1\)) & 11.1 \(\) 4.3 & 33.1 \(\) 7.3 & **12.6** \(\) **3.5** & 11.7 \(\) 3.8 & 10.9 \(\) 4.2 & 40.1 \(\) 9.6 & 12.3 \(\) 4.7 & 11.4 \(\) 4.0 & 10.7 \(\) 4.2 \\ ExPerior-MaxEnt (\(=1\)) & 45.7 \(\) 3.4 & 52.2 \(\) 5.3 & 51.6 \(\) 5.1 & 50.0 \(\) 4.8 & 47.3 \(\) 3.8 & 52.5 \(\) 5.3 & 51.0 \(\) 4.8 & 49.1 \(\) 4.2 & 48.0 \(\) 3.6 \\ ExPerior-Param (\(=1\)) & 9.1 \(\) 3.0 & **21.3** \(\) **1.3** & 13.4 \(\) 2.9 & **10.1** \(\) **3.0** & 9.4 \(\) 3.1 & **22.8** \(\) **1.3** & **9.8** \(\) 3.0** & **8.6** \(\) **2.7** & **8.8** \(\) **2.9** \\ ExPerior-MaxEnt (\(=2\)) & **37.0** \(\) **1.9** & 52.1 \(\) 5.3 & 51.0 \(\) 4.9 & 47.1 \(\) 4.5 & 38.3 \(\) 2.0 & **52.1** \(\) **5.1** & 48.9 \(\) 4.1 & 44.8 \(\) 3.2 & **40.5** \(\) 2.1 \\ ExPerior-Param (\(=2\)) & **8.5** \(\) **2.8** & 24.3 \(\) 1.2 & 19.0 \(\) 2.1 & 12.8 \(\) 2.9 & **9.2** \(\) **3.1** & 24.6 \(\) 12.5 & 19.9 \(\) 3.0 & 10.9 \(\) 3.2 & **40.8** \(\) **2.9** \\ ExPerior-MaxEnt (\(=10\)) & 38.5 \(\) **4.2** & **5.0** \(\) **4.7** & **4.6** \(\) 4.4 & **39.7** \(\) **2.9** & **29.2** \(\) 3.6** & 52.5 \(\) 5.3 & 41.2 \(\) **2.6** & **37.7** \(\) **2.8** & **31.9** \(\) **3.0** \\ ExPerior-Param (\(=10\)) & 11.2 \(\) 4.8 & 26.9 \(\) 1.2 & 25.0 \(\) 1.5 & 21.0 \(\) 2.1 & 11.8 \(\) 3.3 & 26.8 \(\) 1.1 & 23.2 \(\) 1.8 & 20.1 \(\) 2.5 & 16.1 \(\) 3.0 \\ Oracle-TS & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 & 8.5 \(\) 2.7 \\ Oracle-TS (SGLD) & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 & 24.2 \(\) 3.9 \\   

Table 1: Ablation experiments to assess the robustness of ExPerior to misspecified expert models. Random-optimal experts choose the optimal action with probability \(\) and choose random actions with probability \(1-\). ExPerior-MaxEnt achieves consistent out-performance by setting the hyperparameter \(=10\). while ExPerior-Param get almost similar results for \(=1\) and \(=2.5\).

Figure 3: (a) Empirical analysis of ExPerior’s regret in Bernoulli bandits based on the (left) number of arms, (middle) entropy of the optimal action, and (right) number of episodes. (b) The regret bound from Theorem 2 vs. the entropy of the optimal action. The linear relationship is consistent with the middle panel of (a).

The idea of successive elimination is to identify suboptimal arms and deactivate them over time. In particular, it runs a uniform sampling policy among active arms and builds confidence intervals for each. It then deactivates all the arms with an upper confidence bound smaller than at least one arm's lower confidence bound. We modify this algorithm using the policy derived from expert demonstrations instead of a uniform sampling policy. Recall that in \(K\)-armed bandits, each expert trajectory \(_{}\) represents the pulled arm by the expert. Hence, the empirical distribution of expert demonstrations can be seen as a sampling policy over different arms. To simplify the analysis, we employ a deterministic sampling approach by pulling each arm a fixed number of times based on its probability. To do so, we discretize the expert policy with a step size \(p_{}\), which leads to a relative frequency of \(}_{}(a)/p_{}\) for an arm \(a\). In particular, we can choose \(p_{}=_{a;\;}_{}(a) 0}}_{ }(a)\). We provide the concrete algorithm in Algorithm 2 and prove the following Bayesian regret bound:

**Theorem 2**.: _Consider a stochastic \(K\)-armed bandit and let \(p\) be the empirical expert policy. Assume that (i) the mean reward function is bounded in \(\) for all arms, (ii) \(Tp(a)}\), (iii) the expert is optimal, i.e., \( a:\;p(a)=_{}(a\,;\,^{*})\) and \(\), and (iv) the learner follows Algorithm 2. Then, with probability at least \(1-\),_

\[(TK/)}_{a,a^{ },a a^{}})}(1 -)})}[+) }].\] (4)

See Appendix A.4 for the proof. Two terms in (4) depend on expert data: (1) The relative standard deviation between any two pairs of arms and (2) a scaling factor that depends on the magnitude of probability that the arms are optimal. For homogeneous demonstrations, where the expert data only includes one unique pulled arm, the standard deviation (Term 1) is zero, resulting in zero regret. However, in extreme heterogeneity, where the empirical expert distribution is uniform over the arms, we have \( K\). 2 Finally, to assess the relationship between the regret bound and the entropy of the expert data, we fix \(K=2\), \(T=100\), and plot the bound from (4) as a function of the entropy of the optimal action for various prior distributions. Figure 3 (b) demonstrates a linear relationship, similar to the regret incurred by ExPerior in Figure 3 (a). This observation opens up new directions to further analyze the regret for ExPerior and similar approaches in MDPs.

## 6 Learning in Markov Decision Processes (MDPs)

For MDPs, we need to parameterize both the mean reward and transition functions. However, we assume the transition functions are invariant to the context variables to simplify our methodology and avoid extra modelling assumptions. Under this assumption, it is sufficient to parameterize the _optimal_ Q-functions, e.g., using a deep Q-network (DQN) and treat those parameters as a proxy for the unobserved context variables, i.e., \(_{}:=\{(s,a) Q(s,a:\,)\;;\,\}\), where \(\) is the set of parameters for a DQN. We can then derive a closed-form log-pdf of the posterior distribution under the maximum entropy prior. See Appendix A.5 for details. The derived posterior log-pdf can

   & ExPerior-Param & ExPerior-MaxEnt & Gamma Prior & Beta-SGLD & Prior & Normal Prior & Oracle-TS & Oracle-TS (gold) \\ 
**Low Entropy** & \(0.7 0.3\) & \(11.6 1.3\) & \(39.3 2.2\) & \(60.2 6.3\) & \(546.5 153.4\) & \(0.9 0.4\) & \(11.0 1.6\) \\
**Mid-Entropy** & \(6.8 0.8\) & \(25.7 1.2\) & \(36.8 0.9\) & \(40.4 2.0\) & \(492.5 185.6\) & \(7.3 0.8\) & \(21.2 1.0\) \\
**High-Entropy** & \(24.5 2.8\) & \(41.3 2.2\) & \(51.8 3.6\) & \(45.6 2.0\) & \(461.8 104.8\) & \(21.5 2.2\) & \(39.9 3.2\) \\  

Table 2: Superiority of ExPerior-MaxEnt compared to ExPerior-Param with misspecified parametric prior.

then be used as the loss function for DQN Langevin Monte Carlo  as the counterpart for Thompson sampling with SGLD. However, running Langevin dynamics can lead to highly unstable policies due to the complexity of the optimization landscape in DQNs. Instead, we use a heuristic that combines the learned prior distribution with bootstrapped DQNs .

The original method of Bootstrapped DQNs utilizes an ensemble of \(L\) randomly initialized Q-networks. It samples a Q-network uniformly at each episode and uses it to collect data. Then, each Q-network is trained using the temporal difference loss on parts of or possibly the entire collected data. This method and its subsequent iterations  achieve deep exploration by ensuring diversity among the learned Q-networks. To incorporate Bootstrapped DQN into the ExPerior framework and utilize the expert data, we can formulate the ensemble as a discrete prior distribution over the Q-networks. Let \(_{}=(_{}^{1},,_{}^{L})\) be the parameter vector for an ensemble of Q-functions. We can define the ensemble prior, parameterized by \(_{}\), as \(_{_{}}():=_{i= 1}^{L}(_{}^{i}=)\) for any \(\). Based on this prior model, we can learn the parametric expert prior using maximum marginal likelihood estimation, as formulated below.

**Proposition 3** (Ensemble Marginal Likelihood).: _Consider a contextual MDP \(=(,,,R,H,,^{*})\). Assume the transition function \(\) does not depend on the context variables and Assumption 1 holds. Then, the negative marginal log-likelihood of expert data \(_{E}\) under the ensemble prior \(_{_{}}\) is upper bounded by_

\[-_{E}(_{E}\,;\,_{_{ }})_{i=1}^{L}_{_{E} }_{(s,a)}(_{a^{}}\{  Q(s,a^{}\,;\,_{}^{i})\} )- Q(s,a\,;\,_{}^{i}),\]

_where \(\) is the competence level of the expert in Assumption 1._

Proposition 3 is proved in Appendix A.6. We can then initialize the Q-networks in the Bootstrapped DQN method using ensemble parameters that minimize the above upper bound. We will refer to this method as ExPerior-Param. As an alternative approach, instead of minimizing the above upper bound, we can match the discrete prior distribution \(_{_{}}\) to the max-entropy prior by initializing the Q-functions in the ensemble with parameters sampled from the max-entropy expert prior. In particular, we can apply SGLD on the log-pdf of the max-entropy prior derived in Appendix A.5. We will refer to this approach as ExPerior-MaxEnt.

**Experimental Setup.** One challenge in RL is the reward _sparsity_, where the learner needs to explore the environment deeply to observe rewards. Utilizing expert demonstrations can significantly improve the efficiency of exploration. Here, we focus on "Deep Sea," a sparse-reward tabular RL environment proposed by Osband et al.  to assess deep exploration for different RL methods. The environment is an \(M M\) grid, where the agent starts at the top-left corner of the map, and at each time step, it chooses an action from \(=\{,\}\}\) to move to the left or right column, while going down by one row. In the original version of Deep Sea, the goal is always on the bottom-right corner of the map. We introduce unobserved contexts by defining a distribution over the goal columns while keeping the goal row the same. We consider four types of goal distributions where the goal is situated at (1) the bottom-right corner of the grid, (2) uniformly at the bottom of any of the right-most \(\) columns, (3) uniformly at the bottom of any of the right-most \(\) columns, and (4) uniformly at the bottom of any of the \(M\) columns. We set \(M=30\) and generate \(N=1{,}000\)

   &  & =1\)**} \\   & \(=0.1\) & \(=1\) & \(=2.5\) & \(=10\) & \# Harard = 2 & \# Harard = 5 & \# Harard = 7 & \# Harard = 9 \\   \\  ExPerior-MaxEnt & -22.58 \(\) 1.17 & **6.000 \(\) 0.00** & 3.58 \(\) 0.89 & 1.62 \(\) 1.85 & 11.47 \(\) 0.52 & **5.71 \(\) 0.67** & **6.00 \(\) 0.00** & **6.00 \(\) 0.00** \\ ExPerior-Param & -23.32 \(\) 0.69 & -4.31 \(\) 1.80 & 5.27 \(\) 0.51 & **6.00 \(\) 0.00** & **12.00 \(\) 0.37** & 2.11 \(\) 1.41 & 5.42 \(\) 0.40 & -4.31 \(\) 1.80 \\ Naive Boot-DQN & -23.32 \(\) 0.69 & -23.32 \(\) 0.69 & -23.32 \(\) 0.69 & -23.32 \(\) 0.69 & -14.36 \(\) 5.88 & -20.57 \(\) 2.91 & -20.39 \(\) 1.75 & -23.32 \(\) 0.69 \\ ExPLORe & **5.99 \(\) 0.00** & **6.00 \(\) 0.00** & **6.00 \(\) 0.00** & **6.00 \(\) 0.00** & -30.68 \(\) 1.24 & -10.04 \(\) 1.66 & -13.00 \(\) 1.90 & **6.00 \(\) 0.00** \\ Optimal & 6.00 \(\) 0.00 & 6.00 \(\) 0.00 & 6.00 \(\) 0.00 & 6.00 \(\) 0.00 & 1.20 \(\) 0.37 & 6.53 \(\) 0.31 & 6.00 \(\) 0.00 & 6.00 \(\) 0.00 \\   \\  ExPerior-MaxEnt & -23.36 \(\) 1.26 & 12.26 \(\) 0.29 & 12.68 \(\) 0.03 & **12.71 \(\) 0.03** & **13.02 \(\) 0.18** & **12.78 \(\) 0.11** & **12.78 \(\) 0.06** & 12.26 \(\) 0.29 \\ ExPerior-Param & -25.53 \(\) 2.35 & **12.64 \(\) 0.08** & **12.70 \(\) 0.03** & 12.68 \(\) 0.03 & 13.00 \(\) 0.18 & **12.78 \(\) 0.12** & 12.73 \(\) 0.07 & **12.64 \(\) 0.08** \\ Naive Boot-DQN & -23.32 \(\) 0.69 & -23.32 \(\) 0.69 & -23.32 \(\) 0.69 & -23.32 \(\) 0.69 & -14.39 \(\) 5.22 & -20.90 \(\) 2.86 & -20.39 \(\) 1.75 & -23.32 \(\) 0.69 \\ ExPLORe & **11.74 \(\) 0.41** & 11.75 \(\) 0.63 & 11.96 \(\) 0.28 & 12.3 \(\) 0.22 & -113.84 \(\) 17.50 & -54.89 \(\) 13.75 & -10.00 \(\) 7.60 & 11.75 \(\) 0.63 \\ Optimal & 12.71 \(\) 0.03 & 12.71 \(\) 0.03 & 12.71 \(\) 0.03 & 13.02 \(\) 0.18 & 12.78 \(\) 0.11 & 12.76 \(\) 0.06 & 12.64 \(\) 0.03 \\  

Table 3: The average reward per episode in Frozen Lake (PODMP) after \(90{,}000\) training steps.

samples from the optimal policies as offline expert demonstrations. To further evaluate ExPerior and showcase its applicability to partially-observed MDP, we also consider the "Frozen Lake" environment, which requires the learner to navigate to a goal while avoiding hazards . The learner cannot observe the hazard location, while the expert has access to the whole map. Taking action, reaching the goal, and hitting the hazard incur rewards of -2, 20, and -100, respectively. The frozen lake map is \(5 5\), where the hazard (weak ice) is randomly located in the interior squares. We consider different settings with 2, 5, 7, and 9 potential locations for the hazard. At the start of each episode, the hazard will be chosen randomly within the potential locations. We generate \(N=1{,}000\) samples from noisily rational experts with different competence levels for this environment.

**Baselines.** We compare ExPerior to the following: (1) ExPLORe, proposed by Li et al.  to accelerate off-policy reinforcement learning using unlabeled prior data. In this method, the offline demonstrations are assigned optimistic reward labels generated using the online data with regular updates. This information is then combined with the buffer data to perform off-policy learning. (2) Naive Boot-DQN, which is the original Bootstrapped DQN with randomly initialized Q-networks .

**Deep Sea Results.** Figure 4 demonstrates the average reward per episode achieved by the baselines for \(T=2{,}000\) episodes. For each goal distribution, we run the baselines with \(30\) different seeds and take the average to estimate the expected reward. ExPerior outperforms the baselines in all instances. However, the gap between ExPerior and the fully online Naive Boot-DQN, which measures the effect of using the expert data, decreases as we go from the low-entropy setting (upper left) to the high-entropy distribution over the contexts (bottom right). This is consistent with the empirical and theoretical results discussed in section 5 and confirms our expectation that the expert demonstrations may not be helpful under strong unobserved confounding (strong heterogeneity). The ExPLORe baseline substantially underperforms, even compared to the fully online Naive Boot-DQN (except for the first distribution with zero-entropy). We suspect this is because ExPLORe uses actor-critic methods as its backbone model, which are shown to struggle with deep exploration .

**Frozen Lake Results.** We run all the baselines for \(90{,}000\) steps with \(30\) different seeds. Table 3 shows the average reward after \(500\) evaluation steps at the end of the training. ExPerior outperforms the baselines in almost all instances except for the case of \(=0.1\), which corresponds to a nearly random expert. On the other hand, ExPLORe achieves near-optimal results for \(=0.1\). We hypothesize that ExPLORe's performance is mainly due to the superiority of their base actor-critic model since it can achieve near-optimal performance even when the expert trajectories are low-quality.

## 7 Conclusion

We introduce the Experts-as-Priors (ExPerior) framework, a novel empirical Bayes approach to address the problem of sequential decision-making using expert demonstrations with unobserved heterogeneity. We ground our methodology in the maximum entropy principle to infer a prior distribution from expert data that guides the learning process in different settings, including bandits, Markov decision processes (MDPs), and partially-observed MDPs. Our experimental evaluations demonstrate that we can effectively leverage the expert demonstrations to enhance learning efficiency under unobserved confounding. In multi-armed bandits, we illustrated through empirical analysis that the Bayesian regret incurred by our method is proportional to the entropy of the optimal action, highlighting its capacity to adapt based on the informativeness of the expert data. Our work offers a practical framework readily applied to a broad spectrum of decision-making tasks. One limitation of our work is the limited set of experiments, especially the lack of experiments with human-in-the-loop. Future directions include extending to more complex environments and further investigating the theoretical properties of our RL algorithm.

Figure 4: The average reward per episode over \(2{,}000\) episodes in ”Deep Sea.” The goal is located at the right column, uniformly at the right-most quarter of the columns, uniformly at the right-most half, and uniformly at random over all the columns, respectively. ExPerior outperforms the baselines in all instances.