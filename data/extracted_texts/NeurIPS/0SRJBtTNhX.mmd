# IntraMix: Intra-Class Mixup Generation for Accurate Labels and Neighbors

Shenghe Zheng, Hongzhi Wang, Xianglong Liu

Massive Data Computing Lab, Harbin Institute of Technology

shenghez.zheng@gmail.com wangzh@hit.edu.cn

Corresponding author.

###### Abstract

Graph Neural Networks (GNNs) have shown great performance in various tasks, with the core idea of learning from data labels and aggregating messages within the neighborhood of nodes. However, the common challenges in graphs are twofold: insufficient accurate (high-quality) labels and limited neighbors for nodes, resulting in weak GNNs. Existing graph augmentation methods typically address only one of these challenges, often adding training costs or relying on oversimplified or knowledge-intensive strategies, limiting their generalization. To simultaneously address both challenges faced by graphs in a generalized way, we propose an elegant method called IntraMix. Considering the incompatibility of vanilla Mixup with the complex topology of graphs, IntraMix innovatively employs Mixup among inaccurate labeled data of the same class, generating high-quality labeled data at minimal cost. Additionally, it finds data with high confidence of being clustered into the same group as the generated data to serve as their neighbors, thereby enriching the neighborhoods of graphs. IntraMix efficiently tackles both issues faced by graphs and challenges the prior notion of the limited effectiveness of Mixup in node classification. IntraMix is a theoretically grounded plug-in-play method that can be readily applied to all GNNs. Extensive experiments demonstrate the effectiveness of IntraMix across various GNNs and datasets. Our code is available at: https://github.com/Zhengsh123/IntraMix.

## 1 Introduction

Graph Neural Networks (GNNs) have demonstrated great potential in various tasks . However, most graph datasets lack high-quality labeled data and node neighbors, underscoring two key challenges for GNNs: the dual demands for accurate labels and rich neighborhoods .

Data augmentation is one way to address these two issues, but research on graph data augmentation is insufficient. Additionally, it is challenging to apply widely studied data augmentation ways designed for Euclidean data such as images to graphs due to their non-Euclidean nature . Therefore, unique graph augmentation methods are needed. While graph augmentation aims to generate high-quality labeled data and enrich node neighbors, most methods either focus on one aspect and often suffer from poor generalization ability. For example, some require generators, incurring additional training costs . Others rely on overly simplistic ways such as random drop, yielding little improvements . Still, some depend on excessive prior knowledge, weakening generalization abilities . Therefore, there is an urgent need for an effective and generalized augmentation method that can produce high-quality labeled nodes and adequately enrich node neighbors.

Reviewing existing methods, we find that they overlook the potential of low-quality labeled data, which can be obtained at a low cost. Extracting information from such data could enrich data diversity.

The cause of low-quality labels is the noise in labels, which results in a distribution different from the real one . The noise direction is usually mixed, so a natural idea is to blend noisy data, leveraging noise directionality to neutralize it and produce accurate labels. Therefore, Mixup  comes into our eyes as a method that involves mixing data. It is defined as \(= x_{i}+(1-)x_{j},= y_{i}+(1-)y_{j}\), where \((x_{i},y_{i}),(x_{j},y_{j})\) represent selected data, and \(y\) represents the label. Despite Mixup excels in Euclidean data, experiments suggest its limited ability in node classification task . Therefore, a question emerges: **Can Mixup solve augmentation problems for node classification?**

Due to the characteristics of graphs, using Mixup is challenging. The main reason Mixup performs poorly in node classification can be attributed to the graph topology. In Euclidean data such as images, the data generated by Mixup are independent , whereas in graphs, the generated nodes need to be connected to other nodes to be effective. This means that in graphs, the data generated by vanilla Mixup is difficult to determine its neighbors and connecting them to any class of nodes is inappropriate, as its distribution does not belong to any current class of data . Hence, the complex topology of the graph directly leads to the inapplicability of vanilla Mixup.

To address the issues, we propose IntraMix, a novel augmentation method for node classification, as shown in Figure 1(b). The core idea is that since nodes generated by Mixup between different class data are hard to find neighbors for, we mix nodes within the same class obtaining by pseudo-labeling (low-quality labels) instead . Hereby, the main benefit of this approach is that it addresses the primary challenge about neighborhood of applying Mixup on graphs. The generated node features are no longer a mixture of features from multiple groups, making it easier to find neighbors based on their respective groups. Additionally, the generated nodes have higher quality labels. Intuitively, if we simplify the label noise as \( N(0,^{2})\), the mean distribution of two noises \( N(0,^{2})\), with a smaller variance, increases the likelihood that the generated label is accurate. Therefore, we address the sparse high-quality labels by Intra-Class Mixup. It is important to emphasize that although some works have used Intra-Class Mixup , we are the first to use it as the sole augmentation method and to analyze it in depth.

Once the method for generating data is determined, the neighborhood selection method becomes very straightforward. For the neighbor selection strategy of the generated node \(v\), we connect \(v\) to two nodes with high confidence of the same class with \(v\). This has two benefits. Firstly, based on the assumption that nodes of the same class are more likely to appear as neighbors (neighborhood assumption) , we reasonably find neighbors for \(v\), providing it with information gain in training and inferencing. Secondly, by connecting \(v\) to two nodes that belong to the same class, we not only bring message interaction to the neighbors of these two nodes but also reduce the impact of noise that may still be present in direct connecting high-quality labels. In this neighbor selection way, we construct rich and reasonable neighborhood for nodes, enhancing the knowledge on the graph, which allows for the development of stronger GNN models for downstream tasks.

Therefore, IntraMix simultaneously addresses two challenging issues faced by graphs and exhibits strong generalization capabilities in an elegant way. Our key contributions are as follows:

* For the first time, we introduce Intra-Class Mixup as the core data augmentation in node classification, highlighting its effectiveness in generating high-quality labeled data.
* The proposed IntraMix tackles sparse labels and incomplete neighborhoods faced by graph datasets through an elegant and generalized way of Intra-Class Mixup and neighborhood selection.
* Extensive experiments demonstrate that IntraMix improves the performance of GNNs on diverse datasets. Theoretical analysis elucidates the rationale behind IntraMix.

Figure 1: a). Vanilla Mixup may retain label noise, and connecting generated nodes to original nodes may lead to incorrect propagation. b). IntraMix generates high-quality data by Intra-Class Mixup and enriches neighborhoods while preserving correctness by connecting generated data to high-quality nodes.

## 2 Preliminaries

**Notations:** Given a graph \(G=(V,E)\), where \(V=\{v_{i}\}_{i=1}^{N}\) is the node set, and \(E\) represents the edge set, the adjacency relationship between nodes can be represented by \(A\{0,1\}^{N N}\), where \(A_{ij}=1\) if and only if \((v_{i},v_{j}) E\). We use \(X R^{N D}\) to denote the node feature. The node labels are represented by \(Y\). Based on the presence or absence of labels, \(V\) can be divided into \(D_{l}=\{(x_{l_{1}},y_{l_{1}}),...(x_{l_{N}},y_{l_{N}})\}\) and \(D_{u}=\{x_{u_{1}},...x_{u_{N}}\}\). We can use pseudo-labeling to assign low-quality labels \(Y_{u}\) to nodes in \(D_{u}\), getting a low-quality set \(D_{p}=\{(x_{u_{1}},y_{u_{1}}),...(x_{u_{N}},y_{u_{N}})\}\). \(N_{i}=\{v_{j}|A_{ij}=1\}\) are the neighbors of \(v_{i}\). Detailed notation list can be found in Appendix A.

**Node Classification with GNNs:** Given a graph \(G\), the node classification involves determining the category of nodes on \(G\). GNNs achieve this by propagating messages on \(G\), representing each node as a vector \(h_{v}\). The propagation for the \(k\)-th layer of a GNN is represented as follows:

\[h_{v}^{k}=COM(h_{v}^{k-1},AGG(\{h_{u}^{k-1}|u N_{v}\}))\] (1)

where _COM_ and _AGG_ are COMBINE and AGGREGATE functions, respectively, and \(h_{v}^{k}\) denotes the feature of \(v\) at the \(k\)-th layer. The output \(h_{v}\) in the last layer of GNN is used for classification as \(y_{v}=softmax(h_{v})\), where \(y_{v}\) is the predicted label for \(v\).

## 3 Methodology

In this section, we provide a detailed explanation of IntraMix. Firstly, we present the Intra-Class Mixup in 3.1. It generates high-quality labeled nodes from low-quality data, addressing the issue of label sparsity. Then, we show the method for finding node neighbors in 3.2. Next, in 3.3, we conclude the workflow and conduct complexity analysis in 3.4. The framework is shown in Figure 2.

### Intra-Class Mixup

**Motivation:** In supervised learning, having more labels typically allows for learning finer classification boundaries and getting better result . However, obtaining accurately labeled data is costly in node classification. Nevertheless, directly utilizing low-quality labels from pseudo-labeling introduces noise detrimental to learning. As we know, low-quality pseudo-labeled data are often closer to the boundaries that models can learn from current data, containing useful information for classification . It is possible to generate high-quality data from them as mentioned in Sec 1. Due to the fact that label noise is typically spread in multiple directions, we take full advantage of the directional nature of the noise. By blending data, we make the noise distribution closer to zero,

Figure 2: The workflow of IntraMix involves three main steps. First, it utilizes pseudo-labeling to generate low-quality labels for unlabeled nodes. Following that, Intra-Class Mixup is employed to generate high-quality labeled nodes from low-quality ones. Additionally, it identifies nodes with high confidence in the same class and connects them, thus constructing a rich and reasonable neighborhood.

thereby reducing the noise in the labels. At the same time, considering that data generated by vallina Mixup is hard to find neighbors, we innovatively propose Intra-Class Mixup. It generates data with only a single label, making it easy to determine the neighborhood. It not only generates high-quality data but also facilitates the finding of neighbors. Additionally, we can provide theoretical guarantees.

**Approach**: We use pseudo-labeling to transform the unlabeled nodes \(D_{u}\) into nodes with low-quality labels \(D_{p}\). Then, we get \(D=D_{l} D_{p}\), where there are a few high-quality and many low-quality labels. In contrast to the vanilla Mixup, which is performed between random samples, we perform Mixup among nodes with the same low-quality labels to obtain high-quality labeled data guaranteed by Theorem 3.1. The generated dataset is represented as:

\[D_{m}=\{(,)|=M_{}(x_{i},x_{j}),=y_{i}=y_{j}\},\] (2)

where

\[M_{}(x_{i},x_{j})= x_{i}+(1-)x_{j},(x_{i},y_{i}),(x_{j},y _{j}) D.\] (3)

The number of generated nodes is manually set. The labels in \(D_{m}\) are of higher quality compared to \(D\), a guarantee provided by Theorem 3.1. The proof can be found in Appendix B.1. In other words, the generated labels exhibit less noise than those of their source nodes. Through Intra-Class Mixup, we obtain a dataset with high-quality labels, leading to improved performance of GNNs.

**Assumption 3.1**.: Different classes of data have varying noise levels, i.e., \(P_{}(y_{i}|x)=P(y_{i}|x)+_{i}\), where \(P_{}(y_{i}|x)\) and \(P(y_{i}|x)\) represent the label distribution of class \(i\) with and without noise, and \(_{i} N(0,_{i}^{2})\) represent the noise.

**Theorem 3.1**.: _For Intra-Class Mixup satisfying Equation 2 and Assumption 3.1, \(P(<)=((^{2}+(1-)^{2}) ^{-})\), and \()}{E()}=[^{2}+(1-)^{2}]^{}<1\), where \(E()\) represents the expectation, and \(\) and \(\) denote the noise in the generated data and the original data, respectively._

### Neighbor Selection

**Motivation:** The strength of GNN lies in gathering information from neighborhoods to mine node features . After generating node \(v\) in Sec 3.1, to leverage the advantages of GNN, it is necessary to find neighbors for \(v\). We aim to construct a neighborhood that satisfies two requirements: a). The neighborhood is suitable for \(v\); b). The neighbors of \(v\) can obtain richer information through \(v\). If \(v\) is simply connected to nodes that generated it, as the nodes used for Mixup mostly have low-quality labels, it is prone to resulting in incorrect information propagation. Since nodes of the same class are more likely to appear in the neighborhood in homogeneous graphs, a natural idea is to connect \(v\) with nodes of high confidence in the same class. In this way, we can find the correct neighbors for \(v\) and, acting as a bridge, connect the neighbors of two nodes of the same class through \(v\) to obtain more information from the graph, as shown in Figure 1(b).

**Approach:** As mentioned above, neighborhood selection involves two steps. First, finding nodes highly likely to be of the same class as \(v\), and second, determining how to connect \(v\) with these nodes. We will now introduce them separately in the following.

Finding nodes with a high probability of belonging to the same class can be transformed into the problem of finding high-quality labeled data. Then, we ingeniously design an ensemble method without extra training costs. We employ the GNN for pseudo-labeling to predict the label of nodes under \(n\) dropout probabilities. Nodes consistently predicted in all \(n\) trials are considered high-quality. This is an ensemble method with \(n\) GNNs but without \(n\) training costs, significantly reducing cost. Details are in Appendix D.3. It is expressed as:

\[D_{h}=\{(x,y)|f_{1}(x)=...=f_{n}(x),(x,y) D\},\] (4)

where \(f_{i}\) represents GNNs with different dropout probabilities. After obtaining the high-quality set \(D_{h}\), it is time to establish neighborhoods between \(D_{h}\) and \(D_{m}\) generated by Mixup. To ensure the correctness of neighbors, we adopt the way of randomly connecting the generated data to high-quality nodes of the same class. The augmented edge set \(\) of the original set \(E\) can be expressed as:

\[=E\{e(,x_{i})|(,y) D_{m},(x_{i},y) D_{h}\},\] (5)

where \(e(a,b)\) represents an edge between nodes \(a\) and \(b\). In this way, we not only find reasonable neighbors for the generated nodes but also establish an information exchange path between two nodesof the same class. Additionally, by not directly connecting the two nodes, potential noise impacts are avoided. The elimination effect of noise is guaranteed by Theorem 3.2. The detailed proof can be found in Appendix B.2. Through this method, the issue of missing neighborhoods in the graph is alleviated, and a graph with richer structural information is constructed for downstream tasks.

**Assumption 3.2**.: The label noise can be represented as node noise, i.e., \(P_{noise}(x|y_{i})=P(x|y_{i})+_{i}\), where \(_{i} N(0,_{xi}^{2})\). Equation 1 simplifies to \(h_{v}^{k}=MLP^{k}[(1+_{k})h_{v}^{k-1}+|}_{u N_{v}}h_{u }^{k-1}]\), where \(_{k}\) is learnable. \(m\) and \(n\) are nodes from the \(i\)-th class, \(x_{m} P(x|y_{i})\), \(x_{n} P_{noise}(x|y_{i})\).

**Theorem 3.2**.: _In a two-layer GNN, we have \()}{E()}=+(1-)^{2})+ +_{2})}}\), where \(\) represents the noise impact directly connecting \(m\) and \(n\) and \(\) is the impact through IntraMix._

The \()}{E()}\) in Theorem 3.2 can be controlled to be less than 1 by adjusting the learnable \(\), indicating that the neighbor selection method of IntraMix leads to a smaller noise impact. Therefore, Theorem 3.2 guarantees that the neighborhood selection strategy proposed by IntraMix can increase the richness of information in the graph while reducing the impact of noise.

### Workflow

In this section, we introduce the overall workflow of IntraMix. For details, please refer to Algorithm 1. The data augmentation begins by generating low-quality labels for unlabeled nodes through pseudo-labeling (lines 1). Following that, high-quality labeled data is generated by Intra-Class Mixup. Subsequently, we use the neighborhood selection strategy to select appropriate neighbors for the generated nodes (lines 6-8). The output is a graph that is better suited for downstream tasks.

```
0: Graph \(G=(V,E)\), \(V\) can be divided into \(D_{l}\) and \(D_{u}\), Class of nodes \(C\), GNN model \(f\)
1: Generate pseudo labels for \(D_{u}\) using \(f\), get \(_{u}\)
2:\(D=D_{l}_{u}\)
3: Generate Mixup set \(D_{m}=\{V_{m},E_{m}\}\) as Equation.2
4:\(V=V V_{m}\)
5: Generate high-quality set \(D_{h}\) according to Equation.4
6:for\((,) D_{m}\)do
7:\(E\{e(,x_{i}),e(,x_{j})\}\), where \((x_{i}/x_{j},) D_{h}\)
8:endfor
9: the augmented graph \(G=(V,E)\) ```

**Algorithm 1** Workflow of IntraMix

### Complexity Analysis

Overall, the time consumption of using IntraMix can be divided into four aspects: generating pseudo-labels for nodes, generating new nodes, neighborhood discovery, and the downstream task. Next, we will analyze the time complexity of each aspect separately.

In the pseudo-label generation process, there is no additional training cost since the model used is the same GNN employed for downstream tasks. The second part of the time cost comes from the pseudo-labeling process, which only involves \(kN\) inference steps, where \(N\) represents the number of nodes selected for pseudo-labeling and \(k\) represents the number of pseudo-labeling is performed in parallel. The time cost for this step is minimal.

Next is the process of generating new nodes. Assuming the number of generated nodes is \(m\), the time cost incurred during the Mixup generation is \(O(m)\). Simultaneously, the time cost for finding the neighborhood for these \(m\) nodes is also \(O(m)\). Next, we analyze the subsequent training time consumption of IntraMix. Assuming the original time complexity of the GNN is \(O(|V| F F^{})+O(|E| F^{})\), where \(F\) denotes the input feature dimension of nodes, and \(F^{}\) is the hidden layer dimension of GNN. The time complexity after using IntraMix is \(O(|V| F F^{})+O(|E| F^{})+O(m F F^{ })+O(2m F^{})+O(m)\). As in most cases, \(m|V|\), the time complexity is in the same order of magnitude as the original GNN. Therefore, from an overall perspective, IntraMix does not introduce significant additional time cost to the original framework.

## 4 Experiment

In this section, we show the excellent performance of IntraMix in both semi-supervised and full-supervised tasks with various GNNs across multiple datasets in Sec 4.1 and Sec 4.2. Sec 4.3 highlights the inductive learning ability of IntraMix while detailed ablation experiments for in-depth analysis are presented in Sec 4.4. Additionally, we analyze how IntraMix overcomes over-smoothing in Sec 4.5 and evaluate IntraMix on heterophilic and heterogeneous graphs in Sec 4.6 and Sec 4.7, respectively.

### Semi-supervised Learning

**Datasets:** We evaluate IntraMix on commonly used medium-scale semi-supervised datasets for node classification, including Cora, CiteSeer, Pubmed , CS, and Physics . We follow the original splits for these datasets. We also conduct semi-supervised experiments on large-scale graphs, including ogbn-arxiv  and Flickr . To alter the original splits for full-supervised training on these datasets, we use 1% and 5% of the original training data for semi-supervised experiments, respectively. Details can be found in Appendix C.1.

**Baselines:** We utilize four popular GNNs: GCN , GAT , GraphSAGE (SAGE) , and APPNP . Additionally, we compare IntraMix with various mainstream graph augmentation methods [42; 6; 7; 30; 24; 25]. Details of the baselines can be found in Appendix C.2. For each graph augmentation applied to each GNN, we use the same hyperparameters for fairness. When comparing with other methods, we use the settings from their open-source code and report the average results over 30 runs. All experiments are conducted on a single NVIDIA RTX-3090.

**Result:** It is crucial to note that semi-supervised experiments are more important than full-supervised ones. This is primarily due to the sparse labels in most real-world graphs. The semi-supervised results reflect the method's potential when applied to real-world situations. Observing the results in Table 1, it is evident that IntraMix shows superior performance across almost all GNNs and datasets.

   Models & Strategy & Cora & CiteSeer & Pubmed & CS & Physics \\   & Original & \(81.51 0.42\) & \(70.30 0.54\) & \(79.06 0.31\) & \(91.24 0.43\) & \(92.56 1.31\) \\  & GraphMix & \(82.29 3.71\) & \(74.55 0.52\) & \(82.82 0.53\) & \(91.90 0.22\) & \(90.43 1.76\) \\  & CODA & \(83.47 0.48\) & \(73.48 0.24\) & \(78.50 0.35\) & \(91.01 0.75\) & \(92.57 0.41\) \\  & DropMessage & \(83.33 0.41\) & \(71.83 0.35\) & \(79.20 0.25\) & \(91.50 0.31\) & \(92.74 0.72\) \\  & MH-Aug & \(84.21 0.38\) & \(73.82 0.82\) & \(80.51 0.32\) & \(92.52 0.37\) & \(92.91 0.46\) \\  & LA-GCN & \(84.61 0.57\) & \(74.70 0.51\) & \(81.73 0.71\) & \(92.60 0.26\) & \(93.26 0.43\) \\  & NodeMixup & \(83.47 0.32\) & \(74.12 0.35\) & \(81.16 0.21\) & \(92.69 0.44\) & \(93.97 0.45\) \\  & IntraMix & \(\) & \(\) & \(\) & \(\) & \(\) \\   & Original & \(82.04 0.62\) & \(71.82 0.83\) & \(78.00 0.71\) & \(90.52 0.44\) & \(91.97 0.65\) \\  & GraphMix & \(82.76 0.62\) & \(73.04 0.51\) & \(78.82 0.44\) & \(90.57 1.03\) & \(92.90 0.42\) \\  & CODA & \(83.36 0.31\) & \(72.93 0.42\) & \(79.37 1.33\) & \(90.41 0.41\) & \(92.09 0.62\) \\  & DropMessage & \(82.20 0.24\) & \(71.48 0.37\) & \(78.14 0.25\) & \(91.02 0.51\) & \(92.03 0.72\) \\  & MH-Aug & \(84.52 0.91\) & \(73.44 0.81\) & \(79.82 0.55\) & \(91.26 0.35\) & \(92.72 0.42\) \\  & LA-GAT & \(84.72 0.45\) & \(73.71 0.52\) & \(81.04 0.43\) & \(91.52 0.31\) & \(93.42 0.45\) \\  & NodeMixup & \(83.52 0.31\) & \(74.30 0.12\) & \(81.26 0.34\) & \(\) & \(93.87 0.30\) \\  & IntraMix & \(\) & \(\) & \(\) & \(92.40 0.24\) & \(\) \\   & Original & \(78.12 0.32\) & \(68.09 0.81\) & \(77.30 0.74\) & \(91.01 0.93\) & \(93.09 0.41\) \\  & GraphMix & \(80.09 0.82\) & \(70.97 1.21\) & \(79.85 0.42\) & \(91.55 0.33\) & \(93.25 0.33\) \\  & CODA & \(83.55 0.14\) & \(73.24 0.24\) & \(79.28 0.46\) & \(91.64 0.41\) & \(93.42 0.36\) \\  & MH-Aug & \(84.50 0.39\) & \(\) & \(80.68 0.36\) & \(92.27 0.49\) & \(93.58 0.53\) \\  & LA-SAGE & \(84.41 0.35\) & \(74.16 0.32\) & \(80.72 0.42\) & \(92.41 0.54\) & \(93.41 0.31\) \\  & NodeMixup & \(81.93 0.22\) & \(74.12 0.44\) & \(79.97 0.53\) & \(91.97 0.24\) & \(94.76 0.25\) \\  & IntraMix & \(\) & \(74.37 0.45\) & \(\) & \(\) & \(\) \\   & Original & \(80.03 0.53\) & \(70.30 0.61\) & \(78.76 0.24\) & \(91.79 0.55\) & \(92.36 0.81\) \\  & GraphMix & \(82.98 0.42\) & \(70.26 0.43\) & \(78.73 0.45\) & \(91.53 0.61\) & \(94.12 0.14\) \\   & DropMessage & \(82.37 0.23\) & \(72.65 0.53\) & \(80.04 0.42\) & \(91.25 0.51\) & \(93.54 0.63\) \\   & MH-Aug & \(85.04 0.41\) & \(74.52 0.32\) & \(80.71 0.31\) & \(92.95 0.34\) & \(94.03 0.25\) \\   & LA-APPNP & \(85.42 0.33\) & \(74.83 0.29\) & \(81.41 0.55\) & \(92.71 0.47\) & \(94.52 0.27\) \\   & NodeMixup & \(8Additionally, Table 2 shows that the semi-supervised results with 1% and 5% of the training data on large datasets also demonstrates excellent performance. This indicates that the IntraMix generation of high-quality labeled nodes and neighborhoods, enriches the knowledge on the graph, making the graph more conducive for GNNs. Moreover, it is noteworthy that IntraMix exhibits greater advantages on SAGE and APPNP. This is attributed to the neighbor sampling for message aggregation of SAGE and the customized message-passing of APPNP, both of which prioritize the correct and richness of neighborhoods. The superiority on these two models further validates the rationality and richness of the neighborhoods constructed by IntraMix and the correctness of the generated nodes.

### Full-supervised Learning

**Datasets:** To evaluate IntraMix on full-supervised datasets, we utilize the well-known ogbn-arxiv and Flickr, following standard partitioning ways. Detailed information can be found in Appendix C.1.

**Baselines:** In this part, we consider three GNNs: GCN, GAT, and GraphSAGE. We compare IntraMix with various mainstream methods, and details about these methods can be found in C.2.

**Results:** The results in Table 2 show that IntraMix consistently outperforms all GNNs and datasets in full-supervised experiments, aligning with the outcomes in semi-supervised learning. Although graphs typically adhere to semi-supervised settings, some graphs, like citation networks, have sufficient labels . Thus, we conduct supervised experiments to show that the generality of IntraMix. The success in full-supervised settings primarily demonstrates the effectiveness of our neighbor selection strategy, as the ample labeled data in the training set reduces the influence of the high-quality labeled data generated by IntraMix. This further proves that our neighbor selection strategy constructs a graph more conducive to downstream tasks by enriching the high-quality neighborhoods of nodes.

### Inductive Learning

The experiments mentioned above are conducted in transductive settings. In node-level tasks, the common setting is transductive, where the test distribution is known during training, fitting many static graphs. Inductive learning refers to not knowing the test distribution during training. Since many real-world graphs are dynamic, inductive learning is also crucial. To demonstrate the reliability of IntraMix in inductive setups, we conduct experiments on Cora and CiteSeer, utilizing GraphSAGE and GAT. The results are presented in Table 3. In inductive learning, GNNs can only observe non-test data during training.

   Model & Strategy &  &  \\   & 1\% & 5\% & 100\% & 1\% & 5\% & 100\% \\   & Original & \(62.99 1.01\) & \(68.65 0.31\) & \(71.74 0.29\) & \(46.02 1.25\) & \(48.50 0.49\) & \(51.88 0.41\) \\  & FLAG & \(63.68 0.91\) & \(69.14 0.43\) & \(72.04 0.20\) & \(46.52 0.77\) & \(48.74 0.46\) & \(52.05 0.16\) \\  & LAGCN & \(64.09 0.65\) & \(69.62 0.25\) & \(72.08 0.14\) & \(47.12 0.63\) & \(49.45 0.43\) & \(52.63 0.16\) \\  & NodeMixup & \(63.91 0.87\) & \(69.85 0.23\) & \(73.26 0.25\) & \(46.65 1.66\) & \(48.92 0.56\) & \(52.54 0.21\) \\  & IntraMix & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & Original & \(63.21 0.94\) & \(69.75 0.43\) & \(73.65 0.11\) & \(45.88 1.23\) & \(48.24 0.53\) & \(49.88 0.32\) \\  & FLAG & \(63.80 0.84\) & \(69.93 0.51\) & \(73.71 0.13\) & \(46.24 0.75\) & \(48.51 0.43\) & \(51.34 0.27\) \\  & LAGAT & \(64.21 0.54\) & \(69.96 0.22\) & \(73.77 0.12\) & \(47.55 0.65\) & \(49.65 0.28\) & \(52.63 0.16\) \\  & NodeMixup & \(64.26 0.47\) & \(70.05 0.21\) & \(73.24 0.32\) & \(47.02 1.29\) & \(49.05 0.61\) & \(52.82 0.36\) \\  & IntraMix & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & Original & \(62.87 0.81\) & \(68.82 0.40\) & \(71.49 0.27\) & \(45.72 1.12\) & \(48.65 0.43\) & \(50.47 0.21\) \\  & FLAG & \(63.35 0.77\) & \(69.04 0.38\) & \(72.19 0.21\) & \(46.54 0.78\) & \(48.23 0.43\) & \(52.39 0.28\) \\   & LASAGE & \(64.26 0.57\) & \(69.93 0.24\) & \(72.30 0.12\) & \(47.33 0.63\) & \(50.82 0.38\) & \(54.24 0.25\) \\   & NodeMixup & \(64.01 0.44\) & \(69.79 0.23\) & \(72.01 0.35\) & \(47.13 0.58\) & \(50.91 0.24\) & \(53.49 0.24\) \\   & IntraMix & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Semi- and full-supervised node classification accuracy on large-scale graphs. The average result of 10 runs is reported. Training size refers to the proportion of training data used for training.

   Models & Strategy & Cora & CiteSeer \\   & Original & \(81.3 0.5\) & \(70.4 1.2\) \\  & LAGAT & \(82.7 0.8\) & \(72.1 0.7\) \\  & NodeMixup & \(83.1 0.5\) & \(71.8 0.9\) \\  & IntraMix & \(\) & \(\) \\   & Original & \(80.1 1.7\) & \(69.1 2.9\) \\  & LAGSAGE & \(81.7 0.8\) & \(73.0 1.1\) \\   & NodeMixup & \(81.9 0.5\) & \(73.1 1.3\) \\   & IntraMix & \(\) & \(\) \\   

Table 3: Node Classification in inductive settings.

From the results, it is evident that IntraMix also exhibits excellent performance in inductive learning settings. This strongly validates that the generated nodes with more high-quality labels and rich neighborhoods constructed by IntraMix indeed provide the graph with more information beneficial to downstream tasks. As a result, GNNs trained with IntraMix can learn more comprehensive patterns and make accurate predictions even for unseen nodes, confirming IntraMix as a generalizable graph augmentation framework applicable to real-world scenarios.

### Ablation Experiment

To demonstrate the effects of each IntraMix component, we conduct detailed ablation experiments using GCN on Cora, CiteSeer, and Pubmed. All other parts of IntraMix are kept unchanged except for the mentioned ablated components.

**Intra-Class Mixup:** Firstly, we use a simple experiment to show that part of the improvement is closely related to the high-quality data generated by Intra-Class Mixup. In Table 6, we replace the generated data with all-zero and all-one vectors and find that both perform worse than IntraMix. This indicates that the nodes generated by IntraMix are indeed helpful for downstream tasks.

Then, we discuss the effectiveness of Intra-Class Mixup. We compare it with methods that do not use Mixup, relying solely on pseudo-labeling(PL), and introduce an advanced PL method called UPS . Additionally, we compare Intra-Class Mixup with vallina Mixup, which employs various connection methods. The results are shown in Table 4. Among these methods, Intra-Class Mixup has the best performance, showing nearly 3.5% improvement in accuracy compared to the original GCN. This is because, compared to methods using only pseudo-labels, Intra-Class Mixup generates higher-quality labeled nodes, enabling GNNs to get more information useful for downstream tasks.

Regarding Mixup, we utilize three connecting methods: treating generated nodes as isolated (w/o con), connecting them with nodes used for generation (w con), and connecting them with nodes with similar embeddings (sim con). However, none of these methods perform well. As Theorem 3.1 suggests, Intra-Class Mixup ensures the improvement of label quality for each class, a guarantee that Mixup cannot provide. Furthermore, the fact that Intra-Class Mixup data have a single label makes it convenient to select similar neighbors. In contrast, Mixup generates data with mixed labels, introducing the risk of connecting to any class of node and potentially causing errors in propagation. This is a key reason for the poor performance of Mixup in node classification.

**Neighbor Selection:** This part shows the importance of Neighbor Selection. We compare various selection methods in Table 5. We observe that these methods are less effective than IntraMix. _Direct con_ indicates connecting the generated data to low-quality labeled nodes of the same class, and its poor performance proves the necessity of our proposed approach to finding high-quality nodes of the same class as neighboring nodes. The experimental results validate Theorem 3.2.

Compared to other neighbor selection methods, IntraMix proposes a simple way to select nodes more likely to serve as neighbors, leading to more accurate message passing. Among the methods,

   Strategy & Cora & CiteSeer & Pubmed \\  Original & 81.5 \(\) 0.4 & 70.3 \(\) 0.5 & 79.0 \(\) 0.3 \\ Ones & 31.9 (\(\) 49) & 21.5 (\(\) 48) & 38.1 (\(\) 40) \\ Zeros & 83.8 (\(\) 2.3) & 73.6 (\(\) 3.3) & 80.7 (\(\) 1.7) \\  IntraMix & **85.2 (\(\)3.7)** & **74.8 (\(\)4.5)** & **82.9 (\(\)3.9)** \\   

Table 6: Explore the effect of generating node with Intra-Class Mixup. _Zeros_ means replacing the generated nodes with an all-zero vector, and _Ones_ means replacing them with an all-one vector.

   Strategy & Cora & CiteSeer & Pubmed \\  Original & 81.5 \(\) 0.4 & 70.3 \(\) 0.5 & 79.0 \(\) 0.3 \\ Only PL & 82.9 \(\) 0.2 & 72.3 \(\) 0.3 & 79.5 \(\) 0.2 \\ Only UPS & 83.1 \(\) 0.4 & 72.8 \(\) 0.6 & 79.7 \(\) 0.4 \\  Mixup/w/o con & 58.9 \(\) 22.3 & 52.3 \(\) 17.6 & 70.0 \(\) 10.8 \\ Mixup/w con & 83.0 \(\) 1.2 & 71.3 \(\) 3.5 & 79.4 \(\) 1.1 \\ Mixup/sim com & 83.1 \(\) 1.8 & 71.5 \(\) 1.9 & 79.8 \(\) 3.8 \\  Intra-Class Mixup & **85.2 (\(\)3.7)** & **74.8 (\(\)4.5)** & **82.9 (\(\)3.9)** \\   

Table 4: Ablation of Intra-Class Mixup on GCN. _w con_ is vallina mixup connection, and _sim con_ is similar connection. \(\) is the improvement.

   Strategy & Cora & CiteSeer & Pubmed \\  Original-GCN & 81.5 \(\) 0.4 & 70.3 \(\) 0.5 & 79.0 \(\) 0.3 \\ Direct Con & 83.6 (\(\) 2.1) 73.4 (\(\) 3.1) & 78.0 (\(\) 1.0) \\ Random Con & 76.7 (\(\) 4.8) 67.0 (\(\) 3.3) & 65.1 (\(\) 1.39) \\ Without Con & 82.9 (\(\) 1.4) 72.8 (\(\) 2.5) & 79.4 (\(\) 0.4) \\ Vallina Con & 84.3 (\(\) 2.8) 73.6 (\(\) 3.3) & 79.8 (\(\) 0.8) \\ Similar Con & 84.5 (\(\) 3.0) 74.0 (\(\) 3.7) & 80.3 (\(\) 1.3) \\  IntraMix & **85.2 (\(\)3.7)** & **74.8 (\(\)4.5)** & **82.9 (\(\)3.9)** \\   

Table 5: Effects of Neighbor Selection on GCN. \(\) means improvement compared to the original, while \(\) indicates a reduction.

_Vallina Con_ indicates connecting generated nodes to the nodes used for generation. _Similar Con_ (SC) denotes connecting the nodes to nodes with similar embeddings. SC performs great, highlighting the importance of selecting similar nodes as neighbors, aligning with our intuition. However, SC is not as good as IntraMix, mainly because the initial neighbors for generated nodes are empty, making it hard to provide accurate embeddings for similarity measurement. What's more, connecting overly similar nodes resulted in insufficient information. In comparison, IntraMix connects nodes with the same label, maintaining neighborhood correctness while connecting nodes that are not extremely similar. In Table 6, using an all-zero vector to eliminate the influence of Mixup still shows an improvement. This reflects the rationality of our neighbor selection method, which is effective for graphs.

**Utilization of unlabeled data:** In this part, we show the importance of using unlabeled nodes to obtain low-quality data, and the results are shown in Figure 3(a). Even though Mixup can augment the label information to some extent, the insufficient nodes used for generation create a bottleneck in information gain, hindering GNNs from learning enough knowledge. Despite the labels provided by pseudo-labeling for unlabeled data being low-quality, Intra-Class Mixup enhances the label quality, thus providing GNNs with ample knowledge.

**Sensitivity Analysis of \(\):** This part discusses the impact of \(\) in Intra-Class Mixup. The experiment is conducted using GCN on Cora, and details are presented in Figure 3 (b). According to Theorem 3.1, the best noise reduction in each class label is achieved when \(=0.5\). The results validate our theoretical analysis, showing that the performance of GCN gradually improves as \(\) varies from \(0\) to \(0.5\). Therefore, we choose \( B(2,2)\), where \(B\) denotes Beta Distribution.

**Analysis of pseudo-label quality:** In this part we discuss the performance of IntraMix when the quality of pseudo-labels is extremely low. This situation may occur when the initial labeled nodes are extremely sparse. We use 5% of the semi-supervised training dataset for training. As shown in Figure 3 (c), we find that when the pseudo-label quality is low, IntraMix can effectively improve performance. Additionally, we use the model trained in the previous step for the next pseudo-labeling. This iterative method provides a way to enhance the performance of IntraMix on low-quality data. In summary, IntraMix effectively enriches the knowledge with extremely low-quality pseudo-labels.

### Over-smoothing Analysis

As is well known, deep GNNs may result in over-smoothing, a phenomenon characterized by the convergence of node embeddings. We show the ability of IntraMix to alleviate over-smoothing in Figure 4(a). We use MADgap  as the metric, where a larger MADgap indicates a milder over-smoothing. Surprisingly, although IntraMix is not specifically designed to address over-smoothing, it shows a strong ability to counteract over-smoothing, reaching a level similar to GRAND , a method specialized in handling over-smoothing. This is attributed to the bridging effect of the generated nodes, connecting nodes of the same class with high confidence in a random manner. This process resembles random information propagation, providing effective resistance against over-smoothing. Additionally, the richer neighbors and node features generated by IntraMix inherently mitigate over-smoothing . The detailed discussion can be found in Appendix D.1.

Figure 3: a) Experimental results using different proportions of unlabeled nodes show that performance improves as more unlabeled nodes are utilized. b) Sensitivity analysis of \(\) indicates that the best performance is achieved when \(=0.5\). c) Analysis with low-quality pseudo-labels. The model from the previous step is used for pseudo-labeling in the next step.

### Evaluation on Heterophilic Graphs

In this section, we analyze the performance of IntraMix on heterophilic graphs on the Cornell, Texas and Wisconsin datasets . Although the neighbor selection utilizes the neighborhood assumption, we find from the results in Figure 4(b) that IntraMix can also enhance GNN on heterophilic graphs. This is because, despite the existing connections in heterophilic graphs tending to link different types of nodes, they do not exclude connections between similar nodes. The connections between high-quality nodes generated by IntraMix can increase the information on the graph, thereby improving the performance of GNN. More detailed discussion is in Appendix D.2.

### Evaluation on Heterogeneous Graphs

In this section, we discuss the performance of IntraMix on heterogeneous graphs. In this setting, neighboring nodes may belong to different types of entities. For example, different papers can be linked through authors, but there is no direct link between them. This is a common graph configuration. We use SeHGNN as the base model and incorporate IntraMix to conduct experiments on the IMDB  and DBLP  datasets. The results are shown in Figure 4(c). IntraMix also improves performance in heterogeneous graphs, highlighting its versatility.

## 5 Related Work

**Graph Augmentation:** The primary purpose of graph augmentation is to address two common challenges in graphs encountered by GNN, scarcity of labels and incomplete neighborhoods . Graph augmentation can be categorized into Node Manipulation , Edge Manipulation , Feature Manipulation , and Subgraph Manipulation . However, existing methods either require complex generators  or extensive empirical involvement , failing to effectively address the two issues. The proposed IntraMix offers a simple solution to simultaneously tackle the two challenges faced by GNNs. Details about the related augmentation methods can be found in Appendix E.1.

**Mixup:** Mixup is a promising data augmentation medthod , enhancing the generalization of various tasks [41; 37]. However, there has been limited focus on the application in node classification on graphs. We address the shortcomings of vallina Mixup in node classification, proposing IntraMix. IntraMix provides richer information for graphs, improving GNNs in node classification. Details about the related Mixup works can be found in Appendix E.2.

## 6 Conclusion

This paper presents IntraMix, an elegant graph augmentation method for node classification. We utilize Intra-Class Mixup to generate high-quality labels to address the issue of sparse high-quality labels. To address the problem of limited neighborhoods, we connect the generated nodes with nodes that are highly likely from the same class. IntraMix provides an elegant solution to the dual challenges faced by graphs. Moreover, IntraMix is a flexible method that can be applied to all GNNs. Future work will focus on exploring neighbor selection methods to construct more realistic graphs.

Figure 4: a) Analysis reveals that IntraMix shows effective capabilities in overcoming over-smoothing with deep GNNs. b) Evaluation on heterophilic graphs. c) Evaluation on heterogeneous graphs.