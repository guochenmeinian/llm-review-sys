# Generalization bounds for mixing processes via delayed online-to-PAC conversions

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

We study the generalization error of statistical learning algorithms in a non-_i.i.d._ setting, where the training data is sampled from a stationary mixing process. We develop an analytic framework for this scenario based on a reduction to online learning with delayed feedback. In particular, we show that the existence of an online learning algorithm with bounded regret (against a fixed statistical learning algorithm in a specially constructed game of online learning with delayed feedback) implies low generalization error of said statistical learning method even if the data sequence is sampled from a mixing time series. The rates demonstrate a trade-off between the amount of delay in the online learning game and the degree of dependence between consecutive data points, with near-optimal rates recovered in a number of well-studied settings when the delay is tuned appropriately as a function of the mixing time of the process.

## 1 Introduction

In machine learning, generalization denotes the ability of a model to infer patterns from a dataset of training examples and apply them to analyze previously unseen data (Shalev-Shwartz and Ben-David, 2014). The gap in accuracy between the model's predictions on new data and those on the training set is usually referred to as _generalization error_. Providing upper bounds on this quantity is a central goal in statistical learning theory. Classically, bounds based on notions of complexity (_e.g._, VC dimension and Rademacher complexity) for the model's hypothesis space were used to provide uniform worst-case guarantees (see Bousquet et al., 2004; Vapnik, 2013; Shalev-Shwartz and Ben-David, 2014). However, results of this kind are often too loose to be applied to the most common machine learning over-parameterised models, such as deep neural networks (Zhang et al., 2021). As a consequence, several approaches have been proposed to obtain algorithm-dependent generalization bounds, which can adapt to the problem and be much tighter in practice than their uniform counterparts. Often, the underlying idea is that if the algorithm's output does not have a too strong dependence on the specific input dataset used for the training, then the model should not be prone to overfitting, and so generalize well. Examples of results that build onto these ideas are stability bounds, information-theoretic bounds, and PAC-Bayesian bounds (see, _e.g._, Bousquet and Elisseeff, 2002; Russo and Zou, 2020; Hellstrom et al., 2023; Alquier, 2024).

Most results in the literature focus on the _i.i.d._ setting, where the training dataset is made of independent draws from some underlying data distribution. However, for several applications, this assumption is far from realistic. For instance, it excludes the case where observations received by the learner have some inherent temporal dependence, as it is the case for stock prices, daily energy consumption, or sensor data from physical environments (Ariyo et al., 2014; Takeda et al., 2016). This calls for the development of theory for addressing non-_i.i.d._ data. A common approach in the extant literature is to consider a class of non-_i.i.d._ data-generating processes usually referred to as stationary \(\)-mixingor \(\)-mixing processes. This assumption, together with a "blocking" trick introduced by Yu (1994), has led to a few results in the literature: Meir (2000), Mohri and Rostamizadeh (2008), Shalizi and Kontorovich (2013), and Wolfer and Kontorovich (2019) provided uniform worst-case generalization bounds, Steinwart and Christmann (2009) and Agarwal and Duchi (2012) discussed excess risk bound (comparing the algorithm's output with the best possible hypothesis), while Mohri and Rostamizadeh (2010) gave bounds based on a stability analysis (in the sense of Bousquet and Elisseeff, 2002).

Here, we propose propose results for the non-_i.i.d._ setting in the form of PAC-Bayesian bounds (Guedj, 2019; Alquier, 2024): high probability upper bounds on the expected generalization error of randomized learning algorithms. We achieve this by combining the "blocking" argument by Yu (1994) to manage the concentration of sums of correlated random variables, with the recent online-to-PAC conversion technique recently proposed by Lugosi and Neu (2023). Using their framework we show a new way to obtain generalization bounds for stationary dependent processes that satisfy a certain "short-memory" property (intuitively meaning that data points that are closer in time are more heavily dependent on each other). Our assumption slightly differs from \(\)-mixing in the sense that we only need it to hold for a specific class of bounded loss functions. Among other results, this allows us to prove PAC-Bayesian generalization bounds for mixing processes. This complements previous work on such bounds that have only considered mild relaxations of the _i.i.d._ condition such as assuming that the data has a martingale structure (see, _e.g._, Seldin et al., 2012; Chugg et al., 2023; Haddouche and Guedj, 2023). Notable exceptions are the works of Alquier and Wintenberger (2012), Alquier et al. (2013), and Eringis et al. (2022, 2024), who provided generalization bounds for a sequential prediction setting where both the data-generating process and the hypothesis class used for prediction are stable dynamical systems. Their results are proved under some very specific conditions on these systems, and their guarantees involve unspecified problem-dependent constants that may be large. In contrast, our bounds hold under general, simple-to-verify conditions and feature explicit constants.

The rest of the paper is organized as follows. In Section 2 we properly define the generalization error of a statistical learning algorithm for both _i.i.d._ and non-_i.i.d._ cases, and state our main assumption on the data dependence. Our main contribution lies in Section 3, where after recalling the results for the _i.i.d._ setting we show how to adapt this to stationary mixing processes. In Section 4 we provide concrete results of the bounds we can obtain through the online-to-PAC conversion. Finally in Section 5 we extend our results to the setting where the hypothesis class itself may consist of dynamical systems.

**Notation.** For a distribution over hypotheses \(P_{W}\) and bounded function \(f:\) we write \( P,f\) to refer to the expectation of \(_{W P}[f(W)]\). We denote \(_{KL}(P||Q)=_{X P}[( )]\) to refer to the Kullback-Leibler divergence. We use \(||.||\) to denote a norm on the Banach space \(\) of the finite signed measures, and \(||.||_{*}\) the corresponding dual norm on the dual space \(^{*}\) of measurable functions \(f\) on \(\) such that \(||f||_{*}=_{Q:||Q|| 1} Q,f\).

## 2 Preliminaries

The classical statistical learning framework usually considers a dataset \(S_{n}=(Z_{1},...,Z_{n})\), made of \(n\)_i.i.d._ elements drawn from a distribution \(\) over a measurable instance space \(\). Often, one can think of each \(Z_{i}\) as a feature-label pair \((X_{i},Y_{i})\). Furthermore, we are given a measurable class \(\) of hypotheses and a loss function \(:_{+}\), with \((w,z)\) measuring the quality of the hypothesis \(w\) on the data instance \(z\). For any given hypothesis \(w\), two key objects of interest are the _training error_\(}(w,S_{n})=_{i=1}^{n}(w,Z_{i})\) and the _test error_\((w)=_{Z^{}}[(w,Z^{})]\), where the random element \(Z^{}\) has the same distribution as \(Z_{i}\) and is independent of \(S_{n}\).

A learning algorithm \(:^{n}\) maps the training sample to an hypothesis in \(\). More generally, we will focus on randomized learning algorithms, returning a probability distribution \(P_{W_{n}|S_{n}}_{}\) over \(\), conditionally on \(S_{n}\) (deterministic algorithms can be recovered as special cases, whose the outputs are Dirac distributions). The ultimate goal of the learner is to minimize the test error. Yet, this quantity cannot be computed without knowledge of the data generating distribution \(\). In practice, one typically relies on the training error in order to gauge the quality of the algorithm. For an algorithm \(:S_{n} P_{W_{n}|S_{n}}\), we define the _generalization error_ as the expected gap between training and test error:

\[(,S_{n})=[(W_{n})-}(W_{n},S_{n})S_{n}].\]The expectation in the above expression integrates over the randomness in the output of the algorithm \(W_{n} P_{W_{n}|S_{n}}\), conditionally on the sample \(S_{n}\). We remark that the test error is _not_ equal to the mean of the training error, due to the dependence of \(W_{n}\) on the training data.

We extend the previous setting by considering the case where the data have an intrinsic temporally ordered structure, and come in the form of a stationary process \((Z_{t})_{t^{*}}\). Formally, we assume that the joint marginal distribution of any block \((Z_{t},Z_{t-1},,Z_{t-i})\) is the same as the distribution of \((Z_{t+j},Z_{t+j-1},,Z_{t+j-i})\) for any \(t\), \(i\) and \(j\), but the data points are not necessarily independent of each other. In particular, the marginal distribution of \(Z_{t}\) is constant and is denoted by \(\). Thus, it is natural to continue to use the definition of the test loss and generalization error given above, although with the understanding that \(\) now refers to the marginal distribution of an independent copy of \(Z_{1}\), a sample point from a stationary non-_i.i.d._ process. We remark here that other notions of the test loss may also be considered, and the framework that we propose can be extended to most natural definitions with little work (but potentially large notational overhead). In Section 5, we provide such an extension for a more general setting where the hypotheses themselves are allowed to have memory and the process may not be as strongly stationary as our assumption above requires.

In order to obtain generalization results we need to have some control on how strong the dependencies between different datapoints are allowed to be. To this regard, we consider the following assumption.

**Assumption 1**.: _There exists a non-increasing sequence \((_{d})_{d^{*}}\) of non-negative real numbers such that, for all \(w\) and all \(t^{*}\):_

\[[(w)-(w,Z_{t})_{t-d}] _{d}\,,\]

_where \((w)=_{Z^{}}[(w,Z^{})]\), with \(Z^{}\) being independent on the process \((Z_{t})_{t^{*}}\) and having as distribution the stationary marginal \(\) of the \(Z_{t}\)._

The intuition behind this assumption is that the loss associated with the observations \(Z_{t}\) becomes almost independent of the past after \(d\) steps, enabling us to treat each sequence of the form \((Z_{t},Z_{t+d},,Z_{t+(n-t)d})\) as an approximately _i.i.d._ sequence. Note that this assumption differs from the usual \(\)-mixing assumption which requires the distribution of \(Z_{t}|_{t-d}\) to be close to the marginal distribution \(\) for all \(t\), in terms of total variation distance. Our assumption is somewhat weaker in the sense that it only requires the expected losses under these distributions to be close, and only a one-sided inequality is required. It is easy to verify that our assumption is satisfied if the process is \(\)-mixing in the usual sense and the losses are bounded in \(\).

## 3 Proving generalization bounds via online learning

Online learning focuses on algorithms that aim to improve performance incrementally as new information becomes available, often without any underlying assumption on how data are generated. The online learner's performance is typically measured leveraging the idea of regret. This involves introducing a cost function for the problem and defining the regret as the difference between the cumulative cost of the online learner and that of a fixed comparator. We refer to the monographs Cesa-Bianchi and Lugosi, 2006 and Orabona, 2019 for comprehensive overviews on online learning and regret analysis. Recently, Lugosi and Neu (2023) established a connection between upper bounds on the regret and generalization bounds, showing that the existence of a strategy with a bounded regret in a specially designed online game translates into a generalization bound, via a technique dubbed _online-to-PAC conversion_. Their focus is on the _i.i.d._ setting, where the training dataset is made of independent draws. Here, we show that this framework can naturally be extended beyond the _i.i.d._ assumption.

In what follows, we briefly review the setup of Lugosi and Neu (2023) in Section 3.1 and then describe our new extension of their model to the non-_i.i.d._ case in Section 3.2. In particular, we prove a high-probability bound for the generalization error of any statistical learning algorithm learnt with a stationary mixing process verifying Assumption 1.

### Online-to-PAC conversions for _i.i.d._ data

Lugosi and Neu (2023) have recently established a framework to obtain generalization bounds via a reduction to online learning. Their technique allows to recover several classic PAC-Bayesianresults, and provide a range of generalizations thereof. The main idea of Lugosi and Neu (2023) is to introduce an online learning game called the _generalization game_, where the following steps are repeated for a sequence of rounds \(t=1,2,,n\):

* the online learner picks a distribution \(P_{t}_{}\);
* the adversary selects the cost function \(c_{t}:w(w,Z_{t})-(w)\);
* the online learner incurs the cost \( P_{t},c_{t}=_{W P_{t}}[c_{t}(W)]\);
* \(Z_{t}\) is revealed to the learner.

The learner can adopt any strategy to pick \(P_{t}\), but they can only rely on past knowledge to make their prediction. Explicitly, if \(_{t}\) denotes the sigma-algebra generated by \(Z_{1},...,Z_{t}\), then \(P_{t}\) has to be \(_{t-1}\)-measurable. We also emphasize that in this setup the online learner is allowed to know the loss function \(\) and the distribution \(\) of the data points \(Z_{t}\), and therefore by revealing the value of \(Z_{t}\), the online learner may compute the entire cost function \(c_{t}\).

We define the _regret_ of the online learner against the possibly data-dependent _comparator_\(P^{*}_{}\) as \((P^{*})=_{t=1}^{n} P_{t}-P^{*},c_{t}\). Now, denote as \(P_{W_{n}|S_{n}}\) the distribution produced by the supervised learning algorithm. With this notation, the generalization error can be written as \((,S_{n})=-_{t=1}^{n} P_{W_ {n}|S_{n}},c_{t}\). By adding and subtracting the quantity \(M_{n}=-_{t=1}^{n} P_{t},c_{t}\) we get the following decomposition.

**Theorem 1** (Theorem 1 in Lugosi and Neu, 2023; see appendix A.1).: _With the notation introduced above,_

\[(,S_{n})=_{n}(P_{W_{n }|S_{n}})}{n}+M_{n}\,.\] (1)

The first of these terms correspond to the _regret_ of the online learner against a fixed _comparator strategy_ that picks \(P_{W_{n}|S_{n}}\) at each step. The second term is a martingale and can be bounded in high probability with standard concentration tools. Indeed, since \(P_{t}\) is chosen before \(Z_{t}\) is revealed, one can easily check that \([ P_{t},c_{t}|_{t-1}]=0\). Thus, to prove a bound on the generalization error of the statistical learning algorithm, it is enough to find an online learning algorithm with bounded regret against \(P_{W_{n}|S_{n}}\) in the generalization game.

As a concrete application of the above, the following generalization bound is obtained when picking the classic exponential weighted average (EWA) algorithm (Vovk, 1990; Littlestone and Warmuth, 1994; Freund and Schapire, 1997) as online strategy, and plugging its regret bound into (1).

**Theorem 2** (Corollary \(6\) in Lugosi and Neu, 2023).: _Suppose that \((w,z)\) for all \(w,z\). Then, for any \(P_{1}_{}\) and \(>0\), with probability at least \(1-\) on the draw of \(S_{n}\), uniformly on every learning algorithm \(:S_{n} P_{W_{n}|S_{n}}\), we have_

\[(,S_{n})_{KL}(P_{W_{n}|S_{n }}||P_{1})}{ n}++ )}{n}}\,.\]

Proof.: We can bound each term of (1) separately. A data-dependent bound for the regret term is obtained via a direct application of the regret analysis of EWA which brings the term \(_{KL}(P_{W_{n}|S_{n}}||P_{1})}{ n}+\) (see Appendix B.1). The term \()}{n}}\) results from bounding the martingale \(M_{n}\) via an application of Hoeffding-Azuma inequality. 

Note that the first term in the above bound is data-dependent due to the presence of \(P_{W_{n}|S_{n}}\), and thus optimizing it requires a data-dependent choice of \(\), which is not allowed by Theorem 2. However, via a union bound argument it is possible to get a bound in the form

\[(,S_{n})=( _{KL}(P_{W_{n}|S_{n}}||P_{1})}{n}}+()}),\]

For the details, we refer to the proof of Corollary 5 of Lugosi and Neu (2023), which recovers a classical PAC-Bayes bound of McAllester (1998).

### Online-to-PAC conversions for non-_i.i.d._ data

In what follows, we will drop the _i.i.d._ assumption for the data, and instead consider non-_i.i.d._ sequences satisfying Assumption 1. For this setting we define the following variant of the generalization game.

**Definition 1** (Generalization game with delay).: _The generalization game with delay \(d^{*}\) is an online learning game where the following steps are repeated for a sequence of rounds \(t=1,...,n\):_

* _the online learner picks a distribution_ \(P_{t}_{}\)_;_
* _the adversary selects the cost function_ \(c_{t}:w(w,Z_{t})-(w)\)_;_
* _the online learner incurs the cost_ \( P_{t},c_{t}=_{W P_{t}}[c_{t}(W)]\)_;_
* _if_ \(t d\)_,_ \(Z_{t-d+1}\) _(and thus_ \(c_{t-d+1}\)_) is revealed to the learner._

The main difference between our version of the generalization game and the standard one of Lugosi and Neu (2023) is the introduction of a _delay_ on the online learning algorithm's decisions. Specifically, we will force the online learner to only take information into account up to time \(t-d\) when picking their action \(P_{t}\). Clearly, setting \(d=1\) recovers the original version of the generalization game with no delay.

It is easy to see that the regret decomposition of Theorem 1 still remains valid in the current setting. The purpose of introducing the delay is to be able to make sure that the term \(M_{n}=-_{t=1}^{n} P_{t},c_{t}\) is small. The lemma below states that the increments of \(M_{n}\) behave similarly to a martingale-difference sequence, thanks to the introduction of the delay.

**Lemma 1**.: _Fix \(d 1,n\). Under assumption 1, it holds for all \(t 1,n\):_

\[[-P_{t},c_{t}|_{t-d}]_{d}\,.\]

_where \(P_{t}\) and \(c_{t}\) are defined as in 1._

Proof.: Since \(P_{t}\) is \(_{t-d}\)-measurable we have \([-P_{t},c_{t}|_{t-d}]= P_{t}, [-c_{t}|_{t-d}]_{d}\), where the last step uses Assumption 1. 

Thus, by following the decomposition of Theorem 1, we are left with the problem of bounding the regret of the delayed online learning algorithm against \(P_{W_{n}|S_{n}}\), denoted as \(_{d,n}(P_{W_{n}|S_{n}})=_{t=1}^{n} P_{t}-P_{W_ {n}|S_{n}},c_{t}\). The following proposition states a simple and clean bound that one can immediately derive from these insights.

**Proposition 1** (Bound in expectation).: _Consider \((Z_{t})_{t^{*}}\) satisfying Assumption 1 and suppose there exists a \(d\)-delayed online learning algorithm with regret bounded by \(_{d,n}(P^{*})\) against any comparator \(P^{*}\). Then, the expected generalization of \(\) is bounded as_

\[[(,S_{n})] [_{d,n}(P_{W_{n}|S_{n}})]}{n}+_{d}\,.\]

Proof.: By Theorem 1, it holds that \([(,S_{n})]=[_{d,n}(P_{W_{n}|S_{n}})]}{n}+[M_{n}]\), where the regret is for a strategy \(P_{t}\) in the delayed generalization game. Hence, by Lemma 1

\[[M_{n}]=[-_{t=1}^{n} P_{t},c_{ t}]=_{t=1}^{n}[-P_{t},c_{t} ]=_{t=1}^{n}[[-P_{t},c _{t}|_{t-d}]]_{d}\,,\]

which proves the claim. 

The above result holds in expectation over the training sample. We now provide a high-probability guarantee on the generalization error.

**Theorem 3** (Bound in probability).: _Assume that \((Z_{t})_{t^{*}}\) satisfies Assumption 1 and consider a \(d\)-delayed online learning algorithm with regret bounded by \(R_{d,n}(P^{*})\) against any comparator \(P^{*}\). Then, for any \(>0\), it holds with probability \(1-\) on the draw of \(S_{n}\), uniformly for all \(\),_

\[(,S_{n})(P_{W_{n}|S_{n}})}{n}+_{d }+)}{n}}.\]The proof of this claim follows directly from combining the decomposition of Theorem 1 with a standard concentration result for mixing processes that we state below.

**Lemma 2**.: _Fix \(d 1,n\) and consider \((Z_{t})_{t^{*}}\) satisfying Assumption 1. Consider the generalization game of Definition 1. Then, for any \(>0\), the following bound is satisfied with probability at least \(1-\):_

\[M_{n}_{d}+)}{n}}.\]

The proof is based on a classic "blocking" technique due to Yu (1994). For the sake of completeness, we provide a proof in Appendix A.2.

## 4 New generalization bounds for non-_i.i.d._ data

The dependence on the delay \(d\) for the bounds that we presented in the previous section is non-trivial. Indeed, if on the one hand increasing the delay will reduce the magnitude of \(_{d}\), on the other hand the regret of the online learner will grow with \(d\). There is hence a trade-off between these two terms appearing in our bounds. In what follows, we derive some concrete generalization bounds from Theorem 3, under a number of different choices of the online learning algorithm. For concreteness, we will consider two types of mixing assumptions, but stress that the approach can be applied to any process that satisfies Assumption 1.

### Regret bounds for delayed online learning

From Theorem 3, we can obtain a generalization bound using our framework if we have a regret bound for a delayed online algorithm. This is a well-known problem in the area of online learning (see, _e.g._, Weinberger and Ordentlich, 2002; Joulani et al., 2013). In the following, we will leverage the following simple trick that allows us to extend the regret bounds of any online learning algorithm to its delayed counterpart, provided that the regret bound respects some specific assumptions.

**Lemma 3** (Weinberger and Ordentlich, 2002).: _Consider any online algorithm whose regret satisfies \(_{n}(P^{*}) R(n)\) for any comparator \(P^{*}\), where \(R\) is a non-decreasing real-valued function such that \(y yR(x/y)\) is a concave function of \(y\) for any fixed \(x\). Then, for any \(d 1\) there exists an online learning algorithm with delay \(d\) such that, for any comparator \(P^{*}\),_

\[_{d,n}(P^{*}) dR(n/d)\,.\]

The proof idea is closely related to the blocking trick of Yu (1994), with an algorithmic construction that runs one instance of the base method for each index \(i=1,2,,d\), with the \(i\)-th instance being responsible for the regret in rounds \(i,i+d,i+2d,\) (more details are provided in Appendix B.3). For most of the regret bounds that we consider, the function \(R\) takes the form \(R(n)=O()\), so that the first term in the generalization bound is typically of order \(\). Since this term matches the bound on \(M_{n}\) in Lemma 2, in this case the final generalization bound behaves effectively as if the sample size was \(n/d\) instead of \(n\).

### Geometric and algebraic mixing

The following definition gives two concrete examples of mixing processes that satisfy Assumption 1 with different choices of \(_{d}\), and are commonly considered in the related literature (see, e.g., Mohri and Rostamizadeh, 2010; Levin and Peres, 2017).

**Definition 2**.: _We say that a stationary process \((Z_{t})_{t^{*}}\) satisfying Assumption 1 is:_

* geometrically mixing_, if_ \(_{d}=Ce^{-}\)_, for some positive_ \(\) _and_ \(C\)_;_
* algebraically mixing_, if_ \(_{d}=Cd^{-r}\)_, for some positive_ \(r\) _and_ \(C\)_._

Instantiating the bound of Theorem 3 to these two cases yields the following two corollaries.

**Corollary 1**.: _Assume \((Z_{t})_{t^{*}}\) is a geometrically mixing process with constants \(,C>0\). Consider a \(d\)-delayed online learning algorithm with regret bounded by \(R_{d,n}(P^{*})\) for all comparators \(P^{*}\).__Then, setting \(d= n\), for any \(>0\), with probability at least \(1-\) we have that, uniformly for any algorithm \(\),_

\[(,S_{n})(P_{W_{n}|S_{n}})}{n}++ )}{n}}\,.\]

Up to a term linear in \(\) and some logarithmic factors, the above states that under the geometric mixing the same rates are achievable as in the _i.i.d._ setting. Roughly speaking, this amounts to saying that the effective sample size is a factor \(\) smaller than the original number of samples \(n\), as long as generalization is concerned.

**Corollary 2**.: _Assume \((Z_{t})_{t^{*}}\) is an algebraic mixing process with constants \(r,C>0\). Consider a \(d\)-delayed online learning algorithm with regret bounded by \(R_{d,n}(P^{*})\) against any comparator \(P^{*}\). Then, setting \(d=(C^{2}n)^{1/(1+2r)}\), for any \(>0\), with probability at least \(1-\) we have that, uniformly for any algorithm \(\),_

\[(,S_{n})(P_{W_{n}|S_{n}})}{n}+C( 1+)n^{-}\,.\]

This result suggests that the rates achievable for algebraically mixing processes are qualitatively much slower than what one can get for _i.i.d._ or geometrically mixing data sequences (although the rates do eventually approach \(1/\) as \(r\) goes to infinity).

### Multiplicative weights with delay

We start our discussion on possible online strategies by focusing on the classic exponential weighted average (EWA) algorithm (Vovk, 1990; Littlestone and Warmuth, 1994; Freund and Schapire, 1997). We fix a data-free prior \(P_{1}_{}\) and a learning rate parameter \(>0\). We consider the updates

\[P_{t+1}=*{arg\,min}_{P_{}}\{ P, c_{t}+_{KL}(P||P_{t})\},\]

Combining the standard regret bound of EWA (see Appendix B.1) with Lemma 3 and Corollary 1 yields the result that follows.

**Corollary 3**.: _Suppose that \((Z_{t})_{t^{*}}\) is a geometric mixing process with constants \(,C>0\). Suppose that \((w,z)\) for all \(w,z\). Then, for any \(P_{1}_{}\) and any \(>0\), with probability at least \(1-\), uniformly on any learning algorithm \(\) we have_

\[(,S_{n})_{KL}(P^{*}||P_{1})(  n+1)}{ n}+++)}{n}}\,.\]

This results suggests that when considering geometric mixing processes, by applying a union bound over a well-chosen range of \(\) we recover the PAC-Bayes bound of McAllester (1998) up to a \(O()\) factor. A similar result can be derived from Corollary 2 for algebraically mixing processes, leading to a bound typically scaling as \(n^{-2r/(2(1+2r))}\).

### Follow the regularized leader with delay

In this subsection we extend the common class of online learning algorithms known as follow the regularized leader (FTRL, see _e.g._, Abernethy and Rakhlin, 2009; Orabona, 2019) to the problem of learning with delay. FTRL algorithms are defined using a convex regularization function \(h:_{}\). We restrict ourselves to the set of proper, lower semi-continuous and \(\)-strongly convex functions with respect to a norm \(||.||\) (and its respective dual norm \(||.||_{*}\)) defined on the set of signed finite measures on \(\) (see Appendix B.2 for more details). The online procedure (without delay) of the FTRL algorithm is as follows:

\[P_{t+1}=*{arg\,min}_{P_{}}\{_{s=1} ^{t} P,c_{s}+h(P)\}.\]The existence of the minimum is guaranteed by the compactness of \(_{}\) under \(\|\|\), and its uniqueness is ensured by the strong convexity of \(h\). Combining the analysis of FTRL (see Appendix B.2) with Lemma 3 and Corollary 1 yields the following result.

**Corollary 4**.: _Suppose that \((Z_{t})_{t^{*}}\) is a geometric mixing process with constants \(,C>0\). Suppose that \((w,z)\) for all \(w,z\). Assume there exists \(B>0\) such that for all \(t,\|c_{t}\|_{*} B\). Then, for any \(P_{1}_{}\), for any \(>0\) with probability at least \(1-\) on the draw of \(S_{n}\), uniformly for all \(\),_

\[(,S_{n}))-h(P_{1}))(  n+1)}{ n}+}{2}++)}{n}}.\]

This generalization bound is similar to the bound of Theorem 9 of Lugosi and Neu (2023) up to a \(O()\) factor, when applying a union-bound argument over an appropriate grid of learning-rates \(\). In particular, this result recovers PAC-Bayesian bounds like those of Corollary 3 when choosing \(h=_{}(\|P_{1})\). We refer to Section 3.2 in Lugosi and Neu (2023) for more discussion on such bounds. As before, a similar result can be stated for algebraically mixing processes, with the leading terms approaching zero at rate of \(n^{-2r/2(1+2r)}\) instead of \(n^{-1/2}\).

## 5 Generalization bounds for dynamic hypotheses

Finally, inspired by the works of Eringis et al. (2022, 2024), we extend our framework to accommodate loss functions \(\) that rely not only on the last data point \(Z_{t}\), but on the entire data sequence \(_{t}=(Z_{t},Z_{t-1},,Z_{1})\). Formally, we will consider loss functions of the form \(:^{*}_{+}\)1 and write \((w,_{t})\) to denote the loss associated with hypothesis \(w\) on sequence \(_{t}^{t}\). This consideration extends the learning problem to class of dynamical predictors such as Kalman filters, autoregressive models, or recurrent neural networks (RNNs), broadly used in time-series forecasting (Ariyo et al., 2014; Takeda et al., 2016). Specifically, if we think of \(z_{t}=(x_{t},y_{t})\) as a data-pair of context and observation, in time-series prediction we usually not only rely on the context \(x_{t}\) but also on the past sequence of contexts and observations \((x_{t-1},y_{t-1},,x_{1},y_{1})\). As an example, consider \((w,z_{t},,z_{1})=(y_{t}-h_{w}(x_{t},z_{t-1},,z_{1}))^ {2}\) where \(h\) is a function class parameterized by \(\). For this type of loss function a natural definition of the test error is:

\[}(w)=_{n}[(w,Z_{t}^{ },Z_{t-1}^{},...,Z_{t-n}^{})],\]

where \(_{t}^{}=(Z_{t}^{},Z_{t-1}^{},)\) is a semi-infinite random sequence drawn from the same stationary process that has generated the data \(_{t}\). We consider the following assumption.

**Assumption 2**.: _For a given process \((Z_{t})_{t}\) with joint-distribution \(\) over \(^{}\) and same marginals \(\) over \(\), there exists a non-increasing sequence \((_{d})_{d^{*}}\) of non-negative real numbers such that the following holds for all \(w\), for all \(t^{*}\):_

\[[(w,Z_{t},,Z_{1})-}(w)| _{t-d}]_{d}.\]

This is a generalization of Assumption 1 in the sense that taking \((w,Z_{t},,Z_{1})=(w,Z_{t})\) simply amounts to requiring the same mixing condition as before. For our online-to-PAC conversion we consider the same framework as in Definition 1, except that now the cost function is defined as

\[c_{t}:w(w,Z_{t},,Z_{1})-}(w)\,.\]

Then it easy to check that result of Lemma 2 still holds for this specific cost, and we can thus extend all the results of Section 4. For concreteness, we state the following adaptation of Theorem 3 below.

**Theorem 4**.: _Assume \((Z_{t})_{t}\) which satisfies Assumption 2 and consider a \(d\)-delayed online learning algorithm with regret bounded by \(R_{d,n}(P^{*})\) against any comparator \(P^{*}\). Then, for any \(>0\), it holds with probability \(1-\):_

\[(,S_{n})(P_{W_{n} S_{n}})}{n}+_ {d}+)}{n}}.\]To see that Assumption 2 can be verified and the resulting bounds can be meaningfully applied, consider the following concrete assumptions about the hypothesis class, the loss function, and the data generating process. The first assumption says that for any given hypothesis, the influence of past data points on the associated loss vanishes with time (_i.e._, the hypothesis forgets the old data points at a controlled rate).

**Assumption 3**.: _There exists a decreasing sequence \((B_{d})_{d^{*}}\) of non-negative real numbers such that for any two sequences \(_{t}=(z_{t},,z_{i})\) and \(^{}_{t}=(z^{}_{t},,z^{}_{j})\) of possibly different lengths that satisfy \(z_{k}=z^{}_{k}\) for all \(k t,,t-d+1\), we have \(|(w,_{t})-(w,^{}_{t})| B_{d},\) for all \(w\)._

This condition can be verified for stable dynamical systems like autoregressive models, certain classes of RNNs, or sequential predictors that have bounded memory by design (see Eringis et al., 2022, 2024). The next assumption is a refinement of Assumption 1, adapted to the case where the loss function acts on blocks of \(d\) data points \(_{t-d+1:t}=(z_{t},z_{t-1},,z_{t-d+1})\).

**Assumption 4**.: _Let \(_{t}=(Z_{t},,Z_{1})\) be a sequence of data points and let \(^{}_{t}=(Z^{}_{t},,Z^{}_{0},)\) be an independent copy of the same process. Then, there exists a decreasing sequence \((_{d})_{d^{*}}\) non-negative real numbers such that the following is satisfied for all hypotheses \(w\) and all \(d^{*}\):_

\[[.(w,^{}_{t-d+1:t})-(w, {Z}_{t-d+1:t})|_{t-2d}]_{d}\,.\]

This assumption can be verified whenever the loss function is bounded and the joint distribution of the data block \(_{t-d+1:t}\) satisfies a \(\)-mixing assumption. In more detail, this latter condition amounts to requiring that the conditional distribution of each data block given a block that trails \(d\) steps behind is close to the marginal distribution in total variation distance, up to an additive term of \(_{d}\). The following proposition shows that these two simple conditions together imply that Assumption 2 holds, and that thus the bound of Theorem 4 can be meaningfully instantiated for bounded-memory hypothesis classes deployed on mixing processes.

**Proposition 2**.: _Suppose that the loss function satisfies Assumption 3 and the data distribution satisfies Assumption 4. Then Assumption 2 is satisfied with \(_{d}=2B_{d/2}+_{d/2}\)._

## 6 Conclusion

We have developed a general framework for deriving generalization bounds for non-i.i.d. processes under a general mixing assumption, via an extension of the online-to-PAC-conversion framework of Lugosi and Neu (2023). Among other results, this approach has allowed us to prove PAC-Bayesian generalization bounds for such data in a clean and transparent way, and even study classes of dynamic hypotheses under a simple bounded-memory condition. These results provide a clean and tight alternative to the results of (Alquier and Wintenberger, 2012; Eringis et al., 2022). The generality of our approach further demonstrates the power of the Online-to-PAC scheme of Lugosi and Neu (2023), and in particular our results provide further evidence that this framework is particularly promising for developing techniques for generalization in non-i.i.d. settings. We hope that flexibility of our framework will find further uses and enables more rapid progress in the area.