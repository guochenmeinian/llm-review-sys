# Energy-Based Models for Anomaly Detection:

A Manifold Diffusion Recovery Approach

 Sangwoong Yoon

Korea Institute for Advanced Study

swyoon@kias.re.kr

&Young-Uk Jin

Samsung Electronics

yueric.jin@samsung.com

&Yung-Kyun Noh

Hanyang University

Korea Institute for Advanced Study

nohyung@hanyang.ac.kr

Corresponding authors

&Frank C. Park

Seoul National University

Saige Research

fcp@snu.ac.kr

###### Abstract

We present a new method of training energy-based models (EBMs) for anomaly detection that leverages low-dimensional structures within data. The proposed algorithm, Manifold Projection-Diffusion Recovery (MPDR), first perturbs a data point along a low-dimensional manifold that approximates the training dataset. Then, EBM is trained to maximize the probability of recovering the original data. The training involves the generation of negative samples via MCMC, as in conventional EBM training, but from a different distribution concentrated near the manifold. The resulting near-manifold negative samples are highly informative, reflecting relevant modes of variation in data. An energy function of MPDR effectively learns accurate boundaries of the training data distribution and excels at detecting out-of-distribution samples. Experimental results show that MPDR exhibits strong performance across various anomaly detection tasks involving diverse data types, such as images, vectors, and acoustic signals.

## 1 Introduction

Unsupervised detection of anomalous data appears frequently in practical applications, such as industrial surface inspection , machine fault detection [2; 3], and particle physics . Modeling the probability distribution of normal data \(p_{data}()\) is a principled approach for anomaly detection . Anomalies, often called outliers or out-of-distribution (OOD) samples, lie outside of the data distribution and can thus be characterized by low probability density under the distribution. However, many deep generative models that can evaluate the likelihood of data, including variational autoencoders (VAE; ), autoregressive models , and flow-based models  are known to perform poorly on popular anomaly detection benchmarks such as CIFAR-10 (in) vs SVHN (out), by assigning high likelihood on seemingly trivial outliers [9; 10].

On the other hand, deep energy-based models (EBMs) have demonstrated notable improvement in anomaly detection compared to other deep generative models . While the specific reason for the superior performance of EBMs has not been formally analyzed, one probable factor is the explicit mechanism employed in EBM's maximum likelihood training that reduces the likelihood of negative samples. These negative samples are generated from the model distribution \(p_{}()\) using Markov Chain Monte Carlo (MCMC). Since modern EBMs operate in high-dimensional data spaces, it is extremely difficult to cover the entire space with a finite-length Markov chain. The difficulty ingenerating negative samples triggered the development of several heuristics such as using truncated chains , persistent chains , sample replay buffers , and data augmentation in the middle of the chain .

Instead of requiring a Markov chain to cover the entire space, EBM can be trained with MCMC running within the vicinity of each training datum. For example, Contrastive Divergence (CD; ) uses a short Markov chain initialized on training data to generate negative samples close to the training distribution. Diffusion Recovery Likelihood (DRL; ) enables MCMC to focus on the training data's neighborhood using a different objective function called recovery likelihood. Recovery likelihood \(p_{}(|})\) is the conditional probability of training datum \(\) given the observation of its copy \(}\) perturbed with a Gaussian noise. Training of DRL requires sampling from the recovery likelihood distribution \(p_{}(|})\), which is easier than sampling from the model distribution \(p_{}()\), as \(p_{}(|})\) is close to uni-modal and concentrated near \(\). While CD and DRL significantly stabilize the negative sampling process, the resulting EBMs exhibit limited anomaly detection performance due to the insufficient coverage of negative samples in the input space.

In this paper, we present a novel training algorithm for EBM that does not require MCMC covering the entire space while achieving strong anomaly detection performance. The proposed algorithm, **Manifold Projection-Diffusion Recovery** (MPDR), extends the recovery likelihood framework by replacing Gaussian noise with **Manifold Projection-Diffusion** (MPD), a novel perturbation operation that reflects low-dimensional structure of data. Leveraging a separately trained autoencoder manifold, MPD firstly projects a training datum onto the manifold and perturbs the datum along the manifold. Compared to Gaussian noise, MPD reflects relevant modes of variation in data, such as change in colors or shapes in an image, as shown in Fig. 1. A MPD-perturbed sample serves as an informative starting point for MCMC that generates a negative sample, teaching EBM to discriminate challenging outliers that has partial similarity to training data.

Given an MPD-perturbed sample, EBM is trained by maximizing the recovery likelihood. We derive a simple expression for evaluating the recovery likelihood for MPD. We also propose an efficient two-stage MCMC strategy for sampling from the recovery likelihood, leveraging the latent space of the autoencoder. Additionally, we show that the model parameter estimation by MPDR is consistent under standard assumptions.

Compared to existing EBM training algorithms using an autoencoder-like auxiliary module [17; 18; 19; 20], MPDR has two advantages. First, MPDR allows the use of multiple autoencoders for training. The manifold ensemble technique stabilizes the training and improves anomaly detection performance by providing diverse negative samples. Second, MPDR achieves good anomaly detection performance with lightweight autoencoders. For example, autoencoders in our CIFAR-10 experiment has about only 1.5\(\)3 million parameters, which are significantly fewer than that of NVAE  (\(\)130 million) or Glow  (\(\)44 million), both of which are previously used in EBM training [17; 18].

Our contributions can be summarized as follows:

* We propose MPDR, a novel method of using autoencoders for training EBM. MPDR is compatible with any energy functions and gives consistent density parameter estimation with any autoencoders.
* We provide a suite of practical strategies for achieving successful anomaly detection with MPDR, including two-stage sampling, energy function design, and ensembling multiple autoencoders.

Figure 1: (Left) An illustration of Manifold Projection-Diffusion. A datum \(\) is first projected into the latent space \(}\) through the encoder \(f_{e}()\) and then diffused with a local noise, such as Gaussian. The perturbed sample \(}\) is obtained by projecting it back to the input space \(\) through the decoder \(f_{d}()\). (Right) Comparison between the perturbations in MPDR and DRL  on CIFAR-10 examples.

* We demonstrate the effectiveness of MPDR on unsupervised anomaly detection tasks with various data types, including images, representation vectors, and acoustic signals.

## 2 Preliminaries

Energy-Based Models (EBM)An energy-based generative model, or an unnormalized probabilistic model, represents a probability density function using a scalar energy function \(E_{}:\), where \(\) denotes the domain of data. The energy function \(E_{}\) defines a probability density function \(p_{}\) through the following relationship:

\[p_{}()(-E_{}()).\] (1)

The parameters \(\) can be learned by maximum likelihood estimation given iid samples from the underlying data distribution \(p_{data}()\). The gradient of the log-likelihood for a training sample \(\) is well-known  and can be written as follows:

\[_{} p_{}()=-_{}E_{}()+_{^{-} p_{}()}[_{}E_{ }(^{-})],\] (2)

where \(^{-}\) denotes a "negative" sample drawn from the model distribution \(p_{}()\). Typically, \(^{-}\) is generated using Langevin Monte Carlo (LMC). In LMC, points are arbitrarily initialized and then iteratively updated in a stochastic manner to simulate independent sampling from \(p_{}()\). For each time step \(t\), a point \(^{(t)}\) is updated by \(^{(t+1)}=^{(t)}-_{1}_{}E_{}( ^{(t)})+_{2}^{(t)}\), for \(^{(t)}(0,)\). The step size \(_{1}\) and the noise scale \(_{2}\) are often tuned separately in practice. Since LMC needs to be performed in every iteration of training, it is infeasible to run negative sampling until convergence, and a compromise must be made. Popular heuristics include initializing MCMC on training data , using short-run LMC , and utilizing a replay buffer [13; 11].

Recovery Likelihood [22; 16]Instead of directly maximizing the likelihood (Eq. (2)), \(\) can be learned through the process of denoising data from artificially injected Gaussian noises. Denoising corresponds to maximizing the _recovery likelihood_\(p(|})\), the probability of recovering data \(\) from its noise-corrupted version \(}=+\), where \((0,)\).

\[p_{}(|}) p_{}()p( }|)(-E_{}()- {1}{2^{2}}||-}||^{2}),\] (3)

where Bayes' rule is applied. The model parameter \(\) is estimated by maximizing the log recovery likelihood, i.e., \(_{}_{,}}[ p_{}( |})]\), for \( p_{data}(),} p(}|)\), where \(p(}|)=(,^{2})\). This estimation is shown to be consistent under the usual assumptions (Appendix A.2 in ). DRL  uses a slightly modified perturbation \(}=}+\) in training EBM, following . This change introduces a minor modification of Eq. (3).

The recovery likelihood \(p_{}(|})\) (Eq. 3) is essentially a new EBM with the energy \(_{}(|})=E_{}()+|| -}||^{2}/2^{2}\). Therefore, the gradient \(_{} p_{}(|})\) has the same form with the log-likelihood gradient of EBM (Eq. (2)), except that negative samples are drawn from \(p_{}(|})\) instead of \(p_{}()\):

\[_{} p_{}(|})=-_{ }E_{}()+_{^{-} p_{}(| })}[_{}E_{}(^{-})],\] (4)

where \(_{}_{}(|})=_{ }E_{}()\) as the Gaussian noise is independent of \(\). Sampling from \(p_{}(|})\) is more stable than sampling from \(p_{}()\), because \(p_{}(|})\) is close to a uni-modal distribution concentrated near \(\)[16; 22]. However, it is questionable whether Gaussian noise is the most informative way to perturb data in a high-dimensional space.

## 3 Manifold Projection-Diffusion Recovery

We introduce the Manifold Projection-Diffusion Recovery (MPDR) algorithm, which trains EBM by recovering from perturbations that are more informative than Gaussian noise. Firstly, we propose Manifold Projection-Diffusion (MPD), a novel perturbation operation leveraging the low-dimensional structure inherent in the data. Then, we derive the recovery likelihood for MPD. We also provide an efficient sampling strategy and practical implementation techniques for MPDR. The implementation of MPDR is publicly available1.

AutoencodersMPDR assumes that a pretrained autoencoder approximating the training data manifold is given. The autoencoder consists of an encoder \(f_{e}:\) and a decoder \(f_{d}:\), both assumed to be deterministic and differentiable. The latent space is denoted as \(\). The dimensionalities of \(\) and \(\) are denoted as \(D_{}\) and \(D_{}\), respectively. We assume \(f_{e}\) and \(f_{d}\) as typical deep neural networks jointly trained to reduce the training data's reconstruction error \(l()\), where \(l()\) is typically an \(l_{2}\) error, \(l()=||-f_{d}(f_{e}())||^{2}\).

### Manifold Projection-Diffusion

We propose a novel perturbation operation, **Manifold Projection-Diffusion** (MPD). Instead of adding Gaussian noise directly to a datum \(}}\) as in the conventional recovery likelihood, MPD first encodes \(\) using the autoencoder and then applies a noise in the latent space:

\[}}}}},\] (5)

where \(=f_{e}()\), \(}=+\), and \(}=f_{d}(})\). The noise magnitude \(\) is a predefined constant and \((0,)\). The first step **projects**\(\) into \(\), and the second step **diffuses** the encoded vector \(\). When decoded through \(f_{d}\), the output \(}\) always lies on the **decoder manifold**\(=\{|=f_{d}(),\}\), a collection of all possible outputs from the decoder \(f_{d}()\). The process is visualized in the left panel of Fig. 1.

Since \(\) serves as a coordinate of \(\), a Gaussian noise in \(\) corresponds to perturbation of data along the manifold \(\), reflecting more relevant modes of variation in data than Gaussian perturbation in \(\) (Fig. 1). Note that MPD reduces to the conventional Gaussian perturbation if we set \(=\) and set \(f_{e}\) and \(f_{d}\) as identity mappings.

### Manifold Projection-Diffusion Recovery Likelihood

We define the recovery likelihood for MPD as \(p_{}(|})\), which is evaluated as follows:

\[p_{}(|})}}{{}}p_{}()p(}| )=p_{}()( p(}|) p(|))}}{{=}}p_{ }()( p(}|)_{f_{e}( )}())\] (6) \[}}{{=}}p_{}()p (}|=f_{e}())(-E_{}( )+ p(}|))(-_{ }(|})).\] (7)

In (i), we apply Bayes' rule. In (ii), we treat \(p(|)\) as \(_{}()\), a Dirac measure on \(\) at \(=f_{e}()\), as our encoder is deterministic. Equality (iii) results from the property of Dirac measure \( f()_{}()=f( )\). Using Gaussian perturbation simplifies the energy to \(_{}(|})=E_{}()+ }||}-f_{e}()||^{2}\).

Now, \(E_{}()\) is trained by maximizing \( p_{}(|})\). The gradient of \( p_{}(|})\) with respect to \(\) has the same form as the conventional maximum likelihood case (Eq. 2) and the Gaussian recovery likelihood case (Eq. 4) but with a different negative sample distribution \(p_{}(|})\):

\[_{} p_{}(|})=-_{ }E_{}()+_{^{-} p_{}(| })}[_{}E_{}(^{-})].\] (8)

Negative samples \(^{-}\) are drawn from \(p_{}(|})\) using MCMC.

Another possible definition for the recovery likelihood is \(p(|})\), which becomes equivalent to \(p(|})\) when \(f_{d}\) is an injective function, i.e., no two instances of \(}\) map to the same \(}\). However, if \(f_{d}\) is not injective, additional information loss may occur and becomes difficult to compute. As a result, \(p(|})\) serves as a more general and also convenient choice for the recovery likelihood.

Figure 2: Negative sample generation process in MDPR. Data \(\) (blue dots) are first projected and diffused on autoencoder manifold \(\) (gray line), resulting in \(}\). The latent chain starting from \(}\) generates \(_{0}^{-}\). The visible chain draws negative samples \(^{-}\) from \(p(|})\) starting from \(_{0}^{-}\).

ConsistencyMaximizing \( p(|})\) leads to the consistent estimation of \(\). The consistency is shown by transforming the objective function into KL divergence. The assumptions resemble those for maximum likelihood estimation, including infinite data, identifiability, and a correctly specified model. Additionally, the autoencoder \((f_{e},f_{d})\) and the noise magnitude \(\) should be independent of \(\), remaining fixed during training. The consistency holds for any choices of \((f_{e},f_{d})\) and \(\), as long as the recovery likelihood \(p_{}(|})\) is non-zero for any \((,})\). Further details can be found in the Appendix.

### Two-Stage Sampling

A default method for drawing \(^{-}\) from \(p_{}(|})\) is to execute LMC on \(\), starting from \(}\), as done in DRL . While this **visible chain** should suffice in principle with infinite chain length, sampling can be improved by leveraging the latent space \(\), as demonstrated in [24; 18; 20; 25]. For MPDR, we propose a **latent chain**, a short LMC operating on \(\) that generates a better starting point \(_{0}^{-}\) for the visible chain. We first define the auxiliary latent energy \(_{}()=_{}(f_{d}()|})\), the pullback of \(_{}(|})\) through the decoder \(f_{d}()\). Then, we run a latent LMC that samples from a probability density proportional to \((-_{}())\). The latent chain's outcome, \(_{0}^{-}\), is fed to the decoder to produce the visible chain's starting point \(_{0}^{-}=f_{d}()\), which is likely to have a smaller energy than the original starting point \(_{}(_{0}^{-}|})<(}|})\). Introducing a small number of latent chain steps improves anomaly detection performance in our experiments. A similar latent-space LMC method appears in  but requires a sample replay buffer not used in MPDR. Fig.2 illustrates the sampling procedure, and Algorithm1 summarizes the training algorithm.

### Perturbation Ensemble

Using _multiple_ perturbations simultaneously during MPDR training improves performance and stability, while not harming the consistency. Although the consistency of estimation is independent of the specifics of the perturbation, the perturbation design, such as \((f_{e},f_{d})\) and \(\), have a significant impact on performance in practice. Perturbation ensemble alleviates the risk of adhering to a single suboptimal choice of perturbation.

Noise Magnitude EnsembleWe randomly draw the perturbation magnitude \(\) from a uniform distribution over a predefined interval for each sample in a mini-batch. In our implementation, we draw \(\) from the interval \([0.05,0.3]\) throughout all the experiments.

Manifold EnsembleWe can also utilize multiple autoencoder manifolds \(\) in MPD. Given \(K\) autoencoders, a mini-batch is divided into \(K\) equally sized groups. For each group, negative samples are generated separately using the corresponding autoencoder. Only a minimal increase in training time is observed as \(K\) increases, since each autoencoder processes a smaller mini-batch. Memory overhead does exist but is manageable, since MPDR can achieve good performance with relatively smaller autoencoders.

Figure 4: CIFAR-10 negative samples from a manifold ensemble. Negative samples become more visually complex as \(D_{}\) grows larger.

Figure 3: 2D density estimation with a scalar energy function (left) and an autoencoder-based energy function (right).

An effective strategy is to utilize autoencoders with varying latent space dimensionalities \(D_{}\). For high-dimensional data, such as images, \(\) with different \(D_{}\) tends to capture distinct modes of variation in data. As depicted in Fig. 4, a perturbation on \(\) with small \(D_{}\) corresponds to low-frequency variation in \(\), whereas for \(\) with large \(D_{}\), it corresponds to higher-frequency variation. Using multiple \(D_{}\)'s in MPDR gives us more diverse \(^{-}\) and eventually better anomaly detection performance.

### Energy Function Design

MPDR is a versatile training algorithm for general EBMs, compatible with various types of energy functions. The design of an energy function plays a crucial role in anomaly detection performance, as the inductive bias of an energy governs its behavior in out-of-distribution regions. We primarily explore two designs for energy functions: MPDR-Scalar (**MPDR-S**), a feed-forward neural network that takes input \(\) and produces a scalar output, and MPDR-Reconstruction (**MPDR-R**), the reconstruction error from an autoencoder, \(E_{}()=||-g_{d}(g_{e}())||^{2}\), for an encoder \(g_{e}\) and a decoder \(g_{d}\). The autoencoder \((g_{e},g_{d})\) is separate from the autoencoder \((f_{e},f_{d})\) used in MPD. First proposed in , a reconstruction-based energy function has an inductive bias towards assigning high energy values to off-manifold regions (Fig. 3). Training such energy functions using conventional techniques like CD  or persistent chains [13; 11] is reported to be challenging . However, MPDR effectively trains both scalar and reconstruction energies. Additionally, in Sec. 4.4, we demonstrate that MPDR is also compatible with an energy function based on a masked autoencoder.

## 4 Experiment

### Implementation of MPDR

An autoencoder \((f_{e},f_{d})\) is trained by minimizing the reconstruction error of the training data and remains fixed during the training of \(E_{}()\). When using an ensemble of manifolds, each autoencoder is trained independently. For anomaly detection, the energy value \(E_{}()\) serves as an anomaly score which assigns a high value for anomalous \(\). All optimizations are performed using Adam with a learning rate of 0.0001. Each run is executed on a single Tesla V100 GPU. Other details, including network architectures and LMC hyperparameters, can be found in the Appendix.

Spherical Latent SpaceIn all our implementations of autoencoders, we utilize a hyperspherical latent space \(=^{D_{}-1}\)[28; 29; 30]. The encoder output is projected onto \(^{D_{}-1}\) via division by its norm before being fed into the decoder. Employing \(^{D_{}-1}\) standardizes the length scale of

   Hold-Out Digit & 1 & 4 & 5 & 7 & 9 \\  Autoencoder (AE) & 0.062 \(\) 0.000 & 0.204 \(\) 0.003 & 0.259 \(\) 0.006 & 0.125 \(\) 0.003 & 0.113 \(\) 0.001 \\ IGEBM & 0.101 \(\) 0.020 & 0.106 \(\) 0.019 & 0.205 \(\) 0.108 & 0.100 \(\) 0.042 & 0.079 \(\) 0.015 \\ MEG \({}^{*}\) & 0.281 \(\) 0.035 & 0.401 \(\) 0.061 & 0.402 \(\) 0.062 & 0.290 \(\) 0.040 & 0.324 \(\) 0.034 \\ BiGAN-\(\)\({}^{*}\) & 0.287 \(\) 0.023 & 0.443 \(\) 0.029 & 0.514 \(\) 0.029 & 0.347 \(\) 0.017 & 0.307 \(\) 0.028 \\ Latent EBM\({}^{*}\) & 0.336 \(\) 0.008 & 0.630 \(\) 0.017 & 0.619 \(\) 0.013 & 0.463 \(\) 0.009 & 0.413 \(\) 0.010 \\ VAE+EBM \({}^{*}\) & 0.297 \(\) 0.033 & 0.723 \(\) 0.042 & 0.676 \(\) 0.041 & 0.490 \(\) 0.041 & 0.383 \(\) 0.025 \\ NAE  & 0.802 \(\) 0.079 & 0.648 \(\) 0.045 & 0.716 \(\) 0.032 & 0.789 \(\) 0.041 & 0.441 \(\) 0.067 \\ MPDR-S (ours) & 0.764 \(\) 0.045 & **0.823**\(\) 0.018 & 0.741 \(\) 0.041 & **0.857**\(\) 0.022 & 0.478 \(\) 0.048 \\ MPDR-R (ours) & **0.844**\(\) 0.030 & 0.711 \(\) 0.029 & **0.757**\(\) 0.024 & 0.850 \(\) 0.014 & **0.569**\(\) 0.036 \\   

Table 1: MNIST hold-out digit detection. Performance is measured in AUPR. The standard deviation of AUPR is computed over the last 10 epochs. The largest mean value is marked in bold, while the second-largest is underlined. Asterisks denote that the results are adopted from literature.

    & kMNIST & EMNIST & Omniglot & FashionMNIST & Constant \\  AE & 0.999 & 0.977 & 0.947 & 1.000 & 0.954 \\ IGEBM & 0.990 & 0.923 & 0.845 & 0.996 & 1.000 \\ NAE & 1.000 & 0.993 & 0.997 & 1.000 & 1.000 \\ MPDR-S & 0.999 & 0.995 & 0.994 & 0.999 & 1.000 \\ MPDR-R & 0.999 & 0.989 & 0.997 & 0.999 & 0.990 \\   

Table 2: MNIST OOD detection performance measured in AUPR. We test models from hold-out digit 9 experiment (Table 1). The overall performance is high, as detecting these outliers is easier than identifying the hold-out digit.

\(\), allowing us to use the same value of \(\) across various data sets and autoencoder architectures. Meanwhile, the impact of \(^{D_{}-1}\) on the reconstruction error is minimal.

RegularizationFor a scalar energy function, MPDR-S, we add \(L_{reg}=E_{}()^{2}+E_{}(^{-})^{2}\) to the loss function, as proposed by . For a reconstruction energy function, MPDR-R, we add \(L_{reg}=E_{}(^{-})^{2}\), following .

Scaling Perturbation ProbabilityApplying regularization to an energy restricts its scale, causing a mismatch in scales between the two terms in the recovery likelihood (Eq. (7)). To remedy this mismatch, we heuristically introduce a scale factor \(<1\) to \( p(|})\), resulting in the modified recovery energy \(_{}^{()}(|})=E_{}( )+}||}-f_{e}() ||^{2}\). We use \(=0.0001\) for all experiments.

### 2D Density Estimation

We show MPDR's ability to estimate multi-modal density using a mixture of eight circularly arranged 2D Gaussians (Fig. 2). We construct an autoencoder with \(^{1}\) latent space, which approximately captures the circular arrangement. The encoder and the decoder are MLPs with two layers of 128 hidden neurons. To show the impact of the design of energy functions, we implement both scalar energy and reconstruction energy. Three-hidden-layer MLPs are used for the scalar energy function, and the encoder and the decoder in the reconstruction energy function. Note that the network architecture of the reconstruction energy is not the same as the autoencoder used for MPD. The density estimation results are presented in Fig. 3. We quantify density estimation performance using \(l_{1}\) error. After numerically normalizing the energy function and true density on the visualized bounded domain, we compute the \(l_{1}\) error at 10,000 grid points. While both energies capture the overall landscape of the density, the reconstruction energy achieves a smaller error by suppressing probability density at off-manifold regions.

### Image Out-of-Distribution Detection

MNIST Hold-Out Digit DetectionFollowing the protocol of [26; 27], we evaluated the performance of MPDR on MNIST hold-out digit detection benchmark, where one of the ten digits in the MNIST dataset is considered anomalous, and the remaining digits are treated as in-distribution. This is a challenging task due to the diversity of the in-distribution data and a high degree of similarity between target anomalies and inliers. In particular, selecting digits 1, 4, 5, 7, and 9 as anomalous is known to be especially difficult. The results are shown in Table 1.

In MPDR, we use a single autoencoder \((f_{e},f_{d})\) with \(D_{}=32\). The energy function of MPDR-S is initialized from scratch, and the energy function of MPDR-R is initialized from the \((f_{e},f_{d})\) used in MPD. Even without a manifold ensemble, MPDR shows significant improvement over existing algorithms, including ones leveraging an autoencoder in EBM training [19; 24]. The performance of MPDR is stable over a range of \(D_{}\), as demonstrated in Appendix B.1.

MNIST OOD DetectionTo ensure that MPDR is not overfitted to the hold-out digit, we test MPDR in detecting five non-MNIST outlier datasets (Table 2). The results demonstrated that MPDR excels in detecting a wide range of outliers, surpassing the performance of naive algorithms such as autoencoders (AE) and scalar EBMs (IGEBM). Although MPDR achieves high overall detection performance, MPDR-R exhibits slightly weaker performance on EMNIST and Constant datasets. This can be attributed to the limited flexibility of the autoencoder-based energy function employed in MPDR-R.

Cifar-10 OOD DetectionWe evaluate MPDR on the CIFAR-10 inliers, a standard benchmark for EBM-based OOD detection. The manifold ensemble includes three convolutional autoencoders, with \(D_{}=32,64,128\). MPDR-S uses a ResNet energy function used in IGEBM . MPDR-R adopts the ResNet-based autoencoder architecture used in NAE .

Table 3 compares MPDR to state-of-the-art EBMs. MPDR-R shows competitive performance across five OOD datasets, while MPDR-S also achieves high AUROC on SVHN and Constant. As both MPDR-R and NAE use the same autoencoder architecture for the energy, the discrepancy in performance can be attributed to the MPDR training algorithm. MPDR-R outperforms NAE on four out of five OOD datasets. Comparison between MPDR-S and DRL demonstrates the effectiveness of non-Gaussian manifold-aware perturbation used in MPDR. CLEL shows strong

overall performance, indicating that learning semantic information is important in this benchmark. Incorporating contrastive learning into MPDR framework is an interesting future direction.

Cifar-100 OOD Detection on Pretrained RepresentationIn Table 4, we test MPDR on OOD detection with CIFAR-100 inliers. To model a distribution of diverse images like CIFAR-100, we follow  and apply generative modeling in the representation space from a large-scale pretrained model. As we assume an unsupervised setting, we use pretrained representations without fine-tuning. Input images are transformed into 768D vectors by ViT-B\({}_{-}\)16 model . ViT outputs are normalized with its norm and projected onto a hypersphere. We observe that adding a small Gaussian noise of 0.01 to training data improves stability of all algorithms. We use MLP for all energy functions and autoencoders. In MPDR, the manifold ensemble comprises three autoencoders with \(D_{}=128,256,1024\). We also implement supervised baselines (MD  and RMD ). The spherical projection is not applied for MD and RMD to respect their original implementation.

MPDR demonstrates strong anomaly detection performance in the representation space, with MPDR-S and MPDR-R outperforming IGEBM and AE/NAE, respectively. This success can be attributed to the low-dimensional structure often found in the representation space of in-distribution data, as observed in . MPDR's average performance is nearly on par with supervised methods, MD and RMD, which utilize class information. Note that EBM inputs are no longer images, making previous EBM training techniques based on image transformation [14; 33] inapplicable.

Ablation StudyTable 3 and 4 also report the results from single-manifold MPDR-R with varying latent dimensionality \(D_{}\) to show MPDR's sensitivity to a choice of an autoencoder manifold. Manifold ensemble effectively hedges the risk of relying on a single autoencoder which may not be optimal for detecting all types of outliers. Furthermore, manifold ensemble often achieves better AUROC score than MPDR with a single autoencoder. Additional ablation studies can be found in the Appendix. In Sec. B.2, we examine the sensitivity of MPDR to noise magnitude \(\) and the effectiveness of the noise magnitude ensemble. Also in Sec. B.4.4, we investigate the effect to scaling parameter \(\) and show that the training is unstable when \(\) is too large.

Multi-Class Anomaly Detection on MVTec-AD Using Pretrained RepresentationMVTec-AD  is also a popular anomaly detection benchmark dataset, containing images of 15 objects in their normal and defective forms. We follow the "unified" experimental setting from . Normal images from 15 classes are used as the training set, where no label information is provided. We use the same feature extraction procedure used in . Each image is transformed to a 272\(\)14\(\)14 feature map using EfficientNet-b4. When running MPDR, we treat each spatial dimension of a feature map

    & CIFAR10 & SVHN & CelebA \\ 
**Supervised** & & & \\ MD  & 0.8634 & 0.9638 & 0.8833 \\ RMD  & 0.9159 & 0.9685 & 0.4971 \\ 
**Unsupervised** & & & \\ AE & 0.8580 & 0.9645 & 0.8103 \\ NAE  & 0.8041 & 0.9082 & 0.8181 \\ IGEBM  & 0.8217 & 0.9584 & 0.9004 \\ DRL  & 0.5730 & 0.6340 & 0.7293 \\ MPDR-S (ours) & 0.8338 & 0.9911 & **0.9183** \\ MPDR-R (ours) & **0.8626** & **0.9932** & 0.8662 \\ 
**MPDR-R Single AE** & & & \\ \(D_{}=128\) & 0.6965 & 0.9326 & 0.9526 \\ \(D_{}=256\) & 0.8048 & 0.9196 & 0.7772 \\ \(D_{}=1024\) & 0.7443 & 0.9482 & 0.9247 \\   

Table 4: OOD detection on pretrained ViT-B\({}_{-}\)16 representation with CIFAR-100 as in-distribution. Performance is measured in AUROC.

    & SVHN & Textures & Constant & CIFAR100 & CelebA \\  PixelCNN++ \({}^{*}\) & 0.32 & 0.33 & 0.71 & 0.63 & - \\ GLOW \({}^{*}\) & 0.24 & 0.27 & - & 0.55 & 0.57 \\ IGEBM \({}^{*}\) & 0.63 & 0.48 & 0.39 & - & - \\ NVAE  & 0.4402 & 0.4554 & 0.6478 & 0.4943 & 0.6804 \\ VAEBM \({}^{*}\) & 0.83 & - & - & 0.62 & 0.77 \\ JEM \({}^{*}\) & 0.67 & 0.60 & - & 0.67 & 0.75 \\ Improved CD  & 0.7843 & 0.7275 & 0.8000 & 0.5298 & 0.5399 \\ NAE  & 0.9352 & 0.7472 & 0.9793 & 0.6316 & **0.8735** \\ DRL  & 0.8816 & 0.4465 & 0.9884 & 0.4377 & 0.6398 \\ CLEL \({}^{*}\) & 0.9848 & **0.9437** & - & **0.7161** & 0.7717 \\ MPDR-S (ours) & **0.9860** & 0.6583 & **0.9996** & 0.5576 & 0.7313 \\ MPDR-R (ours) & 0.9807 & 0.7978 & **0.9996** & 0.6354 & 0.8282 \\    \\ \(D_{}=128\) & 0.8271 & 0.6606 & 0.9877 & 0.5835 & 0.7751 \\ \(D_{}=64\) & 0.9330 & 0.6631 & 0.9489 & 0.6223 & 0.8272 \\ \(D_{}=128\) & 0.9886 & 0.6942 & 0.9651 & 0.6531 & 0.8500 \\   

Table 3: OOD detection with CIFAR-10 as in-distribution. AUROC values are presented. The largest value in the column is marked as boldface, and the second and the third largest values are underlined. Asterisks denote that the results are adopted from literature.

as an input vector to MPDR, transforming the task into a 272D density estimation problem. We normalize a 272D vector with its norm and add a small white noise with a standard deviation of 0.01 during training. We use the maximum energy value among 14\(\)14 vectors as an anomaly score of an image. For the localization task, we resize 14\(\)14 anomaly score map to 224x224 image and compute AUROC for each pixel with respect to the ground true mask. We compare MPDR-R with UniAD  and DRAEM . The results are shown in Table 12. Details on MPDR implementation can be found in B.5.

MPDR achieves AUROC scores that are very close to that of UniAD, outperforming DRAEM. The trend is consistent in both detection and localization tasks. The performance gap between MPDR and UniAD can be attributed to the fact that UniAD leverages spatial information of 14\(\)14 feature map while our implementation of MPDR processes each pixel in the feature map separately.

### Acoustic Anomaly Detection

We apply MPDR to anomaly detection with acoustic signals, another form of non-image data. We use DCASE 2020 Challenge Task 2 dataset , containing recordings from six different machines, with three to four instances per machine type. The task is to detect anomalous sounds from deliberately damaged machines, which are unavailable during training. We applied the standard preprocessing methods in to obtain a 640-dimensional vector built up of consecutive mel-spectrogram features. Many challenge submissions exploit dataset-specific heuristics and ensembles for high performance, e.g., [42; 43]. Rather than competing, we focus on demonstrating MPDR's effectiveness in improving common approaches, such as autoencoders and Interpolation Deep Neural Networks (IDNN) . IDNN is a variant of masked autoencoders which predicts the middle (the third) frame given the remaining frames. Similarly to autoencoders, IDNN predicts an input as anomaly when the prediction error is large. We first train AE and IDNN for 100 epochs and then apply MPDR by treating the reconstruction (or prediction) error as the energy. Manifold ensemble consists of autoencoders with \(D_{}=32,64,128\). More training details can be found in the Appendix.

Table 5 shows that MPDR improves anomaly detection performance for both AE and IDNN. The known failure mode of AE and IDNN is producing unexpected low prediction (or reconstruction) error for anomalous inputs. By treating the error as the energy and applying generative training, the error in OOD region is increased through training, resulting in improved anomaly detection performance.

### Anomaly Detection on Tabular Data

We test MPDR on ADBench , which consists 47 tabular datasets for anomaly detection. We compare MPDR with 13 baselines from ADBench. The baseline results are reproduced using the official ADBench repository. We consider the setting where the training split does not contain anomalies. For each dataset, we run each algorithm on three random splits and computed the AUROC on the corresponding test split. We then ranked the algorithms based on the averaged AUROC. In Table A, we present a summary table with the average rank across the 47 datasets.

We employ MPDR-R with a single manifold. For both the manifold and the energy, the same MLP-based autoencoder architecture is used. The encoder and the decoder are MLPs with two 1024-hidden-neuron layers. If the input space dimensionality is smaller than or equal to 100, the latent space dimensionality is set to have the same dimensionality \(D_{}=D_{}\). If \(D_{}>100\), we set \(D_{}\) as 70% of \(D_{}\). We employ 1 step of Langevin Monte Carlo in the latent space and 5 steps

    & Toy Car & Toy Conveyor & Fan & Pump & Slider & Valve \\  AE  & 75.40 (62.03) & 77.38 (63.02) & 66.44 (53.40) & 71.42 (61.77) & 89.65 (74.69) & 72.52 (**52.02**) \\ MPDR-R & **81.54 (68.21)** & **78.61 (63.99)** & **71.72 (55.95)** & **78.27 (68.14)** & **90.91 (76.58)** & **75.23 (51.04)** \\  IDNN  & 76.15 (72.36) & 78.87 (62.50) & 72.74 (54.30) & 73.15 (61.25) & 90.83 (74.16) & 90.27 (69.46) \\ MPDR-IDNN & **78.53 (73.34)** & **79.54 (65.35)** & **73.27 (54.57)** & **76.58 (66.49)** & **91.56 (75.19)** & **91.10 (70.87)** \\   

Table 5: Acoustic anomaly detection on DCASE 2020 Track 2 Dataset. AUROC and pAUROC (in parentheses) are displayed per cent. pAUROC is defined as AUROC computed over a restricted false positive rate interval \([0,p]\), where we set \(p=0.1\), following .

in the input space. The step sizes are 0.1 for the latent space and 10 for the input space. All the hyperparameters except \(D_{}\) are fixed across 47 datasets.

As shown in Table 6, MPDR achieves highly competitive performance on ADBench, demonstrating a higher average rank than the isolation forest, with some overlap of confidence interval. This result indicates that MPDR is a general-purpose anomaly detection algorithm capable of handling tabular data. We empirically observe that MPDR is most effective when AUROC from its autoencoder is low, meaning that the outliers are near the autoencoder manifold. When the autoencoder already achieves good AUROC, the improvement from MPDR training is often marginal.

## 5 Related Work

Leveraging autoencoders in training, MPDR is closely related to other EBM training algorithms that incorporate auxiliary modules. While the use of a single variational autoencoder in EBM training is explored in , MPDR employs multiple autoencoders. MPDR is also compatible with non-variational autoencoders, offering greater flexibility. Normalizing flows  provide informative negative samples and a latent space for EBM training, but unlike MPDR, they do not exploit the low-dimensional structure of data. EBMs can also be trained with additional generator modules , which plays a similar role to the decoder of an autoencoder. A contrastive representation learning module  improves EBM performance but relies on domain-specific data augmentations and is only demonstrated on images. In contrast, MPDR is applicable to a wider range of data, utilizing a more general assumption of low-dimensionality in data.

MPDR presents a novel objective function for EBM by extending recovery likelihood framework . Investigating its connection to other previously studied objective functions, such as \(f\)-divergence , pseudo-spherical scoring rule , divergence triangle , and Stein discrepancy , would also be an interesting future direction.

MPDR contributes to the field of anomaly detection . The promising results from MPDR demonstrates that learning the distribution of data is a principled and effective approach for detecting anomalies .

## 6 Conclusion

ContributionsWe propose MPDR, a novel method of utilizing autoencoders for training EBM. An autoencoder in MPDR provides an informative starting point for MCMC, offers the latent space for the effective traversal of a high-dimensional space, and guides the drift of an MCMC sampler (Eq. 7). MPDR performs competitively on various anomaly detection benchmarks involving diverse types of data, contributing to the enhancement of generative modeling for anomaly detection with high-dimensional data.

LimitationsFirst, the practical performance of MPDR is still sensitive to the specifics of autoencoders used in MPD. Second, some data, such as high-resolution images or texts, may not exhibit a clear low-dimensional structure. In these cases, MPDR may require a separate representation learning stage, as demonstrated in our CIFAR-100 experiment with ViT (Sec. 4.3). Third, MPDR is not optimized for generating samples starting from a simple distribution, such as a Gaussian, while DRL is. We may resort to longer-chain MCMC to generate samples from MPDR.

  
**Method** & **Average Rank** \\  MPDR (ours) & \(\) \\ IForest & \(5.28 0.39\) \\ OCSVM & \(7.94 0.47\) \\ CBLOF & \(5.98 0.53\) \\ COF & \(9.23 0.58\) \\ COPOD & \(7.10 0.61\) \\ ECOD & \(6.97 0.58\) \\ HBOS & \(6.53 0.51\) \\ KNN & \(6.64 0.56\) \\ LOF & \(9.04 0.62\) \\ PCA & \(6.45 0.64\) \\ SOD & \(7.91 0.52\) \\ DeepSVDD & \(11.43 0.42\) \\ DAGMM & \(10.09 0.52\) \\   

Table 6: The rank of AUROC averaged over 47 datasets in ADBench. A smaller value indicates the algorithm achieves higher AUROC score compared to other algorithms on average. The standard errors are also displayed.