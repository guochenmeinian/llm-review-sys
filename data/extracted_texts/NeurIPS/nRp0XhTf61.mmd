# InternLM-XComposer2-4KHD: A Pioneering Large Vision-Language Model Handling Resolutions

from 336 Pixels to 4K HD

 Xiaoyi Dong\({}^{*}\)\({}^{1,2}\), Pan Zhang\({}^{*}\)\({}^{1}\), Yuhang Zang\({}^{*}\)\({}^{1}\), Yuhang Cao\({}^{1,2}\), Bin Wang\({}^{1}\), Linke Ouyang\({}^{1}\), Songyang Zhang\({}^{1}\), Haodong Duan\({}^{1}\), Wenwei Zhang\({}^{1}\), Yining Li\({}^{1}\), Hang Yan\({}^{1}\), Yang Gao\({}^{1}\), Zhe Chen\({}^{1}\)

Xinyue Zhang\({}^{1}\), Wei Li\({}^{1}\), Jingwen Li\({}^{1}\), Wenhai Wang\({}^{1,2}\), Kai Chen\({}^{1}\), Conghui He\({}^{3}\), Xingcheng Zhang\({}^{3}\), Jifeng Dai\({}^{4,1}\), Yu Qiao\({}^{1}\), Dahua Lin\({}^{1,2,5}\), Jiaqi Wang\({}^{1,80}\)

\({}^{1}\)Shanghai Artificial Intelligence Laboratory, \({}^{2}\)The Chinese University of Hong Kong,

\({}^{3}\)SenseTime Group, \({}^{4}\)Tsinghua University, \({}^{5}\)CPII under InnoHK

internlm@pjlab.org.cn

###### Abstract

The Large Vision-Language Model (LVLM) field has seen significant advancements, yet its progression has been hindered by challenges in comprehending fine-grained visual content due to limited resolution. Recent efforts have aimed to enhance the high-resolution understanding capabilities of LVLMs, yet they remain capped at approximately 1500 \(\) 1500 pixels and constrained to a relatively narrow resolution range. This paper represents InternLM-XComposer2-4KHD, a groundbreaking exploration into elevating LVLM resolution capabilities up to 4K HD (3840 \(\) 1600) and beyond. Concurrently, considering the ultra-high resolution may not be necessary in all scenarios, it supports a wide range of diverse resolutions from 336 pixels to 4K standard, significantly broadening its scope of applicability. Specifically, this research advances the patch division paradigm by introducing a novel extension: dynamic resolution with automatic patch configuration. It maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pre-trained Vision Transformer (ViT) (336 \(\) 336), leading to dynamic training resolution from 336 pixels to 4K standard. Our research demonstrates that scaling training resolution up to 4K HD leads to consistent performance enhancements without hitting the ceiling of potential improvements. InternLM-XComposer2-4KHD shows superb capability that matches or even surpasses GPT-4V and Gemini Pro in 10 of the 16 benchmarks. The InternLM-XComposer2-4KHD model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.

## 1 Introduction

In recent years, the progress in Large Language Models (LLMs) [(73; 92; 93; 39; 91; 10; 78; 29; 21)] has provoked the development of Large Vision-Language Models (LVLMs). These models have demonstrated proficiency in tasks such as image captioning [(17; 14)] and visual-question-answering (VQA) [(57; 31; 33; 107)]. Nevertheless, due to their limited resolution, they struggle with processing images containing fine details, such as charts [(68)], tables [(87)], documents [(70)], and infographics [(69)]. This limitation constrains their practical applicability in real-world scenarios.

Recent advancements have aimed at enhancing the resolution of Large Vision-Language Models (LVLMs). Some approaches [(66; 36; 97; 48)] involve adapting high-resolution vision encodersdirectly. However, the Vision Transformer (ViT) architecture falls short when dealing with images of varying resolutions and aspect ratios, thereby restricting its ability to handle diverse inputs effectively. Alternatively, some methods [(50; 59; 37; 51; 99; 55; 46) maintain the vision encoder's resolution, segmenting high-resolution images into multiple low-resolution patches. Yet, these methods are constrained by an inadequate resolution, typically around 1500 \(\) 1500, which does not satisfy the demands of daily content, _e.g._, website screenshots [(85)], document pages [(70)], and blueprints [(69)]. Furthermore, they are confined to either a few predefined high-resolution settings [(36; 97; 48; 50; 51; 55; 46; 66; 59)] or a limited range of resolutions [(101; 37; 99)], thereby restricting their utility across a variety of applications.

In this work, we introduce InterLM-XComposer2-4KHD, a pioneering model that for the first time expands the resolution capabilities of Large Vision-Language Models (LVLMs) to 4K HD and even higher, thereby setting a new standard in high-resolution vision-language understanding. Designed to handle a broad range of resolutions, InterLM-XComposer2-4KHD supports images with any aspect ratio from 336 pixels up to 4K HD, facilitating its deployment in real-world contexts.

InterLM-XComposer2-4KHD follows patch division [(50; 46)] paradigm and enhances it by incorporating an innovative extension: dynamic resolution with automatic patch configuration. To be specific, scaling the resolution of Large Vision-Language Models (LVLMs) to 4K HD and even higher standard is far beyond merely increasing the number of patches. It involves a nuanced approach to overcoming specific challenges: (1) **Dynamic Resolution and Automatic Patch Configuration**: Addressing the scarcity of high-resolution training data, our framework introduces a strategy that dynamically adjusts resolution alongside an automatic layout configuration. During training, it maintains the original aspect ratios of images while adaptively altering patch (336 \(\) 336) layouts and counts. This results in a training resolution that exceeds the original image resolutions, reaching up to 4KHD, addressing the shortfall of high-resolution data. (2) **Handling Variability in Patch Layouts**: Despite the apparent simplicity of dynamic resolution training, the variability in patch layouts can heavily confuse LVLMs. To mitigate this, we introduce a newline token after each row of patch tokens to clearly delineate patch layouts, reducing training ambiguity and significantly boosting performance. (3) **Inference Beyond 4K Resolution:** Our observations reveal that, even when trained on images up to 4K resolution, the model can achieve additional performance improvements during inference by processing images at higher resolutions.

Figure 1: **(a) Overview of InternLM-XComposer2-4KHD (IXC-4KHD) performance on benchmarks with different resolutions. Our model based on InternLM-27B [(91)]_matches or even surpasses GPT-4V (74) and Gemini Pro (90) in 10 of the 16 benchmarks._ (b) **Image resolution statistic of 16 benchmarks.** We report the minimum (Min), median, and maximum (Max) image area (resolution). Both the inter-/intra-benchmark resolution diversity are large, and we sort them by the maximum resolution.**

The image is a vibrant and colorful ingoriboric that provides an overview of 7 graphic design trends that are expected to dominate in 2021. The infographic is divided into 7 sections, each representing a different trend.

At the top, there's a section titled "Munted Color Palettes", which introduces the concept of muted colors as a trend for 2021.

This is followed by a section on "Simple Data Visualizations", highlighting the goal of making complex data easy to understand.

The third section focuses on "Geometric Shapes Everywhere", showcasing how designers are using geometric shapes in their designs.

The fourth section delves into "Flat Loss and Illustrations", explaining how brands are using flat icons and illustrations in their social media graphics, website design, and more.

The fifth section introduces "Classic Serif Fonts", discussing how these fonts are seen as classic, elegant, and trustworthy.

The sixth section explores "Social Media Slide Decks", illustrating how slide decks have become a visual way of communicating longer messages on Intagram and LinkedIn.

Finally, the seventh section discusses "Test Heer Valley Videos", predicting that people will likely continue to work remotely well into 2021, leading to an increase in video content that uses text on-screen to communicate messages.

Overall, the infographic provides a comprehensive overview of the 7 graphic design trends that are expected to shape the design landscape in 2021.

Furthermore, scaling the training resolution up to 4K standard results in a consistent improvement in performance, highlighting the potential for training even beyond 4K resolution. This underscores the capacity for further enhancing model capabilities and suggests a promising trajectory for advancing the frontiers of high-resolution image processing within the domain of large vision-language models.

We evaluate our InternLM-XComposer2-4KHD on 16 diverse benchmarks spanning various domains, including 5 challenging OCR datasets (InfographicVQA(69), DocVQA(70), OCRBench(58), TextVQA(87), and ChartQA(68)). Compared to previous open-source LVLM models and closed-source APIs, our approach achieves SOTA results in 6 of 16 benchmarks, demonstrating competitive performance despite only 7B parameters. As shown in Figure 1, InternLM-XComposer2-4KHD even surpasses the performance of GPT4V (74) and Gemini Pro (90) across ten benchmarks. Notably, our method exhibits excellent performance on 5 challenging OCR datasets, over existing open-source LVLMs by a substantial margin.

Figure 2: **Chat with InternLM-XComposer2-4KHD**. Some regions of the input HD images are zoomed in for better visualization. For more results please refer to the Supplementary materials.

Related Works

**Large Vision-Language Models (LVLMs).** Large Language Models (LLMs) [(9; 76; 73; 23; 41; 92; 93; 39; 91; 108; 6; 78; 10)] have gained significant attention due to their impressive performance in various language-related tasks such as text generation and question answering. Following this enthusiasm, recent Large Vision-Language Models (LVLMs) have emerged[(74; 19; 16; 18; 28; 32; 113; 25; 110; 7; 47; 77; 102; 4), combining LLMs with vision encoders [(79; 109; 89)] to leverage the complementary strengths of language and vision modalities. By fusing textual and visual representations, LVLMs can ground language in visual contexts, enabling a more comprehensive understanding and generation of multimodal content [(14; 20; 51; 5; 95; 27; 11; 60)].

**LVLMs for High-Resolution Understanding.** Large Vision-Language Models (LVLMs) often employ CLIP-ViT as the visual encoder for vision-dependent tasks. However, the visual encoder's reliance on low resolutions, such as 224 \(\) 224 or 336 \(\) 336 pixels, limits its effectiveness for high-resolution tasks like OCR and document/chart perception. To enhance high-resolution understanding, recent works have primarily employed the following strategies: (1) High-resolution (HR) visual encoders or dual encoders catering to high-resolution (HR) and low-resolution (LR) inputs [(66; 97; 36; 48)]. For instance, Vary [(97)] introduces a new image encoder supporting HR inputs, which are then concatenated with LR embeddings from the original CLIP visual encoder. Similarly, CogAgent [(36)] and Mini-Gemini [(48)] also separate HR and LR images using distinct vision encoders, subsequently merging their features using a cross-attention module. In contrast, our approach offers a more simplified solution and shows advantages for varying resolutions and aspect ratio inputs. (2) Cropped image patches [(50; 59; 99; 101; 37; 51; 46)]. For example, Monkey [(50)] employs sliding windows to segment images into patches, subsequently processing them with LoRA fine-tuning. TextMonkey [(59)] further proposes shifted window attention and token resampler to consider the connections among different patches. Fuyu [(7)] eliminates the need for the image encoder by directly processing a raw image patch sequence. These approaches are confined to either a few predefined high-resolution settings [(36; 97; 48; 50; 51; 55; 46; 66; 59)] or a limited range of resolutions [(37; 99)]. Conversely, our method devises a dynamic resolution and automatic path configuration strategy to support the scaling from 336 pixels to 4K resolution, and the maximum resolution is larger than previous approaches (_e.g._, 1.5k for Monkey [(50)] and 1.2k for UReader [(101)]). For the first time, our approach discussed the challenges and solutions for handling variability in image feature patch layouts, ensuring effective training with dynamic high resolutions.

**LVLMs for Document Understanding.** Document understanding involves analyzing and comprehending various digital documents, such as figures, tables, and academic papers. Many document understanding tasks require models to handle high-resolution inputs, complex layouts, various aspect ratios, and diverse document formats. To enhance the capabilities of LVLMs for document understanding, several works have collected and constructed high-quality document instruction tuning data, including LLaVAR [(112)], mPLUG-DocOwl [(100)] and TGDoc [(96)]. DocPediaDocPedia [(30)] processes document inputs in the frequency domain. Some previous works have improved document understanding ability by designing special modules for high-resolution inputs, such as HR and LR encoders [(36; 97)] or cropped image patches [(101; 59; 99)]. Our InternLM-XComposer2-4KHD first scales to 4K resolution inputs and demonstrates strong document understanding ability on OCR-related benchmarks. Also, our approach also achieves comparable results to state-of-the-art open-sourced LVLMs on other general LVLM benchmarks like perception and reasoning [(61; 57; 33; 15)].

## 3 Method

### Model Architecture.

The model architecture of InternLM-XComposer2-4KHD mainly follows the design of InternLM-XComposer2(27) (XComposer2 / IXC2 in the following for simplicity ), including a light-weight Vision Encoder OpenAI ViT-Large/14, Large Language Model InternLM2-7B, and Partial LoRA for efficient alignment.

### High-Resolution Input.

**Dynamic Patch Configuration.** Utilizing a static input image size for processing high-resolution images, particularly those with varying aspect ratios, is neither efficient nor effective. To overcome this limitation, we introduce a dynamic patch configuration approach via image partitioning, as shown in Figure 3. Our method strategically segments the image into smaller patches, while maintaining the integrity of the original image's aspect ratio.

Given a maximum patch number \(\), the image \(x\) with size \([h,w]\) is resized and padded to the new image \(\) with size \([p_{h} 336,p_{w} 336]\). This process is subject to the following constraints:

\[p_{w} p_{h};\ p_{h}= p_{w} h/w\] (1)

here \(p_{w}\) and \(p_{h}\) represent the number of patches in each row and column, respectively. We then split the \(\) into \(p_{h} p_{w}\) non-overlapped patches. Each patch is a small image with \(336 336\) size and we treat these patches as individual inputs for the ViT.

In the following, we use 'HD-\(\)' to represent our high-resolution setting with the constraint of \(\) patches. For example, the 'HD-9' allows up to 9 patches, including a range of resolutions such as \(1008 1008\), \(672 1344\), \(336 3024\), _etc_.

**Global-Local Format.** For each input image, we present it to the model with two views. The first is the global view, where the image is resized to a fixed size (in our case, \(336 336\)). This provides a macro understanding of the image. Empirically, we have found this to be crucial for the LVLM to correctly understand the image. The second view is the local view. We divide the image into patches using the previously mentioned Dynamic Image Partition strategy and extract features from each patch. Following feature extraction, the patches are reassembled into a large feature map. The feature map is then flattened to the final local features after a straightforward token merging process.

**Patch Layout Indicator.** Given that an image will have a dynamic patch layout in our method, the number of tokens for each row can vary across different images. This variation will confuse the LVLM, making it difficult to determine which tokens belong to the same row of the image and which ones belong to the next row. This confusion hinders the LVLM's ability to understand the 2D structure of the image, which is crucial for comprehending structural image content such as documents, charts, and tables. To address this issue, we introduce a learnable newline ('\(\)n') token at the end of each row of the image features before the flattening. Finally, we concatenate the global and local views, inserting a special separate ('sep') token between them to distinguish the two views.

### Pre-Training

During the pre-training phase, the LLM is frozen while both the vision encoder and Partial LoRA are fine-tuned to align the visual tokens with the LLM following XComposer2(27). The pre-training data

Figure 3: The framework of InternLM-XComposer2-4KHD. Our model processes the high-resolution image with a Dynamic Image Partition strategy and concatenates the image tokens with text tokens as LLM input.

mainly follow the design in XComposer2 which is curated with **three objectives** in mind: 1) general semantic alignment, 2) world knowledge alignment, 3) vision capability enhancement. In this paper, we focus on high-resolution and structural image understanding. So we collected OCR and chart data from diverse sources to enhance this specific capability, as shown in Table.1.

In practice, we employ the OpenAI CLIP ViT-L-14-336 as the vision encoder. We keep the ViT resolution as \(336 336\) while adopting Dynamic Patch Configuration to handle higher-resolution images. For pretraining, we use the 'HD-25' configuration, which resizes the input image to a random larger resolution to generate more patches, with the constraint that the total number of patches does not exceed 25. For each image or patch tokens, the image token number is decreased to \(1/4\) with a simple **merge operation**. We concatenate the nearby 4 tokens into a new token through the channel dimension, then align it with the LLM by an MLP. The'sep' and '\(\)n' tokens are randomly initialized. For the Partial LoRA, we set a rank of \(256\) for all the linear layers in the LLM decoder block. Our training process involves a batch size of 4096 and spans across 2 epochs. The learning rate linearly increases to \(2 10^{-4}\) within the first \(1\%\) of the training steps. Following this, it decreases to \(0\) according to a cosine decay strategy. To preserve the pre-existing knowledge of the vision encoder, we apply a layer-wise learning rate (LLDR) decay strategy, and the decay factor is set to \(0.90\).

### 4KHD Supervised Fine-tuning (SFT)

After pre-training, we empower the model to understand high-resolution images and solve diverse challenges. Different from conventional perception tasks (_e.g._, VQAv2, GQA) which typically answer questions based on the noticeable object in the image. OCR-related tasks depend on a detailed understanding of text within a high-resolution image. For instance, in InfoVQA, the length of the longer side of 50% of the images exceeds 2000 pixels. Low-resolution inputs can distort the dense text information, causing the model to fail in its understanding. However, we have observed a resolution saturation phenomena with perception tasks, where a higher resolution makes minor gains.

To address this, we introduce a mixed-resolution strategy for more efficient training. For tasks requiring high resolution, we employ the 'HD-55' setting during training. This allows for the input of 4K (\(3840 1600\)) images without necessitating additional image compression. These tasks are referred to as the HD-OCR QA tasks in Table 2. For the other tasks, we apply the 'HD-25' resolution setting for them. As in pre-training, we adopt the dynamic-resolution strategy during SFT, images are resized to fall within a range between their original size and the size specified by the 'HD' setting. This dynamic approach enhances the robustness of the LVLM against differences in input resolution, thereby enabling the LVLM to utilize a larger resolution during inference. For instance, we have observed that using the 'HD30' setting yields better results on most OCR-related tasks when the LVLM is trained under the 'HD25' setting.

In practice, we jointly train all the components with a batch size of 2048 over 3500 steps. Data from multiple sources are sampled in a weighted manner, with the weights based on the number of data from each sourced (See Appendix B.1 for more details). As the 'HD-55' setting has double image tokens than the 'HD-25', we adjust the data loader to enable different batch sizes for them and adjust their weight accordingly. The maximum learning rate is set to \(5 10^{-5}\), and each component has its

   Task & Dataset \\  General Semantic Alignment & ShareGPT4V-PT (14), COCO (17), Nocaps (1), TextCaps (86), SBU (75), LAION400M (80), CC 3M (83) \\ World Knowledge Alignment & Concept Data (110) \\ Vision Capability Enhancement & WanJuan (35), Flicker(103), MMC-Inst(54), RCTW-17(84), CTW(106), LSVT(88), ReCTs(111), ArT(22) \\   

Table 1: **Pre-Training Datasets**. The data are collected from diverse sources for the three objectives.

   Task & Resolution Setting & Dataset \\  Caption & HD-25 & ShareGPT4V (14), COCO (17),Nocaps (1) \\ General QA & HD-25 & VQAv2 (3), GQA (38), OK-VQA (67), VD (26), RD(13), VSR(53), \\ Science QA & HD-25 & A2ID (42), SQA (63), TQA(43), LeonQA(65) \\ Chart QA & HD-25 & DVQA (40), ChartQA, ChartQA-AIDG (68) \\ Math QA & HD-25 & MathQA (104), Geometry3KJG, ZhouW(64), CLEVR-MATH(52)/Super(49) \\ World Knowledge QA & HD-25 & A-OKVQA (81),KYQA (82), VIQAuE(44) \\ OCR QA & HD-25 & TextQA(87), OCR-VQA(72), ST-YQA(8) \\  HD-OCR QA & HD-55 & InfoVQA(69), DocVQA(70) \\ Conversation & – & LLVA-150K (56), LVIS-Instruct4V (94), ShareGPT-en\&zh (21), InternLM-Mat(91) \\   

Table 2: **Supervised Fine-Tuning Datasets**. We collect data from diverse sources to empower the model with different capabilities. The image resolution is also different for different tasks.

own unique learning strategy. For the vision encoder, we set the LLDR to \(0.9\), which aligns with the pretraining strategy. For the LLM, we employ a fixed learning rate scale factor of \(0.2\). This slows down the update of the LLM, achieving a balance between preserving its original capabilities and aligning it with vision knowledge. It takes almost 40 hours with 256 A100 GPUs.

## 4 Experiments

In this section, we validate the benchmark performance of our InternLM-XComposer2-4KHD (IXC2-4KHD in the following for simplicity) after supervised fine-tuning.

### LVLM Benchmark results.

In Table 3 and Table 4, we compare our IXC2-4KHD on a list of benchmarks with both SOTA open-source LVLMs and closed-source APIs. Here we report results in DocVQA(70), ChartQA(68), InfographicVQA(69), TextVQA(87), OCRBench(58), MMStar(15), MathVista(61), MMMU(107), AI2D(42), MME (31), MMBench (MMB) (57), MMBench-Chinese (MMB\({}^{CN}\)) (57), SEED-Bench Image Part (SEED\({}^{I}\))(45), QBench-Testset (QBench\({}^{T}\))(98), MM-Vet (105), HallusionBench (HallB)(34). The evaluation is mainly conducted on the OpenCompass VLMEvalKit(24) for the unified reproduction of the results.

**Comparison with Closed-Source APIs.** As demonstrated in Table 3, IXC2-4KHD exhibits competitive performance across a variety of benchmarks, rivaling that of Closed-Source APIs. Owing to its high-resolution input, IXC2-4KHD achieves a score of \(90.0\%\) on DocVQA and \(81.0\%\) on ChartQA, thereby surpassing GPT-4V and Gemini-Pro with a non-trivial margin. In the challenging InfographicVQA task, our model is the first open-source model that is close to the performance of Closed-Source APIs, exceeding the performance of previous open-source models by nearly \(20\%\). In addition to OCR-related tasks, IXC2-4KHD is a general-purpose Large Vision-Language Modal that excels in semantic-level tasks, demonstrating competitive results.

**Comparison with Open-Source Models.** We also conduct a comprehensive comparison with open-source LVLMs under a similar model scale. As shown in Table 4, our model significantly outperforms existing open-source models, achieving competitive results across all benchmarks.

**High-resolution Understanding Evaluation.** Then we compare IXC2-4KHD with models that are specifically designed for high-resolution understanding tasks. We report the results of 5 high-resolution benchmarks in Table 5, as a general LVLM, IXC2-4KHD shows superb performance on

   Method & LLM & MMStar & MathVista & AI2D & MME\({}^{P}\) & MME\({}^{C}\) & MMB & MMB\({}^{CN}\) & SEED\({}^{I}\) & QBench\({}^{T}\) & MM-Vet \\  Mowen-VL-Chat & Open-78 & 37.5 & 33.8 & 63.0 & 1,487.5 & 360.7 & 60.6 & 56.7 & 58.2 & 61.7 & 47.3 \\ Shared GPT4V & Vicuna-7B & 33.0 & 25.8 & 58.0 & 1,567.4 & 376.4 & 68.8 & 62.2 & 69.7 & - & 37.6 \\ Monkey & Open-78 & 38.3 & 34.8 & 62.5 & 1,522.4 & 401.4 & 72.4 & 67.5 & 68.9 & - & 33.0 \\ CoeyLM-17B & Vicuna-7B & 36.5 & 34.7 & 63.3 & - & - & 65.8 & 55.9 & 68.8 & - & 54.5 \\ LiLAVA-XTuner & InernLM2-20B & - & 24.6 & 65.4 & - & - & 75.1 & 73.7 & 70.2 & - & 37.2 \\ LiLAV-1.5 & Vicuna-13B & 32.8 & 26.1 & 61.1 & 1,531.3 & 295.4 & 67.7 & 63.6 & 68.2 & 61.4 & 35.4 \\ LiLAVA-Next & Vicuna-13B & 38.3 & 32.4 & 72.2 & 1,445.0 & 296.0 & 70.0 & 68.5 & 71.4 & - & 44.9 \\ InterLM-XC (27) & InernLM-7B & - & 29.5 & 56.9 & 1,528.4 & 391.1 & 74.4 & 72.4 & 66.1 & 64.4 & 35.2 \\  IXC2-VL & InernLM2-7B & **55.4** & 57.6 & **81.2** & **1,712.0** & 530.7 & **80.7** & **79.4** & **74.9** & **72.5** & 46.7 \\ IXC2-4KHD & InernLM2-7B & 54.1 & **57.8** & 80.9 & 1,655.9 & **548.9** & 80.2 & 77.7 & 74.7 & 71.8 & **54.9** \\   

Table 4: **Comparison with open-source SOTA methods. IXC2-4KHD outperforms competitors in most benchmarks. The best results are bold** and the second-best results are underlined.

   Method & Doc & Chart & Info & Text & OCR & MM & Math & A2D & MMMU & MMA & MSB & SEED & QBench & MM- & Hall \\  & VQA & QA & VQA & VQA & Bench & Star & Vista & & & & & & & & & & & \\  Open-Source & (37) & (37) & (37) & (36) & (36) & (55) & (55) & (20) & (2) & (55) & (55) & (110) & (95) & (50) \\ Previous SOTA & 8B & 8B & 8B & 18B & 18B & 35B & 35B & 35B & 40B & 34B & 35B & 35B & 35B & 8B & 17B & 10B \\  & 82.2 & 70.2 & 44.5 & 76.1 & 59.0 & 52.1 & 39.0 & 78.9 & 51.6 & 2050.2 & **81.1** & 79.0 & **75.7** & 64.4 & 54.5 & 39.3 \\   \\ GPT-4V & 88.4 & 78.5 & 75.1 & **78.0** & 51.6 & **57.1** & 47.8 & 75.5 & **56.8** & 1,926.5 & 77.0 & 74.4 & 69.1 & **74.1** & 56.8 & **46.5** \\ Gemini-Pro & 88.1 & 74.1 & **75.2** & 74.6 & **68.0** & 42.6 & 45.8 & 70.2 & 47.9 & 1,933.3 & 73.6 & 74.3 & 70.7 & 70.6 & **59.2** & 45.2 \\  IXC2-VL (27) & 57.7 & 72.6 & 34.4 & 70.1 & 53.2 & 55.4 & 57.6 & 82.1 & 41.4 & **2,220.4** & 80.7 & **79.4** & 79.2 & 72.5 & 46.7 & 41.0 \\ IXC2-4KHD & **90.0** & **81.0** & 68.6 & 72.2 & 67.5 & 54.1 & **57.8** & 80.9 & 39.7 & 204.9 & 80.2 & 77.7 & 74.7 & 71.8 & 54.9 & 40.9 \\   

Table 3: **Comparison with closed-source APIs and previous open-source SOTAs. Our InternLM-XComposer2-4KHD gets SOTA results in 6 of the 16 benchmarks with only 8B parameters, showing competitive results with current closed-source APIs. The best results are bold and the second-best results are underlined.**

[MISSING_PAGE_FAIL:8]

the resolution is increased. Additionally, similar to the observation from Figure 4, the impact of resolution on perception-related benchmarks appears to be quite minor.

### High-Resolution Strategy Ablation

**The Role of Global-View.** We first examine the impact of the global view in our Global-Local Format. As indicated in Table 8(a), we find that the global view is essential for the LVLM to accurately comprehend the input image. When it is removed, the model performs worse across all benchmarks. For instance, the model experiences a \(-4.4\%\) drop in performance on the MMBench EN-Test without the global view. We contend that the global view offers a general macro understanding of the image, which the model struggled to derive from the large number of tokens in the local view.

**The Role of the Newline Token.** We incorporate a special newline token at the end of each row of the image features before the flattening operation. This token serves as an indicator of the image's 2D structure. We examine its impact on both the HD-9 and 4KHD strategies in Table 7(a). When a fixed high-resolution strategy HD-9 is employed, we observe that the benefit derived from the newline token is minor. This could be attributed to the LVLM's ability to handle limited differences in image ratios after training. However, when we implement a more challenging 4KHD (HD-25 + HD-55) strategy, which exhibits significant diversity in both image ratio and token number, the LVLM demonstrates a notable decline in performance on OCR-related tasks without the newline indicator. This finding supports our hypothesis that the LVLM struggles to comprehend the shape of the image when the image tokens are directly flattened into a 1D sequence. The newline token can assist the model in better understanding the structure of the image.

**Influence of Token Merging Operation.** In practice, we employ a simple merging operation that concatenates four adjacent tokens along the channel dimension. We have found this approach to be effective in reducing the number of image tokens efficiently. Here we study the influence of different token-merging operations under the 4KHD setting. In Table 7(b), we study two additional strategies: Re-Sampler(5) and C-Abstractor(12), with their default setting and the same compressing rate \(0.25\), _i.e._, reducing an image with 576 tokens to 144 tokens. Results show that both concatenation and C-Abstractor work well and get similar results on most benchmarks, this observation is also consistent with the study in MM-1(71) that the influence of the connector is minor. However, the Re-Sampler performs worse than the other methods with a noticeable margin. We argue this is caused by the learnable queries used for gathering information requiring a great number of data for training, our pre-training data is somewhat lightweight for it to converge fully.

   Model & \(\)n & Doc & Info & Text & Chart & MMB & MME & SEED\({}^{*}\) & Strategy & Doc Info & Text & Chart & MMB & MME & SEED\({}^{*}\) \\  HD9 & \(\) & 79.5 & 50.3 & 74.0 & 78.2 & 79.1 & 2206 & 75.9 & Re-Sampler & 86.2 & 67.1 & 75.3 & 78.8 & 79.6 & 2124 & 74.2 \\ HD9 & \(\) & 79.4 & 50.5 & 73.8 & 78.2 & 79.5 & 2201 & **76.6** & C-Abstractor & 88.6 & 69.5 & 77.1 & 80.6 & 80.4 & 2236 & 76.7 \\ 
4KHD & \(\) & 88.1 & 67.4 & 75.9 & 80.4 & 79.9 & **2232** & 76.4 & Concat & 89.0 & 69.3 & 77.2 & 81.0 & 80.2 & 2205 & 76.2 \\   

Table 7: **(a) Influence of Indicator \(\)n’ in the Image Features. \(\)n’ helps LVLM understand structural images when the input resolution is dynamic and large. (b) Ablation on Token Merging Operation. Both the simple concatenation operation and the C-Abstractor works well.**

   Model Nmae & HD Strategy & Image Tokens & Max Resolution & DocVQA & TextVQA & ChartQA & MMBench \\  LLaVA-Next & LLaVA-Next & 2880 & 672x672 & 78.2 & 52.0 & 69.5 & 72.1 \\  IXC-1LaVA & LLaVA-Next & 2880 & 672x672 & 78.9 & 71.5 & 74.1 & 77.6 \\  IXC-4KHD & HD9 & 1440 & 1008x1008 & 79.4 & 73.8 & 78.2 & 79.5 \\  IXC-4KHD & HD16 & 2448 & 1344x1344 & 84.9 & 75.7 & 80.1 & 80.2 \\   

Table 8: **Influence of Global-View in the Input. Global-view is critical for most benchmarks.**

   Model & Doc & Info & Text & Chart & MMB & MME & SEED\({}^{*}\) \\  HD9 & 79.4 & 50.5 & 73.8 & 78.2 & 79.5 & 2201 & 76.6 \\ + w/o global-view & 78.1 & 47.9 & 71.2 & 77.9 & 75.1 & 2019 & 76.2 \\   

Table 9: **Strategy-Level comparison between LLaVA-Next and our IXC2-4KHD. Our strategy reaches better performance under a similar image token number constrain.**

**Strategy-Level Comparison.** For a fair comparison, we trained a new model with the identical architecture, training strategy, and dataset as our original model, but with one key modification: we adopted the high-resolution strategy from LLaVA-Next. We name it as IXC-LLaVA and compare it with 1) LLaVA-Next 0530 official results and 2) IXC2-4KHD under HD-9/16 setting. The results in Table.9 demonstrate that IXC-LLaVA achieves promising performance across all six benchmarks, leveraging the benefits of additional training data and advanced IXC architecture design. However, it is outperformed by IXC2-4KHD HD-9, which utilizes fewer image tokens yet yields better results. This fair comparison underscores the efficiency and effectiveness of our proposed high-resolution strategy, highlighting its advantages over the LLaVA-Next approach.

**Inference Efficiency Analysis.** Our model processes high-resolution images with numerous image tokens, and here we study its inference efficiency in real-world usages. The model inference process consists of two stages: encoding the prefix (model input) and autoregressively decoding new tokens (model output). Correspondingly, the inference efficiency considers two parts: time to encode the prefix and speed to decode each token. Here we report the prefix encoding time and per-token decoding speed under different HD settings. We test the speed with a Nvidia-A100 80G. With the results in Table 10, we have three observations: 1) Prefix encoding time increases linearly with the number of prefix tokens. 2) Decoding speed remains relatively constant, regardless of prefix length, thanks to optimizations on transformers from research communities and companies, including kv-cache and flash-attention. 3) When generating 2048 tokens, total inference time usage is nearly identical across HD9 to HD55, as encoding time is much smaller. Based on the above analysis, we believe the inference efficiency of our model is acceptable. Besides, we believe some targeted designs can further improve efficiency, while our paper focuses on enabling LVLM to understand high-resolution images with a general and effective solution, and we would leave the efficiency exploration in future work.

## 5 Conclusion

In this paper, we propose the InternLM-Xcomposer2-4KHD that exceeds the performance of previous open-source models on OCR-related tasks and also achieves competitive results on general-purpose LVLM benchmarks. Thanks to our dynamic resolution and automatic patch configuration, our model supports a maximum training resolution of up to 4K HD. We also integrate a global view patch to support the macro understanding and a learnable newline token to handle the various input image resolutions. Our model's performance continues to improve as the training resolution increases for HD-OCR tasks. Notably, we do not observe any performance saturation even for the 4KHD setting, and we have not explored the upper bound due to the computational burden increasing with higher-resolution inputs. In future work, we plan to explore efficient solutions for accurate LVLM training and inference, enabling our model to handle even higher resolutions while maintaining computational efficiency.

## 6 Acknowledgment

This project is funded in part by Shanghai Artificial Intelligence Laboratory, the National Key R&D Program of China (2022ZD0160201), the Centre for Perceptual and Interactive Intelligence (CPII) Ltd under the Innovation and Technology Commission (ITC)'s InnoHK. Dahua Lin is a PI of CPII under the InnoHK.

   HD & image tokens & prefix encoding time & per-token decoding speed & time to generation 2048 new tokens \\  HD9 & 1440 & 0.2845 & 0.0982 & 201.4 \\ HD16 & 2448 & 0.3966 & 0.0983 & 201.7 \\ HD25 & 3744 & 0.5513 & 0.0981 & 201.5 \\   

Table 10: **Inference Efficiency Analysis.** The image token number mainly inference the prefix speed, and their difference in the decoding part is neglectable.