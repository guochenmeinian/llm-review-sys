# Direct Training of SNN using Local Zeroth Order Method

Bhaskar Mukhoty\({}^{1,}\)1

&Velibor Bojkovic\({}^{1,}\)1

&William de Vazelhes\({}^{1}\)&Xiaohan Zhao\({}^{3}\)

&Giulia De Masi\({}^{2,6}\)&Huan Xiong\({}^{1,5,}\)2

&Bin Gu\({}^{1,4,}\)3

\({}^{1}\) Mohamed bin Zayed University of Artificial Intelligence, UAE

\({}^{2}\) ARRC, Technology Innovation Institute, UAE

\({}^{2}\) Nanjing University of Information Science and Technology, China

\({}^{4}\) School of Artificial Intelligence, Jilin University, China

\({}^{5}\) Harbin Institute of Technology, China

\({}^{6}\) BioRobotics Institute, Sant'Anna School of Advanced Studies, Pisa, Italy

###### Abstract

Spiking neural networks are becoming increasingly popular for their low energy requirement in real-world tasks with accuracy comparable to traditional ANNs. SNN training algorithms face the loss of gradient information and non-differentiability due to the Heaviside function in minimizing the model loss over model parameters. To circumvent this problem, the surrogate method employs a differentiable approximation of the Heaviside function in the backward pass, while the forward pass continues to use the Heaviside as the spiking function. We propose to use the zeroth-order technique at the local or neuron level in training SNNs, motivated by its regularizing and potential energy-efficient effects and establish a theoretical connection between it and the existing surrogate methods. We perform experimental validation of the technique on standard static datasets (CIFAR-10, CIFAR-100, ImageNet-100) and neuromorphic datasets (DVS-CIFAR-10, DVS-Gesture, N-Caltech-101, NCARS) and obtain results that offer improvement over the state-of-the-art results. The proposed method also lends itself to efficient implementations of the back-propagation method, which could provide 3-4 times overall speedup in training the. The code is available at https://github.com/BhaskarMukhoty/LocalZ0.

## 1 Introduction

Biological neural networks are known to be significantly more energy efficient than their artificial avatars - the artificial neural networks (ANN). Unlike ANNs, biological neurons use spike trains to communicate and process information asynchronously.  To closely emulate biological neurons, spiking neural networks (SNN) use binary activation to send information to the neighbouring neurons when the membrane potential exceeds membrane threshold. The event-driven binary activation simplifies the accumulation of input potential and reduces the computation burden when the spikes are sparse. Specialized neuromorphic hardware  is designed to carry out such event-driven and sparse computations in an energy-efficient way .

There are broadly three categories of training SNNs: ANN-to-SNN conversion, unsupervised and supervised. The first one is based on the principle that parameters for SNN can be inferred from the corresponding ANN architecture [6; 15; 5]. Although training SNNs through this method achieves performance comparable to ANNs, it suffers from the long latency needed in SNNs to emulate the corresponding ANN or from retraining of ANNs required to achieve near lossless conversion . The unsupervised training is biologically inspired and uses local learning to adjust the SNN parameters . Although it is the most energy-efficient one among the three as it is implementable on neuromorphic chips , it still lags in its performance compared to ANN-to-SNN conversion and supervised training.

Finally, supervised training is a method of direct training of SNNs by using back-propagation (through time). As such, it faces two main challenges. The first is due to the nature of SNNs, or more precisely, due to the Heaviside activation of neurons (applied to the difference between the membrane potential and threshold). As the derivative of the Heaviside function is zero, except at zero where it is not defined, back-propagation does not convey any information for the SNN to learn . One of the most popular ways to circumvent this drawback is to use surrogate methods, where a derivative of a surrogate function is used in the backward pass during training. Due to their simplicity, surrogate methods have been widely used and have seen tremendous success in various supervised learning tasks [36; 28]. However, large and complex network architectures, the time-recursive nature of SNNs, and the fact that the training is oblivious of the sparsity of spikes in SNNs make surrogate methods quite time and energy-consuming.

Regarding regularization or energy efficiency during direct training of SNNs, only a few methods have been proposed addressing these topics together or separately, most of which deal with the forward propagation in SNNs. For example,  uses stochastic neurons to increase energy efficiency during inference. More recently,  uses regularization during the training to increase the sparsity of spikes, reducing the computational burden and energy consumption. Further,  performs the forward pass on a neuromorphic chip, while the backward pass is performed on a standard GPU. Although these methods improve the SNN models' performance, they do not significantly reduce the computational burden or provide the potential to do so. On the other hand,  introduces a threshold for surrogate gradients (or suggests using only a surrogate with bounded support). However, introducing gradient thresholds has the drawback of limiting the full potential of surrogates during training.

This paper proposes a direct training method for SNNs based on the zeroth order technique. We apply it locally, at the neuronal level - hence dubbed Local Zeroth Order (LocalZO) - with twofold benefits: regularization, which comes as a side-effect of the introduced randomness that is naturally associated with this technique, as well as a threshold for gradient backpropagation in the style of  which translates to potential energy-efficient training when properly implemented.

We summarize the main contributions of the paper as follows:

* We introduce zeroth order techniques in SNN training at a local level. We provide extensive theoretical properties of the method, relating it to the surrogate gradients via the internal distributions used in LocalZO.
* We experimentally demonstrate the main properties of LocalZO: its superior performance compared to baselines when it comes to generalizations, its ability to simulate arbitrary surrogates as well as its property to speed up the training process, which translates to energy-efficient training.

## 2 Background

### Spiking neuron dynamics

An SNN consists of Leaky Integrate and Fire neurons (LIF) governed by differential equations in continuous time . They are generally approximated by discrete dynamics given in the form of recurrence equations,

\[u_{i}^{(l)}[t] = u_{i}^{(l)}[t-1]+_{j}w_{ij}x_{j}^{(l-1)}[t]-x_{i}^{(l)}[t- 1]u_{th},\] \[x_{i}^{(l)}[t] =h(u_{i}^{(l)}[t]-u_{th})=1&u_{i}^{(l)}[t]>u_{th} \\ 0&\] (1)

where \(u_{i}^{(l)}[t]\) denote the membrane potential of \(i\)-th neuron in the layer \(l\) at time-step (discrete) \(t\), which recurrently depends upon its previous potential (with scaling factor \(<1\)) and spikes \(x_{j}^{(l-1)}[t]\) received from the neurons of previous layers weighted by \(w_{ij}\). The neuron generates binary spike \(x_{i}^{(l)}[t]\) whenever the membrane potential exceeds threshold \(u_{th}\), represented by the Heaviside function \(h\), followed by a reset effect on the membrane potential.

To implement the back-propagation of training loss through the network, one must obtain a derivative of the spike function, which poses a significant challenge in its original form represented as:

\[[t]}{du}=&u_{i}^{(l)}[t]=u_{th}\\ 0&\] (2)

where we denote \(u:=u_{i}^{(l)}[t]-u_{th}\). To avoid the entire gradient becoming zero, known as the dead neuron problem, the surrogate gradient method (referred to as Surrogate) redefines the derivative using a surrogate:

\[[t]}{du}:=g(u)\] (3)

Here, the function \(g(u)\) can be, for example, the derivative of the Sigmoid function (see section 4.4), but in general, one takes a scaled probability density function as a surrogate (see Section 4.2 for more details).

### Motivation

Classically, the purpose of dropout in ANNs is to prevent a complex and powerful network from over-fitting the training data, which consequently implies better generalization properties . In the forward pass, one usually assigns to each neuron in a targeted layer a probability of being "switched-off" during both forward and backward passes, and this probability does not change during the training. Moreover, the "activity" of the neuron, however we may define it, does not affect whether the neuron will be switched on or off.

Our motivation comes along these lines: how to introduce a dropout-like regularizing effect in the training of SNNs, but keeping in mind the temporal dimension of the data, as well as the neuron activity at that particular moment (heuristically, a more active neuron would be kept "on" with a high probability (randomness of the dropout) while a less active one would be "switched off", again with high probability, in a sense to be made precise shortly). Generally speaking, our idea consists of the following two steps: 1) For each spiking neuron of our SNN network, measure how active the neuron is in the forward pass at each time step \(t\). Here, we define the activity based on how far the current membrane potential of the neuron \(u[t]\) is from the firing threshold \(u_{th}\) (this idea comes from ). However, unlike in  where the sole distance is the determining factor, we introduce the effect of randomness via a fixed PDF, say \(\), sample \(z\) from it and say the neuron is active at time \(t\) if \(|u[t]-u_{th}|<c|z|\), where \(c\) is some upfront fixed constant. 2) In the backward pass at the time \(t\), if the neuron is dubbed active, we will apply some surrogate function \(g(u[t]-u_{th})\); otherwise, we will take the surrogate to be 0 (hence, switching off the propagation of gradients through the neuron in the latter case).

Having said this, we ask ourselves the final question: can we have a systematic way of choosing functions \(\) and \(g\) so that the expected surrogate we use (with respect to \(\)) equals the one we chose upfront? A simple yet elegant solution that satisfies all of the above comes with zeroth order methods.

Zeroth order technique is a popular gradient-free method , well studied in neural networks literature. To briefly introduce it, we consider a function \(f:^{d}\), that we intend to minimize using gradient descent, for which the gradient may not be available or even undefined. The zeroth-order method estimates the gradients using function outputs: given a scalar \(>0\), the 2-point ZO is defined as

\[G^{2}(;,)=(d)+)- f(-)}{2}\] (4)

where, \(\) is a random direction with \(_{z}(\|\|^{2}]=1\) and \((d)\) is a dimension dependent factor, with \(d\) being the dimension. However, to approximate the full gradient of \(f\) up to a constant squared error, we need an average of \(O(d)\) samples of \(G^{2}\), which becomes computationally challenging when \(d\) is large, such as the number of learnable parameters of the neural network. Though well studied in the literature, properties of 2-point ZO are known only for the continuous functions [29; 4]. In the present context, we will apply it locally to the Heaviside function that produces the outputs of spiking neurons, and we justify this by providing the necessary theoretical background for doing so.

## 3 The LocalZO algorithm

Applying ZO on a global scale is challenging due to the large dimensionality of neural networks. Since the non-differentiability of SNN is introduced by the Heaviside function at the neuronal level, we apply the 2-point ZO method on \(h:\{0,1\}\) itself,

\[G^{2}(u;z,)=z=0,& |u|>|z|\\ ,&|u|<|z|\] (5)

where \(u=u_{i}^{(l)}[t]-u_{th}\) and \(z\) is sampled from some distribution \(\). We may average the 2-point ZO gradient over a few samples \(z_{k}\) so that the LocalZO derivative of the spike function is defined as:

\[[t]}{dt}:=_{k=1}^{m}G^{2}(u;z_{k},)\] (6)

where, the number of samples, \(m\), is a hyper-parameter to the LocalZO method. We implement this at the neuronal level of the back-propagation routine, where the forward pass uses the Heaviside function, and the backward pass uses equation (6). Note that the gradient \([t]}{dt}\) being non-zero naturally determines the active neurons of the backward pass (as was discussed in Section 2.2), which can be inferred from the forward pass through the neuron. Algorithm 1 gives an abstract representation of the process at a neuronal level, which hints that the backward call is redundant when the neuron has a zero gradient.

``` Forward
0:\(u:=u_{i}^{(l)}[t]-u_{th}\), dist. \(\), const. \(,m\) sample \(z_{1},z_{2}, z_{m}\)\(grad_{k=1}^{m}[|u|<|z_{k}|]|}{2}\) if\(grad 0\)then  SaveForBackward(\(grad\)) endif return\((u>0)\) ```

**Algorithm 1** LocalZO

## 4 Theoretical Properties of LocalZO

### General ZO function

For the theoretical results around LocalZO, we consider a more general function than what was suggested by eqn. 5, in the form

\[G^{2}(u;z,)=0,&|u|>|z|\\ }{2},&|u||z|,\] (7)

where the new constant \(\) is an integer different from 0, while \(\) is a positive real number (so, for example, setting \(=1\) in (7), we obtain (5)).

The integer \(\) is somewhat a normalizing constant, which allows obtaining different surrogates as the expectation of function \(G^{2}(u;z,)\) when \(z\) is sampled from suitable distributions.

In practice, taking \(= 1\) will suffice to account for most of the surrogates found in the literature. The role of \(\) is somewhat different, as it controls the "shape" of the surrogate (narrowing it and stretching around zero). The role of each constant will be more evident from what follows (see section 4.4).

### Surrogate functions

**Definition 4.1**.: We say that a function \(g:_{ 0}\) is a surrogate function (gradient surrogate) if it is even, non-decreasing on the interval \((-,0)\) and \(c:=_{-}^{}g(z)dz<\).

Note that the integral \(_{-}^{}g(z)dz\) is convergent (as \(g(z)\) is non-negative), but possibly can be \(\) and the last condition means that the function \(g(t)\) is a probability density function. The first two conditions, that is, requirements for the function to be even and non-decreasing, are not essential but rather practical and consistent with examples from SNN literature.

Note that the function \(G:\), defined as \(G(t):=_{-}^{t}g(z)dz\) is the corresponding cumulative distribution function (for PDF \(g(t)\)). Moreover, it is not difficult to see that its graph is "symmetric" around point \((0,)\) (or in more precise terms, \(G(t)=1-G(-t)\)), hence \(G(t)\) can be seen as an approximation of Heaviside function \(h(t)\). Then, its derivative \(G(t)=g(t)\) can serve as an approximation of the "derivative" of \(h(t)\), or in other words, as its surrogate, which somewhat justifies the terminology.

Finally, one may note that "true" surrogates would correspond to those functions \(g\) for which \(c=1\). However, the reason we allow \(c\) to be different from 1 is again practical and simplifies the derivation of the results that follow. We note once again that allowing general \(c\) is in consistency with examples used in the literature.

### Surrogates and ZO

To be in line with classic results around the ZO method and gradient approximation of functions, we pose ourselves two fundamental questions: What sort of functions in variable \(u\) can be obtained as the expectation of \(G^{2}(u;z,)\) when \(z\) is sampled from a suitable distribution \(\), and, given some function \(g(u)\), can we find a distribution \(\) such that we obtain \(g(u)\) in the expectation when \(z\) is sampled from \(\)?

Two theorems that follow answer these questions and are the core of this section. The main player in both of the questions is the expected value of \(G^{2}(u;z,)\), so we start by analyzing it more precisely. Let \(\) be a distribution, \((t)\) its PDF for which we assume that it is even and that \(_{0}^{}z^{}(z)dz<\). Then, we may write

\[_{z}[G^{2}(u;z,)]=_{-}^{}G^ {2}(u;z,)(z)dz=_{|u||z|}}{2 }(z)dz=_{}^{} z^{}(z)dz.\] (8)

It becomes apparent from eqn. (8) that \(_{z}[G^{2}(u;z,)]\) has some properties of surrogate functions (it is even and non-decreasing on \(_{<0}\)). The proofs of the following results are detailed in the appendix A.

**Lemma 1**.: _Assume further that \(_{0}^{}z^{+1}(z)dz<\). Then, \(_{z}[G^{2}(u;z,)]\) is a surrogate function._

**Theorem 2**.: _Let \(\) be a distribution and \((t)\) its corresponding PDF. Assume that integrals \(_{0}^{}t^{}(t)dt\) and \(_{0}^{}t^{+1}(t)dt\) exist and are finite. Let further \(\) be the distribution with corresponding PDF function_

\[(z)=_{|z|}^{}t^{}(t)dt,\]

_where \(c\) is the scaling constant (such that \(_{-}^{}(z)dz=1\)). Then,_

\[_{z}[G^{2}(u;z,)]=_{z }[c\,h(u+ z)].\]

For our next result, which answers the second question we asked at the beginning of this section, note that a surrogate function is differentiable almost everywhere, which follows from the Lebesgue theorem on the differentiability of monotone functions. So, taking derivatives here is understood in an "almost everywhere" sense.

**Theorem 3**.: _Let \(g(u)\) be a surrogate function. Suppose further that \(c=-2^{2}_{0}^{}}g^{}(z)dz<\) and put \((z)=-}{cz^{}}g^{}(z)\) (so that \((z)\) is a PDF). Then,_

\[c\,_{z}[G^{2}(u;z,)]=_{z}[c\,G ^{2}(u;z,)]=g(u).\]

### Application of Theorem 2 and 3

Next, we spell out the results of Theorem 2 applied to some standard distributions, with \(=1\). For clarity, all the distributions' parameters are chosen so that the scaling constant of the resulting surrogate is 1. One may consult Figure 1 for the visual representation of the results, while the details are provided in the appendix A.1. Recall that the standard normal distribution \(N(0,1)\) has PDF of the form \(}(-}{2})\). Consequently, it is straightforward to obtain

\[_{z}[G^{2}(u;z,)]=}_{- }^{}(-}{2})dz=}(-}{2^{2}}).\] (9)

In appendix, A.1, we further derive surrogates when \(z\) is sampled from Uniform and Laplace distribution.

We recall that Theorem 3 provides a way to derive distributions for arbitrary surrogate functions ( that satisfy the conditions of the theorem). Consider the Sigmoid surrogate function, where the differentiable Sigmoid function approximates the Heaviside . The corresponding surrogate gradient is given by,

\[==}=:g(u)\]

Observe that \(g(u)\) satisfies our definition of a surrogate (\(g(u)\) being even, non-decreasing on \((-,0)\) and \(_{-}^{}g(u)du=1<\)). Thus, according to Theorem 3, we have \(c=-2^{2}_{0}^{}(t)}{t}dt=k^{2}}{a^{2}}\) where, \(a:=}\). The corresponding PDF is given by

\[(z)=-}{c}( t)}{z}=a^{2}}\] (10)

Observe that the temperature parameter \(k\) comes from the surrogate to be simulated, while \(\) is used by LocalZO. Appendix A.2 provides calculations and distribution corresponding to the popular Fast-sigmoid surrogate, followed by a description of the inverse sampling method that can be used to simulate sampling for arbitrary distributions using the uniform distribution.

### Expected back-propagation threshold for LocalZO

To study the energy efficiency of the LocalZO method when training SNNs, we compute the expected threshold \(_{th}\) for the activity of the neurons, i.e. the expectation of the quantity \(|z|\) when \(z\) is sampled from a distribution \(\). It is used in the experimental section when comparing our method with the alternative energy-efficient method . The expected threshold values are presented in Table 1 (\(m\) denotes the number of samples used in (6)), while the details of the derivations can be found in A.3.

Figure 1: The figure shows the expected surrogates derived in section 4.4 as \(z\) is sampled from Normal\((0,1)\), Unif\(([,])\) and Laplace\((0,})\) respectively. Each figure shows the surrogates corresponding to \( 0\), \(=0.5\) and \(1\). The surrogates are supplied to SparseGrad methods for a fair comparison with LocalZO as the latter uses respective distributions to sample \(z\).

[MISSING_PAGE_FAIL:7]

   Dataset & Methods & Architecture & Simulation Length & Accuracy \\   & Hybrid training & ResNet-20 & 250 & 92.22 \\  & Diet-SNN & ResNet-20 & 10 & 92.54 \\  & STBP & CIFARNet & 12 & 89.83 \\  & STBP NeuNorm & CIFARNet & 12 & 90.53 \\  & TSSL-BP & CIFARNet & 5 & 91.41 \\   &  &  & 6 & 93.16 \\  & & & 4 & 92.92 \\  & & & 2 & 92.34 \\   &  &  & 6 & 95.07 \\  & & & 4 & 94.89 \\  & & & 2 & 94.65 \\   &  &  & 6 & 94.50 \\  & & & 4 & 94.44 \\  & & & 2 & 94.16 \\   &  &  & 6 & **95.56** \\  & & & 4 & **95.3** \\  & & & 2 & **95.03** \\   & Hybrid training & VGG-11 & 125 & 67.87 \\  & Diet-SNN & ResNet-20 & 5 & 64.07 \\   &  &  & 6 & 71.12 \\  & & & 4 & 70.86 \\  & & & 2 & 69.41 \\   &  &  & 6 & 73.74 \\  & & & 4 & 74.13 \\  & & & 2 & 72.78 \\   &  &  & 6 & 74.72 \\  & & & 4 & 74.47 \\  & & & 2 & 72.87 \\   &  &  & 6 & **77.25** \\  & & & 4 & **76.89** \\  & & & 2 & **76.36** \\   & EfficientLIF-Net & ResNet-19 & 5 & 79.44 \\  & **LocalZO +TET** & SEW-Resnet34 & 4 & 78.58, **81.56\({}^{}\)** \\   & tdBN & ResNet-19 & 10 & 67.8 \\  & Streaming Rollout  & DenseNet & 10 & 66.8 \\  & Conv3D & LIAF-Net & 10 & 71.70 \\  & LIAF & LIAF-Net & 10 & 70.40 \\  & TET & VGGNN & 10 & 74.89\({}^{*}\), 81.45\({}^{*}\) \\  & **LocalZO +tDBN** & VGGSNN & 10 & 72.6, 79.37 \\  & **LocalZO +TET** & VGGSNN & 10 & **75.62**, **81.87** \\   & AEGNN & GNN & - & 66.8 \\  & EST & ResNet-34\({}^{}\) & 9 & 81.7 \\  & **LocalZO +tDBN** & VGGSNN & 10 & 74.65, 79.05 \\  & **LocalZO +TET** & VGGSNN & 10 & **79.86**, **82.99** \\   & AEGNN & GNN & - & 94.5 \\  & EST & ResNet-34\({}^{}\) & 9 & 92.5 \\  & **LocalZO +tDBN** & VGGSNN & 10 & 95.96, 95.68 \\  & **LocalZO +TET** & VGGSNN & 10 & **96.78**, **96.96** \\   & SEW & SEW-Resnet & 16 & 97.92 \\  & **LocalZO +TET** & VGGSNN & 10 & 98.04, **98.43** \\   

* our implementation, \({}^{}\)pre-trained with ImageNet, \({}^{}\) 83.33 % with \(m=20\)

Table 2: Comparison with the existing methods show that LocalZO improves the accuracy of existing direct training algorithms. For the existing methods, we compare the performance with the results reported in respective literatures. For the rows with two accuracies reported, the second one is for training with additional augmentation.

### Performance on Energy Efficient Implementation

In the energy-efficient implementation of the back-propagation , the optimization of the network weights takes place in a layer-wise fashion through the unrolling of recurrence of equation (1) w.r.t time. As the active neurons of each layer for every time step are inferred from the forward pass, gradients of only active neurons are required to be saved for the backward pass, hence saving the computation requirement of the backward pass. One may refer to  for further details of this implementation framework. To compare, we supply SparseGrad method the surrogate approximated by LocalZO, as per section 4.4. The SparseGrad algorithm also requires a back-propagation threshold parameter, \(B_{th}\), to control the number of active neurons participating in the back-propagation. We supply it the expected back-propagation threshold \(_{th}\) of LocalZO as obtained in sections 4.5. We follow the same experimental setting as in  for a fair comparison. We use a fully connected LIF neural network with two hidden layers of 200 neurons each and input and output layers. We train every model for 20 epochs and report the average training and test accuracies computed over five trials. We compute the speedup of SparseGrad and LocalZO, with respect to the full surrogate without truncation, that uses standard back-propagation. The backward speedup (Back.) captures the number of times the backward pass of a gradient update is faster, while the overall speedup (Over.) considers the total time for the forward and the backward pass and then computes the speedup. The speedup reported is averaged over all the gradient updates and the experimental trials.

We compare the performance of the algorithms on three datasets: 1) Neuromorphic-MNIST (NMNIST) , which consists of static images of handwritten digits (between 0 and 9) converted to temporal spiking data using visual neuromorphic sensors; 2) Spiking Heidelberg Digits (SHD) , a neuromorphic audio dataset consisting of spoken digits (between 0 and 9) in English and German language, totalling 20 classes. To challenge the generalizability of the learning task, 81% of test inputs of this dataset are new voice samples, which are not present in the training data; 3) Fashion-MNIST (FMNIST)  dataset is converted using temporal encoding to convert static gray-scale images based on the principle that each input neuron spikes only once, and a higher intensity spike results in an earlier spike.

Table 3 provides a comparison of the algorithms, using surrogates corresponding to the Normal and Sigmoid, with \(=0.05\) and \(m=1\). For the normal distribution, we supply SparseGrad algorithm the back-propagation threshold \(_{th}\) obtained in Table 1. In the section 4.4, we derived distributions corresponding to the Sigmoid surrogate. We use inverse transform sampling (see A.2.3), and take the temperature parameter \(k=a/ 30.63\) so that \(c=k^{2}}{a^{2}}=1\) and supply SparseGrad method the corresponding back-propagation threshold, \(_{th}=0.766\). The LocalZO method offers better test accuracies than SparseGrad, with a slight compromise in speedup due to the sampling of random variable \(z\). The difference between training and test accuracies for the SHD dataset can be attributed to the unseen voice samples in the test data.

Figure 2 shows the training loss, overall speedup, and percentage of active

   Method & Train & Test & Back. & Over. \\ 
**NMNIST** & \(z(0,1)\), \(=0.05,m=1\) & & \\  SparseGrad & 93.26 \(\) 0.31 & 91.86\(\) 0.29 & 99.57 & 3.38 \\ LocalZO & 94.38 \(\) 0.12 & 93.29\(\) 0.08 & 92.27 & 3.34 \\   & Sigmoid, \(=0.05\), \(k 30.63\), \(m=1\) & & \\  SparseGrad & 92.96\(\) 0.26 & 91.04\(\) 0.32 & 87.45 & 3.00 \\ LocalZO & 93.98\(\) 0.08 & 92.97\(\) 0.05 & 83.54 & 3.02 \\  
**SHD** & \(z(0,1)\), \(=0.05\), \(m=1\) & & \\  SparseGrad & 92.03\(\) 0.79 & 74.73\(\) 0.73 & 143.7 & 4.83 \\ LocalZO & 91.77\(\) 0.27 & 76.55\(\) 0.93 & 142.8 & 4.75 \\   & Sigmoid, \(=0.05\), \(k 30.63\), \(m=1\) & & \\  SparseGrad & 92.19\(\) 0.41 & 75.80\(\) 0.97 & 140.8 & 4.46 \\ LocalZO & 91.96\(\) 0.11 & 76.97\(\) 0.40 & 133.6 & 4.36 \\  
**FMNIST** & \(z(0,1)\), \(=0.05\), \(m=1\) & & \\  SparseGrad & 81.91\(\) 0.10 & 80.28\(\) 0.11 & 15.74 & 1.97 \\ LocalZO & 83.83\(\) 0.07 & 81.79\(\) 0.06 & 15.49 & 1.88 \\   & Sigmoid, \(=0.05\), \(k 30.63\), \(m=1\) & & \\  SparseGrad & 81.60\(\) 0.11 & 80.02\(\) 0.08 & 12.12 & 1.65 \\ LocalZO & 83.39\(\) 0.10 & 81.76\(\) 0.10 & 12.50 & 1.57 \\   

Table 3: Performance on NMNIST, SHD and FMNISTneurons after each gradient step for the Sigmoid surrogate. The sparseness of active neurons (under 0.6%) explains the reduced computational requirement that translates to the speedup.

We further implement LocalZO with \(=0.5,z(0,1)\) to train a CNN architecture (Input- 16C5-BN-LIF-MP2-32C5-BN-LIF-MP2-800FC-10) and compare it with the corresponding surrogate gradient algorithm. Figure 3 shows the corresponding sparsity of the methods by plotting the number of zero elements at a neuronal level. The plot suggests that during the training, LocalZO exhibits higher sparsity gradients than the surrogate method.

**Ablation study:** Table4 further shows the test accuracy of the LocalZO method and overall speed-up for a wide range of values of \(m\) with \(z(0,1)\). Like Table 3, the experiments are repeated five times and mean test accuracy is reported along with standard deviation (Std.). In general, by increasing \(m\), the method approximates the surrogate better, still offers the regularizing effect and potentially improves the generalization, but also requires more computation. Larger \(m\) leads to more non-zero gradients at the neuronal level in the backward pass, reducing overall speed-up. On the other hand, smaller \(m\) introduces higher randomness (less "controlled"), still yielding regularization, which helps obtain better generalization, as well as potential speed-up. In conclusion, \(m\) should be treated as a hyper-parameter, its value depending on the training setting itself. In our experiments, we chose \(m=1\) or \(5\) for most of the experiments, as a proof of concept, but also because it offers a nice balance between the speed-up and performance.

## 6 Discussions

We propose a new direct training algorithm for SNNs that establishes a formal connection between the standard surrogate methods and the zeroth order method applied locally to the neurons. The method introduces systematic randomness in the training that helps in better generalization. The method simultaneously lends itself to efficient back-propagation. We experimentally demonstrate the efficiency of the proposed method in terms of speed-up obtained in training under specialized implementations and its top generalization performance when combined with other training methods, ameliorating their respective strengths.