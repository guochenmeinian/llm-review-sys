# Improving self-training under distribution shifts via anchored confidence with theoretical guarantees

Improving self-training under distribution shifts via anchored confidence with theoretical guarantees

Taejong Joo & Diego Klabjan

Department of Industrial Engineering & Management Sciences

Northwestern University

Evanston, IL, USA

{taejong.joo,d-klabjan}@northwestern.edu

###### Abstract

Self-training often falls short under distribution shifts due to an increased discrepancy between prediction confidence and actual accuracy. This typically necessitates computationally demanding methods such as neighborhood or ensemble-based label corrections. Drawing inspiration from insights on early learning regularization, we develop a principled method to improve self-training under distribution shifts based on temporal consistency. Specifically, we build an uncertainty-aware temporal ensemble with a simple relative thresholding. Then, this ensemble smooths noisy pseudo labels to promote selective temporal consistency. We show that our temporal ensemble is asymptotically correct and our label smoothing technique can reduce the optimality gap of self-training. Our extensive experiments validate that our approach consistently improves self-training performances by 8% to 16% across diverse distribution shift scenarios without a computational overhead. Besides, our method exhibits attractive properties, such as improved calibration performance and robustness to different hyperparameter choices.

## 1 Introduction

In this work, we address the challenge of adapting pre-trained neural networks at test time under distribution shifts, a problem known as test-time adaptation (TTA) or source-free domain adaptation (SFDA). Distribution shifts--where a model trained on one distribution is then tested on a different one--are ubiquitous in many practical scenarios due to demographic subpopulation shift  and changes in data collection environments . Despite the robust performance of neural networks under independent and identically distributed (i.i.d.) settings, they often suffer from substantial performance degradation under such shifts . Recently, TTA and SFDA have proven their effectiveness in resolving these critical issues by effectively leveraging information about the distribution shifts contained in unlabeled samples given at the test time.

Self-training is the basis of many state-of-the-art methods in TTA and SFDA , utilizing pseudo labels generated from the model's own predictions to train a model on unlabeled samples . Since pseudo labels are regarded as true labels in self-training, the success of self-training methods highly depends on how to filter incorrect pseudo labels to prevent self-confirmation bias . This issue has been effectively handled via simple confidence-based thresholding in i.i.d. settings . However, the distribution shifts make it hard to filter incorrect pseudo labels due to high noise rates even under high threshold . Thus, sophisticated methods filter incorrect pseudo labels based on a neighborhood structure of the data  and consistency of multiple predictions under different models  or augmentations (cf. Section 5), which are computationally intensive by nature.

Recent insights in  suggest an alternative strategy can be also effective: promoting temporal consistency can enhance self-training performance in SFDA without the computational burden of previous methods. The temporal consistency regularizer, so called early learning regularization (ELR) , was originally developed to address neural networks' tendency to learn clean information first and then gradually memorize noisy labels [22; 23]; this setting is naturally connected to self-training scenarios when we regard the pseudo labels as random noisy labels. However, the impacts of ELR on self-training have not been fully understood. Also, since ELR does not consider unique characteristics of distribution shifts, we aim to answer the following question: _Is there any principled way to improve the way of memorizing all past predictions tailored for self-training under distribution shifts?_

In this work, we show that the answer is affirmative by proposing Anchored Confidence (AnCon) that uses _confident predictions_ to support a generalized notion of _temporal consistency_. Specifically, we construct a generalized temporal ensemble, which weighs predictions based on predictive uncertainty, and then use the ensemble as a smoothing vector in label smoothing . Then, through rigorous theoretical analyses, we show that our simple heuristic for the generalized temporal ensemble is asymptotically correct and that the label smoothing formulation can reduce the optimality gap. As a result, AnCon can correct wrong pseudo labels without expensive computations and can be easily applied to self-training methods by replacing one-hot pseudo labels with smooth pseudo labels, unlike neighborhood-based or centroid methods. Through extensive experiments, we show that AnCon improves self-training under diverse distribution shift scenarios and posses many attractive properties.

Our contribution can be summarized as follows: **1)** We develop AnCon, which is the first algorithm that attempts to improve self-training under distribution shifts by generalizing a notion of temporal consistency with theoretical guarantees; **2)** Without any additional forward passes or neighborhood search, AnCon improves self-training performances 8% and 16% under domain shifts and image corruptions, respectively; **3)** Remarkably, we also show that AnCon significantly improves calibration performance and is robust with respect to model selection methods and hyperparameter choices.

## 2 Background

**Notation and setup** For an input space \(^{d}\) and a label space \(=[K]:=\{1,2,,K\}\), we define \(X\) and \(Y\) to be random variables of input and output with probability densities \(p_{X}\) and \(p_{Y}\), respectively. We also define a neural network \(f(;):^{K-1}\) parameterized by a parameter \(^{p}\) where \(^{K-1}\) is the probability simplex with \(K\) elements. Our goal is to minimize the cross-entropy loss \(_{^{p}}l():=_{XY}[H(f(X;),p_{Y|X })]\) where \(H(f(x;),p_{Y|X=x}):=-_{k[K]}p(Y=k|x) f_{k}(x;)\). In the SFDA setting, we are given an initial parameter \(_{0}\) that is trained on a different data generating distribution \((X^{},Y^{})\), e.g., \(_{0}_{^{p}}_{X^{}Y^{} }[H(f(X^{};),p_{Y^{}|X^{}})]\). We can think about this setting as either (1) \((X^{},Y^{})\) being pre-training data and \(_{0}\) the foundation model with the task of fine-tuning the model on unlabeled \(X\) or (2) a transfer learning problem with \((X^{},Y^{})\) being the source domain data and \(X\) the target domain. Here, we assume only covariate shift without concept shift; that is, \(p_{X}^{D}p_{X^{}}\) but \(p_{Y|X}=^{D}p_{Y^{}|X^{}}\). Even in this case, we note that suboptimality under the distribution shift, i.e., \(_{^{p}}l()-l(_{0})\), can be large.

**Self-training** In this work, we tackle the distribution shifts by using the self-training method that replaces the true label \(Y(x)\) by the pseudo label, i.e., \(_{_{new}}l(_{new};)=_{X}[- f_{(x; )}(x;_{new})]\) where \((x;):=_{k[K]}f_{k}(x;)\) is a pseudo label under \(\). Specifically, the algorithmic framework of self-training is as follows: given \(_{0}\), we iteratively find model parameter \(_{m+1}\) for \(m=0,1,\) with \(_{m+1}_{_{new}}(_{new};_{m})\). For later use, we also define a prediction confidence \(c(x;):=_{k[K]}f_{k}(x;)\) and \(_{0:m}:=(_{0},,_{m})\).

**Early learning regularization** In the learning from noisy labels (LFN) scenario,  identified an "early-learning phenomenon" where neural networks initially learn information contained in clean labels before gradually memorizing noisy labels, leading to a performance deterioration as training progresses. To mitigate this issue, ELR penalizes predictions that deviate from earlier predictions by defining a target network from the past predictions: \(_{ELR}(x;_{0:m}):=_{j=0}^{m}(1-)^{m-j}f(x; _{j})\). Then, adding an auxiliary loss of \(L_{ELR}(;_{0:m})=_{X}[(1-f(X;)^{T}_{ELR }(X;_{0:m}))]\) to \((;_{m})\) can prevent memorization of noisy labels while preserving correct patterns.

Notably, this insight has recently been confirmed to be applicable in the SFDA setting by . This observation is appealing because ELR can be efficiently implemented by reusing past predictions without additional forward passes or neighborhood searching, unlike dominant methods in SFDA[16; 17; 18; 19; 20]. Nevertheless, given that ELR stems from a general property of the neural network training in the i.i.d. setting, herein we aim to step towards a more principled approach to encourage temporal consistency tailored for distribution shift scenarios.

## 3 Anchored confidence

In this section, we introduce AnCon, which promotes the temporal consistency on selectively chosen predictions via label smoothing. In Section 3.1, we first explain the idea of promoting selective temporal consistency based on confident predictions via label smoothing  with a temporal ensemble. Then, in Section 3.2, we explain how to effectively construct temporal ensemble for improving self-training under distribution shifts. Finally, we theoretically analyze the efficacy of AnCon by drawing connection between our method and knowledge distillation in Section 3.3.

### Selective temporal consistency via label smoothing

In this work, we utilize label smoothing  to promote the selective temporal consistency instead of using an auxiliary loss function like ELR. Specifically, given a generalized temporal ensemble \((x;_{0:m},_{0:m})\) with \(_{0:m}:=(w_{0},,w_{m})\) which will be specified in Section 3.2, we construct a regularized pseudo label \((X;_{0:m},_{0:m})\) by using \((x;_{0:m},_{0:m})\) as a smoothing vector for the pseudo label \((x;_{m})\). That is, we perform self-training by

\[_{}_{X}[H(f(X;),(X;_{0:m},_ {0:m}))],(X;_{0:m},_{0:m})=(1-)E_{1}( (x;_{m}))+(x;_{0:m},_{0:m}) \]

where \(\) is a coefficient, \(E_{1}()\) is the one-hot encoding, and \((x):=(_{1}(x),,_{K}(x))\) is the \(K\)-dimensional output of the generalized temporal ensemble (cf. (2)).

Thus, AnCon can control the usage of potentially noisy information in \((x;_{m})\) based on its consistency with \((x;_{0:m},_{0:m})\). Not only can this approach preserve the early learning phenomenon as in ELR (cf. Section 2), but the label smoothing formulation also significantly stabilizes the self-training performance under different hyperparameter choices due to the fact that the optimal values of hyperparameters are less problem dependent compared to its equivalent auxiliary regularization . Beyond removing the burden of hyperparameter search, this robustness is a particularly intriguing property under distribution shift scenarios where the model selection becomes a challenging task.

Further, encouraging the temporal consistency through label smoothing enables us to connect our method with knowledge distillation (KD) , which can provide a wide range of principled techniques and theoretical results developed in KD. As a concrete example, we will show when and how AnCon can reduce the optimality gap of self-training, i.e., a case with \(=0\), in Section 3.3.

### Constructing an effective generalized temporal ensemble with prediction confidences

Next, we construct the generalized temporal ensemble that makes the selective temporal consistency in (1) work effectively in self-training under distribution shifts. Specifically, given \(_{0:m}\) and weights \(_{0:m}(x)^{m}\) for each \(x\), the prediction by the generalized temporal ensemble is

\[_{k}(x;_{0:m},_{0:m}):=_{i=0}^{m}w_{i}(x) p(y =k|x,_{i}), k[K] \]

where \(p(y|x,_{i})\) is the prediction made by \(f(x;_{i})\), which can be either soft (\(p(y=j|x,_{i})=f_{j}(x;_{i})\)) or hard (\(p(y=j|x,_{i})=1\) if \(j=_{k[K]}f_{k}(x;_{i})\) and \(p(y=j|x,_{i})=0\) otherwise). In this work, we use hard prediction because soft prediction puts more weights on recent predictions since self-training tends to keep increasing the prediction confidence during training.

Surprisingly, we will show that the following simple relative thresholding for determining \(_{0:m}\) gives the asymptotic optimal weights achieving the minimum worst-case optimality gap of self-training:

\[w_{m}(x)(c(x;_{m})>_{m}^{()}),_{ m}^{()}:=_{i=0}^{m}(1-)^{m-i}}_{X}[c(X; _{i})] \]

where \(_{m}^{()}\) is an exponential moving average (EMA) of prediction confidence with hyperparameter \(\) and \(}[c(X;_{i})]\) is a Monte-Carlo approximation of \([c(X;_{i})]\) with mini-batch samples.

Intuitively, our weighting mechanism aggregates only relatively confident predictions with a uniform weight. Given the observation that relative ordering of confidence is highly correlated with accuracy even under distribution shifts , our thresholding rule would tend to put non-zero weights on correct predictions. Besides, by employing the relative criterion, the thresholding does not suffer from the problems that neglect predictions obtained in the early stage of training.

We also remark that AnCon has almost the same computational cost as ELR. Specifically, (2) can be implemented by \(_{k}(x;_{0:m},_{0:m})=_{k}(x;_{0:m-1}, _{0:m-1})+w_{m}(x)p(y=k|x,_{m})\) that requires to store the weighted sum of previous predictions without additional forward passes or storing previous parameters, which is the same as storing the previous logit vector in ELR. Similarly, (3) can be efficiently implemented by \(_{m}^{()}=_{m-1}^{()}+(1-)}_{X} [c(X;_{m})]\) that requires constant additional computational costs compared to vanilla self-training with the constant being small. Therefore, AnCon shares the same computational benefits as ELR compared to other state-of-the-art methods in SFDA.

**On optimality of the relative thresholding in (3)** From the optimization perspective, the optimal weights \(_{0:m}^{}\) correspond to the weights under which self-training with \((X;_{0:m},_{0:m})\) can minimize the expected loss:

\[_{0:m}^{}*{arg\,min}_{_{0:m}}l( _{_{0:m}}^{}),_{_{0:m}}^{} *{arg\,min}_{}}_{X}[H(f(X;), (X;_{0:m},_{0:m}))]. \]

Unfortunately, \(l(_{_{0:m}}^{})\), or its empirical counterpart, is not available in self-training due to the absence of labels. Further, even if labels are given, solving (4) is intractable due to non-smoothness of \(l(_{_{0:m}}^{})\) with respect to \(_{0:m}\) and the cost of finding \(_{_{0:m}}^{}\).

To circumvent this issue, we show in Section 3.3 that (4) can be relaxed to the problem of finding ensemble weights that give a maximum likelihood estimation (MLE) solution under certain conditions. As a result, instead of solving the intractable optimization in (4), we find the optimal weights by

\[_{0:m}^{}*{arg\,max}_{_{0:m}} _{XY}[_{Y}(X;_{0:m},_{0:m})]. \]

In the following theorem which is proven in Appendix B.2, we show that the simple relative thresholding in (3) can make \((x;_{0:m},_{0:m})\) asymptotically correct for samples where the neural network tends to be relatively confident during self-training and thus our simple weighting mechanism in (3) to be the solution of (5) in the asymptotic region.

**Theorem 3.1**.: _Let \(A_{i}(c):=\{x|c(x;_{i})>c\}\), \(Q(x;_{0:m}):=_{i=0}^{m}(x A_{i}(c_{i}))\), and \((x;_{0:m})=_{0:m})}_{i=0}^{m} _{Y|X=x}[(Y(x)=(x;_{i}))](x A_{i} (c_{i}))\) for \(x\) such that \(Q(x;_{0:m})\)>0. Let us assume that random events \((Y(x)=(x;_{i}))\) and \((Y(x)=(x;_{j}))\) are conditionally independent given \(X A_{i}(c)\) for \(j\{0,,i-1\}\), \(x\), \(c[0,1)\). If \(x\) such that \((x;_{0:m})>1/2\), then for the generalized temporal ensemble in (3), it holds that_

\[p(*{arg\,max}_{k[K]}_{k}(x;_{0:m},_{0:m }) Y(x))(-_{0:m})}{2}((x; _{0:m}))) \]

_where \((z):=2z-1-(2z)\) is a positive increasing function in \(z[0.5,1]\)._

The result states that as long as the average accuracy for _relatively confident predictions_ over iterations exceeds 50%, the error rate of the generalized temporal ensemble monotonically decreases as \(Q(x;_{0:m})\) increases. Furthermore, \((x;_{0:m},_{0:m})\) is asymptotically correct on \(x\) such that \(Q(x;_{0:m})\) as \(m\). AnCon aims to achieve these desirable properties through the _uncertainty-aware_ temporal consistency that helps to satisfy the condition \((x;_{0:m})>0.5\). Specifically, as shown in Figure 3(a) in Appendix, our generalized temporal ensemble's accuracy tends to significantly increase as the number of confident samples increases, being consistent with our theory. We note that this monotonic improvement would not be the case for the temporal ensemble without uncertainty-awareness and vanilla self-training (cf. Figure 3(a)).

Finally, we emphasize that the assumption \((x;_{0:m})>0.5\) applies only to _relatively confident predictions_ which are _averaged over iterations_. This is significantly weaker than requiring a lower bound of an expected accuracy of _each sample_ for _every iteration_, which is the case when LFN methods are directly applied to the self-training scenario. Also, due to its dependency on the choice of the confidence thresholds \(_{0:m}\), the assumption can hold by controlling \(_{0:m}\) at the expense of loosening the upper bound in (6) (e.g., selecting 90th-quantile as in Figure 3(b) in Appendix). Specifically, increasing the thresholds can improve \(((x;_{0:m}))\) and enhance the chance of satisfying \((x;_{0:m})>1/2\) but reducing \(Q(x;_{0:m})\). While this trade-off necessitates a proper choice of \(_{0:m}\), our extensive experiments show that setting the threshold \(c_{m}\) by the EMA of prediction confidence, i.e., \(_{m}^{()}\) in (3), works effectively.

### Theoretical insights from knowledge distillation

In this section, we present a novel connection between AnCon and KD for addressing intractability of (4). KD is a framework for training a small student network \(f\), e.g., ResNet-50 , with an additional supervision from a large teacher network \(f^{(t)}\), e.g., ResNet-152. Specifically, a cross-entropy under KD is \(l_{KD}()=_{XY}[H(f(X;),(1-_{KD})E_{1}(Y(X))+ _{KD}f^{(t)}(X))]\) with \(_{KD}\), which bears a significant similarity with AnCon (cf. (1)). Indeed, AnCon can be understood as a special case of KD called self-distillation when \(f\) and \(f^{(t)}\) have the same architecture, where the generalized temporal ensemble \(\) corresponds to the teacher network \(f^{(t)}\) with a notable difference that the pseudo label \(\) is used instead of the true label \(Y\). Based on this connection, we perform a convergence analysis of AnCon by modifying the partial variance reduction theory , as given below. We note that the usage of \(\) and \(\) results in an inherently biased gradient estimator, which requires special treatments for the convergence analysis unlike the typical self-distillation setting.

**Setup** Following , we assume a linear model \(f_{k}(x;)=(_{k}^{T}x)/_{i[K]}(_{i}^{T}x)\) with \(_{i}^{d}\) for \(i[K]\), \(:=(_{1},,_{K})^{dK}\), and \(()\) is the concatenation operation. Also, we assume a bounded support for \(X\); that is, \( x C\) for all \(x\) where \(p_{X}(x)>0\). Under this setting, we repeat the following steps starting from a given \(_{0}\) (i.e., \(m=0\)):

1. Outer temporal ensemble update: Update \(_{0:m}\) by (3) to obtain \((x;_{0:m},_{0:m})\) (cf. (2)).
2. Inner parameter update: With \(_{m,0}:=_{m}\) and \((_{0:m},_{0:m})\), solve (1) with stochastic gradient descent \(_{m,t+1}=_{m,t}- g_{}^{(m,t)}\) for \(t\{0,1,,T-1\}\) and set \(_{m+1}=_{m,T}\).
3. Iteration number update: Set \(m=m+1\) and terminate if \(m=\).

Here, \(\) is the learning rate, \(\) contains \(b\) random samples from \(p_{X}\), and the stochastic gradient under AnCon is defined as \(g_{}^{(m,t)}:=_{_{m,t}}_{X_{i}}H(f(X_{i}; _{m,t}),(X_{i};_{0:m},_{0:m}))\). For the linear model, we note that \(g_{}^{(m,t)}=_{}(_{m,t})-_{}( _{0:m},_{0:m})\) where \(_{}(_{m,t})=_{X_{i}}H(f(X_{i};_{ m,t}),(X_{i};_{m}))\) and \(_{}(_{0:m},_{0:m}):=(_{1,}( _{0:m},_{0:m}),,_{K,}(_{0:m}, _{0:m}))\) with \(_{k,}(_{0:m},_{0:m}):=_{X_{i}}[ (_{k}(X_{i};_{0:m},_{0:m})-_{k}(X_{i};_ {m}))X_{i}]\) for \(k[K]\).

In Theorem 3.2 which is proven in Appendix B.3, we analyze the convergence of the inner parameter update step under AnCon and vanilla self-training.

**Theorem 3.2**.: _Let us assume \(l()\) satisfies \(L\)-smoothness, \(\)-expected smoothness, and \(\)-Polyak-Lojasiewicz (PL) condition (cf. Assumptions B.1, B.2, and B.3 in Appendix B.1). For \(\), a carefully chosen \(\) (cf. \(=_{m}^{}:=_{}[(_{} (^{*}),_{}(_{0:m},_{0:m}))]}{_{}[ _{}(_{0:m},_{0:m})]\|^{2}+}\| (_{0:m},_{0:m})\|^{2}}\) in Lemma B.6 in Appendix B.5), and any realization of \(_{m}\), it holds that_

\[[l(_{m,t})-l(^{*})|_{m}]}_{}(l(_{m})-l(^{*}))}_{}+}{}g^{}(_{m})}_{}+N(_{m}^{};_{0:m}, _{0:m})}_{} \]

_where \(l^{*}:=l(^{*})\) with \(^{*}_{}l()\), \(g^{}():=[((X;) Y(X))]\), \(N(;_{0:m},_{0:m})=^{2}(_{0 :m},_{0:m})^{2}+_{}[ _{}(^{*})-_{}(_{0:m}, _{0:m})^{2}]\), and \(N(_{m}^{};_{0:m},_{0:m}) N(0)\) where \(N(0):=N(0;_{0:m},_{0:m})\) for any \((_{0:m},_{0:m})\)._

_Further, when \(_{0:m}\) is such that \(_{}[_{}(^{*}),_{}(_ {0:m},_{0:m})] 0\), i.e., the teacher has a sufficiently good performance, it holds that_

\[^{};_{0:m},_{0:m})}{N(0)} (1,2(1,g^{KL}(_{0:m},_{0:m})}{_{ }(_{0:m},_{0:m})})+g^{C}(_ {0:m},_{0:m})}{L_{(_{0:m},_{0:m})}^{2}}) \]_where \(_{*}^{2}:=_{}_{}(^{*})^ {2}\), \(_{(_{0:m},_{0:m})}^{2}:=_{}_{ }(_{0:m},_{0:m})^{2}\), \(g^{KL}(_{0:m},_{0:m})=_{X}[D_{KL}(f(X;^{*}) (X;_{0:m},_{0:m}))]\) with \(D_{KL}(p q)\) is the KL-divergence between \(p\) and \(q\), and \(g^{C}(_{0:m},_{0:m})=_{X}[(X;_ {0:m},_{0:m})]-_{X}[(X;_{m})]^{2}\)._

In Theorem 3.2, (7) characterizes the optimality gap in the inner loop optimization. Specifically, the first term is about reducing the initial optimality gap over iterations and motivates why we need _adaptation_, e.g., by self-training, if the performance deteriorates under severe distribution shifts. The second term is about the bias of the pseudo label and motivates the challenges of self-training under poorly performing pseudo labels in the case of severe distribution shifts. Crucially, these two terms can be fully characterized by the quality of initial model \(_{m}\) and do not depend on \(_{0:m}\). Thus, we concentrate on the impacts of \(_{0:m}\) designed in (1)-(3) on \(N(_{m}^{};_{0:m},_{0:m})\) to show that AnCon's effectiveness on improving self-training performance under distribution shifts.

First, Theorem 3.2 shows the effectiveness of our label smoothing formulation in (1) under properly chosen \(_{m}^{}\). Specifically, compared to vanilla self-training, AnCon results in the smaller neighborhood size of the stochastic gradient descent; \(N(_{m}^{};_{0:m},_{0:m}) N(0)\). That is, the result suggests that AnCon is at least better than vanilla self-training under the mild regularity conditions.

Further, under the additional assumption of a sufficiently good performance temporal ensemble, (8) motivates AnCon's weighting mechanism as a relaxed solution of the intractable optimization problem in (4). Specifically, if the marginal distribution of the pseudo labels does not change quickly over outer iterations (which is the case especially for the later training stages as shown in Figure 5 in Appendix), changing \(_{0:m}\) would have only a marginal impact on \(g^{C}(_{0:m},_{0:m})\). Thus, the weighting mechanism that minimizes \(g^{KL}(_{0:m},_{0:m})\) would minimize the worst-case optimality gap, which justifies our approach of circumventing intractability of (4) with (5). In this regard, AnCon's weighting mechanism in (3) could be thought of as a relaxed solution of (4) as it minimizes \(g^{KL}(_{0:m},_{0:m})\) in the asymptotic region (cf. Theorem 3.1).

We conclude this section by analyzing the three iterative steps where the pseudo labels and the temporal ensembles keep updating.

**Corollary 3.2.1**.: _Let us assume \(l()\) satisfies \(L\)-smoothness, \(\)-expected smoothness, and \(\)-PL condition. For \(}\), \(\) carefully adjusted at each outer temporal ensemble update (cf. \(=_{j}^{}\) in Lemma B.6 for each outer iteration \(j\{0,,-1\}\)), and any initial parameter \(_{0}\), it holds that_

\[[l(_{})-l^{*}](1-)^{T(-1)}(l( _{0})-l^{*})+_{}_{j I^{}[ }g^{}(_{j})+N(_{j}^{};_{ 0:j},_{0:j})] \]

_where \(p(I^{()}=j)(1-)^{T(-1-j)}\) for \(j\{0,,-1\}\) and \(_{}=_{i=0}^{-1}(1-)^{T i}\)._

Corollary 3.2.1 is proved in Appendix B.4 and gives a whole picture of the optimality gap under AnCon. We first remark the trade-off associated with \(\) on the suboptimality \([l(_{})-l^{*}]\), which characterize the early-learning phenomenon observed in the biased gradient settings (e.g., self-training  and LFN ). Specifically, in (9), increasing \(\) reduces the first term \((1-)^{T(-1)}(l(_{0})-l^{*})\) but increases the coefficient of the second term \(_{}\). Therefore, a longer training with a large \(\) may not enhance the self-training performance especially under a large second term due to inaccurate pseudo labels or the temporal ensemble.

Nevertheless, for each outer loop iteration, \(g^{}(_{j})\) would be smaller under AnCon than its value under vanilla self-training as \([l(_{j})-l^{*}]\) has a tighter upper bound under AnCon due to Theorem 3.2. Therefore, with the guarantee \(N(_{j}^{};_{0:j},_{0:j}) N(0)\), AnCon would achieve a tighter upper bound of (9) than the vanilla self-training method, enabling longer training with smaller value of the second term as observed in Figure 0(b). Finally, we remark that this theoretical superiority of AnCon can be extended to the self-training methods with other weighting mechanisms in the asymptotic region when \(_{}[(_{}(^{*}),_{}(_{0:m },_{0:m}))] 0\) (cf. Theorem 3.2).

## 4 Experiments

**Goal and baselines** Part of the experiments shows that AnCon surpasses the vanilla self-training method that uses \((x;)\) as a pseudo label, which serves as a strong baseline, under different types

[MISSING_PAGE_FAIL:7]

Notably, AnCon significantly improves performances of both GCE and NRC via fundamentally different mechanisms for handling noisy pseudo labels (e.g., reducing test accuracy of 3% and 8% on average in OfficeHome, respectively). Specifically, NRC filters incorrect predictions based on local consistency, while AnCon uses temporal consistency. Combining NRC and AnCon leverages pseudo labels that are both locally and temporally consistent, resulting in significant performance improvements over NRC or Self-Training + AnCon (cf. Table 1). In addition, GCE reduces the impact of wrong pseudo labels rather than finding them. Applying GCE to AnCon minimizes the effects of potentially wrong but temporally consistent pseudo labels, which can be implied by the performance of GCE + AnCon compared to GCE or Self-Training + AnCon (cf. Table 1). Thus, the impressive performance gains from AnCon, which would be orthogonal to the gains from state-of-the-art methods in SFDA, show its significant practical implications.

### Self-training under synthetic corruption operations

While we have considered the domain shift, e.g., adaptation of a model trained on synthetic images to real images, in Section 4.1, this section examines the self-training's ability to adapt to distribution shifts by synthetic image corruptions. This setting has been used to measure the robustness of neural networks with respect to a general out-of-distribution setting. To this end, we consider **ImageNet-C**, which consists of 50,000 images drawn from a validation set of ImageNet  where each image is corrupted by 15 types of synthetic corruptions related to noise, blur, weather and digital.

**Result**: Consistent with the findings under the domain shift, AnCon outperforms the average performances of self-training and ELR under varying levels of corruption intensities (cf. Figure 0(a) and Tables 7-11 in Appendix), improving the self-training method's accuracy by 16% on average. Further, the gains from AnCon is significant when the distribution shifts are intense (e.g., improving accuracies by 20% and 52% on average in intensities of 4 and 5) where the initial model trained on the source domain significantly deteriorates. Specifically, for Shot, Impulse, and Gaussian corruptions with the most extreme shift intensity of 5, where the initial model achieves accuracies of (3.04%, 1.76%, 2.12%), AnCon achieves (22.56%, 26.56%, 25.85%) (cf. Table 11). This striking improvement compared to vanilla self-training with performances (0.26%, 1.72%, 1.04%) and ELR with performances (8.00%, 14.12%, 16.00%), underscores the importance of the AnCon's uncertainty-aware temporal consistency scheme, as shown in Corollary 3.2.1. We note that this impressive result is also explained by AnCon's ability to prevent the gradual performance degradation during the course of training with the extremely noisy pseudo labels (cf. Figure 0(b)). Combined with previous results in domain shift scenarios, we expect that AnCon would work effectively in various out-of-distribution settings.

### Versatility of AnCon

In previous sections, we have shown the universality of AnCon by evaluating it on diverse distribution shift scenarios. In this section, we show versatility of AnCon by analyzing its attractive properties in robustness and uncertainty representation.

#### 4.3.1 Robustness to model selection

There is no universally agreed model selection criterion, such as cross-validation in the i.i.d. setting, in self-training under distribution shifts. This is partly due to the variety of distribution shift scenarios, where an effective criterion in one may be ineffective or inapplicable in another; for instance, a

Figure 1: **Section 4.2: (a) Test accuracy for each intensity level in ImageNet-C. (b) Performance degeneration in the defocus blur corruption with intensity 4. Section 4.3.1: (c) Maximum performance changes under different model selection methods. We present performances for individual corruptions in Appendix. For all boxplots used in the paper, the box represents interquantile range with whiskers as \(\) 1.5 interquantile range and the horizontal line inside the box represents the median.**

principled criteria called importance-weighted cross validation  in UDA cannot be applied to SFDA. In this regard, it would be an important characteristic of a self-training method under distribution shift to be robust with respect to different choices of model selection criteria. Therefore, we evaluate robustness with respect to the following different model selection criteria: InfoMax , Corr-C , and Ent  (see Appendix D.3 for the description).

Figure 0(c) shows that AnCon's maximum performance change due to different model selection methods is much lower than that of other methods, especially under severe distribution shifts. This valuable advantage can be contributed to the property of AnCon that can prevent performance degeneration (cf. Figure 0(b)). Given that, in practice, we barely know when the model collapse happens and which model selection criteria are the best, the results highlight a significant practical value of AnCon.

#### 4.3.2 Robustness to the choice of hyperparameters

Throughout this paper, we have shown that our single configuration of parameters (\(=0.3,=0.9\)) work well across a wide range of benchmark problems. In this section, we aim to show our findings can be preserved when the hyperparameter values deviate from the default setting by performing a sensitivity analysis for values \(\{0.1,0.3,0.5,0.7,0.9\}\) and \(\{0.1,0.3,0.5,0.7,0.9\}\). We also test two frequently used annealing schedules that \(_{m}=m/I\) and \(_{m}=(1,2m/I)\), called full and half, respectively. Figure 2 shows that AnCon is stable even under extreme values of hyperparameters. Specifically, for both hyperparameters, the maximum average performance change is less than 1%, and \(\) barely impacts the performance of AnCon. Indeed, our analysis suggests to increase \(\) from our default setting; that is, to put a higher weight on the general temporal ensemble's prediction. Here, we note that our suboptimal choices of hyperparameters are due to our rigorous and practical hyperparameter choice. Given the challenging nature of hyperparameter optimization under distribution shifts, the stable performances of AnCon under arbitrary choices of hyperparameters would enable AnCon to be seamlessly applied to diverse practical settings.

#### 4.3.3 Improved calibration performance

We have shown that all self-training methods significantly improve the performance of the baseline method after the adaptation period. However, it is widely known that these noticeable improvements come with the price of sacrificing an uncertainty representation ability which is critical in real-world decision-making scenarios . Specifically, the calibration performance, which is the gap between the prediction confidence and accuracy, usually monotonically increases as self-training keeps reducing the uncertainty for all predictions during the course of training. In this regard, we analyze the calibration performance with respect to the expected calibration error (ECE; see Appendix for definition). Here, a lower ECE means a lower gap between confidence and accuracy.

As shown in Figure 2(a) and Table 5 in Appendix, AnCon gives much lower ECE compared to other methods. Considering ELR and GCE both have regularization effects, we conjecture that this phenomenon is due to selective regularization in AnCon that increases prediction confidences of samples only if the past confident predictions are consistent with the current prediction. Especially, in

Figure 3: (a) ECEs under five levels of intensities in ImageNet-C; (b) Accuracy and ECE changes during the course of training in VisDa.

Figure 2: Sensitivity analysis with respect to \(\) and \(\) on four domain pairs (Ar-Pr, Pr-Cl, Rw-Cl, Rw-Pr) in OfficeHome. Here, green triangles are means.

Figure 2(b) which confirms the accuracy-calibration dilemma in VisDa, AnCon is shown to limit the ECE increases during training compared to all other methods. That is, AnCon helps to significantly reduce the price of the calibration performance we need to pay for improving accuracy, which are both important measures in practice.

### Algorithmic design choices

Recall that we define \((x;_{0:m},_{0:m})=_{i=0}^{m}w_{i}(x) p(y|x, _{i})\) with our simple design choices: the relative thresholding for weighting scheme \(w_{i}(x)(c(x;_{i})>_{i}^{()})\) and hard prediction for \(p(y|x,_{i})\). In Appendix C.3, we found that our simple design choices are more appropriate for the distribution shift settings than several more sophisticated alternatives, which can be summarized as follows.

* More sophisticated weighting schemes (e.g., Entropy (\(w_{i}(x)[f(x;_{i})]\}}\))) reduce the self-training performance, despite being a more accurate measure of prediction uncertainty. We conjecture that the poor calibration performance of the neural network in self-training under distribution shifts prevents the sophisticated weighting schemes from accurately reflecting the goodness of the prediction.
* Various soft prediction schemes, which can give more information about the non-leading entry values, leads to performance reductions. We conjecture that the continuously increasing confidence in the later stage of self-training would make soft prediction ignore early-stage predictions which may be valuable to memorize.

## 5 Related work

Filtering incorrect pseudo labelsPopular confidence-based thresholding methods [13; 14; 9] fall short under distribution shifts since even high confident predictions can be highly incorrect. Therefore, recent advances in SFDA and TTA utilize higher order information to filter incorrect pseudo labels. For instance, based on the intuition that true labels of adjacent samples would be same, centroids for each predicted class can be maintained in the feature space and then the pseudo label for each input is corrected by the adjacent centroid . The idea of using per-class centroids has been extended to incorporate more general clustering structures [18; 17]. However, the neighborhood structure-based methods are computationally demanding due to storage of memory banks in the feature space and nearest neighbors search. Such computational complexity persists in other approaches, which are based on the consistency of multiple predictions from different augmentations  and models trained with different loss functions . Compared to these solutions, AnCon can efficiently estimate correct labels with only limited extra memory overhead of storing past predictions.

Learning from noisy labelsTreating pseudo labels as inherently noisy, techniques from the LFN literature have been integrated to self-training. For instance, the LFN literature has proposed robust loss functions that reduce impacts of random noisy labels [48; 33], and a recent large-scale experimental study shows the applicability of the generalized cross-entropy in the SFDA setting . The effectiveness of ELR on SFDA  bears a similar idea because ELR was developed to regularize the neural networks' tendencies to memorize incorrect labels . Despite their effectiveness, by nature, these approaches do not consider important characteristics of the unbounded and instance-dependent noise rates inherent in self-training under distribution shifts, which results in significant suboptimality in both theory and practice. However, by considering the unique characteristics of self-training under distribution shift, AnCon relaxes the conditions required to achieve optimality as well as boosts the self-training performance in diverse scenarios.

## 6 Conclusion

This paper introduces AnCon, which effectively improves self-training performances under diverse distribution shift scenarios by promoting selective temporal consistency based on confident predictions. As a result, AnCon effectively mitigates the detrimental effects of noisy pseudo labels without much computational overhead, unlike the previous methods. We show that AnCon not only advances our theoretical understanding of a generalized notion of temporal consistency in self-training but also can be a practical asset as a simple and effective self-training method with attractive properties. In Appendix C.4, we present limitations and future directions, such as adaptive determination of \(\), combining local and temporal consistency, and extending the selective temporal consistency in the sequential decision making problems.