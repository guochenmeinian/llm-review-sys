# Adaptive whitening with fast gain modulation

and slow synaptic plasticity

 Lyndon R. Duong\({}^{1,2}\)  Eero P. Simoncelli\({}^{1,2}\)  Dmitri B. Chklovskii\({}^{1,3}\)  David Lipshutz\({}^{1}\)

\({}^{1}\) Center for Computational Neuroscience, Flatiron Institute

\({}^{2}\) Center for Neural Science, New York University

\({}^{3}\) Neuroscience Institute, NYU Langone Medical School

{lyndon.duong, eero.simoncelli}@nyu.edu

{dchklovskii, dlipshutz}@flatironinstitute.org

###### Abstract

Neurons in early sensory areas rapidly adapt to changing sensory statistics, both by normalizing the variance of their individual responses and by reducing correlations between their responses. Together, these transformations may be viewed as an adaptive form of statistical whitening. Existing mechanistic models of adaptive whitening exclusively use either synaptic plasticity or gain modulation as the biological substrate for adaptation; however, on their own, each of these models has significant limitations. In this work, we unify these approaches in a normative multi-timescale mechanistic model that adaptively whitens its responses with complementary computational roles for synaptic plasticity and gain modulation. Gains are modified on a fast timescale to adapt to the current statistical context, whereas synapses are modified on a slow timescale to match structural properties of the input statistics that are invariant across contexts. Our model is derived from a novel multi-timescale whitening objective that factorizes the inverse whitening matrix into basis vectors, which correspond to synaptic weights, and a diagonal matrix, which corresponds to neuronal gains. We test our model on synthetic and natural datasets and find that the synapses learn optimal configurations over long timescales that enable adaptive whitening on short timescales using gain modulation.

## 1 Introduction

Individual neurons in early sensory areas rapidly adapt to changing sensory statistics by normalizing the variance of their responses [1; 2; 3]. At the population level, neurons also adapt by reducing correlations between their responses [4; 5]. These adjustments enable the neurons to maximize the information that they transmit by utilizing their entire dynamic range and reducing redundancies in their representations [6; 7; 8; 9]. A natural normative interpretation of these transformations is _adaptive whitening_, a context-dependent linear transformation of the sensory inputs yielding responses that have unit variance and are uncorrelated.

Decorrelation of the neural responses requires coordination between neurons and the neural mechanisms underlying such coordination are not known. Since neurons communicate via synaptic connections, it is perhaps unsurprising that most existing mechanistic models of adaptive whitening decorrelate neural responses by modifying the strength of these connections [10; 11; 12; 13; 14; 15; 16]. However, long-term synaptic plasticity is generally associated with long-term learning and memory , and thus may not be a suitable biological substrate for adaptive whitening (though short-term synapticplasticity has been reported ). On the other hand, there is extensive neuroscience literature on rapid and reversible gain modulation [19; 20; 21; 22; 23; 24; 25; 26]. Motivated by this, Duong et al.  proposed a mechanistic model of adaptive whitening in a neural circuit with _fixed_ synaptic connections that adapts exclusively by modifying the gains of interneurons that mediate communication between the primary neurons. They demonstrate that an appropriate choice of the fixed synaptic weights can both accelerate adaptation and significantly reduce the number of interneurons that the circuit requires. However, it remains unclear how the circuit _learns_ such an optimal synaptic configuration, which would seem to require synaptic plasticity.

In this study, we combine the learning and adaptation of synapses and gains, respectively, in a unified mechanistic neural circuit model that adaptively whitens its inputs over multiple timescales (Fig. 1). Our main contributions are as follows:

1. We introduce a novel adaptive whitening objective in which the (inverse) whitening matrix is factorized into a synaptic weight matrix that is optimized across contexts and a diagonal (gain) matrix that is optimized within each statistical context.
2. With this objective, we derive a multi-timescale online algorithm for adaptive whitening that can be implemented in a neural circuit comprised of primary neurons and an auxiliary population of interneurons with slow synaptic plasticity and fast gain modulation (Fig. 1).
3. We test our algorithm on synthetic and natural datasets, and demonstrate that the synapses learn optimal configurations over long timescales that enable the circuit to adaptively whiten its responses on short timescales exclusively using gain modulation.

Beyond the biological setting, multi-timescale learning and adaptation may also prove important in machine learning systems. For example, Mohan et al.  introduced "gain-tuning", in which the gains of channels in a deep denoising neural network (with pre-trained synaptic weights) are adjusted to improve performance on samples with out-of-distribution noise corruption or signal properties. The normative multi-timescale framework developed here offers a new approach to continual learning and test-time adaptation problems such as this.

## 2 Adaptive symmetric whitening

Consider a neural population with \(N\) primary neurons (Fig. 1). The stimulus inputs to the primary neurons are represented by a random \(N\)-dimensional vector \(\) whose distribution \(p(|c)\) depends

Figure 1: Adaptive whitening circuit, illustrated with \(N=2\) primary neurons and \(K=2\) interneurons. **Left**: Dashed ellipses representing the covariance matrices of 2D stimuli s drawn from different statistical contexts. **Center**: Primary neurons (shaded blue circles) receive feedforward stimulus inputs (shaded purple circles), \(\), and recurrent weighted inputs, \(-_{}\), from the interneurons (real circles), producing responses \(\). The interneurons receive weighted inputs, \(=^{}\), from the primary neurons, which are then multiplied elementwise by gains \(\) to generate their outputs, \(=\). The gains \(\) are modulated at a fast timescale to adaptively whiten within a specific stimulus context. Concurrently, the synaptic weights are optimized at a slower timescale to learn structural properties of the inputs across contexts. **Right:** Dashed unit circles representing the whitened circuit responses \(\) in each statistical context.

on a latent context variable \(c\). The stimulus inputs \(\) can be inputs to peripheral sensory neurons (e.g., the rates at which photons are absorbed by \(N\) cones) or inputs to neurons in an early sensory area (e.g., glomerulus inputs to \(N\) mitral cells in the olfactory bulb). Context variables can include location (e.g., a forest or a meadow) or time (e.g., season or time of day). For simplicity, we assume the context-dependent inputs are centered; that is, \(_{ p(|c)}[]=\), where \(_{ p(|c)}[]\) denotes the expectation over the conditional distribution \(p(|c)\) and \(\) denotes the vector of zeros. See Appx. A for a consolidated list of notation used throughout this work.

The goal of adaptive whitening is to linearly transform the inputs \(\) so that, conditioned on the context variable \(c\), the \(N\)-dimensional neural responses \(\) have identity covariance matrix; that is,

\[ =_{c}_{  p(|c)}[^{}]= _{N},\]

where \(_{c}\) is a context-dependent \(N N\) whitening matrix. Whitening is not a unique transformation--left multiplication of the whitening matrix \(_{c}\) by any \(N N\) orthogonal matrix results in another whitening matrix. We focus on symmetric whitening, also referred to as Zero-phase Components Analysis (ZCA) whitening or Mahalanobis whitening, in which the whitening matrix for context \(c\) is uniquely defined as

\[_{c} =_{ss}^{-1/2}(c), _{ss}(c) :=_{ p(|c)}[ ^{}],\] (1)

where we assume \(_{ss}(c)\) is positive definite for all contexts \(c\). This is the unique whitening transformation that minimizes the mean-squared difference between the inputs and the outputs .

To derive an algorithm that learns the symmetric whitening matrix \(_{c}\), we express \(_{c}\) as the solution to an appropriate optimization problem, which is similar to the optimization problem in [15, top of page 6]. For a context \(c\), we can write the _inverse_ symmetric whitening matrix \(_{c}:=_{c}^{-1}\) as the unique optimal solution to the minimization problem

\[_{c} =*{arg\,min}_{_{++}^{N}}f_{c}( ), f_{c}() :=(^{-1}_{ss}(c)+ ),\] (2)

where \(_{++}^{N}\) denotes the set of \(N N\) positive definite matrices.1 This follows from the fact that \(f_{c}()\) is strictly convex with its unique minimum achieved at \(_{c}\), where \(f_{c}(_{c})=2(_{c})\) (Appx. B.1). Existing recurrent neural circuit models of adaptive whitening solve the minimization problem in Eq. 2 by choosing a matrix factorization of \(_{c}\) and then optimizing the components [12; 13; 15; 27].

## 3 Adaptive whitening in neural circuits: a matrix factorization perspective

Here, we review two adaptive whitening objectives, which we then unify into a single objective that adaptively whitens responses across multiple timescales.

### Objective for adaptive whitening via synaptic plasticity

Pehlevan and Chklovskii  proposed a recurrent neural circuit model that whitens neural responses by adjusting the synaptic weights between the \(N\) primary neurons and \(K N\) auxiliary interneurons according to a Hebbian update rule. Their circuit can be derived by factorizing the context-dependent matrix \(_{c}\) into a symmetric product \(_{c}=_{c}_{c}^{}\) for some context-dependent \(N K\) matrix \(_{c}\). Substituting this factorization into Eq. 2 results in the synaptic plasticity objective in Table 1. In the recurrent circuit implementation, \(_{c}^{}\) denotes the feedforward weight matrix of synapses connecting primary neurons to interneurons and the matrix \(-_{c}\) denotes the feedback weight matrix of synapses connecting interneurons to primary neurons. Importantly, under this formulation, the circuit may reconfigure both the synaptic connections and synaptic strengths each time the context \(c\) changes, which runs counter to the prevailing view that synaptic plasticity implements long-term learning and memory .

### Objective for adaptive whitening via gain modulation

Duong et al.  proposed a neural circuit model with _fixed_ synapses that whitens the \(N\) primary responses by adjusting the multiplicative gains in a set of \(K\) auxiliary interneurons. To derive a neural circuit with gain modulation, they considered a novel diagonalization of the inverse whitening matrix, \(_{c}=_{N}+_{}(_{c}) _{}^{}\), where \(_{}\) is an arbitrary, but fixed \(N K\) matrix of synaptic weights (with \(K K_{N}:=N(N+1)/2\)) and \(_{c}\) is an adaptive, context-dependent real-valued \(K\)-dimensional vector of gains. Note that unlike the conventional eigen-decomposition, the number of elements along the diagonal matrix is significantly larger than the dimensionality of the input space. Substituting this factorization into Eq. 2 results in the gain modulation objective in Table 1. As in the synaptic plasticity model, \(_{}^{}\) denotes the weight matrix of synapses connecting primary neurons to interneurons while \(-_{}\) connects interneurons to primary neurons. In contrast to the synaptic plasticity model, the interneuron outputs are modulated by context-dependent multiplicative gains, \(_{c}\), that are adaptively adjusted to whiten the circuit responses.

Duong et al.  demonstrate that an appropriate choice of the fixed synaptic weight matrix can both accelerate adaptation and significantly reduce the number of interneurons in the circuit. In particular, the gain modulation circuit can whiten _any_ input distribution provided the gains vector \(_{c}\) has dimension \(K K_{N}\) (the number of degrees of freedom in an \(N N\) symmetric covariance matrix). However, in practice, the circuit need only adapt to input distributions corresponding to _natural_ input statistics [7; 30; 31; 32]. For example, the statistics of natural images are approximately translation-invariant, which significantly reduces the degrees of freedom in their covariance matrices, from \((N^{2})\) to \((N)\). Therefore, while the space of all possible correlation structures is \(K_{N}\)-dimensional, the set of natural statistics likely has far fewer degrees of freedom and an optimal selection of the weight matrix \(_{}\) can potentially offer dramatic reductions in the number of interneurons \(K\) required to adapt. As an example, Duong et al.  specify a weight matrix for performing "local" whitening with \((N)\) interneurons when the input correlations are spatially-localized (e.g., as in natural images). However, they do not prescribe a method for _learning_ a (synaptic) weight matrix that is optimal across the set of natural input statistics.

### Unified objective for adaptive whitening via synaptic plasticity and gain modulation

We unify and generalize the two disparate adaptive whitening approaches [12; 27] in a single _multi-timescale_ nested objective in which gains \(\) are optimized within each context and synaptic weights \(\) are optimized across contexts. In particular, we optimize, with respect to \(\), the expectation of the objective from  (for some fixed \(K 1\)) over the distribution of contexts \(p(c)\):

\[_{^{N K}}_{c p(c)}[_{ ^{K}}f_{c}(_{N}+()^{})],\] (3)

where we have also generalized the objective from  by including a fixed multiplicative factor \( 0\) in front of the identity matrix \(_{N}\), and we have relaxed the requirement that \(K K_{N}\).

What is an optimal solution of Eq. 3? Since the convex function \(f_{c}\) is uniquely minimized at \(_{c}\), a sufficient condition for the optimality of a synaptic weight matrix \(\) is that for each context \(c\), there is a gain vector \(_{c}\) such that \(_{N}+(_{c})^{}= _{c}\). Importantly, under such a synaptic configuration, the function \(f_{c}\) can attain its minimum exclusively by adjusting the gains vector \(\). In the space of covariance matrices, we can express the statement as

\[_{ss}(c)():=\{[_{N }+()^{}]^{2}: ^{K}\}_{++}^{N}c,\]

where \(()\) contains the set of covariance matrices that can be whitened with fixed synapses \(\) and adaptive gains \(\). Fig. 2 provides an intuitive Venn diagram comparing a non-optimal synaptic configuration \(_{0}\) and an optimal synaptic configuration \(_{T}\).

   Model & Matrix factorization & Objective \\  Synaptic plasticity  & \(_{c}_{c}^{}\) & \(_{}f_{c}(^{})\) \\ Gain modulation  & \(_{N}+_{}(_{c})_{ }^{}\) & \(_{}f_{c}(_{N}+_{}( )_{}^{})\) \\ Multi-timescale (ours) & \(_{N}+(_{c})^{}\) & \(_{}_{c p(c)}[_{}f_{c}( _{N}+()^{})]\) \\   

Table 1: Factorizations of the inverse whitening matrix \(_{c}\) and objectives for adaptive whitening circuits.

## 4 Multi-timescale adaptive whitening algorithm and circuit implementation

In this section, we derive an online algorithm for optimizing the multi-timescale objective in Eq. 3, then map the algorithm onto a neural circuit with fast gain modulation and slow synaptic plasticity. Direct optimization of Eq. 3 results in an offline (or full batch) algorithm that requires the network to have access to the covariance matrices \(_{ss}(c)\), Appx. C. Therefore, to derive an online algorithm that includes neural dynamics, we first add neural responses \(\) to the objective, which introduces a third timescale to the objective. We then derive a multi-timescale gradient-based algorithm for optimizing the objective.

Adding neural responses to the objective.First, observe that we can write \(f_{c}()\), for \(_{++}^{N}\), in terms of the neural responses \(\):

\[f_{c}()=_{ p(|c)}[_{ ^{N}}(2^{}- ^{}+)].\] (4)

To see this, maximize over \(\) to obtain \(=^{-1}\) and then use the definition of \(_{ss}(c)\) from Eq. 1 (Appx. B.2). Substituting this expression for \(f_{c}\), with \(=_{N}+()^{}\), into Eq. 3, dropping the constant term \(_{N}\) and using the cyclic property of the trace operator results in the following objective with 3 nested optimizations (Appx. B.2):

\[_{^{N K}}_{c p(c)} [_{^{K}}_{ p(|c)}[_{^{N}}(,, ,)]],\] (5) \[(,,,):=2^{}-\|\|^{2}-_{i=1}^{K}g_{i} [(_{i}^{})^{2}-\|_{i}\|^{2}].\]

The inner-most optimization over \(\) corresponds to neural responses and will lead to recurrent neural dynamics. The outer 2 optimizations correspond to the optimizations over the gains \(\) and synaptic weights \(\) from Eq. 3.

To solve Eq. 5 in the online setting, we assume there is a timescale separation between neural dynamics and the gain/weight updates. This allows us to perform the optimization over \(\) before optimizing \(\) and \(\) concurrently. This is biologically sensible: neural responses (e.g., action potential firing) operate on a much faster timescale than gain modulation and synaptic plasticity [25; 33].

Figure 2: Illustration of multi-timescale learning in the space of covariance matrices. Orange and purple regions (identical on the left and right) respectively represent the cone of all positive definite matrices \(_{++}^{N}\), and the subset of naturally-occurring covariance matrices \(\{_{ss}(c)\}\). Blue regions represent the set of covariance matrices that can be whitened with adaptive gains for a particular synaptic weight matrix. On each side, the yellow circle denotes a naturally-occurring input covariance matrix \(_{ss}(c)\) and the dotted white curve illustrates the trajectory of covariance matrices the circuit is adapted to whiten as the gains are modulated (with fixed synapses, note the dotted white curve remains in the blue region). **Left:** With initial synaptic weights \(_{0}\) the circuit cannot whiten some natural input distributions exclusively via gain modulation, i.e., \(\{_{ss}(c)\}(_{0})\). **Right:** After learning optimal synaptic weights \(_{T}\), the circuit can match any naturally-occurring covariance matrix using gain modulation, i.e., \(\{_{ss}(c)\}(_{T})\).

Recurrent neural dynamics.At each iteration, the circuit receives a stimulus \(\). We maximize \((,,,)\) with respect to \(\) by iterating the following gradient-ascent steps that correspond to repeated timesteps of the recurrent circuit (Fig. 1) until the responses equilibrate:

\[+_{r}(-_{i=1}^{K}n_{i} _{i}-),\] (6)

where \(_{r}>0\) is a small constant, \(z_{i}=_{i}^{}\) denotes the weighted input to the \(i^{}\) interneuron, \(n_{i}=g_{i}z_{i}\) denotes the gain-modulated output of the \(i^{}\) interneuron. For each \(i\), synaptic weights, \(_{i}\), connect the primary neurons to the \(i^{}\) interneuron and symmetric weights, \(-_{i}\), connect the \(i^{}\) interneuron to the primary neurons. From Eq. 6, we see that the neural responses are driven by feedforward stimulus inputs \(\), recurrent weighted feedback from the interneurons \(-\), and a leak term \(-\).

Fast gain modulation and slow synaptic plasticity.After the neural activities equilibrate, we minimize \((,,,)\) by taking concurrent gradient-descent steps

\[ g_{i} =_{g}(z_{i}^{2}-\|_{i}\|^{2})\] (7) \[_{i} =_{w}(n_{i}-_{i}g_{i}),\] (8)

where \(_{g}\) and \(_{w}\) are the respective learning rates for the gains and synaptic weights. By choosing \(_{g}_{w}\), we can ensure that the gains are updated at a faster timescale than the synaptic weights.

The update to the \(i^{}\) interneuron's gain \(g_{i}\) depends on the difference between the online estimate of the variance of its input, \(z_{i}^{2}\), and the squared-norm of the \(i^{}\) synaptic weight vector, \(\|_{i}\|^{2}\), quantities that are both locally available to the \(i^{}\) interneuron. Using the fact that \(z_{i}=_{i}^{}\), we can rewrite the gain update as \( g_{i}=_{g}[_{i}^{}(^{}- _{N})_{i}]\). From this expression, we see that the gains equilibrate when the marginal variance of the responses along the direction \(_{i}\) is 1, for \(i=1,,K\).

The update to the \((i,j)^{}\) synaptic weight \(w_{ij}\) is proportional to the difference between \(r_{i}n_{j}\) and \(w_{ij}g_{j}\), which depends only on variables that are available in the pre- and postsynaptic neurons. Since \(r_{i}n_{j}\) is the product of the pre- and postsynaptic activities, we refer to this update as _Hebbian_. In Appx. E.2, we decouple the feedforward weights \(_{i}^{}\) and feedback weights \(-_{i}\) and provide conditions under which the symmetry asymptotically holds.

Multi-timescale online algorithm.Combining the neural dynamics, gain modulation and synaptic plasticity yields our online multi-timescale adaptive whitening algorithm, which we express in vector-matrix form with '\(\)' denoting the Hadamard (elementwise) product of two vectors:

```
1:Input:\(_{1},_{2},^{N}\)
2:Initialize:\(^{N K}\); \(^{K}\); \(_{r}>0\); \(_{g}_{w}>0\)
3:for\(t=1,2,\)do
4:\(_{t}\)
5:while not converged do // interneuron inputs
6:\(_{t}^{}_{t}\) ; // gain-modulated interneuron outputs
7:\(_{t}_{t}+_{r}(_{t}-_{t}-_{t})\) ; // recurrent neural dynamics
8:endwhile
10:\(+_{g}(_{t}_{t}- (^{}))\) ; // gains update
11:\(+_{w}(_{t}_{t}^{ }-())\) ; // synaptic weights update
12:endfor ```

**Algorithm 1**Multi-timescale adaptive whitening via synaptic plasticity and gain modulation

Alg. 1 is naturally viewed as a _unification_ and generalization of previously proposed neural circuit models for adaptation. When \(=0\) and the gains \(\) are constant (e.g., \(_{g}=0\)) and identically equal to the vector of ones \(\) (so that \(_{t}=_{t}\)), we recover the synaptic plasticity algorithm from . Similarly, when \(=1\) and the synaptic weights \(\) are fixed (e.g., \(_{w}=0\)), we recover the gain modulation algorithm from .

## 5 Numerical experiments

We test Alg. 1 on stimuli \(_{1},_{2},\) drawn from slowly fluctuating latent contexts \(c_{1},c_{2},\); that is, \(_{t} p(|c_{t})\) and \(c_{t}=c_{t-1}\) with high probability.2 To measure performance, we evaluate the operator norm on the difference between the expected response covariance and the identity matrix:

\[(t)=\|_{t}^{-1}_{ss}(c_{t})_{t}^{-1 }-_{N}\|_{},_{t}:=_{N} +_{t}()_{t}^{}.\] (9)

Geometrically, this "worst-case" error measures the maximal Euclidean distance between the ellipsoid corresponding to \(_{t}^{-1}_{ss}(c_{t})_{t}^{-1}\) and the \((N-1)\)-sphere along all possible axes. To compare two synaptic weight matrices \(,^{N K}\), we evaluate \(\|}-\|_{F}\), where \(}=\) and \(\) is the permutation matrix (with possible sign flips) that minimizes the error.

### Synthetic dataset

To validate our model, we first consider a 2-dimensional synthetic dataset in which an optimal solution is known. Suppose that each context-dependent inverse whitening matrix is of the form \(_{c}=_{N}+(c)^{}\), where \(\) is a fixed \(2 2\) matrix and \((c)=(_{1}(c),_{2}(c))\) is a context-dependent diagonal matrix. Then, in the case \(=1\) and \(K=2\), an optimal solution of the objective in Eq. 3 is when the column vectors of \(\) align with the column vectors of \(\).

To generate this dataset, we chose the column vectors of \(\) uniformly from the unit circle, so they are _not_ generally orthogonal. For each context \(c=1,,64\), we assume the diagonal entries of \((c)\) are sparse and i.i.d.: with probability 1/2, \(_{i}(c)\) is set to zero and with probability 1/2, \(_{i}(c)\) is chosen uniformly from the interval \(\). Example covariance matrices from different contexts are shown in Fig. 3(a) (note that they do _not_ share a common eigen-decomposition). Finally, for each context, we generate 1E3 i.i.d. samples with context-dependent distribution \((,_{c}^{2})\).

We test Alg. 1 with \(=1\), \(K=2\), \(_{w}=1\)E-5, and \(_{g}=5\)E-2 on these sequences of synthetic inputs with the column vectors of \(_{0}\) chosen uniformly from the unit circle. The model successfully learns to whiten the different contexts, as indicated by the decreasing whitening error with the number of contexts presented (Fig. 3B). At the end of training, the synaptic weight matrix \(_{T}\) is optimized such that the circuit can adapt to changing contexts exclusively by adjusting its gains. This is evidenced by the fact that when the context changes, there is a brief spike in error as the gains adapt to the new context (Fig. 3C, red line). By contrast, the error remains high in many of the contexts when using

Figure 3: Adaptive whitening of a synthetic dataset with \(N=2\), \(_{w}=1\)E-5, \(_{g}=5\)E-2. **A)** Covariance ellipses (orange) of 4 out of 64 synthesized contexts. Black dashed lines are axes corresponding to the column vectors of \(\). The unit circle is shown in green. Since the column vectors of \(\) are not orthogonal, these covariance matrices do _not_ share a common set of eigenvectors (orange lines). **B)** Whitening error with a moving average window spanning the last 10 contexts. **C)** Error at each stimulus presentation within five different contexts (gray panels), presented with \(_{0}\), or \(_{T}\). **D)** Column vectors of \(_{0}\), \(_{T}\), \(\) (each axis corresponds to the span of one column vector in \(^{2}\)). **E)** Smoothed distributions of error (in Frobenius norm) between \(}\) and \(\) across 250 random initializations of \(_{0}\).

the initial random synaptic weight matrix \(_{0}\) (Fig. 3C, blue line). In particular, the synapses learn (across contexts) an optimal configuration in the sense that the column vectors of \(\) learn to align with the column vectors of \(\) over the course of training (Fig. 3DE).

### Natural images dataset

By hand-crafting a particular set of synaptic weights, Duong et al.  showed that their adaptive whitening network can approximately whiten a dataset of natural image patches with \((N)\) gain-modulating interneurons instead of \((N^{2})\). Here, we show that our model can exploit spatial structure across natural scenes to _learn_ an optimal set of synaptic weights by testing our algorithm on 56 high-resolution natural images  (Fig. 4A, top). For each image, which corresponds to a separate context \(c\), \(5 5\) pixel image patches are randomly sampled and vectorized to generate context-dependent samples \(^{25}\) with covariance matrix \(_{ss}(c)_{++}^{25}\) (Fig. 4A, bottom). We train our algorithm in the offline setting where we have direct access to the context-dependent covariance matrices (Appx. C, Alg. 2, \(=1\), \(J=50\), \(_{g}=\)5E-1, \(_{w}=\)5E-2) with \(K=N=25\) and random \(_{0} O(25)\) on a training set of 50 of the images, presented uniformly at random 1E3 total times. We find that the model successfully learns a basis that enables adaptive whitening _across_ different visual contexts via gain modulation, as shown by the decreasing training error (Eq. 9) in Fig. 4B.

How does the network learn to leverage statistical structure that is consistent across contexts? We test the circuit with fixed synaptic weights \(_{T}\) and modulated (adaptive) gains \(\) on stimuli from the held-out images (Fig. 4C, red distribution shows the smoothed error over 2E3 random initializations \(_{0}\)). The circuit performs as well on the held-out images as on the training images (Fig. 4C, red versus blue distributions). In addition, the circuit with learned synaptic weights \(_{T}\) and modulated gains \(\) outperforms the circuit with learned synaptic weights \(_{T}\) and fixed gains (Fig. 4C, green distribution), and significantly outperforms the circuit with random synaptic weights \(_{0}\) and modulated gains (Fig. 4C, purple distribution). Together, these results suggest that the circuit

Figure 4: Adaptive whitening of natural images. **A)** Examples of 2 out of 56 high-resolution images (top) with each image corresponding to a separate context. For each image, \(5 5\) pixel patches are randomly sampled to generate context-dependent stimuli with covariance matrix \(_{ss}(c)_{++}^{25}\) (bottom). **B)** Mean error during training (Eq. 9) with \(K=N=25\). Shaded region is standard deviation over 2E3 random initializations \(_{0} O(25)\). **C)** Smoothed distributions of average adaptive whitening error over all 2E3 initializations. The red distribution corresponds to the error on the held-out images with fixed learned synapses \(_{T}\) and modulated gains \(\). The blue (resp. green, purple) distribution corresponds to the same error, but tested on the training images (resp. with fixed gains equal to the average gains over the final 100 iterations, with fixed random synapses \(_{0}\)). **D)** The learned weights (re-shaped columns of \(_{T}\)) approximate orthogonal 2D sinusoids. **E)** Final error (after \(T=5\)E4 iterations) as a function of number of interneurons \(K\). Bars are standard deviations centered on the mean error at each \(K\). The red horizontal line denotes average error when \(K=0\) (in which case \(=\)). **F)** Frobenius norm between the eigenbasis of \(_{c p(c)}[_{ss}(c)]\) (i.e. across all contexts), \(_{*}\), with \(_{T}\), \(_{0}\), and the eigenbasis of each individual context covariance, \((c)\), when \(K=N=25\). See Appx. D for additional experiments.

learns features \(_{T}\) that enable the circuit to adaptively whiten across statistical contexts exclusively using gain modulation, and that gain modulation is crucial to the circuit's ability to adaptively whiten. In Fig. 4D, we visualize the learned filters (columns of \(_{T}\)), and find that they are approximately equal to the 2D discrete cosine transform (DCT, Appx. D), an orthogonal basis that is known to approximate the eigenvectors of natural image patch covariances [35; 36].

To examine the dependence of performance on the number of interneurons \(K\), we train the algorithm with \(K=1,,2N\) and report the final error in Fig. 4E. There is a steady drop in error as \(K\) ranges from 1 to \(N\), at which point there is a (discontinuous) drop in error followed by a continued, but more gradual decay in _both_ training and test images error as \(K\) ranges from \(N\) to \(2N\) (the overcomplete regime). To understand this behavior, note that the covariance matrices of image patches _approximately_ share an eigen-decomposition . To see this, let \((c)\) denote the orthogonal matrix of eigenvectors corresponding to the context-dependent covariance matrix \(_{ss}(c)\). As shown in Fig. 4F (green histogram), there is a small, but non-negligible, difference between the eigenvectors \((c)\) and the eigenvectors \(_{*}\) of the _average_ covariance matrix \(_{c p(c)}[_{ss}(c)]\). When \(K=N\), the column vectors of \(_{T}\) learn to align with \(_{*}\) (as shown in Fig. 4F, blue histogram), and the circuit _approximately_ adaptively whites the context-dependent stimulus inputs via gain modulation. As \(K\) ranges from 1 to \(N\), \(_{T}\) progressively learns the eigenvectors of \(_{*}\) (Appx. D). Since \(_{T}\) achieves a full set of eigenvectors at \(K=N\), this results in a large drop in error when measured using the operator norm. Finally, as mentioned, there is a non-negligible difference between the eigenvectors \((c)\) and the eigenvectors \(_{*}\). Therefore, increasing the number of interneurons from \(N\) to \(2N\) allows the circuit to learn an overcomplete set of basis vectors \(_{T}\) to account for the small deviations between \((c)\) and \(_{*}\), resulting in improved whitening error (Appx. D).

## 6 Discussion

We've derived an adaptive statistical whitening circuit from a novel objective (Eq. 3) in which the (inverse) whitening matrix is factorized into components that are optimized at different timescales. This model draws inspiration from the extensive neuroscience literature on rapid gain modulation  and long-term synaptic plasticity , and concretely proposes complementary roles for these computations: synaptic plasticity facilitates learning features that are invariant _across_ statistical contexts while gain modulation facilitates adaptation _within_ a statistical context. Experimental support for this will come from detailed understanding of natural sensory statistics across statistical contexts and estimates of (changes in) synaptic connectivity from wiring diagrams (e.g., ) or neural activities (e.g., ).

Our circuit uses local learning rules for the gain and synaptic weight updates that could potentially be implemented in low-power neuromorphic hardware  and incorporated into existing mechanistic models of neural circuits with whitened or decorrelated responses [40; 41; 42; 43; 44; 45]. However, there are aspects of our circuit that are not biologically realistic. For example, we do not sign-constrain the gains or synaptic weight matrices, so our circuit can violate Dale's law. In addition, the feedforward synaptic weights \(^{}\) and feedback weights \(-\) are constrained to be symmetric. In Appx. E, we describe modifications of our model that can make it more biologically realistic. Additionally, while we focus on the potential joint function of gain modulation and synaptic plasticity in adaptation, short-term synaptic plasticity, which operates on similar timescales as gain modulation, has also been reported . Theoretical studies suggest that short-term synaptic plasticity is useful in multi-timescale learning tasks [46; 47; 48] and it may also contribute to multi-timescale adaptive whitening. Ultimately, support for different adaptation mechanisms will be adjudicated by experimental observations.

Our work may also be relevant beyond the biological setting. Decorrelation and whitening transformations are common preprocessing steps in statistical and machine learning methods [49; 50; 51; 52; 53], and are useful for preventing representational collapse in recent self-supervised learning methods [54; 55; 56; 57]. Therefore, our online multi-timescale algorithm may be useful for developing adaptive self-supervised learning algorithms. In addition, our work is related to the general problem of online meta-learning [58; 59]; that is, learning methods that can rapidly adapt to new tasks. Our solution--which is closely related to mechanisms of test-time feature gain modulation developed for machine learning models for denoising , compression [60; 61], and classification --suggests a general approach to meta-learning inspired by neuroscience: structural properties of the tasks (contexts) are encoded in synaptic weights and adaptation to the current task (context) is achieved by adjusting the gains of individual neurons.