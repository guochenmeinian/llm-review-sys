# Learning DAGs from Data with Few Root Causes

Panagiotis Misiakos, Chris Wendler and Markus Puschel

Department of Computer Science, ETH Zurich

{pmisiakos, wendlerc, markusp}@ethz.ch

###### Abstract

We present a novel perspective and algorithm for learning directed acyclic graphs (DAGs) from data generated by a linear structural equation model (SEM). First, we show that a linear SEM can be viewed as a linear transform that, in prior work, computes the data from a dense input vector of random valued root causes (as we will call them) associated with the nodes. Instead, we consider the case of (approximately) few root causes and also introduce noise in the measurement of the data. Intuitively, this means that the DAG data is produced by few data generating events whose effect percolates through the DAG. We prove identifiability in this new setting and show that the true DAG is the global minimizer of the \(L^{0}\)-norm of the vector of root causes. For data satisfying the few root causes assumption, we show superior performance compared to prior DAG learning methods.

## 1 Introduction

We consider the problem of learning the edges of an unknown directed acyclic graph (DAG) given data indexed by its nodes. DAGs can represent causal dependencies (edges) between events (nodes) in the sense that an event only depends on its predecessors. Thus, DAG learning has applications in causal discovery, which, however, is a more demanding problem that we do not consider here, since it requires further concepts of causality analysis, in particular interventions (Peters et al., 2017). However, DAG learning is still NP-hard in general (Chickering et al., 2004). Hence, in practice, one has to make assumptions on the data generating process, to infer information about the underlying DAG in polynomial time. Recent work on DAG learning has focused on identifiable classes of causal data generating processes. A frequent assumption is that the data follow a structural equation model (SEM) (Shimizu et al., 2006; Zheng et al., 2018; Gao et al., 2021), meaning that the value of every node is computed as a function of the values of its direct parents plus noise. Zheng et al. (2018) introduced NOTEARS, a prominent example of DAG learning, which considers the class of linear SEMs and translates the acyclicity constraint into a continuous form for easier optimization. It inspired subsequent works to use continuous optimization schemes for both linear (Ng et al., 2020) and nonlinear SEMs (Lachapelle et al., 2019; Zheng et al., 2020). A more expansive discussion of related work is provided in Section 4.

In this paper we also focus on linear SEMs but change the data generation process. We first translate the common representation of a linear SEM as recurrence into an equivalent closed form. In this form, prior data generation can be viewed as linearly transforming an i.i.d. random, dense vector of root causes (as we will call them) associated with the DAG nodes as input, into the actual data on the DAG nodes as output. Then we impose sparsity in the input (few root causes) and introduce measurement noise in the output. Intuitively, this assumption captures the reasonable situation that DAG data may be mainly determined by few data generation events of predecessor nodes that percolate through the DAG as defined by the linear SEM. Note that our use of the term root causes is related to but different from the one in the root cause analysis by Ikram et al. (2022).

**Contributions.** We provide a novel DAG learning method designed for DAG data generated by linear SEMs with the novel assumption of few root causes. Our specific contributions include the following:* We provide an equivalent, closed form of the linear SEM equation showing that it can be viewed as a linear transform obtained by the reflexive-transitive closure of the DAG's adjacency matrix. In this form, prior data generation assumed a dense, random vector as input, which we call root causes.
* In contrast, we assume a sparse input, or few root causes (with noise) that percolate through the DAG to produce the output, whose measurement is also subject to noise. Interestingly, Seifert et al. (2023) identify the assumption of few root causes as a form of Fourier-sparsity.
* We prove identifiability of our proposed setting under weak assumptions, including zero noise in the measurements. We also show that, given enough data, the original DAG is the unique minimizer of the associated optimization problem in the case of absent noise.
* We propose a novel and practical DAG learning algorithm, called SparseRC, for our setting, based on the minimization of the \(L^{1}\)-norm of the approximated root causes.
* We benchmark SparseRC against prior DAG learning algorithms, showing significant improvements for synthetic data with few root causes and scalability to thousands of nodes. SparseRC also performs among the best on real data from a gene regulatory network, demonstrating that the assumption of few root causes can be relevant in practice.

## 2 Linear SEMs and Root Causes

We first provide background on prior data generation for directed acyclic graphs (DAGs) via linear SEMs. Then we present a different viewpoint on linear SEMs based on the concept of root causes that we introduce. Based on it, we argue for data generation with few root causes, present it mathematically and motivate it, including with a real-world example.

**DAG.** We consider DAGs \(=(V,E)\) with \(|V|=d\) vertices, \(E\) the set of directed edges, no cycles including no self-loops. We assume the vertices to be sorted topologically and set accordingly \(V=\{1,2,...,d\}\). Further, \(a_{ij}\) is the weight of edge \((i,j) E\), and

\[=(a_{ij})_{i,j V}=a_{ij},\,\,(i,j) E,\\ 0,\,.\] (1)

is the weighted adjacency matrix. \(\) is upper triangular with zeros on the diagonal and thus \(^{d}=\).

**Linear SEM.** Linear SEMs (Peters et al., 2017) formulate a data generating process for DAGs \(\). First, the values at the sources of a DAG \(\) are initialized with random noise. Then, the remaining nodes are processed in topological order: the value \(x_{j}\) at node \(j\) is assigned the linear combination of its parents' values (called the causes of \(x_{j}\)) plus independent noise. Mathematically, a data vector \(=(x_{1},...,x_{d})^{d}\) on \(\) follows a linear SEM if \(x_{j}=_{:,j}+n_{j}\), where the subscript \(:,j\) denotes column \(j\), and \(n_{j}\) are i.i.d. random noise variables (Shimizu et al., 2006; Zheng et al., 2018; Ng et al., 2020). Assuming \(n\) such data vectors collected as the rows of a matrix \(^{n d}\), and the noise variables in the matrix \(\), the linear SEM is typically written as

\[=+.\] (2)

### Transitive closure and root causes

Equation (2) can be viewed as a recurrence for computing the data values \(\) from \(\). Here, we interpret linear SEMs differently by first solving this recurrence into a closed form. We define \(}=+^{2}+...+^{d-1}\), which is the Floyd-Warshall (FW) transitive closure of \(\) over the ring \((,+,)\)(Lehmann, 1977), and \(+}\) the associated reflexive-transitive closure of \(\). Since \(^{d}=\) we have \((-)(+})=\) and thus can isolate \(\) in (2):

**Lemma 2.1**.: _The linear SEM (2) computes data \(\) as_

\[=(+}).\] (3)

_In words, the data values in \(\) are computed as linear combinations of the noise values \(\) of all predecessor nodes with weights given by the reflexive-transitive closure \(+}\)._Since \(\) is uniquely determined by \(\), we call the latter the _root causes_ of \(\). The root causes must not be confused with the root nodes (sources) of the DAG, which are the nodes without parents. In particular, \(\) can be non-zero in nodes that are not sources.

**Few root causes.** In (3) we view \(\) as the input to the linear SEM at each node and \(\) as the measured output at each node. With this viewpoint, we argue that it makes sense to consider a data generation process that differs in two ways from (3) and thus (2). First, we assume that only few nodes produce a relevant input that we call \(\), up to low magnitude noise \(_{c}\). Second, we assume that the measurement of \(\) is subject to noise \(_{x}\). Formally, this yields the closed form

\[=(+_{c})(+})+_{x}.\] (4)

Multiplying both sides by \((+})^{-1}=(-)\) yields the (standard) form as recurrence

\[=+(+_{c})+ _{x}(-),\] (5)

i.e., \(\) in (2) is replaced by \(+_{c}+_{x}(-)\), which in general is not i.i.d. Note that (4) generalizes (3), obtained by assuming zero root causes \(=\) and zero measurement noise \(_{x}=\).

We consider \(\) not as an additional noise variable but as the actual information, i.e., the relevant input data at each node, which then percolates through the DAG as determined by the SEM to produce the final output data \(\), whose measurement, as usual in practice, is subject to noise. Few root causes mean that only few nodes input relevant information in one dataset. This assumption can be stated as:

**Definition 2.2** (Few Root Causes assumption).: We assume that the input data \((+_{c})+_{x}(-)\) to a linear SEM are approximately sparse. Formally this holds if:

\[\|\|_{0}/nd<\] (few root causes), \[_{c}+_{x}(- )\|_{1}/nd}{\|\|_{1}/\|\|_{0}}< \] (6)

where \(\|\|_{0}\) counts the nonzero entries of \(\), \(\|\|_{1}\) is elementwise, and \(,\) are small constants.

For example, in Appendix A we show that (6) holds (in expectation) for \(==0.1\) if \(_{c},_{x}\) are normally distributed with zero mean and standard deviation \(=0.01\) and the ground truth DAG \(\) has uniformly random weights in \([-1,1]\) and average degree \(4\). The latter two conditions bound the amplification of \(_{x}\) in (6) when multiplied by \(-\).

The term root cause in the root cause analysis by Ikram et al. (2022) refers to the ancestor that was the only cause of some effect(s). If there is only one root cause in our sense, characterized by a non-zero value, then determining its location is the problem considered by Ikram et al. (2022).

### Example: Pollution model

Linear SEMs and associated algorithms have been extensively studied in the literature (Loh and Buhlmann, 2014; Peters and Buhlmann, 2014; Ghoshal and Honorio, 2017; Aragam and Zhou, 2015). However, we are not aware of any real-world example given in a publication. The motivation for using linear SEMs includes the following two reasons: \(d\)-variate Gaussian distributions can always be expressed as linear SEMs (Aragam and Zhou, 2015) and linearity is often a workable assumption when approximating non-linear systems.

Our aim is to explain why we propose the assumption of few root causes. The high-level intuition is that it can be reasonable to assume that, with the view point of (3), the relevant DAG data is triggered by sparse events on the input size and not by random noise. To illustrate this, we present a linear SEM that describes the propagation of pollution in a river network.

**DAG.** We assume a DAG describing a river network. The acyclicity is guaranteed since flows only occur downstream. The nodes \(i V\) represent geographical points of interest, e.g., cities, and edges are rivers connecting them. We assume that the cities can pollute the rivers. An edge weight \(a_{ij}\), \((i,j) E\), captures what fraction of a pollutant inserted at \(i\) reaches the neighbour \(j\). An example DAG with six nodes is depicted in Fig. 0(a).

**Transitive closure.** Fig. 0(b) shows the transitive closure of the DAG in (a). The \((i,j)\)-th entry of the transitive closure is denoted with \(_{ij}\) and represents the total fraction of a pollutant at \(i\) that reaches \(j\) via all connecting paths.

**Data and root causes.** Fig. 1c shows a possible data vector \(\) on the DAG, for example, the pollution measurement at each node done once a day (and without noise in this case). The measurement is the accumulated pollution from all upstream nodes. Within the model, the associated root causes \(\) in Fig. 1d then show the origin of the pollution, two in this case. Sparsity in \(\) means that each day only a small number of cities pollute. Negligible pollution from other sources is captured by noise \(_{c}\) and \(_{x}\) models the noise in the pollution measurements (both are assumed to be \(0\) in Fig. 1).

We view the pollution model as an abstraction that can be transferred to other real-world scenarios. For example, in gene networks that measure gene expression, few root causes would mean that few genes are activated in a considered dataset. In a citation network where one measures the impact of keywords/ideas, few root causes would correspond to the few origins of them.

## 3 Learning the DAG

In this section we present our approach for recovering the DAG adjacency matrix \(\) from given data \(\) under the assumption of few root causes, i.e., (4) (or (5)) and (6). We first show that under no measurement noise, our proposed setting is identifiable. Then we theoretically analyze our data generating model in the absence of noise and prove that the true DAG adjacency \(\) is the global minimizer of the \(L^{0}\)-norm of the root causes. Finally, to learn the DAG in practice, we perform a continuous relaxation to obtain an optimization problem that is solvable with differentiation.

### Identifiability

The rows of the data \(\) generated by a linear SEM are commonly interpreted as observations of a random row vector \(X=(X_{1},...,X_{d})\)(Peters and Buhlmann, 2014; Zheng et al., 2018), i.e., it is written analogous to (2) as \(X_{i}=X_{,i}+N_{i}\), where \(N_{i}\) are i.i.d. zero-mean random noise variables, or, equivalently, as \(X=X+N\). Given a distribution \(P_{N}\) of the noise vector, the DAG with graph adjacency matrix \(\) is then called identifiable if it is uniquely determined by the distribution \(P_{X}\) of \(X\). Shimizu et al. (2006) state that \(\) is identifiable if all \(N_{i}\) are non-Gaussian noise variables. This applies directly to our setting and yields the following result.

**Theorem 3.1**.: _Assume the data generation \(X=(C+N_{c})(+})\), where \(C\) and \(N_{c}\) are independent. Let \(p[0,1)\) and assume the \(C_{i}\) are independent random variables taking uniform values from \(\) with probability \(p\), and are \(=0\) with probability \(1-p\). The noise vector \(N_{c}\) is defined as before. Then the DAG given by \(\) is identifiable._

Proof.: Using (5), the data generation equation can be viewed as a linear SEM (in standard recursive form) with noise variable \(C+N_{c}\), which is non-Gaussian due to Levy-Cramer decomposition theorem (Levy, 1935; Cramer, 1936), because \(C\) is non-Gaussian. The statement then follows from LiNGAM (Shimizu et al., 2006), given that the noise variables of \(C\) and \(N_{c}\) are independent. Note that this independency would be violated if we assumed non-zero measurement noise \(N_{x}\), as in (4). 

In Theorem 3.1, \(C\) yields sparse observations whenever \(p\) is close to zero (namely \(dp\) root causes in expectation). However, identifiability follows for all \(p\) due to non-Gaussianity.

Figure 1: (a) A DAG for a river network. The weights capture fractions of pollution transported between adjacent nodes. (b) The transitive closure. The weights are fractions of pollution transported between all pairs of connected nodes. (c) A possible vector measuring pollution, and (d) the root causes of the pollution, sparse in this case.

### \(L^{0}\) minimization problem and global minimizer

Suppose that the data \(\) are generated via the noise-free version of (4):

\[=(+}).\] (7)

We assume \(\) to be generated as in Theorem 3.1, i.e., with randomly uniform values from \(\) with probability \(p\) and \(0\) with probability \(1-p\), where \(p\) is small such that \(\) is sparse. Given the data \(\) we aim to find \(\) by enforcing maximal sparsity on the associated \(\), i.e., by minimizing \(\|\|_{0}\):

\[_{^{d d}}\|(+ })^{-1}\|_{0} \] (8)

The following Theorem 3.2 states that given enough data, the true \(\) is the unique global minimizer of (8). This means that in that case the optimization problem (8) correctly specifies the true DAG. One can view the result as a form of concrete, non-probabilistic identifiability. Note that Theorem 3.2 is not a sample-complexity result, but a statement of exact reconstruction in the absence of noise. The sample complexity of our algorithm is empirically evaluated later.

**Theorem 3.2**.: _Consider a DAG with weighted adjacency matrix \(\) with \(d\) nodes. Given exponential (in \(d\)) number \(n\) of samples \(\) the matrix \(\) is, with high probability, the global minimizer of the optimization problem (8)._

Proof.: See Appendix C. 

### Continuous relaxation

In practice the optimization problem (8) is too expensive to solve due its combinatorial nature, and, of course, the noise-free assumption rarely holds in real-world data. We now consider again our general data generation in (4) assuming sparse root causes \(\) and noises \(_{c},_{x}\) satisfying the criterion (6). To solve it we consider a continuous relaxation of the optimization objective. Typically (e.g., (Zheng et al., 2018; Lee et al., 2019; Bello et al., 2022)), continuous optimization DAG learning approaches have the following general formulation:

\[_{^{d d}}(, )+R()\] (9) \[\ h()=0,\]

where \((,)\) is the loss function corresponding to matching the data, \(R()\) is a regularizer that promotes sparsity in the adjacency matrix, usually equal to \(\|\|_{1}\), and \(h()\) is a continuous constraint enforcing acyclicity.

In our case, following a common practice in the literature, we substitute the \(L^{0}\)-norm from (8) with its convex relaxation (Ramirez et al., 2013), the \(L^{1}\)-norm. Doing so allows for some robustness to (low magnitude) noise \(_{c}\) and \(_{x}\). We capture the acyclicity with the continuous constraint \(h()=tr(e^{})-d\) used by Zheng et al. (2018), but could also use the form of Yu et al. (2019). As sparsity regularizer for the adjacency matrix and we choose \(R()=\|\|_{1}\). Hence, our final continuous optimization problem is

\[_{^{d d}}\|( +})^{-1}\|_{1}+\| \|_{1}\ h()=0.\] (10)

We call this method SparseRC (sparse root causes).

## 4 Related work

The approaches to solving the DAG learning problem fall into two categories: combinatorial search or continuous relaxation. In general, DAG learning is an NP-hard problem since the combinatorial space of possible DAGs is super-exponential in the number of vertices (Chickering et al., 2004). Thus, methods that search the space of possible DAGs apply combinatorial strategies to find an approximation of the ground truth DAG (Ramsey et al., 2017; Chickering, 2002; Tsamardinos et al., 2006).

**Continuous optimization.** Lately, with the advances of deep learning, researchers have been focusing on continuous optimization methods (Vowels et al., 2021) modelling the data generation process using SEMs. Among the first methods to utilize SEMs were CAM (Buhlmann et al., 2014) and LiNGAM (Shimizu et al., 2006), which specializes to linear SEMs with non-Gaussian noise. NOTEARS by Zheng et al. (2018) formulates the combinatorial constraint of acyclicity as a continuous one, which enables the use of standard optimization algorithms. Despite concerns, such as lack of scale-invariance (Kaiser and Sipos, 2022, Reisach et al., 2021), it has inspired many subsequent DAG learning methods. The current state-of-the-art of DAG learning methods for linear SEMs include DAGMA by Bello et al. (2022), which introduces a log-det acyclicity constraint, and GOLEM by Ng et al. (2020), which studies the role of the weighted adjacency matrix sparsity, the acyclicity constraint, and proposes to directly minimize the data likelihood. Zheng et al. (2020) extended NOTEARS to apply in non-linear SEMs. Other nonlinear methods for DAG learning include DAG-GNN by Yu et al. (2019), in which also a more efficient acyclicity constraint than the one in NOTEARS is proposed, and DAG-GAN by Gao et al. (2021). DAG-NoCurl by Yu et al. (2021) proposes learning the DAG on the equivalent space of weighted gradients of graph potential functions. Pamfil et al. (2020) implemented DYNATERS, a variation of NOTEARS, compatible with time series data. A recent line of works considers permutation-based methods to parameterize the search space of the DAG (Charpentier et al., 2022, Zantedeschi et al., 2022). The work in this paper considers DAG learning under the new assumption of few root causes. In Misiakos et al. (2024) we further build on this work by learning graphs from time series data.

**Fourier analysis.** Our work is also related to the recently proposed form of Fourier analysis on DAGs from Seifert et al. (2023, 2022), where equation (3) appears as a special case. The authors argue that (what we call) the root causes can be viewed as a form of spectrum of the DAG data. This means the assumption of few root causes is equivalent to Fourier-sparsity, a frequent assumption for common forms of Fourier transform (Hassanieh, 2018, Stobbe and Krause, 2012, Amrollahi et al., 2019). Experiments in (Seifert et al., 2023, Misiakos et al., 2024) reconstruct data from samples under this assumption.

## 5 Experiments

We experimentally evaluate our DAG learning method SparseRC with both synthetically generated data with few root causes and real data from gene regulatory networks (Sachs et al., 2005). We also mention that our method was among the winning solutions (Misiakos et al., 2023, Chevalley et al., 2023) of a competition of DAG learning methods to infer causal relationships between genes (Chevalley et al., 2022). The evaluation was done on non-public data by the organizing institution and is thus not included here.

**Benchmarks.** We compare against prior DAG learning methods suitable for data generated by linear SEMs with additive noise. In particular, we consider the prior GOLEM (Ng et al., 2020), NOTEARS (Zheng et al., 2018), DAGMA (Bello et al., 2022), DirectLiNGAM (Shimizu et al., 2011), PC (Spirtes et al., 2000) and the greedy equivalence search (GES) (Chickering, 2002). We also compared SparseRC against LiNGAM (Shimizu et al., 2006), causal additive models (CAM) (Buhlmann et al., 2014), DAG-NoCurl (Yu et al., 2021), fast greedy equivalence search (fGES) (Ramsey et al., 2017), the recent simple baseline sortnregress (Reisach et al., 2021) and max-min hill-climbing (MMHC) (Tsamardinos et al., 2006), but they where not competitive and thus we only include the results in Appendix B.

**Metrics.** To evaluate the found approximation \(}\) of the true adjacency matrix \(\), we use common performance metrics as in (Ng et al., 2020, Reisach et al., 2021). In particular, the structural Hamming distance (SHD), which is the number of edge insertions, deletions, or reverses needed to convert \(}\) to \(\), and the structural intervention distance (SID), introduced by Peters and Buhlmann (2015), which is the number of falsely estimated intervention distributions. SHD gives a good estimate of a method's performance when it is close to \(0\). In contrast, when it is high it can either be that the approximated DAG has no edges in common with the ground truth, but it can also be that the approximated DAG contains all the edges of the original one together with some false positives. Thus, in such cases, we also consider the true positive rate (TPR) of the discovered edges. We also report the total number of nonzero edges discovered for the real dataset (Sachs et al., 2005) and, for the synthetic data, provide results reporting the TPR, SID, and normalized mean square error (NMSE) metrics in Appendix B. For the methods that can successfully learn the underlying graph we furtherproceed on approximating the true root causes \(\) that generated the data \(\) via (4). With regard to this aspect, we count the number of correctly detected root causes \(\) TPR, their false positive rate \(\) FPR and the weighted approximation \(\) NMSE. We also report the runtime for all methods in seconds. For each performance metric, we compute the average and standard deviation over five repetitions of the same experiment.

**Our implementation.** To solve (10) in practice, we implemented1 a PyTorch model with a trainable parameter representing the weighted adjacency matrix \(\). This allows us to utilize GPUs for the acceleration of the execution of our algorithm (GOLEM uses GPUs, NOTEARS does not). Then we use the standard Adam (Kingma and Ba, 2014) optimizer to minimize the loss defined in (10).

### Evaluation on data with few root causes

**Data generating process and defaults.** In the second, blue column of Table 1 we report the default settings for our experiment. We generate a random Erdos-Renyi graph with \(d=100\) nodes and assign edge directions to make it a DAG as in (Zheng et al., 2018). The ratio of edges to vertices is set to \(4\), so the number of edges is \(400\). The entries of the weighted adjacency matrix are sampled uniformly at random from \((-b,-a)(a,b)\), where \(a=0.1\) and \(b=0.9\). As in (Zheng et al., 2018; Ng et al., 2020; Bello et al., 2022) the resulting adjacency matrix is post-processed with thresholding. In particular, the edges with absolute weight less than the threshold \(=0.09\) are discarded. Next, the root causes \(\) are instantiated by setting each entry either to some random uniform value from \((0,1)\) with probability \(p=0.1\) or to \(0\) with probability \(1-p=0.9\) (thus, as in Theorem 3.1, the location of the root causes will vary). The data matrix \(\) is computed according to (4), using Gaussian noise \(_{c},_{x}\) of standard deviation \(=0.01\) to enforce (6). Finally, we do not standardize (scale for variance \(=1\)) the data, and \(\) contains \(n=1000\) samples (number of rows).

**Experiment 1: Different application scenarios.** Table 1 compares SparseRC to six prior algorithms using the SHD metric. Every row corresponds to a different experiment that alters one particular hyperparameter of the default setting, which is the first row with values of the blue column as explained above. For example, the second row only changes the graph type from Erdos-Renyi to scale-free, while keeping all other settings. Note that in the last row, fixed support means that the location of the root causes is fixed for every sample in the data.

Since the graphs have 400 edges, an SHD \( 400\) can be considered as good and beyond 400 can be considered a failure. In row 3 this threshold is analogously set to \(1000\). The failure cases are indicated as such in Table 1, or the SHD is shown in green if they still achieve a good TPR \(>0.8\). In Appendix B.1 we report the TPR, SID, and the runtimes of all methods.

In the first three rows, we examine scenarios that perfectly match the condition (6) of few root causes. For the default settings (row 1), scale-free graphs (row 2), and different noise distribution (row 3) SparseRC performs best and almost perfectly detects all edges. The next experiments alter parameters that deteriorate the few root causes assumption. Row 4 considers DAGs with average degree \(10\), i.e., about \(1000\) edges. High degree can amplify the measurement noise and hinder the root causes assumption (6). However, our method still performs best, but even larger degrees decay the performance of all methods as shown in experiment 5 below. The standardization of data (row

    & Default & Change & Varset & SparseRC (ours) & GOLEM & NOTEARS & DAGMA & Direct-LNOGAN & PC & GES \\   & Default setting & & 0.55 & \(\) & \(82.3 3.0\) & \(59.2 2.0\) & \(29.0 6.0\) & \(282.34 34\) & \(247 0.0\) & failure \\
2 & Graph type & Erdos-R & Scale-free & 0.99 & \(\) & \(84.9 9.0\) & \(28.9 9.5\) & \(296.4 148\) & \(188.8 27\) & \(273 16\) & \(375 141\) \\
3 & \(_{c},_{x}\) distribution & Gaussian & Gumbel & 0.97 & \(\) & \(87.2 4.4\) & \(59.17 278\) & \(278.4 4.3\) & \(250.1 17\) & \(396 33\) \\
4 & Edges/ Vertices & 10 & 0.99 & \(\) & \(212.7 10.3 26\) & \(396.9 10\) & \(178.1 1.65\) & \(973.17\) & failure \\
5 & Standardization & No & Yes & 0.50 & \(62.4 1.8\) & failure & failure & failure & \(278.25 254\) & \(\) & failure \\
6 & Larger weights in \(\) & \(\) & \((0.52 1.0\) & failure & \( 25\) & \(\) & \(299.4 1.4\) & \(504 121\) & \(490 98\) & failure \\
7 & \(_{c},\) & \(=0.1\) & \(=0.1\) & \(=0.57\) & \(50.1 1.9\) & \(98.4 14\) & \(199.1 12\) & \(238 14\) & \(538 45\) & \(255 11\) & failure \\
8 & Dense root causes \(\) & \(=0.1\) & \(p=0.5\) & \(98.12 3.3\) & \(229.5 2.5\) & \(126.3 3.2\) & \(53.8 3.0\) & \(257 14\) & \(244 12\) & failure \\
9 & Samples & \(n=1000\) & \(n=100\) & \(97.7\) & \(203.6 9.2\) & failure & failure & \(\) & failure & \(321.12\) & failure \\
10 & Fixed support & No & Yes & 0.99 & failure & failure & failure & failure & failure & \(\) & failure \\   

Table 1: SHD metric (lower is better) for learning DAGs with 100 nodes and 400 edges. Each row is an experiment. The first row is the default, whose settings are in the blue column. In each other row, exactly one default parameter is changed (Change column). The last six columns correspond to prior algorithms. The best results are shown in bold. Entries with \(>400\) are reported as _failure_ or shown in green if the TPR is \(>0.8\).

5) is generally known to negatively affect algorithms with continuous objectives (Reisach et al., 2021) as is the case here. Also, standardization changes the relative scale of the root causes and as a consequence affects (6). Row 6 considers edge weights \(>1\) which amplifies the measurement noise in (6) and our method fails. Higher standard deviation in the root causes noise (row 7) or explicitly higher density in \(\) (row 8) decreases performance overall. However, In both cases SparseRC still discovers a significant amount of the unknown edges. For a small number of samples (row 9) most methods fail. Ours achieves a high TPR but requires more data to converge to the solution as we will see in experiment 2 below. Row 10 is out of the scope of our method and practically all methods fail.

Overall, the performance of SparseRC depends heavily on the degree to which the assumption of few root causes (6) is fulfilled. The parameter choices in the table cover a representative set of possibilities from almost perfect recovery (row 1) to complete failure (row 10).

SparseRC is also significantly faster than the best competitors GOLEM and NOTEARS with typical speedups in the range of \(10\)-\(50\) as shown in Table 2. It is worth mentioning that even though LiNGAM provides the identifiability Theorem 3.1 of our proposed setting, it is not able to recover the true DAG. While both DirectLiNGAM and LiNGAM come with theoretical guarantees for their convergence, these require conditions, such as infinite amount of data, which in practice are not met. We include a more extensive reasoning for their subpar performance in Appendix B.4 together a particularly designed experiment for LiNGAM and DirectLiNGAM.

**Varsortability.** In Table 1 we also include the varsortability for each experimental setting. Our measurements for Erdos-Renyi graphs (all rows except row 2) are typically \(1\)-\(2\%\) lower than those reported in (Reisach et al., 2021, Appendix G.1) for linear SEMs, but still high in general. However, the trivial variance-sorting method sortnregress, included in Appendix B, fails overall. Note again that for fixed sparsity support (last row), all methods fail and varsortability is lower. Therefore, in this scenario, our data generating process poses a hard problem for DAG learning.

**Experiment 2: Varying number of nodes or samples.** In this experiment we first benchmark SparseRC with varying number of nodes (and thus number of edges) in the ground truth DAG, while keeping the data samples in \(\) equal to \(n=1000\). Second, we vary the number of samples, while keeping the number of nodes fixed to \(d=100\). All other parameters are set to default. The results are shown in Fig. 2 reporting SHD, SID, and the runtime.

When varying the number of nodes (first row in Fig. 2) SparseRC, GOLEM, and NOTEARS achieve very good performance whereas the other methods perform significantly worse. As the number of nodes increases, SparseRC performs best both in terms of SHD and SID while being significantly faster than GOLEM and NOTEARS. When increasing the number of samples (second row) the overall performances improve. For low number of samples SparseRC, GOLEM and NOTEARS fail. Their performance significantly improves after \(500\) samples where SparseRC overall achieves the best result. The rest of the methods have worse performance. SparseRC is again significantly faster than GOLEM and NOTEARS in this case.

**Experiment 3: Learning the root causes.** Where our method succeeds we can also recover the root causes that generate the data. Namely, if we recover a very good estimate of the true adjacency matrix via (10), we may compute an approximation \(}\) of the root causes \(\), up to noise, by solving (4):

\[}=+_{c}+_{x}( -)=(+})^{- 1}.\] (11)

In the last row of Fig. 2 we evaluate the top-performing methods on the recovery of the root causes (and the associated values) with respect to detecting their locations (\(\) TPR and FPR) and recovering

    & Hyperparameter & Default & Change & SparseRC (ours) & GOLEM & NOTEARS \\ 
1. & Default settings & & & \(\) & \(529 210\) & \(796 185\) \\
2. & Graph type & Erdos-Renyi & Scale-free & \(\) & \(460 184\) & \(180 7.2\) \\
3. & \(_{c},_{x}\) distribution & Gaussian & Gumbel & \(\) & \(349 125\) & \(251 48\) \\
4. & Edges/ Vertices & \(4\) & \(10\) & \(\) & \(347 121\) & \(471 82\) \\
5. & Samples & \(n=1000\) & \(n=100\) & \(\) & \(194 9.6\) & \(679 72\) \\
6. & Standardization & No & Yes & \(\) & \(326 145\) & \(781 76\) \\
7. & Larger weights in \(\) & \((0.1,0.9)\) & \((0.5,2)\) & \(\) & \(431 177\) & \(284 228\) \\
8. & \(_{c},_{x}\) deviation & \(=0.01\) & \(=0.8\) & \(\) & \(309 63\) & \(433 53\) \\
9. & Dense root causes \(\) & \(p=0.1\) & \(p=0.5\) & \(\) & \(334 121\) & \(427 35\) \\
10. & Fixed support & No & Yes & \(\) & \(360 142\) & \(669 386\) \\   

Table 2: Runtime [seconds] report of the top-performing methods in Table 1.

their numerical values using the \(\) NMSE metric. The support is chosen as the entries \((i,j)\) of \(\) that are in magnitude within \(10\%\) of the largest value in \(\) (i.e., relatively large): \(c_{ij}>0.1_{}\). We consider the default experimental settings.

**Experiment 4: Larger DAGs.** In Table 3, we investigate the scalability of the best methods SparseRC, GOLEM, and NOTEARS from the previous experiments. We consider five different settings up to \(d=3000\) nodes and \(n=10000\) samples. We use the prior default settings except for the sparsity of the root causes, which is now set to \(5\%\). The metrics here are SHD and the runtime. The results show that SparseRC excels in the very sparse setting with almost perfect reconstruction, far outperforming the others including in runtime. SparseRC even recovers the edge weights with an average absolute error of about \(5\%\) for each case in Table 3, as shown in Appendix B.3.

**Experiment 5: Varying average degree.** We evaluate the top-scoring methods on DAGs with higher average degree (dense DAGs). Note that this violates the typical assumption of sparse DAGs (Zheng et al., 2018). In Fig. 3 we consider default settings with DAGs of average degree up to \(20\). SID was not computed due to the presence of cycles in the output of the algorithms and TPR is reported instead. We conclude that for dense DAGs the performance decays, as expected.

Table 3: Performance of the top-performing methods on larger DAGs.

Figure 2: Performance report on the default settings. (a,b,c) illustrate SHD, SID and runtime (lower is better) while varying the number of nodes with \(1000\) samples (first row) or varying the number of samples with \(100\) nodes (second row). (d, e) illustrate TPR and FPR of the estimated support of \(\), and (f) reports the accuracy of estimating \(\) as NMSE.

### Evaluation on a real dataset

We apply SparseRC on the causal protein-signaling network data provided by Sachs et al. (2005). The dataset consists of \(7466\) samples (we use the first \(853\)) for a network with \(11\) nodes that represent proteins, and \(17\) edges representing their interactions. It is small, but the task of learning it has been considered a difficult benchmark in (Ng et al., 2020; Gao et al., 2021; Yu et al., 2019; Zheng et al., 2018). It is not known whether the assumption of few root causes holds in this case. We report the performance metrics SHD and SID for the most successful methods in Table 4. The number of total edges is used to ensure that the output of the methods are non-trivial (e.g. empty graph).

The best SID is achieved by GOLEM (equal variance), which, however, has higher SHD. NOTEARS has the best SHD equal to \(11\). Overall, SparseRC performs well, achieving the closest number of edges to the real one with \(16\) and a competitive SHD and SID. We also mention again the good performance of SparseRC on a non-public biological dataset (Misiakos et al., 2023; Chevalley et al., 2023) as part of the competition by Chevalley et al. (2022).

## 6 Broader Impact and Limitations

Our method inherits the broader impact of prior DAG learning methods including the important caveat for practical use that the learned DAG may not represent causal relations, whose discovery requires interventions. Further limitations that we share with prior work include (a) Learning DAGs beyond 10000 nodes are out of reach, (b) there is no theoretical convergence guarantee for the case that includes noise, (c) empirically, the performance drops in low varsortability, (d) our method is designed for linear SEMs like most of the considered benchmarks.

A specific limitation of our contribution is that it works well only for few root causes of varying location in the dataset. This in addition implies that the noise must have low deviation and the graph to have low average degree and weights of magnitude less than one. Also, in our experiments, we only consider root causes with support that follows a multivariate Bernoulli distribution.

## 7 Conclusion

We presented a new perspective on linear SEMs by introducing the notion of root causes. Mathematically, this perspective translates (or solves) the recurrence describing the SEM into an invertible linear transformation that takes as input DAG data, which we call root causes, to produce the observed data as output. Prior data generation for linear SEMs assumed a dense, random valued input vector. In this paper, we motivated and studied the novel scenario of data generation and DAG learning for few root causes, i.e., a sparse input with noise, and noise in the measurement data. Our solution in this setting performs significantly better than prior algorithms, in particular for high sparsity where it can even recover the edge weights, is scalable to thousands of nodes, and thus expands the set of DAGs that can be learned in real-world scenarios where current methods fail.

    & SHD \(\) & SID \(\) & Total edges \\  SparseRC & \(15\) & \(45\) & \(16\) \\ NOTEARS & \(\) & \(44\) & \(15\) \\ GOLEM & \(21\) & \(\) & \(19\) \\ DAGMA & \(14\) & \(46\) & \(11\) \\   

Table 4: Performance of the top-performing methods on the dataset by Sachs et al. (2005).

Figure 3: Evaluation of the top-performing methods on denser DAGs.