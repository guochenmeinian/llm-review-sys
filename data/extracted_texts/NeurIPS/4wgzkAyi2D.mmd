# LCGen: Mining in Low-Certainty Generation for View-consistent Text-to-3D

Zeng Tao1, Tong Yang2, Junxiong Lin1, Xinji Mai1, Haoran Wang1,

**Beining Wang2, Enyu Zhou3, Yan Wang1,*, Wenqiang Zhang2,4,**

1Shanghai Engineering Research Center of AI & Robotics, Academy for Engineering

& Technology, Fudan University, Shanghai, China

2Shanghai Key Lab of Intelligent Information Processing, School of

Computer Science, Fudan University, Shanghai, China

3School of Computer Science, Fudan University, Shanghai, China

4Engineering Research Center of AI & Robotics, Ministry of Education, Academy for

Engineering & Technology, Fudan University, Shanghai, China

{ztao19,yanwang19,wqzhang}@fudan.edu.cn

Corresponding authors

###### Abstract

The Janus Problem is a common issue in SDS-based text-to-3D methods. Due to view encoding approach and 2D diffusion prior guidance, the 3D representation model tends to learn content with higher certainty from each perspective, leading to view inconsistency. In this work, we first model and analyze the problem, visualizing the specific causes of the Janus Problem, which are associated with discrete view encoding and shared priors in 2D lifting. Based on this, we further propose the LCGen method, which guides text-to-3D to obtain different priors with different certainty from various viewpoints, aiding in view-consistent generation. Experiments have proven that our LCGen method can be directly applied to different SDS-based text-to-3D methods, alleviating the Janus Problem without introducing

Figure 1: The main cause of the Janus Problem in SDS-based text-to-3D methods is their reliance on high-certainty 2D priors for 3D representation, which can result in heads appearing from multiple viewpoints. To address this, we introduced the LCGen method, using low-certainty generation to align viewpoints with the optimization direction.

additional information, increasing excessive training burden, or compromising the generation effect. Project page is [here].

## 1 Introduction

At the forefront of the digital domain, considerable advancements have been achieved in converting textual prompts into realistic 2D images, signaling a new epoch of computational creativity [14; 5; 4; 3; 2; 36; 38; 39; 30; 31; 22; 17; 18]. However, transferring such achievements to the 3D domain introduces an additional layer of intricacy. While 3D generation technology [35; 29; 9; 24; 32] is becoming increasingly indispensable across various fields, from virtual reality to architectural design, traditional 3D content generation demands a substantial investment of time and expertise, and the difficulty in acquiring 3D data makes explicit text-to-3D modeling exceedingly challenging. In response, Score Distillation Sampling (SDS) , based on 2D lifting, has emerged to simplify and advance the 3D creative process [6; 16; 23; 34]. In text-to-3D tasks, 3D representation (such as NeRF ) renders and outputs the corresponding visual image of given camera viewpoints. After that, SDS employs the guidance from priors embedded within pre-trained text-conditioned 2D diffusion models to compute losses for images or latents, iteratively guiding the 3D representation to its optimality.

However, this paradigm carries certain risks, with the Janus Problem  standing out as a significant and common issue [33; 37; 40]. This problem occurs when 3D models exhibit multiple, often conflicting viewpoints, resulting in inconsistencies with the original text conditions. As illustrated in Fig. 1, faces appear at various positions within the same 3D object. This is an inevitable result stemming from the inherent nature of SDS-based text-to-3D approaches. Firstly, current SDS-based text-to-3D methods employ discrete viewpoint encoding. Camera perspectives are classified into regions, with each region sharing a uniform view and text guidance. Consequently, the images within each region share the same prior distribution, biasing the 3D representation towards locally optimal synthesis with the highest certainty, as shown in Fig. 2 and Sec. 3. From a global perspective, there is a high probability of bias towards synthesizing heads at multiple biased positions. Secondly, diffusion models lack diverse 3D training data and thus a nuanced understanding of 3D space . For different camera views, there is a tendency to generate images with high certainty that emphasize the most characteristic features of the object , such as the head . These intrinsic shortcomings render the Janus Problem a dominant challenge in the text-to-3D process.

Addressing Janus Problem remains a critical challenge, with numerous studies dedicated to mitigating its effects [15; 19; 20; 21; 28; 40; 12], like DreamControl  and perp-Neg . However, these methods either require extensive multi-stage fine-tuning or object-specific designs and do not address the fundamental causes of the Janus Problem. In response, we have innovatively modeled and analyzed the underlying causes of the text-to-3D Janus Problem and proposed a novel approach named Low Certainty Generation (LCGen) method.

Specifically, we first analyzed the causes of the Janus Problem through probability modeling. Under the paradigm of discrete viewpoint encoding, viewpoint inputs from the same region possess the same prior distribution. We modeled this distribution and analyzed the relationship between the occurrence of the Janus Problem at various biased positions and the distribution itself. By calculating probabilities, we derived the likelihood of biases occurring at specific positions, as illustrated in Fig. 2 showing probability density peaks at different biases, indicating the Janus Problem.

Thus, addressing the text-to-3D Janus Problem necessitates reevaluating the relationship between views and distributions of guidance, advocating for their decoupling rather than adherence to a shared distribution. Direct explicit modeling of the distribution is quite challenging, so we sought implicit distribution constraints based on certainty learning. Here, we define the probability \(C(x_{t-1}|x_{t})\) in Eq. 7, estimated at the current timestep, as generation certainty in diffusion. We discovered that different viewpoint images exhibit varying levels of certainty during the diffusion denoising process. For areas rich in object features (such as the front), diffusion model tends to exhibit higher certainty during the denoising process, and conversely lower elsewhere. This is linked to the data bias in pretrained diffusion models  (see Appendix. B). Leveraging this characteristic, we constrain the generation of 3D representations so that their input \(x_{t}\) into diffusion model achieves certainty consistent with the viewpoint. Consequently, we obtain a decoupled data distribution for precise distribution localization of separated viewpoints, thus releasing the Janus Problem. Extensive data analysis and visualization substantiate the scientific validity and effectiveness of LCGen. Our method can be integrated into various SDS-based text-to-3D methods, consistently mitigating the Janus Problem without compromising generative performance.

Our contributions are as follows:

* We model and analyze the Janus Problem in text-to-3D, identifying the fundamental reasons for its occurrence. Our findings indicate that the inevitability of the Janus Problem is associated with the SDS-based text-to-3D framework that employs discrete view encoding and 2D diffusion lifting.
* We specifically develop LCGen, a method that decouples the distribution of viewpoint data from the perspective of generation certainty, thereby guiding precise view localization and effectively mitigating the Janus Problem.
* We conduct extensive data analysis and experiments to demonstrate the scientific validity and effectiveness of LCGen. Our method is transferable and consistently mitigates the Janus Problem across various baselines without compromising the generative quality.

## 2 Background

### Diffusion model

Diffusion models [10; 27] have proven a powerful class of generative models, particularly excelling in text-to-image synthesis. Building on the progress made with text-to-image diffusion models, new approaches have been developed to extend these capabilities to 3D content generation.

A diffusion model typically involves a forward process that gradually adds noise to a data sample and a reverse process that aims to reconstruct the original sample by progressively denoising it. Mathematically, the forward process is described as:

\[q(_{t}|_{t-1})=(_{t};}_{t- 1},_{t}),\] (1)

where \(\) is Gaussian distribution, \(_{t}\) are variance terms increasing over time, and \(\) is the identity matrix. The reverse process, which is more pertinent to generative tasks, is modeled as:

\[p_{}(_{t-1}|_{t})=(_{t-1};_{}(_ {t},t),_{t}^{2}),\] (2)

where \(_{}(_{t},t)\) represents the mean learned by the diffusion model \(\), and \(_{t}^{2}\) are learned variances.

### SDS-based text-to-3D

**Score Distillation Sampling (SDS)**. Score Distillation Sampling (SDS)  is a novel approach tailored for bridging the gap between 2D image generation and 3D model synthesis. The SDS process utilizes the gradients from a pretrained 2D diffusion model \(\) to guide the generation of 3D model \(\). The key idea is to render 2D projections of a 3D model from various views and adjust the model parameters to maximize the agreement between these projections and the images generated by a text-conditioned diffusion model. Given text prompt \(y\), SDS can be described as:

\[_{}_{}()=_{t,,c }[(t)(}_{}(_{t},t,y^{c})-)}{}]\] (3)

where \(}_{}\) is the noise predicted by \(\), \(\) is the rendered image of view \(c\) by \(\), \(\) is weighting factor.

**Variational Score Distillation (VSD)**. Building upon SDS, Variational Score Distillation (VSD)  introduces a probabilistic framework that treats the problem of text-to-3D synthesis as a distribution optimization task. VSD seeks to create a distribution over possible 3D shapes that is likely under the given text condition, rather than finding a single deterministic shape. VSD can be expressed as:

\[_{}_{}()=_{t,,c }[(t)(}_{}(_{t},t,y^{c})-}_{}(_{t},t,c,y))}{}]\] (4)

where \(}_{}(_{t},t,c,y)\) is the noise predicted by rendered images. This approach allows for exploring a richer space of 3D geometries, potentially capturing more complex and diverse features consistent with the textual description.

Analysis of Janus Problem in Text-to-3D

In this Section, we analyze the reasons for the Janus Problem produced by the SDS-based text-to-3D method. Through modeling, we have identified how the discrete view encoding method leads to shared distributions that cause the Janus Problem. For detailed derivation, please refer to Appendix C. In response to this finding, we have developed the LCGen method, as described in Section 4.

Our modeling focuses on two variables: the camera viewpoint parameter \(\) and the position of the head \(\) in the 3D representation. Both variables can be considered as points on a sphere \(S^{2}\), represented by \((,)\). Taking the head position as an example, our modeling yields the probability \(P()\) of the head appearing at each position \(\) on the sphere. The modeling steps are as follows:

As shown in Fig. 2(a), in the SDS, the sphere \(S^{2}\) is divided into different region intervals \(\). For each \(\) within \(\), the same viewpoint text is used, resulting in the same text condition and diffusion guidance (See Appendix. B). We represent the guidance as a superposition of Gaussian distributions with the spherical position as the variable. In one-dimensional Gaussian distribution across \(\) or \(\) dimension, the probability \(p\) of generating a head at position \(b\) under the viewpoint \(c\) is given by:

\[p^{}(b,c)=_{i}w_{i}^{}^{}}^{2}}}(-^{})^{2}}{2{ _{i}^{}}^{2}})\] (5)

where \(w_{i}^{}\) are the mixture weights for \(i\)th Gaussian component in \(\), \(_{i}^{}\) and \({_{i}^{}}^{2}\) are the mean and variance of the \(i\)-th Gaussian component in \(\), \(_{i}w_{i}^{}=1\) to ensure that the total probability of the mixture distribution in \(\) integrates to one over its domain.

As shown in Fig. 2(b), integrating over the viewpoints \([c_{0},c_{1}]\) within \(\) gives the probability \(P\) of generating a head at position \(b\) within \(\) as \(P^{}(b)=_{}p^{}(b,c)\,dc\).

Now, we extend from one dimension Gaussian distribution to two dimensions \(p^{}(,)=_{i}w_{i}^{}}}_{i}^{}}(- (--_{i}^{})^{}_{i}^{ -1}(--_{i}^{}))\) in Fig. 2(c) and consider spherical integration. the probability of generating a head at position \(\) is given by:

\[P()=_{}_{}p^{}(,)\, dA=_{}_{}p^{}(,,)( )\,d\,d\] (6)

where \(\) represents different regions of the sphere, \(dA\) is the differential solid angle element in spherical coordinates, and \(()\) accounts for the area element on the sphere. Each integral \(_{}\) calculates the contribution to \(P()\) from each region \(\).

Thus, we can use numerical integration techniques to obtain the probability of generating a head at different positions \(\). As shown in Fig. 2(d), probability peaks appear at different positions \(\). This indicates that discrete viewpoint encoding may lead to the generation of heads at different positions in the 3D representation, known as the Janus Problem. Another head is equally likely to appear on the side and back, compromising the realism of the 3D representation. See details in Appendix C.

Figure 2: Analysis of the Janus Problem in Text-to-3D. Due to the discrete encoding of viewpoints, there is a high probability of multiple heads appearing at different positions \(\) on the sphere.

## 4 LCGen: Low Certainty Generation

From the analysis in Sec. 3, it is evident that the discrete view-dependent text condition can lead the 3D representation to manifest the Janus Problem at different positions \(\) during the synthesis process. Therefore, addressing the relationship between \(\) and guidance presents a viable method for mitigating the Janus Problem. We propose LCGen, which leverages the certainty characteristics to decouple distributions across different viewpoints, thereby mitigating the Janus Problem in the text-to-3D task. Specifically, for \(\), we constrain the guidance towards \(p_{t}^{e}(_{t},y)\) rather than \(p_{t}^{T}(_{t},y)\).

**LCGen.** During the diffusion denoising process, different \(\) with different \(_{t}\) of the same object possess distinct certainty \(C(_{t-1}|_{t})\). We decouple these to guide the synthesis process to generate images that correspond more closely to the desired viewpoint.

In the diffusion model in SDS, each denoising step is considered a probabilistic inference process from the current state \(_{t}\) to the previous state \(_{t-1}\). This process typically relies on the following assumptions: 1) The noise \(\) follows a Gaussian distribution, which is estimated by the model at each step. 2) The mapping from \(_{t}\) to \(_{t-1}\) can be represented using a parameterized Gaussian process.

Assuming we have obtained the prediction of \(}_{t}\) through the diffusion model, we can predict the estimation of the certainty of \(_{t-1}\) given \(_{t}\). This estimation typically assumes that the certainty follows a Gaussian distribution:

\[C^{e}(_{t-1}|_{t})(_{t-1};^{e},^{e2})\] (7)

Here, \(^{e}\) and \(^{e2}\) represent the mean and variance that guides the U-Net's prediction, thus affecting the synthetic results. \(^{e}\) and \(^{e2}\) can be calculated as follows:

\[^{e}=}}(_{t}-}}_{t}),^{e2}=1-_{t}\] (8)

Here, \(_{t}\) is the variance parameter at step \(t\), \(_{t}\) is \(1-_{t}\). The Gaussian distribution parameters guide the Diffusion model's prediction to minimize the Janus Problem at step \(t-1\). The certainty function is defined as:

\[C^{e}(_{t-1}|_{t})=^{e}}( -_{t-1}-^{e})^{2}}{2^{e2}})\] (9)

By constraining \(C^{e}(_{t-1}|_{t})\), we can ensure that different viewpoints have different distributions. We have designed the \(_{}\) as follows:

\[_{} C^{e}(_{t-1}| _{t}) G()\] (10)

where \(\) is normalization constant, \(G()\) is the function of view-based guidance.

Figure 3: Overview of LCGen. LCGen can be embedded into any SDS-based text-to-3D method, providing different guidance for various viewpoints by constraining the generation certainty, thus alleviating Janus Problem.

**Back propagation.** Although Eq. 10 specifies the basic form of \(_{}\), direct backpropagation involves redundancy. In this method, the diffusion model serving as guidance is frozen, and it is unnecessary to calculate gradients for it. We hope to perform backpropagation directly on NeRF. In SDS , to simplify the gradient calculation and update processes, researchers simplify the loss function \(_{}\), avoiding the complex gradient calculations involved with a frozen diffusion model \(\). The gradients are applied directly to NeRF \(\) and its rendered images \(_{t}\). In LCGen, we also specifically design the \(_{}\) to bypass the diffusion model and apply directly in the NeRF flow parameter updates.

For \(_{}\), we obtain:

\[_{}}{}=_{}}{_{t}}_{t}}{ }\] (11)

Figure 4: Results of Qualitative Comparison. The areas enclosed in red boxes are where the Janus Problem occurs.

Using the chain rule, we can expand and simplify the first term on the right side in Eq. 11. For details of the process, please refer to the Appendix D. We get:

\[_{}_{}()& =_{t,,}[(t) C^{}(_{t-1}|_{t}) G()_{t}}{}]\\ &=_{t,,}[(t)_{}_{t}}{}]\] (12)

where

\[(t)=-_{t-1}-}}(_{t}-}}_{t})}{}^{2}}}}\] (13)

By simplifying the calculations, we enable \(_{}\) to apply directly in the NeRF flow parameter \(\) updates, avoiding the complex gradient calculations of the Unet layer in diffusion model \(\), and maintaining calculation consistency with \(_{}\).

## 5 Experiment

In this Section, we apply LCGen to several baseline methods of SDS-based text-to-3D, including DreamFusion , Magic3D , and ProlificDreamer , and conduct corresponding experiments. We also compare with other methods that address the Janus Problem. Sec. 5.2 presents the effects of original methods and LCGen, including qualitative and quantitative assessments. Sec. 5.3 demonstrates the ablation of hyperparameters. Furthermore, Sec. 5.4 presents visualization results of LCGen's impact on generation certainty.

### Experiment Settings

We implement original methods and LCGen based on threestudio  and a single A100 GPU. In the experiment, we set \(G()=||\) and \(\) to 10, and obtained the results after a maximum of 10,000 steps. For the sake of experimental consistency, we have chosen the Stable Diffusion 2.1 base  as guidance and NeRF  as the 3D representation in the SDS-based method. See Details in Appendix. E and F.

### Results of LCGen

**Qualitative Comparison.** We selected two sets of text prompts from the library  and conducted experiments on three SDS-based text-to-3D baseline methods, including DreamFusion-sd , Magic3D-sd coarse , and ProlificDreamer , both without and with LCGen, with qualitative results as shown in Fig. 4. It can be observed that, as indicated by the red boxes in Fig. 4, the original methods have a high probability of exhibiting the Janus Problem. For instance, in the first set of examples, the beagle appears with two faces, and in the second set of cases, the fox's front and back both exhibit a cello. As previously analyzed, this is a common issue inherent to SDS-based methods, resulting from the nature of the paradigm. After applying LCGen to each method, the Janus Problem was mitigated, with the generated 3D content exhibiting spatial consistency. In particular, our method has a very strong suppressive effect on the Janus Problem that occurs on the backside of objects.

**Quantitative Comparison.** We have conducted a quantitative analysis of the results from our three sets of baselines, as shown in Fig. 5. The Janus Rate (JR) represents the rate at which the Janus Problem occurs and is used to measure the 3D consistency of the generation method. The CLIP-Score (CS), on the other hand, is a metric for assessing the consistency between the text and the prompt and the generated image. See Appendix. F for details. We selected 30 sets of text prompts from the library and calculate the mean score. It can be observed that our method significantly reduces the probability of the

Figure 5: Results of Quantitative Comparison. \(\) represents that a smaller value is better, while \(\) indicates that a larger value is preferred. The gray background represents the results with LCGen.

Janus Problem occurring. Meanwhile, the CS reflects that LCGen improves spatial consistency without compromising the quality of the generated images. This demonstrates the effectiveness and practicality of LCGen.

**LCGen vs. Other Methods addressing Janus Problem.** Some current work is also designed to address the Janus Problem [15; 19; 20; 21; 28; 40; 13; 1; 12]. However, these methods either require extensive multi-stage fine-tuning or object-specific designs and do not address the fundamental causes of the Janus Problem, as shown in Fig. 6 (See more comparison in Appendix G). Compared to these methods, our approach has the following advantages: 1) It targets the essence of the Janus Problem by tapping into the multi-perspective information within 2D priors without the need to introduce additional information. 2) It can be directly integrated into any SDS-based text-to-3D method. 3) There is no need to alter the training paradigm, and the computational cost for certainty calculations is negligible compared to the baseline.

### Ablation Study

**Certainty within the training step windows.** We also conducted ablation experiments on the certainty at different time step windows during the training process. As shown in Fig. 8, for each step, a window \((step-200,step]\) is selected, and the variance of certainty within the window is calculated. By subtracting the window variance values of the LCGen and origin methods, the results are obtained. It can be observed that during the training process, the certainty variance of most step windows in LCGen is larger. This indicates that the LCGen method effectively separates the certainty of different viewpoints.

### Visualization

In this Section, we conduct a visualization analysis of the LCGen and the corresponding certainty for each viewpoint. Our chosen textual example is "A corgi". We use ProlificDreamer  as our baseline and achieve results after a maximum of 10,000 steps.

Figure 8: The difference in the variance of certainty within the training step windows between w/ and w/o LCGen.

Figure 6: Comparison of different methods dealing with Janus Problem.

Figure 7: The impact of different choices of \(G()\).

As shown in Fig. 9, the left side shows the original ProlificDreamer results, while the right side features the ProlificDreamer using the LCGen method. For each method, the lower half presents visualized outputs showing rendering results at various azimuths \(\) with an elevation \(\) of \(-15^{}\). It can be observed that the original prolificdreamer generates the corgi with faces on both the front and back, which is indicative of the Janus Problem. In contrast, the 3D representation created using LCGen exhibits high spatial consistency. We have also visualized the certainty obtained from the diffusion model after inputting images of the fully trained NeRF from various viewpoints. It is shown that due to the presence of multiple faces, the certainty in the original method does not show a clear pattern of change, consistent with the analysis presented earlier. In comparison, LCGen demonstrates a more distinct pattern of certainty varying with \(\) where the certainty is greater when \(||\) is small and decreases as \(||\) increases. This highlights the effectiveness of LCGen's constraints on certainty and also corroborates the relationship between certainty and \(\). See details in Appendix E.

## 6 Related Works

**Methods addressing Janus Problem.** Past research has explored multi-stage networks that utilize 3D priors to reduce the Janus Problem. A two-stage 2D lifting framework has been proposed in DreamControl , leveraging 3D self-priors to enhance geometric consistency in 3D generation. Other approaches, such as Perp-Neg , innovate by using negative prompts in diffusion models to remove undesirable attributes or views while maintaining the core concept. However, these approaches either necessitate extensive multi-stage pre-training or are tailored to specific objects, without tackling the root causes of the Janus Problem. Additionally, they often require specialized and resource-intensive procedures. See details in Appendix. G

## 7 Conclusion and Discussion

In this study, we initially model the Janus Problem and analyze its causes visually. We then introduce LCGen to guide text-to-3D generation toward spatial consistency by establishing varied certainty priors across viewpoints. Our method, validated through experiments, can integrate seamlessly with

Figure 9: Visualization of Certainty after Generation w/o and w/ LCGen. The upper half of the 3D scatter plot has the camera view parameters \((,)\) on the x and y axes, and Certainty on the z-axis. The lower half depicts the generation results from different views, along with their \(\) and the Certainty colors as represented in the scatter plot.

various SDS-based text-to-3D methods to mitigate the Janus Problem. It does so without adding extra data requirements, excessive computational overhead, or degrading the quality of generated outputs.

**Limitations and Broader Impacts.** Our method performs well in generating individual objects but has limitations with complex multi-object scenes. Due to the lack of 3D training data resulting in an insufficient understanding of the 3D world, LCGen still has some failure cases in the text-to-3D Janus Problem. Additionally, using a fixed \(G\) leaves room for improvement in adaptive generation. At the same time, realistic AI-generated content may have adverse social impacts. Like other generative model researchers, we must remain vigilant and take precautions against generating false content.

**Acknowledgements.** This work was supported by the National Natural Science Foundation of China under Grant 62406075, National Key Research and Development Program of China under Grant 2023YFC3604802; in part by the China Postdoctoral Science Foundation under Grant 2023M730647 and Grant 2023TQ0075.