# [MISSING_PAGE_FAIL:1]

[MISSING_PAGE_FAIL:1]

Listgarten, 2020; Chen et al., 2022). This class of methods potentially enables us to surpass the best design observed in the offline data by harnessing the extrapolative capabilities of reward models.

In our work, we explore how the generative modeling and MBO perspectives could be reconciled, inspired by recent work on RL-based fine-tuning of diffusion models (e.g., Black et al. (2023); Fan et al. (2023)), which aims to finetune diffusion models by optimizing down-stream reward functions. Although these studies do not originally address computational design, we can potentially leverage the strengths of both perspectives. However, these existing studies often focus on scenarios where online reward feedback can be queried or accurate reward functions are available. Such approaches are not well-suited for the typical offline setting, where we lack access to true reward functions and need to rely solely on static offline data (Levine et al., 2020; Kidambi et al., 2020; Yu et al., 2020). In scientific fields, this offline scenario is common due to the high cost of acquiring feedback data. In such contexts, existing works for fine-tuning diffusion models may easily lead to overoptimization, where optimized designs are misled by the trained reward model from the offline data, resulting in out-of-distribution adversarial designs instead of genuinely high-quality designs.

To mitigate overoptimization, we develop a conservative fine-tuning approach for generate models aimed at computational design. Specifically, we consider a critical scenario where we have offline data (with feedback) and a pre-trained diffusion model capable of capturing the space of "valid" designs, and propose a two-stage method (Figure 1). In the initial stage, we train a conservative reward model using offline data, incorporating an uncertainty quantification term that assigns higher penalties to out-of-distribution regions. Subsequently, we finetune pre-trained diffusion models by optimizing the conservative reward model to obtain high-quality designs and prevent the generation of out-of-distribution designs. In the fine-tuning process, we also introduce a KL penalization term to ensure that the generated designs remain within the valid design space.

Our primary contribution lies in the introduction of a novel framework, **BRAID** (doubly conseRvActive fine-tuning diffusIon moDels). The term "doubly conservative" reflects the incorporation of two types of conservative terms, both in reward modeling and KL penalization. By properly penalizing the fine-tuned diffusion model when it deviates significantly from the offline data distribution, we effectively address overoptimization. Additionally, by framing our fine-tuning procedure within the context of soft-entropy regularized Markov Decision Processes, we offer theoretical justification for the inclusion of these conservative terms in terms of regret. This theoretical result shows that fine-tuned generative models outperform the best designs in the offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs. Furthermore, through empirical evaluations, we showcase the efficacy of our approach across diverse domains, such as DNA/RNA sequences and images.

Figure 1: The left figure illustrates our setup with a pre-trained generative model and offline data. On the right, the motivation of the algorithm is depicted. The region surrounded by the green line is the original entire design space, with the colored region indicating the valid design space (e.g., natural images, human-like DNA sequences). The red region denotes areas with more offline data available, while the blue region indicates areas with less data available. We aim to add penalties to the blue regions using conservative reward modeling to prevent overoptimization while imposing a stricter KL penalty on the non-colored regions to prevent the generation of invalid designs.

Related Works

We summarize related works. For additional works such as fine-tuning on LLMs, refer to Section A.

Fine-tuning diffusion models via reward functions.Several previous studies have aimed to improve diffusion models by optimizing reward functions using various methods, including supervised learning (Lee et al., 2023; Wu et al., 2023), RL (Black et al., 2023; Fan et al., 2023) and control-based techniques (Clark et al., 2023; Xu et al., 2023; Prabhudesai et al., 2023). In contrast to our work, their emphasis is not on an offline setting, i.e., their setting assumes online reward feedback is available or accurate reward functions are known. Additionally, while Fan et al. (2023) include the KL term in their algorithms, our innovation lies in integrating conservative reward modeling to mitigate overoptimization and formal statistical guarantees in terms of regret (Theorem 1, 2).

Conditional diffusion models.Confotional diffusion models, which learn conditional distributions of designs given the rewards, have been extensively studied (Ho and Salimans, 2022; Dhariwal and Nichol, 2021; Song et al., 2020; Bansal et al., 2023). However, for the purpose of MBO, these approaches require that the offline data has good coverage on values we want to condition on (Brandfonbrener et al., 2022). Compared to conditional diffusion models, our approach aims to obtain designs that can surpass the best design in offline data by leveraging the extrapolation capabilities of reward models. We compare these approaches with our work in Section 7.

Offline model-based optimization (MBO).Offline MBO is also known as offline black-box optimization and is closely related to offline contextual bandits and offline RL (Levine et al., 2020). While conservative approaches have been studied there (e.g., Kidambi et al. (2020); Yu et al. (2020) and more in Section A); most of the works are not designed to incorporate a diffusion model, unlike our approach. Hence, it remains unclear how these methods can generate designs that remain within intricate valid design spaces (e.g., generating natural images).

It is worth noting a few exceptions (Yuan et al., 2023; Krishnamoorthy et al., 2023) that attempt to integrate diffusion models into MBO. However, the crucial distinctions lie in the fact that we directly optimize rewards with diffusion models, whereas these prior works focus on using conditional diffusion models. Additionally, we delve into the incorporation of conservative terms, an aspect not explored in their works. We compare these methods with ours empirically in Section 7.

## 3 Preliminaries

We outline our framework for offline model-based optimization with a pre-trained generative model. Subsequently, we highlight the challenges arising from distributional shift. Additionally, we provide an overview of diffusion models, as we will employ them as pre-trained generative models.

### Offline Model-Based Optimization with Pre-Trained Generative Model

Our objective is to find a high-quality design within a design space, \(\). Each design \(x\) is associated with a reward, \(r(x)\), where \(r:\) is an unknown reward function. Then, our aim is to find a high-quality generative model \(p()\), that yields a high \(r(x)\). It is formulated as

\[*{argmax}_{p()}_{x p}[r(x)].\] (1)

Avoiding invalid designs.In MBO, the design space \(\) is typically huge. However, in practice, the valid design space denoted by \(_{}\) is effectively contained within this extensive \(\) as a potentially lower-dimensional manifold. For instance, in biology, our focus often centers around discovering highly bioactive protein sequences. While the raw search space might encompass \(|20|^{B}\) possibilities (where \(B\) is the length), the actual design space corresponding to valid proteins is significantly more constrained. Consequently, our problem can be formulated as:

\[*{argmax}_{p(_{})}_{x  p}[r(x)],(*{argmax}_{p()}_{x p}[r(x)] -_{x p}[(x_{})]).\] (2)

Note supposing that a reward \(r()\) is \(0\) outside of \(_{}\), this is actually still equivalent to (1).

Offline data with a pre-trained generative model.Based on the above motivation, we consider scenarios where we have an offline dataset \(_{}\), used for learning the reward function. More specifically, the dataset, \(_{}=\{x^{(j)},y^{(j)}\}_{j=1}^{n_{}}\) contains pairs of designs \(x p_{}()\) and their associated noisy reward feedbacks \(y=r(x)+\), where \(\) is noise.

Compared to settings in many existing papers on MBO, we also assume access to a pre-trained generative model (diffusion model) trained on a large dataset comprising valid designs, in addition to the offline data \(_{}\). For example, in biology, this is expected to capture the valid design space \(_{}\) such as human DNA sequences or physically feasible proteins (Avdeyev et al., 2023; Li et al., 2024; Sarkar et al., 2024; Stark et al., 2024; Campbell et al., 2024). These pre-trained generative models are anticipated to be beneficial for narrowing down the raw search space \(\) to the design space \(_{}\). In our work, denoting the distribution induced by the pre-trained model by \(p_{}\), we regard the support of \(p_{}\) as \(_{}\).

### Challenge: Distributional Shift

To understand our challenges, let's first examine a simple approach for MBO with a pre-trained generative model. For instance, we can adapt methods from Clark et al. (2023); Prabhudesai et al. (2023) to our scenario. This approach involves two steps. In the first step, we perform reward learning: \(=*{argmin}_{}_{i=1}^{n_{}}\{(x^{(i)})-y^{(i)}\}^{2},\) where \(\) represents a function class that includes mappings from \(\) to \(\), aiming to capture the true reward function \(r()\). Then, in the second step, we fine-tune a pre-trained diffusion model to optimize \(\).

Despite its simplicity, this approach faces two types of distributional shifts. Firstly, the fine-tuned generative model might produce invalid designs outside of \(_{}\). As discussed in Section 3.1, we aim to prevent this situation. Secondly, the fine-tuned generative model may over-optimize \(\), exploiting uncertain regions of the learned model \(\). Indeed, in regions not covered by offline data distribution \(p_{}\), the learned reward \(\) can easily have higher values, while the actual reward values in terms of \(r\) might be lower due to the higher uncertainty. We aim to avoid situations where we are misled by out-to-distribution adversarial designs.

### Diffusion Models

We present an overview of denoising diffusion probabilistic models (DDPM) (Song et al., 2020; Ho et al., 2020; Sohl-Dickstein et al., 2015). Note while the original diffusion model was initially introduced in Euclidean spaces, it has since been extended to simplex spaces for biological sequences (Avdeyev et al., 2023), which we will use in Section 7. In diffusion models, the goal is to develop a generative model that accurately emulates the data distribution from the dataset. Specifically, denoting the data distribution by \(p_{}()\), a DDPM aims to approximate using a parametric model structured as \(p(x_{0};)= p(x_{0:T};)dx_{1:T}\), where \(p(x_{0:T};)=p_{T+1}(x_{T};)_{t=T}^{1}p_{t}(x_{t-1}|x_{t}; )\). Here, each \(p_{t}\) is considered as a policy, which is a mapping from a design space \(\) to a distribution over \(\). By optimizing the variational bound on the negative log-likelihood, we can obtain a set of policies \(\{p_{t}\}_{t=T+1}^{1}\) such that \(p(x_{0};) p_{}(x_{0})\). For simplicity, in this work, assuming that pre-trained diffusion models are accurate, we denote the pre-trained policy as \(\{p_{t}^{}(|)\}_{t=T+1}^{1}\), and the generated distribution by the pre-trained diffusion model at \(x_{0}\) by \(p_{}\). With slight abuse of notation, we often denote \(p_{T+1}^{}()\) by \(p_{T+1}^{}(|)\)

## 4 Doubly Conservative Generative Models

We've discussed how naive approaches for computational design may yield invalid designs or over-optimize reward functions, with both challenges stemming from distributional shift. Our goal in this section is to develop doubly conservative generative models to mitigate this distributional shift.

### Avoiding Invalid Designs

To avoid invalid designs, we begin by considering the following generative model:

\[()/)p_{}()}{((x )/)p_{}(x)dx}(:=*{argmax}_{p( )}_{x p}[(x)]-(p\|p_{ })),\] (3)where \((p\|p_{})=_{x p}[(p(x)/p_{}(x))]\). In this formulation, the generative model is designed as an optimizer of a loss function composed of two parts: the first component encourages designs with high rewards, while the second component acts as a regularizer penalizing the generative model for generating invalid designs. This formulation is inspired by our initial objective in (2), where we substitute an indicator function with \((p/p_{})\). This regularizer takes \(\) when \(p\) is not covered by \(p_{}\), and \(\) governs the strength of the regularizer.

### Avoiding Overoptimization

Next, we address the issue of overoptimization. This occurs when we are fooled by the learned reward model in uncertain regions. Therefore, a natural approach is to penalize generative models when they produce designs in uncertain regions.

As a first step, let's consider having an uncertainty oracle \(:\), which is a random variable of \(_{}\). This oracle is expected to quantify the uncertainty of the learned reward function \(\).

**Assumption 1** (Uncertainty oracle).: _With probability \(1-\), we have_

\[ x_{};|(x)-r(x)|(x)\] (4)

These calibrated oracles are well-established when using a variety of models such as linear models, Gaussian processes, and neural networks. We will provide detailed examples of such calibrated oracles in Section 4.3. Essentially, as long as the reward model is well-specified (i.e., there exists \(\) such that \( x_{}:(x)=r(x)\)), we can create such a calibrated oracle.

Doubly Conservative Generative Models.Utilizing the uncertainty oracle defined in Assumption 1, we present our proposal:

\[_{}()=-)()/ )p_{}()}{((-)(x)/ )p_{}(x)dx}\ (:=)}{} _{x p}[(-)(x)]}_{}- (p\|p_{})}_{}).\] (5)

Here, to combat overoptimization, we introduce an additional penalty term \((x)\). This penalty term is expected to prevent \(_{}\) from venturing into regions with high uncertainty because it would take a higher value in such regions. We refer to \(_{}\) as a doubly conservative generative model due to the incorporation of two conservative terms.

An attentive reader might question the necessity of simultaneously introducing two conservative terms. Specifically, the first natural question is whether KL penalties, intended to prevent invalid designs, can replace uncertainty-oracle-based penalties. However, this may not hold true because even if we can entirely avoid venturing outside of \(_{}\) (support of \(p_{}\)), we may still output designs on uncertain regions not covered by \(p_{}\). The second question is whether uncertainty-oracle-based penalties can substitute KL penalties. While it is partly true in situations where the support of \(p_{}\) is contained within that of \(p_{}\), uncertainty-oracle-based penalties, lacking leverage on pre-trained generative models, are ineffective in preventing invalid designs. In contrast, KL penalties are considered a more direct approach to stringently avoid invalid designs by leveraging pre-trained generative models.

### Examples of Uncertainty Oracles

**Example 1** (Gaussian processes.).: _When we use an RKHS as \(\) (a.k.a. GPs) associated with a kernel \(k(,):\)(Srinivas et al., 2009), a typical construction of \(\) and \(\) is_

\[()=(+ I)^{-1}(),\,()=c()(,)},\]

_where \(c()_{>0},_{>0}\), \((x)=[k(x^{(1)},x),,k(x^{(n_{})},x)]^{}\),_

\[=[y^{(1)},,y^{(n_{})}],\{\}_{p,q}=k(x^{( p)},x^{(q)}),(x,x^{})=k(x,x^{})-(x)^{}\{ + I\}^{-1}(x^{}).\]

_Note that when using deep neural networks, by considering the last layer as a feature map, we can still create a kernel (Zhang et al., 2022; Qiu et al., 2022)._

**Example 2** (Bootstrap).: _When we use neural networks as \(\), it is common to use a statistical bootstrap method. Note many variants have been proposed (Chua et al., 2018), and its theory has been analyzed (Kveton et al., 2019). Generally, in our context, we generate multiple models \(_{1},,_{M}\) by resampling datasets, and then consider \(_{i}_{i}\) as \(-\)._

## 5 Conservative Fine-tuning of Diffusion Models

In this section, we consider how to sample from a doubly conservative generative model \(_{}\), using diffusion models as pre-trained generative models. Our algorithm is outlined in Algorithm 1. Initially, we learn a penalized reward \(-\) from the offline data and set it as a target to prevent overoptimization in (6). Additionally, we integrate a KL regularization term to prevent invalid designs. The parameter \(\) governs the intensity of this regularization term.

Formally, this phase can be conceptualized as a planning problem in soft-entropy-regularized MDPs (Neu et al., 2017; Geist et al., 2019). In this MDP formulation:

* The state space \(\) and action space \(\) correspond to the design space \(\).
* The reward at time \(t[0,,T]\) (\(\)) is provided only at \(T\) as \(-\).
* The transition dynamics at time \(t\) (\([()]\)) is an identity \((s_{t+1}=a_{t})\).
* The policy at time \(t\) (\(()\)) corresponds to \(p_{T+1-t}:()\).
* The reference policy at \(t\) is a policy in the pre-trained model \(p_{T+1-t}^{}\)

In these entropy-regularized MDPs, the soft optimal policy corresponds to \(\{_{t}\}\). Importantly, we can analytically derive the fine-tuned distribution in Algorithm 1 and show that it simplifies to a doubly conservative generative model \(_{}\), from which we aim to sample.

**Theorem 1**.: _Let \(_{}()\) be an induced distribution from optimal policies \(\{_{t}\}_{t=T+1}^{1}\) in (6), i.e., \((x_{0})=\{_{t=T+1}^{1}_{t}(x_{t-1}|x_{t})\}dx_{1:T}\) when \(\{_{t}\}\) is a global policy class (\(_{t}=\{()\}\)). Then,_

\[_{}(x)=_{}(x).\]

We have deferred to the proof in Section 5.1. While similar results are known in the context of standard entropy regularized RL (Levine, 2018), our theorem is novel because previous studies did not consider pre-trained diffusion models.

Training algorithms.Based on Theorem 1, to sample from \(_{}\), what we need to is to solve Equation (6). We can employ any off-the-shelf RL algorithms to solve this planning problem. Given that the transition dynamics are known, and differentiable reward models are constructed in our scenario, a straightforward approach to optimize (6) is to directly optimize differentiable loss functions with respect to parameters of neural networks in policies, as detailed in Appendix B. Indeed, this approach has recently been used in fine-tuning diffusion models (Clark et al., 2023; Prabhudesai et al., 2023), demonstrating its stability and computational efficiency.

**Remark 1** (Novelty of Theorem 1).: _A theorem similar to Theorem 1 has been proven for continuous-time diffusion models in Euclidean space (Uehara et al., 2024, Theorem 1). However, the primary distinction lies in the fact that while their findings are restricted to Euclidean space, where diffusion policies take Gaussian polices, our results are not constrained to any specific domain. Hence, for example, our Theorem 1 can handle scenarios where the domain is discrete or lies on the simplex space (Avdeyev et al., 2023) in order to model biological sequences as we do in Section 7._

### Sketch of the Proof of Theorem 1

We explain the sketch of the proof of Theorem 1. The detail is deferred to Theorem C.1.

By induction from \(t=0\) to \(t=T+1\), we can first show

\[_{t}(x_{t-1}|x_{t})=(x_{t-1})/)p_{t-1}^{ pre}(x _{t-1}|x_{t})}{(v_{t}(x_{t})/)}.\] (7)

Here, \(v_{t}(x_{t})\) is a soft optimal value function:

\[_{}[(x_{0})-(x_{0})-_{k=t}^{1}{ KL }(p_{k}(|x_{k})\|p_{k}^{ pre}(|x_{k}))|x_{t}],\]

which satisfies an equation analogous to the soft Bellman equation: \(v_{0}(x)=(x)-(x)\) and for \(t=1\) to \(t=T+1\),

\[((x_{t})}{})=((x_{t-1})}{ })p_{t}^{ pre}(x_{t-1} x_{t})dx_{t-1}.\] (8)

Now, we aim to calculate a marginal distribution at \(t\) defined by \(_{t}(x_{t})=\{_{k=t+1}^{t}_{k}(x_{k-1}|x_{k})\}dx_{t+1 :T}\). Then, by induction, we can show that

\[_{t}(x_{t})=(v_{t}(x_{t})/)p_{t}^{ pre}(x_{t})/C\] (9)

where \(C\) is a normalizing constant. Indeed, supposing that the above (9) hold at \(t\), the equation (9) also holds for \(t-1\) as follows:

\[_{t-1}(x_{t-1}|x_{t})_{t}(x_{t})dx_{t}=(v_{t-1}(x_{t-1}) /)p_{t-1}^{ pre}(x_{t-1})/C=_{t-1}(x_{t-1}).\]

Finally, by setting \(t=0\), the statement in Theorem 1 is concluded.

## 6 Regret Guarantee

In this section, our objective is to demonstrate that a policy \(_{}\) from Algorithm 1 can provably outperform designs in offline data by establishing the regret guarantee.

To assess the performance of our fine-tuned generative model, we introduce the soft-value metric:

\[J_{}(p):=_{x p}[r(x)]-(p\|p_{ pre}).\]

This metric comprises two components: the expected reward and a penalty term applied when \(p\) produces invalid outputs, as we see in (3). Now, in terms of soft-value \(J_{}(p)\), our proposal \(_{}\) offers the following guarantee.

**Theorem 2** (Per-step regret).: _Suppose Assumption 1. Then, with probability \(1-\), we have_

\[();()-J_{}(_{})}_{} 2}_{x p_{ off}}[(x)^{2}]^{1 /2}}_{}, C_{}:=_{x_{ off}}|(x)}|,\]

_where \(_{ off}=\{x:p_{ off}(x)>0\}\). As an immediate corollary,_

\[_{x}[r(x)]-_{x_{}}[r(x)] { KL}(\|p_{ pre})+2}_{x p_{ off }}[(x)^{2}]^{1/2}.\]

In the theorem above, we establish that the per-step regret against a generative model \(\) we aim to compete with is small as long as the generative model \(\) falls within \(_{ off}\) and the learned model \(\) is calibrated as in Assumption 1. First, the term (Stat) corresponds to the statistical error associated with \(\) over the offline data distribution \(p_{ off}\). When the model is well-specified, it is upper-bounded by \(/n}\), where \(\) represents the effective dimension of \(\), as we will discuss shortly. Secondly, the term \(C_{}\) corresponds to the coverage between a comparator generative model \(\) and our generative model \(_{}\). Hence, it indicates that the performance of our learned \(_{}\) is at least as good as that of a comparator generative model \(\) covered by \(p_{ off}\). While this original coverage term \(C_{}\) diverges when \(\) goes outside of \(_{ off}\), we can refine it using the extrapolation capabilities of a function class \(\), as we will discuss shortly. This refined version ensures that we can achieve high-quality designs that outperform designs in the offline data (i.e., best designs in \(_{ off}\)).

**Example 3**.: _We consider a scenario where an RKHS is used for \(\). Let \(\) be a model represented by an infinite-dimensional feature \(()\). Let \(\) denote the effective dimension of \(\)(Valko et al., 2013)._

**Corollary 1** (Informal: Formal characterization is in Section D ).: _Assuming that the model is well-specified, with probability \(1-\), we have:_

\[J_{}()-J_{}(_{})_{}} (^{3}}{n}}),_{}:=_{ :\|\|_{2}=1}_{ }[(x)^{}(x)]}{^{}_{ p _{}}[(x)^{}(x)]}.\]

_The refinement of the coverage term in \(_{}\) is characterized as the relative condition number between covariance matrices on a generative model \(\) and an offline data distribution \(p_{}\), which is smaller than \(C_{}\). This \(_{}\) could still be finite even if \(C_{}\) is infinite. In this regard, Corollary 1 illustrates that the trained generative model can outperform the best design in the offline data by harnessing the extrapolation capabilities of reward models._

## 7 Experiments

We perform experiments to evaluate (a) the effectiveness of conservative methods for fine-tuning diffusion models and (b) the comparison of our approach between existing methods for MBO with diffusion models (Krishnamoorthy et al., 2023; Yuan et al., 2023). We will start by outlining the baselines and explaining the experimental setups. Regarding more detailed setups, hyperparameters, architecture of neural networks, and ablation studies, refer to Appendix E.

Methods to compare.We compare the following methods in our evaluation. For a fair comparison, we always use the same \(\) in **BRAID** and **STRL**. 3.

* **BRAID (proposed method)**: We consider two approaches: (1) **Bonus**, as in Example 1 by setting a last layer as a feature map and constructing a kernel, (2) **Bootstrap**, as in Example 2.
* **Standard RL (STRL)**: RL-fine-tuning that optimizes the standard \(\) without any conservative term, following existing works on fine-tuning (Clark et al., 2023; Prabhudesai et al., 2023).
* **DDOM (Krishnamoorthy et al., 2023)**: We train with weighted classifier-free guidance (Ho and Salimans, 2022) using offline data, conditioning on a class with high \(y\) values (top \(5\%\)) during inference. Note that this method is training from scratch rather than fine-tuning.
* **Offline Guidance (Yuan et al., 2023)**: After training a classifier using offline data, we use guidance (conditional diffusion models) (Dhariwal and Nichol, 2021) on top of pre-trained diffusion models and condition on classes with high \(y\) values (top \(5\%\)) at inference time.

Evaluation.We assess the performance of each generative model primarily by visualizing the histogram of true rewards \(r(x)\) obtained from the generated samples. For completeness, we include similar histograms for both the pre-trained model (**Pretrained**) and the offline dataset (**Offline**). As for hyperparameter selection, such as determining the strengths of conservative terms/epochs, we adhere to conventional practice in offline RL (e.g., Rigter et al. (2022); Kidambi et al. (2020); Matsushima et al. (2020)) and choose the best one through a limited number of online interactions.

**Remark 2**.: _We omit comparisons with pure MBO methods for two reasons: (i) **DDOM**, which we compare against, already demonstrates a good performance across multiple datasets, and (ii) these methods are unable to model complex valid spaces since they do not incorporate state-of-the-art generative models (e.g., stable diffusion), thereby lacking the capability to generate valid designs (e.g., natural images) as we show in Section 7.2._

### Design of Regulatory DNA/RNA Sequences

We examine two publicly available large datasets consisting of enhancers (\(n 700k\)) (Gosai et al., 2023) and UTRs (\(n 300k\)) (Sample et al., 2019) with activity levels collected by massively parallel reporter assays (MPRA) (Inoue et al., 2019). These datasets have been extensively used in sequenceoptimization for DNA and RNA engineering, particularly for the advancement of cell and RNA therapy (Castillo-Hair and Seelig, 2021; Ghiri et al., 2023; Lal et al., 2024; Ferreira DaSilva et al., 2024). In the Enhancers dataset, each \(x\) is a DNA sequence with a length of \(200\), while \(y\) is the measured activity in cell lines. For the UTRs dataset, \(x\) is a 5' UTR RNA sequence with a length of \(50\) while \(y\) is the mean ribosomal load (MRL) measured by polysome profiling.

Setting of oracles and offline data.We aim to explore a scenario where we have a pre-trained model and an offline dataset. Since the true reward function \(r()\) is typically unknown, we initially divide the original dataset \(=\{x^{(i)},y^{(i)}\}\) randomly into two subsets: \(_{}\) and \(^{}\). Then, from \(^{}\), we select datasets below \(95\%\) quantiles for enhancers and \(60\%\) quantiles for UTRs and define them as offline datasets \(_{}\). Subsequently, we construct an oracle \(r()\) by training a neural network on \(_{}\) and use it for testing purposes. Here, we use an Enformer-based model, which is a state-of-the-art model for DNA sequences (Avsec et al., 2021). Regarding pre-trained diffusion models, we use ones customized for sequences over simplex space (Avdeyev et al., 2023). In the subsequent analysis, each algorithm solely has access to the offline data \(_{}\) and a pre-trained diffusion model, but not \(r()\).

Results.The performance results in terms of \(r()\) are depicted in Fig 1(a) and b. It is seen that fine-tuned generative models via RL outperform conditioning-based methods: **DDOM** and **Guidance**. This is expected because conditional models themselves are not originally intended to surpass the conditioned value (\(\) best value in the offline data). Conversely, fine-tuned generative models via RL are capable of exceeding the best value in offline data by harnessing the extrapolation capabilities of reward modeling, as also theoretically supported in Corollary 1. Secondly, both **BRAID-boot** and **BRAID-bonus** demonstrate superior performance compared to **STRL**. This suggests that conservatism aids in achieving fine-tuned generative models with enhanced rewards while mitigating overoptimization.

### Image Generation

We consider the task of generating aesthetically pleasing images, following prior works (Fan et al., 2023; Black et al., 2023). We use Stable Diffusion v1.5 as our pretrained diffusion model, which can generate high-quality images conditioned on prompts such as "cat" and "dog". We use the AVA

Figure 3: Results on Image Generation

Figure 2: Barplots of the rewards \(r(x)\) for samples generated by each algorithm. It reveals that proposals consistently outperform baselines.

dataset (Murray et al., 2012) as our offline data and employ a linear MLP on top of CLIP embeddings to train reward models (\(\) and \(-\)) from offline data for fine-tuning.

Setting of oracles.To construct \(r(x)\), following existing works, we use the LAION Aesthetic Predictor V2 (Schuhmann, 2022), already pre-trained on a large-scale image dataset. However, this LAION predictor gives high scores even if generated images are almost identical regardless of prompts, as in Figure 2(b). These situations are undesirable because it means fine-tuned models are too far away from pre-trained models. Hence, for our evaluation, we define \(r(x)\) as follows: (1) asking vision language models (e.g., LLaVA (Liu et al., 2024)) whether images contain objects in the original prompts 4 (e.g., dog, cat), (2) if Yes, outputting the LAION predictor, and (3) if No, assigning \(0\). This evaluation ensures that high \(r(x)\) still indicates capturing the space of the original stable diffusion.

Results.We show that our proposed approach outperforms the baselines _in terms of \(r(x)\)_, as in Fig 1(c) 4(c)4. We also show the generated images in Figure 2(c)4(c). Additionally, we plot the training curve during the fine-tuning process _in terms of the mean of \((x)\)_ of generated samples in Fig 2(c)4(c). The results indicate that in **STRL**, while the learning curve based on the learned reward quickly grows, fine-tuned models no longer necessarily remain within the space of pre-trained models (**STRL** in Fig 1(c)). In contrast, in our proposal, by carefully regularizing on regions outside of the offline data, we can generate more aesthetically pleasing images than **STRL**, which remain within the space of pre-trained models. For more images/ ablation studies, refer to Appendix E.

## 8 Summary

For the purpose of fine-tuning from offline data, we introduced a conservative fine-tuning approach by optimizing a conservative reward model, which includes additional penalization outside of offline data distributions. Through empirical and theoretical analysis, we demonstrate the capability of our approach to outperform the best designs in offline data, leveraging the extrapolation capabilities of reward models while avoiding the generation of invalid designs through pre-trained diffusion models.