# Natural Language Instruction-following with Task-related Language Development and Translation

Jing-Cheng Pang\({}^{}\), Xinyu Yang\({}^{}\), Si-Hang Yang, Xiong-Hui Chen, Yang Yu\({}^{}\)

National Key Laboratory of Novel Software Technology, Nanjing University

Polixir Technology

{pangjc,yangxy,yangsh,chenxh}@lamda.nju.edu.cn, yuy@nju.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Natural language-conditioned reinforcement learning (RL) enables agents to follow human instructions. Previous approaches generally implemented language-conditioned RL by providing the policy with human instructions in natural language (NL) and training the policy to follow instructions. In this is _outside-in_ approach, the policy must comprehend the NL and manage the task simultaneously. However, the unbounded NL examples often bring much extra complexity for solving concrete RL tasks, which can distract policy learning from completing the task. To ease the learning burden of the policy, we investigate an _inside-out_ scheme for natural language-conditioned RL by developing a task language (TL) that is task-related and easily understood by the policy, thus reducing the policy learning burden. Besides, we employ a translator to translate natural language into the TL, which is used in RL to achieve efficient policy training. We implement this scheme as TALAR (TAsk Language with predicAte Representation) that learns multiple predicates to model object relationships as the TL. Experiments indicate that TALAR not only better comprehends NL instructions but also leads to a better instruction-following policy that significantly improves the success rate over baselines and adapts to unseen expressions of NL instruction. Besides, the TL is also an effective sub-task abstraction compatible with hierarchical RL.

## 1 Introduction

Enabling robots to collaborate effectively with humans is a crucial aspect of machine intelligence. Natural language is a potent tool that reflects human intention and has been widely researched for instructing robot execution, designing rewards, and serving as an observation or action in reinforcement learning (RL). We are particularly interested in developing agents to follow human instructions in this broad context. Natural language-conditioned reinforcement learning (NLC-RL) is a promising approach to training instruction-following agents. It provides the policy with human instructions in natural language (NL) and trains it to follow instructions with RL algorithms. In this _outside-in_ learning (OIL) scheme, the natural language instructions are directly exposed to the policy, requiring it to understand and interpret them while accomplishing RL tasks (Fig. 1-left).

However, natural language is a complex and unbounded representation of human instruction. It places an additional burden on the policy to understand diverse natural languages when solving a specific RL task. To address this issue, previous studies have attempted to convert the natural language to simpler latent embeddings  using pre-trained language models such as BERT  and GPT . However, these embeddings are typically learned independently of the RL task, making capturing the task-related information in NL instructions difficult. Alternatively, some methods[5; 6; 7] design rule-based, task-related semantic vectors to represent NL instructions. These methods show promising results but require a laborious design of semantic vectors and have limitations in handling complex NL instructions.

This paper proposes a novel _Inside-Out_ Learning (IOL) scheme for NLC-RL. As shown in Fig. 1-right, IOL automatically generates a task language (TL) that is both task-related and concise, enabling easy comprehension by the policy. IOL consists of three key components: (1) a TL generator that develops task language, (2) a translator that translates NL into TL, which facilitates policy learning in RL, and (3) a policy that solves the RL tasks. We introduce a specific implementation of IOL, called TALAR for **TA**sk **L**anguage with predic**A**te **R**epresentation. In particular, TALAR models the TL generation process as a referential game, known for its effectiveness in developing effective language. Besides, TALAR learns to model the relationships between objects as TL. Given the labelled NL to TL, the translator is trained to minimize the difference between the translated TL with the target TL. Finally, arbitrary RL policy optimization algorithms can optimize the instruction-following policy.

Our paper presents several contributions to the field of NLC-RL. Firstly, we propose a novel NLC-RL scheme called IOL, which introduces a task language that enhances policy understanding and improves the learning efficiency of instruction-following. Secondly, we introduce a specific implementation of IOL that models TL development as a referential game and uses multiple learned predicates to represent TL. Thirdly, we conduct comprehensive experiments in two environments, FrankaKitchen , and CLEVR-Robot , demonstrating that the policy learned by our proposed method outperforms existing approaches in terms of its ability to follow natural language instructions and adapt to previously unseen instructions. Furthermore, the resulting TL effectively represents human instructions, providing a solid baseline for sub-task abstraction in hierarchical RL .

## 2 Related Work

This section begins with a summary of prior research on instruction following with RL, followed by two paragraphs discussing works pertinent to our methodology, i.e., language generation in RL and language translation.

**Instruction following with RL.** Instruction-following problems require agents to perform tasks specified by natural language instructions. A line of works solves the problems with semantics vectors [11; 12; 13], or language-conditioned policies [14; 15; 16]. To train the policies, previous methods typically process NL using a language model and train a following policy with RL algorithms. For example,  encodes a single-sentence instruction with a pre-trained language model and feeds the policy with the NL encoding.  learns a policy that maps NL instructions to action sequences by marginalizing implied goal locations.  combines human instructions with agent observations via a multiplication-based mechanism and then pre-trains the instruction-following policy using behaviour cloning . Instead of directly converting NL to encoding,  encodes NL to a manually-designed binary vector in which each element has semantics meaning. Besides, the instruction-following policy has close ties to Hierarchical RL  because the instructions can be naturally viewed as a task abstraction for a low-level policy . HAL  takes advantage of the compositional structure of NL and makes decisions directly at the NL level to solve long-term, complex RL tasks. These previous methods expose the unbounded NL instructions directly to the policy or encode the NL instructions to a scenario-specific manual vector, both of which have limitations. In contrast, our method automatically develops a task language, which is easily understood by the policy and facilitates policy learning.

**Language generation in RL.** Language generation plays a crucial role in developing task language. In reinforcement learning, researchers often approach this issue in a multi-agent setting, where agents

Figure 1: An illustration of OIL and IOL schemes in NLC-RL. **Left:** In OIL, the policy directly takes the (outside) NL instructions embedding as the input. **Right:** IOL first develops an (inside) TL, and policy takes TL as the input. The dashed lines represent TL development and translation, while the solid lines represent the instruction following the process.

must learn effective message protocols to communicate with their partners and complete tasks. For example, the study by  investigates emergent language in a referential game. At the same time, a line of works has explored multi-agent communication [23; 24] for practical cooperation, considering the communication message as language. Regarding language representation, prior research has often used discrete vectors due to the discrete nature of human language. For example,  enables agents to communicate using discrete messages and shows that discrete representation performs similarly to continuous representation with a much smaller vocabulary size. One-hot representation [26; 22] and binary representation [5; 27] are popular forms of discrete language representation. For instance,  uses a one-hot language representation to enable two agents to communicate and differentiate between images. In this paper, we develop task language following the discrete form of language representation while using the predication representation.

**Language translation.** In this paper, TALAR translates natural language into task language, which lies in the domain of language translation . The field of natural language processing (NLP) has extensively investigated various approaches to language translation [29; 30]. Among these approaches, the encoder-decoder architecture is a promising method because it can extract effective features from input sentences . For example, the study in  introduces a continuous latent variable as an efficient feature for language translation, utilizing a variational auto-encoder . In this paper, our primary focus is not on language translation. Therefore, we employ a more straightforward method for translation, utilizing the pre-trained BERT model as the NL encoder. In this approach, we consider NL as the source language and the target language (TL) as the target language.

## 3 Background

### RL and NLC-RL

A typical RL task can be formulated as a Markov Decision Process (MDP) [34; 35], which is described as a tuple \(=(,,P,r,,d_{0})\). Here \(\) represents the state space. \(\) is the finite action space defined by \(=\{a_{0},a_{1},,a_{||-1}\}\). \(P\) represents the probability of transition while \(r\) represents the reward function. \(\) is the discount factor determining the weights of future rewards, whereas \(d_{0}\) is the initial state distribution. A policy \(:()\) maps state space to the probability space over action space. In NLC-RL, the agent receives an NL instruction (\(L\)) that reflects the human's instruction on the agent. An instruction-following policy \((|s_{t},L)\) is trained to make decisions based on the current state \(s_{t}\) and NL instruction \(L\). The overall objective of NLC-RL is to maximize the expected return under different NL instructions:

\[_{t=0}^{}^{t}r(s_{t},a_{t},L)s_{0}  d_{0},a_{t}(|s_{t},L).\] (1)

For the accuracy of sake, we use \(L_{}\) and \(L_{}\) to denote NL and TL, respectively.

### Referential Game

Referential game (RG) has been established as a multi-agent communication task , which consists of two agents: a _sender_ and a _receiver_. The two agents communicate using their language to identify a particular object or concept in a specific task. In a classic referential game, a pair of images is presented to the sender and the receiver, with the sender being informed which of the two images is the target. The sender must generate an effective communication message that enables the receiver to identify the target image successfully. Through interactive communication, the agents develop _meaningful_ language. As the sender must capture the key feature of the task, the RG presents an ideal setting for generating task language. In TALAR, we use an extension to the basic RG to generate TL, which will be elaborated in Section 4.

### Predicate Representation

Predicate representation utilizes discrete binary vectors to describe the relationships between objects or abstract concepts. The predicate representation is formally defined as comprising two key components: (1) _predicate_ signifies the relationship between objects or concepts, which may involve one or multiple objects. (2) _argument_ serves as the input for the predicate, indicating the specific objects between which the predicate measures the relationship. By employing these components,predicate representation offers a clear and concise depiction of the relation between objects. For instance, the predicate representation vector3 could represent a predicate expression Pred(a,b). In this case, Pred is a predicate that signifies a relationship, and the symbols a and b are its arguments. In the vector, the initial code  indicates that the value of Pred is True (i.e., the relationship holds). In contrast, the following red and blue one-hot codes represent the indexes of arguments a and b, respectively.

Prior research has shown that predicate representation can be learned by employing networks to output predicate values. In this manner, the learning system can automatically identify relationships in a block stacking task . In TALAR, neural networks learn both predicates and their arguments. For additional discussions on predicate representation, refer to Appendix A.

### Natural Language as Input for the Neural Network

NL sentences are variable lengths and cannot be fed directly into a fully connected network. A standard solution is to encode each word as an embedding  and loop over each embedding using a recurrent model. Besides the recurrent model, the pre-trained language model, e.g., Bert , tokenizes the words in the sentence and extracts sentence embedding based on these tokens. This process involves passing the input sentence through a series of transformer-based encoder layers that capture contextual information and create representations of the input tokens. BERT then applies a pooling layer to obtain a fixed-length vector representation of the input sentence. In this study, we employ Bert to process NL sentences to fixed-length vectors because it is lightweight and efficient.

## 4 Inside-out Learning and Implementation

This section introduces our primary contribution to efficient policy learning in NLC-RL, the IOL framework and its specific implementation, TALAR. We begin by formulating the IOL framework.

### Inside-out Learning Formulation

We consider NLC-RL tasks, where the agent is provided with a natural language instruction as a goal to be completed. The key idea of IOL is to develop a task-related language to facilitate policy understanding. To implement IOL, three key components must be considered in IOL. We will elaborate on these components and discuss their impact on solving NLC-RL tasks.

**TL generator.** The TL generator is the critical component in IOL, designed to develop a task-related language. We expect the resulting TL to effectively and succinctly convey the task's objective to the policy. In this way, IOL mitigates the issues encountered in OIL, where the policy struggles to comprehend unseen natural language instructions. We believe that _expressiveness_ and _conciseness_ are vital properties of TL. Expressiveness ensures the TL accurately conveys the task goal, while conciseness promotes policy comprehension. To fulfil these requirements, we propose representing TL using predicate representation, considered both expressive and concise .

**Translator and Policy learning.** The translator serves as a tool that connects policy with TL, which translates NL to TL. Utilizing the translator, the policy can be optimized through various RL algorithms to improve its ability to follow human instructions. The policy learning is a goal-conditioned RL  problem. For complex tasks, plenty of well-established techniques in machine translation and goal-conditioned RL domains can be employed to improve policy learning in IOL further. Note that the primary focus of this paper is to investigate the effectiveness of IOL. Therefore, we employ straightforward translator and policy learning implementations, as detailed in Section 4.2.

### Our Method

We first introduce the task dataset, which is used for TL development and translation in TALAR.

_Definition \(1\)_ (**Task dataset**). _A task dataset \(=\{(s,s^{},L_{})_{i}\}\) consists of multiple triplets. Each triplet contains a natural language instruction \(L_{}\) and a task state pair \((s,s^{})\), where \(L_{}\) describes state change from \(s\) to \(s^{}\) in natural language, e.g., move the red ball to the blue ball._We use a state pair instead of a single state for the following reasons: (1) NL instruction typically describes a state change, e.g., turning the wheel to the left; (2) it is not straightforward to describe a single state concisely in complex task scenarios. Fig. 2 illustrates how TALAR makes use of task dataset for task language development and translation. The subsequent subsections will elaborate on training three critical components of TALAR, i.e., task language development, natural language translation and policy optimization.

#### 4.2.1 Task Language Development in Predicate Representation

The TL generator denoted as \(g_{}(s,s^{})\), aims to develop a language in predicate representation (Section 3.3). In TALAR, the TL generator is represented by neural networks and parameterized with \(\). It takes a state pair \((s,s^{})\) as the input and outputs task language \(L_{T}\). The We will discuss how TALAR constructs a referential game to facilitate the development of task language.

**Referential game.** The RG process is presented in the right part of Fig. 2. Based on the RG framework in Section 3.2, TALAR adapts it such that the TL generator serves as the _sender_4 and generates the TL \(L_{}=g_{}(s,s^{})\) to describe the state change of the given state pair. Conditioned on the generated TL \(L_{}\), a _receiver_ aims to predict the word in the NL instruction corresponding to \((s,s^{})\). In this way, the TL generator must capture the critical information in the given state pair and generate effective TL, ensuring that the receiver can predict correctly. We use a pre-trained BERT model followed by a fully-connected layer to implement the receiver to predict the next token. We denote this as \(f_{}(T_{i}|L_{},b(T_{1,2...i-1}))\), where \(f\) is the fully-connected layer after the BERT output, \(b\) is the BERT model, \(T_{i}\) is the \(i\)-th token of the sentence, and \(\) represents the parameters of the fully-connected layer. The token to be predicted \(T_{i}\) is randomly sampled from the whole sentence. The objective for training the TL generator and receiver is to minimize the negative log-likelihood of the predicted token given the generated TL and the context tokens, which is expressed as:

\[_{}(,)=,L_{}),T_{i} T_{L_{}}}{}[- f_{ }(T_{i}|g_{}(s,s^{}),b(T_{1,...,i-1}))],\] (2)

where \(T_{L_{}}\) is the token set of \(L_{}\). In practice, we fix the parameters of BERT and only optimize the fully-connected layer.

**Network architecture of TL generator.** We design a specific network architecture for learning the predicate representation (we refer readers to Section 3.3 for elaborations on predicate representation). As illustrated in Fig. 2(b), the TL generator first extracts \(N_{}\) arguments \((_{1},_{2},,_{N_{}})\) based on the input state pair, and subsequently determines the Boolean values of \(N_{}\) predicates, given the extracted argument list. The predicate values are concatenated with the argument list and form the task language \(L_{}^{i}=(_{1},,_{N_{}n}, _{1},,_{N_{}})\). The number of predicate networks \(N_{}\) and arguments networks \(N_{}\) can be adjusted according to the RL task scale.

Figure 2: Overall framework of TALAR. **(a)** Overall training process: The TL generator develops TL via playing a referential game with a receiver, and the translator translates NL to TL. **(b)** Network architecture of the TL generator. **(c)** Network architecture of the translator. The number of predicate arguments and networks can be adjusted according to the task scale.

Specifically, the arguments for the predicate are extracted by argument networks, denoted by \(_{i}(s,s^{})\). An argument network is implemented as a fully-connected network ending with a Gumbel-Softmax activation layer . Through the Gumbel-Softmax, the argument network can output a discrete one-hot vector \(_{i}\) in form like \((0,1,,0)\), which represents an abstract object or concept in the task. TALAR utilizes multiple predicate networks, denoted by \(_{i}(s,s^{},_{1},,_{N_{}})\), to determine the Boolean values of a set of predicates. These predicates are anonymous as we do not need to pre-define them in advance. This learning manner requires predicate networks to automatically discover meaningful relationships in the task. Each predicate network outputs a 0-1 value, ending with a Gumbel-Softmax layer. All these 0-1 values are concatenated with the argument list, yielding the task language \(L_{}\), a discrete binary vector. Note that without the argument list in \(L_{}\), the resulting language cannot express different objects and therefore loses its expressiveness. The Gumbel-Softmax activation technique permits the differentiation of the entire TL generation procedure.

#### 4.2.2 Natural Language Translation

The objective of the translator is to translate the natural language to the task language. TALAR uses a pre-trained BERT model to convert \(L_{}\) into a fixed-length embedding, followed by fully-connected layers to predict the task language corresponding to \(L_{}\). Fig. 2(d) presents the structure of the translator. We let \(}}\) denote the TL generated by the translator, and \(p_{}\) the fully-connected layers parameterized with \(\). The translator is trained to minimize the difference between the predicted TL with the target TL:

\[_{}()=}_{(s,s^{},L_{}) }- p_{}(g_{}(s,s^{})|b(T(L_{}))).\] (3)

Here \(T(L_{})\) denotes the tokenized natural language sentence. The optimized translator trains an instruction-following policy to complete the human instructions, as described below.

#### 4.2.3 Policy Optimization

TALAR uses reinforcement learning to train an Instruction-Following Policy (IFP) \((|s,}})\). When the agent collects samples from the environment, the task generates a random human instruction in NL, which is then translated into the task language \(}}\) by the translator. Next, the IFP makes decisions for the entire episode based on the current observation and \(}}\) until completing the instruction or reaching the maximum timestep. The IFP can be optimized with an arbitrary RL algorithm using the samples collected from the environments. We use PPO  for TALAR and all baselines in our implementation. Note that during IFP training, the translator's parameters are fixed to prevent the translator from overfitting the current IFP.

## 5 Experiments

We conduct a series of experiments to evaluate the effectiveness of TALAR and answer the following questions: (1) How does TALAR perform compared to existing NLC-RL approaches when learning an instruction-following policy? (Section 5.1) (2) Can TALAR learn effective task language? (Section 5.2) (3) Can TL acquire any compositional structure and serve as an abstraction for hierarchical RL? (Section 5.3) (4) What is the impact of each component on the overall performance of TALAR? (Section 5.4)

**Evaluation environments**. We conduct experiments in FrankaKitchen  and CLEVR-Robot  environments, as shown in Fig. 3. FrankaKitchen aims to control a 9-DoF robot to manipulate various objects in a kitchen, involving six sub-tasks: open the microwave door, move the kettle to the top left burner, turn on the light switch, open the slide cabinet, activates the top burner and activate the bottom burner. We treat

Figure 3: A visualization of environments in our experiments. (a) FrankaKitchen. The agent controls a 9-DoF robot to manipulate various objects in a kitchen. (b) CLEVR-Robot. The agent (sil-vepoint) manipulates five movable balls to reach a specific goal configuration. An example of NL instruction is: _Can you move the red ball to the left of the blue ball?_

each sub-task as a different goal configuration. In each trajectory, the environment randomly generates an NL instruction that describes a goal configuration. We let ChatGPT  generate 50 different NL instructions to describe each goal configuration, yielding 300 different NL instructions in this task. CLEVR-Robot is an environment for object interaction based on the MuJoCo physics engine . It contains five movable balls and an agent (silver point). In each trajectory, the agent aims to achieve a goal configuration that requires moving a specific ball in a target direction (front, behind, left, or right) relative to a target ball. An example of NL instruction is _Move the green ball to the left of the blue ball._ There are 80 different goal configurations in total. We use 18 natural language sentence patterns to describe each goal, resulting in 1440 distinct NL instructions. We split the NL instructions into two tasks: training and the testing set. The training set comprises 40 and 9 different NL instructions for each goal configuration in FrankaKitchen and CLEVR-Robot, respectively, resulting in a total of 240 NL instructions for FrankaKitchen and 720 for CLEVR-Robot. During training, the agent can only interact with the NL instructions in the training set. The testing set consists of the remaining NL instructions, which include 120 NL instructions for FrankaKitchen and 720 for CLEVR-Robot. Refer to Appendix D.1 for more details about the environments and the NL instructions in the training and testing sets.

**Task dataset collection**. The task dataset can be obtained from pre-collected data, where state pairs are annotated with natural language instructions through human input or rule-based functions. In our experiment, we utilize PPO to train a goal-conditioned policy capable of achieving any goal configuration. We then use the trained policy to collect trajectory data \((\{s_{0},a_{0},s_{1},a_{1} s_{T}\},L_{})\) across various environments, where \(L_{}\) is the natural language that describes the goal configuration of this trajectory and is randomly sampled from the training instructions set. The collected trajectory data is rearranged, with \(\{(s_{i},s_{T},L_{})\}_{i=0}^{T-1}\) as tuples in the task dataset, resulting in 50,000 tuples for the FrankaKitchen task, and 100,000 for the CLEVR-Robot task. Note that the collected trajectory data can be used to pre-train the policy. With tuple data such as \((s_{i},a_{i},L_{})\), the policy \((s,L_{})\) can be pre-trained to output the action \(a\). Two baselines (Pre-trained BERT-* in Section 5.1) utilize this data for pre-training purposes.

**Implementation Details.** For experiments, we utilize the pre-trained BERT-base-uncased model as the NL encoder. We conduct all experiments with five random seeds, and the shaded area in the figures represents the standard deviation across all five trials. We refer readers to Appendix D for more implementation details about the experiments.

### Performance of Instruction-following Policy

**Baselines for comparison.** We consider multiple representative methods in NLC-RL as baselines: (1) **One-hot** encodes the representation of all possible natural language instructions (including training and testing) to a one-hot vector, where each instruction is assigned a unique position. (2) **BERT-binary** processes the natural language with a pre-trained BERT model, and the resulting language encoding is then transformed into a binary vector using a fully-connected network. This binary vector's size equals the TL generated by TALAR. To ensure differentiability, we employ a reparameterization trick  that converts a continuous vector into a binary vector. (3) **BERT-continuous** is similar to BERT-binary, except it uses a continuous vector of the same size. (4) **Pre-trained BERT-binary/continuous** initially pre-trains the policy networks and the BERT model using behavior cloning  with pre-collected \((s_{i},a_{i},L_{})\) tuples, and subsequently trains the policy networks with RL. The data collection process is described above alongside the task dataset collection. (5) **GLA** (Grounding Language to latent Action)  first employs an auto-encoder to learn a latent

Figure 4: Training curves of different methods on two evaluation environments. The x-axis represents the timesteps agent interacts with the environment, and the y-axis represents the success rate of completing the instructions. The shaded area stands for the standard deviation over five random seeds.

action embedding and then encodes the NL into the learned latent action space, which is a binary vector with the same size as TL.

**Experimental results.** Fig. 4 presents the training curves of the instruction-following policies on different natural language instruction datasets. In general, TALAR achieves a superior instruction-following policy that enhances the success rate by 63.2% on FrankaKitchen and 13.4% on CLEVR-Robot compared to baseline method and generalizes well to the NL instruction expressions that were not previously encountered. On the FrankaKitchen task requiring more difficult robot manipulation, TALAR significantly outperforms all baselines. This result demonstrates the advantage of IOL, which eases the policy's burden on understanding natural language. On the training NL instruction set, TALAR achieves a high success rate within a reasonable number of timesteps, significantly faster than the other baselines. Additionally, TALAR demonstrates a remarkable success rate on testing sets in both environments, showcasing greater capacity than the baselines. Although One-hot performs adequately on the training NL set, its ability to generalize to the testing NL sets is limited. BERT-based baselines, built on the OIL framework, improve slower than TALAR on the training NL instruction set. In addition, the NL instructions on the FrankaKitchen task are typically more complex. OIL methods become increasingly more challenging to achieve a high task success rate.

### Analysis on the Generated Task Language

We further investigate the reasons behind TALAR's more efficient learning compared to baseline methods. To explore this issue, we employ the optimized translator to convert NL instructions into TL and project the resulting TL onto a two-dimensional plane using t-SNE . The projection results are displayed in Fig. 4(a). The results show that the learned TL effectively distinguishes NL instructions representing different goal configurations, developing a unique representation for each configuration. This conclusion is supported by the fact that various NL expressions for the same goal configuration translate into similar, or even identical, TL. Consequently, TALAR can easily recognize these expressions and concentrate on learning how to complete the RL task. For comparison, we also project the NL encoding output generated by other baseline methods, as illustrated in Fig. 4(b-d). The NL encoding output from these baseline methods exhibits greater diversity compared to TALAR. These results suggest that the OIL baseline treats diverse NL expressions for the same task objective as different goal configurations, potentially distracting from RL and slowing policy learning. It is worth noting that BERT, an established pre-trained model for natural language understanding, also fails to generate unique NL encoding. Even when fine-tuned on specific tasks, the output encoding of the BERT model is divergent, as shown in Fig. 4(d).

### TL Is an Effective Abstraction for Hierarchical RL

Previous experiments have demonstrated that the resulting TL can compactly and uniquely represent the NL instructions, facilitating policy learning in the following instruction. In this section, we further explore whether the TL learns any compositional structure by examining if it can serve as an effective goal abstraction for hierarchical RL. Specifically, we train a high-level policy outputting a TL, instructing the IFP to complete a low-level task. We define a long-term task as one in which the

Figure 4: The t-SNE projections of different NL encoding types on FrankaKitchen. Points with the same marker encode forty different NL expressions that describe the same goal configuration. We add a slight noise to the overlapping points for better presentation. **(a)** The translatorâ€™s t-SNE representations of the TL output. **(b)** The encoding output by the language encoder in GLA. **(c)** The encoding output by the NL encoding layer of the BERT model. **(d)** The encoding output by the NL encoding layer of the Pre-trained BERT-continuous.

agent must achieve multiple goal configurations simultaneously. For CLEVR-Robot, we constructed the long-term task following the approach outlined in . In this task, the agent's objective is to arrange objects to satisfy all ten goal configurations that are implicitly contained within it. For FrankaKitchen, the agent is required to complete all six sub-tasks at the same time. Refer to Appendix D.1.3 for additional details regarding the long-term tasks and the hierarchical RL setting.

For comparison, we consider a baseline method called **HAL**, which leverages the compositional structure of NL and directly acts at the NL level. Following HAL's original implementation, the high-level policy outputs the index of the NL instruction rather than the NL words. To ensure a fair comparison, we align all other modules except for the high-level policy of both methods, e.g., using the same IFP trained by TALAR as the low-level policy. Fig. 7 presents the comparison results. The high-level policy that uses TL as a low-level goal abstraction outperforms that using NL in terms of sample efficiency. This result demonstrates that TL can be an effective goal abstraction naturally compatible with hierarchical RL. In practice, the TL can be generated with a task dataset and serves as a sub-task abstraction for high-level policy.

### Ablation Study

We conduct experiments to investigate how the performance of TALAR is affected by varying the number of predicate networks and the number of arguments (i.e., \(N_{}\) and \(N_{}\)). Specifically, we choose \(N_{}\) from the set {2, 4, 6} and \(N_{}\) from {2, 3, 4}. We trained the IFPs for 4 million timesteps on the FrankaKitchen environment. The experiment results are presented in Fig. 7. In general, increasing \(N_{}\) and \(N_{}\) leads to improved performance of the IFP on both the training and testing sets. This result can be attributed to the enhanced representation abilities of the resulting TL, which enables more efficient learning by the policy. However, it is worth noting that excessively high values of \(N_{}\) and \(N_{}\) may lead to reduced training efficiency due to the simultaneous learning of multiple networks. \(N_{}=6\) and \(N_{}=4\) are sufficient for the robot manipulation tasks like FrankaKitchen.

## 6 Conclusion and Future Work

In this study, we introduce a novel learning scheme for NLC-RL, called IOL, which develops task-related language to facilitate policy learning. Our experiments demonstrate that IOL is a promising approach for efficiently training instruction-following agents. However, there are limitations in our proposed implementation of IOL, TALAR. Firstly, TALAR develops the task language using a static task dataset and, therefore, can not be directly applied to the open-world environment, where the NL instructions could vary hugely from the dataset. It is possible to mitigate this issue by dynamically extending the task dataset and fine-tuning the TL generator/translator during the policy learning process. Additionally, TALAR requires a manual reward function for policy training, which may be inaccessible if the reward design is complex. Fortunately, there have been well-validated methods for solving sparse reward problems [48; 49; 50], an effective substitute for the manual reward function. Furthermore, incorporating basic properties of predicate relationships (such as transitivity, reflexivity, and symmetry) when training the TL generator could make the resulting TL more meaningful and self-contained. Lastly, TALAR implements the translator as a fully-connected layer with a pre-trained BERT model, which may be inadequate for more complex natural language instructions. Thislimitation could be addressed by employing other Natural Language Processing (NLP) literature techniques, such as semantic parsing.

Recently, there has been a growing interest in building instruction-following agent with LLMs as the _brain_. Typically, LLMs function as planners, while a low-level policy is responsible for executing the planned tasks. However, the separation between the LLM planner and the task execution process may impede the overall task completion rate. The challenge of managing increasingly complex language instructions in open environments remains an unresolved issue. Furthermore, it is worth investigating whether LLMs can serve as effective task language generator. We hope that future research will explore these intriguing questions and contribute to developing agents that interact with humans more effectively.