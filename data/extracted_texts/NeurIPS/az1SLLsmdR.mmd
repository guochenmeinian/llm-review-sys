# Elucidating the Design Space of Dataset Condensation

Shitong Shao\({}^{}\), Zikai Zhou\({}^{}\), Huanran Chen\({}^{}\), Zhiqiang Shen\({}^{}\)\({}^{*}\)

\({}^{}\) Mohamed bin Zayed University of AI, \({}^{}\) Tsinghua University

\(\) The Hong Kong University of Science and Technology (Guangzhou)

{1090784053sst, choukai003}@gmail.com, huanran_chen@outlook.com zhiqiang.shen@mbzuai.ac.ae, \(*\): Corresponding author

###### Abstract

Dataset condensation, a concept within _data-centric learning_, aims to efficiently transfer critical attributes from an original dataset to a synthetic version, meanwhile maintaining both diversity and realism of syntheses. This approach can significantly improve model training efficiency and is also adaptable for multiple application areas. Previous methods in dataset condensation have faced several challenges: some incur high computational costs which limit scalability to larger datasets (_e.g.,_ MTT, DREAM, and TESLA), while others are restricted to less optimal design spaces, which could hinder potential improvements, especially in smaller datasets (_e.g.,_ SRe\({}^{2}\)L, G-VBSM, and RDED). To address these limitations, we propose a comprehensive designing-centric framework that includes specific, effective strategies like implementing soft category-aware matching, adjusting the learning rate schedule and applying small batch-size. These strategies are grounded in both empirical evidence and theoretical backing. Our resulting approach, **E**lucidate **D**ataset **C**ondensation (**EDC**), establishes a benchmark for both small and large-scale dataset condensation. In our testing, EDC achieves state-of-the-art accuracy, reaching 48.6% on ImageNet-1k with a ResNet-18 model at an IPC of 10, which corresponds to a compression ratio of 0.78%. This performance surpasses those of SRe\({}^{2}\)L, G-VBSM, and RDED by margins of 27.3%, 17.2%, and 6.6%, respectively.

## 1 Introduction

Dataset condensation, also known as dataset distillation, has emerged in response to the ever-increasing training demands of advanced deep learning models (He et al., 2016, 2016, 2020). This task addresses the challenge of requiring massive amount of data to train high-precision models while also being bounded by resource constraints (Dosovitskiy et al., 2020, Shao et al., 2024). In the conventional setup of this problem, the original dataset acts as a "teacher", distilling and preserving essential information into a smaller, surrogate "student" dataset. The ultimate goal of this technique is to achieve comparable performance of models trained on the original and condensed datasets from scratch. This task has become popular in various downstream applications, including continual learning (Masarczyk and Tautkute, 2020, 2022, 2021), neural architecture search (Such et al., 2020, 2020, 2021), and training-free network slimming (Liu et al., 2017).

However, the common solution in traditional dataset distillation methods of bi-level optimization requires prohibitively expensive computation, which limits the practical usage, as in prior works (Cazenavette et al., 2022, 2023, 2023). This has become more severe particularly when being applied to large-scale datasets like ImageNet-1k (Russakovsky et al., 2015). In response, the uni-level optimization paradigm has gained significant attention as an alternative solution, with recent contributions from the research community (Yin et al., 2023, 2024, 2023) highlighting its applicability. These methods primarily leverage the richand extensive information from static, pre-trained observer models, to facilitate a more streamlined optimization process for synthesizing a condensed dataset without the need to adjust other parameters (_e.g.,_ those within the observer models). While uni-level optimization has demonstrated remarkable performance on large datasets, it has yet to achieve the competitive accuracy levels seen with classical methods on small-scale datasets like CIFAR-10/100 (Krizhevsky et al., 2009). Moreover, the recently proposed training-free method RDED (Sun et al., 2024) outperforms training-based methods in efficiency and maintains effectiveness, yet it overlooks the potential information incompleteness due to the lack of optimization on syntheses. Also, some simple but promising skills (_e.g.,_ smoothing learning rate schedule) that could enhance performance have not been well-explored in the existing literature. We observe that a performance improvement of 16.2% in RDED comes from these techniques in this paper rather than the proposed data synthesis approach.

These drawbacks show the constraints of previous methods in several respects, highlighting the need for a thorough investigation and assessment of potential limitations in prior frameworks. In contrast to earlier strategies that targeted one or a few specific improvements, our approach systematically examines all possible facets and integrates them into our comprehensive framework. To establish a strong framework, we carefully analyze all potential deficiencies in different stages of the data synthesis, soft label generation, and post-evaluation stages during dataset condensation, resulting in an extensive exploration of the design space on both large-scale and small-scale datasets. As a result, we introduce Elucidate **D**ataset **C**ondensation (**EDC**), which includes a range of concrete and effective enhancement skills for dataset condensation (refer to Fig. 1). For instance, _soft category-aware matching_ () ensures consistent category representation between the original and condensed data batches for more precise matching. Overall, EDC not only achieves state-of-the-art performance on CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet-10, and ImageNet-1k, using only half of the computational cost compared to the _baseline_ G-VBSM, but it also provides in-depth both empirical and theoretical insights and explanations that affirm the soundness of our design decisions. Our code is available at: https://github.com/shaoshitong/EDC.

## 2 Dataset Condensation

**Preliminary.** Dataset condensation involves generating a synthetic dataset \(^{}:=\{_{i}^{},_{i}^{ }\}_{i=1}^{|^{}|}\) consisting of images \(^{}\) and labels \(^{}\), designed to be as informative as the original dataset \(^{}:=\{_{i}^{},_{i}^{ }\}_{i=1}^{|^{}|}\), which includes images \(^{}\) and labels \(^{}\). The synthetic dataset \(^{}\) is substantially smaller in size than \(^{}\) (\(|^{}||^{}|\)). The goal of this process is to maintain the critical attributes of \(^{}\) to ensure robust or comparable performance during evaluations on real test protocol \(_{}\).

\[_{(,)_{}}[ _{}(,,^{*})],\ \ \ ^{*}=_{}_{(_{i}^{},_{ i}^{})^{}}[((_{i}^{ }),_{i}^{})].\] (1)

Here, \(_{}(,,^{*})\) represents the evaluation loss function, such as cross-entropy loss, which is parameterized by the neural network \(^{*}\) that has been optimized from the distilled dataset \(^{}\). The data synthesis process primarily determines the quality of the distilled datasets, which transfers desirable knowledge from \(^{}\) to \(^{}\) through various matching mechanisms, such as trajectory matching (Cazenavette et al., 2022), gradient matching (Zhao et al., 2021), distribution matching (Zhao and Bilen, 2023) and generalized matching (Shao et al., 2023).

**Small-scale vs. Large-scale Dataset Condensation/Distillation.** Traditional dataset condensation algorithms, as referenced in studies such as (Wang et al., 2018; Cazenavette et al., 2022; Cui et al., 2023; Wang et al., 2022; Nguyen et al., 2020), encounter computational challenges and are generally confined to small-scale datasets like CIFAR-10/100 (Krizhevsky et al., 2009), or larger datasets with limited class diversity, such as ImageNette (Cazenavette et al., 2022) and ImageNet-10 (Kim et al., 2022). The primary inefficiency of these methods stems from their reliance on a bi-level optimization framework, which involves alternating updates between the synthetic dataset and the observer model utilized for distillation. This approach not only heavily depends on the model's intrinsic ability but also limits the versatility of the distilled datasets in generalizing across different architectures. In contrast, the uni-level optimization strategy, noted for its efficiency and enhanced performance on the regular 224\(\)224 scale of ImageNet-1k in recent research (Yin et al., 2023; Shao et al., 2023; Yin and Shen, 2024), shows reduced effectiveness in smaller-scale datasets due to the massive optimization-based iterations required in the data synthesis process without a direct connection to actual data. Recent new methods in training-free distillation paradigms, such as in (Sun et al., 2024;Zhou et al., 2023), offer advancements in efficiency. However, these methods compromise data privacy by sharing original data and do not leverage statistical information from observer models to enhance the capability of synthetic data, thereby restraining their potential in a real environment.

**Generalized Data Synthesis Paradigm.** We consistently describe algorithms (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023; Sun et al., 2024) that efficiently conduct data synthesis on ImageNet-1k as "generalized data synthesis" as these methods are applicable for both small and large-scale datasets. This direction usually avoids the inefficient bi-level optimization and includes both image and label synthesis phases. Note that several recent works (Zhang et al., 2024, 2024; Deng et al., 2024), particularly DANCE (Zhang et al., 2024), can also effectively be applied to ImageNet-1k, but these methods lack enhancements in soft label generation and post-evaluation. Specifically, generalized data synthesis involves first generating highly condensed images followed by acquiring soft labels through predictions from a pre-trained model. The evaluation process resembles knowledge distillation (Hinton et al., 2015), aiming to transfer knowledge from a teacher to a student model (Gou et al., 2021; Hinton et al., 2015). The primary distinction between the training-dependent (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023) and training-free paradigm (Sun et al., 2024) centers on their approach to data synthesis. In detail, the training-dependent paradigm employs _Statistical Matching (SM)_ to extract pertinent information from the entire dataset.

\[_{}&=||p(| ^{})-p(|^{})||_{2}+||p(^{ 2}|^{})-p(^{2}|^{})||_{2}, \ s.t.\ _{}_{},\\ ^{*}&=*{arg\, min}_{^{}}_{_{}_{ }}[_{}(^{},^{})],\] (2)

where \(_{}\) represents the extensive collection of statistical matching operators, which operate across a variety of network architectures and layers as described by (Shao et al., 2023). Here, \(\) and \(^{2}\) are defined as the mean and variance, respectively. For more detailed theoretical insights, please refer to Definition 3.1. The training-free approach, as discussed in (Sun et al., 2024; Zhou et al., 2023), employs a direct reconstruction method for the original dataset, aiming to generate simplified representations of images.

\[^{}=_{i=1}^{}^{}_ {i},\ ^{}_{i}=\{^{i}_{j}=(\{}_{k}\}_{i=1}^{N}^{}_{i})\}_{j=1}^{ },\] (3)

where \(\) denotes the number of classes, \(()\) represents the concatenation operator, \(^{}_{i}\) signifies the set of condensed images belonging to the \(i\)-th class, and \(^{}_{i}\) corresponds to the set of original images of the \(i\)-th class. It is important to note that the default settings for \(N\) are 1 and 4, as specified in the works (Zhou et al., 2023) and (Sun et al., 2024), respectively. Using one or more observer models, denoted as \(\{_{i}\}_{i=1}^{N}\), we then derive the soft labels \(^{}\) from the condensed image set \(^{}\).

\[^{}=_{^{}_{i} ^{}}_{i=1}^{N}_{i}(^{ }_{i}).\] (4)

This plug-and-play component, as outlined in SRe\({}^{2}\)L (Yin et al., 2023) and IDC (Kim et al., 2022), plays a crucial role for enhancing the generalization ability of the distilled dataset \(^{}\).

Figure 1: **Illustration of Elucidating Dataset Condensation (EDC). Left: The overall of our better design choices in dataset condensation on ImageNet-1k. Right: The evaluation performance and data synthesis required time of different configurations on ResNet-18 with IPC 10. Our integral EDC refers to CONFIG G.**

## 3 Improved Design Choices

Design choices in data synthesis, soft label generation, and post-evaluation significantly influence the generalization capabilities of condensed datasets. Effective strategies for small-scale datasets are well-explored, yet these approaches are less examined for large-scale datasets. We first delineate the limitations of existing algorithms' design choices on ImageNet-1k. We then propose solutions, providing experimental results as shown in Fig. 1. For most design choices, we offer both theoretical analysis and empirical insights to facilitate a thorough understanding, as detailed in Sec. 3.2.

### Limitations of Prior Methods

**Lacking Realism (_solved by_ ).** Training-dependent condensation algorithms for datasets, particularly those employed for large-scale datasets, typically initiate the optimization process using Gaussian noise inputs (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023). This initial choice complicates the optimization process and often results in the generation of synthetic images that do not exhibit high levels of realism. The limitations in visualization associated with previous approaches are detailed in Appendix F.

**Coarse-grained Matching Mechanism (_solved by_ ).** The _Statistical Matching (SM)_-based pipeline (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023) computes the global mean and variance by aggregating samples across all categories and uses these statistical parameters for matching purposes. However, this strategy exhibits two critical drawbacks: it does not account for the domain discrepancies among different categories, and it fails to preserve the integrity of category-specific information across the original and condensed samples within each batch. These limitations result in a coarse-grained matching approach that diminishes the accuracy of the matching process.

**Overly Sharp of Loss Landscape (_solved by_ ) _and_ ).** The optimization objective \(()\) can be expanded through a second-order Taylor expansion as \((^{*})+(-^{*})^{}_{} (^{*})+(-^{*})^{}(- ^{*})\), with an upper bound of \((^{*})+||||_{}[||- ^{*}||_{2}^{2}]\) upon model convergence (Chen et al., 2024). However, earlier training-dependent condensation algorithms neglect to minimize the Frobenius norm of the Hessian matrix \(\) to obtain a flat loss landscape for enhancing its generalization capability through sharpness-aware minimization theory (Foret et al., 2020; Chen et al., 2022). Please see Appendix C for more formal information.

**Irrational Hyperparameter Settings (_solved by_ ),,, _and_ ).** RDED (Sun et al., 2024) adopts a smoothing LR schedule () and (Liu et al., 2023; Yin and Shen, 2024; Sun et al., 2024)

Figure 2: **(a):** Illustration of soft category-aware matching () using a Gaussian distribution in \(^{2}\). **(b):** The effect of employing smoothing LR schedule () on loss landscape sharpness reduction. **(c) top:** The role of flatness regularization () in reducing the Frobenius norm of the Hessian matrix driven by data synthesis iteration. **(c) bottom:** Cosine similarity comparison between local gradients (obtained from original and distilled datasets via random batch selection) and the global gradient (obtained from gradient accumulation).

use a reduced batch size () for post-evaluation on the full 224\(\)224 ImageNet-1k. These changes, although critical, lack detailed explanations and impact assessments in the existing literature. Our empirical analysis highlights a remarkable impact on performance: absent these modifications, RDED achieves only 25.8% accuracy on ResNet18 with IPC 10. With these modifications, however, accuracy jumps to 42.0%. In contrast, \(^{2}\) and G-VBSM do not incorporate such strategies in their experimental frameworks. This work aims to fill the gap by providing the first comprehensive empirical analysis and ablation study on the effects of these and similar improvements in the field.

### Our Solutions

To address these limitations described above, we explore the design space and elaborately present a range of optimal solutions at both empirical and theoretical levels, as illustrated in Fig. 1.

**Real Image Initialization**\((\) 6\()\()\). Intuitively, using real images instead of Gaussian noise for data initialization during the data synthesis phase is a practical and effective strategy. As shown in Fig. 3, this method significantly improves the realism of the condensed dataset and simplifies the optimization process, thus enhancing the synthesized dataset's ability to generalize in post-evaluation tests. Additionally, we incorporate considerations of information density and efficiency by employing a training-free condensed dataset (e.g., RDED) for initialization at the start of the synthesis process. According to Theorem 3.1, based on optimal transport theory, the cost of transporting from a Gaussian distribution to the original data distribution is higher than using the training-free condensed distribution as the initial reference. This advantage also allows us to reduce the number of iterations needed to achieve results to half of those required by our baseline G-VBSM model, significantly boosting synthesis efficiency.

**Theorem 3.1**.: _(proof in Appendix B.1) Considering samples \(^{}_{}\), \(^{}_{}\), and \(^{}_{}\) from the original data, training-free condensed (e.g., RDED), and Gaussian distributions, respectively, let us assume a cost function defined in optimal transport theory that satisfies \([c(a-b)] 1/I(L(a),(b))\). Under this assumption, it follows that \([c(^{}_{}-^{}_{})][c(^{}_{}- ^{}_{})]\)._

**Soft Category-Aware Matching**\((\) 6\()\). Previous dataset condensation methods (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023) based on the _Statistical Matching_ (SM) framework have shown satisfactory results predominantly when the data follows a unimodal distribution (_e.g._, a single Gaussian). This limitation is illustrated with a simple example in Fig. 2 (a). Typically, datasets consist of multiple classes with significant variations among their class distributions. Traditional SM-based methods compress data by collectively processing all samples, thus neglecting the differences between classes. As shown in the top part of Fig. 2 (a), this method enhances information density but also creates a big mismatch between the condensed source distribution \(^{}\) and the target distribution \(^{}\). To tackle this problem, we propose the use of a Gaussian Mixture Model (GMM) to effectively approximate any complex distribution. This solution is theoretically justifiable by the Tauberian Theorem under certain conditions (detailed proof is provided in Appendix B.2). In light of this, we define two specific approaches to _Statistical Matching_:

**Sketch Definition 3.1**.: _(formal definition in Appendix B.2) Given \(N\) random samples \(\{x_{i}\}_{i=1}^{N}\) with an unknown distribution \(p_{}(x)\), we define two forms to statistical matching. **Form (1):** involves synthesizing \(M\) distilled samples \(\{y_{i}\}_{i=1}^{M}\), where \(M N\), ensuring that the variances and means of both \(\{x_{i}\}_{i=1}^{N}\) and \(\{y_{i}\}_{i=1}^{M}\) are consistent. **Form (2):** treats \(p_{}(x)\) as a GMM with \(\) components. For random samples \(\{x_{i}^{j}\}_{i=1}^{N_{j}}(_{j}N_{j}=N)\) within each component \(c_{j}\), we synthesize \(M_{j}\) (\(_{j}M_{j}=M\)) distilled samples \(\{y_{i}^{j}\}_{i=1}^{M_{j}}\), where \(M_{j} N_{j}\), to maintain the consistency of variances and means between \(\{x_{i}^{j}\}_{i=1}^{N_{j}}\) and \(\{y_{i}^{j}\}_{i=1}^{M_{j}}\)._

In general, \(^{2}\), CDA, and G-VBSM are all categorized under **Form (1)**, as shown in Fig. 2 (a) at the top, which leads to coarse-grained matching. According to Fig. 2 (a) at the bottom, transitioning to **Form (2)** is identified as a practical and appropriate alternative. However, our empirical result indicates that exclusive reliance on **Form (2)** yields a synthesized dataset that lacks sufficient information density. Consequently, we propose a hybrid method that effectively integrates

Figure 3: Comparison between real image initialization and random initialization.

[MISSING_PAGE_EMPTY:6]

performance of EDC, the loss of statistical matching at the end of data synthesis still fluctuated significantly and did not reach zero. As a result, we choose to apply flatness regularization exclusively to the logits of the observer model, since the cross-entropy loss for these can more straightforwardly reach zero.

\[^{}_{}=D_{}(((^{})/)||((^{}_{})/)),\ ^{}_{}=^{}_{}+(1-)^{},\] (7)

where \(()\), \(\) and \(\) represent the softmax operator, the temperature coefficient and the pre-trained observer model, respectively. As illustrated in Fig. 2 (c) top, it is evident that \(^{}_{}\) significantly lowers the Frobenius norm of the Hessian matrix relative to standard training, thus confirming its efficacy in pushing a flatter loss landscape.

In post-evaluation, we observe that a method analogous to \(^{}_{}\) employing SAM does not lead to appreciable performance improvements. This result is likely due to the limited sample size of the condensed dataset, which hinders the model's ability to fully converge post-training, thereby undermining the advantages of flatness regularization. Conversely, the integration of an EMA-updated model as the validated model noticeably stabilizes performance variations during evaluations. We term this strategy EMA-based evaluation and apply it across all benchmark experiments.

**Smoothing Learning Rate (LR) Schedule \((\)\()\()\) and Smaller Batch Size \((\)\()\)\(()\).** Here, we introduce two effective strategies for post-evaluation training. Firstly, it is crucial to clarify and distinguish between standard or conventional deep model training and post-evaluation in the context of dataset condensation. Specifically, (1) in dataset condensation, the limited number of samples in \(^{}\) results in fewer training iterations per epoch, typically leading to underfitting; and (2) the gradient of a random batch from \(^{}\) aligns more closely with the global gradient than that from a random batch in \(^{}\). To support the latter observation, we utilize a ResNet-18 model with randomly initialized parameters to calculate the gradient of a random batch and assess the cosine similarity with the global gradient of \(^{}\). After conducting over 100 iterations of this procedure, the average cosine similarity is consistently higher between \(^{}\) and the global gradient than with \(^{}\), indicating a greater similarity and reduced sensitivity to batch size fluctuations. Our findings further illustrate that the gradient from a random batch in \(^{}\) effectively approximates the global gradient, as shown in Fig. 2 (c) bottom. Given this, the inaccurate gradient direction problem introduced by the small batch

    &  &  &  &  &  \\   & & ^{-1}\)} &  &  &  &  &  \\   & 1 & - & - & 29.0 \(\) & 0.42 \(\) & 0.61 & - & - & 30.6 \(\) & 0.4 & - & 26.1 \(\) & 0.2 \(\) & 0.4 \\  & 10 & 27.2 \(\) & 0.4 & 53.5 \(\) & 0.6 & 37.1 \(\) & 0.3 & **9.1 \(\) **0.3** & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & 50 & 47.5 \(\) & 0.5 & 59.2 \(\) & 0.4 & 62.1 \(\) & 0.1 & **87.0 \(\)**0.1 & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & 1 & 1 & 0.0 \(\) & 0.2 & 25.9 \(\) & 0.5 & 11.0 & 30.9 \(\) & 0.71 & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & 10 & 31.6 \(\) & 0.5 & 59.6 \(\) & 0.4 & 42.6 \(\) & 0.2 & 63.7 \(\) & 0.3 & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & 50 & 49.5 \(\) & 0.3 & 65.0 \(\) & 0.5 & 62.6 \(\) & 0.1 & **68.6 \(\)**0.2 & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   & 1 & - & - & - & - & 9.7 \(\) & 0.4 & 39.2 \(\) & 0.0 & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & 10 & - & - & 41.9 \(\) & 0.2 & 51.2 \(\) & 0.0 & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & 50 & 41.1 \(\) & 0.4 & 47.6 \(\) & 0.3 & 58.2 \(\) & 0.51 & 57.2 \(\) & 0.4 & 87.0 \(\) & 0.2 & 58.8 \(\) & 0.4 & 0.4 \(\) & 0.5 \(\) & 0.1 & \(\) & \(\) & \(\) \\   & 1 & - & - & - & 24.9 \(\) & 0.5 & 48.2 \(\) & 0.2 & - & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\  & 10 & - & - & - & 53.3 \(\) & 0.1 & 63.4 \(\) & 0.2 & - & \(\) &size becomes less problematic. Instead, using a small batch size effectively increases the number of iterations, thereby helping prevent model under-convergence.

To optimize the training with condensed samples, we implement a smoothed LR schedule that moderates the learning rate reduction throughout the training duration. This approach helps avoid early convergence to suboptimal minima, thereby enhancing the model's generalization capabilities. The mathematical formulation of this schedule is given by \((i)=(i/ N)}{2}\), where \(i\) represents the current epoch, \(N\) is the total number of epochs, \((i)\) is the learning rate for the \(i\)-th epoch, and \(\) is the deceleration factor. Notably, a \(\) value of 1 corresponds to a typical cosine learning rate schedule, whereas setting \(\) to 2 improves performance metrics from 34.4% to 38.7% and effectively moderates loss landscape sharpness during post-evaluation.

**Weak Augmentation () and Better Backbone Choice ().** The principal role of these two design decisions is to address the flawed settings in the _baseline_ G-VBSM. The key finding reveals that the minimum area threshold for cropping during data synthesis was overly restrictive, thereby diminishing the quality of the condensed dataset. To rectify this, we implement mild augmentations to increase this minimum cropping threshold, thereby improving the dataset condensation's ability to generalize. Additionally, we substitute the computationally demanding EfficientNet-B0 with more streamlined AlexNet for generating soft labels on ImageNet-1k, a change we refer to as an improved backbone selection. This modification maintains the performance without degradation. More details on the ablation studies for mild augmentation and improved backbone selection are in Appendix G.

## 4 Experiments

To validate the effectiveness of our proposed EDC, we conduct comparative experiments across various datasets, including ImageNet-1k (Russakovsky et al., 2015), ImageNet-10 (Kim et al., 2022), Tiny-ImageNet (Tavanaei, 2020), CIFAR-100 (Krizhevsky et al., 2009), and CIFAR-10 (Krizhevsky et al., 2009). Additionally, we explore cross-architecture generalization and ablation studies on ImageNet-1k. All experiments are conducted using 4\(\) RTX 4090 GPUs. Due to space constraints, detailed descriptions of the hyperparameter settings, additional ablation studies, and visualizations of synthesized images are provided in the Appendix A.1, G, and H, respectively.

**Network Architectures.** Following prior dataset condensation work (Yin et al., 2023; Yin and Shen, 2024; Shao et al., 2023; Sun et al., 2024), our comparison uses ResNet-{18, 50, 101} (He et al., 2016) as our verified models. We also extend our evaluation to include MobileNet-V2 (Sandler et al., 2018) in Table 1 and explore cross-architecture generalization further with recently advanced backbones such as DeiT-Tiny (Touvron et al., 2021) and Swin-Tiny (Liu et al., 2021) (detailed in Table 2).

**Baselines.** We compare our work with several recent state-of-the-art methods, including SRe\({}^{2}\)L (Yin et al., 2023), G-VBSM (Shao et al., 2023), and RDED (Sun et al., 2024) to assess broader practical

   Design Choices & Loss Type & Loss Weight & \(\) & \(\) & \(\) & ResNet-18 & ResNet-50 & DenseNet-121 \\  CONFIG C & - & - & 1.5 & - & - & 38.7 & 42.0 & 40.6 \\ CONFIG D & \(_{}\) & 0.025 & 1.5 & 0.999 & 4 & 38.8 & 43.2 & 40.3 \\ CONFIG D & \(_{}\) & 0.25 & 1.5 & 0.999 & 4 & 37.9 & 43.5 & 40.3 \\ CONFIG D & \(_{}\) & 2.5 & 1.5 & 0.999 & 4 & 31.7 & 37.0 & 32.9 \\ CONFIG D & \(_{}^{}\) & 0.25 & 1.5 & 0.99 & 4 & 39.0 & 43.3 & 40.2 \\ CONFIG D & \(_{}^{}\) & 0.25 & 1.5 & 0.99 & 4 & 39.5 & 44.1 & 41.9 \\ CONFIG D & \(_{}^{}\) & 0.25 & 1.5 & 0.99 & 1 & 38.9 & 43.5 & 40.7 \\ CONFIG D & vanilla SAM & 0.25 & 1.5 & - & 38.8 & 44.0 & 41.2 \\   

Table 4: **Ablation studies on ImageNet-1k with IPC 10.** Investigate the potential effects of several factors, including loss type, loss weight, \(\), and \(\), amid flatness regularization ().

   Design Choices & \(\) & ResNet-18 & ResNet-50 & ResNet-101 \\  CONFIG C & 1.0 & 34.4 & 36.8 & 42.0 \\ CONFIG C & 1.5 & 38.7 & 42.0 & 46.3 \\ CONFIG C & 2.0 & 38.8 & 45.8 & 47.9 \\ CONFIG C & 2.5 & 39.0 & 44.6 & 46.0 \\ CONFIG C & 3.0 & 38.8 & 45.6 & 46.2 \\    
   Design Choices & ResNet-18 & ResNet-50 & ResNet-101 \\  RDED & 25.8 & 32.7 & 34.8 \\ RDED() & 42.3 & 48.4 & 47.0 \\ G-VBSM() & 34.4 & 36.8 & 42.0 \\ G-VBSM() & 38.8 & 45.8 & 47.9 \\ G-VBSM() & 38.8 & 45.8 & 47.9 \\ G-VBSM() & 45.0 & **51.6** & **48.1** \\   

Table 3: **Ablation studies on ImageNet-1k with IPC 10. Left:** Explore the influence of the slowdown coefficient \(\) with CONFIG C. **Right:** Evaluate the effectiveness of real image initialization (), smoothing LR schedule () and smaller batch size () with \(=2\).

impacts. It is important to note that we have omitted several traditional methods (Cazenavette et al., 2022; Liu et al., 2023; Cui et al., 2023) from our analysis. This exclusion is due to their inadequate performance on the large-scale ImageNet-1k and their lesser effectiveness when applied to practical networks such as ResNet, MobileNet-V2, and Swin-Tiny (Liu et al., 2021). For instance, the MTT method (Cazenavette et al., 2022) encounters an out-of-memory issue on ImageNet-1k, and ResNet-18 achieves only a 46.4% accuracy on CIFAR-10 with IPC 10, which is significantly lower than the 79.1% accuracy reported for our EDC in Table 1.

### Main Results

**Experimental Comparison.** Our integral EDC, represented as \(\) G in Fig. 1, provides a versatile solution that outperforms other approaches across various dataset sizes. The results in Table 1 affirm its ability to consistently deliver substantial performance gains across different IPCs, datasets, and model architectures. Particularly notable is the performance leap in the highly compressed IPC 1 scenario using ResNet-18, where EDC markedly outperforms the latest state-of-the-art method, RDED. Performance rises from 22.9%, 11.0%, 7.0%, 24.9%, and 6.6% to 32.6%, 39.7%, 39.2%, 45.2%, and 12.8% for CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet-10, and ImageNet-1k, respectively. These improvements clearly highlight EDC's superior information encapsulation and enhanced generalization capability, attributed to the efficiently synthesized condensed dataset.

**Cross-Architecture Generalization.** To verify the generalization ability of our condensed datasets, it is essential to assess their performance across various architectures such as ResNet-{18, 50, 101} (He et al., 2016), MobileNet-V2 (Sandler et al., 2018), EfficientNet-B0 (Tan and Le, 2019), DeiT-Tiny (Touvron et al., 2021), Swin-Tiny (Liu et al., 2021), ConvNext-Tiny (Liu et al., 2022) and ShuffleNet-V2 (Zhang et al., 2018). The results of these evaluations are presented in Table 2. During cross-validation that includes all IPCs and the mentioned architectures, our EDC consistently achieves higher accuracy than RDED, demonstrating its strong generalization capabilities. Specifically, EDC surpasses RDED by significant margins of 8.2% and 14.42% on DeiT-Tiny and ShuffleNet-V2, respectively.

**Application.** Our condensed dataset not only serves as a versatile training resource but also enhances the adaptability of models across various downstream tasks. We demonstrate its effectiveness by employing it in scenarios such as data-free network slimming (Liu et al., 2017) (_w.r.t._, parameter pruning (Srinivas and Babu, 2015)) and class-incremental continual learning (Prabhu et al., 2020) outlined in DM (Zhao and Bilen, 2023). Fig. 4 shows the wide applicability of our condensed dataset in both data-free network slimming and class-incremental continual learning. It substantially outperforms SRe\({}^{2}\)L and G-VBSM, achieving significantly better results.

Figure 4: **Application on ImageNet-1k.** We evaluate the effectiveness of data-free network slimming and continual learning using VGG11-BN and ResNet-18, respectively.

    &  &  & Weak Augmentation & EMA-based Evaluation &  &  &  \\  & & & Scale(0.5,1.0) & & EMA Rate=0.99 & \\  CONFIG F & 0.00 & 2.0 & ✗ & ✗ & 46.2 & 53.2 & 49.5 \\ CONFIG F & 0.00 & 2.0 & ✓ & ✗ & 46.7 & 53.7 & 49.4 \\ CONFIG F & 0.00 & 2.0 & ✓ & ✓ & 46.9 & 53.8 & 48.5 \\ CONFIG F & 0.25 & 2.0 & ✗ & ✗ & 46.7 & 53.4 & 50.6 \\ CONFIG F & 0.25 & 2.0 & ✓ & ✗ & 46.8 & 53.6 & 50.8 \\ CONFIG F & 0.25 & 2.0 & ✓ & ✓ & 47.1 & 53.7 & 48.2 \\ CONFIG F & 0.50 & 2.0 & ✗ & ✗ & 48.1 & 53.9 & 50.4 \\ CONFIG F & 0.50 & 2.0 & ✓ & ✗ & 48.4 & 53.9 & 52.7 \\ CONFIG F & 0.50 & 2.0 & ✓ & ✓ & 48.6 & 54.1 & 51.7 \\ CONFIG F & 0.75 & 2.0 & ✗ & ✗ & 46.1 & 52.7 & 51.0 \\ CONFIG F & 0.75 & 2.0 & ✓ & ✗ & 46.9 & 52.8 & 51.6 \\ CONFIG F & 0.75 & 2.0 & ✓ & ✓ & 47.0 & 53.2 & 49.3 \\   

Table 5: **Ablation studies on ImageNet-1k with IPC 10.** Evaluate the effectiveness of several design choices, including soft category-aware matching (), weak augmentation () and EMA-based evaluation ().

### Ablation Studies

**Real Image Initialization \((\)\()\), Smoothing LR Schedule \((\)\()\)  and Smaller Batch Size \((\)\()\).** As shown in Table 3 (left), these design choices, with zero additional computational cost, sufficiently enhance the performance of both G-VBSM and RDED. Furthermore, we investigate the influence of \(\) within smoothing LR schedule in Table 3 (right), concluding that a smoothing learning rate decay is worthwhile for the condensed dataset's generalization ability and the optimal \(\) is model-dependent.

**Flatness Regularization \((\)\).** The results in Table 4 demonstrate the effectiveness of flatness regularization, while requiring a well-designed setup. Specifically, attempting to minimize sharpness across all statistics (_i.e._, \(_{}\)) proves ineffective, instead, it is more effective to apply this regularization exclusively to the logit (_i.e._, \(_{}^{}\)). Setting the loss weights \(\) and \(\) at 0.25, 0.99, and 4, respectively, yields the best accuracy of 39.5%, 44.1%, and 45.9% for ResNet-18, ResNet-50, and DenseNet-121. Moreover, our design of \(_{}^{}\) surpasses the performance of the vanilla SAM, while requiring only half the computational resources.

**Soft Category-Aware Matching \((\)\), Weak Augmentation \((\)\)  and EMA-based Evaluation \((\)\).** Table 5 illustrates the effectiveness of weak augmentation and EMA-based evaluation, with EMA evaluation also playing a crucial role in minimizing performance fluctuations during assessment. The evaluation of soft category-aware matching primarily involves exploring the effect of parameter \(\) across the range \(\). The results in Table 5 suggest that setting \(\) to 0.5 yields the best results based on our empirical analysis. This finding not only confirms the utility of soft category-aware matching but also emphasizes the importance of ensuring that the condensed dataset maintains a high level of information density and bears a distributional resemblance to the original dataset.

## 5 Conclusion

In this paper, we have conducted an extensive exploration and analysis of the design possibilities for scalable dataset condensation techniques. This comprehensive investigation helped us pinpoint a variety of effective and flexible design options, ultimately leading to the construction of a novel framework, which we call EDC. We have extensively examined EDC across five different datasets, which vary in size and number of classes, effectively proving EDC's robustness and scalability. Our results suggest that previous dataset distillation methods have not yet reached their full potential, largely due to suboptimal design decisions. We aim for our findings to motivate further research into developing algorithms capable of efficiently managing datasets of diverse sizes, thus advancing the field of dataset condensation task.