# Geometry Awakening: Cross-Geometry Learning Exhibits Superiority over Individual Structures

Yadong Sun\({}^{1}\) Xiaofeng Cao\({}^{1,2}\) Yu Wang\({}^{1}\) Wei Ye\({}^{2}\) Jingcai Guo\({}^{3}\) Qing Guo\({}^{4}\)

\({}^{1}\)School of Artificial Intelligence, Jilin University, China

\({}^{2}\)College of Electronic and Information Engineering, Tongji University, China

\({}^{3}\)The Hong Kong Polytechnic University

\({}^{4}\)CFAR and IHPC, Agency for Science, Technology and Research (A*STAR), Singapore

sunyd22@mails.jlu.edu.cn, xiaofengcao@jlu.edu.cn, yu_w18@mails.jlu.edu.cn,

yew@tongji.edu.cn, jc-jingcai.guo@polyu.edu.hk, tsinggguo@ieee.org

###### Abstract

Recent research has underscored the efficacy of Graph Neural Networks (GNNs) in modeling diverse geometric structures within graph data. However, real-world graphs typically exhibit geometrically heterogeneous characteristics, rendering the confinement to a single geometric paradigm insufficient for capturing their intricate structural complexities. To address this limitation, we examine the performance of GNNs across various geometries through the lens of knowledge distillation (KD) and introduce a novel cross-geometric framework. This framework encodes graphs by integrating both Euclidean and hyperbolic geometries in a space-mixing fashion. Our approach employs multiple teacher models, each generating hint embeddings that encapsulate distinct geometric properties. We then implement a structure-wise knowledge transfer module that optimally leverages these embeddings within their respective geometric contexts, thereby enhancing the training efficacy of the student model. Additionally, our framework incorporates a geometric optimization network designed to bridge the distributional disparities among these embeddings. Experimental results demonstrate that our model-agnostic framework more effectively captures topological graph knowledge, resulting in superior performance of the student models when compared to traditional KD methodologies.

## 1 Introduction

Graph Neural Networks (GNNs) have emerged as indispensable tools for analyzing relational data in diverse domains, such as natural language processing , computer vision , recommendation systems . Their conventional approach of operating within Euclidean space encounters limitations when confronted with datasets embodying non-Euclidean characteristics, such as power-law distribution and hierarchical structures, prevalent in real-world applications . Recognizing this challenge, our community ventures into the realm of non-Euclidean Graph Neural Networks, seeking to harness alternative geometries, notably hyperbolic space, for more adeptly capturing the intricate topological features inherent in many real-world networks . By synthesizing recent advancements and empirical findings, we endeavor to elucidate the potential of non-Euclidean GNNs in effectively modeling complex relational data structures, thereby paving the way for advancements in various application domains .

Unlike the constant and flat Euclidean geometry, hyperbolic geometry offers greater flexibility by integrating curvature information, enabling better alignment with the characteristics of non-Euclidean input graphs. This endeavor has rendered hyperbolic GNNs more accessible and comparable to their Euclidean counterparts, resulting in promising performance and interpretability in graph representation learning. Hyperbolic GNNs  extended the neighborhood aggregation operation by computing centroids in the hyperbolic geometry. This approach effectively fuses the node features and hierarchical structure, thereby learning superior node representations. Furthermore, a full manifold-preserving feature transformation operation has been developed in hyperbolic geometry , eliminating the complicated transformations between hyperbolic and tangent spaces. With these essential operation, hyperbolic GNNs can achieve comparable or even superior performance than Euclidean GNNs.

**Question**.: _Although there has been a surge of research on Euclidean and non-Euclidean GNNs in the community, it remains unclear which geometry offers greater advantages. Real-world graphs often exhibit geometrically structural heterogeneity, characterized by variations in clustering and density among nodes , as shown in Figure 1. The structural heterogeneity pose a challenge when attempting to accurately model the graph structure using GNNs solely equipped with either Euclidean or non-Euclidean geometry._

**Motivation.** According _Local Subgraph Preservation Property_, the properties of a node largely depend on the properties of the local subgraph centered around it. Considering the hyperbolic property of local subgraphs, i.e., hyperbolicity 1. Employing hyperbolic geometry modeling achieves higher precision and minimal information loss when hyperbolicity is low. Conversely, when hyperbolicity is elevated, opting for Euclidean geometry modeling results in lower complexity and slightly superior performance compared to hyperbolic geometry. Consequently, the primary limitation of existing graph neural networks is their inability to adaptively select the appropriate geometry for representing nodes with different local structures .

**Our scheme.** In this paper, we propose a cross-geometric graph knowledge distillation framework that encodes graphs utilizing both Euclidean and hyperbolic geometry in a locally space-mixing fashion. In contrast to traditional methods that compute hyperbolicity for the overall graph and roughly analyze its applicability to different geometries, our approach performs fine-grained analysis on the local subgraphs surrounding each node. This enables the selection of embeddings in the most appropriate geometry for local subgraphs, which is subsequently utilized to transfer knowledge to the student model. Additionally, we introduce a geometric embedding optimization module to optimize the distribution of embeddings produced by the teacher models. To evaluate the performance of our proposed approach, we conduct distillation experiments on node classification (NC) and link prediction (LP) tasks across various types of graph data. The experimental results demonstrate the superiority of our approach in teaching student models compared to other baseline methods. Our approach highlights enhanced effectiveness and generalization, ultimately achieving state-of-the-art performance in graph data distillation tasks. The salient aspects of our contributions are as follows:

* Structured analysis reveals that both Euclidean and hyperbolic geometries demonstrate commendable performance in graph processing, despite their inherent disparities and potential geometric conflicts. This prompts scholarly inquiry into reconciling these divergent geometric spaces within GNNs, inspiring new research avenues in geometry awareness.
* With heightened awareness, there's potential to represent graph structures across dimensions, transcending singular space limitations. We thus adapt and integrate teacher embeddings from diverse geometries, transferring them to more effective cross-geometric space.

Figure 1: Visualization of the embedding of the same graph in hyperbolic space (left) and Euclidean space (right), with different colors representing different class labels. Tree-like subgraphs maintain significant inter-class margins in hyperbolic space, leading to improved classification boundaries, while intra-class nodes with Euclidean properties may be overstretched due to the utilization of hyperbolic metrics, hence embedding in Euclidean space is preferred.

* Extensive experiments employing KD techniques on diverse graph datasets demonstrate that cross-geometric methods significantly outperform traditional approaches in the context of knowledge transfer. This is particularly evident in NC and LP tasks, thereby affirming the superior efficacy of these methods.

## 2 Related Work

**Graph Neural Networks.** GNNs are neural network models that capture interdependencies between nodes by propagating messages among them within a graph. The most representative model is Graph Nonvolutional Network (GCN) [23; 24; 25], which can be regarded as a generalization of convolutional neural networks to graph data. The GraphSAGE  employs a neighbor sampling strategy to address graph data, enabling information aggregation based on the local neighborhood structure of nodes. The attention-based GNN [27; 28; 29] model employs masked self-attention, assigning diverse weights to node representations based on the varying features of neighboring nodes. Notably, constructing GNNs in the hyperbolic space [13; 15; 30] significantly reduces embedding distortions caused by the inability of Euclidean space to handle power-law distributions, particularly in the case of tree-like or highly hierarchical data.

**Knowledge Distillation.** KD initially proposed by , is a model compression technique that involves leveraging pre-trained teacher models to guide the training of a lightweight student model [32; 33]. After that,  aligns student and teacher model intermediate features using a regressor and a loss function to minimize feature differences.  employs attention mechanisms to extract features from teacher model's intermediate layers and transfer them to the student model. As GNNs have demonstrated breakthrough performance in various deep learning tasks, a number of graph-based KD frameworks have been proposed successively.  introduces a local structure preserving module to extract knowledge from intermediate layers of the GNN model, guiding the student model to optimize learned topological structures.  proposes a novel approach to effectively learn multi-scale topological semantics from multiple GNN teacher models to guide student model.  incorporates a VQ-VAE to learn a codebook that represents informative local structures, and uses these local structures as additional information for distillation. However, all these methods rely on the Euclidean geometry. Our proposed approach leverages both Euclidean and non-Euclidean geometries to learn representations of highly hierarchical local structures, ensuring that the knowledge transmitted to the student model is highly reliable.

## 3 Problem Definition and Preliminaries

### Problem Definition

For the graph KD, given a graph \(=(,)\), where \(\) denotes the node set and \(\) denotes the edge set. Let \(N\) denotes the total number of nodes in the graph \(\), \(\) denotes nodes' feature matrix with each row corresponding to the feature vector of a node, and \(\) denotes graph's \(N N\) adjacency matrix, where \(A_{ij}\) signifies whether there is an edge between nodes \(i\) and \(j\). If \(A_{ij}=1\), an edge exists; otherwise, no edge is present. Let \(_{T}=\{m_{T_{1}},m_{T_{2}},...,m_{T_{R}}\}\) denotes the teacher models pre-trained on \(\), \(R\) represents the number of geometries. Our fundamental objective is to extract information from \(\) by \(_{T}\), and employing it to boost the training process of the student model, denoted as \(m_{S}\), which in Euclidean space and has significantly smaller size. Let \(_{T}=\{_{T_{1}},_{T_{2}},...,_{T_{R}},\}\) be the outputs of the teacher models and \(_{S}\) be the outputs of the student model. The optimization goal is to minimize the disparity between predictions of \(_{}\) and \(m_{S}\) on \(\), i.e.,

\[_{m_{s}}_{i=1}^{N}_{j=1}^{R}_{j }_{dis}(_{T_{j},i}||_{S,i}), \]

where \(_{j}\) denotes the weight of the \(j\)-th teacher model, \(_{dis}\) denotes disparity measurement function.

### Preliminaries

In this paper, We focus on distillation performance of the teacher model individually in Euclidean, hyperbolic, and spherical geometries, as well as across geometries. We give a necessary introduction of hyperbolic geometry in this subsection, with other information available in Appendix A.

Hyperbolic geometry studies the properties of curved space with negative curvature. Hyperbolic space can be modeled using five isometric models [39; 40], and in this paper, we adopt Poincare disk model. The Poincare disk model evinces a distinctive property wherein distances from the geometric center to the periphery undergo a non-linear augmentation as a function of layer depth. This phenomenon engenders a nonlinear and multi-branch composite structure within the model's geometric framework.

**Definition 3.1** (Poincare Disk Model).: _A n-dimensional Poincare disk model \((_{c}^{n},g^{})\) is a complete Riemannian manifold with a negative constant curvature \(c\), which defined as_

\[_{c}^{n}:=\{^{n}:-c\|\|^{2}<1\},  g^{}=(_{}^{c})^{2}g^{},_{ }^{c}=\|^{2}} \]

_where \(\|\|\) denotes the Euclidean norm, \(g\) denotes the Riemannian metric, and The superscripts \({}^{}\) and \({}^{}\) indicate that the vector or matrix resides in hyperbolic space and Euclidean space, respectively._

**Definition 3.2** (Hyperbolic Operations).: _Given two points \(,_{c}^{n}\), the hyperbolic distance between them is defined as_

\[d_{c}(,)=}^{-1}(\|- _{c}\|), \]

_where \(_{c}\) denotes Mobius addition, i.e.,_

\[_{c}:=,+c\| \|^{2})+(1-c\|\|^{2})}{1+2c, +c^{2}\|\|^{2}\|\|^{2}}. \]

**Definition 3.3** (Tangent Space).: _The tangent space at a point \(\) in hyperbolic space, denoted as \(_{}_{c}^{n}\), serves as the first-order approximation of the original space, a n-dimensional tangent space is isomorphic to Euclidean space \(^{n}\). Representations between hyperbolic and tangent space can be transformed via the exponential and logarithmic map as follows:_

\[_{}_{c}^{n}_{c}^{ n}:_{}^{c}()=_{c}((}^{c}\|\|}{2})}{\|\|} ), \] \[_{c}^{n}_{}_{c}^{ n}:_{}^{c}()=d_{c}(,)_{c}}{ _{}^{c}\|-_{c}\|},\]

_where \(_{}_{c}^{n}\), \(_{c}^{n}\) and \(_{}^{c}\) has same meaning in Eq. (2). Here we utilize the origin point \(\) in hyperbolic space as a reference point \(\) to equalize errors across various directions._

## 4 Cross-Geometry Learning with KD

This section reveals three key aspects: why cross-geometry learning demonstrates superior performance, why it is feasible, and how this superiority is achieved by employing KD. Thus, we analyze our method from three perspectives: reasonableness, superiority, and trustworthiness.

### Geometric Features of Local Subgraphs

**Reasonableness:** According _local subgraph perservation peoperty theorem_, nodes near the central node strongly affect its features, while distant nodes typically have negligible impact. The graph data in real-world often exhibits significant complexity, where diverse local subgraphs often entail distinct geometric properties [17; 18], employing cross-geometry system can offer more effective embedding selections for local graphs, thereby achieving performance beyond that of single geometry methods.

**Definition 4.1** (Subgraphs of Centroid \(p\)): _For a given node \(p\) belonging to graph \(\), its corresponding \(k\)-hops subgraph \(_{p}\) comprises all nodes \(q\{p\}\) within a distance no greater than \(k\) from \(p\), along with their respective edges._

We employ the \(k\)-hops neighbors method to generate subgraphs. Hence, we determine the optimal value of \(k\) through the statistical analysis of graph data in section 5.4. For each subgraph \(_{i}\), we calculate their Gromov's \(\)-hyperbolicity (See the appendix A.1 for the calculating process) based on \(\), denoted as \(_{_{i}}\), which serves as a geometric characterization metric for the central node \(i\).

### Geometric Teacher Models

**Superiority:** We leverage KD technology, utilizing its ability to transfer knowledge between different model architectures, as a medium for interoperability between different geometries. Our proposed KD framework is model-agnostic, making it applicable to various geometric models.

Herein, the framework will be explained using the GCN model . To minimize disparities between the intermediate layers of the teacher and student models, our method uses pre-activation node embeddings \(\) to guide the student model and constructs an embedding matrix \(\) for all nodes' \(\). During the forward propagation process of a GCN layer in Euclidean space, the intermediate embeddings of nodes in the \(l\)-th layer of GCN is given by

\[_{T}^{(l),}=}_{T}^{(l-1),}^{(l)}, \]

where \(_{T}^{(l-1),}=activation(_{T}^{(l-1),})\) denotes the node representation matrix from the output of the \((l-1)\)-th GCN layer. \(}\) denotes the symmetrically normalized adjacency matrix, \(^{(l)}\) denotes the weight matrix.

For the \(i\)-th node in layer \(l\), its embedding in hyperbolic space is denoted as \(_{T,i}^{(l),}\) and its representation is denoted as \(_{T,i}^{(l),}\). During the forward propagation process of a GCN layer in hyperbolic space, the transformed feature is given by

\[_{T,i}^{(l),}=_{}^{c}[^{(l)}_{ {o}}^{c}(_{T,i}^{(l-1),})]_{c}^{}, \]

By performing neighborhood aggregation on these features, we obtain the hyperbolic intermediate embeddings of \(i\)-th node in the \(l\)-th layer as follows:

\[_{T,i}^{(l),}=_{_{T,i}^{(l),}}^{c}[ _{j:(i,j)}w_{ij}_{_{T,i}^{(l),} }^{c}(_{T,j}^{(l),})], \]

where \(w_{ij}\) is the weight coefficient computed by \(_{T,i}^{(l),}\) and \(_{T,j}^{(l),}\).

### Structure-Wise Knowledge Transfer

**Trustworthiness:** To achieve a more fine-grained selection of embeddings in appropriate geometry, we designed a Structure-Wise Knowledge Transfer (SWKT) module. This module determines suitable

Figure 2: Illustration of our proposed cross-geometry graph KD framework. **Structure-Wise Knowledge Transfer (SWKT)**: Choosing embeddings in appropriate geometric spaces using \(_{_{i}}\) of nodes, conveying local subgraph topological knowledge to the student model: \(_{T}^{(i),}\) denotes Euclidean teaching, and \(_{T}^{(i),}\) denotes the hyperbolic teaching. **GEO**: Enhancing hint embeddings from the teacher models, reducing the negative effects of inconsistencies between different geometries.

geometric representations based on the geometric feature \(_{_{i}}\) of subgraphs and transfers them to the student, facilitating more effective information extraction and guidance.

Specifically, we obtain representations of local subgraphs centered around each node based on the \(l\)-th intermediate layer hint embeddings of the teacher model in different geometries. We denote \(i\)-th local structure representation in hyperbolic geometry as \(_{T,i}^{(l),}=\{u_{T,i1}^{(l),},u_{T,i2}^{(l),},...,u_{T,ij}^{(l),},...,u_{T,iN}^{(l),}\}\). Element \(u_{T,ij}^{(l),}\) in hyperbolic geometry can be computed as follows:

\[ u_{T,ij}^{(l),}=&_{sub}(^{c}_{}(_{T,i}^{(l)}),^{c}_{}( _{T,j}^{(l)}))\\ =&(\|^{c}_{}(_{T,i }^{(l),}),^{c}_{}(_{T,j}^{(l),})\| ^{2})/_{j:(j,i)}((\|^{c}_{ }(_{T,i}^{(l),}),^{c}_{}(_{T,j }^{(l),})\|^{2})). \]

Similarly, we can obtain representation set of \(i\)-th local structure in Euclidean geometry denoted as \(_{T,i}^{(l),}\). SWKT generates induced \(k\)-hops subgraphs centered at each node, computes their \(_{_{i}}\) as the hierarchical level of the central node \(i\), and obtains teacher models' middle layer representations of the \(i\)-th node in the \(l\)-th layer based on \(_{_{i}}\) as follows:

\[_{T,i}^{(l)}=_{T,i}^{(l),}( _{_{i}}<)+_{T,i}^{(l),} (_{_{i}}) \]

where \(\) denotes indicator function, the threshold \(\) is a hyperparameter that is typically set to a smaller value on graphs with higher \(\)-hyperbolicity values to achieve better performance.

In our method, embeddings of \(l\)-th guided middle layers of student model is \(_{S}^{(l),E}\), and applying Eq. (9) likewise yields the student model \(i\)-th local structure representation \(_{S,i}^{(l)}=\{u_{S,i1}^{(l)},u_{S,i2}^{(l)},...,u_{S,iN}^{(l)}\}\). For node \(i\), the similarity between the local structures of the teacher model and the student model is measured as:

\[_{i}^{(l)}=D_{KL}(_{S,i}^{(l)}||_{T,i}^{(l)} )=_{j:(j,i)}u_{S,ij}^{(l)}(^{(l)}}{u_{T,ij}^{(l)}}), \]

where \(_{}\) represents the Kullback-Leibler divergence.

SWKT minimizes the local structure similarity \(\) to transfer knowledge from hint embeddings in different geometry to student model. Thus, the loss function for the SWKT module is

\[_{SWKT}=_{l=1}^{L} _{i=1}^{N}_{i}^{(l)}, \]

where \(L\) denotes the total number of intermediate layers used for distillation.

### Geometric Embedding Optimization

**Trustworthiness:** Simply concatenating embeddings from Euclidean and hyperbolic teacher models to teach student model can lead to confusion due to geometric inconsistencies. This confusion may result in the student model performing worse than when learning from a single geometric teacher model, as shown in section 5.2. To mitigate the negative effects caused by this inconsistency, we propose a Geometric Embedding Optimization module (GEO) to optimize cross-geometric space.

Specifically, for a given node \(i\) from layer \(l\), we have its local geometric information \(_{_{i}}\) and teacher embeddings from different geometric spaces. We obtain an initial fused feature as follows:

\[_{T,i}^{(l)}=_{i}}- ))}_{T,i}^{(l),}+ _{i}}-))}{1+(-(_{_{i}}-))}_{T,i}^ {(l),}, \]

where \(\) has the same meaning as Eq. (10).

Next, we use a single-layer GCN (which can be replaced by other sufficiently capable networks, such as a Multi-Layer Perceptron (MLP)) to optimize the initially fused features \(_{T,i}^{(l)}\). The optimization network should select loss functions based on the specific downstream task. In this study, we adopt the triplet loss function , which enlarges the distance between different-class nodes and reduces the distance between same-class nodes, to enhance node classification and link prediction performance. To apply triplet loss to graph data, we organize triplets as follows: Given the hint embeddings from teacher, we sample extensive sets of nodes, where each set includes an anchor node, a positive node with the same label as the anchor, and a negative node with a different label.

Assuming \(_{e}\) is corresponding function of a pre-trained GEO network with weight matrix \(_{e}\), the elements of the local structure vector \(_{T,i}^{(l)}\) for the \(i\)-th node of layer \(l\) can be represented as

\[u_{T,ij}^{(l)}=_{sub}(_{e}(_{T,i}^{(l)}; _{e}),_{e}(_{T,j}^{(l)};_{e})), \]

where \(_{sub}\) has the same meaning as Eq. (9).

Subsequently, we can reference Eq. (11) to compute the structural similarity between the outputs of GEO and the guided embeddings of \(l\)-th layer of the student model as:

\[_{GEO}=_{i=1}^{N}D_{KL}(_{T,i}^{(l)} \|_{S,i}^{(l)}). \]

### Distillation Framework

In our proposed graph KD approach, the teacher model's early \(L-1\) layers guide the student model using the SWKT module, while the \(L\)-th layer guides via the GEO module. Additionally, the student model also learns the logits distribution of teacher models, i.e., outputs of GEO in last layer. Given the logits \(_{T}\) from teacher models and the predicted logits \(_{S}\) from student model, our overall KD loss is as follows:

\[=_{SWKT}+_{CE}(_{T},_{S})+ _{GEO}, \]

where \(_{CE}\) denotes the cross-entropy loss function, \(\) is a weight coefficient.

The space complexity is \(O(ND+|E|+RNH+kN|E|)\), where \(N\) is the number of nodes, \(D\) is the feature dimension, \(|E|\) is the number of edges, \(R\) is the number of teacher models, and \(k\) is the \(k\)-pop parameter. For time complexity, forward propagation has a complexity of \(O(NH^{2}+|E|H)\), local subgraph generation is \(O(kN|E|)\), the Structured-Wise Knowledge Transfer module is \(O(kNH)\), similarity computation is \(O(kN)\), and the Geometric Embedding Optimization module contributes \(O(NH^{2})\). Thus, the overall time complexity is \(O(NH^{2}+|E|H+kN(|E|+H))\).

## 5 Experiments

In this section, we first give the experimental setup and baselines. Then we compare our graph KD framework with some baselines on NC and LP tasks. Hyperparameters and ablation analysis also be given. Further experimental results can be found in Appendix C.

### Experimental Settings

**Setups**. We preform NC and LP tasks on citation network datasets Cora,Citeseer and Pubmed, wikipedia-based article hyperlink network dataset Wiki-CS, and Physics part of the Coauthor dataset Co-Physics. The student and teacher models are both GCN composed of two hidden layers and one output layer. The hidden layer node dimensions are 8 for the student and 128 for teachers. The model parameters are uniformly initialized using the Xavier's uniform initialization  method The optimizer uses Adam or Riemannian Adam. We set the value of \(k\) for \(k\)-hops subgraphs to 4. To mitigate errors caused by randomness, each F1-score and ROC AUC is the average of 10 experiments with different random seed values.

**Baselines**. To evaluate the performance of our method, we compare it with KD methods formulated in single geometry, including the following methods. **FitNet** utilizes a regressor to align the intermediate features of the student model with those of the teacher model, quantifying the feature discrepancy using L2 distance. **AT** averages attention maps from both teacher and student models' intermediate hidden layers, quantifying differences between them using a designed loss function. **LSP** extracts local structures from both teacher and student models' intermediate feature maps and measures their difference using KL divergence. **MSKD** Utilizes diverse teacher models with varying layers to guide the student model in capturing topology at different scales. **VQG** incorporates a VQ-VAE to learn a codebook that represents informative local structures, and uses

[MISSING_PAGE_FAIL:8]

0.93% on average, underscoring the efficacy of geometry-specific methods in cross-geometry learning for overall performance enhancement.

### Hyperparameters Analysis

\(k\) **of Subgraphs**. A small radius \(k\) limits local hierarchical assessment, while a large \(k\) increases computational complexity. We explored \(_{i}\) value distributions for local geometric properties across \(k\) values (1 to 7) on Wiki-CS and Cora datasets, shown in Figure 3 (left). Stability in distributions occurs at \(k 4\), suggesting sufficient capture of geometric characteristics in subgraphs of this size. Thus, we set \(k=4\) for our experiments.

We varied the hyperparameters on the Wiki-CS and Cora datasets to test the F1 scores for the NC task, with \(\{0.0,0.5,1.0,1.5,2.0,2.5\}\), \(\{1,2,3,4,5,10\}\). Here, \(\) denotes the threshold value in Eq. (10) and Eq. (13). A larger \(\) leads to more local subgraphs being embedded in hyperbolic space, while a smaller \(\) results in more subgraphs being embedded in Euclidean space. \(\) denotes the weight of the GEO module. The results from Figure 3 (middle) indicate that the hyperparameters \(\) and \(\) have a generally minimal impact on the outcomes. The combination of \(=1.5\) and \(=3.0\) maintains optimal performance.

### Distillation Efficiency

To evaluate the convergence efficiency of our proposed KD method, we have recorded the F1-scores trends for student models guided by all KD methods during training epochs on the Cora and Wiki-CS datasets in Figure 3 (right). As illustrated, our KD method consistently outperform other methods within the same training epochs. Specifically, our KD method achieves state-of-the-art (SOTA) performance before reaching 500 epochs, while the others are still undergoing training. These results serve to validate the effectiveness and efficiency of our method. Due to their simpler architecture, student models generally have faster inference speeds compared to teacher models. The speed-up achieved by student models relative to teacher models is also an indicator of the efficiency of distillation methods. The inference times (in milliseconds) of both teacher and student models, measured on our device, are shown in Table 3. Results demonstrate that our method achieves an average speed-up of approximately 232x.

In addition, we evaluated the training time for each method, the inference time for the corresponding student models, and calculated the compression ratio of student model size relative to the teacher model. The results can be found in Appendix C.

### Ablation Study

To further validate the efficacy of cross-geometric learning and the two proposed modules, we conducted additional experiments by adapting our method to operate on a single Euclidean or

Figure 3: \(_{_{i}}\) distribution of subgraphs (left), hyperparameters sensitivity analysis (middle), comparison of convergence rates (right) on the Cora (row 1) and Wiki-CS (row 2)

hyperbolic geometry. Additionally, we selectively excluded the SWKT and GEO modules. These ablation experiments were conducted on the Wiki-CS dataset, and the results are presented in Table 4. We have the following observations:

* Using only a single geometry, even with the GEO module optimizing embeddings, the enhancement compared to the baseline is minimal.
* The cross-geometric approach consistently outperforms the single-geometric methods. Whether excluding SWKT or GEO, the results are inferior to the comprehensive method, indicating their crucial roles in optimizing geometric embedding distribution.

The overall results of ablation analysis further demonstrate the importance of cross-geometry learning and our proposed two modules.

To demonstrate the model-agnostic nature of our framework, we alter the architecture and the number of layers \(L\) in the teacher model. Due to page limitations, we only present key results above. For more details on the dataset, experimental setup, and results, please refer to Appendix B and C.

## 6 Conclusion

Hyperbolic geometry has shown expressive non-Euclidean modeling in the graph community. Noteworthy models, such as Poincare and Lorentz models, facilitate vector projections between Euclidean and hyperbolic neurons. Our investigation reveals that tree-like or power-law distributed graphs exhibit multiple different hierarchical within locally connected structures. Consequently, training across Euclidean and hyperbolic geometry intuitively emerges as a more flexible approach to graph modeling, yielding significant enhancements in KD tasks. To this end, we introduce a novel KD framework that models the hint embeddings of the teacher models across diverse geometries. By leveraging \(\)-hyperbolicity, we transfer local subgraphs information to the student model. Our analysis and experiments provide positive support for this innovative perspective on geometry modeling.

**Limitations**. Despite the performance improvements achieved by cross-geometry learning across various tasks, it presents some potential issues. For instance, integrating different geometric information introduces hyperparameters, making the task outcomes somewhat dependent on their selection, thus affecting the method's stability. Additionally, the distillation phase demands more complex pre-trained models, increasing resource and time requirements. These limitations are critical areas for future enhancement in cross-geometry learning.

## 7 Acknowledge

This work was supported by National Natural Science Foundation of China, Grant Number: 62476109, 62206108, 62176184, and the Natural Science Foundation of Jilin Province, Grant Number: 20240101373JC, and Jilin Province Budgetary Capital Construction Fund Plan, Grant Number: 2024C008-5, and Research Project of Jilin Provincial Education Department, Grant Number: JJKH20241285KJ.

  
**Datasets** & **Teacher** & **Student** & **Speed-up** \\  Wiki-CS & 906 ms & 3.98 ms & 227x \\ Co-Physics & 3410 ms & 12.0 ms & 284x \\ Pubmed & 914 ms & 4.46 ms & 204x \\ Citeseer & 908 ms & 4.01 ms & 226x \\ Cora & 975 ms & 4.43 ms & 220x \\ 
**Average** & 1422 ms & 4.22 ms & 232x \\   

Table 3: Speed-up comparison.

    & **Teacher / E** & **Teacher / B** & **FitNet / E** & **AT / E** & **LSP / E** & **MSKD / E** & **VQG / E** & **Our / E**, B** \\  Arxiv & 71.91 ±0.06 & 73.21 ±0.19 & 67.56 ±1.79 & 67.48 ±0.25 & 69.53 ±0.03 & 69.27 ±0.21 & 68.59 ±0.11 & **70.89 ±5.53** \\ Proteins & 72.83 ±0.09 & 69.23 ±0.02 & 68.71 ±1.81 & 68.53 ±0.35 & 69.45 ±0.23 & 70.97 ±0.97 & 69.54 ±0.36 & **71.22 ±0.44