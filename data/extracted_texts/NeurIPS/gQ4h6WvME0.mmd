# Optimistic Rates for Multi-Task Representation Learning

Austin Watkins

Johns Hopkins University

Baltimore, MD 21218

awatki29@jhu.edu

&Enayat Ullah

Johns Hopkins University

Baltimore, MD 21218

enayat@jhu.edu

&Thanh Nguyen-Tang

Johns Hopkins University

Baltimore, MD 21218

nguvent@cs.jhu.edu

&Raman Arora

Johns Hopkins University

Baltimore, MD 21218

arora@cs.jhu.edu

###### Abstract

We study the problem of transfer learning via Multi-Task Representation Learning (MTRL), wherein multiple source tasks are used to learn a good common representation, and a predictor is trained on top of it for the target task. Under standard regularity assumptions on the loss function and task diversity, we provide new statistical rates on the excess risk of the target task, which demonstrate the benefit of representation learning. Importantly, our rates are optimistic, i.e., they interpolate between the standard \((m^{-1/2})\) rate and the fast \((m^{-1})\) rate, depending on the difficulty of the learning task, where \(m\) is the number of samples for the target task. Besides the main result, we make several new contributions, including giving optimistic rates for excess risk of source tasks (Multi-Task Learning (MTL)), a local Rademacher complexity theorem for MTRL and MTL, as well as a chain rule for local Rademacher complexity for composite predictor classes.

## 1 Introduction

Transfer learning has emerged as a powerful tool in modern machine learning. The goal is to find a good predictor on a given _target_ task with little data, by extracting knowledge from _source_ task(s). The source tasks can either be explicitly given (supervised learning), or constructed from domain knowledge (unsupervised/self-supervised learning). An example of the supervised learning approach is a widely successful practice in deep learning, wherein a complex model is first trained on a source task with plentiful data, then applied to a target task by extracting _representations_ of data by _freezing_ the last layer of the complex model, and training a simple (linear) classifier on top of it . Similarly, the unsupervised approaches of training a (large) neural network on a _general objective_ has seen tremendous success, especially in natural language processing  and computer vision. As before, a simple linear classifier trained on the output of the penultimate layer works well on a target task.

_Representation learning_, where the goal is to learn representations of data that are useful across multiple domains and tasks, is a common theme in the examples above. We study a popular formalization of this type of machine learning called _Multi-Task Representation Learning (MTRL)_. As the name suggests, we are given \(t\) source tasks each with a small number of data points, say \(n\). We seek to effectively pool the \(nt\) data points to extract a common good representation useful for the target task. Therefore, if our tasks are sufficiently "diverse", we hope to effectively circumvent our data scarcity by sharing knowledge between tasks. As a by-product, since the shared representationcan also be used for the source tasks themselves, this procedure is also effective for the related problem of _multi-task learning (MTL)_.

To study the transfer of knowledge via representations within the MTRL framework, we consider a **composite-learning model**. This model consists of \(\) and \(\), a class of hypotheses and representations respectively, where the final end-to-end predictors are given via the composition \(f h\), for all \(f,h\). Since we must decide between compositions, we use a two-stage empirical risk minimization procedure. First, **the multi-task representation learning stage**, the procedure selects one common representation and \(t\) predictors, one for each task, by minimizing the average empirical loss on the source tasks. Second, **the transfer learning stage**, the procedure selects a predictor for the target task that minimizes the empirical loss, w.r.t. \(m\) samples, on top of the fixed representation from stage one.

We desire to learn a good predictor - i.e., one that has small excess risk - from \(\) for the target task. In this work, we consider the class \(\) to be complex (e.g., deep neural networks), whereas the class \(\) consists of simple functions (e.g., linear predictors). Therefore, the class \(\) is complex, so directly learning it would require a large number of samples. Yet, if we can effectively pool all our data from each task, the total number of examples could be sufficiently plentiful to successfully learn a complex representation. Further, a good representation can significantly reduce the statistical burden of learning a good predictor for the target task. This phenomenon is formalized by the prior work of Tripuraneni et al. (2020), which is the work most related to ours. Specifically, they show that if the loss function is Lipschitz and the tasks are sufficiently diverse (see Definition 2), the excess (transfer) risk for the target task is bounded as,

\[=()+tC( )}{nt}}+)}{m}}),\]

where \(C()\) and \(C()\) denote the complexity measures associated with learning \(\) and \(\) respectively (precise definition to follow in a later section).

While the above rate has the desirable properties described above, it is insufficient on many fronts, which our work seeks to address. First, it is akin to the _standard_ rate of \(n^{-1/2}\), albeit optimal in agnostic settings, for single-task PAC learning. However, it is well-known that a _fast_ rate is achievable under realizability in the standard supervised learning. But, the above guarantee does not capture this possibility. Importantly, the work of Tripuraneni et al. (2020) operates under the assumption that the loss function is Lipschitz - in such settings, it is generally not possible to get fast rates, despite realizability, even for the special case of learning half-spaces (Srebro et al., 2010). In the single task setting, the literature contains a rich theory of guarantees that interpolate between the slow and fast rate depending on the _level_ of realizability, such guarantees are called "optimistic rates". We derive such optimistic rates for MTRL. Consequently, as an important application of our general theory, we show that the (transfer) risk of the MTRL approach on the target task is bounded as,

\[(_{}^{*}+_{}^ {*}(C()+tC())}{nt}}+_{ }^{*}C()}{m}}+)+tC()}{nt }+)}{m}) \]

when using non-negative smooth losses (such as squared loss in linear regression or smoothed ramp loss for classification (Cortes et al., 2021; Srebro et al., 2010)). The values \(_{}^{*}\) and \(_{}^{*}\) represent the minimum average risk over the source tasks and target task respectively, hence (1) yields a fast rate under _mild_ realizability, i.e. when they are small.

In the course of proving the above, we extend a number of foundational results in the single-task setting to the multi-task setting. We list them in our contributions below.

1. We give a general local Rademacher complexity result for transfer learning via multitask representation learning - see Theorem 1.
2. We also provide a local Rademacher complexity result for multitask learning - see Theorem 3. Our result has two benefits compared to prior work (Yousefi et al., 2018). First, we perform MTL via MTRL that leads to improved bounds which show the benefit of pooling data as opposed to learning each task separately. Second, we operate under the special, yet fairly natural setting, of hypothesis classes with product-space structure - this simplifies the proof significantly as well as recovers the result for single-task setting with the exact same constants (unlike rates in Yousefi et al. (2018)).

3. For non-negative smooth losses, we derive optimistic rates (such as Eqn. (1)) for transfer learning and MTL via MTRL. Notably, when we restrict our rates to the single task setting, our proof is significantly simpler and our obtained bound on local Rademacher complexity is smaller compared to the prior work of Srebro et al. (2010).
4. Finally, we present a chain rule for local Rademacher complexity for composite settings, which allows us to decouple the (local) complexities of representation and predictor classes.

### Our techniques

In this section, we expand on our contributions and provide an overview of techniques and challenges overcome in the context of prior art.

**Local Rademacher complexity for transfer learning via MTRL.** We build on the work of Tripuraneni et al. (2020), which introduced the notion of task diversity. They showed that task diversity leads to improved rates on excess transfer risk via MTRL, compared to training only on the target task. Their work however yields the standard \(O(m^{-1/2})\) rate on excess risk, akin to what is obtained in agnostic settings. We provide a local Rademacher complexity result for transfer learning via MTRL, yielding optimistic rates depending on distributional properties. Our methods are based on tools and techniques from the seminal local Rademacher complexity paper of Bartlett et al. (2005). A crucial component of our results is relative deviation Bennett concentration inequalities established via log-Sobolev inequalities together with the entropy method (Bousquet, 2002). We make the crucial observation that the above concentration inequalities, though established for the single-task setting, are general enough to be used to extend the local Rademacher complexity framework to the multi-task and transfer learning setting.

**Local Rademacher complexity for multi-task learning via MTRL.** Intermediate to our above contribution, we provide a local Rademacher complexity result for MTL via MTRL, yielding optimistic rates. We note that the prior work of Yousefi et al. (2018) already provided a local Rademacher complexity result for MTL, though not via MTRL; importantly, our work establishes the benefits of pooling data compared to learning each task separately. However, in contrast to Yousefi et al. (2018)1, our setting is limited to hypothesis classes with a product space structure rather than a general vector-valued hypothesis. This simply means that the tasks are permutation-invariant in our setting, which we argue is a fairly reasonable assumption in the multi-task context. This assumption makes the analysis much simpler as we can reuse concentration inequalities for single task setting whereas Yousefi et al. (2018) had to extend log-Sobolev concentration inequalities to the multi-task setting. Additionally, this yields bounds with better constants than Yousefi et al. (2018) - in the special case of a single task, this recovers the known result with the exact same constants.

**Optimistic rates for non-negative smooth losses.** An important application of our theorems is providing optimistic rates when using non-negative smooth losses, which is known to enable distribution-free optimistic rates in a single task setting (Cortes et al., 2021; Srebro et al., 2010). This result shows the provable benefits of representation learning. Further, it can be combined with the chain rule for Gaussian complexity of Tripuraneni et al. (2020). This allows us to separate the composite class \(\) into the complexities of representation class \(\) and predictor class \(\) and also reuse existing complexity bounds in the literature. While we borrow some of the tools for optimistic rates theory in the single task setting from Srebro et al. (2010), the prior proof technique does not directly extend to the multi-task setting. Consequently, we modify the proof, simplifying it significantly as well as getting an improved bound, even in the single task setting. In particular, Srebro et al. (2010) bounds the local Rademacher complexity of the loss class by an \(_{2}\)-covering number via Dudley's entropy integral formula, which is then bound by \(_{}\)-covering number of hypothesis class using smoothness, which in turn is bound by the fat-shattering dimension, which eventually is bound by the (global) Rademacher complexity of the hypothesis class. Some of the steps seemingly do not generalize in the multi-task setting because there are no (well-studied) multi-task analogues, e.g. the fat-shattering dimension. Our proof starts with the above bound on the local Rademacher complexity of the loss class by an \(_{2}\)-covering number via Dudley's entropy integral formula but then applies the well-known Sudakov minoration inequality that yields a bound in terms of Gaussian width; this is in turn bound by Rademacher complexity by standard known relations between the two quantities.

**Chain rule for local Rademacher complexities.** In the application to non-negative smooth losses, the local Rademacher complexity of the loss class is bounded by a non-trivial function of the (global) Gaussian complexity of the hypothesis class which is further bounded using the chain rule of Tripuraneni et al. (2020). For general loss functions, there may not be any such non-trivial function. To this end, for Lipschitz losses, we develop a chain rule of local Rademacher complexity, Theorem 5, that similarly allows separating the _local complexities_ of the representation and predictor class.

### Related work

As mentioned before, our work is most related to, and builds on, Tripuraneni et al. (2020). Other related work includes an early work of Baxter (2000) that gives guarantees on excess risk of transfer learning and multi-task learning via MTRL under a certain generative model for tasks. This was subsequently improved by Maurer et al. (2016), Pontil and Maurer (2013). These early works give rates of the form \((t^{-1/2}+m^{-1/2})\) and do not capture the advantage of having a large number of source samples per task. Prior work has extensively studied the special case of linear representations and/or linear predictors, partly in the context of meta learning, for instance see Du et al. (2021), Tripuraneni et al. (2021), Xu and Tewari (2021). Further, multitask learning has been explored under various notions of task relatedness (Ben-David and Borbely, 2008; Cavallanti et al., 2010).

Regarding optimistic rates, the seminal work of Vapnik and Cervonenkis (1971) provided the first optimistic rates, via a normalized uniform convergence analysis, but was limited to the PAC learning setting. Much later, a line of work of Bousquet et al. (2002), Koltchinskii and Panchenko (2000) derived optimistic rates in more general settings using the technique of _localization_. In this area, an important result is the local Rademacher complexity theorem of Bartlett et al. (2005), which many works have leveraged to yield improved rates for various problems (Blanchard et al., 2007; Cortes et al., 2013; Ullah et al., 2018). Importantly, this theorem applies to learning with non-negative smooth losses (Srebro et al., 2010) where the local Rademacher complexity-based technique can be used to give distribution-free optimistic rates. Further, optimistic rates have seen renewed interest owing to the interpolation/benign overfitting phenomenon in deep learning. In certain problems, predictors may interpolate the data (zero training error), yet have small risk. A line of work (Zhou et al., 2020, 2021) shows that in certain linear regression problems, optimistic rates can be used to derive risk bounds that explain the phenomenon, where the usual tool of uniform convergence provably fails.

There has been some work on optimistic rates in the multi-task setting. The work of Yousefi et al. (2018) established a local Rademacher complexity theorem for multi-task learning (see Section "Our techniques" for detailed comparison). Besides that, the work of Reeve and Kaban (2020) proved optimistic rates for vector-valued hypothesis classes for self-bounding loss functions (which includes non-negative smooth losses).

Notation.Denote \(\|\|_{2}\) be the Euclidean norm and \(\|\|_{}\) be the infinity norm. We use the convention that \([m]=[1,,m]\) is the list of contiguous integers starting at \(1\) and ending at \(m\). For \(=(f_{1},,f_{t})\) let \(^{2}=(f_{1}^{2},,f_{t}^{2})\) and denote

\[P:=_{j=1}^{t}P_{j}f_{j}=_{j=1}^{ t}(f_{j}(X_{j})),^{m} :=_{j=1}^{t}_{j}^{m}f_{j}=_{j=1}^{t} _{i=1}^{n}f(X_{j}^{i}).\]

## 2 Problem setup and preliminaries

Let \(^{d}\) and \(\) denote the input feature space and the output label space respectively. The source tasks and the target task are represented using probability distributions \(\{P_{j}\}_{j=1}^{t}\) and \(P_{0}\), respectively. Let \(\) and \(_{0}\) be a class of predictor functions from \(^{k}\) to \(\) for the source tasks and target task respectively, and \(\) a class of representation functions from \(^{d}\) to \(^{k}\).

Following Tripuraneni et al. (2020), we assume that marginal distribution of \(P_{j}\) over \(\) is the same for all the \(j\) from \(0\) to \(t\), and that there exists a common representation \(h^{*}\) and task-specific predictors \(f_{j}^{*}\) for \(j[t]\) and \(f_{0}_{0}\) such that \(P_{j}\) can be decomposed as \(P_{j}(x,y)=P_{x}(x)P_{y|x}(y|f_{j}^{*} h^{*}(x))\). Note that this decomposition does not assume that \(f_{j}^{*} h^{*}\) is the optimal in-class predictor; however, we will assume it is to make our optimistic rates more meaningful (although this is not strictly necessary). Also the decomposition implicitly assumes that \(y\) depends on \(x\) only via \(f_{j}^{*} h^{*}(x)\), and thus any additional noise in \(y\) is independent of \(x\). Note that like Tripuraneni et al. (2020), we allow the predictor class for the target task \(_{0}\) to be different than on the source tasks \(\). For example, when performing logistic regression with a linear representation \(B^{k d}\) and linear predictor \(w^{k}\), we have \(P_{y|x}(y=1|f^{*}_{j} h^{*}(x))=(w^{T}B^{T}x)\), where \(\) is the sigmoid function.

Given a loss function \(:\), the goal is to find a good predictor \(\) from the composite-class \(_{0}\) with small transfer risk,

\[R_{}(_{0},)=_{(x,y) P_{0}}[(_{0}(x),y)].\]

Yet, as standard, we do not have access to the joint distribution for all tasks, but instead \(n\) independent and identically distributed (i.i.d.) samples from each source task and \(m\) i.i.d. samples for the target task. Let \((x^{i}_{j},y^{i}_{j})\) be the \(i^{}\) sample for the \(j^{}\) task. To accomplish the above, we use the source data to find a representation, and the target data to find a predictor for the target task. Specifically, in this MTRL procedure, first, we train on all source task data, then freeze the resulting representation. Second, we train over this frozen representation to find a good predictor on the target samples.

We formalize the above as the following two-stage Empirical Risk Minimization (ERM) procedure,

\[(},)*{arg\,min}_{ ^{ t},h}_{j=1}^{t}_{i=1}^{ n}(f_{j} h(x^{i}_{j}),y^{i}_{j}),\] (2) \[_{0}*{arg\,min}_{f_{0}} {1}{m}_{i=1}^{m}(f(x^{i}_{0}),y^{i}_{0}).\] (Transfer learning) (3)

Besides bounding the transfer risk, we seek to understand if the above procedure also yields improved bounds for the source tasks (the problem of MTL).

\[R_{}(},)=_{j=1}^{t}_{ (x,y) P_{j}}[(_{j}(x),y)]\]

We make the following regularity assumptions.

**Assumption 1**.:
* _The loss function_ \(\) _is nonnegative and_ \(b\)_-bounded, i.e.,_ \(0(y^{},y) b<\) _for all_ \(y^{},y\)_._
* _All functions in_ \(\) _are_ \(L\)_-Lipschitz for some_ \(0<L<\) _w.r.t._ \(_{2}\)_, i.e.,_ \( f(z_{1})-f(z_{2})_{2} L z_{1}-z_{2} _{2}\) _for all_ \(z_{1},z_{2}(f)\) _and_ \(f\)_._
* _Any_ \(f h\) _is_ \(D\)_-bounded over_ \(\) _w.r.t._ \(_{}\)_, i.e.,_ \(_{x} f h(x) D<\) _for all_ \(f\) _and_ \(h\)_._

Unlike Tripuraneni et al. (2020), we assume for some of our theorems that our function is smooth instead of Lipschitz, as leveraging smoothness is key to achieving fast rates (Srebro et al., 2010).

**Definition 1** (\(H\)-smoothness).: _The loss function \(:\) is \(H\)-smooth if \(^{}_{y}(y_{1},y)-^{}_{y}(y_{2},y)  H y_{1}-y_{2}\) for all \(y_{1},y_{2},y\)._

**Task diversity (Tripuraneni et al., 2020).** We benefit from learning a representation for a new target task, so long as there must be information encoded for that task by the source tasks. A typical way to quantify this encoded information is to control the difference between the learned representation and the underlying representation. To this end, we use the framework introduced by Tripuraneni et al. (2020) that quantifies a measure of task diversity. As mentioned in their work, this framework recovers task diversity assumptions in Du et al. (2021) exactly. In our setting, the definitions given in Tripuraneni et al. (2020) simplify considerably.

**Definition 2**.: _The tasks \(\{P_{j}\}_{i=1}^{t}\) are \((,)\)-diverse over \(P_{0}\), if for the corresponding \(^{*}^{ t},f^{*}_{0}_{0}\) and representation \(h^{*}\), we have that for all \(h^{}\)_

\[_{f^{}_{0}}R_{}(f^{},h^{})-R_ {}(f^{*}_{0},h^{*})(_{^{} ^{ t}}R_{}(^{},h^{})-R_{ }(^{*},h^{*}))+.\]

Parameters \(\) and \(\) quantify the similarity between learning the source tasks and the target task. For a detailed analysis of these parameters and the framework introduced by Tripuraneni et al. (2020), see Appendix B.

**Local Rademacher Complexity.** We now define the measures of complexity that are used in our work. Let \(u,p,n\), an input space \(\), a class of vector-valued functions \(:^{u}\), and a dataset \(=(z^{i}_{j})_{j[p],i[n]}\) where \(z^{i}_{j}\). Define the _data-dependent_ Rademacher width, \(}_{}()\), as

\[}_{}(^{ p}) =_{_{i,j,k}}_{^{  p}}\!\!_{i,j,k=1}^{n,p,u}_{i,j,k}(q_{j}(z^{i }_{j}))_{k}\!\!,\]

where \(_{i,j,k}\) are i.i.d. Rademacher random variables. Analogously, define the _data-dependent_ Gaussian width, \(}_{}()\), as

\[}_{}(^{ p}) =_{g_{i,j,k}}_{^{  p}}\!\!_{i,j,k=1}^{n,p,u}g_{i,j,k}(q_{j}(z^{i}_{ j}))_{k}\!\!,\]

where \(g_{i,j,k}\) are i.i.d. \((0,1)\) random variables. We define the _worst-case_ Rademacher width as \(}_{n}(^{ p})=_{^{pn}}}_{}(^{ p})\) and, analogously, the _worst-case_ Gaussian width as \(}_{n}(^{ p})=_{^{pn}}}_{}(^{ p})\). The above definitions generalize the standard Rademacher and Gaussian width for real-valued functions and can be derived as a special case of the more general _set-based_ definitions (Wainwright, 2019) (see Appendix A).

We now define _local Rademacher width_ as \(}_{}(^{ p},r)=}_{}(\{^{ p}:V()  r\})\), where \(V:^{p}\). That is, the _local Rademacher width_ is simply the Rademacher width restricted by a functional. In our applications, we consider any \(V\) satisfying \(V() b_{i,j,k=1}(q_{j}(z^{i}_{j}))_{k}\), where \(b\) is the uniform bound on the range of \(q\) in \(_{2}\)-norm. Note this recovers the classical local Rademacher width2 for real-valued functions and non-product spaces (\(p=u=1\)). We are mostly interested in the local Rademacher width of the loss applied to the pairwise composition between our vector-valued hypothesis class and our representation class. Specifically, if we define such a class as \(_{}(^{ t}())=\{(_ {1} h,,_{t} h) h, ^{ t}\}\), then we seek to bound \(}_{n}(_{}(^{ t}()),r)\). Crucially, our bound will be a _sub-root function_ in \(r\), where we define this type of function below.

**Definition 3** (Sub-root Function).: _A function \(:[0,)[0,)\) is sub-root if it is nonnegative, nondecreasing, and if \(r(r)/\) is nonincreasing for \(r>0\)._

Sub-root functions have the desirable properties of always being continuous and having a unique fixed point, i.e., \(r^{*}=(r^{*})\) is only satisfied by some \(r^{*}\); see Lemma 8 in Appendix A.

## 3 Main Results

In this section, we detail our local Rademacher complexity results for transfer learning (TL) and MTL via MTRL, a bound on the local Rademacher complexity of smooth bounded non-negative losses, and the application of this bound to both TL and MTRL.

Our first result is a local Rademacher complexity result for TL via MTRL.

**Theorem 1**.: _Let \(\) and \(_{0}\) be the learned representation and target predictor, as described in Eqns. (2) and (3). Under Assumption 1.A, if \(^{*}\) is \((,)\)-diverse over \(_{0}\) w.r.t. \(h^{*}\) and let \(_{1}\) and \(_{2}\) be sub-root functions such that \(_{1}(r)_{n}(^{ t},r)\) and \(_{2}(r)_{m}(_{0},r)\), then with probability at least \(1-4e^{-}\), the transfer learning risk is upper-bounded by_

\[R_{}(_{0},)  R_{}(f^{*}_{0},h^{*})+c_{1}(}(f^{*}_{0},h^{*})}(}+_{1 }}{b}})++_{1}}{b})\] \[+(c_{2}(}(^{*},h^{*})}(}+_{2}}{b}})++_{2}}{b}))+, \]

_where \(r^{*}_{1}\) and \(r^{*}_{2}\) are the fixed points of \(_{1}(r)\) and \(_{2}(r)\), respectively, and \(c_{1}\) and \(c_{2}\) are absolute constants.3_

Inequality (4) bounds the transfer risk in terms of task diversity parameters \(\) and \(\); local Rademacher complexity parameters, fixed points \(r_{1}^{*}\) and \(r_{1}^{*}\); the minimum risk for the target task; and the minimum average risk for the source tasks.

As discussed in Xu and Tewari (2021), there exist settings where the task diversity parameters \(\) and \(\) are favorable, i.e., \(=0\) and \(=(1)\), so we will disregard them in the subsequent discussion. The bound is an optimistic rate because it interpolates between \(^{*}}+^{*}}\) and \(r_{1}^{*}+r_{2}^{*}\), depending on both \(R_{}(f_{0}^{*},h^{*})\) and \(R_{}(^{*},h^{*})\).

The fixed point \(r_{1}^{*}\) is a function of \(^{ t}()\) and encodes its complexity measured with respect to the number of samples for the source tasks (among other class-specific parameters). The same reasoning holds for \(r_{2}^{*}\) w.r.t. \(_{0}\).

We consider \(\) to be _complex_ and both \(\) and \(_{0}\) to be _simple_. So, if we instead learned the target task directly, then we would pay the complexity of \(_{0}()\) against \(m\) samples. In contrast, in our bound, we only pay the complexity of \(_{0}\) for \(m\) samples and the complexity of \(^{ t}()\) for \(nt\) samples. Since in many settings \(nt\) is much larger than \(m\), we are successfully leveraging our representation to learn the target task.

The dependence on minimum risk is useful in various settings. An instructive regime is when \(nt m\) with \(R_{}(f_{0}^{*},h^{*})=0\) and \(R_{}(^{*},h^{*})>0\); in this case, we achieve a bound of \(^{*}}+r_{2}^{*}\). This demonstrates that data-abundant environments can be leveraged to learn representations that work well in small sample regimes.

The next result uses a decoupling of function classes, the Gaussian chain rule of Tripuraneni et al. (2020), and smoothness to bound \(r_{1}^{*}\) and \(r_{2}^{*}\). We see that under this additional assumption that \(\) is \(H\)-smooth, \(r_{1}^{*}\) and \(r_{2}^{*}\) decay quickly.

**Theorem 2**.: _Under the setting of Theorem 1 along with \(\) being \(H\)-smooth and Assumptions 1.2 and 1.3, with probability at least \(1-6e^{-}\), the fixed points in Inequality (4),_

\[r_{1}^{*} c_{3}b(H}_{m}^{2}(_{0} )^{2}(m)+(1+)),\]

\[r_{2}^{*} c_{4}b((L^{2}}_{nt}^{2}()+}_{n}^{2}())H(nt)^{4}+ H^{2}(nt)+b(1+)}{nt}),\]

_where \(c_{3}\) and \(c_{4}\) are absolute constants._

**Remark 1**.: _Note that in Theorem 2 it is not necessary to apply the chain rule. If the chain rule is not used, Assumptions 1.2 and 1.3 is not needed. The fixed point \(r_{2}^{*}\) would be bounded as a function of \(^{ t}()\). In the following, we use the above decomposed form for interpretability._

Note that for constant \(L,D,H,b\), and \(\), the terms on the right are _always_ fast, so the rate of decay depends crucially on the behavior of the Gaussian widths for each class. For many function classes used in machine learning, it is common that \(_{nt}())}{nt}}\) and \(_{n}())}{n}}\), where \(C()\) is some notion of complexity of the function class, which could be, for instance, the VC dimension, pseudo-dimension, fat-shattering dimension, etc. Therefore, it is common that

\[L^{2}}_{nt}^{2}()+}_{n}^{2} () L^{2})}{nt}+)}{n} \;\;\;\;}_{n}^{2}(_{0}) _{0})}{m}.\]

Recall, from our discussion following Theorem 1, that these fixed points control the order of the rate. Therefore, this theorem interpolates between a rate of \(1/+1/\) and \(1/m+1/nt\), where the fast rate is achieved in a realizable setting.

**Linear classes.** As an example let us consider a linear projection onto a lower dimensional space along with linear regressors and squared loss. Let \(=^{d}\), \(=\), and

\[=\{f f()=^{},^{k},\|\| c\},\]

\[=\{()=^{} ,^{d k},\}.\]

If we assume that \(P_{x}\) is sub-Gaussian, then standard arguments (see Tripuraneni et al. (2020)) show that \(}_{m}^{2}(_{0})( {k}{m})\), \(}_{n}^{2}()()\), and \(}_{nt}^{2}()(d}{nt})\). To simplify the bounds let \(L=1\), \(H=1\), \(D=1\), \(b=1\), and \(=0.05\). Thus, under the assumptions of Theorem 2, the fixed points are bounded by \[r_{1}^{*}(+),r_{2}^{*} (d}{nt}++),\]

which gives

\[R_{}(_{0},)-R_{}(f_{0}^{*},h^{*}) (}(f_{0}^{*},h^{*})}(})+)\] \[+ ((}(^{*},h^{*}) }(d}{nt}+})+d}{nt}+))+.\]

The following result is such a bound on the local Rademacher complexity for a non-negative \(H\)-smooth loss and any function class \(\).

**Proposition 1** (Smooth non-negative local Rademacher complexity bound).: _Under the setting of Theorem 1 along with \(\) being \(H\)-smooth and Assumptions 1.B and 1.C, there exists an absolute constants \(c_{3}\) and \(c_{4}\) such that_

\[_{nt}(_{}(^{ t}()),r)  c_{3}G(^{ t}()) (n)^{2}+(n)}{}+}, \]

\[_{nt}(_{}(^{ t}()),r)  c_{4}(^{ t}(H))}{(^{ t}(H))}+}, \]

_where \(G(^{ t}())=L}_{nt}( )+}_{n}()\) and \((^{ t}(H))=G(^{ t}( ))^{ t}( ))}\)._

When we specialize the above bound to the standard single-task non-compositional setting and compare with Srebro et al. (2010), then

\[}_{n}(_{}(,r))  c}_{n}()r( }{}_{n}()})\] \[ c}_{n}()r(}{}_{n}( )})\] \[ c^{}}_{n}()r {(n)}(} ).\]

The second inequality uses the relation \(}_{n}() 2}_{n}( )\)(Wainwright, 2019, p. 155) and that \(x x eC/x\) is increasing in \(x\) until \(x=C\). The third inequality uses Khintchine's inequality, \(}_{n}()B}{}\), where \(B\) is the \(_{2}\)-bound on the range of \(\). Thus, the upper bound above is always better than the bound in Srebro et al. (2010)4 for large enough \(n\). Furthermore, under the additional though well-studied assumption that \((0) HB^{2}\)(Arora et al., 2022; Amari, 2015), ours is better for \(n=(1)\). Finally, the Gaussian and Rademacher widths are of the same order for the class of linear predictors, bounded in norm, by a strongly convex regularizer, in general non-Euclidean settings. Hence, in this setting, our bound is smaller by a factor of \(\) in Kakade et al. (2008) - see Appendix E for a detailed comparison.

### Multi-Task Learning

Next, we detail some theorems foundational to the results above. For those results, we were able to link MTL with MTRL by using the task diversity assumption. Therefore, we naturally have the following local Rademacher result for MTL.

**Theorem 3**.: _Let \((},)\) be an empirical risk minimizer of \(_{}(,)\) as given in Eqn. (2). Under Assumption 1.A, let \(\) be a sub-root function such that \((r)_{n}(^{ t}(),r)\) with \(r^{*}\) the fixed point of \((r)\), then with probability \(1-2e^{-}\),_

\[R_{}(},) R_{}(^{*},h^{*}) +c}(^{*},h^{*})} {nt}}+}{b}}++}{b}, \]

_where \(c\) is an absolute constant._As above, we can use our bound on the local Rademacher complexity of a \(H\)-smooth loss class to get the following result. This result is similar to Srebro et al. (2010), Theorem 1, yet ours is in an MTL via MTRL setting.

**Theorem 4**.: _Under the setting of Theorem 3 along with \(\) being \(H\)-smooth and Assumptions 1.2 and 1.3, with probability at least \(1-3e^{-}\), \(r^{*}\) of \((r)\) is bounded by_

\[cb(L^{2}}_{nt}^{2}()+ }_{n}^{2}())H(nt)^{4}+H (nt)^{2}}{nt}+,\]

_where \(c\) is an absolute constant._

### Local Rademacher complexity chain rule

Using the chain rule of Tripuraneni et al. (2021), as we did above, to decouple the complexities of the representation and prediction classes is desirable. Yet, our general local Rademacher complexity Theorems 1 and 3, as stated, do not seem to have this property. Consequently, we develop a _local chain rule_, which aims to separate the _local complexities_ of learning the representation and predictor. The main result is the following.

**Theorem 5**.: _Suppose the loss function \(\) is \(L_{}\)-Lipschitz. Define the restricted representation and predictor classes as follows,_

\[_{}(r) :=\{^{ t}:  h:V( h) r\}\] \[_{}(r) :=\{h:^{ t} :V( h) r\},\]

_where \(V\) is the functional in the local Rademacher complexity description. Under Assumptions 1.2 and 1.3 and that the worst-case Gaussian width of the above is bounded by the sub-root functions \(_{}\) and \(_{}\), respectively, there exists an absolute constant \(c\) such that_

\[}_{n}(_{}(^{ t }(),r)) c(LL_{}_{}(r)+_{ }(r))(nt)+}.\]

## 4 Proof Techniques

In this section, we give details on some of the tools we use by giving proof sketches for the results in Section 3. In the next section, we cover some theorems at the root of all our results.

Proof sketch for Theorem 1.Let \(_{0}=_{f}R_{}(f,)\). By a risk decomposition, we can bound the excess transfer risk by \(R_{}(_{0},)-R_{}(_{0},)+ _{f_{0}_{0}}_{f^{}}\{R_{}(f^{},)-R_{}(f_{0},h^{*})\}\). We can now use Theorem 3, with \(t=1\), to bound \(R_{}(_{0},)\). Note that \(R_{}(f_{0}^{*},)-R_{}(_{0},) 0\). Now \(_{f^{}}R_{}(f^{},)-R_{}(f_{0}^{*},h^{*})\) is less than \((_{^{}^{ t}}R_{}( ^{},)-R_{}(^{*},h^{*})+,\) by the task-diversity assumption. As shown in Tripuraneni et al. (2020), we can bound \(_{^{}^{ t}}\{R_{}(^{ },)-R_{}(^{*},h^{*})\}\) with \(R_{}(},)\). We can then apply Theorem 3 to the remaining \(R_{}(},)\) term. By leveraging task diversity again we can bound the remaining \(R_{}(f_{0}^{*},)\) as a function of \(R_{}(f_{0}^{*},h^{*})\).

Proof sketch for Proposition 1.We achieve the following bound on the local Rademacher complexity of the loss class by using a truncated version of Dudley's integral (Srebro et al., 2010, see Lemma A.3).

\[_{nt}(_{}(^{ t}( )),r)_{0}4+10(nt)^{-1 /2}_{}^{}_{2}(_{}( ^{ t}()),r),,nt)}}\]

We observe that we can use smoothness to move the dependence on the loss to an appropriate scaling, which gives that \(_{2}(_{}(^{ t}()),r), ,nt)_{2}(^{ t}(), /,nt)\) (Lemma 18 in the appendix). Next, we apply Sudakov minoration to bound the log covering number by the Gaussian width of our function class: \(_{2}(^{ t}(),/ ,nt)}}{}}_{ n}(^{ t}()).\) With these simplifications, the parameter \(\) from Dudley's integral can either be solved for exactly (as in Inequality 6), so long as \(\), or set in such a way that it always holds (like in Inequality 5). Finally, we can optionally apply the Gaussian chain rule from Tripuraneni et al. (2020) to bound \(}_{n}(^{ t}())\) with \(L}_{nt}()+}_{n}()\).

**Proof sketch for Theorem 2.** The right hand side of Inequality 5 is a sub-root function in \(r\), because it is an affine transformation of \(\). Therefore, we just solve for the fixed point of this sub-root function, i.e., solve for \(r^{*}\) w.r.t. the equation below.

\[c}G(^{ t}()) (n)^{2}+(n)}{}+}=r^{*}. \]

All that remains of the proof is basic algebraic simplifications. The proof for Inequality 6 is similar.

## 5 Discussion

The proofs of the above results hinge on the following local Rademacher complexity result.

**Theorem 6**.: _Let \(\) be a class of non-negative \(b\) bounded functions. Assume that \(X^{1}_{j},,X^{n}_{j}\) are \(n\) draws from \(P_{j}\) for \(j[t]\) and all \(nt\) samples are independent. Suppose that \( V() BP\), where \(V\) is a functional from \(^{ t}\) to \(\) and \(B>0\)._

_Let \(\) be a sub-root function with fixed point \(r^{*}\) such that \(B(^{ t},r)(r)\) for all \(r r^{*}\), then for \(K>1\) and \(>1\), with probability at least \(1-e^{-}\),_

\[^{ t} P^ {n}+200(1+)^{2}}{B}++2( +).\]

Yousefi et al. (2018), Theorem 9, showed a result of this form in a more general setting of Bernstein classes and a function class that is not necessarily a product space. In contrast, for simplicity, we do not consider Bernstein classes, though we believe such a generalization is possible, and our function classes are product spaces. By doing so, we achieve better constants than Yousefi et al. (2018) by using the next theorem.

**Theorem 7** (Concentration with \(t\) functions).: _Let \(\) be a class of functions from \(\) to \(\) and assume that all functions \(f\) in \(\) are \(P_{j}\)-measurable for all \(j[t]\), square-integrable, and satisfy \(f=0\). Let \(_{f}f 1\) and \(Z=_{^{ t}}_{j=1}^{t}_{i=1}^{n}f_{j}(X^{ i}_{j})\). Let \(>0\) such that \(^{2}_{^{ t}}_{j=1}^{t} f_{j}(X^{1}_{j})\) almost surely. Then, for all \( 0\), we have_

\[\{ZZ++2Z)}+)\} e^{-}.\]

_This result also holds for \(_{^{ t}}|_{j=1}^{t}_{i=1}^{n}f_{j} (X^{i}_{j})|\) under the condition that \(_{f}\|f\|_{} 1\)._

Yousefi et al. (2018) also proves something similar to the above from scratch using "Logarithmic Sobolev" inequalities. We make the observation that within our setting a proof similar to the original proof given by Bousquet (2002) still holds. To clarify this further, Yousefi et al. (2018) state:

Note that the difference between the constants in (2) (Bousquet, 2002, Theorem 2.3) and (3) (Yousefi et al., 2018, Theorem 1) is due to the fact that we were unable to directly apply Bousquet's version of Talagrand's inequality (like it was done in Bartlett et al. (2005) for scalar-valued functions) to the class of vector-valued functions.

We show that within the product space structure, which we consider to be the most natural for MTL, it is possible to achieve the same constants. Concretely, when \(t=1\), we realize the single function version of Bousquet (2002) exactly.

The proof of Theorem 6 is similar to the one given in Bartlett et al. (2005), Sec. 3, and generalized to MTL by Yousefi et al. (2018), Appendix B. Indeed, we follow the same steps within these proofs but use our inequality above.

## 6 Conclusion

We provide the first optimistic rates for transfer learning and multi-task learning via multi-task representation learning. This is achieved by establishing a local Rademacher complexity result for multi-task representation learning. We provide distribution-free optimistic rates for smooth non-negative losses by bounding the local Rademacher complexity for multi-task loss classes. Besides our contributions to multi-task representation learning and multi-task learning, our work provides several foundational results and improved rates in the standard single-task setting. Directions for future work include: exploring adversarial robustness, active learning, and fine-tuning of the representation within multi-task representation learning.