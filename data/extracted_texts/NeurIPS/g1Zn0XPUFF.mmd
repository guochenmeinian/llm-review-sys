# UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling

Haider Al-Tahan\({}^{1}\), Quentin Garrido\({}^{1,2}\), Randall Balestriero\({}^{3}\),

**Diane Bouchacourt\({}^{1}\), Caner Hazirbas\({}^{1}\), Mark Ibrahim\({}^{1}\)**

\({}^{1}\)Meta FAIR, \({}^{2}\)Univ Gustave Eiffel, CNRS, LIGM, \({}^{3}\)Brown University

###### Abstract

Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. Yet, with an ever-growing number of benchmarks, researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress. To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a range of carefully categorized vision-centric capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations. Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU. _UniBench_ with model evaluations on all benchmarks are provided as a toolbox at: https://github.com/facebookresearch/unibench

## 1 Introduction

The growing investment in vision-language models (VLMs), capable of a range of open-world multimodal tasks, has spurred the development of numerous benchmarks. Although in principle a more thorough set of evaluations is welcome, the ever-growing number of benchmarks has resulted in a complex, fragmented landscape for evaluation. Researchers are tasked with the heavy burden of implementing the protocol for each benchmark and making sense of how all these benchmarks translate into meaningful axes of progress. Of course, running such a large number of benchmarks also carries a non-trivial computational burden. Consequently, many new models are evaluated only on a _subset of available benchmarks_. When benchmarks are omitted, the research community is faced with blind spots in model strengths and weaknesses. Additionally, comparing the performance of one model versus others becomes challenging as the underlying set of benchmarks is not comparable. Ultimately, drawing well-founded conclusions about the best strategies to advance VLMs in this fragmented landscape of benchmarks is a challenge.

To help researchers navigate this overwhelming landscape of benchmarks and ease the burden of systematically evaluating VLMs, we introduce UniBench. In UniBench we implement 53 visionlanguage model benchmarks in a unified, user-friendly code-base. These benchmarks cover a range of vision-centric capabilities from standard object recognition to spatial understanding, counting, geographic robustness, domain-specific medical and satellite imagery, and many others. With such a comprehensive set of benchmarks, we shine a light on the blind spots in the strengths and weaknesses of the model. Next, to ensure that the research community can translate the many resulting metrics into meaningful axes of progress, we categorize these benchmarks into seven types and seventeen finer-grained capabilities, as shown in Figure 1. Researchers can quickly pinpoint model strengths and weaknesses in a comprehensive, apples-to-apples fashion.

We demonstrate the utility of UniBench by evaluating nearly 60 openly available vision-language models spanning a range of architectures, model sizes, training dataset scales, and learning objectives with scales of up to 12.8B samples and 1B parameters. We systematically compare this diverse set of models across the axes of progress in UniBench. We find that scaling, model size, or training data is a powerful lever for many axes of performance, but offers little benefit for visual relations and reasoning. We also find today's best VLMs struggle with simple benchmarks involving numerical comprehension, even with the right training data, on tasks such as character recognition or counting--including decades old benchmarks such as MNIST and SVHN (LeCun et al., 1998; Netzer et al., 2011). Where scale falls short, we find tailored learning objectives and training data quality are promising levers for relations and reasoning. Finally, we provide practical recommendations on which models practitioners should select. For example, we find large open models such as Eva ViT-E/14 to be a good choice for a general-purpose VLM while models such as NegCLIP excel at specialized tasks such as visual relations.

To facilitate systematic, comparable, yet easy-to-run evaluations we distill the many benchmarks into a few representative evaluations. We provide the UniBench codebase including the 50+ benchmarks with comparisons against all 59 VLMs as well as the distilled set of representative benchmarks that can run in less than 5 minutes on a single A100 GPU. We hope our contribution facilitates thorough and practical evaluation of vision-language model capabilities to faithfully gauge research progress and surface promising strategies to advance VLM research.

## 2 Related Works

### Visual Models From Natural Language Supervision

Visual models trained with natural language supervision have revolutionized computer vision by enabling models to learn rich, joint representations of images and text. A seminal work in this area is CLIP (Contrastive Language-Image Pre-training) introduced by Radford et al. (2021), which demonstrated that pre-training on a large dataset of image-caption pairs using a contrastive objective yields models with remarkable zero-shot transfer capabilities to downstream tasks.

Figure 1: **Benchmark Types in UniBench with their respective performance gains from scaling model size and training dataset size.** Scale offers limited benefits for relational understanding and reasoning tasks.

Following CLIP's success, numerous methods have been proposed to enhance visual models through natural language supervision (Bordes et al., 2024; Jia et al., 2021; Yao et al., 2021; Yu et al., 2022; Li et al., 2022; Singh et al., 2022; Gadre et al., 2023). These models vary in their approaches, including differences in backbone architectures, training objectives (contrastive learning, image-text matching, masked language modeling), and the scale and quality of the training data. To assess the capabilities of these models, the community has developed a diverse set of benchmarks that test various aspects of visual and multimodal understanding (Yuksekgounl et al., 2023; Thrush et al., 2022; Hsieh et al., 2024).

However, the proliferation of benchmarks and models has led to a fragmented evaluation landscape, making it challenging to comprehensively assess and compare models. Different benchmarks emphasize different capabilities, and inconsistent evaluation protocols hinder direct comparison. This fragmentation underscores the need for unified evaluation frameworks like UniBench, which aim to provide a cohesive and comprehensive suite of benchmarks covering a wide range of vision-language understanding tasks.

### CLIP-Style versus LLM-style Evaluation

Evaluation of VLMs has been an active area of research in recent years (Li et al., 2023; Yue et al., 2024; Liu et al., 2024; Salin et al., 2023; Bitton et al., 2023). While these benchmarks provide an insightful perspective of VLM capabilities, they primarily focus on LLM-style evaluations, which generate tokens or text as output. These benchmarks are not suitable for evaluating CLIP-like VLMs, which focus on vision-language classification and understanding capabilities. As a result, they do not allow for direct comparisons with CLIP-Style models.

CLIP-Style evaluation is a widely used approach that calculates the similarity between the image representation and text label. This method focuses on vision-language classification and understanding capabilities, making it particularly useful for evaluating models used as backbone/foundation models for image generation and fine-grain visual tasks (Rombach et al., 2021; Ramesh et al., 2021; Saharia et al., 2022). In contrast, LLM-style evaluation asks the model to demonstrate its knowledge via text generation. While this approach is suitable for evaluating models designed for text-based tasks, it may not be as effective for evaluating models focused on visual tasks. The key difference between CLIP-Style and LLM-style evaluation lies in their respective objectives: CLIP-Style aims to assess a model's ability to align visual and textual representations, whereas LLM-style focuses on assessing a model's ability to generate coherent and accurate text.

UniBench focuses on CLIP-Style evaluation, which provides a more comprehensive understanding of a model's visual reasoning capabilities. By concentrating on traditional zero-shot tasks and predefined choices, we enable an apples-to-apples comparison of progress over the past few years, shedding light on promising directions for future research.

## 3 UniBench: A comprehensive unified evaluation framework for VLMs

Here we describe the benchmarks, protocols, and axes of progress that comprise UniBench as well as the VLMs evaluated.

### VLMs Considered in UniBench

We evaluate 59 openly available VLMs across a range of model sizes, pre-training dataset sizes, learning objectives, and architectures (full list in Appendix Table 6). For training dataset size, we include models trained and/or fine-tuned with datasets ranging from \(13\) million to \(12.8\) billion samples; including DataComp (Gadre et al., 2023) (small, medium, large, and extra-large), LIAON (Schuhmann et al., 2022) (400M, 2B, 5B), MetaCLIP (Xu et al., 2023) (400M and 2.5B), Flickr (Young et al., 2014), PMD (Singh et al., 2022), and COCO (Lin et al., 2015). For model size and architecture, we categorize models based on the number of parameters and whether these models are convolutional or transformer-based models, ranging from ResNet50 (He et al., 2016) with 38 million parameters to EVA02 ViT E (Fang et al., 2023) with 4.3 billion parameters.

Evaluation ProcedureWe evaluate performance of zero-shot classification benchmarks similar to (Radford et al., 2021), by contrasting the representations of class labels (averaged across promptsas defined by Cherti et al. (2022)) with the image representations and using the class with the highest probability as the predicted class. For relation benchmarks, we follow the standard protocol of contrasting correct and incorrect captions with image representations.

### Benchmark Types

To better navigate the overwhelming number of VLM benchmarks, we classify benchmarks into seven distinct types (Figure 1 each covering an key aspect of model performance):

1. **Non-Naural Images:** Consisting of PCam(Veeling et al., 2018), Diabetic Retinopathy(Wang and Yang, 2018), ImageNet Sketch(Wang et al., 2019), imagnetr(Hendrycks et al., 2021a), eurosat(Helber et al., 2019, 2018), and resisc45(Cheng et al., 2017), these benchmarks evaluate the models' ability to handle non-natural images, such as computer-generated graphics, medical images, or satellite imagery.
2. **Object Recognition:** These benchmarks focus on the models' ability to accurately identify and classify objects within images. It includes benchmarks with variety of objects and settings, from everyday items to specific categories like animals or vehicles. Consisting of CUB (Wah et al., 2011), iNaturalist (Van Horn et al., 2018), Pets (Parkhi et al., 2012), MNIST (LeCun et al., 1998), Rendered SST2 (Radford et al., 2021), SVHN (Netzer et al., 2011), Caltech 101 (Fei-Fei et al., 2004), Stanford Cars (Krause et al., 2013), Cifar 10 (Krizhevsky et al., 2009), Cifar 100 (Krizhevsky et al., 2009), Country211 (Radford et al., 2021a), Dollar Street (Gaviria Rojas et al., 2022), FGVC Aircraft (Maji et al., 2013), Flowers 102 (Nilsback and Zisserman, 2008), Food 101 (Bossard et al., 2014), GTSB (Stallkamp et al., 2012), STL-10 (Coates et al., 2011), VOC 2007 (Everingham et al., 2012), ImageNet (Deng et al., 2009), Places365 (Zhou et al., 2017), sun397 (Xiao et al., 2010), MNIST Fashion (Xiao et al., 2017), and PUG: ImageNet (Bordes et al., 2023).
3. **Reasoning:** These benchmarks test the models' capacity to understand relationships between objects, spatial reasoning, and logical inference based on visual input. The benchmarks consist of CLEVR (Johnson et al., 2017), dmlab (Zhai et al., 2019), DSPR (Matthey et al., 2017), Kitti (Geiger et al., 2012), smallNORB (LeCun et al., 2004), and CountBench (Paiss et al., 2023).
4. **Robustness:** These benchmarks evaluates the models' resilience to adversarial attacks and variations in image data. It includes tests with perturbed images to see how well the models can maintain performance under challenging conditions. For example, the ObjectNet benchmark introduces changes in object position, scale, and background, while the ImageNet-R benchmark focuses on transformations related to many types of image renditions. This collection inclues ImageNet-E (Li et al., 2023c), ObjectNet (Barbu et al., 2019), ImageNet-A (Hendrycks et al., 2021b), ImageNet-O (Hendrycks et al., 2021b), ImageNet-9 (Xiao et al., 2020), and ImageNet-V2 (Recht et al., 2019).
5. **Relation:** We include relational benchmarks, such as _Visual Genome_(Yuksekgonul et al., 2023), Winoground (Thrush et al., 2022a), and SugarCrepe (Hsieh et al., 2024) designed to evaluate the models' ability to understand and represent relationships between objects within an image, a crucial aspect of visual understanding. For instance, _Visual Genome_ benchmark includes a variety of relationships (denoted VG-Relation) and attributions (denoted VG-Attribution) tasks, such as spatial relationships (_e.g_, "above", "next to"), action relationships (_e.g_, "riding", "holding"), and appropriate attribution (_e.g_, "the brown horse and the orange cat" vs. "the orange horse and the orange brown").
6. **Texture:** We rely on DTD (Cimpoi et al., 2014) a benchmark focusing on the models' capability to recognize and differentiate textures within images, which is crucial for tasks such as material recognition and scene understanding.
7. **Corruption:** Consisting of ImageNet-C benchmark (Hendrycks and Dietterich, 2019) introduces various types of image corruptions, such as noise, blur, and digital artifacts. These corruptions simulate the types of degradation that images may undergo in real-world scenarios, such as poor lighting conditions, low-quality cameras, or transmission errors.

### Benchmark Capabilities

We further breakdown benchmarks into several capabilities:1. **Depth Estimation, Pose Detection, and Spatial Understanding:** Assessing the models' ability to estimate the depth of objects and scenes from images, and detect object poses which is crucial for understanding spatial relationships.
2. **Medical and Satellite:** Testing the models' performance on medical imaging tasks, such as identifying diseases or conditions from medical scans while testing on satellite imagery requires requires recognizing and interpreting land use, terrain, and other geographic features.
3. **Counting and Character Recognition:** Assessing the models' ability to identify digits and count objects within images, a fundamental skill for quantitative understanding.
4. **Geographic Diversity:** Evaluating the models' capability to recognize and interpret images from diverse geographic locations and settings.
5. **Scene Recognition:** Measuring how well models can identify and classify different scenes or environments.
6. **Standard Object Recognition, ImageNet and Challenging ImageNet:** Evaluating performance on the widely used benchmark for object recognition. We also include the ubiquitous ImageNet and more difficult variants of ImageNet to evaluate model robustness and adaptability.
7. **Specific Classification:** Evaluating models on tasks that require classification of specific categories or fine-grained distinctions between similar objects.
8. **Texture Detection:** Assessing the models' ability to recognize and differentiate various textures within images.
9. **Rendition:** Assessing models' performance on tasks involving rendered or synthetic images, which differ from natural photographs.
10. **Corruptions and Natural Transformations:** Evaluating robustness to image corruptions, such as noise, blur, and other artifacts that degrade image quality whereas natural transformations includes common changes in lighting, rotation, or perspective.

### UniBench: a systematic, practical VLM evaluation

UniBench is framework for comprehensive, fast, and easy-to-use evaluation of VLMs. UniBench also has the ability to expand the existing set of benchmarks and VLMs, as shown in (Code Snippet 1).

```
1importunibench
2fromunibench.models_zoo.wrappers.clipimportClipModel
3fromtorchision.datasetsimportFashionMNIST
4
5evaluator=unibench.Evaluator()
6model=partial(
7ClipModel,
8model=model,
9model_name="vitamin_1_comp1b",
10tokenizer=tokenizer,
11input_resolution=model.visual.image_size,
12logit_scale=model.logit_scale,
13)
14evaluator.add_model(model=model)
15class_names=["T-shirt/top",...]
16templates=["animageof{}",...]
17benchmark=partial(FashionMNIST,root="./",train=False,download=True)
18handler=partial(ZeroShotBenchmarkHandler,benchmark_name="
19fashion_mnist_new",classes=class_names,templates=templates)
20evaluator.add_benchmark(benchmark,handler,meta_data={"
21benchmark_type":"objectrecognition"})
22evaluator.evaluate() ```

Code Snippet 1: Running UniBench with a custom model and a new benchmark. UniBench accepts any torchvision dataset type.

## 4 Gauging progress in Vision Language Modeling with UniBench

We show the overall median performance of the nearly 60 VLMs we examined on 53 benchmarks in Figure 2 ranked by their zero-shot classification performance. The results suggest that, while VLMs perform remarkably well on many tasks, for others, VLM performance is near or below random chance level. These results highlight the need for a unifying pipeline to systematically surface model limitations.

### Scaling improves many benchmarks, but offers little benefit for reasoning and relations

Scaling training dataset size hardly helps for reasoning and relations.While scaling training dataset size improves performance across many tasks, this trend does not hold for benchmarks assessing relation understanding and reasoning capabilities. To control for other confounding factors, we fix the architecture, learning paradigm, model size (for right panel), and training dataset size (for left) by using the same CLIP ViT-B/32 model and LAION 400M dataset, respectively Figure 3. The results suggest despite increasing the training dataset size by a factor of \(1000\), relational and reasoning benchmarks performance is fairly flat compared to the significant boost in performance on other tasks. We observe a similar trend overall when we include all 59 models in Appendix Figure 7. We specifically pinpoint capabilities such as Depth Estimation, Spatial Understanding, Counting, Scene and Text Recognition, as the underlying capabilities where scale does not lead to improvements as shown in Figure 4.

Scaling model size also offers little to no benefit for reasoning or relations.When we scale models' size from 86 million parameters to 1 billion parameters, we also find that models struggle to scale on similar proportions on relation and reasoning tasks as shown in Figure 3. While for other benchmark types including object recognition, robustness, etc. performance improves by 17.4% as model size scales by \(11\), relations and reasoning improve by a modest 3.41% with a fairly flat scaling curve. Similar to scaling training dataset size, scaling model size also offers little benefit for capabilities such as Depth Estimation, Spatial Understanding, Counting, Scene and Text Recognition as shown in Figure 4.

Figure 2: **Median performance of all 59 VLMs on 53 benchmarks, illustrating that despite advancements, VLMs still struggle on several benchmarks. Benchmarks that barely exceed chance-level performance include Winoground, iNaturalist, DSPR, Small Norb, dmlab, Clevr, PCam, Renderedssst2, and Kitti. Blue bars represent the median zero-shot performance of the models, while grey bars indicate the chance-level for each benchmark.**Figure 4: **The effect scaling of training dataset (left) and model size (right) across capabilities for all models.** Accuracy is the difference in performance between the most scaled and the least scaled model across capabilities relative to ImageNet performance.

Figure 3: **The effect of scaling model and training dataset size using a fixed architecture and learning paradigm.** Zero-shot performance of models on various benchmark types. We investigate the impact of training dataset size (left), and model size on various benchmark types (right). To isolate the effect of scale, we fix the architecture, learning paradigm, model size (for right panel), and training dataset size (for left) by using the same CLIP ViT-B/32 model and LAION 400M dataset, respectively. We observe a similar trend when measured across all 59 models as shown in Appendix Figure 7

A Case Study: Digit Recognition and Counting are notable limitations for VLMs even with the right training data

A surprising aspect of VLMs is their poor performance on benchmarks that are traditionally considered straightforward, such as MNIST, CIFAR-10, and CIFAR-100, as shown in Figure 2. For example, a simple 2-layer MLP achieves 99% accuracy on MNIST  significantly outperforming all 59 VLMs we studied. To delve deeper into this unexpected result, we controlled for several variables:

1. **VLM confusions go beyond top-1:** To further understand the performance results on MNIST, we compute more generous top-2,-3,-4, and -5 accuracy measures to understand whether models confuse similar digits. We show in Appendix Figure 10 that even when we compute top-5 accuracy (with 50% being chance), VLMs barely reach 90% accuracy suggesting poor performance is not due to minor confusions among digits.
2. **Prompt engineering isn't enough for good performance:** To ensure that the poor performance was not an artifact of the prompts used, we tested multiple hand-crafted prompts that included detailed descriptions of the image characteristics Appendix Figure 9. Despite these tailored prompts, which explicitly described the black-and-white nature and content of the images, the performance still lagged significantly and simpler models.
3. **Training data contains ample samples with digit concept:** We investigated whether the subpar performance could be attributed to a lack of training images containing digit concepts by analyzing the popular LAION 400M dataset. Our findings reveal a substantial number of captions with both word digits (100k-2M) and integer digits (15M-48M) in the training captions, suggesting that the poor performance is not merely due to insufficient training data (see Figure 11 for exact counts by digit).
4. **VLMs struggle on other digit benchmarks:** To further explore whether the poor performance on MNIST is indicative of broader issues in number comprehension, we extend our investigation to other benchmarks such as SVHN, CountBench, and ClevrCount (Appendix Figure 6). We find across all benchmarks VLMs struggled with number recognition and counting tasks.

TakeawayDespite training on vast datasets, even leading VLMs can struggle with simple tasks solved trivially by much smaller models, including tasks involving basic number comprehension. These findings highlight the need for a comprehensive evaluation pipeline that includes so called, simpler benchmarks, to uncover VLM limitations.

Figure 5: **Performance of 59 VLMs on MNIST, showing despite progress, VLMs still struggle on MNIST.** Blue bars represent zero-shot performance of models, grey bars represent the chance-level for MNIST, and green bar shows performance for a 2-Layer MLP.

[MISSING_PAGE_FAIL:9]

## 5 UniBench: A Practical Way Forward for Faster Comprehensive VLM Evaluations

While ideally, evaluating VLMs across all 53 benchmarks would provide the most comprehensive insights, the computational demands and complexity of parsing such extensive data can be overwhelming (6 million images to evaluate; 2+ hours for one model on an A100 GPU). While ImageNet maybe a tempting candidate as it correlates with many benchmarks, for many others, specifically 18 of the 53 benchmarks, ImageNet performance is poorly or negatively correlated Appendix Figure 12. This suggests that success on ImageNet does not universally translate to proficiency in all tasks.

Comprehensive VLM evaluation with UniBench in 5 minutes.To streamline evaluation, we distill the full set of benchmarks in UniBench into seven benchmarks most representative of each axis of progress (via correlations in Appendix A.6). Fortunately, in UniBench this comprehensive set of benchmarks runs in 5 minutes on a single A100 GPU (for ViT-B/32), offering a fast, yet comprehensive evaluation pipeline.

## 6 Discussion

LimitationsWhile we invested a considerable effort to include as comprehensive set of models and benchmarks as possible, there of course will always be new ones we do not cover. We focus especially on vision-centric benchmarks to track progress since the early contrastive vision-language models. To mitigate that, we provide a flexible interfaces to extend UniBench with additional models or benchmarks (see code 1). Our analysis is also limited to the standard zero-shot evaluation protocol.

ImpactTo guide the research community in navigating the overwhelming and fragmented landscape of VLM benchmarks, we introduced UniBench. UniBench provides a unified implementation of 50+ benchmarks, out-of-the-box comparisons across nearly 60 open VLMs, and a distilled fast-to-run set of representative benchmarks that can run on in 5 minutes a single GPU. In doing so, we uncover the limits of scale for reasoning and relations, the promise of data quality and tailored learning objectives, as well as offer recommendations for which VLMs practitioners should use. We hope UniBench is a step towards avoiding the blindspots in VLM evaluations, enabling researchers to comprehensively, yet efficiently evaluate progress.