# TAIA: Large Language Models are

Out-of-Distribution Data Learners

 Shuyang Jiang

Fudan University

Shanghai Artificial Intelligence Laboratory

shuyangjiang23@m.fudan.edu.cn

&Yusheng Liao

Cooperative Medianet Innovation Center,

Shanghai Jiao Tong University

Shanghai Artificial Intelligence Laboratory

liao20160907@sjtu.edu.cn

&Ya Zhang

Yanfeng Wang

Yu Wang

Cooperative Medianet Innovation Center,

Shanghai Jiao Tong University

Shanghai Artificial Intelligence Laboratory

{ya_zhang, wangyanfeng622, yuwangsjtu}@sjtu.edu.cn

Equal contribution, alphabetical order.Corresponding Author

###### Abstract

Fine-tuning on task-specific question-answer pairs is a predominant method for enhancing the performance of instruction-tuned large language models (LLMs) on downstream tasks. However, in certain specialized domains, such as healthcare or harmless content generation, it is nearly impossible to obtain a large volume of high-quality data that matches the downstream distribution. To improve the performance of LLMs in data-scarce domains with domain-mismatched data, we re-evaluated the Transformer architecture and discovered that not all parameter updates during fine-tuning contribute positively to downstream performance. Our analysis reveals that within the self-attention and feed-forward networks, only the fine-tuned attention parameters are particularly beneficial when the training set's distribution does not fully align with the test set. Based on this insight, we propose an effective inference-time intervention method: Training All parameters but Inferring with only Attention (TAIA). We empirically validate TAIA using two general instruction-tuning datasets and evaluate it on seven downstream tasks involving math, reasoning, and knowledge understanding across LLMs of different parameter sizes and fine-tuning techniques. Our comprehensive experiments demonstrate that TAIA achieves superior improvements compared to both the fully fine-tuned model and the base model in most scenarios, with significant performance gains. The high tolerance of TAIA to data mismatches makes it resistant to jailbreaking tuning and enhances specialized tasks using general data. Code is available in https://github.com/pixas/TAIA_LLM.

## 1 Introduction

Large language models (LLMs) have revolutionized Natural Language Processing (NLP), where LLMs have been pretrained on a massive textual corpus and encoded massive world knowledge . These models achieve remarkable zero-shot and few-shot performance across a wide range of tasks . The innovation of instruction tuning, also known as supervised fine-tuning (SFT), hasfurther enhanced the instruction-following capabilities of LLMs [11; 46], simplifying human-LLM interactions. Despite the availability of high-quality data for SFT being limited [7; 86], expanding SFT datasets remains a straightforward method to adapt LLMs for specific tasks . Various SFT datasets, such as Alpaca  and Natural Instructions [41; 69], have been manually curated or artificially generated to create more generalized instruction-tuned LLMs.

However, real-world applications of LLMs are diverse  and complex , often making public datasets insufficient. While synthetic data is useful, it is expensive and tends to exhibit a distribution shift biased towards the parent LLM . Consequently, the data distribution that LLMs adapt to during fine-tuning often differs significantly from that required for specific tasks. This discrepancy leads to inferior performance on specialized tasks and knowledge forgetting due to disruptions in the parametric knowledge stored in LLMs . Figure 1 also shows that with more out-of-distribution (OOD) tuning data, the vanilla fine-tune method brings catastrophic forgetting problems, degrading models' performance on downstream tasks. The scarcity of natural data and the suboptimal quality of synthetic data present substantial challenges to effectively adapting LLMs for specialized tasks. In essence, the dependency on in-domain distribution fine-tuning corpora hampers the broader deployment of LLMs.

To address this, we propose avoiding such data dependency by leveraging the intrinsic properties of fine-tuning and developing an inference-time method that does not rely on high-quality in-distribution data. We first conduct an in-depth investigation of the internal Transformer architecture. We find that during fine-tuning, LLMs enhance their instruction-following ability, primarily controlled by the self-attention module . Conversely, parameterized knowledge is encoded by the key-value intrinsic of the feed-forward network (FFN) module [18; 40] during pretraining . Fine-tuning primarily elicits this pretrained knowledge [46; 59; 71], which remains relatively fixed . This insight prompts us to discard the FFN updates during fine-tuning, as only a small portion positively contributes to downstream performance, while most disrupt the knowledge when fine-tuned on task-mismatched data.

A naive approach is to fine-tune only the attention parameters, but this fails to generalize to OOD data due to insufficient exploration of non-linearity. To ensure sufficient learning of non-linearity, we introduce additional FFN parameters during fine-tuning but retain only the beneficial self-attention updates. This strategy, named Training-All-Inferring-only-Attention (TAIA), achieves both OOD generalization and sufficient optimization space. The comparisons between the proposed method and the vanilla fine-tuning method are shown in Figure 2

Figure 1: Performance comparison of various fine-tuning methods under three OOD data mixing scenarios. The target domain is medical knowledge, using Chinese subset of MMedBench  as the in-domain training dataset. (a) The dataset is mixed with medical OOD data from CMExam , maintaining a total dataset size of 20k; (b) The dataset is mixed with general OOD data from CoT-Collection , also keeping the total dataset size at 20k; (c) The dataset includes general OOD data from CoT-Collection, while the size of the in-domain training dataset remains at 20k. As the proportion of OOD data increases, the performance of the vanilla fine-tuning declines significantly, whereas TAIA manages to sustain robust performance in the target domain (details in Appendix E.5).

We validate TAIA across seven datasets including math, reasoning, and knowledge understanding, using four LLM families and two fine-tuning techniques. Extensive experiments demonstrate the efficacy of TAIA across various model configurations and its outstanding robustness compared to other baselines. Furthermore, detailed analyses confirm the reproducibility of TAIA in terms of fine-tuning methods and fine-tuning dataset scales. TAIA also maintains the few-shot adaptation ability of base models and withstands multi-level red-teaming attacks. It consistently improves performance in vertical domains like healthcare with increasing OOD data (see Figure 1).

Overall, we conclude our contributions as three-fold:

1. **Necessity Analysis:** We analyze the necessity of leveraging OOD data for effective downstream fine-tuning, revisiting the roles of self-attention and FFN in the Transformer architecture and formalizing their contributions during fine-tuning.
2. **Inference-Time Intervention:** We propose a simple yet effective inference-time method that trains all parameters but retains only the self-attention updates. This approach optimizes performance across downstream and closed-book tasks, as validated by extensive experiments.
3. **Expanding Model Adaptability:** Our approach introduces an innovative method for utilizing OOD data in fine-tuning LLMs, substantially decreasing the dependence on in-domain data. This advancement enhances the adaptability of LLMs, enabling them to perform exceptionally well across a wider range of specialized and real-world tasks.

## 2 Preliminaries

Self-attention moduleLet \(\{t_{i}\}_{i=1}^{N}\) represents the inputs to an Transformer-based LLM, and \(\{_{i}\}_{i=1}^{N}^{d}\) represent the token representations after the embedding layer of a Transformer-based LLM. For each layer \(l\), LLM initially computes the query, key, and value vectors:

\[_{m}^{l}=f_{q}(_{m}^{l},m)_{n}^{l}=f_{k}( _{n}^{l},n)_{n}^{l}=f_{v}(_{n}^{l},n)\]

Figure 2: Comparison between different fine-tuning and inference methods. Parameters colored with green and yellow represent models finetuned with in-domain and out-of-distribution data, respectively. “ID” and “OOD” represents in-distribution and out-of-distribution, respectively. When we train in-domain data (colored as green) and out-of-domain data (colored as yellow) and evaluate in in-domain test sets and out-of-domain test sets, respectively (The second row; fine-tuning). The vanilla fine-tuning method can only perform well when trained on ID data and evaluated in ID test sets. Compared to vanilla tuning, TAIA can perform generally well on both types of test sets when given OOD data. As a similar approach that only trains attention, TOA (Train-only-attention) performs badly on both types of evaluation sets as it loses sufficient exploration of optimal parameter groups.

where \(m,n\) are token indexes in the sequence and \(f_{\{q,k;v\}}\) are position embeddings parameterized by RoPE . After that, the attention score is computed between these two position tokens:

\[_{m}^{}=(_{m}^{ }W_{q}^{l}W_{k}^{l}_{m}^{}}{}})_{m}^{l}W_{v}^{l}\] (1)

where \(W_{q}^{l},W_{k}^{l},W_{v}^{l}^{d d_{k}}\) are learnable weight matrices, \(d\) is the model dimension and \(d_{k}\) is the inner dimension and \(_{m}^{l}=[_{1}^{l},,_{m}^{l}]^{} ^{m d}\), \(_{m}^{l}=[_{1}^{l},,_{m}^{l}]^{} ^{m d}\) and we use the single-head notation for simplicity. Finally, another projection matrix \(W_{o}^{l}^{d d_{k}}\) is used to project \(_{m}^{l}\) back to token space \(_{m}^{l}=W_{o}^{l}_{m}^{l}\). Self-attention with rotary position embedding is more effective for computing contextual mappings  in arbitrary sequences, particularly in long contexts . It incorporates an induction-head mechanism that enables the Transformer architecture to predict co-occurring tokens within a given sequence [15; 44] from an in-context perspective. Meanwhile, Wu et al.  systematically demonstrate that self-attention significantly enhances its instruction-following capability through fine-tuning. Knowledge tokens that do not appear in the context are stored as global tokens in the FFN memory .

Feed-forward network (FFN)In modern transformer architectures, the SiLU  gating linear unit  is adopted by various models [3; 65; 66]. It is formulated as:

\[_{m}^{l}=W_{d}^{l}((W_{g}^{l}_{m}^{l}) W _{u}^{l}_{m}^{l})\] (2)

where \(W_{g}^{l},W_{u}^{l}^{d^{} d}\), \(W_{d}^{l}^{d d^{}}\) and \(d^{}\) is the hidden dimension. \(\) is the element-wise multiplication and \((x)=x(x)\). The feed-forward network uses inverse-bottleneck parameterization methods, which inherently enlarges the representation space and eases the encoding of diverse knowledge from different tasks [18; 40].

Finetuning towards tasksDuring fine-tuning in task-related data, natural practices format data as {instruction,(input),output} pairs: \((I,x^{t},y^{t})\) where \(t\{1,2,,N\}\) and \(N\) is the dataset size. In a causal LLM architecture, the learning objective is to minimize the task distribution with the LLM's internal distribution, via a negative log-likelihood manner:

\[_{}=-_{t=1}^{N}_{i=1}^{T} p_{ {}}(y_{i}^{t}|y_{<i}^{t},x^{t},I)\] (3)

where \(T\) is the output sequence length. This objective prompts \(\) to converge when the generated response \(^{t}\) matches \(y^{t}\), i.e., the internal distribution of \(\) aligns with the fine-tuning dataset.

## 3 TAIA: Training All Parameters but Inferring with Only Attention

### Motivation

Fine-tuning LLMs for downstream tasks typically requires a substantial amount of high-quality conversational data. While a large volume of high-quality synthetic data generated by GPT-4  is publicly available and useful for general domains like Math Word Problems and programming, real-world applications of LLMs are diverse  and complex . This diversity renders public datasets insufficient for many scenarios. Synthetic data, although useful, is costly and often exhibits distribution shifts towards the parent LLM . Consequently, the data distribution achieved through fine-tuning often diverges from that required for specific tasks. The scarcity of natural task-specific data and the suboptimal quality of synthetic data present significant challenges for the effective transfer of LLMs to specific tasks.

To address these issues and reduce LLMs' dependency on specialized data, we aim to enable LLMs to perform proficiently on specific tasks using OOD data. Given that the self-attention and feed-forward network (FFN) modules within LLMs function differently, we re-evaluate their respective roles during supervised fine-tuning. Our study indicates that OOD fine-tuning introduces noisy parametric knowledge into the FFN memory. Therefore, filtering out noisy parameters while retaining beneficial ones is crucial for generalization (SS3.2). Through systematic analysis and empirical validation, we demonstrate that the optimal parameter selection strategy is achieved by TAIA, which involves training all parameters but retaining only attention updates (SS3.3).

### Parameter Selection for Out-of-Distribution (OOD) SFT

Prior research has demonstrated that LLMs possess a wide range of task knowledge [6; 50; 56] after semi-supervised learning on web data. To enhance their proficiency in specific tasks, domain-related instruction-tuning  is utilized to improve the knowledge access process [46; 59; 71]. Moreover, studies  have shown that self-attention improves LLMs ability to follow instructions through fine-tuning, which aids in effective knowledge elicitation. However, when trained on OOD data, the optimization objective (Eq. 3) involves significant distribution shifts in certain parameter groups. This can disrupt the pre-trained knowledge encoded through the Transformer's feed-forward network (FFN) . Therefore, a balanced approach is to disregard the parameters that are noisily disrupted, while preserving the parameters that contribute to held-out tasks. This approach is already endorsed by existing research . Since fine-tuning prioritizes effective instruction following over absorbing potentially misleading knowledge, it can be inferred that the knowledge acquired by the FFNs during fine-tuning could be considered somewhat redundant.

### Towards Optimal Parameter Selection Strategy

Based on the above analysis, a subsequent action is to directly fine-tune only the parameters of the self-attention modules and freeze the FFN ones, which we call TOA (Train Only Attention Parameters). A similar practice to TOA is parameter-efficient fine-tuning (PEFT), as they both only train partial parameters. PEFT has achieved a trade-off between performance and training efficiency due to the superfluity of parameters in LLMs [21; 80]. We anticipate that TOA could yield results comparable to those obtained by training all parameters, as it essentially represents a form of the PEFT method and has been verified in vision tasks . However, as shown in Equation 1, there are few non-linear operations during the attention computation, which inhibits TOA from learning complex representations of the training data. Without sufficient representation exploration, TOA suffers from unlearning of general features via OOD data, despite circumventing catastrophic forgetting problems. We conduct experiments (details in Appendix E.6) to validate the inferiority of TOA in learning general instruction-following ability and show the results in Figure 3. With FFN modules participating in gradient descents, the performance on the downstream task increases with the proper selection of FFN modules, even if the introduction of FFN modules disrupts pretrained memory. However, TOA still lags far behind the base model, indicating its inability to extract non-intervention features from OOD data. To maintain the advantage of the non-linearity of FFN modules on the update of self-attention modules, we adopt another approach: we add all parameters into the optimizer group. In contrast to the vanilla method, we only maintain the updated self-attention part and reuse the pretrained weights for FFN modules after fine-tuning, which we name Training All parameters but Inferring with only Attention (TAIA). TAIA guarantees that self-attention can leverage the gradient descent process to optimize its parameters with redundant FFN fine-tuning parameters. The removal of updated FFN parameters during inference, on the other hand, ensures the integrity of parameterized knowledge stored in original FFN modules and the well-learning of beneficial knowledge from OOD data, as supported in Figure 3.

### Implementation of TAIA

During training, the parameters of FFN and self-attention are updated based on max-likelihood modeling:

\[^{}_{ffn},^{}_{attn}=_{\{\}}_{i=1} ^{N}- p(_{i}|_{i},_{ffn},_{attn})\] (4)

where \(N\) is the number of training samples and \(_{i},_{i}\) are query and response sequences sampled from any conversational-style data, respectively. \(^{}_{()}\) is the updated weight in full fine-tuning or the

Figure 3: Performance of TOA and TAIA with the layer-wise FFN LoRA. All models are equipped with attention LoRA at each layer and fine-tuned on a corpus mixture with 50% OOD data.

merged weight of LoRA tuning. After training, TAIA only utilizes the updated attention parameters and reuses the pre-trained FFN parameters to perform inference:

\[=_{}_{j=1}^{K}- p(y_{j}|_{j-1},,_{ ffn},^{}_{attn})\] (5)

where \(K\) is the generated sequence length and \(\) is the query input to LLMs that shares different distributions with the training data. In this scenario, \(_{ffn}\) is the original parameter of FFN in pre-trained models and \(^{}_{attn}\) is the updated parameter groups of self-attention in full fine-tuning (or merged weight of self-attention in LoRA tuning).

## 4 Experiments

### Backbone LLMs

We select two LLM families, Qwen1.5  and LLaMA  and delicately chose control groups to address the following three concerns: (1) Different Model Sizes within the Same LLM Family: We choose Qwen1.5-1.8B and Qwen1.5-7B to test within the same LLM family, how TAIA works for different sizes of LLMs with the same pretraining data; (2) Same Model Size across Different LLM Families: We choose Qwen1.5-7B and LLaMA2-7B to test among different LLM families but the same size, whether TAIA still holds; and (3) Impact of Enlarged Pretraining Data: We choose

LLaMA2-7B and LLaMA3-8B to test whether TAIA is applicable when the LLM pretraining data is significantly enlarged. We choose the chat version for all models.

### Experiment Details

We choose two instruction tuning corpus to further demonstrate the high generalization of TAIA under PEFT methods. We choose Alpaca-GPT4-bilingual mixed from Alpaca-GPT4 and Alpaca-GPT4-zh . Apart from this, we also adopt CoT-Collection  which is a mixture of various tasks presented in the Chain-of-Thought  format. We train 1 epoch for each dataset with the maximum context set to 3072 and the batch size set to 128. We set the learning rate to \(2e-4\) for all runs and adopt LoRA  and Mixture-of-LoRA (MoLoRA)[30; 76] as representative PEFT methods. The LoRA rank is set to 16 and LoRA alpha is set to 32. In MoLoRA, we set the expert count to 4 and activate 1 during inference for all settings. All experiments were conducted on 4 NVIDIA A100 GPUs with ZeRO3  optimization. For the test set, we selected seven widely used datasets: two for evaluating models' knowledge understanding and five for testing LLMs' reasoning ability. A detailed description of these test sets can be found in Appendix E.

### Quantitative Analysis

Table 1 presents a comprehensive comparison between various fine-tuning methods (vanilla, LoRA, MoLoRA, and our proposed TAIA) across different training datasets and model backbones. The results clearly demonstrate that TAIA enhances the utilization of training data, consistently outperforming other methods across mentioned benchmarks. For weaker LLMs like Qwen1.5-1.8B and LLaMA2-7B, TAIA significantly amplifies the improvements achieved by standard fine-tuning. For stronger backbones such as Qwen1.5-7B and LLaMA3-8B, standard fine-tuning often degrades performance, but TAIA maintains and even enhances the original capabilities. Notably, TAIA-fine-tuned LLaMA3-8B excels in SVAMP and MMB benchmarks, achieving top scores of 85.10 and 59.07, respectively, indicating its robustness in deep math comprehension and medical reasoning tasks. Furthermore, in the MMLU benchmark, TAIA-fine-tuned models achieve superior average scores, confirming that TAIA not only protects pretrainpretrained knowledge from disturbance but also enables better knowledge utilization for reasoning. These findings underscore the superior efficacy of TAIA in enhancing LLMs' performance across diverse reasoning and knowledge domains.

### Compare with Other OOD Generalization Methods

We mainly choose methods aimed for continual learning (CL) which also attempts to improve models with incoming OOD training data. We select L2, EWC , Self-Distill  and LoRACL, which is a variant of AdapterCL  as the competitors and the detailed settings of the experiment are discussed in Appendix E.7 Table 2 shows that although CL-based methods can leverage OOD data for downstream tasks, they are ineffective in certain evaluation sets (e.g., L2 on MMedBench or LoRACL

    &  &  &  &  \\  & & **MATH** & **BBH** & **COA.** & **LogiQA** & & & **SMMP** & **MMB.** & **MMLU** & **Avg.** \\  Base Model & Vanilla & 4.28 & 16.80 & 58.39 & 32.41 & 24.80 & 33.78 & 43.62 & 30.58 \\   Alpaca-GPT4 \\  } & Vanilla & 8.02 & 28.80 & 60.44 & 35.02 & 22.10 & 34.80 & 41.67 & 32.98 \\  & L2 & 3.68 & 24.37 & 57.82 & **35.33** & 21.30 & 35.04 & 41.30 & 31.26 \\  & EWC & 3.56 & 25.02 & 60.52 & 34.10 & 22.50 & 34.88 & 41.07 & 30.10 \\  & Self-Distill & 7.34 & 26.29 & 53.07 & 28.57 & 18.20 & 35.04 & 39.87 & 28.09 \\  & LoRACL & 8.04 & 28.80 & 60.03 & 34.41 & 27.40 & **35.27** & 42.18 & 33.73 \\  & TAIA & **10.82** & **30.03** & **64.29** & 33.03 & **29.90** & 34.64 & **43.73** & **35.21** \\   CoT- \\ Collection \\  } & Vanilla & 7.68 & 13.90 & 58.07 & 21.97 & 39.00 & 27.65 & 25.51 & 27.68 \\  & L2 & 0.12 & 5.66 & 23.26 & 22.12 & 41.50 & 27.73 & 23.63 & 20.64 \\   & EWC & 0.10 & 7.71 & 22.64 & 22.27 & 40.40 & 27.73 & 23.67 & 20.65 \\   & LoRACL & 7.68 & 14.85 & 58.07 & 21.97 & **41.60** & 27.65 & 23.53 & 27.91 \\   & TAIA & **9.64** & **21.93** & **67.32** & **32.57** & 40.30 & **34.64** & **42.39** & **35.54** \\   

Table 2: Comparison with other OOD generalization methods. TAIA is more robust and general than other competitive methods and requires no additional implementation efforts.

on CommonsenseQA). It indicates that these methods have specific preferences for downstream tasks and cannot be perfectly applied to any arbitrary application. In contrast, TAIA is not only implementation friendly but also generalizable enough for improving most downstream performances.

### Ablation Study

We test three variants of TAIA, all designed to reduce distribution shifting after fine-tuning on OOD data: TOA, TOF, and TAIF. The latter two, TOF and TAIF, are similar to TOA and TAIA respectively, but with relevant parameters changed from self-attention to FFN. Experiments were conducted on the Qwen1.5-1.8B model using the same setting described in SS4.2. Results are shown in Table 3. We observe that both TAIA and TAIF demonstrate better generalization properties compared to the vanilla method, with TAIA achieving the best. This again confirms the crucial role of self-attention in maintaining the generalization ability of LLMs. In contrast, TOF and TOA both suffer from inadequate parameter exploration and even perform worse than the baseline when tuned on OpenMath , further supporting the practice of retaining redundant parameters during training.

### Representation Analysis

In SS3.3, we infer that TAIA can obtain more general hidden representations compared to the baseline and TOA. We here examine the generalization of TAIA from a perspective of activation similarities. We define the activation similarity of the \(i\)-th data sample between two models, \(_{p}\) and \(_{q}\), which are trained separately on two corpora \(D_{p}\) and \(D_{q}\) with different distributions, as

\[(_{p},_{q})_{i}=_{l=1}^{L} {_{pi}^{l}_{qi}^{l}}{\|_{pi}^{l}\|\| _{qi}^{l}\|}\] (6)

where \(_{p},_{q}\) are the activation hidden states after certain modules, and \(L\) is the number of hidden layers. We select C-Eval  as the test and Medical-Collection, a 180K subset of CoT-Collection and OpenMath  as our training corpus. We follow the same experimental setting as described in SS4.2 and present the performance-similarity relations in Figure 5. The results show that a large proportion of activation similarities for TAIA are close to 1, significantly higher than those of other methods. This high activation consistency of TAIA correlates with its superior performance, regardless of the training corpus used. It confirms that by emphasizing instruction-following ability through TAIA, LLMs demonstrate robust generalization performance and effective transferability of training data.

## 5 Analysis

In this section, we discuss the following research questions (RQ) of the TAIA strategy:

* Does TAIA suit full fine-tuning where the catastrophic forgetting is even more severe?

    &  &  &  &  &  \\  & & &  &  &  &  &  &  \\   &  &  & 39.60 & 27.47 & 37.74 & 57.88 & 29.69 & 8.24 & 33.44 \\   & &  TAIA \\  & 45.85 & 44.47 & 55.57 & 64.97 & 36.45 & 10.20 & **44.37** \\   & &  TAIF \\  & 47.06 & 42.05 & 45.62 & 58.76 & 32.05 & 9.06 & 39.10 \\   & &  TOA \\  & 43.37 & 29.21 & 39.90 & 59.54 & 30.79 & 7.90 & 35.12 \\   & &  TOF \\  & 41.18 & 26.64 & 39.82 & 58.17 & 29.22 & 8.14 & 33.86 \\   &  &  & 54.04 & 39.36 & 50.67 & 58.03 & 33.62 & 7.60 & 40.55 \\   & &  TAIF \\  & 54.35 & 43.98 & 56.32 & 63.81 & 35.82 & 11.68 & **44.33** \\    & &  TAIF \\  & 53.46 & 43.53 & 54.85 & 62.84 & 36.25 & 7.64 & 43.09 \\    & &  TOA \\  & 53.37 & 33.73 & 50.22 & 57.88 & 30.56 & 7.50 & 38.88 \\    & & 
 TOF \\  & 52.95 & 37.46 & 48.37 & 55.34 & 31.66 & 7.48 & 38.88 \\   

Table 3: Ablation experiments on different inference modes under two training corpus. We validate the performance of inference modes by considering both general tasks and domain tasks. **Bold** indicates the optimal result in each subgroup and underline indicates the suboptimal result. Note that even fine-tuned on out-of-domain data, TAIA still achieves optimal results on specific domain tasks and even surpasses the performance of the base model.

* We have confirmed that TAIA learns only the beneficial parts of the fine-tuning data. Does this mean that it can survive in red teaming and enhance the model's helpfulness?
* How proficient can TAIA show if we scale the training corpus?
* Wang et al.  finds that supervised fine-tuning hurts LLMs few-shot performance on unseen tasks. Can TAIA restore similar few-shot ability as the base LLM?
* Fine-tuning converges to the downstream distribution, leading to the diminishing rank compared to the base LLM. How does the rank change when TAIA is adopted?

Response to RQ1: TAIA is also applicable to the full fine-tuning technique.Our analysis and empirical study focused on PEFT scenarios, mitigating catastrophic forgetting. To test TAIA in a full fine-tuning context, we maintained the same experiment settings as with LoRA tuning but lowered the learning rate to \(5e-5\) for stability and used the CoT-Collection as the fine-tuning corpus. Testing on Qwen1.8b and 7b sizes, the results (Table 4) indicate TAIA maintains superior performance in reasoning tasks (SVAMP, MATH, CommonsenseQA). However, due to extensive parameter modifications during full fine-tuning, TAIA experiences significant catastrophic forgetting in knowledge-intensive tasks (MMLU, MMMedBench). Despite this, it still outperforms the vanilla inference method, validating its applicability and generalization in full fine-tuning scenarios.

Response to RQ2: TAIA significantly reduces harmfulness and improves helpfulness.The analysis and experiments above have demonstrated that TAIA enables LLMs to generalize on OOD data, reducing dependency on data quality. To explore if TAIA can handle training data with harmful information while enhancing LLM usefulness without substantially increasing harmfulness, we followed Qi et al.  to red-team LLaMA2-7B-chat using three attack levels and evaluated on Advbench . We used 100 of the most harmful samples from the Anthropic red team dataset , 10 identity-shifting samples from Qi et al. , and benign data from Alpaca-GPT4 . For models tuned on benign data, we also tested helpfulness on AlpacaEval . Results in Table 5 show that TAIA significantly reduces the attack success rate after tuning on three levels of red-teaming data while gaining higher helpfulness from the benign dataset. This demonstrates that careful parameter selection can distill out unsafe parameters and enhance LLM robustness.

Response to RQ3: TAIA succeeds in varying data sizes.To validate the efficacy of TAIA across different sizes of fine-tuning datasets, we sampled the CoT-Collection dataset to create six fine-tuning corpora of varying sizes: [1K, 10K, 50K, 100K, 200K, 1.8M]. We used the same experimental settings as described in SS4. The results, shown in Figure 3(a), indicate that TAIA achieves higher performance more quickly and with less data, demonstrating a more efficient utilization of OOD data. Additionally, unlike vanilla fine-tuning, which experiences significant performance drops when

    &  &  &  &  &  &  &  &  & **Avg.** \\   & Base Model & 4.28 & 16.80 & 58.39 & 32.41 & 27.90 & 33.78 & 43.62 & 31.03 \\  & Vanilla & 6.88 & 14.51 & 59.21 & 20.28 & 34.30 & 27.65 & 23.36 & 26.60 \\  & TAIA & 8.22 & 15.56 & 60.61 & 25.65 & 39.00 & 28.20 & 25.65 & 28.98 \\   & Base Model & 20.30 & 30.76 & 78.30 & 42.70 & 54.90 & 45.09 & 57.69 & 47.11 \\  & Vanilla & 9.34 & 23.85 & 71.66 & 21.04 & 57.90 & 27.57 & 24.44 & 33.69 \\   & TAIA & 14.60 & 27.32 & 72.65 & 33.79 & 64.50 & 40.39 & 35.90 & 41.31 \\   

Table 4: The application of TAIA on full fine-tuning technique trained on CoT-Collection. It still surpasses the vanilla fine-tuning method but lags behind the base LLM.

    &  &  \\   & & **Explicitly harmful.**\(\) & **Identity Shifting.**\(\) & **Benign.**\(\) & **AlpacaEval\(\)** \\   & – & 0.00 & 0.00 & 0.00 & 7.66 \\  & Vanilla & 84.59 & 93.27 & 4.04 & 7.46 \\   & TAIA & **8.27** & **30.77** & **0.38** & **9.94** \\   

Table 5: Comparison of TAIA with vanilla fine-tuning on red-teaming resistance. When jailbreaking LLMs on harmful datasets, TAIA harvests lower attack success rates than vanilla fine-tuning on both harmful and benign datasets, showing its strong generalization in distilling out harmful features.

trained on a 1K dataset, TAIA is minimally affected by the distribution gap between its internal distribution and that of the 1,000 samples. This demonstrates the high robustness and generalization capability of TAIA. The full results are detailed in Table 14.

Response to RQ4: TAIA fully restores the few-shot capability of the base LLM and even improves the performance.Brown et al.  demonstrate the generalization of LLMs as they can adapt to new tasks with few-shot in-context learning. As LLMs are incapable of few-shot learning after SFT on a specific dataset , we want to verify whether TAIA can maintain the superb few-shot learning ability.We evaluate the few-shot adaptation of TAIA using the same checkpoint trained with CoT-collection and test it in a 100 subset sampled from MATH . Results in Figure 3(b) show that the vanilla fine-tuning method has lost its few-shot learning capability. In contrast, TAIA has regained the ability to learn contextually as the base LLM, achieving performance leap from demonstrations in an approximately linear manner.

Response to RQ5: TAIA increases the representation rank of self-attention.Dong et al.  denote that the residual rank of the representation highly correlates to the final performance of Transformer models. The residual rank of any hidden state \(^{n d}\) can be obtained by \(\|()\|_{1,}=\|-\|_{1, }\), where \(^{d}\) is the averaged representation and \(\|\|_{1,}=\|_{1}\|\|_{}}\). To quantify this metric human-friendly, we recompute the ratio between the residual rank and the original rank of \(\) after each attention module as \(\|(_{l})\|_{1,}/\|_{l}\|_{1,}\), where \(l=0,1,,L\). The comparisons between the vanilla method, TAIA and TOA trained and evaluated on MATH, are shown in Figure 3(c). We notice a negligible difference between the baseline and TOA, indicating that simply training the self-attention modules does not affect dealing with OOD data. Notably, TAIA significantly increases the residual rank of activations of self-attention modules across all layers, which promises high expressiveness and generality.

## 6 Conclusion

We revisit the intrinsic properties of LLM fine-tuning and determine that supervised fine-tuning poses minimal requirements for updated FFN parameters. Building on this insight, we introduce TAIA, an inference-time intervention strategy designed to address data scarcity challenges in real-world applications of LLMs. TAIA adheres to the traditional fine-tuning technique but only retains updated self-attention parameters for inference. This approach demonstrates superior generalization ability across various contexts. The high generality of TAIA enables effective training using a mixture of limited in-domain data and extensive OOD data. This method enhances LLM performance on downstream tasks while filtering out undesired information from the fine-tuning set, such as hallucination interference or the decline of few-shot capability.

Figure 4: (a)Average performance with different sizes of fine-tuning datasets; (b) The few-shot performance on MATH; (c) The layer-wise residual rank of the hidden states on MATH.