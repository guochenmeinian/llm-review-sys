# DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs

Haokun Lin\({}^{1,3,4}\), Haobo Xu\({}^{2}\), Yichen Wu\({}^{*}\)\({}^{4}\), Jingzhi Cui\({}^{2}\), Yingtao Zhang\({}^{2}\),

**Linzhan Mou\({}^{5}\), Linqi Song\({}^{4}\), Zhenan Sun\({}^{1,3}\), Ying Wei\({}^{4,5}\)**

\({}^{1}\) School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{2}\) Tsinghua University \({}^{3}\) NLPR & MAIS, Institute of Automation, CAS

\({}^{4}\) City University of Hong Kong \({}^{5}\) Zhejiang University

haokun.lin@cripac.ia.ac.cn xuhb20@mails.tsinghua.edu.cn wuyichen.am97@gmail.com znsun@nlpr.ia.ac.cn ying.wei@zju.edu.cn

Equal contribution.Corresponding authors.

###### Abstract

Quantization of large language models (LLMs) faces significant challenges, particularly due to the presence of outlier activations that impede efficient low-bit representation. Traditional approaches predominantly address _Normal Outliers_, which are activations across all tokens with relatively large magnitudes. However, these methods struggle with smoothing _Massive Outliers_ that display significantly larger values, which leads to significant performance degradation in low-bit quantization. In this paper, we introduce DuQuant, a novel approach that utilizes rotation and permutation transformations to more effectively mitigate both massive and normal outliers. First, DuQuant starts by constructing the rotation matrix, using specific outlier dimensions as prior knowledge, to redistribute outliers to adjacent channels by block-wise rotation. Second, We further employ a zigzag permutation to balance the distribution of outliers across blocks, thereby reducing block-wise variance. A subsequent rotation further smooths the activation landscape, enhancing model performance. DuQuant simplifies the quantization process and excels in managing outliers, outperforming the state-of-the-art baselines across various sizes and types of LLMs on multiple tasks, even with 4-bit weight-activation quantization. Our code is available at https://github.com/Hsu1023/DuQuant.

## 1 Introduction

Large language models (LLMs) [51; 7; 50] have demonstrated exceptional performance across a wide range of natural language processing tasks. However, their billions of parameters present considerable deployment challenges on resource-constrained edge devices, particularly in terms of memory usage and inference speed [22; 15; 56]. In response to these challenges, network quantization methods [20; 23] have been extensively explored to minimize memory usage by converting floating-point parameters into low-bit formats [18; 32; 8], and to expedite inference by quantizing both activations and weights for accelerating the matrix multiplication process [64; 34; 74].

Among LLM quantization methods, a primary issue is the presence of activation outliers, which enlarge the quantization step sizes and subsequently cause significant accuracy loss . To mitigate this problem, current research has developed various methods to address **Normal Outliers** in activations, which are _persistent in several channels across all tokens_[13; 64]. However, besides Normal Outliers, there exists another type of activation outlier [48; 35], termed **Massive Outliers**.

These outliers are characterized by their _exceedingly high values and limited occurrence in a subset of tokens_, as depicted in Figure 1(b). Unfortunately, existing LLM quantization methods struggle to effectively address these Massive Outliers. For instance, SmoothQuant , despite using a smooth factor to shift some of the activation outliers to the weight part, still cannot effectively handle Massive Outliers with extremely large values, as shown in Figure 1(c)(d). OmniQuant  and AffineQuant , on the other hand, exhibit training instability issues  due to the presence of Massive Outliers. Consequently, there is a pressing need for an LLM quantization approach that effectively addresses both Normal and Massive Outliers.

To tackle this challenge, we propose the **Dual** transformations **Quanti**zation (DuQuant) method. Our motivation is to **redistribute the activation outlier values across different channels**, facilitating easier quantization. Specifically, we construct the orthogonal rotation matrix and the orthogonal permutation matrix. By multiplying these matrices with the activations, we can effectively perform column transformations on the activations, which in turn allows for the redistribution of outliers. For the **rotation transformation** aspect, we first identify specific dimensions of outliers as the prior knowledge and employ a greedy algorithm to construct the rotation matrix. To enhance the multiplication efficiency, we utilize diagonal block-wise rotation matrices, with each matrix responsible for a small portion of the activations. However, this approach may result in uneven outlier magnitudes across different blocks. Therefore, we propose the **zigzag permutation** for reordering the activation channels, which promotes a more uniform distribution across different blocks. Concretely, we distribute the channels with the highest activations across the blocks in a back-and-forth pattern. After establishing blocks with uniformly distributed outlier magnitudes, we employ another rotation transformation to further redistribute the outliers within each block. Note that we multiply the weight matrix with the transpose of the rotation and permutation matrices at the same time, preserving the linear layer equivalence and smoothing weights. Theoretical analysis confirms that the rotation and permutation transformations greatly mitigate quantization challenges induced by outliers.

As a result, DuQuant offers several clear advantages over QuaRot : (1) DuQuant's optimal rotation matrix, derived through a greedy search guided by prior knowledge, surpasses QuaRot's Hadamard rotation in managing outliers; (2) our unique zigzag permutation significantly reduces activation variance across blocks, providing a distinct advantage for handling massive outliers; and (3) by jointly smoothing weights and activations, DuQuant avoids time-consuming GPTQ algorithm in QuaRot. Extensive evaluations demonstrate that our DuQuant approach significantly outperforms existing 4-bit weight-activation quantization baselines across various benchmarks. Notably, DuQuant achieves a 5% improvement in Commonsense QA tasks across all LLaMA model sizes and a 10% increase in zero-shot MMLU benchmarks for the Vicuna-v1.5-13B. Moreover, in practical applications with the LLaMA2-7B model, DuQuant not only accelerates pre-filling phase by up to 2.08\(\) but also reduces memory usage during decoding phase by 3.50\(\), with minimal impact on performance: only a 0.61 increase in perplexity and a 2.71% drop in accuracy compared to the FP16 model. These results highlight the effectiveness of DuQuant in enhancing the efficiency and capacity of quantized LLMs.

## 2 Motivation

Normal Outliers and Massive Outliers.Previous works [13; 72; 32] have highlighted the challenge posed by activation outliers in LLMs for model compression. These outlier features consistently

Figure 1: Visualizations of Outliers in LLaMA2-7B. (a) Input activation of Layer1 attention key projection shows Normal Outliers with relatively high magnitudes across all token sequences. (b) Input activation of Layer1 FFN down projection reveals Massive Outliers, presenting extremely high magnitudes (around 1400) at very few tokens. (c) Application of SmoothQuant on FFN down projection, illustrating its struggle with massive outliers in the Activation matrix. (d) Corresponding weight changes with SmoothQuant, highlighting the emergence of new outliers.

manifest large values across specific feature dimensions and are present in all token sequences , which we refer to as _Normal Outliers_. Recently, a distinct type of outlier [48; 35], termed _Massive Outliers_, has been observed in LLMs. The primary distinctions between normal and massive outliers are: 1) Normal outliers persist across all token sequences, whereas massive outliers are confined to a limited number of tokens. 2) Massive outliers exhibit significantly larger magnitudes, often surpassing 100 and being approximately 1000 times greater than the median of other activations . In our study, we delve deeper into the impact of these two distinct types of outliers on quantization.

Massive Outliers Exist at the Second Linear Layer of FFN Module.In contrast to previous studies [48; 35] that observe massive outliers at the output of Transformer blocks, we **first** discover that these extremely large activations exist at the input of the down-projection layer within the FFN module. As depicted in Figure 1, the input of the down-projection layer in the LLMa2-7B model Layer 1 contains a single activation of significant magnitude (approximately 1400). This activation is isolated to one token and therefore classified as one of massive activations. This phenomenon is consistently observed across different layers and sizes of models, as illustrated in Appendix I.

Massive Outliers Enlarge Quantization Difficulty.Although previous studies [64; 47; 39; 1] have proposed various approaches to eliminate outlier features, they still face challenges in effectively managing massive outliers. SmoothQuant , for instance, attempts to shift the quantization difficulty from activations to weights by dividing the activation by a per-channel smoothing factor and multiplying it to the weight matrix. Nevertheless, we observe that this transfer at the input of the down-projection layer can cause the weights of the down-projection to display noticeable outliers, as demonstrated in Figure 1. This issue arises because massive outliers cause the smoothing factor to become significantly large. Moreover, extremely large outliers can lead optimization-based methods to encounter problems with loss explosion. Both OmniQuant  and AffineQuant  have had to exclude their learnable parameters for the down projection layer due to unstable gradients. Given the poor accuracy observed with 4-bit quantization, QUIK  opts to use INT8 quantization for the down projection layer and Atom  applies INT8 quantization for 128 outlier channels. Consequently, massive outliers introduce new challenges to the quantization process that existing methods cannot fully address. This observation has motivated us to develop rotation and permutation transformations, which effectively handles both massive and normal outliers and achieves state-of-the-art performance.

## 3 Method

In this section, we delve into the distribution of outliers and introduce our proposed DuQuant method. The DuQuant method is built on two key components: 1) the block-diagonal rotation matrix, tasked with the local redistribution of feature outliers, and 2) the zigzag permutation, responsible for the global reordering of outliers across different blocks.

### Preliminaries

As the common modules within each transformer block of LLMs, both Multi-head Self-Attention (MSA) and Feed-Forward Network (FFN) fundamentally consist of basic linear layers, which can be represented as, \(=^{T C_{out}}\). Here, \(^{T C_{in}}\) is the activation input and \(^{C_{in} C_{out}}\) denotes the weight matrix. In this paper, we focus on integer uniform quantization  of both activation and weight, aiming to achieve better hardware support. Specifically, the \(b\)-bit quantization process maps the FP16 tensor \(\) to low-bit integer \(_{q}\):

\[_{q}=(}{} +z,0,2^{b}-1),=)-()}{2^{b}-1},z=- )}{}.\] (1)

The notation \(\) means the nearest rounding operation, \(\) is the quantization step size and \(z\) represents the zero point. Following [64; 47; 34; 39], we employ per-token quantization for activation and per-channel quantization for weight, which entails assigning different step sizes to individual tokens of activations (\(_{}^{T 1}\)) and different output channels of weights (\(_{}^{1 C_{out}}\)).

### The proposed DuQuant Method

To address the Normal Outliers issue stated in Section 2, current quantization methods, such as SmoothQuant  and OmniQuant , usually adopt the smooth technique. Concretely, it involves the utilization of a per-channel smoothing diagonal matrix, denoted as \(\), to scale the input activation and weight matrix. The adjustment allows us to rewrite the original linear layer as \(^{-1})()\). The diagonal element \(_{j}\) within \(\) is computed as \(_{j}=(|_{j}|)^{}/(|_{j}|)^{1-}\), where \(\) is a hyper-parameter representing the migration strength. However, despite the ability of this smoothing technique to shift the quantization challenge from activations to weights, it still faces difficulties in effectively managing Massive Outliers, as depicted in Figure 1. This challenge stems from the extremely large massive outliers inducing large scaling factors \(_{j}\), which in turn introduce new outliers in the weight matrix and result in significant performance declines in 4-bit quantization.

According to these findings, we propose the DuQuant method, which includes the Rotation and Permutation transformations based on the smooth technique. By combining rotation transformation and channel permutation, our DuQuant method aims to redistribute these features within the activation space, thereby mitigating the effects of both Normal and Massive Outliers.

The Rotation Transformation.In contrast to the smooth technique, our aim is to apply a rotation matrix for row or column transformations, mitigating the impact of both Normal and Massive outliers. The ideal rotation matrix, denoted as \(\), should possess the following properties: 1) \(\) is an orthogonal matrix satisfying \(^{}=\) and \(||= 1\). This allows us to reformulate the linear layer within the transformer as \(==()(^{} )\); 2) \(\) should be capable of effectively target the positions of outliers and effectively mitigating them through matrix multiplication. However, due to the Massive Outliers are usually randomly distributed within the activation space, it is challenging to directly identify the optimal rotation matrix \(\) capable of mitigating outliers through a single rotation transformation. To address this problem, we employ a greedy search with prior knowledge to compute a rotation matrix \(}\), thereby approximating the ideal rotation matrix \(\). Specifically, the calculation of \(}\) involves the following steps,

* Identify the feature dimension \(d^{(1)}\) where the outlier are primarily concentrated, i.e., \(d^{(1)}=_{j}(_{i}|_{ij}|)\). Here, \(_{ij}\) represents the element in the \(i\)-th row and \(j\)-th column of \(\).
* Based on the searched dimensions \(d^{(1)}\), we construct the rotation matrix as follows, \[^{1}=_{d^{(1)}}}_{d^{( 1)}},=1&\\ &^{}.\] (2) Here, \(_{d^{(1)}}\) is the switching matrix used to swap the first and the \(d^{(1)}\)-th columns of the activation, and \(}\) represents an orthogonal initialized rotation matrix, in which the first row is specifically uniformly distributed. The motivation behind this is to mitigate outliers in the first column after the transformation by \(_{d^{(1)}}\). To further increase the randomness of the rotation operation,

Figure 2: Transformation Steps for Activation Matrices after smooth technique. (a) Sequential transformations on Normal Outliers: 1 initial rotation to reduce outliers within blocks, 2 permutation to evenly distribute outliers across blocks, and 3 a second rotation for further smoothing. (b) Activation changes for Massive Outliers before and after DuQuant. (c) A sample matrix for highlighting the continual reduction of outliers through rotation and permutation, with outliers marked in dark blue.

we retain the first column, where outliers have been mitigated, and randomly rotate the other columns by multiplying them with a random orthogonal matrix \(^{}\).
* Let \(N\) denote the greedy search steps, then the approximated rotation matrix \(}=^{1}^{2}^{n}\), where \(n=_{k[1:N]}(_{i,j}|(^{1}^{k})_{ ij}|)\). Each \(^{i}\) is constructed according to Eqn. (2) and the identified feature dimension \(d^{(i)}\). Appendix G provides detailed pseudo code.

Through this construction manner, we can ensure that the approximated optimal rotation matrix \(}\) can effectively mitigate outliers with large magnitudes, as opposed to merely using a randomly selected orthogonal rotation matrix. Nevertheless, directly constructing the entire rotation matrix is time-consuming and results in substantial memory overhead. For fast matrix multiplication, following , we approximate the rotation matrix \(}^{C_{in} C_{in}}\) in a block-wise manner,

\[}=(}_{b_{1}},...,}_{b_{K}}),\] (3)

where \(}_{b_{i}}^{2^{n} 2^{n}}\) denotes a square matrix of the \(i\)-th block, which is constructed following the three steps mentioned above. And the block numbers \(K\) is calculated by \(K=C_{in}/2^{n}\).

The Permutation Transformation.Despite adopting the block-diagonal rotation matrix \(}\) for its time and storage efficiency, its focus on local information introduces a potential limitation in further reducing the outliers. This is because the rotation transformation, conducted within each small block, cannot integrate the information across different blocks to further minimize outliers. Consequently, one block may have relatively larger outliers while another block has smaller outliers, resulting in high variance among different blocks, as shown in Figure 2. This limitation explains that merely utilizing the block-diagonal rotation matrix is insufficient to effectively reduce the overall outliers.

To effectively mitigate the overall outliers, it is essential to balance the outliers' magnitudes among various blocks. Specifically, within each small block, we denote the largest outlier in dimension \(d_{j}\) as \(O_{j}\). Meanwhile, \(M_{b_{i}}\) represents the mean value of all \(O_{j}\) in the \(i\)-th block, where \(i=1,2,...,K\). Then the variance in activation magnitudes across various blocks can be expressed as,

\[([M_{b_{1}},M_{b_{2}},...,M_{b_{K}}]).\] (4)

To minimize this variance and further reduce the overall outliers, we introduce the **zigzag permutation**. Concretely, we generate a zigzag sequence that starts by assigning channels with the highest activations to the first block. The process continues by assigning channels with the next highest activations to the subsequent blocks in descending order until the end of block \(K\). Upon reaching the final block, the order reverses, starting from the channel with the next highest activation and proceeding in ascending order. This back-and-forth patterning continues throughout all the blocks, ensuring that no single block consistently receives either the highest or lowest activation channels. It is worth noting that the constructed permutation is an orthogonal matrix, which we denote as \(\), satisfying the conditions \(^{}=\) and \(||= 1\). By employing the zigzag permutation, we achieve a balanced distribution of outliers across different blocks. This allows us to use an additional rotation transformation to further smooth the outliers. Figure 2 provides an illustration of outlier mitigation.

The Overall DuQuant Method.To effectively mitigate both Normal and Massive Outliers, we first employ the smooth technique to shift the quantization challenge from activations to weights. Next, we introduce the block-diagonal rotation matrix \(}\) to locally redistribute feature outliers within the activation space. We then propose the zigzag permutation matrix for globally balancing the outliers across different blocks, followed by another application of the block-diagonal rotation transformation. To sum up, the linear layers within the transformer can be rewrite as,

\[==[( ^{-1}}_{})}_{(1)} }_{(2)}][}_{(2)}^{}^{} }_{(1)}^{}(}_{^{-1}})],\] (5)

where the notation \(\) denotes the orthogonal permutation matrix learned via the zigzag manner, the \(}_{(1)}\) and \(}_{(2)}\) represent the first and second block-diagonal rotation matrix, respectively.

**Remark 1**.: It is worth noting that the proposed DuQuant method can simultaneously smooth the weight matrix. While the commonly adopted smooth technique is effective, it can cause the weight matrix of the down-projection layer to exhibit pronounced outliers, leading to performance degradation. However, in the proposed DuQuant method, the rotation transformation we designed is applied to not only the activation input but also the weight matrix. As a result, the outliers induced by the smooth technique can be mitigated through our approximated rotation matrix \(}\), yielding a smoother, more quantization-friendly weight matrix. Moreover, this approach eliminates the reliance on complex weight quantization techniques, such as GPTQ  used in Atom  and QuaRot .

**Remark 2**.: To further decrease the computation and memory costs, we initially construct the \(k\)-th block rotation matrix \(}_{b_{k}}\), with the \(k\)-th block containing the largest outlier. We then assign \(}_{b_{i}}=}_{b_{k}}\) for all \(1 i K\). This strategy not only effectively mitigates the impact of outliers, but also reduces the number of block rotation matrices from \(K\) to 1, significantly reducing computation and memory requirements. Importantly, incorporating the invertible matrix \(\) from Eqn. (5) significantly eases the quantization challenges for \(\) and \(\). Consequently, the quantization process acts as \(=()(^{-1})=} }_{}}_{ }}(}_{q}-z_{}})(}_{q}-z_{}})\).

### Theoretical Analysis

To further demonstrate the effectiveness of the proposed DuQuant method, we conduct a theoretical analysis of the rotation and permutation transformations. Theorem 1 shows that within each block, the constructed rotation matrix effectively mitigates the maximum outlier, thereby reducing the outlier magnitude through a greedy search. Theorem 2 reveals that the employed zigzag permutation ensures a balanced upper bound shared among different blocks. This suggests that the zigzag permutation effectively reduces the variance shown in Eqn. (4) and thus assists the rotation matrix in further decreasing the outliers. Please refer to Appendix B for detailed proofs.

**Theorem 1** (Rotation).: _For the activation input \(^{T C_{in}}\), \(}^{2^{n} 2^{n}}\) is a diagonal block matrix constructed as per Eqn. (3). For a specific block \(b_{i}\), let \(O_{j}()\) represent the maximum outlier of the \(j\)-th dimension \(d_{j}\) within the input. Then, we can deduce that,_

\[_{1 j 2^{n}}\ O_{j}(_{b_{i}}}_{b_{i}}) _{1 j 2^{n}}\ O_{j}(_{b_{i}}).\] (6)

**Theorem 2** (Zigzag Permutation).: _For the activation input \(^{T C_{in}}\), it can be divided into \(K\) blocks, where \(K=C_{in}/2^{n}\). Let \(O_{j}\) denote the max outlier of the dimension \(d_{j}\) in \(\), the reordered outliers from large to small is expressed as \(O^{(1)},O^{(2)},...,O^{(C_{in})}\). Moreover, the \(M_{b_{i}}\) represents the mean value of all \(O_{j}\) in the \(i\)-th block, \(i=1,2,...,K\). Let \(:=\{|O^{(i+1)}-O^{(i)}|\},i=1,2,...,C_{in}\)-\(1\). Then, following the zigzag permutation described in Section 3.2, the mean value \(M_{b_{i}}\) within each \(i\)-th block consistently satisfies,_

\[M_{b_{i}} O^{(1)}+K-1)(2^{n-1}-1)}{2^{n}}, i=1,2, 3,...,K.\] (7)

## 4 Experiment

Models and Evaluations.We apply our DuQuant on pre-trained LLMs: LLaMA (7B-65B) , LLaMA2 (7B-70B) , LLaMA3 (8B, 70B), Mistral, Phi2 and instruction-tuned LLMs: Vicuna-v1.5 (7B-13B) . We evaluate quantized pre-trained LLMs on language generation tasks and commonsense QA tasks. Specifically, we assess the perplexity on WikiText2  and C4  datasets, as well as the zero-shot accuracy on PIQA , ARC , BoolQ , HellaSwag , and WinoGrande  datasets. Moreover, we evaluate quantized Vicuna models on MMLU  and MT-Bench  benchmarks, as well as their long-form generative capabilities on LongBench .

Implementation Details.In line with prior studies , we apply per-token activation quantization and per-channel weight quantization. Given that W8A8 quantization has been established as lossless in precision by SmoothQuant , our primary evaluation in this paper focuses on 4-bit and 6-bit quantization for weights and activations. As for details, we quantize all intermediate activations, excluding the SoftMax output. Moreover, we have developed two types of quantized models, denoted as **DuQuant** and **DuQuant\(\_\)1**hvc**. For DuQuant, we employ round-to-nearest quantization, using a clipping ratio of 0.9 for activations and 0.8 for weights. To improve weight matrix quantization, DuQuant\(\_\)1**hvc** integrates the learnable weight clipping (LWC) technique from Omni-Quant. Concretely, LWC adjusts weights by training parameters \(,\) to compute step size \(=)-()}{2^{k}-1}\) in Eqn. (1). Notably, the smoothing diagonal matrix and the learned weight clipping factor can be integrated into the quantized weights, introducing no additional computational or memory costs. More details and hyperparameters are left in Appendix C.

Baselines.We compare with state-of-the-art (SOTA) weight-activation PTQ methods, including SmoothQuant , Outlier Supression+ , OmniQuant , QLLM , AffineQuant , and Atom . For Atom, we reproduce the results with no group-wise asymmetric quantization.

[MISSING_PAGE_EMPTY:7]

Appendix D. Table 1 indicates that our DuQuant quantized models notably outperform other baselines on both the WikiText2 and C4 datasets. Notably, LWC technique further enhances model capacity, with our DuQuant\({}_{t}\) achieving comparable performance with FP16 models. Table 2 and Table D1 showcase the zero-shot accuracy of W4A4 quantization on Commonsense QA tasks, where DuQuant significantly improves the average accuracy. Our method surpasses QLLM by +9%, and Atom by +5% for all model sizes. These results demonstrate the superiority of our rotation and permutation transformation, which establishes new SOTA performance by effectively eliminating outlier features.

Quantization of Instruction-tuned Models.We quantize Vicuna-v1.5  models to assess the generalizability of our DuQuant. Table 3 illustrates that our quantized models surpass the baselines across all task categories on MMLU benchmark. For Vicuna-13B, our DuQuant\({}_{t}\) surpasses Atom by 10.01% under zero-shot settings and 6.95% under five-shot settings. Moreover, we compare our DuQuant with Atom and OmniQuant using MT-Bench and utilize GPT-4 to evaluate the answers from quantized models. As shown in Figure 3, DuQuant quantized models significantly outperform both Atom and OmniQuant in win rates. Specifically, for Vicuna-7B, DuQuant only lost 16 and 1 times to Atom and OmniQuant, respectively, while achieving 68 and 155 wins against them.

Evaluation of Long-context Generation.To further evaluate the long-text generative capabilities, we follow  and conduct a comprehensive comparison of DuQuant against state-of-the-art baselines on the LongBench , which includes a variety of generative tasks to provide a broader evaluation. We set the maximum sequence length to 3500 for Vicuna models, with results presented in Table 4. DuQuant achieves performance comparable to FP16 models, demonstrating the effectiveness of our dual transformations. More detailed results on different subtasks are listed in Table D3, D4.

Quantization of LLaMA3 Models.LLaMA3, known for its superior performance in various tasks, faces significant degradation in low-bit quantization . To address this, we apply our DuQuant to quantize LLaMA3-8B. Table 5 displays the perplexity and zero-shot accuracy results. Notably, under W6A6 setting, our DuQuant achieves performance comparable to FP16 model. Furthermore, unlike other methods that show weaker results under W4A4 setting, our DuQuant maintains competitive performance, indicating its robustness with LLaMA3. We attribute this success to the advanced handling of outliers achieved through dual transformations, which is not restricted to specific models.

    &  &  &  &  &  &  &  &  &  &  \\   & FP16 & 23.27 & 21.07 & 26.91 & 66.00 & 82.59 & 41.06 & 25.53 & 48.23 & 41.83 \\  & SmoothQuant & 4.11 & 2.00 & 6.05 & 15.00 & 1.62 & 1.55 & 4.24 & 25.92 & 7.56 \\  & OmniQuant & 1.62 & 3.93 & 2.64 & 1.00 & 81.01 & 6.61 & 1.87 & 14.97 & 3.43 \\  & Atom & 1.97 & 20.24 & 24.60 & 58.00 & 67.20 & 37.94 & 19.41 & 29.34 & 34.34 \\  & **DuQuant** & **19.98** & **21.15** & **25.85** & **64.00** & **78.91** & **42.24** & **23.15** & **47.66** & **40.37** \\   & FP16 & 2.44 & 21.24 & 26.53 & 68.00 & 88.11 & 41.97 & 27.57 & 43.08 & 42.45 \\  & SmoothQuant & 2.18 & 2.95 & 3.54 & 1.50 & 1.83 & 0.35 & 6.71 & 11.57 & 3.83 \\   & OmniQuant & 6.68 & 1.78 & 2.83 & 9.00 & 1.13 & 0.45 & 18.33 & 8.46 & 4.77 \\   & Atom & 17.67 & 20.23 & 23.39 & 59.00 & 80.75 & 38.72 & 21.79 & 37.31 & 37.36 \\   & **DuQuant** & **18.93** & **20.72** & **26.59** & **66.50** & **83.04** & **42.67** & **26.02** & **38.09** & **40.32** \\   

Table 4: Long-context generation results for 4-bit Vicuna models on the LongBench benchmark.

    &  &  &  &  &  &  &  &  &  &  &  \\   HeLaMA3-

[MISSING_PAGE_FAIL:9]

performance compared with "Perm 0" (no permutation), while incurring an additional 8.9% computational cost on LLaMA2-7B and 9.3% on LLaMA2-13B compared to the W4A4 setup. Considering the approximately \(2\) speedup and the impressive performance, these additional costs are deemed acceptable. Further permutations, like "Perm 2," do not improve performance and reduce inference efficiency. Consequently, "Perm 1" strikes the best balance between perplexity and inference speed, making it the optimal configuration for DuQuant.

Inference Speedup.To assess the inference speedup delivered by our DuQuant, we adopt the measurement strategy and W4A4 kernel from . We evaluate the layer-wise speedup of LLaMA2 models on one NVIDIA X090 GPU, with results detailed in Table 9 and 10. We set the pre-filling sequence length at 2048 and decode for 128 steps. In the pre-filling stage, DuQuant achieves a \(2.08\) speedup over FP16 for LLaMA2-7B and a \(2.34\) speedup for LLaMA2-13B, with slight variations across different batch sizes. In the decoding stage, batching the token generation phase yields high throughput without any downside . Consequently, we enlarge the batch size to 64 and the results for LLaMA2-7B in Table 10 prove DuQuant achieves speedup comparable to QuaRot. More detailed analyses and end-to-end speedup are available in Appendix E.1.

Memory Consumption.We measure the peak memory usage of DuQuant with the W4A4 kernel on LLaMA2-7B using a single NVIDIA 3090 GPU. We process 2048 tokens for pre-filing and run 128 decoding steps, with the results listed in Table 11. In the pre-filling stage, DuQuant, SmoothQuant, and QuaRot achieve up to 3.2\(\) memory reduction, while QLLM performs worse. In the decoding stage, DuQuant maintains strong memory efficiency, with superior performance.

Runtime.Our DuQuant stands out for its efficiency, surpassing other baselines . The block-wise rotation ensures fast multiplication between the rotation and activation matrices. Zigzag permutation, involving simple channel swaps, is much faster than complex algorithms like Simulated Annealing, as discussed in E.3. Moreover, the advanced management of outliers makes DuQuant not rely on GPTQ or gradient-based training. Hence, DuQuant enables a rapid quantization process shown in Table F24, e.g., we successfully quantize LLaMA2-13B in just 71s with superior performance.

## 5 Conclusion

In conclusion, this paper presents DuQuant, an innovative quantization strategy for large language models (LLMs) that effectively addresses the challenge of outlier activations. By integrating rotation and permutation transformations, DuQuant effectively mitigates the impacts of both massive and normal outliers. This strategic redistribution of outliers not only simplifies the quantization process but also leads to substantial improvements in model performance. Consequently, DuQuant establishes new state-of-the-art results in 4-bit weight-activation quantization scenarios. This advancement enhances the deployment of efficient LLMs in resource-constrained environments.

  
**INT4, BS=64** & **Speedup** & **LLaMA2-7B** & **pre-filling (GB)** & **Saving** & **Decoding (GB)** & **Saving** \\  FP16 & - & 15.282 & - & 13.638 & - \\ SmoothQuant & 1.508\(\) & SmoothQuant & 4.782 & 3.196\(\) & 3.890 & 3.506\(\) \\ QLLM & 5.349 & 2.857\(\) & 3.894 & 3.502\(\) \\ QuaRot & 4.784 & 3.194\(\) & 3.891 & 3.505\(\) \\ DuQuant & 4.786 & 3.193\(\) & 3.893 & 3.503\(\) \\   

Table 10: Decoding stage.

  
**Model** & **Batch Size** & **Speedup** & & & \\   & 1 & \(1.95\) & & & \\  & 4 & \(2.03\) & & & \\  & 16 & \(2.08\) & & & \\   & 1 & \(2.15\) & & & \\  & 4 & \(2.30\) & & & \\  & 16 & \(2.34\) & & & \\  

Table 9: Layer-wise speedup during pre-filling stage for 4-bit weight-activation quantization.

  
**Model** & **Batch Size** & **Speedup** & & \\   & 1 & \(1.95\) & & \\  & 4 & \(2.03\) & & \\  & 16 & \(2.08\) & & \\   & 1 & \(2.15\) & & \\  & 4 & \(2.30\) & & \\  & 16 & \(2.34\) & & \\   

Table 9: Layer-wise speedup during pre-filling stage for 4-bit weight-activation quantization.