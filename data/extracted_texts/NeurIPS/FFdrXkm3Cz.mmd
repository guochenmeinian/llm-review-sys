# On the spectral bias of two-layer linear networks

Aditya Varre

EPFL

aditya.varre@epfl.ch &Maria-Luiza Vladarean

EPFL

maria-luiza.vladarean@epfl.ch &Loucas Pillaud-Vivien

Courant Institute of Mathematics, NYU / Flatiron Institute

lpiillaudvivien@flatironinstitute.org &Nicolas Flammarion

EPFL

nicolas.flammarion@epfl.ch

###### Abstract

This paper studies the behaviour of two-layer fully connected networks with linear activations trained with gradient flow on the square loss. We show how the optimization process carries an _implicit bias_ on the _parameters_ that depends on the scale of its initialization. The main result of the paper is a variational characterization of the loss minimizers retrieved by the gradient flow for a specific initialization shape. This characterization reveals that, in the small scale initialization regime, the linear neural network's _hidden layer_ is biased toward having a low-rank structure. To complement our results, we showcase a hidden mirror flow that tracks the dynamics of the singular values of the weights matrices and describe their time evolution. We support our findings with numerical experiments illustrating the phenomena.

## 1 Introduction

The most forceful driver of advancements in the field of Machine Learning over the past decades has been the success of deep neural networks. Amongst the striking qualities of these models is the fact that, despite being heavily overparametrized, their optimization consistently yields minima with good generalization properties. A beckoning research direction is thus to unravel the process through which neural networks learn internal representations for a given task (Bengio et al., 2013). Understanding such phenomena is crucial for lowering the interpretability barrier of these models and developing a principled approach to their training and deployment in practice.

Recent experimental evidence identified one of the likely paths towards achieving these goals as the study of the inherent regularization properties (or implicit biases) of training algorithms (Neyshabur et al., 2014; Zhang et al., 2016). These observations laid the foundation for a new line of work (see, e.g., Vardi, 2022) whose driving question is which minimum, amongst the many, awaits at the tail end of optimization.

One of the determining factors for the implicit bias of gradient methods is the initialization scale, which controls their operational regime as shown by empirical studies (Chizat et al., 2019). More precisely, gradient descent with a low-scale initialization is capable of learning rich feature representations from the data. Strikingly, despite overparameterization, the hidden-layer neurons align in the direction of the features (Chizat et al., 2019; Atanasov et al., 2022) and learning of representations reflects in the low-rank structure of the hidden layers. Our work aims to precisely explain this phenomenon and quantify the impact of the initialization scale on feature learning.

Unfortunately, studying such phenomena for the types of neural networks used in practice is mathematically challenging at present due to the non-linearity of their activations. Their less expressive _linear_ counterparts, however, are more tractable and represent a good proxy due to their non-convexloss landscape and non-linear learning dynamics. Consequently, the study of deep linear networks has received significant amounts of attention over the past years, and spans several important directions, including convergence (Arora et al., 2019, 2018; Min et al., 2021), learning dynamics (Saxe et al., 2014; Braun et al., 2022) and the implicit bias of optimization algorithms (Azulay et al., 2021). This work complements these previous approaches by mathematically describing the properties of their parameters at convergence, highlighting the implicit bias phenomenon, and further analyzing the evolution of weight matrices throughout the optimization process.

Specifically, this paper studies overparameterized vector regression problems on two-layer fully-connected linear neural networks. We show the following results when the network is trained with gradient flow (GF).

1. In Theorem 3.1, we prove that the zero-loss solutions retrieved by the gradient flow are the minimizers of a potential that depends on the initialization scale. Additionally, we provide explicit expressions for the singular values of the hidden layer weights, also as a function of the initialization scale. These characterizations reveal how low-magnitude initialization induces a low-rank structure of the hidden layer.
2. In Theorem 3.2, we show that gradient flow on the parameters induces a mirror flow on the singular values. In the specific case of scalar regression, we show that the gradient flow on the weights is equivalent to a mirror flow on the linear predictor. These characterizations give the geometrical structure of the training dynamics of linear neural networks.
3. In Proposition 4.1, we design a simple process to analytically describe how stochastic noise in the training algorithm can likewise induce low-rank structures in the weights _regardless of the initialization scale_.

We proceed by presenting related work in Section 1.1, formalizing the problem setup and assumptions in Section 2, stating and discussing our results in Section 3, and finally, we provide supporting numerical evidence in Section 4.

### Related Work

The first pillar of our work addresses the implicit bias of GF and its stochastic variant in regression problems. One of the hallmarks of bias in this setting is the impact of initialization scale: large initial weights induce a learning regime in which the parameters travel a short distance to convergence and feature learning fails to happen (_lazy_ regime), while small initialization effects a polar opposite behaviour of the system (_rich_ regime) Chizat et al. (2019); Woodworth et al. (2020). Training and generalization in the lazy regime are well-studied (Jacot et al., 2018; Du et al., 2019; Arora et al., 2019; Soltanolkotabi et al., 2017), however this scenario fails to capture the observed behaviour of neural networks in practice (Ghorbani et al., 2019). While the rich regime more faithfully approximates the feature learning abilities of these models, it is comparatively more challenging to analyze and few results are known. Amongst them are those concerning diagonal linear networks, where a preference towards sparse representations is shown (Woodworth et al., 2020), and a restricted setting of the matrix factorization problem, where the implicit bias leads to low-rank representations (Gunasekar et al., 2017; Arora et al., 2019; Li et al., 2018). We similarly study the rich representation learning regime and provide initialization scale-dependent statements on implicit bias for two-layer fully connected linear networks.

Most theoretical results on the implicit bias of GF in overparametrized models rely on the identification of a related mirror flow in a reparametrized space (Gunasekar et al., 2018). Diagonal linear networks are amenable to this technique and therefore well-studied (Woodworth et al., 2020; Pesme et al., 2021). For linear fully-connected networks, however, the existence of a mirror flow is not always guaranteed (Li et al., 2022). To partly alleviate this issue, Azulay et al. (2021) introduce a nonlinear time-rescaling technique and show that for scalar least-squares regression on a two-layer fully connected network with zero-balance initialization, the implicit bias selects low \(_{2}\)-norm predictors. We prove a similar result under _imbalanced_ initialization controlled by a scale parameter, and characterize the weight matrices independently at convergence, thus presenting a higher-resolution view of the problem.

Other works on linear networks include (Min et al., 2021) where convergence is studied in the presence of weight imbalance and implicit bias results are provided in the functional space; and (Yunet al., 2021) where tensor networks are studied with the goal of unifying the implicit bias results for linear parameterization. In the case of linear networks, Yun et al. (2021) further show an implicit bias towards minimum \(_{2}\) linear predictors for vanishingly small initializations. For classification in the case of linear networks, Ji and Telgarsky (2019) show that the weights grow to infinity and the layers of the deep linear network align during the course of optimization. Timor et al. (2023) show a similar phenomenon happens for two-layer ReLU networks.

The second pillar concerns the learning dynamics of linear neural networks. The two-layer case optimized with GF on the square loss has been studied by Fukumizu (1998); Saxe et al. (2014, 2019); Braun et al. (2022). The common setup of these works is that of zero-balance initialization and whitened data. First, Saxe et al. (2014, 2019) provide expressions for the temporal evolution of singular values of the predictor by assuming decoupled dynamics and a specific data-dependent initialization of the weights. This latter condition is alleviated by the approach of Fukumizu (1998) and Braun et al. (2022); Tarmoun et al. (2021) who solve a matrix Ricatti equation yielding solutions for the weight dynamics in the case where the network initialization has full rank. Finally, Gidel et al. (2019) loosen the whitened data assumption through a perturbation analysis and provide the time-evolution of singular values of the weight matrices. Our work removes the requirement of zero-balanced initialization and full-rank network initialization, and gives formulas for the weights' evolution as a function of the initialization scale. We further provide mirror flows on the weights' singular values and show that components are learned in a hierarchical manner for the case of whitened data.

Related work is further addressed in the following sections, as part of the discussion of results.

## 2 Preliminaries and problem setup

Notation.Time-dependent variables are written in bold fonts: we drop the \(t\) in \(A(t)\) and simply denote it as \(\). The time derivative of such variables is denoted \(A(t)\) as \(}{}\).

Vector Regression.The set-up is that of standard vector regression problems with inputs \((x_{1},,x_{n})(^{d})^{n}\) and outputs \((y_{1}, y_{n})(^{k})^{n}\) in the so-called overparametrized regime where \(d n\). Regarding the output dimension, the reader may keep in mind throughout the article that \(k d\), though the analysis holds for any \(k,d\) pair. In order to learn the input/output rule, we minimize the square loss over a class of parametric models \(=\{f_{}():^{d}^{k}^{p}\}\) which we specify in the next paragraph. The train loss therefore can be written as

\[()=_{i=1}^{n}(y _{i}-f_{}(x_{i}))^{2}. \]

Parameterization with a Linear Network.We consider the parametric model of _two-layer linear neural networks_ of width \(l^{*}\): this corresponds to the parametrization \(=(_{1},_{2})\), \(_{1}^{d l},_{2}^{l  k}\) and \(f_{}(x)=_{2}^{}_{1}^{}x\). The model is linear in the input \(x\), and in terms of expressivity, it is equivalent to the linear class of predictors given by \(f_{}(x)=^{}x\), with \(=_{1}_{2}\). We henceforth use the symbol \(\) to denote the associated linear predictor of the network. An important consequence of this reparametrization is that the prediction function \(f_{}\) is positively homogeneous of degree 2 in \(\): \(\), it holds that \(f_{}=^{2}f_{}\), as it is the case for two-layer ReLU networks. This property has important consequences in the loss landscape through which \(\) goes.

Train loss.Assume momentarily that \(k=1\) and denote \((x)=_{1}^{}x^{l}\). It is then clear that the predictor rewrites as \(f_{}(x)=(x),_{2}\). For this reason, we call the hidden layer \(_{1}\) the _feature layer_ and the last layer \(_{2}\) the _weight matrix_. We study the _overparametrized setting_ where \(l d\). Letting \(X^{}[x_{1},,x_{n}]\) and \(Y^{}[y_{1},,y_{n}]\), the loss becomes

\[(_{1},_{2})=\|X _{1}_{2}-Y\|^{2}. \]

For brevity, we ignore the \(N\) in Eq.(2.2) by implicitly rescaling the data as \((X,Y)(}{{}},}{{}})\).

Interpolators.Note that when \(Y(X)\) and \(X\) is non-degenerate (which occurs with probability one if e.g., \(X,Y\) are Gaussian and \(d n\)), there always exists a solution which attains _zero_ loss, i.e., \(^{*}^{d k}\) such that \(X^{*}=Y\). We emphasize the fact that there are two levels of overparametrization here: on one hand, when \(d>n\), the set of zero loss linear predictors \(_{}:=\{^{d k}\ X =Y\}\) is typically an affine set of dimension \((d-n)k\). On the other hand, since we also reparametrize \(\) as a linear network of width \(l d\), the manifold of interpolators in the reparametrized space of \(\), defined by \(_{}:=\{=(_{1},_{2})_{1 }_{2}_{}\},\) is of dimension \(l(d+k)-nk\). A natural question, therefore, is to which of these interpolators \(^{*}_{}\) does a given optimization algorithm converge. This concept is referred to as the _implicit bias_ of an algorithm. The aim of this work is to study that of gradient flow.

Gradient Flow.The dynamics induced in parameter space by running gradient flow on (2.2) is given by

\[}}=-_{}( ). \]

We wish to describe the implicit regularization properties of this continuous-time process, which is the vanishing stepsize limit of (stochastic) gradient descent. While the latter algorithms incur additional regularization properties from using non-zero stepsizes (Keskar et al., 2017), the study of GF is an important stepping stone to understanding the implicit bias of gradient-based methods in practice. In terms of \(_{1},_{2}\) the dynamics translates to

\[}_{1} =X^{}(Y-X_{1}_{2})_{2}^{}\,, \] \[}_{2} =_{1}^{}X^{}(Y-X_{1}_{2}). \]

We emphasize a crucial point: even if the function \(\|X-Y\|^{2}\) is convex, its reparametrization in terms of \(_{1},_{2}\) is not. Non-convexity and non-linearity makes the analysis challenging and a priori it is not even clear whether the time evolution of \(\) can be expressed as a closed system.

Initialization.One of our primary objects of study is the impact of initialization on the behaviour of GF. We describe here our initialization choice, to which we henceforth refer as \(I_{}\).

* **Orthogonal feature layer:** We initialize the inner layer such that the rows of \(_{1}\) are orthogonal and scale with parameter \(>0\). Mathematically, this translates to \(_{1}(0)=P\) for \(P^{d l}\) in the Stiefel manifold \(V_{d}(^{l}):=\{P^{d l},PP^{}=I_{d}\}\). Initializing with an orthogonal matrix is studied by Pennington et al. (2018); Hu et al. (2020), however from an optimization perspective. Note that when \(l\) is very large, this setting approximates the real-world scenario of initializing the hidden neurons with \(d\) i.i.d. Gaussian vectors in \(^{l}\), which are known to be almost orthogonal.
* **Zero weight layer:** In order to remove any initialization bias from the linear layer, we initialize it at \(_{2}(0)=0\). This can be seen as the limiting case of initializing the weight layer with a very small _relative_ scale \(\).

As already mentioned in Section 1.1, existing studies on linear networks assume a "zero-balance initialization", namely that \(_{1}^{}(0)_{1}(0)=_{2}(0)_{2}^{}(0)\)(Saxe et al., 2014; Arora et al., 2019, Azulay et al., 2021). This condition introduces the invariant \(_{1}^{}_{1}=_{2}_{2}^{}\)(Du et al., 2018), which holds for all times \(t 0\). This balancedness can be seen as a degeneracy assumption on the flow, since it implies that \(_{1}\) has at most rank \(k\) during the entire process, irrespective of the scale of initialization \(\). In contrast, we show that _depending on \(\)_ the feature layer \(_{1}\) is biased (or not) toward a low-rank predictor, thus unveiling a truly rich representation learning regime.

## 3 Main result: implicit bias and dynamics description

### Implicit bias on the parameters

Non-convex gradient flows are generally not guaranteed to reach _global_ minimizers of the objective and even when they do, such results are difficult to formally prove. Moreover, the existence of many zero-loss solutions with different generalization properties raises the question of which interpolatingnetwork is yielded by training. An elegant answer to such questions is to express the resulting predictor as the _optimum_ among all the possible interpolators of some new, a priori unspecified cost. In addition to the descriptive power of such variational formulations, they express a form of capacity control over the estimator which can be further used to describe its generalization abilities (Bartlett et al., 2020). The following theorem adds to this series of works, by precisely deriving such a characterization for GF in the setting of linear networks.

**Theorem 3.1**.: _Let \((_{1},_{2})\) be the process that follows the GF equations (2.4a)-(2.4b), initialized according to condition \(I_{}\), for some \(>0\). Then_

1. _The parameters converge to a global optimum of the loss_ \[_{t}(_{1}(t),_{2}(t))=(_{1}^{ },_{2}^{})_{}.\]
2. _The linear predictor_ \(\) _converges to the minimum_ \(_{2}\)_-norm interpolator_ \[_{t}(t)=*{argmin}_{X=Y}\ \|\|_{F}}}{{=}}_{*}.\]
3. _We have the following variational characterization of the limiting parameters_ \[(_{1}^{},_{2}^{})*{argmin}_{X_{ 1}_{2}=Y}_{2}_{F}^{2}+ _{1}_{F}^{2}-\ _{1}_{1}^{}.\] (3.1)

Interpretation of the theorem.The theorem is divided into three parts which state that (i) the matrices converge to a zero loss solution, which is a priori non-trivial since the loss in non-convex; (ii) among all the interpolators in \(_{}\), \(\) converges to the minimum \(_{2}\)-norm interpolator for all \(>0\); and (iii) among all the interpolators in \(_{},(_{1},_{2})\) converge to the ones that minimize a \(\)-dependent potential. To fully capture the richness of this result, we observe that in the limit of \( 0\), problem (3.1) informally translates to (Attouch, 1996)

\[_{ 0}(_{1}^{},_{2}^{}) *{argmin}_{X_{1}_{2}=Y}_{2} _{F}^{2}+_{1}_{F}^{2}.\]

This is equivalent, in the space of linear predictors \(\) to the minimum nuclear norm solution \(*{argmin}_{X=Y}\  _{*}\) (which is also the minimum \(_{2}\)-norm interpolator for the problem we study). We informally derived this interpretation by taking _first_ the limit \(t\)_and only after \( 0\). The theorem naturally does not hold if the two limits are reversed, since \(=0\) places the initialization at a saddle point of the loss, which is a stationary point of the flow.

With increasing \(\), we move towards solutions with a large \((_{1}_{1}^{})\), which is a smooth approximation of the rank (Fazel et al., 2003). Intuitively, this means that solutions with increasing rank are preferred as \(\) grows. This scale-induced implicit bias is reminiscent of the _rich_ and _lazy_ regimes (Choizat et al., 2019; Woodworth et al., 2020), albeit visible in the space of representations rather than in that of predictors. As such, our result for linear networks is akin to Woodworth et al. (2020)'s, which characterizes the rich and lazy regimes for simpler diagonal linear networks.

Comparison with works on implicit bias of \(\).Azulay et al. (2021); Min et al. (2021) also study the implicit bias phenomenon in linear networks, however, these results only address the structure of the final predictor \(\) and not that of the factorized problem \((_{1},_{2})\). As shown in Theorem 3.1, these works fall short of unveiling all the nuances of the implicit regularization induced by GF in the case of linear networks.

To give a precise example, consider the simplest case of scalar regression (\(k=1\)) for which both Theorem 3.1 and (Azulay et al., 2021) show that \(\) is biased towards low-\(_{2}\) interpolators. This view is not complete, since there exist many pairs \((_{1},_{2})\) such that \(_{1}_{2}=\). Theorem 3.1 goes one step further and provides variational characterization of \((_{1},_{2})\) at convergence. Moreover, it shows that when \( 0\) all columns of \(_{1}\) align in the direction of \(\), thus creating a rank one hidden layer. This is an example of rich representation learning, where \(_{1}\) is learning the only feature needed to make a prediction.

[MISSING_PAGE_FAIL:6]

(Appendix C.12) by solving a matrix Ricatti equation (Bittanti et al., 1991). In the limit of \( 0\), we can show that, beyond the case of balanced initialization (Saxe et al., 2014; Gidel et al., 2019), the singular values are learned in a hierarchical manner. When \( 0\) and with appropriately rescaled time, the limiting trajectory for the \(i^{th}\) singular value \(_{i,}\) can be seen as the _jump process_

\[_{i,}(()\!t)=_{i,_{}}(t>_{}}} ),\]

where \(_{i,_{}}\) is the \(i^{th}\) singular value of \(^{}\). Each singular value is activated at time \(-()(2_{i,_{}})^{-1}\). Therefore, we observe an incremental learning process, where the activation begins with the largest singular value and proceeds accordingly.

Mirror descent for scalar regression.The result (b) states that the GF on the parameters \((_{1},_{2})\) implies a mirror flow on the predictor \(\) with the potential \(_{}\). To be more precise, the evolution is governed by a mirror flow with the time scaled as a function of \(\|\|\). This technique of time-warping was proposed in Azulay et al. (2021) for the case of a linear network with a single neuron (\(l\) = 1) with balanced initializations. In contrast, with a specific initialization shape, we show the existence of a mirror flow for an arbitrary number of neurons and unbalanced initialization of any scale. The existence of a mirror flow is surprising since the reparametrization defining linear networks is not commutative in general (Li et al., 2022). However, due to the specific initialization we use, this problem can be circumvented by preserving certain commutative properties.

The equivalence with mirror descent enables us to show that \(((t))=O(1/ t)\) (see Appendix C.8), thus providing a convergence rate for the training loss independent of the conditioning of data, in contrast to Min et al. (2021); Du et al. (2019). Note that with decreasing initialization scale \(\), the convergence speed diminishes, while according to the results in Theorems 3.1 better implicit bias is achieved. This suggests the existence of a trade-off between optimization and implicit bias already observed in several works (Woodworth et al., 2020), where achieving better quality solutions is linked to slower optimization. In contrast to this behaviour, for the case of balanced initialization (Braun et al., 2022) emphasizes a decoupling between the learning speed and the quality of solutions. Conversely, we stress that in the general setting (e.g., under imbalance) such a decoupling is absent.

### Sketch of the proofs

In this section, we give a short description of the proofs of the main results from the previous sections. The common theme of the following intermediate results is to identify natural invariants of the dynamics, which can be leveraged to understand the hidden mirror structure of the flow.

**Lemma 3.1**.: _Consider the dynamics of the gradient flow (2.4) initialized at \((_{1}(0),_{2}(0))=(P,0)\). Let \(_{1}:=_{1}P^{},_{2}:=PW_{2}\) and the residual \(:=X^{}(Y-X_{1}_{2})\), then the evolution of \((_{1},_{2})\) is governed by the following ODE_

\[}_{1}=_{2}^{},}_{2}=_{1} ^{}. \]

_Furthermore, the dynamics of gradient flow (2.4) is equivalent to (3.4), i.e., \((_{1}(t),_{2}(t))=(_{1}(t)P,P^{}_{2}(t))\) at any time \(t\)._

Lemma 3.1 derives an equivalent dynamics to equations (2.4). It shows that weights \(_{1}^{},_{2}\) always stay in the column span of the initialization \(P\), thus restricting their evolution to a subspace. Going forward, we derive the invariants of the dynamics (3.4).

**Lemma 3.2**.: _For the projected matrices given in (3.4), we have the following invariant,_

\[_{1}^{}_{1}-_{2}_{2}^{}=2.\]

This invariant ensures that \(_{1}^{}_{1}\) and \(_{2}_{2}^{}\) commute which is a crucial ingredient in the proofs of Theorems 3.1, 3.2. Now, we derive the evolution of \(:=_{1}^{-}_{2}\), which turns out to be the central quantity enabling our result. The lemma below describes certain properties of the evolution of \(\).

**Lemma 3.3**.: _Let \(:=_{1}^{-}_{2}\), we have the following time evolution of parameters:_

\[}=-^{}\;,=(1-^{})^{-1}.\]An outline of the proof of Theorem 3.1.With an _ansatz_ on the potential that it is decomposable in terms of \(_{1},_{2}\), we derive KKT conditions for the constrained optimization problem

\[*{argmin}_{X_{1}_{2}=Y}_{1}(_{1})+_{2}( _{2}).\]

Using Lemmas 3.3, we show that \(\) stays in \(*{span}(X)\). We use the isotropic property of the imbalance from Lemma 3.2 to find appropriate functions \(_{1},_{2}\) and finally prove Theorem 3.1. The proofs for theorem 3.1, 3.2 and corollary 3.1 can be found in Appendix B.

## 4 Further thoughts and perspectives

The previous section provided a deep-dive into the dynamics of the gradient flow, which we complement here with a few steps in the direction of understanding the dynamics with stochastic gradients. We investigate stochastic gradient descent (SGD) by studying its simpler counterpart, label noise gradient descent (LNGD) Blanc et al. (2020).

### The role of noise

It was observed that the noise in stochastic gradient descent has a parameter-dependent shape that induces certain regularization properties (HaoChen et al., 2021; Blanc et al., 2020). Here, we study the properties of the noise shape induced in the case of parameterization with linear neural networks.

Inspired from the analysis of HaoChen et al. (2021) and the large noise regime described by Pillaud-Vivien et al. (2022) in the context of diagonal neural networks, we design a process driven purely by noise and which carries the same geometric properties as SGD's noise. We consider the scalar (\(k=1\)) regression problem with \(l=d\) and, through an abuse of notation, denote \(_{1}=\), and \(_{2}=\). The noise-driven process which we consider is:

\[=(_{t})^{}, =^{}_{t}, \]

where \(_{t}\) is a \(d\)-dimensional Brownian motion. Details on how this SDE captures the noise of SGD are deferred to Appendix D. We show that, similarly to the rich regime of the gradient flow (\( 0\)), this noise also carries a rich spectral bias _but for any initialization_. Indeed, we have the following result on the SDE dynamics. The proof can be found in Appendix D.

**Proposition 4.1**.: _The dynamics (4.1) has the following convergence properties_

1. _Variance explosion__. The variance of the norms of_ \(,\) _explode, i.e.,_ \[_{t}[\|(t)\|^{2}] ,_{t}[\|(t)\|^{2} ].\]
2. _Scale divergence__. For_ \(d 5\)_, for any_ \(>0\)_, we have that,_ \[_{t}[\|(t)\|^{}] ,_{t}[\|(t)\| ^{}+\|}(t)\|^{}].\] _where_ \(}:=e^{-t}_{0}^{t}e^{s}(s)s\) _is the exponential moving average of_ \(\)_._
3. _Alignment - spectral bias__. Denote the_ \(i^{}\) _row of_ \(\) _as_ \(_{i}\)_. Using_ \([_{i},]}}{{=}}_{i} ^{}-_{i}^{}\)_,_ \[_{t}[|[_{i},]|] 0.\]

For any two vectors \(u,v\), \([u,v]\) denotes the commutator of the vectors: remark that if \([u,v]=0\), then \(u,v\) are aligned, i.e, \(u=cv\), for some scalar \(c\). First, notice that for \(d=1\), the SDE in fact corresponds to the geometric Brownian motion with no drift and the dynamics collapses to zero (Oksendal, 2013). For dimension \(d 2\), the proposition states that the system diverges and the weights grow towards infinity. However, despite the fact that the norm grows, the commutator \([_{i},]\) goes to zero, indicating that all the rows of \(_{i}\) align towards \(\). Overall, similarly to the gradient flow in the rich regime, this induces a low rank structure in \(\). This phenomenon can be further seen through the evolution of singular values, where the top singular value of \(\) grows unboundedly, whereas the remaining singular values decay to \(0\) as depicted in Figure 2(a) in the Appendix. This sheds some light on how SGD induces a particular parameter-dependent noise which implicitly biases the solutions towards having a low-rank structure of the hidden layer (Andriushchenko et al., 2022).

**Intricate dynamics in presence of drift.** Proposition 4.1 focuses on the process that is solely driven by noise. However, in general SGD also encompasses a drift term which corresponds to the dynamics studied in Section 3. The continuous-time SDE describing the process is

\[=-_{}(,) t+(_{t})^{}, =-_{}(,)t+^{}_{t},\]

where \(>0\) indicates the scale of the noise. The presence of drift quickly complicates the analysis, but intuitively, the noise simplifies the model by inducing a rank reduction, whereas the drift terms prevent the weights from growing unbounded. This noise-driven mechanism relaxes the role of initialization. Empirically this is illustrated in Figure 0(b). Gradient descent already exhibits a regularization effect as it increases only one singular value while keeping the others constant. However, gradient descent with label noise (Blanc et al., 2020) enhances this regularization effect by decaying the singular values and promoting low-rank representations. As a result, even for larger initialization scales, we observe the presence of low-rank structures in the hidden layer, unlike in gradient descent. The precise characterization of the this phenomenon is left for future research.

**Experiments.** We consider a regression problem on synthetic data, with \(n=5\) samples of Gaussian data in \(^{10}\) (\(d=10\)) and the labels in \(^{3}\) (\(k=3\)) generated by a ground truth \(_{*}^{d k}\). We consider a network with width \(l=200\). In Figure 0(a), we show the evolution of the top-\(4\) singular values of the hidden layer \(_{1}\). We use orthogonal initialization for the network with the two scales of initialization \(=1,10^{-4}\). Note that, as depicted by Corollary 3.1, for the smaller scale only the first \(k=3\) singular values are significant in comparison to the remaining \(d-k\) singular values. This shows that the matrix is approximately rank \(k\) and the neurons align along three directions. In contrast, for the larger scale \(=1\), the final weight matrix has rank \(d\). To complement this, we also consider a Gaussian initialization with variance \(0.01\) - specifically, we initialize the inner layer with \(d=10\) Gaussian random vectors in \(^{l}\). As described when \(l d\), the initialization is close to the orthogonal initialization. Hence, in this case, we can see that only \(k\) singular values grow and the final model has an approximately rank \(k\) hidden layer. In figure 0(b), we depict the time evolution of singular values for GD and LNGD on a scalar regression problem with orthogonal data in \(^{5}\) (\(n,d=5\)) and a network with \(l=200\). Further details on hyper-parameters can be found in the Appendix.

**Extension to non-linear activations.** Huh et al. (2023), Andriushchenko et al. (2022) empirically demonstrate a low-rank phenomenon through extensive experiments on deep networks with non-linear activations. However, a comprehensive theoretical comprehension of this behavior remains elusive, despite some efforts addressing these issues (Boursier et al., 2022). To show that our analysis extends beyond linear activations, we present a toy experiment for ReLU networks (see Figure 2 and further details in Figure 5). Consider a scalar regression problem in a ReLU teacher-student setup. We generate a training set of size \(10\) sampled from a random Gaussian distribution in \(^{5}\). The training data \((x_{i},y_{i})_{i=1}^{10}^{5}\) is generated by a teacher ReLU network with \(2\) neurons \((w_{0},w_{1})\), i.e.,

\[y_{i}=a_{0}(w_{0}^{}x_{0})+a_{1}(w_{1}^{}x_{1}),\]

Figure 1: (a) Vector regression with orthogonal initialization and scales \(=1,10^{-4}\) and Gaussian initialization with entries from \(N(0,0.01)\) (b) Scalar regression with Gradient Descent (GD) and Label Noise Gradient Descent (LNGD).

where \(\) is the ReLU non-linearity. We train a student network with 20 hidden neurons. Note that there are two relevant directions \(w_{0},w_{1}\) for the student network to learn, therefore we expect the hidden layer to represent these two directions (i.e., a rank 2 hidden layer, and a singular value decomposition with two non-zero singular values). This property is empirically verified in Figure 2. We plot the time evolution of singular values and when initialized at low-scale the network converges to an approximately rank-2 matrix. When initialized at a larger scale, the network weight matrix is high rank and the neurons do not learn the teacher directions.

Perspectives.Learning representations which can be transferred to downstream tasks is a key attribute for the success of deep learning (Bengio et al., 2013; LeCun et al., 2015). In this work, we present an archetypal problem where for the same predictor in functional space, there exist multiple representations in parameter space, some of which can exhibit a rich structure. This scenario presents a case for going beyond the characterization of implicit bias in the functional space (Parhi and Nowak, 2022) and further studying the implicit bias in the parameter space. Such characterizations facilitate the identification of crucial ingredients in training algorithms that enable effective feature learning.

Limitations and Future Work.This paper tackles the phenomenon of implicit bias, with the aim of furthering the understanding of how neural networks learn in practice. Unfortunately, practical models are highly nonlinear due to their activations and rely on various heuristics to achieve state-of-the-art performance, thus being difficult to grasp mathematically. This work therefore studies the simplified setting of two-layer linear neural networks. In terms of the assumptions we make, the orthogonality of initialization is only approximately faithful to practical settings where small random weights are used. Nevertheless, we are confident that this requirement can be loosened through a perturbation analysis in the vein of Gidel et al. (2019). Finally, our dynamical description of the system is yet to be completed in the vector regression case with non-whitened data. A careful set of assumptions is necessary here, and hopefully ones that are weaker than the restricted isometry property used in related works (Li et al., 2018). Finally, we only partially describe the dynamics in the presence of stochastic noise and giving a full characterization remains a desired objective of future investigations. Further discussion on these aspects is presented in Appendix C.1.