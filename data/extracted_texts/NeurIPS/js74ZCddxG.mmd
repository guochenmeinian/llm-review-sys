# RFLPA: A Robust Federated Learning Framework against Poisoning Attacks with Secure Aggregation

Peihua Mai

&Ran Yan

National University of Singapore

Correspondence to bizpyj@nus.edu.sg

Yan Pang

Correspondence to bizpyj@nus.edu.sg

###### Abstract

Federated learning (FL) allows multiple devices to train a model collaboratively without sharing their data. Despite its benefits, FL is vulnerable to privacy leakage and poisoning attacks. To address the privacy concern, secure aggregation (SecAgg) is often used to obtain the aggregation of gradients on sever without inspecting individual user updates. Unfortunately, existing defense strategies against poisoning attacks rely on the analysis of local updates in plaintext, making them incompatible with SecAgg. To reconcile the conflicts, we propose a robust federated learning framework against poisoning attacks (RFLPA) based on SecAgg protocol. Our framework computes the cosine similarity between local updates and server updates to conduct robust aggregation. Furthermore, we leverage verifiable packed Shamir secret sharing to achieve reduced communication cost of \(O(M+N)\) per user, and design a novel dot product aggregation algorithm to resolve the issue of increased information leakage. Our experimental results show that RFLPA significantly reduces communication and computation overhead by over \(75\%\) compared to the state-of-the-art secret sharing method, BREA, while maintaining competitive accuracy.

## 1 Introduction

Federated learning (FL)  is a promising machine learning technique that has been gaining attention in recent years. It enables numerous devices to collaborate on building a machine learning model without sharing their data with each other. Compared with traditional centralized machine learning, FL preserves the data privacy by ensuring that sensitive data remain on local devices.

Despite its benefits, FL still has two key concerns to be addressed. Firstly, there is a threat of privacy leakage from local update. Recent works have demonstrated that the individual updates could reveal sensitive information, such as properties of the training data , or even allows the server to reconstruct the training data . The second issue is that FL is vulnerable to poisoning attacks. Indeed, malicious users could send manipulated updates to corrupt the global model at their will . The poisoning attacks may degrade the performance of the model, in the case of _untargeted attacks_, or bias the model's prediction towards a specific target labels, in the case of _targeted attacks_.

Secure aggregation (SecAgg) has become a potential solution to address the privacy concern. Under SecAgg protocol, the server could obtain the sum of gradients without inspecting individual user updates . However, this protocol poses a significant challenge in resisting poisoning attacks in FL. Most defense strategies  require the server to access local updates to detect the attackers, which increases the risk of privacy leakage. The contradiction makes it difficult to develop a FL framework that simultaneously resolves the privacy and robustness concerns.

To our best knowledge, BREA is the state-of-the-art FL framework that defends against poisoning attacks using secret sharing-based SecAgg protocol . Based on verifiable secret sharing, theirframework leverages pairwise distances to remove outliers. However, their work is limited by the scaling concerns arising from computation and communication complexity. For a model with dimension \(M\) and \(N\) selected clients, the framework incurs \(O(MN+N)\) communication per user, and \(O((N^{2}+MN)^{2}N N)\) computation for the server due to the costly aggregation rule. Furthermore, BREA makes unrealistic assumptions that the users could establish direct communication channels with other mobile devices.

To address the above challenge, we propose a robust federated learning framework against poisoning attacks (RFLPA) based on SecAgg protocol. We leverage verifiable packed Shamir secret sharing to compute the cosine similarity and aggregate gradients in a secure manner with reduced communication cost of \(O(M+N)\) per user. To resolve the increased information leakage from packed secret sharing, we design a dot product aggregation protocol that only reveals a single value of the dot product to the server. Our framework requires the server to store a small and clean root dataset as the benchmark. Each user relies on the server to communicate the secret with each other, and utilizes encryption and signature techniques to ensure the secrecy and integrity of messages. The implementation is available at [https://github.com/NusloraPrivacy/RFLPA](https://github.com/NusloraPrivacy/RFLPA).

Our main contributions involves the following:

(1) We propose a federated learning framework that overcomes privacy and robustness issues with reduced communication cost, especially for high-dimensional models. The convergence analysis and empirical results show that our framework maintains competitive accuracy while reducing communication cost significantly.

(2) To protect the privacy of local gradients, we propose a novel dot product aggregation protocol. Directly using packed Shamir secret sharing for dot product calculation can result in information leakage. Our dot product aggregation algorithm addresses this issue by ensuring that the server only learns the single value of the dot product and not other information about the local updates. Furthermore, the proposed protocol enables degree reduction by converting the degree-2d partial dot product shares into degree-d final product shares.

(3) Our framework guarantees the secrecy and integrity of secret shares for a server-mediated network model using encryption and signature techniques.

## 2 Literature Review

### Defense against Poisoning Attacks.

Various robust aggregation rules have been proposed to defend against poisoning attacks. KRUM selects the benign updates based on the pairwise Euclidean distances between the gradients . Yin et al.  proposes two robust coordinate-wise aggregation rules that computes the median and trimmed mean at each dimension, respectively. Bulyan  selects a set of gradients using Byzantine-resilient algorithm such as KRUM, and then aggregates the updates with trimmed mean. RSA  adds a regularization term to the objective function such that the local models are encouraged to be similar to the global model. In FLTrust , the server maintains a model on its clean root dataset and computes the cosine similarity to detect the malicious users. The aforementioned defense strategies analyze the individual gradients in plaintext, and thus are susceptible to privacy leakage.

### Robust Privacy-Preserving FL.

To enhance privacy and resist poisoning attacks, several frameworks have integrated homomorphic encryption (HE) with existing defense techniques. Based on Paillier cryptosystem, PEFL  calculates the Pearson correlation coefficient between coordinate-wise medians and local gradients to detect malicious users. PBFL  uses cosine similarity to identify poisonous gradients and adopted fully homomorphic encryption (FHE) to ensure security. ShieldFL  computes cosine similarity between encrypted gradients with poisonous baseline for Byzantine-tolerance aggregation. The above approaches inherit the costly computation overhead of HE. Furthermore, they rely two non-colluding parties to perform secure computation and thus might be vulnerable to privacy leakage. Secure Multi-party Computation (SMC) is an alternative to address the privacy concern. To the best of our knowledge, BREA  is the first work that developed Byzantine robust FL framework using verifiable Shamir secret sharing. However, their method suffers high communication complexity of SMC and high computation complexity of KRUM aggregation protocol. Refer to Appendix K.4 for a comprehensive comparison among existing protocols.

This paper explores the integration of SMC with defense strategy against poisoning attacks. We develop a framework that reduces communication cost, employs a more efficient aggregation rule and guarantees the security for a server-mediated model.

## 3 Problem Formulation and Background

### Problem Statement

We assume that the server trains a model \(\) with \(N\) mobile clients in a federated learning setting. All parties are assumed to be computationally bounded. Each client holds a local dataset \(\{D_{i}\}_{i[N]}\), and the server owns a small, clean root dataset \(D_{0}\). The objective is to optimize the expected risk function:

\[F()=_{}_{D}L(D,), \]

where \(L(D,)\) is a empirical loss function given dataset \(D\).

In federated learning, the server aggregates local gradients \(_{i}^{t}\) to obtain global gradient \(^{t}\) for model update:

\[^{t}=_{i S}_{i}^{t}_{i}^{t},\ ^{t}= ^{t-1}-^{t}^{t}, \]

where \(_{i}\) is the weight of client \(i\), \(^{t}\) is the learning rate, and \(S\) is the set of selected clients.

### Adversary Model

We consider two types of users, i.e., honest users and malicious users. The definitions of honest and malicious users are given as follows.

**Definition 3.1** (Honest Users).: A user \(u\) is honest if and only if \(u\) honestly submits its local gradient \(g_{u}\), where \(g_{u}\) is the true gradients trained on its local dataset \(D_{u}\).

**Definition 3.2** (Malicious Users).: A user \(u\) is malicious if and only if \(u\) is manipulated by an adversary who launches model poisoning attack by submitting poisonous gradients \(g_{u}^{*}\).

Server aims to infer users' information with two types of attacks, i.e., passive inference and active inference attack. In passive inference attack, the server tries to infer users' sensitive information by the intermediate result it receives from the user or eardrops during communication. In active inference attack, the server would manipulate certain users' messages to obtain the private values of targeted users.

### Design Goals

We aim to design a federated learning system with three goals.

**Privacy**. Under federated learning, users might still be concerned about the information leakage from individual gradients. To protect privacy, the server shouldn't have access to local update of any user. Instead, the server learns only the aggregation weights and global gradients, ensuring that individual user data remains protected.

**Robustness**. We aim to design a method resilient to model poisonous attack, meaning that the model accuracy should be within a reasonable range under malicious clients.

**Efficiency**. Our framework should maintain computation and communication efficiency even if it is operated on high dimensional vectors.

### Cryptographic Primitives

In this section we briefly describe cryptographic primitives for our framework. For more details refer to Appendix B.

**Packed Shamir Secret Sharing.** This study uses a generalization of Shamir secret sharing scheme , known as "packed secret-sharing" that allows to represent multiple secrets by a single polynomial . A degree-\(d\) (\(d l-1\)) packed Shamir sharing of \(=(s_{1},s_{2},...,s_{l})\) stores the \(l\) secrets at a polynomial \(f()\) of degree at most \(d\). The secret sharing scheme requires \(d+1\) shares for reconstruction, and any \(d-l+1\) shares reveals no information of the secret.

**Key Exchange.** The framework relies on Diffie-Hellman key exchange protocol  that allows two parties to establish a secret key securely.

**Symmetric Encryption.** Symmetric encryption guarantees the secrecy for communication between two parties . The encryption and decryption are conducted with the same key shared by both communication partners.

**Signature Scheme.** To ensure the integrity and authenticity of message, we adopt a UF-CMA secure signature scheme .

## 4 Framework

### Overview

Figure 1 depicts the overall framework of our robust federated learning algorithm. The algorithm consists of four rounds:

**Round 1:** each client receives the server update \(g_{0}\), computes their updates normalized by \(g_{0}\), and distributes the secret shares of their updates to other clients.

**Round 2:** each client computes the local shares of partial dot product for gradient norm and cosine similarity, and conducts secret re-sharing on the local shares.

**Round 3:** each client obtains final shares of partial dot product for gradient norm and cosine similarity, and transmits the shares to server. Then the server would verify the gradient norm, recover cosine similarity, and compute the trust score for each client.

**Round 4:** on receiving the trust score from the server, each client conducts robust aggregation on the secret shares locally, and transmits the secret shares of aggregated gradient to the server. The server finally reconstructs the aggregation on the secret shares.

Figure 1: Overall frameworkTo address increased information leakage caused by packed secret sharing, we design a dot product aggregation protocol to sum up the dot product over sub-groups of elements. Refer to Appendix D for the algorithm to perform robust federated learning.

### Normalization and Quantization

To limit the impact of attackers, we follow  to normalize each local gradient based on the server model update:

\[}_{i}=_{0}\|}{\|_{i}\|}_{i}, \]

where \(_{i}\) is the local gradient of the \(i\)th client, and \(_{0}\) is the server gradient obtained from clean root data.

Each client performs local gradient normalization, and the server validates if the updates are truly normalized. The secret sharing scheme operates over finite field \(_{p}\) for some large prime number \(p\), and thus the user should quantize their normalized update \(}_{i}\). The quantization poses challenge on normalization verification, as \(\|}_{i}\|\) might not be exactly equal to \(\|_{0}\|\) after being converted into finite field.

To address this issue, we define the following rounding function:

\[Q(x)=\{ qx/q,&x 0\\ ( qx+1)/q,&x<0., \]

where \( qx\) is the largest integer less than or equal to \(qx\).

Therefore, the server could verify that \(\|}_{i}\|\|_{0}\|\), which is ensured by the quantization method.

### Robust Aggregation Rule

Consistent with FLTrust, our framework conducts robust aggregation using the cosine similarity between users' and server's updates. The trust score of user \(i\) is:

\[TS_{i}=(0,_{i},_{0}}{\| _{i}\|\|_{0}\|})=(0,}_{i},_{0}}{\|_{0}\|^{2}}), \]

where we clip the negative cosine similarity to zero to avoid the impact of malicious clients.

The global gradient is then aggregated by:

\[=^{N}TS_{i}}_{i=1}^{N}TS_{i}}_{i}. \]

Finally, we use the gradient to update the global model:

\[-. \]

Our framework leverages the robust aggregation rule consistent with FLTrust due to its advantages including low computation cost, the absence of a requirement for prior knowledge about number of poisoners, defend against majority number of poisoners, and compatibility with Shamir Secret Sharing. Appendix C details the comparison between FLTrust and existing robust aggregation rules.

### Verifiable Packed Secret Sharing

The core idea of packed secret sharing is to encode \(l\) secrets within a single polynomial. Consequently, the secret shares of local updates generated by each user would reduce from \(NM\) to \(NM/l\). By selecting \(l=O(N)\), the per-user communication cost at secret sharing stage can be decreased to \(O(M+N)\). We assume that the prime number \(P\) is large enough such that \(P>\{N\|_{0}\|,\|_{0}\|^{2}\}\) to avoid overflow.

One issue with secret sharing is that a malicious client may send invalid secret shares, i.e., shares that are not evaluated at the same polynomial function, to break the training process. To address this issue, the framework utilizes the verifiable secret sharing scheme from , which generates constant size commitment to improve communication efficiency. We construct the verifiable secret shares for both local gradients and partial dot products described in Section 4.5. During verifiable packed secret sharing, the user would send the secret shares \(\), commitment \(\), and witness \(w_{l}\) to other users. A commitment is a value binding to a polynomial function \((x)\), i.e., the underlying generator of the secret shares, without revealing it. A witness allows others to verify that the secret share \(s_{l}\) is generated at \(l\) of the polynomial (see Appendix E for more details).

### Dot Product Aggregation

Directly applying packed secret sharing may increase the risk of information leakage when calculating cosine similarity and gradient norm. In the example provided by Figure 2, the gradient vectors are created as secret shares by packing \(l\) secret into a polynomial function. Following the local similarity computations by each client, the server can reconstruct the element-wise product between the two gradients, which makes it easy to recover the user's gradient \(_{i}\) from the reconstructed metric. On the other hand, our proposed protocol ensures that only the single value of dot product is released to the server. Based on this, we introduce a term _partial dot product_, or _partial cosine similarity (norm square)_ depending on the input vectors, defined as follows:

_Partial dot product_ _represents the multiple dot products of several subgroups of elements from input vectors rather than a single dot product value._

Another related concept is _final dot product_, referring to the single value of dot products between two vectors. For example, given two vectors \(_{1}=(2,-1,4,5,6,3)\) and \(_{2}=(1,2,0,3,-2,1)\), the reconstructed _partial dot product_ could be \((0,15,-9)\) if we pack \(2\) elements into a secret share, while the _final dot product_ is \(6\). If each client directly uploads the shares from local dot product computation, the server would reconstruct a vector of partial cosine similarity (norm square) and thus learn more gradient information.

To ensure that the server only has access to final cosine similarity (norm square), we design a dot product aggregation algorithm based on secret re-sharing that allows the users to sum up the dot products over subgroups.

Suppose that the user \(i\) creates a packed secret sharing \(^{i}=\{v^{i}_{jk}\}_{j[N],k[ m/l]}\) of \(}_{i}=(g^{i}_{1},g^{i}_{2},...,g^{i}_{M})\), by packing each \(l\) elements into a secret. On receiving the secret shares, each user \(i\) can compute the vectors \(^{i}=(cs^{i}_{1},cs^{i}_{2},...,cs^{i}_{N})\) and \(^{i}=(nr^{i}_{1},nr^{i}_{2},...,nr^{i}_{N})\):

\[cs^{i}_{j}=_{l}v^{j}_{il} v^{0}_{il},\ nr^{i}_{j}=_{l}v^{j}_{il}  v^{j}_{il}, \]

where \(cs^{i}_{j}\) and \(nr^{i}_{j}\) denotes the \(i^{th}\) share of partial cosine similarity and partial gradient norm square for user \(j\)'s gradient.

The partial cosine similarity (or gradient norm square) could be further aggregated by the procedure below in four steps.

**Step 1: Secret resharing of partial dot product.** Each user \(i\) could construct the verifiable packed secret shares of \(^{i}\) (or \(^{i}\)) by representing \(p\) secrets on a polynomial:

\[^{i}=(s^{i}_{11}&&s^{i}_{1 N/p }\\ &&\\ s^{i}_{N1}&&s^{i}_{N N/p}), \]

Figure 2: Cosine similarity computation on packed secret sharing

where \(s^{i}_{jk}\) denotes the share sent to user \(j\) for the \(k^{th}\) group of elements in vector \(^{i}\) (or \(^{i}\)). By choosing \(p=O(N)\), each user will generate \(O(N)\) secret shares.

**Step 2: Disaggregation on re-combination vector.** After distributing the secret shares, each user \(i\) receives a re-combination vector \(_{ik}=(s^{1}_{ik},s^{2}_{ik},...,s^{N}_{ik})\) for \(k[ N/p]\). Since we pack \(l\) elements for the secret shares of partial dot product, this step aims to transform the \(_{ik}\) into \(l\) vectors, with each vector representing one element. For each \(j[l]\), user \(i\) locally computes:

\[}^{i}_{jk}=_{ik}B^{-1}_{e_{j}}Chop_{d}, \]

where \(B_{e_{j}}\) is an \(n\) by \(n\) matrix whose \((i,k)\) entry is \((_{k}-e_{j})^{i-1}\), and \(Chop_{d}\) is an \(n\) by \(n\) matrix whose \((i,k)\) entry is \(1\) if \(1 i=k d\) and \(0\) otherwise. After this operation, the degree-2d partial dot product shares are transformed into degree-d shares.

**Step 3: Aggregation along packed index.** The new secrets are summed up along \(j[l]\) at client side:

\[^{i}_{k}=_{j=1}^{l}}^{i}_{jk}. \]

**Step 4: Decoding for final secret shares.** User \(i\) can derive the final secret shares \(x^{i}_{k}\) by recovering from \(^{i}_{k}=(h^{i}_{k1},h^{i}_{k2},...,h^{i}_{kN})\) using Reed-Solomon decoding. Noted that \(\{x^{i}_{k}\}_{k[ N/p]}\) becomes a packed secret share of dot products of degree \(d\) (see Appendix F). Therefore, the server could recover the cosine similarity (or gradient norm square) for all users on receiving the final shares from sufficient users.

### Secret Sharing over Insecure Channel

This framework relies on a server-mediated communication channel for the following reasons: (1) it's challenging for mobile clients to establish direct communication with each other and authenticate other devices; (2) a server could act as central coordinator to ensure that all clients have access to the latest model. On the other hand, the secret sharing stage requires to maintain the privacy and integrity of secret shares.

To protect the secrecy of message, we utilize key agreement and symmetric encryption protocol. The clients establish the secret keys with each other through Diffie-Hellman key exchange protocol. During secret sharing, each client \(u\) uses the common key \(k_{uv}\) to encrypt the message sent to client \(v\), and client \(v\) could decrypt the cyphertext with the same key.

Another concern is that the server may falsify the messages transmitted between clients. Signature scheme is adopted to prevent the active attack from server. We assume that all clients receive their private signing key and public signing keys of all other clients from a trusted third party. Each client \(i\) generates a signature \(_{i}\) along with the message \(m\), and other clients verify the message using client \(i\)'s public key \(d^{PK}_{i}\).

## 5 Theoretical Analysis

### Complexity Analysis

In this section, we analyze the per iteration complexity for \(N\) selected clients, and model dimension of \(M\), and summarize the complexity in Table 1. Further details of the complexity analysis are available in Appendix G. One important observation is that the communication complexity of our protocol reduces from \(O(MN+N)\) to \(O(M+N)\). Furthermore, the server-side computation overhead is reduced to \(O((M+N)^{2}N N)\), benefiting from the efficient aggregation rule and packed secret sharing. It should be noted that while the BERA protocol has similar server communication complexity, it makes an unrealistic assumption that users can share secrets directly with each other, thereby saving the server's overhead.

### Security Analysis

The security analysis is conducted for Algorithm 3. Given a security parameter \(\), a server \(S\), and any subsets of users \(\), let \(^{,t,}_{}\) be a random variable representing the joint view of partiesin \( S\) where the threshold is set to \(t\), and \(_{i}\) be the subset of respondents at round \(i\) such that \(_{1}_{2}_{3 }_{4}\). We show that the joint view of any group of parties from \(\) with users less than \(t\) can be simulated given the inputs of clients in that group, trust score \(\{TS_{j}\}_{j_{4}}\), and global gradient \(\). In other words, _the server learns no information about clients' input except the global gradient and trust score_.

**Theorem 5.1** (Security against active server and clients).: _There exists a PPT simulator \(\) such that for all \(t K-L\), \(|\{S\}|<t\), the output of \(\) is computationally indistinguishable from the output of \(^{,t,}_{}\):_

\[^{,t,}_{}(_{}) ^{,t,}_{}(_{}) \]

_where "\(\)" represents computationally indistinguishable._

### Correctness against Malicious Users

In this section, we show that our protocol executes correctly under the following attacks of malicious users: (1) sending invalid secret shares; (2) sending shares from incorrect computation of 6, 8, 10, or 11. Note that adversaries may also create shares from arbitrary gradients, and we left the discussion of such attack to Section 5.4.

The first attack arises when the user doesn't generate shares from the same polynomial. Such attempt is prevented by verifiable secret sharing that allows for the verification of share validity by testing 18.

The second attack could be addressed by Reed-Solomon codes. For a degree-\(d\) packed Shamir secret sharing with \(n\) shares, the Reed-Solomon decoding algorithm could recover the correct result with \(E\) errors and \(S\) erasures as long as \(S+2E+d+1 n\).

### Convergence Analysis

**Theorem 5.2**.: _Suppose Assumption 1, 2, 3 in Appendix \(J\) hold. For arbitrary number of malicious clients, the difference between the global model \(^{t}\) learnt by our algorithm and the optimal \(^{*}\) is bounded. Formally, we have the following inequality with probability at least \(1-\):_

\[\|^{t}-^{*}\|(1-)^{t}\|^{0}-^ {*}\|+12_{1}+}{q} \]

_where \(=1-(/(4L_{g}^{2})}+24_{2}+2 L)\), \(_{1}=_{1}|}}\), \(_{2}=_{2}|}}}{_{2}}+ d|}{d}+(^{2}r}}{ _{2}_{1}})}\), \(L_{2}=\{L,L_{1}\}\)._

_Remark 5.3_.: \(/q\) is the noise caused by the quantization process in our algorithm.

## 6 Experiments

### Experimental Setup

**Dataset.** We use three standard datasets to evaluation the performance of RFLPA: MNIST , FashionMNIST (F-MNIST) , and CIFAR-10 . MNIST and F-MNIST are trained on the neural network classification model composed of two convolutional layers and two fully connected layers, while CIFAR-10 is trained and evaluated with a ResNet-9  model.

s **Attacks.** We simulate two types of poisoning attacks: gradient manipulation attack (untargeted) and label flipping attack (targeted). Under gradient manipulation attack, the malicious users generate

    &  &  \\  & Computation & Communication & Computation & Communication \\  Server & \(O((M+N)^{2}N N)\) & \(O((M+N)N)\) & \(O((N^{2}+MN)^{2}N N)\) & \(O(MN+N^{2})\) \\ User & \(O((M+N^{2})^{2}N)\) & \(O((M+N))\) & \(O(MN^{2}N+MN^{2})\) & \(O(MN+N)\) \\   

Table 1: Complexity summary of RFLPA and BERAarbitrary gradients from normal distribution of mean 0 and standard deviation 200. For label flipping attack, the adversaries flip the label from \(l\) to \(P-l-1\), where \(P\) is the number of classes. We consider the proportion of attackers from \(0\%\) to \(30\%\).

### Experiment Results

#### 6.2.1 Accuracy Evaluation

We compare our proposed method with several FL frameworks: FedAvg , Bulyan , Trimmean , local differential privacy (LDP) , central differential privacy (CDP) , and BREA . Refer to Table 5 for the corse-grained comparison between RFLPA and the baselines. Noted that several baselines are not included in the accuracy comparison because: (i) The security of the some schemes relies on the assumption of two non-colluding parties, which is vulnerable in real life. (ii) Some frameworks entail significant computation costs, rendering their implementation in real-life scenarios impractical (see Appendix K.8.1). Table 2 summarizes the accuracies for different methods under the two attacks.

When defense strategy is not implemented, the accuracies of FedAvg decrease as the proportion of attackers increases, with a more significant performance drop observed under gradient manipulation attacks. Benefited from the trust benchmark, our proposed framework, RFLPA, demonstrates more stable performance for up to \(30\%\) adversaries compared to other baselines. In the absence of attackers, our method achieves slightly lower accuracies than FedAvg, with an average decrease of \(2.84\%\), \(4.38\%\)and \(3.46\%\), respectively, for MNIST, F-MNIST, and CIFAR-10 dataset.

#### 6.2.2 Overhead Analysis

To verify the effectiveness of our framework on reducing overhead, we compare the per-iteration communication and computation cost for BREA and RFLPA in Figure 3. For each experiment we set the degree as \(0.4N\) and encode \(0.1N\) elements within a polynomial.

The left-most graph presents the overhead with different participating client size using the 1.6M parameter model described in Section 6.1. For \(M N\), the per-client communication complexity for RFLPA remains stable at around 82.5MB, regardless of user size. Conversely, BREA exhibits linear scalability with the number of participating clients. Our framework reduces the communication cost by over \(75\%\) compared with BREA.

The second left graph examines the communication overhead for varying model dimensions with 2,000 participating clients. RFLPA achieves a much lower per-client cost than BREA by leveraging packed secret sharing, leading to a \(99.3\%\) reduction in overhead.

The right two figures presents the computation cost under varying client size using a MNIST classifier with 1.6M parameters. Benefiting from the packed VSS, RFLPA reduces both the user and server computation overhead by over \(80\%\) compared with BREA.

    Proportion of Attacks \\  } &  &  \\  & No & \(10\%\) & \(20\%\) & \(30\%\) & No & \(10\%\) & \(20\%\) & \(30\%\) \\   FedAvg \\  } & MNIST & \(\) & \(0.46 0.1\) & \(0.40 0.1\) & \(0.32 0.0\) & \(\) & \(\) & \(0.32 0.0\) & \(0.82 0.0\) \\  & F-MNIST & \(\) & \(0.55 0.0\) & \(0.51 0.0\) &

#### 6.2.3 Other studies

For other studies, we analyze the impact of iterations on accuracy (see Appendix K.5), evaluate our protocol against additional attacks (see Appendix K.6), conduct further overhead analysis (see K.8), and examine the performance under non-iid setting (see Appendix K.9).

## 7 Conclusion

This paper proposes RFLPA, a robust privacy-preserving FL framework with SecAgg. Our framework leverages verifiable packed Shamir secret sharing to compute the cosine similarity between user and server update and conduct robust aggregation. We design a secret re-sharing algorithm to address the increased information leakage concern, and utilize encryption and signature techniques to ensure the security over server-mediated channel. Our approach achieves the reduced per-user communication overhead of \(O(M+N)\). The empirical study demonstrates that: (1) RFLPA achieves competitive accuracies for up to \(30\%\) poisoning adversaries compared with state-of-the-art defense methods. (2) The communication cost and computation cost for RFLPA is significantly lower than BERA by over \(75\%\) under the same FL settings.

## 8 Discussion and Future Work

**Collection of server data.** One important assumption is that the server is required to collect a small, clean root dataset. Such collection is affordable for most organizations as the required dataset is of small size, e.g., 200 samples. According to theoretical analysis, the convergence is guaranteed when the root dataset is representative of the overall training data. Empirical evidence presented in  suggests that the performance of the global model is robust even when the root dataset diverges slightly from the overall training data distribution. Furthermore, Appendix K.10 proposes several alternative robust aggregation modules, such as KRUM and comparison with global model, to circumvent the assumption.

**Compatibility with other defense strategies.** RFLPA adopts a robust aggregation rule that computes the cosine similarity with server update. The framework can be easily generalized to distance-based method such as KRUM or multi-KRUM by substituting the robust aggregation module. However, extending the framework to rank-based defense methods may be more challenging. Existing SMC techniques for rank-based statistics requires \( M\) rounds of communication, where \(M\) is the range of input values . We leave the problem of communication-efficient rank-based robust FL to future work.

**Differential privacy guarantee.** Differential privacy (DP)  provides formal privacy guarantees to prevent information leakage. The combination of SMC and DP, also known as Distributed DP , reduces the magnitude of noise added by each user compared with pure local DP. However, adopting DP in the privacy-preserving robust FL framework is non-trivial, especially when bounding the privacy leakage of robustness metrics such as cosine similarity may sacrifice utility. We leave the problem of incorporating DP into the privacy-preserving robust FL framework to future work.

Figure 3: Per-iteration communication (left two) and computation cost (right two).