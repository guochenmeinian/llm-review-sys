# Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains

Taming Cross-Domain Representation Variance in Federated Prototype Learning with Heterogeneous Data Domains

 Lei Wang

University of Florida

Gainesville, FL 32611

leiwang1@ufl.edu

&Jieming Bian

University of Florida

Gainesville, FL 32611

jeming.bian@ufl.edu

&Letian Zhang

Middle Tennessee State University

Murfreesboro, TN 37132

letian.zhang@mtsu.edu

&Chen Chen

University of Central Florida

Orlando, FL 32816

chen.chen@crcv.ucf.edu

&Jie Xu

University of Florida

Gainesville, FL 32611

jie.xu@ufl.edu

The first two authors contributed equally to this work.

###### Abstract

Federated learning (FL) allows collaborative machine learning training without sharing private data. While most FL methods assume identical data domains across clients, real-world scenarios often involve heterogeneous data domains. Federated Prototype Learning (FedPL) addresses this issue, using mean feature vectors as prototypes to enhance model generalization. However, existing FedPL methods create the same number of prototypes for each client, leading to cross-domain performance gaps and disparities for clients with varied data distributions. To mitigate cross-domain feature representation variance, we introduce FedPLVM, which establishes variance-aware dual-level prototypes clustering and employs a novel \(\)-sparsity prototype loss. The dual-level prototypes clustering strategy creates local clustered prototypes based on private data features, then performs global prototypes clustering to reduce communication complexity and preserve local data privacy. The \(\)-sparsity prototype loss aligns samples from underrepresented domains, enhancing intra-class similarity and reducing inter-class similarity. Evaluations on Digit-5, Office-10, and DomainNet datasets demonstrate our method's superiority over existing approaches.

## 1 Introduction

Federated Learning  (FL) is a novel distributed learning framework that enables clients to collaboratively train a global model using their respective local datasets, thereby preserving data privacy. FL offers distinct advantages over traditional distributed learning methodologies by mitigating communication costs and addressing privacy concerns, which has led to its increased adoption across various sectors. Despite these benefits, FL comes with its own challenges, especially in terms of data heterogeneity . In FL, clients gather private data from unique sources, resulting in non-independent and identically distributed (non-IID) datasets. Such non-IID distributions can lead clients toward their local optima, potentially diverging from the global objective. Consequently, this may impede convergence rates and diminish overall model performance .

To address the aforementioned issue of data heterogeneity, numerous FL techniques have been devised . While these methods have indeed improved convergence, their focus on non-IID scenarios remains largely restricted. Typically, they assume that the data across clientspertain to a single domain, attributing non-IID distribution to label skew alone. Yet, in more realistic scenarios, clients gather data according to their unique preferences, making it impractical to assume identical domain origins for local data. _Instead, the data often stems from heterogeneous domains, resulting in varied feature distributions._

Current approaches [10; 32] to FL in heterogeneous domains aim to develop a global prototype for each label category. These prototypes are designed to minimize the distance between the individual training samples and their corresponding category's global prototype. Typically, a global prototype is computed as the mean of the local prototypes from each client, whereas a local prototype is itself the average of the representations of samples within the same category. Several studies have proposed unique designs to enhance training performance in these settings. For example, one work  enables contrastive learning at the client level by facilitating the exchange of local prototypes, thus promoting inter-client knowledge transfer. However, this exchange raises privacy concerns and substantially increases communication overhead. Another study  opts for a clustering approach to identify representative prototypes, thereby preserving domain diversity and preventing bias towards predominant domains. This technique has proven effective, especially in scenarios with disproportionate client distribution across domains.

Although these methods improve overall performance across domains, they do not address the unequal learning challenges that arise from domain diversity. For instance, on the Digits dataset , methods may perform well in domains like MNIST  but underperform in more challenging domains such as SVHN . This discrepancy is evident in representation distributions, as exemplified in Fig. 1, where 'easy' domains show tight clustering of samples within the same category and clear separation between different categories, facilitating accurate classification. In contrast, 'hard' domains display looser clustering, increasing the likelihood of misclassification, particularly for samples near category boundaries. Addressing these disparities is crucial for the equitable advancement of FL methodologies across diverse client domains.

Considering the challenges posed by unequal learning due to domain diversity, we introduce a novel method, termed FedPLVM (short for Federated Prototype Learning with Variance Mitigation). FedPLVM devises two main mechanisms. **Firstly**, we develop a dual-level prototype clustering mechanism that adeptly captures variance information, a significant improvement over previous methodologies that rely on averaging local training samples' representations to derive local prototypes. Our local-level clustering generates multiple local clustered prototypes within each domain. To further mitigate increased communication costs and privacy concerns arising from transferring a

Figure 1: **Illustration of federated learning with heterogeneous data domains. The Vanilla column depicts the local feature distribution of the standard FedPL approach, obtaining average local and global prototypes directly. Proposed method showcased in the adjacent column yields a larger inter-class distance and a reduced intra-class distance. Note that without capturing variance information, even for hard domains, local averaged prototypes for each class can be well distinguished while the feature vectors are still mixed up. Both methods illustrate noticeable variations in domain characteristics across datasets, as detailed in Fig. 4.**

comprehensive set of diverse prototypes for each class from every client, we employ global-level prototype clustering on the server side. **Secondly**, by capturing the variance information through clustered prototypes, we design an innovative \(\)-sparsity prototype loss to enhance the training process. To prevent the intermingling of feature distributions from 'hard' clients, instead of simply maximizing the feature-level distance between each local instance and the prototypes of other classes, we refine this distance by elevating it to an \(\) power, with \(\) being a value between 0 and 1. This modification effectively repels other prototypes, thereby introducing greater sparsity in the inter-class feature distributions. Moreover, we incorporate an intra-class similarity correction term to lessen the feature-level distance among intra-class samples, thereby concentrating the \(\)-sparsity prototype loss on amplifying the feature-level distance across inter-class samples. Collectively, these two strategies empower FedPLVM to reasonably leverage variance information, thus equilibrating fairness between 'hard' and 'easy' learning domains and enhancing overall learning performance. Consequently, FedPLVM emerges as a reliable approach for Federated Learning in contexts characterized by heterogeneous data domains. Our main contributions are outlined as follows:

* This study delves into FL with heterogeneous data domains, examining why models exhibit varying performance across different domains. We identify a fundamental limitation in existing methods: their inability to effectively address the disparate learning challenges inherent in diverse domains.
* To tackle these uneven learning challenges, we introduce a novel approach, FedPLVM. This method incorporates a dual-level prototype clustering method, capturing the rich sample representation variance information while ensuring communication efficiency. Additionally, we develop a new \(\)-sparsity prototype loss to address learning difficulties more equitably.
* Extensive experiments conducted on the Digit-5 , Office-10 , and DomainNet  datasets demonstrate the superior performance of our proposed method when compared with multiple state-of-the-art approaches.

## 2 Related Work

**Federated Learning.** FL aims to train a global model through collaboration among multiple clients while preserving their data privacy. FedAvg , the pioneering work in FL, demonstrates the advantages of this approach in terms of privacy and communication efficiency by aggregating local model parameters to train a global model. A significant challenge in FL is data heterogeneity, often manifested as non-IID (independently and identically distributed) data. Subsequent research, following FedAvg, has primarily focused on addressing data heterogeneity to enhance training performance in FL environments. Specifically, studies such as [18; 2; 30] have improved performance by incorporating a global penalty term to mitigate discrepancies. Other works, e.g., [32; 23], have sought to maximize feature-level agreement between local and global models to further boost performance. Recent works [10; 32; 19; 40] have explored data heterogeneity arising from client-specific domain diversity, while overlooking the challenges of unequal learning across different domains. In this paper, we propose a novel approach using dual-level prototype clustering to capture essential local variance information. Additionally, we introduce a new \(\)-sparsity loss, specifically designed to tackle the challenges of learning in diverse domains, thereby facilitating the development of a more generalizable global model.

**Prototype Learning.** Prototype learning has been extensively explored in various tasks, such as transfer learning [27; 13], few-shot learning , zero-shot learning , and unsupervised learning . The concept of a prototype in this context refers to the average feature vectors of samples within the same class. In the FL literature, prototypes serve to abstract knowledge while preserving privacy. Specifically, approaches like FedProc  and FedProto  focus on achieving feature-wise alignment with global prototypes. FedPCL  employs prototypes to capture knowledge across clients, constructing client representations in a prototype-wise contrastive manner using a set of pre-trained models. FPL  highlights the use of cluster-based and unbiased global prototypes to tackle the challenges in FL where clients possess domain-diverse data. Our study addresses a similar issue as FPL but with distinct emphases. While FPL concentrates on the disparities in the number of clients across domains, aiming to mitigate the bias in the overall model caused by domains with more clients, our work focuses on the intrinsic learning challenges that vary across domains.

**Contrastive Learning.** Contrastive learning is a promising self-supervised learning technique. The work by  constructs pairs of positives and negatives for each sample and applies the InfoNCE loss to compare these pairs. Another work by  extends contrastive learning from self-supervised to fully supervised settings, utilizing both label information and contrastive methods. Additionally, some studies [16; 39] integrate contrastive learning into local training to enhance performance in FL. Instead of the conventional InfoNCE loss, our approach introduces a new \(\)-sparsity loss, aiming to further reduce similarity among inter-class sample features while amplifying similarity among intra-class samples.

## 3 Preliminary

**Regular FL Scenario.** Consider the classic FL scenario: there exist \(K\) clients and one server, which aims to assist all clients to train a common machine learning model without sharing their private data, denoted by \(_{k}=\{_{i},y_{i}\}_{i}^{N_{k}}\) for client \(k\). Formally, the global objective of FL can be formulated as:

\[_{w}_{k=1}^{K}}{N}_{k}(w;_{k}),\] (1)

where \(L_{k}\) is the local loss function for client \(k\), \(w\) and \(N=_{k=1}^{K}N_{k}\) denote the shareable model and the total number of samples among all clients respectively.

**Domain Shift in FL.** In the simplest FL setting, the label and feature distribution are the same between two clients \(i\) and \(j\), formally \(P_{i}(y)=P_{j}(y)\) and \(P_{i}(x|y)=P_{j}(x|y)\). Extending such a scenario to the heterogeneous data setting brings us to the point of domain shift, also denoted as feature non-IID setting. Domain shift is caused by distinctive feature distributions among clients, that is \(P_{i}(x|y) P_{j}(x|y)\), though their label space is still the same.

**Federated Prototype Learning.** To handle the FL domain shift problem, previous works [32; 10] divide the classification network into two parts: feature extractor and classifier. The feature extractor \(h:^{V}^{D}\) maps a sample \(^{V}\) into the feature space and generates the corresponding feature vector \(=h()^{D}\). Then the classifier \(f:^{D}^{M}\) outputs the \(M\)-class prediction \(f()=f(h())^{M}\) given the feature vector \(\). The intuition of FedPL is adjusting the feature extractor to generate a consistent feature distribution among different domains. Similar to the global shareable model, the straightforward solution computes the average feature vector of all samples belonging to the same class for further processing, called the **prototype**. Due to the distributed property of FL, we cannot collect all samples via the server, which makes the above-mentioned idea

Figure 2: **An overview of our proposed FedPLVM framework. Once the sample embedding is generated by the feature extractor, the client conducts the first-level local clustering, following Eq. 3. Subsequently, the server gathers all local clustered prototypes and local models (comprising feature extractors and classifiers), initiates the second-level global clustering based on Eq. 4, and averages the local models to form a global model. Finally, clients utilize the received global clustered prototypes to update the local model, employing loss functions \(_{}\) from Eq. 5 and \(_{CE}\) from Eq. 9.**a two-step procedure:

\[_{k}^{m}=_{k}^{m}|}_{(_{i},y_{i}) _{k}^{m}}h(_{i}),\;\;\;^{m}=_{k=1}^{ K}_{k}^{m},\;\;\; k,m,\] (2)

where \(=\{1,2,,K\}\) is the set of clients, \(=\{1,2,,M\}\) is the set of classes. \(_{k}^{m}\) and \(^{m}\) denotes the **local prototype** of class \(m\) on client \(k\) and the **global prototype** of class \(m\), respectively. \(_{k}^{m}\) is the subset that all samples belonging to class \(m\) in the local dataset \(_{k}\). Upon obtaining the prototypes, most FedPL works tend to design a loss function that approximates the prototype of one's own class while staying away from the prototypes of others.

## 4 FedPLVM: FedPL with Variance Mitigation

### Dual-Level Prototype Generation

**Local Prototype Clustering.** To mitigate the impact of domain variance in FedPL, we revise the two-step averaged prototype generation process to a dual-level clustering algorithm. In Fig. 1, the pronounced diversity in domain variances among clients becomes evident, intricately linked to the complexity of their datasets. For instance, comparing the feature distribution between Synth and MNIST shows a visibly more scattered pattern in Synth due to its higher complexity, whereas MNIST, being comparatively easier to learn, displays a more structured distribution. It becomes evident that computing a single local averaged prototype for one class per client is 'unfair', given the differing richness of feature distribution information among clients. Particularly, for 'hard' clients with complex datasets such as SVHN, employing multiple local prototypes becomes imperative to capture the scattered feature distribution comprehensively. Hence, we propose the first-level prototype generation, namely local prototype clustering. Instead of a straightforward averaging approach, our method involves initially clustering the feature vectors of all same-class local samples, forming several local clustered prototypes as a set of local representations.

\[_{k}^{m}=\{p_{k,j}^{m}\}_{j=1}^{J_{k,m}}}{{}}\{h(_{i})|(_{i},y_{i}) _{k}^{m}\},\] (3)

where \(J_{k,m}\) and \(p_{k,j}^{m}\) represents the number of local clustered prototypes and the \(j\)-th local clustered prototype of class \(m\) on client \(k\) clustered from the set of feature vectors \(\{h(_{i})|(_{i},y_{i})_{k}^{m}\}\). It is important to note that the number of local clustered prototypes may differ across various classes and clients. To determine these prototypes, we employ the parameter-free clustering algorithm, FINCH , utilizing cosine similarity as the clustering metric. We also conduct additional experiments on comparison with other clustering methods in Supplementary Material. This choice ensures the alignment of the number of local clustered prototypes with the sparsity of the domain distribution. By leveraging this approach, we enhance the representation of distinct feature distributions, preventing the overdrift of local averaged prototypes towards densely concentrated regions in the feature space.

**Global Prototype Clustering on Server.** Distributing all local clustered prototypes among clients poses challenges due to the extra communication cost and privacy concerns. Hence, we introduce the second level of prototype generation, namely global prototype clustering, which can be formulated as:

\[^{m}=\{g_{j}^{m}\}_{j=1}^{C_{m}}}{{}}^{m}=\{_{k}^{m} \}_{k=1}^{K},\] (4)

where \(_{m}\) and \(g_{j}^{m}\) denote the number of global clustered prototypes and the \(j\)-th global clustered prototype of class \(m\) on the server from the local collected prototypes set \(^{m}=\{_{k}^{m}\}_{k=1}^{K}\). Through the second-level global clustering, we significantly reduce the number of prototypes that the server must distribute to clients compared to distributing all locally clustered prototypes, alleviating potential communication costs. Dual-level clustering also addresses privacy concerns that arise from original local clustered prototypes potentially revealing client-specific features and the corresponding results can be found in Supplementary Material.

### \(\)-Sparsity Prototype Loss

Unlike previous methods in FedPL, which generate a single prototype per class, our approach employs dual-level clustering to create multiple prototypes for each class, thereby capturing valuable variance information. This multiplicity of prototypes could potentially lead to overlapping feature representations among different classes, especially in challenging client scenarios. To mitigate this risk, we introduce a novel \(\)-sparsity prototype loss, inspired by the InfoNCE-based loss. Our newly designed \(\)-sparsity prototype loss enhances inter-class feature distribution sparsity and maintains balanced feature representation distances within classes, unlike the traditional InfoNCE-based loss. The formulation of the \(\)-sparsity prototype loss is detailed below:

\[_{}=_{contra}+_{corr}.\] (5)

The first contrastive term can be formulated as:

\[_{contra}=-}^{y_{i}}} (h(_{i}),g^{y_{i}})/)}}{_{g }(h(_{i}),g)/)}},\] (6)

where \(\) is the temperature hyper-parameter that controls the concentration strength of the similarity , and \(=\{^{m}\}_{m=1}^{M}\) is the set of all global clustered prototypes. \(s_{}(,)\) in the first term is the modified \(\)-sparsity cosine similarity metric between the feature vector \(h(_{i})\) and the prototype \(g^{m}\) from class \(m\) and can be formulated as:

\[s_{}(h(_{i}),g^{m})=(_{i})}{||h( _{i})||}}{||g^{m}||})^{},\] (7)

where \((0,1)\). This metric serves to compel the feature extractor to generate feature outputs closely aligned with the global clustered prototypes of their respective classes while distancing them from other global prototypes. It achieves this by maximizing intra-class similarity and minimizing inter-class similarity. As our feature vectors have positive values, cosine similarity always falls within the range of \(\). By introducing a sparsity factor \(\) to the similarity and applying it as a power, all similarity values are elevated. However, due to the smaller denominator component representing similarity with other classes in the \(_{}\) function, the impact of the concave function \(()^{}\) is more pronounced. This emphasis on maximizing inter-class distance is what we denote as \(\)-**sparsity** operation, which directs more attention toward expanding the overall feature distribution to a broader range. Consequently, it mitigates the issue of feature distributions within one class being excessively dispersed and overlapping with feature distributions of other classes.

This modification does indeed lead to an unintended consequence: an increase in intra-class distance due to heightened similarity between the feature vector and prototypes of the corresponding class, resulting from the concavity of \(()^{}\). This brings us to the second correction term of our \(\)-sparsity prototype loss:

\[_{corr}=||_{g^{y_{i}}^{y_{i}}}s _{}(h(_{i}),g^{y_{i}})-C_{y_{i}}||_{2}.\] (8)

The latter term within the \(\)-sparsity prototype loss serves as a corrective measure, which focuses on pushing the average cosine similarity closer to 1. This correction measure aims to counterbalance the increase in intra-class distance stemming from the adjustment introduced by \(\).

An additional Cross-Entropy (CE) loss  is employed to train the classifier and derive prediction results, which can be formulated as:

\[_{CE}=_{(_{i},y_{i})_{k}}- _{y_{i}}(f(h(_{i}))).\] (9)

The total local loss, combining the previously mentioned loss functions, is expressed as follows:

\[_{local}=_{}+_{CE}.\] (10)

Here, \(\) serves as a hyper-parameter that regulates the balance between the \(\)-sparsity prototype loss and the CE loss. This formulation allows for a unified and weighted consideration of the \(\)-sparsity prototype loss.

In summary, our FedPLVM operates as follows in each training round: Initially, each client generates feature vectors for all local samples and clusters these into several local clustered prototypes. These local clustered prototypes are then uploaded to the server, which aggregates them into distinct prototype sets for various classes and further clusters them to form global clustered prototypes. Concurrently, the server collects local models from all clients and consolidates them into a unified global model. The server then distributes all global clustered prototypes and global model to clients. Subsequently, each client utilizes the global clustered prototypes to train the global model on its private dataset using \(_{local}\), obtaining its local model. Finally, clients employ their local models to generate new feature vectors and repeat the aforementioned procedure in the next training round. For a detailed insight into FedPLVM, refer to Algorithm 1 in the Supplementary Material.

**Comparison with FPL .** FPL studies on FL among clients with distinct domain datasets and is considered a state-of-the-art method. FPL is specifically designed to address imbalances in client distribution across domains, aiming to neutralize the skewed influence of domains with more clients on the global model training. **In contrast**, our study concentrates on the unequal learning obstacles that vary by domain, a challenge that persists regardless of equal client distribution, leading to distinct learning hurdles across domains and thereby leading to our two key operations. **Firstly**, although both our method and FPL employ prototype clustering, the objectives and implementations markedly differ. FPL's clustering is intended to harmonize the impact of local prototypes from each domain, involving only global (server-level) clustering. Conversely, our method integrates dual-level clustering at both the client (local) and server (global) levels. Our technique distinguishes itself by performing local prototype clustering, capturing critical variance and not just the mean data, which is particularly crucial in hard domains. At the global level, our clustering aims to reduce communication overhead and privacy risks by limiting the prototype variety each client sends, thereby enhancing both efficiency and privacy. **Secondly**, we introduce an innovative \(\)-sparsity prototype loss that features a corrective component to reasonably utilize the variance information to reduce feature similarity across different classes while boosting it within the same class, promoting more effective and stable learning.

## 5 Experiments

**Datasets.** We evaluate our proposed algorithm on three datasets: Digit-5 , Office-10  and DomainNet . **Digit-5** is a dataset for digits recognition, consisting of 5 domains: MNIST, SVHN, USPS, Synth and MNIST-M. **Office-10** is a dataset for office item recognition, consisting of 4 domains: Amazon, Caltech, DSLR and Webcam. **DomainNet** is a large-scale classification dataset, consisting of 6 domains: Clipart, Infograph, Painting, Quickdraw, Real and Sketch.

**Baselines.** We compare our algorithm with classic FL methods: FedAvg , FedProx , FedPL methods: FedProto , FedPCL , FPL and FL method on feature skew: FedFA .

**Implement Details.** We employ the ResNet10  as our backbone model, configuring the feature vectors' dimension to 512. The optimization is done using the SGD optimizer, employing a learning rate of \(0.01\), momentum of \(0.5\), and a weight decay of \(1e-5\). For Digit-5, Office-10 and DomainNet, we use 5, 4 and 6 clients respectively. The client data is independent and identically distributed (i.i.d.) and non i.i.d. results can be found in Supplementary Material. It is important to note that each client operates within distinct data domains, meaning different datasets. For Digit-5 and Office-10, each client possessed 100 training samples and 1000 test samples. Global communication rounds are fixed at \(T=50\) for Digit-5 and \(T=80\) for Office-10. Each local training epoch consists of \(E=2\) iterations. We maintain default hyper-parameter values: \(=0.07\), \(=0.25\), and \(=100\). As for DomainNet, we followed the setup in FedPCL using a 10-class subset. Each client employs 300 training samples and all test samples (approximately 1000, vary on domains). \(=1\) and \(T=200\) for DomainNet. The batch size is set at 32 for all datasets. Details regarding hyper-parameter settings will be elaborated in Sec. 5.2. For fair comparisons, we conduct each setting for 5 experiments and report the average result.

### Performance Comparison

We compare our proposed method with the state-of-the-art methods using the Digit-5, Office-10 and DomainNet datasets, as detailed in Tab. 1, Tab. 2 and Tab. 6. Our method demonstrates significant improvements in average accuracy over baseline methods for both datasets. A closer examination of domain-specific results reveals a more pronounced enhancement in performance on domains that are more challenging to learn. For instance, within the Digit-5 dataset, our method achieves a \(5.3\%\) increase in accuracy for the SVHN domain, which is more difficult, and a \(0.58\%\) increase for the MNIST dataset, considered easier. This aligns with the goal of our method, which is to address the disparate learning challenges across various domains, enhancing fairness and aiding in performance improvement in harder domains. Similar improvements can also be observed in Office-10 and DomainNet in Sec. C of Appendix.

### Ablation Study

To evaluate the effectiveness of each component within our proposed methodology, we conducted a series of ablation studies using the Digit-5 dataset.

#### 5.2.1 Impact of Dual-Level Prototype Generation.

In this subsection, we focus on examining the impact of dual-level clustered prototypes, as demonstrated by the results in Tab. 3. The first row illustrates outcomes derived from calculating local prototypes by averaging features of samples within the same class, and global prototypes through the direct averaging of these local prototypes. This approach, utilizing straightforward averaging for both levels, yields the least effective performance. This is attributed to its failure to capture variance information, a non-trivial aspect in this context. Moreover, adopting a solely global clustering approach does not significantly enhance performance. This is attributed to the fact that while global clusters incorporate inter-domain variance, they overlook the critical aspect of high intra-domain sample variance, particularly in domains that are challenging to learn. To elucidate this, we present a t-SNE visualization analysis in Fig. 3 comparing the three prototype generation methods. Our approach fosters a more generalizable decision boundary, as illustrated in the visualization. This ability to effectively capture variance at both local and global levels is key to why our dual-level clustering method outperforms the others.

To further explore alternative methods, we examine the efficacy of directly transferring all local prototypes to each client without employing any aggregation or clustering techniques. This approach

    &  \\   & MNIST & SVHN & USPS & Synth & MNIST-M & Avg & \(\) \\  FedAvg & 84.98 \(\) 0.92 & 29.38 \(\) 1.06 & 82.36 \(\) 1.18 & 47.00 \(\) 0.73 & 53.14 \(\) 0.78 & 59.37 & - \\ FedProx & 85.72 \(\) 1.50 & 28.86 \(\) 1.23 & 82.30 \(\) 0.75 & 46.78 \(\) 1.10 & 52.60 \(\) 2.37 & 59.25 & -0.12 \\ FedProto & 88.60 \(\) 0.72 & 31.94 \(\) 1.58 & 85.54 \(\) 0.34 & 51.82 \(\) 1.12 & 56.86 \(\) 0.42 & 62.95 & +3.58 \\ FedPCL & 88.84 \(\) 1.08 & 39.70 \(\) 2.25 & 84.74 \(\) 0.72 & 54.70 \(\) 1.18 & 59.96 \(\) 1.34 & 65.59 & +6.22 \\ FedFA & 89.96 \(\) 0.55 & 38.96 \(\) 1.69 & 85.86 \(\) 0.36 & 58.04 \(\) 1.06 & 61.38 \(\) 0.98 & 66.74 & +7.37 \\ PPL & 9.10 \(\) 1.39 & 36.78 \(\) 1.88 & 86.10 \(\) 0.66 & 57.36 \(\) 1.96 & 64.02 \(\) 1.38 & 66.88 & +7.51 \\
**Ours** & **90.70 \(\) 0.39** & **42.08 \(\) 1.59** & **86.24 \(\) 1.37** & **60.08 \(\) 1.47** & **67.16 \(\) 0.77** & **69.25** & **+9.88** \\   

Table 1: **Test accuracy on Digit-5**. Avg means average results among all clients. Details in Sec. 5.1.

    &  \\   & Amazon & Caltech & DSLR & Webcam & Avg & \(\) \\  FedAvg & 48.26 \(\) 1.92 & 35.11 \(\) 0.96 & 57.29 \(\) 1.47 & 71.75 \(\) 0.80 & 53.10 & - \\ FedProx & 47.74 \(\) 0.65 & 36.44 \(\) 1.92 & 56.25 \(\) 4.42 & 73.45 \(\) 0.80 & 53.47 & +0.37 \\ FedProto & 49.31 \(\) 2.18 & 36.07 \(\) 0.91 & 57.38 \(\) 2.55 & 79.05 \(\) 2.40 & 55.45 & +2.35 \\ FedPCL & 53.65 \(\) 2.33 & 38.93 \(\) 2.73 & 58.13 \(\) 5.08 & 78.64 \(\) 0.83 & 57.34 & +4.24 \\ FedFA & 56.46 \(\) 2.15 & 40.91 \(\) 2.39 & 60.00 \(\) 4.68 & 78.58 \(\) 1.86 & 58.99 & +5.89 \\ FPL & 54.38 \(\) 1.02 & 38.24 \(\) 2.38 & 61.25 \(\) 3.19 & 80.34 \(\) 1.73 & 58.55 & +5.45 \\
**Ours** & **57.03 \(\) 1.45** & **42.71 \(\) 1.04** & **61.50 \(\) 2.02** & **81.36 \(\) 1.86** & **60.65** & **+7.55** \\   

Table 2: **Test accuracy on Office-10**. Details in Sec. 5.1.

   Local & Global & MNIST & SVHN & USPS & Synth & MNIST-M & Avg & Variance \\  Avg & Avg & 86.90 & 33.10 & 83.90 & 53.40 & 61.40 & 63.54 & 1.725 \\ Avg & Cluster & 89.40 & 37.00 & 85.50 & 56.60 & 63.50 & 66.40 & 1.393 \\
**Cluster** & **Cluster** & **90.20** & **43.70** & **86.90** & **61.20** & **65.40** & **69.48** & **0.825** \\   

Table 3: **Comparison on prototype generation methods. Variance** means the average distance from the normalized feature vector of one sample to its corresponding class feature center (i.e. the averaged prototype). Results are then used for visualization in Fig. 3. Details in Sec. 5.2.1.

preserves variance information but raises significant privacy concerns, unlike our dual-level clustering method, which offers enhanced privacy protection. As demonstrated in Tab. 4, directly transferring all local prototypes yields performance comparable to our dual-cluster approach. However, it requires transmitting approximately five times as many prototypes in each training round.

#### 5.2.2 Impact of \(\)-Sparsity Prototype Loss.

To evaluate the specific impact of our proposed \(\)-sparsity prototype loss, we conduct experiments comparing both contrastive and corrective loss terms. Results in Tab. 5 demonstrates that employing the contrast term led to a \(1.46\%\) improvement in final average accuracy, while the correction term resulted in a \(0.99\%\) enhancement. Combining all components yields the best performance, showcasing a \(2.29\%\) improvement. Our investigation also focuses on the sparsity parameter \(\), depicted in Fig. 4. We find that varying \(\) within the range of \((0,1)\) consistently outperforms the baseline setting with \(=1\). After careful consideration of numerical stability, we opt for an \(\) value of \(0.250\).

Comparing our method to the prior work FPL, which also integrates its proposed prototype loss with the CE loss, we explore different prototype loss weights, denoted by \(\). The results depicted in Fig. 4 substantiate the superiority of our approach over FPL across various \(\) settings.

   Global Clustering & Avg & Avg \(\#\) of Prototypes & Privacy Preservation & Communication Cost \\  w/o & 69.18 \(\) 0.77 & 100.92 & \(\) & \(4.76\) \\
**w/** & **69.47 \(\) 0.71** & **21.20** & \(\) & \(1\) \\   

Table 4: **Comparison between w/o and w/ global clustering. w/o** means the server distributes all local clustered prototypes to the clients for local training. **Avg \(\#\) of prototypes** is the average number of prototypes each client receives from the server during each global round. Details in Sec. 5.2.1.

Figure 3: **Visualization of different prototype generation methods. The first row averages feature vectors locally and averages local prototypes globally. The second row averages feature vectors locally and clusters local prototypes globally. The last row (ours) clusters feature vectors locally and clusters local clustered prototypes globally. The last column Total is the visualization of mixing the feature vectors from all datasets. Details in Sec. 5.2.1.**

   Contrast & Correction & Avg & \(\) \\  w/o & w/o & 66.96 \(\) 0.85 & - \\ w/ & w/o & 68.42 \(\) 0.95 & +1.46 \\ w/o & w/ & 67.95 \(\) 0.63 & +0.99 \\
**w/** & **w/** & **69.25 \(\) 0.62** & **+2.29** \\   

Table 5: **Comparison on components of \(\)-sparsity prototype loss. Contrast and Correction stand for the contrastive and corrective loss term in the total \(\)-sparsity loss respectively. Avg is the average accuracy result for all clients. Details in Sec. 5.2.2.**

### Impact of Temperature \(\).

To assess the impact of the contrastive temperature (\(\)) on our model's performance, we conduct the following experiment. The results, depicted in Fig. 5, demonstrate that our method consistently surpasses the baselines across a range of temperatures (refer to Tab. 1 and Tab. 2 for details, where FPL exhibits the highest baseline performance at \(66.88\%\) and \(58.55\%\) respectively). Further analysis indicates that an optimal temperature setting for our model on Digit-5 is \(=0.070\). Another results on Office-10 identifies an optimal temperature setting for our model at \(=0.045\), while we maintain \(=0.070\) for stability in performance comparisons. It is noteworthy that temperatures either significantly higher or lower than this value lead to training difficulties due to numerical instability.

## 6 Conclusion

In this paper, we start by noting the significant difference in domain-specific representation variance across various datasets within the context of federated learning involving heterogeneous data domains. Traditional methods relying on averaged prototypes, calculated as the mean values of feature representations from samples within the same class, consistently fail to capture this essential local information. Our proposed approach, FedPLVM, addresses this issue by implementing a dual-level prototype generation method. This method leverages first-level local clustering to manage variance information and employs second-level global clustering to streamline communication complexity while ensuring privacy. Additionally, we introduce an \(\)-sparsity prototype loss that prioritizes expanding the inter-class distance, considering the diverse cross-domain variances, and includes a correction term to subsequently reduce the intra-class distance. Comprehensive experiments have been conducted to validate the effectiveness of FedPLVM, demonstrating a significant accuracy improvement over state-of-the-art federated prototype learning methods.

Figure 4: **Impact of \(\) sparsity and \(\) prototype loss weight. The left figure shows the accuracy of two selected datasets and the average accuracy among all clients with different \(\). The right figure shows the effects of different \(\) for both FPL and our proposed approach. Details in Sec. 5.2.2.**

Figure 5: **Impact of \(\). The left figure shows the impact on the Digital-5 dataset while the right figure shows on the Office-10 dataset. Average means the average test accuracy with variance among all clients. Details in Sec. 5.3.**