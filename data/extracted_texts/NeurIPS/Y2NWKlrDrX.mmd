# Conformal Inverse Optimization

Bo Lin

University of Toronto

blin@mie.utoronto.ca

&Erick Delage

HEC Montreal

Mila - Quebec AI Institute

erick.delage@hec.ca

&Timothy C. Y. Chan

University of Toronto

Vector Institute

tcychan@mie.utoronto.ca

###### Abstract

Inverse optimization has been increasingly used to estimate unknown parameters in an optimization model based on decision data. We show that such a point estimation is insufficient in a prescriptive setting where the estimated parameters are used to prescribe new decisions. The prescribed decisions may be of low-quality and misaligned with human intuition and thus are unlikely to be adopted. To tackle this challenge, we propose conformal inverse optimization, which seeks to learn an uncertainty set for the unknown parameters and then solve a robust optimization model to prescribe new decisions. Under mild assumptions, we show that our method enjoys provable guarantees on solution quality, as evaluated using both the ground-truth parameters and the decision maker's perception of the unknown parameters. Our method demonstrates strong empirical performance compared to classic inverse optimization.

## 1 Introduction

Inverse optimization (IO) is a supervised learning approach that fits parameters in an optimization model to decision data. The fitted optimization model can then be used to prescribe future decisions. Such problems naturally arise in AI applications where human preferences are not explicitly given, and instead need to be inferred from historical decisions. For this pipeline to succeed in practice, the prescribed decision should not only be of high quality but also align with human intuition (i.e., perceived to be of high-quality). The latter encourages algorithm adoption (Chen et al., 2023; Donahue et al., 2023), which is critical in many real-world settings (Liu et al., 2023; Sun et al., 2022).

As an example, rideshare platforms, e.g., Uber and Lyft, provide a shortest-path to the driver at the start of a trip based on real-time traffic data (Nguyen, 2015). The driver then relies on her perception of the road network formed through past experience to evaluate the path. The driver may deviate from the suggested path if it is perceived to be low-quality. Although seasoned drivers are often capable of identifying a better path due to their tacit knowledge of the road network (Merchan et al., 2022), such deviations impose operational challenges as it may cause rider safety concerns and affect downstream decisions, such as arrival time estimation, trip pricing, and rider-driver matching (Hu et al., 2022). Therefore, the platform may be interested in leveraging historical paths taken by drivers to suggest high-quality paths for future trips, as evaluated using both the travel time and the driver's perception.

In this paper, we first show that the classic IO pipeline may generate decisions that are low-quality and misaligned with human intuition. We next propose conformal IO, which first learns an uncertainty set from decision data and then solves a robust optimization model with the learned uncertainty set to prescribe decisions. Finally, we show that the proposed approach has provable guarantees on the actual and perceived solution quality. Our contributions are as follows.

**New framework**. We propose a new IO pipeline that integrates i) a novel method to learn uncertainty sets from decision data and ii) a robust optimization model for decision recommendation.

**Theoretical guarantees**. We prove that, with high probability, the learned uncertainty set contains parameters that make future observed decisions optimal. This coverage guarantee leads to bounds on the optimality gap of the decisions from conformal IO, as evaluated using the ground-truth parameters and the decision maker's (DM's) perceived parameters.

**Performance**. Through experiments, we demonstrate strong empirical performance of conformal IO compared to classic IO and provide insights into modeling choices.

## 2 Literature Review

**Inverse optimization**. IO is a method to estimate unknown parameters in an optimization problem based on decision data (Ahuja and Orlin, 2001; Chan et al., 2014; Chan and Kaw, 2020). Early IO papers focus on deterministic settings where the observed decisions are assumed to be optimal to the specified optimization model. Recently, IO has been extended to stochastic settings where the observed decisions are subject to errors and bounded rationality. Progress has been made to provide estimators that are consistent (Aswani et al., 2018; Dong et al., 2018), tractable (Chan et al., 2019; Tan et al., 2020; Zattoni Scroccaro et al., 2024), and robust to data corruption (Mohajerin Esfahani et al., 2018). Our paper is in the stochastic stream. Unlike existing methods that provide a point estimation of the unknown parameters, we learn an uncertainty set that can be used in a robust optimization model.

**Data-driven uncertainty set construction.** Recently, data have become a critical ingredient to design the structure (Delage and Ye, 2010; Mohajerin Esfahani et al., 2018; Gao and Kleywegt, 2023) and calibrate the size (Chemreddy et al., 2022; Sun et al., 2023) of an uncertainty set. Our paper is related to the work of Sun et al. (2023) who first use an ML model to predict the unknown parameters and then calibrate an uncertainty set around the prediction. However, this approach does not apply in our setting as it requires observations of the unknown parameters, which we do not have access to. Our paper presents a new approach to calibrating uncertainty sets using decision data.

**Estimate, then optimize.** Conformal IO belongs to a family of data-driven optimization methods called "estimate, then optimize" (Elmachtoub et al., 2023). Recent research suggests that even small estimation errors may be amplified in the optimization step, resulting in significant decision errors. This issue can be mitigated by training the estimation model with decision-aware losses (Wilder et al., 2019; Mandi et al., 2022) and robustifying the optimization model (Chan et al., 2023). We take a similar approach as the second stream, yet deviate from them by i) utilizing decision data instead of observations of the unknown parameters, and ii) focusing on both the ground-truth and perceived solution quality, the latter of which has not been studied in this stream of literature.

**Preference learning.** Preference learning has been studied in the context of reinforcement learning and has recently attracted significant attention due to its application in AI alignment (Ji et al., 2023). Existing methods focus on learning a reward function/decision policy that is maximally consistent with expert decision trajectories (Ng and Russell, 2000; Wu et al., 2024) or labeled preferences in the form of pair-wise comparison/ranking (Wirth et al., 2017; Christiano et al., 2017; Rafailov et al., 2024). We enrich this stream of literature in two ways. First, we leverage _unlabeled_ decision data that are not state-action trajectories, but solutions to an optimization model. Second, instead of learning a policy that imitates expert behaviors, we aim to extract common wisdom from decision data crowd-sourced from DMs who are not necessarily experts to encourage algorithm adoption.

## 3 Preliminaries

In this section, we first present the problem setup (Section 3.1) and then describe the challenges with the classic IO pipeline (Section 3.2). Finally, we provide intuition on why robustifying the IO pipeline would help (Section 3.3).

### Problem Setup

**Data generation.** Consider a _forward optimization_ problem

\[(,):( )}{}\ f(,)\] (1)where \(^{n}\) is the decision vector whose feasible region \(()\) is non-empty and is parameterized by exogenous parameters \(^{m}\), \(^{d}\) is a parameter vector, and \(f:^{n d}\) is the objective function. Suppose \(\) is distributed according to \(_{}\) supported on \(\). There exists a ground-truth parameter vector \(^{*}\) that is unknown to the DM. Instead, the DM obtains a decision \(}\) by solving \((},)\) where \(}\) is a noisy perception of \(^{*}\). We assume that, while the distribution \(_{}\) of \(}\) is unknown, it is supported on a known bounded set \(^{d}\) and that \(^{*}\). Let \(_{(,)}\) denote the joint distribution of \(}\) and \(\), \(}:^{n}\) be an oracle that returns an optimal solution to \(\) drawn uniformly at random from \(^{}(,):=*{ argmin}\{f(,)\,|\,( )\}\).

**Objective function.** We focus on cases where \(f\) is linear in \(\) and convex in \(\), i.e., \(f(,)=_{i[d]}_{i}f_{i}()\) where \(f_{i}:^{n}\) are convex basis functions. This generalizes the linear objective \(f(,)=^{T}\). Moreover, \(\) with this objective function can be treated as a multi-objective optimization model, which has been used to model routing preferences (Ronnqvist et al., 2017), radiation therapy planning (Chan et al., 2014), and portfolio optimization (Dong and Zeng, 2021). In this setting, the optimal solution to \(\) is invariant to the scale of \(\), i.e., if \(^{}(,)\), then \(^{}(,)\) for any \(_{+}\). So, we set \(=\{^{d}\,\|\, {}\|_{2}=1\}\) without loss of generality.

**Learning task.** Given a dataset of \(N\) decision-exogenous parameter pairs \(=\{}_{k},_{k}\}_{k=1}^{N}\), we are interested in finding a decision policy \(}:^{n}\) to suggest decisions for future \(\). We require \(}()()\) for any \(\). As discussed later, \(}()\) is usually generated by solving an optimization model that may have multiple optimal solutions. So we consider randomized policies (e.g., uniformly sample from a set of optimal solutions). This is nonrestrictive because a deterministic policy can be recovered from a randomized policy that samples the deterministic solution with probability one.

As a running example, \(\) may represent a shortest path problem, where \(_{k}\) specifies the origin and destination of a trip, \(^{*}\) indicates the ground-truth travel times on each road segment, while \(}_{k}\) is a driver's perception of \(^{*}\) (i.e., perceived travel times). Decision \(_{k}\) corresponds to a path taken by a driver based on her perceived travel times. Given a set of historical trips \(\{_{k},}_{k}\}_{k[N]}\), our goal is to derive a routing policy \(}\) that can provide path recommendations for future origin-destination pairs.

#### 3.1.1 Assumptions

**Assumption 1** (I.I.D. Samples).: _The dataset \(\) is generated using \(}_{k}:=}(}_{k},_{k})\) where \((}_{k},_{k})\) are i.i.d. samples from \(_{(,)}\) for all \(k[N]\)._

**Assumption 2** (Bounded Inverse Feasible Set).: _There exists a constant \(_{+}\) such that, for any \(,^{}^{}\,(},)\), for some \(}()\) and \(\), we have \(\|-^{}\|_{2}\), where_

\[^{}(,):=\{^{d}\,\,^{}( ,),\|\|_{2}=1\}.\] (2)

**Assumption 3** (Bounded Divergence).: _There exists a constant \(_{+}\) such that \(\|(})-^{*}\|_{2}\)._

Assumption 1 is standard in the ML and IO literature. Assumption 2 is mild because \(^{}\,(},)\) is by definition bounded and is usually much smaller than \(\). Assumption 3 states that the \(l_{2}\) distance between the expected perceived parameters and the ground-truth parameters is upper bounded. It is reasonable in many real-world settings. For example, a driver's perceived travel cost (\(}\)) should not be too different from the travel time (\(^{*}\)) as the latter is an important factor that drivers consider.

#### 3.1.2 Evaluation Metrics

**Definition 1**.: _The actual optimality gap (AOG) of a decision policy \(}\) is defined as_

\[(}):=[f(^{*},}())-f(^{*}, }(^{*},))]\] (3)

_where the expectation is taken over the joint distribution of the random variable \(\) and the decision sampled using the possibly randomized policy \(}\)._

**Definition 2**.: _The perceived optimality gap (POG) of a decision policy \(}\) is defined as_

\[(}):=[f(},}())-f(},}(},))].\] (4)

_where the expectation is taken with respect to the randomness in \(}\), \(\), and possibly \(}\)._AOG is an objective performance measure. Achieving a low AOG means that \(}\) can generate high-quality decisions. In contrast, POG is a subjective measure that depends on the DM's perception of the problem. Achieving a low POG is critical to mitigate algorithm aversion (Burton et al., 2020).

### An Inverse Optimization Pipeline

Finding \(}\) is challenging for three reasons. First, unlike many ML tasks where the prediction target is unconstrained, we require \(}()\) to be feasible to \(\) which may involve a large number of constraints. Supervised learning approaches that predict \(}\) based on \(\) can often fail as they typically do not provide feasibility guarantees. An optimization module is often needed to recover feasibility or produce feasible solutions based on \(\) and some estimated \(}\). Second, we do not directly observe \(^{*}\) or \(}\), which precludes using classic ML techniques to estimate them. Finally, AOG and POG may not necessarily align with each other, so we are essentially dealing with a bi-objective problem.

In light of the first two challenges, a classic IO pipeline (visualized in Figure 1) has been proposed to first obtain a point estimation \(}\) of the unknown parameters and then employ a policy \(}_{}():=}(},)\) to prescribe decisions for any \(\)(Ronnqvist et al., 2017; Babier et al., 2020). Specifically, we can estimate the parameters by solving the following _inverse optimization_ problem

\[():}{} \ _{k[N]}(}_{k},^{}(,_{k})),\] (5)

where \(\) is a non-negative loss function that returns 0 when \(}_{k}^{}(,_{k})\). For instance, the following loss function is commonly used in the literature.

**Definition 3**.: _The sub-optimality loss of \(\) is given by_

\[_{S}(},^{}(,)):=_{()}f(,})-f(,).\] (6)

The sub-optimality loss penalizes the optimality gap achieved by the observed decision under the estimated parameters. As remarked by Mohajerin Esfahani et al. (2018), this loss function has better computational properties than its alternatives as it is convex in the unknown parameters. In fact, when the dataset \(\) is large in size and the unknown parameters \(\) are high-dimensional, the sub-optimality loss is usually the only loss function that leads to a tractable inverse problem, although it does not enjoy properties such as statistical consistency (see Chan et al. (2023) for detailed discussions). While such a trade-off is acceptable in some applications, we suggest that it is undesirable in our setting because the resulting policy can achieve arbitrarily large AOG and POG. To see this, consider the following example that satisfies Assumptions 1-3. This example is visualized in Figure 2.

**Example 1**.: _Let \((,u)\) be the following problem_

\[ _{1}x_{1}+_{2}x_{2}\] (7a) \[\  x_{1}+ux_{2} u\] (7b) \[0 x_{1} u\] (7c) \[0 x_{2} 2.\] (7d)

_Let the ground-truth \(^{*}=((/4),(/4))\) and \(=\{u\}\) where \(u>1\) is a real constant. We are given a dataset \(=\{}_{k},u\}_{k=1}^{N}\) where \(}_{k}=}(}_{k},u)\) with \(}_{k}\) uniformly and independently drawn from \(=\{(,\,)\,|\,(0,/2)\}\) for all \(k[N]\)._

Figure 1: Classic and conformal IO pipelines.

**Lemma 1**.: _In Example 1, let \(}_{N}\) denote an optimal solution to \(()\) with the sub-optimality loss (6), we have \((}_{N}=_{u}) 1\) as \(N\), where \(_{u}:=(1/},u/})\)._

Lemma 1 shows that, when using \(()\) with the sub-optimality loss to estimate the unknown parameter in Example 1, the probability of the estimated parameter being \(_{u}\) converges to one asymptotically. This implies that asymptotically we are almost certain that \(}_{}(u)=}(_{u},u)\), i.e. the policy that samples uniformly from the facet corresponding to the constraint \(x_{1}+ux_{2} u\). As a result, \(}_{}\) can achieve arbitrarily large AOG and POG when \(u\) is set to a large enough value.

**Proposition 1**.: _In Example 1, let \(}_{}(u)=}(_{u},u)\). For any \(v_{+}\) there exists some \(>1\) such that \((}_{})>v\) and \((}_{})>v\) for any \(u>\)._

### Robustifying the Inverse Optimization Pipeline

A natural idea to improve the AOG and POG of \(}\) is to robustify the decision pipeline. Specifically, instead of solving \(\) with some estimated parameters \(}\), we solve the following _robust forward optimization problem_ with an uncertainty set around \(}\) to prescribe decisions (final steps in Figure 1).

\[((},),) :()}{}\; {(},)}{}\;f (,)\] (8)

where \(\) is an uncertainty set with \(}\) being its center and \(\) representing parameters that control its shape/size. Given the support set \(\) defined in Section 3.1, we focus on the following uncertainty set.

\[(},):=\{^{d}\,| \,\|\|_{2}=1,\;^{}} \}\] (9)

where \((0,]\) represents the max angle between \(}\) and any vector in the uncertainty set.

**Remark 1**.: _An alternative approach to robustify the IO pipeline is to replace \(\) with a distributionally robust IO for parameter estimation (Mohajerin Esfahani et al., 2018). However, this approach would not help as \(_{u}\) is still optimal to the distributionally robust IO in Example 1. So, the AOG and POG can still be arbitrarily large. See Appendix A.3 for complete statement and discussions._

Now, in Example 1, we analyze the performance of a policy that utilizes \(\) to prescribe decisions.

**Lemma 2**.: _In Example 1, let \(}_{}(u)\) be an optimal solution to \(((}_{N},),u)\) where \(}_{N}\) is an optimal solution to \(()\) with the sub-optimality loss (6). When \((0,/2)\), we have \([(_{})=0] 1\) and \([(}_{})</2 ] 1\) as \(N\)._

Lemmas 2 shows that, when using \(\) to prescribe new decisions, the probability of achieving upper-bounded AOG and POG converges to one as \(N\) goes to infinity, as long as \((0,/2)\). These bounds are independent of \(u\), in contrast to the AOG and POG of classic IO that can be arbitrarily large as \(u\) changes (Proposition 1). However, the performance of this approach still depends on the choice of \(\), which is non-trivial when \(\) is more complex than a two-dimensional linear program. We address this problem next.

Figure 2: Illustration of Example 1. The gray areas are the feasible region \((u)\). The black arrows are the ground-truth parameter \(^{*}\). The gray arrows are the extreme rays of \(\). The blue and green arrows are the point estimation \(}\). The green area is the uncertainty set \(}(},)\). The black circles are the optimal solution to \((^{*},u)\). The blue and green circles are the suggested decisions. Note that \(}_{}\) may suggest any decisions on the facet of \(x_{1}+ux_{2} u\), which are omitted for clarity.

Conformal Inverse Optimization

In this section, we present a principled approach to learn uncertainty sets that lead to provable performance guarantees. As presented later, the learned uncertainty set contains parameters that make the next DM's decision optimal with a specified probability. We call this approach conformal IO due to its connection to conformal prediction (Vovk et al., 2005), which aims to predict a set that contains the next prediction target with a specified probability. Our approach first converts each context-decision observation \((_{k},\,}_{k})\) to a parameter set \(^{}(_{k},\,}_{k})\) that contains all the parameters that explain \(}_{k}\) under \(_{k}\). We then adapt conformal prediction to produce a set that has \(\) probability of containing at least one member of the next sampled \(^{}(,})\). Note that if \(^{}(,})\) is a singleton almost surely, the approach is equivalent to applying conformal prediction to \(_{k}\) directly. As illustrated in Figure 1, there are three training steps in conformal IO: i) data split, ii) point estimation, and iii) uncertainty set calibration. We present these steps in Section 4.1 and analyze the properties of conformal IO in Section 4.2.

### Learning an Uncertainty Set

**Data split.** We first split the dataset \(\) into training and validation sets, namely \(_{}\) and \(_{}\). Let \(_{}\) and \(_{}\) index \(_{}\) and \(_{}\), respectively, while \(N_{}=|_{}|\) and \(N_{}=|_{}|\).

**Point estimation.** Given a training set \(_{}\), we apply data-driven IO techniques to obtain a point estimation \(}\) of the unknown parameters. The most straightforward way is to solve \((_{})\) with any loss function. Alternatively, one may consider using end-to-end learning and optimization methods that do not require observations of the parameter vectors, e.g., the one proposed by Berthet et al. (2020). The point estimation can also come from other sources, e.g., from an ML model that predicts the parameters. Our calibration method is independent of the point estimation method.

**Uncertainty set calibration.** Given a point estimation \(}\), we calibrate an uncertainty set that, with a specified probability, contains parameters that make the next observed decision optimal. This property is critical for the results in Section 4.2 to hold. While we can naively achieve this by setting \(=\), the resulting \(\) may generate overly conservative decisions. Hence, we are interested in learning the smallest uncertainty set that satisfies this condition. We solve the following _calibration problem_

\[(},_{}, ):_{k}\}_{k_{}}}{} \] (10a) subject to \[}_{k}^{}(_{k},_{k}),\  k_{}\] (10b) \[_{k_{}}[_{k}(},)](N _{}+1)\] (10c) \[\|_{k}\|_{2}=1,\  k_{}\] (10d) \[0,\] (10e)

where decision \(\) controls the size of the uncertainty set, \(_{k}\) represent a possible parameter vector associated with data point \(k_{}\), \(\) is a DM-specified confidence level. Constraints (10b) ensure that \(_{k}\) can make the decision \(}_{k}\) optimal for \(k_{}\). Constraint (10c) ensures that at least \((N_{}+1)\) of the decisions in \(_{}\) can find a vector in \(\) that makes it optimal. Constraints (10d) ensure that the parameter vectors are on the unit sphere as defined in Equation (9).

**Remark 2** (Optimality Conditions).: _The specific form of Constraints (10b) depends on the structure of \(\). For example, when \(\) is a linear program, Constraints (10b) can be replaced with the dual feasibility and strong duality constraints. When the \(\) is a general convex optimization problem, we can use the KKT conditions. For non-convex forward problems, we can replace Constraints (10b) with \(f(_{k},}_{k}) f(_{k}, )\) for all \(()\), which can be generated in a cutting-plane fashion._

**Remark 3** (Feasibility).: _For \(\) to be feasible, we require, for each observed decision, there exists a \(\) that make it optimal. This condition holds for a range of problems, e.g., routing problems and the knapsack problem, even if the DM is subject to bounded rationality, i.e., the DM settles for suboptimal solutions due to cognitive/computational limitations. For problems where this condition is violated, we may pre-process \(_{}\) to project \(}\) to a point in \(()\) such that the condition is satisfied._

Solving \(\) is hard. First, \(\) is non-convex due to Constraints (10d). Second, Constraints (10b) involve the optimality conditions of \(N_{}\) problems, so the size of \(\) grows quickly as \(N_{}\) in creases. Nevertheless, considering a large \(_{}\) is critical to ensure desirable properties of the learned uncertainty set (Section 4.2). Below we introduce a decomposition method to solve \(\) efficiently.

**Theorem 1**.: _Let \(_{}\) be a dataset, \(\), \(}^{d}\), \(=[(N_{}+1)]\) and \(_{}\) be an operator that returns the \(^{}\) largest value in a set. The optimal solution to \((},_{},)\) is \(_{}:=(_{}(\{c_{k}\}_{k_{ }}))\) with \(c_{k}:=_{_{k}}\{_{k}^{}}\,\|\,}_{k}^{}(_{k}, _{k}),\,\|_{k}\|_{2}=1\}\)._

Theorem 1 states that we can solve \(\) by first solving \(N_{}\) optimization problems whose size is independent of \(N_{}\) and then find a quantile in a set of \(N_{}\) elements. The first step is parallelizable and the second step can be done in \(O(N_{}())\) time. Since the problem required for evaluating \(c_{k}\) is a maximization problem, we can replace the constraint \(\|_{k}\|_{2}=1\) with \(\|_{k}\|_{2} 1\) if \(}_{+}^{d}\), so this problem is convex when the forward problem is a linear program.

### Properties of Conformal IO

**Theorem 2** (Uncertainty Set Validity).: _Let \(_{}\) be a dataset that satisfies Assumption 1, \((},)\) be a new i.i.d. sample from \(_{(,)}\), \(}=}(},)\), \(}:=^{}(},)\), and \(_{}\) be an optimal solution to \((},_{},)\) where \(}^{d}\). We have, for any \([0,N_{}/(N_{}+1)]\), that_

\[(}(},_{ })).\] (11)

_For any \(\), with probability at least \(1-1/N_{}\),_

\[|(}(}, _{}))-|(N_{ }):=}+1)+2 N_{}}{N_{}}}+ }}.\] (12)

Theorem 2 states that our learned uncertainty set is conservatively valid and asymptotically exact (Vovk et al., 2005). More specifically, first, our method will produce a set that contains a \(\) that makes the next DM's decision optimal no less than \(\) of the time that it is used (conservatively valid). The probability in Inequality (11) is with respect to the joint distribution over \(_{}\) and the new sample. Second, once the set is given, we have high confidence that, the probability of the next DM's decision being covered is within \((N_{})\) from \(\). The probability in Inequality (12) is with respect to the new sample, while the high confidence is with respect to the draw of the validation data set. Overall, we have the almost sure convergence of \((}(},_{} ))\) to \(\) as \(N\) goes to infinity. Finally, we note that in many practical applications, the number of decision observations \(N\) can be quite large. For example, in our motivating example, rideshare platforms observe millions of trips on a daily basis, providing ample data for both point estimation and uncertainty set calibration in our pipeline.

Now, we relate the validity results to the performance of conformal IO. The following Lemma is an immediate result of the objective function \(f\) being linear in \(\).

**Lemma 3**.: _For any \(}}(},)\) and \((},)\), there exists a constant \((})_{+}\) such that, for any \(,^{}\), we have \(f(,})-f(^{},})(})\|- ^{}\|_{2}\)._

**Theorem 3** (POG Bound).: _Let \(}_{}()\) be an optimal solution to \(((},_{1}),)\) for any \(\), where \(}^{d}\) and \(_{1}\) are chosen such that, for a new sample \((^{},^{})\) from \(_{(,)}\) and \(^{}=}(^{},^{})\), \(((},_{1})^{ }(^{},^{}))=1\). If Assumptions 2-3 hold, then_

\[(}_{})(-2 2_{1}+2)+ _{},\] (13)

_and_

\[(}_{})(2-2 2_{1}++ )^{*}+(+)_{},\] (14)

_where \(:=[(}(},))]\), \(_{}:=([}_{}()])\), and \(^{*}:=([}(^{*},)])\)._

Theorem 3 states that, when \((},)\) contains a \(\) that makes the next DM's decision optimal almost surely, conformal IO achieves upper-bounded POG and AOG. While we can meet this condition by using a large \(\), the bounds would be large as they increase as \(\) increases, reflecting that the decisions may be overly conservative. Instead, we can use \(\) to obtain a \(\) that achieves close-to-100% coverage and possibly add a small \(_{}_{+}\) as extra protection. Moreover, we show in Section 5that, when using \(<100\%\), conformal IO still demonstrates favorable performance compared to classic IO. Our bounds have problem-specific constants. To demonstrate tightness, we present their numerical values in Example 1 with \(=/4\) (Table 1). They closely follow the performance of conformal IO, which outperforms classic IO by a large margin.

## 5 Numerical Studies

**Data generation.** We consider two forward problems: The shortest path problem (linear program, \(d=120\)) and knapsack problem (integer program, \(d=10\)). See Appendix C.2 for their formulations. We use synthetic instances, which is a common practice as there is no well established IO benchmark (Tan et al., 2020; Dong et al., 2018). For both problems, we randomly generate a ground-truth parameters \(^{*}\) and a dataset of \(N=1000\) DMs. For each DM \(k[N]\), we generate her perceived parameters as \(^{i}_{k}=(^{i*}_{k}*p^{i}_{k}+^{i}_{k})^{+}+ _{0}\) for \(i[d]\) where \(p^{i}_{k}\) is uniformly drawn from \([1/2,2]\), \(^{i}_{k}\) is drawn from a normal distribution with mean 0 and standard deviation 1, and \(_{0}=0.1\). For the shortest path problem, parameters \(_{k}\) represent a random origin-destination pair on the network. For the knapsack problem, \(_{k}\) correspond to the weights of different items and the DM's budget. The item weight \(w^{i}\) for \(i[d]\) are uniformly drawn from \(\) and are shared among DMs. For each DM \(k[N]\), we generate a budget \(u_{k}=q_{k}_{i}w^{i}\) where \(q_{k}\) is uniformly drawn from \([1/5,5]\).

**Experiment design.** Conformal IO is compatible with any approach that can provide a point estimation of unknown parameters using _decision data_ (Step 2 in Section 4.1). To the best of our knowledge, i) **IO** with the sub-optimality loss and ii) the PFYL approach from Berthet et al. (2020) are the only two methods that can perform this task _at scale_. We thus implement conformal IO with these two methods. They also serve as our baselines. We call both i) and ii) classic IO to emphasize that they rely on a point estimation for decision prescription, although PFYL is not an IO approach. See Appendix C for implementation details. In all experiments, conformal IO uses the training set for point estimation and the validation set for calibration, while classic IO uses the union of the training and validation sets for point estimation. So, they have access to the same amount of data and are evaluated on the same test set. Unless otherwise noted, experiments are based on a 60/20/20 train-validation-test split and are repeated 10 times with different random seeds.

**Uncertainty validity.** To verify Theorem 2, we empirically evaluate the out-of-sample coverage achieved by our uncertainty set under different target levels \(\) and sample sizes \(N_{}\). The point estimation is generated by **IO** with the sub-optimality loss. As shown in Figure 3, when the validation set is small (\(N_{}=10\)), we always achieve the specified target but \((,_{})\) tends to over-cover (conservatively valid). When using larger validation sets (\(N_{}\{100,200\}\)), our coverage level gets closer to the specified \(\) (asymptotic exact). These empirical findings echo our theoretical analysis.

    &  &  \\  \(u\) & 2 & 10 & 50 & 100 & 2 & 10 & 50 & 100 \\  Classic IO & 0.35 & 3.18 & 17.32 & 35 & 0.74 & 4.55 & 24.51 & 49.50 \\ Conformal IO & 0.00 & 0.00 & 0.00 & 0.00 & 0.70 & 0.16 & 0.03 & 0.02 \\ Conformal IO bound & 0.70 & 0.05 & 0.002 & 0.001 & 1.58 & 0.67 & 0.15 & 0.08 \\   

Table 1: Performance profile of classic and conformal IO in Example 1.

Figure 3: Empirical coverage achieved by the learned uncertainty set (error bar = range).

**The value of robustness.** As shown in Figure 4, solving **RFO** with an uncertainty set learned by conformal IO leads to decisions of lower AOG and POG, compared to solving **FO** with a point estimation from classic IO. On average, when varying \(\), our approach improves AOG by 20.1-30.4% and POG by 15.0-23.2% for the shortest path problem, and improves AOG by 40.3-57.0% and POG by 13.5-20.1% for the knapsack problem. The improvement is orthogonal to the point estimation method. Our decisions are of higher quality and better align with human intuition than classic IO. Finally, the performance of conformal IO generally improves as the quality of the point estimate increases (see Appendix D for details). Therefore, even though the conformal IO pipeline is robust to estimation errors, using the best available point estimation method is still recommended.

**Computational efficiency.** As shown in Table 2, conformal IO and classic IO require similar training times. When **FO** is an integer program (knapsack), the training of conformal IO is even faster because it replaces a relatively large inverse integer program (associated with \(_{}_{}\)), which is notoriously difficult to solve (Bodur et al., 2022), with a smaller inverse integer program (associated with \(_{}\)) and a set of small calibration problems that are parallelizable (Theorem 1). At the prediction time, our method achieves lower AOG and POG at the cost of solving a more challenging **RFO**. Nevertheless, the solution time of **RFO** is within one second in our instances.

**Important hyper-parameters.** Finally, we provide empirical evidence that sheds light on the choice of two important hyper-parameters in conformal IO: i) _confidence level \(\)_, and ii) _train-validation split ratio_. Regarding \(\), as shown in Figure 4, the performance of conformal IO improves quickly as \(\) increases from 0 to 50% and remains stable and even worsens slightly after that. Hence, it is possible to improve the performance of conformal IO by carefully tuning \(\) using cross-validation. However, this requires an additional validation dataset. If such a dataset is unavailable, setting \(\) to a relatively large value (e.g., 0.99) usually yields decent performance, which aligns with our theoretical analysis. Regarding _train-validation split ratio_, intuitively, both the estimation and calibration can benefit from more data. However, when the dataset is small, we need to strike a balance between these two steps aiming to achieve lower AOG and POG. We implement conformal IO for the shortest path problem under different dataset sizes (\(N_{}+N_{}\)) and train-validation split ratios (\(N_{}/(N_{}+N_{})\)). As shown in Figure 5, when the dataset is small, there is no benefit of using conformal IO because we do not have enough data to obtain good point estimation and uncertainty set simultaneously. However,

    &  &  \\  Problem & Classic IO & Conformal IO & **FO** & **RFO** \\  Shortest Path & 0.18 (0.02) & 0.27 (0.03) & 0.01 (0.00) & 0.63 (0.12) \\ Knapsack & 2.47 (0.37) & 1.95 (0.32) & 0.01 (0.00) & 0.44 (0.15) \\   

Table 2: Average (std) computational time of classic and conformal IO in seconds.

Figure 4: Performance profile of classic (blue) and conformal IO (green).

the performance of classic IO quickly plateaus as the dataset grows. When given a mid- or large-sized dataset, we can generally benefit from putting more data in \(_{}\), echoing our theoretical analysis.

## 6 Conclusion and Future Work

In this paper, we propose conformal IO, a novel IO pipeline to recommend high-quality decisions that align with human intuition. We present a new approach to learning uncertainty sets from decision data, which is then utilized in a robust optimization model to prescribe new decisions. We prove that conformal IO achieves bounded optimality gaps, as measured by the ground-truth parameters and the DM's perceived parameters. We demonstrate the strong empirical performance of conformal IO via extensive numerical studies. Finally, we highlight several challenges that underscore future research directions. First, we focus on objectives that are linear in the unknown parameters. While such objectives are ubiquitous in practice, it is of interest to extend our results for general convex objectives. Second, the robust forward problem, while leading to better decisions, is computationally more costly than the forward problem. Future research can be done to accelerate its solution process. Finally, while Conformal IO consistently outperforms classic IO when the point estimation methods are fixed, our computational experiments suggest that its performance hinges on the quality of the point estimate. Future research could explore point estimation methods that directly optimize the performance of the downstream robust optimization model.