# Nearly Lossless Adaptive Bit Switching

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Model quantization is widely applied for compressing and accelerating deep neural networks (DNNs). However, conventional Quantization-Aware Training (QAT) focuses on training DNNs with uniform bit-width. The bit-width settings vary across different hardware and transmission demands, which induces considerable training and storage costs. Hence, the scheme of one-shot joint training multiple precisions is proposed to address this issue. Previous works either store a larger FP32 model to switch between different precision models for higher accuracy or store a smaller INT8 model but compromise accuracy due to using shared quantization parameters. In this paper, we introduce the _Double Rounding_ quantization method, which fully utilizes the quantized representation range to accomplish nearly lossless bit-switching while reducing storage by using the highest integer precision instead of full precision. Furthermore, we observe a competitive interference among different precisions during one-shot joint training, primarily due to inconsistent gradients of quantization scales during backward propagation. To tackle this problem, we propose an Adaptive Learning Rate Scaling (**ALRS**) technique that dynamically adapts learning rates for various precisions to optimize the training process. Additionally, we extend our _Double Rounding_ to one-shot mixed precision training and develop a Hessian-Aware Stochastic Bit-switching (**HASB**) strategy. Experimental results on the ImageNet-1K classification demonstrate that our methods have enough advantages to state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision. Our codes are available at here.

## 1 Introduction

Recently, with the popularity of mobile and edge devices, more and more researchers have attracted attention to model compression due to the limitation of computing resources and storage. Model quantization [1; 2] has gained significant prominence in the industry. Quantization maps floating-point values to integer values, significantly reducing storage requirements and computational resources without altering the network architecture.

Generally, for a given pre-trained model, the quantization bit-width configuration is predefined for a specific application scenario. The quantized model then undergoes retraining, _i.e._, QAT, to mitigate the accuracy decline. However, when the model is deployed across diverse scenarios with different precisions, it often requires repetitive retraining processes for the same model. A lot of computing resources and training costs are wasted. To address this challenge, involving the simultaneous training of multi-precision [3; 4] or one-shot mixed-precision [3; 5] have been proposed. Among these approaches, some involve sharing weight parameters between low-precision and high-precision models, enabling dynamic bit-width switching during inference.

However, bit-switching from high precision (or bit-width) to low precision may introduce significant accuracy degradation due to the _Rounding_ operation in the quantization process. Additionally, there is severe competition in the convergence process between higher and lower precisions in multi-precisionscheme. In mixed-precision scheme, previous methods often incur vast searching and retraining costs due to decoupling the training and search stages. Due to the above challenges, bit-switching remains a very challenging problem. Our motivation is designing a bit-switching quantization method that doesn't require storing a full-precision model and achieves nearly lossless switching from high-bits to low-bits. Specifically, for different precisions, we propose unified representation, normalized learning steps, and tuned probability distribution so that an efficient and stable learning process is achieved across multiple and mixed precisions, as depicted in Figure 1.

To solve the bit-switching problem, prior methods either store the floating-point parameters [6; 7; 4; 8] to avoid accuracy degradation or abandon some integer values by replacing _rounding_ with _floor[3; 9]_ but leading to accuracy decline or training collapse at lower bit-widths. We propose _Double Rounding_, which applies the _rounding_ operation twice instead of once, as shown in Figure1 (a). This approach ensures nearly lossless bit-switching and allows storing the highest bit-width model instead of the full-precision model. Specifically, the lower precision weight is included in the higher precision weight, reducing storage constraints.

Moreover, we empirically find severe competition between higher and lower precisions, particularly in 2-bit precision, as also noted in [10; 4]. There are two reasons for this phenomenon: The optimal quantization interval itself is different for higher and lower precisions. Furthermore, shared weights are used for different precisions during joint training, but the quantization interval gradients for different precisions exhibit distinct magnitudes during training. Therefore, we introduce an Adaptive Learning Rate Scaling (ALRS) method, designed to dynamically adjust the learning rates across different precisions, which ensures consistent update steps of quantization scales corresponding to different precisions, as shown in the Figure 1 (b).

Finally, we develop an efficient one-shot mixed-precision quantization approach based on _Double Rounding_. Prior mixed-precision approaches first train a SuperNet with predefined bit-width lists, then search for optimal candidate SubNets under restrictive conditions, and finally retrain or fine-tune them, which incurs significant time and training costs. However, we use the Hessian Matrix Trace  as a sensitivity metric for different layers to optimize the SuperNet and propose a Hessian-Aware Stochastic Bit-switching (HASB) strategy, inspired by the Roulette algorithm . This strategy enables tuned probability distribution of switching bit-width across layers, assigning higher bits to more sensitive layers and lower bits to less sensitive ones, as shown in Figure 1 (c). And, we add the sensitivity to the search stage as a constraint factor. So, our approach can omit the last stage.

Figure 1: Overview of our proposed lossless adaptive bit-switching strategy.

In conclusion, our main contributions can be described as:

* _Double Rounding_ quantization method for multi-precision is proposed, which stores a single integer weight to enable adaptive precision switching with nearly lossless accuracy.
* Adaptive Learning Rate Scaling (ALRS) method for the multi-precision scheme is introduced, which effectively narrows the training convergence gap between high-precision and low-precision, enhancing the accuracy of low-precision models without compromising high-precision model accuracy.
* Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision SuperNet is applied, where the access probability of bit-width for each layer is determined based on the layer's sensitivity.
* Experimental results on the ImageNet1K dataset demonstrate that our proposed methods are comparable to state-of-the-art methods across different mainstream CNN architectures.

## 2 Related Works

**Multi-Precision.** Multi-Precision entails a single shared model with multiple precisions by one-shot joint Quantization-Aware Training (QAT). This approach can dynamically adapt uniform bit-switching for the entire model according to computing resources and storage constraints. AdaBits  is the first work to consider adaptive bit-switching but encounters convergence issues with 2-bit quantization on ResNet50 . Bit-Mixer  addresses this problem by using the LSQ  quantization method but discards the lowest state quantized value, resulting in an accuracy decline. Multi-Precision joint QAT can also be viewed as a multi-objective optimization problem. Any-precision  and MultiQuant  combine knowledge distillation techniques to improve model accuracy. Among these methods, MultiQuant's proposed "Online Adaptive Label" training strategy is essentially a form of self-distillation . Similar to our method, AdaBits and Bit-Mixer can save an 8-bit model, while other methods rely on 32-bit models for bit switching. Our _Double Rounding_ method can store the highest bit-width model (e.g., 8-bit) and achieve almost lossless bit-switching, ensuring a stable optimization process. Importantly, this leads to a reduction in training time by approximately 10%  compared to separate quantization training.

**One-shot Mixed-Precision.** Previous works mainly utilize costly approaches, such as reinforcement learning [16; 17] and Neural Architecture Search (NAS) [18; 19; 20], or rely on partial prior knowledge [21; 22] for bit-width allocation, which may not achieve global optimality. In contrast, our proposed one-shot mixed-precision method employs Hessian-Aware optimization to refine a SuperNet via gradient updates, and then obtain the optimal conditional SubNets with less search cost without retraining or fine-tuning. Additionally, Bit-Mixer  and MultiQuant  implement layer-adaptive mixed-precision models, but Bit-Mixer uses a naive search method to attain a sub-optimal solution, while MultiQuant requires 300 epochs of fine-tuning to achieve ideal performance. Unlike NAS approaches , which focus on altering network architecture (e.g., depth, kernel size, or channels), our method optimizes a once-for-all SuperNet using only quantization techniques without altering the model architecture.

## 3 Methodology

### _Double Rounding_

Conventional separate precision quantization using Quantization-Aware Training (QAT)  attain a fixed bit-width quantized model under a pre-trained FP32 model. A pseudo-quantization node is inserted into each layer of the model during training. This pseudo-quantization node comprises two operations: the quantization operation \(quant(x)\), which maps floating-point (FP32) values to lower-bit integer values, and the dequantization operation \(dequant(x)\), which restores the quantized integer value to its original floating-point representation. It can simulate the quantization error incurred when compressing float values into integer values. As quantization involves a non-differentiable \(Rounding\) operation, Straight-Through Estimator (STE)  is commonly used to handle the non-differentiability.

However, for multi-precision quantization, bit-switching can result in significant accuracy loss, especially when transitioning from higher bit-widths to lower ones, _e.g._, from 8-bit to 2-bit. Tomitigate this loss, prior works have mainly employed two strategies: one involves bit-switching from a floating-point model (32-bit) to a lower-bit model each time using multiple learnable quantization parameters, and the other substitutes the \({Rounding}\) operation with the \(Floor\) operation, but this results in accuracy decline (especially in 2-bit). In contrast, we propose a nearly lossless bit-switching quantization method called _Double Rounding_. This method overcomes these limitations by employing a \({Rounding}\) operation twice. It allows the model to be saved in the highest-bit (_e.g._, 8-bit) representation instead of full-precision, facilitating seamless switching to other bit-width models. A detailed comparison of _Double Rounding_ with other quantization methods is shown in Figure 2.

Unlike AdaBits, which relies on the Dorefa  quantization method where the quantization scale is determined based on the given bit-width, the quantization scale of our _Double Rounding_ is learned online and is not fixed. It only requires a pair of shared quantization parameters, _i.e._, _scale_ and _zero-point_. Quantization scales of different precisions adhere to a strict "Power of Two" relationship. Suppose the highest-bit and the target low-bit are denoted as \(h\)-bit and \(l\)-bit respectively, and the difference between them is \(=h-l\). The specific formulation of _Double Rounding_ is as follows:

\[_{h}=(_{h}}{ _{h}},-2^{h-1},2^{h-1}-1)\] (1)

\[_{l}=(_{h}}{2^{} },-2^{l-1},2^{l-1}-1)\] (2)

\[_{l}=_{l}_{h} 2^{}+ _{h}\] (3)

where the symbol \(.\) denotes the \({Rounding}\) function, and \((x,low,upper)\) means \(x\) is limited to the range between \(low\) and \(upper\). Here, \(W\) represents the FP32 model's weights, \(_{h}\) and \(_{h}\) denote the highest-bit (_e.g._, 8-bit) quantization _scale_ and _zero-point_ respectively. \(_{h}\) represent the quantized weights of the highest-bit, while \(_{l}\) and \(_{l}\) represent the quantized weights and dequantized weights of the low-bit respectively.

Hardware shift operations can efficiently execute the division and multiplication by \(2^{}\). Note that in our _Double Rounding_, the model can also be saved at full precision by using unshared quantization parameters to run bit-switching and attain higher accuracy. Because we use symmetric quantization scheme, the \(_{h}\) is \(0\). Please refer to Section A.4 for the gradient formulation of _Double Rounding_.

Unlike fixed weights, activations change online during inference. So, the corresponding _scale_ and _zero-point_ values for different precisions can be learned individually to increase overall accuracy. Suppose \(X\) denotes the full precision activation, and \(}\) and \(}\) are the quantized activation and dequantized activation respectively. The quantization process can be formulated as follows:

\[}=(_{b}}{ _{b}},0,2^{b}-1)\] (4)

\[}=}_{b}+_{b}\] (5)

where \(_{b}\) and \(_{}\) represent the quantization _scale_ and _zero-point_ of different bit-widths activation respectively. Note that \(_{b}\) is \(0\) for the ReLU activation function.

### Adaptive Learning Rate Scaling for Multi-Precision

Although our proposed _Double Rounding_ method represents a significant improvement over most previous multi-precision works, the one-shot joint optimization of multiple precisions remains constrained by severe competition between the highest and lowest precisions [10; 4]. Different precisions simultaneously impact each other during joint training, resulting in substantial differences

Figure 2: Comparison of four quantization schemes:(from left to right) used in _LSQ_, _AdaBits_, _Bit-Mixer_ and Ours _Double Rounding_. In all cases \(y=dequant(quant(x))\).

in convergence rates between them, as shown in Figure 3 (c). We experimentally find that this competitive relationship stems from the inconsistent magnitudes of the quantization scale's gradients between high-bit and low-bit quantization during joint training, as shown in Figure 3 (a) and (b). For other models statistical results please refer to Section A.6 in the appendix.

Motivated by these observations, we introduce a technique termed Adaptive Learning Rate Scaling (ALRS), which dynamically adjusts learning rates for different precisions to optimize the training process. This technique is inspired by the Layer-wise Adaptive Rate Scaling (LARS)  optimizer. Specifically, suppose the current batch iteration's learning rate is \(\), we set learning rates \(_{b}\) of different precisions as follows:

\[_{b}=_{b}(-_{i=1}^{L}((_{b}^{i},1.0)),1.0)}{L} ),\] (6)

\[_{b}=1 10^{-},&\\ 5 10^{-()},&\] (7)

where the \(L\) is the number of layers, \((.)\) represents gradient clipping that prevents gradient explosion, \((.)\) denotes the maximum absolute value of all elements. The \(_{b}^{i}\) denotes the quantization scale's gradients of layer \(i\) and \(_{b}\) denotes scaling hyperparameter of different precisions, _e.g._, 8-bit is \(1\), 6-bit is \(0.1\), and 4-bit is \(0.01\). Note that the ALRS strategy is only used for updating quantization scales. It can adaptively update the learning rates of different precisions and ensure that model can optimize quantization parameters at the same pace, ultimately achieving a minimal convergence gap in higher bits and 2-bit, as shown in Figure 3 (d).

In multi-precision scheme, different precisions share the same model weights during joint training. For conventional multi-precision, the shared weight computes \(n\) forward processes at each training iteration, where \(n\) is the number of candidate bit-widths. The losses attained from different precisions are then accumulated, and the gradients are computed. Finally, the shared parameters are updated. For detailed implementation please refer to Algorithm A.1 in the appendix. However, we find that if different precision losses separately compute gradients and directly update shared parameters at each forward process, it attains better accuracy when combined with our ALRS training strategy. Additionally, we use dual optimizers to update the weight parameters and quantization parameters simultaneously. We also set the weight-decay of the quantization scales to \(0\) to achieve stable convergence. For detailed implementation please refer to Algorithm A.2 in the appendix.

### One-Shot Mixed-Precision SuperNet

Unlike multi-precision, where all layers uniformly utilize the same bit-width, mixed-precision SuperNet provides finer-grained adaptive by configuring the bit-width at different layers. Previous methods typically decouple the training and search stages, which need a third stage for retraining or fine-tuning the searched SubNets. These approaches generally incur substantial search costs in selecting the optimal SubNets, often employing methods such as greedy algorithms [26; 9] or genetic algorithms [27; 4]. Considering the fact that the sensitivity , _i.e._, importance, of each layer is different, we propose a Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision training.

Specifically, the Hessian Matrix Trace (HMT) is utilized to measure the sensitivity of each layer. We first need to compute the pre-trained model's HMT by around 1000 training images , as shown in

Figure 3: The statistics of ResNet18 on ImageNet-1K dataset. (a) and (b): The quantization scale gradients’ statistics for the weights, with outliers removed for clarity. (c) and (d): The multi-precision training processes of our _Double Rounding_ without and with the ALRS strategy.

Figure 4 (c). Then, the HMT of different layers is utilized as the probability metric for bit-switching. Higher bits are priority selected for sensitive layers, while all candidate bits are equally selected for unsensitive layers. Our proposed Roulette algorithm is used for bit-switching processes of different layers during training, as shown in the Algorithm 1. If a layer's HMT exceeds the average HMT of all layers, it is recognized as sensitive, and the probability distribution of Figure 4 (b) is used for bit selection. Conversely, if the HMT is below the average, the probability distribution of Figure 4 (a) is used for selection. Finally, the Integer Linear Programming (ILP)  algorithm is employed to find the optimal SubNets. Considering each layer's sensitivity during training and adding this sensitivity to the ILP's constraint factors (_e.g._, model's FLOPs, latency, and parameters), which depend on the actual deployment requirements. We can efficiently attain a set of optimal SubNets during the search stage without retraining, thereby significant reduce the overall costs. All the searched SubNets collectively constitute the Pareto Frontier optimal solution, as shown in Figure 4 (d). For detailed mixed-precision training and searching process (_i.e._, ILP) please refer to the Algorithm A.3 and the Algorithm 2 respectively.

```
0: Candidate bit-widths set \(b B\), the HMT of current layer: \(t_{l}\), average HMT: \(t_{m}\);
1: Sample \(r U(0,1]\) from a uniform distribution;
2:if\(t_{l}<t_{m}\)then
3: Compute bit-switching probability of all candidate \(b_{i}\) with \(p_{i}=1/n\);
4: Set \(s=0\), and \(i=0\);
5:while\(s<r\)do
6:\(i=i+1\);
7:\(s=p_{i}+s\);
8:endwhile
9:else
10: Compute bit-switching probability of all candidate \(b_{i}\) with \(p_{i}=b_{i}/\|B\|_{1}\);
11: Set \(s=0\), and \(i=0\);
12:while\(s<r\)do
13:\(i=i+1\);
14:\(s=p_{i}+s\);
15:endwhile
16:endif
17:return\(b_{i}\); ```

**Algorithm 1** Roulette algorithm for bit-switching.

## 4 Experimental Results

**Setup.** In this paper, we mainly focus on ImageNet-1K  classification task using both classical networks (ResNet18/50 ) and lightweight networks (MobileNetV2 ), which same as previous works. Experiments cover joint quantization training for multi-precision and mixed precision. We explore two candidate bit configurations, _i.e._, {8,6,4,2}-bit and {4,3,2}-bit, each number represents the quantization level of the weight and activation layers. Like previous methods, we exclude batch

Figure 4: The HAB stochastic process and Mixed-precision of ResNet18 for {2,4,6,8}-bit.

normalization layers from quantization, and the first and last layers are kept at full precision. We initialize the multi-precision models with a pre-trained FP32 model, and initialize the mixed-precision models with a pre-trained multi-precision model. All models use the _Adam_ optimizer  with a batch size of \(256\) for 90 epochs and use a cosine scheduler without warm-up phase. The initial learning rate is 5e-4 and weight decay is 5e-5. Data augmentation uses the standard set of transformations including random cropping, resizing to 224\(\)224 pixels, and random flipping. Images are resized to 256\(\)256 pixels and then center-cropped to 224\(\)224 resolution during evaluation.

### Multi-Precision

**Results.** For {8,6,4,2}-bit configuration, the Top-1 validation accuracy is shown in Table 1. The network weights and the corresponding activations are quantized into w-bit and a-bit respectively. Our _double-rounding_ combined with ALRS training strategy surpasses the previous state-of-the-art (SOTA) methods. For example, in ResNet18, it exceeds Any-Precision  by 2.7%(or 2.83%) under w8a8 setting without(or with) using KD technique , and outperforms MultiQuant  by 0.63%(or 0.73%) under w4a4 setting without(or with) using KD technique respectively. Additionally, when the candidate bit-list includes 2-bit, the previous methods can't converge on MobileNetV2 during training. So, they use {8,6,4}-bit precision for MobileNetV2 experiments. For consistency, we also test {8,6,4}-bit results, as shown in the "Ours {8,6,4}-bit" rows of Table 1. Our method achieves 0.25%/0.11%/0.56% higher accuracy than AdaBits  under the w8a8/w6a6/w4a4 settings.

Notably, our method exhibits the ability to converge but shows a big decline in accuracy on MobileNetV2. On the one hand, the compact model exhibits significant differences in the quantization scale gradients of different channels due to involving DeepWise Convolution . On the other hand, when the bit-list includes 2-bit, it intensifies competition between different precisions during training. To improve the accuracy of compact models, we suggest considering the per-layer or per-channel learning rate scaling techniques in future work.

For {4,3,2}-bit configuration, Table 2 demonstrate that our _double-rounding_ consistently surpasses previous SOTA methods. For instance, in ResNet18, it exceeds Bit-Mixer  by 0.63%/0.7%/1.2%(or 0.37%/0.64%/1.02%) under w4a4/w3a3/w2a settings without(or with) using KD technique, and outperforms ABN by 0.87%/0.74%/1.12% under w4a4/w3a3/w2a2 settings with using KD technique respectively. In ResNet50, Our method outperforms Bit-Mixer  by 0.86%/0.63%/0.1% under w4a4/w3a3/w2a2 settings.

Notably, the overall results of Table 2 are worse than the {8,6,4,2}-bit configuration for joint training. We analyze that this discrepancy arises from information loss in the shared lower precision model

   Model & Method & KD & Storage & Epoch & w8a8 & w6a6 & w4a4 & w2a2 & FP \\   & Hot-Swap & ✗ & 32bit & \(-\) & 70.40 & 70.30 & 70.20 & 64.90 & \(-\) \\  & L1 & ✗ & 32bit & \(-\) & 69.92 & 66.39 & 0.22 & \(-\) & 70.07 \\  & KURE & ✗ & 32bit & 80 & 70.20 & 70.00 & 66.90 & \(-\) & 70.30 \\  & Ours & ✗ & 8bit & 90 & 70.74 & 70.71 & 70.43 & 66.35 & 69.76 \\  & Any-Precision & ✓ & 32bit & 80 & 68.04 & \(-\) & 67.96 & 64.19 & 69.27 \\  & CoQuant & ✓ & 8bit & 100 & 67.90 & 67.60 & 66.60 & 57.10 & 69.90 \\  & MultiQuant & ✓ & 32bit & 90 & 70.28 & 70.14 & 69.80 & 66.56 & 69.76 \\  & Ours & ✓ & 8bit & 90 & **70.87** & **70.79** & **70.53** & **66.84** & 69.76 \\   & Any-Precision & ✗ & 32bit & 80 & 74.68 & \(-\) & 74.43 & 72.88 & 75.95 \\  & Hot-Swap & ✗ & 32bit & \(-\) & 75.60 & 75.50 & 75.30 & 71.90 & \(-\) \\  & KURE & ✗ & 32bit & 80 & \(-\) & 76.20 & 74.30 & \(-\) & 76.30 \\  & Ours & ✗ & 8bit & 90 & 76.51 & 76.28 & 75.74 & 72.31 & 76.13 \\  & Any-Precision & ✓ & 32bit & 80 & 74.91 & \(-\) & 74.75 & 73.24 & 75.95 \\  & MultiQuant & ✓ & 32bit & 90 & 76.94 & 76.85 & 76.46 & 73.76 & 76.13 \\  & Ours & ✓ & 8bit & 90 & **76.98** & **76.86** & **76.52** & **73.78** & 76.13 \\   & AdaBits & ✗ & 8bit & 150 & 72.30 & 72.30 & 70.30 & \(-\) & 71.80 \\  & KURE & ✗ & 32bit & 80 & \(-\) & 70.00 & 59.00 & \(-\) & 71.30 \\   & Ours (8,6,4)-bit & ✗ & 8bit & 90 & 72.42 & 72.06 & 69.92 & \(-\) & 71.14 \\   & MultiQuant & ✓ & 32bit & 90 & 72.33 & 72.09 & 70.59 & \(-\) & 71.88 \\   & Ours (8,6,4)-bit & ✗ & 8bit & 90 & **72.55** & **72.41** & **70.86** & \(-\) & 71.14 \\   & Ours (8,6,4,2)-bit & ✗ & 8bit & 90 & 70.98 & 70.70 & 68.77 & 50.43 & 71.14 \\   & Ours (8,6,4,2)-bit & ✓ & 8bit & 90 & 71.35 & 71.20 & 69.85 & **53.06** & 71.14 \\   

Table 1: Top1 accuracy comparisons on multi-precision of {8,6,4,2}-bit on ImageNet-1K datasets. ’KD’ denotes knowledge distillation. The ”\(-\)” represents the unqueried value.

(_i.e._, 4-bit) used for bit-switching. In other words, compared with 4-bit, it is easier to directly optimize 8-bit quantization parameters to converge to the optimal value. So, we recommend including 8-bit for multi-precision training. Furthermore, independently learning the quantization scales for different precisions, including weights and activations, significantly improves accuracy compared to using shared scales. However, it requires saving the model in 32-bit format, as shown in "Ours*" of Table 2.

### Mixed-Precision

**Results.** We follow previous works to conduct mixed-precision experiments based on the {4,3,2}-bit configuration. Our proposed one-shot mixed-precision joint quantization method with the HASB technique comparable to the previous SOTA methods, as presented in Table 3. For example, in ResNet18, our method exceeds Bit-Mixer  by 0.83%/0.72%/0.77%/7.07% under w4a4/w3a3/w2a2/3MP settings and outperforms EQ-Net by 0.2% under 3MP setting. The results demonstrate the effectiveness of one-shot mixed-precision joint training to consider sensitivity with Hessian Matrix Trace when randomly allocating bit-widths for different layers. Additionally, Table 3 reveals that our results do not achieve optimal performance across all settings. We hypothesize that extending the number of training epochs or combining ILP with other efficient search methods, such as genetic algorithms, may be necessary to achieve optimal results in mixed-precision optimization.

### Ablation Studies

**ALRS vs. Conventional in Multi-Precision.** To verify the effectiveness of our proposed ALRS training strategy, we conduct an ablation experiment without KD, as shown in Table 4, and observe overall accuracy improvements, particularly for the 2bit. Like previous works, where MobileNetV2 can't achieve stable convergence with {4,3,2}-bit, we also opt for {8,6,4}-bit to keep consistent. However, our method can achieve stable convergence with {8,6,4,2}-bit quantization. This demonstrates the superiority of our proposed _Double-Rounding_ and ALRS methods.

**Multi-Precision vs. Separate-Precision in Time Cost.** We statistic the results regarding the time cost for multi-precision compared to separate-precision quantization, as shown in Table 5. Multi-precision training costs stay approximate constant as the number of candidate bit-widths.

   Model & Method & KD & Storage & Epoch & w4a4 & w3a3 & w2a2 & FP \\   & Bit-Mixer & ✗ & 4bit & 160 & 69.10 & 68.50 & 65.10 & 69.60 \\  & Vertical-layer & ✗ & 4bit & 300 & 69.20 & 68.80 & 66.60 & 70.50 \\  & Ours & ✗ & 4bit & 90 & 69.73 & 69.20 & 66.30 & 69.76 \\  & Q-DNNs & ✓ & 32bit & 45 & 66.94 & 66.28 & 62.91 & 68.60 \\  & ABN & ✓ & 4bit & 160 & 68.90 & 68.60 & 65.50 & — \\  & Bit-Mixer & ✓ & 4bit & 160 & 69.40 & 68.70 & 65.60 & 69.60 \\  & Ours & ✓ & 4bit & 90 & **69.77** & **69.34** & **66.62** & 69.76 \\   & Ours & ✗ & 4bit & 90 & 75.81 & 75.24 & 71.62 & 76.13 \\  & AdaBits & ✗ & 32bit & 150 & 76.10 & 75.80 & 73.20 & 75.00 \\  & Ours* & ✗ & 32bit & 90 & **76.42** & **75.82** & **73.28** & 76.13 \\  & Bit-Mixer & ✓ & 4bit & 160 & 75.20 & 74.90 & 72.70 & – \\  & Ours & ✓ & 4bit & 90 & 76.06 & 75.53 & 72.80 & 76.13 \\   

Table 2: Top1 accuracy comparisons on multi-precision of {4,3,2}-bit on ImageNet-1K datasets.

   Model & Method & KD & Training & Searching & Fine-tune & Epoch & w4a4 & w3a3 & w2a2 & 3MP & FP \\   & Ours & ✗ & HASB & ILP & w/o & 90 & 69.80 & 68.63 & 64.88 & 68.85 & 69.76 \\  & Bit-Mixer & ✓ & Random & Greedy & w/o & 160 & 69.20 & 68.60 & 64.40 & 62.90 & 69.60 \\  & ABN & ✓ & DRL & DRL & w. & 160 & 69.80 & 69.00 & **66.20** & 67.70 & — \\  & MultiQuant & ✓ & LRH & Genetic & w. & 90 & – & 67.50 & — & 69.20 & 69.76 \\  & EQ-Net & ✓ & LRH & Genetic & w. & 120 & – & 69.30 & 65.90 & 69.80 & 69.76 \\  & Ours & ✓ & KD & KD & w/o & 90 & **70.03** & **69.32** & 65.17 & **69.92** & 69.76 \\   & Ours & ✗ & HASB & ILP & w/o & 90 & 75.01 & 74.31 & 71.47 & 75.06 & 76.13 \\  & Bit-Mixer & ✓ & Random & Greedy & w/o & 160 & 75.20 & **74.80** & 72.10 & 73.20 & – \\   & EQ-Net & ✓ & LRH & Genetic & w. & 120 & – & 74.70 & **72.50** & 75.10 & 76.13 \\   & Ours & ✓ & HASB & ILP & w/o & 90 & **75.63** & 74.36 & 72.32 & **75.24** & 76.13 \\   

Table 3: Top1 accuracy comparisons on mixed-precision of {4,3,2}-bit on ImageNet-1K dataset. “MP” denotes average bit-width for mixed-precision. The “\(-\)” represents the unqueried value.

**Pareto Frontier of Different Mixed-Precision Configurations.** To verify the effectiveness of our HASB strategy, we conduct ablation experiments on different bit-lists. Figure 5 shows the search results of Mixed-precision SuperNet under {8,6,4,2}-bit, {4,3,2}-bit and {8,4}-bit configurations respectively. Where each point represents a SubNet. These results are obtained directly from ILP sampling without retraining or fine-tuning. As the figure shows, the highest red points are higher than the blue points under the same bit width, indicating that this strategy is effective.

## 5 Conclusion

This paper first introduces _Double Rounding_ quantization method used to address the challenges of multi-precision and mixed-precision joint training. It can store single integer-weight parameters and attain nearly lossless bit-switching. Secondly, we propose an Adaptive Learning Rate Scaling (ALRS) method for multi-precision joint training that narrows the training convergence gap between high-precision and low-precision, enhancing model accuracy of multi-precision. Finally, our proposed Hessian-Aware Stochastic Bit-switching (HASB) strategy for one-shot mixed-precision SuperNet and efficient searching method combined with Integer Linear Programming, achieving approximate Pareto Frontier optimal solution. Our proposed methods aim to achieve a flexible and effective model compression technique for adapting different storage and computation requirements.

    &  & -bit} & -bit} &  \\   & & w84 & w64e & w44 & w2a2 & w44 & w3a3 & w2a2 \\   & w/o & 92.17 & 92.20 & 92.17 & 89.67 & 91.19 & 90.98 & 88.62 & 92.30 \\  & w. & 92.25 & 92.32 & 92.09 & 90.19 & 91.79 & 91.83 & 88.88 & 92.30 \\   & w/o & 70.05 & 69.80 & 69.32 & 65.83 & 69.38 & 68.74 & 65.62 & 69.76 \\  & w. & 70.74 & 70.71 & 70.43 & 66.35 & 69.73 & 69.20 & 66.30 & 69.76 \\   & w/o & 76.18 & 76.08 & 75.64 & 70.28 & 75.48 & 74.85 & 70.64 & 76.13 \\  & w. & 76.51 & 76.28 & 75.74 & 72.31 & 75.81 & 75.24 & 71.62 & 76.13 \\   & w/o & 70.55 & 70.65 & 68.08 & 45.00 & 72.06 & 71.87 & 69.40 & 71.14 \\  & w. & 70.98 & 70.70 & 68.77 & 50.43 & 72.42 & 72.06 & 69.92 & 71.14 \\   

Table 4: Ablation studies of multi-precision, ResNet20 on CIFAR-10 dataset and other models on ImageNet-1K dataset. Note that MobileNetV2 uses {8,6,4}-bit instead of {4,3,2}-bit.

Figure 5: Comparison of HASB and Baseline approaches for Mixed-Precision on ResNet18.

   Model & Dataset & Bit-widths & \#V100 & Epochs & BatchSize & Avg. hours & Save cost (\%) \\   &  & Separate-bit & 1 & 200 & 128 & 0.9 & 0.0 \\  & & {4,3,2}-bit & 1 & 200 & 128 & 0.7 & 28.6 \\  & & {8,6,4,2}-bit & 1 & 200 & 128 & 0.8 & 12.5 \\   &  & Separate-bit & 4 & 90 & 256 & 19.0 & 0.0 \\  & & {4,3,2}-bit & 4 & 90 & 256 & 15.2 & 25.0 \\  & & {8,6,4,2}-bit & 4 & 90 & 256 & 16.3 & 16.6 \\   &  & Separate-bit & 4 & 90 & 256 & 51.6 & 0.0 \\  & & {4,3,2}-bit & 4 & 90 & 256 & 40.7 & 26.8 \\  & & {8,6,4,2}-bit & 4 & 90 & 256 & 40.8 & 26.5 \\   

Table 5: Training costs for multi-precision and separate-precision are averaged over three runs.