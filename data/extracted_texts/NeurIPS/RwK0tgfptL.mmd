# Shaving Weights with Occam's Razor:

Bayesian Sparsification for Neural Networks

using the Marginal Likelihood

Rayen Dhahri

Alexander Immer

Betrand Charpentier

Pruna Al, Munich

Stephan Gunnemann

Vincent Fortuin

###### Abstract

Neural network sparsification is a promising avenue to save computational time and memory costs, especially in an age where many successful AI models are becoming too large to naively deploy on consumer hardware. While much work has focused on different weight pruning criteria, the overall _sparsifiability_ of the network, i.e., its capacity to be pruned without quality loss, has often been overlooked. We present _Sparsifiability via the **M**arginal likelihood (**SpaM**), a pruning framework that highlights the effectiveness of using the Bayesian marginal likelihood in conjunction with sparsity-inducing priors for making neural networks more sparsifiable. Our approach implements an _automatic Occam's razor_ that selects the most sparsifiable model that still explains the data well, both for structured and unstructured sparsification. In addition, we demonstrate that the pre-computed posterior precision from the Laplace approximation can be re-used to define a cheap pruning criterion, which outperforms many existing (more expensive) approaches. We demonstrate the effectiveness of our framework, especially at high sparsity levels, across a range of different neural network architectures and datasets.

## 1 Introduction

The availability of large datasets and powerful computing infrastructure has fueled the growth of deep learning, enabling the training of increasingly complex neural networks (NNs). While catalyzing performance gains across various domains, such as image recognition  and text generation , this development has amplified the challenge of over-parameterization [3; 4] and raised concerns about the increase in model size and computational cost. Over-parameterized neural networks present significant deployment challenges, particularly in hardware-constrained environments [5; 6]. This has sparked the research field of NN _sparsification_ or _pruning_, where the goal is to remove a (potentially large) number of parameters from a trained network to make it smaller and ultimately cheaper to apply [7; 8]. However, most existing research in this domain has focused on the question of finding better pruning criteria, that is, scoring functions that decide which parameters to prune away [9; 4; 10]. This ignores the challenge that many trained networks are not inherently _sparsifiable_, i.e., they resisteffective pruning, regardless of the chosen criterion. Indeed, standard training of NNs does not encourage sparsifiability at all, so it should not be surprising that such trained NNs would use all of their parameters to some degree to fit the data.

Our work tackles this problem by _modifying the training process itself_, showing that more _sparsifiable_ networks can be achieved through Bayesian model selection using the marginal likelihood [11; 12] in conjunction with an adequate prior that will induce such sparsifiability. We call this _Sparsifiability via the Marginal likelihood_, or _SpaM_. Our approach implements an _automatic Occam's razor_, guiding the training process towards models that are faithfully fitting the data using only a small subset of their available parameters, such that the remaining ones can be pruned afterwards. This is achieved by optimizing thousands of prior hyper-parameters to adaptively regularize weight magnitudes. We make use of recent advances in Laplace inference for Bayesian neural networks (BNNs) [12; 14], allowing us to approximate the marginal likelihood  efficiently.

Once trained, we can use any pruning criterion to more effectively sparsify these networks. Notably, the pre-computed posterior precision of the Laplace approximation obtained from the marginal likelihood training readily translates into a powerful pruning criterion, which we call _Optimal Posterior Damage_ (OPD), similar to the popular Optimal Brain Damage [OBD; 3]. Since it reuses existing computations, it is cheaper than many existing criteria in practice, and it often performs on par or even better.

Extensive empirical evaluations demonstrate the strength of our SpaM approach and the derived OPD pruning criterion in both unstructured and structured sparsification tasks across various datasets and architectures. Moreover, they show that our framework strikes a compelling balance between performance and computational cost.

We make the following contributions:

* We propose **SpaM**, a _novel approach to improve the sparsifiability of neural networks during training_, using Bayesian model selection with the Laplace-approximated marginal likelihood, which works well in _structured and unstructured pruning_ scenarios and with _different pruning criteria_.
* We provide evidence-based _recommendations for prior selection_ within the SpaM framework, showing that some priors can improve sparsifiability significantly better than others.

Figure 1: Overview of our proposed SpaM method. We start by training the network to maximize the marginal likelihood using the Laplace approximation, while simplifying the Hessian computation through either the KFAC or a diagonal approximation. We can then use our precomputed posterior precision as a pruning criterion (OPD). For the case of unstructured pruning, we compute thresholds to achieve different target sparsities, compute the mask, and apply it, while for the structured approach, we aggregate the score per layer for easier weight transfer, compute the mask, and then delete the masked structures to obtain a smaller model.

* We present **OPD**, a _cheap pruning criterion_ similar to the popular OBD, which performs comparably or better than many other criteria in practice and _conveniently reuses computations_ performed in the context of SpaM.

## 2 Background

We use deep neural networks to model learning tasks with inputs \(_{n}^{D}\) and targets \(_{n}^{C}\) collected in a dataset \(=\{(_{n},_{n})\}_{n=1}^{N}\) of \(N\) pairs. A model is parameterized by weights \(^{P}\), and maps from inputs to targets using the neural network function \(_{}()\). Assuming the data are _i.i.d._, we have a likelihood \(p(|)=_{n=1}^{N}p(_{n}| _{}(_{n}))\). We minimize the negative log likelihood, which corresponds to common losses like the cross-entropy in classification. Additionally, regularization in the form of weight decay is commonly used and corresponds to a Gaussian prior on parameters \(p()=(;,())\) with diagonal precision. We employ Gaussian priors due to their analytical tractability and seamless integration with the Laplace approximation, which requires differentiability. This is essential for maintaining computational efficiency in our framework to approximate the marginal likelihood in a practical and scalable manner. Furthermore, Gaussian priors enable automatic relevance determination by allowing each parameter to have its own variance, facilitating the regularization process without introducing significant computational overhead.

### Marginal Likelihood for Deep Learning

**The marginal likelihood** serves as the probabilistic foundation for model evaluation and selection. It provides an objective to optimize the tradeoff between data fit and model complexity, akin to the concept of Occam's razor [13; 15], by quantifying how well a model \(\), with all its inherent uncertainties, explains the observed data:

\[p(|)= p(|, )\,p(|)\,.\] (1)

However, it requires computing an intractable integral over all neural network parameters.

**The Laplace approximation**[L4; 16] provides a tractable and effective approximation to the marginal likelihood for deep learning . It arises from a second-order Taylor approximation around an estimate of the mode, \(_{*}\), resulting in

\[ p(|) p(,_ {*}|)-|_{_{*}}()|,\] (2)

where \(_{_{*}}\) is the posterior precision given by the Hessian of the negative log joint distribution, \(-_{}^{2} p(,| )\), evaluated at \(_{*}\). Defining \(_{_{*}}\) as the Hessian of the negative log likelihood objective \(-_{}^{2} p(|, )\), the posterior precision decomposes as \(_{_{*}}=_{_{*}}+ ()\).

In practice, the Hessian of the negative log likelihood is often approximated by the positive semidefinite **generalized Gauss-Newton**[GGN, 17],

\[_{}_{n=1}^{N}_{}_{}(_{n})_{}^{2} p(_{n}|_{}( _{n}))_{}^{}_{}(_{n}),\] (3)

which relies on the Jacobians of the neural network function and second derivative of the negative log likelihood at the output. Further, it is amenable to efficient structured approximations like diagonal or layer-wise variants [e.g., 18; 19].

**Diagonal and block-diagonal GGN approximations** are efficient, and therefore commonly used for Laplace approximations in deep learning [20; 14]. The diagonal LA is cheap in terms of storage and computation by only modeling the marginal variances of parameters. Kronecker-factored LA [KFAC LA, 20] instead relies on a block-diagonal approximation to the GGN of the parameters \(_{l}\) in the \(l\)th layer,

\[_{_{l}}_{l}_{l},\] (4)

where the factors are given by the outer products of pre-activations and Jacobians w.r.t. the output of a layer, respectively [18; 19]. Here, \(A_{l}\) and \(G_{l}\) are the uncentered covariances of the respective layer inputs and output gradients. The top left of Figure 1 shows a comparison of both structures.

### Neural Network Pruning

The goal of the pruning procedure is to remove parameters from \(\) without affecting the quality of the model output \(_{}()\). While unstructured pruning consists in zeroing individual entries \(_{p}\) of the weight matrices, structured pruning consists in deleting entire structured sets of parameters \(g\), like rows or columns [21; 22]. The results of structured pruning enable smaller matrix multiplications that directly provide real-world efficiency gains on most hardware, including GPUs.

Pruning procedures usually follow three steps: **(1)** We use a scoring function \(S()\) to evaluate the importance of each individual parameter \(S(_{p})\) for unstructured pruning, or of a structured set of parameters \(S(g)\) for structured pruning. **(2)** We compute a binary mask \(\) with the same dimensions as \(\), which assigns \(0\) values to parameters whose unstructured or structured pruning scores are below a threshold \(T\), and \(1\) otherwise. While the threshold \(T\) is determined based on the target sparsity across layers for _global_ pruning, it is determined per layer for _uniform_ pruning . **(3)** We apply the mask on the weight matrix with element-wise multiplication, \(\), to effectively remove the least important parameters. Alternatively, structured pruning enables us to directly remove rows or columns whose mask values are \(0\) to reduce weight matrix dimensions.

## 3 Shaving Weights with Occam's Razor

We identify sparsifiable neural networks by automatically regularizing (groups of) parameters to have small magnitudes, to facilitate pruning the least important ones, within a probabilistic framework. Specifically, we utilize priors that regularize parameters in potentially structured ways, leading to smaller magnitudes. To optimize the resulting prior hyperparameters, we employ the Bayesian marginal likelihood as a differentiable objective function, effectively implementing a Bayesian variant of Occam's razor that drives irrelevant parameters towards smaller absolute magnitudes. The regularized networks can then be pruned _with any method_. However, we additionally propose to reuse the computed posterior precision for sparsification as a cheap and effective criterion.

### Structured Priors for Regularization

To reduce the magnitude of parameters and make them more amenable to pruning, we introduce structured priors and show how to combine them with diagonal and KFAC Laplace approximations. While a scalar prior, corresponding to weight decay, is the most common, it suggests that all parameters in a neural network are equally relevant and favors a uniform magnitude of parameters, which is suboptimal for pruning [23, Sec. 3.6].

Instead of scalar priors, we regularize parameters with different strengths using layer-, unit-, and parameter-wise priors. Layer-wise priors regularize individual layers differently and have been shown to aid pruning and improve generalization [12; 24; 25; 26]. Unit-wise regularization has been used mostly in traditional statistics, for example, for group sparsity , but recently also for channels or feature dimensions in neural networks [28; 29].

We consider different priors in the context of the Laplace approximation for marginal likelihood optimization and pruning: Scalar priors correspond to standard weight decay and are identical for all weights. Layer-wise priors provide a scalar regularizer \(_{l}\) per layer that is stacked into a vector \(\) in line with the number of parameters per layer. Parameter-wise priors allow to specify \(_{p}\) for each parameter \(p\) individually. We define unit-wise priors so that each unit, which denotes a channel for convolutional and a hidden neuron for fully-connected layers, has a regularization strength for incoming and outgoing weights separately. Thus, a weight \(_{p}\) that connects unit \(i\) at layer \(l\)-\(1\) with unit \(j\) in layer \(l\) has prior \((0,\,[_{l 1}]_{i}\,[_{l 1}]_{j})\), that is, each layer \(l\) with \(M_{l}\) hidden units has a prior vector \(_{l}^{M_{l}}\). A weight is thus regularized more strongly whenever both its in- and output neurons are.

Our different priors are simple to combine additively with a diagonal Hessian approximation for the Laplace approximation (Equation (2)) but not with a KFAC structure. For that reason, so far, only scalar or layer-wise priors have been used for KFAC posterior approximations . The main issue is that we have to preserve the Kronecker factors to keep the resulting memory cost low. For scalar or layer-wise priors, this can be achieved by an eigendecomposition of the individual factors

\[+}}{{=}}_{A}_{A}_{A}^{} _{G}_{G}_{G}^{}+ =(_{A}_{G})(_{A} _{G}+)(_{A}^{} _{G}^{}),\] (5)

which means that the precision only needs to be added to the diagonal eigenvalues and no Kronecker product needs to be calculated for inversion or determinant calculation.

To add a diagonal prior precision \(_{l}\) to the KFAC of the \(l\)th layer, we derive an optimal approximation in the KFAC eigenbasis, so as to maintain the Kronecker-factored structure of the posterior:

**Proposition 3.1** (Diagonal Prior in KFAC Eigenbasis).: _Considering the Frobenius norm, the optimal diagonal perturbation of the KFAC eigenvalues \(_{A}_{B}\) to add a diagonal prior precision is given by \(_{A}_{B}+}\) with \((})=(_{G}^{})^{2} ()_{A}^{2}\) where the square is element-wise and \(()\) reshapes the vector to match the parameter shape used in KFAC. Thus, it can be computed efficiently without computing a Kronecker product._

We provide the proof in Appendix A. The approach is similar to that of George et al. , who correct KFAC's eigenvalues towards the diagonal Gauss-Newton, but solves the problem of adding a full-rank diagonal instead of a rank-1 outer product to the KFAC eigenbasis.

### Learning Regularization with the Marginal Likelihood

To optimize the potentially millions of regularization parameters, for example, arising from a parameter-wise prior, we employ the marginal likelihood as a differentiable objective. Optimizing regularization parameters has the advantage that different (groups of) parameters will be regularized differently and, therefore, become easier to prune. While it would be intractable to optimize that many regularization parameters using validation-based forms of optimization, the marginal likelihood can be estimated and differentiated during training [12; 31; 32].

Automatically determining the relevance of parameter-groups (ARD) is a common approach in Bayesian learning that can lead to sparsity and smaller parameter magnitudes [33; 34] and has been used especially in linear models. The marginal likelihood provides an objective that automatically regularizes irrelevant parameter groups more to lower their magnitude. Therefore, it implements a Bayesian variant of Occam's razor, finding the simplest model that explains the data well .

Mathematically, all the prior parameters \(\) constitute the hyperparameters of the model \(\) in the log marginal likelihood (Equation (1)) that we optimize interleaved with the neural network parameters. When optimizing the prior parameters, we use gradient ascent

\[_{t+1}_{t}+_{ } p(|)|_{=_{t}},\] (6)

or adaptive optimizers like Adam . We follow Immer et al.  and optimize the Laplace approximation to the marginal likelihood after an initial burn-in phase with a certain frequency. We describe the optimization process and the related hyperparameters in Appendix D.4

### Optimal Posterior Damage (OPD)

While sparsity regularization learned by marginal likelihood training can be advantageously combined with _any pruning criterion_, like Single-shot Network Pruning [SNIP; 36], variants of Gradient Signal Preservation [GraSP; 37; 38; 39], or magnitude pruning , we further propose a new pruning criterion that uses our Laplace approximation and extends the unstructured Optimal Brain Damage (OBD) pruning criterion . While OBD traditionally uses the Hessian (approximation) \(_{}\) of the loss, we propose to adapt it to use the posterior precision \(_{}\), which additionally includes the prior precision \(\). The importance score \(S(_{p})\) for parameter \(_{p}\) becomes

\[S(_{p})=_{pp}_{p}^{2}\] (7)

where \(_{pp}\) denotes the posterior precision for the parameter \(_{p}\), extracted from the diagonal of the posterior precision matrix \(_{}\). We call this novel posterior-based pruning criterion _Optimal Posterior Damage_ (OPD). Intuitively, individual weights with high scores indicate certainty of the posterior distribution and a significant contribution to the model's functionality, as indicated by the magnitude.

We also propose a structured version of OPD by aggregating the score over a set of parameters \(g\), i.e.,

\[S(g)=_{p g}S(_{p})=_{p g}_{pp}_{p}^{2}\] (8)

In practice, the structured set of parameters \(g\) corresponds to all parameters along one dimension of the weight matrix inside a layer, in order to reduce the size of the matrix multiplications. Since subsequent layers might have significantly different weight matrix dimensions impacting the magnitude of the aggregated sum, we opt for uniform structured pruning to guarantee a fair pruning treatment across all layers. This means we prune each layer by the same target percentage, reducing the dimensions of each layer by the same proportion relative to the unpruned model. This approach contrasts withachieving a global target sparsity that varies across layers, which can make it more difficult to consistently compress and adjust the model's size.

Moreover, as removing a full structure is more aggressive, we also apply gradual pruning during training. Finally, we omit pruning the final layer to mitigate overly strong impact on classification accuracy and computational stability .

When using our SpaM approach, the precomputed precision matrix from the Laplace approximation can be reused to compute OPD without computational overhead, in contrast to the other pruning criteria, which often require additional computations to be performed. Note that we will also show in our experiments that OPD additionally avoids the need for potentially expensive fine-tuning after pruning. Moreover, even in the case of maximum a posteriori (MAP) training, Laplace approximations of the inverse Hessian at \(_{*}\) can be additionally computed to approximate OPD. Finally, the OPD criterion can not only be computed _post-hoc_ after training, but even _online_ during training.

## 4 Related work

Laplace-approximated BNNs.From the early inception of Bayesian neural networks [41; 42], the Laplace approximation was a popular inference method . In recent years, it has undergone a renaissance [18; 19; 20; 14], including critical work on using more scalable approximations for the associated marginal likelihood in the context of model selection [11; 12; 43], which we use in our framework. To the best of our knowledge, we are the first to study the benefits of this Laplace-approximated marginal likelihood in the context of sparsification of deep neural networks. However, similar methods that automatically quantify the relevance (ARD) of parameters have been derived and used for linear, bilinear, and kernel models [34; 44; 45] as an alternative to the Lasso. More recently, van der Ouderaa et al.  and Bouchiat et al.  used the ARD mechanism in deep learning to select layers and features by regularization, respectively.

Pruning neural networks.Various pruning criteria have been proposed to determine the importance of model parameters. Many criteria prune based on the weight magnitude [7; 48; 49] but usually required additional fine-tuning to recover accuracy. Sun et al.  proposed to combine activation and weight norms for pruning without fine-tuning. Other approaches include pruning using first-order information based on connectivity  or synaptic flow conservation , or second-order information aiming at preserving gradient flow [37; 38; 39]. Recently, van der Ouderaa et al.  focused on pruning LLMs based on a second-order Taylor expansion. In contrast, OPD uses second-order information provided by the posterior precision given by the Laplace approximation. Beyond pruning criteria, there have been many approaches to prune at initialization [36; 37; 50], during training [52; 53], and after training [7; 8]. In particular, multiple works proposed to leverage specific

    &  &  \\   & & 80 & 85 & 90 & 95 & 99 \\   & MAP & 88.06 (\(\)0.12) & 82.32 (\(\)0.44) & 64.08 (\(\)1.32) & 37.52 (\(\)2.34) & 17.32 (\(\)1.01) \\  & SpaM & 90.78 (\(\)0.66) & 90.78 (\(\)0.65) & **90.68 (\(\)0.65)** & **89.98 (\(\)0.61)** & **66.28 (\(\)5.89)** \\  GraSP & MAP & 82.87 (\(\)0.48) & 68.78 (\(\)1.88) & 48.65 (\(\)2.69) & 26.46 (\(\)1.86) & 15.75 (\(\)0.80) \\  & SpaM & 91.50 (\(\)0.66) & **90.94 (\(\)0.65)** & 89.42 (\(\)0.71) & 82.18 (\(\)2.65) & 41.48 (\(\)7.95) \\  SNIP & MAP & 53.96 (\(\)2.72) & 37.74 (\(\)2.21) & 26.74 (\(\)3.17) & 13.88 (\(\)0.87) & 12.58 (\(\)0.36) \\  & SpaM & 67.40 (\(\)5.68) & 52.62 (\(\)6.84) & 33.75 (\(\)5.71) & 17.06 (\(\)2.23) & 11.90 (\(\)0.51) \\  Magnitude & MAP & 88.17 (\(\)0.12) & 81.92 (\(\)0.37) & 61.60 (\(\)1.11) & 32.88 (\(\)1.52) & 16.12 (\(\)0.90) \\  & SpaM & **91.55 (\(\)0.64)** & 90.92 (\(\)0.64) & 89.23 (\(\)0.62) & 81.80 (\(\)2.22) & 41.78 (\(\)7.20) \\  Random & MAP & 11.25 (\(\)0.48) & 12.15 (\(\)0.92) & 11.65 (\(\)0.62) & 10.45 (\(\)0.17) & 10.27 (\(\)0.17) \\  & SpaM & 11.00 (\(\)0.48) & 10.47 (\(\)0.86) & 10.56 (\(\)1.15) & 10.01 (\(\)0.45) & 9.81 (\(\)0.61) \\   

Table 1: Accuracies of pruned ResNets on CIFAR-10. The best training method for each pruning criterion is highlighted in green, where we see that SpaM improves performance for all criteria except the random baseline. The best performances overall at each sparsity level are shown in **bold**, showing that our cheap OPD criterion outperforms the others at high sparsities.

training schemes promoting zero-invariant parameter groups for structured pruning [54; 55]. In contrast, SpaM induces sparsifiability during training, and is agnostic about the criterion.

## 5 Experiments

We conduct experiments on various datasets and models and outline our experimental setup in detail in Appendix D. We compare MAP training with our proposed SpaM approach with different priors, comparing our OPD pruning criterion with random pruning, magnitude pruning, SNIP , GraSP [37; 38], and SynFlow . We show that **SpaM improves pruning performance with different pruning criteria**, especially at higher sparsities, and that **our OPD criterion often outperforms the other criteria**. This observation extends not only to predictive accuracy, but also uncertainty estimation. Moreover, we show that the choice of prior can play a significant role, and we **introduce parameter-wise and unit-wise priors** for the KFAC approximation. Finally, we show that SpaM and OPD also work in a structured pruning setting, leading to **significant computational benefits**. The code for our methods and experiments can be found at https://github.com/fortuinlab/spam-pruning.

### SpaM Improves Performance at High Sparsities

We compare SpaM to MAP training with different pruning criteria, including OPD, across different models on tabular, vision, and language datasets. For SpaM in this unstructured pruning context, we use the diagonal Laplace approximation with a parameter-wise prior. Encouragingly, MAP and SpaM reach comparable performance during training, showing that the increased sparsifiability of SpaM comes at no additional cost in unpruned performance (see Figure B1 in the appendix).

We see in Table 1 and Figure 2 that SpaM drastically improves the performance for many pruning criteria, especially magnitude pruning, GraSP, and OPD. We also see that OPD, despite being a cheap byproduct of our marginal likelihood computation, often outperforms the other pruning criteria, especially at higher sparsities. For instance, at 95 % pruning rate (i.e., with 20x fewer parameters), our combination of SpaM and OPD still retains almost the same performance as the unpruned model on vision tasks, while the other pruning criteria with MAP training have dropped to essentially unusable performance levels at this sparsity.

Figure 2: Predictive performance as a function of sparsity level in unstructured pruning. We see that SpaM improves the performance over MAP training across most architectures, datasets, and pruning criteria, and that OPD often outperforms the other pruning criteria. Both of these effects are particularly visible at higher sparsity levels. The black star in each subfigure denotes the performance of the unpruned models, which is often identical to the performance of models pruned at 20% sparsity.

**Fine-tuning.** We see in Figure 10 in the appendix that some of this performance difference can be remedied by costly fine-tuning of the networks after pruning, which however still does not allow the other methods to reach the full SpaM-OPD performance. Interestingly, in the case of OPD, this does not further improve its already near-optimal performance.

**Online pruning.** Figure 11 in the appendix shows that our online version of SpaM, which uses the marginal likelihood and OPD during training to iteratively prune the network, reaches comparable performance levels to the post-hoc version, thus offering a computationally even more convenient way to effectively sparsify neural networks.

**Uncertainty estimation.** Given that SpaM is a Bayesian method, it does not only offer high predictive accuracies but also calibrated uncertainty estimates. Indeed, we see in Figure 3 that the trends we have seen for accuracy also apply for negative log-likelihood, expected calibration error, and the Brier score. Again, SpaM improves the uncertainty estimates over MAP training, OPD outperforms most other criteria, and we achieve well-calibrated models up until very high sparsity levels. Note that the random baseline also achieves a low ECE at high sparsity levels because it essentially reverts to random guessing, which is a known weakness of the ECE metric .

Figure 4: Comparison of different priors and Hessian approximations for SpaM-OPD unstructured pruning. The unit-wise and parameter-wise priors show better performance at high sparsity levels, with the parameter-wise one bridging the gap between Diag and KFAC LA.

Figure 3: Uncertainty estimation with pruned ResNets on CIFAR-10. We see that SpaM improves uncertainty estimation in terms of NLL, ECE, and Brier score for many pruning criteria and that our OPD criterion outperforms the other criteria, especially at high sparsities.

### Influence of Priors on Sparsifiability

To understand the influence of the prior and Hessian approximation on performance in our proposed SpaM-OPD approach, we compare diagonal and KFAC approximations with scalar, layer-wise, unit-wise, and parameter-wise priors. Note regarding the latter two, that in this work, we are the first to implement them for the KFAC approximation, thus contributing to the general framework of Laplace-approximated BNNs , independent of the pruning use case.

We see in Figure 4 that our newly introduced unit-wise and parameter-wise priors for KFAC indeed outperform the others, especially at high sparsities. When comparing KFAC to the diagonal approximation, we see that KFAC often leads to slightly better performance at lower sparsity levels. However, we also see that the relatively simple choice of parameter-wise prior and diagonal Hessian approximation, as used in our previous experiments above, is a strong baseline across the board and can be recommended as a safe default option for unstructured pruning. Note that the unit-wise priors

Figure 5: Similarly to unstructured pruning, we see in this experiment on structured pruning that SpaM (using a unit-wise prior) improves performance over MAP and that OPD mostly outperforms other pruning criteria, especially at higher sparsity levels. The black stars reflect the performance of the unpruned models.

Figure 6: Structured pruning with LeNet on FashionMNIST, using unit-wise priors. We see that our SpaM-OPD dominates the Pareto frontier, in terms of predictive performance as a function of computational time and memory cost, and is particularly competitive at lower costs.

can be especially useful for structured pruning, as we will see in the following experiment. More detailed prior comparisons can be found in Appendix B.3.

### SpaM Extends to Structured Sparsification

Here, we study the effect of SpaM and OPD in the more challenging setting of eliminating entire network structures, such as convolutional kernels. Studying different network architectures, we aim to generalize our unstructured pruning approach to the setting of structured pruning, where the structures can be freely defined depending on the use case.

Encouragingly, we see in Figure 5 that our findings from the unstructured case transfer qualitatively also to the structured case, with SpaM-OPD outperforming the baselines at high sparsities. Crucially, while the sparsity patterns generated by unstructured pruning are more difficult to translate into computational benefits, structured pruning directly leads to computational savings on standard GPUs (see also Figure B11 in the appendix). We see in Figure 6 that SpaM-OPD dominates the Pareto frontier of the tradeoff between performance and computational cost at high sparsities (i.e., low costs), yielding 10x-20x savings in FLOPS and memory consumption with only minimal deterioration in performance. This positions our proposed framework as a potentially promising approach for the deployment of AI models in resource-constrained environments.

## 6 Conclusion

We have shown that the Bayesian marginal likelihood, with its associated _Occam's razor_ effect, can be used _during training_ to select neural network models that are _inherently more sparsifiable_. Crucially, we have shown that this sparsifiability _extends across different pruning criteria_ and enables _large gains in performance and uncertainty estimation_, especially at _high sparsity levels_. Conveniently, the computations needed for the marginal likelihood estimation using the Laplace approximation can be re-used to define a _novel pruning criterion called OPD_, which outperforms many existing (more expensive) criteria in our experiments. We have also presented _guidelines for choosing priors_ within our framework and have shown that even in the challenging setting of _structured pruning_, our proposed SpaM approach can yield up to _20x savings in computational time and memory_, with only small reductions in performance. Our work thus offers a promising path towards pruning large AI models at high sparsity levels for deployment on resource-constrained devices.

Limitations.Our approach naturally inherits some limitations of the Laplace approximation, for instance, the fact that it only captures the local geometry of a single posterior mode or potential numerical instabilities in the Hessian computations when used with low-precision weights. Moreover, it accrues an additional computational cost compared to MAP training, which is then, however, amortized by the computational savings during the deployment of the sparsified model.

#### Acknowledgments

AI acknowledges funding through a Max Planck ETH Center for Learning Systems (CLS) fellowship. VF was supported by a Branco Weiss Fellowship.