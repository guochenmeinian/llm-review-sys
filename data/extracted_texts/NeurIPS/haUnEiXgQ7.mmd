# Vision-Language Models are Strong Noisy Label Detectors

Tong Wei\({}^{1,2,3}\) Hao-Tian Li\({}^{1,2}\) Chun-Shu Li\({}^{1,2}\) Jiang-Xin Shi\({}^{3,4}\)

**Yu-Feng Li\({}^{3,4}\) Min-Ling Zhang\({}^{1,2}\)**

\({}^{1}\)School of Computer Science and Engineering, Southeast University, Nanjing, China

\({}^{2}\)Key Laboratory of Computer Network and Information Integration (Southeast University),

Ministry of Education, China

\({}^{3}\)National Key Laboratory for Novel Software Technology, Nanjing University, China

\({}^{4}\)School of Artificial Intelligence, Nanjing University, China

{weit,liht}@seu.edu.cn

###### Abstract

Recent research on fine-tuning vision-language models has demonstrated impressive performance in various downstream tasks. However, the challenge of obtaining accurately labeled data in real-world applications poses a significant obstacle during the fine-tuning process. To address this challenge, this paper presents a **D**enoising **F**ine-**T**uning framework, called DeFT, for adapting vision-language models. DeFT utilizes the robust alignment of textual and visual features pre-trained on millions of auxiliary image-text pairs to sieve out noisy labels. The proposed framework establishes a noisy label detector by learning positive and negative textual prompts for each class. The positive prompt seeks to reveal distinctive features of the class, while the negative prompt serves as a _learnable threshold_ for separating clean and noisy samples. We employ parameter-efficient fine-tuning for the adaptation of a pre-trained visual encoder to promote its alignment with the learned textual prompts. As a general framework, DeFT can seamlessly fine-tune many pre-trained models to downstream tasks by utilizing carefully selected clean samples. Experimental results on seven synthetic and real-world noisy datasets validate the effectiveness of DeFT in both noisy label detection and image classification tasks. Our source code is available at https://github.com/HotanLee/DeFT.

## 1 Introduction

Vision-language models pre-trained on large-scale image-text pairs, such as Contrastive Language-Image Pretraining (CLIP) , have gained widespread adoption in various machine learning tasks including few-shot learning , multi-label learning , and long-tail recognition . Recently, CLIP has shown impressive generalization performance to many downstream tasks without the need for adaptation . Zero-shot CLIP leverages the strong alignment of learned visual and textual features and classifies unseen images by comparing the similarity of image embedding with textual class prompts. Despite the good zero-shot performance, fine-tuning becomes necessary when the data distribution of downstream tasks significantly deviates from the CLIP training source. Two popular fine-tuning paradigms are commonly employed for adaptation, i.e., full fine-tuning (FFT), which modifies all model parameters, and parameter-efficient fine-tuning (PEFT), which fixes the pre-trained parameters while adding a few learnable parameters for adaptation. While promising, fine-tuning CLIP necessitates perfectly labeled datasets which may not be readily available in many real-world tasks, thereby hindering their broader applicability.

To tackle this issue, this paper investigates the fine-tuning of CLIP utilizing imperfectly labeled datasets, i.e., datasets with noisy labels. Intuitively, direct adaptation using noisy labels can significantly degrade the performance. To mitigate the negative impact of noisy labels, researchers have proposed various approaches for learning with noisy labels, including sample selection techniques [10; 44; 43] and noise-robust learning methods [37; 27]. However, the exploration of this problem in the context of CLIP adaptation remains limited. This paper aims to fill this research gap by demonstrating that CLIP can be leveraged as effective noisy label detectors, capitalizing on their strong alignment between visual and textual features. To ensure the generality, we follow the basic idea in mainstream studies of learning with noisy labels. Firstly, we identify potentially mislabeled samples, and subsequently, we adapt the model parameters using carefully selected clean samples.

In the first phase, we propose to construct a noisy label detector by learning positive and negative textual prompts for each class. The positive prompt aims to uncover distinguishable features specific to the class, while the negative prompt functions as a learnable "threshold" for identifying noisy labels. Our noisy label detector can be derived from the similarity between embeddings of a training image and positive/negative prompts associated with its assigned class. If the image exhibits a higher similarity score with the positive prompt than the negative prompt, it is considered a correctly labeled sample, i.e., a clean sample. This design draws inspiration from previous studies that have shown the robustness of prompt tuning to noisy labels, particularly in the presence of high noise ratios . To optimize the positive prompt, we align its representation with image features from the corresponding class during training. For the negative prompts, we introduce a novel negative learning objective. It is important to note that calculating similarities between images and textual prompts requires a strong alignment between these two modalities in the downstream task. To achieve this, we utilize PEFT, such as Visual Prompt Tuning (VPT) , for the adaptation of the visual encoder. This decision is based on a key finding that FFT can distort pre-trained feature representations when noisy labels are present, which is discussed in Section 3.2.

In the second phase, we can adapt the pre-trained model after acquiring carefully selected clean samples. It is worth noting that not only can we employ pre-trained CLIP for adaptation, but we can also utilize other pre-trained visual backbones such as ResNet  and MAE . While the model adaptation phase may seem unnecessary since we can leverage the learned textual prompts for zero-shot classification without further fine-tuning, we emphasize its necessity by the improved performance on curated downstream datasets, particularly in fine-grained classification tasks. In this step, we learn a linear classifier and fully fine-tune the visual encoders using only selected clean samples. The rationale behind this approach is twofold: 1) the linear classifier tends to generalize better than textual class prompts, and 2) full fine-tuning can effectively adapt model parameters to address significant domain shifts, surpassing the capabilities of PEFT. In our implementation, the training is conducted for only 10 epochs, resulting in a minimal increase in computation cost.

Our main contributions can be summarized as follows:

* We propose DeFT, a simple yet effective framework for learning with noisy labels. DeFT offers several compelling advantages over existing methods: instance-dependent (no information required from entire training data), robust to various types of noisy labels, and generalizable to many pre-trained models.
* We conduct extensive experiments and show that DeFT achieves superior performance in both noisy label detection and image classification tasks on a wide range of synthetic and real-world datasets.
* We provide in-depth empirical analysis, providing insights to understand the effectiveness of DeFT. We hope that this work will serve as a springboard for future works on noisy label detection with multi-modal features.

## 2 Related Work

**Learning with Noisy Labels** A diverse variety of approaches have been proposed for addressing learning with noisy labels and can be broadly divided into twofold: robust learning and clean sample selection [9; 32]. Due to the susceptibility of traditional cross-entropy loss to noisy labels, several methods adopt noise-robust loss functions [8; 51; 37; 6] and regularization techniques [27; 38; 16] to enhance model robustness. For example, symmetric cross-entropy loss  introduces a noise-robust objective function for fast convergence and better performance. Early-learning regularization  counteracts the influence of the noisy labels by introducing a regularization term that integrates estimated target probabilities. However, empirical findings suggest that robust model learning facesthe problem of maintaining a balance between robustness and accuracy [51; 35]. Conversely, sample selection approaches strive to select clean samples from noisy datasets using specific criteria. Based on the memorization effect identified in deep neural networks , Numerous methods adopt the small-loss strategy that treats samples with small loss as clean ones [10; 15; 25; 19]. Co-teaching  selects a fraction of training data in each mini-batch based on the noise ratio, whereas DivideMix  fits a Gaussian mixture model to dynamically divide the training samples. Although these methods demonstrate strong performance, they inevitably overlook the significance of clean hard samples with a large loss. Besides, all the aforementioned approaches concentrate exclusively on learning from scratch with label noise. Our proposed approach leverages the pre-trained multi-modal information from visual-language models to construct a noisy label detector, resulting in improved sample selection performance.

**Fine-Tuning Vision-Language Models** Recent years have witnessed incredible progress in pre-trained visual-language models [29; 17; 49]. As a representative method, CLIP  employs contrastive pre-training to acquire informative representations from extensive image-text pairs. Despite the remarkable zero-shot performance of CLIP, its sensitivity to hand-crafted textual prompts is significant. Prompt-tuning aims to learn the optimal prompts from target data by replacing the textual templates with adaptable soft prompts, which demonstrates superior performance in the context of few-show learning [53; 52; 20] and multi-label learning [33; 14]. However, further enhancements can be achieved by adapting the visual encoder to accommodate larger datasets in downstream tasks. A standard approach to obtain task-specific representations is fine-tuning all model parameters. While FFT yields significant performance on curated datasets, recent work  empirically finds that FFT can result in feature distortion in the presence of massive noisy labels. In contrast, PEFT  has been verified to improve generalization capabilities for mitigation of overfitting issues , which fix the pre-trained model and introduce a few number of learnable parameters for adaptation, such as VPT , LoRA , and AdaptFormer . Considering the distinct property of fine-tuning methods, this paper aims to delineate the most effective paradigm for model adaptation on noisy downstream datasets, which remains under-explored yet holds significant promise.

## 3 Preliminary and Initial Findings

### Zero-Shot CLIP

This paper focuses on a \(K\)-class image classification task, where the objective is to categorize an image \(^{d}\) and assign it a label \(y[K]=\{1,2,,K\}\). Benefiting from large-scale datasets and high-capacity models, CLIP  has demonstrated remarkable performance in zero-shot classification by computing the similarity between visual and textual representations. Specifically, with hand-crafted textual prompts such as "a photo of a [CLS]", where the class token is replaced by a specific class name, the prediction probability is computed as:

\[p(y=k)=(,_{k})/)}{_{k=1}^ {K}((,_{k})/)},\] (1)

where \((,)\) denotes the cosine similarity between the extracted image features \(\) and text features \(\), and \(\) is the softmax temperature.

While zero-shot classification with CLIP is promising, its performance can be further enhanced by adapting CLIP on downstream labeled datasets \(=\{(_{i},y_{i})_{i=1}^{N}\}\) to incorporate more task-specific representations. In this paper, we investigate the presence of noisy labels in the training dataset \(\), which is prevalent in real-world applications. We use \(r\) to denote the noise ratio of a training set, meaning that \(N r\) training samples are incorrectly labeled, i.e., the assigned label \(y\) differs from the ground-truth label \(y^{*}\). Since \(y^{*}\) is not accessible, our goal is to fine-tune the CLIP model using training labels \(y\) by eliminating the negative influence of noisy labels. Considering the feature distortion induced by noisy labels , the key challenge lies in obtaining distinguishable yet robust representations when adapting CLIP on noisy datasets.

### Fine-tuning CLIP on Downstream Datasets

To discern the most effective method for CLIP adaptation, we conduct empirical studies on various datasets as shown in Figure 1. Specifically, we utilize three fine-tuning approaches to adapt the pre-trained CLIP model and compare their performance on both noisy and clean datasets, including 1) **FFT** which updates the entire model parameters, 2) **VPT** which fixes pre-trained model parameters and prepends a small subset of extra learnable parameters to the visual encoder during fine-tuning, and 3) **VLPT** (Vision-Language Prompt Tuning) which integrates both visual and textual learnable prompts into the fixed pre-trained model for fine-tuning. For FFT and VPT, we learn an additional linear classifier, while VLPT directly utilizes the learned textual prompts for classification. We present our three primary findings in the following.

**VPT benefits representation learning in the presence of massive noisy labels.** Intuitively, utilizing FFT for CLIP adaptation yields improved performance by leveraging its substantial capacity to incorporate task-specific representations. However, the efficacy of FFT diminishes notably when applied to datasets containing noisy labels, as shown in Figure 0(a). This decline is attributed to the pronounced distortion of representations with an escalating noise ratio . In contrast, VPT benefits representation learning when adapting CLIP on noisy data, particularly under high levels of noise. As only a small set of parameters is introduced, VPT efficiently retains the generalization ability from image-text pre-training while enhancing classification performance on downstream tasks, making it a robust and effective fine-tuning approach for handling noisy downstream datasets.

Textual classifier is robust to noisy labels.Classification with learnable textual prompts (e.g., CoOp ) leverages the multi-modal information in pre-trained vision-language models and enhances the alignment of visual and textual representations on downstream tasks. Recent literature has substantiated its robustness with a frozen visual encoder in the context of few-shot learning . From Figure 0(a), we observe that VLPT with a textual classifier (green line) consistently surpasses VPT with a traditional linear classifier for classification (orange line), especially under severe noise. The improvement in performance across diverse noise ratios further affirms the robustness of learnable textual prompts in mitigating the impact of label noise for model adaptation.

**FFT enhances visual recognition on clean datasets.** Although previous findings show that FFT is more susceptible to noisy labels compared to VPT and VLPT, Figure 0(b) demonstrates its enhanced discriminative ability when training data is clean. This superiority is particularly significant on fine-grained datasets such as Stanford-Cars and CUB-200-2011, with an average improvement of 2.81% against VPT. It is important to note that VLPT exhibits the worst performance on clean datasets. This is primarily due to the implicit regularization of pre-trained textual information when tuning the context of textual prompts, as explained in the recent study .

## 4 The Denoising Fine-tuning Framework

Based on the findings in Section 3.2, we now present a novel denoising fine-tuning framework, DeFT, which comprises two pivotal phases. In the first phase, DeFT learns dual textual prompts to separate clean and noisy samples while adapting the visual encoder utilizing PEFT methods. In the second phase, DeFT re-adapts the pre-trained model using FFT, leveraging the curated clean samples to further boost visual recognition performance. Figure 2 illustrates our proposed framework.

### Identifying Noisy Labels with Dual Prompts

Given the empirically demonstrated robustness of textual prompts to label noise, we intend to harness this property to identify corrupted samples in noisy downstream datasets. The initial strategy is to select a subset of samples with pronounced similarity between their visual and textual representations and designate them as clean. However, this method relies on either prior knowledge of the noise ratio or the manual setting of thresholds, which restricts its practical applicability. Inspired by recent works [33; 14; 36], we utilize dual textual prompts instead of a single prompt to overcome these limitations. Concretely, we design a class-specific pair of _positive_ and _negative_ prompts for the textual encoder,

Figure 1: Comparison of different fine-tuning methods under (a) various ratios of noisy labels and (b) clean datasets.

denoted as \(^{+}_{k}\) and \(^{-}_{k}\), respectively:

\[^{+}_{k}=[V]^{+}_{1}[V]^{+}_{2}[V]^{+}_{3}...[V]^{+}_{M}[]_ {k}\] (2)

\[^{-}_{k}=[V]^{-}_{1}[V]^{-}_{2}[V]^{-}_{3}...[V]^{-}_{M}[] _{k}\] (3)

Here, \([V]_{m}\) is the word vector of context (with dimension 512 in CLIP), \(M\) is the length of context, and \([]_{k}\) is the \(k\)-th provided category name. The positive prompt aims to uncover distinguishable features by maximizing the similarity between the image features and their corresponding text features, while the negative prompt serves as a learnable sample-dependent similarity threshold to select clean data. Specifically, the _learnable threshold_\(_{i}\) for the \(i\)-th training sample with label \(y_{i}=k\) is formulated as:

\[_{i}=(_{i},^{-}_{k})\] (4)

which represents the cosine similarity between the extracted image features and negative text features. Based on this threshold, the clean subset of dataset \(\) can be constructed as follows:

\[^{}=\{(_{i},y_{i})(_{i}, ^{+}_{k})>_{i}y_{i}=k\}\] (5)

The proposed selection strategy surpasses the conventional loss-based approaches in two aspects: 1) It is more practical by utilizing data-driven thresholds thus eliminating the requirement for prior knowledge like noise ratio, and 2) the integration of text modality enhances its robustness to label noise, making it capable of identifying challenging hard noise. Nevertheless, the primary dilemma lies in the optimization of positive and negative prompts using noisy downstream datasets.

### Optimization for Noisy Label Detector

It can be an intricate task to optimize the threshold \(_{i}\) in an end-to-end manner since it is indirectly involved in the forward propagation. However, we can still optimize the associated parameters by formulating the clean probability of the \(i\)-th image with \(y_{i}=k\) as:

\[p^{}_{ik}=(_{i},^{+}_{k})/ )}{((_{i},^{+}_{k})/)+(( _{i},^{-}_{k})/)}\] (6)

Obviously, selecting samples with the clean probability \(p^{}_{ik}>0.5\) is equivalent to the specified criteria in Eq. (5). In this way, the original threshold-based selection approach can be transformed into \(K\) binary classification tasks to determine whether a sample is clean or not. Concretely, given a sample \((_{i},y_{i})\) from noisy datasets \(\), we can employ the dual prompts of class \(y_{i}\) as a binary classifier to identify label noise. If the classifier outputs a positive prediction of \(_{i}\), it is categorized as a clean sample.

To optimize the noisy label detector, we need to construct positive and negative training samples for binary classification, i.e., images with correctly and wrongly assigned labels. Inspired by the prior work on negative learning , we designate each image with a randomly picked complementary label \(\) to form negative samples. For positive samples, we initiate with the original supervision in

Figure 2: Illustration of the proposed DeFT framework. Left: We identify noisy labels with learnable dual textual prompts and improve image-text alignment by optimizing PEFT modules. Right: Adapt pre-trained models using FFT on selected clean samples.

\(\) and refine it with pseudo labels \(\) generated by Eq. (1). The training loss is then formulated as follows:

\[_{dp}=_{i=1}^{N}_{nll}(_{i}^{}, )+_{nll}(1-_{i}^{},)\] (7)

where \(_{i}^{}=\{p_{ik}^{}\}_{k=1}^{K}\) is the clean probability and \(_{nll}(,)\) is the negative log-likelihood loss where \(_{nll}(_{i},y)=- p_{iy}\).

Additionally, to enhance task-specific representations of the noisy label detector, we harness PEFT for the adaptation of the visual encoder, considering its robustness under massive noisy labels compared with FFT, as per our previous finding (see Figure 1a). This is achieved by maximizing the alignment between image embeddings and their corresponding positive textual features:

\[_{sim}=-_{i=1}^{N}(_{i},_{i}^{+})/)}{_{k=1}^{K}((_{i},_ {k}^{+})/)}\] (8)

Note that \(_{sim}\) is exclusively computed on \(^{}\) after the warm-up stage to further eliminate the impact of label noise on representations. With the objective \(=_{dp}+_{sim}\), DeFT effectively sieves out noisy samples with learned thresholds while uncovering distinguishable visual representations on downstream tasks.

### Model Adaptation using Clean Data

Although the learned positive textual prompt can be readily employed for classification, its performance may be suboptimal on curated clean datasets, as demonstrated in our previous finding (see Figure 1b). To mitigate this problem, we introduce the model adaptation phase which learns a linear classifier using selected clean samples, i.e., \(^{}\). Moreover, our findings also indicate that FFT outperforms PEFT on clean datasets, so we remove the PEFT modules and fully fine-tune the pre-trained model parameters for model adaptation. We minimize the classic cross-entropy loss \(_{ce}(,y)=-)}{_{k=1}^{K}(z_{k})}\), where \(=\{z_{k}\}_{k=1}^{K}\) represents the logits predicted by the linear classifier. It is noteworthy that with the clean subset of training data constructed by Eq. (5), the second phase can be applied universally to a wide range of pre-trained models, regardless of their backbones. The versatility of our approach is demonstrated in the experiments.

## 5 Experiment

We now proceed to the experiments, demonstrating the advantages of our approach in noisy label detection and image classification on both synthetic and real-world datasets.

### Experimental Settings

Synthetic DatasetsWe first evaluate the performance of DeFT on four image classification benchmarks by synthesizing noisy labels with varying noise types and ratios. Specifically, we conduct experimental analyses on widely-used CIFAR-100  and Tiny-ImageNet , as well as two fine-grained datasets Stanford-Cars  and CUB-200-2011 . Given the noise transition matrix \(T^{K K}\) with \(T_{ij}\) denoting the probability of the ground-truth \(y^{*}=i\) being flipped into a corrupted label \(y=j\), we introduce two types of label noise: 1) \(T_{ij}=p(y=j y^{*}=i)\), where the corruption probability is assumed to be only dependent on the true label, e.g., the _symmetric_ noise generated by replacing \(y^{*}\) with a random label for a given proportion of training samples [10; 25], and 2) \(T_{ij}=p(y=j y^{*}=i,x)\), which is a more realistic assumption considering the impact of instance \(x\) in the label corruption process, e.g., the _instance-dependent_ noise proposed in . We conduct extensive experiments on each dataset with noise ratio \(r\{0.2,0.4,0.6\}\) for symmetric noise and \(r\{0.2,0.3,0.4\}\) for instance-dependent noise.

Real-World DatasetsWe further examine the performance of DeFT on three real-world noisy label datasets: 1) CIFAR-100N  (\(r 0.4\)) is a variant of CIFAR-100 with real-world human annotations from Amazon's Mechanical Turk, 2) Clothing1M  (\(r 0.4\)) is a large-scale dataset consisting of 1 million clothing images of 14 classes collected from online shopping websites, and 3) WebVision  (\(r 0.2\)) contains 2.4 million images crawled from Flickr and Google using the 1,000 concepts in ImageNet ILSVRC12. Following previous works [25; 19; 24], we experiment on the first 50 classes of the Google image subset.

Evaluation MetricsWe use precision and recall metrics to evaluate the selection of clean samples. A higher precision indicates a greater proportion of actual clean samples in \(^{}\), while a higher recall means that more clean samples are identified from the noisy dataset. For image classification, We report the "best" test accuracy across all training iterations and the "last" test accuracy at the end of training.

Implementation DetailsWe adopt the pre-trained CLIP  with the Transformer as the text encoder and the ViT-B/16 as the image encoder. We use the SGD optimizer with a momentum of 0.9, a weight decay of \(5 10^{-4}\), and a batch size of 64. We run 10 epochs for both the noisy label detection phase and the model adaptation phase with learning rates \(3 10^{-2}\) and \(5 10^{-4}\), respectively. In the noisy label detection phase, we employ VPT  and CoOp  to adapt visual encoder and textual encoder respectively, and perform model warm-up for 1 epoch on all datasets. When identifying noisy labels on real-world datasets, we augment the condition in Eq. (5) with a constraint that the training label should be consistent with its predicted label. This is due to the existence of more complex noise patterns in real-world tasks. All experiments are conducted on a single NVIDIA GeForce RTX 3090.

### Performance for Noisy Label Detection

We conduct a thorough evaluation of DeFT to justify its effectiveness in detecting noisy labels. For this purpose, we simulate various types and ratios of label noise on four benchmark datasets. We compare our approach with two other sample selection strategies: 1) **Label-match** strategy, where a training sample is deemed clean if its given label matches that assigned by the CLIP zero-shot classifier, and 2) **Small-loss** strategy, which selects a proportion (noise ratio) of confident samples in each mini-batch, as commonly used in previous studies [10; 25; 19]. The precision and recall of sample selection are reported in Table 1. The results show that our proposed noisy label detector outperforms the compared strategies, with significant improvements observed in all dataset settings. The trivial label-match strategy tends to be conservative, leading to low selection recall of clean samples. In contrast, the small-loss strategy and DeFT achieve a better trade-off between precision and recall, making better use of training samples. Moreover, leveraging the informative multi-model information in the pre-trained vision-language model, DeFT surpasses the small-loss strategy

    &  &  &  &  &  &  \\   & Prec. & Rec. & Prec. & Rec. & Prec. & Rec. & Prec. & Rec. & Prec. & Rec. & Prec. & Rec. \\   \\  Label-match & 99.83 & 63.62 & 99.61 & 63.85 & 99.31 & 63.52 & 99.93 & 63.65 & 99.85 & 63.72 & 99.81 & 63.69 \\ Small-loss & 97.24 & 96.79 & 95.68 & 94.49 & 92.93 & 90.68 & 95.20 & 95.46 & 94.00 & 92.53 & 90.33 & 89.85 \\ DeFT (ours) & **99.51** & **97.77** & **98.75** & **97.91** & **97.04** & **97.27** & **98.47** & **97.88** & **96.32** & **97.63** & **94.08** & **95.28** \\ \(\) & \(7.22\) & \( 0.98\) & \( 3.07\) & \( 3.42\) & \( 4.11\) & \( 6.59\) & \( 3.27\) & \( 2.42\) & \( 2.32\) & \( 5.10\) & \( 3.75\) & \( 5.43\) \\   \\  Label-match & 99.92 & 60.81 & 99.83 & 60.79 & 99.50 & 60.66 & 99.91 & 60.58 & 99.84 & 60.53 & 99.76 & 60.47 \\ Small-loss & 97.25 & **96.93** & 95.33 & 94.48 & 92.63 & 90.89 & 94.74 & 95.17 & 93.66 & 92.35 & 90.41 & 89.71 \\ DeFT (ours) & **99.50** & 96.00 & **98.78** & **95.97** & **97.21** & **95.44** & **99.21** & **96.21** & **97.80** & **95.80** & **95.45** & **95.77** \\ \(\) & \( 2.25\) & \( 0.93\) & \( 3.45\) & \( 1.49\) & \( 4.58\) & \( 4.55\) & \( 4.47\) & \( 1.04\) & \( 4.14\) & \( 3.45\) & \( 5.04\) & \( 6.06\) \\   \\  Label-match & 99.97 & 60.34 & 99.86 & 60.27 & 99.70 & 60.71 & 99.85 & 60.34 & 99.82 & 60.32 & 99.80 & 60.25 \\ Small-loss & 96.92 & 96.56 & 93.71 & 93.21 & 89.46 & 87.79 & 96.94 & 97.78 & 96.72 & 95.96 & 95.25 & 94.48 \\ DeFT (ours) & **98.72** & **99.56** & **98.98** & **98.56** & **98.58** & **95.62** & **99.02** & **99.09** & **98.96** & **98.15** & **98.75** & **97.71** \\ \(\) & \( 1.80\) & \( 3.00\) & \( 5.27\) & \( 5.35\) & \( 9.12\) & \( 7.83\) & \( 2.08\) & \( 1.31\) & \( 2.24\) & \( 2.19\) & \( 3.50\) & \( 3.23\) \\   \\  Label-match & 99.92 & 53.26 & 99.74 & 53.13 & 99.46 & 53.02 & 99.96 & 53.39 & 99.96 & 53.32 & 99.74 & 53.69 \\ Small-loss & 96.74 & 96.32 & 93.69 & 92.84 & 84.10 & 82.01 & 96.91 & 97.33 & 96.49 & 95.59 & 93.98 & 93.96 \\ DeFT (ours) & **99.04** & **97.01** & **96.76** & **95.60** & **93.88** & **96.43** & **99.15** & **97.45** & **97.93** & **96.85** & **96.03** & **97.11** \\ \(\) & \( 2.30\) & \( 0.69\) & \( 3.07\) & \( 2.76\) & \( 9.78\) & \( 14.42\) & \( 2.24\) & \( 0.12\) & \( 1.44\) & \( 1.26\) & \( 2.05\) & \( 3.15\) \\   

Table 1: On each dataset, we compare the Precision (%) and Recall (%) of DeFT with CLIP label-match and small-loss to evaluate the clean sample selection performance. \(\) is the difference between the performance of DeFT and small-loss.

in detecting noisy labels, particularly under severe noise settings and fine-grained classification tasks. For instance, in Tiny-ImageNet with 60% symmetric noise, DeFT demonstrates significant enhancements of 4.58% and 4.55% in precision and recall, respectively, as well as an improvement of 9.78% and 14.42% in CUB-200-2011. Additionally, DeFT reduces the need for prior knowledge of noise ratios, making it a practical and effective approach to detect label noise in real-world tasks.

### Performance for Image Classification

We evaluate the performance of DeFT in image classification tasks against four baselines, including CE (Cross-Entropy loss), SCE , ELR , and GMM . To further verify the effectiveness of our method on real-world datasets, we also make comparisons with some recent sample selection-based works including RoLT, UNICON, LongReMix, and ProMix. We utilize FFT for adapting the pre-trained CLIP model for all approaches. More details can be found in the Appendix.

**Results on Synthetic Datasets** Table 2 presents the results on four synthetic noisy datasets. Compared with CE, the adoption of noise-robust loss functions such as ELR and SCE improves the classification performance on CIFAR-100 and Tiny-ImageNet under certain noisy label settings, e.g., ELR achieves the best performance on CIFAR-100 with 40% instance-dependent noise. However, these methods are not always effective and may even worsen the performance under varying types and ratios of noise, compared to the sample selection paradigm like GMM and DeFT. While GMM outperforms ELR and SCE in most cases, its performance degrades dreadfully on fine-grained datasets. Nevertheless, DeFT retains the most robust performance and generally advances all compared methods, especially on Stanford-Cars and CUB-200-2011. For example, DeFT boosts the test accuracy

    &  &  &  &  &  &  \\   &  &  &  &  &  &  &  &  &  \\   & CE & 86.71 / 86.70 & 84.06 / 82.60 & 81.05 / 77.45 & 87.30 / 87.18 & 84.60 / 83.64 & 78.41 / 75.66 \\  & ELR & 86.53 / 86.53 & 83.66 / 83.66 & 78.34 / 78.34 & 86.61 / 86.61 & 85.89 / 85.89 & **85.78 / 85.78** \\  & SCE & 86.82 / 86.82 & 83.84 / 83.84 & 78.90 / 77.71 & 86.61 / 86.61 & 83.99 / 83.20 & 80.06 / 73.45 \\  & GMM & 88.49 / 88.49 & 87.21 / 87.21 & 85.22 / 85.20 & 88.44 / 88.44 & 87.95 / 87.95 & 82.14 / 82.11 \\   & Ours & **89.38 / 89.35** & **88.17 / 88.11** & **85.81 / 85.72** & **89.38 / 89.35** & **88.68 / 88.68** & 85.75 / 85.74 \\   &  &  &  &  &  &  &  &  \\   & CE & 81.77 / 81.08 & 76.53 / 76.52 & 73.17 / 71.46 & 80.75 / 80.71 & 78.83 / 78.57 & 74.80 / 74.08 \\  & ELR & 79.40 / 79.40 & 77.13 / 77.13 & 73.74 / 73.74 & 79.98 / 79.98 & 77.13 / 77.13 & 73.74 / 73.74 \\  & SCE & 79.23 / 79.23 & 76.24 / 76.18 & 71.76 / 70.62 & 78.96 / 78.90 & 77.80 / 77.54 & 74.47 / 73.25 \\  & GMM & 81.91 / 81.88 & 80.37 / 80.37 & 43.47 / 43.47 & 81.84 / 81.79 & 81.26 / 81.26 & 79.01 / 79.01 \\   & Ours & **82.91 / 82.91** & **82.48 / 82.37** & **80.60 / 80.59** & **83.37 / 83.33** & **82.69 / 82.65** & **80.52 / 80.49** \\   &  &  &  &  &  &  &  &  \\   & CE & 89.75 / 89.74 & 85.10 / 84.89 & 71.70 / 71.55 & 89.13 / 89.06 & 85.94 / 85.92 & 80.59 / 80.59 \\  & ELR & 86.61 / 86.61 & 76.98 / 76.98 & 61.58 / 61.58 & 84.40 / 84.40 & 83.11 / 83.11 & 75.97 / 75.84 \\  & SCE & 91.11 / 91.11 & 87.73 / 87.45 & 79.09 / 79.09 & 90.34 / 90.34 & 87.35 / 86.23 & 83.50 / 80.69 \\  & GMM & 90.10 / 90.08 & 83.14 / 83.10 & 56.90 / 56.90 & 88.15 / 88.10 & 85.39 / 85.33 & 78.76 / 78.72 \\   & Ours & **92.13 / 92.12** & **90.75 / 90.75** & **85.72 / 85.45** & **92.19 / 92.15** & **90.77 / 90.77** & **89.74 / 89.68** \\   &  &  &  &  &  &  &  &  \\   & CE & 80.76 / 80.76 & 73.09 / 72.87 & 55.42 / 55.21 & 80.36 / 80.25 & 75.80 / 75.53 & 69.62 / 69.62 \\  & ELR & 77.70 / 77.70 & 68.26 / 68.26 & 50.17 / 49.88 & 78.32 / 78.32 & 73.16 / 73.08 & 63.57 / 63.34 \\  & SCE & 82.81 / 82.74 & 78.12 / 77.87 & 63.31 / 63.31 & 81.91 / 81.91 & 78.31 / 78.03 & 71.25 / 70.95 \\  & GMM & 75.79 / 75.73 & 64.39 / 64.38 & 42.84 / 42.84 & 75.73 / 75.65 & 69.95 / 69.95 & 56.13 / 55.80 \\   & Ours & **83.05 / 83.03** & **79.24 / 79.13** & **73.08 / 73.08** & **82.53 / 82.50** & **81.39 / 81.39** & **79.34 / 79.24** \\   

Table 2: Test accuracy (%) on synthetic datasets with _symmetric_ and _instance-dependent_ label noise.

by an average of 4.34% on Stanford-Cars compared to the best results of comparison methods. The results demonstrate the effectiveness of our proposed approach in tackling both symmetric and instance-dependent label noise.

Results on Real-world DatasetsThe results reported in Table 3 demonstrate the superiority of our method on real-world datasets, as DeFT surpasses other methods by a significant margin on all datasets. Notably, while most compared methods show little improvement on Clothing1M due to its fine-grained nature, both ELR and DeFT demonstrate substantial improvement. However, ELR exhibits a performance decrease on WebVision. Moreover, directly fine-tuning pre-trained CLIP models with cross-entropy (CE) achieves competitive results on WebVision, yet DeFT elevates this performance further, showcasing its ability to combat label noise in practical situations.

### Further Analyses

Necessity of Model AdaptationIn this experiment, we emphasize the necessity of the model adaptation phase in DeFT. Figure 3 demonstrates the test accuracy of DeFT without the model adaptation phase (w/o adap.), adaptation utilizing PEFT and FFT. Results show that fully fine-tuning the model on selected clean samples yields the best classification performance, especially on fine-grained datasets, such as Stanford-Cars and CUB-200-2011. The results unveil two primary insights: 1) the noisy label detector in DeFT exhibits strong capabilities in detecting label noise, yielding a nearly clean subset from noisy downstream datasets, and 2) employing FFT for model adaptation is more effective in mitigating significant domain shifts between downstream data and pre-training data, particularly evident in fine-grained datasets.

DeFT for Various Pre-trained ModelsIt is noteworthy that DeFT can seamlessly integrate with various pre-trained visual backbones during the model adaptation phase. To demonstrate the versatility of our proposed denoising fine-tuning framework, we conduct experiments on Clothing1M and apply FFT to a range of pre-trained models, including ViT-B/16 , ResNet-50 , ConvNeXt-T , and MAE-VIT-B . The results are presented in Table 4, where GCE  employs a noise-robust loss function akin to SCE. As a recent work, TURN  is an improved version of GMM, which applies linear probing to initialize the classifier and then uses FFT for adaptation. Compared with previous methods, DeFT exhibits the best performance on average. In particular, DeFT outperforms TURN by 4.51% and 3.27% when adopting ResNet-50 and MAE-ViT-B as the target pre-trained models, respectively.

   Architecture & CE & GCE & ELR & TURN & DeFT (Ours) \\  ResNet-50  & 66.02 & 66.19 & 66.19 & 66.31 & **70.82** \\ MAE-ViT-B  & 61.31 & 60.80 & 61.51 & 61.96 & **65.23** \\ ViT-B/16  & 68.98 & 69.74 & 68.73 & **70.28** & 69.84 \\ ConvNeXt-T  & 68.80 & 68.92 & 68.52 & 69.53 & **71.68** \\   

Table 4: Test accuracy (%) using various pre-trained models on Clothing1M. Partial results are sourced from . The best results across all methods are highlighted in bold, with the second-best results indicated by underscores.

Figure 3: Ablation studies. We report the test accuracy across varying noise ratios for the following variants: 1) **w/o adap.**: DeFT without the model adaptation phase, 2) **PEFT**: use PEFT for model adaptation phase, and 3) **FFT**: use FFT for model adaptation phase.

Conclusion

In this work, we delve into a new landscape for learning with noisy labels, departing from the classic single-modal toward a multi-modal regime. By learning dual textual prompts, we construct a new noisy label detection approach, which offers several compelling advantages: robust to various types of label noise, generalizable to many pre-trained models, and does not require the dynamics of training samples. We investigate the effectiveness of DeFT on a wide range of synthetic and real-world datasets, showing its superior performance in both noisy label detection and image classification tasks. Lastly, we demonstrate the advantage of parameter-efficient fine-tuning over full fine-tuning over noisy label detection. We hope our work will inspire future research toward multi-modal noisy label detection.