# Hierarchical Federated Learning with Multi-Timescale Gradient Correction

Wenzhi Fang

Purdue University

fang375@purdue.edu Dong-Jun Han

Yonsei University

djh@yonsei.ac.kr Evan Chen

Purdue University

chen4388@purdue.edu Shiqiang Wang

IBM Research

wangshiq@us.ibm.com Christopher G. Brinton

Purdue University

cgb@purdue.edu

###### Abstract

While traditional federated learning (FL) typically focuses on a star topology where clients are directly connected to a central server, real-world distributed systems often exhibit hierarchical architectures. Hierarchical FL (HFL) has emerged as a promising solution to bridge this gap, leveraging aggregation points at multiple levels of the system. However, existing algorithms for HFL encounter challenges in dealing with _multi-timescale model drift_, i.e., model drift occurring across hierarchical levels of data heterogeneity. In this paper, we propose a multi-timescale gradient correction (MTGC) methodology to resolve this issue. Our key idea is to introduce distinct control variables to (i) correct the client gradient towards the group gradient, i.e., to reduce _client model drift_ caused by local updates based on individual datasets, and (ii) correct the group gradient towards the global gradient, i.e., to reduce _group model drift_ caused by FL over clients within the group. We analytically characterize the convergence behavior of MTGC under general non-convex settings, overcoming challenges associated with couplings between correction terms. We show that our convergence bound is immune to the extent of data heterogeneity, confirming the stability of the proposed algorithm against multi-level non-i.i.d. data. Through extensive experiments on various datasets and models, we validate the effectiveness of MTGC in diverse HFL settings. The code for this project is available at https://github.com/wenzhifang/MTGC.

## 1 Introduction

In the past several years, federated learning (FL) has emerged as a prevalent approach for distributed training [17; 22; 11; 52; 24]. Conventional FL has typically considered a star topology training architecture, where clients directly communicate with a central server for model synchronization [35; 26]. Scaling this architecture to large numbers of clients becomes problematic, however, given the heterogeneity in FL resource availability and dataset statistics manifesting over large geographies [14; 17; 10]. In practice, such communication networks are often comprised of a _hierarchical architecture_ from clients to the main server, as observed in edge/fog computing [25; 38] and software-defined networks (SDN) , where devices are supported by intermediate edge servers that are in turn connected to the cloud.

To bridge this gap, researchers have proposed _hierarchical federated learning_ (HFL) which integrates _group aggregations_ into FL frameworks [30; 5; 47; 15]. In HFL (see Fig. 1), clients are segmented into multiple groups, and the training within each group is coordinated by a group aggregator node (e.g., an edge server coordinating a cell). Meanwhile, the central server orchestrates the training globally by periodically aggregating models across all client groups, facilitated by the group aggregators.

**Fundamental challenges.** One of the key objectives in FL is to reduce communication overhead while maintaining model performance. Research in conventional FL has established how the global aggregation period, i.e., the number of local iterations during two consecutive communications between clients and the server, impacts FL performance according to the degree of non-i.i.d. (non-independent or non-identically distributed) across client datasets: when local datasets are more heterogeneous, longer aggregation periods cause client models to drift further apart. In HFL, the situation becomes more complex, and is not yet well studied. There are multiple levels of aggregations within/across client groups, and the frequency of these aggregations diminishes further up the hierarchy (since the communication costs become progressively more expensive). As a result, _model drift occurs across multiple levels of non-i.i.d., at different timescales_. In the canonical two-level case from Fig. 1, we have (i) intra-group non-i.i.d., similar to conventional FL, and (ii) inter-group non-i.i.d., arising from data heterogeneity across different groups. This introduces (i) _client model drift_ caused by local updates on individual datasets, usually at a shorter timescale, as well as (ii) _group model drift_ caused by FL over clients within the group, usually at a longer timescale.

In conventional star-topology FL, algorithms like ProxSkip , SCAFFOLD , and FedDyn  have shown promise for correcting client model drift through local regularization and gradient tracking/correction. However, _these approaches are not easily extendable to the HFL scenario due to its multi-timescale communication architecture_. Specifically, when integrating these methods into HFL, control variables introduced to handle data heterogeneity, such as gradient tracking or dynamic regularization, need to be carefully injected at each level of the hierarchy, taking into account their coupled effects in taming non-i.i.d. Convergence analysis elucidating the impact of different updating frequencies for such control variables remains an unsolved challenge. Existing works on HFL have also not aimed to directly correct for multi-timescale model drift. This can be seen by the fact that the convergence bounds in existing HFL methods  become worse as the extent of non-i.i.d. in the system increases (e.g., gradient divergence between hierarchy levels in ). Some works have proposed adaptive control of the aggregation period in HFL , but they require frequent model aggregations to prevent excessive drift. We thus pose the following question:

_How can we tame multi-timescale model drift in non-i.i.d. hierarchical federated learning to provably enhance model convergence performance while not introducing frequent model aggregations?_

### Contributions

In this paper, we propose _multi-timescale gradient correction_ (MTGC), a methodology which can effectively address multi-level model drift over the topology of HFL with a theoretical guarantee. As depicted in Fig. 1, our key idea is to introduce coupled gradient correction terms - client-group correction and group-global correction - to (i) correct the client gradient towards the group gradient, i.e., to reduce client model drift caused by local updates based on their individual datasets, and (ii) correct the group gradient towards the global gradient, i.e., to reduce group model drift caused by FL across clients within the group, respectively. MTGC thus assists each client model to evolve towards improvements in global performance during HFL. We propose a strategy for updating these gradient correction terms after every group aggregation and global aggregation, respectively, and analyze the convergence behavior of MTGC. Due to the coupling of correction terms and their updates being performed at different timescales, additional challenges arise for theoretical analysis compared to prior work. We thoroughly investigate this problem and make the following contributions:

* We develop the multi-timescale gradient correction (MTGC) algorithm for taming leveled model drift in HFL. MTGC incorporates coupled control variables for correcting client gradients and group gradients, effectively tackling model biases arising from various levels of non-i.i.d. data at different timescales. The estimation and update procedures for these control variables rely solely on the model updates, ensuring that no significant additional communication overhead is introduced.
* We characterize the convergence rate for MTGC under the non-convex setup. This rate is immune to the extent of intra and inter-group data heterogeneity, confirming the stability of our approach against multi-level non-i.i.d. statistics. Our theoretical result also demonstrates that MTGC achieves

Figure 1: Illustration of multi-timescale gradient correction (MTGC) for multi-level non-i.i.d. in HFL.

linear speedup in the number of local iterations, group aggregations, and clients. Also, we show that the convergence rate of MTGC recovers that of SCAFFOLD, i.e., the non-hierarchical case, when the number of groups and group aggregation period reduces to one.
* We conduct extensive experiments using various datasets and models across different parameter settings, which demonstrate the superiority of MTGC in diverse non-i.i.d. HFL environments.

### Related Works

**Algorithms for conventional FL.** The seminal work  developed the FedAvg algorithm, incorporating multiple local updates into distributed SGD  to relieve communication bottlenecks within conventional star-topology FL. However, FedAvg convergence analysis makes assumptions such as bounded gradients [28; 56; 41] or bounded gradient dissimilarity [45; 12], showing it is not resistant to non-i.i.d. data. To tackle this issue, numerous techniques have been proposed in the literature, including incorporating static/dynamic regularizers [27; 1; 57; 16], adaptive control variables [29; 8; 51; 50], and/or gradient tracking methods [18; 32]. Despite these efforts, existing FL algorithms are not easily extendable to HFL due to the timescale mismatch of multi-level model aggregations induced by the hierarchical system topology. Optimizing these algorithms and ensuring their theoretical convergence in the presence of hierarchical model drift remains unsolved. Our paper addresses these issues through a principled multi-timescale gradient correction method.

**Hierarchical FL.** The authors of [5; 30; 13; 49; 55; 47] explored a new FL branch, HFL, tailored for hierarchical systems consisting of a central server, group aggregators, and clients. To tackle the issue of limited communication resources, the authors of  developed a FedAvg-like algorithm called hierarchical FedAvg tailored to HFL, and analyzed its convergence behavior. However, their algorithm is built upon an assumption of i.i.d. data. Another work  investigated the convergence behavior of hierarchical FedAvg under the non-i.i.d. setup. However, the convergence bound becomes worse as the extent of data heterogeneity increases, making the algorithm vulnerable to non-i.i.d. data characteristics. In , ProxSkip-HUB is introduced, but requires clients to compute full batch gradients and upload them to group aggregators after every iteration, which is impractical especially when training large-scale models. Overall, there is still a lack of an algorithm that fully addresses the unique challenge of HFL, i.e., the multi-timescale model drift problem, with theoretical guarantees. We fill this gap by introducing multi-timescale gradient correction and providing theoretical insights.

**Gradient tracking/correction.** Both gradient tracking and gradient correction aim to fix the local updating directions of clients to mitigate the impact of model drift caused by data heterogeneity. The gradient tracking concept was originally proposed and analyzed in  and then extended to consider various factors like time-varying graphs and asynchronous updates [37; 40; 42; 54]. Subsequently, SCAFFOLD  applied gradient tracking in FL to mitigate the impact of data heterogeneity across clients, ensuring convergence and stability in non-i.i.d. settings. More recently, in [32; 2], the authors demonstrate the effectiveness of gradient tracking in fully decentralized FL, where clients conduct model aggregations through local client-to-client communications. In [6; 43], gradient tracking is further studied in a semi-decentralized FL setup. Compared to all prior research, our work is the earliest attempt to design an algorithm specifically tailored to multi-timescale model drift in HFL and its training process with periodic local/global aggregations. This presents new challenges in our algorithm design and convergence analysis due to the coupling of our correction terms through their updates at different timescales. In Section 5, we empirically validate the effectiveness of our approach over the prior gradient correction method.

## 2 Background and Motivation

### Problem Setup: Hierarchical FL

We consider the hierarchical system depicted in Fig. 1. The central server is connected to \(N\) group aggregators, each linked to the clients within its region, defined as a group. Each group \(j\{1,2,,N\}\) consists of a set of \(n_{j}\) non-overlapping clients, denoted \(_{j}\), resulting in a total of \(_{j=1}^{N}n_{j}\) clients within the system. Each client \(i\) has its own local data distribution \(_{i}\). The goal of HFL is to construct an optimal global model \(^{*}\) considering the data distributions of all clients in the system. The role of each group aggregator \(j\) involves coordinating the training for the \(n_{j}\) clients within its region, while the central server orchestrates the training across all \(N\) groups through interaction with the group aggregators. We can formally state the HFL learning objective as follows:

\[_{}f():=_{j=1}^{N}f_{j}(),f_{j}():=}_{i_{j}}F_{i}()F_{i}():=_{_{i}_{i}}[F_{i}(,_{i })].\] (1)

Here, \(f:^{d}\) denotes the global loss function, \(f_{j}:^{d}\) is the loss specific to group \(j\), and \(F_{i}:^{d}\) represents the local loss for client \(i\). In addition, \(_{i}\) is the data point sampled from distribution \(_{i}\). Note that our analysis can be easily extended to a weighted average form of (1) by incorporating positive coefficients for each \(f_{j}()\) or \(F_{i}()\). For simplicity, these coefficients are assumed to be included in \(F_{i}()\) as in previous works [18; 48].

### Limitation of Existing Works

In HFL algorithms, group aggregations are conducted after every \(H\) local client updates, while global aggregations are performed after every \(E\) group aggregations, introducing different timescales. Moreover, different forms of data heterogeneity exist in HFL: (i) intra-group non-i.i.d., due to data heterogeneity across different clients \(i_{j}\), and (ii) inter-group non-i.i.d., arising from data heterogeneity across different groups \(_{1},...,_{N}\). These lead to _client model drift_ and _group model drift_, respectively. The model drifts induced by multi-level data heterogeneity at different timescales hinder hierarchical FedAvg from converging. In Fig. 2(a), we see that during local training, each local model gradually converges towards the optimal point of its respective client's objective function. Hence, to guarantee theoretical convergence, existing HFL works either assume an i.i.d. setup  or rely on a bounded gradient dissimilarity assumption similar to the following [47; 15]:

\[_{j=1}^{N}\| f_{j}()- f()\|^{2} _{1}^{2},\ }_{i_{j}}\| F_{i}()- f_{j }()\|^{2}_{2}^{2},, j.\] (2)

The first inequality is employed to limit the group drift, i.e., the deviation of group gradients from the global gradient, while the second one bounds the client drift, i.e., the divergence of client gradients from their group gradient. As a result, the convergence bounds of algorithms in these works become worse as data heterogeneity increases (i.e., as \(_{1}\) or \(_{2}\) increase) . Our approach, developed next, does not require these assumptions and remains stable regardless of the extent of data heterogeneity.

## 3 Algorithm

### Intuition: Gradient Correction in Hierarchical FL

When relying on multiple local SGD iterations as in hierarchical FedAvg, the model update process is not stable even when the model has reached the optimal \(^{*}\) satisfying \( f(^{*})=\). Specifically, with \(\) as the learning rate, we have \(^{*}^{*}- F_{i}(^{*})\), as the global optimum \(^{*}\) may not necessarily be optimal for each client's local loss due to data heterogeneity, i.e., \( F_{i}(^{*})\). Correcting the client gradient \( F_{i}(^{*})\) to the global gradient \( f(^{*})\) is thus necessary to stabilize the process.1

**Motivation and idea.** In HFL, however, due to multi-level aggregations occurring at different timescales, it is infeasible to directly correct the client gradient to the global gradient. In particular,

Figure 2: Visualization of the local update process using multi-timescale gradient correction (MTGC) with \(4\) clients and \(2\) groups. **(a)** Without any gradient correction (e.g., hierarchical FedAvg), each client model moves towards its respective optimal point, denoted by \(_{i}^{*}\). **(b)** When only client-group correction term \(_{i}\) is applied, the model of client \(i_{j}\) moves towards the group optimum \(}_{j}^{*}\). **(c)** In MTGC, the gradient of client \(i_{j}\) is adjusted by both the client-group correction term \(_{i}\) and the group-global correction variable \(_{j}\), assisting each client model to converge towards the global optimum \(^{*}\) during local iterations.

clients are not able to communicate with the central server directly, and there are multiple group aggregation steps before the group aggregative communicate with the main server. Our idea is thus to inject two gradient correction terms: client-group correction and group-global correction. Specifically, the desired iteration to obtain the updated model \(_{}\) at the optimal point \(^{*}\) can be written as

\[_{}=^{*}- F_{i}(^{*})+ {( f_{j}(^{*})- F_{i}(^{*}))}_{}+^{*})- f_{j}(^{*}))}_{}},\] (3)

where \( f_{j}(^{*})- F_{i}(^{*})\) and \( f(^{*})- f_{j}(^{*})\) represent client-group and group-global correction terms, respectively. Since \( f(^{*})=\), the two correction terms will enable the model to remain at the optimal point. Given this intuition, the ideal local iteration at client \(i_{j}\) can be written as

\[^{t,e}_{i,h+1}=^{t,e}_{i,h}- F_{i}(^{t, e}_{i,h})+ f_{j}(}^{t,e}_{j,h})- F_{i}(^{t, e}_{i,h})+ f(}^{t,e}_{h})- f_{j}(}^{t, e}_{j,h})},\] (4)

where \(t\), \(e\), and \(h\) represent global communication rounds, group communication rounds, and client local iterations, respectively, \(}^{t,e}_{j,h}=}_{i_{j}}^{t,e}_ {i,h}\) is the averaged model within group \(j\), and \(}^{t,e}_{h}=_{j=1}^{N}}_{i _{j}}^{t,e}_{i,h}\) is the averaged model across the system. Based on (4), we expect to bring each client model closer to the global optima during local updates, as illustrated in Fig. 2(c).

``` Input: Initial model \(}^{0}\), global aggregation period \(E\), group aggregation period \(H\), learning rate \(\), and group-global correction \(^{0}_{j}=-}_{i_{j}} F_{i}(^{ t,0}_{i,0},^{0,0}_{i,0})+_{j=1}^{N}}_{i _{j}} F_{i}(^{0,0}_{i,0},^{0,0}_{i,0}), j\)
1each global round \(t=0,1,,T-1\)do
2 Group model initialization: \(^{t,0}_{j}=}^{t}, j\)
3 Client-group correction initialization: \(^{t,0}_{i}=- F_{i}(^{t,0}_{i,0},^{t,0}_{i,0})+}_{i_{j}} F_{i}(^{t,0}_{i,0},^{t,0}_{i,0}),  i_{j}, j\)
4each group communication round \(e=0,1,,E-1\)do
5 Local model initialization: \(^{t,e}_{i,0}=}^{t,e}_{i,h}, i,j\)
6each local iteration \(h=0,1,,H-1\)do
7\(^{t,e}_{i,h+1}=^{t,e}_{i,h}-( F_{i}(^{t,e}_ {i,h},^{t,e}_{i,h})^{t,e}_{i}+^{t}_{j}), i _{j}, j\)\(\)Clients do in parallel
8 Group aggregation: \(}^{t,e+1}_{j}=}_{i_{j}}^{t,e}_ {i,H}\)
9 Client-group corr. update: \(^{t,e+1}_{i}^{t,e}_{i}+(^{t,e}_{i,H}-}^{t,e+1}_{j}), i_{j}, j\)\(\)Clients do in parallel
10 Global aggregation: \(}^{t+1}=_{j=1}^{N}}^{t,E}_{j}\)
11 Group-global corr. update: \(^{t+1}_{j}=^{t}_{j}+}}^{t,E }_{j}-}^{t+1}, j\)\(\)Group aggregators in parallel
12 ```

**Algorithm 1**HFL with Multi-Timescale Gradient Correction (MTGC)

**Challenge encountered in HFL.** However, it is important to note that the update process in (4) still cannot be directly used in HFL. This is because client-group communication and group-global communication do not occur at every iteration of HFL training; instead, they happen at different timescales, and clients are not able to obtain the current group information \( f_{j}(}^{t,e}_{i,h})\) and global information \( f(}^{t,e}_{i,h})\) at every local iteration. We next propose a strategy that mimics the gradient correction described above while ensuring theoretical convergence.

### Multi-Timescale Gradient Correction (MTGC)

**Tackling multi-timescale model drifts.** To approximate (4) during HFL training, we introduce two control variables \(\) and \(\) that track/approximate \( f_{j}- F_{i}\) and \( f- f_{j}\), respectively. The variables \(\) and \(\) are then employed to correct the local gradients to prevent model drifts. The challenge here is to keep updating \(\) and \(\) appropriately in the multi-timescale communication scenario, given that communications between the clients and group aggregator, and between the group aggregators and global aggregator, are not always feasible. We propose a strategy to update \(\) after every \(H\) local iterations, i.e., whenever each client is able to communicate with the group aggregator, allowing the group information to be updated and shared among the clients within the same group. Similarly, we propose a strategy to update \(\) after every \(E\) group aggregations, i.e.,whenever the group aggregators are able to communicate with the global aggregator, enabling the global information to be refreshed and shared across all clients in the system. We name our strategy multi-timescale gradient correction (MTGC) due to the updates of \(\) and \(\) occurring in different timescales, to tackle the issue of multi-level model drift coupled across the hierarchy in HFL.

In Fig. 2(c), we illustrate MTGC during client-side model updates. In particular, at each local iteration \(h\) of group round \(e\) of global round \(t\), each client \(i_{j}\) updates its local model as follows:

\[_{i,h+1}^{t,e}=_{i,h}^{t,e}-( F_{i}(_{i,h}^ {t,e},\,_{i,h}^{t,e})+_{i}^{t,e}+_{j}^{t}).\] (5)

**(i) Client-group correction term.** In (5), \(_{i}^{t,e}\) is responsible for correcting the gradient of client \(i_{j}\) towards the gradient of group \(j\) at the \(e\)-th group aggregation of global round \(t\). After every group aggregation \(e\) at global round \(t\), this term is updated at each client \(i\) as follows:

\[_{i}^{t,e1}=_{h=0}^{H-1}((}_{i_{j}} F_{i}(_{i,h}^{t,e},\,_{i,h}^{t,e}) )- F_{i}(_{i,h}^{t,e},\,_{i,h}^{t,e})).\] (6)

**(ii) Group-global correction term.**\(_{j}^{t}\) in (5) aims to correct the gradient of group \(j\) towards the global gradient. At the end of global round \(t\), this term is updated at group aggregator \(j\) as follows:

\[_{j}^{t1}=_{e=0}^{E-1}_{h=0}^{H-1}( (_{j=1}^{N}}_{i_{j}} F _{i}(_{i,h}^{t,e},\,_{i,h}^{t,e}))-}_{i _{j}} F_{i}(_{i,h}^{t,e},\,_{i,h}^{t,e})).\] (7)

**Key remarks.** The updating policies for \(_{i}^{t,e}\) and \(_{j}^{t}\) follow similar patterns to the ideal corrections outlined in (4). Here, we observe that \(_{i_{j}}_{i}^{t,e}=\), \( j\) and \(_{j=1}^{N}_{j}^{t}=\), indicating that the correction terms do not have an impact on the per-iteration model averages. Instead, the introduction of \(_{i}^{t,e}\) and \(_{j}^{t}\) eliminates model drifts of clients and groups, respectively, during local iterations. Intuitively, as the iteration approaches the global optimal point, we expect \(_{i}^{t,e} f_{j}(^{*})- F_{i}(^{*})\) and \(_{j}^{t} f(^{*})- f_{j}(^{*})\) so that the update in (5) stabilizes at the global optimal point. We also see that \(_{i}^{t,e}\) and \(_{j}^{t}\) are coupled (5), i.e., the update of one of the terms affects \(\) which in turn affects the other one, raising challenges for theoretical analysis. In Section 4, we will guarantee convergence of MTGC in general non-convex settings without relying on bounded data heterogeneity assumptions.

**MTGC algorithm.** The overall procedure of our training strategy is summarized in Algorithm 1, where we rewrite the updates of \(_{i}^{t,e}\) and \(_{j}^{t}\) in a different but equivalent manner to facilitate practical implementation of MTGC. Compared to hierarchical FedAvg, which does not consider any correction terms, we see that no additional communication is required for MTGC within each group round \(e\). Additional communication is introduced only after \(E\) group aggregations for initializing \(_{i}^{t,0}\) (Line 4)2 and broadcasting \(_{j}^{t1}\) (obtained in Line 14) to the clients in \(_{j}\). We will see in Section 5 that these marginal additional costs lead to significant performance enhancements for HFL settings.

**Generalization to arbitrary number of levels.** The proposed MTGC algorithm can be extended to an HFL system architecture with an arbitrary number of levels. Further discussions and experimental results for a three-level case are provided in Appendix E.

### Connection with SCAFFOLD

When the number of groups reduces to \(N=1\) with \(E=1\), we have \(_{j}^{t}=0\) (no group-global correction), and thus MTGC reduces to SCAFFOLD . In SCAFFOLD, at each round \(t\), clients perform local updates according to \(_{i,h1}^{t}=_{i,h}^{t}-( F_{i}(_{i,h }^{t},_{i,h}^{t})-_{i}^{t}+^{t}),h=0,1,,H-1\), where \(_{i}^{t1}=_{i}^{t}-^{t}+(}^{t}-_{i,H}^{t})\), and the server aggregates local models and controlling variables as \(}^{t1}=_{i=1}^{N}_{i,H}^{t}\) and \(^{t1}=_{i=1}^{N}_{i}^{t1}\). We can show that \(_{i}^{t}-^{t}\) in SCAFFOLD plays the same role as \(_{i}^{t,e}\) in MTGC. However, the additional term \(_{j}^{t}\) introduced in MTGC for the multi-level setting makes the convergence guarantee more challenging, as \(_{j}^{t}\) is coupled with \(_{i}^{t,e}\) and both are updated at different time scales. These aspects will be thoroughly examined next.

## 4 Convergence Analysis

In this section, we establish a convergence guarantee for the proposed MTGC algorithm. Our theoretical analysis relies on the following standard assumptions commonly used in the literature on stochastic optimization and FL under non-convex settings [18; 44; 4].

**Assumption 1**.: _Each local loss function \(F_{i}\) is differentiable and \(L\)-smooth, i.e., there exists a positive constant \(L\) such that for any \(\) and \(\), \(\| F_{i}()- F_{i}()\| L\|-\|, i\)._

**Assumption 2**.: _The stochastic gradient \( F_{i}(,_{i})\) is an unbiased estimate of the true gradient, i.e., \(_{_{i}_{i}}[ F_{i}(,_{i})]= F _{i}(),\) and the variance of the stochastic gradient \( F_{i}(,_{i})\) is uniformly bounded as \(_{_{i}_{i}}\| F_{i}(,_{i})- F _{i}()\|^{2}^{2},\)._

Note that (i) global aggregation, (ii) the update of upper-level correction variable \(\) and local aggregation, and (iii) the update of lower-level correction variable \(\) are performed at different timescales in MTGC. If we directly consider \(\{ f(}^{t})\}\) as in SCAFFOLD, it is difficult to capture the effects of group aggregation and correction variable \(\). Moreover, it is hard to establish a tight connection between \( f(}^{t})\) and \(^{t,e}_{i,h}, i,h,\) since there is a large lag between \(^{t,e}_{i,h}\) and \(}^{t}\). To tackle this, we introduce a new metric, which is the gradient \( f(}^{t,e})\) at virtual sequence \(\{}^{t,e}=_{j=1}^{N}}^{t,e}_{j}\}\), to characterize the convergence of MTGC.

We next state our main theoretical results. All the proofs are provided in Appendix F:

**Theorem 4.1**.: _Suppose Assumptions 1 and 2 hold and the learning rate satisfies \(\). Then the iterates \(\{}^{t,e}\}\) obtained by the MTGC algorithm satisfy_

\[_{t=0}^{T-1}_{e=0}^{E-1}\|  f(}^{t,e})\|^{2}= (}^{0})-f^{*}}{ TEH}+ }L^{2}+^{2}E^{2}H^{2}L^{2}^{2} ),\] (8)

_where \(=(}_{j=1}^{N}})^{-1}\), and \(f^{*}\) is the lower bound of \(f()\), i.e., \(f() f^{*}\)._

There are two key steps in our proof. The first is the characterization of the evolution of \(\|^{t,e}_{i}+ F_{i}(}^{t,e}_{j})- f _{j}(}^{t,e}_{j})\|^{2}\) and \(\|^{t}_{j}+ f_{j}(}^{t,e})- f (}^{t,e})\|^{2}\). By bounding these values that capture the error between each control variable and the ideal correction, we are able to establish a connection between the local updating direction and the global gradient without relying on the bounded gradient dissimilarity assumption, laying the foundation for the whole proof. The second is that we extracted a recursive relationship for the accumulation of group-level and client-level model drifts, and designed a novel Lyapunov function to mitigate the interplay impact between these drifts. Further details are provided in the appendix.

Applying an appropriate learning rate \(\) to Algorithm 1 yields the following corollary:

**Corollary 4.1**.: _Under the assumptions of Theorem 4.1, let \(_{0}=f(}^{0})-f^{*}\). Then there exists a learning rate \(\) such that the iterates \(\{}^{t,e}\}\) satisfy_

\[_{t=0}^{T-1}_{e=0}^{E-1}\| f(}^{t,e})\|^{2}(_{0}L ^{2}}{TEH}}+(_{0}L}{T})^{ }+_{0}}{T}).\] (9)

**Discussions.** Corollary 4.1 provides the convergence upper bound of the MTGC algorithm. It shows that the error approaches zero as \(T\). If \( 0\), the upper bound is dominated by the first term in the right-hand side of (9), which characterizes the speed of convergence of MTGC to a stationary point in the stochastic case. This reveals MTGC achieves linear speedup in the number of group aggregations \(E\) and local updates \(H\). In other words, we can attain the same level of performance with less global communication rounds, i.e., a smaller value of \(T\), by increasing the number of local iterations, i.e., \(H\), and group aggregations, i.e., \(E\). When considering the special case \(n_{j^{}}=n,\; j^{}\{1,2,,N\}\) with uniform client numbers, the rate becomes \((_{0}L^{2}}/)\). This implies that MTGC attains linear speedup in the number of clients as well.

Moreover, we also see that our convergence rate recovers the results of SCAFFOLD when the number of groups reduces to \(N=1\) and the number of group aggregations reduces to \(E=1\) (see Appendix G for more discussions). We also highlight that, different from prior works on HFL where the convergence bound becomes worse as the extent of data heterogeneity increases, our bound is stable against multi-level non-i.i.d. data due to the multi-timescale gradient correction approach.

## 5 Experimental Results

### Setup

**Dataset, model, hyperparameters, and compute setting.** In our experiments, we consider four widely used datasets: EMNIST-Letters (EMNIST-L) , Fashion-MNIST , CIFAR-10 , andCIFAR-100 . The former two are processed through a multi-layer perceptron (MLP) model, featuring two hidden layers, each comprising \(200\) neurons, and ending with a softmax layer. For the CIFAR-10 classification task, we employ a convolutional neural network (CNN) following the architecture outlined in seminal work . For CIFAR-100, we adopt a ResNet-18 model with batch normalization layers substituted by group normalization layers. Across all algorithms considered, we maintain a consistent learning rate \(=0.1\) and batch size \(50\). We conduct the experiments based on a cluster of \(3\) NVIDIA A100 GPUs with \(40\) GB memory. Our code is based on the framework of .

**FL data distribution.** We set the total number of clients as \(100\), evenly distributed over \(N=10\) groups. We also study the effect of \(N\) in Appendix B. We consider three different data distribution settings: (i) group i.i.d. & client non-i.i.d., (ii) group non-i.i.d. & client i.i.d., and (iii) group non-i.i.d. scenarios. First, in the group i.i.d. & client non-i.i.d. case, the training dataset is initially divided uniformly and randomly into \(N\) segments corresponding to \(N\) groups. Subsequently, each segment is further divided into \(100/N\) partitions for the clients using a Dirichlet distribution . Second, in the group non-i.i.d. & client i.i.d. case, the dataset is first segmented into \(N\) partitions for the groups using a Dirichlet distribution, followed by a uniform random distribution of each segment to \(100/N\) clients. Finally, when both groups and clients are non-i.i.d., the dataset is split into \(N\) segments for the groups using a Dirichlet distribution, and then, each group's segment is distributed among \(100/N\) clients through a Dirichlet distribution. The Dirichlet parameter is set to \(0.1\).

Figure 4: **Comparison with gradient correction baselines. Three different data distribution scenarios are considered. We see that the local correction method is effective for handling client non-i.i.d. within each group (top row), while the group correction method is effective for handling non-i.i.d. across groups (middle row). MTGC obtains the most stable performance (all rows) by combining multiple correction levels.**

Figure 3: **Comparison with FL baselines. In this experiment, popular FL algorithms are extended to the HFL setup for comparison with MTGC. We consider four datasets in the group non-i.i.d. & client non-i.i.d. setting. Experiments are conducted over \(3\) random trials. We see that MTGC obtains the best testing accuracy in each case, validating our multi-level approach for correcting multi-timescale model drifts.**

### Results and Discussion

**Comparison with conventional FL algorithms.** For comparison, we first apply the well-known FL methods, FedProx , SCAFFOLD , and FedDyn , to HFL, by running their training algorithms within each group of the hierarchical system. We also consider HFedAvg  as a baseline. Fig. 3 compares MTGC with these baselines in the group non-i.i.d. & client non-i.i.d. case. We observe that MTGC outperforms all the considered conventional algorithms, achieving the highest testing accuracy, especially for the complicated CIFAR-100 dataset. FedDyn achieves the lowest performance, demonstrating significant variance and instability. The significant performance gap between MTGC and FedDyn, in particular, can be attributed to the hierarchical setup disrupting the special structure of FedDyn. This result reveals that some algorithms designed for the conventional star-topology FL may be non-trivial to be extended to hierarchical setups. The overall results confirm the effectiveness of our approach that effectively tackles the multi-timescale drift problem in HFL.

**Comparison with gradient correction baselines.** In Fig. 4, we compare MTGC with the gradient correction baselines. Specifically, we apply local correction (\(_{i}^{t,e}\)) to HFedAvg, and group correction (\(_{j}^{t}\)) to HFedAvg. These baselines can be viewed as schemes applying SCAFFOLD  within each group and across groups, respectively. We also report the results of the original HFedAvg to see the effects of gradient correction clearly. We make the following key observations. First, the testing accuracy achieved by HFedAvg decreases as the extent of data heterogeneity increases, e.g., from the first or second row to the third row in Fig. 4. This shows that data heterogeneity hinders the convergence of HFedAvg. Second, with the assistance of local or group correction, the algorithm attains a higher accuracy. In the case of group i.i.d. & client non-i.i.d., HFedAvg augmented with client local correction performs better than the variant with group correction. Conversely, in the scenario where groups are non-i.i.d. and clients are i.i.d., the opposite holds. This can be explained by the dominance of data heterogeneity in each case. In the former scenario, because the heterogeneity is primarily at the client-level, local client correction becomes more beneficial. On the other hand, in the latter scenario, where the heterogeneity shifts to the group level, group correction becomes more advantageous. Finally, we see that MTGC consistently outperforms baselines under all settings, where the performance gains brought by the multi-timescale gradient correction become more significant when it comes to the group non-i.i.d.& client non-i.i.d. case.

**Speedup in \(H\) and \(E\).** In Table 5.1, we investigate the effects \(H\) and \(E\), which determine the periods of group aggregation and global aggregation in HFL. We report the number of global rounds required to attain the desired testing accuracy of \(80\%\) for CIFAR-10 under different settings. We have the following observations: As \(E\) or \(H\) increases, the required number of global rounds of MTGC for achieving the desired accuracy decreases. This demonstrates the speedup of the proposed algorithm in the number of local iterations and group aggregations, which fits well with our theory discussed in Section 4. In addition, the speedup achieved by MTGC compared to HFedAvg gets more significant

   Data distribution & Params \((E,H)\) & HFedAvg & Local Correction & Group Correction & MTGC \\   & (\(10,20\)) & \(144.3\)\(\)\(3.4\) (\(\)) & \(57.0\)\(\)\(0.8\) (\(2.5\)) & \(72.0\)\(\)\(1.6\) (\(2.0\)) & \(  & (\(10,40\)) & \(214.0\)\(\)\(5.9\) (\(1\)) & \(45.3\)\(\)\(1.2\) (\(4.7\)) & \(85.7\)\(\)\(1.2\) (\(2.5\)) & \( client non-i.i.d. & & (\(10,20\)) & \(144.3\)\(\)\(3.4\) (\(1\)) & \(57.0\)\(\)\(0.8\) (\(2.5\)) & \(72.0\)\(\)\(1.6\) (\(2.0\)) & \( & (\(10,20\)) & \(246.0\)\(\)\(3.7\) (\(1\)) & \(92.3\)\(\)\(1.7\) (\(2.7\)) & \(53.7\)\(\)\(1.2\) (\(4.6\)) & \( client i.i.d. & & (\(10,20\)) & \(246.0\)\(\)\(3.7\) (\(1\)) & \(92.3\)\(\)\(1.7\) (\(2.7\)) & \(53.7\)\(\)\(1.2\) (\(4.6\)) & \( & (\(10,20\)) & \(363.0\)\(\)\(7.3\) (\(1\)) & \(141.7\)\(\)\(2.9\) (\(2.6\)) & \(83.7\)\(\)\(1.2\) (\(4.3\)) & \( client non-i.i.d. & & (\(10,20\)) & \(363.0\)\(\)\(7.3\) (\(1\)) & \(141.7\)\(\)\(2.9\) (\(2.6\)) & \({83.7\)\(\)\(1.2\) (\(4.3\)) & \(as \(E\) or \(H\) increases. For instance, in the group i.i.d. & client non-i.i.d. case, MTGC attains \(3.3\) speedup when \(E=10\,H=20\), which increases to \(4.7\) when \(E=10\,H=40\). This reveals that MTGC utilizes local iterations better compared with the baselines.

**Impact of data heterogeneity.** Consistent with the results in Fig. 4, we see from Table 5.1 that the required number of global rounds of HFedAvg increases as data heterogeneity increases, while MTGC is more stable against non-i.i.d. data. The gain of MTGC over HFedAvg becomes evident as data heterogeneity increases, confirming the effectiveness of our multi-timescale gradient correction approach for addressing the unique challenges of HFL.

**Further experiments.** Additional experimental results including the impacts of hierarchical system parameters and the performance in \(3\)-level HFL are provided in Appendices B and E.

## 6 Conclusion and Limitation

We have proposed MTGC, a multi-timescale gradient correction approach for HFL. Embedded with control variables updated in different timescales, MTGC effectively corrects gradient biases and alleviates both client model drift and group model drift in hierarchical setups. We established the convergence bound of MTGC in the non-convex setup and showed its stability against multi-level data heterogeneity. Finally, we confirmed the advantage of our MTGC through extensive experiments in different non-i.i.d. HFL settings. A limitation of our work is that despite providing experiments for HFL systems with more than two levels (in Appendix E), our convergence analysis focused on the two-level case, which provides an interesting future direction of investigation.