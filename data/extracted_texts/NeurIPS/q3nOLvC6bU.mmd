# On the Robustness of Neural Collapse and the Neural Collapse of Robustness

Jingtong Su

Center for Data Science

New York University

js12196@nyu.edu &Ya Shi Zhang

Department of Pure Mathematics

and Mathematical Statistics

University of Cambridge

ysz23@cam.ac.uk &Nikolaos Tsilivis

Center for Data Science

New York University

nt2231@nyu.edu &Julia Kempe

Center for Data Science and

Courant Institute of Mathematical Sciences

New York University

###### Abstract

Neural Collapse refers to the curious phenomenon in the end of training of a neural network, where feature vectors and classification weights converge to a very simple geometrical arrangement (a simplex). While it has been observed empirically in various cases and has been theoretically motivated, its connection with crucial properties of neural networks, like their generalization and robustness, remains unclear. In this work, we study the stability properties of these simplices. We find that the simplex structure disappears under small adversarial attacks, and that perturbed examples "leap" between simplex vertices. We further analyze the geometry of networks that are optimized to be robust against adversarial perturbations of the input, and find that Neural Collapse is a pervasive phenomenon in these cases as well, with clean and perturbed representations forming aligned simplices, and giving rise to a robust simple nearest-neighbor classifier. By studying the propagation of the amount of collapse inside the network, we identify novel properties of both robust and non-robust machine learning models, and show that earlier, unlike later layers maintain reliable simplices on perturbed data.

## 1 Introduction

Reinforcing arguments about the simplicity of neural networks found by stochastic gradient descent in classification settings, Papyan et al. (2020) made the surprising empirical observation that both the feature representations in the penultimate layer (grouped by their corresponding class) and the weights of the final layer form a _simplex equiangular tight frame_ (ETF) with \(C\) vertices, where \(C\) is the number of classes. Curiously, such a geometric arrangement becomes more pronounced well-beyond the point of (effectively) zero loss on the training data, motivating the common tendency of practitioners to optimize a network for as long as the computational budget allows. The collection of these empirical phenomena was termed _Neural Collapse_.

While the results of (Papyan et al., 2020) fueled much research in the field, many questions remain regarding the connection of Neural Collapse with properties like generalization and robustness of Neural Networks. In particular with regards to _adversarial robustness_, the ability of a model to withstand adversarial modifications of the input without effective drops in performance, it has been originally claimed that the instantiation of Neural Collapse has positive effect on defending against adversarial attacks (Papyan et al., 2020; Han et al., 2022). However, this seems to at least superficiallycontradict the fact that neural networks are not a priori adversarially robust (Szegedy et al., 2014; Carlini and Wagner, 2017).

In this paper, we thoroughly study the stability properties of the simplices under adversarial attacks and then investigate whether Neural Collapse happens and whether it is necessary for adversarially robust models. In particular, our contributions and findings, partially illustrated in Figure 1, are:

* **Is NC robust?** We initiate the study of the neural collapse phenomenon in the context of adversarial robustness, both for standardly trained networks under adversarial attacks and for adversarially trained robust networks to investigate the stability and prevalence of the NC phenomenon. Our work exposes considerable additional fundamental, and we think, surprising, geometrical structure:
* **No!** For standardly trained networks we find that small, imperceptible adversarial perturbations of the training data remove any simplicial structure at the representation layer: neither variance collapse nor simplex representations appear under standard metrics. Further analysis through class-targeted attacks that preserve class-balance shows a "cluster-leaping" phenomenon: representations of adversarially perturbed data jump to the (angular) vicinity of the original class means.
* **Yes for AT networks! Two identical simplices emerge.** Adversarially trained, robust, networks exhibit a simplex structure both on original clean and adversarially perturbed data, albeit of higher variance. These two simplices turn out to be the same. We find that the simple nearest-neighbor classifiers extracted from such models also exhibit robustness.
* **Early layers are more robust.** Analyzing NC metrics in the representations of the inner layers, we observe that initial layers exhibit a higher degree of collapse on adversarial data. The resulting simplices, when used for Nearest Neighbour clustering, give surprisingly robust classifiers. This phenomenon disappears in later layers.

## 2 Background

Papyan et al. (2020) demonstrated the prevalence of NC on networks optimized by SGD, by tracing the following quantities (please refer to Appendix B.2 for exact definitions):

**(NC1) Variability collapse:** For all classes, the within-class variation of the last layer representations collapses to zero.

**(NC2, Equiangular):** Class-Means converge to equal, maximal pairwise angles.

**(NC2, Equinorm):** Class-Means converge to equal length.

**(NC3) Convergence to self-duality:** The linear classifier and the class-means converge to each other (after rescaling).

**(NC4) Simplification to Nearest Class Center (NCC) classifier:** The prediction of the network is equivalent to that of the NCC classifier formed by the (non-centered) class-means.

Figure 1: Visualisation of our findings. Sticks represent class-means. Small dots correspond to the representation of an individual datum, and the color represents the ground-truth label. **Left to Right:** clean representations with standardly-trained (ST) networks; perturbed representations with ST networks; clean representations with adversarially-trained (AT) networks; perturbed representations with AT networks. With ST nets, the adversarial perturbations push the representation to “leap” towards another cluster with slight angular deviation. AT makes the simplex resilient to such adversarial attacks, with higher and intra-class variance.

We adopt the natural extensions of the above metrics for adversarially trained models, and further study some new quantities, relevant to our analysis, which are defined and explained in Appendix B.2.

## 3 Experiments

In this section, we present our main experimental results measuring neural collapse in standardly (ST) (with SGD) and adversarially trained (AT) models (Madry et al., 2018). We consider image classification tasks on CIFAR-10 and CIFAR-100 and we train two large convolutional networks, a standard VGG and a Pre-Activation ResNet18, from random initializations. We launch 3 independent runs and report the mean and standard deviation throughout our paper. Further results for varying choices of hyperparameters can be found in the Appendix.

**Remark**: When collecting feature representations for adversarially perturbed data, we always compute the _current epoch's_ perturbations.

### Standardly trained neural nets

The first and third column of Figure 2 show the evolution of the NC quantities as described in Section 2 for standardly trained models. We use both adversarially perturbed and Gaussian reference data to study the stability of the original simplices. As expected, NC metrics converge on the clean training

Figure 2: Accuracy, Loss, and NC evolution for standardly (ST) and adversarially (AT) trained VGG and ResNet. For AT models, clean and Guassian curves coincide. Setting: CIFAR-10, \(_{}\) adversary.

data. Neural Collapse is slightly attenuated on Gaussian reference data, but disappears strikingly for adversarially perturbed data, suggesting that the simplex formed by clean training data is robust against random perturbations, but fragile to adversarial attacks. The results certainly corroborate the conclusion that the representation class-means of perturbed points with ground-truth label \(c\) do not form any geometrically-meaningful structure at all.

Figure 3 (left) shows Simplex Similarity and non-centered Angular Distance of the simplices formed by targeted adversarial examples and by clean examples as described in Appendix B.2. These results give us a full glimpse of how standardly trained networks are non-robust and fail under adversarial attacks: adversarial perturbations break the simplex ETF by "leaping" the representation from one class-mean to another, forming a norm-imbalanced less concentrated structure around the original simplex.

### Neural Collapse during Adversarial Training

We train neural nets adversarially to full convergence with perfect clean and robust training accuracy and measure NC metrics for clean and perturbed (epoch-wise) training data in Figure 2 (columns 2 and 4). Interestingly, we find that Neural Collapse _qualitatively_ occurs in this setting as well, both for clean and perturbed data, and two simplices emerge. Notice, however, that the extent of variability collapse (NC1) on the perturbed points is smaller than on the "clean" data or the Gaussian noise benchmark, indicating that clean examples are more concentrated around the vertices. To understand the relative positioning of the two simplices, we investigate the Simplex Similarity and Angular Distance between non-centered class-means in Figure 3 (right). The vanishing distance suggests these two simplices are exactly the same. These results suggest that Adversarial Training nudges the network to learn simple representational structures (namely, a simplex ETF) not only on clean examples but also on perturbed examples to achieve robustness against adversarial perturbations. Equivalently, the simplices induced by robust networks are _not fragile_ anymore, but _resilient_. Note also that NC4 results imply that there is a simple nearest-neighbor classifier that is robust against adversarial perturbations generated from the network.

Curiously, this is not the case for all training algorithms that produce robust models. In particular, a state-of-the-art algorithm that aims to balance clean and robust accuracy, TRADES (Zhang et al., 2019), shows fundamentally different behavior (see Figure 4 in the Appendix). Even though both terms of the loss (see Equation 4) are driven to zero, we do not observe Neural Collapse; the amount of collapse is roughly one order of magnitude larger than for vanilla AT, and the feature representations do not approach the ETF formation, even well past the onset of the terminal phase. _We view this as evidence that the prevalence of Neural Collapse is not necessary for robust classification._

### Layerwise Analysis

Furthermore, the Appendix contains our detailed layerwise analysis on both ST and AT models. We observe that initial layers exhibit a higher degree of collapse on adversarial data, while NCC classifiers defined on intermediate layers show surprising robustness, even if the whole model fails to do so.

Figure 3: Angular distance. _Left and Inner Left:_ Average between targeted attack class-means and clean class-means on **ST** network. _Inner Right and Right:_ Average between perturbed class-means and clean class-means on **AT** network. Setting: CIFAR-10, \(_{}\) adversary.

Conclusion

Neural Collapse is an interesting phenomenon displayed by Neural Networks used in classification tasks. We empirically studied and quantified the sensitivity of this geometric arrangement to input perturbations, and, further, displayed that Neural Collapse can appear (but not always does!) in Neural Networks trained to be robust. We conclude that Neural Collapse is prevalent in many deep learning settings, including adversarially trained networks, though it does not seem to be necessary for robustness.