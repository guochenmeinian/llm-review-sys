# The Unmet Promise of Synthetic Training Images:

Using Retrieved Real Images Performs Better

Scott Geng\({}^{}\) Cheng-Yu Hsieh\({}^{}\) Vivek Ramanujan\({}^{}\) Matthew Wallingford\({}^{}\)

Chun-Liang Li\({}^{}\) Pang Wei Koh\({}^{}\)1 Ranjay Krishna\({}^{}\)\({}^{}\)

Equal advising. \({}^{}\)University of Washington Allen Institute for AI

sgeng@cs.washington.edu

###### Abstract

Generative text-to-image models enable us to synthesize unlimited amounts of images in a controllable manner, spurring many recent efforts to train vision models with synthetic data. However, every synthetic image ultimately originates from the upstream data used to train the generator. Does the intermediate generator provide additional information over directly training on relevant parts of the upstream data? Grounding this question in the setting of image classification, we compare fine-tuning on task-relevant, targeted synthetic data generated by Stable Diffusion--a generative model trained on the LAION-2B dataset--against finetuning on targeted real images retrieved directly from LAION-2B. We show that while synthetic data can benefit some downstream tasks, it is universally matched or outperformed by real data from the simple retrieval baseline. Our analysis suggests that this underperformance is partially due to generator artifacts and inaccurate task-relevant visual details in the synthetic images. Overall, we argue that targeted retrieval is a critical baseline to consider when training with synthetic data--a baseline that current methods do not yet surpass. We release code, data, and models at [https://github.com/scottgeng00/unmet-promise](https://github.com/scottgeng00/unmet-promise).

## 1 Introduction

The success of modern machine learning systems fundamentally relies on the quantity , quality , and distribution  of the data they are trained on. However, acquiring large amounts of quality data remains challenging, due to the sheer cost of data collection and annotation. As demand for training data continues to rise, the field is actively exploring approaches to automatically curate data at scale . One burgeoning approach is to source _synthetic training data_ from conditional generative models. Generative models enable data to be tailored to specific requirements and generated at scale, presenting a promising alternative to the challenges of real data curation. Recent work highlights this potential: for example, in natural language processing (NLP), researchers prompt strong proprietary language models to cheaply synthesize large-scale datasets for instruction tuning .

Analogously, in computer vision--the focus of our research--many recent works train models on synthetic images from modern text-to-image generators, aiming to achieve state-of-the-art visual recognition performance . For example, SynCLR  cleverly prompts Stable Diffusion for synthetic images tailored to pre-specified downstream image recognition domains; they find that a CLIP-like model trained from scratch on the resulting _targeted_ synthetic images can outperform CLIP trained on LAION-2B, a significantly larger untargeted dataset of real images. This result is quite surprising. Stable Diffusion is _also_ trained on LAION-2B, so by the data processing inequality, the synthetic images it generates cannot contain any additional information about LAION2B over the original real LAION-2B images. Yet, training on these derivative synthetic images appears to outperform training directly on LAION-2B. How do we make sense of these additional gains? **Does the generative model truly add useful information on top of its pretraining data?**

In this paper, we argue that the performance gained from training on generated synthetic images needs to be contextualized against a critical baseline often missing in prior work: training on real images subselected from the generative model's pretraining data. In particular, prior work has often compared _task-targeted_ synthetic images to _general, untargeted_ real images (e.g., full LAION-2B), thereby entangling the effects of training on synthetic versus real images with the effects of targeted versus general data collection. However, these variables are not intrinsically conflated. Any generative model we use to synthesize images fundamentally derives from its upstream training data. Instead of using that upstream data to train an intermediate generative model and synthesize targeted synthetic images, we can alternatively seek to directly identify targeted real images from the upstream source through retrieval (Figure 1). By comparing synthetic training data against this retrieval baseline, we isolate the value added by the generative model.

We formalize our study under the ubiquitous problem of task adaptation, where we seek to curate task-targeted images to finetune a pretrained vision model. We empirically compare training on targeted synthetic images generated from Stable Diffusion 1.5--a text-to-image model trained on the upstream LAION-2B dataset--against training on targeted real images retrieved from LAION-2B itself. We perform hundreds of experiment runs across an order of magnitude of data scales on five visual recognition tasks (ImageNet , Describable Textures (DTD) , FGVC-Aircraft , Stanford-Cars , and Oxford Flowers102 ) where training on synthetic data has shown promise [62; 23].

Together, we find that **training on targeted real data retrieved from a generative model's upstream training dataset outperforms training on synthetic data from the generative model.** For example, while training on targeted synthetic images can improve downstream accuracy by up to 7.1% (absolute) on its best-case benchmark (FGVC-Aircraft), training on targeted real images helps even further, boosting accuracy by a massive \(17.7\%\). On other benchmarks, such as ImageNet, we find that training on synthetic images can sometimes _hurt_ performance even when training on real data improves it. We further show that these findings hold across several different versions of Stable Diffusion, as well as when we train on a mix of synthetic and real data. Our analysis suggests that the consistent underperformance of models trained on synthetic images is partially due to low-level generator artifacts in the synthetic images (e.g., blurs), and partially because synthetic images may distort high-level class-specific visual details that real images preserve.

Overall, we conclude that retrieval is a critical baseline to consider when evaluating the true added utility of generated synthetic training data. Our goal is not to make normative claims about whether synthetic data will ever surpass this standard, but to contribute a simple baseline to aim for, and a clean set of experiments to explicitly measure progress towards surpassing it. For instance, by conceptualizing retrieval from a generator's training data as a strong alternative to synthesizing data, a natural future direction for improving synthetic training data is to synthesize image compositions that are explicitly absent from the generator's upstream training set. Images generated in this manner may offer unique value beyond what can be retrieved from the training data. Finally, in settings where the upstream dataset of a generator is unavailable altogether (e.g., due to privacy concerns, due to proprietary data, or due to download bandwidth restrictions), the retrieval baseline is unrealizable by assumption; synthetic data therefore retains strong utility for distilling knowledge from generative

Figure 1: Given an upstream dataset of general real image-text pairs, we aim to curate a _targeted_ dataset to train a learner on some target task. We can either (1) **retrieve targeted real images** directly from the upstream dataset, or we can (2) first train an intermediate generative model and then synthesize targeted synthetic images. By comparing these two approaches, our paper seeks to measure what value training on generated synthetic data adds.

models and for privacy preservation. We release all code, models, and over 1TB of generated images to guide future work ([https://github.com/scottgengo0/unmet-promise](https://github.com/scottgengo0/unmet-promise)).

## 2 Related Work

Learning from synthetic data.Synthetic data has been widely explored in the context of many machine learning problems . For example, in NLP, synthetic data generated from strong large language models  has been used to distill instruction-following behavior  and task-specific knowledge  into smaller models. In computer vision, prior works have sought to use synthetic data to improve the state-of-the-art across a breadth of visual tasks, such as object detection , semantic segmentation , and optical flow . Traditionally, this synthetic training data has been sourced from expert-crafted simulation and rendering pipelines . Recent advances in text-to-image synthesis via diffusion models  are changing this paradigm, inspiring a new wave of works that seek to train visual models on synthetic data algorithmically sampled from large-scale generative models . Recent works have also sought to algorithmically sample synthetic data from diffusion models trained on data-scarce domains . This structural shift in the source of synthetic images from _expert-supervised programmatic simulation_ to _a learned generator_ that itself derives supervision from upstream data raises a critical question: does the intermediate step of training a generator and sampling synthetic data provide any added useful information over simply training on relevant parts of the upstream data directly? Our work formalizes and empirically grounds this question, contributing experiments and baselines to rigorously measure the benefits of training on modern data-derived synthetic data.

Adapting pretrained vision models.Large-scale pretrained image models such as CLIP  offer transferable visual features that benefit a wide range of downstream vision tasks. It is now common practice to use pretrained models as a starting point when deploying downstream task-specific models instead of training them from scratch . From an algorithmic perspective, many methods have been proposed to adapt CLIP models to downstream tasks, each with varying trade-offs . We choose to study simple full-finetuning, centering our work on the _data_ we adapt on as opposed to the algorithm. In particular, the quality and relevance of adaptation data has a crucial impact on downstream task performance; distribution shifts at inference time can significantly hurt performance . Acquiring task-targeted data thus remains an active area of research . Most related to our work is , who also employ retrieval as a technique for collecting task targeted-data. Our work builds upon these methods to construct baselines for systematically measuring the true added utility of model-generated synthetic training images.

Retrieval as a baseline for synthetic data.Recent studies  similarly use retrieval from generative model training data as a baseline for synthetic training images. For example,  adapts Stable Diffusion with real medical images and then generates synthetic medical images to train a ResNet from scratch; the resulting ResNet outperforms a ResNet trained on LAION-retrieved medical images. Consistent with our work,  find that in the low-data regime of general image recognition tasks (e.g., subsampled ImageNet-1K, STL-10), augmenting real datasets with LAION-retrieved images outperforms augmenting with Stable Diffusion generated images when training ResNets from scratch.  further finds that this performance gap in the low-data regime persists across many synthetic image generation methods, including when they finetune Stable Diffusion with task-specific real data. In the medium-data regime (e.g. full ImageNet-1k),  finds augmenting with synthetic data matches but does not outperform augmenting with retrieved data. Our work generalizes the shared motivation of measuring the utility of synthetic training images to the modern data-rich regime and a wide range of visual classification tasks.

## 3 Problem Setting and Method

Given a large dataset \(\) of general real image-text pairs and a downstream visual classification task specified as a set of text class names \(\), we aim to algorithmically curate a **targeted adaptation dataset**\(_{}\) of images \(x_{i}\) and one-hot class labels \(y_{i}\) to finetune and improve a pretrained vision model's performance on the downstream task. We compare two high-level approaches for sourcing this targeted data, shown in Figure 1: (1) we **retrieve targeted real images** directly from \(\), forming a targeted retrieved dataset \(_{}^{()}\). Alternatively, (2) we **generate targeted synthetic images**by prompting an intermediate text-to-image generative model trained on \(\), forming a targeted synthetic dataset \(^{()}_{}\). We detail each approach below.

### Sourcing data by generating synthetic images

We follow SynCLR , a representative method for curating synthetic training data from off-the-shelf text-to-image models. Given the set of visual class names \(\), we first synthesize a large corpus of corresponding image captions by prompting a large language model (details in Appendix C.1). For example, if the class name \(c\) is "rose," then a generated caption might be "a close-up of a pink rose in bloom." We then use these captions as input for a text-to-image generator \(G\) trained on the upstream data \(\), yielding a large set of synthesized images \(}\). Each image is assigned a class label \(y_{i}\) based on the class name \(c\) used to synthesize its caption. These synthetic images and labels \(\{(},y_{i})\}\) form our curated dataset \(^{()}_{}\).

### Sourcing data by retrieving real images

Rather than querying a generator trained on an upstream dataset \(\), we can directly train on parts of \(\) itself by retrieving relevant data. \(\) consists of image-text pairs \((x_{i},t_{i})\). To retrieve relevant pairs, we consider two strategies. We additionally deduplicate all retrieved images with respect to our evaluation datasets following  to minimize test set leakage. We apply NSFW filtering .

Strategy 1: hard substring matching.Inspired by , we retrieve the set \(^{()}_{}\) of all images \(x_{i}\) whose corresponding caption \(t_{i}\) contains at least one target class name \(c\) as a substring:

\[^{()}_{}=\{(x_{i},y_{i})\,:\,(x_{i},t_{i })ct_{i}\}\,.\]

Here, label \(y_{i}\) is assigned based on the class \(c\) contained in \(t_{i}\). If an image-text pair \((x_{i},t_{i})\) has text \(t_{i}\) containing multiple class names \(c,c^{}\), then we simply retrieve \(x_{i}\) multiple times and assign each instance a different label, once for each unique matched class name.

Strategy 2: semantic \(k\)-NN retrieval.Hard substring matching is simple and effective when the target visual concepts \(c\) are concrete entities that are likely to be described in text captions (e.g., \(c=\) "fire lily"), but may be less effective when the concepts are abstract (e.g., \(c=\) "lined texture"). Thus, we also consider semantic (soft) retrieval via CLIP image-text embedding space similarity2. We convert each target class name \(c\) into a set of natural language search queries \(Q_{c}\) based on the templates from the original CLIP paper . For each query \(q_{c} Q_{c}\), we use approximate \(k\)-NN search  to retrieve the set \(S_{q_{c}}\) of \(k\)-nearest image-text pairs \((x_{i},t_{i})\) by CLIP similarity between the query \(q_{c}\) and _either_ the image \(x_{i}\) or the text \(t_{i}\):

\[S_{q_{c}}=,t_{i}) D}{ (x_{i},q_{c})}},t_{i}) D}{ (t_{i},q_{c})}}.\]

We assign each image-text pair \((x_{i},t_{i}) S_{q_{c}}\) a class label \(y_{i}\) based on the class name in query \(q_{c}\). We form the targeted dataset \(^{()}_{}\) by unioning over all queries \(q_{c} Q_{c}\) and all classes \(c\):

\[^{()}_{}=_{c} _{q_{c} Q_{c}}\{(x_{i},y_{i})(x_{i},t_{i},y_{i}) S_{q _{c}}\}.\]

### Additional data filtering and postprocessing

Data filtering has been shown to improve training performance for both real and synthetic data and is widely used [20; 24]. We filter both our synthetic and retrieved datasets following current best filtering practices for synthetic image data . Given a curated dataset \(_{}\), we compute the CLIP similarity of each \(x_{i}_{}\) with text corresponding to its assigned label \(y_{i}\) (e.g., "a photo of {class name}"), constructed using the CLIP zero-shot classification templates . When there are multiple templates for a given class, we aggregate by taking the maximum similarity across templates. Wekeep the top 30% of images per class by aggregate similarity. Intuitively, filtering helps remove generated and retrieved images with class-misaligned content. For example, an image labeled "dog" but without any dogs present (i.e., due to retrieval or generation errors) would receive a lower CLIP similarity score and likely be discarded. See Appendix C.2 for further discussion.

Our synthetic adaptation datasets \(_{}^{()}\) are class-balanced by construction (i.e., we uniformly generate images for each class). We further postprocess the retrieved adaptation datasets \(_{}^{()}\) to improve class balancing by manually fixing a global threshold \(M\) and truncating the dataset such that each class label \(y_{i}\) occurs at most \(M\) times.

## 4 Main Experiments

We seek to measure the utility of learning from model-generated synthetic images. Grounding this question empirically, our experiments compare finetuning a pretrained CLIP model on (1) targeted synthetic images \(_{}^{()}\) to (2) finetuning on targeted retrieved real images \(_{}^{()}\).

**Benchmarks.** We focus on five downstream tasks where synthetic data has shown promise compared to similar scale _untargeted_ real data . We select (a) ImageNet-1K  and Describable Textures (DTD)  to evaluate recognition performance on broad categories and (b) FGVC-Aircraft , StanfordCars , and Oxford Flowers102  to evaluate performance in fine-grained settings. We use standard pre-defined train, test, and validation splits when available, and otherwise randomly subset the training set to create missing train-validation splits (details in Appendix D.2).

**Finetuning data curation.** For each downstream benchmark, we first curate an adaptation dataset \(_{}\) (Section 3) by either (1) generating synthetic images with Stable Diffusion 1.5 , trained on the LAION-2B dataset , or (2) retrieving real images directly from LAION-2B. We treat the choice between our substring-based and semantic retrieval strategies as a hyperparameter, using downstream validation set accuracy to determine the best choice for each benchmark. Hyperparameters for retrieval are detailed in Appendix D.1.

**Model adaptation and evaluation.** We adapt a LAION-2B pretrained CLIP ViT-B/16  image encoder by finetuning on the curated adaptation dataset \(_{}\) with a cross-entropy classification loss for

Figure 2: We adapt a pretrained CLIP image encoder (dashed purple line) to different downstream image classification tasks, using either (a) targeted synthetic data (orange triangles) generated from a Stable Diffusion model trained on LAION-2B or using (b) targeted real data (blue circles) directly retrieved from LAION-2B. We measure performance via downstream **zero-shot (ZS)** and **linear probing (LP)** accuracy, aggregating results over at least 3 seeds (error bars indicate \( 1\) standard deviation). Overall, while adapting CLIP with targeted synthetic data can sometimes improve performance over an off-the-shelf model, synthetic data is universally outperformed or matched by targeted real data. This gap persists even when we scale the sample size of the synthetic adaptation dataset beyond the maximum amount of (finite) targeted real data considered (gray shaded regions).

a pre-set number of epochs. To elucidate the scaling trends of synthetic and retrieved data, we finetune across an order of magnitude of different adaptation dataset scales, subsampled from the full targeted adaptation dataset \(_{}\). We report zero-shot (ZS) and linear probing (LP) test set accuracy, using the benchmark train set to train the LP head. For both LP and ZS evaluation, we use the validation set to identify the best epoch and finetuning hyperparameters. For each data scale, we aggregate accuracy across the results of at least three random seeds, and report the standard deviation due to seed randomness. Please refer to Appendix D.3 for further evaluation details, and Appendix D.4 for further training and hyperparameter details.

### Main results: synthetic training data lags behind a baseline of retrieved real images

We present our main zero-shot and linear probing scaling results in Figure 2.

At equivalent data scales, finetuning with model-generated synthetic images can help, but is universally matched or outperformed by finetuning directly with images from the generator's training data.Consistent with prior research , we find that training with targeted synthetic data can improve an unadapted model. For example, on FGVC-Aircraft--the setting where previous works have found strongest gains--finetuning with 139K Stable-Diffusion generated images improved downstream linear probing accuracy by an average of 3.8 percentage points over an off-the-shelf CLIP model (\(64.9\% 68.7\%\)); on DTD, training with 110K synthetic images improves zero-shot accuracy by 3.3 points (\(56.3\% 59.6\%\)).

However, the gains from training on synthetic data are consistently matched or surpassed by training on retrieved real data. For instance, on FGVC-Aircraft, finetuning with an equivalent 139K LAION-2B retrieved images boosts performance by a massive 17.8 points (\(64.9\% 82.7\%\)). Moreover, adapting with retrieved data can improve performance even when synthetic data does not (e.g., on ImageNet and Flowers102 zero-shot accuracy.) Finally, adapting with synthetic data can sometimes even _hurt_ performance (ImageNet, StanfordCars, Flowers102 zero-shot), while targeted retrieved data improves or at least does not hurt performance across all settings considered. Given equal amounts of targeted retrieved and synthetic data, retrieved data is the clear winner.

Synthetic data can sometimes decrease the gap with retrieved data given additional scale, but remains behind.The amount of data we can retrieve is fundamentally limited by the finite upstream data pool. For example, even after searching all 2 billion LAION-2B samples for images containing an FGVC-Aircraft class name in the caption, substring-based retrieval returned only 139K targeted images post-filtering. In contrast, it is straightforward to create ever-larger synthetic datasets by simply generating more data.

Scaling the synthetic adaptation dataset size beyond the amount of retrieved data considered (illustrated in the gray-shaded regions of Figure 2), we find that increasing the amount of targeted synthetic data does not always improve performance. For example, on DTD, synthetic data exhibits U-shaped scaling, with performance positively scaling up to 110K synthetic training images, after which performance declines. On ImageNet, Flowers102, and StanfordCars, increasing the synthetic dataset size consistently hurts zero-shot accuracy and has minimal impact on linear probing performance.

On Aircraft, scaling helps; there is a log-linear relationship between the size of the synthetic adaptation dataset and downstream linear probing accuracy (_e.g._, scaling from 139K \(\) 250K synthetic images improves linear probing accuracy from \(68.7\% 70.7\%\)). However, synthetic data still lags behind retrieved data: matching the performance of a mere 15K retrieved aircraft images requires scaling the synthetic dataset to 500K images, reflecting a \(\)33x difference in dataset size and required finetuning compute. Naively extrapolating this ratio, matching the performance of the full 139K retrieved adaptation dataset would require nearly 5M synthetic images after top 30% filtering. We note, however, that synthetic data is unlikely to truly scale infinitely, as synthetic data fundamentally derives from the (finite) training set of our generative model. Still, the performance of synthetic data is likely unsaturated at the 500K scale (_i.e._, accuracy is still trending up); due to compute limitations, studying whether further scaling can outperform retrieved data is left for future work.

Synthetic data can improve a model's task representation without significantly improving the model's task performance.Broadly speaking, zero-shot task accuracy measures a model's ability to directly solve the downstream task, whereas linear probing accuracy measures the quality of the model's learned _task-relevant representation_. We find that even when training on synthetic data improves the model's representation (i.e., downstream linear probing accuracy), it may not significantly improve the model's zero-shot accuracy. In contrast, when training on retrieved data improves the model's representation, zero-shot accuracy also exhibits positive scaling. For example, CLIP adapted with either 15K retrieved images or 500K synthetic images both achieve a similar linear probing accuracy (\( 72\%\)), yet the model adapted with synthetic data achieves a much worse zero-shot accuracy (\(28.9\%\) versus \(39.5\%\)). We discuss possible reasons for this qualitative discrepancy in model behavior in our analyses below (Section 5.1).

## 5 Analysis

In this section, we explore two questions to better understand our main results. First, what factors drive the underperformance of synthetic data? Second, do our findings hold under variations of our experimental setup? We focus our analysis experiments on ImageNet, to understand general image recognition performance, and FGVC-Aircraft, the sole benchmark where synthetic data exhibited strong positive log-linear scaling. Additional analysis experiments are presented in Appendix B.

### Why does synthetic data lag retrieved real data?

Qualitative visualizations.We visualize a random selection of images from our curated synthetic and retrieved adaptation datasets in Figure 3. Compared to retrieved real images, we observe that the synthetic images (1) contain low-level generator artifacts, and (2) differ in visual content distribution, both in terms of semantic details and overall image composition. For example, although the synthetic FGVC-Aircraft adaptation images (top two rows of Figure 3) are recognizable as airplanes, the visual content often contains incorrect class-relevant semantic details: a correctly-depicted "Airbus A320" should have one engine per wing and two sets of wheels at its rear, yet our synthetic images often exhibit incorrect engine or wheel configurations. This qualitative discrepancy in visual detail precision may partially explain why training on synthetic data does not improve task zero-shot accuracy; synthetic images do not retain enough class-accurate details to directly teach the model the downstream task. In contrast, training on synthetic images can improve linear probing accuracy, because the synthetic images still broadly look like aircraft and thus may help align the model's representation to the downstream domain.

Figure 3: We visualize **retrieved real images** and synthetic images from our targeted adaptation datasets for FGVC-Aircraft (top two rows) and ImageNet-1K (bottom two rows), alongside ground truth images (left column) for reference. Compared to retrieved images, synthetic images often (1) contain generator artifacts (e.g., the blur on the edges of the “Cessna 172”, the eyes and mouth of the “Tabby Cat”) and also (2) distort class-relevant visual content, such as the engine configuration of a true “Airbus A320” (i.e., exactly one engine per wing) and the entire visual appearance of a “Flute”. We hypothesize that both factors contribute to synthetic training data’s underperformance versus real training data.

Synthetically perturbing retrieved real images.To disentangle the effect of low-level generator artifacts and visual content differences between synthetic and retrieved real images on downstream model performance, we trained on "hybrid" images that have similar semantic visual content as our retrieved real images but contain generator artifacts like our synthetic images. Following SDEdit , we use Stable Diffusion to synthetically perturbed our retrieved images to introduce model-specific artifacts present in the synthetic images Stable Diffusion generates. Given a noise strength parameter \(\) and a retrieved image \(x_{0}\), SDEdit adds Gaussian noise to \(x_{0}\) according to timestep \(t=\) of Stable Diffusion's time-dependent forward process. We then denoise the noisy image using the same reverse diffusion process as in text-to-image generation, yielding a perturbed image \(x^{()}\) that looks semantically like \(x_{0}\) while also containing Stable Diffusion-specific artifacts. Increasing \(\) increases the amount of Gaussian noise added to \(x_{0}\), thereby increasing the severity of visual artifacts introduced in the resulting \(x^{()}\) (see Appendix E.1 for further details). In pseudocode,

\[x^{()}=((x_{0},),).\]

Starting from the full targeted retrieved adaptation datasets \(^{()}_{}\) for FGVC-Aircraft and ImageNet, we use SDEdit to introduce generator artifacts into the retrieved real images over a range of \(\) values. We visualize the resulting perturbed images in Figure 4, and plot the results of training on these perturbed images across \(\) in Figure 5.

Figure 4: We use Stable Diffusion to synthetically perturb real images according to a noise strength parameter \(\), where larger \(\) increases the severity of generator-specific artifacts added by the perturbation. When \( 0.6\), the introduced artifacts can be strong enough to damage task-relevant visual details for finegrained tasks like FGVC-Aircraft (e.g., the airplane’s engine and rear wheels). For broad tasks like ImageNet, artifacts have a lesser impact on class-relevant details; the “Tabby Cat” is recognizable as a cat even after perturbing with high \(\).

Figure 5: We finetune a pretrained CLIP model (dashed purple line) on retrieved real images that are synthetically perturbed (green circles) with Stable Diffusion to introduce generator artifacts. The perturbation strength is controlled by a parameter \(\) where larger \(\) introduces stronger artifacts; within the gray-shaded region, the artifacts are strong enough to damage class-relevant details. Our results suggest that generator artifacts do contribute to synthetic data’s underperformance—any artifact level causes performance to drop below training on retrieved images (dashed blue line). Moreover, differences in visual content between synthetic and retrieved images also matter; even with relatively strong perturbations (\(=0.5\)), training on artifact-afflicted perturbed images that retain the semantic content of retrieved images outperforms training on synthetic images (dashed orange line).

Our results suggest three takeaways. First, generator artifacts indeed contribute to the underperformance of synthetic training images, especially for fine-grained classification tasks. On FGVC-Aircraft, any amount of added generator artifacts drops downstream accuracy. Second, the impact of artifacts is relatively lower for broad classification domains such as ImageNet, where downstream performance is not significantly impacted until we perturb with a relatively strong noise strength of \(=0.5\). Finally, visual content differences between synthetic and retrieved images also play a key role in the performance gap between synthetic and retrieved training data. When we perturb images with strength \(=0.5\), the resulting images are heavily afflicted with artifacts, but still retain the important class-relevant details of retrieved real images, such as correct airplane engine configurations. Training on \(=0.5\) images significantly outperforms training on synthetic images. Intriguingly, training on aircraft images perturbed beyond the point where class-relevant visual details are damaged (\( 0.6\)) still outperforms synthetic data; we speculate that this is because these heavily perturbed images still retain the overall image composition of retrieved images.

### Synthesizing data via another generative model

For our main scaling experiments, we generate synthetic image datasets using Stable Diffusion 1.5 to maintain consistency with prior work [62; 23]. To what degree does our choice of generative model impact our findings? At the time of our study, Stable Diffusion 1.x models are the only modern text-to-image models with open source training data available to retrieve from. Therefore, we focus our study here on Stable Diffusion (SD) versions 1.1, 1.3, and 1.5. Starting from SD v1.1, which is trained on the full LAION-2B dataset, SD v1.3 and v1.5 are derived by further finetuning on high-quality subsets of LAION-2B. This additional finetuning improves image generation fidelity , but may lead to the model forgetting parts of the LAION-2B distribution . Following our main experiment setup (Section 4), we use SD v1.1 and SD v1.3 to generate various-sized targeted synthetic adaptation datasets for ImageNet and FGVC-Aircraft. Results are plotted in Figure 6. Overall, while training with synthetic data from different generative models yields varying performance, synthetic data from all generative models considered consistently fall short of retrieval.

### Mixing synthetic and retrieved data

On FGVC-Aircraft, finetuning CLIP with either synthetic or retrieved data alone consistently improves downstream task accuracy. The gains from retrieved data are stronger than the gains from synthetic data across all data scales; however, synthetic data may improve CLIP in ways that is complementary to retrieved data, and thus present orthogonal value. To test this possibility, we measure whether training on a mix of synthetic and retrieved Aircraft adaptation data significantly outperforms training with either alone. Starting from our largest retrieved adaptation dataset \(_{}^{()}\) (139K images), we progressively add in increasing amounts of synthetic images from our synthetic adaptation dataset \(_{}^{()}\) and finetune a pretrained CLIP model with the resulting mix. We plot results in Figure 7. We find that training on the mixed images outperforms training on synthetic images alone; however, training on a mix significantly drops performance compared to using retrieved data alone.

Figure 6: We ablate our choice of generative model used to synthesize images in our main experiments, comparing training on synthetic images from Stable Diffusion 1.5 (SD v1.5) to synthetic images from SD v1.1 and SD v1.3. Across multiple Stable Diffusion models—the only modern text-to-image generators with open-source training data available—training with images directly retrieved from the generative models’ training dataset (**LAION-2B**) outperforms training with generated images.

## 6 Discussion

Our work sought to answer a key question: given that all model-generated synthetic images derive from the generator's upstream training data, does training on synthetic images provide value over training on the parts of the upstream real data directly? We contribute a set of rigorous experiments to ground this question empirically, and discover that training on upstream real images collected via our simple retrieval baseline significantly outperforms training on synthetic images. Our initial question is answered negatively. We therefore argue that retrieval is a critical baseline to surpass in order to show value from synthetic training data, and encourage comparison against it in future research.

Importantly, we do not seek to make normative claims about whether training with synthetic images will ever surpass this baseline--future work may unlock gains that we have not yet found. As a first step, we contribute analyses of why synthetic training images underperform upstream real images, finding that both generator artifacts and semantic errors within synthetic images are key areas for future improvement. Furthermore, given that image retrieval is a strong alternative to image synthesis, a natural next step is to generate image compositions that are explicitly rare or absent from the generator's upstream training dataset; we are optimistic that synthesizing these "missing" images may offer unique value beyond what is present in the existing upstream real images. Such an approach leverages the compositional generalization abilities of the generator, which recent research promisingly suggests may be stronger than the compositionality of a discriminative model trained on the same upstream data .

Finally, our findings assume access to the generative model's upstream training data, an assumption that may not always hold. The upstream pool may be proprietary or strictly regulated due to privacy concerns. In such settings, training directly on the upstream data is impossible; synthetic data from a generative model trained on this unavailable upstream data remains an exciting alternative to acquire otherwise inaccessible information.

Limitations.As an empirical study, our compute budget limits the number of experimental variations we consider. Our results are derived from adapting CLIP models with standard full finetuning; we conjecture that our findings generalize to other large-scale pretrained backbones and adaptation methods as well, but we were not able to test this empirically. Moreover, at the time of our work, Stable Diffusion is the only text-to-image model with publicly available training data to retrieve from (i.e. LAION-2B); we do not study other generators trained on other data pools. Finally, we focus on model accuracy, leaving a comparison of model robustness and fairness from training on synthetic versus real data to future work.