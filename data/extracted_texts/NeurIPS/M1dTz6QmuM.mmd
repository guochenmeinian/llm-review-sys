# ## The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions

## The RL Perceptron: Generalisation Dynamics of Policy Learning in High Dimensions

### Anonymous Author(s)

Affiliation

Address

email

#### Abstract

Reinforcement learning (RL) algorithms have proven transformative in a range of domains. To tackle real-world domains, these systems often use neural networks to learn policies directly from pixels or other high-dimensional sensory input. By contrast, much theory of RL has focused on discrete state spaces or worst-case analysis, and fundamental questions remain about the dynamics of policy learning in high-dimensional settings. Here, we propose a solvable high-dimensional model of RL that can capture a variety of learning protocols, and derive its typical dynamics as a set of closed-form ordinary differential equations (ODEs). We derive optimal schedules for the learning rates and task difficulty--analogous to annealing schemes and curlula during training in RL--and show that the model exhibits rich behaviour, including delayed learning under sparse rewards; a variety of learning regimes depending on reward baselines; and a speed-accuracy trade-off driven by reward stringency. Experiments on a variant of the Procgen game "Bossfight" also show such a speed-accuracy trade-off in practice. Together, these results take a step towards closing the gap between theory and practice in high-dimensional RL.

Recent years have seen rapid progress in Reinforcement Learning (RL): algorithmic and engineering breakthroughs led to super-human performance in a variety of domains, for example complex games like Go (Silver et al., 2016; Mnih et al., 2015). Despite these practical successes, our theoretical understanding of RL for high-dimensional problems requiring non-linear function approximation is still limited. While comprehensive theoretical results exist for tabular RL, where the state and action spaces are discrete and small enough for value functions to be represented directly, the curse of dimensionality limits these methods to low-dimensional problems. The lack of a clear notion of similarity between discrete states further means that tabular methods do not address the core question of generalisation: how are values and policies extended to unseen states and across seen states (Kirk et al., 2023)? As a consequence, much of this theoretical work is far from the current practice of RL, which increasingly relies on deep neural networks to approximate and generalise value functions, policies and other building blocks of RL. Moreover, while RL theory has often addressed "worst-case" performance and convergence behaviour, the _typical_ behaviour has received comparatively little attention (cf. further related work below). Meanwhile, a growing sub-field of deep learning theory has employed tools from statistical mechanics to analyse various supervised learning paradigms in the average-case, see Seung et al. (1992); Engel and Van den Broeck (2001); Carleo et al. (2019); Bahri et al. (2020); Gabrie et al. (2023) for classical and recent reviews. While this approach has recently been extended to curriculum learning (Saglietti et al., 2022), continual learning (Asanuma et al., 2021; Lee et al., 2021, 2022), few-shot learning (Sorscher et al., 2022) and transfer learning (Lampinen and Ganguli, 2018; Dhifallah and Lu, 2021; Gerace et al., 2022), RL has not been analysed yet using statistical mechanics--a gap we address here by studying the high-dimensional generalisation dynamics of a simple neural network trained on a reinforcement learning task.

[MISSING_PAGE_EMPTY:2]

## Further related work

Sample complexity in RL.An important line of work in the theory of RL focuses on the sample complexity and other learnability measures for specific classes of models such as tabular RL (Azar et al., 2017; Zhang et al., 2020), state aggregation (Dong et al., 2019), various forms of MDPs (Jin et al., 2020; Yang and Wang, 2019; Modi et al., 2020; Ayoub et al., 2020; Du et al., 2019; Zhang et al., 2022), reactive POMDPs (Krishnamurthy et al., 2016), and FLAMBE (Agarwal et al., 2020). Here, we are instead concerned with the learning dynamics: how do reward rates, episode length, etc. influence the speed of learning and the final performance of the model.

Statistical learning theory for RL aims at finding complexity measures analogous to the Rademacher complexity or VC dimension from statistical learning theory for supervised learning Bartlett and Mendelson (2002); Vapnik and Chervonenkis (2015). Proposals include the Bellman Rank Jiang et al. (2017), or the Eluder dimension (Russo and Van Roy, 2013) and its generalisations (Jin et al., 2021). This approach focuses on worst-case analysis, which typically differs significantly from practice (at least in supervised learning (Zhang et al., 2021)). Furthermore, complexity measures for RL are generally more suitable for value-based methods; policy gradient methods have received less attention despite their prevalence in practice Bhandari and Russo (2019); Agarwal et al. (2021). We focus instead on average-case dynamics of policy-gradient methods.

Dynamics of learning.A series of recent papers considered the dynamics of temporal-difference learning and policy gradient in the limit of wide two-layer neural networks Cai et al. (2019); Zhang et al. (2020); Agazzi and Lu (2021, 2022). These works focus on one of two "wide" limits: either the neural tangent kernel (Jacot et al., 2018; Du et al., 2019) or "lazy" regime (Chizat et al., 2019), where the network behaves like an effective kernel machine and does not learn data-dependent features, which is key for efficient generalisation in high-dimensions. In our setting, the success of the student crucially relies on learning the weight vector of the teacher, which is hard for lazy methods (Ghorbani et al., 2019, 2020; Chizat and Bach, 2020; Refinetti et al., 2021). The other "wide" regime is the mean-field limit of interacting particles, akin to Mei et al. (2018); Chizat and Bach (2018); Rotskoff and Vanden-Eijnden (2018), where learning dynamics are captured by a non-linear partial differential equation. While this elegant description allows them to establish global convergence properties, it is hard to solve in practice. The ODE description we derive here instead will allow us to describe a series of effects in the following sections.

## 1 The RL Perceptron: setup and learning algorithm

We study the simplest possible student network, a perceptron with weight vector \(\) that takes in high-dimensional inputs \(^{D}\) and outputs \(y()=(^{})\). We interpret the outputs \(y()\) as decisions, for example whether to go left or right in an environment. Because the student makes choices in response to high-dimensional inputs, it is analogous to a policy network. To train the network, we therefore consider a policy gradient learning update analogous to the REINFORCE algorithm (Sutton et al., 2000) that is adapted to the perceptron. At every timestep \(t\) during the \(\)th episode of length \(T\), the agent occupies some state \(s_{t}\) in the environment, receives an observation \(_{t}^{}\) conditioned on \(s_{t}\), and takes an action \(y_{t}^{}=(^{}_{t}^{})\), with \(t=1,,T\). The correct choice for each input is given by a fixed perceptron teacher with weights \(^{*}\). The crucial point is that the student does not have access to all the correct choices; it only receives a reward at the end of the episode _if_ it completes the episode successfully, for example by making the correct decision at all times. If it does not succeed, it _may_ receive a penalty; we will see in section 2.4 that receiving penalties is not always beneficial. In our setup, this translates into a weight update at the end of the \(^{}\) episode that is given by

\[^{+1}=^{}+}{} (_{t=1}^{T}y_{t}_{t}())^{ }-}{}(_{t=1}^{T}y_{t}_ {t}(1-()))^{},\] (1)

where \(\) is an indicator function and \(\) is the criterion that determines whether the episode was completed successfully--for instance, \(()=_{t}^{T}(y_{t}y_{t}^{*})\) (where \(\) is the step function) if the student has to get every decision right in order to receive a reward. The update is general in the sense that the term proportional to the learning rate \(_{1}>0\) prescribes the reward update for the fulfillment of the condition, while the term proportional to \(_{2} 0\) gives us the possibility to add a a penalty or negativereward should the student not succeed. Note that in the case of \(T=1\), \(_{2}=0\), and \(()=(yy^{*})\), the learning rule updates the weight only if the student is correct on a given sample. It can thus be seen as the "opposite" of the famous perceptron learning rule of supervised learning (Rosenblatt, 1962), where weights are only updated if the student is wrong. For a more in-detail discussion of the relation between the weight update in eq. (1) and the REINFORCE algorithm, see appendix A.

## 2 Theoretical Results

### A set of dynamical equations captures the learning dynamics of an RL perceptron exactly

The goal of the student during training is to emulate the teacher as closely as possible; or in other words, have a small number of disagreements with the teacher \(y() y^{*}()\). The generalisation error is given by the average number of disagreements

\[_{g} y()y^{*}()= (^{*}/) (/)=()()\] (2)

where the average \(\) is taken over the inputs \(\), and we have introduced the scalar pre-activations for the student and the teacher, \(/\) and \(^{*}/\), respectively. We can therefore transform the high-dimensional average over the inputs \(\) into a low-dimensional average over the pre-activations \((,)\). The average in eq. (2) can be carried out by noting that the tuple \((,)\) follow a jointly Gaussian distribution with means \(==0\) and covariances

\[Q^{2}=}{D}, R =^{*}}{D}  S^{2}=^{*}^{*}}{D}.\] (3)

These covariances, or overlaps as they are sometimes called in the literature, have a simple interpretation. The overlap \(S\) is simply the length of the weight vector of the teacher; in the high-dimensional limit \(D\), \(S 1\). Likewise, the overlap \(Q\) gives the length of the student weight vector; however, this is a quantity that will vary during training. For example, when starting from small initial weights, \(Q\) will be small, and grow throughout training. Lastly, the "alignment" \(R\) quantifies the correlation between the student and the teacher weight vector. At the beginning of training, \(R 0\), as both the teacher and the initial condition of the student are drawn at random. As the student starts learning, the overlap \(R\) increases. Evaluating the Gaussian average in eq. (2) shows that the generalisation error is then a function of the normalised overlap \(=R/\), and given by

\[_{g}=(})\] (4)

The crucial point here is that we have reduced the description of the high-dimensional learning problem from the \(D\) parameters of the student weight \(\) to two time-evolving quantities, \(Q\) and \(R\). We now discuss how to analyse their dynamics.

Figure 2: **ODEs accurately describe diverse learning protocols.** Evolution of the normalised student-teacher overlap \(\) for the numerical solution of the ODEs (dashed) and simulation (coloured) in three reward protocols. All students receive a reward of \(_{1}\) for getting all decisions in an episode correct, and additionally: _(a)_ A penalty \(_{2}\) (i.e. negative reward) is received if the agent does not survive until the end of an episode. _(b)_ An additional reward of 0.2 is received if the agent survives beyond \(T_{0}\) timesteps. _(c)_ An additional reward \(r_{b}\) is received for every correct decision made in an episode. _Parameters:_\(D=900\), \(T=12\), \(_{1}=1\).

The dynamics of order parameters.At any given point during training, the value of the order parameters determines the test error via eq. (4). But how do the order parameters evolve during training with the update rule eq. (1)? We followed the approach of Kinzel and Rujan (1990); Saad and Solla (1995); Biehl and Schwarze (1995) to derive a set of dynamical equations that describe the dynamics of the student in the high-dimensional limit where the input dimension goes to infinity. We give explicit dynamics for different reward conditions \(\), namely requiring all decisions correct in an episode of length \(T;\) requiring \(n\) or more decisions correct in an episode of length \(T;\) and receiving reward for each correct response. Due to the length of these expressions, we report the generic expression of the updates in the supplementary material in appendix B. Below, we state a version of the equations for the specific reward condition where the agent must survive until the end of an episode to receive a reward, \(()=_{t}^{T}(y_{t}y_{t}^{*})\). The ODEs for the order parameters then read

\[ =+_{2}}{}(1+ })P^{T-1}-_{2}R}\] (5) \[ =(_{1}+_{2})}(1+})P^{T-1}-2_{2}}+^{2}-_{ 2}^{2})}{T}P^{T}+^{2}}{T},\] (6)

where \(/D\) serves as a continuous time variable in the limit \(D\) (not to be confused with \(t\) which counts episode steps), and \(P=(1-^{-1}(R/)/)\) is the probability of a single correct decision. While our derivation of the equations follow heuristics from statistical physics, we anticipate that their asymptotic correctness in the limit \(D\) can be established rigorously using the techniques of Goldt et al. (2019); Veiga et al. (2022); Arnaboldi et al. (2023). We illustrate the accuracy of these equations already in finite dimensions (\(D=900\)) in fig. 1c, where we show the expected reward, as well as the overlaps \(R\) and \(Q\), of a student as measured during a simulation and from integration of the dynamical equations (solid and dotted lines, respectively).

The derivation of the dynamical equations that govern the learning dynamics of the RL perceptron are our first main result. Equipped with this tool, we now analyse several phenomena exhibited by the RL perceptron through a detailed study of these equations.

### Learning protocols

The RL perceptron allows for the characterization of different RL protocols by adapting the reward condition \(\). We considered the following three settings:

**Vanilla:** The dynamics in the'standard' case without penalty, \(_{2}=0\), is shown in fig. 5a and fig. 5b. Rewards are sparsest in this protocol, and as a result we observe a characteristic initial plateau in expected reward followed by a rapid jump. The length of this plateau increases with \(T\), consistent with the notion that sparse rewards make exploration hard and slow learning (Bellemare et al., 2016). Plateaus during learning, which arise from saddle points in the loss landscape, have also been studied for (deep) neural networks in the supervised setting (Saad and Solla, 1995; Dauphin et al., 2014), but do not arise in the supervised perceptron. Hence the RL setting can qualitatively change the learning trajectory. The benefit of withholding penalties is that while slower, the perceptron reaches the highest level of expected reward in this case. This is a first example of a speed-accuracy trade-off that we will explore in more detail in section 2.5 and that we also found in our experiments with Bossfight in section 3.

**Penalty:** The initial plateau can be reduced by providing a penalty or negative reward (\(_{2}>0\)) when the student fails in the task. This change provides weight updates much earlier in training and thus accelerates the escape from the plateau. The dynamics under this protocol are shown in fig. 2a. It is clear the penalty provides an initial speed-up in learning, as expected if the agent were to be unaligned and more likely to commit an error. However, a high penalty can create additional sub-optimal fixed points in the dynamics leading to a low asymptotic performance (more on this in section 2.4). In the simulations, finite size effects occasionally permit escape from the sub-optimal fixed point and jumps to the optimal one, leading to a high variance in the results.

**Subtask and breadcrumbs:** The model is also able to capture the dynamics of more complicated protocols: fig. 3b shows learning under the protocol where a smaller sub-reward is received if the agent survives beyond a shorter duration \(T_{0}<T\), i.e. some reward is still received even if the agent does not survive for the entire episode. Another learning protocol we can capture is that of 'graded-breadcrumbs', where the agent receives a small reward \(r_{b}\) for every correct decision made in an episode, i.e. like the previous method some reward is still received even if the agent does not survive for the entire episode, these dynamics are captured in fig. 2(c).

### Optimal hyper-parameter schedules: make episodes longer and anneal your learning rate

Hyper-parameter schedules are crucial for successful training of RL agents. In our setup, the two most important hyper-parameters are the learning rates and the episode length. In the RL perceptron, we can derive optimal schedules for both hyper-parameters. For simplicity, here we report the results in the spherical case, where the length of the student vector is fixed at \(\) (we discuss the unconstrained case in the appendix C), then \(Q()=1\) at all times and we only need to track the teacher-student overlap \(=R/\), which quantifies the generalisation performance of the agent. Keeping the choice \(()=_{t=1}^{T}(y_{t}y_{t}^{*})\) and turning off the penalty term (\(_{2}=0\)), we find that the teacher-student overlap is governed by the equation

\[=}(1-^{2})(1-cos^{-1}())^{T-1}-}{2TQ}(1- cos^{-1}())^{T}\] (7)

The optimal schedules over episodes for \(T\) and \(\) can then be found by maximising the change in overlap at each update, i.e. setting \(()\) and \(()\) to zero respectively. After some calculations, we find the optimal schedules to be

\[T_{}=}{2}) }[1+}{})}{ {}P(P)}}]_{}=})}{ P}\] (8)

where \(\) indicates the floor function.

Figure 2(a) shows the evolution of \(\) under the optimal episode length schedule (dashed) compared to other constant episode lengths (green). Similarly, fig. 2(b) shows the evolution of \(\) under the optimal learning rate schedule (dashed) compared to other constant learning rates (blue). The functional forms of \(T_{}\) and \(_{}\) over time are shown in fig. 2(c).

During learning the student seeks increasingly refined information to improve its expected reward. This simple observation explains the monotonic increase of the optimal episode length and the decrease in learning rates. Starting from the episode duration, we can observe that given the discrete nature of the decisions, information obtained from the rewards simply pushes the decision boundary towards a partition of the input space. This partition is determined by the episode length \(T\) and correspond to a fraction \(1/2^{T}\) of the entire input space. Therefore a positive reward conveys \(T\) bits of information. At a fixed learning rate, when the student becomes proficient in the task it will not be able to improve further the decision boundary, and will fluctuate around the optimal solution unless longer episodes are provided.

Figure 3: **Optimal schedules for episode length \(T\) and learning rate \(\). _(a)_ Evolution of the normalised overlap under optimal episode length scheduling (dashed) and various constant episode lengths (green). _(b)_ Evolution of the normalised overlap under optimal learning rate scheduling (dashed) and various constant learning rates (blue). _(c)_ Evolution of optimal \(T\) (green) and \(\) (blue) over learning. _Parameters: \(D=900\), \(Q=1\), \(_{2}=0\), _(a)_\(=1\), _(b)_\(T=8\).

Our analysis shows that a polynomial increase in the episode length gives the optimal performance in the RL perceptron, see fig. 3c (top); increasing \(T\) in the RL perceptron is akin to increasing task difficulty, and the polynomial scheduling of \(T_{}\) specifies a curriculum. Curricula of increasing task difficulty are commonly used in RL to give convergence speed-ups and learn problems that otherwise would be too difficult to learn _ab initio_Narvekar et al. (2020). Analogously, the fluctuations can be reduced by annealing the learning rate and averaging over a larger number of samples. Akin to work in RL literature studying adaptive step-sizes (Dabney, 2014; Pirotta et al., 2013), we find that annealing the learning rate during training is beneficial for greater speed and generalisation performance. For the RL perceptron, a polynomial decay in the learning rate gives optimal performance as shown in fig. 3c (bottom), consistent with work in the parallel area of high-dimensional non-convex optimization problems (d'Ascoli et al., 2022), and stochastic approximation algorithms in RL (Dalal et al., 2017).

### Phase Space

With a non-zero penalty (\(_{2}\)), the generalisation performance of the agent can enter different regimes of learning. This is most clearly exemplified in the spherical case, where the number of fixed points of the ODE governing the dynamics of the overlap exist in distinct phases determined by the combination of reward and penalty. For the simplest case \((()=_{t}^{T}(y_{t}y_{t}^{*}))\) these phases are shown in fig. 4. Figure 4a shows the fixed points achievable over a range of penalties for a fixed \(_{1}=1\) (obtained from a numerical solution of the ODE in \(\)). There are two distinct regions: 1) _Easy_, where there is a unique fixed point and the algorithm naturally converges to this optimal \(_{}\) from a random initialisation, 2) a _Hybrid-hard_ region (given the analogy with results from inference problems Ricci-Tersenghi et al. (2019)), where there are two stable (1 good and 1 bad) fixed points, and 1 unstable fixed point, and either stable point is achievable depending on the initialisation of the student (orange). The 'hybrid-hard' region separates two easy regions with very distinct performance levels. In this region the algorithm with high probability converges to \(_{}\) with the worse performance level. These two regions are visualised in (\(_{1},_{2}\)) space in fig. 4b for an episode length of \(T=13\). The topology of these regions are also governed by episode length, with a sufficiently small T reducing the the area of the 'hybrid-hard' phase to zero, meaning there is always 1 stable fixed point which may not necessarily give 'good' generalisation. Figure 4c shows the phase plot for \(T=8\), where the orange (hybrid-hard) has shrunk, this corresponds to the s-shaped curve in fig. 4a becoming flatter (closer to monotonic). Learning with \(_{2}\) This is not a peculiarity specific to the spherical case, indeed, we observe different regimes in the learning dynamics in the setting with unrestricted \(Q\) which we report in appendix C.

These phases show that at a fixed \(_{1}\) increasing \(_{2}\) will eventually lead to a first order phase transition, and the speed benefits gained from a non-zero \(_{2}\) will be nullified due to the transition into the hybrid-hard phase. In fact, when taking \(_{2}\) close to the transition point, instead of speeding up learning there is the presence of a critical slowing down, which we report in appendix C.

Figure 4: **Phase plots characterising learnability**. In the case where all decisions in an episode of length \(T\) must be correct in order to receive a reward. _(a)_ the fixed points of \(\) for \(T=13\) and \(_{1}=1\), the dashed portion of the line denotes where the fixed points are unstable. _(b)_ Phase plot showing regions of hardness for \(T=13\). _(c)_ Phase plot showing regions or hardness for \(T=8\). The green regions represent the _Easy_ phase where with probability 1 the algorithm naturally converges to the optimal \(_{}\) from a random initialisation. The orange region indicates the _Hybrid-hard_ phase, where with high probability the algorithm converges to the sub-optimal \(_{}\) from random initilisation. _Parameters_: \(D=900\), \(Q=1\).

A common problem with REINFORCE is high variance gradient estimates leading to bad performance (Marbach and Tsitsiklis, 2003; Schulman et al., 2015). The reward (\(_{1}\)) and punishment (\(_{2}\)) magnitude alters the variance of the updates, and we show that the interplay between reward, penalty and reward-condition and their effect on performance can be probed within our model. This framework opens the possibility for studying phase transitions between learning regimes (Gamarnik et al., 2022).

### Speed-accuracy trade-off

Figure 5c shows the evolution of normalised overlap \(=R/\) between the student and teacher obtained from simulations and from solving the ODEs in the case where \(n\) or more decisions must be correctly made in an episode of length \(T=13\) in order to receive a reward (with \(_{2}=0\)). We observe a speed-accuracy trade-off, where decreasing \(n\) increases the initial speed of learning but leads to worse asymptotic performance; this alleviates the initial plateau in learning seen previously in fig. 5b at the cost of good generalisation. In essence, a lax reward function is probabilistically more achievable early in learning; but it rewards some fraction of incorrect decisions, leading to lower asymptotic accuracy. By contrast a stringent reward function slows learning but eventually produces a highly aligned student. For a given MDP, it is known that arbitrary shaping applied to the reward function will change the optimal policy (reduce asymptotic performance) (Ng et al., 1999). Empirically, reward shaping has been shown to speed up learning and help overcome difficult exploration problems (Gullapalli and Barto, 1992). Reconciling these results with the phenomena observed in our setting is an interesting avenue for future work.

## 3 Experiments

To verify that our theoretical framework captures qualitative features of more general settings, we train agents from pixels on the Procgen (Cobbe et al., 2019) game 'Bossfight' (example frame, fig. 6a (top)). To remain close to our theoretical setting, we consider a modified version of the game where the agent cannot defeat the enemy and wins only if it survives for a given duration \(T\). On each timestep the agent has the binary choice of moving left/right and aims to dodge incoming projectiles. We give the agent \(h\) lives, where the agent loses a life if struck by a projectile and continues an episode if it has lives remaining. This reward structure reflects the sparse reward setup from our theory and is analogous to requiring \(n\) out of \(T\) decisions to be correct within an episode. We further add asteroids at the left and right boundaries of the playing field which destroy the agent on contact, such that the agent cannot hide in the corners. Observations, shown in fig. 6a (bottom), are centred on the agent and downsampled to size \(35 64\) with three colour channels, yielding a \(6720\) dimensional input. The pixels corresponding to the agent are set to zero since these otherwise act as near-constant bias inputs not present in our model. The agent is endowed with a shallow policy network with

Figure 5: **Speed-accuracy tradeoff**. Evolution of _(a)_ the expected reward and _(b)_ corresponding normalised overlap for simulation (solid) and ODE solution (dashed) over a range of \(T\) when all decisions in an episode of length \(T\) are required correct, and \(_{2}=0\). _(c)_ Evolution of the normalised overlap between student and teacher weights for simulation (solid) and ODE solution (dashed) for the case where \(n\) or more decisions in an episode of length 13 are required correct for an update with \(_{2}=0\). More stringent reward conditions slow learning but can improve performance. _Parameters:_\(D=900\), \(_{1}=1\), \(_{2}=0\).

logistic output unit that indicates the probability of left or right action. The weights of the policy network are trained using the policy gradient update of eq. (1) under a pure random policy.

To study the speed-accuracy trade-off, we train agents with different numbers of lives. As seen in fig. 6b, we observe a clear speed-accuracy trade-off mediated by agent health consistent with our theoretical findings (c.f. fig. 3c). Figure 6c shows the final policy weights for agents trained with \(h=1\) and \(h=4\). These show interpretable structure, roughly split into thirds vertically: the weights in the top third detect the position of the boss and centre the agent beneath it; this causes projectiles to arrive vertically rather than obliquely, making them easier to dodge. The weights in the middle third dodge projectiles. Finally, the weights in the bottom third avoid asteroids near the agent. Notably, the agent trained in the more stringent reward condition (\(h=1\)) places greater weight on dodging projectiles, showing the qualitative impact of reward on learned policy. Hence similar qualitative phenomena as in our theoretical model can arise in more general settings.

## 4 Concluding perspectives

The RL perceptron provides a framework to investigate high-dimensional policy gradient learning in RL for a range of plausible sparse reward structures. We derive closed ODEs that capture the _average-case_ learning dynamics in high-dimensional settings. The reduction of the high-dimensional learning dynamics to a low-dimensional set of differential equations permits a precise, quantitative analysis of learning behaviours: computing optimal hyper-parameter schedules, or tracing out phase diagrams of learnability. Our framework offers a starting point to explore additional settings that are closer to many real-world RL scenarios, such as those with conditional next states. Furthermore, the RL perceptron offers a means to study common training practices, including curricula; and more advanced algorithms, like actor-critic methods. We hope to extract more analytical insights from the ODEs, particularly on how initialization and learning rate influence an agent's learning regime. Our findings emphasize the intricate interplay of task, reward, architecture, and algorithm in modern RL systems.