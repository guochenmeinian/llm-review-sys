# QueST: Self-Supervised Skill Abstractions for

Learning Continuous Control

 Atharva Mete1, Haotian Xue1, Albert Wilcox1, Yongxin Chen1,2, Animesh Garg1,2

###### Abstract

Generalization capabilities, or rather a lack thereof, is one of the most important unsolved problems in the field of robot learning, and while several large scale efforts have set out to tackle this problem, unsolved it remains. In this paper, we hypothesize that learning temporal action abstractions using latent variable models (LVMs), which learn to map data to a compressed latent space and back, is a promising direction towards low-level skills that can readily be used for new tasks. Although several works have attempted to show this, they have generally been limited by architectures that do not faithfully capture sharable representations. To address this we present Quantized Skill Transformer (QueST), which learns a larger and more flexible latent encoding that is more capable of modeling the breadth of low-level skills necessary for a variety of tasks. To make use of this extra flexibility, QueST imparts causal inductive bias from the action sequence data into the latent space, leading to more semantically useful and transferable representations. We compare to state-of-the-art imitation learning and LVM baselines and see that QueST's architecture leads to strong performance on several multitask and few-shot learning benchmarks. Further results and videos are available at https://quest-model.github.io.

## 1 Introduction

One of the grand goals of robotic learning is a general-purpose model that can learn from complex multitask demonstration data and generalize to new tasks in a zero-shot or few-shot manner. While such general-purpose models have become ubiquitous in natural language (NLP)  and computer vision (CV) , they have eluded robotics researchers. Whereas CV and NLP can achieve positive transfer by scaling up models trained on internet scale datasets , even large scale robot data collection efforts  have been insufficient for this approach. To that end, we posit that in order to achieve positive transfer in robotics, it is important to design architectures that specifically lend themselves to efficient cross-task transfer.

There has recently been a surge of work towards the goal of learning generalist policies from large, diverse datasets. Several papers have used techniques such as action discretization  and implicit models  to model multimodal action distributions. In particular, the behavior transformer line of work shows that a carefully discretized action space combined with a GPT-style transformer leads to impressive capabilities modeling multimodal behavior distributions . In another vein, several works have attempted to scale demonstration data and achieve positive transfer of low-level skills between high-level tasks . While these works have shown some transfer, for example applying policies for known tasks to unfamiliar objects, they have generally failed to achieve transfer of low-level skills to novel tasks . Wehypothesize that in the relatively low-data regime of robot learning, it is promising to explicitly force the model to learn sharable representations. To that end we study latent variable models (LVMs), which learn to map data to a compressed latent space and back, introducing an information bottleneck which encourages the model to learn shared representations across the training data. Specifically we consider the application of LVMs to learn low-dimensional representations of action sequences. Such representations are termed temporal action abstractions or motion primitives - in this paper we refer to these abstractions as'skills'.

A wide body of work has considered the application of LVMs to robotics. One line of work learns temporal action abstractions (skills) in continuous latent space with a Gaussian prior [41; 50; 59]. While this line of work showed some initial promise of learning latent plans, it has failed to scale to difficult multitask settings due to the loose nature of the latent structure and posterior collapse issues that inhibit the learning of shared representations. On the other hand, recent work in CV and NLP has shown that vector-quantized discrete latent spaces are capable of learning semantically meaningful representations from data like phonetics in speech [9; 6] or melody in music [17; 2]. This insight, along with prior work showing that discretized action spaces can help to address the multimodality problem in when learning from large datasets [57; 16; 35; 14], motivates methods learning temporal action abstractions with discrete latent spaces. Several recent works have set out to do this [35; 74; 76; 30], showing some degree of positive transfer between tasks in multi-task and few-shot settings. However, they are generally limited by architectures that do not faithfully capture transferable representations [35; 74], or depend on state prediction and state-based objective functions which are impractical for many real robot tasks [76; 30].

In this paper, we present **Quantized Skill** Transformer (QueST), a simple yet novel architecture for learning generalizable low-level skills within a discrete latent space. The key insight behind QueST is its ability to flexibly capture variable length motion primitives by representing them with a sequence of discrete codebook entries. We achieve this through a unique encoder-decoder architecture primarily designed to impart causal inductive bias in action sequence data into the latent space. Such formulation enables us to employ powerful sequence modeling approaches to plan and composably reason within the space of low-level skills. Through our experiments, we show that autoregressive modeling of these latent skills with a GPT-like transformer outperforms state-of-the-art baselines on challenging robotic manipulation benchmarks, where QueST shows an 8% improvement in multitask and 14% improvement in few-shot imitation learning over the next best baselines. We also conduct a detailed ablation and sensitivity study to validate our key architectural design decisions.

## 2 Related Works

The proposed framework in this paper introduces a methodology for self-supervised skill abstraction, followed by decision-making within this skill space. Several related works have explored similar sub-directions such as decision-making in the latent space and decision making with a transformer:

### Learning from Offline Data

Behavior cloning (BC)  aims to learn a policy by directly mapping observations to actions, and is typically trained end-to-end using pre-collected pairs of observation and behavior data. While this on its surface is a simple supervised learning problem, there are several properties of robot demonstration data that should be considered when building BC systems. First, large BC datasets collected from a variety of human demonstrators tend to contain data sampled from multimodal distributions. To address this, some works opt to sample actions from Gaussian Mixture Models (GMM) , while others explore implicit models including those derived from energy-based models [23; 29] or diffusion models [15; 71; 14; 68; 28]. The Behavior Transformer (BeT) line of work [57; 16; 35] shows that transformer-based categorical policies in carefully discretized action spaces do a good job handling multimodal demonstrator distributions and QueST builds upon this by contributing a more capable discrete latent skill model.

Another key property of robot demonstration data is that sequential actions are often highly correlated with one another, and exploiting this can lead to stronger performance while ignoring it can lead to policies which are susceptible to temporally correlated confounders . Recently several works have set out to handle this by predicting action chunks. For example, the Action Chunking Transformer (ACT) line of work [73; 24] shows that a transformer trained as a CVAE  to output chunks of actions performs well for a wide variety of manipulation tasks, and diffusion policy  shows across the board improvements when predicting action chunks. As discussed in detail in Section 2.3, QueST builds on a long line of work which handles sequential correlations through temporally-extended action abstractions [41; 50; 59; 35; 74; 76; 30].

### Multi-task and Few-shot Imitation Learning

In the past, robot learning researchers have approached multi-task decision making settings using a wide variety of methods such as supervised pre-training and fine-tuning [20; 42], meta-learning [22; 19] and action retrieval [48; 44]. There has recently been a large focus on multi-task language-conditioned imitation learning for robotics with several papers attempting to address the problem by training large models on large demonstration datasets [10; 11; 47; 46; 18; 12; 1]. While these papers achieve impressive multitask results, they mostly rely on sufficient data coverage and fail to generalize beyond their training distribution . Thus, they lack abstractions that can readily be applied to learn new tasks, especially in a low-data regime. On the other hand LVMs like QueST are designed to learn sharable representations that can be applied to new tasks.

### Decision Making in Learned Latent Spaces

LVMs, modeled by a paired encoder-decoder, have found extensive applications in computer vision [65; 32; 8] and generative models [56; 21; 13]. Recent studies also demonstrate the utility of latent space representations in robot decision-making, spanning offline RL [58; 50; 40], imitation learning [14; 67; 37; 35], and temporal action abstraction [57; 74; 75]. Most similar to our work are those that learn temporally abstracted discrete latent skill spaces. PRISE  learns single-step state-action abstractions within a discrete space and then does temporal abstraction by applying BPE tokenization. While this method shows promise in learning multi-task and few-shot policies, BPE is well suited for text and is known to suffer in domains with highly dynamic vocabularies, in robotics its equivalent to varying action distributions across unseen tasks. TAP  and H-GAP  utilize a self-supervised auto-encoder to learn skill codes, but their functionality relies on Model Predictive Control (MPC) using state prediction with ground-truth state-conditioned objective functions, which make it difficult to apply to real-world manipulation tasks. VQ-BeT  bears the strongest resemblance to QueST, also using a pre-trained discrete latent skill space to discretize the action space for a transformer-based policy prior. However, their quantization approach does not leverage the inherent structure in action sequences, limiting the representational capabilities of the latent space. Thus it's shown to heavily rely on an continuous offset predictor for best performance. Unlike these works, QueST's learned latent space is highly flexible yet structured and expressive, allowing it to effectively model action distribution across many distinct tasks in a meaningful shared representation.

## 3 Preliminary

### Problem Setting

We consider a dataset \(D=\{\{(O_{1},a_{1}),,(O_{T_{i}},a_{T_{i}})\}_{i=0}^{N_{k}},L_{k}\}_{k=0}^ {M}\) consisting of \(M\) robot interaction trajectories where \(a_{t}\) is a continuous-valued action and \(O_{t}\) is a tuple consisting of a high-dimensional sensory observation. The data is collected via either human teleoperation or scripted policies for M different task each with a label \(L_{k}\). In our setting, \(O_{t}\) consist of RGB image observations from the front camera and gripper camera (if available) along with proprioceptive state of the agent. \(L_{k}\) is a natural language description of the \(k^{th}\) task but can also be a one-hot encoding.

### Finite Scalar Quantization

We build on Finite Scalar Quantization (FSQ)  as a discrete bottleneck in our model. It's a drop-in replacement for Vector Quantization (VQ) layers in VAEs with a simple scalar quantization scheme. Here the input representation \(e\) is projected to very few dimensions (typically 3 to 5) which are then bounded and rounded, creating an implicit codebook.

\[z=(f(e)),f\] (1)

Given a feature vector \(e^{d}\) to quantize, instead of learning a parameterized codebook  and quantizing \(e\) by matching the nearest neighbor in the codebook, FSQ quantizes \(e\) by first bounding it into certain range with \(f\) (e.g. \(f=/2(e)\)) and then rounding each dimension into integer numbers directly with straight-through gradients (round_ste). \(^{d}\) defines the width of the codebook for each dimension (e.g. \(=,d=4\)). Finally, it is easy to see that the size of the quantization space is \(_{i=1}^{d}_{i}\). An MLP can be used to transform \(z\) into continuous space of required dimension further.

A common problem with vector-quantized codebooks (VQ)  is the under-utilization of the codebook. Recent works have attempted to address by heuristics like reinitializing the codebook, stochastic formulations, or some regularization [34; 69]. In contrast, FSQ achieves much better codebook utilization for large codebook sizes with much fewer parameters and simplified training without any auxiliary losses or aforementioned tricks. Due to its simplicity and proven benefits, we use FSQ in our main experiments, but since many prior works in this space use VQ we also perform an ablation with it (see section 5.6).

## 4 Method

In this section, we describe the key ideas behind Quantized Skill Transformer. In Section 4.1 we present our encoder-decoder architecture, which is designed to provide the flexibility to learn a wide range of skills with inductive biases to ensure that the learned skills are useful. In Section 4.2, we detail our skill prior, which we train to autoregressively predict codebook skills. Our full pipeline is shown in Figure 1.

### Stage I: Learning the Skill Codebook

As a motivating example, consider the task of lifting a pot and placing it on a stove beside. This consist of primitives like reaching the pot, grasping it, lifting it to a certain height, reaching the stove and finally placing it on the stove. Each of these primitives are of variable lengths, and to properly model these skills it is important to learn a latent skill space with the flexibility to model all of them. At the same time, it is important that the learned skills are semantically meaningful so that they can be reused for new tasks, for example reusing the reaching skill for an object lifting task. In order to address these desiderata, we introduce the novel autoencoder architecture shown in Figure 1 consisting of an encoder \(_{}\) and decoder \(_{}\).

The input to the encoder \(_{}\) is an action sequence \(a_{t:t+T-1}\) sampled from the dataset, which we pass through several 1D causal strided-convolution layers . This step reduces the sequence length to achieve the desired temporal abstraction depending on the stride lengths and the number of layers.

Figure 1: **Overview of Quantized Skill Transformer**: we factorize the policy that outputs action based on task descriptions \(e\) and observations \(o\) encoding into two parts: \((A|o,e)=_{}(A|Z)_{}(Z|o,e)\), where \(Z\) is a sequence of skill tokens for the action sequence \(A\). In Stage I, we learn skill abstraction in a self-supervised way with a quantized autoencoder. In Stage II, we learn skill-based policy in the style of next-token prediction using a multi-modal transformer.

We follow the convolutional layers with masked self-attention layers for sequence modeling. With a downsampling factor of \(F\), the encoder outputs in total \(n=T/F\) embeddings. The embeddings are then quantized using FSQ as per the equation 1 into \(n\) discrete latent codes \(\{z^{i}\}\) termed as skill tokens:

\[(z^{1},,z^{n})=(_{}(a_{t},,a_{t+T-1})).\] (2)

Having an input sequence of actions mapped to multiple skill tokens gives this architecture more flexibility to model complex sequences of actions. At the same time, each component of the encoder is causal, meaning that an output representation at a position \(t\) cannot depend on input from any future timesteps. We found this inductive bias to encourage the model to learn semantically useful action representations by modeling the inherent causality in the action data. We validate this design choice in the ablations. (see section 5.6)

Typical autoencoder decoders are simply mirrored versions of the encoders, but this would prevent the decoder from attending to all quantized codes. This is important because individual codes do not represent anything meaningful but a sequence of codes represents a particular meaningful motion . In order to maintain causality while attending to all codes, the decoder \(_{}\) cross attends between fixed sinusoidal positional embedding inputs and the skill tokens, similarly with . The architecture is a transformer decoder block consisting of alternate masked self-attention and cross-attention layers, after which the output embeddings are projected back to the original action dimension using an MLP layer. Thus, given a sequence \(Z\) of skill codes, \(_{}\) reconstructs the original action

\[(_{t},,_{t+T-1})=_{}(z^{1},,z^{n})\] (3)

As in , the autoencoder is trained by minimizing the \(_{1}\) reconstruction loss:

\[_{}()=\|_{}((_{ }(a_{t:t+T-1})))-a_{t:t+T-1}\|_{1}.\] (4)

Unlike prior work which often conditions on the state as well as the actions [30; 5; 74; 37], we choose to learn state-independent abstractions that solely capture motion primitives irrespective of the current scene or task. Through our experiments we show that our model learns generalizable abstractions that are shared and can be transferred across tasks.

### Stage II: Learning the Skill Prior

After training the encoder \(_{}\) and decoder \(_{}\), we train a skill prior \(_{}(Z|e,o)\) to predict skills \(Z=z^{1:n}\) corresponding to the demonstrator action distribution conditioned on a task embedding \(e\) and a length \(h\) sequence of image observations and proprioception inputs, \(o=(i_{t-h},p_{t-h}),,(i_{t},p_{t})\). We encode image observations with a separate learned vision encoder for each camera view and encode proprioception using an MLP encoder, all of which are trained end-to-end with the rest of the skill prior. The observation token \(^{o}_{t}\) for a timestep \(t\) is obtained by concatenating outputs from all the aforementioned encoders. Task embeddings are designed specifically for each environment suite, as discussed in more detail in Section 5. See Appendix B for more details about the encoders.

Because skill tokens are highly dependent on one another according to the complex nonlinear representations learned by the autoencoder, it is important that the skill prior has the modeling capacity to reason about these dependencies. To achieve this, we employ a decoder-only transformer to model the distribution of skill tokens \(_{}(Z|^{o}_{t-h:t},e)\) autoregressively as:

\[_{}(Z|^{o}_{t-h:t},e)=_{i=1}^{n}_{}(z^{i}| ,z^{1:i-1},^{o}_{t-h:t},e)\] (5)

where \(\) is a learnable start token that marks the start of skill tokens. We add sinusoidal positional embeddings only to the skill tokens. To optimize the skill prior, we sample a sequence of demonstrator actions \(a_{t:t+T-1}\) and use the trained encoder \(_{}\) to extract a latent skill vector \(Z_{t}=z^{1:n}\) according to Equation 2. Then, we optimize \(_{}\) using the following negative log-likelihood loss:

\[_{}()=-_{}(Z_{t}|^{o}_{ t-h:t},e).\] (6)

The full skill prior pipeline is shown in Figure 1.

Few-Shot Finetuning:For few-shot finetuning on new tasks, we use a model pre-trained on large set of tasks and finetune it on a small number of demonstrations (5 in our experiments) from the held-out task. Although finetuning only stage-2 is enough, we empirically found that finetuning the decoder on the predicted skill tokens gives a boost in the performance. Specifically, we finetune the decoder using following decoder loss:

\[_{}()=\|_{}((_{t}))- a_{t:t+T-1}\|_{1}\] (7)

where sg is the stop gradient operator. We present the results with and without decoder finetuning both. Additionally, we note that the encoder is still frozen in this setting.

### Inference with Quantized Skill Transformer

At inference time, QueST uses the skill prior \(_{}\) alongside the decoder \(_{}\) to sample actions. Conditioned on the encoded observation history \(^{o}_{t-h:t}\) and task embedding \(e\), we use top-\(k\) sampling with a temperature of \(\) to autoregressively sample a skill vector \(_{}(|^{o}_{t-h:t},e)\) from the skill prior. In practice, we find \(k=5\) and \(=1\) to work well across all environments. Then, we use the decoder to map the skill vector back to the action space, producing a sequence of predicted actions \(_{t:t+T-1}=_{}()\). In a receding horizon fashion, we execute the first \(T_{a} T\) actions before replanning.

## 5 Experiments

We design the experiments to empirically evaluate the performance of Quantized Skill Transformer in three practical settings: (1) Multitask IL, (2) Few-shot transfer, and (3) Long-horizon IL. Lastly, we perform some ablations to empirically justify our model design choices.

### Benchmarks and Baselines

We use the following benchmark suites to evaluate in the settings discussed above:

**LIBERO ** is a lifelong learning benchmark featuring several task suites consisting of a variety of language-labeled rigid- and articulated-body manipulation tasks. Specifically, we evaluate on the LIBERO-90 suite, which consists of 90 manipulation tasks, and the LIBERO-LONG suite, which consists of 10 long-horizon tasks composed of two tasks from the LIBERO-90 suite. As described in more detail below, we use the LIBERO benchmark to study the multitask IL, few-shot transfer and long-horizon IL settings. Because tasks from this benchmark are language-annotated, we use the output of a frozen CLIP  encoder for the task conditioning input \(e\).

**MetaWorld ** features a wide range of manipulation tasks designed to test few-shot learning algorithms. We use the Meta-Learning 45 (ML45) suite which consists of 45 training tasks and 5 difficult held-out tasks which are structurally similar to the training tasks. We use this benchmark to test multi-task and few-shot learning. Because this benchmark does not include language labels, we use learned task embeddings \(e\) for task conditioning.

Baselines:We compare to the following baselines, which include similar discrete LVM pipelines as well as state of the art imitation learning algorithms:

1. The **ResNet-T** model from , which encodes observation and task instructions using ResNet-18 with FiLM , applies a transformer sequence model, and uses a GMM head to predict actions.
2. The UNet-based **diffusion policy** from , which uses a 1D convolutional UNet to map samples from a Gaussian prior to action samples from the demonstrator distribution according to a learned denoising process.
3. **ACT**, which trains a transformer as a CVAE  to predict action chunks.
4. **VQ-BeT**, which learns a discrete latent space using a VQ-VAE  and uses a transformer to predict discrete latent codes.
5. **PRISE**, which first quantizes observation-action pairs and performs temporal abstraction using byte pair encoding (BPE) to learn a skill token vocabulary, which it uses as an action space for few-shot learning.

For a detailed discussion between QueST and the baseline methods, please refer to Appendix B.5.

### Performance on Multitask BC

We evaluate the goal-conditioned multi-task imitation learning capabilities of QueST and the baselines using the LIBERO-90 and ML45 benchmark suites. For LIBERO-90, the learner receives 50 expert demonstrations per task from the author-provided dataset. For ML45, we use the scripted policies provided in the official Metaworld codebase to collect 100 demonstrations per task. We evaluate the model at the end of training and for each task run 40 evaluation rollouts (50 for MetaWorld) starting from the initial states selected sequentially from a predefined set. We report the aggregated results across 4 seeds (5 seeds for MetaWorld).

In Figure 1(a), we present the average success rate across 90 tasks in LIBERO-90 against the aforementioned baselines. Quantized Skill Transformer achieves state-of-the-art results on LIBERO-90 benchmark, outperforming the baseline VQ-BeT and Diffusion Policy by a margin of 8 and 13% respectively. We attribute its performance to its learned latent space that enables effective knowledge sharing across tasks. While VQ-BeT also shows strong performance, we see that QueST's architecture lends itself better to sharing representations across tasks. Our implementation of ResNet-T achieves significantly better performance than the reported number (16.8%) in  but is still lower than QueST. Figure 2(a) shows the average success rate across 45 tasks in ML45 benchmark. Being a simpler benchmark, all methods perform almost similar in Multitask-IL setting which is consistent with the trend observed in .

We attribute the reasonably good performance of the diffusion policy to its nature as a latent variable model, which employs a continuous latent variable with the same dimensionality as the actions. However, the consistent outperformance of QueST over the diffusion policy provides compelling evidence for the benefits of using a bottlenecked latent variable. This bottleneck encourages the model to learn shared representations, resulting in enhanced performance. While both VQ-BeT and PRISE employ a latent bottleneck, VQ-BeT's architecture neglects the inherent inductive biases in the action data, which we believe results in a less well-structured latent space. PRISE incorporates this using a latent forward transition model, but is bottlenecked by the use of BPE which we posit is not suitable for such a dynamic latent space.

### Few-shot Transfer to Unseen Tasks

In this setting we take the pretrained model from section 5.2 and test its 5-shot performance on unseen tasks from LIBERO-LONG and held-out set in ML45. We sample only five demonstrations for each task, generate the skill tokens using pretrained encoder and use them to finetune the skill prior and the decoder as described in Section 4.2. We also present the results without finetuning the decoder (frozen \(_{}\) in figure 1(b) & 2(b)) to validate its generalization to unseen skill tokens sequences.

Figure 1(b) shows the average success rate for 5-shot IL across 8 unseen tasks in LIBERO-LONG. QueST achieves SOTA performance, surpassing all other baselines by an absolute margin of \(14\)%.

Figure 2: Multitask performance on LIBERO-90 (a) and LIBERO-LONG (c), and few shot performance on LIBERO-LONG (b). For (a) and (c) we train on the datasets described in Sections 5.2 and 5.4. For (b) we finetune the model from (a) on a condensed dataset as described in Section 5.3. Results show the mean and error bar represents standard error across four random seeds for multitask experiments and nine random seeds for fewshot experiments. Results for PRISE are taken from Zheng et al. , and the others we reimplemented and ran ourselves.

Though we see a marginal drop of \(1.5\)% without decoder finetuning, it still outperforms all the baselines. These results highlight the superiority of QueST in learning transferable representations of action abstractions and effectively leveraging them for downstream decision making. For a fair comparison, we also tried fine-tuning the decoder of VQ-BeT but did not observe any gains from it. VQ-BeT struggles in this setting as it heavily relies on offset head to output continuous action corrections which requires more data-samples for sufficient coverage in the continuous action space. Figure 2(b) shows the average success rate for 5-shot IL across 5 unseen tasks in MetaWorld. Similar to multitask results, all methods perform comparably, with QueST showing a slight improvement over the others. QueST leverages its learned skill tokens to compositionally model their distribution for an unseen task in just 5 demonstration examples. For few-shot evaluation protocol please refer Appendix D.1.

### Long-horizon BC

In this setting, we aim to purely study and compare the performance of our model on long-horizon tasks. We train the model (both stages) solely on LIBERO-LONG complete dataset (50 demonstrations per task) and evaluate with the same scheme as described earlier.

Figure 1(c) shows the average success rate across 10 LIBERO-LONG tasks. All LVMs perform significantly worse than the ResNet-T model. We attribute this to the relatively small size of LIBERO-LONG dataset which is just not enough to learn a good latent space in LVMs. QueST still outperforms all LVM baselines by a large margin demonstrating its long-horizon modeling capabilities.

Overall, we see that our model outperforms baselines like VQ-BeT in multitask settings, showing stronger modelling capacity. At the same time, it has the correct latent structure to outperform baselines like diffusion in few shot settings, especially even with frozen decoder, indicating strong generalization capabilities of learned skill-space.

### Latency

Our pipeline runs at 33Hz, which is more than suffcient for vision-based real-robot control where most camera systems run at 30 fps. For comparison, our implementation of Resnet-T, VQ-BeT, ACT and Diffusion Policy run at 100Hz, 100Hz, 50Hz, and 12Hz respectively.

  & VQ & Obs. Cond. & Mirror Dec. & Ours \\  LIBERO-90 & \(81.2 0.6\) & \(81.9 1.1\) & \(86.3 0.9\) & \(\) \\ Few Shot & \(62.5 2.0\) & \(61.3 2.2\) & \(45.4 2.0\) & \(\) \\ 

Table 1: Success rates after ablating design details of QueST. We present the mean across four random seeds and error tolerances show the standard error.

  & Non Causal \(_{}\) & Non Causal \(_{}\) & Fully Non Causal & Ours \\  LIBERO-90 & \(82.0 1.6\) & \(85.1 1.8\) & \(78.5 0.5\) & \(\) \\ Few Shot & \(58.8 3.0\) & \(61.6 2.5\) & \(56.1 1.8\) & \(\) \\ 

Table 2: Success rates after ablating the causality in QueST. We present the mean across four random seeds and error tolerances show the standard error.

Figure 3: Multitask and few-shot success rate on the Metaworld ML45 task suite. In (a) we train on the dataset described in Section 5.2, and in (b) we finetune the model from (a) using 5 demonstrations each from a set of held out tasks. Results show the mean and error bar represents standard error across five random seeds. Results for PRISE are taken from Zheng et al. , and the others we reimplemented and ran ourselves.

### Ablations

We validate the proposed architecture by ablating some of its key design decisions. All the ablations are performed on LIBERO studying their effects on both multitask and fewshot IL settings.

1. **Vector Quantization**: We replace the FSQ layer with a Vector Quantization layer of nearly the same codebook size, shown in the _VQ_ column of Table 1. We see that FSQ's superior codebook utilization leads to an improvement in performance.
2. **Observation Conditioned Decoder:** Many prior condition the action decoder with current observation [76; 30]. We experiment with this by appending observation tokens to the skill tokens and allowing the transformer decoder to jointly cross-attend to both, shown in Table 1. We see that conditioning on observations leads to a deterioration in performance.
3. **Mirrored Decoder:** Following a typical autoencoder design, we use a decoder that mirrors the encoder, using transposed convolutions instead of strided convolutions, and with the strides in reverse order as in the encoder. This decoder directly takes skill-token embeddings as input and outputs the continuous actions, and results are shown in Table 1. We see that this method performs worse, suggesting attending to all quantized codes in \(z\), as our decoder does, is important for faithfully predicting actions.
4. **Causality:** We ablate the use of causal layers in various parts of our network in Table 2. We see that removing causality from any part of our architecture leads to worse performance, suggesting that causal masking imparts the model with a helpful inductive bias for modeling robot action data.

We also perform a sensitivity experiment over several hyperparameters including downsampling factor and codebook size in Figure 4. Across the board we see that the hyperparameters are more important in the difficult few-shot learning setting. In Figure 3(a) we see that both algorithms have the best performance with a modest downsampling factor of \(F=4\), and in Figure 3(b) we see that QueST does well with a 1024 codebook vectors. For more discussion on ablations please refer Appendix C.

### Latent Skill-Space Analysis

We present a t-SNE visualization (Figure 5) illustrating the learned skill-space across multiple set of similar tasks. We consider four different combinations of similar tasks to effectively examine the z-embeddings corresponding to their trajectories. Each data point in the plot represents a vector of \(n\) z-embeddings at a specific timestep throughout the entire episode, with decreasing transparency indicating temporal progression. We show that the QueST encoder learns a semantically meaningful skill-space that encodes shared representations of similar motion primitives across different tasks. This analysis includes the first 11 tasks from LIBERO-90. For better comprehension, we encourage readers to review the corresponding rollouts on the website. Notably, the skill-space learning happens in the first stage training which does not make use of any task labels.

## 6 Conclusion

We present Quantized Skill Transformer, a novel LVM architecture for learning sharable skills in a discrete latent space. The key idea behind QueST is to represent action sequences as a series

Figure 4: We conduct a sensitivity experiment across downsampling factors (a) and codebook sizes (b) on the LIBERO benchmark. For (a) we fix a sequence length of \(T=32\). Overall, we see that the few-shot version is more sensitive to hyperparameters and that \(F=4\) with 1024 codebook vectors are good choices.

of codebook vectors, and we demonstrate that using causal convolutions and masked transformers provides an inductive bias that encourages the model to learn useful shared representations. We evaluate QueST across 145 robot manipulation tasks, and show that it outperforms several state-of-the-art baselines in multitask and few-shot learning settings. Our results highlight the usefulness of QueST's encoder (decoder) as semantically-sound, task-agnostic tokenizer (detokenizer) for continuous actions, and its potential to leverage Large Multi-modal Language Models in stage-2.

Limitations:While the benchmarks we consider encompass a wide variety of tasks, the held-out tasks are still structurally similar to the pretraining set, which makes few-shot transfer feasible. In scenarios with a more diverse task, current model may struggle to solve new tasks solely within the learned skill space. A promising direction is to train stage-1 on larger datasets, such as Open X-Embodiment , with an expanded codebook that could capture more diverse motion primitives. Additionally, our current architecture only accounts for causality. Future work should explore other inductive biases, like geometric invariance and dynamic consistency, to enhance abstraction learning.

A statement on societal impact:This paper works towards the broader goal of automating a wide range of manipulation tasks. While this can have positive impacts, such as helping people with mobility impairments or performing menial tasks humans would rather not do, it can also have negative impacts such as automating peoples' jobs away and further concentrating wealth in the hands of a handful of companies. It is important that we in the machine learning community advocate for equitable use of the technology we develop.

Figure 5: t-SNE visualization of skill-token embeddings. Here, the transparency decreases as the episode progresses. The overall patterns clearly shows how similar motion primitives like approaching, picking and placing from different tasks are aligned with one another.