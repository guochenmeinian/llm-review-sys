# Light Unbalanced Optimal Transport

Milena Gazdieva

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

milena.gazdieva@skoltech.ru &Arip Asadulaev

ITMO University

Artificial Intelligence Research Institute

Moscow, Russia

asadulaev@airi.net &Evgeny Burnaev

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

e.burnaev@skoltech.ru &Alexander Korotin

Skolkovo Institute of Science and Technology

Artificial Intelligence Research Institute

Moscow, Russia

a.korotin@skoltech.ru

###### Abstract

While the continuous Entropic Optimal Transport (EOT) field has been actively developing in recent years, it became evident that the classic EOT problem is prone to different issues like the sensitivity to outliers and imbalance of classes in the source and target measures. This fact inspired the development of solvers that deal with the _unbalanced_ EOT (UEOT) problem \(-\) the generalization of EOT allowing for mitigating the mentioned issues by relaxing the marginal constraints. Surprisingly, it turns out that the existing solvers are either based on heuristic principles or heavy-weighted with complex optimization objectives involving several neural networks. We address this challenge and propose a novel theoretically-justified, lightweight, unbalanced EOT solver. Our advancement consists of developing a novel view on the optimization of the UEOT problem yielding tractable and a non-minimax optimization objective. We show that combined with a light parametrization recently proposed in the field our objective leads to a fast, simple, and effective solver which allows solving the continuous UEOT problem in minutes on CPU. We prove that our solver provides a universal approximation of UEOT solutions and obtain its generalization bounds. We give illustrative examples of the solver's performance. The code is publicly available at

https://github.com/milenagazdieva/LightUnbalancedOptimalTransport

## 1 Introduction

The computational _optimal transport_ (OT) has proven to be a powerful tool for solving various popular tasks, e.g., image-to-image translation , image generation  and biological data transfer . Historically, the majority of early works in the field were built upon solving the OT problem between discrete probability measures . Only recently the advances in the field of generative models have led to explosive interest from the ML community in developing the **continuous** OT solvers, see  for a survey. The setup of this problem assumes that the learner needs to estimate the OT plan between continuous measures given only empirical samples of data from them. Due to convexity-related issues of OT problem , many works consider the EOT problem, i.e., use _entropy_ regularizers which guarantee, e.g., the uniqueness of learned plans.

Meanwhile, researches attract attention to other shortcomings of the classic OT problem. It enforces hard constraints on the marginal measures and, thus, does not allow for mass variations. As a result, OT shows high sensitivity to an imbalance of classes and outliers in the source and targetmeasures  which are almost inevitable for large-scale datasets. To overcome these issues, it is common to consider extensions of the OT problem, e.g., unbalanced OT/EOT (UOT/UEOT) [8; 43]. The unbalanced OT/EOT formulations allow for variation of total mass by relaxing the marginal constraints through the use of divergences.

The scope of our paper is the continuous UEOT problem. It seems that in this field, a solver that is fast, light, and theoretically justified has not yet been developed. Indeed, many of the existing solvers follow a kind of heuristical principles and are based on the solutions of discrete OT. For example,  uses a regression to interpolate the discrete solutions, and [16; 36] build a flow matching upon them. Almost all of the other solvers [9; 70] employ several neural networks with many hyper-parameters and require time-consuming optimization procedures. We solve the aforementioned shortcomings by introducing a novel lightweight solver that can play the role of a simple baseline for unbalanced EOT.

**Contributions.** We develop a novel _lightweight_ solver to estimate continuous **unbalanced** EOT couplings between probability measures (SS4). Our solver has a non-minimax optimization objective and employs the Gaussian mixture parametrization for the UEOT plans. We provide the generalization bounds for our solver (SS4.4) and experimentally test in several tasks (SS5.1, SS5.2).

**Notations.** We work in the Euclidian space \((^{d},\|\|)\). We use \(_{2,ac}(^{d})\) to denote the set of absolutely continuous (a.c.) Borel probability measures on \(^{d}\) with finite second moment and differential entropy. The set of nonnegative measures on \(^{d}\) with finite second moment is denoted as \(_{2,+}(^{d})\). We use \(_{2}(^{d})\) to denote the space of all _continuous_ functions \(:^{d}\) for which \( a=a(),b=b()\) such that \( x^{d}:|(x)| a+b\|x\|^{2}\). Its subspace of functions which are additionally _bounded from above_ is denoted as \(_{2,b}(^{d})\). For a.c. measure \(p_{2,ac}(^{d})\) (or \(_{2,+}(^{d})\)), we use \(p(x)\) to denote its density at a point \(x^{d}\). For a given a.c. measure \(_{2,+}(^{d}^{d})\), we denote its total mass by \(\|\|_{1}*{\,=\,}_{^{d}^{d} }(x,y)dxdy\). We use \(_{x}(x)\), \(_{y}(y)\) to denote the marginals of \((x,y)\). They satisfy the equality \(\|_{x}\|_{1}\!=\!\|_{y}\|_{1}\!=\!\|\|_{1}\). We write \((|x)\) to denote the conditional _probability_ measure. Each such measure has a unit total mass. We use \(\) to denote the Fenchel conjugate of a function \(f\): \((t)*{\,=\,}_{u}\{ut-f(u)\}\). We use \(_{A}\) to denote the convex indicator of a set \(A\), i.e., \(_{A}(x)=0\) if \(x A\); \(_{A}(x)=+\) if \(x A\).

## 2 Background

Here we give an overview of the relevant entropic optimal transport (EOT) concepts. For additional details on balanced EOT, we refer to [10; 24; 53], unbalanced EOT - to [8; 43].

\(f\)**-divergences for positive measures**. For _positive_ measures \(_{1},_{2}_{2,+}(^{d^{}})\) and a lower semi-continuous function \(f:\{\}\), the \(f\)_-divergence_ between \(_{1},_{2}\) is defined by

\[D_{f}(_{1}\|_{2})*{\,=\,}_{^{d^{ }}}f\!((x)}{_{2}(x)})\!_{2}(x)dx_{1}_{2}+.\]

We consider \(f(t)\) which are convex, non-negative and attain zero uniquely when \(t=1\). In this case, \(D_{f}\) is a valid measure of dissimilarity between two positive measures (see Appendix C for details). This means that \(D_{f}(_{1}\|_{2})\!\!0\) and \(D_{f}(_{1}\|_{2})\!=\!0\) if and only if \(_{1}=_{2}\). In our paper, we also assume that all \(f\) that we consider satisfy the property that \(\) is a _non-decreasing_ function.

Kullback-Leibler divergence \(}\)[8; 62], is a particular case of such \(f\)-divergence for positive measures. It has a generator function \(f_{}(t)*{\,=\,}\!t t\!-\!t\!+\!1\). Its convex conjugate \(}}(t)\!=\!(t)-1\). Another example is the \(^{2}\)-divergence \(}}\) which is generated by \(f_{^{2}}(t)*{\,=\,}\!\!*{\,=\,}(t-1)^{2}\) when \(t 0\) and \(\) otherwise. The convex conjugate of this function is \(}}(t)=-1\) if \(t\!<\!-2\) and \(t^{2}\!+\!t\) when \(t\!\!2\).

_Remark_.: By the definition, convex conjugates of \(f_{}\) and \(f_{^{2}}\) divergences are proper, non-negative and non-decreasing. These properties are used in some of our theoretical results.

**Entropy for positive measures.** For \(_{2,+}(^{d^{}})\), its entropy  is given by

\[H()*{\,=\,}\!-_{^{d^{}}}(x)\!(x) dx+\|\|_{1}-.\] (1)

When \(_{2,ac}(^{d^{}})\), i.e., \(\|\|_{1}\!=\!1\), equation (1) is the usual differential entropy minus 1.

**Classic EOT formulation (with the quadratic cost).** Consider two probability measures \(p_{2,ac}(^{d})\), \(q\!\!_{2,ac}(^{d})\). For \(>0\), the EOT problem between \(p\) and \(q\) is

\[_{(p,q)}_{^{d}}_{^{d}}}{2}(x,y)dxdy- H(),\] (2)

where \((p,q)\) is the set of probability measures \(\!\!_{2,ac}(^{d}\!\!^{d})\) with marginals \(p\), \(q\) (transport plans). Plan \(^{*}\) attaining the minimum exists, it is unique and called the _EOT plan_.

Classic EOT imposes hard constraints on the marginals which leads to several issues, e.g., sensitivity to outliers , inability to handle potential measure shifts such as class imbalances in the measures \(p,q\). The UEOT problem  overcomes these issues by relaxing the marginal constraints .

**Unbalanced EOT formulation (with the quadratic cost).** Let \(D_{f_{1}}\) and \(D_{f_{2}}\) be two \(f\)-divergences over \(^{d}\). For two probability measures \(p_{2,ac}(^{d})\), \(q_{2,ac}(^{d})\) and \(>0\), the unbalanced EOT problem between \(p\) and \(q\) consists of finding a minimizer of

\[_{_{2,+}(^{d}^{d})}_{ ^{d}}_{^{d}}}{2}(x,y)dxdy-  H()+D_{f_{1}}(_{x}\|p)+D_{f_{2}}( _{y}\|q),\] (3)

see Fig. 1. Here the minimum is attained for a unique \(^{*}\) which is called the _unbalanced optimal entropic_ (UEOT) plan. Typical choices of \(f_{i}\) (\(i\)) are \(f_{i}(t)=_{i}f_{}(t)\) or \(f_{i}(t)=_{i}f_{^{2}}(t)\) (\(_{i}>0\)) yielding the scaled \(_{}\) and \(_{^{2}}\), respectively. In this case, the bigger \(_{1}\) (\(_{2}\)) is, the more \(_{x}\) (\(_{y}\)) is penalized for not matching the corresponding marginal distribution \(p\) (\(q\)).

_Remark 1_.: While the set \(_{2,+}(^{d}^{d})\) contains not only a.c. measures, infimum in problem (3) is attained for a.c. measure \(^{*}\) since \(- H(^{*})\) term turns to \(+\) otherwise. Thus, almost everywhere in the paper we are actually interested in the subset of a.c. measures in \(_{2,+}(^{d}^{d})\).

_Remark 2_.: The balanced EOT problem (2) is a special case of (3). Indeed, let \(f_{1}\) and \(f_{2}\) be the convex indicators of \(\{1\}\), i.e., \(f_{1}=f_{2}=_{x=1}\). Then the \(f\)-divergences \(D_{f_{1}}(_{x}\|p)\) and \(D_{f_{2}}(_{y}\|q)\) become infinity if \(p_{x}\) or \(q_{y}\), and become zeros otherwise.

**Dual form of unbalanced EOT problem (3)** is formulated as follows

\[_{(,)}\!\!\{\!-\!\!\!_{^{d}}\!\!\! _{^{d}}\!\!\!\{((x)\!+\!(y)\!- \!}{2})\}dxdy-\!\!_{^{d}}\!\!\!_{1 }(-(x))p(x)dx-\!\!_{^{d}}\!\!_{2}(-(y))q(y) dy\}\!.\] (4)

It is known that there exist two measurable functions \(^{*}\), \(^{*}\) delivering maximum to (4). They have the following connection with the solution of the primal unbalanced problem (3):

\[^{*}(x,y)\!=\!\{(x)}{}\}\!\{-}{2}\}\!\{(y)}{}\}.\] (5)

_Remark_.: For some of our results, we will need to restrict potentials \((,)\) in problem (4) to the space \(_{2,b}(^{d})_{2,b}(^{d})\). Since established variants of dual forms  typically correspond to other functional spaces, we derive and theoretically justify a variant of the _dual problem_ (4) in Appendix A.3. Note that it may hold that optimal potentials \(^{*},^{*}_{2,b}(^{D})\), i.e., the supremum is not achieved within our considered spaces. This is not principle for our subsequent derivations.

**Computational UEOT setup.** Analytical solution for the _unbalanced_ EOT problem is, in general, not known.1 Moreover, in real-world setups where unbalanced EOT is applicable, the measures \(p\), \(q\) are typically not available explicitly but only through their empirical samples (datasets).

We assume that data measures \(p,q_{2,ac}(^{d})\) are unknown and accessible only by a limited number of i.i.d. empirical samples \(\{x_{0},...,x_{N}\}\!\!p\), \(\{y_{0},...,y_{M}\}\!\!q\). We aim to approximate the optimal UEOT plan \(^{*}\) solving (3) between the entire measures \(p,q\). The recovered plans should allow the out-of-sample estimation, i.e., generation of samples from \(^{*}(|x^{})\) where \(x^{}\) is a new test point (not necessarily present in the train data). Optionally, one may require the ability to sample from \(^{*}_{x}\).

Figure 1: Unbalanced EOT problem.

The described setup is typically called the _continuous EOT_ and should not be twisted up with the _discrete EOT_[53; 10]. There the aim is to recover the (unbalanced) EOT plan between the empirical measures \(=\!_{i=1}^{N}_{x_{i}}\), \(\!=\!_{j=1}^{M}_{y_{j}}\) and out-of-sample estimations are typically not needed.

## 3 Related Work

Nowadays, the sphere of continuous OT/EOT solvers is actively developing. Some of the early works related to this topic utilize OT cost as the loss function [27; 25; 2]. These approaches are not relevant to us as they do not learn OT/EOT maps (or plans). We refer to  for a detailed review.

At the same time, there exist a large amount of works within the discrete OT/EOT setup [10; 15; 69; 51], see  for a survey. We again emphasize that solvers of this type are not relevant to us as they construct discrete matching between the given (train) samples and typically do not provide a generalization to the new unseen (test) data. Only recently ML community started developing out-of-sample estimation procedures based on discrete/batched OT. For example, [19; 56; 32; 14; 48; 57] mostly develop such estimators using the barycentric projections of the discrete EOT plans. Though these procedures have nice theoretical properties, their scalability remains unclear.

**Balanced OT/EOT solvers.** There exists a vast amount of neural solvers for continuous OT problem. Most of them learn the OT maps (or plans) via solving saddle point optimization problems [3; 18; 37; 22; 60; 50]. Though the recent works [28; 61; 11; 41; 30] tackle the EOT problem (2), they consider its balanced version. Hence they are not relevant to us. Among these works, only [41; 30] evade non-trivial training/inference procedures and are ideologically the closest to ours. The difference between them consists of the particular loss function used. In fact, **our paper** proposes the solver which subsumes these solvers and generalizes them for the unbalanced case. The derivation of our solver is non-trivial and requires solid mathematical apparatus, see SS4.

**Unbalanced OT/EOT solvers.** A vast majority of early works in this field tackle the discrete UOT/UEGT setup [6; 20; 54] but the principles behind their construction are not easy to generalize to the continuous setting. Thus, many of the recent papers that tackle the continuous unbalanced OT/EOT setup employ discrete solutions in the construction of their solvers. For example,  regress neural network on top of scaling factors obtained using the discrete UEOT while simultaneously learning the continuous OT maps using an ICNN method . In  and , the authors implement Flow Matching [44, FM] and conditional FM on top of the discrete UEOT plans, respectively. The algorithm of the latter consists of regressing neural networks on top of scaling factors and simultaneously learning a conditional vector field to transport the mass between re-balanced measures. Despite the promising practical performance of these solvers, it is still unclear to what extent such approximations of UEOT plans are theoretically justified.

The recent papers [70; 9] are more related to our study as they do not rely on discrete OT approximations of the transport plan. However, they have non-trivial minimax optimization objectives solved using _complex_ GAN-style procedures. Thus, these GANs often lean on heavy neural parametrization, may incur instabilities during training, and require careful hyperparameter selection .

For completeness, we also mention other papers which are only slightly relevant to us. For example,  considers incomplete OT which relaxes only one of the OT marginal constraints and is less general than the unbalanced OT. Other works [12; 4] incorporate unbalanced OT into the training objective of GANs aimed at generating samples from noise.

In contrast to the listed works, our paper proposes a _theoretically justified and lightweight_ solver to the UEOT problem, see Table 1 for the detailed comparison of solvers.

 
**Solver** & **Problem** & **Phileries** & **What recovers** \\ 
 &  & Solve \(\)-transform based semi-dual & Scaling factor \(^{*}\)/\(\)/\(\)/\(\) and & Complex max-min objective; \\  & & max-min reformulation of UOT using neural nets & stochastic OT map \(T^{*}\) (\(z,z\)) & 3 neural networks \\ 
 &  Custom \\ UOT \\  } & Regression on top of discrete EIT & Scaling factors and & Heuristically uses \\  & & between re-balanced measures & OT maps between re-scaled measures & minibatch OT pre-expantances \\ 
 &  UOT \\  } & Salves semi-dual max-min &  Stochastic UOT map \(T^{*}\) (\(x,z\)) \\  } &  Complex max-min objective; \\ 2 neural networks \\  } \\  & & Flow Matching up of discrete UOT & & Particularly uses \\  & using neural nets & \((v_{r},)\!\!\) to knapper the mass & minibatch OT pre-expantances \\ 
 &  UEOT \\  } & Conditional Flow Matching on top of discrete EIT & Scaling factors and parameter conditional & Heuristically uses \\  & & between re-balanced measures using neural nets & vector feed \((v_{r},)\!\!\) to knapper & Heuristically uses \\  & & the mass between re-scaled measures & minibatch OT pre-expantances \\  U-LightOT &  SUFOOT} & Solve non-minstraent reformulation & Density of UOT \(^{*}\) (together with light \\  & of dual UEOT using Gaussian Mixtures & procedure to sample \(z_{z}^{*}\)) and \(_{y}^{*}\!( z)\) & Heuristically uses \\   } \\  

Table 1: Comparison of the principles of existing UOT/UEGT solvers and **our** proposed light solver.

[MISSING_PAGE_FAIL:5]

### Parametrization and the Optimization Procedure

**Parametrization.** Recall that \(u_{}\) parametrizes the density of the marginal \(_{x}^{*}\) which is unnormalized. Setting \(x=0\) in equation (7), we get \(^{*}(y|x=0) v^{*}(y)\) which means that \(v^{*}\) also corresponds to an unnormalized density of a measure. These motivate us to use the unnormalized Gaussian mixture parametrization for the potential \(v_{}(y)\) and measure \(u_{}(x)\):

\[v_{}(y)\!=\!_{k=1}^{K}\!\!_{k}(y|r_{k},\! S _{k});\ \ u_{}(x)\!\!=\!\!\!_{l=1}^{L}\!_{l}(x|_{l},\! _{l}).\] (11)

Here \(}}{{=}}\{_{k},r_{k},S_{k}\}_ {k=1}^{K}\), \(w}}{{=}}\{_{l},_{l},_{l}\}_ {l=1}^{L}\) with \(_{k},_{l} 0\), \(r_{k},_{l}^{d}\) and \(0 S_{k},_{l}^{d d}\). The covariance matrices are scaled by \(\) just for convenience.

For this type of parametrization, it holds that \(\|u_{}\|_{1}=_{l=1}^{L}_{l}\). Moreover, there exist closed-from expressions for the normalization constant \(c_{}(x)\) and conditional plan \(_{}(y|x)\), see [41, Proposition 3.2]. Specifically, define \(r_{k}(x)}}{{=}}r_{k}+S_{k}x\) and \(_{k}(x)}}{{=}}_{ k}\{S_{k}x+2r_{k}^{T}x}{2}\}\). It holds that

\[c_{}(x)=_{k=1}^{K}_{k}(x);\ \ _{}(y|x)=(x)}_{k=1}^{K}_{k}(x)(y|r_{k}(x),  S_{k}).\] (12)

Using this result and (11), we get the expression for \(_{,w}\):

\[_{,w}(x,y)=u_{}(x)_{}(y|x)=^{L}_{l}(x|_{l},\!_{l})}_{u_{ }(x)}^{K}_{k}(x) (y|r_{k}(x), S_{k})}{_{k=1}^{K}_{ k}(x)}}_{_{}(y|x)}.\] (13)

**Training.** We recall that the measures \(p,q\) are accessible only by a number of empirical samples (see the learning setup in SS2). Thus, given samples \(\{x_{1},...,x_{N}\}\) and \(\{y_{1},...,y_{M}\}\), we optimize the empirical analog of (10):

\[}(,w)\!}}{{=} }\!_{i=1}^{N}_{1}(- (x_{i})}{c_{}(x_{i})}-\|^{2}}{2})\!+\!_{j=1}^ {M}_{2}(- v_{}(y_{j})\!-\!\|^{2 }}{2})\!\!+\!\|u_{}\|_{1}\] (14)

using minibatch gradient descent procedure w.r.t. parameters \((,w)\). In the parametrization of \(v_{}\) and \(u_{}\) (11), we utilize the diagonal matrices \(S_{k}\), \(_{l}\). This allows decreasing the number of learnable parameters in \(\) and speeding up the computation of inverse matrices \(S_{k}^{-1},\,_{l}^{-1}\).

**Inference.** Sampling from the conditional and marginal measures \(_{}(y|x)\!\!^{*}(y|x)\), \(u_{}\!\!_{x}^{*}\) is easy and lightweight since they are explicitly parametrized as Gaussian mixtures, see (12), (11).

### Connection to Related Prior Works

The idea of using Gaussian Mixture parametrization for dual potentials in EOT-related tasks first appeared in the EOT/SB benchmark . There it was used to obtain the benchmark pairs of probability measures with the known EOT solution between them. In , the authors utilized this type of parametrization to obtain a light solver **(LightSB)** for the **balanced** EOT.

Our solver for **unbalanced** EOT (10) subsumes their solver for balanced EOT as well as one problem subsumes the other for the special case of divergences, see the remark in SS2. Let \(f_{1}\), \(f_{2}\) be convex indicators of \(\{1\}\). Then \(}(t)=t\) and \(}(t)=t\) and objective (10) becomes

\[(,w)\!=\!_{^{d}}\!(- (x)}{c_{}(x)}\!-\!}{2})p(x)dx+\!_{ ^{d}}\!(- v_{}(y)\!-\!}{2})q(y) dy\!+\!\|u_{}\|_{1}=\] \[-\!_{^{d}}\!(x )}{c_{}(x)}p(x)dx\!+\!\!\!_{^{d}}\! v_{}(y)q(y) dy\!-\!\|u_{}\|_{1}-^{d}}\!}{2}p(x) dx\!-\!\!_{^{d}}\!}{2}q(y)dy}_{\!(p,q)}\!=\] \[\!_{^{d}} c_{}(x)p(x)dx \!\!-\!\!_{^{d}} v_{}(y)q(y)dy\,-\] (15) \[_{^{d}} u_{}(x)p(x)dx+ \|u_{}\|_{1}-(p,q).\] (16)Here line (15) depends exclusively on \(\) and exactly coincides with the LightSB's objective, see [41, Proposition 8]. At the same time, line (16) depends only on \(\), and its minimum is attained when \(u_{}=p\). Thus, this part is not actually needed in the balanced case, see [41, Appendix C].

### Generalization Bounds and Universal Approximation Property

Theoretically, to recover the UEOT plan \(^{*}\), one needs to solve the problem \((,)_{,}\) as stated in our Theorem 4.1. In practice, the measures \(p\) and \(q\) are accessible via empirical samples \(X}}{{=}}\{x_{1},...,x_{N}\}\) and \(Y}}{{=}}\{y_{1},...,y_{M}\}\), thus, one needs to optimize the empirical counterpart \(}(,)\) of \((,)\), see (14). Besides, the available potentials \(u_{}\), \(v_{}\) over which one optimizes the objective come from the restricted class of functions. Specifically, we consider unnormalized Gaussian mixtures \(u_{}\), \(v_{}\) with \(K\) and \(L\) components respectively. Thus, one may naturally wonder: **how close is the recovered plan \(_{,}\) to the UEOT plan \(^{*}\)** given that \((,)=_{,}}(,)\)?

To address this question, we study the _generalization error_\(_{}(^{*}\|_{,})\), i.e., the expectation of \(_{}\) between \(^{*}\) and \(_{,}\) taken w.r.t. the random realization of the train datasets \(X p\), \(Y q\).

Let \((,)=_{(,) }(,)\) denote the best approximators of \((,)\) in the admissible class. From Theorem 4.1 it follows that that \(_{}(^{*}\|_{, })\) can be upper bounded using \(((,)-^{*})\). The latter can be decomposed into the estimation and approximation errors:

\[((,)-^{*})\!= \![(,)\!-\!( ,)]\!+\![(,)\!-\!^{*}]\!=\![ (,)\!-\!(,)]}_{}\!+\![( ,)\!-\!^{*}]\.\] (17)

We establish the quantitative bound for the estimation error in the proposition below.

**Proposition 4.2** (Bound for the estimation error).: _Let \(p,q\) be compactly supported and assume that \(_{1},\ _{2}\) are Lipshitz. Assume that the considered parametric classes \(\), \(((,))\) consist of unnormalized Gaussian mixtures with \(K\) and \(L\) components respectively with bounded means \(\|r_{k}\|,\|_{l}\| R\) (for some \(R>0\)), covariances \(sI S_{k},_{l} SI\) (for some \(0<s S\)) and weights \(a_{k},_{l} A\) (for some \(0<a A\)). Then the following holds:_

\[(,)-( ,) O(})+O( }),\]

_where \(O()\) hides the constants depending only on \(K,L,R,s,S,a,A,p,q,\) but not on sizes \(M,N\)._

This proposition allows us to conclude that the estimation error converges to zero when \(N\) and \(M\) tend to infinity at the usual parametric rate. It remains to clarify the question: _can we make the approximation error arbitrarily small_? We answer this question positively in our Theorem below.

**Theorem 4.3** (Gaussian mixture parameterization for the variables provides the universal approximation of UEOT plans).: _Let \(p\) and \(q\) be compactly supported and assume that \(_{1},\ _{2}\) are Lipshitz. Then for all \(>0\) there exist Gaussian mixtures \(v_{}\), \(u_{}\) (11) with **scalar** covariances \(S_{k}=_{k}I_{d} 0\), \(_{l}=_{l}I_{d} 0\) of their components that satisfy \(_{}(^{*}\|_{,}) ^{-1}((,)-^{*})<\)._

**In summary**, results of this section show that one can make the generalization error _arbitrarily small_ given a sufficiently large amount of samples and components in the Gaussian parametrization. It means that theoretically our solver can recover the UEOT plan arbitrarily well.

## 5 Experiments

In this section, we test our U-LightOT solver on several setups from the related works. The code is written using PyTorch framework and is publicly available at

https://github.com/milenagazdieva/LightUnbalancedOptimalTransport.

The experiments are issued in the convenient form of *.ipynb notebooks. Each experiment requires several minutes of training on CPU with 4 cores. Technical _training details_ are given in Appendix B.

### Example with the Mixture of Gaussians

We provide an illustrative _'Gaussians Mixture'_ example in 2D to demonstrate the ability of our solver to deal with the imbalance of classes in the source and target measures. We follow the experimental setup proposed in [16, Figure 2] and define the probability measures \(p\), \(q\) as follows (Fig. 1(a)):

\[p(x) = (x|(-3,3),0.1 I_{2})+(x|(1,3),0.1 I_{2}),\] \[q(y) = (y|(-3,0),0.1 I_{2})+ (y|(1,0),0.1 I_{2}).\]

We test our U-LightOT solver with _scaled_\(}\) divergences, i.e., \(f_{1}(t),f_{2}(t)\) are defined by \( f_{}(t)=(t(t)-t+1)\) where \(>0\). We provide the learned plans for \([1,\ 10^{1},\ 10^{2}]\). The results in Fig. 2 show that parameter \(\) can be used to control the level of unbalancedness of the learned plans. For \(=1\), our U-LightOT solver truly learns the UEOT plans, see Fig. 1(d). When \(\) increases, the solver fails to transport the mass from the input Gaussians to the closest target ones. Actually, for \(=10^{2}\), our solutions are similar to the solutions of (41, LightSB) solver which approximates balanced EOT plans between the measures. Hereafter, we treat \(\) as the _unbalancedness_ parameter. In Appendix C, we test the performance of our solver with \(}_{^{2}}\) divergence.

_Remark_.: Here we conduct all experiments with the entropy regularization parameter \(=0.05\). The parameter \(\) is responsible for the stochasticity of the learned transport \(_{}(|x)\). Since we are mostly interested in the correct transport of the mass (controlled by \(f_{1},f_{2}\)) rather than the stochasticity, we do not pay much attention to \(\) throughout the paper.

### Unpaired Image-to-Image Translation

Another popular testbed which is usually considered in OT/EOT papers is the unpaired image-to-image translation  task. Since our solver uses the parametrization based on Gaussian mixture, it may be hard to apply U-LightOT for learning translation directly in the image space. Fortunately, nowadays it is common to use autoencoders  for more efficient translation. We follow the setup of (41, Section 5.4) and use pre-trained ALAE autoencoder  for \(1024 1024\) FFHQ dataset  of human faces. We consider different subsets of FFHQ dataset (_Adult_, _Young_, _Man_, _Woman_) and all variants of translations between them: _Adult\(\)Young_ and _Woman\(\)Man_.

The main challenge of the described translations is the **imbalance** of classes in the images from source and target subsets, see Table 2. Let us consider in more detail _Adult\(\)Young_ translation. In the FFHQ dataset, the amount of _adult men_ significantly outnumbers the _adult women_, while the amount of _young men_ is smaller than that of _young women_. Thus, balanced OT/EOT solvers are expected to translate some of _adult man_ representatives to _young woman_ ones. At the same time, solvers based on unbalanced transport are supposed to alleviate this issue.

**Baselines.** We perform a comparison with the recent procedure (16, UOT-FM) which considers roughly the same setup and demonstrates good performance. This method interpolates the results of unbalanced discrete OT to the continuous setup using the flow matching . For completeness, we include the comparison with LightSB and balanced optimal transport-based flow matching (OT-FM)  to demonstrate the issues of the balanced solvers. We also consider neural network based solvers relying on the adversarial learning such as UOT-SD  and UOT-GAN.

 
**Class** & _Man_ & _Woman_ \\  _Young_ & \(15K\) & \(23K\) \\  _Adult_ & \(7K\) & \(3.5K\) \\  

Table 2: Number of _train_ FFHQ images for each subset.

**Metrics.** We aim to assess the ability of the solvers to perform the translation of latent codes keeping the class of the input images unchanged, e.g., keeping the gender in _Adult\(\)Young_ translation. Thus, we train a \(99\%\) MLP classifier for gender using the latent codes of images. We use it to compute the accuracy of preserving the gender during the translation. We denote this number by _accuracy (keep)_.

Meanwhile, it is also important to ensure that the generated images belong to the distribution of the target images rather than the source ones, e.g., belong to the distribution of _young_ people in our example. To monitor this property, we use another 99% MLP classifier to identify whether each of the generated images belong to the target domain or to the source one. Then we calculate the fraction of the generated images belonging to the target domain which we denote as _accuracy (target)_.

**Results.** Unbalanced OT/EOT approaches are equipped with some kind of unbalancedness parameter (like \(\) in our solver) which influences the methods' results: with the increase of unbalancedness, accuracy of keeping the class increases but the accuracy of mapping to the target decreases. The latter is because of the relaxation of the marginal constraints in UEOT. For a fair comparison, we aim to compute the trade-offs between the keep/target accuracies for different values of the unbalancedness. We do this for our solver and UOT-FM. We found that GAN-based unbalanced approaches show unstable behaviour, so we report UOT-SD and UOT-GAN's results only for one parameter value.

For convenience, we visualize the accuracies' pairs for our solver and its competitors in Fig. 3. The evaluation shows that our U-LightOT method can effectively solve translation tasks in high dimensions (\(d=512\)) and outperforms its alternatives in dealing with class imbalance issues. Namely, we see that our solver allows for achieving the best accuracy of keeping the attributes of the input images than other methods while it provides good accuracy of mapping to the target class. While balanced methods and some of the unbalanced ones provide really high accuracy of mapping to the target, their corresponding accuracies of keeping the attributes are worse than ours meaning that we are better in dealing with class imbalance issues. As expected, for large parameter \(\), the results of our U-LightOT solver coincide with those for LightSB which is a balanced solver. Meanwhile, our solver has the lowest wall-clock running time among the existing unbalanced solvers, see Table 3 for comparison. We demonstrate qualitative results of our solver and baselines in Fig. 4. The choice of unbalancedness parameter for visualization of our method and UOT-FM is detailed in Appendix B.

We present the results of the _quantitative comparison_ in the form of tables in Appendix D.1. In Appendix C, we perform the _ablation study_ of our solver focusing on the selection of parameters \(\), \(\) and number of Gaussian mixtures' components.

## 6 Discussion

**Potential impact.** Our light and unbalanced solver has a lot of advantages in comparison with the other existing UEOT solvers. First, it does not require complex max-min optimization. Second, it provides the closed form of the conditional measures \(_{}(y|x)^{*}(y|x)\) of the UEOT plan.

Figure 3: Visualization of pairs of accuracies (_keep-target_) for our U-LightOT solver and other OT/EOT methods in the image translation experiment. The values of unbalancedness parameters for our U-LightOT solver (\(\)) and [16, UOT-FM] (\(=reg\_m\)) are specified directly on the plots.

 
**Experiment** & [16, UOT-FM] & [9, UOT-SD] & [70, UOT-GAN] & U-LightOT (**ours**) \\  _Time (sec)_ & 03:21 & 18:11 & 16:30 & **02:38** \\  

Table 3: Comparison of wall-clock running times of unbalanced OT/EOT solvers in _Young\(\)Adult_ translation. The best results are in **bold**, second best are underlined.

Moreover, it allows for sampling both from the conditional measure \(_{}(y|x)\) and marginal measure \(u_{}(x)_{x}^{*}(x)\). Besides, the decisive superiority of our lightweight and unbalanced solver is its simplicity and convenience of use. Indeed, it has a straightforward and non-minimax optimization objective and avoids heavy neural parametrization. As a result, our lightweight and unbalanced solver converges in minutes on CPU. We expect that these advantages could boost the usage of our solver as a standard and easy baseline for UEOT task with applications in different spheres.

The limitations and broader impact of our solver are discussed in Appendix E.

ACKNOWLEDGEMENTS. The work of Skoltech was supported by the Analytical center under the RF Government (subsidy agreement 000000D730321P5Q0002, Grant No. 70-2021-00145 02.11.2021). We thank Kirill Sokolov, Mikhail Persiianov and Petr Mokrov for providing valuable feedback and suggestions for improving the proofs and clarity of our paper.

Figure 4: Unpaired translation with LightSB, OT-FM, UOT-FM and our U-LightOT solvers applied in the latent space of ALAE  for FFHQ images  (1024\(\)1024).