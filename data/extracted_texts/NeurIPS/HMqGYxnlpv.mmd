# A Simple Yet Effective Strategy to Robustify the Meta Learning Paradigm

Qi Wang\({}^{1}\)   Yiqin Lv\({}^{1}\)   Yanghe Feng\({}^{2}\)   Zheng Xie\({}^{1}\)   Jincai Huang\({}^{2}\)

\({}^{1}\)College of Science, National University of Defense Technology

\({}^{2}\)College of Systems Engineering, National University of Defense Technology

{wangqi15,lvyiqin98,fengyanghe,xiezheng81,huangjincai}@nudt.edu.cn

These authors contributed equally.Zheng Xie and Jincai Huang are corresponding authors.

###### Abstract

Meta learning is a promising paradigm to enable skill transfer across tasks. Most previous methods employ the empirical risk minimization principle in optimization. However, the resulting worst fast adaptation to a subset of tasks can be catastrophic in risk-sensitive scenarios. To robustify fast adaptation, this paper optimizes meta learning pipelines from a distributionally robust perspective and meta trains models with the measure of expected tail risk. We take the two-stage strategy as heuristics to solve the robust meta learning problem, controlling the worst fast adaptation cases at a certain probabilistic level. Experimental results show that our simple method can improve the robustness of meta learning to task distributions and reduce the conditional expectation of the worst fast adaptation risk.

## 1 Introduction

The past decade has witnessed the remarkable progress of deep learning in real-world applications (LeCun et al., 2015). However, training deep learning models requires an enormous dataset and intensive computational power. At the same time, these pre-trained models can frequently encounter deployment difficulties when the dataset's distribution drifts in testing time (Lesort et al., 2021).

As a result, the paradigm of _meta learning_ or _learning to learn_ is proposed and impacts the machine learning scheme (Finn et al., 2017), which leverages past experiences to enable fast adaptation to unseen tasks. Moreover, in the past few years, there has grown a large body of meta learning methods to find plausible strategies to distill common knowledge into separate tasks (Finn et al., 2017; Duan et al., 2016; Garnelo et al., 2018).

Notably, most previous work concentrates merely on the fast adaptation strategies and employs the standard risk minimization principle, _e.g._ the empirical risk minimization, ignoring the difference between tasks in fast adaptation. Given the sampled batch from the task distribution, the standard meta learning methods weight tasks equally in fast adaptation. Such an implementation raises concerns in some real-world scenarios, when worst fast adaptation is catastrophic in a range of risk-sensitive applications (Johannsmmeier et al., 2019; Jaafra et al., 2019; Wang et al., 2023). For example, in

Figure 1: **Illustrations of Distributionally Robust Fast Adaptation. Shown are histograms of meta risk function values \((_{}^{T},_{}^{C};)\) in the task distribution \(p()\). Given a probability \(\), we optimize meta learning model parameters \(\) to decrease the risk quantity \(_{}\) in Definition (3).**robotic manipulations, humanoid robots (Duan, 2017) can quickly leverage past motor primitives to walk on plain roads but might suffer from tribulation doing this on rough roads.

**Research Motivations.** Instead of seeking novel fast adaptation strategies, we take more interest in optimization principles for meta learning. Given the meta trained model, this paper stresses the performance difference in fast adaptation to various tasks as an indispensable consideration. As _the concept of robustness in fast adaptation_ has not been sufficiently explored from the task distribution perspective, researching this topic has more practical significance and deserves more attention in meta learning. Naturally, we raise the question below:

_Can we reconsider the meta learning paradigm through the lens of risk distribution, and are there plausible measures to enhance the fast adaptation robustness in some vulnerable scenarios?_

**Developed Methods.** In an effort to address the above concerns and answer these questions, _we reduce robust fast adaptation in meta learning to a stochastic optimization problem within the principle of minimizing the expected tail risk, e.g., conditional value-at-risk (CVaR) (Rockafellar et al., 2000)_. To tractably solve the problem, we adopt a two-stage heuristic strategy for optimization with the help of crude Monte Carlo methods (Kroese and Rubinstein, 2012) and give some theoretical analysis. In each optimization step, the algorithm estimates the value-at-risk (VaR) (Rockafellar et al., 2000) from a meta batch of tasks and screens and optimizes a percentile of task samples vulnerable to fast adaptation. As illustrated in Fig. (1), such an operation is equivalent to iteratively reshaping the task risk distribution to increase robustness. The consequence of optimizing the risk function distributions \(_{k}_{k+1}\) is to transport the probability mass in high-risk regions to the left side gradually. In this manner, the distribution of risk functions in the task domain can be optimized toward the anticipated direction that controls the worst-case fast adaptation at a certain probabilistic level.

**Outline & Primary Contributions.** We overview related meta learning and robust optimization work in Section (2). Section (3) introduces general notations and describes meta learning optimization objectives together with typical models. The distributionally robust meta learning problem is presented together with a heuristic optimization strategy in Section (4). We report experimental results and analysis in Section (5), followed by conclusions and limitations in Section (6). **Our primary contribution** is two-fold:

1. We recast the robustification of meta learning to a distributional optimization problem. The resulting framework minimizes the conditional expectation of task risks, namely the tail risk, which unifies vanilla meta-learning and worst-case meta learning frameworks.
2. To resolve the robust meta learning problem, we adopt the heuristic two-stage strategy and demonstrate its improvement guarantee. Experimental results show the effectiveness of our method, enhancing fast adaptation robustness and mitigating the worst-case performance.

## 2 Literature Review

**Meta Learning Methods.** In practice, meta learning enables fast learning (adaptation to unseen tasks) via slow learning (meta training in a collection of tasks). There exist different families of meta learning methods. The optimization-based methods, such as model agnostic meta learning (MAML) (Finn et al., 2017) and its variants (Finn et al., 2018; Rajeswaran et al., 2019; Grant et al., 2018; Vuorio et al., 2019; Abbas et al., 2022), try to find the optimal initial parameters of models and then execute gradient updates over them to achieve adaptation with a few examples. The context-based methods, _e.g._ conditional neural processes (CNPs) (Garnelo et al., 2018), neural processes (NPs) (Garnelo et al., 2018) and extensions (Gordon et al., 2019; Foong et al., 2020; Wang and Van Hoof, 2020; Gondal et al., 2021; Wang and van Hoof, 2022; Shen et al., 2021; Wang et al., 2023), learn the representation of tasks in the function space and formulate meta learning models as exchangeable stochastic processes. The metrics-based methods (Snell et al., 2017; Allen et al., 2019; Bartunov and Vetrov, 2018) embed tasks in a metric space and can achieve competitive performance in few-shot classification tasks. Other methods like memory-augmented models (Santoro et al., 2016; Xiao et al., 2021, 2022), recurrent models (Duan et al., 2016) and hyper networks (Zhao et al., 2020; Beck et al., 2023) are also modified for meta learning purposes.

Besides, there exist several important works investigating the generalization capability of methods. Bai et al. (2021) conduct the theoretical analysis of the train-validation split and connect it to optimality. In (Chen et al., 2021), a generalization bound is constructed for MAML through the lens of information theory. Denevi et al. (2019) study an average risk bound and estimate the bias for improving stochastic gradient optimization in meta learning.

**Robust Optimization.** When performing robust optimization for downstream tasks in deep learning, we can find massive work concerning the adversarial input noise (Goodfellow et al., 2018; Goel et al., 2020; Ren et al., 2021), or the perturbation on the model parameters (Goodfellow et al., 2014; Kurakin et al., 2016; Liu et al., 2018; Silva and Najafirad, 2020). In contrast, this paper studies the robustness of fast adaptation in meta learning. In terms of robust principles, the commonly considered one is the worst-case optimization (Olds, 2015; Zhang et al., 2020; Tay et al., 2022). For example, Collins et al. (2020) conducts the worst-case optimization in MAML to obtain the robust meta initialization. Considering the influence of adversarial examples, Goldblum et al. (2019) propose to adversarially meta train the model for few-shot image classification. Wang et al. (2020) adopt the worst-case optimization in MAML to increase the model robustness by injecting adversarial noise to the input. _However, distributionally robust optimization (Rahimian and Mehrotra, 2019) is rarely examined in the presence of the meta learning task distribution._

## 3 Preliminaries

**Notations.** Consider the distribution of tasks \(p()\) for meta learning and denote the task space by \(_{}\). Let \(\) be a task sampled from \(p()\) with \(\) the set of all tasks. We denote the meta dataset by \(_{}\). For example, in few-shot regression problems, \(_{}\) refers to a set of data points \(\{(x_{i},y_{i})\}_{i=1}^{n}\) to fit.

Generally, \(_{}\) are processed into the context set \(_{}^{C}\) for fast adaptation, and the target set \(_{}^{T}\) for evaluating adaptation performance. As an instance, we process the dataset \(_{}=_{}^{C}_{}^{T}\) with a fixed partition in MAML (Finn et al., 2017). \(_{}^{C}\) and \(_{}^{T}\) are respectively used for the inner loop and the outer loop in model optimization.

**Definition 1** (Meta Risk Function): _With the task \(\) and the pre-processed dataset \(_{}\) and the model parameter \(\), the meta risk function is a map \(:_{}^{+}\)._

In meta learning, the meta risk function \((_{}^{T},_{}^{C};)\), _e.g._ instantiations in Example (1)/(2), is to evaluate the model performance after fast adaptation. Now we turn to the commonly-used risk minimization principle, which plays a crucial role in fast adaptation. To summarize, we include the vanilla and worst-case optimization objectives as follows.

**Expected Risk Minimization for Meta Learning.** The objective that applies to most vanilla meta learning methods can be formulated in Eq. (1), and the optimization executes in a distribution over tasks \(p()\). The Monte Carlo estimate corresponds to the _empirical risk minimization_ principle.

\[_{}():=_{p()} (_{}^{T},_{}^{C};)\] (1)

Here \(\) is the parameter of meta learning models, which includes parameters for common knowledge shared across all tasks and for fast adaptation. Furthermore, the task distribution heavily influences the direction of optimization in meta training.

**Worst-case Risk Minimization for Meta Learning.** This also considers meta learning in the task distribution \(p()\), but the worst case in fast adaptation is the top priority in optimization.

\[_{}_{}(_{}^{ T},_{}^{C};)\] (2)

The optimization objective is built upon the min-max framework, advancing the robustness of meta learning to the worst case. Approaches like TR-MAML (Collins et al., 2020) sample the worst task in a batch to meta train with gradient updates. Nevertheless, this setup might result in a highly conservative solution where the worst case only happens with an incredibly lower chance.

Distributionally Robust Fast Adaptation

This section starts with the concept of risk measures and the derived meta learning optimization objective. Then a heuristic strategy is designed to approximately solve the problem. Finally, we provide two examples of distributionally robust meta learning methods.

### Meta Risk Functions as Random Variables

**Assumption 1**: _The meta risk function \((_{}^{T},_{}^{C};)\) is \(_{}\)-Lipschitz continuous w.r.t. \(\), which suggests: there exists a positive constant \(_{}\) such that \(\{,^{}\}\):_

\[|(_{}^{T},_{}^{C};)-( _{}^{T},_{}^{C};^{})|_ {}||-^{}||.\]

Let \((_{},_{},_{})\) denote a probability measure over the task space, where \(_{}\) corresponds to a \(\)-algebra on the subsets of \(_{}\). And we have \((^{+},)\) a probability measure over the non-negative real domain for the previously mentioned meta risk function \((_{}^{T},_{}^{C};)\) with \(\) a Borel \(\)-algebra. For any \(\), the meta learning operator \(_{}:_{}^{+}\) is defined as:

\[_{}:(_{}^{T},_ {}^{C};).\]

In this way, \((_{}^{T},_{}^{C};)\) can be viewed as a random variable to induce the distribution \(p((_{}^{T},_{}^{C};))\). Further, the cumulative distribution function can be formulated as \(F_{}(l;)=(\{(_{}^{T},_ {}^{C};) l;_{},l^{+}\})\)_w.r.t._ the task space. Note that \(F_{}(l;)\) implicitly depends on the model parameter \(\), and we cannot access a closed-form in practice.

**Definition 2** (Value-at-Risk): _Given the confidence level \(\), the task distribution \(p()\) and the model parameter \(\), the VaR (Rockafellar et al., 2000) of the meta risk function is defined as:_

\[_{}[(,)]=_{l ^{+}}\{l|F_{}(l;),\}.\]

**Definition 3** (Conditional Value-at-Risk): _Given the confidence level \(\), the task distribution \(p()\) and the model parameter \(\), we focus on the constrained domain of the random variable \((_{}^{T},_{}^{C};)\) with \((_{}^{T},_{}^{C};) _{}[(,)]\). The conditional expectation of this is termed as conditional value-at-risk (Rockafellar et al., 2000):_

\[_{}[(,)]=_{0}^{ }ldF_{}^{}(l;),\]

_where the normalized cumulative distribution is as follows:_

\[F_{}^{}(l;)=0,&l<_{}[( ,)]\\ (l;)-}{1-},&l_{}[( ,)].\]

This results in the normalized probability measure \((_{,},_{,},_{,})\) over the task space, where \(_{,}:=_{_{}[(, )]}[_{}^{-1}()]\). For ease of presentation, we denote the corresponding task distribution constrained in \(_{,}\) by \(p_{}(;)\).

Rather than optimizing VaR\({}_{}\), a quantile, in meta learning, we take more interest in CVaR\({}_{}\) optimization, a type of _the expected tail risk_. Such risk measure regards the conditional expectation and has more desirable properties for meta learning: _more adequate in handling adaptation risks in extreme tails_, _more accessible sensitivity analysis w.r.t. \(\)_, and _more efficient optimization_.

**Remark 1**: _CVaR\({}_{}[(,)]\) to minimize is respectively equivalent with the vanilla meta learning optimization objective in Eq. (1) when \(=0\) and the worst-case meta learning optimization objective in Eq. (3) when \(\) is sufficiently close to 1._

### Meta Learning via Controlling the Expected Tail Risk

As mentioned in Remark (1), the previous two meta learning objectives can be viewed as special cases within the CVaR\({}_{}\) principle. Furthermore, we turn to a particular distributionally robust fast adaptation with the adjustable confidence level \(\) to control the expected tail risk in optimization as follows.

**Distributionally Robust Meta Learning Objective.** With the previously introduced normalized probability density function \(p_{}(;)\), minimizing \(_{}[(,)]\) can be rewritten as Eq. (3).

\[_{}_{}():=_{p_{ }(;)}(_{}^{T},_{ }^{C};)\] (3)

Even though \(_{}[(,)]\) is a function of the model parameter \(\), the integral in Eq. (3) is intractable due to the involvement of \(p_{}(;)\) in a non-closed form.

**Assumption 2**: _For meta risk function values, the cumulative distribution function \(F_{}(l;)\) is \(_{}\)-Lipschitz continuous w.r.t. \(l\), and the implicit normalized probability density function of tasks \(p_{}(;)\) is \(_{}\)-Lipschitz continuous w.r.t. \(\)._

**Assumption 3**: _For any valid \(\) and corresponding implicit normalized probability density function of tasks \(p_{}(;)\), the meta risk function value can be bounded by a positive constant \(_{}\):_

\[_{_{,}}(_{_{i}}^{T},_{_{i}}^{C};)_{}.\]

**Proposition 1**: _Under assumptions (1)(2)(3), the meta learning optimization objective \(_{}()\) in Eq. (3) is continuous w.r.t. \(\)._

Further, we use \(_{}()\) to denote the \(_{}[(,)]\) for simple notations. The same as that in (Rockafellar et al., 2000), we introduce a slack variable \(\) and the auxiliary risk function \([(_{}^{T},_{}^{C};)- ]^{+}:=\{(_{}^{T},_{}^{C}; )-,0\}\). To circumvent directly optimizing the non-analytical \(p_{}(;)\), we can convert the probability constrained function \(_{}()\) to the below unconstrained one after optimizing \(\):

\[_{}(;)=+_{p()} [[(_{}^{T},_{}^{C};)- ]^{+}].\]

It is demonstrated that \(_{}()=_{}_{}(; )\) and \(_{}_{}_{}(;)\) in (Rockafellar et al., 2000), and also note that \(_{}\) is the upper bound of \(_{}\), implying

\[_{}_{}(_{};)_{} (;),.\] (4)

With the deductions from Eq.s (3)(4), we can resort the distributionally robust meta learning optimization objective with the probability constraint into a unconstrained optimization objective as Eq.(5).

\[_{,}+_{p ()}[[(_{}^{T},_{}^{C}; )-]^{+}]\] (5)

**Sample Average Approximation.** For the stochastic programming problem above, it is mostly intractable to derive the analytical form of the integral. Hence, we need to perform Monte Carlo estimates of Eq. (5) to obtain Eq. (6) for optimization.

\[_{,}+}_{i=1}^{}[(_{_{i}}^{T}, _{_{i}}^{C};)-]^{+}\] (6)

**Remark 2**: _If \((_{}^{T},_{}^{C};)\) is convex w.r.t. \(\), then Eq.s (5)(6) are also convex functions. In this case, the optimization objective Eq. (6) of our interest can be resolved with the help of several convex programming algorithms (Fan et al., 2017; Meng et al., 2020; Levy et al., 2020)._

### Heuristic Algorithms for Optimization

Unfortunately, most existing meta learning models' risk functions (Finn et al., 2017; Garnelo et al., 2018; Santoro et al., 2016; Li et al., 2017; Duan et al., 2016), \((_{}^{T},_{}^{C};)\) are non-convex _w.r.t._\(\), bringing difficulties in optimization of Eq.s (5)/(6).

To this end, we propose a simple yet effective optimization strategy, where the sampled task batch is used to approximate the VaR\({}_{}\) and the integral in Eq. (5) for deriving Eq. (6). In detail, two stages are involved in iterations: _(i) approximate VaR\({}_{}[(,)]_{}\) with the meta batch values, which can be achieved via either a quantile estimator (Dong and Nakayama, 2018) or other density estimators; (ii) optimize \(\) in Eq. (6) via stochastic updates after replacing \(_{}\) by the estimated \(_{}\)._

**Proposition 2**: _Suppose there exists \(^{+}\) such that \(|_{}()-_{}()|<\) with \(_{}()\) an estimate of \(_{}()\). Then there exists a constant \(_{}=\{,\}\) such that_

\[_{}(_{}();)-_{} <_{}()_{}(_{} ();).\]

The performance gap resulting from VaR\({}_{}\) approximation error is estimated in Proposition (2). For ease of implementation, we adopt crude Monte Carlo methods (Kroese and Rubinstein, 2012) to obtain a consistent estimator of \(_{}\).

**Theorem 1** (Improvement Guarantee): _Under assumptions (1)/(2)/(3), suppose that the estimate error with the crude Monte Carlo holds: \(|_{_{t}}-_{_{t}}|(1- )^{2}}, t^{+}\), with the subscript \(t\) the iteration number, \(\) the learning rate in stochastic gradient descent, \(_{t}\) the Lipschitz constant of the risk cumulative distribution function, and \(\) the confidence level. Then the proposed heuristic algorithm with the crude Monte Carlo can produce at least a local optimum for distributionally robust fast adaptation._

Note that Theorem (1) indicates that under certain conditions, using the above heuristic algorithm has the performance improvement guarantee, which corresponds to Fig. (2). The error resulting from the approximate algorithm is further estimated in Appendix Theorem (2).

### Instantiations & Implementations

Our proposed optimization strategy applies to all meta learning methods and has the improvement guarantee in Theorem (1). Here we take two representative meta learning methods, MAML (Finn et al., 2017) and CNP (Garnelo et al., 2018), as examples and show how to robustify them through the lens of risk distributions. Note that the forms of \((_{}^{T},_{}^{C};)\) sometimes differ in these methods. Also, the detailed implementation of the strategy relies on specific meta learning algorithms and models.

**Example 1** (Dr-Maml): _With the task distribution \(p()\) and model agnostic meta learning (Finn et al., 2017), the distributionally robust MAML treats the meta learning problem as a bi-level

Figure 2: **Optimization Diagram of Distributionally Robust Meta Learning with Surrogate Functions. From left to right: the meta model parameters \(\) in the middle block are optimized _w.r.t._ the constructed surrogate function \((_{_{t}};)\) marked in blue in \(t\)-th iteration. Under certain conditions in Theorem (1), the distributionally robust meta learning objective \((_{};)\) marked in pink can be decreased monotonically until it reaches the convergence in the \(H\)-th iteration.**optimization with a VaR\({}_{}\) relevant constraint._

\[_{\\ }+_{p()}[ [(_{}^{T};-_{}( _{}^{C};))-]^{+}]\] (7)

_The gradient operation \(_{}(_{}^{C};)\) corresponds to the inner loop with the learning rate \(\)._

The resulting distributionally robust MAML (DR-MAML) is still a optimization-based method, where a fixed percentage of tasks are screened for the outer loop. As shown in Fig. (3) and Eq. (7), the objective is to obtain a robust meta initialization of the model parameter \(\).

**Example 2** (DR-CNP): _With the task distribution \(p()\) and the conditional neural process (Garnelo et al., 2018), the distributionally robust conditional neural process learns the functional representations with a CVaR\({}_{}\) constraint._

\[_{\\ }+_{p()} [[(_{}^{T};z,_{2}))-]^{+}]\] (8) \[z=h_{_{1}}(_{}^{C}) =\{_{1},_{2}\}\]

_Here \(h_{_{1}}\) is a set encoder network with \(_{2}\) the parameter of the decoder network._

The resulting distributionally robust CNP (DR-CNP) is to find a robust functional embedding to induce underlying stochastic processes. Still, in Eq. (8), a proportion of tasks with the worst functional representation performance are used in meta training.

Moreover, we convey the pipelines of optimizing these developed distributionally robust models in Appendix **Algorithms (1)/(2)**.

## 5 Experimental Results and Analysis

This section presents experimental results and examines fast adaptation performance in a distributional sense. Without loss of generality, we take DR-MAML in Example (1) to run experiments.

**Benchmarks.** The same as work in (Collins et al., 2020), we use two commonly-seen downstream tasks for meta learning experiments: few-shot regression and image classification. Besides, ablation studies are included to assess other factors' influence or the proposed strategy's scalability.

**Baselines & Evaluations.** Since the primary investigation is regarding risk minimization principles, we consider the previously mentioned _expected risk minimization_, _worst-case minimization_, and _expected tail risk minimization_ for meta learning. Hence, MAML (empirical risk), TR-MAML (worst-case risk), and DR-MAML (expected tail risk) serve as examined methods. We evaluate these methods' performance based on the _Average_, _Worst-case_, and _CVaR\({}_{}\)_ metrics. For the confidence level to meta train DR-MAML, we empirically set \(=0.7\) for few-shot regression tasks and \(=0.5\) image classification tasks without external configurations.

### Sinusoid Regression

Following (Finn et al., 2017; Collins et al., 2020), we conduct experiments in sinusoid regression tasks. The mission is to approximate the function \(f(x)=a(x-b)\) with K-shot randomly sampled function points, where the task is defined by \(a\) and \(b\). In the sine function family, the target range, amplitude range, and phase range are

   &  &  \\  & Average & Worst & _{}\)} &  & _{}\)} \\  MAML (Finn et al., 2017) & 1.02\(\)0.19 & 3.89\(\)0.02 & 2.25\(\)0.18 & 0.66\(\)0.18 & 2.57\(\)0.18 & 1.15\(\)0.19 \\ TR-MAML (Collins et al., 2020) & 1.09\(\)0.08 & **2.28\(\)0.18** & 1.79\(\)0.08 & 0.77\(\)0.11 & **1.65\(\)0.13** & 1.27\(\)0.23 \\ DR-MAML (Ours) & **0.89\(\)0.08** & **2.91\(\)0.08** & **1.76\(\)0.08** & **0.54\(\)0.08** & 1.70\(\)0.17 & **0.96\(\)0.08** \\  

Table 1: **Test average mean square errors (MSEs) with reported standard deviations for sinusoid regression (5 runs). We respectively consider 5-shot and 10-shot cases with \(=0.7\). The results are evaluated across the 490 meta-test tasks, as in (Collins et al., 2020). The best results are in bold.**

Figure 3: **Diagram of Distributionally Robust Fast Adaptation for Model Agnostic Meta Learning (Finn et al., 2017). For example, with the size of the meta batch 5 and \(=40\%\), \(5*(1-)\) tasks in gray with the worst fast adaptation performance are screened for updating meta initialization.**

respectively \([-5.0,5.0]\), \(a[0.1,5.0]\) and \(b[0,2]\). In the setup of meta training and testing datasets, a distributional shift exists: numerous easy tasks and several difficult tasks are generated to formulate the training dataset with all tasks in the space as the testing one. Please refer to Appendix (J) for a detailed partition of meta-training, testing tasks, and neural architectures.

**Result Analysis.** We list meta-testing MSEs in sinusoid regression in Table (1). As expected, the tail risk minimization principle in DR-MAML can lead to an intermediate performance in the worst-case. In both cases, the comparison between MAML and DR-MAML in MSEs indicates that such probabilistic-constrained optimization in the task space even has the potential to advance average fast adaptation performance. In contrast, TR-MAML has to sacrifice more average performance for the worst-case improvement.

More intuitively, Fig. (4) illustrates MSE statistics on the testing task distribution and further verifies the effect of the CVaR\({}_{}\) principle in decreasing the proportional worst-case errors. In 5-shot cases, the difference in MSE statistics is more significant: _the worst-case method tends to increase the skewness in risk distributions_ with many task risk values gathered in regions of intermediate performance, which is unfavored in general cases. As for why DR-MAML surpasses MAML in terms of average performance, we attribute it to the benefits of external robustness in several scenarios, _e.g._, the drift of training/testing task distributions.

### Few-Shot Image Classification

Here we do investigations in few-shot image classification. Each task is an N-way K-shot classification with N the number of classes and K the number of labeled examples in one class. The Omniglot (Lake et al., 2015) and _mini_-ImageNet (Vinyals et al., 2016) datasets work as benchmarks for examination. We retain the setup of datasets in work (Collins et al., 2020).

**Result Analysis.** The classification accuracies in Omniglot are illustrated in Table (2): In 5-way 1-shot cases, DR-MAML obtains the best average and CVaR\({}_{}\) in meta-testing datasets, while TR-MAML achieves the best worst-case performance with a slight degradation of average performance compared to MAML in both training/testing datasets. In 20-way 1-shot cases, for training/testing datasets, we surprisingly notice that the expected tail risk is not well optimized with DR-MAML, but there is an average performance gain; while TR-MAML works best in worst-case/CVaR\({}_{}\) metrics.

When it comes to _mini_-ImageNet, findings are distinguished a lot from the above one: in Table (3), DR-MAML yields the best result in all evaluation metrics and cases, even regarding the worst-case. TR-MAML can also improve all metrics in meta-testing cases. Overall, challenging meta-learning tasks can reveal more advantages of DR-MAML over others.

   &  &  \\   &  &  &  &  \\  & Average & Worst & CVaR\({}_{}\) & Average & Worst & CVaR\({}_{}\) & Average & Worst & CVaR\({}_{}\) & Average & Worst & CVaR\({}_{}\) \\  MAML (Finn et al., 2017) & **98.4\({}_{}\)** & 82.4\({}_{}\)** & 96.9\({}_{}\)** & 99.2\({}_{}\)** & 31.9\({}_{}\)** & 80.9\({}_{}\)** & 99.3\({}_{}\)** & 93.5\({}_{}\)** & 82.5\({}_{}\)** & 91.6\({}_{}\)** & 67.6\({}_{}\)** & 49.7\({}_{}\)** & 60.4\({}_{}\)** \\ TR-MAML (Collins et al., 2020) & 97.4\({}_{}\)** & **95.0\({}_{}\)** & 96.5\({}_{}\)** & 99.2\({}_{}\)** & **99.2\({}_{}\)** & **82.4\({}_{}\)** & **87.2\({}_{}\)** & 93.1\({}_{}\)** & **85.3\({}_{}\)** & 91.3\({}_{}\)** & 74.3\({}_{}\)** & **58.4\({}_{}\)** & **68.5\({}_{}\)** & 11.2 \\ DR-MAML (Ours) & **97.1\({}_{}\)** & 84.0\({}_{}\)** & 95.1\({}_{}\)** & **99.6\({}_{}\)** & 57.9\({}_{}\)** & 84.8\({}_{}\)** & 98.7\({}_{}\)** & 84.1\({}_{}\)** & **92.1\({}_{}\)** & **74.6\({}_{}\)** & 87.6\({}_{}\)** & 87.4\({}_{}\)** & 87.6\({}_{}\)** & 87.4\({}_{}\)** \\  

Table 2: **Average N-way K-shot classification accuracies in Omniglot with reported standard deviations (3 runs). With \(=0.5\),**

Figure 4: **Histograms of Meta-Testing Performance in Sinusoid Regression Problems. With \(=0.7\), we respectively visualize the comparison results, DR-MAML-vs-MAML and TR-MAML-vs-MAML in 5-shot (Two Sub-figures Left Side) and 10-shot (Two Sub-figures Right Side) cases, for a sample trial.**

### Ablation Studies

This part mainly checks the influence of the confidence level \(\), meta batch size, and other optimization strategies towards the distribution of fast adapted risk values. Apart from examining these factors of interest, additional experiments are also conducted in this paper; please refer to Appendix (K).

**Sensitivity to Confidence Levels \(\).** To deepen understanding of the confidence level \(\)'s effect in fast adaptation performance, we vary \(\) to train DR-MAML and evaluate models under previously mentioned metrics. Taking the sinusoid 5-shot regression as an example, we calculate MSEs and visualize the fluctuations with additional \(\)-values in Fig. (5). The results in the worst-case exhibit higher deviations. The trend in Average/CVaR\({}_{}\) metrics shows that with increasing \(\), DR-MAML gradually approaches TR-MAML in average and CVaR\({}_{}\) MSEs. With \( 0.8\), ours is less sensitive to the confidence level and mostly beats MAML/TR-MAML in average performance. Though ours aims at optimizing CVaR\({}_{}\), it cannot always ensure such a metric to surpass TR-MAML in all confidence levels due to rigorous assumptions in Theorem (1).

**Influence of the Task Batch Size.** Note that our optimization strategy relies on the estimate of VaR\({}_{}\), and the improvement guarantee relates to the estimation error. Theoretically, the meta batch in training can directly influence the optimization result. Here we vary the meta batch size in training, and evaluated results are visualized in Fig. (6). In regression scenarios, the average MSEs can decrease to a certain level with an increase of meta batch, and then performance degrades. We attribute the performance gain with a batch increase to more accurate VaR\({}_{}\) estimates; however, a meta batch larger than some threshold can worsen the first-order meta learning algorithms' efficiency, similarly observed in (Nichol et al., 2018). As for classification scenarios, there appears no clear trend since the meta batch is smaller enough.

**Comparison with Other Optimization Strategies.** Note that instantiations of distributionally robust meta learning methods, such as DR-MAML and DR-CNPs in Examples (1)/(2) are regardless of optimization strategies and can be optimized via any heuristic algorithms for CVaR\({}_{}\) objectives.

Additionally, we use DR-MAML as the example and perform the comparison between our two-stage algorithm and the risk reweighted algorithm (Sagawa et al., 2020). The intuition of the risk reweighted algorithm is to relax the weights of tasks and assign more weights to the gradient of worst cases. The normalization of risk weights is achieved via the softmax operator. Though there is an improvement guarantee _w.r.t._ the probabilistic worst group of tasks, the algorithm is not specially designed for meta learning or CVaR\({}_{}\) objective.

\[_{}_{}():=_{p( ;)}(;)}{p(;)}( _{}^{T},_{}^{C};)}_{b=1}^{}_{b}(_{b};)( _{_{b}}^{T},_{_{b}}^{C};)\] (9)

Figure 5: **Meta Testing MSEs of Meta-Trained DR-MAML with Various Confidence Levels \(\). MAML and TR-MAML are irrelevant with the variation of \(\) in meta-training. The plots report testing MSEs with standard error bars in shadow regions.**

Figure 6: **Meta Testing Average Performance of Meta-Trained DR-MAML with Various Sizes of the Meta Batch. The plots report average results with standard error bars in shadow regions.**Meanwhile, the weight of task gradients after normalization is a biased estimator _w.r.t._ the constrained probability \(p_{}(;)\) in the task space. In other words, the risk reweighted method can be viewed as approximation _w.r.t._ the importance weighted method in Eq. (9). In the importance weighted method, for tasks out of the region of \((1-)\)-proportional worst, the probability of sampling such tasks \(_{b}\) is zero, indicating \(_{b}(_{b};)=0\). While in risk reweighted methods, the approximate weight is assumed to satisfy \(_{b}(_{b};)(_{_{ a}}^{T},_{_{b}}^{C};)}{})\), where \(\) means last time updated meta model parameters and the risk function value is evaluated after fast adaptation.

In implementations, we keep the setup the same as Group DRO methods in (Sagawa et al., 2020) for meta-training. As illustrated in Table (4)/(5), DR-MAML with the two-stage optimization strategies consistently outperform that with the Group DRO ones in both 5-shot and 10-shot sinusoid cases regarding all metrics. The performance advantage of using the two-stage ones is not significant in _mini_-ImageNet scenarios. We can hypothesize that the estimate of VaR\({}_{}\) in continuous task domains, _e.g._, sinusoid regression, is more accurate, and this probabilistically ensures the improvement guarantee with two-stage strategies. Both the VaR\({}_{}\) estimate in two-stage strategies and the importance weight estimate in the Group DRO ones may have a lot of biases in few-shot image classification, which lead to comparable performance.

## 6 Conclusion and Limitations

**Technical Discussions.** This work contributes more insights into robustifying fast adaptation in meta learning. Our utilized expected tail risk trades off the expected risk minimization and worst-case risk minimization, and the two-stage strategy works as the heuristic to approximately solve the problem with an improvement guarantee. Our strategy can empirically alleviate the worst-case fast adaptation and sometimes even improve average performance.

**Existing Limitations.** Though our robustification strategy is simple yet effective in implementations, empirical selection of the optimal meta batch size is challenging, especially for first-order optimization methods. Meanwhile, the theoretical analysis only applies to a fraction of meta learning tasks when risk function values are in a compact continuous domain.

**Future Extensions.** Designing a heuristic algorithm with an improvement guarantee is non-trivial and relies on the properties of risk functions. This research direction has practical meaning in the era of large models and deserves more investigations in terms of optimization methods. Also, establishing connections between the optimal meta batch size and specific stochastic optimization algorithms can be a promising theoretical research issue in this domain.