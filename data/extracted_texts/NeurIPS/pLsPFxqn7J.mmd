# Kernelized Cumulants: Beyond Kernel Mean Embeddings

Patric Bonnier\({}^{1*}\)  Harald Oberhauser \({}^{1}\)  Zoltan Szabo\({}^{2}\)

\({}^{1}\)Mathematical Institute, University of Oxford \({}^{2}\)Department of Statistics, London School of Economics

bonnier,oberhauser@maths.ox.ac.uk

z.szabo@lse.ac.uk

###### Abstract

In \(^{d}\), it is well-known that cumulants provide an alternative to moments that can achieve the same goals with numerous benefits such as lower variance estimators. In this paper we extend cumulants to reproducing kernel Hilbert spaces (RKHS) using tools from tensor algebras and show that they are computationally tractable by a kernel trick. These kernelized cumulants provide a new set of all-purpose statistics; the classical maximum mean discrepancy and Hilbert-Schmidt independence criterion arise as the degree one objects in our general construction. We argue both theoretically and empirically (on synthetic, environmental, and traffic data analysis) that going beyond degree one has several advantages and can be achieved with the same computational complexity and minimal overhead in our experiments.

kernel, cumulant, mean embedding, Hilbert-Schmidt independence criterion, kernel Lancaster interaction, kernel Streitberg interaction, maximum mean discrepancy, maximum variance discrepancy

## 1 Introduction

The moments of a random variable are arguably the most popular all-purpose statistic. However, cumulants are often more favorable statistics than moments. For example, if \(_{m}[X^{m}]\) denotes the moments of a real-valued random variable \(X\), then \(_{2}=_{1}^{2}+(X)\) and hence the variance that directly measures the fluctuation around the mean is a much better statistic for scale than the second moment \(_{2}\), see Appendix A. Cumulants provide a systematic way to only record the parts of the moment sequence that are not already captured by lower-order moments. While the moment and cumulant sequences \((_{m})_{m}\) and \((_{m})_{m}\) carry the same information, cumulants have several desirable properties that generalize to \(^{d}\)-valued random variables (McCullagh, 2018). Among these properties of cumulants, the ones that are important for our paper is that they can characterize distributions and statistical (in)dependence.

Kernel embeddings.Mean and covariance arise naturally in the context of kernel-enriched domains. Kernel techniques (Scholkopf and Smola, 2002; Steinwart and Christmann, 2008; Saitoh and Sawano, 2016) provide a principled and powerful approach for lifting data points to a so-called reproducing kernel Hilbert space (RKHS; Aronszajn 1950). Considering the mean of this feature - referred to as kernel mean embedding (KME; Berlinet and Thomas-Agnan 2004; Smola et al. 2007) - also enables one to represent probability measures, and to induce a semi-metric referred to as maximum mean discrepancy (MMD; Smola et al. 2007; Gretton et al. 2012) and an independence measure called theHilbert-Schmidt independence criterion (HSIC1; Gretton et al. 2005). It is known that MMD is a metric when the underlying kernel is _characteristic_(Fukumizu et al., 2008; Sriperumbudur et al., 2010). HSIC captures independence for \(d=2\) components with characteristic kernels (Lyons, 2013, Theorem 3.11) and for \(d>2\) components (Quadrianto et al., 2009; Sejdinovic et al., 2013a; Pfister et al., 2018) with universal ones (Szabo and Sriperumbudur, 2018). MMD belongs to the family of integral probability metrics (IPM; Zolotarev 1983; Muller 1997) when in the IPM the underlying function class is chosen to be the unit ball of the RKHS. MMD and HSIC (with \(d=2\)) are known to be equivalent (Sejdinovic et al., 2013b) to the notions of energy distance (Baringhaus and Franz, 2004; Szekely and Rizzo, 2004, 2005)--also called N-distance (Zinger et al., 1992; Klebanov, 2005)- and distance covariance (Szekely et al., 2007; Szekely and Rizzo, 2009; Lyons, 2013) of the statistics literature. Both MMD and HSIC can be expressed in terms of expectations of kernel values which can be leveraged to design efficient estimators for them; we will refer to this trick as the _expected kernel_ trick. A recent survey on mean embedding and their applications is given by Muandet et al. (2017). The closest previous work to ours is by Makigusa (2020) who considers the variance in the RKHS - which in our setting can be identified as the first kernelized cumulant after the kernel mean embedding - for two-sample testing like we do in parts of this paper. Unfortunately, Makigusa (2020) does not provide conditions on the validity of the resulting maximum variance discrepancy, and it is not formulated in the context of cumulant embeddings.

Contribution.The main contribution of our paper is to introduce cumulants of random variables in RKHSs and to show that under mild conditions the proposed kernelized cumulants characterize distributions (Theorem 2) and independence (Theorem 3). Thanks to the RKHS formulation, kernelized cumulants have computable estimators (Lemma 2 and Lemma 3) and they show strong performance in two-sample and independence testing on various benchmarks (Section 4). Although cumulants are a classic tool in multi-variate statistics, they have not received attention in the kernel literature. The primary technical challenge to circumvent in the derivation of fundamental properties of cumulants is the rich combinatorial structure which already arises in \(^{d}\) from their definition via moment-generating function which is closely linked to the partition lattice (Speed, 1983, 1984). In an RKHS, even the definition of cumulants is non-straightforward. The key insight for our extension is that the combinatorial expressions for cumulants in \(^{d}\) can be generalized by using tools from tensor algebras. This in turn allows us to derive the main properties of the RKHS cumulants that underpin their statistical properties.

Broader impact & limitations.We do not see any direct negative societal impact arising from the proposed new set of all-purpose kernel-based divergence and dependence measure. Choosing the underlying kernels in an optimal fashion--even for MMD in two-sample testing (Hagrass et al., 2022) or goodness-of-fit testing (Hagrass et al., 2023)--and showing optimal rates--even for MMD with radial kernels on \(^{d}\)(Tolstikhin et al., 2016)-- are quite challenging problems requiring dedicated analysis, and are not addressed here.

Outline.The paper is structured as follows: In Section 2 we formulate the notion of cumulants of random variables in Hilbert spaces. In Section 3 we prove a kernelized version of the classical result of \(^{d}\)-valued random variables on the characterization of distributions and independence using cumulants. We show that one can leverage the expected kernel trick to derive efficient estimators for our novel statistics, with MMD and HSIC arising specifically as the "degree \(1\)" objects. In Section 4 we demonstrate numerically that going beyond degree 1 is advantageous. We provide a technical background (on cumulants, tensor products and tensor algebras), proofs, further details on numerical experiments, and our V-statistic based estimators in the Appendices.

## 2 Moments and cumulants

We briefly revisit classical cumulants and define cumulants of random variables in Hilbert spaces.

[MISSING_PAGE_FAIL:3]

_for every \(^{d}\) whenever the above expectation exists. The moment sequence is defined as the element_

\[()=(^{}())_{^{d}}_{1}_{d},_{j}_{m 0}_{j}^{  m},\]

_and for \(m\) we refer to \(^{m}()=_{^{d}:()=m}^ {}()\) as the \(m\)-moments of \(\)._

In case of \(_{i}=\), both definitions (1) and (3) apply for \(^{}()\). Henceforth, we always refer to (3) when we write \(^{}()\). Even in the finite-dimensional case, Def. 1 is useful, for instance when \(X_{1}_{1}\) and \(X_{2}_{2}\) have different state space (\(_{1}_{2}\)).

## 3 Kernelized cumulants

We lift a random variable \(X\!=(X_{1},,X_{d})=_{1} _{d}\) via a feature map \(:\) into a Hilbert space valued random variable \((X)\). For the rest of the paper (i) \(_{1},,_{d}\) will denote a collection of Polish spaces, but the reader is invited to think of them as finite-dimensional Euclidean spaces, (ii) \(\) is an RKHS with kernel \(k\) and canonical feature map \((x)=k(x\,,)\),2 and (iii) all kernels are assumed to be bounded.3 Our main results (Theorem 2 and Theorem 3) are that in this case the expected kernel trick applies to both items in the kernelized version of Theorem 1. The key to these results is an expression for inner products of cumulants in RKHSs (Lemma 1).

A combinatorial expression of cumulants.Classical cumulants can be defined via the moment generating function or via combinatorial sums over _partitions_ (Appendix C.1). To generalize cumulants to RKHSs the combinatorial definition is the most efficient way. A partition \(\) of \(m\) elements is a family of non-empty, disjoint subsets \(_{1},,_{b}\) of \(\{1,,m\}\) whose union is the whole set; formally \(_{j=1}^{b}_{j}=\{1,,m\}\) and \(_{i}_{j}=\) for \(i j\). We call \(b\) the number of blocks of the partition \(\) and use the shorthand \(||\) to denote it. The set of all partitions of \(m\) is denoted with \(P(m)\). To formulate our main results, it is convenient to associate with a measure \(\) and a partition \(\) the so-called partition measure \(_{}\) that is given by permuting the marginals of \(\).

**Definition 2** (Partition measure).: _Let \(\) be a probability measure on \(_{1}_{d}\) and \( P(d)\). Define_

\[_{}\,=|_{_{_{1}}} |_{_{_{b}}},\]

_where \(_{_{i}}\) denotes the product space \(_{j_{i}}_{j}\) and \(|_{_{_{i}}}\) is the corresponding marginal distribution of \(\). We call \(_{}\) the partition measure induced by \(\)._

We also associate with \(\) and a multi-index \(\) the so-called diagonal measure \(^{}\) that is given by repeating marginals according to \(\).

**Definition 3** (Diagonal measure).: _Let \(\) be a probability measure on \(_{1}_{d}\) and \(=(i_{1},,i_{d})^{d}\). Define_

\[^{}\,=(,,X_{1}}_{i_{1 }},,,X_{2}}_{i_{2}},, ,,X_{d}}_{i_{d}}),\]

_where \((X_{1},,X_{d})\). We call \(^{}\) the diagonal measure induced by \(\)._

In general, the partition measure \(_{}\) and the diagonal measure are not probability measures on \(_{1}_{d}\) but on spaces that are constructed by permuting or repeating \(_{1},,_{d}\). Formally, \(_{}\) is a probability measure on \(_{_{1}}_{_{b}}\) and \(^{}\) is a probability measure on \(_{1}^{i_{1}}_{d}^{i_{d}}\); thus, \(_{}\) has \(d\) coordinates and \(^{}\) has \(()\) coordinates. These two constructions can be combined, writing \(^{}_{}\) for the measure \((^{})_{}\) which makes sense whenever \( P(())\). We can now write down our generalization of cumulants.

**Definition 4** (Kernelized cumulants).: _Let \(\) be a probability measure on \(_{1}_{d}\) and let \((_{1},k_{1}),,(_{d},k_{d})\) be RKHSs on \(_{1},,_{d}\) respectively. We define the kernelized cumulants_

\[_{k_{1},,k_{d}}()^{}_{k_{1}, ,k_{d}}()_{^{d}}\]

_as follows_

\[^{}_{k_{1},,k_{d}}()_{ P(m)}c_ {}_{^{}_{}}k^{ 1}((X_{1},,X_{m}), ),\]

_where \(m=()\), \(c_{}(-1)^{||-1}(||-1)!\), \(^{}_{}=(^{})_{}\) and_

\[k^{ 1}((x_{1},,x_{m}),(y_{1},,y_{m})) =k_{1}(x_{1},y_{1}) k_{1}(x_{i_{1}},y_{i_{1}})\] (4) \[ k_{d}(x_{m-i_{d}+1},y_{m-i_{d}+1}) k_{d}(x_{m},y_{m})\]

_is the reproducing kernel of \(^{ 1}\) where \(=_{1}_{d}\)._

Def. 4 is the natural generalization of the combinatorial definition of cumulants in \(^{d}\) and Appendix C.2 gives an equivalent definition via a generating function analogous to (2). However, our posthoc justification that these are the "right" definitions for cumulants in an RKHS are Theorems 2 and 3 that show that these kernelized cumulants have the same powerful properties as classic cumulants in \(^{d}\) (Theorem 1).

**Example 3.1** (Kernelized cumulants).: _Let \(\) be a probability measure on \(_{1}_{2}\), with the RKHSs \((_{1},k_{1}),(_{2},k_{2})\) given. Denote the random variables \(K_{1}=k_{1}(X_{1},),K_{2}=k_{2}(X_{2},)\) where \((X_{1},X_{2})\). Then the degree two kernelized cumulants are given as \(^{(2,0)}_{k_{1},k_{2}}()=[K_{1}^{ 2}]- [K_{1}]^{ 2}\), \(^{(1,1)}_{k_{1},k_{2}}()=[K_{1} K_{2} ]-[K_{1}][K_{2}],^ {(0,2)}_{k_{1},k_{2}}()=[K_{2}^{ 2}]- [K_{2}]^{ 2}\)._

Inner products of cumulants.Computing inner products of moments is straightforward thanks to a nonlinear kernel trick, see Lemma 6 in the Appendix. For example, given two probability measures \(_{1},_{2}\) with corresponding random variables \((X_{1},,X_{d})_{1},(Y_{1},,Y_{d})_{2}\) on \(_{1}_{d}\) and RKHSs \((_{1},k_{1}),,(_{d},k_{d})\) on \(_{1},,_{d}\) with bounded kernels, and \(=_{1}_{d}\), we can express:

\[^{}_{k_{1},,k_{d}}(_{1}),^{}_{k_{ 1},,k_{d}}(_{2})_{^{ 1}}=_{ _{1}_{2}}k_{1}(X_{1},Y_{1})^{i_{1}} k_{d}(X_{d},Y_{d })^{i_{d}},\] (5)

where \(^{}_{k_{1},,k_{d}}\) is defined in Def. 1, and the expectation is taken over the product measure \(_{1}_{2}\).

**Example 3.2**.: _In the particular case of \(d=1\), (5) reduces to the well-known formula for the inner product of mean embeddings \(^{(1)}_{k}(_{1}),^{(1)}_{k}(_{2})_{ _{k}}=_{_{1}_{2}}k(X,Y)\)._

**Lemma 1** (Inner product of cumulants).: _Let \((_{1},k_{1}),,(_{d},k_{d})\) be RKHSs with bounded kernels on \(_{1},,_{d}\) respectively, and let \(\) and \(\) two probability measures on \(_{1}_{d}\), \(=(i_{1},,i_{d})^{d}\) such that \(()=m\). Then_

\[^{}_{k_{1},,k_{d}}(),^{}_{k_{ 1},,k_{d}}()_{^{ 1}}=_{, P(m)}c_{}c_{} _{^{}_{}^{}_{}}k^{  1}((X_{1},,X_{m}),(Y_{1},,Y_{m})).\]

Point separating kernels.In the classic MMD setting the injectivity of the mean embedding \(_{X}[k(X,)]\) on probability measures (known as the characteristic property of the kernel \(k\)) is equivalent to the MMD being a metric; this property is central in applications. We formulate our theoretical results in the next section using the much weaker property of what we term "point-separating" which is satisfied for essentially all popular kernels.

**Definition 5** (Point-separating kernel).: _We call a kernel \(k:\) point-separating if the canonical feature map \(:x k(x,)\) is injective._

### (Semi-)metrics for probability measures

In this section we use cumulants to characterize probability measures and show how to compute the distance between kernelized cumulants with the expected kernel trick.

**Theorem 2** (Characterization of distributions with cumulants).: _Let \(\) and \(\) be two probability measures on \(_{1}_{d}\), \((_{1},k_{1}),,(_{d},k_{d})\) RKHSs on the Polish spaces \(_{1},,_{d}\) such that for every \(1 j d\)\(k_{j}\) is a bounded, continuous, point-separating kernel. Then_

\[=_{k_{1},,k_{d}}()=_{k_{1}, ,k_{d}}().\]

_Moreover, the expected kernel trick applies and for \(^{d}\) with \(()=m\), and \(k^{ 4}\) and \(^{ 1}\) as in (4)_

\[d^{}(,) \|^{}_{k_{1},,k_{d}}()- ^{}_{k_{1},,k_{d}}()\|^{2}_{^{ 4}}\] (6) \[=_{, P(m)}\!\!\!c_{}c_{}_{ ^{}_{}^{}_{}}k^{ }((X_{1},,X_{m}),(Y_{1},,Y_{m}))\] \[+_{^{}_{} ^{}_{}}k^{}((X_{1},,X_{m}),(Y_{1}, ,Y_{m}))\] \[-2_{^{}_{} ^{}_{}}k^{}((X_{1},,X_{m}),(Y _{1},,Y_{m})).\]

We recall Example 3.1 and now give examples of distances between such expressions

**Example 3.3** (\(m=1\)).: _Applied with \(m=1\) and \(d=1\), (6) becomes \(^{2}_{k}(,)\)_

\[\|^{(1)}_{k}()-^{(1)}_{k}()\|^{2}_{_{k}}= k(X,X^{})+k(Y,Y^{})-2k(X,Y),\]

_where \(X,X^{}\) denotes independent copies of \(\) and \(Y,Y^{}\) denotes independent copies of \(\)._

**Example 3.4** (\(m=2\)).: _For \(m=2\) and \(d=1\), (6) reduces to_

\[\|^{(2)}_{k}()-^{(2)}_{k}()\|^{2}_{ ^{(1,1)}}=k(X,X^{})k(X^{},X^{ })+k(Y,Y^{})k(Y^{},Y^{ })+k(X,X^{})^{2}\] \[+k(Y,Y^{})^{2}+2k(X,Y)k(X ^{},Y)+2k(X,Y)k(X,Y^{})-2k(X,Y)k(X^{},Y ^{})\] \[-2k(X,Y)^{2}-2k(X,X^{ })k(X,X^{})-2k(Y,Y^{})k(Y,Y^{}),\]

_where \(X,X^{},X^{},X^{}\) denotes independent copies of \(\) and \(Y,Y^{},Y^{},Y^{}\) denotes independent copies of \(\). This expression compares the variances in the RKHS instead of the means. This is an example of the kernel variance embedding defined in the next subsection._

The price for the weak assumption of a point-separating kernel is that without any stronger assumptions one does not get a metric in general, and the all-purpose way to achieve a metric is to take an infinite sum over all \(d^{i}\)'s. If we only use the degree \(m=1\) term \(d^{}\) reduces to the well-known MMD formula which requires characteristicness to become a metric (see Example 3.3). There are two reasons why working under weaker assumptions is useful: firstly, if the underlying kernel is not characteristic this sum gives a structured way to incorporate finer information that discriminates the two distributions; an extreme case is the linear kernel \(k(x,y)= x,y\) which is point-separating, and in this case the sum reduces to the differences of classical cumulants. Secondly, under the stronger assumption of characteristicness one already has a metric after truncation at degree \(m=1\) (the classical MMD). However, in the finite-sample case adding higher degree terms can lead to increased power. Indeed, our experiments (Section 4) show that even just going one degree further (i.e. taking \(m=2\)), can lead to more powerful tests.

### A characterization of independence

Here we characterize independence in terms of kernelized cumulants.

**Theorem 3** (Characterization of independence with cumulants).: _Let \(\) be a probability measure on \(_{1}_{d}\), and \((_{1},k_{1}),,(_{d},k_{d})\) RKHSs on Polish spaces \(_{1},,_{d}\) such that for every \(1 j d\)\(k_{j}\) is a bounded, continuous, point-separating kernel. Then_

\[=|_{_{1}}|_{_{d}} ^{}_{k_{1},,k_{d}}()=0\]

_for every \(^{d}_{+}\). Moreover, the expected kernel trick applies in the sense that for \(^{d}_{+}\)_

\[\|^{}_{k_{1},,k_{d}}()\|^{2}_{^{ 4}}= _{, P(m)}\!\!c_{}c_{}_{^{}_{ }^{}_{}}k^{}((X_{1},,X_{m }),(Y_{1},,Y_{m})),\] (7)

_where \(m()\), and \(k^{ 4}\) and \(^{ 4}\) are defined as in (4)._Applied to \(=(1,1)\), the expression (7) reduces to the classical HSIC for two components, see Example 3.5 below. But for general \(\) this construction leads to genuine new statistics in RKHSs.

**Example 3.5** (Specific case: HSIC, kernel Lancaster interaction, kernel Streitberg interaction).: _If \(d=2\) there is only one order \(2\) index in \(_{+}^{d}\), namely \(=(1,1)\); in this case (7) reduces to the classical HSIC equation_

\[\|_{k_{1},k_{2}}^{(1,1)}()\|_{^{(1,1)}}^{2} =k_{1}(X,Y)k_{2}(X,Y)+k_{1}(X,Y)k_{2}(X^{},Y^{ })-2k_{1}(X,Y)k_{2}(X^{},Y),\]

_where \((X,Y)\) and \((X^{},Y^{})\) are independent copies of the same random variable following \(\). More generally, with \(=_{d}\) one gets the kernel Streitberg interaction (Streitberg, 1990; Sejdinovic et al., 2013a; Liu et al., 2023), and specifically the kernel Lancaster interaction (Sejdinovic et al., 2013a) for \(d\{2,3\}\); the latter reduces to HSIC for two random variables (\(d=2\))._

### Finite-sample statistics

To apply Theorem 2 and Theorem 3 in practice, one needs to estimate expressions such as \(k^{ 1}((X_{1},,X_{m}),(Y_{1},,Y_{m}))\). One could use classical estimators such as _U-statistic_(Van der Waart, 2000) which lead to unbiased estimators. However, we follow Gretton et al. (2008) and use a _V-statistic_ which is biased but conceptually simpler, easier, and efficient to compute. We note that the estimators presented here all have quadratic complexity like MMD and HSIC, see Appendix E.

A two-sample test for non-characteristic feature maps.If \(k\) is characteristic then \(_{k}(,)=0\) exactly when \(=\), but we can still increase testing power by considering the distance between the kernel variance and skewness embeddings, which leads us to use our semi-metrics \(d^{(2)}(,)\) and \(d^{(3)}(,)\) as defined in (6). An efficient estimator for \(d^{(3)}\) is given in detail in Appendix E; we provide the full expression for \(d^{(2)}\) here.

**Lemma 2** (\(d^{(2)}\) estimation, see (6)).: _The V-statistic for \(d^{(2)}(,)=\|_{k}^{(2)}()-_{k}^{(2)}()\|_{ ^{(1,1)}}^{2}\) is_

\[}(_{x}_{N})^{2}+ }(_{y}_{M})^{2}- _{xy}_{M}_{xy}^{ }_{N},\]

_where \(\) denotes trace, \((x_{n})_{n=1}^{N}}}{{}}\), \((y_{m})_{m=1}^{M}}}{{}}\), \(_{x}=[k(x_{i},x_{j})]_{i,j=1}^{N}^{N N}\), \(_{y}=[k(y_{i},y_{j})]_{i,j=1}^{M}^{M M}\), \(_{x,y}=[k(x_{i},y_{j})]_{i,j=1}^{N,M}^{N M}\), \(_{n}=_{n}-_{n}_{n}^{} ^{n n}\), with \(_{n}=(1,,1)^{n}\)._

A kernel independence test.By Theorem 3, if \(=|_{_{1}}|_{_{2}}\), then \(^{(2,1)}()=0\) and \(^{(1,2)}()=0\). We may compute the magnitude of either \(^{(2,1)}()\) or \(^{(1,2)}()\) - we will refer to these quantities as _cross skewness independence criterion_ (CSIC). Note that these criteria are asymmetric. When \(d=2\) we have a probability measure \(\) on \(_{1}_{2}\) and two kernels \(k:_{1}^{2}\), \(:_{2}^{2}\). Assume that we have samples \((x_{i},y_{i})_{i=1}^{N}\) and use the shorthand notation \(=_{x},=_{y}\) (similarly to Lemma 2) and \(=_{N}=_{N}_{N}^{} ^{N N}\). Denote by \(\) the Hadamard product and \(\) the sum over all elements of a matrix. Then one can derive the following CSIC estimator.(Note that matrix multiplication takes precedence over the Hadamard product.)

**Lemma 3** (CSIC estimation).: _The V-statistic for \(\|_{k,}^{(1,2)}()\|_{_{k}^{ 1}_{}^{  2}}^{2}\) is_

\[} -4-2+4 \] \[+2}{N^{2}} +2+4 +}{N^{2}}\] \[-8}{N^{2}} -4}{N^{2}} +4}{N^{2}}^{2}.\]

Remark (computational complexity w.r.t. degree \(m\)).We saw that the computational complexity of the cumulant based measures is quadratic w.r.t. the sample size. Let \(B_{m}=|P(m)|\) be the \(m\)-thBell number, in other words the number of elements in \(P(m)\). The Bell numbers follow a recursion: \(B_{m+1}=|P(m+1)|=_{k=0}^{m}B_{k}\), with the first elements of the sequence being \(B_{0}=B_{1}=1\), \(B_{2}=2\), \(B_{3}=5\), \(B_{4}=15\), \(B_{5}=52\). By (6)-(7), in the worst case the number of operations to compute \(d^{}(,)\) or \(\|^{}_{k_{1},,k_{d}}()\|_{^{ i} }\) (\(m=()\)) is proportional to \(B_{m}^{2}\) (it equals to \(3B_{m}^{2}\) and to \(B_{m}^{2}\), respectively). Though asymptotically \(B_{m}\) grows quickly (de Bruijn, 1981; Lovasz, 1993), for reasonably small degrees the computation is still manageable. In addition, merging various terms in the estimator can often be carried out, which leads to computational saving. For instance, the estimator of \(d^{(2)}\) (see Lemma 2, Example E.1), CSIC (Lemma 3, Example E.2) and \(d^{(3)}\) (Example E.3) consists of only \(3\), \(11\) and \(10+2 7=24\) terms compared to the predicted worst-case setting of \(3B_{2}^{2}=12\), \(B_{3}^{2}=25\), and \(3B_{3}^{2}=75\) terms, respectively. On a practical side, we found that using \(m\{2,3\}\) is a good compromise between gain in sample efficiency and ease of implementation.

## 4 Experiments

In this section, we demonstrate the efficiency of the proposed kernel cumulants in two-sample and independence testing.4

* Two-sample test: Given \(N-N\) samples from two probability measures \(\) and \(\) on a space \(\), the goal was to test the null hypothesis \(H_{0}:=\) against the alternative \(H_{1}:\). The compared test statistics (\(S\)) were MMD, \(d^{(2)}\), and \(d^{(3)}\).
* Independence test: Given \(N\) paired samples from a probability measure \(\) on a product space \(_{1}_{2}\), the aim was to the test the null hypothesis \(H_{0}:=_{1}_{2}\) against the alternative \(H_{1}:_{1}_{2}\). The compared test statistics (\(S\)) were HSIC and CSIC.

In our experiments \(H_{1}\) held, and the estimated power of the tests is reported. Permutation test was applied to approximate the null distribution and its \(0.95\)-quantile (which corresponds to the level choice \(=0.05\)): We first computed our test statistic \(S\) using the given samples (\(S_{0}=S\)), and then permuted the samples \(100\) times. If \(S_{0}\) was in a high percentile (\( 95\%\) in our case) of the resulting distribution of \(S\) under the permutations, we rejected the null. We repeated these experiments \(100\) times to estimate the power of the test. This procedure was in turn repeated 5 times and the 5 samples are plotted as a box plot along with a line plot showing the mean against the number of samples \((N)\) used. All experiments were performed using the rbf-kernel \(_{}(,)=e^{-- |^{2}_{}}{2^{2}}}\), where the parameter \(\) is called the _bandwidth_. We performed all experiments for every bandwidth of the form \(=a10^{b}\) where \(a=1,2.5,5,7.5\) and \(b=-5,-4,-3,-2,-1,0\) and the optimal value across the bandwidths was chosen for each method and sample size. The experiments were carried out on a laptop with an i7 CPU and 16GBs of RAM.

### Synthetic data

For synthetic data we designed two experiments.

* 2-sample test: We compared a uniform distribution with a mixture of two uniforms.
* Independence test: We considered the joint measure of a uniform and a correlated \(^{2}\) random variable. We also use this same benchmark to compare the efficiency of classical and kernelized cumulants in Appendix D.

Comparing a uniform with a mixture of uniforms.Even for simpler distributions like mixtures of uniform distributions it can be hard to pick up higher-order features, and \(d^{(2)}\) can outperform MMD even when provided with a moderate number of samples. Here we compared one uniform distribution \(U[-1,1]\) with an equal mixture of \(U[0.35,0.778]\) and \(U[-0.35,-0.778]\). The endpoints in the mixture were chosen to match the first three moments of \(U[-1,1]\). The number of samples used ranged from \(5\) to \(50\), and the results are summarized in Fig. 1. One can see that with \(d^{(2)}\) the power approaches \(100\%\) much faster than with using MMD.

[MISSING_PAGE_FAIL:9]

which is admittedly slightly higher than the desired \(5\%\) due to the small sample size, but very similar for both statistics; for further details, the reader is referred to Fig. 5 in Appendix D.

Brazilian traffic data.We used the Sao Paulo traffic benchmark (Ferreira, 2016) to perform independence testing. The dataset consists of 16 different integer-valued statistics about the hourly traffic in Sao Paulo such as blockages, fires and other reasons that might hold up traffic. This is combined with a number that describes the slowness of traffic at the given hour; so \(_{1}=^{16}\), \(_{2}=\). One expects a strong dependence between the two sets--or equivalently, for the null hypothesis to be false--and for the statistics are heavily skewed towards \(0\) as it is naturally sparse. For independence testing we performed permutation testing for \(N\) between \(4\) and \(40\). The resulting test powers are summarized in Fig. 5. As it can be seen, HSIC and CSIC performs similarly for very low sample sizes, but for anything else CSIC is the favorable statistic in terms of test power. For two-sample testing, we sampled \(N\) between \(5\) and \(50\) and compared the distribution of slow moving traffic with the fast moving traffic. The results are summarized in Fig. 5. It is clear that \(d^{(3)}\) performs similarly to MMD in terms of test power for very small sample sizes, but significantly better for larger ones.

## 5 Conclusion

We defined cumulants for random variables in RKHSs by extending the algebraic characterization of cumulants on \(^{d}\). This construction results in a structured description of the law of random variables that goes beyond the classic kernelized mean and covariance. A kernel trick allows us to compute the resulting kernelized cumulants. We applied our theoretical results to two-sample and independence testing; although kernelized mean and covariance are sufficient for this task, the higher-order kernelized cumulants have the potential to increase the test power and to relax the assumptions on the kernel. Our experiments on real and synthetic data show that kernelized cumulants can indeed lead to significant improvement of the test power. A disadvantage of these higher-order statistics is that their theoretical analysis requires more mathematical machinery although we emphasize that the resulting estimators are simple V-statistics.