# ClusterFormer: Clustering As

A Universal Visual Learner

 James C. Liang

Rochester Institute of Technology

&Yiming Cui

University of Florida

&Qifan Wang

Meta AI

&Tong Geng

University of Rochester

&Wenguan Wang

Zhejiang University

&Dongfang Liu

Rochester Institute of Technology

Corresponding author.

###### Abstract

This paper presents ClusterFormer, a universal vision model that is based on the Clustering paradigm with TransFormer. It comprises two novel designs: 1_recurrent cross-attention clustering_, which reformulates the cross-attention mechanism in Transformer and enables recursive updates of cluster centers to facilitate strong representation learning; and 2_feature dispatching_, which uses the updated cluster centers to redistribute image features through similarity-based metrics, resulting in a transparent pipeline. This elegant design streamlines an explainable and transferable workflow, capable of tackling heterogeneous vision tasks (_i.e._, image classification, object detection, and image segmentation) with varying levels of clustering granularity (_i.e._, image-, box-, and pixel-level). Empirical results demonstrate that ClusterFormer outperforms various well-known specialized architectures, achieving 83.41% top-1 acc. over ImageNet-1K for image classification, 54.2% and 47.0% mAP over MS COCO for object detection and instance segmentation, 52.4% mIoU over ADE20K for semantic segmentation, and 55.8% PQ over COCO Panoptic for panoptic segmentation. For its efficacy, we hope our work can catalyze a paradigm shift in universal models in computer vision.

## 1 Introduction

Computer vision has seen the emergence of specialized solutions for different vision tasks (_e.g._, ResNet  for image classification, Faster RCNN  for object detection, and Mask RCNN  for instance segmentation), aiming for superior performance. Nonetheless, neuroscience research  has shown that the human perceptual system exhibits exceptional interpretive capabilities for complex visual stimuli, without task-specific constraints. This trait of human perceptual cognition diverges from current computer vision techniques , which often employ diverse architectural designs.

Human vision possesses a unique attention mechanism that selectively focuses on relevant parts of the visual field while disregarding irrelevant information . This can be likened to a clustering approach , in which individual pixel points are decomposed and reorganized into relevant concepts to address various tasks. This essentially is a hierarchical process that involves combining

Figure 1: ClusterFormer is a clustering-based universal model, offering superior performance over various specialized architectures.

basic visual features, such as lines, shapes, and colors, to create higher-level abstractions of objects, scenes, and individuals [79; 59; 66; 27]. Inspired by the remarkable abilities of the human vision system, this work aims to develop a universal vision model that can replicate the unparalleled prowess.

To this end, we employ a clustering-based strategy that operates at varying levels of granularity for visual comprehension. By solving different vision tasks (_i.e_., image classification, object detection, and image segmentation), we take into account the specificity at which visual information is grouped (_i.e_., image-, box-, and pixel-level). We name our approach, ClusterFormer (SS3.2), as it utilizes a Clustering mechanism integrated within the TransFormer architecture to create a universal network. The method begins by embedding images into discrete tokens, representing essential features that are grouped into distinct clusters. The cluster centers are then recursively updated through a recurrent clustering cross-attention mechanism that considers associated feature representations along the center dimension. Once center assignments and updates are complete, features are dispatched based on updated cluster centers, and then both are fed into the task head for the target tasks.

ClusterFormer enjoys a few attractive qualities.

_Flexibility_: ClusterFormer is a clustering-anchored approach that accommodates a broad array of visual tasks with superior performance (see Fig. 1) under one umbrella. The core epistemology is to handle various tasks with different levels of granularity (_e.g_., image-level classification, box-level detection, pixel-level segmentation, _etc_.), moving towards a universal visual solution.

_Transferability_: The cluster centers generated by the ClusterFormer encoder are directly employed by the task head as initial queries for clustering, allowing the entire architecture to transfer underlying representation for target-task predictions (see Table. 4). This elegant design facilitates the transferability of knowledge acquired from the upstream task (_i.e_., encoder trained on ImageNet ) to downstream tasks (_e.g_., decoder trained on instance segmentation on COCO ).

_Explainability_: Regardless of the target tasks, ClusterFormer's decision-making process is characterized by a transparent pipeline that continuously updates cluster centers through similarity-based metrics. Since the reasoning process is naturally derivable, the model inference behavior is ad-hoc explainable (see SS4.2). This differs ClusterFormer from most existing unified models [17; 44; 95] that fail to elucidate precisely how a model works.

To effectively assess our method, we experimentally show: In SS4.1.1, with the task of image classification, ClusterFormer outperforms traditional counterparts, _e.g_., \(\%\) top-1 accuracy compared with Swin Transformer  on ImageNet , by training from scratch. In SS4.1.2, when using our ImageNet-pretrained, our method can be expanded to the task of object detection and greatly improve the performance compared to Dino  over Swin Transformer on COCO  (\(\%\) mAP). In addition, our method can also adapt to more generic per-pixel tasks, \(a.k.a\), semantic segmentation (see SS4.1.3), instance segmentation (see SS4.1.4), and panoptic segmentation (see SS4.1.5). For instance, we achieve performance gains of \(\%\) mIoU for semantic segmentation on ADE20K , \(\%\) mAP for instance segmentation on MS COCO  and \(\%\) PQ for panoptic segmentation on COCO Panoptic  compared with Mask2Former  over Swin Transformer. Our algorithm are extensively tested, and the efficacy for the core components is also demonstrated through a series of ablative studies outlined in SS4.2.

## 2 Related Work

**Universal Vision Model.** Transformers  have been instrumental in driving universal ambition, fostering models that are capable of tackling tasks of different specificity with the same architecture and embody the potential of these recent developments [23; 17; 16; 95; 96; 4; 80; 30; 57; 86] in the field. In the vision regime, mainstream research endeavors have been concentrating on the development of either encoders [53; 88] or decoders [44; 94]. The encoder is centered around the effort of developing foundation models [4; 53; 24; 22], trained on extensive data that can be adapted and fine-tuned to diverse downstream tasks. For instance, Swin Transformer  capably serves as a general-purpose backbone for computer vision by employing a hierarchical structure consisting of shifted windows; ViT-22B , parameterizes the architecture to 22 billion and achieves superior performance on a variety of vision tasks through learning large-scale data. Conversely, research on decoders [23; 17; 16; 95; 94; 44; 96; 87; 50; 20; 52; 19; 21; 51; 93; 76; 37; 99; 25; 48] is designed to tackle homogeneous target tasks, by using queries to depict visual patterns. For instance, Mask2Former  incorporates mask information into the Transformer architecture and unifies various segmentation tasks (_e.g_., semantic, instance, and panoptic segmentation); Mask-DINO  extends the decoding process from detection to segmentation by directly utilizing query embeddingsfor target task predictions. Conceptually different, we streamline an elegant systemic workflow based on clustering and handle heterogeneous visual tasks (_e.g._, image classification, object detection, and image segmentation) at different clustering granularities.

**Clustering in Vision.** Traditional clustering algorithms in vision [39; 28; 29; 55; 91; 1; 10; 61; 6; 58] can be categorized into the hierarchical and partitional modes. The hierarchical methods [62; 38] involve the modeling of pixel hierarchy and the iterative partitioning and merging of pixel pairs into clusters until reaching a state of saturation. This approach obviates the necessity of a priori determination of cluster quantity and circumvents the predicaments arising from local optima. [98; 12]. However, it exclusively considers the adjacent pixels at each stage and lacks the capacity to assimilate prior information regarding the global configuration or dimensions of the clusters. [69; 64]. In contrast, partitional clustering algorithms [78; 36] directly generate a flat structure with a predetermined number of clusters and exclusively assign pixels to a single cluster. This design exhibits a dynamic nature, allowing pixels to transition between clusters [11; 63]. By employing suitable measures, this approach can effectively integrate complex knowledge within cluster centers. As a powerful system, human vision incorporates the advantages of both clustering modes [89; 83; 67]. We possess the capability of grouping analogous entities at different scales. Meanwhile, we can also effectively categorize objects purely based on their shape, color, or texture, without having the hierarchical information. Drawing on the above insights, we reformulate the attention mechanism (SS3.2 ) in Transformer architectures  from the clustering's perspective to decipher the hierarchy of visual complexity.

## 3 Methodology

### Preliminary

**Clustering.** The objective of clustering is to partition a set of data points, denoted by \(X^{n d}\), into \(C\) distinct clusters based on their intrinsic similarities while ensuring that each data point belongs to only one cluster. Achieving this requires optimizing the stratification of the data points, taking into account both their feature and positional information, to form coherent and meaningful groupings. Clustering methodologies typically employ advanced similarity metrics, such as cosine similarity, to measure the proximity between data points and cluster centroids. Additionally, they consider the spatial locality of the points to make more precise group assignments.

**Cross-Attention for Generic Clustering.** Drawing inspiration from the Transformer decoder architecture , contemporary end-to-end architecture [17; 9] utilize a query-based approach in which a set of \(K\) queries, \(=[_{1};;_{K}]^{K D}\), are learned and updated by a series of cross-attention blocks. In this context, we rethink the term "\(\)" to associate queries with cluster centers at each layer. Specifically, cross-attention is employed at each layer to adaptively aggregate image features and subsequently update the queries:

\[+_{HW}(^{C}(^{I})^{}) ^{I},\] (1)

where \(^{C}\!\!^{K D}\), \(^{I}\!\!^{HW D}\), \(^{I}\!\!^{HW D}\) represent linearly projected features for query, key, and value, respectively. The superscripts "\(C\)" and "\(I\)" denote the features projected from the center and image features, respectively. Motivated by , we follow a reinterpretation of the cross-attention mechanism as a clustering solver by considering queries as cluster centers and applying the _softmax_ function along the query dimension (\(K\)) instead of the image resolution (\(HW\)):

\[+_{K}(^{C}(^{I})^{}) {V}^{I}.\] (2)

### ClusterFormer

In this subsection, we present ClusterFormer (see Fig. 2(a)). The model has a serial of hierarchical stages that enables multi-scale representation learning for universal adaptation. At each stage, image patches are tokenized into feature embedding [81; 53; 24], which are grouped into distinct clusters via a unified pipeline -- first _recurrent cross-attention clustering_ and then _feature dispatching_.

**Recurrent Cross-Attention Clustering.** Considering the feature embeddings \(^{HW D}\) and initial centers \(^{(0)}\), we encapsulate the iterative Expectation-Maximization (EM) clustering process,* _Efficiency:_ While the vanilla self-attention mechanism has a time complexity of \((H^{2}W^{2}D)\), the _Recurrent Cross-Attention_ approach exhibits a lower bound of \((TKHWD)\). This is primarily due to the fact that \(TK HW\) (_i.e._, 4165 in Swin \(vs.\) 1200 in ours). Specifically, considering the nature of the pyramid architecture [88; 53] during the encoding process, \(TK\) can indeed be much smaller than \(HW\), especially in the earlier stages. It is important to note that during each iteration, merely the \(\) matrix requires an update, while the \(\) and \(\) matrices necessitate a single computation. Consequently, the whole model enjoys systemic efficiency (see Table 6c).
* _Transparency:_ The transparency hinges on the unique role that cluster centers play in our _Recurrent Cross-Attention_ mechanism. The cluster centers, derived through our clustering process, act as 'prototypes' for the features they cluster. These 'prototypes' serve as a form of a representative sample for each cluster, reflecting the most salient or characteristic features of the data points within that cluster. Moreover, the _Recurrent Cross-Attention_ method adheres to the widely-established EM clustering algorithm, offering a lucid and transparent framework. This cluster center assignment behaves in a human-understandable manner (see Fig. 3) during representation learning and fosters ad-hoc explainability, allowing for a more intuitive understanding of the underlying relationships.
* _Non-parametric fashion:_ The _Recurrent Cross-Attention_ mechanism achieves a recursive nature by sharing the projection weights for query, key, and value across iterations. This approach effectively ensures recursiveness without the introduction of additional learnable parameters (see Table 6b).

Since the overall architecture is hierarchical, _Recurrent Cross-Attention_ is able to thoroughly explore the representational granularity, which mirrors the process of hierarchical clustering:

\[^{l}=^{l}(^{l},^{l}_{0}),\] (4)

where RCA stands for the recurrent cross-attention layer. \(^{l}\) is the image feature map at different layers by standard pooling operation with \(H/2^{l} W/2^{l}\) resolution. \(^{l}\) is the cluster center matrix for \(l^{th}\) layer and \(^{l}_{0}\) is the initial centers at \(l^{th}\) layer. The parameters for _Recurrent Cross-Attention_ at different layers, _i.e._, \(\{^{l}\}_{l=1}^{L}\), are not shared. In addition, we initialize the centers from image grids:

\[[^{(0)}_{1};;^{(0)}_{K}]=(_{K}()),\] (5)

Figure 2: (a) Overall pipeline of ClusterFormer. (b) Each _Recurrent Cross-Attention Clustering_ layer carries out \(T\) iterations of cross-attention clustering (_E_-step) and center updating (_M_-step) (see Eq. 3). (c) The _feature dispatching_ redistributes the feature embeddings on the top of updated cluster centers (see Eq. 6).

where FFN stands for Position-wise Feedforward Network which is an integral part of the Transformer architecture. It comprises two fully connected layers along with an activation function used in the hidden layer. \(_{K}()\) refers to select \(K\) feature centers from \(\) using adaptive sampling, which calculates an appropriate window size to achieve a desired output size adaptively, offering more flexibility and precision compared to traditional pooling methods.

**Feature Dispatching.** After the cluster assignment, the proposed method employs an adaptive process that dispatches each patch within a cluster based on similarity (see Fig. 2(c)), leading to a more coherent and representative understanding of the overall structure and context within the cluster. For every patch embedding \(p_{i} I\), the updated patch embedding \(p_{i}^{{}^{}}\) is computed as:

\[p_{i}^{{}^{}}=p_{i}+(_{k=0}^{K}sim(C_{k},p_{ i})*C_{k})\] (6)

This equation represents the adaptive dispatching of feature embeddings by considering the similarity between the feature embedding and the cluster centers (\(C\)), weighted by their respective similarities. By incorporating the intrinsic information from the cluster centers, the method refines the feature embeddings, enhancing the overall understanding of the image's underlying structure and context. All feature representations are utilized for handling the target tasks in the decoding process. In SS3.3, we discuss more details about the implementation of the ending tasks.

### Implementation Details

The implementation details and framework of ClusterFormer are shown in (Fig. 2a). We followed the architecture and configuration of Swin Transformer . The code will be available at here.

* _Encoder._ The encoding process is to generate presentation hierarchy, denoted as \(\{^{l}\}\) with \(l=\{1,2,3,4\}\), for a given image \(I\). The pipeline begins with the feature embedding to convert the images into separate feature tokens. Subsequently, multi-head computing [81; 53] is employed to partition the embedded features among them. Center initialization (Eq. 5) is then adopted as a starting for initializing the cluster centers, and the recurrent cross-attention clustering (Eq. 3) is utilized to recursively update these centers. Once the centers have been updated, the features are dispatched based on their association with the updated centers (Eq. 6). The further decoding process leverage both the centers and the features, which guarantees well-rounded learning.
* _Adaptation to Image Classification_. The classification head is a single-layer Multilayer Perceptron (MLP) takes the cluster centers from the encoder for predictions.
* _Adaptation to Detection and Segmentation_. Downstream task head has six Transformer decoder layers with the core design of recurrent cross-attention clustering (Eq.4). Each layer has 3 iterations.

## 4 Experiment

We evaluate our methods over five vision tasks _viz_. image classification, object detection, semantic segmentation, instance segmentation, and panoptic segmentation on four benchmarks.

**ImageNet-1K for Image Classification**. ImageNet-1K includes high-resolution images spanning distinct categories (_e.g._, animals, plants, and vehicles). Following conventional procedures, the dataset is split into 1.2M/50K/100K images for train/validation/test splits.

**MS COCO for Object Detection and Instance Segmentation**. COCO  dataset features dense annotations for 80 common objects in daily contexts. Following standard practices , the dataset is split into 115K/5K/20K images for train2017/val2017/test-dev splits.

**ADE20K for Semantic Segmentation.** ADE20K  dataset offers an extensive collection of images with pixel-level annotations, containing 150 diverse object categories in both indoor and outdoor scenes. The dataset comprises 20K/2K/3K images for train/val/test splits.

**COCO Panoptic for Panoptic Segmentation.** The COCO Panoptic dataset  includes 80 "thing" categories and a carefully annotated set of 53 "stuff" categories. In line with standard practices , the COCO Panoptic dataset is split into 115K/5K/20K images for the train/val/test splits as well.

The ensuing section commences by presenting the main results of each task (SS4.1), succeeded by a series of ablative studies (SS4.2), which aim to confirm the efficacy of each modulating design.

### Main Results

#### 4.1.1 Experiments on Image Classification

**Training.** We use mmclassification2 as codebase and follow the default training settings. The default configuration for our model involves setting the number of centers to 100. To optimize the model's performance, we employ cross-entropy as the default loss function, which is widely used in classification tasks and helps in minimizing the difference between predicted probabilities and ground truth. For the training details, we run the model for 300 epochs, allowing sufficient time for the model to learn and converge. To manage the learning rate, we initialize it at 0.001 as default. The learning rate is then scheduled using a cosine annealing policy, which gradually decreases the learning rate over time. Due to limitations in our GPU capacity, we are constrained to set the total batch size at 1024. Models are trained _from scratch_ on sixteen A100 GPUs.

**Results on ImageNet.** Table 1 illustrates our compelling results over different famous methods. ClusterFormer exceeds the Swin Transformer  by **0.13\(\%\)** and **0.39\(\%\)** on Tiny-based and Small-based models with fewer parameters (_i.e._, 27.85M \(vs.\) 28.29M and 48.71M \(vs.\) 49.61M), respectively. On top-5 accuracy, our approach also outperforms the Swin-Tiny and Swin-Small with gains of **0.71\(\%\)** and **0.84\(\%\)**, respectively. In addition, our margins over the ResNet family  are **3.44\(\%\)\(\) 4.76\(\%\)** on top-1 accuracy with on-par parameters (_i.e._, 27.85M \(vs.\) 25.56M and 48.71M \(vs.\) 44.55M).

#### 4.1.2 Experiments on Object Detection

**Training.** We use mmdetection3 as codebase and follow the default training settings. For a fair comparison, we follow the training protocol in : 1) the number of instances centers is set to 100; 2) a linear combination of the \(_{1}\) loss and the GIoU Loss is used as the optimization objective for bounding box regression. Their coefficients are set to 5 and 2, respectively. In addition, the final object centers are fed into a small FFN for object classification, trained with a binary cross-entropy loss. Moreover, we set the initial learning rate to \(1 10^{-5}\), the training epoch to 50, and the batch size to 16. We use random scale jittering with a factor in \([0.1,2.0]\) and a crop size of \(1024 1024\).

**Test.** We use one input image scale with shorter side as 800.

**Metric.** We adopt AP, AP\({}_{50}\), AP\({}_{75}\), AP\({}_{S}\), AP\({}_{M}\), and AP\({}_{L}\).

**Performance Comparison.** In Table 2, we present the numerical results for ClusterFormer for object detection. We observe that it surpasses all counterparts  with remarkable gains with respect to mAP. In particular, ClusterFormer-Tiny exceeds the vanilla Deformable DETR , Sparse-DETR , and DINO  over Swin-T  by **6.5\(\%\)**, **3.4\(\%\)**, and **0.8\(\%\)** in terms of mAP, respectively. In addition, our approach also outperforms these methods over Swin-S , _i.e._, _54.2\(\%\)_vs_\(48.3\%\)_vs_\(49.9\%\)_vs_\(53.3\%\) in terms of mAP, respectively. Notably, ClusterFormer achieves impressive performance without relying on additional augmentation.

#### 4.1.3 Experiments on Semantic Segmentation

**Training.** We use mmsegmentation4 as codebase and follow the default training settings. The training process for semantic segmentation involves setting the number of cluster centers to match the number of semantic categories, which is 150 for ADE20K . Following the approach employed in recent works , we adopt a combination of the standard cross-entropy loss and an auxiliary dice loss for the loss function. By default, the coefficients for the cross-entropy and dice losses are set to \(5\) and \(1\), respectively. In addition, we configure the initial learning rate to \(1 10^{-5}\), the number of training epochs to 50, and the batch size to 16.

    & Method & \#Params & top-1 & top-5 \\   Context Cluster-Tiny\({}_{0.1}\) & 5.3M & 71.68\(\%\) & 90.49\(\%\) \\ DeiT-\({}_{1000 1000}\) & 5.72M & 74.50\(\%\) & 92.25\(\%\) \\ PVG-Tiny\({}_{0.1}\) & 9.46M & 78.38\(\%\) & 94.38\(\%\) \\ ResNet-50-\({}_{0.0}\) & 25.25M & 76.55\(\%\) & 93.06\(\%\) \\ Swin-Tiny\({}_{0.0}\) & 28.29M & 81.18\(\%\) & 95.61\(\%\) \\
**ClusterFormer-Tiny** & 27.85M & **81.31\(\%\)** & **96.32\(\%\)** \\  Context-Small\({}_{0.5}\) & 14.0M & 77.42\(\%\) & 93.69\(\%\) \\ DeiT-Small\({}_{0.0}\) & 20.25M & 80.69\(\%\) & 95.06\(\%\) \\ PVG-Small\({}_{0.0}\) & 29.02M & 82.00\(\%\) & 95.97\(\%\) \\ ResNet-101-\({}_{0.1}\) & 44.53\(\%\) & 77.97\(\%\) & 94.06\(\%\) \\ Swin-Small\({}_{0.1}\) & 49.61M & 83.02\(\%\) & 96.29\(\%\) \\
**ClusterFormer-Small** & 48.71M & **83.41\(\%\)** & **97.13\(\%\)** \\  

Table 1: **Classification top-1 and top-5 accuracy on ImageNet  val (see §4.1.1 for details).**Furthermore, we employ random scale jittering, applying a factor within the range of [0.5, 2.0], and utilize a crop size with a fixed resolution of \(640 640\) pixels.

**Test.** During the testing phase, we re-scale the input image with a shorter side to 640 pixels without applying any additional data augmentation at test time.

**Metric.** Mean intersection-over-union (mIoU) is used for assessing image semantic segmentation performance.

**Performance Comparison.** Table 3 shows the results on semantic segmentation. Empriailly, our method compares favorably to recent transformer-based approaches . For instance, ClusterFormer-Tiny surpasses both recent advancements, _i.e._, kMaX-Deeplab  and Mask2Former  with Swin-T  (_i.e._, \(\%\)_vs._\(48.3\%\)_vs._\(48.5\%\)), respectively. Moreover, ClusterFormer-Small achieves \(\%\) mIoU and outperforms all other methods in terms of mIoU, making it competitive with _state-of-the-art_ methods as well.

#### 4.1.4 Experiments on Instance Segmentation

**Training.** We adopt the same training strategy for instance segmentation by following SS4.1.2. For instance segmentation, we change the training objective by utilizing a combination of the binary cross-entropy loss and the dice Loss for instance mask optimization.

**Test.** We use one input image scale with a shorter side of 800.

**Metric.** We adopt AP, \(_{50}\), \(_{75}\), \(_{S}\), \(_{M}\), and \(_{L}\).

**Performance Comparison.** Table 4 presents the results of ClusterFormer against famous instance segmentation methods  on COCO test-dev. ClusterFormer shows clear performance advantages over prior arts. For example, ClusterFormer-Tiny outperforms the universal counterparts Mask2Former  by \(1.4\%\) over Swin-T  in terms of mAP and on par with the _state-of-the-art_ method, Mask-Dino  with Swin-T backbone. Moreover, ClusterFormer-Small surpasses all the competitors, _e.g._, yielding significant gains of \(1.0\%\) and \(0.5\%\) mAP compared to Mask2Former and Mask-Dino over Swin-S, respectively. Without bells and whistles, our method establishes a new _state-of-the-art_ on COCO instance segmentation.

#### 4.1.5 Experiments on Panoptic Segmentation

**Training.** Following the convention , we use the following objective for network learning:

\[^{}=^{}^{}+ ^{}^{}+^{}^{ },\] (7)

\(^{}\) and \(^{}\) represent the loss functions for things and stuff, respectively. To ensure a fair comparison, we follow  and incorporate an auxiliary loss calculated as a weighted sum of four different loss terms, specifically, a PQ-style loss, a mask-ID cross-entropy loss, an instance discrimination

   Algorithm & Backbone & Epoch & mAP\(\) & \(_{50}\) & \(_{75}\) & \(_{S}\) & \(_{M}\)\(\) & \(_{L}\)\(\) \\  Faster R-CNN\({}_{}}}}}}}}}}}}}}}\) & ResNet-101 & 36 & 41.7 & 62.3 & 45.7 & 24.7 & 46.0 & 53.2 \\ Cascade R-CNN\({}_{}}}}}}}}}}}}}}\) & ResNet-101 & 36 & 42.8 & 61.1 & 46.7 & 24.9 & 46.5 & 56.4 \\ Grid R-CNN\({}_{}}}}}}}}}}}}}}\) & ResNet-50 & 24 & 40.4 & 58.5 & 43.6 & 22.7 & 43.9 & 53.0 \\ EfficientDetroit\({}_{}}}}}}}}}}}}}}\) & Efficient-B3 & 300 & 45.4 & 63.9 & 49.3 & 27.1 & 49.5 & 61.3 \\ DETRCNN\({}_{}}}}}}}}}}}}}\) & ResNet-50 & 150 & 39.9 & 60.4 & 41.7 & 17.6 & 43.4 & 59.4 \\ Sparse R-CNN\({}_{}}}}}}}}}}}}\) & ResNet-101 & 36 & 46.2 & 65.1 & 50.4 & 29.5 & 49.2 & 61.7 \\ Conditional DETRCNN\({}_{}}}}}}}}}}}}}\) & ResNet-50 & 50 & 41.1 & 61.9 & 43.5 & 20.4 & 44.5 & 59.9 \\  Deformable DETRCNN\({}_{}}}}}}}}}}}}}\) & Swin-T & 50 & 45.3\({}_{-20.6}\) & 65.2 & 22.0 & 49.8\({}_{-1.0}\) & 27.0\({}_{-20.2}\) & 49.1\({}_{-21.0}\) & 64.7\({}_{-0.29}\) \\ Swin-S & Swin-S & 48.3\({}_{-0.2}\) & 68.7\({}_{-27.2}\) & 52.1\({}_{-21.2}\) & 30.5\({}_{-0.5}\) & 51.6\({}_{-0.2}\) & 64.9\({}_{-0.19}\) \\ Swin-T & Swin-T & 50 & 48.6\({}_{-0.2}\) & 69.6\({}_{-0.20}\) & 53.5\({}_{-0.23}\) & 30.1\({}_{-0.27}\) & 51.8\({}_{-0.21}\) & 64.9\({}_{-0.29}\) \\ Sparse-DETRCNN\({}_{}}}}}}}}}}\) & Swin-S & 49.9\({}_{-0.21}\) & 70.3\({}_{-0.27}\) & 54.0\({}_{-0.26}\) & 32.5\({}_{-0.22}\) & 53.6\({}_{-0.26}\) & 65.2\({}_{-0.25}\) \\  DINO\({}_{}}}}}}}}}\) & Swin-T & 51.2\({}_{-0.26}\) & 66.4\({}_{-0.25}\) & 55.3\({}_{-0.26}\) & 31.3\({}_{-0.24}\) & 55.1\({}_{-0.58}\) & 65.3\({}_{-0.26}\) \\   & Swin-T & 53.3\({}_{-0.22}\) & 70.9\({}_{-0.35}\) & **57.6\({}_{-0.23}\)** & **33.8\({}_{-0.25}\)** & **56.4\({}_{-0.26}\)** & **69.9\({}_{-0.26}\)** \\    & Ours-Tiny & 50 & 52.0\({}_{-0.32}\) & 70.4\({}_{-0.25}\) & 57.5\({}_{-0.32}\) & 34.2\({}_{-0.25}\) & 54.8\({}_{-0.29}\) & 64.8\({}_{-0.22}\) \\  & Ours-Small & 50 & **54.2\({}_{-0.33}\)** & **71.8\({}_{-0.16}\)** & **59.1\({}_{-0.17}\)** & **35.6\({}_{-0.28}\)** & **57.2\({}_{-0.20}\)** & **67.4\({}_{-0.18}\)** \\  

Table 2: Quantitative results on COCO  test-dev for **object detection** (see §4.1.2 for details).

  Algorithm & Backbone & Epoch & mIoU\(\) \\  FCN\({}_{^{}\) can be found in [85; 95]. The coefficients \(^{}\), \(^{}\), and \(^{}\) are assigned the values of 5, 3, and 1, respectively. Furthermore, the final centers are input into a small feed-forward neural network (FFN) for semantic classification, which is trained using a binary cross-entropy loss. Moreover, we set the initial learning rate to \(1 10^{-5}\), the number of training epochs to 50, and the batch size to 16. We also employ random scale jittering with a factor range of [0.1, 2.0] and a crop size of 1024\(\)1024.

**Test.** We use one input image scale with a shorter side of 800.

**Metric.** We employ the PQ metric  and report PQ\({}^{}\) and PQ\({}^{}\) for the "thing" and "stuff" classes, respectively. To ensure comprehensiveness, we also include mAP\({}^{}_{}\), which evaluates mean average precision on "thing" classes using instance segmentation annotations, and mIoU\({}_{}\), which calculates mIoU for semantic segmentation by merging instance masks belonging to the same category, using the same model trained for the panoptic segmentation task.

**Performance Comparison.** We perform a comprehensive comparison against two divergent groups of _state-of-the-art_ methods: universal approaches [46; 17; 44] and specialized panoptic methods [41; 92; 16; 45; 85; 97; 94]. As shown in Table 5, ClusterFormer outperforms both types of rivals. For instance, the performance of ClusterFormer-Tiny clear ahead compared to Mask2Former (_i.e._, **54.7\(\%\)** PQ \(vs.\)\(53.2\%\) PQ) and Mask-Dino  (_i.e._, **54.7\(\%\)** PQ \(vs.\)\(53.6\%\) PQ) on the top of Swin-T , and ClusterFormer-Small achieves promising gains of **1.7\(\%\)** and **0.9\(\%\)** PQ against Mask2Former and Mask-Dino over Swin-S, respectively. Moreover, in terms of mAP\({}^{}_{}\) and mIoU\({}_{}\), the ClusterFormer also achieves outstanding performance beyond counterpart approaches.

### Ablative Study

This section ablates ClusterFormer's key components on ImageNet  and MS COCO  validation split. All experiments use the tiny model.

**Key Component Analysis.** We first investigate the two major elements of ClusterFormer, specifically, _Recurrent Cross-Attention Clustering_ for center updating and _Feature Dispatching_ for feature updating. We construct a Baseline model without any center updating and feature dispatching technique. As shown in Table (a)a, Baseline achieves \(74.59\%\) top-1 and \(91.73\%\) top-5 accuracy. Upon applying _Recurrent Cross-Attention Clustering_ to the Baseline, we observe consistent and substantial improvements for both top-1 accuracy (\(74.59\%\%\)) and top-5 accuracy (\(91.73\%\%\)). This highlights the importance of the center updating strategy

   Algorithm & Backbone & Epoch & mAP\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) & AP\({}_{}\) \\   Mask P-CNN\({}_{}\) & ResNet-101 & 12 & 36.1 & 57.5 & 38.6 & 18.8 & 39.7 & 49.5 \\ Cascade MR-CNN\({}_{}\) & ResNet-101 & 12 & 37.3 & 58.2 & 40.1 & 19.7 & 40.6 & 51.5 \\ HTFC\({}_{}\) & ResNet-101 & 20 & 39.6 & 61.0 & 42.8 & 21.3 & 42.9 & 55.0 \\ PointEnd\({}_{}\) & ResNet-S0 & 12 & 36.3 & 56.9 & 38.7 & 19.8 & 39.4 & 48.5 \\ BlendMask\({}_{}\) & ResNet-101 & 36 & 38.4 & 60.7 & 41.3 & 18.2 & 41.5 & 53.3 \\ QueryIn\({}_{}\) & ResNet-101 & 36 & 41.0 & 63.3 & 44.5 & 21.7 & 44.4 & 60.7 \\ SoLO\({}_{}\) & Swin-L & 50 & 46.7 & 72.7 & 50.6 & 29.2 & 50.1 & 60.9 \\ SparseIn\({}_{}\) & ResNet-50 & 36 & 37.9 & 59.2 & 40.2 & 15.7 & 39.4 & 56.9 \\ Max2Former\({}_{}\) & Swin-T & 50 & 44.3\({}_{}\)[13.6] & 67.3\({}_{}\)[13.5] & 47.7\({}_{}\)[13.4] & 23.9\({}_{}\)[13.0] & 48.1\({}_{}\)[13.6] & 66.4\({}_{}\)[13.5] \\ Swin-S & - & 50 & 46.0\({}_{}\)[13.6] & 68.4\({}_{}\)[13.2] & 49.8\({}_{}\)[13.0] & 25.4\({}_{}\)[13.0] & 49.7\({}_{}\)[13.2] & 67.4\({}_{}\)[13.2] \\  & Swin-T & - & 45.8\({}_{}\)[13.2] & 66.0\({}_{}\)[13.2] & 50.2\({}_{}\)[13.6] & 26.0\({}_{}\)[13.6] & 48.7\({}_{}\)[13.2] & 68.4\({}_{}\)[13.2] \\ Mask-Dino\({}_{}\) & Swin-S & 50 & 46.5\({}_{}\)[13.2] & 70.1\({}_{}\)[13.1] & **52.2\({}_{}\)[13.6]** & **27.6\({}_{}\)[13.4]** & 49.9\({}_{}\)[13.2] & 69.5\({}_{}\)[13.2] \\    & Ours-Tiny & 50 & 45.9\({}_{}\)[13.0] & 49.5\({}_{}\)[13.0] & 25.2\({}_{}\)[13.6] & 50.1\({}_{}\)[13.2] & 68.8\({}_{}\)[13.2] \\  & Ours-Tiny & 50 & 47.6\({}_{}\)[13.9] & **71.5\({}_{}\)[13.0]** & 51.8\({}_{}\)[13.4] & 27.3\({}_{}\)[13.6] & **50.5\({}_{}\)[13.0]** & **72.6\({}_{}\)[13.7]** \\   

Table 4: Quantitative results on COCO  test-dev for instance segmentation (see §4.1.4 for details).

   Algorithm & Backbone & Epoch & PQ1 & PQ2\({}^{}\) & PQ3\({}^{}\) & mAP\({}^{}_{}\) & mIoU\({}_{}\) \\  Panoptic-FPN\({}_{}\) & ResNet-101 & 20 & 44.0 & 52.0 & 31.9 & 34.0 & 51.5 \\ UPSNet\({}_{}\) & ResNet-101 & 12 & 46.2 & 52.8 & 36.5 & 36.3 & 56.9 \\ Panoptic-DeepLab\({}_{}\) & Xception-71 & 12 & 41.2 & 44.9 & 35.7 & 31.5 & 55.4 \\ Panoptic-FCN\({}_{}\) & ResNet-50 & 12 & 44.3 & 50.0 & 35.6 & 35.5 & 55.0 \\ Max-Depth\({}_{}\) & Max-L & 55 & 51.1 & 57.0 & 42.2 & – & – \\ CMT-DeepLab\({}_{}\) & Axial-R104\({}^{}\) & 55 & 54.1 & 58.8 & 47.1 & – & – \\ PanopticSegformer\({}_{}\) & ResNet-50 & 24 & 46.6\({}_{}\)[13.2] & 54.2\({}_{}\)[13.2] & 39.5\({}_{}\)[and validates the effectiveness of our approach, even without explicitly performing clustering. Furthermore, after incorporating _Feature Dispatching_ into the Baseline, we achieve significant gains of **3.99\(\%\)** in top-1 accuracy and **2.95\(\%\)** in top-5 accuracy. Finally, by integrating both core techniques, ClusterFormer delivers the best performance across both metrics. This indicates that the proposed _Recurrent Cross-Attention Clustering_ and _Feature Dispatching_ can work synergistically and validates the effectiveness of our comprehensive algorithmic design.

**Recurrent Cross-attention Clustering.** We next study the impact of our _Recurrent Cross-attention Clustering_ (Eq.4) by contrasting it with the cosine similarity updating, basic cross-attention , Criss-attention  and \(K\)-Means cross-attention . As illustrated in Table (c)c, our _Recurrent Cross-Attention_ proves to be _effective_ - it outperforms the cosine similarity, vanilla, Criss and \(K\)-Means by **2.52\(\%\)**, **1.64\(\%\)**, **1.40\(\%\)** and **0.15\(\%\)** top-1 accuracy respectively, and _efficient_ - its #Params are significantly less than the other vanilla and Criss-attention and on par with \(K\)-Means, in line with our analysis in SS3.2. To gain further insights into recursive clustering, we examine the effect of the recursion number \(T\) in Table (b)b. We discover that performance progressively improves from \(81.06\%\) to **81.31\(\%\)** in top-1 accuracy when increasing \(T\) from \(1\) to \(3\), but remains constant after running additional iterations. We also observe that #Params increase as \(T\) increases. Consequently, we set \(T=3\) as the default to strike an optimal balance between accuracy and computation cost.

**Multi-head Dimension.** We then ablate the head embedding dimension for the attention head in Table (d)d. We find that performance significantly improves from \(71.69\%\) to **82.40\(\%\)** in top-1 accuracy when increasing the dimension from \(16\) to \(48\), but #Params steadily increase as the dimension grows. For a fair comparison with Swin , we set the head dimension to 32 as our default.

**Feature Dispatching.** We further analyze the influence of our _Feature Dispatching_. As outlined in Table (e)e, in a standard manner without any dispatching method, the model attains \(80.57\%\) top-1 accuracy and \(95.22\%\) top-5 accuracy. By applying a vanilla fully connected layer to update the feature, we witness a marginal increase of **0.26\(\%\)** in top-1 accuracy. Moreover, using the confidence-based updating method  and fully connected layer with similarity, the model demonstrates a noticeable enhancement in \(0.12\%\) and \(0.39\%\) top-1 accuracy, respectively. Last, our method yields significant performance advancements across both metrics, _i.e._, **81.31\(\%\)** top-1 and **96.32\(\%\)** top-5 accuracy.

**Decoder Query Initialization.** Last, we examine the impact of query initialization in the decoder on a downstream task (_i.e._, instance segmentation) in Table (f)f. For free parameter initialization, the base model can achieve \(44.2\%\) in terms of mAP. By applying direct feature embedding, the method has a slight improvement of \(0.3\%\) mAP. In addition, the model exhibits improvements in

Table 6: A set of **ablative studies** on ImageNet  validation and MS COCO  test-dev split (see §4.2). The adopted designs are marked in red.

mAP, achieving \(44.9\%\) and \(45.1\%\), respectively, by employing the mixed query selection  and scene-adoptive embedding . Outstandingly, ClusterFormer achieves the highest performance in all three metrices, _i.e_., \(45.9\%\) mAP, \(69.1\%\) AP\({}_{50}\) and \(49.5\%\) AP\({}_{75}\), respectively. The empirical evidence proves our design -- using the cluster centers from the encoder to derive the initial query for the decoder -- that facilitates the transferability for representation learning.

**Ad-hoc Explainability.** We visualize the cluster assignment map for image classification in Fig. 3. This figure provides an insightful illustration of how ClusterFormer groups similar features together. Each color represents a cluster of features that share common characteristics.

## 5 Conclusion

This study adopts an epistemological perspective centered on the clustering-based paradigm, which advocates a universal vision framework named ClusterFormer. This framework aims to address diverse visual tasks with varying degrees of clustering granularity. By leveraging insights from clustering, we customize the cross-attention mechanism for recursive clustering and introduce a novel method for feature dispatching. Empirical findings provide substantial evidence to support the effectiveness of this systematic approach. Based on its efficacy, we argue deductively that the proposed universal solution will have a substantial impact on the wider range of visual tasks when viewed through the lens of clustering. This question remains open for our future endeavors.

**Acknowledgement.** This research was supported by the National Science Foundation under Grant No. 2242243.