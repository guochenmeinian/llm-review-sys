# Calibrated Self-Rewarding Vision Language Models

Yiyang Zhou\({}^{1}\), Zhiyuan Fan\({}^{5}\), Dongjie Cheng\({}^{6*}\), Sihan Yang\({}^{7}\), Zhaorun Chen\({}^{2}\)

Chenhang Cui\({}^{8}\), Xiyao Wang\({}^{3}\), Yun Li\({}^{1}\), Linjun Zhang\({}^{4}\), Huaxiu Yao\({}^{1}\)

\({}^{1}\)UNC-Chapel Hill, \({}^{2}\)University of Chicago, \({}^{3}\)University of Maryland

\({}^{4}\)Rutgers University, \({}^{5}\)HKUST, \({}^{6}\)PolyU, \({}^{7}\)NTU, \({}^{8}\)NUS

yiyangai@cs.unc.edu, huaxiu@cs.unc.edu

Equal contribution

###### Abstract

Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches are resource-intensive and may not effectively reflect the target LVLM's preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR significantly enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning. Our data and code are available at https://github.com/YiyangZhou/CSR.

## 1 Introduction

Large Vision-Language Models (LVLMs) [1; 2; 3; 4] have achieved significant success by incorporating pre-trained large language models (LLMs) and vision models through instruction tuning. However, these LVLMs suffer from the hallucination phenomenon , which generates text responses that are linguistically plausible but contradict the visual information in the accompanying image. For instance, the description generated by LVLMs may include visual elements that are not depicted in the image. This issue can also occur when the LLM is highly factual and the visual backbone is capable of producing sufficiently high-quality representations. As indicated in Cui et al. , Guan et al. , the potential reason for this lies in the misalignment problem between image and text modalities in LVLMs, which causes the model to prioritize the text knowledge present in the training language data while ignoring the actual visual input information.

Several works have been proposed to enhance modality alignment capability in LVLMs through preference fine-tuning techniques, such as reinforcement learning from human feedback (RLHF)  and direct preference optimization (DPO) [9; 10]. However, these methods often either introduce additional models, such as GPT-4, or depend on human annotation to generate preference data. This data generation process is not only resource-intensive but, more critically, fails to capture the inherent preferences of the target LVLM. Consequently, the target LVLM may easily discern preferences from such curated data, making them less effective (detailed analysis provided in Appendix A.4). Recently, self-rewarding approaches have emerged, utilizing a single LLM for both response generation and preference modeling, showing promising results in LLM alignment [11; 12]. Unlike LLMs, LVLMs face modality misalignment issues in both response generation and preference modeling stages, potentially resulting in self-generated preferences overlooking visual input information. Directly applying these self-rewarding approaches to LVLMs is not capable of addressing the modality alignment problem and redirecting LVLM's attention towards emphasizing input image information.

To tackle these challenges, our work introduces the **C**alibrated **S**elf-**R**ewarding (**CSR**) approach, aimed at calibrating the self-rewarding paradigm by incorporating visual constraints into the preference modeling process. Specifically, we train the target LVLM using an iterative preference optimization framework that continuously generates preferences and optimizes the target LVLM over multiple iterations. Starting with a seed model, each iteration employs sentence-level beam search [13; 14] to produce fine-grained candidate responses for each image and text prompt. During the beam search, for each generated sentence, we first utilize the language decoder to establish an initial reward (i.e., sentence-level cumulative probabilities). Subsequently, we calibrate this initial reward by incorporating an image-response relevance score, resulting in the calibrated reward score. These calibrated reward scores are utilized to guide the generation of the next batch of candidate sentences. Finally, responses with the highest and lowest cumulative calibrated reward scores are identified as preferred and dispreferred responses, respectively, for preference fine-tuning in the subsequent iteration.

The primary contribution of this paper is CSR, a novel calibrated self-rewarding paradigm for improving modality alignment in LVLMs. Theoretically, with mild assumptions, we show that introducing visual constraints in the self-rewarding paradigm can improve performance. Empirically, when compared with other competitive approaches (see Figure 1 for some representative methods), the results demonstrate that CSR is capable of improving performance on comprehensive LVLM evaluation benchmarks, VQA tasks, and reducing hallucination, achieving up to a 7.62% improvement on average. Additionally, we demonstrate CSR is capable of continuously improving performance over iterations, compatible with different large vision-language backbone models, and redirecting the attention of LVLMs toward the visual modality to achieve stronger modality alignment.

## 2 Preliminaries

In this section, we will provide a brief overview of LVLM and preference optimization.

**Large Vision Language Models.** LVLMs extend LLMs to multimodal scenario, which progressively predict the probability distribution of the next token for each input prompt. Given an <image \(x_{v}\), text \(x_{t}\)> pair as input prompt \(x\), LVLM outputs a text response \(y\).

**Preference Optimization.** Preference optimization has shown promise in fine-tuning language models and aligning their behavior with desired outcomes. Given an input prompt \(x\), a language model with policy \(_{}\) can produce a conditional distribution \(_{}(y x)\) with \(y\) as the output text response. The preference data is defined as \(=\{(x^{(i)},y_{w}^{(i)},y_{l}^{(i)})\}_{i=1}^{N}\), where \(y_{w}^{(i)}\) and \(y_{l}^{(i)}\) denote the preferred

Figure 1: Benchmark performance comparison.

and dispreferred responses for the input prompt \(x^{(i)}\). Preference optimization leverage the preference data to optimize language models. Taking DPO  as a representative example, it formulates the probability of obtaining each preference pair as \(p(y_{w} y_{l})=(r(x,y_{w})-r(x,y_{l}))\), where \(()\) is the sigmoid function. DPO optimizes the language models with the following classification loss:

\[_{}(_{};_{})=-_{(x,y_{w},y_{l})}[((y_{w} |x)}{_{}(y_{w}|x)}-(y_{l}|x)}{_{ }(y_{l}|x)})],\] (1)

where \(_{}(y|x)\) represents the reference policy, i.e., the language model after performing supervised fine-tuning.

## 3 Calibrated Self-Rewarding Vision Language Models

To address this challenge, we propose **C**alibrated **S**elf-**R**ewarding (**CSR**), a novel approach aimed at improving modality alignment in LVLMs by integrating visual constraints into the self-rewarding paradigm. As illustrated in Figure 2, CSR trains the target LVLM by alternately performing two stages: candidate response generation and preference curation and fine-tuning. In the candidate response generation stage, we employ sentence-level beam search for each input prompt to produce fine-grained candidate responses. During this process, the language decoder determines the initial reward for each generated sentence, which is then calibrated by incorporating an image-response relevance score. This calibrated reward score guides the generation of subsequent sentences and finally generate the entire response. Moving on to the preference curation and fine-tuning stage, we use the responses with the highest and lowest cumulative calibrated rewards to construct the preferred and dispreferred responses, and utilize the constructed preference pairs for fine-tuning. In the remaining of this section, we will provide detailed explanations of CSR.

Figure 2: The CSR framework operates an iterative process of preference data generation and learning. During preference data generation, CSR utilizes a sentence-level beam search approach to construct responses sentence by sentence, assigning a reward to each sentence. This reward, initially generated by the model itself, is then calibrated using image-relevance information. Preferences are determined based on the cumulative reward for each response. In each iteration, CSR generates new preference data and performs preference learning based on this data, continuously enhancing the model’s performance.

### Step-Level Reward Modeling and Calibration

Before delving into how to generate candidate response and construct preference data, in this section, we first discuss how to formulate the reward within CSR. The ideal reward in the LVLM fulfills two specific criteria:

* Vision-Constrained Reward: This aspect aims to integrate image-relevance information into the reward definition of LVLMs. By doing so, we address the limitation of LVLM in overlooking image input data when generating preferences.
* Step-Wise Reward: Instead of assigning a single reward for the entire response, we opt for a stepwise approach. This involves assigning rewards at each step of response generation. Compared to a single reward, this finer-grained reward offers more detailed guidance and is more robust.

To fulfill these criteria, we propose a step-wise calibrated reward modeling strategy. Inspired by Process-Supervised Reward Models , we assign a reward score, \(R(s)\), to each generated sentence \(s\) during the sentence-level beam search. This score is a combination of two components: the self-generated instruction-following score, \(R_{T}(s)\), and the image-response relevance score, \(R_{I}(s)\).

Specifically, the self-generated instruction-following score, \(R_{T}(s)\), is calculated using the language decoder of the LVLM. It represents the sentence-level cumulative probability of generating sentence \(s\), formulated as:

\[R_{T}(s)=_{t=1}^{N_{o}}P(r_{o} x,r_{1},r_{2},,r_{o-1}),\] (2)

where \(N_{o}\) is the number of tokens in sentence \(s\) and \(r_{o}\) represents token \(o\) in sentence \(s\). A higher self-generated instruction-following score indicates a stronger capability of the generated response to follow instructions.

While the self-generated instruction-following score partially reflects the LVLM's preference, it still suffers from modality misalignment, potentially overlooking visual input information. To address this, we introduce an image-response relevance score, \(R_{I}(s)\), to calibrate the reward score \(R_{T}(s)\). This score depicts the relevance between the generated sentence \(s\) and input image \(x_{v}\). We leverage CLIP-score  for this calculation, where the vision encoder in the CLIP model aligns with the vision encoder in the target LVLM. The image-response relevance score \(R_{I}(s)\) is defined as:

\[R_{I}(s)=(100*(_{I}(x_{v}),_{T}(s)),0),\] (3)

where the \(_{I}(x_{v})\) and \(_{T}(s)\) represent the visual CLIP embedding and textual CLIP embedding, respectively. Finally, the calibrated reward score \(R(s)\) for the generated sentence \(s\) is defined as:

\[R(s)= R_{I}(s)+(1-) R_{T}(s),\] (4)

where \(\) is a hyperparameter used to balance the language instruction-following and image-response relevance scores. By combining both scores, we aim to redirect the attention of LVLM towards the input visual information, thus enhancing its modality alignment ability.

### Iterative Fine-Tuning

After establishing the reward framework in CSR, we next discuss our iterative fine-tuning process. Within this framework, we iteratively perform two essential steps, namely candidate response generation and preference data curation and optimization. These steps are elaborated upon as follows:

#### 3.2.1 Step-Level Candidate Response Generation

In candidate response generation, our objective is to generate responses to build preference data. To accomplish this, we employ a sentence-level beam search strategy. Initially, we concurrently sample multiple candidate sentences, utilizing the "end of sub-sentence" marker (e.g., _"_" in English) as the delimiter. Subsequently, for each sentence \(s\), we compute its reward score \(R(s)\) using Eqn. (4). From these scores, we then select the top-\(k\) and bottom-\(k\) sentences with the highest and lowest reward scores, respectively, to proceed to the subsequent round of sentence-level beam search. This iterative process continues until reaching the "end of response," conventionally represented as \(\). Once all sentences for a response \(y=\{s_{1},,s_{N_{y}}\}\) are generated, we calculate the cumulative reward score for the response as the sum of the reward scores for each sentence within it. This isdefined as: \(R(y)=_{i=1}^{N_{y}}R(s_{i})\), where \(N_{y}\) is the number of sentences in response \(y\). The detailed algorithm for candidate response generation is outlined in Algorithm 1.

```
0: Dataset: \(=\{x^{(i)}\}_{i=1}^{N}\); Reference model:\(_{}\); Number of iterations: \(T\)
1:for\(t=1,,T\)do
2:for each \(x\)do
3:while not reach the end of response do
4: Generate a bunch of candidate sentences from last-round sentences
5:for each candidate sentence \(s\)do
6: Compute the self-generated instruction-following score \(R_{T}(s)\) by Eqn. (2)
7: Calculate the image representation \(_{I}(x_{v})\) and sentence representation \(_{T}(s)\)
8: Compute the image-response relevance score \(R_{I}(s)\) by Eqn. (3)
9: Compute the calibrated reward score \(R(s)\) by Eqn. (4)
10: Select top-k and bottom-k sentences with the highest and lowest reward scores
11: Select the preferred response \(y_{w,t}\) and dispreferred response \(y_{l,t}\)
12: Update \(_{}_{}_{t}(_{};_{ })\), \(_{}_{}\). ```

**Algorithm 1** Calibrated Self-Rewarding

#### 3.2.2 Preference Curation and Optimization

After generating candidate responses with their reward scores, our next step is to curate preference dataset. Here, for each input prompt, we select the responses with the highest and lowest cumulative calibrated reward scores as the preferred and dispreferred responses, respectively, to construct the preference dataset for fine-tuning. For each iteration \(t\), we denote the constructed preference data as: \(_{t}=\{(x^{(i)},y^{(i)}_{w,t},y^{(i)}_{l,t})\}_{i=1}^{N}\). After obtaining the preference data, we fine-tune the target LVLM using DPO. At iteration \(t\), we use the last iteration fine-tuned model \(_{_{t-1}}\) as the reference model. Following Eqn (1), the loss at iteration \(t\) of CSR is defined as:

\[_{t}=-_{(x,y_{w,t},y_{l,t})}[ ((y_{w,t}|x)}{_{_{t-1}}(y_{w,t }|x)}-(y_{l,t}|x)}{_{_{t-1}}(y_{l,t}|x)} )].\] (5)

The training process of CSR is detailed in Algorithm 1.

## 4 Experiment

In this section, we empirically investigate CSR in addressing the modality misalignment problem of LVLMs, focusing on the following questions: (1) Can CSR help improve the performance of models on both comprehensive benchmarks and hallucination benchmarks? (2) Can CSR iteratively improve multimodal alignment progressively in LVLMs and lead to more factual LVLMs? (3) Is CSR compatible with different open-sourced LVLMs? (4) How does CSR change attention weights and preference pairs to align image and text modalities?

### Experimental Setups

**Implementation Details.** We utilize LLaVA-1.5 7B and 13B  as the backbone models. During the preference learning process, we adapt LoRA fine-tuning . The images and prompts used to construct the preference data are randomly sampled from the detailed description and complex reasoning subclasses of the LLaVA150k dataset, totaling approximately 13,000 samples . It is worth noting that each iteration uses the same prompt and image as the previous round. Overall, the iterative training is conducted over three iterations, completed on one A100 80GB GPU. It takes roughly 3.5 and 5 hours for fine-tuning LLaVA-1.5 7B and LLaVA-1.5 13B, respectively. For more detailed information on training hyperparameters and training data, please refer to Appendix A.1.

**Evaluation Benchmarks.** We conducted evaluations on three types of benchmarks: comprehensive benchmarks, general VQA and hallucination benchmarks. Specifically, this includes: (1) Comprehensive benchmarks (MME , SEEDbench , LLaVA\({}^{}\), MMbench , MM-Vet ); (2) General VQA (ScienceQA (SQA) , VisWiz , GQA ); (3) Hallucination benchmark (POPE , CHAIR ). More detailed description are discussed in Appendix A.1.

**Baselines.** We will first compare CSR with the self-rewarding approach described by Yuan et al. . Here, we directly apply self-rewarding to LVLM, using the prompts and experimental settings outlined in Yuan et al.  (see detailed settings in Appendix A.1 and Table 3). We also compared CSR with several data-driven preference learning methods, including Silkie (Vlfeedback) , LLaVA-RLHF (Human-preference) , POVID , and RLHF-V . Furthermore, we compared the performance of the optimized LLaVA-1.5 via CSR with other state-of-the-art open-source LVLMs, including InstructBLIP , Qwen-VL-Chat , mPLUG-Owl2 , BLIP-2 , and IDEFICS , after the final rounds of training (CSR with iteration = 3). Additionally, to evaluate the effectiveness of CSR on other LVLMs, we applied CSR to a recent LVLM called Vila . For more information on these baselines, please refer to Appendix A.1.

### Results

**CSR Continuously Improves Model Performance over Iterations.** In Figure 3, we report the average performance of LLaVA-1.5 7B and 13B models concerning the number of training iterations on comprehensive benchmarks, general VQA tasks, and hallucination benchmarks. To facilitate score calculation, we first calculated an average score on a 100-point scale by adjusting the original values: \(^{}\) was divided by 16, and \(^{}\) was divided by 4, corresponding to the number of categories in MME. Additionally, since a lower CHAIR value indicates better performance, we standardized all metrics to follow a higher is better approach by transforming the \(_{}\) and \(_{}\) metrics into 100 - \(_{}\) and 100 - \(_{}\). We then calculated the average score by averaging these standardized values, which were used to compute the average percentage increase. In the experiment, the 7B model achieved an improvement of approximately 7.62% across all benchmarks through online iterative updates, while the 13B model saw an improvement of approximately 5.25%. According to the full results in Table 6 and Table 7 of Appendix A.5, the improvement is particularly significant on the LLaVA\({}^{}\) and CHAIR benchmarks, with improvements of 8.9% and 49.50%, respectively. The results indicate that CSR is capable of incrementally improving model performance over iterations, demonstrating its effectiveness in self-improving the quality of generated preference data and leading to stronger modality alignment. The degree of improvement gradually becomes smaller, which is not surprising, indicating that the model is gradually converging.

**CSR Outperforms Competitive Preference Fine-Tuning Baselines.** Compared to preference data curation approaches (e.g., POVID, RHLF-V) that generate preference data from either additional models or human annotations, the superiority of CSR indicates that adapting a self-rewarding paradigm better captures the inherent preferences of the target LVLMs, achieving stronger modality alignment. Furthermore, CSR outperforms existing self-rewarding methods, with an average performance improvement of

    &  &  &  \\  Method & \(^{}\) & \(^{}\) & SEED & LLaVA\({}^{}\) & MMB & MM-Vet & \(^{}\) & VisWiz & GQA & POPE & \(_{}\) & \(_{}\) \\  LLaVA-1.5-18 & 1510.7 & 348.2 & 58.6 & 63.4 & 64.3 & 30.5 & 66.8 & 50.0 & 62.0 & 85.90 & 48.8 & 14.9 \\ + Vlfeedback & 1432.7 & 321.8 & 59.3 & 62.1 & 64.0 & 31.2 & 66.2 & 52.6 & **63.2** & 83.72 & 40.3 & 13.2 \\ + Human-Prefer & 1490.6 & 335.0 & 58.1 & 63.7 & 63.4 & 31.1 & 65.8 & 51.7 & 61.3 & 81.50 & 38.7 & 11.3 \\ + POVID & 1452.8 & 325.3 & 60.2 & 68.7 & 64.9 & 31.8 & 68.8 & 53.6 & 61.7 & 86.90 & 35.2 & 8.3 \\ + RLHF-V & 1489.2 & 349.4 & 60.1 & 65.4 & 63.6 & 30.9 & 67.1 & **54.2** & 62.1 & 86.20 & 29.7 & 7.5 \\ + Self-rewarding & 1505.6 & 362.5 & 60.0 & 61.2 & 64.5 & 31.4 & 69.6 & 53.9 & 61.7 & 86.88 & 24.0 & 6.7 \\ + **CSR (Ours)** & **1524.2** & **367.9** & **60.3** & **71.1** & **65.4** & **33.9** & **70.7** & 54.1 & 62.3 & **87.01** & **21.0** & **6.0** \\  LLaVA-1.5-13B & **1531.3** & 295.4 & 61.6 & 70.7 & 67.7 & 35.4 & 71.6 & 53.6 & 63.3 & 85.90 & 48.3 & 14.1 \\ + Self-rewarding & 1529.0 & 300.1 & 62.8 & 65.6 & 64.5 & 35.3 & 74.3 & 56.1 & 63.2 & 86.58 & 37.0 & 8.8 \\ + **CSR (Ours)** & 1530.6 & **303.9** & **62.9** & **74.7** & **68.8** & **37.8** & **75.1** & **56.8** & **63.7** & **87.30** & **28.0** & **7.3** \\   

Table 1: The performance of CSR on LLaVA-1.5 across all benchmarks is presented. Most baseline results, except those for self-rewarding, are sourced from Zhou et al. .

   Method & 7B & 13B \\  Base & 66.61 & 68.08 \\ Only \(_{T}\) & 68.46 & 68.12 \\ Only \(_{I}\) & 67.49 & 69.23 \\
**CSR (Ours)** & **72.39** & **71.95** \\   

Table 2: Ablation study of vision-text reward score.

Figure 3: Average scores of CSR at different iterations over all benchmarks (see Table 6 and Table 7 in Appendix A.5 for full results).

2.43%, demonstrating its effectiveness in calibrating the reward model by incorporating image-response relevance scores. This mitigates the potential issue of overlooking visual input information when estimating self-generated preferences.

In addition, we compare the performance of LLAVA-1.5 after three rounds of online CSR with other state-of-the-art open-sourced VLLMs and report the results in Table 5 of Appendix A.5. Although different open-sourced VLLMs utilize various image and text encoders, CSR still outperforms other open-sourced VLLMs in 9 out of 10 benchmarks, further corroborating the effectiveness of CSR in improving modality alignment.

### Analysis

**Ablation Study.** To validate the effectiveness of using the image-response relevance score (\(R_{I}\)) to complement the self-generated instruction following score (\(R_{T}\)), we specifically compare CSR with three variants: (1) without applying CSR on LLAVA 1.5 (Base); (2) using CSR with only the self-generated instruction following score (Only \(R_{T}\)); and (3) using CSR with only the image-response relevance score (Only \(R_{I}\)). The results are reported in Table 2. We first observe that CSR improves performance by jointly considering both the self-generated instruction following and image-response relevance scores. This verifies its effectiveness in enhancing modality alignment by calibrating the language-driven self-rewarding paradigm with visual constraints. Additionally, we further conduct the analysis on the change of \(\) in Eqn (4) and found that incorporating external visual scores to calibrate the models rewarding process effectively enhances performance (see detailed results in Appendix A.5.)

**Compatibility Analysis.** To validate CSR for its applicability to other LVLMs, we deployed CSR on Vila 7B and conducted three rounds of online iterations. We conducted experiments on all ten evaluation benchmarks and tasks, and the results are shown in Figure 4. Similar to the findings in Figure 3, Vila demonstrates a similar phenomenon during the online iterations of CSR, where it can self-correct preferences, leading to gradual improvements in all benchmarks. For Vila, the overall performance improved by 3.37% after three rounds of CSR iterations, with particularly notable increases of 8.48% on VisWiz and 14.0% on MM-Vet. The compatibility analysis further corroborates the generalizability and effectiveness of CSR in enhancing the performance of LVLMs.

**How Does CSR Change the Image-Response Relevance Over Iterations?** To investigate how CSR gradually improve the performance over iterations, we analyzed the change of self-generated preference data with the LLAVA-1.5 7B model. In Figure 5, we illustrated the distribution of image-response relevance scores of three iterations over 500 examples from LLAVA-150k . We first observe that both the chosen (preferred) and rejected (dispreferred) responses achieve higher image-response relevance scores after the model undergoes CSR online iterations. This indicates that, following CSR, the responses generated by LVLMs are more closely aligned with the image information. Secondly, it can be observed that after multiple rounds of online iterations with CSR, the average image-response relevance scores for the rejected and chosen responses become closer to each other. This makes the self-generated preference data during CSR iterations more challenging to distinguish, while further strengthening the learning process.

**How Does CSR Improve Modality Alignment?** To further understand how CSR affects modality alignment, in Figure 6, we present the changes in image and text attention maps for three models: the original LLAVA-1.5 7B model, the self-rewarding approach, and CSR. These attention maps illustrate the distribution of attention scores over image and text tokens. We observe that applying CSR strengthens the model's attention to certain visual tokens. Simultaneously, the change of attention values of the text tokens indicates that CSR is capable of alleviating

Figure 4: Average scores of CSR in Vila 7B at different iterations over all benchmarks (see Table 8 in Appendix A.5 for full results).

Figure 5: Image relevance scores before and after employing CSR.

[MISSING_PAGE_FAIL:8]

typically comprises discrete tokens, we follow the CLIP theory literature  in modeling them as continuous-value random vectors in this section to elucidate the rationale behind our proposed method. More specifically, we assume the following data generative model for \(x_{v}\) and \(x_{t}\):

\[x_{v}=U_{1}z_{1}+_{1},x_{t}=U_{2}z_{2}+_{2},\]

where \(U_{1}^{d_{v} r}\) and \(U_{2}^{d_{t} r}\) are two orthonormal matrixces, representing decoders that transform the latent (low-dimensional) signals \(z_{1},z_{2}^{r}\) to images and text respectively. We assume the covariance matrices of \(z_{1},z_{2}\) are identity matrices. \(_{1}^{d_{v}}\) and \(_{2}^{d_{t}}\) are noise vectors, and we assume they follow sub-gaussian distributions with well-conditioned covariance matrices and sub-gaussian norms upper bounded by a universal constant. We consider the infinite data setting. This is a widely used simplification to avoid the influence of sample randomness . According to , with an abundance of image-text pairs, the learned visual CLIP embedding \(_{I}(x_{v})\) and textual CLIP embedding \(_{T}(x_{t})\) converge to \(U_{1}^{}x_{v}\) and \(U_{2}^{}x_{t}\) respectively. To simplify our analysis without loss of generality, we consider a single score for each response \(y\) and define the image-response relevance score \(R_{I}(y)= U_{1}^{}x_{v},U_{2}^{}y\).

We assume the ground truth \(y_{truth}=V_{1}^{*}x_{v}+V_{2}^{*}x_{t}+_{y}\) with weights \(V_{1}^{*}^{d_{v} d_{v}}\) and \(V_{2}^{*}^{d_{x} d_{t}}\). In CSR, we assume the conditional distribution at iteration \(t\), \(_{_{t}}(y x)\) with \(_{t}=(V_{1},V_{2})\), follows a Gaussian distribution \(_{_{t}}(y x)(-\|y-(V_{1}x_{v}+V_{2}x_{t})\|^{2}/^ {2})\), where \(V_{1}^{d_{v} d_{v}}\) and \(V_{2}^{d_{v} d_{t}}\) are the weights matrices for the image and text inputs respectively, and \(>0\) is the standard deviation. As the likelihood is monotonically decreasing with respect to \(\|y-(V_{1}x_{v}+V_{2}x_{t})\|^{2}\), we consider the self-generated instruction-following score \(R_{T}(y)=-\|y-(V_{1}x_{v}+V_{2}x_{t})\|^{2}\). Then the calibrated reward score becomes \(R(y)= R_{I}(y)+(1-) R_{T}(y)\), for some \(\). In theoretical analysis, we consider a simpler version of CSR, where we assume \(y_{w}=_{y}R(y)\) (whose distribution is denoted by \(p_{_{t}}^{*}(y x)\)), and \(y_{l}\) is the text output generated by \(_{_{t}}(y x)\). As \(R(y)\) depends on \(\), we denote the solution \(_{t+1}\) by \(_{t+1}()\). In the special case where \(=1\), this corresponds to the setting where we do not use the image-response relevance score at all.

Figure 7: Two cases selected from the CSR-generated datasets.

To evaluate the quality of the text output \(y\), we consider a regression problem where there is an outcome \(z\) associated with the ground-truth text output \(y_{truth}\): \(z=^{*}y_{truth}\) with \(^{*}^{d_{}}\). We evaluate the quality of \(y\) by considering the loss function \(L(y)=_{^{d_{}}}[(z-^{}y)^{2}]\). We then have the following theorem.

**Theorem 5.1**.: _Suppose that \(_{_{}}^{*}(y x)\) lies in the LLM space \(\{_{}(y x):\}\), \(\|{{^{*}}^{}{V_{1}^{*}}^{}^{*}}\|\|{{^{*}}^{}{V_ {2}^{*}}^{}^{*}}\|\) and \(\|{{^{*}}^{}{V_{1}^{}}^{}^{*}}\|\|{{^{*}}^{} {V_{2}^{}}^{*}}\|\), then there exists \(<1\), such that_

\[_{_{_{t+1}()}(y x)}[L(y)]<_{_{ _{t+1}(1)}(y x)}[L(y)].\]

Our theoretical analysis implies that as long as \(\|{{^{*}}^{}{V_{1}^{}}^{*}}\|\|{{^{*}}^{}{V_{2}^{ }}^{*}}\|\), which happens when the model tends to prioritize textual information over visual input. By incorporating the image-response relevance score (corresponding to \(<1\)), CSR is able to increase the attention on image signals in generating \(y\). As a result, the solution produced by CSR will be better than the method without using the image-response relevance score (corresponding to \(=1\)).

## 6 Related Work

**Large Visual-Language Model Hallucination.** Recently, the rapid development of visual-language alignment methods [19; 44; 45; 46; 47; 48; 49] and LLMs [50; 51; 52; 53; 54] has significantly accelerated the progress of LVLMs, which extend LLMs with visual modalities and demonstrate impressive visual understanding by unifying the encoding of visual and text tokens [34; 55; 56; 57]. However, LVLMs still face the problem of hallucination [58; 59], where generated text descriptions contradict the visual modality information. Various approaches have been proposed to address hallucination in LVLMs, including enhancing dataset quality for fine-tuning [60; 8; 61; 9], manipulating the decoding process [62; 63; 64; 37; 65; 66], and leveraging external closed-source models to facilitate post-hoc mitigation of hallucination [58; 67; 68; 69; 70]. Though these approaches alleviate hallucination to some extent, they do not focus directly on improving modality alignment.

**Preference and Modality Alignment.** In large models, alignment is necessary to ensure their behavior aligns with human preferences [71; 15; 72]. In LVLMs, alignment manifests as modality misalignment, where the generated textual responses are supposed to follow the input visual information. Recently, preference optimization has been used to address the modality misalignment problem. These optimizations involve preference data curated by human annotators [8; 60; 30] and additional models (e.g., GPT-4) [9; 10]. While these methods improve the ability of LVLMs to align modalities, their reliance on human annotation or additional models is resource-intensive and may introduce additional biases. Furthermore, these models cannot fully capture the inherent preferences of LVLMs, making the curated preference data less effective. Instead, CSR leverages a calibrated self-rewarding strategy, aiming to stimulate the LVLMs' self-correction and enhancement capabilities, thereby further improving modality alignment.

**Self-Improvement in Large Language Models.** Self-improvement emerges as a powerful paradigm for LLMs to enhance themselves without significant external intervention. For example, self-rewarding and online alignment propose a method that selects consistent answers generated by the model to fine-tune itself [73; 74], thereby improving its reasoning ability. Similarly, Chen et al.  utilizes self-play to enhance the model's performance by distinguishing its self-generated responses from those in human-annotated training data. Unlike prior methods that primarily target LLMs, CSR addresses the modality misalignment issue in LVLMs during the preference modeling process by introducing visual constraints, making it particularly well-suited for LVLMs.

## 7 Conclusion

In this paper, we investigate the challenge of enhancing modality alignment in LVLMs by introducing a calibrated self-rewarding approach, which integrates visual constraints into the preference modeling process of the self-rewarding paradigm. Empirically, CSR enhances the alignment between image and text modalities, significantly reducing hallucination and improving performance on various LVLM evaluation benchmarks. These empirical results are further supported by rigorous theoretical findings. Additionally, CSR is capable of continuously enhancing LVLM capabilities over iterations, leading to better utilization of visual information.