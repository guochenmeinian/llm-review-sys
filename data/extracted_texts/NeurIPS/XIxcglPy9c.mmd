# Latent Space Simulator for Unveiling Molecular Free Energy Landscapes and Predicting Transition Dynamics

Latent Space Simulator for Unveiling Molecular Free Energy Landscapes and Predicting Transition Dynamics

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Free Energy Surfaces (FES) and metastable transition rates are key elements in understanding the behaviour of molecules within a system. However, the typical approaches require computing force-fields across billions of time-steps in a molecular dynamics (MD) simulation, which is often considered intractable when dealing with large systems or databases. In this work we propose LaMoDy, a latent-space MD simulator to effectively tackle the intractability with around 20-fold speed improvements compared to classical MD's. The model leverages a chirality aware \(SE(3)\)-invariant encoder-decoder architecture to generate a latent space, coupled with a recurrent neural network to run the time-wise dynamics. We show that LaMoDy effectively recovers realistic trajectories and FES more accurately and faster than existing methods, while capturing their major dynamical and conformational properties. Furthermore, the proposed approach can generalize to molecules outside the training distribution.

## 1 Introduction

Fundamental quantities of interest towards understanding a molecule's dynamics and properties are its Free Energy Surface (FES) and metastable states, alongside its transition rates between metastable states. Accessing them enables many real-world applications in drug discovery or material sciences (Peng et al., 2014; Bochevarov et al., 2013). Each 3D conformation of a molecule is associated with a potential energy that determines its probability of occurring (via a Boltzmann distribution).

The FES is a lower-dimensional representation of this energy landscape, providing insights into stable states (energy minima), transition pathways, and free energy differences. Additionally, a molecule's kinetics are of interest, such as the transition rates between metastable states/modes of the Boltzmann distribution.

The usual approach to compute these properties is to run long micro-second molecular dynamics (MD) simulations. Considering that each MD step is

Figure 1: Free Energy Surface (FES) with minima corresponding to different conformations and an example MD trajectory as dotted arrow.

in the scale of femto-seconds, the simulation comes with a high computational cost. To accelerate the recovery of these properties, it is essential to develop a method that (1) can operate at time steps beyond the femtosecond level; (2) captures the key reaction coordinates; (3) does not suffer from instabilities (unphysical states) for long-time simulations.

Learned simulators operating in a latent space suit these requirements if the latent space captures reaction coordinates (a molecule's most important degrees of freedom) since they allow for larger time steps (Sidky et al., 2020; Vlachas et al., 2022). However, existing architectures restrict the simulator to only work on a single molecule at a time, meaning that they cannot generalize to new molecules (Sidky et al., 2020; Vlachas et al., 2022). Furthermore, LED (Vlachas et al., 2022) fails to recover rare metastable states and lacks practical relevance as it has only been shown to work with multiple re-initializations from Boltzmann distributed states, meaning that a long MD simulation is still required to define the starting states.

Other approaches, such as Boltzmann generators (Noe et al., 2019) or Distributional Graphormer (Zheng et al., 2023) can predict the equilibrium distribution of unseen molecules but do not have a notion of time, i.e., no dynamical properties such as the transition rates can be extracted. In this regard, machine learning (ML) force fields (Unke et al., 2021; Batzner et al., 2022; Hu et al., 2021) have made significant progress for ab-initio simulations but are still slower for long simulations and larger molecules where classical force fields are applied (Fu et al., 2023).

To tackle these limitations, we propose a learned Latent Molecular Dynamics LaMoDy, model. We employ an \(SE(3)\)-invariant encoder-propagator-decoder scheme based on message-passing neural networks (MPNN) (Gilmer et al., 2017) that can be trained end-to-end on MD data and can generalize to unseen molecules. For the tasks of FES recovery, past studies used different sampling and evaluation protocols, making it difficult to compare methods. We define scientifically meaningful tasks and metrics that allow that reflect a model's practical relevance in probing the free energy surface of molecules. In summary, our contributions are:

* 20-fold speed improvements compared to classical MD, thanks to a long operating time step of \(100fs\).
* Generalization to unseen molecules thanks to our chirality-aware \(SE(3)\)-invariant encoder-decoder.
* Defining a systematic evaluation scheme to assess the performance of simulation methods against scientifically meaningful tasks for FES recovery.

Figure 2: Overview of LaMoDy. An encoder \(\) computes \(SE(3)\)-invariant latent embeddings of a short initialization sequence, the dynamical propagator \(\) iteratively predicts the next states to produce a long-time trajectory in latent space from which molecular conformers can be reconstructed by the decoder \(\). The warm-up sequence and predicted trajectory are visualized in the FES. Here, \((0,)\) denotes random noise, \(\) is vector addition, \(_{t}\) denotes the 3D graph representation of a molecule at time \(t\), \(\) is a latent space state, \(\) is the time lag between states in a trajectory, and * denotes the point where the MD trajectory crosses the plane.

Related work

**Enhanced sampling** methods inject bias to the potential energy function to facilitate fast sampling of transitions between local energy minima that are separated by high energy barriers. Popular methods include simulated annealing (Bernardi et al., 2015; Tsallis and Stariolo, 1996), metadynamics (Laio and Gervasio, 2008), replica exchange (Bernardi et al., 2015), umbrella sampling (Torrie and Valleau, 1977), and parallel tempering Yang et al. (2019). A major limitation of enhanced sampling methods lies in the fact that they typically require determining collective variables (CVs) in advance, which can be challenging for complex systems Wang et al. (2021). Furthermore, enhanced sampling methods do not have an explicit notion of "time", meaning that no extraction of dynamical properties is possible (Stelzl and Hummer, 2017).

**Latent Space Simulators** enable to accelerate MD simulations in the 3D configuration space, by updating a latent state generated by a learned encoder, instead of moving each atom according to its velocity and computed force. The updates are performed by a dynamical propagator, and the all-atom representation can be constructed with a decoder. Time-lagged autoencoders with propagators (Otto and Rowley, 2019; Lusch et al., 2018) learn a linear propagator whereas Sidky et al. (2020) use a mixture density network (Bishop, 1994) as a propagator. However, the above methods do not obey the \(SE(3)\)-invariance of molecules (they could, e.g., arbitrarily flip a chirality each step). Vlachas et al. (2022) train an LSTM network as propagator and account use a mixture density network as autoencoder. However, this method requires multiple re-initializations from Boltzmann distributed states and it remains unclear if the method stays stable for longer simulations. Additionally, all previously mentioned methods only work on a single molecule they have been trained on - they are not able to generalize unlike LaMoDy.

## 3 Method

### Model Architecture

**Encoder** To make the encoder architecture generalizable to other molecules, we use a graph representation of internal coordinates and employ a Graph Neural Network (GNN) architecture. Concretely, a molecular state is represented by a graph \((,,,)\) with each node representing a bond in the original molecule, and edges representing bond angles and torsion angles defined by triplets and quadruplets of bonds respectively, hence \(||=||\) and \(||=||+||\). Nodes are featurized with information about the atoms forming the bond and the bond length and edges are featurized with the respective bond or torsion angle and a categorical feature indicating whether the edge defines a bond angle or a torsion angle 1. We then employ \(L\) message-passing layers akin to Shi et al. (2021), pool the nodes using a learnable set-to-set mapping (Vinyals et al., 2016), and predict the final latent vector using a linear layer.

**Decoder** To reconstruct the internal coordinates of a molecular state given a latent representation, we use a second GNN similar to Winter et al. (2021). The decoder takes as input a two-dimensional molecular graph with nodes representing atoms and edges representing bonds and a latent vector describing the molecular state in the latent space. First node level embeddings are computed by iteratively applying a sequence of message-passing layers similar to the encoder. Then, bond lengths are predicted by applying a three-layer MLP onto the concatenated pairs of

Figure 3: Training scheme for long sequences: The propagator \(\) takes in a latent state \(_{t}\) and cell state \(_{t}\) to predict the latent state at time \(t+1\). The cell states are not re-initialized and gradients are detached after a fixed-length interval.

nodes and the latent embedding, i.e. \(d_{i}=_{bond}([h_{a},h_{b},z])\) with \(h_{*}\) being the node embeddings, \(z\) the latent vector and \(_{bond}\) the MLP. The same approach is taken for bond angles and torsion angles with triplets/quadruplets of node embeddings and \(_{ang}._{tor}\) respectively.

**Dynamical Propagator** As suggested by Vlachas et al. (2022), sequences of MD states are not necessarily Markovian since complex systems can exhibit long-term correlations in their behavior, meaning that future states can depend on past states, violating the assumption of independence between time steps. To account for this, we use an LSTM (Hochreiter and Schmidhuber, 1997) as the dynamical model that is trained to predict the next latent state given a short history. Concretely, we use

\[(_{t+},_{t+})&=LSTM (_{t},_{t},_{t})\\ _{t+}&=(_{t+})\] (1)

where \(_{t},_{t}\) denote the LSTM hidden state and cell state at time \(t\), \(_{t}\) is the latent state at time \(t\) and \(\) is a two-layer MLP.

### Training

We train our model end-to-end on MD data. To do so, we randomly sample a batch of starting points from the dataset from which we consider the consecutive \(k\) states with a time lag \(\) between states. Hence, we end up with a batch of sub-sequences of the full trajectory of length \(k+1\) states. Starting with an initial LSTM state of \(_{0}=(_{0},_{0})=(,)\), we iteratively unfold the LSTM to predict the next time step, while the LSTM cell states are passed through time. More specifically, we encode \(_{0}\) into latent space by \(z_{0}=(_{0})\), from which together with \(_{0}\) the next time step latent state \(}_{1}\) is predicted. Then \(_{1}\) and \(_{1}=(_{1})\) are used to predict \(}_{2}\), which can all be decoded back to molecular states.

To optimize the parameters of the model with backpropagation, we define an end-to-end propagation loss that is additionally regularized by a reconstruction loss and a latent loss :

\[&=_{e2e} _{i=1}^{k}_{rec}[_{i},(_{i-1})]\\ &+_{lat}_{i=1}^{k}||_{i}-}_ {i}||^{2}+\ _{rec}_{i=0}^{k}_{rec}[ _{i},(_{i})]\] (2)

here \(_{rec},_{lat},_{e2e}\) are hyperparameters and \(_{rec}\) is defined as in Equation 11. Note that \(_{i}=(_{i}),\ }_{i}= (_{i-1})\). Although the end-to-end part of our loss function theoretically encapsulates the latent and the reconstruction loss, we found the explicit presence of both as additional regularization to be crucial for the training process to succeed.

**Training on long sequences** As we aim to predict long-timescale trajectories at inference time with \(N_{steps} k\), we require training on long sequences without suffering from vanishing or exploding gradients. To do so, we sample sub-trajectories of length \(c*k\) with \(c\) being a hyperparameter and iteratively train on sequences of length \(k\) where we keep the LSTM states but detach the gradients as suggested by Vlachas et al. (2022).

### Inference

At inference time, we "warm up" the LSTM with a sequence of \(k\) MD states from which we iteratively unfold the propagator to predict latent trajectories. Additionally, we infuse artificial noise to the latent states before feeding them into the propagator. We found this to be crucial because otherwise, the dynamical model was prone to become stuck at a local energy minimum. Concretely, we predict the next latent state by :

\[}_{t+}=(}_{t}+(0,)),&x U(0,1)\\ (}_{t}),&\] (3)

where \(\) is a hyperparameter, \(x U(0,1)\) indicates a sample from the uniform distribution and \(=*^{2},^{2}^{+}\) is computed from the warmup trajectory.

## 4 Evaluation Protocol for FES recovery

This section aims to provide an evaluation protocol that is both robust and scalable. After identifying the issues with prior metrics, we propose a method of identifying metastable states and measuring the agreement between the model and the ground truth.

**Deficiencies of Past Metrics** Past studies have used different tasks and metrics for evaluation, making it difficult to compare methods. The metastable states of the free energy surface are frequently used for evaluation as they allow to reason about dominant conformations and transition rates. However, previous evaluation protocols are often not applicable to multiple systems but only allow qualitative inspection of single molecules at a time. To overcome these challenges, we propose a systematic evaluation protocol to reliably assess the quality of predicted trajectories for multiple systems.

A common practice to evaluate the quality of predicted FES is to use Kullback-Leibler (KL) divergences, either between one-dimensional marginals or the two-dimensional histogram (Klein et al., 2023). However, this method is heavily dependent on the chosen bin size of the histogram and ignores the fact that variations in the estimated density are negligible for multiple practical applications, where the correct identification of modes and transition rates is the desired goal.

Work on conformation generation (Jing et al., 2022; Zhu et al., 2023) is typically evaluated by computing the coverage of predicted structures (in terms of RMSD) and reporting precision and recall, i.e. the fraction of correctly predicted structures and the fraction of identified structures compared to MD. Similar to the KL-based metrics, this protocol does not capture whether modes and transition rates are correctly identified.

**Identifying metastable states** Identifying modes in a two-dimensional FES is highly non-trivial. While previous works used k-means clustering to identify metastable states (Pandey et al., 2023; Jain and Stock, 2012), we found that k-means frequently converges to incorrect minima. Therefore, we use the method of Novelli et al. (2022) where the FES is first smoothed using a Gaussian kernel and local minima are identified via running multiple BFGS solvers from random starting points. For a detailed explanation, we refer to subsection B.3. Lastly, the identification of reaction coordinates varies across past methods where multiple methods a sophisticated scheme such as Time-Independent-Component-Analysis (TICA) (Perez-Hernandez et al., 2013) to define the reaction coordinates from which the FES is constructed (Sidky et al., 2020; Klein et al., 2023). While TICA is useful for a variety of applications, it requires a Chapman-Kolmogorov test and manual inspection of the lag time to guarantee high-quality dimensionality reduction. Therefore, we use the two dihedral angles \(,\) as they are known to capture the conformation space of peptides (Choudhuri, 2014).

Figure 4: MSPR: Metastable State Precision/Recall; Ramachandran plot of a long and a short MD simulation for a peptide where identified metastable states are indicated by crosses. The third figure shows the long MD trajectory with modes identified by the short MD simulation superimposed and the circles denote the area where a mode is considered to be correct. This allows to compute metastable state precision and recall (MSPR).

**Metrics** With the above-described procedure, we can identify metastable states without the need of manual specification. This allows to compute precision and recall in terms of found metastable states, i.e. the fraction of correctly predicted modes and the percentage of modes found where a mode is considered correct if it lies within close proximity to the ground-truth MD mode 2.

Furthermore, the transition rates between these identified metastable states are relevant for many applications, such as inferring relaxation times or reaction rates, and can be studied using a Markov State Model (MSM) (Bowman et al., 2014). Hence, an MSM can be fitted to predicted and MD trajectories, allowing to compare transition rates. Specifically, the Mean First Passage Times (MFPTs) (Hoel et al., 1986) can be computed which represent the expected times for a transition to happen from a predefined origin state to a target state. The relative error across the MFPTs for multiple molecules compared to MD then gives insight about the practical use of the predicted dynamical properties.

## 5 Experimental Results

In this section, we first show LaMoDy's ability to recover the dynamics and transition states of alanine dipeptide, then show that it effectively generalizes across peptides. We further demonstrate the large benefits of LaMoDy in terms of simulation speed in Appendix B. Finally, we do ablation studies on some of the architectural choices.

### Alanine Dipeptide

Before we evaluate the generalization capabilities to unseen molecules, we test our method on a single molecule, namely alanine dipeptide (ALDP), which is a widely used benchmark for MD simulators and has been the subject of evaluation in previous works. In the case of ALDP, the primary degrees of freedom under consideration are the two backbone dihedral angles \(\) and \(\). Despite the model being trained on this exact molecule, it's important to note that recovering long-time FES and transition rates remains highly nontrivial, as dynamical models are typically designed to predict single or a limited number of steps. Specifically, we train on \(100ns\) of MD data of ALDP in implicit solvent to assess whether the model can qualitatively reproduce the free energy surface in terms of the backbone dihedral angles. Additionally, we analyze the model's ability to predict transition rates between the identified metastable states, comparing them to MD results.

**FES recovery** To use the trained model for simulating MD trajectories, we use the procedure described above. Starting from an initialization sequence of five states, we simulate a trajectory of

Figure 5: Example MSM with three states fitted to MD trajectory with transition probabilities.

Figure 6: Ramachandran plots of trajectories from MD data and predictions of our model for alanine dipeptide with corresponding metastable states as defined by Vlachas et al. (2022).

length \(100ns\) without re-initialization. The Ramachandran plots of the predicted trajectory alongside the MD simulation are visualized in Figure 6. Figure 6 shows that our model is able to capture all metastable states without becoming unstable, i.e. no unphysical states are visited throughout the entire simulation. Notably, the model is able to explore the rare states \(C_{7}^{ax},_{L}\), which previous latent space simulators (Vlachas et al., 2022) failed to achieve. The Ramachandran plots also show that our model slightly overestimates the density of \(_{R}\).

**Transition dynamics** To examine whether the overestimation of \(_{R}\) leads to unrealistic dynamical properties, we can compare the transition rates extracted from MSMs fitted to MD data as well as the predicted trajectory, which are shown in Figure 7. The transition probabilities clearly show that the dynamical properties that can be inferred from the model predictions closely match the true dynamics. Even for the highly unlikely states, our model approximates the correct transition rates. We found the training scheme for long trajectories as described above to be crucial for this.

### Generalization across Molecules

After this first sanity check, we assess the capability of our approach to generalize to unseen molecules. To do so, we constructed a dataset of 216 dipeptides3 with a length of \(12ns\) each of which 200 are used for training and 16 are held out for evaluation. We use the systematic evaluation protocol introduced in section 4.

**FES recovery** In contrast to prior work on latent space simulators (Sidky et al., 2020; Vlachas et al., 2022) where the model can only be evaluated on the same molecule it has been trained on, our architecture is not restricted to single molecules. We evaluate the peptide model on 16 unseen molecules and randomly choose 16 peptides from the training set as a comparison. Figure 8 shows the precision and recall values the dipeptide model achieved. We can observe, that the model is better in terms of precision than recall. This suggests, that the learned simulator is more "conservative"

Figure 8: Metastable state precision and recall (MSPR) for train and test samples of the dipeptide model.

Figure 7: Transition probabilities of MSMs for alanine dipeptide estimated from MD data and predictions of our model. Black squares are transitions that were never observed.

and avoids predicting unphysical modes rather than exploring the full state space which is desirable. However, Figure 8 also shows that the model fails to recover the correct metastable states for a subset of the peptides.

**Predicting transition dynamics** To gain more insight into the predicted trajectories, we evaluate the relative error between predicted and MD MFPTs for MSMs constructed from correctly identified states as defined in section 4. The results of this analysis are shown in Figure 9 where peptides that only contain one mode are excluded, as the MFPT error would be 0 in this case (only one state in the MSM, so no transitions). Figure 9 shows that the mean relative error is below 0.5 except for two peptides from the training set and two peptides from the test set. This confirms the previous results, i.e. that the model can approximate the majority of peptides very well, but misses a small subset. Furthermore, this metric shows that the modes which are found by the model are captured accurately and the transitions between the modes are captured within a relative error that existing latent space simulators (Vlachas et al., 2022) achieve for a single molecule they have been trained on. Furthermore, this shows the practical use of this method, as it can quickly and efficiently recover the leading states of unseen molecules from which accurate transition rates can be extracted making this model especially useful for screening large chemical spaces.

## 6 Discussion

We present MSPR, a reliable evaluation metric for FES that tackles the necessity of comparable evaluation schemes for learnerd simulators. Additionally, we introduce LaMODy, a learned simulator operating in a latent space to efficiently recover free energy surfaces and transition rates. LaMODy is trained end-to-end on MD data constructing its own latent space. The model employs an \(SE(3)\)-invariant encoder-propagator-decoder scheme. We show that our method can operate at integration time steps that are two orders of magnitude larger than for MD while still being able to conduct stable long-timescale simulations required for recovering properties such as FES and transition rates.

In contrast to prior works, LaMoDy does not require re-initialization throughout the simulation, removing the need for prior MD simulations. We demonstrate that the predicted trajectories closely match the results of MD and correct dynamical properties can be recovered even for rare metastable states. Furthermore, our model is generalizable to molecules outside its training distribution and can capture their leading structural and dynamical properties. Overall, our approach is approximately 20 times faster at recovering FES and transition rates than classical MD and can additionally easily be parallelized for up to 128 peptides on a single GPU.

Figure 9: Mean relative error of MFPTs for MSMs fitted to predicted trajectories compared to MD for train and test set. Correctly extracted metastable states from the predicted trajectory are used to construct MSMs on MD and predicted data. Peptides where only one metastable state exists and therefore the MFPT error would always be zero are held out.