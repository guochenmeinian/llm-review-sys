# Provable Partially Observable Reinforcement Learning with Privileged Information

Yang Cai\({}^{1}\) Xiangyu Liu\({}^{2}\) Argyris Oikonomou\({}^{1}\) Kaiqing Zhang\({}^{2}\)

\({}^{1}\) Yale University \({}^{2}\)University of Maryland, College Park

yang.cai@yale.edu, xyliu999@umd.edu

argyris.oikonomou@yale.edu, kaiqing@umd.edu

###### Abstract

Partial observability of the underlying states generally presents significant challenges for reinforcement learning (RL). In practice, certain _privileged information_, e.g., the access to states from simulators, has been exploited in training and has achieved prominent empirical successes. To better understand the benefits of privileged information, we revisit and examine several simple and practically used paradigms in this setting. Specifically, we first formalize the empirical paradigm of _expert distillation_ (also known as _teacher-student_ learning), demonstrating its pitfall in finding near-optimal policies. We then identify a condition of the partially observable environment, the _deterministic filter condition_, under which expert distillation achieves sample and computational complexities that are _both_ polynomial. Furthermore, we investigate another successful empirical paradigm of _asymmetric actor-critic_, and focus on the more challenging setting of observable partially observable Markov decision processes. We develop a belief-weighted asymmetric actor-critic algorithm with polynomial sample and quasi-polynomial computational complexities, in which one key component is a new provable oracle for learning belief states that preserves _filter stability_ under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL (MARL) with privileged information. We develop algorithms featuring _centralized-training-with-decentralized-execution_, a popular framework in empirical MARL, with polynomial sample and (quasi-)polynomial computational complexities in both paradigms above. Compared with a few recent related theoretical studies, our focus is on understanding practically inspired algorithmic paradigms, without computationally intractable oracles.

## 1 Introduction

In most real-world applications of reinforcement learning (RL), e.g., perception-based robot learning , autonomous driving , dialogue systems , and clinical trials , only _partial observations_ of the environment state are available for sequential decision-making. Such partial observability presents significant challenges for efficient decision-making and learning, with known computational  and statistical  barriers under the general model of partially observable Markov decision processes (POMDPs). The curse of partial observability becomes severer when _multiple_ RL agents interact, where not only the environment state, but also other agents' information, are not fully-observable in decision-making .

On the other hand, a flurry of empirical paradigms has made partially observable (multi-agent) RL promising in practice. One notable example is to exploit the _privileged information_ that may be available (only) during training. The privileged information usually includes direct access to the underlying states, as well as access to other agents' observations/actions in multi-agent RL (MARL), due to the use of simulators and/or high-precision sensors for training. The latter is also known asthe _centralized-training-with-decentralized-execution_ (CTDE) framework in deep MARL, and has become prevalent in practice [53; 70; 22; 82]. These approaches can be mainly categorized into two types: i) privileged _policy_ learning, where an expert/teacher policy is trained with privileged information, and then _distilled_ into a student partially observable policy. This _expert distillation_, also known as _teacher-student learning_, approach has been the key to some empirical successes in robotic locomotion [45; 59] and autonomous driving ; ii) privileged _value_ learning, where a value function is trained conditioned on privileged information, and used to improve a partially observable policy. It is typically instantiated as the _asymmetric actor-critic_ algorithm , and serves as the backbone of some high-profiled successes in robotic manipulation [46; 3] and MARL [53; 82].

Despite the remarkable empirical successes, theoretical understandings of partially observable RL with privileged information have been rather limited, except for a few recent prominent advances in RL with _hindsight observability_[44; 30] (see Appendix B for a detailed discussion). However, most of these theoretically sound algorithms are different from those used in practice, and require computationally intractable oracles to achieve provable sample efficiency. The soundness and efficiency of the aforementioned paradigms used in practice remain elusive. In this work, we examine both paradigms of expert distillation and asymmetric actor-critic, with foresight privileged information as in these empirical works. In contrast to [44; 30], which purely focused on sample efficiency, we aim to understand the benefits of privileged information by examining these practically inspired paradigms under several POMDP models, without computationally intractable oracles. We defer a detailed literature review to Appendix B, and summarize our contribution as follows.

Contributions.We first formalize the empirical paradigm of _expert distillation_, and demonstrate its pitfall in distilling near-optimal policies even in observable POMDPs, a model class that was recently shown to allow provable partially observable RL without computationally intractable oracles . We then identify a new condition for POMDPs, the _deterministic filter_ condition, and establish sample and computational complexities that are _both_ polynomial for expert distillation. The new condition is weaker and thus encompasses several known (statistically) tractable POMDP models (see Figure 1 for a summary). Further, we revisit the _asymmetric actor-critic_ paradigm and analyze its efficiency under the more challenging setting of observable POMDPs above (where expert distillation fails). Identifying the inefficiency of vanilla asymmetric actor-critic, and inspired by the empirical success in _belief-state-learning_, we develop a new _belief-weighted_ version of asymmetric actor-critic, with polynomial-sample and quasi-polynomial-time complexities. Key to the results is a new belief-state learning oracle that preserves _filter stability_ under a misspecified model, which may be of independent interest. Finally, we also investigate the provable efficiency of partially observable multi-agent RL with privileged information, by studying algorithms under the CTDE framework, with polynomial-sample and (quasi-)polynomial-time complexities in both paradigms above.

## 2 Preliminaries

### Partially Observable RL (with Privileged Information)

Model.Consider a POMDP characterized by a tuple \(=(H,,,,,,_{ 1},r)\), where \(H\) denotes the length of each episode, \(\) is the state space with \(||=S\), \(\) denotes the action space with \(||=A\). We use \(=\{_{h}\}_{h[H]}\) to denote the collection of transition matrices, so that \(_{h}( s,a)()\) gives the probability of the next state if action \(a\) is taken at state \(s\) and step \(h\). In the following discussions, for any given \(a\), we treat \(_{h}(a)^{||||}\) as a matrix, where each row gives the probability for reaching each next state from different current states. We use \(_{1}\) to denote the distribution of the initial state \(s_{1}\), and \(\) to denote the observation space with \(||=O\). We use \(=\{_{h}\}_{h[H]}\) to denote the collection of emission matrices, so that \(_{h}( s)()\) gives the emission distribution over the observation space \(\) at state \(s\) and step \(h\). For notational convenience, we will at times adopt the matrix convention, where \(_{h}\) is a matrix with rows \(_{h}( s)\) for each \(s\). Finally, \(r=\{r_{h}\}_{h[H]}\) is a collection of reward functions, so that \(r_{h}(s,a)\) is the reward given the state \(s\) and action \(a\) at step \(h\). When privileged information is available, the agent can observe the underlying state \(s\) directly _during training_ (only). We thus denote the trajectory until step \(h\)_with states_ as \(_{h}=(s_{1:h},o_{1:h},a_{1:h-1})\), the one _without states_ as \(_{h}=(o_{1:h},a_{1:h-1})\), and its space as \(_{h}\). Finally, we use \(_{h}(_{h})\) to denote the posterior distribution over the underlying state at step \(h\) given history \(_{h}\), which is known as the _belief state_ (c.f. Appendix C.1 for more details).

Policy and value function.We define a stochastic policy at step \(h\) as:

\[_{h}:^{h}^{h-1}(),\] (2.1)where the agent bases on the entire (partially observable) history for decision-making. The corresponding policy class is denoted as \(_{h}\). We further denote \(=_{h[H]}_{h}\). We also define \(^{}:=\{_{1:H}\,|\,_{h}:^{h}^{h} ^{h-1}()h[H]\}\) to be the most general policy space in partially observable RL with privileged state information, which can potentially depend on all historical states, observations, and actions. It can be seen that \(^{}\). We may also use policies that only receive a _finite memory_ instead of the whole history as inputs: fix an integer \(L>0\), we define the policy space \(^{L}\) to be the space of all possible policies \(=_{1:H}:=(_{h})_{h[H]}\) such that \(_{h}:_{h}()\) with \(_{h}:=^{\{L,h\}}^{\{L,h\}}\) for each \(h[H]\). Finally, we define the space of state-based policies as \(_{}\), i.e., for any \(=_{1:H}_{}\), \(_{h}:()\) for all \(h[H]\).

Given the POMDP model \(\), we write \(_{s_{1:H+1,a_{1:H}},o_{1:H}}^{}()\) to denote the event \(\) when \((s_{1:H+1},a_{1:H},o_{1:H})\) is drawn as a trajectory following the policy \(\) in the model \(\). We will also use the shorthand notation \(^{,}()\) if \((s_{1:H+1},a_{1:H},o_{1:H})\) is evident. We write \(_{}^{}[]\) to denote the expectation similarly. We define the value function at step \(h\) as \(V_{h}^{,}(y_{h}):=_{}^{}[_{h=h}^{H}r _{t}(s_{t},a_{t})\,|\,y_{h}|]\), denoting the expected accumulated rewards from step \(h\), where \(y_{h}(s_{1:h},o_{1:h},a_{1:h-1})\), and we slightly abuse the notation by treating as a set the sequence of states \(s_{1:h}\), the sequence of observations \(o_{1:h}\), and the sequence of actions \(a_{1:h-1}\) up to time \(h\), which are the available information to the agent at step \(h\). We say \(y_{h}\) is _reachable_ if there exists some policy \(^{}\) such that \(^{,}(y_{h})>0\). For \(h=1\), we adopt the simplified notation \(v^{}()=_{}^{}[_{h=1}^{H}r_{h}(s_{h}, a_{h})]\). Meanwhile, we also define \(Q_{h}^{,}(y_{h},a_{h}):=_{}^{}[_{t=h }^{H}r_{t}(s_{t},a_{t})\,|\,y_{h},a_{h}]\). We denote the occupancy measure on the state space as \(d_{h}^{,}(s_{h})=^{,}(s_{h})\). The goal of learning in POMDPs is to find the optimal policy that _maximizes_ the expected accumulated reward over the policies that take \(_{h}\) as input at each step \(h[H]\), i.e., those \(\). Formally, we define:

**Definition 2.1** (\(\)-optimal policy).: Given \(>0\), a policy \(^{}\) is \(\)-optimal, if \(v^{}(^{})_{}v^{}()-\).

Learning with privileged information.Common RL algorithms for POMDPs deal with the scenario where during _both_ the training and test time, the agent can only observe its historical observations and actions \(_{h}\) at step \(h\), while the states are not accessible. In other words, the agent can only utilize policies from \(\) to interact with the environment. In contrast, in settings with _privileged information_, e.g., training in simulators and/or using sensors with higher precision, the underlying state can be used in training. Thus, the agent is allowed to utilize policies from the class \(^{}\) during training. Meanwhile, the objective is still to find the optimal history-dependent policy in the space of \(\), since at test time, the agent cannot access the state information anymore, and it is the performance for such policies that matters eventually. For simplicity, we assume the reward function is known since under our privileged information setting, learning the reward function is much easier than learning the transition and emission, and the sample/computational complexity for the former is dominated by that for the latter. This assumption has also been made for learning in POMDPs without privileged information [36; 47; 48].

### Partially Observable Multi-agent RL with Information Sharing

Partially observable stochastic games (POSGs) are a natural generalization of POMDPs with multiple agents of potentially independent interests. We define a POSG with \(n\) agents by a tuple \(=(H,,\{_{i}\}_{i=1}^{n},\{_{i}\}_{ i=1}^{n},,,_{1},\{r_{i}\}_{i=1}^{n})\), where each agent \(i\) has its individual action space \(_{i}\), observation space \(_{i}\), and reward function \(r_{i}=\{r_{i,h}\}_{h[H]}\) with \(r_{i,h}(s,a)\) denoting the reward given state \(s\) and joint action \(a\) for agent \(i\) at step \(h\). An episode of POSG proceeds as follows: at each step \(h\) and state \(s_{h}\), a joint observation is drawn from \((o_{i,h})_{i[n]}_{h}(\,|\,s_{h})\), and each agent receives its own observation \(o_{i,h}\), takes the corresponding action \(a_{i,h}\), obtains the reward \(r_{i,h}(s_{h},a_{h})\), where \(a_{h}:=(a_{i,h})_{i[n]}\), and then the system transitions to the next state as \(s_{h+1}_{h}(\,|\,s_{h},a_{h})\). Notably, each agent \(i\) may not only know its local information \((o_{i,1:h},a_{i,1:h-1})\), but also information from some other agents. Therefore, we denote the information available to each agent \(i\) at step \(h\) also as \(_{i,h}(o_{1:h},a_{1:h-1})\) and define the _common information_ as \(c_{h}=_{i[n]}_{i,h}\) and _private information_ as \(p_{i,h}=_{i,h} c_{h}\). We denote the space for common information and private information as \(_{h}\) and \(_{i,h}\) for each agent \(i\) and step \(h\). The joint private information at step \(h\) is denoted as \(p_{h}=(p_{i,h})_{i[n]}\), where the collection of the joint private information is given by \(_{h}=_{1,h}_{n,h}\). We refer more examples of this setting of POSG with information-sharing to Appendix C.2 (and also [62; 63; 51]). Correspondingly, the policy each agent \(i\) deploys at test time takes the form of \(_{i,h}:_{h}_{h}_{i,h} (_{i})\), where \(_{h}\) is the space of random seeds. We denote the policy space for agent \(i\) as \(_{i}\). If \(_{i,h}\) takes the state \(s_{h}\) instead of \((c_{h},p_{i,h})\) as input, we denote its policy space as \(_{,i}\), e.g., for each agent \(i\), and policy \(_{1:H}_{,i}\), we have \(_{i,h}:(_{i})\) for each step \(h[H]\). Similar to the POMDP setting, we define \(^{}\) to be the most general policy space, i.e., \(^{}:=\{_{1:H}\,|\,_{h}:^{h}^{h }^{h-1}()h[H]\}\). Note that this model covers several recent POSG models studied for partially observable MARL, e.g., [49; 27]. For example, at each step \(h\), if there is no shared information, then \(c_{h}=\), and if all history information is shared, then \(p_{i,h}=\) for all \(i[n]\). In _privileged-information_-based learning, the training algorithm may exploit not only the underlying state information, but also the observations and actions of other agents.

Solution concepts.The solution concepts for POSGs are usually the _equilibria_, particularly Nash equilibrium (NE) for two-player zero-sum games (i.e., when \(n=2\) and \(r_{1,h}+r_{2,h}=1\)),1 and correlated equilibrium (CE) or coarse correlated equilibrium (CCE) for general-sum games. We defer the formal definitions of these standard solution concepts to Appendix C.2.

### Technical Assumptions for Computational Tractability

A key technical assumption is that the POMDPs/POSGs we consider satisfy an _observability_ assumption, as outlined below. This observability assumption allows us to use short memory policies to approximate the optimal policy, and yields quasi-polynomial-time complexity for both planning and learning in POMDPs/POSGs [26; 25; 51]. Meanwhile, we defer an additional assumption to ensure the traceability for solving POSGs to Appendix C.3.

**Assumption 2.2** (\(\)-observability [20; 26; 25]).: Let \(>0\). For \(h[H]\), we say that the matrix \(_{h}\) satisfies the \(\)-observability assumption if for each \(h[H]\), for any \(b,b^{}()\), \(\|_{h}^{}b-_{h}^{}b^{}\|_{1} \|b-b^{}\|_{1}\). A POMDP/POSG satisfies \(\)-observability if all its \(_{h}\) for \(h[H]\) do so.

## 3 Revisiting Empirical Paradigms of RL with Privileged Information

Most empirical paradigms of RL with privileged information can be categorized into two types: i) privileged _policy_ learning, where the policy in training is conditioned on the privileged information, and the trained policy is then _distilled_ to a policy that does not take the privileged information as input. This is usually referred to as either _expert distillation_[14; 64; 58] or _teacher-student learning_[45; 59; 75] in the literature; ii) privileged _value_ learning, where the value function is conditioned on the privileged information, and is then used to directly output a policy that takes partial observation (history) as input. One prominent example of ii) is _asymmetric-actor-critic_[68; 3]. It is worth noting that asymmetric-actor-critic is also closely related to one of the most successful paradigms for multi-agent RL, _centralized-training-with-decentralized-execution_[53; 86; 21], which is usually instantiated under the actor-critic framework, with the critic taking privileged information as input

   & Without PI & With PI (Ours) \\   & With STD &   } \\  & Oracle-efficient & \\  & & \\  & Without additional assumption : & Poly sample \\  & Computationally harder than SL,  & + time \\  \(\)-decidable & Expectation-in  & & \\ POMDP & sample + time  & FA : \\  & Without WSE : & Poly sample + \\  & Statistically hard  & Classification \\  & With WSE : & (SL), oracle \\  & Poly sample + time  & \\  POSG with & & \\  & dt. filter & N/A & Poly sample + time \\  Observative & Quasi-poly & Poly sample + \\  & simple + time & Quasi-poly time \\  & POSG &  & \\  

Table 1: Comparison of the theoretical guarantees with and without privileged information. PI: privileged information; STD: structural assumptions on transition dynamics, e.g., deterministic transition or reachability of all states; SL: supervised learning; FA: function approximation; WSE: well-separated emission.

Figure 1: A landscape of POMDP models that partially observable RL with privileged information can/cannot address. The axes denote the “restrictiveness” of the assumptions, on the emission channels and transition dynamics, respectively.

in training. Here we formalize and revisit the potential pitfalls of these two paradigms, and further develop theoretical guarantees under certain additional conditions and/or algorithm variants.

### Privileged Policy Learning: Expert Policy Distillation

The motivation behind expert policy distillation is that learning an optimal _fully observable_ policy in MDPs is a much easier and better-studied problem with many known efficient algorithms. The (expected) distillation objective can be formalized as follows:

\[^{}_{}\ \ _{^{}}^{ }[_{h=1}^{H}D_{f}(_{h}^{}(\,|\,s_{h})\,| \,_{h}(\,|\,_{h}))],\] (3.1)

where \(^{}^{}\) is some given behavior policy to collect exploratory trajectories, \(^{}_{_{S}}v^{}()\) denotes the optimal fully observable policy, and \(D_{f}\) denotes the general \(f\)-divergence to measure the discrepancy between \(^{}\) and \(\).

Such a formulation looks promising since it essentially circumvents the challenging issue of _exploration in partially observable environments_, by directly mimicking an expert policy that can be obtained from any off-the-shelf MDP learning algorithm. However, we point out in the following proposition that even if the POMDP satisfies Assumption 2.2, the distilled policy can still be strictly suboptimal even with infinite data, i.e., by solving the expected objective Equation (3.1) completely. We postpone the proof of Proposition 3.1 to Appendix E.

**Proposition 3.1** (Pitfall of expert policy distillation).: For any \(,(0,1)\), there exists a \(\)-observable POMDP \(^{}\) with \(H=1\), \(S=O=A=2\) such that for any behavior policy \(^{}^{}\) and choice of \(D_{f}\) in Equation (3.1), it holds that \(v^{^{}}(^{})_{}v^{ ^{}}()-\).

The key reason Equation (3.1) fails is that in general, the underlying state can remain highly uncertain even given the full history. Thus, the distilled policy may not be able to mimic the state-based expert policy well at different states \(s_{h}\) if the associated \(_{h}^{}(\,|\,s_{h})\) differs significantly across \(s_{h}\). To see how we may rule out such an issue, notice that if \(=1\) (note that according to Assumption 2.2, we have \(\) is at most \(1\) since \(\|_{h}\|_{} 1\)), implying that the observation can decode the underlying state, the bound in Proposition 3.1 becomes vacuous. Inspired by this, we propose the following condition that incorporates this case of \(=1\), and will be shown to suffice to make expert distillation effective.

**Definition 3.2** (Deterministic filter condition).: We say a POMDP \(\) satisfies the _deterministic filter_ condition if for each \(h 2\), the belief update operator under \(\) satisfies that there exists an _unknown function_\(_{h}:\) such that for any reachable \(s_{h-1}\), \(o_{h}\), \(a_{h-1}\), \(U_{h}(b^{s_{h-1}};a_{h-1},o_{h})=b^{_{h}(s_{h-1},a_{h-1},o_{h})}\), where we define for any \(s\), \(b^{s}()\) and \(b^{s}(s)=1\) is a one-hot vector. In addition, for \(h=1\), there exists a function \(_{1}:\) such that for any reachable \(o_{1}\), \(B_{1}(_{1};o_{1})=b^{_{1}(o_{1})}\), where \(B_{h}(b;o_{h}):=_{s_{h} b}^{}(\,|\,o_{h}) ()\), \(U_{h}(b;a_{h-1},o_{h}):=_{s_{h-1} b}^{}(\,|\,a_ {h-1},o_{h-1})()\) are the belief update operators under the Bayes rule for any \(b()\), for which we defer the formal introduction to Appendix C.1.

Notably, this condition is weaker than and thus covers several known tractable classes of POMDPs with sample and computation efficiency guarantees including Block MDP, deterministic POMDP, \(k\)-decodable POMDP as well as a new setting we have identified and existing literature cannot handle. We refer the formal introduction to Appendix E and Figure 1 for an illustration.

In light of the pitfall in Proposition 3.1, we will analyze _both_ the computational and statistical efficiencies of expert distillation in Section 4, under the condition in Definition 3.2.

### Privileged Value Learning: Asymmetric Actor-Critic

Asymmetric actor-critic  iterates between two procedures as in standard actor-critic algorithms : policy _improvement_ and policy _evaluation_. As the name suggests, its key difference from the standard actor-critic is that the algorithm maintains \(Q\)-value functions (the critic) based on the _state/privileged information_, while the policy receives only the (partially observable) _history_ as input.

Policy evaluation.At iteration \(t-1\), given the policy \(^{t-1}\), the algorithm estimates \(Q\)-functions in the form of \(\{Q_{h}^{t-1}(_{h},s_{h},a_{h})\}_{h[H]}\), where we adopt the "unbiased" version  such that functions are conditioned on _both_ the _history_ and the _states_. 2 One key to achieving sample efficiency is by adding some bonus terms in policy evaluation to encourage exploration, i.e., obtaining some _optimistic_\(\)-function estimates, similarly as in the fully-observable MDP setting, see e.g., , for which we defer the detailed introduction to Section 4.

Policy improvementAt each iteration \(t\), given the critic \(\{Q_{h}^{t-1}(_{h},s_{h},a_{h})\}_{h[H]}\) for \(^{t-1}\), the vanilla asymmetric actor-critic algorithm updates the policy according to the _sample-based_ gradient estimation via \(K\) trajectories \(\{s_{1:H+1}^{k},o_{1:H}^{k},a_{1:H}^{k}\}_{k[K]}\) sampled from \(^{t-1}\)

\[^{t}_{}(^{t-1}+}{K} _{k[K]}_{h[H]}_{}_{h}^{t-1}(a_{h}^{k}\,|\,_{ h}^{k})Q_{h}^{t-1}(_{h}^{k},s_{h}^{k},a_{h}^{k})),\] (3.2)

where \(_{t}\) is the step-size and \(_{}\) is the projection operator onto the space of \(\), which corresponds to projecting onto the simplex of \(()\) for each \(h[H]\). Here we point out the potential drawback of the vanilla algorithm as in , where the key insight is that for each iteration of policy evaluation and improvement, one roughly _only_ performs the computation of order \((KH)\), while needing to collect \(K\) new episodes of samples. Thus, the _sample complexity_ will scale in the same order as the _computational complexity_ when the algorithm converges after some iterations to an \(\)-optimal solution, which will be super-polynomial even for \(\)-observable POMDPs . Proof of the result is deferred to Appendix E.

**Proposition 3.3** (Inefficiency of vanilla asymmetric actor-critic).: Under the tabular parameterization for both the policy and the value function, the vanilla asymmetric actor-critic algorithm (Equation (3.2)) suffers from super-polynomial sample complexity for \(\)-observable POMDPs under standard hardness assumptions.

To address such an issue, one may need to perform _more_ computation _per iteration_, so that although the _total_ computational complexity (iteration number \(\) per-iteration computational complexity) is super-polynomial, the total iteration number can be lower such that the total sample complexity may be lower as well. This desideratum is hard to achieve if one _computes_ policy update only on the _sampled_ trajectories \(_{h}\) per iteration, i.e., update _asynchronously_, since this will couple the scales of computational and sample complexities similarly as Equation (3.2). In contrast, we first propose to update _all_ trajectories per iteration in a _synchronous_ way, with the following proximal-policy optimization-type  policy improvement update with the state-history-dependent \(Q\)-functions \(\{Q_{h}^{t-1}(_{h},s_{h},a_{h})\}_{h[H]}\):

\[_{h}^{t}(\,|\,_{h})_{h}^{t-1}(\,|\,_{h}) (_{s_{h}_{h}(_{h})}[Q_{h}^{t-1}(_{h},s_{h},)]), h[H],_{h}_{h},\] (3.3)

where we recall \(_{h}(_{h})()\) denotes the belief state and \(>0\) is the learning rate. This update rule also reduces to the natural policy gradient (NPG)  update under the softmax policy parameterization in the fully-observable case , when updated for each state \(s_{h}\) separately . We defer the detailed derivation of Equation (3.3) to Appendix E.

However, such an update presents two challenges: (1) It requires enumerating all possible \(_{h}\), whose number scales exponentially with the horizon, making it still computationally intractable; (2) An explicit belief function \(_{h}\) is needed. Motivated by these two challenges, we propose to consider _finite-memory_-based policy and assume access to an approximate belief function \(\{_{h}^{}}:_{h}()\}_{h [H]}\) (the learning for which will be made clear later). Correspondingly, the policy update is modified as:

\[_{h}^{t}(\,|\,z_{h})_{h}^{t-1}(\,|\,z_{h})( _{s_{h}_{h}^{}}(z_{h})}[Q_{h}^{t- 1}(z_{h},s_{h},)]), h[H],z_{h}_{h}.\]

Then we develop and analyze one possible approach to learning such an approximate belief efficiently (c.f. Section 5). It is worth noting that the policy optimization algorithm we aim to develop and analyze does not depend on the specific algorithm approximate belief learning. Such a decoupling enables a more modular algorithm design framework, and can potentially incorporate the rich literature on learning approximate beliefs in practice, see e.g., , which has mostly not been theoretically analyzed before. We will thus analyze such an oracle in Section 5.

Provably Efficient Expert Policy Distillation

We now focus on the provable correctness and efficiency of expert policy distillation, under the deterministic filter condition in Definition 3.2. We will defer all the proofs in this section to Appendix F. Definition 3.2 motivates us to consider only succinct policies that incorporate an auxiliary parameter representing the most recent state, as well as the most recent observations and actions. We consider policies that are the composition of two functions: at step \(h\) a function \(g_{h}:\) that decodes the state based on the previous state, the most recent action, and the most recent observation, and a policy \(^{E}_{}\) that takes as input the current (decoded) underlying state and outputs a distribution over actions.

**Definition 4.1**.: We define a policy class \(^{D}\) as:

\[^{D}=\{^{E}_{h}(g_{h}(s_{h-1},a_{h-1},o_{h})):g_{h}: ,^{E}_{h}: ()\}_{h[H]},\]

where \(^{E}\) stands for an arbitrary expert policy, and \(^{D}\) stands for the distilled policy class, and for \(h=1\), \(a_{0}\), \(s_{0}\) are some fixed dummy action and state. Intuitively, the distilled policy \(^{D}\) executes as follows: it firstly _decodes_ the underlying states by applying \(\{g_{h}\}_{h[H]}\)_recursively_ along the history, and then takes actions using \(^{E}\) based on the decoded states.

Our goal is to learn the two functions independently, that is, we want to learn an approximately optimal policy \(^{E}_{}\) with respect to the MDP \(\) derived from POMDP \(\) by omitting the observations and observing the underlying state (see Definition 4.2 for a formal definition), and for each step \(h[H]\), a decoding function \(g_{h}(s_{h-1},a_{h-1},o_{h})\) such that the probability of _incorrectly_ decoding a state-action-observation triplet over the trajectories induced by the policy \(^{E}\) is low.

**Definition 4.2** (POMDP-induced MDP).: Given a POMDP \(=(H,,,,,,_ {1},r)\), we define its associated Markov Decision Process (MDP) \(\) as \(=(H,,,,_{1},r)\) without observations.

**Definition 4.3**.: Consider a POMDP \(\) that satisfies Definition 3.2, and let \(=\{_{h}\}_{h[H]}\) be the promised set of functions that always correctly decode a state-action-observation triplet into an underlying state. Consider policy \(}=\{^{E}_{h}(()): ()\}_{h[H]}^{D}\). We slightly abuse the notation and simply denote by \(v^{}(^{E})=v^{}(})\).

**Lemma 4.4**.: Let \(=(H,,,,,,_ {1},r)\) be a POMDP that satisfies Definition 3.2, and consider a policy \(^{E}_{}\). Consider a set of decoding functions \(\{g_{h}\}_{h[H]}\) such that, \(^{^{E},}\)\([ h[H]:g_{h}(s_{h-1},a_{h-1},o_{h}) s_{h}]\). Consider the policy \(=\{^{E}_{h}(g_{h}()): ()\}_{h[H]}\) on the POMDP \(\), then: \(v^{}() v^{}(})-H\).

We can use any off-the-shelf algorithm to learn an approximate optimal policy \(^{E}\) for the associated MDP \(\) (see Definition 4.2). Thus, in the rest of the section, we focus on learning the decoding function \(\{g_{h}\}_{h[H]}\). To efficiently learn the decoding function, we model the access to the underlying state by keeping track of the most recent pair of the action and the observation, as well as the two most recent states. We summarize the algorithm of decoding-function learning in Algorithm 1.

**Theorem 4.5**.: Consider a POMDP \(\) that satisfies Definition 3.2, a policy \(^{E}_{}\), and let \(\{g_{h}\}_{h[H]}\) be the output of Algorithm 1 with \(M=}\). Then, with probability at least \(1-\), for each step \(h[H]\): \(^{^{E},}\)\([ h[H]:g_{h}(s_{h-1},a_{h-1},o_{h}) s_{h}]\), using \((H,A,O,S,,())\) episodes in time \((H,A,O,S,,())\).

The following is an immediate consequence of Lemma 4.4 and Theorem 4.5. Note that _both_ the sample and computation complexities are _polynomial_, which is in stark contrast to the \(k\)-decodable POMDP case  (a special one covered by our Definition 3.2), for which the sample complexity is necessarily _exponential in \(k\)_ when there is no privileged information . In fact, thanks to privileged information, the complexities are only polynomial in horizon \(H\) even when the decodable length is _unknown_. For the benefits of using privileged information in several other subclasses of problems, we refer to Table 1 for more details.

**Theorem 4.6**.: Let \(\) satisfy Definition 3.2 and consider any policy \(^{E}_{}\). Using \((H,A,O,S,,())\) episodes and in time \((H,A,O,S,,())\), we can compute a policy \(^{D}\) (see Definition 4.1) such that with probability at least \(1-\), \(v^{}() v^{}(^{E})-\).

Extension to the case with general function approximation.Due to the modularity of our algorithmic framework and its compatibility with supervised learning oracles, it can be readily generalized to the function approximation setting to handle large observation spaces. We defer the corresponding results to Appendix G.

Provable Asymmetric Actor-Critic with Approximate Belief Learning

Unlike most existing theoretical studies on provably sample-efficient partially observable RL [36; 25; 47], which directly learn an approximate _POMDP model_ for planning near-optimal policies, we consider a general framework with two steps: firstly learning an approximate _belief function_, followed by adopting a _fully observable RL_ subroutine on the belief state space.

### Belief-Weighted Optimistic Asymmetric Actor-Critic

We now introduce our main algorithmic contribution to the privileged policy learning setting. Our algorithm is conceptually similar to the natural policy gradient methods [40; 1; 74] in the fully-observable setting, with additional weighting over the states \(s_{h}\) using some learned belief states, to handle the additional _state_-dependence in the asymmetric critic. The overall algorithm is presented in Algorithm 2. The algorithm requires a belief-learning subroutine that takes the stored memory as input and outputs a belief about the underlying state (c.f. \(\{_{h}^{}\}_{h[H]}\)). Additionally, similar to the fully observable setting, we include a subroutine to estimate the \(Q\)-function, which introduces additional challenges due to partial observability (see Appendix H). We establish the performance guarantee of Algorithm 2 in the following theorem. We defer the proof to Appendix H.

**Theorem 5.1** (Near-optimal policy).: Fix \(,(0,1)\). Given a POMDP \(\) and an approximate belief \(\{_{h}^{}:_{h}()\}_{h[H]}\), with probability at least \(1-\), Algorithm 2 can learn an approximate optimal policy \(^{}\) of \(\) in the space of \(^{L}\) such that \(v^{}(^{})_{^{L}}v^{}()- (+H^{2}_{})\), with sample complexity \((S,A,O,H,,)\) and time complexity \((S,A,O,H,Z,,)\), where \(_{}\) is the belief-learning error defined as \(_{}:=_{h[H]}_{^{L}}_{} ^{}\|_{h}(_{h})-_{h}^{}(z_{h})\|_{1}\) and \(Z:=_{h[H]}|_{h}|\). Furthermore, if \(\) is additionally \(\)-observable (c.f. Assumption 2.2), then \(^{}\) is also an approximate optimal policy in the space of \(\) such that \(v^{}(^{})_{}v^{}()-(+H^{2}_{})\), as long as \(L(^{-4}(SH/))\).

### Approximate Belief Learning

At a high level, our belief-learning algorithm first learns an approximate POMDP model \(}\) by explicitly exploring the state space. The main technical challenge here is that there may exist states that are reachable with very low probability, making it infeasible to collect enough samples to sufficiently explore them, thus potentially breaking the \(\)-observability property of the ground-truth model \(\). To circumvent this issue, we ignore such hard-to-visit states and _redirect_ probabilities flowing to them to certain other states. Thus, in our truncated POMDP, where each state is sufficiently explored, we can approximate the transition and emission matrices to a desired accuracy _uniformly across all the states_ and preserve the \(\)-observability property. This ensures that the learned approximate belief function in the truncated POMDP is sufficiently close to the actual belief function of the original POMDP \(\). Note that the key to achieving belief learning with _both_ polynomial sample and time complexities is our explicit exploration in the state space, which relies on executing _fully observable_ policies from an _MDP learning_ subroutine. We remark that the belief function may also be learned even if the state space is only explored by partially observable policies, thus utilizing only hindsight observability may be sufficient for this purpose . However, for such exploration to be _computationally tractable_, one requires to avoid using computationally intractable oracles for _POMDP learning_, which is in fact our final goal. We present the guarantees in the next theorem and postpone the proof to Appendix H.

**Theorem 5.2**.: Consider a \(\)-observable POMDP \(\) (c.f. Assumption 2.2) and assume that \(L(^{-4}(SH/))\) for an \(>0\). Then, we can learn an approximate belief \(\{_{h}^{}\}_{h[H]}\) from Algorithm 4 using \(}(AH^{2}O+S^{3}AH^{2}}{e^{2}}+A^{2 }H^{6} Q}{e^{2}})\) episodes in time \((S,H,A,O,,,( {1}{}))\) such that with probability at least \(1-\), for any \(^{L}\) and \(h[H]\), \(_{}^{}\|_{h}(_{h})-_{h}^{}(z_{h })\|_{1}\).

Theorem 5.2 shows that an approximate belief can be learned with both polynomial samples and time, which, combined with Theorem 5.1, yields the final polynomial sample and quasi-polynomial time guarantee below. In contrast to the case without privileged information [25; 27], the sample complexity is reduced from quasi-polynomial to polynomial for \(\)-observable POMDPs. Note that the computational complexity remains quasi-polynomial, which is known to be unimprovable even for planning . The key to such an improvement, as pointed out in Section 3.2, is the more practical update rule of actor-critic (in conjunction with our belief-weighted idea), which allows _more computation_ at each iteration (instead of only performing computation at the _sampled_ finite-memory). This allows the total computation to remain quasi-polynomial, while the overall sample complexity becomes polynomial. A detailed comparison can be found in Table 1.

**Theorem 5.3**.: Let \(\) be a \(\)-observable POMDP (c.f. Assumption 2.2), and consider \(L(^{-4}(SH/))\) for an \(>0\). With probability at least \(1-\), Algorithm 2 can learn a policy \(^{L}\) such that \(v^{}()_{^{}}v^{}(^{} )-\), using \((S,H,1/,1/,(1/),O,A)\) episodes and in time \((S,H,1/,(1/),O^{L},A^{L})\).

## 6 Numerical Validation

We now provide some numerical results for both of our principled algorithms. Here we mainly compare with two baselines, the vanilla asymmetric actor-critic , and asymmetric \(Q\)-learning , on two settings, POMDP under the deterministic filter condition (c.f. Definition 3.2) and general POMDPs. We report the results in Table 2 and Figure 2, where our algorithms converge faster to higher rewards. We defer the implementation details and discussions to Appendix I.

## 7 Extension to Partially Observable MARL with Privileged Information

### Privileged Policy Learning: Equilibrium Distillation

To understand how the deterministic filter condition may be extended for POSGs, we first note the following equivalent characterization of Definition 3.2, the proof of which is deferred to Appendix J.

**Proposition 7.1**.: Definition 3.2 is equivalent to the following: for each \(h[H]\), there exists an _unknown_ function \(_{h}:_{h}\) such that \(^{}(s_{h}=_{h}(_{h})\,|\,_{h})=1\) for any reachable \(_{h}_{h}\).

Proposition 7.1 implies that at each step \(h\), given the _entire_ history information, the agent can uniquely decode the current underlying state \(s_{h}\). Thus, we generalize this condition to POSGs by requiring each agent to uniquely decode the current state \(s_{h}\) given the information it has collected so far.

**Definition 7.2** (Deterministic filter condition for POSGs).: We say a POSG \(\) satisfies the _deterministic filter condition_ if for each \(i[n]\), \(h[H]\), there exists _an unknown_ function \(_{i,h}:_{h}_{i,h}\) such that \(^{}(s_{h}=_{i,h}(c_{h},p_{i,h})\,|\,c_{h},p_{i,h})=1\) for any reachable \((c_{h},p_{i,h})\).

Here we have required that each agent can decode the underlying state through their own information _individually_. Naturally, one may wonder whether one can relax it so that only the _joint_ history information of all the agents can decode the underlying state. However, we point out in the following that it does not circumvent the computational hardness of POSG, the proof of which is deferred to Appendix J. Note that the computational hardness result can not be mitigated even with privileged state information, as the hardness we state here holds even for the planning problem with model knowledge, with which one can simulate the RL problems with privileged information.

**Proposition 7.3**.: Computing CCE in POSGs that satisfy that for each step \(h[H]\), there exists a function \(_{h}:_{h}_{h}\) such that \(^{}(s_{h}=_{h}(c_{h},p_{h})\,|\,c_{h},p_{h})=1\) for any reachable \((c_{h},p_{h})\) is still PSPACE-hard.

Learning multi-agent individual decoding functions with unilateral exploration.Similar to our framework for POMDPs, our framework for POSGs is also decoupled into two steps: i) learning an _expert_ equilibrium policy that is fully observable, ii) learning the _decoding function_, where the first step can be instantiated by any provable off-the-shelf algorithm of learning in Markov games. The major difference from the framework for POMDPs lies in how to learn the decoding function. In Theorem J.1, we prove that the difference of the NE/CE/CCE-gap between the expert policy and the distilled student policy is bounded by the decoding errors under policies from the _unilateral deviation_ of the expert policy. Hence, given the expert policy \(\), the key algorithmic principle is to perform _unilateral exploration_ for each agent \(i\) to make sure the decoding function is accurate under policies

Figure 2: Results for POMDPs of different sizes, where our methods achieve the best performance with the lowest sample complexity (VI: value iteration; AAC: asymmetric actor-critic).

\((^{}_{i},_{-i})\) for any \(^{}_{i}\), keeping \(_{-i}\) fixed. We refer the detailed algorithm to Algorithm 5, and present below the guarantees for learning the decoding functions and the corresponding distilled policy for learning NE/CCE, while we defer the results for learning CE to Theorem J.6.

**Theorem 7.4** (Equilibria learning; Combining Theorem J.1 and Theorem J.4).: Under Assumption C.8 and conditions of Definition 7.2, given a \(\)-NE/CCE \(^{E}\) for the associated Markov game of \(\), Algorithm 5 can learn decoding function \(\{_{i,h}\}_{i[n],h[H]}\) such that with probability at least \(1-\), it is guaranteed that \(_{u_{i},j[n]}^{u_{i}_{-i},}(s_{h} _{j,h}(c_{h},p_{j,h}))},\) for any \(i[n],h[H]\) with both sample and computational complexities \((S,A,H,O,,)\). Consequently, policy \(\) distilled from \(^{E}\) (c.f. Theorem J.1 for the formal distillation procedures) is an \(\)-NE/CCE of \(\).

### Privileged Value Learning: Asymmetric MARL with Approximate Belief Learning

For POMDPs, we have used _finite-memory_ policies for computational efficiency. We generalize to POSGs with information sharing by defining the _compression_ of the common information.

**Definition 7.5** (Compressed approximate common information ).: For each \(h[H]\), given a set \(}_{h}\), we say \(_{h}\) is a compression function if \(_{h}\{f:_{h}}_{h}\}\). For each \(c_{h}_{h}\), we denote \(_{h}:=_{h}(c_{h})\). We also require the compression function to satisfy the regularity condition that for each \(h[H]\), there exists a function \(_{h+1}\) such that \(_{h+1}=_{h+1}(_{h},_{h+1})\), for any \(c_{h}_{h}\), \(_{h+1}_{h+1}\), where we recall \(c_{h+1}:=c_{h}_{h+1}\) and the definition of \(_{h+1}\) in Assumption C.7.

Similar to the framework we developed for POMDPs in Section 5, we firstly develop the multi-agent RL algorithm based on some approximate belief, and then instantiate it with one provable approach for learning such an approximate belief.

Optimistic value iteration of POSGs with approximate belief.For POMDPs, the sufficient statistics for optimal decision-making is the posterior distribution over the state given history. However, for POSGs with information-sharing, as shown in , the sufficient statistics become the posterior distribution over the state _and the private information_ given the common information, instead of only the state. Therefore, we consider the approximate belief in the form of \(_{h}:}_{h}(_{h} )\) for each \(h[H]\), where we define the error compared with the ground-truth belief to be \(_{}:=_{h[H]}_{}_{}^{ }_{s_{h},p_{h}}|^{}(s_{h},p_{h}\,|\,c_{h} )-_{h}(s_{h},p_{h}\,|\,_{h})|\), i.e., the _expected_ total variation distance from the true one. Note that both \(_{h}\) and thus \(_{}\) have implicit dependencies on \(_{h}\), as \(_{h}:=_{h}(c_{h})\). We outline our algorithm in Algorithm 7, which is conceptually similar to the algorithm for POMDPs (Algorithm 2), maintaining the asymmetric critic (i.e., value function), and performing the actor update (i.e., policy update) using the _belief-weighted_ value function.

**Theorem 7.6** (Equilibria learning; Combining Theorem J.15 and Theorem J.16).: Fix \(,(0,1)\). Under Assumption C.8, with probability at least \(1-\), Algorithm 7 can learn an \((+H^{2}_{})\)-NE if \(\) is zero-sum and \((+H^{2}_{})\)-CEE if \(\) is general-sum with sample complexity \((SAO(SAHO/)}{^{2}})\) and computational complexity \((S,(AO)^{(^{-4}(SH/))},H,,)\).

Learning approximate belief with model truncation.The belief learning algorithm we design for POSGs is conceptually similar to that we designed for POMDPs, where the key to achieving _both_ polynomial sample and computational complexity is still to firstly learn approximate models, i.e., transitions and emissions, and then carefully _truncate_ (as in Section 5.2) its transition and emission to build the approximate belief, where we defer the detailed algorithm to Algorithm 8. Next, we provide its provable guarantees, which leads to a final polynomial-sample and quasi-polynomial-time complexity result when combined with Theorem 7.6.

**Theorem 7.7**.: For any \(>0\), under Assumption 2.2, it holds that one can learn the approximate belief \(\{_{h}:}_{h}(_{h})\}_{h[H]}\) such that \(_{}}\) with both polynomial sample complexity and computational complexity \((S,A,O,H,,,)\) for all the examples in Appendix C.3. As a consequence, Algorithm 7 can learn an \(\)-NE if \(\) is zero-sum and \(\)-CEE/CCE if \(\) is general-sum with sample complexity \((SAO(SAHO/)}{^{2}})\) and computational complexity \((S,(AO)^{(^{-4}(SH/))},H,,)\).