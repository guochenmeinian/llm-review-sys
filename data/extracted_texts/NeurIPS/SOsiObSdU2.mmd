# TODO: Full in the code here

[MISSING_PAGE_FAIL:1]

**generalization:** the ability to generalize to state-action distributions beyond those observed during training. This challenge is often compounded by the scarcity of observational data available to accurately learn dynamics, highlighting the importance of **[P2] sample-efficient learning**. Additionally, the model should be **[P3] evolvable:** capable of efficiently adapting (i.e. with minimal retraining) to changes in the _underlying system dynamics_. This is particularly crucial in healthcare domains, such as epidemiological modeling and treatment planning, where DTs are regularly updated to reflect fundamental changes in disease transmission patterns (caused by viral mutations, vaccination coverage) or evolving drug resistance mechanisms, often with minimal additional data of emergent dynamics [6; 7].

Existing approaches for creating DTs primarily utilize two approaches: _mechanistic_ models or ML-based _neural_ models. Mechanistic models, denoted as \(f_{}\), are closed-form equations grounded in domain knowledge such as biological or physical principles. They offer high accuracy and generalization given _sufficient_ domain understanding but are limited in their ability to model systems where scientific knowledge is incomplete [8; 9]. Of related note, techniques have been introduced to discover governing equations directly from data, but face challenges in scaling to more complex problem settings [10; 11]. Conversely, neural approaches, \(f_{}\), leverage neural networks (NN) to learn DTs directly from data, often requiring minimal knowledge [12; 13; 14; 15]. Such models are effective given _sufficient_ training data that provides adequate coverage of state-action distributions, but struggle in data-scarce settings and are difficult to evolve to reflect changing conditions due to their overparameterized, monolithic nature.

**Key considerations.** Informed by this context, _Hybrid Digital Twins_ (**HDTwins**) combine the strengths of both approaches through compositions of neural and mechanistic components, i.e. \(f=f_{} f_{}\). Here, \(f_{}\) symbolically incorporates domain-grounded priors, improving generalization and regularization while simplifying the complexity of patterns that have to be learned by the neural component. In other terms, \(f_{}\) complements the mechanistic component by modeling complex temporal patterns in regions where the mechanistic model might be oversimplified or incomplete. Consequently, HDTwins can more accurately and robustly capture system dynamics, particularly in settings with (_limited_) _empirical data and (partial) domain knowledge_.

Conceptually, hybrid modeling involves two stages: model _specification_, determining the model structure (e.g. neural architecture, symbolic equations), and model _parameterization_, estimating model parameters (e.g. neural weights, coefficients). This process, with model specification in particular, has traditionally relied heavily on human expertise to craft problem-specific models [16; 17; 18; 19]. In this work, we investigate the feasibility of _automatically_ designing hybrid models with minimal expert involvement, which would significantly enhance the efficiency and scalability of model development. This task is challenging, as it requires searching for optimal specification and corresponding parameters within a vast combinatorial model space [20; 21]. To address this, we introduce **HDTwinGen**, a novel evolutionary framework that autonomously and efficiently designs HDTwins. At a high level, our method represents hybrid model specifications in code and leverages large language models (LLMs) for their domain knowledge, contextual understanding, and learning capabilities to propose symbolically represented models and search the model space [22; 23; 24]. This is coupled with offline optimization tools to empirically estimate model parameters from training data. More specifically, HDTwinGen utilizes two LLM agents: the _modeling agent_, whose task is to generate novel model specifications, and the _evaluation agent_, which analyzes performance and provides targeted recommendations for improvement. Through multiple iterations, HDTwinGen efficiently evolves better performing hybrid models with informed modifications.

**Contributions:**1 _Conceptually_, we present the first work in _automated hybrid model design_, jointly optimizing model specification and parameterization of hybrid digital twins. **2 _Technically_**, we introduce **HDTwinGen**, a novel evolutionary framework employing LLMs and offline optimization tools to propose, evaluate, and iteratively enhance hybrid models. **3 _Empirically_**, we demonstrate that our method learns more accurate DTs, achieving **\(\)** better out-of-distribution generalization, **\(\)** sample-efficient learning, and **\(\)** increased flexibility for modular evolvability.

## 2 Digital Twins of Dynamical Systems

A dynamical system \(:=(,,)\) is a tuple of its \(d_{}\)-dimensional state space \(^{d_{}}\), an (optional) \(d_{}\)-dimensional action space \(^{d_{}}\), and a dynamics model \(\). The state at time\(t_{+}\) is represented as a vector, \(x(t)\) and similarly the action taken is represented as a vector \(u(t)\). The _continuous-time_ dynamics of the system can be described by \(x(t)/t=(x(t),u(t),t)\), where \(:\). We optionally consider the existence of some policy \(: P()\) that acts on the system by mapping a state \(x(t)\) to a distribution over actions \(u(t)\).

**Digital Twins.** Digital twins (DTs) aim to approximate \(:\) using a computational model \(f_{,()}\) learned from data. Here, we use \(\) to denote the specification of the model (e.g. linear) and \(()()\) to indicate the set of parameters specified by \(\). Additionally, \(\), \(\), and \(()\) are the spaces of all possible models, specifications, and parameters, respectively. Next, we outline the key desiderata for a DT:

**[P1] Generalization to unseen state-action distributions.** As DTs are required to simulate varying conditions, they should extrapolate to state-action distributions not observed during training time. Formally, the generalization error \(_{(x(t),u(t),y(t)) p_{OOD}}[(f_{,() }(x(t),u(t)),y(t))]\) should be minimized, where \(\) is some loss function, and \(p_{OOD}\) represents the out-of-distribution scenario.

**[P2] Sample-efficient learning.** Given the often limited availability of real-world data, DTs should learn robustly from minimal empirical data. In other words, they must have good _sample complexity_, achieving the desired level of generalization with a limited number of observations .

**[P3] Evolvability.** Dynamical systems are, by nature, non-stationary and evolve over time [26; 27]. From a modeling perspective, the DT should be easily evolved to reflect changing underlying dynamics, minimizing the need for additional data or expensive model re-development, i.e. \(\) and \(()\) should be easily adjustable to reflect changing system dynamics.

For the purpose of model learning, we assume access to an offline dataset containing \(N^{+}\) trajectories, where the measurements of the systems are made at discrete time points \([T]=[t_{1},t_{2}, T]\). This dataset, \(=\{\{(x^{(n)}(t),u^{(n)}(t),y^{(n)}(t)) t[T])\}_{n=1}^{N}\), contains state-action trajectories sampled regularly over time, where \(y^{(n)}(t)=x^{(n)}(t+ t)\) represents the subsequent state.

## 3 Hybrid Digital Twins

**HD Twin.** A Hybrid Digital Twin is a composition of mechanistic and neural components, represented as \(f_{,()}=f_{} f_{}\)[18; 28]. This class of hybrid models offers several advantages that align with our desiderata. The mechanistic component allows partial knowledge to be encoded through its symbolic form, which, while not sufficient alone to accurately predict underlying dynamics, is complemented by the neural components that learn from available data. This combination aids in generalization (**[P1]**), especially moving beyond conditions observed in training, and improves sample complexity (**[P2]**). Furthermore, the mechanistic component can be quickly and easily updated with new parameters due to its simpler, lower-dimensional structure, allowing the overall model to adapt efficiently to remain accurate in changing conditions (**[P3]**). In this work, we focus on _additive_ compositions, \(f_{,()}=f_{}+f_{}\), as they are more interpretable. Additionally, it enables individual contributions of mechanistic and neural components to be easily disentangled and simplifies the optimization to allow gradient-based methods . Nonetheless, we encourage future works to investigate alternative composition strategies (e.g. branching composition) to develop more advanced HDTwins .

Learning the hybrid model can be decomposed into two steps: _(1) model specification_, or learning the structure, \(\), of the dynamics function that describes how the system evolves over time; and _(2) model parameterization_, which estimates the specific values of parameters \(()()\) for a given specification \(\). For instance, the logistic-growth model specifies a structure for population growth, while parameterization involves estimating the growth rate and carrying capacity.3 More generally, this learning problem can be mathematically formulated as a bilevel optimization problem:

\[^{*}=*{arg\,min}_{}_{}(,^{*}()),^{*}()= *{arg\,min}_{()}_{}( ,())\] (1)

Here, the upper-level problem involves finding the optimal specification \(^{*}\) that minimizes the outer objective \(_{}\), while the lower-level problem involves finding the optimal parameters \(^{*}()\) for a given specification \(\) that minimizes the inner objective function \(_{}\). To be more concrete, the outer objective measures the generalization performance, empirically measured on the validation set \(_{}\), while the inner objective measures the fitting error, as evaluated on the training set \(_{}\).

**Combinatorial search space.** The space of possible specifications \(\) (e.g. different networks, functional forms) is discrete and combinatorially large, while \(()\) represents the continuous space of parameters to be optimized. Selecting the optimal \(,()\) thus involves searching through a vast combinatorial space. Performing this search through traditional means, such as genetic programming  or evolutionary algorithms , is computationally challenging, time-consuming, and often technically infeasible. To the best of our knowledge, our work is the first to address the problem of automatic HD Twin development, where we incorporate LLMs (combined with offline optimization tools) to automatically optimize both the specification and the parameterization of hybrid models.

## 4 HDTwinGen: Automatic Design of HDTwins

Human experts craft models by making strategic design decisions based on their domain knowledge, starting with a sensible initial model specification and performing intelligent modifications based on empirical evaluations. Our key insight is that LLMs can effectively emulate these capabilities to efficiently navigate the search space in Equation (1) and _autonomously design HDTwins_. More specifically, our method utilizes LLMs for three major purposes: \(\)**source of domain knowledge**, where LLMs inject domain-consistent knowledge into the model specification, particularly through the symbolic representation \(f_{}\); \(\)**efficient search**, by making intelligent modifications to the specification to converge more efficiently on the optimal hypothesis; and \(\)**contextual understanding**, enabling the algorithm to incorporate task-specific context and targeted feedback for model improvement [22; 23; 24].

**Overview.** We operationalize this insight through **HDTwinGen**, an evolutionary algorithm that iteratively evolves a population of candidate solutions to automatically search for the best HD Twin. Our approach employs a framework comprising three key elements: _(1)_ human experts provide an initial system description, modeling objectives, and requirements as a structured prompt; _(2)_ a _modeling agent_ proposes new model specifications, optimizes their parameters on a training dataset, and collects validation performance metrics; _(3)_ an _evaluation agent_ assesses the proposed models using both data-driven performance metrics and qualitative evaluations against expert-defined objectives and requirements. The agents communicate using natural language and a custom code format representing the HDTwin model, facilitating autonomous and iterative model enhancement. An overview of our method is presented in Figure 1, with pseudocode in Appendix E.1.

**Initial prompt design.** The optimization process begins with a human expert providing a structured prompt, referred to as the _modeling context_\(^{}\). This modeling context outlines the system description, modeling objectives \(\), and requirements \(\):

Figure 1: **HDTwinGen: evolutionary framework. The process begins with user-provided modeling context \(^{}\) and \(=\{_{},_{}\}\). 1) In iteration \(g\), the _modeling agent_ generates model specification as a Python program \(f_{,}(g)\). 2) Parameters are optimized using the _offline optimization tool_ to yield \(f_{,^{*}()}\). 3) The HDTwin is evaluated based on model loss \(v\) and component-wise loss \(\). Subsequently, the model pool \(^{(g)}\) is updated with top-\(K\) models. 4) The _evaluation agent_ provides targeted feedback for model improvement \(H^{(g)}\) by analyzing models in \(^{(g)}\) using performance metrics requirements outlined in \(^{context}\). This iterative loop repeats for \(G\) iterations.**

1. The **system description** semantically describes the system, including state and action variables, giving the algorithm the contextual understanding necessary for informed model development.
2. The **modeling objective** specifies _quantitative_ performance requirements via a metric \(\).
3. The **modeling requirements**\(\) are _qualitative_ and described in natural language, detailing aspects such as interpretability (e.g. fully mechanistic or hybrid model) and additional scientific knowledge (e.g. a log-linear relationship between variables).

In practice, \(\) can incorporate various requirements, allowing for the design of both purely mechanistic and hybrid models, a flexibility that we demonstrate experimentally. The model is represented in Python, where purely mechanistic specifications are represented in native Python and neural components are represented using PyTorch . Moreover, \(^{}\) includes a skeleton code to guide the synthesis of executable code in a predetermined format. For illustrative purposes, an example of \(^{}\) is provided in Appendix E.4.

**Evolutionary optimization overview.** Given \(^{}\) as input, HD TwinGen performs \(G\) iterations of optimization, where \(G^{+}\). The population of proposed HDTwins at iteration \(g\) is represented as \(^{(g)}\). Each iteration creates a new candidate model based on previously created models in \(^{(g)}\) and feedback. Only the top \(K\) models are retained after each iteration, except when \(g<K\), in which case all generated models are kept, i.e. \(_{g[G]}|^{(g)}|=K\). Each model in \(^{(g)}\) is characterized by a tuple containing its model specification (represented symbolically through code) and validation metrics. After completing \(G\) iterations, the model with the best validation performance in \(^{(G)}\) is selected as the final model.

### Modeling Agent

**Proposing HDTwins.** The goal of the modeling step is to propose novel HDTwins based on previously proposed models and feedback from the evaluation agent. Specifically, on the \(g\)-th iteration, the modeling agent takes as input \(^{(g-1)}\): the set of top-\(K\) previously generated models; \( H^{(g-1)}\): the most recent feedback produced by the evaluation agent (where on the initial step, \(g=1\), both are empty, i.e., \(H^{(0)}=\), \(^{(0)}=\)); and \(^{}\): the modeling context. The modeling agent generates a model specification \(\) using a predefined code format (i.e. skeleton code). By observing multiple previously best-performing models and their performances, the modeling agent can exploit this context as a rich form of in context-learning and evolve improved specifications in subsequent generations . Each generated specification emits its corresponding parameters, \(()\) are fitted to the training set \(_{}\). More formally, we represent this generative procedure as \(f_{,()}_{}(H,^{(g)}, ^{})\).

**Model specification.** To generate model specifications, the modeling agent decomposes the system into a set of components, with each component describing the dynamics of a specific state variable. In other words, for a system with \(d_{}\) state variables, there will be \(d_{}\) components. Each component is characterized by its own set of inputs and a unique dynamics function that describes the dynamics of its associated state variable over time. This modular representation enables independent analysis and optimization of individual components. In cases where \(\) specifies purely mechanistic equations, the component dynamics are entirely defined using closed-form equations. Conversely, in a hybrid model, the mechanistic equation can be augmented with a neural network (implemented in PyTorch) to model residuals (i.e. in an _additive_ fashion). The choice between mechanistic and hybrid models is left to the user, balancing the trade-off between transparency and predictive performance. Concretely, the specification step involves 'filling in' the skeleton code with a detailed body of code, specifying the decomposition, and delineating each component's dynamics function as a separate code structure (for a generated HDtwin example, see Appendix I).

**Model optimization.** The generated specification emits \(()\), which are treated as placeholder values, and are then optimized against the training dataset. Specifically, we optimize the mean squared error for the parameters that minimize this loss as \(^{*}()=_{()}\,(f_{,( )},_{})\). In this work, we consider \(()\) to be continuous variables, and as such, we optimize \(\) by stochastic gradient descent, using the Adam optimizer . However, we note other optimization algorithms, such as black-box optimizers, could also be used (for more details, see Appendix F, Equation (5)). The parameter optimization step then yields the complete model, \(f_{,^{*}()}\).

**Quantitative evaluation.** For each generated model, we evaluate them quantitatively. Specifically, we collect the validation mean squared error loss per component, which we denote as \(=[_{1},_{2},,_{d_{}}]\) (Appendix F, Equation (6)). We also compute the validation loss of the overall model as well as \(=(f_{,^{*}()},_{})\). Finally, the generated model and its validation losses are included in a tuple and added to the top-\(K\) models \(^{(g)}^{(g-1)}(f_{,^{*}( )},,)\), where \(^{(g)}\) automatically removes the lowest performing models, and also only adds a new model to \(^{(g)}\) if it is unique. We highlight that we consider the top-\(K\) models only to apply _selection pressure_, such that only the best-performing models are considered when generating the next HD Twin .

#### 4.1.1 Evaluation Agent

**Model evaluation.** The goal of the evaluation step is to reflect on the current set of top-\(K\) models, \(^{(g)}\) against requirements \(\) and provide actionable and detailed feedback to the modeling agent for model improvement: \(H^{(g)}_{}(,^{(g)})\). We note that \(H^{(g)}\) is provided in natural language and can be viewed as a dense feedback signal, a notable distinction from traditional learning methods, where feedback often takes the form of simple scalar values, such as loss gradients or rewards. Leveraging natural language feedback allows the agent to _(1)_ engage in comparative analysis, identifying effective specifications in \(^{(g)}\) contributing to higher performance and discerning patterns common in less effective models, informing its suggestions for further model improvement; _(2)_ qualitatively evaluate models against qualitative requirements \(\)--leveraging the LLM's capacity to reason about proposed HDTwins to reflect these requirements via model improvement feedback.

**Enhancing search.** By providing rich feedback to improve model specification, the evaluation and modeling agent collaborate to efficiently evolve high-performing models. Empirically, in Appendix J, we observe that the evaluation agent provides targeted and specific feedback, including component-specific suggestions, proposing alternative decompositions, removing parameters, or introducing non-linear terms. It is noteworthy that the feedback \(H^{(g)}\), expressed flexibly in natural language, could easily be further enriched through direct human feedback. We demonstrate this _human-in-the-loop_ capability by including expert feedback during the optimization process through \(H^{(g)}\) and observed that it was integrated into newly generated HDTwins. Though further investigation is beyond the scope of this work, this demonstration highlights promising avenues for augmenting human-machine collaboration in the autonomous design of DTs.

## 5 Related Works

For an extended related work, refer to Appendix B. Our work focuses on autonomously learning DTs from data, with several relevant research strands:

**Neural sequence models.** ML approaches commonly address learning system dynamics as a sequential modeling problem. In these settings, \(f_{,()}\) are typically black-box models, where \(\) is the NN architecture and \(()\) are its weights. Early models like Hidden Markov Models  and Kalman filters  made simplifying Markovian and linearity assumptions, later extended to nonlinear settings [36; 37]. Subsequent models, including recurrent neural networks , along with their advanced variants [39; 40; 41], introduced the capability to model longer-term dependencies. More recent advancements include attention mechanisms  and Transformer models , significantly improving the handling of long-term dependencies in sequence data. Another line of work, Neural Ordinary Differential Equations (NODE) [14; 44; 45], interprets neural network operations as differential equations. These methods have found utility in modeling a range of complex systems [46; 47; 48; 49]. While deep sequence models are proficient at capturing complex dynamics, they are heavily reliant on training data for generalization (**[P1, P2]**), and their monolithic and overparameterized structures limit evolvability (**[P3]**).

**Mechanistic (discovery) models.** Beyond purely neural approaches, another line of work aims to discover a system's governing equations directly from data. Here \(\) are closed-form equations and \(()\) are their parameters. These include symbolic regression techniques , Eureqa , SINDy , D-CODE [51; 52], among others [52; 53] that search for \(\) and \(()\) from data. These techniques struggle to scale to higher-dimensional settings and rely on experts to perform variable selection and define the function set and primitives available to the search algorithms.

**Hybrid models.** Recent efforts have also created hybrid models by integrating physical laws with neural models. Physics-informed neural networks [15; 54], and methods including Hamiltonian Neural Networks , Lagrangian Neural Networks  integrate structural priors of physical systems to improve generalization. These techniques introduce specialized mechanisms to incorporate _precisely known_ physical principles. Additionally,  integrates prior ODE/PDE knowledge into a hybrid model, using specialized regularization to penalize the neural component's information content. [58; 59] consider settings where an expert equation is known, but equation variables are latent and unobserved. Correspondingly, they employ two sets of latent variables: one governed by expert equations and another linked to neural components.  performs data augmentation by sampling out-of-distribution trajectories from expert models. While existing approaches rely on expert models to perform the hybrid model design, HDTwinGen is an automated approach to jointly optimize hybrid model specification and its parameters.

## 6 Experiments and Evaluation

In this section, we evaluate HDTwinGen and verify that it significantly outperforms state-of-the-art methods in modeling system dynamics over time from an observed dataset and corresponding system description.4

**Benchmark datasets**. We evaluate against **six** real-world complex system datasets; where each dataset is either a real-world dataset or has been sampled from an accurate simulator designed by human experts. Three are derived from a state-of-the-art biomedical Pharmacokine-Pharmacodynamic (PKPD) model of lung cancer tumor growth, used to simulate the combined effects of chemotherapy and radiotherapy in lung cancer  (Equation (2))--this has been extensively used by other works [62; 63; 64]. Here we use this bio-mathematical lung cancer model to create three variations of lung cancer under the effect of no treatments (**Lung Cancer**), chemotherapy only (**Lung Cancer (with Chemo.)**), and chemotherapy combined with radiotherapy (**Lung Cancer (with Chemo. & Radio.)**). We also compare against an accurate and complex COVID-19 epidemic agent-based simulator (**COVID-19**) , which is capable of modeling non-pharmaceutical interventions, such as physical distancing during a lockdown. Furthermore, we compare against an ecological model of a microcosm of algae, flagellate, and rotifier populations (**Plankton Microcosm**)--replicating an experimental three-species prey-predator system . Moreover, we also compare against a real-world dataset of hare and lynx populations (**Hare-Lynx**), replicating predator-prey dynamics . We detail all benchmark datasets details in Appendix C.

**Evaluation Metrics**. We employ mean squared error (MSE) to evaluate the benchmark methods on a held-out test dataset of state-action trajectories, denoted as \(_{}\), using the loss defined in Equation (5) and report this as \(_{MSE}\). Each metric is averaged over ten runs with different random seeds, and we present these averages along with their 95% confidence intervals, further detailed in Appendix G.

**Benchmark methods**. To assess whether HDTwinGen is state-of-the-art, we compare it with the most competitive and popular neural network models, which, when modeling the dynamics of a system over time, becomes a form of ODE model, that is a neural ODE  with action inputs (**DyNODE**) . Moreover, we also compare against a recurrent neural network (**RNN**)  and a state-of-the-art transformer (**Transformer**) . We also compare against mechanistic dynamical equations derived from equation discovery methods for ODEs, including Genetic Programming (**GP**)  and Sparse Identification of Nonlinear Dynamics (**SINDy**) . Lastly, we compare against a hybrid model (**APHYNITY**) that integrates prior knowledge in the form of ODEs into hybrid models, while penalizing the information content from the neural component . Moreover, we compare against the ablations of our method, of the zero-shot generated HDTwin (**ZeroShot**) and this model with subsequently optimized parameters (**ZeroOptim**). We provide method implementation, hyperparameter, and experimental details in Appendix D.

## 7 Main Results

We evaluated all our benchmark methods across all our datasets tabulated in Table 1. HDTwinGen models the system the most accurately, achieving the lowest test prediction mean squared error on the held-out test dataset of state-action trajectories. In the interest of space, we include additional experimental evaluations in the appendix. Specifically, we also evaluate \(\) HDTwinGen performance on a suite of synthetically and procedurally generated benchmarks (Appendix H.9), \(\) comparisonsagainst domain-specific baselines (Appendix H.8) and \(\) various ablation experiments, including ablation of LLM hyperparameters, prompt design, and algorithm settings (Appendices H.5 to H.7).

### Insight Experiments

This section provides an in-depth analysis of HDTwinGen's effectiveness related to its benchmark counterparts. Specifically, we examine the core desiderata for an effective DT described in Section 2: **[P1]** out-of-distribution generalization, **[P2]** sample-efficient learning, and **[P3]** evolvability.

**[P1] Can an HDTwin generalize to out-of-distribution shifts?** To explore out-of-distribution shifts, we adapt the Lung Cancer (with Chemo. & Radio.) to produce a training dataset of states in a range that is outside those observed in the test set over all trajectories (Appendix H.1). We tabulate this in Table 2. Empirically, we find that HDTwinGen is more robust to out-of-distribution shifts than existing methods, benefiting from explicit decomposition and robust hybrid models. Notably, the neural network method DyNODE shows the largest relative error increase from IID to OOD by two orders of magnitude, while the mechanistic method SINDy exhibits a smaller increase by only one order of magnitude. This demonstrates the importance of hybrid models that leverage both neural and mechanistic components to enhance generalization performance under distribution shifts.

**[P2] Can HDTwinGen improve sample-efficiency in model learning?** To explore the low data settings, we re-ran all benchmark methods with fewer samples in their training dataset on the Lung Cancer (with Chemo. & Radio.) dataset. We plot this in Figure 2. Empirically, we observe that HDTwinGen can achieve lower performance errors, especially in lower-sample regimes.

**[P3] Can HDTwinGen evolve its modular HDTwin to fit the system?** We analyze this from an empirical point of view to determine if HDTwinGen can correctly evolve the generated HDTwin and reduce its prediction error over subsequent generations. We observe that HDTwinGen can indeed understand, reason, and iteratively _evolve_ the generated code representation of the HDTwin to incorporate a better fitting HDTwin, as observed in Figure 3. In particular, the annotated results demonstrate that HDTwinGen effectively refines the hybrid model by strategically adjusting its neural and mechanistic components (in a fashion akin to human experts), leading to significant improvements

    & Lung Cancer & Lung Cancer (with Chemo. \& Radio.) & Heart-gans & Platinum Movement & COVID-19 \\  Method & \(77.7 1\) & \(77.7 1\) & \(77.7 1\) & \(77.7 1\) & \(77.7 1\) & \(77.7 1\) \\ SINDy & \(372.5 1\) & \(118.4 0.095\) & \(17.7 0.573\) & \(88.28 1\) & \(00015.4 0.059\) & \(934.6 0.488\) \\ GP & \(159.4 1\) & \(154.4 0.495\) & \(171.8 0.59\) & \(514.1 0.087\) & \(0014.0 0.054\) & \(10.1 18\) \\ D/NODE & \(77.2 1\) & \(54.4 1\) & \(16.3 0.49\) & \(43.9 0.054\) & \(0006.00 0.007\) & \(74.2 1.36\) \\ RNN & \(1.17 0.6 1.09\) & \(208.8 1\) & \(16.4 5\) & \(3.71 0.49\) & \(0038.01 0.048\) & \(1.36 0.041\) (\(1.64 0.44\)) \\ Transformer & \(7.8 1\) & \(60.8 0.061\) & \(0.26 0.045\) & \(716.4 2\) & \(5.86 0.48\) & \(4.39 0.42\) \\ APPNITY & \(9.6 1\) & \(31.6 2\) & \(1.21 0.49\) & \(16.6 0.43\) & \(312.1 2\) & \(4.26 0.45 0.48\) & \(858.9 9.97\) \\ ZerSoft & \(5.45 0.41 0.71 0.41 0.41\) & \(292.8 1\) & \(5.81 0.47\) & \(0038.1 0.40\) & \(138.0 0.41\) & \(0.13 0.42\) (\(2.31 0.41\)) \\ ZerSoft & \(2.16 1.72\) & \(31.2 4.5\) & \(6.68 1.78\) & \(353.0 0.40\) & \(0123.0 0.0116\) & \(7.88 0.014\) \\
**HDTwinGen** & **4.41\(\)0.87** & **0.889\(\)0.4453** & **0.131\(\)0.198** & **291\(\)0.03** & **2.51\(\)0.66\(\)2.3\(\)0.46** & **1.72\(\)1.28** \\   

Table 1: **Benchmark method performance. Reporting the test prediction MSE (\(_{MSE}\)) of the produced system models on held-out test datasets across all benchmark datasets. HDTwinGen achieves the lowest test prediction error. The results are averaged over ten random seeds, with \(\) indicating 95\(\%\) confidence intervals.**

    &  \\  &  \\ Method & IID \(T_{MSE}\)\(\) & OOD \(T_{MSE}\)\(\) \\  DyNODE & \(0.0115 0.0121\) & \(1.75 0.769\) \\ SINDy & \(0.302 0.286\) & \(5.9 2.55\) \\ RNN & \(1.43 0.4\) & \(2.02 0.265\) & \(1.84 0.54\) \\ Transformer & \(0.0262 0.00541\) & \(1.19 0.42\) & \(2.78 0.43\) \\  ZeroShot & \(4.95 0.3\) & \(1.43 0.4\) & \(1.51 0.46\) \\ ZeroOptim & \(3.49 0.0364\) & \(4.84 5.17\) \\
**HDTwinGen** & **0.00872\(\)0.0187** & **0.0846\(\)0.0891** \\   

Table 2: **Out of distribution shifts. On a variation of the Lung Cancer (with Chemo. & Radio.), HDTwinGen is more robust to OOD shifts in unseen state-action distributions.**

Figure 2: **Sample efficiency. Analyzing performance as a function of the number of training trajectories in the Lung Cancer (with Chemo. & Radio.) dataset. We observe that HDTwinGen achieves the lowest test prediction error, even in the very challenging low data regime. This highlights the role of priors embedded in HDTwin in sample-efficient generalization.**

in accuracy and robustness. This iterative evolution process demonstrates HDTwinGen's ability to adapt and optimize its modular components.

**Can HDTwinGen Understand and Modify Its HDTwin?** We investigate whether large language model (LLM) agents can take an optimized high-dimensional twin (HDTwin) from an existing benchmark dataset and adapt it to model an unobserved intervention that is not present in the training data. We note that this intervention emulates scenarios where the dynamics of the underlying system changes. We affirmatively answer this question by constructing a scenario where our COVID-19 simulator incorporates an unobserved intervention of a lockdown policy, which reduces physical interactions between individuals (Appendix H.2). As demonstrated in Figure 4, we observe that the code-model representation of the HDTwin can be (1) understood by the modeling agent LLM and (2) adapted in its parameters to accurately model and reflect this intervention. We find that HDTwinGen is the _only method capable of changing the overall functional behavior by modifying a single parameter in the model_; in contrast, all other existing data-driven methods require a dataset of state-action trajectories under the new dynamics introduced by this intervention.

**Ablation Studies**. We conducted ablation studies on HDTwinGen and found several key insights. First, retaining the top-\(K\) models within the LLM context leads to improved model generation (Appendix H.5). Additionally, HDTwinGen is compatible with various LLMs and different temperature settings (Appendix H.6). It also benefits from including textual descriptions of the variables to be modeled as prior information (Appendix H.7). Finally, HDTwinGen can be specifically instructed to generate mechanistic white-box models if desired (Appendix H.10).

Figure 4: **COVID-19 unobserved intervention. The symbolic code-based representation of HDTwin can be easily adapted to unobserved interventions through targeted adjustments of parameters.**

Figure 3: **HDTwinGen effectively evolves HDTwin. Validation MSE of the HDTwin generated in each iteration, showing the Pareto-front of the best generated HDTwin (Top-1 HDTwin), and the generated HDTwin per generation step— additionally with a few of the HDTwins labeled with their model descriptions. HDTwinGen can efficiently understand, modify, and hence _evolve_ the HDTwin to achieve a better-fitting model (Appendix H.4).**

## 8 Limitations and Discussions

In summary, this work addresses the problem of learning digital twins for continuous-time dynamical systems. After establishing clear learning objectives and key requirements, we introduce Hybrid Digital Twins (**HDTwins**)--a promising approach that combines mechanistic understanding with neural architectures. HDTwins encode domain knowledge symbolically while leveraging neural networks for enhanced expressiveness. Conventional hybrid models, however, rely heavily on expert specification with learning limited to parameter optimization, constraining their scalability and applicability. To overcome these limitations, we propose a novel approach to _automatically_ specify and parameterize HDTwins through **HDTwinGen**, an evolutionary framework that leverages LLMs to iteratively search for and optimize high-performing hybrid twins. Our empirical results demonstrate that evolved HDTwins consistently outperform existing approaches across multiple criteria, exhibiting superior out-of-distribution generalization, enhanced sample efficiency, and improved modular evolvability.

**Limitations.** While our results are promising, several important limitations remain. HDTwinGen's efficacy depends critically on human experts providing initial system specifications and on the underlying LLM's domain knowledge and model generation capabilities. Our current implementation focuses exclusively on continuous-time systems, which, although broadly applicable, represent only a subset of real-world systems. Future work could extend our approach through human-in-the-loop feedback mechanisms, integration with external tools, and expansion to broader system classes.

**Ethical implications.** We acknowledge the risk of bias transmission from the black-box LLMs into the evolved models. While our hybrid approach enables greater expert scrutiny through its human-interpretable components, we strongly recommend a comprehensive evaluation of evolved models for fairness, bias, and privacy concerns before deployment in sensitive applications.