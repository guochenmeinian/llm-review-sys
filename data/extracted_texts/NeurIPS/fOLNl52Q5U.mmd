# SimVG: A Simple Framework for Visual Grounding

with Decoupled Multi-modal Fusion

Ming Dai\({}^{1}\), Lingfeng Yang\({}^{2}\), Yihao Xu\({}^{1}\), Zhenhua Feng\({}^{3}\), Wankou Yang\({}^{1,4}\)

\({}^{1}\)Southeast University \({}^{2}\)Nanjing University of Science and Technology

\({}^{3}\)Jiangnan University \({}^{4}\)Advanced Ocean Institute of Southeast University, Nantong {mingdai, 220211848, wkyang}@seu.edu.cn,

yanglfnjust@njust.edu.cn, fengzhenhua@jiangnan.edu.cn

Corresponding authors.

###### Abstract

Visual grounding is a common vision task that involves grounding descriptive sentences to the corresponding regions of an image. Most existing methods use independent image-text encoding and apply complex hand-crafted modules or encoder-decoder architectures for modal interaction and query reasoning. However, their performance significantly drops when dealing with complex textual expressions. This is because the former paradigm only utilizes limited downstream data to fit the multi-modal feature fusion. Therefore, it is only effective when the textual expressions are relatively simple. In contrast, given the wide diversity of textual expressions and the uniqueness of downstream training data, the existing fusion module, which extracts multimodal content from a visual-linguistic context, has not been fully investigated. In this paper, we present a simple yet robust transformer-based framework, SimVG, for visual grounding. Specifically, we decouple visual-linguistic feature fusion from downstream tasks by leveraging existing multimodal pre-trained models and incorporating additional object tokens to facilitate deep integration of downstream and pre-training tasks. Furthermore, we design a dynamic weight-balance distillation method in the multi-branch synchronous learning process to enhance the representation capability of the simpler branch. This branch only consists of a lightweight MLP, which simplifies the structure and improves reasoning speed. Experiments on six widely used VG datasets, _i.e._, RefCOCO/+/g, ReferIt, Flickr30K, and GRefCOCO, demonstrate the superiority of SimVG. Finally, the proposed method not only achieves improvements in efficiency and convergence speed but also attains new state-of-the-art performance on these benchmarks. Codes and models are available at https://github.com/Dmmm1997/SimVG.

## 1 Introduction

Visual grounding (VG) aims to predict the corresponding regions of an image through linguistic expressions. The task necessitates a comprehensive understanding of each modality, as well as the modeling of consistency between image context and text. Some benchmarks focus on addressing _phrase localization_, which entails locating all objects mentioned in a sentence within an image. Another aspect emphasizes resolving _referring expression comprehension_ (REC) , characterized by only one target corresponding to a sentence. Recently, a new type of _general referring expression comprehension_ (GREC)  task has emerged. GREC is similar to REC, but in which a sentence can have multiple targets or no target at all.

Existing visual grounding models can be roughly divided into three categories: two-stage, one-stage, and transformer-based. Among them, as shown in Fig. 1(a), two-stage methods [19; 83; 79; 44; 38] require a pre-trained detector to generate proposals and perform localization through region-text retrieval. These methods rely on a complex module with manually designed mechanisms to achieve query reasoning and multi-modal fusion. One-stage methods [75; 36; 46; 73], on the other hand, employ an end-to-end architecture, as shown in Fig. 1(b). Most of them primarily perform dense prediction on multimodal fusion features defined in the form of anchors. Some recent algorithms [8; 25; 87; 57], depicted in Fig. 1(c), adopt an encoder-decoder architecture to perform multimodal fusion in the encoder and then decode the response target position using an object query similar to DETR. The existing methods share a commonality: they adopt architectures that independently encode each modality before merging them, with multimodal fusion intricately linked to each visual grounding task. The feature extraction part of these methods generally employs specific classification [7; 55] or autoregressive [9; 30] tasks in each modality for pre-training. However, the alignment and mutual understanding between modalities only utilize a limited amount of downstream data, which undoubtedly underestimates the difficulty of achieving mutual understanding between modalities. Another observed trend, as noted in [25; 87], is the notable enhancement in the performance of visual grounding with a significant augmentation of pretraining data on large corpora. This implicitly suggests that leveraging a small amount of downstream data does not fully capitalize on the potential for mutual understanding between images and text. Nevertheless, this type of pretraining increases the burden of training resources.

Furthermore, the mutual understanding between multiple modalities is crucial for downstream tasks. As shown in Fig. 2, for a dataset with long sentence characteristics (like RefCOG ), adopting the decoupled multimodal understanding method can significantly improve model performance, while the improvement is relatively modest on datasets with short sentences. This observation aligns with our expectation that shorter captions pose less challenge for inter-modal understanding, whereas SimVG's decoupling approach proves advantageous for challenging longer descriptions that require intricate multi-modal comprehension.

To be specific, we delve into existing multimodal understanding methods in the form of multimodal pre-training. Existing approaches can be broadly categorized into three categories. Dual-stream structures [51; 23; 33] encode image-text modalities independently and supervise them with contrastive learning. One-stream models [6; 27; 30; 22] concatenate multimodal features for feature extraction. Other works [31; 1; 58; 64] use a dual-stream design with a fusion encoder to balance complexity and computational cost, fusing multimodal features in intermediate layers. Some methods

Figure 1: An overview of visual grounding structures: (a) Two-Stage: Applying a detector for proposals, followed by image-text encoding and feature similarity calculation for region matching. (b) One-Stage: Grounding in the fused features through dense prediction. (c) Transformer-based: Employing an encoder-decoder structure in the head. (d) Proposed SimVG: Utilizing Multi-Modality Encoder for multimodal interaction among object, image, and text tokens, directly applies a lightweight MLP for grounding.

Figure 2: The expression length and relative improvement between Dynamic MDETR  and SimVG.

also have applied multimodal pre-training to VG tasks, such as Dynamic MDETR , which uses an image-text encoder pretrained on CLIP  to enhance the performance. Recently, works like CPT , ReCLIP , and FGVP  have improved the performance of zero-shot visual grounding using dual-stream pre-trained models that employ a two-stage-like architecture and prompt engineering techniques. However, these approaches focus on multimodal alignment rather than mutual understanding. Thus, the dual-stream with fusion encoder architecture [31; 1; 58; 64] has been thrown into our sight. Specifically, based on BEiT-3 , we propose a simple framework called SimVG that decouples multimodal fusion from downstream tasks and simultaneously encodes image, text, and object tokens, as illustrated in Fig. 1 (d). The decoder structure used for query reasoning, similar to DETR , is effective but inevitably increases the model's complexity and computational overhead. We aim to develop a more efficient and simpler architecture for visual grounding. To achieve this, in addition to the decoder branch, we introduce a lightweight token branch. This branch leverages object tokens that are deeply integrated with image-text features to enhance grounding performance. To ensure that the token branch maintains high performance while enabling efficient inference, we adhere to the principles of the existing knowledge distillation methods [21; 5; 3] and introduce an innovative dynamic weight-balance distillation (DWBD). This approach enhances the token branch's capability by dynamically assigning weights to the decoder's predictions and ground truth at various learning stages, facilitating more effective learning in the token branch. Furthermore, we introduce a text-guided query generation (TQG) module to integrate textual information into queries, enabling the adaptive incorporation of textual prior knowledge into object queries. Notably, the design of the TQG module allows for the expansion of object tokens, increasing the number of objects it can handle and thereby adapting effectively to the GREC  task. The experiments demonstrate that, by decoupling multimodal fusion from downstream tasks, SimVG achieves rapid convergence and superior performance even with a limited amount of data. Coupled with our proposed DWBD and TQG modules, SimVG sets new state-of-the-art performance across various benchmarks.

Our main contributions are summarized as follows:

\(\) We propose a simple yet strong visual grounding architecture called SimVG, which decouples multimodal understanding from downstream tasks and fully utilizes the inter-modal fusion ability of existing multimodal pre-trained models. To the best of our knowledge, SimVG is the first to employ a unified structure for encoding image, text, and object tokens in visual grounding.

\(\) We propose a novel dynamic weight-balance distillation (DWBD) to dynamically allocate weights to decoder predictions and ground truth at different stages of multi-branch synchronous learning, aiming to minimize the discrepancy between the token and decoder branches. Furthermore, we introduce a text-guided query generation (TQG) module to incorporate textual prior information into object queries, thereby extending its applicability to the GREC task.

\(\) The proposed SimVG architecture has demonstrated state-of-the-art performance across six prominent datasets, while also exhibiting notable gains in efficiency and convergence speed. Particularly noteworthy is that SimVG (ViT-B/32) achieves these results with just 12 hours of training on a single RTX 3090 GPU when applied to the RefCOCO/+/g datasets.

## 2 Related Work

Vision-Language Pre-Training.Existing vision-language pretraining (VLP) models can be broadly categorized into three main types: one-stream, dual-stream, and dual-stream with fusion encoder architectures. One-stream models [6; 27; 30; 22] process both image and text inputs in a single stream, concatenate image and text embeddings, and interact cross-modals information throughout the whole feature extraction process. In contrast, dual-stream models [51; 23; 33] employ separate encoders for each modality. These models do not concatenate modalities at the input level. Instead, the interaction between pooled image and text vectors occurs at a shallow layer. Dual-stream models with fusion encoder [31; 1; 58; 64] combine aspects of both one-stream and dual-stream models. They allow for intermediate interaction between modalities, potentially offering a balance between complexity and performance. In this paper, we improve the performance of visual grounding by decoupling multi-modal fusion from downstream tasks into upstream VLP models .

Referring Expression Comprehension.Early approaches in REC typically followed a two-stage pipeline [19; 83; 88; 79; 44; 72; 18; 38]. This pipeline involves first extracting region proposals , which are then ranked based on their similarity scores with the language query. In contrast, a more recent line of research [85; 75; 36; 46; 73; 20] advocates for a simpler and faster one-stage pipeline based on dense anchors. Several recent approaches [8; 86; 25; 10; 32; 87] have employed a transformer-based structure  for multi-modal fusion. Furthermore, with the vigorous development of multimodal large language models (MLLM) , some of the latest methods have further enhanced the generalization performance of REC through zero-shot [71; 59] or fine-tuning [69; 49] methods, leveraging the powerful capabilities of large models  and general models . In contrast to existing methods, our proposed SimVG method directly feeds object, image, and text tokens into the multi-modality encoder for multimodal feature interaction. We eschew the complex encoder-decoder structure and perform visual grounding directly using a simple MLP.

Knowledge Distillation in Object Detection.The majority of research in knowledge distillation has primarily focused on classification tasks [15; 66; 81]. Several studies [45; 65; 84; 63] have extended knowledge distillation techniques to dense prediction tasks, such as semantic segmentation and object detection. These works commonly exploit pixel-wise correlations or channel-wise interactions between dense features of teacher and student models. Recently, there has been a growing interest in developing tailored knowledge distillation losses for query-based detectors like DETR , as demonstrated by works such as [21; 5; 3]. Unlike previous methods that guide a lightweight student model using a pre-trained teacher model, this paper introduces knowledge distillation during synchronous learning to enhance the performance of the lightweight branch.

## 3 The Proposed Method

### The Overview of SimVG

As shown in Fig. 3, the SimVG structure can be roughly divided into three parts: multi-modality encoder, decoder, and head. The multi-modality encoder adopts a structure similar to BEiT-3 , and additionally sets a learnable object token. The decoder is divided into two branches: one is similar to the transformer decoder in DETR  (_decoder branch_), and the other utilizes a lightweight MLP (_token branch_). The head is referred to as the "Distillation Head". Unlike conventional prediction heads, to reduce the performance gap between the token and decoder branches, we employ a dynamic weight-balance distillation (DWBD) to minimize the performance difference between the two branches during synchronous learning.

Figure 3: Overview of the proposed SimVG. The token branch refers to the upper light yellow region, while the decoder branch refers to the lower light blue region. During model inference, we can independently apply the more lightweight token branch to improve inference speed and simplify the model architecture.

**Multi-Modality Encoder.** The input of SimVG include an image \(I^{3 H W}\) and a caption text \(T^{M}=\{t^{1},t^{2},...,t^{M}\}\), where \(\) denotes the set of words. First, the image is compressed to a scale of 1/32 of the original size using visual embedding to obtain \(P_{img}=\{p^{1},p^{2},...,p^{N}\}\). The text is then mapped to the vector space \(L_{text}=\{l^{1},l^{2},...,l^{K}\}\) and an text padding mask \(L_{mask}=\{l^{1}_{m},l^{2}_{m},...,l^{K}_{m}\}\). Additionally, we define a learnable object token \(T_{obj}\) as the target feature for the token branch. The query and attention padding mask of the transformer can be generated as:

\[=\{T_{obj},p^{1},...,p^{N},l^{1},...,l^{K}\},\ \ \ =\{T_{obj_{m}},p^{1}_{m},...,p^{N}_{m},l^{1}_{m},...,l^{K}_{m}\}.\] (1)

In the case where the image has no padding, \(T_{obj_{m}}\) and \(\{p^{1}_{m},p^{2}_{m},...,p^{N}\}\) are set to 0. In the independent encoding part, FFN adopts a setting of non-shared weights between the image and text modalities, while the rest remains largely the same with the original ViT  model.

**The Decoder Branch.** We first map the channel dimensions of the image tokens \(T_{img}^{ C}\) and text tokens \(T_{text}^{N_{text} C}\) using a linear layer without sharing weights. Then, on the text side, we apply the text-guided query generation (TQG) module to interact the predefined object queries \(Q_{obj}^{N_{eq} C}\) with the text tokens. On the image side, additional positional encoding is applied. Lastly, cross-attention interaction is performed through transformer modules. The entire process can be represented as follows:

\[Q_{decoder}=((T_{img})+pos,((T_ {text}),Q_{obj})),\] (2)

where \(\) and \(\) refer to the text projection and image projection. \(\) refers to multi-head cross attention. \(pos\) refers to position encoding, which applies 2D absolute sine encoding.

**The Token Branch.** We employ a linear layer \(\) to project object token \(T_{obj}^{1 C}\) and use the results of TQG to augment the object token. Lastly, we use an MLP layer to further interact with and enrich the representation of the object token. The process of this branch can be defined as:

\[Q_{token}=((T_{obj})+((T_{text }),Q_{obj})).\] (3)

**The Distillation Head.** We adopt the same Hungarian matching as DETR  for the decoder branch. The matching cost consists of three parts: binary cross-entropy loss, l1 loss, and giou loss . However, to further simplify the inference pipeline, we use the decoder branch as a teacher to guide the learning of the token branch during the whole training process. Therefore, the complete loss can be represented as follows:

\[_{total}=_{det}(p_{d},gt)+_{dwd},\ \ \ \ _{det}=_{1}_{ce}+_{2}_{l1}+ _{3}_{giou},\] (4)

where \(_{1}\), \(_{2}\), and \(_{3}\) are set to 1, 5, and 2. \(p_{d}\) refers to the decoder branch prediction. \(_{dwd}\) refers to the dynamic weight-balance distillation loss, which will be explained in the next part.

### Dynamic Weight-Balance Distillation

To make SimVG both efficient and effective, knowledge distillation is introduced, which leverages the predictions from the decoder branch as a teacher to guide the predictions of the token branch. Since the two branches share the features of the multi-modality encoder, training the teacher model independently using traditional knowledge distillation methods is not feasible. Instead, we employ a synchronous learning approach for both branches. This approach requires a delicate balance, ensuring that the performance of the teacher model is not compromised while maximizing the transfer of knowledge from the teacher branch to the student branch.

Therefore, we design a dynamic weight-balance distillation (DWBD), whose architecture is shown in Fig. 3. Let us denote the ground truth by \(y\), and the set of \(N_{q}\) decoder prediction by \(^{d}=\{^{d}_{i}\}_{i=1}^{N_{q}}\). To find a bipartite matching between these two sets, we search for a permutation of \(N_{q}\) elements \(_{N_{q}}\) with the lowest cost:

\[=_{_{N_{q}}}_{i}^{N_{q}} _{}(y_{i},_{(i)}),\] (5)

where \(_{}(y_{i},_{(i)})\) is a pair-wise matching cost between the ground truth \(y_{i}\) and a prediction with index \((i)\). After pairing, the next step is to assess the decoder branch's understanding capability at the current stage. This is done by measuring the confidence of the joint target box of the decoder target \(dt\) and ground truth \(gt\) based on the IOU of their pairing:

\[W_{dt}=}_{i}^{N_{gt}}[(b_{i},_{ }(i))(_{(i)})],\] (6)where \(N_{gt}\) is the number of ground truth boxes, SCORE represents the foreground score extracted from the predictions. \(W_{dt}\) can be seen as a reflection of the current stage's decoder branch capability, where a higher value indicates a stronger confidence. Lastly, \(_{dwbd}\) can be expressed as follows:

\[_{dwbd}=_{1}(W_{dt}_{det}(p_{t},dt))+_{2 }(W_{gt}_{det}(p_{t},gt)), W_{gt}=1-W_{dt},\] (7)

where \(_{det}\) is computed exactly the same as in Eq. 4. \(p_{t}\) refers to the token branch prediction. \(_{1}\) and \(_{2}\) are set to 2 and 1 in this paper. By design, in the early stage of network training, \(W_{gt} W_{dt}\), the training process of the entire token branch is guided by the ground truth. However, in the later training stage, \(W_{gt}<W_{dt}\), the guidance from the decoder target becomes more significant. This dynamic adjustment of weights during training is the core idea of the proposed DWBD. We will further analyze the changes in \(W_{dt}\) and \(_{dwbd}\) in Sec. 4.4.3. Additionally, to further enhance the performance of the token branch, we use a two-stage distillation approach. In the first stage, we train the decoder branch separately. In the second stage, we apply DWBD under the premise of synchronous learning for the two branches.

### Text-guided Query Generation

The initial object queries, \(Q_{init}\), are defined using learnable embeddings, without any prior information to guide them. From a macro perspective, visual grounding involves using text as a query to locate optimal regions in an image. Embedding text into queries to adaptively provide priors offers a viable solution. Therefore, we propose a text-guided query generation (TQG) module to generate object queries with text priors. As illustrated in Fig. 3, the process of generating queries through TQG can be expressed as follows:

\[Q_{tqg}=(Q_{init},f_{text}+pos,)+(f_{ text},)+Q_{init},\] (8)

where \(f_{text}^{K C}\) is the feature after text projection, Mask is consistent with Eq. 1, and \(pos\) here is 1D absolute sine positional encoding. **MMP** is the process of filtering out valid tokens from \(f_{text}\) using the Mask and applying a max operation: \((f,m)=_{i}[f_{i}( m_{i})]\).

## 4 Experimental Results

### Datasets and Evaluation Metric

The experiments in this paper are conducted on six widely used datasets: RefCOCO/+/g [80; 47; 48], Flickr30K Entities , ReferItGame , and GRefCOCO . For the referring expression comprehension and phrase localization tasks, Precision@0.5 is used as the evaluation metric. The GRefCOCO dataset is evaluated using the Precision@(F\({}_{1}\)=1, IoU\(\)0.5) and N-acc metrics. More descriptions about datasets and evaluation metrics are provided in Appendix A and B.

### Implementation Details

We train SimVG 30 epochs for REC and phrase localization, and 200 epochs for GREC, using a batch size of 32. Following standard practices, images are resized to 640\(\)640, and the length of language expressions is trimmed to 20 for all the datasets. For pre-training, SimVG is trained for 30 epochs and then fine-tuned for another 10 epochs. The pre-training experiments are run on 8 NVIDIA RTX 3090 GPUs. All the other experiments are conducted on 2 NVIDIA RTX 4090 GPUs. More implementation details are reported in Appendix C.

### Comparison with The State-of-the-art

In this part, we compare our SimVG with the SOTA methods on six mainstream datasets. We combine the results of RefCOCO/+/g, ReferItGame and Flickr30K datasets in Table 1, and the results of GREC are reported in Table 2. Table 3 reports the results pre-trained on the large corpus of data.

According to Table 1, our model performs better than two-stage models, especially MAttNet  while being 7 times faster. We also surpass one-stage models that exploit prior and expert knowledge, with +14% absolute average improvement over ReSC . Additionally, the use of a patch stride of 32 and a lightweight head design has enabled SimVG to achieve an inference speed of only 44 ms on a GTX 1080Ti GPU. For transformer-based models, SimVG surpasses the recent SOTA method Dynamic MDETR  with an average of up to 4.4% absolute performance improvement.

As shown in Table 1, on the ReferItGame and Flickr30K Entities datasets which mostly contain short noun phrases, the performance boosts to 74.83 and 82.04 with a large margin over the previous one-stage method . Compared to existing transformer-based methods [8; 32; 87], SimVG still significantly outperforms most SOTA methods by approximately 3.4 points on the ReferItGame dataset, and it also slightly outperforms Dynamic MDETR  on the Flickr30k dataset. Furthermore, scaling the model from base to large has led to significant improvements across all the datasets.

SimVG can be seamlessly extended to GREC without any network modification. As shown in Table 2, SimVG achieves a significant improvement over existing publicly available methods on the GRefCOCO dataset, with an average increase of 9 points, surpassing UNINEXT .

Table 3 demonstrates that when pre-training on a large corpus of image-text pairs, SimVG exhibits greater data efficiency as compared with most of the existing SOTA methods. Despite utilizing only 28K images, which is nearly six times fewer than MDETR , and three times fewer than RefTR , SimVG still achieves SOTA performance, surpassing most existing methods by a significant margin. Compared to MDETR, SimVG demonstrates an average improvement of 5 points, and compared to the recent SOTA model GroundingDINO , it achieves an average improvement of 2 points. Moreover, increasing the volume of pre-training data further enhances performance. Additionally, SimVG applies a lighter transformer structure in the head. Specifically, SimVG-TB only uses 1.58 million parameters, which is smaller than some lightweight models [87; 32]. Lastly, we observe that scaling the multimodal encoder from ViT-B to ViT-L results in the performance of the lightweight token branch surpassing that of its teacher model. We hypothesize that as the model size increases, the reliability of the decoder branch's performance improves, helping to mitigate the impact of mislabeled ground truth data. This, in turn, enhances the generalization ability of the token branch, further demonstrating the effectiveness of the DWBD method.

### Ablation Studies

#### 4.4.1 Multi-Modality Encoder Architecture

To investigate the advantages of decoupling multimodal fusion from visual grounding, we design three architectures for experimental verification. To ensure fairness, we consistently employ the ViT-B/32 model for feature extraction and the VGTR  head for prediction. "CLIP" represents a

   Models &  Visual \\ Encoder \\  &  \\ val \\  &  RefCOCO \\ testA \\  &  \\ testB \\ val \\  &  \\ testB \\  & val &  testB \\ val-g \\  & val &  testB \\ val \\  &  testB \\  &  testB \\  &  testB \\  &  test \\  &  test \\  &  test \\  & 
 (ms) \\  \\  
**Two-Stage** & & & & & & & & & & & & & \\  MANet  & RN101 & 76.40 & 80.43 & 69.28 & 64.93 & 70.26 & 56.00 & - & 66.58 & 67.27 & 29.04 & - & 320 \\ CM-ii-Erase  & RN101 & 78.35 & 83.14 & 71.32 & 68.09 & 73.65 & 58.03 & - & 67.99 & 68.67 & - & - & - \\ DGA  & VGG16 & - & 78.42 & 65.53 & - & 69.07 & 51.99 & - & - & 63.28 & - & - & 341 \\ RvG-Tree  & RN101 & 75.06 & 78.61 & 69.85 & 63.51 & 67.45 & 56.66 & - & 66.95 & 66.51 & - & - & - \\ NMTree  & RN101 & 76.41 & 81.21 & 70.09 & 66.46 & 72.02 & 57.52 & 64.62 & 65.87 & 66.44 & - & - & - \\ 
**One-Stage** & & & & & & & & & & & & & \\  RealGIN  & DN53 & 77.25 & 78.70 & 72.10 & 62.78 & 67.17 & 54.21 & - & 62.75 & 62.33 & - & - & 35 \\ FAOA  & DN53 & 71.15 & 74.88 & 66.32 & 56.86 & 61.89 & 49.46 & - & 59.44 & 58.90 & 60.67 & 68.71 & 39 \\ RCCC  & DLA34 & - & 81.06 & 71.85 & - & 70.35 & 56.32 & - & - & 65.73 & 63.79 & **- & 25** \\ MCN  & DN53 & 80.08 & 82.29 & 74.98 & 67.16 & 72.86 & 57.31 & - & 66.46 & 66.01 & - & - & 56 \\ ReSC\({}_{L}\) & SN53 & 77.63 & 80.45 & 72.30 & 63.59 & 86.36 & 56.81 & 63.12 & 67.30 & 67.20 & 64.60 & 69.28 & 36 \\ LBYL  & DN53 & 79.67 & 82.91 & 74.15 & 68.64 & 73.38 & 59.49 & 62.70 & - & - & 67.47 & - & 30 \\ 
**Transformer-Based** & & & & & & & & & & & & & \\  TANG  & RN101 & 81.02 & 82.72 & 78.35 & 64.82 & 70.00 & 56.94 & 67.02 & 68.67 & 67.73 & 70.73 & 79.10 & 62 \\ TRAR  & DN53 & - & 81.40 & 78.60 & - & 69.10 & 56.10 & - & 68.90 & 68.30 & - & - & - \\ VGTR  & RN50 & 78.29 & 81.49 & 72.38 & 63.29 & 70.01 & 55.64 & 61.64 & 61.49 & 64.01 & 63.63 & 75.44 & - \\ SegTR  & DN53 & 83.72 & 86.51 & 81.24 & 71.45 & 76.26 & 64.88 & 71.50 & 78.46 & 74.21 & 69.66 & 81.23 & 50 \\ VLTVG  & RN50 & 84.53 & 87.69 & 79.22 & 73.60 & 73.60 & 74.53 & 72.53 & 74.90 & 73.88 & 71.60 & 79.18 & 79\({}^{*}\) \\ TransC  & RN50 & 84.25 & 87.38 & 79.78 & 73.07 & 78.05 & 63.35 & 72.60 & - & - & 72.05 & 80.04 & 74\({}^{*}\) \\ DynMDETR  & ViT-B/16 & 85.97 & 88.82 & 80.12 & 74.83 & 81.70 & 63.44 & 72.21 & 74.14 & 74.49 & 70.37 & 81.89 & - \\  SimVG-TB (ours) & ViT-B/2 & 87.07 & 89.04 & 80.57 & 78.84 & 83.50 & 70.67 & 77.66 & 79.82 & 79.93 & 74.59 & 81.59 & 44 \\ SimVG-DB (ours) & ViT-B/32 & 87.63 & 90.22 & 84.04 & 78.65 & 83.76 & 71.82 & 78.81 & 80.37 & 80.51 & 74.83 & 82.04 & 52 \\  SimVG-TB (ours) & ViT-L/32 & **90.61** & **92.53** & **87.68** & **85.36** & **86.91** & **79.74** & 79.34 & **85.99** & **86.83** & **79.30** & 82.61 & 101 \\ SimVG-DB (ours) & ViT-L/32 & 90.51 & 92.37 & 87.07 & 84.88 & 88.50 & 78.66 & **80.42** & 85.72 & 86.70 & 78.75 & **83.15** & 116 \\   

Table 1: Comparison with some SOTA methods. RN101 refers to ResNet101 , DN53 denotes DarkNet53 , ViT-B/32 means ViT-Base  with stride of 32 in visual embedding. \(*\) denotes testing with a NVIDIA RTX 3090 GPU, while other entries are tested with a GTX 1080Ti GPU. SimVG-TB and SimVG-DB refer to the SimVG model using only the token and decoder branch for inference, respectively.

typical dual-stream multimodal pretraining structure. "ViLT" represents a one-stream multimodal fusion method. "BEiT-3" represents a dual-stream method with a fusion encoder. The experimental results are reported in Table 4. Approaches like ViLT and BEiT-3, which decouple the multimodal fusion process from downstream, show significant improvements compared to the methods that adopt a multimodal independent encoder architecture.

However, this experiment does not aim to demonstrate the superiority of architectures like BEiT-3. Our focus is to highlight that, by decoupling multimodal fusion and leveraging readily available multimodal pretrained weights, we can significantly enhance the convergence speed and performance of visual grounding. As depicted in Fig. 4, ViLT and BEiT-3 demonstrate notably accelerated convergence by decoupling multimodal fusion. In contrast, although CLIP leverages a large amount of image-text data for pre-training, it only performs cross-modal alignment and does not integrate the information from the image and text models to achieve a fused representation.

As shown in Table 4, building upon BEiT-3. We observe that increasing the stride size of the original visual embedding from 16 to 32 and applying bilinear interpolation to the convolutional kernel significantly enhances performance. This is because bilinear interpolation preserves the original feature distribution after compression, thereby accelerating convergence. Furthermore, experimental results from the decoder and token branches reveal a notable performance gap, highlighting the necessity of designing dynamic weight-balance distillation to mitigate this disparity.

   &  \\   & val & testA & testB \\  CLIP  & 73.93 & 77.14 & 67.43 \\ ViLT  & 78.54 & 82.31 & 72.47 \\ BEiT-3  & 82.35 & 84.66 & 78.38 \\  Baseline (BEiT-3) & 82.35 & 84.66 & 78.38 \\ +VE Interp. & **85.37(+3.02)** & 86.67(**+2.01)** & 81.57(**+3.19)** \\  Token Branch & 85.47 & 86.75 & 81.66 \\ Decoder Branch & 86.78 & 88.19 & 82.83 \\  

Table 4: Some ablation experiments on different multimodal fusion architectures. VE Interp. refers to the downsampling convolution kernel in Visual Embed that performs bilinear interpolation from pre-trained weights.

Figure 4: The convergence speed of three different multimodal pretraining architecture models.

   &  &  &  &  &  \\   & Encoder & Preco (F\(\), \(\)0.5) & N-acc. & Preco (F\(\), \(\)0.5) & N-acc. & Preco (F\(\), \(\)0.5) & N-acc. \\  MCN  & DN53 & GRU & 28.0 & 30.6 & 32.3 & 32.0 & 26.8 & 30.3 \\ VLT  & DN53 & GRU & 36.6 & 35.2 & 40.2 & 34.1 & 30.2 & 32.5 \\ MDETR  & RN101 & RoBERTa & 42.7 & 36.3 & 50.0 & 34.5 & 36.5 & 31.0 \\ UNNEXT  & RN50 & BERT & 58.2 & 50.6 & 46.4 & 49.3 & 42.9 & 48.2 \\  SimV-TG (ours) & ViT-B/32 & / & 61.3 & **56.1** & 61.7 & **58.0** & 53.1 & **57.5** \\ SimV-GD (ours) & ViT-B/32 & / & **62.1** & 54.7 & **64.6** & 57.2 & **54.8** & 57.2 \\  

Table 2: GREC benchmark results on GRFCOCO dataset. Threshold is set to 0.7 for all the methods.

   &  &  &  &  &  & _{}\)} &  \\  & Encoder & (M) & images & val & testA & testB & val & testA & testB & val-u & test-u & (ms) \\  UNITER\(\&\) & RN101 & - & 4.6M & 81.41 & 87.04 & 74.17 & 75.90 & 81.45 & 66.70 & 74.86 & 75.77 & - \\ VILLA\(\&\) & RN101 & - & 4.6M & 82.39 & 87.48 & 74.84 & 76.17 & 81.54 & 66.84 & 76.18 & 76.71 & - \\ MDETR  & RN101 & 17.36 & 200K & 86.75 & 89.58 & 81.41 & 79.52 & 84.09 & 70.62 & 81.64 & 80.89 & 108 \\ RefTR  & RN101 & 17.86 & 100K & 85.65 & 88.73 & 81.16 & 77.55 & 82.26 & 68.99 & 79.25 & 80.01 & **40** \\ SeqTR  & DN53 & 7.90 & 174K & 87.00 & 91.15 & 83.59 & 78.69 & 84.51 & 71.87 & 82.69 & 83.37 & 50 \\ UniTAB  & RN101 & - & 200K & 88.59 & 91.06 & 83.75 & 80.97 & 85.36 & 71.55 & 84.58 & 84.70 & - \\ DQ-DETR  & RN101 & - & 200K & 88.63 & 91.04 & 83.51 & 81.66 & 86.15 & 73.21 & 82.76 & 83.44 & - \\ GroundingDND  & Swin-T & - & 200K & 89.19 & 91.86 & 85.99 & 81.09 & 87.40 & 74.71 & 84.15 & 84.94 & 120 \\ PolyFormer  & Swin-B & - & 174K & 89.73 & 91.73 & 86.03 & 83.73 & 88.60 & 76.38 & 84.46 & 84.96 & - \\ PolyFormer  & Swin-L & - & 174K & 90.38 & 92.89 & 87.16 & 84.98 & 89.77 & 77.97 & 85.83 & 85.91 & - \\ OFA-L  & RN152 & - & 20M & 90.05 & 92.93 & 85.26 & 85.80 & 89.87 & 79.22 & 85.89 & 86.55 & - \\ mPLUG-2  & VLT-L4 & - & 14M & 92.40 & 94.51 & 88.42 & 86.02 & 90.17 & 78.17 & 85.88 & 86.42 & - \\  SimV-GD (ours) & VTF-B/32 & 6.32 & **28K** & 90.98 & 92.68 & 87.94 & 84.17 & 85.85 & 75.35 & 85.90 & 86.23 & 52 \\ SimV-TG (ours) & VTF-B/32 & **1.58** & 174K & 90.59 & 92.80 & 87.04 & 83.54 & 80.05 & 77.50 & 85.38 & 86.28 & 44 \\ SimV-GD (ours) & VTF-B/32 & 6.32 & 174K & 91.47 & 93.65 & 87.94 & 84.83 & 88.85 & 79.12 & 86.30 & 87.26 & 52 \\  SimV-TG-D (ours) & VLT-L/32 & **1.58** & **28K** & **92.99** & **94.86** & 90.12 & **87.43** & 91.02 & 82.10 & 87.95 & 88.96 & 101 \\ SimV-GD (ours) & VLT-L/32 & 6.32 & **28K** & 92.93 & 94.70 & **90.28** & 87.28 & **91.64** & **82.41** & **87.99** & **89.15** & 116 \\  

Table 3: Comparison with pre-trained models on RefCOCO , RefCOCO+ , and RefCOCOg  datasets. We only count the parameters of transformer architecture in head.

[MISSING_PAGE_FAIL:9]

## 6 Conclusion

In this paper, we re-examine the visual grounding task by decoupling image-text mutual understanding from the downstream task. We construct a simple yet powerful model architecture named SimVG, which leverages the existing research in multimodal fusion to fully explore the contextual associations between modalities. Additionally, to simplify the whole pipeline while maintaining performance, we adopt dynamic weight-balance distillation (DWBD) to let the stronger decoder branch guide the lightweight token branch while learning synchronously. Furthermore, we propose a text-guided query generation (TQG) module to provide textual prior knowledge for object queries. Experimental results demonstrate that SimVG not only achieves improvements in efficiency and convergence speed but also attains new state-of-the-art performance across various benchmarks.