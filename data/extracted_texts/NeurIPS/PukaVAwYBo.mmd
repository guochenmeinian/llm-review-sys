# Learning and Transferring Sparse Contextual Bigrams with Linear Transformers

Yunwei Ren1   Zixuan Wang1   Jason D. Lee

Princeton University

{yunwei.ren, wangzx, jasonlee}@princeton.edu

Equal contribution.

###### Abstract

Transformers have excelled in natural language modeling and one reason behind this success is their exceptional ability to combine contextual informal and global knowledge. However, the theoretical basis remains unclear. In this paper, first we introduce the Sparse Contextual Bigram (SCB), a natural extension of the classical bigram model, where the next token's generation depends on a sparse set of earlier positions determined by the last token. We then analyze the training dynamics and sample complexity of learning SCB using a one-layer linear transformer with a gradient-based algorithm. We show that when trained from scratch, the training process can be split into an initial sample-intensive stage where the correlation is boosted from zero to a nontrivial value, followed by a more sample-efficient stage of further improvement. Additionally, we prove that, provided a nontrivial correlation between the downstream and pretraining tasks, finetuning from a pretrained model allows us to bypass the initial sample-intensive stage. We also empirically demonstrate that our algorithm can outperform SGD in this setting and discuss its relationship with the usual softmax-based transformers.

## 1 Introduction

Transformers have played a central role in modern deep learning, achieving significant success across various fields, including language modeling (OpenAI, 2023), computer vision (Dosovitskiy et al., 2020), and natural sciences (Jumper et al., 2021). The core of transformers is the self-attention layer (Vaswani et al., 2017), which can attend to any subset of the input sequence to output a weighted linear combination of the (transformed) tokens.

Several capabilities of the transformers contribute to their success in language modeling. First, they can extract contextual information from the input token sequences, which is essential in some arithmetic tasks (Edelman et al., 2022; Liu et al., 2022; Nanda et al., 2023; Yao et al., 2021). In addition, transformers can memorize global in-domain knowledge (Petroni et al., 2019; Zhang et al., 2023; Haviv et al., 2022; Carlini et al., 2021). These two abilities combined enable transformers to predict the next token based on the _in-context_ information as well as _global_ knowledge (OpenAI, 2023) acquired during training.

To theoretically understand how transformers learn both capabilities, we propose a minimalist data-generating model, the **Sparse Contextual Bigram** (SCB). This model builds on the classical bigram model and requires learning both contextual information and the (global) transition probabilities. Here, the next token depends on the transition matrix \(\) and a sparse set of prior tokens that is determined by the last token. In particular, SCB can be represented by a one-layer linear transformer -- a simplified architecture that can serve as an abstraction for studying transformer optimization (Ahn et al., 2023), which makes it suitable for theoretical analysis.

In this paper, we investigate the training dynamics and sample complexity of training a linear transformer to learn the SCB task using a stochastic gradient-based algorithm. Our contributions are summarized as follows:

* **Data model:** We introduce the **Sparse Contextual Bigram** (SCB) model, a simple task that requires the model to learn both in-context and global information.
* **Convergence:** We prove convergence guarantees for a one-layer linear transformer trained on with the nonconvex \(_{1}\)-regularized MSE loss using preconditioned projected proximal descent, given a dataset sampled from the SCB model.
* **Sample Complexity:** Under mild conditions on the data distribution, initialization, and hyperparameters, we prove that our algorithm can recover the ground-truth with polynomial dependence on the sequence length \(T\), number of states \(N\), and the sparsity parameter \(Q T\). We show that the training first goes through an initial sample-intensive stage which boosts the signal with \((T)\) samples, followed by a more sample-efficient stage to achieve final convergence with \((N,Q)\) samples. We empirically verify that our gradient-based methods converge to the ground truth with a small batch size, while unregularized stochastic gradient descent fails due to the large variance.
* **Transfer Learning:** We prove that, when there is a nontrivial correlation between the pretraining and downstream tasks, we can transfer a pre-trained model to bypass the first sample intensive stage, so that our algorithm converges to the ground truth of the downstream task with only \((N,Q)\) samples.

### Related works

Training dynamics of transformers.Several works have studied the learnability aspects of specific transformer architectures. Jelassi et al. (2022) demonstrated that a Vision Transformer (ViT) (Dosovitskiy et al., 2020) trained through GD, augmented with positional-embedding attention matrices, can effectively capture spatial structures. Li et al. (2023) investigated the sample complexity necessary to achieve good generalization performance on a similar ViT model. Tarzanagh et al. (2023) established a connection between the optimization landscape of self-attention and the formulation of a hard-margin Support Vector Machine (SVM) problem that separates and selects specific optimal tokens and established global convergence under strong assumptions. Tian et al. (2023, 2020) provided insights into the training dynamics of the self-attention and MLP layers, respectively, although they did not establish convergence guarantees.

Another line of work focuses on the training dynamics of in-context learning. Mahankali et al. (2023) was among the first to introduce linear regression as an in-context learning task, while Zhang et al. (2023) proved global convergence of gradient flow for a single-layer linear self-attention layer on this task. Huang et al. (2023) provided a convergence guarantee for a one-layer transformer with softmax attention on a similar task where the in-context tokens are drawn from a specific data distribution. Chen et al. (2024) generalized the single-task linear regression task to a multi-task setting and proved the global convergence of multi-head attention architecture using gradient flow on the population loss with specific initialization. In contrast, our work focuses on the language modeling ability of transformers instead of their in-context learning ability.

Several recent works analyzed transformers from a Markov chain perspective. Bietti et al. (2024) studied the in-context bigram (phrased as _induction head_) from an associative memory viewpoint. Nichani et al. (2024) proved that a simplified two-layer transformer can learn the induction head and generalize it to certain latent causal graphs. Edelman et al. (2024) further investigated training process on bigram and general \(n\)-gram tasks, and observed multi-phase dynamics. Makkuva et al. (2024) studied the loss landscape of transformers trained on sequences sampled from a single Markov Chain. Our SCB model extends the classical bigram models to allow context-dependent sparse attention on previous tokens.

Several works, including Tian et al. (2023); Zhang et al. (2023); Huang et al. (2023); Tarzanagh et al. (2023); Nichani et al. (2024); Kim and Suzuki (2024), and ours, use a similar reparameterization, consolidating the key and query matrices into a single matrix \(\) to simplify the dynamics of the training process. Most previous studies (Tian et al., 2023; Zhang et al., 2023; Huang et al., 2023; Tarzanagh et al., 2023; Nichani et al., 2024; Kim and Suzuki, 2024; Wang et al., 2024; Chen et al., 2024) uses population loss to simplify the analysis. In contrast, our work goes beyond the population loss to analyze the sample complexity of the stochastic gradient descent dynamics. Although Li et al. (2023) also investigated the sample complexity on a different task, their model requires a pre-trained initialization, while our model is trained from scratch.

**Transfer Learning.** Transfer learning (Devlin et al., 2018) has gained significant attention in this deep learning era. From a theoretical perspective, several works have investigated the statistical guarantees of transfer learning from the representation learning perspective (Tripuraneni et al., 2020; Du et al., 2020; Arora et al., 2019; Hanneke et al., 2023). Recent studies on transfer learning mostly focus on linear models (Li et al., 2022; Tian and Feng, 2023; Fei and Li, 2021; Zhang et al., 2022; Ju et al., 2023; Dar and Baraniuk, 2022). For dynamics of transfer learning, Lampinen and Ganguli (2018) studied the behaviors of multi-layer linear networks in a teacher-student setting, while Dhifallah and Lu (2021) analyzed single-layer perceptrons. Damian et al. (2022) showed that a two-layer neural network can efficiently learn polynomials dependent on a few directions, enabling transfer learning.

To the best of our knowledge, this is the first work studying transfer learning for transformers Moreover, unlike previous works that assume a shared structure between the pretraining and downstream tasks, we only require them to have a non-trivial correlation, which is a much weaker assumption.

### Outline of this paper

In Section 2 we formalize the problem setup, including the SCB task, the transformer architecture, and the training algorithm. Section 3 consists of our main results, and we analyze the population dynamics to provide intuitions. Section 4 contains our transfer learning results. Experimental results can be found in Section 5.

## 2 Setup

In this section, we describe our data-generating model, the one-layer linear transformer architecture, and the training algorithm.

**Notations.** We use \([T]\) to denote the set \(\{1,2,...,T\}\). Matrices and vectors are denoted in upper-case bold letters (\(,,\), etc.) and lower-case bold letters (\(,\), etc.), respectively. For norm, \(\|\|\) denotes \(_{2}\) norm and \(\|\|_{F}\) denotes the Frobenius norm. Additionally, for \(^{N}\), \(\|\|_{}\) denotes \(\)-norm for matrix \(^{d N}\) for arbitrary \(d\), which is defined as \(\|\|_{}^{2}:=( ()^{})\). We use \(\{\}\) to denote the indicator function. We use \(()\) to hide logarithmic factors in the asymptotic notations.

### Data-generating model: Sparse Contextual Bigram

The bigram model, where the next token depends only on the current one, is arguably one of the simplest language models. To learn this model, it suffices to learn the transition probabilities \(^{N N}\) where \(P_{n,m}=[X_{t+1}=n X_{t}=m]\), which is achievable through a linear model (0-layer transformer).

A natural way to extend the classical bigram model is to allow the next token to depend on a context-dependent set of previous tokens. This extension can model situations such as generating the words after the phrase "by Theorem 3.2", which requires us to retrieve the statement of "Theorem 3.2". Here, we propose a simple extension of this type, which we call the **Sparse Contextual Bigram** (SCB). The contextual information is encoded by a sparse probability vector \(\) determined by the last token. To generate the next token, the model retrieves the tokens referenced by \(\) and applies the transition matrix \(\) (global knowledge) to one of them according to the distribution \(\).

Formally, our data-generating model SCB can be described as follows. Let \(T\) be the sequence length and \([N]\) the vocabulary. Let \(^{N N}\) be a transition matrix, with column1\(_{k}\) being the transition probability vector of token \(k\). Suppose that \(^{N}_{ 0}\) is the stationary distribution of \(\).

Each input sequence consists of \(T+1\) tokens \((x_{1},,x_{T+1})\), i.i.d. sampled from distribution \(\). The output token (label) is generated as follows. For each \(k[N]\), there is a probability vector \(^{(k)} R^{T}_{ 0}\) that represents the tokens the model needs to attend to when the last token \(x_{T+1}\) is \(k\). For notational simplicity, we will write \(=(x_{1},,x_{T})\), \(=(_{x_{1}},,_{x_{T}})\) and \(=(^{(1)},,^{(N)})^{T N}\). When \(x_{T+1}=k\), the output token \(x_{o}\) is sampled from the distribution

\[(x_{o}=n x_{T+1}=k,)=_{t=1}^{T}q_{t}^{(k)}_{n,x_{ t}}, n[N].\] (1)

In words, we first sample a position \(s[T]\) according to \(^{(k)}\) and then run one step of the Markov Chain \(\) from \(x_{s}\) to generate \(x_{o}\). Note that this model can be represented by a one-layer linear transformer (see the next subsection for details). We make the following assumptions on SCB task.

**Assumption 2.1** (Sparse Contextual Bigram, SCB).: _In the SCB task, we assume the following:_

1. _(Q-sparse) For some_ \(Q T\) _and each of_ \(^{(k)}\)_, at most_ \(Q\) _entries are nonzero._
2. _(Well-conditioned) There exists some constant_ \(C 1\) _such that for every_ \(k[N]\) _and_ \(t[T]\)_,_ \(q_{t}^{(k)}[1/(CQ),C/Q]\) _if it is nonzero, and_ \(_{k}[1/(CN),C/N]\)_._
3. _(Nontrivial transition)_ \(_{}^{2}-^{2} ^{2}\)_._
4. _(Long sequence)_ \(T(NQ)^{10}\)_._

**Remark on condition (c)**. We say the transition \(\) is trivial if the transition probability vectors are all the same, i.e., \(=^{}\). In this case, we have \(_{}^{2}=^{}, ^{}=^{2}\). Requiring \(_{}^{2}-^{2} ^{2}\) rules out situations where \(\) is too close to the trivial one. Also, note that for any well-conditioned \(\), we have \(^{2}(1/N)\). \(\)

In this work, we focus on the case where \((,x_{T+1},x_{o})\) are given as (one data point of) the training data with \((x_{1},,x_{T+1})\) i.i.d. sampled from \(\). The SCB task can be extended to a sequence-to-sequence model: we drop \(x_{1}\) and append \(x_{o}\) to get a new input sequence \((x_{2},,x_{T+1},x_{o})\), and then repeat the same sampling procedure to generate another token. This generates a sequence \((x_{t})_{t=1}^{}\) where \((x_{T+2},x_{T+1},)\) are not independent, and this makes our model a true language model. We leave the study of the more complicated learning-from-\((x_{t})_{t=1}^{}\) task to future works.

### Transformer architecture

Our learner model is a one-layer single-head linear transformer (Akyurek et al., 2022; Zhang et al., 2023; Ahn et al., 2023). A general linear transformer can be expressed as: \((,x_{T+1};,)=(^{}),\) where \(\) is the embedding of the input tokens and positions, and \(\), \(\) are the parameters of the attention and output layers, respectively. In our setting, we only need a simpler model:

\[(,x_{T+1};,):=(_{T}_ {x_{T+1}})=:^{(x_{T+1})},\] (2)

where \(^{N N}\) and \(^{T N}\) are the trainable parameters, and \(^{(k)}\) denotes the \(k\)-th column of \(\). This model uses cross-attention (replacing the last \(\) with \(_{x_{T+1}}\)), uses only the positional embeddings together with the last token to compute the attention weights (replacing the second \(\) with \(_{T}\)), and discards the positional embeddings in the output layer (replacing the first \(\) with \(\)). This is equivalent to manually set certain blocks in the weight matrices to \(0\), which is a common practice in the theoretical literature to simplify the analysis (Nichani et al., 2024; Huang et al., 2023; Zhang et al., 2023).

Note that our data-generating model (1) can be represented using (2) by setting \(=\) and \(=\). We will show that a modified version of SGD can approximately recover this ground-truth model.

### Training algorithm

We assume that the stationary distribution \(\) and certain norms of the ground-truth \(\) and \(\) are known when choosing the initialization and learning rate. The goal here is to recover \(\) and \(\). Our loss function is the \(_{1}\)-regularized MSE loss. The standard way to optimize an \(_{1}\)-regularized loss is to use the proximal gradient descent. We adopt this algorithm with several additional pre-conditioning and a projection step to ensure some basic properties.

Formally, let the per-sample loss be defined as

\[l(,x_{T+1},x_{o};,):=\|_{x_{o}}-_{x_{T+1}}\|^{2}.\] (3)

We initialize \(=_{T}_{N}^{}/T\) to have uniform attention and \(=^{}\) to be the trivial transition. At each step \( 0\), we sample \(B_{}\) fresh samples \(\{^{(i)},x_{T+1}^{(i)},x_{o}^{(i)}\}_{i=1}^{B_{}}\) to form a mini-batch. The \(_{1}\)-regularized mini-batch loss is defined as

\[l_{}^{(B_{},)}(\{^{(i)},x_{T+1}^{(i)},x_{o}^{ (i)}\}_{i=1}^{B_{}};,):=}_{i=1}^{B _{}}l(^{(i)},x_{T+1}^{(i)},x_{o}^{(i)};,)+_{ k=1}^{N}\|^{(k)}\|_{1},\]

where \(>0\) is a parameter that controls the strength of regularization. Let \(_{}^{(B_{})}l\) and \(_{}^{(B_{})}l\) denote the mini-batch gradients of the original \(l\) w.r.t. \(\) and \(\), respectively. We then define the preconditioned gradients as

\[_{}^{(B_{})}l&:= (_{N}-_{N}_{N}^{}}{N})(_{ {V}}^{(B_{})}l)(1/)(_{N}-^{}}{\|\|^{2}}),\\ _{^{(k)}}^{(B_{})}l&:=}(_{T}-_{T}_{T}^{}}{T})(_ {^{(k)}}^{(B_{})}l), k[N].\] (4)

Here, the \(1/\) rescaling plays a role similar to importance sampling. We multiply \(_{}^{(B_{})}l\) with \(-^{}/N\) and \(-^{}/\|\|^{2}\) to ensure at least \(_{N}^{}=_{N}^{}\), \(_{T}^{}=_{N}^{}\), and \(=\) always hold throughout training. Note that we project each column of \(\) to the affine space \(\{^{T}\,:\,^{}=1\}\) instead of the probability simplex. This is sufficient for our analysis and is much easier to compute than the latter. We update the output layer using

\[_{+1}=_{}-_{V}\,_{}^{(B_{})}l,\] (5)

where \(_{V}>0\) is the step size. Now, consider the attention layer. Due to the existence of the \(_{1}\)-regularization, the update rule becomes a simple variant of the standard proximal gradient descent. Formally, for step size \(_{A}>0\), each \(k[N]\) and \(t[T]\), we have

\[_{+1}^{(k,)}&=_{ }^{(k)}-}{_{k}}_{^{(k)}}^{(B_{})}l,& ,\\ _{t,+1}^{(k,)}&=a_{t, +1}^{(k,)}-,&a_{t,+1}^{(k,)},\\ 0,&|a_{t,+1}^{(k,)}|,,& ,\\ _{+1}^{(k)}&=\\ ^{}=1)(_{+1}^{(k,)})=_{+1}^{(k,)}+(1-^{}_{+1}^{(k,)}) }{T},&.\] (6)

For the proximal step, we will later show that no \(a_{t}^{(k)}\) can ever become smaller than \(-\), so it suffices to consider those two cases. During the proximal step, all small \(a_{t}^{(k)}\) are set to \(0\), and \(\) is subtracted from all large coordinates. For notational simplicity, we define \(_{,}^{(k)}:=-_{A}^{-1}(_{+1}^{(k)}-_{ }^{(k)})\) so that we can write the update as \(_{+1}^{(k)}=_{}^{(k)}-_{A}_{}^{(k)}\). We will choose \(=0\) in certain stages of training. In this case, (6) becomes the usual projected preconditioned gradient descent and we have

\[_{+1}^{(k)}=_{}^{(k)}-_{A}_{^{(k)}}^{(B_{ })}l.\]

Our algorithm consists of three stages with different hyperparameters being used in different stages and certain rounding required between stages. The pseudocode is given in Algorithm 1 and more details on the projection/normalization steps are provided in Appendix E and F. When we train the model from scratch, all three stages are used and the initialization is \(_{0}=^{}\) and \(=_{T}_{N}^{}/T\).

**Transfer learning.** When doing transfer learning, the initialization will be obtained from the weights of the pre-trained model and one step of gradient update. Then, we will run Algorithm 1 from Stage 2.

## 3 Results for training from scratch

In this section, we consider the situations where we train the model from scratch, i.e., the initialization is \(_{0}=^{}\) and \(_{0}=^{}/T\) and discuss the ideas of the proof of the following theorem.

**Theorem 3.1** (Theorem G.1).: _Let \(>0\) be our target accuracy and \(_{1}=\{ 0:\{_{V,},_{A,}\} (1/(QN))\}\). We can choose the hyperparameters in Algorithm 1 such that within \((N,Q,1/, T)\) steps, we have \(-_{}^{2}\) and \(-_{}^{2}\) with probability at least \(1-\) and the numbers of samples used before and after \(_{1}\) are \((T,)\) and \((N,Q,1/, T,)\), respectively._

The overall strategy is analyzing the population process and then controlling the distance between the mini-batch trajectory and the population process2. In Section 3.1, we discuss the key properties of the population process that simplify the analysis. After that, we describe the dynamics of the algorithm and the signal-noise-ratio (SNR) in each of the three stages of Algorithm 1 in Section 3.2\(\)3.4.

### The population process

In this subsection, we analyze the behavior of the population process and the evolution of the signal-noise ratio. More details can be found in Appendix C, where the so-called population projected process are defined and rigorously analyzed.

For ease of presentation, we assume \(=0\) and access to the population loss \(:=\,l\). In other words, we consider the projected preconditioned gradient descent. By Lemma B.8, the dynamics of the population process is controlled by

\[_{+1}&=_{}-_{ V}(_{}^{2}(-^{ })-,_{}(-^{})),\\ _{+1}&=_{}-_{A}(( _{}^{2}-^{2} )(^{(k)}-)-(, _{}-^{2})(^{( k)}-)).\] (7)

One can prove via induction on \(\) that \(\) (resp. \(\)) always stays on the straight line crossing \(^{}\) and \(\) (resp. \(^{}/T\) and \(\)). In other words, there exists some time-dependent real numbers \(_{V,}\), \(_{A,}\), \(_{V,}:=1-_{V,}\), \(_{A,}:=1-_{A,}\) such that \(_{}=_{V,}\,+_{V,}\,^{}\) and \(_{}=_{A,}+_{A,}\,^{}/T\). The same calculation yields the following equations that govern the dynamics of \(_{V}\) and \(_{A}\):

\[_{V,+1}&=_{V,}+ _{V}K_{Q}\,(1-_{A}_{V})_{A}+_{V}}{T},\\ _{A,+1}&=_{A,}+_{A}K_{P}\, (1-_{V}_{A})_{V},\]

where \(_{V,0}=_{A,0}=0\), \(K_{P}:=_{}^{2}- ^{2} 1/N\) and \(K_{Q}:=_{}^{2}-1/T 1/Q\). Choose \(_{V}=/K_{Q}\) and \(_{A}=/K_{P}\), and we can write the above in matrix form as

\[_{V,+1}\\ _{A,+1}=(1-_{A}_{V})0&1 \\ 1&0_{V,}\\ _{A,}+}(1-_{V})/T \\ 0.\] (8)Hence, in order to analyze the population process, it suffices to analyze the above 2-dimensional ODE. In what follows, when we say the signal, we usually refer to these \(\)'s or some quantities whose size is proportional to them. In particular, as one can see from (8), the size of the expected gradients is proportional to \(_{V}\) and/or \(_{A}\)3.

Note that when both \(_{V},_{A}\) are still small, the population dynamics of \(_{V},_{A}\) are a linear system with coefficient matrix \([0&1\\ 1&0]\) and drift \([ T/K_{Q}\\ 0]\). The drift term will provide a small initial signal that guides the process toward the correct direction and then the linear term will amplify this signal. Since the linear term is close to \(0\) at initial and the initial signal provided by the drift term has order \(1/T\), we should expect that \(T\) samples are necessary to distinguish it from noises (Stage 1). After the signal becomes reasonably large, the first term will have order \(1/(N,Q)\), and we can then rely on it (combined with the \(_{1}\)-regularization) instead of the drift term to learn the model (Stage 2).

### Stage 1: boosting the signal

At initialization, we have \(_{V}=_{A}=0\). We define Stage 1 to be the phase until at least one of them has grown from \(0\) to some small \(_{1}=1/(N,Q)\). Note that in this stage, the mini-batch version of (8) is approximately equivalent to

\[_{V,+1}\\ _{A,+1}0&1\\ 1&0_{V,}\\ _{A,}+}1/T\\ 0+_{}+_{},\] (9)

where \(_{}\) and \(_{}\) represent the errors introduced by the difference between the mini-batch and population gradients, and the fact that we are not exactly on the population trajectory. If we had infinite amount of samples so that both \(_{}\) and \(_{}\) were \(0\), then the second term on the RHS of (9) could provide a small positive signal to \(\) and the first term would quickly boost it to \(_{1}\) within \(_{1}=(T)/\) iterations. In order for the above analysis to work, we need both \(_{}\) and \(_{}\) to be at least \(O(1)/T\) small. Since, unfortunately, \(_{}\) does not scale with \(1/T\), we need \((T)\) samples to ensure these conditions.

We conjecture that this \((T)\) dependence is unavoidable (when only a polynomial amount of computing time is available). That is because around the initialization, the only signal comes from the second term and the first term amplifies whatever the second term provides, even if it has been corrupted by the errors. It either takes \((T)\) fresh samples each step to reveal the signal or \((T)\) steps (whence also \((T)\) samples) for the random noises to (hopefully) cancel with each other.

### Stage 2: learning the model

We know that at the end of Stage 1, at least one of \(_{V}\) and \(_{A}\) is \(_{1}=1/(Q,N)\) large. Hence, one may expect that the signal is \(1/(Q,N)\) large now so that we no longer need to make the noises \(1/T\) small and therefore, only \((N,Q)\) samples are needed. Unfortunately, this argument will not work directly, since the variance of the mini-batch gradients scales with \(T\).4 Therefore, we still need \((T)\) samples to reduce the squared \(_{2}\)-norm of \(_{}\) from \((T)\) to \(1/(Q,N)\). To address this issue, we introduce the \(_{1}\)-regularizer and use a variant of proximal gradient descent.

The idea is, while the concentration in the \(_{2}\) sense is difficult, controlling the \(_{}\)-error is easy as every entry of \(_{^{(k)}}l\) is bounded whence subgaussian. As a result, we can make the coordinate-wise difference between the population and mini-batch gradients \(1/(N,Q)\) small using only \((N,Q)\)\( T\) samples by a standard concentration argument. Moreover, we have (cf. the proof of Lemma E.3)

\[-}_{_{t}^{(k)}}l=_{k}_{V}K_{P} (q_{t}^{(k)}-_{V}a_{t}^{(k)})=_{k}_{V}K_{P} (1/Q),&q_{t}^{(k)} 0,\\ O(_{V}a_{t}^{(k)}),&q_{t}^{(k)}=0.\] (10)

Thus, as long as \(_{V} 1/(N,Q)\), the \(_{}\)-norm of the gradient noise being small is enough to create a separation between those useful entries (\(q_{t}^{(k)} 0\)) and useless entries (\(q_{t}^{(k)}=0\)) and ensure the \(_{2}\)-error of those \(Q\) useful entries is small.

The above analysis suggests removing all small entries from the gradient will work. Now, we claim that \(_{1}\)-regularization and proximal gradient descent naturally implement this strategy, at least approximately. We believe softmax-based attention layers also automatically implement this strategy. See Section 5 for more discussion on the relationship between our model and softmax transformers.

Note that, at the end of Stage 1 and after the thresholding-projection step -- which is approximately equivalent to running one proximal step first -- we know that all useful \(a_{t}^{(k)}\) are at least \((_{V}/Q)=1/(N,Q)\), while all useless entries are of size \(O(1)/T\). By our previous discussion, we know that if \(\) is chosen appropriately, with \((N,Q, T)\) samples, the gradients w.r.t. those useful entries can be made approximately correct, while the gradients w.r.t. those useless entries are much smaller than \(\). Thus, after one gradient step, the absolute value of each of those useless \(a_{t}^{(k)}\) is still much smaller than \(\). As a result, they will be set to \(0\) in the proximal step (and to \(O(1/T)\) after the projection step), which is equivalent to filtering out all those entries, up to a small bias. Therefore, the proximal gradient updates stay close to the population trajectory, and the growth of the signals \(_{A},_{V}\) can be analyzed using the population dynamics.

We end Stage 2 when \((_{V}+_{A})/2 1\). Similar to Stage 1, this also only takes \(_{2}=(1/)\) steps. We also show that the difference between the mini-batch trajectory and the "population trajectory" can decrease to a small value (cf. Lemma E.10). This allows us to decouple the error introduced by Stage 1 and the target accuracy. We defer the proof details to Appendix E.

### Stage 3: final rounding and convergence

The purpose of Stage 3 is to fix a minor issue regarding \(|_{V}-_{A}|\). Taylor expand (8) around \((_{A},_{V})=(1,1)\) and one will notice that although \((_{V}+_{A})/2\) can converge to \(1\) at a linear rate (and the approximation error also decreases exponentially fast), the convergence rate of \(_{A}-_{V}\) is much slower, and the process will get stuck around \((1+,1-)\) for some small nonzero \(\), instead of converging to \((1,1)\). To accelerate this process, we directly round \(A\) via normalization, which is possible only after the approximation error becomes small in Stage 2. Then we freeze \(A\) and train \(V\) to the desired accuracy. More details about this stage can be found in Appendix F.

## 4 Results for transfer learning

The transferability of neural networks and transformers and their benefits have been widely observed and studied in both practice and theory. It is often assumed that the downstream and pretraining tasks share a common structure or representations/features, and these models can learn these common structures during training, and then leverage them in fine-tuning.

In this section, we offer a different perspective: as long as there is a (potentially small) nontrivial correlation between the pretraining and downstream tasks, the pretrained model can be used to provide a nonzero initial signal, allowing us to bypass the initial sample-intensive signal-boosting stage.

Formally, we consider the following setting. Let \(}\) be the transition matrix of the pretraining task and \((,)\) the transition matrix and \(\)-matrix of the downstream task. We still assume Assumption 2.1. In addition, we assume \(}\) and \(\) share the same stationary distribution \(\), \(}_{}^{2}=(1) _{}^{2}\) and \(},_{}- ^{2}^{2}.\) The last condition can be viewed as the transfer learning version of condition (c) of Assumption 2.1. Note that we allow the correlation between \(}\) and \(\) to be as small as \(o(1)\).

**Theorem 4.1** (informal version of Theorem H.3).: _Consider the above setting. Initialize \(=^{}/T\), \(=}+(1-)^{}\) for some small \(>0\), and run one step of gradient update on \(\). Then, running Algorithm 1 from Stage 2 allows us to recover \((,)\) to \(\)-accuracy with high probability with \((N,Q,1/, T)\) samples._

To intuitively see why using \((N,Q,1/, T)\) samples is possible, recall from (9) that the reason we need \((T)\) samples in Stage 1 is the signal is additive and has order \(O(1/T)\), so we need the size of the noise to be at most \(O(1/T)\). On the other hand, when we initialize \(=}+(1-)^{}\), we have \(_{V}(/(NK_{P})) 1/T\). Then we can rely on \(_{V}\), instead of the \(1/T\)-sized additive signal, to boost the signal of \(_{A}\) to \((1/T)\) in one step, which leads to a sample complexity that depends on \(_{V}\) instead of the \(1/T\)-sized additive signal. Then, we can reuse the analysis of Stage 2 and 3 to show that the downstream \((,)\) can be approximately recovered using \((N,Q,1/, T)\).

Note that unlike the case of training from scratch, when performing transfer learning, the initial approximation error \(\|_{}\|_{}\), i.e., the distance between \(\) and its population projection, can be much larger than the signal \(_{}\), and it might seem unreasonable to expect that we can leverage the small signal in the presence of a large approximation error. To handle this issue, we show that the _influence_ of \(\|_{}\|_{}^{2}\) on the dynamics scales with \(_{A}(_{})\), which is small. In addition, we also show that as long as \(_{A}\) is bounded away from \(0\) and the batch size is large, the approximation error will not grow. This allows us to ignore the approximation errors in the signal-boosting stage until we enter the regime of the Stage 2 analysis.

## 5 Experiments and relationship with softmax transformers

This section contains our experimental results. We also discuss the relationship between our linear model and the usual softmax transformers.

Experiment setupWe use the same shallow transformer model (2) to train on the synthetic data. The data distribution follows the SCB model (1) with a randomly sampled transition matrix \(\) together with its stationary \(\), and the ground truth attention pattern \(\). We choose the number of states \(N=3\), sparsity \(Q=2\), and the sequence length \(T=5000 N,\). We use a batch size \(B=64\) to run the online projected proximal gradient descent with \(=15\) and the vanilla SGD for \(=1000\) iterations. Through the signal boosting stage \(\), we use \(_{1}=0.01\) to accelerate the process. After \(>400\), we use \(_{2}=0.005\) for further improvement. For SGD, we add another set of experiments with \(_{2}^{}=0.001\) to prevent potential instability. For more details, see Appendix I.

### Convergence

Our experiments (cf. Fig. 1) show that after switching to proximal gradient descent after Stage 1 (the signal-boosting stage), both \(\|-\|_{}\) and \(\|-\|_{}\) decrease faster than SGD. The final distance to the ground-truth after normalization gets close to \(0\), and the similarity between the ground truth and parameters quickly converges close to \(1\). In comparison, SGD struggles to converge with the same small batch size and large learning rate, while the convergence rate is too slow when a smaller learning rate is used. This phenomenon verifies our theory that the variance of the original stochastic gradient will be too large for SGD to converge when \(T Q\), \(N\), while proximal gradient descent with an \(_{1}\) regularizer can resolve this issue.

### Relationship between our model and softmax transformers

We claim that they have our linear model and softmax transformers have qualitatively similar behaviors: there will be a sample-intensive initial stage, and after the model and the target have a nontrivial correlation, proximal gradient descent/SGD will become much more sample efficient.

For ease of presentation, in the following, we will assume \(N=1\), write \(:=^{(1)}\), and assume the ground-truth \(:=^{(1)}\) is \(_{1}=(1,0,,0)\). Most of our argument below can be generalized to the general setting at least at a heuristic level. Recall that our linear model is \(f(;,)=\). By a softmax transformer, we mean the model \(f_{}(;,)=()=: _{}\) where \(\) is the softmax function and \(^{T}\) is the trainable first-layer weights.

Figure 1: **Convergence analysis:** We plot the distance to the ground truth \(\|-\|_{}\), \(\|-\|_{}\) in different settings. After stage 1 ends at \(=400\) (when \(_{A},_{} 0.1\)), we use vanilla SGD and our proximal gradient method to train the transformer. Compared with SGD, the \(_{1}\) regularized proximal gradient descent quickly converges, and the final solution (the star) recovers the ground truth. SGD either suffers from the large gradient variance (when \(_{2}\) is large) or a slow convergence rate (small \(_{2}^{}\)).

Let \(l\) denote the (per-sample) loss. We have \(_{}l(f_{}())=((_{})- _{}_{}^{})()^{} l(f_{}( ))\). As a result, the dynamics of the attention weights are controlled by

\[(+1) ()-(-^{}}{T })()^{} l(f()),\] in our linear model, \[_{}(+1) _{}()-((_{ })-_{}_{}^{})^{2}()^{}  l(f_{}()),\] in softmax transformers.

In other words, the main difference is that there will be a preconditioning matrix \((()-^{})^{2}\) in the dynamics of softmax transformers.

Near initialization, i.e., when the attention pattern is still close to the uniform attention, we have \(((_{})-_{}_{}^{} )^{2}}(-^{}}{T})\). In other words, our linear model and softmax transformers are approximately equivalent up to a change in learning rates.

Now, suppose that there is a nontrivial correlation between \(_{}\) and \(=_{1}\), say, \(a_{,1}\) is a small constant while all other entries are \(O(1/T)\). In this case, we have \(((_{})-_{}_{}^{} )^{2} a_{,1}(1-a_{,1})_{1}_{1}^{}+O (1/T)\). Effectively, softmax transformers automatically adjust the learning rate according to \(a_{,t}\) and roughly ignore those positions with a small attention weight to stabilize the gradients. Note that this is also what \(_{1}\)-regularization does in our algorithm. In fact, mimicking this behavior is one of the motivations of using \(_{1}\)-regularization in our linear setting. We run further experiments to highlight the resemblance between softmax attention and our linear attention model (Figure 2).

## 6 Conclusion and discussion

In this paper, we propose the Sparse Contextual Bigram (SCB) model, which is a natural extension of the bigram model, that requires both contextual and global information. Then, we analyze the problem of learning a SCB model using a one-layer linear transformer and a gradient-based algorithm. We prove quantitative bounds on the convergence rate and the sample complexity. In particular, we show when trained from scratch, the training process can be split into two stages, where the first stage uses a lot of samples to boost the signal from zero to a nontrivial value, while the second stage is much more sample-efficient. Then, we consider the problem in a transfer learning setting and prove that when there is a nontrivial correlation between the pretraining and downstream tasks, the first sample intensive stage can be bypassed.

Our data-generating model and results also lead to some interesting future directions. For example, can we improve the sample complexity of the first stage? What can we gain if the datapoints are sequences generated by repeatedly applying the SCB model?