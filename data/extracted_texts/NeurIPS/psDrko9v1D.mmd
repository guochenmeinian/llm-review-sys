# Efficient Combinatorial Optimization via Heat Diffusion

Hengyuan Ma

Institute of Science and Technology for Brain-inspired Intelligence

Fudan University

Shanghai, China 200433

hangyuanma21@m.fudan.edu.cn

&Wenlian Lu

Institute of Science and Technology for Brain-inspired Intelligence

Fudan University

Shanghai, China 200433

wenlian@fudan.edu.cn

&Jianfeng Feng

Institute of Science and Technology for Brain-inspired Intelligence

Fudan University

Shanghai, China 200433

jianfeng64@gmail.com

###### Abstract

Combinatorial optimization problems are widespread but inherently challenging due to their discrete nature. The primary limitation of existing methods is that they can only access a small fraction of the solution space at each iteration, resulting in limited efficiency for searching the global optimal. To overcome this challenge, diverging from conventional efforts of expanding the solver's search scope, we focus on enabling information to actively propagate to the solver through heat diffusion. By transforming the target function while preserving its optima, heat diffusion facilitates information flow from distant regions to the solver, providing more efficient navigation. Utilizing heat diffusion, we propose a framework for solving general combinatorial optimization problems. The proposed methodology demonstrates superior performance across a range of the most challenging and widely encountered combinatorial optimizations. Echoing recent advancements in harnessing thermodynamics for generative artificial intelligence, our study further reveals its significant potential in advancing combinatorial optimization. The codebase of our study is available in https://github.com/AwakerHhy/HeO.

## 1 Introduction

Combinatorial optimization problems are prevalent in various applications, encompassing circuit design , machine learning , computer vision , molecular dynamics simulation , traffic flow optimization , and financial risk analysis . This widespread application creates a significant demand for accelerated solutions to these problems. Alongside classical algorithms, which encompass both exact solvers and metaheuristics , recent years have seen remarkable advancements in addressing combinatorial optimization. These include quantum adiabatic approaches [8; 9; 10], simulated bifurcation [11; 12; 13], coherent Ising machine [14; 15], high-order Ising machine ,and deep learning techniques [17; 18]. However, due to the exponential growth of the solution number, finding the optima within a limited computational budget remains a daunting challenge.

Our primary focus is on iterative approximation solvers, which constitute a significant class of combinatorial optimization methods. An iterative approximation solvers typically begin with an initial solution and iteratively improve it by finding better solutions within the neighborhood of the current solution, known as the search scope or more vividly, _receptive field_. However, due to combinatorial explosion, as the scope of the receptive field increases, the number of solutions to be assessed grows exponentially, making a thorough evaluation of all these solutions expensive. As a result, current approaches are limited to a narrow receptive field, rendering them blind to distant regions in the solution space and heightening the risk of getting trapped in local minimas or areas with bumpy landscapes. Although methods like large neighborhood search , variable neighborhood search  and path auxiliary sampling  are designed to broaden the search scope, they can only gather a modest increment of information from the expanded search scope. Consequently, the current solvers' receptive field remains significantly constrained, impeding their efficiency.

In this study, we approach the prevalent limitation stated above from a unique perspective. Instead of expanding the solver's receptive field to acquire more information from the solution space, we concentrate on propagating information from distant areas of the solution space to the solver via heat diffusion . To illustrate, imagine a solver searching for the optima in the solution space akin to a person searching for a key in a dark room, as depicted in Fig. 1. Without light, the person is compelled to rely solely on touching his surrounding space. The tactile provides only localized information, leading to inefficient navigation. This mirrors the current situation in combinatorial optimization, wherein the receptive field is predominantly confined to local information. However, if the key were to emit heat, its radiating warmth would be perceptible from a distance, acting as a directional beacon. This would significantly enhance navigational efficiency for finding the key.

Motivated by the above metaphor, we propose a simple but efficient framework utilizing heat diffusion to solve various combinatorial optimization problems. Heat diffusion transforms the target function into different versions, within which the information from distant regions actively flow toward the solver. Crucially, the backward uniqueness of the heat equation  guarantees that the original problem's optima are unchanged under these transformations. Therefore, information of target functions under heat diffusion transformations of different degrees can be cooperatively employed for optimize the original problem (Fig. 1). Empirically, our framework demonstrates superior performance compared to advanced algorithms across a diverse range of combinatorial optimization instances, spanning quadratic to polynomial, binary to ternary, unconstrained to constrained, and discrete to mixed-variable scenarios. Mirroring the recent breakthroughs in generative artificial intelligence through diffusion processes , our research further reveals the potential of heat diffusion, a related thermodynamic phenomenon, in enhancing combinatorial optimization.

Figure 1: **The heat diffusion optimization (HeO) framework.** The efficiency of searching a key in a dark room is significantly improved by employing navigation that utilizes heat emission from the key. In our framework, heat diffusion transforms the target function of a combinatorial optimization problem into different versions while preserving the location of the optima. Therefore, the gradient information of these transformed functions cooperatively help to optimize the original target function.

Failure of gradient-based combinatorial optimization

We will first formulate the general combinatorial optimization problem and then reframe it in a gradient descent-based manner. This reformulation allows us to utilize heat diffusion later. Various combinatorial optimization problems can be naturally formalized as a pseudo-Boolean optimization (PBO) problem , in which we aim to find the minima of a real-value target function \(f^{n}\) subjecting to a binary constraints

\[_{\{-1,1\}^{n}}f(),\] (1)

where \(\) is binary configuration (bits), and \(f()\) is the target function. Through the transformation \((+1)/2\), our definition aligns with that in , where elements of \(\) take \(0\) or \(1\). Given the advanced development of gradient-based algorithms, we are interested in converting the discrete optimization problem into a differentiable one, thereby enabling gradient descent. To achieve this purpose, we encode the bits \(s_{i}\), \(i=1,,n\) as independent Bernoulli variables \(p(s_{i}= 1|)=0.5(_{i}-0.5)\) with \(_{i}\). In this way, we convert the original combinatorial optimization problem into

\[_{}h(),\] (2)

where \(:=^{n}\), and \(h()=_{p(|)}[f()]\). The minima \(^{*}\) of Eq. (2) is \(^{*}=0.5((^{*})+1)\), given that \(^{*}\) is a minima of the original problem Eq. (1). Here, \(()\) is the element-wise sign function. Now Eq. (2) can be solved through gradient descent starting from a given initial \(_{0}\)

\[_{t+1}=_{t}-_{}h(_{t}), t=1,,T,\] (3)

where \(\) is the learning rate and \(T\) is the iteration number. Unfortunately, this yields a probability distribution \(p(|)\) over the configuration space \(\{-1,1\}^{n}\), instead of a deterministic binary configuration \(\) as desired. Although we can manually binarize \(\) through \(():=(-0.5)\) to get the binary configuration which maximizes probability \(p(|)\), the outcome \(f(())\) may be much higher than \(h()\), resulting in significant performance degradation . This suggests that a good gradient-based optimizer should efficiently diminish the uncertainty in the output distribution \(p(|)\), which can be measured by its total variance

\[V()=_{i=1}^{n}_{i}(1-_{i}).\] (4)

### Monte Carlo gradient estimation

Conventionally, we can solve the problem Eq. (2) by approximating the gradient of \(h()\) via Monte Carlo gradient estimation (MCGE)  (Alg. 2, _Appendix_), in which we estimate the gradient in Eq. (3) as

\[_{}h()=_{p(|)}[f()_{} p(|)]_{m=1}^{M}f(^{(m)})_{ } p(^{(m)}|),\] (5)

where \(^{(m)}_{i.i.d.}p(|), m=1,,M.\) However, it turns out that MCGE performs poorly even equipped with momentum, compared to existing solvers such as simulated annealing and Hopfield neural network, as shown in Fig. 2. We interpret this result as follows. Although MCGE turns the combinatorial optimization problem into a differentiable one, it does not reduce any inherent complexity of the original problem, which may contains a convoluted landscape. As gradient only provides local information, MCGE is susceptible to be trapped in local minimas.

## 3 Heat diffusion optimization

The inferior performance of the MCGE is attributed to the narrow receptive field of the gradient descent. To overcome this drawback, we manage to provide more efficient navigation to the solver by employing heat diffusion , which propagates information from distant region to the solver. Intuitively, consider the parameter space as a thermodynamic system, where each parameter \(\) is referred to as a location and is associated with an initial temperature value \(-h()\), as shown in Fig. 1.

Then the optimization procedure can be described as the process that the solver is walking around the parameter space to find the location \(^{*}\) with the highest temperature (or equivalently, the global minima of \(h()\)). As time progresses, heat flows obeying the Newton's law of cooling, leading to an evolution of the temperature distribution across time. The heat at \(^{*}\) flows towards surrounding areas, ultimately reaching the solver's location. This provides valuable information for the solver, as it can trace the direction of heat flow to locate the \(^{*}\).

### Heat diffusion on the parameter space

Now we introduce heat diffusion for combinatorial optimization. We extent the parameter space of \(\) from \(^{n}\) to \(}^{n}\) with \(}=\{-,+\}\). To keep the probabilistic distribution \(p(|)\) meaningful for \(^{n}\), we now redefine \(p(s_{i}= 1|)=(0.5(_{i}-0.5),0,1)\), where the clamp function is calculated as \((x,0,1)=(0,(x,1))\). Denote the temperature at location \(\) and time \(\) as \(u(,)\), which is the solution to the following unbounded heat equation 

\[_{}u(,)&=_{}u( ,),>0,^{n}\\ u(,)&= h(),=0, ^{n},\] (6)

where \(\) is the Laplacian operator: \( g()=_{i=1}^{n}_{x_{i}}g()\). For \(}^{n}/^{n}\), we define \(u(,)=_{_{n}}u(,_{n})\), where \(\{_{n}\}\) is a sequence in \(^{n}\) converged to \(\). Heat equation in the combinatorial optimization exhibits two beneficial characteristics. Firstly, the propagation speed of heat is infinite , implying that the information can reach the solver instantaneously. Secondly, the location of the global minima does not change across time \(\), as demonstrated in the following theorem.

**Theorem 1**.: _For any \(>0\), the function \(u(,)\) and \(h()\) has the same global minima in \(}^{n}\)_

\[_{}^{n}}u(,)=_{ }^{n}}h()\] (7)

Consequentially, we can generalize the gradient descent approach Eq. (3) by substituting the function \(h()\) with \(u(_{t},)\) for different \(_{t}>0\) at different iteration step \(t\) in Eq. (3), as follows

\[_{t+1}=_{t}-_{}u(_ {t},_{t}),\] (8)

where the subscript '\({}_{t}\)' in \(_{t}\) means that \(_{t}\) can vary across different steps. In this way, the solver can receive the gradient information about distant region of the landscape that is propagated by the heat diffusion, resulting in a more efficient navigation. However, Eq. (8) will converge to \(_{i}=+,&s_{i}^{*}=+1\\ -,&s_{i}^{*}=-1\), since \(_{t}\) are unbounded. To make the procedure Eq. (8) practicable, we project the \(_{t}\) back to \(\) after gradient descent at each iteration

\[_{t+1}=_{}_{t}- _{}u(_{t},_{t}),\] (9)

so that \(_{t}\) always holds, where we define the projection as \(_{}()_{i}=(1,(0,x_{i}))\) for \(i=1,,n\) and for \(^{n}\). Eq. (9) is a reasonable update rule for finding the minimum \(^{*}\) within \(\) for the following two reasons: (1) The projection of the \(}\) is the minimum of \(h()\) in \(\), i.e., \(_{}}=^{*}\); (2) Due to the property of the projection, if the solver moves towards \(}\), it also gets closer to \(^{*}\). More importantly, since the coordinates of \(}\) are all infinite, the convergent point of Eq. (9) must be one of the vertices of \(\), i.e., \(\{0,1\}^{n}\). This suggests that Eq. (9) tends to give an output \(\) that diminishes the uncertainty \(V()\) (Eq. (4)).

### Solving the heat equation

To develop an algorithm for solving the combinatorial optimization problem from Eq. (9), we must solve the heat equation Eq. (6), which seems a significant challenge when the dimension \(n\) is high. Fortunately, the solution has a closed form if the target function \(f()\) can be written as a multi-linear polynomial of \(\)

\[f()=a_{0}+_{i_{1}}a_{1,i_{1}}s_{i_{1}}+_{i_{1}<i_{2}}a_{2,i_{ 1}i_{2}}s_{i_{1}}s_{1_{2}}++_{i_{1}<<i_{K}}a_{K,i_{1} i_{K}}s_{i_{1}}  s_{i_{K}},\] (10)

a condition met by a wide range of combinatorial optimization problems .

**Theorem 2**.: _Supposed that \(f()\) is a multilinear polynomial of \(\), then the solution to Eq. (6) is_

\[u(,)=_{p()}[f((-}{}))],^ {n},\] (11)

_where \((x)=}_{0}^{x}e^{-t^{2}}dt\) is the error function._

For more general cases other then Eq. (10) (such as Eq. (13)), we still use the approximation (Eq. (11)).

### Proposed algorithm

Based on Eq. (9) and Thm. 2, we proposed _heat diffusion optimization (HeO)_, a gradient-based algorithm for combinatorial optimization, as illustrated in Alg. 1, where we estimate Eq. (11) with one sample \(_{t}^{n}\), and we denote \(_{t}=_{t}\) for short. Our HeO can be equipped with momentum, which is shown in Alg. 3 in _Appendix_. In contrast to those methods designed for solving special class of PBO (Eq. (1)) such as quadratic unconstrained binary optimization (QUBO), our HeO can directly solve PBO problems with general form. Although PBO can be represented as QUBO , this necessitates the introduction of auxiliary variables, which may consequently increase the problem size and leading to additional computational overhead . Compared to other algorithms, our HeO has relatively low complexity. The most computationally intensive operation at each step is gradient calculation, which can be explicitly expressed or efficiently computed with tools like PyTorch's autograd, and can be accelerated using GPUs. As shown in Fig. S1 of the in _Appendix_, the time cost per iteration of our methods increases linearly with the problem dimension, with a small constant coefficient. Therefore, our HeO is efficient even in high-dimensional cases.

``` Input: target function \(f()\), step size \(\), \(\) schedule \(\{_{t}\}\), iteration number \(T\)  initialize elements of \(_{0}\) as \(0.5\) for\(t=0\)to\(T-1\)do  sample \(_{t}^{n}\) \(_{t}}f((-_{t}}{_{t}}))\) \(_{t+1}_{}_{t}-_{t}\) endfor Output: binary configuration \(_{T}=(_{T}-0.5)\) ```

**Algorithm 1** Heat diffusion optimization (HeO)

One counter-intuitive thing is that to minimize the target function \(h()\), the HeO actually are minimizing different functions \(u(_{t},)\) at different step \(t\). We interpret this by providing an upper bound for the target optimization loss \(h()-h(^{*})\).

**Theorem 3**.: _Denote \(=_{}f()\). Given \(_{2}>0\) and \(>0\), there exists \(_{1}(0,_{2})\), such that_

\[h()-h(^{*})(-f^{*})u(_ {2},)-u(_{2},^{*})+_{_{1}}^{_ {2}})-u(,^{*})}{}d ^{1/2}+.\] (12)

Accordingly, minimizing \(u(,)\) for each \(\) cooperatively aids in minimizing the original target function \(h()\). Thus, we refer to HeO as a _cooperative optimization_ paradigm, as illustrated in Fig. 1.

## 4 Experiments

We apply our HeO to a variety of NP-hard combinatorial optimization problems to demonstrate its broad applicability. Unless explicitly stated otherwise, we employ the \(_{t}\) schedule as \(}=:_{t}=(1-t/T)\) for HeO throughout this work. The sensitivity of other parameter settings including the step size \(\) and iterations \(T\) are shown in Fig. S2. This choice is motivated by the idea that the reversed direction of heat flow guides the solver towards the original of its source, i.e., the global minima. Noticed that this choice is not theoretically necessary, as elaborated in _Discussion_.

**Toy example.** We consider the following target function

\[f()=_{2}^{}(W+ _{1})\] (13)where \((x)=}\), and the elements of the network parameters \(_{1}^{n}\), \(_{2}^{m}\), \(W^{m,n}\) are uniformly sampled from \([-1,1]\) and fixed during optimizing. According to the universal approximation theory , \(f()\) can approximate any continuous function with sufficiently large \(m\), thereby representing a general target function. We compare the performance of our HeO with momentum (Alg. 3) against several representative methods: the conventional gradient-based solver MCGE  (with or without momentum), the simulated annealing , and the Hopfield neural network . As shown in Fig. 2, our HeO demonstrates exceptional superiority over all other methods, and efficiently reduces its uncertainty compared to MCGE.

**Quadratic unconstrained binary optimization (QUBO).** QUBO is the combinatorial optimization problem with quadratic target function (\(J^{n n}\) is a symmetric matrix with zero diagonals)

\[f()=^{}J.\] (14)

This corresponds to the case where \(K=2\) in Eq. (10). A well-known class of QUBO is max-cut problem , in which we divide the vertices of a graph into two distinct subsets and aim to maximize the number of edges between them. Its target function is expressed as Eq. (14), where \(J\) is determined by the adjacency matrix of the graph.

We compare our HeO with representative iterative approximation methods especially developed for solving QUBO including LQA , aSB , bSB , dSB , CIM , and SIM-CIM 

Figure 3: a, Illustration of the max-cut problem. b, Performance of HeO (Alg. 1) and representative iterative approximation methods including LQA , aSB , bSB , dSB , CIM  and SIM-CIM  on max-cut problems from the Biq Mac Library . Top panel: average relative loss for each algorithm over all problems. Bottom panel: the count of instances where each algorithm ended up with one of the bottom-2 worst results among the 7 algorithms.

Figure 2: Performance of HeO (Alg. 3, _Appendix_), Monte Carlo gradient estimation (MCGE), Hopfield neural network (HNN) and simulated annealing (SA) on minimizing the output of a neural network (Eq. (13)). Top panel: the target function. Bottom panel: the uncertainty \(V()\) (Eq. (4)).

on max-cut problems in the Biq Mac Library 1. We report the relative loss averaged over all instances and the count of the instances where each algorithm gives the bottom-2 worse result among the 7 algorithms. As shown in Fig. 3, our HeO is superior to other methods in terms of two metrics.

**Polynomial unconstrained binary optimization (PUBO).** PUBO is a class of combinatorial optimization problems, in which higher-order interactions between bits \(s_{i}\) appears in the target function. Existing methods for solving PUBO fall into two categories: the first approach involves transforming PUBO into QUBO by adding auxiliary variables through a quadratization process, and then solving it as a QUBO problem , and the one directly solves PUBO . Quadratization may dramatically increases the dimension of the problem, hence brings heavier computational overhead, while our HeO can be directly used for solving PUBO. A well-known class of PUBO is the Boolean \(3\)-satisfiability (\(3\)-SAT) problem , which involves determining the satisfiability of a Boolean formula over \(n\) Boolean variables \(b_{1},,b_{n}\) where \(b_{i}\{0,1\}\). The Boolean formula is structured in Conjunctive Normal Form (CNF) consisting of \(H\) conjunction (\(\)) of clauses, and each clause \(h\) is a disjunction (\(\)) of exactly three literals (either a Boolean variable or its negation). An algorithm of \(3\)-SAT aims to find the Boolean variables that makes as many as clauses satisfied.

To apply our HeO to the 3-SAT, we encode each Boolean variable \(b_{i}\) as \(s_{i}\), which is assigned with value \(1\) if \(b_{i}=1\), otherwise \(s_{i}=-1\). For a literal, we define a value \(c_{h_{i}}\), which is \(-1\) if the literal is the negation of the corresponding Boolean variable, otherwise it is \(1\). Then finding the Boolean variables that satisfies as many as clauses is equivalent to minimize the target function

\[f()=_{h=1}^{H}_{i=1}^{3}}s_{h_{i}}}{2}.\] (15)

This corresponds to the case where \(K=3\) in Eq. (10). We compared our HeO (Alg. 4, _Appendix_) with the second-order oscillator Ising machines (OIM) solver that using quadratization and the state-of-art 3-order OIM proposed in  on 3-SAT instance in SATLIB 2. As shown in Fig. 4, our HeO is superior to other methods in attaining higher quality of solutions and finding more the complete satisfiable solution (solutions that satisfying all clauses). Notably, for the cases of 175-250 variables, our HeO is able to find more complete satisfiable solutions, compared to the 3-order OIM, while the 2-order OIM fails to find any complete solutions .

**Ternary optimization.** Neural networks excel in learning and modeling complex functions, but they also bring about considerable computational demands due to their vast number of parameters. A promising strategy to mitigate this issue is quantization, which converts network parameters into discrete values . However, directly training networks with discrete parameters introduces a significant challenge due to the high-dimensional combinatorial optimization problem it presents.

We apply our HeO and MCGE to directly train neural networks with ternary value (\(-1,0,1\)). Supposed that we have an input-output dataset \(\) generated by a ground-truth ternary single-layer per

Figure 4: a, Illustration of the boolean 3-satisfiability (3-SAT) problem. b, Performance of HeO (Alg. 4, _Appendix_), 2-order and 3-order oscillation Ising machine (OIM)  on 3-SAT problems with various number of variables from the SATLIB . We report the mean percent of constraints satisfied (left) and probability of satisfying all claims (right) for each algorithm.

ceptron \(=(;W_{})=(W_{ })\), where \((x)=\{0,x\}\), \(W_{}\{-1,0,1\}^{m n}\) is the ground-truth ternary weight parameter, \(\{-1,0,1\}^{n}\) is the input, and \(\) is the model output. We aim to find the ternary configuration \(W\{-1,0,1\}^{m n}\) minimizing the loss \((W,)=|}_{(,)}\|(;W)-\|^{2}\). We generalize our HeO from the binary to the ternary case by representing a ternary variable with two bits, where each element of \(W\) can be represented as a function of \(\) (see Alg. 5, _Appendix_ for details), and the target function is defined as

\[f()=|}_{(,) }\|(;W())-\| ^{2}.\] (16)

As shown in Fig. 5, HeO robustly exceeds MCGE under different dataset size \(||\) and output size \(m\).

**Mixed combinatorial optimization.** In high-dimensional linear regression, usually only a small fraction of the variables significantly contribute to prediction. Identifying and selecting a subset of variables with strong predictive power--a process known as variable selection--is crucial, as it improves the generalizability and interpretability of the regression model . However, direct variable selection is an NP-hard combinatorial optimization mixed with continuous variables . As a practical alternative, regularization methods like Lasso algorithm are commonly employed .

Supposed a dataset is generated from a linear model, in which the relation between input \(\) and output \(y\) is \(y=^{*}+\), where \(^{*}\) is the ground-truth linear coefficient and \(\) is independent Gaussian noise with standard deviation \(_{}\). Suppose that only a small proportion (denoted as \(q(0,1)\)) of coordinates in \(_{i}^{*}\) are non-zero. Our goal is to identify these coordinates through an indicator vector \(\{-1,1\}^{n}\) (\(1\) for selection and \(-1\) for non-selection) and estimate these non-zero coefficients. The target function of the problem is (\(^{n}\) is the all-one vector)

\[f(,)=|}_{(, )}|( +}{2})-|^{2}.\] (17)

Figure 5: a, Training networks with ternary-value parameters. b, The weight value accuracy of the HeO (Alg. 5, _Appendix_) and Monte Carlo gradient estimation (MCGE) with momentum under different sizes of training set (\(n=100,m=1,2,5\)). We estimate the mean and std from 10 runs.

Figure 6: The variable selection of \(400\)-dimensional linear regressions using HeO (Alg. 6, _Appendix_), Lasso (\(L_{1}\)) regression  and \(L_{0.5}\) regression . We report the accuracy of each algorithm in determining whether each variable should be ignored for prediction and their MSE on the test set. The mean (dots) and standard deviation (bars) are estimated over 10 runs.

We solve the problem through HeO (Alg. 6, _Appendix_), where we minimize the loss relative to \(\) while slowing varying \(\) via its error gradient. After obtaining the indicator \(\), we conduct an ordinary least squares regression on the variables selected by the \(\) to estimate their coefficients, and treat other variables' coefficients as zero. As shown in Fig.6, our HeO outperforms both Lasso regression and the more advanced \(L_{0.5}\) regression  in terms of producing more accurate indicators \(\) and achieving lower test prediction errors across various \(q\) and \(_{}\) settings. Importantly, to give the variable selection prediction, our HeO does not need to know the level of \(q\) and \(_{}\) in advance.

Constrained binary optimization.Minimum vertex cover (MVC) is a class of the constrained combinatorial optimization which has wide applications , as illustrated in Fig. 7. Given an undirected graph \(\) with vertex set \(\) and edge set \(\), the MVC is to find the minimum subset \(_{c}\), so that for each edge \(e\), at least one of its endpoints belongs to \(_{c}\). The target function and the constrains are expressed as

\[f()=_{i=1}^{n}+1}{2},g_{ij}()=(1-+1}{2})(1-+1}{2})=0,  i,j,e_{ij}.\] (18)

We combine the HeO with penalty function (Alg. 7, _Appendix_) for solving MVC. We compare our HeO with FastVC , a powerful MVC heuristic algorithm, on massive real world graph datasets 3. For a fair comparison, we keep the run time of two algorithms as the same for each dataset. As shown in Tab. 1, our HeO can find smaller cover sets than that of FastVC.

## 5 Discussion

Existing model-based combinatorial optimization approaches encode the solution space via a parameterized distribution with iterative parameter updates . In contrast to HeO, which requires only one sample per iteration, they necessitate a large number of samples per iteration. The Gibbs-With-Gradient algorithm  uses gradient information for combinatorial optimization but searches in the discrete solution space instead of the continuous one, as did HeO. Denoising diffusion model (DDM)  has been applied for solving combinatorial optimization problems . Although the diffusion process in DDMs akin to the heat diffusion in our HeO, DDMs require a substantial data for training and necessitate reversing the diffusion process to generate data that from the target distribution. In contrast, HeO needs no training, and it is unnecessary to strictly adhering to the monotonic \(_{t}\) in the optimization process, as under different \(\), the function \(u(,)\) shares the same optima with that of the original problem \(h()\). This claim is empirically corroborated in Fig. S3, _Appendix_, where HeO applying non-monotonic schedules of \(_{t}\) still demonstrates superior performance.

   Graph name & Vertex number & Edge number & FastVC & HeO \\  tech-RL-caida & 190914 & 607610 & 78306 & **77372 (17)** \\ soc-youtube & 495957 & 1936748 & 149458 & **148875 (25)** \\ inf-roadNet-PA & 1087562 & 1541514 & 588306 & **587401 (104)** \\ inf-roadNet-CA & 1957027 & 2760388 & 1063352 & **1061339 (32)** \\ socfb-B-anon & 2937612 & 20959854 & 338724 & **312531 (194)** \\ socfb-A-anon & 3097165 & 23667394 & 421123 & **387730 (355)** \\ socfb-uci-uni & 58790782 & 92208195 & 869457 & **867863 (36)** \\   

Table 1: Attributes of graphs and the vertex cover sizes of HeO (Alg. 7, _Appendix_) and FastVC .

Figure 7: The illustration of minimum vertex cover.

Our HeO can be viewed as a stochastic Gaussian continuation (GC) method  with projection. GC has been applied for non-convex optimization, though it has not yet been used for combinatorial optimization. The optimal convexification of GC  underpins potentially theoretical advantages of HeO. One key distinction is that GC typically optimizes each sub-problem (corresponding to \(u(,_{t})\)) at each \(t\) up to some criteria, whereas HeO merely performs a single-step gradient descent. Also, Eq. (8) corresponds to a variation of the evolution strategy, a robust optimizer for non-differentiable function , while HeO use a different gradient estimation (Eq. (11)), see _Appendix_ for details. Additionally, our HeO is related to randomized smoothing, which has been applied to non-smooth optimization  or neural network regularization . The distinctive feature of our HeO is that, across different \(\), the smoothed function \(u(,)\) retains the optima of the original function \(h()\) (Thm. 1). This distinguishes our HeO from methods based on quantum adiabatic theory , bifurcation theory  and other relaxation strategies , in which the optima of the smoothed function can be different from the original one [27; 55]. This is verified in Fig. S3, _Appendix_.

The heat equation in our HeO can be naturally extended to general parabolic differential equations, given that a broad spectrum of them obey the backward uniqueness . For example, we can use \(_{}u(,)=_{}[A_{}u(,)]\) to replace the Eq. (6), where \(A\) is a real positive definite matrix. Prior researches have demonstrated that the optimization procedure can be significantly accelerated by preconditioning  or Fisher information matrix , implying that choosing a proper matrix \(A\) could substantially improve the efficacy of the HeO. Additionally, since \(_{t}\) does not necessarily have to be monotonic due to the cooperative optimization property of HeO, it is feasible to explore various \(_{t}\) schedules (possibly non-monotonic) to further enhance performance. Moreover, our HeO can be integrated with other existing techniques for combinatorial optimization problems with constraints, such as Augmented Lagrangian methods, to achieve better performance .

Despite the effectiveness of HeO on the combinatorial optimization problems from different domains we have considered, it has limitations. First, current HeO is inefficient for integer linear programming and routing problems, primarily due to that it is cumbersome to encode integer variables through the Bernoulli distribution in our framework. Nevertheless, integrating HeO with other techniques such as advanced Metropolis-Hastings algorithm  may path to broaden the applicability of our methodology to a wider range of combinatorial optimization problems. Besides, our HeO allows for further customization by incorporating additional terms that integrate problem-specific prior knowledge or by hybridizing with other metaheuristic algorithms, allowing for more effective exploration of the configuration space. Second, our HeO can not be theoretically guaranteed for converging to the global minimum. In general, finding the global minimum is not theoretically guaranteed for non-convex optimization problems , such as the combinatorial optimization problems studied in this paper. However, it can be demonstrated that the gradient of the target function under heat diffusion satisfies the inequality :

\[|_{}u(,)| },\]

where the constant \(C\) depends on the dimension. This implies that the target function becomes weakly convex, enabling the finding of global minima and faster convergence under certain conditions .

## 6 Conclusion

In conclusion, grounded in the heat diffusion, we present a framework called heat diffusion optimization (HeO) to solve various combinatorial optimization problems. The heat diffusion facilitates the propagation of information from distant regions to the solver, expanding its receptive field, which in turn enhances its ability to search for global optima. Demonstrating exceptional performance across various scenarios, our HeO highlights the potential of utilizing heat diffusion to address challenges associated with navigating the solution space of combinatorial optimization.