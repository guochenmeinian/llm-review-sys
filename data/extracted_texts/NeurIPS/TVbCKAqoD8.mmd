# Trade-Offs of Diagonal Fisher Information Matrix Estimators

Alexander Soen

The Australian National University

RIKEN AIP

alexander.soen@anu.edu.au &Ke Sun

CSIRO's Data61

The Australian National University

Ke.Sun@data61.csiro.au

###### Abstract

The Fisher information matrix can be used to characterize the local geometry of the parameter space of neural networks. It elucidates insightful theories and useful tools to understand and optimize neural networks. Given its high computational cost, practitioners often use random estimators and evaluate only the diagonal entries. We examine two popular estimators whose accuracy and sample complexity depend on their associated variances. We derive bounds of the variances and instantiate them in neural networks for regression and classification. We navigate trade-offs for both estimators based on analytical and numerical studies. We find that the variance quantities depend on the non-linearity w.r.t. different parameter groups and should not be neglected when estimating the Fisher information.

## 1 Settings

In the parameter space of neural networks (NNs), _i.e_. the _neuromanifold_, the network weights and biases play the role of a coordinate system and the local metric tensor can be described by the Fisher Information Matrix (FIM). As a result, empirical estimation of the FIM helps reveal the geometry of the loss landscape and the intrinsic structure of the neuromanifold. Utilizing these insights has lead to efficient optimization algorithms, _e.g_., the natural gradient  and Adam .

A NN with inputs \(\) and stochastic outputs \(\) can be specified by a conditional p.d.f. \(p(\,|\,;)\), where \(\) is the NN's weights and biases. This paper considers the general parametric form

\[p(\,|\,;)=()(^{}( )_{}()-F(_{}())),\] (1)

where \(_{}^{I}^{T}\) maps \(I\)-dimensional inputs \(\) to \(T\)-dimensional exponential family parameters, \(()\) is a vector of sufficient statistics, \(()\) is a base measure, and \(F()\) is the log-partition function (normalizing the exponential). For example, if \(\) denotes class labels and \(()\) maps to its corresponding one-hot vectors, then Eq. (1) is associated with a multi-class classification network.

Assuming that the marginal distribution \(q()\) is parameter-free, we define parametric joint distributions \(p(,;)=q()p(\,|\,;)\). The (joint) FIM is defined as \(()=_{q()}[( \,|\,)]\), where

\[(\,|\,)}_{p(\,|\, ;)}[\,|\,;) }{}\,|\,;)}{ ^{}}]}{{=}}- {}_{p(\,|\,;)}[ p (\,|\,;)}{^{ }}]\] (2)

is the 'conditional FIM'. The second equality (*) holds if \(_{}\)'s activation functions are in \(C^{2}()\) (_i.e_., \(_{}\) is a sufficiently smooth NN). \((\,|\,)\) does _not_ have this equivalent expression (*) for NNs with ReLU activation functions . Both \(()\) and \((\,|\,)\) define \(()()\) positive semi-definite (PSD) matrices. The distinction in notation is to emphasize that the joint FIM \(()\) (depending only on \(\)) is simply the average over individual conditional FIMs \((\,|\,)\) (depending on both \(\) and \(\)).

In practice, the FIM is typically computationally expensive and needs to be estimated. Given \(q()\) and a NN with weights and biases \(\) parameterizing \(p(\,|\,;)\), as per Eq. (1), we consider two commonly used estimators of the FIM [11; 37] given by

\[}_{1}()_{k=1}^{N}[_{k}\,|\,_{k})}{}_{k}\,|\,_{k})}{^{}}]; }_{2}()_{k=1}^{ N}[- p(_{k}\,|\,_{k})}{^{}}],\] (3)

where \(p(_{k}\,|\,_{k}) p(_{k}\,|\,_{k};)\) and \((_{1},_{1}),,(_{N},_{N})\) are i.i.d. sampled from \(p(,;)\). A conditional variant of the estimators, denoted as \(}_{1}(\,|\,)\) and \(}_{2}(\,|\,)\), can be defined by fixing \(=_{1}==_{N}\) and sampling \(_{1},,_{N}\) independently from \(p(\,|\,)\) in Eq. (3) -- details omitted for brevity.

Both estimators, \(}_{1}()\) and \(}_{2}()\), are random matrices with the same shape as \(()\). By Eq. (2), they are _unbiased_ -- for \(}_{2}()\), this only holds if activations functions are in \(C^{2}()\). Following Eq. (1)'s setting, the estimation variances of \(}_{1}()\) and \(}_{2}()\) can be expressed in closed form and upper bounded . This provides an important, yet not widely discussed, tool for quantifying the estimators' accuracy  and hence insights for where / when different estimators should be used. Despite this, for deep NNs, neither these variances nor their bounds can be computed efficiently due to the huge dimensionality of \(\).

This work focuses on estimating the _diagonal entries_ of the FIM and their associated variances. Our results -- including estimators of the FIM, their variances, and their variance bounds -- can be implemented through automatic differentiation. These computational tools empower us to practically explore the trade-offs between the two estimators. For example, Fig. 1 shows natural gradient descent  for generalized linear models on a toy dataset, where \(}_{2}()\) is preferable (especially for regression) and \(}_{1}()\) suffers from high variance and an unstable learning curve. Our analytical results reveal how moments of the output exponential family and gradients of the NN in Eq. (1) affects the FIM estimators. We discover a general decomposition of the estimators' variances corresponding to the samples of \(\) and \(\). We investigate different scenarios where each FIM estimator is the preferred one and then connect our analysis to the empirical FIM.

## 2 Related Work

Prior efforts aim to analyze the structure of the FIM of NNs with random weights [34; 14; 15; 3; 31]. This body of work hinges on utilizing tools from random matrix theory and spectral analysis, characterizing the behavior and statistics of the FIM. One insight is that randomly weighted NNs have FIMs with a majority of eigenvalues close to zero; with the other eigenvalues taking large values [14; 15]. In our work, the randomness stems from sampling from data distributions \(p(,)\) -- which follows the principle of Monte Carlo (MC) information geometry  that approximates information geometric quantities via MC estimation. We examine a different subject on how the distribution of the FIM on a matrix manifold is affected by finite sampling of the data distribution.

In the literature of NN optimization, a main focus is on deriving a computationally friendly proxy for the FIM. One can consider the _unit-wise_ FIM [30; 20; 39; 3] (also known as quasi-diagonal FIM ), where a block-diagonal approximation of the FIM is taken to capture intra-neuron curvature information. Or one can consider the block-diagonal _layer-wise_ FIM where each block corresponds to parameters within a layer [19; 27; 32; 26; 12; 35; 13]. NN optimizers can approximate the inverse FIM  or approximate the product of the inverse FIM and the gradient vector .

Much less attention is paid to how related approximations deviate from the true FIM [11; 37] or how optimization is affected by such deviation . For the univariate case, one can study the asymptotic variance of the Fisher information  with the central limit theorem. In deep NNs, the estimation variance of the FIM can be derived in closed form and bounded . However, our former analysis  has two limitations: (1) the variance tensors are 4D and can not be easily computed; (2) only the norm of these tensors are bounded, and it is not clear how the variance is distributed among individual parameters. The current work tackles these limitations by focusing on the diagonal elements of the FIM. Our results can be computed numerically at a reasonable cost in typical learning settings. We provide novel bounds so that one can quantify the accuracy of the FIM computation w.r.t. individual parameters or subgroup of parameters.

Issues of utilizing the empirical FIM to approximate the FIM have been highlighted [32; 25]. For example, estimators of the FIM do not in general capture any second-order information about the log-likelihood . The empirical FIM is a biased estimator and can be connected with our unbiased estimators via a generalized definition of the Fisher matrix in Section 6.

Alternative to the FIM, the Generalized Gauss-Newton (GGN) matrix -- a Hessian approximator -- was originally motivated through the squared loss for non-linear models . The GGN is equivalent to the FIM when a loss function is taken to be the empirical expectation of the negative log-likelihood of Eq. (1) [12; 32; 25].

## 3 Variance of Diagonal FIM Estimators

In our notations, all vectors such as \(\), \(\), and \(\) are column vectors. We use \(k\) to index random samples \(\) and \(\) and use \(i\) and \(j\) to index the NN weights and biases \(\). We shorthand \(_{}\), \(p(\,|\,) p(\,|\,;)\), and \(p(,) p(,;)\) whenever the parameters \(\) is clear from context. To be consistent, we use \({}^{*}\,|\,\) conditioning' to distinguish between jointly calculated values versus conditioned values with fixed \(\). By default, the derivatives are w.r.t. \(\). For example, \(_{i}/_{i}\) and \(_{i}^{2}^{2}/_{i}^{2}\). We adopt Einstein notation to express tensor summations, so that an index appearing as both a subscript and a superscript in the same term indicates a summation. For example, \(x^{a}y_{a}\) denotes \(_{a}x^{a}y_{a}\). For clarity, we mix standard \(\)-sum and Einstein notation. We denote the variance and covariance of random variables by \(()\) and \(()\), respectively.

Based on the parametric form of the model in Eq. (1), the diagonal entries of the FIM estimators in Eq. (3) can be written as1:

\[}_{1}(_{i}) (}_{1}())_{ii}= {1}{N}_{k=1}^{N}((_{k}))}{ _{i}}-^{a}(_{k})}{_{i}} _{a}(_{k}))^{2};\] \[}_{2}(_{i}) (}_{2}())_{ii}= {1}{N}_{k=1}^{N}(F((_{k}))}{^ {2}_{i}}-^{a}(_{k})}{^{2}_ {i}}_{a}(_{k})).\]

Correspondingly, the \(i\)'th diagonal entry of the FIM \(()\), which is the expected value of \(}_{1}(_{i})\) and \(}_{2}(_{i})\), is denoted as \((_{i})\). Notation is abused in \((_{i})\), \(}_{1}(_{i})\), and \(}_{2}(_{i})\) as they depend on the whole \(\) vector rather than solely on \(_{i}\). Clearly \(}_{1}(_{i}) 0\), while there is no guarantee for \(}_{2}(_{i})\) which can be negative. Our results will be expressed in terms of the (central) moments of \(()\):

\[_{a}()}_{p(\,|\,)}[_ {a}()]; (\,|\,)}_{p(\,|\,)}[ (()-())(()-())^{}];\]

\[^{p}(\,|\,)}_{p(\,|\,)}[(()-())(()-( ))(()-())(()-( {x}))]\,,\]

where "\(\)" denotes the tensor product. We denote the covariance of \(\) w.r.t. to \(p(\,|\,)\) as \(^{p}(\,|\,)\) -- noting that \((\,|\,)=^{p}(\,|\,)\). The 4D tensor \(^{p}(\,|\,)\) denotes the \(4^{}\) central moment of \(()\) w.r.t. \(p(\,|\,)\). These central moments correspond to the cumulants of \(()\), _i.e_. the derivatives of \(F\) w.r.t. the natural parameters \(()\) of the exponential family. Therefore, the derivatives of \(F\) in \(}_{1}(_{i})\) and \(}_{2}(_{i})\) can further be written in terms of \(()\) and \((\,|\,)\) following the chain rule. Practically, \(}_{1}\) and \(}_{2}\) involves computing the Jacobian \(()/_{i}\) and the Hessian \(^{2}()/^{2}_{i}\), respectively.

[MISSING_PAGE_FAIL:4]

## 4 Practical Variance Estimation

To further understand the dependencies of the derivative and central moment terms, the FIM \((_{i}\,|\,)\) and variances of estimators \(}_{j}(_{i}\,|\,)\) can be bounded to strengthen intuition and to provide a computationally convenient proxy of the interested quantities.

**Theorem 4.1**.: \(^{I}\)_,_

\[\|_{i}()\|_{2}^{2}_{}( (\,|\,))(_{i}\,| \,)\ \|_{i}()\|_{2}^{2} _{}((\,|\,)),\] (7)

\[\|_{i}()\|_{2}^{4} _{}()_{1}(_{i} \,|\,)\|_{i}( )\|_{2}^{4}_{}(),\] (8)

\[\|_{i}^{2}()\|_{2}^{2} _{}((\,|\,)) _{2}(_{i}\,|\,)\| _{i}^{2}()\|_{2}^{2}_{}((\,|\,)),\] (9)

_where \(=^{p}(\,|\,)-( \,|\,)(\,|\, )\); \(_{}\,/\,_{}\) denotes the minimum / maximum matrix eigenvalue; and \(_{},_{}^{T T T  T}\) are defined as_

\[_{}()_{\| \|_{2}=1}^{}^{ }^{}^{} _{abcd};_{}() _{:\|\|_{2}=1}^{ }^{}^{ }^{}_{abcd}.\] (10)

To help ground Theorem 4.1, we summarize different sufficient statistics quantities for common learning settings in Table 1 -- with further learning setting implications presented in Section 5. Note that Eqs. (8) and (9) (and many subsequent results) can be further generalized for off-diagonal elements. See Appendix C for details. Compared to prior work , Theorem 4.1 provides bounds for individual elements of the variance tensors, where the NN weights (the derivatives) and sufficient statistics (the eigenvalues) are neatly disentangled into a product. From a technical point of view, this comes from a difference in proof technique: we utilize variational definitions and computations of eigenvalues to establish bounds whereas  primarily applies Holder's inequality.

We stress that \(_{}()\) and \(_{}()\) in Eq. (10) correspond to tensor eigenvalues iff \(\) is a supersymmetric tensor  (a.k.a. totally symmetric tensor), _i.e._, indices are permutation invariant. In this case, Eq. (10) is exactly the maximum and minimum Z-eigenvalues. These variational forms mirror the Courant-Fischer min-max theorem for symmetric matrices . In the case of Eq. (8), with \(=^{p}(\,|\,)-( \,|\,)(\,|\, )\), the tensor is not a supersymmetric tensor in general. Despite this, we note that the lower bound of Eq. (8) is non-trivial. A weaker bound than Eq. (8) can be established based on the Z-eigenvalue of the supersymmetric tensor \(^{p}(\,|\,)\).

**Corollary 4.2**.: \(^{I}\)_,_

\[_{}(^{p}(\,|\, )-(\,|\,) (\,|\,))\{0,_{}(^{p}(\,|\,) )-_{}^{2}((\,|\,))\};\] (11) \[_{}(^{p}(\,|\, )-(\,|\,) (\,|\,))_{ }(^{p}(\,|\,))-_{ }^{2}((\,|\,)).\] (12)

The tensor eigenvalue is typically expensive to calculate. However in our case, the eigenvalues \(_{}(^{p}(\,|\,))\) and \(_{}(^{p}(\,|\,))\) on the RHS of Eqs. (11) and (12) can be calculated via 's method with \((T^{4}/4!)\) complexity. In this paper, we assume \(T\) is reasonably bounded and are mainly concerned with the complexity w.r.t. \(()\). From this perspective, all our bounds scale linearly w.r.t. \(()\), and thus can be computed efficiently.

When \(()-()\) is bounded (_e.g._ in classification), we can upper bound \(_{}(^{p}(\,|\,))\) with \(_{}((\,|\,))\), which is easier to calculate.

**Proposition 4.3**.: _Suppose \(\|()-()\|_{2}^{2} B\). Then, \(_{}(^{p}(\,|\,) ) B_{}((\,|\,) ) B^{2}\)._

As long as the sufficient statistics \(()\) has bounded norm \(\|\|_{2}\), we have that \(\|()-()\|_{2}^{2} 4\| \|_{2}^{2}<\). A similar lower bound can be established for the minimum tensor eigenvalue \(_{}(^{p}(\,|\,) )_{}^{2}((\,|\,))\), but this ends up being trivial when applying Corollary 4.2's lower bound, Eq. (11).

Examining Theorem 4.1 reveals several trade-offs. An immediate observation is that the first order gradients of \(()\) correspond to the robustness of \(\) to parameter misspecification (w.r.t. an input\(\)). As such, from the bounds in Eqs. (7) and (8), the scale of \((_{i})\) and \(_{1}(_{i})\) will be large when small shifts in parameter space yield large changes in the output \(()\). Another observation is how the spectrum of \(()\) affects the scale of \((_{i})\) and the estimator variances. In particular, when \(_{}(())\) increases, the scale of \(_{1}(_{i})\) decreases but the scale of \((_{i})\) and \(_{2}(_{i})\) increases. When \(_{}(())\) decreases, then the opposite scaling occurs. With these two observations, there is a tension in how the scale of \((_{i})\) follows the different variances \(_{1}(_{i})\) and \(_{2}(_{i})\). The element-wise FIM \((_{i})\) follows \(_{1}(_{i})\) in terms of the scale of NN derivatives \(\|_{i}()\|_{2}\); at the same time, \((_{i})\) follows \(_{2}(_{i})\) in terms of the spectrum of sufficient statistics moment \(()\).

**Remark 4.4**.: Typically, \(\) is the linear output units: \(()=_{-1}_{-1}()\), where \(_{-1}\) is the weights of the last layer, and \(_{-1}()\) is the second last layer's output. We have \(_{2}(_{i})=0_{1}(_{i})\) for any \(_{i}\) in \(_{-1}\). A smaller variance \(_{2}(_{i})\) is guaranteed for the last layer regardless of the choice of the exponential family in Eq. (1).

**Remark 4.5**.: \(()=_{-1}^{j}(_{-2}^{}()_{-2}^{j}+ C_{-2})+_{-1}\) defines the NN mapping w.r.t. the \(j\)'th neuron in the second last layer, where \(_{-2}^{j}\) and \(_{-1}^{j}\) are incoming and outgoing links of the interested neuron, respectively; \(_{-2}()\) is the output of the third last layer; and \(\) is the activation function. The 'constants' \(C_{-2}\) and \(_{-1}\) denote an aggregation of all terms which are independent of \(_{-2}^{j}\) and \(_{-1}^{j}\) in their respective layers. The Hessian of \(_{k}()\) w.r.t. \(_{-2}^{j}\) is \(^{2}_{k}()=(_{-1}^{j})_{k}^{}( _{-2}^{}()_{-2}^{j}+C_{-2})(_{-2}()_{-2}^{}())\). By Theorem 4.1, \(_{2}(_{i})\) can be arbitrarily small depending on \(^{}(_{-2}^{}()_{-2}^{j}+C_{-2})\). For example, if \((t)=1/(1+(-t))\), then \(^{}(t)=(t)(1-(t))(1-2(t))\). In this case, for a neuron in the second last layer, a sufficient condition for \(_{2}(_{i})=0\) (and having \(}_{2}\) favored against \(}_{1}\)) is \(_{-2}^{}()_{-2}^{j}+C_{-2}=0\) for the neuron's pre-activation. When the pre-activation value is saturated (\(-\) or \(\)), we also have that \(_{1}(_{i})=_{2}(_{i})=0\). Alternatively, suppose that \((t)=(t)=(1+(t))\), a continuous relaxation of \(\), then \(^{}(t)=^{}(t)(1-^{}(t))\) where \(^{}(t)=1/(1+(-t))\). Then a sufficient condition for \(_{2}(_{i})=0\) with \(_{1}(_{i}) 0\) for a neuron in the second last layer is \(_{-2}^{}()_{-2}^{j}+C_{-2}+\).

These observations are further clarified by looking at related quantities over multiple parameters. So far we have only examined the variance of the FIM element-wise w.r.t. parameters \(_{i}\). To study all parameters \(\) jointly, we consider the _trace variances_ of the FIM estimators: for any \(j\{1,2\}\), \(_{j}()\) denotes the trace of the covariance matrix of \((}_{j}())\), where \(()\) extracts a matrix's diagonal elements into a column vector. We present upper bounds of these joint quantities.

**Corollary 4.6**.: _For any \(^{I}\),_

\[(()) \|()\|_{F}\{( ()),\|()\|_{F} _{}(())\};\] (13) \[_{1}() \|()\|_{F}^{2} \{_{t,u=1}^{T}_{ttuu}^{p}()-\|( )\|_{F}^{2},\|()\|_{F}^{2}_{}()\};\] (14) \[_{2}() \|()\|_{F }\{(()),\| ()\|_{F}_{}(( ))\},\] (15)

_where \(()=(((_{1} )),,((_{T})))\) and \(\|\|_{F}\) is the Frobenius norm._

This upper bound comes from integrating the parameter-wise variances in Theorem 4.1 and incorporating a trace variance bound which utilizes the full spectrum of the NN derivatives and sufficient statistics quantities. This is fully depicted in Theorem D.1. Lower bounds can also be derived in terms of singular values (deferred to the Appendix). Note the upper bounds in Corollary 4.6 can be improved by expressing the \(\) function's first term with singular value quantities.

Having the \(\) function in Corollary 4.6 is helpful as it shows a trade-off between two upper bounds: the scale of NN derivatives \(()\) and \(^{2}()\) versus the spectrum of the sufficient statistic terms. In the case of Eqs. (13) and (15), the trace of \(()\) is exactly the sum of all eigenvalues, including \(_{}(())\). This can be helpful when the scale of the NN derivatives are not bounded by a small value. It should be noted that, by the chain rule, these NN derivatives scale with the overall sharpness / flatness  of the landscape of the loss, _i.e._, the log-likelihood of Eq. (1). For NNs with large derivatives, the first term of the \(\) could yield tight bounds of the variance, and one can therefore avoid dealing with the quadratic scaling of \(\|()\|\) in the second term. On the other hand, if the sharpness of the NN \(\) can be controlled, _e.g._ via sharpness aware minimization , then one can benefit from the second term of the \(\) and avoid computing the full spectrum of \((\,|\,)\) in the first term.

Joint FIM EstimatorsIn the above, we considered the variance of conditional FIMs, which can scale differently depending on the input \(\). Prior work's analysis was limited to that of conditional FIMs (and their estimators) . Nevertheless, the 'joint FIM' estimators \(}_{j}(_{i})\) depend on sampling of \(\) w.r.t. the data distribution \(q()\). The bounds in Eq. (7) can be extended to the joint FIM \((_{i})_{q()}\,( _{i}\,|\,)\) by simply taking an expectation \(_{q()}\) over the bounds. To analyze the variances of the joint FIM estimators \(_{j}(_{i})\), we present the following theorem which connects the prior results established for \(_{j}(_{i}\,|\,)\), _e.g._ Theorem 4.1, via the law of total variance.

**Theorem 4.7**.: _For any \(j\{1,2\}\), given \(N_{x}\) samples of \( q()\) and \(N\) samples of \(\,|\, p(\,|\,)\) for each \(\) sampled,_

\[_{j}(_{i})=}( (_{i}\,|\,))+} *{}_{q()}[_{j}(_{i }\,|\,)],\] (16)

_where \(((_{i}\,|\,))\) is the variance of \((_{i}\,|\,)\) w.r.t. \(q()\)._

The dependence on \(N\), the number of samples of \(\) for each fixed \(\), is hidden in \(_{j}(_{i}\,|\,)\). When \(N=1\), the hierarchical sampling described in the Theorem corresponds to an i.i.d. sampling of the joint distribution \(p(,)\).

The variance incurred when estimating the FIM has two components. The first term on the RHS of Eq. (16) characterizes the randomness of the FIM w.r.t. \(q()\), _i.e._, the input randomness. It vanishes when the FIM is estimated by taking the expectation w.r.t. \(q()\), or the number of samples \(\) is large enough. The second term (although also depending on \(N_{x}\)) comes from the sampling of \(\,|\,\) according to \(p(\,|\,)\), _i.e._, the output randomness, which scales with the central moments of \(()\). If the NN is trained so that \(p(\,|\,)\) tends to be deterministic, this term will disappear leaving the first term to dominate. Eq. (16) can be further generalized using the law of total covariance to extend prior work considering conditional FIM covariances  to joint FIM covariances. Theorem 4.7 connects the variance of assuming a fixed input \(\) with multiple samples \(_{k}\) with the variance of pairs of samples \((_{k},_{k})\). The \(_{j}(_{i}\,|\,)\) bounds in this section can thus be applied to the corresponding joint variance \(_{j}(_{i})\) by using this theorem. This is straightforward and omitted.

The first variance term in Theorem 4.7 is difficult to compute in practice: it relies on how the closed-form FIM varies w.r.t. \(q()\). As such, it is useful to bound the first term into computable quantities.

**Lemma 4.8**.: \(((_{i}\,|\,)) _{q()}[\|_{i}()\|_{2}^{4}_{}^{2}((\,|\, ))].\)__

This upper bound is very similar to the 4th central moment term \(_{}(^{p}(\,|\,))\) when considering the variance upper bound \(_{2}(_{i}\,|\,)\) in Theorem 4.1 and Corollary 4.2. In general, the eigenvalue terms of Lemma 4.8 and Theorem 4.1 are distinct, _i.e._, \(_{}^{2}((\,|\,)) {}_{}(^{p}(\,|\,))\). This is especially true for the classification and regression problems explored in this paper (see Table 1). However, the maximum eigenvalues can be related for exponential families with bounded sufficient statistic via Proposition 4.3, making both bounds depend only on \(_{}((\,|\,))\).

The total number of samples of \((_{k},_{k})\) is \(N_{x} N\). In terms of sample complexity of the joint variance, using Theorem 4.7 and Lemma 4.8, the bound's rate is given by \((1/N_{x}+1/(N_{x} N))\).

## 5 Case Studies

To make our theoretic results more concrete, we consider regression and classification settings, which correspond to specifying the exponential family in Eq. (1) to an isotropic Gaussian distribution and a categorical distribution, respectively. We include an empirical analysis of NNs trained on MNIST. Notably, our analysis considers general multi-dimensional NN output. This extends the case studies of  which was limited to 1D distributions due to the limitations of their bounds (and their associated computational costs of dealing with a 4D tensor of the full covariance).

Regression: Isotropic Gaussian DistributionTo characterize regression, we consider Gaussian distributions. As per Eq. (1), we have \(()=^{T}\) and base measure \(()(-^{})\)This corresponds to the case where \(()\) is trained via the squared loss. In this case, \((\,|\,)=\) and \(^{p}_{abcd}(\,|\,)=_{ab}(\,|\,) _{cd}(\,|\,)+_{ac}(\,|\,) _{bd}(\,|\,)+_{ad}(\,|\,) _{bc}(\,|\,)\). The derivatives of the log-partition function \(F()\) yields these central moments [37, Lemma 5]. To apply Theorem 4.1, we examine the extreme eigenvalues of \((\,|\,)\) and \(^{p}(\,|\,)-(\,|\,) (\,|\,)\).

**Proposition 5.1**.: _Suppose that Eq. (1) is an isotropic Gaussian distribution. Then:_

\[_{}((\,|\,)) =_{}((\,|\,))=1;\] \[_{}(^{p}(\,|\,)- (\,|\,)(\,|\,)) =_{}(^{p}(\,|\,)- (\,|\,)(\,|\,))=2.\]

Hence, for regression the eigenvalues of sufficient statistics quantities in our bounds are equal. As such, in this case, the bound for \((_{i}\,|\,)\), \(_{1}(_{i}\,|\,)\), and \(_{2}(_{i}\,|\,)\) in Theorem 4.1 are all equalities.

As \((_{i}\,|\,)\) can be written exactly in terms of the gradients of \(\), in practice one does not need to utilize a random estimator when computing the conditional FIM, which simplifies to a Gauss-Newton matrix for the squared loss . When computing the FIM over a random sample \(\), a variance still appears due to Theorem 4.7. By Lemma 4.8 and Proposition 5.1, the variance over a joint distribution is bounded by a function of the derivative \(_{i}()\) over the marginal input distributions: \((_{i})_{q()}[\|_{i}( )\|_{2}^{4}]_{(q)}\|_{i}() \|_{2}^{4}\). In other words, the overall variance to approximate the joint FIM is bounded by the gradients of the NN outputs.

Classification: Categorical DistributionFor multi-class classification, we instantiate our exponential family with a categorical distribution (with \((y)=1\)). This corresponds to training a classifier NN with the log-loss. Let \(=[C]\) for \(T=C\) classes defining the possible labels. Let \((y)=( y=1, y=C)\), where \( r=1\) when the predicate \(r\) is true and \( r=0\) otherwise. Noting our results do not depend on minimal sufficiency, this \((y)\) is sufficient but _not minimal_ sufficient. In this setting, the NN outputs \(\) correspond to the logits of the label probabilities. The resulting probabilities \(p(y\,|\,)\) are the softmax values of \(\) denoted by \(()=(())^{T}\).

Under this setting, we have \((\,|\,)=(())-() ()^{}\) (where \((())\) is the diagonal matrix with its diagonal entries set to \(()\)), whose eigenvalues do not follow a convenient pattern as \(C\) increases . Likewise, the maximum eigenvalue of \(^{p}(\,|\,)\) is not available in simple closed form. As such, we provide upper bounds for the maximum eigenvalues of \((\,|\,)\) and \(^{p}(\,|\,)-(\,|\,) (\,|\,)\) using Corollary 4.2 and Proposition 4.3.

**Theorem 5.2**.: _Suppose that Eq. (1) is a categorical distribution. With \(_{}()_{k}_{k}()\):_

\[_{}((\,|\,)) m(); _{}(^{p}(\,|\,)-( \,|\,)(\,|\,)) 2 m(),\]

_where \(m()_{}(),1-\|()\|_{2}^{2} \)._

This upper bounds provides a tension. When the first term of \(m()\) is maximized, the second is minimized, and vice-versa. In particular, the dominating term depends on the uncertainty of the NN's output. When the NN's output is near random, _e.g._ at initialization, the first term will dominate with \(_{}() 1/C\). However, as the NN becomes more certain with its prediction, the second term will start dominating: a more deterministic output \(p(y\,|\,) 1\) implies that \(_{}((\,|\,)) 0\).

Empirical Verification: ClassificationWe examine the MNIST classification task  (CC BY-SA 3.0) using multilayer perceptrons (MLP) with four densely connected layers, sigmoid activations, and a dropout layer. For classification, we consider a categorical distribution with \(C=10\) class labels. For a random \(\) from the test set, we compute both estimators \(}_{1}(_{i}\,|\,)\) and \(}_{2}(_{i}\,|\,)\) using \(N=5,000\) samples. We record the variances of each estimator and compute their bounds based on Theorem 4.1. For all 20 training epochs, the Fisher information (FI) and their variances of individual parameters are aggregated via arithmetic averages over four parameter groups (corresponding to the four layers).

In Fig. 2, we present the variance scale of the estimators \(}_{1}(_{i}\,|\,)\) and \(}_{2}(_{i}\,|\,)\) in log-space; and the tightness of the bounds in Theorem 4.1 by consider the log-ratio \(}{_{1}(_{i}\,|\,)}\), where UB is the upper bounds in Theorem 4.1. In this experiment, the UB is much tighter than the lower bound (LB), which is omitted in the figures for clarity. More experimental results are given in Appendix F.

We varied the NN's architecture and activation function. Across different settings, the proposed UB and LB are always valid. In Fig. 2, one can observe that the diagonal FIM and the associated variances have a small magnitude. For example, in the first layer, \(_{1}(_{i}\,|\,)\) and \(_{2}(_{i}\,|\,)\) are roughly \(e^{-10} 5 10^{-5}\). The log-ratio \(}{_{1}(_{i}\,|\,)} 4\) means that the UB is roughly 50 times larger than \(_{1}(_{i}\,|\,)\). Comparatively, \(_{2}(_{i}\,|\,)\) has a tighter UB which is approximately 10 times larger than itself. The UB serves as a useful hint on the _order of magnitude_ of the variances. In Appendix D, we present tighter bounds which are more expensive to compute.

In the first three layers of the MLP, \(_{1}(_{i}\,|\,)\) presents a smaller value than \(_{2}(_{i}\,|\,)\), meaning that \(}_{1}\) can more accurately estimate the diagonal FIM. Interestingly, this is not true for the last layer: \(_{2}(_{i}\,|\,)\) becomes zero while \(}_{1}\) presents the largest variance across all parameter groups. Due to this, one should always prefer \(}_{2}\) over \(}_{1}\) for the last layer. In the last two layers, \(}_{2}\) is in simple closed form and, hence, does not need automatic differentiation to calculate (see Remarks 4.4 and 4.5). The shape of the variance curves are sensitive to the choices of activation functions \(\) and inputs \(\). In general, the variance in the first few epochs presents more dynamics than the rest of the training process. If one uses log-sigmoid activations \((t)=-(1+(-t))\) (which is equivalent to \((t)=-(-t)\), as per Remark 4.4), the variances of \(}_{1}\) and \(}_{2}\) only appear in the randomly initialized NN and quickly vanish once training starts, as shown in Appendix F. In this case, the learner more easily approaches a nearly linear region of the loss landscape where local optima lie. In practice, one should estimate and examine the scale of variances -- which should not be neglected as per Fig. 2 -- before choosing a preferred diagonal FIM estimator.

## 6 Relationship with the "Empirical Fisher"

In some scenarios, even the estimators of the diagonal FIM \(}_{1}()\) and \(}_{2}()\) can be prohibitively expensive. Part of the cost comes from requiring label samples \(_{k}\) for each \(_{k}\), as per Eq. (3). For example, when the FIM is used in an iterative optimization procedure, \(_{k}\)'s need to be re-sampled at each learning step w.r.t. the current \(\) alongside their backpropagation (accounting for sampling).

As such, alternative 'FIM-like' objects have been explored which replace the samples from \(p(\,|\,)\) with samples from an underlying true (but unknown) data distribution \(q(\,|\,)\). We define the data's joint distribution as \(q(,) q()q(\,|\,)\). Analogous to the FIM, the _data Fisher information matrix_ (DFIM) can be defined as the PSD tensor \(()=_{q()}[(\,|\, {x})]\), with

\[(\,|\,)=}_{q(}\,|\, {x})}[}\,|\,)}{} }\,|\,)}{^{}} ]=(}{})^{}(\,|\,)(}{}),\] (17)

where \((\,|\,)=_{q(}\,|\,)}[( (})-())((})-( ))^{}]\) denotes the 2nd (non-central) moment of \(((})-())\) w.r.t. \(q(}\,|\,)\), and \(/\) is the Jacobian of the map \(\). In the special case that \(q(\,|\,)=p(\,|\,;)\), then \((\,|\,)\) becomes exactly \((\,|\,)\).

The DFIM \((\,|\,)\) in Eq. (17) is a more general definition. Compared to the FIM \((\,|\,)\), it yields a different PSD tensor on the \(\) parameter space (the neuromanifold) depending on a dis

Figure 2: MNIST for a 4-layer MLP with sigmoid activations. Top: The estimated Fisher information (FI), variances, and variance bounds across 4 parameter groups and 20 training epochs. The FI (green line) is estimated using \(}_{1}\) (\(}_{2}\) is almost identical and not shown for clarity). The s.t.d. (square root of variance) is shown for variances and their bounds. Bottom: the log-ratio of Theorem 4.1’s upper bounds (UBs) and the true variances. The closer to 0, the better the UB. In the right most column, the variance of \(}_{2}\) vanishes: \(_{2}(_{i}\,|\,)=0_{1}(_{i}\,|\, )\). Thus related curves of \(}_{2}\) are not shown.

tribution \(q(,)\), which is neither necessarily on the same neuromanifold nor necessarily parametric at all. The asymmetry in the true data distribution and the empirical one results in different geometric structures . By definition, we have \((\,|\,)(/)(/)^{}\), where \(() q(}\,|\,)}\,|\,)}{p(}\,|\,;)}\,}\) is the Kullback-Leibler (KL) divergence, or the loss in a parameter learning scenario. The DFIM can be regarded as a surrogate function of the squared gradient of the KL divergence. It is a symmetric covariant tensor and satisfies the same rule w.r.t. reparameterization as the FIM. Consider the reparameterization \(\), the DFIM becomes \((\,|\,)=(/)^{}(\,|\,)(/)\).

Notice that \(}()_{q(}\,|\,)}[( })]()\) in general. As such, there will be a miss-match when utilizing \((\,|\,)\) as a substitute for \((\,|\,)\). However, as learning progresses and \(p(}\,|\,)\) becomes more similar to the data's true labeling posterior \(q(}\,|\,)\), the DFIM will become closer to the FIM.

If \(q(,)=_{k=1}^{N}(-_{k}) (-_{k})\) is defined by the observed samples, DFIM gives the widely used "_Empirical Fisher_" , whose diagonal entries are

\[}(_{i})=_{k=1}^{N}(_{i} ^{a}(_{k})(_{a}(}_{k})-_{a}(_{k})) )^{2},\]

where \((_{1},}_{1}),,(_{N},}_{N})\) are i.i.d. sampled from \(q(,})\). Similar to \(}_{1}(_{i}\,|\,)\), an estimator with a fixed input \(\) can be considered, denoted as \(}(_{i}\,|\,)\).

Given the computational benefits of using the data directly -- bypassing a separate sampling routine -- many popular optimization methods employ the empirical Fisher or its approximation. For instance, the Adam optimizer  uses the empirical Fisher to approximate the diagonal FIM. However, switching from sampling \(_{k}\) to \(}_{k}\) is anything but superficial [25, Chapter 11] -- \(}()\) is _not_ an unbiased estimator of \(()\) as \((\,|\,)\) is different from \((\,|\,)\).

The biased nature of the empirical Fisher affects the other moments as well. In particular, we do not have the same equivalence of covariance and the metric being pulled back by \(\).

**Lemma 6.1**.: _Given the conditional data distribution \(q(}\,|\,)\), the covariance of \(\) given \(\) is given by_

\[^{q}(\,|\,)=(\,|\,)-(),\] (18)

_where \(()=(()-}())( ()-}())^{}\)._

As a result, although the variance of the estimator \(}(_{i}\,|\,)\) takes a similar form to \(_{1}(_{i}\,|\,)\) (_i.e._, Eq. (8)), its sufficient statistic terms do not exclusively consist of central moments. Noting the miss-match in \(}()()\), Lemma 6.1 reveals an additional term which shifts \((\,|\,)\) away from the 2nd central moment of \((})\) (w.r.t. \(q(}\,|\,)\)). Instead, these sufficient statistic terms correspond to non-central moments of \((})-()\). Some corresponding empirical Fisher / DFIM bounds are characterized in Appendix G.

## 7 Conclusion

We have analyzed two different estimators \(}_{1}()\) and \(}_{2}()\) for the diagonal entries of the FIM. The variances of these estimators are determined by both the non-linearly of the neural network and the moments of the exponential family. We have identified distinct scenarios on which estimator is preferable. For example, ReLU networks can only apply \(}_{1}()\) due to a lack of smoothness. As another example, \(}_{2}()\) has zero variance in the last layer and thus is always preferable than \(}_{1}()\). Similarly, in the second last layer, \(}_{2}()\) has a simple closed form and potentially preferable for neurons in their linear regions (see Remark 4.5). In general, one has to apply Theorem 4.1 based on their specific neural network and settings and choose the estimator with the smaller variance. Our results suggest that, from a variance perspective, uniformly utilizing one of the FIM estimators \(}_{j}()\) is often suboptimal in NNs. Our work has further extended from analyzing the conditional FIM estimators \(}_{j}(\,|\,)\) to the joint FIM estimators \(}_{j}()\); and we have examined the relationship between the investigated estimators and the empirical Fisher. Future directions include extending the analysis of the variance of FIM estimators to block diagonals (_e.g._[26; 35]) and adapting current NN optimizers (_e.g._) to incorporate the variance of FIM estimators.