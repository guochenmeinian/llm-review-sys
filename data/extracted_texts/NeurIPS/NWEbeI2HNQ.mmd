# Prefix-Tree Decoding for Predicting Mass Spectra from Molecules

Samuel Goldman

Computational and Systems Biology

MIT

Cambridge, MA 02139

samlg@mit.edu

John Bradshaw

Chemical Engineering

MIT

Cambridge, MA 02139

jbrad@mit.edu

Jiayi Xin

Statistics and Actuarial Science

The University of Hong Kong

Pokfulam, Hong Kong

xinjiayi@connect.hku.hk

Connor W. Coley

Chemical Engineering

Electrical Engineering and Computer Science

MIT

Cambridge, MA 02139

ccoley@mit.edu

###### Abstract

Computational predictions of mass spectra from molecules have enabled the discovery of clinically relevant metabolites. However, such predictive tools are still limited as they occupy one of two extremes, either operating (a) by fragmenting molecules combinatorially with overly rigid constraints on potential rearrangements and poor time complexity or (b) by decoding lossy and nonphysical discretized spectra vectors. In this work, we use a new intermediate strategy for predicting mass spectra from molecules by treating mass spectra as sets of molecular formulae, which are themselves multisets of atoms. After first encoding an input molecular graph, we decode a set of molecular subformulae, each of which specify a predicted peak in the mass spectrum, the intensities of which are predicted by a second model. Our key insight is to overcome the combinatorial possibilities for molecular subformulae by decoding the formula set using a prefix tree structure, atom-type by atom-type, representing a general method for ordered multiset decoding. We show promising empirical results on mass spectra prediction tasks.

## 1 Introduction

As the primary tool to discover unknown small molecule structures from biological samples, tandem mass spectrometry (MS/MS) experiments have enabled the identification of numerous important molecules implicated in health and disease . Tandem mass spectrometers are capable of isolating, fragmenting, and measuring the resulting fragment masses of small molecules from a sample, producing a signature (a mass spectrum) for each detected molecule (Figure 1, top).

Figure 1: Tandem mass spectrometers measure fragmentation patterns of molecules, resulting in characteristic peaks that are indicative of their structure. SCARF simulates these fragmentation patterns _in silico_.

Computationally predicting mass spectra from molecules _in silico_ (Figure 1, bottom) is thus a longstanding and important challenge. Not only does this assist practitioners in better understanding the fragmentation process, but it also enables the identification of molecules from newly observed spectra by comparing an observed spectrum to virtual spectra generated from a database of candidate molecules. While a large library of empirical mass spectra could theoretically serve the same purpose, the size of such libraries is limited by the slow and expensive process of acquiring pure chemical standards and measuring their spectra, motivating computational prediction.

We argue that there are three core, interrelated desiderata for a forward molecule-to-spectrum simulation model, or "spectrum predictor". An ideal spectrum predictor should be (i) _accurate_, being able to predict the exact set of fragment masses and intensities with a precision comparable to experimental measurements; (ii) _physically inspired_, to avoid making physically nonsensical ("invalid") suggestions and to provide interpretations of the chemical species responsible for each peak for the benefit of human expert chemists; and (iii) _fast_, such that it is computationally inexpensive to predict spectra for many (e.g., millions) hypothetical molecules.

Unfortunately, many existing spectrum predictors do not meet these criteria. Methods to date have tended to follow one of two approaches: (a) physically motivated fragmentation approaches or (b) molecule-to-vector (or "binned") approaches (Figure 2A-B). Fragmentation approaches (e.g., [2; 17; 40; 52]; Figure 2A) take an input molecule and suggest bonds that may break, creating fragments that are scored by ML algorithms or curated rulesets. While interpretable, these methods are often slow and restrictive; certain mass spectrum peaks are generated by complex chemical rearrangements within the collision cell that cannot be approximated by bond breaking alone. That is, the bonds in observed fragments are not a subset of those in the original molecule [9; 11]. On the other hand, binned prediction approaches (e.g., [50; 55; 58]; Figure 2B) are less physically grounded, using neural networks to directly learn a mapping from molecules to vectors representing discretized versions of the spectra. These methods, while fast, lack interpretability and due to discretization have a mass precision lower than that of most modern spectrometers, limiting their accuracy.

We propose to address the shortcomings of previous work by predicting mass spectra from molecules at the level of molecular formulae (e.g., \(_{x}_{y}_{z}_{w}\)...) and introduce a new method, Subformulae Classification for Autoregressively Reconstructing Fragmentations (SCARF) to do so. Because the molecular formula for each input molecule is known, each subformula in the predicted set of peaks is constrained to contain a subset of the atoms in the original formula. Our primary contributions are:

* posing mass spectrum prediction as a two step process: first generating the set of molecular formulae for the fragments, then associating these formulae with intensities;
* overcoming the combinatorial subformula option space by learning to generate formula prefix trees;
* demonstrating the empirical benefit of SCARF in predicting experimental mass spectra quickly and accurately using two separate datasets, providing a benchmark for future work.

## 2 Background

We provide a short introduction of tandem mass spectrometry suitable for a general machine learning audience, detail previous approaches to modeling this process as they relate to our proposed approach SCARF, and explain how such tools can be utilized to discover molecules from new spectra. We refer interested readers to  for further details on the physical process of mass spectrometry.

### Tandem mass spectrometry

Tandem mass spectrometers (MS/MS) measure fragmentation patterns of molecules in a multi-stage process. The input to the process is a solution containing a _precursor_ molecule, \(\), associated with a molecular formula, \(\), defining the counts of each element present; for instance \(=_{16}_{4}_{12}\) for the precursor molecule shown in Figure 1. The precursor molecule is first ionized (i.e., made charged), often by bonding or associating with an _adduct_ (e.g., a proton, H+) present in the solution. The charged product is then measured by a mass analyzer (MS1), where its mass-to-charge ratio (\(m/z\)) is measured.

This precursor ion is then filtered into a _collision cell_. Here, through interactions with an inert gas, the precursor ion is broken down into a set of one or more _product ions_, each of which is associated with a new chemical formula; for example, one might be \(^{1}=_{7}_{4}\) for the process shown in Figure 1. Finally, this set of product ions is measured by a second mass analyzer (MS2), along with the set of their intensities, \(y^{i}^{+}\) (i.e., their relative frequencies over several repetitions of this process), creating for each ion what is referred to as a _peak_. The collection of all peaks makes up a molecule's _mass spectrum_, and is commonly represented as a plot of intensities versus \(m/z\) (Figure 1, right).

### Predicting mass spectra from molecules (spectrum predictors)

Fragmentation prediction.A complex but physically grounded strategy is to model the bond breakage processes occurring in the collision cell (Figure 2A). Examples include MetFrag , MAGMa , and CFM-ID , which recursively fragment molecules (either bond or atom removals) to generate fragment predictions. These methods combine expert rules and local scoring methods to enumerate molecular fragmentation trees to predict spectra. CFM-ID  learns subsequent fragmentation transition probabilities between fragments with an expectation maximization algorithm to determine intensities at each fragment. Rule-based methods and full tree enumeration reduce the flexibility of these approaches, and along with the inherent ambiguity in the fragmentation process, limit this strategy's overall accuracy and speed.

Binned prediction.An increasingly popular and straightforward approach to spectra prediction is to map molecules to discretized 1D mass spectra from either molecular fingerprint  or graph inputs [55; 58] (Figure 2B). Specifically, these methods divide the \(m/z\) axis into fixed-width "bins" and predict an aggregate statistic of the peaks found in each bin (such as their maximum or summed intensity). While more flexible and end-to-end than fragmentation-based approaches, these methods do not impose the same physical constraints or shared information across fragments, making them less interpretable and susceptible to making invalid predictions. Further, discretizing the input spectrum inherently restricts the precision of such models compared to exact-mass predictions.

Formula prediction.We introduce the strategy of predicting spectra at the level of molecular formulae, an intermediate between binned and fragmentation prediction (Figure 2C). Simultaneous to our work, two groups have separately explored formula prediction strategies [34; 59]. However, to generate plausible subformulae candidates, they either generate a fixed vocabulary of formulae  or restrict their model to molecules under 48 atoms for exhaustive enumeration , which is smaller than many compounds of interest. We overcome the combinatorial problem of formula generation using prefix trees, allowing our method to scale and eliminating the need for large, fixed vocabularies.

Figure 2: Overview of various approaches to spectrum prediction. **A.** Fragmentation prediction approaches use heuristics and scoring rules to break down the molecule into fragments and their associated intensities. **B.** Binned prediction approaches discretize the possible mass-to-charge values and predict intensities for each possible bin. **C.** Formula prediction approaches predict spectra as sets of molecular formulae and intensities. Our model SCARF utilizes a two stage approach, first by predicting the product formulae present (constrained by the precursor formula), which defines the x-axis locations of the peaks, before secondly assigning intensities to these formulae (defining the peaks’ y-axis values).

### Mass spectrum libraries

One important use of spectrum predictors is in building large _in silico_ libraries of molecule spectra to augment the small size of existing, experimentally derived databases (on the order of \(10^{4}\)) which are expensive to curate. These spectra libraries are then leveraged downstream in different ways, for example for training molecular property predictors directly from mass spectra . Another common application of spectra libraries is to infer an unknown molecule's structure from a newly observed spectra - a particularly hard problem, with only 13% of spectra measured from clinical samples identifiable using current elucidation tools . In this problem, spectra libraries are used as part of a process called _retrieval_: The newly observed spectra is compared with the existing spectra in the library using a fixed or learned spectral distance function, such as cosine distance [5; 24], and the molecules associated with the closest spectra are returned as possible matches. In practice, the retrieval process is constrained to choosing among _isomers_ (i.e., molecules with the same molecular formula, and therefore molecular weight, but with different bond configurations) due to the high resolution of modern mass spectrometers (i.e., absolute errors on the order of \(10^{-4}\) to \(10^{-3}\)\(m/z\) for MS1 measurements) [13; 53; 33].

Given the varied use cases of spectra libraries, we focus on evaluating spectrum predictors in terms of both (a) their prediction accuracies (SS4.2), using metrics such as "cosine similarity", and (b) their use in generating virtual spectral libraries to assist with retrieval (SS4.3).

## 3 Model

Here, we describe our model, SCARF, for predicting mass spectra from precursor molecules via first predicting subformulae of the precursor molecule, referred to as _product formulae_. Building upon the notation introduced in the previous section, we continue to denote precursor molecules1 as \(\), and their associated formula vector as \(}_{0}^{e}\), defining at each position, \(j\{1,,e\}\), the count of each possible chemical element present, \(_{j}\) (with zero indicating none of that chemical element is present). Likewise, we define the set of \(n\) product formulae as \(\{^{i}\}_{i=1}^{n}\), and associate with each an intensity, \(y^{i}\). Note that the mass2 corresponding to a given formula (and, as such, the x-axis location of the peak on a mass spectrum) is determined deterministically from the counts of each elements present.

At a high level, SCARF generates mass spectra through the composition of two learned functions:

\[^{i},y^{i}}_{i=1}^{n}=g_{}^{ }g_{}^{}\,()\,, ,\] (1)

first mapping from the original molecule to a set of product formulae, \(g_{}^{}:\{^{i}\}_{i=1}^{n}\), and then mapping from this set of formulae (and the original molecule) to the respective intensities, \(g_{}^{}:(\{^{i}\}_{i=1}^{n},) ^{i},y^{i}}_{i=1}^{n}\,.\) The particularities of both functions are described in detail below. The specific architectures and hyperparameters used are deferred to the appendix; model code can be found at https://github.com/sangoldman97/ms-pred.

### SCARF-Thread : Generating product formulae via generating prefix trees

SCARF-Thread is tasked to learn a mapping to the set of product formulae, \(\{^{i}\}_{i=1}^{n}\), given the original molecule. Naively, one might try to define this model autoregressively, predicting the set formula by formula, chemical element by chemical element. However, such an approach soon runs into a number of problems as (i) the predictions are not invariant to set and ordering permutations; (ii) the time complexity of prediction would scale poorly, being proportional to both the number of elements and number of product formulae (i.e., \((e n)\)); and (iii) the predictions would likely contain duplicates.

We therefore take a different approach using the insight that the set of all product formulae can be compactly represented as a prefix tree (Figure 3A). In this tree, edges at a given depth represent valid counts of a particular chemical element, which are often identical across multiple product formulae (shown in the circles). By following each path from the root node to the different leaf nodes, we can reconstruct each product formula (as the orange dashed path does for a single product formula).

We thus propose SCARF-Thread as an autoregressive generator to define a probability distribution over such a prefix tree (Alg. A.1). We assume that each product formula is a subset of the precursor formula, meaning that the precursor formula sets an upper bound on the maximum number of each element3. At each node in the tree (corresponding to a prefix \(_{<j}^{{}^{}}\)), we pose the prediction of the set of child nodes (corresponding to the set of valid counts of the subsequent element) as a multi-label binary classification problem (Figure 3B). Concretely, we use a neural network module for this task, giving it as input a context vector representing the node being expanded:

\[^{}=[(),( _{<j}^{{}^{}}),(} -_{<j}^{{}^{}}),hot}(j)],\] (2)

where \(()\) specifies a neural encoding of the molecular graph (SA.5.2), \(()\) specifies a count-based encoding of the associated prefix (SSA.5.3), and \(hot}()\) specifies a one-hot encoding of the node's depth (or equivalently, which element the predicted count is for). In our experiments, we use a fixed ordering of the chemical elements (SSA.2), but optimizing or even learning the tree construction order could be carried out .

Formulae as differences.Following Wei et al. , we find it helpful to not only parameterize product formulae in terms of their element counts, but also in terms of the elements that they have lost,

Figure 3: Illustration of the SCARF-Thread architecture. **A.** The formulae of the product fragments can be represented using a prefix tree. SCARF-Thread predicts this tree for new molecules at test time. It does so by expanding each node at a given depth in parallel, treating the counts of subsequent elements as dependent only on the counts of elements predicted so far (i.e., the prefix) and the original molecular structure. **B.** The SCARF-Thread predictive task at the C\({}_{7}\) node from the prefix tree diagram shown in A. Here the network takes as input (i) an embedding of the overall molecule; (ii) a vector representing the counts of each element in the prefix so far (counts yet to be predicted are represented using a special token), (iii) the difference of the counts predicted so far from the precursor molecule, and (iv) a one-hot representation of the element for which the counts are currently being predicted. The network predicts which counts are valid next nodes in the prefix tree (where counts that are greater than those in the original precursor molecular formula are automatically masked out as invalid). See also Alg. A.1.

i.e., their _difference_ from the precursor formula. On the input side, this is already covered by including in the context vector a count-based embedding of the prefix formula minus the product formula (\((-_{<j}^{{}^{}})\)). However, on the output side this is achieved by combining the probabilities of a "forward" and a "difference" network:

\[p(f_{j}^{{}^{}}=a|_{<j}^{{}^{}},)= _{a}(^{F}(^{}))_{a}+(-)_{a}(^{D}(^{}))_{_{j} -a},\] (3)

where \(^{F}()\) and \(^{D}()\) specify multi-layer perceptrons (MLPs) for predicting the probability of observing a count of \(a\) and a loss of \(_{j}-a\) atoms respectively; \(\) is a variable (output from a third, unknown network) deciding how to weight these predictions; and \(()\) is the element-wise sigmoid function.

### SCARF-Weave: Predicting intensities given product formulae

Given the product formulae outputs from SCARF-Thread, SCARF-Weave predicts corresponding intensities at each formula. This is a set-to-set problem, well suited for any equivariant set2set architecture [56, SS3.1]. In our experiments, we use a Set Transformer , which enables the model to consider all the formulae present in the mass spectrum (and their possible interactions) when predicting final intensities.

We choose to represent formula in the set similarly to the context vectors used in SCARF-Thread. For each input, we concatenate a vector embedding of the initial molecular graph with count-based embeddings of the product formula and its difference from the precursor formula (Figure 4). We again defer the particularities of the embedding functions to the Appendix (SSA.5).

### Training and inference

Provided with a dataset of molecules and formula-labeled mass spectra, we could train the two components of SCARF separately. However, in practice we find it beneficial to first train SCARF-Thread and then train SCARF-Weave on its outputs so that the distribution the latter model sees is the same at training and prediction time. SCARF-Weave is trained using a cosine loss (SSA.5.5), as this most closely resembles the "retrieval" setting (SS 4.3).

SCARF-Thread is trained using the binary cross entropy losses associated with the multi-label classification tasks at each non-leaf node in the prefix tree. We use teacher forcing, i.e., we train on each level of the tree in parallel by conditioning on the ground-truth set of prefixes at each stage. In our experiments, when generating the set of product formulae from this model we always pick the top 300. Empirically, we find that this provides better performance than picking a variable number based on a likelihood threshold.

## 4 Experiments

We evaluate SCARF on spectra prediction (SS 4.2) and molecule identification in a retrieval task (SS 4.3).

### Dataset

We train and validate SCARF on two libraries: a gold standard commercial tandem mass spectrometry dataset, NIST20, as well as a more heterogeneous public dataset, NPLIB1, extracted from the

Figure 4: The SCARF-Weave network, which takes in the product formulae (e.g., predicted by SCARF-Thread) and predicts their intensities. We use a Set Transformer architecture , such that our model takes in the details of the other product formulae present when predicting intensities.

GNPS database  by Duhrkop et al.  and subsequently processed by Goldman et al. . We prepare both datasets by extracting and preprocessing spectra, as well as filtering to compounds that (a) are under 1,500 Da (i.e., typically under 100 heavy atoms), (b) only contain predefined elements, and (c) are only charged with common positive-mode adduct types (SS4.1).

Overall, NIST20 contains 35,129 total spectra with 24,403 unique structures, and 12,975 unique molecular formulae; NPLIB1 contains 10,709 spectra, 8,553 unique structures and 5,433 unique molecular formulae. Both datasets are evaluated using a structure-disjoint 90%/10% train/test split with 10% of training data held out for validation, such that all compounds in the test set are not seen in the train and validation sets.

Annotating spectra.We emphasize that SCARF can be trained with any product formula annotations, which can be labeled  or inferred with varied computational strategies . Herein, we utilize the MAGMA algorithm . In brief, for a given molecule-spectrum pair in the training dataset, the molecule is combinatorially fragmented at each atom up to a depth of 3 breakages to create sub-fragments. This creates a bank of possible molecular formulae, and each peak in the spectrum is assigned to its nearest possible formula within a mass difference of 20 parts-per-million.

### Spectra prediction

Predicting product formulae (SCARF-Thread).SCARF-Thread is trained and used to reconstruct prefix trees and evaluated by its ability to recover the ground truth product formula set. The set of generated product formulae is rank-ordered by the probability of each product formula and filtered to the top \(k\) predicted product formulae. The fraction of ground truth formulae (22.29 peaks on average in NIST20) contained in the top k set is computed as _coverage_.

We compare coverage achieved by SCARF-Thread to several baselines: (i) CFM-ID, a fragmentation based approach (SS A.3.1); (ii) a random baseline that samples product formulae from a uniform distribution; (iii) a frequency baseline, which ranks product formulae by the frequency the product formula candidate (or product formula difference) appears in the training set; (iv) an LSTM autoregressive neural network baseline (SS A.3.2) that is trained to predict molecular formula vectors in sequence from highest to lowest intensity; and two model ablations, (v) SCARF-Thread-D and (vi) SCARF-Thread-F, which only make uni-directional elements difference or forward predictions of element counts respectively (i.e., \(\) in Equation 3 is fixed to \(\) for (v) and \(\) for (vi)).

In general, SCARF-Thread starkly outperforms all baselines tested (Table 1). By generating \(300\) peaks, SCARF-Thread is able to cover on average \(91\%\) and \(72\%\) of the true formulae in the ground truth test set for NIST20 and NPLIB1 respectively. Our difference- and forward-only directional prediction ablations demonstrate the benefits of modeling both the atom counts for each element and the differences in counts from the original molecule.

Predicting mass spectra.We next evaluate the strength of SCARF-Weave for intensity prediction on the same test dataset. We compare against five baselines: a fragmentation-based approach, CFM-ID

   Dataset &  &  \\  Coverage @ & 10 & 30 & 300 & 1000 & 10 & 30 & 300 & 1000 \\  Random & 0.009 & 0.026 & 0.232 & 0.532 & 0.004 & 0.014 & 0.126 & 0.336 \\ Frequency & 0.173 & 0.275 & 0.659 & 0.830 & 0.090 & 0.151 & 0.466 & 0.688 \\ CFM-ID & 0.197 & 0.282 & – & – & **0.170** & 0.267 & – & – \\ Autoregressive & 0.204 & 0.262 & 0.309 & 0.317 & 0.072 & 0.082 & 0.095 & 0.099 \\   SCARF-Thread-D & 0.248 & 0.425 & 0.839 & 0.941 & 0.158 & 0.284 & 0.681 & 0.856 \\ SCARF-Thread-F & 0.249 & 0.476 & 0.855 & 0.943 & 0.155 & 0.306 & 0.708 & 0.859 \\ SCARF-Thread & **0.308** & **0.552** & **0.907** & **0.968** & 0.164 & **0.309** & **0.724** & **0.879** \\   

Table 1: Model coverage (higher better) of true peak formulae as determined by MAGMA at various max formula cutoffs for the NIST20 and NPLIB1 datasets. Best result for each column is in bold. Results are computed for a single test set; all re-trained models (i.e., Autoregressive and SCARF variants) are averaged across three random seeds.

; two NEIMS binned prediction models (; SSA.3.3), using either feed forward network modules (FFNs), as in the original work, or graph neural network modules (GNNs) as in SCARF-Weave and described by Zhu et al. ; a retrained variant of 3DMolMS (; SSA.3.4), a binned spectrum predictor that utilizes a point cloud neural network over a single molecular conformer input generated by RDKit ; and FixedVocab, a formula prediction model that predicts intensities at a fixed library of formulae and formulae differences inspired by GRAFF-MS (; SSA.3.5).

To enable fair comparison across models, we predict test spectra at 15k bins (0.1 bin resolution between \(0\) and \(1500\)) with a maximum of 100 peaks for each predicted molecule. With the exception of CFM-ID, all models are hyperparameter optimized (SA.5.6), retrained completely, and conditioned on the same covariate inputs as SCARF; such steps lead to large performance boosts to the prior NEIMS method in particular. We evaluate the quality of our predictions based upon four core criteria reflecting our original desiderata of accuracy, physical-sensibleness, and speed:

1. _Cosine sim._: Cosine similarity between the ground truth and predicted spectra, indicating spectrum prediction accuracy.
2. _Coverage:_ The fraction of ground truth spectrum peaks covered by the predicted spectrum.
3. _Valid_: The fraction of predicted peaks that can be explained by a subformula (that obeys basic ring-double bond equivalent heuristics ) of the predicted molecule.
4. _Time (s)_: The wall time it takes (using a single CPU and no batched calculations) to load the model and predict spectra for 100 randomly selected molecules.

SCARF is more accurate than all other approaches on NIST20, improving cosine similarity over a GNN binned prediction approach by over 0.02 points in NIST20 and 0.01 in NPLIB1 (Table 2). Further, our method is more physically grounded insofar as all predicted peaks are guaranteed to be valid subformulae, unlike the unconstrained binned approaches, where nearly 5% of peak predictions cannot be explained by a valid molecular formula. Importantly, SCARF still operates 2 orders of magnitude faster than CFM-ID (Table 2).

The heterogeneity, reduced dataset size, and increased average molecular weight (Figure A3) of NPLIB1 leads to substantially worse absolute performance across all models. Interestingly, in this setting, the FixedVocab approach  performs better, perhaps because the strict priors of formula constraints are more helpful with fewer and more challenging training examples. We further stratify results by molecule size in Figure A2, showing that all models are generally more accurate on smaller compounds. We additionally validate that cosine similarity is not merely measuring a model's ability to predict the parent mass peak by computing a modified cosine similarity with the original molecule's mass masked (Table A7).

### Retrieval

A key application for forward spectrum prediction is to use predicted spectra to determine the most plausible molecular structure assignment. We posit forward spectrum prediction models should be

   Dataset &  &  & \\  Metric & Cosine sim. & Coverage & Valid & Cosine sim. & Coverage & Valid & Time (s) \\  CFM-ID & 0.412 & 0.278 & **1.000** & 0.377 & 0.235 & **1.000** & 1114.7 \\
3DMolMS & 0.510 & 0.734 & 0.945 & 0.394 & 0.507 & 0.919 & **3.5** \\ FixedVocab & 0.704 & 0.788 & 0.997 & **0.568** & **0.563** & 0.998 & 5.5 \\ NEIMS (FFN) & 0.617 & 0.746 & 0.948 & 0.491 & 0.524 & 0.949 & 3.9 \\ NEIMS (GNN) & 0.694 & 0.780 & 0.947 & 0.521 & 0.547 & 0.943 & 4.9 \\   SCARF & **0.726** & **0.807** & **1.000** & 0.536 & 0.552 & **1.000** & 21.1 \\   

Table 2: Spectra prediction in terms of cosine similarity, coverage (proportion of ground-truth peaks that are covered by the top 100 non-zero predictions), validity (the fraction of predicted peaks for which a chemically plausible explanation is possible), and time. Best value in each column is typeset in bold (higher is better for all metrics but time). Results are averaged across 3 random seeds on a single data split for all retrainable models (i.e,. not CFM-ID).

particularly helpful in differentiating structurally similar molecules and design a retrieval task to showcase such potential. For each test set molecule, we extract 49 potential "decoy" options based upon the most structurally similar _isomers_ (i.e., compounds with the same precursor formula) within PubChem  as judged by Tanimoto similarity using Morgan fingerprints. While retrieval could be conducted on the entirety of PubChem or other similarly large molecular databases, we believe this subset retrieval setting is more practical and better mirrors a real-world setting (see SSA.4 for justification). We predict the spectra for all molecules and rank them according to their similarity to the ground truth spectrum, computing the _accuracy_ for retrieval. Herein, we specifically emphasize models and retrieval on the NIST20 dataset, as it is a much larger and higher quality dataset.

SCARF reaches a top 1 and top 5 retrieval accuracy in this task of 18.7% and 54.1% respectively, representing an improvement over the methods with the second best top 1 accuracy of 17.5% (NEIMS (GNN)) and top 5 accuracy of 52.2% (Fixed Vocab) (Figure 5A). We highlight two example predictions from SCARF (Figure 5B-C), with additional randomly sampled test set predictions shown in Figure A.1. We repeat similar experiments within NPLIB1, but find that cosine similarity performance is uncorrelated with relative ranking performance; feed forward fingerprint based approaches are better at retrieval, despite relatively weak cosine similarity ( SSA.1). FixedVocab  performs especially well on NPLIB1, again likely due to the helpful biases imparted by constraining the formula vocabulary.

This result underscores previous observations regarding how database and model biases can skew retrieval results under certain settings . That is, models may be more or less robust for certain classes of molecules, so the composition of these classes in the retrieval library may affect the retrieval accuracy accordingly. The observed discrepancy between cosine similarity and retrieval performance can further be explained by the dataset shift required for computing retrieval accuracy; cosine similarity is evaluated on "in-domain" data, whereas retrieval relies also on accuracy on unlabeled data that may be "out-of-domain."

## 5 Related work

Forward vs backward models.Computational tools to identify mass spectra are often divided into two categories: forward and backward models. Forward models, i.e., spectrum predictors, such as SCARF or the methods discussed in Section 2, operate in the causal direction and try to predict the spectrum given the molecule. Backward models start from the spectrum and predict features or even full molecule structures. Early backward models used heuristics, expert rules, and even neural networks [8; 10; 42]. Such approaches have more recently been augmented with kernel methods and more modern, deep representation learning techniques [12; 16; 19; 47]. These models are complementary to spectrum predictors.

Figure 5: SCARF enables more accurate retrieval of ground truth molecules within the NIST20 dataset. **A.** Average retrieval accuracy of SCARF at various top k thresholds. Retrieval is conducted on the same test split, and retrieval accuracy is averaged across models trained for three separate random seeds. **B-C.** Example spectrum predictions made by SCARF (top) compared to the ground truth spectrum (bottom). Up to 5 predicted peaks with the highest intensity are annotated with their molecular formula explanation as predicted by SCARF. The full molecule is shown inset. Further examples are in the Appendix (Figure A1).

Mass spectra for proteomics.Although this paper has focused on small molecules, similar trends of deep representation learning for mass spectra are also emerging in the adjacent field of proteomics , with Shouman et al.  recently proposing a benchmark challenge in this domain. While small molecule and protein spectra are similar, proteomics spectra tend to be more easily predicted as fragments are often formed at peptide bonds. We believe adapting SCARF to this task would be an interesting direction for future work.

Neural set generation.Our work is also related to methods for modeling sets and multisets. SCARF-Thread generates a set as output, which has been studied elsewhere in the context of n-gram generation , object detection , and point cloud generation . The product formulae sets we generate, however, are different to those considered in these other task; in our setting, each member of the set (i.e., individual product formula) represents a multiset of atom types (i.e., multiple carbons, multiple hydrogens, etc.) and is constrained physically by the precursor formula.

## 6 Conclusion

In this paper we introduced SCARF, an approach utilizing prefix tree data structures to efficiently decode mass spectra from molecules. By first predicting product formulae and then assigning these formulae intensities, we are able to combine the advantages of previous neural and fragment based approaches, providing fast and physically grounded predictions. We show how these resulting predictions are both more accurate in predicting experimentally-observed spectra and yield improvements in identifying a molecule's structure from its respective mass spectrum, as tested on a widely used dataset.

In term of limitations, our model is data dependent, as indicated by the relative performance across the NIST20 and NPLIB1 datasets. SCARF is also highly reliant upon the quality of product formula label assignment. The current commercial status of mass spectrometry training data poses a barrier to entry, and identifying high quality public domain data is critical for future studies. A key contribution of this work is to retrain and optimize the hyperparameters of competing methods on a publicly available dataset under equivalent conditions to allow for future extensions. Directly training on a ranking-based loss or learning a model specific spectra distance function may be one way to improve upon our model's performance in the retrieval setting, and we outline additional potential ideas more explicitly in SSA.6.

Future directions will involve further developing SCARF for real world use cases such as unknown metabolite elucidation in clinical samples. Specific directions will include more carefully modeling covariates (e.g., collision energies and MS/MS instrument types), grounding product formulae in molecular graph substructures, and utilizing such models to augment inverse spectrum-to-molecule annotation tools.