# Riemannian Projection-free Online Learning

Zihao Hu\({}^{}\), Guanghui Wang\({}^{}\), Jacob Abernethy\({}^{,}\)

College of Computing, Georgia Institute of Technology\({}^{}\)

Google Research\({}^{}\)

{zihaohu,gwang369}@gatech.edu, abernethyj@google.com

###### Abstract

The projection operation is a critical component in a wide range of optimization algorithms, such as online gradient descent (OGD), for enforcing constraints and achieving optimal regret bounds. However, it suffers from computational complexity limitations in high-dimensional settings or when dealing with ill-conditioned constraint sets. _Projection-free_ algorithms address this issue by replacing the projection oracle with more efficient optimization subroutines. But to date, these methods have been developed primarily in the _Euclidean setting_, and while there has been growing interest in optimization on _Riemannian manifolds_, there has been essentially no work in trying to utilize projection-free tools here. An apparent issue is that non-trivial affine functions are generally non-convex in such domains. In this paper, we present methods for obtaining sub-linear regret guarantees in _online geodesically convex optimization_ on curved spaces for two scenarios: when we have access to (a) a _separation oracle_ or (b) a _linear optimization oracle_. For geodesically convex losses, and when a separation oracle is available, our algorithms achieve \(O(T^{}{{2}}})\), \(O(T^{}{{4}}})\) and \(O(T^{}{{2}}})\) adaptive regret guarantees in the full information setting, the bandit setting with one-point feedback and the bandit setting with two-point feedback, respectively. When a linear optimization oracle is available, we obtain regret rates of \(O(T^{}{{4}}})\) for geodesically convex losses and \(O(T^{}{{3}}} T)\) for strongly geodesically convex losses.

## 1 Introduction

Online convex optimization (OCO) offers a framework for modeling sequential decision-making problems (Hazan et al., 2016). The standard setting depicts the learning process as a zero-sum game between a learner and an adversary. At round \(t\), the learner selects a decision \(_{t}\) from a convex set \(\) and observes the encountered convex loss function \(f_{t}\). The learner's goal is to minimize _regret_, defined as

\[_{T}_{t=1}^{T}f_{t}(_{t})-_{}_{t=1}^{T}f_{t}().\]

In Euclidean space, OCO boasts a robust theoretical foundation and numerous real-world applications, such as online load balancing (Molinaro, 2017), optimal control (Li et al., 2019), revenue maximization (Lin et al., 2019), and portfolio management (Jezequel et al., 2022). The standard approach for OCO is online gradient descent (OGD), which performs

\[_{t+1}=_{}(_{t}-_{t} f_{t}( _{t})),\]

where \(_{}\) represents the orthogonal projection onto \(\), ensuring the sequence \(\{_{t}\}_{t=1}^{T}\) remains feasible. However, the projection operation can be computationally expensive in high-dimensional or complex feasible sets. Projection-free online learning provides a reasonable way to handle this situation. At the heart of this approach is the understanding that, in many cases, the complexity ofimplementing an optimization oracle can be significantly lower than that of the orthogonal projection. As a result, both practical and theoretical interests lie in replacing the projection operation with these optimization oracles. The two most well-known projection-free optimization oracles are:

\[\,*{argmin}_{ }\,,\]

and

\[\,:\,\,-, >0,.\]

Whereas much attention has been given to the development of sub-linear regret guarantees using optimization oracles in Euclidean space (Hazan & Kale, 2012; Levy & Krause, 2019; Hazan & Minasyan, 2020; Wan et al., 2022; Garber & Kretzu, 2022; Mhammedi, 2022a; Wan et al., 2023), there is considerably less research on projection-free optimization on _Riemannian manifolds_. There are numerous scenarios where the space of feasible parameters is not a convex subset of Euclidean space, but instead has a manifold structure with a Riemannian metric; examples include non-negative PCA (Montanari & Richard, 2015), \(K\)-means clustering (Carson et al., 2017) and computing Wasserstein-Barycenters (Weber & Sra, 2022b). However, there has been significantly less research on efficiently computing projections or projection-free optimization in the Riemannian setting, even for highly-structured geodesically convex (gsc-convex) feasible sets like:

\[=\{|_{1 i m}h_{i}()\}\]

where each \(h_{i}()\) is gsc-convex. In this paper we focus on projection-free optimization on Riemannian manifolds in the online setting. A Riemannian version of Online Gradient Descent (R-OGD) was given by Wang et al. (2021):

\[_{t+1}=_{}_{_{t}}(-_{t}  f_{t}(_{t})),\]

where \(\) is now a gsc-convex set, and \(_{}\) is the metric projection onto \(\). This method achieves sub-linear regret guarantees for Riemannian OCO. The metric projection is often the most expensive operation, and it is not always clear how to implement it on a manifold. In the present work, we propose the use of two alternative operations: a separation oracle (SO) or a linear optimization oracle (LOO) on the Riemannian manifold, with definitions deferred to Section 3. It is important to note that, in Euclidean space, both oracles rely on the definition of a hyperplane. In the realm of Riemannian manifolds, there are two natural extensions of a hyperplane: one is the sub-level set of a Busemann function, known as a horosphere (Bridson & Haefliger, 2013); the other relies on the inverse exponential map, as outlined in (2). While a horosphere is gsc-convex, the corresponding separation oracle exists if every boundary point of the feasible gsc-convex set has a locally supporting horosphere (Borisenko, 2002). The existence of a separation oracle is assured for any gsc-convex sets on Hadamard manifolds (Silva Louzeiro et al., 2022), if defined via the inverse exponential map. Hence, we adopt this latter definition. It is worth noting that this object is typically non-convex (Kristaly et al., 2016), leading to geometric complexities that necessitate careful management. Also, our work builds upon the findings of Garber & Kretzu (2022) and inherits the adaptive regret guarantee for gsc-convex losses, as defined by Hazan & Seshadhri (2009):

\[_{T}_{|s,e|[T]}\{_{t=s}^{e}f _{t}(_{t})-_{}_{t=s}^{e}f_{t}( )\}.\]

Our main contributions are summarized in Table 1. More specifically,

   Oracle & Losses & Feedback & Measure & Regret \\   & gsc-convex & full information & adaptive regret & \(O(T^{}{{2}}})\), Thm. 1 \\  & gsc-convex & bandit, one-point & expected adaptive regret & \(O(T^{}{{4}}})\), Thm. 2 \\  & gsc-convex & bandit, two-point & expected adaptive regret & \(O(T^{}{{2}}})\), Thm. 3 \\   & gsc-convex & full information & adaptive regret & \(O(T^{}{{4}}})\), Thm. 4 \\  & strongly gsc-convex & full information & regret & \(O(T^{}{{3}}} T)\), Thm. 5 \\   

Table 1: Summary of main results. Our approach allows us to invoke either a Separation Oracle (SO) or a Linear Optimization Oracle (LOO) for \(O(T)\) times throughout the \(T\) rounds. Notably, our results match those presented by Garber & Kretzu (2022).

* Given a separation oracle, we attain adaptive regret bounds of \(O(T^{}{{2}}})\), \(O(T^{}{{4}}})\) and \(O(T^{}{{2}}})\) for gsc-convex losses in the full information setting, the bandit convex optimization setting with one-point feedback1, and the bandit convex optimization setting with two-point feedback, respectively. * Assuming access to a linear optimization oracle, we provide algorithms that enjoy \(O(T^{}{{4}}})\) adaptive regret for gsc-convex losses and \(O(T^{}{{3}}} T)\) regret for strongly gsc-convex losses.
* We highlight some key differences between convex sets on a Hadamard manifold and in Euclidean space. In particular, shrinking a gsc-convex set towards an interior point does not preserve convexity, and the Minkowski functional on a Hadamard manifold is non-convex. These results, which may not be well-known within the machine learning community, could be of independent interest.

The technical challenges of this paper can be divided into two parts.

Firstly, for the separation oracle, we need to bound the "thickness" of part of the feasible set cut by the separating hyperplane. While in Euclidean space, this can be achieved through a convex combination argument (Garber and Kretzu, 2022), the task becomes challenging on manifolds due to the varying metric at different points and the non-convex nature of the separating hyperplane. Fortunately, this problem can be addressed using the Jacobi field comparison technique. Also, unlike in Euclidean space, one cannot directly construct a separation oracle for \((1-)\) using a separation oracle for \(\). This poses significant challenges in the bandit setting. Nonetheless, we have identified a novel solution to this issue in the two-point feedback setting.

Secondly, in Euclidean space, the linear optimization oracle is typically invoked by online Frank-Wolfe (OFW) (Hazan and Kale, 2012; Kretzu and Garber, 2021) to achieve no-regret online learning. The analysis of OFW relies on the fact that the Hessian of a linear function is zero everywhere. But on Hadamard manifolds, such functions' existence implies that the manifold has zero sectional curvature everywhere (Kristaly et al., 2016). The algorithms in Garber and Kretzu (2022) do not require affinity and serve as a starting point for our results. However, the analysis still needs to be conducted carefully due to the non-convexity of the separating hyperplane on manifolds.

## 2 Related Work

In this section, we briefly review previous work on projection-free online learning in Euclidean space as well as online and projection-free optimization on Riemannian manifolds.

### Projection-free OCO in Euclidean Space

**Linear Optimization Oracle.** The pioneering work of Hazan and Kale (2012) first introduced an online variant of the Frank-Wolfe algorithm (OFW) and achieved \(O(T^{}{{4}}})\) regret for convex functions. Hazan and Minasyan (2020) proposed a randomized algorithm that leverages smoothness to achieve \(O(T^{}{{3}}})\) expected regret. The insightful analysis of Wan and Zhang (2021) and Kretzu and Garber (2021) demonstrated that OFW indeed attains \(O(T^{}{{3}}})\) regret for strongly convex functions. Garber and Kretzu (2022) showed that it is possible to achieve \(O(T^{}{{4}}})\) adaptive regret and \(O(T^{}{{3}}} T)\) regret for convex and strongly convex functions, respectively. Mhammedi (2022b) illustrated how to obtain \((T^{}{{3}}})\) regret for convex functions, where \(()\) hides logarithmic terms. Our results are in line with those of Garber and Kretzu (2022) and inherit the adaptive regret guarantee.

**Separation Oracle and Membership Oracle.** Levy and Krause (2019) demonstrated that it is possible to achieve \(O()\) and \(O( T)\) regret bounds for convex and strongly convex functions when the feasible set is a sublevel set of a smooth and convex function. Mhammedi (2022a) generalized the idea of Levy and Krause (2019) and showed how to obtain \(O()\) and \(O( T)\) regret guarantees for general convex sets. Garber and Kretzu (2022) provided algorithms that ensure \(O()\) and \(O(T^{}{{4}}})\) adaptive regret guarantees for convex losses in the full information and bandit settings, respectively.

### Online and Projection-free Optimization on Manifolds

**Online Optimization on Manifolds.**Becigneul and Ganea (2019) demonstrated that a series of adaptive optimization algorithms can be implemented on a product of Riemannian manifolds, with each factor manifold being assigned a learning rate. Antonakopoulos et al. (2020) proposed using Follow the Regularized Leader with a strongly gsc-convex regularizer to achieve \(O()\) regret when the loss satisfies Riemannian Lipschitzness. Wang et al. (2021) introduced Riemannian OGD (R-OGD) and showed regret guarantees in full information and bandit convex optimization settings. Hu et al. (2023) considered achieving optimistic and dynamic regret on Riemannian manifolds.

**Projection-free Optimization on Manifolds.**Rusciano (2018) provided a non-constructive cutting hyperplane method on Hadamard manifolds. By comparison, our algorithms are constructive and deterministic. Weber and Sra (2022b) proposed Riemannian Frank-Wolfe (RFW) for gsc-convex optimization and showed some practical applications on the manifold of SPD matrices. In a subsequent work, Weber and Sra (2022a) generalized RFW to the stochastic and non-convex setting. We use RFW as a subroutine to invoke the linear optimization oracle and establish sub-linear regret guarantees. Hirai et al. (2023) implemented the interior point method on Riemannian manifolds and used a self-concordant barrier to enforce the constraint.

## 3 Preliminaries and Assumptions

In this section, we lay the groundwork for our study by presenting an overview of Riemannian manifolds, the separation oracle and the linear optimization oracle within these spaces. Additionally, we establish key definitions and assumptions that will be integral to the following sections.

**Riemannian Manifolds.** We provide key notations in Riemannian geometry that will be employed throughout this paper. Readers looking for a more comprehensive treatment are encouraged to consult Petersen (2006); Lee (2018). Our proof also relies on the concept of the _Jacobi field_, and we provide some backgrounds in Appendix E.1. We consider an \(n\)-dimensional smooth manifold \(\) equipped with a Riemannian metric \(g\). This metric confers a point-wise inner product \(,_{}\) at every point \(\), where \(,\) are vectors in the tangent space \(T_{}\) at \(\). This tangent space, a vector space of dimension \(n\), encompasses all vectors tangent to \(\). The Riemannian metric also determines the norm of a tangent vector \(\) as: \(\|\|,}\). A geodesic \((t):[0,c]\) is a piecewise smooth curve with a constant velocity that locally minimizes the distance between its endpoints, say, \(\) and \(\). The Riemannian distance between these two points is given by \(d(,)_{0}^{c}\|(t)\|dt=_{0}^{ c}\|(0)\|dt\). It's important to note that the Riemannian distance remains invariant under reparameterizations of \((t)\). Consider a geodesic \((t):\) with \((0)=\), \((1)=\) and \((0)=\). The exponential map \(_{}()\) transforms \( T_{}\) to \(\), and the inverse exponential map \(_{}^{-1}\) performs the inverse operation, mapping \(\) to \( T_{}\). The inverse exponential map also offers a handy way to express the Riemannian distance: \(d(,)=\|_{}^{-1}\|\).

The sectional curvature at a point \(\) is contingent on two-dimensional subspaces of \(T_{}\), and describes the curvature near \(\). Generally, geodesics diverge on manifolds with negative sectional curvature, converge on manifolds with positive sectional curvature, and manifolds with zero sectional curvature are locally isometric to Euclidean space. In line with Zhang and Sra (2016); Wang et al. (2021), we primarily explore Hadamard manifolds, which are simply connected manifolds with non-positive curvature that admit a unique global minimizing geodesic between any pair of points. A set \(\) is geodesically convex (gsc-convex) if it includes the geodesic connecting \(\) and \(\) for any \(,\). A \(\)-strongly gsc-convex (or gsc-convex, when \(=0\)) function \(f:\) fulfills

\[f() f()+ f(),_{ }^{-1}+d(,)^{2},\] (1)

for any \(,\), where \( f() T_{}\) is the Riemannian gradient.

**Optimization Oracles on Riemannian Manifolds.** In this part, we introduce the concept of a separation oracle and a linear optimization oracle on Riemannian manifolds.

A separation oracle, given a point \(\) not in the gsc-convex set \(\), returns a non-convex separating hyperplane that satisfies the following condition:

\[-_{}^{-1},>0, ,\] (2)where \( T_{}\). Even with the non-convexity of the separating hyperplane, for certain gsc-convex sets like \(=\{|_{1 i m}h_{i}() 0\}\) where each \(h_{i}()\) is gsc-convex, a separation oracle can be efficiently implemented by Lemma 18 and Remark 4.2

On the other hand, a linear optimization oracle is responsible for solving the following problem:

\[*{argmin}_{}, *{Exp}_{_{0}}^{-1}\]

on a gsc-convex set \(\), where \(_{0}\) and \( T_{_{0}}\). Although this objective is not gsc-convex, it can still be solved in closed form for certain problems. Examples include computing the geometric mean and the Bures-Wasserstein barycenter on the manifold of SPD matrices (Weber and Sra, 2022).

In this paper, we rely on a series of definitions and assumptions, which we introduce here for clarity and reference in subsequent sections.

**Assumption 1**.: _The manifold \(\) is Hadamard with sectional curvature bounded below by \(\), so the sectional curvature of \(\) lies in the interval \([,0]\)._

**Assumption 2**.: _The manifold \(\) is a homogeneous Hadamard manifold with sectional curvature bounded below by \(\). For every \(t[T]\), the inequality \(|f_{t}()| M\) holds. For the bandit setting with two-point feedback, we additionally require \(\) to be symmetric._

**Assumption 3**.: _The set \(\) is a gsc-convex decision set and satisfies \(_{}(r)_{ }(R)\). Here, \(_{}(r)\) represents the geodesic ball centered at \(\) with radius \(r\)._

**Assumption 4**.: _For every \(t[T]\) and \(_{}(R)\), the function \(f_{t}()\) is gsc-convex (or strongly gsc-convex), and the norm of its gradient is bounded by \(G\), i.e., \(\| f_{t}()\| G\)._

Let us make two important comments. First, the homogeneity and the symmetry of \(\) allows us to employ the unbiased estimator presented in Wang et al. (2023) for the bandit setting. It should be noted that homogeneous and symmetric Hadamard manifolds include two of the most commonly used ones: the hyperbolic space and the manifold of SPD matrices. Second, the projection onto a geodesic ball, denoted as \(_{_{}(R)}()\), is considered projection-free as it can be computed to an \(\) precision using \((}{{}})\) bisections 3.

**Definition 1**.: _We define a geometric constant \(\) as \( 2R(2R)\)._

**Definition 2**.: _Fixing \(\), for any \(c(0,)\), we define \(c=\{*{Exp}_{}(c*{Exp}_{ }^{-1})|\}\)._

**Definition 3**.: _We call \(}\) an infeasible projection of \(\) onto a simply connected closed set \(}\) if for every point \(}\), the inequality \(d(},) d(,)\) holds. We define \(_{IP}(},)\) as an infeasible oracle for \(}\) which, given any \(\), returns an infeasible projection of \(\) onto \(}\)._

We note that, in Euclidean space where the sectional curvature is zero everywhere, we have \(=_{ 0}2R(2R)=1\). We also observe that the definition of the infeasible projection in Garber and Kretzu (2022) requires \(}\) to be convex, which is indeed unnecessary. This distinction is essential because, in the case of the separation-oracle-based OCO, we construct an infeasible projection oracle onto \(}=(1-)\), which may be non-convex (Theorem 6).

## 4 Warm-up: the High-level Idea

We briefly illustrate the overarching strategy of achieving regret guarantees. Our basic algorithm is Algorithm 1, which generates a sequence \(\{_{t}\}_{t=1}^{T}\) by R-OGD that does not necessarily fall within the feasible set. A key insight is that we can build an infeasible projection oracle using either a separation oracle or a linear optimization oracle, resulting in a sequence \(\{}_{t}\}_{t=1}^{T}\) that exhibits a desirable regret guarantee (as shown in Lemma 1). The design of the infeasible projection oracle rests on a straightforward fact: whenever \(_{t}\) deviates significantly from \(\), we can call upon either oracle to produce a descent direction and then apply Lemma 2 to gauge the progress. Additional error terms, arising from the fact that \(}_{t}\) may not necessarily lie in \(\), can be quantified by leveraging the boundedness of the gradient in Assumption 4.

**Data:** horizon \(T\), feasible set \(}\), step-sizes \(\{_{t}\}_{t=1}^{T}\), infeasible projection oracle \(_{IP}(},)\).

**for \(t=1,,T\)do**

 Play \(}_{t}\), and observe \(f_{t}(}_{t})\)

 Update \(_{t+1}=_{}_{t}}(-_{t} f_{t}( }_{t}))\), and set \(}_{t+1}=_{IP}(},_{t+1})\)

**end**

**We have the following guarantee for Algorithm 1.**

**Lemma 1**.: _(Proof in Appendix A.1) Assume \(}_{t}_{}(R)\) and let \(_{t}= f_{t}(}_{t})\) and \(_{}(R)\) be a gsc-convex subset of \(\). Consider \(}\) as a simply connected and compact set, and \(_{IP}(},)\) be an infeasible projection oracle as in Definition 3._

1. _Suppose all losses are gsc-convex on_ \(_{}(R)\)_. Fix some_ \(>0\) _and let_ \(_{t}=\) _for all_ \(t 1\)_._

_Algorithm 1 guarantees that the adaptive regret is upper-bounded by:_

\[ I=[s,e][T]:_{t=s}^{e}f_{t}(}_{t})-_{ _{t}}}_{t=s}^{e}f_{t}(_{I}) }_{s},_{I})^{2}}{2}+^{e}\|_{t}\|^{2}}{2}.\]
2. _Suppose all losses are_ \(\)_-strongly gsc-convex on_ \(_{}(R)\) _for some_ \(>0\)_. Let_ \(_{t}=\) _for all_ \(t 1\)_._

_Algorithm 1 guarantees that the static regret is upper bounded by:_

\[_{t=1}^{T}f_{t}(}_{t})-_{}}_{t=1}^{T}f_{t}()_{t=1}^{T}\|^{2}}{2 t}.\]

**Remark 1**.: _To apply Lemma 1, we need to ensure that \(}_{t}_{}(R)\) for any \(t[T]\). In the case of a separation oracle, we have \(}=(1-)\) for some \((0,1)\), and \(}_{t}_{}(R)\) by Lemma 4. With a linear optimization oracle, we have \(}=\), and \(}_{t}_{}(R)\) is guaranteed by Lemma 6._

**Lemma 2**.: _(Proof in Appendix A.2) Consider \(}_{}(R)\) as a simply connected and compact subset of \(\). If \(}\) and \( T_{}\) satisfies \(-_{}^{-1}, Q\), where \(Q>0\), then consider \(}=_{}(-)\). For \(=}\) and \(\|\| C\), assume \(d(,) 2R\), then we have_

\[d(},)^{2} d(,)^{2}-}{ C^{2}}.\]

Unlike Garber & Kretzu (2022), in Lemma 2, we also do not require \(}\) to be gsc-convex.

## 5 Riemannian OCO with a Separation Oracle

In this section, we show how to use a separation oracle to construct an infeasible projection oracle and achieve sublinear regret guarantees. We note that we rely on an infeasible projection oracle onto \((1-)\) rather than directly onto \(\). While we have a separation oracle that results in \(-_{}^{-1},>0\), using Lemma 2 on this separating hyperplane may lead to minuscule progress, given that \(Q\) can be arbitrarily small. Consequently, achieving sublinear regret with only \(O(T)\) oracle calls becomes unfeasible. In contrast, constructing an infeasible projection onto \((1-)\) always ensures meaningful progress, as quantified by Lemmas 3 and 4.

``` Data: feasible gsc-convex set \(\), radius \(r\), squeeze parameter \(\), initial point \(_{0}\). \(_{1}=_{_{}(R)}_{0}\) for\(i=1,\)do if\(_{i}\)then \(_{}\) returns \(_{i}\) satisfying \(-_{_{i}}^{-1},_{i}>0\) \(_{i+1}=_{_{i}}(-_{i}_{i})\) where \(_{i}=\|}\) and \(\) is defined in Equation (3) else return\(=_{i}\)  end if  end for  end while ```

**Algorithm 2**Infeasible Projection onto \((1-)\) with a Riemannian Separation Oracle

**Lemma 3**.: _(Proof in Appendix B.1) Let \(_{}(R)\) and let \( T_{}\) be the output of the separation oracle for \(\). Then, under Assumptions 1 and 3, we have that \(-_{}^{-1},> \|\|\) for any \((1-)\), where_

\[:=(2R+r)}{((2R+r))}(R+r)}{((R+r))} r.\] (3)

In Euclidean space, we can establish that \(-,> r\|\|\)(Garber and Kretzu, 2022, Lemma 11). However, as indicated in Lemma 3, the result on manifolds is significantly worse with respect to \(R\), given the exponential nature of \(\). It is an interesting line of inquiry to explore whether this dependence is unavoidable.4

Based on Lemma 3, to implement an infeasible projection oracle onto \((1-)\), the number of calls to the separation oracle is bounded in Lemma 4.

**Lemma 4**.: _(Proof in Appendix B.2) Under Assumptions 1 and 3. Let \(0<<1\) and set \(_{i}=}{\|\|}\). Algorithm 2 executes at most \(_{0},(1-))^{2}-d(_{0},(1- ))^{2})}{^{2}^{2}}+1\) iterations and returns \(\) such that \(d(,)^{2} d(_{0},)^{2}\) holds for any \((1-)\)._

In the full information setting, with a separation oracle, infeasible R-OGD is shown in Algorithm 3.

**Data:** feasible gsc-convex set \(\), radius \(r\), step-size \(\) and squeeze parameter \(\).

\(}_{1}=(1-)\)

**for \(t=1,,T\)do**

**Algorithm 2 executes at most \(_{0},(1-))^{2}-d(_{0},(1- ))^{2})}{^{2}^{2}}+1\) iterations and returns \(\) such that \(d(,)^{2} d(_{0},)^{2}\) holds for any \((1-)\).**

**Algorithm 3:** Infeasible R-OGD with a separation oracle

We can show the following regret guarantee for Algorithm 3.

**Theorem 1**.: _(Proof in Appendix B.3) Under Assumptions 1, 3 and 4. Set \(=T}\) and \(=}\), then the regret of Algorithm 3 is upper bounded by_

\[_{[s,e][T]}\{_{t=s}^{e}f_{t}( }_{t})-_{_{t}}_{t=s}^{e}f_{t}(_{I}) \}GRT,\]

_and the number of calls to the separation oracle is \(O(T)\)._

**Moving on, we demonstrate how to achieve a sublinear regret guarantee in the bandit convex optimization setting. A major challenge is that, while in Euclidean space, we can construct a separation oracle on \((1-)\) using the separation oracle on \(\) (Garber and Kretzu, 2022, Lemma 11.). On Hadamard manifolds, \((1-)\) can even be non-convex (Theorem 6), thus a separation oracle for \((1-)\) may not exist. For Riemannian BCO with one-point feedback, in Algorithm 4, we address this by resorting to a non-standard setting: we play \(}_{t}\) but we receive feedback at \(_{t}\) where \(_{t}\), \(}_{t}\) are nearby points. We present the algorithm and the corresponding regret guarantee in Algorithm 4 and Theorem 2.

**Data:** feasible gsc-convex set \(\), radii \((R,r)\), step-size \(\), squeeze parameters \((,^{},)\), \(}_{1}=\).

**for \(t=1,,T\)do**

Sample \(_{t}_{}_{t}}(^{})\); play \(}_{t}_{}(_{ }^{-1}_{t}}{1+})\) // \(^{}=(R+r)}{((R+r))} r\)

Observe \(f_{t}(_{t})\); \(_{t}=f_{t}(_{t})_{}_{ 1}}^{-1}_{t}}{\|_{}_{1}}^{-1}_{t }\|}\); \(_{t+1}=_{}_{t}}(-_{t})\)

\(}_{t+1}\) Output of Algorithm 2 with \(\), radius \(r\), squeeze parameter \(\) and initial point \(_{t+1}\).

**end**

[MISSING_PAGE_FAIL:8]

``` Data: feasible gsc-convex set \(\), error tolerance \(\), initial point \(_{1}\), target vector \(\). for\(i=1,\)do \(_{i}=*{argmin}_{}\{- _{_{i}}^{-1},_{_{i}}^{-1} \}\) if\(\{_{_{i}}^{-1},_{_{i}}^{-1} _{i}\}\) or \(d(_{i},)^{2} 3\)then return\(}_{i}\)  end if \(_{i}=*{argmin}_{}\{d(,_{ _{i}}(_{_{i}}^{-1}_{i}))^{2}\}\) \(_{i+1}=_{_{i}}(_{i}_{_{i}}^{-1}_{i})\)  end for ```

**Algorithm 6**Separating Hyperplane via RFW

**Lemma 5**.: _(Proof in Appendix C.1) Under Assumptions 1 and 3. For any \(_{}(R)\), Algorithm 6 terminates after at most \((27R^{2}/)-2\) iterations and returns \(}\) satisfies:_

1. \(d(},)^{2} d(_{1},)^{2}\)_._
2. _At least one of the following holds:_ \(d(},)^{2} 3\) _or_ \(:_{}^{-1} ,_{}^{-1}} 2\)_._
3. _If_ \(d(,)\) _then_ \(d(},)^{2} 3\)_._

**Remark 3**.: _Note that the second item of Lemma 5 provides a separating hyperplane between \(\) and \(\). One of the challenges in its proof is to find an analog of the Euclidean identity \(-,}-=\|}-\|_{2}^{2}--},-}\) on manifolds, which initially appears to be a daunting task. However, a clever application of Lemma 30 (Appendix E) provides a solution._

``` Data: feasible gsc-convex set \(\), \(_{0}\), initial point \(_{0}\), error tolerance \(\), step size \(\). \(_{1}=_{_{}(R)}_{0}\) if\(d(_{0},_{0})^{2} 3\)then return\(_{0},_{1}\). end for\(i=1,,T\)do \(_{i}\) Output of Algorithm 6 with set \(\), feasible point \(_{i-1}\), initial point \(_{i}\) and tolerance \(\). if\(d(_{i},_{i})^{2}>3\)then \(_{i+1}=_{_{i}}((1-)_{_{i}}^{-1}_{i})\) else return\(_{i},_{i}\).  end if  end for ```

**Algorithm 7**Closer Infeasible Projection via LOO

Algorithm 7 demonstrates how to "pull" an initial point \(_{0}\) towards \(\) using RFW, while Lemma 6 verifies that the output of Algorithm 7 is indeed an infeasible projection onto \(\).

**Lemma 6**.: _(Proof in Appendix C.2) Under Assumptions 1 and 3. Fix \(>0\). Setting \(=_{0},_{0})^{2}}\), Algorithm 7 stops after at most \(\{_{0},_{0})^{2}(d(_{0}, _{0})^{2}-)}{4^{2}}+1,1\}\) iterations, and returns \((,)_{}(R)\) such that_

\[:\ d(,)^{2} d(_{0},)^{2} d(,)^{2} 3.\]

Given that Lemma 6 provides an infeasible projection oracle, we can combine Algorithms 1 and 7 to achieve sublinear regret by setting the error tolerance as \(=o(1)\). However, RFW requires \((}{{}})=(1)\) iterations in the worst-case scenario (Lemma 5), and the resulting algorithm necessitates \((T)\) calls to the linear optimization oracle. Garber & Kretzu (2022) utilize a block trick to address this challenge: the time horizon \(T\) is broken into \(B\) blocks, and the infeasible projection is computed once in each block. We demonstrate that this trick can be implemented on Riemannian manifolds in Algorithm 8. We present the regret guarantees for gsc-convex and strongly gsc-convex losses in Theorem 4 and Theorem 5, respectively.

**Theorem 4**.: _(Proof in Appendix C.3) Under Assumptions 1, 3 and 4. Fixing \(_{i}\) and \(_{i}\) as \(=T^{-}\) and \(=60R^{2}^{2}T^{-}\), respectively, for any \(i\{1,,\}\). Setting \(B=5T^{}{{2}}}\). Then the regret of Algorithm 8 for gsc-convex losses is bounded by

\[_{I=[s,c][T]}\{_{t=s}^{e}f_{t}(_{t})-_{ _{I}}_{t=s}^{e}f_{t}(_{I})\} GR (T^{}{{4}}}^{2}+T^{}{{4}}}+4T^{}{{4}}}/+20T^{}{{2}}}),\]

and the number of calls to the linear optimization oracle is bounded by \(T\).

**Theorem 5**.: _(Proof in Appendix C.4) Under Assumptions 1, 3 and 4. Suppose all losses \(\{f_{t}\}_{t=1}^{T}\) are \(\)-strongly gsc-convex for some known \(>0\) and \(T 3B\). Choosing \(_{i}=()^{2}\) and \(_{i}=\) for any \(i=1,,\). With \(B=()^{}T^{}{{3}}}\) and assume \(T 3B\), the regret guarantee of Algorithm 8 is bounded by_

\[_{t=1}^{T}f_{t}(_{t})-_{}_{t=1} ^{T}f_{t}()(20+1)(G^{4}R^{2}/)^{}T ^{}{{3}}}(1+(G}{ R} )).\]

_And the number of total calls to the linear optimization oracle is bounded by \( T\)._

## 7 Conclusion and Perspective

This paper pioneers the exploration of projection-free online optimization on Riemannian manifolds. The primary technical challenges originate from the non-convex nature of the Riemannian hyperplane and the variable metric. These challenges are tackled effectively with the aid of the Jacobi field comparison, enabling us to establish a spectrum of sub-linear regret guarantees. Interested readers may question the difficulty of generalizing these techniques from Hadamard manifolds to CAT\(()\) manifolds. Some hints toward this generalization are provided in Appendix E.3.

There are several promising directions for future research. First, there exists the potential to refine the regret bounds, particularly for strongly gsc-convex losses via a separation oracle. In the context of the separation oracle, reducing the dependence on the number of calls about the diameter of the decision set would be an intriguing objective. Moreover, devising an efficient method to optimize the linear optimization oracle objective, \(*{argmin}_{}, *{Exp}_{_{0}}^{-1}\), remains a notable open problem. This paper does not discuss the membership oracle, primarily because related work (Mhammedi, 2022; Lu et al., 2023) heavily relies on the convexity of the Minkowski functional in Euclidean space, a property not guaranteed to hold on Hadamard manifolds (Theorem 7). However, this does not rule out the potential for executing OCO or convex optimization on manifolds using a membership oracle. Thus, uncovering alternative strategies to tackle this issue remains a compelling research question.