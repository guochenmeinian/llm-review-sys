# Efficiently Identifying Watermarked Segments

in Mixed-Source Texts

 Xuandong Zhao

UC Berkeley

xuandongzhao@berkeley.edu

&Chenwen Liao

Zhejiang University

liaochenwen@zju.edu.cn

Yu-Xiang Wang

UC San Diego

yuxiangw@ucsd.edu

&Lei Li

Carnegie Mellon University

leili@cs.cmu.edu

Co-first authors.

###### Abstract

Text watermarks in large language models (LLMs) are increasingly used to detect synthetic text, mitigating misuse cases like fake news and academic dishonesty. While existing watermarking detection techniques primarily focus on classifying entire documents as watermarked or not, they often neglect the common scenario of identifying individual watermark segments within longer, mixed-source documents. Drawing inspiration from plagiarism detection systems, we propose two novel methods for partial watermark detection. First, we develop a geometry cover detection framework aimed at determining whether there is a watermark segment in long text. Second, we introduce an adaptive online learning algorithm to pinpoint the precise location of watermark segments within the text. Evaluated on three popular watermarking techniques (KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves high accuracy, significantly outperforming baseline methods. Moreover, our framework is adaptable to other watermarking techniques, offering new insights for precise watermark detection.

## 1 Introduction

Large Language Models (LLMs) have revolutionized human activities, enabling applications ranging from chatbots (OpenAI, 2022) to medical diagnostics (Google, 2024) and robotics (Ahn et al., 2024). Their ease of use, however, presents serious societal challenges. In education (Intelligent, 2024), students can effortlessly generate essays and homework answers, undermining academic integrity. In journalism (Blum, 2024), distinguishing credible news from fabricated content erodes public trust. The potential for malicious uses, such as phishing (Violino, 2023), and the risk of model collapse due to synthetic data (Shumailov et al., 2024), further underscore the urgent need to detect LLM-generated text and promote the responsible use of this powerful technology.

However, identifying AI-generated text is becoming increasingly difficult as LLMs reach human-like proficiency in various tasks. One line of research (OpenAI, 2023; Tian, 2023; Mitchell et al., 2023) trains machine learning models as AI detectors by collecting datasets consisting of both human and LLM-generated texts. Unfortunately, these approaches are often fragile (Shi et al., 2024) and error-prone (Liang et al., 2023), ultimately leading OpenAI to terminate its deployed detector (Kelly, 2023). Watermarking has emerged as a promising solution to this challenge. By embedding identifiable patterns or markers within the generated text, watermarks can signal whether a piece of text originates from an LLM.

Existing watermark detection methods (Aaronson, 2023; Kirchenbauer et al., 2023; Zhao et al., 2023; Kuditipudi et al., 2023; Christ et al., 2023; Hu et al., 2024) are primarily designed for text-level classification, labeling a piece of text as either watermarked or not. However, these methods are insufficient for many real-world scenarios where documents contain mixed-source texts, and only specific sections are LLM-generated. For instance, malicious actors might use LLMs to manipulatecertain sections of a news article to spread misinformation. Detecting watermarks within long, mixed-source texts presents a significant challenge, especially when aiming for subsequence-level detection with uncertainty quantification, similar to plagiarism detection systems like "Turnitin1". This is because the watermarked signal may be weakened throughout the increasing text length and may not be easily identifiable using conventional detection methods.

To bridge the gap, we propose partial watermark detection methods that offer a reliable solution for identifying watermark segments in long texts. A straightforward approach, which involves examining all possible segments of a text containing \(n\) tokens, yields an inefficiently high time complexity of \((n^{2})\). Instead, we employ the _geometric cover_ trick (Daniely et al., 2015) to partition the long texts into subsequences of varying lengths and then perform watermark detection within each interval. This approach, termed the _Geometric Cover Detector_ (GCD), enables efficient classification of whether a document contains any watermarked text in \((n n)\) time. However, GCD does not assign a score to every token, providing only a rough localization of watermark segments.

To refine this localization, we introduce the _Adaptive Online Locator_ (AOL). AOL reformulate the problem as an online denoising task, where each token score from the watermark detector serves as a noisy observation for the mean value of scores within watermark segments. By applying an adaptive online learning method, specifically the _Alligator_ algorithm (Baby et al., 2021), we retain the \((n n)\) time complexity while significantly improving the accuracy of detected segments.

We validate GCD and AOL using the C4 (Raffel et al., 2020) and Arxiv (Cohan et al., 2018) datasets, employing Llama (Touvron et al., 2023) and Mistral (Jiang et al., 2023) models for evaluation. Our empirical results demonstrate strong performance across both classification and localization tasks. In the classification task, our method consistently achieves a higher true positive rate compared to the baseline at the same false positive rate. For localization, we achieve an average intersection over union (IoU) score of over 0.55, far exceeding baseline methods.

In summary, our contributions are threefold:

1. We introduce novel approaches to watermark detection, moving beyond simple text-level classification to identification of watermark segments within long, mixed-source texts.
2. We employ the _geometric cover_ trick and the _Alligator_ algorithm from online learning to reliably detect and localize watermark segments efficiently and accurately.
3. We conduct extensive experiments on state-of-the-art public LLMs and diverse datasets. Our empirical results show that our approach significantly outperforms baseline methods.

## 2 Background and Related Work

Language Models and Watermarking.A language model \(\) is a statistical model that generates natural language text based on a preceding context. Given an input sequence \(x\) (prompt) and previous output \(y_{<t}=(y_{1},,y_{t-1})\), an autoregressive language model computes the probability distribution \(P_{}(|x,y_{<t})\) of the next token \(y_{t}\) in the vocabulary \(\). The full response is generated by iteratively sampling \(y_{t}\) from this distribution until a maximum length is reached or an end-token is generated. _Decoding-based watermarking_(Aaronson, 2023; Kirchenbauer et al., 2023; Zhao et al., 2023; Kuditipudi et al., 2023; Christ et al., 2023; Hu et al., 2024) modifies this text generation process by using a secret key k to transform the original next-token distribution \(P_{}(|x,y_{<t})\) into a new distribution. This new distribution is used to generate watermarked text containing an embedded watermark signal. The watermark detection algorithm then identifies this signal within a suspect text using the same watermark key k.

Red-Green Watermark.Red-Green (statistical) watermarking methods partition the vocabulary into two sets, "green" and "red", using a pseudorandom function \(R(h,,)\). This function takes as input the length of the preceding token sequence (\(h\)), a secret watermark key (k), and the target proportion of green tokens (\(\)). During text generation, the logits of green tokens are subtly increased by a small value \(\), resulting in a higher proportion of green tokens in the watermarked text compared to non-watermarked text. Two prominent Red-Green watermarking methods are KGW-Watermark (Kirchenbauer et al., 2023; 2024) and Unigram-Watermark (Zhao et al., 2023). KGW-Watermarkutilizes \(h 1\), considering the prefix for hashing. Unigram-Watermark employs fixed green and red lists, disregarding previous tokens by effectively setting \(h=0\) to enhance robustness. Watermark detection in both methods involves identifying each token's membership in the green or red list

\[(y)=_{t=1}^{n}(y_{t})\] (1)

and calculating the \(z\)-score of the entire sequence:\(z_{y}=(y)- n}{}\). This \(z\)-score reflects the deviation of the observed proportion of green tokens from the expected proportion \( n\), where \(n\) is the total number of tokens in the sequence. A significantly high \(z\)-score yields a small p-value, indicating the presence of the watermark.

Gumbel Watermark.The watermarking techniques proposed by Aaronson (2023) and Kuditipudi et al. (2023) can be described using a sampling algorithm based on the Gumbel trick (Zhao et al., 2024). This algorithm hashes the preceding \(h\) tokens using the key k to obtain a score \(r_{i}\) for each token \(i\) in the vocabulary \(\), where each \(r_{i}\) is uniformly distributed in \(\). The next token is chosen deterministically as follows: \(_{y_{t}}[ P(y_{i}|x_{<t})-(-(r_{y_{i}}))]\). Thus, given a random vector \(r(())^{||}\), \(-(-(r_{y_{i}}))\) follows a Gumbel(0,1) distribution. This results in a distortion-free deterministic sampling algorithm (for large \(h\)) for generating text. During detection, if the observed score

\[(y)=_{t=1}^{n}(1/(1-r_{y_{t}}))\] (2)

is high, the p-value is low, indicating the presence of the watermark.

## 3 Method

Problem StatementIdentifying watermark segments within a long text sequence \(y\) presents two key challenges. First, we need to design a classification rule \((x)\{0,1\}\) that determines whether \(y\) contains a watermark segment. To address this, we propose the _Geometric Cover Detector_ (GCD), which enables multi-scale watermark detection. Second, accurately locating the watermark segments \(y_{s_{i};e_{i}}\) within the full sequence \(y\) requires finding the start and end token indices, \(s_{i}\) and \(e_{i}\), for each watermark segment. We introduce the _Adaptive Online Locator_ (AOL) with the Aligator algorithm to precisely identify the position of the watermarked text span within the longer sequence.

Figure 1: Illustration of the watermark segment detection process. The input sequence could be mixed-source of watermark text and unwatermark text. The input sequence could be a mixed-source of watermarked text and unwatermarked text. We use geometric covers to partition the text and detect watermarks in intervals. We also formulate localization as an online denoising problem to reduce computational complexity. The example shown is drawn from the abstract of Bengio et al. (2024), with the watermarked part generated by a watermarked Mistral-7B model.

### Watermark Segment Classification

A straightforward approach to detect whether an article contains watermarked text is to pass it through the original watermark detector (as we discussed in Section 2). If the detection score from the original detector is larger than a threshold, the text contains a watermark; otherwise, no watermark is found. However, this approach is ineffective for long, mixed-source texts where only a small portion originates from the watermarked LLM. Since a large portion of the text lacks the watermark signal, the overall score for the entire document will be dominated by the unwatermarked portion, rendering the detection unreliable.

To overcome this limitation, we need a method that analyzes the text at different scales or chunks. If a chunk is flagged as watermarked, we can then classify the entire sequence as containing watermarked text. The question then becomes: how do we design these intervals or chunks effectively? We leverage the Geometric Cover (GC) technique introduced by Daniely et al. (2015) to construct an efficient collection of intervals for analysis.

Geometric Cover (GC) is a collection of intervals belonging to the set \(\), defined as follows:

\[=_{k 0}^{(k)},\,\,  k 0,\,\,^{(k)}=[i 2^{k},(i+1)  2^{k}-1]:i.\] (3)

Essentially, each \(^{(k)}\) represents a partition of \(\) into consecutive intervals of length \(2^{k}\). For example, \(^{(4)}\) contains all consecutive 16-token intervals. Due to this structure, each token belongs to \([ n]+1\) different intervals (as illustrated in Figure 1), and there are a total of \(n+n/2+n/4+n/8+=(n)\) intervals in the GC set. This allows us to establish a multi-scale watermark detection framework. Moreover, Lemma 5 from Daniely et al. (2015) ensures that for any unknown watermarked interval, there is a corresponding interval in the geometric cover that is fully contained within it and is at least _one-fourth_ its length. This ensures the effectiveness of watermark detection using the geometric cover framework.

Leveraging the GC construction, our multi-scale watermark detection framework divides the input text into segments based on the GC intervals. In real-world applications, we need to balance the granularity of the intervals. For instance, classifying a 4-token chunk as watermarked might not be convincing. Therefore, we start from higher-order intervals, such as \(^{(5)}\), which comprises all geometric cover intervals longer than 32 tokens.

Algorithm 1 outlines our approach. For each segment \(_{t}:y_{i_{t}:j_{t}}\) in the GC, we first compute a detection score using the appropriate watermark detector for the scheme employed (e.g., Equation 1 for Red-Green Watermark or Equation 2 for Gumbel Watermark). This score, along with the segment itself, is then passed to an FPR calibration function \(F\). This function estimates the FPR associated with the segment. Further details on FPR calibration can be found in the Appendix A.2.

If the estimated FPR, denoted as \(\), falls below a predefined target FPR (\(\)), we classify the entire sequence as containing a watermark. It is important to note that \(\) is set at the segment level. Using the union bound, consider a mixed-source text composed of \(n\) tokens. The geometric cover of the text is constructed from \((n)\) intervals. Let \(\) represent the false positive rate for each interval test (Type I error rate). In this case, the Family-Wise Error Rate (FWER), which is the probability of incorrectly classifying the entire document as watermarked, is bounded by \(n\).
While the previous section focused on detecting the presence of watermarks, simply knowing a watermark exists doesn't reveal which specific paragraphs warrant scrutiny. Here, we aim to localize the exact location of watermark text. A naive approach would involve iterating through all possible interval combinations within the sequence, applying the watermark detection rule to each segment \(y_{i:j}\) for all \(i\{1,,n\}\) and \(j\{i,,n\}\). While this brute-force method can identify watermark segments, its \((n^{2})\) time complexity makes it computationally expensive for long sequences.

Furthermore, relying solely on individual token scores for localization is unreliable due to the inherent noise in the watermarking process. To address this issue, we propose to formulate it as a **sequence denoising problem** (a.k.a., smoothing or nonparametric regression) so we can provide a pointwise estimate of the _expected_ detection score _for each token_. Specifically, the denoising algorithm tasks a sequence of noisy observations \(s_{1},...,s_{n}\) and output \(\{_{t}\}_{t[n]}\) as an estimate to \(\{[s_{t}]\}_{t[n]}\).

As an example, for the Green-Red Watermark, the sequence of noisy observations \(\{s_{t}=(y_{t}Tokens})\}_{t[n]}\) consists of Bernoulli random variables. The expectation \([s_{t}]=\) if \(y_{t}\) is not watermarked and \([s_{t}]>\) otherwise. For the Gumbel Watermark, the noisy observations \(\{s_{t}=(1/(1-r_{y_{t}}))\}_{t[n]}\) consists of exponential random variables satisfying \([s_{t}]=1\) if \(y_{t}\) is unwatermarked and larger otherwise. The intuition is that, while individually they are too noisy, if we average them appropriately within a local neighborhood, we can substantially reduce the noise. If we can accurately estimate the sequence \([s_{i}]\), we can localize watermarked segments by simply thresholding the estimated score pointwise.

The challenge, again, is that we do not know the appropriate window size to use. In fact, the appropriate size of the window should be larger if \(s_{i}\) is in the interior of a long segment of either watermarked or unwatermarked text. The sharp toggles among text from different sources add additional challenges to most smoothing algorithms.

For these reasons, we employ the Aligator (**A**ggregation of on**L**I**ne **a**ver**A**G**es using **A** geome**T**ric **c**O**ve**R**) algorithm (Baby et al., 2021). In short, Aligator is an online smoothing algorithm that optimally competes with an oracle that knows the segments of watermarked sequences ahead of time. The algorithm employs a Geometric Cover approach internally, where words positioned mid-paragraph are typically included in multiple intervals of varying lengths for updates. Notably, Aligator provides the following estimation guarantee:

\[_{t}(_{t}-[s_{t}])^{2}=(\{ n^{-1}(1+_{t=2}^{n}_{[s_{t}][s_{t-1}]}),n^{-1}  n^{-2/3}(_{t=2}^{n}|[s_{t}]-[s_{t-1}]|)\} ).\]

Moreover, for all segments with start/end indices \((i,j)[n]^{2}\), i.e.

\[_{t=i}^{j}(_{t}-_{t^{}=i}^{j} [s_{t^{}}])^{2}(1/).\]

This ensures that for every segment, the estimated value is as accurate as statistically permitted. The time complexity for Aligator is \((n n)\). For a detailed implementation of Aligator, please refer to the original paper (Baby et al., 2021). For the theoretical results, see (Baby and Wang, 2021).

**Circular Aligator.** To mitigate the boundary effects common in online learning, where prediction accuracy suffers at the beginning and end of sequences, we introduce a circular starting strategy. Instead of processing the text linearly, we treat it as a circular buffer. For each iteration, we randomly choose a starting point and traverse the entire sequence, effectively mitigating edge effects. The final prediction for each token is then obtained by averaging the predictions across all iterations.

Finally, we apply a threshold to this denoised average score function to delineate the boundaries of watermark segments within the text (as illustrated in Figure 1). The high-level implementation of this method is detailed in Algorithm 2. This approach enables us to precisely identify the location of suspected plagiarism within large documents with high confidence, facilitating further investigation and verification.

## 4 Experiment

Datasets and Mixed-source TextsWe utilize two text datasets: C4 (Raffel et al., 2020) and Arxiv (Cohan et al., 2018). The "Colossal Clean Crawled Corpus" (C4) dataset is a collection of English-language text sourced from the public Common Crawl web scrape, a rich source for unwatermarked human-written text. We use random samples from the news-like subset of the C4 dataset in our experiments. The Arxiv dataset is part of the Scientific-Papers dataset collected from scientific repositories, arXiv.org and PubMed.com. We use the Arxiv split in our experiments, which contains abstracts and articles of scientific papers. Both datasets are used to construct watermarked positive samples and human-written negative samples. To transform unwatermarked samples into partially watermarked samples, we randomly select 3-5 sentences in a long text and set them as prompts. Then, we generate 300 tokens of watermarked text conditioned on the prompts using large language models. The generated responses replace the original suffix sentences after the prompt. In this way, we embed 300-token watermarks into 3000-token contexts from the datasets, making the watermark 10% of the mix-sourced text. We randomly choose the position of the watermark in this longer context and record the locations for later testing. Our goal is to determine if a document contains watermark text and locate its position. For each dataset, we use 500 samples as the test set to show the results.

Language Models and Watermarking Methods.We use the publicly available LLaMA-7B (Touvron et al., 2023) and Mistral (Jiang et al., 2023) models. To verify the general applicability of the watermark detection methods, we select three watermarking techniques: Gumbel-Watermark (Aaronson, 2023), KGW-Watermark (Kirchenbauer et al., 2023), and Unigram-Watermark (Zhao et al., 2023). These methods represent the state-of-the-art watermarking approaches for large language models, offering high quality, detectability, and robustness against adversarial attacks. For all watermarking generations, we configure the temperature to 1.0 for multinomial sampling. Additionally, for KGW-Watermark and Unigram-Watermark, we set the green token ratio \(\) to 0.5 and the perturbation \(\) to 2.0.

BaselinesIn watermark segment detection, we use the original watermark detector in each watermarking method as the Vanilla baseline to compare with our approach GCD. In watermark segment localization, we use RoBERTa (Liu et al., 2019) models for comparing with our method AOL. We train each RoBERTa (designed for different watermarking methods) to predict whether a sequence is a watermarked sequence or not, given the watermark detection scores \(r\) for each token. We add an extra fully connected layer after getting the representation of the [CLS] token. We construct 1000 training samples with 60 token scores as input and the binary label of this segment as the label. We train the RoBERTa model for 20 epochs and enable early stopping if the loss converges. It can reach over 90% accuracy in the training set. During testing on mixed-source text, we employ the sliding window idea to test each chunk for watermarks and then calculate the IoU score.

EvaluationFor the watermarked text classification task, we report the true positive rates (TPR) based on different specified false positive rates (FPR). Maintaining a low FPR is critical to ensure that human-written text is rarely misclassified as LLM-generated text.

Since the FPR at the per-instance level differs from the document-level FPR, we calibrate FPR to three distinct levels in each scenario to enable fair comparisons. Specifically, we manipulate the pre-segment FPR (Seg-FPR) by adjusting the threshold parameter \(\) as outlined in Algorithm 1. Then, we can get the empirical document FPR (Doc-FPR) by evaluating our method GCD based onpure natural text. For Vanilla, we set the FPR according to GCD's empirical FPR and subsequently test for its empirical TPR.

For locating specific watermark segments, we calculate the Intersection over Union (IoU) score to measure the accuracy of watermark segment localization. The IoU score computes the ratio of the intersection and union between the ground truth and inference, serving as one of the main metrics for evaluating the accuracy of object detection algorithms:

\[=}{}=|}{||}\]

### Detection Results

Watermark Segment Classification ResultsAs shown in Table 1, our proposed _Geometric Cover Detector_ (GCD) consistently outperforms the baseline Vanilla method across all watermarking techniques and large language models on both the C4 and Arixy datasets. The robustness of GCD across diverse conditions underscores its effectiveness in watermark segment classification, demonstrating clear superiority over Vanilla. Additionally, we observe that Vanilla exhibits near-zero detection rates when the target false positive rate is low. This suggests that Vanilla struggles to detect watermarked segments in longer contexts, as the watermark signal weakens, rendering the simpler detector ineffective.

Precise Watermark Position Localization ResultsFor the watermark position localization task, we evaluate our proposed method AOL against the baseline method RoBERTa (Table 2). We calculate the average IoU score to quantify the precision of the watermark localization. Our method consistently outperforms the baseline across all test settings. For example, on the C4 dataset using the mistral-7B model, AOL achieves a substantially higher IoU score of 0.809 compared to 0.301 for RoBERTa. We also test AOL's ability to detect multiple watermarks by inserting 3x300-token Gumbel watermarks (generated by Mistral-7B) into 6000-token texts. Across 200 samples, the average IoU for detecting the watermarks is 0.802, demonstrating AOL's effectiveness for multiple watermark detection. Figure 2 provides a case example illustrating the improved localization performance of AOL on the Gumbel watermark with the Mistral-7B model. The upper image shows the boundary effects of using online learning. The lower image demonstrates more precise localization resulting from the circular starting strategy with 10 random starting points.

  
**Method** &  &  &  \\   \\ Sg-FPR & 1e-5 & 5e-5 & 1e-4 & 1e-4 & 2e-4 & 0.001 & 1e-4 & 0.001 & 0.010 \\ Doc-FPR & 0.03\(\)0.04 & 0.076 & 0.082 & 0.002 & 0.004 & 0.030 & 0.026 & 0.080 & 0.358 \\ Vanilla & 0.602 & 0.676 & 0.692 & 0.006 & 0.006 & 0.058 & 0.650 & 0.762 & 0.918 \\ GCD & **0.912** & **0.934** & **0.934** & **0.874** & **0.906** & **0.958** & **1.000** & **1.000** & **1.000** \\   \\ Sg-FPR & 1e-5 & 1e-4 & 2e-4 & 0.001 & 0.010 & 0.020 & 1e-4 & 5e-4 & 0.001 \\ Doc-FPR & 0.037 & 0.087 & 0.153 & 0.001 & 0.012 & 0.040 & 0.024 & 0.046 & 0.054 \\ Vanilla & 0.697 & 0.830 & 0.877 & 0.000 & 0.012 & 0.030 & 0.690 & 0.760 & 0.780 \\ GCD & **0.960** & **0.903** & **0.990** & **0.722** & **0.974** & **1.000** & **0.970** & **0.980** & **0.990** \\   \\ Sg-FPR & 1e-5 & 5e-5 & 2e-4 & 1e-4 & 2e-4 & 0.001 & 1e-4 & 0.001 & 0.010 \\ Doc-FPR & 0.068 & 0.116 & 0.186 & 1e-4 & 2e-4 & 0.014 & 0.024 & 0.066 & 0.280 \\ Vanilla & 0.844 & 0.896 & 0.908 & 0.000 & 0.000 & 0.026 & 0.593 & 0.655 & 0.825 \\ GCD & **0.990** & **0.994** & **0.996** & **0.892** & **0.922** & **0.974** & **0.958** & **0.978** & **1.000** \\   \\ Sg-FPR & 1e-5 & 1e-4 & 2e-4 & 0.001 & 0.020 & 0.020 & 1e-5 & 1e-4 & 2e-4 \\ Doc-FPR & 0.033 & 0.197 & 0.253 & 0.001 & 0.028 & 0.036 & 0.082 & 0.192 & 0.230 \\ Vanilla & 0.757 & 0.838 & 0.907 & 0.002 & 0.032 & 0.088 & 0.860 & 0.930 & 0.930 \\ GCD & **0.967** & **0.990** & **1.000** & **0.566** & **0.920** & **0.964** & **0.950** & **0.960** & **0.970** \\   

Table 1: True Positive Rate (TPR) at various False Positive Rate (FPR) levels for baseline Vanilla and our method GCD. For each setting, we select three distinct segment-level FPRs (Seg-FPR) and compare the performance of Vanilla and GCD at equivalent document-level FPRs (Doc-FPR). GCD consistently outperforms Vanilla across different models and datasets.

### Detection Results with Different Lengths

As mentioned previously, watermark detection can easily be disturbed by long natural paragraphs, and our approach aims to minimize the effect of length scale. We test our method on texts of varying total lengths, ranging from 3000 to 18000 tokens, while keeping the watermark segment length constant at 300 tokens. The same detection threshold and parameters used for 3000 total tokens are applied across all lengths. We find that the Gumbel watermark segment classification performs well even as total length increases, as shown in Table 3. For repetitive watermarks like KGW and Uniform, longer texts in the Geometry Cover also cause a decrease in segment detection, as shown in Figure 3. However, compared to directly detecting on the whole paragraph, this decrease is more acceptable. Importantly, the parameters used in these tests are identical to those for 3000 tokens. In practice though, for texts of different lengths, the number of starting points in the circular buffer should be adjusted accordingly. This way, similarly strong results can be achieved as with 3000 tokens.

    &  &  \\   & & FPR-1 & FPR-2 & FPR-3 \\   & Vanilla & 0.000 & 0.012 & 0.038 \\  & GCD & **0.722** & **0.974** & **1.000** \\   & Vanilla & 0.000 & 0.000 & 0.005 \\  & GCD & **0.730** & **0.980** & **1.000** \\   & Vanilla & 0.000 & 0.000 & 0.000 \\  & GCD & **0.730** & **0.980** & **1.000** \\   & Vanilla & 0.000 & 0.000 & 0.000 \\  & GCD & **0.730** & **0.980** & **1.000** \\   

Table 3: Vanilla and GCD watermark segment classification result with Unigram Watermark on Mistral-7B with different target false positive rates.

  
**Method** & **KGW-WM LU** & **Unigram-WM IIoU** & **Gumbel-WM IIoU** \\  _CG Detector_, _Llama-7B_ & & & \\ RoBERTA & 0.563 & 0.444 & 0.535 \\ AOL & **0.657** & **0.818** & **0.758** \\  _C Detector_, _Mistral-7B_ & & & \\ RoBERTA & 0.238 & 0.019 & 0.301 \\ AOL & **0.620** & **0.790** & **0.809** \\  _Arous Dataset_, _Llama-7B_ & & & \\ RoBERTA & 0.321 & 0.519 & 0.579 \\  _Arous Dataset_, _Mistral-7B_ & & & \\ RoBERTA & 0.372 & 0.249 & 0.421 \\ AOL & **0.571** & **0.682** & **0.802** \\   

Table 2: Precise Watermark Position Localization Performance: Intersection over Union (IoU) score for baseline RoBERTA and our method AOL. AOL consistently outperforms RoBERTA.

Figure 3: Watermark localization results using different watermarking methods and varying text lengths.

Figure 2: Example of precise watermark localization using AOL with Gumbel Watermark. Light green lines show token scores, and dark green lines show predicted mean scores. The horizontal dashed line shows the score threshold \(=1.3\). The vertical dashed line marks the original watermark position. The top image demonstrates inaccurate localization from a single pass of the Aligator algorithm, highlighting boundary artifacts. In contrast, the bottom image shows precise localization achieved by AOLâ€™s circular initialization strategy with \(m=10\) random starts.

## 5 Conclusion

This paper introduces novel methods for partial watermark detection in LLM-generated text, addressing the critical need for identifying watermark segments within longer, mixed-source documents. By leveraging the geometric cover trick and the Alligator algorithm, our approach achieves high accuracy in both classifying and localizing watermarks, significantly outperforming baseline methods. These advancements pave the way for more robust and reliable detection of synthetic text, promoting responsible use and mitigating potential misuse of LLMs in various domains.