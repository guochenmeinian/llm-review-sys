# ID and OOD Performance Are Sometimes

Inversely Correlated on Real-world Datasets

 Damien Teney

Idiap Research Institute, Switzerland

damien.teney@idiap.ch

&Yong Lin

Hong Kong Uni. of Science and Technology

ylindf@connect.ust.hk

&Seong Joon Oh

University of Tubingen, Germany

coallaoh@gmail.com

&Ehsan Abbasnejad

University of Adelaide, Australia

ehsan.abbasnejad@adelaide.edu.au

###### Abstract

Context.Several studies have compared the in-distribution (ID) and out-of-distribution (OOD) performance of models in computer vision and NLP. They report a frequent positive correlation, but surprisingly, almost never an inverse correlation that would be indicative of a necessary trade-off. Such inverse patterns are possible theoretically, and their occurrence in practice is important to determine whether ID performance can serve as a proxy for OOD generalization.

Findings.This paper shows that inverse correlations between ID and OOD performance do happen with multiple real-world datasets, not only in artificial worst-case settings. We explain theoretically how these cases arise and how past studies missed them because of improper methodologies that examined a biased selection of models.

Implications.Our observations lead to recommendations that contradict those found in much of the current literature.

* High OOD performance sometimes requires trading off ID performance.
* Focusing on ID performance alone may not lead to optimal OOD performance. It may produce diminishing (eventually negative) returns in OOD performance.
* In these cases, studies on OOD generalization that use ID performance for model selection (a common recommended practice) will necessarily miss the best-performing models, making these studies blind to a whole range of phenomena.

## 1 Introduction

Past observations.This paper complements existing studies that empirically compare in-distribution (ID) and out-of-distribution1 (OOD) performance of deep learning models . It has long been known that models applied to OOD data suffer a drop in performance, e.g. in classification accuracy. The above studies show that, despite this gap, ID and OOD performance are often positively correlated2 across models on benchmarks in computer vision  and NLP .

Past explanations.Frequent positive correlations are surprising because inverse correlations are similarly possible theoretically. Indeed, ID and OOD data contain different associations between labels and features. One could imagine e.g. that an image background is associated with class \(_{1}\) ID and class \(_{2}\) OOD. The more a model relies on the presence of this background, the better itsID performance but the worse its OOD performance, resulting in an inverse correlation. The fact that inverse correlations were almost never observed in practice was explained with the fact that **real-world benchmarks only contain mild distribution shifts**. However, we will show in Section 4 that the reason may also be in flawed experimental design.

A recent study  shows various correlation patterns across datasets, but never _inverse_ ones:

"_We did not observe any trade-off between accuracy and robustness, where more accurate models would overfit to spurious features that do not generalize._" 

This does not match our past general experience, so we set out to expose in this paper actual cases of inverse correlations, using popular datasets used for OOD research. Our results show inverse correlations across models trained with varying numbers of epochs and random seeds. These patterns are even more striking when models are trained with a regularizer that identifies a diverse set of solutions to the ERM objective .

**Explaining inverse correlations.** We name the underlying cause a "_misspecification_", an extension of the "_underspecification_" used previously to explain why models with similar ID performance can vary in OOD performance [5; 16; 47]. In cases of misspecification, the standard ERM objective (empirical risk minimization) aligns with a maximization of the ID performance but is in conflict with the OOD performance. ID and OOD metrics then vary inversely to one another. In Appendix C, we present a minimal theoretical example to illustrate how inverse correlations originate from the presence of both robust and spurious features in the data. In Section 7, we show that different patterns of ID/OOD performance occur with different magnitudes of distribution shifts.

**Summary of contributions.**

* An empirical examination of ID vs. OOD performance on several real-world datasets showing inverse correlation patterns that conflict with past evidence (Section 3).
* An explanation and an empirical verification of why past studies missed such patterns (Section 4).
* A theoretical analysis showing how inverse correlation patterns can occur (Appendix C).
* A revision of conclusions and recommendations made in past studies (Section 8).

## 2 Previously-observed patterns of ID/OOD performance

Multiple studies conclude that ID and OOD performance vary jointly across models on many real-world datasets [6; 25; 42]. Miller et al.  report an almost-systematic linear correlation3 between probit-scaled ID and OOD accuracies. Mania and Sra  explain this trend with the fact that many benchmarks contain only mild shifts.4 Andreassen et al.  find that pretrained models perform "above the linear trend" in early stages of fine-tuning. The OOD accuracy then rises more quickly than the ID accuracy early on, though the final accuracies agree with a linear trend across models.

Most recently, the large-scale study of Wenzel et al.  is more nuanced: they observe a linear trend only on some datasets. Their setup consists in fine-tuning an ImageNet-pretrained model on a chosen dataset and evaluating it on ID and OOD splits w.r.t. this dataset. They repeat the procedure with a variety of datasets, architectures, and other implementation choices such as data augmentations. The scatter plots of ID/OOD accuracy in  show four typical patterns (Figure 2).

Figure 1: Several past studies suggest that positive correlations between ID/OOD performance are ubiquitous. This paper shows empirically and theoretically why inverse correlations are also possible and can be accidentally overlooked. The possibility of ID/OOD trade-offs goes counter the common practice of model selection based on ID performance, which is recommended in many benchmarks for OOD generalization.

1. **Increasing line (positive correlation): mild distribution shift.** ID and OOD accuracies are positively correlated. Focusing on classical (ID) generalization brings concurrent OOD improvements.
2. **Vertical line: underspecification**[5; 16; 47]. Different models obtain a similar high ID performance but different OOD performance. The objective of high ID performance does not sufficiently constrains the learning. Typically, multiple features in the data (a.k.a. biased or spurious features) can be used to obtain high ID performance, but not all of them are equally reliable on OOD data. To improve OOD performance, additional task-specific information is necessary, e.g. additional supervision or inductive biases (custom architectures, regularizers, etc.).
3. **Horizontal line, low OOD accuracy: severe distribution shift.** No model performs well OOD. A severe shift prevents any transfer between training and OOD test data. The task needs to be significantly more constrained e.g. with task-specific inductive biases.
4. **No clear trend: underspecification.** Models show a variety of ID and OOD accuracies. The difference with (2) is the wider variety along the ID axis, e.g. because a difficult learning task yields solutions of lower ID accuracy from local minima of the ERM objective.

The authors note the absence of decreasing patterns, which are however possible in theory.

5. **Decreasing line (inverse correlation): misspecification.** The highest accuracy ID and OOD are achieved by different models. Optima of the ERM objective, which are expected to be optima in ID performance, do not correspond to optima in OOD performance. This implies a trade-off: higher OOD performance is possible at the cost of lower ID performance.

**When does an inverse correlation occur between ID and OOD performance?** Intuitively, it can occur when a pattern in the data is predictive in one distribution and misleading in the other. For example, object classes \(_{1}\) and \(_{2}\) are respectively associated with image backgrounds \(_{1}\) and \(_{2}\) in ID data, and respectively \(_{2}\) in \(_{1}\) (swapped) in OOD data. Relying on the background can improve performance on either distribution but not both simultaneously. While such severe shifts might be rare, the next section presents an actual example.

## 3 New observations: inversely-correlated ID / OOD performance

This section is an in-depth examination using the WILDS-Camelyon17 dataset . We include experiments on other datasets in Section 5.1 and Appendix B. Here, we use Camelyon17 in a manner similar to Wenzel et al. . These authors evaluated different architectures and assumed that their different inductive biases can produce models that cover a range of ID / OOD accuracies. In contrast and for simplicity, we rely instead on different random seeds since  showed that this is sufficient to

Figure 3: Our new observations show that higher OOD accuracy can sometimes be traded for lower ID accuracy. Each panel corresponds to a different pretraining seed. Each dot represents a linear classifier on frozen features, re-trained with a different seed and/or number of epochs. They are re-trained with standard ERM (red dots ) or a diversity-inducing method (gray dots ). The latter set includes models with higher OOD / lower ID accuracies. See Appendix A for additional plots.

Figure 2: Typical patterns observed in  (reproduced with permission).

cover a variety of ID / OOD accuracies on this dataset. We also want to minimize the experimenter's bias. Therefore, to further increase variety without the manual arbitrary selection of architectures of , we also train models with the general-purpose, diversity-inducing method of .

**Background: learning diverse solutions.**

A range of methods have been proposed to train multiple networks to similar ID performance while differing in other properties such as OOD generalization. These "diversification" methods are relevant in cases of underspecification  when the standard ERM objective does not constrain the solution space to a unique one. Recent methods train multiple models (in parallel or sequentially) while encouraging diversity in **feature space**, **prediction space**, or **gradient space**. **We use method of  that encourages gradient diversity**.

The method trains many copies of the same model in parallel - in our case, a linear classifier on top of a frozen DenseNet backbone (see Figure 4). The models are optimized by standard SGD to minimize the sum of a standard classification loss (cross-entropy) with a diversity loss that encourages diversity across models. Using \(\) a weight hyperparameter, the complete loss is \(\!=\!_{}+\,_{ }\). The second term encourages each copy to rely on different features by minimizing the mutual alignment of input gradients:

\[_{}=_{}\,_{i=1}^{n} \,\,_{j=i+1}^{n}\,\,_{}\,g_{i}()._{}\,g_{ j}(),\;\;=f().\] (1)

These pairwise dot products quantify the mutual alignment of the gradients. Intuitively, minimizing (1) makes each model locally sensitive along different directions in its input space.

Assuming that \(g\) produces a vector of logits (as many as there are classes), \(_{}\,g()\) refers to the gradient of the largest logit w.r.t. the classifier's input \(\). We use \(n\)=24 copies and a weight \(\)=10 that were selected for giving a wide range of ID accuracies. See  for details about the method.

**Experimental details.** We use 10 DenseNet-121 models pretrained by the authors of the dataset with different seeds . For each, we re-train the last linear layer from a random initialization for 1 to 10 epochs, keeping other layers frozen. These are referred to as **ERM models**. We perform this re-training with 10 different seeds which gives \(10^{3}\) ERM models (10 pretraining seeds \(\) 10 re-training seeds \(\) 10 numbers of epochs). In addition, we repeat this re-training of the last layer with the diversity-inducing method of  (details in the box below). These are referred to as **diverse models**. Each run of the methods produces 24 different models, giving a total of \(10^{3}.24\) such models.

**Results with ERM models.** In Figure 3 we plot the ID vs. OOD accuracy of ERM models as red dots (\(\)). Each panel corresponds to a different pretraining seed. The variation across panels (note the different Y-axis limits) shows that OOD performance varies across pre-training seeds even though the ID accuracy is similar, as noted by . Our new observations are visible _within_ each panel. The dots (models) in any panel differ in their re-training seed and/or number of epochs. The seeds induce little variation, but the number of epochs produce patterns of decreasing trend (negative correlation). Despite the narrow ID variation (X axis), careful inspection confirms that the pattern appears in nearly all cases (see Appendix A for zoomed-in plots).

**Results with diverse models.** We plot models trained with the diversity method  as gray dots (\(\)). These models cover a wider range of accuracies and form patterns that extend those of ERM models. The decreasing trend is now obvious. This trend is clearly juxtaposed with a _rising_ trend of positive ID / OOD correlation. This suggests a point of highest OOD performance after which the model overfits to ID data. Appendix A shows similar results with other pretraining seeds. The patterns are

Figure 4: Method used to train a diverse set of models. Each training image \(\) goes through a frozen pretrained DenseNet to produce features \(\)=\(f()\). We train a set of linear classifiers \(\{g_{i}\}_{i=1}^{n}\) on these features. A diversity loss minimizes the pairwise similarity between their input gradients.

not always clearly discernible because large regions of the performance landscape are not covered, despite the diversity-inducing method. We further discuss this issue next.

## 4 Past studies missed negative correlations due to biased sampling of models

We identified several factors explaining the discrepancy between our observations and past studies.

* ERM models alone do not always form clear patterns (red dots \(\) in Figure 3). In our observations, the **models trained with a diversity-inducing method** (gray dots \(\)) were key in making the suspected patterns more obvious, because they cover a wider range of accuracies.
* The ID / OOD trade-off varies during training, as noted by . This **variation across training epochs** is responsible for much of the newly observed patterns. However, models of different architectures or pretraining seeds are not always comparable with one another because of shifts in their overall performance (see e.g. different Y-axis limits across panels in Figure 3). Therefore the performance across epochs should be analyzed individually per model.
* The "inverse correlation" patterns are not equally visible with all **pretraining seeds**. In some cases, a careful examination of zoomed-in plots is necessary, see Appendix A. This is a reminder that stochastic factors in deep learning can have large effects and that empirical studies should randomize them as much as possible.

To demonstrate these points, we plot our data (same as in Figure 3) while keeping only the ERM models trained for 10 epochs and including all pretraining seeds on the same panel. Figure 5 shows that these small changes reproduce the vertical line observed by Wenzel et al. , which completely misses the inverse correlations patterns visible in Figure 3.

A general explanation is that past studies **undersample regions of the ID / OOD performance space**. They usually consider a variety of architectures in an attempt to sample this space. However, different architectures do not necessarily behave very differently from one another (see the box below). We lack methods to reliably identify models of high OOD performance, but the diversity-inducing method that we use yields models spanning a wide range of the performance spectrum.

**Why isn't it sufficient to evaluate a variety of architectures?**

Different architectures do not necessarily induce radically different behaviour . Even CNNs and vision transformers have similar failure modes . Distinct architectures can share similar inductive biases due e.g. to SGD, the simplicity bias [39; 40], or neural anisotropies .

## 5 Occurrences in other datasets

In addition to Camelyon17, we performed experiments on five additional datasets used in OOD research and found inverse correlations on four out of five. We present two of them below and the others in Appendix B.

In these experiments, we train standard architectures with well-known methods: standard ERM, simple class balancing , mixup , selective mixup , and post hoc adjustment for label shift  (we did not use the diversification method from Section 3). We repeat every experiment with 10 seeds and plot the ID / OOD accuracy from every epoch in the following figures.

Figure 5: We plot again the ERM models of Figure 3 (red dots \(\)) but **only include models trained for a fixed number of epochs** and combine all pretraining seeds in the same plot. This reproduces the vertical line from , which completely misses the patterns of inverse correlation.

### WildTime-arXiv

**Data.** The WildTime-arXiv  dataset contains text abstracts from arXiv preprints. The task is to predict each paper's category among 172 classes. The ID and OOD splits are made of data from different time periods.

**Methods.** We fine-tune a standard BERT-tiny model with a new linear head, using any of these well-known methods: standard ERM, simple class balancing , mixup , selective mixup , and post hoc adjustment for label shift  (we did not use the diversification method from Section 3). We repeat every experiment with 10 seeds and record the ID and OOD accuracy at every training epoch. We then plot each of these points in Figure 6 and highlight the epoch of highest ID or OOD accuracy per run (method/seed combination).

**Results.** As discussed in Section 5.1, there is a clear trade-off both within methods (i.e. across seeds and epochs) and across methods.

### Waterbirds

**Data.** The **waterbirds** dataset  is a synthetic dataset widely used for evaluating OOD generalization. The task is to classify images of birds into 2 classes. The image backgrounds are also of two types, and the correlation between birds and background is reversed across the training and test splits. The standard metric is the worst-group accuracy, where each group is any of the 4 combinations of bird/background.

**Methods.** We follow the same procedure as described above. We experiment two classes of architectures: ResNet-50 models pretrained on ImageNet and fine-tuned on waterbirds, and linear classifiers trained of features from the same frozen (non-fine-tuned) ResNet-50.

**Results.** We observe in Figure 7 patterns of inverse correlations in both cases.

### Occurrences in the existing literature

A literature review reveals other cases across a range of topics, many of which were not particularly highlighted by their authors and required close examination.

* A close examination of [9, Table 2] reveals a clear inverse pattern on three benchmarks for natural language inference (NLI). This task is known for biases and shortcuts in the training data, and the OOD test sets in these benchmarks correspond to severe distribution shifts. Our

Figure 6: Results on WildTime-arXiv.

Figure 7: Results on waterbirds with linear probing (left) and fine-tuned ResNet-50 models.

proposed explanation (right end of the spectrum in Figure 8) therefore aligns with these observations. Experiments on question answering from the same authors use data with milder distribution shifts. Correspondingly, they show instead a positive correlation.
* Kaplan et al. [14, Figure 7] find that the CIFAR dataset contains a subset (CIFAR-10-Neg) on which the performance of visual classifiers is inversely correlated with their ID performance.
* albeit within an overall positive trend.
* McCoy et al.  show that BERT models trained with different seeds vary widely in performance on the HANS benchmark for NLI while their ID performance on MNLI is similar.
* Naganuma et al.  performed an extensive evaluation on OOD benchmarks after the initial release of this paper. They consider a wider range of hyperparameters than existing works, and as expected from our claims, they observe a broader range of ID / OOD relations than the "linear trend".
* Liang et al.  focused on datasets with subpopulation shifts and analyzed the relation between performance across subgroups. They also find a linear trend to be an inaccurate description. They observe non-linear relations with a transition point between models showing a negative correlation and others showing a positive one.
* In their paper on _Model recycling_, Rame et al.  include plots of ID / OOD performance (Appendix A) that are nothing like linear correlations. Instead, bell-shaped curves unambiguously indicate a necessary trade-off between ID and OOD performance.
* Work on adversarial examples has examined the trade-off between standard and adversarially-robust accuracy . This agrees with our explanations (Figure 8) since adversarial inputs correspond to extreme distribution shifts.
* The literature on transfer learning has previously shown occasional cases of negative transfer, a related phenomenon where improving performance in one domain hurts in others .

## 6 Theoretical analysis

The theoretical possibility of an inverse ID/OOD correlation is quite obvious. In Appendix C), we show below theoretically how to construct a toy example where ID and OOD are negatively correlated. We then explain how ID and OOD metrics can diverge as training progresses, or as one trains a model on data containing more and more spurious features.

However, the theoretical analysis is not central to our argument. The more important open question is whether these cases happen merely in pathological situations, or also in real-world data. Prior work strongly argued for the former, but we show that the latter is actually correct. The key contribution of this paper is therefore in the observations on actual data, rather than in a theoretical argument.

## 7 Ordering ID / OOD patterns according to shift magnitude

The above analysis shows that inverse correlation patterns are essentially due to the presence of spurious features, i.e. features whose predictive relation with the target in ID data becomes misleading OOD. Occurrences of spurious features increase with the magnitude of the distribution shift. Therefore, the possible patterns in ID / OOD performance presented in Section 2 can be ordered according to the magnitude of the distribution shift they are likely to occur with (see Figure 8).

With the smallest distribution shifts (leftmost case in Figure 8), for example training on ImageNet and testing on its replication ImageNet v2 , ID validation performance closely correlates with OOD test performance. This OOD setting is the easiest because one can focus on improving classical generalization and reap concurrent improvements OOD.

With a larger distribution shift, more features are likely to be spurious, which is likely to break the ID / OOD correlation. The task of improving OOD performance is likely to be under- or misspecified, i.e. there is not enough information to determine which features a model should rely on.

Valid approaches include modifying the training objective, injecting task-specific information (e.g. building-in invariance to rotations as in ), well-chosen data augmentations, or inhomogeneous training data such as multiple training environments  or counterfactuals .

With extreme distribution shifts, most predictive features are overwhelmingly spurious and it is very difficult to learn any one relevant in OOD data (rightmost case in Figure 8).

The proposed ordering of patterns is rather informal and could be further developed following the two axes of diversity shifts and correlation shifts proposed by  (see also ). More recently,  showed that the suitability of various methods for OOD generalization depends on particularities of the underlying causal structure of the task - which must therefore be known to select a suitable method. Identifying which ID / OOD patterns occur with particular causal structures might serve as a tool to understand the type of OOD situation one is facing and identify a suitable method.

## 8 Revisiting advice from past studies

We have established that observations in past studies were incomplete. We now bring nuance to some recommendations and conclusions made in these studies.

* **Focusing on a single metric.** [leftmargin=*]
* _We see the following potential prescriptive outcomes (...) correlation between OOD and ID performance can simplify model development since **we can focus on a single metric.**_"  We demonstrated that inverse correlations do occur, hence there exist scenarios where ID performance is misleading. Relying on a single metric for model development is ill-advised  especially if it cannot capture necessary trade-offs. We recommend tracking multiple metrics e.g. performance on multiple distributions or interpretable predictions on representative test points.
* **Improving ID performance for OOD robustness.** [leftmargin=*]
* **_If practitioners want to make the model more robust on OOD data, **the main focus should be to improve the ID classification error**. (...) We speculate that the risk of overfitting large pretrained models to the downstream test set is minimal, and it seems to be not a good strategy to, e.g., reduce the capacity of the model in the hope of better OOD generalization._"  This recommendation assumes the persistence of a positive correlation. On the opposite, we saw that a positive correlation can precede a regime of inverse correlation (Figure 3, left panels). If the goal is to improve OOD, focusing on ID performance is a blind alley since this goal requires to increase ID performance at times and reduce it at others.
* **Future achievable OOD performance.** As obvious as it is, it feels necessary to point out that empirical studies only chart regimes achievable with existing methods. Observations have limited predictive power, hence more care seems warranted when deriving prescriptive recommendations from empirical evidence. The best possible performance e.g. on Camelyon17 is obviously not limited to the Pareto front of our experiments. The state of the art on this dataset [33; 51] injects additional task knowledge to bypass the under/misspecification of ERM, and exceeds both our highest ID and OOD performance.

Figure 8: Overview of ID vs. OOD patterns occurring at different levels of distribution shift.

The important message remains that a given hypothesis class (DenseNet architecture in our case) admits parametrizations on which ID and OOD metrics do not necessarily correlate.
* **Possible invalidation of existing studies.** The possibility of inverse correlations may invalidate studies that implicitly assume a positive one. For example, Angarano et al.  evaluate the OOD robustness of computer vision backbones. They find modern architectures surpass domain generalization methods. However, they discard any model with submaximal ID performance by performing "_training domain validation_" as in . Any model with high OOD performance but suboptimal ID is ignored. They also train every model for a fixed, large number of epochs. And this may additionally prevent from finding models with high OOD performance since robustness is often progressively lost during fine-tuning . By design, this study  is incapable of finding OOD benefits of architectures or methods that require trading off some ID performance. Most importantly, once the assumption of a positive correlation is enacted by throwing away models with submaximal ID performance, there is no more opportunity to demonstrate its validity. An even more recent example of this fallacy is found in . The authors construct a new OOD benchmark, train and tune baselines for maximum ID performance, then observe: [noitemsep,topsep=0pt,parsep=0pt,topsep=0pt] _"We find no [positive nor inverse] correlation between in- and out-of-distribution environment performance. All methods consistently achieve 98-99% ID test performance."_ But the authors did not give the chance to make any other observation. As in Figure 5, the ID criterion means that we only get to observe a thin vertical slice of the ID vs. OOD plot.5

## 9 Discussion

This paper highlighted that inverse correlations between ID / OOD performance are possible, not only theoretically but also with real-world data. It is difficult to estimate how frequent this situation is. Although we examined a single counterexample,we also showed that past studies may have systematically overlooked such cases. This suffices to show that one cannot know a priori where a task falls on the spectrum of Figure 8. It is clearly ill-advised to blindly assume a positive correlation.

**Can we avoid inverse correlations with more training data?** Scaling alone without data curation seems unlikely to prevent inverse correlations.  examined a more general question and determined that the impressive robustness of the large vision-and-language model CLIP is determined by the _distribution_ of its training data rather than its quantity. Similarly, inverse correlations stem from biases in the training distribution (e.g. a class \(_{1}\) appearing more frequently with image background \(_{1}\) than any other). And biases in a distribution do not vanish with more i.i.d. samples. Indeed, more data can cover more of the support of the distribution, but this coverage will remain uneven, i.e. biased. The problem can become one of "subpopulation shift"  rather than distribution shift, but it remains similarly challenging.

**Training full networks with a diversity-inducing method.** We showed inverse correlations with standard ERM models and with linear classifiers trained with a diversity-inducing method . To the best of our knowledge, this diversity method has not been applied to deep models because of its computational expense. It would be interesting to confirm our observations on networks trained entirely with diversity-inducing methods.

**Qualitative differences along the Pareto frontier.** Besides quantitative performance, interpretability methods could examine whether various ID / OOD trade-off models rely on different features and generalization strategies as done in NLP in .

**Model selection for OOD generalization** has recently seen promising advances.  get around selection based on either ID or OOD validation data with a tunable trade-off in their Quantile Risk Minimization method. And  examined existing approaches to OOD generalization from their suitability to various distribution shifts and causal structures.