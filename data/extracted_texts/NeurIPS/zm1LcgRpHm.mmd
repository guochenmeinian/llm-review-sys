# Segment, Shuffle, and Stitch: A Simple Layer for Improving Time-Series Representations

Shivam Grover  Amin Jalali  Ali Etemad

Queen's University, Canada

{shivam.grover, amin.jalali, ali.etemad}@queensu.ca

###### Abstract

Existing approaches for learning representations of time-series keep the temporal arrangement of the time-steps intact with the presumption that the original order is the most optimal for learning. However, non-adjacent sections of real-world time-series may have strong dependencies. Accordingly, we raise the question: _Is there an alternative arrangement for time-series which could enable more effective representation learning?_ To address this, we propose a simple plug-and-play neural network layer called Segment, Shuffle, and Stitch (S3) designed to improve representation learning in time-series models. S3 works by creating non-overlapping segments from the original sequence and shuffling them in a learned manner that is optimal for the task at hand. It then re-attaches the shuffled segments back together and performs a learned weighted sum with the original input to capture both the newly shuffled sequence along with the original sequence. S3 is modular and can be stacked to achieve different levels of granularity, and can be added to many forms of neural architectures including CNNs or Transformers with negligible computation overhead. Through extensive experiments on several datasets and state-of-the-art baselines, we show that incorporating S3 results in significant improvements for the tasks of time-series classification, forecasting, and anomaly detection, improving performance on certain datasets by up to 68%. We also show that S3 makes the learning more stable with a smoother training loss curve and loss landscape compared to the original baseline. The code is available at https://github.com/shivam-grover/S3-TimeSeries.

## 1 Introduction

Time-series data serve an important role across diverse domains, including but not limited to health analytics [1; 2; 3], human-computer interaction [4; 5], human activity recognition [6; 7], climate analysis [8; 9], energy consumption [10; 11; 12], traffic management [13; 14], financial markets , and others. The pervasive nature of time-series data has resulted in considerable interest among researchers, leading to the development of a variety of deep learning solutions for classification [16; 17; 18] and forecasting tasks [10; 19; 20]. Neural architectures such as convolutional networks [16; 21], recurrent networks [22; 23], and Transformers [10; 20] can capture the essential spatial and temporal information from time-series. Notably, these approaches have frequently outperformed traditional approaches, including Dynamic Time Warping , Bag of Stochastic Frontier Analysis Symbols , and the Collective of Transformation-Based Ensembles  in various scenarios.

While both traditional machine learning and deep learning solutions aim to extract effective goal-related representations prior to classification or forecasting, the general approach is to keep the original temporal arrangement of the time-steps in the time-series intact, with the presumption that the original order is the most optimal. Moreover, most existing models do not have explicit mechanisms to explore the inter-relations between distant segments within each time-series, which may in fact have strong dependencies despite their lack of proximity. For example, CNN-based models fortime-series learning generally utilize fixed convolutional filters and receptive fields, causing them to only capture patterns within a limited temporal window [27; 28]. As a result, when faced with time-series where important patterns or correlations span across longer time windows, these models often struggle to capture this information effectively . Dilated convolutional neural networks partially solve this by increasing the receptive field through the dilation rate. However, they are still practically limited by their inherent architectures, as their receptive fields rely on the number of layers, which may not be large enough to fully capture the long-range dependencies and can lead to vanishing gradients as more layers are added . Similarly, the out-of-the-box effectiveness of Transformers  in capturing long-term dependencies highly depends on a variety of factors such as sequence length, positional encoding, and tokenization strategies. Accordingly, we ask a simple question: _Is there a better arrangement for the time-series that would enable more effective representation learning considering the classification/forecasting task at hand?_

In this paper, we introduce a simple and plug-and-play network layer called **S**egment, **S**huffle, and **S**itch, or S3 for short, designed to enhance time-series representation learning. As the name suggests, S3 operates by segmenting the time-series into several segments, shuffling these segments in the most optimal order controlled by learned shuffling parameters, and then stitching the shuffled segments. In addition to this, our module integrates the original time-series through a learned weighted sum operation with the shuffled version to also preserve the key information in the original order. S3 acts as a modular mechanism intended to seamlessly integrate with any time-series model and as we will demonstrate experimentally, results in a smoother training procedure and loss landscape. Since S3 is trained along with the backbone network, the shuffling parameter is updated in a goal-centric manner, adapting to the characteristics of the data and the backbone model to better capture the temporal dynamics. Finally, S3 can be stacked to create a more fine-grained shuffling with higher levels of granularity, has very few hyper-parameters to tune, and has negligible computation overhead.

For evaluation, we integrate S3 in a variety of neural architectures including CNN-based and Transformer-based models, and evaluate performance across various classification, univariate forecasting, and multivariate forecasting datasets, observing that S3 results in substantial improvements when integrated into state-of-the-art models. Specifically, the results demonstrate that integrating S3 into state-of-the-art methods can improve performance by up to 39.59% for classification, and by up to 68.71% and 51.22% for univariate and multivariate forecasting, respectively. We perform detailed ablation and sensitivity studies to analyze different components of our proposed S3 layer.

Our contributions are summarized as follows:

* We propose S3, a simple and modular network layer that can be plugged into existing neural architectures to improve time-series representation learning. By dynamically segmenting and shuffling the input time-series across the temporal dimension, S3 helps the model perform more effective task-centric representation learning.
* By stacking multiple instances of S3, the model can perform shuffling at different granularity levels. Our proposed layer has very few hyperparameters and negligible added computational cost.
* Rigorous experiments on various benchmark time-series datasets across both classification and forecasting tasks demonstrate that by incorporating S3 into existing state-of-the-art models, we improve performance significantly. Experiments also show that by adding our proposed network layer, more stable training is achieved.
* We make our code public to contribute to the field of time-series representation learning and enable fast and accurate reproducibility.

## 2 Related work

Deep learning architectures have recently made significant progress in the area of time-series representation learning. In the category of convolution-based methods, DSN  introduces dynamic sparse connections to cover different receptive fields in convolution layers for time-series classification. In another convolution-based approach, SCINet  captures temporal features by partitioning each time sequence into two subsequences at each level to effectively model the complex temporal dynamics within hierarchical time-series data.

Transformer-based approaches are another category of solutions that have drawn considerable attention. Informer  improves the capabilities of the vanilla Transformer on long input sequences by introducing self-attention distillation using ProbSparse which is based on Kullback-Leibler divergence, and a generative style decoder. Autoformer  introduces decomposition blocks with an auto-correlation mechanism that allows progressive decomposition of the data. ContiFormer  integrates the continuous dynamics of Neural Ordinary Differential Equations with the attention mechanism of Transformers. PatchTST  enhances time-series forecasting by leveraging patching techniques to split long input sequences into smaller patches, which are then processed by a Transformer to capture long-term dependencies efficiently. Finally,  investigated the effectiveness of Transformers in dealing with long sequences for forecasting and highlighted the challenges Transformers encounter in this regard.

Recently, foundation models for time-series data, which are large-scale, pre-trained models designed to capture complex temporal patterns, have gained popularity. A prominent example is Moment , which pre-trains a Transformer encoder using a univariate setting on a large and diverse collection of data called 'Pile'. It leverages a masked reconstruction method and is capable of performing various downstream forecasting tasks after fine-tuning, demonstrating strong adaptability and forecasting accuracy.

Contrastive methods have recently demonstrated state-of-the-art performances in time-series learning. TS2Vec  employs unsupervised hierarchical contrastive learning across augmented contextual views, capturing robust representations at various semantic levels. InfoTS  uses information-aware augmentations for contrastive learning which dynamically chooses the optimal augmentations. To address long-term forecasting,  introduces a contrastive approach that incorporates global autocorrelation alongside a decomposition network. Finally, SoftCLT  is a recent contrastive approach that uses the distances between time-series samples (instance-wise contrastive) along with the difference in timestamps (temporal contrastive), to capture the correlations between adjacent samples and improve representations.

Given the existence of seasonal and trend information in time-series, disentanglement is an approach that has been widely used across both classical machine learning [37; 38; 39] and deep learning solutions [40; 19]. For instance, CoST  attempts to capture periodic patterns using frequency domain contrastive loss to disentangle seasonal-trend representations in both time and frequency domains. Similarly, LaST  recently employed variational inference to learn and disentangle latent seasonal and trend components within time-series data.

## 3 Method

**Problem definition.** Given a set of \(N\) time-series instances as \(\{_{i}\}_{i=1}^{N}\), where \(_{i}^{T C}\) has a length of \(T\) and \(C\) channels, the goal is to optimally rearrange the segments of \(_{i}\) and form a new sequence \(_{i}^{}\) to better capture the underlying temporal relationships and dependencies within the time-series, which would consequently lead to improved representations given the target task.

**Proposed mechanism.** We propose S3, a simple neural network component for addressing the aforementioned problem in three steps, as the name suggests, Segment, Shuffle, and Stitch, described below (see Figure 1).

The Segment module splits the input sequence \(_{i}\) into \(n\) non-overlapping segments, each containing \(\) time-steps, where \(=T/n\). The set of segments can be represented by \(_{i}=\{_{i,1},_{i,2},,_{i,n}\} ^{( C) n}\) where \(_{i,j}=_{i}[(j-1):j]\) and \(_{i,j}^{ C}\).

The segments are then fed into the Shuffle module, which uses a shuffle vector \(=\{_{1},_{2},,_{n}\} ^{n}\) to rearrange the segments in the optimal order for the task at hand. Each shuffling parameter \(_{j}\) in \(\) corresponds to a segment

Figure 1: Stacking S3 layers. In this depiction, we use \(n=2\), \(=3\), and \(=2\) as the hyperparameters.

in \(_{i}\). \(\) is essentially a set of learnable weights optimized through the network's learning process, which controls the position and priority of the segment in the reordered sequence. The shuffling process is quite simple and intuitive: the higher the value of \(_{j}\), the higher the priority of segment \(_{i,j}\) is in the shuffled sequence. The shuffled sequence \(_{i}^{}\) can be represented as

\[_{i}^{}=(_{i},= ),\] (1)

where the segments in \(_{i}\) are sorted according to the values in \(\). Permuting \(_{i}\) based on the sorted order of \(\) is not differentiable by default, because it involves discrete operations and introduces discontinuities . Soft sorting methods such as [42; 41; 43] approximate the sorted order by assigning probabilities that reflect how much larger each element is compared to others. While this approximation is differentiable in nature, it may introduce noise and inaccuracies, while making the sorting non-intuitive. To achieve differentiable sorting and permutation that are as accurate and intuitive as traditional methods, we introduce a few intermediate steps. These steps create a path for gradients to flow through the shuffling parameters \(\) while performing discrete permutations on \(_{}\) based on the sorted order of \(\). We first obtain the indices that sort the elements of \(\) using \(=()\). We have a list of tensors \(=[_{1},_{2},_{3},..._{n}]\) (for simplicity, we skip the index \(i\)) that we aim to reorder based on the list of indices \(=[_{1},_{2},...._{n}]\) in a differentiable way. We then create a \(( C) n n\) matrix \(\), which we populate by repeating each \(_{i}\), \(n\) times. Next, we form an \(n n\) matrix \(\) where each row \(j\) has a single non-zero element at position \(k=_{j}\) which is \(p_{k}\). We convert \(\) to a binary matrix \(}\) by scaling each non-zero element to 1 using a scaling factor \(}\). This process creates a path for the gradients to flow through \(\) during backpropagation.

By performing the Hadamard product between \(\) and \(}\), we obtain a matrix \(\) where each row \(j\) has one non-zero element \(k\) equals to \(_{k}\). Finally, by summing along the final dimension and transposing the outcome, we obtain the final shuffled matrix of size \(_{i}^{}^{ C n}\). To better illustrate, we show a simple example with \(=[_{1},_{2},_{3},_{4}]\) and a given permutation \(=\). We calculate \(\) as

\[=}=_{ 1}&_{2}&_{3}&_{4}\\ _{1}&_{2}&_{3}&_{4}\\ _{1}&_{2}&_{3}&_{4}\\ _{1}&_{2}&_{3}&_{4} 0&0&1&0\\ 0&0&0&1\\ 1&0&0&0\\ 0&1&0&0=0&0&_{3}&0\\ 0&0&0&_{4}\\ _{1}&0&0&0\\ 0&_{2}&0&0,\] (2)

and \(_{i}^{}\) is obtained by

\[_{i}^{}=(_{k=1}^{n}(_{j,k}))^{T}=[ _{3}_{4}_{1}_{2}]\,.\] (3)

We previously assumed that the set of shuffling parameters \(\) is one-dimensional, containing \(n\) scalar elements, each corresponding to one of the segments. By employing a higher-dimensional \(\), we can introduce additional parameters that enable the model to capture complex representations that a single-dimensional \(\) could struggle with. Therefore, we introduce a hyperparameter \(\) to determine the dimensionality of the vector \(\). When \(=m\), the size of \(\) becomes \(n n n\) (repeated \(m\) times). We then perform a summation of \(\) over the first \(m-1\) dimensions to obtain a one-dimensional vector. Mathematically, this is represented as

\[}=_{d_{1}=1}^{n}_{d_{2}=1}^{n}_{d_{m-1}=1}^ {n}_{d_{1},d_{2},...,d_{m-1}}.\] (4)

This results in a one-dimensional matrix \(}\), which we then use to compute the permutation indices \(=(})\). This approach allows us to increase the number of shuffling parameters, thereby capturing more complex dependencies within the time-series data, without affecting the sorting operations.

In the final step, the \(\) module concatenates the shuffled segments \(_{i}^{}\) to create a single shuffled sequence \(}_{i}^{T C}\) as \(}_{i}=(_{i}^{})\). To retain the information present in the original order along with the newly generated shuffled sequence, we perform a weighted sum between \(_{i}\) and \(}_{i}\) with learnable weights \(_{1}\) and \(_{2}\) optimized through the training of the main network. For practical convenience, we can use a simple learnable Conv1D or MLP layer taking \(_{i}\) and \(}_{i}\) as inputs and generating the final time-series output \(_{i}^{}^{T C}\).

**Stacking S3 layers.** Considering S3 as a modular layer, we can stack them sequentially within a neural architecture. Let's define \(\) as a hyperparameter that determines the number of S3 layers. For simplicity and to avoid defining a separate segment hyperparameter for each S3 layer, we define \(\) which acts as a multiplier for the number of segments in subsequent layers as

\[n_{}=n^{-1}=1,2,,,\] (5)

where \(n\) is the number of segments in the first S3 layer. When multiple S3 layers are stacked, each layer \(\) from 1 to \(\) will segment and shuffle its input based on the output from the previous layer. We can formally represent the output of each layer \(\) as \(^{}_{}\) by

\[^{}_{}=((( ^{}_{-1},n_{}),_{}),(_{,1 },_{,2})),=1,2,,,\] (6)

where \(^{}_{0}\) is the original time-series \(\), \(_{}\) is the set of shuffling parameters for layer \(\), and (\(_{,1}\), \(_{,2}\)) are the learnable weights for the sum operation between the concatenation of the shuffled segments and the original input, at layer \(\). Figure 1 presents an example of three S3 layers (\(=3\)) applied to a time-series with \(n=2\) and \(=2\).

All the \(_{}\) values are updated along with the model parameters, and there is no intermediate loss for any of the S3 layers. This ensures that the S3 layers are trained according to the specific task and the baseline. In cases where the length of input sequence \(\) is not divisible by the number of segments \(n\), we resort to truncating the first \(T\)**mod**\(n\) time-steps from the input sequence. In order to ensure that no data is lost and the input and output shapes are the same, we later add the truncated samples back at the beginning of the output of the final S3 layer.

## 4 Experiment setup

**Evaluation protocol.** For our experiments, we integrate S3 into existing state-of-the-art models for both time-series classification and forecasting. We first train and evaluate each model adhering strictly to their original setups and experimental protocols. We then integrate S3 at the beginning of the model, and train and evaluate it with the same setups and protocols as the original models. This meticulous approach ensures that any observed deviation in performance can be fairly attributed to the integration of S3. For classification, we measure the improvement as the percentage difference (Diff.) in accuracy resulting from S3, calculated as \((Acc_{}-Acc_{})/Acc_{}\). For forecasting, since lower MSE is better, we use \((MSE_{}-MSE_{})/MSE_{}\). A similar equation is used for measuring the percentage difference in MAE.

**Classification datasets.** For classification we use the following datasets: (1) The **UCR archive** which consists of 128 univariate datasets, (2) the **UEA archive** which consists of 30 multivariate datasets, and (3) three multivariate datasets namely **EEG**, **EEG2**, and **HAR** from the UCI archive . For our experiments with pre-trained foundation model in Section 5, we also use the **PTB-XL** dataset. The train/test splits for all classification datasets are as provided in the original papers.

**Forecasting datasets.** For forecasting, we use the following datasets: (1) both the univariate and multivariate versions of three **ETT** datasets , namely **ETTh1** and **ETTh2** recorded hourly, and **ETTm1** recorded at every 15 minutes, (2) both the univariate and multivariate versions of the **Electricity** dataset , and (3) the multivariate version of the **Weather** dataset .

**Anomaly detection datasets.** We employ the widely used **Yahoo** and **KPI** datasets for anomaly detection tasks. The Yahoo dataset consists of 367 time-series sampled hourly, each with labeled anomaly points, while the KPI dataset contains 58 minutely sampled KPI curves from various internet companies. Our experiments are performed under normal settings, as outlined in [17; 36].

**Baselines.** We select baselines from a variety of different time-series learning approaches. Specifically, for classification, we use four state-of-the-art baseline methods, SoftCLT , TS2Vec , DSN , and InfoTS . For forecasting, we use five state-of-the-art baseline methods, TS2Vec , LaST , Informer , PatchTST  and CoST . For anomaly detection, we use two state-of-the-art baseline methods, SoftCLT  and TS2Vec . Additionally, we use MOMENT  for experiments with foundation model. For each baseline method, we integrate one to three layers of S3 at the input level of the model and compare its performance with that of the original model.

**Implementation details.** All implementation details match those of the baselines. We used the exact hyperparameters of the baselines according to the original papers when they were specified in the papers or when the code was available; alternatively, when the hyperparameters were not exactlyspecified in the paper or in the code, we tried to maximize performance with our own search for the optimum hyperparameters. Additionally, for experiments involving PatchTST and LaST with the Electricity dataset, we use a batch size of 8 due to memory constraints. Accordingly, some baseline results may slightly differ from those available in the original papers. Note that deviations in baseline results affect both the baseline model and baseline+S3. For the weighted sum operation in S3, we use Conv1D. Our code is implemented with PyTorch, and our experiments are conducted on a single NVIDIA Quadro RTX 6000 GPU. We release the code at: https://github.com/shivam-grover/S3-TimeSeries.

## 5 Results

**Classification.** The results of our experiments on time-series classification are presented in Table 1. In this table we observe that the addition of S3 results in substantial improvements across all baselines for all univariate and multivariate datasets. For UCR, UEA, EEG, EEG2, and HAR, we achieve average improvements of 3.89%, 5.25%, 32.03%, 9.675%, and 5.18% respectively over all models. The full classification results on UCR and UEA datasets for all baselines with and without S3 are mentioned in Appendix (Table A1 and A2). These results highlight the efficacy of S3 in improving a variety of different classification methods over a diverse set of datasets despite negligible added computational complexity (we provide a detailed discussion on computation overhead later in this section). In addition to quantitative metrics, we also visualize the t-SNE plots for several datasets in Figure 2 and Appendix A1, which show that adding S3 results in representations with better class separability in the latent space.

**Forecasting.** Table 2 presents a comprehensive overview of the results for univariate forecasting on different datasets and horizons (H), with and without the incorporation of S3. We observe that S3 consistently leads to improvements for all baseline methods across all datasets, with an average improvement in MSE and MAE of 4.58% and 4.20% for TS2Vec, 4.02% and 3.01% for LaST, 16.69% and 8.19% for Informer, 1.64% and 1.20% for PatchTST, and 3.01% and 2.62% for CoST. Figure 3 shows two forecasting outputs for Informer with and without S3, on two different horizon lengths for a sample from the ETH1 dataset. In both cases, S3 improves the ability of the baseline Informer to generate time-series samples that better align with the ground truth. Similarly, Table 3 presents the results of multivariate forecasting. Consistent with univariate forecasting, S3 significantly enhances the performance of the baseline on multivariate forecasting and achieves average improvements in MSE and MAE of 13.71% and 7.78% for TS2Vec, 9.88% and 5.45% for LaST, 25.13% and 14.82% for Informer, 1.62% and 1.17% for PatchTST, and 4.41% and 1.97% for CoST.

**Anomaly detection.** Table 4 presents the results for anomaly detection on Yahoo and KPI datasets. The anomaly score is computed as the L1 distance between two encoded representations derived from masked and unmasked inputs, following the methodology described in previous studies [17; 36]. We observe that the proposed S3 layer enhances the performance in terms of F1 for both datasets under the normal settings, with scores of 0.7498 and 0.6892, respectively.

**Loss behaviour.** We make interesting observations while training the baseline models as well as the baseline+S3 variants. First, we observe that the training loss vs. epochs for baseline+S3 variants generally converge faster than the original baselines. See Figure 4 where we demonstrate examples of this behavior. Second, we observe that the training loss curves for baseline+S3 are generally much smoother than the original baselines. This can again be observed in Figure 4. Additionally, we measure the standard deviations of the loss curves for SoftCLT with and without S3 on all the UCR datasets and measure an average reduction in standard deviation of 36.66%. The detailed values for the standard deviations of the loss curves are presented in Appendix A4. Lastly, according to , we investigate the loss landscape of the baselines and observe that the addition of S3 generally results in a smoother loss landscape with fewer local minima (Figure 5). Additional examples are provided in Appendix A2.

  
**Method** & **UCR** & **UEA** & **EEG** & **EEG2** & **HAR** \\  TS2Vec & 0.819 & 0.695 & 0.593 & 0.845 & 0.930 \\ TS2Vec+S3 & **0.851** & 0.720 & 0.672 & 0.9733 & 0.935 \\ Diff. & 3.90\% & 3.63\% & 19.47\% & 15.18\% & 11.31\% \\  DSN & 0.794 & 0.687 & 0.516 & 0.961 & 0.957 \\ DSN+S3 & 0.833 & **0.736** & 0.717 & **0.980** & **0.971** \\ Diff. & 4.85\% & 7.24\% & 38.90\% & 1.39\% & 1.41\% \\  InfoTS & 0.733 & 0.691 & 0.515 & 0.822 & 0.876 \\ InfoTS+S3 & 0.760 & 0.727 & **0.719** & 0.947 & 0.929 \\ Diff. & 3.71\% & 5.34\% & 39.59\% & 15.21\% & 6.05\% \\  SoftCLT & 0.825 & 0.699 & 0.516 & 0.893 & 0.918 \\ SoftCLT+S3 & 0.845 & 0.734 & 0.672 & 0.950 & 0.936 \\ Diff. & 2.42\% & 4.76\% & 30.16\% & 6.38\% & 19.96\% \\   

Table 1: Comparison with baselines on time-series classification 

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

average improvements of 2.12% in MSE and 2.59% in MAE, and for ETTh2, the improvements are 4.96% in MSE and 3.56% in MAE.

**Hyperparameters.** Next, we investigate the impact of the number of segments \(n\) on performance. Considering all the experiments, we observe that no general rule of thumb can be advised for \(n\), as its optimum value is naturally highly dependent on factors such as dataset complexity, length, baselines, and others. We present two examples in Figure 6 and more in Appendix A3 where we plot the accuracy vs. number of segments for datasets of different lengths from UCR. Here we only take into account the layer with the largest number of segments. As for the other hyperparameters, the number of S3 layers \(\) has been set between 1, 2, or 3 across all experiments in this paper. \(\), the multiplier for the number of segments in stacked S3 layers, has been set to 0.5, 1, or 2 across all our experiments. And finally, \(\) has only been set to 1, 2, or 3. The ranges of values for all the hyperparameters used in this work are presented in Table 6. While optimizing hyperparameters is important, similar to any other form of representation learning, we find that even sub-optimal tuning of S3 hyperparameters still yields meaningful improvements. We perform a simple experiment where we apply a uniform set of hyperparameters \([n=2,=2,=1,=1]\) to all the baselines and baselines with added S3 layers. The results for this experiment are presented in Table 8 where we observe that despite not using the optimum hyperparameters, the addition of S3 still yields considerable performance boosts. The optimum hyperparameters used for each experiment are made available in the code.

**Sensitivity to random seed.** To evaluate the impact of the random seed on the performance of S3, we perform 10 separate trials with different seed values and present the standard deviations in Table 9. The first 4 rows show the classification baselines trained on the UCI datasets with and without S3. Similarly, the next 3 rows show forecasting baselines trained on the multivariate ETTh1 dataset (H=24) with and without S3. From this experiment, we observe that the addition of S3 has no considerable impact on the sensitivity of the original baselines to the random seed.

**Shuffling parameters.** In Figure 7, we present a visual overview of how the parameters of \(\) update along with the parent model during training on ETTh1 (H = 48) and ETTm1 (H = 48). Each parameter in \(\) corresponds to a segment of \(\). In both cases, we set \(n\) to 16 and \(\) to 1. The baseline used is LaST . It can be seen that the individual weights in \(\) rearrange the corresponding segments until they reach a stable point (the optimal order) after which the order of the segments is maintained. Similarly, Figure A4 (Appendix) shows how \(_{1}\) and \(_{2}\) update during training. Finally, in Figure A5

    & **Range** \\  \(n\) & \(\{2,4,8,16,24\}\) \\ \(\) & \(\{1,2,3\}\) \\ \(\) & \(\{0.5,1,2\}\) \\ \(\) & \(\{1,2,3\}\) \\   

Table 6: Range of hyperparameters.

(Appendix), we show sample visualizations of how S3 segments, shuffles, and stitches a time-series once the optimal shuffling parameters are obtained.

**Computation overhead.** Our proposed network layer adds very few learnable parameters to the baseline models, which stem from a learnable shuffling vector **P** along with \(_{1}\) and \(_{2}\), per each S3 layer. To emphasize the negligible computation overhead of S3 in comparison to the baseline models, we show the total number of parameters in the baseline and the total learnable parameters that S3 adds, in Table 10 for classification on the EEG2 dataset and forecasting on the multivariate ETTh1 dataset (H = 24).

## 6 Conclusion

**Summary.** We propose S3, a simple plug-and-play neural network component that rearranges the time-series in three steps: Segment the time-series, **S**huffle the segments, and **S**titch the shuffled time-series by concatenating the segments and performing a learned sum with the original time-series. Through extensive experiments on time-series classification and forecasting with state-of-the-art methods, we show that S3 improves the learning capabilities of the baseline with negligible computation overhead. We also show empirically that S3 helps in faster and smoother training leading to better performance.

**Limitations.** While we present the effectiveness of S3 on a diverse set of baselines on classification, forecasting, and anomaly detection tasks, we acknowledge that the evaluation of other time-series tasks such as imputation remain for future work. Additionally applying S3 to learning representations from other forms of time-series such as videos is an interesting direction for future research.

**Broader impact.** Since S3 is a plug-and-play network layer with negligible added parameters, it allows researchers from various domains related to time-series such as health signal processing, biometrics, climate analysis, financial markets, and others to use this module in their existing models without the need for redesign. The low computation overhead also makes S3 a suitable choice for edge-devices.

  
**Method** & **EEG** & **EEG2** & **HAR** \\  TS3vc+S3 & 0.672 & 0.973 & 0.935 \\ TS3vc+S3 w/o Segment & 0.552 & 0.872 & 0.365 \\ TS3vc+S3 w/o Segment & 0.552 & 0.864 & 0.906 \\ TS3vc+S3 w/o Switch & 0.584 & 0.871 & 0.901 \\  DNS+S3 & 0.717 & 0.980 & 0.971 \\ DNS+S3 w/o Segment & 0.516 & 0.929 & 0.910 \\ DNS+S3 w/o Up & 0.400 & 0.953 & 0.828 \\ DNS+S3 w/o Switch & 0.550 & 0.934 & 0.856 \\  InfoTS+S3 & 0.719 & 0.947 & 0.929 \\ InfoTS+S3 w/o Segment & 0.535 & 0.865 & 0.898 \\ InfoTS+S3 w/o Up & 0.499 & 0.845 & 0.869 \\ InfoTS+S3 w/o Switch & 0.515 & 0.821 & 0.887 \\  SoHCLT+S3 & 0.672 & 0.950 & 0.936 \\ SoHCLT+S3 w/o Segment & 0.548 & 0.919 & 0.879 \\ SoHCLT+S3 & 0.640 & 0.890 & 0.923 \\ SoftCLT+S3 w/o Switch & 0.600 & 0.913 & 0.921 \\   

Table 7: **Ablation results for classification.**

  
**Method** & **EEG** & **EEG2** & **HAR** \\  TS3vc & 0.672 & 0.973 & 0.935 \\ TS3vc+S3 w/o Segment & 0.552 & 0.872 & 0.365 \\ TS3vc+S3 w/o Up & 0.562 & 0.864 & 0.906 \\ TS3vc+S3 w/o Switch & 0.584 & 0.871 & 0.901 \\  DNS+S3 & 0.717 & 0.980 & 0.971 \\ DNS+S3 w/o Segment & 0.516 & 0.929 & 0.910 \\ DNS+S3 w/o Up & 0.400 & 0.953 & 0.828 \\ DNS+S3 w/o Switch & 0.550 & 0.934 & 0.856 \\ InfoTS+S3 & 0.719 & 0.947 & 0.929 \\ InfoTS+S3 w/o Up & 0.535 & 0.865 & 0.898 \\ InfoTS+S3 w/o Up & 0.499 & 0.845 & 0.869 \\ InfoTS+S3 w/o Switch & 0.515 & 0.821 & 0.887 \\  SoHCLT+S3 & 0.672 & 0.950 & 0.936 \\ SoHCLT+S3 w/o Segment & 0.548 & 0.919 & 0.879 \\ SoHCLT+S3 w/o Up & 0.523 & 0.905 & 0.883 \\ SoftCLT+S3 w/o Switch & 0.600 & 0.913 & 0.921 \\   

Table 8: Classification results with common hyperparameters.

  
**Method** & **EEG** & **EEG2** & **HAR** \\  TS3vc & 0.672 & 0.973 & 0.935 \\ TS3vc+S3 w/o Segment & 0.552 & 0.872 & 0.365 \\ TS3vc+S3 w/o Up & 0.562 & 0.964 & 0.906 \\ TS3vc+S3 w/o Up & 0.584 & 0.871 & 0.901 \\  DNS+S3 & 0.616 & 0.957 & 0.941 \\ Diff. & 0.9385 & 0.789 & 0.974 \\ Diff. & 0.9385 & 0.788 & 0.726 \\ InfoTS+S3 & 0.558 & 0.865 & 0.888 \\ InfoTS+S3 w/o Up & 0.499 & 0.845 & 0.869 \\ InfoTS+S3 w/o Switch & 0.515 & 0.821 & 0.887 \\  SoHCLT+S3 & 0.672 & 0.950 & 0.936 \\ SoHCLT+S3 w/o Segment & 0.548 & 0.919 & 0.879 \\ SoHCLT+S3 w/o Up & 0.523 & 0.905 & 0.883 \\ SoftCLT+S3 & 0.600 & 0.913 & 0.921 \\  

Table 9: Variation to **seed**.

Figure 7: The progression of the shuffling parameters during training for LaST+S3.