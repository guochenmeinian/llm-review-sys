# Multi-model Ensemble Conformal Prediction

in Dynamic Environments

Erfan Hajihashemi

Department of Electrical Engineering & Computer Science

University of California, Irvine

ehajihas@uci.edu

Yanning Shen

Department of Electrical Engineering & Computer Science

University of California, Irvine

yannings@uci.edu

Corresponding author

###### Abstract

Conformal prediction is an uncertainty quantification method that constructs a prediction set for a previously unseen datum, ensuring the true label is included with a predetermined coverage probability. Adaptive conformal prediction has been developed to address data distribution shifts in dynamic environments. However, the efficiency of prediction sets varies depending on the learning model used. Employing a single fixed model may not consistently offer the best performance in dynamic environments with unknown data distribution shifts. To address this issue, we introduce a novel adaptive conformal prediction framework, where the model used for creating prediction sets is selected 'on the fly' from multiple candidate models. The proposed algorithm is proven to achieve strongly adaptive regret over all intervals while maintaining valid coverage. Experiments on real and synthetic datasets corroborate that the proposed approach consistently yields more efficient prediction sets while maintaining valid coverage, outperforming alternative methods.

## 1 Introduction

Most machine learning algorithm designs aim to enhance label prediction accuracy. Nevertheless, a significant challenge persists as many models demonstrate limitations in predicting labels with high certainty, falling short of achieving the desired levels of accuracy and other critical evaluation metrics. In applications such as medical diagnosis, it is sometimes more efficient to predict a subset of labels rather than a single label . This necessitates predicting a set of candidate labels with a valid coverage probability, rather than limiting to a single label.

One of the most widely used frameworks for set prediction is conformal prediction . Conventional conformal prediction algorithms can achieve the desired coverage assuming the exchangeability of data . However, in many real-world online problems, the distribution of data shifts over time, making the exchangeability assumption no longer applicable. Consequently, adaptive conformal prediction algorithms have been developed , where prediction sets are constructed in a time-varying manner. Despite these advancements, the efficiency (e.g., prediction set size or regret) of previous conformal prediction methods in online settings with distribution shifts heavily depends on the model employed, anda single model may not consistently perform well across various distribution shifts. Creating an efficient prediction set size is as important as obtaining the desired coverage. Trivial cases may arise where the prediction set alternates between an empty set and the full set of labels, with the full set being created with a probability equal to the desired coverage probability, and empty sets otherwise. This approach achieves the desired coverage but leads to impractical prediction sets (Bhatnagar et al., 2023). To address this limitation, our proposed algorithm incorporates multiple learning models simultaneously. It dynamically selects the suitable model based on the performance of each model with the most recently received data.

**Related work:** Conformal prediction (Vovk et al., 2005; Shafer and Vovk, 2008; Vovk, 2015) is an effective method for uncertainty quantification that has been widely used to predict a set of candidate labels for income data. It treats the learning model as a black box and provides a prediction set for new test data. Conformal prediction is applicable in both Classification (Ding et al., 2024; Shi et al., 2013; Romano et al., 2020) and Regression (Romano et al., 2019; Papadopoulos et al., 2011; Bostrom et al., 2017) problems. In dynamic environments where data distribution shifts over time, using vanilla conformal prediction algorithms may not lead to the desired coverage performance. To cope with this challenge, conformal prediction in dynamic environments has been studied recently. (Tibshirani et al., 2019) explored conformal prediction for dynamic settings using a reweighting approach, but their method requires prior information about the data dependency structure; (Barber et al., 2023) resolved this dependency by requiring weights to be fixed. Incorporating time-varying coverage probability was introduced in (Gibbs and Candes, 2021), but determining the appropriate step size remains a significant challenge. One way to address dynamic environments is adopting learning with expert advice (Cesa-Bianchi et al., 1997; Vovk, 1995; Littlestone and Warmuth, 1994). (Zaffran et al., 2022) suggested that tuning the step size based on expert (base learner) aggregation could be an effective strategy; however, this method causes each expert to receive equal impact from all historical data, which makes the algorithm unable to adapt to sharp distribution shifts. Furthermore, (Gibbs and Candes, 2022) introduced an approach that employs multiple experts, each assigned a distinct step size from a pool of candidate sizes, resulting in varying coverage probabilities at each time \(t\). While (Gibbs and Candes, 2022) successfully demonstrated adaptive regret across time intervals of a fixed width, (Bhatnagar et al., 2023) points out that this approach fails to achieve suitable regret across varying widths of arbitrary time intervals simultaneously. (Bhatnagar et al., 2023) addressed this limitation by establishing strongly adaptive regret (Daniely et al., 2015) across any arbitrary time interval width. Their proposed methodology assigns a specific time interval to each expert. Despite achieving sublinear strongly adaptive regret, the efficacy of this approach depends on the hyper-parameter selection that determines each expert's lifetime. The proposed method in our study also employs experts who operate within specific time intervals. However, each expert consists of multiple learning models, aiming to select the appropriate model according to the specific data distribution during its operation, resulting in more efficient prediction sets.

**Contributions.** Overall, our contributions can be summarized as follows:

**I)** We introduce a novel adaptive conformal prediction algorithm, **S**trongly **A**daptive **M**lultimodel **E**nsemble **O**nline **C**onformal **P**rediction (SAMOCP), designed for dynamic environments with unknown distribution shifts. This algorithm incorporates multiple models and dynamically selects a model based on its performance in previous time steps.

**II)** We demonstrate that SAMOCP exhibits strongly adaptive regret for any arbitrary time interval while ensuring valid coverage.

**III)** Through experimental tests on classification tasks subject to distribution shifts, we demonstrate that SAMOCP outperforms existing methods by constructing more efficient prediction sets while also achieving a coverage probability closely aligned with the target value.

## 2 Preliminaries

This section explains standard conformal prediction and adaptive online conformal predictions, where data is collected sequentially. We begin with outlining standard conformal prediction. Given a miss coverage probability \(\), a learning model \(m\), a historical dataset \(\{(X_{},Y_{}^{})\}_{=1}^{t-1}\), and a new data \(X_{t}\), the objective is to construct a prediction set \(C_{}^{m}(X_{t}):=\{1,2,,K\}\), where \(K\) denotes the total number of classes, such that \(C_{}^{m}(X_{t})\) contains the true label \(Y_{t}^{}\) with probability \(1-\). In the online setting, historical dataset is updated to \(\{(X_{},Y_{}^{})\}_{=1}^{t}\) at the end of each time \(t\) when true label \(Y_{t}^{true}\) for input data \(X_{t}\) is observed. In this scenario, conformal prediction treatsthe historical dataset as a calibration dataset, which is utilized to determine whether a candidate label \(Y\) should be included in the prediction set. Consequently, in an online manner, conformal prediction relies on an evolving calibration dataset, which contains all the historical data "on the fly' to decide which candidate labels should be included in the prediction set. Non-conformity scores \(\{S^{m}(X_{},Y_{}^{})\}_{=1}^{t-1}\) are introduced. Specifically, each non-conformity score \(S^{m}(X_{},Y_{}^{})\) assesses the disagreement between the ground-truth label \(Y_{}^{}\) and predicted label \(^{m}(X_{})\). Upon obtaining a new datum \(X_{t}\), the standard conformal prediction algorithm constructs the prediction set for \(X_{t}\) as \(C_{}^{m}(X_{t})=\{Y S^{m}(X_{t},Y)_{} ^{m}\}\), where the threshold \(_{}^{m}\) is obtained as

\[_{}^{m}=Quantile(,\{S^{ m}(X_{},Y_{}^{true})\}_{=1}^{t-1}).\] (1)

The Quantile function sorts all non-conformity scores of the historical data and then identifies the \( t(1-)\)th smallest score as \(_{}^{m}\). Note that \(1-\) is fixed, hence conformal prediction cannot readily cope with potential data distribution shifts in dynamic environments. Adaptive conformal prediction algorithms have been developed to address this issue, allowing the miss coverage probability to vary at each time \(t\) and thereby enabling the algorithm to dynamically adapt to potential shifts in the distribution. In such a scenario, \(_{}^{m}\) can be obtained by replacing \(\) with \(_{t}\) in (1). Then at each time step \(t\), \(_{t}\) is updated after observing \(Y_{t}^{true}\).

Recent studies on online conformal prediction with distribution shifts have incorporated adaptive miss coverage probabilities to address dynamic environments. However, methods based on a single learning model may not achieve consistently reliable performance in dynamic environments. This underscores the necessity for employing multiple models and adaptive strategies to determine the appropriate model for each time \(t\). To this end, in this work, we introduce a novel adaptive multi-model online conformal prediction algorithm designed to identify the suitable learning model at each time \(t\) within dynamic environments. At time slot \(t\), the goal is to construct a prediction set for the new data \(X_{t}\), based on the historical dataset \(\{(X_{},Y_{}^{})\}_{=1}^{t-1}\), such that the true label is included in the prediction set with probability \(1-\). The proposed algorithm for dynamic settings achieves strongly adaptive regret while ensuring valid coverage.

## 3 Methodology

In this section, two adaptive algorithms are developed for static and dynamic environments respectively. Subsection 3.1 develops the **M**ultimodel **E**nsemble **O**nline **C**onformal **P**rediction (MOCP) algorithm to identify the suitable learning model among \(M\) distinct candidates in a static environment. Subsequently, in Subsection 3.2, we propose SAMOCP, an adaptation of MOCP tailored for dynamic environments with unknown distribution shifts.

### Multi-model Conformal Prediction in Static Environments

Note that the non-conformity score \(S^{m}(X_{},Y_{}^{true})\) depends on the learning model. Such dependency leads to a model-specific ordering of non-conformity scores, yielding different prediction sets for each model. After observing \(Y_{t}^{true}\), the adaptive miss coverage probability \(_{t}\) must be updated for time \(t+1\) to cope with distribution shifts effectively. Given that different models achieve different prediction sets, assigning and updating the same \(_{t}\) for different models would be inadequate. Instead, at each time \(t\), we assign a specific miss coverage probability to each model \(m[M]\), denoted as \(_{t}^{m}\), and update it based on the corresponding prediction set. Consequently, for \(M\) learning models, there are \(M\) candidates for miss coverage probability \(_{t}\) at each time \(t\). Each candidate is updated according to a distinct rule. These \(M\) update rules operate in parallel, with each one updating the corresponding miss coverage probability upon observing the true label. Next, the update procedure for \(_{t}^{m}\) will be examined, followed by a detailed explanation of how each instance of MOCP selects the appropriate miss coverage probability from \(M\) distinct options at each time step.

To update miss coverage probability \(_{t}^{m}\), we adopt the pinball loss (Koenker and Bassett, 1978), which can be written as

\[L(_{t}^{m},_{t}^{m})=(_{t}^{m}-_{t}^ {m})-\{0,_{t}^{m}-_{t}^{m}\},\] (2)

where

\[_{t}^{m}=\{:Y_{t}^{true} C_{}^{m} (X_{t})\}\] (3)is the best possible value of miss coverage probability for model \(m\) at time \(t\) which constructs the smallest prediction set that covers \(Y_{t}^{true}\).The miss coverage probability \(_{t+1}^{m}\) can be updated via SF-OGD  as

\[_{t+1}^{m}=_{t}^{m}-^{m}}L(_ {t}^{m},_{t}^{m})}{^{t}\|_{_{t}^{m}}L( _{}^{m},_{}^{m})\|_{2}^{2}}},\] (4)

where \(\) is the learning rate and

\[_{_{t}^{m}}L(_{t}^{m},_{t}^{m})=[ {}_{t}^{m}<_{t}^{m}]-=err_{t}^{m}-,\] (5)

with \(err_{t}^{m}:=[Y_{t}^{true} C_{_{t}^{m}}^{m}]\) equals \(1\) if the predicted set does not contain the true label \(Y_{t}^{true}\), and \(0\) otherwise. According to the updating rule outlined in equation (4), the adjustment of \(_{t}^{m}\) at each time \(t\) is governed by the \(_{_{t}^{m}}L(_{t}^{m},_{t}^{m})\), as detailed in equation (5). When \(err_{t}^{m}=1\), it signals that the coverage probability \(1-_{t}^{m}\) is too small, resulting in a prediction set that can not encompass \(Y_{t}^{true}\). Consequently, there's a necessity to enlarge the coverage probability, effectively achieved by reducing \(_{t}^{m}\), which would be facilitated by (4); given that the denominator in the second term is always positive and the gradient will be positive in this scenario. On the other hand, when \(_{t}^{m}>_{t}^{m}\), \(1-_{t}^{m}\) leads to a prediction set that covers \(Y_{t}^{true}\) but also includes unnecessary labels \(^{}:=\{Y^{}^{}_{ _{t}^{m}}^{m}<S^{m}(X_{t},Y^{})_{_{t}^{m}}^ {m}\}\). In such cases, optimization necessitates increasing \(_{t}^{m}\) to avoid including unnecessary labels and output a more efficient prediction set. This adjustment is facilitated by the update rule (4).

Additionally, the weight \(w_{t}^{m}\) is assigned to each model \(m[M]\), which influences the selection of its corresponding miss coverage probability \(_{t}^{m}\). MOCP learns which model to select over time based on the performance of each model over previous time steps, as reflected in \(w_{t}^{m}\). The algorithm updates the weight associated with each model after revealing the true label \(Y_{t}^{true}\). This update is performed with respect to the loss function of the corresponding miss coverage probability. Specifically, \(w_{t}^{m}\) can be updated by

\[w_{t+1}^{m}=w_{t}^{m}(- L(_{t}^{m},_{t }^{m})),\] (6)

where \(0<<1\) is the step size. At each time \(t\), upon receiving new data \(X_{t}\), MOCP first calculates the normalized weights, denoted as \(\{_{t}^{m}\}_{m=1}^{M}\). For any \(m[M]\), \(_{t}^{m}=^{m}}{_{j=1}^{M}w_{t}^{j}}\) ensures that \(_{t}^{m}\) and represents the likelihood of selecting miss coverage probability \(_{t}^{m}\). Then, the algorithm selects miss coverage probability \(_{t}^{}\), where \([M]\), according to PMF \(}_{t}:=(_{t}^{m})_{m=1}^{M}\), i.e., each miss coverage probability \(_{t}^{m}\) is selected with probability proportional to the corresponding normalized weight \(_{t}^{m}\). The prediction set for \(X_{t}\) is constructed according to the threshold in (1), by replacing \(\) and \(m\) with \(_{t}^{}\) and \(\) respectively. After receiving \(Y_{t}^{true}\), each weight \(w_{t}^{m}\) and miss coverage probability \(_{t}^{m}\) are updated according to (6) and (4), respectively. This entire process is detailed in Algorithm 1. The MOCP algorithm achieves a runtime of \((T)\) when the number of models \(M\) is constant.

Given that the environment is static, there exists a miss coverage probability that can minimize the loss function for each model \(m[M]\) over \([T]\), denoted as \(^{m}\). The best miss coverage probability among \(\{^{m}\}_{m=1}^{M}\) can be obtained by

\[^{m^{*}}=*{arg\,min}_{\{^{m},m[M]\}}_{t=1}^{T} L(_{t}^{m},^{m})\ \ \ \ ^{m}=*{arg\,min}_{_{t}^{m}}_{t=1}^{T}L(_{t}^{m},_{t}^{m}).\] (7)

The following theorem demonstrates that MOCP achieves sublinear regret (See proof in A.2).

**Theorem 1**: _Algorithm 1 achieves the following regret bound in a static environment_

\[_{t=1}^{T}_{m=1}^{M}_{t}^{m}L(_{t}^{m},_{t}^{m} )-_{t=1}^{T}L(_{t}^{m^{*}},^{m^{*}})( }{2}++ M+(1+)^{2}).\] (8)

### Multi-model Ensemble Conformal Prediction In Dynamic Environments

Through Algorithm 1, we demonstrated how to select the suitable miss coverage probability of a model that has achieved lower loss compared to other models over previous time steps. However,this approach assumes a static environment that does not change over time, which may limit the algorithm's effectiveness in dynamic environments where data distribution shifts occur. In addition, the selection of stepsize \(\) in (6) critically affects the performance. In environments with unknown distribution shifts, a large \(\) indicates faster adaptation to abrupt changes, whereas a small \(\) is more suitable for environments with less variability. Thus, the efficient choice of \(\) depends on the variability of the environment, which poses a challenge in scenarios with unknown distribution shifts.

To address this limitation, we introduce Strongly Adaptive (SA)MOCP. Specifically, each instance of MOCP is treated as an 'expert', and multiple experts are created at distinct time steps with specific step sizes and lifetimes to cope with potential distribution shifts. At the end of its lifetime, the expert becomes inactive, ensuring that it no longer affects the decision-making process, refer to Figure 1 for an illustration. This strategy prevents outdated experts from contributing to the selection of the suitable miss coverage probability in dynamic environments. Subsequently, SAMOCP dynamically selects the appropriate expert for each time \(t\) and utilizes its chosen miss coverage probability to construct the prediction set. The lifetime of each expert is determined by the specific time \(t\) at which it was created and hyperparameter \(g\), as [Bhatnagar et al., 2023].

\[(t)=g_{n}\{2^{n}:t 0 2^{n}\}.\] (9)

The active interval of the expert created at time \(t\) is defined as \([t,t+(t)-1]\). Consequently, experts considered active at any given time \(\) are those whose active intervals include \(\). In a dynamic setting with unknown distribution shifts, the best model may vary across different distributions. This variability results in scenarios where, at each time \(t\), some active experts might not have adapted to the current data distribution yet, and thus they rely on different models compared to more recently established active experts. In such cases, the miss coverage probability of model \([M]\) chosen by expert \(n\) at time \(t\) is represented as \(_{t}^{bin}\), and its best possible value is denoted as \(_{t}^{bin}\). To select the suitable miss coverage probability among different experts, we assigned distinct weights to each,

Figure 1: Expert creation over 5 time steps using lifetime formula (9) when \(g=1\). At each time \(t\), an expert is created, marked by a filled circle to indicate the start of the activity, and an unfilled circle to denote the end of the expert’s activity.

with specific initialization and step sizes for updates. Specifically, this weight for expert \(n\) at time \(t\) is denoted as \(h_{t}^{n}\), and its step size is defined as \(^{n}:=(,})\), where \(>1\) is a constant and \((n)\) is the lifetime for expert \(n\) that obtained by (9). Given that \(n\)th expert is activated at \(t=n\), the initialization and update rule for \(h_{t}^{n}\) is as follows:

\[h_{t+1}^{n}=^{n}&t=n-1\\ h_{t}^{n} r_{t}^{n})}&t[n,n+(n)-1)\\ 0,&\] (10)

where \(r_{t}^{n}=L(_{t}^{},_{t}^{})-L( _{t}^{},_{t}^{})\) represents the loss of the \(n\)th expert relative to the loss of the learner who selects expert \(\). We denote the set of active experts at each time \(t\) as \((t)\). The learner selects the suitable miss coverage probability among all active experts according to the PMF \(}_{t}:=(_{t}^{n})_{n(t)}\), where each \(_{t}^{n}\) represents the normalized version of the weight \(h_{t}^{n}\), calculated as \(_{t}^{n}=^{n}}{_{i(t)}h_{t}^{i}}\), ensuring that \(_{t}^{n}\). Algorithm 2 summarizes the SAMOCP method. It can be observed from (9) that the maximum number of active experts (MOCP instances) at each time \(t\) is \(g_{2}t\). Hence, the complexity of SAMOCP is of order \(}(T_{2}T)\).

```
0:\(\), hyperparameters \( 0,(0,1)\), and \(>1\). for\(t[T]\)do  Create new expert \(n\) (where \(n=t\)) by Algorithm 1(\(_{t-1}^{},,^{n}\)).  Remove experts whose lifetime has been finished.  Every active expert selects miss coverage probability from \(M\) options.  Calculate normalized weights by \(_{t}^{n}=^{n}}{_{i(t)}h_{t}^{i}}\).  Select one miss overage probability from active experts according to PMF \(}_{t}=(_{t}^{n})_{n(t)}\).  Construct prediction set for \(X_{t}\) using selected miss coverage probability.  Observe true label \(Y_{t}^{true}\). for\(n(t)\)do  Obtain learner loss \(L(_{t}^{},_{t}^{})\).  Update every parameters assigned to each model for expert \(n\) via Algorithm 1.  Update \(h_{t+1}^{n}\) with (10). endfor endfor ```

**Algorithm 2** Strongly Adaptive Multi-model Ensemble Online Conformal Prediction (SAMOCP)

Let \(CovE(T):=|_{t=1}^{T}[err_{t}]-|\) represent the coverage error. In a dynamic setting where multiple experts are incorporated, each including \(M\) miss coverage probabilities, the expected error is calculated as \([err_{t}]=_{n=1}^{t}_{m=1}^{M}_{t}^{n}_{t}^{ mn}err_{t}^{mn}\), where \(mn\) represents the \(m\)th model by expert \(n\). Using the two following theorems, we prove that SAMOCP has bounded coverage error and achieves strongly adaptive regret across any time interval of arbitrary width (Proofs can be found A.3 and A.4).

**Theorem 2**: _For any \(T 1\) and any \((,1)\), Algorithm 2 achieves the coverage error bound_

\[CovE(T)(_{}\{T^{-}+T^{ -1}_{}(T)\}),\] (11)

_where \(_{}(T)\) measures the smoothness of model weights within experts and the cumulative gradient norm for each model within experts. The definition of \(_{}(T)\) is provided in detail in equation (31) in the Appendix A.3. If there exists a \((,1)\) such that \(_{}(T)}(T^{})\) where \(<1-\), then the coverage bound (11) will be \(CovE(T)}(T^{-(-,-1+ )})=_{T}(1)\)._

**Theorem 3**: _Algorithm 2 achieves strongly adaptive regret over any interval \(I[T]\) and positive constants A, B, as follows_

\[_{t I}_{n(t)}_{m=1}^{M}_{t}^{n}_{t}^ {mn}L(_{t}^{mn},_{t}^{mn})-_{t I}L(_{t}^{ m^{*}n^{*}},^{m^{*}n^{*}}) A+B T,\] (12)_where_

\[^{m^{*}n^{*}}=*{arg\,min}_{^{m^{*}n}}_{t I}L( _{t}^{m^{*}n},^{m^{*}n}).\] (13)

_Note that in equation (13), \(^{m^{*}n}\) represents the miss coverage probability assigned to the best model for expert \(n\), as obtained by equation (7)._

The miss coverage probability \(^{m^{*}n^{*}}\) in (13), is related to specific interval \(I\), which can vary across different intervals with distinct distributions in a dynamic environment. In such settings, there is no fixed miss coverage probability \(^{m^{*}n^{*}}\) that can be consistently applied over various time intervals. This necessitates establishing that SAMOCP has bounded regret in dynamic environments with respect to the time-varying benchmark in (16), as demonstrated by the following lemma. The proof is provided in A.5.

**Lemma 1**: _By defining the variation of the loss function to be_

\[V(L()_{t=1}^{T}):=_{t=1}^{T}_{\{m[M],n(t)\}} |L(_{t+1}^{mn},_{t+1}^{mn})-L(_{t}^{mn}, _{t}^{mn})|.\] (14)

_We establish the following bound for the dynamic regret of Algorithm 2_

\[_{t=1}^{T}_{n(t)}_{m=1}^{M}_{t}^{n}_{ t}^{mn}L(_{t}^{mn},_{t}^{mn})-_{t=1}^{T}L(_{t}^ {m^{*}n^{*}},_{t}^{m^{*}n^{*}})}(T^{}V ^{}(L()_{t=1}^{T}))\] (15)

_where \(}\) suppresses positive constants and polylogarithmic factors, e.g., \( T\). Also the best miss coverage probability at each time \(t\) can be obtained by_

\[_{t}^{m^{*}n^{*}}=*{arg\,min}_{\{m[M],n(t )\}}L(_{t}^{mn},_{t}^{mn}).\] (16)

Lemma 1 establishes that the dynamic regret of SAMOCP (15) depends on the variation of the loss functions (14). In addition, it can be obtained from (15) that SAMCOP achieves sublinear regret if the variation of the loss function is also sublinear, i.e., \(V(L()_{t=1}^{T})=(T)\).

## 4 Experiments

In this section, the performance of the proposed method, SAMOCP, is assessed within the context of classification tasks. We conduct a comprehensive comparison with recently proposed methods in online conformal prediction for dynamic environments within classification tasks. The section begins with a detailed explanation of the experimental settings, followed by a discussion of the results. Note that throughout the experiments in this section, the desired miss coverage probability \(\) is \(0.1\). All experiments were performed on a workstation with NVIDIA RTX A4000 GPU. Codes are available at hyperrefhttps://github.com/erfanhajihashemi/Multi-model-Ensemble-Conformal-Prediction-in-Dynamic-Environments.

**Dataset:** We utilize corrupted versions of CIFAR-10 and CIFAR-100 (Krizhevsky, 2009), known as CIFAR-10C and CIFAR-100C (Hendrycks and Dietterich, 2019). These datasets consist of \(15\) generated corruptions spanning 5 distinct levels of severity. The evaluation encompasses two settings: sudden and gradual distribution shifts. For both settings, the data sequence is split into batches of \(500\) data samples each. The severity of corruption changes (increases or decreases) after each batch of data. In the sudden shifts, the severity level alternates between the version of the data without any corruption (severity level \(0\)) and the most severe corruption (severity level \(5\)). In the gradual setting, severity starts at level \(0\) and increases one by one after each batch until it reaches level \(5\). After reaching level \(5\), the severity decreases one by one and goes back to level \(0\) in subsequent batches. This cycle of increasing and decreasing severity continues throughout the experiment. Also, additional experiments on TinyImageNet-C (Hendrycks and Dietterich, 2019) and synthetic data are provided in the Appendix, Section B.

**Baselines and experimental settings:** We employ ResNet-50, ResNet-18 (He et al., 2016), GoogLeNet (Szegedy et al., 2015), and DenseNet-121 (Huang et al., 2017) as candidate learning models. Each active expert consists of all these learning models and needs to select the appropriate model during its active interval. The proposed method is compared with the most recent adaptive conformal prediction algorithms designed for dynamic environments, including FACI (Gibbs and Candes, 2022), ScaleFreeOGD (Bhatnagar et al., 2023), and SAOCP (Bhatnagar et al., 2023). FACI employs a fixed number of active experts over all time steps, with each expert assigned one of the candidate learning rates for updating the miss coverage probability. ScaleFreeOGD reduces the learning rate based on the cumulative norms of gradients (Orabona and Pal, 2018). SAOCP allows each expert to have its own active interval, within which it operates similarly to ScaleFreeOGD. In order to show how SAMOCP results in more efficient sets compared to SAOCP in a multi-model setting, we developed a multi-model ensemble version of SAOCP, denoted as SAOCP(MM), where each expert consists of \(M\) update rules, each corresponding to a different learning model. This approach follows our multi-model approach but employs a similar rule to SAOCP for updating weights. To determine the value of \(g\), we employed a grid search approach within the candidates \(\{4,8,16,24,32,48,64\}\). The one that led to the smallest prediction set size (Avg Width) while maintaining reasonable coverage and runtime was selected, which was \(g=8\). While the hyperparameter \(g\) is set to \(8\) for both SAMOCP and SAOCP(MM), it is set to \(32\) for SAOCP, as in (Bhatnagar et al., 2023). Since \(4\) learning models are incorporated in this section, the maximum number of updates at each time \(t\) in SAMOCP, \(Mg_{2}t\), is equal to that in SAOCP and SAOCP(MM), which is \(32_{2}t\). Meanwhile, note that randomness might be undesirable in practice, the predicted miss coverage in SAMOCP is calculated in a deterministic fashion, i.e., \(_{t}=_{n(t)}_{m=1}^{M}_{t}^{n}_{t}^ {mn}_{t}^{mn}\). For every experiment conducted on the synthetic dataset, CIFAR-10C, CIFAR-100C, parameters \(\), \(\), and \(\) were selected through grid search, with values of \(0.9\), \(140\), and \(0.05\), respectively.

**Score Functions:** We utilized the nonconformity score defined as in (Angelopoulos et al., 2020) to construct prediction sets. Let

\[S^{m}(X,Y)=-k_{reg}],0)}+U_{t}_{Y}^{m}(X)+(X,Y),\] (17)

where \(_{Y}^{m}(X)\) denotes the probability of predicting label \(Y\) for input \(X\) by model \(m\), and \(U_{t}\) is a random variable sampled from a uniform distribution over the interval \(\). The term \(k_{Y}:=|\{Y^{}_{Y}^{m}(X)_{Y}^{m}(X )\}|\) denotes the number of labels that have a higher or equal predicted probability than label \(Y\) according to the model's output probability distribution, e.g., the softmax output. \((X,Y):=_{Y^{}=1}^{K}_{Y^{}}^{m}(X)[_{Y^{}}^{m}(X)>_{Y}^{m}(X)]\) sums up the probabilities of all labels that have a higher predicted probability than label \(Y\). The hyperparameters \(\) and \(k_{reg}\) are set to \(0.02\) and \(5\) for CIFAR-100C, and \(0.1\) and \(1\) for Cifar-10C, respectively.

**Evaluation Metrics:** Coverage measures the percentage of instances where the true label is included in the prediction sets outputted by the conformal prediction algorithm over the period \([T]\). Avg Width refers to the average size of these prediction sets. Adaptive regret is calculated for time intervals of length 100. The metric Avg Regret represents the average of these regret values across the entire time horizon \([T]\). Run Time indicates the time required to complete each iteration of the algorithm. Lastly, Single Width measures the probability that prediction sets contain exactly one element while accurately covering the true label, highlighting cases that are most informative for predictions.

### Results

Table 1 presents the performance of SAMOCP for the classification task on the CIFAR-100C dataset under a gradual shift setting, where each method receives \(8,550\) data points sequentially. It is evident that the performance of previous methods, particularly in terms of prediction set size and single-width prediction sets, depends on the learning model employed. The proposed method SAMOCP outperforms existing methods by creating smaller prediction sets, lower regret, and more single-width prediction sets that correctly cover the true label, while also achieving coverage close to the targeted level. It is noteworthy that SAMOCP surpasses every variant of previous methods in these aspects. Furthermore, SAMOCP is faster than SAOCP and SAOCP(MM), despite having the same maximum number of updates at each time \(t\).

In dynamic environments, data distribution does not necessarily shift gradually. Instead, we may encounter abrupt distribution shifts, with significant differences between data distributions in two successive time slots. To demonstrate how SAMOCP behaves in such environments, another experiment was conducted, in which SAMOCP can successfully track these sharp transitions and select a suitable learning model for creating a prediction set. Experimental results on CIFAR-10C are detailed in Table 2, where the proposed algorithm again outperforms previous methods in terms of prediction set size, regret, and single width prediction sets that accurately cover the true labels while maintaining coverage close to the target value.

To demonstrate that SAMOCP achieves the lowest regret over different intervals compared to existing methods, we illustrate the regret for various interval sizes in Figure 2. For each existing method, there are \(4\) different regrets corresponding to the \(4\) learning models used and the lowest regret is depicted. The results show that our method consistently leads to lower regret than the best version of each previous method across different learning models. Note that a lower regret implies that the algorithm adapts faster to changes. The regret calculated over different time intervals indicates the algorithm's adaptivity in capturing the distribution shift at different time scales. Therefore, Figure 2 indicates that the SAMOCP can adapt faster to distribution shifts compared to benchmarks in various time scales. Furthermore, we include experiments on synthetic data and another real dataset, TinyImageNet-C, using different sets of learning models that do not necessarily contain \(4\) models in the Appendix, Section B. This demonstrates how SAMOCP can rely on a mixture of learning models over the period \([T]\) and select the appropriate one for each distribution setting.

   Model & Method & Coverage (\%) & Avg Width & Avg Regret(\( 10^{-3}\)) & Run Time & Single Width \\   & SAMOCP & 88.16 \(\) 0.18 & **5.43 \(\) 0.28** & **0.92 \(\) 0.07** & 34.87 \(\) 0.67 & **0.29 \(\) 0.01** \\  & SAOCP(MM) & 85.89 \(\) 0.84 & 6.61 \(\) 0.26 & 4.42 \(\) 0.51 & 47.80 \(\) 0.22 & 0.27 \(\) 0.01 \\  & FACI & 89.64 \(\) 0.28 & 5.77 \(\) 0.62 & 1.18 \(\) 0.74 & 8.22 \(\) 0.07 & 0.28 \(\) 0.01 \\  & ScaleFreeOGD & **89.97 \(\) 0.02** & 6.04 \(\) 0.10 & 1.55 \(\) 0.06 & **3.02 \(\) 0.06** & 0.26 \(\) 0.01 \\  & SAOCP & 88.90 \(\) 0.10 & 5.80 \(\) 0.08 & 2.04 \(\) 0.04 & 40.74 \(\) 0.31 & 0.27 \(\) 0.00 \\  & FACI & 89.67 \(\) 0.33 & 6.50 \(\) 0.57 & 1.24 \(\) 0.90 & 8.45 \(\) 0.09 & 0.26 \(\) 0.01 \\  & ScaleFreeOGD & **89.97 \(\) 0.02** & 6.72 \(\) 0.13 & 1.58 \(\) 0.07 & 3.12 \(\) 0.04 & 0.25 \(\) 0.01 \\  & SAOCP & 88.79 \(\) 0.14 & 6.48 \(\) 0.13 & 2.08 \(\) 0.10 & 41.35 \(\) 0.34 & 0.25 \(\) 0.00 \\  & FACI & 89.56 \(\) 0.28 & 6.82 \(\) 0.77 & 1.17 \(\) 0.75 & 8.39 \(\) 0.06 & 0.25 \(\) 0.01 \\  & ScaleFreeOGD & 89.96 \(\) 0.02 & 7.29 \(\) 0.19 & 1.55 \(\) 0.07 & 3.05 \(\) 0.04 & 0.23 \(\) 0.01 \\  & SAOCP & 88.76 \(\) 0.22 & 6.9 \(\) 0.17 & 2.06 \(\) 0.07 & 41.23 \(\) 0.26 & 0.24 \(\) 0.01 \\  & FACI & 89.63 \(\) 0.30 & 6.33 \(\) 0.74 & 1.10 \(\) 0.78 & 8.30 \(\) 0.09 & 0.27 \(\) 0.01 \\  & ScaleFreeOGD & 89.96 \(\) 0.01 & 6.71 \(\) 0.15 & 1.52 \(\) 0.06 & 3.04 \(\) 0.04 & 0.24 \(\) 0.00 \\  & FACI & 88.68 \(\) 0.11 & 6.38 \(\) 0.12 & 2.07 \(\) 0.07 & 41.13 \(\) 0.39 & 0.26 \(\) 0.01 \\    &  & & & & & \\   

Table 1: Results on the CIFAR-100C dataset with a gradual distribution shift. The target coverage is \(90\%\), and the average regret is calculated over an interval size of \(100\). Bold numbers denote the best results in each column. SAMOCP achieves the best performance in terms of average width, average regret, and single width.

   Model & Method & Coverage (\%) & Avg Width & Avg Regret(\( 10^{-3}\)) & Run Time & Single Width \\   & SAMOCP & 88.37 \(\) 0.23 & **1.24 \(\) 0.06** & **0.98 \(\) 0.11** & 33.75 \(\) 0.34 & **0.69 \(\) 0.03** \\  & SAOCP(MM) & 86.80 \(\) 2.39 & 1.45 \(\) 0.13 & 3.87 \(\) 1.05 & 47.08 \(\) 0.19 & 0.56 \(\) 0.05 \\  & FACI & 89.57 \(\) 0.37 & 1.30 \(\) 0.12 & 1.46 \(\) 0.73 & 8.11 \(\) 0.10 & 0.68 \(\) 0.05 \\  & ScaleFreeOGD & **89.99 \(\) 0.01** & 1.46 \(\) 0.02 & 1.71 \(\) 0.04 & 2.92 \(\) 0.07 & 0.52 \(\) 0.02 \\  & SAOCP & 88.77 \(\) 0.18 & 1.41 \(\) 0.02 & 2.24 \(\) 0.06 & 39.62 \(\) 0.22 & 0.54 \(\) 0.01 \\  & FACI & 89.74 \(\) 0.35 & 1.50 \(\) 0.04 & 1.35 \(\) 0.03 & 8.11 \(\) 0.08 & 0.55 \(\) 0.01 \\  & SaIFreeOGD & 89.98 \(\) 0.01 & 1.52 \(\) 0.01 & 1.71 \(\) 0.05 & **2.89 \(\) 0.04** & 0.54 \(\) 0.01 \\  & FACI & 89.63 \(\) 0.34 & 1.36 \(\) 0.13 & 1.52 \(\) 0.76 & 8.11 \(\) 0.08 & 0.64 \(\) 0.05 \\  & ScaleFreeOGD & **89.99 \(\) 0.01** & 1.52 \(\) 0.02 & 1.69 \(\) 0.06 & 2.91 \(\) 0.06 & 0.49 \(\) 0.01 \\  & SAOCP & 88.83 \(\) 0.06 & 1.48 \(\) 0.02 & 2.24 \(\) 0.07 & 40.16 \(\) 0.22 & 0.51 \(\) 0.01 \\  & FACI & 89.73 \(\) 0.34 & 1.43 \(\) 0.06 & 1.41 \(\) 0.89 & 8.10 \(\) 0.10 & 0.58 \(\) 0.02 \\  & ScaleFreeOGD & 89.99 \(\) 0.02 & 1.46 \(\) 0.01 & 1.70 \(\) 0.06 & **2.89 \(\) 0.04** & 0.55 \(\) 0.00 \\  & SAOCP & 89.09 \(\) 0.14 & 1.44 \(\) 0.01 & 2.17 \(\) 0.08 & 40.13 \(\)

## Conclusion

In this study, we introduced a novel conformal prediction algorithm designed for online environments undergoing distribution shifts. Recognizing that the selection of baseline models affects the efficiency of conformal prediction, our algorithm incorporates multiple models simultaneously. For each expert, the contribution of each model is dynamically adjusted based on its time-evolving weight. We demonstrated that our proposed method SAMOCP achieves strongly adaptive regret across any time interval of arbitrary width and maintains valid coverage. Experimental results in environments with both gradual and sudden distribution shifts indicated that our algorithm produces more informative prediction sets and achieves a coverage rate close to the target value, compared to those created by previous methods using their best baseline models.