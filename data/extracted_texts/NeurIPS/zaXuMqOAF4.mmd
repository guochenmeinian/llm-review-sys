# Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs

Xin Ma1, Yang Liu2,3, Jingjing Liu2, Xiaoxu Ma1

1Digital Research Institute, Enn Group, Beijing, China

2Institute for AI Industry Research, Tsinghua University, Beijing, China

3Shanghai Artificial Intelligence Laboratory, China

{xin.ma0206, xiaoxuma}@gmail.com, {liuy03, jjliu}@air.tsinghua.edu.cn

Corresponding author

###### Abstract

Large language models (LLMs), although having revolutionized many fields, still suffer from the challenging extrapolation problem, where the inference ability of LLMs sharply declines beyond their max training lengths. In this work, we conduct a theoretical analysis to better understand why _No Position Encoding_ (NoPE) fails outside its effective range, as well as examining the power of _Position Encoding_ (PE) in this context. Our findings reveal that with meticulous weave position, PE can indeed be extended beyond effective range. Our theorems establish that LLMs equipped with _weave PE_ can achieve improved extrapolation performance without additional cost. Furthermore, we introduce a novel weave PE method, _Mesa-Extrapolation_, which utilizes a chunk-based triangular attention matrix and applies _Stair PE_ to manage the final chunk. This method not only retains competitive performance but also offers substantial benefits such as significantly reduced memory demand and faster inference speed. Extensive experiments validate the effectiveness of Mesa-Extrapolation, demonstrating its potential as a scalable solution to enhancing LLMs' applicative reach. Our code is available at https://github.com/soacker/Mesa-Extrapolation.

## 1 Introduction

Large Language Models (LLMs) with their powerful in-context learning capabilities Brown et al. (2020) offer versatile solutions to a wide-range of intelligent applications. However, one pressing challenge, the _extrapolation problem_Deletang et al. (2022)Zhang et al. (2022), dictates that the inference ability of LLMs sharply declines beyond their max training lengths, imposing a serious limitation on applications with long inputs. An naive solution is to extend the length of training samples. However, the inherent quadratic complexity of calculations presents practical challenges, demanding more resources, longer training time and higher cost.

Positional encoding (PE) has become a pivotal component of Transformer architecture, to compensate for the overlooking of position information by the attention mechanism. In the realm of extrapolation capability, PE is considered as a key factor influencing the extrapolating ability of LLMs. A few PE approaches, such as the popular RoPE Su et al. (2023) and ALiBi Press et al. (2021), claim to offer improved extrapolation capabilities and have gained widespread usage in industrial applications. Meanwhile, a counter-narrative has emerged. Some works demonstrate that Transformer can achieve better extrapolation capabilities by removing position encoding (NoPE) Kazemnejad et al. (2023), and contend that the mask already plays a significant role in capturing position information.

Inspired by Kazemnejad et al. (2023), we conduct a thorough investigation of the extrapolation problem by designing a specific Transformer model for this purpose and presenting detailed theoretical analysis. To the best of our knowledge, this is the first theoretical endeavor to understand the inner workings of extrapolation. We first elucidate the cause of NoPE's failure when input exceeds the effective window length in Theorem 3.1, and analyse PE's failure in 3.2. Building upon Theorem 3.2, we prove in Theorem 3.3 that through meticulous weave position, (i.e., _weave PE_), it is feasible to achieve extrapolation beyond the effective window length.

We further introduce a novel weave-PE-based method, _Mesa-Extrapolation_, which demonstrates substantial extrapolation powers without the need for additional training. Specifically, we use chunk-based triangular attention matrix, aiming at memory-friendly resource consumption (Figure 1 left), and we employ Stair PE to handle the last chunk (Figure 1 right below), effectively making Mesa-Extrapolation a completely free plug-in for LLMs.

Our contributions are summarized as follows:

1. **Theoretical Analysis** We provide a comprehensive theoretical analysis on the challenges encountered by NoPE beyond its effective window length, and investigate the effect of PE in this context. Our theorems prove that through meticulous weave position, PE can be effectively extrapolated beyond the effective window length. This theoretical foundation establishes a clear understanding of the extrapolation problem and potential solutions for LLMs utilizing PE.
2. **Introduction of Mesa-Extrapolation** We introduce a novel extrapolation approach called _"Mesa-Extrapolation."_ Based on triangular attention matrix, this method strategically organizes input tokens into chunks, with the final chunk employing a weave PE method (e.g., Stair PE) to integrate all token states. Mesa-Extrapolation provides a practical and effective solution to the extrapolation problem.
3. **Empirical Validation** Comprehensive experiments demonstrate that our approach achieves competitive performance, offering the benefits of extremely low memory usage and the fastest inference speed compared to existing methods. Importantly, it is an easy plug-in and can enhance extrapolation performance of LLMs without any additional resource consumption.

## 2 Background

Since the self-attention mechanism itself does not contain position information, PE components have become an integral part of the Transformer architecture. Cmmon choices for PE are either _absolute_, where each absolute position (e.g., \(1,2,3,...\)) is directly represented, or _relative_, where the distance between tokens is used as positional information. Absolute Position Embedding (APE) embeds

Figure 1: Chunk-based triangular attention matrix, PE and Stair PE. The left figure shows the Chunk-based triangular attention matrix (before SoftMax operation) of Mesa-Extrapolation when an exemplar sequence of length \(13\) is fed into a LLM. The right figure shows an example of PE and Stair PE. The Stair PE is used to weave the relative position in Mesa-Extrapolation.

each absolute position \(i\) into position vector \(_{i}\) and adds word embeddings to their corresponding \(_{i}\), before feeding them to the LLMs Vaswani et al. (2017). However, the extrapolation ability is limited because it cannot generalize to unseen positions. RoPE Su et al. (2023) rotates the query and key vectors with an angle proportional to their absolute positions, so the attention dot production only depends on relative distance between tokens, providing a relative positional encoding. T5's Relative Bias first maps the relative distance (\(i-j\)) between tokens at positions \(i\) and \(j\) to a scalar bias value \(b=f(i-j)\), where \(f\) is a lookup table. ALBi is similar to T5's Relative Bias but instead subtracts a scalar bias from the attention score Press et al. (2021)(refer to Appendix D.2 for more details). Recent works such as ReRoPE and Leaky-ReRoPE Su (2023) achieve effective extrapolation without fine-tuning by meticulously weaving relative positions of RoPE. **We refer to this class of methods that achieve extrapolation by weaving the relative positions of PE without fine-tuning as _Weave PE_**.**

Kazemmejad et al. (2023) indicates that the decoder-only transformer with position encoding removed (NoPE) demonstrates stronger extrapolation capabilities. Furthermore, it theoretically shows that a specific transformer model can get relative and absolute positional information, even in the absence of PE. Haviv et al. (2022) also demonstrates NoPE achieves comparative performance to standard Transformer models. **These new studies pose a key challenge regarding the choice of whether using PE or not in Transformer architecture** (refer to Appendix A for more related works).

## 3 Model Extrapolation: NoPE vs. Weave PE

### Problem Definition

We mainly consider relative PE methods and formally define their self-attention dot product as a function \(f_{}\), which takes the query \(_{t}\) located on position \(t\), the key \(_{i}\) located on position \(i\), and their relative positions \(t-i\) as input parameters, as follows:

\[_{t},_{i}:=f_{}(_{t},_{i},t- i),\] (1)

where \(f_{}\) denotes a relative PE method such as RoPE or ALiBi.

For ALiBi,

\[_{t},_{i}:=f_{}(_{t},_{i },t-i)=_{t}^{T}_{i}-(t-i) C^{m+1},\]

where \(m\) is head index and \(C=2^{-2^{-_{2}(\#+3)}}\).

For NoPE,

\[_{t},_{i}:=_{t}^{T}_{i}.\]

Based on Equ.1 we formally define **weave PE** as follows:

\[_{t},_{i}:=f_{}(_{t},_{ i},t-i)=f_{}(_{t},_{i},(t-i)),\] (2)

where \(\) is a weave function which takes the relative position \(t-i\) as input parameter.

For example, ReRoPE Su (2023) can be considered as an example of weave PE, with its \(\) function defined as follows:

\[(t-i):=\{t-i&,&t-i N\\ N&,&t-i>N.\]

where \(N\) is a constant. ReRoPE's dot-product attention is:

\[_{t},_{i}:=f_{}(_{t},_{ i},t-i)=f_{}(_{t},_{i},(t-i))=_{t}^{T}R^{ (t-i)}_{i},\]

where \(R\) is a rotation matrix that rotates \((t-i)\) radians. This is based on RoPE's dot-product attention:

\[_{t},_{i}:=f_{}(_{t},_{i},t -i))=_{t}^{T}R^{(t-i)}_{i}.\]

### Motivation

Chen et al. (2023) explores the evolution of hidden state values and reveals a noticable phenomenon: as the position increases, the hidden state values will explode. This finding appears consistent with observed failures in extrapolation. Through probe experiments (refer to Appendix F), we investigate the alterations in hidden state values across various positions and layers. Results show a significant shift in the hidden state's value range upon surpassing the effective window. Interestingly, when employing extrapolation techniques, hidden state values exhibit noticeable suppression. This indicates that the effective range of hidden state values likely lies within a specific threshold. When the position exceeds the effective window length, the hidden state values surpass this threshold, resulting in extrapolation failures.

Previous work Kazemnejad et al. (2023) utilizes a constructive approach. By constructing the Transformer's weights, it enables the first and second layers to independently generate position information. Drawing inspiration from this, we endeavor to construct a Transformer model capable of mirroring this observation.

### Theoretical Analysis

In a multi-layer neural network, each layer's outputs, a.k.a hidden state values \(o\), become the inputs for the subsequent layer. To maintain stable network behavior, these values must remain within a reasonable range. **We define this observable boundary as the threshold \(\)**. This threshold can be either an upper bound or a lower bound. For our analysis, we focus on the lower bound of this threshold. A **successful extrapolation** occurs when a large model consistently generates accurate next tokens for a long input sequence. Conversely, a **failed extrapolation** happens when the model produces incorrect or nonsensical next tokens. Based on these definitions, we make the following assumptions:

**Assumptions.** In LLM, there is a lower bound as threshold \(\) for the hidden state value \(o\) in specific dimension and specific layer. Let \(M\) be the max window length for LLM. Predefine query \(_{Q}\), key \(_{K}\), value \(_{V}\) and output \(_{O}\) matrices, and feed-forward sub-layer \(_{1}\), \(_{2}\) matrices. When \(o>\), LLM extrapolates successfully. Once \(o<\), LLM extrapolation fails.

These assumptions indicate that by observing whether the hidden state value \(o\) in this dimension exceed the threshold \(\), we can predict whether the large model's extrapolation has failed. Building upon these assumptions, theoretical results for NoPE exceeding the effective window length are as follows:

**Theorem 3.1** (NoPE Extrapolation).: _Let \(x=[<bos>,x_{1},,x_{T}]\) be an input sequence of length \(T+1\) to the model. Then, there exists \(_{Q}\), \(_{K}\), \(_{V}\), \(_{O}\), \(_{1}\), and \(_{2}\) matrices, such that when \(T<M\), \(o_{T}>\); and when \(T>M\), \(o_{T}<\)._

Full proof is given in Appendix E.1. This theorem reveals the internal mechanism of NoPE extrapolation as the input length changes. The theoretical results for PE are as follows:

**Theorem 3.2** (PE Extrapolation).: _Let \(x=[<bos>,x_{1},,x_{T}]\) be an input sequence of length T+1 to the model. Consider a simple relative PE schema where dot product between query \(_{t}\) and key \(_{i}\) at positions \(t\) and \(i\)\((t i)\) can be expressed as: \(_{t},_{i}:=_{t}^{T}_{i}-(t-i)\). Then, there exists \(_{Q}\), \(_{K}\), \(_{V}\), \(_{O}\), \(_{1}\), and \(_{2}\) matrices, such that when \(T<M\), \(o_{T}>\); and when \(T>M\), \(o_{T}<\)._

Full proof is given in Appendix E.2. Theorems 3.1 and 3.2 state the failure of length extrapolation in NoPE and PE, respectively.

Building on Theorem 3.2, we further investigate the case for carefully orchestrated weave PE. The theoretical result is as follows:

**Theorem 3.3** (Weave PE Extrapolation).: _Let \(N\) be a positive constant. Consider a simple weave PE extrapolation schema: when \(t-i<N\), \((t-i)=t-i\); and when \(t-i N\), \((t-i)=N\). Then, the attention dot product is fixed as below:_

\[_{t},_{i}:=\{_{t}^{T} _{i}-(t-i)&, t-i<N\\ _{t}^{T}_{i}-N&, t-i N.\]

_, where \(N M\). Then, applying \(_{Q}\), \(_{K}\), \(_{V}\), \(_{O}\), \(_{1}\), and \(_{2}\) matrices from Theorem 3.2, we have when \(T>M\), \(o_{T}>\)._Full proof is given in Appendix E.3. This theorem suggests that for existing LLMs relying on PE, simply weaving its relative positional encoding can effectively extend the input window.

Through Theorem 3.1 and Theorem 3.2, we formulate pertinent theoretical models for NoPE and PE, respectively, shedding light on the intricate relationship between extrapolation and the effective window length. Building upon these findings, Theorem 3.3 delves deeper into the realm of explicit PE, revealing that a well-designed weave PE scheme can effectively broaden the original effective window length. The theorems show the existence of an effective length \(M\), which is typically related to the maximum training window length of the LLM. Furthermore, within the proof of Theorem 3.3, our results indicate that \(N M\), consistent to experimental findings in Su (2023b), where although the extrapolation position can theoretically start from 2048, the best extrapolation starting position is 512. More experimental parameters also validate this setting (refer to Appendix B.2).

### Validating Extrapolation Using Observed Thresholds

Further, we design probe experiments (refer to Appendix F.3 for more results) to validate the observed phenomena and our theorems, as shown in Figure 2. From Figure 2, two observations are noted: Firstly, for hidden state values of the same dimension, the first layer undergoes minimal change, while the second layer exhibits a more pronounced transition. Exceeding the threshold implies extrapolation failure. This observation aligns with our theoretical model construction, where the first layer primarily refines positional information, with significant signal changes occurring in the second layer, as demonstrated in Theorem 3.2. Secondly, based on observational thresholds, when the length of the input sequence is around 12k, the values of hidden state corresponding to Dynamic-NTK Liu et al. (2023) surpass the threshold, implying extrapolation failure. Conversely, for ReRoPE Su (2023b), extrapolation succeeds. These two predictive outcomes corroborate with subsequent experimental results.

## 4 Mesa-Extrapolation

In this section, we begin by introducing a novel weave position encoding method, termed Stair Position Encoding (Stair PE). Following this, we propose a chunk-based triangular attention matrix. Building on these concepts, we introduce the implementation of Mesa-Extrapolation. Lastly, we discuss the theoretical properties of these innovations.

### Stair PE

Following the concept of weave PE in Equ.2, we define a novel weave PE method, namely Stair PE as follows:

\[_{t},_{i}:=f_{StairPE}(q_{t},k_{i},t-i)=f_{PE}(q_{t}, k_{i},(t-i)),\ \ \ \ (t-i):=\{t-i&,&t-i N\\ I&,&t-i>N.\] (3)

Figure 2: Thresholds for hidden states observed at specific dimensions on LLaMA2-7B-Chat, allowing for extrapolative judgments based on these thresholds. The vertical black dashed line indicate the position of maximum training length of the model. In this case, it is 4k for LLaMA2-7B-Chat model. The hidden state value at this position is designated as the observed threshold and marked with a horizontal red dashed line. When the hidden state value exceeds the red dashed line as the position changes, it signifies that the hidden state value has surpassed the threshold, suggesting a failure in extrapolation after that position.

where \(I=N+\). \(N\) denotes the extrapolated position, and \(E\) denotes the extrapolated width. Both \(N\) and \(E\) are positive constants. Stair PE can be applied to existing relative PEs such as RoPE and ALBiBi. For example, for RoPE:

\[_{t},_{i}:=f_{}(_{t},_{ i},t-i)=f_{}(_{t},_{i},(t-i))=_{t}^{T}R^{ (t-i)}_{i}.\]

Taken a sequence of length \(10\) as an example, the right subplot of Figure 1 shows that the relative positions generated by PE (Both RoPE and ALBi) are linear, while those generated by Stair PE (here \(N=4\) and \(E=2\)) are non-linear. Since weave PE changes the linear relative position, it has to calculate the attention matrix more than once Su (2023b). Compared with ReRoPE (detailed on Appendix B.3), Stair PE provides a finer-grained extrapolated positions. Meanwhile, compared with Leaky-ReRoPE, Stair PE reuses known positions, reducing possible generalization errors. We also provide an ablation experiment to compare these weave PE methods (refer to Appendix C.6).

While our work was conducted independently, we note that Jin et al. (2024) have recently explored a similar idea through flooring the original positions and obtaining the relative position matrix with grouped self-attention. Although the two parallel thought processes are different, under certain conditions their formulations are equivalent (refer to Appendix G). Consequently, Self-Extend can be categorized as a Weave PE method. Our proposed Chunk-based Triangular Attention Matrix (detailed in Section 4.2) and its corresponding theoretical properties (Section 4.4) are also applicable to this parallel approach.

### Chunk-based Triangular Attention Matrix

We design a chunk-based triangular attention matrix as shown in the left subplot of Figure 1. To achieve approximate linear memory consumption and computational speed, we further split the triangular attention matrix into several chunks and concatenate these chunks. We segment the input sequence into several sub-sequences according to _DynamicSplit_ function (defined in Appendix 2), which divides a sequence into sub-sequences of equal length, with the exception of the first and last sequences. The length of each sub-sequence is determined by both the input token length and the max training length. Each of the generated sub-sequences then undergoes a self-attention operation to generate a corresponding chunk. That is, a sub-sequence of length \(l\) will generate a corresponding chunk with the size \(l l\).

### Implementation

Mesa-Extrapolation mainly utilizes the chunk-based triangular attention matrix and Stair PE. Notice that regular PE (such as RoPE or ALBi) is applied to all chunks except for the last chunk, for which Stair PE is applied. For the last chunk, all previous chunks are concatenated, and Stair PE is used to rearrange relative positional encoding to achieve extrapolation beyond the effective window length.

In summary, the process of Mesa-Extrapolation mainly contains four steps (Algorithm 1): The first three steps correspond to the _prefill_ stage, which is used to calculate all input tokens. The last step corresponds to the _decoding_ stage, which is used to generate next-token one by one.

Firstly, _DynamicSplit_ function segments the input sequence, and the first segmented sub-sequence is fed into LLM to generate the first attention matrix chunk (line 3 in Algo.1). Secondly, subsequential sequences are iteratively processed while simultaneously feeding the key and value pairs of the first chunk into LLM to generate subsequent chunks (line 6-10 in Algo.1). Thirdly, the last sub-sequence is processed by concatenating the key and value pairs of all previous chunks together and using Stair PE to modify the relative positional encoding. Then, it is fed into the LLM to produce the last chunk (line 11-12 in Algo.1). Finally, Stair PE is applied to process the current token and cached Key and Value pairs to generate next-token one by one (line 13-14 in Algo.1). We establish the effectiveness of our proposed Mesa-Extrapolation in the next section.

### Theoretical Properties

Theorem 3.2 establishes a theoretical measurement for evaluating the effectiveness of extrapolation. We consistently apply this indicator, along with adjustments in relative positioning using Stair PE, to validate the feasibility of extrapolation. The result is as below:

**Corollary 4.1** (Mesa Extrapolation).: _Let \(N\) be a positive constant. Consider a simple Stair PE extrapolation schema, and the attention dot product is fixed as:_

\[_{t},_{i}:=f_{}(_{t},_{i },t-i)=\{_{t}^{T}_{i}-(t-i)&,&t-i<N\\ _{t}^{T}_{i}-I&,&t-i N.\]

_where \(N M\), \(I=N+\), and the extrapolated width \(E\) is a constant. Then, Apply \(_{Q}\), \(_{K}\), \(_{V}\), \(_{O}\), \(_{1}\), and \(_{2}\) matrices from Theorem 3.2. Although \(T>M\), it still \(o_{T}>\)._

Full proof is provided in Appendix E.4. We prove that Mesa-Extrapolation can effectively extrapolate outside the max window length.

## 5 Experiments

In this section, we validate the performance of Mesa-Extrapolation through experiments measured over multiple metrics. We choose GovReport Huang et al. (2021), Pile Gao et al. (2020), LongBench Bai et al. (2023), and LongEval Krishna et al. (2023) datasets, and also generate a passkey dataset, which has been integrated in the code warehouse. More experimental details and results are referred to Appendix B and C.

Since our method is completely free plug-in and does not require fine-tuning, we choose methods of this type for comparison, including: model self (Origin), ReRoPE Su (2023b), Leaky-ReRoPE Su (2023b), Dynamic-NTK Liu et al. (2023), LM-Infinite Han et al. (2023), and Streaming-LLM Xiao et al. (2023).

We evaluate Mesa-Extrapolation using three prominent LLM families: LLaMA Touvron et al. (2023a) Touvron et al. (2023b) (including LLaMA-3B (Open-LLaMA-3B), LLaMA2-7B-Chat, and Vicuna-13B-V1.3), MPT Team (2023) (including MPT-7B), and PyThia Biderman et al. (2023) (including PyThia-6.9B and PyThia-12B). Notably, LLaMA and PyThia incorporate RoPE Su et al. (2023), whereas MPT employs ALiBi Press et al. (2021) - two of the most influential PE techniques in recent research. Furthermore, we validated our approach using the Phi-3-mini-128k-instruct model Microsoft (2024) (refer to Appendix C.9).

We also conduct ablation experiments (refer to Appendix C.6). We use a 2xA800 80GB NVIDIA GPU server as the experimental environment and adopt the PyTorch framework.

### Evaluation on Passkey Retrieval Tasks

We assess the accuracy of Mesa-Extrapolation using the generated passkey dataset. This dataset comprises samples of varying lengths, each storing a random password at a random position. The sample length initiates at \(1024\) and increments by \(1024\). Simultaneously, \(100\) samples are randomly generated for each length. The proportion of correct answers found by LLMs is calculated for each input length.

Fig.3 shows the results of \(6\) LLMs on passkey retrieval task. The LLaMA families can employ various methods, including Origin, ReRoPE, Leaky-ReRoPE, Dynamic-NTK, LM-Infinite, Streaming-LLM, and our Mesa-Extrapolation. Note that these methods are model-specific and may not be universally applicable across all model series. For the MPT model, Origin, Streaming-LLM, ReRoPE, and Mesa-Extrapolation can be utilized. Similarly, for the PyThia model, Origin, Streaming-LLM, and Mesa-Extrapolation can be applied.

Analyzing the LLaMA model series reveals that weave PE-based methods, including ReRoPE, LeakyReRoPE and Mesa-Extrapolation, achieve superior extrapolation capabilities. Additionally, under our existing hardware constraints, Mesa-Extrapolation demonstrates longer extrapolation capabilities.

In the case of MPT model, after surpassing the maximum training length, the extrapolation capabilities of Mesa-Extrapolation and ReRoPE show a decline. We speculate that this may be attributed to an approximate ALiBi PE applied on MPT model. This approximate ALiBi would lead to significant disruptions when weaving its positions. (refer to Appendix C.2 for more details about MPT).

In the PyThia models, an additional observation is that \(100\)% accuracy is still not achieved within the training length. We attribute this to the PyThia model being a pre-trained model without instruction alignment, resulting in a weakened intrinsic understanding of the task. Results with enhanced prompts are referred to Appendix C.5.

Dynamic-NTK exhibits partial extrapolation capabilities beyond the maximum training length. Streaming-LLM and LM-Infinite also exhibit poor performance. This is because these methods discard portions of the input token, leading to potential information loss and incorrect answers.

### Evaluation on Language Modeling

We further assess the fluency of Mesa-Extrapolation utilizing the perplexity metric. Results evaluated on the Pile dataset are presented in Fig.4. X-axis represents the length of the input token, while the

Figure 4: Perplexity (PPL) metrics on LLaMA models using the Pile dataset. Some observations: (1) The PPL value of Origin consistently increases when the maximum training length is exceeded. (2) Other methods maintain low PPL values, with Dynamic-NTK exhibiting a slight increase as the input length grows.

Figure 3: Passkey Retrieval Accuracy for different methods on various LLMs. X-axis represents the input token length, and Y-axis represents the accuracy of password found by LLMs. Different color regions denote the variance value, averaged on \(100\) samples for each input token length. The black dashed line represent the max training length for LLMs. Some observations: Weave PE-based methods, including ReRoPE, Leaky-ReRoPE, and Mesa-Extrapolation, consistently demonstrate stable extrapolation capabilities even when the input length surpasses the maximum training length. We claim that “early stopping” phenomenon in certain methods is attributed to GPU memory exhaustion under our existing hardware resources.

Y-axis corresponds to NLL (Negative Log-Likelihood) values. It can be observed that the NLL value of Origin consistently increases when the maximum training length is exceeded. Other methods maintain low NLL values. LM-Infinite performs marginally better on Vicuna-13B. Dynamic-NTK method exhibits slightly weaker performance after \(11\)k, and the performance continues to drop as the input length increases. In summary, our Mesa-Extrapolation demonstrates comparable performance to other methods on PPL metric.

### Evaluation on Summary of Tasks

We conduct a summary task using the GovReport dataset and employ ROUGE ROUGE (2004) (ROUGE-1/2/L) as evaluation metrics. ROUGE assess overlapping N-grams by comparing the generated text with reference answers.

For the GovReport dataset, we segment the range from \(3\)*\(1024\) to \(11\)*\(1024\) based on sample length, with each interval of \(1024\) units. A test set is created by randomly selecting \(8\) samples from each interval. We choose LLaMA2-7B-Chat as the evaluated LLM. The experimental results for ROUGE is presented in Tables 1 below:

In Table 1, we record the average scores of Rouge-1, Rouge-2, and Rouge-L within each interval. It is evident that once the effective input window is exceeded, the performance of Origin and Streaming-LLM declines rapidly, rendering it useless. For LM-Infinite, the scores exhibit a slight decrease as the length increases. Dynamic-NTK shows slightly better performance within \(11\)k. However, combined with the Fluency experiment on Fig.4, it seems that the effective extrapolation range of Dynamic-NTK cannot exceed \(12\)k, which is consistent with the threshold observed in Fig.2. Weave PE-based methods, including ReRoPE, Leaky-ReRoPE and Mesa-Extrapolation maintain similar generation quality as the length increases. Note our Mesa-Extrapolation shows slight variability in performance within mid-length (8k-11k) in the summary task. We speculate that with a fixed extrapolation width param (\(E=50\)), the \(7\)k-\(11\)k range may spread the model's attention more thinly compared to the \(4\)k-\(6\)k range. We hypothesize that optimizing the extrapolation width could alleviate or improve performance in the \(7\)k-\(11\)k range.

### Latency & Memory Usage

To compare actual memory consumption and inference speed, we conduct experiments using both the 3B and 7B versions of the LLaMA model. The results are presented in Fig.5. In Fig.5, the X-axis

 
**Input Length** & **M** & **M** & **M** & **M** & **M** & **M** & **M** & **M** & **M** \\  Origin & 35.91/4287 & 36.64/73.85 & 6.61/61.11 & 36.09/60.64 & 0.91/60.68 & 0.90/60.00 & 0.90/60.00 & 0.90/60.00 & 0.90/60.00 \\ ReRoPE & 35.91/42519 & 35.72/43.9322 & 10.39/35.78 & 39.21/61.76 & 36.67/12.64 & 3.109/37.08 & 35.72/33.10/34.02 & **35.72/33.42** & 34.61/302.5 \\ Leaky-ReRoPE & 35.61/22.003 & 36.34/73.13 & 35.75/43.42 & 35.47/42.73 & 35.47/42.63 & 35.46/43.93 & 35.46/43.93 & 35.46/43.93 & 35.72/33.29 & 35.91/35.28 & 35.01/35.28 \\ Dynamic-NTK & 33.87/13606 & 3.91/60.16 & 36.91/62.13 & 36.27/62.05 & 36.65/43.93 & **35.46/43.93** & **35.43/43.93** & **35.72/33.29** & 35.72/33.29 & 35.94/43.93 \\ L4-bit Infinite & 34.61/83.83 & 35.25/12.05 & 33.51/62.06 & 33.71/10.49 & 31.04/29.290 & 32.10/30.78 & 2.79/42.41 & 27.98/12.56 & 29.49/43.92 \\ Streaming-LLM & **32.81/375062** & 3.81/57.65 & 16.00/61.67 & 6.00/61.67 & 6.00/60.00 & 0.90/60.00 & 0.90/60.00 & 0.90/60.00 & 0.90/60.00 & 0.90/60.00 \\ Mesa-Extrapolation & 35.91/64.63 & 4.12/**482.03/47.35** & 35.71/43.14 & **35.41/28.03** & **35.41** & 31.11/41.63 & 32.70/39.04 & 30.71/32.73 & 33.94/42.00 & 30.71/32.66 \\  

Table 1: ROUGE metric on LLaMA2-7B-Chat, averaged on \(8\) samples within each interval using the GovReport dataset. Each cell contains ROUGE-1/ROUGE-2/ROUGE-L. The best values are marked in bold. Some observations: (1) Dynamic-NTK shows slightly better performance within \(11\)k. (2) Other methods showcase the ability to achieve scores of varying degrees.

Figure 5: Memory Usage and Decoding Speed Comparison for LLaMA Models: 3B and 7B. The X-axis represents the input token length, the left Y-axis denotes memory usage, and the right Y-axis indicates speed about decoding time during inference. Some observations: (1) ReRoPE and Leaky-ReRoPE exhibit the largest memory footprint for the same input length, and their inference speed follows a quadratic function trend. (2) Mesa-Extrapolation shows an approximately linear inference speed, boasting the fastest inference speed and the smallest memory usage under the same input conditions.

represents the input token length, the left Y-axis denotes memory usage, and the right Y-axis indicates decoding time. It is noteworthy that decoding time is closely related to memory usage, primarily from the computation of the attention matrix.

Observing Fig.5, both memory usage and decoding time for Origin and Dynamic-NTK exhibit a quadratic trend. Similarly, ReRoPE and Leaky-ReRoPE exhibit the highest memory usage and decoding time, showcasing a quadratic trend, which aligns with our analysis (refer to Appendix C.7). Although LM-Infinite demonstrates a linear trend, its increase is substantial. In contrast, Mesa-Extrapolation method also exhibits a linear trend but significantly outperforms other methods in terms of memory usage and decoding time. Furthermore, as the input length increases, this trend becomes more pronounced.

## 6 Conclusion

Our study addresses the critical challenge faced by Large Language Models (LLMs) when confronted with longer input lengths, commonly referred to as the extrapolation problem. Through theoretical exploration, we uncover the underlying mechanisms of this challenge, shedding light on the reason of extrapolation failure for both NoPE and PE. Furthermore, we present theoretical evidence demonstrating the potential for effective extrapolation using Weave PE. Based on Weave PE, we introduce a practical solution called Mesa-Extrapolation, which strategically organizes input tokens into chunks to achieve competitive performance with minimal resource usage. Empirical validation demonstrates its effectiveness. Our work not only advances the understanding of the extrapolation problem but also offers a practical solution, with a complete free plug-in for LLMs.

**Limitations.** Mesa-Extrapolation is a plug-and-play method that does not require additional fine-tuning. However, previous work, "NTK-aware" ntk (2023) also shows that applying further fine-tuning to plug-in extrapolation is possible. Therefore exploring fine-tuning based on Mesa-Extrapolation can be an interesting next step. Due to limitations of resources, we have not yet validated our method at longer lengths.

**Broader Impacts** We contend that the significance of completely free plug-in extrapolation method lies in two aspects. Firstly, it enables the expansion of the effective window length of already trained LLMs with no additional cost. Secondly, it allows for training LLMs from scratch with short texts and subsequently expanding their effective window length with a free plug-in extrapolation method, which can greatly help the industry improve the extrapolation capabilities of diverse LLMs.

## 7 Acknowledgement

This work was supported by the National Key R&D Program of China under Grant No.2022ZD0160504 and Tsinghua University(AIR)-Asiainfo Technologies (China) Inc. Joint Research Center. We would also like to acknowledge anonymous reviewers and Zengxiang Lu for their valuable feedback and discussions.