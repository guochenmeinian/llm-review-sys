# Understanding Diffusion Objectives as the ELBO

with Simple Data Augmentation

 Diederik P. Kingma

Google DeepMind

durk@google.com &Ruiqi Gao

Google DeepMind

ruiqig@google.com

###### Abstract

To achieve the highest perceptual quality, state-of-the-art diffusion models are optimized with objectives that typically look very different from the maximum likelihood and the Evidence Lower Bound (ELBO) objectives. In this work, we reveal that diffusion model objectives are actually closely related to the ELBO.

Specifically, we show that all commonly used diffusion model objectives equate to a weighted integral of ELBOs over different noise levels, where the weighting depends on the specific objective used. Under the condition of monotonic weighting, the connection is even closer: the diffusion objective then equals the ELBO, combined with simple data augmentation, namely Gaussian noise perturbation. We show that this condition holds for a number of state-of-the-art diffusion models.

In experiments, we explore new monotonic weightings and demonstrate their effectiveness, achieving state-of-the-art FID scores on the high-resolution ImageNet benchmark.

## 1 Introduction

Diffusion-based generative models, or diffusion models in short, were first introduced by Sohl-Dickstein et al. (2015). After years of relative obscurity, this class of models suddenly rose to prominence with the work of Song and Ermon (2019) and Ho et al. (2020) who demonstrated that, with further refinements in model architectures and objective functions, diffusion models can perform state-of-the-art image generation.

Figure 1: Samples generated from our _VDM++_ diffusion models trained on the ImageNet dataset; see Section 5 for details and Appendix M for more samples.

The diffusion model framework has since been successfully applied to text-to-image generation (Rombach et al., 2022b; Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022; Yu et al., 2022; Nichol and Dhariwal, 2021; Ho et al., 2022; Dhariwal and Nichol, 2022; Ding et al., 2021), image-to-image generation (Saharia et al., 2022c, a; Whang et al., 2022), 3D synthesis (Poole et al., 2022; Watson et al., 2022), text-to-speech (Chen et al., 2021a; Kong et al., 2021; Chen et al., 2021b), and density estimation (Kingma et al., 2021; Song et al., 2021a).

Diffusion models can be interpreted as a special case of deep variational autoencoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) with a particular choice of inference model and generative model. Just like VAEs, the original diffusion models (Sohl-Dickstein et al., 2015) were optimized by maximizing the variational lower bound of the log-likelihood of the data, also called the evidence lower bound, or ELBO for short. It was shown by _Variational Diffusion Models_ (VDM) (Kingma et al., 2021) and (Song et al., 2021a) how to optimize _continuous-time_ diffusion models with the ELBO objective, achieving state-of-the-art likelihoods on image density estimation benchmarks.

However, the best results in terms of sample quality metrics such as FID scores were achieved with other objectives, for example a denoising score matching objective (Song and Ermon, 2019) or a simple noise-prediction objective (Ho et al., 2020). These now-popular objective functions look, on the face of it, very different from the traditionally popular maximum likelihood and ELBO objectives. Through the analysis in this paper, we reveal that all training objectives used in state-of-the-art diffusion models are actually closely related to the ELBO objective.

This paper is structured as follows:

* In Section 2 we introduce the broad diffusion model family under consideration.
* In Section 3, we show how the various diffusion model objectives in the literature can be understood as special cases of a _weighted loss_(Kingma et al., 2021; Song et al., 2021a), with different choices of weighting. The weighting function specifies the weight per noise level. In Section 3.2 we show that during training, the noise schedule acts as a importance sampling distribution for estimating the loss, and is thus important for efficient optimization. Based on this insight we propose a simple adaptive noise schedule.
* In Section 4, we present our main result: that if the weighting function is a monotonic function of time, then the weighted loss corresponds to maximizing the ELBO with data augmentation, namely Gaussian noise perturbation. This holds for, for example, the \(\)-prediction loss of (Salimans and Ho, 2022) and flow matching with the optimal transport path (Lipman et al., 2022).
* In Section 5 we perform experiments with various new monotonic weights on the ImageNet dataset, and find that our proposed monotonic weighting produces models with sample quality that are competitive with the best published results, achieving state-of-art FID and IS scores on high resolution ImageNet generation.

### Related work

The main sections reference much of the related work. Earlier work (Kingma et al., 2021; Song et al., 2021a; Huang et al., 2021; Vahdat et al., 2021), including Variational Diffusion Models (Kingma et al., 2021), showed how to optimize continous-time diffusion models towards the ELBO objective. We generalize these earlier results by showing that any diffusion objective that corresponds with monotonic weighting corresponds to the ELBO, combined with a form of DistAug (Child et al., 2019). DistAug is a method of training data distribution augmentation for generative models where the model is conditioned on the data augmentation parameter at training time, and conditioned on 'no augmentation' at inference time. The type of data augmentation under consideration in this paper, namely additive Gaussian noise, is also a form of data distribution smoothing, which has been shown to improve sample quality in autoregressive models by Meng et al. (2021).

Kingma et al. (2021) showed how the ELBO is invariant to the choice of noise schedule, except for the endpoints. We generalize this result by showing that the invariance holds for any weighting function.

## 2 Model

Suppose we have a dataset of datapoints drawn from \(q()\). We wish to learn a generative model \(p_{}()\) that approximates \(q()\). We'll use shorthand notation \(p:=p_{}\).

The observed variable \(\) might be the output of a pre-trained encoder, as in _latent diffusion models_(Vahdat et al., 2021; Rombach et al., 2022), on which the popular _Stable Diffusion_ model is based. Our theoretical analysis also applies to this type of model.

In addition to the observed variable \(\), we have a series of latent variables \(_{t}\) for timesteps \(t\): \(_{0,,1}:=_{0},...,_{1}\). The model consists of two parts: a _forward process_ forming a conditional joint distribution \(q(_{0,,1}|)\), and a _generative model_ forming a joint distribution \(p(_{0,,1})\).

### Forward process and noise schedule

The forward process is a Gaussian diffusion process, giving rise to a conditional distribution \(q(_{0,,1}|)\); see Appendix E.1 for details. For every \(t\), the marginal distribution \(q(_{t}|)\) is given by:

\[_{t}=_{}+_{}\ \ \ \ (0,).\] (1)

In case of the often-used _variance preserving_ (VP) forward process, \(_{}^{2}=(_{t})\) and \(_{}^{2}=(-_{t})\), but other choices are possible; our results are agnostic to this choice. The _log signal-to-noise ratio_ (log-SNR) for timestep \(t\) is given by \(=(_{}^{2}/_{}^{2})\).

The _noise schedule_ is a strictly monotonically decreasing function \(f_{}\) that maps from the time variable \(t\) to the corresponding log-SNR \(\): \(=f_{}(t)\). We sometimes denote the log-SNR as \(_{t}\) to emphasize that it is a function of \(t\). The endpoints of the noise schedule are given by \(_{}:=f_{}(0)\) and \(_{}:=f_{}(1)\). See Figure 2 for a visualization of commonly used noise schedules in the literature, and Appendix E.3 for more details.

Due to its monotonicity, \(f_{}\) is invertible: \(t=f_{}^{-1}()\). Given this bijection, we can do a change of variables: a function of the value \(t\) can be equivalently written as a function of the corresponding value \(\), and vice versa, which we'll make use of in this work.

During model training, we sample time \(t\) uniformly: \(t(0,1)\), then compute \(=f_{}(t)\). This results in a distribution over noise levels \(p()=-dt/d=-1/f_{}^{}(t)\) (see Section E.3), which we also plot in Figure 2.

Sometimes it is best to use a different noise schedule for sampling from the model than for training. During sampling, the density \(p()\) gives the relative amount of time the sampler spends at different noise levels.

Figure 2: **Left:** Noise schedules used in our experiments: cosine (Nichol and Dhariwal, 2021) and EDM (Karras et al., 2022) training and sampling schedules. **Right:** The same noise schedules, expressed as probability densities \(p()=-dt/d\). See Section 2.1 and Appendix E.3 for details.

### Generative model

The data \(\), with density \(q()\), plus the forward model defines a joint distribution \(q(_{0},...,_{1})= q(_{0},...,_{1}| )q()d\), with marginals \(q_{t}():=q(_{t})\). The generative model defines a corresponding joint distribution over latent variables: \(p(_{0},...,_{1})\).

For large enough \(_{}\), \(_{0}\) is almost identical to \(\), so learning a model \(p(_{0})\) is practically equivalent to learning a model \(p()\). For small enough \(_{}\), \(_{1}\) holds almost no information about \(\), such that there exists a distribution \(p(_{1})\) satisfying \(D_{KL}(q(_{1}|)||p(_{1})) 0\). Usually we can use \(p(_{1})=(0,)\).

Let \(_{}(;)\) denote a _score model_, which is a neural network that we let approximate \(_{} q_{t}()\) through methods introduced in the next sections. If \(_{}(;)=_{} q _{t}()\), then the forward process can be exactly reversed; see Appendix E.4.

If \(D_{KL}(q(_{1})||p(_{1})) 0\) and \(_{}(;)_{}  q_{t}()\), then we have a good generative model in the sense that \(D_{KL}(q(_{0,...,1})||p(_{0,...,1})) 0\), which implies that \(D_{KL}(q(_{0})||p(_{0})) 0\) which achieves our goal. So, our generative modeling task is reduced to learning a score network \(_{}(;)\) that approximates \(_{} q_{t}()\).

Sampling from the generative model can be performed by sampling \(_{1} p(_{1})\), then (approximately) solving the reverse SDE using the estimated \(_{}(;)\). Recent diffusion models have used increasingly sophisticated procedures for approximating the reverse SDE; see Appendix E.4. In experiments we use the DDPM sampler from Ho et al. (2020) and the stochastic sampler with Heun's second order method proposed by Karras et al. (2022).

## 3 Diffusion Model Objectives

Denoising score matching.Above, we saw that we need to learn a score network \(_{}(;_{t})\) that approximates \(_{} q_{t}()\), for all noise levels \(_{t}\). It was shown by (Vincent, 2011; Song and Ermon, 2019) that this can be achieved by minimizing a denoising score matching objective over all noise scales and all datapoints \(\):

\[_{}()=_{t(0,1), (0,)}[(t)|| _{}(_{t};_{t})-_{_{t}} q(_{t}|)||_{2}^{2}]\] (2)

where \(_{t}=_{}+_{}\).

The \(\)-prediction objective.Most contemporary diffusion models are optimized towards a noise-prediction loss introduced by (Ho et al., 2020). In this case, the score network is typically parameterized through a noise-prediction (\(\)-prediction) model: \(_{}(;)=-}_{}(;)/_{}\). (Other options, such as \(\)-prediction, \(\)-prediction, and EDM parameterizations, are explained in Appendix E.2.) The noise-prediction loss is:

\[_{}()=_{t (0,1),(0,)}[|| }_{}(_{t};_{t}) -||_{2}^{2}]\] (3)

Since \(||_{}(_{t};_{t})-_{_{t}} q(_{t}|)||_{2}^{2}=_{}^{-2}|| }_{}(_{t};_{t})- ||_{2}^{2}\), this is simply a version of the denoising score matching objective in Equation 2, but where \((t)=_{t}^{2}\). Ho et al. (2020) showed that this noise-prediction objective can result in high-quality samples. Dhariwal and Nichol (2022) later improved upon these results by switching from a 'linear' to a 'cosine' noise schedule \(_{t}\) (see Figure 2). This noise-prediction loss with the cosine schedule is currently broadly used.

The ELBO objective.It was shown by (Kingma et al., 2021; Song et al., 2021) that the evidence lower bound (ELBO) of continuous-time diffusion models simplifies to:

\[-()=_{t(0,1), (0,)}[- ||}_{}(_{t}; _{t})-||_{2}^{2}]+c\] (4)

where \(c\) is constant w.r.t. the score network parameters.

### The weighted loss

The different objective functions used in practice, including the ones above, can be viewed as special cases of the _weighted loss_ introduced by Kingma et al. (2021)1with a different choices of weighting function \(w(_{t})\):

\[_{w}()=_{t(0,1 ),(0,)}[w(_{t}) -||}_{} (_{t};_{t})-||_{2}^{2}]}\] (5)

See Appendix D for a derivation of the implied weighting functions for all popular diffusion losses. Results are compiled in Table 1, and visualized in Figure 3.

The ELBO objective (Equation 4) corresponds to uniform weighting, i.e. \(w(_{t})=1\).

The popular noise-prediction objective (Equation 3) corresponds to \(w(_{t})=-1/(d/dt)\). This is more compactly expressed as \(w(_{t})=p(_{t})\), i.e., the PDF of the implied distribution over noise levels \(\) during training. Typically, the noise-prediction objective is used with the cosine schedule \(_{t}\), which implies \(w(_{t})(_{t}/2)\). See Section E.3 for the expression of \(p(_{t})\) for various noise schedules.

### Invariance of the weighted loss to the noise schedule \(_{t}\)

In Kingma et al. (2021), it was shown that the ELBO objective (Equation 4) is invariant to the choice of noise schedule, except for its endpoints \(_{}\) and \(_{}\). This result can be extended to the much

  
**Loss function** & **Implied weighting \(w()\)** & **Monotonic?** \\  ELBO (Kingma et al., 2021; Song et al., 2021) & 1 & \(\) \\ IDDPM (\(\)-prediction with ‘cosine’ schedule) (Nichol and Dhariwal, 2021) & \((/2)\) & \\ EDM (Karras et al., 2022) (Appendix D.1) & \((;2,4,2.4^{2})(e^{-}+0.5^{2})\) & \\ \(\)-prediction with ‘cosine’ schedule (Salimans and Ho, 2022) (Appendix D.2) & \(e^{-/2}\) & \(\) \\ Flow Matching with OT path (FM-OT) (Djømman et al., 2022) (Appendix D.3) & \(e^{-/2}\) & \(\) \\ Indl (Djømchao and Milanfar, 2023) (Appendix D.4) & \(e^{-}\)sech\({}^{2}(/4)\) & \(\) \\ P2 weighting with ‘cosine’ schedule (Cloi et al., 2022) (Appendix D.5) & \((/2)/(1+e^{})^{},=0.51\) \\ Min-SNR-\(\)(Hang et al., 2023) (Appendix D.6) & \((/2)(1, e^{-})\) & \\   

Table 1: Diffusion model objectives in the literature are special cases of the weighted loss with a weighting function \(w()\) given in this table. See Section 3.1 and Appendix D for more details and derivations. Most existing weighting functions are non-monotonic, except for the ELBO objective and the \(\)-prediction objective with ‘cosine’ schedule.

Figure 3: Diffusion model objectives in the literature are special cases of the weighted loss with non-monotonic (left) or monotonic (right) weighting functions. Each weighting function is scaled such that the maximum is 1 over the plotted range. See Table 1 and Appendix D.

more general weighted diffusion loss of Equation 5, since with a change of variables from \(t\) to \(\), it can be rewritten to:

\[_{w}()=_{_{}}^{_{ }}w()_{(0, )}[||_{}(_{};)-||_{2}^{2}}\ \ d.\] (6)

Note that this integral does not depend on the noise schedule \(f_{}\) (the mapping between \(t\) and \(\)), except for its endpoints \(_{}\) and \(_{}\). The shape of the function \(f_{}\) between \(_{}\) and \(_{}\) does not affect the loss; only the weighting function \(w()\) does. Given a chosen weighting function \(w()\), the loss is invariant to the noise schedule \(_{t}\) between \(t=0\) and \(t=1\). This is important, since it means that the only real difference between diffusion objectives is their difference in weighting \(w()\).

This invariance does _not_ hold for the Monte Carlo estimator of the loss that we use in training, based on random samples \(t(0,1),(0,)\). The noise schedule still affects the _variance_ of this Monte Carlo estimator and its gradients; therefore, the noise schedule affects the efficiency of optimization. More specifically, we'll show that the noise schedule acts as an importance sampling distribution for estimating the loss integral of Equation 6. Note that \(p()=-1/(d/dt)\). We can therefore rewrite the weighted loss as the following, which clarifies the role of \(p()\) as an importance sampling distribution:

\[_{w}()=_{(0,), p()}[ {p()}||_{}(_{ };)-||_{2}^{2}}]}\] (7)

Using this insight, we implemented an adaptive noise schedule, detailed in Appendix F. We find that by lowering the variance of the loss estimator, this often significantly speeds op optimization.

## 4 The weighted loss as the ELBO with data augmentation

We prove the following main result:

**Theorem 1**.: _If the weighting \(w(_{t})\) is monotonic, then the weighted diffusion objective of Equation 5 is equivalent to the ELBO with data augmentation (additive noise)._

With monotonic \(w(_{t})\) we mean that \(w\) is a monotonically increasing function of \(t\), and therefore a monotonically decreasing function of \(\).

We'll use shorthand notation \((t;)\) for the KL divergence between the joint distributions of the forward process \(q(_{t, 1}|)\) and the reverse model \(p(_{t, 1})\), for the subset of timesteps from \(t\) to 1:

\[(t;):=D_{KL}(q(_{t,,1}| )||p(_{t,,1}))}\] (8)

In Appendix A.1, we prove that2:

\[(t;)=_{(0,)}[||-_{}(_{ };)}||_{2}^{2}]\] (9)

As shown in Appendix A.1, this allows us to rewrite the weighted loss of Equation 5 as simply:

\[_{w}()=-_{0}^{1}(t;) \,w(_{t})\ dt\] (10)

In Appendix A.2, we prove that using integration by parts, the weighted loss can then be rewritten as:

\[_{w}()=_{0}^{1}w(_{t})\,(t;)\ dt+w(_{})\,(0;)+\] (11)Now, assume that \(w(_{t})\) is a monotonically increasing function of \(t\). Also, without loss of generality, assume that \(w(_{t})\) is normalized such that \(w(_{1})=1\). We can then further simplify the weighted loss to an expected KL divergence:

\[_{w}()=_{p_{w}(t)}[(t; )]+}\] (12)

where \(p_{w}(t)\) is a probability distribution determined by the weighting function, namely \(p_{w}(t):=(d/dt\ w(_{t}))\), with support on \(t\). The probability distribution \(p_{w}(t)\) has Dirac delta peak of typically very small mass \(w(_{})\) at \(t=0\).

Note that:

\[(t;) =D_{KL}(q(_{t,,1}|)||p(_{t, ,1}))\] (13) \[ D_{KL}(q(_{t}|)||p(_{t}))=- _{q(_{t}|)}[ p(_{t})]+.\] (14)

More specifically, \((t;)\) equals the expected negative ELBO of noise-perturbed data, plus a constant; see Section C for a detailed derivation.

This concludes our proof of Theorem 1. 

This result provides us the new insight that any of the objectives with (implied) monotonic weighting, as listed in Table 1, can be understood as equivalent to the ELBO with simple data augmentation, namely additive noise. Specifically, this is a form of Distribution Augmentation (DistAug), where the model is conditioned on the augmentation indicator during training, and conditioned on 'no augmentation' during sampling.

Monotonicity of \(w()\) holds for a number of models with state-of-the-art perceptual quality, including VoiceBox for speech generation (Le et al., 2023), and Simple Diffusion for image generation (Hoogeboom et al., 2023).

### Understanding the effect of weighting functions on perceptual quality

In this subsection, we propose possible explanations of the reason that the weighted objective, or equivalently the ELBO with data augmentation assuming monotonic weightings, can effectively improve perceptual quality.

Connection to low-bit training.Earlier work (Kingma and Dhariwal, 2018) found that training on 5-bit data, removing the three least significant bits from the original 8-bit data, can lead to higher fidelity models. A likely reason is the more significant bits are also more important to human perception, and removing the three least significant bits from the data allows the model to spend more capacity on modeling the more significant bits. In (Kingma and Dhariwal, 2018), training on 5-bit data was performed by adding uniform noise (with the appropriate scale) to the data before feeding it to the model, but it was found that adding Gaussian noise had a similar effect. Adding Gaussian noise with a single noise level is a special case of the weighted objective where the weighting function is a step function. The effect of uniform noise can be approximated with a sigmoidal weighting function; in Appendix I we dive into more details.

Fourier analysis.To better understand why additive Gaussian noise can improve perceptual quality, consider the Fourier transform (FT) of a clean natural image perturbed with additive Gaussian noise. Since the FT is a linear transformation, the FT of a the sum of a natural image and Gaussian noise, equals the sum of the FT of the clean image and the FT of the Gaussian noise. For natural images, it is known that the power spectrum, i.e. the average magnitude of the FT as a function of spatial frequency, decreases fast as the frequency increases (Burton and Moorhead, 1987), which means that the lowest frequencies have by far the highest magnitudes. On the other hand, the power spectrum of a Gaussian white noise is roughly constant. When adding noise to a clean natural image, the signal-to-noise ratio (i.e. the ratio of the power spectra of the clean and Gaussian noise images) in high frequency regions is lower. As a result, additive Gaussian noise effectively destroys high-frequency information in the data more quickly than the low-frequency information, pushing the model to focus more on the low frequency components of the data, which often correspond to high-level content and global structure that is more crucial to perception. See Appendix J for a more detailed discussion.

## 5 Experiments

Inspired by the theoretical results in Section 4, in this section we propose several monotonic weighting functions, and present experiments that test the effectiveness of the monotonic weighting functions compared to baseline (non-monotonic) weighting functions. In addition, we test the adaptive noise schedule (Section 3.2). For brevity we had to pick a name for our models. Since we build on earlier results on Variational Diffusion Models (VDM) (Kingma et al., 2021), and our objective is equivalent to the VDM objective combined with data augmentation, we name our models _VDM++_.

### Weighting functions

We experimented with several new monotonic weighting functions. First, a sigmoidal weighting of the form \((-+k)\), inspired by the sigmoidal weighting corresponding to low-bit training (see Section 4.1 and Appendix I for details), where \(k\) is a hyper-parameter. In addition, inspired by the EDM weighting function from Karras et al. (2022) (non-monotonic, Table 1), we test a weighting function indicated with 'EDM-monotonic', which is identical to the EDM weighting, except that it is made monotonic by letting \(w()=_{}()\) for \(<_{}()\), where \(()\) indicates the original EDM weighting function. Put simply, 'EDM-monotonic' equals the EDM weighting, except that it is constant to the left of its peak; see Figure 3.

### ImageNet 64x64

All experiments on ImageNet 64x64 were done with the U-Net diffusion model architecture from (Nichol and Dhariwal, 2021). We carried out extensive ablation studies over several design choices of diffusion models, namely model parametrization, training noise schedules, weighting functions and samplers. For \(\)-parametrization model, we took iDDPM (Nichol and Dhariwal, 2021) as the baseline, which utilized cosine noise schedule and a non-monotonic \((/2)\) weighting. For EDM parmetrization, we recruited the setting in Karras et al. (2022) as the baseline, with EDM training noise schedule (Figure 2) and non-monotonic EDM weighting (Figure 3). For \(\)-parametrization model, we followed Salimans and Ho (2022) taking the \(\)-prediction loss as the baseline, which leads to a monotonic \((-/2)\) weighting. We always use adaptive noise schedule under this setting. We started by searching the optimal hyperparameter for the sigmoidal weighting under the \(\)-parametrization model, and then applied the best sigmoidal weighting and EDM-monotonic weighting under other settings. We evaluated models with two samplers, DDPM and EDM samplers, with their corresponding sampling noise schedules. Table 2 summarizes the FID (Heusel et al., 2017) and Inception scores (Salimans et al., 2016) across different settings. From the table we made several observations as follows.

    & & & &  &  \\
**Model parameterization** & **Training noise schedule** & **Weighting function** & **Monotonic?** & **FID \(\)** & **IS \(\)** & **FID \(\)** & **IS \(\)** \\  \(\)-parametrization & Cosine & \((/2)\) (Baseline) & & 1.85 & 54.1 \(\) 0.79 & 1.55 & 59.2 \(\) 0.78 \\ \(\) & Cosine & \((-+1)\) & ✓ & 1.75 & 55.3 \(\) 1.23 & & & \\ \(\) & Cosine & \((-+2)\) & ✓ & **1.68** & **56.8 \(\) 0.85 & 1.46 & 60.4 \(\) 0.86 \\ \(\) & Cosine & \((-+3)\) & ✓ & 1.73 & 56.1 \(\) 1.36 & & & \\ \(\) & Cosine & \((-+4)\) & ✓ & 1.80 & 55.1 \(\) 1.65 & & & \\ \(\) & Cosine & \((-+5)\) & ✓ & 1.94 & 53.5 \(\) 1.12 & & & \\ \(\) & Adaptive & \((-+2)\) & ✓ & 1.70 & 54.8 \(\) 1.20 & **1.44** & 60.6 \(\) 1.44 \\ \(\) & Adaptive & EDM-monotonic & ✓ & **1.67** & **56.8 \(\) 0.90 & **1.44** & **61.1 \(\) 1.80** \\  EDM (Karras et al., 2022) & EDM (training) & EDM (Baseline) & & & & 1.36 & & \\ EDM (our reproduction) & EDM (training) & EDM (Baseline) & & & & 1.45 & 60.7 \(\) 1.19 \\ \(\) & Adaptive & EDM & & & & **1.43** & 63.2 \(\) 1.76 \\ \(\) & Adaptive & sigmoid(\(-+2)\) & ✓ & & & 1.55 & **63.7 \(\) 1.14 \\ \(\) & Adaptive & EDM-monotonic & ✓ & & & **1.43** & **63.7 \(\) 1.48** \\  \(\)-parametrization & Adaptive & \((-/2)\) (Baseline) & ✓ & & & 1.62 & 58.0 \(\) 1.56 \\ \(\) & Adaptive & sigmoid(\(-+2)\) & ✓ & & & 1.51 & 64.4 \(\) 1.28 \\ \(\) & Adaptive & EDM-monotonic & ✓ & & & **1.45** & **64.6 \(\) 1.35** \\   

Table 2: ImageNet 64x64 results. See Section 5.2.

First, our adaptive noise schedule (Appendix F) works equally well as the hand-tuned fixed noise schedules. We verified this under \(\)-parametrization with \((-+2)\) weighting and EDM parametrization with EDM weighting. Under both settings, the adaptive noise schedule results in slightly better FID and Inception scores with the EDM sampler, and slightly worse scores with the DDPM sampler for the former setting. Although the adaptive noise schedule did not significantly affect the end result, it allows us to experiment with new weighting functions while avoiding hand-tuning of the noise schedule. In addition, it sometimes leads to faster training; see Appendix F.

Second, it is possible to modify an existing non-monotonic weighting function to a monotonic one with the minimal change, and results in on-par sample quality. Specifically, under EDM parameterization and adaptive noise schedule, we change EDM weighting to EDM-monotonic weighting with the very straightforward approach stated in Section 5.1. It results in nearly the same FID score and slightly better Inception score.

Third, our proposed new weighting functions, \((-+2)\) and EDM monotonic weightings, work better than the baseline settings across different model parameterization and samplers in most cases. Larger performance gain has been observed under \(\)-parameterization and \(\)-parameterization settings. Our-reimplementation of the EDM parameterization baseline setting could not exactly reproduce their reported FID number (1.36), but comes close (1.45). We observed less significant improvement under this setting, possibly because their already exhaustive search of the design space.

Finally, the above observations remain consistent across different model parameterization and samplers, indicating the generalizability of our proposed weighting functions and noise schedules.

### High resolution ImageNet

In our final experiments, we tested whether the weighting functions that resulted in the best scores on ImageNet 64\(\)64, namely \((-+2)\) and 'EDM-monotonic', also results in competitive scores on high-resolution generation. As baseline we use the _Simple Diffusion_ model from Hoogeboom et al. (2023), which reported the best FID scores to date on high-resolution ImageNet without sampling modifications (e.g. guidance).

We recruited the large U-ViT model from Simple Diffusion (Hoogeboom et al., 2023), and changed the training noise schedule and weighting function to our proposed ones. Note that for higher-resolution models, Hoogeboom et al. (2023) proposed a shifted version of the cosine noise schedule (Table 4), that leads to a shifted version of the weighting function \(w()\). Similarly, we extended our proposed sigmoidal and 'EDM-monotonic' weightings to their shifted versions (see Appendix D.2.1 for details). For fair comparison, we adopted the same vanilla DDPM sampler as Simple Diffusion, without other advanced sampling techniques such as second-order sampling or rejection sampling. As shown in Table 3, with our adaptive noise schedule for training, the two weighting functions we proposed led to either better or comparable FID and IS scores on ImageNet 128\(\)128, compared to the baseline Simple Diffusion approach.

Next, we test our approach on ImageNet generation of multiple high resolutions (i.e., resolutions 128, 256 and 512), and compare with existing methods in the literature. See Table 3 for the summary of quantitative evaluations and Figure 1 for some generated samples by our approach. With the shifted version of 'EDM-monotonic' weighting, we achieved state-of-the-art FID and IS scores on all three resolutions of ImageNet generation among all approaches without guidance. With classifier-free guidance (CFG) (Ho and Salimans, 2022), our method outperforms all diffusion-based approaches on resolutions 128 and 512. On resolution 256, our method only falls a bit behind Gao et al. (2023) and Hang et al. (2023), both of which were build upon the latent space of a pretrained auto-encoder from _latent diffusion models_(Rombach et al., 2022) that was trained on much larger image datasets than ImageNet, while our model was trained on the ImageNet dataset only. It is worth noting that we achieve significant improvements compared to Simple Diffusion which serves as the backbone of our method, on all resolutions, with and without guidance. It is possible to apply our proposed weighting functions and adaptive noise schedules to other diffusion-based approaches such as Gao et al. (2023) to further improve their performance, which we shall leave to the future work.

## 6 Conclusion and Discussion

In summary, we have shown that the weighted diffusion loss, which generalizes diffusion objectives in the literature, has an interpretation as a weighted integral of ELBO objectives, with one ELBO per noise level. If the weighting function is monotonic, then we show that the objective has an interpretation as the ELBO objective with data augmentation, where the augmentation is additive Gaussian noise, with a distribution of noise levels.

Our results open up exciting new directions for future work. The newfound equivalence between monotonically weighted diffusion objectives and the ELBO with data augmentation allows for a direct apples-to-apples comparison of diffusion models with other likelihood-based models. For example, it allows one to optimize other likelihood-based models, such as autoregressive models, towards the same objective as monotonically weighted diffusion models. This would shine light on whether diffusion models are better or worse than other model types, as measured in terms of their held-out objectives as opposed to FID scores. We leave such interesting experiments to future work.

    &  &  \\  & **FID \(\)** &  \\
**Method** & **train** & **eval** & **IS \(\)** & **train** & **eval** & **IS \(\)** \\ 
**128 \(\) 128 resolution** & & & & & \\ ADM [Dhariwal and Nichol, 2022] & 5.91 & & & 2.97 & \\ CDM [Ho et al., 2022] & 3.52 & 3.76 & 128.8 \(\) 2.5 & & \\ RIN [Jabri et al., 2022] & 2.75 & & 144.1 & & \\ Simple Diffusion (U-Net) [Hoogeboom et al., 2023] & 2.26 & 2.88 & 137.3 \(\) 2.0 & & \\ Simple Diffusion (U-ViT, L) [Hoogeboom et al., 2023] & 1.91 & 3.23 & 171.9 \(\) 2.5 & 2.05 & 3.57 & 189.9 \(\) 3.5 \\
**VDM++ (Ours)**, \(w()=(-+2)\) & 1.91 & 3.41 & **183.1 \(\) 2.2 & & \\
**VDM++ (Ours)**, EDM-monotonic weighting & **1.75** & **2.88** & 171.1 \(\) 2.7 & **1.78** & **3.16** & **190.5 \(\) 2.3** \\ 
**256 \(\) 256 resolution** & & & & & \\ BigGAN-deep (no truncation) [Brock et al., 2018] & 6.9 & 171.4 \(\) 2.0 & & & \\ MaxSMT [Chang et al., 2022] & 6.18 & 182.1 & & & \\ ADM [Dhariwal and Nichol, 2022] & 10.94 & & 3.94 & 215.9 & \\ CDM [Ho et al., 2022] & 4.88 & 4.63 & 158.7 \(\) 2.3 & & & \\ RIN [Jabri et al., 2022] & 3.42 & & 182.0 & & \\ Simple Diffusion (U-Net) [Hoogeboom et al., 2023] & 3.76 & 3.71 & 171.6 \(\) 3.1 & & & \\ Simple Diffusion (U-ViT, L) [Hoogeboom et al., 2023] & 2.77 & 3.75 & 211.8 \(\) 2.9 & 2.44 & 4.08 & 256.3 \(\) 5.0 \\
**VDM++ (Ours)**, EDM-monotonic weighting & **2.40** & **3.36** & **225.3 \(\)** 3.2 & **2.12** & **3.69** & **267.7 \(\) 4.9** \\    _Latent diffusion with pretrained VAE:_

DT-XL/2 [Peebles and Xie, 2022] & 9.62 & 121.5 & 2.27 & 278.2 \\ U-ViT [Bao et al., 2023] & & & 3.40 & & \\ Min-SNR-? [Hang et al., 2023] & & & 2.06 & & \\ MDT [Gao et al., 2023] & 6.23 & 143.0 & 1.79 & 283.0 \\ 
**512 \(\) 512 resolution** & & & & & \\ MaskGIT [Chang et al., 2022] & 7.32 & 156.0 & & & \\ ADM [Dhariwal and Nichol, 2022] & 23.24 & & 3.85 & 221.7 & \\ RIN [Jabri et al., 2022] & 4.30 & 4.28 & 171.0 \(\) 3.0 & 3.95 & 216.0 \\ Simple Diffusion (U-ViT, L) [Hoogeboom et al., 2023] & 3.54 & 4.53 & 205.3 \(\) 2.7 & 3.02 & 4.60 & 248.7 \(\) 3.4 \\
**VDM++ (Ours)**, EDM-monotonic weighting & **2.99** & **4.09** & **232.2 \(\)** 4.2 & **2.65** & **4.43** & **278.1 \(\) 5.5** \\    _Latent diffusion with pretrained VAE:_

DM-XL/2 [Peebles and Xie, 2022] & 12.03 & 105.3 & 3.04 & 240.8 \\ LDM-4 [Rombach et al., 2022a] & 10.56 & 103.5 \(\) 1.2 & 3.60 & 247.7 \(\) 5.6 \\   

Table 3: Comparison to approaches in the literature for high-resolution ImageNet generation. ‘With guidance’ indicates that the method was combined with classifier-free guidance [Ho and Salimans, 2022]. \({}^{}\) Models under ‘Latent diffusion with pretrained VAE’ use the pre-trained VAE from Stable Diffusion [Rombach et al., 2022a], which used a much larger training corpus than the other models in this table.