# Unsupervised learning of categorical structure

Matteo Alleman

Department of Neurobiology and Behavior

Columbia University

New York City, NY 10027

ma3811@columbia.edu

&Stefano Fusi

Department of Neurobiology and Behavior

Columbia University

New York City, NY 10027

###### Abstract

Humans occasionally reason using logic and abstract categories, and yet most state of the art neural models use continuous distributed representations. These representations are impressive in their learning capabilities, but have proven difficult to interpret, or to compare to biological representations. But continuous representations can sometimes be interpreted symbolically, and a distributed code can seem to be constructed by composing abstract categories. We ask whether it is possible to detect and get back this structure, and we answer that it sort of is. The demixing problem is equivalent to factorizing the data into a continuous and a binary part: \(=^{T}\). After establishing some general facts and intuitions, we present two algorithms which work on low-rank or full-rank data, assess their reliability on extensive simulated data, and use them to interpret neural word embeddings where we expect some compositional structure. We hope this problem is interesting and that our simple algorithms provide a promising direction for solving it.

## 1 Introduction

In biological and artificial learning systems, compositional structure is important to flexible behavior, yet difficult to detect at the representational level. Neural representations are rarely factorized into purely-selective concept neurons; when there is a neat conceptual structure it is most often embedded into high-dimensional neural modes (Kaufman et al., 2022; Higgins et al., 2021; She et al., 2021; Bernardi et al., 2020). Machine learning systems which either explicitly incorporate abstraction, or have just found to exhibit it, also use distributed continuous representations which are rarely factorized (Altabaa et al., 2024; Rigotti et al., 2022; Mikolov et al., 2013).

This poses a challenge for both mechanistic interpretability and structural alignment between representations. While there has been impressive success recently using sparse autoencoders for interpretability (Huben et al., 2024) the fact that items vary in degree of feature membership makes it hard to treat the discovered features as logical or fully compositional concepts. It could also be nice to have the tidiness of a discrete structure for the purpose of comparing representations. Unstructured representational alignment is somewhat ill-defined (Ahlert et al., 2024), and so another method for finding the 'content' of a representation could be a useful addition to the toolbox.

We will show that the discovery of latent categories can be formulated as a binary matrix factorization. Given a representation matrix, \(\), we will try to find a binary representation, \(\), which encodes an assignment of items to logical variables. We think of these logical variables as abstract 'concepts', which are optimized to match the geometry of the data. Many structures-including disentanglement, clustering, hierarchy, linear ordering, and hybrids of these-can be captured by this simple model. But finding an optimal factorization is a difficult combinatorial problem, and has not been extensively studied in the general case.

### Existing algorithms

Like many matrix factorizations, our method has an aesthetic similarity to the eigenvalue decomposition and PCA. In fact, when the concepts are uncorrelated with each other, i.e. they are orthogonal after subtracting the means, then it is actually equivalent to PCA. However-the rarity of this situation aside-PCA is not likely to find binary features when there is some approximate rotation-invariance to the solution, like with the square. Furthermore, when there are multiple factorizations, the sparsest one will have correlated features in general and thus not be recoverable.

We can also view our problem as a case of non-disjoint similarity-based clustering. Dasgupta (2016) defined an objective similar to ours for hierarchical clustering, and many of the theoretical results therein could be of interest. Work has also been done on generalizing \(k\)-means clustering to allow for overlap (Cleuziou, 2007; Whang et al., 2015) which is conceptually similar but uses a different cost function with a different implied generative model. The mixed-membership stochastic blockmodel (Airoldi et al., 2008) is another famous clustering model tackling a similar problem, and the connection to ours is made explicit by Sorensen et al. (2022).

In the community detection and applied math literature, our factorization problem has been studied as (semi) binary matrix factorization (SBMF). Remarkably, in special cases an algebraic solution is available via tensor decomposition (Sorensen et al., 2021), but it is highly sensitive to violations of its assumptions. There are several optimization-based approaches (Zhang et al., 2007; Kolomvakis and Gillis, 2023; Sorensen et al., 2022) which are generally built around the assumption of very low-rank data, and thus may not be applicable in the general case.

The specific formulation of our model closely follows the Binary Component Decomposition of Kueng and Tropp (2021). The algorithm they present (and the noise-robust algorithm of Kolomvakis and Gillis (2023)) requires that the category vectors satisfy a type of general position property called 'Schur independence' (Laurent and Poljak, 1996). If there are sufficiently few categories (fewer than approximately \(\) for \(p\) items) then this is not a restrictive requirement - yet as we will see it excludes many interesting categorical structures like hierarchies. One of our main contributions on top of that work is to show that mild regularization sometimes allows for the recovery of meaningful structure in this problem despite lack of identifiability.

## 2 Problem formulation

### Model

In the kind of conceptualisation we seek it should be possible to get from one thing to another by switching certain concepts. For example, in Fig. 1a we can get from the white square to the shaded square by adding the \(t\) vector; or we can predict what we will see when we add both the \(s\) and the \(t\) vector. This kind of conceptual navigation is what we will try to discover from the data.

In matrix terms, we are modelling the data as the product of a continuous and a \(\{0,1\}\)-valued matrix:

\[^{T}\]

in which the columns of \(^{n p}\) are the representations of data points, the columns of \(^{n b}\) are the representations of pure categories (henceforth 'weight vectors'), and the columns of \(\{0,1\}^{p b}\) indicate which points belongs to a given category (henceforth 'concept vectors'). We

Figure 1: Illustration of a binary decomposition of a kernel matrix. (**a**) A representation with compositional structure in which items are made by adding shared component vectors. (**b**) The resulting (linear) kernel matrix, which we model as a conic combination of rank-one binary matrices.

will denote individual concept vectors by \(\). Note that we will work with an affine model, which can be achieved by an explicit intercept term or by including a trivial concept of all 1.

Constrained matrix factorization is a challenging problem, especially when some factors are discrete. To make things somewhat easier, we will assume orthogonal weights, \(\). This helps us because our objective function (see next section) depends only on the Gram matrices, so it is possible to fit the concepts independently of the weights. Notice that the Gram matrix of the model prediction, \(}\), depends only on the category vectors and the squared norms of the components:

\[}^{T}}=^{T} ^{T}=^{T}\]

where \(\) is a (positive) diagonal matrix containing the squared norms of the weights. This is implicitly the approach taken by Kueng and Tropp (2019) and Kolomvakis and Gillis (2023) for semi-BMF. From this perspective, we are modeling the dot product between each pair of observations by the weighted sum of the number of shared concepts:

\[_{i}^{T}_{j}_{=1}^{b}_{}_ {i,}_{j,}\] (1)

which is represented visually in Fig.1c.

#### 2.1.1 Existence and uniqueness

Without an orthogonality constraint, clearly all data, \(\), can be decomposed with an exact SBMF; just set \(=\) and \(=\). With orthogonal \(\), not all \(\) can be factorized exactly, and it is NP-hard to check for a particular \(\)(Deza and Laurent, 1997). Nevertheless, most data is fairly 'close' to an exactly-embeddable representation (Laurent and Poljak, 1996), and we give some tiny examples in Figure 2. Among our examples, the square, tetrahedron, and tree are exactly embeddable, while the grid and hexagon are best approximations (as defined in the following section).

Exact or approximate, the optima are rarely unique1. A necessary and sufficient condition for uniqueness would be NP-hard to check (Deza and Laurent, 1997), but several sufficient conditions have been derived of varying restrictiveness and complexity (Kueng and Tropp, 2021; Sorensen et al., 2022). A crude intuition: the higher-dimensional the data, the more possible solutions. Two extreme examples are the \(b\)-cube and the \(p\)-simplex. A hypercube has \(p\) points in \(b\) dimensions, and its binary representation is unique; meanwhile, a \(p\)-simplex (every point equidistant) has \(p-1\) dimensions, and a tremendous number of possible representations, including the identity matrix and all Hadamard matrices. Because of this, we argue for a sparsity inductive bias in Section 2.2.

#### 2.1.2 Graphical representation

High-dimensional binary vectors might be hard to interpret, and we would like a visualization tool. Hierarchical clustering is much more useful when visualized with a dendrogram, a tree on which observations are leaves, and cluster assignments can be recovered by cutting at a certain depth of the tree. Just as our problem generalizes hierarchical clustering, we can generalize the dendrogram.

Given a set of \(b\) concepts, \(\), we can isometrically embed each point (row of \(\)) on a 'partial cube', i.e. an isometric subgraph of the \(b\)-bit hypercube. In this representation (e.g. Fig. 2, bottom row), nodes are connected by an edge when they differ by one concept. Some hidden nodes might be necessary to form the graph. Concepts can be read out by cutting across 'parallel' edges (those corresponding to the same concept), and for visualization it is often easier to color the edges and make them directed by fixing one arbitrary point as the 'origin' (as we have done in Fig. 2). By analogy to a dendrogram, which is a type of partial cube, we will call this kind of graph an 'analogram'.

Every partial cube has an easily-obtained unique binary representation (Deza and Laurent (1997), and see the Appendix A.1 for explicit construction), but there are a combinatorial number of graphs whose binary representation is \(\). The analogram should be the smallest such graph. Unfortunately, like almost everything related to this problem, finding such an analogram is NP-complete and it needn't even be unique (Knauer and Nisse, 2019). Nevertheless, we made a heuristic algorithm that works well on moderately-sized \(\), and always on trees (see A.1). For larger and non-tree structures, even a minimal graph will be too messy to view, so other visualization tools would be required.

### Objective

Since an exact fit is not always possible (or even desirable), we must define goodness of approximation. The centered kernel alignment (CKA) is commonly used to measure representational similarity in machine learning and neuroscience. It has an interesting interpretation as a non-parametric measure of dependence (Gretton et al., 2005; Sejdinovic et al., 2013) but for our purposes is just an empirically useful objective. Unlike the mean squared error between the kernels, it is translation and scale invariant, which is desirable in a measure of geometric similarity. Compared to the MSE between \(^{T}\) and \(\), the CKA formulation also has fewer free parameters since we avoid fitting \(\) (due to assumed orthogonality).

The CKA is the cosine similarity of the flattened, double-centered Gram matrices. Specifically, if we have \(p p\) Gram matrices \(=^{T}\), and \(=^{T}\), then double-centering means computing the dot products after subtracting the mean from each dimension. If we can define the centering matrix \(=_{p}-_{p}\), then feature centering is expressed by \(}=^{T}\). For short, we will denote the Frobenius inner product applied to double-centered matrices by \(,_{ F}=,_{}\).

So, the problem we want to solve is

\[*{arg\,max}_{=^{T}} ,_{}}{,_{},_{ }}}\] (2)

In this form the CKA is not nice to optimize, but since our constraining set is a cone\({}^{2}\) it is equivalent to the Frobenius distance between the double-centered matrices. In our case this reduces to:

\[*{arg\,min}_{\{0,1\}^{p b}, _{+}^{b}}^{T}(}^{T} {})^{2}-2\,^{T}(^{T} })^{2}\] (3)

where \({}^{2}\) is element-wise squaring and \(}=\). If we fix \(\), this is just a non-negative least squares problem in \(\) and we can expect at most \(\) non-zero elements. For sufficiently small \(p\) this can be used to solve the problem by enumerating over all possible \(\) vectors and keeping those with non-zero \(\) value. In general, though, we use it to refine a solution which might have more concepts than strictly necessary, or to derive an iterative optimizer.

Figure 2: Examples of some categorical structures recoverable from geometry. The concepts are found by optimizing Equation 3 with brute force (i.e. setting \(=\{0,1\}^{p}\)) and node regularization. The graphs (see Section 2.1.2) are drawn by manual inspection. Notice that the concept labels (in curly brackets) can be gotten by cutting the graph at the corresponding edges, and labelling the partition on the arrow side. We chose the source (no concept) nodes arbitrarily.

#### 2.2.1 Regularization

There could be an astronomical number of optima for a given geometry, so we need an inductive bias to break the tie. Parsimony is desirable, but there are several ways to define it. Using the minimal number of concepts might seem like a reasonable choice - we will argue that this is not the right measure of complexity, and instead we should try to minimize the size of the analogram (Section 2.1.2). Since that is very hard to do directly, we propose regularizing for sparsity.

Consider the two examples in Figure 3. There are two possible embeddings for the 4-simplex (panel a), one with 4 concepts but only 1 hidden node, and one with 3 concepts but three hidden nodes. A similar situation is true for the two solutions of a stretched simplex (panel b), and in this case the structure with more concepts is potentially more in line with our intuition. But as was mentioned, we cannot optimize for this directly. Instead, we notice that the simpler graphs tend to have less intersection between the concepts, as well as sparser concepts. For that reason, we will try to encourage sparsity as a proxy for graph complexity.

To encourage sparsity, we can just add a linear term to the loss: if \(_{}=\{^{T}_{},p-^{T} _{}\}\), we add the term \(^{T}\) to Equation 3 to be minimized. We have considered other proxy which better correlate with graph complexity, but do not include them here.

## 3 Optimization

Being a challenging combinatorial problem, we cannot expect efficient solutions that work in every situation. There are already remarkably effective approaches for very low-rank data, but they do not always fail gracefully when their assumptions are violated. We will therefore adapt some of these existing techniques to the non-identifiable case, by iteratively sampling category vectors. In addition, we develop a complementary online algorithm which builds all category vectors simultaneously, but one item at a time. Note that both algorithms can in principle work on any positive semidefinite matrix, and are thus quite general. Each approach has its strengths, and also ample room for improvement.

### Rejection sampling for low-rank data

When the dimensionality of the data is sufficiently small, Kueng and Tropp (2021) and Kolomvakis and Gillis (2023) provide efficient algorithms based on randomly sampling columns of \(\). Their methods are based on the following observation: If the data admits a factorization of the form \(=^{T}\), and \(^{n b}\) has full column rank, then, since the linear mapping can be inverted, each column of \(\) is also in the rowspace3 of \(\). So, these algorithms randomly search for vectors \(\{-1,1\}^{p}\) that are in the rowspace of \(\). When the rank, \(r\), is very small, it is possible to use exhaustive search since there are at most \(2^{r}\) such binary vectors (Slawski et al., 2013).

Figure 3: Examples of degenerate solutions.

The aforementioned algorithms are able to search efficiently by assuming that the \(\) vectors are Schur-independent. In that case, because of the special structure of the set of correlation matrices (Laurent and Poljak, 1996; Kueng and Tropp, 2021), each \(\) can be found via semidefinite programming (SDP). But in the general case the SDP is not guaranteed to have a rank-one optimum, and would require a heuristic rounding step. In such a situation it might not be worth the computational burden of solving an SDP, and so we propose a first-order method based on Hopfield networks.

In the presence of noise, the true concept vectors might not be exactly in the rowspace of \(\), but just be the closest among other binary vectors. If \(\) are the right singular vectors (with non-zero singular values) of \(\), then we expect the true concept vectors, \(\), to be local maxima of \(E()=^{T}^{T}\). This is precisely (the negative of) the energy function of a Hopfield network, which we can maximize by iteratively updating \(_{t}(^{T}_{t-1})\) from some initial guess.

There is an interesting connectionist interpretation of this procedure. Let us imagine the concept \(\) is the binary response pattern of neuron to the whitened inputs - then the Hopfield updates amount to Hebbian plasticity. If the weights of the neuron are \(\), then the update above tells us that it should be set to \(_{i}=_{j}_{i,j}_{j}\), which is a Hebbian rule. Taking this interpretation, we call this algorithm the binary autoencoder4 (BAE). It works by randomly sampling \(\) vectors according to Algorithm 1, with a tolerance parameter \(\) and inverse temperature \(\) on an exponential annealing schedule.

We find that this simple algorithm performs very well on simulated low-rank data. For \(p\) points, we draw \(\) random \(\) vectors, which almost certainly admit a unique decomposition, and randomly embed them in \(d\)-dimensions. We then add iid Gaussian noise to achieve a specified signal-noise ratio (SNR). Since the algorithm scales with the rank of \(\), which is \(\), we observe a nearly linear scaling with \(p\) (Fig. 4a). Furthermore, we recover the ground-truth factors for most SNR values up to the largest we tried: \(p=2^{12}\). For SNR of 1 (log SNR of 0) we can recover up until around 100 points, at which point the errors sharply increase (Fig. 4b). It is likely that there are similar inflection points for higher SNRs at larger values of \(p\), but we did not explore that far.

```
1:functionSample(\(^{p n}\), \(>0\))
2:\((_{n},_{n})\)
3:\(=\)
4:while not converged do
5:\(_{t}([(+ )])\)
6:\(^{T}_{t}\)
7:\(^{T}_{t}\)
8:if\(\|^{T}_{t}\|_{2}^{2} 1-\)then
9:Return\(_{t}\)
10:else
11:Return\((,\,)\) ```

**Algorithm 1** Rejection sampler for BAE

### Iterative refinement for full-rank data

When \(\) is full rank, we cannot use Algorithm 1. It is possible to run it the top few principle components of \(\), and this often will recover some high-level concepts, but it will not recover hierarchical structures which are genuinely full rank. In such a situation, every concept is in \((^{T})\), so we must optimize our objective function (3) over \(\{0,1\}^{p}_{+}^{p}\). Our approach will be to update the item-wise category assignments to match similarities to other items.

Let us assume we have a solution for the first \(n\) items. When we receive a new item, our data kernel, \(\), and model kernel \(\) will each get a new column (and row) and a new diagonal element:

\[^{(n+1)}=^{(n)}&\\ ^{T}&k_{0}\;,\;^{(n+1)}= ^{(n)}&\\ ^{T}&q_{0}\]Our algorithm will modify \(\) and \(\) in order maximize the objective, only changing \(\) and \(q_{0}\). To motivate our approach, we will use the neural network interpretation of the BAE. Imagine a subpopulation of neurons which all respond in the same way to the first \(n\) items. represented by column \(\) of \(\) (with associated \(_{}\)). In response to item \(n+1\), only a certain fraction, \(p_{}\), might respond. At the same time, another sub-population which has not responded to any previous item might also chime in. If we define the response probabilities as \(=(p_{1},...,p_{b})\), and the strength of the newly-active population as \(_{0}\), then the effect on the total kernel is:

\[=\,,\;\;q_{0}=^ {T}+_{0}\]

Finding the optimal \(\) and \(_{0}\) is a quadratic problem. We have to account for centering, which is shown in the Appendix A.2, but the end result is a \((d+1)\)-dimensional interval-constrained QP:

\[*{arg\,min}_{},_{0}} 2\|}(}-})-\|_{2}^{2}+((-2 )^{T}}+_{0}+}^ {T}-k_{0})^{2}\] (4) \[ },\;0_{0}\]

where we have defined \(=^{T}\), \(}=\) and \(}=\). We can adapt our complexity regularization to work iteratively as well, while preserving convexity. Having done this, we split any columns with fractional \(\) and delete any categories with a \(\) value less than some small threshold. The general outline is provided in Algorithm 2.

The iterative assignment algorithm is greedy and sensitive to item order. In order to deal with resulting local minima, we add some randomization and can run multiple fits in parallel (the \(\) term in Algorithm 2). In principle, the intuition we used to motivate the algorithm can be made more concrete by implementing it with an explicit population, where the optimization of Equation 4 is implemented by recurrent weights. It would be interesting to explore such a model, but we find that it is faster and more stable to solve the problem directly by convex optimization (using the cvxpy package, Diamond and Boyd (2016)).

We test this algorithm on hierarchical concepts, as well as a hybrid low-rank and hierarchical structure. To generate this data, we recursively partition the points by randomly choosing a number of partitions between 2 and 4, defining a concept for each partition, and then repeating within each

Figure 4: Numerical experiments. All were carried out on 2.9 Ghz, 16-core CPUs without parallelization, and plots are the average of 25 random seeds. (**a**) The time taken by Algorithm 1. (**b**) Average Hamming distance from ground-truth concept to nearest discovered concept. Values are normalize by \(p\) to be able to see the slight non-zero values in some lines. (**c**) Time taken by Algorithm 2. (**d**, **f**) Raw (i.e. unnormalized) Hamming distances from ground truth to nearest discovered concept, for random tree-structured and hybrid data. (**e**, **g**) CKA of the model fit to \(\).

partition. The same noise model was used as before. The hybrid structure is just the concatenation of randomly-sampled concepts and the hierarchical ones. Because the number of concepts is much higher, and we are solving a \(p b\) NNLS on each iteration, the scaling is much poorer than the BAE (Fig. 4 c). We note that, while the complexity appears super-polynomial, this is because of a change in the slope around \(p>64\), which is visible when sampling with finer granularity.

Regularizing for sparsity allows us to recover the ground truth structure up to around 100 points, but since these structures do not have unique solutions, noise very quickly diminishes our ability to recover the ground-truth solution in both cases (Fig. 4d,f). Interestingly, the ground-truth recovery is correlated with our objective (Fig. 4e,g), which suggests that improving performance might also improve recovery of the ground truth.

Finally, we will see how this works on a simple example from word2vec (Mikolov et al., 2013), a well-known word embedding. The embedding is generated by predicting word-context pairs, in which contexts are the neighboring words in the Google News corpus. The embeddings can themselves be seen as a type of matrix factorization (Levy and Goldberg, 2014). What earned word2vec its notoriety is the fact that certain abstract concepts seem to be encoded along approximately parallel dimensions-exactly the kind of structure that we are looking for.

Since word2vec has embeddings for several million words, we need to select a subset to study. WordNet (Fellbaum, 1998) contains manually-encoded 'is-a' relations (i.e. 'dog' is-a 'canine') over a large vocabulary, and we used it to generate a list of words that are descendants of 'person'. This comes out to 3841 words. This is just a way to generate a reasonably-sized dataset which might be somewhat related, and we do not use the hypernymy after this. With around 4000 points in 300 dimensions, this is a good use case for Algorithm 1; we achieve a CKA of around 0.93 (Fig. 5a) using 993 concepts (Fig.5b)

It is hard to make sense of high-dimensional clusters, and the analogram of this embedding will be too dense to be useful, so we will use a small example to orient ourselves. In particular, we will look at the famous 'king:queen::man:woman' quadruplet. To do so, we will take the binary embedding of the four words, call it \(_{}\), and use the unique columns. The analogram of this subset is shown

Figure 5: (**a**) Cosine similarity of 3841 words, in the word2vec representation (left) and the binary reconstruction (right). Words were selected using WordNet by taking all hyponyms of ‘person’ which were also present in word2vec. (**b**) Weight (\(\)) value of each concept. (**c**) PCA plot of four words of interest, with analogram overlaid and projection of all other selected words. (**d**) Example of sub-concept vectors, which are averages over concept vectors conditioned some subset of points. (**e**) The ‘conceptual projection’ of all fitted words onto the two high-level concepts. Highlighted words were selected by manual inspection.

in Fig. 5c. Like Mikolov et al. (2013), we see a concept which groups together 'king' and 'queen' (concept 1), and one which groups together 'king' and'man' (concept 2), but we also have word-specific concepts to account for the deviation from a perfect rectangle. In this accounting, concept 1 is more strongly represented than concept 2, as is seen in the correlation matrix.

While these clusters are often interpreted as 'class' and 'gender', it is not clear from just 4 words whether this is the right interpretation. When we take a subset, we are combining multiple dataset-level concepts into a single quadruplet-level concept; that is, there are many dataset-level concepts which group together 'king' and 'queen' vs.'man' and 'woman'. For example,'man' and 'woman' are very generic terms, and something like'specificity' is a concept which could plausibly affect the contexts in which they appear in the news.

When projecting the data onto the top 2 principle components of our four points, it is not clear which concepts are being captured. As we can see in Fig.5c, the quadruplet is not fully captured by just the two balanced features. Instead, we will try to project words in a way that is specific to the concepts we are trying to interrogate. For each word, we can take the average over all concepts conditional on a particular value of the four items of interest, producing a \(\) score for each other item (Fig.5d).

In Fig. 5e we are plotting this average value (which is between 0 and 1) for every word in grey, and some specific words are highlighted. In general, the right side of concept 1 seems to be courtly roles rather than rulers _per se_, while the left side are perhaps called'mundane'. Concept 2 does seem to capture some gendered words ('duke' vs 'duchess' and 'boy' vs 'girl'), and the expected parallelisms are visible in this projection. On the other hand, concept 2 seems less about class than expected - words with low projection are indeed fairly 'kingly', while many words with a high projection relate to crime, like 'victim','shooter', and 'eyewitness' (shown in the plot). We note that this qualitative picture persists for many random seeds and hyperparameter choices (annealing schedule, regularization, etc.).

There is clearly much to be desired in the discovered embedding. At least part of that can be addressed by improving the model fit, and we are interested in combining our two algorithms to better fit the more local structure being missed by the low-rank algorithm. On the other hand, some of the counter-intuitive or (not shown) unsavory placements of words in this schema are visible in some form in the raw representations. In that sense, the added interpretability of our method's unambiguous category assignments (along with good estimation of the weight vectors \(\)) could make it useful for removing unwanted concepts present in the dataset.

## 4 Discussion

Here we studied the problem of turning a continuous representation into a logical one. We provided two simple algorithms with complementary use cases and demonstrate their efficacy. When dealing with very low-rank data, we leveraged results from previous work to develop a very fast and fairly robust method based on Hopfield networks. For when the data is higher-dimensional, we developed an online algorithm which iteratively solves convex subproblems. Both models can in principle be implemented by a population of neurons with Hebbian input and recurrent synapses, and though we treat this as a curiosity for now, it raises the question of biological relevance.

To the extent that we, humans, are engaged in concept discovery, our work here could also provide a minimalistic model of that cognitive process. We are not modelling the difficult process of discovering concepts from the real world: our model is linear, and therefore assumes a representation which approximately encodes the relevant concepts. Instead, we can model the process of turning a distributed, noisy code into abstract symbols and structures (Kemp and Tenenbaum, 2008). Future work can address how the inherent challenges of unsupervised abstraction, which we laid out, might be dealt with by more sophisticated algorithms with different inductive biases.