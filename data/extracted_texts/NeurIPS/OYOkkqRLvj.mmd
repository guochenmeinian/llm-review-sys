# Amortized Eigendecomposition for Neural Networks

Tianbo Li\({}^{1,*}\), Zekun Shi\({}^{1,2}\), Jiaxi Zhao\({}^{3}\), Min Lin\({}^{1}\)

\({}^{1}\)SEA AI Lab

\({}^{2}\) School of Computing, National University of Singapore

\({}^{3}\) Department of Mathematics, National University of Singapore

\({}^{*}\)litb@sea.com

###### Abstract

Performing eigendecomposition during neural network training is essential for tasks such as dimensionality reduction, network compression, image denoising, and graph learning. However, eigendecomposition is computationally expensive as it is orders of magnitude slower than other neural network operations. To address this challenge, we propose a novel approach called "amortized eigendecomposition" that relaxes the exact eigendecomposition by introducing an additional loss term called eigen loss. Our approach offers significant speed improvements by replacing the computationally expensive eigendecomposition with a more affordable QR decomposition at each iteration. Theoretical analysis guarantees that the desired eigempair is attained as optima of the eigen loss. Empirical studies on nuclear norm regularization, latent-space principal component analysis, and graphs adversarial learning demonstrate significant improvements in training efficiency while producing nearly identical outcomes to conventional approaches. This novel methodology promises to integrate eigendecomposition efficiently into neural network training, overcoming existing computational challenges and unlocking new potential for advanced deep learning applications.

## 1 Introduction

Eigendecomposition is a fundamental technique in linear algebra that finds applications across numerous scientific domains ranging from quantum many-body problems to multivariate statistical analysis. In the context of deep learning, eigendecomposition also plays a crucial role in tasks such as weights normalization [8; 16; 41], dimensionality reduction [6; 44; 27; 38], network compression [17; 30], image denoising [13; 12; 14], graph adversarial learning [10; 18; 47]. By uncovering the structure of networks, eigendecomposition allows us to enforce low-rankness, ensuring generalization, robustness, and computational efficiency. Eigendecomposition is also instrumental in the spectral analysis of graphs, where it can detect community structure, which is essential in spectral graph neural networks. The ability of eigendecomposition to detect the intrinsic matrix structures and properties makes it a valuable tool in various machine learning tasks with neural networks.

Figure 1: A comparison that illustrates the forward execution time of three linear algebra operations: qr, eigh, and svd, when performed on a \(10000 10000\) matrix using PyTorch and JAX. The presented values represent the mean ratios of the execution time relative to that of matrix multiplication (matmul) of 100 runs.

Despite its straightforward definition, the computation of eigenvalues is quite challenging. Eigendecomposition algorithms are inherently iterative, involving a sequence of expensive operations such as Arnoldi iteration, QR iteration, and Rayleigh quotient Iteration . Additionally, it usually takes thousands of iterations to reach a desirable tolerance level. For instance, the locally optimal block preconditioned conjugate gradient (LOBPCG) method -a widely-used eigenvalue solver-often requires dozens to hundreds of iterations to achieve convergence [22; 9]. Figure 1 presents a comparative analysis of the execution times for eigendecomposition and other computational operations using PyTorch  and JAX . The figure shows that the execution speeds for eigh and svd are remarkably slower--by 4 to 5 orders of magnitude--relative to matrix multiplication. This substantial disparity in execution speed indicates operations such as eigh and svd, once used, will be the bottleneck of computation cost. Conversely, the QR decomposition, often employed for orthogonality, is considerably less computationally expensive. This observation has motivated us to explore the possibility of reducing the iterative computation of eigendecomposition with lower-cost operations.

When eigendecomposition is incorporated into the training of a neural network, a nested loop scenario arises, where eigendecomposition acts as the inner loop and the neural network's loss minimization serves as the outer loop. Notably, it is not always easy to be aware of this inner loop of eigendecomposition, as it is encapsulated by the high-level functions provided within deep learning frameworks. However, in this context, this inner loop does not require full convergence during each iteration, given that the remaining parameters have not reached optima. The inner loop can be relaxed and optimized jointly with the training loss, allowing for a more flexible and efficient training process. The key idea can be summarized as follows:

_Eigendecomposition within a neural network does not have to reach full convergence during each training step; it simply needs to contribute to the desired outcome by the end of the training process._

In this paper, we present a novel approach named "amortized eigendecomposition" for training neural networks that require eigenvalues or eigenvectors. Instead of using computationally expensive eigendecomposition decoupled with the training of neural networks, we proposed to relax it into an unconstrained optimization problem on the Stiefel manifold by adding an eigen loss. This relaxation only requires a QR decomposition at each iteration, thus is more efficient. Moreover, through empirical observations, we have found that although the relaxed optimization problem with eigen loss does not involve eigendecomposition in every iteration, the amortized optimization approach consistently achieves the desired results. It achieves nearly identical performance to traditional methods but with significantly improved speed.

## 2 Eigendecomposition in Neural Networks

In this paper, we consider a general class of neural networks that incorporate eigendecomposition. We formulate this family of problems as a constrained optimization problem:

\[_{}fh_{}(),,, ^{}=\] (1)

Here, the encoder \(h_{}\) maps the data \(^{n p}\) into a latent space. In addition to the latent representation, the loss function \(f\) also incorporates the eigenpair \(\) and \(\) of a symmetric matrix \(\). The matrix \(\) can be constructed from \(h_{}()\), such as a covariance matrix or a similarity matrix, or it can depend solely on the parameters. Notably, \(\) is subject to changes during network training due to its dependency on \(\). The computational graph for each iteration of such model structure can be written as,

\[h_{}()}_{} (,)}_{ }h_{}( ),,}_{}.\] (2)

The corresponding algorithm of the above computational graph is shown in Algorithm 1. Solving such problems, however, is computationally expensive, since it requires preserving the eigendecomposition constraint of \(\) after each update of \(\). This structure encompasses a wide range of learning problems. We present several representative examples and investigate them in our numerical experiments next.

Nuclear Norm RegularizationThe nuclear norm of a matrix is defined as the sum its the singular values. Due to its convexity, regularization via the nuclear norm is employed to encourage low-rankstructures within the learned parameters. This approach proves beneficial in a variety of applications, such as matrix completion [5; 4], image denoising [13; 12; 14; 45]. Furthermore, eigendecomposition and singular value decomposition are also used for pruning or compressing neural networks, by decomposing the weight matrices of the network and approximating the original network with fewer parameters [17; 30]. The objective function of this type of problem can be written as,

\[_{}fh_{}()+\|\|_{*}\] (3)

Here, \(\) is a regularization coefficient that controls the rank of the parameter matrix. In classical methods, the nuclear norm \(\|\|_{*}\) is usually calculated via singular value decomposition.

Whitening in Neural NetworksWhitening is a transformation technique extensively utilized in neural networks to standardize features by ensuring they have zero mean and unit variance, and are decorrelated from each other. In neural network applications, whitening can be categorized into parameter-space whitening and feature-space whitening. Parameter-space whitening, often achieved through PCA/ZCA, is a prevalent method applied during neural network training to improve stability and accelerate convergence [37; 41; 16; 8]. Feature-space whitening, in contrast, applies PCA to the intermediate representations within the network. This process aligns features with the axes of greatest variance, thereby facilitating dimensionality reduction [15; 42; 34; 25]. A concrete example of feature-space whitening is its incorporation into an auto-encoder architecture, which can be expressed mathematically as:

\[_{,}\|_{} ^{}_{}()-\|_ {}\] (4)

where \(\) is spanned by the first several largest eigenvectors of the covariance of \(_{}()\) and \(\), \(\) represent the parameters of the decoder and encoder, respectively.

Graph Structure LearningGraph structure learning is an area of machine learning that aims to deduce the latent structure of a graph or network from observed data . This domain has significant applications in graph adversarial learning, which seeks to bolster the robustness of graph neural networks against adversarial attacks [10; 18; 47]. In such contexts, the adjacency matrix is usually often compromised by adversarial modifications while feature matrix \(\) remains unaffected. The primary objective is to learn both a clean graph structure and perform accurate node classification. From this perspective, we follow the approach proposed in  and the objective function can be formulated as:

\[_{,}\|_{}(,})-\|_{2}+}_{*}+ }-_{}^{2}\] (5)

Here \(}\) represents a low-rank Laplacian matrix that approximates the original graph Laplacian \(\). The three parts of the loss function correspond to the node classification loss, low-rank constraint, and unnoticeable adversarial attacks, respectively. However, this approach relies on singular value decomposition at every iteration, which is computationally prohibitive for large-scale networks.

## 3 Differentiable Optimization for Eigendecomposition

Before introducing the proposed method, let's begin by examining a more straightforward case: determining the eigenvalues of a symmetric matrix \(^{n n}\) through constrained optimization.1 The eigenvalues of \(\) are denoted as \(_{1}_{2}_{n}\). The largest eigenvalue of \(\) can be obtained by directly maximizing the _Rayleigh quotient_:

\[_{1}=_{^{n}}\ \ ^{}}{^{}}.\] (6)

where the maximum value corresponds to the largest eigenvalue and the associated eigenvector is given by the normalized \(\) at the optimum. This method can be generalized to identifying _the optimal subspace_, which is a common problem in dimensionality reduction and feature selection contexts. The optimal subspace for a symmetric matrix is defined as the subspace spanned by its \(k\) largest eigenvectors. This can be found by solving the constrained optimization problem:

\[_{^{n k}}\ \ \ (^{})\] (7)where the Stiefel manifold \(^{n k}=\{^{n k}^{} =\}\) is the set of \(n k\) matrices with orthonormal columns. Maximizing this trace function yields the optimal subspace spanned by the column vectors of \(\).

Rotational SymmetryIt's important to recognize, however, that the optimal \(\) of the above optimization problem does NOT represent the eigenvectors of \(\). This is due to the rotational symmetry of the trace function that for any orthonormal matrix \(^{k k}\), the equation \((^{})=(^ {}^{})\) always holds. As a result, there are an infinite number of solutions to the optimization problem Eq. (7), all spanning the same subspace as the desired sets of eigenvectors. A visual illustration is provided in Figure 1(a).

To accurately obtain the eigenvalues and eigenvectors, it is necessary to refine the traditional trace loss function. This paper introduces two approaches for achieving the correct eigendecomposition. The first method utilizes Brockett's cost function, which applies distinct weights to the diagonal elements of the matrix product \(^{}\), effectively differentiating the importance of each eigenvalue. The second method involves applying an element-wise convex function directly to the diagonal elements, resulting in an exact eigendecomposition. We will now elaborate on these methods.

The Brockett's Cost FunctionThe first modification made to the trace loss is Brockett's cost function . It enables the extraction of eigenvectors and eigenvalues by solving the following optimization problem:

\[_{^{n k}}(^{ }).\] (8)

Here \(=(m_{1},m_{2},,m_{k})^{k k}\) is a diagonal weight matrix with distinct diagonal elements. The matrix \(\) is structured such that all its elements are distinct numbers. Specifically, we can denote these elements as \(0<m_{1}<m_{2}<<m_{k}\). This ordering allows \(\) to assign distinct weights to the diagonal elements of the product \(^{}\), effectively disrupting the rotational invariance inherent in the trace loss and thus enabling the determination of eigenvalues and eigenvectors through optimization. In fact, \(\) can be any diagonal matrix with distinct diagonal

Figure 2: An illustration on disrupting the rotational symmetry of the trace loss. We aim to solve eigenvectors for a 2-dimensional symmetric matrix \(=0.8&0.2\\ 0.2&0.4\) with three loss functions. We parameterize an orthonormal matrix \(=x&y\\ -y&x\), which is subject to the constraint \(x^{2}+y^{2}=1\). The plot displays the contours of the landscapes of three different loss functions as they vary with \(x\) and \(y\): (**a**) trace loss \((^{})\), (**b**) Brockett’s cost function \((^{})\) where \(=(1,0.2)\) and (**c**) convex loss function \((f(^{}))\) where \(f\) is an element-wise exponential function. The feasible area of the constraint is depicted with a black circle. The red stars signify the optima of the loss in the feasible area. The dashed grey lines represent the true eigenvector direction of \(\). We see that, the trace loss results in infinitely many optimal solutions due to its rotational symmetry. In contrast, both Brockett’s cost function and the convex loss function reshape the optimization landscape, breaking this symmetry and leading to the identification of the correct eigenvectors.

elements. A practical choice could be \(m_{i}=i/k\) or \(m_{i}=i\) as suggested by . The next theorem illustrates why this approach can yield the exact eigendecomposition.

**Theorem 1** (Trace inequality with weight matrix).: _Consider a \(n\)-dimensional symmetric matrix \(\). Let \(\) be a \(k\)-dimensional diagonal matrix with elements \(0<m_{1}<m_{2}<m_{k}\). For an arbitrary matrix \(^{n k}\) with orthogonal columns, the following inequality always holds:_

\[_{i=1}^{k}m_{k-i+1}_{i}(^{} )_{i=1}^{k}m_{i}_{i}.\] (9)

_The equalities are achieved if and only if \(\) contains the eigenvectors corresponding to the \(k\) largest (smallest) eigenvalues of \(\)._

This approach can be viewed as an extension of the trace inequalities originally put forward by Von Neumann  and further developed by Ruhe . A detailed demonstration is available in the seminal work by Brockett . Additionally, we offer an alternative proof leveraging the Cauchy-Schwarz inequality, which is presented in the Appendix for reference. A more generalized version of this trace inequality is discussed in Liang et al. .

The Convex Trace LossThe second method employs a strictly monotonic convex function \(f\), applied element-wise to the diagonal components of \(^{}\). This perturbation also disrupts the rotational symmetry inherent in the trace loss. The convex nature of \(f\) alters the curvature of the loss landscape, thereby ensuring a unique optimal solution corresponding to the eigenvectors. The convex trace loss function, aimed at extracting the \(k\) largest eigenvalues, is expressed as:

\[_{^{n k}}f(^{})\] (10)

The optimal \(_{*}\) that achieves the maximum in the above objective are the eigenvectors corresponding to the \(k\) largest eigenvalues, and the eigenvalues can be obtained by the diagonal elements of \(_{*}^{}_{*}\). The next theorem provides a formal validation of this assertion.

**Theorem 2** (Trace inequality with convex function).: _Let \(\) be a given \(n\)-dimensional symmetric matrix and let \(\) be a matrix of size \(n k\) that resides on the Stiefel manifold. Suppose \(f:\) be a monotonically increasing, convex function applied element-wise. The following inequalities hold:_

\[k(f(())) f^{}_ {i=1}^{k}f(_{i}),\] (11)

_The rightmost inequality becomes an equality if and only if \(\) comprises the eigenvectors of \(\) that correspond to the \(k\) largest eigenvalues, The leftmost inequality is met with equality when \(\) is such that all diagonal elements of the matrix \(^{}\) are equal._

This theorem is established through the application of Jensen's inequality. The detailed proof is provided in the Appendix B.3 for reference. In the above objective, suitable choices of \(f\) include: \(f(x)=(x)\) on \(\); \(f(x)=x^{}\) on \(^{+}\) where \(>1\); and \(f(x)=(x)\) on \([0,/2]\). For finding the \(k\) smallest eigenvalues, a simple modification can be made by replacing the function \(f\) with an element-wise monotonically increasing concave function and then minimizing the trace loss. A visual representation of the optimization process of the three trace losses is presented in Figure 2.

## 4 The Amortized Eigendecomposition Approach

The proposed amortized eigendecomposition approach aims to modify the eigendecomposition operation within the neural network's computational graph, as illustrated in Eq. (2). This replacement involves two steps: First, the set of eigenvectors form a matrix on the Stiefel manifold reparameterized through computationally efficient operations such as QR decomposition. Then, the loss function is adjusted to ensure that its optimal solutions precisely correspond to the eigendecomposition. The computational graph for this amortized eigendecomposition method is formulated as follows:

\[h_{}()}_{} h_{}( ),,}_{}+,)}_{}^{}}_{}.\] (12)

In the computation graph, the eigendecomposition operation is circumvented and substituted with a more efficient QR operation. The QR operation is employed to reparameterize the orthogonal matrix on the Stiefel Manifold, leading to a substantial acceleration at each iteration. Moreover, instead of forcing the eigendecomposition constraint at each iteration, we relax it to an eigen loss which is jointly optimized with the training loss of the neural network as a nested optimization loop. This training process is outlined in Algorithm 2, where the key difference of our amortized optimization for the eigendecomposition approach is highlighted in red background color.

```
1:Dataset \(\), encoder \(h_{}\), task \(f_{}\);
2:Initialize model parameter \(\) and \(\);
3:while not converged do
4: compute \(\) from \(h_{}()\);
5:\(,=()\);
6: compute \(f_{}(h_{}();_{})\);
7: update \(\), \(\) by gradient descent;
8:endwhile ```

**Algorithm 1** The conventional eigendecomposition in a neural network outlined in Eq. (2)

Reparameterize the Stiefel ManifoldThere are three prevalent methods for reparameterizing an orthogonal matrix: through the matrix exponential, the Cayley transform, and QR decomposition. Due to the QR decomposition's better numerical stability and efficiency for non-square matrices \(\), we employ it for reparameterizing a matrix with orthonormal columns:

\[=().\] (13)

In this formulation, \(\) is dimensionally consistent with \(\). QR decomposition is more computationally efficient than eigendecomposition and singular value decomposition as shown in Fig. 1. Additionally, the backward computation of the QR decomposition is well-defined and has been efficiently optimized in modern deep learning frameworks, such as PyTorch and JAX. For details on the back-propagation process of the QR decomposition, see .

Relaxation with Eigen LossPreviously, we observed that optimizing the Brockett-type or convex trace loss directly enables us to obtain precise eigenvalues and eigenvectors. For any loss function that depends on the eigenvectors or eigenvalues, as specified in Eq.(1), we can transform this loss into a regularized version incorporating the eigen loss. This relaxation allows us to forego the need for explicit eigendecomposition at every iteration, while ultimately achieving equivalent outcomes. We now examine several general scenarios that encompass the majority of cases and explore how to implement this relaxation technique.

In the general case, the model loss in Eq. (1) depends on both the eigenvectors and eigenvalues. This constrained optimization problem can be relaxed by introducing an eigen loss as a regularizer, which is formulated as:

\[_{}fh_{}(),, []{(,)k}_{,}fh_{}(),,-(^{}_{}).\] (14)

In this reformulation, \(^{n k}\), is reparameterized via a QR operation as shown in Eq. (13). This relaxation circumvents the need for eigendecomposition at each iteration by using the computationally cheaper QR decomposition, while still ensuring that the optimal solution corresponds to the precise eigendecomposition of the matrix \(_{}\).

The second type of optimization problem involves scenarios where the model loss is independent of the eigenvalues and depends solely on the eigenvectors, such as in the latent-space PCA network expressed in Eq. (4). To enhance the efficiency of the solution process, the problem can be reformulated to include a trace penalty term, such as Brockett's cost, which is given by

\[_{}fh_{}(),[ ]{klargest}_{,}fh_{}(),- (^{}(_{})).\] (15)

In this formulation, a stop gradient operation is applied to \(\), since the eigen loss involves \(\) which relies on the parameter \(\). By introducing the stop gradient operation, we prevent this regularization term from propagating gradients back to \(\).

Besides the above relaxations, further simplification is possible if the loss function conforms to the structure of Brockett's cost function or a convex trace function. That is, when the model loss \(f_{}\) is a non-uniform linear combination of the eigenvalues, or it is monotonic and convex (or concave) to the eigenvalues, the trace penalty term is unnecessary. The loss function will inherently converge to the correct eigendecomposition. This principle is exemplified in the context of the nuclear norm regularization problem, which will be illustrated later. This can be formulated as:

\[_{}fh_{}(),, [}_{,}fh_{}(),,.\] (16)

## 5 Experiments

In this section, we present an evaluation of our approach, focusing on four specific tasks. Firstly, we demonstrate the convergence properties of our method empirically. Next, we measure the efficacy and efficiency of our amortized eigendecomposition technique over nuclear norm regularized auto-encoder and latent-space PCA using the MNIST dataset. Lastly, we assess the effectiveness of our approach in the context of graph adversarial learning tasks. We implement our approach with the deep learning framework JAX . All the experiments of our approach are conducted on a single NVIDIA A100 GPU with 40GB memory. The two fundamental questions we investigate are as follows:

* _Does our approach accurately identify the eigendecomposition and singular value decomposition?_
* _How does the efficiency of our method compare to that of traditional techniques?_

### Convergence

In this experiment, we evaluate the numerical error and convergence speed of our algorithm applied to solving eigendecomposition. We randomly generate ten symmetric matrices of size \(1000 1000\). The first 50 eigenvalues of these matrices range from 1 to 50, while the remaining eigenvalues lie between 0 and 1. Our objective is to compute the 50 largest eigenvalues by minimizing Brockett's cost function and convex trace loss (we adopt \(f(x)=x^{1.5}\) as the convex function). To achieve this, we employ several optimization algorithms, including Adam , Adamax , Yogi , SGD, and L-BFGS . These algorithms are provided by the Optax and JAX-opt libraries . We measure the mean square error (MSE) of the eigenvalues to the number of training iterations. The results are illustrated in Figure 3.

Figure 3: Convergence analysis on finding 50 largest eigenvalues on random \(1000 1000\)-dimensional symmetric matrices. (a): Convergence curves using Brockett’s cost and convex trace loss (\(f(x)=x^{1.5}\)). (b) The fine-tuning convergence on a series of similar matrices.

This result demonstrates that both loss functions are capable of identifying the correct eigenvalues with a small numerical error of \(10^{-9}\). However, there is a noticeable difference in convergence speed. For both trace loss functions, the Adam and Adamax optimizers outperform the others, achieving faster convergence rates. Brockett's cost function, which introduces a linear combination of the trace elements, is more numerically stable compared to the convex trace loss, resulting in faster convergence. This experiment validates the efficiency of the differentiable optimization framework for computing the \(k\) largest eigenvalues.

### Nuclear Norm Regularization

In this experiment, we apply the amortized eigendecomposition approach to the nuclear norm regularization problem, as outlined in Eq. (3). The experimental framework entails training an auto-encoder on the MNIST dataset by minimizing the reconstruction loss with a nuclear norm regularizer applied to the weight matrix \(^{n k}\) of the encoder's last layer. We employ a relaxation technique to the original problem defined in Eq. (16), which can be expressed as:

\[_{,}\|_{}_{}()-\|_{}+\| \|_{*}_{,,}\|_{}_{}()- \|_{}+_{i=1}^{k}\|_{i}\|_{2}\] (17)

where \(_{i}\)'s are the orthogonal column vectors of \(\), which are parameterized by \(\) through Eq. (13). The architectures of the encoder and the decoder are constructed as a 2-layer MLP with hidden layer dimensions of D = 128, 256, and 512. For comparison, we also implement the approach based on singular value decomposition (using the svd function). It should be noted that in the current versions of JAX, both the eigh and svd functions are limited to operations on full matrices.

The average execution time per iteration for the baseline backbone with only reconstruction loss, the backbone with svd, i.e. LHS of Eq. (17), and the backbone utilizing amortized eigendecomposition, i.e. RHS of Eq. (17) are reported in Table 1. We denote these execution times as \(t_{0}\), \(t_{1}\), and \(t_{2}\) respectively, and define the speed-up ratio for our approach relative to the svd as:

\[=-t_{0}}{t_{2}-t_{0}}.\] (18)

This ratio represents the improvement in execution speed of our eigendecomposition method compared to the standard svd, relative to the baseline backbone performance.

### Latent-space Principle Component Analysis

We investigate the effectiveness of our approach for the latent-space PCA method, as described in Eq. (4), using the MNIST dataset. Computing the eigenvectors of the large-scale covariance matrix in each iteration significantly increases the computation overhead, while our method amortizes this cost by jointly minimizing an additional loss. Moreover, we also aim to ensure that the first two eigenvalues are significantly larger than the subsequent ones. In the following objective function, the

    &  & Backbone & Backbone+ & Backbone+ &  \\  & & time (s/iter) & eigh/svd & our method & \\  & & time (s/iter) & time (s/iter) & & \(-t_{0}}{t_{2}-t_{0}}\) \\  & & \(t_{0}\) & \(t_{1}\) & \(t_{2}\) & \\   Nuclear norm \\ regularization \\  } & \(128 128\) & 5.275E-2 & 8.323E-2 & 6.025E-2 & \(4.06\) \\  & \(256 256\) & 5.600E-2 & 1.209E-1 & 6.080E-2 & \(13.5\) \\  & \(512 512\) & 7.186E-2 & 2.616E-1 & 7.366E-2 & \(105.4\) \\   Latent-space \\ PCA \\  } & \(256 2\) & 4.178E-3 & 1.446E-2 & 1.117E-2 & \(1.47\) \\  & \(512 2\) & 6.792E-3 & 2.918E-2 & 2.224E-2 & \(1.45\) \\  & \(1028 2\) & 1.434E-2 & 7.018E-2 & 5.467E-2 & \(1.39\) \\   Low-rank \\ GCN \\  } & \(2708 16\) & 1.021E-3 & 1.769E-2 & 1.732E-3 & \(23.4\) \\  & \(3312 16\) & 1.367E-3 & 2.825E-2 & 2.498E-3 & \(23.7\) \\   & \(19717 16\) & 1.931E-2 & 4.941E+0 & 2.731E-2 & \(615.2\) \\   

Table 1: Evaluation of execution times per iteration on three tasks.

additional term resembles Brockett's cost function while the trace of the covariance matrix ensures homogeneity:

\[_{,,}\|_{}^{T}_{}()-\|_{}- (^{}(h_{}()))}{((h_{}()))},\] (19)

where cov represents the covariance function. The architecture for both the encoder and decoder mirrors that of the nuclear norm regularization model, with the exception that there is a linear projection aligning with the direction of the principal components. The average execution times are reported in Table 1. More results and analysis are provided in Appendix A.2. The experimental results demonstrate that our approach achieves an average training speed improvement of 40% compared to the conventional eigendecomposition approach.

Additionally, we conducted a scalability study, with the results presented in Figure 4. This study examined the scaling behavior of latent PCA on the Celeb-A-HQ (256x256) dataset  by varying both the depth and width of the backbone autoencoder, with average execution time per iteration reported. The largest model in our tests, a 64-layer autoencoder with a 4096-dimensional latent space, contains over 1 billion parameters. From these results, we draw two main conclusions. First, our amortized eigen loss substantially reduces the eigendecomposition training time without significantly increasing the computational load of the backbone, as evidenced by the close alignment of the red (backbone) and green (backbone + our approach) lines. In contrast, the traditional eigendecomposition approach (blue line) scales steeply with increasing dimensionality, whereas our approach exhibits a much slower growth rate. Second, eigendecomposition emerges as the primary computational bottleneck within these neural network architectures, while the fully-connected layer computation

Figure 4: A comparison of scaling in the latent PCA task using the Celeb-A-HQ (256x256) dataset. The backbone autoencoders used in this study consist entirely of fully-connected layers with ReLU activation, all maintaining the same dimensions. Between the encoder and decoder, we applied both an eigen solver from the JAX eigh function and our amortized eigendecomposition method. We varied the depth of the autoencoders across 8, 16, 32, and 64 layers, and explored dimensionalities of 128, 256, 512, 1024, 2048, and 4096. The results present the average execution time per iteration over 100 runs. Notably, the largest model tested, featuring an autoencoder with 64 layers and a dimension of 4096, comprises up to 1.0 billion parameters.

remains minor, particularly at large widths (>2000). This is reflected in the widening gap between the backbone (red line) and backbone + eigh (blue line) as dimensionality increases. Notably, increasing the depth of the backbone while keeping the hidden dimension constant results in minimal change in execution time, indicating that the cost of fully-connected layers is small relative to eigendecomposition--further underscoring the results shown in Figure 1.

### Adversarial Attacks on Graph Convolutional Networks

In this study, we explore the robustness of graph convolutional networks (GCNs)  by implementing adversarial attacks on graph structures. The objective of this problem is described in Eq. (5). Our approach simplifies the attainment of a low-rank structure by optimizing:

\[_{,}\|_{}(, })-\|_{2}-(^{} ),\] (20)

where \(}=^{}(^{})\), with \(\) being parameterized by Eq. (13). The motivation for this formulation is that at optimum, the columns of \(\) correspond to the top-k eigenvectors of \(\). Then \(}\) becomes the best rank \(k\) approximation of \(\) under the Frobenius norm which corresponds to the terms \(\|}\|_{*},\|}-\|_ {}^{2}\) in Eq. (5). This formulation allows the GNN to operate on a low-rank graph \(}\), which has been shown to enhance robustness against adversarial attacks on the graph structure.

Our architecture consists of a three-layer GCN, which is utilized for semi-supervised node classification tasks on several citation networks, namely Cora, Citeseer, and Pubmed. Each layer has a hidden dimension of 32. The dropout rates are set to 0.4 for Cora and Citeseer, and to 0.1 for Pubmed, to prevent overfitting. For optimization, we employ the Adam algorithm with a learning rate of \(10^{-3}\).

The adversarial attacks are executed by perturbing the graph structure through the random addition and deletion of edges in the adjacency matrix. We quantify the extent of these perturbations using a _contamination rate_, which is defined as the ratio of the altered edge count to the total node pairs.

Additionally, we propose a graph modification based on the original objective, as detailed in Eq. (5). The graph Laplacian is represented by a symmetric matrix, on which the eigh function is applied to compute the eigenvalues. The corresponding execution times are documented in Table 1. For a more comprehensive experiment results and analysis of our findings, please refer to the Appendix A.3.

## 6 Discussion and Conclusion

In this study, we address a class of deep learning problems that incorporate eigendecomposition within their constraints or objective functions. Such problems are prevalent in applications like nuclear-norm regularized denoising, network compression, graph structure learning, and whitening normalization. The traditional approach requires performing eigendecomposition or singular value decomposition within the computational graph, which becomes the bottleneck in the training process. To circumvent this computation overhead, we introduce an amortized eigendecomposition framework integrating a relaxation eigen loss into the learning objective, which relies on a set of orthonormal vectors. These vectors are reparameterized efficiently through QR decomposition, thereby substantially reducing the computational cost of eigendecomposition in each iteration. Furthermore, the differentiable nature of QR decomposition allows for its seamless incorporation into the neural network training workflow. Our experimental results demonstrate that, when applied to network tasks, our algorithm not only accelerates the process but also maintains the precision of the conventional method with eigendecomposition. The method proves particularly beneficial as a differentiable top \(k\) eigensolver in environments where the backward gradient computation for top \(k\) eigendecomposition, such as in JAX or PyTorch, is not well-supported. Consequently, our approach provides a viable alternative for integrating top \(k\) eigendecomposition into neural networks.

**Limitations**. While our method excels in scenarios where eigendecomposition operations on large matrices are embedded within neural networks, it is important to note that when employed as a pure numerical eigensolver, it does not offer any speed or precision advantages over conventional methods. Thus, its suitability is specifically aligned with applications where eigendecomposition is a substantial component of the neural network training process.