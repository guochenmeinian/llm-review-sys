# DDK: Distilling Domain Knowledge for Efficient

Large Language Models

 Jiaheng Liu\({}^{,1}\), Chenchen Zhang*, Jinyang Guo\({}^{3}\), Yuanxing Zhang\({}^{2}\), Haoran Que\({}^{1}\),

**Ken Deng**\({}^{1}\), **Zhiqi Bai**\({}^{1}\), Jie Liu\({}^{4}\), Ge Zhang\({}^{5}\), Jiakai Wang\({}^{2}\), Yanan Wu\({}^{1}\), **Congnan Liu\({}^{1}\),

**Jiaming Wang**\({}^{2}\), **Lin Qu**\({}^{2}\), **Wenbo Su**\({}^{1}\), **Bo Zheng**\({}^{1}\)

\({}^{1}\)Taobao & Tmall Group of Alibaba, \({}^{2}\)Alibaba Group, \({}^{3}\)The University of Sydney,

\({}^{4}\)The Chinese University of Hong Kong, \({}^{5}\)University of Waterloo

{ljh411989}@alibaba-inc.com

* First two authors contributed equally.

###### Abstract

Despite the advanced intelligence abilities of large language models (LLMs) in various applications, they still face significant computational and storage demands. Knowledge Distillation (KD) has emerged as an effective strategy to improve the performance of a smaller LLM (i.e., the student model) by transferring knowledge from a high-performing LLM (i.e., the teacher model). Prevailing techniques in LLM distillation typically use a black-box model API to generate high-quality pretrained and aligned datasets, or utilize white-box distillation by altering the loss function to better transfer knowledge from the teacher LLM. However, these methods ignore the knowledge differences between the student and teacher LLMs across domains. This results in excessive focus on domains with minimal performance gaps and insufficient attention to domains with large gaps, reducing overall performance. In this paper, we introduce a new LLM distillation framework called DDK, which dynamically adjusts the composition of the distillation dataset in a smooth manner according to the domain performance differences between the teacher and student models, making the distillation process more stable and effective. Extensive evaluations show that DDK significantly improves the performance of student models, outperforming both continuously pretrained baselines and existing knowledge distillation methods by a large margin.

## 1 Introduction

Recent advancements in Large Language Models (LLMs) such as LLaMA [7; 9; 58; 59] have garnered significant attention due to their strong intelligence. However, these models also impose considerable computational and storage demands, particularly in practical deployments such as instant chat, copilot, and query rewriting. Consequently, the development of lightweight yet efficacious LLMs suitable for real-world applications has become an area of increasing research interest. Several small-scale LLMs, e.g., Phi  and MiniCPM , have been designed to facilitate rapid inference on devices with limited resources. These models are generally trained from scratch using a large volume of selectively curated high-quality datasets, which could be prohibitive for the broader research community. Meanwhile, there has been a surge in the exploration of model compression techniques  to reduce the resource footprint of LLMs. Apart from these techniques, knowledge distillation (KD) emerges as a prominent method for creating effective neural networks, which transfer knowledge from a high-performing teacher model to a compact student model.

The primary challenges in enhancing the performance of KD approaches on LLMs stem from two main aspects: i) appropriately utilizing the data [3; 67]; ii) stabilize the distillation process . Recently, it has become increasingly acknowledged that the mixture ratios of various domains within the training dataset substantially affect the performance [20; 67; 69]. Regarding the issue of data composition, the influence of domain-specific mixtures for KD remains underexplored. As shown in Fig. 1, the performance between Qwen-1.5 1.8B  (student) and Qwen-1.5 14B  (teacher) reveals that the performance gap varies significantly across domains. For instance, in the "Books" domain, the student model significantly underperforms the teacher model, while in "The Stack" domain, the difference is minimal, which indicates that the "Books" domain is relatively not optimized well for the student model compared to the teacher model, and more data from the "Books" domain should be included. Therefore, we aim to design a knowledge distillation framework that can dynamically adjust the data composition during distillation to reallocate more computation to domains, where the student and teacher models have larger performance gaps.

In this paper, we introduce a novel methodology, termed **D**istill **D**omain **K**nowledge for LLMs (DDK), which effectively optimizes domain-specific mixtures to address the performance discrepancy between teacher and student models across different domains. Specifically, DDK begins by quantifying the performance deviations between the teacher and student LLMs using an offline-collected validation dataset across various domains. Next, it periodically re-calculates the domain discrepancy factor based on the performance gap between the teacher and student models. Finally, DDK employs a domain knowledge-guided sampling strategy to sample data from different domains with varying probabilities based on the calculated domain discrepancy factor. Additionally, inspired by the optimization algorithms , we propose a factor smooth updating mechanism to augment the stability and robustness of the DDK approach. For the supervision loss, we minimize the differences in the output logits between the teacher and student models. As demonstrated in Fig. 1, the performance gap across domains is significantly reduced by DDK.

Our main contributions are summarized as follows:

* To the best of our knowledge, we are the first to study the influence of domain-specific data mixtures for distilling LLMs, and efficiently transfer the domain knowledge of the teacher network upon the domain weights.
* DDK proposes a factor smooth updating strategy to strategically enhance the appropriate focus of the distillation process on targeted domains, which effectively stabilizes the domain knowledge guided sampling process for smoother distillation.
* Extensive experiments on multiple benchmark datasets demonstrate the effectiveness and generalization ability of our proposed DDK.

## 2 Related Works

**Large Language Models.** The emergence of LLMs [62; 72; 24; 41; 19; 65; 49; 53; 27; 6; 66] marks a significant milestone in the domain of natural language processing, with notable examples including GPT3, Lamda, Palm, and several others [2; 4; 10; 44; 57]. For example, Radford and Narasimhan

Figure 1: The perplexity scores of different methods across different domains for different methods (See Section 4 for more details.). Note that “Chinese CC” denotes “Chinese CommonCrawl”.

 introduced the GPT model, leveraging multiple layers of transformer decoder blocks, while Meta later developed LLaMA  employing an enhanced transformer architecture, subsequently evolved into LLaMA2 . Recent advancements have also seen the application of instruction tuning [13; 63] and learning through human feedback [8; 45; 74] to better align LLMs with human understanding and foster the creation of versatile AI assistants [21; 43]. Despite their potential, LLMs' extensive capabilities are often accompanied by vast sizes [35; 64], demanding significant computational resources. In this work, we aim to focus on how to produce small LLMs based on the knowledge distillation approach.

**Knowledge Distillation**. Knowledge distillation is a pivotal technique in model compression and acceleration [61; 42; 26; 70; 25], primarily employed to transfer knowledge from a robust, well-trained teacher model to a compact student model [29; 1; 37]. Recently, several approaches to knowledge distillation tailored for LLMs have been proposed. These approaches can be broadly classified into two categories: _White-box KD_ leverages either the internal parameters or the logits of the teacher LLM during the distillation process [22; 46; 56; 71]. For example, Gu et al.  propose that traditional Kullback-Leibler divergence (KLD) objective is inappropriate for open text generation tasks and propose MiniLLM to minimize reverse KLD through policy gradient techniques . Conversely, _black-box KD_ relies solely on the outputs from the teacher model [12; 30; 34; 48; 60]. For example, "Distilling Step-by-Step" strategy  employs Chain of Thought (CoT) prompting to provide sophisticated guidance during distillation. These two types of KD approaches mainly focus on aligning the generative behaviors of the teacher and student models. DDK delves into the efficacies of domain-specific distillation, aiming to mitigate the discrepancies in performance between the teacher and student model across different domains. Hence, DDK is fundamentally orthogonal to these methods.

## 3 Methodology

### Overview

Figure 2 illustrates the comprehensive architecture of the DDK framework. DDK employs a large-scale teacher LLM and a comparatively smaller student LLM, with the objective of transferring knowledge from the former to the latter to enhance performance utilizing a specially curated distillation dataset. Initially, the distillation dataset is constructed by randomly sampling from the training corpus. Throughout the distillation process, we continuously assess the domain-specific performance of both the teacher and student LLMs, and use domain knowledge guided sampling to dynamically update the data mixture on the student's abilities within specific domains. As the domain proficiency of the student LLM evolves during distillation, we introduce a factor smooth updating strategy to ensure the robustness of the domain knowledge-guided sampling approach. Finally, DDK provides of a better student LLM, optimized for enhanced performance across targeted domains.

### Domain Knowledge Guided Sampling

The distilled student LLMs are anticipated to exhibit robust competence across various preset domains. Nevertheless, prevailing knowledge distillation techniques tailored for LLMs tend to homogeneously optimize performance across these domains, leading to potential performance degradation. To address this issue, we design the domain knowledge guided sampling strategy to enhance distillation efficacy by prioritizing domain-specific complexities.

**Domain discrepancy factor construction.** We consider a dataset \(\) that has been partitioned into \(N\) distinct domains. We denote the pre-trained teacher LLM as \(_{}\) and the student model, which is currently under training, as \(_{}\). To efficiently identify and prioritize data that may yield the most learning benefit, particularly from domains where the student model underperforms, we introduce a _domain discrepancy factor_ denoted as \(^{N}\). Each component \([i]\) of this vector quantitatively represents the discrepancy in performance between the teacher and student models within the \(i\)-th domain. As we assume a good student should exhibit close approximation to the teacher across all domains, \(\) is calibrated to reflect differential performance indices as follows:

\[[i]=(_{}[i]/_{}[i])/_{i^{} \{1,,N\}}(_{}[i^{}]/_{}[i^{}])\] (1)

where \(_{}[i]= ((_{}(V_{i}),Y_{i}))\) and \(_{}[i]=((_{}(V_{i}),Y_{i}))\).

Here, \(V_{i}\) and \(Y_{i}\) are the inputs and the ground-truth labels of the validation dataset of the \(i\)th domain. \(()\) represents the cross-entropy loss. \(_{}^{N}\) and \(_{}^{N}\) are the perplexity scores over the validation sets of all domains for student and teacher respectively, indexed by the domain index \(i\). In this case, a higher value of \([i]\) signifies a pronounced disparity in domain-specific proficiency between the student model and the teacher model. Accordingly, it is imperative to allocate more relevant data to enhance the domain expertise.

**Domain knowledge guided sampling.** We employ a domain knowledge-informed sampling strategy to refine the composition of the distillation dataset, which utilizes a probabilistic mechanism defined by vector \(\) to iteratively select samples from the training corpus. The process continues cyclically once a domain data has been exhausted. Finally, DDK strategically increases the data allocation towards underperforming domains, thereby mitigating the performance discrepancies between the teacher and student models across all domains.

### Factor Smooth Updating

With the domain knowledge guided sampling strategy, we can dynamically focus on more challenging domains during the distillation process. Nonetheless, we observe that the domain discrepancy factor exhibits significant fluctuations throughout this procedure. Such rapid alterations may precipitate exceedingly unbalanced data sampling, potentially compromising the stability of the distillation.

**Factor smooth updating.** To enhance the stability of the distillation process, we periodically adjust the domain discrepancy factor every \(K\) iterations throughout the distillation process, thereby partitioning it into discrete intervals. The parameter \(K\) is pivotal as it governs the system's capacity to address immediate discrepancies and influences the stability of the data mixture. We denote the domain discrepancy factor for the \(i\)-th domain at the \(t\)-th interval of distillation as \(^{t}[i]\). Similarly, let \(_{}^{t}[i]\) and \(_{}^{t}[i]\) denote the perplexity scores at the beginning of the \(t\)-th distillation interval. In DDK, the domain discrepancy factor at the \((t+1)\)-th interval is defined as:

\[&^{t+1}[i]=^{t+1}[i]}{ _{i=1}^{N}^{t+1}[i]}+(1-)/N,\\ &^{t+1}[i]=^{t}[i](_{ }^{t+1}[i]/_{}^{t+1}[i]).\] (2)

Note that a constant term is incorporated in \(^{t}[i]\) to preclude the occurrence of excessively small values, thereby guaranteeing a baseline probability for data sampling across various domains. The parameter \(\), designated as the smoothing coefficient, is fixed at a value of 0.5 in our experimental

Figure 2: Overview of the distillation process of DDK. First, the training dataset is divided into distinct domains based on predefined criteria. Then, DDK dynamically modulates the distribution of domain-specific data, augmenting the amount allocated to domains where the student model struggles the most. The proportions attributed to each domain are recalculated at distillation intervals by employing a factor smooth updating approach.

setup. In addition, the inclusion of \(^{t}\) imparts a history mixture information on the modification of the domain discrepancy factor. This mechanism facilitates a gradual modification of \(^{t}[i]\), thereby minimizing fluctuations and ensuring a stable, domain knowledge-driven distillation process for fetching informative data.

### Overall Optimization

As we jointly update the student LLM parameters and the domain discrepancy factor in the distillation process, the optimization object can be written as follows:

\[_{_{}}_{i\{1,,N\}}(_{}(V_{i}),Y_{i})+((z_{}(V_{i}),T), (z_{}(V_{i}),T)),\] (3)

where \(_{}\) is the parameters of the student model. \(z_{}()\) and \(z_{}()\) are the output hidden states from student and teacher LLMs, respectively. We leverage KL-divergence to approximate the student model's output to the teacher model's output, over a distillation temperature \(T\). \(\) is the factor to balance these two terms. Algorithm 1 summarizes the pseudo-code of the DDK process. In practice, the distillation process is typically concluded either when all available data has been fully utilized or when the domain discrepancy factor approaches a threshold indicative of minimal disparity between the teacher and student models.

```
0: Distillation dataset \(D\); The steps per distillation interval \(K\);
1: Initialize domain discrepancy factor \(^{0}\) based on Eq. 1;
2: Randomly sample \(D^{0} D\) that supports \(K\) steps distillation;
3: Initialize student training iteration \(c=0\), distillation interval \(t=0\);
4:for each iteration in the training process do
5: // Update student LLM parameters
6: Read a batch of samples and use Eq. 3 to update the parameters of student LLM;
7:\(c=c+1\)
8:if\(c\) mod \(K\) == 0 then
9: // Update distillation data mixture
10: \(t=t+1\);
11: Use Eq. 2 to update domain discrepancy factor \(^{t}\);
12: Sample a dataset, \(D^{t} D\), that supports \(K\) steps distillation according to \(^{t}\);
13: Shuffle \(D^{t}\);
14:if\(t\) reaches a preset maximal number of intervals then
15: Stop the distillation loop;
16: The distilled student LLM; ```

**Algorithm 1** Distillation procedure of the DDK framework.

## 4 Experiments

In this section, we make comprehensive evaluations to answer two research questions: **RQ1**: To what extent does the DDK process improve the performance of a small-scale LLM? **RQ2**: How does the dynamic domain-specific guidance contribute to the overall improvement?

### Experimental Setup

Model configuration details.We use the Qwen-1.5  and LLaMA2  to demonstrate the effectiveness of DDK. Regarding the Qwen-1.5 series, we use Qwen-1.5 14B and Qwen-1.5 1.8B as the teacher and student models, respectively. For LLaMA2 series, we use LLaMA2 13B and TinyLLaMA 1.1B  as the teacher and student models, respectively.

Training details.Due to the unavailability of training data for LLaMA2 and Qwen-1.5 models, we mainly utilize RedPajama  for distillation, which consists of training data derived from seven distinct domains: CommonCrawl, C4, The Stack, Wikipedia, Books, ArXiv, and StackExchange.

[MISSING_PAGE_FAIL:6]

### Main Results

As shown in Table 1-2, we report the performance results of different baseline methods. The following observations provide a comprehensive response to RQ1: (1) We see that the absence of knowledge transfer from the teacher model significantly impedes the student model's capabilities in intricate tasks such as coding (e.g., HumanEval) and Chinese comprehension (e.g., C3). (2) DDK outperforms other baseline methods when using different types of teacher and student models, which demonstrates the effectiveness of DDK for training small student LLMs. (3) The baseline methods KD, TED, and MiniLLM exhibit similar performance. For instance, the average accuracy of these three approaches hovers around 52% when distilling onto the Qwen student model. We hypothesize that in the context of LLM distillation, domain data mixture may emerge as a key performance bottleneck, and the existing baseline techniques fail to adequately address this challenge. (4) The performance gains vary across different domains. Notably, when distilling the Qwen model, we achieve significant improvements on the reasoning tasks (e.g., Code on Humaneval and MBPP, Math on GSM8K), which indicates that the student model can improve a lot on the reasoning tasks under the guidance of the teacher model. This empirical observation suggests that DDK is successful in directing additional attention toward the more challenging problem domains.

### Ablation Study

In this section, we perform ablation studies to assess the robustness of the DDK model and its sensitivity to key hyperparameters. We collected data using Qwen 1.5 and reported its performance on the validation sets of MMLU, RACE, and ARC-C, which differ from those discussed in the previous subsection. Initially, we concentrate on addressing RQ1 through fine-grained analyses.

**Effect of data sampling strategies.** We propose two variants of data sampling strategies on DDK. For DDK (w/o FS), we just remove the factor smooth updating mechanism and directly take \(^{t}\) as the probability of each domain. For DDK (ES), we sample data from each domain equally. The results are shown in Fig. 4, and we can suppose that both factor smooth updating and domain knowledge guided sampling contribute to the distillation owing to the existence of domain-specific discrepancy.

**Effect of distillation interval.** Fig. 3 (a) shows the evaluation results on the effect of the distillation interval hyperparameter (i.e., \(K\)) in Alg. 1. We observe that increasing \(K\) from 100 to 1,000 leads to better performance, indicating that a rapid updating frequency may destabilize the distillation process. However, further increasing \(K\) leads to inferior results. We conclude that when the updating

Figure 4: Effect of data sampling strategies.

Figure 3: (a). Effect of distillation interval. (b). Effect of the number of training tokens.

[MISSING_PAGE_FAIL:8]

**Visualization.** To better show the effectiveness of the factor smooth strategy in DDK, we compare the DDK (w/o FS) with our DDK by showing the domain discrepancy in the training process, where DDK (w/o FS) means that we remove the factor smooth updating strategy. Specifically, in Fig. 5, we compute the \((_{}[i]/_{}[i])\) as the ratio to represent the domain discrepancy for \(i\)-th domain, where a large ratio means a large discrepancy. As shown in Fig. 5, we observe that the ratio updates smoothly in DDK. Besides, in Table 4, the DDK is better than DDK (w/o FS), which means DDK can benefit a lot when using the factor smooth updating strategy.

Moreover, we refer readers to see Appendix B.2 and Appendix C for more details on the training costs and inference examples.

## 5 Conclusion

In this study, we introduce DDK, a novel framework for knowledge distillation tailored for LLMs. Our initial investigations underscore the criticality of optimizing domain data mixtures in the context of LLM distillation. To address this, we propose a domain knowledge-guided sampling approach that dynamically modulates the sampling probabilities across various domains. Furthermore, we put forward a factor smooth update strategy aimed at enhancing both the stability and the efficacy of the distillation process. Comprehensive evaluations of several benchmark datasets with diverse teacher-student model configurations demonstrate the effectiveness of the DDK framework.

  
**Methods** & **CEval** & **MMLU** & **GSM8K** & **Arc-E** & **Arc-C** & **Avg.** \\  Qwen-14B & 79.86 & 66.30 & 69.14 & 89.24 & 82.25 & 77.36 \\  Student (1.8B) & 61.96 & 45.59 & 38.4 & 72.16 & 52.11 & 54.04 \\ + CPT & 60.92 & 45.60 & 43.36 & 73.10 & 52.28 & 55.05 \\ + KD & 61.66 & 44.28 & 50.26 & 73.87 & 54.69 & 56.95 \\ + **DDK (Ours)** & 65.38 & 47.59 & 55.19 & 76.64 & 57.01 & **60.36** \\   

Table 6: Few-shot (**5-shot**) performance results of different methods on the Qwen-1.5 models. Note that we use Qwen-1.5 14B and Qwen-1.5 1.8B as teacher and student models, respectively.

Figure 5: Visualization on the domain discrepancy among three domains.

    &  &  &  &  &  \\  & EM & ES & EM & ES & EM & ES & EM & ES & EM & ES \\  Teacher (15.5B) & 35.9 & 66.1 & 41.5 & 72.9 & 38.7 & 73.7 & 56.3 & 79.3 & 43.1 & 73.0 \\  Student (3B) & 20.8 & 41.5 & 25.3 & 51.4 & 25.7 & 56.2 & 40.5 & 60.5 & 28.1 & 52.4 \\ + CPT & 24.8 & 49.3 & 31.6 & 61.5 & 30.5 & 63.7 & 47.1 & 68.4 & 33.5 & 60.7 \\ + KD & 26.5 & 53.2 & 32.4 & 61.1 & 31.6 & 64.5 & 48.0 & 69.8 & 34.6 & 61.2 \\ **+ DDK (Ours)** & 31.7 & 62.2 & 34.6 & 69.8 & 33.2 & 69.3 & 50.9 & 76.2 & 37.6 & 69.4 \\   

Table 5: Results of different methods on the StarCoder models. Note that we use StarCoder 15.5B and StarCoder 3B as teacher and student models, respectively.