# Self-Play Fine-Tuning of Diffusion Models for Text-to-Image Generation

Huizhuo Yuan Zixiang Chen Kaixuan Ji Quanquan Gu

Department of Computer Science

University of California, Los Angeles

Los Angeles, CA 90095

{hzyuan,chenzx19,kaixuanji,qgu}@cs.ucla.edu

Equal contribution

###### Abstract

Fine-tuning Diffusion Models remains an underexplored frontier in generative artificial intelligence (GenAI), especially when compared with the remarkable progress made in fine-tuning Large Language Models (LLMs). While cutting-edge diffusion models such as Stable Diffusion (SD) and SDXL rely on supervised fine-tuning, their performance inevitably plateaus after seeing a certain volume of data. Recently, reinforcement learning (RL) has been employed to fine-tune diffusion models with human preference data, but it requires at least two images ("winner" and "loser" images) for each text prompt. In this paper, we introduce an innovative technique called self-play fine-tuning for diffusion models (SPIN-Diffusion), where the diffusion model engages in competition with its earlier versions, facilitating an iterative self-improvement process. Our approach offers an alternative to conventional supervised fine-tuning and RL strategies, significantly improving both model performance and alignment. Our experiments on the Pick-a-Pic dataset reveal that SPIN-Diffusion outperforms the existing supervised fine-tuning method in aspects of human preference alignment and visual appeal right from its first iteration. By the second iteration, it exceeds the performance of RLHF-based methods across all metrics, achieving these results with less data. Codes are available at https://github.com/uclaml/SPIN-Diffusion/.

## 1 Introduction

Diffusion models (Ho et al., 2020; Peebles and Xie, 2023; Podell et al., 2023; Nichol et al., 2021; Rombach et al., 2022a; Song et al., 2020) have rapidly emerged as critical entities within the realm of generative AIs (Creswell et al., 2018; Kingma and Welling, 2013), demonstrating exceptional capabilities in generating high-fidelity outputs. Their versatility spans a diverse area of applications, ranging from image generation (Rombach et al., 2022a; Podell et al., 2023; Ramesh et al., 2022) to more complex tasks like structure-based drug design (Corso et al., 2022; Guan et al., 2023), protein structure prediction (Watson et al., 2021), text generation (Austin et al., 2021; Zheng et al., 2023; Chen et al., 2023), and more. Prominent diffusion models in image generation, including DALL-E (Ramesh et al., 2022), Stable Diffusion (Rombach et al., 2022b), SDXL (Podell et al., 2023), and Dreamlike, etc., typically undergo a fine-tuning process following their initial pre-training phase.

Standard fine-tuning method for diffusion models suffers from low alignment with human preferences and low data efficiency due to two main reasons: (1) it does not directly optimize for alignment with human preferences, and (2) only one round of training can be performed. Recently, using Reinforcement Learning (RL) for fine-tuning diffusion models has received increasing attention. Lee et al. (2023) first studied the alignment of text-image diffusion models to human preferences using reward-weighted likelihood maximization with a reward function trained on human preference data.

Black et al. (2023) formulated the fine-tuning of diffusion models as a RL problem solved by policy gradient optimization. In a concurrent work, Fan et al. (2023) studied a similar formulation but with a KL regularization. Very recently, Wallace et al. (2023) have bypassed the need for training reward functions by using Direct Preference Optimization (DPO) (Rafailov et al., 2023) for fine-tuning diffusion models. Similar approach was proposed in Yang et al. (2023) as well.

While RL fine-tuning of diffusion methods has been proven effective, its dependency on human preference data, often necessitating multiple images per prompt, poses a significant challenge. In many datasets including the community-sourced ones featuring custom content, it is often the case to have only one image associated with each prompt. This makes RL fine-tuning infeasible.

In this paper, drawing inspiration from the recently proposed self-play fine-tuning (SPIN) technique (Chen et al., 2024) for large language models (LLM), we introduce a new supervised fine-tuning (SFT) method for diffusion models, eliminating the necessity for human preference data in the fine-tuning process. Central to our method is a general-sum minimax game, where both the participating players, namely the main player and the opponent player, are diffusion models. The main player's goal is to discern between samples drawn from the target data distribution and those generated by the opponent player. The opponent player's goal is to garner the highest score possible, as assessed by the main player. A self-play mechanism can be made possible, if and only if the main player and the opponent player have the same structure, and therefore the opponent player can be designed to be previous copies of the main player (Chen et al., 2024). The proposed algorithm SPIN-Diffusion overcomes the drawbacks of both supervised fine-tuning (SFT) and RL fine-tuning. Compared with SFT, our method is more data-efficient, by repeatedly using the prompts from the SFT dataset to improve the model through self-play. Compared with RL fine-tuning methods, our method does not need external reward models or expensive human-annotated winner/loser pairs.

When applying the self-play fine-tuning technique (Chen et al., 2024) to diffusion models, there are two challenges: (a) an exponential or even infinite number of possible trajectories can lead to the same image. The generator in a diffusion model operates through a sequence of intermediate steps, but the performance of the generator is only determined by the quality of the image in the last step; and (b) diffusion models are parameterized by a sequence of score functions, which are the gradient of the probabilities rather than probabilities in LLMs. Our algorithm design effectively surmounts these challenges by (a) designing an objective function that considers all intermediate images generated during the reverse sampling process; and (b) decomposing and approximating the probability function step-by-step into products related to the score function. We also employ the Gaussian reparameterization technique in DDIM (Song et al., 2020) to support the advanced sampling method. All these techniques together lead to an unbiased objective function that can be effectively calculated based on intermediate samples. For computational efficiency, we further propose an approximate objective function, which eliminates the need for intermediate images used in our model.

**Contributions.** Our contributions are summarized below:

* We propose a novel fine-tuning method for diffusion models based on the self-play mechanism, called SPIN-Diffusion. The proposed algorithm iteratively improves upon a diffusion model until converging to the target distribution. Theoretically, we prove that the model obtained by SPIN-Diffusion cannot be further improved via standard SFT. Moreover, the stationary point of our self-play mechanism is achieved when the diffusion model aligns with the target distribution.
* Empirically, we evaluate the performance of SPIN-Diffusion on text-to-image generation tasks (Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022). Our experiments on the Pick-a-Pic dataset (Kirstain et al., 2023), with base model being Stable Diffusion-1.5 (Rombach et al., 2022), demonstrate that SPIN-Diffusion surpasses SFT from the very first iteration. Notably, by the second iteration, SPIN-Diffusion outperforms Diffusion-DPO (Wallace et al., 2023) that utilizes additional data from 'loser' samples. By the third iteration, the images produced by SPIN-Diffusion achieve a higher PickScore (Kirstain et al., 2023) than the base model SD-1.5 \(79.8\%\) of the times, and a superior Aesthetic score \(88.4\%\) of the times.

SPIN-Diffusion exhibits a remarkable performance improvement over current state-of-the-art fine-tuning algorithms, retaining this advantage even against models trained with more extensive data usage. This highlights its exceptional efficiency in dataset utilization. It is beneficial for the general public, particularly those with restricted access to datasets containing multiple images per prompt.

**Notation.** We use lowercase letters and lowercase boldface letters to denote scalars and vectors, respectively. We use \(0:T\) to denote the index set \(\{0,,T\}\). In the function space, let \(\) be the function class. We use the symbol \(\) to denote the real distribution in a diffusion process, while \(}\) represents the distribution parameterized by a nueral network during sampling. The Gaussian distribution is represented as \((,)\), where \(\) and \(\) are the mean and covariance matrix, respectively. Lastly, \(\{1,,T\}\) denotes the uniform distribution over the set \(\{1,,T\}\).

## 2 Related Work

**Diffusion Models.** Diffusion-based generative models (Sohl-Dickstein et al., 2015) have recently gained prominence, attributed to their ability to produce high-quality and diverse samples. A popular diffusion model is denoising diffusion probabilistic modeling (DDPM) (Ho et al., 2020). Song et al. (2020) proposed a denoising diffusion implicit model (DDIM), which extended DDPM to a non-Markov diffusion process, enabling a deterministic sampling process and the accelerated generation of high-quality samples. In addition to DDPM and DDIM, diffusion models have also been studied with a score-matching probabilistic model using Langevin dynamics (Song and Ermon, 2019; Song et al., 2020). Diffusion models evolved to encompass guided diffusion models, which are designed to generate conditional distributions. When the conditioning input is text and the output is image, these models transform into text-to-image diffusion models (Rombach et al., 2022; Ramesh et al., 2022; Ho et al., 2022; Saharia et al., 2022). They bridge the gap between textual descriptions and image synthesis, offering exciting possibilities for content generation. A significant advancement in text-to-image generation is the introduction of Stable Diffusion (SD) (Rombach et al., 2022). SD has expanded the potential of diffusion models by integrating latent variables into the generation process. This innovation in latent diffusion models enables the exploration of latent spaces and improves the diversity of generated content. Despite the introduction of latent spaces, generating images with desired content from text prompts remains a significant challenge (Gal et al., 2022; Ruiz et al., 2023). This is due to the difficulty in learning the semantic properties of text prompts with limited high-quality data.

**Fine-Tuning Diffusion Models.** Efforts to improve diffusion models have focused on aligning them more closely with human preferences. Rombach et al. (2022) fine-tuned a pre-trained model using the COCO dataset (Caesar et al., 2018), demonstrating superior performance compared to a generative model directly trained on the same dataset. Podell et al. (2023) expanded the model size of Stable Diffusion (SD) to create the SDXL model, which was fine-tuned on a high-quality but private dataset, leading to a significant improvement in the aesthetics of the generated images. Dai et al. (2023) further demonstrated the effectiveness of fine-tuning and highlighted the importance of the supervised fine-tuning (SFT) dataset. In addition to using datasets with high-quality images, Betker et al. (2023); Segalis et al. (2023) found that SFT on a data set with high text fidelity can also improve the performance of the diffusion model. The aforementioned methods only requires a high-quality SFT dataset. Recently, preference datasets have been studied in finetuning diffusion models (Lee et al., 2023). Concurrently, DDPO (Black et al., 2023) and DPOK (Fan et al., 2023) proposed to use the preference dataset to train a reward model and then fine-tune diffusion models using reinforcement learning. Drawing inspiration from the recent Direct Preference Optimization (DPO) (Rafailov et al., 2023), Diffusion-DPO (Wallace et al., 2023) and D3PO (Yang et al., 2023) used the implicit reward to fine-tune diffusion models directly on the preference dataset. Furthermore, when a differentiable reward model is available, Clark et al. (2023); Prabhudesai et al. (2023) applied reward backpropagation for fine-tuning diffusion models. Our SPIN-Diffusion is most related to the SFT method, as it only assumes access to high-quality image-text pairs. However, the high-quality image-text dataset can be obtained from various sources, including selecting the winner from a preference dataset or identifying high-reward image-text pairs through a reward model.

## 3 Problem Setting and Preliminaries

In this section, we introduce basic settings for text-to-image generation by diffusion models and the self-play fine-tuning (SPIN) method.

### Text-to-Image Diffusion Model

Denoising diffusion implicit models (DDIM) (Song et al., 2020) is a generalized framework of denoising diffusion probabilistic models (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020). DDIM enables the fast generation of high-quality samples and has been widely used in text-to-imagediffusion models such as Stable Diffusion (Rombach et al., 2022). We formulate our method building upon DDIM, which makes it more general.

**Forwrd Process.** Following Saharia et al. (2022), the problem of text-to-image generation can be formulated as conditional diffusion models. We use \(_{0}^{d}\) to denote the value of image pixels where \(d\) is the dimension and use \(\) to denote the text prompt. Given a prompt \(\), image \(_{0}\) is drawn from a target data distribution \(p_{}(|)\). The diffusion process is characterized by the following dynamic parameterized by a positive decreasing sequence \(\{_{t}\}_{t=1}^{T}\) with \(_{0}=1\),

\[q(_{1:T}|_{0}):=q(_{T}|_{0})_{t=2}^{ T}q(_{t-1}|_{t},_{0}),\] (3.1)

where \(q(_{t-1}|_{t},_{0})\) represents a Gaussian distribution \((_{t},_{t}^{2})\). Here, \(_{t}\) is the mean of Gaussian defined as

\[_{t}:=}_{0}+-_ {t}^{2}}_{t}-}_{0}}{}}.\]

It can be derived from (3.1) that \(q(_{t}|_{0})=(}_{0}, (1-_{t}))\) for all \(t\)(Song et al., 2020). As a generalized diffusion process of DDPM, (3.1) reduces to DDPM (Ho et al., 2020) with a special choice of \(_{t}=)/(1-_{t})/_{t- 1})}}\).

**Generative Process.** Given the sequence of \(\{_{t}\}_{t=1}^{T}\) and \(\{_{t}\}_{t=1}^{T}\), examples from the generative model follows

\[p_{}(_{0:T}|)=_{t=1}^{T}p_{}( _{t-1}|_{t},) p_{}(_{ T}|), p_{}(_{t-1}|_{t}, )=_{}(_{t},,t),_{t}^{2}.\] (3.2)

Here \(\) belongs to the parameter space \(\) and \(_{}(_{t},,t)\) is the estimator of mean \(_{t}\) that can be reparameterized (Ho et al., 2020; Song et al., 2020) as the combination of \(_{t}\) and a neural network \(_{}(_{t},,t)\) named score function. Please see Appendix C for more details.

**Training Objective.** The score function \(_{}(_{t},,t)\) is trained by minimizing the evidence lower bound (ELBO) associated with the diffusion models in (3.1) and (3.2), which is equivalent to minimizing the following denoising score matching objective function \(L_{}\):

\[L_{}()=_{t}_{}(_{t},,t)-_{t} _{2}^{2},\] (3.3)

where \(_{t}=}_{0}+}_{t}\) and the expectation is computed over the distribution \( q(),_{0} q_{}(|),_{t}(0,)\), \(t\{1,,T\}\). In addition, \(\{_{t}\}_{t=1}^{T}\) are pre-specified weights that depends on the sequences \(\{_{t}\}_{t=1}^{T}\) and \(\{_{t}\}_{t=1}^{T}\).

### Self-Play Fine-Tuning

Self-Play mechanism, originating from TD-Gammon (Tesauro et al., 1995), has achieved great successes in various fields, particularly in strategic games (Silver et al., 2017, 201). Central to Self-Play is the idea of progressively improving a model by competing against its previous iteration. This approach has recently been adapted to fine-tuning Large Language Models (LLMs) (Chen et al., 2024), called self-play fine-tuning (SPIN). Considering an LLM where \(\) is the input prompt and \(_{0}\) is the response, the goal of SPIN is to fine-tune an LLM agent, denoted by \(p_{}(|)\), based on an SFT dataset. Chen et al. (2024) assumed access to a main player and an opponent player at each iteration and takes the following steps iteratively:

1. The main player maximizes the expected value gap between the target data distribution \(p_{}\) and the opponent player's distribution \(p_{_{k}}\):
2. The opponent player generates responses that are indistinguishable from \(p_{}\) by the main player.

Instead of alternating optimization, SPIN directly utilizes a closed-form solution of the opponent player, which results in the opponent player at iteration \(k+1\) to copy parameters \(_{k+1}\), and forming an end-to-end training objective:

\[L_{}= }(_{0}|)}{p_{_{k}}(_{0}|)} -}(_{0}^{}|)}{p_{_{k}}(_{0}^{}|)}.\] (3.4)

Here the expectation is taken over the distribution \( q(), p_{}(| ),^{} p_{_{k}}(^{}| )\), \(()\) is a loss function that is both monotonically decreasing and convex, and \(>0\) is a hyperparameter. Notably, (3.4) only requires the knowledge of demonstration/SFT data, i.e., prompt-response pairs.

## 4 Method

In this section, we are going to present a method for fine-tuning diffusion models with self-play mechanism.

Consider a setting where we are training on a high-quality dataset containing image-text pairs \((,_{0}) p_{}(_{0}|)q( )\) where \(\) is the text prompt and \(_{0}\) is the image. Our goal is to fine-tune a pretrained diffusion model, denoted by \(p_{}\), to align with the distribution \(p_{}(_{0}|)\). Instead of directly minimizing the denoising score matching objective function \(L_{}\) in (3.3), we adapt SPIN to diffusion models. However, applying SPIN to fine-tuning diffusion models presents unique challenges. Specifically, the objective of SPIN (3.4) necessitates access to the marginal probability \(p_{}(_{0}|)\). While obtaining \(p_{}(_{0}|)\) is straightforward in LLMs, this is not the case with diffusion models. Given the parameterization of the diffusion model as \(p_{}(_{0:T}|)\), computing the marginal probability \(p_{}(_{0}|)\) requires integration over all potential trajectories \(_{_{1:T}}p_{}(_{0:T}|)d _{1:T}\), which is computationally intractable.

In the following, we propose a novel SPIN-Diffusion method with a decomposed objective function that only requires the estimation of score function \(_{}\). This is achieved by employing the DDIM formulation discussed in Section 3. The key technique is self-play mechanism with a focus on the joint distributions of the entire diffusion process, i.e., \(p_{}(_{0:T}|)=q(_{1:T}|_ {0})p_{}(_{0}|)\) and \(p_{}(_{0:T}|)\), instead of marginal distributions.

### Differentiating Diffusion Processes

In iteration \(k+1\), we focus on training a function \(f_{k+1}\) to differentiate between the diffusion trajectory \(_{0:T}\) generated by the diffusion model parameterized by \(p_{}(_{0:T}|)\), and the diffusion process \(p_{}(_{0:T}|)\) from the data. Specifically, the training of \(f_{k+1}\) involves minimizing a generalized Integral Probability Metric (IPM) (Muller, 1997):

\[f_{k+1}=*{argmin}_{f_{k}} f(,_{0:T})-f(,_{0:T}^{}) .\] (4.1)

Here, the expectation is taken over the distributions \( q(),_{0:T} p_{}(|)\), and \(_{0:T}^{} p_{_{k}}(|)\). \(_{k}\) denotes the class of functions under consideration and \(()\) is a monotonically decreasing and convex function that helps stabilize training. The value of \(f\) reflects the degree of belief that the diffusion process \(_{0:T}\) given context \(\) originates from the target diffusion process \(p_{}(_{0:T}|)\) rather than the diffusion model \(p_{}(_{0:T}|)\). We name \(f\) the test function.

### Receiving the Test Function

The opponent player wants to maximize the expected value \(_{ q(),_{0:T} p(|)} [f_{k+1}(,)]\). In addition, to prevent excessive deviation of \(p_{_{k+1}}\) from \(p_{_{k}}\) and stabilize the self-play fine-tuning, we incorporate a Kullback-Leibler (KL) regularization term. Putting these together gives rise to the following optimization problem:

\[*{argmax}_{p}_{ q(),_{0:T } p(|)}[f_{k+1}(,_{0:T})]- _{ q()}p(|)||p_{ _{k}}(|),\] (4.2)

where \(>0\) is the regularization parameter. Notably, (4.2) has a closed-form solution \((|)\):

\[(_{0:T}|) p_{_{k}}( _{0:T}|)^{-1}f_{k+1}(,_{0:T}).\] (4.3)To ensure that \(\) lies in the diffusion process space \(\{p_{}(|)|\}\), we utilize the following test function class (Chen et al., 2024):

\[_{k}=}( _{1:T}|)}{p_{_{k}}(_{1:T}|)} }.\] (4.4)

Given the choice of \(_{k}\) in (4.4), optimizing (4.1) gives \(f_{k+1}\) parameterized by \(_{k+1}\) in the following form:

\[f_{k+1}(,_{0:T})=_{k+1}}(_{0:T}|)}{p_{_{k}}(_{0:T}|)}.\] (4.5)

Substituting (4.5) into (4.3) yields \((_{0:T}|)=p_{_{k+1}}(_{0 :T}|)\). In other words, \(_{k+1}\) learned from (4.1) is exactly the diffusion parameter for the ideal choice of opponent.

### Decomposed Training Objective

The above two steps provide a training scheme depending on the full trajectory of \(_{0:T}\). Specifically, substituting (4.4) into (4.1) yields the update rule \(_{k+1}=*{argmin}_{}L_{ }(,_{k})\), where \(L_{}\) is defined as:

\[L_{}= {p_{}(_{0:T}|)}{p_{_{k}}( _{0:T}|)}-}(^{}_{0:T} |)}{p_{_{k}}(^{}_{0:T}|)} .\] (4.6)

Here the expectation is taken over the distributions \( q(),_{0:T} p_{}(|),^{}_{0:T} p_{_{k}}(|)\). To formulate a computationally feasible objective, we decompose \( p_{}(_{0:T}|)\) using the backward process of diffusion models. Substituting (3.2) into (4.6), we have that

\[ p_{}(_{0:T}|) =_{t=1}^{T}p_{}(_{t-1}| _{t},) p_{}(_{T}|) \] \[= p_{}(_{T}|)+_{t=1}^{T} p_{}(_{t-1}|_{t},)\] \[=-_{t=1}^{T}^{2}} _{t-1}-_{}(_{t},,t)_ {2}^{2}.\] (4.7)

where the last equality holds since \(p_{}(_{t-1}|_{t},)\) is a Gaussian distribution \(_{}(_{t},,t),_{t }^{2}\) according to (3.2), and \(p_{}(_{T}|)\) is approximately a Gaussian independent of \(\). By substituting (4.7) into (4.6) and introducing a reparameterization \(_{t}^{2}= T/(2_{t})\), where \(_{t}\) is a fixed positive value, we obtain

\[L_{}(,_{k}) =-_{t=1}^{T}}{T} _{t-1}-_{}(_{t},,t)_{2}^{2}-_{t-1}-_{_{k}}( _{t},,t)_{2}^{2}\] \[-^{}_{t-1}-_{}( ^{}_{t},,t)_{2}^{2}+^{ }_{t-1}-_{_{k}}(^{}_{t},,t )_{2}^{2}.\] (4.8)

Here the expectation is taken over the distributions \( q(),_{0:T} p_{}(|),^{}_{0:T} p_{_{k}}(|)\). Note that by considering the main player (reward function) across the full trajectory (3.2), rather than focusing solely on the final state as in Fan et al. (2023); Black et al. (2023); Wallace et al. (2023), we are able to formulate an exact objective function up to Equation (4.8). The detailed algorithm is presented in Algorithm 1. (4.8) naturally provides an objective function for DDIM with \(_{t}>0\), where \(_{t}\) controls the determinism of the reverse process (3.2). (4.8) remains valid for deterministic generation processes as \(_{t} 0\).

### Approximate Training Objective

While (4.8) is the exact ELBO, optimizing it requires storing all intermediate images during the reverse sampling. When the trajectory length \(T\) is large, it would require an impractical amount of GPU memory when the loss is summed over \(T\). Additionally, the required samples from a reverse process are not readily accessible. To address these limitations, we propose an approximate objective function. By applying Jensen's inequality and the convexity of the loss function \(\), we can give an upper bound of (4.8) and thus move the average over \(t\) outside the loss function \(\):

\[L_{}^{}(,_{k}) =-_{t}_{t-1}-_{}(_{t},,t)_{2}^{2}- _{t-1}-_{_{k}}(_{t},,t)_{2 }^{2}\]\[-\|_{t-1}^{}-_{}( _{t}^{},,t)\|_{2}^{2}+\|_{t-1}^{ }-_{_{k}}(_{t}^{}, ,t)\|_{2}^{2}]),\] (4.9)

where the expectation is taken over the distributions \( q(),(_{t-1},_{t}) p_{}(_{t-1},_{t}|),(_{t-1}^{}, _{t}^{}) p_{_{k}}(_{t-1}^{ },_{t}^{}|)\), \(t\{1,,T\}\). This approximation is directly motivated by the nature of diffusion models, which inherently decouple operations on a per-time-step basis. We provide theoretical justifications for our approximation method in the following sections.

The following lemma shows that \(L^{}_{}\) is an upper bound of \(L_{}\).

**Lemma 4.1**.: Fix \(_{k}\) which serves as the starting point of Algorithm 1 for iteration \(k+1\). It holds that \(L_{}(,_{k}) L^{ }_{}(,_{k})\) for all \(\).

\(L^{}_{}\) eliminates the need to store all intermediate steps, as it only involves two consecutive sampling steps \(t-1\) and \(t\). Since the reverse process \(p_{}(_{1:T}^{}|_{0}^{}, )\) approximates the forward process \(q(_{1:T}^{}|_{0}^{})\), we use the per step forward process \(q(_{t-1}^{},_{t}^{}|_{0}^{})\) to approximate \(p_{_{k}}(_{t-1}^{},_{t}^{}| _{0}^{},)\). We can further approximate \(p_{_{k}}(_{t-1}^{},_{t}^{}| )= p_{_{k}}(_{t-1}^{}, _{t}^{}|_{0}^{},)p_{_{k}}(_{0}^{}|)d_{0}^{}\) with \( q(_{t-1}^{},_{t}^{}|_{0}^{ })p_{_{k}}(_{0}^{}|)d _{0}^{}\). Substituting the corresponding terms in (4.9) with the above approximation allows us to only compute the expectation of (4.9) over the distribution \( q(),(_{t-1},_{t}) p_{}(_{t-1},_{t}|),(_{t-1}^{}, _{t}^{}) p_{_{k}}(_{0}^ {}|)q(_{t-1}^{},_{t}^{}| _{0}^{})d_{0}^{}\), \(t\{1,,T\}\). Furthermore, by incorporating the parameterization of \(_{}\) into (4.8) and (4.9), we can express (4.8) and (4.9) in terms of \(_{}(_{t},,t)\). Detailed derivations of (4.8) and (4.9) are provided in Appendix C.

## 5 Main Theory

In this section, we provide a theoretical analysis of Algorithm 1. Section 4 introduces two distinct objective functions, as defined in (4.8) and (4.9), both of which use the loss function \(\). Since (4.8) is an exact objective function, its analysis closely follows the framework established by Chen et al. (2024). Consequently, we instead focus on the approximate objective function \(L^{}_{}\) defined in (4.9), which is more efficient to optimize and is the algorithm we use in our experiments. However, its behavior is more difficult to analyze. We begin with a formal assumption regarding the loss function \(\) as follows.

**Assumption 5.1**.: The function \((t):\) in (4.9) is monotonically decreasing, i.e., \( t,^{}(t) 0\) and satisfies \(^{}(0)<0\). In addition, \((t)\) is a convex function.

Assumption 5.1 can be satisfied by various commonly used loss functions in machine learning. This includes the correlation loss \((t)=1-t\), the hinge loss \((t)=(0,1-t)\), and the logistic loss \((t)=(1+(-t))\). In our experiments, we are using the logistic loss.

To understand the behavior of SPIN-Diffusion, let us first analyze the gradient of the objective function (4.9),

\[ L^{}_{}=_{t}^{})}_{}}_{t-1}-_{ }(_{t},,t)_{2}^{2}}_{}-}_{t-1}^{ }-_{}(_{t}^{},,t)_{2}^{2}}_{},\] (5.1)

where the expectation is taken over the distributions \( q(),(_{t-1},_{t}) p_{}(_{t-1},_{t}|),(_{t-1}^{ },_{t}^{}) p_{_{k}}(_{t-1}^{}, _{t}^{}|)\). (D.3) can be divided into three parts:

* **Reweighting:**\(^{}()\) in the "Reweighting" term is negative and increasing because \(()\) is monotonically decreasing and convex according to Assumption 5.1. Therefore, \(-_{t}_{t}^{}=-_{t}^{}-_{t} \|_{t-1}-_{}(_{t}, ,t)\|_{2}^{2}-+\|_{t-1}^{}-_{ _{k}}(_{t}^{},,t)\|_{2}^{2} \) is always non-negative. Furthermore, \(-_{t}_{t}^{}\) decreases as the argument inside \(()\) increases.
* **Matching:** The "Matching" term matches \(_{}(_{t},,t)\) to \(_{t-1}\) coming from pairs \((_{t-1},_{t})\), that are sampled from the target distribution. This increases the likelihood of \((_{t-1},_{t}) p_{}(_{t-1}, _{t})\) following the generative process (3.2).
* **Pushing:** Contrary to the "Matching" term, the "Pushing" term pushes \(_{}(_{t}^{},,t)\) away from \(_{t-1}^{}\) coming from pairs \((_{t-1}^{},_{t}^{})\) drawn from the synthetic distribution \(p_{_{k}}(_{t-1}^{},_{t}^{})\). Therefore, the "Pushing" term decreases the likelihood of these samples following the process in the generative process (3.2).

The "Matching" term aligns conceptually with the \(L_{}\) in SFT, as both aim to maximize the likelihood that the target trajectory \(_{0:T}\) follows the generative process described in (3.2). The following theorem shows a formal connection, which is pivotal for understanding the optimization dynamics of our method.

**Theorem 5.2**.: Under Assumption 5.1, if \(_{k}\) is not the global optima of \(L_{}\) in (3.3), there exists an appropriately chosen \(_{t}\), such that \(_{k}\) is not the global minima of (4.9) and thus \(_{k+1}_{k}\).

Theorem 5.2 suggests that the optimization process stops only when \(\) reaches global optimality of \(L_{}\). Consequently, the optimal diffusion model \(^{*}\) found by Algorithm 1 cannot be further improved using \(L_{}\). This theoretically supports that SFT with (3.3) cannot improve over SPIN-Diffusion. It is also worth noting that Theorem 5.2 does not assert that every global minimum of \(L_{}\) meets the convergence criterion (i.e., \(_{k+1}=_{k}\)), particularly due to the influence of the "Pushing" term in (D.3). The following theorem provides additional insight into the conditions under which Algorithm 1 converges.

**Theorem 5.3**.: Under Assumption 5.1, if \(p_{_{k}}(|)=p_{}(|)\), then \(_{k}\) is the global minimum of (4.9) for any \( 0\).

Theorem 5.3 shows that Algorithm 1 converges when \(p_{}(|)=p_{}(|)\), indicating the efficacy of SPIN-Diffusion in aligning with the target data distribution. In addition, while Theorems 5.2 and 5.3 are directly applicable to (4.9), the analogous conclusion can be drawn for (4.8) as well (see Appendix D for a detailed discussion).

## 6 Experiments

In this section, we conduct extensive experiments to demonstrate the effectiveness of SPIN-Diffusion. Our results show that SPIN-Diffusion outperforms other baseline fine-tuning methods including SFT and Diffusion-DPO.

### Experiment Setup

**Models, Datasets and Baselines.** We use the stable diffusion v1.5 (SD-1.5) (Rombach et al., 2022) as our base model. While adopting the original network structure, we use its Huggingface pretrained version2, which is trained on LAION-5B (Schuhmann et al., 2022) dataset, a text-image pair dataset containing approximately 5.85 billion CLIP-filtered image-text pairs. We use the Pick-a-Pic dataset (Kirstain et al., 2023) for fine-tuning. Pick-a-Pic is a dataset with pairs of images generated by Dreamlike3 (a fine-tuned version of SD-1.5) and SDXL-beta (Podell et al., 2023), where each pair corresponds to a human preference label. We also train SD-1.5 with SFT and Diffusion-DPO (Wallace et al., 2023) as the baselines. For SFT, we train the model to fit the winner images in the Pick-a-Pic (Kirstain et al., 2023) dataset. In addition to the Diffusion-DPO checkpoint provided by Wallace et al. (2023)4 (denoted by Diffusion-DPO), we also fine-tune an SD-1.5 using Diffusion-DPO and denote it by "Diffusion-DPO (reproduced)".

**Evaluation.** We use the Pick-a-Pic test set, PartiPrompts (Yu et al., 2022) and HPSv2 (Wu et al., 2023) as our evaluation benchmarks. We defer the detailed introduction and results of PartiPrompts and HPSv2 to Appendix A.3. Our evaluation rubric contains two dimensions, human preference alignment and visual appeal. For visual appeal assessment, we follow Wallace et al. (2023); Lee et al. (2024) and use Aesthetic score. For human-preference alignment, we employ reward models including PickScore (Kirstain et al., 2023), ImageReward (Xu et al., 2023) and HPS (Wu et al., 2023). All these reward models are trained according to the Bradley-Terry-Luce (Bradley and Terry, 1952) model on different human-labeled preference datasets. For each prompt, we generate \(5\) images and choose the image with highest average score over those four metrics (best out of \(5\)). We report the average of HPS, PickScore, ImageReward and Aesthetic scores over all the prompts. To investigate how the scores align with human preference, we further compare the accuracy of these reward models on a small portion of the Pick-a-Pic training set. It is worth noticing that PickScore is most aligned with human preference according to the experiments conducted by Kirstain et al. (2023).

### Main Results

In this subsection, we provide empirical evidence demonstrating the superiority of our SPIN-Diffusion model over previous fine-tuning baselines based on the network structure of SD1.5.

**Comparison in Terms of Average Score.** The results are presented in Table 1. While all fine-tuning algorithms yield improvements over the SD1.5 baseline, at iteration 1, our SPIN-Diffusion not only exceeds the original DPO checkpoint but also surpasses SFT in both Aesthetic score and PickScore.

At iteration 2, the superiority of our model becomes even more pronounced, particularly in terms of Aesthetic score, where it consistently outperforms other fine-tuning methods, indicating a dominant performance in visual quality. Furthermore, at iteration 3, our model's HPSv2 score surpasses all competing models, highlighting the effectiveness and robustness of the SPIN-Diffusion approach. Specifically, on the Pick-a-Pic dataset, while SFT achieves a PickScore of \(21.45\), and Diffusion-DPO has a slightly higher score of \(21.45\), SPIN-Diffusion achieves \(22.00\) at iteration \(3\), showing a total improvement of \(0.80\) over the original SD1.5 checkpoint. Furthermore, SPIN-Diffusion demonstrates exceptional performance in terms of Aesthetic score, achieving \(6.25\) at iteration \(3\), which significantly surpasses \(5.86\) achieved by Diffusion-DPO and \(5.77\) by SD1.5.

**Comparison in Terms of Winning Rate.** We further validate our claim by a comparative analysis of the winning rate for our trained model. The winning rate is defined as the proportion of prompts for which a model's generated images exceed the quality of those produced by another model. This experiment is conducted on the Pick-a-Pic test set. We show both the winning rate over SD-1.5, as well as the winning rate over Diffusion-DPO (reproduced) in Figure 1. The complete results are detailed in Tables 3 and 4 in Appendix A.2. We observe that throughout fine-tuning, our SPIN-Diffusion tremendously beats the baselines. When competing with SD-1.5, SPIN-Diffusion achieves an impressive winning rate of 90.0% at iteration 2, which further increases to 91.6% at iteration 3. This winning rate surpasses 73.2% achieved by SFT and 84.8% achieved by Diffusion-DPO (reproduced). When competing with Diffusion-DPO (reproduced), at iteration 3, SPIN-Diffusion

   Model & HPS \(\) & Aesthetic \(\) & ImageReward \(\) & PickScore \(\) & Average \(\) \\  SD-1.5 & 0.2699 & 5.7691 & 0.8159 & 21.1983 & 7.0133 \\ SFT (reproduced) & 0.2749 & 5.9451 & 1.1051 & 21.4542 & 7.1948 \\ Diffusion-DPO & 0.2724 & 5.8635 & 0.9625 & 21.5919 & 7.1726 \\ Diffusion-DPO (reproduced) & 0.2753 & 5.8918 & 1.0495 & 21.8866 & 7.2758 \\  SPIN-Diffusion-Iter1 & 0.2728 & 6.1206 & 1.0131 & 21.6651 & 7.2679 \\ SPIN-Diffusion-Iter2 & 0.2751 & 6.2399 & 1.1086 & 21.9567 & 7.3951 \\ SPIN-Diffusion-Iter3 & **0.2759** & **6.2481** & **1.1239** & **22.0024** & **7.4126** \\   

Table 1: The results on the Pick-a-Pic test set. We report the mean of PickScore, HPS, ImageReward and Aesthetic over the whole test set. We also report the average score over the three evaluation metrics. SPIN-Diffusion outperforms all the baselines in terms of four metrics. For this and following tables, we use blue background to indicate our method, **bold** numbers to denote the best and underlined for the second best.

Figure 1: Left: winning rate in percentage of SFT, Diffusion-DPO, Diffusion-DPO (reproduced) and SPIN-Diffusion over SD1.5 checkpoint. Right: winning rate in percentage of SFT, Diffusion-DPO, Diffusion-DPO (reproduced) and SPIN-Diffusion over SD1.5 checkpoint. SPIN-Diffusion shows a much higher winning rate than SFT and Diffusion-DPO tuned models.

achieves a winning rate of 56.2% on HPS, 86.8% on Aesthetic, 62.4% on PickScore, 55.8% on Image Reward, and has an overall winning rate of 70.2%.

### Qualitative Analysis

We illustrate the qualitative performance of our model on three prompts coming from the Pick-a-Pic test dataset. We prompt SD-1.5, SFT, Diffusion-DPO (reproduced), and SPIN-Diffusion at iteration 1 to 3 and present the generated images in Figure 2. Compared to the baseline methods, SPIN-Diffusion demonstrates a notable improvement in image quality, even more apparent than the improvements in scores. This is especially evident in aspects such as aligning, shading, visual appeal, and the intricacy of details within each image. This qualitative assessment underscores the effectiveness of SPIN-Diffusion in producing images that are not only contextually accurate but also visually superior to those generated by other existing models.

## 7 Conclusion

This paper presents SPIN-Diffusion, an innovative fine-tuning approach tailored for diffusion models, particularly effective in scenarios where only a single image is available per text prompt. By employing a self-play mechanism, SPIN-Diffusion iteratively refines the model's performance, converging towards the target data distribution. Theoretical evidence underpins the superiority of SPIN-Diffusion, demonstrating that traditional supervised fine-tuning cannot surpass its stationary point, achievable at the target data distribution. Empirical evaluations highlight SPIN-Diffusion's remarkable success in text-to-image generation tasks, surpassing the state-of-the-art fine-tuning methods even without the need for additional data. This underscores SPIN-Diffusion's potential to revolutionize the practice of diffusion model fine-tuning, leveraging solely demonstration data to achieve unprecedented performance levels.

**Limitations** While our theoretical analysis ensures that \(_{k}\) is the only global optimum of our objective function, it relies on the assumption that the data distribution can be adequately represented by the parameterized family. Additionally, as our methodology is fundamentally a distribution matching algorithm, it cannot, in principle, exceed the performance of the underlying data distribution. Finally, although SPIN-Diffusion is data-efficient, it requires additional sampling overhead. The high sampling cost can be alleviated by software-level upgrades such as larger batch size, and memory-efficient attention backends. On the algorithm level, advanced sampling acceleration techniques also offer promising improvements. These techniques are orthogonal to our efforts in improving the performance of fine-tuning diffusion models, and therefore we decide to explore them as a future work.

Figure 2: We show the images generated by different models. The prompts are “_a very cute boy, looking at audience, silver hair, in his room, wearing hoodie, at daytime, ai language model, 3d art, c4d, blender, pop mart, blind box, clay material, pixar trend, animation lighting, depth of field, ultra detailed_”, “_painting of a castle in the distance_” and “_red and green eagle_”. The models are: SD-1.5, SFT, Diffusion-DPO (reproduced), SPIN-Diffusion-Iter1, SPIN-Diffusion-Iter2, SPIN-Diffusion-Iter3 from left to right. SPIN-Diffusion demonstrates a notable improvement in image quality. The quantitative evaluation of the aesthetic score of the above images is in Table 5.