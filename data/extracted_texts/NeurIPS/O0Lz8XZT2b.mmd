# A U-turn on Double Descent: Rethinking Parameter Counting in Statistical Learning

Alicia Curth

University of Cambridge

amc253@cam.ac.uk

&Alan Jeffares

University of Cambridge

aj659@cam.ac.uk

&Mihaela van der Schaar

University of Cambridge

mv472@cam.ac.uk

Equal contribution

###### Abstract

Conventional statistical wisdom established a well-understood relationship between model complexity and prediction error, typically presented as a _U-shaped curve_ reflecting a transition between under- and overfitting regimes. However, motivated by the success of overparametrized neural networks, recent influential work has suggested this theory to be generally incomplete, introducing an additional regime that exhibits a second descent in test error as the parameter count \(p\) grows past sample size \(n\) - a phenomenon dubbed _double descent_. While most attention has naturally been given to the deep-learning setting, double descent was shown to emerge more generally across non-neural models: known cases include _linear regression, trees, and boosting_. In this work, we take a closer look at the evidence surrounding these more classical statistical machine learning methods and challenge the claim that observed cases of double descent truly extend the limits of a traditional U-shaped complexity-generalization curve therein. We show that once careful consideration is given to _what is being plotted_ on the x-axes of their double descent plots, it becomes apparent that there are implicitly multiple, distinct complexity axes along which the parameter count grows. We demonstrate that the second descent appears exactly (and _only_) when and where the transition between these underlying axes occurs, and that its location is thus _not_ inherently tied to the interpolation threshold \(p\!=\!n\). We then gain further insight by adopting a classical nonparametric statistics perspective. We interpret the investigated methods as _smoothers_ and propose a generalized measure for the _effective_ number of parameters they use _on unseen examples_, using which we find that their apparent double descent curves do indeed fold back into more traditional convex shapes - providing a resolution to the ostensible tension between double descent and traditional statistical intuition.

## 1 Introduction

Historically, throughout the statistical learning literature, the relationship between model complexity and prediction error has been well-understood as a careful balancing act between _underfitting_, associated with models of high bias, and _overfitting_, associated with high model variability. This implied tradeoff, with optimal performance achieved between extremes, gives rise to a U-shaped curve, illustrated in the left panel of Fig. 1. It has been a fundamental tenet of learning from data, omnipresent in introductions to statistical learning , and is also practically reflected in numerous classical model selection criteria that explicitly trade off training error with model complexity . Importantly, much of the intuition relating to this U-shaped curve was originally developed in the context of the earlier statistics literature (see e.g. the historical note in ), which focussed on conceptually simple learning methods such as linear regression, splines or nearest neighbor methods  and their expected _in-sample_ prediction error, which fixes inputs and resamples noisy outcomes .

The modern machine learning (ML) literature, conversely, focuses on far more flexible methods with relatively huge parameter counts and considers their generalization to _unseen_ inputs . A similar U-shaped curve was long accepted to also govern the complexity-generalization relationship of such methods - until highly overparametrized models, e.g. neural networks, were recently found to achieve near-zero training error _and_ excellent test set performance . In this light, the seminal paper of Belkin et al. (2019)  sparked a new line of research by arguing for a need to extend on the apparent limitations of classic understanding to account for a _double descent_ in prediction performance as the total number of model parameters (and thus - presumably - model complexity) grows. This is illustrated in the right panel of Fig. 1. Intuitively, it is argued that while the traditional U-curve is appropriate for the regime in which the number of total model parameters \(p\) is smaller than the number of instances \(n\), it no longer holds in the modern, zero train-error, _interpolation regime_ where \(p\!>\!n\) - here, test error experiences a second descent. Further, it was demonstrated that this modern double descent view of model complexity applies not only in deep learning where it was first observed , but also ubiquitously appears across many non-deep learning methods such as trees, boosting and even linear regression .

**Contributions.** In this work, we investigate whether the double descent behavior observed in recent empirical studies of such _non-deep_ ML methods _truly_ disagrees with the traditional notion of a U-shaped tradeoff between model complexity and prediction error. In two parts, we argue that once careful consideration is given to _what is being plotted_ on the axes of these double descent plots, the originally counter-intuitive peaking behavior can be comprehensively explained under existing paradigms:

\(\)**Part 1: Revisiting existing experimental evidence.** We show that in the experimental evidence for non-deep double descent - using trees, boosting, and linear regressions - there is implicitly _more than one complexity axis_ along which the parameter count grows. Conceptually, as illustrated in Fig. 1, we demonstrate that this empirical evidence for double descent can thus be comprehensively explained as a consequence of an implicit _unfolding_ of a 3D plot with two orthogonal complexity axes (that both individually display a classical convex curve) into a single 2D-curve. We also highlight that the location of the second descent is thus _not_ inherently tied to the interpolation threshold. While this is straightforward to show for the tree- and boosting examples (Sec. 2), reconstructing the underlying axes in the linear regression example is non-trivial (and involves understanding the connections between min-norm solutions and _unsupervised dimensionality reduction_). Our analysis in this case (Sec. 3) could thus be of independent interest as a simple new interpretation of double descent in linear regression.

\(\)**Part 2: Rethinking parameter counting through a classical statistics lens.** We then note that all methods considered in Part 1 can be interpreted as _smoothers_ (Sec. 4), which are usually compared in terms of a measure of the _effective_ (instead of raw) number of parameters they use when issuing predictions . As existing measures were derived with _in-sample_ prediction in mind, we propose a generalized effective parameter measure \(p_{}^{0}\) that allows to consider arbitrary sets of inputs \(_{0}\). Using \(p_{}^{0}\) to measure complexity, we then indeed discover that the apparent double descent curves fold back into more traditional U-shapes - because \(p_{}^{0}\) is _not actually increasing_ in the interpolation regime. Further, we find that, in the interpolation regime, trained models tend to use a different number of effective parameters when issuing predictions on unseen test inputs than on previously observed training inputs. We also note that, while such interpolating models can generalize well to unseen inputs, overparametrization _cannot_ improve their performance in terms of the in-sample prediction error originally of interest in statistics - providing a new reason for the historical absence of double descent curves. Finally, we discuss practical implications for e.g. model comparison.

Figure 1: **A 3D generalization plot with two complexity axes unfolding into double descent.** A generalization plot with two complexity axes, each exhibiting a convex curve (left). By increasing raw parameters along different axes sequentially, a double descent effect appears to emerge along their composite axis (right).

## Part 1: Revisiting the evidence for double descent in non-deep ML models

**Experimental setup.** We center our study around the non-neural experiments in  as it is _the_ seminal paper on double descent and provides the broadest account of non-deep learning methods that exhibit double descent. Through multiple empirical studies,  demonstrate that double descent arises in trees, boosting and linear regression. Below, we _re-analyze_ these experiments and highlight that in each study, as we transition from the classical U-shaped regime into the subsequent second descent regime, _something else_ implicitly changes in the model or training definition, fundamentally changing the class of models under consideration _exactly at the transition threshold_ between the observed regimes - which is precisely the cause of the second descent phenomenon. In Sec. 2, we begin by investigating the tree and boosting experiments, where this is straightforward to show. In Sec. 3, we then investigate the linear regression example, where decomposing the underlying mechanisms is non-trivial. Throughout, we closely follow 's experimental setup: they use standard benchmark datasets and train all ML methods by minimizing the _squared_ loss1. Similarly to their work, we focus on results using MNIST (with \(n_{train}=10000\)) in the main text. We present additional results, including other datasets, and further discussion of the experimental setup in Appendix E.

## 2 Warm-up: Observations of double descent in trees and boosting

### Understanding double descent in trees

In the left panel of Fig. 2, we replicate the experiment in 's Fig. 4, demonstrating double descent in trees. In their experiment, the number of model parameters is initially controlled through the maximum allowed number of terminal leaf nodes \(P^{leaf}\). However, \(P^{leaf}\) for a single tree cannot be increased past \(n\) (which is when every leaf contains only one instance), and often \((P^{leaf})\!<\!n\) whenever larger leaves are already pure. Therefore, when \(P^{leaf}\) reaches its maximum, in order to further increase the raw number of parameters, it is necessary to change _how_ further parameters are added to the model.  thus transition to showing how test error evolves as one averages over an increasing number \(P^{ens}\) of different trees grown to full depth, where each tree will generally be distinct due to the randomness in features considered for each split. As one switches between plotting increasing \(P^{leaf}\) and \(P^{ens}\) on the x-axis, one is thus conceptually no longer increasing the number of parameters _within the same model class_: in fact, when \(P^{ens}\!>\!1\) one is no longer actually considering a tree, but instead _an ensemble_ of trees (i.e. a random forest  without bootstrapping).

In Fig. 2, we illustrate this empirically: in the center plot we show that, on the one hand, for fixed \(P^{ens}\), error exhibits a classical convex U- (or L-)shape in tree-depth \(P^{leaf}\). On the other hand, for fixed \(P^{leaf}\) in the right plot, error also exhibits an L-shape in the number of trees \(P^{ens}\), i.e. a convex shape without any ascent - which is in line with the known empirical observation that adding trees to a random forest generally does not hurt [15, Ch. 15.3.4]. Thus, only by transitioning from increasing \(P^{leaf}\) (with \(P^{ens}=1\)) to increasing \(P^{ens}\) (with \(P^{leaf}\!=\!n\)) - i.e. by connecting the two solid curves across the middle and the right plot - do we obtain the doubledescent curve in the left plot of Fig. 2. In Fig. 3, we show that we could therefore _arbitrarily move or even remove_ the first peak by changing _when_ we switch from increasing parameters through \(P^{leaf}\) to \(P^{ens}\). Finally, we note that the interpolation threshold \(p=n\) plays a special role only on the \(P^{leaf}\) axis where it determines maximal depth, while parameters on the \(P^{ens}\) axis can be increased indefinitely - further suggesting that parameter counts alone are not always meaningful2.

### Understanding double descent in gradient boosting

Another experiment is considered in Appendix S5 of  seeking to provide evidence for the emergence of double descent in gradient boosting. Recall that in gradient boosting, new base-learners (trees) are trained _sequentially_, accounting for current residuals by performing multiple _boosting rounds_ which improve upon predictions of previous trees. In their experiments,  use trees with 10 leaves as base learners and a high learning rate of \(=0.85\) to encourage quick interpolation. The raw number of parameters is controlled by first increasing the number of boosting rounds \(P^{boost}\) until the squared training error reaches approximately zero, after which \(P^{boost}\) is fixed and ensembling of \(P^{ens}\) independent models is used to further increase the raw parameter count.

In Fig. 4, we first replicate the original experiment and then again provide experiments varying each of \(P^{boost}\) and \(P^{ens}\) separately. Our findings parallel those above for trees: for a fixed number of ensemble members \(P^{ens}\), test error has a U- or L-shape in the number of boosting rounds \(P^{boost}\) and an L-shape in \(P^{ens}\) for fixed boosting rounds \(P^{boost}\). As a consequence, a double descent shape occurs _only_ when and where we switch from one method of increasing complexity to another.

## 3 Deep dive: Understanding double descent in linear regression

We are now ready to consider Fig. 2 of , which provides experiments demonstrating double descent in the case of linear regression. Recall that linear regression with \(^{n}\) and \(^{n d}\) estimates the coefficients in a model \(=\), thus the number of raw model parameters equals the number of input dimensions (i.e. the dimension \(d\) of the regression coefficient vector \(\)) by design. Therefore, in order to flexibly control the number of model parameters,  apply basis expansions using random Fourier features (RFF). Specifically, given input \(^{d}\), the number of raw model parameters \(P^{}\) is controlled by randomly generating features \(_{p}()=(^{-1^{T}})\) for all \(p P^{}\), where each \(_{p}}}{{}}(,}_{d})\). For any given number of features \(P^{}\), these are stacked to give a \(n P^{}\) dimensional random design matrix \(\), which is then used to solve the regression problem \(=\) by least squares. For \(P^{} n\), this has a unique solution \((=(^{T})^{-1}^{T} )\) while for \(P^{}>n\) the problem becomes undetermined (i.e. there are infinite solutions) which is why  rely on a specific choice: the min-norm solution \((}=^{T}(^{T})^{ -1})\).

Unlike the experiments discussed in Sec. 2, there appears to be only one obvious mechanism for increasing raw parameters in this case study. Instead, as we show in Sec. 3.1, the change in the used solution at \(P^{}=n\) turns out to be the crucial factor here: we find that the min-norm solution leads to implicit _unsupervised dimensionality reduction_, resulting in two distinct mechanisms for increasing the total parameter count in linear regression. Then, we again demonstrate empirically in Sec. 3.2 that each individual mechanism is indeed associated with a standard generalization curve, such that the combined generalization curve exhibits double descent only because they are applied in succession.

Figure 4: **Decomposing double descent for gradient boosting.** Reproducing ’s boosting experiment (left). Test error by \(P^{boost}\) for fixed \(P^{ens}\) (center). Test error by \(P^{ens}\) for fixed \(P^{boost}\) (right).

### Understanding the connections between min-norm solutions and dimensionality reduction

In this section, we show that, while the min-norm solution finds coefficients \(}\) of _raw_ dimension \(P^{}\), only \(n\) of its dimensions are well-determined - i.e. the true parameter count is not actually increasing in \(P^{}\) once \(P^{}>n\). Conceptually, this is because min-norm solutions project \(\) onto the row-space of \(\), which is \(n-\)dimensional as \(rank()=(P^{},n)\) (when \(\) has full rank). To make the consequence of this more explicit, we can show that the min-norm solution (which has \(P^{}\) raw parameters) can always be represented by using a \(n\)-dimensional coefficient vector applied to a \(n-\)dimensional basis of \(\). In fact, as we formalize in Proposition 1, this becomes most salient when noting that applying the min-norm solution to \(\) is _exactly_ equivalent to a learning algorithm that (i) first constructs a \(n-\)dimensional basis \(_{SVD}\) from the \(n\) right singular vectors of the input matrix computed using the singular value decomposition (SVD) in an _unsupervised pre-processing step_ and (ii) then applies standard (fully determined) least squares using the discovered \(n-\)dimensional basis3.

**Proposition 1**.: _[Min-norm least squares as dimensionality reduction.] For a full rank matrix \(^{n d}\) with \(n<d\) and a vector of targets \(^{n}\), the min-norm least squares solution \(}^{}=\{_{}|| ||_{2}^{2}:\,=\}\) and the least squares solution \(}^{}=\{:\, =\}\) using the matrix of basis vectors \(^{n n}\), constructed using the first \(n\) right singular vectors of \(\), are equivalent; i.e. \(^{T}}^{}=^{T}}^{}\) for all \(^{d}\) and corresponding basis representation \(()\)._

Proof.: Please refer to Appendix B.2. 

Then what is _really_ causing the second descent if not an increasing number of fitted dimensions? While the addition of feature dimensions _does_ correspond to an increase in fitted model parameters while \(P^{}<n\), the performance gains in the \(P^{}>n\) regime are better explained as a linear model of fixed size \(n\) being fit to an _increasingly rich basis constructed in an unsupervised step_. To disentangle the two mechanisms further, we note that the procedure described above, when applied to a centered design matrix4 a special case of principal component (PC) regression , where we select _all_ empirical principal components to form a complete basis of \(\). The more general approach would instead consist of selecting the top \(P^{PC}\) principal components and fitting a linear model to that basis. Varying \(P^{PC}\), the number of used principal components, is thus actually the first mechanism by which the raw parameter count can be altered; this controls the number of parameters being fit in the supervised step. The second, less obvious, mechanism is then the number of _excess features_\(P^{ex}=P^{}-P^{PC}\); this is the number of raw dimensions that only contribute to the creation of a richer basis, which is learned in an unsupervised manner5.

Based on this, we can now provide a new explanation for the emergence of double descent in this case study: due to the use of the min-norm solution, the two uncovered mechanisms are implicitly entangled

Figure 5: **Decomposing double descent for RFF Regression.** Double descent reproduced from  (left) can be decomposed into the standard U-curve of ordinary linear regression with \(P^{PC}\) features (center) and decreasing error achieved by a _fixed capacity_ model with basis improving in \(P^{ex}\) (right).

through \(P^{}=P^{PC}+P^{ex}\), \(P^{PC}\!=\!(n,P^{})\) and \(P^{ex}\!=\!(0,P^{}-n)\). Thus, we have indeed arrived back at a setup that parallels the previous two experiments: when \(P^{}\!\!n\), \(P^{PC}\) increases monotonically while \(P^{ex}\!=\!0\) is constant, while when \(P^{}>n\) we have constant \(P^{PC}=n\) but \(P^{ex}\) increases monotonically. Below, we can now test empirically whether studying the two mechanisms separately indeed leads us back to standard convex curves as before. In particular, we also show that - while a transition between the mechanisms increasing \(P^{PC}\) and \(P^{ex}\) naturally happens at \(P^{}=n\) in the original experiments - it is possible to transition elsewhere across the implied complexity axes, creating other thresholds, and to thus disentangle the double descent phenomenon from \(n\).

### Empirical resolutions to double descent in RFF regression

Mirroring the analyses in Sec. 2, we now investigate the effects of \(P^{PC}\) and \(P^{ex}\) in Fig. 5. In the left plot, we once more replicate 's original experiment, and observe the same apparent trend that double descent emerges as we increase the number of raw parameters \(P^{}=P^{PC}+P^{ex}\) (where the min-norm solution needs to be applied once \(P^{}\!=\!n\)). We then proceed to analyze the effects of varying \(P^{PC}\) and \(P^{ex}\) separately (while holding the other fixed). As before, we observe in the center plot that varying \(P^{PC}\) (determining the actual number of parameters being fit in the regression) for different levels of excess features indeed gives rise to the traditional U-shaped generalization curve. Conversely, in the right plot, we observe that increasing \(P^{ex}\) for a fixed number of \(P^{PC}\) results in an L-shaped generalization curve - indeed providing evidence that the effect of increasing the number of raw parameters past \(n\) in the original experiment can be more accurately explained as a gradual improvement in the quality of a basis to which a _fixed capacity model_ is being fit.

As before, note that if we connect the solid lines in the center and right plots we recover exactly the double descent curve shown in the left plot of Fig. 5. Alternatively, as we demonstrate in Fig. 6(a), fixing \(P^{PC}\) at other values and then starting to increase the total number of parameters through \(P^{ex}\) allows us _to move or remove the first peak arbitrarily_. In Fig. 6(b), we demonstrate that one could even create multiple peaks6 by switching between parameter-increasing mechanisms _more than once_. This highlights that, while the transition from parameter increase through \(P^{PC}\) to \(P^{ex}\) naturally occurs at \(P^{}\!=\!n\) due to the use of the min-norm solution, the second descent is not actually caused by the interpolation threshold \(P^{}\!=\!n\) itself - but rather is due to the implicit change in model at exactly this point. Indeed, comparing the generalization curves in Fig. 6 with their train-error trajectories which we plot in Appendix E.2, it becomes clear that such a second descent can also occur in models that have not yet and will never achieve interpolation of the training data.

## 4 Part 2: Rethinking parameter counting through a classical statistics lens

Thus far, we have highlighted that "not all model parameters are created equal" - i.e. the intuitive notion that not all ways of increasing the number of _raw_ parameters in an ML method have the same

Figure 6: **Disentangling double descent from the interpolation threshold. The location of the peak(s) in RFF regression generalization error is not inherently linked to the point where \(P^{}=n\). Instead, changes in the mechanism for parameter increase determine the appearance of peaks.**

effect. However, as we saw in the linear regression example, it is not always trivial to deconstruct the underlying mechanisms driving performance. Therefore, instead of having to reason about implicit complexity axes on a case-by-case basis for different models, hyperparameters, or inductive biases, we would rather be able to _quantify_ the effect of these factors objectively. In what follows, we highlight that all previously considered methods can be interpreted as _smoothers_ (in the classical statistics sense ). By making this connection in Sec. 4.1, we can exploit the properties of this class of models providing us with measures of their _effective_ number of parameters. After adapting this concept to our setting (Sec. 4.2), we are finally able to _re-calibrate_ the complexity axis of the original double descent experiments, finding that they do indeed fold back into more traditional U-shapes (Sec. 4.3).

### Connections to smoothers

Smoothers are a class of supervised learning methods that summarize the relationship between outcomes \(Y\!\!\!\!^{k}\) and inputs \(X\!\!\!\!^{d}\) by "smoothing" over values of \(Y\) observed in training. More formally, let \(k\!=\!1\) w.l.o.g., and denote by \(^{}\!=\!\{(y_{i},x_{i})\}_{i=1}^{n}\) the training realizations of \((X,Y)\!\!\!\!\!\) and by \(_{}\!=\!(y_{1},,y_{n})^{T}\) the \(n\!\!1\) vector of training outcomes with respective training indices \(_{}\!=\!\{1,,n\}\). Then, for any admissible input \(x_{0}\!\!\), a smoother issues predictions

\[(x_{0})=}(x_{0})_{}=_{i _{}}^{i}(x_{0})y_{i}\] (1)

where \(}(x_{0})=(^{1}(x_{0}),,^{n}(x_{0}))\) is a \(1\!\!n\) vector containing smoother weights for input \(x_{0}\). A smoother is _linear_ if \(}()\) does not depend on \(_{}\). The most well-known examples of smoothers rely on weighted (moving-) averages, which includes k-nearest neighbor (kNN) methods and kernel smoothers as special cases. (Local) linear regression and basis-expanded linear regressions, including splines, are other popular examples of linear smoothers (see e.g. [12, Ch. 2-3]).

In Appendix C, we show that all methods studied in Part 1 can be interpreted as smoothers, and derive \(}()\) for each method. To provide some intuition, note that linear regression is a simple textbook example of a linear smoother , where \(}()\) is constructed from the so-called projection (or hat) matrix . Further, trees - which issue predictions by averaging training outcomes within leaves - are sometimes interpreted as _adaptive nearest neighbor methods_[12, Ch. 15.4.3] with _learned_ (i.e. non-linear) weights, and as a corollary, boosted trees and sums of either admit similar interpretations.

### A generalized measure of the _effective_ number of parameters used by a smoother

The _effective number of parameters_\(p_{e}\) of a smoother was introduced to provide a measure of model complexity which can account for a broad class of models as well as different levels of model regularization (see e.g. [12, Ch. 3.5], [12, Ch. 7.6]). This generalized the approach of simply counting raw parameters - which is not always possible or appropriate - to measure model complexity. This concept is _calibrated_ towards linear regression so that, as we might desire, effective and raw parameter numbers are equal in the case of ordinary linear regression with \(p<n\). In this section, we adapt the _variance based_ effective parameter definition discussed in [12, Ch. 3.5]. Because for fixed \(}()\) and outcomes generated with homoskedastic variance \(^{2}\) we have \(Var((x_{0}))=||}(x_{0})||^{2}^{2}\), this definition uses that \(_{i_{}}Var((x_{i}))=}{n}_{i_{}}||}(x_{i})|| ^{2}=}{n}p\) for linear regression: one can define \(p_{e}\!=\!_{i_{}}||}(x_{i})||^{2}\) and thus have \(p_{e}\!=\!p\) for ordinary linear regression with \(n\!<\!p\). As we discuss further in Appendix D, other variations of such effective parameter count definitions can also be found in the literature. However, this choice is particularly appropriate for our purposes as it has the unique characteristic that it can easily be _adapted to arbitrary input points_ - a key distinction we will motivate next.

Historically, the smoothing literature has primarily focused on prediction in the classical fixed design setup where expected in-sample prediction error on the _training inputs_\(x_{i}\), with only newly sampled targets \(y^{}_{i}\), was considered the main quantity of interest . The modern ML literature, on the other hand, largely focuses its evaluations on out-of-sample prediction error in which we are interested in model performance, or _generalization_, on both unseen targets _and_ unseen inputs (see e.g. [1, Ch. 5.2]; [12, Ch. 4.1]). To make effective parameters fit for modern purposes, it is therefore necessary to adapt \(p_{e}\) to measure the level of smoothing applied _conditional on a given input_, thus distinguishing between training and testing inputs. As \(||}(x_{0})||^{2}\) can be computed for _any_ input \(x_{0}\), this is straightforward and can be done by replacing \(_{}\) in the definition of \(p_{e}\) by any other set of inputs indexed by \(_{0}\). Note that the scale of \(p_{e}\) would then depend on \(|_{0}|\) due to the summation,

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

ML methods:  use Rademacher complexity (RC) in their study of random forests but find that RC cannot explain their generalization because forests do not have lower RC than individual trees (unlike the behavior of our \(p_{}^{}\) in this case).  use 's Bayesian interpretation of effective parameters based on the Hessian of the training loss when studying neural networks. While this proxy was originally motivated in a linear regression setting, it _cannot_ explain double descent in linear regression because, as discussed further in Appendix D, it does not decrease therein once \(p>n\). Finally,  compute effective ridge penalties implied by min-norm linear regression, and  consider minimum description length principles to measure complexity in ridge regression. Relative to this literature, our work differs not only in terms of the ML methods we can consider (in particular, we are uniquely able to explain the tree- and boosting experiments through our lens), but also in the insights we provide e.g. we distinctively propose to distinguish between effective parameters used on train- versus test-examples, which we showed to be crucial to explaining the double descent phenomenon in Sec. 4.3.

## 6 Conclusion and Discussion

**Conclusion:**_A Resolution to the ostensible tension between non-deep double descent and statistical intuition._ We demonstrated that existing experimental evidence for double descent in trees, boosting and linear regression does not contradict the traditional notion of a U-shaped complexity-generalization curve: to the contrary, we showed that in all three cases, there are actually two _independent_ underlying complexity axes that each exhibit a standard convex shape, and that the observed double descent phenomenon is a direct consequence of transitioning between these two distinct mechanisms of increasing the total number of model parameters. Furthermore, we highlighted that when we plot a measure of the _effective_, _test-time,_ parameter count (instead of _raw_ parameters) on their x-axes, the apparent double descent curves indeed fold back into more traditional U-shapes.

**What about _deep_ double descent?** In this work, we intentionally limited ourselves to the study of _non-deep_ double descent. Whether the approach pursued in this work could provide an alternative path to understanding double descent in the case of deep learning - arguably its most prominent setting - is thus a very natural next question. It may indeed be instructive to investigate whether there also exist multiple implicitly entangled complexity axes in neural networks, and whether this may help to explain double descent in that setting. In particular, one promising approach to bridging this gap could be to combine our insights of Sec. 3 with the known connections between random feature models and two-layer neural networks , and stochastic gradient descent and min-norm solutions . We consider this a fruitful and non-trivial direction for future research.

**What are the practical implications of these findings?** On the one hand, with regards to the specific ML methods under investigation, our empirical results in Sections 2 and 3 imply interesting trade-offs between the need for hyperparameter tuning and raw model size. All methods appear to have one hyperparameter axis to which error can be highly sensitive - \(P^{leaf}\), \(P^{boost}\) and \(P^{PC}\) - while along the other axis, "bigger is better" (or at least, not worse). In fact, it appears that the higher \(P^{ens}\) or \(P^{ex}\), the less sensitive the model becomes to changes along the first axis. This may constitute anecdotal evidence that the respective first axis can be best understood as a train-time bias-reduction axis - it controls how well the _training_ data can be fit (increasing parameters along this axis reduces underfitting - only when set to its maximum can interpolation be achieved). The second axis, conversely, appears to predominantly achieve variance-reduction at _test-time_: it decreases \(||}(x_{0})||\), reducing the impact of noise by smoothing over more training examples when issuing predictions for unseen inputs.

On the other hand, our results in Sec. 4 suggest interesting new avenues for model selection more generally, by highlighting potential routes of redemption for classical criteria trading off in-sample performance and parameter counts (e.g. ) - which have been largely abandoned in ML in favor of selection strategies evaluating held-out prediction error . While criteria based on _raw_ parameter counts may be outdated in the modern ML regime, selection criteria based on _effective_ parameter counts \(p_{}^{}\) used on a test-set could provide an interesting new alternative, with the advantage of not requiring access to labels on held-out data, unlike error-based methods. In Appendix E.5, we provide anecdotal evidence that when choosing between models with different hyperparameter settings that all achieve _zero training error_, considering each model's \(p_{}^{}\) could be used to identify a good choice in terms of generalization performance: we illustrate this for the case of gradient boosting where we use \(p_{}^{}\) to choose additional hyperparameters. Investigating such approaches to model selection more extensively could be another promising avenue for future work.