# The Surprising Effectiveness of SP Voting with Partial Preferences

Hadi Hosseini

College of Information Sciences and Technology

Penn State University, USA

hadi@psu.edu &Debmalya Mandal

Department of Computer Science

University of Warwick, UK

Debmalya.Mandal@warwick.ac.uk

Amrit Puhan

College of Information Sciences and Technology

Penn State University, USA

avp6267@psu.edu

###### Abstract

We consider the problem of recovering the ground truth ordering (ranking, top-\(k\), or others) over a large number of alternatives. The wisdom of crowd is a heuristic approach based on Condorcet's Jury theorem to address this problem through collective opinions. This approach fails to recover the ground truth when the majority of the crowd is misinformed. The _surprisingly popular_ (SP) algorithm  is an alternative approach that is able to recover the ground truth even when experts are in minority. The SP algorithm requires the voters to predict other voters' report in the form of a full probability distribution over all rankings of alternatives. However, when the number of alternatives, \(m\), is large, eliciting the prediction report or even the vote over \(m\) alternatives might be too costly. In this paper, we design a scalable alternative of the SP algorithm which only requires eliciting partial preferences from the voters, and propose new variants of the SP algorithm. In particular, we propose two versions--_Aggregated-SP_ and _Partial-SP_--that ask voters to report vote and prediction on a subset of size \(k\) (\( m\)) in terms of top alternative, partial rank, or an approval set. Through a large-scale crowdsourcing experiment on MTurk, we show that both of our approaches outperform conventional preference aggregation algorithms for the recovery of ground truth rankings, when measured in terms of Kendall-Tau distance and Spearman's \(\). We further analyze the collected data and demonstrate that voters' behavior in the experiment, including the minority of the experts, and the SP phenomenon, can be correctly simulated by a concentric mixtures of Mallows model. Finally, we provide theoretical bounds on the sample complexity of SP algorithms with partial rankings to demonstrate the theoretical guarantees of the proposed methods.

## 1 Introduction

The wisdom of the crowds is a systematic approach to statistically combine the opinions of a diverse group of (non-expert) individuals to achieve a final collective truth. It dates back to Sir Francis Galton's observation--based on Aristotle's hypothesis--that the point estimation of a continuous value using the noisy individual opinions can recover its true value with high accuracy . In the modern era, the wisdom of the crowd has been the foundation of legal, political, and social systems with the premise that one can recover the truth by collecting the opinion of a large number of diverse individuals (e.g. trial by jury, election polling, and Q&A platforms such as Reddit/Quora).

The formal arguments of this phenomenon--rooted in social choice theory--is provided by _Condorcet's Jury Theorem_, which states that under the condition of independent opinions and each individual having more than a \(50\%\) chance of selecting the correct answer, the probability of the majority decision being correct increases as the size of the crowd increases. However, this approach may fail when the majority of the crowd are misinformed (less than 50% chance of selecting the correct answer)  or are systematically biased . In other words, when the experts are in minority, simply aggregating individuals' opinions (regardless of the aggregation method) cannot recover the truth.

To overcome this challenge, Prelec et al.  proposed a simple, yet effective, method called the _Surprisingly Popular_ (SP) algorithm, which is able to uncover the ground truth even when the majority opinion is wrong. The approach works by asking each individual about their opinion (the _vote_) along with an additional _meta-question_ to predict the majority opinion of other individuals (the _prediction_). The surprisingly popular algorithm then selects an answer whose actual frequency in the votes is greater than its average predicted frequency, and it will provably recover the correct answer with probability \(1\), as the number of individuals grows in the limit, even when experts are in minority.

While the SP algorithm is effective in estimating a continuous value (e.g. the value of a painting) or a binary vote (e.g. "Is Sao Paulo the capital of Brazil?"), it cannot be directly applied to recover true ordinal rankings over a set of \(m\) alternatives due to the large number of votes (\(m!\)), and more importantly, eliciting predictions over a complete rankings. Hosseini et al.  extended this approach to rankings by proposing an algorithm, called _Surprisingly Popular Voting_, that can accurately recover the ground-truth ranking over multiple alternatives by eliciting a complete ranking as a vote and only a single majority prediction (as opposed to full probability distributions over \(m!\) rankings).1 Despite its success in finding the ground-truth ranking over a small number of alternatives, it remains unclear how to adapt it to settings with large number of alternatives where only partial preferences (e.g. pairwise comparisons or partial ranks) can be elicited. Thus, it raises the following questions:

_How can we design scalable algorithms based on the surprisingly popular method that recovers the ground truth only by eliciting partial preferences from voters? What elicitation formats and aggregation algorithms are more effective in recovering the full ranking over all alternatives?_

**Our Contributions.** We focus on developing methods, based on the surprisingly popular approach, that _only_ elicit partial vote and prediction information to find the full ranking. Given a set of \(m\) alternatives, we ask individuals to provide their rank-ordered vote and predictions on a subset of size \(k\) (\( m\)) of alternatives. Informally, we ask them to identify the most preferred alternative among the \(k\) choices (Top), select the \(t<k\) most preferred alternatives with no order (Approval(\(t\))), or provide a rank-ordered list of all \(k\) alternatives (Rank). The precise formulation is provided in Section 2.1.

Given that the SP algorithm  and its extension to rankings  do not generalize to partial preferences with large number of alternatives, we design two novel aggregation methods, namely Partial-SP and Aggregated-SP algorithms. On a high level, these algorithms use a carefully crafted method to select subsets of size \(k m\) for vote and prediction elicitation, and apply the SP method either independently on each subset (Partial-SP) or on the aggregated (potentially partial) votes and ranks (Aggregated-SP).

We conduct a human-subject study with 432 participants recruited from Amazon's Mechanical Turk (MTurk) to empirically evaluate the performance of our SP algorithms using metrics such as the _Kendall-Tau distance_ from the full ground truth ranking and _Spearman's rank correlation_ coefficient. We consider several classical vote aggregation methods (e.g. Borda, Copeland, Maximin, Schulze) as benchmarks--rooted in the computational social choice theory--that operate solely on votes (and not prediction information). Our results show that the SP voting algorithms perform significantly better than the classical methods when the vote and prediction information only contain partial rankings. We also observe that SP voting algorithms are effective even when restricted to approval votes.

Moreover, we demonstrate that voters' behavior in the experiment, including the minority of the experts can be correctly simulated by a concentric mixtures of Mallows model . Finally, we provide theoretical bounds on the sample complexity of the SP algorithms with partial preferences to further demonstrate the theoretical guarantees of the proposed methods. We show that the sample complexity only depends on the size of the subset \(k\) which is significantly smaller than \(m\).

### Related Work

Our work is related to _information elicitation_, and _partial aggregation_ which we discuss briefly.

**Information Elicitation**. Various information elicitation schemes [35; 36; 48; 17] attempt to incentivize voters to reveal useful information, often through the investment of efforts. Our work is primarily related to the _surprisingly popular algorithm_ which is a a novel second-order information based elicitation scheme. This framework has since been used to incentivize truthful behaviour in agents [35; 42; 43], mitigate biases in academic peer review , elicit expert knowledge , and aggregate information . Our study builds upon this literature, specifically addressing the challenges in rank recovery. Originally, the SP algorithm by Prelec et al.  required data on all \(m!\) potential rankings for \(m\) alternatives, a requirement that becomes impractical as \(m\) increases. Hosseini et al.  addressed this by developing a Surprisingly Popular Voting algorithm that leverages pairwise preference data across \(\) alternatives. This approach doesn't scale when \(m\) is large, and our contribution lies in advancing this methodology by proposing a scalable generalization of the Surprisingly Popular Voting method with partial preferences.

**Partial Aggregation**. In situations where it is difficult or not necessary to elicit complete rankings from voters, partial preferences are used. Partial vote aggregation has different solution concepts . Partial preferences can be used to conclude which alternatives are necessary and possible winners based on the preference profiles [26; 14; 47; 2; 3; 49]. The primary goal of partial aggregation methods is to either minimize the amount of information communicated by the voters [12; 45; 40] or to reduce the number of queries that each voter needs to answer [37; 14]. Our work attempts to reduce such communication from the voters by eliciting partial preferences.

More broadly, our work is also related to _information aggregation_, and _probabilistic rank-order models_ which we discuss further in Appendix A.

## 2 Model

In this section, we formally define the model for Surprisingly Popular Voting in the context of partial preferences. Let \(A\) = \(\{a_{1},a_{2},...,a_{m}\}\) denote the set of \(m\) possible alternatives. The set \((A)\) represents all possible complete rankings over the alternatives. Let \((A)\) represent a complete ranking of the \(m\) possible alternatives. We denote the ground truth ranking by \(^{}(A)\); which is assumed to be drawn from a prior \(P()\) over \((A)\). Voter \(i\) observes a ranking \(_{i}\) that is assumed to be a noisy version of the ground truth ranking \(^{}\). We will write \(_{s}(_{i}^{})\) to denote the probability that the voter \(i\) observes her ranking \(_{i}\) given the ground truth \(^{}\).

Given voter \(i\)'s ranking \(_{i}\) and the prior \(P()\), voter \(i\) can compute the posterior distribution over the ground truth using the Bayes rule.

\[_{g}(^{}_{i})=(_{i}^{}) P( ^{})}{_{^{}(A)}_{g}(_{i}^{ }) P(^{})}\] (1)

Using the posterior over the ground truth, voter \(i\) can also compute a distribution over the rankings observed by another voter.

\[_{o}(_{j}_{i})=_{^{}(A)}_{s}(_{ j}^{})_{g}(^{}_{i})\] (2)

The _surprisingly popular algorithm_ asks voters to report their votes, and posterior over others' votes. For each ranking \(^{}\), it then computes the frequency \(f(^{})=_{i}[=^{}]\), and posterior \(g(^{})==^{}\}|}_{i:_{i}= ^{}}_{o}(_{i})\), and finally picks the ranking with highest _prediction normalized votes_.2

\[_{}()=f()_{^{ }})}{g(^{})}\] (3)

As observed by Hosseini et al. , eliciting full posterior and even the vote might be prohibitive if the number of alternatives \(m\) is huge. In this work, we are concerned about eliciting partial rankingsover subsets of size \(k m\). Let us fix a subset \(T A\) of size \(k\). Then the probability of a partial ranking \(_{i}\) is given as

\[_{s}(_{i}^{})=_{:_{i}}_{s} (^{})\] (4)

Here we use the notation \(_{i}\) to indicate that the ranking \(\) when restricted to the set \(T\) is \(_{i}\).

We can also naturally extend definition 1 to define the posterior distribution over partial preferences given a partial preference \(_{i}\). In order to do so, let us first define the posterior over full ground truth \(^{}\) given \(_{i}\) as,

\[_{g}(^{}_{i})=(_{i}^{})P( ^{})}{_{}_{s}(_{i})P()}\]

where one can use definition (4) to compute \(_{s}(^{})\). Now we can write down the posterior probability over the partial ground truth \(\) as follows.

\[_{g}(_{i})=_{:}}_{g}(_{i})\] (5)

Finally, we can write the posterior over another partial ranking \(^{}\) over the subset \(T\).

\[_{o}(^{}_{i})=_{}_{g}(_{i})_{s}(^{})\] (6)

In this work, our aim is to propose several versions of surprising popular algorithm that work with partial preferences. As shown in definition 3, it requires eliciting information regarding voters partial preferences, and posterior over others' partial preferences (as defined in eq. (6)). Next, we discuss various ways of eliciting such information from the voters.

### Elicitation Formats

Given a subset of size \(k m\) alternatives, voter \(i\)'s prediction \(_{o}(_{i})\) is a distribution over \(k!\) rankings. In practice, this renders elicitation of full prediction information difficult, if not impossible, due to its cognitive overload. Thus, we focus on simple, and more explainable, elicitation methods that rely on ordinal information either by identifying the most preferred alternative (Top), selecting the most preferred \(t\) alternatives (Approval(\(t\))), or a complete ranking of the partial set (Rank). The formal definitions can be found in Appendix B.

Given these elicitation methods, we study different combinations of formats for votes and predictions where the first component indicates the vote format and the second component denotes the prediction format. These give rise to nine formats: Top-None, Top-Top, Top-Approval(\(t\)), Top-Rank, Approval(\(t\))-Rank, Approval(\(t\))-Approval(\(t\)), Rank-None, Rank-Top, and Rank-Rank. For Approval(\(t\)), the approval set of size \(t\{1,2,3\}\) is selected. Note that Approval(\(1\)) \(\)Top.

## 3 Aggregation Algorithms for Partial Preferences

The surprisingly popular method (as we discussed in section 1) cannot be applied directly to find a full ranking using only partial preferences. Thus, we develop two vote aggregation algorithms that _only_ rely on partial ordinal preferences both for votes and predictions. On the high level, the two algorithms differ on how and when they implement the SP method, whether independently on each subset (Partial-SP) or on the aggregated (potentially partial) votes and ranks (Aggregated-SP). Here we provide a high-level description for each of the algorithms; additional details and exact pseudo-codes are relegated to Appendix D.

**Partial-SP.** The key element of this algorithm is utilizing SP voting on the partial rankings obtained at each step. It takes a set of potentially overlapping subsets of alternatives and a voting rule as input and proceeds as follows: For each subset \(S_{j}\) of alternatives, collect votes and predictions from voters on this subset according to one of the elicitation formats detailed in Section 2.1. Compute the ground truth partial ranking on the subset \(S_{j}\) using the SP algorithm. Aggregate all partial rankings using a voting rule (e.g. Condorcet) to find a full ranking over all alternatives (breaking ties at random).

**Aggregated-SP.** This variation utilizes SP voting on the final rankings over votes and predictions. It takes a set of potentially overlapping subsets of alternatives and a voting rule as input and proceeds as follows: For each subset \(S_{j}\) of alternatives, collect votes and predictions from voters on this subset according to one of the elicitation formats detailed in Section 2.1. Aggregate all votes (partialrankings) using a voting rule (e.g., Condorcet) to find the aggregated vote over all alternatives, breaking ties at random. Predictions are not aggregated to preserve conditional prediction information crucial for SP voting. Apply SP algorithm pairwise across all alternatives where for each pair \((a,b)\), the vote information is derived from the scores of \(a\) and \(b\) based on the aggregation rule used, and the prediction information is used to find the conditional probabilities, \(P(a|b)\) and \(P(b|a)\). Breaking ties at random throughout this process results in a full ranking over all alternatives.

**Subset Selection.** The algorithms described in this section rely on partial rankings on the subset of alternatives. Given \(m\) alternatives, we carefully select subsets of size \(k\) with an inter-alternative pairwise distance of \(s\) between elements from the ground-truth ranking. Formally, a subset \(S_{j}\) of size \(k\) is generated as follows:

\[S_{j}=\{a_{1+j},a_{1+j+s},,a_{1+j+(k-1)s}\},\] (7)

where \(j 0\) and \(j+(k-1)s<m\), ensuring elements are within the range of \(m\) alternatives. We get a total of \(m-(k-1)s\) subsets. Note that we use overlapping subsets so as to introduce transitivity among different subsets enabling us to compare alternatives across different subsets. This leads to an improvement in the accuracy of our algorithms as we discuss in Section 5.

## 4 Experimental Design

This section describes the experimental design of the Amazon Mechanical Turk (MTurk) study to assess the comparative efficacy of Partial-SP and Aggregated-SP against other voting rules for partial preferences. Participants in this study were asked to answer a series of questions, wherein they were required to express their preferences by voting on a range of alternatives. In addition to casting their own votes, participants were asked to predict the collective preference of others for the same set of alternatives. Data was collected from \(432\) respondents. Each participant was given a 20-minute window to complete a series of 18 questions (see details below).3**Datasets.** The survey encompassed three distinct domains: (i) The _geography_ dataset contains 36 countries with their population estimates, according to the United Nations, (ii) The _movies_ dataset contains 36 movies with their lifetime box-office gross earnings, and (iii) The _paintings_ dataset contains 36 paintings with their latest auction prices.4

**Questions.** We explored 36 alternatives per domain, aiming to gather partial preferences from voters. Each question featured a subset of alternatives, with the size of each subset maintained uniformly throughout the experiment.

Each participant was presented with a subset of \(5\) alternatives, selected based on an inter-alternative gap of \(6\) positions within the ground-truth ranking. This strategy was designed to balance the cognitive load against the quality of the responses. We tested subset sizes of \(4\) to \(6\) and inter-alternative gaps of 3 to 8, finding that larger sizes and wider gaps generally enhanced ground-truth recovery. However, larger subset sizes increase cognitive load for participants, and wider gaps reduce overlap between subsets when limited to 36 alternatives. For each combination of \(12\) subsets, \(9\) elicitation formats, and \(3\) domains, each question received \(16\) responses.

The survey was structured for each participant to answer two questions from each of the three domains and two elicitation formats, totaling 12 questions per participant. Figure 1 shows the workflow for each participant; each participant was assigned 18 questions to answer. Refer to Appendix E for details on the tutorials, participant qualifications for the MTurk study, and review questions regarding the perceived difficulty and expressiveness of the study.

**Elicitation Formats.** We use various elicitation formats (as described in section 2.1). For example, consider a question that requires participants to rank five movies: a) Rogue One: A Star Wars Story, b) Titanic, c) Toy Story 3, d) The Dark Knight Rises, and e) Jumanji: Welcome to the Jungle--based on their lifetime gross earnings. Under the Approval(3)-Rank elicitation format, the structure of the vote and prediction questions would be framed as follows:

Figure 1: Workflow of a participant

* **Part A (vote):**_"Which among the following movies are the top three in terms of highest grossing income of all time?"_
* **Part B (prediction):**_"Considering that other participants will also respond to Part A, in what order do you predict the following movies will be ranked, from the most common response (top) to the least common (bottom)?"_

Refer to Appendix I for further details about formulations of all nine elicitation formats, the consent form, the tutorial for each domain, screenshots, and other details.

## 5 Results and Analysis

In this section, we present the results of this study averaged across all three domains. We measure the accuracy of the proposed SP algorithms (Partial-SP and Aggregated-SP) in predicting the full ground-truth ranking, in comparison with common vote aggregation methods (e.g. Borda, Copeland, Maximin, Schulze). The details of these aggregation methods is provided in Appendix C.

Additionally, we compare the elicitation formats (described in Section 2.1) with respect to cognitive effort (measured by response time and difficulty) and expressiveness (measured directly by survey questions). They are provided in Appendix G.2

### Accuracy Metrics

To capture the error in predicting the full ground-truth ranking, we use three different metrics: (i) the _Kendall-Tau_ correlation, which measures the distance between ordinal rankings, (ii) _Spearman's_\(\) correlation, which measures the statistical dependence between ordinal rankings, (iii) _Pairwise hit rate_, which measures the fraction of pairs at distance \(d\) that are correctly ranked with respect to the ground-truth ranking, and (iv) _Top-\(t\) hit rate_, which measures the fraction of alternatives that are predicted correctly (in no order) in most preferred \(t\) compared to the ground-truth ranking. The formal definitions can be found in Appendix G.1.

For example, consider the ground-truth ranking \(a b c d\). The predicted ranking \(b a d c\) has a pairwise hit rate of \(1/3\) at distance 1, 1 at distances 2 and 3. Its Top-1 hit rate is \(0\), Top-2 is \(1\), Top-3 is \(2/3\), and Top-4 is \(1\).

### Predicting the Ground Truth Ranking

Figure 2 illustrates the performance of SP algorithms measured by Kendall-Tau and Spearman's correlations. We fix Copeland as the aggregation rule used in both variations of SP voting and compare the accuracy with applying Copeland on votes alone (without the use of prediction information).

**Statistical correlations and elicitation.** SP voting produces rankings with a significantly higher correlation with the ground truth ranking, and this effect improves as the information provided as votes and prediction becomes more expressive. In particular, Rank-Rank and Approval(3) -Rank outperform all other elicitation formats. We note that Aggregated-SP seem to be more reliant on the vote information, compared to the predictions, as it can seen in Top-Rank vs. Rank-Top. In contrast, Partial-SP does not exhibit any significant favor for vote vs. prediction information as both Top-Rank and Rank-Top improve by additional information. However, the difference between them is not statistically significant.

Interestingly, eliciting unordered information for both noisy votes and predictions (e.g. Approval(2) -Approval(2)) seem to be sufficient in recovering the ground truth--raising the question of whether pairwise comparisons are necessary in designing SP algorithms.

**Hit rates.** With respect to pairwise hit rate and the Top-\(t\) hit rate, the noisy prediction information significantly improves the performance of the Partial-SP algorithm (with lower variance) as shown in Figure 3. The results for Aggregated-SP are qualitatively similar and are presented in Appendix G.5. The slight dip in pairwise hit rate, can be explained by the survey's design choice of an inter-alternative distance of 6, leading to fewer comparisons being available for these pairs.

**Partial-SP vs. Aggregated-SP.** While both variants of the SP algorithm significantly outperform common voting rules by utilizing (noisy) prediction information, the Partial-SP algorithm significantly outperforms the Aggregated-SP algorithm (see Figure 2 and Figure 4). This could be explainedby the importance of 'correcting' noisy votes on the subsets of alternatives because the prediction information of the Partial-SP algorithm helps identify experts early on in predicting partial rankings of these alternatives.

For Partial-SP, the plots indicate no statistical significance between Approval(2)-Approval(2), Approval(3)-Rank, and Rank-Rank elicitation formats, suggesting they perform as well as Rank-Rank. An interesting ramification here is demonstrating that approval sets not only perform well in predicting the ground truth, but also pose less cognitive burden on voters compared to those elicitation formats that ask for rankings (see appendix G.2).

**Domain impacts.** Performance of Partial-SP and Aggregated-SP is robust across domains. They outperform common voting rules with the sole exception of the Schulze method, which matches the performance of Aggregated-SP (see Figure 4). The difference in performance is notably high for Paintings domain, where specialized expertise is required to predict painting prices. Here, Partial-SP significantly outperforms common aggregation rules, showcasing its effectiveness in leveraging expert knowledge and correcting misinformation. For further details see Appendix G.3.

## 6 Simulated Model of Voter Behavior

In this section, we investigate whether there is any underlying probabilistic model that can explain the voters' behaviours when measured in terms of the vote and predictions. If successful, such a model will also enable us to theoretically analyze the sample complexity of SP algorithms (as we present in Section 7). In particular, we posit that a _concentric mixtures of Mallows model_ can explain the users' reports (both vote and prediction) in the dataset. The concentric mixtures of Mallows model is a type of mixture models where there is one ground truth, but different groups of users have different dispersion parameters, and hence different distribution over observed preferences.

Figure 3: Comparing the Partial-SP algorithm with Copeland (no prediction information) measured by pairwise and Top-\(t\) hit rates. The elicitation format is Approval(2)-Approval(2).

Figure 2: Comparing the predicted and ground-truth rankings for different elicitation formats using Kendall-Tau and Spearman’s \(\) correlations (higher is better). All results use Copeland as their aggregation rule.

**Concentric mixtures of Mallows model.** We assume that each voter is likely to be an expert with probability \(p( 1)\) and a non-expert with probability \(1-p\). Given a ground truth ranking \(^{}\), an expert voter observes a ranking that is distributed according to a Mallows model with center \(^{}\) and dispersion parameter \(_{E}\). On the other hand, a non-expert voter observes a ranking that is again distributed according to a Mallows model with center \(^{}\), but with a larger dispersion parameter \(_{NE}\). In particular, the ranking observed by voter \(i\) is distributed as

\[_{s}(_{i}^{})=p_{s}(_{i}^{},_{E}) +(1-p)_{s}(_{i}^{},_{NE})\] (8)

where \(_{s}(^{},)\) is the standard Mallows model with dispersion \(\) i.e. \(_{s}(^{},)=)}}{Z(,m)}\). The term \(Z(,m)\) is the normalization constant, and is defined as \(Z(,m)=_{}^{d(,^{})}\). With a slight abuse of notation, we will write \(Z()\) as \(Z(,m)\) since the number of alternatives in the ground truth is assumed to be fixed.

Note that, Equation (8) defines a distribution over complete preferences, but given a subset of size \(k\) we can naturally extend this definition to define a distribution over partial preferences e.g. \(_{s}(_{i}^{})\) (eq. (4)), and posterior over partial preferences of other voters e.g. \(_{o}(_{i})\) (eq. (6)).

We fit the mixture model eq. (8) on the real datasets and estimate the following parameters - proportion of experts (\(p\)), dispersion parameters of experts' votes (\(_{Evotes}\)) and predictions (\(_{Epredictions}\)), as well as the dispersion parameters of non-experts' votes (\(_{NEvotes}\)) and predictions (\(_{NEpredictions}\)). The parameters were inferred using Bayesian inference . We also generated synthetic data using the concentric mixtures of Mallows model, and again used Bayesian inference to estimate the parameters. The details of estimation and data generation are provided in the appendix F. Figure 5 shows the posterior distributions for the dispersion parameters when the datasets of all the three domains are combined. We see that the synthetic data generation process accurately replicates real data characteristics, and highlights that the concentric mixtures of Mallows accurately model voters' behaviours on MTurk. Furthermore, Figure 6 also plots the same posterior distributions but only for

Figure 4: Comparing the predicted and ground-truth rankings for different aggregation rules using Kendall-Tau and Spearman’s \(\) correlations (higher is better). The elicitation format is Rank-Rank; each comparison uses the same aggregation rule in the SP algorithm.

Figure 5: Comparison of inferred parameters of the _Concentric mixtures of Mallows model_ for real data with all domains combined and synthetic data. The experts vote closer to and predict farther from the ground-truth. The non-experts vote and predict far from the ground truth. The proportion of experts in both datasets was found to be less than 20%.

the Movies domain. Now we see almost perfect fit between the synthetic data and the original data. Further similarity in results between real and simulated data is described in Appendix G.6.

## 7 Analysis of Sample Complexity

In this section, we use a _concentric mixture of Mallows_ models, and provide upper bound on the sample complexity of the surprisingly popular voting method with partial preferences. We start with a simple problem. Given a subset \(T\) of size \(k\), suppose our goal is to recover the true partial ranking over the alternatives in \(T\), then how many samples does SP algorithm require?

We will analyze the following simplified version of the SP algorithm: Voter \(i\) reports vote \(_{i}\) over the subset \(T\), which is used to build an estimate of \(f()\) for all \(_{s}\). For each \(\), the posterior report by the voter is a partial ranking drawn from the distribution \(g()\). These reports are used to build an estimate \((^{})\) for all \(^{}\), \(\). Select partial ranking \(_{}()=( )_{_{s}}(^{}|)} {(|^{})}\).

It is impossible to recover the partial ground truth ranking if the fraction of the experts \(p\) can be very small, or the dispersion parameter of the non-experts \(_{NE}\) can be very large. In order to ensure recovery of the true partial ranking we will make the following assumption.

**Assumption 1**.: _The dispersion parameters \(_{E},_{NE}\), and the fraction of experts \(p\) satisfy the following inequality,_

\[()^{2} 2()}{Z(_{ E})})^{2}Z(_{NE},k)_{E}^{k(k-1)/2}\]

_where \(Z(,k)=_{:[k][k]}^{d(,^{*})}\)._

The next theorem states that sample complexity under the above assumption.

**Theorem 1**.: _Suppose Assumption 1 holds, and the total number of samples \(n k!}\) where \(=p,m-k)}{Z(_{E})}_{E}^{k(k-1)/2}+(1-p) ,m-k)}{Z(_{NE})}_{NE}^{k(k-1)/2}\). Then the surprisingly popular algorithm recovers true ranking over the subset \(T\) of size \(k\) with probability at least \(1-\)._

Suppose \(_{E}_{NE}<1\). Then Assumption 1 requires \((_{NE}^{k^{2}/4+1}_{E}^{k^{2}/4-1})\), and it implies that if \(_{NE}\) is very large compared to \(_{E}\) (i.e. noisy non-experts) then we need a larger value of \(p\) (i.e. more experts).

We provide the full proof of the theorem in Appendix I. The main ingredient of the proof is Lemma 2 which shows that under Assumption 1 there is a strict separation between the true prediction-normalized score of the true partial ranking and any other ranking. In fact, we show that \((^{}) 2()\) for any \(\) with \(d(,^{}) 1\). Given this result, we can apply standard concentration inequality to show that \(()\) is close to \(()\) for all \(\) when the number of samples is large, and \((^{})\) will be larger than \(()\) for any \(^{}\). Therefore, picking the ranking with the largest empirical prediction-normalized score returns the correct ranking.

Note that the sample complexity grows proportional to \(k!\) only because we compute prediction-normalized votes over all \(k!\) partial rankings. If we are interested in recovering top \(t\)-alternatives then

Figure 6: Comparison of inferred parameters of the _Concentric mixtures of Mallows model_ for real data of Movie domain and synthetic data. The quality of model fit improves if the focus is on one single domain.

it will grow proportional to \(\). Moreover, the subset size \(k\) is assumed to be very small compared to the number of alternatives \(m\), and Theorem 1 shows the benefit of applying SP algorithm to partial preferences. We can immediately apply Theorem 1 to a collection of subsets \(S\) through a union bound, and extend our analysis to the Partial-SP algorithm. Let us assume that in the second stage of Partial-SP, we apply a \(t\)-_consistent_ voting rule \(f\) that recovers top-\(t\) alternatives as long as each partial ranking in \(S\) is correct.

**Corollary 1**.: _Under the same setting as Theorem 1, suppose the number of samples from each subset in \(S\) is \(n k!}\). Then the Partial-SP algorithm with a \(t\)-consistent voting rule, recovers the top \(t\) alternatives of the ground truth \(^{}\) with probability at least \(1-\)._

Finally note that, the total sample complexity of \((|S| k!)\) is needed only because we adopt a naive version of the SP algorithm for proving theoretical guarantees. For the experiments, we adopt a pairwise version of the SP algorithm which applies SP-voting to each pair within a subset. We believe that under further assumptions, the total sample complexity can be reduced to \((|S| k^{2})\) with such a pairwise variant of the partial-SP algorithm, and we leave this analysis for the future.

## 8 Discussion and Future Work

We conclude by discussing some limitations and future directions. When dealing with partial preferences, even when majority have the correct information, effective preference elicitation or finding a necessary winner in most vote aggregation rules are often computationally intractable . These challenges, together with the minority of experts, further highlight the efficacy of the SP approach in balancing information elicitation and accuracy, by employing additional prediction information. Future research can explore the setting of SP beyond the majority-minority dichotomy (e.g. informed, but not expert voters) or when malicious voters are present (e.g. in detecting misinformation). Theoretically, the sample complexity can be explored beyond Mallows model under other probabilistic models to enhance our understanding of this approach, particularly in notable applications such as political polling or collective moderation of online content.