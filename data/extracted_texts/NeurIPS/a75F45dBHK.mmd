# Orchid: Flexible and Data-Dependent Convolution

for Sequence Modeling

Mahdi Karami

Google Research

mahdika@google.com

&Ali Ghodsi

School of Computer Science

University of Waterloo, ON, Canada

ali.ghodsi@uwaterloo.ca

###### Abstract

In the rapidly evolving field of deep learning, the demand for models that are both expressive and computationally efficient has never been more critical. This paper introduces Orchid, a novel architecture designed to address the quadratic complexity of traditional attention mechanisms without compromising the ability to capture long-range dependencies and in-context learning. At the core of this architecture lies a new data-dependent global convolution layer, which contextually adapts its kernel conditioned on input sequence using a dedicated conditioning neural network. We design two simple conditioning networks that maintain shift equivariance in our data-dependent convolution operation. The dynamic nature of the proposed convolution kernel grants Orchid high expressivity while maintaining quasilinear scalability for long sequences. We evaluate the proposed model across multiple domains, including language modeling and image classification, to highlight its performance and generality. Our experiments demonstrate that this architecture not only outperforms traditional attention-based architectures such as BERT and Vision Transformers with smaller model sizes, but also extends the feasible sequence length beyond the limitations of the dense attention layers. This achievement represents a significant step towards more efficient and scalable deep learning models for sequence modeling.

## 1 Introduction

In modern deep neural networks, attention mechanisms have emerged as a gold standard, pivotal in domains such as natural language processing, image, and audio processing, and even complex fields like biology (Vaswani et al., 2017; Dosovitskiy et al., 2020; Dwivedi and Bresson, 2020). However, despite their strong sequence analysis capabilities, these sequence modeling mechanisms suffer from their high computational complexity, which scales quadratically with sequence length, hindering their application to long-context tasks. This complexity has driven a shift towards innovative solutions to overcome this computational barrier, enabling analysis of long sequences in areas like genomics, DNA sequencing, and the creation of long musical compositions.

In the past years, researchers have explored various strategies to tackle the computational bottleneck of traditional dense attention layers (Tay et al., 2022). One key strategy involves _sparsifying_ the dense attention matrix. Instead of calculating the entire matrix, Qiu et al. (2019); Parmar et al. (2018) focus on specific local blocks of the receptive fields of sequences by chunking them into fixed-size blocks. Moreover, Sparse Transformer (Child et al., 2019); Longformer (Beltagy et al., 2020) and BigBird (Zaheer et al., 2020) use strided attention patterns combined with local sliding windows to reduce computation. In contrast to using pre-determined patterns, other techniques include learning to cluster/sort tokens based on a similarity function, thereby enhancing the global view of the sequence, as seen in Reformer (Kitaev et al., 2020), Routing Transformer (Roy et al., 2020) SparseSinkhorn attention (Tay et al., 2020). Another approach involves _low-rank approximations_ of the self-attention matrix, leveraging the insight that these matrices often exhibit low-rank properties, as demonstrated by Linformer (Wang et al., 2020) which projects keys and values matrices to lower-dimensional representation matrices. Another paradigm to reduce quadratic computation cost, is to replace the dot-product similarity between keys and query matrices of attention mechanism with a _kernel function_ and avoid explicitly computing the attention matrix (Katharopoulos et al., 2020). Notable examples in this family include Performers (Choromanski et al., 2020), Random Feature Attention (Peng et al., 2021) that are based on random feature approximation of the kernel function. Additionally, some models leverage a combinations of such techniques to design an efficient transformer (Zhu et al., 2021; Zhang et al., 2021). However, while these methods significantly reduce computational overhead, they may sacrifice expressiveness and performance, often requiring hybrid approaches that combine them with dense attention layers (Mehta et al., 2022; Fu et al., 2023). On the other hand, recent works have aimed at sparsifying dense linear layers, used for feature mixing in Transformer blocks, to tackle another major source of high computation and memory demand in large models (Dao et al., 2022; Chen et al., 2021, 2021).

Finding sub-quadratic and hardware-efficient mixing operators that are also expressive remains a significant challenge. Recent studies have explored attention-free solutions, particularly using state space models (SSMs) (Gu et al., 2021; Mehta et al., 2022; Wang et al., 2022; Fu et al., 2023; Orvieto et al., 2023; Gu and Dao, 2023; De et al., 2024), and long convolutions (Romero et al., 2021; Li et al., 2022; Poli et al., 2023). A state space model characterizes a dynamical system's behavior in terms of its internal state using a state equation, describing the dynamics of the system using first-order differential equations over the states, and an observation equation, relating state variables to observed outputs.1 A key insight is that, most of these SSM models can be formulated as a long convolution model between the input and output sequences (Gu et al., 2021), allowing parallel and efficient training. However, recent work by Poli et al. (2023) demonstrated that directly parameterizing the filter impulse response of the long-convolution leads to an even more expressive sequence mixing layer.

This paper proposes a novel data-dependent convolution mechanism to tackle the inherent quadratic complexity of traditional attention mechanisms, while maintaining the model's ability to capture long-range dependencies and in-context learning. The data-dependent convolution layer contextually adapts its kernel based on input data using a dedicated conditioning neural network. We design two simple yet effective conditioning networks that maintain shift equivariance in the adaptive convolution operation. By combining these adaptive mechanisms with gating operations, our proposed model--named _Orchid_--achieves high expressivity while offering quasilinear scalability (with a complexity of \((L L)\)) for long sequences. Evaluation across various domains, including language modeling and image classification, presented in section 4 and Appendix, demonstrates the Orchid architecture's performance and generality, outperforming attention-based architectures, like BERT and Vision Transformers, with smaller model sizes. Moreover, its allows for handling very large sequence lengths that are beyond the limitations of the dense attention layers. This achievement layers the foundation for further advancements in more efficient and scalable sequence modeling architectures.

## 2 Background

Self-Attention Mechanism:Given a length-\(L\) sequence of embeddings (tokens) \(=(x_{1},x_{2},,x_{L})\), the self-attention layer generates a new sequence by computing a weighted sum of these embeddings. To achieve this, it linearly project \(\) into three components: queries (\(\)), keys (\(\)), and values (\(\)), as: \(=^{Q},=^{K},=^{V}\). Each individual attention mechanism within a multi-head self-attention layer operates as a dense linear transformation, expressed as:

\[=(,,)=( ^{T}}{}})=(x),\]

where the matrix \((x)\) contains the normalized attention scores between each pair of tokens. This description of the attention layer highlights its notable benefits, including its capability to capture long-range dependencies using a sublinear parameter count. The attention mechanism enables direct computation of interactions between any two positions in the input sequence, regardless of their distance, without a corresponding rise in parameter counts. Additionally, the attention layer implements a _data-dependent_ dense linear filter, which effectively filter the input based on on weights conditioned by a mapping of the data. This property makes it expressive and flexible enough to encode a large family of linear functions. However, these advantages come at the expense of quadratic computational complexity and memory costs.

This motivates us to develop an efficient and scalable _data-dependent convolution_ mechanism, featuring an adaptive kernel that adjusts based on the input data. The kernel size of this convolution layer is as long as the input sequence length, enabling the capture of long-range dependencies across the input sequence while maintaining high scalability.

Linear Convolution:Discrete-time linear convolution is a fundamental operation in digital signal processing that calculates the output as the weighted sum of the finite-length input \(\) with shifted versions of the convolution kernel, \(\), also known as the impulse response of a linear time-invariant (LTI) system.2 Formally, it can be written as

\[[t]=(*)[t]_{=0}^{L-1}h[t-]x[].\]

In this definition, the output is a linear filter of the _zero-padded_ input and convolution kernel. However, other padding schemes leads to different forms of convolution. A well-known form is _circular convolution_, defined as

which is equivalent to the linear convolution of two sequences if one is cyclically padded at its edges.

Global Convolution and Fast Convolution Algorithm:Standard convolution layers, explicitly parameterized with a short kernel, struggle to capture long-range dependencies in sequential data. Extending the kernel to match the input length enables modeling such dependencies but leads to linear growth in parameter counts and quadratic computational complexity. To mitigate the parameter growth challenge, the kernel of global (a.k.a. long) convolution can be implicitly

Figure 2.1: Orchid block architecture. This diagram illustrates the structure of the Orchid block. The core operation is a convolution (denoted by \(*\)), efficiently implemented in the frequency domain using FFT. Element-wise multiplication is denoted by \(\). On the right side, two different conditioning networks, introduced in equations (2) and (3) as shift-invariant convolution kernels, are illustrated. In this model, the convolution is performed efficiently in the spectral domain, so the kernel in the frequency domain, \(h^{}=h_{0}^{}+h_{}^{}()\), is computed. The block also includes MLPs for linear projection and pointwise mixing of features at the beginning, that is common design choice used in various sequence modeling architectures.

parameterized using a multilayer perceptron (MLP), a technique that has been shown to maintain sub-linear parameter scaling (Karami et al., 2019; Romero et al., 2021; Li et al., 2022; Poli et al., 2023). Furthermore, a key advantage of convolution operators is that, leveraging the convolution theorem, convolution operators can be efficiently computed in the frequency domain using Fast Fourier Transform (FFT) algorithms, thereby reducing the computational complexity to \((L L)\)(Cooley and Tukey, 1965). Formally, the linear convolution can be expressed in the frequency domain as \(}=^{-1}((})(}))=^{-1}(^{}})\), where \(\) is the DFT matrix, \(\) denotes the discrete Fourier transformation, and \(}\) denotes the zero-padded signal, defined as \(}_{2L}()=[_{L};\;]\). Additionally, the circular convolution can be simply computed as: \(=}=^{-1}(() ())\).

## 3 Orchid Operator

This section introduces the _Data-Dependent Convolution Filter_, a novel operator aimed at increasing the expressiveness of long convolution operations. This operator serves as the foundational building block for the _Orchid_ layer, which we will explore later in the section.

### Data-Dependent Convolution Filter

We hypothesize that making the convolutional kernel data-dependent allows the filter to adapt to the specific characteristics of its input, potentially capturing more complex patterns within the sequence. Formally, this input-dependent filter is defined as:

\[=h_{}()*=_{}()*\] (1)

The key innovation is to replace the static convolutional kernel with a conditionally generated one controlled by the input data. This is achieved through a _conditioning network_, denoted as \(h_{}()=_{}()\), a neural network parameterized by \(\). The conditioning network outputs a vector matching the input sequence in length. This allows each input token to 'attend' to the entire sequence with personalized, adaptive weights derived from its specific representation. Convolving the surrounding context using this data-dependent weighting scheme can potentially offer more effective sequence mixing compared to conventional static convolutions.

### Preserving Shift-Equivariance in Data-Dependent Convolution

A fundamental property of discrete convolution is _shift equivariance_, meaning that shifting the input by a certain amount leads to a corresponding shift in the output (disregarding boundary effects). This is formally expressed for circular convolution as: \(_{m}()=_{m}()\)(Bronstein et al., 2021), where this property holds exactly regardless of boundary conditions. The shift operation is defined as \(_{m}()[t][t+m]\).

This property is particularly important because it ensures the operator's response is robust to shift of features within the input, thereby enhancing the model's generalization capabilities. This inductive bias is at the core of the widespread success of convolution operations (Thomas et al., 2017). Therefore, it is desirable to design conditioning network in the data-dependent convolution (1) to preserve shift equivariance property. To maintain this property for data-dependent convolution operations, it is sufficient to design filter kernel to be _shift-invariant_, _i.e._\(h(_{m}())=h()\) (refer to Appendix B for the proof). In the following, we present two conditioning network designs satisfying shift-invariance.

I) Phase Suppression for Shift Invariance:A circular shift of a sequence \(\) results in a linear phase shift of its frequency components: \(_{m}()[]=^{}[] e^{-}{L} m}\)(Oppenheim, 1999). Given a shift-equivariant function \(g(x)\) (such as a depthwise Conv1d()) (satisfying: \(g(_{m}())=_{m}(g())\)). Its frequency components after a spatial shift of its input maintain this phase shift:

\[g(_{m}()[]=( g())[] e^{-}{L} m}.\]

By taking the magnitude (absolute value or squared) of these complex-valued frequency components, we effectively eliminate the phase shift, therefore, defining \(h^{}()=g()\) satisfies shift-invariance property: \(h^{}(_{m}())=h^{}()\).

In our design, we deploy a hybrid spatial-frequency domain conditioning network. This network consists of a 1D depthwise linear convolution (\(()\)) with a short kernel length (typically 3-5) acting at the spatial domain, followed by a short convolution in the frequency domain. By operating in both spatial and frequency domains, the conditioning network effectively mixes information from neighboring tokens and spectral components. The resulting conditioning neural network is formulated as:

\[h_{}^{}()=( ())\] (2)

This architecture choice aims to minimize the number of parameters and computational overhead introduced by the conditioning network within the overall model.

II) Leveraging Cross-Correlation for Shift-Invariance:An alternative approach to achieving shift-invariance involves computing the cross-correlation between two mapping of the input sequence. Let \(k()\) and \(q()\) be two shift-equivariant functions, satisfying: \(k(_{m}())=_{m}(k())\) and \(q(_{m}())=_{m}(q())\). We define \(h()\) as the cross-correlation of \(k()\) and \(q()\), given by:

\[h()[t]=(k() q())[t]_{=0}^{L-1}k()[] q()[t+ L].\]

This operation essentially slides \(q()\) over \(k()\) and measures their similarity at different offsets. Remarkably, the resulting cross-correlation function, \(h()\), is also shift invariant:

\[h(_{m}()) =k(_{m}()) q(_{m}())\] \[=_{m}(k())_{m}(q())\] \[=k() q()=h()\]

Furthermore, the convolution theorem enables efficient computation of the cross-correlation in the frequency domain: \(h^{}()=(k() q())=k^{ ^{*}}() q^{}()\) where \(k^{^{*}}\) denotes the complex conjugate of \(k^{}\) and \(\) represents element-wise multiplication.

_Remark 3.1_.: By setting \(k()=q()=g()\), we obtain \(h^{}()=|g^{}()|^{2}\), This indicates that the cross-correlation approach generalizes the magnitude-based approach, demonstrating its versatility.

Similar to the previous approach, we employ separate 1D depth-wise short convolutions for both \(k()\) and \(q()\), followed by another convolution post cross-correlation in the frequency domain. As a result, the conditioning neural network is defined as

\[h_{}^{}()=^{*}( ())(() ).\] (3)

Both conditioning functions, as defined in (2) and (3), are illustrated schematically in Figure 2.1.

_Remark 3.2_.: For convolution operations, we augment the data-dependent conditioning network by incorporating a fixed (static) term. This term adds positional encoding to the convolution kernel by implicitly parametrizing it using a positional embedding, \(()\), of time step (token index in the sequence) and a feed forward networks as \(h_{0}=((t))\)(Romero et al., 2021; Li et al., 2022; Poli et al., 2023). The final convolution kernel is obtained by summing this positional bias with the output of the conditioning network, \(h=h_{}()+h_{0}\).

_Remark 3.3_ (**Data-Dependent Convolution as a Cross-attention Alternative**).: The kernel of convolution \(h_{}()\), defined in equations (2) or (3), is conditioned on the input of the convolution layer, making it input-dependent. However, we can generalize this concept further. The kernel could be a function of any arbitrary sequence \(\), leading to a broader definition of data-dependent convolution:

\[(,)=h_{}()=_{}() \]

This definition couples the input sequence \(\) with another sequence \(\), creating a potential alternative to cross-attention layers in sequence processing tasks. We therefore refer to the proposed layer as _"data-dependent"_ in a more general sense. When dealing with sequences of different lengths, the shorter sequence can be zero-padded to match the length of the longer one. Specifically, assuming \(^{L}\) is longer than \(^{N}\) (\(L>N\)), we use the zero-padded sequence \(}=_{L}()^{L}\) as input to the conditioning network \(_{}(})\). Since the long convolution is implemented in the frequency domain, this zero-padding in the time domain translates to interpolation in the frequency domain (Smith, 2008) ensuring that both sequences have frequency components of the same length.

### Orchid Block

Unlike attention layers, convolution filters leverage parameter sharing. This means they slide the same kernel weights and apply them to different positions within the input sequence. Mathematically, this operation is equivalent to multiplying an input vector with a structured matrix, such as a Toeplitz matrix for linear convolutions or a circulant matrix for circular convolutions, which results in computational efficiency (Gray et al., 2006; Karami et al., 2019). To achieve a location-dependent filtering scheme, we complement the data-dependent convolution with element-wise multiplications, allowing the model to emphasize specific tokens within by assigning higher weights prior to applying the location-invariant convolution. Notably, prior research has demonstrated that a cascade of circulant and diagonal matrices can effectively approximate dense linear layers (Moczulski et al., 2015; Cheng et al., 2015). Building upon these insights, the overall architecture of the Orchid block, is composed of a chain of \(M\) data-dependent convolution and element-wise multiplications (gated connections). In our experiments, we utilize a simple chain of order 1.5, consisting of data-dependent convolution sandwiched by two element-wise multiplications: \(=(f_{}^{2} f_{*} f_{}^{1})()\) where \(\) denotes composition, \(f_{*}()(h_{}()+h_{0})*\), and \(f_{}^{i}()()\). The Orchid block is illustrated in Figure 2.1 and its basic implementation is presented in appendix D.

**Overall Computational complexity.** All global convolutions within the Orchid block are computed in the frequency domain using FFT algorithm, inheriting its computational efficiency with complexity of \((L L)\). Furthermore, the element-wise multiplications contribute an additional \((L)\) complexity. Consequently, the overall complexity of the Orchid block scales quasi-linearly with the sequence length, resulting in a total complexity of \((ML L)\), where \(M\) is the number of layers in the block. A recent hardware and I/O optimized implementation of FFT, introduced in (Fu et al., 2023), can speed up the overall computation of Orchid on modern accelerators. Empirical runtime comparisons against standard attention mechanisms, detailed in Appendix C.5, highlight its expected scalability, especially for longer sequences.

## 4 Experiments

Our evaluation of Orchid focuses on three different Transformer-based models to evaluate its expressivity and generalization capabilities as an alternative to attention layers. Firstly, we conduct a set of experiments on a synthetic task to assess the in-context learning ability and scalability of the proposed model. Subsequently, we evaluate the performance of the proposed architecture on language modeling tasks. Moreover, we extend our experiments to image classification tasks, aiming to evaluate the model's generalizability across diverse domains. Additional ablation studies on model architecture and also an experiments on raw speech classification with long sequences are presented in Appendices C.5 and C.2. Unless otherwise specified, our experiments adopt the phase supersession (Type I) conditioning network (Equation 2) due to its simpler form. For an overview experimental details, please refer to Appendix C.

### Synthetic In-context Learning

The aim of the first experiment is to assess how well our model performs on a synthetic reasoning task. This task, inspired by prior work on language model benchmarking (Liang et al., 2022) and in-context learning (ICL) (Garg et al., 2022), is known as Associative Recall. It involves generating a value from a key given a string of key-value tuples from a random dictionary. For instance, given the input ([a, b, e, f, 3], b), the model is expected to return e, the value associated with the key b. This task assesses whether a model can effectively retrieve the correct value from a key in a prompt, essentially applying a data-controlled shift. Attention mechanisms offer this capability by computing attention scores through token comparisons and then weighting the entire sequence accordingly (Olsson et al., 2022). Associative recall has been pivotal in guiding the design of long convolution models, as demonstrated in (Fu et al., 2023), and a more complex variant of this task was employed in (Poli et al., 2023).

For these experiments, we benchmark Orchid against several leading long convolution models, including: I) H3, which utilizes state-space models (SSMs) for implicit parametrization of long convolution, as proposed in (Fu et al., 2023). II) CKConv, that employs feedforward networks (FFNs) and positional embeddings for the implicit parametrization of convolution operations, detailed in (Romero et al., 2021). III) Hyena, built upon the CKConv framework by incorporating anadditional exponential decay modulation into the implicit convolution process, as detailed in (Poli et al., 2023). It is further augmented by a multiplication in a chain of order 2.

As illustrated in Figure 4.1 and Tables 4.1 and 4.2, Orchid demonstrates superior expressiveness and outperforms existing long convolution models in associative recall tasks. These tasks become increasingly challenging with shorter sequences and larger vocabulary sizes. This difficulty arises because specific (key, value) pairs appear less frequently within shorter strings, hindering the model's ability to learn and reason on these associations. Remarkably, in such challenging scenarios with short sequence lengths of 128 and large vocabulary sizes, Orchid significantly improves the model's accuracy and closes the gap between Transformer and implicit convolution models. Furthermore, Orchid successfully learns the task even with extended sequence lengths of up to 131K tokens, a scale at which Transformer models encounter computational difficulties, which highlights Orchid's superior scalability and efficiency in learning long context.

The insights from this experiment guide us in integrating the proposed model into Transformer-based models for extensive language modeling, suggesting its potential to enhance performance in natural language processing tasks.

### Language Modeling

We evaluate the Orchid layer in language models. Orchid is designed to integrate seamlessly with existing BERT-style language models, such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2020), and others (Jin et al., 2020). In our experiments, we replace the attention layers in the standard BERT framework in (Devlin et al., 2018) with Orchid layers. For each Transformer block in the BERT-style model, we replace the attention layers with Orchid layers for sequence mixing. We also replace the two dense matrices in the MLP layers, used for dimension mixing in Transformers, with block-diagonal matrices (Dao et al., 2022). Following (Fu et al., 2023), we add a residual long convolution to each Orchid layer.

Our BERT-style model, called Orchid-BERT-base, has 12 layers with a hidden size of 768, the same dimension and depth as BERT-base. The resulting Orchid-BERT-base have 77M parameters, compared to BERT-base's 110M parameters. We also pretrain Orchid-BERT-large of 254M parameters

   Model & 20 & 30 & 40 \\   Transformer & 100 & 100 & 100 \\ CKConv & 91 & 25.7 & 20.4 \\ H3 & 71.5 & 13.2 & 10.2 \\ Hyena & 93 & 38.8 & 12.4 \\ Mamba & 100 & 100 & 35.8 \\ Orchid & 100 & 99.4 & 99.2 \\   

Table 4.2: The test accuracy of the associative recall task with varying vocabulary sizes and a sequence length of 128.

Figure 4.1: Test accuracy of the associative recall task across different long implicit convolution models on various sequence lengths and vocabulary sizes (number of possible token values).

   Model & 128 & 512 & 2K & 8K & 32K & 128K \\   Transformer & 100 & 100 & 100 & 100 & ✗ & ✗ \\ Monarch-Mixer & - & 98.7 & 99.4 & 99.4 & 99.4 \\ Hyena & 93 & 99 & 99.6 & 100 & 100 & - \\ Orchid & 100 & 100 & 100 & 100 & 100 \\   

Table 4.1: The performance (test accuracy) of in-context learning on the associative recall task with different sequence lengths and a vocabulary size of 20. The results for the baseline models are drawn from Poli et al. (2023), Fu et al. (2023). The symbol ✗ indicates that the Transformer model failed to complete the task within a week or the model does not fit in memory.

with hidden dimensions of 1536 and 12 layers. Orchid models are pre-trained using masked language modeling over the C4 dataset (Raffel et al., 2019) with the bert-base-uncased tokenizer.

Finetuning Performance on GLUE Benchmark.We conducted an evaluation of Orchid-BERT models on GLUE fine-tuning tasks, comparing them against the baseline models: BERT-base and BERT-large, and the recent long convolution-based models: M2-BERT-base and M2-BERT-large (Fu et al., 2023). The fine-tuning process was executed in accordance with the methodology described by Izsak et al. (2021). As the results outlined in Table 4.3 show, Orchid-BERT-base is able to achieve \(1.0\) points improvement in average GLUE score performance compared to the BERT-base on the GLUE benchmark with utilizing \(30\%\) fewer parameters. Similarly, Orchid-BERT-large outperforms the performance of BERT-large by \(.6\) points with a \(25\%\) reduction in parameter counts.

### Image Classification

We extend the application of Orchid to the Vision Transformer (ViT) architecture, introduced by Dosovitskiy et al. (2020), by replacing its attention mechanism with Orchid similar to language modeling task. Similar to language modeling task, we substitute the dense matrices in the MLP layers, which perform dimension mixing, with block-diagonal matrices and incorporate a residual long convolution within each Orchid block. We benchmark our model against recent long convolution-based models, specifically Hyena-ViT-b (Poli et al., 2023) and M2-ViT-b (Fu et al., 2023).

Models are evaluated for image classification on two widely used image datasets: CIFAR-10 and ImageNet-1K. For CIFAR-10, images are transformed into sequences of \(4 4\) pixel patches and processed using a ViT architecture composed of 6 Transformer layers with hidden sizes of either 128 and 220 for Orchid-s and Orchid-m, respectively. In the case of ImageNet-1K, we segmented images into patches of \(16 16\) pixels, and we trained a ViT-base architecture featuring 12 Transformer layers and a hidden size of 768.

The results presented in Table 4.4 and 4.5 demonstrate that Orchid significantly outperforms both the Vision Transformer baseline and long convolution-based models on the CIFAR-10 and ImageNet-1K datasets. Notably, Table 4.5 shows that utilizing smaller image patches, which lead to longer sequences, can further enhance performance. This observation underscores the advantage of scalable sequence models like Orchid. These results confirm the generalizability and effectiveness of the Orchid architecture beyond the domain of language modeling, highlighting its potential advantage in broader range of applications such as image processing tasks.

## 5 Related Work

Previous works have explored various forms of dynamic convolution architectures (Wu et al., 2019; Karami et al., 2019; Chen et al., 2020; Jiang et al., 2020). The dynamic convolution in (Wu et al., 2019) utilizes a short convolution kernel that depends solely on the current time-step, whereas (Jiang et al., 2020) expands the kernel span to depend on a local window. Meanwhile, Chen et al. (2020) modeled the convolution kernel as a mixture of short convolution kernels with mixture weights controlled by average pooling of the input embedding. However, the reliance on short convolutions and their specific kernel modeling approaches limits their ability to capture long-range dependencies and scaling their kernel to match the sequence length is computationally impractical. In a different line of research, Fourier Neural Operator (FNO) (Li et al., 2020) and adaptive FNO (Guibas et al.,

  
**Model (size)** & **GLUE Score** & \(\)**Params** & \(\)**GLUE Score** \\   BERT-base (110M) & 79.6 & - & - \\ M2-BERT-base (80M) & 79.9 & -27.3\% & +0.3 \\ Orchid-BERT-base (77M) & **80.6** & **-30.0\%** & +1.0 \\  BERT-large (340M) & 82.1 & - & - \\ M2-BERT-large (260M) & 82.2 & -23.6\% & +0.1 \\ Orchid-BERT-large (254M) & **82.7** & **-25.3\%** & +0.6 \\   

Table 4.3: Average GLUE Score of BERT-base and BERT-large (Devlin et al., 2018) in comparison to Orchid-BERT-base and Orchid-BERT-base, and M2-BERT-base and M2-BERT-large Dao et al. (2022). Baseline results are drawn from (Fu et al., 2023).

2021) operate in the spectral domain, applying a dense linear layer or an MLP with a block diagonal linear layer. However, these models do not explicitly model convolution kernel and do not enforce shift-equivariance.

Recent advances in input-dependent state space models (SSMs) have shown promise in efficient sequence modeling by allowing model parameters to dynamically adapt based on the input (Gu and Dao, 2023). However, the input-dependent mechanisms in these models typically rely on the current token or its local neighbors, preventing them from leveraging the benefits of global convolution for efficient parallelizable training Consequently, they heavily depend on hardware-aware implementations optimized for modern GPUs. While effective in certain tasks, these models have been shown to struggle with recall-intensive scenarios (Arora et al., 2024). In contrast, our proposed data-dependent global convolution allows the conditioning network to be influenced by the entire input context, enabling efficient sequence mixing. Moreover, the current formulation of input-dependent SSMs is not readily adaptable for efficient cross-attention between sequence pairs. This highlights a promising future direction for research: exploring how the complementary strengths of data-dependent global convolutions and input-dependent SSMs can be combined to develop foundation models that excel across a broader range of tasks.

Recent studies have explored sub-quadratic sequence mixing methods using long convolutions or state space models, leveraging the fast convolution algorithm for computational efficiency. While utilizing Fast Fourier transform results in \((L L)\) computational complexity, FFT algorithms exhibit suboptimal hardware utilization and suffer from slow I/O between layers of the memory hierarchy on modern GPUs due to their sequential nature. To address this bottleneck, FlashFFT-Conv (Fu et al., 2023) utilizes a matrix decomposition to leverage matrix multiply units and enable kernel fusion resulting in a more hardware and I/O efficient implementation of long convolutions. Moreover, the Monarch Mixer (M2) (Fu et al., 2023), offers an expressive family of sub-quadratic structured matrices that generalizes the DFT and other structures. These matrices are parameterized as products of block-diagonal matrices, offering sub-quadratic computation costs ranging from \((L L)\) to \((L^{3/2})\). By trading-off computational complexity with FLOP utilization, M2 achieves a hardware-efficient alternative for Transformers.

  
**Model (size)** & Top-1(\%) & **Model (size)** & Top-1(\%) \\   _CIFAR-10_ (\(4 4\)) & & \\ ViT (1.2M) & 78.6 & CKConv (1M) & 63.74 \\ ViT+Monarch (607K) & 79.0 & S4 (7.8M) & 91.13 \\ Hyena-ViT (1.3M) & 80.6 & M2-ViT (741K) & 91.0 \\ M2-ViT (741K) & 80.8 & CCNN (2M) & 93.08 \\ Orchid-s (735K) & **88.5** & Orchid-m (2M) & 93.0 \\    
  
**Model (size)** & Top-1(\%) \\   _CIFAR-10_ (\(1 1\)) & \\  ViT (1.2M) \\ ViT+Monarch (607K) \\ Hyena-ViT (1.3M) \\ M2-ViT (741K) \\ Orchid-s (735K) \\  & 78.6 \\    
  
**Model (size)** & Top-1(\%) \\   _CIFAR-10_ (\(2 2\)) & \\  Orichid-s (790K) \\ Orchid-s-cc (799K) \\ Orchid-m (2.1M) \\  & 92.2 \\  
  
**Model (size)** & **Top-1(\%)** \\   _CIFAR-10_ (\(2 2\)) & \\ 
 Orichid-s (790K) \\ Orchid-s-cc (799K) \\ Orchid-m (2.1M) \\  & 93.33 \\ 

Table 4.5: Performance comparison of Orchid with ViT-based models on CIFAR-10 dataset. Orchid’s performance is also evaluated over different patch sizes, \(4 4\), \(2 2\) and \(1 1\) pixels. Orchid-s and Orchid-m refers to the ViT architecture composed of 6 layers with hidden sizes of 128 and 220, respectively. Cross-Correlation (Type II) conditioning network (equation 3) is identified with -cc and the rest are using type I (equation 2). Baseline results are drawn from (Fu et al., 2023) and (Knigge et al., 2023).

Discussion and Conclusion

In conclusion, our work introduces Orchid, a novel model that addresses some critical challenges of efficiency and scalability in sequence modeling through the innovative use of data-dependent convolution. Orchid successfully mitigates the quadratic computational and memory costs associated with attention layers, while retaining, and in many cases enhancing, the model performance across different domains. The introduction of a data-dependent convolution layer represents a significant step forward, offering a scalable and expressive alternative sequence mixing scheme that adapts its weights to the input data. Through evaluation across multiple domains, including in-context learning, language and image processing tasks, Orchid has demonstrated not only its superior performance over traditional attention-based models but also its generality and scalability. This positions the Orchid model not just as a alternative to existing paradigms but as a potential catalyst for innovation, driving the exploration of novel, efficient, and powerful architectures in artificial intelligence.

The superior performance of Orchid compared to traditional transformer-based models raises intriguing questions about the current state and future direction of deep learning architectures. One plausible explanation for our model's effectiveness could be the over-parameterization prevalent in transformer-based models. A growing body of evidence indicates that attention mechanisms, despite their computational complexity, utilize only a fraction of their capabilities for tasks like language processing. This challenges the common belief that attention is the key ingredient for large-scale deep learning, leading us to reconsider its role and seek more computationally efficient alternatives.

#### Limitations and Future Directions

Looking ahead, extending our model to accommodate causal models, particularly for autoregressive language models akin to GPT, is an intriguing future direction. The current form of the proposed model is not inherently compatible with these architectures, primarily due to differences in how data-dependent global convolution handle dependencies and sequence generation.3

Furthermore, exploring the capability of Orchid as an efficient alternative to cross-attention layer, employed in sequence-to-sequence models, offers another avenue for research. These considerations open up new possibilities for integrating our model into more advanced foundation models and cross-domain applications.

Beyond sequence modeling, the Orchid block, with its input-dependent long convolution, local depthwise linear convolution (Conv1d), and element-wise multiplications, is inherently extendable to multi-dimensional data. While the primary focus of this work was designing an efficient and scalable architecture specifically for sequence modeling, expanding the proposed architecture to include 2D or 3D long convolutional approaches is an interesting future direction.