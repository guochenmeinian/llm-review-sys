# FilterNet: Harnessing Frequency Filters for

Time Series Forecasting

 Kun Yi\({}^{1,2}\), Jingru Fei\({}^{3}\), Qi Zhang\({}^{4}\), Hui He\({}^{3}\), Shufeng Hao\({}^{5}\), Defu Lian\({}^{6}\), Wei Fan\({}^{7}\)

\({}^{1}\)North China Institute of Computing Technology, \({}^{2}\)State Information Center of China

\({}^{3}\)Beijing Institute of Technology, \({}^{4}\)Tongji University, \({}^{5}\)Taiyuan University of Technology

\({}^{6}\)University of Science and Technology of China, \({}^{7}\)University of Oxford

kunyi.cn@gmail.com, {jingrufei, hehui617}@bit.edu.cn, zhangqi_cs@tongji.edu.cn

haoshufeng@tyut.edu.cn, liandefu@ustc.edu.cn, weifan.oxford@gmail.com

Corresponding author

###### Abstract

Given the ubiquitous presence of time series data across various domains, precise forecasting of time series holds significant importance and finds widespread real-world applications such as energy, weather, healthcare, etc. While numerous forecasters have been proposed using different network architectures, the Transformer-based models have state-of-the-art performance in time series forecasting. However, forecasters based on Transformers are still suffering from vulnerability to high-frequency signals, efficiency in computation, and bottleneck in full-spectrum utilization, which essentially are the cornerstones for accurately predicting time series with thousands of points. In this paper, we explore a novel perspective of enlightening _signal processing_ for deep time series forecasting. Inspired by the _filtering_ process, we introduce one simple yet effective network, namely _FilterNet_, built upon our proposed learnable _frequency filters_ to extract key informative temporal patterns by selectively passing or attenuating certain components of time series signals. Concretely, we propose two kinds of learnable filters in the FilterNet: (i) Plain shaping filter, that adopts a universal frequency kernel for signal filtering and temporal modeling; (ii) Contextual shaping filter, that utilizes filtered frequencies examined in terms of its compatibility with input signals for dependency learning. Equipped with the two filters, FilterNet can approximately surrogate the linear and attention mappings widely adopted in time series literature, while enjoying superb abilities in handling high-frequency noises and utilizing the whole frequency spectrum that is beneficial for forecasting. Finally, we conduct extensive experiments on eight time series forecasting benchmarks, and experimental results have demonstrated our superior performance in terms of both effectiveness and efficiency compared with state-of-the-art methods. Our code is available at 1.

## 1 Introduction

Time series forecasting has been playing a pivotal role across a multitude of contemporary applications, spanning diverse domains such as climate analysis , energy production , traffic flow patterns , financial markets , and various industrial systems . The ubiquity and profound significance of time series data has recently garnered substantial research efforts, culminating in a plethora of deep learning forecasting models  that have significantly enhanced the domain of time series forecasting.

Previously, leveraging different kinds of deep neural networks derives a series of time series forecasting methods, such as Recurrent Neural Network-based methods including DeepAR , LSTNet ,Convolution Neural Network-based methods including TCN , SCINet , etc. Recently, however, with the continuing advancement of deep learning, two branches of methods that received particularly more attention have been steering the development of time series forecasting, i.e., Multilayer Perceptron (MLP)-based methods, such as N-BEATS , DLinear , and FreTS , and Transformer-based methods, such as Informer , Autoformer , PatchTST , and iTransformer . While MLP-based models are capable of providing accurate forecasts, Transformer-based models continue to achieve state-of-the-art time series forecasting performance.

However, forecasters based on Transformers are still suffering from _vulnerability_ to high-frequency signals, _efficiency_ in computation, and _bottleneck_ in full-spectrum utilization, which essentially are the cornerstones for accurately predicting time series composed of thousands of timesteps. In designing a very simple simulation experiment on the synthetic data only composed of a low-, middle- and high-frequency signal respectively (see Figure 1(a)), we find the state-of-the-art iTransformer  model performs much worse in forecasting (Figure 1(b) and Figure 1(c)). This observation shows that state-of-the-art Transformer-based model cannot utilize the full spectrum information, even for a naive signal of three different frequency components. In contrast, in the field of _signal processing_, a _frequency filter_ enjoys many good properties such as frequency selectivity, signal conditioning, and multi-rate processing. These could have great potential in advancing the model's ability to extract key informative frequency patterns in time series forecasting.

Thus, inspired by the _filtering_ process  in signal processing, in this paper, we introduce one simple yet effective framework, namely _FilterNet_, for effective time series forecasting. Specifically, we start by proposing two kinds of learnable filters as the key units in our framework: (i) Plain shaping filter, which makes the naive but universal frequency kernel learnable for signal filtering and temporal modeling, and (ii) Contextual shaping filter, which utilizes filtered frequencies examined in terms of its compatibility with input signals for dependency learning. The plain shaping filter is more likely to be adopted in predefined conditions and efficient in handling simple time series structures, while the contextual filter can adaptively weight the filtering process based on the changing conditions of input and thus have more flexibility in facing more complex situations. Besides, these two filters as the built-in functions of the FilterNet can also approximately surrogate the widely adopted linear mappings and attention mappings in time series literature [12; 14; 17]. This also illustrates the effectiveness of our FilterNet in forecasting by selectively passing or attenuating certain signal components while capturing the core time series structure with adequate learning expressiveness. Moreover, since filters are better fit in the stationary frequency domain, we let filters wrapped by two reversible transformations, i.e., instance normalization  and fast Fourier transform  to reduce the influence of non-stationarity and accomplish the domain transformation of time series respectively. In summary, our contributions can be listed as follows:

* In studying state-of-the-art deep Transformer-based time series forecasting models, an interesting observation from a simple simulation experiment motivates us to explore a novel perspective of enlightening _signal processing_ techniques for deep time series forecasting.
* Inspired by the _filtering_ process in signal processing, we introduce a simple yet effective network, _FilterNet_, built upon our proposed two learnable frequency filters to extract key informative temporal patterns by selectively passing or attenuating certain components of time series signals, thereby enhancing the forecasting performance.
* We conduct extensive experiments on eight time series forecasting benchmarks, and the results have demonstrated that our model achieves superior performance compared with state-of-the-art forecasting algorithms in terms of effectiveness and efficiency.

Figure 1: Performance of Mean Squared Error (MSE) on a simple synthetic multi-frequency signal. More details about the experimental settings can be found in Appendix C.4.

[MISSING_PAGE_FAIL:3]

## 4 Methodology

As aforementioned, frequency filters enjoy numerous advantageous properties for time series forecasting, functioning equivalently to circular convolution operations in the time domain. Therefore, we design the time series forecaster from the perspective of frequency filters. In this regard, we propose _FilterNet_, a forecasting architecture grounded in frequency filters. First, we introduce the overall architecture of FilterNet in Section 4.1, which primarily comprises the basic blocks and the frequency filter block. Second, we delve into the details of two types of frequency filter blocks: the _plain shaping filter_ presented in Section 4.2 and the _contextual shaping filter_ discussed in Section 4.3.

### Overview

The overall architecture of FilterNet is depicted in Figure 2, which mainly consists of the instance normalization part, the frequency filter block, and the feed-forward network. Specifically, for a given time series input \(=[X_{1}^{1:L},X_{2}^{1:L},...,X_{N}^{1:L}]^{N L}\) with the number of variables \(N\) and the lookback window length \(L\), where \(X_{1}^{1:L}^{L}\) denotes the \(N\)-th variable, we employ FilterNet to predict the future \(\) time steps \(=[X_{1}^{L+1:L+},X_{2}^{L+1:L+},...,X_{N}^{L+1:L+}] ^{N}\). We provide further analysis about the architecture design of FilterNet in Appendix A.

Instance NormalizationNon-stationarity is widely existing in time series data and poses a crucial challenge for accurate forecasting [19; 36]. Considering that time series data are typically collected over a long duration, these non-stationary sequences inevitably expose forecasting models to distribution shifts over time. Such shifts can result in performance degradation during testing due to the covariate shift or the conditional shift . To address this problem, we utilize an instance normalization method, denoted as \(\), on the time series input \(\), which can be formulated as:

\[()=[^{1:L}-_{L}(X_{i}^{1:L})}{ _{L}(X_{i}^{1:L})}]_{i=1}^{N},\] (2)

where \(_{L}\) denotes the operation that calculates the mean value along the time dimension, and \(_{L}\) represents the operation that calculates the standard deviation along the time dimension.

Correspondingly, the inverse instance normalization, denoted as \(\), is formulated as:

\[()=[P_{i}^{L+1:L+}_{L}(X_{ i}^{1:L})+_{L}(X_{i}^{1:L})]_{i=1}^{N},\] (3)

where \(=[P_{1}^{L+1:L+},P_{2}^{L+1:L+},...,P_{N}^{L+1:L+}] ^{N}\) are the predictive values.

Figure 2: The overall architecture of FilterNet. (i) Instance normalization is employed to address the non-stationarity among time series data; (ii) The frequency filter block is applied to capture the temporal patterns, which has two different implementations, i.e., plain shaping filter and contextual shaping filter; (iii) Feed-forward network is adopted to project the temporal patterns extracted by frequency filter block back onto the time series data and make predictions.

Frequency Filter BlockPrevious representative works primarily leverage MLP architectures (e.g., DLinear , RLinear ) or Transformer architectures (e.g., PatchTST , iTransformer ) to model the temporal dependencies among time series data. As mentioned earlier, time series forecasters can be implemented through performing a _frequency filter process_ in the frequency domain, and thus we propose to directly apply the frequency filter in the frequency domain, denoted as \(\), to replace the aforementioned methods for modeling corresponding temporal dependencies, such as:

\[()=^{-1}(() _{filter}),\] (4)

where \(\) is Fourier transform, \(^{-1}\) is inverse Fourier transform and \(_{filter}\) is the frequency filter.

Inspired by MLP that randomly initializes a learnable weight parameters and Transformer that learns the data-dependent attention scores from data (further explanations are provided in Appendix B), we introduce two types of frequency filters, i.e., _plain shaping filter_ (\(\)) and _contextual shaping filter_ (\(\)). \(\) applies a random initialized learnable weight \(_{}\) to instantiate the frequency filter \(_{filter}\), and then the frequency filter process is reformulated as:

\[()=^{-1}(() _{}).\] (5)

\(\) learns a data-dependent frequency filter \(_{}(())\) from the input data by using a neural network \(_{}()\), and then the corresponding frequency filter process is reformulated as:

\[()=^{-1}(() _{}(())).\] (6)

Feed-forward NetworkThe frequency filter block has captured temporal dependencies among time series data, and then we employ a feed-forward network (\(\)) to project them back onto the time series data and make predictions for the future \(\) time steps. As the output \(\) of \(\) are instance-normalized values, we conduct an inverse instance normalization operation (\(\)) on them and obtain the final predictions \(}\). The entire process can be formulated as follows:

\[ =(),\] (7) \[} =().\]

### Plain Shaping Filter

\(\) instantiates the frequency filter by randomly initializing learnable parameters and then performing multiplication with the input time series. In general, for multivariate time series data, the channel-independence strategy in channel modeling has proven to be more effective compared to the channel-mixing strategy [12; 16]. Following this principle, we also adopt the channel-independence strategy for designing the frequency filter. Specifically, we propose two types of plain shaping filters: the universal type, where parameters are shared across different channels, and the individual type, where parameters are unique to each channel, as illustrated in Figure 3(a).

Given the time series input \(^{N L}\) and the plain shaping filter \(_{}\), we apply \(\) by:

\[ =(),\] (8) \[ =_{L}_{},_{} \{_{}^{(Uni)},_{}^{(Ind)}\}\] \[ =^{-1}(),\]

Figure 3: The structure of frequency filters. (a) Plain shaping filter: the plain shaping filter is initialized randomly with channel-shared (left) or channel-unique (right) parameters, and then performs circular convolution (i.e., the symbol ) with the input time series; (b) Contextual shaping filter: the contextual shaping filter firstly learns a data-dependent filter and then conducts multiplication (i.e., the symbol \(\)) with the frequency representation of the input time series.

where \(\) is Fourier transform, \(^{-1}\) is inverse Fourier transform, \(_{L}\) denotes the element-wise product along \(L\) dimension, \(_{}^{(Uni)}^{1 L}\) is the universal plain shaping filter, \(_{}^{(Ind)}^{N L}\) is the individual plain shaping filter, and \(^{N L}\) is the output of \(\). We further compare and analyze the two types of \(\) in Section 5.3.

### Contextual Shaping Filter

In contrast to \(\), which randomly initializes the parameters of frequency filters and fixes them after training, \(\) learns the parameters generated from the input data, allowing for better adaptation to the data. Consequently, we devise a neural network \(_{}\) that flexibly adjusts the frequency filter in response to the input data, as depicted in Figure 3(b).

Given the time series input \(^{N L}\) and its corresponding Fourier transform denoted as \(=()^{N L}\), the network \(_{}\) is utilized to derive the contextual shaping filter, expressed as \(_{}:^{N L}^{N D}\). First, it embeds the raw data by a linear dense operation \(:^{L}^{D}\) to improve the capability of modeling complex data. Then, it applies a series of complex-value multiplication with \(K\) learnable parameters \(_{1:K}^{1 D}\) yielding \((()_{1:K})\) where \(\) is the activation function, and finally outputs \(_{}()\). Then we apply \(\) by:

\[=(),\] (9) \[=(),\] \[_{}()=(_{D} _{1:K}),_{1:K}=_{i=1}^{K}_{i}\] \[=_{D}_{}(),\] \[=^{-1}(),\]

where \(_{D}\) denotes the element-wise product along \(D\) dimension and \(^{N D}\) is the output. The contextual shaping filter can adaptively weight the filtering process based on the changing conditions of input and thus have more flexibility in facing more complex situations.

## 5 Experiments

In this section, we extensively experiment with eight real-world time series benchmarks to assess the performance of our proposed FilterNet. Furthermore, we conduct thorough analytical experiments concerning the frequency filters to validate the effectiveness of our proposed framework.

### Experimental Setup

DatasetsWe conduct empirical analyses on diverse datasets spanning multiple domains, including traffic, energy, and weather, among others. Specifically, we utilize datasets such as ETT datasets , Exchange , Traffic , Electricity , and Weather , consistent with prior studies on long time series forecasting [16; 17; 25]. We preprocess all datasets according to the methods outlined in [16; 17], and normalize them with the standard normalization method. We split the datasets into training, validation, and test sets in a 7:2:1 ratio. More dataset details are in Appendix C.1.

BaselinesWe compare our proposed FilterNet with the representative and state-of-the-art models to evaluate their effectiveness for time series forecasting. We choose the baseline methods from four categories: (1) Frequency-based models, including FreTS  and FITS ; (2) TCN-based models, such as MICN  and TimesNet ; (3) MLP-based models, namely DLinear  and RLinear ; and (4) Transformer-based models, which include Informer , Autoformer , Pyraformer , FEDformer , PatchTST , and the more recent iTransformer  for comparison. Further details about the baselines can be found in Appendix C.2.

Implementation DetailsAll experiments are implemented using Pytorch 1.8  and conducted on a single NVIDIA RTX 3080 10GB GPU. We employ MSE (Mean Squared Error) as the loss function and present MAE (Mean Absolute Errors) and MSE (Mean Squared Errors) results as the evaluation metrics. For further implementation details, please refer to Appendix C.3.

[MISSING_PAGE_FAIL:7]

Shared vs. Unique Filters Among ChannelsTo analyze the different channel strategies of filters, we further conduct experiments on the ETH and Exchange datasets. Specifically, we compare forecasting performance under different prediction lengths between two different types of frequency filters, i.e., \(_{}^{(Uni)}\) and \(_{}^{(Ind)}\). In \(_{}^{(Uni)}\), filters are shared across different channels, whereas \(_{}^{(Ind)}\) signifies filters unique to each channel. The evaluation results are presented in Table 2. It demonstrates that filters shared among different channels consistently outperform across all prediction lengths. In addition, we visualize the prediction values predicted on the ETH1 dataset by the two different types of filters, as illustrated in Figure 11 (see Appendix G.1). The visualization reveals that the prediction values generated by filters shared among different channels exhibit a better fit than the unique filters. Therefore, the strategy of channel sharing seems to be better suited for time series forecasting and filter designs, which is also validated in DLinear  and PatchTST .

Visualization of PredictionWe present a prediction showcase on ETH1 dataset, as shown in Figure 5. We select iTransformer , PatchTST  as the representative compared methods. Comparing with these different state-of-the-art models, we can observe FilterNet delivers the most accurate predictions of future series variations, which has demonstrated superior performance. In addition, we include more visualization cases and please refer to Appendix G.3.

Visualization of Frequency FiltersTo provide a comprehensive overview of the frequency response characteristics of frequency filters, we conduct visualization experiments on the Weather, ETH, and Traffic datasets with the lookback window length of 96 and the prediction length of 96. The frequency response characteristics of learned filters are visualized in Figure 7. From Figures 7(a) and 7(b), we can observe that compared with Transformer-based approaches (e.g., iTransformer , PatchTST ) tend to attenuate high-frequency components and preserve low-frequency information, FilterNet exhibits a more nuanced and adaptive filtering behavior that can be capable of attending to all frequency components. Figure 7(c) demonstrates that the main patterns of the Traffic dataset primarily resides in the low-frequency range. This observation also explains why iTransformer performs well on the Traffic dataset, despite its low-frequency nature. Overall, Figure 7 demonstrates

   Datasets &  &  \\  Lengths &  &  &  &  &  &  &  &  \\  Metrics & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE \\  \(_{}^{(Uni)}\) & 0.375 & 0.394 & 0.436 & 0.422 & 0.476 & 0.443 & 0.474 & 0.469 & 0.083 & 0.202 & 0.174 & 0.296 & 0.326 & 0.413 & 0.840 & 0.670 \\ \(_{}^{(Ind)}\) & 0.382 & 0.402 & 0.430 & 0.429 & 0.472 & 0.451 & 0.481 & 0.473 & 0.091 & 0.211 & 0.186 & 0.305 & 0.380 & 0.449 & 0.896 & 0.712 \\   

Table 2: Performance evaluation of forecasting using two different kinds of frequency filters on the ETH1 and Exchange datasets with a lookback window size of 96 and the prediction lengths \(\{96,192,336,720\}\). Results highlighted in red indicate the best performance.

Figure 4: Predictions produced by FilterNet on trend and multi-periodic signals with noises. When adding noises for interference, FilterNet can perform more robust forecasting than iTransformer .

Figure 5: Visualization of prediction on the ETH1 dataset with lookback and horizon length as 96.

that FilterNet possesses comprehensive processing capabilities. Moreover, visualization experiments conducted on the ETTm1 dataset across various prediction lengths, as shown in Figure 8, further illustrate the extensive processing abilities of FilterNet. Additional results conducted on different lookback window lengths and prediction lengths can be found in Appendix G.2.

Efficiency AnalysisThe complexity of FilterNet is \(( L)\) where \(L\) is the input length. To comprehensively assess efficiency, we evaluate it based on two dimensions: memory usage and training time. Specifically, we choose two different sizes of datasets: the Exchange (8 variables, 7588 timestamps) and Electricity datasets (321 variables, 26304 timestamps). We compare the efficiency of our FilterNet with the representative Transformer- and MLP-based methods under the same settings (lookback window length of 96 and prediction length of 96), and the results are shown in Figure 6. It highlights that FilterNet surpasses other Transformer models, regardless of dataset size. While our approach exhibits similar efficiency to DLinear, our effective results outperform its performance. In Appendix E, we further conduct ablation studies to validate the rationale of FilterNet designs.

## 6 Conclusion Remarks

In this paper, we explore an interesting direction from a signal processing perspective and make a new attempt to apply frequency filters directly for time series forecasting. We propose a simple yet effective architecture, _FilterNet_, built upon our proposed two kinds of frequency filters to accomplish the forecasting. Our comprehensive empirical experiments on eight benchmarks have validated the superiority of our proposed method in terms of effectiveness and efficiency. We also include many careful and in-depth model analyses of FilterNet and the internal filters, which demonstrate many good properties. We hope this work can facilitate more future research integrating signal processing techniques or filtering processes with deep learning on time series modeling and accurate forecasting.

Figure 8: Spectrum visualizations of filters learned on ETTm1 with different prediction lengths.

Figure 6: Model effectiveness and efficiency comparison on the Exchange and Electricity datasets.

Figure 7: Spectrum visualizations of filters learned on the Weather, ETTh1, and Traffic datasets.