# RAGraph: A General Retrieval-Augmented Graph Learning Framework

Xinke Jiang\({}^{}\)

###### Abstract

Graph Neural Networks (GNNs) have become essential in interpreting relational data across various domains, yet, they often struggle to generalize to unseen graph data that differs markedly from training instances. In this paper, we introduce a novel framework called General **R**etrieval-**A**ugmented **G**raph** Learning (**RAGraph**), which brings external graph data into the general graph foundation model to improve model generalization on unseen scenarios. On the top of our framework is a toy graph vector library that we established, which captures key attributes, such as features and task-specific label information. During inference, the **RAGraph** adeptly retrieves similar toy graphs based on key similarities in downstream tasks, integrating the retrieved data to enrich the learning context via the message-passing prompting mechanism. Our extensive experimental evaluations demonstrate that **RAGraph** significantly outperforms state-of-the-art graph learning methods in multiple tasks such as node classification, link prediction, and graph classification across both dynamic and static datasets. Furthermore, extensive testing confirms that **RAGraph** consistently maintains high performance without the need for task-specific fine-tuning, highlighting its adaptability, robustness, and broad applicability.

## 1 Introduction

**Graph Neural Networks (GNNs)** have recently burgeoned a surge of interest in both academic and industry communities due to their robust capability to model complex, real-world data in diverse domains, including societal , biochemical , and traffic-related  fields and etc . Utilizing a message-passing mechanism , GNNs have transcended traditional node embedding approaches , enabling the capture of intricate relationships within data through sophisticated architectures and advanced graph representation learning techniques . However, the challenge of generalizing GNNs across different modalities, domains , and tasks remains largely unexplored . This is in stark contrast to the significant successes of large models such as GPTs  in NLP and Sora  in CV, presenting a crucial frontier for further research and realms for graph data generalizing.

In graph learning tasks, providing the necessary context is crucial for graph generalization , _i.e.,_ retrieve similar shopping context as illustrated in Figure 1 (c). Therefore, our insight is to enhance the model's generalization ability and prediction accuracy by retrieving necessary contexts during graph learning through retrieval. **Retrieval-Augmented Generation (RAG)** represents a prominent methodology, significantly augmenting language model functionalities through the integration of a dynamic retrieval mechanism during the generation process  (_e.g._, a person asks what animal it is, and we use some visual  or text retrieval  methods to retrieve more descriptive features or even the wanted category). RAG enriches not only accurate and reliable content but also reduces factual errors, addressing challenges such as incorrect answers, hallucinations, and limited interpretability in knowledge-intensive tasks , obviating the need for updating model parameters and could be generalized even in unseen scenarios.

However, how to enable retrieval-augmented generation for graph learning, _i.e.,_ retrieving the user's historical purchasing behavior to enhance recommendation ability  and identifying fraud crimes by searching for similar fraudulent relationship behaviors , still remains unexplored and faces the following challenges _CI& C2_.

_C1._ The first challenge is how to leverage the retrieved context _i.e.,_ features (\(X\)) and labels (\(Y\)) into the GNNs model under dynamic changing scenarios. Previous studies, such as PRODIGY , have adopted the concept of in-context learning (ICL) by constructing consistent and static task graphs for each specific task or dataset. These task graphs determine labels through the calculation of similarities using hidden vectors, employing a few-shot learning approach. However, PRODIGY's reliance on a fixed set of examples as rules may not sufficiently address and generalize the variety of scenarios encountered in real-world settings, which is particularly problematic in dynamically changing environments, as the system focuses primarily on teaching the direct mapping paradigm from inputs to outputs (\(X Y\)), rather than truly integrate the input (\(X\)) and output (\(Y\)) data into the analysis. In contrast to RAG, PRODIGY struggles to incorporate external information (\(X\) and \(Y\)) related to data nodes, which is crucial for enriching the learning process in graph-based systems.

_C2._ Moreover, it is challenging to develop a tune-free prompt mechanism to support retrieved knowledge and be applicable to seamlessly switch unseen scenarios and multi-tasks. Numerous initiatives have been undertaken in the realm of graph pre-training , however, the challenge persists in designing a plug-and-play RAG module that can seamlessly interface with already pre-trained models. Insights derived from prior investigations into the graph prompt , the knowledge obtained by RAG can be facilitated and injected into prompt via a plug-and-play manner.

For endeavoring to address these two challenges previously mentioned, we put forward the General **R**etrieval-**A**ugmented **Graph** Learning Framework (**RAGraph**). Drawing inspiration from the success of RAG on LLMs  and the ICL on GNNs  (we detail the difference between RAG and ICL in Appendix E), we constructed a toy graphs vector library by chunking from resource graphs, where the library key stores key information, including environmental, historical, structural, and semantic details, while node features and label information (task-specific output vector) are stored as values. For downstream tasks, the key value of the query node would be leveraged to retrieve toy graphs by the key similarities,

Figure 1: **(a)** RAG in NLP utilizes retrieval to enhance model responses, based on a query to retrieve related features (_e.g., a tail, primarily feeds on mice_) and answers (_e.g., Cat_). **(b)** In CV, RAG employs similar photo retrieval to enhance model comprehension, assisting in downstream tasks such as inpainting or image question answering. **(c)** For GNNs, RAG could leverage retrieval of similar historical subgraphs or scenarios to aid in graph-based tasks (_e.g.,_ recommendations or fraud detection).

and the stored features (X) and labels (Y) would be aggregated structurally to provide essential knowledge to the query node, instead of the mapping paradigm, to address challenge _C1_. In prompt mechanism design, we start by transferring features and task-specific output from the toy graphs to their master nodes (the central node of the toy graph) via message-passing. Subsequently, features from the master nodes and the query node's neighbors are aggregated to the query node, along with the task-specific output from master nodes. This process could be parameter-free, indicating that our model can be applied across different tasks and datasets without the need to fine-tune for downstream tasks, effectively addressing challenge _C2_.

In summary, our contributions are listed as follows:

* To the best of our knowledge, our proposed framework, RAGraph, is the first to integrate RAG with pre-trained GNNs. By constructing a key-value vector library for toy graphs, RAGraph facilitates explicit plug-and-play access to pre-trained GNNs, achieving commendable performance even without fine-tuning, demonstrating its superiority on cross-task and cross-dataset capabilities.
* Our RAGraph employs a classic message-passing mechanism and introduces a well-designed prompt mechanism to integrate knowledge. This approach effectively incorporates the retrieved knowledge \(X\) and \(Y\) from toy graphs, into the pre-trained GNNs model, enhancing the accuracy and relevance of the model's outputs.
* We have extensively tested RAGraph on both static and dynamic graphs across multiple levels of graph tasks (node, edge, and graph). The results validate the effectiveness of our model, showing significant improvements over state-of-the-art baselines in both fine-tuned and tuning-free scenarios, particularly in cross-dataset validations.

## 2 Related Work

### Retrieval-Augmented Generation on Large Language Models

RAG integrates an external knowledge retrieval component and through prompt engineering into pre-trained language models to enhance factual consistency, thus improving the reliability and interpretability of LLM responses [134; 138; 51; 23; 45; 119; 59; 111; 130]. Traditional RAG approaches utilize retriever models to source relevant documents from extensive knowledge corpora [107; 83; 71; 49], which are then processed further by reader models--primarily LLMs [77; 85]. Furthermore, several studies focus on fine-tuning reader LLMs by applying prompt-tuning with retrieved knowledge or using RAG API calls [69; 42; 2; 116; 102; 131; 62]. While RAG has seen considerable success in the NLP field, it has also been applied to tasks involving joint visual and text retrieval [141; 61; 60; 8; 125; 10], code retrieval [68; 136], audio retrieval [6; 32] and video retrieval [3; 101]. Although there have been applications of RAG on structured data such as KG-RAG for knowledge graphs [48; 45; 87; 93; 94; 40], these primarily leverage the text information of knowledge graph nodes to enhance language or graph models. In contrast, there are no significant studies utilizing RAG on structured graphs without text information to enhance pre-trained GNNs. Our work aims to extend this successful approach similarly to graph data, to enhance the capabilities of pre-trained GNNs, and can be adapted to various tasks and across different graphs without additional fine-tuning by integrating a plug-and-play RAG module.

### Graph Prompt Learning

Inspired by the application of pre-training models [75; 76] and prompt learning [103; 136; 43] in NLP, recently, learning on the graph has been divided into pre-training models on large-scale graph data [34; 117; 35; 82; 39; 105; 90; 120; 122; 123; 121; 124], with or without labels, followed by fine-tuning model parameters via prompts for diverse downstream tasks [67; 114; 39; 140; 90; 95]. The adoption of prompting mechanisms in graph learning represents a promising avenue to overcome the constraints of traditional graph representation methods, striking a balance between flexibility and expressiveness . For instance, VNT  utilizes virtual nodes as prompts to refine the application of pre-trained graph models. GraphPrompt  introduces a task-specific readout mechanism to tailor models for various tasks, while GraphPro  implements spatial- and temporal-based gating mechanisms suited for dynamic recommendation systems. Furthermore, PRODIGY  constructs task graphs (prompts) and data graphs to enhance the model's ICL capabilities. Leveraging the successes in graph prompt learning, we aim to inject retrieved knowledge via prompt into pre-trained GNNs to support downstream tasks.

Preliminaries

In RAGraph, we focus on RAG on multi-level graph tasks. For consistency, we define the graphs as dynamic graphs, considering static graphs as the special cases within this framework. The subsequent definition provides a detailed description of toy graphs, including the definitions of keys and values utilized in RAGraph. Additionally, inspired by GraphPrompt , we have unified node-level, edge-level, and graph-level tasks into a cohesive framework, and employ query graphs to tackle downstream tasks with precision.

Definition 1. (Dynamic Graph)Let \(\!=\!\{G_{t}\}_{t=1}^{T}\) denote a dynamic graph comprising a sequence of graph snapshots, each represented as a static graph \(G_{t}\!=\!(V_{t},\!E_{t},\!X_{t},\!A_{t},\!Y_{t})\). \(\!=\!_{t=1}^{T}\!V_{t}\!=\!\{v_{1},\!...,\!v_{n}\}\) defines the combined set of nodes across all snapshots and \(=_{t=1}^{T}E_{t}\) is the edge set, where \(V_{t}\) and \(E_{t}\) represent the nodes and edges of the \(t\)-th snapshot, respectively. Feature matrix \(X_{t}\!=\!\{x_{v}\,|\,v\}\!\!^{n d}\) contains the feature vectors for the nodes in the \(t\)-th snapshot, where \(d\) is the feature dimension. \(A_{t}\) denotes the edge weight matrix at time \(t\), where edge weight \(A_{t}[i,j]\!\!(0,\!1]\) if \(v_{i},\!v_{j}\!\!V_{t}\) and \((v_{i},\!v_{j}) E_{t}\), and 0 otherwise. Furthermore, \(Y_{t}\) represents the task-related labels associated with nodes, edges, or the graph at time \(t\). Note that a graph is static if \(T=1\) and for consistency in terminology, we unify static graphs as a particular instance of dynamic graphs.

Definition 2. (Toy Graph Vector Base)Let \(^{}=\{G^{}_{t}\}_{t=1}^{T}\) denote a dynamic resource graph. We chunk \(^{}\) into snapshots and take each node in \(^{}\) as the master node \(v_{m}\) of the corresponding toy graph, and then store \(v_{m}\) with its neighbors within \(k\) hops as subgraphs. Data augmentation techniques [56; 135] such as node dropout, edge dropout, and random noise addition are employed on subgraphs to enhance the robustness and variability when generating each toy graph \(G^{}\) (c.f. Section 4.1 for details). Each toy graph \(G^{}\!\!^{}\) is associated with a specific timestamp \(\) and master node \(v_{m}\!\!\), with each toy graph's scale being considerably smaller in scale compared to their corresponding \(^{}\).

\(\) Toy graphs can be retrieved using **keys** that include the timestamp \(\), the hidden embedding of the master node \(h^{}_{m}\!\!^{f_{1}}\) (_e.g.,_ embedded by pre-trained GNNs in RAGraph), the environmental key (_e.g.,_ the neighbors set \((v^{}_{m})\!=\!\{v^{}_{i}\,|\,A_{}[m,i]\!>\!0,\!v^{} _{i}\!\!G^{}\}\)) and the structure-based position-aware code \(s^{}_{m}\) (cf. Appendix C.2 for details).

\(\) By retrieving based on key similarity (c.f. Section 4.2 for details), we can obtain the required **values** of \(G^{}\), _i.e._ task-specific output vector \(\{v^{}_{i}\!\!^{f_{2}}|v_{i}\!\!G^{}\}\) and hidden embeddings \(\{h^{}_{i}\!\!^{f_{1}}|v_{i}\!\!G^{}\}\) of the master node and its neighbors, where \(f_{1}\) and \(f_{2}\) represent the dimensions. Finally, we denote the **key-value** vector base for the toy graph as \(^{}\).

Definition 3. (A Unified Graph Task Definition)Given a dynamic graph \(\), it can be divided into training and testing subsets, _i.e._\(\!=\!_{ train}\!_{ test}\) based on either snapshot or node set partitioning. The label \(y_{i}\) of a node \(v_{i}\), edge \((v_{i},\!v_{j})\) or subgraph \(G_{i}\) can be observed only if they belong to \(_{ train}\). The objective of label prediction is to predict test labels \(Y_{ test}\!\!_{ test}\). Following GraphPrompt , we unify the three types of graph learning tasks (node-level, edge-level, and graph-level) into a single framework via similarity comparison \((,)\) of the task-specific output vector (abbreviated as \(O\), where each entry is \(o\)) with the ground-truth (_i.e.,_ the one-hot vector or the prototype embedding under few-shot setting). It's noted that \(o\) can be either low-dimensional (with the dimension equal to the number of predicted classes) under normal settings [50; 129], or high-dimensional under few-shot settings  or in link prediction tasks [114; 31]. In our experiment,

\(\) for node-level and graph-level tasks, the downstream tasks are given in few-shot settings following : For node / graph classification on a node / graph set, let \(\) be the set of classes with \(y_{i}\!\!\) denoting the class label of node / graph. For each node / graph class, the class prototypical output vector is calculated by the mean value of the \(\)-shot set \(\): \(_{c}\!=\!_{(i,y_{i}),y_{i}=}\!o_{i}\). The class \(y_{i}\) of the node or graph is determined by calculating similarity with the class prototype as: \(y_{i}\!=\!*{argmax}_{c}(o_{i},\! _{c})\).

\(\) For edge-level tasks, to predict a link between nodes \(v_{i}\) and \(v_{q}\), if \( v_{j},(v_{i},v_{j})\!\!_{}\!\!_ {}\) and \((o_{i},\!o_{q})\!\!(o_{i},\!o_{j})\!+\!\), we regard \((v_{i},\!v_{q})\) as linked. Following PRODIGY  and GraphPrompt , we also apply a query graph \(G^{}\) that includes the center node and its neighbors within \(k\) hops. Specifically, for graph-level task, we apply a full-link virtual node as the center node inside the query graph \(G^{}\).

RAGraph Framework

In this section, we introduce RAGraph, a general and novel retrieval-augmented graph learning framework that can operate on arbitrary graphs with or without additional fine-tuning, as illustrated in Figure 2. Initially, in Section 4.1, we elucidate the methodology for constructing the Resource Toy Graphs. Subsequently, in Section 4.2 we detail the Toy Graphs Retrieval Process. Finally, the Training and Inference processes are elaborated in Section 4.3, which utilize retrieved toy graphs from two propagation views--intra and inter-propagation--and handle two types of information: hidden embeddings and task-specific output vectors in two techniques (noisy trainable approach or parameter-free approach). The main notations of RAGraph are summarized in Table 3, Appendix A. For enhanced clarity, the Toy Graph Construction is outlined in Algorithm 1 (cf. Appendix C.5) and the Training and Inference with Toy Graphs Retrieval are detailed in Algorithm 2 (cf. Appendix C.5). Moreover, in Appendix C.4, we theoretically prove the effectiveness of applying RAG on GNNs from the perspective of mutual information gain.

### Toy Graphs Embedding Pipeline

In graph-based learning, nodes with higher connectivity--typically with higher degrees--often hold more significance, meaning their information is more extensively learned during graph-pre-training processes. Conversely, less important nodes--those in the long tail--often have their features over

Figure 2: The overall framework of RAGraph. \(\) Given resource graph \(^{}\), we chunk it and augment toy graphs \(\{G^{}\}\), and feed them into pre-trained GNNs to generate hidden embeddings via the encoder and task-specific output vectors via decoder, which are stored as values. Keys such as environment, history, position-aware, and hidden embeddings are stored to form the key-value database of toy graphs \(^{}\). \(\) For a given query graph \(G^{}\), the keys are fetched to retrieve the \(topK\) toy graphs \(G^{}_{}\) from the database. \(\) Leveraging \(G^{}_{}\), intra- and inter-propagation are performed to propagate hidden embeddings and task-specific output vectors to pass retrieved knowledge to center node \(v_{c}\). Through a weighted fusion, the aggregated output is used to perform graph-, node- and edge-level tasks.

looked. This issue is particularly pronounced in LLMs performing RAG, where the predominance of common knowledge overshadows the long-tail knowledge that RAG is meant to leverage. To tackle this, we construct toy graphs using an inverse importance sampling strategy, thereby countering this bias by preferentially sampling and augmenting toy graphs that accentuate the long-tail knowledge.

Inverse Importance Sampling Strategy.To achieve this, we calculate each node's importance \(I(v)\) for node \(v G_{}^{}\) by combining PageRank \((v)\) and Degree Centrality \((v)\) using the formula \(I(v)\!=\!(v)\!+\!(1\!-\!)(v)\), where \(\!\!(0,\!1)\) is the balance weight. We reverse the node importance with \(I^{}(v)\!=\!,\!\!\!0\), normalize it to obtain node \(v_{i}\)'s sampling probabilities \(p_{i}\!=\!(v_{i})}{_{j=1}^{n}\!I^{}(v_{j})}\), and perform weighted sampling function WeightedSampling(\(G_{}^{}\), \(p_{i}\)) to prioritize nodes with higher sampling probability (lower importance) according to \(p_{i}\). When sampling, for each master node \(v_{m}\), we generate its \(k\)-hop neighbors, termed an ego net \(G_{}^{e}(v_{m})\). Given the constrained size of the resource graph, we adopt data augmentation techniques commonly used in contrastive learning [56; 118; 117] to enhance the representativeness and diversity of the resultant toy graphs.

Toy Graphs Augmentation Strategy.For augmentation, we first calculate the average reversed importance \(^{}(G_{}^{e}(v_{m}))\) of the nodes within an ego graph as \(^{}(G_{}^{e}(v_{m}))\!=\!^{e}(v_{m})|}_ {v G_{}^{e}(v_{m})}\!I^{}(v)\), which then determines the number of augmentations \(n_{}(G_{}^{e}(v_{m}))\!=\![K\!\!^{}(G_{}^ {e}(v_{m}))]\), where \(K\) is a scaling constant that adjusts the intensity of the augmentation. For node \(v_{i},v_{j} G_{}^{e}(v_{m})\), the augmentation techniques DataAugmentation(\(G_{}^{e}(v_{m})\), \(n_{}\)) employed include:

* **Node Dropout:**\(v_{i}\!\!G_{}^{e}(v_{m})\) has a probability of being dropped: \(p(v_{i})\!=\!1-p_{i}\).
* **Addition of Gaussian Noise:** we add gaussian noise to node features as augmentation \(X^{}(v_{i})\!=\!X(v_{i})\!+\!(0,\!^{2})\).
* **Node Interpolation:** a new node feature \(X^{}(v_{new})\) is created by linearly combining the features of two existing nodes \(v_{i}\) and \(v_{j}\), calculated as \(X^{}(v_{new})\!=\! X(v_{i})\!+\!(1\!-\!)X(v_{j})\!,\!v_{i} \!,\!v_{j} G^{}\). And the edge weight between the new node \(v_{new}\) and node \(v_{i}\) is updated to \( A[i,j]\) and node \(v_{j}\) is \((1\!-\!)A[i,j]\) accordingly .
* **Edge Rewriting:** we alter connections based on the average of the involved nodes' sampling probabilities, expressed as \(p((v_{i}\!,\!v_{j}))\!=\!+p_{j}}{2}\).

Key-Value Pairs Construction.After completing the sampling and augmentation procedures, the generated toy graphs are transformed into key-value pairs for storage . Specifically, we collect each master node's \(v_{m}\) historical information (timestamps \(\)), environmental information (neighbors \((v_{m}^{})\)), structural encodings \(s_{m}^{}\) (as described in the Appendix C.2), and the hidden embeddings \(h_{m}^{}\) (obtained by processing the toy graph through the frozen pre-trained GNNs) and store them as keys at the master node \(v_{m}\) of the toy graph. Additionally, we store task-specific output vectors \(\{o_{i}^{}|v_{i}\!\!G^{}\}\) and hidden embeddings \(\{h_{i}^{}|v_{i}\!\!G^{}\}\) as values at each node of the toy graph. For storage of these key-value pairs, we utilize the FAISS vector library  to facilitate accelerated retrieval and storage.

### Toy Graphs Retrieval Process

After constructing the key-value toy graphs vector database, we proceed with the retrieval process for sub-tasks according to the four sub-similarities between the key values of the master node \(v_{m}\) in the toy graph and the center node \(v_{c}\) in the query graph, as detailed in Appendix C.3. The final similarity score is a weighted combination of these factors, and the \(topK\) toy graphs are selected as the retrieval results:

\[S(v_{c},\!v_{m})\!=\!\!\![S_{}(v_{c},\!v_{m}),S_{ }(v_{c},\!v_{m}),\!S_{}(v_{c},\!v_{m}),\!S_{}(v_{c},\!v_{m})]^{},\] (1)

where \(\!=\![w_{1},w_{2},w_{3},w_{4}]\) are the hyper-parameterized weights attributed to the time, structure, environment, and semantic similarities, respectively. Using this composite similarity, we rank and retrieve the \(topK\) toy graphs:

\[G_{}^{}\!=\!_{G^{}^{ }}S(v_{c},\!v_{m}),\] (2)

where \(G_{}^{}\) represents the subset of toy graphs that best match the query based on the combined criteria. This process ensures that we retrieve the most relevant toy graphs based on a comprehensive similarity measure, incorporating historical, structural, and environmental information.

### Training and Inference

In Section 4.3.1, we detail the Knowledge Injection Propagation process, which includes two distinct propagation manners. Next, in Section 4.3.2, we present our approach for combining the retrieved hidden embeddings with the task-specific output vectors. Additionally, to enhance the robustness of RAGraph, a noise-based prompt tuning strategy is introduced in Section 4.3.3.

#### 4.3.1 Knowledge Injection Propagation

After retrieving the \(topK\) toy graphs \(G_{}^{}\), knowledge, specifically the task-specific output vectors \(O\) and hidden embeddings \(H\), is propagated from these toy graphs to the master nodes (Toy Graph Intra Propagation) and then to the center node \(v_{c}\) (Query-Toy Graph Inter Propagation). This propagation utilizes message-passing mechanisms via GNNs (cf. Appendix C.1). Each master node \(v_{m}\) in the toy graphs is connected to the center node \(v_{c}\) of the query graph based on the similarity scores \(S(v_{c}\),\(v_{m})\) computed in Eq.(1) and the connection weights dictate the influence of each toy graph, ensuring that graphs with higher similarity have a more substantial impact. This process can be implemented using either a parameter-free or a learnable approach. Moreover, it is worth noting that for learnable methods, the parameters of GNN are different.

**Toy Graph Intra Propagation** Within each toy graph, information \(\) is propagated from neighbors to the master node using pre-trained GNNs. The task-specific output vectors \(o\) and hidden embeddings \(h\) from the neighbors are aggregated and transmitted to the master node. For each node \(v_{i}\) in a toy graph \(G^{}\), the GNN aggregates information from its neighbors \((v_{i})\) to update the master node \(v_{m}\):

\[_{m}=}(\{_{i}\,|\,v_{i} (v_{m})\}),\] (3)

where \(_{i}\) and \(_{m}\) represent the hidden embeddings \(h_{i}\),\(h_{m}\) or task-specific output vectors \(o_{i}\),\(o_{m}\) of the neighbor nodes and master node, respectively. For parameter-free situations, we can prepare \(_{m}\) in advance when constructing the toy graph to improve inference efficiency.

**C Query-Toy Graph Inter Propagation** Next, information from the toy graphs is aggregated to the query graph. Specifically, during propagation, information \(\) from the neighbors and master node of the toy graph is propagated to the center node using the same pre-trained GNNs. For a center node \(v_{c}\) in the query graph \(G^{}\), the GNN aggregates hidden embeddings \(H\) from its neighbors \((v_{c})\) and the master node \(v_{m}\) from the toy graph:

\[h_{c}=}(\{h_{i}\,|\,v_{i}(v_{c}) \{v_{m}\}\}).\] (4)

When propagating the task-specific output vector \(O\), only the master node's information is passed to the center node:

\[o_{c}=}(\{o_{i}\,|\,v_{i}\{v_{m}\}\}).\] (5)

For scenarios where the propagation mechanism is learnable, attention mechanisms can be adapted on the edges. In parameter-free scenarios--where there are no learnable weights--the attention on the edges is determined based on the edge weights from the previous resource graph.

#### 4.3.2 Knowledge Fusion Layer

Finally, at the data fusion layer, the aggregated hidden embeddings \(H\) of the center node \(v_{c}\) are processed through the pre-trained GNN's decoder Decoder\(()\) to obtain an output vector \(O\). This output vector is then combined with the aggregated task-specific output vector in a weighted manner to produce the final output for downstream tasks as illustrated in Definition 3. The combined output is formulated as follows:

\[_{c}= o_{c}+(1-)(h_{c}),\] (6)

where \(\) is a reweighting hyper-parameter. The resulting vector \(_{c}\) is then utilized to perform node-, graph-, or edge-level tasks via a similarity function.

For the same task, the decoder can be directly used to generate outputs. For different tasks, the decoder can be masked, allowing the model to utilize pre-computed embeddings without additional training. Furthermore, the decoder can be fine-tuned to better meet the specific requirements of each task, providing both flexibility and optimized performance. This approach ensures that the model effectively integrates and leverages information from both the toy graphs and the query graph, enhancing its effectiveness in various downstream tasks through the use of the aggregated task-specific output vector.

#### 4.3.3 Noise-based Graph Prompting Tuning

When prompt tuning, RAGraph employs the same prompt loss function \(_{}\) as the backbone model (e.g., GraphPro, GraphPrompt). However, to mitigate the challenge of noise retrieval--a common issue in traditional RAG where highly related but irrelevant data is often retrieved--we enhance the training process by incorporating noise data to bolster model robustness, motivated by . Specifically, we implement two types of noise integration strategies:

* **Inner-Toy-Graph Noise:** This strategy involves artificially introducing irrelevant nodes (\(v_{j} G^{c}_{}(v_{m})\)) into the toy graph during its construction, complementing other augmentation techniques.
* **Toy-Graph Noise:** Throughout the training phase, we not only retrieve the \(topK\) toy graphs that are most relevant but also deliberately include the \(bottomK\) toy graphs to incorporate noise knowledge.

The integration of these noise elements is intended to enhance the model's ability to distinguish relevant information from irrelevant information, significantly improving its robustness and overall performance in downstream tasks by noise training. However, during the inference stage, we do not incorporate the noise.

## 5 Experiments

In this section, we conduct a series of experiments to evaluate the performance of RAGraph against state-of-the-art baselines on three dynamic and five static datasets on three-level graph tasks. Further details and experiment results are provided in Appendix D.

### Experimental Setup

Datasets.We use four static datasets _PROTEINS, COX2, ENZYMES_ and _BZR_ for graph classification and node classification, as well as three dynamic datasets _TAOBAO, KOUBEI_ and _AMAZON_ for link prediction. More details about these datasets can be found in Table 4 in Appendix D.1.

Methods and Baselines.We consider three versions of our proposed framework RAGraph: 1) RAGraph/NF, which indicates we utilize the plug-and-play RAGraph without fine-tuning on the train set; 2) RAGraph/FT, which employs prompt tuning on the train set with RAG; and 3) RAGraph/NFT, which applies noise prompt tuning on the train set with RAG. For the baseline of the dynamic graph, we choose LightGCN , SGL , MixGCF , SimGCL , GraphPro  and GraphPro+PRODIGY . For the static graph, we choose GCN , GraphSAGE ,GAT , GIN , GraphPrompt , GraphPrompt+PRODIGY  as baselines. In addition, we denote '/NF' and '/FT' respectively to represent without fine-tuning and fine-tuning. A detailed description of baselines can be referred to in Appendix D.3.

Settings and Evaluation.We establish a training-resource split with the remainder of the data reserved as unseen during fine-tuning. For static graphs, the split is based on node partitioning with the ratio of 50%:30% , while for dynamic graphs, it is based on partitioning snapshots with the history snapshots as resource graph . For fair comparisons, for methods employing PRODIGY and RAGraph,  we fine-tune models using the training set while retrieving the resource graph to prevent information leakage and over-fitting;  when testing, we retrieve the combined training and resource graphs. For other methods, fine-tuning was directly performed on the combined train and resource set for fairness. For the evaluation of static graphs, we refer GraphPrompt, utilizing pre-trained GNNs for both node- and graph-level tasks within a \(k\)-shot classification framework. For dynamic graphs, we follow GraphPro to employ pre-trained GNNs on a substantial dataset fraction, with fine-tuning and testing conducted on later snapshots. Moreover, we pre-train GraphPro and GraphPrompt unsupervised on other datasets within the similar domain following [67; 39] to avoid information leakage. For classification tasks, we utilize the accuracy as evaluation matric; For link prediction tasks, we use standard metrics Recall@k and nDCG@k at \(k=20\), in line with existing methodologies [31; 104; 118]. The metrics used in the experiment are detailed in Appendix D.2 and the implementation details of RAGraph and baselines are in Appendix D.4.

### Retrieval-Augmented Graph Results

As discussed, we conduct experiments and report the results of the three graph tasks for static graph and dynamic graph, as illustrated in Table 1 and Table 2. From the reported accuracy, we can find the following observations:

Outperforming SOTA Methods.First, our proposed RAGraph outperforms almost all the baselines across the three graph tasks, demonstrating the effectiveness of RAGraph in transferring knowledge from the pre-training to downstream tasks compared to traditional GNNs _i.e.,_ GCN and GraphSAGE. It achieves the highest average accuracy across almost all tasks on ENZYMES, with an improvement of at least 5.19% in the static graph, and up to 11.78% on the dynamic graph over the best baseline PRODIGY/FT. We argue that by virtue of the integration of hidden embedding and task-specific output vector, RAGraph is able to comprehend more knowledge than simply learns the paradigm from \(X Y\). Second, compared with the models of PRODIGY/NF and RAGraph/NF, the introduction of noise training in noise prompt tuning also improves the robustness of the model, avoiding the influence of a large amount of noise on the information aggregation inside the query graph.

Strong Retrieval-Augmented Performance on Unseen Datasets.We observe that PRODIGY/NF and RAGraph/NF are better to Vanilla/NF, indicating that the retrieval knowledge truly works when testing on unseen datasets. Moreover, the difference between PRODIGY/NF and PRODIGY/FT is much greater than that of RAGraph, which also indicates that a simple learning paradigm for ICL is not enough and that RAGraph can achieve acceptable results even on unseen downstream datasets without the need for sophisticated fine-tuning.

    &  &  \\   & PROTEINS & ENZYMES & PROTEINS & COX2 & ENZYMES & BZR \\  & (5-shot) & (5-shot) & (5-shot) & (5-shot) & (5-shot) \\   GCN & 46.63\(\)03.04 & 52.80\(\)12.89 & 54.80\(\)06.64 & 67.87\(\)03.39 & 22.67\(\)05.20 & 58.76\(\)05.08 \\ GraphSAGE & 48.87\(\)02.64 & 48.75\(\)01.59 & 52.99\(\)10.57 & 67.02\(\)05.42 & 21.17\(\)05.49 & 58.27\(\)04.79 \\ GAT & 48.13\(\)07.90 & 47.75\(\)01.23 & 55.82\(\)07.31 & 64.89\(\)03.23 & 20.67\(\)03.27 & 57.04\(\)06.70 \\ GIN & 49.61\(\)01.58 & 48.82\(\)01.58 & 56.17\(\)08.58 & 62.77\(\)02.85 & 19.00\(\)03.74 & 56.54\(\)04.20 \\ 
**GraphPrompt+** & & & & & \\  Vanilla/NF & 44.88\(\)13.17 & 48.81\(\)01.88 & 56.68\(\)03.63 & 53.04\(\)04.13 & 36.50\(\)03.31 & 68.77\(\)03.44 \\ Vanilla/FT & 48.99\(\)01.88 & 51.99\(\)01.36 & 57.04\(\)03.88 & 64.04\(\)08.20 & 40.00\(\)04.36 & 69.01\(\)02.21 \\ PRODIGY/NF & 47.32\(\)08.12 & 43.80\(\)14.03 & 53.48\(\)06.72 & 53.97\(\)10.34 & 22.12\(\)13.84 & 67.18\(\)08.93 \\ PRODIGY/FT & 53.26\(\)06.42 & 57.98\(\)12.37 & 57.14\(\)10.34 & 65.31\(\)04.28 & 25.94\(\)05.12 & 68.08\(\)06.68 \\  RAGraph/NF & 56.12\(\)04.11 & 75.92\(\)01.72 & 58.48\(\)03.93 & 55.32\(\)04.15 & 38.17\(\)03.39 & **77.53\(\)**05.26 \\ RAGraph/FT & 58.74\(\)00.87 & 75.74\(\)01.92 & **62.33\(\)**02.52 & **76.60\(\)**02.30 & 47.71\(\)06.88 & 76.79\(\)05.02 \\ RAGraph/NF & **59.83\(\)**00.40 & **76.23\(\)**01.63 & 59.08\(\)03.50 & 71.70\(\)04.29 & **49.17\(\)**04.64 & 74.81\(\)04.25 \\   

Table 1: Accuracy evaluation on node and graph classification. All tabular results (%) are in mean\(\) standard deviation across five seeds run, with best **bolded** and runner-up underlined.

Figure 3: Hyper-parameter study with hops\(k\) (**Left**) from 1 to 5 and\(topb\) from 1 to 20 (**Right**) on node classification with PROTEINS, and ENZYMES datasets with the setting in Table 1.

### Hyper-parameter Study

In this section, we examine the impact of various hyper-parameters on RAGraph. We specifically analyze the effects of varying the number of hops \(k\) in toy graphs from the list \(\) and the number of linked toy graphs \(topK\) from the list \(\) to verify the sensitive:

Figure 3 (Left) illustrates relationships between accuracy and the toy graph hop \(k\). We observe that as \(k\) increases, the volume of retrieved knowledge grows exponentially. However, an excessive accumulation of knowledge not only fails to enhance accuracy but also introduces increased irrelevant noise that burdens the GNNs. Notably, accuracy shows a trend of initial improvement followed by a decline as \(k\) is increased. This pattern suggests that at lower \(k\) values, the retrieved information tends to consist of isolated, less useful knowledge. In contrast, at higher \(k\) values, the GNNs struggle to process extensive reasoning chains, leading to the utilization of complex and abundant information that is less effective than even the baseline model's performance. Figure 3 (Right) shows effects on accuracy with different numbers of toy graphs \(topK\). As with the previous figure, increasing \(topK\) demonstrates that an excessive amount of knowledge can hinder the GNNs' comprehension capabilities. Conversely, smaller \(topK\) results in insufficient knowledge to enhance performance on downstream tasks.

## 6 Conclusion

We introduced RAGraph, a novel and general framework that enhances Graph Neural Networks (GNNs) by integrating Retrieval-Augmented Generation (RAG) techniques. This plug-and-play approach improves GNNs' ability to generalize to unseen data by retrieving relevant information. Experimental results show that RAGraph outperforms state-of-the-art methods in various graph learning tasks, demonstrating its adaptability and robustness. While RAGraph is currently limited to retrieving subgraphs, future research could explore using more graph-structured data such as nodes, edges, and trees to further enhance its capabilities. In general, our work provides valuable insights and serves as a reference for future Large Graph Models.