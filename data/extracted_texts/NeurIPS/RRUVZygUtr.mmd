# Spectral Invariant Learning for Dynamic Graphs

under Distribution Shifts

 Zeyang Zhang1, Xin Wang1, Ziwei Zhang1, Zhou Qin2,

Weigao Wen2, Hui Xue2, Haoyang Li1, Wenwu Zhu1

1Department of Computer Science and Technology, BNRist, Tsinghua University, 2Alibaba Group

zy-zhang20@mails.tsinghua.edu.cn, {xin_wang, zwzhang}@tsinghua.edu.cn,

{qinzhou.qinzhou, weigao.wen, hui.xueh}@alibaba-inc.com,

lihy18@mails.tsinghua.edu.cn, wwzhu@tsinghua.edu.cn

This work was done during the author's internship at Alibaba GroupCorresponding authors

###### Abstract

Dynamic graph neural networks (DyGNNs) currently struggle with handling distribution shifts that are inherent in dynamic graphs. Existing work on DyGNNs with out-of-distribution settings only focuses on the time domain, failing to handle cases involving distribution shifts in the spectral domain. In this paper, we discover that there exist cases with distribution shifts unobservable in the time domain while observable in the spectral domain, and propose to study distribution shifts on dynamic graphs in the spectral domain for the first time. However, this investigation poses two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain; and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns. To address these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD**), which can handle distribution shifts on dynamic graphs by capturing and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums, allowing the mixed dynamic graph patterns to be transformed into separate frequency components. We then develop a disentangled spectrum mask to filter graph dynamics from various frequency components and discover the invariant and variant spectral patterns. Finally, we propose invariant spectral filtering, which encourages the model to rely on invariant patterns for generalization under distribution shifts. Experimental results on synthetic and real-world dynamic graph datasets demonstrate the superiority of our method for both node classification and link prediction tasks under distribution shifts.

## 1 Introduction

Dynamic graph neural networks (DyGNNs) have achieved remarkable success in many predictive tasks over dynamic graphs [1; 2]. Existing DyGNNs exhibit limitations in handling distribution shifts, which naturally exist in dynamic graphs due to multiple uncontrollable factors [3; 4; 5; 6]. Existing work on out-of-distribution generalized DyGNNs focuses on handling distribution shifts in the time domain. For example, DIDA  utilizes dynamic graph attention to mask the graph trajectories to capture the invariant patterns on dynamic graphs, which assumes that in the time domain, the distribution shift is observable and the invariant and variant patterns can be easily disentangled.

However, there exist cases that the distribution shift is unobservable in the time domain while observable in the spectral domain, as shown in Figure 1. The shift in frequency components canbe clearly observed in the spectral domain, while these components are indistinguishable in the time domain. Moreover, in real-world applications, the observed dynamic graphs usually consist of multiple mixed graph structural and featural dynamics from various frequency components [8; 9; 10].

To address this problem, in this paper, we study the problem of handling distribution shifts on dynamic graphs in the spectral domain for the first time, which poses the following two key challenges: i) it is non-trivial to capture different graph patterns that are driven by various frequency components entangled in the spectral domain, and ii) it remains unclear how to handle distribution shifts with the discovered spectral patterns.

To tackle these challenges, we propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD3**). Our proposed **SILD** model can effectively handle distribution shifts on dynamic graphs by discovering and utilizing invariant and variant spectral patterns. Specifically, we first design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums so that the mixed graph dynamics can be transformed into separate frequency components. Then we develop a disentangled spectrum mask that leverages the amplitude and phase information of the ego-graph trajectory spectrums to obtain invariant and variant spectrum masks so that graph dynamics from various frequency components can be filtered. Finally, we propose an invariant spectral filtering that discovers the invariant and variant patterns via the disentangled spectrum masks, and minimize the variance of predictions with exposure to various variant patterns. As such, **SILD** is able to exploit invariant patterns to make predictions under distribution shifts. Experimental results on several synthetic and real-world datasets, including both node classification and link prediction tasks, demonstrate the superior performance of our **SILD** model compared to state-of-the-art baselines under distribution shifts. To summarize, we make the following contributions:

* We propose to study distribution shifts on dynamic graphs in the spectral domain, to the best of our knowledge, for the first time.
* We propose Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD**), which can handle distribution shifts on dynamic graphs in the spectral domain.
* We employ DyGNN with Fourier transform to obtain the node spectrums, design a disentangled spectrum mask to obtain invariant and variant spectrum masks in the spectral domain, and propose the invariant spectral filtering mechanism so that **SILD** is able to handle distribution shifts.
* We conduct extensive experiments on several synthetic and real-world datasets, including both node classification and link prediction tasks, to demonstrate the superior performance of our method compared to state-of-the-art baselines under distribution shifts.

Figure 1: An illustration example: the graph dynamics from different frequency components are entangled in the temporal domain, while it is much easier to distinguish different frequency components by masking the spectrums in the spectral domain. In this case, the frequency components in the invariant spectrums determine the node labels, while the relationship between the variant spectrums and labels is not stable under distribution shifts.

## 2 Problem Formulation and Notations

Dynamic GraphsA dynamic graph can be represented as \(=(\{^{t}\}_{t=1}^{T})\), where \(T\) represents the total number of time stamps, and each \(^{t}=(^{t},^{t})\) corresponds to a graph snapshot at time stamp \(t\) with the node set \(^{t}\) and the edge set \(^{t}\). For simplicity, we also represent a graph snapshot as \(^{t}=(^{t},^{t})\), which includes the node feature matrix \(^{t}\) and the adjacency matrix \(^{t}\). We further denote a random variable of \(^{t}\) as \(^{t}\). The prediction task on dynamic graphs aims to utilize past graph snapshots to make predictions, _i.e._, \(p(^{t}|^{1:t})\), where \(^{1:t}=\{^{1},^{2},,^{t}\}\) denotes the graph trajectory, and the label \(^{t}\) represent the node properties or the links at time \(t+1\). For brevity, we take node-level prediction tasks as an example in this paper. Following , the distribution of graph trajectory can be factorized into ego-graph trajectories, such that \(p(^{t}^{1:t})=_{v}p(_{v}^{t} _{v}^{1:t})\).

Distribution Shifts on Dynamic GraphsThe common optimization objective for prediction tasks on dynamic graphs is to learn an optimal predictor with empirical risk minimization (ERM), _i.e._\(_{}_{(y^{t},^{1:t}_{v}) p_{tr}(^{t}, ^{1:t}_{v})}(f_{}(^{1:t}_{v}),y^{t})\), where \(f_{}\) is a learnable dynamic graph neural networks. Under distribution shifts, however, the optimal predictor trained with ERM and the training distribution may not generalize well to the test distribution, since the risk minimization objectives under two distributions are different due to \(p_{tr}(^{t},^{1:t}) p_{te}(^{t},^{ 1:t})\). The distribution shift on dynamic graphs is complex that may originate from temporal distribution shifts [11; 6; 12; 13; 14] as well as structural distribution shifts [15; 16; 17]. For example, trends or community structures can affect interaction patterns in co-author networks  and recommendation networks , _i.e._, the distribution of ego-graph trajectories may vary through time and structures.

Following out-of-distribution (OOD) generalization literature [7; 15; 11; 20; 21; 22], we make the following assumptions of distribution shifts on dynamic graphs:

**Assumption 1**: _For a given task, there exists a predictor \(f()\), for samples (\(_{v}^{1:t},y^{t}\)) from any distribution, there exists an invariant pattern \(P_{I}^{t}(v)\) and a variant pattern \(P_{V}^{t}(v)\) such that the following conditions are satisfied: 1) the invariant patterns are sufficient to predict the labels, \(y_{v}^{t}=f(P_{I}^{t}(v))+\), where \(\) is a random noise, 2) the observed data is composed of invariant and variant patterns, \(P_{I}^{t}(v)=_{v}^{1:t} P_{V}^{t}(v)\), 3) the influence of the variant patterns on labels is shielded by the invariant patterns, \(_{v}^{t}_{V}^{t}(v)_{I}^{t}(v)\)._

In the next section, inspired by , we give a motivation example to provide some high-level intuition before going to our formal method.

## 3 Motivation Example

Here we introduce a toy dynamic graph example to motivate learning invariant patterns in the spectral domain. We assume that the invariant and variant patterns lie in the 1-hop neighbors, _i.e._, each node has an invariant subgraph and a variant subgraph. For simplicity, we focus on the number of neighbors, _i.e._, each node \(v\) has an invariant subgraph related degree \(_{v,1}^{T 1}\) and a variant subgraph related degree \(_{v,2}^{T 1}\). Only the former determines the node label, _i.e._, \(y_{v}=^{}_{v,1}\). Note that invariant and variant subgraphs are not observed in the data. We further assume a one-dimensional constant feature for each node, which is set as \(1\) without loss of generality.

For the model in the spatial-temporal domain, we adopt sum pooling as one-layer graph convolution, _i.e._, the message passing for each node and time is \(_{v}=_{u_{v}}1=_{v,1}+_{v,2}\). We further adopt a mask \(^{T 1}\) to filter patterns in the temporal domain and make predictions by a linear classifier, _i.e._, \(_{v}=^{}(_{v})\), where \(^{T 1}\) denotes the learnable parameters. Then, the empirical risk in the training dataset \(D_{tr}\) is \(R_{tr}()=|}_{v D_{tr}}(_{v}-y_{v})^{2}\). We have the following proposition.

**Proposition 1**: _For any mask \(^{T 1}\), for the optimal classifier in the training data \(^{*}=_{}R_{tr}()\), if \(||^{*}||_{2} 0\), there exist OOD nodes with unbounded error, i.e., \( v\) s.t. \(_{||_{v,2}||}(_{v}-y_{v})^{2}=\)._

The proposition 1 shows that a classifier trained with masks and empirical risk minimization has unbounded risks in testing data under distribution shifts as the classifier uses variant patterns to make predictions. Next, we show that under mild conditions, an invariant linear classifier in the spectraldomain can solve this problem. Denote \(^{T T}\) as the Fourier bases, where \(_{k,t}=}e^{-j}\). Denote \(_{v}=_{u_{v}}1\) as the spectral representation after a linear message-passing. The prediction is \(_{v}=^{}(_{v})\), where \(^{T 1}\) is the mask to filter the spectral patterns, \(^{T 1}\) is a linear classifier, and \(()^{}\) denotes Hermitian transpose. We have the following proposition.

**Proposition 2**: _If \((_{v,1}}_{v,1} )(_{v,2}}_ {v,2})=,_{v,1},_{v,2}\), then \(^{T 1}\) such that the optimal spectral classifier in the training data has bounded error, i.e., for \(^{*}=_{}R_{tr}()\), \(>0\), \( v,_{_{v,2}}( _{v}-y_{v})^{2}<\)._

The proposition 2 shows that if the frequency bandwidths of invariant and variant patterns do not have any overlap, there exists a spectral mask such that a linear classifier trained with empirical risk minimization in the spectral domain will have bounded risk in any testing data distribution. This example motivates us to capture invariant and variant patterns in the spectral domain, which is not feasible in the spatial-temporal domain.

## 4 Method

In this section, we introduce our method named Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD**) to handle distribution shifts in dynamic graphs, including three modules, dynamic graph neural networks with Fourier transform, disentangled spectrum mask, and invariant spectral filtering. The framework of our method is shown in Figure 2.

### Dynamic Graph Neural Network with Spectral Transform

Dynamic Graph Trajectories ModelingEach node on the dynamic graph has its ego-graph trajectory evolving through time that may determine the node properties or the occurrence of future links. Following [23; 24; 25], we adopt a message-passing network for each graph snapshot to aggregate the neighborhood information at the current time, _i.e._,

\[_{u v}^{t}(_{u}^{t}, _{v}^{t}),_{v}^{t}(\{_{u  v}^{t} u^{t}(v)\},_{v}^{t})),\] (1)

where 'MSG' and 'AGG' denote message and aggregation functions, \(_{u v}^{t}\) is the message from node \(u\) to node \(v\), \(_{u}^{t}\) is the node embedding for node \(u\) at time \(t\), \(^{t}(v)=\{u(u,v)^{t}\}\) is node \(v\)'s neighborhood at time \(t\). To model the high-order neighborhood information, we can stack multiple message-passing layers. In this way, the node embedding along time \(\{_{u}^{t}\}_{t=1}^{T}\) summarizes the evolution of node \(u\)'s ego-graph trajectories. We denote \(^{T N d}\) as the ego-graph trajectory signals for all nodes on the dynamic graph, where \(T\) denotes the total time length, \(N\) denotes the number of nodes and \(d\) denotes the hidden dimensionality.

Spectral TransformAs some patterns on dynamic graphs are unobservable in the time domain, while observable in the spectral domain, we transform the summarized ego-graph trajectory signals \(\) into the spectral domain via Fourier transform for each node and hidden dimension, _i.e._,

\[_{k,t}=}e^{-j},=,\] (2)

where \(^{K T}\) denotes the Fourier bases, \(K\) denotes the number of frequency components, and \(^{K N d}\) denote the node embeddings along frequency components in the spectral domain, and \(_{k,n,m}=_{t=1}^{T}_{k,t}_{t,n,m}\). By choosing the Fourier bases, our spectral transform has the following advantages: 1) we can use fast Fourier transform (FFT)  to accelerate the computation. The computation complexity of Eq. (2) can be reduced from \(O(NdT^{2})\) to \(O(NdTlogT)\). 2) Each basis has clear semantics, _e.g._, \(_{k}\) denotes the node embeddings at the \(k\)-th frequency component in the spectral domain. In this way, we can observe how the nodes on the dynamic graph evolve in different frequency bandwidths. 3) Fourier transform is able to capture global and periodic patterns [27; 28], which are common in real-world dynamic graphs, _e.g._, the interactions on e-commerce networks may result from seasonal sales or product service cycles.

### Disentangled Spectrum Mask

To capture invariant patterns in the spectral domain, we propose to explicitly learn spectrum masks to disentangle the invariant and variant patterns. The embeddings in the spectral domain contain the amplitude information as well as the phase information for each node

\[()=^{2}()+^{2}( )^{},()=( )}{()},\] (3)

where \(()\) and \(()\) denote the real and imaginary part of the complex number, _i.e._, \(=()+j()\), \(j\) denotes the imaginary unit, \(()^{K N d}\) and \(()^{K N d}\) denote the amplitude and phase information respectively. For brevity, the tensor operators in Eq. (3) are all element-wise, _e.g._, \((^{2}())_{i,j,k}=((_{i,j,k}))^{2}\). Then, we obtain the spectrum masks by leveraging both the amplitude and phase information

\[=(()||()), _{I}=(/),_{V}=(- /),\] (4)

where MLP denotes multi-layer perceptrons, \(\) is the temperature, \(_{I}^{K}\) and \(_{V}^{K}\) denote the spectrum mask for invariant and variant patterns, and \(||\) represents the concatenation of the embeddings. In this way, the invariant and variant masks have a negative relationship, and each node can have its own spectrum mask. As the phase information includes high-level semantics in the original signals [29; 30; 31; 32; 33], we keep the phase information unchanged to reduce harm in the fine-grained semantic information for the graph trajectories, and filter the spectrums by the learned disentangled masks in terms of amplitudes,

\[_{I}=_{I}()( ()+j()),_{V}=_ {V}()(()+j()),\] (5)

where \(_{I}\) and \(_{V}\) denote the summarized invariant and variant patterns in the spectral domain. For node classification tasks, we can directly adopt the spectrums for the classifier to predict classes. For link prediction tasks, we can utilize inverse fast Fourier transform (IFFT) to transform the embeddings into the temporal domain for future link prediction

\[_{I}^{}=^{}_{I},_{V} ^{}=^{}_{V},\] (6)

Figure 2: The framework of our proposed method **SILD**. Given a dynamic graph evolving through time, the dynamic graph neural networks with spectral transform first obtain the ego-graph trajectory spectrums in the spectral domain. Then the disentangled spectrum mask leverages the amplitude and phase information of the ego-graph trajectory spectrums to obtain invariant and variant spectrum masks. Last, invariant spectral filtering discovers the invariant and variant patterns via the disentangled spectrum masks, and minimizes the variance of predictions with exposure to various variant patterns, to help the model exploit invariant patterns to make predictions under distribution shifts.

where \(()^{}\) is Hermitian transpose, \(_{I}^{}\) and \(_{V}^{}\) denote the filtered invariant and variant patterns that are transformed back into the temporal domain respectively.

### Invariant Spectral Filtering

Under distribution shifts, the variant patterns on dynamic graphs have varying relationships with labels, while the invariant patterns have sufficient predictive abilities with regard to labels. We propose invariant spectral filtering to capture the invariant and variant patterns in the spectral domain, and help the model focus on invariant patterns to make predictions, thus handling distribution shifts. We take node classification tasks for an example as follows.

Let \(_{I}^{K N d}\) and \(_{V}^{K N d}\) be the filtered invariant and variant spectrums in the spectral domain. Then we can utilize the invariant and variant node spectrums to calculate the task loss

\[_{I}=l(f_{I}(_{I}),),_{V}=l(f_{V}( _{V}),),\] (7)

where \(f_{I}()\) and \(f_{V}()\) are the classifiers for invariant and variant patterns respectively, \(\) is the labels, and \(l\) is the loss function. The task loss is utilized to capture the patterns with the predictive abilities of labels. Recall in Assumption 1, the influence of variant patterns on labels is shielded given invariant patterns as the invariant patterns have sufficient predictive abilities w.r.t labels, and thus the model's predictions should not change when being exposed to different variant patterns and the original invariant patterns. Inspired by [15; 7], we calculate the invariance loss by

\[_{INV}=(\{_{m}}:}\}),\] (8)

where \(_{m}}\) denotes the mixed loss to measure the model's prediction ability with exposure to the specific variant pattern \(}^{K d}\) that is sampled from a set of variant patterns \(\). We adopt all the node embeddings in \(_{V}\) to construct the set of variant patterns \(\). Inspired by [34; 15], we calculate the mixed loss as

\[_{m}}=l(f_{I}(_{I})(f_{V }(})),),\] (9)

where \(\) denotes the sigmoid function. Then, the final training objective is

\[_{,f_{I}}_{I}+_{INV}+_{f_{V}} _{V},\] (10)

where \(\) is the parameters that encompass all the model parameters except the classifiers, \(\) is a hyper-parameter to balance the trade-off between the model's predictive ability and invariance properties. A larger \(\) encourages the model to capture patterns with better invariance under distribution shifts, with the potential risk of lower predictive ability during training, as the shortcuts brought by the variant patterns might be discarded in the training process. After training, we only adopt invariant patterns to make predictions in the inference stage. The overall algorithm for training on node classification datasets is summarized in Algo. 1.

```
0: Training epochs \(L\), sample number \(S\), hyperparameter \(\)
1:for\(l=1,,L\)do
2: Obtain the node embeddings \(\) with snapshot-wise message passing as Eq. (1)
3: Transform the node embeddings into the spectral domain with FFT as Eq. (2)
4: Calculate the disentangled spectrum masks as Eq. (4)
5: Filter spectrums into invariant and variant patterns as Eq. (5)
6: Calculate the task loss as Eq. (7)
7: Sample \(S\) variant patterns from collections of \(_{V}\) and calculate the invariance loss as Eq. (8)
8: Update the model according to Eq. (10)
9:endfor ```

**Algorithm 1** Training pipeline for **SILD** on node classification datasets

## 5 Experiments

In this section, we conduct extensive experiments to verify that our proposed method can handle distribution shifts on dynamic graphs by discovering and utilizing invariant patterns in the spectral domain. More details of the settings and other results can be found in Appendix.

**Baselines.** We adopt several representative dynamic GNNs and Out-of-Distribution(OOD) generalization methods as our baselines:* Dynamic GNNs: **GCRN** is a representative dynamic GNN that first adopts a GCN to obtain node embeddings and then a GRU  to model the network evolution. **EGCN** adopts an LSTM  or GRU  to flexibly evolve the GCN  parameters through time. **DySAT** aggregates neighborhood information at each graph snapshot using structural attention and models network dynamics with temporal self-attention.
* OOD generalization methods: **IRM** aims at learning an invariant predictor which minimizes the empirical risks for all training domains. **GroupDRO** puts more weight on training domains with larger errors to minimize the worst-group risks across training domains. **V-REx** reduces the differences in the risks across training domains to reduce the model's sensitivity to distributional shifts. As these methods are not specifically designed for dynamic graphs, we adopt the best-performed dynamic GNNs as their backbones on each dataset.
* OOD generalization methods for dynamic graphs: **DIDA** utilizes disentangled attention to capture invariant and variant patterns in the spatial-temporal domain, and conducts spatial-temporal intervention mechanism to let the model focus on invariant patterns to make predictions.

### Real-world Datasets

SettingsWe use 3 real-world dynamic graph datasets, including Collab [41; 7], Yelp [24; 7] and Aminer [42; 43]. Following , we adopt the challenging inductive future link prediction task on Collab and Yelp, where the model should exploit historical graphs to predict the occurrence of links in the next time step. To measure the model's performance under distribution shifts, the model is tested on another dynamic graph with different fields, which is unseen during training. For node classification, we adopt Aminer, a citation network, where nodes represent papers, and edges from \(u\) to \(v\) with timestamp \(t\) denote the paper \(u\) published at year \(t\) sites the paper \(v\). The task is to predict the venues of the papers. We train on papers published between 2001 - 2011, validate on

  
**Task** & **Link Prediction (AUC\%)** &  \\
**Dataset** & **Collab** & **Yelp** & **Aminer15** & **Aminer16** & **Aminer17** \\  GCRN & 69.72\(\)0.45 & 54.68\(\)7.59 & 47.96\(\)1.12 & 51.33\(\)0.62 & 42.93\(\)0.71 \\ EGCN & 76.15\(\)0.91 & 53.82\(\)2.06 & 44.14\(\)1.12 & 46.28\(\)1.84 & 37.71\(\)1.84 \\ DySAT & 76.59\(\)0.20 & 66.09\(\)1.42 & 48.41\(\)0.81 & 49.76\(\)0.96 & 42.39\(\)0.62 \\ IRM & 75.42\(\)0.87 & 56.02\(\)16.08 & 48.44\(\)0.13 & 50.18\(\)0.73 & 42.40\(\)0.27 \\ VREx & 76.24\(\)0.77 & 66.41\(\)1.87 & 48.70\(\)0.73 & 49.24\(\)0.27 & 42.59\(\)0.37 \\ GroupDRO & 76.33\(\)0.29 & 66.97\(\)0.61 & 48.73\(\)0.61 & 49.74\(\)0.26 & 42.80\(\)0.36 \\ DIDA & 81.87\(\)0.40 & 75.92\(\)0.90 & 50.34\(\)0.81 & 51.43\(\)0.27 & 44.69\(\)0.06 \\ 
**SILD** & **84.09\(\)0.16** & **78.65\(\)2.22** & **52.35\(\)1.04** & **54.11\(\)0.62** & **45.54\(\)1.19** \\   

Table 1: Results of different methods on real-world link prediction and node classification datasets. The best results are in bold and the second-best results are underlined. The year in the Aminer dataset denotes the test split, _e.g._, ‘Aminer15’ denotes the average test accuracy in 2015.

  
**Dataset** &  &  \\
**Shift** & **0.4** & **0.6** & **0.8** & **0.4** & **0.6** & **0.8** \\  GCRN & 72.57\(\)0.72 & 72.29\(\)0.47 & 67.26\(\)0.22 & 27.19\(\)2.18 & 25.95\(\)0.80 & 29.26\(\)0.69 \\ EGCN & 69.00\(\)0.53 & 62.70\(\)1.14 & 60.13\(\)0.89 & 24.01\(\)2.29 & 22.75\(\)0.96 & 24.98\(\)1.32 \\ DySAT & 70.24\(\)1.26 & 64.01\(\)0.19 & 62.19\(\)0.39 & 40.95\(\)2.89 & 37.94\(\)1.01 & 30.90\(\)1.97 \\ IRM & 69.40\(\)0.09 & 63.97\(\)0.37 & 62.66\(\)0.33 & 33.23\(\)4.70 & 30.29\(\)1.71 & 29.43\(\)1.38 \\ VREx & 70.44\(\)1.08 & 63.99\(\)0.21 & 62.21\(\)0.40 & 41.78\(\)1.30 & 38.11\(\)2.81 & 29.56\(\)0.44 \\ GroupDRO & 70.30\(\)1.23 & 64.05\(\)0.21 & 62.13\(\)0.35 & 41.35\(\)2.19 & 35.74\(\)3.93 & 31.03\(\)1.24 \\ DIDA & 85.20\(\)0.84 & 82.89\(\)0.23 & 72.59\(\)3.31 & 43.33\(\)7.74 & 39.48\(\)7.93 & 28.14\(\)3.07 \\ 
**SILD** & **85.95\(\)0.18** & **84.69\(\)1.18** & **78.01\(\)0.71** & **43.62\(\)2.74** & **39.78\(\)3.56** & **38.64\(\)2.76** \\   

Table 2: Results of different methods on synthetic link prediction and node classification datasets. The best results are in bold and the second-best results are underlined. A larger ‘shift’ denotes a higher distribution shift level.

those published in 2012 - 2014, and test on those published since 2015. On this dataset, the model is tested to exploit the invariant patterns and make stable predictions under distribution shifts, where the patterns on the dynamic graph may vary in different years.

ResultsBased on the results in Table 1, we have the following observations: 1) _Under distribution shifts, the general OOD generalization baselines have limited improvements over the dynamic GNNs, e.g.,_ GroupDRO improves over DySAT with 0.9% in Yelp and 0.3% in Aminer15 respectively. A plausible reason is that they are not specially designed to handle distribution shifts on dynamic graphs, and may not consider the graph structural and temporal dynamics to capture invariant patterns. Another reason might be that they strongly rely on high-quality environment labels to capture invariant patterns, which are almost unavailable on real-world dynamic graphs. 2) _Our method can better handle distribution shifts than the baselines_. The datasets have strong distribution shifts, e.g., COVID-19 happens midway and has considerable influence on the consumer behavior on Yelp, and the citation patterns may shift with the outbreak of deep neural networks on Aminer. Nevertheless, our method **SILD** has significant improvements over the state-of-the-art OOD generalization baseline for dynamic graphs DIDA on all datasets, e.g., 2% on average for most datasets, which verifies that our method can better capture the invariant and variant patterns in the spectral domain, and thus handling distribution shifts on dynamic graphs.

### Synthetic Datasets

SettingsTo evaluate the model's generalization ability under distribution shifts, we conduct experiments on synthetic link prediction and node classification datasets, which are constructed by introducing manually-designed distribution shifts. For link prediction datasets, we follow  to generate additional varying features for each node and timestamps on the original dataset Collab, where these additional features are constructed with spurious correlations w.r.t the labels, the links in the next timestamps. The spurious correlation degree is determined by a shift level parameter. On this dataset, to have better generalization ability, the model should not rely on variant patterns that exploit the additional features with spurious correlations. For node classification, we briefly introduce the construction of the synthetic dataset as follows. We generate the dynamic graph with stochastic block model , where the link probability between nodes at each graph snapshot is determined by two frequency factors. The correlation of one of the factors with class labels is always 1, while the other factor has a variant relationship with labels, where the relationship is also controlled by a shift level parameter. The model should discover and focus on the invariant frequency factors whose relationship with labels is invariant under distribution shifts. For both datasets, we set the shift level parameters as 0.4, 0.6, 0.8 for training and validation splits, and 0 for test splits.

ResultsBased on the results in Table 2, we have the following observations: 1) _Our method can better handle distribution shifts than the baselines, especially under stronger distribution shifts_. **SILD** consistently outperforms DyGNN and general OOD generalization baselines by a significantly large margin, which can credit to our special design to handle distribution shifts on dynamic graphs in the spectral domain. Our method also has a significant improvement over the best-performed baseline under the strongest distribution shift, e.g., with absolute improvements of 5% in Link-Synthetic(0.8) and 7% in Node-Synthetic(0.8) respectively. 2) _Our method can exploit invariant patterns to consistently alleviate the harmful effects of variant patterns under different distribution shift levels_. As the distribution shift level increases, almost all methods decline in performance since the relationship between variant patterns and labels goes stronger, so that the variant patterns are much easier to be exploited by the model, misleading the training process. However, the performance drop of **SILD** is significantly lower than baselines, which demonstrates that our method can alleviate the harmful effects of variant patterns under distribution shifts by exploiting invariant patterns in the spectral domain.

### Ablation Studies

We conduct ablation studies to verify the effectiveness of the proposed disentangled spectrum mask and invariant spectral filtering in **SILD**. The ablated version 'SILD w/o I' removes invariant spectral filtering in **SILD** by setting \(=0\), and 'SILD w/o M' is trained without the disentangled spectrum masks. From Figure 3, we have the following observations. First, our proposed **SILD** outperforms all the variants as well as the best-performed baseline on all datasets, demonstrating the effectiveness of each component of our proposed method. Second, 'SILD w/o I' and 'SILD w/o M' drop drastically in performance on all datasets compared to the full version, which verifies that our proposed disentangled spectrum mask and spectral invariant learning can help the model to focus on invariant patterns to make predictions and significantly improve the performance under distribution shifts.

## 6 Related Works

Dynamic Graph Neural NetworksDynamic graphs ubiquitously exist in real-world applications [45; 46; 47; 48; 49; 50; 51; 52; 53] such as event forecasting, recommendation, etc. In comparison with static graphs [54; 55; 56; 57; 58; 59], dynamic graphs contain rich temporal information. Considerable research attention has been devoted to dynamic graph neural networks (DyGNNs) [1; 2; 60] to model the complex graph dynamics that include structures and features evolving through time. Some works adopt GNN to aggregate neighborhood information for each graph snapshot, and then utilize a sequence module to model the temporal information [61; 62; 63; 35; 24]. Some others utilize time-encoding techniques to encode the temporal links into time-aware embeddings and adopt a GNN or memory module [64; 65; 66; 67] to process structural information. Some other related works leverage spectral graph neural networks , global graph framelet convolution , and graph wavelets  to obtain better dynamic graph representations. However, distribution shifts remain largely unexplored in dynamic graph neural networks literature. The sole prior work DIDA  handles spatial-temporal distribution shifts on dynamic graphs in the temporal domain. To the best of our knowledge, this is the first study of handling distribution shifts on dynamic graphs in the spectral domain.

Out-of-Distribution GeneralizationA significant proportion of existing machine learning methodologies operate on the assumption that training and testing data are independent and identically distributed (i.i.d.). However, this assumption may not always hold true, especially in the context of complex real-world scenarios , and the uncontrollable distribution shifts between training and testing data distribution may lead to a significant decline in the model performance. Out-of-Distribution (OOD) generalization problem has recently drawn great attention in various areas [72; 71; 73]. Some works handle structural distribution shifts on static graphs [74; 15; 16; 75; 5; 76; 77; 78; 79; 80; 81] and temporal distribution shifts on time-series data [11; 12; 6; 13; 14; 82]. However, how to handle distribution shifts on dynamic graphs in the spectral domain remains unexplored.

Spectral Methods in Neural NetworksThe applications of spectral methods in neural networks have been broadly explored in many areas, including static graph data [83; 84; 85; 86; 87], time-series data [68; 88; 89; 90; 91], etc., for their advantages of modeling global patterns, powerful expressiveness and interpretability [92; 28]. Some work  proposes to reconstruct the image in the spectral domain to obtain robust image representations. Some work  proposes to augment the image data by perturbing the amplitude information in the spectral domain. Some work  proposes a multiwavelet-based method for compressing operator kernels. However, these methods are not applicable to dynamic graphs, not to mention the more complex scenarios under distribution shifts.

Figure 3: Results of ablation studies, where ‘w/o I’ removes invariant spectral filtering in **SILD**, ‘w/o M’ removes disentangled spectrum masks, and ‘Best baseline’ denotes the best-performed baseline on each dataset. The error bars report the standard deviations. (Best viewed in color)

Conclusion

In this paper, we propose a novel model named Spectral Invariant Learning for Dynamic Graphs under Distribution Shifts (**SILD**), which can handle distribution shifts on dynamic graphs in the spectral domain. We design a DyGNN with Fourier transform to obtain the ego-graph trajectory spectrums. Then we propose a disentangled spectrum mask and invariant spectral filtering to discover the invariant and variant patterns in the spectral domain, and help the model rely on invariant spectral patterns to make predictions. Extensive experimental results on several synthetic and real-world datasets, including both node classification and link prediction tasks, demonstrate the superior performance of our method compared to state-of-the-art baselines under distribution shifts. One limitation is that in this paper we mainly focus on dynamic graphs in scenarios of discrete snapshots, and we leave extending our methods to continuous dynamic graphs for further explorations.