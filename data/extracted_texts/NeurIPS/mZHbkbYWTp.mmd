# Stochastic Optimal Control and Estimation with Multiplicative and Internal Noise

Francesco Damiani

Center for Brain and Cognition,

Department of Engineering

Pompeu Fabra University

Barcelona, ES

francesco.damiani@upf.edu

&Akiyuki Anzai

Department of Brain and Cognitive Sciences

University of Rochester

Rochester, USA

aanzai@ur.rochester.edu

&Jan Drugowitsch

Department of Neurobiology

Harvard Medical School

Boston, USA

jan_drugowitsch@hms.harvard.edu

&Gregory C. DeAngelis

Department of Brain and Cognitive Sciences

University of Rochester

Rochester, USA

gdeangelis@ur.rochester.edu

&Ruben Moreno-Bote

Center for Brain and Cognition, Department of Engineering,

Serra Hunter Fellow Programme

Pompeu Fabra University

Barcelona, ES

ruben.moreno@upf.edu

###### Abstract

A pivotal brain computation relies on the ability to sustain perception-action loops. Stochastic optimal control theory offers a mathematical framework to explain these processes at the algorithmic level through optimality principles. However, incorporating a realistic noise model of the sensorimotor system -- accounting for multiplicative noise in feedback and motor output, as well as internal noise in estimation -- makes the problem challenging. Currently, the algorithm that is commonly used is the one proposed in the seminal study in . After discovering some pitfalls in the original derivation, i.e., unbiased estimation does not hold, we improve the algorithm by proposing an efficient gradient descent-based optimization that minimizes the cost-to-go while only imposing linearity of the control law. The optimal solution is obtained by iteratively propagating in closed form the sufficient statistics to compute the expected cost and then minimizing this cost with respect to the filter and control gains. We demonstrate that this approach results in a significantly lower overall cost than current state-of-the-art solutions, particularly in the presence of internal noise, though the improvement is present in other circumstances as well, with theoretical explanations for this enhanced performance. Providing the optimal control law is key for inverse control inference, especially in explaining behavioral data under rationality assumptions.

## 1 Introduction

The sensorimotor system possesses a remarkable ability to reliably execute actions aligned with external and internal goals in spite of the noise sources affecting it  and the numerous solutionsthrough which the same goals can be achieved . Our nervous system is able to combine the complexity of the mechanical properties of the body with a regulatory control system . How such control is implemented at a computational and algorithmic level is still an open question in systems neuroscience.

Optimal feedback control provides a valuable framework for understanding how the motor system creates coordinated and adaptable behavior . However, using optimality principles to infer the underlying computation  is a powerful yet potentially risky approach. Indeed, multiple independent factors can lead to discrepancies with experimental data. For instance, predictions might be based on unsuitable approximations or conditions. Consequently, although optimal feedback control, and more broadly, stochastic optimal control theory, can be regarded as ideal candidates to understand the principles of motor control , their effectiveness depends on the mathematical correctness of the derived predictions and the accuracy of their assumptions.

Solving an optimal control problem implies deriving an optimal state-to-action policy, or control-law, to minimize a certain cost function, usually embedding control effort and task related goals . The classical framework in which analytical solutions for optimal feedback control of stochastic, partially observable, continuous, non stationary, and high-dimensional systems can be derived, is the Linear-Quadratic-Additive-Gaussian (LQAG) problem (see Section 2), that assumes linear dynamics, a quadratic cost function and additive Gaussian noise . Despite having been used in the past to model motor control , these assumptions are too limiting to explain a wide range of observed, relevant behaviors, like smooth velocity profiles , speed-accuracy trade-offs  and movement corrections . Including a realistic noise model for the sensorimotor system is crucial to fill this gap, even at the cost of decreasing the mathematical tractability of the problem . Indeed, accounting for control  and signal-dependent  noise at the motor output and sensory feedback level, and for internal noise  in the estimation process, permits explaining a broad range of experimentally observed phenomena , as discussed in Section 2.

The seminal study in , widely regarded as state-of-the-art for solving optimal control problems under this extended noise model, offers an iterative algorithm whereby a stochastic optimal control problem, incorporating multiplicative motor and sensory noise and additive internal noise, can be efficiently solved. Such an algorithm is currently used to explain behavioral data in the context of inverse optimal control . Unfortunately, the derivation used in  erroneously assumes unbiased estimators. We propose an alternative algorithm that addresses this issue by assuming only linear control. The algorithm leverages the fact that the cost function can be computed from closed-form moment expressions, which can then be minimized numerically. To handle potential high computational costs, we derive an analytical counterpart for the optimization, based on the efficient propagation of cost function derivatives over time. For simplicity, the algorithm is derived for a simpler, yet relevant, case as outlined in Section 3.3, with extensions to the more general case also discussed. Our algorithm outperforms the solutions in  under internal noise, providing both theoretical and heuristic explanations for the performance differences. In a sensorimotor hand-reaching task, it reduces the cost by up to \(90\%\) when internal noise constitutes \(10\%\) of the total. This reveals qualitatively different behaviors, underscoring the importance of using the actual optimal controller, particularly when explaining behavior in a principled way .

In Section 2, to fix notation and ideas, we begin by formalizing the optimal control problem using the classic LQAG framework , addressing partial observability and assuming fully _additive_ noise (Section 2.1). We then introduce the Linear-Quadratic-Multiplicative-Gaussian (LQMG) framework, which extends the noise model to include _multiplicative_ noise in both control and observations, as well as additive _internal_ noise, following the approach of  (Section 2.2). We demonstrate that the well-established solution from  produces suboptimal solutions in the presence of internal noise and prior to full algorithmic convergence (Section 2.3). In Section 3, we introduce a novel numerical algorithm that achieves optimal solutions and outperforms the approach in , as demonstrated empirically in Section 3.2. Finally, in Section 3.3, we present the analytical counterpart to the numerical algorithm.

Control and Estimation with Multiplicative and Internal Noise

### The Classic Linear-Quadratic-Additive-Gaussian (LQAG) Problem

Stochastic optimal control theory formalizes the idea of controlling a dynamical system under partial observability to accomplish a goal . In the LQAG problem (typically referred to as LQG), a linear system with latent state \(x_{t}^{m}\)

\[x_{t+1}=Ax_{t}+Bu_{t}+_{t}\] (1)

is controlled by a control signal \(u_{t}^{p}\), with time-independent matrices \(A^{m m}\) and \(B^{m p}\), and initial condition \(x_{1}\) - considering time-dependent matrices is straightforward. The term \(_{t}^{m}\) stands for a Gaussian random variable with zero mean and covariance \(_{}\) (we always consider i.i.d. random variables, but note that temporally correlated random variables can be generated by filtering the noise with the linear dynamics in Eq. 1). In the most relevant case, the controller does not have full access to the latent state \(x_{t}\): the observation \(y_{t}^{k}\) is a noisy version of \(x_{t}\),

\[y_{t}=Hx_{t}+_{t}\,\] (2)

with observation matrix \(H^{k m}\) and \(_{t}^{k}\) being a Gaussian random variable with zero mean and covariance \(_{}\). Note that all noise sources are _additive_, that is, state independent, hence we refer to the classic LQG problem as LQAG. The controller \(u_{t} u_{t}(y_{t})\) is constrained to be a function of the past observations only, \(y_{t}=(y_{1},...,y_{t-1})\), and it must be optimized to minimize the total quadratic cost

\[[J]=_{t=1}^{T}[j_{t}]=_{t=1}^{T}[x _{t}^{}Q_{t}x_{t}+u_{t}^{}R_{t}u_{t}]\,\] (3)

where \(T\) is the duration of the task, and \(j_{t}\) is the cost per step in a trial, which includes a control cost (reflecting the internal goal of minimizing control effort) determined by the symmetric positive definite matrix \(R_{t}^{p p}\), with \(R_{t}>0\), and a state cost (modeling potential external goals, such as minimizing the distance to a chosen target), determined by \(Q_{t}^{m m}\). Again, \(Q_{t}\) is symmetric and positive definite, \(Q_{t}>0\), and modulates the cost of the state being far from a chosen target. \(J\) is the total cost, over a whole trial, while \([J]\) is the total expected cost. Here, the expectation \([f()]\) denotes an average over all noise random variables with the same initial condition, that is, \([f()]= dx_{2,...,T}\ dy_{1,...,T}f()\ p(x_{2,...,T},y_{ 1,...,T})\).

The optimal controller can be derived analytically  (see Appendix A.1). In summary, it is a linear function of the state estimate \(_{t}\), \(u_{t}=L_{t}_{t}\), where \(L_{t}^{p m}\) is the control gain, and \(_{t}\) is the estimator of the unobserved variable \(x_{t}\), recursively computed with a linear Kalman filter

\[_{t+1}=A_{t}+Bu_{t}+K_{t}(y_{t}-H_{t})\] (4)

with filter gains \(K_{t}^{m k}\) and initial condition \(_{1}=[x_{1}]\) (\(x_{1}\) is a random Gaussian variable with covariance \(_{x_{1}}\)) - to start with an unbiased estimate of the latent variable. Intuitively, the estimate at time \(t+1\) consists of a next-state prediction term (the first two terms in the r.h.s.) from the current estimate \(_{t}\) plus a correction (third term) that depends on the prediction error, the difference between the new observation \(y_{t}\) and the previous state prediction, weighted by its reliability. For example, if the noise magnitude is very large, \(K_{t}=0\), indicating that an open-loop strategy would be optimal .

For the classic LQAG problem, it is well known that the optimal Kalman filter satisfies the _orthogonality principle_, stating that the estimation error is orthogonal to the optimal estimator \(_{t}\), i.e. \([(x_{t}-_{t})_{t}^{}]=0\), where we define from here onwards the expectation as \([f()]= dx_{2,...,T}\ d_{2,...,T}f()\ p(x_{2,..,T}, _{2,...,T})\), where \(p\) is the joint density of latent and estimation variables with initial condition \(_{1}=[x_{1}]\) - e.g., \([_{t}]= d_{t}_{t}p(_{t})\).

Also, as it is clear from the analytical expression (Appendix A.1), the computation for the optimal controller and filter gains are mathematically independent of each other, the so-called _separation principle_[28; 29; 30], which is closely related to the concept of certainty equivalence .

### An Extended Noise Model: Optimal Control Beyond the LQAG Framework

Purely additive noise sources alone are insufficient to model the sensorimotor action-perception loop, as multiplicative noise affects both motor control [1; 15; 17; 16] and sensory feedback, includingvisual and proprioceptive signals [6; 1; 23; 21; 22; 14]. For instance, stronger muscle forces produce greater noise [15; 17], and visual sensory noise increases in the periphery relative to the fovea [1; 23; 21; 22]. Accounting for these characteristics is crucial for explaining and reproducing various behavioral features in reaching movements, such as stereotyped bell-shaped velocity profiles [1; 13; 14] and the speed-accuracy trade-off [1; 15; 16; 17].

These considerations result in the Linear-Quadratic-Multiplicative-Gaussian (LQMG) model, with the following dynamics for state and sensory feedback 

\[x_{t+1} =Ax_{t}+Bu_{t}+_{t}+_{i=1}^{c}_{t}^{i}C_{i}u_{t}\] (5) \[y_{t} =Hx_{t}+_{t}+_{i=1}^{d}_{t}^{i}D_{i}x_{t}\;.\] (6)

In comparison to Eqs. 1-2 for the classic LQAG model, the LQMG model adds the final terms to account for multiplicative motor noise (Eq. 5) and sensory noise (Eq. 6). Specifically, performing a control action \(u_{t}\) introduces noise proportional to the control signal itself (Eq. 5), while perceiving the state variable \(x_{t}\) induces noise proportional to the observed state (Eq. 6). Here, \(C_{i}^{m p}\) and \(D_{i}^{k m}\) are constant scaling matrices, and \(_{t}^{c}\) and \(_{t}^{d}\) are zero-mean Gaussian noise terms with covariances \(_{}=\) and \(_{}=\), respectively . For simplicity, in the expressions derived below, we set \(c=d=1\) to improve readability, without loss of generality.

As in the LQAG problem, the objective is to find the optimal control signal \(u_{t}^{p}\) that depends solely on past observations \(y_{1,,t-1}\) to minimize the cost function defined in Eq. 3. In this case, the optimal state estimate and corresponding filters are state-dependent and in general intractable, but we can simplify the problem by assuming, as in , that the filter is non-adaptive (i.e. independent of the state estimate, ), similar to the classic LQAG problem. This leads to the assumption that the state estimate follows the equation

\[_{t+1}=A_{t}+Bu_{t}+K_{t}(y_{t}-H_{t})+_{t}\;,\] (7)

with the same terminology as in Eq. 4. The initial state \(x_{1}\) and its estimate \(_{1}\), assumed to be independent, are Gaussian variables with the same mean \([x_{1}]=[_{1}]\), and covariances \(_{x_{1}}\) and \(_{_{1}}\), respectively. This dynamics of the state estimate only differs from Eq. 7 due to the presence of an additional zero-mean Gaussian noise term \(_{t}^{m}\) with covariance \(_{}\), which models the possibility of inefficient filtering of the past observations. This noise term may represent internal fluctuations in neural activity [2; 24; 32; 3] or inaccuracies in the filtering process, and is important for explaining behavioral data .

Under the new LQMG model (Eqs. 5-7), the task is to find the optimal control signal \(u_{t}=u_{t}(_{t})\), for \(t=1,...,T-1\), and the filter gains \(K_{1,,T-2}\), that minimize the quadratic cost in Eq. 3.

### State-of-the-Art Solutions for the LQMG Model: Causes of Suboptimality

It is important to first recognize that solving the LQMG problem in Eqs. 5-7 is significantly more complex than in the classic LQAG problem: while in the latter the separation principle applies, allowing for a direct analytical solution (Appendix A.1), in the former the principle does not hold, resulting in tightly intertwined controller and filter gains. Notably, the presence of internal noise alone introduces control-estimation interdependencies-- a factor previously overlooked in earlier approaches.

The algorithm currently used to solve the optimal control problem under the LQMG model (Eqs. 5-7) is the one introduced in the seminal work of , whose solutions are detailed in Appendix A.2. The impact of this research extends beyond theoretical considerations [27; 33; 34; 35; 36; 37; 38]. We now describe some pitfalls in the original derivation and explain why certain assumptions fail, leading to suboptimality. In Section 3, we propose an alternative algorithm, and in Section 3.2, we demonstrate that our solutions outperform the previous ones.

The algorithm in  assumes, throughout the derivation of the optimal control-estimation loop, that the estimator is unbiased, meaning \([x_{t}|_{t}]=_{t}\). However, this condition is never truly satisfied. To illustrate this conceptually, we can consider a one-dimensional toy problem involving a partially observable stochastic process \(x_{t}\) (Fig. 0(a)). Assume that at time \(t-1\), the condition \([x_{t-1}|_{t-1}|=_{t-1}\) holds. Now, suppose that at the same time, a large positive fluctuation, possibly caused by sensory or internal noise, affects the agent's internal estimate. As a result, while the actual state \(x_{t}\) changes only slightly compared to \(x_{t-1}\), the state estimate \(_{t}\) changes significantly, so that \(_{t}_{t-1}\). At this point, it becomes clear that the expected value of \(x_{t}\) conditioned on \(_{t}\) cannot equal \(_{t}\), thus violating the unbiasedness condition (see Fig. 1a). This effect is more pronounced when the state estimate undergoes large fluctuations, but a similar bias, although smaller, would still be present with minor fluctuations.

We demonstrate this issue numerically by considering a one-dimensional problem (\(m=p=k=1\)) with multiplicative, additive, and internal noise (for the details see Appendix A.3). In the absence of internal noise, the violation of unbiasedness is still present, though it becomes pronounced only for large values of \(_{t}\) (Fig. 1b,c). However, when internal noise is introduced, the bias increases substantially since the internal fluctuations are not directly attenuated by the gains \(K_{t}\) (see also Appendix A.3).

As said above, in the classic LQAG problem the optimal Kalman filter satisfies the orthogonality principle . In contrast, when internal noise is non-zero we show that the orthogonality principle does not hold anymore for the optimal filter (see Appendix A.3.1). We further show that the condition of unbiasedness implies the orthogonality principle, but not the other way around. Therefore, applying unbiasedness to solve the optimal control problem introduced in Section 2 leads to suboptimal solutions when internal noise is present, irrespective of control or signal-dependent noise. A similar issue arises when the algorithm has not yet converged, even for zero internal noise, because the orthogonality principle only holds for optimized filter gains (and in the absence of internal noise). As a result, the algorithm in  also fails to produce the optimal control when derived with a fixed suboptimal estimator, and to produce the optimal filter estimate with a fixed suboptimal controller.

## 3 A Novel Algorithm for Optimal Control Problems

To address the issues outlined in the previous section, we introduce an alternative method for solving the LQMG problem presented in Section 2.2. We compute the expected total accumulated cost, \([J]\), by averaging over all stochastic terms present in Eqs. 5-7, and as a function of \(L_{t}\) and \(K_{t}\). For fixed \(L_{t}\) and \(K_{t}\), \([J]\) serves as the objective function for a standard gradient descent algorithm aimed at minimizing it. In Section 3.1, we detail the computation of this objective function through moment propagation and discuss the minimization process with respect to \(L_{1,,T-1}\) and \(K_{1,,T-2}\). In Section 3.2, we demonstrate that this approach outperforms state-of-the-art algorithms. In Section 3.3, we derive the analytical counterpart to our numerical algorithm.

### Minimization of Theoretical Expected Cost Through Numerical Gradient Descent (GD)

Our method assumes linear control, where the control signal \(u_{t}\) is linear in the internal estimate

\[u_{t}=L_{t}_{t}\.\] (8)

This assumption is not very limiting, as it is correct for the classic LQAG problem and has been used before for the LQMG problem . Crucially, we do not assume unbiasedness to solve the

Figure 1: _The invalidity of the unbiasedness condition._ **(a)** A toy example illustrating estimation bias. The black line represents the dynamics of a partially observable process, \(x_{t}\), while the red line shows the state estimate, \(_{t}\), biased by random internal fluctuations from the noise term \(_{t}\) (orange arrow). **(b-c)**\([x_{t}|_{t}]\), for \(t=8\), as a function of \(_{t}\) for \(_{}=0.0,0.6\), respectively, using the solutions from . The conditional expectation \([x_{t}|_{t}]\) is computed through Monte Carlo simulations (dots \(\) error bars: mean \(\) 1std). The gray straight line represents the identity line, where \([x_{t}|_{t}]=_{t}\).

optimal control problem. The expected total accumulated cost is computed by propagating the first two moments of \(x\) and \(\) in closed form. Given that both control and estimation are linear in \(x\) and \(\), and the cost function is quadratic in \(x\) and \(u\), the first and second moments act as sufficient statistics. As a result, no additional approximations (e.g., assuming Gaussianity of \(x\) and \(\)) are required to find the optimal solutions. By using Eq. 8 and the formula for the expected cost of a quadratic form, Eq. 3 can be rewritten as

\[[J]&=_{t=1}^{T}[j_{t}]=_{t=1}^{T}([x_{t}]^{}Q_{t}[x_{t}]+ [_{t}]^{}L_{t}^{}R_{t}L_{t}[ _{t}]+\\ &+Tr[Q_{t}_{x_{t}}]+Tr[L_{t}^{}R_{t}L_{t}_{ _{t}}]),\] (9)

where \(Tr[]\) stands for the trace operation, \(_{x_{t}}\) is the covariance matrix of the latent state \(x_{t}\) and \(_{_{t}}\) is the covariance of the state estimate at time \(t\). Note that \([x_{t}]\), \([_{t}]\), \(_{x_{t}}\) and \(_{_{t}}\) will implicitly depend on \(L_{1,,t-1}\) and \(K_{1,,t-1}\). From Eqs. 5-7 we can derive the update equations to propagate the first and second-order moments \([x_{t}]\)\([_{t}]\), \(_{x_{t}}\) and \(_{_{t}}\) in a closed-form manner, in order to compute the total expected cost \([J]\) at fixed \(L_{1,,T-1}\) and \(K_{1,,T-2}\) (for the derivation see Appendix A.4.1). Here and in the following we set \(c=d=1\) for simplicity (the case \(c,d>1\) follows simply by replacing terms with \(D\) or \(C\) matrices by \(C_{i}\) and \(D_{i}\), respectively, and sum over \(i\)). To rewrite our results in a more compact form, we define (similarly to [27; 39])

\[_{t}=_{x_{t}}\\ _{_{t}}=[x_{t}]\\ [_{t}],\] (10) \[_{t}=_{x_{t}}&_{x_{t},_{t }}\\ _{_{t},x_{t}}&_{_{t}},\] (11) \[M_{t}=A&BL_{t}\\ K_{t}H&A+BL_{t}-K_{t}H\] (12)

and

\[G_{t}=CL_{t}(_{_{t}}+_{_{t}}_{ _{t}}^{})L_{t}^{}C^{}+_{}&0\\ 0&K_{t}D(_{x_{t}}+_{x_{t}}_{x_{t}}^{})D^{}K_{t} ^{}+K_{t}_{}K_{t}^{}+_{},\] (13)

where \(_{x_{t},_{t}}=[x_{t}_{t}^{}]-[x_{t}][_{t}]^{}\) and \(_{_{t},x_{t}}=_{x_{t},_{t}}^{}\). We have defined \(_{t}\) as a column vector whose block elements are \(m-\)dimensional vectors. Similarly, \(_{t}\), \(M_{t}\) and \(G_{t}\) are block matrices, whose block elements are \(m m\) matrices.

With these definitions, we see that the first and second moments propagate in a closed manner as

\[_{t+1}=M_{t}_{t}\;,\] (14) \[_{t+1}=M_{t}_{t}M_{t}^{}+G_{t}\;.\] (15)

In other words, if the first and second moments are known at time \(t\), their values can be recursively computed at time \(t+1\), and no other moments are involved in the calculations.

As a result, given the initial conditions for \(_{1}\) and \(_{1}\), we can compute the expected accumulated cost \([J]\) at fixed \(L_{1,,T-1}\) and \(K_{1,,T-2}\), by using Eqs. 14-15 together with Eq. 9. The pseudo-code for the algorithm to compute the expected cost \([J]\) is provided in Appendix A.4.2, Algorithm 1. To find the optimal control and filter gains we would then use \([J]\) as the objective function of a numerical gradient descent procedure. The analytical gradient descent counterpart is discussed in Section 3.3.

### Experiments: Enhanced Performance with the GD Algorithm

We apply our algorithm and compare it with the state-of-the-art solutions in two scenarios governed by a linear dynamical system (Eqs. 5-8). Hereafter, GD refers to our numerical algorithm (Section 3.1), and TOD refers to the algorithm from . First, in a simplified one-dimensional reaching task (\(m=p=k=1\)) with all noise sources present, we show that for non-zero internal noise, \(_{}>0\), GD outperforms TOD, resulting in a lower accumulated cost. Second, in a reaching task with a four-dimensional state and one-dimensional control and sensory feedback (\(m=4\), \(p=k=1\)), GD predicts qualitatively different behavior and shows a \(90\%\) performance improvement when internal noise contributes \(10\%\) of the total.

One-Dimensional Case: Understanding the Qualitative DifferencesWe examine the case where \(m=p=k=1\), incorporating multiplicative, additive, and internal noise. The system parameters are provided in Table 2 in Appendix A.5.1 (note that we define the strength of the internal noise as \(_{}=}\)). With non-zero internal noise, our algorithm achieves lower-cost solutions compared to the method proposed in  (Fig. 2a). This improved performance arises from different modulations of \(L_{t}\) and \(K_{t}\) as \(_{}\) varies (Figs. 2b, c). Crucially, our solution results in control gains that decrease as internal noise increases, while the TOD solution shows little sensitivity to internal noise magnitude (Fig. 2b): internal noise increasingly intertwines the optimal solutions for \(K_{t}\) and \(L_{t}\). In Appendix A.5.2, we provide a geometric interpretation of why this modulation is optimal, demonstrating that this optimality enhances adaptability, and showing how internal noise disrupts the orthogonality principle.

As outlined in Section 3, the incorrect unbiasedness condition implies the orthogonality principle. Thus, even if the estimator is biased, the algorithm in  finds the optimal solution with zero internal noise, as this principle holds for the optimal Kalman filter. However, for non-zero internal noise, \(_{}>0\), the TOD algorithm underperforms due to the breakdown of the orthogonality principle. In Appendix A.5.3, we also discuss that this suboptimality is observed before the algorithm converges, when the optimal control law is derived from fixed suboptimal filters, and vice versa, regardless of the presence of internal noise.

Multi-Dimensional Case: A Motor Control ApplicationTo demonstrate the scalability of our algorithm to more realistic motor control scenarios, we examine a problem with a four-dimensional state vector (\(m=4\)) and one-dimensional control and sensory feedback (\(p=k=1\)). The task is identical to that in , except that we include internal noise, which was absent in the original formulation. We model a single-joint reaching movement aimed at minimizing the distance between the hand position \(p_{t}\) and a target, while minimizing control effort. The state variable of the problem is \(x_{t}=[p_{t},_{t} dp_{t}/dt,f_{t},g_{t}]\), where \(f_{t}\) is the force acting on the hand and \(g_{t}\) is an auxiliary variable used to filter the control signal \(u_{t}\) (see  and  for a more detailed discussion). We include control and state-dependent noise, as well as internal noise, perturbing the estimate of \(p_{t}\). Note that now we denote \(_{}=^{1,1}}\). All parameters are listed in Appendix A.5.4.

Our results confirm the findings from the previous scenario in this more complex sensorimotor task. The GD algorithm achieves a lower expected accumulated cost, with the performance gap widening as internal noise \(_{}\) increases (Fig. 3a). This is achieved by reducing control gains with increasing \(_{}\) (Fig. 3b), resulting in a smoother control signal on individual trials (Fig. 3c) and overall reduced control effort (Fig. 3d). These adjustments lead to two key behavioral outcomes: slower movements compared to TOD solutions and significantly reduced trial-to-trial variability (Fig. 3e). As mentioned

Figure 2: _Enhanced performance and different solutions with internal noise_. **(a)** Expected accumulated cost \([J]\), computed by averaging the quantity from Eq. 3 over \(50k\) trials, as a function of the internal noise strength \(_{}\), for TOD  and GD (Section 3.1) algorithms (mean \(\) 1SEM from Monte Carlo simulations, error bars not visible as too small). The expected value aligns with our theoretical estimate of \([J]\), as derived in Section 3.1. **(b-c)** Optimal control and filter gains, \(L_{t}\) and \(K_{t}\), for TOD and GD and algorithms. We also present the solutions derived from the analytical counterpart of the numerical GD algorithm to demonstrate that they match the optimal solutions (‘Fixed Point Optimization with Moments Propagation’ – FPOMP – algorithm, see Section 3.3).

earlier, GD outperforms the algorithm proposed by , reducing the cost by up to \(90\%\) when internal noise contributes approximately \(10\%\) of the total noise (\(_{}=0.05\)). To further quantify the impact of internal noise in this scenario and enhance clarity, we calculate the ratio between the average fluctuation amplitude of the state estimate (_FA_, the standard deviation of the state estimate) and the average range of variation of the state (_RV_, the range of variation in position \(p_{t}\), defined as \(_{t}(p_{t})-_{t}(p_{t})\)). The resulting ratio is \(}/} 0.5\) for \(_{}=0.05\) (see also Appendix A.5.5).

We emphasize that the GD algorithm naturally handles arbitrarily high-dimensional problems without requiring any further adjustments. Algorithm 1 in Appendix A.4.2, which serves as the objective function for the numerical optimization via gradient descent, is designed to accommodate arbitrary dimensions for state, control, and sensory feedback, as well as trial duration. However, the time horizon must remain finite by assumption, similar to . We empirically demonstrate the scalability of our algorithm by applying it to a significantly higher-dimensional problem, where the linear dynamical system is governed by random matrices with Gaussian entries. Specifically, we consider \(m=10\), \(p=4\), and \(k=10\) for the dimensions of state, control, and observation, respectively (Appendix A.5.6).

### An Analytical Approach: FPOMP Algorithm

Although, as discussed in the previous section, our GD algorithm performs well for arbitrarily high-dimensional problems (see also Appendix A.5.6), its application to complex, real-world tasks can become computationally expensive. With \(L_{1,,T-1}^{p m}\) and \(K_{1,,T-2}^{m k}\), the total number of parameters to optimize via numerical Gradient Descent is \(mp(T-1)+mk(T-2)\), which can become quite large for high-dimensional state spaces. For example, in the 1D problem, the TOD and GD algorithms have comparable and short computation times. However, for the multi-dimensional sensorimotor task presented above, the GD algorithm takes significantly longer: while the TOD algorithm completes in just a few minutes on a standard laptop, the GD optimization requires several hours (approximately 4 hours).

To address this, we propose an analytically derived iterative algorithm, where we alternate between finding the optimal (i.e., cost-minimizing) \(L_{1,,T-1}\) and \(K_{1,,T-2}\), denoted as \(L_{t}^{*}\) and \(K_{t}^{*}\), for fixed state and state estimate moments, \(_{t}\) and \(_{t}\), and re-computing these moments in light of the updated \(L_{t}\)'s and \(K_{t}\)'s. We refer to this method as the 'Fixed Point Optimization with Moments

Figure 3: _Single-joint reaching task._ **(a)** Expected accumulated cost \([J]\) as a function of the internal noise (mean \(\) 1SEM, error bars not visible as too small), for TOD and GD algorithms. **(b)** Magnitude of the control gain vector as a function of time for TOD and GD solutions. **(c)** control signal \(u_{t}\) in a sample trial for the two algorithms for \(_{}=0.05\). **(d)** Amount of control as \(_{}\) increases, that is, mean integral of the absolute control signal for the two algorithms. **(e)** Mean position over time for the two solutions. All averages are over \(50k\) trials; shadowed areas are SEM.

Propagation' (FPOMP) algorithm. Note that, when optimizing for \(L_{t}\) and \(K_{t}\), we condition also on all the future control and filter gains, therefore not only on \(_{t}\) and \(_{t}\), but also on \(L_{t+1,,T-1}\) and \(K_{t+1,,T-2}\). To simplify the notation, we will omit this implicit dependence and explicitly state only the dependence on \(_{t}\) and \(_{t}\). Therefore, at each iteration, we identify the critical points of the total conditional expected cost with respect to \(L_{t}\) and \(K_{t}\). We can compute the conditional expected cost per step at time \(t+i\), \(i=0,...,T-t\) as

\[[j_{t+i}|_{t},_{t}] =[x_{t+i}|_{t},_{t}]^{}Q_{t+i}[x_{t+i}|_{t},_{t}]+\] (16) \[+[_{t+i}|_{t},_{t}]^{}L_{t+i} ^{}R_{t+i}L_{t+1}[_{t+i}|_{t},_{t}]+\] \[+Tr[Q_{t+i}_{x_{t+i}|_{t},_{t}]}+Tr[L_{t+i}^{ }R_{t+i}L_{t+i}_{_{t+i}|_{t},_{t}}]\,\]

where \([x_{t+i}|_{t},_{t}]\), \([x_{t+i}|_{t},_{t}]\), \(_{x_{t+i}|_{t},_{t}}\) and \(_{_{t+i}|_{t},_{t}}\) are computed by propagating the moments \(_{t}\) and \(_{t}\) (Eqs. 14-15) until \(=t+i\). Indeed, as discussed in Section 3.1, \(_{t}\) and \(_{t}\) serve as sufficient statistics for computing the expected cost. To derive \(L_{t}^{*}\) and \(K_{t}^{*}\), we set the derivatives of the expected cost in Eq. 9 to zero. Excluding the constant terms, we obtain

\[}_{i=0}^{T-t}[j_{t+i}| _{t},_{t}] =0\] (17) \[}_{i=1}^{T-t}[j_{t+i }|_{t},_{t}] =0\.\] (18)

As shown in Appendix A.6.1 and A.6.2, solving Eqs. 17-18 leads to a backward algorithm to compute \(L_{t}^{*}\) and \(K_{t}^{*}\),

\[L_{t}^{*} =f(_{t},_{t},L_{t+1,,T-1}^{*},K_{t+1,,T-2}^{ *})\] (19) \[K_{t}^{*} =g(_{t},_{t},L_{t+1,,T-1}^{*},K_{t+1,,T-2}^{ *})\,\] (20)

with \(t=1,...,T-1\) for \(L_{t}^{*}\) and \(t=1,...,T-2\) for \(K_{t}^{*}\). From this we can build a recursive relationship that, starting from an initial guess for \(L_{1,,T-1}^{*}\) and \(K_{1,,T-2}^{*}\), iteratively computes all the moments \(_{1,,T}\) and \(_{1,,T}\) (Eqs. 14-15) at fixed \(L_{1,,T-1}^{*}\) and \(K_{1,,T-2}^{*}\). Given those moments, \(L_{1,,T-1}^{*}\) and \(K_{1,,T-2}^{*}\) are updated by using Eqs. 19-20, and so on, until convergence is attained. The pseudo-code for the FPOMP algorithm, with its implementation details, can be found in Appendix A.6.3, Algorithm 2. In such a way, we eliminate the numerical optimization procedure, making the algorithm suitable for extremely large optimal control problems. The FPOMP algorithm is flexible and works for arbitrary dimensions of state, control, sensory feedback, and trial duration, with computational costs and runtime comparable to those of the approach proposed in .

In Appendix A.6.1, we explicitly solve Eqs. 17-18 for the one-dimensional case, while in Appendix A.6.2 we extend the approach to a multi-dimensional scenario, considering, for the sake of simplicity, the LQAG problem (but, crucially, including internal noise), to prove the generalizability of Algorithm 2. In Appendix A.6.2, we provide the solution for Eq. 17, with the same procedure applying to Eq. 18. We also discuss the potential extension to the full noise model. Lastly, in Appendix A.7, we examine the assumption of linear dynamics and extend our approach to a switching linear dynamical system to make it less restrictive.

ExperimentsTo empirically validate our iterative algorithm, we apply it to the same one-dimensional problem discussed in Section 3.2. The FPOMP algorithm aligns with the optimal solutions found by the GD algorithm, resulting in identical solution and performance (Figs. 1(b),fig. 1(c)). In Fig. 1(a), the FPOMP algorithm follows the same cost trend as the GD algorithm, with the curves overlapping (the FPOMP curve is omitted for clarity, see also Fig. 10 in Appendix A.8.1).

In Appendix A.8.2, we analyze the same multi-dimensional task as in Section 3.2, excluding multiplicative noise but including internal noise. The results show that the FPOMP algorithm matches the GD optimal solutions for the controller and outperforms TOD when \(_{}>0\). Even in its current form, FPOMP surpasses the method from  when internal noise is considered.

## 4 Conclusion

In this paper, we provide a novel approach for solving stochastic optimal control problems adapted to the noise characteristics of the human sensorimotor system. Our work builds on the seminal study in , where the classical LQG framework (called here LQAG) is extended to the LQMG framework to include both control and signal-dependent noise, as well as internal noise in the estimation process. This extension provides a more realistic description of the sensorimotor system, enabling the reproduction of a larger sample of behavioral data in motor control, albeit at the cost of reduced mathematical tractability.

However, the solution derived in , which is widely used , suffers from an ill-conditioned derivation. Specifically, that solution assumes unbiased estimators -a condition that, as we prove numerically and conceptually in this work, does not hold, leading to suboptimal performance when internal noise is considered or before algorithmic convergence. This suboptimality arises from the close relationship between unbiasedness and the orthogonality principle of an optimal estimator, where the former, mathematically, implies the latter. Yet, the orthogonality principle is satisfied by the optimal filter only in the absence of internal noise and under algorithmic convergence.

Assuming only that control is linear in the current state estimate, we derive an alternative algorithm that optimizes control and estimation without requiring unbiasedness. The optimal solution is obtained by propagating sufficient statistics to compute the expected cost, which is minimized via numerical gradient descent on filter and control gains. For a more constrained, but still relevant, version of the problem, we derive the analytical counterpart, which alternates between forward propagation of moments and backward optimization of control and filter gains until convergence. This makes our approach suitable for high-dimensional problems, significantly reducing computational cost.

We demonstrate superior performance in the presence of internal noise and before convergence is reached (that is, when filter or control gains are fixed at suboptimal values, regardless of the level of internal noise), and provide both mathematical and heuristic explanations. Joint modulation of control and filter gains helps filter internal fluctuations, enhancing adaptability and generalization across internal noise levels, as discussed in Appendix A.5.2. By applying our algorithm to a sensorimotor task, we make novel behavioral predictions that distinguish our solution from previous ones. Specifically, we find that control gains decrease with increasing internal noise, leading to smoother control signals in individual trials. This results in slower movements with reduced trial-to-trial variability.

In summary, our algorithm extends optimal feedback control to a broader range of problems in systems neuroscience.

Limitations and Future WorkOne limitation of our work is the assumption of state-independent filter gains for the optimal estimator: in the presence of multiplicative noise, non-adaptive estimation proves sub-optimal. Additionally, incorporating more realistic cost functions could extend our framework beyond the traditional quadratic dependence. Further investigation into the connections between our optimal control law and biologically plausible learning rules  may also be necessary. Moreover, we have not formally demonstrated the convergence properties of our algorithm to a global minimum, although our algorithm is guaranteed to converge at least to a local minima, and we did not find any numerical evidence for multiple local minima. The next immediate step is to derive the FPOMP algorithm for the general case with multiplicative noise, as discussed in Appendix A.6.2.