# Exact Optimality of Communication-Privacy-Utility Tradeoffs in Distributed Mean Estimation

Berivan Isik

Stanford University

berivan.isik@stanford.edu

&Wei-Ning Chen

Stanford University

wnchen@stanford.edu

&Ayfer Ozgur

Stanford University

aozgur@stanford.edu

&Tsachy Weissman

Stanford University

tsachy@stanford.edu

&Albert No

Hongik University

albertno@hongik.ac.kr

###### Abstract

We study the mean estimation problem under communication and local differential privacy constraints. While previous work has proposed _order_-optimal algorithms for the same problem (i.e., asymptotically optimal as we spend more bits), _exact_ optimality (in the non-asymptotic setting) still has not been achieved. In this work, we take a step towards characterizing the _exact_-optimal approach in the presence of shared randomness (a random variable shared between the server and the user) and identify several conditions for _exact_ optimality. We prove that one of the conditions is to utilize a rotationally symmetric shared random codebook. Based on this, we propose a randomization mechanism where the codebook is a randomly rotated simplex - satisfying the properties of the _exact_-optimal codebook. The proposed mechanism is based on a \(k\)-closest encoding which we prove to be _exact_-optimal for the randomly rotated simplex codebook.

## 1 Introduction

The distributed mean estimation problem has attracted attention from the machine learning community as it is a canonical statistical formulation for many stochastic optimization problems such as distributed SGD [1; 3; 31; 32] and federated learning [33; 34]. As these tasks require data collection from the users, the mean estimation problem has often been studied under privacy constraints to protect users' sensitive information. More specifically, several works [2; 4; 7; 8; 9; 29; 35] have analyzed and improved the tradeoff between the utility and \(\)-local differential privacy (\(\)-LDP) - the predominant paradigm in privacy mechanisms, which guarantees that an adversary cannot distinguish the user data from the outcome of the privacy mechanism [10; 24]. Among them, [4; 8; 9] developed algorithms that are asymptotically optimal, achieving an optimal mean squared error (MSE) proportional to \(()})\), where \(n\) is the number of users, and \(d\) is the input dimension. Later,  proved the corresponding lower bounds that hold for all privacy regimes. However, only PrivUnit enjoys _exact_ optimality among a large family of mechanisms, as proved by , while others provide only _order_ optimality and their performance in practice depends heavily on the constant factors.

Another important consideration in the applications of mean estimation is the communication cost during user data collection. For instance, in federated learning, clients need to send overparameterized machine learning models at every round, which becomes a significant bottleneck due to limited resources and bandwidth available to the clients [22; 25; 28]. This has motivated extensive research on mean estimation [31; 34; 38] and distributed SGD [1; 3; 13; 26; 37] under communication constraints; and communication-efficient federated learning [17; 18; 21; 33].

In addition to the lines of work that studied these constraints (either privacy or communication) separately, recently, there has also been advancement in the joint problem of mean estimation under both privacy and communication constraints.  introduced an _order_-optimal mechanism SQKR requiring only \(O()\) bits by using shared randomness - a random variable shared between the server and the user (see Section 2 for the formal definition). Later,  demonstrated better MSE with another _order_-optimal algorithm, MMRC, by simulating PrivUnit using an importance sampling technique [5; 15] - again with shared randomness. In the absence of shared randomness, the _order_-optimal mechanisms proposed by  do not achieve the best-known accuracy under this setting and are outperformed by the lossless compression approach in  that compresses PrivUnit using a pseudorandom generator (PRG). Due to not using shared randomness, these mechanisms require more bits than others [6; 30] that use shared randomness in the scenarios where it is actually available.

### Contributions

To our knowledge, no existing mechanism achieves _exact_ optimality under both privacy and communication constraints with shared randomness1. In this work, we address this gap by treating the joint problem as a lossy compression problem under \(\)-LDP constraints.

Our first contribution is to demonstrate that the _exact_ optimal scheme with shared randomness can be represented as random coding with a codebook-generating distribution. Specifically, under \(b\) bits of communication constraint, the server and the user generate a codebook consisting of \(M=2^{b}\) vectors (codewords) using shared randomness. The user then selects an index of a vector under a distribution that satisfies \(\)-LDP constraints, and the server claims the corresponding vector upon receiving the index. We term this approach as "random coding with a codebook" and demonstrate that this (random codebook generation) is the optimal way to use shared randomness.

Next, we prove that the _exact_-optimal codebook-generating distribution must be rotationally symmetric. In other words, for any codebook-generating distribution, the distribution remains the same after random rotation. Based on this insight, we propose Random Rotating Simplex Coding (RRSC), where the codebook-generating distribution is a uniformly randomly rotating simplex. This choice of codebook distribution is reasonable as it maximizes the separation between codewords, which efficiently covers the sphere. The corresponding encoding scheme is the \(k\)-closest encoding, where the top-\(k\) closest codewords to the input obtain high sampling probability, and the remaining ones are assigned low probabilities. We show that this scheme is _exact_-optimal for the random rotating simplex codebook.

The proposed codebook generation is valid only when \(M<d\) (or \(b d\) where \(b\) is the communication budget) due to the simplex structure of the codebook. Note that as shown in , \(b d\) bits of communication budget is sufficient to achieve orderwise optimal MSEs under an \(\)-LDP constraint for any \( O( d)\), which is usually a common scenario in practical applications such as federated learning where \(d\) can range from millions to billions. In addition, we can also extend the scheme for cases when \(M d\), by using a codebook consisting of (nearly) maximally separated \(M\) vectors on the sphere. As the number of bits \(b\) used for communication increases, we demonstrate that the proposed scheme approaches PrivUnit, which is the _exact_-optimal scheme without communication constraints.

Finally, through empirical comparisons, we demonstrate that RRSC outperforms the existing _order_-optimal methods such as SQKR and MMRC. We also observe that the performance of RRSC is remarkably close to that of PrivUnit when the number of bits is set to \(b=\).

### Related Work

The \(_{2}\) mean estimation problem is a canonical statistical formulation for many distributed stochastic optimization methods, such as communication (memory)-efficient SGD [31; 34] or private SGD . For instance, as shown in , as long as the final estimator of the mean is unbiased, the \(_{2}\) estimation error (i.e., the variance) determines the convergence rate of the distributed SGD. As a result, there is a long thread of works that study the mean estimation problem under communication constraints [3; 8; 13; 31; 34; 39], privacy constraints [2; 16], or a joint of both [1; 6; 12; 30].

Among them,  shows that \(()\) bits are sufficient to achieve the _order_-optimal MSE \(()})\) and proposes SQKR, an _order_-optimal mean estimation scheme under both privacy and communication constraints. Notice that the MSE of SQKR is _orderwise_ optimal up to a constant factor. Later on, in , it is shown that the pre-constant factor in SQKR is indeed suboptimal, resulting in an unignorable gap in the MSE compared to PrivUnit - an optimal \(_{2}\) mean estimation scheme under \(\)-LDP. In the original PrivUnit, the output space is a \(d\)-dimensional sphere \(^{d-1}\) and hence requires \(O(d)\) bits of communication, which is far from the optimal \(O()\) communication bound. However,  shows that one can (almost) losslessly compress PrivUnit via a pseudo-random generator (PRG). Under the assumption of an existing exponentially strong PRG,  proves that one can compress the output of PrivUnit using \((d)\) bits with negligible performance loss. Similarly,  shows that with the help of shared randomness, PrivUnit can be (nearly) losslessly compressed to \(()\) bits via a channel simulation technique, called MMRC. We remark that although the privacy-utility trade-offs in  and  are (nearly) _exactly_ optimal, the communication efficiency is only _order_-optimal. That is, under an exact \(b\)-bit communication constraint, the MSEs of  (denoted as FT21) and MMRC  may be suboptimal. In this work, we aim to achieve the _exact_-optimal MSE under _both_ communication and privacy constraints.

Furthermore, we show that SQKR, FT21, and MMRC can be viewed as special cases in our framework - i.e., (random) coding with their own codebook design. We elaborate on this in Section 5 and provide more details on prior work in Appendix I.

## 2 Problem Setting and Preliminaries

In this section, we formally define LDP (with shared randomness) and describe our problem setting.

Local Differential Privacy (LDP)A randomized algorithm \(:\) is \(\)-LDP if

\[ x,x^{},y,(y|x)}{ (y|x^{})} e^{}.\] (1)

LDP with Shared RandomnessIn this work, we assume that the encoder and the decoder have access to a shared source of randomness \(U\), where the random encoder (randomizer) \(\) privatizes \(x\) with additional randomness \(U\). Then, the corresponding \(\)-LDP constraint is

\[ x,x^{},y,(y|x,u)}{ (y|x^{},u)} e^{}\] (2)

for \(P_{U}\)-almost all \(u\).

NotationWe let \(^{d-1}=\{u^{d}:||u||_{2}=1\}\) denote the unit sphere, \(e_{i}^{d}\) the standard basis vectors for \(i=1,,d\), \( k\) the greatest integer less than or equal to \(k\), and \([M]=\{1,,M\}\).

Problem SettingWe consider the \(_{2}\) mean estimation problem with \(n\) users where each user \(i\) has a private unit vector \(v_{i}^{d-1}\) for \(1 i n\). The server wants to recover the mean \(_{i=1}^{n}v_{i}\) after each user sends a message using up to \(b\)-bits under an \(\)-LDP constraint. We allow shared randomness between each user and the server. More concretely, the \(i\)-th user and the server both have access to a random variable \(U_{i}^{t}\) (which is independent of the private local vector \(v_{i}\)) for some \(t 1\) and the \(i\)-th user has a random encoder (randomizer) \(f_{i}:^{d-1}^{t}[M]\), where \(M=2^{b}\). We denote by \(Q_{f_{i}}(m_{i}|v_{i},u_{i})\) the transition probability induced by the random encoder \(f_{i}\), i.e., the probability that \(f_{i}\) outputs \(m_{i}\) given the source \(v_{i}\) and the shared randomness \(u_{i}\) is

\[[f_{i}(v_{i},u_{i})=m_{i}]=Q_{f_{i}}(m_{i}|v_{i},u_{i}).\] (3)

We require that the random encoder \(f_{i}\) satisfies \(\)-LDP, i.e.,

\[}(m_{i}|v_{i},u_{i})}{Q_{f_{i}}(m_{i}|v_{i}^{},u_{i})} e ^{}\] (4)for all \(v_{i},v_{i}^{}^{d-1},m_{i}[M]\) and \(P_{U_{i}}\)-almost all \(u_{i}^{t}\).

The server receives \(m_{i}=f_{i}(v_{i},U_{i})\) from all users and generates unbiased estimate of the mean \((m_{1},,m_{n},U_{1},,U_{n})\) that satisfies

\[[(m_{1},,m_{n},U_{1},,U_{n})]= _{i=1}^{n}v_{i}.\] (5)

Then, the goal is to minimize the worst-case error

\[_{n}(f,,P_{U^{n}})=_{v_{1},,v_{n} ^{d-1}}[\|(m_{1},,m_{n},U_{1}, ,U_{n})-_{i=1}^{n}v_{i}\|_{2}^{2}],\] (6)

where \(f\) denotes the collection of all encoders \((f_{1},,f_{n})\). We note that the error is also a function of the distribution of shared randomness, which was not the case for PrivUnit.

## 3 Main Results

### Canonical Protocols

Similar to Asi et al. , we first define the canonical protocol when both communication and privacy constraints exist. The canonical protocols are where the server recovers each user's vector and estimates the mean by averaging them. In other words, the server has a decoder \(g_{i}:[M]^{t}^{d-1}\) for \(1 i M\) which is dedicated to the \(i\)-th user's encoder \(f_{i}\), where the mean estimation is a simple additive aggregation, i.e.,

\[^{+}(m_{1},,m_{n},U_{1},,U_{n})=_{i=1}^ {n}g_{i}(m_{i},U_{i}).\] (7)

Our first result is that the _exact_-optimal mean estimation scheme should follow the above canonical protocol.

**Lemma 3.1**.: _For any \(n\)-user mean estimation protocol \((f,,P_{U^{n}})\) that satisfies unbiasedness and \(\)-LDP, there exists an unbiased canonical protocol with decoders \(g=(g_{1},,g_{n})\) that satisfies \(\)-LDP and achieves lower MSE, i.e.,_

\[_{n}(f,,P_{U^{n}}) _{v_{1},,v_{n}^{d-1}}[ \|_{i=1}^{n}g_{i}(m_{i},U_{i})-_{i=1}^{n}v_ {i}\|^{2}]\] (8) \[}_{i=1}^{n}_{1}(f_{i},g_ {i},P_{U_{i}}),\] (9)

_where \(_{1}(f,g,P_{U})\) is the worst-case error for a single user with a decoder \(g\)._

The main proof techniques are similar to , where we define the marginalizing decoder:

\[g_{i}(m_{i},U_{i})=_{\{v_{j},m_{j},U_{j}\}_{j i}}[n(\{m_{j},U_{j}\}_{j=1}^{n})\ \ f_{i}(v_{i},U_{i})=m_{i},U_{i}].\] (10)

The expectation in (10) is with respect to the uniform distribution of \(v_{j}\)'s. We defer the full proof to Appendix A.

Since the _exact_-optimal \(n\)-user mean estimation scheme is simply additively aggregating user-wise _exact_-optimal scheme, throughout the paper, we will focus on the single-user case and drop the index \(i\) when it is clear from the context. In this simpler formulation, we want the server to have an unbiased estimate \(=g(m,U)\), i.e.,

\[v= _{P_{U},f}[g(f(v,U),U)]\] (11) \[= _{P_{U}}[_{m=1}^{M}g(m,U)Q_{f}(m|v,U)]\] (12)for all \(v^{d-1}\). We assume that the decoder \(g:[M]^{t}^{d}\) is deterministic, since the randomized decoder does not improve the performance. Then, the corresponding error becomes

\[D(v,f,g,P_{U})= _{P_{U},f}\,\|{|g(f(U,v),U)-v|}^{2}\] (13) \[= _{P_{U}}\,_{m=1}^{M}\|{g(m,U)-v}\|^{2}Q_{f}(m |v,U).\] (14)

Finally, we want to minimize the following worst-case error over all \((f,g)\) pairs that satisfy the unbiasedness condition in (12)

\[_{1}(f,g,P_{U})=_{v^{d-1}}D(v,f,g,P_{U}).\] (15)

### Exact Optimality of the Codebook

We propose a special way of leveraging shared randomness, which we term as _random codebook_. First, we define a codebook \(U^{M}=(U_{1},,U_{M})(^{d})^{M}\), consisting of \(M\) number of \(d\)-dimensional random vectors generated via shared randomness (i.e., both the server and the user know these random vectors). We then define the corresponding simple selecting decoder \(g^{+}:[M](^{d})^{M}^{d}\), which simply picks the \(m\)-th vector of the codebook upon receiving the message \(m\) from the user:

\[g^{+}(m,U^{M})=U_{m}.\] (16)

Our first theorem shows that there exists a scheme with a random codebook and a simple selecting decoder that achieves the _exact_-optimal error. More precisely, instead of considering the general class of shared randomness (with general dimension \(t\)) and the decoder, it is enough to consider the random codebook \(U^{M}(^{d})^{M}\) as the shared randomness and the simple selector \(g^{+}\) as the decoder.

**Lemma 3.2**.: _For any \(f,g,P_{U}\) with \(U^{t}\) that are unbiased and that satisfy \(\)-LDP, there exists a shared randomness \(^{M}(^{d})^{M}\) and random encoder \(f_{0}:^{d-1}(^{d})^{M}[M]\) such that_

\[D(v,f,g,P_{U})=D(v,f_{0},g^{+},P_{^{M}})\] (17)

_for all \(v^{d-1}\), where \(f_{0},g^{+},P_{^{M}}\) also satisfy unbiasedness and \(\)-LDP._

The main step of the proof is to set an implicit random codebook with codewords \(_{m}=g(m,U)\) for \(m=1,...,2^{b}\) and show that we can obtain an essentially equivalent scheme with a different form of shared randomness \(^{M}\), which is an explicit random codebook. The detailed proof is given in Appendix B. Thus, without loss of generality, we can assume \(t=M d\) and the random codebook \(U^{M}\) is the new shared randomness, where the decoder is a simple selector. Since we fix the decoder, we drop \(g\) to simplify our notation. We say the random encoder \(f\) satisfies unbiasedness condition if

\[_{P_{U}}\,_{m=1}^{M}U_{m}Q_{f}(m|v,U^{M})=v,\] (18)

and the worst-case error is

\[(f,P_{U^{M}}) =_{v^{d-1}}D(v,f,P_{U^{M}})\] (19) \[=_{v^{d-1}}_{P_{U^{M}}}\,_ {m=1}^{M}\|{U_{m}-v}\|^{2}Q_{f}(m|v,U).\] (20)

Thus, the goal is now to find the _exact_-optimum codebook generating distribution \(P_{U^{M}}\), and the random encoder \(f\) (or the probability assignment \(Q_{f}(|v,U)\)). We then argue that the _exact_-optimal codebook should be rotationally symmetric.

**Definition 3.3**.: A random codebook \(U^{M}(^{d})^{M}\) is **rotationally symmetric** if \((U_{1},,U_{M})}{{=}}(A_{0}U_{1},,A_{0} U_{M})\) for any \(d d\) orthonormal matrix \(A_{0}\).

The next lemma shows that the _exact_-optimal \(P_{U^{M}}\) is rotationally symmetric.

**Lemma 3.4**.: _Let \(P_{U^{M}}\) be a codebook generating distribution, and suppose random encoder \(f\) satisfies unbiasedness and \(\)-LDP. Then, there exists a random encoder \(f_{1}\) and rotationally symmetric random codebook \(^{M}\) such that_

\[(f,P_{U^{M}})(f_{1},P_{^{M}}),\] (21)

_which also satisfies unbiasedness and \(\)-LDP._This is mainly because the goal is to minimize the worst-case error, and the codebook-generating distribution should be symmetric in all directions. The proof is provided in Appendix C. The next lemma shows that the _exact_-optimal scheme has constant error for all \(v^{d-1}\).

**Lemma 3.5**.: _For any rotationally symmetric codebook generating distribution \(P_{U^{M}}\) and an unbiased randomized encoder \(f\) that satisfies \(\)-LDP, there exists a random encoder \(f_{2}\) such that_

\[(f,P_{U^{M}})(f_{2},P_{U^{M}}), { where }D(v,f_{2},P_{U^{M}})=D(v^{},f_{2},P_{U^{M}})\] (22)

_for all \(v,v^{}^{d-1}\)._

The formal proof is given in Appendix D. Since the codebook is symmetric (Lemma 3.4), the _exact_-optimal encoding strategy remains the same for any input \(v\). Thus, without loss of generality, we can assume that the input is a standard unit vector \(v=e_{1}=(1,0,,0)\).

### Rotationally Symmetric Simplex Codebook

Now, we focus on a particular rotationally symmetric codebook. Notice that the codebook \(U^{M}\) has a similar role to the codebook in lossy compression, in the sense that we prefer the codeword \(U_{m}\) close to the input vector \(v\). Thus, it is natural to consider the maximally separated codebook so that the \(M\) vectors \(U_{1},,U_{M}\) cover the source space effectively. For \(M<d\), the maximally separated \(M\) vectors on the unit sphere \(^{d-1}\) is a simplex. More precisely, let \(s_{1},,s_{M}^{d}\) form a simplex:

\[(s_{i})_{j}=}&i=j\\ -}&i jj M\\ 0&j>M.\] (23)

Then, we can define the rotationally symmetric simplex codebook \(U^{M}\)

\[(U_{1},U_{2},,U_{M})}{{=}}(rAs_{1},rAs_{2}, ,rAs_{M}),\] (24)

where \(A\) is uniformly drawn orthogonal matrix and \(r>0\) is a normalizing constant. We then need to find the corresponding encoder \(f\) that minimizes the error. Recall that the error is

\[_{P_{U^{M}}}[_{m=1}^{M}\|U_{m}-v\|^{2}Q_{f}(m|v,U)],\] (25)

and it is natural to assign high probabilities to the message \(m\) with low distortion \(\|U_{m}-v\|^{2}\) as long as \(\)-LDP constraint allows. More precisely, we call the following probability assignment "\(k\)-closest" encoding:

\[Q_{f}(m|v,U^{M})=}{ke^{}+(M-k)}& \|v-U_{m}\|^{2} k\\ -1)+1}{ke^{}+(M-k)}& \|v-U_{m}\|^{2} k+1\\ +(M-k)}&,\] (26)

where we allow non-integer \(k\). The choice of \(r=r_{k}\) is described in Section 3.4. We call this approach Randomly Rotating Simplex Coding (RRSC) and provide the pseudocode in Algorithm 1. We note that the codewords \(U_{m}\)'s with smallest \(\|v-U_{m}\|^{2}\) and codewords \(U_{m}\)'s with largest \( v,U_{m}\) coincide for a codebook with fixed-norm codewords \(U_{m}\)'s, which is the case for the rotationally symmetric simplex codebook. Our main theorem is that the \(k\)-closest encoding is _exact_-optimum if the codebook generating distribution is rotationally symmetric simplex.

**Theorem 3.6**.: _For a rotationally symmetric simplex codebook, there exists a \(k\) such that the "\(k\)-closest" encoding is the exact-optimum unbiased scheme that satisfies \(\)-LDP constraint._

The main step of the proof is to show that all the probabilities should be either the maximum or the minimum in order to minimize the error, and the proof is given in Appendix E.

### \(k\)-closest Encoding for General Rotationally Symmetric Codebook

In this section, we demonstrate that the \(k\)-closest encoding consistently yields an unbiased scheme for any rotationally symmetric codebook. To be more specific, for any given spherically symmetric codebook \(U^{M}\), there exists a scalar \(r_{k}\) that ensures that the \(k\)-closest encoding with \(r_{k}U^{M}=(r_{k}U_{1},,r_{k}U_{M})\) is unbiased. Let \(T_{k}(v,U^{M})=\{m\,:\,U_{m}k\}\), and without loss of generality, let us assume \(v=e_{1}\). Then,

\[_{P_{U^{M}}}[_{m=1}^{M}Q_{f}(m|e_{1},U^{M})U_{m}]\] \[=_{P_{U^{M}}}[-1}{ke^{ }+(M-k)}_{m T_{k}(e_{1},U^{M})}U_{m}++(M-k)}_{m=1}^{M}U_{m}]\] (27) \[=_{P_{U^{M}}}[-1}{ke^{ }+(M-k)}_{m T_{k}(e_{1},U^{M})}U_{m}],\] (28)

where \([ U_{m}]=0\) due to rotationally symmetric codebook and we assume an integer \(k\) for the sake of simplicity. Since the codebook is rotationally symmetric and we pick \(k\)-closest vectors toward \(v=e_{1}\), each codeword \(U_{m} T_{k}(e_{1},U^{M})\) is symmetric in all directions other than \(v=e_{1}\). Thus, in expectation, the decoded vector is aligned with \(e_{1}\), and there exists \(r_{k}\) such that

\[r_{k}_{P_{U^{M}}}[_{m=1}^{M}Q_{f}(m|e_{1},U^{M})U_{m }]=e_{1}.\] (29)

For a rotationally symmetric simplex codebook, where \(U_{m}=As_{m}\) for a uniform random orthogonal matrix \(A\), we have an (almost) analytic formula.

**Lemma 3.7**.: _Normalization constant \(r_{k}\) for \((k)\) is_

\[r_{k}=+M-k}{e^{}-1}} {1}{C_{k}},\] (30)

_where \({C_{k}}^{2}\) is an expected sum of top-\(k\) coordinates of uniform random vector \(a^{d-1}\)._

The key idea in the proof is to show that encoding \(e_{1}\) with \(As^{M}\) is equivalent to encoding uniform random vector \(a^{d-1}\) with \(s^{M}\). The formal proof is provided in Appendix F.

The following lemma controls the asymptotic behavior of \(C_{k}\):

**Lemma 3.8**.: _Let \(C_{k}\) be defined as in Lemma 3.7. Then, it holds that_

\[C_{k}=O( M}{d}}).\] (31)_In addition, there exist absolute constants \(C_{1},C_{2}>0\) such that as long as \( M/k>C_{1}\) and \(k>C_{2}\),_

\[C_{k}=(}{d}()}).\] (32)

As a corollary, Lemma 3.8 implies the order-wise optimality of RRSC:

\[() r_{k}^{2}-1=O(-1-)^{2}}{(e^{}-1)^{2}})}).\]

By picking \(k=(1,Me^{-})\), the above error is \(O(,,b)})\). We provide the proof of Lemma 3.8 in Appendix G.

### Convergence to PrivUnit

As the communication constraint \(b\) increases, the _exact_-optimal scheme with communication constraint should coincide with the _exact_-optimal scheme _without_ communication constraint, which is PrivUnit. Note that the rotationally symmetric simplex can be defined only when \(M=2^{b}<d\), due to its simplex structure. However, we have a natural extension where the codebook is a collection of \(M\) (nearly) maximally separated vectors on the sphere of radius \(r\), where we can assume that \(M\) codewords are uniformly distributed on the sphere of radius \(r_{k}\) if \(M\) is large enough. Consider the case where \(q=\) is fixed and \(M=2^{b}\) is large. Since the \(k\)-closest encoding yields an unbiased scheme with error \((f,P_{U^{M}})=r_{k}^{2}-1\), where \(r_{k}\) is normalizing constant, for uniformly distributed \(M\) codewords on the sphere, the constant \(r_{k}\) should satisfy

\[r_{k}-1}{ke^{}+(M-k)}[ _{mk}U_{m,1}]=1\] (33)

where \(U_{m,1}\) is the first coordinate of uniformly drawn \(U_{m}\) from the unit sphere \(^{d-1}\). Then, as \(M\) increases, \(U_{m,1}\) being one of the top-\(k\) becomes equivalent to \(U_{m,1}>\), where \(\) is the threshold such that \([U_{m,1}>]=q\). Hence, assigning higher probabilities to the top-\(k\) closest codewords becomes equivalent to assigning high probabilities to the codewords with \( U_{m},e_{1}>\) where \(v=e_{1}\). This is essentially how PrivUnit operates.

### Complexity of RRSC

Each user has \(d d\) orthonormal matrix \(A\) and needs to find \(k\) smallest \( v,As_{m}\) for \(1 m M\). Since \( v,As_{m}= A^{}v,s_{m}\), it requires \(O(d^{2})\) to compute \(A^{}v\) and additional \(O(Md)\) to compute all inner products for \(1 m M\). However, if \(M d\), we have a simpler equivalent scheme using

\[ A^{}v,s_{m}=a_{m}^{}v- _{i=1}^{M}a_{i}^{}v}},\] (34)

where \(a_{m}^{}\) is the \(m\)-th row of the matrix \(A\). Then, it only requires storing the first \(M\) rows of the matrix and \(O(Md)\) to obtain all inner products in (34) by avoiding \(O(d^{2})\) to construct \(A^{}v\).

On the other hand, the server computes \(As_{m}\) upon receiving a message \(m\). The corresponding time complexity is \(O(Md)\) (per user) since \(s_{m}\) has \(M\) non-zero values. We note that both MMRC and FT21 require the same encoding complexity \(O(Md)\) as RRSC, where they choose \(M=O(())\).

## 4 Experiments

We empirically demonstrate the communication-privacy-utility tradeoffs of RRSC and compare it with _order_-optimal schemes under privacy and communication constraints, namely SQKR and MMRC. We also show that RRSC performs comparably with PrivUnit, which offers the _exact_-optimal privacy-utility tradeoffs without communication constraints . In our simulations, we use the "optimized" PrivUnit mechanism, called PrivUnitG, introduced in , which performs better than PrivUnit in practice since it provides an easy-to-analyze approximation of PrivUnit but with analytically better-optimized hyperparameters. Similar to [6; 30], we generate data independently but non-identically to capture the distribution-free setting with \( 0\). More precisely, for the first half of the users, we set \(v_{1},,v_{n/2}}}{{}}N(1,1)^{  d}\); and for the second half of the users, we set \(v_{n/2+1},,v_{n}}}{{}}N(10,1)^ { d}\). We further normalize each \(v_{i}\) to ensure that they lie on \(^{d-1}\). We report the average \(_{2}\) error over \(10\) rounds together with the confidence intervals. To find the optimal values for \(k\) and \(r_{k}\), we compute the optimal \(r_{k}\) using the formula in (33) for \(k=1,,M\) and pick the \(k\) that gives the smallest \(r_{k}\) (which corresponds to the bias). To estimate the expectation \(C_{k}\) in (33), we run a Monte Carlo simulation with \(1M\) trials. We report the \(k\) we use for each experiment in the captions. Additional experimental results are provided in Appendix H.

In Figure 1-(left, middle), we report \(_{2}\) error for \(=1,,8\), where for each method (except PrivUnitG), the number of bits is equal to \(b=\). In Figure 1-(right), we report \(_{2}\) error by fixing \(=6\) and sweeping the bitrate from \(b=1\) to \(b=8\) for RRSC and MMRC. For SQKR, we only sweep for \(b\) as it leads to poor performance for \(b>\). In each figure, RRSC performs comparably to PrivUnitG even for small \(b\) and outperforms both SKQR and MMRC by large margins.

The codebase for this work is open-sourced at https://github.com/BerivanIsik/rrsc.

## 5 Discussion & Conclusion

We proved that using a rotationally symmetric codebook is a necessary condition for the _exact_ optimality of mean estimation mechanisms with privacy and communication constraints. We then proposed Random Rotating Simplex Coding (RRSC) based on a \(k\)-closest encoding mechanism and proved that RRSC is _exact_-optimal for the random rotating simplex codebook. We now discuss some important features of RRSC and provide conjectures for future work.

Unified FrameworkIt turns out that SQKR , FT21  and MMRC  can be viewed as special cases in our framework. Specifically, SQKR  uses Kashin's representation of \(v=_{j=1}^{N}a_{j}u_{j}\), where \(\{a_{j}\}_{j=1}^{N}[-c/,c/]\) for some \((1+)d\) with \(>0\) and \(c>0\). Then the SQKR encoder quantizes each \(a_{j}\) into a 1-bit message \(q_{i}\), and draws \(k\) samples with the help of shared randomness. This can be viewed as random coding with a codebook-generating distribution. More concretely, the corresponding codebook \(U^{M}\) consists of \(k\) non-zero values of \( c/\) where the randomness is from selecting \(k\) indices using shared randomness. On the other hand, since MMRC  is simulating the channel corresponding to a privacy mechanism, it can be viewed as pre-generating random codewords \(U^{M}\) according to the reference distribution, where the importance sampling is also a way of assigning probabilities to each codeword. As elaborated in Section 3.5, it is observed that with an increase in the communication constraint \(b\), the suggested \(k\)-closest encoding gradually transforms into a threshold-based encoding, analogous to that of MMRC. The codebook associated with FT21 

Figure 1: Comparison of RRSC with SQKR , MMRC , and PrivUnitG . **(left)**\(_{2}\) error vs \(\) with \(n=5000\), \(d=500\). The number of bits is \(b=\) for RRSC, SQKR, and MMRC. The choice of \(k\) for \(k\)-closest encoding is \(k=1\) for each \(\). **(middle)** Same plot zoomed into higher \(\), lower \(_{2}\) error region. **(right)**\(_{2}\) error vs number of bits \(b\) for \(n=5000\), \(d=500\), and \(=6\). For SQKR, we only report \(b=6\) since it performs poorly when \(b>\). The choice of \(k\) for \(k\)-closest encoding is \(k=\) for \(b=\), respectively.

depends on the PRG it uses. Let \(:\{0,1\}^{b}\{0,1\}^{(d)}\) be a PRG that takes a \(b\)-bit seed and maps it into \((d)\) bits, where \(b d\), and let \(g:\{0,1\}^{(d)}^{d}\). For example, if we represent each coordinate of \(x^{d}\) as a 32-bit float, then \(g()\) maps the float representation of \(x\) (a \(32\)-bit string) to \(x\). With a PRG, FT21 mimics PrivUnit by first generating a \(b\)-bit seed \(m\), computing \(g((m))\), and then performing rejection sampling on the seed space. The above procedure can be treated as a special case in our framework, where the _deterministic_ codebook consists of \(2^{b}\) points on \(^{d}\): \(_{}:=\{g((m)):m\{0,1\}^{b}\}\). The probabilities assigned to each codeword according to the rejection sampling are equivalent to a threshold-based assignment.

Shared randomnessWhen \(M d+1\), additional randomization is required during codebook generation to achieve an unbiased scheme, as discussed in . Furthermore, both the encoder and decoder must possess this randomization information. In the proposed RRSC scheme, this randomization is achieved through the random rotation of the simplex code using shared randomness. However, it is possible to circumvent the need for shared randomness by having the server generate random rotation matrices using its private coin and communicate them to the users. This approach replaces shared randomness with downlink communication, which is typically more affordable than uplink communication. It should be noted that directly transmitting the rotation matrices would require \(O(d^{2})\) bits. Nonetheless, the server can generate them using a predetermined pseudo-random generator (PRG) and transmit only the seeds of it to the users. Drawing from a similar argument as in , assuming the existence of exponentially strong PRGs, seeds with \((d)\) bits are sufficient.

Future WorkWe showed the _exact_-optimality of \(k\)-closest encoding for the rotating simplex codebook. In general, it also achieves unbiasedness and the following error formulation \(_{P_{UM}}[_{m=1}^{M}Q_{f}(m|v,U^{M})\|v-U_{m}\|^{2}]\) implies the _exact_-optimality of \(k\)-closest encoding for any rotationally symmetric codebook, which leads us to the following conjecture.

**Conjecture 5.1**.: _The proposed \(k\)-closest encoding is exact-optimal for any rotationally symmetric codebook._

It also remains unclear whether \(k\) can depend on the realization of the codebook \(U^{M}\) in general, which we leave to future work. We also proved that the _exact_-optimal codebook must be rotationally symmetric. We conjecture that the maximally separated codebook (simplex codebook) is _exact_-optimal as it provides the most effective coverage of the space \(^{d-1}\). This, too, is left as a topic for future work.

**Conjecture 5.2**.: _The rotationally symmetric simplex codebook is the exact-optimal codebook._

Limitations and Broader ImpactWhile we take an important step towards exact optimality by proving several necessary conditions and by providing a mechanism that is _exact_-optimal for a family of codebooks, we still have the above conjectures left to be proven in future work.

## 6 Acknowledgement

The authors would like to thank Peter Kairouz for the helpful discussions in the early stage of this work; and Hilal Asi, Vitaly Feldman, and Kunal Talwar for sharing their implementation for the PrivUnitG mechanism . BI was supported by a Stanford Graduate Fellowship, a Google PhD Fellowship, and a Meta research grant. WNC was supported by NSF under Grant CCF-2213223. AN was supported by the Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (2021R1F1A1059567).