# Learning to Embed Distributions via Maximum Kernel Entropy

Oleksii Kachaiev

Dipartimento di Matematica, Universita degli Studi di Genova, Genoa, Italy

oleksii.kachaiev@gmail.com

Stefano Recanatesi

Technion Israel Institute of Technology, Haifa, Israel

Allen Institute for Neural Dynamics, Seattle, USA

stefano.recanatesi@gmail.com

###### Abstract

Empirical data can often be considered as samples from a set of probability distributions. Kernel methods have emerged as a natural approach for learning to classify these distributions. Although numerous kernels between distributions have been proposed, applying kernel methods to distribution regression tasks remains challenging, primarily because selecting a suitable kernel is not straightforward. Surprisingly, the question of learning a data-dependent distribution kernel has received little attention. In this paper, we propose a novel objective for the unsupervised learning of data-dependent distribution kernel, based on the principle of entropy maximization in the space of probability measure embeddings. We examine the theoretical properties of the latent embedding space induced by our objective, demonstrating that its geometric structure is well-suited for solving downstream discriminative tasks. Finally, we demonstrate the performance of the learned kernel across different modalities.

## 1 Introduction

Most discriminative learning methods conventionally assume that each data point is represented as a real-valued vector. In practical scenarios, however, data points often manifest as a'set' of features or a 'group' of objects. A quintessential example is the task of predicting a health indicator based on multiple blood measurements. In this case, the single data point of a patient has multiple, or a distribution of, measurements. One approach to accommodate such cases involves representing each input point as a probability distribution. Beyond mere convenience, it is more appropriate to model input points as distributions when dealing with missing data or measurement uncertainty, as often encountered when facing the abundance of data, which commonly presents a challenge for data-rich fields such as genetics, neuroscience, meteorology, astrophysics, or economics.

The task of regressing a mapping of probability distributions to a real-valued response is known as _distribution regression_. Distribution regression has been successfully applied in various fields, such as voting behavior prediction , dark matter halo mass learning , human cancer cells detection , brain-age prediction , among others . The versatility and effectiveness of this framework underscore its power in solving complex problems . Kernel methods have become a widely used approach for solving distribution regression tasks by exploiting a kernel between distributions referred to as a _distribution kernel_. Despite the multitude of proposed kernels, the practical application of kernel methods remains challenging due to the nontrivial choice of the appropriate kernel. While some efforts focus on identifying kernels with broad applicability andfavorable statistical properties , others aim to tailor kernels to the geometric characteristics of specific input spaces . Remarkably, the question of learning data-dependent kernels has received limited attention. This study is thus driven by a fundamental question: What are the underlying principles that facilitate the unsupervised learning of an effective kernel, one that optimally encapsulates the data properties and is well suited for discriminative learning on distributions?

In this work, we leverage a key insight: an appropriate selection of the distribution kernel enables the embedding of a set of distributions into the space of covariance operators. Building on this theoretical idea, we claim that quantum entropy maximization of the corresponding covariance operator is a suitable guiding principle to learn data-dependent kernel. This, combined with a careful design of kernel parametrization, let us to devise a differentiable optimization objective for learning data-specific distribution embedding kernels from unlabeled datasets (i.e. unsupervised) (Fig. 1). We show that the entropy maximization principle facilitates learning of the latent space with geometrical configuration suitable for solving discriminative tasks [3; 20]. We empirically demonstrate the performance of our method by performing classification tasks in multiple modalities.

In summary, our unsupervised data-dependent distribution kernel learning framework introduces a theoretically grounded alternative to the common practice of hand-picking kernels. Such framework could be further leveraged for generalizing existing learning approaches [61; 32] catalyzing the use of distribution-based representations within the broader scientific community.

## 2 Preliminaries

We first introduce the main concepts necessary to formalize our learning framework: kernel mean embeddings and covariance operator embeddings.

### Kernel Embeddings of Distributions

Consider an input space \(\) and a positive-definite (p.d.) kernel \(k:\). Let \(\) be the corresponding reproducible kernel Hilbert space (RKHS) induced by such kernel. Consider a probability distribution \(P()\). The _kernel mean embedding_ map embeds the distribution \(P\) as a function in Hilbert space:

\[_{P}(P)_{}k(x,)\,dP(x)=_{ }(x)\,dP(x)\;, \]

where \(:\) is a feature map such that \((x)=k(x,)\).

Importantly, if the kernel \(k\) is _characteristic_, the mapping \(:()\) is injective, implying that all information about the original distribution is preserved in \(\). This last property underscores much of power under recent applications of kernel mean embeddings [41; 48; 14]. The natural empirical estimator for the kernel mean embedding approximates the true distribution with a finite

Figure 1: **Learning to embed distributions. (a) Example of multiple distributions over the input space. (b) The trainable function \(f_{}\) encodes the input dataset into a compact latent space, in our case \(=^{d-1}\). (c) The first-level _embedding_ kernel \(k\) induces kernel mean embedding map to \(\). The encoder is optimized to maximize the entropy of the covariance operator embedding of the dataset w.r.t. the second-level _distribution_ kernel \(K\) between kernel mean embeddings in \(\). (d) Utilizing learned data-dependent kernel, downstream classification tasks can be solved using tools such as Kernel SVM or Kernel Ridge Regression.**

sum of Dirac delta functions:

\[_{P}_{i=1}^{N}(x_{i}) \]

where \(x_{1},,x_{N} P\) are \(N\) empirical i.i.d. samples. The estimator has a dimension-free sample complexity error rate of \((N^{-})\).

Additionally, we denote \(d_{k}:_{ 0}\) a _kernel metric_ induced by a kernel \(k\),

\[d_{k}(x,x^{})=\|(x)-(x^{})\|_{}\,. \]

Note that \(d_{k}\) is a metric on \(\) if feature map \(\) is injective.

### Covariance Operators and Entropy

A second way of mapping a probability distribution to a Hilbert space can be defined by means of a covariance operators. For a given feature map \((x)=k(x,):\)1, and a given probability distribution \(P()\), the _covariance operator embedding_ is defined as:

\[_{P}_{}(x)(x)\,dP(x) \]

where \(\) is a tensor product. \(_{P}\) is a self-adjoint positive semi-definite (p.s.d.) operator acting on \(\). Such operator can be seen as a mean embedding w.r.t. the feature map \(x(x)(x)\) and therefore, for a _universal_ kernel \(k\), the map \(P_{P}\) is injective (see Bach ).

Similarly to Eq. (2), the natural empirical estimator is:

\[_{P}=_{i=1}^{N}(x_{i})(x_{i}) \]

where \(x_{1},,x_{N} P\) are \(N\) i.i.d. samples.

For a translation-invariant kernel \(k(x,x^{})=(x-x^{})\) normalized such that \(k(x,x)=1\), the covariance operator \(_{P}\) is a _density operator_. Henceforth, entropy measures can be applied to it, and the quantum Renyi entropy of the order \(\) can be defined as:

\[_{}(_{P})[(_{P})^{}]=_{i}_{i}^{} \]

where \(\{_{i}\}_{i}\) are the eigenvalues of \(_{P}\). The Von Neumann entropy can be seen as a special case of Renyi entropy in the limit \( 1\). However, in our work, we focus primarily on the second-order case of Renyi entropy, i.e. \(=2\) (see Carlen , Muller-Lennert et al. , Wilde , Giraldo et al.  for an in-depth overview of the properties and theory of quantum entropies).

## 3 Unsupervised Distribution Kernel Learning

### Distribution Regression

In this section, we discuss the key topics in distribution regression, including problem setup, the notion of a 2-stage sampling process, and the common solutions to regression employing kernel methods.

_Distribution regression_ extends the common regression framework to the setup where covariates are given as probability distributions available only through samples. Formally, consider the task of finding a regressor \(f:()\) from the dataset of samples \(_{M}=\{(P_{i},y_{i})\}_{i=1}^{M}\) where \(P_{i}()\) are distributions provided as a set of i.i.d. empirical samples \(x_{1},,x_{N_{i}} P_{i}\) (see Poczos et al. , Szabo et al.  for a comprehensive analysis). A viable approach to solving this problem is to define a kernel \(K:()()\) that is _universal_ in \(()\). By setting up such a kernel \(K\), we can utilize kernel-based regression techniques, with SVM often being the preferred method for classification tasks  or Kernel Ridge Regression (KRR) when the space \(\) is continuous 2. To this end, several kernels have been proposed over time (see details in Sec. 4).

One possibility, proposed by Muandet et al. , is to introduce a kernel in the input space, the so called _embedding kernel_\(k_{}:\) and exploit the induced mean embeddings \(_{}:()_{}\) to map input distributions to points in RKHS. Subsequently, to define a second level kernel, _distribution kernel_\(K_{}:_{}_{} \) between points in the RKHS \(_{}\). The simplest choice for such a distribution kernel is the linear kernel:

\[K_{l}(_{P},_{Q})_{P},_{Q}_{_{ }}=_{}k(x,x^{})\,dP(x)\,dQ(x ^{})\;. \]

A standard alternative to the linear kernel is a Gaussian kernel with bandwidth parameter \(>0\):

\[K_{}(_{P},_{Q})(-\|_{P }-_{Q}\|_{_{}}^{2})=(-d_{k_{}}(_{P},_{Q})^{2})\,, \]

which was shown to be _universal_ in \(()\). The Gaussian kernel \(K_{}\) can be computed from the linear kernel \(K_{l}\) as \(d_{k_{}}(_{P},_{Q})^{2}=K_{l}(P,P)+K_{l}(Q,Q)-2K_{l}(P,Q)\).

In practice, we only have access to a finite number of samples of each input distribution, thus the _distribution kernel_ is approximated using the natural estimator for the kernel mean embedding. The related excess risk for the regression solution is analyzed in Szabo et al. .

### Dataset Embedding

Instead of using standard kernels designed to encapsulate the geometry of the input space, we consider learning a data-dependent kernel, tailored to the specific properties of the dataset. In a similar vein, Yoshikawa et al.  proposed learning an optimal kernel (or equivalently, a feature map) jointly with the classifier to address the text modality. In this work, we focus on an _unsupervised_ problem, aiming to learn a data-dependent kernel between probability distributions without access to classification labels.

We first introduce proper parametrization to ensure both expressivity and robustness followed by the definition of the optimization objective. Leveraging the idea of 2-level kernel setup, we define the _embedding kernel_ as

\[k_{}:=k_{}(f_{} (x),f_{}(x^{}))\;. \]

where \(f_{}\) is a trainable encoder function \(f_{}:\), \(\) is a latent encoding space, and \(k_{}\) is a kernel defined on the latent space \(k_{}:\). The encoder function \(f_{}\) transforms every

Figure 2: **Properties of the entropy on the toy example.** (a) Entropy and Distributional Variance for 6 distributions on a sphere as a function of their geometrical arrangement parametrized by \(\). (b) Kernel norms that enter the distributional variance bound. The blue shaded area (difference between blue and red lines) corresponds to the dotted red line in (a) (up to multiplicative factor). (c) Flattening of Gram matrix eigenvalues as a function of \(\).

input probability distribution \(P()\) into a distribution over the latent space \(_{}()\)3 (Fig. 1a). Furthermore, we denote RKHS corresponding to the kernel \(k_{}\) as \(_{}\) and the kernel mean embedding map as \(_{}\) (see Eq. (1)).

\[P()}P_{}( )}}_{}(P)_{ }\,. \]

These transformations define mean embeddings for each input probability distributions through the first level, embedding kernel \(k_{}\) (Fig. 1b). The second level, _distribution kernel_\(K_{}:_{}_{} \) is defined over the mean embeddings \(_{}(P)\)'s. We can now consider to embed dataset \(_{M}=\{P_{i}\}_{i=1}^{M}\) as an empirical covariance operator (see Eq. (5)), i.e.

\[_{M}},K_{}}_{ }=_{P_{M}}K_{}(_{}(P),) K_{}(_{}(P),)\,. \]

As \(_{}\) encapsulates information about the entire dataset, we term it _dataset embedding_. With the assumption that the dataset \(_{M}\) is sampled i.i.d. from the (unknown) true meta-distribution \(\), \(_{}\) is a natural estimator to approximate the true covariance operator \(_{}\). To simplify the notation we use \(_{}\) in place of \(_{}\) unless required by the context.

Both the _embedding_ kernel \(k_{}\) and the _distribution_ kernel \(K_{}\) remain fixed throughout the training, learning happens by adjusting the parametrization of the latent space encoder \(f_{}\). Such separation ensures expressivity while conforming to all technical requirements for a distribution kernel. Throughout the paper, we make the following assumptions on the latent space \(\) and embedding kernel \(k\).

**Assumption 3.1**.: Latent space \(\) is a compact subset of \(^{d}\). Kernel \(k_{}:\) is a p.d. characteristic translation-invariant kernel \(k_{}(z,z^{})=f(\|z-z^{}\|^{2})\) such that \(-f^{}\) is completely monotone on \((0,)\) (see Definition 2.2.4 of Borodachov et al. ) and \( z:\,k_{}(z,z)=1\).

The choice of a kernel based on Euclidean distance in the latent space makes its definition similar to that in Weinberger and Tesauro , though the parametrization of the encoding process differs. Learning kernels by explicitly learning feature maps has been explored in a wide range of settings . In contrast, the parametrization proposed in our study applies a known characteristic kernel to a learned latent representation. To facilitate our optimization process (which will be explained shortly), we opt for \(=^{d-1}\) (the d-dimensional hypersphere) and Gaussian kernel both for \(k_{}\) and \(K_{}\) (see Eq. (8)). We retain other suitable choices as potential avenues for future research.

### Unsupervised Optimization Objective

This dataset level representation depends on the choice of first and second level kernels \(k,K\) and, in turn, on the trainable function \(f_{}\) parameterized by the set of parameters \(\). In this work, we propose learning the parameters \(\) to **maximize quantum entropy** of the _dataset embedding_, i.e.,

\[=_{2}(_{})- [(_{})^{2}]}\,. \]

As we will describe in brief, optimizing this target has clear benefits inherited from the underlying geometry of the setup. But, first, we show how to empirically compute \(_{2}(_{})\). Building upon previous work 4, we exploit the following property of the covariance estimator:

\[[(_{})^{2}]=[(K_{})^{2}] \]

where \(K_{}^{M M}\) is the _distribution kernel_ matrix, with \([K_{}]_{ij}=K_{}(_{P_{i}},_{P_{j}})\). This equation follows directly from the fact that \(_{}\) and \(K_{}\) share the same set of eigenvalues. Leveraging this relationship, we can define tractable unsupervised training loss, which we term _Maximum Distribution Kernel Entropy_ (MDKE), with respect to the parameters of the encoder \(f_{}\):

\[_{}()-_{2}(_{}) =[(K_{})^{2}]= _{i=1}^{M}_{i}^{2}(K_{})=\| K_{}\|_{F}^{2} \]

where the latter relies on the fact that the Frobenius norm \(\|A\|_{F}^{2}=_{i}_{i}^{2}(A)\), where \(_{i}(A)\) are eigenvalues \(A\).

The MDKE objective is differentiable w.r.t. \(\) for commonly used kernels, provided that the encoder \(f_{}\) is differentiable as well. While the entropy estimator \(_{2}(_{})\) is convex in the kernel matrix \(K_{}\), the objective as a whole is generally not convex in \(\). However, in practice, as we show in Sec. 5, mini-batch Stochastic Gradient Descent (SGD) proves to be an effective method for optimizing this objective. The effectiveness of this optimization process is significantly influenced by the parameters of the Gaussian kernels. We elaborate on the methodologies for kernel bandwidth selection in Appendix B.1.

The Frobenius norm formulation in the loss Eq. (14) significantly reduces computational complexity. However, as we have observed in some of our experiments, it can lead to the collapse of small eigenvalues of \(K_{}\), particularly near the optimal value of the objective. To address this challenge we introduced a regularized version of the loss \(_{}\) that incorporates optional regularization, based on the determinant \(K_{}\) inspired by the connection with Fekete points (see details in Appendix B.2).

### Geometrical Interpretation

The optimization objective is specifically designed to minimize the variance within each distribution (inner-distribution variance) while simultaneously maximizing the spread of distributions over the compact latent space \(=^{d-1}\). This shaping of the distributions embeddings in the latent space facilitates easier separation in downstream tasks. In this section we show that the geometry of the optimal (w.r.t. the MDKE loss) configuration of mean embeddings in the RKHS attains describe properties. For doing so we leverage the notion of _distributional variance_\(_{}\) (Definition 1 in Muandet et al. ).

**Definition 3.2**.: For a set of \(M\) probability distributions \(_{M}\), _distributional variance_\(_{}(_{M})\) of the mean embeddings in the RKHS \(\) is given by

\[_{}(_{M})[G ]-}_{i=1}^{M}_{j=1}^{M}G_{ij}, \]

where \(G\) is the \(M M\) Gram matrix of mean embeddings in \(\), i.e. \(G_{ij}=_{P_{i}},_{P_{j}}_{}\).

Here we show that the distributional variance \(_{}\) can be equally reformulated into two separate contributions:

\[_{}(_{M})_{i=1}^{M}\|_ {P_{i}}\|_{}^{2}-\|_{P}\|_{}^{2}\, \]

where \(\) denotes mixture distribution with elements of \(_{M}\) being uniformly weighted mixture components (see proof in the Appendix A.1).

The relevance of distributional variance for MDKE objective is established by the following result.

**Proposition 3.3**.: _For a set of \(M\) probability distributions \(_{M}\), the second-order Renyi entropy \(_{2}\) of the empirical covariance operator embedding \(_{}\) induced by the choice of Gaussian distribution kernel \(K_{}\) over points in the RKHS \(_{}\), - as defined in Eq. (8), - is upper bounded by the distributional variance \(_{_{}}(_{M})\), i.e.,_

\[_{2}(_{})_{ _{}}(_{M}) \]

_where \(\) is the bandwidth of the distribution kernel \(K_{}\)._

The proof of this proposition is provided in Appendix A.3. This result formalizes the fact that our objective increases distributional variance, pushing up the average squared norm of mean embeddingof input distributions while minimizing squared norm of the mean embedding of the mixture. We further explore the geometrical implications of such optimization by formalizing connection between the variance of the distribution and the squared norm of the its mean embedding in RKHS.

**Proposition 3.4**.: _Under Assumption 3.1, the maximum norm of kernel mean embedding is attained by Dirac distributions \(\{_{z}\}_{z}\)._

This result is trivial due to the fact that the set of mean embeddings is contained in the convex hull of \(\{k_{}(z,)\}_{z}\), and, under Assumption 3.1, \( z:\|k_{}(z,)\|_{_{}} ^{2}=k_{}(z,z)=1\).

**Proposition 3.5**.: _Under Assumption 3.1, uniform distribution \(()\) is a unique solution of_

\[*{arg\,min}_{P()}\|_{}(P)\|_{_{}}^{2}_{ }k_{}(z,z^{})\,dP(z)\,dP(z^{})}. \]

The key intuition here comes from the fact that minimization of the squared norm of mean embedding in RKHS could be seen as minimization of total interaction energy over the given surface where the potential is defined by the kernel \(k\). Thus Proposition 3.5 is a special case of Theorem 6.2.1 of Borodachov et al. . Similar setup was used w.r.t. Gaussian potential over the unit hypersphere in Proposition 1 from Wang and Isola . The reformulation in Eq. (16) together with Propositions 3.4 and 3.5 immediately suggests that the framework could be seen as an extension of the _Hyperspherical Uniformity Gap_ to infinite-dimensional spaces of probability distributions This extension maintains the goal of reducing variance among input distributions while maximizing the separability between their means. See Appendix C.2 for a broader explanation of the connection.

More generally, utilizing the fact that under Assumption 3.1, the _kernel metric_\(d_{k_{}}\) (see Eq. (3)) is a monotonically increasing function of Euclidean metric on the latent space, we establish a precise connection between the generalized variance  and the norm of the mean embedding. Further details can be found in Appendix A.4.

An attempt to directly optimize \(_{_{}}\) using SGD resulted in significantly weaker outcomes. While a thorough mathematical explanation necessitates further investigation, we contend that this issue aligns with the recurring challenge reported across various studies regarding direct optimization over Maximum Mean Discrepancy (MMD). We hypothesize that optimal solutions exhibit similar geometric configurations, while the entropy of the covariance operator providing a smoother objective. Nonetheless, \(_{_{}}\) retains its value as an insightful and intuitive measure for describing the geometric configuration of the learned system.

### An Illustrative Example

We use a simple example to illustrate the connection between geometrical configurations of embedded distributions and distribution kernel entropy \(_{2}(_{})\) (see Fig. 2). We sample a number of points from \(6\) different Gaussian distributions and project on a sphere \(^{2}\) varying their projected variance \(\). As \(\) decreases, the distributional variance of the overall distribution of Gaussians increases (Fig. 2a). For very small \(\) each distribution converges to a point (a Dirac distribution). This results in the entropy interpolating between lower and upper bounds, demonstrating how entropy behaves in response to changes in distribution variance. Fig. 2b showcases the behavior of two terms comprising distributional variance (Eq. (16)): the average kernel norm of the distributions alongside the kernel norm of the mixture. The increase in entropy and variance corresponds to a 'flattening' effect on the spectrum of the distribution kernel matrix. This example provides a simplified picture of how input distributions configurations influence kernel entropy.

### Limitations

**Runtime complexity.** The applicability of a data-dependent distribution kernel to solving discriminative tasks relies on the structure of the dataset being well-suited for distribution regression modeling. The model performs best when the number of input distributions is relatively small (e.g., thousands rather than millions), while the number of samples per distribution is large. It is crucial to note that the computational complexity of the proposed method, which is a common concern in practical applications, is most favorable for the tasks described. A detailed analysis of runtime complexity can be found in Appendix B.3.

**Broader impact.** We wish to emphasize that the distribution regression framework has emerged as a powerful tool for analysis and predictive modeling, especially in domains where traditional methods face challenges, including social science, economics, and medical studies. We urge researchers and practitioners applying distribution regression in these areas to give special consideration to issues such as bias, fairness, data quality, and interpretability, - aspects that are currently under-researched in the context of distributional regression, largely due to the relative novelty.

## 4 Related Work

### Distribution Regression

Distribution regression was introduced in Poczos et al. , while the seminal work of Szabo et al.  provides a comprehensive theoretical analysis of this regression technique. A natural approach to solving distribution regression problems involves using kernels between measures. Notable examples include the Fisher kernel , Bhattacharyya kernel , Probability product kernel , kernels based on nonparametric divergence estimates , and Sliced Wasserstein kernels [23; 37]. Muandet et al.  proposed leveraging the mean embedding of measures in RKHS, and Szabo et al.  provided theoretical guarantees for learning a ridge regressor from distribution embeddings in Hilbert space to the outputs. Distribution kernels have been successfully applied in various kernel-based methods, such as SVM , Ridge Regression , Bayesian Regression , and Gaussian Process Regression . They have also been adapted for different modalities like distribution to distribution regression , sequential data , and more. Some learning paradigms can be considered closely related to distributional classification settings, such as multiple instance learning, where group-level information (i.e., labels) is available during training [70; 26; 25]. For an in-depth exploration of the diverse methodologies employed in distributional regression settings, we invite readers to consult Appendix C.1.

### Matrix Information Theory

Quantum entropy, including Renyi entropy, is a powerful metric to describe information in a unique way (see Muller-Lennert et al.  for foundational insights). Giraldo et al.  designed the measure of entropy using operators in RKHS to mimic Renyi entropy's behavior, offering the advantage of direct estimation from data. Bach  applied von Neumann entropy of the density matrix to the covariance operator embedding of probability distributions, thereby defining an information-theoretic framework utilizing kernel methods. In machine learning, especially within self-supervised learning (SSL) setups, entropy concepts have recently found novel applications. Our study builds on most recent developments [49; 21; 55] by applying quantum Renyi entropy to the covariance operator in RKHS.

## 5 Experiments

We here demonstrate that our proposed method successfully performs unsupervised learning of data-dependent distribution kernel across different modalities. The experimental setup is divided into two phases: unsupervised pre-training and downstream regression classification using the learned kernel.

For each dataset, we select a hold-out validation subset with balanced classes, while the remainder of the dataset is utilized for unsupervised pre-training. We use mini-batch ADAM  with a static learning rate of \(0.0005\). We report mini-batch based (instead of epoch based) training dynamics as our tasks do not require cycling over the entire dataset to converge to the optimal loss value. All experiments use Gaussian kernel both as an _embedding kernel_ and _distribution kernel_, the hyperparameter selection is performed as described in Appendix B.1.

Once the samples encoder \(f_{}\) is learned, we employ it to compute distribution kernel Gram matrix, used as an input to the Support Vector Machine (SVM) for solving downstream classification tasks. A grid search with \(5\) splits (70/30) is conducted to optimize the strength of the squared \(l_{2}\) regularization penalty \(C\), exploring \(50\) values over the log-spaced range \(\{10^{-7},,10^{5}\}\). The best estimator is then applied to evaluate classification accuracy on the validation subset, which we report.

Additional experiments exploring the application of data-dependent distribution kernels in domains where distribution regression models are less common, such as image and text, are presented in Appendix D.

### Flow Cytometry

Flow Cytometry (FC) is a widely used technique for measuring chemical characteristics of mixed cell population. Because population-level properties are described through (randomized) sampling of cells, FC is used as a canonical setup of distribution regression. For this study we used a dataset  where more than \(100.000\) cells are measured per each patient (subject). For each cell a total of ten parameters are reported, hence, we treated each subject as an empirical distribution over \(^{10}\). We considered downstream classification tasks on two different sets of labels. The first ('Tissue' classification) contains peripheral blood (pB) and bone marrow (BM) samples from \(N=44\) subjects. The second ('Leukemia' classification) presents healthy and leukemia BM cell samples, \(N=50\). Classes were balanced in both cases. We sampled \(16\) subjects for Tissue classification and \(20\) subjects for Leukemia for training. Unsupervised learning was performed over the entire dataset. The encoder \(f_{}\) was parametrized by a 2-layers neural network (NN) with ReLU nonlinearity and \(l_{2}\) normalized output (on the unit hypersphere \(^{9}\)). Per each subject we sampled a small percentage of cells, and we report performance for the sample size of \(200\). We repeated each training and testing phase for \(100\) times to track the variance induced by this aggressive subsampling.

To demonstrate the impact of unsupervised pre-training, we compared several methods across multiple configurations (Table 1):

a) **Kernels on distributions.** This group includes Fisher kernels applied to parametric Gaussian Mixture Model (GMM) estimates, as suggested in Krapac et al. , along with Sliced Wasserstein-1 and Sliced Wasserstein-2 kernels .

b) **MMD kernels.** Here, we employ a Gaussian embedding kernel for mean embeddings, marked as 'MMD' (Maximum Mean Discrepancy) in the table. This category includes various options for the distribution kernel, such as linear, Gaussian, Cauchy, and inverse multiquadrics.

c) **Distributional Variance.** An ablation study is conducted to demonstrate the results of directly optimizing the _distributional variance_ defined in Eq. (15).

d) **MDKE.** We explore various configurations of the encoder optimized with the MDKE objective. We report performance for randomly initialized encoder, and for unsupervised pre-trained encoder with and without regularization. Random initialization happens only once, and all subsequent accuracy measurements are taken using the same encoder.

The variance reported for each model is measured across multiple runs to demonstrate the effect of the sampling. Importantly, the optimization of the MDKE objective results in embedding kernels with significantly lower variance compared to non-data-specific kernels.

### Image and Text Modalities

In the following section, we present additional experiments on learning data-dependent distribution kernels in domains not typically considered distributional regression tasks, specifically the image and text domains. While we acknowledge the existence of more powerful domain-specific models and methods for both modalities, we provide these results to demonstrate the framework's applicability across a wide range of settings under appropriate choice of the representation model. Representing text as empirical samples from the finite space of tokens (i.e. words from the dictionary) is quite

   Model &  &  \\   & Acc. & Var. & Acc. & Var. \\  GMM-FV & \(93.07\%\) & \( 0.308\) & \(94.80\%\) & \( 0.186\) \\ SW1 & \(87.10\%\) & \( 0.530\) & \(95.07\%\) & \( 0.111\) \\ SW2 & \(81.71\%\) & \( 0.341\) & **95.30\%** & \( 0.224\) \\  MMD Linear & \(82.42\%\) & \( 0.840\) & \(90.57\%\) & \( 0.208\) \\ MMD Gaussian & \(81.71\%\) & \( 0.574\) & \(92.23\%\) & \( 0.216\) \\ MMD Cauchy & \(81.57\%\) & \( 0.662\) & \(93.77\%\) & \( 0.080\) \\ MMD lnq & \(82.89\%\) & \( 0.698\) & \(91.43\%\) & \( 0.217\) \\  Distr. Var. & \(79.47\%\) & \( 0.011\) & \(91.82\%\) & \( 0.007\) \\  MDKE Rand & \(77.50\%\) & \( 0.002\) & \(89.50\%\) & \( 0.003\) \\ MDKE no Reg. & \(95.30\%\) & \( 0.002\) & \(92.46\%\) & \( 0.002\) \\ MDKE Reg. & **98.89\%** & \( 0.010\) & \(94.57\%\) & \( 0.005\) \\   

Table 1: Distribution regression accuracy on Flow Cytometry datasets.

common, while the choice to model images as histograms over pixel positions is more subtle. We demonstrate that, in both scenarios, unsupervised pre-training of the encoder yields distribution kernel that achieves strong performance on downstream classification tasks, showcasing the versatility of the proposed learning framework in scenarios where distribution regression formulations are uncommon.

Images.MNIST  and Fashion-MNIST  consist of \(28 28\) pixel grayscale images divided into 10 classes. We considered each individual image to be a probability distribution (via rescaling pixel intensities so that \(l_{1}()=1\)) over the discrete space of pixel positions (i.e., histogram). Given this support space, the encoder \(f_{}\) is a discrete map which we implemented as a table lookup (i.e., embeddings) from pixel indices to the points on the hypersphere \(^{31}\). Embeddings were initialized by sampling points uniformly. Gradients of the MDKE objective with respect to the embeddings parameters were computed via automatic differentiation using projected gradient steps to ensure that the embeddings remain on the hypersphere. The small size of the support space enables the _exact_ computation of the inner product between kernel mean embeddings of input distributions \(\) (7) and, subsequently, the distribution kernel \(\) (8) during both training and evaluation. This ensured a lower variance of the accuracy for the downstream classification. Performing MNIST classification upon pre-training with our unsupervised encoder significantly improves the baseline (random initialization of latent embeddings) accuracy of \(85.0\%\) by reaching a plateau at \(92.15\%\). Refer to the detailed analysis of the latent spaces of the trained encoders in Appendix D.1.

Text.To assess our method's performance in a larger discrete support space, we utilized the "20 Newsgroups" , a multi-class text classification dataset. We reduced the size of the dataset to \(5\) classes (resulting in \(2,628\) sentences and \(38,969\) unique words) by subsampling both training and test subsets. We treated sentences as empirical distributions over words, assuming word sets to be enough for topic classification, despite no positional info. The encoder \(f_{}\) mirrored the setup used in the MNIST case (Appendix D.1), with \(l_{2}\) normalized word embeddings on \(^{31}\). However, while in the MNIST case embeddings computations were performed _exactly_, here considering the entire _embedding kernel_ Gram matrix is impractical due to its large size. Instead, we optimized embeddings by randomly sampling \(20\) words per sentence, making the inner product between embeddings a _stochastic approximation_. This setup is meant to confirm that the optimization of the proposed MDKE objective yields a solution that is robust w.r.t. the excessive risk induced by first-level subsampling. Detailed results can be found in Appendix D.2.

## 6 Conclusion

In this work, we presented an unsupervised way of learning data-dependent distribution kernel. While previous studies in distribution regression predominantly relied on hand-crafted kernels, our work, in contrast, demonstrates that entropy maximization can serve as a powerful guiding principle for learning adaptable, data-dependent kernel in the space of distributions. Our empirical findings show that this technique can not only serve as a pre-training step to enhance the performance of downstream distribution regression tasks, but also facilitate complex analyses of the input space. The interpretation of the learning dynamics induced by the proposed objective relies on a theoretical link between the quantum entropy of the dataset embedding and distributional variance. This theoretical link, which we have proven, enables us to approach the optimization from a geometrical perspective, providing crucial insights into the flexibility of the learned latent space encoding.

We hope that theoretically grounded way of learning data-dependent kernel for distribution regression tasks will become a strong alternative to the common practice of hand-picking kernels. More broadly, our results present a methodology for leveraging the distributional nature of input data along side the novel perspective on the encoding of complex input spaces. This highlights the potential to extend the application of more advanced learning methods, embracing the ever-increasing complexity of data by going beyond more conventional vector-based representations.

Acknowledgments.We thank the Allen Institute for Brain Science founder, Paul G. Allen, for his vision, encouragement, and support.