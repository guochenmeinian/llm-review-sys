# Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Benefiting from the impressive diffusion models, conditional generative models have exhibited exceptional capabilities in various generation tasks, for example, image or short video generation based on text description. In this work, we focus on the task of generating a series of coherent images based on a given storyline, denoted as _open-ended visual storytelling_. We make the following three contributions: (i) to fulfill the task of visual storytelling, we introduce two modules into a pre-trained stable diffusion model, and construct an auto-regressive image generator, termed as **StoryGen**, that enables to generate the current frame by conditioning on a text prompt and preceding frame; (ii) to train our proposed model, we collect paired image and text samples by sourcing from various online sources, such as videos, E-books, and establish a data processing pipeline for constructing a diverse dataset, named **StorySalon**, with a far larger vocabulary than existing animation-specific datasets; (iii) we adopt a three-stage curriculum training strategy, that enables style transfer, visual context conditioning, and human feedback alignment, respectively. Quantitative experiments and human evaluation have validated the superiority of our proposed model, in terms of image quality, style consistency, content consistency, and visual-language alignment. We will make the code, model, and dataset publicly available to the research community.

## 1 Introduction

_"Mirror, mirror, here I stand! Who is the fairest in the land?"_

-- _Grimms' Fairy Tales_

This paper explores an exciting yet challenging task of _visual storytelling_, with the goal of training a model that can effectively capture the relationship between visual elements in images and their corresponding language descriptions, to generate a sequence of images that tell a visual coherent story. The ultimate goal is to generate a sequence of images that tell a coherent story, provided in the form of natural language. The outcome of this task has significant potential for education, providing children with an engaging and interactive way to learn complex visual concepts, and develop imagination, creativity, emotional intelligence, and language skills, as evidenced by the research in psychology [4; 41].

In the recent literature, there has been significant progress in image generation, particularly with the guidance of text, such as stable diffusion , DALL-E  and Imagen . However, these models are not sufficient for visual storytelling for two reasons: (i) existing models generate images independently without considering the context of previous frames or the overall narrative, resulting in visual inconsistencies and a lack of coherence in the visual story; (ii) generating images by only conditioning on text can lead to ambiguities or require unnecessarily long descriptions, particularlywhen dealing with subtle differences, for example, distinguishing between animals from the same category but different breeds. To address the limitations, we introduce a novel auto-regressive architecture, termed as **StoryGen**, that builds upon pre-trained stable diffusion model, with two extra modules serving for style transfer and visual context conditioning. At inference time, StoryGen takes the preceding frames and text prompts as conditions to synthesize the current frame, _i.e._, iteratively creating visual sequences that are not only aligning to language description, but also coherent.

In practise, visual storytelling is faced with significant challenges due to the lack of high-quality image-text data. Most existing works are limited to train on a few specific animations, for example, StoryGAN , StoryDALL-E  and AR-LDM , resulting in a small and restricted vocabulary and characters. To overcome this limitation, we have created a dataset called **StorySalon**, that features a rich source of coherent images and stories, primarily comprising children's storybooks collected from three different sources: videos, E-books, and synthesized sample data from our StoryGen model with human verification. As a result, our dataset includes a diverse vocabulary with different characters, storylines, and artistic styles.

We follow a three-stage training procedure: _Firstly_, we insert a LoRA-like architecture into text conditioning module in stable diffusion model, _i.e._, on top of the image-to-text cross-attention, in order to adapt the pre-trained diffusion model on our collected dataset; _Secondly_, we introduce a visual context module that enables the generation process to condition on preceding image; _Lastly_, we finetune the model with data after human verification, _i.e._, leveraging the feedback to further align the model with human preference. As a result, the scale and diversity of our collected dataset enable the model to acquire the ability of open-vocabulary visual storytelling, by that we mean, our model can generate new image sequences that are not limited to pre-defined storylines, characters, or scenes. For example, we can prompt a large language model to create unique and engaging stories, then feed into StoryGen for generation, as shown in Figure 1.

To summarise, we make the following contributions in this paper: (i) we propose the task of _open-ended visual storytelling_, that involves generating engaging image sequence that is aligning to a given storyline, for example, written by a large language model; (ii) we develop a novel architecture based on stable diffusion, termed as StoryGen, which can generate image sequences in an auto-regressive manner, taking both preceding image and text prompt of current frame as condition; (iii) we initiate a data collection pipeline and collect a large-scale, diverse datasets of storybooks, from online videos, E-books and synthesized samples, including paired image-text samples of a diverse vocabulary with different characters, storylines, and artistic styles; (iv) we adopt a three-stage curriculum training strategy, that enables style transfer, visual context conditioning, and human feedback alignment, respectively. Experimentally, we conduct both quantitative comparison and human evaluation, showing that the outputs from ou proposed model are more preferred, in terms of image quality, style consistency, and image coherence.

## 2 Related Works

**Diffusion Models** learn to model a data distribution via iterative denoising and are trained with denoising score matching. Drawing from the principles of Langevin dynamics and physical diffusion processes, diffusion models have undergone refinement through a multitude of works [39; 27; 45]. Notably, DDPM  has demonstrated improved performance over other generative models, while DDIM  has significantly boosted generation efficiency. In view of their superior generative

Figure 1: An illustration of open-ended visual storytelling. In practise, one user can prompt a large language model, for example, ChatGPT, to generate a unique and engaging story, which is then fed into our proposed **StoryGen** model, to generate a sequence of images that are not only aligning to the given storyline, but also coherent. We recommend the reader to zoom in and read the story.

capabilities, diffusion models have found extensive utility in various downstream applications besides image generation, such as video generation [46; 5; 12; 9; 38], image manipulation [2; 24; 14; 7], grounded generation , 3D texturing , and image inpainting [47; 26; 19; 1].

**Text-to-image Generation** involves the creation of images from textual descriptions. The task has been tackled using various generative models, with Generative Adversarial Networks (GANs)  being the first widely-used model. Several GAN-based architectures such as StackGAN , StackGAN++ , and AttnGAN  have achieved notable success in this area. Additionally, pre-trained auto-regressive transformers  such as DALL-E  have demonstrated the ability to generate high-quality images in response to textual prompts. Recently, diffusion models have emerged as a popular approach to text-to-image generation. New images can be sampled under text guidance from the data distribution learned by diffusion models with iterative denoising process. DALL-E 2  leverages CLIP  features to achieve well-aligned text-image latent space, while Imagen  relies on large language models like T5  to encode text. Stable Diffusion (or Latent Diffusion) Model  performs diffusion process in latent space, and it can generate impressive images after pre-training on a large-scale text-image datasets.

**Story Synthesis** is first introduced as the task of story visualization (SV) by StoryGAN , which presents a GAN-based framework and the inaugural dataset named Pororo, derived from cartoons. Subsequently, some other works also follow the GAN-based framework, such as DUCO-StoryGAN  and VLC-StoryGAN . In the case of word-level SV , more emphasis is placed on the representation of text, whereas VP-CSV  employs VQ-VAE  and a transformer-based language model to conserve character appearance and enhance visual quality. StoryDALL-E  extends the story visualization task to story continuation with the initial image given and recommends using a pre-trained DALL-E generative model  to produce coherent images. AR-LDM  introduces an auto-regressive latent diffusion model that can generate highly realistic images, but with only a limited vocabulary. NUWA-XL  is a concurrent work that exploits hierarchical diffusion model to synthesize long videos, with the keyframes generated first, followed by frame interpolation.

Existing models are mostly developed on specific scenes, which limits their ability for generating image sequences for diverse stories. In this paper, we target for more ambitious applications, to develop an open-ended visual storytelling model, that can digest stories of arbitrary length and diverse topics, and synthesize a sequence of coherent images in terms of both style and semantic.

## 3 Method

To be self-contained, we first present a brief overview to diffusion model in Section 3.1; then, we detail our proposed model for storybook generation in Section 3.2, starting from problem formulation, then architecture details, and lastly on training details.

### Preliminaries on Diffusion Models

Diffusion models are a type of generative models that undergo a denoising process, converting input noise into meaningful data samples. Diffusion models comprise a forward diffusion process that incorporates Gaussian noise into an image sample \(_{0}\), accomplished via a Markovian process over \(T\) steps. If we denote the noisy image at step \(t\) as \(_{t}\), the transition function \(q(_{t}|_{t-1})\) connecting \(_{t-1}\) and \(_{t}\) can be expressed as follows:

\[q(_{t}|_{t-1})=(_{t};}_{t-1},_{t}) q(_{1:T}|_{0})= _{t=1}^{T}q(_{t}|_{t-1})\] (1)

where \(_{t}(0,1)\) is the variance schedule controlling the step size.

Using Gaussian distribution property and reparametrization, if we define \(_{t}=1-_{t}\) and \(_{t}=_{i=1}^{t}_{i}\), we can write equation 1 as follows:

\[q(_{t}|_{0})=(_{t};_{t}}_{0},(1-_{t}))\] (2)

Diffusion models also comprise a reverse diffusion process that learns to restore the initial image sample from noise. A UNet-based model  is utilized in the diffusion model to learn the reverse diffusion process \(p_{}\). The process \(p_{}\) can be expressed using the following equation.

\[p_{}(_{0:T})=p(_{T})_{t=1}^{T}p_{}(_{t-1}|_{t}) p_{}(_{t-1}|_{t})= (_{t-1};_{}(_{t},t),_{ }(_{t},t))\] (3)

where \(_{}\) is the predicted Gaussian distribution mean value.

As we compute the loss function by taking the mean absolute error of the noise term \(_{}\) into account, we can express the mean value \(_{}\) in terms of the noise term \(_{}\) as follows:

\[_{}(_{t},t)=}} _{t}-}{_{t}}}_{ }(_{t},t)\] (4)

Therefore, the objective can be written as:

\[_{t}=_{t[1,T],_{0},_{t}} _{t}-_{}(_{t},t)}^{2}\] (5)

### StoryGen Model

In this section, we start by defining the problem for visual storytelling, then we introduce the core components in our proposed architecture, namely, Style Transfer Module and Visual Context Module, lastly, we present details for training the model with a curriculum learning regime.

#### 3.2.1 Problem Formulation

In visual storytelling, our goal is to generate a sequence of coherent and consistent images that correspond to a given story in the form of natural language. To achieve this, we propose an auto-regressive image generation model, called **StoryGen**, that generates the current frame \(_{k}\) by conditioning on both current text description \(_{k}\) and the previous frame \(_{k-1}\), as illustrated in Figure 2. The model is formulated as follows:

\[\{}_{1},}_{2},,}_{L}\} =_{}(\{_{1},_{2},, _{L}\};),\] \[=_{}(}_{1}|_{1}; )_{}(}_{k}|}_{k-1}, _{k};)_{}(}_{L}|}_{L-1},_{L};)\]

Here, \(\{_{1},_{2},,_{L}\}\) represents the given story in text sentences, \(\{}_{1},}_{2},,}_{L}| }_{i}^{H W 3}\}\) denotes the generated storybook, \(H,W\) refer to the width and height, respectively. \(_{}()\) refers

Figure 2: **Architecture Overview**. The **left** figure illustrates the complete procedure of _visual storytelling_. Our StoryGen model utilizes contextual information from previous frame and the text description at current step, to generate an image. The **right** figure displays the structure of our proposed modules, (i) style transfer module that is inserted into the text-conditioning module, with a LoRA-like achitecture; (ii) visual context module that enables the model to also condition on the features from preceding image for generation.

to a semi-frozen stable diffusion model (SDM), with a small number of newly-added trainable parameters (\(\)). It takes randomly sampled gaussian noise, text description and preceding image as input, and generate coherent image sequence that align with the story's narrative. In the following sections, we present the architecture detail for one-step generation conditioned on text and image.

#### 3.2.2 Architecture Details

Generally speaking, our model is built upon the foundation of a pre-trained stable diffusion model (SDM), that has been pre-trained on large number of paired image-caption samples, to gradually transform the noisy latent into an image. To tackle the problem of open-ended storytelling, we introduce two computational modules, namely, Style Transfer Module, and Visual Context Module, that enables the model to condition on not only text descriptions, but also the preceding RGB image, as shown in Figure 2. Formally, we can express the generation procedure as:

\[_{k}=_{}(}_{k}|}_{k-1 },_{k})=_{}(,_{}(_{k}),_{}(}_{k-1}))\]

where \(\), \(_{}()\) and \(_{}()\) denote the noisy latent, encoded text description and preceding image.

**Style Transfer Module.** To steer a pre-trained stable diffusion model towards the style of children's storybooks, we propose to insert a lightweight, LoRA-like  architecture into the text conditioning module, effectively acting as style transfer. This can be expressed as:

\[Q=W^{Q}+ Q,\;K=W^{K}_{k}^{}+  K,\;V=W^{V}_{k}^{}+ V,\; \;\;_{k}^{}=_{}(_{k})\]

\(W^{Q},W^{K}\) and \(W^{V}\) denote the projection matrices, adopted from the text conditioning module in pre-trained stable diffusion model. \(_{}()\) and \(_{k}^{}\) refer to the pre-trained CLIP text encoder and extracted text embedding respectively. \( Q, K\) and \( V\) are calculated by a learnable projection of \(,_{k}^{}\) and \(_{k}^{}\), respectively, resembling LoRA operations.

**Visual Context Module.** In order to generate visually coherent images, we insert a visual context module after the text conditioning, specifically, it is a transformer decoder comprising a self-attention layer, a cross-attention layer, and a feed-forward network, where the cross-attention layer employs a casual attention mechanism by using the noisy latent as query and the visual features of the previous frame as key and value, which can be formally denoted as:

\[Q=W^{Q},\;\;\;K=W^{K}_{k}^{},\;\;\;V= W^{V}_{k}^{},\;\;\;\;\;_{k}^{ }=_{}(_{k-1})\]

\(W^{Q},W^{K}\) and \(W^{V}\) refer to three learnable projection matrices, \(_{}()\) denotes the visual feature extracted by a pre-trained CLIP visual encoder. It is worth noting that the visual context module can also extend to multiple condition frames by concatenating their CLIP features as visual contexts.

**Training Objective.** At training stage, we randomly sample a triplet each time, _i.e._, \(\{_{k},_{k-1},_{k}\}\), and the objective defined in Equation 5 can now be transformed into:

\[_{t}=_{t[1,T],_{0},_{t}, _{k}^{},_{k}^{}}\|_{t}-_{}(_{t},t,_{k}^{},_{k}^{})\|^{2}\] (6)

and as we adopt classifier-free guidance  in inference, the predicted noise can be expressed as:

\[}_{}(_{t},t,_{k}^{}, _{k}^{})=(w+1)_{}(_{t},t, _{k}^{},_{k}^{})-w_{ }(_{t},t)\] (7)

where \(w\) is the guidance scale.

#### 3.2.3 Curriculum Learning

In this section, we describe the three-stage training strategy, that includes single-frame pre-training, multiple-frame fine-tuning, and alignment with human feedback. This curriculum learning approach enables the model to gradually learn from simple to complex tasks, ultimately improving its ability to generate high-quality images that align with the given story narratives. Details for our proposed curriculum learning are presented below.

**Single-frame Pre-training.** We start by training the style transfer module, which has been inserted into the text conditioning module of a pre-trained stable diffusion model, in a single-frame manner. At this pre-training stage, we do not introduce the visual context module, and freeze all other parameters except for the LoRA-like plug-in. This training approach allows us to quickly adjust to the desired visual style and character appearance in storybooks, while also maintaining the generation ability of the pre-trained stable diffusion model.

**Multiple-frame Fine-tuning.** Here, we fine-tune the visual context module while freezing other parameters of the model. Till this point, this allows the generation procedure to utilize information from either the text description or the preceding frame. To avoid over-fitting to the text descriptions, we adopt a technique inspired by BERT training , randomly dropping some words in the text with certain probability. The entire visual context module is fine-tuned during this stage.

**Fine-tuning with Human Feedback.** After multiple-frame fine-tuning, the model has developed basic storybook generation capabilities, to avoid from generating, potentially scary, toxic or biased content, we also propose to align the model with human preference. Specifically, we prompt ChatGPT to generate approximately 200 stories and use our model to synthesize images. After manually filtering around 100 high-quality storybooks from this corpus, we add them to the training set for further fine-tuning. As future work, we aim to add more books into this human feedback step.

**Inference.** With the three-stage training regime, we can streamline the entire inference process into a unified generation framework. As shown in the Figure 1, at inference time, we can prompt the ChatGPT to generate engaging, yet educational storylines, and synthesize the first image using a single-frame approach with only style transfer module involved; the previously synthesized frames, along with story description at current step, are treated as condition to generate image sequence in an auto-regressive manner. Experimentally, our proposed StoryGen is shown to generate images that align with the storyline, as well as maintaining consistency with previously generated frames.

## 4 Dataset Preparation

For training visual storytelling model, we collect a dataset called **StorySalon**, that contains approximately 2K storybooks and more than 30K well-aligned text-image pairs. This dataset is comprised of storybooks with potentially aligned text and image pairs, sourced from three different sources: video data, E-book data, and additional data from human feedback.

### Image-text Data from Videos & E-books

Here, we elaborate the procedure of extracting paired image-text samples from YouTube videos and E-books (pdf and corresponding audios available).

**Visual Frame Extraction.** To begin with, we download a significant amount of videos and subtitles from YouTube, by querying keywords related to children story, for example, _storytime_. We then extract the keyframes from the videos, along with the corresponding subtitles and their timestamps. To remove duplicate frames, we extract ViT features for each frame using pre-trained DINO , for the image groups with high similarity score, we only keep one of them. Next, we use YOLOv7  to segment and remove person frames and headshots, as they often correspond to the story-teller and are unrelated to the content of the storybook. Finally, we manually screen out frames that are entirely white or black. Similarly, we also acquire a number of electronic storybooks from the Internet, and extract images from E-book, except for those with extraneous information, for example, the authorship page. We acquire the corresponding text description with Whisper  from the audio file. For E-books that do not have corresponding audio files, we use OCR algorithms, to directly recognize the text on each page.

**Visual-Language Alignment.** Here, for each image, we acquire two types of text description, namely, story-level description, and visual description. This is based on our observation that there actually exists semantic gap between story narrative and descriptive text, for example, the same image can be well described as _"The cat is isolated by others, sitting alone in front of a village."_ in story, or _"A black cat sits in front of a number of houses."_ as visual description, therefore, directly finetuning stable diffusion model with story narrative maybe detrimental to its pre-trained text-image alignment.

In practise, to get story-level paired image-text samples, we align the subtitles with visual frames by using Dynamic Time Warping (DTW) algorithm . To get visual descriptions, we use ChatCaptioner  to generate captions for each image, as shown in Figure 3. s At training time, this allows us to substitute the original story with more accurate and descriptive captions.

Visual Frame Post-processing.In practice, we discovered that books in the frames can potentially interfere with our image generation model by having story texts printed on them. To address this, we use an OCR detector to identify the text regions in the frames and an image inpainting model to fill in the text and headshot regions. This process results in more precise image-text pairs that can be fed into the diffusion model.

### Additional Data from Human Feedback

As outlined in Section 3.2.3, we use the model (trained after two stages) to generate a set of new storybooks, and incorporate human feedback into the fine-tuning process. Following a rigorous manual review, we carefully select the best pieces, and add them into the training dataset. This allows us to continually improve the quality of our model and ensure that it produces engaging, yet educational storybooks, that align with human's preference.

### Discussion

Given the diversity of our data sources and data types, the StorySalon dataset exhibits a significantly broader range of visual styles and character appearances over other animation-specific datasets. Moreover, our dataset surpasses others in terms of vocabulary coverage by a substantial margin. Notably, our texts seamlessly integrate narrative storylines and descriptive visual prompts, ensuring the preservation of text-image alignment while adapting to the art style of storybooks.

## 5 Experiment

In this section, we start by describing our experimental settings, then compare with other models from three different perspectives: style, quality and coherence with quantitative and subjective human evaluation. Additionally, we present the results of our ablation experiments to prove the effectiveness of our proposed training regime.

### Training Settings

Our model is based on publicly released stable diffusion checkpoints, with a learning rate of \(1 10^{-5}\) and a batch size of \(512\). We begin with a single-frame pre-training stage, which involves 10,000 iterations on 8 NVIDIA RTX3090. In the multiple-frame fine-tuning stage, we fine-tune the model for 40,000 iterations using a single condition image. To improve the robustness of the training procedure, we apply a \(10\% 30\%\) words dropout with a probability to the texts in the current frames. We also fine-tune the model with human feedback on the training set with 100 manually generated storybooks in addition for 5,000 iterations. During inference, we utilize DDIM sampling and classifier-free guidance with a weight of 6.0.

Figure 3: **Dataset Pipeline Overview**. The **left** figure provides an overview of the complete dataset collection pipeline. Unstructured metadata sourced from the Internet undergoes a series of steps including frame extraction, visual-language alignment and image inpainting, resulting in properly aligned image-text pairs. The **right** figure displays examples of video data, E-book data, and synthetic samples. The accompanying texts represent their corresponding textual content, respectively.

### Quantitative Results

To evaluate the quality of our generated image sequence, we adopt the widely-used Frechet Inception Distance (FID) score . However, as there is no standardized metric for evaluating the consistency of images, we include human evaluation for comparison.

**Frechet Inception Distance (FID).** We present a comparison of the FID scores between our model and other existing ones, including SDM and Prompt-SDM, which conditions on an additional cartoon-style-directed prompt _"A cartoon style image"_. Specifically, we calculate the FID scores between the distribution of the generated results from these models and the distribution of the our proposed **StorySalon** testset. As shown in Table 1, we evaluate the generated image sequences from 100 storylines, obtained by prompting ChatGPT. Our StoryGen model outperforms the original stable diffusion models (SDM) and Prompt-SDM by a large margin, demonstrating the effectiveness of our model in generating high-quality coherent images.

**Human Evaluation.** We conduct two types of human evaluation experiments to assess the quality of our generated storybooks. In the _first_ experiment, we randomly select an equal number of groundtruth storybooks, the results of our StoryGen and the generation results of SDM and Prompt SDM. Participants are then invited to rate these four categories of storybooks on a score ranging from 1 to 5, taking into account text-image alignment, style consistency, content consistency, and image quality, higher scores indicate better samples. In the _second_ experiment, we prompt ChatGPT to produce a number of storylines and use our StoryGen along with the two variations of stable diffusion to generate corresponding image sequences. Participants are asked to choose the preferred results of each storyline. To mitigate bias, participants are unaware of the type of storybooks they are evaluating during these two human evaluation experiments. In both experiments, we have invited approximately 30 participants in total.

Table 1 presents the results of our human evaluation. As can be seen, our model has shown significant performance improvement in its overall score compared to stable diffusion models, especially in terms of consistency and alignment, indicating that it can generate images that are highly consistent with the given text prompts and visual contexts, thus better exploiting the contextual information.

**Ablation Studies.** Our study compares the performance of our model with and without fine-tuning using human feedback. We evaluate the FID score of between our generated image sequence and the test set of our StorySalon dataset. The results demonstrate that fine-tuning with human feedback slightly improves performance, we conjecture that this could be attributed to the fact that the number of human-verified samples are limited, due to a result of resource limitation. In future work, we intend to address this quantity constrain by continually augmenting the dataset with new samples and closely monitoring the progressive advancements.

### Qualitative Results

In Figure 4, we present the visualization results, showing that our model can generate storybooks with a broad vocabulary, while maintaining coherence and consistency throughout the narrative. The generated images successfully maintain the consistency of the artistic style and character appearance, whereas the results from SDM and Prompt-SDM fail to do so. Moreover, style of the generated results from SDM's are also incongruent with the requirements of visual storytelling for children.

## 6 Conclusion

In this paper, we consider the exciting yet challenging task known as _open-ended visual storytelling_, which involves generating a sequence of consistent images that tell a coherent visual story basedon the given storyline. Our proposed **StoryGen** architecture can take input from the preceding frame along with the text prompt to generate the current frame in an auto-regressive manner. A three-stage curriculum training strategy has been introduced for effective training and alignment with human preference. Due to the limitations of dataset in previous works, we have also collected a large-scale, diverse dataset named **StorySalon** that includes paired image-text samples sourced from storybook data from videos, e-books and synthesized samples. The StorySalon dataset possesses a diverse vocabulary of storyline, character appearances and artistic styles. While comparing with stable diffusion models with quantitative experiment and human evaluation, our proposed model substantially outperform existing models, from the perspective of image quality, style consistency, content consistency, and image-language alignment.

Figure 4: **Qualitative Comparison with other baselines. The images in green, orange and blue boxes are generated SDM, Prompt-SDM and StoryGen respectively. Our results have superior style and content consistency, text-image alignment, and image quality.**