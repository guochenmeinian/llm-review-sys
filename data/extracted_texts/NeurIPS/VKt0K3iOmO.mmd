# Spiking Graph Neural Network on Riemannian Manifolds

Li Sun

North China Electric Power University

Beijing 102206, China

ccesunli@ncepu.edu.cn

&Zhenhao Huang

North China Electric Power University

Beijing 102206, China

huangzhenhao@ncepu.edu.cn

&Qiqi Wan

North China Electric Power University

Beijing 102206, China

wanqiqi@ncepu.edu.cn

&Hao Peng

Beihang University

Beijing 100191, China

penghao@buaa.edu.cn

&Philip S. Yu

University of Illinois at Chicago

IL, USA

psyu@uic.edu

Corresponding Author: Li Sun, ccesunli@ncepu.edu.cn

###### Abstract

Graph neural networks (GNNs) have become the dominant solution for learning on graphs, the typical non-Euclidean structures. Conventional GNNs, constructed with the Artificial Neuron Network (ANN), have achieved impressive performance at the cost of high computation and energy consumption. In parallel, spiking GNNs with brain-like spiking neurons are drawing increasing research attention owing to the energy efficiency. So far, existing spiking GNNs consider graphs in Euclidean space, ignoring the structural geometry, and suffer from the high latency issue due to Back-Propagation-Through-Time (BPTT) with the surrogate gradient. In light of the aforementioned issues, _we are devoted to exploring spiking GNN on Riemannian manifolds_, and present a Manifold-valued Spiking GNN (MSG). In particular, we design a new spiking neuron on geodesically complete manifolds with the diffeomorphism, so that BPTT regarding the spikes is replaced by the proposed differentiation via manifold. Theoretically, we show that MSG approximates a solver of the manifold ordinary differential equation. Extensive experiments on common graphs show the proposed MSG achieves superior performance to previous spiking GNNs and energy efficiency to conventional GNNs.

## 1 Introduction

Graphs are the ubiquitous, non-Euclidean structures that describe the relationship among objects. Graph neural networks (GNNs), constructed with the floating-point Artificial Neuron Network (ANN), have achieved state-of-the-art accuracy for learning on graphs [1; 2; 3; 4]. However, they raise the concerns about computation and energy consumption, particularly when dealing with real-world graphs of considerable scale [5; 6]. In contrast, Spiking Neuron Networks (SNNs), inspired by the biological mechanism of brains, utilize neurons that communicate using sparse and discrete spikes, showcasing their superiority in energy efficiency [7; 8]. Attempting to bring the best of both worlds, **spiking GNNs** are drawing increasing research attention.

In the literature of spiking GNNs, recent efforts have been made to design different architectures with spiking neurons, e.g., graph convolution , attention mechanism , variational autoencoder  and continuous GNN . While achieving encouraging results, existing spiking GNNs still face several fundamental issues: **(1) Representation Space**. Spiking GNNs consider the graph in Euclidean space, ignoring the inherent geometry of graph structures. Unlike the Euclidean structures (e.g., pixel matrix and grid structures), graphs cannot be embedded in Euclidean space with bounded distortion . Instead, _Riemannian manifolds_ have been shown as the promising spaces to model graphs in recent years [3; 13; 4] (e.g., hyperbolic space, a type of Riemannian manifolds, is well aligned with the graphs dominated by hierarchical structures). However, none of the existing works study SNN on Riemannian manifolds, to the best of our knowledge. It is thus an interesting and urgent problem to consider how to endow the spiking GNN with a Riemannian manifold. **(2) Training Algorithm.** Training spiking GNN is challenging, since the spikes are non-differentiable. Existing studies consider the spiking GNN as a recurrent neural network and apply Backward-Passing-Through-Time (BPTT) with the surrogate gradient [5; 9; 10; 11]. They recurrently compute the backward gradient at each time step, and thus suffer from the high latency issue [14; 15; 16; 6] especially when the spike trains are long.

Present work.Deviating from previous spiking GNNs in Euclidean space, in this paper, we open a new direction to explore spiking GNNs on Riemannian manifolds, and propose a novel Manifold-valued Spiking GNN (MSG) sketched in Fig. 1. It is not realistic to place spike trains in a manifold such as hyperbolic or hyperspherical space, given the fact that spike trains cannot align with the defining domain. Instead, we design a Manifold Spiking Layer that conducts parallel forwarding of spike trains and manifold representations. Specifically, we first incorporate the structural information into spike trains by graph convolution. Then, a new _manifold spiking neuron_ is proposed to emit spike trains and relate them to manifold representations with _diffeomorphism_, where the spike train generates a momentum that forwards manifold representation along the geodesic. Instead of applying BPTT in spike domain, the proposed neuron provides us with an alternative of _Differentiation via Manifold (DvM)_. (The red dashed line in Fig. 1.) Yet, differentiation in Riemannian manifold is nontrivial. We leverage the properties of pullback and derive the closed-form backward gradient (Theorem 4.1). _DvM_ enables the recurrence-free gradient backpropagation, which no longer needs to perform recurrent computation of time steps as in BPTT. Theoretically, MSG is essentially related to manifold Ordinary Differential Equation (ODE). Each layer creates a _chart_ of the manifold, and MSG approximates the dynamic chart solver  of manifold ODE (Theorem 5.2).

Contributions.Overall, the key contributions are summarized as follows: (1) To the best of our knowledge, we propose the first spiking neural network on Riemannian manifolds (MSG)2, and show its connection to manifold ODE theoretically. (2) We design a new training algorithm of differentiation via manifold, which avoids the high latency of BPTT methods. (3) Extensive experiments show the superior effectiveness and energy efficiency of the proposed MSG.

## 2 Related Work

We briefly overview the ANN-based GNNs (i.e., the conventional, floating-point GNNs living in either Euclidean space or Riemannian manifolds) and SNN-based GNNs (i.e., spiking GNNs).

ANN-based GNNs (Euclidean and Riemannian).The majority of GNNs are built with floating-point ANN, conducting message passing on the graphs [18; 2; 19]. The Euclidean space has been the

Figure 1: MSG conducts parallel forwarding and enables a new training algorithm alleviating the high latency issue.

workhorse for graph representation learning for decades, and the popular GCN , GAT  and SGC  are also designed in the Euclidean space. In recent years, Riemannian manifolds have emerged as an exciting alternative considering the geometry of graph structures [20; 21]. Among Riemannian manifolds, hyperbolic space is recognized for its alignment with the graphs of hierarchical structures, and a series of hyperbolic GNNs (e.g., HGNN , HGCN ) show superior performance to their Euclidean versions. Beyond hyperbolic space, hyperspherical space is well suited for cyclical structures , and recent studies further investigate the constant curvature spaces , product spaces [24; 25; 26; 27], quotient spaces , SPD manifolds [29; 30], etc. Riemannian manifolds achieve remarkable success in graph clustering [31; 32], structural learning , graph dynamics [34; 35; 36; 37], information diffusion  and graph generation [39; 40], but have rarely been touched yet in the SNN counterpart.

Spiking Neural Networks (SNNs) & Spiking GNNs.Mimicking the biological neural networks, SNNs [7; 8] utilize the spiking neuron to process spike trains, and offer the advantage of energy efficiency. Despite the wide application of SNN in computer vision [41; 42], SNNs are still at an early stage in the graph domain. The basic idea of spiking GNNs is adapting ANN-based GNNs to the SNN framework by substituting the activation functions with spiking neurons. Pioneering works study the graph convolution [43; 5], and efforts have also been made to the graph attention , variational graph autoencoder , graph differential equations , etc. SpikeGCL  is a recent endeavor to conduct graph contrastive learning with SNN. In parallel, spiking GNNs are extended to model the dynamic graphs [45; 46; 47]. We focus on the static graph in this work. In both dynamic and static cases, previous spiking GNNs are trained with the surrogate gradient, leading to high latency, and consider the graphs in the Euclidean space.

## 3 Preliminaries

Different from aforementioned spiking GNNs, we study the spiking GNN on Riemannian manifolds. Thus, we formally introduce the basic concepts of Riemannian geometry and SNN. Throughout this paper, the lowercase boldfaced \(\) and uppercase \(\) denote vector and matrix, respectively. Important notations are summarized in Appendix A.

Riemannian Geometry & Riemannian Manifold.Riemannian geometry provides elegant framework to study structures and manifolds. A Riemannian manifold is described as a smooth and real manifold \(\) endowed with a Riemannian metric. Each point \(\) in the manifold is associated with the _tangent space_\(T_{}\) that "looks Euclidean", and the Riemannian metric is given by the inner product in the tangent space, so that geometric properties (e.g, angle, length) can be defined. A _geodesic_ between two points on the manifold is the smooth path connecting them with the minimal length. There exist three types of isotropic manifold, namely, the _Constant Curvature Space_ (CCS): hyperbolic space \(\), hyperspherical space \(\) and the special case of Euclidean space with "flat" geometry \(\).

Graph & Riemannian Graph Representation Learning.A graph \(=(,,,)\) is defined on the node set \(\) and edge set \(\), and \(^{||||}\) is the adjacency matrix describing the structure information. Each node \(v_{i}\) is associated with a feature \(_{i}\), and node features are summarized in \(^{|| d}\). In this paper, we resolve the problem of _Riemannian Graph Representation Learning_ with SNN. Specifically, we seek a graph encoder \(_{}:v\) where \(\) is a point on the manifold, instead of Euclidean space, and \(_{}\) is defined with an energy-efficient SNN.

Spiking Neural Network.SNNs are constructed by _spiking neurons_ that communicate with each other by spike trains. Concretely, a spiking neuron is conceptualized as "a capacitor of the membrane potential", and processes the spike trains by the following \(3\) phases . First, the incoming current \(I[t]\) is accumulated in the capacitor, leading to the potential buildup (_integrate_). When the membrane potential \(V[t]\) reaches or exceeds a specific threshold \(V_{th}\), the neuron emits a spike (_fire_). After that, the membrane potential is reset to the resting potential \(V_{reset}\) (_reset_). There are two popular spiking neurons: IF model and LIF model . In particular, the three phases of IF model are formalized as

\[: V[t]=g(V[t-1],I[t])=V[t-1]+I[t]\] (1) \[: S[t]=H(V[t]-V_{th})\] (2) \[: V[t]=(1-S[t])V[t]+S[t]V_{rest},&},\\ V[t]-V_{th}S[t],&}.\] (3)where the incoming current \(I[t]\) is related to the input spike train, and \(V_{reset}\) is lower than \(V_{th}\). \(t\) denotes the time index of the spike. The Heaviside function \(H()\) is non-differentiable, \(H(x)=1\) if \(x 0\), and \(0\) otherwise. There are two options for reset, and fixed-reset is adopted in this paper. Overall, an IF model is given as \(S[t]=(I[t])\), and the only difference between IF model and LIF model lies in the definition of \(g()\) in Eq. (1). In this paper, we are interested in designing a new spiking neuron on Riemannian manifold.

## 4 Methodology: Manifold-valued Spiking GNN

In this section, we present a simple yet effective Manifold-valued Spiking GNN (MSG), which can be applied to any geodesically complete manifolds, e.g., the Constant Curvature Space (CCS), including hyperbolic space and hyperspherical space, or the product of CCS. In particular, we design a spiking neuron on Riemannian manifolds (named as _Manifold Spiking Neuron_) that allows for the _differentiation via manifold_. It provides a new perspective of training spiking GNN, so that we avoid the high latency of typical backward-passing-through-time (BPTT) training.

### Manifold Spiking Layer

We elaborate on the sole building block of the proposed model -- Manifold Spiking Layer. Note that, the spike train or spiking representation in existing spiking GNNs [43; 5; 9; 10; 6; 45; 46; 47] cannot align with the defining domain of Riemannian manifolds (e.g., hyperbolic space and hyperspherical space), thus posing a fundamental challenge. Our solution is to generate node representation on the manifold (referred to as manifold representation) in parallel, and leverage the notion of Diffeomorphism to create the alignment between the two domains. We formulate the _parallel forwarding_ of spike trains and manifold representations as follows.

Unified Formulation.The forward pass of the spiking layer consists of a graph convolution and one proposed manifold spiking neuron. Without loss of generality, for each node \(v_{i}\), the \(l-\)th spiking layer is formulated as follows,

\[_{i}^{l-1}[t] =(_{i}^{l-1}[t];^{l}),\] (4) \[[_{i}^{l},_{i}^{l}] =(_{i}^{(l-1)},_{i}^{(l-1)}),\] (5)

where \(\) is the incoming current to generate spike trains. \(\) and \(\) denote the spike trains and manifold representation, respectively. \(()\) is a GNN in the Euclidean space, and \(^{l}\) is the learnable parameter in the layer. Different from the neuron of previous spiking GNNs, we design _a novel manifold spiking neuron_ (\(\)) as shown in Fig. 2. It emits spike trains and relates them to manifold representations simultaneously, which is formulated as follows,

\[_{i}^{l} =(\{_{i}^{l-1}[t]\}_{t=1,,T})\] (6) \[_{i}^{l-1} =(_{i}^{l-1}[t])\] (7) \[_{i}^{l} =f(_{i}^{l-1},_{i}^{l-1})\] (8)

where \(t\) is the time step of spike trains. The IF model can be replaced by LIF model, and we utilize IF model by default for simplicity. \(\) is defined as the average pooling of the current \(\) over \(t\), and \(\) is given as the Euclidean vector. \(f\) denotes the diffeomorphism to the Riemannian manifold in which \(\) is the step size.

Figure 2: Manifold Spiking Layer. It conducts parallel forwarding of spike trains and manifold representations, and creates an alternative backward pass (red dashed line). The backward gradient with \(^{l-1}}{^{l}}\), \(D_{^{l-1}}^{l-1}\) and \(_{^{l}}\) will be introduced in Sec. 4.2.

Incorporating structural information.We inject the structural information when the received spikes transform into the incoming current of the neuron. A GNN is leveraged to define the current, conducting message-passing over the graph. Each node's representation is derived recursively by neighborhood aggregation [18; 2; 1]. Accordingly, \(\) in the proposed neuron is given as follows,

\[(_{i}^{l-1}[t];^{l})=(_{i}^{l-1}[t],(\{_{j}[t]:j_{i}\}; ^{l})),\] (9)

where the neighborhood \(_{i}\) is the set of immediate neighbors centering at node \(v_{i}\). The \(\) function aggregates the messages from neighborhood \(_{i}\), where we create the message of a node by \(^{l}_{j}[t]\). \(()\) denotes the combination of the center node's message and aggregated message. We utilize GCN  as the backbone to define \(\) and \(\).

Diffeomorphism between manifolds.In the proposed neuron, we bridge the spikes and manifold representation with the notion of _diffeomorphism_ in differential geometry. A diffeomorphism connects two smooth manifolds, saying \(\) and \(\). Formally, a map \(f:\) is a diffeomorphism between \(\) and \(\) if the smooth \(f\) is bijective and its inverse \(f^{-1}\) is also smooth.

Recall that the tangent space is locally Euclidean. We propose to place the Euclidean \(\), a representation of the spikes, in the tangent space \(T_{}\) of the point \(\). In MSG, we choose the _exponential map_ to act as the diffeomorphism between the tangent space and manifold. With a step size \(\), we have

\[f(_{i}^{(l-1)},_{i}^{(l-1)})=_{ _{i}^{(l-1)}}(_{i}^{(l-1)})\] (10)

Concretely, given \(\) and \( T_{}\), the exponential map3 of \(\) at point \(\), \(_{}():T_{} \), maps tangent vector \(\) onto the manifold \(\). The map pushes \(\) along the _geodesic_\(_{,}(t):\) starting at \(_{,}(0)=\) and ending at \(=_{,}(1)\). \(_{,}(t)\) denotes the velocity of \(_{,}(t)\), and the direction of geodesic at the beginning is given as \(_{,}(0)=\). That is, the tangent vector \(\), derived from the spikes, pushes the manifold representation along the geodesic via the exponential map. The advantage of our choice is that we are able to define the diffeomorphism in arbitrary geodesically complete manifold (detailed in Appendix D).

Note that, our idea is inherently different from the exponential/logarithmic based Riemannian GNNs [3; 4; 22], which leverage the tangent space of the origin for neighborhood aggregation. In contrast, we consider the successive process over the tangent spaces of manifold representations, which will be further studied in Sec. 5.

Model InitializationIn MSG, we need to simultaneously initialize the spiking input \(^{0}\) and manifold representation \(^{0}\), which is a collection of points on the given manifold. Given a graph \((,,,)\), the node features are first encoded by one graph convolution layer \(=(,;^{0})\), and we generate \(T\) copies of the node encodings \(\), where \(T\) is the number of time steps in spike trains. Then, we complete model initialization with the proposed manifold neuron \([^{0},^{0}]=(,)\), where the encoding \(\) is regarded as the incoming current that charges the neuron in each time step. \(\) consists of the original points of the manifold, e.g., in the sphere model of hyperspherical space, the original point is given as the south pole \(=[-1,0,...,0]^{}\) and \(=[^{},...,^{}]^{}\). Note that, the exponential map in the proposed neuron guarantees that \(^{0}\) lives in the manifold.

### Learning Approach: Differentiation via Manifold

Optimizing SNNs is challenging, as the Heaviside step function is non-differentiable. In the literature, existing spiking GNNs typically regard SNN as the recurrent neural network, and leverage backward-passing-through-time (BPTT) to train the model [50; 51; 52]. Concretely, given a real loss function \(\), the gradient backpropagation conducts **Differentiation via Spikes** (_DvS_) \(\) as follows,

\[_{^{l}}=_{t}[^{l}[t]}{ ^{l}}]^{*}_{^{l}[t]},\] (11)

where \(\) is the parameter, and \(t\) denotes the time step. The surrogate gradient  is required for \(D_{^{l}}^{l}[t]\), where Heaviside step function is replaced by a differentiable surrogate, e.g., sigmoid function. The differentiation via spikes presents high latency in the backward pass [14; 15; 16], as it needs to recur all the time steps in BPTT. We notice that, in the computer vision domain, the sub-gradient method  is proposed to address such issues in Euclidean space. However, it cannot be generalized to the Riemannian manifold since the linearity does not hold in Riemannian geometry.

In MSG, we decouple the forward pass and backward pass, and propose **Differentiation via Manifold** (_DvM_) to avoid the high latency in differentiation via spikes. The overall procedure of training MSG by the proposed learning approach is summarized in Algorithm 1. Thanks to the parallel forwarding of spikes and manifold representation, the proposed neuron provides us with an alternative of studying \(_{^{l}}\) through the forwarding pass on the manifold (i.e., differentiation via manifold). Nevertheless, _it is nontrivial and it requires to derive the pullback between different dual spaces._

**Pushforward, Pullback and Dual Space.** We first introduce the differentiation in Riemannian geometry which is essentially different from that in Euclidean space. In Riemannian geometry, a _pushforward_ refers to a derivative of a map connecting two manifolds \(\) and \(\). Concretely, given \(f:\) and a point \(\), the pushforward \(D_{}f\) maps a tangent vector \( T_{}\) to the tangent vector \(D_{}f() T_{f()}\). On the notation, for a manifold-valued function \(f()=\), \(/\) is equivalent to \(D_{}f\). For a scalar function \(f\), \(D_{}f\) is interchangeable with \(_{}f\).

In the proposed MSG, we consider a scalar loss function on the manifold \(:\). The pushforward \(D_{}\) at point \(\) maps tangent vector \( T_{}\) to a scalar value and, correspondingly, \(D_{}\) belongs to the _dual space_ of the tangent space \(T_{}^{*}\), which is a vector space consisting all linear functional \(F:T_{}\). As the tangent spaces at different points of the manifold are different, it requires a _pullback_ that maps the dual space \(T_{^{}}^{*}\) to the dual space \(T_{^{(l+1)}}^{*}\).

We derive the backward gradient with properties of differential \(1-\)form (Lemma B.1), communication (Lemma B.2), and pullback of a sum and a product (Lemma B.3) detailed in Appendix B.1.

**Theorem 4.1** (Backward Gradient).: _Let \(\) be the scalar-valued function, and \(^{l}\) is the output of \(l\)-th layer with parameter \(^{l}\), which is delivered by tangent vector \(^{l}\). Then, the gradient of function \(\) w.r.t \(^{l}\) is given as follows:_

\[_{^{l}}=[^{l-1}}{ ^{l}}]^{*}[D_{^{l-1}}^{l-1}]^{*}_{^{l} },_{^{l}}=[D_{^{}} ^{l}]^{*}_{^{l+1}},\] (12)

_where \(^{l-1}()=_{^{l-1}}()\), \(^{l}()=_{()}(^{l})\), and \([]^{*}\) means the matrix form of pullback._

The detailed proof is given in Appendix B.1, and we derive the two Jacobian matrices \(D_{^{l-1}}^{l-1}\) and \(D_{^{}}^{l}\) in Appendix C. There are three key advantages of the proposed _DvM_. First, every term in Equation (12) is **differentiable**, and thereby the surrogate gradient is no longer needed. Second, _DvM_ enables the **recurrence-free** backward pass alleviating the high latency training. We specify that both _DvM_ and the previous _DvS_ recurrently compute every time step in the forward pass, and the difference lies in the backward pass. In particular, we only conduct recurrence-free gradient backpropagation layer by layer, while the previous _DvS_ recurs every time step of each layer in BPTT. In addition to the differentiable and recurrence-free properties, _DvM_ does not suffer from gradient vanishing/explosion, and the empirical evidence is provided in Appendix F.

```
0: Graph \((,,,)\), Manifold \(\), Loss function over the manifold \(()\), Number of spiking layers \(L\), Original points \(\).
0: Parameters \(\{^{l}\}_{l=0,,L}\)
1:while not converging do
2:\(\)forward pass
3: Input current \(^{0}=(,;^{0})\);
4: Initialize \([^{0},^{0}]=(^{0},)\);
5:for each spiking layer \(l=1\) to \(L\)do
6:\(^{(l-1)}=(,^{(l-1)};^{l})\);
7:\([^{l},^{l}]=(^{(l-1)},^{(l-1)})\);
8:endfor\(\)backward pass
9: Compute \(_{^{l}}\) from \((^{L})\).
10:for layer \(l=L-1\) to 1 do
11: Compute \(D_{^{l}}^{l},D_{^{l-1}}^{l-1}\), \(^{l-1}}{^{l}}\).
12: Compute \(_{^{l}}\), \(_{^{l}}\) as Eq. 12.
13: Update \(^{l}\).
14:endfor
15:endwhile ```

**Algorithm 1** Training MSG by the proposed Differentiation via Manifold

## 5 Theory: MSG as Neural ODE Solver

Next, we demonstrate the theoretical aspects of our model that MSG approximates a solver of manifold Ordinary Differential Equations (ODEs).

\((t):[,+]^{n}\) is the solution of

\[(t)}{dt}=(D_{_{i}^{-1}((t))}_{i})u(_{i}^{ -1}((t)),t),\] (14)

then \((t)=_{i}((t))\) is a valid solution of Eq. (13) on \(t[,+]\).

**Definition 5.1** (Dynamic Chart Solver ).: _The manifold ODE in Eq. (13) with initial condition \((0)=\) can be solved with a finite collection of successive charts \(\{(U_{i},_{i})\}_{i=1,...,L}\). If \(_{i}\) is the numerical solver to Euclidean ODE corresponding to the \(i\)-th chart, \((t)=_{i}(t)\) on \([_{i},_{i}+_{i}]\), then \((t)\) in Eq. (13) is given as_

\[(_{L}^{-1}_{L}(_{L}_{L-1}^{-1})... (_{2}_{1}^{-1})_{1}_{1})(t).\] (15)

That is, a manifold ODE can be solved in Euclidean subspaces given by a series of successive charts.

In MSG, we consider the charts given by the logarithmic map as illustrated in 3, and we prove that MSG approximates a dynamic chart solver of manifold ODE (Theorem 5.2).

**Theorem 5.2** (Msg as Dynamic Chart Solver).: _If \((t):[,+]^{n}\) is the solution of_

\[(t)}{dt}=(D_{_{}((t))}\, _{})u(_{}((t)),t),\] (16)

_then \((t)=_{}((t))\) is a valid solution to the manifold ODE of Eq. (13) on \(t[,+]\), where \(=()\). If \((t)\) is given by the first-order approximation with the \(\) small enough,_

\[(+)=(D_{}\,_{ })u((),),\] (17)

_then the update process of Eqs. (4) and (8) in MSG is equivalent to Dynamic Chart Solver in Eq. (15)._

Proof.: The proof utilizes some facts of Riemannian manifolds and is detailed in Appendix B.2. 

In other words, in MSG, the transformation of manifold input and output is described as some manifold ODE, whose vector field is governed by a spiking-related neural network in the tangent bundle. To solve the manifold ODE, MSG leverages the Dynamic Chart Solver (Definition 5.1). Specifically, each manifold spiking layer corresponds to a chart, and thus the number of spiking layers equals to the number of charts. Each layer solves the ODE of a smooth path \((t):[,+]^{n}\) in the tangent space that centered at the manifold layer input. With the first-order approximation in Theorem 5.2, given a step size \(\), the endpoint \((+)\) of the path is parameterized by a GNN related to the spikes. Layer-by-layer forwarding solves the manifold ODE from the current chart to the successive chart. Consequently, the manifold output of MSG approximates the solution to the manifold ODE.

We notice that a recent work  connects spiking GNN to an ODE in Euclidean space. In contrast, the proposed MSG is essentially related to the manifold ODE.

The **Appendix** contains the proofs, the derivation of Jacobian, necessary facts on Riemannian geometry (i.e., Lorentz/Sphere model, stereographic projection and \(\)-stereographic model, and Cartesian product and product space), empirical details and additional results.

Figure 3: Charts given by the logarithmic map.

## 6 Experiments

We conduct extensive experiments with \(12\) strong baselines to evaluate the proposed MSG in terms of (1) the representation effectiveness, (2) the energy efficiency, and (3) the advantages of the proposed components. Additional results are presented in Appendix F.

### Experimental Setups

Datasets.Our experiments are conducted on \(4\) commonly used benchmark datasets including two popular co-purchase graphs: _Computers_ and _Photo_, and two co-author graphs: _CS_ and _Physics_. Datasets are detailed in Appendix E.

Baselines.We compare the proposed MSG with \(12\) strong baselines of three categories: (1) _ANN-based Euclidean GNNs_: the popular GCN , GAT , GraphSAGE  and SGC , (2) _ANN-based Riemannian GNNs_: HGCN  and HyboNet  of hyperbolic spaces, \(-\)GCN  of the constant curvature space, and the recent \(Q-\)GCN  of the quotient space, (3) _The previous Euclidean Spiking GNNs_: SpikeNet , SpikeGCN , SpikeGraphormer  (termed as SpikeGT for short) and the recent SpikeGCL . Note that, we focus on the graph representation learning on static graphs, and thereby graph models for the dynamic ones are out of the scope of this paper. SpikeNet  was originally designed for dynamic graphs, and we utilize its variant for static graphs according to . So far, spiking GNN has not yet been connected to Riemannian manifolds, and we are devoted to bridging this gap.

Evaluation Protocol.All models are evaluated by node classification and link prediction tasks. The evaluation metrics of node classification is classification accuracy; we employ the popular Area Under Curve (AUC) for link prediction. The hyperparameter setting is the same as the original papers. We perform \(10\) independent runs for each case, and report the mean with standard derivations. Experiments are conducted on the hardware of NVIDIA GeForce RTX 4090 GPU 24GB memory, and AMD EPYC 9654 CPU with 96-Core Processor. Our model is built upon GeoOpt , SpikingJelly  and PyTorch .

Model Instantiation & Configuration.Note that, the proposed MSG applies to any Constant Curvature Space (CCS) or the product of CCS. We instantiate MSG in the Lorentz model of hyperbolic space by default (whose Riemannian metric, exponential map, and the derived Jacobian is given in Appendix C), and study the impact of representation space in the Ablation Study. The dimension of the representation space is set as \(32\). The manifold spiking neuron is based on the IF model  by default, and it is ready to switch to the LIF model  whose results are given in Appendix F. The time steps \(T\) for neurons is set to \(5\) or \(15\). The step size \(\) in Eq. 8 is set to \(0.1\). The hyperparameters are tuned with grid search, in which the learning rate is \(\{0.01,0.003\}\) for node classification and \(\{0.003,0.001\}\) for link prediction, and the dropout rate is in \(\{0.1,0.3,0.5\}\). We provide the source code of MSG at the anonymous link https://github.com/ZhenhHuang/MSG.

   & &  &  &  &  \\  & & NC & LP & NC & LP & NC & LP & NC & LP & NC & LP \\   & GCN  & 83.55\(\)0.71 & 92.07\(\)0.40 & 86.01\(\)0.20 & 88.84\(\)0.39 & 91.68\(\)0.84 & 93.68\(\)0.84 & 95.03\(\)0.19 & 93.46\(\)0.39 \\  & GAT  & 86.82\(\)0.04 & 91.91\(\)0.88 & 86.68\(\)1.32 & 88.45\(\)0.07 & 91.74\(\)2.22 & 94.06\(\)0.70 & 95.11\(\)0.29 & 93.44\(\)0.70 \\  & SGC  & 82.71\(\)1.52 & 90.46\(\)0.80 & 87.91\(\)0.68 & 89.44\(\)0.40 & 92.09\(\)0.08 & **95.94\(\)0.83** & 94.77\(\)0.32 & 95.93\(\)0.70 \\  & SAGE  & 81.69\(\)0.86 & 90.56\(\)0.84 & 89.41\(\)1.28 & 89.86\(\)0.90 & **92.71\(\)0.73** & 95.22\(\)0.43 & 95.62\(\)0.86 & 95.75\(\)0.37 \\   & HGCN  & 88.71\(\)0.24 & 96.88\(\)0.53 & 98.18\(\)0.50 & 94.40\(\)0.20 & 97.21\(\)0.63 & 93.02\(\)0.26 & 94.66\(\)0.20 & 94.10\(\)0.64 \\  & \(\)-GCN  & 89.20\(\)0.50 & 95.30\(\)0.30 & 92.22\(\)0.62 & 94.89\(\)0.95 & 91.98\(\)0.15 & 94.86\(\)0.15 & 95.85\(\)0.20 & 94.58\(\)0.22 \\  & Q-GCN  & 85.94\(\)0.93 & **96.98\(\)**0.88 & 92.50\(\)0.95 & 94.77\(\)0.40 & 91.18\(\)0.82 & 93.99\(\)0.20 & 94.84\(\)0.25 & OOM \\  & HyboNet  & 86.29\(\)0.20 & 96.80\(\)0.80 & 92.67\(\)0.90 & **97.79\(\)0.47** & 92.34\(\)0.50 & 95.65\(\)0.25 & 95.56\(\)0.18 & **98.64\(\)0.49** \\   & SpikeNet  & 88.00\(\)0.70 & - & 92.90\(\)0.10 & - & 92.15\(\)0.18 & - & 92.66\(\)0.30 & - \\  & SpikeGCN  & 86.90\(\)0.30 & 91.12\(\)1.79 & 92.60\(\)0.70 & 93.84\(\)0.03 & 90.86\(\)0.11 & 95.07\(\)1.22 & 94.53\(\)0.18 & 92.88\(\)0.80 \\  & SpikeGCL  & 89.04\(\)0.48 & 92.72\(\)0.03 & 92.50\(\)0.17 & 95.58\(\)0.11 & 91.77\(\)0.11 & 95.13\(\)0.24 & 95.21\(\)0.10 & 94.15\(\)0.29 \\  & SpikeGT  & 81.00\(\)0.16 & 90.66\(\)0.38 & - & 91.86\(\)0.61 & 94

### Results & Discussion

Effectiveness.We evaluate the effectiveness of MSG in both node classification and link prediction tasks. Specifically, for node classification, we cannot directly feed the manifold representations of Riemannian baselines to a softmax layer with Euclidean measure. We bridge the manifold representation and Euclidean softmax with the logarithmic map of respective manifold. For link prediction, we utilize the generalized sigmoid for all the baselines, i.e., the Fermi-Dirac decoder  in which the distance function is defined under the respective geometry. In the proposed MSG, the model inference does not need the expensive successive exponential maps, and only limited float-point operations (i.e., addition) are involved. Accordingly, we leverage the tangent vectors for the downstream tasks. The performance of both learning tasks on Computer, Photo, CS and Physics datasets are collected in Table 1. Note that, SpikeNet and SpikeGT cannot do link prediction, since they are designed for node classification and do not offer spiking representation. _The proposed MSG consistently achieves the best results among SNN-based models_. In addition, MSG generally outperforms the best ANN-based baselines in node classification, and has competitive results to the recent ANN-based Riemannian baselines in link prediction.

Ablation Study.Here, we examine the impact of representation space and the effectiveness of the proposed _Differentiation via Manifold (DvM)_. For the former goal, we instantiate \(6\) geometric variants of MSG in hyperbolic space \(^{32}\), hyperspherical space \(^{32}\), Euclidean space \(^{32}\) and the products of \(^{16}^{16}\), \(^{16}^{16}\) and \(^{16}^{16}\). The superscript denotes the dimension of representation space, and we leverage _DvM_ for optimization. Manifold variants generally achieve superior results to the Euclidean one, thus verifying our motivation. On CS dataset, the performance of geometric variants is aligned with that of Euclidean and Riemannian baselines in Table 1. The proposed MSG is ready to switch among \(\), \(\), \(\), and their products, matching the geometry of graphs.

To examine the effectiveness of _DvM_, we design the optimization variant (named as Surrogate) for a given representation space. In the variant, we conduct differentiation via spikes and leverage BPTT for optimization, same as previous spiking GNNs. The training time of the optimization variants in different representation spaces are given in Fig. 4(a). Backward time of _DvM_ is significantly less than that of BPTT algorithm. The reason is that _DvM_ no longer needs recurrent gradient calculation of each time step (recurrence-free), while BPTT leads to high training time especially when the time step is large. In addition, we examine the backward gradient of _DvM_, and plot the gradient norm of each layer in Fig. 4(b). It demonstrates that _DvM_ does not suffer from gradient vanishing/explosion.

Energy Cost.We investigate the energy cost of the graph models in terms of theoretical energy consumption (mJ) [5; 6], whose formula is specified in Appendix E. We summarize the results for node classification in Table 3 in which the number of parameters at the running time is listed as a reference. It shows that SNN-based models generally enjoy less energy cost than ANN-based ones.

   & Computers & Photo & CS & Physics \\  \(^{32}\) & **89.27\(\)0.19** & **93.11\(\)0.11** & 92.65\(\)0.04 & **95.93\(\)0.07** \\ \(^{32}\) & 87.84\(\)0.77 & 92.03\(\)0.79 & 92.72\(\)0.06 & 95.85\(\)0.02 \\ \(^{32}\) & 88.94\(\)0.24 & 92.93\(\)0.21 & **92.82\(\)0.04** & 95.81\(\)0.04 \\  \(^{16}^{16}\) & 89.18\(\)0.25 & 92.06\(\)0.14 & 92.67\(\)0.10 & 95.90\(\)0.04 \\ \(^{16}^{16}\) & 88.00\(\)1.05 & 91.97\(\)0.08 & 92.33\(\)0.21 & 95.73\(\)0.11 \\ \(^{16}^{16}\) & 82.49\(\)1.18 & 92.31\(\)0.45 & 92.18\(\)0.21 & 95.81\(\)0.10 \\  

Table 2: Ablation study of geometric variants. Results of node classification in terms of ACC (%).

Figure 4: Backward time and gradient norm for node classification on Computer.

Note, MSG achieves the best energy efficiency among SNN-based models except Photo dataset. In addition, it has at least \(1/20\) energy cost to the Riemannian baselines.

Visualization & Discussion.We empirically study the connection between the proposed MSG and manifold ODE. In particular, we visualize a toy example of Zachary Karate Club dataset  on a \(^{1}^{1}\) in Fig. 5, where we plot each layer output on the manifold. The red curve is the path connecting the layer input and layer output, and the blue one is the direction of the geodesic. As shown in Fig. 5, the red and blue curves are coincided, that is, each layer solves an ODE describing the geodesic on the manifold.

## 7 Conclusion

In this paper, we study spiking GNN from a fundamentally different perspective of Riemannian geometry, and present a simple yet effective Manifold-valued Spiking GNN (MSG). Concretely, we design a manifold spiking neuron which leverages the diffeomorphism to bridge spiking representations and manifold representations. With the proposed neuron, we propose a new training algorithm with Differentiation via Manifold, which no longer needs to recur the backward gradient and thus alleviates the high latency of previous methods. An interesting theoretical result is that, MSG is essentially related to manifold ODE. Extensive empirical results on benchmark datasets demonstrate the superior effectiveness and energy efficiency of the proposed MSG.

## 8 Broader Impact and Limitations

Our work brings together two previously separate domains: spiking neural network and Riemannian geometry, and presents a novel Manifold-valued Spiking GNN for energy-efficiency graph learning, especially for the large graphs. Our work is mainly a theoretical exploration, and not tied to particular applications. A positive societal impact is the possibility of decreasing carbon emissions in training large models. None of negative societal impacts we feel must be specifically highlighted here.

Limitation.Our work as well as the previous spiking GNNs considers the undirected, homophilous graphs, while the spiking GNN on directed or heterophilous graphs still remains open. Also, readers may find it challenging to implement the proposed method. However, we provide downloadable code and will offer an easy-to-use interface.

   &  &  &  &  \\  &  & energy & \#(para.) & energy & \#(para.) & energy & \#(para.) & energy \\   & GCN  & 24.91 & 1.671 & 24.14 & 0.893 & 218.29 & 18.444 & 269.48 & 42.842 \\  & GAT  & 24.99 & 2.477 & 24.22 & 1.273 & 218.38 & 28.782 & 269.55 & 81.466 \\  & SGC  & **7.68** & 0.508 & **5.97** & 0.219 & **102.09** & 8.621 & **42.08** & 6.688 \\  & SAGE  & 49.77 & 1.671 & 48.23 & 0.893 & 436.53 & 18.444 & 538.92 & 42.842 \\   & HGCN  & 24.94 & 1.614 & 24.96 & 0.869 & 217.79 & 18.390 & 269.31 & 42.800 \\  & \(\)-GCN  & 25.89 & 1.647 & 25.12 & 0.889 & 218.24 & 18.440 & 269.44 & 42.836 \\  & \(\)â€“GCN  & 24.93 & 1.629 & 24.96 & 0.876 & 217.83 & 18.393 & 269.34 & 42.809 \\  & HyboNet  & 27.06 & 1.625 & 26.29 & 0.875 & 219.94 & 18.399 & 271.47 & 42.825 \\   & SpikeNet  & 101.22 & 0.070 & 98.07 & **0.040** & 438.51 & 0.218 & 540.04 & 0.334 \\  & SpikingGCN  & 38.40 & 0.105 & 29.84 & 0.046 & 510.45 & 1.871 & 210.40 & 1.451 \\  & SpikeGCL  & 59.26 & 0.121 & 57.85 & 0.067 & 445.69 & 0.128 & 548.74 & 0.214 \\  & SpikeGT  & 77.07 & 1.090 & 74.46 & 0.584 & 365.28 & 6.985 & 355.77 & 12.524 \\   & MSG(Ours) & 26.95 & **0.047** & 25.68 & 0.043 & 226.15 & **0.026** & 143.72 & **0.029** \\    &  &  &  &  &  &  &  &  &  \\ 

Table 3: Energy cost. The number of parameters at the running time (KB) and theoretical energy consumption (mJ) on Computers, Photo, CS and Physics datasets. The best results are **boldfaced**, and the runner ups are underlined.