# ## On quantum backpropagation, information reuse, and cheating measurement collapse

## On quantum backpropagation, information reuse, and cheating measurement collapse

### Amira Abbas

Google Quantum AI, Venice, California 90291, USA

University of KwaZulu-Natal, South Africa

QuSoft, University of Amsterdam, Science Park 123, 1098 XG Amsterdam, The Netherlands

### Robbie King

Department of Computing and Mathematical Sciences, Caltech, Pasadena, CA 91125, USA

### Hsin-Yuan Huang

Department of Computing and Mathematical Sciences, Caltech, Pasadena, CA 91125, USA

### William J. Huggins

Google Quantum AI, Venice, California 90291, USA

### Ramis Movassagh

Google Quantum AI, Venice, California 90291, USA

### Dar Gilboa

Google Quantum AI, Venice, California 90291, USA

### Jarrod R. McClean

Google Quantum AI, Venice, California 90291, USA

jmcclean@google.com

### Abstract

The success of modern deep learning hinges on the ability to train neural networks at scale. Through clever reuse of intermediate information, backpropagation facilitates training through gradient computation at a total cost roughly proportional to running the function, rather than incurring an additional factor proportional to the number of parameters - which can now be in the trillions. Naively, one expects that quantum measurement collapse entirely rules out the reuse of quantum information as in backpropagation. But recent developments in shadow tomography, which assumes access to multiple copies of a quantum state, have challenged that notion. Here, we investigate whether parameterized quantum models can train as efficiently as classical neural networks. We show that achieving backpropagation scaling is impossible without access to multiple copies of a state. With this added ability, we introduce an algorithm with foundations in shadow tomography that matches backpropagation scaling in quantum resources while reducing classical auxiliary computational costs to open problems in shadow tomography. These results highlight the nuance of reusing quantum information for practical purposes and clarify the unique difficulties in training large quantum models, which could alter the course of quantum machine learning.

## 1 Introduction

Computing gradients through backpropagation is crucial to the success of modern deep neural networks. Rather than a naive manifestation of the chain rule to compute gradients, backpropagation leverages white-box knowledge of a computational graph, as well as intermediate values, to asymptotically improve run times (Goodfellow et al., 2016; Rumelhart et al., 1986; LeCun et al., 2015, 1989; Bengio et al., 2014). Remarkably, computing the gradient of, say, a neural network function with respect to all its parameters, can be done at a total cost roughly proportional to running the function, instead of incurring an additional factor proportional to the number of parameters. This relative scaling, owed to backpropagation, has facilitated the training of very deep networks, with parameter counts now in order of \(10^{10}\) - accompanied with unparalleled empirical success (Goodfellow et al., 2016; Szegedy et al., 2017; Iandola et al., 2016; Wu et al., 2019). When considering the number of function calls required to compute gradients, backpropagation in classical circuits remains exponentially more efficient with respect to the number of parameters, than the best known algorithms for determining gradients of parameterized quantum circuits - with or without the aid of a quantum computer (Gilven et al., 2019; Van Apeldoorn et al., 2020; Brandao et al., 2017; Jordan, 2005; Schuld and Killoran, 2022). Nevertheless, the allure of large-scale models inspires the need for efficient training of parameterized quantum models (Kandala et al., 2017; Farhi et al., 2014; Cerezo et al., 2021), which frequently arise in fields like quantum machine learning and quantum chemistry. But if backpropagation scaling cannot be matched, practically reaching overparameterized regimes may be impossible, even before accounting for additional challenges like barren plateaus (McClean et al., 2018; Wang et al., 2021; Cerezo et al., 2020). Since trainability influences the power and applicability of a model, this could radically shift the current trajectory of preferred quantum models.

In this work, we provide an operational definition of backpropagation, and subsequently determine its feasibility for parameterized quantum models. We investigate learning algorithms with and without quantum memory, where the former is able to store a product of multiple copies of a particular state, perform joint quantum operations followed by an entangled measurement. Whereas, a learning algorithm without quantum memory can only perform operations on each copy, implement a (conditional) measurement, and use the resulting classical data. Without access to multiple copies, we highlight that all known methods to compute gradients of simple variational models, do not achieve an overhead in line with backpropagation, unless one considers very special cases. Interestingly, closely related probabilistic classical analogues can exhibit backpropagation scaling, which points out that the barrier in the quantum setting is due to quantum phenomena. In an attempt to mimic classical backpropagation, which leverages information reuse to produce a favourable scaling, we lean on a similar concept in a quantum setting, namely _gentle measurements_(Aaronson, 2019; Aaronson and Rothblum, 2019; Badescu and O'Donnell, 2021). When combined with online learning (Aaronson

Figure 1: **Quantum backpropagation algorithm. Our proposal for quantum backpropagation consists of an online shadow tomography protocol, coupled with a threshold search procedure (Aaronson et al., 2018; Badescu and Oâ€™Donnell, 2021). The algorithm is executed in batches of size \(O((M))\), of which roughly \(n\) batches are needed, where \(\) is an \(n\) qubit quantum state. A classically constructed hypothesis state \(\) is also necessary for the algorithm. Crucially, quantum states _and_ the hypothesis state are rotated before each threshold check, to rotate through the layers of a quantum neural network \(F(),\;^{M}\) and reuse information for gradients. This enables a cost reduction from \(O(M^{2}M)\) to \(O(M(M))\) to compute the full gradient. For convenience, we suppress precision factors which scale as \(O(1/^{4})\) for this proposal.**et al., 2018), this technique has proven useful in problems like _shadow tomography_ as it aims to conserve quantum resources, but has yet to be explored in the context of backpropagation. In an information-theoretic sense, when access to multiple copies of a state is provided, a modification of existing shadow tomography routines enables backpropagation scaling if one restricts costs to the quantum overhead and ignores the classical cost incurred to implement known shadow tomography schemes. We present our proposed quantum backpropagation algorithm in Figure 1 which highlights the reduction in quantum resources due to the ability to exploit structure in a quantum neural network and reuse information through gentle measurement. Unfortunately, the true computational efficiency of our scheme remains an open question and we rule out a general strategy based on gentle measurement alone, by linking to computational models known to be more powerful than those contained in the complexity class of BQP (Aaronson et al., 2016). Despite failure to achieve backpropagation scaling in this general setting, the construction is suggestive of approximate or restricted models that may yield the desired scaling without violating complexity-theoretic bounds. This avenue remains rich for future work. We hope that these results illustrate the difficulty of replicating backpropagation scaling in parameterized quantum circuits and inspires the development of alternative quantum models that can train at scale.

## 2 Backpropagation scaling

A comprehensive overview of gradient evaluation, given by automatic differentiation on classical computers, can be found in Griewank and Walther (2008). The key advantage, however, can be summarized in one sentence: computational and memory resources employed to compute gradients of a function are bounded multiples of those used to compute the function. We use this bound to define the requirements for backpropagation scaling.

**Definition 1** (Backpropagation scaling).: Given a parameterized function \(F(),\ ^{M}\), let \(F^{}()\) be an estimate of the gradient vector accurate to within some constant \(\) in the infinity norm. The total computational cost incurred to obtain \(F^{}()\) with backpropagation is bounded such that

\[(F^{}()) c_{t}(F()), \]

and

\[(F^{}()) c_{m}(F()), \]

where \(c_{t},c_{m}=(M)\), and \(()\) and \(()\) capture the time and space complexity respectively, for either computing the function \(F\) or its gradient \(F^{}\).

As a further specification of backpropagation scaling in Definition 1, one can specify whether one achieves this scaling in quantum resources, classical resources, or all resources. While it is, of course, the goal to achieve this scaling in all resources, the distinction remains relevant due to the ability to leverage classical resources in order to improve the scaling in quantum resources, which we elaborate on in Section 5. In purely classical models, like neural networks, the overhead for both time and memory can be constant, and typically by a small factor. This efficiency has been instrumental for training very large models and is arguably the main contributor to the success of modern day machine learning. Given that variational quantum models, which utilize parameterized quantum circuits, are believed to be the most promising candidates to solve quantum machine learning tasks, we investigate their ability to reproduce this scaling.

## 3 Variational quantum models

Variational algorithms have become a go-to approach when looking to solve various optimization and machine learning problems on quantum devices (Cerezo et al., 2021). We present a slightly restricted model for ease of analysis which still covers a very broad range of practical scenarios. Notably, if backpropagation scaling cannot be achieved in this simplified setting, it is unlikely to succeed in a more sophisticated one.

**Definition 2** (Simple variational model).: Consider an initial quantum state \(\) and a quantum circuit with \(M\) parameterized operations \(U_{j}(_{j})=e^{-_{j}P_{j}}\), where each \(P_{j}\) is a Pauli operator acting on up to \(n\) qubits. We define a simple variational quantum model as the parameterized function

\[F()=[O()], \]where \(O\) is a Hermitian and unitary observable, and the quantum state \(()\) is expressed as \(()=U() U()^{}\). In the most general case, \(\) will be an unknown quantum state that we refer to as the quantum data setting, but we will also be interested in the simplified setting where \(()=\) and \(=U()^{ n}=_{j=1}^{M}U_{j}(_{j })^{ n}\).

In the simplified setting, the \(k^{}\) gradient component of \(F()\) can be expressed as

\[[F^{}()]_{_{k}}=2\;[(_{j=1}^ {M}e^{i_{j}P_{j}})O(_{m=k+1}^{M}e^{-i_{m}P_{m}} )(-iP_{k})(_{l=1}^{k}e^{-i_{l}P_{l}})]. \]

It becomes clear that computing all \(M\) components involves a large number of common operations. At face value, one might think it straightforward to exploit this overlap of operations to gain computational efficiency, as is done classically. The problem is that intermediate information in a quantum circuit is not easily retrievable without consequence, which is investigated in the following section.

## 4 Learning algorithms without quantum memory

Recall that a learning algorithm without quantum memory would perform operations and measurements on each individual copy of a quantum state. In this regime, which is prominent in current quantum machine learning settings, we have the following proposition.

**Proposition 3** (Backpropagation scaling is impossible for quantum data using single copies).: _Given the quantum data setting where one seeks to train a variational model using copies of the unknown state \(\) and the additional constraint of no quantum memory, then backpropagation scaling is not possible in the general case._

Proof.: Take the Pauli circuit model above and let us consider the case of all possible Pauli operators \(P_{j}\) on \(n\) qubits, such that \(M=O(4^{n})\). If we take the special case of quantum data and initializing all \(_{j}=0\), then the gradient with respect to each of the parameters is given by the expected value of all possible Pauli operators on \(n\) qubits on the unknown quantum state \(\), up to a small constant. If no quantum memory is available, that is, we only have the ability to perform measurements on single copies at a time, then by Chen et al. (2022, Corollary 5.9), the minimal number of copies of \(\) is lower bounded by \((2^{n}/^{2})\) in order to predict all Pauli operators to at most \(\)-error with probability \(2/3\). Hence, backpropagation scaling is not possible in general in the single copy case. 

Notably, Proposition 3 is based on an information-theoretic separation that does not generalize to the simplified case, \(=\!\!\), or even when \(\) is simply guaranteed to be a pure state generated by a polynomial sized circuit, which we detail in Appendix C. Hence, for the simplified case and polynomial complexity pure state cases, we must turn to computational arguments. Furthermore, if it were possible to find a polynomial time algorithm for the approach in Appendix C, then it would be possible to efficiently clone pseudo-random states, which is not believed to be possible (Ji et al., 2018), despite the fact that they are pure states generated by polynomial sized circuits (see Appendix C.2). The following remark aims to clarify the status of current methods for approaching this problem.

**Remark 4** (Current gradient methods fail to achieve backpropagation scaling).: Given a variational model \(F()\) defined in (3) with time complexity

\[(F())=(M/^{k}),\]

for some integer \(k\) and precision \(\), then all known schemes to estimate the gradient of \(F()\) to the same precision, do not, in general, achieve a time complexity in line with backpropagation scaling.

We briefly explain why known gradient methods fail, but defer details to Appendix A. A promising gradient algorithm put forth in Jordan (2005) requires only a single black-box query to a function to estimate its full gradient with a desired precision. But, as shown in Gilyen et al. (2019), when considering variational models, a different query model must be applied and the original single-query advantage becomes unattainable. The authors derive lower bounds requiring a quantum computational cost of \(O(M/^{2})\) and in a high precision regime, \(O(M/)\) is worst-caseoptimal (Huggins et al., 2021) when using a black-box simplified model of \(U()\). In other contexts, it is also sometimes argued that the simultaneous perturbation stochastic approximation (SPSA) algorithm is computationally efficient since it requires two function evaluations to estimate the gradient, irrespective of \(M\). This seemingly satisfies the scaling required, however, as \(M\) increases, the variance of the gradient estimate increases and, thus, to counteract this, either a smaller learning rate must be used - increasing the number of optimization steps - or more samples are needed to estimate the gradient with an appropriate accuracy at every step. We derive a sample complexity bound in Appendix A.2.3 which demonstrates SPSA's inability to exhibit backpropagation scaling. Thus far, other sampling schemes constructed to estimate the gradient of \(F()\), like the parameter-shift rule, perform destructive measurements that typically only retrieve a partial amount of information for one component of the gradient. As a result, reducing the infinity norm error in the gradient with reasonable probability, has a cost that scales like converging each component, i.e.

\[(F^{}())  M\ (F()) \] \[=(M^{2}/^{k}), \]

which unfortunately, does not achieve backpropagation scaling.

While this quadratic dependence on the number of parameters may not seem problematic, a linear dependence was the necessary catalyst in the age of modern deep learning, with overparameterized networks that perform exceedingly well on practical tasks. We illustrate the consequences of quadratic scaling in Appendix A.2, Figure 2, where one could wait up to a day to evaluate a single gradient estimate of a model with fewer than \(10\ 000\) parameters.

But perhaps neural networks are not a fair benchmark. One could dig deeper in automatic differentiation literature to investigate whether a direct classical analogue for these parameterized quantum circuits attains backpropagation scaling. Interestingly, a particular analogue can.

**Proposition 5** (Classical analogue achieves backpropagation scaling).: _Parameterized Markov chains, which are much closer classical analogues to variational models than neural networks, exhibit backpropagation scaling._

We detail the proof and scaling comparison in Appendix B by drawing an analogy between quantum and classical probabilistic states. Under some reasonable assumptions on the set of classical operations, the desired scaling is indeed possible in analogous classical parameterized stochastic processes. The formulation of this classical-quantum analogy allows us to probe the root cause of why backpropagation scaling is so difficult to obtain in the quantum variational setting. The origin of the challenge lies within quantum measurement collapse and the inability to read out intermediate states while continuing a computation, rather than the probabilistic formulation of the problem. In the classical setting, one is always promised to be in a computational basis state, making it possible to do perfect measurements non-destructively at intermediate steps. It remains an interesting open question to better understand the performance separation on practical tasks between quantum variational methods and this type of classical analogue, given the advantage in trainability of the latter.

Although Proposition 3 presents a strict lower bound ruling out backpropagation in the quantum data case with single copies, this leads one to wonder whether backpropagation scaling is possible when one has access to multiple copies. Moreover, destructive quantum measurements are the inhibitor of backpropagation scaling in single copies, so perhaps there is some middle ground where one could perform measurements that are only partially destructive on multiple copies. This idea has led to breakthroughs in the shadow tomography problem, which we examine next in the context of backpropagation.

## 5 Reusing multiple copies through gentle measurement

By allowing access to multiple copies of \(\), it is especially interesting to note that gentle measurements can facilitate backpropagation scaling in all resources, when considering the special case outlined in Proposition 3. We first define gentle measurement, followed by the special case construction.

**Definition 6** (Gentle measurement).: Fix a subset of quantum mixed states \(\). A measurement \(F\) is _\(\)-gentle on \(\)_ if for every state \(\), and every outcome \(y\) of \(F\), the post-selected state \(_{F=y}\) obeys

\[||_{F=y}-||,\]

where \(\). Hence, the smaller the \(\), the less damage incurred by \(\).

**Proposition 7** (A special case variational model achieves backpropagation scaling).: _Given a variational model \(F()=U() U()^{}\), where \(U()=_{j=1}^{M}e^{-i_{j}P_{j}}V\) for some fixed unitary \(V\), setting \(\) to zero and \(O=I\), the \(k^{th}\) gradient component may be written as_

\[F^{}(0)_{k}=2(V V^{ }(-i)P_{k})=2( V V^{}P_{k}). \]

_Then all \(M\) gradient components can be estimated to within a fixed precision \(\) using \(O((M)/^{4})\) function calls, and thus,_

\[(F^{}())=O((M))(F( )),\]

_which is in line with backpropagation scaling._

Proof.: With use of an ancilla qubit and a slightly modified circuit, the imaginary components of \(V V^{}P_{k}\), which represent gradient components, can be estimated with \(O( M/^{4})\) copies of \(\) for all \(k\) via application of V and two-copy Bell measurements of the resulting state that harness gentleness. Similarly, using \(O((M)/^{2})\) copies along with the magnitude information from the Bell measurements, one may estimate the sign of the gradient components using a majority vote scheme that also exploits gentleness. The total number of copies scales as \(O((M)/^{4})\), inducing a time complexity of \(O((M) M/^{4})\) with efficient classical overhead. The details of the implementation are discussed in Huang et al. (2021, Appendix E). 

This result indicates that there are at least some choices of circuits and generators for which backpropagation scaling can be achieved using gentle measurements. The exception naturally leads one to ask if this may be possible in more general cases with techniques like shadow tomography (Aaronson, 2019), however, with just a small perturbation away from this special case, the same technique no longer works, and the general computational efficiency remains unknown (Aaronson and Rothblum, 2019; Badescu and O'Donnell, 2021). In the subsequent section, we adapt shadow tomography results and exploit the sequential structure in variational models equipped with quantum data, to obtain backpropagation scaling in quantum resources, but leave open the question of classical computational efficiency. This represents substantial progress over current gradient methods for these models.

### A quantum-efficient protocol for backpropagation

Our main contribution in this more general quantum data setting with multi-copy access, is the establishment of a connection between gradient estimation and shadow tomography. This gives an exponential improvement to the sample complexity of the input state from \((M)\) to \(O((M))\) for computing gradients. It also gives a quadratic improvement in the number of quantum operations from \((M^{2})\) to \((M)\), analogous to classical backpropagation. The algorithm is depicted diagrammatically in Figure 1. Our proposal, however, houses a large caveat: it requires the classical storage and manipulation of a _hypothesis state_, which results in an exponential classical overhead, unless an approximation scheme can be effectively applied. It is argued in Aaronson (2019) that this cost is unavoidable in general, since removing it would imply that quantum advice can always be simulated by classical advice. Nevertheless, the exponential saving in sample complexity could be important in settings where the labelled quantum states coming from Nature are limited, and valuable. In Huang et al. (2022) for example, there were sources of quantum data that, when limited in quantity, could achieve a substantial data advantage over classical learners - even in the range of \(20\)-\(40\) qubits. In this size range, keeping the classical model in full detail would be completely feasible without ruining the potential for quantum advantage. Further, the linear scaling of quantum operations, even in the face of exponential classical overhead, could be beneficial if classical computation is extremely cheap when compared to quantum computation.

Our protocol will apply to an even more general model than Equation (3), which we term a quantum neural network.

**Definition 8** (Quantum neural network).: Let a quantum neural network be a variational quantum circuit on \(n+1\) qubits, numbered \(0,1,,n\). Qubits \(1,,n\) act as the data register, which will take as input an unknown quantum state \(\) to be classified. Qubit \(0\) acts as the output register, which is measured in the \(Z\)-basis and initialized to \(\). The variational circuit belongs to the following simple class

\[()=e^{i_{M}P_{M}}U_{M} e^{i_{1}P_{1} }U_{1},\]where \(\{P_{k}\}\) are fixed \((n+1)\)-qubit Pauli operators and \(\{U_{k}\}\) are fixed circuits. The output prediction on \(|\) is then given by a quantum neural network function defined as

\[_{}(|)= 0||^{ }()\,Z_{0}\,()|0| [-1,1]. \]

Note that running the circuit on \(|0|\) gives a coin flip \((+_{}(|))\) rather than \(_{}(|)\) itself. This allows us to estimate \(_{}(|)\) to \(\) precision with high probability by running the circuit \((^{-1})\) times, as usual. Furthermore, note the sequential nature of the function's gradients, highlighted in a similar sense in Equation (4). This leads us to the following theorem.

**Theorem 9** (Quantum-efficient backpropagation).: _Given an unknown \(n\) qubit input state \(|\), there exists an explicit algorithm which produces estimates \(b_{k}\) for all \(k=1,...,M\) such that \(|b_{k}-_{b_{k}}_{}(|)|\) using only_

\[m=O(M}{^{4}}),\]

_copies of \(|\). The required number of quantum operations for the proposed algorithm is \((mM)\), which is quasi-linear in \(M\). However, classical storage of a hypothesis state is used and incurs a classical cost of \(M 2^{(n)}\) when no effective approximation schemes are known._

The full details of the proof and the explicit quantum backpropagation algorithm are given in Appendix D. We first show that estimating the gradient component \(_{_{k}}_{}(|)\) reduces to estimating the expectation value of a certain traceless Hermitian unitary operator on \(|+|0|\). Shadow tomography results then imply that estimating all gradient components to precision \(\) is possible using only \(( M,n,^{-1})\) copies of \(|\). In order to fully specify the algorithm, we adapt an improved shadow tomography protocol from Badescu and O'Donnell (2021) that makes use of gentle measurements and is online. The key difference in our proposal, which enables us to achieve linear scaling in \(M\), is the reuse of quantum computation in a way reminiscent of backpropagation through observation that one can rotate through the layers of the quantum neural network sequentially and estimate the appropriate expectation value between each rotation step, as shown in Figure 1. Naive implementation of the shadow tomography protocol for gradients would yield \((M^{2})\) quantum operations, in line with most existing methods for quantum gradient estimation.

### Reduction from shadow tomography

We now show that a fully efficient algorithm for computing gradients would give rise to a fully efficient shadow tomography procedure for observables which can be efficiently implemented. This very general class of observables, however, is not known to have a computationally efficient shadow tomography protocol. Thus, this connection presents yet another obstacle to improving the exponential classical run time of our quantum backpropagation algorithm since removing the \((n)\) classical run time overhead in general, necessitates a breakthrough in shadow tomography.

**Definition 10** (Shadow tomography problem).: Let \(\) be a class of two-outcome measurements with outcomes in \(\{ 1\}\). Given an unknown \(n\)-qubit quantum state \(|\), and known measurements \(E_{1},,E_{M}\), output estimates \(b_{1},,b_{M}[-1,1]\) such that \(|b_{k}-|E_{k}||\; k\). In particular, do this via a measurement of \(|^{ m}\) where \(m\) is as small as possible.

**Definition 11** (Poly-time observables).: A _poly-time observable_ on \(n\) qubits is defined to be an observable of the form \(U^{}Z_{1}U\) where \(U\) is a poly-size circuit.

The shadow tomography problem is well-studied in quantum information theory. There are indeed special cases where this problem may produce a favourable scaling in \(M\) and \(n\), as outlined in Proposition 7. But, in general, it is not trivial to remove the exponential classical cost when it comes to shadow tomography.

**Theorem 12** (Shadow tomography reduction).: _Suppose there is an algorithm which can estimate the gradients \(_{_{k}}_{}(|)\), \(k=1,,M\), to precision \(\), with \(m\) copies of \(|\), and with runtime \(T\). Then, this gives an algorithm for shadow tomography of poly-time observables, to precision \(\), with \(m\) copies of \(|\) and runtime \(T\)._

Proof.: Consider an instance of shadow tomography on \(n\) qubits, with \(E_{1},,E_{M}\) given by \(E_{k}=U_{k}^{}Z_{1}U_{k}\), where \(\{U_{k}\}\) are poly-size circuits. Construct the quantum neural network with the following variational circuit

\[()|0| =e^{i_{M}Y_{0} Z_{1}}_{M} e^{i_{1}Y_{0 } Z_{1}}_{1}H_{0}|0|\] \[=e^{i_{M}Y_{0} Z_{1}}_{M} e^{i_{1}Y _{0} Z_{1}}_{1}|+|\]

where

\[_{1} =_{0} U_{1}\] \[_{k} =_{0} U_{k}U_{k-1}^{}\;, 1<k M\]

Then, by Equation (7), the gradients at \(=\) are

\[_{_{k}}_{}(|)|_{ =} =2 0||^{ }()Z_{0}_{_{k}}()|0|\] \[=2+||_{1}^{ }_{M}^{}Z_{0}_{M}_{k+1}(iY_{0}  Z_{1})_{k}_{1}|+|\] \[=2+||_{1}^{ }_{k}^{}(iZ_{0}(Y_{0} Z_{1}))_{k} _{1}|+|\] \[=2+||(_{0} U_{k}^{})(X _{0} Z_{1})(_{0} U_{k})|+|\] \[=2+||(X_{0} E_{k})|+|\] \[=2|E_{k}|\]

Thus computing the gradients allows us to solve the shadow tomography instance. 

Seeing as there is no known classically efficient procedure for shadow tomography with respect to poly-time observables, this reduction illustrates the difficulty of replicating true backpropagation scaling in general.

### A fully gentle gradient strategy

Shadow tomography makes use of multiple copies and a hypothesis state model, often stored classically, to require a minimal number of destructive measurements. It is useful to examine the limits of gentle measurement alone for gradient estimation in order to reduce the classical overhead. In particular, it would be ideal if it were possible to use a small number of copies (e.g. \((1/)\)) of a quantum state to achieve \(\)-gentleness in the general case through a simple, direct measurement scheme. While we do not explicitly construct a protocol here, this capability would naturally lead to a scheme for gradient estimation that achieves backpropagation scaling. This capability, however, would also allow us to violate known query lower bounds for the unstructured search problem. Thus, for our gradient purposes, it seems as if successful schemes must limit the number of potentially destructive accesses to a quantum state via the use of a classical model. We formalize the general failure of gentle measurement alone in the following theorem.

**Theorem 13** (Repeated gentle measurements).: _Assume it is possible to perform an arbitrary two-outcome measurement gently by using up to a polylogarithmic number of copies of the state. Specifically, assume that any measurement can be made \(\)-gentle by using \(O((1/))\) copies of the state. Such an ability leads to a violation of known query bounds given by Grover's search algorithm, and thus, cannot be possible in general._

Proof.: The proof is adapted from results in Aaronson et al. (2016). Consider the \(n\)-qubit Grover state after \(i\) iterations with an ancilla present to mark the state \(\),

\[((2i+1))+((2i+1))_{y\{0,1\}^{n}} {1}{y x}}, \]

where \(= 2^{-}\). For each of the \(M=2^{n}\) possible marked elements \(x\), one can define a two-element POVM of the form \(\{,\;\;I-\}\). One may ensure that the marked bit string is found with high probability by performing a measurement of each of these POVMs with respect to the Grover state \((2^{n})\) times, even in the case where the state is constructed with a single Grover oracle query. Performing this procedure using standard, destructive, measurements of the POVMs would require a fresh set of oracle queries with each round. However, using sufficiently gentle measurements removes this requirement. If the distance between the pre- and post-measurementstates is sufficiently small, one obtains results that are close to those that one would obtain from a fresh copy of the state. In order to guarantee that the marked bit string can be extracted with high probability, we demand that the state obtained after any number of measurement rounds be within \(}{3}\) in the trace distance of the state prior to any measurements1. To guarantee that the state be sufficiently unchanged by the end of the series of \((2^{2n})\) measurements, each measurement should therefore be \((2^{-3n})\)-gentle. By assumption, this is possible with \((2^{3n})\), or simply \((n)\), copies of the original state, each of which is prepared using the Grover oracle a set number of times. By performing the whole sequence of measurements gently, one can avoid biasing the result too much before the marked state is found. Hence, one can identify \(x\) with high probability, using \((n) 2^{n/2}\) Grover oracle calls, which is a violation of known lower bounds (Grover, 1996). In Appendix E.4, we discuss how sufficiently gentle measurements lead to a violation of known bounds when considering a different notion of time complexity that combines measurements and oracle queries. 

**Remark 14** (Shadow tomography does not violate known bounds).: After seeing this result, one might question how this relates to shadow tomography schemes that use gentle measurement plus classical computation. It is consistent when one considers that \(\)-gentleness considered in isolation must apply to both the number of distinct measurements one may perform and the precision to which one performs a particular repeated measurement. That is, from the point of view of gentle measurement alone, gentleness on different measurements and gentleness on repeated measurement to high precision \(\), are on the same footing and hence, must respect known bounds for information extraction. Indeed, all known shadow tomography schemes are consistent with a number of copies of the state scaling polynomially with \(1/\), despite scaling logarithmically in the number of distinct measurements, which prevents the above violation of known Grover query and time complexity bounds. This reflects an asymmetry between the number of distinct measurements and the precision of a single measurement present in all shadow tomography schemes and noted in the original work on the topic (Aaronson, 2019) that hypothesized that there are fewer independent observables within a quantum state than one might expect intuitively. The success of shadow tomography schemes, as distinct from simple gentle measurement, depends crucially on the existence of models that update quickly enough to limit the number of measurements made to the actual quantum states.

### Approximate schemes

The failure of a fully gentle approach points to the necessity of a classical model to enable backpropagation scaling. But, the key challenge in the general application of the proposed shadow tomography algorithm is the use of an explicit classical representation of the quantum state, which in general, scales exponentially with system size. While there have been a few special cases found that have fully efficient schemes, like with Pauli operators (Huang et al., 2021), the case of whether there exists an efficient computational scheme for poly-time observables remains open. However, an exact scheme may not be required in practice, especially when dealing with noisy data. This raises the possibility of using approximate classical representations of the state. For example, it is known that in cases where states exhibit low entanglement, they may be efficiently represented by matrix product or tensor network states (White, 1992; Perez-Garcia et al., 2006; Cramer et al., 2010; Evenbly and Vidal, 2015; Orus, 2019). Moreover, in the case of shadow tomography, one is not explicitly seeking an exact representation of the density matrix, but rather a proxy, capable of reproducing the desired observables with high probability. This relaxation of requirements may render an approximation scheme effective, even when the true state is challenging to represent with a particular ansatz. This area represents an interesting and potentially fruitful research direction that could dramatically increase the efficiency of training in quantum machine learning models, and we leave it open for future work.

## 6 Discussion

Special cases aside, the inadequacy of current gradient methods to provide backpropagation scaling in parameterized quantum models leaves room for many questions. One particular conclusive direction would be developing a concrete computational argument to rule out backpropagation scaling in the multi-copy setting, thereby confirming the true computational complexity of shadow tomography. Even though the proposed information-efficient scheme in this study fails to satisfy classical cost requirements of backpropagation, the possibility of a computationally efficient procedure remains open, especially for cases with known, structured observables. Similarly, failure in the general case of gentle measurements again suggests potential approximate or restricted models that may be more trainable. One may find an alternative architecture where gradient computation has favorable scaling, and even though the model is perhaps not as powerful or universal, it may still be useful in practice. Interestingly, closely related probabilistic classical analogues to variational models can exhibit backpropagation scaling. If the difficulty to achieve an efficient scaling is due to inherently quantum properties, perhaps backpropagation is not the correct method for optimization of quantum models, which seems to be a growing belief for classical models too, albeit for completely different reasons (Hinton, 2022). We hope that these results spark the development of either alternative quantum models that can train at scale or new methods for efficient optimization.