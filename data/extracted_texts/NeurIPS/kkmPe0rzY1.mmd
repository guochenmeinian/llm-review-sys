# Robust Conformal Prediction Using Privileged Information

Shai Feldman

Department of Computer Science

Technion, Israel

shai.feldman@cs.technion.ac.il

Yaniv Romano

Departments of Electrical and Computer Engineering and of Computer Science

Technion, Israel

yromano@cs.technion.ac.il

###### Abstract

We develop a method to generate prediction sets with a guaranteed coverage rate that is robust to corruptions in the training data, such as missing or noisy variables. Our approach builds on conformal prediction, a powerful framework to construct prediction sets that are valid under the i.i.d assumption. Importantly, naively applying conformal prediction does not provide reliable predictions in this setting, due to the distribution shift induced by the corruptions. To account for the distribution shift, we assume access to privileged information (PI). The PI is formulated as additional features that explain the distribution shift, however, they are only available during training and absent at test time. We approach this problem by introducing a novel generalization of weighted conformal prediction and support our method with theoretical coverage guarantees. Empirical experiments on both real and synthetic datasets indicate that our approach achieves a valid coverage rate and constructs more informative predictions compared to existing methods, which are not supported by theoretical guarantees.

## 1 Introduction

### Motivation

Uncertainty quantification plays a pivotal role in increasing the reliability of machine learning models. In this paper, we focus on situations where the training data is corrupted, e.g., due to missing variables or noisy labels. These corruptions are ubiquitous in high-stakes applications--such as diagnosing diseases, predicting financial outcomes, or personalizing treatment plans for patients--in which the data-collection process is complex, resource-intensive, or time-consuming .

One way to enhance the trustworthiness of data-driven predictions is to provide an uncertainty set containing the correct outcome at a user-specified coverage rate, e.g., 90%. Conformal prediction (CP)  is a general framework for constructing such reliable prediction sets, however, it assumes that the training and test data samples are drawn i.i.d from the same distribution. This assumption does not hold in the problem setting we consider in this work, in which the training data is a corrupted or a biased version of the ground truth. For instance, consider a medical application in which training data have missing or incorrect labels for some patients in a non-random pattern. Another example is a situation where we have missing feature values in the training data but, at test-time, we observe the full set of features. These examples illustrate common sources for a distribution shift betweenthe training and test data, which breaks the coverage guarantee of traditional CP techniques. In this work, we address this gap and propose a novel calibration technique, called _privileged conformal prediction_ (PCP), which constructs provably valid uncertainty sets despite being employed with corrupted samples. Technically, we achieve this by utilizing privileged information--additional data available only during training time--to account for the distribution shift induced by the corruptions.

### Problem setup

Suppose we are given \(n\) training samples \(\{(X_{i}(M_{i}),Y_{i}(M_{i}),Z_{i},M_{i})\}_{i=1}^{n}\), where \(X_{i}^{}=X_{i}(M_{i})\) is the observed covariates, \(Y_{i}^{}=Y_{i}(M_{i})\) is the observed response, \(Z_{i}\) is the privileged information (PI), and \(M_{i}\{0,1\}\) is the corruption indicator. Specifically, if \(M_{i}=1\) then either \(X_{i}^{}\) or \(Y_{i}^{}\) are corrupted, and if \(M_{i}=0\), then \(X_{i}^{}\) and \(Y_{i}^{}\) correctly reflect the ground truth. In our setup, we require that the privileged information \(Z_{i}\) explains the corruption occurences \(M_{i}\). Formally, we assume that the clean data variables are independent of the corruption indicator given the privileged information, i.e., \((X(0),Y(0))\!\!\! M Z\).

At inference time, we aim to provide reliable predictions for the clean test response \(Y^{}=Y_{n+1}(0)\) given the clean version of the features: \(X^{}=X_{n+1}(0)\). That is, even though the observed \(X^{},Y^{}\) might be corrupted, the test \(X^{},Y^{}\) are always uncorrupted. Crucially, at test-time, we do not have access to the test privileged information \(Z^{}=Z_{n+1}\), nor the clean test label \(Y^{}\). Moreover, we assume that the PI \(Z\) is always clean and correctly reflects the ground truth. We now emphasize the importance of this problem setup by providing several examples.

**Example 1** (Noisy response).: _Here, we refer to \(Y_{i}(1)\) as a noisy version of the ground truth response \(Y_{i}(0)\). For instance, \(Y_{i}^{}=Y_{i}(M_{i})\) could be a label obtained either by a non-expert annotator or an expert annotator, and \(Z_{i}\) could be information about the annotator, such as their level of expertise. In this case, \(M_{i}=0\) (\(M_{i}=1\)) indicates that the annotator chose the correct (wrong) label \(Y_{i}^{}=Y_{i}(0)\) (\(Y_{i}^{}=Y_{i}(1)\)). In contrast, the features are always uncorrupted: \(X_{i}(0)=X_{i}(1)\). Notice that the test \(Y^{}=Y_{n+1}(0)\) always refers to the clean response. In this setup, since the PI, \(Z_{i}\), is the information about the annotator, it is likely to explain the corruption appearances \(M_{i}\). That is, it is sensible to believe that the assumption \((X(0),Y(0))\!\!\! M Z\) is approximately satisfied._

**Example 2** (Missing features).: _Here, \(X_{i}(0)\) is the full clean feature vector, and \(X_{i}(1)\) is a partial version of it, i.e., \(X_{i,j}(1)=\)'\(}\)' for some entries \(j\). For example, consider an application where participants are requested to fill a user experience (UX) questionnaire, in which the goal is to predict user engagement. This trial consists of expert participants, who tend to fully answer the questionnaire, resulting in a full \(X_{i}(0)\), and non-experts, who tend to partially answer it, resulting in the incomplete \(X_{i}(1)\). The PI \(Z_{i}\) could be the level of expertise of the participant. Also, the response is always uncorrupted: \(Y_{i}(0)=Y_{i}(1)\). We remark that at test time, the full feature vector \(X^{}=X_{n+1}(0)\) is completely available. Since the PI \(Z_{i}\) is the information about the participant, it is likely to explain the missing indices \(M_{i}\). Therefore, for this choice of PI, we have a good reason to believe that the conditional independence requirement, \(X(0),Y(0)\!\!\! M Z\), is approximately satisfied._

**Example 3** (Missing response).: _Consider a medical setup where patients are being selected for a costly diagnosis, such as an MRI scan. Here, \(X_{i}(0)=X_{i}(1)\) is the more standard medical measurements of the \(i\)-th patient, such as age, gender, medical history, and disease-specific measurements. The PI \(Z_{i}\) is the information manually collected by the doctor to choose whether the patient should be examined by an MRI scan. This information is obtained through, e.g., a discussion of the doctor with the patient, or a physical examination, and could include, for instance, shortness of breath, swelling, blurred vision, etc. The response \(Y_{i}(0)\) is the disease diagnosis obtained by the MRI scan, and \(Y_{i}(1)=\)'\(}\)'. The missingness indicator \(M_{i}\) equals 0 if the doctor decides to conduct an MRI scan, and 1 otherwise. At test time, our goal is to assist the doctors in future decisions before examining the patients, and hence the test PI \(Z_{}\) is unavailable. This task is relevant in situations where the number of available doctors is insufficient to examine all patients. Here, \(Z_{i}\) explains the missingness \(M_{i}\), and \(M_{i}\) does not depend on \(X_{i}\) or \(Y_{i}\) given \(Z_{i}\)._

With the above use cases in mind, our goal is to construct an uncertainty set \(C(X^{})\) for the unknown clean test variable \(Y^{}=Y_{n+1}(0)\) given the clean features \(X^{}=X_{n+1}(0)\). Importantly, this uncertainty set should be statistically valid and satisfy the following coverage requirement:

\[(Y^{} C(X^{})) 1-,\] (1)

where \(1-(0,1)\) is a pre-specified coverage rate, e.g., 90%. This property is called _marginal coverage_, as the probability is taken over all samples \(\{(X_{i}(0),X_{i}(1),Y_{i}(0),Y_{i}(1),Z_{i},M_{i})\}_{i=1}^{n+1}\)which are assumed to be drawn exchangeably (e.g., i.i.d.) from \(P_{X(0),X(1),Y(0),Y(1),Z,M}\). The challenge in achieving (1) lies in the fact that there is a distribution shift between the training data \(\{(X_{i}(M_{i}),Y_{i}(M_{i}))\}_{i=1}^{n}\) and the test data \((X_{n+1}(0),Y_{n+1}(0))\). Indeed, naively calibrating the model with the corrupted data may produce invalid uncertainty estimates . Also, calibrating using only the clean data would result in biased predictions as the clean training samples are drawn from \(P_{X(0),Y(0)|M=0}\), while the test samples are drawn from \(P_{X(0),Y(0)}\).

To bypass this bias, we assume that the privileged information explains away the corruption appearances. Formally, we require that the corruption indicator is independent of the clean data conditional on the value of the privileged information, \((X(0),Y(0))\!\!\! M Z\). This assumption implies that our setting is a special case of covariate shift, with the covariates being the privileged information \(Z\). Since the test PI \(Z^{}=Z_{n+1}\) is unknown at test time, conformal methods that account for covariate shift, such as _weighted conformal prediction_ (WCP)  cannot be applied directly in the setup. In this paper, we re-formulate weighted conformal prediction and show how to construct uncertainty sets that satisfy the coverage requirement in (1) although \(Z^{}\) is unavailable.

### Our contribution

We introduce _privileged conformal prediction_ (PCP)--a novel calibration scheme that effectively handles corrupted data, and constructs provably valid uncertainty sets in the sense of (1). Our key assumption is that the corruption indicator does not depend on the observed clean data given the privileged information, namely, \((Y(0),X(0))\!\!\! M Z\). This assumption implies that the privileged information explains away the corruption appearances. Building on WCP, we offer a specialized calibration scheme that carefully utilizes only the observed training privileged data \(\{Z_{i}\}_{i=1}^{n}\) to attain a valid predictive inference at test time. To enhance statistical efficiency, we further adapt PCP for scarce data, building on leave-one-out arguments [9; 10]. Importantly, all methods we offer are supported by a theoretical valid coverage rate guarantee. Numerical experiments on both synthetic and real data show that naive conformal prediction techniques do not provide reliable uncertainty estimates, in contrast with the proposed PCP. To the best of our knowledge, this work is the first to propose a calibration scheme that generates statistically valid prediction sets, assuming that the privileged information explains away the corruption appearances. Software implementing the proposed method and reproducing our experiments is available at https://github.com/Shail28/pcp.

## 2 Background and related work

### Conformal prediction

Conformal Prediction (CP)  is a powerful framework for constructing prediction sets that hold a marginal coverage rate guarantee, in the sense of (1). The general recipe to construct such prediction sets is as follows. First, split the data into a proper training set, indexed by \(_{1}\), and a calibration set, indexed by \(_{2}\). Then, fit a given learning model \(\), e.g., a random forest or a neural network, on the training data. Next, evaluate the holdout prediction error of \(\) by applying a non-conformity score function \(()\) to the calibration samples: \(S_{i}=(X_{i},Y_{i};), i_{2}\). Popular score functions include the absolute residual \((x,y;)=|(x)-y|\) in regression cases, where \(\) is a mean estimator, or \(1-(x)_{y}\) in classification settings, where \((x)_{y}\) is the estimated probability of the \(y\) label given \(X=x\). The latter is known as the homogeneous prediction sets (HPS) score . Other score functions include the CQR score  for regression tasks and the APS score  for classification tasks. Armed with the non-conformity scores, the conformal procedure proceeds by computing the \((1+1/|_{2}|)(1-)\)-th empirical quantile of the calibration scores:

\[Q^{}=(1+1/|_{2}|)\,(1-)\,\{S_{i}\}_{i_{2}},\] (2)

where \(1-\) is a user-specified coverage level. Lastly, the prediction set for the test point is given by

\[C^{}(X^{})=\{y:(X^{},y;)  Q^{}\}.\]

The above procedure is guaranteed to generate predictive sets with a valid marginal coverage (1) under the assumption that the calibration and test samples are exchangeable. We now turn to describe _weighted conformal prediction_ (WCP) which is designed to handle exchangeability violations that arise from covariate shifts.

### Weighted conformal prediction

Weighted Conformal Prediction (WCP)  extends the conformal prediction framework to handle covariate shifts. The key idea behind WCP is to weight the distribution of the calibration scores when taking their quantile in (2), so that the weighted scores 'look exchangeable' with the test non-conformity score. For the interest of space, we will not present the general form of WCP, and instead focus on the setup presented in this work, in which \((X(0),Y(0))\!\!\! M Z\). Under this assumption, the corruption indicator induces a covariate shift between the observed clean calibration samples and the test sample, which is explained by \(Z\). That is, the clean calibration samples are drawn from \(P_{X(0),Y(0)|M=0}\), while the test sample is drawn from \(P_{X(0),Y(0)}\). Nevertheless, their distributions are equal conditionally on \(Z\): \(P_{X(0),Y(0)|Z=z,M=0}=P_{X(0),Y(0)|Z=z}\). With this in place, we follow the recipe of WCP and construct a prediction set as follows. First, we compute the ratio of likelihoods between the test and train data:

\[w(z)=^{}(z)}{dP_{Z}^{}(z)}=^{ }(z)}{f_{Z|M=0}^{}(z)}=^{}(z)}{f _{Z}^{}(z)(M=0)}}=(M=0  Z=z)}{(M=0 Z=z)}.\] (3)

We define the set of uncorrupted calibration indexes as: \(_{2}^{}=\{j:_{2},M_{j}=0\}\). The normalized weights are formulated as:

\[p_{i}(Z^{})=)}{_{k_{2}^{} }w(Z_{k})+w(Z^{})},\ \ p_{}(Z^{})=})}{_{k _{2}^{}}w(Z_{k})+w(Z^{})}\] (4)

Then, the calibration threshold for the test point is defined as the \(1-\) empirical quantile of the weighted distribution of the scores:

\[Q^{}(Z^{}):=(1-;_{i _{2}^{}}p_{i}(Z^{})_{S_{i}}+p_{}(Z^{})_{}),\] (5)

and, the prediction set for the test sample is defined similarly to CP:

\[C^{}(X^{},Z^{})=\{y:(X^{},y;) Q^{}(Z^{})\}.\]

Remarkably, WCP produces uncertainty sets that achieve the desired marginal coverage rate (1) despite the induced covariate shift. Nonetheless, to implement this method, we must have access to \(Z^{}\), which is required to obtain \(w(Z^{})\). In our problem setup, however, we assume that \(Z^{}\) is unavailable, and thereby WCP cannot be directly applied. This highlights the key challenge we aim to tackle in this paper, but before describing our method we first outline additional related work.

### Additional related work

The concept of learning from privileged information was introduced by , which proposes techniques to leverage additional knowledge available during training to improve the prediction accuracy and accelerate algorithm convergence rate. This idea has been further explored to train models that are more robust to distribution shifts in the context of domain adaptation [14; 15; 16]. The method proposed in  utilizes PI to handle datasets containing weak labels and to obtain more accurate predictions. Furthermore,  combined model distillation with privileged information as a way to enhance learning from multiple models and data representations. The integration of PI with traditional conformal prediction to generate more informative uncertainty estimates was explored in [19; 20]. This line of work stands in striking contrast with our proposal, as we present a novel robust conformal calibration procedure based on PI. More broadly, there have been developed conformal methods that advance beyond the exchangeability assumption, such as WCP, among other contributions [21; 22; 23; 24; 7; 25; 26]. However, none of these works utilize PI to ensure the validity of the constructed prediction sets.

## 3 Proposed method

In this section, we present our main contribution, the _privileged conformal prediction_ (PCP) method. Since the setup we study in this paper has not been explored in the literature of conformal prediction, we start by suggesting a naive approach to achieve (1). Beyond serving as a baseline method for our PCP, this naive approach also reveals the challenges involved in constructing valid prediction sets when the calibration data is corrupted.

### A naive approach: Two-Staged Conformal

Recall that WCP cannot be directly applied in our setup, since \(Z^{}\) is unknown. To overcome this, the naive approach presented below consists of two stages: (i) estimate the unknown \(Z^{}\) from the feature vector \(X^{}\), and (ii) employ WCP with the estimated privileged information.

While this approach is intuitive, the estimation of \(Z^{}\) must be done in care: if the prediction of \(Z\) is incorrect, then WCP would not provide us the desired coverage guarantee. As a way out, instead of providing a point estimate, we will construct an interval \(C^{Z}(X^{})\) for \(Z^{}\) given \(X^{}\) using conformal prediction. This interval is guaranteed to contain the true PI \(Z^{}\) with probability \(1-\), where \(\) is a miscoverage rate of our choice, e.g., \(=0.01\). Since we do not know which \(z C^{Z}(X^{})\) is the correct \(Z^{}\), we sweep over all possible elements \(z C^{Z}(X^{})\), compute their weights, \(w(z)\), and take the largest weight:

\[w^{}(X^{}):=_{z C^{Z}(X^{})}w(z).\] (6)

The intuition behind taking the largest weight lies in Lemma 1, which states that the larger the test weight is, the larger the threshold \(Q^{}\) produced by WCP, which, in turn, increases the size of the prediction set. Armed with \(w^{}(X^{})\), we can run WCP with a nominal coverage level \(1-+\) using the clean calibration samples and their weights \(\{(X^{}_{i},Y^{}_{i},w(Z_{i}))\}_{i Z^{}}\), and the conservative test weight, \(w^{}(X^{})\). We denote the weighted score quantile provided by WCP in (5) with this conservative test weight by \(Q^{}_{}\). The prediction set is therefore defined as:

\[C^{}(X^{}):=\{y:(X^{}, y;) Q^{}_{}\}.\] (7)

An outline of this procedure is given in Algorithm 2 in Appendix B.1. The proposition below states that the uncertainty set generated by this naive approach is guaranteed to contain the test label \(Y^{}\), despite the presence of corrupted labels in the calibration set.

**Proposition 1**.: _Suppose that \(\{(X_{i}(0),X_{i}(1),Y_{i}(0),Y_{i}(1),Z_{i},M_{i})\}_{i=1}^{n+1}\) are exchangeable, the observed covariates are clean, i.e., \( i:X^{}_{i}=X_{i}(0)=X_{i}(1)\), the covariate shift assumption holds, i.e., \((X(0),Y(0))\!\!\! M Z\), and \(P_{Z}\) is absolutely continuous with respect to \(P_{Z|M=0}\). Then, the prediction set \(C^{}(X^{})\) from (7) achieves a valid coverage rate:_

\[(Y^{} C^{}(X^{})) 1-.\]

The proof is given in Appendix A.1. While this two-staged approach constructs valid prediction sets, it has several limitations. First, it requires predicting not only \(Y^{}\) but also \(Z^{}\), and the prediction of the latter is anticipated to increase the uncertainty encapsulated in the resulting prediction set for \(Y^{}\). This algorithm also requires iterating over all \(z C^{Z}(X^{})\), which can be computationally expensive, especially when \(Z\) is continuous or multi-dimensional. In addition, and perhaps more importantly, the prediction set \(C^{Z}(X^{})\) for \(Z^{}\) might contain unlikely, or off-support values of \(Z\). This can lead to an extreme \(w^{}(X^{})\), which, in turn, results in unnecessarily large prediction sets for \(Y^{}\). Moreover, this naive method assumes that the calibration features \(X^{}_{i}\) reflect the ground truth, i.e., \(X^{}_{i}=X_{i}(0)\), and thus the coverage guarantee does not hold in situations where the features are missing or noisy. This discussion emphasizes the challenges in designing a calibration scheme that not only provides robust coverage guarantees but is also computationally and statistically efficient. In the next section, we present our main proposal which fully resolves all limitations of this naive approach.

### Our main proposal: Privileged Conformal Prediction

In this section, we introduce our procedure to construct prediction sets with a valid coverage rate under the setting of corrupted samples. We begin similarly to CP, as described in Section 2.1, and split the data into a training set, \(_{1}\), and a calibration set, \(_{2}\). Next, we fit a predictive model \(\) on the training data, and compute a non-conformity score for each calibration sample:

\[S_{i}=(X^{}_{i},Y^{}_{i};), i _{2}.\]

Similarly to WCP and Two-Staged methods, we rely on the likelihood ratio of the training and test distributions, and compute the weight of the \(i\)-th sample: \(w_{i}:=(M=0)}{(M=0|Z=Z_{i})}\). The problem inWCP is that the scores threshold \(Q^{}(Z^{})\) from (5) depends on \(Z^{}\). Here, we follow the intuition behind the two-staged baseline and propose an algorithm that provides a fixed threshold \(Q^{}\) that is not a function of \(Z^{}\). This threshold can be thought of as a conservative estimate, or an upper bound of \(Q^{}(Z^{})\), which is based on the calibration data, and does not require \(Z^{}\). To achieve this, we consider every calibration point \(i_{2}\) as a test point, and run \(\) as a subroutine to obtain the \(i\)-th score threshold \(Q_{i}\). The final PCP test score threshold, \(Q^{}\), is defined as the \((1-)\)-th empirical quantile of the calibration thresholds \(\{Q_{i}\}_{i_{2}}\), where \((0,)\) is a level of our choice, e.g., \(=0.01\).

Formally, we consider the \(i\)-th sample as a test point and compute the normalized weight of the \(j\)-th sample:

\[p_{j}^{i}=}{_{k_{2}^{}}w_{k}+w_{i}},\;  i,j_{2}.\]

Notice that \(p_{j}^{i}\) extends the WCP weights, \(p_{j}\), from (4), since \(p_{j}=p_{j}^{n+1}\). Now, we compute the \(i\)-th threshold \(Q_{i}\) by applying WCP using the uncorrupted calibration data:

\[Q_{i}:=(1-+;_{j_{2}^{}}p_{j}^{i}_{S_{j}}+p_{i}^{i}_{}),\] (8)

Next, we extract from \(\{Q_{i}\}_{i_{2}}\) a conservative estimate of \(Q^{}(Z^{})=Q_{n+1}\), denoted by \(Q^{}\):

\[Q^{}:=(1-;_{i_{2}} _{2}|+1}_{Q_{i}}+_{2}|+1} _{}).\] (9)

Finally, for a new input data \(X^{}\), we construct the prediction set for \(Y^{}\) as follows:

\[C^{}(X^{})=\{y:(X^{},y, ) Q^{}\}.\]

For convenience, Algorithm 1 summarizes the above procedure and Algorithm 3 details a more efficient version of this procedure. We now show that the prediction sets constructed by PCP achieve a valid marginal coverage rate.

**Theorem 1**.: _Suppose that \(\{(X_{i}(0),X_{i}(1),Y_{i}(0),Y_{i}(1),Z_{i},M_{i})\}_{i=1}^{n+1}\) are exchangeable, \((X(0),Y(0))\!\!\! M Z\), and \(P_{Z}\) is absolutely continuous with respect to \(P_{Z|M=0}\). Then, the prediction set \(C^{}(X^{})\) constructed according to Algorithm 1 achieves the desired coverage rate:_

\[(Y^{} C^{}(X^{})) 1-.\]

The proof is given in Appendix A.2. We remark that while Theorem 1 requires that the PI satisfies the conditional independence assumption, i.e., \((X(0),Y(0))\!\!\! M Z\), in Appendix A.5 we relax this assumption and provide a lower bound for the coverage rate for settings where the conditional independence assumption is not exactly satisfied. We pause here to emphasize the significance of Theorem 1. The key challenge in proving this result lies in the fact that the \(\{Q_{i}\}_{i_{2}\{n+1\}}\) are not exchangeable. This is attributed to the fact that for every \(i_{2}^{}\), the threshold \(Q_{i}\) is defined using its own score \(S_{i}\), while the test \(Q_{n+1}\) does not rely on its corresponding score \(S_{n+1}=(X^{},Y^{},)\). As a side comment, if the thresholds were exchangeable, the proof was much simpler, as \(Q^{}\) would be greater than \(Q_{n+1}\) with probability \(1-\). In this case, \(C^{}\) includes \(C^{}\) at a high probability, meaning that it achieves the desired coverage rate. Due to the lack of exchangeability, the argument above is incorrect. Indeed, the validity of PCP does not follow directly from the guarantee of WCP, and it requires additional technical steps.

We now turn to discuss the role of \(\). First, we emphasize that Theorem 1 holds for any choice of \((0,)\). Therefore, \(\) only affects the sizes of the uncertainty sets. Intuitively, as \(\), a higher quantile of the weighted distribution of the scores is taken, and a lower quantile of the \(Q_{i}\)'s is taken. Similarly, as \( 0\) a lower quantile of the weighted distribution of the scores is taken, and a higher quantile of the \(Q_{i}\)'s is taken. An optimal \(\) can be considered as the \(\) that leads to the narrowest intervals. Such optimal \(\) can be practically computed with a grid of values for \(\) in \((0,)\), using a validation set. Nonetheless, in our experiments, we directly chose \(\) that is close to 0. In Appendix E.5 we conduct an ablation study analyzing the effect of \(\) on a synthetic dataset.

Lastly, we note that while the real ratios of likelihoods, \(w_{i}\), are required to provide the validity guarantee in Theorem 1, PCP can be employed with estimates of \(w_{i}\). These weights can be estimated in the following way. The first step is estimating the conditional corruption probability given \(Z\), i.e., \((M=0 Z=z)\), using the training and validation sets with any off-the-shelf classifier. We remark that this classifier can be fit on unlabeled data, as this classifier only requires the PI \(Z\) and the corruption indicator \(M\). We denote the model outputs by \((M=0 Z=z)\). Next, we estimate the marginal corruption probability directly from the data: \((M=0)=_{i=1}^{n}M_{i}\). Finally, the estimated weights are computed according to (3): \(_{i}=(z_{i})=(M=0)}{(M=0)Z=z_{i}}\). Even though PCP is not guaranteed to attain the nominal coverage level when employed with the estimates \(_{i}\), the experiments from Section 4.3 indicate that it does achieve a conservative coverage rate in this case. The effect of inaccurate estimates of \(w_{i}\) on the coverage rate attained by PCP could be an exciting future direction to explore, perhaps by drawing on ideas from .

``` Data \((X_{i}^{},Y_{i}^{},Z_{i},M_{i}) \{0,1\},1 i n\), weights \(\{w_{i}\}_{i=1}^{n}\), miscoverage level \((0,1)\), level \((0,)\), an algorithm \(\), a score function \(\), and a test point \(X^{}=x\). Process: Randomly split \(\{1,...,n\}\) into two disjoint sets \(_{1},_{2}\). Fit the base algorithm \(\) on the training data \(\{(X_{i}^{},Y_{i}^{})\}_{i_{1}}\). Compute the scores \(S_{i}=(X_{i}^{},Y_{i}^{};)\) for the calibration samples, \(i_{2}^{}\). Compute a threshold \(Q_{i}\) for each calibration sample according to (8). Compute \(Q^{}\), the \((1-)\) quantile of \(\{Q_{i}\}_{i_{2}}\), according to (9). Output: Prediction set \(C^{}(x)=\{y:(x,y;) Q^{ }\}\). ```

**Algorithm 1**Privileged Conformal Prediction (PCP)

### Privileged Conformal Prediction for scarce data

In this section, we present an adaptation of PCP to handle situations where the sample size is small. While PCP is computationally light, it requires splitting the data into training and calibration sets. This restriction is significant for small datasets in which the reduction in computations from the data splitting comes at the expense of statistical efficiency. To avoid data splitting, we build on the leave-one-out jackknife+ method [9; 27], and, in particular, its weighted version JAW . The method we propose, which we refer to as LOO-PCP, better utilizes the training data compared to PCP. For the interest of space, we refer to Appendix B.3 for the description of LOO-PCP. The following Theorem states that prediction set \(C^{}\) constructed by LOO-PCP is guaranteed to achieve a valid coverage rate under our setup. In Appendix A.3 we provide the proof, which relies on results from .

**Theorem 2**.: _Suppose that \(\{(X_{i}(0),X_{i}(1),Y_{i}(0),Y_{i}(1),Z_{i},M_{i})\}_{i=1}^{n+1}\) are exchangeable, \((X(0),Y(0))\!\!\! M Z\), and \(P_{Z}\) is absolutely continuous with respect to \(P_{Z M=0}\). Then, the prediction set \(C^{}(X^{})\) constructed according to Algorithm 4 satisfies:_

\[(Y^{} C^{}(X^{})) 1-2.\]

We note that in contrast to split CP, here, the coverage guarantee appears with a factor \(2\) in \(\). We refer the reader to  for a detailed explanation of why the factor \(2\) is necessary and cannot be removed. Nonetheless, it is well-known that this jackknife approach greatly improves statistical efficiency compared to split conformal methods.

## 4 Applications

In this section, we exemplify our proposal in three real-life applications. In all experiments, we randomly split the data into training, validation, and test sets. We fit a base learning model on the training data and use the validation set to avoid overfitting. We calibrate the model using the calibration data with the proposed PCP or with a baseline technique, and evaluate the performance on the test set. In all experiments, the calibration schemes are applied to achieve a \(1-=90\%\) marginal coverage rate. We use the CQR non-conformity score in regression tasks, and the _homogeneous prediction sets_ (HPS) non-conformity score [6; 28] in classification tasks. Appendix D describes the full details about the network architecture, training strategy, datasets, corruption technique, and this experimental protocol. The specific formulation of the PI is described in each experiment. In this section, we focus on three use cases: causal inference, missing response, and noisy response. We demonstrate the applicability of PCP on more datasets, and under different corruptions, including additional causal inference tasks in Appendix E.1, more response corruptions in Appendix E.3 and in Appendix E.4.1, and missing features settings in Appendix E.4.2.

### Causal inference: semi-synthetic example

We begin with a causal inference example, in which the goal is to obtain inference for individual treatment effects . In this setting, \(X_{i}(0)=X_{i}(1)\) denotes the features, \(Z_{i}\) denotes the privileged information, \(M_{i}\{0,1\}\) denotes the binary treatment indicator, and \(Y_{i}(0),Y_{i}(1)\) denote the counterfactual outcomes under control and treatment conditions, respectively. Recall that we only observe \(Y_{i}^{}=Y_{i}(M_{i})\) and that the PI explains the treatment pattern \((X(0),Y(0)) M Z\). In this experimental setup, our goal is to construct a prediction set that covers the true test potential outcome under control conditions, i.e., \(Y_{n+1}(0)\), at a user-specified level \(1-=90\%\). Alternatively, we could also aim to predict the outcome under treatment \(Y_{n+1}(1)\). However, in this experiment, we focus on \(Y_{n+1}(0)\) since the dataset we use is highly imbalanced and there are few samples from the treatment group. This task is compelling since it can be used to generate a valid uncertainty interval for the individual treatment effect (ITE), \(Y_{i}(1)-Y_{i}(0)\), which is a great interest for many problems [30; 31; 32; 33]. For instance, the work in  shows how to construct a valid interval for the ITE by combining intervals for \(Y_{n+1}(0)\) and \(Y_{n+1}(1)\). We remark that providing statistically valid prediction intervals for \(Y_{n+1}(0)\) is challenging due to the distribution shift between the observed control responses, which are drawn from \(P_{Y(0)|M=0}\), whereas the test control response is drawn from \(P_{Y(0)}\). Moreover, in this example, we intentionally design \(M_{i}\) to induce such a distribution shift; see Appendix D.1 for more details on the definition of \(M_{i}\).

We test the applicability of our method on the semi-synthetic Infant Health and Development Program (IHDP) dataset , in which the objective is to find the effect of specialist home visits on a child's future cognitive test scores. That is, the feature vector \(X_{i}\) contains covariates describing the child's characteristics, the treatment \(M_{i}\) is the specialist home visits indicator, and the potential outcomes \(Y_{i}(0),Y_{i}(1)\) are the future cognitive test scores. Since this dataset does not originally contain a privileged information variable, we artificially define it as the entry in \(X_{i}\) that correlates the most with \(Y_{i}(0)\). This feature is then removed from \(X_{i}\), so it is unavailable at inference time.

Since the IHDP dataset contains only 747 samples, we apply LOO-PCP in this example and compare it to the following calibration techniques. The first method is a naive jackife+, which uses only the control samples and does not account for distribution shifts. The second and third techniques are two versions of JAW, which is a weighted conformal version of the jackknife+. The first version (Naive WCP) naively uses an estimate of \(P_{M|X}\) as the likelihood ratio weights instead of \(P_{M|Z}\), as \(Z^{}\) is unknown. The second (Infeasible WCP) is an **infeasible** method which requires access to the unknown test privileged information \(Z^{}\) for the computation of the likelihood ratio weights, which can be considered as an oracle calibration process. Importantly, the infeasible JAW and the proposed method use the true corruption probabilities when computing the weights \(w(z)\) from (3).

Figure 1 reports the coverage rates and interval lengths of each calibration scheme. This figure shows that naive jackkife+ and the naive JAW achieve a lower coverage rate than desired. This is anticipated, as both schemes do not accurately account for the distribution shift. In contrast, the infeasible JAW and PCP achieve the desired coverage rate. This is not a surprise, as JAW is guaranteed to attain the nominal coverage level when applied with the correct weights [21, Theorem 1]. However, this method is infeasible to implement in contrast with our proposal, which, according to Theorem 2, is guaranteed to cover the response at the desired rate without using the test privileged information. Furthermore, Figure 1 reveals that PCP constructs intervals with approximately the same width as the ones generated by the infeasible JAW. This indicates that we do not lose much in terms of statistical efficiency by not having access to the test privileged information \(Z^{}\).

### Missing response variable: semi-synthetic example

In this section, we study the performance of PCP and compare it to baselines in a missing response setting using six real datasets: Facebook1,2 , Bio , House , Meps19  and Blog . Since these datasets do not originally contain privileged information, we artificially define \(Z_{i}\) as the feature from \(X_{i}\) that correlates the most with \(Y_{i}\) and then remove it from \(X_{i}\). Furthermore, since all response variables are present in these datasets, we artificially remove the responses in 20% of the samples. We intentionally set the missing probability in a way that induces a distribution shift between the missing and observed variables. In Appendix D.1 we provide the full details about the corruption process and how we impute the missing data.

We compare the proposed method (PCP) to the following calibration schemes: a naive conformal prediction (Naive CP); a naive WCP, which considers only \(X\) to cope with the distribution shift; the two-staged baseline (Two-Staged); and an infeasible weighted conformal prediction (Infeasible WCP) which has access to the test privileged information \(Z^{}\). Importantly, the baseline Two-Staged, the infeasible WCP, and PCP use the real corruption probabilities when computing the weights \(w(z)\) in (3). In contrast, Naive WCP estimates the corruption probability conditioned on \(X\) from the data. Figure 2 presents the performance of each calibration scheme, showing that the naive approach (Naive CP) consistently produces invalid prediction intervals. This is anticipated, as Naive CP does not provide guarantees under distribution shifts. Figure 2 also shows that Two-Staged generates too wide intervals, resulting in a conservative coverage rate of approximately 95%. By contrast, the infeasible WCP and the proposed PCP consistently achieve the desired 90% level. Crucially, PCP is comparable in the interval length to the infeasible WCP. In conclusion, this experiment demonstrates that PCP constructs intervals that are both reliable and informative.

Figure 1: **Causal inference experiment: HHDP dataset. The coverage rate and average interval length achieved by naive jackknife+ (Naive CP), naive JAW which considers only \(X\) to cope with the distribution shift (Naive WCP), an infeasible JAW which uses \(Z^{}\) (Infeasible WCP), and the proposed method (Privileged CP). The metrics are evaluated over 50 random data splits.**

Figure 2: **Missing response experiment. The coverage rate and average interval length obtained by various methods; see text for details. Performance metrics are evaluated over 20 random data splits.**

### Noisy response variable: real example

In what follows, we examine the performance of the proposed technique on the CIFAR-10N  image recognition dataset that contains noisy labels. Here, \(X\) is an image of one out of ten possible objects, and \(Y\) is its corresponding label. The noisy response, \(Y(1)\), is the label annotated by one human annotator, while \(Y(0)\) denotes the clean label obtained from CIFAR-10 . That is, \(M=0\) indicates that the annotator correctly labeled the image. Similarly to , we define the privileged data as information about the annotators. Specifically, the variable \(Z_{i}\) contains two features: (i) the number of unique labels suggested by three annotators for the \(i\)-th sample, and (ii) the time took to annotate the corresponding sample batch, which contains ten images. In this experiment, we compare our method (PCP) to the following calibration schemes: a naive conformal prediction, applied either with the noisy labels Naive CP (clean + noisy) or ignoring them Naive CP (only clean); the two-staged baseline (Two Staged CP); an infeasible WCP (Infeasible WCP) which assumes access to the unknown test privileged information \(Z^{}\). Additionally, since the corruption probabilities are not given in this dataset, we estimate them from the data and use these estimates to compute the weights \(w\) in (3).

Figure 3 presents each calibration scheme's coverage rate and uncertainty set size. This figure shows that Naive CP applied with noisy labels tends to overcover the clean label. This behavior is consistent with the work in [25; 43], which suggests that naive CP constructs conservative uncertainty sets when employed on data with dispersive label-noise. Figure 3 also indicates that calibrating the model only on the clean samples leads to invalid prediction sets that tend to undercover the clean test label. Observe also that the two-stage baseline is overly conservative, as it encapsulates the error in predicting both \(Z^{}\) and the label. In contrast, the coverage rate of infeasible WCP and our proposed PCP is much closer to the desired level, yet slightly conservative. We suggest two possible explanations for this behavior: (i) the weights used are only estimates of the true likelihood ratios; (ii) in this real data we consider here, the PI may not fully explain the corruption mechanism. This highlights the robustness of our method to violations of our assumptions in the specific use-case studied here. Lastly, we remark that the prediction sets of PCP have a similar set size to the sets constructed by the infeasible WCP, which is in line with the results from Section 4.1 and Section 4.2.

## 5 Discussion and impact statement

In this paper, we introduced PCP, a novel calibration scheme to reliability quantify prediction uncertainty in situations where the training data is corrupted. The validity of our proposal is supported by theoretical guarantees and demonstrated in numerical experiments. The key assumption behind our method is that the features and responses are independent of the corruption indicator given the privileged information. This conditional independence resembles the strong ignorability assumption in causal inference [44; 45; 46]. While acquiring PI that satisfies this requirement can be challenging, our work relaxes the strong ignorability assumption, as the confounders are allowed to be absent during inference time. An additional restriction we make is that the true conditional corruption probability must be known to provide a theoretical coverage validity. However, our numerical experiments indicate that estimating these probabilities leads to reliable uncertainty estimates. A promising future direction would be to theoretically analyze the effect of inaccurate weights on the coverage guarantee, e.g., by borrowing ideas from . Finally, we should note that there are potential social implications of our method, akin to many other works that aim to advance the ML field.

Figure 3: **Noisy response experiment: CIFAR-10N dataset. Average coverage and set size obtained by various methods; see text for details. The metrics are evaluated over 20 random data splits.**