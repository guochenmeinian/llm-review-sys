# Moving Off-the-Grid: Scene-Grounded Video Representations

Sjoerd van Steenkiste\({}^{*,}\)\({}^{1}\), Daniel Zoran\({}^{*,}\)\({}^{,}\)\({}^{2}\), Yi Yang\({}^{2}\), Yulia Rubanova\({}^{2}\),

**Rishabh Kabra\({}^{2}\)**, **Carl Doersch\({}^{2}\)**, **Dilara Gokay\({}^{2}\)**, **Joseph Heyward\({}^{2}\)**,

**Etienne Pot\({}^{2}\)**, **Klaus Greff\({}^{2}\)**, **Drew A. Hudson\({}^{2}\)**, **Thomas Albert Keck\({}^{2}\)**,

**Joao Carreira\({}^{2}\)**, **Alexey Dosovitskiy\({}^{1,3}\)**, **Mehdi S. M. Sajjadi\({}^{2}\)**, **Thomas Kipf\({}^{*,}\)\({}^{,}\)\({}^{2}\)**

\({}^{1}\)Google Research, \({}^{2}\)Google DeepMind, \({}^{3}\)Inceptive

###### Abstract

Current vision models typically maintain a fixed correspondence between their representation structure and image space. Each layer comprises a set of tokens arranged "on-the-grid," which biases patches or tokens to encode information at a specific spatio(-temporal) location. In this work we present _Moving Off-the-Grid_ (MooG), a self-supervised video representation model that offers an alternative approach, allowing tokens to move "off-the-grid" to better enable them to represent scene elements consistently, even as they move across the image plane through time. By using a combination of cross-attention and positional embeddings we disentangle the representation structure and image structure. We find that a simple self-supervised objective--next frame prediction--trained on video data, results in a set of latent tokens which bind to specific scene structures and track them as they move. We demonstrate the usefulness of MooG's learned representation both qualitatively and quantitatively by training readouts on top of the learned representation on a variety of downstream tasks. We show that MooG can provide a strong foundation for different vision tasks when compared to "on-the-grid" baselines1.

\({}^{*}\)Equal contribution, \({}^{}\)Work done while at Google.

## 1 Introduction

Learning visual representations of the physical world is at the core of computer vision. Recent years have seen a surge of vision models that address this problem via self-supervised learning . By leveraging objectives such as contrastive learning  and masked image modelling , great strides have been made towards learning useful representations from image data. The vast majority of these methods use convolutional networks , vision transformers  or a combination thereof . This choice of architecture comes to no surprise, as it inherently reflects the structure of the underlying datasets: images are (typically) represented as grids of pixels, which are conveniently and efficiently processed using 2D convolutions and patch-based heuristics. This _grid-based_ processing, however, leads to an inherent entanglement between the representation structure and image structure. In other words, specific tokens or feature vectors of the representation are encouraged to capture the contents of a specific image location, instead of binding to the underlying content of the physical scene.

This issue is particularly apparent when processing video: when there is motion in the scene, either by ego-motion or object motion, the contents of the scene will move across the image plane and as such the representation (i.e. in terms of what is encoded where) will change accordingly. However,many down-stream scene understanding tasks require observing how individual objects (or object parts) change their configuration over time, even when other factors like camera motion translate the objects around the image plane. In this case, a representation that preserves correspondences between meaningful scene elements and representational elements is likely preferred.

As a consequence, many works targeting object-centric tasks such as object detection [4; 36], tracking [25; 33; 38], or segmentation , have adopted specialized architectural components that learn object-based representations: representations that are lifted from the image grid to bind to individual objects. These representations, however, are specialized to object-centric tasks and either need to be learned with detailed supervision [4; 34; 38] or have difficulty scaling to diverse real-world raw video data [19; 33].

In this paper we propose a transformer-based video model that learns representations that are "off-the-grid" (OTG) in a self-supervised manner, providing consistent features that bind to underlying scene elements, and tracking them as they move through time. Our method, _Moving Off-the-Grid_ (MooG), makes extensive use of cross-attention to learn a latent set of tokens that is decoupled from the image grid: tokens are updated via cross-attention when a new input frame arrives, and decoded back into images via cross-attention. MooG can process videos of arbitrary length by iteratively updating the representation as new frames are observed.

In summary, our contributions are as follows:

* We introduce _Moving Off-the-Grid_ (MooG), a novel transformer-based recurrent video representation model that is capable of learning OTG representations via a simple next-frame prediction loss.
* We qualitatively demonstrate that the OTG representation of MooG binds to different parts of the scene and tracks its content under motion, whereas a grid-based representation fails to do so.
* Finally, we demonstrate how this representation facilitates a variety of downstream vision tasks, including point tracking, monocular depth estimation, and object tracking. Our approach outperforms self-supervised grid-based baselines, such as DINO [5; 40], and performs competitively with domain-specific approaches, such as TAP-Net  and TAPIR  for point tracking.

## 2 Related Work

Transformer architectures  for visual tasks have gained substantial traction in the machine learning community in recent years. Starting with methods such as the self-attention architecture applied to CNN feature maps by Zambaldi et al. , the Image Transformer  and later popular approaches such as the Vision Transformer (ViT) , the vast majority of this class of methods operates on a _grid_ of image features (e.g. patches or CNN feature maps), all the way from pixels to the final output of the transformer. This choice of representation, while extremely successful on a wide range of tasks, naturally couples representations to spatial 2D locations in image space.

The predominant approach for decoupling internal model representations from the image grid is by using _cross-attention_, where one set of tokens is updated based on the value of another set of tokens. In particular, object-centric tasks such as detection [4; 64], tracking [33; 38; 63], and instance segmentation [9; 34] have found widespread adoption of this architectural principle to learn individual object tokens that are detached from the image grid, both in supervised methods such as the Detection Transformer (DETR)  or GroupViT , and unsupervised methods such as Slot Attention [36; 57] or CLIPpy . Especially when extended to multi-view observations [27; 48] and video [19; 33; 38; 65], this one-token-per-object representation allows for consistent representation of individual objects across views and frames in a video. In contrast to these approaches, our method does not assume a one-to-one mapping between OTG tokens and _objects_, but instead assigns a large set of latent tokens that can flexibly bind to any part of a scene, such as small surface elements, without committing to any particular notion of an object.

The Perceiver  is closely related to our work: it uses a large set of latent tokens, updated via cross-attention from visual inputs, to support a range of downstream tasks. While the original Perceiver model primarily focuses on end-to-end classification tasks, PerceiverIO  extends this framework to use pixel-based objectives or predict other modalities (such as audio). A single time step of our model can be seen as a variant thereof: we similarly use cross-attention to map to a latent set of tokens, and decode targets (such as pixels, point tracks, etc.) similarly using cross-attention. This type of cross-attention decoder is also used in Scene Representation Transformers , which have recently been applied to video modeling [50; 51]. Different from these works, our approach processes video in a recurrent fashion, encouraging tokens to stay consistently attached to the same scene element independently of how the camera or the content of the scene moves.

Several recent works use a separate set of tokens with either back-and-forth cross-attention, such as RIN  and FIT , or pure self-attention, with extra tokens simply appended to the original set of grid-based tokens. The latter category includes approaches such as AdaTape  and Register Tokens . In our work, we solely update tokens by cross-attending into grid-based representations, without "writing back" into the grid-based representations.

A related line of work explores autoregressive prediction of visual tokens for self-supervised representation learning [2; 18]. Different form this line of work, MooG uses a recurrent architecture to enable consistent binding of latent tokens to scene elements (as opposed to using fixed image patches).

Naturally, most _explicit_ 3D approaches for vision are "off-the-grid", such as architectures operating on top of point clouds [44; 45], and methods for 3D rendering such as 3D Gaussian Splatting [30; 37] and particle-based neural radiance fields [56; 60]. In contrast, our method does not associate explicit 3D coordinates with individual OTG tokens but instead learns high-dimensional vector representations. Outside of the field of computer vision, off-the-grid representations are the predominant representation used to model the physical world at various scales, e.g. in terms of particle-based representations for atomistic systems (one vector per atom) [20; 32; 52] or mesh-based representations for macroscopic physical systems , where representations are anchored to surface elements.

## 3 Method

Moving Off-the-Grid (MooG) is a self-supervised transformer model for representation learning from video. In Section 3.1 we describe its model architecture, which enables learning of scene-grounded video representations. To obtain predictions for various vision tasks, we connect readout modules to MooG's OTG representation, which we describe in Section 3.2.

### Learning self-supervised OTG video representations

We design MooG as a recurrent model that can process an arbitrary number of video frames, while keeping a consistent scene-grounded OTG representation of the video. MooG takes as input a sequence of observed frames \(\{X_{t}\}_{t=1}^{T}\), \(X_{t}^{H W 3}\) and iteratively encodes them into a set of latent tokens. We separate the latent state into _corrected_ states \(\{z_{t}^{c}\}_{t=1}^{T}\), \(z_{t}^{c}^{K D}\), which are obtained by encoding input frames, and _predicted_ states \(\{z_{t}^{p}\}_{t=1}^{T}\), \(z_{t}^{p}^{K D}\), which are the

Figure 1: MooG is a recurrent, transformer-based, video representation model that can be unrolled through time. MooG learns a set of “off-the-grid” latent representation. The model first predicts a _predicted_ state based on the previous model state and observation. The current observation is then encoded and cross-attended to using the predicted state as queries to produce a correction to the prediction. When training, the _predicted_ state is decoded with cross-attention using pixel coordinates as queries in order to reconstruct the current frame. The corrected state is used as input to the predictor to produce the next time step prediction, and so on. The model is trained to minimize the pixel prediction error. By decoupling the latent structure from the image grid structure the model is able to learn tokens that track scene content through time.

model's internal prediction for what will be observed at the next time step. This recurrent processing is an important part of MooG as it allows individual tokens to consistently track elements of the scene through videos of arbitrary length and "anticipate" where a scene element will be observed next.

MooG's training objective is next frame prediction given the previous frame and model state. The model is comprised of three main networks: a predictor \(\) which predicts the current _predicted_ state from the previous _corrected_ state, a decoder \(\) which decodes the _predicted_ state to reconstruct the current frame and a corrector \(\) which encodes the current frame and attempts to correct prediction errors made by the predictor2. We now describe, in order of operation, each component role and inner workings. We refer to Figure 1 for an overview of the model's structure and to Appendix C for details. For comparison, we depict a typical "on-the-grid" baseline model in Figure 2.

PredictorThe predictor takes the previous _corrected_ state \(z^{c}_{t-1}\) and produces the _predicted_ state for the current time step \(z^{p}_{t}\):

\[z^{p}_{t}=z^{c}_{t-1}+(=z^{c}_{t-1}).\] (1)

The role of the predictor is to predict the current state based on previous observations, before the current time step's inputs are observed. The predictor network itself \(\) is a simple self-attention transformer network . Note that the initial corrected state \(z^{c}_{0}\) is initialized to random Gaussian noise (with zero mean and \(=10^{-4}\)) and is not learned. This choice of initialization comes from the need to break the symmetry among the state's tokens, as well as preventing "specialization" of tokens and maintaining permutation symmetry.

CorrectorThe role of the corrector is to convey information from the current observation \(X_{t}\) and use it to update the predicted state \(z^{p}_{t}\) to form the _corrected_ state \(z^{c}_{t}\) for the current time-step. The resulting corrected state should contain the new information obtained from the observation. The image is first encoded using a convolutional neural network \(\) with an added Fourier positional embedding to its output and linearly projecting the result to produce a feature grid \(F_{t}^{H^{} W^{} D}\). Here \(H^{}\) and \(W^{}\) are the resulting spatial dimensions after accounting for striding. This feature grid is then attended to with a cross-attention transformer \(\) using the current predicted state \(z^{p}_{t}\) as initial queries to obtain the state update:

\[z^{c}_{t}=z^{p}_{t}+(=F_{t},=z^{p}_{t}), \;\;F_{t}=(X_{t}).\] (2)

It is important to note that the corrected state does not receive its own individual loss and is only used to provide a better estimate for the predictor in order to predict the next step. In this sense, the separation between the corrector transformer and predictor is somewhat artificial. It is, however, crucial that the image is decoded _only_ from the predicted state--decoding the current frame from the corrected state reduces the problem to simple auto-encoding and hurts representation quality considerably.

DecoderThe decoder takes the current predicted state \(z^{p}_{t}\) and decodes it into an RGB image of arbitrary resolution. Decoding is done using cross-attention where queries are embedded pixel coordinates \(P\) and keys and values come from the predicted state \(z^{t}_{t}\). We utilize the same architecture for arbitrary pixel-based readouts (described in Section 3.2); see Figure 3 for a schematic depiction. Note that the states that are learned are comprised of tokens that are OTG and offer no direct correspondence to the spatial image grid, which necessitates this design of our decoder. At training time we decode \(_{t}\), a sub-sampled version of the target image for efficiency:

\[_{t}=(=z^{t}_{p},=P).\] (3)

To allow for efficient training, we decode only a randomly selected subset of pixels at each training iteration, which reduces computational demands significantly. For details see Appendix C.

Of special note are the attention weights of the decoder network \(\) as they allow us to understand the relationship between a specific spatial position in the image and specific tokens in the latent representation. We analyze this relationship in Section 4.1.

Figure 2: For comparison to MooG we here depict a classic “on-the-grid” model where tokens in the latent state are inherently tied to specific pixel locations.

Loss and trainingWe use a simple \(L_{2}\) training loss on image pixels. For each frame we decode the predicted state \(z_{t}^{p}\) into a sub-sampled output image \(_{t}\), sub-sample the input image \(X_{t}\) at the same pixel locations and calculate the per-frame loss \(L_{t}\):

\[L_{t}=L_{2}(_{t},X_{t}).\] (4)

Note that this is a next-frame prediction loss as the predicted state depends only on the previous frame and model state. During training we unroll the model over \(8\) frames, initializing the state with random Gaussian noise (as described above) and equally weighting all frames for the loss. The final self-supervised prediction training loss averages the \(L_{2}\) loss across frames and pixels.

### Readout decoders for downstream tasks

We qualitatively assess properties of the learned representation in Section 4.1. To make a quantitative assessment, we propose general readout decoders that support a variety of downstream tasks. We distinguish between two types of readouts: grid-based readouts (e.g. RGB or depth pixels) and tracking-based readouts (e.g. 2D points or object tracking). To produce the desired output, all readouts read from the OTG tokens contained in the predicted and corrected states for a given timestep. See Figure 3 for a schematic overview.

Grid-based readoutsFor dense grid-based readouts, we reuse the same decoder architecture as we use for the pixel decoder: individual spatial locations are queried using their \((x,y)\) location in a transformer that solely uses cross-attention. For computational efficiency, we query using a subsampled spatial grid with a random offset to avoid overfitting on a particular subset of pixels.

Recurrent, query-based readoutsFor tracking-based readouts that require keeping track of content in the video after starting from an initial location, we adopt a more sophisticated design similar to the corrector-predictor component of MooG. Given a set of \(N\) queries \(q_{1}^{N D_{q}}\) (e.g. points or boxes) and a sequence of observed frames \(\{X_{t}\}_{t=1}^{T}\), the task is to predict all future readout targets \(\{q_{t}\}\) for \(t=2...T\). We associate a latent encoding \(y_{t}^{N D_{y}}\) with every target at time step \(t\). We first encode the queries \(q_{1}\) using positional encoding followed by an MLP to obtain the readout latents \(y_{t}\). Latents are processed by a corrector, followed by a predictor. Different from MooG, the corrector is implemented as a transformer which solely cross-attends into the inputs (here: MooG states \([z_{t}^{c},z_{t}^{p}]\)) without self-attention between readout latents, to avoid interaction between them. Likewise, the predictor is implemented solely by an MLP, i.e. without any self-attention between readout latents. To read out target values, we apply a learnable MLP head to the (corrected) target latents \(y_{t}\) for each output type, eg. coords, visibility, etc.

Figure 3: Readout decoders overview: for grid-based readouts (e.g. pixels), we use a simple per-frame cross-attention architecture with spatial coordinates as queries, whereas for set-based readouts (points, boxes), we adopt a recurrent readout architecture.

## 4 Experiments

### Self-supervised Training and Qualitative Results

We begin by qualitatively investigating properties of the learned OTG representation. We trained MooG with 1024 512-dimensional OTG tokens on natural videos from the Ego4D dataset  and Kinetics700 dataset  using only the self-supervised prediction loss in (4). The model is trained on randomly sampled sub-sequences of 8 frames, and we observe how it learns to predict next frames very well, achieving a PSNR of 25.64 (on the evaluation set) after 500K steps of training. For evaluation we unroll the model on sequences of 36 frames from a validation set -- these are sequences the model has not been trained on and are much longer in duration. We first observe that the model has no trouble unrolling for much longer sequences than it was trained on, and that the predictions made are inline with the actual ground truth frames (see Figure 4). When motion is fast or erratic the model produces blurry outputs, as would be expected from a deterministic model.

Cross-attention mapsTo understand the role of each token we focus on the cross-attention weights of the decoder, which works by having \(x,y\) coordinates as queries that cross-attend into the representation in order to produce the pixel output (Section 3.1). Using the attention weights for each image location we can visualize how much each token is "responsible" for predicting a specific image location at any given time. We observe that tokens bind to the same image structure consistently through time: the attention maps of specific tokens track the image content at a particular location as it moves across the scene. A representative example of this behavior is shown in Figure 4, which we observe consistently for different tokens and on different sequences. Note that an alternative strategy for the model could have been to "tile" the image across space and make each token responsible for a specific spatial position, which is indeed is what we find the grid-based baseline tokens end up capturing (see Figure 4). For additional examples (and a better viewing experience), we refer to the videos in the supplementary material3.

A more comprehensive way of visualizing the role of individual tokens is by taking the argmax over attention weights at each image location and visualizing the result by colour coding according to the token index. We observe the model learns to assign different parts of the scene to different tokens, tiling the image while aligning well with image structure, not unlike super-pixels. This is visualized

Figure 4: Qualitative analysis of MooG trained on natural videos, shown here are every 4 frames of the original 36 frame long sequence. From top to bottom: Ground truth frames, predicted frames, example MooG token attention map super-imposed on the ground truth frames, example token attention from the recurrent grid-based baseline (see text for details). As can be seen the model is able to predict the next frame well, blurring when there is fast motion or unknown elements enter the scene. The MooG attention map indicates that the visualized token tracks the scene element it binds to across the full range of motion. In contrast, the grid-based token attention map demonstrates how these tokens end up being associated with a specific image location that does not track the scene content. Please see the supplementary material (and website) for other representative examples.

in Figure 7 in Appendix D, supplementary material and website. If there is good binding between specific tokens and specific image structures we should expect the colour coded image to reflect the motion present in the underlying scene. Indeed, MooG does exactly that -- the model learns to devote specific tokens to specific scene structures and bind them consistently across time.

Principal Component AnalysisTo make sense of the content of the representation at a slightly higher level we can use PCA analysis and visualize the results. We unrolled MooG on a batch of 24 sequences, 12 frames each and ran PCA on the concatenated set (across batch and time) of all tokens from the predicted state. We then project all tokens on the 64 leading PCA components and use the decoder attention weights to output 3 of the leading PCA components to image space. We observe that many of the leading components capture the positional embedding and do not relate to sequence content. However, several others end up capturing interesting (semantic) scene structure. Figure 5 shows the 3rd, 20th and 21st components in RGB space, from which it can be seen how tokens end up capturing scene content with similar projections for hands, for example, across several scenes. See website and appendix for more examples.

### End-to-End Training and Quantitative Analysis

In the previous subsection we observed qualitatively how learned off-the-grid representations end up capturing meaningful elements of the scene. Here we study the quality of the learned representation quantitatively, focusing on three down-stream tasks: point tracking, depth prediction and object tracking. For each down-stream task we consider two different approaches for training a readout head that reflect common use cases: (1) training on top of the representations obtained from a frozen pre-trained MooG model and (2) training the readout decoder alongside MooG in an end-to-end manner, i.e. by back-propagating gradients into the model. MooG learned representations are quite local in nature due to the simplicity of the loss and the short prediction horizon. As such we do not expect it to learn abstract representations suitable for more high level tasks such as action recognition etc. We focus here on low and mid level downstream tasks. Details are available in Appendix C.

We focus on two classes of baselines in our comparison: (1) on-the-grid baselines derived from MooG, DINO [5; 40] or VideoMAE v2 , and (2) expert baselines that are more domain specific. The

Figure 5: PCA of MooG tokens unrolled over a batch of short sequences. The model was unrolled over a batch of 24 sequences, 12 frames each. Predicted states from all time steps and batch samples were concatenated and PCA analysis was performed on the entire set jointly. We then reshape the projected set back to its original shape and use the arg-max token to visualize the result in image space (see text and Appendix for full details). Depicted are 3 of the leading PCA components in RGB. Note the salient high-level scene structure (e.g hands) learned by the model.

former grid-based baselines have the same capacity as MooG which makes them directly comparable. We consider two variations: a simple auto-encoder with high capacity, where we have removed the corrector and predictor (named _Grid_); and a recurrent on-the-grid baseline where the corrector implements a cross-attention between the output of the encoder (i.e. on-the-grid latents) and the corrected latents from the previous step. In both cases we account for the absence of the predictor (and corrector) by adding self-attention transformer layers to the encoder. For DINO, we compute representations for each frame using the official pre-trained ViT-B/16 and ViT-B/14 checkpoints that are available for v1  and v2  respectively. Similarly we use the publicly available checkpoints for VideoMAE v2  for three different model sizes: S/B/G. Note that the available VideoMAE v2 model size S and B are distilled from G, i.e. not trained from scratch (unlike MooG). The on-the-grid baselines (including DINO and VideoMAE v2) make use of the same readout decoders as for MooG. We discuss expert baselines in the relevant paragraphs below.

Point trackingThe task of point tracking requires tracking of individual 2D points \((x,y)\) in a video. This can be viewed as a generalization of optical flow prediction, which similarly requires understanding of movement of physical surfaces in a scene relative to the observer. Intuitively, an OTG scene-grounded latent representation should align well with this task and support generalization.

We train MooG on Kubric MOVi-E  using point annotations computed in a similar manner as in Doersch et al. . For each video, we randomly sample a clip of 8 frames to train on. We sample 64 points per frame and use the location of each point in the first frame as the query. To evaluate each model we report the average Jaccard (AJ) as in Doersch et al. , which evaluates both occlusion and position accuracy. Tables 1 and 2 report results for MooG and on-the-grid baselines. It can be seen how MooG learns representations that are better suited for this down-stream task as is evident from the considerable improvements over many of the baselines, especially in the frozen setting (Table 1).

In addition to results on MOVI-E, Tables 1 and 2 also report results for a zero-shot transfer setting to the real-world DAVIS dataset  using point tracks from Doersch et al. . Interestingly, it can be seen how in the end-to-end setting (Table 2) the gap between the grid-based models and MooG decreases considerably, suggesting that while off-the-grid representations generalize better in the pre-training scenario, when optimized for a specific downstream task on-the-grid representations can still be competetive. We also compare to two expert baselines--TAP-Net  and TAPIR --which are designed specifically for point tracking and achieved state-of-the-art performance when published. Both models use explicit cost volumes, i.e., exhaustive comparisons of a query point's features with features on every other frame, and have steps which detect peaks in the similarity maps. Our model is more general and does not have explicit mechanisms such as these at the representation level. The readout mechanism is also quite general. All of these steps exploit the nature of point tracking as a one-to-one feature matching problem, making it difficult to adapt the architecture to other problems. In Table 3 we directly compare MooG (trained end-to-end using a slightly larger encoder backbone) to these methods and report results for 8 frames as well as for the full sequence length. It can be seen how on 8 frames, MooG rivals TAPIR's performance, despite being a less specialized approach.

    &  &  & Waymo \\  Name & Points (\(\)AI) & Depth (\(\)AbsRel) & Boxes (\(\)IoU) & Points (\(\)AI) & Boxes (\(\)IoU) \\  MooG & **0.839** & 0.0359 & **0.793** & 0.687 & **0.730** \\  Grid Rec. & 0.769 & 0.0451 & 0.730 & 0.518 & 0.625 \\ Grid Rec. & 0.778 & 0.0443 & 0.734 & 0.559 & 0.629 \\  DINOv1 (B) & 0.518 & 0.0371 & 0.724 & 0.409 & 0.566 \\ DINOv2 (B) & 0.544 & 0.0370 & 0.738 & 0.402 & 0.559 \\  VMAEv2 (S) & 0.595 & 0.0567 & 0.700 & 0.365 & 0.567 \\ VMAEv2 (B) & 0.681 & 0.0458 & 0.736 & 0.434 & 0.611 \\ VMAEv2 (G) & 0.822 & **0.0311** & **0.793** & **0.720** & 0.708 \\   

Table 1: Down-stream readout performance from frozen representations. For DINO and VideoMAE v2 baselines we highlight what ViT configuration their encoder is based on: (S) \(\)20M parameters, (B) \(\)80M parameters, (G) \(\)1000M parameters. **MooG uses fewer than 35M parameters for encoder, corrector and predictor combined, which suggests a comparison to B-sized models.**

[MISSING_PAGE_FAIL:9]

Figure 6 shows how using only a single layer yields a marginal improvement on MOVi-E, while on DAVIS-8 a slight drop in performance can be observed.

Number of tokensAn advantage of having an "off-the-grid" representation is that the number of tokens is independent of input frame resolution. Furthermore, because we initialize the representation randomly, and because none of our model parameters depend on the number of tokens in the representation, we can instantiate the model with a different number of tokens at test time. Indeed, in Figure 7 we qualitatively show how MooG adapts to this change elegantly and is is still able to predict future frames well even with half or quarter of the number of tokens used in training. The model makes tokens cover larger areas of the scene to adapt for this change (these results are best view in video format). Quantitatively in Figure 6 it can be seen how increasing the number of tokens during training to 2048 or decreasing to 512 doesn't significantly affect MooG's performance.

## 5 Conclusion

While the vast majority of computer vision advances in the past decade can be attributed to successful "on-the-grid" architectures such as CNNs and Vision Transformers, the physical world ultimately does not live on a pixel grid. Instead of coupling the visual processing architecture to the architecture of the camera sensor (a pixel grid), we here propose to move visual representations off the image grid. Our MooG architecture allows representations to flexibly bind to scene surface elements and track the content of the scene as it is subject to motion. We demonstrated that this representation can serve as an effective alternative to the established grid-based counterpart, facilitating tasks that require understanding of motion and scene geometry. The proposed model in this paper is still quite simple - it is deterministic and ignores uncertainty inherent to the prediction task, and uses a very simple L2 pixel loss as the objective. A possible next step is to introduce stochasticity into the model to account for this inherent uncertainty, improving the representations and allowing for longer term prediction. The latter may help the model learn richer, higher-level features of the scene. We have observed that MooG struggles when applied to more semantic downstream tasks and this can likely be explained by the simple deterministic nature of the prediction task and the short-term prediction horizon of the model.