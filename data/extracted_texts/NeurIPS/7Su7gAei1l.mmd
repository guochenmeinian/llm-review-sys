# SymmetricDiffusers: Learning

Discrete Diffusion on Finite Symmetric Groups

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Finite symmetric groups \(S_{n}\) are essential in fields such as combinatorics, physics, and chemistry. However, learning a probability distribution over \(S_{n}\) poses significant challenges due to its intractable size and discrete nature. In this paper, we introduce SymmetricDiffusers, a novel discrete diffusion model that simplifies the task of learning a complicated distribution over \(S_{n}\) by decomposing it into learning simpler transitions of the reverse diffusion using deep neural networks. We identify the riffle shuffle as an effective forward transition and provide empirical guidelines for selecting the diffusion length based on the theory of random walks on finite groups. Additionally, we propose a generalized Plackett-Luce (PL) distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded "denoising schedule" to improve sampling and learning efficiency. Extensive experiments show that our model achieves state-of-the-art or comparable performances on solving tasks including sorting 4-digit MNIST images, jigsaw puzzles, and traveling salesman problems.

## 1 Introduction

As a vital area of abstract algebra, finite groups provide a structured framework for analyzing symmetries and transformations which are fundamental to a wide range of fields, including combinatorics, physics, chemistry, and computer science. One of the most important finite groups is the _finite symmetric group_\(S_{n}\), defined as the group whose elements are all the bijections (or permutations) from a set of \(n\) elements to itself, with the group operation being function composition.

Classic probabilistic models for finite symmetric groups \(S_{n}\), such as the Plackett-Luce (PL) model [35; 27], the Mallows model , and card shuffling methods , are crucial in analyzing preference data and understanding the convergence of random walks. Therefore, studying probabilistic models over \(S_{n}\) through the lens of modern machine learning is both natural and beneficial. This problem is theoretically intriguing as it bridges abstract algebra and machine learning. For instance, Cayley's Theorem, a fundamental result in abstract algebra, states that every group is isomorphic to a subgroup of a symmetric group. This implies that learning a probability distribution over finite symmetric groups could, in principle, yield a distribution over any finite group. Moreover, exploring this problem could lead to the development of advanced models capable of addressing tasks such as permutations in ranking problems, sequence alignment in bioinformatics, and sorting.

However, learning a probability distribution over finite symmetric groups \(S_{n}\) poses significant challenges. First, the number of permutations of \(n\) objects grows factorially with \(n\), making the inference and learning computationally expensive for large \(n\). Second, the discrete nature of the data brings difficulties in designing expressive parameterizations and impedes the gradient-based learning.

In this work, we propose a novel discrete (state space) diffusion model over finite symmetric groups, dubbed as _SymmetricDiffusers_. It overcomes the above challenges by decomposing the difficultproblem of learning a complicated distribution over \(S_{n}\) into a sequence of simpler problems, _i.e_., learning individual transitions of a reverse diffusion process using deep neural networks. Based on the theory of random walks on finite groups, we investigate various shuffling methods as the forward process and identify the riffle shuffle as the most effective. We also provide empirical guidelines on choosing the diffusion length based on the mixing time of the riffle shuffle. Furthermore, we examine potential transitions for the reverse diffusion, such as inverse shuffling methods and the PL distribution, and introduce a novel generalized PL distribution. We prove that our generalized PL is more expressive than the PL distribution. Additionally, we propose a theoretically grounded "denoising schedule" that merges reverse steps to improve the efficiency of sampling and learning. To validate the effectiveness of our SymmetricDiffusers, we conduct extensive experiments on three tasks: sorting 4-Digit MNIST images, solving Jigsaw Puzzles on the Noisy MNIST and CIFAR-10 datasets, and addressing traveling salesman problems (TSPs). Our model achieves the state-of-the-art or comparable performance across all tasks.

## 2 Related Works

**Random Walks on Finite Groups.** The field of random walks on finite groups, especially finite symmetric groups, have been extensively studied by previous mathematicians [37; 11; 4; 38]. Techniques from a variety of different fields, including probability, combinatorics, and representation theory, have been used to study random walks on finite groups . In particular, random walks on finite symmetric groups are first studied in the application of card shuffling, with many profound theoretical results of shuffling established. A famous result in the field shows that 7 riffle shuffles are enough to mix up a deck of 52 cards , where a riffle shuffle is a mathematically precise model that simulates how people shuffle cards in real life. The idea of shuffling to mix up a deck of cards aligns naturally with the idea of diffusion, and we seek to fuse the modern techniques of diffusion models with the classical theories of random walks on finite groups.

**Diffusion Models.** Diffusion models [40; 41; 16; 42] are a powerful class of generative models that typically deals with continuous data. They consist of forward and reverse processes. The forward process is typically a discrete-time continuous-state Markov chain or a continuous-time continuous-state Markov process that gradually adds noise to data, and the reverse process learn neural networks to denoise. Discrete (state space) diffusion models have also been proposed to handle discrete data like image, text , and graphs . Existing discrete diffusion models are applicable for learning distributions of permutations. However, they focused on cases where the state space is small or has a special (_e.g_., decomposable) structure and are unable to deal with intractable-sized state spaces like the symmetric group. In particular,  requires an explicit transition matrix, which has size \(n! n!\) in the case of finite symmetric groups and has no simple representations or sparsifications.

**Differentiable Sorting and Learning Permutations.** A popular paradigm to learn permutations is through differentiable sorting or matching algorithms. Various differentiable sorting algorithms have been proposed that uses continuous relaxations of permutation matrices [13; 8; 5], or uses differentiable swap functions [33; 34; 20]. The Gumbel-Sinkhorn method  has also been proposed to learn latent permutations using the continuous Sinkhorn operator. Such methods often focus on finding the optimal permutation instead of learning a distribution over the finite symmetric group. Moreover, they tend to be less effective as \(n\) grows larger due to their high complexities.

## 3 Learning Diffusion Models on Finite Symmetric Groups

We first introduce some notations. Fix \(n\). Let \([n]\) denote the set \(\{1,2,,n\}\). A _permutation_\(\) on \([n]\) is a function from \([n]\) to \([n]\), and we usually write \(\) as \(1&2&&n\\ (1)&(2)&&(n)\). The _identity permutation_, denoted by \(\), is the permutation given by \((i)=i\) for all \(i[n]\). Let \(S_{n}\) be the set of all permutations (or bijections) from a set of \(n\) elements to itself, called the _finite symmetric group_, whose group operation is the function composition. For a permutation \( S_{n}\), the permutation matrix \(Q_{}^{n n}\) associated with \(\) satisfies \(e_{i}^{}Q_{}=e_{(i)}^{}\) for all \(i[n]\). In this paper, we consider a set of \(n\) distinctive objects \(=\{_{1},,_{n}\}\), where the \(i\)-th object is represented by a \(d\)-dimensional vector \(_{i}\). Therefore, a ranked list of objects can be represented as a matrix \(X=[_{1},,_{n}]^{}^{n d}\), where the ordering of rows corresponds to the ordering of objects. We can permute \(X\) via permutation \(\) to obtain \(Q_{}X\).

Our goal is to learn a distribution over \(S_{n}\). We propose learning discrete (state space) diffusion models, which consist of a _forward process_ and a _reverse process_. In the forward process, starting from the unknown data distribution, we simulate a random walk until it reaches a known stationary "noise" distribution. In the reverse process, starting from the known noise distribution, we simulate another random walk, where the transition probability is computed using a neural network, until it recovers the data distribution. Learning a transition distribution over \(S_{n}\) is often more manageable than learning the original distribution because: (1) the support size (the number of states that can be reached in one transition) could be much smaller than \(n!\), and (2) the distance between the initial and target distributions is smaller. By doing so, we break down the hard problem (learning the original distribution) into a sequence of simpler subproblems (learning the transition distribution). The overall framework is illustrated in Fig. 1. In the following, we will introduce the forward card shuffling process in Section 3.1, the reverse process in Section 3.2, the network architecture and training in Section 3.3, denoising schedule in Section 3.4, and reverse decoding methods in Section 3.5.

### Forward Diffusion Process: Card Shuffling

Suppose we observe a set of objects \(\) and their ranked list \(X_{0}\). They are assumed to be generated from an unknown data distribution in an IID manner, _i.e._, \(X_{0},}}{{}}p_{}(X,)\). One can construct a bijection between a ranked list of \(n\) objects and an ordered deck of \(n\) cards. Therefore, permuting objects is equivalent to shuffling cards. In the forward diffusion process, we would like to add "random noise" to the rank list so that it reaches to some known stationary distribution like the uniform. Formally, we let \( S_{n}\) be a set of permutations that are realizable by a given shuffling method in one step. \(\) does not change across steps in common shuffling methods. We will provide concrete examples later. We then define the _forward process_ as a Markov chain,

\[q(X_{1:T}|X_{0},)=q(X_{1:T}|X_{0})=_{t=1}^{T}q(X_{t}|X_{t-1}),\] (1)

where \(q(X_{t}|X_{t-1})=_{_{t}}q(X_{t}|X_{t-1},_{t})q( _{t})\) and the first equality in Eq. (1) holds since \(X_{0}\) implies \(\). In the forward process, although the set \(\) does not change, the rank list of objects \(X_{t}\) changes. Here \(q(_{t})\) has the support \(\) and describes the permutation generated by the underlying shuffling method. Note that common shuffling methods are time-homogeneous Markov chains, _i.e._, \(q(_{t})\) stays the same across time. \(q(X_{t}|X_{t-1},_{t})\) is a delta distribution \((X_{t}=Q_{_{t}}X_{t-1})\) since the permuted objects \(X_{t}\) are uniquely determined given the permutation \(_{t}\) and \(X_{t-1}\). We denote the _neighbouring states_ of \(X\) via one-step shuffling as \(N_{}(X):=\{Q_{}X|\}\). Therefore, we have,

\[q(X_{t}|X_{t-1})=q(_{t})&X_{t} N_{}(X_ {t-1})\\ 0&\] (2)

Figure 1: This figure illustrates our discrete diffusion model on finite symmetric groups. The middle graphical model displays the forward and reverse diffusion processes. We demonstrate learning distributions over the symmetric group \(S_{3}\) via the task of sorting three MNIST 4-digit images. The top part of the figure shows the marginal distribution of a ranked list of images \(X_{t}\) at time \(t\), while the bottom shows a randomly drawn list of images.

Note that \(X_{t} N_{}(X_{t-1})\) is equivalent to \(_{t}\) and \(X_{t}=Q_{_{t}}X_{t-1}\).

#### 3.1.1 Card Shuffling Methods

We now consider several popular shuffling methods as the forward transition, _i.e._, _random transpositions_, _random insertions_, and _riffle shuffles_. Different shuffling methods provide different design choices of \(q(_{t})\), thus corresponding to different forward diffusion processes. Although all these forward diffusion processes share the same stationary distribution, _i.e._, the uniform, they differ in their mixing time. We will introduce stronger quantitative results on their mixing time later.

**Random Transpositions.** One natural way of shuffling is to swap pairs of objects. Formally, a _transposition_ or a _swap_ is a permutation \( S_{n}\) such that there exist \(i j[n]\) with \((i)=j\), \((j)=i\), and \((k)=k\) for all \(k\{i,j\}\), in which case we denote \(=(i j)\). We let \(=\{(i j):i j[n]\}\{\}\). For any time \(t\), we define \(q(_{t})\) by choosing two indices from \([n]\) uniformly and independently and swap the two indices. If the two chosen indices are the same, then this means that we have sampled the identity permutation. Specifically, \(q(_{t}=(i j))=2/n^{2}\) when \(i j\) and \(q(_{t}=)=1/n\).

**Random Insertions.** Another shuffling method is to insert the last piece to somewhere in the middle. Let \(_{i}\) denote the permutation that inserts the last piece right before the \(i^{}\) piece, and let \(:=\{_{i}:i[n]\}\). Note that \(_{n}=\). Specifically, we have \(q(_{t}=_{i})=1/n\) when \(i n\) and \(q(_{t}=)=1/n\).

**Riffle Shuffles.** Finally, we introduce the riffle shuffle, a method similar to how serious card players shuffle cards. The process begins by roughly cutting the deck into two halves and then interleaving the two halves together. A formal mathematical model of the riffle shuffle, known as the _GSR model_, was introduced by Gilbert and Shannon , and independently by Reeds . The model is described as follows. A deck of \(n\) cards is cut into two piles according to binomial distribution, where the probability of having \(k\) cards in the top pile is \(/2^{n}\) for \(0 k n\). The top pile is held in the left hand and the bottom pile in the right hand. The two piles are then riffled together such that, if there are \(A\) cards left in the left hand and \(B\) cards in the right hand, the probability that the next card drops from the left is \(A/(A+B)\), and from right is \(B/(A+B)\). We implement the riffle shuffles according to the GSR model. For simplicity, we will omit the term "GSR" when referring to riffle shuffles hereafter.

There exists an exact formula for the probability over \(S_{n}\) obtained through one-step riffle shuffle. Let \( S_{n}\). A _rising sequence_ of \(\) is a subsequence of \(\) constructed by finding a maximal subset of indices \(i_{1}<i_{2}<<i_{j}\) such that permuted values are contiguously increasing, _i.e._, \((i_{2})-(i_{1})=(i_{3})-(i_{2})==(i_{j})- (i_{j-1})=1\). For example, the permutation \((1&2&3&4&5\\ 1&4&2&5&3)\) has 2 rising sequences, _i.e._, 123 (red) and 45 (blue). Note that a permutation has 1 rising sequence if and only if it is the identity permutation. Denoting by \(q_{}()\) the probability of obtaining \(\) through one-step riffle shuffle, it is shown in  that

\[q_{}()=}=(n+1)/2^{ n}&$}\\ 1/2^{n}&$}\\ 0&\] (3)

where \(r\) is the number of rising sequences of \(\). The support \(\) is thus the set of all permutations with at most two rising sequences. We let the forward process be \(q(_{t})=q_{}(_{t})\) for all \(t\).

#### 3.1.2 Mixing Times and Cut-off Phenomenon

All of the above shuffling methods have the uniform distribution as the stationary distribution. However, they have different mixing times (_i.e._, the time until the Markov chain is close to its stationary distribution measured by some distance), and there exist quantitative results on their mixing times. Let \(q\{q_{},q_{},q_{}\}\), and for \(t\), let \(q^{(t)}\) be the marginal distribution of the Markov chain after \(t\) shuffles. We describe the mixing time in terms of the total variation (TV) distance between two probability distributions, _i.e._, \(D_{}(q^{(t)},u)\), where \(u\) is the uniform distribution.

For all three shuffling methods, there exists a _cut-off phenomenon_, where \(D_{}(q^{(t)},u)\) stays around 1 for initial steps and then abruptly drops to values that are close to 0. The _cut-off time_ is the time when the abrupt change happens. For the formal definition, we refer the readers to Definition 3.3 of . In , they also provided the cut-off time for random transposition, random insertion, and riffle shuffle, which are \( n\), \(n n\), and \(_{2}n\) respectively. Observe that the riffle shuffle reaches the cut-off much faster than the other two methods, which means it has a much faster mixing time. Therefore, we use the riffle shuffle in the forward process.

### The Reverse Diffusion Process

We now model the _reverse process_ as another Markov chain conditioned on the set of objects \(\). We denote the set of realizable _reverse permutations_ as \(\), and the neighbours of \(X\) with respect to \(\) as \(N_{}(X):=\{Q_{}X:\}\). The conditional joint distribution is given by

\[p_{}(X_{0:T}|)=p(X_{T}|)_{t=1}^{T}p_{}( X_{t-1}|X_{t}),\] (4)

where \(p_{}(X_{t-1}|X_{t})=_{^{}_{t}}p(X_{t-1}|X _{t},^{}_{t})p_{}(^{}_{t}|X_{t})\). To sample from \(p(X_{T}|)\), one simply samples a random permutation from the uniform distribution and then shuffle the objects accordingly to obtain \(X_{T}\). \(p(X_{t-1}|X_{t},^{}_{t})\) is again a delta distribution \((X_{t-1}=Q_{^{}_{t}}X_{t})\). We have

\[p_{}(X_{t-1}|X_{t})=p_{}(^{}_{t}|X_{ t})&X_{t-1} N_{}(X_{t})\\ 0&\] (5)

where \(X_{t-1} N_{}(X_{t})\) is equivalent to \(^{}_{t}\) and \(X_{t-1}=Q_{^{}_{t}}X_{t}\). In the following, we will introduce the specific design choices of the distribution \(p_{}(^{}_{t}|X_{t})\).

#### 3.2.1 Inverse Card Shuffling

A natural choice is to use the inverse operations of the aforementioned card shuffling operations in the forward process. Specifically, for the forward shuffling \(\), we introduce their inverse operations \(:=\{^{-1}:\}\), from which we can parameterize \(p_{}(^{}_{t}|X_{t})\).

**Inverse Transposition.** Since the inverse of a transposition is also a transposition, we can let \(:==\{(i j):i j[n]\}\{\}\). We define a distribution of inverse transposition (IT) over \(\) using \(n+1\) real-valued parameters \(=(s_{1},,s_{n})\) and \(\) such that

\[p_{}()=1-()&=\\ ()(,_{ij})_{1}(,_{ij})_{2}+ (,_{ji})_{1}(,_{ji})_{2}&= i ji j,\] (6)

where \((,)_{i}=s_{(i)}/_{k=i}^{n} s_{(k)}\) and \(()\) is the sigmoid function. \(_{ij}\) is any permutation starting with \(i\) and \(j\), _i.e_., \(_{ij}(1)=i\) and \(_{ij}(2)=j\). \(_{ji}\) is any permutation starting with \(j\) and \(i\), _i.e_., \(_{ji}(1)=j\) and \(_{ji}(2)=i\).

**Inverse Insertion.** For the random insertion, the inverse operation is to insert some piece to the end. Let \(_{i}\) denote the permutation that moves the \(i^{}\) component to the end, and let \(:=\{_{i}:i[n]\}\). We define a categorial distribution of inverse insertion (II) over \(\) using parameters \(=(s_{1},,s_{n})\) such that,

\[p_{}(=_{i})=(s_{i})/_{ j=1}^{n}(s_{j}).\] (7)

**Inverse Riffle Shuffle.** In the riffle shuffle, the deck of card is first cut into two piles, and the two piles are riffled together. So to undo a riffle shuffle, we need to figure out which pile each card belongs to, _i.e_., making a sequence of \(n\) binary decisions. We define the Inverse Riffle Shuffle (IRS) distribution using parameters \(=(s_{1},,s_{n})\) as follows. Starting from the last (the \(n^{}\)) object, each object \(i\) has probability \((s_{i})\) of being put on the top of the left pile. Otherwise, it falls on the top of the right pile. Finally, put the left pile on top of the right pile, which gives the shuffled result.

#### 3.2.2 The Plackett-Luce Distribution and Its Generalization

Other than specific inverse shuffling methods to parameterize the reverse process, we also consider general distributions \(p_{}(^{}_{t}|X_{t})\) whose support are the whole \(S_{n}\), _i.e_., \(=S_{n}\).

**The PL Distribution.** A popular distribution over \(S_{n}\) is the Plackett-Luce (PL) distribution , which is constructed from \(n\) real-valued scores \(=(s_{1},,s_{n})\) as follows,

\[p_{}()=_{i=1}^{n}s_{(i)} /(_{j=i}^{n}s_{(j)}),\] (8)for all \( S_{n}\). Intuitively, \((s_{1},,s_{n})\) represents the preference given to each index in \([n]\). To sample from \(_{}\), we first sample \((1)\) from \((n,())\). Then we remove \((1)\) from the list and sample \((2)\) from the categorical distribution corresponding to the rest of the scores (logits). We continue in this manner until we have sampled \((1),,(n)\). By , the mode of the PL distribution is the permutation that sorts \(\) in descending order.

**The Generalized PL (GPL) Distribution.** We also propose a generalization of the PL distribution, referred to as _Generalized Plackett-Luce (GPL) Distribution_. Unlike the PL distribution, which uses a set of \(n\) scores, the GPL distribution uses \(n^{2}\) scores \(\{_{1},,_{n}\}\), where each \(_{i}=\{s_{i,1},,s_{i,n}\}\) consists of \(n\) scores. The GPL distribution is constructed as follows,

\[p_{}():=_{i=1}^{n}(s_{i,(i)} )/(_{j=i}^{n}(s_{i,(j)})).\] (9)

Sampling of the GPL distribution begins with sampling \((1)\) using \(n\) scores \(_{1}\). For \(2 i n\), we remove \(i-1\) scores from \(_{i}\) that correspond to \((1),,(i-1)\) and sample \((i)\) from a categorical distribution constructed from the remaining \(n-i+1\) scores in \(_{i}\). It is important to note that the family of PL distributions is a strict subset of the GPL family. Since the GPL distribution has more parameters than the PL distribution, it is expected to be more expressive. In fact, when considering their ability to express the delta distribution, which is the target distribution for many permutation learning problems, we have the following result.

**Proposition 1**.: _The PL distribution cannot exactly represent a delta distribution. That is, there does not exist an \(\) such that \(p_{}=_{}\) for any \( S_{n}\), where \(_{}()=1\) and \(_{}()=0\) for all \(\). But the GPL distribution can represent a delta distribution exactly._

### Network Architecture and Training

We now briefly introduce how to use neural networks to parameterize the above distributions used in the reverse process. At any time \(t\), given \(X_{t}^{n d}\), we use a neural network with parameters \(\) to construct \(p_{}(_{t}^{}|X_{t})\). In particular, we treat \(n\) rows of \(X_{t}\) as \(n\) tokens and use a Transformer architecture along with the time embedding of \(t\) and the positional encoding to predict the previously mentioned scores. For example, for the GPL distribution, to predict \(n^{2}\) scores, we introduce \(n\) dummy tokens that correspond to the \(n\) permuted output positions. We then perform a few layers of masked self-attention (\(2n 2n\)) to obtain the token embedding \(Z_{1}^{n d_{}}\) corresponding to \(n\) input tokens and \(Z_{2}^{n d_{}}\) corresponding to \(n\) dummy tokens. Finally, the GPL score matrix is obtained as \(S_{}=Z_{1}Z_{2}^{}^{n n}\). Since the aforementioned distributions have different numbers of scores, the specific architectures of the Transformer differ. We provide more details in Appendix B.

To learn the diffusion model, we maximize the following variational lower bound:

\[_{p_{}(X_{0},)} p_{}(X_{0}| )_{p_{}(X_{0},)q(X_{1: }|X_{0},)}[ p(X_{T}|)+_{t=1}^{T} (X_{t-1}|X_{t})}{q(X_{t}|X_{t-1})}].\] (10)

In practice, one can draw samples to obtain the Monte Carlo estimation of the lower bound. Due to the complexity of shuffling transition in the forward process, we can not obtain \(q(X_{t}|X_{0})\) analytically, as is done in common diffusion models [16; 3]. Therefore, we have to run the forward process to collect samples. Fortunately, it is efficient as the forward process only involves shuffling integers. We include more training details in Appendix E.

### Denoising Schedule via Merging Reverse Steps

If one merges some steps in the reverse process, sampling and learning would be faster and more memory efficient. The variance of the training loss could also be reduced. Specifically, at time \(t\) of the reverse process, instead of predicting \(p_{}(X_{t-1}|X_{t})\), we can predict \(p_{}(X_{t^{}}|X_{t})\) for any \(0 t^{}<t\). Given a sequence of timesteps \(0=t_{0}<<t_{k}=T\), we can now model the reverse process as

\[p_{}(X_{t_{0}},,X_{t_{k}}|)=p(X_{T}|) _{i=1}^{k}p_{}(X_{t_{i-1}}|X_{t_{i}}).\] (11)

To align with the literature of diffusion models, we call the list \([t_{0},,t_{k}]\) the _denoising schedule_. After incorporating the denoising schedule in Eq. (10), we obtain the loss function:

\[()=_{p_{}(X_{0},)}_ {q(X_{1:T}|X_{0},)}[- p(X_{T}|)-_{i=1}^{k }(X_{t_{i-1}}|X_{t_{i}})}{q(X_{t_{i}}|X_{t_{i-1}})}].\] (12)Note that although we may not have the analytical form of \(q(X_{t_{i}}|X_{t_{i-1}})\), we can draw samples from it. Merging is feasible if the support of \(p_{}(X_{t_{i-1}}|X_{t_{i}})\) is equal or larger than the support of \(q(X_{t_{i}}|X_{t_{i-1}})\); otherwise, the inverse of some forward permutations would be almost surely unrecoverable. Therefore, we can implement a non-trivial denoising schedule (_i.e._, \(k<T\)), when \(p_{}(^{}_{i}|X_{t})\) follows the PL or GPL distribution, as they have whole \(S_{n}\) as their support. However, merging is not possible for inverse shuffling methods, as their support is smaller than that of the corresponding multi-step forward shuffling. To design a successful denoising schedule, we first describe the intuitive principles and then provide some theoretical insights. 1) The length of forward diffusion \(T\) should be minimal so long as the forward process approaches the uniform distribution. 2) If distributions of \(X_{t}\) and \(X_{t+1}\) are similar, we should merge these two steps. Otherwise, we should not merge them, as it would make the learning problem harder.

To quantify the similarity between distributions shown in 1) and 2), the TV distance is commonly used in the literature. In particular, we can measure \(D_{}(q^{(t)},q^{(t^{})})\) for \(t t^{}\) and \(D_{}(q^{(t)},u)\), where \(q^{(t)}\) is the distribution at time \(t\) in the forward process and \(u\) is the uniform distribution. For riffle shuffles, the total variation distance can be computed exactly. Specifically, we first introduce the _Eulerian Numbers_\(A_{n,r}\), _i.e._, the number of permutations in \(S_{n}\) that have exactly \(r\) rising sequences where \(1 r n\). \(A_{n,r}\) can be computed using the following recursive formula \(A_{n,r}=rA_{n-1,r}+(n-r+1)A_{n-1,r-1}\) where \(A_{1,1}=1\). We then have the following result.

**Proposition 2**.: _Let \(t t^{}\) be positive integers. Then_

\[D_{}(q^{(t)}_{},q^{(t^{})}_{} )=_{r=1}^{n}A_{n,r}|}}-r}{n}-n}}}-r}{n}|,\] (13)

_and_

\[D_{}(q^{(t)}_{},u)=_{r=1}^{n }A_{n,r}|}}-r}{n}-|.\] (14)

Note that Eq. (14) was originally given in . We restate it here for completeness. Once the Eulerian numbers are precomputed, the TV distances can be computed in \(O(n)\) time instead of \(O(n!)\). Through extensive experiments, we have the following empirical observation. For the principle 1), choosing \(T\) so that \(D_{}(q^{(T)}_{},u) 0.005\) yields good results. For the principle 2), a denoising schedule \([t_{0},,t_{k}]\) with \(D_{}(q^{(t_{i})}_{},q^{(t_{i+1})}_{}) 0.3\) for most \(i\) works well. We show an example on sorting \(n=100\) four-digit MNIST images in Fig. 2.

### Reverse Process Decoding

We now discuss how to decode predictions from the reverse process at test time. In practice, one is often interested in the most probable state or a few states with high probabilities under \(p_{}(X_{0}|)\). However, since we can only draw samples from \(p_{}(X_{0}|)\) via running the reverse process, exact decoding is intractable. The simplest approximated method is greedy search, _i.e._, successively finding the mode or an approximated mode of \(p_{}(X_{t_{i-1}}|X_{t_{i}})\). Another approach is beam search, which maintains a dynamic buffer of \(k\) candidates with highest probabilities. Nevertheless, for one-step reverse transitions like the GPL distribution, even finding the mode is intractable. To address this, we employ a hierarchical beam search that performs an inner beam search within \(n^{2}\) scores at each step of the outer beam search. Further details are provided in Appendix C.

## 4 Experiments

We now demonstrate the general applicability and effectiveness of our model through a variety of experiments, including sorting 4-digit MNIST numbers, solving jigsaw puzzles, and addressing traveling salesman problems. Additional details are provided in the appendix due to space constraints.

### Sorting 4-digit MNIST Images

We first evaluate our SymmetricDiffusers on the four-digit MNIST sorting benchmark, a well-established testbed for differentiable sorting [5; 8; 13; 20; 33; 34]. Each four-digit image in this benchmark is obtained by concatenating 4 individual images from MNIST. For evaluation, we employ several metrics to compare methods, including Kendall-Tau coefficient (measuring the correlation between rankings), accuracy (percentage of images perfectly reassembled), and correctness (percentage of pieces that are correctly placed).

**Ablation Study.** We conduct an ablation study to verify our design choices for reverse transition and decoding strategies. As shown in Table 3, combining PL with either beam search (BS) or greedy search yields good results in terms of Kendall-Tau and correctness metrics. In contrast, the IRS (inverse riffle shuffle) method, along with greedy search, performs poorly across all metrics, showing the limitations of IRS in handling complicated sorting tasks. Finally, combining GPL and BS achieves the best accuracy in correctly sorting the entire sequence of images. Given that accuracy is the most

    &  &  &  \\   & & \(2 2\) & \(3 3\) & \(4 4\) & \(5 5\) & \(6 6\) & \(2 2\) & \(3 3\) & \(4 4\) \\    & Kendall-Tau \(\) & 0.9984 & 0.6908 & 0.3578 & 0.2430 & 0.1755 & 0.8378 & 0.5044 & **0.016** \\  & Accuracy (\%) & 99.81 & 44.65 & 00.86 & 0.00 & 0.00 & 76.54 & 6.07 & 0.21 \\  & Correct (\%) & 99.91 & 80.20 & 49.51 & 26.94 & 14.91 & 86.10 & 43.59 & 25.31 \\  &  & **0.0022** & 0.1704 & 0.4572 & 0.8915 & 14.91 & 86.10 & 43.59 & 25.31 \\  & & MAE \(\) & 0.0003 & 0.0233 & 0.1005 & 0.3239 & 0.4515 & 0.1368 & 0.5320 & 0.6873 \\   & Kendall-Tau \(\) & 0.9931 & 0.3054 & 0.0374 & 0.0176 & 0.0095 & 0.6463 & 0.1460 & 0.0490 \\  & Accuracy (\%) & 99.02 & 5.56 & 0.00 & 0.00 & 0.00 & 59.18 & 0.96 & 0.00 \\  & Correct (\%) & 99.50 & 42.25 & 10.77 & 6.39 & 3.77 & 5.48 & 27.87 & 12.27 \\  &  & 0.0689 & 1.0746 & 1.3290 & 14.883 & 1.5478 & 0.7389 & 1.2691 & 1.3876 \\  & & MAE \(\) & 0.0030 & 0.4283 & 0.6531 & 0.8204 & 0.8899 & 0.2800 & 0.8123 & 0.9737 \\   & Kendall-Tau \(\) & 0.9899 & 0.2014 & 0.0100 & 0.0034 & -0.0021 & 0.6604 & 0.1362 & 0.0318 \\  & Accuracy (\%) & 98.62 & 0.82 & 0.00 & 0.00 & 0.00 & 60.96 & 0.68 & 0.00 \\  & Correct (\%) & 99.28 & 32.65 & 7.40 & 4.39 & 2.50 & 75.99 & 26.75 & 10.33 \\  &  & 0.0814 & 1.1764 & 1.3579 & 1.5084 & 1.5606 & 0.7295 & 1.2820 & 1.4095 \\  & & MAE \(\) & 0.0041 & 0.5124 & 0.6818 & 0.8424 & 0.9041 & 0.2731 & 0.8260 & 0.9990 \\   & Kendall-Tau \(\) & **0.9992** & **0.8126** & **0.4859** & **0.2853** & **0.1208** & **0.9023** & **0.8363** & 0.2518 \\  & Accuracy (\%) & **99.88** & **57.38** & **1.38** & 0.00 & 0.00 & **90.15** & **70.94** & **0.64** \\   & Correct (\%) & **99.94** & **86.16** & **58.51** & **37.91** & **18.54** & **92.99** & **86.84** & **34.69** \\   & & RMSE \(\) & 0.0026 & **0.0241** & **0.1002** & **0.2926** & **0.4350** & **0.3248** & **0.3892** & **0.8953** \\   & & MAE \(\) & **0.0001** & **0.0022** & **0.0130** & **0.0749** & **0.1587** & **0.0651** & **0.0977** & **0.5044** \\   

Table 1: Results (averaged over \(5\) runs) on the four-digit MNIST sorting benchmark.

    &  &  &  \\   & & \(2 2\) & \(3 3\) & \(4 4\) & \(5 5\) & \(6 6\) & \(2 2\) & \(3 3\) & \(4 4\) \\    & Kendall-Tau \(\) & 0.9984 & 0.6908 & 0.3578 & 0.2430 & 0.1755 & 0.8378 & 0.5044 & **0.016** \\  & Accuracy (\%) & 99.81 & 44.65 & 00.86 & 0.00 & 0.00 & 76.54 & 6.07 & 0.21 \\   & Correct (\%) & 99.91 & 80.20 & 49.51 & 26.94 & 14.91 & 86.10 & 43.59 & 25.31 \\   & RMSE \(\) & **0.0022** & 0.1704 & 0.4572 & 0.8915 & 14.91 & 86.10 & 43.59 & 25.31 \\   & MAE \(\) & 0.0003 & 0.0233 & 0.1005 & 0.3239 & 0.4515 & 0.1368 & 0.5320 & 0.6873 \\   & Kendall-Tau \(\) & 0.9931 & 0.3054 & 0.0374 & 0.0176 & 0.0095 & 0.6463 & 0.1460 & 0.0490 \\   & Accuracy (\%) & 99.02 & 5.56 & 0.00 & 0.00 & 0.00 & 59.18 & 0.96 & 0.00 \\   & Correct (\%) & 99.50 & 42.25 & 10.77 & 6.39 & 3.77 & 5.48 & 27.87 & 12.27 \\   & RMSE \(\) & 0.0689 & 1.0746 & 1.3290 & 14.883 & 1.5478 & 0.7389 & 1.2691 & 1.3876 \\   & MAE \(\) & 0.0030 & 0.4283 & 0.6531 & 0.8204challenging metric to improve, we selecte GPL and BS for all remaining experiments. More ablation study (_e.g_., denoising schedule) is provided in Appendix E.2.

**Full Results.** From Table 2, we can see that Error-free DiffSort achieves the best performance in sorting sequences with lengths up to 32. However, its performances drop significantly with long sequences (_e.g_., length of 52 or 100). Meanwhile, DiffSort performs the worse due to the error accumulation of its soft differentiable swap function [20; 33]. In contrast, our method is on par with Error-free DiffSort in sorting short sequences and significantly outperforms others on long sequences.

### Jigsaw Puzzle

We then explore image reassembly from segmented "jigsaw" puzzles [29; 31; 39]. We evaluate the performance using the MNIST and the CIFAR10 datasets, which comprises puzzles of up to \(6 6\) and \(4 4\) pieces respectively. We add slight noise to pieces from the MNIST dataset to ensure background pieces are distinctive. To evaluate our models, we use Kendall-Tau coefficient, accuracy, correctness, RMSE (root mean square error of reassembled images), and MAE (mean absolute error) as metrics.

Table 1 presents results comparing our method with the Gumbel-Sinkhorn Network, Diffsort , and Error-free DiffSort . DiffSort and Error-free DiffSort are primarily designed for sorting high-dimensional ordinal data which have clearly different patterns. Since jigsaw puzzles on MNIST and CIFAR10 contain pieces that are visually similar, these methods do not perform well. The Gumbel-Sinkhorn performs better for tasks involving fewer than \(4 4\) pieces. In more challenging scenarios (_e.g_., \(5 5\) and \(6 6\)), our method significantly outperforms all competitors.

### The Travelling Salesman Problem

At last, we explore the travelling salesman problem (TSP) to demonstrate the general applicability of our model. TSPs are classical NP-complete combinatorial optimization problems which are solved using integer programming or heuristic solvers [2; 12]. There exists a vast literature on learning-based models to solve TSPs [22; 23; 18; 17; 6; 24; 10; 36; 21; 43; 30]. They often focus on the Euclidean TSPs, which are formulated as follows. Let \(V=\{v_{1},,v_{n}\}\) be points in \(^{2}\). We need to find some \( S_{n}\) such that \(_{i=1}^{n}\|v_{(i)}-v_{(i+1)}\|_{2}\) is minimized, where we let \((n+1):=(1)\). Further experimental details are provided in Appendix B.

We compare with operations research (OR) solvers and other learning based approaches on TSP instances with 20 nodes. The metrics are the total tour length and the optimality gap. Given the ground truth (GT) length produced by the best OR solver, the optimality gap is given by \(-()/()\). As shown in Table 4, SymmetricDiffusers achieves comparable results with both OR solvers and the state-of-the-art learning-based methods.

## 5 Conclusion

In this paper, we introduce a novel discrete diffusion model over finite symmetric groups. We identify the riffle shuffle as an effective forward transition and provide empirical rules for selecting the diffusion length. Additionally, we propose a generalized PL distribution for the reverse transition, which is provably more expressive than the PL distribution. We further introduce a theoretically grounded "denoising schedule" to improve sampling and learning efficiency. Extensive experiments verify the effectiveness of our proposed model. In the future, we are interested in generalizing our model to general finite groups and exploring diffusion models on Lie groups.

    &  &  \\   & Gurobi  & Concorde  & LKH-3  & 2-Opt  & GCN*  & DIFUSCO*  & Ours \\  Tour Length \(\) & **3.842** & 3.843 & **3.842** & 4.020 & 3.850 & 3.883 & **3.849** \\ Optimality Gap (\%) \(\) & 0.00 & 0.00 & 0.00 & 4.64 & 0.21 & 1.07 & 0.18 \\   

Table 4: Results on TSP-20. * means we remove the post-processing heuristics for a fair comparison.

    & GPL + BS & GPL + Greedy & PL + Greedy & PL + BS & IRS + Greedy \\  Kendall-Tau \(\) & 0.786 & **0.799** & **0.799** & 0.797 & 0.390 \\ Accuracy (\%) & **27.4** & 24.4 & 26.4 & 26.4 & 0.6 \\ Correct (\%) & 82.1 & 81.6 & **83.3** & 83.1 & 44.6 \\   

Table 3: Ablation study on transitions of reverse diffusion and decoding strategies. Results are averaged over three runs on sorting 52 four-digit MNIST images.