# DePLM: Denoising Protein Language Models for

Property Optimization

 Zeyuan Wang1,2 Keyan Ding2 Ming Qin1,2 Xiaotong Li1,2 Xiang Zhuang1,2

Yu Zhao4 Jianhua Yao4 Qiang Zhang3,2 Huajun Chen1,2

Corresponding author.1College of Computer Science and Technology, Zhejiang University

2ZIJU-Hangzhou Global Scientific and Technological Innovation Center

3The ZJU-UIUC Institute, International Campus, Zhejiang University

4Tencent AI Lab, Tencent

{yuanzew,dingkeyan,qinandming,3190104904}@zju.edu.cn

yu.zhao@tum.de jianhua.yao@gmail.com

{zhuangxiang,qiang.zhang.cs,huajunsir}@zju.edu.cn

###### Abstract

Protein optimization is a fundamental biological task aimed at enhancing the performance of proteins by modifying their sequences. Computational methods primarily rely on evolutionary information (EI) encoded by protein language models (PLMs) to predict fitness landscape for optimization. However, these methods suffer from a few limitations. (1) Evolutionary processes involve the simultaneous consideration of multiple functional properties, often overshadowing the specific property of interest. (2) Measurements of these properties tend to be tailored to experimental conditions, leading to reduced generalizability of trained models to novel proteins. To address these limitations, we introduce Denoising Protein Language Models (DePLM), a novel approach that refines the evolutionary information embodied in PLMs for improved protein optimization. Specifically, we conceptualize EI as comprising both property-relevant and irrelevant information, with the latter acting as "noise" for the optimization task at hand. Our approach involves denoising this EI in PLMs through a diffusion process conducted in the rank space of property values, thereby enhancing model generalization and ensuring dataset-agnostic learning. Extensive experimental results have demonstrated that DePLM not only surpasses the state-of-the-art in mutation effect prediction but also exhibits strong generalization capabilities for novel proteins.

## 1 Introduction

Proteins play vital roles in numerous biological processes, shaping their structure and function over billions of years of evolution. This evolutionary diversity presents significant opportunities for advancing fields such as drug discovery and materials science . However, the inherent properties of existing proteins, such as thermostability, often fall short of practical requirements in various scenarios. Consequently, researchers have dedicated themselves to optimizing proteins to enhance their properties of interest. Protein optimization is the task that involves modifying protein sequences and efficiently identifying well-performing proteins.

Traditional deep mutational scans (DMS) and directed evolution (DE) rely on expensive wet-lab experiments . Recently, computational approaches that accurately model the relationship between proteins and their property fitness, often termed a "fitness landscape" , have becomecrucial for efficient protein optimization. These approaches, powered by machine learning, enable rapid evaluation of mutation effects and are pivotal for efficient protein optimization.

One widely explored avenue involves leveraging the "Evolutionary Information" (EI), which can be instantiated by the likelihood of an amino acid appearing at a certain position of protein sequences, to infer the mutational effects [41; 49; 16]. This stems from the observation that as organisms evolve through natural selection, mutations that improve functional properties become more prevalent. Therefore, the likelihood of a mutation occurring is directly linked to its impact on biological function . To compute the informative likelihoods of mutating one amino acid to another, the predominant methods involve protein language models (PLMs) trained on millions of protein sequences, which capture the EI in a self-supervised manner [50; 43; 39]. Thanks to their strong generalization capabilities, PLMs have been utilized to guide the artificial selection of beneficial mutations with notable efficacy .

With the advent of high-throughput experimentation, large-scale and diverse annotated datasets for DMS are becoming increasingly accessible . Consequently, researchers have extended the use of self-supervised EI into supervised prediction settings [18; 17; 23; 21; 11; 65]. Specifically, they usually fine-tune PLMs on experimentally annotated datasets, with the objective of minimizing the disparities between predicted and experimental fitness values [8; 2; 46]. However, two critical aspects are often overlooked. Firstly, it fails to account for the removal of irrelevant EI. Evolution optimizes multiple properties simultaneously to meet survival needs, often overshadowing the optimization target of interest . Therefore, conventional fine-tuning methods using the whole evolutionary information are suboptimal. Secondly, the prevalent learning objective incorporates dataset-specific information that is often overfitted to the training data at hand, hindering the model's ability to generalize toward new proteins. This limitation is significant since DMS experimental techniques often encounter constraints regarding their applicability across a wide range of proteins .

In this work, we introduce a novel **D**enoising **P**rotein **L**anguage **M**odel (DePLM) tailored for protein fitness prediction. _The central concept revolves around perceiving the EI captured by PLMs as a blend of property-relevant and irrelevant information, with the latter akin to "noise" for the targeted property, necessitating its elimination._ To achieve this, drawing inspiration from denoising diffusion models that refine noisy inputs to generate desired outputs [55; 10; 29], we devise a rank-based forward process to extend the diffusion model for denoising EI, as illustrated in Figure 1. Specifically, we refine the likelihood of mutations provided by PLMs. To parameterize this framework, we initially extract protein representations considering both primary and tertiary structures. Subsequently, we utilize this representation to guide the denoising process. In pursuit of dataset-agnostic learning and robust model generalization, we conduct the diffusion process in the rank space of property values and replace the conventional objective of minimizing numerical errors with maximizing rank correlation. Extensive experiments have demonstrated that the introduced rank-based denoising process significantly improves the protein fitness prediction performance, and simultaneously maintains strong generalization ability for novel proteins. Our contributions can be summarized as follows:

* We introduce DePLM, a novel approach for refining evolutionary information captured by protein language models to predict mutation effects, effectively filtering out irrelevant information to improve mutation effect predictions.

Figure 1: Comparison of fitness landscape prediction methods. WT: wildtype sequence. MTs: mutant sequences. A2B: Amino acid A in wildtype mutates to B. GT.: groundtruth. Corr.: Correlation.

* We design a rank-based forward process within the denoising diffusion framework, extending the diffusion process to the rank space of mutation likelihoods.
* We shift the learning objective from minimizing numerical errors to maximizing rank correlation, fostering dataset-agnostic learning and ensuring robust generalization.
* DePLM significantly outperforms state-of-the-art models for mutational effect prediction and demonstrates strong potential for optimizing unseen proteins.

## 2 Background and Related Works

### Task Formulation and Evaluation

Protein optimization seeks to elucidate the impacts of sequence mutations on protein properties. Formally, given a widotype protein sequence \(^{}=[x_{1}^{},,x_{n}^{},,x_{N}^ {}]\) with \(N\) amino acids, a mutation \(=\{_{n}:x_{n}^{} x_{n}^{},n[\![1,N]\!]\}\) refers to the substitution of the amino acid \(x_{n}^{}\) at certain positions \(n\) with another amino acid \(x_{n}^{}\). Note that a mutation can affect multiple positions simultaneously. The task of protein optimization is mathematically formulated as learning a function \(_{}\), parameterized by \(\): \(_{}(^{},)=y,\) where \(y\) denotes the impact of the mutation \(\) on the wildtype sequence \(^{}\), i.e., the property value of the mutated protein sequence. With numerous \((,y)\) pairs, one can approximate the fitness landscape of the protein \(^{}\) for the target property.

To evaluate the consistency between predicted and ground-truth fitness landscape, one often uses Spearman's rank correlation coefficient, which prioritizes relative rankings over absolute values . Specifically, given the set of predictions \(Y=\{y\}\) and ground-truth \(Y^{*}=\{y^{*}\}\), both sets are first converted to their respective ranks \(R(Y)\) and \(R(Y^{*})\), then the coefficient \(\) is calculated as

\[=(R(Y),R(Y^{*}))}{_{R(Y)}_{R(Y^{*})}},\] (1)

where \((,)\) is the covariance of the ranked variables, \(\) is the standard deviations of these ranks.

### Related Works

**Self-supervised modeling of sequence mutation effects.** Evolutionary information provides valuable insights into how sequence mutations affect biological functions . A straightforward approach to capture EI is through Multiple Sequence Alignments (MSAs). For example, the SIFT model  predicted the mutation effect by performing position-specific statistical analysis of aligned sequences. Riesselman et al.  and Frazer et al.  employed variational autoencoder trained on protein-specific MSAs to detect patterns of interaction among positions, achieving higher prediction performance. However, the MSA approach is limited by its protein-specific nature, rendering it less effective for proteins, such as orphan proteins, that lack sufficient homologous sequences.

Instead, many studies [2; 40; 50; 39; 4; 37] explored PLMs trained on evolutionary-scale data to capture EI, which can generalize beyond specific proteins. Further, several studies aim to merge PLMs with MSAs to harness the benefits of both approaches. Rao et al.  proposed MSA Transformer to apply language modeling to aligned sequences, and PoET  modeled the distribution over protein families rather than sequences. Tan et al.  suggested that mutation effects are related to their structural context, and Notin et al.  showed that the likelihood derived from structure-based design models [28; 9] can be complementary to those generated by PLMs. Despite the utility of EI, the information captured by PLMs is often entangled with multiple protein properties, leading to suboptimal optimization of the target property when used directly.

**Supervised modeling of fitness landscape.** The utility of EI extends into supervised prediction scenarios [18; 17; 23; 21]. Dallago et al.  demonstrated that straightforward fine-tuning of the representations from PLMs holds the potential to predict fitness. Hie et al.  used PLMs to predict the evolutionary trajectory of protein families. Yang et al.  advocated for using machine learning tools for small datasets and neural networks for larger datasets. In response to sparse labels, Elnaggar et al.  proposed a lightweight parametric model called ConvBERT to avoid overfitting. ProteinNPT  leveraged a non-parametric Transformer that combines masked language modeling and fitness prediction tasks, demonstrating excellence in low-resource scenarios. However, these methods often lose the generalization capabilities of PLMs after dataset-specific fine-tuning. Inthis study, we show that combining EI with experimental data in a way that minimizes reliance on dataset-specific information is crucial for enhancing performance while maintaining generalization.

**Denoising diffusion models.** Several diffusion models have been applied to protein research. Diffusion processes in discrete token spaces have been promising for designing protein sequences [3; 1; 22]. Jing et al.  focused on torsion for generating conformations, while Corso et al.  studied the product space \((3)(2)^{m}\) for protein docking. Additionally, backbone generation requires defining \((3)\) diffusion process [66; 61; 30]. In contrast to these works, we operate the diffusion process in the rank space of mutation likelihoods to predict the fitness landscape.

## 3 Method

### Overall Framework

DePLM, as depicted in Figure 2, is designed to filter out irrelevant information from the noisy evolutionary likelihoods produced by PLMs. Given a wildtype protein \(^{}\), the evolutionary likelihood produced by a PLM can be denoted as \(}=[}_{1},,}_{n},, }_{N}]\), where \(}_{n}^{20}\) denotes the probability of 20 amino acids occurring at the position \(n\). This likelihood can be decomposed into the target property likelihood \(^{}\) and additive noise \(^{}\) introduced by irrelevant properties, such that \(}=^{}+^{}\). The DePLM takes the noisy likelihood \(}\) as input and refines it to isolate the desired likelihood \(^{}\) via a rank-based denoising diffusion process. A comprehensive explanation of the symbols and operations utilized is available in Appendix A.

### Rank-based Denosing Diffusion Process

Denoising diffusion models consist of two main processes: a forward corruption process and a learned reverse denoising process. In the forward corruption process, small amounts of noise are progressively added to the ground truth. The reverse denoising process then learns to recover the ground truth by gradually eliminating the accumulated noise. When applying these models to denoise the mutation likelihood \(}\) in protein optimization, however, there are two significant challenges. First, the relationship between actual property values and experimental measurements frequently exhibits nonlinearity, stemming from the diversity of experimental approaches . Consequently, reliance on minimizing discrepancies between predicted and observed values for denoising purposes risks overfitting to the specific dataset utilized, thereby diminishing the model's generalization capabilities. Second, unlike those conventional denoising diffusion models, our final noisy state \(}\) is deterministic, requiring the accumulated noise to converge .

Figure 2: The architecture overview of DePLM. **Left**: DePLM utilizes evolutionary likelihoods derived from PLMs as input, and generates denoised likelihoods tailored to specific properties for predicting the effects of mutations. **Middle & Right**: Denoising module utilizes a feature encoder to derive representations of proteins, taking into account both primary and tertiary structures. These representations are then employed to filter out noise from the likelihoods via denoising blocks.

To navigate these challenges, we propose a rank-based denoising diffusion process that focuses on maximizing rank correlation (see Figure 3). Let \(_{0}=R(^{})_{+}^{20 N}\) be the rank of ground-truth likelihood of the target property, \(_{T}=R(})_{+}^{20 N}\) be the rank of noising evolutionary likelihood. Intermediate rank variables, \(_{t}+^{20 N}\) for \(t=1,,T-1\), are generated along this sequence.

Forward process.The forward process operates in the rank space, gradually transitioning from the initial rank \(_{0}\) through increasingly chaotic states \(_{t}\), culminating in the final state \(_{T}\) after \(T\) steps. Unlike traditional models where corruption can be random, the corruption process here must carefully manage the progression because the initial state \(_{0}\) and the final state \(_{T}\) are pre-defined.

To manage this transition, we leverage the QuickSort algorithm  to create a feasible space for sampling intermediate rank variables \(_{t}\). At each time step \(t\), we apply the sort algorithm to generate sorting trajectories from the variable \(_{t-1}\) towards the end variable \(_{T}\). The rank variable \(_{t}\) chosen along these trajectories ensures a progressive decrease in rank correlation from the initial to the intermediate states, and converse increase towards the final state. This approach ensures a controlled and meaningful progression through the rank space, as detailed in Appendix B.1. The forward process can be conceptualized as a Markov chain:

\[q(_{1:T-1}|_{0},_{T})=_{t=1}^{T-1}q(_{t}|_{ t-1},_{T}).\] (2)

Backward process.Given that the rank variables are non-differential, the backward process operates in the likelihood space rather than the rank space. This process constructs the likelihood \(_{0}\) from the noisy evolutionary likelihood \(_{T}=}\), ensuring that \(R(_{0})\) equals \(R(^{})\). Importantly, while \(_{0}\) and \(^{}\) are aligned in rank, they do not need to be identical in value. The key focus is on the rank variables, as the relative rankings are more critical for optimizing protein performance than the exact numerical values of the likelihoods. To facilitate this process, we employ the protein \(^{}\) as a guiding signal, and model it as a conditional Markov chain with learned transitions:

\[p_{}(R(_{0:T-1})|^{},_{T}) p_{ }(_{0:T-1}|^{},_{T})=_{t=1}^{T}p_{ }(_{t-1}|^{},_{t}),\] (3)

where \(p_{}(_{t-1}|^{},_{t})\) is the learnable transition kernel, \(\) denotes its parameters, and \(_{t}\) is the intermediate likelihood variables during the backward process, with \(r_{t}=R(_{t})\).

Learning objective functionThe forward process introduces noise in rank space, while the reverse process denoises in likelihood space. To effectively link these two processes, we leverage Spearman's

Figure 3: The training process of DePLM. **Left**: The training of DePLM involves two main steps: rank-based controlled forward corruption and learned denoising backward processes. In the corruption step, we use sorting algorithms to generate trajectories, shifting from the rank of property-specific likelihood to that of the evolutionary likelihood. DePLM is trained to model the backward process. **Right**: We illustrate the alteration of the Spearman coefficient during the transformation from evolutionary likelihood to property-specific likelihood via the sorting algorithm.

rank correlation \(\) defined in Eq. (1). Building upon this, we modify the variational Evidence Lower BOund (ELBO) as our learning objective function:

\[[ p_{}(_{0:T}|^{})] =[_{q(_{1:T-1}|R(^{ }),R())}(_{0:T}|^{})}{q(_{1:T-1}|R(^{}),R())}]\] \[-_{q}[_{t=1}^{T}(1-(q(_{t-1}|R(_{0}),R(_{t}))||p_{}(R(_{t-1})|^ {},_{t})))].\] (4)

Detailed derivations are provided in the Appendix B.2. Overall, given a protein \(^{}\), its property likelihood \(^{}\) is generated by first drawing evolutionary likelihood \(\) from PLMs, and then iteratively refined through \(p_{}(_{t-1}|^{},_{t})\). We predict \(y\) according to Eq. 10 in Appendix C.1.

### Implementation of Denoising Markov Kernel

To parameterize the transition kernel \(p_{}(_{t-1}|^{},_{t})\), as shown in the Denoising Module of Figure 2, DePLM first learns a protein representation (Feature Encoder of Figure 2), and then utilizes it to guide the process of discerning and eliminating noise (Denoising Block of Figure 2).

**Feature encoder**. We encode features from both sequences and structures because they complementarily describe the impact of mutations. Sequence information is derived from representations \(_{e}\) and attention weights \(^{N N m}\) generated by PLMs, where \(m\) is the head number. Meanwhile, structural information is obtained through trained structure encoders like ESM-IF , which process the protein backbone to produce structural representations \(_{s}\). We merge the two sets of representation using Multi-Layer Perceptrons (MLPs), yielding a unified representation \(\) in \(^{N d}\), where \(d\) represents the hidden dimension.

The feature encoder leverages multiple stacked layers to update \(\) and \(\). Let's denote the output feature vectors of representations and attention weights in layer \(l\) as \(^{l}\) and \(^{l}\), respectively. Initially, \(^{0}\) equals \(\) and \(^{0}\) equals \(\). We update the representations using the standard attention mechanism, incorporating a bias derived from the attention weights:

\[^{l+1}=((^{l}(^{l })^{T}+^{l}_{}}{})^{l}),\] (5)

where \(^{l}\), \(^{l}\), and \(^{l}\) are the linear projection of \(^{l}\) and \(^{l}_{}\) is the linear projection of \(^{l}\). Then, the attention weights are updated by communicating with the sequence representation through both an outer product \(\) and an outer difference \(\):

\[^{l+1}_{}=(^{l+1}^{l+1})||(^{l+1} ^{l+1}),\] (6)

\[^{l+1}=(^{l}+^{l+1}_{}),\] (7)

where \(||\) means the concatenation operation. The final layer output \(^{L}\) is used in the denoising block.

**Denoising block**. Given the intermediate likelihood variable at the \(t\) step \(_{t}\), we employ a denoising block to implement Eq. (3). The central premise is that \(^{L}\) should encapsulate only property-specific protein information. By subtracting \(^{L}\) from the hidden representation of the noisy \(_{t}\), we isolate the hidden representation of the noise. This hidden representation is then transformed into the likelihood space using a PLM head (PLMHead),

\[_{t-1}=_{t}-((_{t})-^{L}).\] (8)

Here \((_{t})\) denotes a trainable MLP that maps \(_{t}\) to the hidden representation space, while \(\) maps hidden presentations back to the likelihood space. It is important to use a frozen \(\) to ensure consistency between noise and noisy likelihoods in a unified space. This approach effectively denoises \(_{t}\) to obtain \(_{t-1}\).

## 4 Experiments

In this section, we extensively evaluate DePLM across various datasets and demonstrate its superior performance and robust generalization capabilities. Specifically, we aim to address the following key questions. Performance comparison (Q1): Can DePLM beat SOTA on protein fitness prediction tasks? Generalization ability (Q2): Does DePLM maintain its generalization ability post-training? Ablation study (Q3): What is the extent of improvement achievable for each component? Analysis (Q4): Does the assumption that EI contains noise hold?

### Experimental Setup

We begin by outlining the general experimental setups used in our evaluations. We use ESM-IF  as the structure encoder and the structures are predicted by AlphaFold2 . DePLM comprises 42.2 million trainable parameters and involves 3 diffusion steps. We set the learning rate at 0.0001, with a weight decay of 0.005, utilizing AdamW as the optimizer. All models are trained on four Nvidia V100 32G GPUs for up to 100 epochs by default.

### Performance Comparison (Q1)

**Datasets and Baselines.** We conducted a thorough study across four benchmarks, including ProteinGym , \(\)-Lactamase (Abbr., \(\)-lact.) and Fluorescence (Abbr., Fluo.) from PEER , and GB1 (utilizing a _2-vs-rest_ split) from FLIP , where the latter two involve multiple mutants. We compare DePLM with nine baselines, including 1) four protein sequence encoders trained from scratch (CNN , ResNet , LSTM , and Transformer ) as naive baselines, 2) five extended baselines that incorporate self-supervised models (OHE , fine-tuned versions of ESM-1v , ESM-MSA , and Tranception , as well as ProteinNPT ). More details about datasets and baselines can be found in Appendix C.1 and C.2.

**Results.** We present the evaluation results in Table 9. DePLM achieves better performance compared to the baselines, affirming the advantage of integrating evolutionary information with experimental data for protein engineering tasks. It is worth noting that ESM-MSA and Tranception exhibit enhanced EI compared to ESM-1v due to the introduction of MSAs. By comparing their results, we demonstrate that higher-quality EI significantly improves outcomes after fine-tuning. However, even with these improvements, their performance still falls short of that achieved by DePLM. We attribute this difference to the architecture employed by our model, which enables more efficient utilization of experimental data. We also notice that DePLM yields better performance than ProteinNPT, underscoring the efficacy of the proposed denoising training process.

### Generalization Ability (Q2)

**Datasets and Baselines.** Computational techniques that can generalize across different proteins are essential, given the limitations of DMS experimental methods in handling various proteins . In our study, we leverage ProteinGym to test this generalization capability. Specifically, ProteinGym categorizes DMS datasets into five coarse categories based on the protein properties they measure: stability, fitness, expression, binding, and activity. Given a testing dataset, we randomly select an additional 40 datasets from the same category for training. Importantly, we ensure that the sequence similarity between the datasets used for training and testing is kept below 50% to prevent potential data leakage. In this experiment, we compare DePLM with four self-supervised baselines (ESM-1v , ESM-2 , and TranceptEVE ), two structure-based baselines (ESM-IF 

    &  &  &  &  \\    & Stability & Fitness & & & & & & & \\  CNN & 0.788 & 0.588 & 0.627 & 0.599 & 0.573 & 0.781 & 0.502 & **0.682** \\ ResNet & 0.734 & 0.489 & 0.521 & 0.525 & 0.481 & 0.152 & 0.133 & 0.636 \\ LSTM & 0.745 & 0.413 & 0.477 & 0.496 & 0.408 & 0.139 & \(-\)0.002 & 0.494 \\ Transformer & 0.560 & 0.149 & 0.156 & 0.172 & 0.155 & 0.261 & 0.271 & 0.643 \\  OHE & 0.718 & 0.545 & 0.573 & 0.562 & 0.555 & 0.823 & 0.533 & 0.657 \\ ESM-1v & 0.880 & 0.566 & 0.642 & 0.596 & 0.572 & 0.536 & 0.394 & 0.438 \\ ESM-2 & 0.882 & 0.573 & 0.645 & 0.587 & 0.576 & - & - & - \\ ESM-MSA & 0.885 & 0.568 & 0.632 & 0.565 & 0.600 & - & - & - \\ ProSSN & 0.877 & 0.692 & 0.718 & 0.757 & 0.678 & - & - & - \\ SaPrt & 0.882 & 0.686 & 0.716 & 0.749 & 0.677 & - & - & - \\ Tranception & 0.871 & 0.632 & 0.704 & 0.671 & 0.623 & - & - & - \\ ProteinNPT & **0.904** & 0.668 & 0.736 & 0.706 & 0.680 & - & - & - \\  DePLM (ESM1v) & 0.887 & 0.704 & 0.738 & **0.773** & 0.688 & 0.900 & **0.676** & 0.662 \\ DePLM (ESM2) & 0.897 & **0.707** & **0.742** & 0.764 & **0.693** & **0.904** & 0.665 & 0.662 \\   

Table 1: Model performance on protein engineering tasks. The **best** and suboptimal results are labeled with bold and underline, respectively. ProteinGym results of OHE, ESM-MSA, Tranception, and ProteinNPT are borrowed from Notin et al. . Other results are obtained by our own experiments.

and ProteinMPNN ), and three supervised baselines (CNN , fine-tuned version of ESM-1v and ESM-2). The choice of baselines is elaborated in Appendix C.1.

**Results.** As shown in Table 2, one can observe that DePLM consistently outperforms all baseline models. This finding underscores the inadequacy of baselines that rely solely on unfiltered evolutionary information, which often dilutes target properties due to concurrent optimization of multiple objectives. By eliminating the influence of irrelevant factors, DePLM enhances its performance significantly. In addition, the performance of baselines trained to minimize the disparity between predicted and experimental scores (ESM1v (FT) and ESM2 (FT)) falls significantly short of that achieved by our DePLM. This observation highlights that optimizing the model in a rank space introduces less bias from specific datasets and yields superior generalization. Furthermore, we observe that protein structure information contributes to the stability and binding properties, whereas evolutionary information enhances fitness and activity attributes.

### Ablation Study (Q3)

The ablation study aims to validate the efficacy of the modules devised for DePLM. These modules include the use of structural information, the feature encoder, the denoising module, and the rank objective. The results in Table 8 show the absence of any of these modules leads to a decline in performance, indicating their collective significance. Additionally, we observe that the rank objective has the largest impact on performance, highlighting the importance of reducing dataset-specific information. Further ablation studies are detailed in Appendix C.4.

### Analysis and Discussion (Q4)

Necessity of Filtering Property-Irrelevant Information in EI.To ascertain the importance of filtering out property-irrelevant information, we analyzed the impact of training with datasets targeting various optimization objectives. As illustrated in Figure 4 (left), we observe that using training datasets with characteristics divergent from those of the test dataset leads to diminished performance across stability, expression, and activity properties. This reduction in performance

    &  &  \\   & Evo. & Struct. & Exp. & Stability & Fitness & Expression & Binding & Activity \\  ESM1v & ✓ & & & 0.437 & 0.395 & 0.427 & 0.287 & 0.415 \\ ESM2 & ✓ & & & 0.523 & 0.396 & 0.439 & 0.356 & 0.433 \\ ProtSSN & ✓ & ✓ & & 0.560 & 0.408 & 0.435 & 0.362 & 0.458 \\ TrancepPEVE L & ✓ & & & 0.500 & 0.477 & 0.457 & 0.360 & 0.487 \\  ESM-IF & & ✓ & & 0.624 & 0.346 & 0.436 & 0.380 & 0.412 \\ ProteinMPNN & & ✓ & & 0.564 & 0.166 & 0.209 & 0.159 & 0.203 \\  CNN & & & ✓ & 0.141 & 0.053 & 0.043 & 0.056 & 0.095 \\ ESM1v (FT) & ✓ & & ✓ & 0.497 & 0.318 & 0.301 & 0.216 & 0.385 \\ ESM2 (FT) & ✓ & & ✓ & 0.454 & 0.359 & 0.338 & 0.276 & 0.391 \\ ProtSSN (FT) & ✓ & ✓ & ✓ & 0.689 & 0.448 & 0.478 & 0.421 & 0.488 \\ SaProt (FT) & ✓ & ✓ & ✓ & 0.703 & 0.442 & 0.496 & 0.391 & 0.495 \\  DePLM (ESM1v) & ✓ & ✓ & ✓ & 0.763 & 0.467 & 0.506 & 0.409 & 0.499 \\ DePLM (ESM2) & ✓ & ✓ & ✓ & **0.773** & **0.480** & **0.510** & **0.441** & **0.518** \\   

Table 2: Generalization ability evaluation. The **best** and **suboptimal** results are labeled with bold and underline, respectively. The information (evolutionary, structural or experimental) involved in each model is provided. Results of unsupervised methods are borrowed from Notin et al.. Other results are obtained by our own experiments. (FT=Fine-tuned version)

   Method & ProteinGym (Valid) & GB1 & Fluorescence & Average \\  DePLM & **0.690** & 0.665 & **0.662** & **0.672** \\  w/o structural information & 0.683 & **0.672** & 0.659 & 0.671 \\ w/o feature encoder & 0.682 & 0.659 & 0.661 & 0.667 \\ w/o denoising block & 0.656 & 0.644 & 0.655 & 0.652 \\ w/o rank objective & 0.322 & 0.588 & 0.552 & 0.487 \\   

Table 3: Ablation study of the modules in DePLM.

underscores the detrimental interference among different properties, emphasizing the need to mitigate these adverse effects. Interestingly, we observed that training on datasets targeting alternative properties can improve performance in the binding property. This improvement is likely due to the scarce availability of datasets specifically focusing on the binding property within ProteinGym. This observation suggests a beneficial cross-utilization of data, where leveraging information from unrelated attributes can enhance inference for properties with limited data. In Figure 4 (right), we illustrate the influence of dataset size on performance. We found that even with a minimal number of datasets (K=2), DePLM significantly boosts performance, indicating its proficiency in filtering out irrelevant information. Furthermore, as the number of training datasets increases, there is a corresponding improvement in performance, showcasing the model's capability to continuously enhance its filtering efficacy with more data.

Differentiating the Noisy Evolutionary Likelihood from the Property-specific Likelihood.De-noising the evolutionary likelihood is helpful in identifying protein sequences that manifest specific properties. In Figure 5 (left), we utilize entropy to gauge the degree of conservation at each position and illustrate the likelihood attributed to hydrophobic amino acids within the structure. Residues with outward-facing side chains on alpha helices are associated with higher entropy, whereas inward-facing positions exhibit lower entropy. Figure 5 (right) compares the evolutionary likelihood and the property-specific likelihood and visualizes the differences. We notice that binding and active share similar offsets. This suggests that the indole-3-glycerolphosphate synthase functions by attaching to other molecules, which makes sense considering its role as an enzyme . We also observe that the property-specific likelihoods exhibit a more uniform entropy when compared to the noisy evolutionary likelihood. We assume this may arise due to a bias towards inward-facing positions across all properties, potentially overshadowing the evolutionary significance of outward-facing side chains. This imbalance is addressed through the denoising process described in Section 3.2.

Figure 4: Visualization of the impact of optimization targets and size of training data on performance.

Figure 5: Visualization of the impact of denoising process on the evolutionary likelihood.

Conclusion and Limitations

In this paper, we propose DePLM, a simple yet effective fine-tuning approach that leverages a feature encoder to obtain expressive protein representations and then uses them to extract property-specific likelihood from the noisy evolutionary likelihood for mutational effect prediction. Our experiments demonstrate that DePLM not only surpasses state-of-the-art baselines but also shows exceptional generalization capabilities. Additionally, our analysis confirms that utilizing sufficiently large datasets or incorporating data from other relevant properties can significantly enhance performance.

Due to limited resources, our experiments are conducted using wild-type marginal probability. This approach predicts the impacts of all mutations in a single forward pass, summing the effects of individual mutations to estimate the consequences of multiple mutations simultaneously. However, this method is not ideal, as it overlooks the complex interactions between mutations. Our method can potentially achieve better performance in predicting the effects of multiple mutations, leveraging more effective prediction techniques such as masked marginal probability.