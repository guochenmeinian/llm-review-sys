# On Convergence of Polynomial Approximations to the Gaussian Mixture Entropy

Caleb Dahlke

Department of Mathematics

University of Arizona

&Jason Pacheco

Department of Computer Science

University of Arizona

###### Abstract

Gaussian mixture models (GMMs) are fundamental to machine learning due to their flexibility as approximating densities. However, uncertainty quantification of GMMs remains a challenge as differential entropy lacks a closed form. This paper explores polynomial approximations, specifically Taylor and Legendre, to the GMM entropy from a theoretical and practical perspective. We provide new analysis of a widely used approach due to Huber et al. (2008) and show that the series diverges under simple conditions. Motivated by this divergence we provide a novel Taylor series that is provably convergent to the true entropy of any GMM. We demonstrate a method for selecting a center such that the series converges from below, providing a lower bound on GMM entropy. Furthermore, we demonstrate that orthogonal polynomial series result in more accurate polynomial approximations. Experimental validation supports our theoretical results while showing that our method is comparable in computation to Huber et al. We also show that in application, the use of these polynomial approximations, such as in Nonparametric Variational Inference, rely on the convergence of the methods in computing accurate approximations. This work contributes useful analysis to existing methods while introducing novel approximations supported by firm theoretical guarantees.

## 1 Introduction

Entropy is a natural measure of uncertainty and is fundamental to many information-theoretic quantities such as mutual information (MI) and Kullback-Leibler (KL) divergence . As a result, entropy plays a key role in many problems of ML including model interpretation , feature selection , and representation learning . It is often used in the data acquisition process as in active learning , Bayesian optimal experimental design , and Bayesian optimization . Yet, despite its important role entropy is difficult to calculate in general.

One such case is the Gaussian mixture model (GMM), where entropy lacks a closed form and is the focus of this paper. GMMs are fundamental to machine learning and statistics due to their property as universal density approximators . However, the lack of a closed-form entropy requires approximation, often via Monte Carlo expectation. Such stochastic estimates can be undesirable as computation becomes coupled with sample size and a deterministic approach is often preferred. Simple deterministic bounds can be calculated via Jensen's inequality or Gaussian moment matching . Such bounds are often too loose to be useful, leading to other options such as variational approximations  and neural network-based approximation . Yet, these deterministic estimators do not allow a straightforward tradeoff of computation and accuracy as in the Monte Carlo setting.

Polynomial series approximations are both deterministic and provide a mechanism for computation-accuracy tradeoff by varying the polynomial degree. In this paper we focus on three such polynomial approximations of the GMM entropy. We begin with the widely used approximation of Huber et al. (2008). While this approximation yields good empirical accuracy in many settings, a proof ofconvergence is lacking. In this work we show that the Huber et al. approximation in fact does not converge in general, and we provide a divergence criterion (Theorem 3.1). In response to the divergent behavior, we propose two alternative polynomial approximations, a Taylor and Legendre series approximation of GMM entropy that are provably convergent. We establish in Theorem 4.2 and Theorem 4.5 that each series converges everywhere under conditions on the center point or interval, respectively. In Theorem 4.4 we provide a simple mechanism for choosing a value to ensure that these series converge everywhere. We additionally establish, in Theorem 4.3, that our Taylor approximation is a convergent lower bound on the true entropy for any finite poynomial order.

The complexity of both Huber et al. and our proposed methods have similar computation largely driven by polynomial order. To address this we propose an approximation that estimates the higher-order terms by fitting a polynomial regression. This approach requires the evaluation of only three consecutive polynomial orders to approximate higher order series. In this way we can obtain more accurate estimates without the computational overhead of evaluating higher order polynomial terms.

We conclude with an empirical comparison of all polynomial approximations that produce the divergent behavior of the Huber et al. approximation while our propsed methods maintain convergence. We also compare accuracy and computation time for each method accross a varaitey of dimensions, number of GMM components, and polynomial orders. Finally, we show an application of our methods in Nonparametric Variational Inference  where the guarantees of convergence play a large role in the accuracy of posterior approximation via GMMs.

## 2 Preliminaries

We briefly introduce required notation and concepts, beginning with a definition of the Gaussian mixture entropy. We will highlight the challenges that preclude efficient computation of entropy. We conclude by defining notation that will be used for discussion of polynomial approximations.

### Gaussian Mixture Entropy

The differential entropy of a continuous-valued random vector \(x^{d}\) with a probability density function \(p(x)\) is given by,

\[H(p(x))=- p(x) p(x)dx=[- p(x)].\] (1)

The differential entropy is in \([-,]\) for continuous random variables. It is a measure of uncertainty in the random variable in the sense that its minimum is achieved when there is no uncertainty in the random vector, i.e., a Dirac delta, and approaches the maximum as the density becomes uniformly distributed.

Gaussian mixtures are ubiquitous in statistics and machine learning due to their property as universal density approximators . However, despite this flexibility, the entropy of a Gaussian mixture requires computing the expectation of the log-sum operator, which lacks a closed form. Many approximations and bounds are used in practice. A simple upper bound is given by the entropy of a single Gaussian with the same mean and covariance as the mixture , and a lower bound can be obtained by Jensen's inequality. Though efficient, these bounds are very loose in practice, leading to more accurate methods being used, such as various Monte Carlo approximations, deterministic sampling , and numerous variational bounds and approximations .

### Taylor Polynomials

In this paper we explore entropy approximation using Taylor polynomials. The \(n^{th}\)-order Taylor polynomial of a function \(f(x)\) with evaluation point \(c\) is given by,

\[T_{f,n,c}(x)=_{i=0}^{n}(c)}{n!}(x-c)^{n},\] (2)

where \(f^{(n)}(c)\) denotes the \(n^{}\) derivative of \(f\) evaluated at point \(c\). The Taylor series has a region of convergence which determines the range of x-values where the series accurately represents the original function. It depends on the behavior of the function and its derivatives at the expansion point. Analyzing the region of convergence is crucial for ensuring the validity of the Taylor series approximation. Various convergence tests, such as the ratio test, help determine the x-values where the Taylor series provides an accurate approximation.

### Orthogonal Polynomials

Taylor series are versatile approximations, however predominately behave well near the center point chosen. We ideally would like an approximation that performs well across a range of values. To achieve this, we consider series approximation via orthogonal polynomials. A set of orthogonal polynomials on the range \([a,b]\) is an infinite sequence of polynomials \(P_{0}(x),P_{1}(x),\) where \(P_{n}(x)\) is an \(n^{th}\) degree polynomial and for any pair of polynomials satisfies

\[ P_{i}(x),P_{j}(x)=_{a}^{b}P_{i}(x)P_{j}(x)dx=c_{i}_{ij}\] (3)

where \(_{ij}\) is the Kronecker delta function and \(c_{i}\) is some constant. Orthogonal polynomials can be used to approximate a function, \(f(x)\), on their interval, \([a,b]\), by finding the projection of \(f(x)\) onto each polynomial in the series \(P_{i}(x)\).

\[f(x)=_{i=1}^{}(x)}{ P_{i}(x), P_{i}(x)}P_{i}(x)\] (4)

Any appropriate choice of orthogonal polynomials can be used. One might be interested in considering the Chebyshev polynomials for their property of minimizing interpolation error or Legendre polynomials for their versatility and ease of computation.

## 3 Convergence of Polynomial Approximations

To estimate the entropy \(H(p)=_{p}[-(p(x))]\) using a polynomial approximation one may approximate either the log-density \((p(x))\) or just the logarithm \((y)\). We will show that estimating \((p(x))\) has convergence issues and that it can be complicated to compute due to tensor arithmetic in higher dimensions. Both of these issues will be addressed by simply approximating \((y)\) and computing the exact \(p(x)\). All proofs are deferred to the Appendix for space.

### Divergence of Huber et al. Approximation

We begin our exploration with a widely used approximation of the GMM entropy due to . Let \(p(x)\) be a GMM and the log-GMM \(h(x)=(p(x))\). Huber et al. provides a Taylor series approximation of the GMM entropy given by,

\[(p(x))=-_{i=1}^{M}w_{i}_{n=0}^{}(_{i})}{n!} (x-_{i})^{n},\] (5)

The series is \(M\) individual Taylor series evaluated at each component mean, \(_{i}\). The equality in Eqn. (5) only holds if the series converges, which we will show is not the case in general.

**Theorem 3.1** (Divergence Criterion for Huber et al.).: _Let \(p(x)=_{i=1}^{M}w_{i}(x_{i},_{i})\) and consider the Taylor series presented in Eqn. (5). If any mean component, \(_{i}\), satisfies the condition \(p(_{i})<(p(x))\), then the series diverges, otherwise it converges._

Theorem 3.1 provides us with the condition that Huber et al.'s approximation Eqn. (5) will diverge. This means that the entropy approximation will be inaccurate for any GMM with any of its modes less than half the probability of any other point, as illustrated in Fig. 1.

### Taylor Series Approximation of the Logarithm

Motivated by the divergence of the previous Taylor series we propose a different approach that is provably convergent. While Huber et al. perform a Taylor decomposition of the log-GMM PDF, our approach decomposes only the \((y)\) function using a Taylor series of the function centered about the point \(a\). It is well-known that this series converges for values \(|y-a|<a\) and is given by,

\[(y)=(a)+_{n=1}^{}}{na^{n}}(y-a)^{n}.\] (6)

Note the change of \(c\) to \(a\) as the Taylor series center. This change highlights the difference in function domains. In particular, the former series is computed on values of the random vector \(x\), whereas ours is computed on the PDF \(y=p(x)\). Choosing any center \(a>(p(x))\) will ensure that the series converges everywhere.

**Lemma 3.2** (Convergent Taylor Series of Log).: _If \(a>(p(x))\), then for all \(x\)_

\[(p(x))=(a)+_{n=1}^{}}{na^{n}}_{k=0}^{n} (-a)^{n-k}p(x)^{k}\] (7)

The proof of Lemma 3.2 is a simple ratio test. The only assumption on \(p(x)\) is that it has a finite maximum, which is true for any non-degenerate GMM with positive definite component covariances. As a result, the Taylor series converges for all \(x\) regardless of the GMM form.

### Legendre Series Approximation of the Logarithm

For the orthogonal polynomial approximation, we consider the Legendre polynomials, specifically the shifted Legendre polynomials  which are orthogonal on \([0,a]\),

\[P_{n}(y)=L_{[0,a],n}(y)=_{k=0}^{n}(n+k)!}{(n-k)!(k!)^{2}a ^{k}}y^{k}\] (8)

**Lemma 3.3** (Convergent Legendre Series of Log).: _If \(a>(p(x))\), and consider the shifted Legendre polynomials on the interval \([0,a]\) in Eqn. (8). Then for \(x\) a.e._

\[(p(x))=_{n=0}^{}(2n+1)_{j=0}^{n}(n+j)!((j+1) (a)-1)}{(n-j)!((j+1)!)^{2}}L_{[0,a],n}(p(x))\] (9)

Again, all that is assumed about this approximation is that the max of a GMM can be bounded, so this approximation converges for all GMMs regardless of structure.

## 4 GMM Entropy Approximations

Having established multiple polynomial approximations in Sec. 3, we now consider applying them to the definition of entropy for a GMM. We can directly substitute the series approximation into the entropy definition, \(H(p(x))=_{p}[-(p(x))]\), and push the expectation through the summations.

### Huber et al. Entropy approximation

Applying Huber et al.'s Taylor series approximation of the \((p(x))\), we see the GMM entropy can be approximated by,

\[H(p(x))=-_{i=1}^{m}w_{i}_{n=0}^{}(_{i})}{n!} _{q_{i}}[(x-_{i})^{n}],\] (10)

where \(q_{i}(x)=(x_{i},_{i})\) is shorthand for the \(i^{}\) Gaussian component. The attractive feature of Eqn. (10) is that it simplifies the expected value of a log-GMM to the \(n^{}\) central moments of the \(i^{}\)

Figure 1: **Divergence of Huber et al. and convergence of our polynomial series approximation are plotted for the Gaussian mixture, \(p(x)=.35(x-3,2)+.65(x 0,.2)\). In the left graph, the log-GMM is plotted, which is what each series is defined for. The right plot is the exponential of the series so we can see how each converge in the more familiar framework of a GMM. Notice that the Huber et al. is centered on the first component mean \(_{1}=-3\) and diverges around the mean of the second component \(_{2}=0\) as supported by Theorem 3.1 since the mode \(_{1}\) is less than half the probability at the mode \(_{2}\). Both of our methods are convergent, the Taylor series is a bound (Theorem 4.3) while the Legendre series has a lower global error.**component which, is exactly zero when \(n\) is odd and has a closed form when \(n\) is even. However, Theorem 3.1 shows that this approximation is not guaranteed to converge, which is supported by experimental results in Sec. 6. Furthermore, in higher dimensions, \(h^{(n)}(_{i})=h(_{i})}{ x_{1}^{_{1}}-  x_{d}^{_{n}}}\), where \(j_{1}++j_{d}=n\) which grows rapidly, is an \(n\) dimensional tensor. This is cumbersome to compute and is difficult to deal with the tensor arithmetic required beyond a Hessian. In practice, this limits Eqn. (10) to only second order approximations for random vectors.

### Taylor Series Entropy Approximation

Having established the convergent Taylor series of the logarithm in Lemma 3.2, we can apply the approximation and push the expectation through the summations. This reduces the computation of the entropy to computing \(_{p}[p(x)^{k}]\) for all \(k<n\) where \(n\) is the order of the polynomial approximation.

**Lemma 4.1** (Closed form expectation of powers of GMMs).: _Let \(p(x)=_{i=1}^{M}w_{i}(x|_{i},_{i})\) be a GMM and \(k\) be a non-negative integer. Then_

\[_{p}[p(x)^{k}]=_{j_{1}++j_{M}=k},,j_ {M}}_{i=1}^{M}w_{i}((0|_{i},_{i})}{ (0|,)}_{t=1}^{M}(w_{t}(0|_{t},_{ t})^{j_{t}}))\] (11)

_where \(=(_{i}^{-1}+_{t=1}^{M}j_{t}_{t}^{-1})^{-1}\) and \(=(_{i}^{-1}_{i}+_{t=1}^{M}j_{t}_{t}^{-1}_{t})\)._

While Eqn. (11) may seem complicated at first glance, it is straightforward to compute. All terms are Gaussian densities, polynomial functions, and binomial coefficients. Lemma 4.1 is defined for \(_{p}[p(x)^{k}]\) but an analogous definition holds for \(_{p}[q(x)^{k}]\) allowing us to apply all the following results not only to entropy, but cross-entropy, KL and MI of GMMs. This is not the focus of this paper, however a discussion can be found in A.4 for completeness. Using Lemma 3.2 and Eqn. (11), we can obtain the following approximation,

\[_{N,a}^{T}(p(x))=-(a)-_{n=1}^{N}}{na^{n}}_ {k=0}^{n}(-a)^{n-k}_{p}[p(x)^{k}]\] (12)

To ensure the expected value can be pushed through the infinite sum of the series, we check that our finite order entropy approximation does still converge to the true entropy.

**Theorem 4.2** (Convergence of \(_{N,a}^{T}(p(x))\)).: _Let \(p(x)=_{i=1}^{M}w_{i}(x|_{i},_{i})\) be a GMM and choose a Taylor center such that \(a>(p(x))\). Then, for \(_{N,a}^{T}(p(x))\) defined in Eqn. (12)_

\[_{N}_{N,a}^{T}(p(x))=H(p(x))\] (13)

Having established convergence of our estimator, it remains to provide a method for selecting a Taylor center that meets the convergence criterion \(a>(p(x))\). In fact, we show in Theorem 4.3 that selecting a looser condition \(a>(p(x))\) ensures convergence from below, thus yielding a lower bound on the true entropy.

**Theorem 4.3** (Taylor Series is Lower Bound of Entropy).: _Let \(p(x)=_{i=1}^{M}w_{i}(x|_{i},_{i})\) and \(a>(p(x))\). Then, for all finite \(N\),_

\[_{N,a}^{T}(p(x)) H(p(x))\] (14)

We have now established that the Taylor center chosen as \(a>(p(x))\) is both convergent and yields a lower bound. In fact, it is easy to find such a point by upper bounding the maximum of a GMM as given in Theorem 4.4.

**Theorem 4.4** (Upper bound on maximum of a GMM).: _Let \(p(x)=_{i=1}^{M}w_{i}(x|_{i},_{i})\), then_

\[(p(x)) a=_{i}^{M}w_{i}|2_{i}|^{-}\] (15)

In our experience choosing a center closer to the convergence criterion \(a>(p(x))\) yields slightly more accurate estimates, but not significantly so.

### Legendre Entropy Approximation

Now, starting with the convergent Legendre approximation considered in Lemma 3.3 and Eqn. (11), we can obtain the following approximation,

\[^{L}_{N,a}(p(x))=-_{n=0}^{N}(2n+1)_{j=0}^{n}(n+j)! ((j+1)(a)-1)}{(n-j)!((j+1)!)^{2}}L_{[0,a],n}(_{p}[p(x)^{k}])\] (16)

Again, we verify that taking the expectation of our series does not effect convergence.

**Theorem 4.5** (Convergence of \(^{L}_{N,a}(p(x))\)).: _Let \(p(x)=_{i=1}^{M}w_{i}(x|_{i},_{i})\) be a GMM and choose an interval such that \(a>(p(x))\). Then for \(^{L}_{N,a}(p(x))\) defined in Eqn. (16)_

\[_{N}^{L}_{N,a}(p(x))=H(p(x))\] (17)

Now having established converge criterion for the Legendre series approximation, we need to choose an upper point of the interval for the Legendre series. We need to choose \(a>(p(x))\) which is satisfied by the same \(a\) found in Lemma 4.4.

### Approximation of Taylor Series Limit

Computing the series in Eqn. (12) for higher orders can be computationally prohibitive. In particular, the sum \(_{j_{1}++j_{M}=n}\) is over \(M\) integers summing to \(n\), which is \(((n+M-1)!)\). In this section, we provide an approximation that avoids explicit computation of this sum for higher orders. We employ a method (similar to Richardson extrapolation ) that is based on a polynomial fit of the convergence rate for the lower bound property discussed in Theorem 4.3. From Taylor's theorem, we know that a function can be represented as \(f=T_{n}+R_{n}\), where \(T_{n}\) is the \(n^{ h}\) order Taylor polynomial and \(R_{n}=(^{n})\) is the remainder. Rewriting this in terms of the Taylor polynomial, we observe that \(T_{n}=-R_{n}+f=^{n}+\). We take \(<0\) to represent the negative scale factor in front of the remainder, and \(0<<1\) to model the decay of the remainder.

\[^{T}_{n,a}(p(x))=^{n}+\] (18)

We require three consecutive orders of our Taylor series approximation, \(^{T}_{n,a}(p(x))\), \(^{T}_{n+1,a}(p(x))\), and \(^{T}_{n+2,a}(p(x))\), to solve for the three unknown parameters:

\[^{T}_{n,a}(p(x))= ^{n}+\] \[^{T}_{n+1,a}(p(x))= ^{n+1}+\] \[^{T}_{n+2,a}(p(x))= ^{n+2}+.\]

Since \(0<<1\), \(_{n}^{n}=0\), indicating that we aim to solve for \(\) as our approximation of the limit of the Taylor series entropy.

\[^{TL}_{N,a}(p(x))==^{T}_{N-2,a}(p(x))-^{T}_{N- 1,a}(p(x))-^{T}_{N-2,a}(p(x)))^{2}}{^{T}_{N,a}(p(x))-2^{T} _{N-1,a}(p(x))+^{T}_{N-2,a}(p(x))}\] (19)

This approach assumes that the Taylor series converges according to Eqn. (18), which is not the case in general. Identifying the exact rate of convergence is a topic of future work. Nevertheless, this simple approximation has shown higher accuracy in practice with negligible additional computation, as demonstrated in the experiments of Sec. 6. With a slight abuse of terminology, we refer to this approach as the _Taylor limit_. We do not apply this method to the Legendre approximation (Eqn. (16)) as it doesn't maintain a lower bound during its convergence. Although equivalent methods have been considered to model potential oscillation convergence, in practice, we do not find an increase in accuracy.

## 5 Related Work

Numerous approximation methods exist in the literature for estimating entropy and related information measures, such as mutual information and Kullback-Leibler divergence, in the context of Gaussian Mixture Models (GMMs). Monte Carlo estimation, deterministic bounds using Jensen's inequality,best-fit moment matched Gaussians, and numerical integral approximations based on the unscented transform have been explored [16; 14; 15]. This paper focuses on the Taylor approximation by Huber et al., an alternative Taylor approximation is proposed by Sebastiani , which assumes a shared covariance matrix among GMM components in the high variance setting. However, neither Huber et al. or Sebastiani provide theoretical analysis or convergence guarantees offered in our present work. An analysis conducted by Ru et al.  explores the efficiency of Huber et al.'s method and demonstrates that deterministic quadrature methods can be equally fast and accurate in a single dimension, however quadrature methods scale poorly with dimension, at \((N^{D})\) where \(N\) is the number of quadrature points per dimension and \(D\) is the dimension of the problem.

Variational approximations and bounds are also widely explored for estimating entropy and mutual information (MI). Much of this work is motivated by the use of Gibbs' inequality, which leads to bounds on entropy and MI . Later work explored similar techniques for upper and lower bounds on MI [21; 10]. More recent work uses artificial neural networks (ANNs) as function approximators for a variety of information-theoretic measures based on differential entropy. The MI neural estimator (MINE) uses such an approach for representation learning via the _information bottleneck_ based on the Donsker-Varadhan (DV) lower bound on KL . Related methods use ANNs for optimizing the convex conjugate representation of Nguyen et al. . McAllester and Stratos  show that many of these distribution-free approaches based on ANN approximation rely on Monte Carlo approximations that have poor bias-variance characteristics which they provide their own Difference of Entropies (DoE) estimator that achieves the theoretical limit on estimator confidence.

## 6 Experiments

We consider two experiments, a synthetic GMM section where we look at divergence of Huber et al. approximation (Eqn. (10)) and convergence of our three methods, our Taylor (Eqn. (12)), Taylor limit (Eqn. (19)), and our Legendre (Eqn. (16)). Furthermore, we give comparisons of accuracy and computation time across a variety of setting of approximation order, number of GMM components, and dimension for all methods. We then show our how our methods can be applied in practice to Nonparametric Variational Inference  where the convergence guarantees of the estimators has a noticeable accuracy improvement on their algorithm.

### Synthetic Multivariate GMM

To highlight the theoretical properties, such as convergence, divergence, accuracy, and lower-bound of methods as discussed in Sec. 4, we will consider some synthetic GMMs. We create two GMMs similar to the example published in  (original experiment recreated in A.5). We consider a single and multi-dimensional dimensional case that satisfy the divergence criterion in Theorem 3.1. We also look at a time and accuracy analysis versus dimension, components, and polynomial order.

Figure 2: **Scalar GMM** example is plotted on the left. The variance of a component of a two component GMM is varied between \(_{2}^{2}(0,1]\) as in theory according to Theorem 3.1, the example will be divergent where \(_{2}^{2}<.46\) and convergent above. We plot the fourth order of each method and see that Huber et al.â€™s approximation does diverge where the theory predicts. **Two dimensional GMM** with five components is consider on the right. Here the mean of a single component is shifted from \(_{5}=[-3,-3]^{T}\) to \(_{5}=^{T}\). We consider the third order approximation of each method and see that Huber et al. is poorly behaved. In both examples, we see that our Taylor method is a lower bound (Theorem 4.3), the Taylor limit provides higher accuracy, and Our Legendre method is a highly accurate approximate.

Scalar GMMIn this experiment, we consider a scalar GMM as fourth order and above cannot be easily computed in higher dimensions for Huber et al. due to tensor arithmetic. We use a simple two-component GMM with parameters \(w_{1}=0.35\), \(w_{2}=0.65\), \(_{1}=-2\), \(_{2}=-1\), \(_{1}^{2}=2\), and \(_{2}^{2}(0,1]\). We are changing the variance of the second Gaussian, \(_{2}^{2}\), in the range \((0,1]\) because the condition for divergence in Theorem 3.1 (\(p(x=_{1})<p(x=_{2})\)) is satisfied approximately when \(_{2}^{2}<0.46\) meaning this experiment should have regions of both convergence and divergence for Huber et al. approximation. Fig. 2 (left) shows the fourth order approximations of all methods. We see that the Huber et al. approximations diverges as expected in the range where \(_{2}^{2}<.46\). Our Taylor method remains convergent and accurate for all values while maintaining a lower bound. Again, our limit method gains us some accuracy and still manages to be lower bound. In this case, we see that the Legendre approximation is a near perfect fit for the entropy.

Multivariate GMMTo demonstrate that divergence is not limited to single dimension or higher orders, we consider a five-component, two-dimensional GMM with the parameters \(w_{i}=.2\  i\), \(_{1}=^{T}\), \(_{2}=^{T}\), \(_{3}=[1,-.5]^{T}\), \(_{4}=[2.5,1.5]^{T}\), \(_{5}=c^{T}\) for \(c[-3,3]\), \(_{1}=.25_{2}\), \(_{2}=3_{2}\), and \(_{3}=_{4}=_{5}=2_{2}\) where \(_{2}\) is the two dimensional identity matrix. This examples shifts the mean of the fifth component to show that simply the location of components can make the Huber et al. approximation behave poorly. Fig. 2 (right) shows the third order approximation of each method. We see that Huber et al. is clearly not well behaved in this case even with low order approximation. Furthermore, we continue to see a lower bound by our Taylor method, an increased accuracy from out limit method, and that the Legendre approximation is very close to the true entropy.

#### 6.1.1 Computation Time

In this experiment we empirically analyze the computation time of each method as a function of Gaussian dimension, number of Gaussian components, and the order of each polynomial approximation. The baseline of each method will be compared to the Monte Carlo estimation of entropy using \(L=1000\) samples \(\{x_{j}\}_{j=1}^{L} p\). The Monte Carlo estimator is given by \(=_{j}(- p(x_{j}))\).

DimensionIn Fig. 3 (left), we evaluate the accuracy and computation time for 30 two-component GMMs per dimension in the range of \(\). Comparing second order approximations of all methods against the Monte Carlo estimator, our polynomial approximations demonstrate similar accuracy and

Figure 3: **Dimension (left) of a two-component GMM varies from zero to fifty, for the second order of each method. Our methods show comparable accuracy and computation time to Huber, regardless of dimension. Number of components (middle) in a two-dimensional GMM is considered for the second order approximation of all methods. Huber et al. and our Legendre approximations are nearly equivalent in accuracy, while the Taylor series and Taylor limit serve as lower bounds. Computation time for all our methods is identical and comparable to Huber et al., deviating only at high numbers of components. Order (right) of each approximation is varied for a three-dimensional, two-component GMM. Huber et al. is plotted up to order three, as higher orders are restrictive due to tensor arithmetic and Taylor limit starts at order two as it requires three consecutive terms.**

nearly identical computation time. The results are comparable to Huber, indicating that our methods preserve accuracy and computation efficiency while providing convergence guarantees.

GMM ComponentsIn Fig. 3 (middle), accuracy and computation time are presented for 30 two-dimensional GMMs with varying numbers of components (from \(1\) to \(20\)) using second order approximations. Legendre and Huber methods show slightly higher accuracy compared to our Taylor approximation and Taylor limit. Notice, Huber's standard deviation also increases with more components, due to the increased likelihood of satisfying the divergence condition in Theorem 3.1. Computation time remains similar for all methods, but is more prohibitive for higher components.

Polynomial OrderFig. 3 (right) shows as the order of the polynomial approximation increases for two-component GMMs in three dimensions. Legendre and Huber methods show higher accuracy compared to Taylor approximation and Taylor limit. Huber is limited to order 3 due to Tensor arithmetic, while Taylor limit starts at order 2 as it requires multiple orders. Computation times are similar across all methods. Notice no accuracy is gained from zero to first order and from second to third order in Huber's approximation due to relying on odd moments of Gaussians which are zero.

### Nonparametric Variational Inference

Consider a target density \(p(x,)\) with latent variables \(x\) and observations \(\). The NPV approach  optimizes the evidence lower bound (ELBO), \( p(x,)_{q}H_{q}(p(x,))-H_{q}(q(x)) (q)\) w.r.t. an \(m\)-component GMM variational distribution \(q(x)=_{i=1}^{m}(x|_{i},_{i}^{2}I_{d})\). The GMM entropy lacks a closed-form so NPV applies Jensen's lower bound as an approximation, \(_{q}^{J}(q(x))\). The cross entropy also lacks a closed-form, so NPV approximates this term using the analogous Huber et al. Taylor approximation. Specifically, NPV expands the log density around the means of each GMM component as,

\[H_{q}(p(x))-_{i=1}^{M}w_{i}_{n=0}^{N}(p( _{i}))}{n!}_{q_{i}}[(x-_{i})^{n}]=_{N,q}^{H}(p (x))\] (20)

However, Eqn. (20) is subject to the divergence criterion of Theorem 3.1 if \(2p(_{i})(p(x))\). By replacing the entropy terms with our convergent series approximations we observe significant improvements in accuracy.

**In our approach**, we will highlight and address two problems with the NPV algorithm; the potential divergence of \(_{N,q}^{H}(p(x))\) and the poor estimation of the GMM entropy via \(_{q}^{J}(q(x))\). To address the potential divergence of \(_{N,q}^{H}(p(x))\), we will take motivation from the results found in  and use a \(2\) point Gauss-Hermite quadrature method to approximate \(H_{q}(p(x))\). This method will be a limiting factor in scaling the NPV algorithm in dimension, however it guarantees that the cross-entropy approximation will not diverge. This alteration leads to a solution for the inconsistency of the ELBO approximations. Then, Jensen's inequality is a very poor approximation for entropy in general, instead we will use the three methods we have introduced, our Taylor, Taylor limit, and our Legendre, as the GMM entropy approximations for higher accuracy. Fig. 4 shows an approximation of a two dimensional, three component mixture Student T distribution using a five component GMM in the traditional NPV, our modified NPV algorithm with our Taylor and Legendre approximation.

**The results**, as seen in Fig. 5, highlight the accuracy of each method versus the number of components, the order of our polynomial approximation, and the dimension of the GMM. In each experiment, we are approximating a multivariate mixture T distribution, \(p(x)\). We randomize the parameters of \(p(x)\) and the initialization parameters of the variational GMM, \(q(x)\), for optimization. The KL is approximated using a \(100000\) Monte Carlo approximation after convergence of each algorithm. We see that in all cases of components, order, and dimension, our method achieves significant accuracy improvements. We see that we can use low order approximations to receive substantial approximation improvement (Fig. 5 (middle)). We see all methods gain accuracy as number of components increase (Fig. 5 (left)) however our methods see most of the accuracy improvements with only a few components, whereas NPV has substantially worse approximations with low components. Finally, we see we maintain a lower variance and KL than NPV with all our methods as the dimension grows (Fig. 5 (right)). For further discussion of the experiment, see A.6.

## 7 Limitations

Our Taylor and Legendre methods ensure convergence and deliver comparable accuracy to that of Huber et al. However, the computational complexity of these methods grows with the number of components, \(M\), in the GMM, following an \(((n+M-1)!)\) time complexity, where \(n\) represents the order of the approximating polynomial. This summation comprises only scalar terms and is amenable to parallelization but may become prohibitively expensive for large \(M\) and \(n\) values. Furthermore, our polynomial methods are specifically tailored to the entropy, \(H_{p}(p(x))\), and cross-entropy, \(H_{p}(q(x))\), where both \(p(x)\) and \(q(x)\) are GMMs. In contrast, Huber et al.'s approximation can be more readily extended to cross-entropy scenarios where \(q(x)\) pertains to any distribution amenable to the construction of a second-order Taylor polynomial. This limitation hinders the broader applicability of our approximation.

## 8 Discussion

We have provided novel theoretical analysis of the convergence for the widely used Huber et al. Taylor approximation of GMM entropy and established that the series diverges under conditions on the component means. We address this divergence by introducing multiple novel methods which provably converge. We wish to emphasize that the Huber et al. approximation tends to yield accurate results when it is convergent and the intention of this work is not to dissuade the use of this approximator. Quite the contrary, this work encourages the use of either Huber et al. or our own estimator by providing a solid theoretical foundation for both methods. We acknowledge that there are contexts in which one method may be preferred over the other, for example when bounds are preferred, or when convergence criteria are provably satisfied.

There are several areas that require further investigation. For example, one limitation of both methods is that they scale poorly with polynomial order and number of components. In fact, Huber et al. cannot easily be calculated for fourth order and above, due to tensor arithmetic. Our approximation works well in practice, but is limited solely to GMM densities. Further work is necessary to efficiently apply our convergent series to situations of cross-entropy's that contain non-GMM distributions.

Figure 4: A three component mixture Student T distribution PDF (far-left) is approximated by a five component GMM using traditional NPV (left), our algorithm using a \(6^{th}\) order Taylor polynomial (right), and Legendre polynomial (far-right). We see that NPV both has issues with finding correct placement of means and sets the variances of the GMM components to be too narrow. Our methods do a better job of assigning means and the Legendre method seems to set the variances slightly better than our Taylor.

Figure 5: The above figures show the accuracy of each method across varying components, orders, and dimensions in approximating a multivariate mixture T distribution with a GMM. Our method consistently improves accuracy significantly. Low order of the convergent estimators provide substantial approximation improvement (middle). Most accuracy improvements are achieved with a small number of components, unlike NPV (left) which continues to need higher number of components to see good accuracy return. The guaranteed convergence of the approximation in higher dimensions seems to have a drastic improvement on accuracy ().