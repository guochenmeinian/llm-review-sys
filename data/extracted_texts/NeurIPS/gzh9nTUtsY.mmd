# Least Squares Regression Can Exhibit

Under-Parameterized Double Descent

Xinyue Li

Applied Math, Yale University

xinyue.li.x1728@yale.edu &Rishi Sonthalia

Math, Boston College

rishi.sonthalia@bc.edu

###### Abstract

The relationship between the number of training data points, the number of parameters, and the generalization capabilities of models has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime and that the standard bias-variance trade-off holds in the under-parameterized regime. These works provide multiple reasons for the existence of the peak. We postulate that the location of the peak depends on the technical properties of both the spectrum as well as the eigenvectors of the sample covariance. We present two simple examples that provably exhibit double descent in the under-parameterized regime and do not seem to occur for reasons provided in prior work.

## 1 Introduction

This paper demonstrates interesting new phenomena that suggest that our understanding of the relationship between the number of data points, the number of parameters, and the generalization error is incomplete, even for simple linear models. The classical bias-variance theory postulates that the generalization risk versus the number of parameters for a fixed number of training data points is U-shaped (Figure (a)a1). However, modern machine learning has shown that if we keep increasing the number of parameters, the generalization error eventually starts decreasing again  (Figure (b)b2). This second descent has been termed as _double descent_ and occurs in the _over-parameterized regime_, which is when the number of parameters exceeds the number of data points. _Understanding the location and the cause of such peaks in the generalization error is of significant importance._

Many different theories have been postulated for the appearance of the peak. The prevalent theory is that when the model is under-parameterized, the learning is constrained. This constraint on the

Figure 1: Bias-variance trade-off and double descent.

learning results in increased variance until the interpolation point. After this point, there exists a high dimensional space of solutions, and learning methods, such as gradient descent, pick solutions that generalize well. This conjecture has been empirically validated for deep neural networks. Due to the challenges of analyzing deep neural networks, theoretical understanding of this phenomenon has focused on linear models - linear regression [4; 5; 6; 7; 8; 9; 10; 11; 12; 13] and kernelized regression [14; 15; 16; 17; 18; 19; 20; 21; 22]. These works show that there exists a peak at the _boundary between the under and over-parameterized regimes_. Hence validating the above postulated theory for their setting. Careful theoretical analysis then shows that the generalization error can be decomposed into various terms, one of which is the norm of the estimator. Specifically, it has been shown that the curve for the norm of the estimator versus the dimension of the data also exhibits double descent, with the peak occurring at the same point as the peak in the generalization error curve. In most cases, this is the only term in the decomposition that exhibits double descent. This leads to a second theory for the occurrence of the peak, that is, _the peak in the generalization error for linear models occurs due to the norm of the estimator blowing up._

Contributions.Since understanding the reasons that peaks occur is of critical importance, it is crucial that we have a robust theory for their appearance. However, most work focuses on the over-parameterized regime and ignores the under-parameterized regime. This is because it is commonly believed that the variance is monotonic in the under-parameterized regime. We show that this is not true and present two simple examples that exhibit double descent in the _under-parameterized regime_.

* **Why does the Peak Occur**. We argue that the location of the peak depends on two factors: the alignment between the targets \(y\) and singular vectors \(V\) of the training data matrix and the spectrum of the data. We show that by modifying these quantities appropriately, we can move the peak into the underparameterized regime.
* **Modifying the Alignment.** For the first example, we consider a spiked covariate model, where one eigendirection dominates, and the regression target only depends on the dominant eigendirection. For this model, we consider the ridge regularized problem with ridge parameter \(^{2}\) and show that the ridge parameter \(^{2}\) controls the alignment between the targets \(y\) and the singular vectors \(V\). We show that for \(>0\) the peak occurs in the under-parameterized regime (Theorem 2). Specifically, when the ratio of the dimension to the number of training data points \(c\) is equal to \((1+^{2})^{-1}\) (\(c:=d/n=1/(1+^{2})\)).
* **Modifying the Spectrum** For the second example, we consider training data that is a mixture of isotropic Gaussian vectors and vectors from along a fixed direction \(z\). By varying the mixture proportions \((_{1},_{2})\), we can modify the spectrum of the covariance matrix. We show that the expected risk displays under-parameterized double descent (Theorem 4), with the peak occurring when \(c:=d/n=_{1}\)).
* **Norm of the estimator**. We further analyze the first example and show that if we fix the number of training data points \(n\) and vary the dimension \(d\) of the problem, then for large values of \(\), the risk curve does not display a double descent. However, the curve for the norm of the estimator does display descent. Thus, the peak in the norm of the estimator does not imply a peak in the generalization error.

Organization.The rest of the paper is organized as follows. Section 2 presents a quick overview of prior work on double descent for linear models. Section 3 highlights two less-studied properties that influence the location of the peak. Section 4 presents the first of the two examples of under-parameterized double descent. This model also shows that a double descent in the norm of the estimator does not translate to a double descent in the risk. Finally, Section 5 presents the second example of under-parameterized double descent.

## 2 Prior Work on Double Descent

In this section, we present the current prevailing theories for the occurrence of local maximums in the risk curve. Concretely, consider the following simple linear model that is a special case of the general models studied in [5; 8; 11; 23] amongst many other works. Let \(x_{i}(0,I_{d})\) and let \(^{d}\) be a linear model with \(\|\|=1\). Let \(y_{i}=^{T}x_{i}+_{i}\) where \((0,1)\). Then, let \(_{opt}\) be the minimum norm solution to \(_{}\|^{T}X_{trn}-^{T}X_{trn}+_{trn}\|\), where \(_{trn}^{n 1}\). One important quantity is the aspect ratio of \(X\). Specifically, for a \(d n\) matrix, the aspect ratio is \(c:=d/n\). With this terminology, we see that a model is under-parameterized if \(c<1\) and over-parameterized when \(c>1\)Finally, the interpolation point, i.e., the point at which we can exactly fit the training data, is \(c=1\), assuming we have full-rank data.

Then, the excess risk \(\) and the expected norm of \(_{opt}\) can be expressed as follows:

\[=&c<1\\ +&c>1_{opt}= 1+&c<1\\ +&c>1.\]

In this model, there are a few important features that are ubiquitous in many prior double descent studies for linear models:

1. The peak happens at \(c=1\), on the border between the under and over-parameterized regimes.
2. Further, at \(c=1\) the training error equals zero. Hence, this is the interpolation point.
3. The peak occurs due to the expected norm of the estimator \(_{opt}\) blowing up near the interpolation point.

This is further validated by works that study ridge regularized regression . Works such as  have shown that optimally regularized regression no longer exhibits double descent. Further, increasing the amount of regularization from zero to the optimal amount of regularization results in the magnitude of the peak in the generalization getting smaller until a peak no longer exists. _However, the location of the peak does not change by changing the amount of regularization._ Further, Chen, Min, Belkin, and Karbasi  proved that double descent cannot take place in the under-parameterized regime for the above model.

Subsequently, works such as  show that there can be multiple descents in the over-parameterized regime. Specifically, d'Ascoli, Sagun, and Biroli  show that the first peak in triple descent is due to the norm of the estimator peaking and that the second peak is due to the initialization of the random features. Their results, Figure 3 in , show that the peaks only occur if the model is over-parameterized. Further Chen, Min, Belkin, and Karbasi  show that by considering a variety of product data distributions, any shaped risk curve can be observed in the _over-parameterized_ regime. Finally, Curth, Jeffares, and van der Schaar  says that the peak occurs at the point of effective dimensionality of the model and not the true dimensionality. Here, we see that there are three other reasons provided for the occurrence of peaks in the risk curve.

1. Regularization can reduce the effective dimensionality of the model and move the peak to the right into the over-parameterized regime .
2. For random feature models, we see that the random initialization results in a second peak in the over-parameterized regime .
3. Due to the data having a complex covariance structure, any shaped risk curve is possible in the over-parameterized regime .

Other works  have considered the problem for other loss models and shown a variety of different risk curves can exist. Table 1 summarizes some of the prior work.

   Noise & Ridge Reg. & Dim. & Peak Location & Reference \\  Input & Yes & 1 & Under-parameterized & This paper. \\ Output & No & Full & Under-parameterized & This paper \\ Input & No & Low & Interpolation point &  \\ Input & Yes & Full & Interpolation point &  \\ Output & No & Full & Over-parameterized/interpolation point &  \\ Output & Yes & Full & Over-parameterized/interpolation point &  \\ Output & No & Low & Over-parameterized/interpolation point &  \\ Output & Yes & Low & Over-parameterized/interpolation point &  \\ Output & No & Low & No peak &  \\   

Table 1: Table showing various assumptions on the data and the location of the double descent peak for linear regression and denoising. We only present a subset of references for each problem setting. For the low rank setting in this paper, see Appendix F.

Double Descent with Input Noise.There has also been prior work that studies double descent for models with input noise rather than output noise [24; 37; 38]. From these Sonthalia and Nadakuditi  and Kausik, Srivastava, and Sonthalia  consider the unregularized problem and show that the peak occurs at the boundary. Dhifallah and Lu  considers ridge regularization with isotropic Gaussian data and again sees that the peak occurs at the boundary.

## 3 Spectral Properties of the Data Affect the Peak Location

In this section, we identify two important spectral properties that govern the location of the peak. In later sections, we delve into two examples that modify these properties and move the peak into the under-parameterized regime. We begin with definitions and notations. Throughout the paper, we assume that training data \(X=[x_{1}\,\,x_{n}]^{d n}\) and \(y=[y_{1}\,,y_{n}]^{k n}\). We are interested in the standard ridge regularized least squares problem.

\[_{}\|y-^{T}X\|_{F}^{2}+^{2}\|\|^{2}\]

In the unregularized case, the minimum norm solution is given by \(^{T}=yX^{}\), where \(X^{}\) is the Moore-Penrose Pseudoinverse of \(X\). Prior work on linear models has shown that a double descent in the risk is due to a double descent in the norm of the estimator. Suppose \(X=U V^{T}\) is the SVD, \(^{d 1}\), then using unitary invariance, we have that

\[\|\|^{2}=_{i=1}^{(X)}^{2}}{_{i} ^{2}}\]

where \(_{i}\) is the \(i\)th singular value. Hence, this is controlled by

1. The alignment between \(y\) and \(V\). Here \(V\) are the eigenvectors of the data gram matrix.
2. The spectrum of the gram matrix \(X^{T}X\).

Many prior works deal with the alignment in one of two ways. If \(y=^{T}X+\), with \(\) having an isotropic distribution, then prior works either assume that \(\) has an isotropic distribution [4; 5; 42] or they assume that \(X\) is isotropic Gaussian or that \(x_{i}=^{1/2}z_{i}\), where \(z_{i}\) is from an isotropic Gaussian [23; 43] and \(\) is a deterministic matrix. For example, if \(\) has an isotropic distribution, taking the expectation with respect to \(,\) we get that

\[_{,}[\|\|^{2}]=[_{1}^{2 }]\|XX^{}\|_{F}^{2}+[_{1}^{2}]_{i=1}^{(X)} ^{2}}.\]

This quantity is then studied by looking at the distribution of the spectrum as \(d,n\).

**Definition 1**.: _Given a random matrix \(A\), if \(_{1},,_{k}\) are its eigenvalues. Then the **empirical spectral distribution** (ESD) is the following sum of Dirac delta measure_

\[^{k}=_{i=1}^{k}_{_{i}}\]

_and the limiting spectral distribution \(\) is a measure such that \(^{k}\) weakly almost surely._

In general, the limiting distribution \(_{c}\) depends on the limiting aspect ratio \((i.e.,d/n c)\).

**Definition 2**.: _Given a measure \(_{c}\) that is supported on the interval \(J\), the Stieltjes transform is_

\[m_{_{c}}()=_{_{c}}[],\ \  J.\]

One common assumption is that the limiting distribution of the empirical spectral distribution is the Marchenko-Pastur distribution . Other limiting distributions have been considered in [5; 45; 46]. For the Marchenko-Pastur distribution, it is known (see, for example, Lemma 5 in ) that

\[m_{_{c}}(0)=_{_{c}}[]= &c<1\\ &c>1.\]Hence, the risk is governed by the value of the Stieltjes transform of the limiting spectrum at \(=0\). In particular, for the above example, the location of the peak of the risk as a function of \(c\) depended on the location of the peak of

\[c m_{_{c}}(0)=:G(c).\]

Hence the risk depends on both the spectrum and the alignment between \(y\) and \(V\). Thus, the peak occurs at \(c=1\) because of the following two conditions.

[style=MyFrame]

**Assumption 1**.: _If \(X=U V^{T}\) is the SVD, then \(yV\) is isotropic._

[style=MyFrame]

**Striples Transform Peak Assumption**

**Assumption 2**.: _The function \(c m_{_{c}}(0)=:G(c)\) has a local maximum at \(c=1\)._

In this paper, we show that violating either one of the above two assumptions can move the peak from the interpolation point into the **under-parameterized regime**.

## 4 Alignment Mismatch

In this section, we present the first example that exhibits double descent in the under-parameterized regime. This model violates Assumption 1.

### Model Assumptions

For any \(k d\), let \(^{d k}\) be fixed such that the operator norm \(||^{T}||\) is \((1)\). Let \(X_{trn}^{d n}\) be the signal matrix and \(A_{trn}^{d n}\) be the noise matrix. Then, the ridge regularized least square estimator \(W_{opt}\) is the minimum norm solution to

\[W_{opt}:=*{arg\,min}_{W}\|^{T}X_{trn}-W(X_{trn}+A_{trn})\|_ {F}^{2}+^{2}\|W\|_{F}^{2}. \]

Given test data \(X_{tst}+A_{tst}\), the mean squared generalization error is given by

\[(W_{opt})=_{A_{trn},A_{tst}}[X_{tst }-W_{opt}(X_{tst}+A_{tst})\|_{F}^{2}}{n_{tst}}]. \]

**Assumption 3**.: _Let \(^{d}\) be a one dimensional space with a unit basis vector \(u\). Then let \(X_{trn}=_{trn}uv_{trn}^{T}^{d n}\) and \(X_{tst}=_{tst}uv_{tst}^{T}^{d n_{tst}}\) be the respective SVDs for the training data and test data matrices. We further assume that \(_{trn}=O()\) and \(_{tst}=O(})\)._

There are no assumptions on the distribution of \(v_{trn},v_{tst}\) besides having unit norm. First, we see that the data \(X+A\) has a spiked covariance, with the dominant eigendirection closely aligned with \(u\). Since the targets only depend on \(X\), we consider \(A\) to represent noise. Even with the rank 1 assumption, the model captures many different scenarios. If \(k=1\), then the problem is similar to error-in-variables regression. If \(k=d\) and \(=I\), then this is the supervised denoising problem. If the columns of \(X_{trn}\) are all \( u\) and \(=u\), then this captures the binary classification problem (with MSE loss) for two Gaussian clusters centered at \(u\) and \(-u\) with labels \( 1\).

Assumptions about \(A\).The analysis works for general assumptions in . For simplicity, we assume that the matrix \(A\) has I.I.D. entries drawn from a normalized Gaussian.

**Assumption 4**.: _The entries of the matrices \(A^{d n}\) are I.I.D. from \((0,1/d)\)._

### Expected Risk and Peak Location

We begin by providing a formula for the generalization error given by Equation 2 for the least squares solution given by Equation 1. All proofs are in Appendix E.

**Theorem 1** (Generalization Error Formula).: _Suppose the training data \(X_{trn}\) and test data \(X_{tst}\) satisfy Assumption 3 and the noise \(A_{trn},A_{tst}\) satisfy Assumption 4. Let \(\) be the regularization parameter. Then for the under-parameterized regime (i.e., \(c<1\)) for the solution \(W_{opt}\) to Problem 1, the generalization error or risk given by Equation 2 is given by_

\[(c,)=^{2}(_{trn}^{2}+1))}{2d^{2}} c}{c)^{2}+4^{2}c^{2}}}-^{-2}^{2}(_{trn}^{2}+1))}{2d}+^{-2}^{2}}{n_ {tst}}+o(),\]

_where_

\[=u\|}{2+_{trn}^{2}(1+c+^{2}c-c)+4^{2}c^{2}})}.\]

Sketch.: The proof proceeds in four key steps. First, we use the Sherman-Morrison formula for pseudoinverses . Next, we decompose the error into constituent dependent quadratic forms. Through random matrix theory and concentration of measure arguments, we demonstrate that each quadratic form concentrates around a deterministic value characterized by the Stieltjes transform of the limiting empirical spectral distribution. Finally, we establish concentration bounds for the products and sums of these dependent forms, yielding the desired error rate. 

Since the focus is on the under-parameterized regime, Theorem 1 only presents the under-parameterized case. The over-parameterized case can be found in Appendix E.3. Due to the complexity of the expression, it is difficult to discern how the risk scales with respect to the training data signal strength \(_{trn}^{2}\), the regularization strength \(\), or the aspect ratio \(c\). Since the focus of the paper is the scaling with respect to \(c\), we present the connection between the risk curve and \(c\) in the main text. However, the shape of the risk curve with respect to the other parameters is also interesting and can be found in Appendix D.

To understand the shape of the risk curve as \(c\) varies, we first consider that _data scaling regime_. That is, fix \(d\) and change \(n\). The following theorem 2 shows that the risk curve is theoretically guaranteed to have a peak at \(c=}\).

**Theorem 2** (Under-Parameterized Peak).: _Let \(_{>0}\), \(_{trn}^{2}=n=d/c\) and \(_{tst}^{2}=n_{tst}\), and \(d\) is sufficiently large, so that the error term \(o(1/d)\) is small, then the risk \((c)\) from Theorem 1, as a function of \(c\), has a local maximum in the under-parameterized regime at \(c=}\)._

Theorem 2 contrasts with prior works, in which double descent occurs in the over-parameterized regime or on the boundary between the two regimes. We numerically verify the predictions from Theorems 1 and 2. Figure 2 shows that the theoretically predicted risk matches the numerical risk, thus _verifying that double descent occurs in the under-parameterized regime_.3

Figure 2: Figure showing the theoretical risk curve from Theorem 1 and empirical values in the data scaling regime for different values of \(\) ((L) \(=0.1\), (C) \(=1\), (R) \(=2\)]. Here \(_{trn}=,_{tst}=},d=1000,n_{tst}=1000\). For each empirical point, we ran at least 100 trials. More details can be found in Appendix G.

### The Peak Occurs Due to Alignment Mismatch

We now show that the peak occurs due to a mismatch between the target vector and the right singular vectors of the input data. To begin, note that the ridge regularized problem can be written as follows

\[\|^{T}\ _{d d}]}_{_{trn}}-W([X_{ trn}\ _{d d}]+\  I]}_{_{trn}})\|_{F}^{2}.\]

In this expression, \(y=^{T}_{trn}=(^{T}u)[v_{trn}^{T}\ _{p}]\). Hence the direction is given by \(_{trn}^{T}=[v_{trn}^{T}\ _{p}]\). The right singular vectors of \(_{trn}+_{trn}\) are more difficult to compute,thus we use proxies. Since \(_{trn}\) is rank 1, we use the right singular vectors of \(_{trn}\) as a proxy. Lemma 5 in the appendix, shows that if \(A=U V^{T}\), then we can express \(_{trn}\) as \(^{T}\), where \(=U\), \(^{2}=^{2}+^{2}I\), and \(=V_{1:p}^{-1}\\  U^{-1}^{n+d d}\). Here \(V_{1:d}\) are the first \(d\) columns of \(V\). Then,

\[y=(^{T}u)(_{trn}^{T}V_{1:d})^{-1}.\]

Since \(V_{1:d}\) came from a Gaussian random matrix, \((_{trn}^{T}V_{1:d})\) has isotropic entries. However, the diagonal matrix \(^{-1}\) results in the entries of \(y\) not being isotropic. Note when \(=0\), \(^{-1}=I\), hence it is isotropic. Hence, \(\) controls the deviation from isotropy.

We use \(^{T}I\) as a proxy for the spectrum of the sample covariance. By Lemma 5, we have that \(^{T}=^{T}+^{2}\). We know that the limiting spectrum for \(^{T}\) is the Marchenko-Pastur distribution for which the map \(G(c)=m_{_{e}}(0)\) has a maximum for \(c=1\). Shifting the spectrum changes the magnitude of the peak but does not change the location. This suggests that the peak occurs due to the misalignment between the target vector and the right singular vectors of the input data.

Ablation experimentTo verify that the location of the peak is due to the misalignment, we conduct two experiments. First, we solve the unregularized problem. However, we change the spectrum of the noise matrix \(A_{trn}\). That is, instead of using \(A_{trn}=U V^{T}\), we use \(=U(^{2}+^{2}I)^{1/2}V^{T}\). If the spectrum was the primary factor determining the location of the peak, the peak should occur at \(c=1/(1+^{2})\). However, as seen in Figure 2(a) it still occurs at \(c=1\). Second, we replace \(\) with a uniformly random orthogonal matrix \(Q\)4. Clearly, \(yQ\) is now isotropic. Figure 2(b) shows that, in this case, the peak moves to the over-parameterized regime.

Connection to Prior Double Descent TheoryPrior double descent theory postulates that the peak for the ridge regularized model occurs at the interpolation point for the unregularized model or further to the right into the over-parameterized regime. Hence, this model goes against prior expectations, with the peak moving to the left into the under-parameterized regime. However, there might still be some connection between the training error and the location of the peak. For example, the peak may correspond to a local minimum of the training loss. We explore this in Appendix C and see only a weak connection with the third derivative of the training error.

Figure 3: Risk for the ablation experiment. Left: Empirical Expected Risk when using \(\) for the noise. Right: Empirical risk when we replace \(\) with a random orthogonal matrix.

### Peak in the Norm of the Estimator Does Not Imply a Peak in the Risk

Prior double descent theories suggest that double descent occurs due to the norm of the estimator increasing and then decreasing. This is true for the above model where we fixed \(d\) and varied \(n\). However, the connection breaks if we fix \(n\) and vary \(d\) instead (_parameter scaling regime_). This difference between the two regimes is due to the normalization considered and has been observed before .

Figure 4 shows that for the parameter scaling regime, for small values of \(\), we see under-parameterized double descent. However, as we increase \(\), the risk curve becomes monotonic. Nevertheless, as shown in Figure 5, for larger values of \(\), _there is still a peak_ in the curve for the norm of the estimator \(\|W_{opt}\|_{F}^{2}\). Hence, the curve for the norm of the estimator exhibits under-parameterized double descent even if the risk does not. This is further highlighted in Figure 6. Here, we see that even though the variance is non-monotonic, the risk is dominated by the bias term. Thus, we show that a peak in the generalization error for linear models does not imply a peak in the norm of the estimator. The following theorem provides a local maximum in the \([\|W_{opt}\|_{F}^{2}]\) curve for \(c<1\).

**Theorem 3** (\(\|W_{opt}\|_{F}\) Peak).: _If \(_{tst}=}\), \(_{trn}=\) and \(\) is such that \(p()<0\), then for fixed \(n\) that is sufficiently large enough, we have that \([\|W_{opt}\|_{F}]\) versus \(c=d/n\) curve has a local maximum in the under-parameterized regime at \(c=(^{2}+1)^{-1}\)._

## 5 Shifting Local Maximum for Stieltjes Transform as a Function of \(c\)

In this section, we present the next example of under-parameterized double descent that violates Assumption 2. In particular, the maximum of the map \(c m_{_{e}}(0)\) does not occur at \(c=1\). We show that the maximum can be chosen to be any value in \((0,1)\). We consider the following mixture model. Let \(_{1},_{2}\) be mixture weights such that \(_{1}+_{2}=1\). Then, with probability \(_{1}\), the data is sampled from \((0,I)\) and with probability \(_{2}\), the data point is \( z\) for fixed \(z^{d}\) and \((0,1)\). For this model, the uncentered covariance matrix is given by

\[[xx^{T}]=_{1}_{x(0,I)}[xx^{T} ]+_{2}[^{2}zz^{T}]=}{d}I+_{2}zz^{T}.\]

Figure 4: Figure showing the theoretical risk curve from Theorem 1 and empirical values in the parameter scaling regime for different values of \(\) (L) \(=0.1\), (C) \(=0.2\), (R) \(=0.5\)]. Here, only \(=0.1\) has a local peak. Here \(n=n_{tst}=1000\) and \(_{trn}=_{tst}=\). Each empirical point is an average of 100 trials.

Figure 5: Figure showing generalization error versus \([\|W_{opt}\|_{F}^{2}]\) for the parameter scaling regime for three different values of \(\).

Then, the expected excess risk for a solution \(\) compared to \(\) is

\[=[\|^{T}x-^{T}x\|^{2}|X]=[ }{d}\|^{T}-^{T}\|^{2}+_{2}\|(-) ^{T}z\|^{2}|X].\]

Let \(X_{trn}=[A\;\;zv^{T}]^{d n}\), where the \(A^{d n-k}\) with each column a data point sampled I.I.D. from \((0,I)\) and \(v^{k}\) is the vector with random coefficients in front of \(z\). Let \(^{d}\) be the target regressor function and let \(y^{T}=^{T}X_{trn}+_{trn}^{T}\), where \(_{trn}^{T}\) has I.I.D. entries from a standard normal. Let \(_{opt}^{T}=y^{T}X_{trn}^{T}(X_{trn}X_{trn}^{T})^{-1}\) be the minimum norm. Then, Theorem 4 shows that the peak occurs when \(d/n=c=_{1}<1\). To experimentally verify Theorem 4, we consider two cases, one where we enforce \( z\) and one where we do not. As seen in Figure 7, Theorem 4 is accurate for both cases. This suggests that the \( z\) assumption seems to only be needed for simplifying the proof.

**Theorem 4** (Under-parametrized Peak).: _For the above model, if \(k/n_{2}\), and \(d/n c\), then the expected risk is given by \(=c}{_{1}-c}&c<_{1}\\ _{1}(}{c-_{1}}+(1-}{c})( }{d}-z)^{2}}{\|z\|^{2}d}))&c> _{1}.\)_

Theorem 4 is quite surprising as it shows that the _only_ peak in the risk curve occurs in the under-parameterized regime. One might assume that is due to the low rankness of the data from the second mixture. While this is true, prior work does not indicate that this is the reason. Specifically, Huang, Hogg, and Villar  shows that projecting onto low-dimensional versions of the data acts as a regularizer and removes the peak altogether. Xu and Hsu , also looks at a similar problem, but they consider isotropic Gaussian data and project onto the first \(p\) components. In this case, the data is artificially high-dimensional (since only the first \(k\) coordinates are non-zero).

Figure 6: Figure showing the \([\|W_{opt}\|_{F}^{2}]\), and the generalization error in the parameter scaling regime for \(=1\), \(_{trn}=\), and \(_{tst}=}\). Here \(n=1000\) and \(n_{tst}=1000\). For each empirical data point, we ran at least 100 trials. More details can be found in Appendix G.

Figure 7: Figure showing under-parameterized double descent. (Left) We have \(=d z\). (Right) We have \( z\). The solid blue line represents the theoretical estimate from Theorem 4 and the scatter points are from empirical experiments with \(d=600\). For the empirical points, we average over 50 trials. The dashed vertical purple line is \(_{1}\).

They again see a peak at the interpolation point (\(n=p\)). Wu and Xu  also looks at a version of Principal Component Regression in which the data dimension is reduced. That is, the data is not embedded in high-dimensional space anymore. Wu and Xu  sees a peak at the boundary between the under and over-parameterized regions. Finally, Sonthalia and Nadakuditi  and Kausik, Srivastava, and Sonthalia  look at the denoising problem for low-dimensional data and have peaks at \(c=1\). Therefore, prior work does not immediately imply that low dimensional data results in under-parameterized double descent. If we had only low-dimensional data, then the peak "should" move into the over-parameterized regime. This is because if the true dimensionality of the data is \(r<d\). Then, one might think that the peak occurs when the number of training data points \(n\) equals \(r\) since that is the interpolation point5. We are in the over-parameterized regime since \(d>r=n\).

We can understand this phenomenon as follows. The data from the second mixture does not affect the smallest eigenvalue of the covariance matrix. This is because the second mixture lives in a one-dimensional space. Hence, it only affects the top eigenvalue. Since the Stieltjes transform at 0 is dominated by the behavior of the smallest non-zero eigenvalue, data from the second mixture has very little effect on the Stieltjes transform of the ESD at 0. We expect the above intuition to hold, even when replacing rank \(1\) with rank \(r\) for any fixed small \(r\).

Connection to Prior Double Descent TheoryFor this example, it is easy to show that there is a strong connection to prior double descent theory. Specifically, even though we cannot interpolate the data, the minimum training error will occur at \(c=_{1}\). Further, we see that the blow-up in the excess risk is due to the norm of the estimator blowing up.

Additionally, we see that comparing with the result from  (which is the case when \(_{2}=0\)), we see that the \(_{2}>0\) results in sifting the peak to \(_{1}\) and rescales the variance by \(_{1}\). However, we also see an additional correction term in the overparameterized regime:

\[(1-}{c})(-z)^{2}}{\|z\|^{2}d})\]

Here we see that the term depends on the alignment between the target \(\) and the spike direction \(z\).

## 6 Conclusion

This paper presents two simple models with double descent in the under-parameterized regime. While such peaks seem limited to special cases, understanding the cause is important for a complete understanding of the double descent phenomenon. Our analysis reveals that the location of peaks depends critically on two properties: the alignment between targets and the eigenvectors of the training data gram matrix and the behavior of the Stieltjes transform of the limiting empirical spectral distribution.

We demonstrate that violating either of these properties can shift the peak into the under-parameterized regime. The first model shows that ridge regularization can create a misalignment between targets and singular vectors, leading to a peak at \(c=1/(1+^{2})\). The second model, using a mixture of isotropic Gaussian vectors and directional vectors, demonstrates that modifying the spectrum can result in a peak at \(c=_{1}\). These findings challenge several prevailing theories about double descent. They show that peaks need not occur at or beyond the interpolation threshold and that a peak in the estimator's norm does not necessarily imply a peak in the generalization error.

Investigating the interaction between spectral properties and generalization in deep neural networks to provide a general theory of double descent is an important avenue for future work. Understanding whether similar phenomena occur in other architectures and the implications for model selection and regularization remain open questions.