# On the Universal Approximation Properties of

Deep Neural Networks using MAM Neurons

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

As Deep Neural Networks (DNNs) are trained to perform tasks of increasing complexity, their size grows, presenting several challenges when it comes to deploying them on edge devices that have limited resources. To cope with this, a recently proposed approach hinges on substituting the classical Multiply-and-Accumulate (MAC) neurons in the hidden layers of a DNN with other neurons called Multiply-And-Max/min (MAM) whose selective behaviour helps identifying important interconnections and allows extremely aggressive pruning. Hybrid structures with MAC and MAM neurons promise a reduction in the number of interconnections that outperforms what can be achieved with MAC-only structures by more than an order of magnitude. However, by now, the lack of any theoretical demonstration of their ability to work as universal approximators limits their diffusion. Here, we take a first step in the theoretical characterization of the capabilities of MAM&MAC networks. In details, we prove two theorems that confirm that they are universal approximators providing that two hidden MAM layers are followed either by a MAC neuron without nonlinearity or by a normalized variant of the same. Approximation quality is measured either in terms of the first-order \(L^{p}\) Sobolev norm or by the \(L^{}\) norm.

## 1 Introduction

Deep Neural Networks (DNNs) solve complex tasks leveraging a massive number of trainable parameters. Yet, thanks to the recent increasing interest in mobile Artificial Intelligence, there has been a growing emphasis on designing lightweight structures able to run on devices with constrained resources. This can be obtained by removing parameters that do not appreciably influence performance by means of one of the many pruning techniques that have been proposed. Some approaches entail removing, in a single shot, individual interconnections or entire neurons once the DNN has been trained, while others methods are applied iteratively, and require multiple rounds of training. These techniques eliminate interconnections but do not alter the underlying Multiply-and-ACcumulate (MAC) paradigm that governs the neuron's inner functioning.

In [1; 2], the authors address the challenge of designing neural networks that can have a smaller memory footprint presenting a novel neuron model based on the Multiply-And-Max/min (MAM) paradigm that can be substituted to classical MAC neurons in the hidden layers of a DNN to allow a more aggressive pruning of interconnections, while substantially preserving the network performance. In a standard MAC-based neuron, inputs are modulated independently of each other through multiplication with their respective weights, and the resulting products are then summed into a single quantity. As MAC neurons, MAM neurons multiply each input by a weight but then only the maximum and the minimum quantity of the products are summed together.

In formulas, if \(v_{1},v_{2},\) are the inputs after being multiplied by their respective weights, the output \(u\) of a MAM neuron is

\[u=[_{j}v_{j}+_{j}v_{j}+b]^{+}\] (1)

where \(b\) is the bias and \([]^{+}=\{0,\}\) represents the nowadays common ReLU nonlinearity.

It is shown empirically that, starting from an architecture originally designed using MAC neurons, one may substitute them with MAM neurons in several hidden layers and use a proper training strategy to achieve the same performances as the corresponding MAC-only network. Yet, in the resulting hybrid network, one may leverage the extremely selective behaviour of min and max operations to reduce very aggressively the number of weights. MAM neurons can be pruned with almost every technique proposed in the literature with little to no modifications. As a motivating example, Table 1 reports some of the results described in  showing cases in which, once the quality level is set (in this case to 3% less accuracy than the original non-pruned network), MAM neuron substitution, retraining and pruning reduce the number of weights 1 to 2 orders of magnitude more than what is obtained by pruning the original MAC-only network. Moreover, these neurons can also be pruned iteratively requiring less training iterations to guarantee a given accuracy compared to standard MAC neurons.

Though the equivalence between MAC-only and MAM&MAC networks has been demonstrated in practice, a change in the model of some neurons opens the problem of the abstract capability of such hybrid architectures. This contribution is a step forward in clarifying that, despite the locally different input-output relationships, also hybrid MAM&MAC networks enjoy some universal approximation capabilities analogous to those of the MAC-only networks.

### Brief background on universal approximation properties

The development of models with universal approximation properties has been a significant breakthrough in many fields of science and engineering. In 1989  proved that a network with a single hidden layer could approximate any continuous function, given enough hidden neurons. Some years later,  and  showed that also fuzzy systems could approximate any continuous function to arbitrary accuracy. These works were later extended to multiple inputs and outputs, demonstrating the universal approximation properties of fuzzy systems more broadly ([6; 7]). In the following years, a large number of researchers have studied the universal approximation properties of neural networks with MAC neurons in the case of bounded depth and arbitrary width ([8; 9]), bounded width and arbitrary depth ([10; 11; 12]) and bounded width and depth ([13; 14]). In the recent work , authors obtained the optimal minimum width bound of a neural network with arbitrary depth to retain universal approximation capabilities.

The research in this field is still very active and aims at proving the universal approximation capabilities of networks with different architectural or computational paradigm choices, such as deep convolutional neural networks , dropout neural networks , networks representing probability distributions  and spiking neural networks .

    & AlexNet + Cifar-10 & AlexNet + Cifar-100 & VGG-16 + ImageNet \\  Top-1 accuracy (3\% lower & & & \\ than non-pruned network) & 87.69\% & & 63.89\% & 61.03\% \\ Surviving interconnections & & & \\ (MAC) & 1.01\% & & 25.01\% & 10.82\% \\ Surviving interconnections & & & \\ (MAM) & **0.06\%** & **0.26\%** & **0.04\%** \\   

Table 1: Approximate remaining interconnections in the hidden fully-connected layers with one-shot global magnitude pruning built either with MAC or MAM neurons.

Mathematical model

We indicate with \(()\) a fully connected layer in which all neurons are based on the MAM paradigm (1). We consider networks with \(N\) inputs collected in the vector \(=(x_{1},,x_{N})\), two MAM hidden layers producing a vector \(()=(z_{1}(),z_{2}(),)= ^{}(^{}())\) and a single output \(Z()\) produced by a final layer that computes either the normalized linear combination

\[Z()=c_{k}z_{k}()}{_{k}z_{k}()}\] (2)

or the linear combination

\[Z()=_{k}c_{k}z_{k}()\] (3)

We normalize the input domain by assuming \(x_{i}=[0,1]\) for \(i=1,,N\) and indicate with \(^{*}\) the family of functions in (2) while with \(\) the analogous family of functions in (3). Smoothness conditions on our target functions \(f:^{N}\) is formalized by assuming that they belong to \(^{d}(^{N})\), i.e., that their \(d\)-th order derivatives are continuous. Distances between functions are measured by means of the norms defined as

\[\|\|_{k,p}=[_{^{N}}|(x)|^{p} x+k_{j=1}^{N}_{^{N}}|}(x)|^{p}x]^{1/p}\]

with \(k=\{0,1\}\) and \(p 1\).

## 3 Main results

Within the above framework, we prove two theorems that describe the universal approximation properties of DNNs using MAM neurons in the hidden layers.

**Theorem 1**.: _For any function \(f^{0}(^{N})\) and any prescribed level of tolerance \(>0\), there is a \(Z^{*}\) such that \(\|f-Z\|_{0,}\)._

**Theorem 2**.: _For any function \(f^{2}(^{N})\), any prescribed level of tolerance \(>0\) and finite \(p 1\), there is a \(Z\) such that \(\|f-Z\|_{1,p}\)._

The proofs of both theorems are reported in Section 6 and are constructive. In particular, subnetworks in the cascade \(()=^{}(^{ }())\) are identified and programmed to make each \(z_{k}()\) a weakly unimodal piecewise-linear function of the inputs, whose maximum is \(1\) and is reached in a hyperrectangular subset of the domain, while the function vanishes for points far from the center of that hyper-rectangle. The shapes and positions of these functions can then be designed along with the values of the weights \(c_{k}\) so that their combination by means of either (2) or (3) is capable of approximating the target function arbitrarily well as measured either by \(\|\|_{1,p}\) or \(\|\|_{0,}\).

## 4 Examples

Figure 1 proposes a visual representation of the constructions behind Theorem 1 and Theorem 2 for \(N=2\). From left to right, we report the target function \(f:^{2}\)

\[f(x_{1},x_{2})=-2)(4x_{2}-2)(4x_{1}+ )}{1+(4x_{1}-2)^{2}+(4x_{2}-2)^{2}}+3\] (4)

and its approximation \(Z^{*}\) implied by the proof of Theorem 1 and its approximation \(X\) implied by the proof of Theorem 2. In both cases the parameter \(n\) used in Section 6 is set to \(n=7\).

## 5 Limitations

Theorem 1 and Theorem 2 rely on networks in which constraints are put neither on the layer width nor on the total number of neurons. Hence, despite proving universal approximation capabilities, they do not imply _efficient_ approximation. Yet, such theoretical limitation is never strongly experienced in practice, since MAM networks are able to guarantee acceptable performance in real use cases. Nevertheless, a deeper look at universal approximation aimed at meeting efficiency will be the focus of future analysis.

## 6 Network construction and proofs of Theorems

### Network construction

The aim of this subsection is to show that our network can be programmed to make the outputs of the second hidden layer specific weakly unimodal piecewise-linear functions \(z_{k}()\) of the inputs.

**Lemma 1**.: _Let \(z\) be any of the outputs of the second hidden layer. For \(N>1\) and any choice of the quantities \(_{1},,_{N}[0,1]\), \(l_{1},,l_{N} 0\), \(_{1}^{c},,_{N}^{c} 0\), and \(_{1}^{a},,_{N}^{s} 0\), the two MAM hidden layers can be programmed to yield_

\[z()=[1-()]^{+}\] (5)

_where_

\[()=_{i\{1,,N\}}\{0,-_{i}|-l_{i}}{\{_{i}^{i}x_{i}<_{i} ..}}\}\] (6)

Proof of Lemma 1.: We assume that neurons in the first hidden layer come in pairs \((y_{1}^{k},y_{1}^{k},y_{2}^{k},y_{2}^{k},)=^{ }()\) and the output of a pair depends on only one of the inputs.

Without any loss of generality, we assume that \(y_{i}^{}\) and \(y_{i}^{}\) depend only on \(x_{i}\) for \(i=1,,N\) while all the other \(N-1\) input weights are set to \(0\). The other outputs of the first hidden layer are involved in the computation of the outputs of the second hidden layer further to the \(z\) we are considering.

For \(y_{i}^{}\) the non-null input weight is equal to \(}{{_{i}^{}}}\) and the bias is \(-l_{i})}}{{_{i}^{}}}\), while for \(y_{i}^{}\) the non-null input weight is equal to \(}{{_{i}^{}}}\) and bias is \(-l_{i})}}{{_{i}^{}}}\). By recalling (1) one gets

\[y_{i}^{}=[+_{i}-l_{i}}{_{i}^{}}]^{+} y_{i}^{}=[-_{i}-l_{i}}{_{i}^{}}]^{+}\] (7)

In the second hidden layer, the neuron computing the \(z\) we consider has all input weights equal to \(0\) but those connecting to \(y_{1}^{k},y_{1}^{k},,y_{N}^{k},y_{N}^{}\). Non-null input weights are equal to \(-1\) and the bias is \(1\) so that\[z=[_{i\{1,,N\}}\{0,-y_{i}^{k},-y_{i}^{k}\} +_{i\{1,,N\}}\{0,-y_{i}^{k},-y_{i}^{k}\}+1 ]^{+}=[1-_{i\{1,,N\}}\{y_{i}^{k},y_{i}^ {k}\}]^{+}\] (8)

Considering the last expression, note that, if \(x_{i}_{i}\) then \(y_{i}^{k} 0\) and \(y_{i}^{k}=0\) while, if \(x_{i}<_{i}\) then \(y_{i}^{k}=0\) and \(y_{i}^{k} 0\). Hence, without loss of generality, we may assume that \(x_{i}_{i}\) for \(i=1,,N\), being all other cases a variation of this one by suitable symmetry and scaling. With this, \(y_{i}^{k}=0\) for \(i=1,,N\) and (8) becomes

\[z=[1-_{i=1,,N}[-_{i}-l_{i}}{_{i}^{ k}}]^{+}]^{+}=[1-_{i=1,,N}\{0,-_{i }-l_{i}}{_{i}^{k}}\}]^{+}\] (9)

that is equivalent to the thesis. 

To interpret Lemma 1 note that \(()\) is a scaled measure of how far the input vector \(\) is from the hyper-rectangle centered at \(=(_{1},,_{N})\) with sides \(2l_{1},,2l_{N}\). Hence, \(z()\) is maximum and equal to \(1\) if \(\) belongs to such a hyper-rectangle and has a piecewise-linear decreasing profile when \(\) gets further from \(\). Figure 2 reports an example of a \(z()\) when \(N=2\).

In the following, we will assume that each neuron in the second hidden layer matches a whole subnetwork as implied by Lemma 1. With this, we may re-index the outputs of the second hidden layer as \(z_{}()\) associating each of them with the center of the hyper-rectangle in which \(z_{}()=1\). The same is done with the corresponding weights \(c_{}\) in the output layers.

### Universal approximation properties with normalized linear output neuron

Given a positive integer \(n\), define \(=\{0,,,,1\}^{N}\) and include in the two hidden layers all the subnetworks implied by Lemma 1 to implement the function \(z_{}()\) for each \(\).

In each of these subnetworks set \(_{i}^{}=_{i}^{}==}{{n}}\) for \(i=1,,N\) and \(l_{i}=0\) for \(i=1,,N\).

With this, \(z_{}()\) is and \((N+1)\)-dimensional pyramid whose base is an \(N\)-dimensional hypercube with sides of length \(2\) and center in \(\).

Proof of Theorem 1.: Note first that for any given \(^{N}\), only a limited number of functions \(z_{}()\) are not null. In particular, if \(k_{i}= nx_{i}\) for \(i=1,,N\) is the largest integer not exceeding \(nx_{i}\), then \(z_{}()>0\) only if \(\) belongs to the set \(_{}=\{k_{1},(k_{1}+1)\} \{k_{N},(k_{N}+1)\}\) that contains the \(2^{N}\) corners of the \(N\)-dimensional hypercube \(C_{}=[k_{1},(k_{1}+1)][k_ {N},(k_{N}+1)]\). Hence, we may evaluate \(Z()\) focusing on functions \(z_{}()\) with \(_{}\).

Define the functions

Figure 2: Three dimensional plot of a generic \(z_{}()\) for \(N=2\) and its contour plot showing the role of the various parameters.

\[_{}(x)=}()}{_{ ^{}}z_{^{}}()}\] (10)

that are such that \(_{}_{}()=_{_{}}_{}()=1\) for any \(^{N}\), and set \(c_{}=f()\) for each \(\).

The error \(\|f()-Z()\|_{0,}\) in Theorem 1 can be written as

\[\|f()-_{_{}}f( )_{}()\|_{0,}= \|_{_{}}[f()-f ()]_{}()\| _{0,}_{^{N}}_{  C_{}\\ _{x}}|f()-f( )|\]

Since \(f:^{N}\) is continuous on the compact domain \(^{N}\), it is also uniformly continuous and, for any given level of tolerance \(>0\), there is a \( x\) such that for any \(^{}\),\(^{}^{N}\) with distance \(\|^{}-^{}\|_{2} x\) we have \(|f(^{})-f(^{})| \). For a given \(\), the distance between any \( C_{}\) and any \(_{}\) is \(\|-\|_{2}\). Since \(=1/n\) we can select \(n\) so that

\[\|f()-Z()\|_{0,}_{ ^{N}}_{ C_{ }\\ _{}}|f()-f ()|\]

### Universal approximation properties with linear output neuron

In this case, the approximation capabilities of our network over the whole domain depend on the local behaviour of subnetworks converging not in a single second-hidden-layer neuron but in \(2N\) second-hidden-layer neurons.

Formally, given a center \(^{N}\) we include in a subnetwork neurons of the second hidden layer with outputs labelled \(z_{^{1-}},z_{^{1+}}\),...,\(z_{^{N-}},z_{^{N+}}\) as well as all the previous neurons needed to compute such outputs.

The expression of each \(z_{^{j,k}}\) is given by Lemma 1 and thus is defined by the center point \(^{j}=(_{1}^{j},,_{N}^{j})\), by the slopes \(_{1}^{,j},,_{N}^{,j}\) and \(_{1}^{,j},,_{N}^{,j}\), as well as by the side lengths \(l_{1}^{j},,l_{N}^{j}\).

In a subnetwork, everything depends on two quantities \(, 0\) that are used to set

\[_{i}^{j}=_{i}&i j \\ _{i}&i=j l_{i}^{j}=&i  j\\ 0&i=j\\ \\ _{i}^{,j-}=&_{i}^{,j+}=&i j\\ 2&i=j\\ \\ _{i}^{,j+}=&\]

for \(i,j=1,,N\).

To give some intuitive grounding to the above definitions, Figure 3 reports example profiles for \(4\) output functions \(z_{^{1-}},z_{^{1+}},z_{^{2-}},z_{^{ 2+}}\) with \(N=2\).

Given a center \(\), the same quantities \(\) and \(\) allow to define the two domain subsets

\[X_{}^{}=\{^{N}\ |\ _{i=1,,N}\{|x_{i}-_{i}| \}.\} X_{}^{}=\{^{N}\ |\ <_{i=1,,N}\{|x_{i}-_{i}| \}+.\}\]

as well as \(X_{}=X_{}^{} X_{}^{}\).

The approximation capabilities depend on the behaviour of the output of the subnetworks in the three disjoint domains \(X_{}^{}\), \(X_{}^{}\), and \(^{N} X_{}\).

It is easy to see that if \(^{N} X_{}\) then \(z_{^{j}}=0\) for \(j=1,,N\).

For \( X_{}^{}\) the following Lemma holds.

**Lemma 2**.: _Given any choice of \(N+1\) coefficients \(a\) and \(b_{j}\) for \(j=1,,N\), one may choose \(2N\) weights \(c^{j}\) with \(j=1,,N\) such that_

\[Z_{}()_{j=1}^{N}c^{j}z_{^{ j}}()=a+_{j=1}^{N}b_{j}x_{j}\] (11)

_for any \( X_{}^{}\), where \(Z_{}()\) remains implicitly defined._

Proof of Lemma 2.: Due to the definition of \(^{j}\) we have

\[X_{}^{} = [_{1}-,_{1}+] [_{N}-,_{N}+]=[_{1}^{1-},_{1} ^{1+}][_{N}^{N-},_{N}^{N+}]\]

Hence, if \( X_{}^{}\) we know that \(_{j}^{j-} x_{j}_{j}^{j+}\) for \(j=1,,N\).

Moreover, since by definition for any \(i,j=1,,N\) and \(i j\) we have \(_{i}^{j+}-_{i}^{j-}=2\) and \(_{i}^{j-}+_{i}^{j+}=2_{i}\), then \(|x_{i}-_{i}^{j}|\) when \(i j\). Therefore, one can apply Lemma 1 and compute \(()\), for which all the terms in (6) but \(|x_{j}-_{j}^{j}|\) are non-positive, thus yielding \(z_{_{i}}()=1-|x_{j}-_{j}^{j}|\! ||||||||||||| ||||||||||||| ||||||||||||| |||\|||||||| |||||}|).\)

Without any loss of generality, translate \(X_{}\) so that \(=(,,)\). This implies \(_{j}^{j-}=0\) and \(_{j}^{j+}=2\) for \(j=1,,N\), thus yielding \(z_{^{j-}}()=1-}{2}\) and \(z_{^{j+}}()=}{2}\). With this,

\[_{j=1}^{N}c^{j}z_{^{j}}() = _{j=1}^{N}[c^{j-}(1-}{2})+c^{j +}}{2}] = _{j=1}^{N}c^{j-}+_{j=1}^{N}(c^{j+}-c^{j-}) }{2}\]

that can yield any affine function \(f(x)=a+_{j=1}^{N}b_{j}x_{j}\) by setting, for \(j=1,,N\),

\[c^{j-}= c^{j+}=c^{j-}+2 b_{j}\] (12)

Finally, what happens for \( X_{}^{}\) is described by the following Lemma.

**Lemma 3**.: _If the \(2N\) weights \(c^{j}\) with \(j=1,,N\) are set according to Lemma 2 so that \(Z_{}()=a+_{j=1}^{N}b_{j}x_{j}\) for any \(x X_{}^{}\), for coefficients satisfying \(|a|,|b_{j}| M\) for some \(M>0\) and \(j=1,,N\), then \(|Z_{}()| 3MN\) for any \( X_{}\) and thus for any \( X_{}^{}\)._Proof of Lemma 3.: From \(|a|,|b_{j}| M\) and from (12) we get \(|c^{j-}|}{{N}}\) and \(|c^{j+}|}{{N}}+2 M\).

Overall, since \( 1\) and \(N 1\) we have \(|c^{j}| 3M\). Since \(0 z_{^{j}} 1\) and \(Z_{}()=_{j=1}^{N}c^{j}z_{^{j}}()\) we finally get the thesis. 

The above characterization of the output of \(Z\)-subnetworks allows to prove their local approximation capabilities.

**Lemma 4**.: _Given any function \(f^{2}(^{N})\), there are two constants \(P,Q>0\) such that_

\[E_{} \] \[ (2+2)^{N}\{P^{p}[1-o( }{{}})]+Qo(}{{}} )\}\]

_with \(o()=1-}{{(1+)}^{N}}\)_

Proof of Lemma 4.: Since \(f^{2}(^{N})\) and \(^{N}\) is compact, \(M_{0},M_{1},M_{2} 0\) exists such that

\[|f()| M_{0},|}()| M_{1},|f}{ x_{i}x_{j}}()| M_{2}\] (13)

for any \(^{M}\) and \(i,j=1,,N\).

Assuming \( X_{}^{}\), and thus \(|x_{i}-_{i}|\), the above bounds can be used jointly with the Taylor expansions of \(f\) and its derivatives around \(\)

\[f() = f()+_{i=1}^{N}}()(x_{i}-_{i})+_{i=1 }^{N}_{j=1}^{N}R_{i,j}()(x_{i}-_{i}) (x_{j}-_{j})\] (14) \[}() = }()+_{j=1 }^{N}S_{i,j}()(x_{j}-_{j}) i=1,,N\] (15)

to ensure that their error terms satisfy

\[|_{i=1}^{N}_{j=1}^{N}R_{i,j}()(x_{i}- _{i})(x_{j}-_{j})| N^{2}^{2} _{k,l=1,,N}_{^{N}}| f}{ x_{k}x_{l}}()|M_{2}N^{2} ^{2}\] (16)

and

\[|_{j=1}^{N}S_{i,j}()(x_{j}-_{j}) | N^{2}^{2}_{j=1,,N}_{ ^{N}}|f}{ x_{i}x_{j}}( )|M_{2}N i=1,,N\] (17)

Again focusing on \( X_{}^{}\), exploit Lemma 2 to set the weights \(c^{j}\) yielding

\[Z_{}()=f()+_{i=1}^{N} }()(x_{i}-_{i} )=[f()-_{i=1}^{N}}()_{i}]+_{i=1}^{N}}()x_{i}\] (18)

which is also such that \(}}{ x_{i}}()=}()\).

Hence, we may program \(Z_{}\) to reproduce the behaviour of \(f\) and its derivatives in \(X_{}^{}\), and the approximation errors can be derived exploiting (14) with (16) and (15) with (17) to obtain

\[|Z_{}()-f()|M_{2}N^{2}^{2},|}}{  x_{i}}()-}()|M_{2}N\] (19)To address the case \( X_{}^{}\), we may apply Lemma 3. By matching (18) with (13) we get that \(|a| M_{0}+M_{1}N\) and \(|b_{i}| M_{1} M_{0}+M_{1}N\) for \(i=1,,N\). Hence, if \(x X_{}^{}\), then if \(M_{3}=M_{0}(1+3N)+3M_{1}N^{2}\) we have

\[|Z_{}()-f()| M_{3},|}}{ x_{i}}( )-}()|=| }()-}()| 2M_{1}\] (20)

Since we have different error bounds in \(X_{}^{}\) and \(X_{}^{}\), we bound the overall error \(E_{}\) by splitting

\[E_{} = _{X_{}^{}}|f()-Z_{ }()|^{p}+_{j=1}^{N}_ {X_{}^{}}|}()-}}{ x_{j}}() |^{p}+\] \[_{X_{}^{}}|f()-Z_ {}()|^{p}+_{j=1}^{N}_ {X_{}^{}}|}( )-}}{ x_{j}}( )|^{p}+\]

and apply (19) and (20) to bound each integrand. Adding the fact that the measure of \(X_{}^{}\) is \((2)^{N}\), while the measure of \(X_{}^{}\) is \((2+2)^{N}-(2)^{N}\) we obtain

\[E_{}[(M_{2}N^{2}^{2})^{p}+( M_{2}N)^{p}](2)^{N}+[M_{3}^{p} +(2M_{1})^{p}][(2+2)^{N}-(2)^{N}]\]

from which we may set \(P=(M_{2}N^{2})^{p}+(M_{2}N)^{p}\) and \(Q=M_{3}^{p}+(2M_{1})^{p}\) to get the thesis. 

We are now in the position of proving our second result.

Proof of Theorem 2.: For \(n>0\) integer define \(\) and \(\) such that \(=^{2}\) and \(2+2=}{{n}}\). Let also \(=\{,,,\}^{N}\) so that \(^{N}\) is partitioned in \(n^{N}\) hyper-cubes \(X_{}\) with centers \(\) and side \(2+2\). The output of the whole network is \(Z()=_{}Z_{}()\).

Since \(Z_{}()\) is null for \( X_{}\), the error measure over \(^{N}\) can be decomposed into

\[\|f-Z\|_{1,p}^{p}=_{}\{_{X_{}}|f()-Z_{}()|^{ p}+_{j=1}^{N}_{X_{}}|}()-}}{ x _{j}}()|^{p}\}\]

Each of the terms in the last sum can be bounded using Lemma 4 in which we may also substitute \(2+2=}{{n}}\) and \(=^{2}\) to yield

\[\|f-Z\|_{1,p}^{p}_{}} \{P^{p}[1-o()]+Qo()\}=P^{p}[1-o( )]+Qo()\]

Since when \(n\) we have \( 0\) and thus \(o() 0\) the thesis is proven. 

## 7 Conclusions

We established that neural networks in which hidden MAC neurons are substituted with MAM neurons to obtain more aggressively prunable architectures are still universal approximators.