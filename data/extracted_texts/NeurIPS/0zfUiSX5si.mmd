# AdaNovo: Towards Robust _De Novo_ Peptide Sequencing in Proteomics against Data Biases

Jun Xia\({}^{1}\), Shaorong Chen\({}^{1}\), Jingbo Zhou\({}^{1}\), Xiaojun Shan\({}^{2}\),

Wenjie Du\({}^{3}\), Zhangyang Gao \({}^{1}\), Cheng Tan\({}^{1}\), Bozhen Hu\({}^{1}\), Jiangbin Zheng\({}^{1}\), Stan Z. Li\({}^{1}\)

\({}^{1}\)School of Engineering, Westlake University

\({}^{2}\)University of California San Diego \({}^{3}\)University of Science and Technology of China

{xiajun, stan.zq.l1}@westlake.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the high-throughput analysis of protein composition in biological tissues. Despite the development of several deep learning methods for predicting amino acid sequences (peptides) responsible for generating the observed mass spectra, training data biases hinder further advancements of _de novo_ peptide sequencing. Firstly, prior methods struggle to identify amino acids with Post-Translational Modifications (PTMs) due to their lower frequency in training data compared to canonical amino acids, further resulting in unsatisfactory peptide sequencing performance. Secondly, various noise and missing peaks in mass spectra reduce the reliability of training data (Peptide-Spectrum Matches, PSMs). To address these challenges, we propose AdaNovo, a novel and domain knowledge-inspired framework that calculates Conditional Mutual Information (CMI) between the mass spectra and amino acids or peptides, using CMI for robust training against above biases. Extensive experiments indicate that AdaNovo outperforms previous competitors on the widely-used 9-species benchmark, meanwhile yielding 3.6% - 9.4% improvements in PTMs identification. The code for reproducing the results is available at: [https://github.com/Westlake-OmicsAI/adanovo_v1](https://github.com/Westlake-OmicsAI/adanovo_v1).

## 1 Introduction

Proteomics research focuses on large-scale studies to characterize the proteome, the entire set of proteins in a living organism. Tandem mass spectrometry serves as the only high-throughput method to analyze the protein composition in complex biological samples, playing an essential role in drug target discovery , PTMs discovery  and precision medicine . Peptide sequencing, i.e., predicting the peptide sequence for each observed mass spectrum, is at core of proteomics .

Currently, two mainstream methods are employed in peptide sequencing: database search and _de novo_ peptide sequencing. The database search approaches  compare the observed spectrum against the spectra in a pre-constructed PSMs database and pick the peptide sequence of the most similar spectrum as the identification result. Obviously, database search cannot sequence the peptides out of the database. In contrast, _de novo_ peptide sequencing deduces peptide sequence without prior knowledge of the database and thus it is essential in applications where the database is not available, such as antibody sequencing, human leukocyte antigen neoantigen sequencing, and identification of new proteins and peptides which are missing from the database.

Since the early 1990s, _de novo_ sequencing methods based on the graph theory [2; 8], Hidden Markov Model , or dynamic programming [3; 15; 7] were developed to score peptide sequences against observed spectra. With the prosperity of deep learning, some researchers train the deep neural networks with mass spectrum as the input and peptide sequence as the label [27; 19; 35]. Although these methods have achieved notable progress, as shown in Figure 1, we observe that they struggle to identify the amino acids with PTMs (such as the oxidation of methionine shown in Figure 1(a)), further leading to low peptide sequencing performance. On the other hand, the identification of amino acids with PTMs holds significant biological importance because PTMs plays a pivotal role in elucidating protein function and studying disease mechanisms . Additionally, some expected peaks in mass spectra may be missing due to instrument malfunction or multiple cleavage events occurring on the peptides, and some additional peaks may undesirably appear in the spectrum, created by instrument noise or non-peptide molecules in the biological samples. All of these make the spectra and peptides labels for training being poorly matched.

To address above issues, we propose a novel framework, AdaNovo, to calculate the conditional mutual information (CMI) between the spectrum and each amino acid in its peptide label. This is inspired by the domain knowledge that the mass shifts of PTMs over canonical amino acids are only manifested in the mass spectrum. The CMI can measure the importance of different target amino acids by their dependence on the source spectrum. Based on the amino acid-level CMI, we can obtain the Mutual Information (MI) between the spectrum and the entire peptide to measure the matching level of each spectrum-peptide pair in the training PSM data. Subsequently, we design a robust training approach based on both the amino acid- and PSM-level CMI or MI, which re-weights the training losses of the corresponding amino acids adaptively.

The extensive experiments on the 9-species benchmark  indicate that AdaNovo generally outperforms state-of-the-art _de novo_ peptide sequencing methods in amino acid-level or peptide-level precision and demonstrates significantly higher precision in identifying the amino acids with PTMs.

## 2 Background and Related Work

As shown in Figure 2, in a standard protein identification workflow of shotgun proteomics , proteins undergo initial digestion by enzymes, yielding a mixture of peptides. The peptides are then separated using liquid chromatography. Each charged peptide is analysed by mass spectrometer, which produces the first scan (MS1) spectra, displaying the mass-to-charge (_m_/\(\)) ratio of the intact peptide. And then, each peptide will be fragmented in the mass spectrometer, and each generated second scan (MS2) spectrum comprises a collection of peaks. Each peak is tuple constitutes _m_/\(\) value and an associated intensity value. The core of the above pipeline is the **peptide sequencing**, where we aim to predict the peptide sequence using the observed MS2 spectrum and the corresponding precursor information (mass and charge of the intact peptide). Finally, we can infer the entire protein sequence using assembly methods . There exist two lines of works for peptide sequencing.

Figure 1: (**a**): An example of PTMs (oxidation of methionine). (**b**): Comparisons of previous _de novo_ sequencing methods in terms of amino acid-level precision. ‘G’ and ‘A’ denote Glycine and Alanine, respectively. Both of them are canonical amino acids. ‘M(+15.99)’ and ‘Q(+.98)’ represent oxidation of methionine and deamidation of glutamic, both of which are modified amino acids (the amino acids with PTMs). The results are reported using the human dataset in 9-species benchmark as test set.

The first line is **database search**, where we compare the observed mass spectra against the theoretical fragmentation mass spectra of peptide sequences in the database and pick the peptide sequence with the highest matching score as the result. Typical methods and tools include SEQUEST , pFind , MSFragger  and Open-pFind . However, these methods cannot sequence the peptides out of the database.

The second line of works is _de novo_ peptide sequencing, where we predict the peptide sequences for observed spectra without relying on pre-constructed databases. Initially, researchers cast the _de novo_ peptide sequencing task as finding the largest path in the spectrum graph  or compute the best sequences whose fragment ions can best interpret the peaks in the observed MS2 spectrum using Hidden Markov Model  or dynamic programming algorithm .

With the prosperity of deep learning, DeepNovo  is the first method applying deep neural networks to the task of _de novo_ peptide sequencing. It regards the task as the image caption  in computer vision and incorporates the encoder-decoder architecture to predict the peptide sequence. To annotate the high-resolution mass spectrometry data, PointNovo  adopts an order invariant network structure for peptide sequencing. More recently, Casanovo  first employs a transformer encoder-decoder architecture  to predict the peptide sequence for the observed spectra. SearchNovo  integrates the strengths of database search and _de novo_ sequencing to enhance peptide sequencing.

Although _de novo_ peptide sequencing methods have achieved notable progress, we observe that they have difficulty in identifying the amino acids with PTMs because these amino acids occur much less frequently in datasets compared to other canonical ones. Additionally, mass spectrometry data contains a significant amount of noise. All of these make the peptides labels being less reliable. The AdaNovo model proposed in this paper effectively alleviates both of them.

## 3 Methods

### Task Formulation

Formally, we denote mass spectrum peaks in a MS2 spectrum as \(=\{(m_{i},t_{i})\}_{i=1}^{M}\), where each peak \((m_{i},t_{i})\) forms a 2-tuple representing the _m/z_ and intensity value, and \(M\) is the number of peaks that can be varied across different mass spectra. Also, we denote the precursor as \(=\{(m_{prec},c_{prec})\}\), consisting of the total mass \(m_{prec}\) and charge state \(c_{prec}\{1,2,,10\}\) of the spectrum. Additionally, we represent the peptide sequence as \(=\{(y_{1},y_{2},,y_{N})\}\), where \(y_{i}\) is the type of the \(i\)-th amino acid, \(N\) is the peptide length and can be varied across different peptides. \(_{<j}\) means the previous amino acids sequence appearing before the index \(j\) in the peptide \(\). The _de novo_ peptide

Figure 2: The identification workflow of shotgun proteomics . The peptide sequencing task is to predict the peptide sequence (e.g., ATASPPRQK) for the observed MS2 spectrum, where peaks in blue are signal peaks (real ions) and grey peaks denote noisy ones. The spectrum annotation are obtained with ProteomeXchange .

sequencing models are designed to predict the probability of each amino acid \(y_{i}\) given \(\), \(\) and \(_{<j}\):

\[P(,;)=_{j=1}^{N}p(y_{j} _{<j},,;), \]

where \(j\) is the index of each amino acid position in the peptide sequence and \(\) is the model parameter. In general, previous models [27; 35; 19] are optimized using the cross-entropy (CE) loss:

\[_{}()=-_{j=1}^{N} p(y_{j}_{<j},,;). \]

During inference, these models typically predict the probabilities of target amino acids in an autoregressive manner and generate hypotheses using heuristic search algorithms like beam search .

### Model Architectures

As shown in Figure 3, AdaNovo consists of a mass spectrum encoder (MS Encoder) and two peptide decoders (Peptide Decoder #1 and Peptide Decoder #2). All of these models are built on Transformer . In order to feed the mass spectrum peaks to MS Encoder, following Casanovo, we regard each mass spectrum peak \((m_{i},t_{i})\) as a 'word' in natural language processing and obtain the peak embedding by individually encoding its _m/z_ value (\(m_{i}\)) and intensity value (\(t_{i}\)) before combining them through summation. We employ the similar embedding approach for the precursor \(=\{(m_{prec},c_{prec})\}\). As the embedding method is not our original contribution, we introduce the details in Appendix A. As for the peptide sequence, the amino acid vocabulary encompasses the 20 canonical amino acids, 3 PTMs (oxidation of methionine, deamidation of asparagine or glutamine) and a special [stop] token indicates the end of decoding. Peptide Decoder #1 and Peptide Decoder #2 undergo autoregressive training, wherein they receive the previous sequence \(_{<j}\) prior to amino acid \(y_{j}\) during the prediction process. However, different from Peptide Decoder #1, Peptide Decoder #2 exclusively employs previous sequence \(_{<j}\) as input because we want to calculate the conditional probability \(p(y_{j}_{<j})\), which is the prerequisite for calculating the conditional mutual information between the mass spectrum (\(\) and \(\)) and amino acids.

### Training Strategies

The training strategies consist of amino acid-level (SS 3.3.1) and PSM-level training methods (SS 3.3.2).

#### 3.3.1 Amino Acid-level Training Objective

As mentioned above, previous _de novo_ peptide sequencing models struggle to identify amino acids with PTMs because they occur much less frequently in datasets compared to other canonical amino

Figure 3: Schematic diagram of AdaNovo framework.

acids. Therefore, we expect to emphasize the amino acids with PTMs to improve the models' ability in identifying them during inference. This resembles the up-sampling methods in long-tailed classification where researchers emphasize samples from the tail class during training . We explain the reasons why these methods are unsuitable to _de novo_ peptide sequencing and compare AdaNovo with them in Section 4.6. On the other hand, when predicting the amino acid with PTMs \(y_{j}\), we should rely more on mass spectrometry data (peaks x and precursor z) and less on the historical predictions of previous amino acids sequence \(y_{<j}\) because the mass shifts resulting from PTMs are only manifested in the mass spectrometry data. This unique attribute of PSMs data motivates us to measure the mutual information (MI) between each target amino acid (\(y_{j}\)) and the mass spectrum conditioned on previous amino acids, i.e., conditional mutual information (CMI)  between target amino acid and mass spectrum. Given that \(,y_{j},,_{<j}\) are drawn from the underlying random variable \(X,Y_{j},Z,Y_{<j}\), respectively, we can calculate the conditional mutual information (CMI) as,

\[ I(X,Z;Y_{j} Y_{<j})&= _{(X,Y_{j},Z)}\{(, _{<j})}{p(y_{j}_{<j})  p(,_{<j})})\}\\ &=_{(X,Y_{j},Z)}\{( ,,_{<j}) p(,_{<j})}{p(y_{j}_{<j}) p (,_{<j})})\}\\ &=_{(X,Y_{j},Z)}\{( ,,_{<j})}{p(y_{j}_{<j} )})\}. \]

In this way, the CMI can be obtained with \(p(y_{j},,_{<j})\) and \(p(y_{j}_{<j})\), which are the output of the Peptide Decoder #1 and Peptide Decoder #2, respectively. Each data point \((,,y_{j})\) is independently sampled from the joint distribution uniformly. Therefore, we can measure the dependence between mass spectrometry \((,)\) and \(y_{j}\) conditioned on \(_{<j}\) using \(I_{j}=(,,_{<j} )}{p(y_{j}_{<j})})\). Moreover, to reduce the variances and stabilize the distribution of the amino acid-level CMI in each peptide, we normalize the CMI values in the peptide using Z-score normalization and then scale the normalized values to obtain the amino acid-level training weight \(w_{j}^{aa}\) for \(y_{j}\),

\[w_{j}^{aa}=(s_{1}-^{aa}+^{aa}}{^{aa}},0), \]

where \(^{aa}\) and \(^{aa}\) are the mean values and the standard deviations of all the CMI values in each peptide, and \(s_{1}\) is a hyperparameter that controls the effect of amino acid-level adaptive training.

#### 3.3.2 PSM-level Training Objective

As we introduced before, the training PSMs samples are of different matching levels because of the unexpected signal noise and missing peaks. To alleviate the negative effect of poorly matched mass spectrometry and peptide pairs and encourage the well-matched ones, we adopt the mutual information between them as a measure of matching levels. Formally,

\[ I(X,Z;Y)&=_{(X,Y,Z)}\{ (,)}{p()}) \}=_{(X,Y,Z)}\{(^{N}p(y_ {j},,_{<j})}{_{j=1}^{N}p(y_ {j}_{<j})})\}\\ &=_{(X,Y,Z)}\{_{j=1}^{N}(,,_{<j})}{p(y_{j} _{<j})})\}=_{(X,Y,Z)}\{_{j=1}^{ N}I_{j}\}. \]

In other words, the mutual information can be derived by summarizing all the amino acid-level \(\) over the peptide. Similarly, we can measure the matching level between mass spectrometry \((,)\) and the peptide \(()\) using \(=_{j=1}^{N}I_{j}\). And then, we normalize all the \(\) values across all the PSMs in each mini-batch and scale the normalized values to obtain the PSM-level training weight \(w^{psm}\).

\[w^{psm}=(s_{2}\!-\!^{psm}+^{psm}}{^{ psm}},0), \]

where \(^{psm}\) and \(^{psm}\) are the mean values and the standard deviations of the MI values of all the PSMs in each minibatch, and \(s_{2}\) is a hyperparameter that controls the effect of PSM-level adaptive training. We studied the influence of \(s_{1}\) and \(s_{2}\) in Section 4.7.

#### 3.3.3 Overall Training Objective

In the proposed method, we re-weight each target amino acid \(y_{j}\) with the following loss,

\[_{1}(_{1})=-_{j=1}^{N}w_{j} p(y_{j}_ {<j},,;_{1}), \]

where \(_{1}\) are the parameters of MS Encoder and Peptide Decoder #1, and

\[w_{j}=w_{j}^{aa} w^{psm}. \]

Additionally, Peptide Decoder #2 is trained with the following loss,

\[_{2}(_{2})=-_{j=1}^{N} p(y_{j}_{<j} ;_{2}), \]

where \(_{2}\) are the parameters of Peptide Decoder #2. The overall training loss is,

\[_{}(_{1},_{2})=_{1}(_{1})+ _{2}(_{2}). \]

### Inference Strategies

In the inference phase, we feed the mass spectrometry to the encoder MS Encoder and the decoder Peptide Decoder #1 predicts the highest-scoring amino acid for each peptide sequence position. The decoder is then fed its preceding amino acid predictions at each decoding step. The decoding process concludes upon predicting the [stop] token or reaching the predefined maximum peptide length of \(=100\) amino acids. We discuss the computational overhead of AdaNovo in Section 4.8.

## 4 Experiments

### Datasets

We employ the nine-species benchmark initially introduced by DeepNovo . This dataset amalgamates approximately 1.5 million mass spectra from nine distinct species, all employing the same instrument but analyzing peptides from different species. Each spectra is associated with a ground-truth peptide sequence, which comes from database search identification with a standard false discovery rate (FDR) set at 1%. Following previous works , we adopt a leave-one-out cross-validation framework. This entails training a model on eight species and testing it on the species held out for each of the nine species. We also split the eight species into training set and validation set with the ratio 9:1. This framework facilitates the testing of the model on peptide samples that have never been encountered before, which is precisely the advantage of _de novo_ peptide sequencing methods over database search methods.

### Evaluation Metrics

In our assessment of model predictions, we employ precision calculated at both the amino acid and peptide levels, following methodologies presented by previous works . We first calculates the number of matched amino acid predictions, \(N_{}^{aa}\), which are defined as predicted amino acids that exhibit a mass difference of \(<0.1\) from the real amino acids and have either a prefix or suffix with a mass difference of \( 0.5\) from the corresponding real amino acid sequence in the ground truth peptide. Amino acid-level precision is then defined as \(N_{}^{aa}/N_{}^{aa}\), where \(N_{}^{aa}\) represents the number of predicted amino acids in predicted peptide sequences. Similarly, PTMs identification precision can be formulated as \(N_{}^{brm}/N_{}^{brm}\), where \(N_{}^{brm}\) and \(N_{}^{brm}\) denote the number of matched PTMs and predicted amino acids with PTMs, respectively. For peptide prediction, a predicted peptide is deemed a correct match only if all of its amino acids are matched. In a collection of \(N_{}^{p}\) spectra, if a model accurately predicts \(N_{}^{p}\) peptides, the peptide-level precision are defined as \(N_{}^{p}/N_{}^{p}\). Kindly note that peptide-level performance measures are the primary quantifier of the model's practical utility because the goal is to assign a complete peptide sequence to each spectrum.

[MISSING_PAGE_FAIL:7]

Section 4.3. The results shown in Table 2 indicate that both modules are necessary and effective for the AdaNovo model. More specifically, when we remove the AA-level training strategy in AdaNovo, the precision of the amino acids with PTMs identification drops significantly because the amino acid-level training strategy is designed for identifying amino acids with PTMs.

**Performance on mass spectra with synthetic noise.** To verify the effectiveness of the PSM-level adaptive training strategy, we randomly choose 20% spectrum in the training datasets, and add synthetic noise peaks or remove original peaks with higher intensity values. We report the results in Table 3, from which we can observe that the performance would degrade sharply when we remove the PSM-level training strategy. This indicates that PSM-level adaptive training strategy can enhance models' robustness against data noise in mass spectrum.

### Comparisons with Alternative Methods for identifying amino acids with PTMs

In this section, we show the performance of AdaNovo only with amino acid-level loss (denoted as 'AdaNovo w/o PSM-level objective') and compare to some alternative methods in terms of identifying amino acids with PTMs. The first alternative is to re-weight each amino acid \(y_{j}\) with \(w_{j}=N_{total}/N_{y_{j}}\), where \(N_{total}\) and \(N_{y_{j}}\) represent the total number of amino acids and the number of amino acids in the \(y_{j}\) category in the dataset, respectively. The second alternative is the focal loss , we replace the cross entropy loss of Casanovo  with the focal loss,

\[=-(1- p(y_{j},,_{<j}))^{ } p(y_{j},,_{<j}),\]

where \(\) and \(\) are hyperparameters to adjust the loss weight. The results shown in Table 4 indicate that both AdaNovo and the first alternative can help improve Casanovo's ability. Moreover, AdaNovo outperforms the first alternative by a notable margin probably because the training and testing datasets are derived from different species, there exists a significant difference in the distribution of PTMs quantities. Therefore, the above \(w_{j}\) obtained with the train set is not suitable for test set. Also, AdaNovo is inspired by the domain knowledge that the mass shift of PTMs only be manifested in the mass spectra, thus shows superiority over the re-weighting methods in long-tailed classification.

   Model & AA. Prec. & Peptide Prec. & PTM prec. \\  Casanovo & 0.753 & 0.568 & 0.552 \\ + Re-weight (Upsampling) & 0.762 & 0.576 & 0.564 \\ + Focal loss & 0.745 & 0.543 & 0.550 \\ AdaNovo (w/o PSM-level objective) & 0.793 & 0.594 & 0.616 \\ AdaNovo & 0.825 & 0.612 & 0.665 \\   

Table 4: Comparisons with alternative methods in terms of identifying amino acids with PTMs. All results are reported using the yeast dataset as test set.

   Model & AA. Prec. & Peptide Prec. & PTM Prec. \\  Casanovo & 0.585 & 0.343 & 0.300 \\ AdaNovo (w/o PSM-level objective) & 0.607 & 0.360 & 0.478 \\ AdaNovo (w/o AA-level objective) & 0.594 & 0.349 & 0.314 \\ AdaNovo & 0.618 & 0.373 & 0.483 \\   

Table 2: Ablations on amino acid-level (AA-level) and peptide-level training strategies. The results are reported using the Human datasets as test set.

   Model & AA. Prec. & Peptide Prec. \\  Casanovo & 0.582 & 0.297 \\ AdaNovo (w/o PSM-level objective) & 0.617 & 0.336 \\ AdaNovo (w/o AA-level objective) & 0.644 & 0.372 \\ AdaNovo & 0.656 & 0.397 \\   

Table 3: Models’ Performance on mass spectrum dataset with synthetic noise. The results are reported using the Clam bacteria as test set.

### Sensitivity Analysis

In this section, we investigate the effects of the two hyperparameters \(s_{1}\) and \(s_{2}\), which determines the influence of amino acid-level and PSM-level training strategy. As shown in Figure 5, we tune both \(s_{1}\) and \(s_{2}\) within the range \([0.05,0.1,0.3]\) and observe that the values of these two hyperparameters significantly affect the final performance of the model. Additionally, the optimal hyperparameters for peptide-level metrics may be sub-optimal to amino acids-level metrics. It is necessary to finely adjust the values of \(s_{1}\) and \(s_{2}\) based on the dataset, striking the balance between amino acid-level and PSM-level training strategies.

### Costs of Computing and Storage

In this part, we compare AdaNovo with Casanovo in terms of the number of model parameters, training time and inference time. The results shown in Table 5. Although AdaNovo outperforms Casanovo in peptide sequencing and PTMs identification, it inevitably introduced extra parameters (Peptide Decoder #2), resulting in a 40.04% increase in parameter count (from 47.35M to 66.31M). Also, under the same hardware settings (1 Nvidia A100-SXM4-80GB GPU), the training time of AdaNovo increased by 6.3% (from 56.52h to 60.17h) over Casanovo. However, AdaNovo and Casanovo share the similar inference speed, which is more important in real-world applications.

## 5 Conclusion and Future Work

In this paper, we discern that data biases limit progress in _de novo_ peptide sequencing by hindering the accurate identification of PTMs and reducing the reliability of PSMs due to low PTM frequency and spectral noise. To address these issues, we introduce a novel approach involving the calculation of conditional mutual information between the spectrum and each amino acid, followed by re-weighting the training loss of corresponding amino acids. Extensive experiments on 9-species datasets affirm that AdaNovo surpasses previous _de novo_ sequencing methods, showcasing superior performance in both amino acid- and peptide-level precision. Notably, AdaNovo exhibits a distinct advantage in identifying amino acids with PTMs. Despite the significant progress made by AdaNovo, identifying previously unseen PTMs remains challenging, prompting the need for further research in the future.

Figure 5: The effects of the two hyperparameters \(s_{1}\) and \(s_{2}\) for AdaNovo.

   Model & **\#Params (M)** & **Training time (h)** & **Inference time (h)** \\  Casanovo & 47.35 & 56.52 & 7.14 \\ AdaNovo & 66.31 & 60.17 & 7.09 \\   

Table 5: Comparisons with competitive methods in terms of computational overhead. The training and inference time are evaluated on Honeybee dataset with the same Nvidia A100 GPU.

Acknowledgement

This work was supported by National Natural Science Foundation of China Project No. 623B2086 and No. U21A20427, the Science & Technology Innovation 2030 Major Program Project No. 2021ZD0150100, Project No. WU2022A009 from the Center of Synthetic Biology and Integrated Bioengineering of Westlake University, and Project No. WU2023C019 from the Westlake University Industries of the Future Research. Finally, we thank the Westlake University HPC Center for providing part of the computational resources. Xiaojun Shan was not supported by any funds from China.