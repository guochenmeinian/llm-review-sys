# Augmented Memory Replay-based Continual Learning Approaches for Network Intrusion Detection

Suresh Kumar Amalapuram\({}^{*}\), Sumohana S Channappayya\({}^{\#}\), and Bheemarjuna Reddy Tamma\({}^{*}\)

Dept. of Computer Science and Engineering\({}^{*}\), Dept. of Electrical Engineering\({}^{\#}\)

Indian Institute of Technology Hyderabad, India

{cs19resch11001,sumohana,tbr}@iith.ac.in

###### Abstract

Intrusion detection is a form of anomalous activity detection in communication network traffic. Continual learning (CL) approaches to the intrusion detection task accumulate old knowledge while adapting to the latest threat knowledge. Previous works have shown the effectiveness of memory replay-based CL approaches for this task. In this work, we present two novel contributions to improve the performance of CL-based network intrusion detection in the context of class imbalance and scalability. First, we extend class balancing reservoir sampling (CBRS), a memory-based CL method, to address the problems of severe class imbalance for large datasets. Second, we propose a novel approach titled perturbation assistance for parameter approximation (PAPA) based on the Gaussian mixture model to reduce the number of _virtual stochastic gradient descent (SGD) parameter_ computations needed to discover maximally interfering samples for CL. We demonstrate that the proposed approaches perform remarkably better than the baselines on standard intrusion detection benchmarks created over shorter periods (KDDCUP'99, NSL-KDD, CICIDS-2017/2018, UNSW-NB15, and CTU-13) and a longer period with distribution shift (AnoShift). We also validated proposed approaches on standard continual learning benchmarks (SVHN, CIFAR-10/100, and CLEAR-10/100) and anomaly detection benchmarks (SMAP, SMD, and MSL). Further, the proposed PAPA approach significantly lowers the number of virtual SGD update operations, thus resulting in training time savings in the range of 12 to 40% compared to the maximally interfered samples retrieval algorithm.

## 1 Introduction

Learning from a continuum of data with distribution shift over time is known as continual/lifelong learning (CL) [1; 2]. CL algorithms aim to preserve learned knowledge (stability) while embracing the knowledge of new tasks (plasticity). The conflict in balancing stability and plasticity often leads to the problem of catastrophic forgetting (CF) [3; 4]. In other words, past tasks' knowledge degrades over time due to interference from the newer task. Unlike computer vision [5; 6; 7; 8], natural language processing [9; 10], and speech processing [11; 12], CL applications to many real-world problems are under-explored, and one such exemplar is network intrusion detection (NID) in communication networks. Intrusion detection is a form of distribution shift detection that aims to detect anomalous activity patterns in the network traffic [13; 14]. To ensure protection against novel cyber-attacks, network intrusion detection systems (NIDS) must evolve continuously and effortlessly learn from the newer attack data.

The concept drift/distribution shift is prevalent in cybersecurity, as malicious behavior can shift either suddenly or gradually over time [15; 13]. While NID formulated as anomaly detection (AD) is typically trained on normal data (_zero positive_ learning ) and is immune to the drift of malicious

[MISSING_PAGE_FAIL:2]

2. A perturbation assistance for parameter approximation (PAPA) method for estimating VSP updates that significantly reduces the training time for methods like MIR and GMED, leading to improved scalability.
3. A demonstration of the improved performance of the proposed methods relative to the considered baselines using standard network intrusion detection, computer vision, and anomaly detection benchmarks (including AnoShift and CLEAR-100, whose training data spans over ten years). Further, we show that using the PAPA method results in training time savings of 12 to 40% (relative to the baselines), thus it is highly suitable for large-scale training.

## 2 Related Work

**Intrusion detection:** An intrusion detection system (IDS) [32; 33] is one of the fundamental tools in security infrastructure to safeguard against cyber-attacks at the host or network (NIDS) perimeter level . A signature-based NIDS uses a rule-based engine to identify known patterns, whereas anomaly-based NIDS identifies patterns that deviate from normal behavior . NIDS are built using techniques ranging from traditional machine learning to deep learning and reinforcement learning . Typically, network intrusions represent a tiny subset of network traffic . As a result, a heavy class imbalance is observed in NID datasets, making the intrusion detection performance vulnerable. Sampling (algorithm) based approaches have been proposed in the literature to mitigate this problem [38; 39; 40]. But, this work focuses on building an NIDS that adapts to both benign and attack data distribution shifts without any aid of sampling strategies to handle the CI effect.

**Continual learning:** Recently, there has been a significant effort to expand the horizon of continual learning by mitigating the impact of CF . These efforts can be broadly grouped into the following families : regularization-based [41; 42], expansion-based , meta-learning-based , and rehearsal or memory replay-based approaches.

A subset of rehearsal methods that focus on class imbalance are CBRS  and partition reservoir sampling , in which samples are preserved in memory in proportion to their class strength. However, these methods do not handle severe CI between the minority samples. The proposed ECBRS addresses this CI issue. Further works like maximum interfered retrieval (MIR)  and gradient-based sample editing (GMED)  use virtual SGD parameter updates to identify the most interfered samples. These are samples in the replay memory that suffer an increase in loss due to previous parameter updates. However, for large-scale training, these virtual update computations add significant overhead. Unlike these, our proposed PAPA method significantly reduces this overhead, making it suitable for training over large datasets.

## 3 Methodology

In this section, we present the proposed ECBRS and PAPA methods. In the ECBRS method, we adopt two definitions introduced in the base paper of CBRS . They are _full_ and _largest_ classes. When a particular class contains the most instances among all the different classes in the memory,

Figure 1: Comparison between CBRS and ECBRS over an imbalanced stream from a CICIDS-2018 dataset setting with a memory size \(()=1500\). (a) Running statistics indicate the number of classwise samples seen so far. (b) Memory distribution represents the strength of each class in buffer memory at a particular instance. Upon the arrival of ten new samples from the class _attack5_, using CBRS, the memory distribution changes to (c). However, using ECBRS, the memory distribution changes to (d). For (c) and (d), the class with the red-colored bar is chosen for replacement, the green-colored bar class receives new samples, and the class with the blue-colored bar remains intact. Detailed illustrations of these configuration changes for CBRS and ECBRS are presented in the supplementary material (SM:A.3).

we call it the _largest_. Two or more classes can be the largest if they have the same size. We call a class _full_ if it currently is, or has been in one of the previous time steps, the largest class. Once a class becomes full, it remains so in the future. More details of the CBRS algorithm are given in the supplementary material (SM:A.14).

### ECBRS: An Extended Class Balancing Reservoir Sampling Algorithm

The legacy CBRS algorithm assigns higher probability weights to minority class samples using a weighted replay approach. However, these weights are computed based on the number of data samples in the buffer memory (local information). This approach has a limitation when the class imbalance present between the minority classes themselves is severe. This limitation is illustrated in Figure 1, where _running statistics_ of each class is the corresponding class running frequency(refer Figure 0(a)), and _benign_ is the maximal (majority) class. Typically, the _benign class_ samples must be chosen for replacement whenever new minority class samples arrive. However, CBRS treats all classes equally for a replacement to accommodate newly arrived _attack5_ (minority) class samples (instead of the benign class). This is because CBRS relies on local information for sample replacement. From Figure 0(b), all class samples appear to be the majority classes in the buffer memory distribution. As a result, different class samples in memory are uniformly selected for sample replacement (refer Figure 0(c), in which red-colored classes are chosen for sample replacement for accommodating newly arrived _attack5_ class samples). This choice will have an adverse impact on large-scale training, where the class imbalance between the minority classes is significant. Motivated by this, we extended the CBRS to rely on each class's running statistics (global information) for replacement decision-making. Our method (ECBRS) will always prefer the class with higher global information until a threshold \(\) (refer Equation 1), later it will choose the class with the next highest running statistic value, thereby ensuring that a majority class sample is replaced in the memory buffer. In summary, we use original class labels to organize the buffer memory to learn SBCP and choose class samples in memory with higher running statistic values for the replacement to accommodate newly arriving samples.

Further, the class imbalance among minority classes poses another challenge known as the conflict of equal weights. It will occur whenever the majority (maximal) class samples are completely replaced or underrepresented in the buffer memory. So, undermining the chosen minority class based on global information will lead to the loss of the essential samples in combating the class imbalance problem. This drawback can be mitigated by disallowing the undermining of the minority class beyond a certain threshold \(\). In ECBRS method, we choose this parameter based on global information. Thus, we guarantee that attack class samples can never be replaced beyond this threshold. From Figure 0(d), it can be observed that the proposed ECBRS initially chose benign class samples for replacements to accommodate newly arriving _attack5_ class samples, as it has the highest running statistics value. In the process of sample replacement, once it reaches the threshold of the benign class (\((.)\)= 262), ECBRS selects the class with the next highest running static value (_attack4_). Thus remaining, different class samples (colored blue) will remain intact. We compute \(()\) (for class \(i\)) as the expected number of samples to be present based on the global information, as shown in Equation 1.

\[(i)=m w(i),\] (1)

where \(i\) is the class index, \(w(i)=)}{_{j}(-n_{j})}\), \(m\) is the buffer memory capacity, \((i),n_{i}\) are the threshold and running class frequency for class \(i\), respectively, and \(w(i)\) is the weight associated with class \(i\). \(w(i)\) is the softmax of the negative running frequencies favoring the minority classes. In our experiments, we also observe that computing a single loss over the replayed and stream samples achieves better performance compared to CBRS (which involves computing two-component loss). One pragmatic reason for this behavior could be that concatenating replay samples with stream samples achieves better batch-wise class balance. We present the pseudo-code of the proposed ECBRS in Algorithm 1.

### Perturbation Assistance for Parameter Approximation (PAPA)

Memory replay-based CL techniques such as MIR and GMED select a subset of samples from the buffer memory based on the loss incurred during virtual stochastic gradient descent (SGD) parameter (VSP) updates. Samples that incur a higher loss are preferred. These updates are computed using the current batch of the incoming stream samples. Further, these updates are ignored after the subset selection. For lengthy data streams (used to train very deep models), these frequent VSP updates result in significant computational overhead (due to SGD operations). Each SGD involves computing the gradients for larger weight matrices, leading to increased computational overhead.

Motivated by this problem, we try to understand the relationship between the regular and virtual SGD update operations. Towards this, we plot various t-SNE visualizations on CICIDS-2017, CICIDS-2018, CIFAR-10, and CLEAR-10 datasets. The CIDIDS-2017 and CICIDS-2018 datasets are used to train an MLP, and the CIFAR-10 and CLEAR-10 datasets are used to train a ResNet-18 model (refer Figure 2). We make the following observations from these plots: virtual SGD parameter update is a slowly varying process. In other words, the VSP overlaps with or is scattered closely around the regular SGD updates. One way to capture this notion is to sufficiently _perturb_ the regular SGD parameter (RSP) updates to estimate the VSP updates. Then, the next question is: _how to quantify the perturbation required for a given task?_

To quantify the perturbation, we model the error (difference between the VSP and RSP) distribution of randomly chosen model parameters on a diverse set of benchmarks. These are shown in Figure 3. We observe that the error values of the chosen parameters exhibit a skewed Gaussian (heavy-tailed) distribution. We model this error distribution using a two-component Gaussian mixture model (GMM). However, modelling individual error components with a GMM is a time-consuming process. So, we model the joint distribution \((_{e})\) of error values of all the parameters using a GMM, whose marginals remain Gaussian. As a result, each parameter-level error distribution is preserved.

Figure 3: For randomly chosen model parameters, the plots show the distribution of the error between the regular SGD parameter updates and virtual SGD parameter updates on various datasets. In (a) and (b), an MLP is trained on CICIDS-2017 and CICIDS-2018. In (c) and (d), a ResNet-18 is trained on CIFAR-10 and CLEAR-10.

Figure 2: t-SNE visualization of regular SGD updates and virtual SGD updates. An MLP is trained on the CICIDS-2017 and CICIDS-2018 datasets. A ResNet-18 (pretrain ) model is trained on the CIFAR-10 and CLEAR-10 datasets.

Thus, our approach for approximating the VSP is mathematically formulated as a simple additive model which is given in Equation 2.

\[_{vpu}=_{rpu}+\] (2)

The perturbation \(\) is drawn from \((_{e})\) where \((_{e})=_{1}(_{1}|_{1},_{1})+_{2 }(_{2}|_{2},_{2}),\) where \(_{1},_{2}\), are the mean vectors and \(_{1},_{2}\) are the covariance matrices of the two Gaussian components, respectively. \(_{1},_{2}\) are the mixing coefficients and \(_{rpu}\) is the most recent regular parameter update.

**GMM Training:** We use the MIR algorithm for the first CL task in our proposed approach to estimate the error distribution (ED). This ED is used to train the GMM once, and this GMM is used in all remaining tasks. Our empirical study also confirms that different **first** task (used to construct ED) has no adverse effect on the performance results (refer to **SM:A.10.2**). The pseudo-code of the proposed PAPA method is outlined in Algorithm 2.

## 4 Experiments and Analysis

```
1:Input: data stream \((x_{i},y_{i})_{i=1}^{n}\), initial task \(T_{1}\), maximally interfered retrieval algorithm \(()\), gaussian mixture model with two components \(\), buffer memory \(\), perturbation array \(\), policy to update buffer memory \(\), function approximator \(f(x,y:)\), parameters after updating on previous batch of samples \(_{rpu}\).
2:for the task \(T_{1}\)do
3: sample batch \(_{j} T_{1}\)
4: render \(_{j-1}\); parameters after updating on \(_{j-1}\)
5: Run \((_{j})\)
6: store \(_{j}\); parameters after updating on \(_{j}\)
7: store \((_{j}-_{j-1})\) to \(\)
8:endfor
9: train a \(\) using \(\)
10:for remaining tasks do
11: sample batch \(\) from current task
12: sample \(\)
13:\(_{vpu}=_{rpu}+\)
14:\(\) compute **inter**ter samples using \(_{vpu}\) on \(\)
15:\(_{int}\) subsample from \(\) based on certain criteria
16:\(_{new}=_{int}\)
17: train \(f(:)\) with \(_{new}\)
18: update \(\) with \(\) using \(\)
19:endfor ```

**Algorithm 2** Perturbation assistance for parameter approximation

**Datasets and Tasks Formulation:** We conduct the experiments in the domain-incremental (class labels space is fixed to benign and attack, whereas data space horizon may change) learning approach, similar to the prior study . We created tasks by dividing the benign data and combining them with attack class samples. This approach ensures that each task contains a mix of benign and attack data, maintaining the class imbalance resembling real-world network traffic. We created five tasks for KDDCUP'99  and NSL-KDD , ten for CICIDS-2017/2018 , nine for UNSW-NB , and ten for AnoShift  benchmark.

Existing network intrusion detection benchmarks are artificially created over short time periods and may not exhibit natural distribution shifts  (except AnoShift). Distribution shifts are quantified by the optimal transport dataset distance (OTDD) (refer to Table 2). Large OTDD values indicate a higher distribution shift. More details on _how_ OTDD values are computed for each dataset are available in **SM:A.7**. Due to the absence of distribution shifts in NIDS datasets, the proposed approaches were validated using computer vision (CV) CL benchmarks. Furthermore, the ease of visualizing distribution shifts makes CV benchmarks suitable for validating the proposed approaches. For CV benchmarks, we randomly selected one or more classes as the attack classes. We split and distributed the attack class data among the remaining benign classes. This way of class splitting ensures experimentaly estimated using vision benchmarks are identical to the intrusion detection experiments. Specifically, we created nine tasks for SVHN  and CIFAR-10 . Similarly, we created ten tasks for the CIFAR-100  benchmark using the coarse/super class labels. The data spanned over ten years for the CLEAR-10/100  benchmark and contained all the classes' natural temporal evolution for each year. We created ten tasks as the data spans a decade. We also maintained **1:100** class imbalance ratio per task in CIFAR-100, CLEAR-10, and CLEAR-100 experiments. More details about datasets, preprocessing (and feature selection), and task formulations are presented in **SM:A.4, SM:A.5**, and **SM:A.6**.

**Baselines:** We compare the proposed ECBRS method with elastic weight consolidation (EWC) , synaptic intelligence (SI)  (regularization methods), memory(+gradient)-based algorithms like gradient episodic memory(GEM) , A-GEM , gradient-based sample selection (GSS-greedy) , and rehearsal-based methods like MIR , and CBRS . We compare the proposed PAPA with the MIR algorithm in all facets. We deliberately omit partition reservoir sampling  and GMED 

[MISSING_PAGE_FAIL:7]

especially on AnoShift, SVHN, CIFAR-10, CIFAR-100, CLEAR-10, and CLEAR-100 benchmarks. Specifically, ECBRS achieves 7% (avg) and 3% (avg) performance gains in attack and benign sample detection in terms of PR-AUC (A) and PR-AUC (B) values relative to the CBRS on aforementioned benchmarks. Third, GEM, AGEM, and GSS-greedy methods strongly compete with our approach. Specifically, for GEM and A-GEM, the reason for this would be due to the usage of the ring buffer memory organization policy  that stores task-wise representative samples, which are used to reduce CF of previous tasks , thus allowing positive backward transfer by constraining directions of the gradient projections. However, such a buffer organization policy requires knowing task boundaries apriori. Despite their effectiveness, such a policy is inappropriate in an online task-free continual learning setting like streaming intrusion detection in which task boundaries and the number of tasks are unknown. On the contrary, our approach organizes the memory as a pool in which any memory slot can be chosen for replacement. Eventually, we observe that training time for larger datasets is high for gradient-based methods like GEM and GSS-greedy. As a result, we omit the most recent gradient-based memory replay methods like online coreset selection (OCS)  in our baselines (refer to **SM:A.10.1** for more details). Besides, ECBRS is an efficient algorithm with respect to total training time, and it outperforms CBRS in total training time on five benchmark datasets. More discussions on training times are available in **SM:A.8**.

**ECBRS as a memory population method:** We can also integrate ECBRS as a memory population technique with other memory replay-based continual learning algorithms. Specifically, we demonstrate this for the MIR algorithm and observe an average performance gain of 12% and 14% in terms of PR-AUC (A) and PR-AUC (B) values relative to MIR. We observe that the performance gains of using ECBRS will increase with large-scale benchmarks, especially on AnoShift, CICIDS-2018, UNSW-NB15, and CICIDS-2017 benchmarks. We achieved an average gain of 30% (PR-AUC (A)) and 31% (PR-AUC (B)) on the aforementioned benchmarks. The results of the MIR+ECBRS algorithm (rows) are highlighted in light grey in Table 3. In the ECBRS experiments, we use the random memory population method for the MIR algorithm.

**Threshold parameter (\(\)):** As described earlier, we are also interested in learning the distribution shifts of the benign data. Since there are two distribution shifts to adapt to, one may wonder which one is more important. The answer depends on the problem context; intrusion detection requires paying more attention to the distribution shifts in attack traffic, which can be controlled using the hyperparameter \(w()\). We conduct extensive experiments on intrusion detection and computer vision benchmarks to understand the behavior of setting the default value for the \(w()\) Equation 1. We found that all the datasets' performance is consistent for the lower \(w()\) value, so we used a fixed value of 0.1. Eventually, \(w()\) will be multiplied by the buffer memory size to obtain the expected number of samples per class (\(\)) to be present in the buffer memory. Ablation studies on \(\)(.) are presented in **SM:A.10.1**.

**PAP:** We present the results of the proposed PAPA algorithm and its comparison with the MIR algorithm in Table 4. In this set of experiments, we use ECBRS as a memory population algorithm for both MIR and PAPA algorithms. Results are presented in the non-decreasing order of their respective training times. Based on performance metrics, our evaluation of the benchmarks demonstrates PAPA as a replacement for a method like MIR, which uses numerous virtual SGD updates. From Table 4, it is also interesting to see that our approach's training time efficiency progresses with the dataset's size. Specifically, we note that PAPA achieves nearly 29%, 30%, 33%, 34%, and 40% scalable efficiency

    &  &  &  \\  Datasets & PR-AUC (A) & PR-AUC(B) & ROC-AUC & PR-AUC (A) & PR-AUC(B) & ROC-AUC & MIR & PAPA & Scalable efficiency \\  NSL-KDD & 0.964 & 0.971 & 0.970 & 0.961 & 0.968 & 0.969 & 25.4 & 21.4 & 15.7\% \\ CICIDS-2017 & 0.999 & 0.999 & 0.999 & 0.999 & 0.999 & 316.0 & 188.8 & 40.2\% \\ CTU-13 & 1.009 & 1.000 & 0.999 & 0.999 & 0.999 & 375.5 & 330.8 & 11.9\% \\ KDDCUT-99 & 1.009 & 0.929 & 0.999 & 1.000 & 0.920 & 0.999 & 457.0 & 395.0 & 13.5\% \\ UNSW-NB15 & 0.999 & 0.999 & 0.999 & 0.999 & 0.999 & 0.999 & 49.30 & 36.29 & 29.7\% \\ AnoShift & 0.944 & 0.926 & 0.934 & 0.947 & 0.927 & 0.934 & 1300 & 906.6 & 30.7\% \\ CICIDS-2018 & 0.994 & 0.994 & 0.992 & 0.998 & 0.999 & 0.998 & 9940.0 & 5948.0 & 34.2\% \\  CIFAR-100 & 0.663 & 0.659 & 0.663 & 0.673 & 0.647 & 0.672 & 115.2 & 76.7 & 33.4\% \\ CIFAR-10 & 0.949 & 0.936 & 0.944 & 0.948 & 0.936 & 0.944 & 133.0 & 108.0 & 18.7\% \\ CIFAR-10 & 0.953 & 0.933 & 0.942 & 0.943 & 0.927 & 0.932 & 262.9 & 175.4 & 33.2\% \\ SVHN & 0.979 & 0.972 & 0.976 & 0.981 & 0.974 & 0.978 & 296.0 & 208.0 & 29.7\% \\  CLEAR-100 & 0.839 & 0.793 & 0.817 & 0.845 & 0.793 & 0.823 & 1706.0 & 1299.0 & 29.1\% \\   

Table 4: Performance comparison of the proposed PAPA method with other baselines on intrusion detection and computer vision benchmarks. We report the arithmetic mean of each evaluation metric with each experiment repeated five times independently.

on the CLEAR-100, AnoShift, CIFAR-100, CICIDS-2018 and CICIDS-2017 datasets, respectively. By scalable efficiency, we mean the total training time saved using the PAPA algorithm compared to the MIR method.

**Virtual SGD parameter updates:** Here, we present the core of the proposed PAPA algorithm in terms of the number of virtual stochastic gradient descent parameter updates. In Table 5, we present the results in the non-decreasing order of the size of the datasets and make the following observations. First, the number of virtual SGD operations in MIR increases with the size of the dataset. On the contrary, the number of these operations is reduced with the benchmark size in our proposed method. Second, the number of virtual SGD operations of the PAPA algorithm per benchmark is always fixed. The fixed number of virtual SGD operations in the PAPA is due to using the MIR algorithm only for the first task.

**Ablation studies:** We conduct various experiments as a part of ablation studies to validate the robustness of the proposed approaches. These include the effect of gradient-based sample selection instead of a random selection (refer to line 177) of the ECBRS performance, the effect of batch size and \(\) on ECBRS performance, robustness to different task orders on the PAPA algorithm, the effect of batch size and buffer memory size (\(\)) on the PAPA algorithm. We present and discuss the results in **SM:A.10**. Further, limitations of the proposed methods are discussed in **SM:A.13**.

**Additional experiments:** We also validate the proposed approaches on the standard unsupervised anomaly detection (SMAP , SMD , and MSL ) benchmarks. Our findings on the supervised dataset are equally valid in this new set of experiments (refer to **SM:A.9**).

### Impact of task similarity on virtual SGD updates

All the preceding discussions of the PAPA algorithm focus on leveraging the slowly varying parameter update process to approximate virtual SGD updates. Here, we explain the reason for the slowness in the virtual parameter updates using task similarity. Specifically,

we use optimal transport dataset distance (OTDD)  between two consecutive tasks to quantify the _task similarity_. Also, an error vector (with a dimension equal to the number of parameters in the model) whose entry _'i'_ is equal to the difference

    &  &  &  \\  Datasets & Vir.SGD ops & Reg. SGD ops & Vir. SGD ops & Reg. SGD ops & Vir. SGD ops & Total SGD ops \\  NSL-KDD & 1140 & 1578 & 210 & 1740 & 81.5\% & 28.2\% \\ CICIDS-2017 & 9160 & 15784 & 550 & 10881 & 89.5\% & 54.1\% \\ UNSW-NB15 & 11915 & 14638 & 600 & 12587 & 94.9\% & 50.3\% \\ CTU-13 & 13235 & 18007 & 120 & 24658 & 99.0\% & 20.6\% \\ KDDCUP-99 & 19555 & 17525 & 480 & 24450 & 97.5\% & 32.7\% \\ AnoShift & 48825 & 53187 & 1200 & 58421 & 97.5\% & 41.5\% \\ CICIDS-2018 & 30590 & 41234 & 1200 & 36605 & 96.1\% & 47.3\% \\  SVHN & 3850 & 7143 & 360 & 6267 & 90.6\% & 39.7\% \\ CIFAR-10 & 1935 & 2526 & 180 & 3028 & 90.6\% & 28.0\% \\ CIFAR-100 & 1700 & 2436 & 135 & 2253 & 92.0\% & 42.2\% \\ CLEAR-10 & 500 & 602 & 40 & 607 & 92.0\% & 41.2\% \\ CLEAR-100 & 3250 & 3250 & 320 & 3848 & 90.1\% & 35.8\% \\   

Table 5: Performance comparison of the number of regular and virtual SGD operations required for the MIR and proposed PAPA approach on benchmark datasets. Each experiment is repeated five times independently.

Figure 4: Illustration of the relationship between task similarity and slowness in the parameter update process of the MIR algorithm, using (a) optimal transport dataset distance and (b) visualization of parameter value shift using the \(l_{1}\)-norm over the error vector.

in the parameter (\(_{i}\)) value prior and after learning a particular task. We perform experiments to compute OTDD and \(l_{1}\)-norm of the error vector over the MNIST and CIFAR-10 datasets. We create nine tasks in each of these experiments. We recall that a higher OTDD value signifies lower task similarity. The OTDD value is zero for all the consecutive tasks in the MNIST dataset and therefore ignored in this discussion.

In the second experiment, the tasks are formulated using the CIFAR-10 dataset. The third experiment creates a sequence of two tasks from the MNIST and CIFAR-10 datasets. From Figure 3(a), we observe that the OTDD value is higher for the third experiment than the CIFAR-10 experiment. The reason for this behavior is evident since the consecutive tasks in the third experiment learn from two dissimilar datasets. We discover similar behavior in the error vector in which \(l_{1}\)-norm over the error vector of the third experiment exhibits a _zig-zag_ pattern in Figure 3(b). In other words, parameter values go back and forth between two dissimilar tasks. Further, we provided detailed empirical analysis to showcase that the modelling error distribution of the first task using the Gaussian mixture model is sufficient and useful to approximate error distributions of the subsequent tasks. In other words, the occurrence of two dissimilar tasks resulting in different error distributions that can't be modelled by PAPA is rare in the context of NID, as demonstrated in **SM:A.12**. To conclude, assuming overparameterized models, parameter value shifts between the regular SGD and virtual SGD update are low, given the sequence of similar tasks.

## 5 Conclusions and Future Work

In this work, we improved the performance of online memory replay-based CL approaches for network intrusion detection while addressing class imbalance and scalability issues. Specifically, we focused on two popular algorithms viz., CBRS and MIR. The CBRS algorithm is extended (that we call ECBRS) using global information that helps it to maintain accurate class imbalance information. We showed that ECBRS outperforms all the baselines, including CBRS, by maintaining higher minority class samples in the buffer memory. Furthermore, ECBRS is also efficient in terms of the training time incurred as compared to the baselines. We proposed a simple perturbation-assisted parameter approximation (PAPA) method for virtual parameter updation in the MIR algorithm that helps significantly reduce the number of SGD update operations. The efficacy and scalability of these augmentation strategies have been demonstrated on standard network intrusion detection, computer vision, and anomaly detection datasets. In summary, the proposed augmentations to online memory replay-based CL approaches achieved improved performance using fewer computations and thereby at a lower training cost.

As a part of future work, we plan to extend the supervised binary classification setting of the network intrusion detection problem to a supervised multi-class classification problem. Furthermore, we are interested in exploring the semi-supervised techniques for intrusion detection to understand the challenges in the context of class imbalance and distribution shifts in conjunction with open-world learning, explainability, and adversarial robustness settings.