# SDP4Bit: Toward 4-bit Communication Quantization

in Sharded Data Parallelism for LLM Training

Jinda Jia

Indiana University

jindjia@iu.edu

&Cong Xie1

ByteDance Inc.

cong.xie@bytedance.com

&Hanlin Lu

ByteDance Inc.

hanlin.lu@bytedance.com

&Daoce Wang

Indiana University

daocwang@iu.edu

&Hao Feng

Indiana University

haofeng@iu.edu

&Chengming Zhang

University of Houston

czhang59@cougarnet.uh.edu

&Baixi Sun

Indiana University

sunbaix@iu.edu

&Haibin Lin

ByteDance Inc.

haibin.lin@bytedance.com

&Zhi Zhang

ByteDance Inc.

zhangzhi.joshua@bytedance.com

&Xin Liu

ByteDance Inc.

liuxin.ai@bytedance.com &Dingwen Tao

Indiana University

ditao@iu.edu

Equal Contribution.

###### Abstract

Recent years have witnessed a clear trend towards language models with an ever-increasing number of parameters, as well as the growing training overhead and memory usage. Distributed training, particularly through Sharded Data Parallelism (ShardedDP) which partitions optimizer states among workers, has emerged as a crucial technique to mitigate training time and memory usage. Yet, a major challenge in the scalability of ShardedDP is the intensive communication of weights and gradients. While compression techniques can alleviate this issue, they often result in worse accuracy. Driven by this limitation, we propose **SDP4Bit** (Toward 4Bit Communication Quantization in Sharded Data Parallelism for LLM Training), which effectively reduces the communication of weights and gradients to nearly 4 bits via two novel techniques: quantization on weight differences, and two-level gradient smooth quantization. Furthermore, SDP4Bit presents an algorithm-system co-design with runtime optimization to minimize the computation overhead of compression. In addition to the theoretical guarantees of convergence, we empirically evaluate the accuracy of SDP4Bit on the pre-training of GPT models with up to 6.7 billion parameters, and the results demonstrate a negligible impact on training loss. Furthermore, speed experiments show that SDP4Bit achieves up to 4.08\(\) speedup in end-to-end throughput on a scale of 128 GPUs.

## 1 Introduction

Large Language Models (LLMs) are increasingly utilized across various applications, leading to a trend toward larger model sizes. This expansion in model size significantly escalates training overheads, making the process more costly and resource-intensive. To mitigate the time-consumingnature of training LLMs, it is common to employ multiple GPUs in a data-parallel configuration. However, naive Data Parallelism (DP) necessitates that each GPU replicates the entire optimizer states, a strategy often impractical due to the limited memory capacity of individual GPUs. This limitation becomes particularly critical with the substantial size of modern LLMs.

Sharded Data Parallelism (ShardedDP) evolves from naive DP to reduce the memory footprint by sharding optimizer states among GPUs. However, the sharding mechanism significantly changes the communication pattern of DP, which brings up new challenges in system optimization. As a result, ShardedDP suffers from heavy communication overheads of both weights and gradients, particularly when inter-node bandwidth is limited. This can significantly increase the end-to-end (E2E) training time, especially when using a small gradient accumulation step.

Quantization is a widely used strategy to reduce the communication overhead of naive DP, albeit with some accuracy loss. Unfortunately, few prior studies have specifically addressed the issue of communication reduction in ShardedDP. Recently, QSDP  and ZeRO++  attempted to quantize the communication of ShardedDP to Int4. However, when pushing the communication ratio to its limits, both QSDP and ZeRO++ fail to maintain comparable training loss to the baseline. Furthermore, ZeRO++ lacks theoretical convergence guarantees, and QSDP is limited to one specific quantizer called "random shift" and strong assumptions. Thus, _there is no effective solution to reduce ShardedDP's communication to nearly 4 bits without compromising the training loss_.

To address these issues, this paper proposes a novel communication reduction strategy, **SDP4Bit**. SDP4Bit comprises two main techniques: (1) **Quantization on Weight Differences**: Instead of directly quantizing weights, we apply 4-bit quantization to compress the weight differences between current and previous iterations; (2) **Two-Level Gradient Smooth Quantization**: We apply 8-bit quantization to intra-node gradients and 4-bit quantization to inter-node gradients, with Hadamard Transform for smoothing the outliers. To the best of our knowledge, SDP4Bit is the first work to successfully reduce both gradients and weights to nearly 4 bits without compromising training accuracy. As shown in Figure 1, the training validation loss for GPT-6.7B using SDP4Bit is closely aligned with full precision training. Our main contributions are summarized as follows:

* We propose a low-bit (i.e., nearly 4-bit) communication reduction strategy for ShardedDP that preserves E2E training accuracy.
* We establish a convergence guarantee for the proposed strategy, showing the same convergence rate as the ordinary Stochastic Gradient Descent (SGD), with extended choices of biased compressors and weaker assumptions compared to the previous theoretical results.
* We implement our method within the Megatron-LM framework and enhance it with runtime optimizations such as buffer reuse, operation pruning, and kernel fusion.
* Our results validate that SDP4Bit successfully compresses the communication of weights and gradients to nearly 4 bits, with a negligible impact on final loss. Notably, compared to non-quantized baseline, it achieves 4.08\(\) speedup for a GPT-18B model trained on 128 H800 GPUs.

## 2 Preliminaries

### Sharded Data Parallelism

Sharded Data Parallelism (ShardedDP) modifies traditional Data Parallelism (DP) to reduce the memory footprint per GPU. Unlike traditional DP, which duplicates high-precision optimizer states (typically including model weights and momentum variables in Float32) on each GPU, ShardedDP partitions them across all GPUs. Each GPU manages \(\) of the optimizer states, hence reducing the corresponding memory footprint by a factor of \(\), where \(P\) represents the number of GPUs involved.

Figure 1: Training validation loss for GPT-6.7B; SDP4Bit is closely aligned with full precision training.

With high-precision model weights sharded across GPUs (referred to as **"main weights"**), an all-gather operation is required to collect the weights for the forward-backward steps (typically in relatively low precision, such as Float16, referred to as **"model weights"**). For gradient synchronization, a reduce-scatter operation is performed before the optimization steps to ensure that each GPU has the corresponding hard of averaged gradients. In summary, each iteration necessitates an all-gather for weights and a reduce-scatter for gradients.

Driven by the need to train larger models within the constraints of GPU memory, ShardedDP is incorporated into several popular training frameworks, including Megatron-LM (Distributed Optimizer), DeepSpeed (ZeRO), and PyTorch (FSDP), each with slightly different implementation strategies. Notably, ZeRO-3 and FSDP release the collected weights after each computation to enhance memory efficiency, necessitating additional weight collective communication during the backward pass. Conversely, ZeRO-2 and Megatron-LM retain the collected weights throughout, thus eliminating the need for weight collection during the backward pass. Our weight reduction strategy is particularly well-suited for Megatron-LM, as it maintains a full model's weights at all times (see Algorithm 2). Additionally, Megatron-LM provides flexible parallelism support, such as tensor parallelism, which partitions models vertically to alleviate memory limitations. This approach enables the training of larger models compared to DeepSpeed.

```
0: worker: \(p\), weight in shard \(p\): \(w[p]\), local gradient on worker \(p\): \(g_{model}^{p}\), global gradient in shard \(p\): \(g_{main}[p]\), weight difference: \(d\)
1:function CompressedForwardPass
2:\(_{main}[p](w_{main}[p])\)
3:\(w_{model}(_{main}[p])\)
4:\(output^{p}(w_{model},input^{p})\)
5:\((w_{model})\)
6:function CompressedBackwardPass
7:\(_{main}[p](w_{main}[p])\)
8:\(w_{model}(_{main}[p])\)
9:\(g_{model}^{p}(w_{model},output^{p})\)
10:\((w_{model})\)
11:\(_{model}^{p}(g_{model}^{p})\)
12:\(g_{main}[p](_{model}^{p})\)
13:\(w_{main}[p](g_{main}[p],w_{main}[p])\) ```

**Algorithm 1**QSDP / ZeRO++

### Quantization

Quantization is a commonly used strategy in data compression. In this paper, we explore symmetric linear (integer) quantization due to its low overhead and latency. It is defined as follows:

\[x_{}=((2^{k-1}-1)), s= (x),\]

where \(k\) represents the bit-width of the quantized values, and \(s\) is referred to as "scales".

Additionally, group-wise quantization  is employed to minimize quantization error by dividing the data into multiple groups and quantizing each group individually. This approach results in a lower compression ratio due to the need to store additional scales.

### Collective Reduction Communication with Quantization

State-of-the-art (SOTA) collective communication libraries (e.g., NCCL, Gloo) employ a ring-based algorithm for its optimal bandwidth . This algorithm executes reduce-scatter operations across \(P-1\) rounds, during which each GPU sends local data and aggregates the received data. When quantization is applied, this necessitates \(P-1\) rounds of quantization and dequantization, potentially leading to error propagation and increased latency . Some strategies replace reduce-scatter with all-to-all communication, but this increases inter-node communication, typically with lower bandwidth.

ZeRO++  modifies this approach by substituting the conventional reduce-scatter (used by QSDP) with two all-to-all operations (shown in Algorithm 1, with different colors to distinguish ZeRO++from QSDP). The first operation is confined within each node, and post-reduction, the data size is diminished to \(\), where \(N\) is the number of GPUs per node. The subsequent all-to-all operation occurs between GPUs across different nodes that share the same local rank. In ZeRO++, each all-to-all operation follows a 4-bit quantization step to minimize communication data size.

As shown in Figure 5, while this method efficiently integrates quantization into reduce-scatter without augmenting inter-node communication, _the repeated 4-bit quantization steps can accumulate quantization errors, potentially leading to suboptimal training outcomes_.

## 3 Methodology

### Quantization on Weight Differences (_qWD_)

As discussed in Section 2.1, ShardedDP requires each GPU to send/receive updated weights (main weights) to/from other GPUs in each iteration. _However, weights generally exhibit a wide range and directly applying 4-bit quantization leads to significant quantization errors_. Even with group-wise quantization, a gap in E2E training loss compared to full precision remains despite using small group sizes.

**qWD:** To address this issue, we quantize weight differences instead of the original weights during communication. As illustrated in Figure 2 and Algorithm 2, after the optimizer step, each GPU calculates the differences between the main weights and the model weights. These differences are then quantized and all-gathered across all GPUs. After all-gathering, each GPU dequantizes the received data to obtain the weight differences. These differences are then added to the model weights to obtain the updated weights.

There are two main benefits to applying quantization to weight differences.

**1)** In practice, weight differences are generally easier to quantize. As shown in Figure 4, weight differences are more uniformly distributed in a smaller range compared to the weights themselves, resulting in smaller errors for INT4 quantization. Furthermore, since intuitively the magnitudes of weight differences are smaller than those of weights themselves (informally supposed that \(\| w_{t}\|=\|w_{t}-w_{t-1}\|<\|w_{t}\|\)) and the relative quantization errors are similar between weights and weight differences (informally supposed that \()- w_{t}\|}{\| w_{t}\|})-w_{t}\|}{\|w_{t}\|}\)), the weight differences compression potentially has a smaller error relative to the weights themselves: \()- w_{t}\|}{\|w_{t}\|})-w_{t}\|}{ \|w_{t}\|}\), where \(q()\) is the quantization function.

**2)** In theory, weight differences compression improves convergence compared to naive weight compression. When extended to biased compressors, we present theoretical guarantees for convergence at the same rate as ordinary SGD, as detailed in Section 4.2. In contrast, we demonstrate that using biased compressors directly on weights can lead to convergence failure, as illustrated in an example in Section 4.1. This proves that biased compressors are not compatible with QSDP or ZeRO++.

### Two-Level Gradient Smooth Quantization

#### 3.2.1 Two-level Gradient Quantization (_TLq_)

As discussed in Section 2.3, the 2-step all-to-all communication strategy benefits gradient communication when quantization is applied. However, it also introduces error accumulation due to consecutive 4-bit quantization steps, necessitating additional rounds of training and communication that diminish the per-iteration communication savings. We observe that _applying ULq, which quantizes gradients to extremely low precision, such as 4-bit, leads to noticeable deviations in training loss_ compared to full-precision training, as illustrated in Figure 5.

_TLq:_ Instead of employing a global 4-bit quantization strategy for both inter-node and intra-node all-to-all communications, we propose a two-level precision quantization strategy. This approach balances performance and accuracy by enhancing accuracy without introducing additional overhead. For intra-node all-to-all communication, gradients are quantized to INT8 before sending. After receiving the data, each GPU dequantizes the received data back to the full precision (i.e., FP32) for local reduction. This reduced data is then quantized to INT4 to minimize inter-node communication overhead. The detailed methodology is depicted in Figure 3. Since the two all-to-all operations utilize different network bandwidths, their communications can be effectively overlapped (see Table 4).

#### 3.2.2 _TLq_ with Hadamard Smoother (_TLq-Hs_)

While _TLq_ brings the training loss closer to the baseline, it does not achieve perfect alignment. In gradient quantization, _outliers can significantly amplify quantization errors_. Although group-wise quantization isolates outliers to minimize their impact on the precision of values in other groups, the values within the same group remain affected.

_TLq-HS:_ To mitigate the outlier issue, we apply Hadamard transform  to the gradients before quantization. The Hadamard transform, a specific type of generalized Fourier transform, exhibits properties such as \(H=H^{T}\) and \(H H^{T}=I\), distributing outlier information across nearby elements and effectively smoothing them out. For a detailed description of the methodology, see Algorithm 3.

```
0:gradient\(grad\)
1:functionTLq-HS
2:\(\) Hadamard\((grad)\)
3:\(g_{bsit}\) QuantizeRN(\(\))
4:\(list(g_{bsit\_intra})\)IntraAlltoAll(\(g_{bsit}\))
5:\(list(_{intra})\)Dequantize(\(list(g_{bsit\_intra})\))
6:\(list(_{intra})\)Hadamard(\(list(_{intra})\))
7:\(_{reduced}\)Reduction(\(list(_{intra})\))
8:\(_{reduced}\)Hadamard(\(_{reduced}\))
9:\(g_{abit}\)QuantizeBH(\(_{reduced}\))
10:\(list(g_{abit\_inter})\)InterAlltoAll(\(g_{abit}\))
11:\(list(_{inter})\)Dequantize(\(list(g_{abit\_inter})\))
12:\(_{final\_reduced}\)Reduction(\(list(_{inter})\))
13:\(g_{final\_reduced}\)Hadamard(\(_{final\_reduced}\)) ```

**Algorithm 3** TLq with Hadamard Smoother

### Performance Optimizations in Implementation

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

```
0:gradient\(grad\)
1:functionTLq-HS
2:\(\)Hadamard\((grad)\)
3:\(g_{bsit}\)QuantizeRN(\(\))
4:\(list(g_{bsit\_intra})\)IntraAlltoAll(\(g_{bsit}\))
5:\(list(_{intra})\)Dequantize(\(list(g_{bsit\_intra})\))
6:\(list(_{intra})\)Hadamard(\(list(_{intra})\))
7:\(_{reduced}\)Reduction(\(list(_{intra})\))
8:\(_{reduced}\)Hadamard(\(_{reduced}\))
9:\(g_{abit}\)QuantizeBH(\(_{reduced}\))
10:\(list(g_{abit\_inter})\)InterAlltoAll(\(g_{abit}\))
11:\(list(_{inter})\)Dequantize(\(list(g_{abit\_inter})\))
12:\(_{final\_reduced}\)Reduction(\(list(_{inter})\))
13:\(g_{final\_reduced}\)Hadamard(\(_{final\_reduced}\)) ```

**Algorithm 4** TLq with Hadamard Smoother

### Performance Optimizations in Implementation

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, our implementation eliminates the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.1, Sharded enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for backward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to retain model weights for calculating the differences, thus enhancing memory efficiency.

**Optimizing Memory Efficiency for Weight Differences Computation:** After the all-gather communication, each GPU receives weight differences from the others, which are then added to the model weights to update them. As discussed in Section 2.1, with ShardedDP enabled in Megatron-LM, each GPU maintains a complete copy of the model weights for forward and backward computation. This contrasts with ZeRO, where model weights are released after computation. By reusing these locally stored model weights in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.1, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.2, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.3, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.4, Sharded in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.5, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.6, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.7, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.8, Sharded enabled in Megatron-LM, we implement the need for additional buffers to update them. As discussed in Section 2.9, Sharded enabled in Megatron-

**Simplifying Hadamard Transforms:** In a naive implementation, the Hadamard transform would be applied at each step before quantization and after dequantization. However, by leveraging the orthogonality of the Hadamard transform, i.e. \(H H=I\), we omit the transform after the intra-node all-to-all dequantization (Algorithm 3, Line 6) and before the inter-node quantization (Algorithm 3, Line 8). Furthermore, by utilizing the distributive property, i.e., \(_{i}Hg_{i}=H_{i}g_{i}\), we move the second Hadamard transform from after the inter-node dequantization (Algorithm 3, Line 11) to after the final reduction (Algorithm 3, Line 12). These simplifications reduce the unnecessary computational overhead associated with repeated Hadamard transforms.

**Fusing GPU Kernels with Group Size Alignment:** To mitigate additional data movement from global memory--which typically exhibits the slowest memory bandwidth--, we fuse the Hadamard transform with the (de)quantization operations into a single CUDA kernel. This fusion allows the operations to run nearly as fast as the quantization operation alone. It is worth noting that, for this fusion to be efficient, there must be an alignment between the two. Specifically, the size of the quantization group must be divisible by the size of the Hadamard matrix, ensuring that memory traffic remains within the kernel block. We choose \(H\) to be small (e.g., 32\(\)32) because, at this size, the transform operation on the GPU is typically memory-bound and incurs minimal overhead. While larger \(H\) sizes offer better smoothing capabilities, we find that a 32\(\)32 matrix is sufficient to effectively smooth outliers in gradients.

## 4 Theoretical Analysis

### Counterexample of Biased Weight Compression

One of the advantages of our proposed weight difference compression is the compatibility to both biased and unbiased compressors. Note that using biased compressors directly on weight compression incurs issues in convergence under standard assumptions, as avoided in QSDP  or ZeRO++ . We illustrate such issues in the following toy example.

**Counterexample 4.1**.: Consider a least square problem with \(w^{*}=(0,0)^{}\): \(_{w^{2}}[f(w)=\|w\|^{2}]\), and stochastic gradient \(g(w)=(4w_{1},0)^{}\) with probability 0.5 and \(g(w)=(0,4w_{2})^{}\) with probability 0.5, thus \([g(w)]=_{w}f(w)\). We use the initial value \(w_{init}=(1,-1)^{}\), the learning rate \(<0.125\), and the following nearest ternary quantizer: \(s=(|w|),q(w)=round(w/s)*s\), where \(||\) is element-wise absolute value, and \(round()\) quantizes each element to the nearest value in \(\{-1,0,1\}\). It is easy to check that for SGD under such settings, the weights before quantization will be either \((1-4,-1)^{}\) or \((1,-1+4)^{}\), resulting in \(q(w)=(1,-1)^{}\), which means that SGD with ternary weight quantization gets stuck at the initial value in this case, while SGD without weight quantization and SGD with weight difference quantization both converge to the optimal.

### Convergence Analysis

To theoretically analyze the convergence of our distributed training algorithm with communication compression, we focus on the following SGD variant with gradient compression and weight difference compression. We use SGD to solve the following optimization problem: \(f^{*}=_{w}f(w)\), where \(f(w)\) is the objective function, \(w^{d}\) is the model parameter.

```
1:Initialize main parameter weights \(w_{0}\)
2:Initialize compressed parameter weights \(_{0} w_{0}\)
3:for all iteration \(t[T]\)do
4: Compute gradient: \(g_{t-1}= f(_{t-1};_{t-1})\)
5: Compress gradient: \(_{t-1}=_{g}(g_{t-1})\)
6: Update main weights: \(w_{t} w_{t-1}-_{t-1}\)
7: Compress weight difference: \(_{t}=_{w}(w_{t}-_{t-1})\)
8: Update compressed weights: \(_{t}_{t-1}+_{t}\)
9:endfor ```

**Algorithm 4** SGD with SDP4BitNote that we use unbiased compressors for gradient reduction, and arbitrary (potentially biased) compressors for weight collection. We formally define these two classes of compressors as follows.

**Definition 4.1** (Unbiased \(\)-approximate compressor ).: An operator \(:^{d}^{d}\) is a \(\)-approximate compressor for \( 0\) if \([(v)]=v\) and \([(v)-v]^{2}\|v\|^{2}, v^{d}\).

**Definition 4.2** (\(\)-approximate compressor ).: An operator \(:^{d}^{d}\) is a \(\)-approximate compressor for \(\) if \([(v)-v]^{2}(1-)\|v\|^{2}, v^{d}\).

_Remark 4.1_.: Note that, in a certain sense, the class of \(\)-approximate compressors contains the class of unbiased compressors. It is easy to check that any \(\)-approximate unbiased compressor \(\) can be converted to a \(\)-approximate biased compressor \((v)=(v)\). Furthermore, the class of \(\)-approximate compressors typically provides more options such as top-\(k\) sparsifiers, and top-\(k\) low-rank compressors. Thus, we consider arbitrary (biased or unbiased) \(\)-approximate compressors for weight compression in our theoretical analysis.

_Remark 4.2_.: For distributed training with \(P\) workers, we define the compressed gradient as \(_{t}=_{g}(g_{t})=_{i[P]}_{g} ^{}(g_{t,i})\), where \(g_{t}=_{i[P]}g_{t,i}\), and \(g_{t,i}\) is the stochastic gradient from the \(i\)th worker in \(t\) iteration. We assume that \(_{g}\) is an unbiased \(\)-approximate compressor of the average gradient \(g_{t}\).

**Assumption 4.1**.: (Smoothness) We assume that \(f(x)\) is \(L\)-smooth: \(\| f(x)- f(y)\| L\|x-y\|, x,y^{d}\), which implies \(f(y)-f(x) f(x),y-x+\|y-x\|^{2}\).

**Assumption 4.2**.: For any stochastic gradient \( f(w;)\), where \(\) is an independent random sample, we assume unbiasedness \([ f(w;)|w]= f(w)\), and bounded variance \([ f(w;)- f(w)\|^{2}|w]\| f(w)\|^{2}+ ^{2}\) (, Assumption 3).

We derive the following error bounds on the convergence of SDP4Bit under the above assumptions. All proofs can be found in Appendix A.

**Theorem 4.1** (Convergence error bound).: _For arbitrary non-convex function under Assumption 4.1 and Assumption 4.2, taking learning rate \(+++)}\), Algorithm 4 converges to a critical point with the following error bound:_

\[^{T}[\| f(_{t})\|^{2}]}{T+1} +++)(f(w_{0})-f^{*} )}{T+1}+4)-f^{*})}{T+1}}.\]

_Remark 4.3_.: Note that compared to QSDP , our convergence analysis does not require Polyak-Lojasiewicz condition or the specific choice of weight quantization (random shift). In other words, Theorem 4.1 shows that our proposed algorithm has the same \((})\) convergence rate as ordinary SGD for general non-convex functions, but under much weaker assumptions compared to QSDP.

## 5 Evaluation

### Experimental Setup

**Hardware:** The experiments are conducted on two different clusters to evaluate SDP4Bit across varying network environments: **1)** 16 nodes, each node equipped with 4 Nvidia A100-SXM4-40GB GPUs. All nodes are interconnected with a 100 Gbps Slingshot10 network, providing slower inter-node bandwidth. **2)** 16 nodes, each node equipped with 8 Nvidia H800-SXM5-80GB GPUs. Each node is connected using 8 InfiniBand links, achieving a total bandwidth of 3.2 Tbps, providing higher inter-node bandwidth.

**Baselines:** We use BFloat16/Float32 (weights/gradients) mixed-precision in Megatron-LM  as our basic _Baseline_ for both accuracy and E2E throughput analysis. Within each set of experiments, we ensure consistent hyper-parameters to ensure fairness. Detailed parameters are provided in Appendix D. Additionally, we implement another baseline for comparison in Megatron-LM, using the same quantization strategy in ZeRO++, employing 4-bit quantization for both weights (group-wise weight quantization, refered to as \(qW\)) and gradients (twice all-to-all with uniform level 4-bit quantization, refer to as _ULq_).

**Dataset and Models:** To demonstrate that SDP4Bit does not adversely affect end-to-end training loss, we conduct pre-training on GPT-series  models ranging from 125M to 6.7B parameters

[MISSING_PAGE_FAIL:8]

### Throughput Evaluation

Next, we evaluate the improved E2E throughput, measured in FLOPS per second, of SDP4Bit on both hardware platforms. For all tests, the results are averaged over 10 iterations after 20 warm-up iterations. As shown in Table 2, SDP4Bit achieves an E2E training speedup of up to 4.08\(\). For models with the same model parallel configuration (e.g., 1.3B and 2.7B; 13B and 18B), both the E2E throughput and speedup from SDP4Bit increase as the model size grows due to larger models having higher computational efficiency but also encountering increased communication overhead.

The throughput of the 1.3B, 2.7B, and 6.7B models across the two platforms indicates that SDP4Bit provides a more significant speedup when network bandwidth is lower. This is because lower bandwidth results in higher communication overhead, which SDP4Bit effectively reduces through efficient quantization techniques.

In addition, we demonstrate the scalability of SDP4Bit using GPT models of 6.7B and 13B parameters, with tests conducted on up to 128 GPUs, as shown in Figure 8. Under low bandwidth conditions, SDP4Bit achieves an average speedup of 3.40\(\) for the 6.7B model and 2.49\(\) for the 13B model. In high-bandwidth InfiniBand environments, the speedup averages 3.08\(\) for the 6.7B model and 3.73\(\) for the 13B model. The comparatively lower speedup for the 13B model under low bandwidth conditions can be attributed to the introduction of pipeline parallelism, which diminishes the proportion of communication handled by ShardedDP. Overall, SDP4Bit consistently maintains stable speedup performance across various GPU numbers and network environments.

### Ablation Study

**Components Breakdown.** Figure 8 demonstrates the throughput improvement of qWD, TLq-HS, and their combination (SDP4Bit) on two different platforms. qWD alone provides a speedup ranging from 1.1\(\) to 1.2\(\), while TLq-HS alone results in an E2E speedup of 1.4\(\) to 1.8\(\). The notable benefit from gradient quantization stems from the high communication overhead associated with Float32 gradients in baseline training, which is higher compared to BFloat16 weights. When they are applied together, SDP4Bit achieves a more substantial speedup, ranging from 1.6\(\) to 2.4\(\).

**TLq-HS vs. ULq.** Table 4 compares gradient quantization between TLq-HS and ULq. The results show that although TLq-HS employs 8-bit quantization for intra-node gradient communication, it introduces negligible overhead compared to 4-bit communication. This is due to 1) the high bandwidth of intra-node communication and 2) the fact that most intra-node communication is overlapped with the slower inter-node communication.

**Hadamard Kernel Fusion.** Table 4 shows that, compared to the SDP4Bit without fusing Hadamard Transform kernel, our optimized SDP4Bit reduces gradient communication overhead by 29%. Additionally, we provide a throughput comparison in Table 5 to further illustrate the impact of the Hadamard transformation. The results confirm that our Hadamard kernel fusion effectively reduces the overhead, making the transformation nearly zero-overhead and even matching the performance of quantization without the Hadamard transformation.

**Convergence with Different Group Sizes.** Table 3 examines the impact of various quantization granularities on the end-to-end validation loss during the pre-training of the GPT-125M model. For TLq-HS, a gradient quantization group size of 128 presents sufficient, with smaller sizes yielding no significant accuracy improvements. For qWD, a quantization group size of 2048 achieves training accuracy comparable to the baseline. Table 3 also presents the 4-bit weight quantization (\(qW\)) while using small group size. It is evident that even with very small group size (e.g., 32), direct 4-bit quantization leads to a significant gap in accuracy compared to the baseline, making 4-bit quantization for weights suboptimal.

## 6 Related Work

Apart from ZeRO++  and QSDP , which are specifically designed for communication compression in ShardedDP, most previous studies have focused on traditional DP, primarily utilizing gradient compression. This includes both unbiased compression techniques [1; 33; 38; 5], which employ randomized compressors, and biased compression methods with error compensation [12; 31; 30; 29; 24] that require extra storage for residual errors, making them less suitable for resource-intensive training of LLMs. Other strategies like local optimization or federated learning reduce communication frequency rather than volume [16; 28; 35; 34; 2; 20], but increase memory use, complicating their application in LLM training. In addition, techniques like low-precision training [19; 22] and parameter-efficient fine-tuning [10; 3; 14] minimize the volume of trainable variables to reduce communication. In a different vein, weight quantization for inference has also been explored [7; 6; 39; 37; 4], employing more resource-intensive methods compared to those used in training to fine-tune compression parameters.

The Hadamard transform has been applied to machine learning data, as seen in HQ-MM's  compression of activations and THC's  gradient communication within a parameter server framework. Unlike THC, SDP4Bit enhances collective communication operations and GPU optimization.

## 7 Conclusion

In this paper, we propose SDP4Bit, a communication reduction strategy for Sharded Data Parallelism. SDP4Bit reduces both weight and gradient communication to nearly 4 bits while maintaining model accuracy comparable to the baseline. We implemented SDP4Bit in Megatron-LM and optimized it to reduce quantization overhead. Specifically, our experimental results demonstrate a training speedup of up to 4.08 \(\) on 128 GPUs. This paper focuses on LLM pre-training, but we plan to extend our work to other models and areas such as MoE, computer vision, and fine-tuning in the future.