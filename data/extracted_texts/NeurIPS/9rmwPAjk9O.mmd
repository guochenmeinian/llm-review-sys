# Gradient-Free Kernel Stein Discrepancy

Matthew A. Fisher\({}^{1}\), Chris. J Oates\({}^{1,2}\)

\({}^{1}\)Newcastle University, UK

\({}^{2}\)Alan Turing Institute, UK

###### Abstract

Stein discrepancies have emerged as a powerful statistical tool, being applied to fundamental statistical problems including parameter inference, goodness-of-fit testing, and sampling. The canonical Stein discrepancies require the derivatives of a statistical model to be computed, and in return provide theoretical guarantees of convergence detection and control. However, for complex statistical models, the stable numerical computation of derivatives can require bespoke algorithmic development and render Stein discrepancies impractical. This paper focuses on posterior approximation using Stein discrepancies, and introduces a collection of non-canonical Stein discrepancies that are _gradient-free_, meaning that derivatives of the statistical model are not required. Sufficient conditions for convergence detection and control are established, and applications to sampling and variational inference are presented.

## 1 Introduction

Stein discrepancies were introduced in Gorham and Mackey (2015), as a way to measure the quality of an empirical approximation to a continuous statistical model involving an intractable normalisation constant. Rooted in _Stein's method_(Stein, 1972), the idea is to consider empirical averages of a large collection of test functions, each of which is known to integrate to zero under the statistical model. To date, test functions have been constructed by combining derivatives of the statistical model with reproducing kernels (Chwialkowski et al., 2016; Liu et al., 2016; Gorham and Mackey, 2017; Gong et al., 2021, 2021), random features (Huggins and Mackey, 2018), diffusion coefficients and functions with bounded derivatives (Gorham et al., 2019), neural networks (Grathwohl et al., 2020), and polynomials (Chopin and Ducrocq, 2021). The resulting discrepancies have been shown to be powerful statistical tools, with diverse applications including parameter inference (Barp et al., 2019; Matsubara et al., 2022), goodness-of-fit testing (Jitkrittum et al., 2017; Fernandez et al., 2020), and sampling (Liu and Lee, 2017; Chen et al., 2018, 2019; Riabiz et al., 2022; Hodgkinson et al., 2020; Fisher et al., 2021). However, one of the main drawbacks of these existing works is the requirement that derivatives both exist and can be computed.

The use of non-differentiable statistical models is somewhat limited but includes, for example, Bayesian analyses where Laplace priors are used (Park and Casella, 2008; Rockova and George, 2018). Much more common is the situation where derivatives exist but cannot easily be computed. In particular, for statistical models with parametric differential equations involved, one often requires different, more computationally intensive numerical methods to be used if the _sensitivities_(i.e. derivatives of the solution with respect to the parameters) are to be stably computed (Cockayne and Duncan, 2021). For large-scale partial differential equation models, as used in finite element simulation, computation of sensitivities can increase simulation times by several orders of magnitude, if it is practical at all.

The motivation and focus of this paper is on computational methods for posterior approximation, and to this end we propose a collection of non-canonical Stein discrepancies that are _gradient free_, meaning that computation of the derivatives of the statistical model is not required. Gradient-freeStein operators were introduced in Han and Liu (2018) in the context of Stein variational gradient descent (Liu and Wang, 2016), but the theoretical properties of the corresponding discrepancy have yet to be investigated. General classes of Stein discrepancies were analysed in Huggins and Mackey (2018); Gorham et al. (2019), but their main results do not cover the gradient-free Stein discrepancies developed in this work, for reasons that will be explained. The combination of gradient-free Stein operators and reproducing kernels is studied in detail, to obtain discrepancies that can be explicitly computed. The usefulness of these discrepancies depends crucially on their ability to detect the convergence and non-convergence of sequences of probability measures to the posterior target, and in both directions positive results are established.

OutlineGradient-free KSD (GF-KSD) is proposed and theoretically analysed in Section 2. The proposed discrepancy involves certain degrees of freedom, including a probability density denoted \(q\) in the sequel, and strategies for specifying these degrees of freedom are empirically assessed in Section 3. Two applications are then explored in detail; Stein importance sampling (Section 4.1) and Stein variational inference (Section 4.2). Conclusions are drawn in Section 5.

## 2 Methods

This section contains our core methodological (Section 2.1) and theoretical (Section 2.2) development. The following notation will be used:

Real Analytic NotationFor a twice differentiable function \(f:^{d}\), let \(_{i}f\) denote the partial derivative of \(f\) with respect to its \(i\)th argument, let \( f\) denote the gradient vector with entries \(_{i}f\), and let \(^{2}f\) denote the Hessian matrix with entries \(_{i}_{j}f\). For a sufficiently regular bivariate function \(f:^{d}^{d}\), let \((_{i}_{j})f\) indicate the application of \(_{i}\) to the first argument of \(f\), followed by the application of \(_{j}\) to the second argument. (For derivatives of other orders, the same tensor notation \(\) will be used.)

Probabilistic NotationLet \((^{d})\) denote the set of probability distributions on \(^{d}\). Let \((x)(^{d})\) denote an atomic distribution located at \(x^{d}\). For \(,_{0}(^{d})\), let \(_{0}\) indicate that \(\) is absolutely continuous with respect to \(_{0}\). For \((^{d})\) and \((_{n})_{n}(^{d})\), write \(_{n}\) to indicate weak convergence of \(_{n}\) to \(\). The symbols \(p\) and \(q\) are reserved for probability density functions on \(^{d}\), while \(\) is reserved for a generic element of \((^{d})\). For convenience, the symbols \(p\) and \(q\) will also be used to refer to the probability distributions that these densities represent.

### Gradient-Free Kernel Stein Discrepancy

The aim of this section is to explain how a GF-KSD can be constructed. Let \(p(^{d})\) be a target distribution of interest. Our starting point is a gradient-free Stein operator, introduced in Han and Liu (2018) in the context of Stein variational gradient descent (Liu and Wang, 2016):

**Definition 1** (Gradient-Free Stein Operator).: _For \(p,q(^{d})\) with \(q p\) and \( q\) well-defined, the gradient-free Stein operator is defined as_

\[_{p,q}h:=( h+h q),\]

_acting on differentiable functions \(h:^{d}^{d}\)._

The _Langevin_ Stein operator of Gorham and Mackey (2015) is recovered when \(p=q\), but when \(q p\) the dependence on the derivatives of \(p\) is removed. The operator \(_{p,q}\) can still be recognised as a _diffusion_ Stein operator, being related to the infinitesimal generator of a diffusion process that leaves \(p\) invariant; however, it falls outside the scope of the theoretical analysis of Huggins and Mackey (2018); Gorham et al. (2019), for reasons explained in Remark 2. It can also be viewed as a non-standard instance of the _density method_ of Diaconis et al. (2004); see Section 2 of Anastasiou et al. (2023). The inclusion of \(q\) introduces an additional degree of freedom, specific choices for which are discussed in Section 3.

**Remark 1**.: _The ratio \(q/p\) in Definition 1 could be viewed as an importance weight, but the construction does not fully correspond to importance sampling due to the \( q\) term, which is \(q\)-dependent._The _Stein operator_ nomenclature derives from the vanishing integral property in Proposition 1 below, which is central to Stein's method [Stein, 1972]:

**Proposition 1**.: _In the setting of Definition 1, assume that \(\|x\|^{d-1}q(x) 0\) as \(\|x\|\) and \(\| q\|\ q<\). Then, for any function \(h:^{d}^{d}\) whose first derivatives exist and are bounded, it holds that \(_{p,q}h\ p=0\)._

All proofs are contained in Appendix A. From Proposition 1, the expectation of \(_{p,q}h\) with respect to \((^{d})\) will be zero when \(\) and \(p\) are equal; conversely, the value of such an expectation can be used to quantify the extent to which \(\) and \(p\) are different. Consideration of multiple test functions increases the number and nature of the differences between \(\) and \(p\) that may be detected. A _discrepancy_ is obtained by specifying which test functions \(h\) are considered, and then taking a supremum over the expectations associated to this set. For computational convenience we take \(h\) to be contained in the unit ball of a reproducing kernel Hilbert space, as described next.

For a symmetric positive definite function \(k:^{d}^{d}\), called a _kernel_, denote the associated reproducing kernel Hilbert space as \((k)\). Let \((k)^{d}\) denote the Cartesian product of \(d\) copies of \((k)\), equipped with the inner product \( h,g_{(k)^{d}}:=_{i=1}^{d} h_{i},g_{i} _{(k)}\).

**Proposition 2**.: _Let \((^{d})\). In the setting of Definition 1, assume there is an \(>1\) such that \((q/p)^{}\ <\) and \(\| q\|^{/(-1)}\ <\). Let \(k:^{d}^{d}\) be a continuously differentiable kernel such that both \(k\) and its first derivatives \(x(_{i}_{i})k(x,x)\), \(i=1,,d\), are bounded. Then \(_{p,q}\) is a bounded linear operator from \((k)^{d}\) to \(L^{1}()\)._

For discrete distributions \(\), supported on a finite subset of \(^{d}\), the moment conditions in Proposition 2 are automatically satisfied. For general distributions \(\) on \(^{d}\), the exponent \(\) can be taken arbitrarily close to 1 to enable the more stringent moment condition \((q/p)^{}\ <\) to hold. An immediate consequence of Proposition 2 is that Definition 2 below is well-defined.

**Definition 2** (Gradient-Free Kernel Stein Discrepancy).: _For \(p\), \(q\), \(k\) and \(\) satisfying the preconditions of Proposition 2, the gradient-free KSD is defined as_

\[_{p,q}()=\{_{p,q}h\ :\|h\|_{ (k)^{d}} 1\}.\] (1)

The GF-KSD coincides with the canonical kernel Stein discrepancy (KSD) when \(p=q\), and is thus strictly more general. Note that \(_{p,q}()\) is precisely the operator norm of the linear functional \(h_{p,q}h\ \), which exists due to Proposition 2. Most common kernels satisfy the assumptions of Proposition 2, and a particularly important example is the _inverse multi-quadratic_ kernel

\[k(x,y)=(^{2}+\|x-y\|^{2})^{-},(0,),\ (0,1),\] (2)

which has bounded derivatives of all orders; see Lemma 4 of Fisher et al. .

The use of reproducing kernels ensures that GF-KSD can be explicitly computed; see Proposition 5 and Corollary 1 in Appendix A. A general spectral characterisation of GF-KSD is provided in Proposition 6 of the supplement, inspired by the recent work of Wynne et al. . Note that the scale of Equation (6) does not matter when one is interested in the relative performance of different \((^{d})\) as approximations of a _fixed_ target \(p(^{d})\). In this sense, GF-KSD may be employed with \(\) in place of \(p\), where \(p/Z\) and \(\) is a normalisation constant. This feature makes GF-KSD applicable to problems of posterior approximation, and will be exploited for both sampling and variational inference in Section 4. On the other hand, GF-KSD is not applicable to problems in which the target distribution \(p_{}\) involves a parameter \(\), such as estimation and composite hypothesis testing, since then the normalisation term \(Z_{}\) cannot be treated as constant.

Going beyond the discrepancies discussed in Section 1, several non-canonical discrepancies have recently been proposed based on Stein's method, for example _sliced_[Gong et al., 2021a,b], _stochastic_[Gorham et al., 2020], and _conditional_[Singhal et al., 2019] Stein discrepancies (in all cases derivatives of \(p\) are required). However, only a subset of these discrepancies have been shown to enjoy important guarantees of convergence detection and control. Convergence control is critical for the posterior approximation task considered in this work, since this guarantees that minimisation of Stein discrepancy will produce a consistent approximation of the posterior target. The aim of the next section is to establish such guarantees for GF-KSD.

### Convergence Detection and Control

The canonical KSD benefits from theoretical guarantees of convergence detection and control [Gorham and Mackey, 2017]. The aim of this section is to establish analogous guarantees for GF-KSD. To set the scene, for a Lipschitz function \(f:^{d}^{d}\), denote its Lipschitz constant \(L(f):=_{x y}\|f(x)-f(y)\|/\|x-y\|\). Then, for measurable \(g:^{d}\), we denote the _tilted_ Wasserstein distance as \(_{1}(,p;g):=_{L(f) 1} fg\;- fg\; p\) whenever this expression is well-defined [Huggins and Mackey, 2018]. Note that the standard 1-Wasserstein distance \(_{1}(,p)\) is recovered when \(g=1\). There is no dominance relation between \(_{1}(,;g)\) for different \(g\); the topologies they induce are different. Tilted Wasserstein distance induces a much weaker topology than, for example, divergences such as Kullback-Leibler or Hellinger, since it does not require absolute continuity of measures.

**Theorem 1** (Convergence Detection).: _Let \(p,q(^{d})\) with \(q p\), \( q\) Lipschitz and \(\| q\|^{2}\;q<\). Assume there is an \(>1\) such that the sequence \((_{n})_{n}(^{d})\) satisfies \((q/p)^{}_{n}(0,)\), \(\| q\|^{/(-1)}_{n}<\), \(\| q\|^{/(-1)}(q/p)\;_{n}<\), and \( fq/p\;_{n}<\) with \(f(x)=\|x\|\), for each \(n\). Let \(k\) be a kernel such that each of \(k\), \((_{i}_{i})k\) and \((_{i}_{j}_{i}_{j})k\) exist, are continuous, and are bounded, for \(i,j\{1,,d\}\). Then \(_{1}(_{n},p;q/p) 0\) implies \(_{p,q}(_{n}) 0\)._

Thus the convergence of \(_{n}\) to \(p\), in the sense of the tilted Wasserstein distance with \(g=q/p\), is _detected_ by the GF-KSD \(_{p,q}\). Note that the conditions on \((_{n})_{n}\) in Theorem 1 are automatically satisfied when each \(_{n}\) has a finite support.

Despite being a natural generalisation of KSD, this GF-KSD does _not_ in general provide weak convergence control in the equivalent theoretical context. Indeed, Gorham and Mackey  established positive results on convergence control for distributions in \((^{d})\), the set of probability distributions on \(^{d}\) with positive density function \(q:^{d}(0,)\) for which \( q\) is Lipschitz and \(q\) is _disantly dissipative_, meaning that

\[_{r}\;\{-}:\|x-y\|=r\}>0.\]

Under the equivalent assumptions, convergence control fails for GF-KSD in general:

**Proposition 3** (Convergence Control Fails in General).: _Let \(k\) be a (non-identically zero) radial kernel of the form \(k(x,y)=(x-y)\) for some twice continuously differentiable \(:^{d}\), for which the preconditions of Proposition 2 are satisfied. Then there exist \(p(^{d})\), \(q(^{d})\) and a sequence \((_{n})_{n}(^{d})\), also satisfying the preconditions of Proposition 2, such that \(_{p,q}(_{n}) 0\) and yet \(_{n} p\)._

The purpose of Proposition 3 is to highlight that GF-KSD is not a trivial extension of canonical KSD; it requires a bespoke treatment. This is provided in Theorem 2, next. Indeed, to ensure that GF-KSD provides convergence control, additional condition on the tails of \(q\) are required:

**Theorem 2** (Convergence Control).: _Let \(p(^{d})\), \(q(^{d})\) be such that \(p\) is continuous and \(_{x^{d}}q(x)/p(x)>0\). Assume there is an \(>1\) such that the sequence \((_{n})_{n}(^{d})\) satisfies \((q/p)^{}_{n}<\) and \(\| q\|^{/(-1)}(q/p)\;_{n}<\), for each \(n\). Let \(k\) be the inverse multi-quadric kernel in Equation (2). Then \(_{p,q}(_{n}) 0\) implies \(_{n} p\)._

The proof of Theorem 2 is based on carefully re-casting GF-KSD between \(\) and \(p\) as a canonical KSD between \(q\) and a transformed distribution \(\) (see Proposition 7 in the supplement), then appealing to the analysis of Gorham and Mackey . Compared to the analysis of KSD in Gorham and Mackey , the distant dissipativity condition now appears on \(q\) (a degree of freedom), rather than on \(p\) (a distribution determined by the task at hand), offering a realistic opportunity for this condition to be verified. For example, one could take a Gaussian measure \(q\) that dominates the target distribution \(p\) for use in GF-KSD. Nevertheless, the conditions of Theorem 2 rule out distributions \(p\) that are heavy-tailed. Suitable choices for \(q\) are considered in Section 3.

**Remark 2** (Related Work).: _Convergence control was established for discrepancies based on general classes of Stein operator in earlier work, but the required assumptions are too stringent when applied in our context. In particular, to use Huggins and Mackey  it is required that the gradient\((q/p)\) is bounded1, while to use Gorham et al. (2019) it is required that \(q/p\) is Lipschitz2. In our context, where \(q\) must be specified in ignorance of \(p\), such conditions, which require that \(q\) is almost as light as \(p\) in the tail, cannot be guaranteed to hold. The present paper therefore instead contributes novel analysis for the regime where \(q\) may be appreciably heavier than \(p\) in the tail._

This completes our theoretical assessment, but the practical performance of GF-KSD remains to be assessed. Suitable choices for both \(q\) and the kernel parameters \(\) and \(\) are proposed and investigated in Section 3, and practical demonstrations of GF-KSD are presented in Section 4.

## 3 Implementation Detail

The purpose of this section is to empirically explore the effect of varying \(q\), \(\) and \(\), aiming to arrive at reasonable default settings. In the absence of an application-specific optimality criterion, we aim to select values that perform well (in a sense to be specified) over a range of scenarios that may be encountered. Here, to assess performance several sequences \((_{n})_{n}\) are considered, some of which converge to a specified limit \(p\) and the rest of which converge to an alternative Gaussian target; see Figure 1(a). An effective discrepancy should clearly indicate which of these sequences are convergent and which are not. On this basis, recommendations for \(q\) are considered in Section 3.1, and recommendations for \(\) and \(b\) in Section 3.2. Of course, we cannot expect default settings to perform universally well, so in Section 3.3 we highlight scenarios where our defaults may fail. Python code to reproduce the experiments reported below can be downloaded at [blinded].

### Choice of \(q\)

In what follows we cast \(q\) as an approximation of \(p\), aiming to inherit the desirable performance of canonical KSD for which \(q\) and \(p\) are equal. The task to which the discrepancy is being applied will,

Figure 1: Empirical assessment of gradient-free kernel Stein discrepancy. (a) Test sequences \((_{n})_{n}\), defined in Appendix C.1. The first column displays sequences (solid) that converge to the distributional target \(p\) (black), while the second column displays sequences (dashed) which converge instead to a fixed Gaussian target. (b) Performance of gradient-free kernel Stein discrepancy, when approaches to selecting \(q\) (described in the main text) are employed. The colour and style of each curve in (b) indicates which of the sequences in (a) is being considered. [Here we fixed the kernel parameters \(=1\) and \(=1/2\).]

in practice, constrain the nature and form of the distributions \(q\) that can be implemented. For expository purposes (only), the following qualitatively distinct approaches to choosing \(q\) are considered:

* Prior In Bayesian settings where \(p\) is a posterior distribution, selecting \(q\) to be the prior distribution ensures that the condition \(_{x^{d}}q(x)/p(x)>0\) is satisfied.
* Laplace If the target \(p\) can be differentiated, albeit at a possibly high computational cost, it may be practical to construct a Laplace approximation \(q\) to the target (Gelman et al., 2013).
* GMM One could take \(q\) to be a Gaussian mixture model fitted to approximate samples from \(p\), representing a more flexible alternative to Laplace.
* KDE Additional flexibility can be obtained by employing a kernel density estimator as a non-parametric alternative to GMM.

Of course, there is a circularity to GMM and KDE which renders these methods impractical in general. The specific details of how each of the \(q\) were constructed are contained in the code that accompanies this paper, but the resulting \(q\) are displayed as insets in Figure 1(b). The performance of GF-KSD with these different choices of \(q\) is also displayed in Figure 1(b). It was observed that all four choices of \(q\) produced a discrepancy that could detect convergence of \(_{n}\) to \(p\), though the detection of convergence was less clear for Prior due to slower convergence of the discrepancy to 0 as \(n\) was increased. On the other hand, all approaches were able to clearly detect non-convergence to the target. That Laplace performed comparably with GMM and KDE was surprising, given that the target \(p\) is not well-approximated by a single Gaussian component. These results are for a specific choice of target \(p\), but in Appendix C.5.1 a range of \(p\) are considered and similar conclusions are obtained. Section 3.3 explores how challenging \(p\) must be before the convergence detection and control properties associated to Laplace fail.

### Choice of \(\) and \(\)

For the investigation in Section 3.1 the parameters of the inverse multi-quadric kernel (2) were fixed to \(=1\) and \(=1/2\), the latter being the midpoint of the permitted range \((0,1)\). In general, care in the selection of these parameters may be required. The parameter \(\) captures the scale of the data, and thus standardisation of the data may be employed to arrive at \(=1\) as a natural default. In this paper (with the exception of Section 4.2) the standardisation \(x C^{-1}x\) was performed, where \(C\) is the covariance matrix of the approximating distribution \(q\) being used. In Appendix C.5.2 we reproduce the investigation of Section 3.1 using a range of values for \(\) and \(\); these results indicate the performance of GF-KSD is remarkably insensitive to perturbations around \((,)=(1,1/2)\).

### Avoidance of Failure Modes

GF-KSD is not a silver bullet, and there are a number of specific failure modes that care may be required to avoid. The four main failure modes are illustrated in Figure 2. These are as follows: (a) \(q\) is substantially heavier than \(p\) in a tail; (b) \(q\) is substantially lighter than \(p\) in a tail; (c) the dimension \(d\) is too high; (d) \(p\) has well-separated high-probability regions. Under both (a) and (c), convergence detection can fail, either because theoretical conditions are violated or because the terms \(_{n}\) must be extremely close to \(p\) before convergence begins to be detected. Under (b), the values of GF-KSD at small \(n\) can mislead. Point (d) is a well-known pathology of all score-based methods; see Wenliang and Kanagawa (2021); Liu et al. (2023). These four failure modes inform our recommended usage of GF-KSD, summarised next.

Summary of RecommendationsBased on the investigation just reported, the default settings we recommend are Laplace with \(=1\) (post-standardisation) and \(=1/2\). Although not universally applicable, Laplace does not require samples from \(p\), and has no settings that must be user-specified. Thus we recommend the use of Laplace in situations where a Laplace approximation can be justified and computed. If Laplace not applicable, then one may attempt to construct an approximation \(q\) using techniques available for the task at hand (e.g. in a Bayesian setting, one may obtain \(q\) via variational inference, or via inference based on an approximate likelihood). These recommended settings will be used for the application presented in Section 4.1, next.

## 4 Applications

To demonstrate potential uses of GF-KSD, two applications to posterior approximation are now presented; Stein importance sampling (Section 4.1) and Stein variational inference using measure transport (Section 4.2). In each case, we extend the applicability of existing algorithms to statistical models for which certain derivatives of \(p\) are either expensive or non-existent.

### Gradient-Free Stein Importance Sampling

Stein importance sampling (Liu and Lee, 2017; Hodgkinson et al., 2020) operates by first sampling independently from a tractable approximation of the target \(p\) and then correcting the bias in the samples so-obtained. To date, applications of Stein importance sampling have been limited to instances where the statistical model \(p\) can be differentiated; our contribution is to remove this requirement. In what follows we analyse Stein importance sampling in which independent samples \((x_{n})_{n}\) are generated from the same approximating distribution \(q\) that is employed within GF-KSD:

**Theorem 3** (Gradient-Free Stein Importance Sampling).: _Let \(p(^{d})\), \(q(^{d})\) be such that \(p\) is continuous and \(_{x^{d}}q(x)/p(x)>0\). Suppose that \(\{\| q\|^{2}\}\,q<\) for some \(>0\). Let \(k\) be the inverse multi-quadric kernel in Equation (2). Let \((x_{n})_{n}\) be independent samples from \(q\). To the sample, assign optimal weights_

\[w^{*}\{_{p,q}(_{i=1}^{n}w_{i}(x_{i} )):0 w_{1},,w_{n},\;w_{1}++w_{n}=1\}.\]

_Then \(_{n}:=_{i=1}^{n}w_{i}^{*}(x_{i})\) satisfies \(_{n}}}{{}}p\) almost surely as \(n\)._

The proof builds on earlier work in Riabiz et al. (2022). Note that the optimal weights \(w^{*}\) can be computed without the normalisation constant of \(p\), by solving a constrained quadratic programme at cost \(O(n^{3})\).

As an illustration, we implemented gradient-free Stein importance sampling to approximate a posterior arising from a discretely observed Lotka-Volterra model

\[(t)= u(t)- u(t)v(t),(t)=- v(t)+ u( t)v(t),(u(0),v(0))=(u_{0},v_{0}),\]

with independent log-normal observations with covariance matrix \((_{1}^{2},_{2}^{2})\). The parameters to be inferred are \(\{,,,,u_{0},v_{0},_{1},_{2}\}\) and therefore \(d=8\). The data analysed are due to Hewitt (1921), and full details are contained in Appendix C.3. The direct application of Stein importance sampling to this task requires the numerical calculation of _sensitivities_ of the differential equation at each of the \(n\) samples that are to be re-weighted. Aside from simple cases where automatic differentiation or adjoint methods can be used, the stable computation of sensitivities can form a major computational bottleneck; see (Riabiz et al., 2022). In contrast, our approach required a

Figure 2: Failure modes: (a) \(q\) is substantially heavier than \(p\) in a tail; (b) \(q\) is substantially lighter than \(p\) in a tail; (c) the dimension \(d\) is too high; (d) \(p\) has separated high-probability regions. [For each of (a), (b) and (d), the colour and style of the curves refers to the same sense of convergence or non-convergence (outward/inward/oblique) presented in Figure 1, and we plot the logarithm of the gradient-free kernel Stein discrepancy as a function of the index \(n\) of the sequence \((_{n})_{n}\). For (c) we consider convergent sequences \((_{n})_{n}\) of distributions on \(^{d}\).

fixed number of gradient computations to construct a Laplace approximation3, independent of the number \(n\) of samples required; see Appendix C.3 for detail. In the lower triangular portion of Figure 3(a), biased samples from the Laplace approximation \(q\) (\( p\)) are displayed, while in the upper triangular portion the same samples are re-weighted using gradient-free Stein importance sampling to form a consistent approximation of \(p\). A visual reduction in bias and improvement in approximation quality can be observed. As a baseline against which to assess the quality of our approximation, we consider self-normalised importance sampling; i.e. the approximation with weights \(\) such that \(_{i} p(x_{i})/q(x_{i})\). Figure 3(b) reports the accuracy of the approximations to \(p\) as quantified using _energy distance_(Cramer, 1928). These results indicate that the approximations produced using gradient-free Stein importance sampling improve on those constructed using self-normalised importance sampling. This may be explained by the fact that the optimal weights \(w^{*}\) attempt to mitigate both bias due to \(q p\) and Monte Carlo error, while the weights \(\) only address the bias due to \(q p\), and do not attempt to mitigate error due to the randomness in Monte Carlo. Additional experiments in Appendix C.5.3 confirm that gradient-free Stein importance sampling achieves comparable performance with gradient-based Stein importance sampling in regimes where the Laplace approximation can be justified.

Although our recommended default settings for GF-KSD were successful in this example, an interesting theoretical question would be to characterise an optimal choice of \(q\) in this context. This appears to be a challenging problem but we hope to address it in future work. In addition, although we focused on Stein importance sampling, our methodology offers the possibility to construct gradient-free versions of other related algorithms, including the _Stein points_ algorithms of Chen et al. (2018, 2019), and the _Stein thinning_ algorithm of Riabiz et al. (2022).

### Stein Variational Inference Without Second-Order Gradient

Stein discrepancy was proposed as a variational objective in Ranganath et al. (2016) and has demonstrated comparable performance with the traditional Kullback-Leibler objective in certain application areas, whilst abolishing the requirement that the variational family is absolutely continuous with

Figure 3: Gradient-Free Stein Importance Sampling: (a) The lower triangular panels display \(n=20\) independent (biased) samples from the Laplace approximation \(q\), while the upper triangular panels display the same number of re-weighted samples obtained using gradient-free Stein importance sampling. [Samples are shown in blue, with their size proportional to the square of their weight, to aid visualisation. The shaded background indicates the high probability regions of \(p\), the target.] (b) The approximation quality, as a function of the number \(n\) of samples from \(q\), is measured as the energy distance between the approximation and the target. [The solid line corresponds to the output of gradient-free Stein importance sampling, while the dashed line corresponds to the output of self-normalised importance sampling. Standard error regions are shaded.]

respect to the statistical model (Fisher et al., 2021). This offers an exciting, as yet largely unexplored opportunity to construct flexible variational families outside the conventional setting of normalising flows (which are constrained to be diffeomorphisms of \(^{d}\)). However, gradient-based stochastic optimisation of the canonical Stein discrepancy objective function means that second-order derivatives of the statistical model are required. GF-KSD reduces the order of derivatives that are required from second-order to first-order, and in many cases (such as when differential equations appear in the statistical model) this will correspond to a considerable reduction in computational cost.

In what follows we fix a reference distribution \(R(^{d})\) and a parametric class of maps \(T^{}:^{d}^{d}\), \(^{p}\), and consider the variational family \((_{})_{^{p}}(^{d})\) whose elements \(_{}:=T^{}_{\#}R\) are the _pushforwards_ of \(R\) through the maps \(T^{}\), \(^{p}\). The aim is to use stochastic optimisation to minimise \(_{p,q}(_{})\) (or, equivalently, any strictly increasing transformation thereof). For this purpose a low-cost unbiased estimate of the gradient of the objective function is required; the details and sufficient theoretical conditions are contained in Appendix B.

As an interesting methodological extension, that departs from the usual setting of stochastic optimisation, here we consider interlacing stochastic optimisation over \(\) and the selection of \(q\), leveraging the current value \(_{m}\) on the optimisation path to provide a natural candidate \(_{_{m}}\) for \(q\) in this context4. Thus, for example, a vanilla stochastic gradient descent routine becomes \(_{m+1}=_{m}-\,\,_{}_{p,_{ _{m}}}(_{})^{2}_{=_{m}}\) for some learning rate \(>0\). (In this iterative setting, to ensure the variational objective remains fixed, we do not perform the standardisation of the data described in Section 3.2.)

To assess the performance of GF-KSD in this context, we re-instantiated an experiment from Fisher et al. (2021). The results, in Figure 4, concern the approximation of "banana" and "sinusoidal" distributions in dimension \(d=2\), and were obtained using the reference distribution \(R=(0,2I)\) and taking \(T^{}\) to be the _inverse autoregressive flow_ of Kingma et al. (2016). These are both toy problems, which do not themselves motivate our methodological development, but do enable us to have an explicit ground truth to benchmark performance against. Full experimental detail is contained in Appendix C.4; we highlight that _gradient clipping_ was used, both to avoid extreme values of \(q/p\) encountered on the optimisation path, and to accelerate the optimisation itself (Zhang et al., 2019). The rightmost panel depicts the result of performing variational inference with the standard KSD objective functional. It is interesting to observe that GF-KSD leads to a similar performance in both examples, with the caveat that stochastic optimisation was more prone to occasional failure when GF-KSD was used. The development of a robust optimisation technique in this context requires care and a detailed empirical assessment, and is left as a promising avenue for further research.

Figure 4: Stein Variational Inference Without Second Order Gradient: The top row concerns approximation of a distributional target \(p\) that is “banana” shaped, while the bottom row concerns a “sinusoidal” target. The first four columns depict the variational approximation \(_{_{m}}\) to \(p\) constructed using gradient descent applied to gradient-free kernel Stein discrepancy (i.e. first order derivatives of \(p\) required) along the stochastic optimisation sample path (\(m\{0,2 10^{3},10^{4},2 10^{4}\}\)), while the final column reports the corresponding approximation (\(m=2 10^{4}\)) constructed using standard kernel Stein discrepancy (i.e. second order derivatives of \(p\) required).

Conclusion

In this paper GF-KSD was proposed and studied. Theoretical and empirical results support the use of GF-KSD in settings where an initial approximation to the distributional target can readily be constructed, and where the distributional target itself does not contain distant high probability regions, but poor performance can occur outside this context. Nevertheless, for many statistical analyses the principal challenge is the cost of evaluating the statistical model and its derivatives, rather than the complexity of the target itself, and in these settings the proposed discrepancy has the potential to be usefully employed. The focus of this work was on posterior approximation, with illustrative applications to sampling and variational inference being presented. A natural extension to this work would involve a systematic empirical assessment of the performance of GF-KSD across a broad range of applied contexts. However, we note that GF-KSD is not applicable to problems such as estimation and composite hypothesis testing, where the target \(p_{}\) ranges over a parametric model class, and alternative strategies will be required to circumvent gradient computation in that context.

AcknowledgementsMAF was supported by EP/W522387/1. CJO was supported by EP/W019590/1. The authors are grateful to Francois-Xavier Briol, Jon Cockayne, Jeremias Knoblauch, Lester Mackey, Marina Riabiz, Rob Salomone, Leah South, and George Wynne for insightful comments on an early draft of the manuscript, as well as to the Reviewers at NeurIPS.