# Is This Loss Informative?

Faster Text-to-Image Customization

by Tracking Objective Dynamics

 Anton Voronov

MIPT, Yandex

&Mikhail Khoroshikh

HSE University, Yandex

&Artem Babenko

HSE University, Yandex

&Max Ryabinin

HSE University, Yandex

Equal contribution. Correspondence to mryabinin0@gmail.com

###### Abstract

Text-to-image generation models represent the next step of evolution in image synthesis, offering a natural way to achieve flexible yet fine-grained control over the result. One emerging area of research is the fast adaptation of large text-to-image models to smaller datasets or new visual concepts. However, many efficient methods of adaptation have a long training time, which limits their practical applications, slows down experiments, and spends excessive GPU resources. In this work, we study the training dynamics of popular text-to-image personalization methods (such as Textual Inversion or DreamBooth), aiming to speed them up. We observe that most concepts are learned at early stages and do not improve in quality later, but standard training convergence metrics fail to indicate that. Instead, we propose a simple drop-in early stopping criterion that only requires computing the regular training objective on a fixed set of inputs for all training iterations. Our experiments on Stable Diffusion for 48 different concepts and three personalization methods demonstrate the competitive performance of our approach, which makes adaptation up to 8 times faster with no significant drops in quality.

## 1 Introduction

Large text-to-image synthesis models have recently attracted the attention of the research community due to their ability to generate high-quality and diverse images that correspond to the user's prompt in natural language . The success of these models has driven the development of new problem settings that leverage their ability to draw objects in novel environments. One particularly interesting task is _personalization_ (or _adaptation_) of text-to-image models to a small dataset of images provided by the user. The goal of this task is to learn the precise details of a specific object or visual style captured in these images: after personalization, the model should be able to generate novel renditions of this object in different contexts or imitate the style that was provided as an input.

Several recent methods, such as Textual Inversion , DreamBooth , and Custom Diffusion , offer easy and parameter-efficient personalization of text-to-image models. Still, a major obstacle on the path to broader use of such methods is their computational inefficiency. Ideally, models should adapt to user's images in real or close to real time. However, as noted by the authors of the first two papers, the training time of these methods can be prohibitively long, taking up to two hours for a single concept. The reported training time for Custom Diffusion is 12 minutes per concept on a single GPU, which is much faster but still outside the limits of many practical applications.

In this work, we seek answers to the following questions: **are text-to-image customization techniques inherently time-consuming** and **can we decrease their runtime without major quality tradeoffs?** Focusing on the training dynamics, we observe that the CLIP  image similarity score (often used to assess image quality in such tasks ) grows sharply only in the early stages of Textual Inversion and hardly improves after that. However, as shown in Figure 1, neither the training loss nor the gradient norm indicate the convergence of the concept embedding, which prevents us from stopping the adaptation process earlier. While it is possible to score samples from the model with CLIP during training, generating them can take quite a long time.

With that in mind, we study the training objective itself and attempt to understand the reasons of its non-informative dynamics. As we demonstrate, the primary cause lies in several sources of stochasticity (such as diffusion time steps or the diffusion noise) introducing noise to the loss function. If we sample random variables only once and fix them across training steps, the loss for these inputs reflects convergence better even when training with the _original_ (fully stochastic) objective.

Motivated by this finding, we propose **D**eterministic **VAR**iance Evaluation (DVAR), an early stopping criterion for text-to-image customization methods that generates a single fixed batch of inputs at the beginning of training and evaluates the model on this batch after each optimization step. This criterion is straightforward to implement, has two interpretable hyperparameters, and its stopping time corresponds to convergence in terms of the CLIP image score. We validate DVAR by comparing it with a range of baselines on three popular adaptation methods, showing that it is possible to run methods like Textual Inversion and DreamBooth much faster with up to 8x speedups and little to no decline in quality. For Custom Diffusion, our method recovers the optimal number of training iterations, which makes it useful to avoid empirical tuning of the step count for every specific dataset.

In summary, our contributions are as follows:

* We investigate the difficulties in detecting the convergence of text-to-image adaptation methods from training-time metrics. As we demonstrate, the objective becomes much more interpretable across iterations if computed on the same batch of inputs without resampling any random variables.
* We propose DVAR, a simple early stopping criterion for text-to-image personalization techniques. This criterion is easy to compute and use, does not affect the training process and correlates with convergence in terms of visual fidelity.
* We compare this criterion with multiple baselines on Stable Diffusion v1.5, a popular text-to-image model, across three methods: Textual Inversion, DreamBooth, and Custom Diffusion.1 DVAR offers a significant decrease in runtime while having quality comparable to both original methods and other early stopping baselines.

Figure 1: A summary of our key findings: the quality of methods like Textual Inversion saturates early on, but the training loss does not indicate that. Evaluating the same loss on a batch of data fixed throughout the run makes the training dynamics significantly more informative.

Background

### Denoising Diffusion Probabilistic Models

Diffusion models  are a class of generative models that has become popular in recent years due to their ability to generate both diverse and high-fidelity samples . They approximate the data distribution through iterative denoising of a variable \(z_{t}\) sampled from Gaussian distribution. In simple words, the model \(_{}\) is trained to predict the noise \(\), following the objective below:

\[_{}_{z_{0},\; N(0,I),\;t U[1,T]}|| -_{}(z_{t},c,t)||_{2}^{2}.\] (1)

Here, \(z_{t}\) corresponds to a Markov chain _forward_ process \(z_{t}(z_{0},)=}z_{0}+}\) that starts from a sample \(z_{0}\) of the data distribution. For example, in the image domain, \(z_{0}\) corresponds to the target image, and \(z_{T}\) is its version with the highest degree of corruption. In general, \(c\) can represent the condition embedding from any other modality. The inference (_reverse_) process occurs with a fixed time step \(t\) and starts from \(z_{t}\) equal to a sample from the Gaussian distribution.

### Text-to-image generation

The most natural and well-studied source of guidance for generative models is the natural language [13; 14; 15] because of its convenience for the user, ease of collecting training data, and significant improvements in text representations over the past several years . The condition vector \(c\) is often obtained from Transformer  models like BERT  applied to the input text.

State-of-the-art text-to-image models perform forward and reverse processes in the latent space of an autoencoder model \(z_{0}=(x),x=(z_{0})\), where \(x\) is the original image, and \(,\) are the encoder and the decoder, respectively. We experiment with Stable Diffusion v1.5 , which uses a variational autoencoder  for latent representations. Importantly, this class of autoencoder models is not deterministic, which makes inference even more stochastic but leads to a higher diversity of samples. In total, given image and caption distributions \(X,Y\), the training loss can be formulated as:

\[_{train}=_{y,x,}\|-_ {}(z_{t}((x),),c(y),t)\|_{2}^{2},\] (2) \[y Y,x X,(0,I),t U[0,T]\] (3)

### Adaptation of text-to-image models

While text-to-image generation models are flexible because of the natural language input, it is often difficult to design a prompt that corresponds to an exact depiction of an object of choice. Hence, several methods for adapting such models to a given collection of images were developed. In this work, we focus on the most well-known methods, discussing others in Appendix A.

Most approaches inject the target concept into the text-to-image model by learning the representation of a new token \(\) inserted into the language encoder vocabulary. The representation is learned by optimizing the reconstruction loss from Equation 2 for a few (typically 3 - 5) reference images \(I\) with respect to the embedding of \(\), while other parts of the model are frozen. The main advantage of these methods is the ability to flexibly operate with the "pseudo-word" \(\) in natural language, for example, by placing the concept corresponding to that word into different visual environments.

Textual Inversion  is the simplest method for adaptation of text-to-image models that updates only the token embedding for \(\). While this method is parameter-efficient, its authors report that reaching acceptable inversion quality requires 6000 training iterations, which equals \(\)2 GPU hours on most machines. This number is a conservative estimate of the number of steps sufficient for all concepts: authors of the method propose selecting the earliest checkpoint that exhibits sufficient train image reconstruction quality2.

While Textual Inversion only learns the embedding of the target token, DreamBooth  does the same with a fully unfrozen U-Net component, and Custom Diffusion  updates the projection matrices of cross-attention layers that correspond to keys and values. These methods use a significantly smaller number of iterations, 1000 and 500 respectively, which is also not adaptive and can lead to overfitting.

Understanding adaptation dynamics

As we explained previously, the goal of our study is to find ways of speeding up text-to-image without significantly degrading the quality of learned concept representations. To accomplish this goal, we analyze the optimization process of running Textual Inversion on a given dataset. As we demonstrate in Section 4 and in Appendix B, the same findings hold for DreamBooth and Custom Diffusion, which makes our analysis broadly applicable to different methods for customized text-to-image generation.

We apply Textual Inversion to concepts released by Gal et al. , using Stable Diffusion v1.53 as the base model. We monitor several metrics during training:

1. First, one would hope to observe that optimizing the actual training objective would lead to its convergence, and thus we track the value of \(_{train}\).
2. Second, we monitor the gradient norm, which is often used for analyzing convergence in non-convex optimization. As the model converges to a local optimum, the norm of its gradient should also decrease to zero or stabilize at a point determined by the gradient noise.
3. Lastly, every 50 training iterations, we generate 8 samples from the model using the current concept embedding and score them with the CLIP image similarity score using the training set as references. In the original Textual Inversion paper, this metric is named the reconstruction score and is used for quantitative evaluation.

Note that we do not rely on the CLIP text-image score for captions: in our preliminary experiments, we observed no identifiable dynamics for this metric when using the entire set of CLIP caption templates. Writing more specific captions and choosing the most appropriate ones for each concept takes substantial manual effort; hence, we leave caption alignment out of the scope for this study.

### Initial observations

First, we would like to view the training dynamics in terms of extrinsic evaluation: by measuring how the CLIP image score changes throughout training, we can at least estimate how fast the samples begin to resemble the training set. For this, we perform inversion of all concepts released by [6; 7; 8] (a total of 48 concepts): an example of such an experiment for one concept is available in Figure 2.

From these experiments, we observe that the CLIP image score exhibits sharper growth at an early point of training (often within the first 1000 iterations) and stabilizes later. This finding agrees with the results of our own visual inspection: the generated samples for most concepts undergo **the most drastic changes at the beginning** and do not improve afterwards. Practically, this observation means that we can interrupt the inversion process much earlier without major drawbacks if we had a criterion for detecting its convergence. What indicators can we use to create such a criterion?

The most straightforward idea is to consider the training loss \(_{train}\). Unfortunately, it is not informative by default: as we demonstrate in Figure 2, the loss exhibits too much noise and has no signs of convergence. The gradient norm of the concept embedding is also hardly informative: in the same experiment, we can see that it actually increases during training instead of decreasing. As we show in Appendix C, these findings generally hold even for much larger training batches, which means that the direct way of making the loss less stochastic is not practical. Still, as reported in the original work and shown by samples and their CLIP scores, the model successfully learns the input concepts. Curiously, we see no reflection of that in the dynamics of the training objective.

Another approach to early stopping is to leverage our observations about the CLIP image score and measure it during training, terminating when the score fails to improve for a specific number of iterations. However, there are two downsides to this approach. First, frequently sampling images during training significantly increases the runtime of the method. Second, this criterion can be viewed as directly maximizing the CLIP score, which is known to produce adversarial examples for CLIP instead of actually improving the image quality .

### Investigating the sources of randomness

We hypothesize that the cause of excessive noise in \(_{train}\) is several factors of randomness injected at each training step, as we mentioned previously in Section 2.2. Thus, we aim to estimate the influence of the following factors on the dynamics of the inversion objective:

1. Input images \(x\)
2. Captions corresponding to images \(y\)
3. VAE Latent representations for images \((x)\)
4. Diffusion time steps \(t\)
5. Gaussian diffusion noise \(\)

Now, our goal is to identify the factors of stochasticity that affect the training loss. Importantly, we **do not change** the **training** batches, as it alters the objective of Textual Inversion and might affect its outcome. Thus, we train the model in the original setting (with batches of entirely random data) but evaluate it on batches with some sources of randomness **fixed** across all iterations. Note that \((x)\) depends on \(x\): if we resample the input images, we also need to resample their latent representations.

First, we try the most direct approach of making _everything_ deterministic: in other words, we compute \(_{det}\), which is the same as \(_{train}\), but instead of the expectation over random data and noise, we compute it on the same inputs after each training step. Formally, we can define it as

\[_{det}=||-_{}(z_{t}((x),),c(y),t)||_{2}^{2},\] (4)

with \(x\), \(y\), \((x)\), \(t\), and \(\) sampled only once in the beginning of inversion. Essentially, this means that the only argument of this function that changes across training iterations is \(c(y)\) that depends on the trained concept embedding.

As we show in Figure 2, this version of the objective becomes informative, indicating convergence across a broad range of concepts. Moreover, it displays approximately the same behavior as the CLIP score and is much less expensive to compute, which makes \(_{det}\) particularly useful as a metric for the stopping criterion.

For the next step, we aim to find if any of the above sources of stochasticity have negligible impact on the noise in \(_{train}\) or can be compensated with larger batches. We evaluate them separately and provide results in Section 4.5. Our key findings are that (1) resampling **captions and VAE encoder noise** still **preserves the convergence trend**, (2) using **random images or resampling diffusion noise** reveals the training dynamics **only for large batches**, and (3) sampling **different diffusion timesteps** leads to a **non-informative training loss** regardless of the batch size. Still, for the sake of simplicity and efficiency, we compute \(_{det}\) on a batch of 8 inputs sampled only once at the beginning of training for the rest of our experiments.

Figure 2: An overview of the convergence process for Textual Inversion with an example concept. Figures 6 and 7 in Appendix B present more examples.

### Deterministic Variance Evaluation

The results above show that fixing all random components of the textual inversion loss makes its dynamics more interpretable. To achieve our final goal of decreasing the inversion runtime, we need to design an early stopping criterion that leverages \(_{det}\) to indicate convergence.

We propose Deterministic Variance Evaluation (DVAR), a simple variance-based early stopping criterion. It maintains a rolling variance estimate of \(_{det}\) over the last \(N\) steps, and once this rolling variance becomes less than \(\) of the global variance estimate (\(0<<1\)), we stop training. A pseudocode implementation of DVAR is shown in Figure 3.

This criterion is easy to implement and has two hyperparameters that are easy to tune: the window size for local variance estimation \(N\) and the threshold \(\). In our experiments, we found \(\{N=310,=0.15\}\) for Textual Inversion, \(\{N=440,=0.4\}\) for DreamBooth, and \(\{N=180,=0.15\}\) for Custom Diffusion to work relatively well across all concepts we evaluated.

Importantly, we use this criterion while training in the **regular fully stochastic setting**: our goal is not to modify the objective, and using fixed random variables and data can affect the model's generalization capabilities. As we demonstrate in Section 4, our approach demonstrates significant improvements when compared to baselines, even when all sources of randomness are fixed.

Along with DVAR, we tested other early stopping strategies that use \(_{det}\) and are based on different notions of loss value stabilization, such as estimating the trend coefficient or tracking changes in the mean instead of variance. As we show in Appendix E, most of them result in less reliable convergence indicators and have hyperparameters that do not transfer as well between different image collections.

## 4 Experiments

In this section, we compare DVAR with several baselines in terms of sample fidelity for the learned concept, output alignment with novel prompts, and training time. Our goal is to verify that this early stopping criterion is broadly applicable and has little impact on the outcome of training.

### Setup and data

We evaluate approaches to early stopping on three popular text-to-image personalization methods: Textual Inversion, DreamBooth with LoRA , and Custom Diffusion, all applied to Stable Diffusion v1.54. For Textual Inversion, we use the implementation from the Diffusers library5 and take the hyperparameters from the official repository6. We use the official code and hyperparameters7 for Custom Diffusion, and we use the implementation of DreamBooth-LoRA from the Diffusers library8.

For evaluation, we combine the datasets published by authors of the three techniques above that were available as of March 2023, which results in a total of 48 concepts. Our main results are obtained by applying each method to all of these concepts, which in total took around 80 GPU hours. Each experiment used a single NVIDIA A100 80GB GPU. Although the methods we study can be used with several concepts at a time, we train on the images of each concept separately. Following Equation 2, the training batches are generated from a small set of original images with randomly sampled augmentations (central/horizontal crop), captions, timesteps and diffusion noise.

Figure 3: An example NumPy/PyTorch implementation of DVAR. See Appendix D for an example of its usage in the training code.

To automatically assess the quality of generated images, we employ CLIP image-to-image similarity for images generated from prompts used during training ("Train CLIP img"), which allows us to monitor the degree to which the target concept is learned. However, relying solely on this metric can be risky, as it does not capture overfitting. To measure generalization, we utilize CLIP text-to-image similarity, evaluating the model's ability to generate the concept in new contexts with prompts that are not seen at training time ("Val CLIP txt"). For each method, we report both the number of iterations and the average adaptation runtime, since the average duration of one iteration may differ due to intermediate image sampling or additional loss computation. In addition, we evaluate the identity preservation and diversity of all methods in Appendix F.

### Baselines

We compare DVAR with several baselines: the original training setup of each adaptation method (named as "Baseline"), early stopping based on the CLIP similarity of intermediate samples to the training set ("CLIP-s"), as well as the original setup with the reduced number of iterations and no intermediate sampling ("Few iters").

Specifically, the original setup runs for a predetermined number of steps (6100, 1000, and 500 for Textual Inversion, Dreambooth-LoRA, and Custom Diffusion accordingly), sampling 8 images every 500/100/100 iterations and computing their CLIP similarity to the training set images. For each customization method, we tune the hyperparameters of CLIP-s: sampling frequency (number of training iterations between generating samples), as well as the threshold for minimal improvement and the number of consecutive measurements without improvement before stopping. These hyperparameters are chosen on a held-out set of 4 concepts to achieve 90% of the maximum possible train CLIP image score for the least number of iterations. The final checkpoint is selected from the iteration with the best train CLIP image score.

After running CLIP-s on all concepts, we determine the average and maximum number of steps before early stopping across concepts. Then, we run the baseline method with a reduced number of iterations and no intermediate sampling. This baseline can serve as an estimate of the minimum time required for the adaptation to converge, as it has the most efficient iterations among all methods. However, in real-world scenarios and for new customization approaches, the exact number of iterations is unknown in advance, which makes this method less applicable in practice, as it would involve rerunning such large-scale experiments for every new approach.

### Results

The results of our experiments are shown in Table 1. As we can see, DVAR is more efficient than the baseline and CLIP-s in terms of the number of iterations and overall runtime, approaching the performance of "Few iters" (which can be considered an oracle method) while being fully input-adaptive and not relying on costly intermediate sampling. In particular, the fixed step count of "Few Iters" results in higher standard deviations for almost all metrics: from a practical standpoint, it means that this number of iterations might be either sufficient or excessive when adapting a model to a given set of images. We additionally discuss the need for the adaptive number of iterations in Appendix G.

Furthermore, although CLIP-s and the original setup optimize the train CLIP image score by design (in case of the baseline, this metric is used for choosing the best final checkpoint), DVAR is able to achieve nearly the same final results. Finaly, with a sufficiently long training time, the increase in the train CLIP image score leads to a decline in the validation CLIP text score, which indicates overfitting on input images: this phenomenon is illustrated in more detail in Figure 4 and Appendix H. As we can see from the CLIP text scores of DVAR, early stopping helps mitigate this issue.

### Human evaluation

To verify the validity of our findings, we compare the quality of images obtained with DVAR with that of the original methods. To do that, we used a crowdsourcing platform, asking its users to compare samples of the baseline approach and samples from the model finetuned for the number of iterations determined by DVAR.

We conducted two surveys: the first survey compared samples derived from prompts seen by the model during training, measuring the reconstruction ability of adaptation. Participants were instructed to select the image that resembled the reference object the most. The second survey compared samples generated using novel prompts, measuring the final model capability for customization. Participants were asked to determine which image corresponded better to the given prompt. Each pair of images was assessed by multiple participants, ensuring an overlap of 10 responses. For each set of 4 responses, we paid 0.1$, and the annotation instructions can be viewed in Appendix I.

Our findings obtained from these evaluations are shown in Table 2. As we can see, DVAR allows for early stopping without sacrificing the reconstruction quality for two out of three evaluated customization methods. Hence, our approach enables efficient training and reduces the computational cost of finetuning on new concepts. While applying DVAR to Textual Inversion slightly reduces the reconstruction quality, this can be attributed to the overfitting of the original method: other methods, which use significantly fewer iterations, are able to avoid this issue.

### Analysis and ablation

Having demonstrated the advantages of DVAR, we now perform a more detailed analysis of the effect of the changes we introduce to the procedure of text-to-image adaptation. We aim to answer a series of research questions around our proposed criterion and the behavior of the training objective with all factors of variation fixed across iterations.

Is it possible to observe convergence without determinism?To answer this question, we conduct a series of experiments where we "unfix" each component responsible for the stochasticity of the training procedure one by one. For each component, we aim to find the smallest size of the batch that preserves the indicativity of the evaluation loss. For large batch sizes, we accumulate losses from several forward passes.

    &  &  &  &  &  \\   \\  Baseline & \(0.840_{ 0.051}\) & \(0.786_{ 0.075}\) & \(0.209_{ 0.021}\) & \(6100\) & \(27.0_{ 0.3}\) \\ CLIP-s & \(0.824_{ 0.053}\) & \(0.757_{ 0.067}\) & \(0.233_{ 0.024}\) & \(666.7_{ 174.5}\) & \(9.6_{ 2.5}\) \\ Few iters (mean) & \(0.796_{ 0.069}\) & \(0.744_{ 0.073}\) & \(0.232_{ 0.023}\) & \(\) & \(_{ 0.0}\) \\ Few iters (max) & \(0.806_{ 0.066}\) & \(0.767_{ 0.071}\) & \(0.219_{ 0.022}\) & \(850\) & \(2.8_{ 0.0}\) \\ DVAR & \(0.795_{ 0.067}\) & \(0.748_{ 0.068}\) & \(0.227_{ 0.024}\) & \(566.0_{ 141.5}\) & \(3.1_{ 0.8}\) \\   \\  Baseline & \(0.865_{ 0.045}\) & \(0.833_{ 0.067}\) & \(0.203_{ 0.019}\) & \(1000\) & \(7.8_{ 1.5}\) \\ CLIP-s & \(0.862_{ 0.045}\) & \(0.788_{ 0.075}\) & \(0.225_{ 0.022}\) & \(353.2_{ 88.1}\) & \(6.1_{ 1.5}\) \\ Few iters (mean) & \(0.855_{ 0.052}\) & \(0.806_{ 0.085}\) & \(0.219_{ 0.023}\) & \(\) & \(_{ 0.1}\) \\ Few iters (max) & \(0.851_{ 0.053}\) & \(0.800_{ 0.097}\) & \(0.214_{ 0.019}\) & \(500\) & \(2.6_{ 0.1}\) \\ DVAR & \(0.784_{ 0.106}\) & \(0.687_{ 0.140}\) & \(0.238_{ 0.034}\) & \(665.3_{ 94.9}\) & \(4.9_{ 0.7}\) \\   \\  Baseline & \(0.755_{ 0.077}\) & \(0.695_{ 0.069}\) & \(0.258_{ 0.021}\) & \(500\) & \(6.5_{ 0.9}\) \\ CLIP-s & \(0.757_{ 0.076}\) & \(0.691_{ 0.069}\) & \(0.258_{ 0.023}\) & \(510.4_{ 134.1}\) & \(9.7_{ 3.0}\) \\ Few iters (mean) & \(0.751_{ 0.078}\) & \(0.691_{ 0.070}\) & \(0.259_{ 0.023}\) & \(450.0_{ 0.0}\) & \(_{ 0.9}\) \\ Few iters (max) & \(0.754_{ 0.078}\) & \(0.691_{ 0.073}\) & \(0.257_{ 0.022}\) & \(700.0_{ 0.0}\) & \(5.3_{ 1.4}\) \\ DVAR & \(0.742_{ 0.074}\) & \(0.693_{ 0.066}\) & \(0.259_{ 0.022}\) & \(_{ 46.6}\) & \(_{ 1.0}\) \\   

Table 1: Comparison of speedup methods for three approaches to text-to-image personalization. The best value is in bold.

   Method & Reconstruction & Customization \\  Textual Inversion & 41.6 & 77.7 \\ DreamBooth-LoRA & 67.9 & 91.4 \\ Custom Diffusion & 69.9 & 93.8 \\   

Table 2: Human evaluation results: the percentage of cases where the quality of DVAR was better or equal to Baseline quality.

Figure 5 shows the results of these experiments. While for some parameters increasing the batch size leads to a more evident trend in loss, the stochasticity brought by varying the timesteps \(t\) on each iteration cannot be eliminated even with batches as large as 512. This can be explained by the scale of the reconstruction loss being highly dependent on the timestep of the reverse diffusion process. Thus, aggregating it over multiple different points makes it completely uninformative, as the scale of the trend is several orders of magnitude less than the average values of the loss.

Is it possible to use smaller batches for evaluation and still detect convergence?To address this question, we ran experiments with deterministic batches of size \(1,2\) and \(4\) for multiple concepts and generally observed the same behavior (as illustrated in Figure 8 of Appendix C). However, the dynamics of the objective become more dependent on the timesteps that are sampled: as a result, the early stopping criterion becomes more brittle. Hence, we use a batch size of 8 in our main experiments, as it corresponds to a reasonable tradeoff between stability and computational cost.

Does the selection of diffusion timesteps influence the behavior of the deterministic objective?In our approach, timesteps are uniformly sampled from a range of \(0\) to 1000 once at the beginning of the training process. However, according to the analysis presented in prior work , timesteps from

Figure 4: Comparison of early stopping techniques applied to personalization methods.

different ranges have varying effects on the results of text-to-image generation. This raises a question of whether it is possible to sample timesteps only from a specific range, as it might lead to a higher correlation between the loss and the target metrics. Our findings in Appendix J indicate that there is no significant difference in correlations when we change the sampling interval.

## 5 Conclusion

In this paper, we analyze the training process of text-to-image adaptation and the impact of different sources of stochasticity on the dynamics of its objective. We show that removing all these origins of noise makes the training loss much more informative during training. This finding motivates the creation of DVAR, a simple early stopping criterion that monitors the stabilization of the deterministic loss based on its running variance. Through extensive experiments, we verify that DVAR reduces the training time by \(2-8\) times while still achieving image quality similar to baselines.

Our work offers a robust and efficient approach for monitoring the training dynamics without requiring computationally expensive procedures. Moreover, it has adaptive runtime depending on the input concept, which naturally mitigates overfitting. These advantages make DVAR a useful tool for text-to-image researchers and practitioners, as it provides a more efficient and controlled training process, ultimately leading to improved results while avoiding the unnecessary computational overhead.

Figure 5: Loss behavior in the semi-deterministic setup: row names correspond to inputs that are resampled for each evaluation batch.