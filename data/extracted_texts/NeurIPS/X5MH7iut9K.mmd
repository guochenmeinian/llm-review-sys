# Towards Evaluating Transfer-based Attacks Systematically, Practically, and Fairly

Qizhang Li\({}^{1,2}\), Yiwen Guo\({}^{3}\), Wangmeng Zuo\({}^{1}\), Hao Chen\({}^{4}\)

\({}^{1}\)Harbin Institute of Technology, \({}^{2}\)Tencent Security Big Data Lab, \({}^{3}\)Independent Researcher, \({}^{4}\)UC Davis

(liqizhang95,guoyiwen89)@gmail.com wmzuo@hit.edu.cn chen@ucdavis.edu

Yiwen Guo leads the project and serves as the corresponding author.

###### Abstract

The adversarial vulnerability of deep neural networks (DNNs) has drawn great attention due to the security risk of applying these models in real-world applications. Based on transferability of adversarial examples, an increasing number of transfer-based methods have been developed to fool black-box DNN models whose architecture and parameters are inaccessible. Although tremendous effort has been exerted, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Our investigation shows that the evaluation of some methods needs to be more reasonable and more thorough to verify their effectiveness, to avoid, for example, unfair comparison and insufficient consideration of possible substitute/victim models. Therefore, we establish a transfer-based attack benchmark (TA-Bench) which implements 30+ methods. In this paper, we evaluate and compare them comprehensively on 25 popular substitute/victim models on ImageNet. New insights about the effectiveness of these methods are gained and guidelines for future evaluations are provided. Code at: https://github.com/qizhangli/TA-Bench.

## 1 Introduction

In recent years, deep neural networks (DNNs) have demonstrated unprecedented success in various applications. However, the success comes at a price: DNNs are vulnerable to adversarial examples crafted by adding imperceptible perturbations to inputs (_e.g._, images). The existence of adversarial examples poses a significant threat to the security and reliability of DNNs, especially in safety-critical applications such as autonomous driving, biometrics, and medical image analysis.

There are many different ways of generating adversarial examples and performing attacks. Transfer-based attacks, that are capability of compromising DNNs without having access to their network architecture and parameters, have been widely studied over the past few years. To issue such transfer-based attacks, an attacker first collects a substitute model or a set of substitute models, then computes gradients on the substitute model(s) and perform optimization based on the gradients. Performance of the attacks largely rely on the transferability of the generated adversarial examples. Over the years, a large number of methods have been proposed to improve the adversarial transferability. The methods innovate various aspects of the attack procedure, ranging from _substitute model training_ to _gradient computation_ (that modifies loss or forward/backward architectures given well-trained substitute models)  to _input augmentation__and the optimizer_ that applies the gradients . Some methods further propose to train a generative model using additional data for obtaining transferable adversarial examples .

Despite all the progress, there still lacks a standardized benchmark that could be taken advantage of to compare these methods systematically, fairly, and practically. Existing evaluations in the literature suffer from several limitations. First, there is a paucity of variety in the tested models, while certain methods may exhibit superior performance only with specific substitute/victim architectures. Second, many methods verified their efficacy only with a basic optimization back-end, _e.g._, I-FGSM , despite the existence of more advanced optimization including input augmentations. It is hence unclear whether the benefits of an innovation in these methods can be similarly brought by incorporating these more advanced optimization techniques, which could undermine the reliability of the evaluations.

To address these problems, we create a transfer-based attack benchmark (TA-Bench), allowing researchers to compare a variety of methods in a fair and reliable manner. We believe that TA-Bench will foster the development of adversarial machine learning and inspire effective ways of generating adversarial examples for evaluating the robustness of DNNs. Our main contributions include (but not limited to) the following items.

**A More Advanced Optimization Back-end.** We have evaluated various combinations of input augmentation mechanisms and optimizers on our TA-Bench. Our results convey that, with a few exceptions, combining more types of augmentations leads to more powerful attacks, and it could be more reasonable to evaluate the effectiveness of input augmentation mechanisms and optimizers in combination rather than in isolation. Besides, the obtained combination is suggested to be a more advanced optimization back-end for evaluating the "gradient computation" methods and "substitute model training" methods.

**Insightful Observations from Comprehensive Evaluations.** We consider various substitute/victim models that are considered popular, including CNNs (_e.g._, ResNet-50 , VGG-19  with batch normalization, Inception v3 , EfficientNetV2-M , and ConvNeXt-B ), vision transformers (_e.g._, ViT-B , DeiT-B , Swin-B , and BEiT-B ), and a MLP (MLP-Mixer ). Comprehensive results on our TA-Bench systematically shows how the performance of transfer-based attacks varies with different choices of substitute/victim models. These results demonstrate that adopting transformers as the substitute model generally yields superior attack performance for "gradient computation" methods, comparing with using traditional convolutional models (as the substitute model).

**A Unified Codebase.** We offer an open-source codebase for TA-Bench, featuring a well-organized code structure that can effectively accommodate a diverse range of transfer-based attacks, as well as various substitute/victim models. It provides a unified setting for evaluations, ensuring consistency and reproducibility in experimental results. The code is at https://github.com/qizhangli/TA-Bench.

## 2 Related Work

**Transfer-based Attacks.** Transfer-based attacks emerged as a result of the discovery that adversarial examples are not only powerful against the model they were generated on but also effective against other models. By exploiting such a discovery, attackers can use adversarial examples created on some substitute models to attack the victim model, and a series of transfer-based attacks have been proposed. These methods have innovated various aspects of the attack procedure, and we will carefully discuss them in Section 3.2.

**Related Benchmarks.** There have been several libraries or benchmarks for generating adversarial examples, such as CleverHans , Foolbox , RobustART , Torchattacks , _etc_. However, these benchmarks only cover a very limited number of transfer-based methods, as they were developed for evaluating the robustness of DNNs against not only black-box attacks but also white-box attacks. In particular, most of the methods are outdated and only considered as baseline methods due to the rapid development in this field. By contrast, our TA-Bench focuses on transfer-based attacks, and it implements 30+ methods in order to perform systematical, practical, and fair comparisons. A contemporary benchmark  also evaluates many transfer-based attacks and it draws attention to the stealthiness of the adversarial examples. Our contributions are mostly orthogonal to theirs, as we focus on fair and practical comparison between different methods. We will show some previous conclusions might be overthrown in practice. We provide new insights and guidance for evaluation to assist the future work in the community.

Our Benchmark

### Threat Model

In a black-box scenario, the adversary has limited access to the victim model. Transfer-based attack aims to compromise the victim model by generating adversarial examples on a substitute model or a set of substitute models. Recent transfer-based methods, with few exceptions [25; 48], tested in a setting where: 1) training data of the victim model is accessible and training/fine-tuning a model on such data is possible for the adversary [47; 78; 13; 27; 40; 35; 44; 75; 36], or 2) the adversary is able to collect at least one substitute model trained using the same dataset that the victim model learned from [37; 77; 22; 11; 20; 14; 65; 26; 60; 57; 61; 74; 21; 15; 74; 24; 34; 67; 7; 58; 6; 28]. We follow this assumption for setting up the benchmark. Specifically, nothing except for the training data is known about the victim model for performing attacks in our threat model, _i.e._, the adversary has no idea about the pre-processing pipeline, architecture, and parameters.

Adversarial examples on the benchmark are all obtained by performing pixel-wise perturbations to benign images under an \(_{p}\) constraint.

### Methodologies

In order to compare different methods more reasonably, we roughly divide existing methods, based on their main innovations, into four categories highlighted as follows.

**Input Augmentation and Optimizer.** To craft an adversarial example on any given substitute model, the perturbation can be optimized as in the white-box setting. Gradient-based iterative optimization is commonly utilized, in which the perturbation is initialized to a zero tensor (_e.g._, in I-FGSM ) or a random tensor whose entries are sampled from a distribution (_e.g._, in PGD ). Image data augmentation has been considered for generating transformation-robust perturbations to each benign example, for example in diverse inputs I-FGSM (DI\({}^{2}\)-FGSM) , translation-invariant I-FGSM (TI-FGSM) , scale-invariant I-FGSM (SI-FGSM) , and Admix . Several attacks also innovate by taking advantage of the momentum optimizer, _e.g._, momentum I-FGSM (MI-FGSM) , Nesterov I-FGSM (NI-FGSM) , and pre-gradient guided momentum I-FGSM (PI-FGSM) . In general, these methods are all architecture-independent.

**Gradient Computation (DNN-Specific).** There is a belief that improved adversarial transferability can be achieved by modifying the loss or the backpropagation process. For backpropagation, both the forward and the backward pass can be altered to achieve powerful attacks, and a series of methods, including SGM , LinBP , ConBP , PNA , and SE  have been proposed. Some methods advocate loss terms obtained on a middle layer of the substitute model (_e.g._, NRDM , TAP , FDA , LIA , ILA++ , FIA , NAA ), while other methods stick with loss computed on the final layers (_e.g._, IR , VT , and TAIG ).

**Substitute Model Training.** Though most prior work uses off-the-shelf models (that could be collected on the Internet) directly as substitute models, some methods advocate fine-tuning these models or even training new ones to better suit the goal of achieving transferable adversarial examples. For instance, RFA  suggests adopting adversarial training to obtain substitute models. DRA  fine-tunes the substitute model to push the adversarial examples away from the distribution of their benign counterparts during performing attacks. LGV  fine-tunes the substitute model with a high learning rate and to collect a set of models on the training trajectory. A very recent method proposed by Li _et al._ performs fine-tuning in order to obtain Bayesian substitute models .

**Generative Modeling.** In addition to the above mentioned attacks, there are another line of transfer-based attacks that use generative models to craft adversarial examples. These methods often require additional data for training the generative model. Once properly trained, the model can generate transferable adversarial examples across different victim models. Some effective methods concerned in this line include CDA , GAPF , BIA , TTP , C-GSP , _etc._

**All these mentioned methods have been implemented on our benchmark.** That is, we implemented **30+ methods** from these four categories for comprehensive evaluation and comparison of transfer-based attacks.

### Victim Models and Substitute Models

With a surge of innovation in deep learning, there exists a variety of image classification DNNs. Each has its own advantages. In practice, an engineer can choose any of them to train and deploy according to his or her specific requests. An adversary whose aim is to compromise a computer vision service developed by the engineer then anticipate the generated adversarial examples transfer well to all these possible models. Hence, to make the evaluation comprehensive and practical, we consider various victim models that are considered popular, including **CNNs** (_e.g._, ResNet-50 , VGG-19  with batch normalization, Inception v3 , EfficientNetV2-M , and ConvNeXt-B ), **vision transformers** (_e.g._, ViT-B , DeiT-B , Swin-B , and BEiT-B ), and **a MLP** (MLP-Mixer-B ). It is worth noting that all of these models were obtained directly from an open-source repository timm  on GitHub, and our benchmark can be easily updated to include new models in the future.

Having witnessed the development of image classification architectures, it is unwise for the adversary to stick with conventional models (_e.g._, ResNets, VGG-Nets, and Inceptions), since it is possible that generating adversarial examples on a more advanced substitute model leads to superior attack success rates. Thus, on this benchmark, we employ the 10 victim models named above to generate adversarial examples and evaluate the attack performance of all options. As will be shown in Section 4.3, many new insights can be gained from the evaluation results.

### Experimental Settings and Implementation

All evaluations on our benchmark are conducted on ImageNet . We randomly selected 5,000 benign examples that could be correctly classified by all the victim models, from the ImageNet validation set, to craft adversarial examples. Filenames of these benign examples will be provided, for reproducing results and testing new methods in the future. A distance metric is required to measure the magnitude of perturbations. We adopt the popular \(_{p}\) distance for \(p\{,2\}\) and set the perturbation budget under \(_{}\) and \(_{2}\) constraints to \(=8/255\) and \(=5\), respectively, to guarantee that the adversarial perturbations are almost imperceptible. The optimization process of each compared method runs \(100\) iterations with a step size of \(1/255\) and \(1\) for \(_{}\) constraint and \(_{2}\) constraint, respectively. For each victim model, we pre-process its input in exactly the same way as their official implementation to ensure a practical evaluation of attack performance, and note that the adversary has no idea about the detailed implementation of this pre-processing. For instance, when evaluating the performance of attacking a ResNet-50 victim, we first resize an adversarial example to 256 \(\) 256 by bilinear interpolation, then crop the image to 224 \(\) 224 in the center, and finally feed the 224 \(\) 224 image into the victim model and evaluate whether the attack is successful or not. Implementation details about all the supported methods are provided in Section F in the Appendix. All experiments are performed on an NVIDIA V100 GPU.

For evaluating the transferability of adversarial examples, we use the accuracy of victim models for classifying the adversarial examples as a measure. Using the prediction accuracy of victim models instead of the attack success rate makes it easier to incorporate other victim models in the future, as a reasonable calculation of attack success rate often requires the benign counterparts of the adversarial examples be classified correctly by all victim models. With a specific choice of the substitute model, prior work often evaluates the average accuracy (AA) over all victim models for comparing different attacks. However, since our benchmark studies a variety of substitute models, we further evaluate the average AA (AAA), the worst AA (WAA), and the best AA (BAA) over all choices of these substitute models. **Lower AAA, WAA, and BAA indicate stronger attacks and more vulnerable victim models.**

We have built a codebase consisting of modular components that serve as the basis of TA-Bench. By leveraging modular design principles, the substitute and victim models, back-end methods, and hyper-parameters can be easily adapted to help the future work of the community.

## 4 Evaluations and Analyses

In Section 4.1, we identify a pre-processing pipeline that is more practical. In Section 4.2, we investigate an improved back-end method by evaluating the possible combinations of iterative optimization methods. We then re-evaluate state-of-the-arts under a comprehensive and unifiedbenchmark, which incorporates various substitute and victim models, to assess the effectiveness of these methods in Section 4.3 and Section 4.4.

### Pre-processing in Practice

Modern image classification models (especially CNNs) can take images of various sizes. After investigating experimental settings of previous transfer-based attacks, we found that the adversarial examples were often evaluated by feeding them into the victim models directly (without taking pre-processing operations of the victim models into account, _e.g._, resize and crop) [28; 57; 59; 58] or just resize to the input size of the victim models [61; 74]. However, these victim models, when deployed, are often equipped with inference time data pre-processing to improve their effectiveness and efficiency. Getting rid of it may lead to over-estimation of adversarial vulnerability.

As mentioned in Section 3.4, on our benchmark, we follow the default pre-processing pipeline of each victim model and evaluate under the circumstances where pre-processing often exists. We observed degraded attack performance under such circumstances (see Figure 1 for results). Obviously, the adversarial examples seem less vulnerable when the default test-time pre-processing is re-introduced to each victim model. Given the same I-FGSM adversarial examples, the average accuracy of victim models increases to 87.79% (from 86.16%) with pre-processing, confirming that **ignoring pre-processing operations of victim models indeed leads to over-estimation of their adversarial vulnerability**. Similar observations can be made with other substitute models and other attack methods. Considering that the pre-processing pipeline of the victim model is inaccessible to the adversary, it is infeasible for the adversary to follow the same pipeline when generating adversarial examples. On the substitute models, a safe choice is to resize each of their inputs to the default size without cropping them.

### Evaluation of Input Augmentation and the Optimizer

To verifying the effectiveness of a newly developed computer vision architecture (_e.g._, vision transformers), it is common to show that its performance surpasses previous state-of-the-arts on a challenging benchmark dataset (_e.g._, ImageNet), and test of the newly developed architecture may utilize a combination of advanced optimization strategies (_e.g._, AdamW  + mixup  + cutmix  + stochastic depth ) if such a combination is beneficial [2; 55]. Novel optimization strategies are also often tested in combination with existing ones, to show their consistent effectiveness .

Likewise, the transferability of adversarial examples can also benefit from appropriate choices of input augmentations and the optimizer, however, ways of innovating these optimization strategies are often evaluated in isolation in this setting. On our benchmark, we, for the first time, evaluate combinations of different choices of input augmentations and the optimizer systematically. Specifically, we performed a grid search to seek the optimal combination of I-FGSM , PGD , DI\({}^{2}\)-FGSM , TI-FGSM , SI-FGSM , Admix , NI-FGSM , MI-FGSM , PI-FGSM , _etc_. Since Admix inherently includes SI-FGSM, we have SI-FGSM by default when Admix is chosen. NI-FGSM, MI-FGSM, and PI-FGSM seem not orthogonal and thus they are tested in a mutually exclusive way in our experiment. The same for I-FGSM and PGD. For TI-FGSM, an alternative implementation where inputs are translated is adopted, as it is more effective than its suggested approximation which convolves gradients. Besides the augmentations in DI\({}^{2}\)-FGSM, TI-FGSM, SI-FGSM, and Admix, we consider several other input augmentation mechanisms including adding uniform noise to the input (which was used in VT  and TAIG ) and randomly dropping some patches of the perturbation (which was used in IR ). We call the two methods as UN and DP, respectively. Note that in a previous work , Gaussian noise is added to the input in each attack iteration. In this study, we

Figure 1: Attack performance with different pre-processing strategies in the target environment, and the AAA is indicated by dotted lines (lower means more vulnerable to the attack). The adversarial examples were generated using I-FGSM on a ResNet-50 substitute model under the \(_{}\) constraint with \(=8/255\).

ot for uniform noise, as it is now more commonly adopted. It is also worth noting that the original implementation of SI-FGSM and Admix uses several augmented copies of an input and averages the gradients computed on these copies for optimization. Such an approach increases the computational complexity of performing attacks, and, in fact, all input augmentation mechanisms in this category can be improved by such an approach , and, for reducing computational complexity, we only craft one augmented input at each iteration.

Each possible combination is tested with every possible substitute model whose name has been mentioned in Section 3.3 to evaluate the attacking performance over other models which are considered as the victim models. The results are sorted by AAA, in a descending order from left to right, and demonstrated in Figure 2. It illustrates not only AAA but also the range between BAA and WAA (as "error bars"). **We see that the optimal AAA is achieved by UN-DP-DI\({}^{2}\)-TI-PI-FGSM, which is \(\)**. The detailed AA when each substitute model is chosen for crafting adversarial examples by the method is reported in Section E. Inspecting the obtained results, we found that the performance gap between MI-FGSM, NI-FGSM, and PI-FGSM is marginal. For instance, the top 3 solutions (with PI-FGSM, NI-FGSM, and MI-FGSM, respectively) lead to similar AAA (\(42.42\%\), \(42.45\%\), and \(42.46\%\)). The performance of I-FGSM and PGD is also similar. In most cases, PGD leads to slightly inferior performance than that of I-FGSM, accordingly to our experimental results.

In general, more input augmentation mechanisms leads to more powerful attacks. Yet there are exceptions. With DP adopted, SI-FGSM and Admix that apply input scaling and mixing fail to manifest their gains regarding the adversarial transferability. In particular, the AAA increases to \(43.12\%\) and \(44.11\%\) when further adding SI and Admix to UN-DP-DI\({}^{2}\)-TI-PI-FGSM, respectively. This is in contrast to the previous belief that these two methods are effective, and a possible explanation is that, when adopting DP and Admix/SI-FGSM simultaneously, the augmentation becomes too strong to keep the input a in-distribution sample. We adopted the default hyper-parameters for all combined methods and it is possible (yet computationally very intensive since the number of combinations is huge) to carefully tune hyper-parameters to achieve even better combinations. We will leave it to future work.

### Evaluation of "Gradient Computation" Methods and "Substitute Model Training" Methods

The previous section evaluates input augmentations and optimizers, and, in this section, we shall focus on "gradient computation" and "substitute model training" methods.

Firstly, we would like to emphasize that certain gradient computation innovations (_e.g._, IR , VT , and TAIG ) incorporate input augmentations for averaging the gradients obtained from multiple randomly augmented inputs at each iteration. These methods require high computational cost, and comparing them with other methods in the same category directly is unfair, as the other methods can also apply random augmentation and gradient averaging to boost their performance. Taking VT  as an example, it introduces random noise to the input and perform backpropagation for 20 times at each iteration. Similar for IR and TAIG. We compare them with their corresponding baselines that employ the same input augmentations and keep the same number of backpropagation operations at each iteration. That is, the corresponding baselines of VT and IR (_i.e._, VT-baseline and IR-baseline) perform multiple rounds of backpropagation with UN and DP augmentations, respectively, at each

Figure 2: Comparing different combinations of the optimization strategies. The red solid circles indicate AAA, while the grey triangles show BAA and WAA (lower indicates more powerful attack).

iteration of optimization, and the corresponding baseline of TAIG (_i.e._, TAIG-baseline) perform both input scaling and UN augmentations. Figure 3 reports the comparison results. It can be seen that the simple baselines achieve even lower AAA, **indicating that the most effective factor of these methods may be input augmentation and gradient averaging, instead of what were claimed.** Based on these findings, we strongly recommend that newly designed methods, which incorporate input augmentations, either explicitly or implicitly, should be carefully compared with reasonable baselines that adopt the same mechanisms to show their effectiveness.

There are still 10+ methods to be evaluated and compared. Previous work often compares using a simple optimization back-end, _e.g._, I-FGSM or MI-FGSM. As we have obtained a combination of optimization strategies (_i.e._, UN-DP-DI\({}^{2}\)-TI-PI-FGSM) which has been proven to be more powerful in Section 4.2, we further introduce it as a more advanced optimization back-end. It aids in better exploring true advantages of compared methods. To ensure optimal performance across different substitute models, we employed a validation set consisting of 500 samples that were distinct from the test examples tune hyper-parameters of compared methods. The hyper-parameters that yielded the best results on the validation set were then adopted for testing. The hyper-parameters with each substitute model are tuned using models whose name have been mentioned in Section 3.3. The attack performance is evaluated not only on these "validation" models, but also on 15 more victim models which are distinct from them. The detailed hyper-parameters are reported in Section F.

In Table 1 (column 2-11), we report the AA achieved by utilizing each model as the substitute model to attack the 9 "validation" models under the \(_{}\) constraint. AAA (which is the average AA over all substitute models) is provided in the 12-th column in the table. Note that some methods are unable to be performed on some architectures. For instance, ConBP  suggests replacing ReLU with a softplus function in the backward pass to ensure smooth gradient backpropagation, making it not suitable to substitute models that are equipped with smooth activations only. SGM  is not applicable to substitute architectures without skip-connections (_e.g._, VGG-19). PNA  and SE  focus on vision transformers, and thus they are not suitable to most convolutional substitute models. For "substitute model training" methods, ResNet-50 is commonly chosen as the substitute model, as is adopted in the official GitHub implementations of these methods, and we leave the exploration of training with more advanced substitute architectures to future work. In addition to the \(_{}\) experiment, we also conduct evaluate with the \(_{2}\) constraint and the results are provided in Section A.

When I-FGSM  is simply applied as the optimization back-end, NAA  beats the other gradient computation innovations and achieves the lowest AAA with a value of \(72.65\%\) (see the upper half of Table 1), while RFA demonstrates the best results among all "substitute model training" methods. Table 1 shows that the most suitable gradient computation strategy can be different for issuing attacks from different substitute models. In particular, if ResNet-50 or VGG-19 is chosen as the substitute model, FIA  seems even superior to NAA in the sense of achiving higher attack success rate and lower victim accuracy, however, NAA is the best if any other model is chosen as the substitute model.

When UN-DP-DI\({}^{2}\)-TI-PI-FGSM is introduced as the new optimization back-end, almost all "gradient computation" methods show improved performance (see the lower half of Table 1). Yet, in combination with the new baseline, the advantage of most methods becomes less obvious. SGM , PNA , and SE  still produce performance gains when being combined with UN-DP-DI\({}^{2}\)-TI-PI-FGSM. In particular, with the new optimization back-end, SE on the DeiT-B substitute model leads to the optimal attack performance among all concerned options, in the sense of BAA, fooling the victim models to show an accuracy of only \(18.61\%\) (on average). PNA obtains the lowest WAA among all, which is \(31.50\%\). As for substitute model training, we see that the MoreBayesian method significantly outperforms the other methods, and it fools the victim models to show an average accuracy of \(27.09\%\) using a ResNet-50 substitute model.

Figure 3: Comparing IR, VT, and TAIG with their corresponding baselines. The dotted lines indicate AAA (lower indicates more powerful attack).

According to Table 1, vision transformers should be preferable when choosing the substitute model, as the best attack performance (_i.e._, BAA) is often obtained on vision transformers for many attacks. To compare the transfer performance from vision transformers to convolutional networks and from the opposite direction, we report the accuracy of victim models in predicting SGM adversarial examples generated on ResNet-50 and ViT-B as the substitute model, respectively. The results are shown in Table 2. It can be seen that transferring from vision transformers to convolutional networks seems easier. When utilizing ViT-B as the substitute model, the accuracy of convolutional networks shows a range in \([28.32\%,37.24\%]\), while, with ResNet-50, the accuracy of vision transformers lies in \([36.82\%,48.32\%]\). Overall, using ViT-B as the substitute model leads to lower average accuracy (\(28.80\%\) vs \(31.97\%\)) and the worst accuracy (\(37.24\%\) vs \(48.32\%\)) on victim models, which means better average and worst-case attack performance, respectively. ConvNeXt  that follows some designing principles of the vision transformers is also a favorable choice of the substitute model, according to our results. When performing LinBP on ConvNeXt-B, the generated adversarial examples are capable of fooling the victim models to show an average accuracy of only \(18.81\%\), which is super close to the best attack performance that could be achieved in Table 1. Additionally, it

   & ResNet & VGG & Inception & EIHNetV2 & ConvNeXt & ViT & DeiT & BEiT & Swin & Mixer \\  & -50 & -19 & v3 & -M & -B & -B & -B & -B & -B & -B \\   \\ 
1-FGSM & 87.79\% & 91.21\% & 93.71\% & 95.46\% & 88.32\% & 90.28\% & 90.28\% & 89.56\% & 94.81\% & 94.37\% & 91.58\% \\   \\  TAP (2018)  & 81.75\% & 89.80\% & 91.01\% & 93.84\% & 90.20\% & 91.90\% & 92.86\% & 92.11\% & 95.08\% & 93.93\% & 91.25\% \\ NRDM (2018)  & 82.19\% & 87.62\% & 85.29\% & 96.12\% & 94.36\% & 94.70\% & 95.02\% & 95.23\% & 95.01\% & 90.25\% & 91.58\% \\ FDA (2019)  & 85.11\% & 93.91\% & 89.91\% & 98.00\% & 96.27\% & 96.60\% & 95.52\% & 96.67\% & 97.66\% & 94.72\% \\ ILA (2019)  & 74.76\% & 72.18\% & 83.38\% & 90.20\% & 84.13\% & 77.91\% & 80.62\% & 78.29\% & 89.18\% & 85.30\% & 82.10\% \\ SGM (2020)  & 72.56\% & - & - & 79.64\% & 71.37\% & 85.72\% & 87.04\% & 83.67\% & **90.55\%** & 91.01\% & - \\ ILA++ (2020)  & 71.80\% & 73.60\% & 80.07\% & 88.01\% & 83.12\% & 74.50\% & 80.19\% & 77.02\% & 88.08\% & 82.08\% & 79.85\% \\ LinBP (2020)  & 75.84\% & 86.66\% & 92.87\% & 96.96\% & 89.05\% & 91.74\% & 91.26\% & 92.62\% & 95.65\% & 96.07\% & 90.87\% \\ ConBP (2021)  & 73.46\% & 85.49\% & 91.00\% & - & - & - & - & - & - & - & - \\ SE (2021)  & - & - & - & - & - & - & - & - & - & - & - \\ FIA (2021)  & 68.48\% & 71.86\% & 83.84\% & 89.66\% & 80.35\% & 76.06\% & 80.13\% & 82.42\% & 88.75\% & 79.13\% & 80.07\% \\ PNA (2022)  & - & - & - & - & - & 88.13\% & 87.14\% & 87.97\% & 93.62\% & - & - \\ NAA (2022)  & 70.34\% & 78.41\% & 76.37\% & 83.56\% & 63.93\% & 65.04\% & 69.02\% & 66.24\% & 79.26\% & 74.33\% & 72.65\% \\   \\  RFA (2021)  & **47.49\%** & - & - & - & - & - & - & - & - & - & - \\ LGV (2022)  & 74.84\% & - & - & - & - & - & - & - & - & - \\ DRA (2022)  & 48.55\% & - & - & - & - & - & - & - & - & - \\ MoreBayesian (2023)  & 63.40\% & - & - & - & - & - & - & - & - & - \\   \\ 
1-Baseline &  &  &  &  &  &  &  &  &  &  &  \\ 
**-Gradient Computation** &  &  &  &  &  &  &  &  &  &  &  \\ TAP (2018)  & 63.34\% & 54.64\% & 68.02\% & 68.00\% & 27.26\% & 41.48\% & 46.78\% & 34.45\% & 56.02\% & 54.49\% & 51.54\% \\ NRDM (2018)  & 51.78\% & 63.14\% & 70.76\% & 61.81\% & 40.04\% & 52.12\% & 60.89\% & 48.98\% & 77.87\% & 57.84\% & 58.52\% \\ FDA (2019)  & 42.62\% & 52.83\% & 60.25\% & 89.48\% & 69.01\% & 94.83\% & 83.99\% & 78.26\% & 83.19\% & 94.97\% & 74.94\% \\ ILA (2019)  & 37.80\% & **45.66\%** & 54.99\% & 48.86\% & 30.72\% & **28.93\%** & **33.28\%** & **29.40\%** & 5

[MISSING_PAGE_FAIL:9]

Conclusion

In this paper, we have presented benchmark for transfer-based attacks, called TA-Bench. On TA-Bench, we have implemented 30+ advanced transfer-based attack methods, including those focus on input augmentation and optimizer innovation, those "gradient computation" methods, those "substitute model training" methods, and those applying generative modeling. With TA-Bench, we are capable of evaluating and comparing transfer-based attacks systematically, practically, and fairly. Given comprehensive experimental results on TA-Bench, we have provided new insights about the effectiveness of these attacks, including but not limited to useful combinations of input augmentations and optimizers, reasonable choices of substitute/victim models, _etc._ Hoping to offer a sagacious judge of the state of transfer attacks and help future innovations in this field.