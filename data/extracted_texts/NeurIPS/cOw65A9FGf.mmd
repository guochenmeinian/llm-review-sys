# Text-Guided Attention is All You Need for

Zero-Shot Robustness in Vision-Language Models

 Lu Yu

1School of Computer Science and Engineering, Tianjin University of Technology

Haiyang Zhang

1School of Computer Science and Engineering, Tianjin University of Technology

Changsheng Xu

2State Key Laboratory of Multimodal Artificial Intelligence Systems,

Institute of Automation, University of Chinese Academy of Sciences

{luyu@email, zshy@stud}.tjut.edu.cn, csxu@nlpr.ia.ac.cn

###### Abstract

Due to the impressive zero-shot capabilities, pre-trained vision-language models (e.g. CLIP), have attracted widespread attention and adoption across various domains. Nonetheless, CLIP has been observed to be susceptible to adversarial examples. Through experimental analysis, we have observed a phenomenon wherein adversarial perturbations induce shifts in text-guided attention. Building upon this observation, we propose a simple yet effective strategy: _Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR)_. This framework incorporates two components: the Attention Refinement module and the Attention-based Model Constraint module. Our goal is to maintain the generalization of the CLIP model and enhance its adversarial robustness: The Attention Refinement module aligns the text-guided attention obtained from the target model via adversarial examples with the text-guided attention acquired from the original model via clean examples. This alignment enhances the model's robustness. Additionally, the Attention-based Model Constraint module acquires text-guided attention from both the target and original models using clean examples. Its objective is to maintain model performance on clean samples while enhancing overall robustness. The experiments validate that our method yields a 9.58% enhancement in zero-shot robust accuracy over the current state-of-the-art techniques across 16 datasets. _Our code is available at https://github.com/zhyblue424/TGA-ZSR_.

## 1 Introduction

Large-scale pre-trained vision-language models (VLMs) have showcased remarkable success in artificial intelligence by seamlessly integrating visual and textual data to understand complex multimodal information, such as CLIP . Leveraging vast datasets and powerful architectures such as BERT  and its variants , these models adeptly capture semantic relationships between images and texts, offering significant advantages across numerous applications. From image classification  and semantic segmentation  to image captioning  and vision question answering , pre-trained VLMs revolutionize how machines perceive and interact with multimodal information. Their importance lies in their ability to learn rich representations from varied data streams, enabling zero-shot learning and transfer learning across domains and tasks. Thus ensuring the reliability of large-scale models is crucial. However, these models are vulnerable to adversarial attacks as many other networks as demonstrated by recent studies , even slight perturbations to input data can result in misclassification or altered outputs. Such attacks pose a significant challenge, particularly in critical applications like autonomous vehicles , medical diagnosis , and maritime navigation , where the consequences of erroneous decisions can be severe. As these large-scale models become increasingly prevalent in real-world applications, understanding andmitigating the risks posed by adversarial attacks is essential to maintain trust and reliability in AI systems.

Adversarial training [53; 61; 69] has emerged as a crucial technique in enhancing the robustness of deep learning models against adversarial attacks. By augmenting training data with adversarial examples generated through perturbations of input data, models are forced to learn more robust decision boundaries, thereby improving their resilience to adversarial manipulation. Given the rising significance of large-scale VLMs in various applications, understanding their vulnerability to adversarial attacks is essential. While adversarial training presents practical challenges when applied to downstream tasks, especially with large-scale models. Firstly, adversarial training typically involves generating adversarial examples during each training iteration, which increases the computational overhead and may lead to overfitting on the training data. This phenomenon is exacerbated in large-scale models with vast parameter spaces, where fine-tuning becomes more susceptible to overfitting. Moreover, adversarial training may not adequately prepare models for all possible adversarial scenarios, potentially leaving them vulnerable to unknown data distributions encountered in real-world settings. Exploring zero-shot adversarial robustness in these models is particularly pertinent as it sheds light on their ability to generalize and perform reliably in unseen scenarios. Additionally, considering the multimodal nature of VLMs, the exploration of zero-shot adversarial robustness offers insights into the complex interactions between visual and textual modalities, paving the way for more robust and trustworthy multimodal AI systems.

Text-guided Contrastive Adversarial Training (TeCoA) method  represents the pioneering effort in investigating the zero-shot adversarial robustness of large-scale VLMs. They aim to bolster CLIP's zero-shot generalization capacity against adversarial inputs. While their primary focus lies on enhancing accuracy in the face of adversarial samples, this improvement comes at the expense of decreased performance on clean data. Subsequent work by PMG-AFT  builds upon this by introducing a pre-trained model guided adversarial fine-tuning technique, further enhancing both generalizability and adversarial robustness. However, despite the advancements made by both studies in enhancing CLIP's zero-shot robustness, significant questions regarding the interpretability of adversarial attacks and the efficacy of adversarial training remain unanswered. Specifically, _the mechanisms through which adversarial attacks influence network outputs and the reasons behind the effectiveness of adversarial training strategies remain elusive_. In our paper, we delve into the text-guided attention shift phenomenon to shed light on how adversarial attacks alter model outputs. Leveraging these insights, we propose a simple yet effective strategy, TGA-ZSR, aimed at enhancing the robustness of the CLIP model and preserving its performance on clean examples.

Our main contributions are summarized follows:

* To our knowledge, we are the first to introduce text-guided attention to enhance zero-shot robustness on vision-language models while maintaining performance on clean sample.
* We improve the interpretability of adversarial attacks for zero-shot robustness on vision-language models through a text-guided attention mechanism.
* The experimental results show that TGA-ZSR surpasses previous state-of-the-art methods, establishing a new benchmark in model zero-shot robust accuracy.

## 2 Related Work

**Pre-trained Vision-language Models.** In recent years, advancements in computer vision[12; 17; 34] have primarily relied on training models with image-label pairs to recognize predefined object categories. However, these approaches often overlook the inherent semantic connections between textual descriptions and visual content. Motivated by the remarkable progress witnessed in natural language processing (NLP), exemplified by breakthroughs like Transformer , BERT , and GPT-3 , researchers are increasingly drawn to the prospect of using textual data to enhance the capabilities of DNNs. These methodologies are referred to as VLMs [21; 48; 49; 64] and one prominent approach is to directly learn the semantic similarity between images and corresponding textual descriptions through image-text pairs. By aligning the embeddings of these two modalities, models like CLIP , ALIGN , BLIP , Visual-BERT , and ALBEF  aim to achieve superior performance across various tasks. CLIP  leverages a vast dataset of 400 million image-text pairs sourced from the internet and employs contrastive loss to effectively align the embeddings of both modalities, thereby enhancing the model's capabilities. Experimental results underscore the significant performance gains achieved by incorporating textual information into the model, with zero-shot performance surpassing that of earlier deep neural network architectures. However, despite its impressive zero-shot accuracy, experiments [38; 59] reveal vulnerabilities to adversarial examples, resulting in a notable decline in robustness.

**Adversarial Robustness.** Deep neural networks have been found to be vulnerable to adversarial examples [54; 36; 40; 66], which can fool DNNs to produce false outputs, rendering trained models unreliable. To bolster robustness against such adversarial attacks, various advanced methods have been proposed, including data augmentation [28; 58; 27; 65], adversarial training [69; 53; 61; 68], progressive self-distillation , randomization strategy [11; 35], and adversarial purification [41; 24; 62]. While these strategies aim to improve DNNs' adversarial robustness, they often come with increased complexity or limited generalizability. Adversarial training [69; 53; 61; 68] stands out as one of the most widely used and effective approaches, fine-tuning DNNs by generating adversarial examples during training. After the emergence of CLIP , many subsequent works [45; 16; 63] have utilized CLIP as a backbone, yet little attention has been given to studying its adversarial robustness. CLIP is shown to be susceptible to adversarial examples  as well, posing a significant threat to downstream tasks utilizing CLIP as a backbone. Hence, investigating the adversarial robustness of CLIP is crucial.

**Zero-shot Adversarial Robustness for VLMs.** The visual-language model, trained on both image and text data, serves as a foundational model for various tasks. However, it has shown vulnerability to adversarial examples [38; 59], and training from scratch is time-intensive. TeCoA  was the first to explore zero-shot adversarial robustness for VLMs, aiming to enhance CLIP's adversarial robustness by minimizing the cross-entropy loss between image logits and targets. While TeCoA solely utilizes cross-entropy loss, yielding only marginal performance improvements, PMG-AFT  extends this approach by minimizing the distance between features of adversarial examples and those of the pre-trained model. FARE  primarily focuses on maintaining high clean accuracy while improving model robustness, achieving this by constraining the distance between the original and target model embeddings. Our experiments reveal significant differences in attention maps between original examples and adversarial examples. Leveraging this insight, we enhance model robustness by constraining it with text-guided attention.

## 3 Methodology

### Preliminaries and Problem Setup

Following the previous works [38; 59], we choose CLIP model as the pre-trained VLMs for image classification task. Given an image-text pair \((x,t)\), where \(x\) represents an image and \(t\) represents a textual prompt, CLIP learns to encode both the image and the text into fixed-dimensional embeddings. Let \(f(x)\) denote the embedding of the image \(x\) and \(g(t)\) denote the embedding of the text prompt \(t\), \(y\) is the one-hot vector label. For training or fine-tuning on the downstream tasks, we use the cross-entropy loss, denoted as \(L(x,t,y)\).

\[L(x,t,y)=-_{i,j}y_{ij}log,g(t)_{j}))/ )}{_{k}exp(cos(f(x)_{i},g(t)_{k}))/)}\] (1)

where we set \(y_{ij}=1\) if the image-text pair is positive, otherwise, \(y_{ij}=0\). \(\) is the temperature parameter and \(cos\) indicates calculating the cosine similarity of the two embeddings.

**Adversarial Attacks.** Adversarial attacks are a concerning phenomenon where small, often imperceptible perturbations are intentionally applied to input data with the aim of deceiving a model into producing incorrect outputs. These perturbations are crafted with the goal of causing the model to misclassify or generate erroneous predictions while appearing indistinguishable to human observers. The Projected Gradient Descent (PGD)  method is an iterative approach for crafting adversarial examples. It starts with the original input data and then iteratively adjusts the data in the direction that maximizes the model's loss function while ensuring the perturbed data remains within a specified perturbation budget. Mathematically, the PGD attack can be expressed as follows:

\[x_{a+1}=_{x+S}(x_{a}+ sign(_{x_{a}}L(x_{a}, t,y)))\] (2)

Here, \(L\) represents the loss function, \(x\) denotes the original input data, \(\) controls the magnitude of perturbation, and \(_{x}L\) represents the gradient of the loss function with respect to the input data.

By adding or subtracting \(\) times the sign of this gradient to the original input data, the PGD attack generates adversarial examples that lead to misclassification or incorrect predictions by the model. \(_{x+S}\) makes the perturbed data remains within an \(\)-neighborhood of the original input, preventing the generated adversarial examples from straying too far. \(S\) is a set of allowed perturbations that formalizes the manipulative power of the adversary.

**Adversarial Examples Generation and Adversarial Training.** The optimization objective for crafting adversarial examples aims to maximize the loss of model \(f_{}\) with respect to a perturbed input \(x_{a}\) which can be formulated as:

\[x_{a}=}{argmax}\,L(f_{}(x_{a},t,y))\] (3)

Adversarial training is a technique to generate adversarial examples from the original training data and then use these examples to train the model, forcing it to learn to resist adversarial perturbations. To adapt the model to the downstream tasks, we apply adversarial fine-tuning on one target model towards robustness with the following loss:

\[=\,(f_{}(x_{a},t,y))\] (4)

Where \(\) represents the total loss function used for training the model.

**Zero-Shot Adversarial Robustness.** In this paper, we investigate the zero-shot adversarial robustness of CLIP model, which refers to the ability of these models to maintain performance and reliability even when encountering unseen adversarial samples during inference, with only adversarial fine-tuning the original CLIP model on one target dataset, such as Tiny-ImageNet.

### Text-Guided Attention based Interpretation of Adversarial Attacks

**Text-Guided Attention.** Attention mechanisms  play a crucial role in enhancing vision model performance across various tasks. At its core, attention enables models to focus on relevant parts of the input data while suppressing irrelevant information. Similarly, in VLMs, by incorporating textual guidance, the models can effectively focus on relevant visual features while processing

Figure 1: The four rows depict the original image, its associated attention map, the generated adversarial example, and the attention map of the adversarial example. Labels in black indicate the ground truth, while those in red represent mis-classified labels for the adversarial examples.

language, thus facilitating more accurate and coherent multimodal understanding. Additionally, text-guided attention enhances interpretability by providing insights into the model's decision-making process, fostering trust and understanding in complex multimodal systems. Thus, we investigate the impact of text-guided attention on enhancing and interpreting zero-shot adversarial robustness in VLMs in this paper. We define the text-guided attention as following:

\[A(x)=f_{g}(x) g(t)^{}, A^{P 1}\] (5)

Where \(f_{g}(x)\) represents the global image feature before the pooling operation of \(f(x)\), and \(P\) denotes the dimension of the attention embeddings. We reshape \(A\) to \(^{}\) to obtain the attention map, which is then resized to \(A^{H W}\). Finally, we apply a normalization operation (\(norm\)) on \(A\) to obtain the final text-guided attention map.

**Interpretation of Adversarial Attacks.** The previous research has predominantly focused on bolstering the zero-shot robustness of Vision-Language Models (VLMs), yet the reasons leading to mis-classifications induced by adversarial attacks remain unclear. This paper aims to shed light on interpreting the impact of adversarial attacks on VLMs. By employing Eq. 5, we compute the text-guided attention for both the original image (_Ori. image_) and its corresponding adversarial counterpart (_Adv. image_), as depicted in Fig. 1. Remarkably, despite the subtle discrepancies imperceptible to the human eye between the adversarial example and the original image, the former is mis-classified (labels in red). However, a significant difference emerges in the respective text-guided attention maps. Specifically, we observe a notable shift in the text-guided attention of the adversarial example, characterized by instances of displacement towards other objects, backgrounds, or even disappearance. For instance, while the original images in the first, second, and fourth columns pay attention to their subjects' heads, in their adversarial counterparts, attention diverges elsewhere. In the third column, the attention shift leads from the correct object to an incorrect one, resulting in mis-classification. In the fifth and seventh columns, the attention in their adversarial counterparts is redirected towards the background.

Figure 2: An overview of our TGA-ZSR framework: We generate adversarial examples and feed them into the target image encoder. To enhance the adversarial robustness of the CLIP model and maintain its generalization, we introduce text-guided attention. This involves refining the framework for adversarial examples through the Attention Refinement module and constraining the model to prevent significant drift via the Attention-based Model Constraint module.

### Text-Guided Attention for Zero-Shot Robustness (TGA-ZSR)

The semantic information embedded within text representations are preserved through a frozen text encoder, offering invaluable guidance when adversarial perturbations disrupt relevant visual features, which has not been explored for zero-shot robustness of vision-language models. We introduce the Attention Refinement Module, designed to effectively filter out irrelevant information, thereby mitigating the impact of adversarial attacks seeking to exploit vulnerabilities in the model's decision-making process. Moreover, to maintain model's ability to generalize effectively on clean images, we introduce the Attention-based Model Constraint Module. This module ensures consistent performance on clean data while enhancing the model against adversarial disruptions. Additionally, employing text-guided attention enhances interpretability, offering crucial insights into how the model integrates and processes information across modalities. This interpretability not only instills trust in the model's predictions but also facilitates the detection and mitigation of adversarial attacks. Our approach (i.e. TGA-ZSR) presents a comprehensive framework (as shown in Fig. 2) for enhancing model robustness to adversarial perturbations while concurrently improving interpretability. We will introduce the details as follows.

**Attention Refinement Module.** Based on the insights gained in Section 3.2, we propose an attention refinement module aimed at enhancing the robustness of the model. This module is designed to rectify the text-guided attention of adversarial samples, which often leads to altered predictions. Our approach aligns the adversarial attention map with that of the clean samples, known for their high-accuracy attention distribution. This simple yet effective strategy serves to mitigate the impact of adversarial perturbations on the model's predictions.

We take the generated adversarial sample \(x_{a}\) to the target model \(f_{g}^{tar}()\) and the clean sample \(x\) to the original model \(f_{g}^{ori}()\) and obtain the adversarial attention map \(A(x_{a}^{i})_{tar}\) and the clean attention map \(A(x^{i})_{ori}\) respectively. The attention refinement loss \(L_{AR}\) is thus defined as:

\[L_{AR}=_{i=0}^{N}\|A(x_{a}^{i})_{tar}-A(x^{i})_{ori}\|_{2}\] (6)

where \(A(x_{a})_{tar}=f_{g}^{tar}(x_{a}) g(t)^{}\) and \(A(x)_{ori}=f_{g}^{ori}(x) g(t)^{}\)1, \(\|\|_{2}\) denotes the \(L_{2}\) distance computation between two attention maps.

**Attention-based Model Constraint Module.** The Attention Refinement module serves to enhance the robustness of the models, consequently improving the accuracy of adversarial samples. However, this enhancement comes with a trade-off: it may marginally sacrifice the accuracy on clean samples due to shifts in model parameters. To preserve the generalization capability of pre-trained VLMs, we introduce an Attention-based Model Constraint module. This module aims to mitigate performance drops on clean images, thereby ensuring the overall effectiveness and reliability of the model.

Specifically, we input the clean sample \(x\) into the target model \(f_{g}^{tar}()\), adversarially fine-tuned on the Tiny-ImageNet dataset, to acquire the text-guided attention map \(A(x)_{tar}\). Concurrently, the original text-guided attention map outputted from the original CLIP model \(f_{g}^{ori}()\) is denoted as \(A(x)_{ori}\). To ensure the preservation of importance parameters for clean images, we enforce an \(L_{2}\) distance constraint between these two attention maps. The attention-based model constraint loss \(L_{AMC}\) is formulated as:

\[L_{AMC}=_{i=0}^{N}\|A(x^{i})_{tar}-A(x^{i})_{ori} \|_{2}\] (7)

Thus the final loss function can be represented as:

\[L_{total}=L_{CE}+ L_{AR}+ L_{AMC}\] (8)

## 4 Experiments

### Experimental Setup

**Datasets.** Our experiments begin with training the pre-trained CLIP model on the Tiny-ImageNet . Then we evaluate the model's zero-shot adversarial robustness across 15 subsequent datasets, followed by previous studies, such as TeCoA  and PMG-AFT . These datasets include several commonly used classfication datasets, including CIFAR-10 , CIFAR-100 , STL-10 , ImageNet , Caltech-101 , and Caltech-256 . Additionally, fine-grained image classification datasets such as StanfordCars , Flowers102 , Food101 , FGVC Aircraft , and Oxford-Pets  are included. Furthermore, the scene recognition dataset SUN397 , the medical image dataset PCAM , and the satellite image classification dataset EuroSAT  and the texture recognition dataset DTD  are incorporated for comprehensive evaluation. We also conduct experiments on four additional datasets (i.e. ImageNet_subset, ImageNet-A, ImageNet-O and ImageNet-R) as shown in Supp. Mat. A.1.

**Implementation Details.** Following the protocol of previous works , we fine-tuned the CLIP model on the adversarial samples of Tiny-ImageNet  as _'adversarial fine-tuning'_ and subsequently evaluated its performance across 15 datasets and Tiny-ImageNet itself. We employ ViT-B/32 as the backbone in CLIP and utilize the SGD optimizer to minimize loss. During adversarial fine-tuning, we update all parameters of the image encoder with a learning rate of 1e-4, weight decay of 0, momentum of 0.9, and a batch size of 128. We utilize \(l_{}\) norm PGD-2  with 2 iterations to generate adversarial examples, with an attack strength \(\) of 1/255 and the attack step size is 1/255. To evaluate zero-shot adversarial inference, we employ \(l_{}\) norm PGD-100  with 100 iterations, attack step of 1/255 and a batch size of 256 to generate adversarial examples for verifying CLIP's adversarial robustness. Additionally, to assess the model's robustness under different attack strengths, we perform inference using adversarial strengths \(\) of 1/255, 2/255, and 4/255. The hyper-parameters \(\) and \(\) are set to 0.08 and 0.05 respectively in Eq. 8 in the main experiments. Maintain the same parameters for the CW attack. For the AutoAttack  experiments, \(\) and \(\) are set to 0.08 and 0.009. We conducted the experiment utilizing the RTX 3090, which required a training period ranging from 3 to 4 hours.

### Main Results

To validate the effectiveness of our approach, we conduct comparisons with several state-of-the-art methods such as TeCoA , PMG-AFT , and FARE . Additionally, we extend the comparison to include CLIP (the original pre-trained CLIP model), FT-Adv. (adversarial fine-tuning using the contrastive loss of the original CLIP) and FT-Clean (fine-tuning on clean examples with the contrastive loss of the original CLIP) for a comprehensive evaluation.

  
**Methods** & ** **\#** \\  & ** **\#** \\ ** & **

    &  &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & & & & & & \\  CLIP  & 0.21 & 0.36 & 0.10 & 0.59 & 1.16 & 0.82 & 1.23 & 1.09 & 2.18 & 0.01 & 0.00 & 1.14 & 13.50 & 7.36 & 2.36 & 0.07 & 3.64 \\ PMG-AFT  & 44.59 & 44.86 & 24.15 & 74.11 & 19.99 & 37.33 & 39.83 & 20.95 & 13.51 & 12.09 & 1.47 & 19.51 & 69.09 & 44.46 & 10.57 & 48.99 & 31.17 \\ TGA-ZSR(ours) & **63.85** & **60.90** & **34.62** & **84.11** & **22.03** & **33.28** & **58.33** & **32.95** & **21.22** & **13.99** & **4.56** & **20.42** & **70.34** & **59.73** & **20.20** & 48.02 & **40.50** \\   

Table 4: Zero-shot robust accuracy across 16 datasets with CW attack . The optimal accuracy is highlighted in **bold**.

    &  &  &  &  &  &  &  \\  & & & & & & & & & & & & & & & & & & & & & \\  CLIP  & 0.02 & 0.01 & 0.08 & 0.03 & 0.04 & 0.01 & 0.00 & 0.03 & 0.16 & 0.12 & 0.06 & 0.04 & 0.03 & 0.10 & 0.11 & 0.22 & 0.09 \\ FT-clean & 0.08 & 0.03 & 0.01 & 0.91 & 0.09 & 0.04 & 0.06 & 0.03 & 0.48 & 0.02 & 0.03 & 0.12 & 1.38 & 0.66 & 0.03 & 0.02 & 0.25 \\ FT-Adv. & **59.08** & 37.55 & 20.39 & 69.14 & 16.25 & 11.23 & 33.91 & 18.54 & **19.95** & 11.59 & 12.65 & 16.21 & 69.90 & 39.24 & 7.57 & **48.84** & 28.28 \\ TeCoA  & 35.03 & 25.18 & 16.09 & 69.08 & 17.41 & 13.05 & 34.41 & 20.80 & 15.37 & 11.04 & 12.32 & 16.32 & 54.54 & 40.15 & 7.15 & 47.12 & 26.55 \\ FABE  & 28.59 & 23.37 & 13.58 & 67.00 & 97.23 & 18.38 & 27.15 & **15.48** & 9.15 & 0.25 & 0.87 & 12.67 & 47.43 & 36.68 & 6.77 & 10.23 & 19.78 \\ PMG-AFT  & 44.26 & **44.12** & **23.56** & **73.90** & 19.63 & **17.25** & 92.05 & 20.87 & 13.72 & **11.59** & 1.68 & 19.17 & **69.57** & 44.42 & 9.59 & 48.53 & 30.78 \\ TGA-ZSR(ours) & 39.45 & 40.53 & **22.38** & 72.06 & **20.36** & **25.55** & **40.31** & **21.45** & 17.233 & 11.19 & **1.94** & **19.28** & 52.16 & **45.68** & **19.07** & 48.63 & **30.86** \\   

Table 3: Zero-shot robust accuracy on images attacked with \(\) of 1/255 of AutoAttack . We performed several different methods on Tiny-ImageNet and evaluated on 16 datasets.

[MISSING_PAGE_FAIL:9]

mately 15% compared to state-of-the-art method PMG-AFT. This is due to the additional computation required for the text-guided attention map. The training time for our method is comparable to that of PMG-AFT. The test time remains consistent across all methods.

## 5 Conclusion and Limitations

In this paper, we discovered that adversarial attacks lead shift of text-guided attention. Building on this observation, we introduce a text-guided approach, TGA-ZSR, which incorporates two key components to perform adversarial fine-tuning and constrain the model. This strategy prevents model drift while enhancing model robustness. Extensive experiments validate the performance of TGA-ZSR, which not only improves CLIP's zero-shot adversarial robustness but also maintains zero-shot clean accuracy on clean examples, gaining a favorable balance.

**Limitations.** We use a simple text-guided attention mechanism by multiplying the text embedding and vision embedding which is effective against most attack types. However, for more challenging attacks such as AutoAttack, the improvement remains limited. This indicates that while our approach shows promise, it may require further refinement to enhance robustness under stronger adversarial scenarios.

**Border Impact.** Large-scale pre-trained vision-language models (VLMs) like CLIP  integrate visual and textual data, revolutionizing applications such as image classification, semantic segmentation, and vision question answering. While these models excel in zero-shot learning and transfer learning, they are vulnerable to adversarial attacks, posing risks in critical applications like autonomous vehicles and medical diagnosis. Adversarial training improves robustness but has practical challenges, including increased computational overhead and potential overfitting. Exploring zero-shot adversarial robustness is essential to ensure reliability.

**Acknowledgement.** This work was supported by National Science and Technology Major Project under Grant 2021ZD0112200, in part by the National Natural Science Foundation of China under Grants 62202331, U23A20387, 62036012, 62276118.

    & Robust & Clean & Average \\  CLIP & 4.90 & 64.42 & 34.66 \\  \(L_{CE}\) & 29.45 & 44.97 & 37.21 \\ \(+L_{AR}\) & 31.71 & 49.96 & 40.84 \\ \(+L_{AMC}\) & 41.96 & 56.48 & 49.22 \\   

Table 7: Ablation study on each component. After adversarial fine-tuning the model using adversarial examples generated by PGD-2, we verify the robustness of the model using adversarial examples generated by PGD-100.

  
**Methods** & Train memory usage & Train time (per epoch / batch) & Test time (per batch) \\  CLIP  & 0Mb & 0s / 0s & 21s \\ TeCoA  & 12873Mb & 512s / 0.65s & 21s \\ PMG-AFT & 18449Mb & 828s / 1.06s & 21s \\ TGA-ZSR (ours) & 21227Mb & 885s / 1.13s & 21s \\   

Table 8: Comparison of memory usage, training time, and test time.