# RandNet-Parareal: a time-parallel PDE solver using Random Neural Networks

Guglielmo Gattiglio

Department of Statistics

University of Warwick

Coventry, CV4 7AL, UK

Guglielmo.Gattiglio@warwick.ac.uk

Lyudmila Grigoryeva

Faculty of Mathematics and Statistics

University of St. Gallen

Rosenbergstrasse 20, CH-9000 St. Gallen, Switzerland

Lyudmila.Grigoryeva@unisg.ch

Honary Associate Professor, Department of Statistics, University of Warwick, Coventry, CV4 7AL, UK. (Lyudmila.Grigoryeva@warwick.ac.uk)

Honary Associate Professor, Department of Statistics, University of Warwick, Coventry, CV4 7AL, UK. (Lyudmila.Grigoryeva@warwick.ac.uk)

###### Abstract

Parallel-in-time (PinT) techniques have been proposed to solve systems of time-dependent differential equations by parallelizing the temporal domain. Among them, Parareal computes the solution sequentially using an inaccurate (fast) solver, and then "corrects" it using an accurate (slow) integrator that runs in parallel across temporal subintervals. This work introduces RandNet-Parareal, a novel method to learn the discrepancy between the coarse and fine solutions using random neural networks (RandNets). RandNet-Parareal achieves speed gains up to x125 and x22 compared to the fine solver run serially and Parareal, respectively. Beyond theoretical guarantees of RandNets as universal approximators, these models are quick to train, allowing the PinT solution of partial differential equations on a spatial mesh of up to \(10^{5}\) points with minimal overhead, dramatically increasing the scalability of existing PinT approaches. RandNet-Parareal's numerical performance is illustrated on systems of real-world significance, such as the viscous Burgers' equation, the Diffusion-Reaction equation, the two- and three-dimensional Brusselator, and the shallow water equation.

## 1 Introduction

Parallel-in-time (PinT) methods have been used to overcome the saturation of well-established spatial parallelism approaches for solving (prohibitively expensive) initial value problems (IVPs) for ordinary and partial differential equations (ODEs and PDEs), described by systems of \(d\) ODEs (andsimilarly for PDEs)

\[}{dt}=h((t),t)\ \ \ \ t[t_{0},t_{N}],\ \ \ (t_{0})=^{0},\ \ N,\] (1)

where \(h:^{d}[t_{0},t_{N}]^{d}\) is a smooth multivariate function, \(:[t_{0},t_{N}]^{d}\) is the time dependent column vector solution, and \(^{0}^{d}\) is the initial value at \(t_{0}\). PinT schemes are particularly important when the sequential application of an accurate numerical integrator \(\) over \([t_{0},t_{N}]\) is infeasible in a reasonable wallclock time. There are three general approaches for PinT computation: parallel across-the-problem, parallel-across-the-step, and parallel-across-the-method. In , another classification is provided: multiple shooting, methods based on waveform relaxation and domain decomposition, multigrid approaches, and direct time-parallel methods. Parallel-across-the-step methods, in which solutions at multiple time-grid points are computed simultaneously, include Parareal (approximation of the derivative in the shooting method) , Parallel Full Approximation Scheme in Space and Time (PFASST) (multigrid method) , and Multigrid Reduction in Time (MGRIT)  methods (see  for details). Among them, Parareal  has garnered popularity, with extensive theoretical analyses, improved versions, and empirical applications . This is due to its non-intrusive nature which allows seamless integration with arbitrary temporal and spatial discretizations, and to its successful performance across diverse fields, such as plasma physics , finance , and weather modeling . Limited theoretical results are available for MGRIT and PFASST, with a few extensions and empirical applications. Interestingly, combined analyses have shown equivalences between Parareal and MGRIT, and connections between MGRIT and PFASST. In Parareal, a coarse and fast solver \(\) is run sequentially to obtain a first approximation of the solution, which is then corrected by running a fine (accurate) but slow integrator \(\) in parallel across \(N\) temporal subintervals. This procedure is then iterated until a convergence criterion is met after \(k N\) iterations, leading to a speed-up compared to running \(\) sequentially over the entire time interval. A recent advancement, GParareal , improves Parareal convergence rates (measured as \(k/N\)) by learning the discrepancy \(-\) using Gaussian Processes (GPs). This method outperforms Parareal for low-dimensional ODEs and a moderate number of computer cores \(N\). However, the cubic cost (in the number of data points, roughly \(kN\) at iteration \(k\)) of inverting the GP covariance matrix hinders its broader application. Subsequent research introduced nearest neighbors (nns) GParareal (nnGParareal) , enhancing GParareal's scalability properties in both \(N\) and \(d\) through data reduction. Significant computational gains were achieved by training the GP on a small subset of nns, resulting in an algorithm loglinear in the sample size. This allowed scaling its effectiveness up to systems with a few thousand ODEs, beyond which it loses its potential. Indeed, being based on the original GP framework, it uses a costly hyperparameter optimization procedure that requires fitting one GP per ODE dimension.

This study introduces RandNet-Parareal, a new approach using random neural networks (RandNets) to learn the discrepancy \(-\). RandNets are a family of single-hidden-layer feed-forward neural networks (NNs), where hidden layer weights are randomly sampled and fixed, and only the output (or readout) layer is subject to training. Compared to standard artificial NNs, RandNets are hence much simpler to train: the input data are fed through the network, the predictions observed, and the weights of the linear output (or readout) layer are obtained as minimizers of a penalized squared loss between the NN outputs and the training targets. Since this optimization problem admits a closed-form solution, no backpropagation is required, and the issues of vanishing and exploding gradients persisting for standard fully trainable NNs are therefore avoided. The literature on the topic is rich and somewhat fragmented, and different names are used for essentially the same model. RandNets are related to Random Feature Networks  and Reservoir Computing , Random Fourier Features (RFFs) and kernel methods . Some authors use the name Extreme Learning Machines (ELMs)  to refer to RandNets, while others use the term randomized or random NNs  for the same paradigm. RandNets show excellent empirical performance, and have been used in the context of mathematical finance , mathematical physics , electronic circuits , photonic  and quantum systems , random deep splitting schemes , scientific computing , and have shown excellent empirical performance in numerous further applications. Moreover, recent work  proves that RandNets are universal approximators within spaces of sufficiently regular functions, and provides explicit approximation error bounds, with these results generalized to a large class of Bochner spaces in . These contributions show that RandNets are a reliable machine learning paradigm with provable theoretical guarantees.

In this paper, we show that endowing Parareal with RandNets-based learning of \(-\), the new proposed RandNet-Parareal algorithm, leads to significantly improved scalability, convergence speed, and parallel performance with respect to nnGParareal, GParareal, and Parareal. This allows us to solve PDE systems on a fine mesh of up to \(10^{5}\) discretization points with negligible overhead, outperforming nnGParareal by two orders of magnitude and reducing its model cost by several orders.

Here, we compare the performance of Parareal, nnGParareal, and RandNet-Parareal on five increasingly complex systems, some of which are drawn from an extensive benchmark study of time-dependent PDEs . These include the one-dimensional viscous Burgers' equation, the two-dimensional Diffusion-Reaction equation, a challenging benchmark used to model biological pattern formation , the two- and three-dimensional Brusselator, known for its complex behavior, including oscillations, spatial patterns, and chaos, and the shallow water equations (SWEs). Derived from the compressible Navier-Stokes equations, the SWEs are a system of hyperbolic PDEs exhibiting several types of real-world significance behaviors known to challenge numerical integrators, such as sharp shock formation dynamics, sensitive dependence on initial conditions, diverse boundary conditions, and spatial heterogeneity. Example applications include of tsunamis or flooding simulations.

We intentionally chose two hyperbolic equations (Burgers' and SWE) to challenge RandNet-Parareal on systems for which Parareal is known to struggle, with slow or non-convergent behavior [2; 3; 9; 18; 72]. Previous works have developed ad-hoc coarse solvers to address Parareal's slow convergence for Burgers' [7; 40; 68; 71], and for SWE [1; 31; 54; 73]. Here, we adopt a different strategy: by leveraging the generalization capabilities of RandNets within the Parareal algorithm, we enhance the performance of standard, off-the-shelf integration methods such as Runge-Kutta, obtaining speed gains up to x125 and x22 compared to the accurate integrator \(\) and Parareal, respectively. All experiments have been executed on Dell PowerEdge C6420 compute nodes each with 2 x Intel Xeon Platinum 826 (Cascade Lake) 2.9 GHz 24-core processors, 48 cores and 192 GB DDR4-2933 RAM per node. To illustrate our proposed algorithm and facilitate code adoption, we provide a step-by-step Jupyter notebook outlining RandNet-Parareal. Moreover, all simulation outcomes, including tables and figures, are fully reproducible and accompanied by the necessary Python code at https://github.com/Parallel-in-Time-Differential-Equations/RandNet-Parareal.

It is well acknowledged that comparing PinT methods based on different working principles is extremely hard, with  representing a recent survey article with some comparisons. Quoting ,"caution should be taken when directly comparing speedup numbers across methods and implementations. In particular, some of the speedup and efficiency numbers are only theoretical in nature, and many of the parallel time methods do not address the storage or communication overhead of the parallel time integrator".  is one of very few recent attempts to systematically compare different PinT classes. However, it is limited exclusively to the Dahlquist problem. Thus, it has become conventional to compare new techniques to the existing state-of-the-art methods within the same group of solvers. This is why, in this work, we compare RandNet-Parareal with the original Parareal and its recently improved versions, GParareal , and nnGParareal .

The rest of the paper is organized as follows. In Section 2, we describe the Parareal algorithm. Section 3 briefly explains GParareal and nnGParareal, focusing on the latter. RandNet-Parareal is introduced in Section 4, while Sections 5 and 6 present our numerical results, and a final discussion. A computational complexity analysis of RandNet-Parareal, a robustness evaluation of the proposed algorithm, complementary simulation studies, and other additional results are available in the Supplementary Material.

**Notation.** We denote by \(^{n}\) a column vector with entries \(v_{i}\), \(i\{1,,n\}\), and by \(\|\|\) and \(\|\|_{}\) its Euclidean and infinity norms, respectively. We use \(A^{n m}\) to denote a real-valued \(n m\) matrix, \(n,m\), with elements \(A_{ij}\), \(j\)th column \(A_{(,j)}\), \(j\{1, m\}\), and \(i\)th row \(A_{(i,)}\), \(i\{1,,n\}\). We write \(A^{}\), \(A^{}\), and \(\|A\|_{}\) for the \(A\) matrix transpose, Moore-Penrose pseudoinverse, and Frobenius norm, respectively. \(_{n}\) denotes the identity matrix of dimension \(n\).

## 2 The Parareal algorithm

The idea of Parareal is to solve the \(d\)-dimensional ODE (and similarly PDE) system (1) in a parallel-in-time fashion, dividing the original IVP into \(N\) sub-IVPs

\[_{i}}{dt}=h(_{i}(t {U}_{i}),t), t[t_{i},t_{i+1}],_{i}(t_{i})=_{i},\ \ i=0,,N-1,\]where the number of time intervals \(N\) is also the number of available machines/cores/processors, \(_{i}(t_{i})\) is the solution at time \(t\) of the \(i^{ th}\) IVP with initial condition \((t_{i})=_{i}^{d}\), \(i=0,,N-1\). If the initial conditions were known and satisfied the continuity conditions \(_{i}=_{i-1}(t_{i}|_{i-1})\) (for the coherent temporal evolution of the system across sub-intervals), then the sub-IVPs could be trivially solved in parallel on a dedicated machine. Unfortunately, this is not the case, as only the first initial condition \(_{0}=^{0}^{d}\) at time \(t_{0}\) appears available. To account for this, Parareal introduces another numerical integrator \(\), much faster but less accurate than \(\), to approximate the missing initial conditions \(_{i}\), \(i=1,,N-1\), _sequentially_. \(\) trades off accuracy for computational feasibility, usually taking seconds/minutes instead of hours/days of \(\)2.

The algorithm works as follows. We use \(_{i}^{k}\) to denote the Parareal approximation of \(_{i}(t_{i})=_{i}\) at iteration \(k 0\). At \(k=0\), the initial conditions \(\{_{i}^{0}\}_{i=1}^{N-1}\) are initialized using a _sequential_ application of the coarse solver \(\), obtaining \(_{i}^{0}=(_{i-1}^{k-1})\), \(i=1,,N-1\), with \(_{0}^{0}=_{0}\). At \(k 1\), the obtained initial conditions \(_{i-1}^{k-1}\) are "propagated" through \(\) in _parallel_ on \(N\) cores to obtain \((_{i-1}^{k-1})\), \(i=1,,N\). Note that for every initial condition \(_{i-1}^{k-1}\), we compute both \((_{i-1}^{k-1})\), i.e. a precise evaluation of \(_{i-1}(t_{i}|U_{i-1}^{k-1})\), and \((_{i-1}^{k-1})\), an inaccurate evaluation of the same term. Hence, we can interpret \(\) and \(\) as functions mapping an initial condition to the next one, thereby evolving (1) by one interval. We can then use their difference, \((-)(_{i-1}^{k-1})\), to correct the inaccuracy of \(\) on future evaluations. This gives rise to the original Parareal predictor-corrector rule \(_{i}^{k}=(_{i-1}^{k})+(-)( _{i-1}^{k-1})\), with \(i=1,,N-1\), \(k 1\), where the _sequential_ prediction \((_{i-1}^{k})\) is corrected by adding the discrepancy \(-\) computed at the previous iteration \(k-1\). However, this formulation can be changed to use data from the current iteration \(k\), and generalized to account for different ways of computing the discrepancy, leading to 

\[_{i}^{k}=(_{i-1}^{k})+(_{i-1}^{k}),\] (2)

where \(:^{d}^{d}\) specifies how the correction function \(-\) is computed or approximated based on some observation \(^{d}\). Parareal uses

\[_{ Para}(_{i-1}^{k})=(-)(_{i- 1}^{k-1}),\] (3)

while other variants will be introduced in the subsequent sections. The Parareal solution (2) is considered converged for a given threshold \(>0\) and up to time \(t_{L} t_{N}\), if solutions across consecutive iterations have stabilized. That is, for some pre-defined accuracy level \(>0\), it holds that

\[\|_{i}^{k}-_{i}^{k-1}\|_{}<, 0<i L N-1.\] (4)

Other stopping criteria are also possible [66; 67]. Converged Parareal approximations \(_{i}^{k}\), \(i L\), are no longer iterated to avoid unnecessary overhead [12; 20; 21; 57; 58]. Instead, unconverged solution values \(_{i}^{k}\), \(i>L\), are updated during future iterations by first running \(\) in parallel and then using the prediction-correction rule (2). The Parareal algorithm stops at some iteration \(K_{ Para} N\) when all initial conditions have converged, that is when (4) is satisfied with \(L=N-1\) and thus \(K_{ Para}=k\). Note that during every Parareal iteration \(k>1\), the "leftmost" fine solver evaluation \((_{L}^{k})\) is either run from the outcome of a previous fine computation \(_{L}^{k}=(_{L-1}^{k-1})\), or from a converged initial condition \(\|_{L}^{k}-_{L}^{k-1}\|_{}<\). This guarantees that, either way, the maximum number of iterations to convergence for _any_ Parareal-based algorithm is \(K_{ Para}=N\), in which case it sequentially attains the fine solver solution, with the added computational cost of running \(\) and evaluating \(\)\(N\) times. A Parareal pseudocode is presented in Algorithm 1 in Supplementary Material A.

## 3 GParareal and Nearest Neighbors GParareal

The performance of Parareal can be improved by a careful selection of \(\) in (2), combined with a better use of the available information present at iteration \(k\). Let \(_{k}\) denote the dataset consisting of \(Nk\) pairs of inputs \(_{i-1}^{j}^{d}\) and their corresponding outputs \((-)(_{i-1}^{j})^{d}\), \(i=1,,N\), \(j=0,,k-1\), that is

\[_{k}:=\{(_{i-1}^{j},(-)(_{i-1}^{j})), \;\;i=1,,N,\;\;j=0,,k-1\}.\] (5)While Parareal relies on one observation to construct the correction \(\) in (3), GParareal and following works, including this one, use all the discrepancy terms \(-\) and information in \(_{k}\) to make their predictions. The idea of GParareal is to learn the map \(^{d}^{d}\), \(_{i-1}^{k}(-)(_{i-1}^{k})\), via \(d\) independent scalar GPs \(^{d}\), \(_{i-1}^{k}_{}^{(s)}(_{i-1}^{k})\), \(s=1,,d\), one per ODE dimension, whose predictions are concatenated into \(_{}(_{i-1}^{k})=(_{}^{ (1)}(_{i-1}^{k}),,_{}^{(d)}(_{i-1}^ {k}))^{}^{d}\), and finally plugged into the predictor-corrector rule (2). In particular, each GP prediction \(_{}^{(s)}(_{i-1}^{k})\) is obtained as the GP posterior mean \(_{_{k}}^{(s)}(_{i-1}^{k})\), computed by conditioning the corresponding GP prior on the dataset \(_{k}\), i.e. \(_{}^{(s)}(_{i-1}^{k})=_{_{k}}^{( s)}(_{i-1}^{k})\). We refer to Supplementary Material B and  for a thorough description of the algorithm, including all relevant quantities of interest, namely the \(d\) GP priors, the likelihood, the hyperparameters and their optimization procedure, and an explicit expression of the posterior means. Here, it is worth highlighting that the GPs are trained once per iteration to leverage the new incoming data, and then their predictions are used to _sequentially_ update the initial conditions in (2). Using all information stored in \(_{k}\) instead of a single observation (as for Parareal) is the primary driver of faster convergence rates experienced by GParareal. Other benefits of this algorithm are increased stability to different initial conditions, the ability to incorporate legacy data (that is, the possibility of using datasets coming from previous runs of the algorithm with different starting conditions or settings, leading to faster convergence), lower sensitivity to poor choices of the coarse solver \(\), and the possibility of parallelizing the training of the \(d\) GPs over the \(N\) available cores. The main drawback of GParareal is the heavy computational burden incurred when inverting the GP covariance matrices, which is of order \(O(d(Nk)^{3})\) at iteration \(k\). This negatively impacts the algorithm's wallclock time, which may be higher than Parareal despite a lower number of iterations needed to converge. This is why GParareal has been proposed mainly for low-dimensional ODE systems with a relatively small number of processors/intervals \(N\) (up to hundreds), limiting its use and parallel scalability .

The nnGParareal algorithm  has been proposed to tackle GParareal's scalability issue, sensibly reducing the computational time and memory footprint of GPs by using their nns version (nnGPs). In this framework, at iteration \(k\), the \(d\) GPs are all trained on a smaller dataset of size \(m\), \(_{i-1,k}\), composed out of the \(m\) nns (in Euclidean distance) of \(_{i-1}^{k}\) in \(_{k}\), leading to the nnGParareal correction \(_{}(_{i-1}^{k})=(_{ }^{(1)}(_{i-1}^{k}),,_{}^{(d)}(_{ i-1}^{k}))^{}\), with

\[_{}^{(s)}(_{i-1}^{k})=_{_{i-1,k }}^{(s)}(_{i-1}^{k}), s=1,,d.\]

Here, \(_{_{i-1,k}}^{(s)}\), \(s=1,,d\), denotes the nnGP posterior mean computed by conditioning the corresponding GP prior on the reduced dataset \(D_{i-1,k}\) of size \(m\). Due to the decreased sample size, each nnGP covariance matrix can be inverted at a cost of \(O(m^{3})\) independent of \(k\) or \(N\). However, contrary to GParareal which trains the GPs once per iteration, the nnGPs are re-trained _every time a new prediction_\(_{}(_{i-1}^{k})\) is made, which are at most \(N-k\) at iteration \(k\) (as at least \(k\) intervals have converged at iteration \(k\)), yielding a combined \(O(d(N-k)m^{3})\) complexity. Several experiments on different ODE and PDE systems have shown that \(m\{15,,20\}\) offer accuracy comparable to the full GP  at a much lower cost. Although faster than GParareal, nnGParareal still exhibits some of the drawbacks inherited from the GP framework, such as the cost of optimizing the hyperparameters through a numerical maximization of a non-convex likelihood, and the use of \(d\) scalar nnGPs. The latter is particularly critical. On the one hand, despite the possibility of training the \(d\) nnGPs in parallel, the inversion of a \(m m\) matrix is so efficient that parallel overheads may outweigh the theoretical benefits. On the other hand, when solving PDEs, nnGParareal will incur additional costs due to insufficient hardware resources, as usually \(d N\), forcing the \(d\) nnGPs to queue among the \(N\) available processors, which is why the algorithm has been proposed for high-dimensional ODE and PDE systems with \(d N\). We refer to Supplementary Material B and  for more details on nnGParareal, and to Algorithm 2 in Supplementary Material A for the pseudocode of the nnGP training. In the next section, we address the nnGParareal issues by introducing RandNets.

## 4 Random neural networks Parareal (RandNets-Parareal)

In RandNet-Parareal, we propose to learn the map \(^{d}^{d}\), \((-)()\) via RandNets, obtaining the RandNet-Parareal correction \(_{}\), which we then use within the predictor-corrector rule (2). Prior to that, we define how RandNets work in a general setting with input and output or target \(^{d}\). Later in the text we will go back to the input of interest \(_{i}^{k}\). Let \(M\) denote the number of hidden neurons, and \(H_{W}^{A,}()\) be a single-hidden-layer feed-forward neural network used to learn \(-\), given by

\[H_{W}^{A,}()=W^{}(A+) ^{d},^{d},\] (6)

where \(A^{M d}\) is the matrix of random, non-trainable weights of the hidden layer, \(^{M}\) is a random non-trainable bias vector, and \(W^{M d}\) is the matrix of trainable output weights. Here, \(:^{M}^{M}\) denotes an activation function obtained as the componentwise application of a non-linear map \(:\) which we choose to be ReLU \((x)=(x,0)\) with \(x\), to satisfy the assumption of Proposition 1 below. The entries of \(A\) and \(\) are randomly sampled from given distributions \(_{A}\) and \(_{}\), respectively, and kept fixed. After observing the dataset \(_{k}\), the output weights \(W\) are obtained as the minimum \(_{2}\) norm least squares (or simply min-norm least squares) estimator or as the solution of the following penalized empirical minimization problem:

\[^{_{k}}=_{ 0}_{W^{ M d}}\{_{(,)_{k}}\|H_{W}^{A,}()-\|^{2}+\|W\|_{}^{2} \},\]

which is also called a "ridgeless" (interpolation) estimator , and can be more compactly written as

\[^{_{k}}=_{ 0}X^{}X+ _{M}^{-1}\,X^{}Y.\] (7)

Here, \(X^{Nk M}\) is a matrix with \((X_{(l,)})^{}:=(A(U_{(l,)})^{}+)\), \(l=1,,Nk\), and \(U,Y^{Nk d}\) are the collection of inputs and outputs of \(_{k}\) in matrix form,respectively, defined as \((U_{(l,)})^{}=_{i}^{j}\), \((Y_{(l,)})^{}=_{i}^{j}\), \(l=jN+i+1\), \(i=0,,N-1\), \(j=0,,k-1\). Whenever \(Nk M\) and the rank of \(X^{}X^{M M}\) is \(M\), (7) reduces to the standard least squares estimator \(^{_{k}}=X^{}X^{-1}\,X^{}Y\), while if the rank of \(X^{}X\) is \(Nk\), the solution admits a closed form

\[^{_{k}}=X^{}X^{}\,X^{}Y.\]

We get inspired by , where only \(m\) nns are used in the training. In this setting, \(M Nk=m\), and in this overparametrized linear regression case, the ridgeless estimator interpolates the training data, which is a desirable feature since the problem is genuinely deterministic .

Several ingredients control the performance of RandNets, such as the dimension of the network \(M\) and the choice of distributions \(_{A}\) and \(_{}\). In this work, we take the rows of the weight matrix \(A\) and the bias entries of \(\) to be independent and uniformly distributed. For this case, the approximation bounds are available [25, Proposition 3], which we report below using our notation.

**Proposition 1** (Approximation bound, , Proposition 3).: _Let \(H^{*}:^{d}\), \( H^{*}()\) be an unknown function we wish to approximate with \(H_{W}^{A,}\) defined in (6). Suppose \(H^{*}\) can be represented as \(H^{*}()=_{^{d}}e^{i,}g() \) for some complex-valued function \(g\) on \(^{d}\) and all \(^{d}\) with \(\|\| Q\), where \(,\) is the inner product on \(^{d}\). Assume that \(_{^{d}}1,\|\|^{2d+6}\,|g()|^{2}\, <\). For \(>0\), suppose the rows of \(A\) are i.i.d. random variables with uniform distribution on \(B_{}^{d}\), the Euclidean ball of radius \(\) around \(\), and that the \(M\) components of \(\) are i.i.d. uniform random variables on \([-(Q,1),(Q,1)]\). Assume that \(A\) and \(\) are independent and let \(:\) be given by \((x)=(x,0)\). Then, there exist a \(^{M d}\)-valued random variable \(W\) and an explicit (see (33) in ) constant \(C^{*}>0\) such that_

\[[\|H_{W}^{A,}()-H^{*}()\|^{2}] }{M},\]

_and for any \((0,1)\), the random neural network \(H_{W}^{A,}\) satisfies_

\[_{^{d}}\|H_{W}^{A,}()-H^ {*}()\|^{2}_{}()^{1/2}}}{} 1-.\]

Our choice of \(_{A}\) and \(_{}\) satisfies the conditions of Proposition 1 if \(\|\| Q\). If this is not met, we rescale the ODE/PDE system via a change of variables. We found these bounds empirically useful in informing a good choice for the sampling distribution, which we follow. If no prior information were available, the common approach would have been to take \(_{A}(-a,a)^{M d}\), \(_{}(-b,b)^{M}\), and optimize \(a,b^{+}\) via expensive cross-validation procedure.

Unlike nnGParareal, GParareal, and the corresponding nnGPs and GPs, training RandNets is so fast that parallelization across the \(d\) dimensions is unnecessary. Hence, the predictions of the random network are computed jointly on all \(d\) coordinates, yielding the RandNet-Parareal correction function

\[_{}(_{i-1}^{k})=H_{^{ _{i-1,k}}}^{A,}(_{i-1}^{k}) ^{d}.\] (8)

Here, the estimated weights \(^{_{i-1,k}}\) are obtained using the reduced dataset \(_{i-1,k}\) consisting of the \(m_{}\) nns of \(_{i-1}^{k}\), requiring the retraining of the RandNet for every prediction. Employing a multi-output model instead of independently training \(d\) scalar-output models addresses one of the pitfalls of GPs, allowing for better scalability when \(d N\). The fact that training the RandNets reduces to a closed-form ridgeless interpolation solution presents a substantial difference and improvement with respect to (nn)GPs. Moreover, expensive hyperparameter optimization is avoided in RandNets, addressing the other major pitfall of GParareal and nnGParareal. The pseudocode for training RandNets is reported in Algorithm 3 in Supplementary Material A.

In Supplementary Material C, we derive the theoretical computational costs of nnGParareal and RandNet-Parareal, illustrating them as a function of dimension \(d\) and number of processors \(N\) in Figure 3. These theoretical findings confirm the significantly superior scalability of RandNet-Parareal which we observe in the numerical experiments reported in Section 5.

In Supplementary Material D, we study the robustness of RandNet-Parareal to changes in the number of nns \(m_{}\) (and thus the input data size), the number of neurons \(M\), and the randomly sampled network weights \(A,\). Intuitively, one might anticipate that a larger data sample would yield a more accurate approximation of the correction \(-\), and that a higher number of neurons \(M\) would reduce the prediction error of RandNets (as in Proposition 1). One may also suspect the algorithm to be sensitive to the particular sampling seed. Remarkably, our empirical findings demonstrate that these factors have a limited impact on the number of iterations needed by RandNet-Parareal to converge, which remains largely consistent (up to a few iterations) across different values and ODE/PDE systems, for sensible choices of \(m_{}\) and \(M\). For the end user, this eliminates the need of ad-hoc tuning, making the proposed RandNet-Parareal a convenient out-of-the-box algorithm.

## 5 Numerical Experiments

In this section, we first compare the performance of Parareal, nnGParareal, and RandNet-Parareal on the viscous Burgers' equation (one spatial dimension and one variable, also considered in nnGParareal ), to showcase Parareal and nnGParareal challenges as the number of space discretization and, correspondingly, the dimensions \(d\), increases. Then, we consider the Diffusion-Reaction equation, a larger system defined on a two-dimensional spatial domain with two non-linearly coupled variables, and the SWEs (two spatial dimensions and three variables), representing a suitable framework for modeling free-surface flow problems on a two-dimensional domain. Two additional challenging systems, the 2D and 3D Brusselator PDEs, known for their complex behavior, including oscillations, spatial patterns, and chaos, are considered in Supplementary Material E. The simulation setups used for obtaining the results in this section are provided in Supplementary Material G, with the corresponding accuracies and runtimes for RandNet-Parareal, Parareal, and nnGParareal reported in Supplementary Material F.

Let \(T_{}\) and \(T_{}\) be the time it takes to run \(\) and \(\) over one interval \([t_{i},t_{i+1}]\), respectively, and let \(N_{}\) and \(N_{}\) denote the number of steps for the fine and coarse solvers over one interval, respectively. We can measure the parallel efficiency of an algorithm via its parallel speed-up \(S_{}\), defined as the ratio of the serial over the parallel runtime, i.e. \(S_{}:=NT_{}/T_{}\). \(S_{}\) captures the wallclock gains of parallel procedures and, unlike other quantities (such as the number of algorithm iterations needed to converge), also includes the model training cost.

### Viscous Burgers' equation

Our initial example is a non-linear, one-dimensional PDE (illustrated in Figure 7 of Supplementary Material H) exhibiting hyperbolic behavior , described by the equation

\[v_{t}= v_{xx}-vv_{x},(x,t)[-L,L][t_{0},t_{N}],\] (9)with initial condition \(v(x,t_{0})=v_{0}(x)\), \(x[-L,L],L>0\), and Dirichlet boundary conditions \(v(-L,t)=v(L,t)\), \(v_{x}(-L,t)=v_{x}(L,t)\), \(t[t_{0},t_{N}]\). We use the same setting and parameter values as in . More specifically, we choose \(L=1\), diffusion coefficient \(=0.01\), and discretize the spatial domain using finite difference  and equally spaced points \(x_{j+1}=x_{j}+ x\), with \( x=2L/d\) and \(j=0,,d\). We hence reformulate the PDE as a \(d\)-dimensional ODE system.

In our first numerical experiment, we choose \(N=d=128\), \(v_{0}(x)=0.5(( x)+1)\), \(t_{0}=0\), and \(t_{N}=5.9\) as in , and consider \(=,=\), \(N_{}=4\) and \(N_{}=4e^{4}\), where \(\) stands for Runge-Kutta of order 1, and similarly for \(\) and \(\). The results, reported at the top of Table 1, show how RandNet-Parareal converges in fewer iterations and has a higher speed-up than Parareal and nnGParareal. The difference in the model training costs is striking, with the nnGP's being approximately 700 times higher than that of RandNets, reducing thus its potential speed-up.

As real-world (one-dimensional) problems would require a higher spatial discretization, we increase \(d\) by one thousand to \(d=1128\), keeping \(N\) fixed. Unlike assuming matching hardware resources to the system size (as implicitly done in , where \(d=N\)), we deliberately do not increase \(N\) to assess the algorithms' performances under constrained conditions. Instead, both time discretization numbers are increased to \(N_{}=6e^{5}\) and \(N_{}=293\) (resulting thus in longer \(T_{}\) and \(T_{}\) times) to account for the finer spatial mesh . As observed from the bottom of Table 1, as \(d/N>1\), nnGParareal's issues become more pronounced, as the \(d\) scalar GPs cannot be run all in parallel across the \(N\) processors, but need \(d/N=10\) runs instead, slowing down the algorithm. In contrast, RandNet-Parareal has a training cost comparable with the previous example, leading to an even higher speed-up, running in approximately 38 minutes compared to the almost 13 hours of Parareal.

### Diffusion-Reaction system

We now turn to a more challenging case study. The Diffusion-Reaction equation  (illustrated in Figure 8 in Supplementary Material H) is a system of two non-linearly coupled variables, the activator \(u=u(t,x,y)\) and the inhibitor \(v=v(t,x,y)\), defined on a two-dimensional spatial domain as

\[_{t}u=D_{u}_{xx}u+D_{u}_{yy}u+R_{u},_{t}v =D_{v}_{xx}v+D_{v}_{yy}v+R_{v}.\]

Here, \(D_{u}\), \(D_{v}\) are the diffusion coefficients for the activator and inhibitor, respectively, and \(R_{u}=R_{u}(u,v)\), \(R_{v}=R_{v}(u,v)\) are their reaction functions defined by the Fitzhugh-Nagumo equation 

\[R_{u}(u,v)=u-u^{3}-c-v, R_{v}(u,v)=u-v,\]

where \(c=5e^{-3}\), \(D_{u}=1e^{-3}\), and \(D_{v}=5e^{-3}\). We take \((x,y)(-1,1)^{2}\) and \(t\). The initial condition \(u(0,x,y)\) is generated as standard Gaussian noise. We apply a no-flow Neumann boundary

  \\   Algorithm & \(K\) & \(NT_{}\) & \(T_{}\) & \(T_{}\) & \(T_{}\) & \(S_{}\) \\  Fine & – & – & – & – & 13h 5m & 1 \\ Parareal & 90 & 0s & 6m & 0s & 8h 54m & 1.47 \\ nnGParareal & 14 & 0s & 6m & 12m & 1h 39m & 7.90 \\ RandNet-Parareal & 10 & 0s & 6m & 1s & 1h 2m & **12.61** \\    \\   Algorithm & \(K\) & \(NT_{}\) & \(T_{}\) & \(T_{}\) & \(T_{}\) & \(S_{}\) \\  Fine & – & – & – & – & 18h 52m & 1 \\ Parareal & 91 & 0s & 9m & 0s & 12h 57m & 1.41 \\ nnGParareal & 6 & 2s & 9m & 1h 25m & 2h 17m & 8.26 \\ RandNet-Parareal & 4 & 2s & 9m & 1s & 38m & **29.98** \\   

Table 1: Empirical scalability and speed-up analysis for viscous Burgers’ equation condition \(D_{u}_{x}u=0\), \(D_{v}_{x}v=0\), \(D_{u}_{y}u=0\), \(D_{v}_{y}v=0\) for \((x,y)(-1,1)^{2}\). The spatial domain is discretized by the finite volume method , resulting in a \(d=2N_{x}N_{y}\)-dimensional ODE with \(N_{x}\) and \(N_{y}\) the number of space discretizations along \(x\) and \(y\), respectively. The time integration is conducted with RK of variable order for \(\) and \(\) (see Table 6 in Supplementary Material G).

As in the previous example, we conduct two experiments for this system, with speed-ups and runtimes reported in Figure 1. In the first one, we increased \(d\) and \(N\) proportionately (with \(d/N\)) while maintaining all other quantities (i.e. \(,,m_{},m_{}\)) fixed until \(N=256\). This scenario reflects a situation where more resources are allocated to solve larger problem sizes. In contrast, in the second experiment, \(N\) remains fixed at \(512\), with \(d\) increasing proportionately with \(N_{}\) to maintain algorithm stability. Moreover, \(\) is chosen to be RK8, with \(N_{}\) automatically selected by the used Python library _scipy_. This second setting simulates a scenario with constrained resources, where the user aims to solve the system using a finer spatial mesh. Table 8 in Supplementary Material I shows that for \(N 256\) and \(d/N 1\), nnGParareal fails to converge within a 48-hour budget. Parareal converges always, albeit at a considerably slower rate than RandNet-Parareal, which is x3-5 faster than Parareal (and up to x120 than the fine solver).

### Shallow water equation

Finally, we focus on SWEs on a two-dimensional domain, described by a system of hyperbolic PDEs

\[_{t}h+ h=0,_{t}h+(u^{2}h +g_{r}h^{2})=-g_{r}h b,\]

where \(=(u,v)\) represents the velocities in the horizontal \(u=u(t,x,y)\) and vertical \(v=v(t,x,y)\) directions, \(h=h(t,x,y)\) denotes the water depth, \(b=b(x,y)\) describes a (given) spatially varying bathymetry, and \(h\) can be interpreted as the directional momentum components. The parameter \(g_{r}\) describes the gravitational acceleration, while \(_{t}f\) denotes the partial derivative with respect to time, and \( f\) the gradient of a function \(f\). Following , we solve a radial dam break scenario where a Gaussian-shaped water column (blue) inundates nearby plains (green) within a rectangular box subject to Neumann boundary conditions, causing the water to rebound off the sides of the box, as depicted in Figure 2. More details on the simulation setup are given in Supplementary Material G.1.

In this case, our algorithm also converges much faster than Parareal, with a speed gain of x1.3-3.6, while nnGParareal fails to converge within the 48-hour time budget as \(d N\). Although the speed gain is lower than for the Diffusion-Reaction, the improvements are remarkable. RandNet-Parareal takes up to 4-10 hours and 37 days less than the Parareal and sequential solver, respectively.

## 6 Discussion and limitations

This study improves the scalability properties, convergence rates, and parallel performance of Parareal and a more recently proposed PinT solver for ODEs and PDEs, nnGParareal . By replacing the

Figure 1: Speed-ups (left) and runtimes (right) of Parareal, nnGParareal (\(m_{}\)=\(20\)), and RandNet-Parareal (\(m_{}\)=\(4\), \(M\)=\(100\)) for the two-dimensional Diffusion-Reaction system versus the number \(d\) of dimensions (bottom x-axis) and \(N\) cores (top x-axis) capped at \(512\) to simulate limited resources.

nnGP with random networks, we decreased the model costs (in learning the discrepancy between the fine and coarse solvers) by several orders of magnitude. The reasons behind this are multi-fold. Training of RandNets is cheap due to the availability of the closed-form solution for its output (readout) weights, and avoids any expensive hyperparameter optimization. Moreover, it is possible to simultaneously learn and predict the \(d\)-dimensional correction map instead of \(d\) scalar maps (in parallel if the number of processors \(N\) is comparable to \(d\), or queuing if smaller). The latter "liberates" RandNet-Parareal from requiring \(d N\), extending its application to high-dimensional settings, a key/notable improvement with respect to nnGParareal. We tested the proposed algorithm on systems of real-world significance, such as the Diffusion-Reaction equation, the SWE, and the Brusselator. solving them on a fine spatial mesh of up to \(10^{5}\) discretization points. These systems and requirements align with those outlined in the benchmark PDE dataset  as necessary prerequisites for using such algorithms in practical scenarios. The strength of RandNet-Parareal is the cheap cost of RandNets, which can be embedded within Parareal with virtually no overhead, irrespective of the implementation or solvers, leading to notable speed gains over Parareal (x8.6-21.2 for viscous Burgers', x3-5 for Diffusion-Reaction, x1.3-3.6 for SWE, and x3.4-4.4 for Brusselator). Moreover, training RandNets is easily conducted with established linear algebra routines, and requires no ad-hoc parameter tuning.

Despite its excellent performance, RandNet-Parareal has limitations common to all Parareal algorithms, as its rate of convergence relies on the accuracy of the coarse solver \(\). Although neural networks can help mitigate the impact of suboptimal choices of \(\) (as observed for GPs in (nn)GParareal), if the solver is mismatched for the system -- for example, an unstable solver for a stiff ODE -- RandNet-Parareal, similar to Parareal and (nn)GParareal, is likely to exhibit non-convergent behavior. It would then be of interest to investigate RandNet-Parareal's performance when using customized solvers tailored to specific systems, such as those outlined in Section 1 for the shallow water equation and the viscous Burgers' equation, which we defer to future research.