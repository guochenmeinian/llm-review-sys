# Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms

Peiyao Xiao\({}^{1}\), Hao Ban\({}^{2}\), Kaiyi Ji\({}^{1}\)

\({}^{1}\)Department of CSE, University at Buffalo

\({}^{2}\) Zuoyi Technology

{peiyaoxi,kaiyiji}@buffalo.edu, bhsimon0810@gmail.com

First two authors contributed equally.

###### Abstract

Multi-objective optimization (MOO) has become an influential framework in many machine learning problems with multiple objectives such as learning with multiple criteria and multi-task learning (MTL). In this paper, we propose a new direction-oriented multi-objective formulation by regularizing the common descent direction within a neighborhood of a direction that optimizes a linear combination of objectives such as the average loss in MTL or a weighted loss that places higher emphasis on some tasks than the others. This formulation includes GD and MGDA as special cases, enjoys the direction-oriented benefit as in CAGrad, and facilitates the design of stochastic algorithms. To solve this problem, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS with an efficient objective sampling. We develop a comprehensive convergence analysis for the proposed methods with different loop sizes and regularization coefficients. We show that both SDMGrad and SDMGrad-OS achieve improved sample complexities to find an \(\)-accurate Pareto stationary point while achieving a small \(\)-level distance toward a conflict-avoidant (CA) direction. For a constant-level CA distance, their sample complexities match the best known \((^{-2})\) without bounded function value assumption. Extensive experiments show that our methods achieve competitive or improved performance compared to existing gradient manipulation approaches in a series of tasks on multi-task supervised learning and reinforcement learning. Code is available at https://github.com/ml-opt-lab/sdmgrad.

## 1 Introduction

In recent years, multi-objective optimization (MOO) has drawn intensive attention in a wide range of applications such as online advertising , hydrocarbon production , autonomous driving , safe reinforcement learning , etc. In this paper, we focus on the stochastic MOO problem, which takes the formulation of

\[_{^{m}}():=(_{}[L_{1}(; )],_{}[L_{2}(;)],...,_{}[L_{K}(; )]),\] (1)

where \(m\) is the parameter dimension, \(K 2\) is the number of objectives and \(L_{i}():=_{}L_{i}(;)\). One important example of MOO in eq.1 is the multi-task learning (MTL) , whose objectivefunction is often regarded as the average loss over \(K\) objectives.

\[^{*}=_{^{n}}() _{i=1}^{K}L_{i}()},\] (2)

where \(L_{i}()\) is the loss function for task \(i\). However, solving the MOO problem is challenging because it can rarely find a common parameter \(\) that minimizes all individual objective functions simultaneously. As a result, a widely-adopted target is to find the _Pareto stationary point_ at which there is no common descent direction for all objective functions. In this context, a variety of gradient manipulation methods have been proposed, including the multiple gradient descent algorithm (MGDA) , PCGrad , CAGrad  and Nash-MTL . Among them, MGDA updates the model parameter \(\) using a time-varying multi-gradient, which is a convex combination of gradients for all objectives. CAGrad further improves MGDA by imposing a new constraint on the difference between the common descent direction and the average gradient to ensure convergence to the minimum of the average loss of MTL. However, these approaches mainly focus on the deterministic case, and their stochastic versions still remain under-explored.

Stochastic MOO has not been well understood except for several attempts recently. Inspired by MGDA,  proposed stochastic multi-gradient (SMG), and established its convergence with convex objectives. However, their analysis requires the batch size to increase linearly. In the more practical nonconvex setting,  proposed a correlation-reduced stochastic multi-objective gradient manipulation (CR-MOGM) to address the non-convergence issue of MGDA, CAGrad and PCGrad in the stochastic setting. However, their analysis requires a restrictive assumption on the bounded function value. Toward this end,  recently proposed a method named MoCo as a stochastic counterpart of MGDA by introducing a tracking variable to approximate the stochastic gradient. However, their analysis requires that the number \(T\) of iterations is big at an order of \(K^{10}\), where \(K\) is the number of objectives, and hence may be unsuitable for the scenario with many objectives. Thus, it is highly demanding but still challenging to develop efficient stochastic MOO methods with guaranteed convergence in the nonconvex setting under mild assumptions.

### Our Contributions

Motivated by the limitations of existing methods, we propose Stochastic Direction-oriented Multi-objective Gradient descent (SDGMGrad), which is easy to implement, flexible with a new direction-oriented objective formulation, and achieves a better convergence performance under milder assumptions. Our specific contributions are summarized as follows.

**New direction-oriented formulation.** We propose a new MOO objective (see eq. (6)) by regularizing the common descent direction \(d\) within a neighborhood of a direction \(d_{0}\) that optimizes a

  
**Algorithms** &  Batch \\ size \\  & Nonconvex &  Bounded \\ function value \\  & 
 Sample \\ complexity \\  & CA distance \\   SMG  & \((^{-2})\) & ✗ & ✗ & \((^{-4})\) & N/A \\  CR-MOGM  & \((1)\) & ✓ & ✓ & \((^{-2})\) & N/A \\  MoCo  & \((1)\) & ✓ & ✓ & \((^{-2})\) & N/A \\  MoDo  & \((1)\) & ✓ & ✗ & \((^{-2})\) & \((1)\) \\  SDGMGrad (Theorem 3) & \((1)\) & ✓ & ✗ & \((^{-2})\) & \((1)\) \\  SDGMGrad-OS (Theorem 4) & \((1)\) & ✓ & ✗ & \((^{-2})\) & \((1)\) \\   MoCo  & \((1)\) & ✓ & ✗ & \((^{-10})\) & \(()\) \\  MoDo  & \((1)\) & ✓ & ✗ & \((^{-8})\) & \(()\) \\  SDGMGrad (Corollary 1) & \((1)\) & ✓ & ✗ & \((^{-6})\) & \(()\) \\ SDGMGrad-OS (Theorem 2) & \((1)\) & ✓ & ✗ & \((^{-6})\) & \(()\) \\   

Table 1: Comparison of different algorithms for stochastic MOO problem to achieve an \(\)-accurate Pareto stationary point. MoDo  is a concurrent work. Definition of CA distance can be found in Definition 2. \(()\) omits logarithm factors.

linear combination \(L_{0}()\) of objectives such as the average loss in MTL or a weighted loss that places higher emphasis on some tasks than the others. This formulation is general to include gradient descent (GD) and MGDA as special cases and takes a similar direction-oriented spirit as in CAGrad . However, different from the objective in CAGrad  that enforces the gap between \(d\) and \(d_{0}\) to be small via constraint, our regularized objective is easier to design near-unbiased multi-gradient, which is crucial in developing provably convergent stochastic MOO algorithms.

**New stochastic MOO algorithms.** The proposed SDMGrad is simple with efficient stochastic gradient descent (SGD) type of updates on the weights of individual objectives in the multi-gradient and on the model parameters with (near-)unbiased (multi-)gradient approximations at each iteration. SDMGrad does not require either the linearly-growing batch sizes as in SMG or the extra tracking variable for gradient estimation as in MoCo. Moreover, we also propose SDMGrad-OS (which refers to SDMGrad with objective sampling) as an efficient and scalable counterpart of SDMGrad in the setting with a large number of objectives. SDMGrad-OS samples a subset of data points and objectives simultaneously in all updates and is particularly suitable in large-scale MTL scenarios.

**Convergence analysis and improved complexity.** We provide a comprehensive convergence analysis of SDMGrad for stochastic MOO with smooth nonconvex objective functions. For a constant-level regularization parameter \(\) (which covers the MGDA case), when an \(\)-level conflict-avoidant (CA) distance (see Definition 2) is required, the sample complexities (i.e., the number of samples to achieve an \(\)-accurate Pareto stationary point) of the proposed SDMGrad and SDMGrad-OS improve those of MoCo  and MoDo  by an order of \(^{-4}\) and \(^{-2}\) (see Table 1), respectively. In addition, when there is no requirement on CA distance, the sample complexities of SDMGrad and SDMGrad-OS are improved to \((^{-2})\), matching the existing best result in the concurrent work . For an increasing \(\), we also show this convergent point reduces to a stationary point of the linear combination \(L_{0}()\) of objectives.

**Promising empirical performance.** We conduct extensive experiments in multi-task supervised learning and reinforcement learning on multiple datasets and show that SDMGrad can achieve competitive or improved performance compared to existing state-of-the-art gradient manipulation methods such as MGDA, PCGrad, GradDrop, CAGrad, IMTL-G, MoCo, MoDo, Nash-MTL and FAMO, and can strike a better performance balance on different tasks. SDMGrad-OS also exhibits a much better efficiency than SDMGrad in the setting with a large number of tasks due to the efficient objective sampling.

## 2 Related Works

**Multi-task learning.** One important application of MOO is MTL, whose target is to learn multiple tasks with possible correlation simultaneously. Due to this capability, MTL has received significant attention in various applications in computer vision, natural language process, robotics, and reinforcement learning [5; 15; 16; 17]. A group of studies have focused on how to design better MTL model architectures. For example, [18; 19; 20] enhance the MTL models by introducing task-specific modules, an attention mechanism, and different activation functions for different tasks, respectively. Another line of research aims to learn a bunch of smaller models of local tasks split from the original problem, which are then aggregated into a single model via knowledge distillation . Recent several works [22; 23] have explored the connection between MTL and gradient-based meta-learning.  and  highlight the importance of task grouping and unrelated tasks in MTL, respectively. This paper focuses on a single model by learning multiple tasks simultaneously with novel model-agnostic SDMGrad and SDMGrad-OS methods.

**Gradient-based MOO.** Various gradient manipulation methods have been developed to learn multiple tasks simultaneously. One popular class of approaches re-weight different objectives based on uncertainty , gradient norm , and training difficulty . MOO-based approaches have received more attention due to their principled designs and training stability. For example,  viewed MTL as a MOO problem and proposed an MGDA-type method for optimization. A class of approaches has been proposed to address the gradient conflict problem. Among them,  proposed PCGrad by projecting the gradient direction of each task on the norm plane of other tasks. GradDrop randomly dropped out highly conflicted gradients , and RotoGrad rotated task gradients to alleviate the conflict .  proposed CAGrad by constraining the common direction direction within a local region around the average gradient. These approaches mainly focus on the deterministic setting. In the stochastic case,  proposed MoCo as a stochastic counterpart of MGDA, and provided a comprehensive convergence and complexity analysis. More recently, a concurrent work  analyzed a three-way trade-off among optimization, generalization, and conflict avoidance, providing an impact on designing the MOO algorithm. In addition,  found that scalarization SGD could be incapable of fully exploring the Pareto front compared with MGDA-variant methods. In this paper, we propose a new stochastic MOO method named SDMGrad, which benefits from a direction-oriented regularization and an improved convergence and complexity performance.

**Concurrent work.** A concurrent work  proposed a multi-objective approach named MoDo, which uses a similar double sampling strategy (see Section 4.2). However, there are still several differences between this work and . First, our method benefits from a direction-oriented mechanism and is general to include MGDA and CAGrad as special cases, whereas MoDo  does not have such features. Second, MoDo takes a single-loop structure, whereas our SDMGrad features a double-loop scheme with an improved sample complexity. Third,  focuses more on the trade-off among optimization, generalization and conflict-avoidance, whereas our work focuses more on the optimization efficiency and performance balance in theory and in experiments.

## 3 Preliminaries

### Pareto Stationarity in MOO

Differently from single-objective optimization, MOO aims to find points at which all objectives cannot be further optimized. Consider two points \(\) and \(^{}\). It is claimed that \(\) dominates \(^{}\) if \(L_{i}() L_{i}(^{})\) for all \(i[K]\) and \(L() L(^{})\). In the general nonconvex setting, MOO aims to find a Pareto stationary point \(\), at which there is no common descent direction for all objectives. In other words, we say \(\) is a Pareto stationary point if \(( L())(-^{K}_{++})=\) where \(^{K}_{++}\) is the positive orthant cone.

### MGDA and Its Stochastic Variants

**Deterministic MGDA.** The deterministic MGDA algorithm was first studied by , which updates the model parameter \(\) along a multi-gradient \(d=_{i=1}^{K}w_{i}^{*}g_{i}()=G()w^{*}\), where \(G()=(g_{1}(),g_{2}(),...,g_{K}())\) are the gradients of different objectives, and \(w^{*}:=(w_{1}^{*},w_{2}^{*},...,w_{K}^{*})^{T}\) are the weights of different objectives obtained via solving the following problem.

\[w^{*}_{w}\|G()w\|^{2}\ \ s.t.\ \ w:=\{w ^{K}|^{T}w=1,w 0\},\] (3)

where \(\) is the probability simplex. The deterministic MGDA and its variants such as PCGrad and CAGrad have been well studied, but their stochastic counterparts have not been understood well.

**Stochastic MOO algorithms.** SMG  is the first stochastic variant of MGDA by replacing the full gradient \(G()\) in eq. (3) by its stochastic gradient \(G(;):=(g_{1}(;),...,g_{K}(;))\), and updates the model parameters \(\) along the direction given by

\[d_{}=G(;)w_{}^{*}\ \ s.t.\ \ w_{}^{*}_{w}\|G( ;)w\|^{2}.\]

However, this direct replacement can introduce **biased** multi-gradient estimation, and hence SMG required to increase the batch sizes linearly with the iteration number. To address this limitation,  proposed MoCo by introducing an additional tracking variable \(y_{t,i}\) as the stochastic estimate of the gradient \( L_{i}()\), which is iteratively updated via

\[y_{t+1,i}=_{_{i}}y_{t,i}-_{t}(y_{t,i}-h_{t,i}),i=1,2,...,K,\] (4)

where \(_{t}\) is the step size, \(_{_{i}}\) denotes the projection on a bounded set \(_{i}=\{y^{m}\|y\| C_{y,i}\}\) for some constant \(C_{y,i}\), and \(h_{t,i}\) denotes stochastic estimator of \( L_{i}()\) at t-th iteration. Then, MoCo was shown to achieve an asymptotically unbiased multi-gradient, but with a relatively strong assumption that the number \(T\) of iterations is much larger than the number \(K\) of objectives. Thus, it is important but still challenging to develop provable and easy-to-implement stochastic MOO algorithms with mild assumptions.

Our Method

We first provide a new direction-oriented MOO problem, and then introduce a new stochastic MOO algorithm named SDMGrad and its variant SDMGrad-OS with objective sampling.

### Direction-oriented Multi-objective Optimization

MOO generally targets at finding a direction \(d\) to maximize the minimum decrease across all objectives via solving the following problem.

\[_{d^{m}}_{i[K]}(L_{i}()- L_{i}(- d))}_{d^{m}}_{i[K]}  g_{i},d,\] (5)

where the first-order Taylor approximation of \(L_{i}\) is applied at \(\) with a small stepsize \(\).

In some scenarios, the target is to not only find the above common descent direction but also optimize a specific objective that is often a linear combination \(L_{0}()=_{i}_{i}L_{i}()\) for some \(\) of objectives. For instance, MTL often takes the averaged loss over tasks as the objective function, and every element in \(\) will be set as \(\). In addition, it is quite possible that there is a preference for tasks. In this case, motivated by the framework in , we can regard \(\) as a preference vector and tend to approach a preferred stationary point along this direction. To address this problem, we propose the following multi-objective problem formulation by adding an inner-product regularization \( g_{0},d\) in eq. (5) such that the common descent direction \(d\) stays not far away from a target direction \(g_{0}=_{i}_{i}g_{i}\).

\[_{d^{m}}_{i[K]} g_{i},d- \|d\|^{2}+ g_{0},d.\] (6)

In the above eq. (6), the term \(-\|d\|^{2}\) is used to regularize the magnitude of the direction \(d\) and the constant \(\) controls the distance between the update vector \(d\) and the target direction \(g_{0}\).

Compared to CAGrad.We note that CAGrad also takes a direction-oriented objective but uses a different constraint-enforced formulation as follows.

\[_{d^{m}}_{i[K]} g_{i},d\ \ \ \ \|d-h_{0}\| c\|h_{0}\|\] (7)

where \(h_{0}\) is the average gradient and \(c[0,1)\) is a constant. To optimize eq. (7), CAGrad involves the evaluations of the product \(\|h_{0}\|\|g_{w}\|\) and the relation \(\|}{|g_{w}|}\) with \(g_{w}:=_{i}w_{i}g_{i}\), both of which complicate the designs of unbiased stochastic gradient/multi-gradient in the \(w\) and \(\) updates. As a comparison, our formulation in eq. (6), as shown later, admits very simple and provable stochastic algorithmic designs, while still enjoying the direction-oriented benefit as in CAGrad.

To efficiently solve the problem in eq. (6), we then substitute the relation that \(_{i[K]} g_{i},d=_{w}_{i=1}^{ K}g_{i}w_{i},d=_{w} g_{w},d\) into eq. (6), and obtain an equivalent problem as

\[_{d^{m}}_{w} g_{w}+  g_{0},d-\|d\|^{2}.\]

By switching min and max in the above problem, which does not change the solution due to the concavity in \(d\) and the convexity in \(w\), we finally aim to solve \(_{w}_{d^{m}} g_{w}+ g_{0},d -\|d\|^{2}\), where the solution to the min problem on \(w\) is

\[w_{}^{*}_{w}\|g_{w}+  g_{0}\|^{2},\] (8)

and the solution to the max problem is \(d^{*}=g_{w_{}^{*}}+ g_{0}\).

Connection with MGDA and GD.It can be seen from eq. (8) that the updating direction \(d^{*}\) reduces to that of MGDA whenwe set \(=0\), and is consistent with that of GD for \(\) large enough. This consistency is also validated empirically in Appendix A.2 by varying \(\).

### Proposed SDMGrad Algorithm

The natural idea to solve the problem in eq. (8) is to use a simple SGD-type method. The detailed steps are provided in algorithm 1. At each iteration \(t\), we run \(S\) steps of projected SGD with warm-start initialization and with a double-sampling-based stochastic gradient estimator, which is unbiased by noting that

\[_{,^{}}[G(_{t};)^{T}G(_{t};^{ })w_{t,s}+ g_{0}(_{t};^{})]=G(_{t})^{ T}G(_{t})w_{t,s}+ g_{0}(_{t}),\]

where \(,^{}\) are sampled data and \(g_{0}(_{t})=G(_{t})\) denotes the orientated direction. After obtaining the estimate \(w_{t,S}\), SDMGrad updates the model parameters \(_{t}\) based on the stochastic counterpart \(G(_{t};)w_{t,S}+ g_{0}(_{t};)\) of the direction \(d^{*}=g_{w_{1}^{*}}+ g_{0}\) with a stepsize of \(_{t}\). It can be seen that SDMGrad is simple to implement with efficient SGD type of updates on both \(w\) and \(\) without introducing other auxiliary variables.

```
1:Initialize: model parameters \(_{0}\) and weights \(w_{0}\)
2:for\(t=0,1,...,T-1\)do
3:for\(s=0,1,...,S-1\)do
4: Set \(w_{t,0}=w_{t-1,S}\) (warm start) and sample data \(,^{}\)
5:\(w_{t,s+1}=_{}w_{t,s}-_{t,s}[G(_{t};)^{T} G(_{t};^{})w_{t,s}+ g_{0}(_{t};^{} )]\)
6:endfor
7: Sample data \(\) and \(_{t+1}=_{t}-_{t}G(_{t};)w_{t,S}+ g_ {0}(_{t};)\)
8:endfor ```

**Algorithm 1** Stochastic Direction-oriented Multi-objective Gradient descent (SDMGrad)

### SDMGrad with Objective Sampling (SDMGrad-OS)

Another advantage of SDMGrad is its simple extension via objective sampling to the more practical setting with a large of objectives, e.g., in large-scale MTL. In this setting, we propose SDMGrad-OS by replacing the gradient matrix \(G(_{t};)\) in Algorithm 1 by a matrix \(H(_{t};)\) with randomly sampled columns, which takes the form of

\[H(_{t};,)=h_{1}(_{t};),h_{2}(_{t}; ),...,h_{K}(_{t};),\] (9)

where \(h_{i}(_{t};)=g_{i}(_{t};)\) with a probability of \(n/K\) or 0 otherwise, \(\) corresponds to the randomness by objective sampling, and \(n\) is the expected number of sampled objectives. Then, the stochastic gradient in the \(w\) update is adapted to

\[(w)}{n^{2}}H(_{t};, )^{T}H(_{t};^{},^{})w_{t,s} + h_{0}(_{t};^{},^{}),\]

where \(h_{0}()=H()\). Similarly, the updating direction \(d=H(_{t};,})w_{t,S}+  h_{0}(_{t};,})\). In the practical implementation, we first use _np.random.binomial_ to sample a \(0\)-\(1\) sequence \(s\) following a Bernoulli distribution with length \(K\) and probability \(n/K\), and then compute the gradient \(g_{i}(;)\) of \(G(;)\) only if the \(i^{th}\) entry of \(s\) equals to \(1\). This sampling strategy greatly speeds up the training with a large number of objectives, as shown by the reinforcement learning experiments in Section 6.3.

## 5 Main results

### Definitions and Assumptions

We first make some standard definitions and assumptions, as also adopted by existing MOO studies in [11; 12; 13]. Since we focus on the practical setting where the objectives are nonconvex, algorithms are often expected to find an \(\)-accurate Pareto stationary point, as defined below.

**Definition 1**.: _We say \(^{m}\) is an \(\)-accurate Pareto stationary point if \(_{w}\|g_{w}()\|^{2}\) where \(\) is the probability simplex._

MGDA-variant methods find a direction \(d()\) (e.g., \(d^{*}=g_{w_{}^{*}}+ g_{0}\) with \(w_{}^{*}\) given by eq. (8)) that tends to optimize all objective functions simultaneously, which is called a conflict-avoidant (CA)direction . Thus, it is important to measure the distance between a stochastic direction estimate \(()\) and the CA direction, which we call as CA distance, as defined below.

**Definition 2**.: \(\|_{()}[()]-d()\|\) _denotes the CA distance._

The following assumption imposes the Lipschitz continuity on the objectives and their gradients.

**Assumption 1**.: _For every task \(i[K],\ L_{i}()\) is \(l_{i}\)-Lipschitz continuous and \( L_{i}()\) is \(l_{i,1}\)-Lipschitz continuous for any \(^{m}\)._

We next make an assumption on the bias and variance of the stochastic gradient \(g_{i}(;)\).

**Assumption 2**.: _For every task \(i[K]\), the gradient \(g_{i}(;)\) is the unbiased estimate of \(g_{i}()\). Moreover, the gradient variance is bounded by \(_{}[\|g_{i}(;)-g_{i}()\|^{2}]_{i}^{2}\)._

**Assumption 3**.: _Assume there exists a constant \(C_{g}>0\) such that \(\|G()\| C_{g}\)._

The bounded gradient condition in Assumption 3 is necessary to ensure the boundedness of the multi-gradient estimation error, as also adopted by .

### Convergence Analysis with Nonconvex Objectives

We first upper-bound the CA distance for our proposed method.

**Proposition 1**.: _Suppose Assumptions 1-3 are satisfied. If we choose \(_{t,s}=c/\) and \(S>1\), then_

\[\|_{,w_{t,S}|_{t}}[G(_{t};)w_{t,S}+ g_ {0}(_{t};)]-G(_{t})w_{t,}^{*}- g_{0}(_{t} )\|+2cC_{1}}},\]

_where \(C_{1}=(K(1+))\) and \(c\) is a constant._

Proposition 1 shows that CA distance is decreasing with the number \(S\) of iterations on \(w\) updates. Then, by selecting a properly large \(S\), we obtain the following general convergence result.

**Theorem 1** (SDMGrad).: _Suppose Assumption 1-3 are satisfied. Set \(_{t}==((1+)^{-1}K^{-}T^{-})\), \(_{t,s}=c/\), where \(c\) is a constant, and \(S=((1+)^{-2}T^{2})\). Then the outputs of the proposed SDMGrad algorithm satisfy_

\[_{t=0}^{T-1}[\|G(_{t})w_{t,}^{*}+  g_{0}(_{t})\|^{2}]=}((1+^{2})K^{ }T^{-}),\] (10)

_where \(}\) omits the order of \( T\)._

Theorem 1 establishes a general convergence guarantee for SDMGrad along the multi-gradient direction \(d^{*}=g_{w_{}^{*}}+ g_{0}\). Building on Theorem 1, we next show that with different choices of the regularization parameter \(\), two types of convergence results can be obtained in the following two corollaries.

**Corollary 1** (Constant \(\)).: _Under the same setting as in Theorem 1, choosing a constant-level \(>0\), we have \(_{t=0}^{T-1}[\|G(_{t})w_{t}^{*}\|^{2}]= }(K^{}T^{-})\), where \(w_{t}^{*}*{arg\,min}_{w V}\|G(_{t})w\|^{2}\). To achieve an \(\)-accurate Pareto stationary point, each objective requires \((K^{3}^{-6})\) samples in \(\) (\(^{}\)) and \((K^{-2})\) samples in \(\), respectively. Meanwhile, the CA distance takes the order of \(}()\)._

Corollary 1 covers the MGDA case when \(=0\). In this setting, the sample complexity \((^{-6})\) improves those of MoCo  and MoDo  by an order of \(^{-4}\) and \(^{-2}\), respectively, while achieving an \(\)-level CA distance.

**Corollary 2** (Increasing \(\)).: _Under the same setting as in Theorem 1 and choosing \(=(T^{})\), we have \(_{t=0}^{T-1}[\|g_{0}(_{t})\|^{2}]=( K^{}T^{-}).\) To achieve an \(\)-accurate stationary point, each objective requires \((K^{2}^{-4})\) samples in \(\) (\(^{}\)) and \((K^{-2})\) samples in \(\), respectively. Meanwhile, the CA distance takes the order of \(}()\)._Corollary 2 analyzes the case with an increasing \(=(T^{})\). We show that SDMGrad converges to a stationary point of the objective \(L_{0}()=_{i}_{i}L_{i}()\) with an improved sample complexity, but with a worse constant-level CA distance. This justifies the flexibility of our framework that the \(\) can balance the worst local improvement of individual objectives and the target objective \(L_{0}()\).

**Convergence under objective sampling.** We provide a convergence analysis for SDMGrad-OS.

**Theorem 2** (SDMGrad-OS).: _Suppose Assumption 1-3 are satisfied. Define \(=\), \(_{t}==((1+^{2})^{-}^{-}K^ {-}T^{-})\), \(_{t,s}=c/\) where \(c\) is a constant, and \(S=((1+^{2})^{-2}^{-2}T^{2})\). Then, by choosing a constant \(\), the iterates of the proposed SDMGrad-OS algorithm satisfy_

\[_{t=0}^{T-1}[\|G(_{t})w_{t}^{*}\|^{2}]= {}(K^{}^{}T^{-}).\]

Theorem 2 establishes the convergence guarantee for our SDMGrad-OS algorithm, which achieves a per-objective sample complexity of \((^{-6})\) comparable to that of SDMGrad, but with a much better efficiency due to the objective sampling, as also validated by the empirical comparison in Table 4.

### Lower sample complexity but constant-level CA distance

Without the requirement on the \(\)-level CA distance, we further improve the sample complexity of our method to \(}(^{-2})\), as shown in the following theorem.

**Theorem 3**.: _Suppose Assumptions 1-3 are satisfied. Set \(S=1\), \(_{t}==(K^{-}T^{-})\), \(_{t}==(K^{-1}T^{-})\) and \(\) as constant. The iterates of the proposed SDMGrad satisfy_

\[_{t=0}^{T-1}[\|G(_{t})w_{t}^{*}\|^{2}]=(KT^{-}).\]

Theorem 3 shows that to achieve an \(\)-accurate Pareto stationary point, our method requires \(T=(K^{2}^{-2})\). In this case, each objective requires a number \((K^{2}^{-2})\) of samples in \((^{})\) and \(\).

**Convergence under objective sampling.** We next analyze the convergence of SDMGrad-OS.

**Theorem 4**.: _Suppose Assumptions 1-3 are satisfied. Set \(S=1\), \(=\), \(_{t}==(K^{-}^{-}T^{-})\), \(_{t}==(K^{-1}^{-1}T^{-})\) and \(\) as a constant. The iterates of the proposed SDMGrad-OS algorithm satisfy_

\[_{t=0}^{T-1}[\|G(_{t})w_{t}^{*}\|^{2}]= (K T^{-}).\]

Theorem 4 shows that to achieve an \(\)-accurate Pareto stationary point, our algorithm requires \(T=(^{2}K^{2}^{-2})\), and each objective requires a number \((^{2}K^{2}^{-2})\) of samples in \((^{})\) and \(\).

## 6 Experiments

In this section, we first describe the implementation details of our proposed methods. Then, we demonstrate the effectiveness of the methods under a couple of multi-task supervised learning and reinforcement settings. The experimental details and more empirical results such as the two-objective toy example, consistency with GD and MGDA, and ablation studies over \(\) can be found in the Appendix A.

### Practical Implementation

**Double sampling.** In supervised learning, double sampling (i.e., drawing two samples simultaneously for gradient estimation) is employed, whereas in reinforcement learning, single sampling is used because double sampling requires to visit the entire episode twice a time, which is much more time-consuming.

**Gradient normalization and rescale.** During the training process, the gradient norms of tasks may change over time. Thus, directly solving the objective in eq. (8) may trigger numerical problems.

Inspired by CAGrad , we normalize the gradient of each task and rescale the final update \(d\) by multiplying a factor of \(\) to stabilize the training.

**Projected gradient descent.** The computation of the projection to the probability simplex we use is the Euclidean projection proposed by , which involves solving a convex problem via quadratic programming. The implementation used in our experiments follows the repository in , which is very efficient in practice.

### Supervised Learning

For the supervised learning setting, we evaluate the performance on the Cityscapes  and NYU-v2  datasets. The former dataset involves 2 pixel-wise tasks: 7-class semantic segmentation and depth estimation, and the latter one involves 3 pixel-wise tasks: 13-class semantic segmentation, depth estimation and surface normal estimation. Following the experimental setup of , we embed a MTL method MTAN  into our SDMGrad method, which builds on SegNet  and is empowered by a task-specific attention mechanism. We compare SDMGrad with Linear Scalarization (LS) which minimizes the average loss, Scale-invariant (SI) which minimizes the average logarithmic loss, RLW , DWA , UW , MGDA , PCGrad , GradDrop , CAGrad , IMTL-G , MoCo , MoDo , Nash-MTL , and FAMO . Following [42; 9; 13; 10; 38], we compute two metrics reflecting the overall performance: **(1) \(m\%\)**, the average per-task performance drop versus the single-task (STL) baseline \(b\) to assess method \(m\): \( m\%=_{k=1}^{K}(-1)^{l_{k}}(M_{m,k}-M_{b,k})/M_{b,k}  100\), where \(K\) is the number of metrics, \(M_{b,k}\) is the value of metric \(M_{k}\) obtained by baseline \(b\), and \(M_{m,k}\) obtained by the compared method \(m\). \(l_{k}=1\) if the evaluation metric \(M_{k}\) on task \(k\) prefers a higher value and \(0\) otherwise. **(2) Mean Rank (MR)**: the average rank of each method across all tasks.

We search the hyperparameter \(\{0.1,0.2,,1.0\}\) for our SDMGrad method and report the results in Table 2 and Table 3. Each experiment is repeated 3 times with different random seeds and the average is reported. It can be seen that our proposed method is able to obtain better or comparable results than the baselines, and in addition, can strike a better performance balance on multiple tasks than other baselines. For example, although MGDA achieves better results on Surface Normal, it performs the worst on both Segmentation and Depth. As a comparison, our SDMGrad method can achieve more balanced results on all tasks.

### Reinforcement Learning

For the reinforcement learning setting, we evaluate the performance on the MT10 benchmarks, which include 10 robot manipulation tasks under the Meta-World environment . Following the experiment setup in [9; 13; 10], we adopt Soft Actor-Critic (SAC)  as the underlying training algorithm. We compare SDMGrad with Multi-task SAC , Multi-headed SAC , Multi-task SAC + Task Encoder , PCGrad , CAGrad , MoCo , Nash-MTL  and FAMO . We search \(\{0.1,0.2,,1.0\}\) and provide the success rate and average training time (in sec

    &  &  &  &  \\  & mIoU \(\) &  & & & & \\  STL & 74.01 & 93.16 & 0.0125 & 27.77 & & \\  LS & 75.18 & 93.49 & 0.0155 & 46.77 & 8.50 & 22.60 \\ SI & 70.95 & 91.73 & 0.0161 & 33.83 & 11.50 & 14.11 \\ RLW  & 74.57 & 93.41 & 0.0158 & 47.79 & 11.25 & 24.38 \\ DWA  & 75.24 & 93.52 & 0.0160 & 44.37 & 8.50 & 21.45 \\ UW  & 72.02 & 92.85 & 0.0140 & **30.13** & 7.75 & **5.89** \\ MGDA  & 68.84 & 91.54 & 0.0309 & 33.50 & 12.00 & 44.14 \\ PCGrad  & 75.13 & 93.48 & 0.0154 & 42.07 & 8.75 & 18.29 \\ GradDrop  & 75.27 & 93.53 & 0.0157 & 47.54 & 7.75 & 23.73 \\ CAGGrad  & 75.16 & 93.48 & 0.0141 & 37.60 & 7.25 & 11.64 \\ IMTL-G  & 75.33 & 93.49 & 0.0135 & 38.41 & 5.25 & 11.10 \\ MoCo  & 75.42 & 93.55 & 0.0149 & 34.19 & 4.00 & 9.90 \\ MoDo  & 74.55 & 93.32 & 0.0159 & 41.51 & 10.75 & 18.89 \\ Nash-MTL  & **75.41** & **93.66** & **0.0129** & 35.02 & **2.75** & 6.82 \\ FAMO  & 74.54 & 93.29 & 0.0145 & 32.59 & 7.75 & 8.13 \\  SDMGrad & 74.53 & 93.52 & 0.0137 & 34.01 & 6.25 & 7.79 \\   

Table 2: Multi-task supervised learning on Cityscapes dataset.

[MISSING_PAGE_FAIL:10]