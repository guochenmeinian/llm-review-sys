# Geometric Algebra Transformer

Johann Brehmer

Equal contribution.

Pim de Haan

Equal contribution.

Sonke Behrends

Qualcomm AI Research

Taco Cohen

{jbrehmer, pim, sbehrend, tacos}@qti.qualcomm.com

###### Abstract

Problems involving geometric data arise in physics, chemistry, robotics, computer vision, and many other fields. Such data can take numerous forms, for instance points, direction vectors, translations, or rotations, but to date there is no single architecture that can be applied to such a wide variety of geometric types while respecting their symmetries. In this paper we introduce the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data. GATr represents inputs, outputs, and hidden states in the projective geometric (or Clifford) algebra, which offers an efficient \(16\)-dimensional vector-space representation of common geometric objects as well as operators acting on them. GATr is equivariant with respect to \((3)\), the symmetry group of 3D Euclidean space. As a Transformer, GATr is versatile, efficient, and scalable. We demonstrate GATr in problems from \(n\)-body modeling to wall-shear-stress estimation on large arterial meshes to robotic motion planning. GATr consistently outperforms both non-geometric and equivariant baselines in terms of error, data efficiency, and scalability.

## 1 Introduction

From molecular dynamics to astrophysics, from material design to robotics, fields across science and engineering deal with geometric data such as positions, shapes, orientations, or directions. The geometric nature of data provides a rich structure: a notion of common operations between geometric types (computing distances between points, applying rotations to orientations, etc.), a well-defined behaviour of data under transformations of a system, and the independence of certain properties of coordinate system choices. When learning from geometric data, incorporating this rich structure into the architecture has the potential to improve the performance.

With this goal, we introduce the _Geometric Algebra Transformer_ (GATr), a general-purpose network architecture for geometric data. GATr brings together three key ideas.

**Geometric algebra:**: To naturally describe both geometric objects as well as their transformations in three-dimensional space, GATr represents data as multivectors of the projective geometric algebra \(_{3,0,1}\). Geometric (or Clifford) algebra is an principled yet practical mathematical framework for geometrical computations. The particular algebra \(_{3,0,1}\) extends the vector space \(^{3}\) to 16-dimensional multivectors, which can natively represent various geometric types and \((3)\) poses. Unlike the \((3)\) representations popular in geometric deep learning, this algebra can represent data that is not invariant to translations, such as absolute positions.
**Equivariance:**: GATr is equivariant with respect to \((3)\), the symmetry group of three-dimensional space. To this end, we develop several new \((3)\)-equivariant primitives mapping between multivectors, including equivariant linear maps, an attention mechanism, nonlinearities, and normalization layers.

**Transformer:**: Due to its favorable scaling properties, expressiveness, trainability, and versatility, the Transformer architecture  has become the de-facto standard for a wide range of problems. GATr is based on the Transformer architecture, in particular on dot-product attention, and inherits these benefits.

GATr thus combines two lines of research: the representation of geometric objects with geometric algebra [17; 18; 38], popular in computer graphics and physics and recently gaining traction in deep learning [5; 41; 46], and the encoding of symmetries through equivariant deep learning . The result--to the best of our knowledge the first \((3)\)-equivariant architecture with internal geometric algebra representations3--is a versatile network for problems involving geometric data.

We demonstrate GATr in three problems from entirely different fields. In an \(n\)-body modelling task, we compare GATr to various baselines. We turn towards the task of predicting the wall shear stress in human arteries, demonstrating that GATr scales to realistic problems with meshes of thousands of nodes. Finally, experiment with robotic motion planning, using GATr as the backbone of an \((3)\)-invariant diffusion model. In all cases, GATr substantially outperforms both non-geometric and equivariant baselines.

Our implementation of GATr is available at https://github.com/qualcomm-ai-research/geometric-algebra-transformer.

## 2 Background

Geometric algebraWe begin with a brief overview of geometric algebra (GA); for more detail, see e. g. Refs. [17; 18; 38; 41]. Whereas a plain vector space like \(^{3}\) allows us to take linear combinations of elements \(x\) and \(y\) (vectors), a geometric algebra additionally has a bilinear associative operation: the _geometric product_, denoted simply by \(xy\). By multiplying vectors, one obtains so-called _multivectors_, which can represent both geometrical _objects_ and _operators_. Like vectors, multivectors have a notion of direction as well as magnitude and orientation (sign), and can be linearly combined.

Multivectors can be expanded on a multivector basis, consisting of products of basis vectors. For example, in a 3D GA with orthogonal basis \(e_{1},e_{2},e_{3}\), a general multivector takes the form

\[x=x_{s}+x_{1}e_{1}+x_{2}e_{2}+x_{3}e_{3}+x_{12}e_{1}e_{2}+x_{13}e_{1}e_{3}+x_{ 23}e_{2}e_{3}+x_{123}e_{1}e_{2}e_{3},\] (1)

with real coefficients \((x_{s},x_{1},,x_{123})^{8}\). Thus, similar to how a complex number \(a+bi\) is a sum of a real scalar and an imaginary number,4 a general multivector is a sum of different kinds of elements. These are characterized by their dimensionality (grade), such as scalars (grade \(0\)), vectors \(e_{i}\) (grade \(1\)), bivectors \(e_{i}e_{j}\) (grade \(2\)), all the way up to the _pseudoscalar_\(e_{1} e_{d}\) (grade \(d\)).

The geometric product is characterized by the fundamental equation \(vv= v,v\), where \(,\) is an inner product. In other words, we require that the square of a vector is its squared norm. In an orthogonal basis, where \( e_{i},e_{j}_{ij}\), one can deduce that the geometric product of two different basis vectors is antisymmetric: \(e_{i}e_{j}=-e_{j}e_{i}\)5. Since reordering only produces a sign flip, we only get one basis multivector per unordered subset of basis vectors, and so the total dimensionality of a GA is \(_{i=0}^{d}=2^{d}\). Moreover, using bilinearity and the fundamental equation one can compute the geometric product of arbitrary multivectors.

The symmetric and antisymmetric parts of the geometric product are called the interior and exterior (wedge) product. For vectors \(x\) and \(y\), these are defined as \( x,y=(xy+yx)/2\) and \(x y(xy-yx)/2\). The former is indeed equal to the inner product used to define the GA, whereas the latter is new notation. Whereas the inner product computes the similarity, the exterior product constructs a multivector (called a blade) representing the weighted and oriented subspace spanned by the vectors. Both operations can be extended to general multivectors .

The final primitive of the GA that we will require is the dualization operator \(x x^{*}\). It acts on basis elements by swapping "empty" and "full" dimensions, e. g. sending \(e_{1} e_{23}\).

Projective geometric algebraIn order to represent three-dimensional objects as well as arbitrary rotations and translations acting on them, the 3D GA is not enough: as it turns out, its multivectors can only represent linear subspaces passing through the origin as well as rotations around it. A common trick to expand the range of objects and operators is to embed the space of interest (e.g. \(^{3}\)) into a higher dimensional space whose multivectors represent more general objects and operators in the original space.

In this paper we work with the projective geometric algebra \(_{3,0,1}\). Here one adds a fourth _homogeneous coordinate_\(x_{0}e_{0}\) to the vector space, yielding a \(2^{4}=16\)-dimensional geometric algebra. The metric of \(_{3,0,1}\) is such that \(e_{0}^{2}=0\) and \(e_{i}^{2}=1\) for \(i=1,2,3\). As we will explain in the following, in this setup the 16-dimensional multivectors can represent 3D points, lines, and planes, which need not pass through the origin, and arbitrary rotations, reflections, and translations in \(^{3}\).

Representing transformationsIn geometric algebra, a vector \(u\) can act as an operator, reflecting other elements in the hyperplane orthogonal to \(u\). Since any orthogonal transformation is equal to a sequence of reflections, this allows us to express any such transformation as a geometric product of (unit) vectors, called a (unit) _versor_\(u=u_{1} u_{k}\). Furthermore, since the product of unit versors is a unit versor, and unit vectors are their own inverse (\(u^{2}=1\)), these form a group called the Pin group associated with the metric. Similarly, products of an even number of reflections form the Spin group. In the projective geometric algebra \(_{3,0,1}\), these are the double cover6 of \((3)\) and \((3)\), respectively. We can thus represent any rotation, translation, and mirroring--the symmetries of three-dimensional space--as \(_{3,0,1}\) multivectors.

In order to apply a versor \(u\) to an arbitrary element \(x\), one uses the _sandwich product_:

\[_{u}(x)=uxu^{-1}&\\ uu^{-1}&\] (2)

Here \(\) is the _grade involution_, which flips the sign of odd-grade elements such as vectors and trivectors, while leaving even-grade elements unchanged. Equation 2 thus gives us a linear action (i. e. group representation) of the Pin and Spin groups on the \(2^{d}\)-dimensional space of multivectors. The sandwich product is grade-preserving, so this representation splits into a direct sum of representations on each grade.

Representing 3D objectsFollowing Refs. , we represent planes with vectors, and require that the intersection of two geometric objects is given by the wedge product of their representations. Lines (the intersection of two planes) are thus represented as bivectors, points (the intersection of three planes) as trivectors. This leads to a duality between objects and operators, where objects are represented like transformations that leave them invariant. Table 1 provides a dictionary of these embeddings. It is easy to check that this representation is consistent with using the sandwich product for transformations.

   Object / operator &  Scalar \\ 1 \\  &  Vector \\ \(}\) \\  &  **Bivector** \\ \(}\) \\  &  **Trivector** \\ \(}\) \\  & 
 **PS** \\ \(}\) \\  \\   Scalar \(\) & \(\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) \\ Plane w/ normal \(n^{3}\), origin shift \(d\) & \(0\) & \(\) & \(\) & \(0\) & \(0\) & \(0\) & \(0\) \\ Line w/ direction \(n^{3}\), orthogonal shift \(s^{3}\) & \(0\) & \(0\) & \(0\) & \(\) & \(\) & \(0\) & \(0\) & \(0\) \\ Point \(p^{3}\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) & \(\) & \(\) & \(0\) \\ Pseudoscalar \(\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) & \(\) \\   Reflection through plane w/ normal \(n^{3}\), origin shift \(d\) & \(0\) & \(\) & \(\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) \\ Translation \(t^{3}\) & \(\) & \(0\) & \(0\) & \(\) & \(0\) & \(0\) & \(0\) & \(0\) \\ Rotation expressed as quaternion \(q^{4}\) & \(}\) & \(0\) & \(0\) & \(0\) & \(}\) & \(0\) & \(0\) & \(0\) \\ Point reflection through \(p^{3}\) & \(0\) & \(0\) & \(0\) & \(0\) & \(0\) & \(\) & \(\) & \(0\) \\   

Table 1: Embeddings of common geometric objects and transformations into the projective geometric algebra \(_{3,0,1}\). The columns show different components of the multivectors with the corresponding basis elements, with \(i,j\{1,2,3\},j i\), i.e. \(ij\{12,13,23\}\). For simplicity, we fix gauge ambiguities (the weight of the multivectors) and leave out signs (which depend on the ordering of indices in the basis elements).

EquivarianceWe construct network layers that are equivariant with respect to \((3)\), or equivalently its double cover \((3,0,1)\). A function \(f:_{3,0,1}_{3,0,1}\) is \((3,0,1)\)-equivariant with respect to the representation \(\) (or \((3,0,1)\)-equivariant for short) if

\[f(_{u}(x))=_{u}(f(x))\] (3)

for any \(u(3,0,1)\) and \(x_{3,0,1}\), where \(_{u}(x)\) is the sandwich product defined in Eq. (2).

## 3 The Geometric Algebra Transformer

### Design principles and architecture overview

Geometric inductive bias through geometric algebra representationsGATr is designed to provide a strong inductive bias for geometric data. It should be able to represent different geometric objects and their transformations, for instance points, lines, planes, translations, rotations, and so on. In addition, it should be able to represent common interactions between these types with few layers, and be able to identify them from little data (while maintaining the low bias of large transformer models). Examples of such common patterns include computing the relative distances between points, applying geometric transformations to objects, or computing the intersections of planes and lines.

Following a body of research in computer graphics, we propose that geometric algebra gives us a language that is well-suited to this task. We use the projective geometric algebra \(_{3,0,1}\) and use the plane-based representation of geometric structure outlined in the previous section.

Symmetry awareness through \((3)\) equivarianceOur architecture should respect the symmetries of 3D space. Therefore we design GATr to be equivariant with respect to the symmetry group \((3)\) of translations, rotations, and reflections.

Note that the projective geometric algebra naturally offers a faithful representation of \((3)\), including translations. We can thus represent objects that transform arbitrarily under \((3)\), including with respect to translations of the inputs. This is in stark contrast with most \((3)\)-equivariant architectures, which only use \((3)\) representations and whose features only transform under rotation and are invariant under translations--and hence can not represent absolute positions like GATr can. Those architectures must handle points in hand-crafted ways, like by canonicalizing w. r. t. the center of mass or by treating the difference between points as a translation-invariant vector.

Many systems will not exhibit the full \((3)\) symmetry group. The direction of gravity, for instance, often breaks it down to the smaller \((2)\) group. To maximize the versatility of GATr, we choose to develop a \((3)\)-equivariant architecture and to include symmetry-breaking as part of the network inputs, similar to how position embeddings break permutation equivariance in transformers.

Scalability and flexibility through dot-product attentionFinally, GATr should be expressive, easy to train, efficient, and scalable to large systems. It should also be as flexible as possible, supporting variable geometric inputs and both static scenes and time series.

These desiderata motivate us to implement GATr as a transformer , based on attention over multiple objects (similar to tokens in MLP or image patches in computer vision). This choice makes GATr equivariant also with respect to permutations along the object dimension. As in standard transformers, we can break this equivariance when desired (in particular, along time dimensions) through positional embedding.

Like a vanilla transformer, GATr is based on a dot-product attention mechanism, for which heavily optimized implementations exist [14; 32; 37]. We will demonstrate later that this allows us to scale GATr to problems with many thousands of tokens, much further than equivariant architectures based on graph neural networks and message-passing algorithms.

Architecture overviewWe sketch GATr in Fig. 1. In the top row, we sketch the overall workflow. If necessary, raw inputs are first preprocessed into geometric types. The geometric objects are then embedded into multivectors of the geometric algebra \(_{3,0,1}\), following the recipe described in Tbl. 1.

The multivector-valued data are processed with a GATr network. We show this architecture in more detail in the bottom row of Fig. 1. GATr consists of \(N\) transformer blocks, each consistingof an equivariant multivector LayerNorm, an equivariant multivector self-attention mechanism, a residual connection, another equivariant LayerNorm, an equivariant multivector MLP with geometric bilinear interactions, and another residual connection. The architecture is thus similar to a typical transformer  with pre-layer normalization [2; 54], but adapted to correctly handle multivector data and be \((3)\) equivariant. We describe the individual layers below.

Finally, from the outputs of the GATr network we extract the target variables, again following the mapping given in Tbl. 1.

### GATr primitives

Linear layersWe begin with linear layers between multivectors. In Appendix A, we show that the equivariance condition of Eq. (3) severely constrains them:

**Proposition 1**.: _Any linear map \(:_{d,0,1}_{d,0,1}\) that is equivariant to \((d,0,1)\) is of the form_

\[(x)=_{k=0}^{d+1}w_{k} x_{k}+_{k=0}^{d}v_{k}e_{0}  x_{k}\] (4)

_for parameters \(w^{d+2},v^{d+1}\). Here \( x_{k}\) is the blade projection of a multivector, which sets all non-grade-\(k\) elements to zero._

Thus, \((3)\)-equivariant linear maps between \(_{3,0,1}\) multivectors can be parameterized with nine coefficients, five of which are the grade projections and four include a multiplication with the homogeneous basis vector \(e_{0}\). We thus parameterize affine layers between multivector-valued arrays with Eq. (4), with learnable coefficients \(w_{k}\) and \(v_{k}\) for each combination of input channel and output channel. In addition, there is a learnable bias term for the scalar components of the outputs (biases for the other components are not equivariant).

Geometric bilinearsEquivariant linear maps are not sufficient to build expressive networks. The reason is that these operations allow for only very limited grade mixing, as shown in Prop. 1. For the network to be able to construct new geometric features from existing ones, such as the translation vector between two points, two additional primitives are essential.

The first is the geometric product \(x,y xy\), the fundamental bilinear operation of geometric algebra. It allows for substantial mixing between grades: for instance, the geometric product of vectors consists of scalars and bivector components. The geometric product is equivariant (Appendix A).

The second geometric primitive we use is derived from the so-called _join_7\(x,y(x^{*} y^{*})^{*}\). This map may appear complicated, but it plays a simple role in our architecture: an equivariant map that involves the dual \(x x^{*}\). Including the dual in an architecture is essential for expressivity: in \(_{3,0,1}\), without any dualization it is impossible to represent even simple functions such as the Euclidean

Figure 1: Overview over the GATr architecture. Boxes with solid lines are learnable components, those with dashed lines are fixed.

distance between two points ; we show this in Appendix A. While the dual itself is not \((3,0,1)\)-equivariant (w. r. t. \(\)), the join operation is equivariant to even (non-mirror) transformations. To make the join equivariant to mirrorings as well, we multiply its output with a pseudoscalar derived from the network inputs: \(x,y,z(x,y;z)=z_{0123}(x^{} y^{*})^{*}\), where \(z_{0123}\) is the pseudoscalar component of a reference multivector \(z\) (see Appendix B).

We define a _geometric bilinear layer_ that combines the geometric product and the join of the two inputs as \((x,y;z)=_{}(xy,(x,y;z))\). In GATr, this layer is included in the MLP.

Nonlinearities and normalizationWe use scalar-gated GELU nonlinearities \((x)=(x_{1})x\), where \(x_{1}\) is the scalar component of the multivector \(x\). Moreover, we define an \((3)\)-equivariant LayerNorm operation for multivectors as \((x)=x/_{c} x,x}\), where the expectation goes over channels and we use the invariant inner product \(,\) of \(_{3,0,1}\).

AttentionGiven multivector-valued query, key, and value tensors, each consisting of \(n_{i}\) items (or tokens) and \(n_{c}\) channels (key length), we define the \((3)\)-equivariant multivector attention as

\[(q,k,v)_{i^{}c^{}}=_{i}_{i} q_{i^{}c},k_{ic}}{ }}\;v_{ic^{}}\,.\] (5)

Here the indices \(i,i^{}\) label items, \(c,c^{}\) label channels, and \(,\) is the invariant inner product of the geometric algebra. Just as in the original transformer , we thus compute scalar attention weights with a scaled dot product; the difference is that we use the inner product of \(_{3,0,1}\), which is the regular \(^{8}\) dot product on 8 of the 16 dimensions, ignoring the dimensions containing \(e_{0}\).8 We compute the attention using highly efficient implementations of conventional dot-product attention [14; 32; 37]. As we will demonstrate later, this allows us to scale GATr to systems with many thousands of tokens. We extend this attention mechanism to multi-head self-attention in the usual way.

### Extensions

Auxiliary scalar representationsWhile multivectors are well-suited to model geometric data, many problems contain non-geometric information as well. Such scalar information may be high-dimensional, for instance in sinusoidal positional encoding schemes. Rather than embedding into the scalar components of the multivectors, we add an auxiliary scalar representation to the hidden states of GATr. Each layer thus has both scalar and multivector inputs and outputs. They have the same batch dimension and item dimension, but may have different number of channels.

This additional scalar information interacts with the multivector data in two ways. In linear layers, we allow the auxiliary scalars to mix with the scalar component of the multivectors. In the attention layer, we compute attention weights both from the multivectors, as given in Eq. (5), and from the auxiliary scalars, using a regular scaled dot-product attention. The two attention maps are summed before computing the softmax, and the normalizing factor is adapted. In all other layers, the scalar information is processed separately from the multivector information, using the unrestricted form of the multivector map. For instance, nonlinearities transform multivectors with equivariant gated GELUs and auxiliary scalars with regular GELU functions. We describe the scalar path of our architecture in more detail in Appendix B.

Distance-aware dot-product attentionThe dot-product attention in Eq. (5) ignores the 8 dimensions involving the basis element \(e_{0}\). These dimensions vary under translations, and thus their straightforward Euclidean inner product violates equivariance. We can, however, extend the attention mechanism to incorporate more components, while still maintaining \((3)\) equivariance and the computational efficiency of dot-product attention. To this end, we define certain auxiliary, nonlinear query features \((q)\) and key features \((k)\) and extend the attention weights in Eq. (5) as \( q_{i^{}c},k_{ic} q_{i^{}c},k_{ic} +(q_{i^{}c})(k_{ic})\), adapting the normalization appropriately. We define these nonlinear features in Appendix B.

Our choice of these nonlinear features not only maintains equivariance, but has a straightforward geometric interpretation. When the trivector components of queries and keys represent 3D points (see Tbl. 1), \((q)(k)\) is proportional to the pairwise negative squared Euclidean distance. GATr's attention mechanism is therefore directly sensitive to Euclidean distance, while still respecting the highly efficient dot-product attention format.

Positional embeddingsGATr assumes the data can be described as a set of items (or tokens). If these items are distinguishable and form a sequence, we encode their position using "rotary positional"9 embeddings  in the auxiliary scalar variables.10

Axial attentionThe architecture is flexible about the structure of the data. In some use cases, there will be a single dimension along which objects are organized, for instance when describing a static scene or the time evolution of a single object. But GATr also supports the organization of a problem along multiple axes, for example with one dimension describing objects and another time steps. In this case, we follow an axial transformer layout , alternating between transformer blocks that attend over different dimensions. (The not-attended dimensions in each block are treated like a batch dimension.)

## 4 Experiments

### \(n\)-body dynamics

We start our empirical demonstration of GATr with an \(n\)-body dynamics problem, on which we compare GATr to a wide range of baselines. Given the masses, initial positions, and velocities of a star and a few planets, the goal is to predict the final position after the system has evolved under Newtonian gravity for 1000 Euler integration time steps. We compare GATr to a Transformer and an MLP, the equivariant SEGNN  and \((3)\)-Transformer , as well as the geometric-algebra-based--but not equivariant--GCA-GNN . The experiment is described in detail in Appendix C.1.

In the left panel of Fig. 2 we show the prediction error as a function of the number of training samples used. GATr clearly outperforms all non-equivariant baselines, including the geometric-algebra-based GCA-GNN. Compared to the equivariant SEGNN and \((3)\)-Transformer, GATr is more sample

Figure 2: \(n\)-body dynamics experiments. We show the error in predicting future positions of planets as a function of the training dataset size. Out of five independent training runs, the mean and standard error are shown. **Left**: Evaluating without distribution shift. GATr ()is more sample efficient than the equivariant SEGNN  () and \((3)\)-Transformer  () and clearly outperforms non-equivariant baselines (). **Middle**: Evaluating on systems with more planets than trained on. Transformer architectures generalize well to different object counts. GCA-GNN has larger errors than the visible range. **Right**: Evaluating on translated data. Because GATr is \((3)\) equivariant, it generalizes under this domain shift.

efficient, while all three methods reach the same prediction error when trained on enough data. This provides evidence for the usefulness of geometric algebra representations as an inductive bias.

GATr also generalizes robustly out of domain, as we show in the middle and right panels of Fig. 1. When evaluating on a larger number of bodies than trained on, methods that use a softmax over attention weights (GATr, Transformer, \((3)\)-Transformer) generalize best. Finally, the performance of the \((3)\)-equivariant GATr, SEGNN, and \((3)\)-Transformer does not drop when evaluated on spatially translated data, while the non-equivariant baselines fail in this setting.

### Wall-shear-stress estimation on large meshes of human arteries

Next, we turn towards a realistic experiment involving more complex geometric objects. We study the prediction of the wall shear stress exerted by the blood flow on the arterial wall, an important predictor of aneurysms. While the wall shear stress can be computed with computational fluid dynamics, simulating a single artery can take many hours, and efficient neural surrogates can have substantial impact. However, training such neural surrogates is challenging, as meshes are large (around 7000 nodes in our data) and datasets typically small (we work with 1600 training meshes).

We train GATr on a dataset of arterial meshes and simulated wall shear stress published by Suk et al. . They are compared to a Transformer and to the results reported by Suk et al. , including the equivariant GEM-CNN  and the non-equivariant PointNet++ .11 See Appendix C.2 for experiment details.

The results are shown in Fig. 3. On non-canonicalized data, with randomly rotated meshes, GATr improves upon all previous methods and sets a new state of the art.

We also experiment with canonicalization: rotating the arteries such that blood always flows in the same direction. This helps the Transformer to be almost competitive with GATr. However, canonicalization is only feasible for relatively straight arteries as in this dataset, not in more complex scenarios with branching and turning arteries. We find it likely that GATr will be more robust in such scenarios.

### Robotic planning through invariant diffusion

In our third experiment, we show how GATr defines an \((3)\)-invariant diffusion model, that it can be used for model-based reinforcement learning and planning, and that this combination is well-suited to solve robotics problems. We follow Janner et al. , who propose to treat learning a world model and planning within that model as a unified generative modeling problem. After training a diffusion model  on offline trajectories, one can use it in a planning loop, sampling from it conditional on the current state, desired future states, or to maximize a given reward, as needed.

We use a GATr model as the denoising network in a diffusion model and to use it for planning. We call this combination _GATr-Diffuser_. Combining the equivariant GATr with an invariant base density defines an \((3)_{n}\)-invariant diffusion model .

GATr-Diffuser is demonstrated on the problem of a Kuka robotic gripper stacking blocks using the "unconditional" environment introduced by Janner et al. . We train GATr-Diffuser on the offline

Figure 3: Arterial wall-shear-stress estimation . We show the mean approximation error (lower is better) as a function of training dataset size, reporting results both on randomly oriented training and test samples (solid markers) and on a version of the dataset in which all artery meshes are canonically oriented (hollow markers). Without canonicalization, GATr () predicts wall shear stress more precisely and is more sample-efficient than the baselines.

trajectory dataset published with that paper and then use it for planning, following the setup of Janner et al. . We compare our GATr-Diffuser model to a reproduction of the original Diffuser model and a new Transformer backbone for the Diffuser model. In addition, we show the published results of Diffuser , the equivariant EDGI , and the offline RL algorithms CQL  and BCQ  as published in Ref. . The problem and hyperparameters are described in detail in Appendix C.3.

As shown in Fig. 4, GATr-Diffuser solves the block-stacking task better than all baselines. It is also clearly more sample-efficient, matching the performance of a Diffuser model or Transformer trained on the full dataset even when training only on 1% of the trajectories.

### Scaling

Finally, we study GATr's computational requirements and scaling. We measure the memory usage and compute time of forward and backward passes on synthetic data as a function of the number of items. GATr is compared to a Transformer, to SEGNN  in the official implementation, and to the \((3)\)-Transformer ; for the latter we use the highly optimized implementation by Milesi . We choose hyperparameters for the four architectures such that they have the same depth and width and require that the methods allow all items to interact at each step (i. e. fully connected graphs). Because of the fundamental differences between the architectures, it is impossible to find fully equivalent settings; our results should thus be interpreted with care. The details of our experiment are described in Appendix C.4.

We show the results in Fig. 5. For few tokens, GATr is slower than a Transformer. However, this difference is partially due to the low utilization of the GPUs in this test; GATr is closer to the Transformer when using larger batch sizes or when pre-compiling the computation graph.

For larger problems, compute and memory are dominated by the pairwise interactions in the attention mechanism. Here GATr and the baseline Transformer perform on par, as they use the same efficient implementation of dot-product attention. In terms of memory, both scale linearly in the number of tokens, while the equivariant baselines scale quadratically. In terms of time, all methods scale quadratically, but the equivariant baselines have a worse prefactor than GATr and the Transformer. All in all, we can easily scale GATr to tens of thousands of tokens, while the equivariant baselines run out of memory two orders of magnitude earlier.

## 5 Related work

Geometric algebraGeometric (or Clifford) algebra was first conceived in the 19th century [10; 22] and has been used widely in quantum physics [16; 34]. Recently, it has found new popularity in computer graphics . In particular,

Figure 4: Diffusion-based robotic planning. We show normalized rewards (higher is better) as a function of training dataset size. GATr () is more successful at block stacking and more sample-efficient than the baselines, including the original Diffuser  () and our modification of it based on a Transformer (). In grey, we show results reported by Brehmer et al.  and Janner et al. .

Figure 5: Computational cost and scaling. We measure peak GPU memory usage (top) and wall time (bottom) per combined forward and backward pass as a function of the number of items in synthetic data. Despite some compute overhead, GATr () scales just like the Transformer () and orders of magnitude more favorably than the equivariant baselines ().

the projective geometric algebra used in this work [17; 38] and a conformal model  are suitable for 3D computations.

Geometric algebras have been used in machine learning in various ways. Spellings  use \(_{3,0,0}\) geometric products to compute rotation-invariant features from small point clouds. Unlike us, they do not learn internal geometric representations.

Our work was inspired by Brandstetter et al.  and Ruhe et al. . These works also use multivector features (the latter even of the same \(_{3,0,1}\)), and process them with operations such as the geometric / sandwich product, Clifford Fourier transforms and Clifford convolutions. The main difference to our work is that GATr is \((3)\) equivariant, while both of these works are not. We compare to the GCA-GNN network from Ref.  in our experiments.

Concurrently to this publication, Ruhe et al.  also study equivariant, geometric algebra-based architectures. While some of our and their findings overlap, there are several differences: They develop theory for generic geometric algebras of the form \(_{p,q,r}\), while we focus on the projective algebra \(_{3,0,1}\) with its faithful \((3)\) representations, with our theory also applicable to the group \(E(n)\) and the algebras \(_{n,0,0}\) and \(_{n,0,1}\). Ruhe et al.  also finds the linear maps of Eq. (4), but does not discuss the join or distance-aware dot-product, which we found to be essential for performance in the projective algebra. Moreover, they propose an MLP-like architecture and use it in a message-passing graph network, while our GATr is a Transformer.

Equivariant deep learningEquivariance to symmetries  is the primary design principle in modern geometric deep learning . Equivariant networks have been applied successfully in areas such as medical imaging [31; 53] and robotics [7; 26; 44; 51; 52; 55], and are ubiquitous in applications of machine learning to physics and chemistry [1; 3; 4; 19; 28; 33].

In recent years, a number of works have investigated equivariant Transformer and message-passing architectures [3; 4; 6; 19; 20; 39; 42; 49]. These works are generally more limited in terms of the types of geometric quantities they can process compared to our multivector features. Furthermore, our architecture is equivariant to the full group of Euclidean transformations, whereas previous works focus on \((3)\) equivariance.

## 6 Discussion

We introduced the Geometric Algebra Transformer (GATr), a general-purpose architecture for geometric data, and implemented it at https://github.com/qualcomm-ai-research/geometric-algebra-transformer. We argued and demonstrated that GATr effectively combines structure and scalability.

GATr incorporates geometric _structure_ by representing data in projective geometric algebra, as well as through \((3)\) equivariance. Unlike most equivariant architectures, GATr features faithful \((3)\) representations, including absolute positions and equivariance with respect to translations. Empirically, GATr outperforms non-geometric, equivariant, and geometric algebra-based non-equivariant baselines across three experiments.

At the same time, GATr _scales_ much better than most geometric networks. This is because GATr is a Transformer and computes pairwise interactions through dot-product attention. Using recent efficient attention implementations, we demonstrated that we can scale GATr to systems with many thousands of tokens and fully connected interactions. For few tokens and small batch sizes, GATr has some computational overhead, which we hope to address in future implementations.

One drawback of our approach is that since geometric algebra is not widely known yet, it may present an obstacle to understanding the details of the method. However, given a dictionary for embeddings of common objects and a library of primitives that act on them, _using_ this framework is no more difficult than using typical neural network layers grounded in linear algebra. Another potential downside is that GATr is not yet shown to be a universal approximator, which is an interesting direction for future work.

Given the promising results presented in this work, we look forward to further study the potential of Geometric Algebra Transformers in problems from molecular dynamics to robotics.

AcknowledgementsWe would like to thank Joey Bose, Johannes Brandstetter, Gabriele Cesa, Steven De Keninck, Daniel Dijkman, Leo Dorst, Mario Geiger, Jonas Kohler, Ekdeep Singh Lubana, Evgeny Mironov, and David Ruhe for generous guidance regarding geometry, enthusiastic encouragement on equivariance, and careful counsel concerning computing complications.