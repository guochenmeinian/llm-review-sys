# Sourcerer: Sample-based Maximum Entropy Source Distribution Estimation

Julius Vetter\({}^{,1,2,}\)

Guy Moss\({}^{,1,2,}\)

G. Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.Moss\({}^{,1,2,}\)

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){firstname.secondname}@uni-tuebingen.de

\({}^{}\){G.

a distribution \(q()\) over parameters that, once passed through the simulator, yields a "pushforward" distribution of simulations \(q^{\#}(x)= p(x|)q()d\) that is indistinguishable from the empirical distribution. This setting is known by different names in different disciplines, for example as _unfolding_ in high energy physics , _stochastic inverse problems_ in various disciplines , _population of models_ in electrophysiology  and _population inference_ in gravitational wave astronomy . Adopting the terminology of Vandegar et al. , we refer to this task as _source distribution estimation_.

A common approach to source distribution estimation is empirical Bayes [51; 15]. Empirical Bayes uses hierarchical models in which each observation is modeled as arising from different parameters \(p(x_{i}|_{i})\). The hyper-parameters of the prior (and thus the source \(q_{}\)) are found by optimizing the marginal likelihood \(p(D)=_{i} p(x_{i}|)q_{}()d\) over \(\). Empirical Bayes has been successfully applied to a range of applications [31; 32; 55]. However, empirical Bayes is typically not applicable to models with intractable likelihoods, which is usually the case for scientific simulators. Using surrogate models for such likelihoods, empirical Bayes has been extended to increasingly more complicated parameterizations \(\) of the source distribution, including neural networks [59; 58].

A more general issue, however, is that the source distribution problem can often be ill-posed without the introduction of a hyper-prior or other regularization principles, as also noted in Vandegar et al. : Distinct source distributions \(q()\) can give rise to the same data distribution \(q^{\#}(x)\) when pushed through the simulator \(p(x|)\) (Fig. 1, illustrative example in Appendix A.7).

We here propose to use the maximum entropy principle, i.e., choosing the "maximum ignorance" distribution within a class of distributions to resolve the ill-posedness of the source distribution problem [19; 24]. The maximum entropy principle formalizes the notion that a good choice for distributions should "assume less". It has been applied to specific source distribution estimation problems in scientific disciplines such as cosmology  and high-energy physics .

Our contributionsWe introduce _Sourcerer_, a general method for source distribution estimation, providing two key innovations: First, we target the maximum entropy source distribution to obtain a well-posed problem, thereby increasing the entropy of the estimated source distributions at no cost to their fidelity. Second, we use general distance metrics between distributions, in particular the Sliced-Wasserstein distance, instead of maximizing the marginal likelihood as in empirical Bayes. This allows evaluation of the objective using _only samples_ from differentiable simulators, removing the requirement to have tractable likelihoods. We validate our method on multiple tasks, including tasks with high-dimensional observation space, which are challenging for likelihood-based methods. Finally, we apply our method to estimate the source distribution over the mechanistic parameters of the Hodgkin-Huxley model from a large (\( 1000\) samples) dataset of electrophysiological recordings.

## 2 Methods

We formulate the source distribution estimation problem in terms of the maximum entropy principle. The (differential) entropy \(H(p)\) of a distribution \(p()\) is defined as

\[H(p)=- p() p()d.\] (1)

Figure 1: **Maximum entropy source distribution estimation.** Given an observed dataset \(=\{x_{1},,x_{n}\}\) from some data distribution \(p_{o}(x)\), the _source distribution estimation_ problem is to find the parameter distribution \(q()\) that reproduces \(p_{o}(x)\) when passed through the simulator \(p(x|)\), i.e. \(q^{\#}(x)= p(x|)q()d=p_{o}(x)\) for all \(x\). This problem can be ill-posed, as there might be more than one distinct source distribution. We resolve this by targeting the maximum entropy distribution, which is unique.

### Data-consistency and regularized objective

For a given distribution \(q()\) and a simulator with (possibly intractable) likelihood \(p(x|)\), the _pushforward_ of \(q\) is given by \(q^{\#}(x)= p(x|)q()d\). The distribution \(q()\) is a source distribution if its pushforward matches the observed data distribution \(p_{o}(x)\), that is, \(q^{\#}=p_{o}\) almost everywhere. Equivalently, given a distance metric \(D(,)\) between probability distributions \(P()\) over the data space \(\), a source distribution \(q\) is one which satisfies \(D(q^{\#},p_{o})=0\). In general, for a given distribution of observations \(p_{o}(x)\) and likelihood \(p(x|)\), the source distribution problem is ill-posed as there are possibly many different source distributions. The maximum entropy principle can be employed to resolve this ill-posedness:

**Proposition 2.1**.: _Let \(Q=\{q|q^{\#}=p_{o}\}\) be the set of source distributions for a given likelihood \(p(x|)\) and data distribution \(p_{o}\). Suppose that \(Q\) is non-empty and compact. Then \(q^{*}=_{q Q}H(q)\) exists and is unique._

This proposition follows from the fact that the set of source distributions is convex and that the (differential) entropy \(H(q)\) is a strictly concave functional. See Appendix A.7 for a proof and additional assumptions.

Proposition 2.1 suggests to solve the constrained optimization problem

\[_{} H(q_{}) D(q_{}^{\#},p_{o})=0,\] (2)

where \(q_{}\) is some parametric family of distributions.

Practically, however, a solution might not exist, for example due to simulator misspecification. Furthermore, even if a solution exists, it is difficult to obtain since we only have a fixed number of samples from \(p_{o}\) and can thus only estimate \(D(q_{}^{\#},p_{o})\). We therefore propose a _regularized_ approximation of Eq. (2) and solve

\[_{} H(q_{})-(1-)(D(q_{}^{\#},p_{o}))\] (3)

instead, where \(\) is a parameter determining the strength of the data-consistency term and the logarithm is added for numerical stability. This regularized objective is related to the Lagrangian relaxation of Eq. (2), where now \( D(q^{\#},p_{o})\) for some \(>0\) and the dual variable is \((1-)/\).

For \( 1\), the loss in Eq. (3) is dominated by the entropy term, and for \( 0\) by the data-consistency term. We apply ideas from constrained optimization and reinforcement learning  and use a dynamical schedule during training. We initialize training with \(_{t=1}=1\), and decay this value linearly to a final value \(_{t=T}=>0\) over the course of training. This dynamical schedule encourages the variational source model to first explore high-entropy distributions, and later increase consistency with the data between high-entropy distributions. Pseudocode and details of the schedule in Appendix A.3.

### Reference distribution

For many tasks, there is an additional constraint in terms of a reference distribution \(p()\). For example, in the Bayesian inference framework, it is common to have a prior distribution \(p()\), encoding existing knowledge about the parameters \(\) from previous studies. In such cases, a distribution with higher entropy than \(p()\), even if it is a source distribution, is not always desirable. We therefore adapt our objective function in Eq. (3) to minimize the Kullback-Leibler (KL) divergence between the source \(q()\) and the reference \(p()\):

\[_{} D_{KL}(q||p)+(1-)(D(q^{\#},p_{o})).\] (4)

Figure 2: **Overview of Sourcerer.** Given a source distribution \(q()\), we sample \( q\) and simulate using \(p(x|)\) to obtain samples from the pushforward distribution \(q^{\#}(x)= p(x|)q()d\). We maximize the entropy of the source distribution \(q()\) while regularizing with a Sliced-Wasserstein (SWD) term between the pushforward of \(q^{\#}\) and the data distribution \(p_{o}(x)\) (Eq. (3)). \(\) and \(\) in top right corner of boxes denote parameter space and data/observation space, respectively.

The KL divergence term can be rewritten as \(D_{KL}(q||p)=-H(q)+H(q,p)\), where \(H(q,p)=-(p())q()d\) is the cross-entropy between \(q\) and \(p\). Thus, provided we can evaluate the density \(p()\), we can obtain a sample-based estimate of the loss in Eq. (4). In our work, we consider \(p()\) to be the uniform distribution over some bounded domain \(B_{}\) (and hence the maximum entropy distribution on this domain). This "box prior" is often used as the naive estimate from literature observations in inference studies. More specifically, in this case, \(H(q,p)=-1/|B_{}|\), where \(|B_{}|\) is the volume of \(B_{}\). Therefore, it is independent of \(q\), and hence minimizing the KL divergence is equivalent to maximizing \(H(q)\) on \(B_{}\). In the case where \(p()\) is non-uniform (e.g., Gaussian) the cross-entropy term regularizes the loss by penalizing large \(q()\) when \(p()\) is small.

### Sliced-Wasserstein as a distance metric

We are free to choose any distance metric \(D(,)\) for the loss function Eq. (4). In this work, we use the fast, sample-based, and differentiable Sliced-Wasserstein distance (SWD) [6; 27; 42] of order two. The SWD is defined as the expected value of the one-dimensional Wasserstein distance between the projections of the distribution onto uniformly random directions \(u\) on the unit sphere \(^{d-1}\) in \(^{d}\). More precisely, the SWD is defined as

\[_{m}(p,q)=_{u(^{d-1})}[W_{m}(p_{u },q_{u})]\,,\] (5)

where \(p_{u}\) is the one-dimensional distribution with samples \(u^{}x\) for \(x p(x)\), and \(W_{m}\) is the one-dimensional Wasserstein distance of order \(m\). In the empirical setting, where we are given \(n\) samples each from \(p_{u}\) and \(q_{u}\) respectively, the one-dimensional Wasserstein distance is computed from the order statistics as

\[W_{m}(p_{u},q_{u})=(_{i=1}^{n}||x_{p}^{(i)}-x_{q}^{(i)}||_{m}^{m} )^{1/m},\] (6)

where \(x_{p}^{(i)}\) denotes the \(i\)-th order statistic of the samples from \(p_{u}\) (and similarly for \(x_{q}^{(i)}\)), and \(||||_{m}\) denotes the \(L^{m}\) distance on \(\). The time complexity of computing the sample-based one-dimensional Wasserstein distance is thus the time complexity of computing the order statistics, which is \((n n)\) in the number of datapoints \(n\). This is significantly faster than computing the multi-dimensional Wasserstein distance (\((n^{3})\), 29), or the commonly used Sinkhorn algorithm for approximating the Wasserstein distance (\((n^{2})\) 47). While the SWD is not the same as the multi-dimensional Wasserstein distance, it is still a valid metric on the space of probability distributions. In particular, the SWD converges quickly with rate \(O()\) to its true value [41; 42].

### Differentiable simulators and surrogates

Our method only requires that sampling from the simulator \(p(x|)\) is a differentiable operation. In practice, however, many simulators do not satisfy this property. For such simulators, we first train a surrogate model. In particular, our method can make use of surrogates that model the likelihood only implicitly. Such surrogate models can be easier to train and evaluate in practice. This is a distinct requirement from likelihood-based approaches such as Vandegar et al. , which require that the likelihood \(p(x|)\) can be evaluated explicitly _and_ is differentiable. This means that our sample-based approach can be readily applied to a larger set of simulators than likelihood-based approaches.

### Source model and entropy estimation

In this work we use neural samplers as proposed in Vandegar et al.  to parameterize a source model \(q_{}\). These samplers employ unconstrained neural network architectures (in our case a multi-layer perceptron) to transform a random sample from \(z(0,I)\) into a sample from \(q_{}\). While neural samplers do not have a tractable likelihood, they are faster to evaluate than models with tractable likelihoods. Furthermore, by using unconstrained network architectures, neural samplers are flexible and additional constraints (e.g., symmetry, monotonicity) are easy to introduce.

To use likelihood-free source parameterizations, we require a purely sample-based estimator for the entropy \(H(q_{})\). This can be done using the _Kozachenko-Leonenko_ entropy estimator [28; 3], which is based on a nearest-neighbor density estimate. We use the Kozachenko-Leonenko estimator in this work for its simplicity, but note that sample-based entropy estimation is an active area of research, and other choices are possible . Details about the Kozachenko-Leonenko estimator can be found in Appendix A.6.

## 3 Experiments

To evaluate the data-consistency and entropy of source distributions estimated by Sourcerer, we benchmark our method against Neural Empirical Bayes (NEB) , a state-of-the-art approach to source distribution estimation. The benchmark comparison is performed on four source distribution estimation tasks including three presented in Vandegar et al. . We then demonstrate the advantage of Sourcerer in the case of differentiable simulators with a high-dimensional data domain, where likelihood-based empirical Bayes approaches would require training a likelihood surrogate. Finally, we use Sourcerer to estimate the source distribution for a Hodkgin-Huxley simulator of single-neuron voltage dynamics from a large dataset of experimental electrophysiological recordings. For all tasks except the Hodgkin-Huxley task (where the observed dataset is experimentally measured), we generate two datasets of observations of equal size from the same reference source distribution. The first is used to train the source model, and the second is used to evaluate the quality of the learned source.

### Source Estimation Benchmark

Benchmark tasksThe source estimation benchmark contains four simulators: two moons (TM), inverse kinematics (IK), simple likelihood complex posterior (SLCP), and Gaussian Mixture (GM) (details about simulators and source distributions are in Appendix A.2). Notably, all four simulators are differentiable. Therefore, we can evaluate our method directly on the simulator as well as trained surrogates. For all four simulators, source estimation is performed on a synthetic dataset of 10000 observations that were generated by sampling from a pre-defined original source distribution and evaluating the resulting pushforward distribution using the corresponding simulator. The quality of the estimated source distributions is measured using a classifier two sample test (C2ST)  between the observations and simulations from the source. We also report the entropy of the estimated sources. Given two sources with the same C2ST accuracy, the higher entropy source is preferable. We compare

Figure 3: **Results for the source estimation benchmark.****(a)** Original and estimated source and corresponding pushforward for the differentiable IK simulator (\(=0.35\)). The estimated source has higher entropy than the original source that was used to generate the data. The observations (simulated with parameters from the original source) and simulations (simulated with parameters from the estimated source) match. **(b)** Performance of our approach for all four benchmark tasks (TM, IK, SLCP, GM) using both the original (differentiable) simulators, and learned surrogates. Source estimation is performed without (NA) and with entropy regularization for different choices of \(\). For all cases, mean C2ST accuracy between observations and simulations (lower is better) as well as the mean entropy of estimated sources (higher is better) over five runs are shown together with the standard deviation. The gray line at \(=0.35\) (\(=0.062\) for GM) indicates our choice of final \(\) for the numerical benchmark results (Table 1).

[MISSING_PAGE_FAIL:6]

highlight this capability of our method by estimating source distributions for two high-dimensional, differentiable simulators: The Lotka-Volterra model and the SIR (Susceptible, Infectious, Recovered) model. The Lotka-Volterra model is used to model the density of two populations, predators and prey. The SIR model is commonly used in epidemiology to model the spread of disease in a population (details about both models and source distributions in Appendix A.2). Compared to the benchmark tasks in Sec. 3.1, the dimensionality of the data space is much larger: Both the Lotka-Volterra and the SIR model are simulated for 50 time points resulting in a 100 and 50 dimensional time series, respectively.

Furthermore, to show that unlike NEB (which maximizes the marginal likelihood), our sample-based approach is applicable to deterministic simulators, we use a deterministic version of the SIR model with no observation noise. Similarly to the benchmark tasks, we define a source, and simulate 10000 observations using samples from this source to define a synthetic dataset on which to perform source distribution estimation. Here, we directly evaluate the quality of the estimated source distributions using the Sliced-Wasserstein distance. We compare this distance to the minimum expected distance, which is the distance between simulations of different sets of samples from the same original source. For a comparison with NEB, we train surrogate models with a reduced dimensionality and again compute C2ST accuracies and entropies of the estimated sources (see Appendix A.5 and Fig. A3 for details on surrogate training and pushforward plots).

Source estimation for the deterministic SIR modelOur method is able to estimate a good source distribution for the deterministic SIR model: The Sliced-Wasserstein distance between simulations and observations is close to the minimum expected distance (Fig. 4a). In contrast to the benchmark tasks, estimating sources with entropy regularization does not lead to an increase in entropy for the SIR model, and the quality of the estimated source remains constant for various choices of \(\). A possible explanation for this is that there is no degeneracy in the parameter space of the deterministic simulator, and there exists only one source distribution.

Source estimation for the probabilistic Lotka-Volterra modelFor the probabilistic Lotka-Volterra model, our method is also capable of estimating source distributions. As for the SIR model, the Sliced-Wasserstein distance between simulations and observations is close to the minimum expected distance (Fig. 4b). However, unlike the SIR model, estimating the source with entropy regularization yields a large increase in entropy compared to when not using the regularization. For the Lotka-Volterra model, our method yields a substantially higher entropy at no additional cost in terms of source quality.

When using the surrogate models with reduced dimensionality to estimate the source distributions, we find that Sourcerer achieves better C2ST accuracies than NEB. Furthermore, for the Lotka-Volterra model, the entropy regularization again leads to a substantial increase in the entropy of the estimated sources (Table 2). In summary, the experiments on the SIR and Lotka-Volterra models show that our approach is able to scale to higher dimensional problems and can use gradients of complex simulators to estimate source distributions directly from a set of observations.

Figure 4: **Source estimation on differentiable simulators.** For both the deterministic SIR model **(a)** and probabilistic Lotka-Volterra model **(b)**, the Sliced-Wasserstein distance (lower is better) between observations and simulations as well as entropy of estimated sources (higher is better) for different choices of \(\) and without the entropy regularization (NA) are shown. Mean and standard deviation are computed over five runs.

### Estimating source distributions for a single-compartment Hodgkin-Huxley model

Single-compartment Hodgkin-Huxley simulator and summary statisticsThe single-compartment Hodgkin-Huxley model consists of a system of coupled ordinary differential equations simulating different ion channels in a neuron. We use the simulator described in Bernaerts et al.  with 13 parameters. In data space, we use five commonly used summary statistics of the observed and simulated spike trains. These are the (log of the) number of spikes, the mean of the resting potential, and the mean, variance and skewness of the voltage during external current stimulation. As the internal noise in the simulator has little effect on the summary statistics, we train a simple multi-layer perceptron as surrogate on \(10^{6}\) simulations. The parameters used to generate these training simulations were sampled from a uniform distribution that was used as the prior in Bernaerts et al.  (details on simulator, choice of surrogate and the surrogate training in Appendix A.9).

Using this surrogate, we estimate source distributions from a real-world dataset of electrophysiological recordings. The dataset  consists of 1033 electrophysiological recordings from the mouse motor cortex. In general, parameter inference for Hodgkin-Huxley models can be challenging as models are often misspecified [56; 2]. Thus, estimating the source distribution for this task is useful for downstream inference tasks, as the prior knowledge gained can significantly constrain the parameters of interest.

Source estimation for the Hodgkin-Huxley modelOn visual inspection, simulations from the estimated source look similar to the original recordings (all observations spike at least once, spikes have similar magnitudes) and show none of the unrealistic properties (e.g., spiking before the stimulus is applied) that can be observed in some of the box uniform prior simulations (Fig. 5a). This match is also confirmed by the distribution of summary statistics, which match closely between simulations and observations (Fig. 5b). Furthermore, our method achieves good C2ST accuracy of \( 61\%\) for different choices of \(\) (Fig. 5d), as well as a small Sliced-Wasserstein distance of \( 0.08\) in the standardized space of summary statistics (Fig. 5e). While the source estimated without entropy regularization also achieves good fidelity, its entropy is significantly lower than any of the source distributions estimated with entropy regularization (Fig. 5d/e, example source distribution in Fig. 5c, full source in Fig. A11).

Overall, these results demonstrate the importance of estimating source distributions using the entropy regularization, especially on real-world datasets: Estimating the source distribution without any entropy regularization can introduce severe bias, since the estimated source may ignore entire regions of the parameter space. In this example, the parameter space of the single-compartment Hodgkin-Huxley model is known to be highly degenerate, and a given observation can be generated by multiple parameter configurations [14; 39].

    &  &  &  &  \\  &  &  &  &  & \\   & C2ST acc. & 0.56 (0.013) & 0.56 (0.015) & 0.55 (0.005) & 0.55 (0.005) & 0.76 (0.024) \\  & Entropy & -2.3 (0.079) & -2.37 (0.169) & -2.29 (0.076) & -2.5 (0.05) & -0.63 (0.174) \\   & C2ST acc. & 0.57 (0.009) & 0.52 (0.001) & 0.56 (0.005) & 0.54 (0.009) & 0.62 (0.011) \\  & Entropy & **0.29** (0.017) & -1.34 (0.087) & **0.34** (0.05) & -1.01 (0.13) & -1.28 (0.073) \\   

Table 2: **Numerical results for the SIR and Lotka-Volterra model** We show the mean and standard deviation over five runs for differentiable simulators and surrogates of Sourcerer on the high-dimensional SIR and Lotka-Volterra (LV) models, and compare to NEB. For the comparison with NEB, we train the required surrogate models with reduced dimensionality (25 dimensions instead of 50 or 100). Sourcerer achieves C2ST accuracies close to 50%. For NEB, the C2ST accuracies are worse. For the LV model, the entropies of the estimated sources are higher with the entropy regularization (\(=0.015\) for SIR, \(=0.125\) for LV).

## 4 Related Work

Neural Empirical BayesHigh-dimensional source distributions have been estimated through variational approximations to the empirical Bayes problem. Louppe et al.  train a generative adversarial network (GAN) \(q_{}\) to approximate the source. The use of a discriminator to compute an implicit distance makes this approach purely sample-based as well. In order to find the optimal \(^{*}\) of the true data-generating process, they augment the adversarial loss with a small entropy penalty on the source \(q_{}\). This penalty encourages low entropy, point mass distributions, which is the _opposite_ of our approach. Vandegar et al.  take an empirical Bayes approach, and use normalizing flows for both the variational approximation of the source and as a surrogate for the likelihood \(p(x|)\). This allows for direct regression on the marginal likelihood, as all likelihoods can be computed directly. Finally, the empirical Bayes problem is also known as "unfolding" in the particle physics literature , "population inference" in gravitational wave astronomy , and "population of models" in electrophysiology . Approaches have been developed to identify the source distribution, including classical approaches that seek to increase the entropy of the learned sources .

Simulation-Based InferenceThe use of variational surrogates of the likelihood of a simulator with intractable likelihood is known as _Neural Likelihood Estimation_ in the simulation-based inference (SBI) literature [60; 45; 36; 11]. In neural posterior estimation [44; 35; 21], an _amortized_ posterior density estimate is learned, which can be applied to evaluate the posterior of a single observation \(x_{i}\), if a prior distribution \(p()\) is already known. An intuitive but incorrect approach to source distribution estimation would be to take the _average posterior_ distribution over the observations \(\),

\[G_{n}()=_{i=1}^{n}p(|x_{i}).\] (7)

The average posterior does not always (and typically does not) converge to a source distribution in the infinite data limit, as shown for simple examples in Appendix A.8. Intuitively, the average posterior becomes a worse approximation of a source distribution for simulators that have broader likelihoods. Instead, SBI can be seen as a downstream task of source distribution estimation; once a prior has been learned from the dataset of observations with source estimation, the posterior can be estimated for each new observation individually.

Figure 5: **Source estimation for the single-compartment Hodgkin-Huxley model.****(a)** Example voltage traces of the real observations of the motor cortex dataset, simulations from the estimated source (\(=0.25\)), and samples from the uniform distribution used to train the surrogate. **(b)** 1D and 2D marginals for three of the five summary statistics used to perform source estimation. **(c)** 1D and 2D marginal distributions of the estimated source for three of the 13 simulator parameters. **(d)** and **(e)** C2ST accuracy and Sliced-Wasserstein distance (lower is better) as well as entropy of estimated sources (higher is better) for different choices of \(\) including \(=0.25\) (gray line) and without entropy regularization (NA). Mean and standard deviation over five runs are shown.

Generalized Bayesian InferenceAnother field related to source estimation is Generalized Bayesian Inference (GBI) [5; 40; 26]. GBI performs distance-based inference, as opposed to targeting the exact Bayesian posterior. Similarly to our work, the distance function used in GBI can be arbitrarily chosen for different tasks. However, GBI is used for single-parameter inference tasks, as opposed to the source distribution estimation task considered in this work. Similarly, Bayesian non-parametric methods [43; 38; 12] learn a posterior directly on the data space which can then be used to sample from a posterior distribution over the parameter space.

## 5 Summary and Discussion

In this work, we introduced Sourcerer as a method to estimate source distributions of simulator parameters given datasets of observations. This is a common problem setting across a range of scientific and engineering disciplines. Our method has several advantages: first, we employ a maximum entropy approach, improving reproducibility of the learned source, as the maximum entropy source distribution is unique while the traditional source distribution estimation problem can be ill-posed. Second, our method allows for sample-based optimization. In contrast to previous likelihood-based approaches, this scales more readily to higher dimensional problems, and can be applied to simulators without a tractable likelihood. We demonstrated the performance of our approach across a diverse suite of tasks, including deterministic and probabilistic simulators, differentiable simulators and surrogate models, low- and high-dimensional observation spaces, and a contemporary scientific task of estimating a source distribution for the single-compartment Hodgkin-Huxley model from a dataset of electrophysiological recordings. Throughout our experiments, we have consistently found that our approach yields higher entropy sources without reducing the fidelity of simulations from the learned source.

LimitationsIn this work, we used the Sliced-Wasserstein distance (and MMD) for the data-consistency term between simulations and observations. In practice, different distance metrics can lead to different estimated sources, depending on its sensitivity to different features. While our method is compatible with any sample-based differentiable distance metric between two distributions, there is still an onus on the practitioner to carefully select a reasonable distance metric for the data at hand. For example, in some cases, it might be appropriate to use a combination of several distance metrics for different modalities of the data. Similarly, there is a dependence on the final regularization strength \(\). Principled methods for defining the regularization strength are desirable, though as we demonstrate, our results are robust to a large range of \(\).

In addition, the method requires a differentiable simulator, which in practice may require the training of a surrogate model, for example, when dealing with a (partially) discrete simulator. While this is a common requirement for simulation-based methods, this could present a challenge for some applications. Finally, in our work, we enforce the maximum entropy principle on the entire (parameter) source distribution. In practice, for example when constructing prior distributions for Bayesian inference, there are other choices, such as the Jeffrey's prior .