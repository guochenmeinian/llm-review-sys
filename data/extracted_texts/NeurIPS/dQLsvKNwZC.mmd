# Safe Exploration in Reinforcement Learning:

A Generalized Formulation and Algorithms

 Akifumi Wachi

LINE Corporation

akifumi.wachi@linecorp.com

&Wataru Hashimoto

Osaka University

hashimoto@is.eei.eng.osaka-u.ac.jp

&Xun Shen

Osaka University

shenxun@eei.eng.osaka-u.ac.jp

&Kazumune Hashimoto

Osaka University

hashimoto@eei.eng.osaka-u.ac.jp

###### Abstract

Safe exploration is essential for the practical use of reinforcement learning (RL) in many real-world scenarios. In this paper, we present a generalized safe exploration (GSE) problem as a unified formulation of common safe exploration problems. We then propose a solution of the GSE problem in the form of a meta-algorithm for safe exploration, MASE, which combines an unconstrained RL algorithm with an uncertainty quantifier to guarantee safety in the current episode while properly penalizing unsafe explorations _before actual safety violation_ to discourage them in future episodes. The advantage of MASE is that we can optimize a policy while guaranteeing with a high probability that no safety constraint will be violated under proper assumptions. Specifically, we present two variants of MASE with different constructions of the uncertainty quantifier: one based on generalized linear models with theoretical guarantees of safety and near-optimality, and another that combines a Gaussian process to ensure safety with a deep RL algorithm to maximize the reward. Finally, we demonstrate that our proposed algorithm achieves better performance than state-of-the-art algorithms on grid-world and Safety Gym benchmarks without violating any safety constraints, even during training.

## 1 Introduction

Safe reinforcement learning (RL) is a promising paradigm that enables policy optimizations for safety-critical decision-making problems (e.g., autonomous driving, healthcare, and robotics), where it is necessary to incorporate safety requirements to prevent RL policies from posing risks to humans or objects . As a result, safe exploration has received significant attention in recent years as a crucial issue for ensuring the safety of RL during both the learning and execution phases .

Safe exploration in RL has typically been addressed by formulating a constrained RL problem in which the policy optimization is subject to safety constraints [9; 18]. While there have been many attempts under different types of constraint representations (e.g., expected cumulative cost , CVaR ), satisfying constraints almost surely or with high probability received less attention to date. Imagine safety-critical applications such as planetary exploration where even a single constraint violation may result in catastrophic failure. NASA's engineers hope Mars rovers to ensure safety at least with high probability ; thus, constraint satisfaction "on average" does not fit their purpose.

While several algorithms have addressed this problem with this stricter notion of safety, there are several formulations in terms of how the constraints are represented, including cumulative , state , and instantaneous constraints , which respectively correspond to Problems 1, 2, and 3as we will discuss shortly in Section 2. Unfortunately, there has been limited discussion on the relationships between these approaches, making it challenging for researchers to acquire a systematic understanding of the field as a whole. If a generalized problem were to be formulated, then the research community could pool their efforts to develop suitable algorithms.

A closer examination of existing algorithms that span the entire theory-to-practice spectrum reveals several areas for improvement. Practical algorithms using deep RL (e.g., ,,) may provide satisfactory performance after convergence, but do not usually guarantee safety during training. In contrast, theoretical studies (e.g., , ) that guarantee safety with high probability during training often have limitations, such as relying on strong assumptions (e.g., known state transition) or experiencing decreased performance in complex environments. In summary, many algorithms have been proposed in various safe RL formulations, but the creation of a safe exploration algorithm that is both practically useful and supported by theoretical foundations remains an open problem.

**Contributions.** We first present a generalized safe exploration (GSE) problem and prove its generality compared with existing safe exploration problems. By taking advantage of the tractable form of the safety constraint in the GSE problem, we establish a meta-algorithm for safe exploration, MASE. This algorithm employs an uncertainty quantifier for a high-probability guarantee that the safety constraints are not violated and penalizes the agent _before_ safety violation, under the assumption that the agent has access to an "emergency stop" authority. Our MASE is both practically useful and theoretically well-founded, which allows us to optimize a policy via an arbitrary RL algorithm under the high-probability safety guarantee, even during training. We then provide two specific variants of MASE with different uncertainty quantifiers. One is based on generalized linear models (GLMs), for which we theoretically provide high-probability guarantees of safety and near-optimality. The other is more practical, combining a Gaussian process (GP, ) to ensure safety with a deep RL algorithm to maximize the reward. Finally, we show that MASE performs better than state-of-the-art algorithms on the grid-world and Safety Gym  without violating any safety constraints, even during training.

## 2 Preliminaries

**Definitions.** We consider an episodic safe RL problem in a constrained Markov decision process (CMDP, ), \(=\,,,H,,r,g,s_{1}\,\), where \(\) is a state space, \(\) is an action space, \(H_{>0}\) is a (fixed) length of each episode, \(:\) is a state transition probability, \(r:\) is a reward function, \(g:\) is a safety (cost) function, and \(s_{1}\) is an initial state. At each discrete time step, with a given (fully-observable) state \(s\), the agent selects an action \(a\) with respect to its policy \(:\), receiving the new state \(s^{}\), reward \(r\), and safety cost \(g\). Though we assume a deterministic policy, our core ideas can be extended to stochastic policy settings. Given a policy \(\), the value and action-value functions in a state \(s\) at time \(h\) are respectively defined as

\[V_{r,h}^{}(s)_{}[._{h^{}=h}^{H} _{r}^{h^{}}r(s_{h^{}},a_{h^{}})\,|\,s_{h}=s\,]\]

and \(Q_{r,h}^{}(s,a)_{}[._{h^{}=h}^{ H}_{r}^{h^{}}r(s_{h^{}},a_{h^{}})\,|\,s_{h}=s,a_{h}=a\,]\), where the expectation \(_{}\) is taken over the random state-action sequence \(\{(s_{h^{}},a_{h^{}})\}_{h^{}=h}^{H}\) induced by the policy \(\). Additionally, \(_{r}(0,1]\) is a discount factor for the reward function. In the remainder of this paper, we define \(V_{}^{h}}{1-_{r}}\) and let \(_{h}:() ()\) denote the Bellman update operator \(_{h}(Q)(s,a)[r(s_{h},a_{h})+_{r}V_{ Q}(s_{h+1})\,|\,s_{h}=s,a_{h}=a]\), where \(V_{Q}(s)_{a}Q(s,a)\).

**Three common safe RL problems.** We tackle safe RL problems where constraints must be satisfied almost surely, even during training. While such problems have garnered attention in the research community, there are several types of formulations, and their relations are yet to be fully investigated.

One of the most popular formulations for safe RL problems involves maximizing \(V_{r}^{} V_{r,1}^{}(s_{1})\) under the constraint that the cumulative cost is less than a threshold, which is described as follows:

**Problem 1** (Almost surely safe RL with cumulative constraint ).: \[_{}V_{r}^{}[._{h=1}^{H} _{g}^{h}g(s_{h},a_{h})_{1}\,|,]=1,\]

where \(_{1}_{ 0}\) is a constant representing a threshold, and \(_{g}(0,1]\) is a discount factor for \(g\).

Observe that, the expectation is _not_ taken regarding the safety constraint in Problem 1. This problem was studied in , which is stricter than the conventional one where the expectation is taken with respect to the cumulative safety cost function (i.e., \(_{}\![\,_{h=1}^{H}_{g}^{h}g(s_{h},a_{h})\,] _{1}\)).

Another popular formulation involves leveraging the state constraints so that safety corresponds to avoiding visits to a set of unsafe states. This type of formulation has been widely adopted by previous studies on safe-critical robotics tasks [38; 39; 40; 45], which is written as follows:

**Problem 2** (Safe RL with state constraints).: \[_{}V_{r}^{}[\,_{h=1}^{H} _{g}^{h}\,(s_{h} S_{})\,|\,,\,]_{2},\]

where \(_{2}_{ 0}\) is a threshold, \(()\) is the indicator function, and \(S_{}\) is a set of unsafe states.

Finally, some existing studies formulate safe RL problems via an instantaneous constraint, attempting to ensure safety even during the learning stage while aiming for extremely safety-critical applications such as planetary exploration  or healthcare . Such studies typically require the agent to satisfy the following instantaneous safety constraint at every time step.

**Problem 3** (Safe RL with instantaneous constraints).: \[_{}V_{r}^{}\![\,g(s_{h},a_{h}) _{3},\,]=1, h[\,1,H\,],\]

where \(_{3}_{ 0}\) is a time-invariant safety threshold.

## 3 Problem Formulation

This paper also requires an agent to optimize a policy under a safety constraint, as in the three common safe RL problems. We seek to find the optimal policy \(^{*}:\) of the following problem, which will hereinafter be referred to as the "generalized" safe exploration (GSE) problem:

**Problem 4** (Gse problem).: Let \(b_{h}\) denote a time-varying threshold.

\[_{}V_{r}^{}\![\,g(s_{h},a_{h})  b_{h},\,]=1, h[\,1,H\,].\]

This constraint is instantaneous, which requires the agent to learn a policy without a single constraint violation not only after convergence but also during training. We assume that the threshold is myopically known; that is, \(b_{h}\) is known at time \(h\), but unknown before that. Crucially, at every time step \(h\), since \(s_{h}\) is a fully observable state and the agent's policy is deterministic, we will use a simplified inequality represented as \(g(s_{h},a_{h}) b_{h}\) in the rest of this paper. This constraint is akin to that in Problem 3, with the difference that the safety threshold is time-varying.

**Importance of the GSE problem.** Though our problem may not seem relevant to Problems 1 and 2, we will shortly present and prove a theorem on the relationship between the GSE problem and the three common safe RL problems.

**Theorem 3.1**.: _Problems 1, 2, and 3 can be transformed into the GSE problem (i.e., Problem 4)._

See Appendix B for the proof. In other words, the feasible policy space in the GSE problem can be identical to those in the other three problems by properly defining the safety cost function \(g\) and threshold \(b_{h}\). Crucially, Problem 1 is a special case of the GSE problem with \(b_{h}=_{h}\) for all \(h\), where \(_{h+1}=_{g}^{-1}(_{h}-g(s_{h},a_{h}))\) with \(_{0}=_{1}\). It is particularly beneficial to convert Problems 1 and 2, which have additive constraint structures, to the GSE problem, which has an instantaneous constraint. The accurate estimation of the cumulative safety value in Problems 1 and 2 is difficult because they depend on the trajectories induced by a policy. Dealing with the instantaneous constraint in the GSE problem is easier, both theoretically and empirically. Also, especially when the environment is time-varying (e.g., there are moving obstacles), the GSE problem is more useful than Problem 3.

Typical CMDP formulations with expected cumulative (safety) cost are out of the scope of the GSE problem. In such problems, the safety notion is milder; hence, although many advanced deep RL algorithms have been actively proposed that perform well in complex environments after convergence,their performance in terms of safety during learning is usually low, as reported by Stooke et al.  or Wachi et al. . Risk-constrained MDPs are also important safe RL problems that are _not_ covered by the GSE problem; they have been widely studied by representing risk as a constraint on some conditional value-at-risk  or using chance constraints .1

Difficulties and Assumptions.Theorem 3.1 insists that the GSE problem covers a wide range of safe RL formulations and is worth solving, but the problem is intractable without assumptions. We now discuss the difficulties in solving the GSE problem, and then list the assumptions in this paper.

The biggest difficulty with the GSE problem lies in the fact that there may be no viable safe action given the current state \(s_{h}\), safety cost \(g\), and threshold \(b_{h}\). When \(b_{h}=0.1\) and \(g(s_{h},a)=0.5, a\), the agent has no viable action for ensuring safety. The agent needs to guarantee safety, even during training, where little environmental information is available; hence, it is significant for the agent to avoid such situations where there is no action that guarantees safety. Another difficulty is related to the regularity of the safety cost function and the strictness of the safety constraint. In this paper, the safety cost function is unknown a priori.; thus, when the safety cost does not exhibit any regularity, the agent can neither infer the safety of decisions nor guarantee safety almost surely.

To address the first difficulty mentioned above, we use Assumptions 3.2 and 3.3.

**Assumption 3.2** (Safety margin).: There exists \(_{>0}\) such that \([\,g(s_{h},a_{h}) b_{h}-,^{}\,]=1, h [1,H]\).

**Assumption 3.3** (Emergency stop action).: Let \(\) be an emergency stop action such that \((s_{1} s,)=1\) for all \(s\). The agent is allowed to execute the emergency stop action and reset the environment if and only if the probability of guaranteed safety is not sufficiently high.

Assumption 3.2 is mild; this is similar to the Slater condition, which is widely adopted in the CMDP literature . We consider Assumption 3.3 is also natural for safety-critical applications because it is usually better to guarantee safety, even with human interventions, if the agent requires help in emergency cases. In some applications (e.g., the agent is in a hazardous environment), however, emergency stop actions should often be avoided because of the expensive cost of human intervention. In such cases, the agent needs to learn a reset policy allowing them to return to the initial state as in Eysenbach et al. , rather than asking for human help, which we will leave to future work.

As for the second difficulty, we assume that the safety cost function belongs to a class where uncertainty can be estimated and guarantee the satisfaction of the safety constraint with high probability. We present an assumption regarding an important notion called an uncertainty quantifier:

**Assumption 3.4** (\(\)-uncertainty quantifier).: Let \(:\) denote the estimated mean function of safety. There exists a \(\)-uncertainty quantifier \(:\) such that \(|\,g(s,a)-(s,a)\,|(s,a)\) for all \((s,a)\), with a probability of at least \(1-\).

## 4 Method

We propose MASE for the GSE problem, which combines an unconstrained RL algorithm with additional mechanisms for addressing the safety constraints. The pseudo-code is provided in Algorithm 1, and a conceptual illustration can be seen in Figure 1.

The most notable feature of MASE is that safety is guaranteed via the \(\)-uncertainty quantifier and the emergency stop action (lines \(3-9\)). The \(\)-uncertainty quantifier is particularly useful because we can guarantee that the confidence bound contains the true safety cost function, that is, \(g(s,a)[\,(s,a)(s,a)\,]\) for all \(s\) and \(a\). This means that, if the agent chooses actions such that \((s_{h},a_{h})+(s_{h},a_{h}) b_{h}\), then \(g(s_{h},a_{h}) b_{h}\) holds with a probability of at least \(1-\). Regarding the first difficulty mentioned in Section 3, it is crucial that there is at least one safe action. Thus, at every time step \(h\), the agent computes a set of actions that are considered to satisfy the safety constraints with a probability at least \(1-\) given the state \(s_{h}\) and threshold \(b_{h}\). This is represented as

\[_{h}^{+}\{\,a\{\,1,(s_{h},a)+ (s_{h},a)\,\} b_{h}\,\}.\]

Whenever the agent identifies that at least one action will guarantee safety, the agent is required to choose an action within \(_{h}^{+}\) (line \(3\)). The emergency stop action is executed if and only if there is no viable action satisfying the safety constraint (i.e., \(_{h+1}^{+}\)); that is, the agent is allowed to execute \(\) and start a new episode from an initial safe state (lines \(6-9\)). The safety cost is upper-bounded by \(1\) because \(g\) by definition. Note that \(\) proactively avoids unsafe actions by selecting the emergency stop action to take _beforehand_; this is in contrast to Sun et al. , whose method terminates the episode immediately _after_ the agent has already violated a safety constraint.

When safety is guaranteed in the manner described above, the question remains as to how to obtain a policy that maximizes the expected cumulative reward. As such, we first convert the original CMDP \(\) to the following unconstrained MDP

\[}\,,\{, \},H,,,s_{1}\,.\]

The changes from \(\) lie in the action space and the reward function, as well as in the absence of the safety cost function. First, the action space is augmented so that the agent can execute the emergency stop action, \(\). The second modification concerns the reward function. When executing the emergency stop action \(\), the agent is penalized as its sacrifice so that the same situation will not occur in future episodes; hence, we modify the reward function as follows:

\[(s_{h},a_{h})=\{ &-c/_{a} (s_{h+1},a)&&\;\;_{h+1}^{+}=,\\ & r(s_{h},a_{h})&&,.\] (1)

where \(c_{>0}\) is a positive scalar representing a penalty for performing the emergency stop. This penalty is assigned to the state-action pair \((s_{h},a_{h})\) that placed the agent into the undesirable situation at time step \(h+1\), represented as \(_{h+1}^{+}=\) (see Figure 1).

To show that \(\) is a reasonable safe RL algorithm, we express the following intuitions. Consider the ideal situation in which the safety cost function is accurately estimated for any state-action pairs; that is, \((s,a)=0\) for all \((s,a)\). In this case, all emergency stop actions are properly executed, and the safety constraint will be violated at the next time step if the agent executes other actions. It is reasonable for the agent to receive a penalty of \((s,a)=-\) because this state-action pair surely causes a safety violation without the emergency stop action. Unfortunately, however, the safety cost is uncertain and the agent conservatively executes \(\) although there are still actions satisfying the safety constraint, especially in the early phase of training; hence, we increase or reduce the penalty according to the magnitude of uncertainty in (1) to avoid an excessively large penalty.

We must carefully consider the fact that the quality of information regarding the modified reward function \(\) is uneven in the replay buffer \(\). Specifically, in the early phase of training, the \(\)

Figure 1: Conceptual illustration of \(\). At every time step \(h\), the agent chooses action \(a_{h}\) within \(_{h}^{+}\). If there is no safe action at state \(s_{h+1}\) satisfying the constraint, the emergency stop action \(\) is executed and the agent receives a large penalty for \((s_{h},a_{h})\).

uncertainty quantifier is loose. Hence, the emergency stop action is likely to be executed even if viable actions remain; that is, the agent will receive unnecessary penalties. In contrast, the emergency stop actions in later phases are executed with confidence, as reflected by the tight \(\)-uncertainty quantifier. Thus, as in line \(15\), we rewrite the replay buffer \(\) while updating \(\) depending on the model in terms of the safety cost function (as for specific methods to update \(\), see Sections 5 and 6).

Connections to shielding methods.The notion of the emergency stop action is akin to shielding  which has been actively studied in various problem settings including partially-observable environments  or multi-agent settings . Thus, \(\) can be regarded as a variant of shielding methods (especially, preemptive shielding in ) that is specialized for the \(\) problem. On the other hand, \(\) does not only block unsafe actions but also provides proper penalties for executing the emergency stop actions based on the uncertainty quantifier, which leads to rigorous theoretical guarantees presented shortly. Such theoretical advantages can be enjoyed in many safe RL problems because of the wide applicability of the \(\) problem backed by Theorem 3.1.

Advantages of \(\).Though certain existing algorithms for Problem 3 (i.e., the closest problem to the \(\) problem) theoretically guarantee safety during learning, several strong assumptions are needed, such as a known and deterministic state transition and regular safety function as in  and a known feature mapping function that is linear with respect to transition kernels, reward, and safety as in . Such algorithms have little affinity with deep RL; thus, their actual performance in complex environments tends to be poor. In contrast, \(\) is compatible with any advanced RL algorithms, which can also handle various constraint formulations while maintaining the safety guarantee.

Validity of \(\).We conclude this section by presenting the following two theorems to show that our \(\) produces reasonable operations in solving the \(\) problem.

**Theorem 4.1**.: _Under Assumption 3.4, \(\) guarantees safety with a probability of at least \(1-\)._

**Theorem 4.2**.: _Assume that the safety cost function is estimated for any state-action pairs with an accuracy of better than \(\); that is, \((s,a)\) for all \((s,a)\). Set \(c\) to be a sufficiently large scalar such that \(c>}{2_{}^{*}}\). Then, the optimal policy in \(}\) is identical to that in \(\)._

See Appendix D for the proofs. Unfortunately, obtaining a \(\)-uncertainty qualifier that works in general cases is highly challenging. To develop a feasible model for the uncertainty quantification, we assume that the safety cost can be modeled via a GLM in Section 5 and via a GP in Section 6.

## 5 A Provable Algorithm under Generalized Linear CMDPs

In this section, we focus on CMDPs with generalized linear structures and analyze the theoretical properties of \(\). Specifically, we provide a provable algorithm to use a class of GLMs denoted as \(\) for modeling \(Q^{}_{r,h}:=Q^{}_{r,h}\) and \(g\), and then provide theoretical results on safety and optimality.

### Generalized Linear CMDPs

We extend the assumption in Wang et al.  from unconstrained MDPs to CMDPs settings. Our assumption is based on GLMs as with  that makes a strictly weaker assumption than their Linear MDP assumption . As preliminaries, we first list the necessary definitions and assumptions.

**Definition 5.1** (Glm.).: Let \(d_{>0}\) be a feature dimension and let \(^{d}\{^{d}:\|\|_{2} 1\}\) be the \(l_{2}\) ball in \(^{d}\). For a known feature mapping function \(:^{d}\) and a known link function \(f:[-1,1][-1,1]\), the class of generalized linear model is denoted as \(\{(s,a) f(_{s,a},): ^{d}\}\) where \(_{s,a}(s,a)\).

**Assumption 5.2** (Regular link function).: The link function \(f()\) is twice differentiable and is either monotonically increasing or decreasing. Furthermore, there exist absolute constants \(0<<<\) and \(M<\) such that \(<|f^{}()|<\) and \(|f^{}()|<M\) for all \(|| 1\).

This assumption on the regular link function is standard in previous studies (e.g., ). Linear and logistic models are the special cases of the GLM where the link functions are defined as \(f()=\) and \(f()=1/(1+e^{-})\). In both cases, the link functions satisfy Assumption 5.2.

We finally make the assumption of generalized linear CMDPs (GL-CMDPs), which extends the notion of the optimistic closure for unconstrained MDP settings in Wang et al. .

**Assumption 5.3** (GL-CMDP).: For any \(1 h<H\) and \(u_{}\), we have \(_{h}(u)\) and \(g\).

Recall that \(_{h}\) is the Bellman update operator. In Assumption 5.3, with a positive semi-definite matrix \(A^{d d} 0\) and a fixed positive constant \(_{}_{>0}\), we define

\[_{}\{(s,a)\{V_{},f(_{s,a},)+\|_{s,a}\|_{A}\}:^{d},0 _{},\|A\|_{} 1\},\]

where \(\|\|_{A}^{}A}\) is the matrix Mahalanobis seminorm, and \(\|A\|_{}\) is the matrix operator norm. For simplicity, we suppose the same link functions for the Q-function and the safety cost function, but it is acceptable to use different link functions. Note that Assumption 5.3 is a more general assumption than Amani et al.  that assumes linear transition kernel, reward, and safety cost functions or Wachi et al.  that assumes a known transition and GLMs in terms of reward and safety cost functions.

### GLM-MASE Algorithm

We introduce an algorithm GLM-MASE under Assumptions 5.2 and 5.3. Hereinafter, we explicitly denote the episode for each variable. For example, we let \(s_{h}^{(t)}\) or \(a_{h}^{(t)}\) denote a state or action at the time step \(h\) of episode \(t\). We also let \(_{h}^{(t)}(s_{h}^{(t)},a_{h}^{(t)})\) for more concise notations.

**Uncertainty quantifiers.** To actualize MASE in the generalized linear CMDP settings, we first need to consider how to obtain the \(\)-uncertainty quantifier in terms of the safety cost function. Since we assume \(g\), we can define the \(\)-uncertainty quantifier based on the existing studies on GLMs, especially in the field of multi-armed bandit . Based on Assumptions 5.2 and 5.3, we now provide a lemma regarding the \(\)-uncertainty quantifier on safety.

**Lemma 5.4**.: _Suppose Assumptions 5.2 and 5.3 hold. Set \(=\). With a universal constant \(C_{>0}\), let \(C_{g} C^{-1}+d^{2}(+_{}}{})}\). Define_

\[(s,a) C_{g}\|_{s,a}\|_{_{h,t}^{-1}} {with}_{h,t}_{ t}_{h}^{( )}_{h}^{()}+I,\]

_where \(I\) is the identity matrix. Let \(_{h,t}^{g}^{d}\) be the ridge estimate, which is computed by \(_{h,t}^{g}_{\|\|_{2} 1}_{  t}(g(s_{h}^{()},a_{h}^{()})-f(\,_{h}^ {()},\,))^{2}\). Then, the following inequality holds_

\[|\,g(s_{h}^{(t)},a_{h}^{(t)})-f(\,_{h}^{(t)},_{h,t}^{g}\,)\,|(s_{h}^{(t)},a_{h}^{(t)})\]

_for all \((s,a)\), with a probability at least \(1-\)._

For the purpose of qualifying uncertainty in GLMs, the weighted \(l_{2}\)-norm of \(\) (i.e., \(\|_{s,a}\|_{_{h,t}^{-1}}\)) plays an important role. Because we assume that the Q-function and safety cost function share the same feature, we have a similar lemma on the uncertainty quantifier regarding the Q-function as follows:

**Lemma 5.5**.: _Suppose Assumptions 5.2 and 5.3 hold. Let \(_{h,t}^{Q}^{d}\) denote the ridge estimate; that is, \(_{h,t}^{Q}_{\|\|_{2} 1}_{  t}(y_{h}^{()}-f(\,_{h}^{()},\, ))^{2}\), where \(y_{h}^{()} r(s_{h}^{()},a_{h}^{()})+_{a^{} }_{r,h+1}^{()}(s_{h+1}^{()},a^{})\) for all \( t\) with_

\[_{r,h}^{(t)}(s,a)\{V_{},f(_{s,a},_{h,t}^{Q})+C_{}{{g}}}(s,a)\}\]

_that is initialized with \(_{r,h}^{(0)}=0\) for all \(h H\) and \(_{r,H+1}^{(t)}=0\) for all \(1 t T\). Then, with a universal constant \(C_{}{{g}}}_{>0}\), the following inequalities holds_

\[|\,Q_{r,h}^{}(s_{h}^{(t)},a_{h}^{(t)})-f(\,_{h}^{( t)},_{h,t}^{Q}\,)\,| C_{}{{g}}} (s_{h}^{(t)},a_{h}^{(t)})\]

_for all \((s,a)\), with a probability at least \(1-\)._

Note that \((s_{h}^{(t)},a_{h}^{(t)})\) is the \(\)-uncertainty quantifier with respect to the safety cost function. One of the biggest advantages of the generalized linear CMDPs is that the magnitude of uncertainty for the Q-function is proportional to that for the safety cost function. Hence, by exploring the Q-functionbased on the optimism in the face of the uncertainty principle [7; 35], the safety cost function is also explored simultaneously, which contributes to the efficient exploration of state-action spaces.

**Integration into Mase.** The GLM-MASE is an algorithm to integrate the \(\)-uncertainty quantifiers inferred by the GLM into the MASE sequence. Detailed pseudo code is presented in Appendix E.

To deal with the safety constraint, GLM-MASE leverages the upper bound inferred by the GLM; that is, for all \(h\) and \(t\), the agent takes only actions that satisfy

\[f(\,_{h}^{(t)},_{h,t}^{g}\, )+(s_{h}^{(t)},a_{h}^{(t)}) b_{h}.\]

By Lemma 5.4, such state-action pairs satisfy the safety constraint, i.e. \(g(s_{h}^{(t)},s_{h}^{(t)}) b_{h}\), for all \(h\) and \(t\), with a probability at least \(1-\). If there is no action satisfying the safety constraint (i.e., \(_{h}^{+}=\)), the emergency stop action \(\) is taken, and then the agent receives a penalty defined in (1).

As for policy optimization, we follow the optimism in the face of the uncertainty principle. Specifically, the policy \(\) is optimized so that the upper-confidence bound of the Q-function characterized by \(\) is maximized; that is, for any state \(s\), the policy is computed as follows:

\[_{h}^{(t)}(s)=*{arg\,max}_{a}_{ ,h}^{(t)}(s,a).\]

Intuitively, this equation enables us to 1) solve the exploration and exploitation dilemma by incorporating the optimistic estimates of the Q-function and 2) make the agent avoid generating trajectories to violate the safety constraint via the modified reward function.

**Theoretical results.** We now provide two theorems regarding safety and near-optimality. For both theorems, see Appendix E for the proofs.

**Theorem 5.6**.: _Suppose the assumptions in Lemma 5.4 hold. Then, the GLM-MASE satisfies \(g(s_{h}^{(t)},a_{h}^{(t)}) b_{h}\) for all \(t[1,T]\) and \(h[1,H]\), with a probability at least \(1-\)._

**Theorem 5.7**.: _Suppose the assumptions in Lemmas 5.4 and 5.5 hold. Let \(C_{1}\) and \(C_{2}\) be positive, universal constants. Also, with a sufficiently large \(T\), let \(t^{}\) denote the smallest integer satisfying \(_{}()tH-C_{1}-C_{2}} 2C _{g}^{-1}\), where \(_{}()\) is the minimum eigenvalue of the second moment matrix \(\). Then, the policy \(^{(t)}\) obtained by GLM-MASE at episode \(t\) satisfies_

\[_{t=t^{}}^{T}[V_{r}^{^{}}-V_{r}^{^{(t)}}] (H(T-t^{})})\]

_with probability at least \(1-\)._

Theorem 5.6 shows that the GLM-MASE guarantees safety with high probability for every time step and episode, which is a variant of Theorem 4.1 under the generalized linear CMDP assumption and corresponding \(\)-uncertainty quantifier. Theorem 5.7 demonstrates the agent's ability to act near-optimally after a sufficiently large number of episodes. The proof is based on the following idea. After \(t^{}\) episodes, the safety cost function and the Q-function are estimated with an accuracy better than \(\). Then, based on Theorem 4.2, the optimal policy in \(}\) is identical to that in \(\); thus, the agent achieves a near-optimal policy by leveraging the (well-estimated) optimistic Q-function.

## 6 A Practical Algorithm

Though we established an algorithm backed by theory under the generalized linear CMDP assumption in Section 5, it is often challenging to obtain proper feature mapping functions in complicated environments. Thus, in this section, we propose a more practical algorithm combining a GP-based estimator to guarantee safety with unconstrained deep RL algorithms to maximize the reward.

Guaranteeing safety via GPs.As shown in the previous sections, the \(\)-uncertainty quantifier plays a critical role in MASE. To qualify the uncertainty in terms of the safety cost function \(g\), we consider modeling it as a GP: \(g()((),k(,^{}))\), where \([s,a]\), \(()\) is a mean function, and \(k(,^{})\) is a covariance function. The posterior distribution over \(g(,)\) is computed based on \(n_{>0}\) observations at state-action pairs \((_{1},_{2},,_{n})\) with safety measurements \(_{n}\{\,y_{1},y_{2},,y_{n}\,\}\), where \(y_{n} g(_{n})+N_{n}\) and \(N_{n}(0,^{2})\) is zero-mean Gaussian noise with a standard deviation of \(_{ 0}\). We consider episodic RL problems, and so \(n tH+h\) for episode \(t\) and time step \(h\), although the equality does not hold because of the episode cutoffs. Using the past measurements, the posterior mean, variance, and covariance are computed analytically as \(_{n}()=_{n}^{}()(_{n}+^{2})^{-1}_{n}\), \(_{n}()=k_{n}(,)\), and \(k_{n}(,^{})=k(,^{})-_{n}^{}()(_{n}+^{2})^{-1}_{n}(^{})\), where \(_{n}()=[k(_{1},),,k(_{n},)]^{}\) and \(_{n}\) is the positive definite kernel matrix. We now present a theorem on the safety guarantee.

**Theorem 6.1**.: _Assume \(\|g\|_{k}^{2} B\) and \(N_{n}\) for all \(n 1\). Set \(_{n}^{1/2} B+4+1+(1/)}\) and construct the \(\)-uncertainty quantifier by_

\[(s,a)_{n}_{n}(s,a),(s,a) ,\] (2)

_where \(_{n}\) is the information capacity associated with kernel \(k\). Then, \(\) based on (2) satisfies the safety constraint \(g(s_{h}^{(t)},a_{h}^{(t)}) b_{h}\) for all \(t\) and \(h\) with a probability of at least \(1-\)._

See Appendix F for the proofs. Theorem 6.1 guarantees that the safety constraint is satisfied by combining the GP-based \(\)-uncertainty quantifier in (2) and the emergency stop action.

Maximizing reward via deep RL.The remaining task is to optimize the policy via the modified reward function \(\) in (1), whereby the agent is penalized for emergency stop actions. This problem is decoupled from the safety constraint and can be solved as the following unconstrained RL problem:

\[_{}V_{}^{}.\] (3)

There are many excellent algorithms for solving (3) such as trust region policy optimization (TRPO, ) and twin delayed deep deterministic policy gradient (TD3, ). One of the key benefits of our \(\) is such compatibility with a broad range of unconstrained (deep) RL algorithms.

## 7 Experiments

We conduct two experiments. The first is on Safety Gym , where an agent must maximize the expected cumulative reward under a safety constraint with additive structures as in Problems 1 and 2. The safety cost function \(g\) is binary (i.e., \(1\) for an unsafe state-action pair and \(0\) otherwise), and the safety threshold is set to \(_{1}=20\). The reason for choosing Safety Gym is that this benchmark is complex and elaborate, and has been used to evaluate a variety of excellent algorithms. The second is a grid world where a safety constraint is instantaneous as in Problem 3. Due to the page limit, we present the settings and results of the grid-world experiment in Appendix H.

To solve the Safety Gym tasks, we implement the practical algorithm presented in Section 6 as follows. First, we convert the problem into a \(\) problem by defining \(b_{h}_{g}^{-1}(_{1}-_{h^{}=0}^{h-1}_{g}^ {h^{}}g(s_{h^{}},a_{h^{}}))\) and enforcing the safety constraint represented as \(g(s_{h},a_{h}) b_{h}\) for every time step \(h\) in each episode. Second, to infer the safety cost, we use deep GP to conduct training and inference, as in  when dealing with high-dimensional input spaces. Finally, as for the policy optimization in \(}\), we leverage the TRPO algorithm. We delegate other details to Appendix G.

Baselines and metrics.We use the following four algorithms as baselines. The first is TRPO, which is a safety-agnostic deep RL algorithm that purely optimizes a policy without safety consideration. The second and third are CPO  and TRPO-Lagrangian , which are well-known algorithms for solving CMDPs. The final algorithm is Saute RL , which is a recent, state-of-the-art algorithm for solving safe RL problems where constraints must be satisfied almost surely. We employ the following three metrics to evaluate our \(\) and the aforementioned four baselines: 1) the expected cumulative reward, 2) the expected cumulative safety, and 3) the maximum cumulative safety. We execute each algorithm with five random seeds and compute the means and confidence intervals.

Results.The experimental results are summarized in Figure 2. The figures show that TRPO, TRPO-Lagrangian, and CPO successfully learn the policies, but violate the safety constraints during training and even after convergence. Saute RL is much safer than those three algorithms, but the safety constraint is not satisfied in some episodes, and the performance of the policy significantly deteriorates in terms of the cumulative reward during training. Our \(\) obtains better policies in a smaller number of samples compared with Saute RL, while also satisfying the safety constraints with respect to both the average and the worst-case. Note that, after convergence, the policy obtained by \(\) performs worse than those obtained by the baseline algorithms in terms of reward, as shown in Figure 3. The emergency stop action is a variant of resetting actions that are common in episodic RL settings, which prevent the agent from exploring the state-action spaces since the uncertainty quantifier is sometimes quite conservative. We consider that this is a reason why the converged reward performance of \(\) is worse than other methods. However, because we require the agent to solve difficult problems where safety is guaranteed at every time step and episode, we consider that this result is reasonable, and further performance improvements are left to future work.

## 8 Conclusion

In this article, we first introduced the GSE problem and proved that it is more general than three common safe RL problems. We then proposed \(\) to optimize a policy under safety constraints that allow the agent to execute an emergency stop action at the sacrifice of a penalty based on the \(\)-uncertainty qualifier. As a specific instance of \(\), we first presented \(\)-\(\) to theoretically guarantee the near-optimality and safety of the acquired policy under generalized linear CMDP assumptions. Finally, we provided a practical \(\) and empirically evaluated its performance in comparison with several baselines on the Safety Gym and grid-world.

Figure 3: Experimental results on Safety Gym (CarGoal1) with the epoch of \(1000\). The box plots show the converged performance. Though MASE performs worse than other baselines in terms of reward, the acquired policy is still near-optimal. As for safety, while baselines violate the safety constraint in most of the episodes, MASE guarantees the satisfaction of the severe safety constraint.

Figure 2: Experimental results on Safety Gym (Top: PointGoal1, Bottom: CarGoal1). The proposed \(\) satisfies the safety constraint in every episode and achieves better performance in terms of the reward than the state-of-the-art method called \(\) RL. Conventional methods (i.e., TRPO, TRPO-Lagrangian, and CPO) repeatedly violate the safety constraint, especially in the early phase of training. Shaded areas represent \(1\) confidence intervals across five different random seeds.