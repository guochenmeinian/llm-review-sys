# Learning Large-Scale MTP\({}_{2}\) Gaussian Graphical Models via Bridge-Block Decomposition

Xiwen Wang\({}^{1}\), Jiaxi Ying\({}^{1,2}\), Daniel P. Palomar\({}^{1}\)

The Hong Kong University of Science and Technology\({}^{1}\)

HKUST Shenzhen-Hong Kong Collaborative Innovation Research Institute\({}^{2}\)

{xwangew, jx.ying}@connect.ust.hk, palomar@ust.hk

Corresponding author.

###### Abstract

This paper studies the problem of learning the large-scale Gaussian graphical models that are multivariate totally positive of order two (MTP\({}_{2}\)). By introducing the concept of bridge, which commonly exists in large-scale sparse graphs, we show that the entire problem can be equivalently optimized through (1) several smaller-scaled sub-problems induced by a _bridge-block decomposition_ on the thresholded sample covariance graph and (2) a set of explicit solutions on entries corresponding to _bridges_. From practical aspect, this simple and provable discipline can be applied to break down a large problem into small tractable ones, leading to enormous reduction on the computational complexity and substantial improvements for all existing algorithms. The synthetic and real-world experiments demonstrate that our proposed method presents a significant speed-up compared to the state-of-the-art benchmarks.

## 1 Introduction

In recent decades, the surge in data availability has posed challenges for analyzing and understanding large-scale datasets. A key challenge is examining pairwise relationships among numerous variables, which can be represented using Gaussian graphical models (GGMs)  that depict variable connections as graphs. The precision matrix, which is the inverse of the covariance matrix, helps determine the non-zero patterns of GGMs [2; 3]. A traditional approach to estimate the precision matrix is the graphical lasso [4; 5], which is formulated as a regularized log-determinant program. The solutions of graphical lasso possess an appealing property known as sparsity, which severs as a common assumption particularly in large graphical models and has been shown to offer numerous benefits by a significant amount of research. For example, the sparsity can reduce the model size and improve the interpretability of the model by highlighting the most important variables and their relationships, allowing better understanding the underlying causal structure [6; 7; 8].

In this paper, we solve the large-scale sparse precision matrix estimation problem for GGMs that follow a multivariate totally positive of order two (MTP\({}_{2}\)) Gaussian distribution [9; 10], or equivalently, possess a precision matrix that is a symmetric \(M\)-matrix , where all off-diagonal elements are non-positive. The so-called MTP\({}_{2}\) property is a special form of positive dependence and plays an essential role in applications where all interaction potentials are considered non-negative or the focus is on emphasizing positive associations between variables [12; 13; 14; 15]. The MTP\({}_{2}\) property has been applied in various fields. Here are some examples. In finance, MTP\({}_{2}\) structures are often exploited as the asset returns are often considered positively correlated [16; 17; 18]; In machine learning, MTP\({}_{2}\) GGMs are recognized as attractive Markov random field and have been used in applications such as taxonomic reasoning  and psychometrics . The MTP\({}_{2}\) precision matrix, also referred toas the generalized graph Laplacian [20; 21; 22], along with the combinatorial graph Laplacian [23; 24; 25; 26; 27], have found broad applications across a variety of fields due to their unique characteristics. These applications include, but are not limited to, graph Fourier transform , electrical circuits analysis , image and video coding , financial time-series analysis [31; 32], and structured graph learning [33; 34].

The objective of this paper is to build a theoretical foundation and devise effective approaches for learning large-scale, sparse \(_{2}\) GGMs. The contributions of this paper are threefold.

* This is the first work in the literature that introduces the notion of bridges in the context of learning \(_{2}\) GGMs to the best of our knowledge. Building upon this notion, we prove that the presence of explicit solutions for some entries depends on whether an edge functions as a bridge. Meanwhile, we demonstrate that the optimal solution exhibits a decomposed structure through a vertex-partition known as bridge-block decomposition.
* Based on theoretical findings, we propose an efficient framework that decomposes a large problem into several small tractable ones, each of which can be solved using any existing algorithm. With some negligible extra cost for bridge-block decomposition, the proposed method results in a dimension reduction that significantly cuts down the computational complexity.
* The synthetic and real-world experiments demonstrate that our proposed methods prompt a considerable speed-up for all existing methods and enable the solving of large-scale \(_{2}\) GGMs that were previously considered infeasible.

### Notation and Organization

Vectors and matrices are written as lower and upper case bold letters, respectively. An undirected graph is denoted as \(G=(,)\), where \(\) is the set of nodes with size \(||=p\) and \(\) is the set of edges. Note that we shall not distinguish between \((i,j) G\) and \((i,j)\). Some graph terminology frequently used throughout the paper are introduced as follows.

* **Support graph** Given a symmetric matrix \(^{p}\), the support graph of \(\), denoted as \(()\), is defined as an undirected graph with the vertex set \(=\{1,,p\}\) and the edge set \(\) such that \((i,j)\) if and only if \(A_{ij} 0\) for every two different vertices \(i,j\).
* **Neighbors** The neighbors of a node \(i\) refer to the subset of vertices that are connected to this node, i.e., \((i)=\{j|(i,j).\}\).
* **Path and Cycle** A path from node \(i_{1}\) to node \(i_{T}\) is a sequence (i.e., an ordered set) of edges denoted as \(d_{i_{1},i_{T}}\), each one incident to the next, i.e., \(\{(i_{t},i_{t+1})\}_{t=1}^{T-1}\). A cycle is a path from a node to itself when it does not duplicately include the same edge.
* **Partition** A partition \(=\{_{1},,_{K}\}\) is a collection of non-empty, disjoint vertex sets \(_{1},,_{K}\) called cluster such that \(_{i=1}^{K}_{k}=\).
* **Component** A component of \(G\) refers to a subset of vertices that are connected to each other by edges, but are not connected to any other vertices in the graph.

The paper is organized as follows. In Section 2, we introduce the problem formulation and related works. In Section 3, we present our main result along with its potential impact. Section 4 presents the numerical experiments to show that the proposed methods can facilitate the performance of existing algorithms and solve large-scale \(_{2}\) GGMs problems 2. In Section 5 we draw the conclusions.

## 2 Problem Formulation and Related Works

### Problem Formulation

Let \(\) be a random vector that follows a Gaussian distribution with zero mean and a precision matrix \(\), a.k.a. the inverse of covariance matrix \(\). A Gaussian graphical model represents the conditional dependency between random variables via a graph \(G\), in which nodes correspond to \(\) and edges between these variables represent conditional dependencies, which can be equivalently characterized by the non-zero patterns of the precision matrix \(\).

This paper considers estimating the precision matrix \(\) given \(n\) independent and identically distributed observations \(\{_{1},,_{n}\}\) that follow an \(_{2}\) Gaussian distribution. Equivalently, \(\) is assumed to be a symmetric \(M\)-matrix, i.e., \(_{ij} 0\) for any \(i j\). The problem is formulated as

\[^{p}}{} -()+, +_{i j}_{ij}|_{ij}|,\] (1)

where \(\) is the sample covariance matrix, \(_{ij} 0\) are the regularization coefficients, the objective is to minimize the negative log-likelihood of the data subject to a weighted \(_{1}\)-norm penalty on the precision matrix, and \(^{p}\) refers to a set of \(M\)-matrices with dimension \(p\), i.e.,

\[^{p}=\{^{p}| _{ij} 0, i j.\}.\] (2)

We refer to \(^{}\) as the optimal solution of (1) and to the support graph of \(^{}\), i.e., \((^{})\), as the _optimal graph_. Particularly, we define the _thresholded sample covariance matrix_\(\) as

\[T_{ij}=S_{ij}-_{ij}&i jS_{ij}>_{ij},\\ 0&,\] (3)

and the support graph of \(\) as the _thresholded sample covariance graph_ (short for _thresholded graph_). Clearly, entries corresponding to \(S_{ij}_{ij}\) will be thresholded to zero. Since the thresholded graph holds immense importance throughout the paper, unless explicitly mentioned otherwise, we denote \(G=()\).

The involvement of \(_{2}\) constraints, which require that \(_{ij} 0\) for all \(i j\), confers several advantages . For example, all \(M\)-matrices are inverse-positive, i.e., \(^{-1}\), and the non-smoothness from \(_{1}\) norms can be eliminated via \(_{i j}_{ij}|_{ij}|=-,,\) where \(=(_{ij})\) and \(()=\). Meanwhile, the \(_{2}\) constraints maintain the sparsity in the estimated precision matrix as an implicit regularizer . Throughout the paper, we require the following assumption.

**Assumption 2.1**.: For any \(i j\), we have \(S_{ij}<S_{jj}}\).

Assumption 2.1 holds with probability 1 if the number of observations \(n 2\)[13; 19]. Under that assumption, the minimizer of Problem (1) exists and is unique according to [19, Theorem 1].

### Related Works

In literature, devising algorithms for learning \(_{2}\) GGMs has garnered considerable attentions. For instance, block coordinate descent (BCD) methods [19; 26; 35] update a single row/column of the precision matrix in a cyclic manner by solving non-negative least squares problems. Projection-based methods, such as projected gradient descent  and projected quasi-Newton algorithms , iteratively take steps along the descent direction and then project the solutions back to the feasible region. Despite these efforts, existing research has not fully explored the potential of exploiting \(_{2}\) properties to reduce computational complexity. With complexities of \((p^{4})\) for BCD methods and \((p^{3})\) for projection-based techniques, addressing large-scale problems continues to be challenging, particularly when manipulating full-dimensional matrices without problem reduction.

Instead of devising efficient algorithms, recently, leveraging the properties of the thresholded sample covariance graph has emerged as a popular approach for learning GGMs. Specifically, the existence of closed-form solution has been established for graphical lasso when the thresholded sample covariance graph is acyclic (i.e., contains no cycles) [38; 39]. However, the applicability of their main results is limited by certain conditions that are difficult to verify. Interestingly, those conditions are unnecessary for the existence of closed-from solutions in \(_{2}\) GGMs . Despite the theoretical establishment on closed-form solutions, an exact acyclic structure is rarely observed in practice. Therefore, more research dives into the relationship between \(()\) and \((^{})\). Research found that \((^{})\) is a subset of \(()\), the number of connected components in \(()\) is identical to that in \((^{})\), and there exist necessary conditions for the presence of edges in \((^{})\) via analyzing the path products on thresholded matrix .

This paper advances prior research in two ways. Firstly, unlike previous studies that only provided an explicit solution for \(_{ij}\) in the case of acyclic thresholded graphs, we reveal that this explicit solution for \(_{ij}\) consistently applies to all \((i,j)\) pairs acting as bridges, regardless of whether the thresholded graph is acyclic or non-acyclic. Secondly, we highlight that the optimal graph can be represented in an equivalent decomposed form through a vertex partition, termed as the bridge-block decomposition of the thresholded graph by leveraging \(_{2}\) properties.

## 3 Proposed Methods

The goal of this paper is not about developing an algorithm but to shed light on the remarkable properties concealed in the thresholded graph. This section is organized as follows. In Section 3.1, we introduce bridge and bridge-block decomposition. Section 3.2 presents our main result. Then in Section 3.3, we elaborate on the contributions and connections to existing research.

### Bridge and Bridge-Block Decomposition

Bridge is one of the important concepts in graph theory. Technically, an edge is called a _bridge_ if and only if its deletion increases the number of graph components. Therefore, an edge is a bridge only when it is not contained in any cycles . Notably, when a graph consists solely of bridges, it is referred to as _acyclic_. In this paper, we denote \(\) as the set of all bridges, i.e,

\[:=\{.(i,j)|(i,j)G\}.\] (4)

_Remark 3.1_.: Bridges are frequently observed, particularly in large-scale sparse graphs [41; 42; 43]. This is attributed to the fact that in sparse graphs, only the most significant relationships between variables are retained, and the removal of edges can create additional connected components, giving rise to the presence of bridges.

_Remark 3.2_.: In practice, the set \(\) can be efficiently obtained via various bridge-finding algorithms [40; 44]. These algorithms employ a depth-first search approach, resulting in a computational complexity of \((||+||)\). In the sparse graphs that are of interest to us, the number of edges \(||\) typically scales similarly to the number of nodes \(||\). As a result, the computational cost of identifying bridges in large-scale sparse graphs remains low.

Using Figure 1 as an illustration, we perform the _bridge-block decomposition_ as follows. By removing all the bridges, we compute the clusters \(_{k}\) corresponding to the components of \(G=(,)\). This process results in a vertex-partition, known as the bridge-block decomposition:

\[^{}=\{_{1},_{2},, _{K}\},\] (5)

where \(K\) refers to the number of clusters, also the number of components in the graph \((,)\).

Without loss of generality, we define the operator \(\) as \((i)=k\) if node \(i\) belongs to the \(k\)-th cluster. In the literature, the bridge-block decomposition is also known as the _2-edge-connected decomposition_[46; 47], a fundamental concept in graph theory with numerous applications such as community search , social network mining , and transmission networks .

In practice, the cost of obtaining the bridge-block decomposition \(^{}\), which involves calculating the thresholded graph, bridges, and clusters, is negligible. With the aforementioned preliminary knowledge, we formally introduce our main result as follows.

### Main Results

Considering a square matrix \(^{p}\), we define \(_{_{k}}^{p_{k}}\) as the principal sub-matrix of \(\) keeping the rows and columns indexed by \(_{k}\), in which \(p_{k}=|_{k}|\) is the number of nodes in \(k\)-th cluster and

Figure 1: An illustration of the bridge-block decomposition. Edges \((5,6)\) and \((9,10)\) are identified as bridges since removing either of them increases the number of connected components. After removing the bridge edges, the resulting decomposition \(^{}=\{\{1,2,3,4,5\},\{7,8,9,10\}, \{11,12,13,14,15,16\}\}\).

we have \(p=_{k}p_{k}\). For each \(i_{k}\), we mark \((i)\) as its corresponding index in \(_{k}\). Hence, we define \(}_{k}\) as the optimal solution of \(k\)-th sub-problem, i.e.,

\[}_{k}=_{_{k}^{p_{k}}}- (_{k})+_{k},_{ _{k}}-_{_{k}}.\] (6)

The main result is then given as follows.

**Theorem 3.3**.: _Under Assumption 2.1, given the bridge-block decomposition of the thresholded graph \(()\) as \(^{}\), and the optimal solution of each sub-problem (46) as \(}_{k}\), the optimal solution of Problem (1), i.e., \(}\), can be obtained as_

\[_{i,j}^{*}=[}_{k}]_{(i),(i) }+_{i}&i=j_{k},\\ [}_{k}]_{(i),(j)}&i ji,j _{k},\\ -T_{ij}/(S_{ii}S_{jj}-T_{ij}^{2})&(i,j),\\ 0&.\] (7)

_in which \(_{i}=}_{(i,m)}^{2}}{S_{ii}S_ {mm}-T_{im}^{2}}\) and \(_{i}=0\) if \( m:(i,m)\)._

Figure 2 depicts an example of how to apply Theorem 3.3 to obtain the optimal solution more efficiently. Theoretically, the key to show the optimality of (7) is via an explicit expression of the inverse of \(\), i.e., \(=^{-1}\) using following theorem.

**Theorem 3.4**.: _Given \(,^{p}\), the bridge-block decomposition \(^{}=\{_{1},,_{K}\}\) of \(()\), and a set of matrices \(\{}_{k}_{++}^{|_{k}|}\}_{k=1}^{K}\), the inverse of \(\) in the form of (7) is derived as_

\[R_{ij}=[}_{k}]_{(i),(j)}&i,j_{k},\\ T_{ij}&(i,j),\\ S_{jj}} g_{ij}()&,\] (8)

_where \(}_{k}=[}_{k}]^{-1}\) and for each \(i,\,j\) in different clusters, \(g_{ij}\) is given as_

\[g_{ij}()=_{t=0}^{2T}R_{u_{t},u_{t+1}},u_{t}}S_{u_{t+1},u_{t+1}}},\ \ u_{0}i,\ u_{2T+1}j,\] (9)

_in which \(T-1\) is the number of bridges in \(d_{ij}\), \(g_{ij}=0\) if \(d_{ij}=\), and \(u\) denotes a sequence of incident' bridges, i.e., \(d_{ij}=\{(u_{1},u_{2}),,(u_{2T-1},u_{2T} )\}\) following the orders of \(d_{ij}\) while preserving the elements that are bridges._

We note that all terms in (9) can be computed via (8) and Theorem 3.4 itself generally holds for arbitrary \(,\), and \(\{}_{k}\}_{k=1}^{K}\). Detailed proofs and discussions are deferred to Appendix A.

Theorem 3.4 is theoretically critical as it provides an explicit form of the inverse of \(\), which was previously difficult to compute. It is vital for demonstrating the optimality of \(\) by connecting the KKT system of the original large-scale problem to the KKT systems of smaller sub-problems through the bridge-block decomposition. Consequently, together with \(_{2}\) properties, we show that \(\) in the form of (7) satisfies the KKT condition of Problem (1) in Appendix B.

**The role of \(_{2}\) constraints:** Although Theorem 3.4 can be applied without requiring \(_{2}\) properties, it is important to highlight that these properties serve as sufficient conditions to demonstrate the optimality of \(=^{-1}\). By incorporating the MTP\({}_{2}\) constraints, the non-smoothness in graphical lasso is eliminated, resulting in simplified optimality conditions as shown below:

\[ i: -R_{ii}+S_{ii}=0 -R_{ii}+S_{ii}=0\] (10a) \[_{ij} 0: -R_{ij}+S_{ij}+_{ij}(_{ij})=0  -R_{ij}+S_{ij}-_{ij}=0\] (10b) \[_{ij}=0: |-R_{ij}+S_{ij}|_{ij} -R_{ij}+S_{ij}-_{ij} 0\] (10c)

Simultaneously, \(\) becomes non-negative (\(\)). As a result, The most challenging part of the KKT conditions \(-R_{ij}+S_{ij}-_{ij} 0\) holds under MTP\({}_{2}\) properties. In Appendix C, we will elaborate the details and explore additional conditions under which we can extend the applicability of Theorem 3.3 to the traditional graphical lasso.

**Proposed solving framework:** In practical applications, it is often more efficient to employ the bridge-block decomposition technique instead of directly manipulating full-sized matrices. This approach involves breaking down the problem into smaller isolated sub-problems. By addressing these sub-problems individually, we can compute the optimal solution by utilizing the solutions obtained from each sub-problem, as outlined in Theorem 3.3. This approach offers several advantages:

* **Significant reduction in computational cost**. Foremost, the cost of bridge-block decomposition is cheap. Suppose that we use a BCD solver of complexity \((p^{4})\), then the total cost is reduced to \(_{k=1}^{K}(|_{k}|^{4})(p^{4})\), where \(_{k}|_{k}|=p\). This can prompt an enormous difference.
* **Considerable reduction in memory cost**. The memory cost is typically troublesome for large-scale problems as each full-dimensional matrix contains \(p^{2}\) elements. Theorem 3.3 can avoid generating a number of full-dimensional intermediate variables during computation.
* **Potential speed-up via parallel computing**. The sub-problems can be optimized independently, which allows parallel computing for significant speed-up.

### Connection to Existing Research

From Theorem 3.3, we obtain a very interesting result that the \((i,j)\)-th entries of \(\) admits an explicit solution, i.e., \(_{ij}=-T_{ij}/(S_{ii}S_{jj}-T_{ij}^{2})\) if edge \((i,j)\) is a bridge in the thresholded graph \(()\). To the best of our knowledge, this result, shown in Corollary 3.5, is the first sufficient condition for an edge belonging to optimal graph \((^{})\).

**Corollary 3.5**.: _If edge \((i,j)\) is a bridge in the thresholded graph \(()\), then \((i,j)\) is also a bridge in the optimal graph \((^{})\)._

Corollary 3.5 can be obtained as follows. By the definition of a bridge, from nodes \(i\) to \(j\) the path \(d_{ij}=\{(i,j)\}\) is the only path in the thresholded graph, and removing it would result in an increase in the number of components. Given that \((^{})()\) and \(_{ij} 0\), it follows that \(d_{ij}=\{(i,j)\}\) remains the unique path from nodes \(i\) to \(j\) in the optimal graph. This indicates that the edge \((i,j)\) continues to function as a bridge.

In the literature, only some necessary conditions for edges in the thresholded graph \(()\) to be retained in the optimal graph \((^{})\) have been established. For instance,  and  indicate that \((^{})()\), which implies that \((i,j)(^{})\) if \((i,j)()\).

Another explicit solution is also applicable when \(k\)-th cluster only contains one node, i.e., \(_{k}=\{i\}\), then \(}_{k}=1/S_{ii}\). As a consequence, Theorem 3.3 generalizes to the following corollary for acyclic graph which admits a bridge-block decomposition into \(p\) clusters.

**Corollary 3.6**.: _Suppose that the thresholded graph \(()\) is acyclic, then the optimal graph \((^{})\) is also acyclic and \(^{}\) admits a closed-form solution as_

\[_{ij}^{}=}(1+_{m (i)}^{2}}{S_{ii}S_{mm}-T_{im}^{2}})&i=j,\\ -}{S_{ii}S_{jj}-T_{ij}^{2}}&(i,j),\\ 0&.\] (11)

The proof directly follows Theorem 3.3. Corollary 3.6 coincides with [20, Theorem 3] and . Compared to our results in Theorem 3.3, explicit solutions in literature heavily depend on the graph structure. Hence, our theory appears as the first to reveal that, fundamentally, it is the edge property, i.e., whether the edge is a bridge, that decides the existence of explicit solutions.

In conclusion, our proposed methods generalize the existing results with more profound understandings, offering significant potential for solving large-scale precision matrix estimation problems.

## 4 Numerical Simulations

We conduct experiments on synthetic and real-world data to evaluate how the proposed method accelerates the convergence of existing algorithms compared to algorithms that do not exploit it. All experiments were conducted on 2.60GHZ Xeon Gold 6348 machines and Linux OS. All methods are implemented in MATLAB and the state-of-the-art methods we consider includes

* **BCD**: Block Coordinate Descent  of complexity \((p^{4})\).
* **PGD**: Projected Gradient Descent  of complexity \((p^{3})\).
* **PQN-LBFGS**: A projected quasi-Newton method with limited memory BFGS . The complexity is \(((m+p)p^{2})\), where \(m\) is the number of iterations stored for approximating the Hessian.
* **FPN**: Fast projected Newton-like method  of complexity \((p^{3})\).

Note that all methods converge to the optimal solution. The target here is not to compare these methods but to evaluate how our proposed framework promotes the efficiency of these methods.

### Synthetic Data Experiments

We use the processes described in  to generate the data. We begin with an underlying graph that has an adjacency matrix \(^{p}\), and define \(=-\), where \(=1.05_{}()\) and \(_{}()\) represents the largest eigenvalue of \(\). his ensures that \(\) is a positive definite matrix with off-diagonal elements being negative, making \(\) a randomly generated \(M\)-matrix. Next, we normalize \(\) by substituting it with \(\), where \(\) is a suitably chosen diagonal matrix, ensuring that \((^{-1})=\). We then sample \(n=10p\) data points from a Gaussian distribution \((,^{-1})\) and calculate the sample covariance matrix as \(\).

Following , we construct the regularization matrix \(\). Briefly, given an initial estimate \(^{(0)}\), we set \(_{ij}=(_{ij}^{(0)}+)\) when \(i j\) and \(_{ij}=0\) when \(i=j\). Here, \(>0\) determines the sparsity level and \(\) is a small positive constant, such as \(10^{-3}\). Generally, a large penalty is expected on \(_{ij}\) when \(_{ij}^{(0)}\) is small. We efficiently compute \(^{(0)}\) using the following equation:

\[_{ij}^{(0)}=-T_{ij}(S_{ii}S_{jj}-T_{ij}^{2}),  i j,\] (12)

and then appropriately adjust the value of \(\) so that \(()\) has only one connected component, while \((^{})\) can roughly recover the underlying structure.

We consider two scale-free random graphs as underlying with their typical structures in Figure 4.

**1. Barabasi-Albert (BA)** graph [52; 53] of order one. BA models generate random scale-free networks via preferential attachment, suitable for modeling various networks like the Internet, world wide web, protein interactions, citations, and social/online networks.

**2. Stochastic Block Model (SBM)** of networks , a.k.a. the community graph . Stochastic block models serve as fundamental tools in network science, creating random networks based on community structures, where nodes within the same group are more likely to form connections.

In the experiments, we consider \(p=5000\) and \(p=20000\) and calculate the relative errors as follows:

\[()=|f()-f( {}^{})|/|f(^{})|,\] (13)

where we compare the relative errors at each iteration against the computational time. Here, \(^{}\) represents the optimal solution, and \(f\) signifies the objective function of (1). For methods employing bridge-block decomposition (bbd), we first calculate \(}_{k}^{(s)}\) for each cluster at the \(s\)-th iteration, followed by calculating the intermediate solution \(^{(s)}\) using (7). The computational time at the \(s\)-th iteration is then the sum of the costs to obtain \(^{}\), calculate all \(}_{k}^{(s)}\), and compute \(^{(s)}\) via (7).

The results for \(p=5000\) and \(p=20000\) are displayed in Figure 3, with all results averaged over five realizations. We highlight that the extra computational cost compared to methods without acceleration is negligible as shown in Table 1. From the figures, we observe that:

* **Significant Speed-up:** Our proposed framework substantially accelerates state-of-the-art methods by one to four orders of magnitude.
* **Solving Otherwise Infeasible Problems:** When \(p=20000\), existing methods cannot optimize the problem within \(10^{4}\) seconds. In contrast, our framework greatly speeds up convergence, making it possible to solve large-scale problems.
* **Higher Speed-up for Sparser Graph:** Proposed method achieves greater acceleration on the BA graph than the SBM graph, as the former exhibits a stronger sparsity pattern.
* **Higher Speed-up for Methods of Higher-complexity:** Figure 3 demonstrates that our method has a more significant impact on improving the efficiency of the BCD method, which has higher computational complexity and thus benefits more from dimension reduction.

To further shed light on the factors that determine the magnitudes of improvement, in the next sub-section, we conduct additional experiments that dive into how bridge-block decomposition boosts the performance of existing methods subject to different structures of the thresholded graph.

    &  & \) from \(\{}_{k}\}_{k=1}^{K}\)} \\   & \(p=5k\) & \(p=20k\) & \(p=5k\) & \(p=20k\) \\  BA & \(0.2\) & \(2.2\) & \(0.0\) & \(0.0\) \\ SBM & \(0.3\) & \(2.3\) & \(0.1\) & \(0.2\) \\   

Table 1: Average computational time (seconds) of extra cost.

Figure 3: Relative errors of the objective values versus the computational time. Colors are used for distinguishing the state-of-the-art methods. Solid circle symbols stand for methods accelerated by bridge-block decomposition (bbd) and square symbols stand for methods without acceleration.

### Further Experiments on Ratios of Improvement

We investigate a scenario where the underlying graph is a community graph with a block-tridiagonal adjacency matrix, and each cluster contains an equal number of nodes (\(p_{1}=|_{k}|, k\)). Without loss of generality, we assume that the internal edges within clusters form a cycle. To generate \(\) and \(\), we follow the procedures in Section 4.1. We then modify \(\) to \((^{T}-)+(1-)\), ensuring that \(()=()\), where \(\) and \(\) is the adjacency matrix of the considered graph.

We experiment with various values of \(K\) (the number of clusters) and \(p_{1}\) (the number of nodes in each cluster). For each configuration, we calculate the ratio of improvement, defined as the quotient of the time required to achieve \(()<10^{-6}\) without using bridge-block decomposition to the time when applying the decomposition. We conduct multiple trials of the BCD method and display the results in Figure 5. The results in Figure 5 suggest that the number of sub-graphs \(K\) is a primary factor influencing the ratio values. The improvement is deemed tremendous, especially for large-scale settings. The involvement of bridge-block decomposition enables solving an MTP\({}_{2}\) GGMs with hundreds of millions of variables as long as we can conduce bridge-block decomposition. Consequently, many previously unattainable real-life applications can now be optimized.

### Real-word Data Experiment

We consider learning the MTP\({}_{2}\) GGM for the Crop Image dataset available from the UCR Time Series Archive . This dataset comprises \(p=24000\) pixels, where each pixel corresponds to a time series record capturing spectral information in \(n=46\) instances. These instances represent geometrically and radiometrically corrected satellite images. By examining this dataset, we can observe the temporal changes in the observed areas through the recorded measurements. To facilitate our analysis, the data has been pre-processed by  using an indexing technique. This preprocessing step enables us to work with compressed informative indexes instead of handling large image files.

Our goal is to perform graph-based clustering to the time series data that contains 46 observations, denoted as \(\{_{1},,_{46}\}\) using MTP\({}_{2}\) GGMs, where \(_{i}^{24000}\). The data includes \(24\) classes corresponding to different indexed land covers, such as corn, barley, or lake. It is important to note that these labels are not involved in the clustering process but help evaluate the quality of the final graph-based clusters by computing the graph modularity . Via analyzing the data, in Appendix D, we discuss that why learning MTP\({}_{2}\) GGMs is statistically meaningful in the context of clustering and show that the MTP\({}_{2}\) assumptions approximately hold for this dataset.

The initial estimate \(^{(0)}\) is computed using the method proposed in . The regularization matrix \(\) is determined using the approach in Section 4.1 with \(=0.01\) and \(=0.2\). We then compute the sample correlation matrix as \(^{24000}\). This results in a precision matrix estimation problem involving \(5.76 10^{8}\) variables to optimize, which is quite challenging for state-of-the-art methods without bridge-block decomposition.

Figure 5(b) shows the results of empirical convergence, and Figure 5(a) visualizes a local structure of \(()^{*}\) (modularity = \(0.6849\)). In line with previous findings, our suggested approach greatly speeds up the convergence of all current algorithms by a minimum of three orders of magnitude. This demonstrates immense potential for managing large-scale sparse graphical models.

While our paper's primary focus is not to uncover insights into the estimated \(_{2}\) GGMs for a deeper understanding of the underlying data, interesting phenomena can still be observed in the structures presented in Figure 5(a). Notably, we find that the majority of edges exist within the same crop type, while connections between nodes associated with different crops are relatively sparse. This observation is valuable for clustering processes and aligns with our expectations, as stronger positive dependencies are often observed within the same class, while dependencies between different classes tend to be weaker.

Moreover, our graphical representation reveals more nuanced patterns. For instance, in Figure 5(a) of our manuscript, we observe a dense network of edges between two crop types, 'temporary meadow' and 'pasture', indicating a significantly stronger conditional dependency compared to other categories. This observation further supports our understanding. Hence, the inferred conditional dependency structure encapsulates the inherent interrelations among different crops. As a result, our proposed framework based on bridge-block decomposition effectively facilitates learning in large-scale \(_{2}\) graphical models.

## 5 Conclusions and Discussions

Real-world sparse Gaussian graphical models often comprise subsets of variables that are densely connected to one another, while variables in different clusters maintain weak connections. However, standard estimation algorithms do not account for this property. In this paper, by introducing the concept of bridge, we leverage these characteristics to reveal an interesting finding: the optimal solution for Problem (1) equivalently admits a decomposed form via the bridge-block decomposition of the thresholded graph. We provide theoretical insights into the separability of optimal solutions and the existence of explicit expressions based on edge properties, specifically whether an edge is a bridge. This method surpasses conventional approaches that depend on unique graph structures, such as determining if a graph is acyclic. From the practical aspect, our proposed method offers a handy architect for accelerating any existing algorithms by handling a large-scale learning problem via a number of small and tractable sub-problems. Although our method is mainly developed for sparse large-scale graphs and therefore seems inapplicable for dense graph, as elaborated in the appendix C, it can also provide a quick way to obtain a solution, which can serve as either a starting point (warm-start) for numerical algorithms or an alternative for acquiring approximate solutions. Overall, our simple and provable approach demonstrates exceptional performance and paves a novel way for more extensive designs of large-scale \(_{2}\) Gaussian graphical models.

Figure 6: Visualizations and experimental results of sparse \(_{2}\) GGMs for Crop data set. (a) A local structure of optimal graph with \(2008\) nodes. Nodes with matching labels are assigned the same color and connected by a matching edge color, while different groups of nodes are connected by gray edges. (b) Results of convergence to learn sparse \(_{2}\) GGMs for Crop data set.

Acknowledgements

This work was supported by the Hong Kong Research Grants Council GRF 16207820, 16310620, and 16306821, the Hong Kong Innovation and Technology Fund (ITF) MHP/009/20, and the Project of Hetao Shenzhen-Hong Kong Science and Technology Innovation Cooperation Zone under Grant HZQB-KCZYB-2020083. We would also like to thank the anonymous reviewers for their valuable feedback on the manuscript.