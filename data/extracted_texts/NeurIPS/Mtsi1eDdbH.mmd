# A Unifying Post-Processing Framework for

Multi-Objective Learn-to-Defer Problems

 Mohammad-Amin Charusaie

Max Planck Institute for Intelligent Systems

Tuebingen, Germany

mcharusaie@tuebingen.mpg.de

&Samira Samadi

Max Planck Institute for Intelligent Systems

Tuebingen, Germany

samira.samadi@tuebingen.mpg.de

###### Abstract

Learn-to-Defer is a paradigm that enables learning algorithms to work not in isolation but as a team with human experts. In this paradigm, we permit the system to defer a subset of its tasks to the expert. Although there are currently systems that follow this paradigm and are designed to optimize the accuracy of the final human-AI team, the general methodology for developing such systems under a set of constraints (e.g., algorithmic fairness, expert intervention budget, defer of anomaly, etc.) remains largely unexplored. In this paper, using a \(d\)-dimensional generalization to the fundamental lemma of Neyman and Pearson (\(d\)-GNP), we obtain the Bayes optimal solution for learn-to-defer systems under various constraints. Furthermore, we design a generalizable algorithm to estimate that solution and apply this algorithm to the COMPAS, Hatespeech, and ACSIncome datasets. Our algorithm shows improvements in terms of constraint violation over a set of learn-to-defer baselines and can control multiple constraint violations at once. The use of \(d\)-GNP is beyond learn-to-defer applications and can potentially obtain a solution to decision-making problems with a set of controlled expected performance measures.

## 1 Introduction

Machine learning algorithms are increasingly used in diverse fields, including critical applications, such as medical diagnostics  and predicting optimal prognostics . To address the sensitivity of such tasks, existing approaches suggest keeping the human expert in the loop and using the machine learning prediction as advice , or playing a supportive role by taking over the tasks on which machine learning is uncertain . The abstention of the classifier in making decisions, and letting the human expert do so, is where the paradigm of learn-to-defer (L2D) started to exist.

The development of L2D algorithms has mainly revolved around optimizing the accuracy of the final system under such paradigm . Although they achieve better accuracy than either the machine learning algorithm or the human expert in isolation, these works provide inherently single-objective solutions to the L2D problem. In the critical tasks that are mentioned earlier, more often than not, we face a challenging multi-objective problem of ensuring the safety, algorithmic fairness, and practicality of the final solution. In such settings, we seek to limit the cost of incorrect decisions , algorithmic biases , or human expert intervention , while optimizing the accuracy of the system. Although the seminal paper that introduced the first L2D algorithm targeted an instance of such multi-objective problem , a general solution to such class of problems, besides specific examples , has remained unknown to date. Multi-objective machine learning extends beyond the realm of L2D problems. A prime example that is extensively studied in various settings is ensuring algorithmic fairness  while optimizing accuracy. Recent advances in the algorithmic fairness literature have suggested the superiority of _post-processing_ methodology for tackling thisulti-objective problem [73; 14; 20; 76]. Post-processing algorithms operate in two steps: first, they find a calibrated estimation of a set of probability scores for each input via learning algorithms, and then they obtain the optimal predictor as a function of these scores. Similarly, in a recent set of works, optimal algorithms to reject the decision-making under a variety of secondary objectives are determined via post-processing algorithms [51; 52], which is in line with classical results such as Chow's rule  that is the simplest form of a post-processing method, thresholding the likelihood.

Inspired by the above works, in this paper, we fully characterize the solution to multi-objective L2D problems using a post-processing framework. In particular, we consider a deferral system together with a set of conditional performance measures \(\{_{0},,_{m}\}\) that are functions of the system outcome \(\), the target label \(Y\), and the input \(X\). The goal is to optimize the average value of \(_{0}\) over data distribution while keeping the average value of the rest of performance measures \(_{1},,_{m}\) for all inputs under control. As an example, in binary classification, \(_{0}\) can be the \(0-1\) deferral loss function, while \(_{1}\) can be the difference between positive prediction rates of \(\) for all instances of \(X\) that belong to demographic group \(A=0\) or \(A=1\). The solution for which we aim optimizes the accuracy while assuring that the demographic parity measure between the two groups is bounded by a tolerance value \(_{1}\).

To provide the optimal solution, we move beyond staged learning  methodology, in which the classifier \(h(x)\) is trained in the absence of human decision-makers, and then the optimal rejection function \(r(x)\) is obtained for that classifier to decide when the human expert should intervene (\(r(x)=1\)). Instead, we jointly obtain the classifier and rejection function. The reason that we avoid this methodology is that firstly, objectives such as algorithmic fairness are not compositional, i.e., even if the classifier and the human are fair, due to the emergence of Yule's effect  the obtained deferral system is not necessarily fair (see Appendix A), and in fact abstention systems can deter the algorithmic biases . Secondly, the feasibility of constraints is not guaranteed under staged learning methodology , e.g., there can be cases in which achieving a purely fair solution is impossible, while this occurs neither in vanilla classification  nor in our solution.

This paper shows that the joint learning of classifier and rejection function for finding the optimal multi-objective L2D solution boils down to a generalization of the fundamental Neyman-Pearson lemma . This lemma is initially introduced in studying hypothesis testing problems and characterizes the most powerful test (i.e., the test with the highest true positive rate) while keeping the significance level (true negative rate) under control. As a natural extension to this paradigm, we consider a multi-hypothesis setting where for each true positive prediction and false negative prediction, we receive a reward and loss, respectively. Then, we show that the extension of Neyman-Pearson lemma to this setting provides us with a solution for our multi-objective L2D problem.

In summary, the contribution of this paper is as below:

* In Section 3, we show that obtaining the optimal deterministic classifier and rejection function under a constraint is, in general, an NP-Hard problem, then

Figure 1: Diagram of applying \(d\)-GNP to solve multi-objective L2D problem. The role of randomness is neglected due to simplicity of presentation.

* by introducing randomness, we rephrase the multi-objective L2D problem into a functional linear programming.
* In Section 4, we show that such linear programming problem is an instance of \(d\)-dimensional generalized Neyman-Pearson (\(d\)-GNP) problem, then
* we characterize the solution to \(d\)-GNP problem, and we particularly derive the corresponding parameters of the solution when the optimization is restricted by a single constraint.
* In Section 5, we show that a post-processing algorithm that is based on \(d\)-GNP solution generalizes in constraints and objective with the rate \(O(,,^{})\) and \(O(( n/n)^{1/2},((1/)/n)^{1/2},^{})\), respectively, with probability at least \(1-\) where \(n\) is the size of the set using which we fine-tune the algorithm, \(^{}\) measures the accuracy of learned post-processing scores, and \(\) is a parameter that measures the sensitivity of the constraint to the change of the predictor. Then,
* we show that the use of in-processing methods in L2D problem does not necessarily generalize to the unobserved data, and finally
* we experiment our post-processing algorithm on two tabular datasets and a text dataset, and observe its performance compared to the baselines for ensuring demographic parity and equality of opportunity on final predictions.

Lastly, the \(d\)-GNP theorem has potential use cases beyond the L2D problem, particularly in vanilla classification problems under constraints. However, such applications are beyond the scope of this paper, and except for a brief explanation of the use of \(d\)-GNP in algorithmic fairness for multiclass classification, we leave them to future works.

## 2 Related Works

Human and ML's collaboration in decision-making has been demonstrated to enhance the accuracy of final decisions compared to predictions that are made solely by humans or ML [37; 68]. This overperformance is due to the ability to estimate the accuracy and confidence of each agent on different regions of data and subsequently allocate instances between human and ML to optimize the overall accuracy . Since the introduction of the L2D problem, the implementation of its optimal rule has been the focus of interest in this field [8; 50; 12; 51; 9; 43; 48; 45]. The multi-objective classification with abstention problems is studied for specific objectives in [44; 57; 48] via in-processing methods. The application of Neyman-Pearson lemma for learning problems with fairness criteria is recently introduced in .

We refer the reader to Appendix B for further discussion on related works.

## 3 Problem Setting

Assume that we are given input features \(x_{i}\), corresponding labels \(y_{i}=\{1,,L\}\), and the human expert decision \(m_{i}\) for such input, and assume that these are i.i.d. realizations of random variables \(X,Y,M=_{XYM}\). Since there exists randomness in the human decision-making process, for the sake of generality, we treat \(M\) as a random variable similar to \(Y\) and do not assume that \(m_{i}=m(x_{i})\) for some function \(m\). Further, assume that for the true label \(y\) and a certain feature vector \(x\), the cost of incorrect predictions is measured by a loss function \(_{AI}(y,h(x))\) for the classifier prediction \(h(x)\), and a loss function \(_{H}(y,m)\) for human's prediction \(m\). The question that we tackle in this paper is the following: _What is an optimal classifier and otherwise an optimal way of deferring the decision to the human when there are constraints that limit the decision-making?_ The constraints above can be algorithmic fairness constraints (e.g., demographic parity, equality of opportunity, equalized odds), expert intervention constraints (e.g., when the human expert can classify up to \(b\) proportion of the data), or spatial constraints to enforce deferral on certain inputs, or any combination thereof.

Let us put the above question in a formal optimization form. To that end, let \(r(x)\{0,1\}\) be the rejection function1, i.e., when \(r(x)=0\) the classifier makes the decision for input \(x\) and otherwise \(x\) is deferred to the expert. We obtain the deferral loss on \(x\) and given a label \(y\) and the expert decision \(m\) as

\[_{}(y,m,h(x),r(x))=r(x)_{H}(y,m)+(1-r(x))_{AI}(y,h(x)).\]Therefore, we can find the average deferral loss on distribution \(\) as

\[L_{}^{}(h,r):=_{X,Y,M}_{ }(Y,M,h(X),r(X)).\] (1)

We aim to find a randomized algorithm \(\) that defines a probability distribution \(_{}\) on \(\) that solves the optimization problem

\[_{}*{argmin}_{_{}} _{(h,r)}L_{}^{}(h,r),\] \[s.t. _{X,Y,M}_{(h,r)_{}} _{i}X,Y,M,h(X),r(X)_{i}\] (2)

where \(_{i}\) is a performance measure that induces the desired constraint in our optimization problem. We assume that \(_{i}\), similar to \(_{}\), is an _outcome-dependent_ function, i.e., if the deferral occurs, the outcome of the classifier does not change \(_{i}\), and otherwise, if deferral does not occur, the human decision does not change \(_{i}\). In other words, the value of the constraints can only be a function of input feature \(x\) and of the deferral system prediction \(=r(x)M+1-r(x)h(x)\). Here, \(\) is the expert decision when deferral occurs, and is the classifier decision otherwise.

**Types of constraints.** Before we discuss our methodology to solve (2), it is beneficial to review the types of constraints with which we are concerned: **(1) expert intervention budget** that can be written in form of \(r(X)=1\), limits the rejection function to defer up to \(\) proportion of the instance, **(2) demographic parity** that is formulated as \(P(=1|A=0)-P(=1|A=0)\), ensures that the proportion of positive predictions for the first demographic group (\(A=0\)) is comparable to that for the second demographic group (\(A=1\)). **(3) equality of opportunity** that is defined as \(|Pr(=1|A=1,Y=1)-Pr(|A=0,Y=1)|\) limits the differences between correct positive predictions among two demographic groups, **(4) equalized odds** that is similar to equality of opportunity but targets the differences of correct positive and negative predictions among two groups, i.e., \(_{y=0,1}Pr(=1|A=1,Y=y)-Pr(=1|A=0,Y=y)\), **(5) out-of-distribution (OOD) detection** that is written as \(_{}(r(X)=0)\) limits the prediction of the classifier on points that are outside its training distribution and incentivizes deferral in such cases, **(6) long-tail classification** deals with high class imbalances. This method aims to minimize a balanced error of classifier prediction on instances where deferral does not occur. Achieving this objective as

   Name & Embedding Function \(_{i}(x)\) \\  Accuracy & \([(Y=0|x),,(Y=n|x),(Y=M|x)]\) \\  Expert Intervention Budget  & \([0,,0,1]\) \\  OOD Detection  & \([0,,0,^{}(x)}{f_{X}^{}(x)}]\) \\  Long-Tail Classification  & \(-_{i=1}^{K}|X=x)}{_{i}(Y G _{i})},,_{i=1}^{K}|X=x)}{_{i}(Y  G_{i})},0\) \\  & and \\  & \(|X=x)}{(Y G_{i})}1,,1,0-}{K}\) \\  Bound on Type-\(K\) Error  & \([1,,_{k},,1, (M k\,|\,Y=k,x)]\) \\  Demographic Parity  & \((_{A=1}}{Pr(A=1)}-_{A=0}}{Pr(A=0)})[0,1,(M= 1|x)]\) \\  Equality of Opportunity  & \(t(A,1)[0,(Y=1|x),(M=1,Y=1|x)]\) \\  Equalized Odds  & \(t(A,1)[0,(Y=1|x),(M=1,Y=1|x)]\) \\  & and \\  & \(t(A,0)[(Y=0|x),0,(M=0,Y=0|x)]\) \\   

Table 1: A list of embedding functions corresponding to the constraints that are discussed in Section 3. This list is a version of the results in Appendix D when we assume that the input feature contains demographic group identifier \(A\). To simplify the notations, we define \(t(A,y):=_{A=1}}{Pr(Y=y,A=1)}-_{A=0}}{Pr(Y=y,A=0 )}\).

mentioned in  is equivalent to minimizing \(_{i=1}^{K}}(Y h(X),r(X)=0|Y G_{i})\) when the feasible set is \((r(X)=0,Y G_{i})=}{K}\), and where \(\{G_{i}\}_{i=1}^{K}\) is a partition of classes, and finally (7) **type-\(k\) error bounds** that is a generalization of Type-1 and Type-2I errors, limits errors of a specific class \(k\) using \(( k|Y=k)\).

All above constraints are expected values of outcome-dependent functions (see Appendix D for proof). To put it informally, if we change the classifier outcome after the rejection, such constraints do not vary.

**Linear Programming Equivalent to (2).** The outcome-dependence property helps us to show that (see Appendix C) obtaining the optimal classifier and rejection function is equivalent to obtaining the solution of

\[f^{*}=[f_{1}^{*},,f_{d}^{*}]*{argmax}_{f_{d}^{ X}} f(X),_{0}(X),\  f(x),_{i}(x)_{i},i[1:m]\] (3)

where \(_{d}\) is a simplex of \(d\) dimensions, \(d=L+1\), and \(_{i}:^{d}\) is defined as

\[_{i}(x):=_{Y,M|X=x}_{i}(x,Y,M,1,0),, _{i}(x,Y,M,l,0),_{i}(x,Y,M,0,1)\] (4)

that we name the _embedding function_2 corresponding to the performance measure \(_{i}\) for \(i[0:m]\), where for simplifying the notation we define \(_{0}-_{}\). Furthermore, the optimal algorithm is obtained by predicting \(h(x)=i\) with normalized probability of \(f_{i}^{*}(x)/_{j=1}^{d-1}f_{j}^{*}(x)\), where \(_{j=1}^{d-1}f_{j}^{*}(x) 0\), and rejecting \(r(x)=1\) with probability \(f_{d}^{*}(x)\). In case of \(_{j=1}^{d-1}f_{j}^{*}(x)=0\) the classifier is defined arbitrarily. A list of embedding functions for the mentioned constraints and objectives is provided in Table 1 (See Appendix D for derivations).

**Hardness.** We first derive the following negative result for the optimal deterministic predictor in (3). We use the similarity between (3) and \(0-1\) Knapsack problem (see (58, pp. 374)) to show that there are cases in which solving the former is equivalent to solving an NP-Hard problem. More particularly, if we assume that the distribution of \(X\) contains finite atoms \(x_{1},,x_{n}\), each of which have probability of \((X=x_{i})=p_{i}\), and if we set \(_{1}(x_{i})=[0,}{p_{i}}]\) and \(_{0}(x_{i})=[0,}{p_{i}}]\) for \(v_{i},w_{i}^{+}\), then (3) reduces in \(*{argmax}_{i}f^{1}(x_{i})v_{i}\) subjected to \(f^{1}:\{0,1\}\) and \(_{i}f^{1}(x_{i})w_{i}_{1}\), which is the main form of the Knapsack problem. In the following theorem, we show that a similar result can be obtained if we choose \(_{0}\) and \(_{1}\) to be embedding functions corresponding to accuracy and expert intervention budget. All proofs of theorems can be found in the appendix.

**Theorem 3.1** (NP-Hardness of (2)).: _Let the human expert and the classifier induce \(0-1\) losses and assume \(\) to be finite. Finding an optimal deterministic classifier and rejection function for a bounded expert intervention budget is an NP-Hard problem._

Note that the above finding is different from the complexity results for deferral problems in [49, Theorem 1] and [23, Theorem 1]. NP-hardness results in these settings are consequences of restricting the search to a specific space of models, i.e., the intersection of half-spaces and linear models on a subset of the data. However, in our theorem, the hardness arises due to a possibly complex data distribution and not because of the complex model space.

The above hardness theorem for deterministic predictors justifies our choice of using randomized algorithms to solve multi-objective L2D. In the next section, by finding a closed-form solution for the randomized algorithm, we show that such relaxation indeed simplifies the problem.

## 4 \(d\)-dimensional Generalization of Neyman-Pearson Lemma

The idea behind minimizing an expected error while keeping another expected error bounded is naturally related to the problem that is designed by Neyman and Pearson . They consider two hypotheses \(H_{0},H_{1}\) as two distributions with density functions \(g_{0}(x)\) and \(g_{1}(x)\) for which a given point \(x\) can be drawn. Then, they maximize the probability of correctly rejecting \(H_{0}\), while bounding the probability of incorrectly rejecting \(H_{0}\), i.e., for a test \(T(x)\) that rejects the null hypothesis when \(T(x)=1\), they solved the problem

\[_{T^{}}_{X g_{1}}T(X), s.t.\ _{X g_{0}}T(X).\] (5)They concluded that thresholding the likelihood ratio is a solution to the above problem. Formally, they show that all optimal hypothesis tests take the value \(T(x)=1\) when \(g_{1}(x)/g_{0}(x)>k\) and take the value \(T(x)=0\) when \(g_{1}(x)/g_{0}(x)<k\), where \(k\) is a scalar and dependent on \(\).

**Multi-hypothesis testing with rewards.** In this section, we aim to solve (3) as a generalization of Neyman-Pearson lemma for binary testing to the case of multi-hypothesis testing, in which correctly and incorrectly rejecting each hypothesis has a certain reward and loss. To clarify how the extension of this setting and the problem (3) are equivalent, assume the general case of \(d\) hypotheses \(H_{0},,H_{d-1}\), each of which corresponding to \(X\) being drawn from the density function \(g_{i}(x)\) for \(i\{0,,d-1\}\). Further, assume that for each hypothesis \(H_{i}\), in case of true positive, we receive the reward \(r_{i}(x)\), and in case of false negative, we receive the loss \(_{i}(x)\). Assume that we aim to find a test \(f:_{d}\) that for each input \(x\) rejects \(d-1\) hypotheses, each hypothesis \(H_{i}\) with probability \(1-f^{i}(x)\) and maximizes a sum of true positive rewards, and that keeps the sum of false negative losses under control. Then, this is equivalent to \(*{argmax}_{f_{d}^{}}_{i=0}^{d-1}_{X g_{i}}f^{i}(x)r_{i}(x)\) subjected to \(_{i=0}^{d-1}_{X g_{i}}(1-f^{i}(x))_{i}(x) _{1}\) which in turns is equivalent to

\[*{argmax}_{f_{d}^{}}_{X g_{0} }_{i=0}^{d-1}f^{i}(x)r_{i}(x)(x)}{g_{0}(x)} _{X g_{0}}_{i=0}^{d-1}f^{i}(x)_{j  i}_{j}(x)(x)}{g_{0}(x)}_{1}.\] (6)

This problem can be seen as instance of (3), when we set \(_{0}(x)=[r_{0}(x),,r_{d-1}(x)(x)}{g_{0}(x)}]\) and \(_{1}(x)=_{j 0}_{j}(x)(x)}{g_{0}(x)},, _{j d-1}_{j}(x)(x)}{g_{0}(x)}\). Similarly, we can show that for all \(_{0}(x),_{1}(x)\) in (3) there exists a set of densities \(g_{1}(x),,g_{d-1}(x)\) and rewards and losses such that (6) and (3) are equivalent. This can be done by setting \(g_{i} g_{0}\) and noting that the mapping from \(_{i}\)s and \(r_{i}\)s into \(_{0}\) and \(_{2}\) is invertible.

The formulation of (3) can be seen as an extension of the setting in  when we move beyond type-\(k\) error bounds to a general set of constraints. That work achieves the optimal test by applying strong duality on the Lagrangian form of the constrained optimization problem. However, we avoided using this approach in proving our solution, since finding \(f^{*}\), and not the optimal objective, is possible via strong duality only when we know apriori that the Lagrangian has a single saddle point (for more details and fallacy of such approach, see Section E). As another improvement to the duality method, we not only find a solution to (3), but also show that there is no other solution that works as well as ours.

Before we express our solution in the following theorem, we define an import notation as an extension of the \(*{argmax}\) function that helps us articulate the optimal predictor. In fact, we define

\[_{d}=:^{d}^{d}_{d} \,|\,_{i:x_{i}=\{x_{1},,x_{d}\}}(_{1}^{d},)(i)=1}\] (7)

that is a set of functions that result in one-hot encoded \(*{argmax}\) when there is a clear maximum, and otherwise, based on its second argument, results in a probability distribution on all components that achieved the maximum value.

**Theorem 4.1** (\(d\)-Gnp).: _For a set of functions \(_{i}\) where \(i[0,m]\), assume that \((_{1},,_{m})\) is an interior point3 of the set \(=[ r(x),_{1}(x)],,[ r(x),_{m}(x)]:f_{d}^{}}\). Then, there is a set of fixed values \(k_{1},,k_{m}\) and \(_{d}\) such that the predictor_

\[f^{*}(x)=_{0}(x)-_{i=1}^{m}k_{i}_{i}(x),x,\] (8)

_obtains the optimal solution of \(_{f_{d}^{}} f(x),_{0}(x) \), subjected to the constraints being achieved tightly, i.e., when for \(i[1:m]\) we have \( f(x),_{i}(x)=_{i}\). If \(k_{1},,k_{m}\) are further non-negative, then \(f^{*}(x)\) is the optimal solution to (3). Moreover, all optimal solutions of (3) that tightly achieve the constraints are in form of (8) almost everywhere on \(\)._

**Example 1** (L2D with Demographic Parity).: In the setting that we have a deferral system and we aim for controlling demographic disparity under the tolerance \(\), we can set \(0|x),(Y=1|x),(Y=M|x)\) and \(_{1}(x)=s(A)0,1,(M=1|x)\), using Table 1, where \(s(A):=}{Pr(A=1)}-}{Pr(A=0)}\). Therefore, \(d\)-GNP, together with the discussion after (4) shows that the optimal classifier and rejection function are obtained as

\[h(x)=\{1&(Y=1|x)>\\ 0&(Y=1|x)<.,\]

and

\[r(x)=\{1&(Y=M|x)-ks(A)(M=1|x)>(A,x)\\ 0&(Y=M|x)-ks(A)(M=1|x)<(A,x).,\]

for a fixed value \(k\), and where \((A,x):=\{(Y=0|x),(Y=1|x)-ks(A)\}\). The above identities imply that the optimal fair classifier for the deferral system thresholds the scores for different demographic groups using two thresholds \(ks(0)\) and \(ks(1)\). This is similar in form to the optimal fair classifier in vanilla classification problem . However, the rejection function does not merely threshold the scores for different groups, but adds an input-dependent threshold \(ks(A)(M=1|x)\) to the unconstrained deferral system scores.

It is important to note that although we have a thresholding rule for the classifier, the thresholds are not necessarily the same as of isolated classifier under fairness criteria. Furthermore, the deferral rule is dependent on the thresholds that we use for the classifier. Therefore, we cannot train the classifier for a certain demographic parity and a rejection function in two independent stages. This further affirms the lack of compositionality of algorithmic fairness that we discussed earlier in the introduction of this paper.

**Example 2** (L2D with Equality of Opportunity).: Here, similar to the previous example, we can obtain the embedding function for accuracy and equality of opportunity constraint as \(_{0}(x)=p_{x}^{0},p_{x}^{1},p_{x}^{M}\) and \(_{1}(x)=t(A,1)0,p_{x}^{1},(M=1,Y=1|x)\), respectively, where \(p_{x}^{i}:=(Y=i|x)\) for \(i\{1,2\}\) and similarly \(p_{x}^{M}=(Y=M|x)\). Therefore, the characterization of optimal classifier and rejection function using \(d\)-GNP results in

\[h(x)=\{1&2-kt(A,1)p_{x}^{1}>1\\ 0&2-kt(A,1)p_{x}^{1}<1.,\]

and

\[r(x)=\{1&p_{x}^{M}1-kt(A,1)(M=1|Y=M,x) >(A,x)\\ 0&p_{x}^{M}1-kt(A,1)(M=1|Y=M,x)<(A,x).,\]

for \(k\) and where \((A,x):=\{p_{x}^{0},1-kt(A,1)p_{x}^{1}\}\). Assuming \(2-kt(A,1)\) takes positive values for all choices of \(A\), we conclude that the optimal classifier is to threshold positive scores differently for different demographic groups. However, the optimal deferral is a function of probability of positive prediction by human expert.

**Example 3** (Algorithmic Fairness for Multiclass Classification).: In addition to addressing the L2D problem, the formulation of \(d\)-GNP in Theorem 4.1 allows for finding the optimal solution in vanilla classification. In fact, for an \(L\)-class classifier, if we aim to set constraints on demographic parity \((=0|A=0)-(=0|A=1)\) or equality of opportunity \((=0|Y=0,A=0)-(=0|Y=0,A=1)\) on Class \(0\), then we can follow similar steps as in Appendix D to find the embedding functions as \(_{}=s(A)1,0,,0\) and \(_{}=t(A,0)p_{x}^{0},0,,0\), where \(p_{x}^{i}:=(Y=i|x)\) for \(i[L]\).

As a result, since the accuracy embedding function is \(_{0}(x)=p_{x}^{0},,p_{x}^{L}\), then, by neglecting the effect of randomness, the optimal classifier under such constraints are as

\[h_{}(x)=*{argmax}p_{x}^{0}-ks(A),p_{x}^{1}, ,p_{x}^{L}},\]

and

\[h_{}(x)=*{argmax}p_{x}^{0}1-kt(A,0) ,p_{x}^{1},,p_{x}^{L}}.\]

Equivalently, for demographic parity, the optimal classifier includes a shift on the score of Class \(0\) as a function of demographic group, and for equality of opportunity, the optimal classifier includes a multiplication of the score of Class \(0\) with a value that is a function of demographic group. It is easy to show that under condition of positivity of the multiplied value, these classifiers both reduce to thresholding rules in binary setting.

Note that although Theorem 4.1 characterizes the optimal solution of (3), it leaves us uninformed regarding parameters \(k_{1},,k_{m}\), and further does not give us the form of the optimal solution when \(_{0}(x)-_{i=1}^{m}k_{i}_{i}(x)\) has more than one maximizer. In the following theorem, we address these issues for the case that we have a single constraint.

**Theorem 4.2** (\(d\)-Gnp with a single constraint).: _The optimal solution (8) of the optimization problem (3) with one constraint is equal to \(f_{k,p}^{*}(x)=_{0}(x)-k_{1}(x),x\) where \(\) is a member of \(_{d}\) such that if there is a non-singleton set \(\) of maximizers of a vector \(^{d}\), then we have \((,x)(i)=p\) and \((,x)(j)=1-p\), where \(i\) and \(j\) are the first indices in \(\) that minimizes \(_{1}(x)\), and maximizes \(_{0}(x)\), respectively. In this case, \(k\) is a member of the set \(=t:\;_{ t}C(),C(t) }\) where \(C(t)= f_{t,0}^{*}(x),_{0}(x)\) is the expected constraint of the predictor \(f_{t,0}^{*}\). Moreover, \(p=}C()}\), if \(C()\) is lower-discontinuous at \(k\), and otherwise \(p=0\)._

This theorem reduces the complexity of finding \(k_{i}\)s from the complexity of an exhaustive search to the complexity of finding the root of the monotone function \(C(t)-\) (see Lemma J.2 for the proof of monotonicity), and further finds the randomized response for the cases that Theorem 4.1 leaves undetermined.

Before we proceed to the designed algorithm based on \(d\)-GNP, we should address two issues. Firstly, during the course of optimization, it can occur that the solution of Theorem 4.1 does not compute non-negative values \(k_{i}\) for an \(i[1:m]\). This means that the constraints are not achieved tightly in the final solution of (3). Therefore, we are able to achieve the optimal solution with the constraint \(_{i}^{}<_{i}\). Now, if we can assure that the constraint tuples are still inner points of \(\) when we substitute \(_{i}\) by \(_{i}^{}\), then Theorem 4.1 shows that (8) is still an optimal solution to (3).

Secondly, for tackling various objectives that are defined in Section 3, we usually need to upper- and lower-bound a performance measure by \(\) and \(-\). However, since both bounds cannot hold tightly and simultaneously unless the tolerance is \(=0\), then we can use only one of the constraints in turn and apply the result of Theorem 4.2 and check whether the constraint is active in the final solution.

In the next section, we design an algorithm based on these results and show its generalization to the unseen data.

## 5 Empirical \(d\)-Gnp and its Statistical Generalization

In previous sections, we obtained the optimal solution to the constrained optimization problem (3) using \(d\)-GNP. Based on this optimal solution, we can design a plug-in method (see Algorithm 1 in Appendix F) to solve the constrained learning problem using empirical data. This algorithm varies from many Lagrangian-based algorithms for solving constrained learning problem (e.g., Primal-Dual method ) in which the optimal predictor parameter and constraint penalties are dependent to each other, and therefore we should learn them iteratively. However, as we saw in Theorem 5.1 (respectively in Algorithm 1), the solution of \(d\)-GNP is a mere thresholding on the corresponding embedding functions, where the threshold is obtained in a post-hoc manner and from validation dataset. Therefore, although Lagrangian-based algorithms can lead to oscillations or converge with a large computational cost, the \(d\)-GNP can potentially reduce such complexity costs and improve convergence conditions. To show such convergence, we bound the generalization error of the objective and constraints based on this solution. These results are extensions of the generalization results for Neyman-Pearson  and further hold when multiple constraints should be controlled at once. The first result is the following theorem that shows if the solution to our plug-in method meets constraints of the optimization problem on training data, this generalizes to the unseen data.

**Theorem 5.1** (Generalization of the Constraints).: _For the approximation of the Neyman-Pearson solution \(_{k,}(x)\) in Algorithm 1 such that \(_{S^{n}}_{k,}(x),_{i}(x) _{i}\) for \(i[1:m]\), if we assume that embedding functions are bounded, then for \(d_{n}() O(+}{})\) and \(S^{n}\) we have \(_{}_{k,}(x),_{i}(x) _{i}+d_{n}()\) for all \(i[1:m]\) and with probability at least \(1-\)._

In the above theorem, we show that the optimal empirical solution for the constraint, probably and approximately satisfies the constraint on true distribution. Therefore, if we assume that we have an approximation \(_{i}(x)\) in hand where \(\|_{i}(x)-_{i}(x)\|_{}^{}\) with high probability, this theorem together with Holder's inequality shows that we need to assure \(_{S^{n}}_{k,}(x),_{i}(x) -d_{n}()-^{}\) to achieve the corresponding generalization with high probability.

Next, we ask whether the objectives of the empirical optimal solution and the true optimal solution are close. We answer to this question positively in the following theorem. First, however, let us define the notions of (\(\), \(\))-sensitivity condition as the following. This is an extension to detection condition in  and assumes that changing the parameter in predictor leads to a detectable change in constraints.

**Definition 5.2**.: For an embedding function \(_{1}\), and a distribution \(_{X}\) on \(\), we refer to a function \(r_{k}(x)\) as a prediction with (\(\), \(\))-sensitivity around \(k\), if there exists \(C^{+}\) such that for all \((0,]\) we have

\[|_{_{X}} r_{k}(x)-r_{k+}(x),_{1}(x) | C^{}.\] (9)

Now, we express the following generalization theorem for predictors that address the above conditions:

**Theorem 5.3** (Generalization of Objective).: _Assume that \((-_{l},+_{u})\) is a subset of of all achievable constraints \( f(x),_{1}(x)\), and that \(\|_{i}(x)\|_{} 1\) for \(i=1,2\). Further, let the size \(n\) of validation data be large enough such that \(d_{n}(/3)}{2}\). Now, if the optimal predictor \(f^{*}_{k,0}(x)\) is (\(\), \(\))-sensitive around optimal \(k^{*}\) for \(d_{n}^{1/}(/3),_{0}^{1/2}, _{1}^{1/2}\) and \( 1\), then for \(n^{2}}\), and with probability at least \(1-\), the optimal empirical classifier, as of Algorithm 1 has an objective that is at most \(Od_{n}^{1/}(/3),_{0}^{1/},_{0}^{1/2}, _{1}^{1/2},C^{-1/},C^{-1/2}\)-far from the true optimal objective._

Now that we have proven generalization of our post-processing method, we should briefly compare this to other possible algorithms to learn an approximation of the optimal classifier and rejection function pair. A possible method is to find the appropriate 'defer' or 'no defer' value for each instance in the training dataset, and for a given set of constraints. Although these types of in-processing algorithms can perform computationally efficient (e.g., \(O(n n)\) complexity for \(\)-suboptimal solution for human intervention budget as shown in Theorem G.1), they do not necessarily generalize to unseen data. In particular, we can show that for all algorithms that estimate _deferral labels_ from empirical data, there exist two underlying distributions on the data on which the algorithm results in similar deferral labels, while the optimal rejection functions for these two distributions are not interchangeable. This argument is further formalized in the following proposition:

**Proposition 5.4** (Impossibility of generalization of deferral labels).: _For every deterministic deferral rule \(\) for empirical distributions and based on the two losses \(_{m y}\) and \(_{h(x) y}\), there exist two probability measures \(_{1}\) and \(_{2}\) on \(\) such that the corresponding \((,X)\) for both measures is distributed equally. However, the optimal deferral \(r^{*}_{_{1}}\) and \(r^{*}_{_{2}}\) for these measures are not interchangeable, that is \(L^{_{i}}_{}(h,r^{*}_{_{i}})\) while \(L^{_{i}}_{}(h,r^{*}_{_{j}})=\) for \(i=1,2\) and \(j i\)._

In a nutshell, this proposition implies that, every algorithm that reduces the two-bit data of human accuracy and AI accuracy for an input into a single-bit data of 'defer' or 'no defer' looses the information that is important for obtaining the optimal rejection function that generalizes to the unseen data. This is a drawback of in-processing algorithms that are used in multi-objective L2D problems. We refer the reader to Appendix M for more details and proof of aforementioned proposition.

## 6 Experiments

**COMPAS dataset.** We implemented 4 Algorithm 1, first for COMPAS dataset  in which the recidivism rate of 7214 criminal defendants is predicted. The human assessment is done in this

Figure 2: Performance of \(d\)-GNP on COMPAS dataset (left), and ACSIncome (center and right)

dataset on 1000 cases by giving humans a description of the case and asking them whether the defendant would recidivate within two years of their most recent crime.5 The demographic parity is assessed for two racial groups of white and non-white defendants. Figure 2 shows the average performance of \(d\)-GNP over \(10\) random seeds compared to two baselines: (1) Madras et al.  in which a demographic parity regularizer is added to the surrogate loss, and over a variation of \(100\) regularizer coefficient, and (2) Mozannar et al.  in which after training the classifier and rejector pair, we shift the corresponding scores to find a new thresholding rule. All scores, classifiers, and rejection functions are trained on a \(1\)-layer feed-forward neural network. The figure shows that achieving better fairness criteria is possible using \(d\)-GNP, while this might not lead to better accuracy when the constraint violation is not of interest.

**Hatespeech dataset.** The next experiment is on flagging offensive tweets in Hatespeech dataset . This dataset contains 24,802 tweets that are labeled by at least three crowd workers as hate speech, offensive but not hate speech, or neither hate speech nor offensive. We used a pre-trained model  to detect whether the tweet contains an African-American dialect. Next, we used \(d\)-GNP method to control the demographic disparity of predicting a tweet hate speech or offensive bounded by \(_{HS}=0.1\) and \(_{O}=0.01\). In the result of this experiment that is displayed in Figure 3 we can observe the following points: (i) in test-time the resulting demographic disparity for both classes are bounded as expected, (ii) the accuracy of \(d\)-GNP method is bounded by the vanilla deferral method, while stricter constraint control (in here offensive prediction parity) keeps the accuracy lower, and (iii) interestingly, the performance of \(d\)-GNP for controlled offensive prediction parity copies that of human. Therefore, a good strategy for obtaining such constrained learn-to-defer system seems to be to defer the offensive tweet prediction to human, when the tweet contains African-American dialect, and otherwise either bias the classifier scores or use a mixture of human and classifier involvement to achieve the final controlled disparity.

**ACS dataset.** We further tested our method on folktables dataset  that contains an income prediction task based on 1.6M rows of American Community Survey data. Since we had no access to human expert data, we simulated a human expert that has different accuracy on two racial groups of white and non-white individuals (85% and 60%, respectively). We considered the L2D problem with bounded equalized odds violation. Figure 2 shows our method's accuracy and constraint violation, coupled with a confidence bound that is obtained using ten iterations of bootstrapping. This figure shows that violation bounds are accurately met for the test data, and the performance increases when these bounds are loosened.

## 7 Conclusion

The \(d\)-GNP is a general framework that obtains the optimal solution to various constrained learning problems, including but not limited to multi-objective L2D problems. Using this post-processing framework, we can first estimate the scores related to our problem and then find a linear rule of these scores by fine-tuning for specific violation tolerances. This method reduces the computational complexity of in-processing methods while guaranteeing achieving a near-optimal solution in a large data regime.

Figure 3: Prediction of \(d\)-GNP on Hatespeech dataset  and for tweets with predicted African-American (left), and Non-African-American (center) dialect and the disparity between groups (right).

Acknowledgment

M.A. Charusaie thanks the International Max Planck Research School for Intelligent Systems (IMPRS-IS) and Tubingen AI Center for the support and funding of this project. He is further grateful to Matthaus Kleindessner for his significant intellectual contributions to the first draft of this paper. The idea of obtaining an extension to the Neyman-Pearson lemma emerged from discussions with Andre Cruz and Florian Dorner. The very initial draft of this paper was written during an hours-long train delay in Germany, and thus, M.A. Charusaie is thankful to Deutsche Bahn in that regard.