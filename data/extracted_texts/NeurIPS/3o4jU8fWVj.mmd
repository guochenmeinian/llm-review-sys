# EquivformerV2: Improved Equivariant Transformer

for Scaling to Higher-Degree Representations

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

Equivariant Transformers such as Equivformer have demonstrated the efficacy of applying Transformers to the domain of 3D atomistic systems. However, they are still limited to small degrees of equivariant representations due to their computational complexity. In this paper, we investigate whether these architectures can scale well to higher degrees. Starting from Equivformer, we first replace \(SO(3)\) convolutions with eSCN convolutions to efficiently incorporate higher-degree tensors. Then, to better leverage the power of higher degrees, we propose three architectural improvements - attention re-normalization, separable \(S^{2}\) activation and separable layer normalization. Putting this all together, we propose EquivformerV2, which outperforms previous state-of-the-art methods on the large-scale OC20 dataset by up to \(15\%\) on forces, \(5\%\) on energies, offers better speed-accuracy trade-offs, and \(2\) reduction in DFT calculations needed for computing adsorption energies.

## 1 Introduction

In recent years, machine learning (ML) models have shown promising results in accelerating and scaling high-accuracy but compute-intensive quantum mechanical calculations by effectively accounting for key features of atomic systems, such as the discrete nature of atoms, and Euclidean and permutation symmetries [1; 2; 3; 4; 5; 6; 7; 8; 9; 10]. By bringing down computational costs from hours or days to fractions of seconds, these methods enable new insights in many applications such as molecular simulations, material design and drug discovery. A promising class of ML models that have enabled this progress is equivariant graph neural networks (GNNs) [11; 12; 13; 14; 15; 16; 17; 18].

Equivariant GNNs treat 3D atomistic systems as graphs, and incorporate inductive biases such that their internal representations and predictions are equivariant to 3D translations, rotations and optionally inversions. Specifically, they build up equivariant features of each node as vector spaces of irreducible representations (or irreps) and have interactions or message passing between nodes based on equivariant operations such as tensor products. Recent works on equivariant Transformers, specifically Equivformer , have shown the efficacy of applying Transformers [19; 20], which have previously enjoyed widespread success in computer vision [21; 22; 23], language [24; 25], and graphs [26; 27; 28; 29], to this domain of 3D atomistic systems.

A bottleneck in scaling Equivformer as well as other equivariant GNNs is the computational complexity of tensor products, especially when we increase the maximum degree of irreps \(L_{max}\). This limits these models to use small values of \(L_{max}\) (e.g., \(L_{max}\) 3), which consequently limits their performance. Higher degrees can better capture angular resolution and directional information, which is critical to accurate prediction of atomic energies and forces. To this end, eSCN  recently proposes efficient convolutions to reduce \(SO(3)\) tensor products to \(SO(2)\) linear operations, bringing down the computational cost from \((L_{max}^{6})\) to \((L_{max}^{3})\) and enabling scaling to larger values of \(L_{max}\) (e.g., \(L_{max}\) up to \(8\)). However, except using efficient convolutions for higher \(L_{max}\), eSCN still follows SEGNN -like message passing network design, and Equiformer has been shown to improve upon SEGNN. Additionally, this ability to use higher \(L_{max}\) challenges whether the previous design of equivariant Transformers can scale well to higher-degree representations.

In this paper, we are interested in adapting eSCN convolutions for higher-degree representations to equivariant Transformers. We start with Equiformer  and replace \(SO(3)\) convolutions with eSCN convolutions. We find that naively incorporating eSCN convolutions does not result in better performance than the original eSCN model. Therefore, to better leverage the power of higher degrees, we propose three architectural improvements - attention re-normalization, separable \(S^{2}\) activation and separable layer normalization. Putting this all together, we propose EquiformerV2, which is developed on the large and diverse OC20 dataset . Experiments on OC20 show that EquiformerV2 outperforms previous state-of-the-art methods with improvements of up to \(15\%\) on forces and \(5\%\) on energies, and offers better speed-accuracy trade-offs compared to existing invariant and equivariant GNNs. Additionally, when used in the AdsorbML algorithm  for performing adsorption energy calculations, EquiformerV2 achieves the highest success rate and \(2\) reduction in DFT calculations to achieve comparable adsorption energy accuracies as previous methods.

## 2 Related Works

_SE(3)/E(3)-Equivariant GNNs._Equivariant neural networks _ use equivariant irreps features built from vector spaces of irreducible representations (irreps) to achieve equivariance to 3D rotation . They operate on irreps features with equivariant operations like tensor products. Previous works differ in equivariant operations used in their networks and how they combine those operations. TFN  and NequIP  use equivariant graph convolution with linear messages built from tensor products, with the latter utilizing extra equivariant gate activation . SEGNN  introduces non-linearity to messages passing  with equivariant gate activation, and the non-linear messages improve upon linear messages. SE(3)-Transformer  adopts equivariant dot product attention  with linear messages. Equiformer  improves upon previously mentioned equivariant GNNs by combining MLP attention and non-linear messages. Equiformer additionally introduces equivariant layer normalization and regularizations like dropout  and stochastic depth . However, the networks mentioned above rely on compute-intensive \(SO(3)\) tensor products to mix the information of vectors of different degrees during message passing, and therefore they are limited to small values for maximum degrees \(L_{max}\) of equivariant representations. SCN  proposes rotating irreps features based on relative position vectors and identifies a subset of spherical harmonics coefficients, on which they can apply unconstrained functions. They further propose relaxing the requirement for strict equivariance and apply typical functions to rotated features during message passing, which trades strict equivariance for computational efficiency and enables using higher values of \(L_{max}\). eSCN  further improves upon SCN by replacing typical functions with \(SO(2)\) linear layers for rotated features and imposing strict equivariance during message passing.

Figure 1: Overview of EquiformerV2. We highlight the differences from Equiformer  in red. For (b), (c), and (d), the left figure is the original module in Equiformer, and the right figure is the revised module in EquiformerV2. Input 3D graphs are embedded with atom and edge-degree embeddings and processed with Transformer blocks, which consist of equivariant graph attention and feed forward networks. “\(\)” denotes multiplication, “\(\)” denotes addition, and \(\) within a circle denotes summation over all neighbors. “DTP” denotes depth-wise tensor products used in Equiformer. Gray cells indicate intermediate irreps features.

[MISSING_PAGE_FAIL:3]

tensor products and equivariant linear operations, equivariant layer normalization  and gate activation [12; 34]. For stronger expressivity in the attention compared to typical Transformers, Equiformer uses non-linear functions for both attention weights and message passing. Additionally, Equiformer incorporates regularization techniques common in Transformers applied to other domains, e.g., dropout  to attention weights  and stochastic depth  to the outputs of equivariant graph attention and feed forward networks. Please refer to the Equiformer paper  for more details.

### eSCN Convolution

While tensor products are necessary to interact vectors of different degrees, they are compute-intensive. To reduce the complexity, eSCN convolutions  are proposed to use \(SO(2)\) linear operations for efficient tensor products. We provide an outline and intuition for their method here, and please refer to Sec. A and their work  for mathematical details.

A traditional \(SO(3)\) convolution interacts input irreps features \(x_{m_{i}}^{(L_{i})}\) and spherical harmonic projections of relative positions \(Y_{m_{f}}^{(L_{f})}(r_{ij}^{})\) with an \(SO(3)\) tensor product with Clebsch-Gordan coefficients \(C_{(L_{i},m_{i}),(L_{f},m_{f})}^{(L_{o},m_{o})}\). The projection \(Y_{m_{f}}^{(L_{f})}(r_{ij}^{})\) becomes sparse if we rotate the relative position vector \(_{ij}\) with a rotation matrix \(D_{ij}\) to align with the direction of \(L=0\) and \(m=0\), which corresponds to the z axis traditionally but the y axis in the conventions of e3nn. Concretely, given \(D_{ij}_{ij}\) aligned with the y axis, \(Y_{m_{f}}^{(L_{f})}(D_{ij}_{ij}) 0\) only for \(m_{f}=0\). If we consider only \(m_{f}=0\), \(C_{(L_{i},m_{i}),(L_{f},m_{f})}^{(L_{o},m_{o})}\) can be simplified, and \(C_{(L_{i},m_{i}),(L_{f},0)}^{(L_{o},m_{o})} 0\) only when \(m_{i}= m_{o}\). Therefore, the original expression depending on \(m_{i}\), \(m_{f}\), and \(m_{o}\) is now reduced to only depend on \(m_{o}\). This means we are no longer mixing all integer values of \(m_{i}\) and \(m_{f}\), and outputs of order \(m_{o}\) are linear combinations of inputs of order \( m_{o}\). eSCN convolutions go one step further and replace the remaining non-trivial paths of the \(SO(3)\) tensor product with an \(SO(2)\) linear operation to allow for additional parameters of interaction between \( m_{o}\) without breaking equivariance. To summarize, eSCN convolutions achieve efficient equivariant convolutions by first rotating irreps features based on relative position vectors and then performing \(SO(2)\) linear operations on the rotated features. The key idea is that the rotation sparsifies tensor products and simplifies the computation.

## 4 EquiformerV2

Starting from Equiformer , we first use eSCN convolutions to scale to higher-degree representations (Sec. 4.1). Then, we propose three architectural improvements, which yield further performance gain when using higher degrees: attention re-normalization (Sec. 4.2), separable \(S^{2}\) activation (Sec. 4.3) and separable layer normalization (Sec. 4.4). Figure 1 illustrates the overall architecture of EquiformerV2 and the differences from Equiformer.

### Incorporating eSCN Convolutions for Efficient Tensor Products and Higher Degrees

The computational complexity of \(SO(3)\) tensor products used in traditional \(SO(3)\) convolutions during equivariant message passing scale unfavorably with \(L_{max}\). Because of this, it is impractical for Equiformer to use beyond \(L_{max}=1\) for large-scale datasets like OC20  and beyond \(L_{max}=3\) for small-scale datasets like MD17 [56; 57; 58]. Since higher \(L_{max}\) can better capture angular information and are correlated with model expressivity , low values of \(L_{max}\) can lead to limited performance on certain tasks such as predicting forces. Therefore, we replace original tensor products with eSCN convolutions  for efficient tensor products, enabling Equiformer to scale up \(L_{max}\) to \(6\) or \(8\) on the large-scale OC20 dataset.

Equiformer uses equivariant graph attention for message passing. The attention consists of depth-wise tensor products, which mix information across different degrees, and linear layers, which mix information between channels of the same degree. Since eSCN convolutions mix information across both degrees and channels, we replace the \(SO(3)\) convolution, which involves one depth-wise tensor product layer and one linear layer, with a single eSCN convolutional layer, which consists of a rotation matrix \(D_{ij}\) and an \(SO(2)\) linear layer as shown in Figure 0(b).

### Attention Re-normalization

Equivariant graph attention in Equiformer uses tensor products to project node embeddings \(x_{i}\) and \(x_{j}\), which contain vectors of different degrees, to scalar features \(f_{ij}^{(0)}\) and applies non-linear functions to \(f_{ij}^{(0)}\) for attention weights \(a_{ij}\). The node embeddings \(x_{i}\) and \(x_{j}\) are obtained by applying equivariantlayer normalization  to previous outputs. We note that vectors of different degrees in \(x_{i}\) and \(x_{j}\) are normalized independently, and therefore when they are projected to the same degree, the resulting \(f_{ij}^{(0)}\) can be less well-normalized. To address the issue, we propose attention re-normalization and introduce one additional layer normalization (LN)  before non-linear functions. Specifically, given \(f_{ij}^{(0)}\), we first apply LN and then use one leaky ReLU layer and one linear layer to calculate \(z_{ij}=a^{}((f_{ij}^{(0)}))\) and \(a_{ij}=_{j}(z_{ij})=(z_{ij})}{_{k(i)}(z_{ik})}\), where \(a\) is a learnable vector of the same dimension as \(f_{ij}^{(0)}\).

### Separable \(S^{2}\) Activation

The gate activation  used by Equiformer applies sigmoid activation to scalar features to obtain non-linear weights and then multiply irreps features of degree \(>0\) with non-linear weights to add non-linearity to equivariant features. The activation, however, only accounts for the interaction from vectors of degree \(0\) to those of degree \(>0\) and could be sub-optimal when we scale up \(L_{max}\).

To better mix the information across degrees, SCN  and eSCN  propose to use \(S^{2}\) activation . The activation first converts vectors of all degrees to point samples on a sphere for each channel, applies unconstrained functions \(F\) to those samples, and finally convert them back to vectors. Specifically, given an input irreps feature \(x^{(L_{max}+1)^{2} C}\), the output is \(y=G^{-1}(F(G(x)))\), where \(G\) denotes the conversion from vectors to point samples on a sphere, \(F\) can be typical SiLU activation  or typical MLPs, and \(G^{-1}\) is the inverse of \(G\).

While \(S^{2}\) activation can better mix vectors of different degrees, we find that directly replacing the gate activation with \(S^{2}\) activation results in training instability (row 3 in Table 0(a)). To address the issue, we propose separable \(S^{2}\) activation, which separates activation for vectors of degree \(0\) and those of degree \(>0\). Similar to gate activation, we have more channels for vectors of degree \(0\). As shown in Figure 1(c), we apply a SiLU activation to the first part of vectors of degree \(0\), and the second part of vectors of degree \(0\) are used for \(S^{2}\) activation along with vectors of higher degrees. After \(S^{2}\) activation, we concatenate the first part of vectors of degree \(0\) with vectors of degrees \(>0\) as the final output and ignore the second part of vectors of degree \(0\). Additionally, we also use separable \(S^{2}\) activation in point-wise feed forward networks (FFNs). Figure 2 illustrates the differences between gate activation, \(S^{2}\) activation and separable \(S^{2}\) activation.

### Separable Layer Normalization

As mentioned in Sec. 4.2, equivariant layer normalization used by Equiformer normalizes vectors of different degrees independently, and when those vectors are projected to the same degree, the projected vectors can be less well-normalized. Therefore, instead of performing normalization to each degree independently, we propose separable layer normalization (SLN), which separates normalization for vectors of degree \(0\) and those of degrees \(>0\). Mathematically, let \(x^{(L_{max}+1)^{2} C}\) denote an input irreps feature of maximum degree \(L_{max}\) and \(C\) channels, and \(x_{m,i}^{(L)}\) denote the \(L\)-th degree, \(m\)-th order and \(i\)-th channel of \(x\). SLN calculates the output \(y\) as follows. For \(L=0\), \(y^{(0)}=^{(0)}(-^{(0)}}{^{(0)}})+ ^{(0)}\), where \(^{(0)}=_{i=1}^{C}x_{0,i}^{(0)}\) and \(^{(0)}=_{i=1}^{C}(x_{0,i}^{(0)}-^{(0)})^{2}}\). For \(L>0\), \(y^{(L)}=^{(L)}(}{^{(L>0)}})\), where \(^{(L>0)}=}_{L=1}^{L_{max}}(^{(L)} )^{2}}\) and \(^{(L)}=_{i=1}^{C}_{m=-L}^{L} (x_{m,i}^{(L)})^{2}}\).

\(^{(0)},^{(L)},^{(0)}^{C}\) are learnable parameters, \(^{(0)}\) and \(^{(0)}\) are mean and standard deviation of vectors of degree \(0\), \(^{(L)}\) and \(^{(L>0)}\) are root mean square values (RMS), and \(\) denotes element-wise product. The computation of \(y^{(0)}\)corresponds to typical layer normalization. We note that the difference between equivariant layer normalization and SLN lies only in \(y^{(L)}\) with \(L>0\) and that equivariant layer normalization divides \(x^{(L)}\) by \(^{(L)}\), which is calculated independently for each degree \(L\), instead of \(^{(L>0)}\), which considers all degrees \(L>0\). Figure 3 compares how \(^{(0)}\), \(^{(0)}\), \(^{(L)}\) and \(^{(L>0)}\) are calculated in equivariant layer normalization and SLN.

### Overall Architecture

Here, we discuss all the other modules in EquiformerV2 and focus on the differences from Equiformer.

Equivariant Graph Attention.Figure 0(b) illustrates equivariant graph attention after the above modifications. As described in Sec. 4.1, given node embeddings \(x_{i}\) and \(x_{j}\), we first concatenate them along the channel dimension and then rotate them with rotation matrices \(D_{ij}\) based on their relative positions or edge directions \(_{ij}\). The rotation enables reducing \(SO(3)\) tensor products to \(SO(2)\) linear operations, and we replace depth-wise tensor products and linear layers between \(x_{i}\), \(x_{j}\) and \(f_{ij}\) with a single \(SO(2)\) linear layer. To consider the information of relative distances \(||_{ij}||\), in the same way as eSCN , we transform \(||_{ij}||\) with a radial function to obtain distance embeddings and then multiply distance embeddings with concatenated node embeddings before the first \(SO(2)\) linear layer. We split the outputs \(f_{ij}\) of the first \(SO(2)\) linear layer into two parts. The first part is scalar features \(f_{ij}^{(0)}\), which only contains vectors of degree \(0\), and the second part is irreps features \(f_{ij}^{(L)}\) and includes vectors of all degrees up to \(L_{max}\). As mentioned in Sec. 4.2, we first apply an additional LN to \(f_{ij}^{(0)}\) and then follow the design of Equiformer by applying one leaky ReLU layer, one linear layer and a final softmax layer to obtain attention weights \(a_{ij}\). As for value \(v_{ij}\), we replace the gate activation with separable \(S^{2}\) activation with \(F\) being a single SiLU activation and then apply the second \(SO(2)\) linear layer. While in Equiformer, the message \(m_{ij}\) sent from node \(j\) to node \(i\) is \(m_{ij}=a_{ij} v_{ij}\), here we need to rotate \(a_{ij} v_{ij}\) back to original coordinate frames and the message \(m_{ij}\) becomes \(D_{ij}^{-1}(a_{ij} v_{ij})\). Finally, we can perform \(h\) parallel equivariant graph attention functions given \(f_{ij}\). The \(h\) different outputs are concatenated and projected with a linear layer to become the final output \(y_{i}\). Parallelizing attention functions and concatenating can be implemented with "Reshape".

Feed Forward Network.As illustrated in Figure 0(d), we replace the gate activation with separable \(S^{2}\) activation. The function \(F\) consists of a two-layer MLP, with each linear layer followed by SiLU, and a final linear layer.

Embedding.This module consists of atom embedding and edge-degree embedding. The former is the same as that in Equiformer. For the latter, as depicted in the right branch in Figure 0(c), we replace original linear layers and depth-wise tensor products with a single \(SO(2)\) linear layer followed by a rotation matrix \(D_{ij}^{-1}\). Similar to equivariant graph attention, we consider the information of relative distances by multiplying the outputs of the \(SO(2)\) linear layer with distance embeddings.

Radial Basis and Radial Function.We represent relative distances \(||_{ij}||\) with a finite radial basis like Gaussian radial basis functions  to capture their subtle changes. We transform radial basis with a learnable radial function to generate distance embeddings. The function consists of a two-layer MLP, with each linear layer followed by LN and SiLU, and a final linear layer.

Output Head.To predict scalar quantities like energy, we use one feed forward network to transform irreps features on each node into a scalar and then perform sum aggregation over all nodes. As for predicting forces acting on each node, we use a block of equivariant graph attention and treat the output of degree \(1\) as our predictions.

## 5 OC20 Experiments

Our experiments focus on the large and diverse OC20 dataset  (Creative Commons Attribution 4.0 License), which consists of \(1.2\)M DFT relaxations for training and evaluation, computed with the revised Perdew-Burke-Ernzerhof (RPBE) functional . Each structure in OC20 has an adsorbate molecule placed on a catalyst surface, and the core task is Structure-to-Energy-Forces (S2EF), which is to predict the energy of the structure and per-atom forces. Models trained for the S2EF task are evaluated on energy and force mean absolute error (MAE). These models can in turn be used for performing structure relaxations by using the model's force predictions to iteratively update the atomic positions until a relaxed structure corresponding to a local energy minimum is found. Theserelaxed structure and energy predictions are evaluated on the Initial Structure to Relaxed Structure (IS2RS) and Initial Structure to Relaxed Energy (IS2RE) tasks. The "All" split of OC20 contains \(134\)M training structures spanning \(56\) elements, and "MD" split consists of \(38\)M structures. We first conduct ablation studies on EquiformerV2 trained on the smaller S2EF-2M subset (Sec. 5.1). Then, we report the results of training on S2EF-All and S2EF-All+MD splits (Sec. 5.2). Additionally, we investigate the performance of EquiformerV2 when used in the AdsorbML algorithm  (Sec. 5.3). Please refer to Sec. B and C for details of models and training.

### Ablation Studies

**Architectural Improvements**. In Table 0(a), we ablate the three proposed architectural changes - attention re-normalization, separable \(S^{2}\) activation and separable layer normalization. First, with attention re-normalization (row 1 and 2), energy errors improve by \(2.4\%\), while force errors are about the same. Next, we replace the gate activation with \(S^{2}\) activation used in SCN  and eSCN , but that does not converge (row 3). Instead, using the proposed separable \(S^{2}\) activation (row 4), where we have separate paths for invariant and equivariant features, converges to \(5\%\) better forces albeit hurting energies. Similarly, replacing equivariant layer normalization with separable layer normalization (row 5) further improves forces by \(1.5\%\). Finally, these modifications enable training for longer without overfitting (row 7), further improving forces by \(3.6\%\) and recovering energies to similar accuracies as Index 2. Overall, our modifications improve forces by \(10\%\) and energies by \(3\%\). Note that simply incorporating eSCN convolutions into Equiformer (row 1) and using higher degrees does not result in improving the original eSCN baseline (row 8), and that the proposed architectural changes are necessary.

**Scaling of Parameters**. In Tables 0(c), 0(d), 0(e), we systematically vary the maximum degree \(L_{max}\), the maximum order \(M_{max}\), and the number of Transformer blocks and compare with equivalent eSCN variants. There are several key takeaways. First, across all experiments, EquiformerV2 performs better than its eSCN counterparts. Second, while one might intuitively expect higher resolution features and larger models to perform better, this is only true for EquiformerV2, not eSCN. For example, increasing \(L_{max}\) from \(6\) to \(8\) or \(M_{max}\) from \(3\) to \(4\) degrades the performance of eSCN on energy predictions but helps that of EquiformerV2. In Table 0(b), we show that longer training regimes are crucial. Increasing the training epochs from \(12\) to \(30\) with \(L_{max}=6\) improves force and energy predictions by \(5\%\) and \(2.5\%\), respectively.

**Comparison of Speed-Accuracy Trade-offs**. To be practically useful for atomistic simulations and material screening, models should offer flexibility in speed-accuracy tradeoffs. We compare these trade-offs for EquiformerV2 with prior works in Figure 3(a). Here, the speed is reported as the number of structures processed per GPU-second during inference and measured on V100 GPUs.

Table 1: Ablation results with EquiformerV2. We report mean absolute errors for forces in meV/Å and energy in meV, and lower is better. All models are trained on the 2M subset of OC20 , and errors are averaged over the four validation splits of OC20. The base model setting is marked in gray.

[MISSING_PAGE_FAIL:8]

adsorption energies within a \(0.1\)eV margin of DFT results with an \(87\%\) success rate. This is done by using OC20-trained models to perform structure relaxations for an average \(90\) configurations of an adsorbate placed on a catalyst surface, followed by DFT single-point calculations for the top-\(k\) structures with lowest predicted relaxed energies, as a proxy for calculating the global energy minimum or adsorption energy. We refer the reader to the AdsorbML paper  for more details. We benchmark AdsorbML with EquiformerV2, and Table 3 shows that it improves over SCN by a significant margin, with \(8\%\) and \(5\%\) absolute improvements at \(k=1\) and \(k=2\), respectively. Moreover, EquiformerV2 at \(k=2\) is more accurate at adsorption energy calculations than all the other models even at \(k=5\), thus requiring at least \(2\) fewer DFT calculations.

## 6 Conclusion

In this work, we investigate how equivariant Transformers can be scaled up to higher degrees of equivariant representations. We start by replacing \(SO(3)\) convolutions in Equiformer with eSCN convolutions, and propose three architectural improvements to better leverage the power of higher degrees - attention re-normalization, separable \(S^{2}\) activation and separable layer normalization. With these modifications, we propose EquiformerV2, which outperforms state-of-the-art methods on the S2EF, IS2RS, and IS2RE tasks on the OC20 dataset, improves speed-accuracy trade-offs, and achieves the best success rate when used in AdsorbML.

Broader Impacts.EquiformerV2 achieves more accurate approximation of quantum mechanical calculations and demonstrates one further step toward replacing DFT force fields with machine learned ones. By demonstrating its promising results, we hope to encourage the community to make further progress in applications like material design and drug discovery than to use it for adversarial purposes. Additionally, the method only facilitates identification of molecules or materials of specific properties, and there are substantial hurdles from their large-scale deployment. Finally, we note that the proposed method is general and can be applied to different problems like protein structure prediction  as long as inputs can be modeled as 3D graphs.

Limitations.Although EquiformerV2 improves upon state-of-the-art methods on the large and diverse OC20 dataset, we acknolwdge that the performance gains brought by scaling to higher degrees and the proposed architectural improvements can depend on tasks and datasets. For example, the increased expressivity may lead to overfitting on smaller datasets like QM9  and MD17 . However, the issue can be mitigated by pre-training on large datasets like OC20  and PCQM4Mv2  optionally via denoising  and then finetuning on smaller datasets.