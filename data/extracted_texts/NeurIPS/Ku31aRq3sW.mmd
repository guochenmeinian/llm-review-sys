# How to Solve Contextual Goal-Oriented Problems with Offline Datasets?

Ying Fan\({}^{1}\), Jingling Li\({}^{2}\), Adith Swaminathan\({}^{3}\), Aditya Modi\({}^{3}\), Ching-An Cheng\({}^{3}\)

\({}^{1}\)University of Wisconsin-Madison \({}^{2}\)ByteDance Research \({}^{3}\)Microsoft Research

###### Abstract

We present a novel method, Contextual goal-Oriented Data Augmentation (CODA), which uses commonly available unlabeled trajectories and context-goal pairs to solve Contextual Goal-Oriented (CGO) problems. By carefully constructing an action-augmented MDP that is equivalent to the original MDP, CODA creates a fully labeled transition dataset under training contexts without additional approximation error. We conduct a novel theoretical analysis to demonstrate CODA's capability to solve CGO problems in the offline data setup. Empirical results also showcase the effectiveness of CODA, which outperforms other baseline methods across various context-goal relationships of CGO problem. This approach offers a promising direction to solving CGO problems using offline datasets.

## 1 Introduction

Goal-oriented problems  are an important class of sequential decision-making problems with widespread applications, ranging from robotics , game-playing , to logistics . In particular, many real-world goal oriented problems are _contextual_, where the objective of the agent is to reach a goal set communicated by a context. For example, consider instructing a truck operator with the context "Deliver goods to a warehouse in the Bay area". Given such a context and an initial state, it is acceptable to reach any feasible goal (a reachable warehouse location) in the goal set (warehouse locations including non-reachable ones). We call such problems _Contextual Goal-Oriented_ (CGO) problems, which form an important special case of contextual Markov Decision Process (MDP) .

CGO is a practical setup that includes goal-conditioned reinforcement learning (GCRL) as a special case (the context in GCRL is just the target goal), but in general contexts in CGO problem can be more abstract (like high-level task instructions in the above example) and the relationship between contexts and goals are not known beforehand. CGO problems are challenging because 1) the rewards are sparse as in GCRL and 2) the contexts can be difficult to map into feasible goals. Nevertheless, CGO problem has an important structure that the transition dynamics (e.g., navigating a city road network) are independent of the contexts that specify tasks. Therefore, efficient multitask learning can be achieved by sharing dynamics data across tasks.

In this paper, we study solving for CGO problems in an offline setup. We suppose access to two datasets -- an (unlabeled) _dynamics_ dataset of trajectories, and a (labeled) _context-goal_ dataset containing pairs of contexts and goal examples. Such datasets are commonly available in practice. The typical contextual datasets for imitation learning (IL) (which has pairs of contexts and expert trajectories) is one example, since we can convert the contextual IL data into dynamics data and context-goal pairs. Generally, this setup also covers scenarios where expert trajectories are _not_ accessible (e.g., because of diverse contexts and initial states), since it does not assume goal examples to appear in the trajectories or the contexts are readily paired with transitions in expert trajectories. Instead, it allows the dynamics datasets and the context-goal datasets to be independently collected. For example, in robotics, task-agnostic play data can be obtained at scale  in an unsupervised manner whereas instruction datasets (e.g., ) can provide context-goal pairs. In navigation, selfdriving car trajectories (e.g., [35; 32]) also allow us to learn dynamics whereas landmarks datasets (e.g. [24; 9]) provide context-goal pairs.

While offline CGO problems as described above are common in practical scenarios, to our knowledge, no algorithms have been specifically designed to solve such problems and CGO has not been formally studied yet. Some baseline methods could be easily conceptualized from the literature, but their drawbacks are equally apparent. One intuitive approach is to extend the goal prediction methods in GCRL [26; 27]: given a test context, we can predict a goal and navigate to it using a goal-conditioned policy, where the goal prediction model can be learned from the context-goal dataset and the goal-conditioned policy can be learned from the trajectory dataset. However, the predicted goal might not always be feasible given the initial state since our context-goal dataset is not necessarily paired with transitions. Alternatively, the offline problem could be formulated as a special case of missing label problems  and we can learn a context-conditioned reward model to label the unsupervised transitions when paired with contexts as in . However, this approach ignores the goal-oriented nature of the problem and the fact that here only positive data (i.e. goal examples) are available for reward learning, which poses extra significant challenges. CGO can be framed as an offline reinforcement learning (RL) problem with missing labels; However, existing algorithms [42; 14; 21] in family assume access to both positive data (contexts-goal pairs) and negative data (contexts and non-goal examples), whereas only positive data are available here.

In this work, we present the first precise formalization of the CGO setting, and propose a novel Contextual goal-Oriented Data Augmentation (CODA) technique that can provably solve CGO problems subject to natural assumptions on the datasets' quality. The core idea is to **convert the context-goal dataset and the unsupervised dynamics dataset to a fully labeled transition dataset of an equivalent action-augmented MDP**, which circumvents the drawbacks in other baseline methods by fully making use of the CGO structure of the problem. We give a high-level illustration of this idea in Figure 1. In Figure 1, given a randomly sampled context-goal pair from the context-goal dataset, we create fictitious transitions from the corresponding goal example to a fictitious terminal state with a fictitious action and reward 1, and pair with the corresponding context. Also, we label all unsupervised transitions with reward 0 and non-terminal, and pair with the contexts randomly. Combining the two, we then have a fully labeled dataset (of an action-augmented contextual MDP, which this data augmentation and relabeling process effectively creates), making it possible to propagate supervision signals from the context-goal dataset to unsupervised transitions via the Bellman equation. We can then apply any offline RL algorithm based on Bellman updates like CQL , IQL , PSPI , ATAC  etc. In comparison with the baseline methods discussed earlier, our method naturally circumvents their intrinsic challenges: 1) CODA directly learns context-conditioned policy and avoids the need to predict goals; 2) CODA effectively uses a fully labeled dataset, avoiding the need to learn a reward model and extra costs from inaccurate reward modeling.

## 2 Related Work

Offline RL.Offline RL methods have proven to be effective in goal-oriented problems as it also allows learning a common set of sub-goals/skills [3; 23; 38]. A variety of approaches are used to mitigate the distribution shift between the collected datasets and the trajectories likely to be generated by learned policies: 1) constrain target policies to be close to the dataset distribution [8; 36; 7], 2) incorporate value pessimism for low-coverage or Out-Of-Distribution states and actions [19; 40; 15] and 3) adversarial training via a two-player game [37; 4].

Figure 1: Illustration of CODA: We create fictitious transitions from goal examples to terminal states under the given context in the action-augmented MDP with reward 1, which enables the supervised signal to propagate back to unsupervised transitions via Bellman equation.1

Offline RL with unlabeled data.Our CGO setting is a special case of offline RL with unlabeled data, or more broadly the offline policy learning from observations paradigm : There is only a subset of the offline data labeled with rewards (in our setting, that is the contexts dataset, as we don't know which samples in the dynamics dataset are goals.). However, the MAHALO scheme in  is much more general than necessary for CGO problems, and we show instead that our CODA scheme has better theoretical guarantees than MAHALO in Section 5. In our experiments, we compare CGO with several offline RL algorithms designed for unlabeled data: UDS  where unlabeled data is assigned zero rewards and PDS  where a pessimistic reward function is learned from a labeled dataset.

Goal-conditioned RL (GCRL).GCRL is a special case of our CGO setting, which has been extensively studied since . There are two critical aspects of GCRL: 1) data relabeling to make better use of available data and 2) learning reusable skills to solve long-horizon problems by chaining sub-goals or skills. On the one hand, hindsight relabeling methods  are effective by reusing visited states in the trajectories as successful goal examples. For 2), hierarchical methods for determining sub-goals, and training goal reaching policies have been effective in long-horizon problems . Another key objective of GCRL is goal generalization. Popular strategies include universal value function approximators , unsupervised representation learning , and pessimism-induced generalization in offline GCRL formulations . Our CGO framing enables both data reuse and goal generalization, by using contextual representations and a reduction to offline RL to combine dynamics and context-goal datasets.

Data-sharing in RLSharing information across multiple tasks is a promising approach to accelerate learning and to identify transferable features across tasks. In RL, both multi-task and transfer learning settings have been studied under varying assumption on the shared properties and structures of different tasks . For data sharing in CGO, we adopt the contextual MDP formulation , which enables knowledge transfer via high-level contextual cues. Prior work on offline RL has also shown the utility of sharing data across tasks: hindsight relabeling and manual skill grouping , inverse RL , sharing Q-value estimates  and reward labeling .

## 3 Preliminaries

In this section, we introduce the setup of CGO problems, infinite-horizon formulation for CGO, and the offline learning setup with basic assumptions for our offline dataset.

CGO SetupA Contextual Goal-Oriented (CGO) problem describes a multi-task goal-oriented setting with a _shared_ transition kernel. We consider a Markovian CGO problem, defined by the tuple \(=(,,P,R,,C,d_{0})\), where \(\) is the state space, \(\) is the action space, \(P:()\) is the transition kernel, \(R:\{0,1\}\) is the sparse reward function, \([0,1)\) is the discount factor, \(\) is the context space, and \(\) denotes the space of distributions.

Each context \(c\) specifies a goal-reaching task with a goal set \(G_{c}\), and reaching any goal in the goal set \(G_{c}\) is regarded as successful, inducing the reward function \(R(s,c)=(s G_{c})\). An episode of a CGO problem starts from an initial state \(s_{0}\) and a context \(c\) sampled from \(d_{0}(s_{0},c)\), and terminates when the agent reaches the goal set \(G_{c}\). \(c\) does not change during the transition; only \(s_{t}\) changes according to \(P(s^{}|s,a)\) and the transition kernel is context-independent.

Infinite-horizon Formulation for CGO setupA fictitious zero-reward absorbing state \(s^{+}\) can translate termination after reaching the goal to an infinite horizon formulation: _whenever the agent enters \(G_{c}\) it transits to \(s^{+}\) in the next step (for all actions) and stays at \(s^{+}\) forever_. This is a standard technique to convert a goal-reaching problem (with a random problem horizon) to an infinite horizon problem. This translation does _not_ change the problem, but allows cleaner analyses. We adopt this formulation in the following.

We give details of this infinite-horizon conversion in the following. First, we extend the reward and the dynamics: Let \(}=\{s^{+}\}\), \(:=\), and \(}:=}\). Define \(^{+}:=\{x:x=(s,c),s=s^{+},c\}\). With abuse of notation, we define the reward and transition on \(}\) as \(R(x)=(s G_{c})\)where \(x=(s,c)\). The transition kernel \(P(x^{}|x,a) P(s^{}|s,c,a)(c^{}=c)\), where

\[P(s^{}|s,c,a)=(s^{}=s^{+})&$ or $s=s^{+}$,}\\ P(s^{}|s,a)&\]

Given a policy \(:()\), the state-action value function (i.e., Q function) is \(Q^{}(x,a)_{,P}[_{t=0}^{}^{t}R(x) |x_{0}=x,a_{0}=a]\). \(V^{}(x) Q^{}(x,)\) is the value function given \(\), where \(Q(x,)_{a}[Q(x,a)]\). The return \(J()=V^{}(d_{0})=Q^{}(d_{0},)\). \(^{*}\) is the optimal policy that maximized \(J()\) and \(Q^{*} Q^{^{*}}\), \(V^{*} V^{^{*}}\). Let \(G\) represent the goal set on \(\), that is, \(G\{x:x=(s,c),s G_{c}\}\).

Offline Learning for CGOWe aim to solve CGO problems using offline datasets without additional online environment interactions, namely, by offline RL. We identify two types of data that are commonly available: \(D_{}\{(s,a,s^{})\}\) is an _unsupervised_ dynamics dataset of agent trajectories collected from \(P(s^{}|s,a)\), and \(D_{}\{(c,s):s G_{c}\}\) is a _supervised_ dataset of context-goal pairs, which can be easier to collect than expert trajectories. We suppose that there are two distributions \(_{}(s,a,s^{})\) and \(_{}(s,c)\), where \(_{}(s^{}|s,a)=P(s^{}|s,a)\) and \(_{}\) has support within \(G_{c}\), i.e., \(_{}(s|c)>0 s G_{c}\). We assume that \(D_{}\) and \(D_{}\) are i.i.d. samples drawn from the distributions \(_{}\) and \(_{}\), i.e.,

\[D_{}=\{(s_{i},a_{i},s^{}_{i})_{}\},D_{ {goal}}=\{(s_{j},c_{j})_{}\}.\]

Notice that we do not assume the goal states in \(D_{}\) to be in \(D_{}\), thus we cannot always naively pair transitions in \(D_{}\) with contexts in \(D_{}\) and assign them with reward \(1\). To our knowledge, no existing algorithm can provably learn near-optimal \(\) using only the positive \(D_{}\) data (i.e., without non-goal examples) when combined with \(D_{}\) data.

## 4 Contextual Goal-Oriented Data Augmentation (CODA)

The key idea of CODA is the construction of an _action_-augmented MDP with which the dynamics and context-goal datasets can be combined into a fully labeled offline RL dataset. In the following, we first describe this action-augmented MDP (Section 4.1) and show that it preserves the optimal policies of the original MDP (Appendix A.1). Then we outline a practical algorithm to convert the two datasets of an offline CGO problem into a dataset for this augmented MDP (Section 4.2) such that any generic offline RL algorithm based on Bellman equation can be used as a solver.

### Action-Augmented MDP

We propose an action-augmented MDP (shown in Figure 1), which augments the action space of the contextual MDP in Section 3 with _a fictitious action_\(a^{+}\).

Let \(}=\{a^{+}\}\). We define the reward of this action-augmented MDP to be _action-dependent_: for \(x=(s,c)}\), \((x,a)(s G_{c})(a=a^{+}),\) which means the reward is 1 only if \(a^{+}\) is taken in the goal set, otherwise 0.

We also extend the transition upon taking action \(a^{+}\); \((x^{}|x,a^{+})(s^{}=s^{+}),\) and maintain the transition with real actions: \((x^{}|x,a) P(s^{}|s,a)(c^{}=c),\) which means whenever taking \(a^{+}\), the agent would always transit to \(s^{+}\), and the transition remains the same as in the original MDP given real actions. Further, we implement \(s^{+}\) as \(=\).

We define this augmented MDP as \(}(},},, ,)\).

Policy conversion.For a policy \(:()\) in the original MDP, define its extension on \(}\):

\[(a|x)=(a|x),&x G,\\ a^{+},&\] (1)

Regret equivalence.An observation that comes with the construction is that if a policy is optimal in the original MDP, we can easily use the extension above to create an optimal policy in the augmented one. If a policy is optimal in the augmented MDP, it must take \(a^{+}\) only when \(x G\) (otherwise the return is lower, due to entering \(s^{+}\) too early), thus we can revert this optimal policy of the augmented MDP to find an optimal policy in the original MDP without changing its behavior and performance. We stated this property below; details can be found as Lemma A.3 in Appendix A.1.

**Theorem 4.1** (Informal).: _The regret of a policy extended to the augmented MDP is equal to the regret of the policy in the original MDP, and any policy defined in the augmented MDP can be converted into that in the original MDP without increasing the regret. Thus, solving the augmented MDP can yield correspondingly optimal policies for the original problem._

**Remark 4.2**.: _The benefit of using the equivalent \(}\) is to avoid missing labels: given contexts in \(D_{}\), the rewards in \(}\) are known from our dataset setup in Section 3, whereas the rewards of the original MDP \(\) are missing._

### Method

CODA is designed based on the observation on regret relationship in Theorem 4.1: As described in Figure 1, given a context-goal pair \((s,c)\) from the dataset \(D_{}\), we create a fictitious transition from \(s\) to \(s^{+}\) with action \(a^{+}\), reward \(1\) under context \(c\). We also label all unsupervised transitions in the dataset \(D_{}\) with the original action and reward \(0\) under \(c\). In this way, we can have a fully labeled transition dataset in the augmented MDP given any \(c\) from the context-goal dataset and then run offline algorithms (based on the Bellman equation) on this dataset. This CODA algorithm is formally stated in Algorithm 1. It takes two datasets \(D_{}\) and \(D_{}\) as input, and produces a labeled transition dataset \(_{}_{}\) that is suitable for use by any offline RL algorithm based on Bellman equation like CQL , IQL , PSPI , ATAC , etc.

**Interpretation.** Why would our action augmentation make sense? We consider dynamic programming on the created dataset. Imagine we have a fictitious transition from \(s\) to \(s^{+}\) with \(a^{+}\) under context \(c\). When we calculate \(V^{*}(x)\) via Bellman equation where \(x=(s,c)\), it will choose the action with the highest \(Q^{*}\) value in the augmented action space. The fictitious action would be the optimal action since it induces the highest \(Q^{*}\) value2, meaning \(s\) is already in \(G_{c}\), and no further action is needed. Then the value of \(V^{*}(x)\) would _naturally propagate to some state \(x_{}=(s_{},c)\) via Bellman equation if \(x\) is reachable starting from \(x_{}\)_ as shown in Figure 1, so \(x_{}\) would still have meaningful values even with the intermediate reward 0. For \(x\) to be reachable starting from \(x_{}\), we do not require the exact \(s\) to appear in the trajectory dataset due to the generalization ability of the value function (details in Section 5). For non-goal states, such fictitious action never appears in the dataset, thus it would not be the optimal action in Bellman equation in pessimistic offline RL. For example, the fictitious action never appears as the candidate in argmax in algorithms like IQL, and would be punished as OOD actions in algorithms like CQL. We will prove this insight formally below in Section 5.

**Input**: Dynamics dataset \(D_{}\), context-goal dataset \(D_{}\)

**for** each sample \((s,c) D_{}\)**do**

Create transition3\((x,a^{+},1,x^{+})\), where \(x=(s,c)\) and \(x^{+}=(s^{+},c)\), add it to \(_{}\)

**end for**

**for** each \((s,a,s^{}) D_{}\)**do**

**for** each \((,c)_{}\)**do**

Create transition \((x,a^{+},0,x^{})\), where \(x=(s,c)\) and \(x^{}=(s^{},c)\), add it to \(_{}\)

**end for**

**end for**

**Output**: \(_{}\) and \(_{}\)

**Algorithm 1** CODA for CGO

**Remark 4.3**.: _We do not need to learn to perform \(a^{+}\) for the policy in practice since it is only for fictitious transitions which is already inside the goal set in the original MDP. (From the proof of Lemma A.3, we know taking \(a^{+}\) is always strictly worse than taking actions in the original action space \(\).) Therefore, we simply use the original action space for policy modeling and only use the fictitious transitions in value learning. We note that in practice Algorithm 1 can be implemented as a pre-processing step in the minibatch sampling of a deep offline RL algorithm (as opposed to computing the full \(_{}\) and \(_{}\) once before learning)._CGO is Learnable with Positive Data Only

In Section 4, we show that a fully labeled dataset can be created in the augmented MDP without inducing extra approximation errors. But we still have no access to negative data, i.e., context and non-goal pairs. A natural question arises: _Can we learn to solve CGO problems with positive data only? What conditions are needed for CGO to be learnable with offline datasets?_

We show in theory that we do _not_ need negative data to solve CGO problems by conducting a formal analysis for our method, instantiated with PSPI  as an example of the base algorithm. We present the detailed algorithm CODA+PSPI in Appendix A.3. This algorithm uses function classes \(:\) and \(:\) to model value functions and optimizes the policy given a policy class \(\) based on absolute pessimism defined on initial states.

We present our assumptions and the main theoretical result as follows.

**Assumption 5.1** (Realizability).: _We assume for any \(\), \(Q^{}\) and \(R\), where \(,\) are the function classes for action-value and reward respectively._

**Assumption 5.2** (Completeness).: _We assume: For any \(f\), \(g\) and \(\), \((g(x),f(x,))\); And for any \(f\), \(\), \(^{}f(x,a)\), where \(^{}\) is a zero-reward Bellman backup operator with respect to \(P(s^{}|s,a)\): \(^{}f(x,a)_{x^{} P(s^{} |s,a)(c^{}=c)}[f(x^{},)]\)._

These two assumptions mean that the function classes \(\) and \(\) are expressive enough, which are standard assumptions in offline RL based on Bellman equation . For deriving our main result, we define the coverage assumption needed below.

**Definition 5.3**.: _We define the generalized concentrability coefficients:_

\[_{}()_{f,f^{}} {\|f-^{}f^{}\|_{^{}_{^{}_{^{ }_{}}}}^{2}}{\|f-^{}f^{}\|_{_{}} ^{2}}_{}()_{g }_{^{}_{}}}^{2}}{\|g- R\|_{^{}_{^{}_{}}}^{2}}\] (2)

_where \(\|h\|_{}^{2}_{x}[h(x)^{2}]\), \(^{}_{^{}_{}}(x,a)=_{,P}[_{t=0}^{ T-1}^{t}(x_{t}=x,a_{t}=a)]\), \(^{}_{}(x)=_{,P}[^{T}(x_{T}=x)]\), and \(T\) is the first time the agent enters the goal set._

Concentrability coefficients is a generalization notion of density ratio: It describes how much the (unnormalized) distribution in the numerator is "covered" by that in the denominator in terms of the generalization ability of function approximators . If \(_{}(),_{}()\) are finite given \(_{},_{},,\) and \(\), then we say \(\) is covered by the data distributions, and conceptually offline RL can learn a policy to be no worse than \(\).

We now state our theoretical result, which is proven by a careful reformulation of the Bellman equation of the action-augmented MDP, and construct augmented value function and policy classes in the analysis using the CGO structures (see Appendix A).

**Theorem 5.4**.: _Let \(^{}\) denote the learned policy of CODA + PSPI with datasets \(D_{}\) and \(D_{}\), using value function classes \(=\{\}\) and \(=\{\}\). Under Assumption 5.1, 5.2 and 5.3, with probability \(1-\), it holds, for any \(\),_

\[J()-J(^{})_{}()(|||||/)}{|D_{}|}}+|||||/)}{|D_{}|}})+ _{}()|/)}{|D_{ }|}}\]

_where \(_{}()\) and \(_{}()\) are concentrability coefficients4._

Interpretation.We can interpret Theorem 5.4 as follows: The statistical errors in value function estimation would decrease as we have more data from \(_{}\) and \(_{}\); For any comparator \(\) with finite coefficients \(_{}(),_{}()\), the final regret upper bound would also decrease. Taking \(=^{*}\) as an example. For the coefficients \(_{}(),_{}()\) to be finite, it indicates 1) the state-action distribution from the dynamics data "covers" the trajectories generated by \(^{*}\), which includes the case of stitching5; 2) the support of \(_{}\) "covers" the goals \(^{*}\) would reach. We note that these conditions are _not_ any stronger than general requirements to solve offline algorithms: The "coverage"above is measured based on the generalization ability of \(f\) and \(g\) respectively as in Definition 5.3; e.g., if \(f(x_{1})\) and \(f(x_{2})\) are similar for \(x_{1} x_{2}\), then \(x_{2}\) is within the coverage of \(\) so long as \(x_{1}\) can be generated by \(\) in terms of the generalization ability of \(f\). Such a coverage condition is weaker than coverage conditions based on density ratios. Besides, Theorem 5.4 simultaneously apply to all \(\) not just \(^{*}\). Therefore, as long as the above "coverage" conditions hold for any policy \(\) that can reach the goal set, the agent can learn to reach the goal set. Thus, we show that CODA with PSPI can provably solve CGO without the need for additional non-goal samples, i.e., CGO is learnable with positive data only.

**Remark 5.5**.: _Here we only require function approximation assumptions made in the original MDP, without relying on functions defined on the fictitious action or completeness assumptions based on the fictitious transition. As a result, our theoretical results are comparable with those of other approaches._

**Remark 5.6**.: _MAHALO  is a SOTA offline RL algorithm that can provably learn from unlabeled data. One version of MAHALO is realized on top of PSPI in theory; however, their theoretical result (Theorem D.1) requires a stronger version concentrability, \(_{g}}^{2}}}{{\|g- \|_{g}^{2}}}\), to be small. In other words, it needs negative examples of (context, non-goal state) tuples for learning._

**Intuition for other base algorithms.** Notice that PSPI is just one instantiation. Conceptually, the coverage conditions above also make sense for other pessimistic offline RL instantiations based on the Bellman equation (like IQL), since the key ideas used in the above analyses are that the regret relationship (Theorem 4.1) between the original MDP and the action augmented MDP (which is algorithm agnostic) and that pessimism together with Bellman equations can effectively propagate information from the context-goal dataset (without the need for negative data). However, performing complete theoretical analyses of CODA for all different offline RL algorithms is out of the scope of this paper.

## 6 Experiments

In this section, we present the experimental setup and results for CODA. Code is publicly available at: https://github.com/yingfan-bot/coda.

For a comprehensive empirical study, we first introduce the diverse spectrum of practical CGO setups.

**Diverse spectrum of practical CGO problems.** The main challenge of the CGO problem compared with traditional goal-conditioned RL is the potential complexity in the context-goal relationship. Therefore, to showcase the efficacy of different methods, we construct three levels with _increasing difficulty_ as shown in Figure 2: (a) has a similar complexity as a single-task problem where the context does not play a significant role; (b) requires a context-dependent policy but only has finite contexts; (c) has infinite continuous context, requiring a context-dependent policy and generalization ability to contexts outside the offline data set. We aim to answer the following questions: 1) Does our method work under the data assumptions in Section 3, with different levels of context-goal complexity? 2) Is there any empirical benefit from using CODA, compared with baseline methods including reward learning, goal prediction, etc?

Figure 2: Illustration of the context-goal relationship with increasing complexity (Each red boundary defines a goal set with its center location as context). (a) Contexts and goal sets are very similar such that it could be approximately solved by a context-agnostic policy. (b) Contexts are finite, and different contexts map to distinct goal sets, which requires context-dependent policies. (c) Contexts are continuous and infinite. The context-goal mapping is neither one-to-many nor many-to-one, creating a CGO problem with full complexity.

### Environments and Datasets

**Dynamics dataset.** For all experiments, we use the original AntMaze-v2 datasets (3 different mazes and 6 offline datasets) of D4RL  as dynamics datasets \(D_{}\), removing all rewards and terminals.

**Context-goal dataset.** We construct three levels of context and goal relationships as shown in Figure 2. For each setup, we first define the context set, and then sample a fixed set of states from the offline trajectory dataset that satisfies the context-goal relationship, and then randomly _perturb_ the states such that there would be no way to directly match goal examples to some states in the trajectories given contexts. Notice that this context-goal relationship is only used for dataset construction and is not accessible to the learning algorithm.6 The specific context-goal relationship are discussed in Section 6.3 with the construction/evaluation details in Appendix B.2.

### Method and Baselines

For controlled experiments, we use IQL  as the same backbone offline algorithm for all the methods with the same set of hyperparameters. Our choice of IQL is motivated by both its benchmarked performance on several RL domains and its structural similarity to PSPI (use of value/policy function classes along with pessimism). Please see Appendix B.1 for hyperparameters.

We describe the algorithms compared in the experiments.

**CODA.** We apply CODA in Algorithm 1 with IQL as the offline RL algorithm to solve the augmented MDP defined in Section 4.1 More specifically, we set \(a^{+}\) to be an extra dimension in the action space of the action-value function, and model the policy with the original action space. Empirically, we found that equally balancing the samples \(_{}\) and \(_{}\) generates the best result7. Then we apply IQL on this labeled dataset.

**Reward prediction.** For this family of baselines, we need to use the learned reward to predict the label of context-goal samples in the randomly sampled context-transition pairs during training, so we need to pre-train a reward model using the context-goal dataset. We use PDS  for reward modeling, and learn a _pessimistic_ reward function using ensembles of models on the context-goal dataset. Then we apply the reward model to label the transitions with contexts, run IQL on this labeled dataset, and get a context-dependent policy. Besides PDS, we also test naive reward prediction (RP, which follows the same setup of PDS but without ensembles) and UDS  +RP in Section 6.3 (See details in Appendix B.1). Additionally, we add results from training with the oracle reward (marked as "Oracle Reward") where we provide the oracle reward for any query context-goal pairs, as a reference of the performance upper bound for reward prediction methods.

**Goal prediction.** We consider another GCRL-based baseline. Notice that the relationship between contexts and goals is unknown in CGO, we cannot directly apply traditional GCRL methods to CGO problems. Therefore, we adopt a workaround to use GCRL methods: We learn a conditional generative model as the goal predictor using classifier-free diffusion guidance , where the contexts serve as the condition, and the goal examples are used to train the generative model. We also learn a general goal-conditioned policy with the dynamics-only dataset using HER +IQL. Given a test context, the goal predictor samples the goal given the context, which is then passed as the condition to the policy.

### Results

**Original AntMaze: Figure 2(a).** In the original AntMaze, 2D goal locations (contexts) are limited to a small area as in Figure 2 (a). To make it a CGO problem, we make the test context visible to the agent. This setting in Figure 2 is approximately a single-task problem.

CODA generally achieves better performance than reward learning and goal prediction methods. Comparing the normalized return in each AntMaze environment for all methods, our method consistently achieves equivalent or better performance in each environment compared to other baselines(Table 1). 8 Moreover, the performance of Goal Prediction is rather poor, which mainly comes from not enough goal examples to learn from in this setup due to a limited goal area.

We show the normalized return (average success rate in percentage) in each modified Four Rooms environment for our method and baseline methods in Table 2, where our method consistently outperforms the performances of baseline methods.

**Random Cells: Figure 2(c).** We use a diverse distribution of contexts as shown in Figure 2(c), where the contexts are randomly sampled from non-wall states. For test contexts, we have two settings: 1) sampling from the training distribution; 2) sampling from a far-away area from the start states.

Overall, CODA outperforms the baselines under the setup in Figure 2(c). We show the normalized return (average success rate in percentage) in each modified Random Cells environment in Table 3, which also shows the generalization ability of our method in the context space. CODA also generalizes to a different test context distribution: We also test with a distribution shift of the contexts in Table 4. We can observe that when tested with this different context distribution, CODA still generates better overall results compared to reward learning and goal prediction baselines.

**Reference to training with oracle reward.** Notice that training with oracle reward is the skyline performance. From the results, training with oracle reward does not generally improve the performance much compared to CODA, though it generally outperforms PDS and Goal Prediction. This is mainly due to the sparsity of the positive samples in the randomly sampled context-transition pairs. On the other hand, CODA easily uses these positive examples via our augmentation, which is another advantage of our method over reward prediction baselines.

   Env/Method & CODA (Ours) & PDS & Goal Prediction & RP & UDS+RP & Oracle Reward \\  unaze & **94.8±1.3** & 93.0±1.3 & 46.4±6.0 & 50.5±2.1 & 54.3±6.3 & 94.4±0.61 \\ unaze diverse & **72.8±7.7** & 50.6±7.8 & 42.8±4.4 & **72.8±2.6** & 71.5±4.3 & 76.8±5.44 \\ medium play & **75.8±1.9** & 66.8±4.9 & 43.8±4.7 & 0.5±0.3 & 0.3±0.3 & 80.6±1.56 \\ medium diverse & **84.5±5.2** & 22.8±2.4 & 28.6±3.9 & 0.5±0.5 & 0.8±0.5 & 72.4±4.26 \\ large play & **60.0±7.6** & 39.6±4.9 & 13.0±4.0 & 0±0 & 0±0 & 41.2±3.58 \\ large diverse & **36.8±6.9** & 30.0±5.3 & 12.6±2.7 & 0±0 & 0±0 & 34.2±2.59 \\  average & **70.8** & 50.5 & 31.2 & 20.7 & 21.2 & 66.6 \\   

Table 1: Average success rate (%) in AntMaze-v2 from all environments.

   Env/Method & CODA (Ours) & PDS & Goal Prediction & Oracle Reward \\  medium-play & **78.7±0.9** & 46.0±4.47 & 59.3±2.6 & 77.7±2.0 \\ medium-diverse & **83.6±1.9** & 51.3±3.6 & 66.7±2.4 & 87.4±1.2 \\ large-play & **65.5±2.5** & 13.9±2.4 & 41.4±3.6 & 67.2±2.7 \\ large-diverse & **72.2±2.9** & 11.1±3.8 & 42.0±3.0 & 69.6±3.1 \\  average & **75.0** & 30.6 & 52.4 & 75.5 \\   

Table 2: Average scores from Four Rooms with perturbation. The score for each run is the average success rate (%) of the other three rooms.

   Env/Method & CODA (Ours) & PDS & Goal Prediction & Oracle Reward \\  medium-play & **76.8±6.1** & 52.0±8.8 & 66.7±7.2 & 71.9±0.1 \\ medium-diverse & **78.2±6.5** & 60.9±11.3 & 69.7±8.7 & 79.3±6.1 \\ large-play & **57.6±12.4** & 50.6±6.4 & 42.4±8.2 & 49.4±9.3 \\ large-diverse & 54.7±8.8 & **58.3±9.2** & 44.2±8.1 & 58.2±3.4 \\  average & **66.8** & 55.5 & 55.8 & 64.7 \\   

Table 3: Average scores from Random Cells. The score for each run is the average success rate (%) of random test contexts from the same training distribution.

**Evaluation of the Reward Model.** We also visualize the learned reward model from reward learning baselines in Appendix B.3: PDS is consistently better at separating positive and negative datasets than UDS and naive RP, but PDS can still fail at fully separating positive and negative examples. Intuitively, our method does not require reward learning thanks to the construction of the augmented MDP, which avoids the extra errors in reward prediction and leads to better performance.

### Discussion and Limitation

Our experiments are limited to low-dimensional simulations. Nevertheless, the success of our method with diverse context-goal relationships serves as a first milestone to showcase its effectiveness, and we believe CODA would be useful in real-world settings (e.g., learning visual-language robot policies) for its simplicity and theoretical guarantees. Potential scaling up by incorporating features from large pretrained models would be an exciting future direction, which can make our method generalizable to the real world.

## 7 Conclusion

We propose CODA for offline CGO problems, and prove CODA can learn near-optimal policies without the need for negative labels with natural assumptions. We also validate the efficacy of CODA experimentally, and find it outperforms other reward-learning and goal prediction baselines across various CGO complexities. We believe our method has the potential to generalize to real-world applications by further scaling up.

   Env/Method & CODA (Ours) & PDS & Goal Prediction & Oracle Reward \\  medium-play & 67.9\(\)8.2 & 50.1\(\)13.4 & **70.5\(\)1.9** & 67.2\(\)7.2 \\ medium-diverse & **72.5\(\)6.5** & 57.5\(\)14.8 & 63.0\(\)7.2 & 68.7\(\)7.9 \\ large-play & **60.2\(\)4.8** & 48.1\(\)8.0 & 44.3\(\)4.1 & 59.8\(\)4.4 \\ large-diverse & **58.0\(\)5.8** & 44.1\(\)9.9 & 55.4\(\)5.7 & 57.6\(\)7.6 \\  average & **64.7** & 49.9 & 58.3 & 63.3 \\   

Table 4: Average scores from Random Cells with perturbation. The score for each run is the average success rate (%) of random test contexts with a distribution shift.

[MISSING_PAGE_FAIL:11]

*  Piotr Mirowski, Matthew Koichi Grimes, Mateusz Malinowski, Karl Moritz Hermann, Keith Anderson, Denis Teplyashin, Karen Simonyan, Koray Kavukcuoglu, Andrew Zisserman, and Raia Hadsell. Learning to navigate in cities without a map. In _NeurIPS_, 2018.
*  Dipendra K Misra, Jaeyong Sung, Kevin Lee, and Ashutosh Saxena. Tell me dave: Context-sensitive grounding of natural language to manipulation instructions. _International Journal of Robotics Research_, 35(1-3):281-300, 2016.
*  Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and Sergey Levine. Visual reinforcement learning with imagined goals. In _NeurIPS_, 2018.
*  Ashvin Nair, Shikhar Bahl, Alexander Khazatsky, Vitchyr Pong, Glen Berseth, and Sergey Levine. Contextual imagined goals for self-supervised robotic learning. In _Conference on Robot Learning_, pages 530-539. PMLR, 2020.
*  Suraj Nair and Chelsea Finn. Hierarchical foresight: Self-supervised learning of long-horizon tasks via visual subgoal generation. In _ICLR_, 2019.
*  Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver. Universal value function approximators. In _ICML_, 2015.
*  Avi Singh, Albert Yu, Jonathan Yang, Jesse Zhang, Aviral Kumar, and Sergey Levine. Cog: Connecting new skills to past experience with offline reinforcement learning. _arXiv preprint arXiv:2010.14500_, 2020.
*  Shagun Sodhani, Amy Zhang, and Joelle Pineau. Multi-task reinforcement learning with context-based representations. In _ICML_, 2021.
*  Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In _CVPR_, 2020.
*  Yee Whye Teh, Victor Bapst, Wojciech Marian Czarnecki, John Quan, James Kirkpatrick, Raia Hadsell, Nicolas Heess, and Razvan Pascanu. Distral: robust multitask reinforcement learning. In _NeurIPS_, 2017.
*  Homer Rich Walke, Kevin Black, Tony Z. Zhao, Quan Vuong, Chongyi Zheng, Philippe Hansen-Estruch, Andre Wang He, Vivek Myers, Moo Jin Kim, Max Du, Abraham Lee, Kuan Fang, Chelsea Finn, and Sergey Levine. Bridgedata v2: A dataset for robot learning at scale. In _CORL_, 2023.
*  Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, and James Hays. Argoverse 2: Next generation datasets for self-driving perception and forecasting. In _NeurIPS_, 2021.
*  Yifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. _arXiv preprint arXiv:1911.11361_, 2019.
*  Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for offline reinforcement learning. In _NeurIPS_, 2021.
*  Rui Yang, Lin Yong, Xiaoteng Ma, Hao Hu, Chongjie Zhang, and Tong Zhang. What is essential for unseen goal generalization of offline goal-conditioned rl? In _ICML_, 2023.
*  Albert Yu and Ray Mooney. Using both demonstrations and language instructions to efficiently learn robotic tasks. In _ICLR_, 2023.
*  Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: model-based offline policy optimization. In _NeurIPS_, 2020.
*  Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Sergey Levine, and Chelsea Finn. Conservative data sharing for multi-task offline reinforcement learning. In _NeurIPS_, 2021.
*  Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and Sergey Levine. How to leverage unlabeled data in offline reinforcement learning. In _ICML_, 2022.
*  Zhuangdi Zhu, Kaixiang Lin, Anil K Jain, and Jiayu Zhou. Transfer learning in deep reinforcement learning: A survey. _IEEE Transactions on Pattern Analysis and Machine Intelligence_, 2023.

Theoretical Analysis

In this section, we provide a detailed analysis for the instantiation of CODA using PSPI . We follow the same notation for the value functions, augmented MDP, and extended function classes as stated in Section 3 and Section 4 in the main text.

### Equivalence Relations between Original and Augmented MDP

We begin by showing that the optimal policy and any value function in the augmented MDP can be expressed using their analog in the original MDP. With the augmented MDP defined as \(}(},},,,)\) in Section 4.1, we first define the value function in the augmented MDP. For a policy \(:}}\), we define the Q function for the augmented MDP as

\[^{}(x,a)_{,}[_{t=0 }^{}^{t}(x,a)|x_{0}=x,a_{0}=a]\]

Notice that we don't have a reaching time random variable \(T\) in this definition; instead the agent would enter an absorbing state \(s^{+}\) after taking \(a^{+}\) in the augmented MDP. We can define similarly \(^{}(s)^{}(x,)\).

**Remark A.1**.: _Let \(^{}_{R}\) be the extension of \(Q^{}\) based on \(R\). We have, for \(x G\), \(^{}_{R}(x,a)=^{}(x,a)\)\( a}\), and for \(x G\), \(^{}_{R}(x,a)=^{}(x,a^{+})=1\), \( a}\)._

By the construction of the augmented MDP, it is obvious that the following is true.

**Lemma A.2**.: _Given \(:()\), let \(\) be its extension. For any \(h:\), it holds_

\[_{,P}[_{t=0}^{T}^{t}h(x,a)]=_{ {},}[_{t=0}^{}^{t}^{}(x,a)|x ^{+}]\]

_where \(T\) is the goal-reaching time (random variable) and we define \(^{}(x,a^{+})=h(x,)\)._

We can now relate the value functions between the two MDPs.

**Proposition A.3**.: _For a policy \(:()\), let \(\) be its extension (defined above). We have for all \(x\), \(a\),_

\[Q^{}(x,a) ^{}(x,a)\] \[V^{}(x) =^{}(x)\]

_Conversely, for a policy \(:}(})\), define its restriction \(\) on \(\) and \(\) by translating probability of \(\) originally on \(a^{+}\) to be uniform over \(\). Then we have for all \(s\), \(a\)_

\[Q^{}(x,a) ^{}(x,a)\] \[V^{}(x) ^{}(x)\]

Proof.: The first direction follows from Lemma A.2. For the latter, whenever \(\) takes \(a^{+}\) at some \(x G\), it has \(^{}(x)=0\) but \(^{}(x) 0\) since there is no negative reward in the original MDP. By performing a telescoping argument, we can derive the second claim. 

By this lemma, we know the extension of \(^{*}\) (i.e., \(^{*}\)) is also optimal to the augmented MDP and \(V^{*}(x)=^{*}(x)\) for \(x\). Furthermore, we have a reduction that we can solve for the optimal policy in the original MDP by the solving augmented MDP, since

\[V^{}(d_{0})-V^{*}(d_{0}) V^{}(d_{0})-^{*}(d_{0})\]

for all \(:}(})\). In particular,

\[() V^{}(d_{0})-V^{*}(d_{0})=V^{}(d_{0})- ^{*}(d_{0})}()\] (3)Since the augmented MDP replaces the random reaching time construction with an absorbing-state version, the Q function \(^{}\) of the extended policy \(\) satisfies the Bellman equation

\[^{}(x,a) =(x,a)+_{x^{}(|x,a)}[ ^{}(x^{},)]\] \[}^{}^{}(x,a)\] (4)

For \(x\) and \(a\), we show how the above equation can be rewritten in \(Q^{}\) and \(R\).

**Proposition A.4**.: _For \(x\) and \(a\),_

\[^{}(x,a)=0+_{x^{}(|x,a)} [(R(x^{}),Q^{}(x^{},))]\]

_For \(a=a^{+}\), \(^{}(x,a^{+})=(x,a^{+})=R(x)\). For \(x^{+}\), \(^{}(x,a)=0\)._

Proof.: The proof follows from Lemma A.5 and the definition of \(\). 

**Lemma A.5**.: _For \(x\), \(^{}(x,)=(R(x),Q^{}(x,))\)_

Proof.: For \(x\),

\[^{}(x,) =^{}(x,a^{+}),&\\ ^{}(x,),&\] (Because of definition of 

\[\]

) \[=^{}(x,a^{+}),&\\ Q^{}(x,),&\] (Because of Proposition A.3 ) \[=(x,a^{+}),&\\ Q^{}(x,),&\] (Definition of augmented MDP ) \[=R(x),&\\ Q^{}(x,),&\] \[=(R(x),Q^{}(x,))\]

where in the last step we use \((x)=1\) for \(x G\) and \((x)=0\) otherwise. 

### Function Approximator Assumptions

In Theorem 5.4, we assume access to a policy class \(=\{:()\}\). We also assume access to a function class \(=\{f:\}\) and a function class \(=\{g:\}\). We can think of them as approximators for the Q function and the reward function of the original MDP.

For an action value function \(f:\), define its extension:

\[_{g}(x,a)=g(x),&$ and $x^{+}$}\\ 0,&^{+}$}\\ f(x,a),&.\] (5)

The extension of \(f\) is based on a state value function \(g:\) which determines the action value of \(x\) only at \(a^{+}\). One could also view \(g(x)\) as a goal indicator: after taking \(a^{+}\) the agent would always transit to the zero-reward absorbing state \(s^{+}\), so \(g(x)=(x,a^{+})\) which is the indicator of whether \(s G_{c}\).

Recall the zero-reward Bellman backup operator \(^{}\) with respect to \(P(s^{}|s,a)\) as defined in Assumption 5.2:

\[^{}f(x,a)_{x^{} P_{0}(|x,a )}[f(x^{},)]\]

where \(P_{0}(x^{}|x,a) P(s^{}|s,a)(c^{}=c)\). Note this definition is different from the one with absorbing state \(s^{+}\) in Section 3. Using this modified backup operator, we can show that the following realizability assumption is true for the augmented MDP:

**Proposition A.6** (Realizability).: _By Assumption 5.1 and Assumption 5.2, there is \(f\) and \(g\) such that \(^{}=_{g}\)._Proof.: By Assumption 5.2, there is \(h\) such that \(h(x,a)=(R(x),Q^{}(x,a))\). By Proposition A.4, we have for \(x\), \(a a^{+}\)

\[^{}(x,a) =0+_{x^{}(|x,a)}[(R(x^{ }),Q^{}(x^{},))]\] \[=0+_{x^{} P_{0}(|x,a)}[h(x,)]\] \[=^{}h\]

For \(a=a^{*}\), we have \(^{}(x,a^{*})=(x,a^{+})=R(x)\). Finally \(^{}(x^{+},a)=0\) for \(x^{+}^{+}\). Therefore, \(^{}=_{g}\) for some \(f\) and \(g\). 

### Coda+PSPI Algorithm

In this section, we describe the instantiation of PSPI with CODA in detail along with the necessary notation. The main theoretical result and its proof is then given in Section A.4. As discussed in Section 5, our algorithm is based on the idea of reduction, which turns the offline CGO problem into a standard offline RL problem in the augmented MDP. To this end, we construct augmented datasets \(_{}\) and \(_{}\) in Algorithm 1 as follows:

\[_{} =\{(x_{n},a_{n},r_{n},x_{n}^{})|r_{n}=0,x_{n}=(s_{i},c_{j}),x_{n}^{}=(s_{i}^{},c_{j}),a_{n}=a_{i},(s_{i},a_{i},s_{i}^{} ) D_{},(,c_{j}) D_{}\}\] \[_{} =\{(x_{n},a^{+},r_{n},x_{n}^{+})|r_{n}=1,x_{n}=(s_{n},c_{n}),x_{n} ^{+}=(s^{+},c_{n}),(s_{n},c_{n}) D_{}\}\]

With this construction, we have: \(_{}_{}(s,a,s^{})_{}(c)\) and \(_{}_{}(c,s)(a=a^{+})( s^{}=s^{+})\). We use the notation, \(_{}(x,a,x^{})=_{}(s,a,s^{})_{ }(c)\) and \(_{}(x,a,x^{})=_{}(c,s)(a=a ^{+})(s^{}=s^{+})\). We will also use the notation \(x_{ij}(s_{i},c_{j})\), \(x_{ij}^{}(s_{i}^{},c_{j})\) in the above construction. These two datasets have the standard tuple format, so we can run offline RL on \(_{}_{}\). Also, note that \(|_{}|=|D_{}||D_{}|\) and \(|_{}|=|D_{}|\).

Pspi.We consider the information theoretic version of PSPI  which can be summarized as follows: For an MDP \((,,R,P,)\), given a tuple dataset \(D=\{(x,a,r,x^{})\}\), a policy class \(\), and a value class \(\), it finds the policy through solving the two-player game:

\[_{}_{f} f(d_{0},) (f,f;,D)-_{f^{}}(f^{},f;,D) _{b}\] (6)

where \(f(d_{0},)=_{x_{0} d_{0}}[f(x_{0},)]\), \((f,f^{};,D)_{(x,a,r,x^{}) D}( f(x,a)-r-f^{}(x^{},))^{2}\). The term \((f,f;,D)-_{f^{}}(f^{},f;,D)\) in the constraint is an empirical estimation of the Bellman error on \(f\) with respect to \(\) on the data distribution \(\), i.e. \(_{x,a}[(f(x,a)-^{}f(x,a))^{2}]\). It constrains the Bellman error to be small, since \(_{x,a}[(Q^{}(x,a)-^{}Q^{}(x,a))^{2}]=0\).

Coda+Pspi.Below we show how to run PSPI to solve the augmented MDP with offline dataset \(_{}_{}\). To this end, we extend the policy class from \(\) to \(\), and the value class from \(\) to \(}_{}\) using the function class \(\) based on the extensions defined in Section 4.1. One natural attempt is to implement equation 6 with the extended policy and value classes \(\) and \(}\) and \(=_{}_{}\). This would lead to the two player game:

\[_{}_{_{g}_{}}_{g}(d_{0},)(_{g},_{g}; ,)-_{_{g^{}}^{}}_{ }}(_{g^{}}^{},_{g};,) _{b}\] (7)

However, equation 7 is not a well-defined algorithm, because its usage of the extended policy \(\) in the constraint requires knowledge of \(G\), which is unknown to the agent.

Fortunately, we show that equation 7 can be slightly modified so that the implementation does not actually require knowing \(G\). Here we use a property (Proposition A.4) that the Bellman equation of the augmented MDP:

\[^{}(x,a) =(x,a)+_{x^{}(|x,a)}[ ^{}(x^{},)]\] \[=0+_{x^{}(|x,a)}[(R(x^{ }),Q^{}(x^{},))]\]

for \(x\) and \(a a^{+}\), and \(^{}(x,a)=1\) for \(x G\) and \(a=a^{+}\).

We can rewrite the squared Bellman error on these two data distributions, \(_{}\) and \(_{}\), using the Bellman backup defined on the augmented MDP (see eq.4) as below:

\[_{_{}}[(^{}(x,a)-}^{}^{}(x,a))^{2}]=_{_{}}[(^{ }(x,a)-0-_{x^{}(|x,a)}[(R(x),Q^{ }(x,))])^{2}]\]\[_{x,a_{}}[(Q^{}(x,a)-^{}Q^{ }(x,a))^{2}]=_{x,a_{}}[(Q^{}(x,a^{+})-1) ^{2}]\]

We can construct an approximator \(_{g}(x,a)\) for \(^{}(x,a)\). Substituting the estimator \(_{g}(x,a)\) for \(^{}(x,a)\) in the squared Bellman errors above and approximating them by finite samples, we derive the empirical losses below.

\[_{}(_{g},_{g^{}}^{};) _{}|}_{(x,a,r,x^{}) _{}}(f(x,a)-(g^{}(x^{}),f^{}( x^{},)))^{2}\] (8) \[_{}(_{g}) _{}|}_{(x,a,r,x^{ })_{}}(g(x)-1)^{2}\] (9)

where we have \(_{g}(x,a)=f(x,a)(a a^{+})+g(x)(a=a^{+})\) for \(x^{+}\).

Using this loss, we define the two-player game of PSPI for the augmented MDP:

\[_{}_{_{g}}_{g}(d_{0},)\] (10) s.t. \[_{}(_{g},_{g};)-_{_{g^{}}^{}}_{}(_{g^{}}^{ },_{g};)_{}\] \[_{}(_{g}) 0\]

Notice \(_{g}(d_{0},)=f(d_{0},)\). Therefore, this problem can be solved using samples from \(D\) without knowing \(G\).

A.4 Analysis of CODA+PSPI

Covering number.We first define the covering number on the function classes \(\), \(\), and \(\)9. For \(\) and \(\), we use the \(L_{}\) metric. We use \(_{}(,)\) and \(_{}(,)\) to denote the their \(\)-covering numbers. For \(\), we use the \(L_{}\)-\(L_{1}\) metric, i.e., \(\|_{1}-_{2}\|_{,1}_{x}\|_{1}(|s )-_{2}(|s)\|_{1}\). We use \(_{,1}(,)\) to denote its \(\)-covering number.

High-probability events.In CODA+PSPI (eq. 10), we choose the policy in class \(\) which has the best _pessimistic_ value function estimate. In order to show this, we will need two high probability results (we defer their proofs to Section A.4.1). To that end, we will use the following notation for the expected value of the empirical losses:

\[_{_{}}(_{g},_{g^{}}^{ };) _{(x,a,x^{})_{}}( f(x,a)-(g^{}(x^{}),f^{}(x^{},)))^{2}\] \[_{_{}}(_{g}) _{(x,a^{+},x^{+})_{}}( g(x)-1)^{2}\]

First, we show that for any policy \(\), the true value function \(^{}\) satisfies the two empirical constraints specified in eq. 10.

**Lemma A.7**.: _With probability at least \(1-\), it holds for all \(\),_

\[_{}(^{},^{};)-_{ _{g^{}}^{}}_{}(_{g^{ }}^{},^{};) O((}|}}+}|}})^{2})\]

\[_{}(^{}) 0\]

_where10\(_{}(,1/|D_{}||D_{}||D_{}||D_{}|)_{,1}(,1/|D_{ }||D_{}|)}{}\)._

We use the notation \(_{}(|}}+}|}})^{2}\) for the first upper bound in Lemma A.7.

Next, we show that for every pair of value function \(_{g}}\) and policy \(\) which satisfies the constraints in eq. equation 10, the empirical estimates provide a bound on the population error with high probability.

**Lemma A.8**.: _For all \(f,g\) and \(\) satisfying_

\[_{}(_{g},_{g};)-_{^{}_{ g^{}}}}_{}(^{}_{g^{}}, _{g};)_{}\]

\[_{}(_{g}) 0,\]

_with probability at least \(1-\), we have:_

\[\|_{g}(x,a)-_{x^{}(|x,a)} [(g(x^{}),f(x^{},))]\|_{_{}} O(}})\]

\[\|g(x)-1\|_{_{}} O(_{}(,1/|D_{}|)}{}}{|D_{ }|}})}}\]

Pessimistic estimate.Our next step is to show that the solution of the constrained optimization problem in equation 10 is pessimistic and that the amount of pessimism is bounded.

**Lemma A.9**.: _Given \(\), let \(_{g}^{}\) denote the minimizer in equation 10. With high probability, \(_{g}^{}(d_{0},) Q^{}(d_{0},)\)_

Proof.: By Lemma A.7, for any policy \(\), we know that \(_{R}^{}\) satisfies the constraints in equation equation 10. Therefore, we have

\[_{g}^{}(d_{0},)_{R}^{}(d_{0},)=Q^{ }(d_{0},).\]

We will now bound the amount of underestimation for the minimizer \(_{g}^{}\) in the above lemma.

**Lemma A.10**.: _Suppose \(x_{0} d_{0}\) is not in \(G\) almost surely. For any \(\),_

\[Q^{}(d_{0},)-_{g}^{}(d_{0},)\] \[_{}[_{t=0}^{T-1}^{t}( (g^{}(x_{t+1}),f^{}(x_{t+1},))-f^{}(x_{t},a_{t}))+ ^{T}(R(x_{T})-g^{}(x_{T}))]\]

_Note that in a trajectory \(x_{T} G\) whereas \(x_{t} G\) for \(t<T\) by definition of \(T\)._

Proof.: Let \(_{g}^{}=(f^{},g^{})\) be the empirical minimizer. By performance difference lemma, we can write

\[(1-)Q^{}(d_{0},)-(1-)_{g}^{}(d_{0}, ) =(1-)^{}(d_{0},)-(1-)_{g}^{ }(d_{0},)\] \[=_{^{}}[(x,a)+_{g}^{} (x^{},)-_{g}^{}(x,a)]\]

where with abuse of notation we define \(^{}(x,a,x^{})^{}(x,a)(x ^{}|x,a)\), where \(^{}(x,a)\) is the average state-action distribution of \(\) in the augmented MDP.

In the above expectation, for \(x G\), we have \(a=a^{+}\) and \(x^{+}=(s^{+},c)\) after taking \(a^{+}\) at \(x=(s,c)\), which leads to

\[(x,a)+_{g}^{}(x^{},)-_{g}^{}(x, a)=(x,a^{+})+_{g}^{}(x^{+},)-_{g}^{}(x,a^{+} )=R(x)-g^{}(x)\]

For \(x G\) and \(x^{+}\), we have \(a a^{+}\) and \(x^{}^{+}\); therefore

\[(x,a)+_{g}^{}(x^{},)-_ {g}^{}(x,a) =R(x)+_{g}^{}(x^{},)-f^{}(x,a)\] \[(g^{}(x^{}),f^{}(x^{},))-f^{ }(x,a)\]where the last step is because of the definition of \(_{g}^{}\). For \(x^{+}\), we have \(x^{+}\) and the reward is zero, so

\[(x,a)+_{g}^{}(x^{},)-_{g}^{}(x,a )=0\]

Therefore, we can derive

\[(1-)Q^{}(x_{0},)-(1-)_{g}^{}(x_{0}, )\] \[_{^{\#}}[(g^{}(x^{}),f^{ }(x^{},))-f^{}(x,a)|x G,x^{+}]+_{^{\#}}[R(x)-g^{}(x)|x G]\]

Finally, using Lemma A.2 we can have the final upper bound. 

Main Result: Performance Bound.Let \(^{}\) be the learned policy and let \(_{g}^{^{}}\) be the learned function approximators. For any comparator policy \(\), let \(_{g}^{}=(f^{},g^{})\) be the estimator of \(\) on the data. We have.

\[V^{}(d_{0})-V^{^{}}(d_{0})\] \[=Q^{}(d_{0},)-Q^{^{}}(d_{0},^{})\] \[=Q^{}(d_{0},)-_{g}^{^{}}(d_{0},^{ })+_{g}^{^{}}(d_{0},^{})-Q^{^{ }}(d_{0},^{})\] \[ Q^{}(d_{0},)-_{g}^{^{}}(d_{0},)\] \[_{,P}[_{t=0}^{T-1}^{t}( (g^{}(x_{t+1}),f^{}(x_{t+1},))-f^{}(x_{t},a_{t}))+^{T}( R(x_{T})-g^{}(x_{T}))]\] \[_{,P}[_{t=0}^{T-1}^{t}| (g^{}(x_{t+1}),f^{}(x_{t+1},))-f^{}(x_{t},a_{t})|+^{T}| R(x_{T})-g^{}(x_{T})|]\] \[_{}()_{_{}}[ |(g^{}(x^{}),f^{}(x^{},))-f^{}(x,a)|]+ _{}()_{_{}}[|g(x)-1|]\] \[_{}() }}++_{}()}}\]

where \(_{}()\) and \(_{}()\) are the concentrability coefficients defined in Definition 5.3.

**Theorem A.11**.: _Let \(^{}\) denote the learned policy of CODA + PSPI with datasets \(D_{}\) and \(D_{}\), using value function classes \(=\{\}\) and \(=\{\}\). Under realizability and completeness assumptions as stated in Assumption 5.1 and Assumption 5.2 respectively, with probability \(1-\), it holds, for any \(\),_

\[J()-J(^{})_{}()(}|}}+}|}} )+_{}()_{ }(,1/|D_{}|)}{}}{|D_{}|}}\]

_where \((_{}(,1/|D_{}||D_{}|)_{}(,1/|D_{}||D_{ }||D_{}||)_{,1}(,1/|D_{}||D _{}|)}{})\), and \(_{}()\) and \(_{}()\) are concentrability coefficients which decrease as the data coverage increases._

#### a.4.1 Proof of Lemmas A.12 and A.13

We first show the following complementary lemma where we use a concentration bound on the constructed datasets \(_{}\) and \(_{}\). Lemmas A.7 and A.8 will follow deterministically from this main auxiliary result.

**Lemma A.12**.: _With probability at least \(1-\), for any \(f,f_{1},f_{2}\) and \(g\), we have:_

\[_{_{}}(f_{1},_{g},)-_{ _{}}(f_{2},_{g},)-_{}(f_{1}, _{g},)+_{}(f_{2},_{g},)\] \[(\|f_{1}-f_{2}\|_{_{}} (}|}}+ }|}})+}||D_{}|}}+}|})\]

_where \((_{}(,1/|D_{}||D_{}||)_{}(,1/|D_{}||D_{ }||)_{,1}(,1/|D_{}||D_{}| )}{})\)._Proof.: Our proof is similar to proof of corresponding results in Xie et al.  (Lemma A.4) and Cheng et al.  (Lemma 10) but we derive the result for the product distribution \(_{}=_{}_{}\) and its empirical approximation using \(_{}\). Throughout this proof, we omit the bar on \(\) as \(_{}\) does not use the extended definition of the policy \(\) and further use \(M,N\) for the dataset sizes \(|D_{}|,|D_{}|\). For any observed context \((c_{j},s_{j}) D_{}\), we define the following quantity:

\[_{_{}}^{j}(f,^{}{}_{g^{}},)= _{(s,a,s^{})_{}}[(f((s,c_{j}),a)-(g^{ }((s^{},c_{j}))),f^{}((s^{},c_{j}),)))^{2}]\]

For conciseness, we use notation \(x_{ j}\) for \((s,c_{j})\) and \(x^{}_{ j}\) for \((s^{},c_{j})\) where \((s,a,s^{})\) is sampled from a dynamics distribution and \(c_{j} D_{}\). We first start with the following:

\[_{_{}}(f_{1},_{g},)-_{_{}}(f_{2},_{g},)-_{}(f_{1},_{g},)+_{}(f_{2},_{g},)\] \[_{_{}}(f_{1},_{g},)-_{ _{}}(f_{2},_{g},)-_{j=1}^{M}_ {_{}}^{j}(f_{1},_{g},)+_{j=1}^{M}_{ _{}}^{j}(f_{2},_{g},)\] (11) \[+_{j=1}^{M}_{_{}}^{j}(f_{1},_{g },)-_{j=1}^{M}_{_{}}^{j}(f_{2},_{g},)-_ {}(f_{1},_{g},)+_{}(f_{2},_{g},)\] (12)

We will derive the final deviation bound by bounding each of these two empirical deviations in lines equation 11,equation 12. First, we will bound the term in line equation 11:

\[_{j=1}^{M}_{_{}}^{j}(f_{1},_{g}, )-_{j=1}^{M}_{_{}}^{j}(f_{2},_{g},)\] \[=_{j=1}^{M}_{_{}}^{j}(f_{1},_{g}, )-_{_{}}^{j}(f_{2},_{g},)\] \[=_{j=1}^{M}_{_{}}(f_{1}(x_{  j},a)-(g(x^{}_{ j}),f(x^{}_{ j},)))^{ 2}-(f_{2}(x_{ j},a)-(g(x^{}_{ j}),f(x^{}_{  j},)))^{2}\] \[=_{j=1}^{M}_{_{}}[(f_{1}(x_{  j},a)-f_{2}(x_{ j},a))(f_{1}(x_{ j},a)+f_{2}(x_{ j},a)-2 (g(x^{}_{ j}),f(x^{}_{ j},)))]\] \[=_{j=1}^{M}_{(s,a,)_{}}[( f_{1}(x_{ j},a)-f_{2}(x_{ j},a))(f_{1}(x_{ j},a)+f_{2}(x_{  j},a)-2}^{}_{g})(x_{ j},a)]\] (13) \[=_{j=1}^{M}_{(s,a,)_{}}[ (f_{1}(x_{ j},a)-}^{}_{g}(x_{ j},a))^{2}-( f_{2}(x_{ j},a)-}^{}_{g}(x_{ j},a))^{2}]\]

Using a similar argument, we can show that:

\[_{_{}}(f_{1},_{g},)-_{_{}}(f_{2},_{g},)\] \[=_{_{}}(f_{1}((s,c),a)-}^{}_{g}((s,c),a))^{2}-(f_{2}((s,c),a)-}^ {}_{g}((s,c),a))^{2}\] (14)

Let \(_{}\), \(_{}\) be \(\)-cover of \(\) and \(\), and \(_{}\) be \(\)-cover of \(\), i.e., \(_{1},_{2},_{}\), \( GG_{}\) and \(_{}\) such that \(\|f-\|_{},\|f_{1}-_{1}\|_{},\|f_{2}-_{2} \|_{}\) and \(\|\|_{,1}\).

Then, for any \(f,f_{1},f_{2}\), \(g\), \(\) and their corresponding \(,_{1},_{2}_{}\), \(_{}\), \(_{}\):

\[_{_{}}(_{1},}_{ },)-_{_{}}(_{2},}_{},)-_{j=1}^{M}(_{_{ }}^{j}(_{1},}_{},)-_ {_{}}^{j}(_{2},}_{},))\] \[=_{_{}}[(_{1}((s,c), a)-}^{}}_{}((s,c),a))^{2}-(_ {2}((s,c),a)-}^{}}_{}((s,c),a))^{2}]\] \[-_{j=1}^{M}_{(s,a,)_{ }}[(_{1}(x_{ j},a)-_{2}(x_{ j},a))( _{1}(x_{ j},a)+_{2}(x_{ j},a)-2}^{ }}_{})]\] \[(_{}( ,)_{}(,)_{ ,1}(_{})}{})}{M}}+_{}(,)_{}(,) _{,1}(_{})}{})}{3M}.\]

where the first equation follows from eqs. equation 13 and equation 14, and the last inequality follows from Bernstein's inequality with a union bound over the classes \(_{},_{},_{}\) where \(\) is the variance term as follows:

\[_{c_{}}[_{(s,a,) _{}}[(f_{1}((s,c),a)-f_{2}((s,c),a))(f_{1}((s,c),a)+f_{ 2}((s,c),a)-2}^{}_{g}((s,c),a))]]\] \[ 4_{_{}}[(f_{1}((s,c),a)-f_{ 2}((s,c),a))^{2}]\]

where we used that fact that \(f,g\).

Thus, with probability \(1-\),

\[_{_{}}(_{1},}_{ },)-_{_{}}(_{2},}_{},)-_{j=1}^{M}(_{_{ }}^{j}(_{1},}_{},)- _{_{}}^{j}(_{2},}_{},))\] \[ 2\|_{1}-_{2}\|_{_{}} _{}(,)_{}(,)_{,1}(_{})}{ })}{M}}+_{}(, )_{}(,)_{,1}( _{})}{})}{3M}.\]

Using the property of the set covers of \(,,\), we can easily conclude that:

\[_{_{}}(f_{1},_{},)- _{_{}}(f_{2},_{},)- _{j=1}^{M}(_{_{}}^{j}(f_{1},_{g},)-_ {_{}}^{j}(f_{2},_{g},))\] \[\|f_{1}-f_{2}\|_{_{}}_{}(,)_{}( ,)_{,1}(_{})}{})}{M} }+_{}(,)_{ }(,)_{,1}(_{})}{} )}{M}\] \[+_{}( ,)_{}(,)_{ ,1}(_{})}{})}{M}}+.\] (15)

Now, we bound the second deviation term in eq. line equation 12:

\[_{j=1}^{M}(_{_{}}^{j}(f_{1}, _{g},)-_{_{}}^{j}(f_{2},_{g},))- (_{}(f_{1},_{g},)-_{}(f_{2}, _{g},))\] \[=_{j=1}^{M}[_{_{}} [(f_{1}(x_{ j},a)-(g(x_{ j}^{}),f(x_{ j }^{},)))^{2}-(f_{2}(x_{ j},a)-(g(x_{ j}^ {}),f(x_{ j}^{},)))^{2}]\] \[-_{i=1}^{N}[(f_{1}(x_{ij},a)- (g(x_{ij}^{}),f(x_{ j}^{},)))^{2}-(f_{2}(x_{ ij},a)-(g(x_{ j}^{}),f(x_{ j}^{},)))^{2}]\] (16)For any fixed \(c_{j}\), using the same strategy as we used for bounding the first term in eq. line equation 11, for any \(f,f_{1},f_{2}\), \(g\), \(\) and their corresponding \(,_{1},_{2}_{}\), \(_{}\), \(_{}\), with probability at least \(1-\):

\[&(_{_{}}^{j}(_{1}, }_{},)-_{_{}}^{j}( _{2},}_{},))-(_{}( _{1},}_{},)-_{}( _{2},}_{},))\\ &\|_{1}-_{2}\|_{_{} \{c_{j}\}}_{}(, )_{}(,)_{,1}(,)}{})}{N}}+_{}( ,)_{}(,)_{ ,1}(,)}{})}{N}\\ &+_{}( ,)_{}(,)_{ ,1}(,)}{})}{N}}+.\]

We can now consider the sum in the second term in eq. line equation 12 for \(,_{1},_{2},,\) as:

\[&_{j=1}^{M}(_{_{}}^{j}( _{1},}_{},)-_{_{} }^{j}(_{2},}_{},))-( _{}(_{1},}_{},)- _{}(_{2},}_{},)) \\ &_{j=1}^{M}\|_{1}-_{2}\|_ {_{}\{c_{j}\}}_{ }(,)_{}(,) _{,1}(,)}{})}{N}}\\ &+_{}(, )_{}(,)_{,1}(,)}{})}{N}+_{}(,)_{}(,) _{,1}(,)}{})}{}}+.\\ &(_{j=1}^{M}\|_{1}-_ {2}\|_{_{}\{c_{j}\}}-\|_{1}-_{2}\|_{ {}_{}})_{}( ,)_{}(,)_{ ,1}(,)}{})}{N}}\\ &+\|_{1}-_{2}\|_{_{}} _{}(,)_{}(,)_{,1}(,)}{} )}{N}}\\ &+_{}(, )_{}(,)_{,1}(,)}{})}{N}+_{}(,)_{}(,) _{,1}(,)}{})}{N}}+.\\ &\|_{1}-_{2}\|_{_{}} _{}(,)_{}(,)_{,1}(,)}{} )}{N}}+_{}(, )_{}(,)_{,1}( ,)}{})}{}}\\ &+_{}(, )_{}(,)_{,1}( ,)}{})}{N}+_{}(,)_{}(,) _{,1}(,)}{})}{N}}+.\] (17)

where the last inequality follows from Hoeffding's inequality. We can now bound the term in eq. line equation 12 as:

\[_{j=1}^{M}(_{_{}}^{j}(f_{1},_{},)-_{_{}}^{j}(f_{2},_{}, ))-(_{}(f_{1},_{g},)-_{}(f_ {2},_{},))\] \[\|_{1}-_{2}\|_{_{}} _{}(,) _{}(,)_{,1}(,)}{} )}{N}}+_{}(,) _{}(,)_{,1}(,)}{ })}{}\] \[+_{}(, )_{}(,)_{,1}(, )}{})}{N}+_{ }(,)_{}(,)_{ ,1}(,)}{})}{}}+.\] (18)Combining eqs. equation 15 and equation 18 with \(=()\), we get the final result. 

**Lemma A.13**.: _With probability at least \(1-\), for any \(g,g,g_{1},g_{2}\) and \(f\), we have:_

\[_{_{}}(_{g_{1}})-_{_{ }}(_{g_{2}})-_{}(_{g_{1}})+_{ }(_{g_{2}})\] \[(\|g_{1}-g_{2}\|_{_{}} _{}(,1/|D_{}|)}{})}{|D_{}|}}+_{ }(,1/|D_{}|)}{})}{|D_{}|} ).\]

Proof.: This result can be proven using the same arguments as used in Lemma A.12 using a covering argument just over \(\). 

Using these two main concentration results, we can now prove Lemmas A.7 and A.8.

Proof of Lemma a.7.: Note \(^{}=_{g}\) for some \(f\) and \(g\) (Proposition A.6) and

\[0 =_{x,a_{}}[(^{}(x,a) -}^{}Q^{}(x,a))^{2}]\] \[=_{x,a_{}}[(^{}(x,a) -0-_{x^{}(|x,a)}[(^{})(x^{},)])^{2}]\]

The lemma can now be proved by following a similar proof of Theorem A.1 of Xie et al. . The key difference is the use of our concentration bounds in Lemmas A.12 and A.13 instead of Lemma A.4 in the proof of Xie et al. . On the other hand, \(_{}(_{g})=0\) because the reward \(R(x)\) is deterministic which results in the second inequality. 

Proof of Lemma a.8.: This result can again be proved using the same steps as in Lemma A.5 from Xie et al.  based on the concentration bound in Lemmas A.12 and A.13. 

## Appendix B Experimental Details

### Hyperparameters and Experimental Settings

Iqla.For IQL, we keep the hyperparameter of \(=0.99\), \(=0.9\), \(=10.0\), and \(=0.005\) in , and tune other hyperparameters on the antmaze-medium-play-v2 environment and choose batch size = 1024 from candidate choices {256, 512, 1024, 2046}, learning rate = \(10^{-4}\) from candidate choices {\(5 10^{-5},10^{-4},3 10^{-4}\)} and 3 layer MLP with RuLU activating and 256 hidden units for all networks. We use the same set of IQL hyperparameters for both our methods and all the baseline methods included in Section 6.2, and apply it to all environments. In the experiments, we follow the convention of the \(-1/0\) reward in the IQL implementation for Antmaze, which can be shown to be the same as the \(0/1\) reward notion in terms of ranking policies under the discounted MDP setting.

Reward Prediction (RP).For naive reward prediction, we use the full context-goal dataset as positive data, and train a reward model with 3-layer MLP and ReLU activations, learning rate = 10\({}^{-4}\), batch size = 1024, and training for 100 epochs for convergence. To label the transition dataset, we need to find some appropriate threshold to label states predicted as goals given contexts. We choose the percentile as 5% in the reward distribution evaluated by the context-goal set as the threshold to label goals (if a reward is larger than the threshold than it is labeled as terminal), from candidate choices {0%, 5%, 10%}. Then we apply it to all environments. Another trick we apply for the reward prediction is that instead of predicting 0 for the context-goal dataset, we let it predict 1 but shift the reward prediction by -1 during reward evaluation, which prevents the model from learning all 0 weights. Similar tricks are also used in other reward learning baselines.

Uds+Rp.We use the same structure and training procedure for the reward model as RP, except that we also randomly sample a minibatch of "negative" contextual transitions with the same batch size for a balanced distribution, which is constructed by randomly sampling combinations of a state in the trajectory-only dataset and a context from the context-goal dataset. To create a balanced distribution of positive and negative samples, we sample from each dataset with equal probability. For the threshold, we choose the percentile as 5% in the reward distribution evaluated by the context-goal set as the threshold to label goals in the antmaze-medium-play-v2 environment, from candidate choices {0%, 5%, 10%}. Then we apply it to all environments.

Pds.We use the same structure and training procedure for the reward model as RP, except that we train an ensemble of \(10\) networks as in . To select the threshold percentile and the pessimistic weight \(k\), we choose the percentile as 15% in the reward distribution evaluated by the context-goal set as the threshold to label goals from candidate choices {0%, 5%, 10%, 15%, 20%}, and \(k=15\) from the candidate choices {5,10,15,20} in the antmaze-medium-play-v2 environment. Then we apply them to all environments.

CODA (ours).We do not require extra parameters other than the possibility of sampling from the real and fake transitions. Intuitively, we should sample from both datasets with the same probability to create an overall balanced distribution. We ran additional experiments to study the effect of this sampling ratio hyperparameter: ratio of samples from the context-goal dataset \(D_{}\) to total samples in each minibatch. Table 5 shows that CODA well as long as the ratio is roughly balanced in sampling from both dataset.

Compute Resources.For all methods, each training run takes about 8h on a NVIDIA T4 GPU.

### Context-Goal dataset Construction and Environmental Evaluation.

Here we introduce the context-goal dataset in the three levels of context-goal setup mentioned in Section 6 and how to evaluate in each setup. We also include our code implementation for reference.

Original Antmaze.We extract the 2D locations from the states in the trajectory dataset with terminal=True as the context (in original antmaze, it suffices to reach the \(L_{2}\) ball with radius 0.5 around the center), where the contexts are distributed very closely as visualized in Figure 2(a), and the corresponding states serve as the goal examples with Gaussian perturbations \(N(0,0.05)\) on the dimensions other than the 2D location.

Four Rooms.For each maze map, we partition 4 rooms like Figure 2(b) and use the room number as the context. To construct goal examples, we create a copy of all states in the trajectory dataset, perturb the states in the copy by \(N(0,0.05)\) on each dimension, and then randomly select the states (up to 20K) according to the room partition.

Random Cells.For each maze map, we construct a range of non-wall 2D locations in the maze map and uniformly sample from it to get the training contexts. To construct the goal set given context, we randomly sample up to 20K states with the 2D locations within the \(L_{2}\) ball with radius \(2\). Figure 2(C) is a intuitive visualization of the corresponding context-goal sets. For test distributions, we have two settings: 1) the same as the training distribution; 2) test contexts are drawn from a limited area that is far away from the starting point of the agent.

Evaluation.We follow the conventional evaluation procedure in , where the success rate is normalized to be 0-100 and evaluated with 100 trajectories. We report the result with standard error across 5 random seeds. The oracle condition we define in each context-goal setup is used to evaluate whether the agent has successfully reached the goal and also defines the termination of an episode.

   Env/Ratio & 0.1 & 0.3 & 0.5 & 0.7 & 0.9 \\  unaze & 91.6\(\)1.3 & 92.4\(\)1.0 & 94.8\(\)1.3 & 86.4\(\)1.8 & 84.8\(\)3.0 \\ umaze diverse & 76.8\(\)1.9 & 79.2\(\)1.6 & 72.8\(\)7.7 & 76.6\(\)2.3 & 65.4\(\)8.8 \\ medium play & 82.3\(\)2.1 & 85.0\(\)1.8 & 75.8\(\)1.9 & 72.8\(\)1.3 & 76.6\(\)1.3 \\ medium diverse & 79.4\(\)1.6 & 76.6\(\)3.0 & 84.5\(\)5.2 & 75.6\(\)2.0 & 72.0\(\)3.5 \\ large play & 50.8\(\)2.0 & 45.2\(\)3.7 & 60.0\(\)7.6 & 43.6\(\)2.3 & 46.6\(\)2.3 \\ large diverse & 35.8\(\)5.7 & 37.4\(\)4.7 & 36.8\(\)6.9 & 34.4\(\)2.4 & 27.0\(\)2.1 \\  average & 69.5 & 68.9 & 70.8 & 64.9 & 62.1 \\   

Table 5: Average success rate (%) in AntMaze-v2 from all environments, with different sampling ratios from the context-goal dataset.

[MISSING_PAGE_EMPTY:24]

Figure 6: Reward model evaluation for the Four Rooms environment. Green dots are outliers.

Figure 7: Reward evaluation for Random Cells environment (the test context distribution is the same as training). Green dots are outliers.

### NeurIPS Paper Checklist

1. **Claims** Question: Do the main claims made in the abstract and introduction accurately reflect the paper's contributions and scope? Answer: [Yes] Justification: All the theoretical results and experimental findings referenced in the abstract and introduction are included in the paper. Guidelines: * The answer NA means that the abstract and introduction do not include the claims made in the paper. * The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. A No or NA answer to this question will not be perceived well by the reviewers. * The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. * It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper.
2. **Limitations** Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: We have added a limitations section (see Section 6.4). Guidelines: * The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. * The authors are encouraged to create a separate "Limitations" section in their paper. * The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. * The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. * The authors should reflect on the factors that influence the performance of the approach. For example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. * The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. * If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. * While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.
3. **Theory Assumptions and Proofs** Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof? Answer: [Yes]Justification: We include the full detailed proof of our theoretical result in the appendix. Guidelines: * The answer NA means that the paper does not include theoretical results. * All the theorems, formulas, and proofs in the paper should be numbered and cross-referenced. * All assumptions should be clearly stated or referenced in the statement of any theorems. * The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. * Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. * Theorems and Lemmas that the proof relies upon should be properly referenced.
4. **Experimental Result Reproducibility** Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: Our experiments are based on standard RL domains and we discuss all the modifications that we consider for our setup in detail. Further, all algorithmic details and hyperparameters are also discussed. Guidelines: * The answer NA means that the paper does not include experiments. * If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. * If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. * Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. * While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example 1. If the contribution is primarily a new algorithm, the paper should make it clear how to reproduce that algorithm. 2. If the contribution is primarily a new model architecture, the paper should describe the architecture clearly and fully. 3. If the contribution is a new model (e.g., a large language model), then there should either be a way to access this model for reproducing the results or a way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). 4. We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results.
5. **Open access to data and code**Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [Yes] Justification: We provide code in supplementary files. Guidelines: * The answer NA means that paper does not include experiments requiring code. * Please see the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * While we encourage the release of code and data, we understand that this might not be possible, so "No" is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for a new open-source benchmark). * The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https://nips.cc/public/guides/CodeSubmissionPolicy) for more details. * The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. * The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only a subset of experiments are reproducible, they should state which ones are omitted from the script and why. * At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). * Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted.
6. **Experimental Setting/Details** Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: We provide all the necessary details in the main paper (Section 6) with additional information in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The experimental setting should be presented in the core of the paper to a level of detail that is necessary to appreciate the results and make sense of them. * The full details can be provided either with the code, in appendix, or as supplemental material.
7. **Experiment Statistical Significance** Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [Yes] Justification: We report the standard errors across multiple runs for all our experiments. Guidelines: * The answer NA means that the paper does not include experiments. * The authors should answer "Yes" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. * The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions).

* The method for calculating the error bars should be explained (closed form formula, call to a library function, bootstrap, etc.)
* The assumptions made should be given (e.g., Normally distributed errors).
* It should be clear whether the error bar is the standard deviation or the standard error of the mean.
* It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified.
* For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates).
* If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.
8. **Experiments Compute Resources** Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: We provide compute resource in the appendix. Guidelines: * The answer NA means that the paper does not include experiments. * The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. * The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. * The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).
9. **Code Of Ethics** Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: Based on the NeurIPS code of ethics, we do not see any direct ethical or societal impact considerations for our work. Guidelines: * The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. * If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. * The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).
10. **Broader Impacts** Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] Justification: Based on the NeurIPS code of ethics, we do not see any direct ethical or societal impact considerations for our work. Guidelines: * The answer NA means that there is no societal impact of the work performed. * If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact.

* Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations.
* The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster.
* The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology.
* If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).
11. **Safeguards** Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: These safeguard concerns do not apply to our experimental domains. Guidelines: * The answer NA means that the paper poses no such risks. * Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. * Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. * We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.
12. **Licenses for existing assets** Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: We include the information in the supplementary files. Guidelines: * The answer NA means that the paper does not use existing assets. * The authors should cite the original paper that produced the code package or dataset. * The authors should state which version of the asset is used and, if possible, include a URL. * The name of the license (e.g., CC-BY 4.0) should be included for each asset. * For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. * If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset.

* For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided.
* If this information is not available online, the authors are encouraged to reach out to the asset's creators.
13. **New Assets** Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: Not applicable to this paper. Guidelines: * The answer NA means that the paper does not release new assets. * Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. * The paper should discuss whether and how consent was obtained from people whose asset is used. * At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file.
14. **Crowdsourcing and Research with Human Subjects** Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: No crowdsourcing or human subjects involved. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. * According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector.
15. **Institutional Review Board (IRB) Approvals or Equivalent for Research with Human Subjects** Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] Justification: No human subjects or crowd-sourcing involved in experiments for this paper. Guidelines: * The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. * Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. * We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. * For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.