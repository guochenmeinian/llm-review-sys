# NeRF Revisited: Fixing Quadrature Instability in Volume Rendering

Mikaela Angelina Uy\({}^{1}\) George Kiyohiro Nakayama\({}^{1}\) Guandao Yang\({}^{1,2}\)

**Rahul Krishna Thomas\({}^{1}\) Leonidas Guibas\({}^{1}\) Ke Li\({}^{3,4}\)**

\({}^{1}\)Stanford University \({}^{2}\)Cornell University \({}^{3}\)Simon Fraser University \({}^{4}\)Google {mikacuy, w4756677, guandao, rt03mas, guibas}@stanford.edu, keli@sfu.ca

###### Abstract

Neural radiance fields (NeRF) rely on volume rendering to synthesize novel views. Volume rendering requires evaluating an integral along each ray, which is numerically approximated with a finite sum that corresponds to the exact integral along the ray under piecewise constant volume density. As a consequence, the rendered result is unstable w.r.t. the choice of samples along the ray, a phenomenon that we dub _quadrature instability_. We propose a mathematically principled solution by reformulating the sample-based rendering equation so that it corresponds to the exact integral under piecewise linear volume density. This simultaneously resolves multiple issues: conflicts between samples along different rays, imprecise hierarchical sampling, and non-differentiability of quantiles of ray termination distances w.r.t. model parameters. We demonstrate several benefits over the classical sample-based rendering equation, such as sharper textures, better geometric reconstruction, and stronger depth supervision. Our proposed formulation can be also be used as a drop-in replacement to the volume rendering equation for existing methods like NeRFs. Our project page can be found at pl-nerf.github.io.

## 1 Introduction

The advent of neural radiance fields (NeRF)  has sparked a flurry of work on neural rendering and has opened the way to many exciting applications . One of the key underpinnings of NeRF is volume rendering  - it is especially well-suited to end-to-end differentiable rendering , since the rendered image is a smooth function of the model parameters. This has made it possible to learn the 3D geometry and appearance solely from a 2D photometric loss on rendered images.

In volume rendering, the rendered colour \(\) for every pixel is an expectation of the colours along the ray cast through the pixel w.r.t. the distribution over ray termination distance \(s\).

\[=_{s p(s)}[c(s)]=_{0}^{}p(s)c(s)\,s\] (1)

where \(p(s)\) denotes the probability density function (PDF) of the distribution over ray termination distance \(s\) and \(c(s)\) denotes the the colour as a function of different points along the ray.

In general, \(p(s)\) and \(c(s)\) can be of arbitrary forms, so evaluating this integral analytically is not possible. Therefore, in practice, \(_{s p(s)}[c(s)]\) is approximated with quadrature. The quadrature formula that is most commonly used in the NeRF literature takes the following form:

\[_{s p(s)}[c(s)]=_{j=0}^{N}T_{j}(1-e^{-_{j}(s_{j+1}- s_{j})})c_{j},\] (2)

where \(T_{j}=-_{k=0}^{j}-_{k}(s_{k+1}-s_{k})\) and \(_{k}\) is the opacity evaluated at a sample \(s_{k}\) along the ray.

This expression is derived from the exact integral under a piecewise constant assumption to the opacity and colour along the given ray .

However, this seemingly simple, innocuous assumption can result in the rendered image being sensitive to the choice of samples along the ray at which the opacity \((s)\) and colour \(c(s)\) are evaluated. While this does not necessarily cause a practical issue in classical rendering pipelines [14; 15; 10], it has surprising consequences when used in neural rendering. Specifically, because the opacity at all points within the interval between two samples is assumed to be the same, there is a band near the surface of the geometry where opacity at all points within the band is as high as points on the surface itself. Because different rays cast from different cameras can pass through this band at different angles and offsets, both the number and the positions of samples within this band can be very different across different rays (see Fig 1 and Fig 2 for two example scenarios). Hence, simultaneously supervising these rays to produce the same colour values as in the real image captures can give rise to conflicting supervisory signals, which can result in artifacts like fuzzy surfaces and blurry texture.

Moreover, because the piecewise constant opacity assumption gives rise to a closed form expression that is equivalent to an expectation w.r.t. a discrete random variable, it is common practice in the NeRF literature to draw samples along the ray from the discrete distribution . This is commonly used to draw importance samples, and also to supervise the samples along the ray , for example in losses that penalize deviation of the samples from the true depth . Sampling from the discrete distribution requires the definition of a continuous surrogate function to the cumulative distribution function (CDF) of the discrete random variable, which unfortunately yields imprecise samples. As a result, samples that are drawn may not be close to the surface even if the underlying probability density induced by the NeRF is concentrated at the surface. Additionally, individual supervision cannot be provided to each sample drawn from the surrogate, because the gradient of the loss w.r.t. each sample would be almost zero everywhere.

All these issues, i.e. conflicting supervision, imprecise samples and lack of supervision on the CDF from the samples, stem from the assumption that opacity is piecewise constant causing the _sensitivity to the choice of samples_ both during rendering and sampling. We dub this problem as _quadrature instability_. In this paper, we revisit the quadrature used to approximate volume rendering in NeRF and devise a different quadrature formula  based on a different approximation to the opacity. We first show that interestingly a closed-form expression can be derived under any piecewise polynomial approximation for opacity. When the polynomial degree is 0, it reduces to the piecewise constant opacity as in existing literature, and when the degree is 2 or more, we show that it would lead to poor numerical conditioning

Therefore, we further explore a degree of 1 (i.e., piecewise _linear_) and show that it both resolves quadrature instability and has good numerical conditioning. We derive the rendering equation under _piecewise linear opacity_ explicitly and show that it has a simple and intuitive form. This results in a new quadrature method for volume rendering, which can serve as a drop-in replacement for existing methods like NeRFs. We demonstrate that this reduces artifacts, improves rendering quality and results in better geometric reconstruction. We also devise a new way to sample directly from the distribution of samples along each ray induced by NeRF without going through the surrogate, which opens the way to a more refined importance sampling approach and a more effective method to supervise samples using depth.

## 2 Related Work

NeRFs.Neural Radiance Field (NeRF) is a powerful representation for novel-view synthesis  that represents a scene using the weights of an MLP that is rendered by volumetric rendering . A key finding to the success of NeRF was the use of positional encoding [26; 29] to effectively increase the capacity of the MLPs that models the opacity and emitted color as a function of a 3D coordinate and viewing direction. Many works extend NeRF such as handling larger or unbounded scenes [40; 3; 37; 25], unconstrained photo collections , dynamic and deformable scenes [11; 19] and sparser input views [6; 39; 31; 28]. There are a number of papers that aim to improve the rendering quality of NeRF. Some do so by utilizing different kinds of supervision such as NeRF in the Dark , while others tackle this by improving the model [2; 36; 4]. MipNeRF  changes the model input by introducing integrated positional encoding (IPE) to reduce the aliasing effect along the xy coordinates. DiVeR  predicts the rendered colour within a line interval directly from a trilinearly interpolated feature in a voxel-based representation ZipNeRF  modifies the proposal network to a grid enabling it to be used together with IPE. In contrast, our work focuses on changing the objective function by modifying the rendering equation from piecewise constant opacity to piecewise linear, while keeping the model and supervision fixed. Additionally, ZipNeRF  also brings up a model specific issue on z-aliasing, where their model struggles under this setting. Similar to z-aliasing observed by ZipNeRF, we consider the setting of having conflicting supervision when presented with training views at different distances from the scene. While they may appear similar on the surface, the phenomena we study is different in that it is general and independent of the model, on having conflicting ray supervision from camera views, e.g. different camera-to-scene distances and the grazing angle setup.

Importance Sampling on NeRFs.Densely sampling and evaluating NeRF along multiple points in each camera ray is inefficient. Inspired by an early work on volume rendering , prior works typically use a coarse-to-fine hierarchical sampling strategy where the final samples are obtained by importance sampling of a coarse proposal distribution [18; 2; 3; 8]. These importance samples are drawn using inverse transform sampling where a sample is obtained by taking the inverse of the cumulative density function (CDF) of the proposal ray distribution. However, prior NeRF works that assume piecewise constant opacity result in a non-invertible CDF, and instead introduce a surrogate invertible function derived from the CDF in order to perform importance sampling. In contrast, our work that utilizes a piecewise linear opacity assumption results in an invertible CDF and a closed-form solution to obtain samples with inverse transform sampling. Other works also attempt to alter sampling using neural networks or occupancy caching [31; 23; 24; 12]. These techniques are orthogonal to our work as they propose changes to the model as opposed to our work where importance sampling is derived from a given model.

Volume rendering.Volume rendering is an important technique in various computer graphics and vision applications as explored in different classical works [10; 33; 7; 14]. These works include studying ray sampling efficiency  and data structures, e.g. octree  and volume hierarchy  for coarse-to-fine hierarchical sampling. The crux behind volume rendering is the integration over the weighted average of the color along the ray, where the weights is a function of the volume density (opacity). Max and Chen [14; 15] derive the volume rendering equation under the assumption of piecewise constant opacity and color, which NeRF  and its succeeding works use to learn their neural scene representation. However, the piecewise constant assumption results in rendering outputs that are sensitive to the choice of samples as well the non-invertible CDF introducing drawbacks to NeRF training. Following up on , works also  derive the volume rendering equation under the assumption that both opacity and color are piecewise linear that yield unwieldy expressions that lead to numerical issues and/or are expensive to compute. Some earlier works on rendering unstructured polygonal meshes that attempt to use this model , but it is in general not commonly used in practice due to the mentioned issues and hence has yet to be adopted into learning neural scene representations. In this work, we address both sets of issues that arise from the piecewise constant opacity and color and piecewise linear opacity and color by reformulating the volume rendering equation to assume piecewise linear opacity and piecewise constant color. Our derivation results in a simple and closed-form formulation to volume rendering making it suitable for NeRFs.

## 3 Background

### Volume Rendering Review

Definitions.In classical literature, the process of volume rendering  mapping a 3D field of optical properties to a 2D image and the visual appearance is computed through the exact _integration_ of these optical properties along the viewing rays. In this optical model, each point in space is an infinitesimal particle with a certain _opacity_\(\) that emits varying amounts of light, represented as a scalar _color_\(c\), in all viewing directions. The opacity \(\) is the differential probability of a viewing ray hitting a particle - that is for a viewing ray \((s)=+s\), where \(\) is the view origin and \(\) is the ray direction, the probability of ray \(\) hitting a particle along an infinitesimal interval \(s\) is \(((s))s\). Moreover, the transmittance \(T_{}(s)\) is defined as the probability that the viewing ray \(\) travels a distance \(s\) from the view origin without terminating, i.e. without hitting any particles.

Continuous probability distribution along the ray \(\).As illustrated by Max and Chen , the probability of hitting a particle \(s+ds\) only depends on the probability of hitting a particle at \(s\) and not any particles before it the probability of ray \(\) terminating at distance \(s\) is given by \(((s))T_{}(s)\), where \(T_{}(s)=(-_{0}^{s}()u)\). Hence the continuous probability density function (PDF) of ray \((s)\), which describes the likelihood of a ray terminating and emitting at \(s\), is given by

\[p(s)=((s))T_{}(s),\] (3)where \(s[0,]\) and \((s)\) is a point on the ray \(\). For notational simplicity we omit \(\) and write it as \(p(s)=(s)T(s)\).

Volume Rendering as a Continuous Integral.The observed color of the ray is then the expected value of the colors \(c(s)\) of all particles \(s\) along the ray weighted by the probability of hitting them. Mathematically, this results in the following continuous integral1:

\[_{s p(s)}[c(s)]=_{0}^{}p(s)c(s)\,s=_{0}^ {}(s)T(s)c(s)\,s\,.\] (4)

Quadrature under Piecewise Constant Opacity \(\).Since this integral cannot in general be evaluated analytically, it is approximated with quadrature. Let \(s_{1},s_{2},...,s_{N}\) be \(N\) (ordered) samples on the ray that define the intervals, where \(I_{j}=[s_{j},s_{j+1}]\) is the \(j^{}\) interval, and \(I_{0}=[0,s_{1}],I_{N}=[s_{N},]\). The volume density for particles along the interval \(I_{j}\) is then approximated under the assumption that opacity is constant along each interval, making it _piecewise constant_ along the ray . That is, for all \(j\) we have:

\[ s[s_{j},s_{j+1}],(s)=(s_{j}),\] (5)

for brevity we denote \((s_{j})=_{j}\), i.e. \(_{j}\) is the opacity for sample \(s_{j}\). Under this piecewise constant opacity assumption, the volume rendering equation Eq. 4 then becomes as follows:

\[_{s p(s)}[c(s)]=_{j=0}^{N}P_{j}c_{j}=_{j=0}^{N}( _{s_{j}}^{s_{j+1}}(u)T(u)\,u)c_{j}=_{j=0}^{N}T_{j }(1-e^{-_{j}(s_{j+1}-s_{j})})c_{j},\] (6)

Figure 1: **Ray Conflicts: Grazing Angle.** (Left) Illustration of conflicting ray supervision at the grazing under the piecewise constant opacity. For the constant setting, to render perpendicular rays (yellow) correctly, the model has to store the associated optical properties at a region in front of the surface as a sample takes the values of the left bin boundary. In the presence of a ray near the grazing angle, it will be crossing this region of high opacity (the gradient in front of the surface), associating it with conflicting opacity/color signals. (Middle) This results in fuzzier surfaces as shown along the side of the microphone as there is a conflict in ray supervision between the perpendicular and grazing angle rays. Our piecewise linear opacity assumption alleviates this issue and results in a clearer rendered view. (Right) As shown, the resulting PDF is peakier and the CDF is sharper for our linear setting, where the plotted distributions correspond to the ray from the marked pixel in red.

where \(T_{j}=-_{k=0}^{j}-_{k}(s_{k+1}-s_{k})\). Here, color \(c_{j}\) is also approximated to be constant along each interval \(I_{j}\), and \(P_{j}\) is the probability of each interval. Now, let us define the discrete random variable \(=(s)\), where

\[(x)=s_{0}&x s_{0}\\ s_{j}&s_{j} x<s_{j+1}j\{1,...,N-1\}\\ s_{N}&x>s_{N},\] (7)

which gives corresponding probability mass function \((s)\). Observe that the analytical expression of the integral Eq. 6 turns out to be the same as taking the expectation w.r.t. the discrete random variable \(\), i.e. \(_{s p(s)}[c(s)]=_{(s)}[c( {s})]\). This piecewise constant opacity assumption in the volume rendering equation is used in most, if not all, existing NeRF works. We recommend the reader to read  for more detailed derivations and our supplementary for a more thorough walkthrough.

### Neural Radiance Fields.

Following Max and Chen , Mildenhall et.al.  introduced neural radiance fields, a neural scene representation that uses the volume rendering equation under the piecewise constant opacity assumption for novel view synthesis. A neural radiance field (NeRF) is a coordinate-based neural scene representation, where opacity \(^{}:^{3}_{ 0}\) and color \(c^{}:^{3}^{2}^{3}\) are predicted at each continuous coordinate by parameterizing them as a neural network. To train the neural network, 2D images are used as supervision where each viewing ray is associated with a ground truth color. Volume rendering allows for the 3D coordinate outputs to be aggregated into an observed pixel color allowing for end-to-end training with 2D supervision. The supervision signal are on the coordinates of the ray samples \(s_{1},...,s_{N}\), which updates the corresponding output opacity and color at those samples. NeRF uses a importance sampling strategy by drawing samples from the ray distribution from a coarse network to generate better samples for rendering of their fine network. To sample from a distribution, inverse transform sampling is needed, that is, one draws \(u U(0,1)\) then passes it to the inverse of a cumulative distribution (CDF), i.e. a sample \(x=F^{-1}(u)\), where \(F\) is the CDF of the distribution. Under the piecewise constant assumption, the CDF of the discrete random variable \(\) is given by:

\[(x)=0&x s_{0}\\ _{k<j}(s_{k})&1 x<s_{j+1}j\{1,...,N-1\}\\ 1&x>s_{N}.\] (8)

This CDF is however non-continuous and non-invertible. NeRF's approach to get around this is to define a surrogate invertible function \(G\) derived from its CDF, then taking \(x=G^{-1}(u)\). Concretely, \(G(y)=}{s_{j}-s_{j-1}}(s_{j})+-y}{s_{j}-s_{ j-1}}(s_{j-1}),\) where \(y[s_{j-1},s_{j}]\). However, this does not necessarily result in the samples from the actual ray distribution \(p(s)\) from the model.

## 4 Drawbacks of Piecewise Constant Opacity \(\) in NeRFs

Unfortunately, there are properties associated with the piecewise constant opacity formulation that may not be desirable in the context of NeRFs. First, it is sensitive to the choice of samples, i.e. sample positions \(s\), along the ray, a phenomenon we dub as _quadrature instability_. This quadrature instability is due to the assumption that all points in an interval take the opacity of the left bin (Eq 5), making it sensitive to sample positions. As illustrated in Figure 1, this would lead to _ray conflicts_ in optimizing a NeRF when you have rays that are directly facing, i.e. perpendicular to, the surface (yellow rays) and rays that are close to the grazing angle (red ray), i.e. parallel to, the object. To render the perpendicular rays correctly, vanilla NeRF has to store the optical properties (opacity and color) associated with the perpendicular rays at a point before its intersection with the surface. This creates inaccurate signals to the optimization process when NeRF renders the ray at a grazing angle, as it will cross multiple conflicting opacity/colors (illustrated by the blue gradient). The sample sensitivity issue also arises when having cameras at different distances from the object as illustrated in Fig. 2, as this would lead to shifted sets of samples, causing inconsistencies when rendering at different camera-to-object distances. Notice that the noise on the texture of the chair is different across different viewing distances, where the middle view has fewer artifacts compared to the closer and further views.

## 5 Generalized Form for \(P_{j}\)

We first show a generalized derivation for the probability \(P_{j}\) of each interval \(I_{j}\), which we use to formulate our approach that alleviates the problems described above. From \(T(s)=^{s}(u)u)}\), we first notice that:

\[T}{s} =-^{s}(u)u)}(s)=-T(s)(s)\] \[T^{}(s) =-T(s)(s).\]

This results in the probability of each interval \(I_{j}\) given as follows:

\[P_{j}=_{s_{j}}^{s_{j+1}}(s)T(s)\,s=-_{s_{j}}^{s_{j+1}}T^ {}(s)\,s=T(s_{j})-T(s_{j+1}).\] (9)

Since \(s_{j}\)'s are arbitrarily sampled, \(P_{j}\) can be exactly evaluated in a closed-form expression, if and only if \(T()\) is in closed-form.

## 6 Our PL-NeRF

We observe from Eq. 15 that we can obtain a closed-form expression for \(P_{j}\) for any piecewise polynomial function in \(\), which can be of any degree \(d=0,1,2,...,n\). Commonly used in existing NeRF literature is choosing \(d=0\), i.e. piecewise constant, that is unstable w.r.t. the choice of samples as highlighted in the previous section. Interestingly, we also observe and show that the problem becomes numerically ill-conditioned for \(d 2\) making it difficult and unstable to optimize. Please see supplementary for the full proof. Hence, we propose to make opacity piecewise linear (\(d=1\)), which we call **PL-NeRF**, leading to a _simple_ and _closed-form_ expression for the volume rendering integral that is numerically stable and is a drop-in replacement to existing NeRF-based methods. We show both theoretically and experimentally that the piecewise linear assumption is sufficient and alleviates the problems caused by quadrature instability under the piecewise constant assumption.

Figure 2: **Ray Conflicts: Different Camera-to-Scene Distances. (Left) Rendered views from cameras at different distances from the object. At all distances, the rendered output for linear have sharper texture than constant because of the latterâ€™s sensitivity to the choice of samples. We also highlight the instability of the constant model as shown by the noisier texture of the middle view compared to the closer and further views. (Right) An illustration that moving the camera to different distances from the object result in different samples that lead to conflicts.**

The second issue comes from the CDF \(\) being piecewise constant (Eq 8). This leads to two consequences. First, the piecewise constant assumption makes \(\) non-invertible, hence, as mentioned in the previous section, importance sampling needs to be performed via a surrogate function \(G\). This results in uniformity across the samples within the bin - samples within a bin are assigned with equal probability, leading to imprecise importance samples. The second consequence comes from the fact that \(\) is not continuous, leading to an issue in training a NeRF that has a loss based on its samples. In other words, there will be a vanishing gradient effect when taking the gradient w.r.t. the samples, and one such example of a sample-based loss used for NeRFs is depth .**Volume Rendering with Piecewise Linear Opacity.We propose an elegant reformulation to the sample-based rendering equation that corresponds to the exact integral under **piecewise linear** opacity while keeping piecewise constant color leading to a simple and closed-form expression for the integral. That is, instead of piecewise constant opacity as in Eq 5, we assume a linear opacity for each interval \(I_{j}\). Concretely, for \(s[s_{j},s_{j+1}]\), where \(_{j}=(s_{j}),_{j+1}=(s_{j+1})\), we have

\[(s)=(-s}{s_{j+1}-s_{j}})_{j}+(}{s_{j+1}-s_{j}})_{j+1}.\] (10)

which is linear w.r.t. \(s[s_{j},s_{j+1}]\) as illustrated in Fig. 3.

Now, under the piecewise linear opacity assumption, transmittance is derived as the following closed-form expression:

\[T(s_{j})=[-_{0}^{s_{j}}(u)\,u]= _{k=1}^{i}[-_{s_{k-1}}^{s_{k}}(u)\,u],\] \[)=_{k=1}^{i}[-+_{k -1})(s_{k}-s_{k-1})}{2}].}\] (11)

Together with Eq. 15, this leads to the following simple and closed-form expression for \(P_{j}\), corresponding to the exact integral under the piecewise linear opacity assumption:

\[=T(s_{j})(1-[-+_{j})(s_{j +1}-s_{j})}{2}]).}\] (12)

Precision Importance Sampling.Moreover, it also turns out that with our piecewise linear opacity assumption, we are able to derive an exact closed-form solution for inverse transform sampling. Recall that in Sec 4, we pointed a drawback of the CDF \(\) being non-invertible and discontinuous under piecewise constant opacity. We show that this is alleviated in our piecewise linear setting. Concretely, given samples \(s_{1},...,s_{N}\) resulting in interval probabilities \(P_{1},...,{P_{N}}\)2 from our derivation (Eq 12), the CDF for _continuous_ random variable \(t\) is then given as

\[F(t)=_{0}^{t}p(s)\,s=_{s_{j}<t}P_{j}+_{s_{j}}^{t}p(s)\, s=_{s_{j}<t}P_{j}+_{s_{j}}^{t}(s)T(s)\,s.\] (13)

Note that unlike in piecewise constant opacity, we do not convert a continuous random variable \(s\) to a discrete random variable \(\), thus, the resulting CDF \(F\) being continuous. Now, assuming that opacity \( 0\) everywhere, from Eq. 13 we see that \(F\) is strictly increasing. Since \(F\) is continuous and strictly increasing, then it is invertible.

Finally, we have our precision importance sampling, where by inverse transform sampling, we can solve for the exact sample \(x=F^{-1}(u)\) for \(u U(0,1)\) from the given ray distribution \(p(s)\) since the CDF \(F\) is invertible under piecewise linear opacity. That is, without loss of generality, let sample \(u U(0,1)\) fall into the bin \(u[C_{k},C_{k+1}]\), where \(C_{k}=_{j<k}P_{j}\), which is equivalent to solving for \(x[s_{k},s_{k+1}]\). Reparameterizing \(x=s_{k}+t\), where \(t[0,s_{k+1}-s_{k}]\), the exact solution for sample \(u\) is given by

\[-s_{k}}{_{k+1}-_{k}}[-_{k}+^{2}+-_{k})(-)})}{(s_ {k+1}-s_{k})}}].}\] (14)

Please see supplementary for full derivation. This leads to precisely sampling from the ray distribution \(p(s)\) resulting in better importance sampling and stronger depth supervision.

Figure 3: Illustration of opacities \(\) along a ray under the piecewise constant (green) and piecewise linear (orange) assumptions.

## 7 Results

In this section, we present our experimental evaluations to demonstrate the advantages our piecewise linear opacity formulation for volume rendering, which we call **PL-NeRF**.

### Datasets, Evaluation Metrics and Implementation Details.

**Datasets and Evaluation Metrics.** We evaluate our method on the standard datasets: Blender and Real Forward Facing (LLFF) datasets as used in . We use the released training and test splits for each. See supplementary for more details. For quantitative comparison, we follow the standard evaluation metrics and report PSNR, SSIM  and LPIPS  on unseen test views. We also report the root-mean-squared-error (RSME) on the expected ray termination in our depth experiments.

**Implementation Details.** **PL-NeRF** is implemented on top of NeRF-Pytorch , a reproducible Pytorch implementation of the constant (vanilla) NeRF, where we simply change the volume rendering to our formulation under piecewise linear opacity and utilize our exact importance sampling derivation. Similar to  we optimize a separate network for the coarse and fine models that are jointly trained with the MSE loss on ground truth images. We use a batch size of 1024 rays and a learning rate of \(5 10^{-4}\) that decays exponentially to \(5 10^{-5}\) throughout the course of optimization. We train each scene for 500k iterations which takes \( 21\) hours on a single Nvidia V100 GPU 3. Our precision importance sampling enables us to use fewer samples for the fine network, hence keeping the total number of rendering samples the same, we use 128 coarse samples and 64 fine samples to train and test our method.

### Experiments on Blender and LLFF Datasets

We first evaluate our **PL-NeRF** on the standard Blender and Real Forward Facing datasets. Table 1 shows that our **PL-NeRF** (linear) outperforms the vanilla  (constant) model that assumes piecewise constant opacity in all metrics for both the synthetic Blender and Real Forward Facing datasets. Figure 1 and Figure 4 show qualitative results. As shown our **PL-NeRF** is able to achieve sharper

  & **Blender** & Avg. & Chair & Drums & Ficus & Hotdog & Lego & Mat. & Mic & Ship \\  ^{}\)} & Const. (Vanilla) & 30.61 & 32.54 & 24.79 & 29.63 & 36.08 & 32.01 & 29.31 & 32.55 & 27.95 \\  & Linear (Ours) & **31.10** & **32.92** & **25.07** & **30.18** & **36.46** & **32.90** & **29.52** & **33.08** & **28.71** \\  ^{}\)} & Const. (Vanilla) & 0.943 & 0.966 & 0.918 & 0.960 & 0.975 & 0.959 & 0.943 & 0.978 & 0.846 \\  & Linear (Ours) & **0.948** & **0.969** & **0.923** & **0.965** & **0.977** & **0.966** & **0.948** & **0.981** & **0.857** \\  ^{}\)} & Const. (Vanilla) & 5.17 & 3.19 & 7.97 & 4.14 & 2.48 & 2.33 & 4.32 & 2.16 & 14.8 \\  & Linear (Ours) & **4.39** & **2.85** & **7.10** & **3.03** & **2.28** & **1.81** & **3.21** & **1.73** & **13.1** \\   ^{}\)} & **LLFF** & Avg. & Fern & Flower & Fortress & Horns & Leaves & Orchid & Room & Trex \\  ^{}\)} & Const. (Vanilla) & 27.53 & 26.79 & 28.23 & 32.53 & 28.54 & 22.35 & 21.20 & 33.03 & 27.58 \\  & Linear (Ours) & **28.05** & **26.85** & **28.71** & **32.95** & **29.38** & **22.51** & **21.25** & **33.99** & **28.79** \\  ^{}\)} & Const. (Vanilla) & 0.874 & 0.746 & 0.886 & 0.925 & 0.893 & 0.816 & 0.746 & 0.956 & 0.916 \\  & Linear (Ours) & **0.885** & **0.863** & **0.902** & **0.932** & **0.911** & **0.826** & **0.754** & **0.961** & **0.933** \\  ^{}\)} & Const. (Vanilla) & 7.37 & 9.67 & 6.34 & 2.92 & 7.26 & 11.0 & 11.8 & 4.33 & 5.66 \\  & Linear (Ours) & **6.06** & **7.92** & **4.93** & **2.46** & **5.51** & **9.59** & **10.2** & **3.54** & **4.38** \\  

Table 1: **Quantitative Results on Blender and LLFF Datasets. LPIPS scores \( 10^{2}\).**

Figure 4: **Qualitative Results for Blender and Real Forward Facing.**textures as shown in the Lego's scooper and the bread's surface in the hodog. Moreover, our approach is also able to recover less fuzzy surfaces as shown in the microphone scene (Figure 1) where training views are close to the grazing angle of its head. As illustrated, the resulting probability density of the ray corresponding to the marked pixel is peakier than constant as our precision importance sampling allows us to have better samples closer to the surface. We also see clearer ropes in ship, less cloudy interior of the drum, and more solid surfaces such as cleaner leg of the swivel chair in the room scene.

### Geometric Extraction

We also show improvement in geometric reconstruction of **PL-NeRF**. We extract the geometry from the learned density field from the trained models of PL-NeRF and Vanilla NeRF using marching cubes with a threshold of 25 following . Figure 5 shows qualitative results on the reconstruction of our piecewise linear vs the original piecewise constant formulation. As shown, we are able to better recover the holes on the body and wheels of the Lego scene as well as the interior structure inside the Mic. Moreover, interestingly, the surface of the drum is reconstructed to be transparent as visually depicted in the images, as opposed to the ground truth being opaque.

### Effectiveness of our formulation on other Radiance Field Methods

We also demonstrate our formulation's effectiveness on other radiance field methods and show that our approach can be used as a drop-in replacement to existing NeRF-based methods. We integrate our piecewise linear opacity formulation to the volume rendering integral into Mip-NeRF (**PL-MipNeRF**). Table 2 shows our quantitative results demonstrating consistent improvement across all scenes in the original hemisphere Blender dataset. Figure 6 shows qualitative examples where we see that under difficult scenarios such as when ray conflicts arise in the fine details of the Chair and in the presence of grazing angle views in the Mic, our PL-MipNeRF shows significant improvement over the baseline. Our results show that our piecewise linear opacity and piecewise constant color formulation scales well to Mip-NeRF as well. See supplementary for implementation details. We also plug our

   & Avg. & Chair & Drums & Ficus & Hotdog & Lego & Mat. & Mic & Ship \\   & Mip-NeRF & 31.76 & 33.95 & 24.39 & 31.20 & 36.12 & 33.84 & 30.55 & 34.63 & 29.41 \\  & PL-MipNeRF & **32.48** & **35.11** & **24.92** & **32.25** & **36.51** & **35.15** & **30.69** & **35.22** & **30.00** \\   & Mip-NeRF & 0.955 & 0.975 & 0.921 & 0.971 & 0.978 & 0.971 & 0.957 & 0.987 & 0.876 \\  & PL-MipNeRF & **0.959** & **0.981** & **0.928** & **0.977** & **0.980** & **0.976** & **0.959** & **0.989** & **0.882** \\   & Mip-NeRF & 3.64 & 1.80 & 6.82 & 2.35 & 1.97 & 1.44 & 2.39 & 0.973 & 11.4 \\  & PL-MipNeRF & **3.09** & **1.32** & **5.78** & **1.66** & **1.67** & **1.07** & **2.09** & **0.788** & **10.3** \\  

Table 2: **Quantitative Results of Mip-NeRF v.s. PL-MipNeRF** LPIPS scores \( 10^{2}\).

Figure 5: Geometry Extraction Qualitative Examples.

Figure 6: **Qualitative Results for Mip-NeRF vs PL-MipNeRF.** We see that under difficult scenarios such as in fine texture details of the chair and grazing angle views on the mic, PL-MipNeRF visually shows significant improvement.

piecewise linear opacity formulation into DIVeR , a voxel-based NeRF model, and show that our formulation is also an effective drop-in replacement outperforming the original DIVeR . Please see supplementary for experiment results and implementation details.

### Experiments on Close-up Views

We further consider the challenging setting of testing on cameras closer to the objects. Table 3 (top) shows quantitative results when training on the original hemisphere dataset and tested on different close-up views. As shown by the drop in metrics, the difficulty increases as the camera moves closer to the object (0.75x to 0.5x to 0.25x of the original radius) where details get more apparent. Our **PL-NeRF** outperforms the vanilla piecewise constant model in all settings and the gap (SSIM and LPIPS) between ours and the constant assumption increases as the setting becomes harder highlighting the importance of recovering shaper texture and less fuzzy surfaces.

We also consider the set-up of training with cameras with different distances to the object that result in different sets of ray samples causing conflicts. We generate training views following the data processing pipeline from  with a random distance scaling factor sampled from \(U(0.5,1.0)\). As shown in Table 3 (bottom), our **PL-NeRF** outperforms the vanilla constant baseline in all metrics across different camera distances, where the gap (LPIPS and SSIM) is also larger the closer the camera is to the object, where details are more apparent. The difficulty for the constant (vanilla) case under multiple camera distances is its sensitivity to the choice of samples along the ray. Figure 2 shows that the conflicting rays cause quadrature instability for under piecewise constant opacity leads to unstable outputs as shown by the noisy texture on the chair. For the constant, the level of noise (gold specs) and blurriness vary at different camera distances, whereas our **PL-NeRF** renders crisper and more consistent outputs even as the camera is moved closer or further for the object.

### Experiments with Depth Supervision

Finally, we also show that our **PL-NeRF** enables stronger depth supervision under our piecewise linear opacity assumption due our precision importance sampling that allows for gradients to flow to these more refined samples resulting in more accurate depth. As in previous works , we use a sample-based loss for to incorporate depth supervision4. Table 4 shows quantitative results when training and testing on the less constrained Blender dataset with cameras at random distances from the object, as described in the previous section, with depth supervision. As shown, our **PL-NeRF** outperforms the vanilla constant baseline on all metrics including depth RSME demonstrating that our approach allows for stronger depth supervision.

## 8 Conclusion

We proposed a new way to approximate the volume rendering integral that avoids quadrature instability, by considering a piecewise linear approximation to opacity and a piecewise constant approximation to color. We showed that this results in a simple closed-form expression for the integral that is easy to evaluate. We turned this into a new objective for training NeRFs that is a drop-in replacement to existing methods and demonstrated improved rendering quality and geometric reconstruction, more accurate importance sampling and stronger depth supervision.

  & PSNR\(\) & SSIM\(\) & LPIPS\(\) & RMSE\(\) \\ 
**Const. (Vanilla)** & 29.20 & 0.898 & 11.2 & 0.178 \\
**Linear (Ours)** & **29.54** & **0.905** & **10.4** & **0.147** \\  

Table 4: **Depth Supervision. Reported LPIPS score is multiplied by \(10^{2}\).**

  &  &  &  \\  Train Set & & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) \\   & Const. (Vanilla) & 20.18 & 0.612 & 54.1 & 22.80 & 0.753 & 30.4 & 25.97 & 0.867 & 14.1 \\  & Linear (Ours) & **20.34** & **0.637** & **50.0** & **23.00** & **0.767** & **27.5** & **26.28** & **0.876** & **12.6** \\   & Const. (Vanilla) & 22.30 & 0.677 & 45.7 & 25.51 & 0.811 & 23.1 & 28.02 & 0.891 & 11.3 \\  & Linear (Ours) & **22.66** & **0.705** & **41.1** & **26.04** & **0.828** & **20.3** & **28.55** & **0.900** & **9.90** \\  

Table 3: **Testing on close-up views:**_Hemisphere are with training cameras located on the original hemisphere. _Multi Dist._ with training cameras are at random distances across a depth scale range of \(0.5-1.0\) of the original hemisphere. Reported LPIPS score is multiplied by \(10^{2}\).

Acknowledgements.This work is supported by a Apple Scholars in AI/ML PhD Fellowship, a Snap Research Fellowship, a Vannevar Bush Faculty Fellowship, ARL grant W911NF-21-2-0104, a gift from the Adobe corporation, the Natural Sciences and Engineering Research Council of Canada (NSERC), the BC DRI Group and the Digital Research Alliance of Canada.