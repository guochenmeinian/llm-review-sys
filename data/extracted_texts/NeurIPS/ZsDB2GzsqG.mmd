# MagicBrush : A Manually Annotated Dataset

for Instruction-Guided Image Editing

 Kai Zhang\({}^{1}\) Lingbo Mo\({}^{1}\)1 Wenhu Chen\({}^{2}\) Huan Sun\({}^{1}\) Yu Su\({}^{1}\)

\({}^{1}\)The Ohio State University \({}^{2}\)University of Waterloo

{zhang.13253, mo.169, su.809}@osu.edu

https://osu-nlp-group.github.io/MagicBrush

###### Abstract

Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triplets (source image, instruction, target image), which supports training large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush

Figure 1: MagicBrush provides 10K manually annotated real image editing triplets (source image, instruction, target image), supporting both single-turn and multi-turn instruction-guided editing.

and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate current image editing baselines from multiple dimensions including quantitative, qualitative, and human evaluations. The results reveal the challenging nature of our dataset and the gap between current baselines and real-world editing needs.

## 1 Introduction

Applying non-trivial semantic edits to real photos has long been an interesting task in image processing . With the ever-increasing demand for visual content, image editing has become even more essential for enhancing and manipulating images in various fields including photography, advertising, and social media. Natural language, as our innate and flexible interface, serves as an easy way to guide the image editing process. As a result, text-guided image editing [21; 3; 8; 16; 14] has recently gained more popularity compared to other mask-based image editing techniques [20; 35; 23].

Many text-guided image editing methods have been proposed recently and achieved impressive results. These methods can be roughly divided into two categories: (1) zero-shot editing [2; 1; 24], these pipeline methods require massive amount of manual tuning of its hyperparameters to produce reasonable results. (2) end-to-end editing trained on synthetic datasets [4; 37; 7]. However, such silver training data may not only contain annotation errors but also not well capture the need and diversity of real-world editing cases, leading to models with limited editing and generalization abilities.

Therefore, there is an urgent need for a high-quality dataset to facilitate real-world text-guided image editing. In this paper, we present MagicBrush, a large-scale and manually annotated dataset for instruction-guided real image editing. We adopt natural language instruction [29; 4; 41; 22] for its flexibility, which enables users to easily express desired edits with phrases like _"Remove the crowd in the background"_ or others shown in Figure 1. Additionally, we extend the dataset to include the multi-turn scenario considering the editing could be conducted iteratively on an image in practice.

We employ a rigorous training and selection for crowd workers, where they need to pass a qualification quiz and undergo manual grading during a trial period. Ongoing spot-checks ensure consistent quality, and failure to maintain high standards results in elimination from the task as shown in Figure 2. During the task, qualified workers need to propose edit instructions and utilize the DALL-E 2  image editing platform to interactively synthesize target image. They will interact with the DALL-E 2 platform with different prompts and hyperparameters until they harvest their desired outputs, otherwise, the example will be dropped. Workers may perform continuous edits on the input image, leading to a series of edit turns. Each turn has a source image (may be the original or output from the previous turn), an instruction, and a target image. We refer to such a complete edit process on a real input image as an edit session. Eventually, we manually check the generated images to ensure quality. MagicBrush consists of 5,313 sessions and 10,388 turns, supporting various editing scenarios including single-/multi-turn, mask-provided, and mask-free for both training and evaluation.

Experiments show that an end-to-end editing method InstructPix2Pix , delivers much better results after fine-tuning on MagicBrush and outperforms other baselines according to human preferences. Furthermore, we conduct extensive experiments to evaluate current editing methods from multiple dimensions including quantitative, qualitative, and human evaluations. All these results reveal the challenging nature of MagicBrush and the gap between existing methods and real-world editing needs, calling for more advanced model development in the future.

## 2 Related Work

### Text-guided Image Editing

Editing real images has long been an essential task in the field of image processing  and recent text-guided image editing has drawn considerable attention. Specifically, it can be categorized into three types in terms of different forms of text.

**Global Description-guided Editing.** Previous methods build fine-grained word and image region alignment for image editing [9; 17; 18]. Recently, Prompt2Prompt  modifies words in the original prompts to perform both local editing and global editing by cross-attention control. With the re weighting technique, follow-up work Null Text Inversion  further removes the need of original caption for editing by optimizing the inverted diffusion trajectory of the input image. Imagic  optimizes a text embedding that aligns with the input image, then interpolates it with the target description, thus generating correspondingly different images for editing. In addition, Text2LIVE  trains a model to add an edit layer and combines the edit layer and input image to enable local editing. For global description-guided editing, generally CLIP  can be applied to rank generated images _w.r.t_ the alignment, thereby delivering higher-ranked results. However, the requirement for detailed descriptions of the target image poses an inconvenience for users.

**Local Description-guided Editing.** Another line of work utilizes masked regions and corresponding regional descriptions for local editing. Blended Diffusion  blends edited areas with the other parts of the image at different noise levels along the diffusion process. Imagen Editor  trains diffusion editing models by inpainting the masked objects. Local description-guided editing enables fine-grained control by using masks and preserves the other areas intact. However, this method places a greater burden on users, as they must provide additional masks. Also, this approach may be complicated for certain editing types, such as object removal due to the difficulty of describing missing elements.

**Instruction-guided Editing.** Another form of text is instruction, which describes which aspect and how an image should be edited, such as _"change the season to spring"_. Instruction-guided editing, as initially proposed in various studies , enables users to edit images without requiring elaborate descriptions or region masking. With advancements in instruction following  and image synthesis , InstructPix2Pix  and SuTI  learn to edit images using instructions. Trained with synthetic texts by fine-tuned GPT-3 and images by Prompt2Prompt , InstructPix2Pix enables image editing by following instructions. Later work HIVE  introduces more training triplets and human ranking results to provide stronger supervision signals for better model training.

### Image Editing Datasets

Table 1 compares various semantic editing datasets. Prior work  repurposes close-domain image caption datasets  for image editing. However, these datasets primarily focus on specific categories like birds and flowers, resulting in limited generalization abilities for the models trained on them. In contrast, open-domain editing meets real-world needs, but high-quality data for training are scarce and challenging to obtain. Although large-scale silver data can be automatically synthesized , Table 1 shows the quality may not be desired. EditBench  is manually curated

  
**Datasets** & **Real Image?** & **Open-domain?** & **Multi-turn?** & **\# Edits** & **Example** & **Text** & **Target** \\  Oxford-Flower  & ✓ & ✗ & ✗ & 8,189 &  _~mmecons pale yellow_ \\ _petals and green pediced_ \\ _with green oval lanes_\({}^{*}\) \\  &  _~this is a grey bar with_ \\ _a brown and yellow nail_ \\ _wing and a red head_\({}^{*}\) \\  &  _~~a flat, dark-colored_ \\ _sakeboard with yellow_ \\ _wheels_\({}^{*}\) \\  \\  InstructPix2Pix  & ✗ & ✓ & ✗ & 313,010 &  _~add a cat_\({}^{*}\) \\  \\  MagicBrush & ✓ & ✓ & ✓ & 10,388 & 
 _~make the man ride a motorcycle_\({}^{*}\) \\  \\   

Table 1: Comparison of different image editing datasets. Flower and Bird are domain-specific datasets with global descriptions of target images. EditBench adopts masks (white regions) and local descriptions as guidance, and the size (240) may be insufficient for training. Due to the automatic synthesis process, InstructPix2Pix may contain failure cases.

while it includes only 240 examples, which is insufficient for model training and comprehensive evaluations. Consequently, there is an urgent need for a manually annotated and large-scale dataset.

## 3 MagicBrush Dataset

### Problem Definition

Instruction-guided image editing aims to edit a given image following the instruction. In terms of the editing guidance type, this task can be divided into two settings: In **mask-free setting**, given a source image \(I_{s}\) and a textual instruction \(T\) of how to edit this image, models are required to generate a target image \(I_{t}\) following the instruction. In **mask-provided setting**, models take an additional free-form mask \(M\) to limit the editing region, in addition to the source image and textual instruction. This setting is easier for models but less user-friendly as it requires extra guidance (mask) from users.

Orthogonally, depending on whether the edits are conducted iteratively, we can categorize instruction-guided image editing into two scenarios: single-turn and multi-turn. In **multi-turn scenario**, models take the source image \(I_{s}\) and a sequence of textual instructions \(\{T_{1},T_{2},...,T_{n}\}\) to generate intermediate images \(\{}},...,}}\}\) and final image \(}}\). We term the entire process involving iterative edits as an edit session. The evaluation compares \(}}\) with the ground truth final image \(I_{t_{n}}\). In **single-turn scenario**, models take both the original source images and intermediate ground truth images \(\{I_{s},I_{t_{1}},...,I_{t_{n-1}}\}\) as input, editing them only once with corresponding instructions to have \(\{}},}},...,}}\}\), respectively. Note that \(}}\) and \(}}\) are usually different except when \(i=1\) where models take the same source image \(I_{s}\) and instruction \(T_{1}\). For single-turn evaluation, we compare all generated images \(\{}},}},...,}}\}\) and ground truths \(\{I_{t_{1}},I_{t_{2}},...,I_{t_{n}}\}\) pairwisely.

Among these scenarios, mask-free multi-turn editing is the most user-friendly yet challenging setting. Users can achieve complex editing goals with just textual instructions; however, this requires models to edit images iteratively, which easily leads to error accumulations.

### Dataset Annotation Pipeline

We focus on real image editing and sample source images from MS COCO dataset  for subsequent annotations. We balance 80 object classes of COCO image to increase diversity, thus reducing the over-representation of the person object while keeping the image diversity. Figure 2(a) shows the final distribution of MagicBrush, with 34.0% person-included images.

We hire crowd workers on Amazon Mechanical Turk (AMT) to manually annotate images using the DALL-E 2 platform.2 DALL-E 2 is a highly capable text-guided image synthesis platform that can generate high-quality candidate images for editing purposes. However, it requires expertise in providing specific editing guidance, including both global descriptions and masked regions. To

Figure 2: The three-stage crowdsourcing workflow designed for dataset construction.

ensure the workers could proficiently use the DALL-E 2 platform, we provide them with detailed tutorials, teaching them how to edit images by writing prompts and drawing masks. We employ a stringent worker selection process as shown in Figure 2, and ultimately select 19 workers after thorough filtering. In recognition of the workers' contributions, we spend around $1 for each edit turn, which includes payment for workers on AMT along with the DALL-E 2 platform fees. Qualified workers will interact with DALL-E 2 using various prompts and masks until they achieve desired target images. Please refer to Appendix E for more annotation details.

Specifically, starting from the first edit turn, workers propose a textual instruction \(T_{1}\), its corresponding global description, and a free-form region mask \(M_{1}\) to enable high-quality image synthesis. Then workers try to select the most description-faithful and photo-realistic synthesized image as target image. Note that workers may need to modify their descriptions and masks to find a qualified target image, or even restart with another instruction after several trials. After getting a qualified target image \(I_{t_{1}}\), workers may repeat the annotation process with a new textual instruction \(T_{2}\) based on the current target image \(I_{t_{1}}\) to obtain \(I_{t_{2}}\). In practice, we limit the max number of turns \(n\) to 3 for a session, considering workers' possible lack of motivation or inspiration for annotating more turns.

### Dataset Analysis and Quality Evaluation

**Data Composition.** Through crowdsourcing, we collect a large-scale instruction-guided image editing dataset named MagicBrush, consisting of over 5K edit sessions and more than 10K edit turns. Figure 2(b) provides the data splits, as well as the distributions of sessions with varying numbers of edits. Meanwhile, MagicBrush includes a wide range of edit instructions such as object addition/replacement/removal, action changes, color alterations, text or pattern modifications, and object quantity adjustments. The keywords associated with each edit type demonstrate a broad spectrum, covering various objects, actions, and attributes as shown in Figure 4. This diversity indicates that MagicBrush well captures a rich array of editing scenarios, allowing for comprehensive training and evaluation of instruction-guided image editing models.

**Data Quality Evaluation.** We invite five AMT workers to review 500 randomly sampled edit turns from MagicBrush, with each evaluating 100 turns. Given an edit turn (source image, edit instruction, and target image), the worker is required to measure the edited image from two aspects: _consistency_ and _image quality_. Consistency evaluates how well the editing to the original image aligns with the instruction. Image quality assesses the overall quality of the edited image, considering factors such as maintaining the visual fidelity of the original image, seamless blending of edited elements with the original image, and the natural appearance of the changes. Workers provide a score between 1 and 5 for each criterion. The average scores for consistency and image quality are reported as 4.1 and 3.9 out of 5.0, respectively. Compared to edited images by existing methods in Section 4.4, these numbers demonstrate the high quality of the MagicBrush dataset.

## 4 Experiments

### Experiment Setup

**Baselines.** For comprehensiveness, we consider multiple baselines in both mask-free and mask-provided settings. For all baselines, we adopt the default hyperparameters available in the official

Figure 3: Statistics for the MagicBrush dataset.

code repositories to guarantee reproducibility and fairness. Given that some baselines may require global and local descriptions, inspired by InstructPix2Pix , we instruct ChatGPT  to generate desired text formats. Please refer to the Appendix C.4 for prompt details. Specifically, for **mask-free editing baselines**, we consider: (1) _Open-Edit_, (2) _VQGAN-CLIP_, (3) _SD-SDEdit_, (4) _Text2LIVE_, (5) _Null Text Inversion_, (6) _InstructPix2Pix_ and its fine-tuned version on the training set of MagicBrush, (7) _HIVE_ and its fine-tuned version on MagicBrush. For **mask-provided baselines**, we consider: (1) _GILDE_ and (2) _Blended Diffusion_. Please refer to Appendix C.2 for more implementation and fine-tuning details.

**Evaluation Metrics.** We utilize L1 and L2 to measure the average pixel-level absolute difference between the generated image and ground truth image. In addition, we adopt CLIP-I and DINO, which measure the image quality with the cosine similarity between the generated image and reference ground truth image using their CLIP  and DINO  embeddings. Finally, CLIP-T [34; 7] is used to measure the text-image alignment with the cosine similarity between local descriptions and generated images CLIP embeddings. We use local description because the global one is not specific to the editing region and the edit instruction may not describe the target image.

### Quantitative Evaluation

We evaluate mask-free and mask-provided baselines separately with the same 535 sessions from test set, as the latter requires mask as additional editing guidance, making it relatively easier. For each setting, we consider single- and multi-turn editing scenarios described in Section 3.1.

**Mask-free Editing.** Table 2 shows the results of mask-free methods which are given instructions only to edit images. We have the following observations: (1) In general, all methods perform worse in the multi-turn scenario due to the error accumulation in iterative editing. (2) The off-the-shelf InstructPix2Pix  checkpoint is not competitive compared to other baselines, in both single-turn and multi-turn scenarios. However, after fine-tuning on MagicBrush, InstructPix2Pix shows significant performance improvements across all metrics, achieving the best or second-best results under most metrics. Such improvement introduced by MagicBrush is consistent on HIVE . These suggest that instruction-guided image editing models could substantially benefit from training on our MagicBrush dataset, demonstrating its usefulness. (3) Text2LIVE  performs well in L1

Figure 4: An overview of keywords in edit instructions. The inner circle depicts the types of edits and outer circle showcases the most frequent words used within each type.

and L2 evaluations, likely due to the addition of an extra editing layer that minimizes changes to the source image. As a result, edited images fail to satisfy the instructions, as evidenced by the low CLIP-T score. VQGAN-CLIP  achieves the highest CLIP-T score because it fine-tunes the model during inference with CLIP as the direct supervision. However, the edited images may change too significantly, leading to unfavorable results on other metrics.

Mask-provided Editing.Table 3 lists the results of two mask-provided methods. As observed in the mask-free setting, the multi-turn scenario is more challenging than the single-turn scenario. While both mask-provided methods achieve high scores under the CLIP-I and DINO metrics, they fail to deliver satisfactory results according to the other three metrics (L1, L2, and CLIP-T) that evaluate local regions. Notably, after tuning on MagicBrush, InstructPix2Pix  achieves better editing results than mask-provided Blended Diffusion  in terms of CLIP-I and DINO metrics. This suggests that fine-tuning with our data could maintain good image quality.

### Qualitative Evaluation

We present the results of the top-performing mask-free (Text2LIVE ) and mask-provided (GLIDE ) methods in our qualitative analysis. We also compare the original and fine-tuned checkpoints of InstructPix2Pix . Figure 5 illustrates the iterative results of these four models

    &  & L1\(\) & L2\(\) & CLIP-I\(\) & DINO\(\) & CLIP-T\(\) \\   &  \\   & Open-Edit  & 0.1430 & 0.0431 & 0.8381 & 0.7632 & 0.2610 \\  & VQGAN-CLIP  & 0.2200 & 0.0833 & 0.6751 & 0.4946 & **0.3879** \\  & SD-SDEdit  & 0.1014 & 0.0278 & 0.8526 & 0.7726 & 0.2777 \\  & Text2LIVE  & 0.0636 & **0.0169** & 0.9244 & 0.8807 & 0.2424 \\  & Null Text Inversion  & 0.0749 & 0.0197 & 0.8827 & 0.8206 & 0.2737 \\   &  \\   & HIVE  & 0.1092 & 0.0341 & 0.8519 & 0.7500 & 0.2752 \\  & w/ MagicBrush & 0.0658 & 0.0224 & 0.9189 & 0.8655 & 0.2812 \\  & InstructPix2Pix  & 0.1122 & 0.0371 & 0.8524 & 0.7428 & 0.2764 \\  & w/ MagicBrush & **0.0625** & 0.0203 & **0.9332** & **0.8987** & 0.2781 \\   &  \\   & Open-Edit  & 0.1655 & 0.0550 & 0.8038 & 0.6835 & 0.2527 \\  & VQGAN-CLIP  & 0.2471 & 0.1025 & 0.6606 & 0.4592 & **0.3845** \\  & SD-SDEdit  & 0.1616 & 0.0602 & 0.7933 & 0.6212 & 0.2694 \\  & Text2LIVE  & 0.0989 & **0.0284** & 0.8795 & 0.7926 & 0.2716 \\  & Null Text Inversion  & 0.1057 & 0.0335 & 0.8468 & 0.7529 & 0.2710 \\   &  \\   & HIVE  & 0.1521 & 0.0557 & 0.8004 & 0.6463 & 0.2673 \\  & w/ MagicBrush & 0.0966 & 0.0365 & 0.8785 & 0.7891 & 0.2796 \\  & InstructPix2Pix  & 0.1584 & 0.0598 & 0.7924 & 0.6177 & 0.2726 \\  & w/ MagicBrush & **0.0964** & 0.0353 & **0.8924** & **0.8273** & 0.2754 \\   

Table 2: Quantitative study on mask-free baselines on MagicBrush test set. Multi-turn setting evaluates the final target images that iteratively edited on the first source images in edit sessions. The best results are marked in **bold**.

  
**Settings** & **Methods** & L1\(\) & L2\(\) & CLIP-I\(\) & DINO\(\) & CLIP-T\(\) \\   & GLIDE  & **3.4973** & **115.8347** & **0.9487** & **0.9206** & 0.2249 \\  & Blended Diffusion  & 3.5631 & 119.2813 & 0.9291 & 0.8644 & **0.2622** \\   & GLIDE  & **11.7487** & **1079.5997** & **0.9094** & **0.8494** & 0.2252 \\  & Blended Diffusion  & 14.5439 & 1510.2271 & 0.8782 & 0.7690 & **0.2619** \\   

Table 3: Quantitative study on mask-provided baselines on MagicBrush test set. L1, L2, and CLIP-T are measured over the masked regions only. The best results are marked in **bold**.

and ground truth images from MagicBrush. Both Text2LIVE and GLIDE are unsuccessful in editing the man's face and clothes. The original InstructPix2Pix changes the images following the instructions; however, the resulting images exhibit excessive modification and lack photorealism. Fine-tuning InstructPix2Pix on MagicBrush alleviates this issue, but the images remain notably inferior to the ground truth ones. Please see Appendix D for more examples of qualitative evaluation.

### Human Evaluation

We conduct comprehensive human evaluations to assess both _consistency_ and _image quality_ on generated images. Our evaluations encompass three tasks: multi-choice image comparison, one-on-one comparison, and individual image evaluation. We randomly sample 100 image examples from test set for each task and hire 5 AMT workers as evaluators to perform the tasks. For each task, the images are evenly assigned to evaluators and the averaged scores (if applicable) are reported.

**Multi-choice Comparison.** The multi-choice comparison involves four top-performing methods in Table 2 and Table 3, including Text2LIVE, GLIDE, InstructPix2Pix, and fine-tuned InstructPix2Pix on MagicBrush. For each example, evaluators need to select the best edited image based on consistency and image quality, respectively. The results in Table 4 indicate that fine-tuned InstructPix2Pix attains the highest performance, significantly surpassing the other three methods. This outcome validates the effectiveness of training on our MagicBrush dataset. Interestingly, while Text2LIVE achieves a high score in auto evaluation, its performance in human evaluation appears to be less desirable, especially in terms of the instruction consistency. This indicates current automatic metrics that focus on the overall image quality may not align well with human preferences, emphasizing the need for future research to develop better automatic metrics.

Figure 5: Qualitative evaluation of multi-turn editing scenario. We provide all baselines their desired input formats (e.g., masks and local descriptions for GLIDE).

**One-on-one Comparison.** The one-on-one comparison provides a detailed and nuanced evaluation of the fine-tuned InstructPix2Pix by comparing it against strong baselines and ground truth. Evaluators are asked to determine the preferred option based on consistency and image quality, respectively. We divide the comparisons into two scenarios as mentioned in Section 3.1: (1) In the single-turn scenario, we compare fine-tuned InstructPix2Pix and two other methods (InstructPix2Pix and Text2LIVE). As shown in Table 5, fine-tuned InstructPix2Pix consistently outperforms the other two methods in terms of both consistency and image quality. (2) In the multi-turn scenario, we compare the fine-tuned InstructPix2Pix with ground truth images to observe how the quality of edited images varies across different turns. The results reveal that the performance gap generally widens as the number of edit turn increases. This finding highlights the challenges associated with error accumulation in current top-performing models and underscores the difficulties posed by our dataset.

**Individual Evaluation.** The individual evaluation employs a 5-point Likert scale to measure the quality of individual images generated by four specific models, gathering subjective user feedback. Evaluators are asked to rate the images on a scale from 1 to 5, assessing both consistency and image quality. Each evaluator receives an equal share of the images, specifically evaluating 80 images in total, with 20 images from each of the four models. The results in Table 6 clearly demonstrate that fine-tuned InstructPix2Pix outperforms Text2LIVE and GLIDE, and further improves upon the performance of InstructPix2Pix. This finding highlights the advantages of training or fine-tuning models using the MagicBrush dataset.

## 5 Conclusion and Future Work

In this work, we present MagicBrush, a large-scale and manually annotated dataset for instruction-guided real image editing. Although extensive experiments show that InstructPix2Pix fine-tuned on

    & **Consistency** & **Image Quality** \\  Text2LIVE  & 1.1 & 2.8 \\ GLIDE  & 1.8 & 2.8 \\ InstructPix2Pix  & 3.0 & 3.2 \\ Fine-tuned InstructPix2Pix & **3.1** & **3.6** \\   

Table 6: Individual evaluation using a 5-point Likert scale. The numbers in the table represent the average scores calculated for each aspect.

    &  &  \\   & Fine-tuned InstructPix2Pix & InstructPix2Pix  & The & Fine-tuned InstructPix2Pix & InstructPix2Pix  & Tie \\  & **40** & 35 & **25** & **48** & 33 & 19 \\  & Fine-tuned InstructPix2Pix & Text2LIVE  & Tie & Fine-tuned InstructPix2Pix & Text2LIVE  & Tie \\  & **68** & 4 & 28 & **61** & 19 & 20 \\    & Fine-tuned InstructPix2Pix & GT (Turn 1) & Tie & Fine-tuned InstructPix2Pix & GT (Turn 1) & Tie \\  & 13 & **72** & 15 & 19 & **64** & 17 \\   & Fine-tuned InstructPix2Pix & GT (Turn 2) & Tie & Fine-tuned InstructPix2Pix & GT (Turn 2) & Tie \\   & 13 & **80** & 7 & 19 & **60** & 21 \\   & Fine-tuned InstructPix2Pix & GT (Turn 3) & Tie & Fine-tuned InstructPix2Pix & GT (Turn 3) & Tie \\   & 11 & **80** & 9 & 6 & **75** & 19 \\   

Table 4: Multi-choice comparison of four methods. The numbers represent the frequency of each method being chosen as the best for each aspect.

    & **Consistency** & **Image Quality** \\  Text2LIVE  & 1.1 & 2.8 \\ GLIDE  & 1.8 & 2.8 \\ InstructPix2Pix  & 3.0 & 3.2 \\ Fine-tuned InstructPix2Pix & **3.1** & **3.6** \\   

Table 5: One-on-one comparisons between fine-tuned InstructPix2Pix and other methods including InstructPix2Pix and Text2LIVE, as well as ground truth (GT). The numbers in the table indicate the frequency of each method being chosen as the better option. To account for scenarios where two methods perform equally, we include a “Tie” option in each question for comprehensive evaluation.

MagicBrush achieves the best results, its edited images are still notably inferior compared to the ground truth ones. This observation indicates the effectiveness of our dataset for training and the gap between current methods and real-world editing needs. We hope MagicBrush will contribute to the development of more advanced models and human-preference-aligned evaluation metrics for instruction-guided real image editing in the future.