# Training Data Attribution via Approximate Unrolling

Juhan Bae\({}^{1,2}\), Wu Lin\({}^{2}\), Jonathan Lorraine\({}^{1,2,3}\), Roger Grosse\({}^{1,2,4}\)

\({}^{1}\)University of Toronto, \({}^{2}\)Vector Institute, \({}^{3}\)NVIDIA, \({}^{3}\)Anthropic

{jbae, lorraine, rgrosse}@cs.toronto.edu

wu.lin@vectorinstitute.ai

###### Abstract

Many training data attribution (TDA) methods aim to estimate how a model's behavior would change if one or more data points were removed from the training set. Methods based on implicit differentiation, such as influence functions, can be made computationally efficient, but fail to account for underspecification, the implicit bias of the optimization algorithm, or multi-stage training pipelines. By contrast, methods based on unrolling address these issues but face scalability challenges. In this work, we connect the implicit-differentiation-based and unrolling-based approaches and combine their benefits by introducing Source, an approximate unrolling-based TDA method that is computed using an influence-function-like formula. While being computationally efficient compared to unrolling-based approaches, Source is suitable in cases where implicit-differentiation-based approaches struggle, such as in non-converged models and multi-stage training pipelines. Empirically, Source outperforms existing TDA techniques in counterfactual prediction, especially in settings where implicit-differentiation-based approaches fall short.

## 1 Introduction

Training data attribution (TDA) techniques are motivated by understanding the relationship between training data and the properties of trained models . They have diverse applications in machine learning, such as detecting mislabeled data points , crafting data poisoning attacks , and curating datasets . Many TDA methods aim to perform a _counterfactual prediction_, which estimates how a trained model's behavior would change if certain data points were removed from (or added to) the training dataset. Unlike sampling-based approaches, which require repeated model retraining with different subsets of the dataset, gradient-based TDA techniques estimate an infinitesimal version of the counterfactual without model retraining. Two main strategies for gradient-based counterfactual TDA are _implicit differentiation_ and _unrolling_.

Implicit-differentiation-based TDA, most notably influence functions , uses the Implicit Function Theorem  to estimate the sensitivity of the optimal solution to downweighting a training data point. These methods are well-motivated for models with strongly convex objectives and provide convenient estimation algorithms that depend solely on the optimal model parameters rather than intermediate checkpoints throughout training. However, the classical formulation relies on assumptions such as the uniqueness of and convergence to the optimal solution, which limits their applicability to modern neural networks .

By contrast, unrolling-based methods, such as Sgd-Influence, approximate the impact of downweighting a data point's gradient update on the final model parameters by backpropagating through the preceding optimization steps. Unrolling is conceptually appealing in modern neural networks because it does not rely on the uniqueness of or convergence to the optimal solution. Furthermore, it can incorporate details of the training process, such as the choice of optimizer, learning rate schedules, or a data point's position during training. For example, unrolling-basedapproaches can support TDA for multi-stage training procedures, such as in continual learning or foundation models, where the model undergoes multiple training phases with different objectives or datasets. However, they require storing all intermediate variables generated during the training process (_e.g._, parameter vectors for each optimization step) for backpropagation, which can be prohibitively expensive for large-scale models. Past works have considered applying unrolling to only the last epoch for large-scale models [31; 10], restricting their applicability in analyzing the effect of removing a data point at the beginning of training or in analyzing multi-stage training procedures.

In this work, we connect implicit-differentiation-based and unrolling-based approaches and introduce a novel algorithm that enjoys the advantages of both methods. We start from the unrolled differentiation perspective and, after introducing suitable approximations, arrive at an influence-function-like estimation algorithm. The key idea is to divide the training trajectory into one or more segments and approximate the distributions of gradients and Hessians as stationary within each segment. These segments may represent explicit training stages, such as in continual learning or foundation models, or changes in the Hessian and gradients throughout training. We use these estimated statistical summaries for each segment to approximate unrolling. Hence, we call our method Source (**S**egmented ****s**ati**O**nary **U**n**Rolling for **C**ounterfactual **E**stimation). While our method approximately coincides with influence functions in the simple setting of a deterministic objective optimized to convergence, it applies to more general settings where unrolling is typically required.

Source inherits several key advantages from unrolling. Firstly, it allows the attribution of data points at different stages of training, providing a more comprehensive framework for TDA. Secondly, Source can incorporate algorithmic choices into the analysis, accounting for learning rate schedules and the implicit bias of optimizers such as SGD or Adam . Lastly, it maintains a close connection with the counterfactuals, even in cases where the assumptions made in implicit-differentiation-based methods, such as the optimality of the final parameters, are not met. However, unlike unrolling, Source does not require storing all intermediate optimization variables generated during training; instead, it leverages only a handful of model checkpoints.

We evaluate Source for counterfactual prediction across various tasks, including regression, image classification, and text classification. Source outperforms existing TDA techniques in approximating the effect of retraining the network without groups of data points and identifying training data points that would flip predictions on some test examples when trained without them. Source demonstrates distinct advantages in scenarios where traditional implicit-differentiation-based methods fall short, such as models that have not fully converged or those trained in multiple stages. Our empirical evidence suggests that Source is a valuable TDA tool in various scenarios.

## 2 Background

Consider a finite training dataset \(\{_{i}\}_{i=1}^{N}\). We assume that the model parameters \(^{D}\) are optimized with a gradient-based iterative optimizer to minimize the empirical risk on this dataset:

\[(,)_{i=1}^{N} (_{i},), \]

where \(\) is the (twice-differentiable) loss function. We use the notation \(^{}()\) to denote the optimal solution obtained when the model is trained on a specific subset of the dataset \(\), and \(^{}\) to denote the optimal solution on the full dataset. In practice, it is common to employ parameters \(^{s}\) that approximately minimize the empirical risk (_e.g._, the result of running an optimization algorithm for \(T\) iterations), as obtaining the exact optimal solution for neural networks can be challenging and may lead to overfitting . When necessary, we use the notation \(^{s}(;,)\) to indicate the final parameters obtained by training with the dataset \(\), along with hyperparameters \(\) (_e.g._, learning rate and number of epochs) and random choices \(\) (_e.g._, parameter initialization and mini-batch order).

### Training Data Attribution

TDA aims to explain model behavior on a query data point \(_{q}\) (_e.g._, test example) by referencing data points used to fit the model. The model behavior is typically quantified using a measurement \(f(_{q},)\), selected based on metrics relevant to the analysis, such as loss, margin, or log probability. Given hyperparameters \(\) and a training data point \(_{m}\), an attribution method \((_{q},_{m},;)\) assignsa score to a training data point, indicating its _importance_ in influencing the expected measurable quantity \(_{}[f(_{q},^{*}(;,))]\), where the expectation is taken over the randomness in the training process. In cases where an optimal solution to Equation (1) exists, is unique, and can be precisely computed, and TDA is performed on this optimal solution, the TDA method is simply written as \((_{q},_{m},)\).

One idealized TDA method is _leave-one-out_ (LOO) retraining , which assesses a data point's importance through counterfactual analysis. Assuming the above optimality condition is satisfied, for a chosen query data point \(_{q}\) and a training data point \(_{m}\), the LOO score can be formulated as:

\[_{}(_{q},_{m},) f(_{q}, ^{*}(\{_{m}\}))-f(_{q},^{*}). \]

When the measurement is defined as the loss, a higher absolute LOO score signifies a more substantial change in the query loss when the data point \(_{m}\) is excluded from the training dataset, particularly when the model parameters are optimized for convergence.

### Influence Functions

Influence functions estimate the change in optimal parameters resulting from an infinitesimal perturbation in the weight of a training example \(_{m}\). Assuming that an optimal solution to Equation (1) exists and is unique for various values of the data point's weight \([-1,1]\), the relationship between this weight and the optimal parameters is captured through the _response function_:

\[r()*{arg\,min}_{}(,)+(_{m},). \]

Influence functions approximate Equation (3) using the first-order Taylor expansion around \(=0\):

\[r() r(0)+r}{}_{ =0}=^{*}-^{-1}_{} (_{m},^{*}), \]

where the Jacobian of the response function \(r}}{{}}|_{=0}\) is obtained using the Implicit Function Theorem  and \(_{}^{2}(^{*}, )\) represents the Hessian of the cost function at the optimal solution. The change in the optimal parameters due to the removal of \(_{m}\) can be approximated by setting \(=-1\):

\[^{*}(\{_{m}\})-^{*} ^{-1}_{}(_{m},^{*}). \]

By applying the chain rule of derivatives, influence functions estimate the change in a measurable quantity for a query example \(_{q}\) as:

\[_{}(_{q},_{m},)_{}f(_{q},^{*})^{}^{-1}_{}(_{m},^{*}). \]

We refer readers to Koh and Liang  for detailed derivations and discussions of influence functions. As observed in Equation (6), influence functions provide algorithms that only depend on the optimal parameters \(^{*}\) rather than intermediate checkpoints. However, when applied to neural networks, the connection to the counterfactual prediction is tenuous due to the unrealistic assumptions that the optimal solution exists, is unique, and can be found . In practice, the gradients and Hessian are computed using the final parameters \(^{*}\) from a single training run instead of the optimal solution.

## 3 Methods

In this section, we introduce Source, a gradient-based TDA technique combining the advantages of implicit and unrolled differentiation. We motivate our approach from the unrolling perspective and, after introducing suitable approximations, arrive at an influence-function-like algorithm. Finally, we describe a practical instantiation of Source by approximating the Hessian with the Eigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)  parameterization.

### Motivation: Unrolling for Training Data Attribution

Consider optimizing the model parameters using SGD with a fixed batch size \(B\), starting from the initial parameters \(_{0}\).1 The update rule at each iteration is expressed as follows:

\[_{k+1}_{k}-}{B}_{i=1}^{B} _{}(_{ki},_{k}), \]where \(_{k}\) denotes the learning rate for iteration \(k\), \(_{ki}\) is the \(i\)-th data point in \(_{k}\), where \(_{k}\) denotes a mini-batch of examples drawn randomly with replacement from the training dataset \(\), and \(T\) denotes the total number of iterations. We aim to understand the effect of removing a training data point \(_{m}\) on the terminal model parameters \(_{T}\). To this end, we parameterize the weight of \(_{m}\) as \(1+\), where \(=0\) corresponds to the original training run and \(=-1\) represents the removal of a data point. This parameterization results in the following update rule:

\[_{k+1}()_{k}()- }{B}_{i=1}^{B}(1+_{ki})_{}(_{ki},_{k}()), \]

where \(_{ki}[_{ki}=_{m}]\) is the indicator function for having selected \(_{m}\). The dependence of \(\) on \(\) will usually be suppressed for brevity. Similarly to other gradient-based TDA methods, such as influence functions, we approximate the change in the terminal parameters due to the data removal \(_{T}(-1)-_{T}(0)\) with its first-order Taylor approximation \(_{T}}}{{}}\) (the notation \(|_{=0}\) is suppressed as it will always be evaluated at \(=0\)). Let \(_{k}\) denote the number of times \(_{m}\) is chosen in batch \(_{k}\). By chain rule, the contribution of iteration \(k\) to the total derivative \(_{T}}}{{}}\) can be found by multiplying all the Jacobian matrices along the accumulation path, giving the value \(-}{B}_{k}}_{k+1:T}}_{k}\), where:

\[}_{k}_{k+1}}{_{k}}=}-_{k}}_{k},}_{ k:k^{}}_{k^{}}}{_{k}}=}_{k^{}-1}}_{k+1}}_{k},}_{k}_{}(_{m},_{k}). \]

Here, \(}_{k}_{i=1}^{B}_{}^{2 }(_{ki},_{k})\) is the mini-batch Hessian for iteration \(k\) and we define \(}_{k:k}}\) for any \(0 k<T\) by convention. A simplified illustration of unrolling is shown in Figure 1.

In contrast to influence functions, unrolling does not assume uniqueness or convergence to the optimal solution. An illustrative comparison of the two approaches is shown in Figure 2. Exact influence functions differentiate the response function (Equation (4)), estimating the sensitivity of the optimal solution (\(\)) to downweighting a data point. By contrast, unrolling estimates the sensitivity of the _final_ model parameters (at the end of training) to downweighting a data point; hence, it can account for details of the training process such as learning rate schedules, implicit bias of optimizers, or a data point's position during training. For instance, in our illustrative example, gradient descent optimization is stopped early, such that the optimizer makes much progress in the high curvature direction and little in the low curvature direction. Unrolling-based TDA (but not implicit differentiation) accounts for this effect, resulting in a smaller influence along the low curvature direction.

The effect of removing \(_{m}\) on any single training trajectory may be noisy and idiosyncratic. For stability, we instead consider the expectation over training trajectories, where the selection of training examples in each batch (and all downstream quantities such as the iterates \(_{k}\)) are treated as random variables.2 We are interested in the average treatment effect \([_{T}(-1)-_{T}(0)]\), where the expectation is over the batch selection, and approximate this quantity with \(-[_{T}}}{{}}]\). The expected

Figure 1: A simplified illustration of unrolled differentiation in SGD with a batch size of \(1\) and a data point of interest \(_{m}\) appearing at iteration \(k\). Unrolling backpropagates through the optimization steps from \(_{T}\) to compute the total derivative with respect to \(\).

Figure 2: Illustrative comparision of influence functions and unrolling-based TDA. Each contour represents the cost function at different values of \(\), which controls the degree of downweighting a data point \(_{m}\).

total derivative can be expanded as a sum over all iterations, applying linearity of expectation:

\[[_{T}}{} ]=[-_{k=0}^{T-1}}{B}_{k}_{k+1:T}_{k}]=-_{k=0}^{T-1}}{B} [_{k}_{k+1:T}_{k}]. \]

In principle, we could compute a Monte Carlo estimate of this expectation by averaging many training trajectories. For each trajectory, \(_{T}}}{{}}\) can be evaluated using reverse accumulation (_i.e._, backpropagation) on the computation graph. However, this approach is prohibitively expensive as it requires storing all intermediate variables for the backward pass. Furthermore, many Monte Carlo samples may be required to achieve accurate estimates.

### Segmenting the Training Trajectory

To derive a more efficient algorithm for approximating expected total derivative \([_{T}}}{{ }}]\), we now partition the training procedure into \(L\) segments and approximate the reverse accumulation computations for each segment with statistical summaries thereof (instead of storing all intermediate variables). Our motivations for segmenting the training procedure are twofold. First, the training procedure may explicitly include multiple stages with distinct objectives or datasets, as in continual learning or foundation models. Second, the Hessians and gradients are likely to evolve significantly over training, and segmenting the training allows us to approximate their distributions as stationary within a segment (rather than over the entire training run).

We index the segments as \(=1,,L\), with segment boundaries denoted as \(T_{}\). By convention, \(T_{L} T\) and \(T_{0} 0\) denote the end of training and beginning of training, respectively, and \(K_{} T_{}-T_{-1}\) denotes the total number of iterations within a segment. Conceptually, we can compute \(_{T}}}{{}}\) using reverse accumulation over a coarse-grained computation graph represented in terms of segments rather than individual iterations. The Jacobian associated with each segment is denoted as \(_{}_{T_{-1}:T_{}}\). To approximate the expected total derivative \([_{T}}}{{ }}]\), we first rewrite Equation (10) using the segment notation introduced. We then approximate the Jacobians of different segments as statistically independent (see discussion below):

\[[_{T}}{} ]=-[_{=1}^{L}(_{^{}=L}^{+ 1}_{^{}})}^{T_{}- 1}}{B}_{k}_{k+1:T_{}}_{k}}_{ _{}}]-_{=1}^{L}(_{^{ }=L}^{+1}[_{^{}}])[_{}], \]

where \(\) uses our independence approximation to push the expectations inward. Note that our product notation \(_{^{}=L}^{+1}\) takes \(^{}\) in decreasing order from \(L\) down to \(+1\).

To obtain tractable approximations for \([_{}]\) and \([_{}]\), we approximate the Hessian and gradients distributions as stationary within each segment. This implies that the Hessians within a segment share a common mean \(}_{}[_{k}]\) for \(T_{-1} k<T_{}\). Analogously, the gradients within a segment share a common mean \(_{}[_{k}]\). Moreover, we approximate the step sizes within each segment with their mean \(_{}\). If these stationarity approximations are too inaccurate (_e.g._, \([_{k}]\) and/or \([_{k}]\) change rapidly throughout the segment), one can improve the fidelity by carving the training trajectory into a larger number of segments, at the expense of increased computational and memory requirements. Finally, we approximate the Hessians and gradients in different time steps as statistically independent.3

**Approximation of \([_{}]\).** We approximate \([_{}]\) in Equation (11) as follows:

\[[_{}]=[_{T_{-1}:T_{}}] (-_{}}_{})^{K_{ }}(-_{}K_{}}_{}) }_{}, \]

where the first \(\) uses the stationary and independence approximations, and the second \(\) uses the definition of matrix exponential. One can gain an intuition for \(}_{}\) by observing that it is a matrix function of \(}_{}\).4 Let \(}_{}=^{}\) be the eigendecomposition of \(}_{}\) and let \(_{j}\) be the \(j\)-th eigenvalue of \(}_{}\). The expression in Equation (12) can be seen as applying the function \(F_{}()(-_{}K_{})\) to each of the eigenvalues \(\) of \(}_{}\). The value is close to zero in high-curvature directions, so the training procedure "forgets" the components of \(\) which lie in these directions. However, information about \(\) is retained throughout the \(\)-th segment for low-curvature directions.

**Approximation of \([_{}]\).** We further approximate \([_{}]\) as follows:

\[[_{}] =[_{k=T_{-1}}^{T_{}-1}}{ B}_{k}_{k+1:T_{}}_{k}]_{k=T _{-1}}^{T_{}-1}_{}(-_{}}_{})^{T_{}-1-k}}_{} \] \[=(-(-_{}}_{})^{K_{}})}_{}^{-1}}_{ }-(-_{}K_{ }}_{}))}_{}^{-1}}_{} }_{}}_{:=F_{}()}, \]

where Equation (13) uses the stationary and independence approximations and \([_{k}]=}{{N}}\), and Equation (14) uses the finite series5 and the definition of the matrix exponential. Because all the matrices commute, \(}_{}\) can also be written in terms of a matrix function, defined as \(F_{}()(1-_{}K_{})})/\). In high-curvature directions, this term approaches to \(}{{}}\), whereas in low-curvature directions, it approaches to \(_{}K_{}\). The qualitative behavior of \(F_{}\) can be captured with the function \(F_{}() 1/(+)\), where \(=_{}^{-1}K_{}^{-1}\), as shown in Figure 6 (Appendix C). Applying this to \(}_{}\) results in approximating Equation (14) with the damped inverse-Hessian-vector product \((}_{}+)^{-1}}_{}\). This is essentially the formula for influence functions, except that \(}_{}\) and \(}_{}\) represent the expected Hessian and gradient rather than the terminal one, and our analysis yields an explicit formula for the damping parameter \(\). Hence, influence functions can be regarded approximately as a special case with only a single segment, so our damped unrolling analysis gives an alternative motivation for influence functions.

**Full Procedure.** We derived a closed-form term to approximate the expected total derivative:

\[[_{T}}{}] -_{=1}^{L}(_{^{}=L}^{+1} }_{^{}})}_{}, \]

where \(}_{}\) and \(}_{}\) are obtained with Equation (12) and Equation (14), respectively. We term our algorithm Source (**S**egmented \(}\)**O**nary **U**nRolling for **C**ounterfactual **E**stimation) and refer readers to Figure 3 for a visual illustration. Similarly to unrolling, Source can incorporate fine-grained information about optimization trajectories into the analysis. For instance, Source can support TDA for non-converged models, accounting for the total number of iterations \(T\) the model was trained with. It can also support TDA for multi-stage training pipelines: when the model was sequentially trained with two datasets \(_{1}\) and \(_{2}\), Source can compute the contribution of a data point \(_{m}_{1}\) that appeared in the first segment by partitioning the training trajectory into two segments and computing the expected total derivative at the first segment with \(-}}_{2}}_{1}\), where \(N_{1}\) is the size of the first training dataset.

Given terminal parameters \(_{T}\) from a single training run and a query data point \(_{q}\), Source approximates the change in the measurable quantity due to the removal of a training data point \(_{m}\) as:

\[_{}(_{q},_{m},;) _{}f(_{q},_{T})^{}(_{=1}^{L }(_{^{}=L}^{+1}}_{^{}}) }_{}). \]

Unlike the single-training-run estimator for unrolling-based approaches, Source does not require access to the exact location where the data point \(_{m}\) was used during training, as it estimates the averaged effect of removing a data point within a given segment. To further account for other sources of randomness, such as model initialization, the multiple-training-run estimator for Source averages the final scores in Equation (16) obtained for each training run with different random choices.

### Practical Algorithm for Source

We now describe an instantiation of Source which is practical to implement. Given the \(C\) model checkpoints saved during training, Source begins by organizing them into \(L\) distinct segments. These segments may represent explicit stages in training (_e.g._, continual learning) or account for the change in Hessian and gradient throughout training. Within each segment \(\), Source estimates the stationary Hessian \(}_{}\) and gradient \(}_{}\) by averaging the Hessian and gradient across all checkpoints in the segment. Source further estimates \(_{}\) by averaging the learning rates used within a segment.

However, computing Equation (15) has two practical bottlenecks for neural networks: computation of the Hessian and its matrix exponential. We fit a parametric approximation to the Hessian using EK-FAC . EK-FAC parameterization is convenient for Source as the approximate Hessian has an explicit eigendecomposition, which enables efficient computation of \(}_{}\) and \(}_{}\) by applying appropriate matrix functions to the eigenvalues. Note that EK-FAC approximates the Hessian with the Gauss-Newton Hessian (GNH) . Unlike the Hessian, the GNH is guaranteed to be positive semi-definite, as long as the loss function is convex in the model outputs . The GNH approximation within EK-FAC is also advantageous for Source as it can avoid numerical instability in computing Equation (15), especially when the Hessian has negative eigenvalues. The implementation details are provided in Appendix D.

Compared to influence functions with the same EK-FAC approximation , Source requires computing the EK-FAC factors and training gradients for each model checkpoint when performing TDA on all segments. Hence, Source is \(C\) times more computationally expensive, where \(C\) is the number of checkpoints. In Appendix F.2, we introduce a more computationally efficient version of Source, where we average the parameters within a segment instead of averaging Hessians and gradients. This variant of Source is \(L\) times more computationally expensive than influence functions, as the EK-FAC factors and gradients only need to be computed once for each segment.

While we described one instantiation of Source with the EK-FAC approximation, we note that Source can be integrated with other techniques used for approximating implicit-differentiation-based TDA methods, such as Trak, DataINF , and Logra. For example, with Trak, we can use random projection  and efficiently compute the averaged Hessian and gradients in the lower-dimensional space. Trak can be advantageous over the EK-FAC approximation when there are a large number of query data points, as it caches the compressed training gradients in memory, avoiding the need to recompute them for each query.

## 4 Related Works

Modern TDA techniques for neural networks can be broadly categorized into three main groups: sampling-based, representation-based, and gradient-based. For a comprehensive overview of TDA, including practical applications, we refer the reader to Hammoudeh and Lowd  and Mucsanyi et al. . Sampling-based (or retraining-based) approaches, such as Shapley-value estimators [78; 21; 39; 54; 87], Downsampling[17; 96], Datamodels, and Data Banzhaf[5; 86], approximate counterfactuals by repeatedly retraining models on different data subsets. Although effective, these methods are often impractical for modern neural networks due to the significant computational cost of repeated model retraining.

Figure 3: A simplified illustration of Source with \(3\) segments (\(L=3\)). Source divides the training trajectory into one or more segments and approximate the gradient \(}_{}\) and Hessian \(}_{}\) distributions and learning rate \(_{}\) as stationary within each segment \(\) to approximate unrolling. Source does not require storing the entire intermediate variables throughout training. Instead, it requires a handful of checkpoints throughout training to approximate the means of the Hessians and gradients.

Representation-based techniques evaluate the relevance between a training and query data point by examining the similarity in their representation space (_e.g._, the output of the last hidden layer) [9; 30]. These techniques offer computational advantages compared to other attribution methods, as they only require forward passes through the trained network. Rajani et al.  further improves efficiency by caching all hidden representations of the training dataset and using approximate nearest neighbor search . Past works have also proposed model-agnostic TDA approaches, such as computing the similarity between query and training sequences with BM25  for language models [1; 56] or with an embedding vector obtained from a separate pre-trained self-supervised model for image classification tasks . However, representation-based and input-similarity-based techniques lack a connection to the counterfactual and do not provide a notion of negatively influential data points.

Two main strategies for gradient-based TDA are implicit differentiation and unrolling. To the best of our knowledge, the largest model to which exact unrolling has been applied is a \(300\) thousand parameter model . Our experiments in Section 5 cover TDA for models ranging from \(560\) thousand parameters (MNIST & MLP) to \(120\) million parameters (WikiText-2 & GPT-2). SGD-Influence  also considers applying unrolling to only the last epoch for large-scale models. However, this limits its applicability in analyzing the effect of removing a data point at the beginning of training or analyzing multi-stage training processes. In contrast, Hydra approximates the mini-batch Hessian \(_{k}\) in Equation (10) as zero when computing the total derivatives, avoiding the need to compute Hessian-vector products (HVPs) for each optimization step. However, in Appendix F.1, we empirically observe that an accurate approximation of the Hessian is important to achieve good TDA performance. Both approaches require storing a large number of optimization variables during training. Relatedly, Nickl et al.  use local perturbation methods  to approximate the data point's sensitivity to the training trajectory.

Apart from implicit-differentiation-based and unrolling-based approaches, TracIn is another prominent gradient-based TDA technique, which estimates the importance of a training data point by approximating the total change in the query's measurable quantity with the gradient update from this data point throughout training. Similarly to Source, the practical version of TracIn (TracInCP) leverages intermediate checkpoints saved during training. While TracInCP is straightforward to implement as it does not involve approximation of the Hessians, its connection to the counterfactual is unclear [27; 77]. However, past works have shown its strengths in downstream tasks, such as mislabeled data detection  and curating fine-tuning data .

## 5 Experiments

Our experiments investigate two key questions: (1) How does Source compare to existing TDA techniques, as measured by the linear datamodeling score (LDS)  and through subset removal counterfactual evaluation [33; 93; 35; 97; 70; 8; 79; 19]? (2) Can Source support data attribution in situations where implicit-differentiation-based approaches struggle, particularly with models that have not converged or have been trained in multiple stages with different objectives or datasets?

We compare Source against existing TDA techniques: representation similarity (RepSim) [9; 30], TracIn, Trak, and influence functions (IF) with the EK-FAC approximation . For consistency with Park et al. , the measurement \(f\) is defined as the margin for classification tasks and the absolute error for regression tasks. Our evaluations are conducted under two separate settings. First is a single model setup, where TDA techniques use model checkpoints from a single training run. Unless specified otherwise, RepSim, Trak, and IF are computed at the final training checkpoint, and TracIn and Source use at most \(6\) intermediate checkpoints saved throughout training. In the second setting, TDA techniques use checkpoints from \(10\) distinct models, each trained with varying sources of randomness. Past works have shown that ensembling attribution scores across models can improve TDA performance [70; 67]. For all TDA techniques, including Source, we simply average the final attribution scores from distinctly trained models with the full dataset, except for Trak, which uses its custom ensembling procedures with models each trained on sampled \(50\%\) of the original dataset.

Our experiments consider diverse machine learning tasks, including: (a) regression using datasets from the UCI Repository , (b) image classification with datasets such as MNIST , FashionMNIST , CIFAR-10 , RotatedMNIST , and PACS , and (c) text classification using the GLUE benchmark . Our tasks can be categorized into three groups:

**Concrete, FashionMNIST, CIFAR-10, & RTE.** Models fully trained using a fixed dataset \(\), where implicit-differentiation-based methods are expected to perform similarly to unrolling-based methods. We use \(6\) intermediate checkpoints throughout training for TracIn and Source. Source use \(3\) segments (\(L=3\)) equally partitioned at the early, middle, and late stages of training to account for the changes in distributions of Hessian and gradients during training.

**Concrete-N & FashionMNIST-N.** Non-converged models trained with a smaller number of update steps. This is a challenging setup for implicit-differentiation-based methods, such as Trak and IF, as they inherently assume that TDA is performed on the optimal solution. We use versions of the Concrete and FashionMNIST datasets that have been modified, either by corrupting target values or relabeling \(30\%\) of the data points. Then, we train the models for only \(3\) epochs to avoid overfitting. We use \(3\) checkpoints (at the end of each epoch) for TracIn and Source (\(L=3\)).

**RotatedMNIST & PACS.** Models initially trained with a dataset \(_{1}\), and subsequently trained with another dataset \(_{2}\) (a common setup in continual learning). We use test examples from \(_{2}\) for query data points and attribute the final model's behavior to the first dataset. Since implicit-differentiation-based methods do not provide any way to separate multiple stages of training, for Trak and IF, we simply combine the data from both stages into a larger dataset for TDA. We use two segments for Source, partitioned at different stages, and perform TDA only for the first segment. Our experiments use the RotatedMNIST and PACS datasets, both containing multiple data distributions. We select one of these domains for the second training stage, while the remaining ones are used in the first stage.

The detailed description of the experimental setup is provided in Appendix E. Additional results, including comparisons on additional tasks and with additional baselines, further analysis on linear models, and visualizations of the top influential images obtained by each TDA technique, are shown in Appendix F.

### TDA Evaluations with Linear Datamodeling Score (LDS)

We evaluate TDA techniques using the linear datamodeling score (LDS) from Park et al. . To compute LDS, we first generate \(M\) random subsets \(\{_{j}\}_{j=1}^{M}\) from the training dataset, each containing \( N\) data points for some \((0,1)\). Given a query data point \(_{q}\) and hyperparameters \(\) used to train the original model, the expected measurable quantity for each data subset \(_{}[f(_{q},^{s}(_{j},,))]\) is estimated by retraining the model \(R\) times under different random choices (which requires \(MR\) model retrainings in total). The LDS measures the Spearman correlation  between the estimated quantities and the predictions made by the TDA method. Note that, although a TDA method in Section 2.1 assigns a score to each pair of a query and training data point, the inherently _additive_ nature of most TDA techniques allows for the computation of a group prediction score for the data subset \(\) by summing the individual scores attributed to each data point within this subset. The final LDS is obtained by averaging the scores across many (typically up to \(2000\)) query data points. We use \(100\) data subsets (\(M=100\)) and conduct a minimum of \(5\) retraining iterations (\(R 5\)) for each subset. We refer readers to Appendix A for the detailed formulation and to Appendix E.2 for the practical procedures.

The LDS at \(=0.5\) for Source and baseline TDA techniques are shown in Figure 4. Source consistently outperforms all baseline methods in a single model setup, achieving high LDS. When aggregating TDA scores from multiple models, we observe a large improvement in the LDS, particu

Figure 4: LDS at \(=0.5\) for Source and baseline techniques on regression, image classification, and text classification tasks. The error bars represent \(95\%\) bootstrap confidence intervals (Appendix E.2).

larly for Trak, IF, and Source. Our method achieves the highest LDS across all tasks, except for the CIFAR-10 classification task using ResNet-9. Source especially performs strongly against other baseline techniques on settings that pose challenges to implicit-differentiation-based approaches (_e_.\(g\)., non-converged models and models trained with multiple stages), and indeed, even the non-ensembled version of Source typically outperforms the ensembled versions of the competing methods.

### TDA Evaluations with Subset Removal

Subset removal counterfactual evaluation examines the change in model behavior before and after removing data points highly ranked by a TDA technique. For classification tasks, we consider \(100\) test data points that are correctly classified when trained with the full dataset (across all \(5\) random seeds) and, for each test data point \(_{q}\), examine if removing and retraining without the top-\(k\)_positively_ influential data points can cause misclassification on average (over \(3\) random seeds). By assessing the impact of removing influential training examples on the model's performance, counterfactual evaluation provides a direct measure of the effectiveness of TDA techniques in identifying data points that significantly contribute to the model's behavior. The detailed procedures are described in Appendix E.3. In Figure 5, we show the fraction of test examples (out of the selected \(100\) test points) that get misclassified on average after removing at most \(k\) positively influential training examples identified by each TDA method. We observe that Source better identifies the top influential data points causing misclassification than other baseline TDA techniques. The improvement is more substantial for settings that pose challenges to implicit-differentiation-based methods.

## 6 Conclusion

We introduced Source (**S**egmented **s**ati**O**nary **U**n**R**olling for **C**ounterfactual **E**stimation), a novel TDA technique that combines the strengths of implicit-differentiation-based and unrolling-based techniques. Source approximates unrolled differentiation by partitioning the training trajectory into one or more segments and approximating the gradients and Hessians as stationary within each segment, yielding an influence-function-like estimation algorithm. We showed one instantiation of Source by approximating the Hessian with the EK-FAC parameterization. On a diverse task set, we demonstrated Source's effectiveness compared to existing data attribution techniques, especially when the network has not converged or has been trained with multiple stages.