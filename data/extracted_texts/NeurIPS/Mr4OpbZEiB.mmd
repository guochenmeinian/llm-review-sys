# Task-Robust Pre-Training for Worst-Case Downstream Adaptation

Jianghui Wang, Yang Chen1, Xingyu Xie, Cong Fang, Zhouchen Lin

School of Intelligence Science and Technology, Peking University

jianghuiwang.ai@gmail.com, {yangchen, xyxie, fangcong, zlin}@pku.edu.cn

Equal contribution.Corresponding author.

###### Abstract

Pre-training has achieved remarkable success when transferred to downstream tasks. In machine learning, we care about not only the good performance of a model but also its behavior under reasonable shifts of condition. The same philosophy holds when pre-training a foundation model. However, the foundation model may not uniformly behave well for a series of related downstream tasks. This happens, for example, when conducting mask recovery regression where the recovery ability or the training instances diverge like pattern features are extracted dominantly on pre-training, but semantic features are also required on a downstream task. This paper considers pre-training a model that guarantees a uniformly good performance over the downstream tasks. We call this goal as _downstream-task robustness_. Our method first separates the upstream task into several representative ones and applies a simple minimax loss for pre-training. We then design an efficient algorithm to solve the minimax loss and prove its convergence in the convex setting. In the experiments, we show both on large-scale natural language processing and computer vision datasets our method increases the metrics on worse-case downstream tasks. Additionally, some theoretical explanations for why our loss is beneficial are provided. Specifically, we show fewer samples are inherently required for the most challenging downstream task in some cases.

## 1 Introduction

The rapid development of machine learning is promoting a shift in the learning paradigm, where one first trains a very large model, often called a foundation model, with massive data, and then adapts it to desired tasks using much less data. The hope is to obtain a model that serves as an infrastructure and is transferable to a wide range of tasks. Pre-training plays the role of an engine to acquire the foundation model. Typical pre-training methods are to minimize the average expected risks of the upstream tasks. Such pre-trained models can achieve good performance for a lot of downstream tasks but may fail in some hard cases. For example, a vision pre-trained model for animal and plant recognition may work well for typical characteristics species but fail when identifying mimicry animals and plants; a common green mantis can be correctly recognized as a mantis, while an orchid mantis might be falsely classified as an orchid.

In machine learning, one cares about not only the good performance of a model but also its behavior under reasonable shifts of conditions. The same philosophy holds for pre-training a foundation model. To guarantee a uniformly good performance over a series of tasks, one has to consider the robustness of pre-training. We call this _downstream-task robustness_. The aim is to develop a pre-training method to train the foundation model that admits a good adaptation performance over a series of downstreamtasks. It is crucial to achieving the downstream-task robustness for pre-training: (i) safety is critical for some applications, such as deep learning systems in medicine and finance; (ii) our goal of the foundation model requires reliably good performance on all downstream tasks.

In recent times, popular large-scale models like ChatGPT  have also faced safety issues, sparking discussions among various stakeholders . The concerns largely stem from the potential misuse or unintended behavior of the model in real-world applications. Some argue that the vast and diverse knowledge base of these models, coupled with their ability to generate human-like text, could be leveraged for malicious purposes. Others worry about the potential of the model to generate inappropriate or harmful content .

Addressing these safety concerns is crucial, particularly in the context of pre-training foundation models that are intended for downstream tasks. An initial way to mitigate these safety issues is to consider downstream-task robustness. By focusing on downstream-task robustness, we expect that the models are resilient to perturbations in the input data, thereby reducing their susceptibility to adversarial attacks or misuse. This approach can help in maintaining consistent performance across a range of tasks and so enhance the safety and reliability of the model.

In recent years, the concept of Distributionally Robust Optimization (DRO) [6; 44; 36] has attracted wide attention among theorists and practitioners. Most DRO frameworks [55; 23; 22] consider training a parameterized model that minimizes the worst-case expectation loss over the data from a family of probability distributions. Downstream-task robustness can be considered a generalization of DRO. The destination of downstream-task robustness is to guarantee good worst-case performance for a series of downstream adaptations.

This paper proposes a pre-training method as a starting point for downstream-task robustness. To take a step forward, our method considers learning several upstream tasks. The choice of how to design the upstream tasks allows us to incorporate prior knowledge of the domain and problems. For example, in language models, we can design upstream tasks by masking different types of words; how we generate such upstream tasks by grouping samples reflects our prior knowledge of the natural language. Then instead of minimizing the average expected risk of the upstream tasks, we minimize the worst-case expected risk. We also introduce a simple but practical algorithm called softmax weighted gradient descent to pre-train the model. We prove the algorithm's convergence in the ideal convex setting and show its effectiveness in our empirical study.

We consider the application of the framework in two experiments -- Part-of-Speech masked language models in section 4.1 and multi-modal masked image models in section 4.2. We first pre-train a foundation model with the proposed task-robust pre-training method on multiple upstream tasks generated by different masks and adapt the foundation model for downstream tasks. Compared with the average expected risk minimization, our method achieves better worst-case performance and comparable average performance on all downstream tasks.

We also explain why our framework can benefit downstream-task robustness. Specifically, by simplifying the model-task relationship, we show fewer samples are needed for the hardest downstream task. The key intuition is that proper worse-case training for upstream tasks leads to an initiation close to the solution for the worst-case downstream tasks, thus reducing the downstream burden.

The contributions of our study can be primarily encapsulated within three key areas: (i) The introduction of the concept of task-robust pretraining, a novel theoretical framework that holds significant potential for future research. (ii) The provision of a simple yet efficacious method for task-robust pretraining, accompanied by a comprehensive exposition of its theoretical feasibility. (iii) A series of empirical validations across multiple domains, substantiating the effectiveness of our methodology.

## 2 Setup and Methodology

Consider a traditional machine learning task where the data \(z\) follows an underlying distribution \(P\). Given a model, a parameter space \(^{d}\), a loss function \(:_{+}\), the goal is to find the optimal parameter \(^{*}\) for the model such that \(^{*}=*{arg\,min}_{}_{z P} [(,z)]\). The classic empirical risk minimization (ERM) tackles the problem by first collecting i.i.d. training data from \(P\) and then finding a parameter \(_{}\) via minimizing the empirical risk:

\[_{}:=*{arg\,min}_{} _{i=1}^{N}(,z_{i}).\] (1)

In statistical learning theory, it is well-known that under mild conditions (such as the VC dimension of the model is bounded above), \(_{}\) is a good approximation of \(^{*}\) in the sense that with high probability at least \(1-\) ( \(0< 1\)),

\[_{z P}[(_{},z)] -_{}_{z P}[(,z) ],\] (2)

when the number of training samples \(N\) is sufficiently large. We simply denote the training data requirement by \(N(,)\).

In machine learning, a foundation model is trained and then adapted for each downstream task. The adaptation process involves initializing a downstream model with the pre-trained foundation model's parameters and then training on the downstream task. Denote the parameter of the foundation model by \(_{}\). Let \(\) be the downstream task space. The goal is to find an initial parameter \(_{}\) that enables fast adaptations for each downstream task \(\). Different downstream tasks are characterized by different loss functions with a shared data distribution 3. We use the foundation model's parameters \(_{}\) as initialization to find an approximately optimal parameter \(_{,}\) by ERM. We study the sample complexity required to guarantee a good approximate solution with high probability while considering the effect of initialization. For the task \(\) and the model initialized by \(_{}\), denote the number of samples required to find an \(\)-approximately optimal parameter by ERM with probability at least \(1-\) as \(N_{}\) (\(_{},,\)). The aim is to find the optimal initialization that minimizes the worst-case sample complexity required to find an approximately optimal parameter for all tasks, i.e.,

\[_{}^{*}:=*{arg\,min}_{_{}}_{}N_{}(_{},,).\] (3)

Directly training for the optimal worst-case initialization is generally infeasible or computationally expensive. Pre-training provides a feasible alternative \(_{}^{*}\) for \(_{}^{*}\) by training for available surrogate upstream tasks. When the upstream tasks are related to the downstream tasks, the pre-trained parameter can lead to lower initial expected risks and accelerate downstream training. For example, if we pre-train a model on generated upstream tasks of reconstructing images corrupted by different masks, the pre-trained model can learn some prior knowledge for general vision tasks; with the pre-trained parameter as the initialization, we can accelerate the training process of downstream vision tasks such as image classification and object detection . Consider there are \(T\) representative upstream tasks. Denote the loss function of the task \(t\) as \(_{t}\). A typical choice of the pre-trained parameter is the minimizer of the average expected risk over the \(T\) upstream tasks , i.e.,

\[_{}^{*}:=*{arg\,min}_{} {T}_{t=1}^{T}_{z P}[_{t}(,z) ].\] (4)

However, minimizing the average expected risk over upstream tasks may neglect extreme cases and lead to limited benefit for some downstream tasks.

To alleviate the aforementioned issue of the average expected risk minimization, we propose to use the minimizer of the worst-case expected risks over the upstream tasks, i.e.,

\[_{}^{*}:=*{arg\,min}_{}_{t [T]}_{z P}[_{t}(,z)],\] (5)

as the initial parameter, where \([m]\) denotes the set \(\{1,,m\}\). We show that \(_{}^{*}\) is a better choice than \(_{}^{*}\) in terms of downstream-task robustness.

Algorithm

Recall that our minimax pre-training method is to minimize the worst-case expected risks over the upstream tasks, i.e.,

\[_{}_{t[T]}_{z P}[_{t}( ,z)]\] (6)

There is extensive literature on minimax optimization. The minimax optimization algorithms can be generally classified into two types: (i) minimization for the maximum function \(_{t[T]}_{z P}[_{t}(,z)]\)[12; 4; 60; 28], and (ii) direct minimax optimization for the objective [35; 59; 45; 37; 38]. We introduce a new simple optimization algorithm called softmax weighted gradient descent (Algorithm 1) that we find is very practical to pre-train the model. The algorithm can be roughly seen as the first type. It is an adaptation of the classic subgradient descent for minimizing the maximum function to enable its practical use in pre-training. Concretely, in one update, we take a descent step at the current point \(\) along the direction of the gradient weighted by softmax-type weights, i.e., \(_{t=1}^{T}w_{,t}()_{}_{z P }[_{t}(,z)]\), where

\[w_{,t}():=_{z P} [_{t}(,z)])}{_{t^{}=1}^{T} (_{z P}[_{t^{}}(,z) ])},\] (7)

and \(>0\) is a hyperparameter. (In practice, we use estimations for the expected risks and the gradients on minibatch samples.)

The motivation behind softmax weighted gradient descent is to use the softmax weighted gradient to approximate the subgradient in the classic subgradient descent for the minimax optimization of (6). As the softmax weighted gradient descent algorithm optimizes for the minimax loss directly, it can achieve better worst-case loss than other pre-training methods. One advantage of the softmax approximation is that it avoids the non-differentiability caused by the maximum operator via the softmax approximation, making the algorithm easily implementable for pre-training applications. Also, as the softmax weighted gradient descent includes only single weighted gradient step in each update, it has computational efficiency comparable to gradient descent in deep learning. In contrast, standard minimax algorithms often cost several times gradient oracles in single step. Moreover, our algorithm can be directly combined with commonly-used optimization tricks in deep learning, such as momentum and adaptive learning rates. In our experiments, we observe that the algorithm with a simple implementation achieves better worst-case errors in various real-world tasks than a number of benchmark pre-training algorithms.

``` Input: Step sizes \(\{_{k}\}_{k=1}^{K-1}\), softmax hyperparameters \(\{_{k}\}_{k=0}^{K-1}\) and an initial parameter \(_{0}\); for\(k=1,,K-1\)do  Compute the softmax weights \(\{w_{_{k},t}(_{k-1})\}_{t=1}^{T}\) as in (7);  Update the parameter as \(_{k}_{k-1}-_{k}_{t=1}^{T}w_{_{k},t}( _{k-1})_{}_{z P}[_{t}( _{k-1},z)]\); endfor ```

**Algorithm 1** Softmax Weighted Gradient Descent

For completeness, we also provide some convergence analysis for our proposed algorithm. We consider a relatively basic setting where for all \(t[T]\), the loss function \((,z)\) is convex and \(L^{}\)-Lipschitz continuous for any fixed \(z\) Intuitively, when the hyperparameter \(_{k}\) is sufficiently large, the function \(_{t=1}^{T}w_{_{k},t}(_{k})_{z P} [_{t}(,z)]\) is a good differentiable approximation for the objective \(_{z P}[_{t}(,z)]\) Softmax weighted gradient descent can be roughly seen as a remedy for non-differentiability in subgradient descent, at the expense of controllable approximation errors. We show in Theorem 3.1 that Algorithm 1 can achieve a convergence rate \(O(})\) if the hyperparameter \(_{k}\) is as large as \(()\). This result is comparable to the standard convergence rate \(O(})\) of subgradient descent [10, Chapter 3].

**Theorem 3.1**.: _Suppose that for all \(t[T]\) the loss function \(_{t}(,z)\) is convex, \(L^{}\)-Lipschitz continuous and bounded by \(B\) for all \(\) and any fixed \(z\), Denote the optimal solution of (6) as \(^{*}\) and the distance \(\|_{0}-^{*}\|\) as \(R_{0}\). If the step size \(_{k}==}{L^{}}\) and the softmax hyperparameter \(_{k}}{R_{0}L^{}}}{R_{0 }L^{}}\) for all \(k=0,,K-1\), the average \(_{K}\) of the iteration points in _Algorithm 1, i.e., \(_{K}=_{k=0}^{K-1}_{k}\), satisfies_

\[_{t[T]}_{z P}[_{1}]-_{} _{t[T]}_{z P}[_{2}]L^{ }}{},\] (8)

_where \(_{1}=_{t}(_{K},z)\) and \(_{2}=_{t}(,z)\)._

_Remark 3.2_.: The convexity assumption on the loss functions is an oversimplification in deep learning. However, for some cases such as the neural tangent kernel , the deep neural networks exhibit properties similar to convexity. In our experiments with non-convex models, we also observe that the algorithm behaves well.

_Remark 3.3_.: The above analysis requires increasing softmax hyperparameters \(\{_{k}\}_{k=0}^{K-1}\), i.e., \(_{k}=()\) to guarantee the convergence rate. In our experiments, however, we find that constant softmax hyperparameters, or more concretely \(_{k}=1\) for all \(k=0,,K-1\), work well for most problems. We attribute these phenomena to some properties of deep neural networks, which are left for future exploration.

## 4 Experiments

In this section, we subject our methods to rigorous testing through two experiments, each encompassing tasks germane to the fields of Natural Language Processing (NLP) and Computer Vision (CV). A minimalist design approach was adopted for both the models and the tasks to demonstrate the universality of our design across a broad spectrum of model tasks. We extended the functionalities of BERT and MAE, thereby constructing Part-of-Speech Mask BERT (PoS-BERT) and Multi-Modal Mask MAE (MM-MAE), respectively.

### NLP Scenario: Part-of-Speech Mask BERT

#### 4.1.1 Model and Settings

ArchitecturesAn overview of the Part-of-Speech Mask BERT model is shown in Figure 1. PoSBERT model first samples the datasets from the task space and groups them according to different Part-of-Speech types. The loss function term is calculated separately for each data group entering the BERT encoder. The loss term with the highest weight is selected to enter the optimizer through a minimax layer. Finally, we run experiments on downstream tasks to compare our minimax task

Figure 1: **Part-of-Speech Mask BERT. We first sample the datasets from the task space and group them according to different Part-of-Speech types, and then recover the predicted sentence by a BERT encoder. The optimizer selects the most challenging task and then updates the model’s weight through a minimax layer.**

balancing learning algorithm with other methods to verify our theory. We follow the common practice to design the feature representations for masked language modeling and next-sentence prediction.

Tasks and DatasetsDuring pre-training, Part-of-Speech Mask BERT has two objectives: masked language modeling and next-sentence prediction. We define 9 task categories that recover different parts of speech-type words on masked language modeling: (1)verb, (2) noun, (3) adjective, (4) determiner, (5) adverb, (6) pronoun, (7) preposition, (8) conjunction, (9) interjection. Following previous work , we evaluate our pre-trained models on downstream tasks using the GLUE  benchmarks. Downstream tasks we fine-tuned include MNLI, QQP, QNLI, SST-2, CoLA, STS-B, MRPC, and RTE. By swapping out the appropriate inputs and outputs, Part-of-Speech Mask BERT can model many downstream tasks and has a unified way to handle the tasks that involve single text and text pairs. After setting the masks, we utilize Natural Language Toolkitannotate (NLTK)  to pseudo-label the words with POS annotations.

#### 4.1.2 Quantitative Result

Quantitative results are presented in Table 1. Our model obtains comparable results on GLUE tasks. PoS-BERT with minimax task-balancing outperforms on half tasks by a substantial margin and obtains a 1.8% average score improvement over BERT. As for the most challenging training task, CoLA, which has the lowest accuracy on BERT, our method gets a 9.2% improvement which is a significant boost among downstream tasks. Benefiting from task-robust grouping, on QQP and RTE tasks, our method outperforms the original BERT by 5.7 F1-score and 4.3% accuracy, respectively. Our method also shows superiority on the challenging downstream task, MNLI, by achieving a 1.1% higher matched accuracy. As compensation for working better on the more challenging tasks, our method loses little correctness on some downstream tasks that already transfer well. On QNLI, SST-2, STS-B, and MRPC, our results are lower than that of the original BERT model by a margin of 1% accuracy, 1.9% accuracy, 1.6 spearson correlation, and 0.7 F1-score. The empirical result shows that, with our task-robust pre-training strategy, the downstream tasks perform on the whole, especially those tricky tasks. We expect future work to further improve these results by incorporating more sophisticated multi-task and grouping procedures.

  
**Model** & **Task-Balancing** & **MNLI** & **QQP** & **QNLI** & **SST-2** & **CoLA** & **STS-B** & **MRPC** & **RTE** & **Avg.** \\  BERT & - & 84.6 & 71.2 & **90.5** & **93.5** & 52.2 & **85.8** & **88.9** & 66.4 & 79.6 \\  PoS-BERT & None & 84.9 & 72.6 & 89.1 & 90.8 & 54.4 & 83.6 & 88.1 & 68.2 & 79.3 \\  & Minimax (Ours) & **85.6** & **76.9** & 88.6 & 91.3 & **61.4** & 84.2 & 88.2 & **70.7** & **81.4 (+1.8)** \\   

Table 1: Results on GLUE. The “Averages” are obtained from GLUE leaderboard. F1 scores are reported for QQP and MRPC., spearman correlations are reported for STS-B, Matthews correlations are reported for CoLA, and accuracy scores are reported for the other tasks.

Figure 2: **Multi-Modal Mask MAE:** Randomly sampled patches from multiple modalities are projected to tokens. Task-specific decoders reconstruct the masked-out patches by first performing a cross-attention step from queries to the encoded tokens.

### CV Scenario: Multi-Modal Mask MAE

#### 4.2.1 Model and Settings

ArchitecturesAn overview of MM-MAE is shown in Figure 4.1.2. Multi-Modal Mask MAE contains three encoders, each of which processes different modalities of one image. During pre-training, we try to recover each modality from its masked tokens. Each modality is divided into 16\(\)16 patches and then tokenize the patches with modality-dependent linear projections. Projected patches are concatenated into a sequence of tokens and given as input to the same transformer encoder. We also add a global token with 2D sine-cosine positional embeddings. Each task owns a specialized decoder, and the computational cost of decoders scales linearly with the number of tasks.

Tasks and DatasetsWe select two datasets with different scales, ImageNet1K  and ImageNet50 , to conduct unsupervised training upstream to see whether the minimax pre-training method can help the downstream tasks with poor performance. The classification task is evaluated on the validation part of the original dataset, while the semantic segmentation and depth estimation tasks are validated on the NYUv2 dataset  by fine-tuning. Due to the absence of a sizeable multi-task dataset with aligned task images  we generate pseudo-labels on ImageNet and ImageNetS50 with GPT-3 and Mask2Former.

#### 4.2.2 Quantitative Result

Classification tasksThe quantitative results are presented in Table 2. We evaluate our models and baseline by fine-tuning them on the supervised ImageNetS50 and ImageNet1K. We fine-tune our models for 100 epochs and report the top-1 validation accuracy. The result shows a tiny gap between our method and the average method in the classification task. Classification tasks are regarded as the least challenging task category of the three. Cause different downstream tasks have different optimal parameter requirements, this gap is unavoidable. After the pre-training of the model reaches a specific step, the training weight of the classification tasks will continue to decrease.

Semantic segmentation tasksWe further evaluate our models on semantic segmentation tasks on the NYUv2 dataset. We report the mean intersection over the union (mIoU) metric. Notice that semantic segmentation is the most challenging task of these downstream transfers. Our method

    &  \\  Data & Epoch & Task-Balancing & Cls. (Top-1 Acc. \%) & Semseg. (mIoU) & Depth. (\(_{1}\) Acc. \%) \\   &  & None & 92.2 & 51.9 & 52.1 \\  & & Uncertainty  & 92.6 & 54.5 & 70.2 \\  & & GradNorm  & 93.0 & 56.5 & 65.8 \\  & & DWA  & **93.4** & 52.7 & 65.7 \\  & & Minimax(Ours) & 91.8 & **61.5** & **74.1** \\   &  & Uncertainty & **82.6** & 48.9 & 85.2 \\  & & Minimax(Ours) & 82.3 & **50.1** & **85.3** \\    & & Uncertainty & **83.3** & 52.0 & 86.4 \\    & & Minimax(Ours) & 83.0 & **53.2** & **86.8** \\   

Table 2: Comparison between task-robust method and other task-balancing methods on ImageNet1K and ImageNetS50 pre-training. Our methodology demonstrates superior performance across the majority of downstream tasks, particularly excelling in the most challenging among them.

    & **Balance** & **Balance** & **Grads** & **No Extra** & **FLOPs** & **Motivation** \\  & **Magnitude** & **Learning** & **Required** & **Tuning** & & \\  None & ✓ & & & ✓ & \(T\) & / \\ Uncertainty & ✓ & & & ✓ & \(2T\) & Homoscedastic uncertainty \\ Gradnorm & ✓ & ✓ & ✓ & ✓ & \(4T\) & Balance learning and magnitudes \\ DWA & & ✓ & & & \(3T\) & Balance learning \\ Minimax (Ours) & ✓ & & & ✓ & \(2T\) & Task robust \\   

Table 3: A qualitative comparison between task balancing techniques. \(T\) representing the computation cost when no additional task-balancing techniques are employed.

benefits more than the average loss model from pseudo-labeled modalities as input. In particular, the correctness is improved by 9.6% on ImageNetS50 pre-training. With the progress of model training, our task-robust loss forces the model to improve poorly trained semantic segmentation tasks by increasing the training weight. The following section 5 will explain why a simple strategy can significantly help worst-case downstream tasks.

Depth evaluate tasksFor depth estimation, we use NYUv2. We report \(_{1}\) on the NUYv2 test set, showing the percentage of pixels \(p\) with error max \(\{}{y_{p}},}{y_{p}}\}\) less than 1.25 . According to Table 2, the accuracy is improved by 3.9% on ImageNetS50 pre-training with the help of the downstream-task robustness loss function. The depth estimation task in the same data volume has a higher tolerance for prediction errors per pixel than semantic segmentation. However, it is still more complicated than the classification task, which only predicts the image once. After the classification task is well-trained, the depth estimation task will benefit from our strategy in the subsequent training.

### Qualitative Comparison

Table 2 delineates several strategies designed to equilibrate the contribution of each task during the training of a multi-task network. For a qualitative comparison of these methods, refer to Table 3. We appraise these strategies based on several criteria . An overview of our examination suggests that our approach achieves a synergistic blend of simplicity, efficiency, and effectiveness.

To facilitate a more intuitive comparison of the differences in results throughout the training process, we undertook downstream tasks in semantic segmentation, comparing the performance of various approaches midway through pre-training (400 epoch, ImageNetS50). As depicted in Figure 3, our methodology exhibits superior performance, even under conditions of insufficient training.

## 5 Explanation

We show why the proposed minimax pre-training method can be more effective than the average expected risk minimization in some cases. We consider a simplification of the model and the task relationship. Such a simplification makes our analysis convenient and intuitive. We assume that for all \(t[T]\), the function \(_{t}(,z)\) is \(\)-strongly-convex, \(L\)-smooth, and \(L^{}\)-Lipschitz continuous for any fixed \(z\) and the function \(_{t}(,) B\) for all \(\) and \(z^{4}\). Note that pre-training on

Figure 3: Comparative intermediate results. The first and final columns represent the image input and ground truth, respectively, while the intermediary images depict the intermediate results yielded by various task-balancing methodologies. Distinct colors correspond to the prediction of different objects. Our approach ensures robustness in downstream tasks.

irrelevant upstream tasks does little help to the downstream tasks in general. Here, we only discuss the case where the upstream tasks and the downstream tasks are closed related. We ideally assume that the loss functions of the downstream tasks are convex combinations of the loss functions of the upstream tasks, i.e.,

\[_{}=_{t=1}^{T}_{t}_{t},=_{T},\] (9)

where \(_{T}\) is the \((T-1)\)-dimensional probability simplex. We further assume that for each task there exists a parameter such that the expected risk of the task is zero, i.e., \(_{}_{z P}[_{t}(,z) ]=0\) for all \(t[T]\).

We first show that in the above setting, the proposed minimax optimization pre-training method can guarantee a better worst-case initial expected risk than the minimization method.

**Proposition 5.1**.: _Let \(^{*}_{}\) and \(^{*}_{}\) be the pre-trained parameters obtained by minimizing the maximal expected risk and the average expected risk, respectively. Then for the worst-case expected risks of the downstream tasks, we have_

\[_{}_{z P}[_{}(^{*}_{ },z)]\!\!_{}_{z P} _{}^{*}_{},z.\] (10)

_Remark 5.2_.: The gap between \(_{}_{z P}[_{}(^{ *}_{},z)]\) and \(_{}_{z P}[_{}(^ {*}_{},z)]\) can be large. We provide an example in the appendix, where the ratio between \(_{}_{z P}[_{}(^ {*}_{},z)]\) and \(_{}_{z P}[_{}(^ {*}_{},z)]\) is as large as \(O(T)\).

We then illustrate that a good initialization can serve as an implicit regularization. We suppose that the downstream tasks are trained with gradient descent. (For stochastic gradients with bounded variances, the analysis below also holds for sufficiently small step sizes, within neglectable approximation errors.) For the downstream task \(\), with certain step sizes, the parameters will always be in a subset

\[_{}(_{0})\!=\!\{\!\!\ |\ \| \!-\!^{*}_{}\|^{2}\!\!_{z  P}[_{}(_{0},z)]\!\},\] (11)

where \(_{0}\) is the initial parameter and \(^{*}_{}=_{}_{z P}[ _{}(,z)]\).

**Proposition 5.3**.: _Suppose that a function \(f:^{d}\) is \(_{f}\)-strongly-convex and \(L_{f}\)-smooth for all \(x^{d}\) and \(x^{*}_{x^{d}}f(x)\). Let \(\{x_{k}\}_{k=0}^{K-1}\) be the sequence generated by gradient descent with a step size \(>0\), i.e., \(x_{k}=x_{k-1}- f(x_{k-1})\) for all \(k[K-1]\). If the step size \(}\), then we have_

\[\|x_{k}-x^{*}\|^{2}}(f(x_{0})-f(x^{*})),\] (12)

_for all \(k=0,1,,K-1\)._

By Proposition 5.3, we can deem that the downstream task's parameter space is the subset \(_{}(_{0})\).

Consider the worst sample complexity to find an \(\)-approximately optimal parameter by ERM within the parameter space \(_{}(_{0})\) for a downstream task \(\).

**Theorem 5.4**.: _The worst-case sample complexity \(_{}N_{}(_{0},,)\) with initialization \(_{0}\) satisfies_

\[_{}N_{}(_{0},,) }{^{2}}(1+\ }{}_{} _{z P}[_{}(_{0},z)]} )+}{^{2}}.\] (13)

Theorem 5.4 characterizes the upper bound of the worst-case sample complexity of downstream tasks. If we regard \(\) and \(\) as constants, the worst-case sample complexity with respect to the initialization \(_{0}\) is \(O(_{}_{z P}[_{} (_{0},z)])\). Combined with (10), Theorem 5.4 demonstrates that the proposed minimax pre-training procedure implies tighter sample complexity than the average minimization pre-training procedure in the worst case. Though the dependency on the worst-case initial expected risk is logarithmic in the upper bound analysis, we find that the initialization can have an evident effect on the generalization of the downstream tasks in practice. We claim that the upper bound for general cases may not be tight for our deep learning applications. Special structures in applications might lead to tighter bounds for generalization errors, which remains for further study.

Conclusion

This paper introduces the concept of downstream-task robustness for pre-training, aiming to improve the performance of foundation models across various downstream tasks. As models such as ChatGPT become more prevalent, safety and consistent performance are increasingly important. Our proposed minimax loss for pre-training, validated through extensive experiments, offers a potential solution to enhance the robustness and safety of such models. In the future, we will explore grouping the upstream tasks adaptively. We would say though still in its early stages, the study of downstream-task robustness holds significant promise for the reliable and safe deployment of AI infrastructure.

## 7 Acknowledgments

C. Fang and Z. Lin were supported by National Key R&D Program of China (2022ZD0160301). C. Fang was also supported by the NSF China (No. 62376008) and Wudao Foundation. Z. Lin was also supported by the NSF China (No. 62276004), the major key project of PCL, China (No. PCL2021A12) an Qualcomm.