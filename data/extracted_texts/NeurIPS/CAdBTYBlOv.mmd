# Improving Linear System Solvers for Hyperparameter Optimisation in Iterative Gaussian Processes

Jihao Andreas Lin\({}^{1,\,2}\) &Shreyas Padhy\({}^{1}\) &Bruno Mlodozenie\({}^{1,\,2}\) &Javier Antoran\({}^{1,\,3}\) &Jose Miguel Hernandez-Lobato\({}^{1,\,3}\)

\({}^{1}\)University of Cambridge \({}^{2}\)MPI for Intelligent Systems, Tubingen \({}^{3}\)Angstrom AI

###### Abstract

Scaling hyperparameter optimisation to very large datasets remains an open problem in the Gaussian process community. This paper focuses on iterative methods, which use linear system solvers, like conjugate gradients, alternating projections or stochastic gradient descent, to construct an estimate of the marginal likelihood gradient. We discuss three key improvements which are applicable across solvers: (i) a pathwise gradient estimator, which reduces the required number of solver iterations and amortises the computational cost of making predictions, (ii) warm starting linear system solvers with the solution from the previous step, which leads to faster solver convergence at the cost of negligible bias, (iii) early stopping linear system solvers after a limited computational budget, which synergises with warm starting, allowing solver progress to accumulate over multiple marginal likelihood steps. These techniques provide speed-ups of up to \(72\) when solving to tolerance, and decrease the average residual norm by up to \(7\) when stopping early.

## 1 Introduction

Gaussian processes  (GPs) are a versatile class of probabilistic machine learning models which are used widely for Bayesian optimisation of black-box functions , climate and earth sciences , and data-efficient learning in robotics and control . However, their effectiveness depends on good estimates of hyperparameters, such as kernel length scales and observation noise. These quantities are typically learned by maximising the marginal likelihood, which balances model complexity with training data fit. In general, the marginal likelihood is a non-convex function of the hyperparameters and evaluating its gradient requires inverting the kernel matrix. Using direct methods, this requires compute and memory resources which are respectively cubic and quadratic in the number of training examples. This is intractable when dealing with large datasets of modern interest.

Methods to improve the scalability of Gaussian processes can roughly be grouped into two categories. Sparse methods  approximate the kernel matrix with a low-rank surrogate, which is cheaper to invert. This reduced flexibility may result in failure to properly fit increasingly large or sufficiently complex data . On the other hand, iterative methods  express GP computations in terms of systems of linear equations. The solution to these linear systems is approximated up to a specified numerical precision with linear system solvers, such as conjugate gradients (CG) , alternating projections (AP) , or stochastic gradient descent (SGD) . These methods allow for a trade-off between compute time and accuracy. However, convergence can be slow in the large data regime, where system conditioning is often poor.

In this paper, we focus on iterative GPs and identify techniques, which were important to the success of previously proposed methods, but did not receive special attention in the literature. Many of these amount to amortisations which leverage previous computations to accelerate subsequent ones. We analyse and adapt these techniques, and show that they can be applied to accelerate different linear solvers, obtaining speed-ups of up to \(72\) without sacrificing predictive performance (see Figure 1).

In the following, we summarise our contributions:

* We introduce a **pathwise estimator** of the marginal likelihood gradient and show that, under real-world conditions, the solutions to the linear systems required by this estimator are closer to the origin than those of the standard estimator, allowing our solvers to converge faster. Additionally, these solutions transform into samples from the GP posterior without further matrix inversions, amortising the computational costs of predictive posterior inference.
* We propose to **warm start** linear system solvers throughout marginal likelihood optimisation by reusing linear system solutions to initialise the solver in the subsequent step. This results in faster convergence. Although this technically introduces bias into the optimisation, we show that, theoretically and empirically, the optimisation quality does not suffer.
* We investigate the behaviour of linear system solvers on a **limited compute budget**, such that reaching the specified tolerance is not guaranteed. Here, warm starting allows the linear system solver to accumulate solver progress across marginal likelihood steps, progressively improving the solution quality of the linear system solver despite early stopping.
* We demonstrate empirically that the methods above either reduce the required number of iterations until convergence without sacrificing performance or improve the performance if a limited compute budget hinders convergence. Across different UCI regression datasets and linear system solvers, we observe average **speed-ups of up to 72\(\)** when solving until the tolerance is reached, and **increased performance** when the compute budget is limited.

Source code available at: https://github.com/jandylin/iterative-gaussian-processes

## 2 Gaussian Process Regression and Marginal Likelihood Optimisation

Formally, a GP is a stochastic process \(f:\), such that, for any finite subset \(\{x_{i}\}_{i=1}^{n}\), the set of random variables \(\{f(x_{i})\}_{i=1}^{n}\) is jointly Gaussian. In particular, \(f\) is uniquely identified by a mean function \(()=[f()]\) and a positive-definite kernel function \(k(,^{};)=(f(),f(^ {}))\) with kernel hyperparameters \(\). We use a \(}{{2}}\) kernel with length scales per dimension and a scalar signal scale and write \(f(,k)\) to express that \(f\) is a GP with mean \(\) and kernel \(k\).

For GP regression, let the training data consist of \(n\) inputs \(\) and targets \(^{n}\). We consider the Bayesian model \(y_{i}=f(x_{i})+_{i}\), where each \(_{i}(0,^{2})\) i.i.d. and \(f(,k)\). We assume \(=0\) without loss of generality. The posterior of this model is \(f|(_{f|},k_{f|})\), with

\[_{f|}() =k(,;)(k(,;)+^{2})^{-1},\] (1) \[k_{f|}(,^{}) =k(,^{};)-k(, ;)(k(,; )+^{2})^{-1}k(,^{ };),\] (2)

where \(k(,;)\), \(k(,;)\) and \(k(,;)\) refer to pairwise evaluations, resulting in a \(1 n\) row vector, a \(n 1\) column vector and a \(n n\) matrix respectively.

Pathwise ConditioningWilson et al. [31; 32] express a GP posterior sample as a random function

\[(f|)()=f()+k(,;)(k(,;)+^{2} )^{-1}(-(f()+)),\] (3)

Figure 1: Comparison of relative runtimes for different methods, linear system solvers, and datasets. The linear system solver (hatched areas) dominates the total training time (coloured patches). The pathwise gradient estimator requires less time than the standard estimator. Initialising at the previous solution (warm start) further reduces the runtime of the linear system solver for both estimators.

where \((,^{2})\) is a random vector, \(f(0,k)\) is a zero-mean prior function sample, and \(f()\) is its evaluation at the training data. Following previous work , we efficiently approximate the prior function sample using random features  (see Appendix B for details). Using pathwise conditioning, a single linear solve suffices to evaluate a posterior function sample at arbitrary locations without further linear solves. In Section 3, we amortise the cost of this single linear solve during marginal likelihood optimisation to obtain posterior samples efficiently.

The Marginal Likelihood and Its GradientWith hyperparameters \(=\{,\}\) and regularised kernel matrix \(_{}=k(,;)+^{2} ^{n n}\), the marginal likelihood \(\) as a function of \(\) and its gradient \(_{_{k}}\) with respect to \(_{k}\) can be expressed as

\[() =-^{}_{}^{-1} {y}-_{}- 2,\] (4) \[_{_{k}}() =(_{}^{-1})^{} _{}}{_{k}}_{}^{-1}-(_{}^{-1} _{}}{_{k}}),\] (5)

where the partial derivative of \(_{}\) with respect to \(_{k}\) is a \(n n\) matrix. We assume \(n\) is too large to compute the inverse or log-determinant of \(_{}\) and iterative methods are used instead.

### Hierarchical View of Marginal Likelihood Optimisation for Iterative Gaussian Processes

Marginal likelihood optimisation for iterative GPs consists of bi-level optimisation, where the outer loop maximises the marginal likelihood (4) using stochastic estimates of its gradient (5). Computing these gradient estimates requires the solution to systems of linear equations. These solutions are obtained using an iterative solver in the inner loop. Figure 2 illustrates this three-level hierarchy.

Outer-Loop OptimiserThe outer-loop optimiser maximises the marginal likelihood \(\) (4) using its gradient (5). Common choices are L-BFGS , when exact gradients are available, and Adam  in the large-data setting, when stochastic approximation is required. We consider the case where gradients are stochastic and use Adam.

Gradient EstimatorThe gradient (5) involves two computationally expensive components: linear solves against the targets \(_{}^{-1}\) and the trace term \((_{}^{-1}_{} /_{k})\). An unbiased estimate of the latter can be obtained using \(s\) probe vectors and Hutchinson's trace estimator ,

\[(_{}^{-1}_{}}{_{k}})=_{}[^{}_{}^{-1}_{}}{ _{k}}]_{j=1}^{s}_{j}^{ }_{}^{-1}_{} }{_{k}}_{j},\] (6)

where the probe vectors \(_{j}^{n}\) satisfy \( j:[_{j}_{j}^{}]=\), and \(_{j}^{}_{}^{-1}\) is obtained using a linear solve. We refer to this as the _standard estimator_ and set \(s=64\), unless otherwise specified.

Linear System SolverSubstituting the trace estimator (6) back into the gradient (5), we obtain an unbiased gradient estimate in terms of the solution to a batch of systems of linear equations,

\[_{}\,[\,_{},_{1},,_{s} \,]=[\,,_{1},,_{s}\,],\] (7)

which share the same coefficient matrix \(_{}\). Since \(_{}\) is positive-definite, the solution \(=_{}^{-1}\) to the system \(_{}\,=\) can be obtained by finding the unique minimiser of the quadratic objective

\[=*{arg\,min}_{}\ ^{}_{ }\,-^{},\] (8)

facilitating the use of iterative solvers. Most popular in the GP literature are conjugate gradients (CG) , alternating projections (AP)  and stochastic gradient descent (SGD) . We consider these in our study and provide detailed descriptions of them in Appendix B. Solvers are often run until the relative residual norm \(\|-_{}\|/\|\|\) reaches a certain tolerance \(\). We set \(=0.01\), following Maddox et al. . The linear system solver in the inner loop dominates the computational costs of marginal likelihood optimisation for iterative GPs, as shown in Figure 1. Therefore, improving linear system solvers is the main focus of our work.

Figure 2: Marginal likelihood optimisation for iterative GPs.

## 3 Pathwise Estimation of Marginal Likelihood Gradients

We introduce the _pathwise estimator_, an alternative to the standard estimator (6) which reduces the required number of linear system solver iterations until convergence (see Figure 3). Additionally, the estimator simultaneously provides us with posterior function samples via pathwise conditioning, hence the name _pathwise_ estimator. This facilitates predictions without further linear solves.

We modify the standard estimator (6) to absorb \(_{}^{-1}\) into the distribution of the probe vectors ,

\[(_{}^{-1}_ {}}{_{k}})=(_{ }^{-}_{}}{ _{k}}_{}^{-})=_{} [}^{}_{}}{ _{k}}}]\ _{j=1}^{s}}_{j}^{}_{}}{_{k}}}_{j},\] (9)

where \( j:[}_{j}}_{j}^{}]= _{}^{-1}\). Probe vectors \(}\) with the desired second moment can be obtained as

\[f() (,k(,;))\] \[ (,^{2})\] (10)

where \(=f()+\). Akin to the standard estimator in Section 2.1, we obtain \(}\) and \(}_{j}\) by solving

\[_{}[},}_{1},,}_ {s}]=[,_{1},,_{s}].\] (11)

Initial Distance to the Linear System SolutionUnder realistic conditions, the pathwise estimator moves the solution of the linear system closer to the origin. To show this, we consider the generic linear system \(_{}=\) and measure the RKHS distance between the initialisation \(_{}\) and the solution \(=_{}^{-1}\) as \(\|_{}-\|_{_{}}^{2}\). With \(_{}=\), which is standard [9; 30; 1; 15; 33; 16],

\[\|_{}-\|_{_{}}^{2}=\|\|_{ _{}}^{2}=^{}_{}=^{}_{}^{-1}_{}_{}^{-1}=^{}_{}^{-1}.\] (12)

Since \(\) is a random vector (\(\) in (7) and \(\) in (11)), we analyse the expected squared distance

\[[^{}_{}^{-1}] =[(^{}_{}^{-1})]=([^{}_{}^{-1}])= ([^{}] _{}^{-1}).\] (13)

For the standard estimator (6), we substitute \(:=\) with \([^{}]=\), yielding

\[[\|_{}-\|_{_{}}^{2} ]=([^{} ]_{}^{-1})=( _{}^{-1})=(_{}^{-1}).\] (14)

For the pathwise estimator (9), we substitute \(:=\) with \([^{}]=_{}\), yielding

\[[\|_{}-\|_{_{}}^{2} ]=([^{} ]_{}^{-1})=( _{}\,_{}^{-1})=( )=n.\] (15)

The initial distance for the standard estimator is equal to the trace of \(_{}^{-1}\), whereas it is constant for the pathwise estimator. Figure 3 illustrates that this trace follows the top eigenvalue, which roughly matches the noise precision. As the model fits the data better, the noise precision increases, increasing the initial distance for the standard but not for the pathwise estimator. In practice, the latter leads to faster solver convergence, especially for problems with high noise precision (see Table 1).

Figure 3: On the pol and elevators datasets, the pathwise estimator results in a lower RKHS distance (12) between solver initialisation and solution, as predicted by theory (14,15) (left). This results in fewer AP iterations until reaching the tolerance (left middle). When using the standard estimator, the initial distance follows the top eigenvalue of \(_{}^{-1}\) (right middle), which is strongly related to the noise precision (right). The latter tends to increase during marginal likelihood optimisation when fitting the data. The effects are greater on pol due to the higher noise precision.

Amortising Linear Solves for Optimisation and PredictionThe name of the _pathwise_ estimator comes from the fact that solving the linear systems (11) provides us with all of the terms we need to construct a set of \(s\) posterior samples via pathwise conditioning (3). Each of these is given by

\[(f|)()=f()+k(,;)\,_{}^{-1}(-)=f()+k(,;)( {v_{y}}-}).\] (16)

We can use these to make predictions without requiring any additional linear system solves.

How Many Probe Vectors and Posterior Samples Do We Need?In the literature , it is common to use \(s 16\) probe vectors for marginal likelihood optimisation. However, a larger number of posterior samples, around \(s=64\), is necessary to make accurate predictions  (see Figure 4). Thus, to amortise linear system solves across marginal likelihood optimisation and prediction, we must use the same number of probes for both. Interestingly, as shown in Figure 4, using 64 instead of 16 probe vectors only increases the runtime by around 10% because the computational costs are dominated by kernel function evaluations, which are shared among probe vectors.

Estimator VarianceThe standard estimator with Gaussian probe vectors and the pathwise estimator have the same variance if \(}}\) and \(}}/_{k}\) commute with each other (see Appendix A.1). There has been work developing trace estimators with lower variance , however, we did not pursue these as we find variance to be sufficiently low, even when relying on only \(s=16\) probe vectors.

Approximate Prior Function Samples Using Random FeaturesIn practice, the pathwise estimator requires samples from the prior \(f(0,k)\), which are intractable for large datasets without the use of random features . In Figure 5, we show that, despite using random features, most of the time the marginal likelihood optimisation trajectory of the pathwise estimator matches the trajectory of exact optimisation using Cholesky factorisation and backpropagation. Further, we confirm that deviations of the pathwise estimator are indeed due to the use of random features by demonstrating that we can remove these deviations using exact samples from the prior instead.

Figure 4: On the pol dataset, increasing the number of posterior samples improves the performance of pathwise conditioning until diminishing returns start to manifest with more than 64 samples (left). Furthermore, with \(4\) as many probe vectors, the total cumulative runtime only increases by around 10% because the computational costs are dominated by shared kernel function evaluations (right).

Figure 5: Across all datasets and marginal likelihood steps, most hyperparameter trajectories of the pathwise estimator rarely differ from exact optimisation, as shown by the histogram illustrating the differences between hyperparameters (left). On selected length scales of the elevators dataset, the pathwise estimator deviates due to the use of random features to approximate prior function samples. With exact samples from the prior, the pathwise estimator matches exact optimisation again (right).

## 4 Warm Starting Linear System Solvers

Linear system solvers are typically initialised at zero [9; 30; 1; 15; 33; 16].1 However, because the outer-loop marginal likelihood optimisation does not change the hyperparameters much between consecutive steps, we expect that the solution to inner-loop linear systems also does not change much between consecutive steps (see Appendix A.2 for a more formal argument). Therefore, we suggest to _warm start_ linear system solvers by initialising them at the solution of the previous . This requires that the targets of the linear systems, \(_{j}\) or \(_{j}\), are not resampled throughout optimisation, which can introduce bias . However, we find that warm starting consistently provides gains across all linear system solvers for both the standard and the pathwise estimator, and that the bias is negligible.

Visualising Warm StartsFigure 6 visualises the two top eigendirections of the inner-loop quadratic objective on pol. Throughout training, warm starting solvers at the solution to the previous linear system results in a substantially smaller initial distance to the current solution.

Effects on Linear System Solver ConvergenceReducing the initial RKHS distance to the solution reduces the required number of solver iterations until the tolerance \(=0.01\) is reached for all solvers and all five datasets, as shown in Figure 7, Table 1 and Appendix C. However, the effectiveness depends on the solver type. CG is more sensitive to the direction of descent rather than the distance to the solution because it uses line searches to take big steps. It only obtains a \(2.1\) speed-up on average. AP and SGD benefit more, with average speed-ups of \(18.9\) and \(5.1\), respectively.

Figure 6: Two-dimensional cross-sections along top eigendirections of the inner-loop quadratic objective after 20 marginal likelihood steps on the pol dataset. The current solution is placed at the origin of coordinates (left and middle). Warm starting significantly reduces the initial root-mean-square RKHS distance to the solution throughout marginal likelihood optimisation (right).

Figure 7: Required number of solver iterations to the tolerance \(=0.01\) during marginal likelihood optimisation on the pol and elevators datasets. Warm starting with the previous solution reduces the required number of iterations to reach the tolerance without sacrificing predictive performance.

Does Warm Starting Introduce Bias?A potential concern when warm starting is that the latter introduces bias into the optimisation trajectory because the linear system targets are not resampled throughout optimisation. Although individual gradient estimates are unbiased, estimates are correlated along the optimisation trajectory. In fact, after fixing the targets, gradients become deterministic and it is unclear whether the induced optimum converges to the true optimum.2 Fortunately, one can show that the marginal likelihood at the optimum implied by these gradients will converge in probability to the marginal likelihood of the true optimum.

**Theorem 1**.: _(informal) Under reasonable assumptions, the marginal likelihood \(\) of the hyperparameters obtained by maximising the objective implied by the warm-started gradients \(}^{*}\) will converge in probability to the marginal likelihood of a true maximum \(^{*}\): \((}^{*})(^{*})\) as \(s\)._

See Appendices A.3 and A.4 for details. In practice, a small number of samples seems to be sufficient. In Appendix C, we illustrate that optimisation trajectories of warm-started solvers are almost identical to trajectories obtained by non-warm-started solvers across solver types and datasets.

Warm Starting the Pathwise EstimatorOne advantage of the pathwise estimator from Section 3 is the reduced RKHS distance between the origin and the solution. When warm starting, the inner-loop solver no longer initialises at the origin, and thus, one may be concerned that we lose this advantage. However, empirically, this is not the case. As shown in Table 1, combining both techniques further accelerates AP and SGD, reaching \(72.1\) and \(7.2\) average speed-ups across our datasets relative to the standard estimator without warm starting. Furthermore, since we run solvers until reaching the tolerance, the predictive performance is almost identical among all methods and solvers.

    & path & warm &  &  & Average \\  & wise & start & POL & elev & bike & prot & kegg & pol & elev & bike & prot & kegg & Speed-Up \\  \)} & & & 1.27 & -0.39 & 2.15 & -0.59 & 1.08 & 4.83 & 1.58 & 5.08 & 29.9 & 28.0 & — \\  & ✓ & & 1.27 & -0.39 & 2.07 & -0.62 & 1.08 & 3.96 & 1.49 & 4.41 & 20.0 & 26.4 & **1.2**\(\) \\  & & ✓ & 1.27 & -0.39 & 2.15 & -0.59 & 1.08 & 2.28 & 1.03 & 2.74 & 11.5 & 12.8 & **2.1**\(\) \\  & ✓ & ✓ & 1.27 & -0.39 & 2.06 & -0.62 & 1.08 & 2.47 & 1.00 & 3.07 & 13.7 & 13.0 & **1.9**\(\) \\  \)} & & & 1.27 & -0.39 & 2.15 & -0.59 & — & 493. & 77.8 & 302. & 131. & \(>\) 24 h & — \\  & ✓ & & 1.27 & -0.39 & 2.07 & -0.62 & 1.08 & 27.9 & 1.67 & 19.9 & 16.4 & 211. & \(>\) **5.4**\(\) \\  & & ✓ & 1.27 & -0.39 & 2.15 & -0.59 & 1.08 & 44.0 & 36.4 & 35.1 & 55.8 & 491. & \(>\) **18.9**\(\) \\  & ✓ & ✓ & 1.27 & -0.39 & 2.06 & -0.62 & 1.08 & 3.90 & 1.21 & 5.40 & 12.3 & 14.0 & \(>\) **72.1**\(\) \\  \)} & & & 1.27 & -0.39 & 2.15 & -0.59 & 1.08 & 139. & 5.54 & 412. & 75.2 & 620. & — \\  & ✓ & & 1.27 & -0.39 & 2.07 & -0.63 & 1.08 & 73.6 & 4.58 & 156. & 24.0 & 412. & **2.1**\(\) \\  & & ✓ & 1.27 & -0.39 & 2.15 & -0.59 & 1.08 & 26.5 & 1.22 & 74.3 & 11.2 & 168. & **5.1**\(\) \\  & ✓ & ✓ & 1.27 & -0.39 & 2.06 & -0.62 & 1.07 & 17.9 & 1.14 & 64.2 & 11.9 & 58.7 & **7.2**\(\) \\   

Table 1: Test log-likelihoods, total training times, and average speed-up among datasets for CG, AP, and SGD after 100 outer-loop marginal likelihood steps with learning rate of 0.1. We consider five datasets with \(n<50\)k, which allows us to solve to tolerance, and report the mean over 10 data splits.

Figure 8: Across all marginal likelihood steps and datasets, warm starting results in hyperparameter trajectories which barely differ from exact optimisation, as shown by the histogram (left). On the same selected length scales from Figure 5, warm starting matches exact optimisation (right).

## 5 Solving Linear Systems on a Limited Compute Budget

Our experiments so far have only considered relatively small datasets with \(n<50\)k, such that inner-loop solvers can reach the tolerance in a reasonable amount of time. However, on large datasets, where linear system conditioning may be poor, reaching a low relative residual norm can become computationally infeasible. Instead, linear system solvers are commonly given a limited compute budget. Gardner et al.  limit the number of CG iterations to 20, Wu et al.  use 11 epochs of AP, Antoran et al.  run SGD for 50k iterations, and Lin et al.  run SGD for 100k iterations. While effective for managing computational costs, it is not well understood how early stopping before reaching the tolerance affects different solvers and marginal likelihood optimisation. Furthermore, it is unclear whether a certain tolerance is required to obtain good downstream predictive performance.

The Effects of Early StoppingWe repeat the experiments from Table 1 but introduce limited compute budgets: 10, 20, 30, 40 or 50 solver epochs, where one epoch refers to computing each value in \(_{}\) once (see Appendix B for details).3 In this setting, solvers terminate upon either reaching the relative residual norm tolerance or when the compute budget is exhausted, whichever occurs first.

In Figure 9, we illustrate the relative residual norms reached for each compute budget on the POL dataset (see Figures 14 to 17 in Appendix C for other datasets). In general, the residual norms increase as \(_{}\) becomes more ill-conditioned during optimisation, and as the compute budget is decreased. The increase in residual norms is much larger for CG than the other solvers, which is consistent with previous reports of CG not being amenable to early stopping . AP seems to behave slightly better than SGD under a limited compute budget. Both the pathwise estimator and warm starting combine well with early stopping, reaching lower residual norms when using a budget of 10 solver epochs than the standard estimator without warm starting using a budget of 50 solver epochs.

In terms of predictive performance, we see that CG with the standard estimator and no warm starting suffers the most from early stopping. Changing to the pathwise estimator and warm starting recovers good performance most of the time. SGD also shows some sensitivity to early stopping, but there seems to be a stronger correlation between invested compute and final performance. Surprisingly, AP generally achieves good predictive performance even on the smallest compute budget, despite not reaching the tolerance of \(=0.01\). Overall, the relationship between reaching a low residual norm and obtaining good predictive performance seems to be weak. This is an unexpected yet interesting observation, and future research should investigate the suitability of the relative residual norm as a metric to determine solver convergence.

Figure 9: Relative residual norms of the probe vector linear systems at each marginal likelihood step on the pol dataset when solving until the tolerance or a maximum number of solver epochs is reached. Increasing the compute budget generally reduces the residual norm. Given the same compute budget, the pathwise estimator reaches lower residual norms than the standard estimator. Adding warm starts further reduces the residual norm for both estimators. However, the final test log-likelihood does not always match the residual norm. Surprisingly, good predictive performance can be obtained even if the residual norm is much higher than the tolerance \(=0.01\).

Demonstration on Large DatasetsAfter analysing early stopping on small datasets, we now turn to larger UCI datasets \(391<n<1.8\)M, where solving until reaching the tolerance becomes computationally infeasible. Thus, we introduce a compute budget of 10 solver epochs per marginal likelihood step. Hyperparameters are initialised with the heuristic of Lin et al.  and optimised using a learning rate of 0.03 for 30 Adam steps (15 for housekeeping due to high computational costs). We use the pathwise estimator because it accelerates solver convergence (see Section 3), and it enables efficient tracking of predictive performance during optimisation. See Appendix B for details.

Figure 10 visualises the evolution of the relative residual norm of the probe vector linear systems and the predictive test log-likelihood during marginal likelihood optimisation. A full set of results is in Appendix C. For all solvers, warm starting leads to lower residual norms throughout outer-loop steps. This suggests a synergistic behaviour between early stopping and warm starting: the latter allows solver progress to accumulate across marginal likelihood steps. This can be interpreted as amortising the inner-loop linear system solve over multiple outer-loop steps. Despite the lower residual norm, CG is brittle under early stopping, obtaining significantly worse performance than AP and SGD on buzz and housekeeping. AP and SGD seem to be more robust to early stopping. However, lower residual norms do not always translate to improved predictive performance. Furthermore, we find that SGD can suffer due to the optimal learning rate changing as the hyperparameters change.

## 6 Conclusion

Building upon a hierarchical view of marginal likelihood optimisation, this paper consolidates several iterative GP techniques into a common framework, analysing them and showing their applicability across different linear system solvers. Overall, these provide speed-ups of up to \(72\) when solving until a specified tolerance is reached, and decrease the average relative residual norm by up to \(7\) under a limited compute budget. Additionally, our analyses lead to the following findings: Firstly, the pathwise gradient estimate accelerates linear system solvers by moving solutions closer to the origin, and also provides amortised predictions as an added benefit by turning probe vectors into posterior samples via pathwise conditioning. Secondly, warm starting solvers at previous solutions during marginal likelihood optimisation reduces the number of solver iterations to tolerance at the cost of introducing negligible bias into the optimisation trajectory. Furthermore, warm starting combines well with pathwise gradient estimation. Finally, stopping linear system solvers after exhausting a limited compute budget generally increases the relative residual norm. However, when paired with warm starting, solver progress accumulates, amortising inner-loop linear system solves over multiple outer-loop steps. Nonetheless, we observe that low relative residual norms are not always necessary to obtain good predictive performance, which presents an interesting avenue for future research.

Figure 10: Relative residual norms and test log-likelihoods during marginal likelihood optimisation on large datasets using the pathwise estimator. Warm starting allows solver progress to accumulate over multiple marginal likelihood steps, leading to decreasing residual norms. Without warm starting, residual norms tend to remain similar or increase during optimisation. Despite reaching significantly lower residual norms, the predictive performance does not always improve, akin to Figure 9.