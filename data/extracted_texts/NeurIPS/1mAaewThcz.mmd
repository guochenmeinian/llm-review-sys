# Theoretical and Empirical Insights into the Origins of Degree Bias in Graph Neural Networks

Arjun Subramonian\({}^{1}\), Jian Kang\({}^{2}\), Yizhou Sun\({}^{1}\)

\({}^{1}\)University of California, Los Angeles, \({}^{2}\)University of Rochester

{arjunsub, yzsun}@cs.ucla.edu

jian.kang@rochester.edu

###### Abstract

Graph Neural Networks (GNNs) often perform better for high-degree nodes than low-degree nodes on node classification tasks. This degree bias can reinforce social marginalization by, e.g., privileging celebrities and other high-degree actors in social networks during social and content recommendation. While researchers have proposed numerous hypotheses for why GNN degree bias occurs, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory. Thus, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters. We prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is not significantly limited by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 common real-world networks, and based on our theoretical and empirical insights, describe a roadmap to alleviate degree bias. Our code can be found at: github.com/ArjunSubramonian/degree-bias-exploration.

## 1 Introduction

Graph neural networks (GNNs) have been applied to node classification tasks such as document topic prediction  and content moderation . However, in recent years, researchers have shown that GNNs exhibit better performance for high-degree nodes on node classification tasks. This has significant social implications, such as the marginalization of: (1) authors of less-cited papers when predicting the topic of papers in citation networks; (2) junior researchers when predicting the suitability of prospective collaborators in academic collaboration networks; (3) creators of newer or niche products when predicting the category of products in online product networks; and (4) authors of short or standalone websites when predicting the topic of websites in hyperlink networks.

To illustrate this phenomenon, Figure 1 shows that across different message-passing GNNs (see SSD for details about architectures) applied to the CiteSeer dataset (where nodes represent documents and the classification task is to predict their topic), high-degree nodes generally incur a lower test loss than low-degree nodes. In practice, if such GNNs are applied to predict the topic of documents in social scientific studies, less-cited documents will be misclassified, which can lead to the contributions of their authors not being appropriately recognized and erroneous scientific results. We present additional evidence of degree bias across different GNNs and datasets in SSE.

Researchers have proposed various hypotheses for why GNN degree bias occurs in node classification tasks. However, we find via a survey of 38 degree bias papers that these hypotheses are often not rigorously validated, and can even be contradictory (SS2). Furthermore, almost no prior works on degree bias provide a comprehensive theoretical analysis of the origins of degree bias that explicitly links a node's degree to its test and training error in the semi-supervised learning setting (SS2).

Hence, we theoretically analyze the origins of degree bias in node classification during test and training time for _general_ message-passing GNNs, with separate parameters for source and target nodes and residual connections. Our analysis spans different graph filter choices: **RW** (random walk-normalized filter), **SYM** (symmetric-normalized filter), and **ATT** (attention-based filter) (see SSD for formal definitions). In particular, we prove that high-degree test nodes tend to have a lower probability of misclassification regardless of how GNNs are trained. Moreover, we show that degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, SYM (compared to RW) may adjust its loss on low-degree nodes more slowly than on high-degree nodes; however, with sufficiently many epochs of training, message-passing GNNs can achieve their maximum possible training accuracy, which is only trivially curtailed by their expressive power. Throughout our analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We validate our theoretical findings on 8 real-world datasets (see SSC)that are commonly used in degree bias papers (see Figure 2, SSF). Based on our theoretical and empirical insights, we describe a principled roadmap to alleviate degree bias.

## 2 Background and Related Work

Numerous prior works have proposed hypotheses for why GNN degree bias occurs in node classification tasks. We summarize these hypotheses in Table 2 based on a survey of 38 non-review papers about degree bias in node classification that cite , a seminal work on degree bias.

While many of these papers have contributed solutions to degree bias (see SSA for a thorough overview), we find that their hypotheses for the origins of degree bias are often not rigorously validated, and can even be contradictory. For example, some hypotheses locate the source of degree bias in the training stage, while others cite interactions between training and test-time factors or purely test-time issues. Moreover, hypothesis **(H5)** in Table 2, which posits that high-degree node representations cluster more strongly, conflicts with and **(H10)**, which argues that high-degree node representations have a larger variance. In our theoretical analysis of the origins of degree bias, we connect our findings to these hypotheses.

We further find that almost no prior works on degree bias provide a comprehensive theoretical analysis of the origins of GNN degree bias that explicitly links a node's degree to its test and training

Figure 1: Test loss vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. High-degree nodes generally incur a lower test loss than low-degree nodes do. Error bars are reported over 10 random seeds; all error bars are 1-sigma and represent the standard deviation about the mean.

error in the semi-supervised learning setting (see Table 3). For example, most works prove that: (a) high-degree nodes have a larger influence on GNN node representations or parameter gradients, or (b) high-degree nodes cluster more strongly around their class centers or are more likely to be linearly separable; however, these works do not directly bound the probability of misclassifying a node during training vs. test time in terms of its degree.

The few works that do provide a theoretical analysis of degree bias: **(A1)** perform this analysis with overly strong assumptions, e.g., that graphs are sampled from a Contextual Stochastic Block Model (CSBM) , or **(A2)** posit that GNNs do not have sufficient expressive power to map nodes with different degrees to distinct representations. However, in the case of (1), for CSBM graphs, as the number of nodes \(n\), the degrees of nodes in each class concentrate around a constant value, which is contradictory to real-world graphs, making CSBM an inappropriate model to theoretically analyze degree bias. Moreover, many real-world social networks exhibit a power-law degree distribution , which is not captured by a CSBM. In the case of (2), SSI shows that the accuracy of GNNs on real-world networks is not significantly limited by the Weisfeiler-Leman (WL) test, which draws doubt to hypothesis **(H7)**.

Ultimately, previously-proposed hypotheses for why GNN degree bias occurs lack rigorous validation, and can even be contradictory. To unify and distill these hypotheses, we provide an analysis of the origins of degree bias in message-passing GNNs with different graph filters.

## 3 Preliminaries

Throughout our theoretical analysis, we connect our findings to previously-proposed hypotheses for the origins of degree bias, supporting and unifying some while drawing doubt to others. We further validate our findings on 8 real-world datasets (see SSC) that are commonly used in degree bias papers (see Figure 2, SSF). In all figures (except the PCA plots), error bars are reported over 10 random seeds. The factors of variability include model parameter initialization and training dynamics. All error bars are 1-sigma and represent the standard deviation (not standard error) of the mean. We

  
**Hypothesis** & **Papers** \\ 
**(H1)** Neighborhoods of low-degree nodes contain insufficient or overly noisy information for effective representations. & , , , , , , , , , , , , , , , , , , , ,  \\ 
**(H2)** High-degree nodes have a larger influence on GNN training because they have a greater number of links with other nodes, thereby dominating message passing. & , , , ,  \\ 
**(H3)** High-degree nodes exert more influence on the representations of and predictions for nodes as the number of GNN layers increases. & , ,  \\ 
**(H4)** In semi-supervised learning, if training nodes are picked randomly, test predictions for high-degree nodes are more likely to be influenced by these training nodes because they have a greater number of links with other nodes. & , ,  \\ 
**(H5)** Representations of high-degree nodes cluster more strongly around their corresponding class centers, or are more likely to be linearly separable. & , ,  \\   

Table 1: Five most popular hypotheses for the origins of degree bias proposed by papers. The remaining hypotheses can be found in Table 2.

implicitly assume that errors are normally-distributed. Error bars are computed using PyTorch's std function . We relegate all proofs to SSB.

We first introduce relevant notation and assumptions. Suppose we have a \(C\)-class node classification problem defined over an undirected connected graph \(=(,)\) with \(N=||\) nodes. We assume that our graph structure \(\{0,1\}^{N N}\) and node labels \(_{<C}^{N}\) are fixed, but our node features \(^{N d^{(0)}}\) are independently sampled from class-specific feature distributions, i.e., \( i,_{i}_{_{i}}\). We further have a model \(\) that maps \(,\) to predictions \(}^{N C}\). We use a cross-entropy loss function \((|i,c)=-}_{i,c}\) that computes the loss for node \(i\) with respect to class \(c\) for \(\). Per the semi-supervised learning paradigm , we train \(\) with the full graph \(,\) but only a labeled subset of nodes \(S\).

## 4 Test-Time Degree Bias

The test-time degree bias of models is important to study, as it can yield disparate performance for low-degree nodes when models are deployed in the real world. We prove that high-degree test nodes tend to have a lower probability of misclassification. Moreover, we show that GNN degree bias arises from a variety of factors that are associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). We first present a theorem that bounds the probability of a test node \(i S\) being misclassified. We suppose \(\) is a neural network that has \(L\) layers. It takes as input \(,\) and generates node representations \(^{(L)}^{N C}\); these representations are then passed through a softmax activation function to get \(}=^{(L)}=(^{(L)})\). At this point, we make few assumptions about the architecture of \(\); \(\) could be a graph neural network (GNN), or even an MLP or logistic regression model.

**Theorem 1**.: _Consider a test node \(i S\), with \(_{i}=c\). Furthermore, consider a label \(c^{} c\). Let \(((|i,c)>(|i,c^{}))\) be the probability of misclassifying \(i\). Then, if \([^{(L)}_{i,c^{}}-^{(L)}_{i,c}]<0\) (i.e., \(\) generalizes in expectation):_

\[((|i,c)>(|i,c^{})) }}, \]

_where the squared inverse coefficient of variation \(R_{i,c^{}}=[^{(L)}_{i,c^{}}-^{(L)}_{i,c}])^{2}}{[^{(L)}_{i,c^{}}-^{(L)}_{i,c}]}\)._

The assumption that \(\) generalizes in expectation is required for the application of Cantelli's inequality in the proof. Notably, it is not possible to prove a similar lower bound without making assumptions about the higher-order moments of \(^{(L)}_{i,c^{}}-^{(L)}_{i,c}\). The coefficient of variation \([^{(L)}_{i,c^{}}-^{(L)}_{i,c}]}{ [^{(L)}_{i,c^{}}-^{(L)}_{i,c}]}\) is a normalized measure of dispersion that is often used in economics to quantify inequality . Thus, \(R_{i,c^{}}\) captures how little \(Z_{i}\) varies relative to its expected value. In summary, the probability of misclassification \(((|i,c)>(|i,c^{}))\) can be minimized when \(R_{i,c^{}}\) is maximized. Intuitively, the probability of misclassification is reduced when \(_{i}\) is farther away, in expectation, from the decision boundary that separates classes \(c\) and \(c^{}\), and has low dispersion. The following subsections reveal why \(R_{i,c^{}}\) is large when \(i\) is high-degree.

### Random Walk Graph Filter

So far, we have made few assumptions about \(\). Now, we suppose \(\) is a general message-passing GNN . In particular, for layer \(l\):

\[^{(l)}=^{(l)}(^{(l)})=^{(l)}(^{(l -1)}^{(l)}_{1}+^{(l)}^{(l-1)}^{(l)}_{2}+^{ (l)}_{3}), \]

where \(^{(l)}^{N d^{(l)}}\) are the \(l\)-th layer node representations (with \(^{(0)}=\) and \(d^{(L)}=C\)), \(^{(l)}\) is an instance-wise non-linearity (with \(^{(L)}\) being softmax), \(^{(l)}^{N N}\) is a graph filter, and \(^{(l)}_{1},^{(l)}_{2},^{(l)}_{3}^{d^{(l-1)}  d^{(l)}}\) are the \(l\)-th layer model parameters.

We first consider the special case that \( l_{ L},^{(l)}=_{}=^{-1}\) (i.e., the uniform random walk transition matrix), where \(\) is the diagonal degree matrix with entries \(_{ii}=_{j}_{ij}\). We further simplify the model by choosing all \(^{(l)}\) (\(l<L\)) to be the identity function (e.g., as in ). By doing so, we get the following linear jumping knowledge model \(}\):

\[^{(L)}=(^{(L)})=(_ {l=0}^{L}_{}^{l}^{(l)}), \]

where \( l_{ L},^{(l)}^{d^{(0)} C}\). \(^{(l)}\) is the sum of all the weight terms that correspond to \(_{rw}^{l}\) in Eqn. 2; for simplicity, we collapse each sum of weight terms into a single weight matrix. It is still reasonable to have a different weight matrix \(^{(l)}\) for each term \(_{rw}^{l}\), as we may need to extract different information from features aggregated from neighborhoods at different hops. For each model \(\), \(}\) denotes the linearized version of the model that we theoretically analyze. Linearizing GNNs is a common practice in the literature .

We now prove a lower bound for \(R_{i,^{}}\). By identifying nodes for which this lower bound is larger, we can indirectly figure out which nodes have a lower probability of misclassification. In particular, we find that the bound is generally larger for high-degree nodes, which sheds light on the origins of degree bias. For simplicity of notation, we denote the weights corresponding to the decision boundary of the \(l\)-th term that separates classes \(c\) and \(c^{}\) by \(_{c^{}-c}^{(l)}=_{..,c^{}}^{(l)}-_{..,c}^{(l)}\), and \(^{(l)}(i)\) to

Figure 2: Visual summary of the geometry of representations, variance of representations, and training dynamics of RW, SYM, and ATT GNNs on CiteSeer. We consider low-degree nodes to be the 100 nodes with the smallest degrees and high-degree nodes to be the 100 nodes with the largest degrees. Each point in the plots in the left column corresponds to a test node representation and its color represents the nodeâ€™s class. (In this particular dataset, low-degree nodes are more heavily concentrated in a few classes.) The plots in the left column are based on a single random seed, while the plots in the middle and right columns are based on 10 random seeds. RW representations of low-degree nodes often have a larger variance than high-degree node representations, while SYM representations of low-degree nodes often have a smaller variance. Furthermore, SYM generally adjusts its training loss on low-degree nodes less rapidly.

be the distribution over the terminal nodes of length-\(l\) uniform random walks starting from node \(i\). We further define:

\[_{i,c^{}}^{(l)}=_{j^{(l)}(i)}[ _{_{j}}[^{T}_{c^{}-c}^{(l)} ]] \]

as the \(l\)-hop prediction homogeneity of \(i\) with respect to \(c^{}\) when \(_{i}=c\). In essence, \(_{_{j}}[^{T}_{c^{} -c}^{(l)}]\) captures the expected prediction score of \(_{c^{}-c}^{(l)}\) for a node \(j\) whose features \(_{j}_{j}\); when \(_{_{j}}[^{T}_{c^{} -c}^{(l)}]\) is more negative on average, \(_{c^{}-c}^{(l)}\) predicts \(j\) to belong to class \(c\) with higher likelihood. Thus, \(_{i,c^{}}^{(l)}\) measures the expected prediction score for nodes \(j\), weighted by their probability of being reached by a length-\(l\) random walk starting from \(i\).

From a topological perspective, because \(_{i,c^{}}^{(l)}\) depends on the distribution of random walks from \(i\), it is intimately related to local graph structure. Indeed, \(_{i,c^{}}^{(l)}\) can be interpreted as a "local subgraph difference" and is highly influenced by the local homophily of \(i\). However, \(_{i,c^{}}^{(l)}\) is also influenced by the presence of \(l\)-hop neighbors contained in the training set, as the model is more likely to correctly classify these nodes by a large margin; hence, \(_{i,c^{}}^{(l)}\) does not _only_ boil down to local homophily. We discuss other connections between prediction homogeneity, homophily, and separability in SSA.4.

In addition to the \(l\)-hop prediction homogeneity, we denote the \(l\)-hop collision probability by:

\[_{i}^{(l)}=_{j}[(_{}^{l} )_{ij}]^{2}, \]

which quantifies the probability of two length-\(l\) random walks starting from \(i\) colliding at the same end node \(j\). When the collision probability is lower, random walks starting from \(i\) have a higher likelihood of ending at distinct nodes; in effect, the random walks can be considered to be more diverse.

**Theorem 2**.: _Assume that \( l_{ L}, j,_{ _{j}}[^{T}_{c^{}-c}^{(l)}]  M\). Then:_

\[R_{i,c^{}}^{L}_{i,c^{}}^{(l)}) ^{2}}{M(L+1)_{l=0}^{L}_{i}^{(l)}}. \]

We observe that to make \(R_{i,c^{}}\) larger, and thus minimize the probability of misclassification, it is sufficient (although not necessary) that the inverse collision probability \(^{L}_{i}^{(l)}}\) is larger. When \(L=1\), \(^{L}_{i}^{(l)}}=_{ii}^{L}+1}\), which is larger for high-degree nodes. We find empirically that the inverse collision probability is positively associated with node degree (see Figures 3, 13, 14). (We elaborate on connections between the inverse collision probability and node degree in SSA.) Furthermore, disparities in the inverse collision probability across nodes with different degrees is _reduced_ by residual connections and _increased_ by self-loops. Intuitively, random walks starting from high-degree nodes diffuse more quickly, maximizing the probability of any two random walks not colliding at the same end node; in this way, a higher inverse collision probability indicates a more diverse and possibly informative \(L\)-hop neighborhood. This finding supports hypothesis **(H1)** (see Table 2).

Additionally, to make \(R_{i,c^{}}\) larger, it is sufficient that for all \(l_{ L}\), \(_{i,c^{}}^{(l)}\) is more negative, e.g., when most nodes in the \(l\)-hop neighborhood of \(i\) are predicted to belong to class \(c\). Thus, \(_{i,c^{}}^{(l)}\) can be more negative when nodes in the \(l\)-hop neighborhood of \(i\) also are in class \(c\) (i.e., node \(i\) has high local homophily) and were part of the training set \(\), leading to them being correctly classified. This finding supports hypotheses **(H4)** and **(H6)**. Notably, we cannot make \(_{l=0}^{L}_{i,c^{}}^{l}\) more positive to increase \(R_{i,c^{}}\); this would violate the assumption of Theorem 2 that the model generalizes in expectation, which is necessary to make a mathematically rigorous statement about degree bias via tail bounds. Intuitively, it also would not make sense that RW and SYM reduce the misclassification error for a node by predicting its neighbors to be of a different class, since message passing smooths the representations of adjacent nodes. Moreover, distribution shifts in local homophily from train to test time can reduce test-time prediction performance, bringing \(^{(l)}_{i,c^{}}\) closer to 0; this can increase \(R_{i,c^{}}\), thereby not inducing as much degree bias at the expense of overall test performance.

Furthermore, our proof of Theorem 2 (Eqn. 21) reveals that in expectation, the linearized model \(}\) produces similar representations for low and high-degree nodes with similar \(L\)-hop neighborhood homophily levels. However, low-degree nodes (specifically nodes with a lower inverse collision probability) tend to have a higher variance in \(}\)'s representation space than high-degree nodes do (Eqn. 24). This entails that factors beyond homophily (e.g., diversity of neighbors) induce degree bias. We validate these findings empirically in Figure 2 and SSF. In Figure 2, we see in the left plot in the RW row (first row) that low-degree test nodes have representations that are similarly centered but more spread out in the first two principal components of all the test representations than high-degree nodes; we confirm that low-degree node representations have a larger variance in the middle plot in the RW row. Thus, regardless of how RW is trained, low-degree nodes have a higher probability of being on the wrong side of RW's decision boundaries. Indeed, the left plot in the RW row shows that low-degree nodes of a certain class end up closer to nodes of a different class at a higher rate. Notably, this occurs even when RW is relatively shallow (i.e., 3 layers). Thus, this finding supports hypothesis **(H5)**, as well as draws doubt to hypotheses **(H3)** and **(H10)**. Our results for \(}\) may also hold for ATT when low-degree nodes are generally less attended to since like random walk transition matrices, attention matrices are row-stochastic.

### Symmetric Graph Filter

We now consider the special case that \( l_{ L},^{(l)}=_{}=^{- }^{-}\). We once again simplify \(\) by making all \(^{(l)}\) the identity function, getting SYM:

\[^{(L)}=(^{(L)})=(_{ l=0}^{L}_{}^{l}^{(l)}). \]

We define:

\[^{(l)}_{i,c^{}}=_{j^{(l)}(i)} [_{jj}}}_{_{_{j}}}[^{T}^{(l)}_{c^{}-c}]] \]

as the degree-discounted \(l\)-hop prediction homogeneity. Similar to \(^{(l)}_{i,c^{}},^{(l)}_{i,c^{}}\) measures the expected prediction score for nodes \(j\), but weighted by the inverse square root of their degree in addition to their probability of being reached by a length-\(l\) random walk starting from \(i\). In effect, \(^{(l)}_{i,c^{}}\) more heavily discounts the prediction scores for high-degree nodes. We also denote the degree-discounted sum of collision probabilities by:

\[^{(l)}_{i}=_{j}_{jj}}[ (_{}^{l})_{ij}]^{2}, \]

Figure 3: Inverse collision probability vs. degree of nodes in CiteSeer for RW, SYM, and ATT GNNs. Node degrees generally have a strong association with inverse collision probabilities.

where each summation term \([(_{}^{l})_{ij}]^{2}\) quantifies the probability of two length-\(l\) random walks starting from \(i\) ending at \(j\) and is discounted by the degree of \(j\). Compared to the random walk setting, the degree-discounted prediction homogeneity and sum of collision probabilities suppress the contributions of high-degree nodes. We now prove a lower bound for \(R_{i,e^{}}\) for \(}\).

**Theorem 3**.: _Assume that \( l_{ L}, j,_{ _{_{j}}}[^{T}_{c^{}-c}^{(l)}]  M\). Then:_

\[R_{i,e^{}}^{L}_{i,e^{}}^ {(l)})^{2}}{M(L+1)_{l=0}^{L}_{i}^{(l)}}. \]

Once again, we observe that \(R_{i,e^{}}\) is larger, and thus the probability of misclassification is minimized, when the inverse (degree-discounted) sum of collision probabilities \(^{L}_{i}^{(l)}}\) is larger and for all \(l_{ L}\), the (degree-discounted) \(l\)-hop prediction homogeneity \(_{i,e^{}}^{(l)}\) is more negative. Like for RW, these findings support hypotheses **(H1)**, **(H4)**, and **(H6)** (see Table 2).

Furthermore, our proof of Theorem 3 (Eqn. 33) reveals that in expectation, \(}\) often produces representations for low-degree nodes that lie closer to \(}\)'s decision boundary than representations of high-degree nodes with similar \(L\)-hop neighborhood homophily levels. This is because \(}\) produces node representations that are approximately scaled by the square root of the node's degree. However, for the same reason, unlike for \(}\), low-degree nodes tend to have a lower variance in \(}\)'s representation space than high-degree nodes do (Eqn. 36); this corroborates the findings of . We validate this empirically in Figure 2 and SSF on the homophilic datasets (i.e., all datasets except chameleon and squirrel). In Figure 2, we see in the left plot in the SYM row (second row) that low-degree test nodes (particularly low-degree nodes with many high-degree nodes in their \(L\)-hop neighborhood) have representations that are closer to SYM's decision boundaries but less spread out in the first two principal components of all the test representations than high-degree nodes; we confirm that low-degree node representations have a smaller or comparable variance in the middle plot in the SYM row. We emphasize that while SYM representations of high-degree nodes have a higher variance, this itself is _not_ the cause of degree bias; since the standard deviation _and_ expectation of SYM node representations are approximately scaled by the same factor, by Theorem 1, the variance of SYM representations of high-degree nodes does not enlarge \(R_{i,e^{}}\) noticeably more than in the RW case.

Notably, our theoretical findings do extend to heterophilic graphs. In particular, high-degree nodes in heterophilic networks (e.g., chameleon and squirrel) do not have higher negative \(L\)-hop prediction homogeneity levels due to higher local heterophily (see SF), and hence we do not necessarily observe better test performance for them (see Figure 5). None of our theoretical analysis assumes homophilic networks.

Ultimately, like for RW, low-degree nodes (specifically nodes with a lower inverse collision probability) have a larger probability of being on the wrong side of SYM's decision boundaries (regardless of how SYM is trained). Indeed, low-degree nodes of a certain class end up closer to nodes of a different class at a higher rate. Notably, this occurs even when SYM is relatively shallow (i.e., 3 layers). Thus, this finding supports hypothesis **(H5)**, and draws doubt to hypotheses **(H3)**, **(H7)**, and **(H10)**.

## 5 Training-Time Degree Bias

We show that during training, SYM (compared to RW) may adjust its loss on low-degree nodes more slowly than on high-degree nodes. This finding is important because as GNNs are applied to increasingly large networks, only a few epochs of training may be possible due to limited compute; as such, we must ask: which nodes receive superior utility from limited training? Even though we know the labels for training nodes, GNNs may serve as an efficient lookup mechanism for training nodes in deployed systems; thus, if partially-trained, GNNs can perform poorly for low-degree training nodes. We also empirically demonstrate that despite learning at different rates for low vs. high-degree nodes, message-passing GNNs (even those with static filters) can achieve their maximum possible training accuracy, which is not significantly curtailed by their expressive power.

We first demonstrate that during each step of training of SYM with gradient descent, the loss of low-degree nodes is adjusted more slowly than high-degree nodes. We consider the setting that, for all \(l_{ L}\), at each training step \(t\):

\[^{(l)}[t+1]^{(l)}[t]-^{(l)}[t]}(B[t]), \]

where \(^{(l)}[t]\) is \(^{(l)}\) at training step \(t\), \(\) is the learning rate, \([t]\) is the model's loss at \(t\), and \(B[t] S\) (where \(S\) is the labeled subset of nodes) is the batch used at step \(t\).

Consider a node \(i\), with \(_{i}=c\). We define \(_{i}^{(L)}[t]\) to be \(_{i}^{(L)}\) at timestep \(t\). We begin by proving the following lemma, which states that for any model \(\), \([t](|i,c)\) (for all \(t\)) is \(\)-Lipschitz continuous with respect to \(_{i}^{(L)}[t]\).

**Lemma 1**.: _For all \(t\), \([t](|i,c)\) is \(\)-Lipschitz continuous with respect to \(_{i}^{(L)}[t]\) with constant \(=\), that is:_

\[|[t+1](|i,c)-[t](|i,c)|\|_{i}^{(L) }[t+1]-_{i}^{(L)}[t]\|_{2} \]

Now, we move to the main theorem where we bound the change in loss \(i\) after an arbitrary training step \(t\) (regardless of batching paradigm) in terms of its degree. We denote the residual of the predictions of \(}\) at step \(t\) by \([t]=^{(L)}[t]-([t])\), where \(^{(L)}[t]\) and onehot \(([t])\) are the submatrices formed from the rows of \(^{(L)}\) and onehot \(()\), respectively, that correspond to the nodes in \(B[t]\). Furthermore, we denote \( l_{ L}\), the expected similarity of the neighborhoods of \(i\) and \(B[t]\) by \(_{i}^{(l)}^{|B[t]|}\), where for \(m B[t],(_{i}^{(l)}[t])_{m}=_{mm}} _{j^{(l)}(i),k^{(l)}(m)}[D_{kk}}}_{j}_{k}^{T}]\). Specifically, \((_{i}^{(l)}[t])_{m}\) captures the degree-discounted expected similarity between the raw features of nodes \(j\) and \(k\) with respect to the \(l\)-hop random walk distributions of \(i\) and \(m B[t]\). Notably, our matrix is _pre_-feature aggregation (e.g., unlike ).

**Theorem 4**.: _The change in loss for \(i\) after an arbitrary training step \(t\) obeys:_

\[|[t+1](}|i,c)-[t](}|i,c) |_{ii}}\|[t]\|_{F} _{l=0}^{L}\|_{i}^{(l)}[t]\|_{2}. \]

As observed, the change (either increase or decrease) in loss for \(i\) after an arbitrary training step has a smaller magnitude if \(i\) is low-degree. Thus, when \([t+1](}|i,c)<[t](}|i,c)\) (e.g., if \(i B[t]\)), the loss for \(i\) decreases more slowly when \(i\) is low-degree. In effect, because the magnitude of SYM node representations is positively associated with node degree while the magnitude of each gradient descent step is the same across nodes, the representations of low-degree nodes experience a smaller change during each step. We additionally notice that the loss for \(i\) changes more slowly when the features of nodes in its \(L\)-hop neighborhood are not similar to the features in the \(L\)-hop neighborhoods of the nodes in each training batch (i.e., \(_{l=0}^{L}\|_{i}^{(l)}[t]\|_{2}\) is small). Because the \(L\)-hop neighborhoods of low-degree nodes tend to be smaller than those of high-degree nodes, their neighborhoods often have less overlap with the neighborhoods of training nodes, which can further constrain the rate at which the loss for \(i\) changes. Notably, while node degree highly affects the rate of learning, differences in \(\) across nodes due to factors other than degree are also influential.

We confirm these findings empirically in Figure 2 and SSF. For all the datasets, when training SYM, the blue curve (i.e., the loss for low-degree nodes) has a less steep rate of decrease than the orange curve (i.e., the loss for high-degree nodes) as the number of epochs increases. For example, in Figure 2, in the case of RW and ATT, the training loss curves for low and high-degree nodes (including error bars) overlap during the first \( 20\) epochs of training. However, for SYM, the loss curve for high-degree nodes descends more rapidly than the curve for low-degree nodes. These findings support hypothesis **(H2)** (c.f. Table 2).

In SSH, we demonstrate that during each step of training \(}\) with gradient descent, compared to \(}\), the loss of low-degree nodes in \(S\) is not necessarily adjusted more slowly. Furthermore,in SS1, we empirically show that SYM (despite learning at different rates for low vs. high-degree nodes), RW, and ATT can achieve their maximum possible training accuracy, which is often close to 100%; this indicates that expressive power does not significantly limit the accuracy of these models in practice and draws doubt to hypothesis (**H7**).

## 6 Principled Roadmap to Address Degree Bias

The primary aim of this work is to explore and explain the origins of GNN degree bias, which lacks a principled understanding. Future research can build on the strong theoretical and empirical foundation laid by this paper to propose alleviation strategies for degree bias. In particular, our findings reveal that any alleviation strategies should target the following theoretically-justified criteria, which we have empirically validated on 8 real-world datasets:

* **Maximizing the inverse collision probability of low-degree nodes (e.g., via edge augmentation for low-degree nodes).** Figure 3 and the plots in Section G show strong positive associations between inverse collision probability and degree for the RW, SYM, and ATT filters, and Figure 1 and the plots in Section E show strong negative associations between degree and test loss for the homophilic datasets. Hence, we validate that a higher inverse collision probability is associated with lower test loss, as our theory predicts.
* **Increasing the \(L\)-hop prediction homogeneity of low-degree nodes (e.g., by ensuring similar label densities in the neighborhoods of low and high-degree nodes).** The lack of degree bias observed in Figure 5 for chameleon and squirrel (which are heterophilic networks), compared to Figure 1 and the plots in Section E, confirms our theoretical finding that under heterophily, the prediction homogeneity for high-degree nodes is closer to 0, so high-degree nodes do not necessarily experience better performance.
* **Minimizing distributional differences (e.g., differences in expectation, variance) in the representations of low and high-degree nodes.** Figures 2 and 6-10 empirically confirm our theoretical finding that disparities in the expectation and variance of node representations are responsible for performance disparities. Figures 11 and 12 suggest that smaller distributional differences among representations (due to heterophily) can alleviate degree bias.
* **Reducing training discrepancies with regards to the rate at which GNNs learn for low vs. high-degree nodes.** Figure 2 and the plots in Section F validate our theoretical finding that SYM adjusts its loss on low-degree nodes more slowly than on high-degree nodes (see 5).

These criteria are important because they reflect (to a large extent) inherent fairness issues with the graph filters that are popular in graph learning. For instance, the random walk and symmetric filters disadvantage low-degree nodes by yielding representations with high variance and low magnitude, respectively. It is valuable for graph learning practitioners to investigate filters that are adaptive or not restricted to the graph topology in a way that ensures that low-degree nodes are not marginalized through disparate representational distributions or poor neighborhood diversity.

## 7 Conclusion

Our theoretical analysis aims to unify and distill previously-proposed hypotheses for the origins of GNN degree bias. We prove that high-degree test nodes tend to have a lower probability of misclassification and that degree bias arises from a variety of factors associated with a node's degree (e.g., homophily of neighbors, diversity of neighbors). Furthermore, we show that during training, some GNNs may adjust their loss on low-degree nodes more slowly; however, GNNs often achieve their maximum possible training accuracy and are trivially limited by their expressive power. We validate our theoretical findings on 8 real-world networks. Finally, based on our theoretical and empirical insights, we describe a roadmap to alleviate degree bias. More broadly, we encourage research efforts that unveil forms of inequality reinforced by GNNs. We detail the limitations and possible future directions of our work in SS1, including our survey, theoretical analysis (e.g., focusing on linearized GNNs, node classification), empirical validation (e.g., exploring degree bias in the inductive learning setting and heterogeneous and directed networks), and roadmap. We additionally discuss broader impacts in SS1.

#### Acknowledgments and Disclosure of Funding

This work was partially supported by NSF 2211557, NSF 1937599, NSF 2119643, NSF 2303037, NSF 2312501, NASA, SRC JUMP 2.0 Center, Amazon Research Awards, and Snapchat Gifts.