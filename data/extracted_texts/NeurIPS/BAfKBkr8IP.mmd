# Rethinking Fourier Transform from A Basis Functions Perspective for Long-term Time Series Forecasting

Rethinking Fourier Transform from A Basis Functions Perspective for Long-term Time Series Forecasting

Runze Yang\({}^{1,2}\), Longbing Cao\({}^{2*}\), Jianxun Li\({}^{1}\), Jie Yang\({}^{1}\)

\({}^{1}\)Department of Automation, Shanghai Jiao Tong University

\({}^{2}\)School of Computing, Macquarie University

{runze.y, jieyang, lijx}@sjtu.edu.cn

{runze.yang}@hdr.mq.edu.au, {longbing.cao}@mq.edu.au

Corresponding author

###### Abstract

The interaction between Fourier transform and deep learning opens new avenues for long-term time series forecasting (LTSF). We propose a new perspective to reconsider the Fourier transform from a basis functions perspective. Specifically, the real and imaginary parts of the frequency components can be viewed as the coefficients of cosine and sine basis functions at tiered frequency levels, respectively. We argue existing Fourier-based methods do not involve basis functions thus fail to interpret frequency coefficients precisely and consider the time-frequency relationship sufficiently, leading to inconsistent starting cycles and inconsistent series length issues. Accordingly, a novel Fourier basis mapping (FBM) method addresses these issues by mixing time and frequency domain features through Fourier basis expansion. Differing from existing approaches, FBM (i) embeds the discrete Fourier transform with basis functions, and then (ii) can enable plug-and-play in various types of neural networks for better performance. FBM extracts explicit frequency features while preserving temporal characteristics, enabling the mapping network to capture the time-frequency relationships. By incorporating our unique time-frequency features, the FBM variants can enhance any type of networks like linear, multilayer-perceptron-based, transformer-based, and Fourier-based networks, achieving state-of-the-art LTSF results on diverse real-world datasets with just one or three fully connected layers. The code is available at: https://github.com/runze1223/Fourier-Basis-Mapping.

## 1 Introduction

Long-term time series forecasting (LTSF) is essential for applications with long-range sequences, which presents significant challenges including long-range temporal dependencies, and frequency-oriented global dynamics. Recently, deep neural networks (DNNs) have thrived to tackle LTSF challenges for the presence of hierarchical effects, varied outliers, and nonlinear dynamics. They use various DNN architectures including recurrent neural networks (RNNs) , convolution neural networks (CNNs) , multi-layer perceptron (MLP) based networks , Transformer-based networks . However, a recent study NLinear  shows that an embarrassingly simple normalized linear model can outperform most of the DNN-based methods. This raises the question: _Would a complex Transformer-like architecture necessarily lead to better LTSF performance?_ According to CrossGNN , DNN-based methods like Transformers suffer from the influence of noise signals and produce high attention scores for unexpected noises.

Thus, Fourier-based time series modeling emerges as a new paradigm to remove noise signals by considering diverse effects hierarchically at different frequency levels. If we rethink the Fourier transform from a basis functions perspective, the real and imaginary parts can be interpreted as the coefficients of cosine and sine basis functions at tiered frequencies, respectively. However, existing Fourier-based methods do not involve basis functions, thus failing to interpret frequency coefficients precisely and do not consider the time-frequency relationships sufficiently, as shown in Figure 1. They face two main issues: inconsistent starting cycles and inconsistent series length issues. For instance, FEDformer , FreTS , FiLM , FITS , FGNet , and FL-Net  use the real and imaginary parts of frequency components as the input of the mapping network and conduct the mapping in the frequency space. From our new perspective, we find that the amplitude and arctangent of the real and imaginary components carry more explicit meanings than the components themselves. Furthermore, it is worth noting that the meanings of frequency components are bounded by the series length, and failing to establish the connections will lead to ambiguity in understanding the precise frequency in these models, e.g., 2 Hz sine and cosine wave with series lengths 96 and 336 has distinct meaning. More importantly, Figure 1 shows that a Fourier basis function is time-dependent when the input length is not divisible by a certain frequency level, making it even more challenging for the model to correctly interpret those frequency components without the basis functions. Consequently, constructing the mapping in the frequency space is not enough and cannot capture time-frequency relationships, which has been ignored by all existing Fourier-based methods. Although some methods like TimesNet  introduce time-frequency features, these features are non-interpretable and complex, as the summation over the frequency dimension does not recover the original time series. We provide a more detailed discussion of their limitations in Section 2.

Accordingly, we propose the Fourier Basis Mapping (FBM) to address the aforementioned issues, involving learnable and non-learnable features. In the first stage, we embed the discrete Fourier transform with basis functions, referred to as _Fourier basis expansion_, to extract time-frequency features. In the second stage, we validate its effectiveness using three types of mapping networks through three FBM variants (FBM-L, FBM-NL, FBM-NP) against four categories of LTSF baseline methods: (1) Linear method: NLinear; (2) Transformer-based methods: iTransformer  and PatchTST ; (3) MLP-based methods: N-BEATS , FreTS, and TimeMixer ; and (4) Fourier-based methods: N-BEATS, FITS, FreTS, and CrossGNN. The experimental results reveal

Figure 1: Comparison of Existing Fourier-based Methods with Our Approach Fourier basis mapping. Existing methods primarily operate in the frequency domain, with the mapping focusing on frequency features. Our FBM simultaneously operates in both the time and frequency domains, with the mapping focusing on time-frequency features.

that Fourier basis expansion can enhance any types of DNNs. With time-frequency features, simple mapping networks (e.g, L-vanilla linear networks and NL-three-layer MLPs) can achieve the SOTA of LTSF on eight real-world datasets and of STSF on PEMS datasets.

## 2 Related work

Recent studies have investigated the effect of the Fourier transform in addressing LTSF challenges. Consequently, we categorize the relevant studies into two groups: (1) Fourier-based architectures, and (2) DNN-based architectures. In the first category, we discuss the applications of the Fourier transform, while the second addresses innovative model architectures.

### Fourier-based architectures

Fourier transform has been integrated into a wide range of network architectures, including CNNs , recurrent networks , graph neural networks (GNNs) [35; 40], Transformer , and MLP networks [18; 16; 17]. However, by rethinking the Fourier transform from a basis functions perspective, we identify the inconsistent starting cycles and series length issues. In path (i) of Figure 1, methods like FEDformer , FreTS , FiLM , FITS , FGNet , and FL-Net  use real and imaginary parts as inputs to their mapping networks but the networks cannot easily interpret those coefficients because the crucial information is stored in the amplitude, phase and length of each cycle. In path (ii) of Figure 1, methods like CrossGNN , and TimesNet  use the discrete Fourier transform (DFT) to select the top-k amplitudes for noise filtering. However, a higher amplitude does not necessarily indicate a useful frequency and a lower amplitude is not necessarily useless. In addition, CrossGNN only stores the values of the first cycles without considering the effects of phase shifts and time-dependent basis, while TimesNet introduces the multi-window Fourier transform, which compromises the integrity of the temporal information. In path (iii) of Figure 1, methods like N-BEATS  and N-Hits  can similarly be viewed as forecasting through the inverse discrete Fourier transform (IDFT) by computing the output frequency spectrum of the time series, but their networks fail to capture time-dependent effects. In contrast, our method distinguishes itself from existing approaches by leveraging the Fourier basis expansion to provide a mixture of time and frequency domain features, thus avoiding the aforementioned issues.

### DNN-based architectures

Firstly, we introduce key techniques for innovative model architectures. In , initial normalization is introduced to enhance the robustness. Autoformer  advocates the trend and seasonal decomposition with the moving average kernel, consistent with other methods like DLinear  and TimeMixer . However, the effectiveness of the decomposition relies on the choice of kernel size, which is improved by our methods. Methods like VH-NBEATS  and BasisFormer  involve the pre-trained basis functions to account for hourly, daily, and weekly effects. In contrast, our method uses the Fourier basis to capture those effects automatically. Transformer-based methods are one of the most popular LTSF architectures. Some transformer variants improve efficiency by reducing computational complexity and memory cost, such as LogTrans , Pyraformer , Informer , and Autoformer . On the other hand, PatchTST  introduces patching to treat a period of local time steps as a semantic vector, effectively reducing complexity and achieving state-of-the-art results over Transformer-based methods. In a recent study, iTransformer  suggests that employing attention layers to capture relationships between variables may yield better results. In this paper, we validate the effectiveness of Fourier basis expansion on linear, MLP, and Transformer-based networks, proving that it can enhance any types of networks. The Fourier basis expansion can also be viewed as an enhanced version of trend and seasonal decomposition, as the basis functions, shown in Figure 1, automatically decompose various effects by frequency levels.

## 3 Rethinking Fourier transform w.r.t. basis functions

In this section, we offer a new perspective of the Fourier Transform for LTSF. Firstly, we discuss the mathematical reasoning behind the DFT and IDFT from the perspective of basis functions. From this new perceptive, we find that the real and imaginary parts of the frequency components can be viewed as the coefficients of cosine and sine basis functions at tiered frequency levels. Consequently,the effect of the IDFT for frequency-based mapping can be seen as analogous to the design of the N-BEATS seasonal block. As such, we identify the inconsistent starting cycles and inconsistent series length issues in existing studies.

Let \(\) and \(\) be the input and output time series, respectively, and \(T\) and \(L\) represent the look-back window and forecast horizon, assuming both of them are even numbers. \(^{X}\) and \(^{Y}\) refer to the frequency spectrum of the input and the output, respectively. Then, DFT and IDFT of the input time series \(\) can be expressed as follows:

\[(k)=DFT() =_{n=0}^{T-1}[n](-i), k=0,1,,T-1,\] (1) \[[n] =IDFT() =_{k=0}^{T-1}[k](i), n=0,1,,T-1.\] (2)

From the perspective of basis functions, both DFT and IDFT can be expressed by \(+1\) orthogonal cosine basis functions and \(-1\) orthogonal sine basis functions. This is because the DFT of a real-valued signal is Hermitian symmetric. Their proofs and further explanations can be found in Appendices B.1 and B.2. Subsequently, we can rewrite the IDFT w.r.t. basis functions, and the connection between \(\) and \(^{X}\) can be expressed as follows:

\[[n] =_{k=0}^{}(} ()-}( )), n=0,1, T-1,\] (3) \[} =}[k],&k=0,\\ 2}[k],&k=1,,-1}=}[k]=0,&k=0,\\ 2}[k],&k=1,,-1.\]

\(}[k]\) and \(}[k]\) represent the real and imaginary parts of \([k]\) respectively, where \(k\) refers to the frequency level, and \([k]=}[k]+i}[k]\). Eq. (4) provides an essential insight that the real and imaginary parts of the frequency spectrum can be interpreted as the coefficients of cosine and sine basis functions, respectively. Hence, computing the frequency spectrum of the output time series is equivalent to computing the coefficients between cosine and sine basis functions, which can be considered analogous to the N-BEATS-seasonal-block design. Further explanation and proofs can be found in Appendix B.3.

Consequently, we identify two issues in existing Fourier-based studies: inconsistent starting cycles and inconsistent series length issues. The inconsistent starting cycles issue arises since the real and imaginary parts do not carry explicit meanings without the basis functions. This is because that adding a sine wave and a cosine wave with the same frequency results in a shifted cosine wave with the same frequency, which can be expressed as follows:

\[(t) =A(wt)+B(wt)=R(wt-),\] (4) \[R =+B^{2}},=arctan(B,A).\]

Therefore, the key information is embedded in the amplitude and arctangent of the real and imaginary values rather than the values themselves. The inconsistent series length issue arises since the meaning of frequency in hertz (Hz) is bound by the series length.

Figure 2 illustrates these two issues as Cases I and II through a manually generated time series with pure sine and cosine basis functions, assuming using time series \(\) to forecast time series \(\). In Case I, \(\) and \(\) have the same frequency and series length but different starting cycles. If the real and imaginary parts of the frequency spectrum of the input are used to compute that of the output through independent channels, we cannot find such a mapping as there is no mathematical solution. However, their relationships can be easily identified through the amplitude and arctangent of the real and imaginary values, which are embedded in basis functions. As shown in Figure 2, a mathematical solution exists which is achievable through Eq. (4).

In Case II, \(\) and \(\) have almost the same frequency and starting cycles but different series lengths. Although the landscapes of the frequency spectrum look similar, their components (in Hz) have distinct meanings. For instance, an 8Hz cosine function with a series length of 192 is equivalent to a 14Hz cosine function with a series length of 336. Unfortunately, such a frequency connection is not provided by existing models. Hence, the precise interpretation of frequency components is challenging for the model due to varying series lengths. The presence of time-dependent basis functions makes it even harder for the model to accurately interpret the frequency components. Instead, these issues can be solved easily by the incorporation of basis functions, which will be discussed in Section 4.

## 4 FBM: Fourier basis mapping

To address two aforementioned issues, we introduce the Fourier basis mapping (FBM). Figure 3 shows the architecture of FBM. The primary strength of FBM lies in constructing time-frequency features, which capture explicit frequency information while preserving temporal characteristics. Subsequently, the downstream mapping considers the time-frequency space rather than solely the time or frequency space for forecasting.

First, we apply data normalization to enhance the robustness of FBM, the same as the implementation suggested in . Then, we generate the frequency spectrum \(H\) using DFT. Next, we multiply the real part of \(\) (denoted as \(}\)) with the orthogonal cosine basis \(\) and the imaginary part of \(\) (denoted as \(}\)) with the orthogonal sine basis \(\) to obtain the mixture of frequency and temporal features. Let \(=[0,1,,T-1]\), then \(\) and \(\) can be expressed as follows:

\[ =[,2(}{T}),,2 (}{T}),()],\] (5) \[ =-[0,2(}{T}),,2( }{T}),()].\]

Figure 2: Two Issues of Existing Fourier-based LTSF Models: Inconsistent Starting Cycles Issue and Inconsistent Series Length Issue. In Case I, \(\) and \(\) have a starting cycle gap of 104 over 336. In Case II, \(\) and \(\) have different series lengths of 336 and 192, respectively.

Further, we combine the cosine and sine basis functions to obtain the shifted starting cycles of the time series, forming \(\). The scalar \(2\) is derived in Eq. (4) for Hermitian symmetric. Thus, by summing over the \(+1\) frequency domain, the model can recover the original time series without compromising the integrity of the temporal information. The process is analogous to decomposing the original time series into \(+1\) components with tiered frequency levels.

Finally, we use a decoder to map the features \(\) to the output time series. Our time-frequency features automatically decompose hierarchical effects by frequency levels, accounting for their interactive effects while filtering out associated noise through the mapping network. The Fourier basis expansion provides more effective and interpretable initial features, facilitating a simpler downstream mapping process. To this end, we further propose three mapping methods: linear (L), nonlinear three-layer perception (NL), and nonlinear PatchTST (NP). Consequently, three FBM variants are generated: FBM-L with linear network, FBM-NL with MLP network, and FBM-NP with transformer-based network with patching. The first one includes a single vanilla linear layer, the second one consists of three fully connected layers with activation functions, and the last one follows the same structure as PatchTST, but we conduct the patching after the Fourier basis expansion. It is worth noting that FBM-NP employs fewer patches than PatchTST to reduce complexity, making it more efficient while delivering superior performance. Details of our three mapping networks, including hyperparameters and layer specifications, are provided in Figure 6 and Table 5 in Appendix C.

## 5 Experiment results

### Baselines and experiment settings

We compare FBM with eight baseline methods: NLinear, PatchTST, N-BEATS, CrossGNN, FEDformer, FreTS, FiLM, and BasisFormer. These methods are chosen because they represent four categories of LTSF modeling methods: (1) Linear method: NLinear; (2) Transformer-based methods: iTransformer and PatchTST; (3) MLP-based methods: N-BEATS, FreTS, and TimeMixer; and (4) Fourier-based methods: N-BEATS, FITS, FreTS, and CrossGNN. Furthermore, FBM is evaluated w.r.t. its three variants: FBM-L, FBM-NL, and FBM-NP. The mean squared error (MSE) and mean absolute error (MAE) are used as evaluation metrics. The look-back window is set to \(336\), and the forecasting horizons are set to \(96,192,336,720\). In Appendix A.3, we also show that a look-back window of \(336\) is always associated with better results than that of \(96\) for most baseline methods. Eight datasets are used for the evaluation: ETTh1, ETTh2, ETTm1, ETTm2, Electricity, Traffic, Weather, and Exchange. In addition, we also evaluate the STSF of four PEMS datasets: PEMS03, PEMS04, PEMS07, PEMS08. More details about these baselines and datasets can be found in Appendix A.1. We split the ETT dataset into 12/4/4 months and the other datasets into training, validation, and test sets by the ratio of 6.5/1.5/2. Here, we use a smaller training set and a larger validation set compared to baseline methods to prevent early stopping and ensure a fairer comparison. With time-frequency features, the model can achieve state-of-the-art performance for both LTSF and STSF most of the time using either one linear layer or three non-linear layers.

### Effects of FBM mechanisms for LTSF

We evaluate the forecasting performance of three FBM variants against eight diversified baselines incorporated by different architectures: Linear, MLP, Transformer, and Fourier-based architectures. Table 1 shows the quantitative results on eight datasets. While preserving a simple structure, FBM variants achieve the best performance on most datasets across most prediction horizons.

Figure 3: Architecture of Fourier Basis Mapping (FBM): with three feature-output mapping methods, denoted as L - vanilla linear network, NL - nonlinear MLP, and NP - nonlinear PatchTST.

**FBM-L vs MLP-based network and FBM-NP VS Transformer-based network**: FBM-NL performs better than TimeMixer and FBM-NP performs better than PatchTST at most of the time, where the former two are both MLP-based networks and the latter two are both Transformer-based networks with patching. The experimental results further demonstrate the effectiveness of Fourier basis expansion, suggesting that extracting time-frequency features efficiently is more advantageous than relying solely on deeper architectures. It is worth noting that TimeMixer also decomposes the original time series into trend and seasonal effects, but its performance largely depends on the choice of the moving average kernel size. In contrast, our Fourier basis expansion automatically decomposes various effects hierarchically within distinct frequency levels. Although FBM-NP uses fewer patches than PatchTST, it still achieves better performance. Our FBM variants also consistently outperform other Transformer-based and MLP-based architectures, including iTransformer and N-BEATS.

**FBM variants vs. Fourier-based network**: We also observe that FBM variants make a significant improvement over all Fourier-based methods, including N-BEATS, FreTS, CrossGNN, and FITS. This discrepancy may largely be attributed to the inconsistent starting cycles and series length issues discussed in Section 3. Notably, FITS and CrossGNN also emphasize the importance of the amplitude of real and imaginary values, similar to FBM. However, they overlook the fact that Fourier basis functions are time-dependent when the input length is not divisible by a certain frequency level. Consequently, mapping in the frequency space cannot capture the time-frequency relationship effectively. For instance, CrossGNN retains only the values from the first cycle but fails to account for variations in the subsequent cycles as well as shifts in the phase of the cycles. In contrast, FBM

   Method &  & FBM-NL & FBM-NP &  &  &  &  &  &  &  &  \\  Error & 385.4 & 384.4 & 386.6 & 387.4 & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE & MAE \\   & 96.1 & 0.66 & 0.66 & 0.66 & 0.59 & 0.52 & 0.50 & 0.50 & 0.51 & 0.41 & 0.43 & 0.59 & 0.59 & 0.41 & 0.53 & 0.48 & 0.57 & 0.41 & 0.56 & 0.40 & 0.58 & 0.52 & 0.54 & 0.42 \\  & 192.4 & 0.49 & 0.41 & 0.48 & 0.43 & 0.47 & 0.46 & 0.42 & 0.45 & 0.47 & 0.42 & 0.46 & 0.40 & 0.42 & 0.42 & 0.43 & 0.43 & 0.49 & 0.47 & 0.64 & 0.61 & 0.40 \\  & 370.6 & 0.49 & 0.43 & 0.42 & 0.50 & 0.53 & 0.48 & 0.45 & 0.47 & 0.44 & 0.48 & 0.46 & 0.41 & 0.48 & 0.46 & 0.44 & 0.47 & 0.49 & 0.42 & 0.45 & 0.49 & 0.48 & 0.40 \\  & 70.4 & 0.44 & 0.53 & 0.46 & 0.44 & 0.49 & 0.49 & 0.47 & 0.44 & 0.48 & 0.42 & 0.52 & 0.58 & 0.47 & 0.42 & 0.46 & 0.47 & 0.46 & 0.41 & 0.48 & 0.56 & 0.53 \\   & 90.1 & 0.51 & 0.52 & 0.387 & 0.34 & 0.50 & 0.50 & 0.52 & 0.52 & 0.50 & 0.51 & 0.53 & 0.53 & 0.54 & 0.52 & 0.56 & 0.53 & 0.50 & 0.50 & 0.50 & 0.54 & 0.52 & 0.58 & 0.52 & 0.57 & 0.58 \\  & 192.7 & 0.57 & 0.57 & 0.54 & 0.56 & 0.52 & 0.50 & 0.50 & 0.50 & 0.57 & 0.51 & 0.53 & 0.52 & 0.50 & 0.51 & 0.54 & 0.54 & 0.52 & 0.57 & 0.55 & 0.52 & 0.52 & 0.58 \\  & 20.3 & 0.51 & 0.52 & 0.52 & 0.52 & 0.50 & 0.50 & 0.50 & 0.50 & 0.52 & 0.50 & 0.51 & 0.52 & 0.50 & 0.54 & 0.54 & 0.54 & 0.54 & 0.52 & 0.52 & 0.57 & 0.59 & 0.57 \\  & 720.0 & 0.59 & 0.41 & 0.57 & 0.52 & 0.50 & 0.50 & 0.50 & 0.56 & 0.57 & 0.53 & 0.52 & 0.50 & 0.50 & 0.53 & 0.52 & 0.50 & 0.54 & 0.56 & 0.52 & 0.56 & 0.57 & 0.57 \\   & 90.1 & 0.54 & 0.53 & 0.50 & 0.56 & 0.59 & 0.52 & 0.50 & 0.50 & 0.50 & 0.50 & 0.52 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.53 & 0.57 & 0.53 & 0.57 \\  & 370.1 & 0.57 & 0.54 & 0.54 & 0.54 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 & 0.50 &variants demonstrate a better representation of time-frequency domain features, as evidenced by their superior performance. In the following sections, we empirically discuss the reasons for their effectiveness through visualization.

Although our model primarily focuses on addressing LTSF challenges, we also evaluate the performance of STSF on PEMS dataset in Table 2, under the same settings as previous baseline methods. Our FBM variants consistently achieve the best performance in most cases.

### Effect of weight on time-frequency features

In Figure 4, we visualize the effect of the weights of the one-layer vanilla linear network to show how FBM-L considers the time-frequency relationship. We decompose the weights \(\) into \(L\) time-specific parts, denoted as \(=[_{0},_{1},,_{L-1}]\), where \(_{i}\) represents the impact of the time-frequency features on the \(i\)-th time step of \(}\). We flatten the weights into an array similar to \(\) in Figure 3, where the horizontal axis represents time, and the vertical axis represents frequency. Specifically, we visualize the heat maps of \(_{0}\) for the ETHh1, Electricity, and Traffic datasets, comparing them with the Fourier basis. Since the datasets Electricity and Traffic are more stable than ETTh1, the weights of the linear layer for the former datasets are closer to the Fourier basis than those for the latter datasets. This implies that FBM-L considers more time-frequency relationships in ETTh1 than in Electricity and Traffic, leading to more significant improvement. The further visualization of the weights \(_{0}\), \(_{10}\), \(_{20}\), \(_{30}\), \(_{40}\), and \(_{50}\) for Electricity is provided in Figure 4 in Appendix C. It is not surprising to see that the heatmap shifts over time. In conclusion, our approach allows the model to eliminate noises across a mixture of time and frequency domains and consider time-frequency relationships. In contrast, existing studies only eliminate noises in the frequency domain.

### Linear networks vs non-linear networks

We compare the performance of three FBM-variants on eight datasets, as shown in Table 1. FBP-NL and FBM-NP perform similarly across the datasets, as both are nonlinear. In contrast, FBM-L and FBM-NL reveal great performance differences on distinct datasets, which may be attributed to their varied data characteristics. To further explain such differences, Figure 5 shows the visualization of the frequency spectrum for a random input on the last seven dimensions across eight datasets. It shows that trending effects usually appear on high-frequency levels, seasonal effects usually fall on intermediate frequency levels, and noise signals typically manifest on low-frequency levels. This explains why the Fourier transform is beneficial for LTSF, as it hierarchically separates various effects, with specific effects (e.g., hourly, daily, weekly) aligning with their corresponding frequency levels. For instance, there is consistently high energy at the multiples of 14 on all hourly-level data ETTh1, ETTh2, Electricity, and Traffic in Figure 5. This is attributed to the day effect falling into these frequency levels, given the repeating cycle of 24 with a look-back window of 336.

By examining the frequency spectrums of all hourly-level datasets, we also find that ETTh1 and ETTh2 display a much richer frequency spectrum compared to other datasets. Figures 8 and 9 in

  Method & FBM-L & FBM-NL & FBM-NP & NL-linear & P-netJST & TimeMiser &  &  &  \\  Error & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MSE & MAE & MAP & MAE & MSE & MAE \\  PEMS04 & 0.088 & 0.195 & 0.071 & 0.172 & 0.071 & 0.170 & 0.089 & 0.196 & 0.075 & 0.179 & 0.071 & 0.173 & 0.073 & 0.176 & 0.118 & 0.237 & 0.091 & 0.196 \\ PEMS08 & 0.081 & 0.189 & 0.060 & 0.159 & 0.061 & 0.152 & 0.081 & 0.190 & 0.062 & 0.165 & 0.060 & 0.160 & 0.062 & 0.165 & 0.108 & 0.226 & 0.094 & 0.204 \\ PEMS03 & 0.077 & 0.183 & 0.086 & 0.164 & 0.061 & 0.162 & 0.078 & 0.184 & 0.065 & 0.174 & 0.062 & 0.164 & 0.061 & 0.163 & 0.108 & 0.223 & 0.089 & 0.199 \\ PEMS07 & 0.073 & 0.180 & 0.052 & 0.148 & 0.054 & 0.150 & 0.073 & 0.180 & 0.057 & 0.164 & 0.053 & 0.151 & 0.053 & 0.148 & 0.101 & 0.221 & 0.079 & 0.182 \\  

Table 2: Short Term Time Series Forecasting on Dataset PEMS with Forecast Horizon \(L=12\).

Figure 4: Effect of Weight \(_{0}\) on Datasets ETTh1, Electricity, and Traffic.

Appendix C further show that the frequency spectrums of Electricity and Traffic are more stable than those of ETTh1 and ETTh2. This suggests that there is a larger proportion of noise signals in datasets ETTh1 and ETTh2, but less in datasets Electricity and Traffic. The model is more likely to overfit when the data is mixed with a significant amount of noise. This is why FBM-L achieves better performance on datasets ETTh1 and ETTh2, while FBM-NL performs better on datasets Electricity and Traffic. On the other hand, the frequency spectrums for ETTm1 and ETTm2 shift to the left, contrasting with those for ETTh1 and ETTh2. This is because the granularity of ETTh1 and ETTh2 is four times larger than that of ETTm1 and ETTm2. This leads to the actual forecast horizon of the time being reduced, resulting in less noise in the data. This explains why FBM-NL performs slightly better than FBM-L on ETTm1 when the granularity changes. This also applies to datasets with other levels of granularity, such as the Exchange and Weather data. This observation validates our original hypothesis that providing the frequency spectrum is insufficient, as the real-world frequency can be ambiguous for the model when changes in granularity or series length alter the meanings of the spectrum. In conclusion, when a dataset consists of a large proportion of noise signals with a limited amount of data, a simpler network is preferred, and vice versa.

## 6 Conclusion

We make the first attempt to theoretically and empirically discuss several issues associated with the existing research on the deep Fourier transform for LTSF from the perspective of basis functions. Our insights and findings reveal that the real and imaginary parts of the frequency spectrum can be interpreted as the coefficients of cosine and sine basis functions, disclosing two issues commonly appearing in existing studies: inconsistent starting cycles and inconsistent series length issues. We further propose a Fourier basis mapping model Fourier basis mapping (FBM) to address these issues by extracting more efficient time-frequency features that carry explicit frequency information while retaining temporal patterns. Extensive experiments empirically verify the effectiveness of the FBM approach, to enhance various types of mapping networks and achieve state-of-the-art performance with simple one or three layers. The visualization analysis also reveals that deep neural mapping can replace the effect of inverse discrete Fourier transform and allow for more flexible consideration of time-frequency relationships. The ablation study further shows that extracting time-frequency features is more valuable than simply increasing the depth of the network. The primary focus of this study is to refine Fourier-based methods in a way that is more reasonable and interpretable. The time-frequency features hold great potential for future time series analysis tasks by enhancing various types of mapping networks (e.g., linear, MLP, Transformer networks). For example, Fourier basis expansion can enhance the performance of Transformer-based networks by only adjusting the initial projection layer after Fourier basis expansion. In future, we will also consider applying Fourier basis expansion to CNN- and RNN-based networks, as well as to tasks such as anomaly detection.

Figure 5: Frequency Spectrum of A Random Input of Last Seven Dimensions on Eight Datasets.