# UQ-Guided Hyperparameter Optimization for Iterative Learners

Jiesong Liu\({}^{}\), Feng Zhang\({}^{}\), Jiawei Guan\({}^{}\), Xipeng Shen\({}^{}\),

\({}^{}\)Department of Computer Science, North Carolina State University

\({}^{}\)School of Information, Renmin University of China

jliu93@ncsu.edu, guanjw@ruc.edu.cn, fengzhang@ruc.edu.cn, xshen5@ncsu.edu

###### Abstract

Hyperparameter Optimization (HPO) plays a pivotal role in unleashing the potential of iterative machine learning models. This paper addresses a crucial aspect that has largely been overlooked in HPO: the impact of uncertainty in ML model training. The paper introduces the concept of _uncertainty-aware HPO_ and presents a novel approach called the _UQ-guided scheme_ for quantifying uncertainty. This scheme offers a principled and versatile method to empower HPO techniques in handling model uncertainty during their exploration of the candidate space. By constructing a probabilistic model and implementing probability-driven candidate selection and budget allocation, this approach enhances the quality of the resulting model hyperparameters. It achieves a notable performance improvement of over 50% in terms of accuracy regret and exploration time.

## 1 Introduction

Hyperparameter optimization (HPO) is essential for unleashing the power of iterative machine learning models . Hyperparameters include traditional parameters like learning rates and more complex ones like neural architectures and data augmentation policies. For iterative learners (multi-fidelity and early stopping methods specifically), practitioners can obtain intermediate validation loss after each iteration and use them for model assessment; the main goal of HPO is to explore a vast candidate space to find candidates that lead to optimal model performance.

There are many designs in the literature to solve the HPO problem. Successive Halving (SH) , for example, terminates training of candidate configurations with poor performance early so as to save computing resources for more well-behaved candidates. Bayesian optimization , another optimization method, uses a surrogate model to guide the selection of candidate configurations for assessment.

There is however a lack of systematic treatment of an important factor in HPO designs, the uncertainty inherent in the dynamics of the training process of iterative machine learning applications. Because of the uncertainty, a model with a candidate hyperparameter configuration (or candidate in short) performing poorly in an early stage of its training could turn out to be the best model after convergence. Such candidates are however likely to be stopped from proceeding further or be completely discarded by existing HPO methods in the early stages, because their selections of candidates are mostly based on the currently observed performance, for lack of a way to treat the uncertainty properly. In 100 experiments of Successive Halving, for instance, the actually best candidates were discarded in the first 8-22 iterations of training, causing 48% performance regrets in validation loss (details in Section 3.1 Figure 1 and Section 4 Figure 3).

This paper introduces model uncertainty into the design of HPO methods for iterative learners and establishes the concept of _uncertainty-aware HPO_. At the core of uncertainty-aware HPO is a novel uncertainty quantization (UQ) guided scheme, named _UQ-guided scheme_, which unifies the selection of candidates and the scheduling of training budget--two most important operations in HPO--into a single UQ-based formal decision process. The UQ-guided scheme builds on a probabilistic uncertainty-based model, designed to approximate the statistical effects of discardinga set of candidates at the end of a step in HPO. It uses a lightweight method to efficiently quantify model uncertainty on the fly. It offers a principled, efficient way for HPO to treat model training uncertainty.

As a general scheme, the _UQ-guided scheme_ can be integrated into a variety of HPO methods for iterative learners, especially DNNs. This paper demonstrates its usefulness and generality for DNNs by integrating it into four existing HPO methods. Experiments on two widely used HPO benchmarks, NAS-BENCH-201  and LCBench , show that the enhanced methods produce models that have 21-55% regret reduction over the models from the original methods at the same exploration cost. And those enhanced methods need only 30-75% time to produce models with accuracy comparable to those by the original HPO methods. The paper further gives a theoretical analysis of the impact of the _UQ-guided scheme_ for HPO.

## 2 Background and Related Work

Many studies are committed to solving the HPO problem for iterative learners efficiently [26; 20; 29; 39; 41]. Bayesian optimization, early stop-based mechanisms, and multi-fidelity optimizations are some important approaches.

**Bayesian Optimization (BO)**. BO is a sequential design strategy used to optimize black-box functions [34; 17; 11]. In HPO scenarios, it can be used as a surrogate model to sample high-quality candidates.

**Early Stop Mechanisms.** Early stop-based approaches can be effective since they evaluate different candidates during training and make adaptive selections accordingly [2; 8; 35]. The early stopping mechanism, which stops the training of poorly-performed candidates early, has been widely employed in the HPO community including Successive Halving (SH)  and Hyperband (HB) ; BOHB  combines both BO and HB methods to take advantage of both the BO surrogate model and the early stopping mechanism.

**Multi-fidelity Optimizations.** Multi-fidelity evaluation focuses on using low-fidelity results trained with small resources to accelerate the evaluation of candidates [2; 3; 8; 22; 23; 38; 21; 35; 36; 14; 26]. Sub-sampling (SS)  is proposed mainly using multi-fidelity methods to collect high-quality data to select good configurations without early stopping.

**Model Uncertainty in HPO.** Various optimization methods in HPO scenarios focus on specific training metrics to assess candidate performance. However, these methods typically overlook the uncertainty in the candidate selection process. Machine learning models inherently have approximation uncertainties [5; 6; 10; 12; 24; 28; 31]. Some HPO designs sample the candidate space based on distributions on the effect of each hyperparameter dimension on the quality of the candidates, but without considering the uncertainty in the model training process. For example, one of the studies  separates candidates into "good" or "bad" groups in order to build the distributions. The separation is based on the same deterministic metrics as other HPO methods use, giving no consideration of the uncertainty in model trainings. The only work we find that considers uncertainty in training metrics  selects configurations for further training based on its assessment of the upperbound of those configurations. In each round, the configurations it chooses are those that, considering the best possible performance at the last iteration, show a smaller validation loss than the validation loss current best configuration shows. The selection treats model uncertainty preliminarily and does not use it to guide the allocation of training budget. We compare other related works in Appendix E.

## 3 Uncertainty Quantification (UQ)-Guided Hyperparameter Optimization

This section gives an exploration of model uncertainty, introduces _UQ-guided scheme_ for incorporating UQ into the design of HPO, discusses examples of ways to use the UQ-guided scheme to enhance existing HPO methods, and theoretically analyzes its effects.

### Uncertainty in Iterative Machine Learning

Uncertainty in iterative machine learning originates mainly from two factors: inherent noise in the data and the variability of model predictions due to restricted knowledge [16; 1]. Since data uncertainty is constant, it is the variability in model predictions, referred to as model uncertainty, that primarily influences decisions on HPO. To estimate the model uncertainty, we can incur small perturbations to the model, evaluate these model variants, and calculate the variance of the results as the approximation for the model uncertainty .

Figure 1 shows how the model uncertainty affects the quality of the returned candidate. In a given SH run, half of the candidates are eliminated at each checkpoint marked by a vertical red dashed line. The solid blue line represents the best validation loss up to the current point, while the orange dashed line signifies the true quality (in terms of validation loss after convergence) of the candidates SH retains at that specific juncture. From the figure, we see that in every round, SH discards the actually best candidates, causing a continuous increase of the regret. The reason is that the discarding decision of SH is solely based on the current validation loss, but model uncertainty, particularly pronounced in the early stages, obfuscating the true model potential.

### Quantify Uncertainty and the Impact

We explain how we quantify model uncertainty, and how, based on it, at any point of time, estimate the performance and uncertainty of a candidate model when its training converges.

High efficiency is essential here as the UQ happens during the HPO process. We employ a lightweight approach to conduct the UQ efficiently on the fly, as explained next.

Let \(_{1},_{2},,_{K}\) be \(K\) candidates drawn from the hyperparameter space \(\). Consider a supervised learning setting, where a machine learning model \(M\) is trained on some training set \(D_{Train}=\{(_{1},y_{1}),(_{2},y_{2}),,( _{n_{train}},y_{n_{train}})\}\). Let \(M^{t}_{}\) denote the model with hyperparameter \(\) trained on \(D_{Train}\) after \(t\) epochs, and \(M^{*}_{}\) the converged model. \(M^{t}_{}()\) gives the prediction on a given input \(^{d}\).

We use \((,)\) to denote the metric that evaluates the performance of a candidate model. Given a hyperparameter configuration \(_{c}\), the validation loss of a model instance, \((,M^{*}_{_{c}}())\), can be affected by training data and other factors, and hence has some uncertainty. Let \((_{c}*,^{2}_{c}*)\) represent the distribution of \((,M^{*}_{_{c}}())\), where, \(^{2}_{c}*\) embodies the uncertainty.

Our objective here is a way that can, at any point in the HPO process, estimate \(^{*2}_{c}\) and \(^{*}_{c}\) for a candidate model configured with \(_{c}\). Our solution leverages the validation history in the HPO process, \((,M^{1}_{_{c}}()),,(,M^{t} _{_{c}}())\), to construct an estimated loss curve, explained as follows.

Decomposition of momentum and the underlying structure of the metric.This part uses breakdowns to characterize the loss curve and introduces the objective function we want to minimize to estimate the curve parameters. The first component, referred to as momentum, models the decaying trend of the loss curve. The second component is the bias term for each candidate's loss curve; it models a latent effect underlying the hyperparameter space by allowing correlation among the candidates. The details are as follows.

Typically a candidate in HPO can be represented as a vector. We use a set \(_{r}^{r}\) to represent the candidates \(\{_{i},i[n]\}\); \(r>0\) is the vector length. For modeling the loss curve, we set \(_{t}^{L}\) as a vector whose elements are functions of training epochs \(t=1,2,\). (In our experiments, we set \(_{t}=[t^{-1/2},t^{-1}]\), for the general decreasing trend of loss curves as training epochs increase.) We model metric \(^{}_{t}\) of candidate \(_{r}\) at time \(t\) as the summation of the impact from three sources:

\[^{}_{t}=(,M^{t}_{_{c}}())=_{t}^{}^{}+^{}+ ^{}_{t},\] (3.1)

where \(^{}\) and \(\) are the parameter vectors to determine. The three components in Equation 3.1 are (1) the momentum part (\(_{t}^{}^{}\)) that denotes the contribution from the trends in the loss curve of the training specific candidate model variant, (2) the contribution from the underlying model structure (\(^{}\)), and (3) the noises by other elements \(^{}_{t}\), which is asssumed to follow a Gaussian distribution \((0,^{2}_{t})\) independent of each other and of \(\).

For a candidate \(_{r}\), let \(^{}=^{}\). The loss curve parameters \(\{^{},^{}\}\) can be determined by optimizing a weighted least squares objective

Figure 1: Demonstration of the negative impact from uncertainty on HPO; Successive Halving (SH) is used; the benchmark is NAS-BENCH-2.0 . More detailed are in Section 4.1 and Appendix F. Due to its overlooking at model uncertainty, at each halving point, SH discards the actual best candidates, causing an increase in the regret.

\[=_{^{u},^{u}}_{t=1}^{T}_{j=1}^{F_{k}}w_{jt}( _{jt}^{}-^{}-_{t}^{}^{ })^{2}\] (3.2)

where \(w_{jt}=_{t}^{2}}\). \(F_{t}\) is the number of models trained with time \(t\).

Solving for the Momentum Mean and Variance.This part analyzes the mean and variance of the loss curve we constructed and makes inferences to the converged loss based on the loss curve. The quantified uncertainty of the estimated converged loss is then used for model selection and budget resource allocation. The specifics are as follows.

For a candidate \(i\) at iteration \(T\), concatenating the validation losses across training epochs (indexed by \(t\)) will lead to a validation loss vector \(\) of dimension \(D=_{t=1}^{T}F_{t}\). For each \(d\{1,,D\}\), the \(d^{th}\) element in \(\) is an observation of loss at time \(t_{d}\) that follows \((_{i_{t_{d}}},_{t_{d}}^{2})\) with \(t_{d}\) mapping \(d\) to its corresponding epoch \(t\).

For a given candidate \(\) (for better readability, we omit the superscript \(\) in the notations in the following discussion), the weighted least squares problem can be formulated as solving the equation \(^{}=^{}\) for \(\) with \(^{D D}\) being a diagonal matrix of weights \(_{dd}=}_{t_{d}}^{2}}\).

\(=[]\), \(^{D(L+1)}\) with \([d,:]=_{t_{d}}^{}\), and \(^{}=[\;^{}]\). The empirical estimate of \(_{i}^{2}\) is computed as the variance of the loss in the recent several epochs of \(i\)--those instances can be regarded as results of small perturbations to the model at epoch \(i\).

Solving the weighted least square objective, we have the estimator as \(}=(^{})^{-1}(^{ })\). The covariance of the estimator is given by \(_{}}=(^{})^{-1}\).

Since the estimated curve is given by \((t)=[1_{t}^{}]}\), the variance of this estimation is given by

\[^{2}(t)=[1_{t}^{}]_{}} 1\\ _{t}.\]

With the formulas for \((t)\) and \(^{2}(t)\), we can then approximate the distribution of \((,M_{_{i}}^{*}())\) as \(((N),^{2}(N))\) for a large \(N\) value (\(N=200\) in our experiments).

Algorithm.Based on the analysis, we devise an iterative algorithm to compute the estimated loss and variance. Without the loss of generality, we make the following assumption.

**Assumption 1**.: _There exist positive constants \(\) such that for any \(r\), \(_{_{r}}\|\|,\) and the set of candidates \(_{r}^{r}\) has \(r\) linearly independent elements \(_{1},,_{r}\)._

The algorithm goes as follows. At the beginning of HPO, it sets \(\) with a random vector. At the end of each epoch, it conducts the following two operations. First, it solves the weighted least squares objective 3.2 for each of the \(r\) candidates (mentioned in Assumption 1) by following the formulas described earlier in this section, with the current \(\) value being used. Second, for \(p=1,2, r\), it observes the metrics \(X^{_{p}}(t)=_{t}^{_{p}}-_{t}^{}}^{_{p}}\) and refines the ordinary least square estimate for \(\) as follows:

\[}=(_{p=1}^{r}_{p}_{p}^{} )^{-1}_{p=1}^{r}_{p}X^{_{p}}\]

At the end of an HPO round (e.g., at the halving time in SH), it performs the first step for every candidate model to compute the estimated distributions of their \((,M_{_{i}}^{*}())\), so that the HPO can use the estimates to select promising candidates to continue in the next round.

We next show how to use the approximated loss and uncertainty to compare two candidates:

**Definition 1** (UQ-guided comparison of candidates).: _UQ-guided comparison of candidates compares two candidates based on the probability that the validation loss of the converged model \(_{c_{1}}\) is lower than that of \(_{c_{2}}\), represented as follows based on the approximation from the current validation losses and uncertainty of the two candidates:_

\[P=(,M_{_{c_{1}}}^{*}())>(,M_{_{c_{2}}}^{*}())(,M_{_{c_{1}}}^ {t=1,2,}()),_{c_{1}},(,M_{_{c_ {2}}}^{t=1,2,}()),_{c_{2}}}.\] (3.3)The main idea of Definition 1 is to use the current validation loss history and quantified uncertainty to approximate the converged validation loss, so that we compare two candidates - more precisely, we compute the probability that one candidate is better than the other - based on the probability distribution of their converged validation loss. For example, if the approximated loss and uncertainty of two candidates \(_{j}\) and \(_{k}\), at epoch \(t\), are \((_{j},_{j})\) and \((_{k},_{k})\), using converged validation loss as the metric, we have \(((,M^{t}_{_{j}}())>(,M^{t}_{ _{k}}()))\)\(=-_{j}}{^{2}+_{k}^{2}}}\), where \(\) denotes the cumulative distribution function (CDF) of the standard normal distribution.

We next present _UQ-guided scheme_, a principled way to use UQ to guide HPO.

### UQ-Guided Scheme

Figure 2 illustrates how UQ-guided scheme works in HPO. For the purpose of clarity, we base our explanation of the scheme on HPO that uses early stop mechanisms, but will show in Section 3.5 that the scheme is general, applicable to other HPO methods for iterative learners as well.

The original HPO method, Successive Halving , evaluates and eliminates candidates over multiple rounds. At the end of each round, it drops those candidates regarded as unpromising. With our UQ-guided scheme, at the end of each round, the scheme derives a _confidence curve_ from the current probabilistic model, and uses a _discarding mechanism_ to drop candidates that are unlikely to perform well after convergence. In contrast to the original HPO that drops a fixed amount (or fraction) of candidates in each round, the UQ-guided scheme carefully calculates the number of candidates to drop in a round based on the probabilistic model such that the expected quality of the HPO outcomes can be maximized, as explained later.

The UQ-guided scheme respects the HPO budget--that is, the total amount of time usable by the HPO for identifying the best candidate. By default, it works around the given budget constraint: the budget for each round (\(R\)) equals the total budget divided by the number of rounds. We next discuss each step.

#### 3.3.1 Confidence Curve Derived from Uncertainty Quantification

The concept of _confidence curve_ is central in UQ-guided HPO. Define \([n]=\{1,2,,n\}\).

Figure 2: Illustration of using UQ-guided scheme to enhance Successive Halving. The goal is to select an optimal hyperparameter configuration from \(K\) candidates. It involves multiple rounds. \(R\) is the predefined budget resources (e.g., training epochs) for each round. For the first round, \(K\) candidates each get trained for \(\) epochs. Based on the observed validation loss and the quantified uncertainty for each candidate, our method represents each candidateâ€™s converged loss with a probability distribution. From that, it constructs a _confidence curve_, capturing the probability that the best configuration is among the current top \(k\) candidates for \(1 k K\). From the curve, it then calculates \(f((_{i},_{i}^{2})_{i=1}^{K},k)\), which captures the effects of keeping \(k\) top candidates (\(1 k K\)) for the next round, by considering the tradeoff between the risks of discarding the best candidate and the training budget each top candidate can get. From that, it identifies the best \(k\) value, discards the least promising \(K-k\) candidates, and enters the next round. The process continues until the total budget is used up.

**Definition 2** (Confidence curve).: _At epoch \(t\), we evaluate each candidate's performance and sort them based on validation loss. A confidence curve\(\) is a trajectory of a series of probabilities, \(\{P_{k}|k[n]\}\), that depicts the probability that the optimal configuration (with the lowest loss after convergence) is among the first \(k\) configurations. For \(k[n]\), \(P_{k}\) can be expressed as_

\[P_{k}=((,M^{*}_{_{1}}()),, (,M^{*}_{_{k}}()))((,M^{*} _{_{k+1}}()),,(,M^{*}_{_{n}}()))}.\]

The _confidence curve_ is derived based on joint probability distribution in the following way. Suppose there are \(n\) candidates. At the end of a certain round, the probabilistic model returns \(n\) pairs of \((_{1},_{1}),(_{2},_{2}),,(_{n},_{n})\) as estimations for \((,M^{*}_{_{1}}())\), \((,M^{*}_{_{2}}())\), \(\), \((,M^{*}_{_{n}}())\). For simplicity, assume that \(_{1}<_{2}<<_{n}\), and \(_{1}=_{2}==\).

Let \(\) and \(\) be the CDF and PDF of the standard normal distribution. For \(k=m\), we can calculate

\[P_{m}=_{-}^{}_{i=1}^{m}}{})}{(}{})}_{i=1}^{n}( }{})dy.\] (3.4)

The details of obtaining Equation 3.4 are in Appendix A.1.

#### 3.3.2 Discarding Mechanism

The next step is to decide, at the end of each round, the appropriate value of \(k\), which determines how many \((n-k)\) lowest-ranked candidates will be discarded in this round. Our scheme decides \(k\) based on the confidence curve: choosing the smallest \(k\) that satisfies \(P_{k}\), where \(\) is a parameter determined by our scheme adaptively as follows.

**Choosing \(\)**. At the end of round \(i\), we have the _confidence curve_\(_{i}(P_{1}^{i},P_{2}^{i},,P_{n}^{i})\) that is the trajectory of a series of probabilities. We quantify how \(\) influences the probability for the HPO to select the best candidate.

Let \(_{i}\) be the value of \(\) for round \(i\), \(k_{i}\) be \(\{k:_{i}(P_{k})_{i}\}\). As the scheme discards the worst \(n-k_{i}\) candidates and further trains the best \(k_{i}\) candidates in round \(i+1\), we can derive the _confidence curve_ of round \(i+1\) as \(_{i+1}(P_{1}^{i+1},P_{2}^{i+1},,P_{k_{i}}^{i+1})\) based on those selected \(k_{i}\) candidates. Since we want to quantify the effect of \(\) on the probability that the scheme returns the best candidate (that is, to suppose round \(i+1\) is our final round), \(P_{1}^{i+1}\) is the target we desire to maximize. Define \(_{i}\) to be the current condition \((_{i},_{i})_{i=1}^{n}\). Let \(f(,)\) be a mapping such that \(f(_{i},_{i})=P_{1}^{i+1}\). We want to use a selector function \(:D\) where \(D=([0,)[0,))^{n}\). \(\) takes \(_{i}\) as input and returns an optimal \(_{i}\):

\[(_{i})=_{_{i}}\{f(_{i },_{i})\}.\] (3.5)

The effect of \(_{i}\) on \(f\) manifests through its influence on the number of candidates \(k_{i}\) retained in the subsequent round, and can be ultimately broken down into the influence of (1) _exploration_, meaning keeping more candidates in the next round can reduce this round's discarding error, and (2) _exploitation_, meaning keeping fewer candidates in the next round can allow each candidate to receive more training time (recall that the training time budget is fixed for each round) and hence will increase the reliability of the validation at the end of the next round.

**Exploration.** If \(k_{i}\) drops by 1 to \(k^{}_{i}\), according to the definition of the _confidence curve_, the probability that the final optimal configuration is among the remaining candidates we keep drops by \( c_{}=P_{k_{i}}-P_{k^{}_{i}}\):

\[ c_{}=_{-}^{}}}{})}{(}}{})} _{i=1}^{n}(}{})dy.\] (3.6)

**Exploitation.** At the same time, a drop in \(k_{i}\) leads to an increase in the individual training budget \(b\). Let \(\) be the coefficient that relates the increase in the number of training epochs to its corresponding effect on confidence. Using an approach similar to that employed in formulating the _confidence curve_,

\[=_{-}^{}}(}{-_{t}})_{i=2}^{k_{i}}(}{ -_{t}})dy-_{-}^{}(}{})_{i=2}^{k_{i}}(}{})dy\] (3.7)

where \(t\) represents the current total number of epochs and \(_{t}\) represents the reduction in the uncertainty \(\) that would result from training each candidate for one additional epoch. The specifics for Equations 3.6 and 3.7 can be found in Appendix A.2. Given that \(b\) increases by \(_{i}}-}\), the overall influence of _exploitation_ on the probability of selecting an optimal candidate is \( c_{}=(k_{i}-1)}\).

Let \((k_{i},_{i})\) be the confidence increase, given condition \(_{i}\), when each of the \(k_{i}\) candidates gets a unit extra training budget. \(_{i}(P_{k}),k[n]\) are the _confidence curves_. Balancing _exploration_ and _exploitation_ leads to a sweet point where \( c_{}= c_{}\). That gives the way to derive the appropriate value for \(\), which just needs to make the following hold:

\[P_{k_{i}}-P_{k_{i}-1}=(k_{i}-1)}(k_{i},_{i}).\] (3.8)

### Theoretical Analysis

We consider how the method performs in terms of identifying the best candidate. For convenience, we let \(_{i,t}\) be the approximation of the converged loss for the model with hyperparameter \(_{i}\) at time \(t\). For each \(i\), assume \(_{i}=_{}_{i,}\) exists. The goal is to identify \(*{arg\,min}_{i}_{i}\). Without loss of generality, assume that \(_{1}<_{2}_{n}\). The assumption that \(_{}_{i,}\) exists implies that as \(\) grows, the overall gap between \(_{i,}\) and \(_{i}\) decreases. Let \(_{t}=f(t)\) be the model uncertainty at epoch \(t\). We then introduce a random variable that characterizes the approximation error of \(_{i,t}\) relative to \(_{i}\), modeling it as a distribution that incorporates \(t\) as a parameter:

\[X_{t}=_{i,t}-_{i},X_{t}(0,_{t}^{2}) t.\]

By applying Chebyshev inequality, we have

\[|_{i,t}-_{i}|>-_{1}}{2}}^{2}}{(_{i}-_{1})^{2}}=}{(_{i}-_{1})^{2} }\ \ i=2,,n.\] (3.9)

Let \(\) denote the event \(_{i,t}>_{1,t}\), then by Equation 3.9

\[()=(_{i,t}-_{i})+(_{1}-_{1,t})+2( -_{1}}{2})>0} 1-}{(_{i}- _{1})^{2}}^{2}.\] (3.10)

Equation 3.10 tells us that \(_{i,t}>_{1,t}\) has a high probability with respect to \(t\) if \(f(t) O(t^{-1/4})\) (see Lemma in Appendix C). That is, comparing the intermediate values at a certain time \(t\) is likely to establish an order similar to the order of the final values of \(_{i}\) and \(_{1}\).

The following theorem is stated using the abovementioned quantities with proofs in Appendix C.1.

**Theorem 1**.: _Let \(n\) be the number of total candidates, and \(_{i}=_{}_{i,}\). For a given \(c>0\), there exists a \(T>0\) s.t. \(_{i=2}^{n}(1-(}{(_{i}-_{1})^{2}})^{2})>1-c\). If the round budget \(R>T n\), then the best candidate is returned with probability \(P>(1- c)(1-c)\), where \(B\) is the total budget._

In comparison, the bound in the UQ-oblivious approach is as follows:

**Theorem 2**.: _Let \(>0,_{i}=_{}_{i,}\) and assume \(_{1}_{2}_{n}\). Let \(^{-1}(,)=\{t: ^{}\}\), and_

\[z_{ob} =2_{2}(n)_{i=2,,n}i(1+^{-1} (-_{1}}{2},))\] \[ 2_{2}(n)(n+_{i=2,,n}^{-1}( -_{1}}{2},)).\]

_If the UQ-oblivious early stopping method is run with any budget \(B_{ob}>z_{ob}\) then the best candidate is returned with probability \(P_{ob}>1-n\)._

**Example 3**.: _Consider \(f(t)=\). According to Theorem 2, if \(B_{ob}>2_{2}(n)(n+_{i=1,,n}^{-1}( -_{1}}{2},))\), the UQ-oblivious method can return the best candidate with probability over \(1-n\). But if \(B_{UQ}^{-1}(-_{1}}{2},) n\)1, the UQ method can return the best candidate with probability over \(1-n\). As shown in Appendix C.2, Theorems 1 and 2 together show that the UQ approach guarantees the same probability of identifying the optimal candidate as the UQ-oblivious counterpart with a smaller budget lowerbound \(B\) (see Corollary 6)._

### UQ-Guided HPO Family

The _UQ-guided scheme_ is a general approach to enhancing HPO with uncertainty awareness. We next explain how it is integrated into several existing HPO methods to transform them into UQ-guided ones, yielding a UQ-guided HPO family. In the following, we use the suffix "plus (+)" to indicate the UQ-guided HPO methods.

**Successive Halving plus (SH+)** is derived from the early stop-based HPO design Successive Halving (SH) . Algorithms 1 and 2 in Appendix A.3 show the pseudo-code. Given total budget \(B\) and round budget \(R\) and an initial \(K\), SH+ first trains \(K\) candidates each with the initial \(b=\) units of budget, and ranks them by the evaluation performance. Then SH+ updates \(K\) based on Section 3.3.2 and keeps the top \(K\) configurations according to the UQ-guided scheme (\(OracleModel\) in Algorithms 1 and 2), and continues the process until the budget runs out.

**Hyperband plus (HB+)** originates from the popular HPO design Hyperband (HB). HB is an HPO method trying to better balance exploration and exploitation than SH does  by adding an outer loop for grid search of the value of \(K\). HB+ simply extends HB by using SH+ rather than SH as its inner loop, changing the target of the grid search to the initial value of \(K\).

**Bayesian Optimization and Hyperband plus (BOHB+)** is developed from BOHB . BOHB is similar to HB except that it replaces the random sampling from the uniform distribution with BO-based sampling. BOHB+ makes the corresponding changes from HB+ by adopting BO-based sampling for its outer loop.

**Sub-sampling plus (SS+)** is derived from the Sub-sampling (SS) algorithm . It showcases the applicability of the UQ-guided scheme to non-early stop-based methods. Similar to other methods, in each round, SS also chooses candidates for further training based on its assessment of the potential of those candidates. But unlike the other methods, SS does not discard any candidates, but keeps all in play throughout the entire HPO process. In each round, the candidates it chooses are those that show smaller validation loss than the most trained candidate shows. If there is none, it trains only the most trained candidate in that round. SS+ integrates the UQ-guided scheme into the candidate selection process of SS. When SS+ compares a candidate (\(c_{i}\)) against the most trained candidate (\(c_{m}\)), rather than checking their validation losses, it uses the UQ-guided scheme to compute the probability for the convergence loss of \(c_{i}\) to be smaller than that of \(c_{m}\) and checks whether the probability is over a threshold \(\) (0.9 in our experiments), that is, \(((y,M^{*}_{_{c_{m}}}())(y,M^{*}_{_{c_{i}} }()))\).

## 4 Experiments

We conduct a series of experiments on the four UQ-guided HPO methods to validate the efficacy of the UQ-guided scheme for HPO.

### Experimental Setup

**Methodology.** To check the benefits of the UQ-guided scheme for HPO, we apply the proposed UQ-guided HPO family to different HPO benchmarks, including NAS-BENCH-201 and LCBench, each for 30 repetitions, to measure the performance for different hyperparameter optimization tasks, and compared those with their original UQ-oblivious versions.

**Platform.** Our experiments are conducted on a platform equipped with an Intel i9-9900k CPU and an NVIDIA GEFORCE RTX 2080 TI GPU. The CPU has 8 cores, each of which can support 2 threads. The GPU has 4,352 cores of Turing architecture with a computing capability of 7.5. The GPU can achieve a maximum memory bandwidth of 616 GB/s, 0.4 tera floating-point operations per second (TFLOPS) on double-precision, and 13 TFLOPS on single-precision.

**Workloads.** We evaluate the UQ-guided methods on two real-world benchmarks. Nas-Bench-201  (CC-BY 4.0) encompasses three heavyweight neural architecture search tasks (NAS) on CIFAR-10, CIFAR-100, and ImageNet-16-12 (CC-BY 4.0) datasets. In addition, we investigate the performance of optimizing traditional ML pipelines, hyperparameters, and neural architecture in LCBench . For example, we optimized 7 parameters for the Fashion-MNIST dataset , where the resource type is determined by the number of iterations. Additional information regarding these benchmarks can be found in Appendix F. In this context, one unit of budget equates to a single training epoch, and by default, the total HPO budget (\(B\)) allocated for each method is 4 hours.

### Experimental Results

Figure 3 illustrates the results of NAS-BENCH-201 trained on ImageNet-16-120. It shows the results of four UQ-guided methods compared to their original ones. For each comparison, we show threemetrics, namely top-1 rank on different trials, top-1 rank on different fractions of budgets, and regret on different fractions of budgets. The fraction of budgets denotes the portion of the budget that we allocate for that particular experiment compared to the standard full budget. Top-1 rank refers to the real ranking of the candidate ultimately chosen by the method. Regret (%) refers to the accuracy difference between the returned candidate and the real best candidate. In Figure 3, the average results of 30 repetitions are reported. For the right two columns, we also report the uncertainty bands, defined as the interval between the 30th and 70th percentiles. The benefits of the UQ-guided scheme are obvious, both for individual trials and across different fractions of budgets. It brings a 21-55% regret reduction. Similar results are observed on other benchmarks (LCBench results shown in Appendix G).

Table 1 provides the fraction of the total exploration time needed for the UQ-guided methods to achieve comparable model accuracy as the original methods do. The UQ-guided methods need much less time than their counterparts to obtain a similar performance. For instance, SH+ achieves the same average regret of 5% on NAS with only half of the budgets required by SH. These results indicate that the UQ technique can conduct HPO efficiently and effectively.

Our paper's experiments concentrate on DNN because efficient HPO is crucial for the time-consuming nature of DNN training. We also use the ridge regression in Section H as a demonstration to show the

   Methods & NAS-BENCH-201 & LCBench \\  SH+ & 50.78 & 43 \\ HB+ & 75 & 60 \\ BOHB+ & 68.4 & 53.34 \\ SS+ & 47.84 & 30.93 \\   

Table 1: Fraction of time (%) required for the UQ-guided methods to achieve comparable model performance as the original HPO methods do.

Figure 3: Experimental results of UQ-oblivious HPO methods and their UQ-guided enhancements on NAS-BENCH-2.0.

potential of the methodology for other iterative learners, and leave a systematic study of which to the future.

## 5 Conclusion

This paper points out the importance of systematic treatment to the uncertainty in model trainings for HPO. It introduces a novel scheme named _UQ-guided scheme_, which offers a general way to enhance HPO methods for DNNs with uncertainty awareness. Experiments demonstrate that the UQ-guided scheme can be easily integrated into various HPO methods. The enhanced methods achieve 21-55% reduction of regret over their original versions, and require only 30-75% time to identify a candidate with a matching performance as the original methods do. The paper in addition provides a theoretical analysis of the effects of the UQ-guided scheme for HPO.

The key characteristic of the UQ method is the necessity to rank multiple learners during the HPO process. Gradient-based HPO methods , for instance, may not benefit from our UQ-guided scheme because of their sequential properties. One limitation of this paper is that it is mostly suitable for iterative learners, and needs adaptations for other learners: To go beyond, it could be, for instance, applied to the model selection work in previous studies  that use training dataset size as the budget dimension. In this case, the learner does not need to be iterative; the selection is based on the validation loss history trained with incremental dataset sizes. The UQ component can still guide the configuration selection and budget allocation in the HPO process.

Overall, this study concludes that UQ is important for HPO to consider, simple on-the-fly UQ goes a long way for HPO, and the _UQ-guided scheme_ can serve as a general effective scheme for enhancing HPO designs.