# TuneTables: Context Optimization for Scalable

Prior-Data Fitted Networks

 Benjamin Feuer\({}^{1}\), Robin Tibor Schirrmeister\({}^{2}\), Valeria Cherepanova \({}^{3}\), Chinmay Hegde\({}^{1}\),

**Frank Hutter\({}^{2}\), Micah Goldblum\({}^{1}\), Niv Cohen\({}^{11}\), Colin White\({}^{ 4}\)**

\({}^{1}\) New York University, \({}^{2}\) University of Freiburg, \({}^{3}\) University of Maryland, \({}^{4}\) Abacus.AI

Equal advising. Correspondence to: bf996@nyu.edu.

###### Abstract

While tabular classification has traditionally relied on from-scratch training, a recent breakthrough called prior-data fitted networks (PFNs) challenges this approach. Similar to large language models, PFNs make use of pretraining and in-context learning to achieve strong performance on new tasks in a single forward pass. However, current PFNs have limitations that prohibit their widespread adoption. Notably, TabPFN achieves very strong performance on small tabular datasets but is not designed to make predictions for datasets of size larger than 1000. In this work, we overcome these limitations and substantially improve the performance of PFNs via context optimization. We introduce TuneTables, a parameter-efficient fine-tuning strategy for PFNs that compresses large datasets into a smaller learned context. We conduct extensive experiments on nineteen algorithms over 98 datasets and find that TuneTables achieves the best performance on average, outperforming boosted trees such as CatBoost, while optimizing fewer than 5% of TabPFN's parameters. Furthermore, we show that TuneTables can be used as an interpretability tool and can even be used to mitigate biases by optimizing a fairness objective. We open-source our code and raw results at https://github.com/penfever/TuneTables.

## 1 Introduction

Tabular data, or data organized into rows and columns consisting of distinct features, are the oldest and one of the most ubiquitous types of data in machine learning in practice [10; 67]. Tabular data has numerous applications across medicine [41; 71], online advertising [33; 52; 64], finance [7; 18], and other areas [11; 12; 72].

Competitive classification algorithms for tabular data include gradient-boosted decision trees [16; 61] and deep neural networks [31; 42; 69]. Both approaches fit their respective models on a labeled dataset containing samples from a distribution reflecting the task at hand. A recent breakthrough, prior-data fitted networks (PFNs) [35; 55], are are a specific type of neural process which learn to perform approximate Bayesian inference in a single forward pass using in-context learning . PFNs do not require optimizing parameters or fitting a model on downstream training data, instead feeding training data into the context and conditioning on it. In particular, TabPFN achieved state-of-the-art classification on small tabular datasets [35; 51].

The in-context learning approach of PFNs parallels that of large language models (LLMs) . Both approaches can be viewed as approximate Bayesian inference, whether implicitly  or explicitly . While researchers have successfully used various context optimization strategies for enhancing LLM performance , no prior work has studied context optimization strategies for PFNs. Furthermore, although TabPFN achieves very strong performance on small datasets, its limitationscurrently prohibit its widespread adoption: it only runs on datasets whose number of training samples, number of features, and number of classes are at most 1000, 100, and 10, respectively.

In this work, we perform the first investigation into context optimization strategies for PFNs, allowing us to substantially improve their performance when scaled to large datasets. Specifically, we introduce **TuneTables**, a novel parameter-efficient fine-tuning technique for PFNs that compresses large datasets into a smaller learned context (Figure 1). We conduct an extensive empirical investigation into TuneTables' performance on the TabZilla Benchmark Suite, the largest benchmark considered by recent tabular data literature . Over 98 datasets and 19 algorithms, we find that TuneTables achieves the best average performance and is the best-performing method on 30 of them.

Because TuneTables effectively compresses the contents of large tabular datasets into the tuned prompt, no training data is needed in the context during inference, significantly speeding up inference time (similar to works on neural processes; see Section 6). We also show that the learned prompt can be used as a tool for interpretability. Finally, we show how to use TuneTables for multi-objective optimization, such as optimizing both accuracy and fairness simultaneously, allowing users to mitigate biased predictions of a pretrained PFNs with just a lightweight tuning procedure. We open-source our code and raw results at https://github.com/penfever/TuneTables.

**Our contributions.** We describe our main contributions below.

* We introduce **TuneTables**, a parameter-efficient fine-tuning technique for PFNs. TuneTables achieves the highest number of wins (30) when compared to 19 algorithms over 98 datasets, while requiring less inference time than TabPFN.
* We show how to use prompt tuning for **multi-objective optimization**, such as optimizing both accuracy and fairness simultaneously, allowing users to mitigate biased predictions of a pretrained PFNs with just a lightweight tuning procedure.
* We show that TuneTables' condensed representations can be used as an **interpretability** tool.
* We conduct an extensive study on context optimization strategies for PFNs by performing an ablation study on TuneTables, as well as studying sketching and feature selection techniques.
* In order to better manage the limitations of our method, we introduce TuneTables-medium and TuneTables-light, which achieve strong tradeoffs between precision and speed.

Figure 1: **TuneTables: a novel prompt-tuning technique for prior-data fitted networks.** TuneTables performs prompt tuning on a pre-trained prior-fitted network (TabPFN) to distill real-world datasets into learned embeddings, allowing for stronger performance and faster inference time than TabPFN in many cases. TuneTables also expands the capabilities of pre-trained PFNs; by way of example, we demonstrate its effectiveness for bias mitigation, and as an interpretability tool.

Background

PFNs: review and limitations.In this section, we give a background on PFNs and discuss their limitations. For a complete description, see [35; 55; 58]. Assume that we have a classification problem with features \(^{d}\) and labels \(\). Given a dataset \(D=D_{} D_{}\), where \(D_{}=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) and \(D_{}=\{(x_{},y_{})\}\), our goal is to predict the conditional class probabilities \(p( x_{})\). In the Bayesian framework for supervised learning, the mechanism for generating the data distribution is a hypothesis \(\), drawn from \(\), the space of all hypotheses. \(\) encodes our prior beliefs on the system before observing data. In this framework, datasets \(D\) are generated by first drawing \(\), and then drawing _i.i.d._ samples according to \(\). The posterior predictive distribution (PPD) for a test sample \(x_{}\) is the label distribution \(p( x_{},D_{})\) that follows from our prior. We can obtain the PPD by integrating over the space of hypotheses \(\):

\[p(y x,D)_{}p(y x,)p(D)p()d.\] (1)

A PFN is a transformer-based architecture trained to approximate the PPD via _synthetic prior-fitting_. Given a prior, we first sample hypotheses \( p()\) and then synthetic datasets \(D p(D)\). We optimize the parameters of the PFN by predicting the class labels of \(D_{} D\), conditioned on \(D_{}=D D_{}\). We compute the loss by:

\[_{}=_{D p(D)}[- q_{}(y_{ } x_{},D_{})],\] (2)

for simplicity assuming all training and test sets are size \(n\) and \(1\), respectively. We then approximately solve this optimization problem, \(=_{}_{}\), allowing \(q_{}\) to approximate Equation (1):

\[q_{}(y_{} x_{},D_{}) p(y_{ } x_{},D_{}).\] (3)

Scaling challenges.While PFNs, specifically TabPFN, have shown remarkable success in classification by in-context learning, several important obstacles constrain their more widespread adoption:

1. **PFNs only accept a fixed number of features.** The current design of PFNs fixes the quantity of features at the time of pretraining. This quantity cannot be changed without retraining the PFN.
2. **PFNs scale poorly with the dataset size**. While PFN accuracy can improve with more real-data samples at inference time [35; 58], the memory requirements scale with the context length, making extensions beyond a certain number of samples impractical.
3. **PFNs only select from a fixed number of classes**. The MLP decoder head co-trained with the PFN fixes the number of classes that can be identified at test time.

A motivating example.In Figure 3 (left), we present a comparison between CatBoost and TabPFNs3000 from , a version of TabPFN that, for datasets above 3000 data points, uses a random subset of 3000 data points, for fitting; this version also evaluates 30 feature subsets based on mutual information (requiring 30\(\) more time). We ablate our feature and sample subselection strategies for TabPFNs3000 in Appendix Table 13. Although TabPFNs3000 performs very well on datasets with fewer than 1000 datapoints and 100 features, it significantly underperforms CatBoost beyond those constraints.

Approach.We propose sketching, feature selection, and fine-tuning as an attempt to remedy _(1)_ and _(2)_. Then in Section 3, we describe novel prompt-tuning and class-extension strategies to create TuneTables, a robust classifier which remedies _(1)+(3)_.

### Classical sketching and feature selection

Sketching.The number of samples that a PFN can handle is limited to around 3000 by conventional GPU sizes. However, in the real world, datasets are often much larger. Going forward, we refer the maximum allowable context size of the PFN as \(n\), and to the size of the real-world dataset as \(N\).

Given a training dataset \(D_{}\) of size \(N>>n\), one option is to select a representative subset of the dataset, \(D_{} D_{}\), to use as the context. In general, researchers have studied a variety of data summarization techniques, often called _sketching_, for tabular data . In the context of a pretrained PFN \(q_{}\), the goal is to find a sketching function \(s:^{N d}^{n d}\) such that

\[_{D p(D)}[- q_{}(y_{} x _{},s(D_{}))]\] (4) \[\,\,_{D p(D)}[- q_{}(y_{ {test}} x_{},D_{})].\]

Here, we consider three sketching methods for TabPFN: _random_, in which we select a random subset of \(n\) datapoints from the full training set; _k-means_, in which we compute the \(n\)-means clustering  of \(D_{}\) and select the \(n\) centers; and _CoreSet_, in which we compute a core-set of size \(n\).

Feature selection.In addition to the limitation on dataset size, PFNs, such as TabPFN, also impose a limitation on the number of features \(d\). Similar to sketching, given a dataset with \(D>>d\) features, we can perform feature selection or summarization in order to adhere to the constraint (formally, finding a function that reduces the feature dimension and approximates an expression similar to Equation (4)). Feature selection is a critical part of tabular classification, and there are several popular feature selection methods [13; 17]. We investigate three different methods: _random_, in which we randomly select a set of \(d\) features; _mutual information_, in which we select \(d\) features with the highest mutual information of the target dataset ; and _principal component analysis (PCA)_, in which we take the \(d\) first principal components. In Section 4, we find that all sketching and feature selection methods plateau in performance well before approaching parity with GBDTs, which motivates our investigation of new scaling techniques.

### Fine-tuning

We conclude this section with a discussion of another potential approach for scaling PFNs: **fine-tuning**. In this approach, given a dataset of size \(N>>n\), we use gradient descent to continue training _all parameters_ of the PFN. However, we show in Section 4 that fine-tuning takes up considerable memory resources while still not achieving competitive accuracy on large datasets; we postulate that for most datasets, the synthetic prior of TabPFN is more robust to overfitting than the actual training data; fine-tuning overfits to the validation set. This is borne out by the observation that when fine-tuning TabPFN, validation accuracy continues to improve even as test accuracy declines. To remedy this, we present new parameter-efficient solutions for scaling PFNs in the next section.

## 3 TuneTables

Motivated by the strong performance of prompt tuning, we introduce **TuneTables**, a new tabular classification algorithm. Using a pretrained PFN (TabPFN, in our experiments) as a base model, TuneTables overcomes the limitations of PFNs, allowing them to be applied to any tabular classification problem.

For the full details of the algorithm, please refer to Appendix D. We summarize here: _(a)_ if a dataset is small enough to run with the original zero-shot version of TabPFN, we include it in our search, as the TabPFN is already highly optimized for such datasets; The reason why we do not only use TabPFN is that the exact size at which TuneTables outperforms TabPFN is dataset-dependent (with an average transition of around 800 samples). _(b)_ if there are more than 100 features, we perform grid search over a set of feature subselection methods and select the one which performs best on the validation set; _(c)_ orthogonally, if there are too many labels, we fit a new decoder to a frozen TabPFN; _(d)_ we optimize over a search space of tuned prompts, both with and without real-data context during training, fitted to the dataset; _(e)_ we report the best-performing model according to accuracy.

Prompt tuning as a scalable context for PFNs.Motivated by the limitations of sketching for large contexts, we explore _soft prompt tuning_ as an alternative. In soft prompt tuning , given a dataset \(D=D_{} D_{}\), a parameterized matrix \(D_{}^{p e}\) is prepended to the input embedding \(D_{}^{n e}\), where \(e\) is the transformer embedding dimension and \(p\) is a hyperparameter-the size of the tuned prompt.

The paper  demonstrates the effectiveness of soft prompt tuning for NLP tasks by prepending a small task-specific prompt (\(p 5\)). These task-specific learned tokens prove effective at the extremely large model scales commonly encountered in NLP. Somewhat surprisingly, we show that prompt tuning is effective at similar scales of \(p\) even for the much smaller tabular data models (see Appendix D and Appendix F). However, prompt tuning increases in effectiveness when \(p\) is larger.

Soft prompt tuning for tabular data.Unlike in NLP, transformers for tabular data, including PFNs, generally accept two input embeddings; a continuous-valued embedding \(D_{X}\), and a categorically-valued \(D_{y}\) which is passed through directly. We adapt the method of  by fitting the parameters of \(D_{X}^{p e}\) to \(D_{X}\) and randomly initializing \(D_{y}^{p 1}\) with an equal number of labels from each class in \(D_{}\). These synthetic datapoints are optimized on the entire labeled set, and therefore give PFNs access to a much larger training set not accessible by existing methods.

We further adjust the method of  to allow for the possibility that \(D_{}\) has learned, in essence, a distilled version of \(D_{}\); at test time, we evaluate two settings, hereafter referred to as \(C\) ('context') and \(NC\) ('no context'). In the \(C\) setting, following , we have \(D_{}^{p e}\) prepended to the input embedding \(D_{}^{n e}\). In the \(NC\) setting, we provide only \(D_{}^{p e}\) at test time. In Section4, we empirically evaluate both approaches. We ablate specifics of our implementation choices in AppendixD. Unlike prompt tuning for NLP, we show that the \(NC\) setting is often competitive with, if not better than, the \(C\) setting. We also ablate this choice during training; see AppendixD for implementation specifics.

Extending the number of predicted classes.TabPFN uses a pretrained transformer, with a two-layer MLP as a final classifier. The pretraining procedures limit the naive use of TabPFN to classification tasks with at most 10 classes. Yet, in many cases, datasets of interest might contain a larger number of classes, which would require pretraining a new PFN from scratch.

Following the method of last-layer retraining [44; 45], for such datasets, we fit a PFN to new posteriors given by real-world classification datasets with more than \(10\) classes, freezing all weights except for those of the decoder MLP and the tuned prompt (see AppendixD for more implementation details). Even after this modification, our method remains highly parameter-efficient, optimizing fewer than 5% of TabPFN's parameters.

## 4 Experiments

Algorithms and datasets used.We compare TuneTables to nineteen algorithms, including three GBDTs: CatBoost , LightGBM , and XGBoost ; 11 neural networks: DANet , FT-Transformer , two MLPs , NODE , ResNet , SAINT , STG , TabNet , TabPFN , VIME , and ExcelFormer ; and five baselines: Decision Tree , KNN , Logistic Regression , Random Forest , and SVM . For all algorithms, we use their official implementation; see AppendixC for more details. We also compare to TabPFNs3000. We run the algorithms on the TabZilla Benchmark Suite introduced in . This suite consists of 98 classification datasets from OpenML  with a diversity of sizes and number of features . See Table4 in AppendixC for a list of all datasets with their statistics.

Experimental setup.For all algorithms other than TuneTables, we perform light hyperparameter tuning by running one default setting and 29 iterations of random search using Optuna ); see AppendixC for details. Following , all of the algorithms come with their default set of hyperparameters used in the official implementation, and we used all of these settings. For TuneTables, we optimize via a grid search described in AppendixD.

Figure 2: **TuneTables and state-of-the-art tabular models.** A critical difference plot according to mean accuracy rank across the 98 datasets in Table 1 of . Algorithms which are _not significantly different_ (\(p>0.05\)) are connected with a horizontal black bar. TuneTables achieves the highest mean rank of any algorithm.

We fit for up to 100 epochs with early stopping. For all algorithms, we report the test performance of the hyperparameter set with the best performance on the validation set and cross-validate on three train/test folds from OpenML. We conduct our experiments on an NVIDIA L4 TPU with 24GB VRAM. We summarize our results across datasets by reporting mean accuracy as well as the mean normalized accuracy (after Z-score normalization) for each algorithm.

Algorithm comparison.We compute _statistically significant_ performance differences among algorithms averaged across all datasets, as done in prior work . We first use a Friedman test to check whether performance differences between all algorithms are significant . We reject the null hypothesis for \(p<0.05\). Then we use a Wilcoxon signed-rank test to check which pairs of algorithms have significant performance differences . We use a Holm-Bonferroni correction to account for multiple comparisons . See Figure 2. We find that TuneTables achieves the highest average rank, although the difference among the top three algorithms is not statistically significant. In Table 1, we present the accuracy, rank, and Z-score of all algorithms, averaged across all datasets, and find that TuneTables performs very well on all metrics but runtime.

Large datasets.One limitation of the TabZilla Benchmark Suite is that even the largest dataset in the comparison contains only 45,211 samples. This scale is modest by the standards of modern tabular problems. In order to better understand the performance of TuneTables on extremely large datasets, we curate from  a novel benchmark, LargeScaleTables, consisting of 29 datasets with up to 1 900 000 samples, and 7200 features. We curate the datasets from OpenML, omitting image classification datasets. Since TabPFN generally outperforms boosted trees on these smaller datasets, and TuneTables extends TabPFN, we also heuristically select smaller datasets to LargeScaleTables so as not to favor either TabPFN or boosted trees. TuneTables achieves the highest average accuracy of any algorithm, and achieves the best performance on poker-hand, a dataset of size 1 025 009. The complete results can be found in Appendix Table 8 and Figure 3 (right). In order to assess the performance of TuneTables on datasets with many classes, we curate another subset of , presenting results on fifteen datasets with up to 100 classes. In appendix Table 7 we show that despite the large divergence from the PFN's pretraining, TuneTables achieves a mean rank of 2.52, ahead of CatBoost and a ResNet, second only to XGBoost, whose mean rank is 2.0.

Runtime comparison.We divide our consideration of runtime into inference and training. At inference time, TuneTables is around 9x faster than TabPFNs3000, on average; see Appendix Table 10 for the per-dataset details. However, the end-to-end runtime of TuneTables is over 7x that of CatBoost and XGBoost, and also slower than TabPFNs3000, because of the increased _training time_.

In order to better understand the trade-off between accuracy and runtime, we introduce efficient variants of our method. _TuneTables-medium_ utilizes a more efficient adaptive sequence size (i.e., the number of real data points received at train time) which scales with the size of the dataset, validates

Figure 3: **TuneTables addresses TabPFN’s limitations. (Left) Motivating example (using the subset of on which both CatBoost and TabPFNs3000 report results): TabPFNs3000 is best on small datasets, but when scaled past 3000 datapoints and 100 features, TabPFNs3000 significantly underperforms. (Middle) CatBoost vs. TuneTables on LargeScaleTables : By contrast, TuneTables is competitive with CatBoost on all datasets, mitigating the limitations of TabPFN. (Right) TabPFNs3000 vs. TuneTables on LargeScaleTables : TuneTables outperforms TabPFNs3000 on datasets with a high number of datapoints or features. The colorbar on the y axis represents the comparative change in per-dataset accuracy between two algorithms (A: blue, B: red). Positive numbers represent the absolute gain in accuracy of B w.r.t. A, negative numbers represent the absolute gain in accuracy of A w.r.t. B.**

on a subset of the available validation set when the validation set is large, employs lower patience for early stopping, omits a zero-shot TabPFN grid search over 30 random seeds which TuneTables-standard uses to find more optimal feature selection subsets, and, most impactfully, omits ensembling for datasets larger than a cutoff hyperparameter (which we fix at 150 000 samples). _TuneTables-light_ includes all of the efficiency modifications of TuneTables-medium, trains for just one epoch, and uses TabPFN zero-shot to preselect the feature selection method rather than performing a grid search using TuneTables itself. In Table 2, we compare these lighter methods to TuneTables-standard on the LargeScaleTables benchmark. TuneTables-medium decreases runtime by 72% compared to TuneTables-standard, while the accuracy decreases by less than a quarter of a percent. Furthermore, the runtime of TuneTables-light is comparable to CatBoost; however, performance also degrades by 5% when going from TuneTables-medium to TuneTables-light. Still, TuneTables-light shows stronger performance than TabPFNs3000 (78.7% vs. 78.1% accuracy) while having a lower inference time.

Ablations.In order to better understand the significance of the changes we introduce in our method, we separately ablate the tuned prompt size and the use of ensembles in Table 12 and Table 13, finding that smaller datasets pair well with smaller prompts and rarely benefit from ensembling.

We also compare TuneTables with and without keeping the real data as additional context (referred to as C and NC, respectively, in Section 3); see Appendix Table 12. We find that smaller datasets cannot always be fully learned by the tuned prompt, but for larger datasets, the tuned prompt alone suffices, and in some cases even outperforms the real data.

  
**Model** & **Mean Acc.** & **Mean Rank** & **Mean Z-Scores** & **Std Z-Scores** & **Med Z-Scores** & **Num. Wins** \\  TuneTables & **0.860** & **7.435** & 0.494 & **0.624** & 0.490 & **30** \\ CatBoost & 0.856 & 7.514 & **0.496** & 0.669 & **0.566** & 13 \\ XGBoost & 0.854 & 7.991 & 0.411 & 0.783 & 0.533 & 16 \\ ExcelFormer & 0.847 & 9.349 & 0.212 & 0.863 & 0.384 & 7 \\ LightGBM & 0.845 & 9.122 & 0.284 & 0.894 & 0.431 & 20 \\ RandomForest & 0.843 & 9.713 & 0.206 & 0.776 & 0.374 & 4 \\ SAINT & 0.840 & 9.619 & 0.132 & 0.985 & 0.337 & 10 \\ DANet & 0.840 & 9.646 & 0.209 & 0.763 & 0.345 & 3 \\ rtdl\_ResNet & 0.839 & 9.691 & 0.175 & 0.798 & 0.356 & 5 \\ rtdl\_FTTransformer & 0.838 & 9.782 & 0.198 & 0.786 & 0.273 & 8 \\ NODE & 0.838 & 9.335 & 0.224 & 0.695 & 0.357 & 3 \\ SVM & 0.835 & 9.847 & 0.121 & 0.812 & 0.318 & 12 \\ DecisionTree & 0.823 & 12.192 & -0.308 & 1.215 & -0.038 & 4 \\ rtdl\_MLP & 0.813 & 10.825 & -0.019 & 0.907 & 0.223 & 1 \\ STG & 0.810 & 12.196 & -0.290 & 1.051 & -0.057 & 5 \\ TabNet & 0.804 & 11.643 & -0.216 & 1.104 & 0.075 & 5 \\ MLP & 0.802 & 12.220 & -0.232 & 0.845 & -0.105 & 1 \\ LinearModel & 0.793 & 13.069 & -0.520 & 1.233 & -0.345 & 5 \\ KNN & 0.781 & 14.101 & -0.727 & 1.187 & -0.608 & 0 \\ VIME & 0.756 & 14.711 & -0.849 & 1.192 & -0.694 & 5 \\   

Table 1: **TuneTables matches SOTA algorithms on 98 datasets. In this table, we compare algorithms over the 98 datasets in the TabZilla benchmark suite from . For each algorithm, we compute its mean accuracy and mean rank in terms of accuracy. We also compute the mean Z-score, computed by normalizing the set of results on each dataset (by mean 0 std. 1), so that each dataset has the same weight, and averaging each algorithm’s normalized performances. Std. Z-Score is computed with respect to random splits and averaged across datasets. Num. wins values are averaged over three splits per dataset.**

    & **TuneTables** & **TuneTables-medium** & **TuneTables-light** \\  Avg. acc (LargScaleTables, size \(<\) 50K) & 0.831 & 0.830 & 0.810 \\ Avg. runtime (LargScaleTables, size \(<\) 50K) & 1325 & 1026 & 450 \\  Avg. acc (LargScaleLetTables, all datasets) & 0.830 & 0.828 & 0.787 \\ Avg. runtime (LargScaleLetTables, all datasets) & 6975 & 1908 & 486 \\  Avg. acc (TabZilla, all datasets) & 0.861 & 0.855 & 0.854 \\ Avg. runtime (TabZilla, all datasets) & 573 & 305 & 196 \\   

Table 2: **TuneTables-medium and TuneTables-light are substantially faster with only a modest decrease in accuracy. We compare the average accuracy and runtime (in seconds) of three versions of TuneTables and find that the medium and light versions of the algorithm are substantially faster on large datasets; we also find that TuneTables-medium sacrifices little accuracy.**Sketching and feature selectionFinally, in Appendix Table 6 we give a study on the three sketching and three feature selection techniques described in Section 2. As described earlier, TabPFNs3000's performance when relying on feature selection plateaus well before approaching parity with CatBoost, a top-performing GBDT, on seven very large datasets.

## 5 TuneTables Extensions

**Mitigating bias with prompt tuning.** Many real-world applications of machine learning involve a set of protected attributes (such as race or gender) that partition the dataset into groups, in which some have higher model performance than others. Removing the sensitive attributes does not fix the algorithmic bias, because the sensitive attributes are often non-trivially correlated with other attributes in the dataset. Due to this issue, researchers have put in significant effort into mitigating the bias of ML models, with the majority of techniques devising new training strategies [9; 53].

Given TabPFN's pretrained nature and considerable retraining cost, the only options for mitigating biased predictions are to run a post-processing routine on the output predictions, which generally do not perform as well as in-processing strategies . We show how to use prompt tuning to substantially reduce the bias of predictions while also improving accuracy.

We conduct experiments on four datasets widely used for research in fairness: the Adult Census Income database (with sex as the sensitive attribute) , speed dating (with same race as the sensitive attribute) , COMPAS (with sex as the sensitive attribute) , and National Longitudinal Survey (with gender as the sensitive attribute) . To quantify bias, we use _demographic parity_[76; 25], which measures the difference in probability of a positive outcome among the protected and unprotected groups. Formally, given protected group \(G_{1}\), unprotected group \(G_{0}\), and protected attribute \(x_{,a}\), it can be computed as

\[P_{(x_{i},y_{i}) G_{0}}(y_{i}=1 x_{i,a})-P_{(x_{i},y_{i}) G_{1}}(y_ {i}=1 x_{i,a}).\]

During prompt tuning, we employ a demographic parity regularizer that aims to minimize the difference in positive outcome probabilities between the two groups:

\[|_{(x_{i},y_{i}) G_{0}}P(y_{i}=1 x_{i,a})-_{(x_{i},y_{i})  G_{1}}P(y_{i}=1 x_{i,a})|\]

We use the same experimental setup as in Section 4, except that we report one shift rather than the average of three, and TuneTables is fitted to a single prompt rather than ensembled. We compare the default TabPFN to TuneTables, fine-tuning for accuracy alone vs. accuracy and demographic

Figure 4: **Dataset with high accuracies from just two datapoints. Shown is a two-example prompt dataset for the breast cancer dataset . Malign class example has higher values for all features than benign class.**

    & Adult & Speeddating &  &  &  \\  Acc \(\) & DP \(\) & Acc \(\) & DP \(\) & Acc \(\) & DP \(\) & Acc \(\) & DP \(\) & Acc \(\) & DP \(\) \\  TabPFN & 0.832 & 0.174 & 0.86 & 0.012 & 0.688 & 0.22 & **0.986** & 0.326 & 0.842 & 0.183 \\  TuneTables (Acc) & **0.845** & 0.13 & **0.865** & 0.006 & 0.688 & 0.209 & 0.974 & 0.302 & **0.843** & 0.162 \\  TuneTables (Acc + DP) & 0.837 & **0.034** & 0.863 & **0.003** & **0.693** & **0.121** & 0.965 & **0.277** & 0.840 & **0.109** \\   

Table 3: **TuneTables significantly improves accuracy and demographic parity. In these multi-objective optimization experiments, we consider prompt tuning for mitigating predictive bias, comparing TabPFN to TuneTables, tuning for accuracy alone vs. accuracy and demographic parity. TuneTables improves over TabPFN with respect to both objectives.**parity. The latter significantly improves demographic parity, compared to the default TabPFN and TuneTables fine-tuned for accuracy, and enhances accuracy relative to the default TabPFN across all datasets but one; see Table 3.

**Summarizing and understanding datasets with prompt tuning.** While we have demonstrated that TuneTables scales and improves the performance of PFNs, now we show that it can also help understand the discriminative features in a dataset. Often, besides a good predictive model for a dataset, users want to gain further insights into the dataset. In prompt tuning, the tuned smaller dataset can be seen as a summary of the complete dataset that emphasizes discriminative features for the given task. As an example, in Figure 4 we show that on the Breast Cancer dataset , a prompt with just two synthetic examples is enough to reach high accuracies and at the same time allows an understanding of the predictive features. For example, in the Breast Cancer dataset, the malign example has higher values for all features compared to the benign example, suggesting high feature values indicate malignancy. We give further examples in Appendix F.

## 6 Related work

Neural Processes and Prior-Data Fitted Networks.Prior-data Fitted Networks (PFNs) [35; 55] are a recently-proposed paradigm for machine learning, which show that fast approximate Bayesian inference is possible by training a neural network to mimic the posterior predictive distribution (PPD) in a single forward pass using in-context learning [54; 55; 58]. PFNs were shown to yield state-of-the-art empirical performance on small tabular datasets [35; 51]. PFNs have been used in other applications, including Bayesian optimization  learning curve extrapolation , and as foundation models for hypernetworks .

PFNs are neural processes (NPs) . Recent advances in NP are similar in nature to our work, especially recent works that aim to scale attention-based methods to larger contexts. Feng et al.  propose Latent Bottlenecked Attentive Neural Processes (LBANPs), a transformer-based neural process which overcomes the quadratic complexity of transformers by encoding the context dataset into a constant number of latent vectors. Guo et al.  propose Versatile Neural Processes (VNP), which increases the capability of NPs to handle compex signals by using a new bottleneck encoder.  introduces semi-parametric inducing point networks (SPIN), which can attend to a training set at inference time with linear complexity via inducing point methods.  introduce Constant Memory Attention Block (CMAB), an attention block that is permutation-invariant and has constant memory complexity when computing the output, as well as Constant Memory Attentive Neural Processes (CMANPs), a NP that requires constant memory. For additional related work, see Appendix B.

## 7 Conclusions, limitations, and future work

In this work, we gave the first investigation into context optimization techniques for PFNs, allowing us to substantially improve their performance when scaled to large datasets. In particular, we introduced TuneTables, which uses a novel prompt-tuning technique to achieve strong performance on large datasets. We demonstrate that TuneTables mitigates the constraints of TabPFN on the dataset size, number of features, and number of class labels. Additionally, we use prompt tuning to mitigate bias without retraining TabPFN and as an interpretability tool. We open-source our code, results, and all other materials needed to reproduce our work. As PFN models improve, the context optimization techniques explored in our work will allow researchers to further optimize and scale. For example, a next-generation TabPFN might have a longer total context length, and prompt tuning will allow us to push the dataset size even further.

Limitations.No current TuneTables is on par with GBDTs in terms of both accuracy and runtime simultaneously. We therefore emphasize that while we achieve strong performance metrics, we do not claim practical superiority of our method over gradient boosting, when taking into account training time. However, given the novelty of our method, we expect future research to further improve the accuracy-runtime tradeoff of TuneTables. It also does not improve on TabPFN for small datasets (fewer than 1000 samples, 100 features and 10 classes); we postulate that this is a result of overfitting.

Future work.Parameter-efficient fine-tuning can be used with PFNs to ensure high-quality, differentially private predictions, given that the pretraining is done on purely synthetic data, and prompt tuning only updates a small number of parameters. Low-rank adaptation (LoRA)  and quantized low-rank adaptation (QLoRA)  have been used successfully for large language models and would be a promising technique for parameter-efficient fine-tuning of PFNs. Designing a sparse mixture-of-experts PFN using router networks is another promising technique, due to its success in the field of large language models .