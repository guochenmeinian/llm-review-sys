# Optimal Multi-Fidelity Best-Arm Identification

Riccardo Poiani

DEIB, Politecnico di Milano, Milan, Italy

riccardo.poiani@polimi.it

Work done while at Inria, Lille, France.

Remy Degenne

Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France

remy.degenne@inria.fr

Emilie Kaufmann

Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189-CRIStAL, F-59000 Lille, France

emilie.kaufmann@univ-lille.fr

Alberto Maria Metelli

DEIB, Politecnico di Milano, Milan, Italy

albertomaria.metelli@polimi.it

Marcello Restelli

DEIB, Politecnico di Milano, Milan, Italy

marcello.restelli@polimi.it

###### Abstract

In bandit best-arm identification, an algorithm is tasked with finding the arm with highest mean reward with a specified accuracy as fast as possible. We study multi-fidelity best-arm identification, in which the algorithm can choose to sample an arm at a lower fidelity (less accurate mean estimate) for a lower cost. Several methods have been proposed for tackling this problem, but their optimality remain elusive, notably due to loose lower bounds on the total cost needed to identify the best arm. Our first contribution is a tight, instance-dependent lower bound on the cost complexity. The study of the optimization problem featured in the lower bound provides new insights to devise computationally efficient algorithms, and leads us to propose a gradient-based approach with asymptotically optimal cost complexity. We demonstrate the benefits of the new algorithm compared to existing methods in experiments. Our theoretical and empirical findings also shed light on an intriguing concept of optimal fidelity for each arm.

## 1 Introduction

In multi-armed bandits , an algorithm chooses at each step one _arm_ among \(K>1\) possibilities. It then observes a reward, sampled from a probability distribution on \(\) corresponding to the arm. Several goals are possible for the algorithm, and we focus on the _best arm identification_ task (BAI) in which we aim to identify the arm with the largest mean, using as few samples as possible. This is a well-studied problem  with potential applications to, e.g. A/B/n testing  or hyper-parameter optimization .

In some applications, like physics, parameter studies, or hyper-parameter optimization, getting a sample from the arm distribution might be expensive since it requires evaluating or training a complex model and is computationally demanding. However, it is often the case that cheaper, less accurate sampling methods are available, for instance, by using a coarser model in the physics study example.

The _multi-fidelity_ bandit framework takes such scenarios into account. When choosing an arm, the algorithm also chooses a fidelity, with a trade-off: a higher fidelity gives a more precise observation but has a higher cost. We assume that the algorithm knows both the cost and the maximal bias of the observations from each fidelity. This is also how the knowledge about the fidelity was modeled in prior work (see, e.g., 16; 15; 25; 31). The goal is then to find the best arm (i.e., the arm with the highest mean at the highest fidelity) with high probability and minimal cost.

Specifically, the bandit algorithm interacts with the multi-fidelity environment and gathers information to find which arm has the highest mean when pulled at the highest fidelity. In the fixed confidence setting, we want to ensure that the algorithm returns a correct answer with probably at least \(1-\) for a given parameter \((0,1)\). A good algorithm should do that at a minimum cost, and thus, the appropriate quality metric for evaluating an algorithm's performance is the sum of costs paid until it stops, i.e., the cost complexity. Previous work on the multi-fidelity BAI problem (25; 31) provided lower bounds on the cost complexity as well as algorithms with cost upper bounds. Those lower and upper bounds do not match, and the proposed methods require additional prior information (31), or their guarantees are restricted to problems satisfying additional hypotheses (25). We lift all those requirements and provide an improved lower bound and an algorithm with a matching upper bound.

Contributions and organization of the paperAfter presenting additional related works, in Section 2, we define fixed-confidence best arm identification in multi-fidelity bandits in more mathematical detail and introduce the notations used throughout the paper. Then, Section 3 contains our first contribution: a tight instance-dependent lower bound on the cost complexity of any algorithm expressed with the maximum of a complex function over all possible _cost_ allocations. We also highlight features of that lower bound, like the existence of an optimal fidelity for each arm, which should be chosen exclusively. In Section 4, we propose a computationally efficient procedure for computing gradients of the function featured in the lower bound and describe a gradient-based algorithm whose cost complexity is asymptotically matching the lower bound. Finally, in Section 5, we present the results of numerical experiments which demonstrate the good empirical performance of our new algorithm compared to prior work.

Additional related worksThe multi-fidelity setting has mostly been studied in the context of Bayesian optimization (9; 24; 17; 26; 14; 21) and black-box function optimization with different structural assumptions (28; 29; 7; 23). The goal there is to find the minimum of a function by successive queries of that function or of cheaper approximations. The metric for success in these works is most often the simple regret, that is, the difference between the best value found and the true minimum, although other goals were considered like the cumulative regret (16; 15). Furthermore, we notice that best arm identification with costs has recently been studied in (13) for BAI with only one fidelity. The authors introduce a variant of the Track-and-Stop algorithm (8) and prove its asymptotic optimality. However, we will not be able to adapt this study to the multi-fidelity case because, as we shall see, it requires solving a complex optimization problem for which we have no efficient solution. Finally, our work is related to the vast strand of BAI studies that proposes tight lower bound with asymptotically optimal algorithms (e.g., 8; 4; 22); nevertheless, as we discuss throughout the text, these studies cannot be directly applied to the multi-fidelity BAI problem.

## 2 Background

In this section, we provide essential background and notation that is used throughout the rest of the paper. A table that summarizes the notation is available in Appendix A.

A multi-fidelity bandit model with \(K\) arms and \(M\) fidelities is a set of \(K M\) probability distributions \(=(_{a,m})_{a[K],m[M]}\) where \(_{a,m}\) has mean of \(_{a,m}\). For each arm \(a[K]\), \(_{a,m}\) represents the mean value of an observation of arm \(a\) using fidelity \(m\), and let \(=(_{a,m})_{a[K],m[M]}\). An observation at fidelity \(m\) is assigned a (known) cost \(_{m} 0\) with \(_{1}<_{2}<<_{M}\). The goal is to identify the arm that has the largest mean at the highest fidelity \(M\), \(a_{}():=*{argmax}_{a[K]}_{a,M}\) (sometimes denoted by \(\) in the sequel to ease notation) with a small total sampling cost, by exploring the arms at different fidelities and using some prior knowledge about their precision. Specifically, we assume that there are some (known) values \(_{1}>_{2}>>_{M}=0\) such that, for all arm \(a[K]\), the vector \(_{a}:=(_{a,m})_{m[M]}\) satisfies

\[ m[M],\;\;\;|_{a,m}-_{a,M}|_{m}\.\]We write \(_{a}\) to indicate that arm \(a\) satisfies these multi-fidelity constraints, with these particular parameters \(_{m}\) (although they are not shown in the notation). In this paper, we consider arms that belong to a canonical exponential family . This includes, e.g. arms that have Bernoulli distributions or Gaussian distributions with known variances. Such models are known to be characterized by their means and we refer to such an exponential multi-fidelity bandit model \(\) using the means of its arms \(\), which belongs to the set \(_{}:=\{^{K M}: a[K],_ {a}\}\), where \(\) is the interval of possible means.

At each interaction round \(t=1,2,\), the agent selects an arm \(A_{t}\) and a fidelity \(M_{t}\), observes a sample \(X_{t}_{A_{t},M_{t}}\) and pays a cost \(_{M_{t}}\). Letting \(_{t}=(A_{1},M_{1},X_{1},,A_{t},M_{t},X_{t})\) be the sigma field generated by the observations up to time \(t\), a fixed-confidence identification algorithm takes as input a risk parameter \((0,1)\) and is defined by the following ingredients: (i) a _sampling rule_\((A_{t},M_{t})_{t}\), where \((A_{t},M_{t})\) is \(_{t-1}\)-measurable, (ii) a _stopping rule_\(_{}\), which is a stopping time w.r.t. \(_{t},\) and (iii) a _decision rule_\(_{_{}}[K]\), which is \(_{_{}}\)-measurable. We want to build strategies that ensure \(_{}(_{_{}} a_{*}())\) for all \(_{}\) with a unique optimal arm. Such a strategy is called \(\)_-correct_. Among \(\)-correct strategies, we are looking for strategies that minimize the expected identification cost (i.e., _cost complexity_) defined as

\[_{}[c_{_{}}]_{a[K]}_{m[M]} _{m}_{}[N_{a,m}(_{})]=_{a[K]}_{ m[M]}_{}[C_{a,m}(_{})],\]

where \(N_{a,m}(t)\) denotes the number of pulls of arm \(a\) at fidelity \(m\) up to time \(t\) and \(C_{a,m}(t)=_{m}N_{a,m}(t)\) denotes the cost associated to these pulls. In the sequel, we will provide cost complexity guarantees for multi-fidelity instances \(\) that belong to the set \(_{}^{*}\) of multi-fidelity instances with a unique optimal arm, i.e., for which \(|a_{*}()|=1\). We remark that for \(M=1\) and \(_{m}=1\) we recover the best arm identification problem in a classical bandit model, for which the cost complexity coincides with the sample complexity, \(_{}[_{}]\).

Additional notationGiven an integer \(n\), we denote by \(_{n}\) the \(n\)-dimensional simplex. Furthermore, given \(x,y(0,1)\), we define \((x,y)=x(x/y)+(1-x)((1-x)/(1-y))\). Given \((p,q)^{2}\), we denote by \(d(p,q)\) the Kullback-Leibler (KL) divergence between the distribution in the exponential family with mean \(p\) and that with mean \(q\). We also write \(d^{-}(x,y)=d(x,y)\{x y\}\) and \(d^{+}(x,y)=d(x,y)\{x y\}\). Finally, we denote by \(v(p)\) the variance of the distribution with mean \(p\).

## 3 On the cost complexity of multi-fidelity best-arm identification

In this section, we discuss the statistical complexity of identifying the best-arm in MF-BAI problems. Formal proofs of the claims of this section are presented in Appendix B.

### Lower bound on the cost complexity

We present an instance-dependent lower bound on the expected cost-complexity. The lower bound uses the solution to an optimization problem, where the functions optimized quantify the trade-off between the information gained by pulling an arm at some fidelity and the cost of that fidelity. Since those functions also appear in our algorithm, we will now introduce notation for them. For all \(_{K M}\) and \(^{K M}\), we define

\[f_{i,j}(,) _{_{i},\; _{j}\\ _{j,M}_{i,M}}_{a\{i,j\}}_{m[M]} _{a,m},_{a,m})}{_{m}}\;,\] (1) \[F(,) _{i[K]}_{j i}f_{i,j}(, )\;.\] (2)

The quantity \(f_{i,j}(,)\) is the dissimilarity according to a KL weighted by the costs between \(\) and the closest \(^{K M}\) such that arms \(i\) and \(j\) satisfy the multi-fidelity constraints and \(_{k}=_{k}\) for \(k\{i,j\}\), with arm \(j\) better than arm \(i\). If \(_{}\) then that closest \(\) is also in \(_{}\) but otherwise it might not be the case: if an arm \(k\{i,j\}\) is not in MF for \(\), then it is equally not in MF for \(\). For \(_{}\) the maximum in the definition of \(F\) is realized at the best arm \(\), as \(_{a i}f_{i,a}(,)\) is zero for \(i\). That is, \(F(,)=_{j}f_{,j}(,)\). We define \(F\) with a maximum over \(i\) and not with that last expression because we want to define it for all points in \(^{K M}\), even the points which are not in \(_{}\). For those points, we could imagine different notions of best arm, for example, \(_{k}_{k,M}\), but the right one for our algorithm is the arm for which we have the most evidence (weighted by cost) to say that all other arms are not better. That arm is the argmax in our definition of \(F\). Given these definitions, we now introduce our new lower bound.

**Theorem 3.1**.: _Let \((0,1)\). For any \(\)-correct strategy, and any multi-fidelity bandit model \(_{}^{*}\), it holds that:_

\[_{}[c_{_{s}}]  C^{*}()(),\] (3)

_where \(C^{*}()^{-1}_{_{K M}}F(,)=_{_{K M}}_{a} f_{,a}(,)\,.\)_

The quantity \(C^{*}()\) describes the statistical complexity of an MF problem \(\) as the typical max-min game that appears in lower bounds for BAI problems (see, e.g., 8, 3). Specifically, first, the max-player chooses a vector \(_{K M}\), and then the min-player chooses a bandit model \(_{}\) in which the optimal arm is different, with the goal of minimizing the function \(F(,)\). Following the methods from previous work, the objective value for \(\) and \(\) should be \(_{a[K]}_{m[M]}_{a,m},_{a,m})}{ _{m}}\), featuring a sum over all arms and fidelities. However in the definition of \(f_{i,a}(,)\) we restrict \(\) to be different from \(\) on only two arms. We can prove that if \(_{}\), this gives the same objective value at the minimizing \(\) as the full sum. The difference will be important in our algorithm, which will compute that minimizer for points \(}\) that do not belong to \(_{}\).

A difference with standard BAI settings is that in Equation (1) each \(_{K M}\) should be interpreted as a vector of _cost proportions_ that the max-player is investing (in expectation) in each arm-fidelity pair to identify the optimal arm \(_{,M}\).2 We can interpret the _oracle weights_\(^{*}*{argmax}_{_{K M}}F(, {})\) as the optimal cost proportions that the agent should follow in order to identify \(_{,M}\) while minimizing the identification cost. To clarify the difference and the relationship between cost and pull proportions we notice that, given a cost proportion \(\), it is always possible to compute the pull proportions \(()_{K M}\) that the agent should play in order to incur the costs proportions specified by \(\), and vice versa. More specifically, these relationships are described for each arm-fidelity pair by the following equations for every \(a[K]\) and \(m[M]\):

\[_{a,m}()=}{_{m}}_{j[M]}}{_{j}}}_{a,m }()=_{a,m}}{_{i[K]}_{j[M]}_{ j}_{i,j}}.\] (4)

As a direct consequence, it is possible to rewrite \(C^{*}()^{-1}\) as a function of \(\), the pull proportions. Doing so reveals that the minimizer \(\) in \(f_{,j}\) does not depend on the costs: it is also the minimizer of \(_{a\{i,j\},m[M]}_{a,m}d(_{a,m},_{a,m})\). While the agent optimizes the cost proportions \(\) to get the best possible information/cost ratio, the min-player minimizes only the information available to the algorithm to tell \(\) and \(\) apart. Finally, we notice that \(F(,)\) is concave in \(^{3}\) but \(F((),)\) is not concave in \(\). As we shall see in Section 4, this difference will play a crucial role in constructing an asymptotically optimal algorithm.

The formulation of the lower bound as a game where one player maximizes an information/cost ratio while the other player minimizes information makes our result close to lower bounds for regret minimization like the one of , where the (unknown) gap of an arm plays the role of the cost.

Comparison to previous workThe only known lower bound for the multi-fidelity BAI problem is the one presented in . That same bound was then shown in . The bound from those previous works is looser than Theorem 3.1. For example, in a two-arms bandit with a single fidelity (denoted by \(M\)) and Gaussian rewards with variance 1, the bound from previous work is \(_{M}(_{1,M}-_{2,M})^{-2}(1/2.4)\), while our lower bound is \(8_{M}(_{1,M}-_{2,M})^{-2}(1/2.4)\). Furthermore, on particular instances with 2 arms and 2 fidelity, we can prove that our lower bound improves by a factor \(_{M}/_{1}\), which can be arbitrarily large (See Appendix B.2). More generally, the proof of the previous lower bounds exhibits a particular point in the alternative, which makes it always looser than our bound which features an infimum over all points. Theorem 3.1 is also optimal in the regime \( 0\) since it is matched by the algorithm we introduce in the next section.

### Sparsity of the oracle weights: a tight concept of optimal fidelity

We conclude our study of the lower bound by further analyzing the optimal allocation \(^{*}\). Unlike in the standard best arm identification problem, we did not find an efficient algorithm to compute it, which prevents us from using a Track-and-Stop-like approach . Nevertheless, we will explain in the next section how to efficiently compute the \(f_{i,j}\) functions and their gradient. These computations are crucial for our algorithm but also allow us to prove our next result about the possible sparsity of \(^{*}\). For each arm \(a[K]\), it is not difficult to show that there must exist some fidelity \(m[M]\) for which \(^{*}_{a,m}>0\) (Lemma B.2). However, as the following result highlights, in most cases, only one fidelity per arm has non-zero weight.

**Theorem 3.2**.: _Let \(^{*}_{K M}()*{argmax}_{ _{K M}}F(,)\) and_

\[}_{}\{ _{}^{*}: i[K], m_{1},m_{2}[M]^{2}, ^{*}^{*}_{K M}():^{*}_{i,m_{1} }>0,^{*}_{i,m_{2}}>0\}.\]

_The set \(}_{}\) is a subset of \(^{K M}\) whose Lebesgue measure is zero._

Theorem 3.2 implies that in almost all multi-fidelity bandits, for any \(^{*}^{*}_{K M}()\) and each arm \(a[K]\), there exists a single fidelity \(m_{a}^{*}[M]\) for which \(^{*}_{a,m_{2}^{*}}>0\) holds. However, we note that this result does not offer an easy way to compute these optimal arm-dependent fidelities.4 Nevertheless, as we shall see in the next section, our algorithm does not actually require identifying these optimal fidelity levels to enjoy optimality guarantees.

Finally, we remark that existing MF-BAI works [25; 31] already proposed notions of optimal, arm-dependent fidelity that the agent should employ to identify the optimal arm \(\). Nevertheless, as we verify in Appendix B.5, these concepts do not comply with the concept of optimal fidelity that arises from the tight lower bound of Theorem 3.1. In other words, there exist bandit models \(\) in which following these alternative concepts of optimal fidelity leads to sub-optimal performance.

## 4 The multi-fidelity sub-gradient ascent algorithm

We present our solution for solving MF-BAI problems, an algorithm called Multi-Fidelity Sub-Gradient Ascent (MF-GRAD). Its pseudocode can be found in Algorithm 1. All proofs for this section are presented in Appendix C.

A reader familiar with the literature on BAI algorithms inspired from lower bounds like Theorem 3.1 may have the natural idea of simply using the Track-and-Stop algorithm  or the related game-based algorithm of . Those algorithms can't be directly applied here, first because of the costs: we want to bound the cost complexity, not the stopping time, and adapting those methods to costs is not trivial. Furthermore, Track-and-Stop (even in the cost-aware variant of ) would require the computation of the optimal cost proportions at \(}(t)\), which is a max-min problem for which we don't have an efficient algorithm. Our solution is inspired by the gradient ascent algorithm of , which requires computing gradients of \(F\) (hence only a minimization problem and not a max-min). The same innovations required to extend this method to the multi-fidelity case could likely allow us to adapt the algorithm of , or the exploration part of the regret-minimizing algorithm of .

Let us introduce some auxiliary notation. Let \(}_{K M}\) be the uniform vector \(,,\). For all \(t\), we define \(_{t}(x)=\{x_{a,m},G\}_{a[K],m[M]}\) for an arbitrary constant \(G>0\). We also define \(_{t}=}\) and \(_{t}=}\). Finally, for all \(t\), we denote by \((t)^{KM}\) the vector whose \((a,m)\)-th dimension is given by \(C_{a,m}(t)\). We now present Algorithm 1.

Sampling ruleAfter a first initialization phase in which the algorithm pulls each arm at each fidelity once (Line 1), the agent starts its sub-gradient ascent routine. More specifically at each iteration \(t\), the agent first computes the vector \(}(t+1)\) using the Exponential Weights algorithm on the sequence of gain functions \(\{g_{s}\}_{s=1}^{t}\{_{s}((_{a,m}_{m} _{a,m}(s)) F(}(s),}(s)) )\}_{s=1}^{t}\), where \(}(t)(}(t))\) represents the pull-proporportions induced by \(}(t)\) and \( F(}(s),}(s))\) denotes a sub-gradient of \(F(,)\) w.r.t \(\) (Line 3). Neglecting for a moment the clipping function and the term\((s)(_{a,m}_{m}_{a,m}(s))\) (these terms are present mainly for technical reasons), this step can be interpreted, from an intuitive perspective, as finding a sequence of weights \(\{}(t)\}_{t}\) that minimizes the regret on the sequence of empirical losses \(F(^{*},}(s))-F(}(s),}(s))\). 3 At this point, once \(}(t+1)\) is computed, Algorithm 1 will convert these cost proportions into pull proportions while adding some forced exploration (Line 4-5), and then, it applies a standard cumulative tracking procedure  in the pull-proportion space so to ensure that \(N_{a,m}(t)_{s=1}^{t}^{}_{a,m}(s)\) (Line 6).

Stopping and decision ruleFinally, the algorithm applies a generalized likelihood ratio (GLR) test to decide when to stop (Line 7). For \(i,j[K]\), \(f_{i,j}((t),}(t))\) can be interpreted a GLR statistics for comparing two classes: \(^{KM}\) versus \(\{_{i},\;_{j},\;_{j,M}_{i,M}\}\). If that GLR is large enough (if it exceeds a threshold \(_{t,}\)), we can reject the hypothesis that \(\) belongs to the second class. If there is an arm \(i\) for which we can reject the alternative class for all \(j i\), we have rejected all \(_{}\) where \(i\) is not the best arm and we can safely stop and return the answer \(_{_{}}=i\). Since each \(f_{i,j}\) is expressed as a sum of only two arms and \(M\) fidelities, it is possible to show that choosing \(_{t,}(K/)+2M((t)+1)\) (see its exact expression in (31)) guarantees the correctness of the test, namely that \(_{}(_{_{}})\) holds (Proposition C.13).

### Theoretical guarantees

At this point, we are ready to state the main theoretical result on the performance of our algorithm.

**Theorem 4.1**.: _For any multi-fidelity bandit model \(_{}\), Algorithm 1 using the threshold \(_{t,}\) given in (31) is \(\)-correct and satisfies_

\[_{ 0}_{}[c_{_{}}]}{(1/ )} C^{*}().\] (5)

As we can see from Theorem 4.1, Algorithm 1 is asymptotically optimal, meaning that it matches the lower bound we presented in Theorem 3.1 for the asymptotic regime of \( 0\).

Comparison with existing MF-BAI algorithmsWe conclude this section by comparing our results with the literature [25; 31]. First,  and  rely on _additional assumptions_ that play a crucial role both for the algorithm design and the resulting theoretical guarantees. In , the authors enforce an additional and intricate structural assumption on the relationships between \(\)'s and \(\)'s (see Assumption 1 in ). In , instead, the authors assume additional knowledge expressed as an upper bound on \(_{,M}\) and a lower bound on \(*{argmax}_{i}_{i,M}\). For both works, whenever these assumptions are not satisfied (i.e., \(\)'s and \(\)'s do not respect Assumption 1 in , and the knowledge on \(_{,M}\), \(*{argmax}_{i}_{i,M}\) is imprecise/not available), the theoretical guarantees offered by existing algorithms are arbitrarily sub-optimal. On the other hand, our algorithm requires no additional assumptions and is the only one that matches exactly the cost complexity lower bound. Indeed, neither the cost upper bound of  nor the one of  matches the lower bound of Theorem 3.1, even when their additional hypotheses are satisfied.

### Computing the gradient of \(F(,)\)

Algorithm 1 requires computing a sub-gradient of \(F(,)\). Notably, we remark that this is needed for a generic \(^{KM}\), as \(}(t)\) might violate the fidelity constraints due to inaccurate estimations or degenerate cases in which the multi-fidelity constraints are attained with equality. In this section, we provide an efficient algorithm for the computation of the sub-gradient that arises from a more in-depth study of the function \(F(,)\). To this end, we begin by presenting some intermediate characterization of the functions \(f_{i,j}(,)\) that define \(F(,)\).

**Lemma 4.2**.: _Consider \(^{KM}\) and \(_{K M}\). Define for \(k[K]\),_

\[_{k}^{*}:=*{argmin}_{}_{m=1}^{M}_ {k,m}(_{k,m},+_{m})+d^{+}(_{k,m},-_{m})}{ _{m}}\]

_Then, the following holds:_

\[f_{i,j}(,) =_{k\{i,j\}}_{m=1}^{M}_{k,m}(_{k,m },_{k}^{*}+_{m})+d^{+}(_{k,m},_{k}^{*}-_{m})}{_{m}} _{j}^{*}>_{i}^{*}\] (6) \[f_{i,j}(,) =_{}_{k\{i,j\}}_{m=1}^{M}_ {k,m}(_{k,m},+_{m})+d^{+}(_{k,m},-_{m})}{ _{m}} \] (7)

We further introduce \(_{i,j}^{*}\) as the minimizer in the expression in (7) 6. When \(_{}\), we can show that \(_{k}^{*}=_{k,M}\) for all \(k\) and due to the multi-fidelity constraints the expression in (6) is always equal to zero. Hence in both cases \(f_{i,j}(,)\) is equal to the expression in (7), which can be rewritten

\[f_{i,j}(,)=(_{i,M}_{j,m})_{k \{i,j\}}_{m=1}^{M}_{k,m}(_{k,m},_{i,j}^{*}+_ {m})+d^{+}(_{k,m},_{i,j}^{*}-_{m})}{_{m}}\.\]

This quantity can be interpreted as the transportation cost for making \(_{j,M}\) larger than \(_{i,M}\). When \(_{i}\) or \(_{j}\), if \(_{j}^{*}>_{i}^{*}\), \(f_{i,j}(,)\) is equal to the expression (6) that can be interpreted as a transportation cost with an alternative in which \(i\) and \(j\) satisfy the multi-fidelity constraints.

Using this preliminary result, we provide a precise expression for the sub-gradient of \(F(,)\).

**Theorem 4.3**.: _Consider \(^{KM}\) and \(_{K M}\) such that \(F(,)>0\) holds. Let \((i,a)[K]^{2}\) be a pair of arms that attains the max-min value in Equation (2). Then a sub-gradient \( F(,)\) of \(F(,)\) w.r.t. to \(\) is given by one of the two following expressions: for \(j\{a,i\}\) and \(m[M]\),_

\[ F(,)_{j,m} =(_{j,m},_{i,a}^{*}-_{m})+d^{-}(_{j,m}, _{i,a}^{*}+_{m})}{_{m}} _{i}^{*}_{a}^{*}\,,\] (8) \[ F(,)_{j,m} =(_{j,m},_{j}^{*}-_{m})+d^{-}(_{j,m}, _{j}^{*}+_{m})}{_{m}} \] (9)

_That sub-gradient \( F(,)\) is \(0\) in all the remaining \(KM-2M\) dimensions._

Theorem 4.3 shows how to compute a sub-gradient of \(F(,)\) under the mild assumption that \(F(,)>0\).7 More specifically, it is sufficient to consider the pair \((i,a)\) that attains the max-minvalue in Equation (2), and then test whether \(_{i}^{*}_{a}^{*}\) holds to choose which expression to use among Equations (8) and (9). An interesting interpretation of the sub-gradient expression is that, whenever \(_{i}^{*}_{a}^{*}\), the sub-gradient is pointing toward the direction of the space that aims at increasing the information to discriminate the eventual optimality of arm \(a\) against \(i\). On the other hand, whenever \(_{a}^{*}>_{i}^{*}\) holds, the sub-gradient points towards the direction of minimizing errors in the multi-fidelity constraints for arm \(i\) and arm \(a\) (if any).

**Computing the sub-gradient efficiently** To conclude, we notice that to compute a sub-gradient, it is required to compute \(_{k}^{*}\) for all arm \(k\) and \(_{i,j}^{*}\) for all pairs of arms such that \(_{i}^{*}_{j}^{*}\). Using their definitions, this will require solving \((K^{2})\) one-dimensional optimization problems of functions that involve \((M)\) variables, which leads to a computational complexity which is roughly \((K^{2}Mn)\), where \(n\) is the number of iterations of the convex solver. In the following, we show that it is possible to exploit the structure of the \(f_{i,j}\)'s to obtain an algorithm whose total complexity is \((K^{2}M^{2})\) and that does not suffer from any approximation error due to the optimization procedure. Specifically, we now present a result that shows how to compute \(_{i,j}^{*}\). A similar result holds also for \(_{k}^{*}\) and is deferred to Appendix C.

**Lemma 4.4**.: _Consider \(^{KM}\) and \(_{K M}\) such that \(f_{i,j}(,)>0\). Suppose that \(_{i}^{*}_{j}^{*}\) holds. Then, there exists a unique minimizer \(_{i,j}^{*}()\) of Equation (7) which is the unique solution of the following equation of \(\):_

\[=}_{m}}{_{m}}( _{a,m}()+_{m}}{v(-_{m})}+_{a,m}()-_{m}}{v(+_{m})})}{_{a\{i,j\}} _{m}}{_{m}}(_{a,m}())}+_{a,m}())})},\] (10)

_where \(_{a,m}(x)=\{x_{a,m}+_{m}\}\) and \(_{a,m}(x)=\{x_{i,m}-_{m}\}\)._

From Lemma 4.4, to compute \(_{i,j}^{*}\) it is sufficient to find the unique solution to the fixed point equation given in (10). To do this efficiently, we observe that the right hand side of Equation (10) depends on \(\) only for the presence of the indicator functions \(_{a,m}()\) and \(_{a,m}()\), which can only take a finite number of values. Hence, it is sufficient to evaluate the right-hand side at an arbitrary point within a given interval where the values of the indicator functions do not change. If the resulting value is within the considered interval, then this value is our fixed point. Since there are at most \((M)\) candidate fixed points, this procedure takes at most \((M^{2})\) steps.

**Computational complexity remark** It follows that the per-iteration computational complexity of Algorithm 1 is \((K^{2}M^{2})\). The computationally efficient technique explained above indeed applies not only to the sampling rule but also to the stopping and the decision rules.8

## 5 Numerical experiments

We conclude this work by presenting numerical simulations whose goal is to show the empirical benefits of our approach. We compare MF-GRAD against IISE , and the gradient approach of  that simply does BAI using samples collected at fidelity \(M\). We will refer to this additional baseline as GRAD. In the following, we avoided the comparison with the multi-fidelity algorithms in  as we ran into issues when doing experiments. We elaborate more on this point in Appendix D.6, where we provide numerical evidence of the fact those algorithms might fail at stopping, together with an argument that shows a mistake in the proofs of .

Given this setup, first, we test all methods on a \(4 5\) multi-fidelity bandit with Gaussian arms that have been randomly generated, using a risk parameter \(=0.01\). Due to space constraints and for the sake of exposition, we refer the reader to Appendix D.1 for the value of \(\), \(\)'s and \(\)'s and details on the stopping rules calibration. We report the empirical distribution of the resulting cost complexities in Figure 1. As one can verify, MF-GRAD obtains the most competitive performance. Experiments on additional \(4 5\) bandits that are reported in Appendix D.3 provide a similar conclusion.

Furthermore, to illustrate the sub-optimality of IISE and GRAD from an intuitive perspective, we test our algorithm on a simple \(5 2\) instance that allows to easily understand why existing methods underperform MF-GRAD. Specifically, we consider \(_{i}=[0.4,0.5]\) for all \(i\), \(_{5}=[0.5,0.6]\)\(=[0.5,5]\), \(=[0.1,0]\) and we report the cost complexity of the three algorithms in Figure 2. In this case, we can prove that the optimal fidelity is sparse on fidelity \(m=1\) for \(i\), and on fidelity \(m=2\) for arm \(5\). Furthermore, thanks to the symmetry of the problem, it is possible to show that \(_{i}^{*}=[0.09621,0]\) for all \(i\), and \(_{5}^{*}=[0,0.61516]\) (see Appendix D.1). As one can see, IISE obtains the worst performance in this domain. The reason is that the concept of optimal fidelity on which IISE relies is sub-optimal (i.e., according to the design principle of IISE, the optimal fidelity is \(m=2\) for all arms), and the algorithm, in practice, will discard sub-optimal arms using samples that have been collected only at fidelity \(m=2\). Nevertheless, this will only happen after a first period in which IISE tries to exploit (unsuccessfully) data at fidelity \(m=1\). GRAD, on the other hand, obtains sub-optimal performances since although most of the budget should be spent on fidelity \(2\) (as \(^{*}{}_{5,2}=0.61516\)), it never pulls the cheapest (and optimal) fidelity for arms \(i\). Finally, MF-GRAD, on the other hand, obtains the most competitive performance since, as learning progresses, its empirical cost proportions eventually approach the one prescribed by \(^{*}\). To verify this behavior, we removed the stopping rule from MF-GRAD, and let the algorithm run for \(10^{5}\) iterations. In Figure 3, we report the entire evolution of the cost proportions during learning. As one can appreciate, at the end of this process, the empirical cost proportions of MF-GRAD are approaching the one described by \(^{*}\). 9. Finally, we also refer the reader to Appendix D for additional results (e.g., additional domains, smaller regimes of \(\)) and further insights.

Figure 3: Empirical cost proportions of MF-GRAD for \(100000\) iterations on the \(5 2\) bandit model. Results are average over \(100\) runs and shaded area report \(95\%\) confidence intervals. Empirical cost proportions of a certain arm are plotted with the same color. Cost proportions at fidelity \(1\) and \(2\) are visualized with a circle and a squared respectively.

Conclusions

For fixed-confidence best arm identification in multi-fidelity bandits, we presented a lower bound on the cost complexity and an algorithm with a matching upper bound in the regime of high confidence. The algorithm uses features of the lower bound optimization problem in order to compute its updates efficiently. Unlike prior work, it does not require any assumption or prior knowledge on the bandit instance. Our work also confirmed the existence in most cases of an "optimal fidelity" to explore each arm in the asymptotic regime, and revealed that the intuitive such notions proposed in prior work were inaccurate. Yet, our algorithm does not need to identify these optimal fidelities in order to be asymptotically optimal.

This raises the following question: could the performance of the algorithm be enhanced by exploiting the sparsity pattern? We conjecture that estimating the optimal fidelities accurately may actually be harder than identifying the best arm. However, leveraging some sufficient conditions for \(w^{*}_{a,m}=0\) (such as the ones given in Proposition B.6) to eliminate some fidelities and reduce the support of the forced exploration component of the algorithm seems a promising idea. A limitation of our current analysis is that it only provides asymptotic guarantees in the high confidence regime, although our experiments reveal good performance for moderate values of \(\). In future work, we will seek a better understanding of the moderate confidence regime . To this end, we may leverage some proof techniques from other works using online optimization that obtain finite-time bounds [4; 5]. On the lower bound side, while \(C^{*}()\) essentially scales with \(K\) due to the sparsity pattern, an interesting open question is whether there is a worse case \((KM)\) scaling in the moderate confidence regime, indicating that all fidelities do need to be explored at least a constant amount of times.