# Mixture of Nested Experts: Adaptive Processing of Visual Tokens

Gagan Jain\({}^{}\) Nidhi Hegde\({}^{}\) Aditya Kusupati\({}^{}\)

Arsha Nagrani\({}^{}\) Shyamal Buch\({}^{}\) Prateek Jain\({}^{}\) Anurag Arnab\({}^{}\) Sujoy Paul\({}^{}\)

\({}^{}\)Google DeepMind University of Washington

{jangagan,sujoyp}@google.com

equal contribution

###### Abstract

The visual medium (images and videos) naturally contains a large amount of information redundancy, thereby providing a great opportunity for leveraging efficiency in processing. While Vision Transformer (ViT) based models scale effectively to large data regimes, they fail to capitalize on this inherent redundancy, leading to higher computational costs. Mixture of Experts (MoE) networks demonstrate scalability while maintaining same inference-time costs, but they come with a larger parameter footprint. We present Mixture of Nested Experts (MoNE), which utilizes a nested structure for experts, wherein individual experts fall on an increasing compute-accuracy curve. Given a compute budget, MoNE learns to dynamically choose tokens in a priority order, and thus redundant tokens are processed through cheaper nested experts. Using this framework, we achieve equivalent performance as the baseline models, while reducing inference time compute by over _two-fold_. We validate our approach on standard image and video datasets - ImageNet-21K, Kinetics400, and Something-Something-v2. We further highlight MoNE's adaptability by showcasing its ability to maintain strong performance across different inference-time compute budgets on videos, using only a single trained model.

## 1 Introduction

Visual tokens, the fundamental building blocks of image and video representations, often exhibit strong inter-dependencies, spatially in images and spatio-temporally in videos. This offers a potential avenue for optimization in visual processing, as processing every token with equal emphasis may not be necessary for achieving optimal results. Traditional Vision Transformer (ViT)  and Video Vision Transformer (ViViT)  based models, however, process all tokens with equal emphasis, disregarding this inherent codependency and leading to unnecessary computational burden. This becomes a major bottleneck when deploying these models in real-world scenarios, where computational resources may be limited and real-time processing is required.

To this end, conditional computation has become a promising line of research to increase the capacity of a network, while only conditionally activating a part of it during inference. Sparse Mixture of Experts (MoEs) was initially popularized for Natural Language Processing (NLP) ,but it has been gaining attention for furthering conditional computation ideas in vision  as well. While MoEs bring in improved performance at a given inference cost, they also increase the overall parameter count, leading to increased storage requirements. Moreover, these works rely on experts that have the same parameter count and compute, limiting their ability to reduce computational costs without resorting to skipping tokens entirely.

In this work, we devise the **Mixture of Nested Experts (MoNE)** framework, which provides a scalable approach to conditional computation, bringing in significant reductions at inference time, while working with the same parameter space as the baseline model. MoNE draws inspiration from nested architectures [47; 31; 53], particularly MatFormer , that learns multiple representations of the same data with varying levels of details, based on structured slices of the parameter space. MoNE employs these structured nested models as experts in the MoE framework (without increasing parameter count), and learns a network to route tokens to these experts. We explore various design choices and present an effective recipe for allocating compute to experts, assigning tokens to experts, and training the MoNE framework. For the assignment operation, we propose Expert Preferred Routing (EPR), a routing algorithm that greedily assigns tokens to experts under capacity constraints based on router predictions. Figure 1 shows token importance as perceived by MoNE. We propose the following **three primary contributions**:

1. We introduce the novel Mixture of Nested Experts (MoNE) framework to dynamically allocate computational resources for Vision Transformer (ViT) based models.
2. Given a fixed parameter count, MoNE offers the flexibility of learning networks at much lower FLOPs (\( 2.3\) on video datasets) and real-time latency (\( 2\)), while being quality neutral.
3. Rigorous experiments show that MoNE works well for both image and video transformers, and visualizations depict that tokens routed to larger experts correlate well with regions of interest.

## 2 Related Work

Transformers  have become the de-facto architecture for processing data across multiple modalities spanning language [10; 36], images [20; 17], video [2; 49] and audio  and combinations thereof . Consequently, there have been numerous efforts to improve the efficiency of transformers to make them more amenable for deployment in real-world applications . These include approaches like efficient approximations of attention [12; 48], local attention [32; 4; 13] and reducing the number of tokens in the transformer [40; 29; 8] among others. Our work focuses on conditional computation [5; 34], observing that some input tokens are easier to process than others, and therefore require less computation during inference.

Mixtures-of-Experts (MoE) transformers learn to route tokens to one of multiple expert MLPs [42; 22]. Although such models conditionally process input tokens, each expert has the same parameter- and FLOP-count, meaning that the total computation is constant for each input. More relevant to our approach, Mixture of Depths  extends the routing logic of MoE to conditionally skip an expert completely, thus total computation for each input varies dynamically. Completely skipping tokens being a hard unretrievable decision, our work chooses from an array of nested network, which effectively process information and help to stabilize training by getting rid of discontinuities.

Nested architectures [47; 31; 53] on the other hand, learn hierarchical representations of the input, where the first \(k\) hidden dimensions encode the most relevant information. This allows to extract multiple models with varying inference compute from a single trained model, similar to 'Mix-n-Match' in . However, these models do not process tokens adaptively. Our model, in contrast, consists of a learned router which dynamically routes tokens to experts of different hidden dimensions based on the given compute constraints. Therefore, instead of requiring the user to select the hidden dimensions of

Figure 1: **MoNE’s learned token importance: From _left to right_, fewer image tokens are processed using the full model – to fit a compute budget – by an increasing threshold on MoNE’s router logits.**

each transformer layer, our model only needs a single compute constraint input. Moreover, we show experimentally the superior accuracy-efficiency trade-offs achieved by our approach.

We note that other conditional computation approaches include "early exiting" [46; 41; 21; 28] such that the processing of "easy inputs" terminates before passing through all layers of the transformer. In addition, the ACT  algorithm was proposed for recurrent neural networks, and uses a "ponder cost" to learn a "halting score" for when to stop processing a particular input. This has since been extended to recurrent transformers , and also to each individual token in a transformer [52; 51], thus adaptively determining which tokens in a transformer to process. In contrast, our approach does not drop tokens, rather processes them with smaller nested models. This allows us to retain most of the information, and hence dampen the effect of irrecoverable decisions. We experimentally verify that our adaptive approach offers strong compute-performance trade-offs. Flextron  is a concurrent work, which looks at elastic inference, specified by user latency needs, with a focus on language modeling. Unlike Flextron, MoNE is guaranteed to learn models bounded by the specified latency needs and is able to learn from a single training phase, without using a surrogate model.

## 3 Preliminaries

Here, we discuss the concept of _nested models_, on which we build Mixture of Nested Experts (MoNE), followed by a discussion about Mixture of Nested Experts (MoE), and its differences from MoNE.

### Nested Models

For the purposes of this work, we use the Vision Transformer (ViT)  as an example of a full model, from which nested submodels can be derived. Inspired by MatFormer , we define these submodels for every layer of the network, for both Self-Attention and MLP (see Appendix A.1). The key idea is that in a feature projection operation \(\), where \(=[_{[;]},_{[]}]\), and \(_{[;]}\) denotes "slicing" the first \(\) dimensions, we can extract a partial projection \(_{[;]}_{[;]}\). This can be done for any projection in the transformer, and we can extract smaller models from it. We refer to these as nested models, and \(}{{m}}\) as the nested model dimension. This is shown in Figure 1(a). The _Extract_ operation extracts the first \(}{{m}}\) features and applies the corresponding projection sub-matrix to it, while the _Pad_ operation pads it back to full dimension \(D\) before residual connections and LayerNorm. While MatFormer applies the nested structure only to the hidden dimension of the MLP layer, in our approach we extend it to the in- and out-projections of both the Self-Attention (SA) and MLP layer. In the SA block, irrespective of the sub-model used in the in-projections, it is always projected to the model dimension \(D\) for the \((^{T})\)V operation. The same thing is performed in MLP, where the hidden dimension is always \(4D\), as in ViT, irrespective of in-out-projection dimension.

We extract \(E\) nested models with exponentially-spaced model dimensions. Therefore, for a typical value of \(E=4\), the model dimension for the nested models are \([,,,D]\). Note that while we build upon the idea of nested models from MatFormer, we do not share their training strategy which involves joint optimization through a weighted loss over these submodels. In contrast, we treat these nested models as distinct experts with varying compute requirements. The Mixture of Nested Experts (MoNE) framework (described in detail in Sec. 4.1) then dynamically routes input tokens to these nested experts based on their information content, with the idea that more informative tokens should be processed by larger (and thus more computationally expensive) nested models.

### Mixture of Experts

A Mixture of Experts (MoE) layer in a transformer can be represented as \(()=_{i=1}^{E}g()_{i}e_{i}()\), where \(E\) is the number of experts, \(e_{i}()\) are the expert models each having their own parameters, \(g:^{D}^{E}\) is the routing/gating function, which decides the experts which should process \(\). Note that \(g\) is sparse with only \(k<<E\) non-zero terms. During inference, only those experts are active.

MoE strictly increases the parameter count, but maintains the same inference FLOPs by setting \(k=1\). However, it still needs to process all tokens with the same pre-defined compute. In contrast, in MoNE, we do not extend the parameter count of the model, due to the nesting structure (see Sec. 3.1), and dynamically choose a nested expert during inference. Unlike in MoE, where all experts have the same capacity, in MoNE with \(k=1\) always, \(e_{i} e_{i+1}\), which allows us to dynamically allocate compute.

## 4 Methodology

In this section, we describe the details of our Mixture of Nested Experts (MoNE) framework for efficient inference. We assume a Vision Transformer (ViT)  based architecture for our approach, and then extend it to Video ViT (ViViT)  as well.

### Mixture of Nested Experts (MoNE)

**Tokenization:** In this paper, as our primary focus is images and videos, the model input is in \(^{H W 3 T}\), where \(T=1\) for images and \(T>1\) for videos. After tokenization, the input to the transformer is \(^{D N}\) where \(N\) is the number of tokens, and \(D\) their model dimension. For images, we have \(N=H/p_{h} W/p_{w}\), and for video, \(N=T/p_{t} H/p_{h} W/p_{w}\), where \(H,W,T\) are the input height, width and duration respectively. \(p_{h}\), \(p_{w}\) and \(p_{t}\) are the patch sizes along these respective dimensions. We use the ViT  and ViViT  architectures to tokenize images and videos respectively, obtaining a list of tokens \(=\{_{i}\}_{i=1}^{N}\).

**MoNE Block:** The Mixture of Nested Experts (MoNE) framework is a dynamic routing mechanism that processes visual tokens using nested models with varying computational capacities, instead of processing all tokens with the full model. A pictorial representation of the model is presented in Figure 1(b). Let \(^{l}=\{^{l}_{1},,^{l}_{E}\}\) denote the nested blocks at a certain layer \(l\) with increasing parameter sizes, \(^{l}_{E}(.)\) being the full model block. A router network decides the appropriate nested block to use for every token. Hence information from tokens of different model dimension interact with each other. This is enabled by performing self-attention at the full model dimension \(D\) as discussed before. For each token \(_{i}\), a router produces a probability distribution over the \(E\) nested experts, \(_{i}=(_{}_{i}+_{})\), where \(_{}\) and \(_{}\) denote the router weights and bias respectively.

These router predictions are sent to an assignment algorithm, which assigns every token to a single appropriate nested expert. Based on the assignments, we update the features for the \(i^{th}\) token in the \(l^{th}\) layer as follows -

\[_{i}^{l+1}=_{i}^{l}+(_{i,j}^{l}+1 )^{,l}_{j}(_{i}^{l}) _{i}^{l}=_{i}^{l}+^{,l}_{j}(_{i}^{l})\] (1)

Figure 2: (a) **Nested model:** Partial in- and out-projections in the SA and MLP layers create nested models. \(m\) controls the parameter count and the FLOPs of nested models. The self-attention information exchange happens at the full model dimension \(D\), MLP dimension is set to \(4D\) as in ViT. (b) **Mixture of Nested Experts (MoNE)**: Each token \(\) is routed to a nested network, denoted by different model dimension in the diagram. Here \(_{i}\) gets routed to a nested model with model dimension \({}^{D}\)/4, whereas \(_{i+1}\) gets to the full model. The information exchange between these tokens of different dimension happens in the self-attention block, where they are always projected to the same dimension. The router weights are also multiplied with the features for proper flow of gradients. A lighter color in the weight matrix indicate a sliced matrix to construct the nestedness.

where the \(j^{th}\) nested expert is chosen by the Expert Preferred Router [EPR(.)] algorithm for the \(i^{th}\) token as per Eq. 2:

\[j^{*}=i;\{_{i}^{l}\}_{i=1}^{N}\] (2)

Note that the multiplication of the router predictions with the model output in Eq. 1 allows gradient propagation through the router weights. We also introduce a learnable parameter \([0,1)\), initialized to \(0\), which ensures proper gradient flow during the initial training stages, specifically during finetuning from a pre-trained MatFormer model. Without scaling, a low initial router prediction would dampen the block output, whereas the initial multiplicative factor being 1 ensures a stable starting point.

**Features and Loss:** The feature of the last layer \(_{i}^{L}\) is used for downstream applications. For classification tasks, we apply global average pooling on all the token features and apply a linear classifier layer to predict the categories.

### Token to Nested Expert Assignments

Within the MoNE framework, the routing strategy is crucial for achieving an optimal balance between performance and computational efficiency. Traditionally there are two primary routing strategies - token choice  and expert choice . In token-choice routing, the router predicts the probability distribution over the available experts, and picks the expert with the highest probability. However, this can suffer from load balancing issues, with most of the tokens being routed to one or few experts. Hence, inference time compute is only bounded by the compute of the full model. On the other hand, in expert choice routing, each expert selects the top-\(k\) tokens with the highest preference for that expert. This guarantees perfect bounds on computation. Potential conflicts due to token selection by multiple experts are resolved by prioritizing based on model size.

Formally, we consider a given distribution of nested models applied to the tokens, represented as \(=\{c_{1},,c_{E}\},,_{i}c_{i}=1\), which we call the capacity distribution over the nested models. The method for obtaining a suitable capacity distribution, given the inference time compute requirements, will be discussed in Sec. 4.3. Given router probabilities \(_{i}\) for \(N\) tokens across \(E\) experts, we employ an Expert Preferred Routing algorithm (Algorithm 1). This is a greedy assignment approach that gives higher preference to larger nested models, aiming to identify the most important tokens first. We begin by examining the router predictions for the biggest to the smallest model, assigning \(k_{j}= c_{j}N\) of the remaining tokens to \(j^{th}\) nested model. Any remaining tokens, arising from integer packing constraints, are assigned to the smallest model. Algorithm 1 presents the proposed Expert Preferred Routing (EPR) algorithm.

```
0:\(^{E N}\) (router predictions), \(\) (capacity distribution, s.t., \(^{T}\) = 1),
0:\(M\{1,,E\}^{N}\) (nested model index)
1:\(M+1_{N}\) Default assignments to the smallest model
2:for\(j=E\) to \(1\)do
3:\(k_{j}= c_{j} N\)
4:\(Ik([j,],k_{i})\) Returns value and indices of Top-K
5:\(M[I] j\)\([,I] 0\)Null out assigned ones
6:endfor
7:return\(M\) ```

**Algorithm 1** Expert Preferred Routing (EPR)

### Capacity Distribution Across Experts

The Expert Preferred Routing (EPR) as described in Section 4.2 needs the individual expert's capacity bounds \(c_{i}\) to be specified. To get this, we define a metric called the effective capacity : \(e_{e}=_{i=1}^{E}c_{i}d_{i}/D\), where \(d_{i}=}{{2}}e^{-i}\) is the model dimension of the \(i^{th}\) nested model. Given a certain inference FLOP requirement, we can translate that to an equivalent effective capacity \(e_{c}\). Since every token gets processed through exactly one nested expert, this along with the given budget imposes two constraints on the unknown capacity distribution \(\). However, since the individual expert capacities vary log-linearly, multiple distributions \(\) can lead to the same \(e_{c}\) for \(E>2\) and it is non-trivial to choose one over the other. MoEs generally use auxilliary loss functions [39; 42] to promote equal usage of experts. But in MoNE, that would render a certain fixed capacity, missing out on the flexibility that the framework provides to function with any capacity (as depicted later in Figure 4(b)). Hence, we invoke intuitive constraints to solve for \(\). Specifically, we incentivize the usage of larger models, while also adding an entropy term to ensure uniformity of capacities across experts. Given these constraints, we solve the following optimization problem:

\[&_{i=1}^{E}}{^{i-1}}-_{i=1}^{E}c_{i} c_{i}\\ &_{i=1}^{E}c_{i}=1_{i= 1}^{E}}{2^{E-i}}=e_{c} 0 c_{i} 1 i \{1,...,E\}\\ & 0<e_{c}<1, E,>1, >0\] (3)

In practice, we set \((,)\) to \((10,2)\) and use a Sequential Least SQuares Programming (SLSQP) optimizer to solve Eq. 3 for the capacity distribution \(\), which is then used by EPR (Algorithm 1) to get token to expert mappings. We empirically verify these choices in Section 6.

### Videos

MoNE can be seamlessly adapted for video-based tasks. In videos, there exists another dimension - time - which adds to the significant redundancy in the tokens. Given the large number of tokens that can be obtained from a video, the computational costs grow drastically. To tackle this problem, works in literature factorize computation along space and time [2; 6], perform local windowed computation , etc. MoNE being a token based approach, directly extends to video encoders.

For video processing, we leverage the Factorized Encoder architecture of ViViT . This architecture employs two distinct transformers: spatial and temporal. After tokenization, each temporal index yields a set of tokens representing information from local spatio-temporal neighborhoods. These spatial tokens interact within their temporal index for \(L_{s}\) layers, culminating in a single global token per index. Subsequently, a temporal transformer processes these global tokens across \(L_{t}\) layers. Given that the spatial transformer significantly dominates computational costs in this model, we integrate MoNE into the spatial component while maintaining full capacity for the temporal transformer. The router predicts expert assignments for all temporal frames independently, which are then consumed by the EPR(.) algorithm to produce frame-wise expert assignments.

## 5 Results

In this section, we empirically evaluate MoNE on multiple datasets spanning images and videos for different model sizes, assess its adaptability to stringent FLOP constraints, and depict real-time latency gains achieved by MoNE during inference.

**Implementation details:** We empirically evaluate MoNE on image and video classification. For image classification, we train the network with random initialization. As for video classification, we follow previous literature and start from a pre-trained MatViT  model due to the inherent nested structure required in MoNE. We follow the joint training strategy of MatViT, with separate losses an all model granularities. We implement MoNE on JAX  using BigVision  for image classification and Scenic  for video classification. We follow the AugReg  training strategy to train all our image classification models. For video classification tasks, we inherit all augmentations and hyperparameter values directly from the ViViT  paper.

For all experiments in this section, we place a single router at the first transformer layer, and propagate the router decisions to all the layers. We also multiply the router predictions (Eqn 1) to all layers, which ensures differentiable paths through the router network in all layers and allows the more evolved features from later layers to influence router learning. We also perform analysis of router placement in Section 6.

**Baselines:** We first compare with MatViT's nested models. As mentioned in the paper , we perform joint training over all four nested models that we consider in this work - \(\{,,,D\}\). MatViT is equivalent to MoNE, with a deterministic router to pass all tokens to the same nestedmodel. We show that adaptively mixing tokens with different model dimensions performs much better across datasets and tasks. We also compare with Mixture of Depths (MoD) , which is also a token routing algorithm, but proposed for language tasks. MoD takes the extreme decision of either processing or skipping for every token in a layer. MoNE, on the other hand, makes fuzzy decisions to choose intermediate-sized models, instead of skipping, which helps to retain significant information at the expense of low compute. We adopt the best reported MoD configuration: processing \(12.5\%\) of tokens every other layer while processing all tokens in the remaining layers.

We also emphasize that MoNE acts as a complementary framework to traditional MoEs like Sparse VMoE , and inference-time optimization techniques like Token Merging (ToMe) . We present an extended discussion and further results in Appendix A.2, comparing with other adaptive baselines and validating the compounded savings by applying ToMe on MoNE.

**Images:** First, we evaluate MoNE on ImageNet-21k  classification using ViT. We experiment with S, B, and L models to showcase the efficacy of MoNE across model sizes. As ImageNet-21k can have multiple labels for an image, we report the commonly used precision@1 metric. Figure 3 shows the results for all the models on ImageNet-21k. MoNE performs much better than MatViT's nested models and MoD, specifically in the low FLOPs regimes. MoNE achieves comparable performance to baselines with around \(2\) reduction in FLOPs.

Following the literature on language models [37; 27], we experimented with isoFLOPs training, which involves training for the same number of FLOPs as the baseline models. Since MoNE models have fewer FLOPs compared to their ViT counterparts, they require more training epochs to achieve the same total training FLOPs. We conducted this experiment on the S/16 model (see Figure 2(a)) and observed additional improvements in MoNE's performance, particularly for the lower FLOPs models.

**Videos:** Since video models rely on heavy pre-training , we first train a baseline model with nested structure on the benchmark datasets - Kinetics-400  and Something-Something-v2 (SSv2) . We use the ViViT Factorized Encoder B/16 model  for our experiments and consistently report the 8x1 test accuracy, averaging predictions over 8 temporal clips . Figure 4 illustrates the results of the MoNE framework, significantly outperforming the individual nested models. MoNE offers \(2\) - \(3\) reduction in FLOPs compared to the ViViT baseline, without any accuracy drop (On SSv2, the

Figure 4: **Video classification**: MoNE vs. baselines on video datasets. Finetuning with the isoFLOPs training regime leads to matching baseline with \(>2\) FLOP improvement.

Figure 3: **Image classification:** Performance comparison of MoNE with baselines on ImageNet-21k for different model sizes. MoNE performs significantly better than MatViT and Mixture-of-Depth (MoD) and even benefits from isoFLOPs training (see fig a).

FLOPs for MoNE are 162.8 vs 376.3, with similar accuracy - 64.6 vs 64.4). We always do isoFLOPs training while fine-tuning these models. We attribute the higher compute gains compared to images due to the greater (spatial and temporal) redundancy in videos, which MoNE exploits well.

**Inference time capacity adaptation:** Capacity adaptation during inference is crucial, as the inference time budget is often dynamic, changing based on user needs. Ideally, a model should adjust with little to no retraining. To evaluate this ability, we test how MoNE, trained at a specific effective capacity (\(e_{c}\)) performs when evaluated at other capacities. Fig. 5 presents the results for image and video classification. We observe that the model adapts well to nearby capacities. However, as expected, its ability declines with extreme shifts in the capacity budget between train and eval. The performance degradation is steeper while adapting a model trained at high capacity to low capacity. We also note that the performance degrades more gracefully in videos than on images, presumably due to the larger temporal redundancy.

To enhance model adaptability, we train a model with the capacity sampled uniformly at random from \(\{0.15,0.25,,0.95\}\) at each training step. The results on SS-v2 (Figure 4(b)) demonstrate our framework's strong capability to adapt to any inference-time budget using a single model. It is interesting to note that the training FLOPs of this adaptively trained model are equal to those of a baseline model (isoFLOPs training). The model adapts extremely well even to capacities that are significantly different (\(\{0.2,0.3,\}\)) from those sampled during training.

**Real Time Latency Gains**: In addition to the theoretical FLOP gains, Table 1 presents the real-time latency/throughput gains of MoNE-based ViViT model as compared to its baseline variant. The absolute wall clock times and throughput are compared on a single V100 GPU, achieving nearly two-fold improvement in both FLOPs as well as runtime, whilst maintaining accuracy.

Additionally, the variation of latency and throughput with FLOPs for varying model capacities of MoNE is depicted in Figure 5(a) to 5(d). The plots show that latency and throughput gains scales linearly with FLOPs reductions. It is important to note that inference gains depend heavily on implementation and while a simple high-level efficient implementation of our framework yields gains of this scale, we believe that further improvements can be obtained by optimizing a low-level GPU kernel implementation for MoNE.

In addition, it is worth noting that the proposed Expert Preferred Routing (EPR) in Algorithm 1 loops only over the number of experts, which is typically a small number and fixed to \(4\) in our framework. While the nature of the EPR algorithm does not allow parallelization of the computation any further, the time taken by the algorithm is negligibly small as compared to the total time taken by the model. For comparison, on a V100 GPU, the EPR algorithm adds just \(0.5\ ms\) to the forward propagation time of a ViT-B/16 model (\(190\ ms\)), accounting for \(<0.3\%\) of the total computation time.

   Method & FLOPs (G) & Throughput (clips/sec) & Latency (ms) & Top-1 Accuracy \\  ViViT-FE-B/16 & 376 & 15.8 & 129.2 & 64.4 \\ MoNE (\(e_{c}=0.3\)) & 162 & 30.7 & 65.5 & 64.6 \\   

Table 1: Real Time Latency and Throughput gains for MoNE on a single V100 GPU

Figure 5: **Capacity adaptation during inference:** Performance changes when a model trained at a certain capacity (denoted as \(\)) is evaluated at other capacities. The “Train Adaptive” plot for SSv2 denotes a single model evaluated at different inference-time budgets.

## 6 Router Analysis

In this section, we discuss, analyse and visualise the design choices in implementing the router network. We choose the SSv2 dataset for this analysis. We further provide an extended discussion in A.3, detailing on the choice of number of routers, associating router outputs with Task Difficulty, and understanding the implications of having of learnable router.

**Router Position:** As discussed before, we use a single router at the first layer, and propagate its decisions for all layers. While a delayed router might benefit from a more processed feature representation as input, this also diminishes the compute gains, as the initial layers operate at full capacity. We reason this choice by monitoring performance while placing the router at different layers in the network. As Figure 6(a) suggests, the gains through richer features from the later layers is outweighed by the shift in the curve to the right, and an equivalent capacity with our default router produces higher points on the curve.

**Number of Routers:** We vary the number of routers, placing them at different regular intervals in the network in Figure 6(b). The decision from one router is carried out until the next router block is encountered. We notice a clear downtrend in performance with increase in number of routers from being present in the first layer to being present in all layers. Intuitively, more routers demand learning more decisions, and the network has to adapt to these decisions, making optimization harder.

Figure 6: Wallclock realization of MoNE’s computational savings with varying effective capacities, depicted on the Something-Something-v2 dataset.

Figure 7: **Router Analysis:** Effect of router placement and learning on Something-Something v2.

**Comparison with Random Router:** We compare our learned router approach to a random router, which maps tokens to nested experts randomly, while still maintaining the capacity limits of each expert (\(c_{i}\)), as computed in Section 4.3. Results in Figure 6(c) suggests that with lower effective capacities, the random router performance degrades while the learned router still manages to understand relevant patterns from the input, thus upholding performance.

**Visualizing Important Tokens:** The above claim is further backed by visualizing the token importance during inference at a low effective capacity (\(e_{c}\)). We highlight the tokens selected by the largest expert, i.e., the full model on a few images in Figure 7(a). It can be easily observed that the tokens sent to the largest model correlate well with the regions of interest in the images. On videos (Figure 7(b)) as well, the highlighted regions across temporal stamps consistently track the regions of motion.

**Capacity Allotment:** Given a fixed input capacity \(e_{c}\), we demonstrate the superior performance of our heuristic-based allocation method (Section 4.3) compared to other approaches, as shown in Table 2. While the Proportionate allocation (assigning capacity inversely proportional to expert compute cost) and Uniform allocation (assigning equal capacity to all experts) show promising results, they lack the flexibility to adapt to varying budgets. Additionally, greedy approaches, such as allocating the entire budget to the largest expert and dropping other tokens (MoD style), or a greedy approach where the largest expert is assigned capacity such that all the remaining tokens are routed through the smallest expert, exhibit inferior performance.

## 7 Conclusion

In this work, we presented Mixture of Nested Experts (MoNE), a novel framework for adaptive processing of visual tokens by dynamically allocating computational resources to different tokens. Through a nested structure with shared parameters and the proposed expert-choice routing algorithm, MoNE achieves significant reductions in inference time (over two-fold) without sacrificing accuracy on benchmark image and video datasets. Future works can be centered around extending MoNE to denser tasks like object detection, captioning, etc.

**Limitations:** Extending this to auto-regressive decoding in LLMs is non-trivial, as this is designed primarily with an encoder architecture in mind. We leave this further exploration for future work.

**Societal Impact:** The MoNE framework dynamically allocates computational resources with a given budget, thereby significantly minimizing energy usage and carbon emissions during inference of vision models. MoNE can also play a role in democratization of AI, allowing broader access to trained models without the need for large resources.

    &  &  \\ Distribution & Proportionate & Uniform & MoD Greedy  & Greedy & MoNE & MoNE \\  Effective Capacity (\(e_{c}\)) & 0.27 & 0.47 & 0.4 & 0.4 & 0.3 & 0.4 \\ Accuracy & 64.3 & 64.6 & 63.9 & 64.2 & 64.2 & **64.6** \\   

Table 2: SSv2 Performance of different capacity distribution methods

Figure 8: **Tokens routed to the full model:** Highlighted regions are the tokens sent to the full model, while rest of the tokens are sent to the smaller nested models. (a) shows examples on images and (b) shows an example on a video at multiple temporal indices. As we can see, the necessary and important tokens are sent to the full model.