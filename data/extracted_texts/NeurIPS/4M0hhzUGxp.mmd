# Sufficient conditions for offline reactivation in recurrent neural networks

Nanda H Krishna

Mila - Quebec AI Institute

Universite de Montreal

nanda.harishankar-krishna@mila.quebec

&Colin Bredenberg

Mila - Quebec AI Institute

Mila - Quebec AI Institute

daniel.levenstein@mila.quebec

&Daniel Levenstein

Mila - Quebec AI Institute

McGill University

blake.richards@mila.quebec

&Blake Aaron Richards

Mila - Quebec AI Institute

McGill University

blake.richards@mila.quebec

&Guillaume Lajoie

Mila - Quebec AI Institute

Universite de Montreal

g.lajoie@umontreal.ca

###### Abstract

During periods of quiescence, such as sleep, neural activity in many brain circuits resembles that observed during periods of task engagement. However, the precise conditions under which task-optimized networks can autonomously reactivate the same network states responsible for online behavior are poorly understood. In this study, we develop a mathematical framework that outlines sufficient conditions for the emergence of neural reactivation in circuits that encode features of smoothly varying stimuli. We demonstrate mathematically that noisy recurrent networks optimized to track environmental state variables using change-based sensory information naturally develop denoising dynamics, which, in the absence of input, cause the network to revisit state configurations observed during periods of online activity. We validate our findings using numerical experiments on two canonical neuroscience tasks: spatial position estimation based on self-motion cues, and head direction estimation based on angular velocity cues. Overall, our work provides theoretical support for modeling offline reactivation as an emergent consequence of task optimization in noisy neural circuits.

## 1 Introduction

Neural circuits in the brain are known to recapitulate task-like activity during periods of quiescence, such as sleep . For example, the hippocampus "replays" sequences of represented spatial locations akin to behavioral trajectories during wakefulness [2; 3; 4]. Furthermore, frontal [5; 6], sensory [7; 8], and motor  cortices reactivate representations associated with recent experiences; and sleep activity in the anterior thalamus  and entorhinal cortex  is constrained to the same neural manifolds that represent head direction and spatial position in those circuits during wakefulness.

This neural reactivation phenomenon is thought to have a number of functional benefits, including the formation of long term memories [12; 13], abstraction of general rules or "schema" , and offline planning of future actions [15; 16]. Similarly, replay in artificial systems has been shown tobe valuable in reinforcement learning, when training is sparse or expensive , and in supervised learning, to prevent catastrophic forgetting in continual learning tasks . However, where machine learning approaches tend to save sensory inputs from individual experiences in an external memory buffer, or use external networks that are explicitly trained to generate artificial training data , reactivation in the brain is autonomously generated in the same circuits that operate during active perception and action. Currently, it is unknown how reactivation can emerge in the same networks that encode information during active behavior, or why it is so widespread in neural circuits.

Previous approaches to modeling reactivation in neural circuits fall into two broad categories: generative models that have been explicitly trained to reproduce realistic sensory inputs , and models in which replay is an emergent consequence of the architecture of network models with a particular connectivity structure [21; 22] or local synaptic plasticity mechanism [23; 24; 25; 26]. Generative modeling approaches have strong theoretical guarantees that reactivation will occur, because networks are explicitly optimized to provide this functionality. However, modeling approaches that argue for _emergent_ reactivation typically rely on empirical results, and lack rigorous mathematical justification.

In this study, we demonstrate that a certain type of reactivation--diffusive reactivation--can emerge from a system attempting to optimally encode features of its environment in the presence of internal noise. We observe that continuous-time recurrent neural networks (RNNs), trained to optimally integrate and track perceptual variables based on sensations of change (angular velocity, motion through space, etc.), will naturally exhibit reactivation during quiescent states (when subject to noise but in the absence of perceptual inputs). We explain these phenomena by demonstrating that noise compensation dynamics naturally induce diffusion on task-relevant neural manifolds in optimally trained networks. We provide a mathematical derivation that outlines sufficient conditions for this phenomenon to occur. Subsequently, we follow with a series of empirical validations in the context of two ecologically relevant tasks: a spatial navigation task, and a head direction integration task.

## 2 Mathematical Results

In this study, we will consider a noisy discrete-time approximation of a continuous-time RNN, receiving change-based information \((t)}{t}\) about an \(N_{s}\)-dimensional environmental state vector \((t)\). The network's objective will be to reconstruct some function of these environmental state variables, \(f(t):^{N_{s}}^{N_{o}}\), where \(N_{s}\) is the number of stimulus dimensions and \(N_{o}\) is the number of output dimensions. An underlying demand for this family of tasks is that path integration needs to be performed, possibly followed by some computations based on that integration. These requirements are often met in natural settings, as it is widely believed that animals are able to estimate their location in space \((t)\) through path integration based exclusively on local motion cues \((t)}{t}\), and neural circuits in the brain that perform this computation have been identified (specifically the entorhinal cortex ). For our analysis, we will assume that the stimuli the network receives are drawn from a stationary distribution, such that \(p((t))\) does not depend on time--for navigation, this amounts to ignoring the effects of initial conditions on an animal's state occupancy statistics, and assumes that the animal's navigation policy remains constant throughout time. The RNN's dynamics are given by:

\[(t+ t) =(t)+(t)\] (1) \[(t) =((t),(t),(t)}{t}) t+(t),\] (2)

where \((t)\) is a function that describes the network's update dynamics as a function of the stimulus, \(()\) is a sufficiently expressive nonlinearity, \((t)(0, t)\) is Brownian noise, and \( t\) is taken to be small as to approximate corresponding continuous-time dynamics. We work with a discrete-time approximation here for the sake of simplicity, and also to illustrate how the equations are implemented in practice during simulations. Suppose that the network's output is given by \(=(t)\), where \(\) is an \(N_{o} N_{r}\) matrix that maps neural activity to outputs, and \(N_{r}\) is the number of neurons in the RNN.

We formalize our loss function for each time point as follows:

\[(t)=_{}\|f(t)- (t)\|_{2},\] (3)

so that as the loss is minimized over timesteps, the system is optimized to match its target at every timestep while compensating for its own intrinsic noise. We find that the greedily optimal dynamics,in the presence of noise, for an upper bound of this loss are given by:

\[^{*}(t)=[\,}{(t)} p((t))}_{}+^{}f((t))}{(t)}(t)}{t}}_{}\,] t+ (t).\] (4)

These dynamics are interpretable: any system attempting to maintain a relationship to a stimulus in the presence of noise must first perform denoising, and then use instantaneous changes in the state variable (\((t)}{t}\)) to update state information. A detailed derivation is provided in Appendix A.

We are now in a position to ask: what happens in the absence of any input to the system, as would be observed in a quiescent state? We will make two assumptions for our model of the quiescent state: 1) \((t)}{t}=0\), so that no time-varying input is being provided to the system, and 2) the variance of the noise is increased by a factor of two (deviating from this factor is not catastrophic as discussed below). This gives the following quiescent dynamics \(}\):

\[}(t)=[^{2}}{ (t)} p((t))] t+(t).\] (5)

Interestingly, this corresponds to Langevin sampling of \(p()\). Therefore, we can predict an equivalence between the steady-state quiescent sampling distribution \(()\) and the active probability distribution over neural states \(p()\) (so that \(p()=()\), and consequently \(p()=()\)). There are two key components that made this occur: first, the system needed to be performing near-optimal noisy state estimation; second, the system state needed to be determined purely by integrating changes in sensory variables of interest. The final assumption--that noise is doubled during quiescent states--is necessary only to produce sampling from the _exact_ same distribution \(p()\). Different noise variances will result in sampling from similar steady-state distributions with different temperature parameters. When these conditions are present, we can expect to see reactivation phenomena during quiescence in optimally trained networks.

## 3 Numerical Experiments

To validate our mathematical results, we consider numerical experiments with noisy "vanilla" continuous-time RNNs on two canonical neuroscience tasks: spatial position estimation using motion cues, and head direction estimation using angular velocity cues. We provide task and network details in Appendix B. Both of these tasks conform to the structure of the general estimation task considered in our mathematical analysis. For each task, we minimize the mean-squared error between the network output \(\) and the task-specific target given by \(f((t))\), summed across timesteps. In this section and Appendix C.1, we discuss the analyses associated with the spatial position estimation task. We refer the reader to Appendix C.2 for experiments on the head direction estimation task.

First, we visualized the decoded output activity for the active and quiescent phases (Fig. 1-b). It is clear that decoded output activity during the quiescent phase is smooth, and tiles output space similarly to trajectories sampled during the waking phase. To quantify the similarity in the _distributions_ of activity during the active and quiescent phases, we computed 2D kernel density estimates1 (KDEs) on the output trajectories (Fig. 1-f). We indeed found that the distribution of activity was similar across active and quiescent phases, as predicted by our mathematical results. However, output trajectories in the quiescent phase do not tile space as uniformly as those in the active phase.

Our theory additionally predicts that if the distribution of network states during the active phase is biased in some way during training, the distribution during the quiescent phase should also be biased accordingly. To test this, we modified the behavioral policy of our agent during training, introducing a drift term that caused it to occupy a ring of spatial locations in the center of the field rather than uniformly tiling space. We found again a close correspondence between decoded output trajectories of the active and quiescent phases (Fig. 1-d), which was also reflected in the KDEs (Fig. 1-h).

We carried out several additional analyses in order to validate our mathematical results, and these are presented in Appendix C. Our results collectively verify that during quiescence, our trained networks do indeed approximately sample from the waking trajectory distribution.

## 4 Discussion

In this study, we have provided mathematical conditions under which reactivation is expected to emerge in task-optimized recurrent neural circuits. Our criteria are as follows: first, the network must implement a noisy, continuous-time dynamical system; second, the network must be solving a state variable estimation task near-optimally, by integrating exclusively change-based inputs (\((t)}{t}\)) to reconstruct some function of the state variables (\(f((t))\)). Under these conditions, we demonstrated that a greedily optimal solution to the task involves a combination of integrating the state variables and denoising. In absence of inputs (quiescent phase), we assumed that the system would receive no stimuli (\((t)}{t}=0\)) so that the system is dominated by its denoising dynamics, and that noise variance would increase slightly (by a factor of \(2\)). Under these conditions, we showed that the steady-state probability distribution of network states during quiescence (\(()\)) should be equivalent to the distribution of network states during active task performance (\(p()\)). Thus, these conditions constitute criteria for a form of reactivation to emerge in trained neural systems.

We have validated our mathematical results empirically in two tasks with neuroscientific relevance. The first, a path integration task, required the network to identify its location in space based on motion cues. This form of path integration has been used to model the entorhinal cortex , a key brain area in which reactivation dynamics have been observed [11; 29]. The second task required the network to estimate a head direction orientation based on angular velocity cues. This function in the mammalian brain has been attributed to the anterodorsal thalamic nucleus (ADn) and post-subiculum (PoS) [30; 31; 32], another critical locus for reactivation dynamics . Previous attempts to model these systems that have relied on hand-crafted network models have been able to reproduce reactivation dynamics. This was done by embedding a smooth attractor structure in the network's recurrent connectivity along which activity may diffuse during quiescence . Similarly, we have identified attractors in our trained networks' latent activation space--we found a smooth map of space in the spatial navigation task (Fig. C.1) and a ring attractor in the head direction task (Fig. C.3). In our case, these attractors proved to be _optimal_ for task performance, and consequently did not require hand crafting. Furthermore, beyond previous studies, we were able to show that the statistics of reactivation in our trained networks mimicked the statistics of activity during waking behavior, and that manipulation of

Figure 1: **Reactivation in a spatial position estimation task.****a-b**) Sample decoded outputs during active (a) and quiescent (b) behavior for networks trained under uniform trajectories. Circles indicate the initial location, triangles indicate the final location. **c-d**) Same as (a-b), but for biased trajectories. **e-f**) 2D kernel density estimate (KDE) plot for decoded outputs in the active (**e**) and quiescent (f) phases, for 500 uniform trajectories. **g-h**) Same as (e-f) but for biased trajectories.

waking behavioral statistics was directly reflected in offline reactivation dynamics. Thus, our work complements these previous studies by providing a mathematical justification for the emergence of reactivation dynamics in terms of optimal task performance.

Our results suggest that reactivation in the brain could be a natural consequence of learning in the presence of noise, rather than the product of an explicit generative demand [20; 34]. Thus, reactivation during quiescence in a brain area should not be taken as evidence exclusively in favor of generative modeling: the alternative possibility, as identified by our work, is that reactivation could be an emergent consequence of optimization for certain tasks (though it could be used for other computations). Our hypothesis and generative modeling hypotheses may be experimentally dissociable: while generative models necessarily recapitulate the moment-to-moment transition statistics of sensory data, our approach only predicts that the _stationary distribution_ will be identical. This opens the possibility for reactivation of sequences that do not respect the ordering of states observed during waking (e.g. reverse replay [35; 36]), as well as changes in the timescale of reactivation .

In addition to the tasks discussed in our numerical experiments, our work has the potential to function as a justification for a wide variety of reactivation phenomena observed in the brain. Further details have been provided in Appendix D. Beyond this, it may further provide a mechanism for inducing reactivation in neural circuits in order to support critical maintenance functions, such as memory consolidation or learning.