# The Minimax Rate of HSIC Estimation for

Translation-Invariant Kernels

 Florian Kalinke

Institute for Program Structures and Data Organization

Karlsruhe Institute of Technology

Karlsruhe, Germany

florian.kalinke@kit.edu

&Zoltan Szabo

Department of Statistics

London School of Economics

London, UK

z.szabo@lse.ac.uk

###### Abstract

Kernel techniques are among the most influential approaches in data science and statistics. Under mild conditions, the reproducing kernel Hilbert space associated to a kernel is capable of encoding the independence of \(M 2\) random variables. Probably the most widespread independence measure relying on kernels is the so-called Hilbert-Schmidt independence criterion (HSIC; also referred to as distance covariance in the statistics literature). Despite various existing HSIC estimators designed since its introduction close to two decades ago, the fundamental question of the rate at which HSIC can be estimated is still open. In this work, we prove that the minimax optimal rate of HSIC estimation on \(^{d}\) for Borel measures containing the Gaussians with continuous bounded translation-invariant characteristic kernels is \(n^{-1/2}\). Specifically, our result implies the optimality in the minimax sense of many of the most-frequently used estimators (including the U-statistic, the V-statistic, and the Nystrom-based one) on \(^{d}\).

## 1 Introduction

Kernel methods (Steinwart and Christmann, 2008; Berlinet and Thomas-Agnan, 2004; Saitoh and Sawano, 2016) allow embedding probability measures into reproducing kernel Hilbert spaces (RKHS; (Aronszajn, 1950)) by use of a positive definite function, the _kernel function_. This approach has gained considerable attention over the last 20 years. Such embeddings induce the so-called maximum mean discrepancy (MMD; (Smola et al., 2007; Gretton et al., 2012)), which quantifies the discrepancy of two probability measures by considering the RKHS norm of the distance of their respective embeddings. MMD is a metric on the space of probability distributions if the kernel is characteristic (Fukumizu et al., 2008; Sriperumbudur et al., 2010). MMD is also an integral probability metric (Zolotarev, 1983; Muller, 1997) where the underlying function class is chosen to be the unit ball in the corresponding RKHS.

MMD allows for the quantification of dependence by considering the distance between the embedding of a joint distribution and that of the product of its marginals. This construction gives rise to the so-called Hilbert-Schmidt independence criterion (HSIC; (Gretton et al., 2005)), which is also equal to the RKHS norm of the centered cross-covariance operator. In fact, one of the most widely-used independence measures in statistics, distance covariance (Szekely et al., 2007; Szekely and Rizzo, 2009; Lyons, 2013), was shown to be equivalent to HSIC (Sejdinovic et al., 2013) when the latter is specialized to \(M=2\) components; Sheng and Sriperumbudur (2023) proved a similar result for the conditional case. For \(M>2\) components (Quadrianto et al., 2009; Sejdinovic et al., 2013; Pfister et al., 2018), universality (Steinwart, 2001; Micchelli et al., 2006; Carmeli et al., 2010; Sriperumbudur et al., 2011) of the kernels \((k_{m})_{m=1}^{M}\) (on the respective domains) underlying HSIC guarantees that this measure captures independence (Szabo and Sriperumbudur, 2018). In the case of \(M=2\), characteristic \((k_{m})_{m=1}^{2}\) suffice (Lyons, 2013).

HSIC has been deployed successfully in numerous contexts, including independence testing in batch (Gretton et al., 2008; Wehbe and Ramdas, 2015; Bilodeau and Nangue, 2017; Gorecki et al., 2018; Pfister et al., 2018; Albert et al., 2022; Shekhar et al., 2023) and streaming (Podkopaev et al., 2023) settings, feature selection (Camps-Valls et al., 2010; Song et al., 2012; Yamada et al., 2014; Wang et al., 2022) with applications in biomarker detection (Climente-Gonzalez et al., 2019) and wind power prediction (Bouche et al., 2023), clustering (Song et al., 2007; Climente-Gonzalez et al., 2019), and causal discovery (Mooij et al., 2016; Pfister et al., 2018; Chakraborty and Zhang, 2019; Scholkopf et al., 2021; Kalinke and Szabo, 2023). In addition, HSIC has recently found successful applications in sensitivity analysis (Veiga, 2015; Freitas Gustavo et al., 2023; Fellmann et al., 2024; Herrando-Perez and Saltre, 2024), in the context of uncertainty quantification (Stenger et al., 2020), for the analysis of data augmentation methods for brain tumor detection (Anaya-Isaza and Mera-Jimenez, 2022), and that of multimodal neural networks trained on neuroimaging data (Fedorov et al., 2024).

Many estimators for HSIC exist. The classical ones rely on U-statistics or V-statistics (Gretton et al., 2005; Quadrianto et al., 2009; Pfister et al., 2018) and are known to converge at a rate of \(_{P}(n^{-1/2})\). In fact, the V-statistic-based estimators are obtained by replacing the population kernel mean embedding with its empirical counterpart; estimating the mean embedding can be carried out at a speed \(_{P}(n^{-1/2})\)(Smola et al., 2007; Theorem 2), which implies that HSIC can be estimated at the same rate. Existing approximations such as Nystrom HSIC (Kalinke and Szabo, 2023), also achieve this rate under the assumption of an appropriate rate of decay of the effective dimension. While all of these upper bounds match asymptotically, it is not known whether HSIC can be estimated at a faster rate, that is, whether the upper bound of \(_{P}(n^{-1/2})\) is optimal in the minimax sense, or if designing estimators achieving better rates is possible. Lower bounds for the related MMD are known (Tolstikhin et al., 2016), but the existing analysis considers radial kernels and relies on independent Gaussian distributions. Radial kernels are a special case of the more general class of translation-invariant kernels that we consider.1 The reliance on independent Gaussian distributions renders the analysis of Tolstikhin et al. (2016) inapplicable for HSIC estimation. We tackle both of these severe restrictions in the present article.

We make the following **contributions**.

* We establish the minimax lower bound \((n^{-1/2})\) of HSIC estimation with \(M 2\) components on \(^{d}\) with continuous bounded translation-invariant characteristic kernels. As this lower bound matches the known upper bounds of the existing "classical" U-statistic and V-statistic-based estimators, and that of the Nystrom HSIC estimator, our result settles their minimax optimality.
* Specifically, our result also implies the minimax lower bound of \((n^{-1/2})\) for the estimation of the cross-covariance operator, which can be further specialized to get back the minimax result (Zhou et al., 2019, Theorem 5) on the estimation of the covariance operator.

The paper is structured as follows. Notations are introduced in Section 2. Section 3 is dedicated to our main result on the minimax rate of HSIC estimation on \(^{d}\), with proof presented in Section 4. An auxiliary result on the Kullback-Leibler divergence is shown in Appendix A.

## 2 Notations

In this section, we introduce a few notations \(_{>0}\), \([M]\), \(_{n}\), \(_{n}\), \(_{n}\), \(^{}\), \(,\), \(\|\|_{^{d}}\), \((_{1},,_{N})\), \(||\), \(_{1}^{+}(^{d})\), \(_{}\), \((\|)\), \(L^{2}(^{d},)\), \(\|f\|_{L^{2}(^{d},)}\), \(()\), \(_{k}\), \(_{k}\), \(k\), \(_{k}\), \(_{k}\), \(_{m=1}^{M}_{k_{m}}\), \(_{m=1}^{M}k_{m}\), \(_{m}\), \(_{m=1}^{M}_{m}\), \(_{P}(r_{n})\), \(_{On}(a_{n})\), \(a_{n} b_{n}\), \(_{k}\), and \(C_{X}\). Throughout the paper we consider random variables, probability measures, and kernels on \(^{d}\).

For \(M_{>0}:=\{1,2,\}\), let \([M]:=\{1,,M\}\). Denote by \(_{n}\) the \(n n\)-sized identity matrix and by \(_{n}=(0,,0)^{}^{n}\) (resp. \(_{n}=(1,,1)^{}^{n}\)) a column vector of zeros (resp. ones). The transpose of a matrix \(^{d_{1} d_{2}}\) is written as \(^{}^{d_{2} d_{1}}\). For \(,^{d}\), \(,=^{} \) stands for their Euclidean inner product; \(\|\|_{^{d}}=, }\) is the associated Euclidean norm.

\((_{1},,_{N})\) forms a block-diagonal matrix from its arguments \((_{n})_{n=1}^{N}\) (\(_{n}^{d_{n} d_{n}}\), \(n[N]\)) and \(||\) denotes the determinant of a matrix \(^{d d}\).

The set of Borel probability measures on \(^{d}\) is denoted by \(_{1}^{+}(^{d})\). For a random variable \(X_{1}^{+}(^{d})\), we denote its characteristic function by \(_{}()=_{X}[e^{ i,X}]\) with \(^{d}\) and \(i=\). Let \(,_{1}^{+}(^{d})\), assume that \(\) is absolutely continuous w.r.t. \(\), and let \(}{}\) denote the corresponding Radon-Nikodym derivative (of \(\) w.r.t. \(\)). Then, the Kullback-Leibler divergence of \(\) and \(\) is defined as \((||):=_{^{d}}(}{}()) ()\). Given a measure space \((^{d},(^{d}),)\), we denote by \(L^{2}(^{d},):=L^{2}(^{d},( ^{d}),)\) the Hilbert space of (equivalence classes of) measurable functions \(f:(^{d},(^{d})) (,())\) for which \(\|f\|_{L^{2}(^{d},)}^{2}:=_{^{d}}|f( )|^{2}()<\). The support of a probability measure \(_{1}^{+}(^{d})\) denoted by \(()\) is the subset of \(^{d}\) for which every open neighborhood of \(^{d}\) has positive measure (Cohn, 2013, p. 207).

A function \(k:^{d}^{d}\) is called a kernel if there exists a Hilbert space \(\) and a feature map \(:^{d}\) such that \(k(,^{})=(),(^{})_{}\) for all \(,^{}^{d}\). A Hilbert space of functions \(h:^{d}\) is an RKHS \(_{k}\) associated to a kernel \(k:^{d}^{d}\) if \(k(,)_{k}\) and \( h,k(,)_{_{k}}=h()\) for all \(^{d}\) and \(h_{k}\).2 In this work, we assume all kernels to be measurable and bounded.3 The function \(_{k}():=k(,)\) is the canonical feature map, and \(k(,^{})= k(,),k(, ^{})_{_{k}}=_{k}( ),_{k}(^{})_{_{k}}\) for all \(,^{}^{d}\). A function \(:^{d}\) is called positive definite if \(_{i,j[n]}c_{i}c_{j}(_{i}-_{j}) 0\) for all \(n_{>0}\), \(=(c_{i})_{i=1}^{n}^{n}\), and \(\{_{i}\}_{i=1}^{n}^{d}\). A kernel \(k:^{d}^{d}\) is said to be translation-invariant if there exists a positive definite function \(:^{d}\) such that \(k(,^{})=(-^{})\) for all \(,^{}^{d}\). By Bochner's theorem (Wendland, 2005, Theorem 6.6) (recalled in Theorem B.1) for a continuous bounded translation-invariant kernel \(k:^{d}^{d}\) there exists a finite non-negative Borel measure \(_{k}\) such that

\[k(,)=_{^{d}}e^{-i(-, )}_{k}()\] (1)

for all \(,^{d}\). The (kernel) mean embedding of a probability measure \(_{1}^{+}(^{d})\) is

\[_{k}()=_{^{d}}_{k}() ()_{k},\]

where the integral is meant in Bochner's sense (Diestel and Uhl, 1977, Chapter II.2); the boundedness of \(k\) ensures that it is well-defined. For \(,_{1}^{+}(^{d})\) one can define the (semi-)metric called maximum mean discrepancy (Smola et al., 2007, Gretton et al., 2012) as

\[_{k}(,)=\|_{k}()-_{k}() \|_{_{k}}\,.\]

If the mean embedding \(_{k}\) is injective, MMD is a metric and the kernel \(k\) is called characteristic (Fukumizu et al., 2008, Sriperumbudur et al., 2010, Szabo and Sriperumbudur, 2018).

Let \(^{d}=_{m=1}^{M}^{d_{m}}\) (\(d=_{m=1}^{M}d_{m}\)) and assume that each domain \(^{d_{m}}\) is equipped with a kernel \(k_{m}:^{d_{m}}^{d_{m}}\) with associated RKHS \(_{k_{m}}\) (\(m[M]\)). The tensor product Hilbert space of \((_{k_{m}})_{m=1}^{M}\) is denoted by \(_{m=1}^{M}_{k_{m}}\); it is an RKHS (Berlinet and Thomas-Agnan, 2004, Theorem 13) with the tensor product kernel \(k=_{m=1}^{M}k_{m}:^{d}^{d}\) defined by

\[k((_{m})_{m=1}^{M},(_{m}^{})_{m=1}^{M})= _{m[M]}k_{m}(_{m},_{m}^{})_{m},_{m}^{}^{d_{m}},\,m[M].\]

The kernel \(k\) has the canonical feature map \(_{k}((_{m})_{m=1}^{M})=_{m=1}^{M}_{kwith joint distribution \(_{1}^{+}(^{d})\) and marginal distributions \(_{m}_{1}^{+}(^{d_{m}})\) (\(m[M]\); \(d=_{m=1}^{M}d_{m}\)). We write \(_{m=1}^{M}_{m}_{1}^{+}(^{d})\) for the product of measures \(_{m}\) (\(m[M]\)). Specifically, \(^{n}:=_{i=1}^{n}_{1}^{+}(( ^{d})^{n})\) denotes the \(n\)-fold product of \(\). For a sequence of real-valued random variables \((X_{n})_{n=1}^{}\) and a sequence \((r_{n})_{n=1}^{}\) (\(r_{n}>0\) for all \(n\)), \(X_{n}=_{P}(r_{n})\) denotes that \(}{r_{n}}\) is bounded in probability. For positive sequences \((a_{n})_{n=1}^{}\) and \((b_{n})_{n=1}^{}\), \(b_{n}=(a_{n})\) if there exist constants \(C>0\) and \(n_{0}_{>0}\) such that \(b_{n} Ca_{n}\) for all \(n n_{0}\); \(a_{n} b_{n}\) if \(a_{n}=(b_{n})\) and \(b_{n}=(a_{n})\). One can define our quantity of interest, the Hilbert-Schmidt independence criterion (HSIC; (Gretton et al., 2005; Quadiantno et al., 2009; Pfister et al., 2018; Szabo and Sriperumbudur, 2018)), as

\[_{k}() =_{k}(,_{m=1}^{M}_{m} )=\|C_{X}\|_{_{k}},\] \[C_{X} =_{k}()-_{k}(_{m=1}^{M}_{m })_{k},\] (2)

and \(C_{X}\) denotes the centered cross-covariance operator.

## 3 Results

This section is dedicated to our results: The minimax lower bound for the estimation of \(_{k}()\), where \(k\) is a product of continuous bounded translation-invariant characteristic kernels is given in Theorem 1(ii). For the specific case where \(k\) is a product of Gaussian kernels (stated in Theorem 1(i)), the constant in the lower bound is made explicit. Theorem 1(ii) also helps to establish a lower bound on the estimation of the cross-covariance operator (Corollary 1).

Before presenting our results, we recall the framework of minimax estimation (Tsybakov, 2009) adapted to our setting. Let \(_{n}\) denote any estimator of \(_{k}()\) based on \(n\) i.i.d. samples from \(\). A sequence \((_{n})_{n=1}^{}\) (\(_{n}>0\) for all \(n\)) is said to be a lower bound of HSIC estimation w.r.t. a class \(\) of Borel probability measures on \(^{d}\) if there exists a constant \(c>0\) such that

\[_{_{n}}_{}^{n}\{_{n}^ {-1}|_{k}()-_{n}| c\}>0.\] (3)

If a specific estimator of HSIC \(_{n}\) has an upper bound that matches \((_{n})_{n=1}^{}\) up to constants, that is,

\[|_{k}()-_{n}|=_{P}( _{n}),\] (4)

then \(_{n}\) is called minimax optimal.

We use Le Cam's method (Le Cam, 1973; Tsybakov, 2009) (recalled in Theorem B.5) to obtain bounds as in (3); estimators of HSIC achieving the bounds in (4) with \(_{n}=n^{-1/2}\) are quoted in the introduction. The key to the application of the method is to show that there exist \(>0\) and \(n_{0}_{>0}\) such that for all \(n n_{0}\) one can find an adversarial pair of distributions \((_{_{0}},_{_{1}})=(_ {_{0}}(n),_{_{1}}(n))\) and \(s_{n}>0\) for which

1. \((_{_{1}}^{n}\|_{_{0}}^{n})\), in other words, the corresponding \(n\)-fold product measures must be similar in the sense of Kullback-Leibler divergence, but
2. \(|_{k}(_{_{1}})-_{k}( _{_{0}})| 2s_{n}\), that is, their corresponding values of HSIC must be dissimilar.

In this case, \(_{_{n}}_{}^{n}\{| _{k}()-_{n}| s_{n}\} (}{4},}{2})\) for all \(n n_{0}\); hence to establish the minimax optimality of existing estimators w.r.t. their known upper bounds, it is sufficient to find adversarial pairs \(\{(_{_{0}}(n),_{_{1}}(n))\}_ {n n_{0}}\) that satisfy 1. for some positive constant \(\) and also fulfill 2. with \(s_{n} n^{-1/2}\).

The proof of the first part of our statement relies on the following Lemma 1 which yields the analytical value of \(_{k}((,))\), where \(k=_{m=1}^{M}k_{m}\) is the product of Gaussian kernels \(k_{m}\) (\(m[M]\)) and \((,)\) denotes the multivariate normal distribution with mean \(^{d}\) and covariance matrix \(^{d d}\).

**Lemma 1** (Analytical value of HSIC for the Gaussian setting).: _Let us consider the Gaussian kernel \(k(,)=e^{-\|-\|_ {2^{d}}^{2}}\) (\(>0\), \(,^{d}\)) and Gaussian random variable \(X=(X_{m})_{m=1}^{M}(,)=:\), where \(X_{m}^{d_{m}}\) (\(m[M]\)), \(=(_{m})_{m=1}^{M}^{d}\), \(=[_{i,j}]_{i,j[M]}^{d d}\), \(_{i,j}^{d_{i} d_{j}}\), and \(d=_{m[M]}d_{m}\). In this case, with \(_{1}=\) and \(_{2}=(_{1,1},,_{M,M})\), we have_

\[_{k}^{2}()=_{1}+ _{d}|^{}}+_{2} +_{d}|^{}}-_{1 }+_{2}+_{d}|^{}}.\]

In this work, we focus on continuous bounded translation-invariant kernels, which are fully characterized by Bochner's theorem (Wendland, 2005, Theorem 6.6); the theorem states that a function on \(^{d}\) is positive definite if and only if it is the Fourier transform of a finite nonnegative measure.4 We use this description to obtain our main result, which is as follows.

**Theorem 1** (Lower bound for HSIC estimation on \(^{d}\)).: _Let \(\) be a class of Borel probability measures over \(^{d}\) containing the \(d\)-dimensional Gaussian distributions. Let \(d=_{m[M]}d_{m}\) and \(_{n}\) denote any estimator of \(_{k}()\) with \(n 2=:n_{0}\) i.i.d. samples from \(\). Assume further that \(k=_{m=1}^{M}k_{m}\) where either, for \(m[M]\),_

1. _the kernels_ \(k_{m}:^{d_{m}}^{d_{m}}\) _are Gaussian with common bandwidth parameter_ \(>0\) _defined by_ \((_{m},_{m}^{}) e^{-\| _{m}-_{m}^{}\|_{_{d_{m}}}^{2}}\)__(_\(_{m},_{m}^{}^{d_{m}}\)_), or_
2. _the kernels_ \(k_{m}:^{d_{m}}^{d_{m}}\) _are continuous bounded translation-invariant characteristic kernels._

_Then, for any \(n n_{0}\), it holds that_

\[_{_{n}}_{}^{n}\{| _{k}()-_{n}|}\}}}{2},\]

_with (i) the constant \(c=+1}}>0\) (depending on \(\) and \(d\) only) in the first case, or (ii) some constant \(c>0\) in the second case._

We note that while Theorem 1(ii) applies to the more general class of translation-invariant kernels, we include Theorem 1(i) as it makes the constant \(c\) explicit.

The following corollary allows to recover the recent lower bound on the estimation of the covariance operator by Zhou et al. (2019, Theorem 5) as a special case that we detail in Remark 1(e).

**Corollary 1** (Lower bound on cross-covariance operator estimation).: _In the setting of Theorem 1(ii), let \(_{n}\) denote any estimator of the centered cross-covariance operator \(C_{X}_{k}\) defined in (2) with \(n 2=:n_{0}\) i.i.d. samples from \(\). Then, for any \(n n_{0}\), it holds that_

\[_{_{n}}_{}^{n}\{\|C_ {X}-_{n}\|_{_{k}}}\} }}{2},\]

_for some constant \(c>0\)._

**Remark 1**.:
1. _[leftmargin=*]_
2. _Validness of HSIC. Though generally the characteristic property of_ \((k_{m})_{m=1}^{M}\)_-s is not enough_ _(Szabo and Sriperumbudur, 2018, Example 2)_ _for_ \(M>2\) _to ensure the_ \(\)_-characteristic property of_ \(k=_{m=1}^{M}k_{m}\) _(in other words, that_ \(_{k}()=0\) _iff._ \(=_{m=1}^{M}_{m}\)_), on_ \(^{d}\) _under the imposed continuous bounded translation-invariant assumption (i)_ \(k\) _being characteristic, (ii)_ \(k\) _being_ \(\)_-characteristic, and (iii)_ \((k_{m})_{m=1}^{M}\)_-s being characteristic are equivalent (Theorem_ B.4_)._
3. _Minimax optimality of existing HSIC estimators. The lower bounds in Theorem_ 1 _asymptotically match the known upper bounds of the U-statistic and V-statistic-based estimators of_ \(_{n}=n^{-1/2}\)_The Nystrom-based HSIC estimator achieves the same rate under an appropriate decay of the eigenspectrum of the respective covariance operator. Hence, Theorem 1 implies the optimality of these estimators on \(^{d}\) with continuous bounded translation-invariant characteristic kernels in the minimax sense._
3. _Difference compared to Tolstikhin et al. (2016) (minimax MMD estimation). We note that a lower bound for the related \(_{k}\) exists. However, the adversarial distribution pair \((_{_{1}},_{_{0}})\) constructed by Tolstikhin et al. (2016, Theorem 1) to obtain the lower bound on MMD estimation has a product structure which implies that \(|_{k}(_{_{1}})-_{k}(_{ _{0}})|=0\) and hence it is not applicable in our case of HSIC; Tolstikhin et al. (2016, Theorem 2) with radial kernels has the same restriction._
4. _Difference compared to Tolstikhin et al. (2017) (minimax mean embedding estimation). The estimation of the mean embedding \(_{k}()\) is known to have a minimax rate of \((n^{-1/2})\). But, this rate does not imply an optimal lower bound for the estimation of MMD as is evident from the two works (Tolstikhin et al., 2016, 2017). The same conclusion holds for HSIC estimation._
5. _Difference compared to Zhou et al. (2019) (minimax covariance operator estimation). For the related problem of estimating the centered covariance operator_ \[C_{XX}=_{^{d}}(_{k}(x)-_{k}()) (_{k}(x)-_{k}())(x) _{k}_{k},\]

Zhou et al. (2019, Theorem 5) give the lower bound

\[_{_{n}}_{}^{n}\{\| C_{XX}-_{n}\|_{_{k}_{k}} }\} 1/8\]

_in the same setting as in Theorem 1(ii), where \(_{n}\) is any estimator of the centered covariance \(C_{XX}\), and \(c\) is a positive constant. By noting that the centered covariance is the centered cross-covariance of a random variable with itself, Corollary 1 recovers their result._

The next section contains our proofs.

## 4 Proofs

This section is dedicated to our proofs. We present the proof of Lemma 1 in Section 4.1, that of Theorem 1 in Section 4.2, and that of Corollary 1 in Section 4.3.

### Proof of Lemma 1

As

\[_{k}^{2}() =_{k}^{2}(,)=\|_{k}( )-_{k}()\|_{_{k}}^{2}\] \[=_{k}(),_{k}()_{_{k}}+_{k}(),_{k}()_{_{k}}-2 _{k}(),_{k}()_{_{k}}\]

with \(=_{m=1}^{M}_{m}=(,(_{1,1},,_{M,M}))\), \(_{m}=(_{m},_{m,m})\), it is sufficient to be able to compute \(_{k}(),_{k}()_{_{k}}\)-type quantities with \(=(_{1},_{1})\) and \(=(_{2},_{2})\). One can show (Muandet et al., 2011, Table 1) that \(_{k}(),_{k}()_{_{k}}=(_{1}-_{2})^{T}(_{1}+_{2}+^{-1}_{d})^{-1}(_{1}-_{2})}}{ |_{1}+_{2}+_{d}|^{ {2}}}\). Using this fact and that \(=_{1}=_{2}\), the result follows.

### Proof of Theorem 1

The setup and the upper bound on \((_{_{1}}^{n}||_{_{0}}^{n})\) agree for (i) and (ii) but the methods that we use to lower bound \(|_{k}(_{_{1}})-_{k}(_{ _{0}})|\) differ. We structure the proof accordingly and present the overlapping part before we branch out into (i) and (ii). Both parts of the statement rely on Le Cam's method, which we state as Theorem B.5 for self-completeness.

To construct the adversarial pair, we consider a class \(\) of Gaussian distributions over \(^{d}\) such that every element \(,\), with

\[=(i,j,)=1&&0&0& &0\\ &&&&&\\ 0&&1&&&0\\ 0&&&1&&0\\ &&&&&\\ 0&&0&0&&1^{d d},\] (5)

and (fixed) \(i=d_{1}\), \(j=d_{1}+1\), \((-1,1)\). In other words, \(\) is essentially the \(d\)-dimensional matrix \(_{d}\) except for the \((i,j)\) and \((j,i)\) entry; both entries are identical to \(\), and they specify the correlation of the respective coordinates. This family of distributions is indexed by a tuple \((,)^{d}(-1,1)=:\) and, for \(a\), we write \(_{a}\) for the associated distribution. To bring ourselves into the setting of Theorem B.5, we fix \(n_{>0}\), choose \(=(^{d})^{n}\), set \(=\{_{a}:=_{k}(_{a})\,:\,a\},_{}=\{_{a}^{n}\,:\,a\}=\{_ {a}^{n}\,:\,_{a}\}\), and use the metric \((x,y)|x-y|\) for \(x,y\). Hence, the data \(D_{}_{}\). For brevity, let \(F:\) stand for \(a_{k}(_{a})\), and let \(_{n}\) stand for the corresponding estimator based on \(n\) samples.

As \(\), it holds for every positive \(s\) that

\[_{}^{n}\{|_{k} ()-_{n}| s\}_{ }^{n}\{|_{k}( )-_{n}| s\}.\]

Let \(_{_{0}}=(_{0},_{0})\) and \(_{_{1}}=(_{1},_{1})\) with

\[_{0} =_{d}^{d}, _{0} =(d_{1},d_{1}+1,0)=_{d}^{d d},\] \[_{1} =n}_{d}^{d}, _{1} =(d_{1},d_{1}+1,_{n})^{d  d},\]

where \(_{n}(-1,1)\) will be chosen appropriately later.5 We now proceed to upper bound \((_{_{1}}^{n}||_{_{0}}^{n})\) and lower bound \(|F(_{1})-F(_{0})|\).

Upper bound for KL divergenceLemma A.1 implies that with \(_{n}^{2}=\), one has the bound \((_{_{1}}^{n}||_{_{0}}^{n}) :=\) for \(n 2=:n_{0}\).

Lower bound (i): Gaussian kernels.Recall that the considered kernel is \(k(,)=e^{-\|- \|_{^{d}}^{2}}\) (\(>0\)). The idea of the proof is as follows.

1. We express \(|F(_{1})-F(_{0})|\) in closed form as a function of \(\), \(_{n}\), and \(d\).
2. Using the analytical form obtained in the 1st step, we construct the lower bound.

This is what we detail next.

* **Analytical form of \(|F(_{1})-F(_{0})|\)**: Using the fact that \(_{k}(_{_{0}})=0\), we have that \[|F(_{1})- )}_{=0}|^{2}=F^{2}(_{1})=_{k}^{2} (_{_{1}})=_{k}^{2}( (_{1},_{1}),( _{1},_{d}))\] \[=(( _{1},_{1})),_{k}( (_{1},_{1})) _{_{k}}}_{(i)}+( (_{1},_{d})),_{k} ((_{1},_{d})) _{_{k}}}_{(ii)}\] \[-2(( _{1},_{1})),_{k}( (_{1},_{d})) _{_{k}}}_{(iii)},\]which we compute term-by-term with Lemma 1, and obtain

\[(i) =|2_{1}+_{d}|^{-1/2}=[ (2+1)^{d-2}((2+1)^{2}-(2_ {n})^{2})]^{-1/2},\] \[(ii) =|2_{d}+_{d}|^{-1/2}=[ (2+1)^{d}]^{-1/2},\] \[(iii) =|_{1}+_{d}+_{d}|^{-1/2}=[(2+1)^{d-2}((2+1 )^{2}-(_{n})^{2})]^{-1/2}.\]

Combining (i), (ii), and (iii) yields that

\[_{k}^{2}(_{_{1}}) =(i)+(ii)-2(iii)\] \[=[(2+1)^{d-2}((2+1)^ {2}-(2_{n})^{2})]^{-1/2}+[(2 +1)^{d}]^{-1/2}\] \[-2[(2+1)^{d-2}((2+1 )^{2}-(_{n})^{2})]^{-1/2}.\]
* **Lower bound on \(|F(_{1})-F(_{0})|\)**: Next, we show that there exists \(c>0\) such that for any \(n_{>0}\) it holds that \(_{k}^{2}(_{_{1}})\). For \(0<x<(1+)^{2}\), let us consider the function \[f_{c}(x) =[(2+1)^{d-2}((2+1)^ {2}-4^{2}x)]^{-1/2}+[(2+1)^{d}] ^{-1/2}\] \[-2[(2+1)^{d-2}((2+1 )^{2}-^{2}x)]^{-1/2}-cx\] \[=[z^{d-2}(z^{2}-4^{2}x)]^{-1/2}+ (z^{d})^{-1/2}-2[z^{d-2}(z^{2}-^{2}x)] ^{-1/2}-cx,\] with the shorthand \(z:=2+1\).6 With this notation, \(f_{c}(1/n)=_{k}^{2}(_{_{1}})-c/n\); our aim is to determine \(c>0\) such that \(f_{c}(1/n) 0\) for any positive integer \(n\). To achieve this goal, notice that \(f_{c}(0)=0\), and 
Lower bound (ii): translation-invariant kernels.Let \(_{k}\) denote the spectral measure associated to the kernel \(k\) according to (1). Using the fact that \(_{k}(_{_{0}})=0\), we have for \(|F(_{1})-F(_{0})|\) that

\[|F(_{1})- )}_{=0}|^{2}=F^{2}(_{1})=_{k}^{2} (_{_{1}})=_{k}^{2}( (_{1},_{1}),( _{1},_{0}))\] \[}{{=}}\|_{ (_{1},_{1})}-_{ (_{1},_{0})}\|_{L^{2}( ^{d},_{k})}^{2}\] \[}{{=}}_{^{d}}|e ^{i(_{1},)-( ,_{1})}-e^{i (_{1},)-( ,_{0})} |^{2}_{k}()\] \[}{{}}_{A}|e^{- (,_{1})}-e ^{-(,_{0})}|^{2}_{k}()}{{}}_{n}[h_{}^{}(0)]^{2}_{k}()} }{{=:(2s)^{2}>0}},\]

where \((i)\) holds by Sriperumbudur et al. (2010, Corollary 4(i)) (recalled in Theorem B.2). \((ii)\) follows from the analytical form \(_{(,)}( )=e^{i(,)-( ,})}\) of the characteristic function of a multivariate normal distribution \((,)\). For \((iii)\), we define the non-empty open set

\[A=\{=(_{1},,_{d})^{} ^{d}\,:\,_{d_{1}}_{d_{1}+1}<0\}^ {d},\]

and use that the integration of a non-negative function over a subset yields a lower bound. In \((iv)\), fix \( A\) and let

\[h_{}: e^{-(,(d_{1},d_{1}+1,))}( 0,1].\]

Note that \(h_{}()=e^{-(^{ }+2_{d_{1}}_{d_{1}+1})}\); \(h_{}\) is continuous on \(\) and differentiable on \((0,1)\). Hence for any \((0,1)\), by the mean value theorem, there exists \((0,1)\) such that

\[h_{}()-h_{}(0)= h_{}^{}()_{c}h_{ }^{}(c).\]

We have the first and second derivatives

\[h_{}^{}(c)=-_{d_{1}}_{d_{1}+1}e^{-(^{}+2c_{d_{1} }_{d_{1}+1})}, h_{}^{}(c)= _{d_{1}}^{2}_{d_{1}+1}^{2}e^{-( ^{}+2c_{d_{1}}_{d_{1}+1})}>0,\]

which implies that \(c h_{}^{}(c)\) is a strictly increasing function of \(c\) and that it attains its minimum at \(c=0\), that is,

\[h_{}()-h_{}(0) h_{ }^{}(0)>0,\]

where the 2nd inequality holds by \(>0\) and \( A\). This shows that

\[[h_{}()-h_{}(0)]^{2} [ h_{}^{}(0)]^{2},\]

and the monotonicity of integration gives \((iv)\). For \((v)\), we note that the kernel \(k=_{n=1}^{M}k_{m}\) is characteristic (Szabo and Sriperumbudur, 2018, Theorem 4) (recalled in Theorem B.4) as the \((k_{m})_{m=1}^{M}\)-s are characteristic. Thus, \((_{k})=^{d}\) (see Sriperumbudur et al. (2010, Theorem 9); recalled in Theorem B.3), implying that \(_{k}(A)>0\). \((v)\) follows from the positivity of \(h_{}^{}(0)\) (for any \( A\)), from the fact that the integral of a positive function on a set with positive measure is positive, and from our choice of \(_{n}=n^{-1/2}\).

Now, by taking the positive square root, we have

\[|F(_{1})-F(_{0})|}=:2s.\] (11)

We conclude by the application of Theorem B.5 using that \(=\) and \((}}{4},}}{2})= }}{2}\).

### Proof of Corollary 1

We use the same argument as in the beginning of the proof of Theorem 1 in Section 4.2 but adjust the setting in which we apply Theorem B.5. Specifically, we now let \(=\{_{a}:=C_{X_{a}}\,:\,X_{a}_{a},\,a\}\) with \(C_{X}\) defined as in (2) be the set of covariance operators, use the metric \((x,y)\|x-y\|_{_{k}}\) for \(x,y_{k}\), and keep the remaining part of the setup the same. Hence, it remains to lower bound \(\|C_{X_{_{1}}}-C_{X_{_{0}}}\|_{_{k}}\). By using that HSIC is the RKHS norm of the cross-covariance operator, we obtain that

\[\|C_{X_{_{1}}}-C_{X_{_{0}}}\|_{_{k}}|\|C_{X_{_{1}}}\|_{_{k}}-}}\|_{_{k}}}_{= _{k}(_{_{1}})}=|F( _{1})-F(_{0})|2s=},\]

where \((i)\) holds by the reverse triangle inequality, \(F\) is defined as in Section 4.2, and \((ii)\) is guaranteed by (11) for \(c>0\). We conclude as in the proof of Theorem 1(ii) to obtain the stated result.