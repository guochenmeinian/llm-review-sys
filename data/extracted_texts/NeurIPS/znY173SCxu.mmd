# Time-Reversed Dissipation Induces Duality Between Minimizing Gradient Norm and Function Value

Jaeyeon Kim

Seoul National University

kjy011102@snu.ac.kr

&Asuman Ozdaglar

MIT EECS

asuman@mit.edu

&Chanwoo Park

MIT EECS

cpark97@mit.edu

&Ernest K. Ryu

Seoul National University

ernestryu@snu.ac.kr

###### Abstract

In convex optimization, first-order optimization methods efficiently minimizing function values have been a central subject study since Nesterov's seminal work of 1983. Recently, however, Kim and Fessler's OGM-G and Lee et al.'s FISTA-G have been presented as alternatives that efficiently minimize the gradient magnitude instead. In this paper, we present H-duality, which represents a surprising one-to-one correspondence between methods efficiently minimizing function values and methods efficiently minimizing gradient magnitude. In continuous-time formulations, H-duality corresponds to reversing the time dependence of the dissipation/friction term. To the best of our knowledge, H-duality is different from Lagrange/Fenchel duality and is distinct from any previously known duality or symmetry relations. Using H-duality, we obtain a clearer understanding of the symmetry between Nesterov's method and OGM-G, derive a new class of methods efficiently reducing gradient magnitudes of smooth convex functions, and find a new composite minimization method that is simpler and faster than FISTA-G.

## 1 Introduction

Since Nesterov's seminal work of 1983 , accelerated first-order optimization methods that efficiently reduce _function values_ have been central to the theory and practice of large-scale optimization and machine learning. In 2012, however, Nesterov initiated the study of first-order methods that efficiently reduce _gradient magnitudes_ of convex functions . In convex optimization, making the function value exactly optimal is equivalent to making the gradient exactly zero, but reducing the function-value suboptimality below a threshold is not equivalent to reducing the gradient magnitude below a threshold. This line of research showed that accelerated methods for reducing function values, such as Nesterov's FGM , the more modern OGM , and the accelerated composite optimization method FISTA  are not optimal for reducing gradient magnitude, and new optimal alternatives, such as OGM-G  and FISTA-G , were presented.

These new accelerated methods for reducing gradient magnitudes are understood far less than those for minimizing function values. However, an interesting observation of symmetry, described in Section 2, was made between these two types of methods, and it was conjectured that this symmetry might be a key to understanding the acceleration mechanism for efficiently reducing gradient magnitude.

Contribution.We present a surprising one-to-one correspondence between methods efficiently minimizing function values and methods efficiently minimizing gradient magnitude. We call this correspondence H-duality and formally establish a duality theory in both discrete- and continuous-time dynamics. Using H-duality, we obtain a clearer understanding of the symmetry between FGM/OGM and OGM-G, derive a new class of methods efficiently reducing gradient magnitudes, and find a new composite minimization method that is simpler and faster than FISTA-G, the prior state-of-the-art in efficiently reducing gradient magnitude in the composite minimization setup.

### Preliminaries and Notation

Given \(f^{d}\), write \(f_{}=_{x^{d}}f(x)(-,)\) for the minimum value and \(x_{}*{argmin}_{x^{n}}f(x)\) for a minimizer, if one exists. Throughout this paper, we assume \(f_{}-\), but we do not always assume a minimizer \(x_{}\) exists. Given a differentiable \(f^{d}\) and a pre-specified value of \(L>0\), we define the notation

\[[x,y] :=f(y)-f(x)+ f(y),x-y\] \[:=f(y)-f(x)+ f(y),x-y+\|  f(x)- f(y)\|^{2}\] \[:=f_{}-f(x)+\| f(x)\|^{2}\]

for \(x,y^{d}\). A differentiable function \(f^{d}\) is convex if the convexity inequality \([x,y] 0\) holds for all \(x,y^{d}\). For \(L>0\), a function \(f^{d}\) is \(L\)-smooth convex if it is differentiable and the cocercivity inequality \([x,y] 0\) holds for all \(x,y^{d}\). If \(f\) has a minimizer \(x_{}\), then \( x,= x,x_{}\), but the notation \( x,\) is well defined even when a minimizer \(x_{}\) does not exist. If \(f\) is \(L\)-smooth convex, then \( x, 0\) holds for all \(x^{d}\).

Throughout this paper, we consider the duality between the following two problems.

* [label=(P0)]
* Efficiently reduce \(f(x_{N})-f_{}\) assuming \(x_{}\) exists and \(\|x_{0}-x_{}\| R\).
* Efficiently reduce \(\| f(y_{N})\|^{2}\) assuming \(f_{}>-\) and \(f(y_{0})-f_{} R\).

Here, \(R(0,)\) is a parameter, \(x_{0}\) and \(y_{0}\) denote initial points of methods for (P1) and (P2), and \(x_{N}\) and \(y_{N}\) denote outputs of methods for (P1) and (P2).

Finally, the standard gradient descent (GD) with stepsize \(h\) is

\[x_{i+1}=x_{i}- f(x_{i}), i=0,1,.\] (GD)

### Prior works

Classically, the goal of optimization methods is to reduce the function value efficiently. In the smooth convex setup, Nesterov's fast gradient method (FGM)  achieves an accelerated \((1/N^{2})\)-rate, and the optimized gradient method (OGM)  improves this rate by a factor of \(2\), which is, in fact, exactly optimal .

On the other hand, Nesterov initiated the study of methods for reducing the gradient magnitude of convex functions  as such methods help us understand non-convex optimization better and design faster non-convex machine learning methods. For smooth convex functions, (GD) achieves a \(((f(x_{0})-f_{})/N)\)-rate on the squared gradient magnitude [34, Proposition 3.3.1], while (OGM-G) achieves an accelerated \(((f(x_{0})-f_{})/N^{2})\)-rate , which matches a lower bound and is therefore optimal . Interestingly, (OGM) and (OGM-G) exhibit an interesting hint of symmetry, as we detail in Section 2, and the goal of this work is to derive a more general duality principle from this observation.

In the composite optimization setup, iterative shrinkage-thresholding algorithm (ISTA)  achieves a \((\|x_{0}-x_{}\|^{2}/N)\)-rate on function-value suboptimality, while the fast iterative shrinkage-thresholding algorithm (FISTA)  achieves an accelerated \((\|x_{0}-x_{}\|/N^{2})\)-rate. On the squared gradient mapping norm, FISTA-G achieves \(((F(x_{0})-f_{})/N^{2})\)-rate , which is optimal . Analysis of an accelerated method often uses the estimate sequence technique  or a Lyapunov analysis . In this work, we focus on the Lyapunov analysis technique, as it is simpler and more amenable to a continuous-time view.

The notion of duality is fundamental in many branches of mathematics, including optimization. Lagrange duality , Wolfe duality , and Fenchel-Rockacheller duality  are related (arguably equivalent) notions that consider a pairing of primal and dual optimization problems. The recent gauge duality  and radial duality  are alternative notions of duality for optimization problems. Attouch-Thera duality  generalizes Fenchel-Rockacheller to the setup of monotone inclusion problems. In this work, we present H-duality, which is a notion of duality for optimization _algorithms_, and it is, to the best of our knowledge, distinct from any previously known duality or symmetry relations.

H-duality

In this section, we will introduce H-duality, state the main H-duality theorem, and provide applications. Let \(N 1\) be a pre-specified iteration count. Let \(\{h_{k,i}\}_{0 i<k N}\) be an array of (scalar) stepsizes and identify it with a lower triangular matrix \(H^{N N}\) via \(H_{k+1,i+1}=h_{k+1,i}\) if \(0 i k N-1\) and \(H_{k,i}=0\) otherwise. An \(N\)-step Fixed Step First Order Method (FSFOM) with \(H\) is

\[x_{k+1}=x_{k}-_{i=0}^{k}h_{k+1,i} f(x_{i}), \,k=0,,N-1\] (1)

for any initial point \(x_{0}^{d}\) and differentiable \(f\). For \(H^{N N}\), define its _anti-transpose_\(H^{A}^{N N}\) with \(H^{A}_{i,j}=H_{N-j+1,N-i+1}\) for \(i,j=1,,N\). We call [FSFOM with \(H^{A}\)] the **H-dual** of [FSFOM with \(H\)].

### Symmetry between OGM and OGM-G

Let \(f\) be an \(L\)-smooth convex function. Define the notation \(z^{+}=z- f(z)\) for \(z^{d}\). The accelerated methods OGM  and OGM-G  are

\[x_{k+1} =x_{k}^{+}+-1}{_{k+1}}(x_{k}^{+}-x_{k-1}^{ +})+}{_{k+1}}(x_{k}^{+}-x_{k})\] (OGM) \[y_{k+1} =y_{k}^{+}+-1)(2_{N-k-1}-1)}{_{N- k}(2_{N-k}-1)}(y_{k}^{+}-y_{k-1}^{+})+-1}{2_{N-k}- 1}(y_{k}^{+}-y_{k})\] (OGM-G)

for \(k=0,,N-1\), where \(\{_{i}\}_{i=0}^{N}\) are defined as \(_{0}=1\), \(_{i+1}^{2}-_{i+1}=_{i}^{2}\) for \(0 i N-2\), and \(_{N}^{2}-_{N}=2_{N-1}^{2}\).1 (OGM) and (OGM-G) are two representative accelerated methods for the setups (P1) and (P2), respectively. As a surface-level symmetry, the methods both access the \(\{_{i}\}_{i=0}^{N}\) sequence, but (OGM-G) does so in a reversed ordering . There turns out to be a deeper-level symmetry: (OGM) and (OGM-G) are H-duals of each other, i.e., \(H^{A}_{}=H_{}\). The proof structures of (OGM) and (OGM-G) also exhibit symmetry. We can analyze (OGM) with the Lyapunov function

\[_{k}=\|x_{0}-x_{}\|^{2}+_{i=0}^{k-1}u_{i } x_{i},x_{i+1}+_{i=0}^{k}(u_{i}-u_{i-1})  x_{},x_{i}\] (2)

for \(-1 k N\) with \(\{u_{i}\}_{i=0}^{N}=(2_{0}^{2},,2_{N-1}^{2},_{N}^{2})\) and \(u_{-1}=0\). Since \(, 0\) and \(\{u_{i}\}_{i=0}^{N}\) is a positive monotonically increasing sequence, \(\{_{k}\}_{k=-1}^{N}\) is dissipative, i.e., \(_{N}_{N-1}_{0} _{-1}\). So

\[_{N}^{2}(f(x_{N})-f_{})_{N}^{2}(f(x_{N})-f_ {})+\|x^{}-x_{0}+z\|^{2}}{{=}}_{N}_{-1}=-x^{}\|^ {2}}{2},\]

where \(z=_{i=0}^{N}-u_{i-1}}{L} f(x_{i})\). The justification of \(()\) is the main technical challenge of this analysis, and it is provided in Appendix B.2. Dividing both sides by \(_{N}^{2}\), we conclude the rate

\[f(x_{N})-f_{}^{2}}\|x_{0}-x^{}\|^{2}.\]

Likewise, we can analyze (OGM-G) with the Lyapunov function

\[_{k}=v_{0}(f(y_{0})-f_{}+ y_{N}, )+_{i=0}^{k-1}v_{i+1} y_{i},y_{i+1}+ _{i=0}^{k-1}(v_{i+1}-v_{i}) y_{N},y_{i}\] (3)

for \(0 k N\) with \(\{v_{i}\}_{i=0}^{N}=(^{2}},^{2}}, ,^{2}})\). Similarly, \(\{_{k}\}_{k=0}^{N}\) is dissipative, so

\[\| f(y_{N})\|^{2}}{{=}} _{N}_{0}=^{2}}(f(y_{0})-f_{ })+^{2}} y_{N}, ^{2}}(f(y_{0})-f_{}).\]Again, the justification of \(()\) is the main technical challenge of this analysis, and it is provided in Appendix B.2. The crucial observations are **(i)**\(u_{i}=1/v_{N-i}\) for \(0 i N\) and **(ii)** the convergence rates share the identical factor \(1/_{N}^{2}=1/u_{N}=v_{0}\). Interestingly, a similar symmetry relation holds between method pairs \([(_{}),(_{})]\) and \([(),()]\), which we discuss later in Section 2.4.

### H-duality theorem

The symmetry observed in Section 2.1 is, in fact, not a coincidence. Suppose we have \(N\)-step FSFOMs with \(H\) and \(H^{A}\). We denote their iterates as \(\{x_{i}\}_{i=0}^{N}\) and \(\{y_{i}\}_{i=0}^{N}\). For clarity, note that \(\{u_{i}\}_{i=0}^{N}\) are free variables and can be appropriately chosen for the convergence rate analysis. For the FSFOM with \(H\), define \(\{_{k}\}_{k=-1}^{N}\) with the general form (2) with \(u_{-1}=0\). If \(0=u_{-1} u_{0} u_{1} u_{N}\), then \(\{_{k}\}_{k=-1}^{N}\) is monotonically nonincreasing (dissipative). Assume we can show

\[u_{N}(f(x_{N})-f_{})_{N}(\,x_{0},x_{},  f(x_{0}),, f(x_{N}){^{d}}).\] (C1)

To clarify, since \(\{x_{i}\}_{i=0}^{N}\) lies within \(\{x_{0}, f(x_{0}),, f(x_{N})\}\), the \(_{N}\) depends on \(x_{0},x_{},\{ f(x_{i})\}_{i=0}^{N},\{u_{i}\}_{i=0}^{N},H\). If (C1) holds, the FSFOM with \(H\) exhibits the convergence rate

\[u_{N}(f(x_{N})-f_{})_{N}_{-1}= \|x_{0}-x_{}\|^{2}.\] (4)

For the FSFOM with \(H^{A}\), define \(\{_{k}\}_{k=0}^{N}\) with the general form (3). Also, note that \(\{v_{i}\}_{i=0}^{N}\) are free variables and can be appropriately chosen for the convergence rate analysis. If \(0 v_{0} v_{1} v_{N}\), then \(\{_{k}\}_{k=0}^{N}\) is monotonically nonincreasing (dissipative). Assume we can show

\[\| f(y_{N})\|^{2}_{N}(\,y_{0},  f(y_{0}),, f(y_{N})^{d},\,f_{}).\] (C2)

To clarify, since \(\{y_{i}\}_{i=0}^{N}\) lies within \(\{y_{0}, f(y_{0}),, f(y_{N})\}\), the \(_{N}\) depends on \(y_{0},\{ f(y_{i})\}_{i=0}^{N},f_{},\{v_{i}\}_{i=0}^{N},H^{A} \). If (C2) holds, the FSFOM with \(H^{A}\) exhibits the convergence rate

\[\| f(y_{N})\|^{2}_{N}_{0}=v_{0}(f(y_{0})-f_{})+v_{0} y_{N}, v_{0} (f(y_{0})-f_{}).\] (5)

We now state our main H-duality theorem, which establishes a correspondence between the two types of bounds for the FSFOMs induced by \(H\) and \(H^{A}\).

**Theorem 1**.: Consider sequences of positive real numbers \(\{u_{i}\}_{i=0}^{N}\) and \(\{v_{i}\}_{i=0}^{N}\) related through \(v_{i}=}\) for \(i=0,,N\). Let \(H^{N N}\) be lower triangular. Then,

\[()\{u_{i}\}_{i=0}^{N}H()\{v_{i}\}_{i=0}^{N}H^{A}\,.\]

Theorem 1 provides a sufficient condition that ensures an FSFOM with \(H\) with a convergence guarantee on \((f(x_{N})-f_{})\) can be H-dualized to obtain an FSFOM with \(H^{A}\) with a convergence guarantee on \(\| f(y_{N})\|^{2}\). To the best of our knowledge, this is the first result establishing a symmetrical relationship between (P1) and (P2). Section 2.3 provides a proof outline of Theorem 1.

### Proof outline of Theorem 1

Define

\[ =_{N}-u_{N}(f(x_{N})-f_{})-x_{ }-x_{0}+_{i=0}^{N}(u_{i}-u_{i-1}) f(x_{i}) ^{2}\] \[ =_{N}-\| f(y_{N})\|^{2}.\]

Expanding \(\) and \(\) reveals that all function value terms are eliminated and only quadratic terms of \(\{ f(x_{i})\}_{i=0}^{N}\) and \(\{ f(y_{i})\}_{i=0}^{N}\) remain. Now, (C1) and (C2) are equivalent to the conditions

\[ 0,\  f(x_{0}),, f(x_{N}) ^{d}\,, 0\  f(y_{0}),, f(y_{N})^{d}\, ,\]respectively. Next, define \(g_{x}=[ f(x_{0})| f(x_{1})|| f(x_{N})] ^{d(N+1)}\) and \(g_{y}=[ f(y_{0})| f(y_{1})|| f(y_{N})] ^{d(N+1)}\). We show that there is \((H,u)\) and \((H^{A},v)^{N+1}\) such that

\[=(g_{x}(H,u)g_{x}^{}),=(g_{y}(H^{A},v)g_{y}^{ }).\]

Next, we find an explicit invertible matrix \(M(u)^{(N+1)(N+1)}\) such that \((H,u)=(u)^{}(H^{A},v)(u)\). Therefore,

\[(g_{x}(H,u)g_{x}^{})\,= \,(g_{y}(H^{A},v)g_{y}^{})\]

with \(g_{y}=g_{x}(u)^{}\) and we conclude the proof. This technique of considering the quadratic forms of Lyapunov functions as a trace of matrices is inspired by the ideas from the Performance Estimation Problem (PEP) literature [19; 55]. The full proof is given in Appendix A.

### Verifying conditions for H-duality theorem

In this section, we illustrate how to verify conditions (C1) and (C2) through examples. Detailed calculations are deferred to Appendices B.1 and B.2.

Example 1.For (OGM) and (OGM-G), the choice

\[\{u_{i}\}_{i=0}^{N}=(2_{0}^{2},,2_{N-1}^{2},_{N}^{2}),\{v_{i}\}_{i=0}^{N}=(^{2}},^{2}},,^{2}})\]

leads to

\[=0,=0.\]

Therefore, (C1) and (C2) hold.

Example 2.Again, define \(z^{+}=z- f(z)\) for \(z^{d}\). Consider the FSFSOMs 

\[& x_{k+1}=x_{k}^{+}+(x_{k}^{+}-x_{k- 1}^{+})+(x_{k}^{+}-x_{k}) k=0,,N-2\\ & x_{N}=x_{N-1}^{+}+(x_{N-1}^{+}-x_{N -2}^{+})+(x_{N-1}^{+}-x_{N-1})\] (OBL-F \[{}_{}\] )

and

\[& y_{1}=y_{0}^{+}+(y_{0}^ {+}-y_{-1}^{+})+(y_{0}^{+}-y_{0})\\ & y_{k+1}=y_{k}^{+}+(y_{k}^{+}-y_{k-1}^{+ })+(y_{k}^{+}-y_{k}) k=1,,N-1 \] (OBL-G \[{}_{}\] )

where \(y_{-1}^{+}=y_{0}\), \(x_{-1}^{+}=x_{0}\) and \(=\). It turns out that (OBL-F\({}_{}\)) and (OBL-G\({}_{}\)) are H-duals of each other. The choice

\[\{u_{i}\}_{i=0}^{N}=(,,,^{2 }+),\{v_{i}\}_{i=0}^{N}=(+}, ,,)\]

leads to

\[=_{i=0}^{N}-u_{i-1}}{2L}\| f(x_{i})\|^{2}, =}{2L}\| f(y_{N})\|^{2}+_{i=0}^{N-1}-v_{i}}{2L}\| f(y_{i})- f(y_{N})\|^{2}\]

where \(u_{-1}=0\). Since \(\) and \(\) are expressed as a sum of squares, (C1) and (C2) hold.

Example 3.Interestingly, (GD) is a self-dual FSFOM in the H-dual sense. For the case \(h=1\), the choice

\[\{u_{i}\}_{i=0}^{N}=(,,,2N+1), \{v_{i}\}_{i=0}^{N}=(,,,)\]

leads to

\[=_{0 i,j N}}{L} f(x_{i}),  f(x_{j}),=_{0 i,j N}} {L} f(y_{i}), f(y_{j})\]

for some \(\{s_{ij}\}\) and \(\{t_{ij}\}\) stated precisely in Appendix B.2. \( 0\) can be established by showing that the \(\{t_{ij}\}\) forms a diagonally dominant and hence positive semidefinite matrix . \( 0\) can be established with a more elaborate argument , but that is not necessary; \( 0\) implies (C2), and, by Theorem 1, this implies (C1).

### Applications of the H-duality theorem

A family of gradient reduction methods.Parameterized families of accelerated FSFOMs for reducing function values have been presented throughout the extensive prior literature. Such families generalize Nesterov's method and elucidate the essential algorithmic component that enables acceleration. For reducing gradient magnitude, however, there are only four accelerated FSFOMs (OGM-G), (OBL-G\({}_{}\)), and M-OGM-G , and [17, Lemma 2.6]. Here, we construct a simple parameterized family of accelerated FSFOMs for reducing gradient magnitude by H-dualizing an accelerated FSFOM family for reducing function values.

Let \(\{t_{i}\}_{i=0}^{N}\) and \(\{T_{i}\}_{i=0}^{N}\) be sequences positive real numbers satisfying \(t_{i}^{2} 2T_{i}=2_{j=0}^{i}t_{j}\) for \(0 i N-1\) and \(t_{N}^{2} T_{N}=_{j=0}^{N}t_{j}\). Consider a family of FSFOMs

\[x_{k+1}=x_{k}^{+}+-t_{k})t_{k+1}}{t_{k}T_{k+1}}(x_{k}^{+}-x_{k -1}^{+})+^{2}-T_{k})t_{k+1}}{t_{k}T_{k+1}}(x_{k}^{+}-x_ {k})\] (6)

for \(k=0,1,,N-1\), where \(x_{-1}^{+}=x_{0}\). This family coincides with the GOGM of , and it exhibits the rate [28, Theorem 5]

\[f(x_{N})-f_{}}\|x_{0}-x_{}\|^{2},\]

which can be established from (2) with \(u_{i}=T_{i}\) for \(0 i N\).

**Corollary 1**.: The H-dual of (6) is

\[y_{k+1}=y_{k}^{+}+(t_{N-k-1}-1)}{T_{N-k}(t_{N-k}-1)}(y_{k} ^{+}-y_{k-1}^{+})+^{2}-T_{N-k})(t_{N-k-1}-1)}{T_{N-k}(t_{ N-k}-1)}(y_{k}^{+}-y_{k})\]

for \(k=0,,N-1\), where \(y_{-1}^{+}=y_{0}\), and it exhibits the rate

\[\| f(y_{N})\|^{2}}(f(y_{0 })-f_{}).\]

Proof outline.: By Theorem 1, (C2) holds with \(v_{i}=1/T_{N-i}\) for \(0 i N\). We then use (5). 

When \(T_{i}=t_{i}^{2}\) for \(0 i N\), the FSFOM (6) reduces to Nestrov's FGM  and its H-dual is, to the best of our knowledge, a new method without a name. If \(t_{i}^{2}=2T_{i}\) for \(0 i N-1\) and \(t_{N}^{2}=T_{N}\), (6) reduces to (OGM) and its H-dual is (OGM-G). If \(t_{i}=i+1\) for \(0 i N-1\) and \(t_{N}=\), (6) reduces to (OBL-F\({}_{}\)) and its H-dual is (OBL-G\({}_{}\)).

Gradient magnitude rate of (GD).For gradient descent (GD) with stepsize \(h\), the \(H\) matrix is the identity matrix scaled by \(h\), and the H-dual is (GD) itself, i.e., (GD) is self-dual. For \(0<h 1\), the rate \(f(x_{N})-f_{}\|x_{0}-x_{}\|^{2}\), originally due to , can be established from (2) with \(\{u_{i}\}_{i=0}^{N}=(,,,2Nh+1)\). Applying Theorem 1 leads to the following.

**Corollary 2**.: Consider (GD) with \(0<h 1\) applied to an \(L\)-smooth convex \(f\). For \(N 1\),

\[\| f(x_{N})\|^{2}()-f_ {}}{2Nh+1},-x_{}\|^{2}}{2(2 {2} h+1)(2 h+1)}).\]

To the best of our knowledge, Corollary 2 is the tightest rate on gradient magnitude for (GD) for the general step size \(0<h<1\), and it matches [53, Theorem 3] for \(h=1\).

Resolving conjectures of \(^{}\)-optimality of (OGM-G) and (OBL-F\({}_{}\)).The prior work of  defines the notion of \(^{}\)-optimality, a certain restricted sense of optimality of FSFOMs, and shows that (OGM) and (OBL-F\({}_{}\)) are \(^{}\)-optimal under a certain set of relaxed inequalities. On the other hand, \(^{}\)-optimality of (OGM-G) and (OBL-G\({}_{}\)) are presented as conjectures. Combining Theorem 1 and the \(^{}\)-optimality of (OGM) and (OBL-F\({}_{}\)) resolves these conjectures; (OGM-G) and (OBL-G\({}_{}\)) are \(^{}\)-optimal.

### Intuition behind energy functions: Lagrangian Formulation

One might ask where the energy functions (2) and (3) came from. In this section, we provide an intuitive explanation of these energy functions using the QCQP and its Lagrangian. Consider an FSFOM (1) given with a lower triangular matrix \(H^{N N}\) with function \(f\), resulting in the sequence \(\{x_{i}\}_{i=0}^{N}\). To analyze the convergence rate of the function value, we formulate the following optimization problem:

\[_{f} f(x_{N})-f_{}\] subject to \[f^{n},\|x_{0}-x_{}\|^{2} R^{2}.\]

However, this optimization problem is not solvable, as \(f\) is a functional variable. To address this,  demonstrated its equivalence to a QCQP:

\[_{f} f(x_{N})-f_{}\] subject to \[[\![x_{i},x_{j}]\!] 0[i,j][-1,,N]^{2},\|x_{0 }-x_{}\|^{2} R^{2}\]

where \(x_{-1}=x_{}\). To clarify, the optimization variables are \(\{ f(x_{i}),f(x_{i})\}_{i=0}^{N}\), \(f_{}\), \(x_{0}\), and \(x_{}\) since \(\{x_{i}\}_{i=0}^{N}\) lies within \(\{x_{0}, f(x_{0}),, f(x_{N})\}\). We consider a relaxed optimization problem as :

\[_{f} f(x_{N})-f_{}\] subject to \[[\![x_{i},x_{i+1}]\!] 0, i=0,1,,N-1,[\![x_{ },x_{i}]\!] 0, i=0,1,,N,\] \[\|x_{0}-x_{}\|^{2} R^{2}\]

Now, consider the Lagrangian function and the convex dual.

\[_{1}(f,\{a_{i}\}_{i=0}^{N-1},\{b_{i}\}_{i=0}^{N},)=-f(x_{N}) +f_{}+_{i=0}^{N-1}a_{i}[\![x_{i},x_{i+1}]\!]+_{i=0}^{N}b_{i}[\![x_ {},x_{i}]\!]+\|x_{0}-x_{}\|^{2}- R^{2}\]

where \(\{a_{i}\}_{i=0}^{N-1}\), \(\{b_{i}\}_{i=0}^{N}\), and \(\) are dual variables which are nonnegative. Considering \(a_{-1}=0\) and \(a_{N}=1\), the infimum of \(_{1}\) equals \(-\) unless \(b_{i}=a_{i}-a_{i-1}\) for \(0 i N\). Therefore, by introducing \(u_{N}=\) and \(u_{i}=a_{i}u_{N}\) for \(0 i N\), the convex dual problem can be simplified as follows:

\[_{\{u_{i}\}_{i=0}^{N}} -}{2u_{N}}\] s.t. \[_{x_{0},x_{},\{ f(x_{i})\}_{i=0}^{N}}-u_{N}(f(x_{N })-f_{})+\|x_{0}-x_{}\|^{2}+_{i=0}^{N-1}u_{i}[\![x_{ i},x_{i+1}]\!]+_{i=0}^{N}(u_{i}-u_{i-1})[\![x_{},x_{i}]\!] 0\] \[\{u_{i}\}_{i=0}^{N}\]

If the above two constraints holds for \(\{u_{i}\}_{i=0}^{N}\), we have \(f(x_{N})-f_{}}R^{2}\). This understanding motivates the introduction of (2) and (C1). We can perform a similar analysis on the gradient norm minimization problem with the relaxed optimization problem as follows:

\[_{f} \| f(y_{N})\|^{2}\] subject to \[[\![y_{i},y_{i+1}]\!] 0, i=0,1,,N-1,[\![y_{N},y_{i}]\!]  0, i=0,1,,N-1,\] \[[\![y_{N},]\!] 0, f(y_{0})-f_{} R.\]

Finally, we note that although (2) and (3) both originate from the relaxed optimization problems, they have been commonly employed to achieve the convergence analysis. The function value convergence rate of OGM, FGM, G-OGM, GD, OBL-F\({}_{9}\) can be proved by using (2) with appropriate \(\{u_{i}\}_{i=0}^{N}\). The gradient norm convergence rate of OGM-G, OBL-G\({}_{9}\), M-OGM-G, and [17, Lemma 2.6] can be proved by using (3) with appropriate \(\{v_{i}\}_{i=0}^{N}\). We also note that recent works  do not employ (2) to achieve the convergence rate, particularly for gradient descent with varying step sizes.

H-duality in continuous time

We now establish a continuous-time analog of the H-duality theorem. As the continuous-time result and, especially, its proof is much simpler than its discrete-time counterpart, the results of this section serve as a vehicle to convey the key ideas more clearly. Let \(T>0\) be a pre-specified terminal time. Let \(H(t,s)\) be an appropriately integrable2 real-valued kernel with domain \(\{(t,s)\,|\,0<s<t<T\}\). We define a Continuous-time Fixed Step First Order Method (C-FSFOM) with \(H\) as

\[X(0)=x_{0},(t)=-_{0}^{t}H(t,s) f(X(s))\;ds,\, t(0,T)\] (7)

for any initial point \(x_{0}^{d}\) and differentiable \(f\). Note, the Euler discretization of C-FSFOMs (7) corresponds to FSFOMs (1). The notion of C-FSFOMs has been considered previously in .

Given a kernel \(H(t,s)\), analogously define its _anti-transpose_ as \(H^{A}(t,s)=H(T-s,T-t)\). We call [C-FSFOM with \(H^{A}\)] the **H-dual** of [C-FSFOM with \(H\)]. In the special case \(H(t,s)=e^{(s)-(t)}\) for some function \(()\), the C-FSFOMs with \(H\) and its H-dual have the form

\[(t)+(t)(t)+ f(X(t))=0\] (C-FSFOM with \[H(t,s)=e^{(s)-(t)}\] ) \[(t)+(T-t)(t)+ f(Y(t))=0\] (C-FSFOM with \[H^{A}(t,s)\] )

Interestingly, friction terms with \(^{}\) have time-reversed dependence between the H-duals, and this is why we refer to this phenomenon as time-reversed dissipation.

### Continuous-time H-duality theorem

For the C-FSFOM with \(H\), define the energy function

\[(t)=\|X(0)-x_{}\|^{2}+_{0}^{t}u^{}(s)[x_ {},X(s)]ds\] (8)

for \(t[0,T]\) with differentiable \(u(0,T)\). If \(u^{}() 0\), then \(\{(t)\}_{t[0,T]}\) is dissipative. Assume we can show

\[u(T)(f(X(T))-f_{})(T)(\,X(0),x_{ },\{ f(X(s))\}_{s[0,T]}^{d}).\] (C3)

Then, the C-FSFOM with \(H\) exhibits the convergence rate

\[u(T)(f(X(T))-f_{})(T)(0)=\|X(0)-x_{}\|^{2}.\]

For the C-FSFOM with \(H^{A}\), define the energy function

\[(t)=v(0)f(Y(0))-f(Y(T))+_{0}^{t}v^{}(s)[Y(T ),Y(s)]ds\] (9)

for \(t[0,T]\) with differentiable \(v(0,T)\). If \(v^{}() 0\), then \(\{(t)\}_{t[0,T]}\) is dissipative. Assume we can show

\[\| f(Y(T))\|^{2}(T)(\,Y(0),\{  f(Y(s))\}_{s[0,T]}^{d}).\] (C4)

Then, the C-FSFOM with \(H^{A}\) exhibits the convergence rate

\[\| f(Y(T))\|^{2}(T)(0)=v(0) (f(Y(0))-f(Y(T))) v(0)(f(Y(0))-f_{}).\]

**Theorem 2** (informal).: Consider differentiable functions \(u,v(0,T)\) related through \(v(t)=\) for \(t[0,T]\). Assume certain regularity conditions (specified in Appendix C.2). Then,

\[[] $}\,.\]

The formal statement of 2 and its proof are given in Appendix C.2. Loosely speaking, we can consider Theorem 2 as the limit of Theorem 1 with \(N\).

### Verifying conditions for H-duality theorem

As an illustrative example, consider the case \(H(t,s)=}{t^{r}}\) for \(r 3\) which corresponds to an ODE studied in the prior work [50; 51]. For the C-FSFOM with \(H\), the choice \(u(t)=}{2(r-1)}\) for the dissipative energy function \(\{(t)\}_{t=0}^{T}\) of (8) leads to

\[(T)-u(T)(f(X(T))-f_{})=(T)+2(X (T)-x_{})\|^{2}+2(r-3)\|X(T)-x_{}\|^{2}}{4(r-1)}+ _{0}^{T}\|(s)\|^{2}ds.\]

For the C-FSFOM with \(H^{A}\), the choice \(v(t)==}\) for the dissipative energy function \(\{(t)\}_{t=0}^{T}\) of (9) leads to

\[(T)-\| f(Y(T))\|^{2}=}{T^{4}}+_{0}^{T}(s)+2(Y(s)-Y(T) )\|^{2}}{(T-s)^{5}}ds.\]

Since the right-hand sides are expressed as sums/integrals of squares, they are nonnegative, so (C3) and (C4) hold. (By Theorem 2, verifying (C3) implies (C4) and vice versa.) The detailed calculations are provided in Appendix C.1.

### Applications of continuous-time H-duality theorem

The C-FSFOM (7) with \(H(t,s)=s^{2p-1}}{t^{p+1}}\) recovers

\[(t)+(t)+Cp^{2}t^{p-2} f(X(t))=0,\]

an ODE considered in . The rate \(f(X(T))-f_{}}\|X(0)-x_{}\|^{2}\) can be established from (8) with \(u(t)=Ct^{p}\). The C-FSFOM with \(H^{A}\) can be expressed as the ODE

\[(t)+(t)+Cp^{2}(T-t)^{p-2} f(Y(t))=0.\] (10)

By Theorem 2, using (9) with \(v(t)=}\) leads to the rate

\[\| f(Y(T))\|^{2}}(f(Y(0))-f_{} ).\]

Note that the continuous-time models of (OGM) and (OGM-G), considered in , are special cases of this setup with \(p=2\) and \(C=1/2\). The detailed derivation and well-definedness of the ODE are presented in Appendix C.3.

## 4 New method efficiently reducing gradient mapping norm: (SFG)

In this section, we introduce a novel algorithm obtained using the insights of Theorem 1. Consider minimizing \(F(x):=f(x)+g(x)\), where \(f^{d}\) is \(L\)-smooth convex with \(0<L<\) and \(g^{d}\{\}\) is a closed convex proper function. Write \(F_{}=_{x^{n}}F(x)\) for the minimum value. For \(>0\), define the \(\)-_proximal gradient step_ as

\[y^{,}=*{argmin}_{z^{n}}(f(y)+  f(y),z-y+g(z)+\|z-y\|^{2} )=_{}(y-  f(y)).\]

Consider FSFOMs defined by a lower triangular matrix \(H=\{h_{k,i}\}_{0 i<k N}\) as follows:

\[x_{k+1}=x_{k}-_{i=0}^{k} h_{k+1,i}(x_{i}-x_{i}^{,} ),\,k=0,,N-1.\]

When \(g=0\), this reduces to (1). FISTA , FISTA-G  and GFPGM  are instances of this FSFOM with \(=1\). In this section, we present a new method for efficiently reducing the gradient mapping norm. This method is faster than the prior state-of-the-art FISTA-G  by a constant factor of \(5.28\) while having substantially simpler coefficients.

**Theorem 3**.: Consider the method

\[y_{k+1} =y_{k}^{,4}+(y_{k }^{,4}-y_{k-1}^{,4})+(y_{k}^{,4}-y_{k})\] \[y_{N} =y_{N-1}^{,4}+(y_{N-1}^{,4}-y_{N-2}^ {,4})+(y_{N-1}^{,4}-y_{N-1})\] (SFG)

for \(k=0,,N-2\), where \(y_{-1}^{,4}=y_{0}\). This method exhibits the rate

\[_{v F(y_{N}^{,4})}\|v\|^{2} 25L^{2}\|y_ {N}-y_{N}^{,4}\|^{2}(F(y_{0})-F_{ }).\]

We call this method _Super FISTA-G_ (SFG), and in Appendix D.3, we present a further general parameterized family (SFG-family). To derive (SFG-family), we start with the parameterized family GFPGM , which exhibits an accelerated rate on function values, and expresses it as FSFOMs with \(H\). We then obtain the FSFOMs with \(H^{A}+C\), where \(C\) is a lower triangular matrix satisfying certain constraints. We find that the appropriate H-dual for the composite setup is given by this \(H^{A}+C\), rather than \(H^{A}\). We provide the proof of Theorem 3 in Appendix D.2.

(SFG) is an instance of (SFG-family) with simple rational coefficients. Among the family, the optimal choice has complicated coefficients, but its rate has a leading coefficient of \(46\), which is slightly smaller than the \(50\) of (SFG). We provide the details Appendix D.4.

## 5 Conclusion

In this work, we defined the notion of H-duality and formally established that the H-dual of an optimization method designed to efficiently reduce function values is another method that efficiently reduces gradient magnitude. For optimization algorithms, the notion of equivalence, whether informal or formal , is intuitive and standard. For optimization problems, the notion of equivalence is also standard, but the beauty of convex optimization is arguably derived from the elegant duality of optimization problems. In fact, there are many notions of duality for spaces, problems, operators, functions, sets, etc. However, the notion of duality for algorithms is something we, the authors, are unfamiliar with within the context of optimization, applied mathematics, and computer science. In our view, the significance of this work is establishing the first instance of a duality of algorithms. The idea that an optimization algorithm is an abstract mathematical object that we can take the dual of opens the door to many interesting questions. In particular, exploring for what type of algorithms the H-dual or a similar notion of duality makes sense is an interesting direction for future work.