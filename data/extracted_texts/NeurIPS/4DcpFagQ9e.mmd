# Score Distillation via Reparametrized DDIM

Artem Lukoianov1

Haitz Saez de Ocariz Borde2

Kristjan Greenewald3

Vitor Campagnolo Guizilini4

Timur Bagautdinov5

Vincent Sitzmann1

Justin Solomon1

###### Abstract

While 2D diffusion models generate realistic, high-detail images, 3D shape generation methods like Score Distillation Sampling (SDS) built on these 2D diffusion models produce cartoon-like, over-smoothed shapes. To help explain this discrepancy, we show that the image guidance used in Score Distillation can be understood as the velocity field of a 2D denoising generative process, up to the choice of a noise term. In particular, after a change of variables, SDS resembles a high-variance version of Denoising Diffusion Implicit Models (DDIM) with a differently-sampled noise term: SDS introduces noise i.i.d. randomly at each step, while DDIM infers it from the previous noise predictions. This excessive variance can lead to over-smoothing and unrealistic outputs. We show that a better noise approximation can be recovered by inverting DDIM in each SDS update step. This modification makes SDS's generative process for 2D images almost identical to DDIM. In 3D, it removes over-smoothing, preserves higher-frequency detail, and brings the generation quality closer to that of 2D samplers. Experimentally, our method achieves better or similar 3D generation quality compared to other state-of-the-art Score Distillation methods, all without training additional neural networks or multi-view supervision, and providing useful insights into relationship between 2D and 3D asset generation with diffusion models.

Figure 1: Score Distillation Sampling (SDS) “distills” 3D shapes from 2D image generative models like DDIM. While DDIM produces high-quality images (a), the same diffusion model, yields blurry results with SDS in the task of 2D image generation (b); in 3D, SDS yields over-saturated and simplified shapes (d). By replacing the noise term in SDS to agree with DDIM, our algorithm better matches the quality of the diffusion model in 2D (c) and significantly improves 3D generation (e).

## 1 Introduction

Image generative modeling saw drastic quality improvement with the advent of text-to-image diffusion models  trained on billion-scale datasets  with large parameter counts . From a short prompt, these models generate photorealistic images, with strong zero-shot generalization to new classes . Efficient training methods for image data, combined with Internet-scale datasets, enabled the development of these models. However, applying similar techniques to domains where huge datasets are scarce, such as 3D shape generation, remains challenging.

The need for 3D objects in downstream applications like vision, graphics, and robotics motivated methods like Score Distillation Sampling (SDS)  and Score Jacobian Chaining (SJC) , which optimize volumetric 3D representations [7; 8; 9] using queries to a 2D generative model . In every iteration, SDS renders the current state of the 3D representation from a random viewpoint, adds noise to the result, and then denoises it using the pre-trained 2D diffusion model conditioned on a text prompt. The difference between the added and predicted noise is used as a gradient-style update on the rendered images, which is propagated to the parameters of the 3D model. The underlying 3D representation helps make the generated images multi-view consistent, and the 2D model guides individual views towards a learned distribution of realistic images.

In practice, however, as noted in [11; 12; 13], SDS often produces 3D representations with over-saturated colors and over-smoothed textures (fig. 1d), not matching the quality of the underlying 2D model. Existing approaches tackling this problem improve quality at the cost of expensive re-training or fine-tuning of the image diffusion model , complex multi-stage handling of 3D representations like mesh extraction and texture fine-tuning [14; 15; 11], or altering the SDS guidance [13; 12; 16].

As an alternative to engineering-based improvements to SDS, in this paper we reanalyze the vanilla SDS algorithm to understand the underlying source of artifacts. Our key insight is that the SDS update rule steps along an approximation of the DDIM velocity field. In particular, we derive Score Distillation from DDIM with a change of variables to the space of single-step denoised images. In this light, SDS updates are nearly identical to DDIM updates, apart from one difference: while DDIM samples noise _conditionally_ on the previous predictions, SDS resamples noise i.i.d. in every iteration. This breaks the denoising trajectory for each independent view and introduces excessive variance. Our perspective unifies DDIM and SDS and helps explain why SDS can produce blurry and

Figure 2: Examples of 3D objects generated with our method.

over-saturated results: the variance-boosting effect of noisy guidance is usually mitigated with high Classifier-Free Guidance (CFG)  to reduce sample diversity at the cost of over-saturation .

Based on our analysis, we propose an alternative score distillation algorithm dubbed Score Distillation via Inversion (SDI), closing the gap to DDIM. We obtain the conditional noise required for consistency of the denoising trajectories by inverting DDIM on each step of score distillation (fig. 5). This modification yields 3D objects with high-quality textures consistent with the 2D diffusion model (fig. 1e). Moreover, in 2D, our method closely approximates DDIM while preserving the incremental generation schedule of SDS (fig. 1c).

Our key contributions are as follows:

* We prove that guidance for each view in the SDS algorithm is a simplified reparameterization of DDIM sampling: vanilla SDS samples random noise at each step, while DDIM keeps the trajectories consistent with previously-predicted noise.
* We propose a new method titled Score Distillation via Inversion (SDI), which replaces the problematic random noise sampling in SDS with prompt-conditioned DDIM inversion and significantly improves 3D generation, closing the quality gap to samples from the 2D model.
* We systematically compare SDI with the state-of-the-art Score Distillation algorithms and show that SDI achieves similar or better generation quality, while not requiring training additional neural networks or multiple generation stages.

## 2 Related work

**3D generation by training on multi-view data.** Recent 3D generation methods leverage multi-view or 3D data. Zero123  and MVDream  generate consistent multi-view images from text; a 3D radiance field is then obtained via score distillation. Video generative models can be fine-tuned on videos of camera tracks around 3D objects, similarly yielding a model that samples multi-view consistent images to train a 3D radiance field [21; 22]. Diffusion with Forward Models  and Viewset Diffusion  directly train 3D generative models from 2D images. While these methods excel at generating multi-view consistent, plausible 3D objects, they depend on multi-view data with known camera trajectories, limiting them to synthetic or small bundle-adjusted 3D datasets. We instead focus on methods that require only single-view training images.

**Distilling 2D into 3D.** Score Distillation was introduced concurrently in Dreamfusion or SDS , Score Jacobian Chaining (SJC) , and Magic3D . The key idea is to use a frozen diffusion model trained on 2D images and "distill" it into 3D assets. A volumetric representation of the shape is rendered from a random view, perturbed with random noise, and denoised with the diffusion model; the difference between added and predicted noise is used to improve the rendering. These works, however, suffer from over-smoothing and lack of detail. Usually a high value of classifier free guidance (CFG \( 100\))  is used to reduce variance at the cost of over-saturation .

ProlificDreamer  generates sharp, detailed results with standard CFG values (\( 7.5\)) and without over-saturation. The key idea is to overfit an additional diffusion model to specifically denoise the current 3D shape estimate. Fine-tuning the second model, however, is cumbersome and theoretical justification for this change is still unclear. Recent papers further improve on ProlificDreamer's results or try to explain its behavior. SteinDreamer , for example, hypothesizes that ProlificDreamer's improvements come from variance reduction of the sampling procedure .

Other papers propose heuristics that improve SDS. [12; 16; 27] decompose the guidance terms and speculate about their relative importance. Empirically, visual quality can be improved by suppressing the denoising term with negative prompts  or highlighting the classification term . [14; 15; 11; 28] use multi-stage optimization: they first train a volumetric representation and then extract a mesh or voxel grid to fine-tune geometry and texture. HiFA  combines ad-hoc techniques: additional supervision in latent space, time annealing, kernel smoothing, \(z\)-variance regularization, and a pretrained mono-depth estimation network to improve the quality of single-stage NeRF generation.

LucidDreamer, or Interval Score Matching (ISM), hypothesizes based on empirical evidence that in SDS, the high-variance random noise term and large reconstruction error of a single-step prediction togehter cause over-smoothing . Based on these observations, the authors replace the random noise term in SDS with noise obtained by running DDIM inversion and introduce multi-step denoising to improve reconstruction quality. As we will show in section 4, the update rule of ISM can be seen as a special case of our formulation. Moreover, our analysis reveals that the added noise should be inferred _conditionally_ on the text prompt \(y\), which further improves quality.

In this work, rather than augmenting the SDS pipeline or relying on heuristics, we derive Score Distillation through the denoising process of DDIM and propose a simple modification of SDS that significantly improves 3D generation.

## 3 Background

**Diffusion models.** Denoising Diffusion Implicit Models (DDIM) generate images by reversing a diffusion process [30; 31; 32]. After training a denoiser \(^{t}_{}\) and freezing its weights \(\), the denoising process can be seen as an ODE on rescaled noisy images \((t)=x(t)/\). Given a prompt \(y\) and current time step \(t\), the denoising process satisfies:

\[(t)}{dt}=^{t}_{}(t),y,\] (1)

where \((1)\) is sampled from a Gaussian distribution, \((t)=/\), and \((t)\) are scaling factors. When discretized with forward Euler, this equation yields the following update to transition from step \(t\) to a less noisy step \(t-<t\):

\[(t-)=(t)+^{t}_{}(t),y[(t-)-(t)].\] (2)

The DDIM ODE can also be integrated in reverse direction to estimate \((t)\) for any \(t\) from a clean image \(x_{0}\). This operation is called _DDIM inversion_ and is studied in multiple works [33; 34].

**Classifier-free guidance.** Classifier-free guidance (CFG)  provides high-quality conditional samples without gradients from auxilary models . CFG modifies the noise prediction \(^{t}_{}\) (score function) by linearly combining conditional and unconditional predictions:

\[^{t}_{}x(t),y=^{t}_{}(x(t), )+^{t}_{}(x(t),y)-^{t}_{}(x(t),),\] (3)

where the guidance scale \(\) is a scalar, with \(=0\) corresponding to unconditional sampling and \(=1\) to conditional. In practice, larger values \(>1\) are necessary to obtain high-quality samples at a cost of reduced diversity and extreme over-saturation. We demonstrate the effect of different CFG values in fig. 3. For the rest of the paper we use the modified CFG version of the denoiser \(^{t}_{}x(t),y\).

**Score Distillation.** Diffusion models efficiently generate images and can learn to represent common objects from arbitrary angles  and with varying lighting . Capitalizing on this success, Score Distillation Sampling (SDS)  distills a pre-trained diffusion model \(^{t}_{}\) to produce a 3D asset. In practice, the 3D shape is usually parameterized by a NeRF , InstantNGP , or Gaussian Splatting . Multiple works additionally extract an explicit representation for further optimization [14; 15; 11; 28]. We use InstantNGP  to balance between speed and ease of optimization.

Denote the parameters of a differentiable 3D shape representation by \(^{d}\) and differentiable rendering by a function \(g(,c):^{d} C^{N N}\) that returns an image given camera parameters \(c C\). Intuitively, in each iteration, SDS samples \(c\), renders the corresponding (image) view \(g(,c)\), perturbs it with \((0,I)\) to level \(t\), and denoises it with \(^{t}_{}\); the difference between the true and predicted noise is propagated to the parameters of the 3D shape. More formally, after sampling the camera view \(c\) and randomly drawing a time \(t\), SDS renders the volume and adds

Figure 3: The effect of CFG values on 2D generation with StableDiffusion 2.1 . For small values, the model tends to ignore certain words in the prompt. For high values, images become over-saturated.

Gaussian noise \(\) to obtain a noisy image \(x(t)=g(,c)+\). Then, SDS improves the generated volume by using a gradient(-like) direction to update its parameters \(\):

\[_{}_{SDS}=_{t,,c}(t)[ _{}^{t}x(t),y-].\] (4)

We refer to the term \([_{}^{t}x(t),y-]\) as _guidance_ in score distillation, as it 'guides' the views of the shape. In theory, this expression may not correspond to the true gradient of a function and there are many hypotheses about its effectiveness [5; 25; 12; 16; 26]. In this work, we show that instead it can be seen as a high-variance version of DDIM after a change of variables.

## 4 Linking SDS to DDIM

**Discrepancy in image sampling.** Beyond the lack of formal justification of eq. (4), in practice SDS results are over-saturated and miss details for high CFG values, while they are blurry for low CFG values. To illustrate this phenomenon, fig. 1 shows a simple experiment, inspired by : We replace the volumetric representation in eq. (4) with an image \(g_{2D}(_{2D},c):=_{2D}^{N N}\). In this case, SDS becomes an image generation algorithm that can be compared to other sampling algorithms like DDIM . Even in this 2D setting, SDS fails to generate sharp details, while DDIM with the same underlying diffusion model produces photorealistic results, motivating our derivation below.

**Why not use DDIM as guidance?** Given the experiment above, a natural question to ask is if it is possible to directly use DDIM's update direction from eq. (1) as SDS guidance in eq. (4) to update the 3D representation. The problem with this approach lies in the discrepancy between the training data of the denoising model and the images generated by rendering the current 3D representation. More specifically, the denoising network expects an image with a certain level of noise corresponding to time \(t\) as defined by the forward (noising) diffusion process, whereas renderings of 3D representations \(g(,c)\) evolve from a blurry cloud to a well-defined sample (fig. 4 left).

**Evolution of \(x_{0}(t)\).** Instead of seeing DDIM as a denoising process defined on the space of noisy images \(x(t)\), we reparametrize it to a new variable:

\[x_{0}(t)(t)-(t)_{}^{t}x(t),y.\] (5)

In words, \(x_{0}(t)\) is the noisy image at time \(t\) denoised with a single step of noise prediction. Empirically, the evolution of \(x_{0}(t)\) is similar to the evolution of \(g(,c)\)--from blurry to sharp. The left side of fig. 4 compares these processes. This similarity motivates us to rewrite eq. (1) in terms of \(x_{0}(t)\), and to understand SDS as applying similar updates to the renderings of its 3D representation.

**Reparametrizing DDIM.** Figure 4 (right) shows schematically how the DDIM update to \((t)\) alternates between denoising to obtain \(x_{0}(t)\) and adding the predicted noise back to get a cleaner \((t-)\). Based on the intuition above, we reorder the steps, adding noise to \(x_{0}(t)\) and then denoising to estimate \(x_{0}(t-)\). Consider neighboring time points \(t\) and \(t-<t\) in discretized DDIM eq. (2) (lower time means less noise). We rewrite eq. (2) using the definition of \(x_{0}(t)\) from eq. (5) to find

\[x_{0}(t-)=x_{0}(t)-(t-)[_{}^{t-}x (t-),y-_{}^{t}x(t),y],\] (6)

Figure 4: Left: Evolution of variables in Score Distillation with time. The top row depicts how noisy images \(x(t)\) evolve during 2D generation; the middle row shows evolution of a NeRF for 3D generation; and the bottom row shows how the single step denoised variable \(x_{0}(t)\) changes with \(t\). Right: Each step of DDIM steps toward a denoised image. This can be seen as a step to \(x_{0}(t)\) and a step back to a slightly less noisy image. Through a change-of-variables we obtain a process on \(x_{0}(t)\).

which is consistent with the intuition behind SDS: improving an image involves perturbing the current image and then denoising it with a better noise estimate. We cannot directly apply eq. (6) to SDS in 3D, since it depends on \(x(t)\); if we think of \(x_{0}(t)\) as similar to a rendering of the 3D representation for some camera angle, it is unclear how to obtain a consistent set of preimages \(x(t)\) at each step of 3D generation. From eq. (5), however, \(x(t)\) should satisfy the following fixed point equation:

\[x(t) =x_{0}(t)+_{}^{t} x(t),y,\] (7)

or rewritten in terms of noise \(=[x(t)-x_{0}(t)]/\):

\[=_{}^{t}x_{0}(t)+,y.\] (8)

Define \(_{y}^{t}x_{0}(t)=\) as a solution of this equation given \(x_{0}(t)\). Then, we can write:

\[_{}^{t}x(t),y=_{y}^{t}x_{0}(t)  x(t-)=x_{0}(t)+_{y}^{t}x_{0}(t).\] (9)

Thus, eq. (6) turns into:

\[x_{0}(t-)=x_{0}(t)-(t-)^{t- }x_{0}(t)+ _{y}^{t}x_{0}(t)}^{x_{0}}_{y}^{t}t-}}_{}},y-^{t}x_{0}(t) }_{_{y}^{t}}.\] (10)

We can already see that the structure of eq. (10) is very similar to the SDS update rule in eq. (4). Note that the update direction in eq. (10) is the same as in the SDS update rule in eq. (4), where \(_{y}^{t}\) plays the role of the random noise sample \(\). We could use it as a guidance for the 3D generative process in SDS by replacing \(\) in eq. (4) with \(_{y}^{t}(x_{0}(t))\). In practice, however, it is hard to solve eq. (8), as \(_{}^{t}\) is high-dimensional and nonlinear. In an unconstrained 2D generation, \(_{y}^{t}\) can be cached from a previous denoising step, matching the update step to DDIM exactly as in fig. 0(c). In 3D, however, this is impossible due to the simultaneous optimization of multiple views and projections to the space of viable 3D shapes. Below we show that a naive approximation replacing \(_{y}^{t}\) with a Gaussian yields SDS, and we will propose alternatives that are more faithful to the derivation above.

**SDS as a special case.** From eq. (10), to get a cleaner image, we need to bring the current image to time \(t\) with noise sample \(_{y}^{t}\), denoise the obtained image, and then subtract the difference between added and predicted noise from the initial image. A coarse approximation of \(_{y}^{t}\) uses i.i.d. random noise \(_{}(x_{0}(t))(0,I)\), matching the forward process by which diffusion adds noise. This choice of \(_{}\) precisely matches the update rule eq. (10) to the SDS guidance in eq. (4).

**ISM as a special case.** The main update formula in Interval Score Matching (ISM)  is a particular case of eq. (10), where \(_{y}^{t}\) is obtained via DDIM inversion without conditioning on the text prompt \(y\). As demonstrated in section 5, DDIM inversion approximates a solution of eq. (10), explaining the improved performance of ISM. However, our analysis suggests that an even better-performing \(_{y}^{t}\) should incorporate the text prompt \(y\), which further improves the results and avoids over-saturation.

## 5 Score Distillation via Inversion (SDI)

As we have shown, SDS follows the velocity field of reparametrized DDIM in eq. (10), when \(_{}(x_{0}(t))\) is randomly sampled in each step. Our derivation, however, suggests that \(_{}(x_{0}(t))\)

Figure 5: **Overview of SDI.** At each training iteration, SDI renders a random view of the 3D shape, runs DDIM inversion up to the noise level \(t\), and denoises the image with a pre-trained diffusion model for noise level \(t-\). Finally, the denoised image is back-propagated into the 3D shape.

could be improved by bringing it closer to a solution of the fixed-point equation in eq. (8). Indeed, randomly sampling \(_{}\) as in Dreamfusion yields excessive variance and blurry results for standard CFG values, while using higher CFG values leads to over-saturation and lack of detail. On the other hand, solving eq. (8) exactly is challenging due to its high dimensionality and nonlinearity.

Like ISM , we suggest to obtain \(_{y}^{t}\) by inverting DDIM, that is, by integrating the ODE in eq. (1) with \(t\) evolving backwards (from images to noise) as in . As we can see in eq. (8), \(_{y}^{t}\) should be a function of the text prompt \(y\), leading us to perform DDIM inversion _conditionally_ on \(y\), unlike ISM. This process approximates but is not identical to the exact solution for \(_{y}^{t}\): fixed points of eq. (8) invert a single large step of DDIM, while running the ODE in reverse inverts the entire DDIM trajectory. We ablate alternative choices for \(_{y}^{t}\) in section 6.2 and conclude that in practice DDIM inversion offers the best approximation quality. Additionally, to match the iterative nature of DDIM, we employ a linear annealing schedule of \(t\). We refer to our modified version of SDS as _Score Distillation via Inversion_, or _SDI_.

Figure 6 shows the effect of inferring the noise via DDIM inversion instead of sampling it randomly. The special structure of the improved \(_{y}^{t}\) results in more consistent single-step generations and produces intricate features at earlier times. When inverted and not sampled, the noise appears 'in the right place': in SDS the noise covers the whole view, including the background, whereas in ours the noise is concentrated on the meaningful part of the 3D shape. This improves geometric and temporal coherency for \(x_{0}(t)\) predictions even for large \(t\). The reduced variance drastically increases sharpness and level of detail. Moreover, it allows to reduce CFG value of generation \(_{}\) to the standard \(7.5\), avoiding over-saturation. Another interesting finding is that DDIM inversion works best when the reverse integration is performed with negative CFG \(_{}=-_{}=-7.5\). The overview of our method is presented in fig. 5, the details about inversion algorithm are presented in section 6.2, and the implementation details are discussed in appendix A.

## 6 Experiments

### 3D generation

We demonstrate the high-fidelity 3D shapes generated with our algorithm in fig. 2 and provide more examples of \(360^{}\) views in appendix H.2. A more detailed qualitative and quantitative comparison of our method with ISM  is provided in appendix B. Additionally, we report the diversity of the generated shapes in appendix C.

**Qualitative comparisons.** Figure 7 compares 3D generation quality with the results reported in past work using a similar protocol to . For the baselines we chose: Dreamfusion  (the work we build on), Noise Free Score Distillation (NFSD)  (uses negative prompts in SDS), ProlificDreamer  (fine-tunes and trains a neural network to denoise the 3D shape), Interval Score Matching (ISM)  (obtain the noise sample by inverting DDIM and perform multi-step denoising), and HiFA  (guides in both image and latent spaces, regularizes the NeRF, and supervises the geometry with mono-depth estimation). Our figures indicate that Score Distillation via Inversion (SDI) yields similar or better results compared with state-of-the-art. Appendix H.3 presents more extensive comparison.

Figure 6: Comparison of intermediate variables in SDS and SDI (ours) for different timesteps \(t\). Starting with a rendering of a 3D shape we demonstrate how each algorithm perturbs it (\(x(t)\) variable on the top row) and how it is denoised with a single step of diffusion (\(x_{0}(t)\) variable on the bottom row). The prompt used is “Pumpkin head zombie, skinny, highly detailed, photorealistic, side view.”

**Quantitative comparison.** We follow  to quantitatively evaluate generation quality. Table 1 provides CLIP scores  to measure prompt-generation alignment, computed with torchmetrics and the ViT-B/32 model . We also report ImageNetReward (IR)  to imitate possible human preference. We include CLIP Image Quality Assessment (IQA)  to measure quality ("Good photo" vs. "Bad photo"), sharpness ("Sharp photo" vs. "Blurry photo"), and realism ("Real photo" vs. "Abstract photo"). For each method, we test \(43\) prompts with 50 views. For multi-stage baselines, we run only the first stage for fair comparison. We report the percentage of generations that run out-of-memory or generate an empty volume as diverged ("Div." in the table). as well as mean run time and VRAM usage. For VRAM, we average the maximum usage of GPU memory between runs. As many baselines are not open-source, we use their implementations in threestudio. SDI outperforms SDS and matches or outperforms the quality of state-of-the-art methods, offering a simple fix to SDS without additional supervision or multi-stage training.

### Ablations

Dreamfusion + 512 rendering + t annealing + noise inversion (ours) rendering + ours w/o 1

Figure 8: Ablation study of proposed improvements.

Figure 7: Comparison of 3D generation with other methods using their reported results. The prompts are “An ice cream sundae” and “A 3D model of an adorable cottage with a hatched roof”.

   Method & CLIP Score (\(\)) &  & IR (\(\)) & Div. (\%) \(\) & Time & VRAM \\   & & “quality” & “sharpness” & “real” & & & \\  SDS , \(10k\) steps & \(29.81 2.49\) & \(76 6.6\) & \(99 1.2\) & \(98 2.4\) & \(-1.51 0.83\) & \(18.6\) & 66min & 6.2GB \\ SJC , \(10k\) steps & \(30.39 1.98\) & \(76 6.4\) & \(99 0.1\) & \(98 1.1\) & \(-1.76 0.51\) & \(11.6\) & 13min & 13.1GB \\ VSD , \(25k\) steps & \(33.31 2.39\) & \(77 6.7\) & \(98 1.3\) & \(96 4.4\) & \(-1.17 0.58\) & \(23.2\) & 334min & 47.9GB \\ ESD , \(25k\) steps & \(32.79 2.15\) & \(77 7.2\) & \(98 1.2\) & \(97 2.7\) & \(-1.20 0.64\) & \(14.0\) & 331min & 46.8GB \\ HIFA , \(25k\) steps & \(32.80 2.35\) & \(81 6.5\) & \(98 1.5\) & \(97 1.2\) & \(-1.16 0.69\) & \(4.7\) & 235min & 46.4GB \\
**SDI(ours)**, \(10k\)** steps** & \(33.47 2.49\) & \(82 6.3\) & \(98 1.3\) & \(97 1.2\) & \(-1.18 0.59\) & \(4.7\) & 119min & 39.2GB \\   

Table 1: Quantitative comparisons to baselines for text-to-3D generation, evaluated by CLIP Score and CLIP IQA. We report mean and standard deviation across 43 prompts and 50 views for each.

* _Random, resampled:_ Sample \(_{y}^{t}(x)\) in each new update step from \((0,I)\) ;
* _Random, fixed:_ Sample \(_{y}^{t}(x)\) from \((0,I)\) once, and keep it fixed for each iteration;
* _Fixed point iteration:_ Since the optimal solution is a fixed point of eq. (8), initialize \(_{y}^{t}(x)(0,I)\) and run fixed point iteration  for 10 steps (in practice, more steps did not help).
* _SGD optimization:_ Optimize \(_{y}^{t}(x)\) via gradient decent for 10 steps, initializing with noise.
* _DDIM inversion:_ Run DDIM inversion for \(int(10t)\) (fewer steps for smaller \(t\)) steps to time \(t\), with negative CFG \(_{}=-7.5\) for inversion and positive \(_{}=7.5\) for forward inference.

The left side of fig. 9 compares the choices for 3D generation qualitatively; the right side plots error induced in eq. (8) (rescaled to \(x_{0}\) variable due to its ambiguity around 0). As can be seen, the regressed noise has a big impact on the final generations. Both fixed point iteration and optimization via gradient descent fail to improve the approximation of \(_{y}^{t}\), while fixed point iteration diverges in 3D. On the other hand, DDIM inversion yields a reasonable approximation of \(_{y}^{t}\) and significantly improves 3D generation quality. We provide visual comparison of the obtained noisy images for each baseline in appendix D.

CFG for inversion. report that DDIM inversion accumulates big numerical error for CFG \(>0\). Surprisingly, we find that DDIM inversion for CFG \(_{}>0\) can be adequately estimated by running the inversion with negative CFG \(_{}=-_{}\). Figure 10 compares 3D inversion strategies qualitatively and quantitatively. Naively taking \(_{}=_{}=7.5\) yields the biggest numerical error, while other strategies perform on par. 3D generations, illustrated on the right, show that \(_{}=_{}=7.5\) introduces excessive numerical errors, causing generation to drift in a random direction. The best parameters (as we demonstrate in appendix E) for 2D inversion (\(_{}=_{}=0\)) fail to converge in 3D as there is not enough guidance toward a class sample. Introducing guidance only on the forward pass with \(_{}=0,_{}=7.5\) solves the problem, and the algorithm generates the desired 3D shape, but constantly adding CFG on each step over-saturates the image. Note in that configuration the inversion is performed unconditionally from the text prompt, matching ISM . As we can see, prompt conditioning is an important component in eq. (8), and using \(_{}=-7.5,_{}=7.5\) cancels the over-saturation and produces accurate 3D generations.

Number of steps for DDIM inversion.Figure 11 ablates the number of steps required for DDIM inversion. We use \(n=10\) as it provides a good balance between generation quality and speed.

Figure 10: Comparing DDIM inversion strategies. Left: Numerical error in eq. (8) from the inferred noise. Right: Generations for different strategies of using CFG values for denoising and inversion.

Figure 9: The ablation study of different \(_{y}^{t}\) choices in our algorithm. We show the obtained 3D generations on the left (Fixed Point Iteration diverges), and the numerical error in eq. (8) induced for each timestep on the right. The resampled and fixed noise strategies produce the same error.

## 7 Conclusion, Limitations, and Future Work

Helping explain the discrepancy between high-quality image generation with diffusion models and the blurry, over-saturated 3D generation of SDS, our derivation exposes how the strategies are reparameterizations of one another up to a single term. Our proposed algorithm SDI closes the gap between these methods, matching the performance of the two in 2D and significantly improving 3D results. The ablations show that DDIM inversion adequately approximates the correct noise term, and adding it to SDS significantly improves visual quality. The results of SDI match or surpass state-of-the-art 3D generations, all without separate diffusion models or additional generation steps.

Some limitations of our algorithm motivate future work. While we improve the sample quality of each view, 3D consistency between views remains challenging; as a result, despite the convexity loss, our algorithm occasionally produces flat or concave "billboards." A possible resolution might involve supervision with pre-trained depth or normal estimators. A related problem involves content drift from one view to another. Since there was no 3D supervision, there is little to no communication between opposite views, which can lead to inconsistent 3D assets. Stronger view conditioning, multi-view supervision, or video generation models might resolve this problem. Finally, score distillation is capped by the performance of the underlying diffusion model and is hence prone to reproduce similar "hallucinations" (e.g., text and limbs anomalies); the algorithm inherits the biases of the 2D diffusion model and can produce skewed distributions. Appendix H.1 demonstrates typical failure cases.

**Potential broader impacts.** Our work extends existing 2D diffusion models to generation in 3D settings. As such, SDI could marginally improve the ability of bad actors to generate deepfakes or to create 3D assets corresponding to real humans to interact with in virtual environments or games; it also inherits any biases present in the 2D model. While this is a critical problem in industry, our work does not explicitly focus on this use case and, in our view, represents a negligible change in such risks, as highly convincing deepfake tools are already widely available.