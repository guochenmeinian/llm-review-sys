# Learning diverse causally emergent representations from time series data

David McSharry

Department of Computing

Imperial College London

dm2223@ic.ac.uk

&Christos Kaplanis

Google DeepMind

kaplanis@google.com

Fernando E. Rosas

Sussex AI, University of Sussex

f.rosas@sussex.ac.uk

&Pedro A.M. Mediano

Department of Computing

Imperial College London

p.mediano@imperial.ac.uk

###### Abstract

Cognitive processes usually take place at a macroscopic scale in systems characterised by emergent properties that make the whole'more than the sum of its parts.' While recent proposals have provided quantitative, information-theoretic metrics to detect emergence in time series data, it is often highly non-trivial to identify the relevant macroscopic variables a priori. In this paper we leverage recent advances in representation learning and differentiable information estimators to put forward a data-driven method to find variables with emergent properties. The proposed method successfully detects variables that exhibit emergent behaviour and recovers the ground-truth emergence values in a synthetic dataset. Furthermore, we show the method can be extended to learn multiple independent features, extracting a diverse set of emergent quantities. We finally show that a modified method scales to real experimental data from several brain activity datasets, paving the ground for future analyses uncovering the emergent structure of cognitive representations in biological and artificial intelligence systems.

## 1 Introduction

Cognitive processes usually take place in systems made of multiple interacting parts, e.g. neurons composing the nervous system of an organism. Importantly, cognitive processes themselves don't seem to take place at a'microscopic' level of individual units, but at'macroscopic' levels involving assemblies of several coordinated units . Hence, when trying to unveil the inner workings of a -- natural or artificial -- cognitive system, it is crucial to be able to identify relevant macroscopic variables that best characterise the corresponding cognitive processes.

The identification of macroscopic variables has traditionally been driven by intuition and expert knowledge. For example, the investigation of collective behaviour in statistical physics is based on macroscopic variables known as 'order parameters,' which are typically identified heuristically and then used to describe phase transitions and other phenomena of interest . Unfortunately, identifying relevant macroscopic variables is often more an art than a science, being heavily dependent on prior knowledge and expectations. Having automated procedures to identify relevant macroscopic variables of cognitive systems would open important avenues for investigating the inner workings of different cognitive architectures.

A promising approach to identify empirically useful macroscopic variables is provided by unsupervised representation learning [38; 22; 52]. For example, information maximisation has proven to bea powerful objective for learning representations within neural networks [23; 38]. In this paper we combine this approach with recent breakthroughs in our ability to formally characterise emergent phenomena [43; 34], which have proven to be not only theoretically sound but also empirically powerful [30; 40].

Building on this literature, in this paper we leverage recently proposed metrics of emergence to identify representations that display emergent properties. Specifically, we propose an end-to-end differentiable architecture that can learn maximally emergent representations of multivariate time series data. Our results show that causal emergence speeds up learning of more complex features of the data relative to pure mutual information maximisation, and we demonstrate the scalability of our method through an analysis of real-world brain activity data.

## 2 Methods

### Quantifying emergence

Consider a scientist measuring a system of interest composed of \(n\) parts, and let \(X^{i}_{t}\) denote the state of part \(i\) at time \(t\). The information that the joint process carries together from \(t\) to \(t^{}\) can be quantified by the standard Shannon mutual information \(I(_{t};_{t^{}})\), where \(_{t}=(X^{1}_{t},...,X^{n}_{t})\) is the joint state of the system at time \(t\).

How can one characterise an emergent macroscopic variable of such system? Following Ref. , one can define emergent variables \(V_{t}\) as satisfying two key criteria:

1. **Supervenience**: there exists a function (or _coarse-graining_) \(f\) such that \(V_{t}=f(_{t})\).
2. **Unique information**: \(V_{t}\) holds unique predictive information about the future evolution of the system \(_{t^{}}\) that cannot be found in the individual parts \(X^{1}_{t},,X^{n}_{t}\) by themselves.

Critically, the unique information that \(V_{t}\) holds about \(_{t^{}}\) can be rigorously quantified using the framework of _Partial Information Decomposition_ (PID, ), and its recent extension to time series data (\(\)ID, ). Emergence, therefore, is defined as the capability of a supervenient variable to provide predictive power that cannot be reduced to underlying microscale phenomena.

Quantifying unique information in high-dimensional systems can be highly non-trivial. Luckily, the \(\)ID formalism allows us to derive simpler measures that provide sufficient criteria for emergence. In particular, it has been shown that the following is a sufficient condition for causal emergence :

\[:=I(V_{t};V_{t+1})-_{i}I(X^{i}_{t};V_{t+1})>0\;.\] (1)

Importantly, \(\) is comparatively easy to calculate, as it relies only on pairwise marginal distributions and on standard Shannon mutual information. These key features allow the framework to be applicable on a wide range of scenarios, as illustrated by the applications reviewed in Ref. . Note that here we take \(t^{}=t+1\), but in principle any \(t^{}>t\) is valid.

The reason why \(>0\) is only a sufficient, but not necessary, condition for emergence is that in some systems multiple \(X^{i}_{t}\) can have the same information about \(V_{t+1}\), and hence the sum of terms \(I(X^{i}_{t};V_{t+1})\) may 'double-count' information -- resulting in a negative bias in \(\). This double-counting can be alleviated by discounting from Eq. (1) the redundant information between the \(X^{i}_{t}\)'s about \(V_{t+1}\). Here we do this using a measure of redundancy known as 'Minimum Mutual Information' , which yields the following 'adjusted' emergence criterion (Supp. Sec. C):

\[_{A}:=I(V_{t};V_{t+1})-_{i}I(X^{i}_{t};V_{t+1})+(n-1)_{i}I(X^{i}_ {t};V_{t+1})>0\;.\] (2)

### Model architecture and information estimators

Maximising emergence.Our aim is to establish an automated procedure to identify emergent macroscopic variables \(V_{t}\) with respect to a microscopic substrate \(_{t}\). For this, we investigate parametric coarse-grainings \(V_{t}=f_{}(_{t})\) that can be optimised to maximise \(\) via a differentiable objective function.

A key ingredient to maximising \(\) is employing a suitable estimator of Shannon's mutual information. Although many estimators of mutual information exist [27; 29], most are not differentiable, and thus not suitable for optimisation with standard representation learning architectures. Fortunately, a number of differentiable estimators of mutual information exist [39; 50].

We use the Smoothed Mutual Information "Lower-bound" Estimator (SMILE, ), which is one of a family of approaches that formulates mutual information estimation as a variational problem, and was specifically designed to address the issue of high variance in existing estimators such as NWJ  and MINE . The SMILE mutual information estimator is given by

\[I(X;Y) =_{p(x,y)}[]\] (3) \[_{p(x,y)}[g_{}(x,y)]-_{p(x)p(y)}[(e^{g_{}(x,y)},e^{-},e^{} )] I_{}^{}(X;Y),\] (4)

where \(g_{}\) is a parameterised function that estimates the log density ratio \((p(x,y)/(p(x)p(y)))\), \((v,l,u)=((v,u),l)\) and \( 0\) is a hyperparameter. As \(\), \(I^{}\) converges to the MINE estimation , but a finite \(\) prevents the potentially exponential growth of the variance of the estimate with MI, which MINE suffers from .

Equipped with this estimator, we can now formulate our representation learning algorithm for causally emergent features. The architecture is schematically shown in Fig. 1. Our main method involves three learnable functions:

1. A representation network \(f_{}\), that learns a supervenient variable \(V_{t}=f_{}(_{t})\).
2. A critic for the macroscopic variable \(g_{}\), that controls the estimation of \(I(V_{t};V_{t+1})\).
3. A set of critics for the microscopic variable \(h_{^{i}}\), that control the estimation of \(I(X_{t}^{i};V_{t+1})\). The number of critics equals the number of constituent atoms of the system.1 
We will demonstrate that the representation critic can learn emergent features on an algorithmic dataset by maximising the following metric:

\[^{}(,,\{^{i}\}):=I_{}^{}(f_{ }(_{t});f_{}(_{t+1}))-_{i}I_{^{i}}^{}(X_{t}^{ i};f_{}(_{t+1}))\;.\] (5)

Figure 1: **Architecture for calculating loss terms for learning causally emergent representations**. A representation network \(f_{}\) applied to the data \(_{t}\) learns a feature \(V_{t}\). This feature is trained to optimise \(\) made up of predictive and marginal mutual information terms estimated by \(g_{}\) and \(h_{^{i}}\) respectively (left). A further critic \(k_{}\) may be added to calculate the mutual information with another emergent feature \(V_{t}^{A}\), to encourage the learning of a diverse set of emergent features (right).

We refer to \(^{}\) as the _emergence objective function_, and the first term in the RHS of Eq. (5) as the predictive mutual information  - since (by the data processing inequality ) it represents a lower bound on the joint mutual information between the past and future states of the whole system, \(I(_{t};_{t^{}})\). As a control condition, we also ran experiments with an objective function consisting of only the predictive information, removing the marginal mutual information terms. After training has converged, we ran 'post-hoc' tests to further verify that the learned feature is emergent by freezing the representation network \(f_{}\), retraining the critics to accurately estimate \(I(V_{t};V_{t+1})\) and \(I(X_{t}^{i};V_{t+1})\), and calculating \(_{A}\). If this resulting \(_{A}\) is positive, we conclude the feature is emergent. For convenience, pseudocode for the algorithm used to train the model is provided in Supp. Sec. A.

Learning diverse emergent features.A well-known fact about complex systems is that they can have more than one emergent property, and/or be described by multiple order parameters .

To learn a diverse set of emergent features, we consider a scenario where an emergent feature \(V_{t}^{A}\) has been learned and its corresponding parameters \(^{A}\) are fixed. Intuitively, our goal is to find a new feature that is substantially different from the existing one by incentivising it to be statistically independent from \(V_{t}^{A}\). To implement this, we add a penalty term \(I_{}^{}(f_{^{A}}(_{t});f_{}(_{t}))\) to the objective function and include a fourth learnable function to our method: a critic \(k_{}\) that estimates this mutual information, also using SMILE.

Improving training stability.An important point to note of the proposed method is that there is an adversarial relationship between the microscopic critics \(h_{^{i}}\) and the macroscopic critic \(g_{}\) through the representation network \(f_{}\), reminiscent of the adversarial learning dynamics in GANs . In essence, the former pushes the representation network \(f_{}\) to maximize information about the system's state, while the latter pushes it to minimize it. This could lead to potentially unstable learning dynamics and various failure modes - for example, a representation network could "trick" the microscopic critics into reducing their MI estimate by exploiting their mistakes, rather than genuinely reducing \(I(X_{t}^{i};V_{t+1})\), resulting in an artificially inflated value of \(\).

Fortunately, unlike in GANs, it is not harmful for the critics to be overpowered compared to the representation learner2 because the SMILE estimation is bounded from above by the true MI . Following this reasoning, we _i_) update the critics multiple times for each representation learner update, using a higher learning rate and more parameters; and _ii_) pre-train the critics before we start training the representation learner, so that they provide a more robust MI estimation and better training signal. This results in slower, but more stable, learning dynamics for \(f_{}\).

Another method to increase training stability is to use a dedicated choice of the architecture of the representation network. \(f_{}\) first linearly projects the input to a higher-dimensional space, then passes it through an MLP with residual ('skip') connections, and then projects it linearly down to the output vector \(V_{t}\). Because of the skip connections (and because of the auto-correlation of \(_{t}\)), at initialisation the output already contains useful information about the system and yields a high value of the macroscopic information \(I_{}^{}(f_{}(_{t});f_{}(_{t+1}))\) before \(\) has been trained. Therefore, this allows us to reduce the contribution of this term in the objective function, and thus reduce the influence of \(g_{}\) on \(f_{}\).3

### Datasets

#### 2.3.1 Synthetic datasets

Bit-string dataset.We evaluate our method for learning causally emergent representations by applying it to sequences of random bit-strings of length \(n\) with two constructed temporal correlations:

1. The _parity_ of the first \(n-1\) bits is auto-correlated across time, such that \[_{i=1}^{n-1}X_{t+1}^{i}=_{i=1}^{n-1}X_{t}^{i} }=_{}>\;,\] where \(\) represents modulo-2 addition.

2. The last (or _extra_) bit in the bit-string \(X^{n}\) is auto-correlated across time, such that \[X^{n}_{t+1}=X^{n}_{t}}=_{}>\;.\]

Since parity is a synergistic function of the bits of a bit-string (i.e. it cannot be predicted from any of the input bits individually ), and since the parity predicts some information about the future evolution of the system, \(V_{t}=_{i=1}^{n-1}X_{t}\) is an emergent feature of the system.

Despite its simplicity, this dataset has two important advantages: there is a known emergent feature (the parity), and one can calculate the mutual information and the emergence measure analytically.4 These properties will allow us to verify that the model has successfully extracted the expected emergent properties and that mutual information is being accurately estimated.

Conway's Game of Life.To further evaluate our method on a more complex system, we apply it to simulations from Conway's Game of Life (GOL) - a canonical setting for the study of emergence . GOL is a cellular automaton on an \(N N\) grid where each cell's state evolves based on simple, deterministic rules. We initialize a "glider" pattern  at a random position within the grid, and this glider moves diagonally by cycling through four distinct states until it reaches a boundary and the simulation concludes. The dataset consists of multiple such simulations concatenated to form a continuous sequence. To effectively capture the spatial dependencies inherent in the Game of Life, we replace the multilayer perceptron used in previous experiments with a convolutional neural network for the representation learner \(f_{}\), allowing us to exploit the 2D structure of the grid-based simulations.

#### 2.3.2 Real world brain datasets

Primate ECoG dataset.We evaluate our method on a dataset of electrocorticography (ECoG) brain activity data from a macaque monkey, originally reported by Chao _et al._. The dataset contains long time series of electrical activity measured with 64 electrodes placed across the monkey's brain surface. We minimally pre-process the data by applying a second-order Butterworth high-pass filter with a \(1\,\) cutoff, downsampling the signals to \(300\,\), and finally standardising the data before applying our method to learn emergent features from it.

Human MEG dataset.We evaluate our method on a magnetoencephalography (MEG) brain activity dataset from healthy human participants, originally reported in four pharmaco-MEG studies . The dataset includes resting-state recordings collected before the administration of various pharmacological agents. MEG signals were sampled at \(600\,\) with a \(0\)-\(300\,\) bandpass filter. For our analysis, we focus on the placebo condition (pre-drug administration) and train our model \(f_{}\) on data from multiple subjects, using each epoch to represent a different participant to capture features common across individuals. Data was pre-processed following standard procedure  and standardised before applying our method to learn emergent features.

Human fMRI dataset.We also evaluate our method on a functional magnetic resonance imaging (fMRI) dataset from 100 unrelated human participants from the "minimally preprocessed" release by the Human Connectome Project [53; 20]. We performed additional preprocessing following Luppi _et al._ and calculated 100 time series capturing the activity of each of the regions in the Schaefer brain atlas . We further standardise the data before applying our method to learn emergent features from it.

## 3 Results

In this section, we discuss results on learning emergent features on the synthetic and real world datasets introduced above, as well as comparisons to baselines and some ablation studies. All experiments can be run on a single A10G GPU in less than two hours.

### Learning emergent features in synthetic datasets

Bit-string dataset.Results show that our proposed architecture can accurately estimate the ground-truth value of \(\) in the synthetic dataset, confirming it is able to learn causally emergent representations (Fig. 2). To interpret the contents of the learned representation, we trained decoders with standard supervised learning to predict both the parity of the first \(n-1\) bits (the parity bit) and the last auto-correlated bit (the extra bit). We found that the parity bit could be decoded with high accuracy but the extra bit could not, confirming that the learned representation indeed corresponded to an emergent feature. For additional verification, we fixed the representation learner's weights \(\) and re-trained both critics to estimate both \(\) and \(_{A}\), which were both positive and confirmed our results.

As expected, when the marginal MI terms are removed from the objective function (Fig. 2, right column), the model is no longer able to obtain the correct \(\) value - and, interestingly, only the extra bit (but not the parity bit) is encoded in the representation. We hypothesise that, in the absence of the regularisation induced by the marginal MI, the system's inductive biases lead it towards learning "low-order" (i.e. non-emergent) representations. Note that, despite having a constraint removed, the model without marginal MI loss is unable to extract the full predictive information of the system (which equals approximately \(1.84\,\)), showing that using the full emergence loss could incentivise the system to learn features that provide information about the system's dynamics that would otherwise be ignored (we elaborate on this further in Section 3.3). We obtain qualitatively similar results with a noisier version of the same data generating process (Supp. Fig. 5).

Conway's Game of Life.By employing a convolutional neural network for the representation learner \(f_{}\), we successfully learned emergent features from this dataset, evidenced by a positive \(_{A}\) value after freezing the representation network and retraining the critics (Supp. Fig. (a)a). To interpret the emergent feature, we attempted to predict the glider's position and state from the representation. While the position prediction performed at chance level, the state prediction achieved an accuracy of 53%, significantly higher than the random baseline of 25% (Supp. Fig. (b)b). This suggests that the emergent feature encodes information about the glider's state. When \(f_{}\) is trained with a prediction-only objective, the state prediction achieved an accuracy of 35%, suggesting the learned feature encodes less information about the state of the glider when trained on this objective.

Figure 2: **The proposed architecture recovers ground truth emergent features**. Using the emergence objective function (left column), the model finds the correct \(\) value and is able to recover the known emergent feature (parity bit). Using only predictive MI as the objective (right column), the model fails to discover any emergent features.

These findings demonstrate that our approach can learn meaningful emergent features in complex, high-dimensional systems like the Game of Life, capturing collective behaviors that are not localized to individual components.

### Learning diverse features in the bit-string dataset

As our next step, we set out to test our algorithm to learn diverse features in the same bit-string dataset. For this set of experiments we used the representation learner with skip connections described in Section 2.2, ran our algorithm as above until convergence, and fixed the representation learner's weights (denoted by \(^{A}\)). We then trained a second representation learner with the regular \(^{}\) objective plus a penalty term \(I(V_{t};V_{t}^{A})\) to obtain a new feature that is statistically independent from the parity bit.

Interestingly, this process revealed a new, unexpected feature that was not originally designed (Fig. 3): the XOR of the parity and extra bit (which we will refer to simply as the _bonus_ bit). This bit is emergent (since it cannot be predicted by any of the \(X_{t}^{i}\) and is auto-correlated, since both the parity and extra bits are), albeit with a lower \(\) than the parity bit - which fits well with the result that our model did not learn this feature spontaneously. This shows the capability of our method to discover new aspects of a system under study, even in the case of simple and explicitly constructed systems.

### Comparison to baselines and ablation studies

Comparison with RNNs.In order to investigate whether the failure of the prediction-only baseline to learn emergent features was to do with the capacity of the architecture, we trained a standard RNN on the bit-string dataset using mean squared error (MSE) loss, with its hidden state serving as the representation for decoding. The hidden state dimension matched that of our emergent feature network \(f_{}\) for a fair comparison. As shown in Fig. 4, the RNN consistently learned the non-emergent extra bit but encoded negligible information about the two emergent bits, resulting in \(_{A}=0\). This indicates that the RNN did not capture any emergent features.

Figure 3: **Our method learns a diverse set of multiple emergent features from the same system**. Training two representation learners to learn independent emergent feature on the synthetic dataset. Both learned features were emergent (top row). The first learner (left column) yielded a feature that has high mutual information with the system’s parity bit (bottom left), while for the second learner (right column) it had high mutual information with the bonus bit (bottom right).

Combining emergent and non-emergent features.The fact that standard architectures learn representations that are predictive, yet not emergent, suggests they may be learning different aspects of the data compared to our method. To test this hypothesis, we investigated whether combining emergent features from our method with representations from standard models would enhance the predictive performance of either. As a proof of concept, we trained both an RNN and our emergence learner \(f_{}\) on the bit-string dataset, and compared the mutual information between \(X_{t^{}}\) and:

1. The hidden state of the RNN.
2. The emergent feature \(V_{t}\) learned by \(f_{}\).
3. The concatenation of \(V_{t}\) with the hidden state of an RNN.5 
As shown in Fig. 4, the combined representation encoded more information about the future state of the system, \(X_{t^{}}\), than either the RNN or the emergence learner alone. While this result is only shown on a very simple synthetic dataset, it suggests that our emergence learner extracts meaningful features that are different from, and can be effectively combined with, other representation learning architectures. We hypothesise that standard representation learning techniques can more effectively capture microscale properties, and since our method captures macroscale properties, both can be naturally combined to enhance performance in downstream tasks.

Ablations to emergence loss.The main emergence loss function we propose in this work has three terms: a predictive MI term estimating \(I(V_{t};V_{t^{}})\), marginal MI terms estimating \(I(X_{t}^{i};V_{t^{}})\), and a diversity term estimating \(I(V_{t}^{A};V_{t})\). In order to understand the contribution of each term to the behaviour of the model, we performed ablation studies removing each one of these. We describe each ablation in turn:

1. As seen in Sec. 3.2 (and later in Sec. 3.4), ablating the \(I(V_{t};V_{t^{}})\) term from \(\) objective results in the network learning features that are not as correlated in time as they would otherwise be, but that still satisfy the criteria for emergence. Therefore, our proposed architecture with skip connections may learn slightly less emergent features, but with the advantage of more stable training dynamics.
2. Ablating the \(I(X_{t}^{i};V_{t^{}})\) terms objective results in an algorithm that only maximises predictive information \(I(V_{t};V_{t^{}})\), akin to the Deep InfoMax method . As seen in Sec. 3.1, and as expected, training without the marginal terms results in representations that are not emergent.

Figure 4: **Standard methods do not learn emergent features, and their performance increases when combined with emergent features.**. The hidden state of an RNN trained on the bit-string dataset has negligible \(_{A}\), indicating no emergent feature learned (left). Accordingly, the mutual information between the hidden state and the extra, parity, and bonus bits shows that only the non-emergent extra bit is encoded (middle). Interestingly, representations learned by an RNN and by our method can be combined to yield better predictions of the future state of the system (right).

3. Ablating the diversity term in the experiments in Sec. 3.2 and training multiple runs on the bit-string dataset results in the representation network learning the extra bit \(91\%\) of runs, and the bonus bit \(9\%\) of runs. This implies there are strong inductive biases towards which emergent features are learned, and it is thanks to the diversity term that we can reliably recover both.

### Learning emergent features in brain activity data

Finally, we demonstrate the scalability of our method by learning emergent features in three types of high-dimensional brain activity datasets - ECoG, MEG, and fMRI - to evaluate its ability to learn emergent features across different neural recording modalities and spatial scales.

ECoG data.For the ECoG dataset , it was crucial to use skip connections in the representation learner, since the method as used in Section 3.1 was consistently unable to find emergent features (despite their known presence in this dataset ).

With this configuration, our method is able to successfully learn emergent representations of the ECoG data (Supp. Fig. (a)a). The resulting feature was verified to be emergent by a positive post-hoc \(_{A}\) test. Interestingly, using an \(f_{}\) with skip connections we obtained emergent features even when the macroscopic MI term \(I(V_{t};V_{t+1})\) was completely removed from the objective function, which further simplified learning dynamics. Empirically, we found that this does not cause \(I^{}_{}(f_{}(_{t});f_{}(_{t+1}))\) to decrease over training substantially, suggesting that using just the marginal MI terms is a good objective function to remove information about the parts while preserving information about the whole that was present at initialisation.

As a final experiment, we also found that our method could learn a second emergent feature, also verified with a post-hoc \(_{A}\) check (Supp. Fig. 10).

MEG data.We extended our analysis to magnetoencephalography (MEG) recordings from healthy human participants . Training on data from multiple subjects, our method readily learned emergent features, as evidenced by a positive post-hoc \(_{A}\) check (Supp. Fig. (b)b). The consistency of emergent features across different individuals suggests that our model captures fundamental aspects of neural dynamics common to human brain activity.

Notably, emergent features were learned more easily from MEG data compared to ECoG data, potentially due to differences in spatial resolution and the nature of the recorded signals.

fMRI data.Finally, we applied our method to functional magnetic resonance imaging (fMRI) data from 100 unrelated human participants . Despite the coarser spatial and temporal resolution of fMRI, our method was able to learn emergent features, as indicated by a positive \(_{A}\) value (Supp. Fig. (c)c). This demonstrates that emergent dynamics are present even at the macroscopic scale of brain activity. On this dataset, we also determined that a standard MLP did not learn emergent features in its representation (see Supp. Sec. B.2 for more details).

The successful extraction of emergent features across all three datasets highlights the robustness of our method. It underscores its capability to uncover collective neural dynamics in complex, high-dimensional brain activity data, regardless of the recording modality or spatial scale.

    & **ECoG** & **MEG** & **fMRI** \\  Training \(\) & 0.8 \(\) 0.12 & 3.0 \(\) 0.30 & 2.7 \(\) 1.3 \\ Post-hoc \(_{A}\) & 1.2 \(\) 0.19 & 2.6 \(\) 0.52 & 3.3 \(\) 0.44 \\   

Table 1: **Our method learns emergent features in multiscale datasets of brain activity**. Final values of \(\) and \(_{A}\) as found during the training run of \(f_{}\) and post-hoc evaluation respectively. All values are greater than zero, indicating that in each case an emergent feature has been found.

Related work and future directions

The method we have proposed here is part of a growing literature leveraging methods from information theory to enhance deep learning architectures, and representation learning algorithms in particular. A small, far from exhaustive list of some noteworthy examples includes InfoNCE , deep variational information bottleneck , \(\)-VAE , or TC-VAE . In future work, it would be interesting to see if our method is competitive or can be combined with recent representation learning methods on standard benchmarks .

In practice, one of the challenging aspects of this method is to overcome the instability of training to make sure a valid emergent solution is found (i.e. one where the post-hoc check still shows \(_{A}>0\)). One potential approach is to incorporate stabilisation techniques from the GAN literature (such as e.g. spectral normalisation ). Another potential approach is to use a mutual information upper bound (instead of our current lower bound) for the marginal MI terms. Although mutual information upper bounds are not as common as lower bounds, there are a few options available . In practice, we found both of these to be less effective than our skip connection method, but they remain a promising avenue for future work.

There is a growing body of machine learning research that engages directly with PID, the information-theoretic backbone of our emergence theory. For example, recent work has proposed a differentiable redundancy measure , which has been used as an objective function to train deep neural networks . Alternatively, there are also methods that estimate redundancy using deep neural networks , analogous to SMILE. Although none of these estimators can be directly applied to estimate the unique information that underlies the definition of emergence (Section 2.1), we expect that extensions of these exciting developments will also open new possibilities in the study of emergence.

Finally, it is worth mentioning that there are a number of other approaches that focus on different aspects of emergence (see Supp. Sec. D). The approach presented here was chosen for two reasons: (i) its intuitive nature, focusing on how emergent properties arise from collective interactions that cannot be fully explained by examining components in isolation; and (ii) the existence of efficient, scalable, and differentiable proxies for its estimation. Constructing similar methods as the one proposed here for other metrics of emergence would be a challenging but extremely interesting line of future work.

## 5 Conclusion

Emergence, the phenomenon whereby a system becomes'more than the sum of its parts', is an promising conceptual tool to investigate cognitive processes in artificial and biological systems. In this paper, we proposed a machine learning method for discovering emergent variables in time series data that leverages a recent information-theoretic characterisation of emergence  and advances in mutual information estimation from data with neural networks .

We first showed in a synthetic dataset that our method can estimate emergence and successfully discover a known emergent feature. Interestingly, a pure information maximisation objective struggled to learn this feature, suggesting that our method facilitates the identification of complex features of the data. Furthermore, we also proposed a slight modification of our method that can learn a diverse set of features from the same system. Finally, we also showed that our method can scale up to learn emergent features in real-world brain activity data.

Overall, our method opens up a range of possibilities for the practical study of emergence, as well as for other machine learning problems more broadly. For example, our method may be explored in conjunction with other representation learning algorithms to capture aspects of complex systems that are otherwise difficult to learn. From an application perspective, we hope this method can be further leveraged in neuroscience to reveal new aspects of brain function.

### Software availability

Code implementing our proposed architecture and reproducing our key results is available at https://github.com/Imperial-MIND-lab/causally-emergent-representations