# Make Your LLM Fully Utilize the Context

Shengnan An\({}^{,}\), Zexiong Ma\({}^{,}\), Zeqi Lin\({}^{,}\),

**Nanning Zheng\({}^{}\)\({}^{}\), Jian-Guang Lou\({}^{}\), Weizhu Chen\({}^{}\)**

\({}^{}\)National Key Laboratory of Human-Machine Hybrid Augmented Intelligence,

National Engineering Research Center of Visual Information and Applications,

Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University

\({}^{}\)Microsoft \({}^{}\)Peking University

\({}^{}\){an1006634493@stu,nnzheng@mail}.xjtu.edu.cn,

\({}^{}\)maexiong@stu.pku.edu.cn, \({}^{}\){Zeqi.Lin,jlou,wzchen}@microsoft.com

Work done during the internship at Microsoft.Corresponding authors.

###### Abstract

While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the _lost-in-the-middle_ challenge. We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information. Based on this intuition, our study presents **information-intensive (In2) training**, a purely data-driven solution to overcome lost-in-the-middle. Specifically, In2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) **fine-grained information awareness** on a short segment (\(\)128 tokens) within a synthesized long context (4K\(-\)32K tokens), and (2) the **integration and reasoning** of information from two or more short segments. Through applying this information-intensive training on Mistral-7B, we present **FilM-7B** (**FILI-**in-the-Middle). To thoroughly assess the ability of **FilM-7B** for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results demonstrate that **FilM-7B** can robustly retrieve information from different positions in its 32K context window. Beyond these probing tasks, **FilM-7B** significantly improves the performance on real-world long-context tasks (e.g., 23.5\(\)26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3\(\)59.2 accuracy on MMLU).

## 1 Introduction

_To a great mind, nothing is little._

_--Arthur Conan Doyle_

Long-context large language models (LLMs) have recently received significant attention within the open-source community (Jiang et al., 2023; Du et al., 2022; Li et al., 2023; Shi et al., 2023; Team et al., 2023; Team, 2023; Chen et al., 2023; Song et al., 2023; Liu et al., 2023; Peng et al., 2023; Chen et al., 2023; Xiong et al., 2023; Tworkowski et al., 2024; AI et al., 2024; Ding et al., 2024; Mohtashami & Jaggi, 2024; Fu et al., 2024; Cai et al., 2024; Bai et al., 2024; Lv et al., 2024). The training context windows of many contemporary LLMs have been expanded to tens of thousands oftokens, thereby enabling these models to process extensive context as input. This extended training context window can enhance many real-world downstream tasks such as long-context question answering (Kocisky et al., 2018; Dasigi et al., 2021; Bai et al., 2023) and summarization (Fabbri et al., 2019; Huang et al., 2021; Zhong et al., 2021).

However, recent studies have revealed that these long-context LLMs struggle to effectively and robustly utilize all the information provided in the context, known as the _lost-in-the-middle_ challenge (Liu et al., 2024; Xu et al., 2023). It implies that while the LLM can comprehend the information at the beginning and end of the long context, it often overlooks the information in the middle. This challenge could significantly hinder the development of long-context LLMs, as they even often fail to pass simple probing tasks such as Needle-in-the-Haystack and passkey retrieval (Mothashami and Jaggi, 2024). Consequently, a pressing research question arises: _how can we make long-context LLMs fully utilize the information in the long context?_

We hypothesize that the root cause of lost-in-the-middle stems from the unintentional bias hidden in the general training data. In auto-regressive pre-training, the loss on predicting the next token is more likely to be influenced by a few nearby pre-tokens rather than long-distance tokens (Sharan et al., 2018; Sun et al., 2021). For supervised fine-tuning and alignment, the system message, which strongly influences the generation of the response, is typically presented at the beginning of the context (Touvron et al., 2023; Cai et al., 2024). As a result, the general training process may inadvertently introduce a position bias, suggesting that important information is always located at the beginning and end of the context.

Based on this hypothesis, our work introduces **information-intensive (N2) training** to explicitly teach the model that **the crucial information can be intensively present throughout the context**, not just at the beginning and end. In2 training is a purely data-driven solution that utilizes a synthesized long-context question-answer dataset. The long context (ranging from 4K to 32K tokens) is concatenated from many short segments (\(\)128 tokens), and the question-answer (QA) pairs ask for the information contained in one or more segments which are _randomly_ placed in the long context. Specifically, we generate two types of questions, requiring (1) **fine-grained information awareness** on exactly one short segment, and (2) the **integration and reasoning of information** from two or more segments. These QA pairs are generated by prompting GPT-4-Turbo (OpenAI, 2023b) with the designed instructions and the raw segments.

By applying this information-intensive training on Mistral-7B (Jiang et al., 2023), we present **FiLM-7B** (**FILI**-in-the-**M**iddle). To thoroughly assess the long-context information awareness of FilM-7B, we design three probing tasks encompassing various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval). The probing results (Figure 1) demonstrate that In2 training significantly overcomes the lost-in-the-middle problem for the backbone model. Moreover, it can enhance the open-source model to achieve comparable or even more robust performance compared with proprietary LLMs such as GPT-4-Turbo.

Beyond these probing tasks, the performance of FilM-7B on real-world long-context tasks also exhibits significant improvements (e.g., 23.5\(\)26.9 F1 score on NarrativeQA (Kocisky et al., 2018)). This demonstrates that the post-training on synthesized long-context data can be generalized to

Figure 1: Performance of FilM-7B, Mistral-7B-Instruct-v0.2, and GPT4-Turbo on our three probing tasks. FilM-7B significantly overcomes the problem of information loss in the middle of the context.

real-world scenarios. Moreover, FilM-7B maintains a comparable performance on short-context tasks compared with the vanilla backbone model (e.g., 59.3\(\)59.2 accuracy on MMLU (Hendrycks et al., 2020)). This indicates that the short-context capability of FilM-7B is not compromised during training. Our further analysis explores how the sliding window strategy and the choice of RoPE base \(\) influence the performance of In2 training.

## 2 Information-Intensive Training

This section introduces the construction of the dataset for In2 training and the detailed training process of our model FilM-7B.

### Training Data Construction

Overview.The In2 training aims to explicitly teach the model that any position in a long context can contain crucial information. To achieve this goal, we construct a long-context question-answer training dataset \(=\{_{i},q_{i},a_{i}\}\), where the answer \(a_{i}\) to the question \(q_{i}\) requires the information contained in some short segments that are randomly placed in the whole long context \(_{i}\).

Figure 2 illustrates an overview of the data construction process. Specifically, the training data \(\) is constructed based on a general natural language corpus \(\). Given a raw text \(_{i}\), we first generate a question-answer pair \((q_{i},a_{i})\) using a powerful LLM, then synthesize a long context \(_{i}\) that includes the necessary information from \(_{i}\) and other randomly sampled texts from \(\). We generate two types of question-answer pairs that require (1) the awareness of fine-grained information in the long context, and (2) the integration and reasoning of information appearing at different positions in the long context. We take the realnewslike subset from the C4 corpus (Raffel et al., 2020) as \(\), and take GPT-4-Turbo (OpenAI, 2023b) as the LLM to generate QA pairs.

Fine-grained information awareness.We consider a 128-token segment as the minimum information unit of the context3. Given a raw text \(_{i}\), we first randomly extract a 128-token segment \(s_{i}\) from it, then generate the \(q_{i}\), \(a_{i}\) and \(_{i}\) accordingly,

\[(q_{i},a_{i})(s_{i},I_{f};),_{i}=\{(s_{i},[r_{j}])\},\] (1)

Figure 2: The data construction process for In2 training, aimed at enhancing the fine-grained information awareness (upper), and the integration and reasoning of information (lower).

where \((q_{i},a_{i})\) is sampled by prompting the powerful LLM with the segment \(s_{i}\) and the instruction \(I_{f}\), \(\{\}\) represents the concatenation of the contained segments, and \([r_{j}]\) are randomly sampled from 128-token segments in \(\). Note that \(I_{f}\) instructs the LLM to make the question-answer pair highly specific to the information provided in \(s_{i}\).

Integration and reasoning of information.Beyond utilizing each single segment, we consider to generate question-answer pairs for information contained in two or more segments. Following the setting of the minimum information unit above, we split a full text \(_{i}\) into a set of 128-token segments \([s_{i}]\), then generate the \(q_{i}\), \(a_{i}\) and \(_{i}\) accordingly,

\[(q_{i},a_{i})([s_{i}],I_{r};),_{i}=\{([s_{i}],[r_{j}])\},\] (2)

where \(I_{r}\) instructs the LLM to generate a multi-hop question-answer pair that requires the information within at least two segments in \([s_{i}]\). All segments in \([s_{i}]\) and \([r_{j}]\) are jointly shuffled, so the required segments may appear far apart in the context.

Context length balance and data mixture.To prevent length bias during In2 training, we ensure the length of the long context \(_{i}\) is evenly distributed from 4K to 32K tokens. Such a length balance strategy can be implemented with restricted sampling on \([r_{j}]\), according to Equation 1 and 2. To alleviate catastrophic forgetting on short-context capabilities, we retain \(\)10% question-answer pairs with the original texts \(_{i}\) instead of converting them into a longer context, and add some general instruction-tuning data from the OpenOrca (Lian et al., 2023) dataset.

Overall, our dataset for In2 training contains 1.1M long-context data for the fine-grained information awareness (\(\)63%), 300K long-context data for the integration and reasoning of information (\(\)17%), 150K short-context question-answer data (\(\)9%), and 200K general instruction-tuning data (\(\)11%). Appendix I contains the handcraft instructions for data generation. Appendix H illustrates some examples of our constructed long-context QA data. Appendix A describes the filtering strategy to avoid data contamination for evaluation.

### Training Details

Using the training data constructed above, we further fine-tune the Mistral-7B-Instruct-v0.24(Jiang et al., 2023) to get our FiLM-7B (**FILI**-in-the-**M**iddle). We perform In2 training in the instruction-tuning paradigm: the long contexts and questions are used as instructions, and the loss on the answer parts are used to update the model. Appendix I contains the system template used for formatting the training data. For hyper-parameters, we set the global batch size as 128 and conduct one-epoch training with \(\)14K training steps. We use the cosine learning rate decay with a 1e-6 maximum

Figure 3: Three tasks in VAL Probing. The retrieval patterns are determined by the relative positions between the retrieval keywords and the information to be retrieved.

learning rate and 3% warm-up steps. The training process is conducted on 16 nodes of 8x80G A100 GPUs with the full sharding strategy and cpu offload strategy implemented by pytorch FSDP (Zhao et al., 2023). One entire training process (for a single FilM-7B model) consumes \(\)300 GPU days.

## 3 Long-Context Probing

In this section, we first show the preliminary evaluation of FilM-7B on the Needle-in-the-Haystack and discuss about the inadequacies of this probing task. Subsequently, to comprehensively evaluate the long-context information awareness of FilM-7B, we introduce **Various Long-context (VaL) Probing**. This includes three tasks that cover various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval).

Figure 4: Performance of FilM-7B on VaL Probing and the comparisons with (a) Mistral, (b) LongAlign, and (c) InternLM2. The X-axis is the relative position in the context (\(\)32K tokens).

### Near-Perfect Performance on Needle-in-the-Haystack: Are We There Yet?

The Needle-in-the-Haystack (Ivgi et al., 2023; Liu et al., 2024b) is widely used to assess how robustly a model utilizes information positioned in the long context. It reveals that even some powerful proprietary LLMs, such as GPT-4 and Claude 2.1 (Anthropic, 2023), struggle to fully exploit the information within the long context.

We use the Needle-in-the-Haystack task5 to preliminarily evaluate the long-context capability of FilM-7B. Appendix B demonstrates that FilM-7B has achieved near-perfect performance on this task. This result is not surprising as recent open-source LLMs, such as LongAlign (Bai et al., 2024) and InternLM2 (Cai et al., 2024), have also shown near-perfect performance on this task.

However, the near-perfect performance on Needle-in-the-Haystack may overestimate the long-context capabilities of LLMs (Lei et al., 2024; Hsieh et al., 2024). Specifically, we have the following two concerns:

* Needle-in-the-Haystack employs a document-style context, which LLMs could be quite familiar with due to the pre-training on natural language corpora.
* The **forward retrieval** pattern in Needle-in-the-Haystack may simplify the difficulty of information seeking in the long context.

The "forward retrieval" means that the information being retrieved directly follows the retrieval keyword in a long context. For example, the default question used in Needle-in-the-Haystack is "What is the best thing to do in San Francisco?" and the answer is contained in "The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day." The retrieved information "eat a sandwich and..." just follows the retrieval keywords "best thing to do in San Francisco". According to the mechanism of induction head (Olsson et al., 2022), such a following-up copying is an easily learned pattern for LLMs, thus less challenging for evaluating long context utilization (just like the observation of "reversal curse" (Berglund et al., 2024)).

Given these considerations, we suggest that performances on Needle-in-the-Haystack may not adequately reflect the long-context capabilities of LLMs. Therefore, we propose VaL Probing for a more comprehensive evaluation involving various context styles and retrieval patterns.

### VaL Probing

Our retrieval-based VaL Probing considers three context styles (document, code, and structured-data context) and three retrieval patterns (forward, backward, and bi-directional retrieval). Each context in VaL Probing contains \(\)32K tokens, and each task contains \(\)3K examples. Figure 3 briefly illustrates the contexts and retrieval instructions in VaL Probing.

Document Sentence Retrieval (Bi-Direction).The contexts consist of numerous natural language sentences, and the instruction aims to retrieve a single sentence containing a given piece. The sentences are sampled from the abstracts of papers on arXiv6. This task follows the bi-directional

    &  &  &  &  \\  & Avg & Gap\(\) & Avg & Gap\(\) & Avg & Gap\(\) & Avg & Gap\(\) \\  Mistral-7B-Instruct-v0.1 (Jiang et al., 2023) & 44.8 & 29.9 & 6.8 & 53.2 & 8.8 & 74.5 & 20.1 & 52.5 \\ Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) & 74.2 & 32.1 & 20.3 & 59.5 & 47.5 & 77.0 & 47.3 & 56.2 \\ LongAlign-7B-64K (Bai et al., 2024) & 65.3 & 16.9 & 39.3 & 56.0 & 55.0 & 36.2 & 53.2 & 36.4 \\ LongAlign-13B-64K (Bai et al., 2024) & 71.7 & 13.4 & 50.8 & 40.8 & 82.9 & 27.0 & 68.5 & 27.1 \\ InternLM2-chat-7B (Cai et al., 2024) & 68.8 & 18.7 & 50.2 & 44.1 & 61.2 & 57.1 & 60.1 & 40.0 \\ InternLM2-chat-20B (Cai et al., 2024) & 66.4 & 27.2 & 63.4 & 45.5 & 74.9 & 57.2 & 68.2 & 43.3 \\ GPT-4-Turbo (OpenAI, 2023b) & 81.3 & 31.7 & 66.1 & 46.5 & **89.6** & 18.0 & 79.0 & 32.1 \\  FilM-7B (ours) & **85.4** & **6.1** & **83.3** & **18.7** & 89.0 & **16.8** & **85.9** & **13.9** \\   

Table 1: Quantified performances of various models on VaL Probing.

retrieval pattern, as the expected retrieval results contain words both before and after the given piece in the context. The evaluation metric is the word-level recall score.

Code Function Retrieval (Backward).The contexts consist of Python functions, and the instruction aims to retrieve the function name for a given line of code within the function definition. The raw code functions are sampled from the StarCoder (Li et al., 2023c) dataset7. We randomly select three lines of definitions for each function. This task follows the backward retrieval pattern, as the function name always precedes the definition. The evaluation metric is the exact-match accuracy.

Database Entity Retrieval (Forward).The contexts contain lists of structured entities, each with three fields: ID, label, and description. The query aims to retrieve the label and description for a given ID. The entities are sampled from Wikidata 8. This task follows the forward retrieval pattern, as the label and description follow the ID. We take a relaxed exact-match accuracy as the metric: a 1 score is given if either the label or the description is exactly matched in the response, otherwise a 0 score.

## 4 Experiments and Analysis

We assess the long-context capability of FilM-7B on both probing tasks and real-world long-context tasks. Moreover, we investigate if the performance in short-context scenarios is affected.

### Experimental Setup

Models.We mainly compare FilM-7B with long-context open-source models that have been trained with \(\)32K context windows, including the Mistral (Jiang et al., 2023), LongChat (Li et al., 2023a), ChatGLM (Du et al., 2022), LongAlign (Bai et al., 2024), LongWanjuan (Lv et al., 2024), Yi (AI et al., 2024) and InternLM2 (Cai et al., 2024). We utilize the instruct/chat versions of these models as most of our evaluation tasks are under the zero-shot instruction-following paradigm. We also draw comparisons with popular proprietary LLMs such as GPT-3.5-Turbo (OpenAI, 2023a) and

    & Document &  &  &  \\  & Avg & Gap\(\) & Avg & Gap\(\) & Avg & Gap\(\) & Avg & Gap\(\) \\  Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) & 74.2 & 32.1 & 20.3 & 59.5 & 47.5 & 77.0 & 47.3 & 56.2 \\ + Normal Instruction Tuning (Lian et al., 2023) & 69.0 & 25.9 & 30.2 & 76.5 & 53.4 & 54.4 & 50.9 & 52.3 \\ + Information-Intensive Training (ours) & **82.9** & **11.5** & **74.5** & **27.7** & **83.5** & **31.6** & **80.3** & **23.6** \\   

Table 2: Quantified comparison between IN2 training and normal instruction tuning.

Figure 5: Compare the performance of IN2 training and general instruction tuning (IT). Both two training process takes the same number of training instances (20% of the full data size, 300K examples). Compare with Mistral + IN2 training, the gains from normal instruction tuning are marginal and unstable.

GPT-4-Turbo (OpenAI, 2023b). All models and tasks employ greedy decoding. For probing tasks, we primarily compare FilM-7B with LongAlign and InternLM2 series, as these models have shown near-perfect performances on Needle-in-the-Haystack.

Real-world long-context tasks.We take 9 tasks from the LongBench (Bai et al., 2023) collection to evaluate the long-context capability on real-world scenarios. These tasks encompass long-document question answering (NarrativeQA (Kocisky et al., 2018), Qasper (Dasigi et al., 2021) and Multi-FieldQA (MultiFQA) (Bai et al., 2023), multi-document multi-hop reasoning (HotpotQA (Yang et al., 2018), 2WikiMultihopQA (2WikiMQA) (Ho et al., 2020) and MuSiQue (Trivedi et al., 2022)), and long-context summarization (GovReport (Huang et al., 2021), QMSum (Zhong et al., 2021) and MultiNews (Fabbri et al., 2019)). We employ the middle truncation strategy in LongBench to limit the input within 32K tokens. We report ROUGE-L (Lin, 2004) for summarization tasks and F1 scores for other tasks. The evaluation metrics are computed using the official evaluation scripts 9.

Despite these QA and summarization tasks, we also conduct evaluations on few-shot learning tasks, in which the contexts could also be extremely lengthy if there are many "few-shot" examples presented in the context. We take three in-context learning tasks from LongBench, including TREC (Li & Roth, 2002) for few-shot classification, TriviaQA (Joshi et al., 2017) for few-shot QA, and SAMSum (Gliwa et al., 2019) for few-shot summarization.

Short-context tasks.We select 8 short-context tasks commonly used for evaluating the general capabilities of models. These include MMLU (Hendrycks et al., 2020), BoolQ (Clark et al., 2019), RACE-High (RACE-H) (Lai et al., 2017), CommonsenseQA (CSQA) (Talmor et al., 2019), ARC-Challenge (ARC-C) (Clark et al., 2018), HellaSwag (Zellers et al., 2019), GSM8K (Cobbe et al., 2021), and MATH (Hendrycks et al., 2021). We use 5-shot for MMLU, 8-shot for GSM8K, 4-shot for MATH, and 0-shot for other tasks. We utilize the lm_eval (Gao et al., 2024) for the evaluations on MMLU, BoolQ, RACE-H, ARC-C and HellaSwag, and use the evaluation scripts from An et al. (2024) for other tasks.

### Main Results and Analysis

FilM-7B significantly mitigates the lost-in-the-middle problem.Figure 3(a) presents the probing results for both FilM-7B and the backbone model, Mistral-7B-Instruct-v0.2. In all three probing tasks within Val Probing, the vanilla Mistral model experiences substantial information loss at the middle positions in the long contexts. In contrast, our FilM-7B model consistently exhibits robust

  Model &  & TriviaQA & SAMSum & Average \\  GPT4-Turbo (OpenAI, 2023b) & 77.0 & 91.7 & 39.7 & 69.5 & & & \\ Mistral-7B-Instruct-v0.2 (Jiang et al., 2023) & 71.0 & 84.5 & 35.8 & 63.8 & & & \\  
**FILM-7B (ours)** & **76.0** & **90.0** & **39.5** & **68.5** & & & \\  

Table 4: Model performances on few-shot learning tasks.

  Model &  & MultiFQA & HotpotQA & 2WikiMQA & MuSiQue & GovReport & QMSum & MultiNews & Avg \\   \\ GPT-4-Turbo (OpenAI, 2023b) & 33.0 & 50.7 & 52.7 & 68.5 & 64.3 & 49.1 & 33.9 & 25.4 & 24.9 & 44.7 \\ GPT-3.5-Turbo (OpenAI, 2023a) & 23.6 & 43.3 & 52.3 & 51.6 & 37.7 & 26.9 & 29.5 & 23.4 & 26.7 & 35.0 \\   \\ LengthCat\(\)1.5-7B-32K’ (Li et al., 2023a) & 16.9 & 27.7 & 41.4 & 31.5 & 20.6 & 9.7 & 30.8 & 22.7 & 26.4 & 25.3 \\ ChatML-62-83K’ (Die et al., 2022) & 21.1 & 31.5 & 46.2 & 25.3 & 20.8 & 9.8 & 32.4 & 24.0 & 26.5 & 26.4 \\ LongAlign-7B-64K (Bai et al., 2024) & 18.7 & 33.8 & 49.1 & 28.6 & 23.4 & 12.5 & 30.6 & 23.7 & 27.5 & 27.5 \\ Mistral-7B-Instruct-v0.01 (Jiang et al., 2023) & 19.6 & 33.2 & 38.8 & 42.9 & 31.2 & 17.4 & 27.5 & 22.4 & 26.6 & 28.9 \\ Mistral-7B-Instruct-v0.02 (Jiang et al., 2023) & 23.5 & 33.8 & 45.9 & 42.4 & 24.3 & 20.8 & 33.3 & 24.8 & 26.8 & 30.6 \\ Yi-60-200K’ (Li et al., 2023) & 12.4 & 26.4 & 36.8 & 46.6 & 40.4 & 25.8 & 29.3 & 20.7 & 27.1 & 29.5 \\ ChatML3-68-32K’ (Die et al., 2022) & 9.2 & **43.1** & 50.9 & 55.3 & 43.7 & **38.9** & **36.0** & 24.7 & 27.4 & 36.6 \\ InternalM2-chat-7B (Cai et al., 2024) & 24.4 & 35.4 & 50.2 & 52.4 & **48.2** & 30.5 & 33.6 & **25.3** & **29.0** & 36.5 \\ InternalM2-7B-LogWangian’ (Lv et al., 2024) & **29.9** & 39.6 & 50.2 & 53.7 & 42.3 & 32.1 & 33.0 & **25.5** & 27.8 & 37.1 \\  FilM-7B (ours) & 26.9 & **42.2** & **56.0** & **62.1** & **47.0** & **39.0** & 33.8 & **25.1** & 26.9 & **38.9** \\  

Table 3: Performances of various models on real-world long-context tasks. Results of models with \({}^{*}\) are reported in Bai et al. (2023) and Lv et al. (2024).

[MISSING_PAGE_FAIL:9]

Training on synthesized long-context data effectively generalizes to real-world scenarios.Table 3 and 4 contain the results on various real-world long-context tasks. It shows that FilM-7B also significantly improves the performance of the backbone model in real-world long-context scenarios. Moreover, it also achieves SOTA-level10 performances on these tasks among \(\)7B size open-source models. Notably, the long contexts used in InN2 training are all synthesized from short segments. These improvements suggest that the long-context capabilities learned from the synthesized data can be successfully applied to real-world tasks.

FilM-7B maintains the performance on short-context tasks.Figure 6 illustrates the performances of FilM-7B and the vanilla backbone model on short-context tasks. It reveals that the overall performances on short-context tasks are almost comparable with minor variances. These results confirm that FilM-7B does not compromise the short-context capabilities of the backbone model.

Analysis on training strategies.We are specifically interested in investigating the impact of the following two training strategies: applying the sliding window and adjusting the position encoding. Due to the page limitation, we provide these further ablations and analysis in Appendix C.

## 5 Related Work

Long-context LLMs.Recent research has significantly contributed to the exploration of training large models with extended context windows (Jiang et al., 2023; Du et al., 2022; Li et al., 2023; Team et al., 2023; Xiong et al., 2023; Song et al., 2023; Tworkowski et al., 2024; Ai et al., 2024; Cai et al., 2024). There are primarily two directions in the development of long-context LLMs. (1) Data engineering, which emphasizes the construction of long-context data for training the LLMs. This includes data balancing (Fu et al., 2024), data order arrangement (Shi et al., 2023), instruction data collection (Bai et al., 2024), and data quality measurement (Lv et al., 2024). Our InN2 training can be categorized into this field. (2) Effective and efficient training, which investigates methods to optimize the training of a long-context model. This encompasses the design of position encoding (Chen et al., 2023; Liu et al., 2023; Peng et al., 2023; Ding et al., 2024), batching strategy (Bai et al., 2024), parameter-efficient training (Chen et al., 2023), and the development of new model architectures (Peng et al., 2023; Gu and Dao, 2023).

Long-context evaluations.Existing benchmarks for evaluating long-context models can be divided into two categories. (1) Real-world benchmarks that assess general long-context capabilities (e.g., long-context QA, summarization, and language modeling), such as NarrativeQA (Kocisky et al., 2018), LongBench (Bai et al., 2023), ZeroSCROLLS (Shaham et al., 2023), L-Eval (An et al., 2023), Loogle (Li et al., 2023), \(\)Bench (Zhang et al., 2024), and a series of work on perplexity evaluation (Beltagy et al., 2020; Roy et al., 2021; Press et al., 2021; Chen et al., 2023; Liu et al., 2023; Peng et al., 2023; Chen et al., 2023; Ding et al., 2024; Mohtsami and Jaggi, 2024). (2) Probing tasks that provide a more concise reflection of the long-context utilization across different context lengths and positions. These include Needle-in-the-Haystack, passkey retrieval (Mothashami and Jaggi, 2024), synthesized document QA (Liu et al., 2024), S3Eval (Lei et al., 2024), Discovery (Li et al., 2024), RULER (Hsieh et al., 2024), and the VaL Probing proposed in this study. Among these probing tasks, our VaL Probing is the first to explicitly incorporate a variety of retrieval patterns.

## 6 Conclusion

This work introduces In2 training to overcome the lost-in-the-middle problem. By applying In2 training on the open-source model, our FilM-7B exhibits significant improvements on probing tasks and real-world long-context tasks while does not compromise the short-context performance.