# MedTrinity-25M: A Large-scale Multimodal Dataset

with Multigranular Annotations for Medicine

 Yunfei Xie\({}^{1,*}\), Ce Zhou\({}^{1,*}\), Lang Gao\({}^{1,*}\), Juncheng Wu\({}^{2,*}\), Xianhang Li\({}^{3}\), Hong-Yu Zhou\({}^{4}\), Sheng Liu\({}^{5}\), Lei Xing\({}^{5}\), James Zou\({}^{5}\), Cihang Xie\({}^{3}\), Yuyin Zhou\({}^{3}\)

\({}^{*}\)equal technical contribution

\({}^{1}\)Huazhong University of Science and Technology,

\({}^{2}\)Tongji University,

\({}^{3}\)UC Santa Cruz,

\({}^{4}\)Harvard University,

\({}^{5}\)Stanford University

###### Abstract

This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and textual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular textual descriptions. Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. This dataset can be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. The dataset is publicly available at https://yunfeixie233.github.io/MedTrinity-25M/.

## 1 Introduction

Large-scale multimodal foundation models [1; 2; 3; 4; 5] have demonstrated remarkable success across various domains due to their ability to understand complex visual patterns in conjunction with natural language. This success has sparked significant interest in applying such models to medical vision-language tasks. Much progress has been made to improve the medical capacity of general domain multimodal foundation models by constructing medical datasets with image-text pairs and fine-tuning general domain models on these datasets [6; 7; 8; 9; 10].

However, current medical datasets have several limitations. Firstly, these datasets lack **multigranular** annotations that reveal the correlation between local and global information within medical images.

Medical images often contain detailed cues, such as regional abnormal textures or structures, which may indicate specific types of lesions. Therefore, multimodal models need the ability to infer global information, such as disease or lesion type, from local details. The absence of such data limits the models' capacity to comprehensively understand medical images. Moreover, current dataset construction methods heavily rely on medical images paired with reports or captions, which restricts their scalability.

In this paper, we address the above challenges by proposing an automated data construction pipeline using multimodal large language models (MLLMss) without relying on paired text descriptions. To address the lack of comprehensive medical knowledge in general-purpose MLLMs, we leverage domain-specific expert grounding models and retrieval-augmented generation (RAG) to extract relevant medical knowledge. We then prompt MLLMs to generate multigranular visual and textual annotations enriched with this knowledge based on identified regions of interest (ROIs). We utilize this pipeline to transform the collected data, including large-scale unpaired images, into image-ROI-description triplets. These triplets provide multigranular annotations that encompass both global textual information, such as disease/lesion type, modality, and inter-regional relationships, as well as detailed local annotations for ROIs, including bounding boxes, segmentation masks, and region-specific textual descriptions. Using the proposed pipeline, we create a large-scale multimodal multigranular medical dataset containing over 25 million triplets, named **MedTrinity-25M**. To our best knowledge, this is the largest multimodal dataset in medicine to date.

Initially, we assemble a large amount of medical data from over 90 online resources such as TCIA, Kaggle, Zenodo, Synapse, etc. In addition to images with a small amount of high-quality paired manual reports, this assembled data also includes two types of coarse medical data: 1) Image data with segmentation masks, lesion bounding boxes, or only disease types but lacking detailed textual descriptions, and 2) Images paired with coarse captions that describe only global modality or disease information, but lack detailed descriptions of local regions. To generate multigranular annotations from the massive coarse medical data, we first identify ROIs that contain disease or lesion patterns by applying expert grounding models. We then build a comprehensive knowledge base from online corpora (e.g., PubMed) and retrieve image-related medical knowledge. Finally, we prompt MLLMs to integrate medical knowledge with guidance of identified ROIs to generate multigranular textual descriptions.

## 2 Related Work

Medical Multimodal Foundation Models.Due to the effectiveness of multimodal foundation models in understanding visual features, adapting these models to perform medical vision-language tasks has garnered increasing attention in recent years [11; 12; 9; 5]. Several papers attempt to adapt general domain multimodal foundation models with varying architecture to medical domain through end-to-end training on medical datasets. For example, Med-Flamingo  enhances the medical capacity of OpenFlamingo-9B  by fine-tuning it with 0.8M interleaved and 1.6M paired medical image-text data. While Med-PalM  adapts PaLM-E  to medical domain using approximately 1M medical data points, demonstrating competitive or surpassing performance compared to state-of-the-art models. Additionally, LLaVA-Med  employs end-to-end visual instruction tuning  with two stages, achieving remarkable results in medical Visual Question Answering (VQA) tasks. Similarly, Med-Gemini  employs a long-form question answering dataset to enhance the multimodal and long-context capabilities of baseline Gemini . Although these models have achieved remarkable performance, they are still limited by the scale of training data. Prior research  has shown that scaling up the training data improves the performance of large multimodal foundation models. In this paper, we aim to build a large-scale medical dataset to facilitate the development of more powerful medical multimodal foundation models.

**Multimodal Datasets for medicine.** The significance of construting comprehensive medical multimodal datasets has garnered considerable attention [9; 18; 19; 7]. Several works attempt to collect images and paired clinical reports prepared by pathology specialist [19; 7; 8], which provide comprehensive descriptions of images, including disease types and corresponding reasoning. For example, MIMIC-CXR comprises 227,835 images for 65,379 patients, containing pathological findings and impressions in reports paired with each images. However, manually constructing such reports is both time-consuming and expensive, thereby limiting the scale of these datasets. PMC-OA  aims to expand the dataset scale by extracting a large number of image-caption pairs from medical papers, increasing the number of data samples to 1.65 million. However, the extracted captions are less detailed compared to manual clinical reports, resulting in a lack of multigranular annotations. RadGenome-Chest CT  includes more detailed annotations, such as segmentation masks and medical reports generated by MLLMs. Nonetheless, its construction method still relies on paired image-text data, which limits its scalability. Unlike these existing methods, we devise the first automated data construction pipeline to generate multigranular annotations for unpaired images, achieving a comprehensive multigranular dataset with 25 million data samples.

## 3 MedTrinity-25M Dataset

### Data Triplet

Our dataset comprises triplets of \(\{,,\}\). Each ROI is associated with an abnormality and is represented by a bounding box or a segmentation mask, specifying the relevant region within the image. For each image, we provide a multigranular textual description, which includes the disease/lesion type, modality, region-specific description, and inter-regional relationships as illustrated in Figure 2.

Figure 1: Qualitative comparison with different types of dataset.

Images.We use the original medical image in the source dataset, we extensively collected medical datasets from the following sources: (1) online resources such as TCIA, Kaggle, Zenodo, Synapse, Hugging Face,Grand Challenge, GitHub, etc. (2) relevant medical dataset research, such as CheXpert  and DeepLesion . These datasets were first categorized into two types: (1) datasets containing local annotations, such as MIMIC-CXR  with corresponding radiology reports, and PMC-OA  with corresponding captions, where the reports or captions provide analysis of specific local conditions in the images; another example is the 3D image segmentation dataset BraTS2024 , which marks the tumor regions in CT scans with masks. (2) datasets containing global annotations: such as image classification datasets ISIC2019  and ISIC2020 , whose classification labels reflect the overall pathological condition of tissue sections; another example is the CheXpert  dataset, which provides detailed classification of disease types for each chest X-ray. We collect 25,001,668 samples spanning 10 modalities and over 65 diseases. For 3D volumetric images stored in DICOM or NIfTI formats, we converted each 2D slice to PNG format. Additional caption and annotations like masks and bounding boxes from these datasets were utilized to construct ROIs and corresponding textual descriptions as below.

ROIs.For each image, ROIs are highlighted using segmentation masks or bounding boxes. These ROIs mostly contain pathological findings such as lesions, inflammation, neoplasms, infections, or other potential abnormalities. In the few cases without abnormalities, the ROIs generally indicate the primary object or organ in the image, as shown in examples in the supplementary material.

Textual Descriptions.The textual descriptions for each image are provided with detailed information across various aspects. Unlike the unstructured free-text descriptions found in previous medical report datasets[7; 8; 6] or simple short sentences in visual QA dataset[28; 22] and caption dataset[18; 24], our textual descriptions are multigranular and structured. General attributes related to the image are described first, including the image modality, the specific organ depicted, and the type of disease presented. Subsequently, ROI-related information is provided, including their locations and the abnormal characteristics within them that indicate underlying pathology, such as distinctive color and texture. Additionally, comparisons between the ROIs and surrounding regions are presented to highlight differences in features and the extent of disease progression.

We also demonstrate the multigranular textual descriptions in our dataset with those in other common forms. As illustrated in Figure 1, our textual description is multigranular with more attributes than radiology report of chest x-rays dataset MIMIC-CXR , visual QA dataset SLAKE and radiology objects caption dataset ROCO.

### Data Construction Pipeline

Given a medical image, we aim to generate corresponding multigranular visual and texual annotations by leveraging MLLMs. Specifically, as shown in Figure 2, our pipeline can be decomposed into two stages - **Data Processing** and **Generation of Multigranular Text Description**. In the **Data Processing** stage (Section 3.2.1), we address the lack of domain-specific knowledge in general-purpose MLLMs by leveraging expert grounding models and retrieval-augmented generation (RAG). This stage includes three key steps: 1) **Metadata Integration** to produce coarse captions encapsulating fundamental image information such as modality and disease types; 2) **ROI Locating** to identify regions of abnormalities; and 3) **Medical Knowledge Retrieval** to extract relevant fine-grained medical details. Based on the processed data, we then prompt MLLMs to generate multigranular text descriptions, resulting in the creation of fine-grained captions, as detailed in Section 3.2.2.

#### 3.2.1 Data Processing

Coarse Caption Generation via Metadata Integration.We aim to generate coarse captions that provide fundamental information for a given image, including modality, organ labels, disease types, and optionally, camera views and equipment information. Instead of extracting features directly from the images, we generate these captions by integrating dataset metadata. We first extract metadata from the datasets and then apply a fixed rule to integrate this information into coarse captions. For example, for an image from the QaTa-COV19 dataset1, we derive metadata from the dataset's accompanying paper or documentation, indicating that it consists of COVID-19 chest X-ray images. Next, we construct coarse captions like "A chest X-ray image with COVID-19 in the lungs" highlighting the modality, organ types, and disease labels. If the image contains additional textual information like radiological findings, this is also integrated to enhance the richness of the caption. The effectiveness of adding coarse captions when generating fine-grained captions is illustrated in Figure 3. In contrast to the scenario without a coarse caption where MLLMs fails to recognize the disease, providing MLLMs with a coarse caption that includes the disease type "COVID-19" enables it to identify and categorize the disease, thereby laying the foundation for further analysis.

ROI Locating.We employ various strategies to locate Regions of Interest (ROIs) in images. For datasets that already include localization annotations, such as segmentation masks or bounding boxes, we derive the ROIs from these existing annotations. Specifically, bounding boxes are directly used

Figure 3: **A qualitative comparison example of generated textual description with and without coarse caption.** Without a coarse caption, MLLMs fails to detect diseases. On the contrary, providing a caption mentioning “COVID-19” allows MLLMs to identify and categorize the disease, facilitating further analysis.

Figure 2: **Data construction pipeline.** 1) Data processing: extracting essential information from collected data, including **metadata integration** to generate coarse caption, **ROI locating**, and **medical knowledge collection**. 2) Multigranular textual description generation: using this information to prompt MLLMs to generate fine-grained captions.

as the ROIs, while segmentation masks are converted to ROIs by creating the smallest bounding box that covers the mask. When such localization annotations are not available, we apply different pretrained expert models listed in the Appendix to generate ROIs. For text-prompt driven grounding model, we use disease and organ information in coarse captions as text prompts to guide the model in segmenting specific parts. Examples of generated ROIs from various modalities with different models are demonstrated in Figure 6.

Without ROIs, the original description is limited to a brief global analysis of the image. However, with ROIs, MLLMs can perform a more detailed local analysis of the ROIs and assess the impact of lesion ROIs on the surrounding normal regions, as demonstrated in Figure 4.

Medical Knowledge Retrieval.General-purpose MLLMs often produce content that lacks specialized medical terminology and professional expression. To address this issue, we build a medical knowledge database following the approach in MedRAG . We collect three main corpora: PubMed2 for biomedical knowledge, StatPearls3 for clinical decision support, and medical textbooks  for domain-specific knowledge. We segment these corpora into short snippets and encode

Figure 4: **A qualitative comparison example of generated textual description with and without locating ROIs.** Without ROIs, the caption offers only a brief global analysis; with ROIs, MLLMs conducts detailed local analysis and assesses the impact of lesion ROIs on adjacent normal regions.

Figure 5: **A qualitative comparison example of generated textual description with and without external medical knowledge.** MLLMs can standardize medical terminology in its expressions and refine its diagnosis based on disease progressions detailed in medical literature.

them into high-dimensional vectors using the text encoder from Med-CPT . These vectors are then indexed into a specialized vector knowledge base using Faiss, optimized for efficient retrieval.

For a given image, we retrieve relevant medical knowledge by using its coarse caption, which is generated through metadata integration. Specifically, we encode the coarse captions, including disease and organ classifications, into vectors using the Med-CPT text encoder. We then perform a vector similarity search in the medical vector database, retrieving the top eight medical knowledge snippets that semantically match the query. These snippets provide the external medical knowledge paired with the image. A qualitative example demonstrating the effectiveness of incorporating external medical knowledge is shown in Figure 7. With access to COVID-19-related medical knowledge, MLLMs can standardize medical terminology and refine diagnoses based on the disease progressions outlined in medical literature.

#### 3.2.2 Generation of Multigranular Text Description

After data processing, a comprehensive prompt is utilized to guide the MLLMs in generating multi-granular descriptions. The prompt template consists of a three-level hierarchical framework with questions to instruct MLLMs: (1) a global description that captures all details of the image; (2) a local-focused analysis of specific ROIs that potentially are unusual; and (3) a local-global examination of the interaction between local and global attributes to understand the impact of local abnormalities on the entire organ. Detailed prompt template is presented in supplementary materials.

To ensure that the MLLMs are guided by relevant medical information not inherently present in their training data, we incorporate the processed data (coarse captions, ROIs, and retrieved medical knowledge) into the prompts. Specifically, for global information, coarse captions are directly integrated into the prompt. For local information, ROIs on images are converted into textual descriptions based on their coordinates and area ratio within the images. Examples of these textual descriptions are shown in Figure 6, using terms such as "left-center" and "area ratio: 1.2%."

To refine terminology and diagnosis within ROIs, relevant medical knowledge about specific diseases is incorporated into the prompt. Instead of merely inserting this knowledge, we instruct MLLMs to identify and align the relevant knowledge to ROIs that require analysis.

**Choice of MLLMs** We first prompt GPT-4V with the provided medical coarse captions, ROIs, and medical knowledge to generate a subset of 200,000 samples, maintaining a similar modality and organ distribution to our full 25 million dataset. The goal of curating this subset is to calibrate a medical knowledge-guided MLLM to adhere to the formatting instructions specified for our text.

Figure 6: Example of ROIs and their corresponding textual descriptions.

Figure 7: **An example of the Top-8 retrieval results.** By leveraging COVID-19-related medical knowledge, MLLMs can standardize medical terminology and enhance diagnoses according to the disease progressions described in medical literature.

Subsequently, we employ our model, LLaVA-Med++, which is based on LLAVA-Med , the state-of-the-art medical MLLM. To further improve this model, we leverage the latest LLaMA3 to enhance its linguistic capabilities, and incorporate multi-scale feature extraction  to improve its vision capabilities. LLaVA-Med++ undergoes continuous training on medical multimodal data and is fine-tuned using our multigranular annotations, resulting in a specialized medical model.

After fine-tuning, we then use this specialized model to generate the multigranular text descriptions on our entire dataset, resulting in 25 million image-ROI-description triplets. The fine-tuning process leverages the advanced language organization capabilities of GPT-4V, providing an effective template for fine-grained captions, which our model uses to learn the formatting of fine-grained captions. As a result, our model generates more detailed descriptions compared to GPT-4V, as illustrated in Figure 8. We also show a detailed quantitative comparison in the supplementary material.

Figure 8: **Qualitative Comparison with sample generated by GPT-4V Compared to GPT-4V, our model generate more detailed caption.**

Figure 9: Statistical overview of MedTrinity-25M.

[MISSING_PAGE_EMPTY:9]