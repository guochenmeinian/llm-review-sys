# The Fragility of Fairness:

Causal Sensitivity Analysis for Fair Machine Learning

 Jake Fawkes\({}^{*}\)

Department of Statistics

University of Oxford

jake.fawkes@st-hughs.ox.ac.uk &Nic Fishman\({}^{*}\)

Department of Statistics

Harvard University

njwfish@gmail.com &Mel Andrews

Philosophy Department

University of Cincinnati &Zachary C. Lipton

Machine Learning Department

Carnegie Mellon University

###### Abstract

Fairness metrics are a core tool in the fair machine learning literature (FairML), used to determine that ML models are, in some sense, "fair." Real-world data, however, are typically plagued by various measurement biases and other violated assumptions, which can render fairness assessments meaningless. We adapt tools from causal sensitivity analysis to the FairML context, providing a general framework which (1) accommodates effectively any combination of fairness metric and bias that can be posed in the "oblivious setting"; (2) allows researchers to investigate combinations of biases, resulting in non-linear sensitivity; and (3) enables flexible encoding of domain-specific constraints and assumptions. Employing this framework, we analyze the sensitivity of the most common parity metrics under 3 varieties of classifier across 14 canonical fairness datasets. Our analysis reveals the striking fragility of fairness assessments to even minor dataset biases. We show that causal sensitivity analysis provides a powerful and necessary toolkit for gauging the informativeness of parity metric evaluations. Our repository is available here.

## 1 Introduction

Fair machine learning (FairML) is a theoretical approach to studying and remediating disparities in prediction and allocation systems based on machine learning algorithms. A core focus of the field has been to develop, evaluate, and train models to satisfy a number of "fairness metrics". These metrics operationalize the social ideal of fairness as a statistical quantification of some performance measure compared across demographic groups. Such evaluations often play an important role in auditing ML systems  to certify whether models satisfy some tolerable level of statistical disparity.

Real-world data, however, is frequently plagued by a variety of measurement biases and other violated assumptions which can undermine the validity of fairness metrics. While such biases come in many forms, in this work we focus on the following: noisy or poorly-defined outcome measures (proxy bias) , the observation of samples or outcomes from only a subset of the population (selection bias) , or causal impacts on outcomes through background policies within a firm's control , which we term _extra-classificatory policies_, or ECPs. We focus on these varieties of bias due to their ubiquity in FairML applications, which we demonstrate through an analysis of their prevalence and magnitude in a range of benchmark datasets (see Table 1 and App. D).

Motivated by the problems posed by such measurement biases, we offer a framework based on graphical causal inference to operationalize assumptions about data quality issues, alongside methods adapted from causal sensitivity analysis for statistical quantification of their impacts on fairness evaluations. This framework enables both ML practitioners and auditors to empirically gauge thesensitivity of parity metrics to assumption violations for specific combinations of metrics, datasets, and use cases. Causal inference is particularly apt for this problem, as it provides a formal language within which to precisely identify the goals of a particular study: what is the quantity we seek to estimate, and in which population? This accounts for the success of causal inference in the social sciences and makes it similarly well-suited for use in the arsenal of ML auditing tools.

We leverage recent developments in automated discrete causal inference, particularly the autobounds framework of Duarte et al. , to provide a unified causal sensitivity analysis framework for the "oblivious" setting, as laid out in Hardt et al. . In this setting, we only have access to protected attributes \(A\), the true target labels \(Y\), and the predicted labels \(\), but not to covariates \(X\). For example, in evaluating racial discrimination in loan granting, one has access to the race attribute \(A\), the true repayment rate \(Y\), and the predictions \(\), but not to input features \(X\) nor the form of \((X)\).

This lends us a straightforward procedure for performing sensitivity analyses for any combination of measurement bias and suitably well-behaved metric that can be posed obliviously: (i) Express the bias in terms of a causal graph--a directed acyclic graph, henceforth DAG, (ii) choose a sensitivity parameter to control the degree of bias, (iii) provide any additional probabilistic assumptions or relevant structural knowledge. The problem of bounding a statistic under a given degree of bias can then be converted to solving a given constrained optimization problem , which is achieved via a branch and bound solver , leading to valid bounds even when a global optimum is not reached.

We apply this framework to systematically explore the sensitivity of different metrics to the three biases--proxy label bias, selection bias, and extra-classificatory policy bias--for different datasets and classifiers. Our results reveal that many fairness metrics are in fact _fragile_: realistic violations of core underlying assumptions can imply vacuously wide sensitivity bounds. In other words, features of typical deployment contexts can easily render fairness evaluations useless or uninformative.

The fragility of well-known fairness metrics to pervasive biases represents one key empirical finding. Our second core result demonstrates the existence of tradeoffs between the complexity and fragility of fairness metrics. The robustness of parity notions scales inversely with their dependence on predictive outcomes and the intricacy of this dependence function. We find demographic parity to be most robust to measurement biases, while predictive parity metrics exhibit the most fragility to bias. In light of known incommensurability results , we urge the importance of understanding these tradeoffs and their practical implications, for both practitioners and auditors alike.

With these experiments, we aim to demonstrate that the biases we describe are an unavoidable aspect of the FairML problem, not a mere addendum. As such, we have hopes that our unified sensitivity analysis framework can enable both auditors and practitioners to understand how robust their "fairness" evaluations are to various measurement biases by precisely articulating the quantity they wish to evaluate and its divergence from what has been measured. Finally, we hope this work will inspire greater emphasis going forward on the realities of real-world deployment scenarios, such as measurement biases, and their impacts on fairness evaluations.

## 2 Related work

Proxy Label BiasProxy Label bias is a foundational problem in FairML, endemic within criminal justice and legal applications of ML, which the fairness literature originally arose to address [5; 28]. Fogliato et al.  presents one of the earliest considerations of sensitivity analysis within FairML, algebraically deriving sensitivity bounds for proxy label bias. As we demonstrate in 6, our approach is capable of re-deriving and extending these results. In a similar vein, Adebayo et al.  studied the effects of label noise on fairness metric evaluation, although this work was largely empirical. Guerdan et al.  propose several causal models for reasoning about proxy labels in human--algorithm joint decision making, which can be rendered compatible with our sensitivity analysis framework. Further work has explored alternate aspects of fairness evaluations under proxy label bias [63; 64].

Selection BiasSelection bias was first considered in FairML as "selective labels" by Lakkaraju et al. , focused on the scenario in which a policy determines which outcomes are observed. Kallus and Zhou  study the effects such a biased policy can have on equalized odds for the unselected population when predictors are trained only on the selected population. Various works now link selection bias in fairness to causal inference [26; 59] with Goel et al.  providing a summary of the different types of selection in terms of causal graphs. Coston et al.  take an importance-weighting approach to train FairML classifiers with selective labels, under assumptions on the structure of the missingness. Zhang and Long  study how to assess the accuracy parity in unselected data, from the selected data, under assumptions on selection structure and the FairML model class.

Extra Classificatory Policy BiasWe investigate the impacts of extra classificatory policies, that is, policies under the control of the predicting agent which causally affect the outcome of interest. This work is related to counterfactual risk assessments , and subsequent work on counterfactual equalized odds . Sensitivity analysis approaches have further been developed for unmeasured confounding [12; 52]. Our methodology differs from these works in our focus on the oblivious setting. As such, we focus less on identification, but rather on how influential a policy would need to be before it could significantly impact a fairness evaluation.

Additional Related WorkBeyond the above work on sensitivity analysis there are other general approaches to understanding the robustness of algorithmic fairness to data bias , such as adversarial robustness [13; 41] and distributional robustness . These are very flexible in terms of the types of bias they can represent, however, this renders them less interpretable and less able to incorporate additional assumptions. One measurement bias we did not consider is proxy attribute bias , for which there exist quite comprehensive sensitivity analysis results . There is also a selection of work performing sensitivity analysis for unmeasured confounding in causal fairness [38; 58; 65]

## 3 Measurement Biases

This section introduces the measurement biases we consider via two reccurring examples, demonstrating how they arise in the wild and emphasizing the role of practitioner choice. We first deliver a conceptual illustration of said biases (3.1-3.3) before presenting an empirical analysis of their prevalence across a range of FairML benchmark datasets .

### Proxy Label Bias

We first discuss proxy label bias, introducing it via the following example:

**An Algorithmic Hiring System:**_Suppose that a company receives thousands of applications for every job they advertise. To handle this, they elect to build an ML-based system to assist in sifting through candidates. They opt to construct a model for predicting employees' performance review scores from their resumes, which is then used to assign scores to applicants based on predicted performance reviews. Finally, to ensure that the model is fair, they check against standard fairness metrics on a held-out subset of the training data._

The company described has taken steps to ensure that its job candidate filtering model is "fair." However, it has only run parity tests relative to the variable chosen as its target of prediction. The latent variable of interest in this scenario, what the firm ideally strives to predict, is "employee quality." However, "employee quality" is multifaceted and socially constructed; it is not a phenomenon that can be directly and objectively measured.1. Instead, engineers leverage a _proxy label_. Unlike the nebulous property of employee quality, employee performance reviews are readily available. It is facially not unreasonable to take performance reviews to stand in for employee quality; after all, the one exists, at least putatively, to track the other. However, there is a problem with this strategy: It has been extensively documented that performance reviews are often discriminatory--in other words, performance reviews are both a worse signal of the true underlying latent for certain demographics, and are skewed negative for those demographics relative to the true latent [20; 55].

If an outcome is biased, then classifiers optimizing predictive accuracy on a proxy can appear to satisfy a fairness metric when, in reality, the metric has inherited the biases of the outcome. Practitioners must understand whether a particular outcome is well-suited to the underlying decision problem, alongside any skew or measurement bias that may be induced by using such a measure. Often the use of a proxy outcome is unavoidable. If so, sensitivity analysis is a key tool for understanding what impact biases in the proxy could have on parity metric evaluations.

### Selection Bias:

Introducing our second example to discuss selection bias:

**An Automated Loan-Approval Algorithm:**_A large bank has an online lending platform that receives thousands of loan applications per day, the vast majority of them for under USD $1,000. The bank cannot afford to assign employees to assess all the applications, and elects to automate the process. The bank uses its data on loan repayment to fit a model for likelihood of repayment, with loans automatically approved if the estimated probability of repayment sits above some threshold. The model is once again assessed via standard fairness metrics on holdout data._

As in the previous example, the bank only possesses repayment information for the population that has historically been approved for loans; the subset of the broader population that was not granted a loan is therefore unobserved. Each firm trains a classifier on the subpopulation for which it possesses outcome data. The classifiers' performance on the entire population of job applicants and loan applicants, respectively, are unknown. Further, fairness guarantees on only selected populations can fail to meaningfully extrapolate to deployed models, especially when historical selection procedures encode biases. This phenomenon has been referred to as _selective labels_ or _rejudiced data_, and now more commonly under the catch-all term of _selection bias_.

While selection bias covers the selective labels case, it encompasses a broader class of examples. An important question in the evaluation of fairness metrics is what population the practitioner would ideally want to evaluate the statistic in. We refer to this as the _reference population_. At bare minimum, the reference population should be the population the model is deployed on, not the training population--as the selective labels literature points out. However, there are situations where arguments could be made for broader reference populations. For example, should fairness metrics for an algorithmic hiring system be assessed on the local pool of applicants to that company, or the global pool of applicants to similar roles? Dai et al.  demonstrate that, with reputational dynamics in play, applicants may strategically apply to firms based on their chances of success. A firm's hiring practices can therefore satisfy fairness desiderata relative to their applicant pool having radically skewed the demographics of that population via a history of discriminatory hiring policies. Put crudely: the firm that "women candidates know not to apply to" is not non-discriminatory, but an evaluation of hiree demographics relative to the applicant pool might deceptively depict it as such.

There are no universalisable solutions to such issues, as the choice of reference population must depend on precisely what task the model is being trained to perform, and where it will ultimately be deployed. This further points to a need for practitioners to be clear about what population they have chosen, how it may differ from the data they have measured, and why this specific reference population is preferable to others for the task at hand.

### Extra-Classificatory Policy Bias

A third issue emerges when considering additional context in the loan-approval case study:

**Automated Loan-Approval (cont):**_Our bank now has a model for loan approval. However, they must also set interest rates for approved loans. For this, they employ a legacy model, which works off of a number of factors including credit score, loan amount, lendee address, and various macroeconomic variables._

While the bank has determined that the classifier is fair in the sense that it does not discriminate according to known demographic features and relative to repayment history, these notions of fairness fail to account for the interest rate that the bank determines. As this has a direct and powerful impact on a lendee's likelihood of repayment, if it is set in an (intentionally or unintentionally) discriminatory manner, it can have a large and unaccounted-for effect on the evaluation of fairness metrics.

This is not specific to the loan approval setting; in many scenarios, firms control a number of "levers" that causally impact outcomes for classified populations--we call these _extra-classificatory policies_. Such extra-classificatory policies can drastically affect outcomes, and so shape the data on which models are trained. Through such policies, a firm can create the appearance of demographic base rate discrepancies which then, via predictive models trained on historical data, serve to justify differential lending policies across demographic subpopulations.

The issues pointed towards by extra-classificatory policies are multi-faceted and context-specific. As such, a key challenge to the FairML community is detailing such issues and forming mathematical models of them. We focus on binary policies (e.g. ), leaving more general settings to future work.

### Cross-Dataset Analysis

To demonstrate the prevalence of these issues, we analyze the presence of each measurement bias for the FairML benchmark datasets given in Le Quy et al. . We remove any datasets not associated with a concrete decision problem, leaving 14 datasets from a variety of different domains, including financial risk, criminal justice, and employment/university admissions. We summarise the results in Table 1, with the full table of datasets and biases along with rationales available in Appendix D. Our results demonstrate the scope of the problem created by measurement bias, with all the datasets displaying at least one of the problems and 60% displaying all three simultaneously. From this, we see that such biases are themselves a key part of the FairML problem, not optional complications. Moreover, this points to the need for methodological solutions for evaluating and training FairML models in settings where multiple biases are present simultaneously.

## 4 Graphical Causal Sensitivity Analysis

Here we outline some technical background on graphical causal sensitivity analysis, from graphical sensitivity analysis to the autobounds framework  which automates discrete sensitivity analysis. In the following section, we apply this to FairML to construct a sensitivity analysis tool for oblivious settings , where we do not observe covariates and all other variables are discrete.

NotationWe let \(Y\) denote the outcome the practitioner wishes to measure, \(X\) the observed covariates, \(A\) the protected/sensitive attribute, and \(\) the prediction of \(Y\) with domains \(,,,}\).

### Causal Background

We begin by defining the structural causal model (SCM) approach to causality [49; 54]. Here we model causal relationships via deterministic functions of the observed variables and additional latent variables, with the latter representing the unobserved or random parts of the system. We will always label the observed variables \(\) and the unobserved variables \(\). These equations lead to a causal graph that has a node for each variable and a directed edge \(V_{1} V_{2}\) if \(V_{1}\) is an argument of the function determining the value of \(V_{2}\). To illustrate this, the following figure demonstrates the SCM and corresponding graph we assume throughout for the relationships between \(A,X,Y,\):

We will use \(\) to depict an SCM, which consists of a collection of functions and a probability distribution over the noise terms. A complete definition of SCMs can be found in Appendix B.1.

Marginalisation In Causal ModelsOften--and especially in FairML applications--we do not observe all relevant variables. For example, in this work, we assume the covariates \(X\) are unobservable. However, this is less of a problem than it originally appears due to latent projection, as introduced by Verma and Pearl . Latent projection allows us to marginalize out any unobserved variables

  Proxy Bias & Selection Bias & ECP Bias & One Bias & Two Biases & Three Biases \\ 
69\% & 85\% & 85\% & 100\% & 92\% & 61\% \\  

Table 1: We document the proportion of realistic FairML benchmark datasets (from Le Quy et al. ) that exhibit each of the three biases we discuss. For more details see App. D.

while preserving the causal structure over observed variables, as demonstrated by Evans . We visualize this process in Fig. 2, where we marginalize over \(X\) leaving just \(A,,Y\) and latent variable, \(U\). We outline this procedure in detail in Appendix B.2. The important point is that we can always preserve the causal structure over our observable variables by using a finite number of latent variables. This is the case regardless of how many variables we marginalize out.

### Partial Identification and Sensitivity Analysis

We now show how structural causal models can be used to perform sensitivity analyses for causal (or non-causal) queries, first introducing the important concept of partial identification.

Partial IdentificationIn partial identification, the goal is to understand what values a particular statistic can take relative to our assumptions. We call this a _Query_ and write it as a function \(()\) which takes an SCM and returns a real number. For example, if we wanted to measure counterfactual fairness (for binary \(A\)), we could define the query \(_{}\) as:

\[_{}() P_{}((A=1) (A=0))\]

Where \(_{}()=0\) exactly when the predictor is counterfactually fair according \(\).

Given a query of interest, \(\), the goal of partial identification is to understand what possible values \(\) can take given the practitioner's prior knowledge and assumptions. Here we will consider partial identification for a fixed DAG and a set of constraints on the probability distributions and functions defining the SCM. Practitioners can encode these assumptions by defining a set of SCM models, which we will write \(\). The most natural example here is to let \(\) contain all possible causal models arising from the graph with the same observational distribution as the measured dataset. Practitioners can restrict this set of SCMs to incorporate more domain-specific information, making the bounds more informative. Using this notation, partial identification is rendered as a pair of optimization problems that lower and upper bound the query of interest over \(\):

\[_{}}(}) ()_{}( })\] (1)

Sensitivity AnalysisIn sensitivity analysis, the goal is to understand how violations of assumptions affect the measure of a statistic. The initial step is to define some sensitivity parameter, which measures the degree to which an assumption is violated. For example, in the proxy attribute literature [14; 36], the goal is to understand how sensitive fairness metrics are to mismeasured protected attributes. For example, as we will discuss later in terms of proxy bias, a natural sensitivity parameter would be \(P(Y_{P} Y)\), where \(Y_{P}\) is the proxy outcome. We may then let \(_{}()\) be the set of causal models that have \(P(Y_{P} Y)\) and comply with the practitioner's assumptions. By repeatedly solving the partial identification problem for different \(\) to understand how large \(\) must be, the statistic \(\) becomes uninformative.

### Discrete Causal Sensitivity Analysis

To perform the partial identification required for causal sensitivity analysis, we have to solve the max/min problem in (1). This problem as formulated generically is not tractably solvable, but additional structure on either the query, \(\), or set of models, \(\), can lead to tractable optimization problems and computable bounds. We focus on settings where all variables are discrete, which is particularly helpful for partial identification problems [29; 9; 53] due to the function response framework . This framework takes advantage of the fact that, given a causal graph \(\), if we fix the latent variables \(\), the structural equations are deterministic functions of their other inputs in \(\). If the observed variables are discrete, there are only finitely many such functions. As a result of this, we can represent every single SCM using the one set of fixed structural equations and a distribution over some discrete latent variables, \(}\). This means that any SCM with discrete observed variables and a fixed graph \(\) can be represented entirely by the distribution \(P(})\), and so by a point in the probability simplex \(^{k}\) for some \(k\)[21; 24].

Duarte et al.  showed that this allows partial identification problems (1) to be converted into tractable optimization problems, where now the set of causal models \(\) corresponds to a subset of the probability simplex \(^{}^{k}\) and the query \(\) becomes a function \(f_{}:^{k}\). Moreover, any statement that can be written as a polynomial in probabilities over factual and counterfactual statements corresponds to a fractional polynomial in \(_{C}\). So, if \(f_{}\) is a polynomial in such probabilitiesand any prior assumptions can be stated via probabilistic statements and arithmetic operations, the partial identification problem can be converted into a polynomial programming problem. Duarte et al.  propose to solve these partial identification problems via branch and bound solvers , which ensures that the program produces valid bounds even if convergence is not reached.

## 5 Causal Sensitivity Analysis for FairML

We now apply the methodology outlined in Section 4.3 to the FairML setting to create a sensitivity analysis tool for parity metric evaluations. We focus on settings where \((A,Y,)\) are discrete and the auditor does not have access to the covariates \(X\). The following steps lead to a sensitivity analysis tool for any measurement bias that can be stated obliviously and any statistic that can be written as a polynomial in factual and counterfactual probabilities:

1. Determine how the sampled population differs from the target population, expressing the difference in terms of a causal graph over all variables. Marginalize out \(X\), to leave a causal structure over \((A,Y,)\) and any bias-specific variables.
2. Choose a sensitivity parameter to control the degree of measurement bias and provide any additional knowledge relevant to the task at hand.
3. With all this perform the sensitivity analysis by repeatedly solving the optimization problem in (1) for the test statistic using the methodology outlined in 4.3.

We apply this procedure to each of the measurement biases given in Section 3, using the causal graphs in Fig. 2 to depict the biases. Causal graphs are context-specific, so we do not expect these to be appropriate in all cases. Instead, we use them as plausible graphs for showcasing our framework.

Proxy Label Bias **(1)** We represent the difference between the measured and target populations using an additional variable \(Y_{P}\). This denotes the measured proxy of the outcome. We assume that this outcome is a noisy version of the true outcome \(Y\), where the noise depends on \(A\) and can optionally depend on \(X\). As we show in Appendix B.3, assuming the proxy depends on additional unobservables leads to the same graph over \((Y,A,)\). **(2)** For the sensitivity parameter, we use the probability that the proxy differs from the outcome the practitioner hopes to measure, so \(P(Y_{P} Y)\).

Selection Bias**(1)** We signal whether or not an individual is selected with a binary variable \(S\), which we assume depends on an individual's protected attribute, covariates, and, in some instances, the outcome. **(2)** For the sensitivity parameter, we choose \(P(S=0)\), the probability of a sample not being selected. This controls the proportion of the population which remains unobserved. However, there are other natural choices for sensitivity parameters, such as statistical measures of sample quality . For selection bias, the practitioner could have significant information about the unselected population which could be used to tighten bounds. For example, selective labels would lead to information on the covariates for the unselected populations, and thus the \((A,)\) proportions.

Extra-Classificatory PoliciesThere are a plurality of plausible ways to mathematically represent the problems arising from ECPs, with different formulations being better suited to different concerns. Here we proceed as follows: **(1)** We use an additional variable, \(D\), to depict the policies in the firm's control, which we take to be binary. We then use counterfactual versions of parity metrics, similar to Coston et al. , with formulations given in Appendix A.2. **(2)** We assume the treatment is monotonic, so \(Y(D=1) Y(D=0)\) and use the average treatment effect, \((Y(D=1)-Y(D=0))\), as a sensitivity parameter. For ECPs, there are additional constraints that could be added, for example, if a policy is observed we can include that data explicitly.

Figure 2: Causal graphs for each of the biases showing the assumed causal structure over all variables, and the implied structure upon marginalizing out \(X\). Dashed lines denote varying assumptions.

## 6 Experiments

In this section, we showcase the use of causal sensitivity analysis for fairness applications by performing sensitivity analyses for each of the biases introduced above on a set of benchmark datasets. In doing so, we aim to reveal some of the nuances of performing sensitivity analyses for various types of bias. These experiments further lend themselves to some general conclusions about the complexity of real-world fairness evaluations. We present additional results in Appendix C, including tests of causal fairness metrics that reveal the effects of measurement bias in a causal setting.

### Recreating Fogliato et al.  under varying assumptions

Fogliato et al.  aims to assess how sensitive the false positive rate (FPR), false negative rate (FNR), and positive predictive value (PPV) are to proxy bias for a predictor trained on the COMPAS dataset , under the assumption that \(P(Y_{P}=1|Y=0)=0\). The outcome in the COMPAS dataset is reported re-offense. This outcome a proxy, because we cannot actually observe whether someone has re-offended, we only know if they were convicted of a new offense. The assumption that \(P(Y_{P}=1|Y=0)=0\) implies that whenever someone is reported to have re-offended they actually re-offend; all the measurement error hence comes from people who did re-offend who did not get caught. We start by recreating the results of the original study from just the DAG in Fig. 8(a). This confirms the correctness of the computational bounds: they always match the true algebraically derived bounds in Fogliato et al. .

Next we demonstrate the flexibility of our framework by switching from the assumption that \(P(Y_{P}=1|Y=0)=0\) to \(P(Y_{P}=0|Y=1)=0\). This is probably a less realistic assumption in the COMPAS dataset, implying all measurement error comes from false convictions rather than under-reporting, but in a different context, this might be a more reasonable assumption. We derive the algebraic bounds to confirm the computational bounds match. The point here is that practitioners can easily encode whatever assumptions make sense in their particular context and quickly get results without needing to algebraically derive bounds, and that these results really do vary under different assumptions.

Finally, in Fig. 8(c), we drop the dashed edge between \(X\) and \(Y_{P}\) in Fig. 2a. This might seem like an odd experiment: we do not actually observe \(X\), and \(X\) already causes \(Y\), so how could dropping the edge from \(X\) to \(Y_{P}\) really matter? Without the \(X Y_{P}\) edge \(Y_{P}\) is independent of \(U\) conditional on \(Y\), which significantly tightens the bounds for FPR and PPV and fully identifies FNR.2 Once again, we derive the algebraic bounds and find that the computational bounds match. This final experiment simultaneously demonstrates potential drawbacks of causal sensitivity analysis and the enormous benefit of being able to easily run analyses under varying assumptions. Sensitivity analysis can be dangerously misleading if we include unrealistic assumptions, giving us a false sense of security.

Figure 3: In this we directly recreate the plots from Fogliato et al.  for a predictor trained on the COMPAS dataset, allowing for some probabilistic and causal assumptions to vary. The dashed lines represent exact bounds on each statistic for increasing \(P(Y_{P} Y)\), which follow from Fogliato et al.  or our derivations in Appendix C.1.2. (a) represents the original setting, where we have \(P(Y_{P}=1 Y=0)=0\), in (b) we drop the dashed edge between \(X\) and \(Y_{P}\) in the causal graph in Fig. 2a, and finally for (c) we instead take \(P(Y_{P}=0 Y=1)=0\). As we can see, at all points we query, the automatically derived bounds recover the algebraically derived bounds.

But the remedy is straightforward: analysts should always run an assumption-lite analysis before incorporating more domain-specific information so that they can understand which assumptions are driving their results and make a decision about whether those assumptions are realistic.

### Intersection of Biases

Motivated by Section 3.4, we now consider settings exhibiting multiple simultaneous biases. In Fig. 4, we provide a sensitivity plot for proxy and selection bias simultaneously for an equalized odds predictor trained on the _Adult_ dataset . We plot the upper bound of the equalized odds value as the sensitivity parameters for both biases vary, with a maximum of 5% proxy labels and 5% of the population being unmeasured. This exercise demonstrates that the presence of multiple biases can quickly render parity evaluation meaningless, with the upper bound reaching the maximum possible value of the statistic. Whilst an upper bound inherently represents the worst-case scenario, this experiment showcases that practitioners must provide additional context-specific assumptions to imbue parity evaluations with meaning in the presence of measurement biases, especially when multiple biases are present simultaneously. We include additional results in Appendix C.2.2, which show that biases compound in unpredictable, nonlinear ways.

### Cross-Dataset Experiments

Finally, we leverage our framework to systematically explore the sensitivity of the most ubiquitous parity metrics to the measurement biases here surveyed on real benchmark datasets. We run sensitivity analyses on 14 commonly used fairness datasets from  training logistic regression, naive Bayes, and decision trees to satisfy different parity metrics. In Fig. 5, we provide the results of this experiment, plotting the average upper bound for each metric value at different levels of sensitivity. This further supports our thesis that measurement biases present a severe issue to the informativeness of parity metric evaluation, as we see that even small amounts of bias can render the original parity evaluation meaningless relative to what it seeks to measure. Secondly, we find that this fragility is not uniform across metrics. We can see that demographic parity is, unsurprisingly, more robust than more complicated outcome-dependent metrics. Equalized odds is less robust than FPR/FNR due to the fact that it is measured as the maximum of both. Finally, predictive parity metrics are less robust to biased outcomes than metrics which involve conditioning on the outcome, such as FPR/FNR and equalized odds. We present full experiment details, further analysis, and plots per dataset in Appendix E. We also include a sensitivity for the Folktables dataset , which represents one of the most popular datasets for modern FairML.

Figure 4: Combination of Proxy and Selection Bias for an equalized odds predictor on the Adult dataset.

Figure 5: Results of our cross-dataset study, in which we assess the sensitivity of multiple ML predictors trained to satisfy various parity constraints on the fairness benchmarking datasets listed in Appendix D. We can see different metrics are susceptible to bias in different ways, with notably demographic parity being more robust than more complicated, outcome-dependent metrics.

Codebase and Web Interface

We have developed both a codebase and a web interface to ensure our framework is as usable as possible. The core of both tools are our bias configs - described in App. F and our codebase documentation--which allow for portable, modular, and reproducible sensitivity analysis. Our codebase is essentially a parser for these configs along with a set of fairness metrics we have implemented. Biases and metrics are designed to allow flexibility to suit particular use cases. The codebase wraps around these biases and fairness metrics and parses them into optimization problems which can be solved to produce bounds, currently using the Autobounds backend. We also provide a website that acts as a user interface for less technical users. The website allows for configs to be loaded or exported, and every element of the config can be edited via the interface. Users can also upload datasets and analyze the sensitivity to their chosen fairness metric/bias combinations.

## Discussion & Limitations

In this work, we have described three prevalent measurement biases, argued that they are almost always present in FairML applications, and put forward a toolkit based on newly available methods in causal inference  for understanding how these biases impact parity metric evaluation. To apply this methodology, we have focused on the discrete, oblivious setting, however, causal sensitivity analysis is not limited to this domain  and therefore its usefulness to FairML should not be either. We hope that the present work will encourage the causal inference community to look to FairML as a key application area for sensitivity analysis. Finally, additional biases that are likely topical to FairML cannot be readily expressed in this framework. For example, the issue of interference, where individuals within a classified population can affect the outcomes of others, would seem to hold in many FairML applications but can be a challenge to express graphically.