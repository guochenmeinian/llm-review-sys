# SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models

Hongxin Li\({}^{*,1,2}\), Jingran Su\({}^{*,3,4}\), Yuntao Chen\({}^{}\)\({}^{3}\), Qing Li\({}^{}\)\({}^{4}\), and Zhaoxiang Zhang\({}^{}\)\({}^{1,2,3,5}\)

\({}^{1}\)School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS)

\({}^{2}\)State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences

\({}^{3}\)Center for Artificial Intelligence and Robotics, HKISI, CAS

\({}^{4}\)The Hong Kong Polytechnic University

\({}^{5}\)Shanghai Artificial Intelligence Laboratory

###### Abstract

Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page: https://sheetcopilot.github.io/.

## 1 Introduction

The ability to intuitively direct sophisticated software through natural language has long been an aspiration pursued across generations. With the emergence of Large Language Models (LLMs) that can comprehend and respond to human directives, this vision is within closer reach now than ever. LLMs augmented with tools  and reasoning abilities  have shown promising results in recent research.

While LLMs continue advancing rapidly, their ability to interoperate seamlessly with existing software tools remains under-explored and not fully understood. Enabling LLMs to harness the rich functionality of countless existing software tools could unlock nearly limitless potential .

Progress in endowing LLMs with the ability to direct complex software systems has been hindered by the lack of both a standardized framework for model-application interaction and a comprehensive benchmark for evaluating their performance. Several challenges exist in designing a framework to facilitate interaction between LLMs and sophisticated software applications, including 1) Translating the complex internal state and vast functionality of applications into text forms comprehensible formodels . This requires determining how to systematically represent software interfaces and logic through natural language; 2) Enabling models to generate software commands and parameters accurately and safely [9; 2]. Mechanisms must exist for validating, debugging, and as needed, rejecting or revising model outputs to avoid undesirable operations or states; 3) Providing models with means of monitoring software state changes, exceptions, and errors during multi-step tasks so that these models can respond appropriately. Models are required to understand software feedback and diagnose issues, and adjust directives as needed to robustly accomplish goals. In addition, enabling LLMs to direct complex software also requires curating datasets that capture the diversity and ambiguity of real-world language use, as well as developing automated techniques to reliably evaluate model performance at scale .

To systematically investigate the substantial challenges in developing natural language interfaces for software control, a robust application platform is required; as the most pervasive and multi-functional productivity tool, the spreadsheet serves as an ideal substrate for this work. To this end, we propose a general framework for facilitating interaction between language models (LMs) and software applications, along with an agent called **SheetCopilot**. As shown in Fig. 1 SheetCopilot understands high-level spreadsheet manipulation requests expressed in natural language. It decomposes these complex requests into step-by-step plans, and issues commands to automatically carry out the necessary operations using the spreadsheet application. In addition to our spreadsheet-manipulating agent, SheetCopilot, we propose a dataset consisting of complex, interactive spreadsheet manipulation requests expressed through natural language and an evaluation framework with automated metrics to assess how accurately models comprehend requests, devise optimal plans, and perform operations through the spreadsheet interface. We believe robust measurement is key to accelerating progress in this area.

Our agent SheetCopilot achieved substantial capabilities for guiding spreadsheet software through natural language. It generated fully executable command sequences for 87.3% of problems in our benchmark suite and produced completely correct solutions for over 44.3% of tasks--surpassing the traditional programming approaches by a wide margin. To rigorously assess model performance, we curated a dataset of 221 representative spreadsheet tasks collected from superuser.com, including verified solutions created by the authors for each task.

We present three primary contributions to the goal of achieving sophisticated interfaces between language models and traditional software applications:

* We proposed a general framework for facilitating model-software interaction along with SheetCopilot, an agent specialized for spreadsheets that translates high-level, task-oriented requests expressed in natural language into executable command sequences.
* We developed comprehensive resources for systematically evaluating model and interface performance, including a benchmark suite of interactive spreadsheet tasks reflecting real-world requests and a fully automated pipeline for measuring how accurately models comprehend complex prompts, devise optimal plans and execute operations through the software interface.
* We conducted an in-depth assessment benchmarking the abilities of leading LLMs in this challenging domain. The experiments show that LLMs equipped with our method significantly outperform the strong code generation baseline.

## 2 Related Works

**Tool-augmented Large Language Models** Recently, the impressive performance of LLMs has sparked significant interest in exploring the task-planning capabilities of LLMs in various fields. Benefitting from the internalized knowledge about the world , a number of works have managed to enable LLMs to solve tasks by following instructions. One line of research [14; 3; 16; 15; 8] has utilized prompt engineering to elicit a sequence of mid-level steps from LLMs for household robotic tasks. To ground LLMs in the real world, these works have used auxiliary models [3; 16; 15] or trained LLMs via mixing visual-language data and embodied data . Another promising direction is to connect LLMs with external tools , such as a web browser , HuggingFace model hub , chemical software , PowerPoint , and even a tool library [25; 24]. These works employ LLMs to generate action sequences which are further parsed into API calls of the tools. Compared with these works, our work is targeted at spreadsheet manipulation, which is a common demand in almost all scenarios (e.g. economics, management, and research). To the best of our knowledge, limited research has been conducted on benchmarking the capability of LLMs for spreadsheet manipulation.

**Natural Language Processing (NLP) for Spreadsheets** Several studies [12; 11; 28; 6; 17] have investigated the feasibility of using NLP methods to guide the manipulation of Excel sheets. An early work is Flash Fill , which automates string processing tasks using program synthesis by example. NLyze  utilizes a translation algorithm to convert a user's natural language instruction to a ranked set of likely programs. Inspired by the success of Codex  and AlphaCode , one recent study  has explored the use of LLMs for generating Excel formulas given natural language descriptions of the desired output. They compared the performance of several state-of-the-art LLMs, including GPT-3 and T5, and found that these models can generate accurate formulas with high efficiency. However, this study focused on formula generation rather than general sheet control tasks. In this paper, we aim to address this gap by benchmarking the capability of LLMs for sheet control tasks.

## 3 Dataset and Evaluation

Prior research on language interfaces for spreadsheet control [12; 6; 17] has focused primarily on limited subsets of tasks like formula generation and lacked comprehensive, standardized means of evaluation. To address this issue, we aim to construct a high-quality evaluation benchmark as a foundation for assessing the spreadsheet control capabilities of LLM-based agents.

Our dataset compilation procedure incorporates gathering tasks and worksheets from the Internet, filtering low-quality or irrelevant tasks, consolidating redundant entries, adapting seed tasks, and manually annotating a core set. The end product is a comprehensive and cleanly-labeled collection of spreadsheet-related tasks. We also report statistics and analysis to characterize the dataset properties,

Figure 1: We maneuver SheetCopilot to control software such as Microsoft Excel, generate step-by-step solutions fulfilling the userâ€™s requirements. In each step, SheetCopilot plans an initial atomic action according to the sheet state and then revises this step using the external document which provides the action usage and examples. Finally, the action with its arguments is extracted from the revised step and submitted to the simulation environment for execution. The entire process on the right shows that SheetCopilot successfully solves the task specified in the instruction using the provided available atomic actions.

guide future work, and set initial baselines. Moreover, we develop an automated, reproducible evaluation framework closely tailored to our curated natural language spreadsheet control dataset. This enables systematically assessing model abilities, gaining insights into current limitations, and driving continued progress in this domain.

### Diverse Seed Task and Workbench Collection

We first scrape all questions with spreadsheet-related tags on www.superuser.com and obtain a raw dataset comprising \(\)16k question and answer (Q&A) pairs. Sourcing questions from SuperUser ensures our task dataset is both comprehensive and representative. As not every question represents a sheet manipulation task we apply keyword-based and LLM-based filters to remove Q&A pairs unrelated to spreadsheet automation, resulting in a remain of \(\)13k pairs. To analyze the distribution of the dataset, we define six task categories: Entry and Manipulation, Management, Formatting, Charts, Pivot Tables, and Formulas. We label exemplar Q&A pairs with at least one category to prompt the language model to categorize each pair, as pairs may belong to multiple categories. To identify representative Q&A pairs, we embed and cluster pairs within each unique category combination. We then choose 67 pairs representing the clustering centers and involving operations supported by our evaluation environment. The spreadsheet tasks described in these pairs are regarded as the seed tasks which capture the most important patterns of our dataset.

To evaluate LLMs, we also collect 28 real-world spreadsheets as our workbench by 1) downloading practical sheets from the Internet, and 2) Generating typical daily-use sheets by hand. These sheets represent common uses such as analyzing sales data, calculating financial metrics, and visualizing data with charts.

### Core Set Collection

The seed tasks cannot be directly used since their original sheets differ from the evaluation sheets. We propose collecting a core dataset by adapting and simplifying the seed tasks to bridge this gap.

**Adaptation**. Inspired by self-instruct , we prompt an LLM to adapt the seed tasks according to the detailed descriptions of the evaluation sheets. Specifically, GPT-4 is prompted to change the manipulated elements in the seed tasks to generate new tasks compatible with the evaluation sheets. For instance, GPT-4 can change the data types required to be set or ranges to be modified in the original seed task. In this step, 1669 new task instructions are generated (See Tab. D for examples).

**Simplification**. The adaptations are likely to mention specific formulas and operations. To address this issue, we prompt an LLM to simplify each task by replacing specific expressions with natural spoken language so that the task instruction reads like the fashion of a non-expert user. This step reduces the average token length from \(47.1\) to \(33.8\)1.

Figure 2: Dataset overview. We present an overview of the core set by showing the wordclouds of the instructions and involved atomic actions. The two clouds show that the core set contains diverse tasks that involve various spreadsheet operations.

To collect a core set, the authors select several tasks for each category combination from the simplified tasks. The authors also act as non-expert users to compose more tasks to enrich the core set, obtaining 221 tasks in total. Finally, multiple reference solutions are collected as the ground truth for each task. See Fig. 2 for the dataset statistics and more in the appendix.

### Task Evaluation by Execution

It is hard to evaluate solutions generated by LLMs through verbatim comparison, as it is likely that multiple solutions can successfully complete a task. A viable approach is assessing whether the final sheet state after executing the solution meets the task instruction. We only assess the necessary properties required for the ground truth spreadsheet's operation. For example, in the task "Plot a line chart with the X-axis showing the week and the Y-axis showing sales", we only consider properties related to the chart, ignoring other aspects. To assess an LLM-generated solution, we evaluate the consistency of the necessary properties between the spreadsheet resulting from executing this solution and the ground truth spreadsheet in our evaluation environment. If the necessary properties of the resulting spreadsheet fully match any potential ground truth candidate, the associated solution is deemed correct.

## 4 Method

SheetCopilot enables natural language interactions with spreadsheets. It takes spreadsheets and user tasks described in natural language as input and generates plans to modify spreadsheet contents and create diagrams or tables. We adopt an in-context learning framework inspired by models such as GPT-3 . We propose "atomic actions" - a set of virtual APIs representing common spreadsheet functions. These actions allow language models to accurately interact with the spreadsheet software. We also propose a state machine-based task planning framework to handle the complex, multi-turn interaction between the language models and the spreadsheets. The atomic actions and state machine-based planning framework enable language models to effectively and robustly direct spreadsheet software through natural language.

### Prompting LMs as a SheetCopilot

We design a systematic prompt template to turn LMs into copilots as shown in the left of Fig. 1. Our prompt consists of a general role description, a list of atomic actions with arguments, a set of output requirements, and a multi-round interaction example between a user and an assistant.

The general role description serves as an anchor for enabling LMs to understand the context. A list of atomic actions provides LMs with the interface information needed for task planning. The output requirement tells LMs how to generate texts that can be programmatically extracted and executed. The multi-round example hints LMs how the observe-propose-revise-act loop appears and improves the overall planning quality.

### Atomic Action as A Bridge for LMs and Software

State-of-the-art LMs have shown the superb ability to generate detailed plans for household tasks , software control , and debugging . However, the generated plans are in natural language which is easy for humans to read but not directly admissible for machine execution.

To overcome the limitation mentioned above, we propose to model the functionalities of existing spreadsheet software as a set of virtual APIs called atomic actions. An atomic action is comprised of an API name, a typed argument list, a usage document string, and several usage examples. These atomic actions can be implemented on different spreadsheet platforms. The example implementations in Tab. H of the appendix show that the atomic actions involve cell value modification, formatting, sheet management, formula and functions, charts, and pivot tables.

Choosing proper atomic action granularity is crucial, as actions must be expressive yet concise to fit in the LM context windows. We determine our atomic actions as follows: 1) Extract all actions involved in the top SuperUser spreadsheet Q&As; 2) Embed and cluster the extracted actions into candidates; 3) Select the minimum set of actions covering all the tasks we collected in Sec. 3.1.

**Relation to Agents Generating VBA Codes** LMs are also capable of generating machine-readable codes . This approach is especially tempting for Microsoft Excel as it comes with an embedded script language called Visual Basic for Applications(VBA). However, the code generation approach faces challenges from both the LMs side and the spreadsheet software side. On the code LMs side, the existing training corpus [10; 13; 5] for code LMs hardly contains VBA source files as it is only a niche programming language compared with C++ or Python. On the spreadsheet software side, software such as Google Sheets, Libre Office, and WPS either do not support VBA at all (Google Sheets) or only support a limited subset of VBA functions (Libre Office and WPS). Therefore, we advocate a more software-agnostic approach that does not rely on embedded programming language support.

### State Machine-based Task Planning

A normal spreadsheet task usually involves several steps, while a sophisticated one often requires over ten steps. Open-loop planning - directly generating a complete task plan from the instruction - becomes exponentially harder as steps increase. Each step changes the sheet state so the correct step \(T+1\) relies on perfectly understanding how the sheet state changes after the previous \(T\) steps. As tasks become more complex, open-loop planning struggles.

We propose a state machine-based planner which revises the plan according to feedback from either LMs or software. Our planner is divided into observing, proposing, revising, and acting stages. The state transition between these stages will be described below. Due to the page limit, please refer to the appendix for examples of complete planning logs.

**Observing Stage** In this stage, we add a brief description of the sheet state \(S_{t}\) to the query, providing information such as the name of each column and the total number of rows for LMs to determine atomic action arguments. This allows LMs to generate solutions in a closed-loop manner by observing the previous actions' consequences without implicitly modeling sheet states.

**Proposing Stage** In this stage, we concatenate the system prompt \(P\), the initial task instruction \(I\), the sheet state \(S_{t}\) and the planning history \(H_{t}\) and ask the LMs to plan the next atomic action \(A_{t+1}\).

\[A_{t+1}=(R_{t+1})=((P,I,S _{t},H_{t})).\] (1)

The direct response \(R_{t+1}\) from the language model is not always convertible to an admissible atomic action \(A_{t+1}\). Common errors found in the validating step include missing the format requirement, hallucinating undefined actions, and incorrectly determining action parameters.

**Revising Stage** Two ways are adopted to revise a proposed atomic action: a feedback-based one and a retrieval-based one. Feedback-based revision utilizes the error feedback from both the atomic action validation and the spreadsheet software execution. For example, if the atomic action validating step detects a hallucinated atomic action, a new prompt will be created to inform the LM of this error and to ask it to reiterate the available atomic actions. Additionally, we use retrieval-based revision to supply the LM with detailed external knowledge that does not fit in the system prompt due to the context window limit. For example, if the LM uses an atomic action with wrong arguments, a detailed document containing the argument descriptions and usage examples of this action is provided in the new prompt to enhance the probability of the LM correctly determining the atomic action arguments. This process resembles how a human programmer behaves when encountering less familiar APIs.

A special case in the revision stage is that after being supplied with more information about the initially proposed atomic action, the LM suddenly finds that it has chosen a wrong action and decides to return to the revising stage.

**Acting Stage** After the proposing and revising stages, the atomic action \(A_{t+1}\) is submitted to the spreadsheet software for execution.

\[S_{t+1}=(A_{t+1},S_{t}).\] (2)

The planning history \(H_{t}\) is updated if the execution succeeds,

\[H_{t+1}=H_{t}\{A_{t+1},S_{t+1}\}.\] (3)

If the software reports a run-time error, the state machine will return to the proposing stage to prompt the LM to re-plan according to the error feedback.

### Hallucination Mitigation

To enable the state machine to less frequently return to the proposing stage due to hallucination-induced errors, we adopt the following means.

**Output Formatting** The underlying functions of atomic actions require precisely formatted planning results. However, we found that LMs probably generate semantically correct yet inadmissible action plans as shown in Fig. 1. Therefore, we require LMs to wrap actions with special tokens (e.g. @) and detect the tokens in the output to check whether the output is correctly formatted.

**Atomic Action Disambiguation** The internalized knowledge in LMs is likely to be confused with the atomic action definitions in the document. Due to this conflict, LMs are prone to self-delusion, which means that it hallucinates undefined actions or adds illegal action arguments . To tackle this problem, the atomic action names are substituted with a set of synonyms that are far away from the official names in an embedding space. For instance, Write and SetConditionalFormat are substituted with RangeInputValue and FormatWithRules, respectively (See the details in the appendix).

## 5 Experiments

The goals of our experiments are threefold: (i) compare representative LLMs on the proposed dataset; (ii) demonstrate that the proposed method improves the success rate and efficiency over a simple baseline; (iii) show the flexibility and stability of our method.

### Benchmark Protocol

**Dataset** The 221 tasks introduced in Sec. 3.2 are used to conduct the following experiments.

**Metrics** Exec@1 measures the proportion of solutions executed without throwing exceptions. Pass@1 is used to evaluate functional correctness . A generated plan is considered correct if the final sheet state completely fulfills the task requirements. Beyond correctness, we propose A50 and A90 scores to measure solution efficiency. These divide the number of atomic actions in a generated plan by the number in the ground truth and then calculate the 50th and 90th percentiles over all tasks. Lower A50 and A90 scores indicate that the LLM tends to use fewer actions to complete a task.

**Models** We adopt leading large language models with public API access, including GPT-3.5-Turbo/GPT-4 from OpenAI and Claude v1 from Anthropic. Details of the models and hyper-arguments used for generation could be found in the appendix.

### Comparing Task Planning Ability of Different LLMs

We compare the three LLMs on the proposed dataset with the same token limit of 4096. For less accessible LLM APIs like GPT-4 and Claude, only 10% of the dataset is used for evaluation. We have maintained the diversity of this subset to avoid data distribution shift (see the appendix for details). The results in Tab. 1 show that GPT-4 demonstrates its strong planning capability by significantly outperforming both GPT-3.5-Turbo and Claude in the Pass@1 and A50/A90. To explain why GPT-4 is inferior in Exec@1, we check the results and find that it is mainly because GPT-4 exceeds the

   Data & Models & Exec@1\(\) & Pass@1\(\) & A50\(\) & A90\(\) \\ 
10\% & GPT-3.5-Turbo & **85.0\%** & 45.0\% & 2.00 & 4.50 \\
10\% & GPT-4 & 65.0\% & **55.0\%** & **1.33** & **2.00** \\
10\% & Claude & 80.0\% & 40.0\% & 1.50 & 4.40 \\ 
100\% & GPT-3.5-Turbo & 87.3\% & 44.3\% & 1.50 & 3.00 \\ 
100\% & VBA & 77.8\% & 16.3\% & - & - \\   

Table 1: Performances of the compared LLMs and a VBA-based method. The three LLMs exhibit impressive Exec@1 and Pass@1, with GPT-3.5-Turbo achieving the highest Exec@1 and GPT-4 obtaining the best Pass@1 and efficiency. With our method, GPT-3.5-Turbo outperforms the method that generates and runs VBA code by a large margin.

token limit when solving difficult tasks although it has generated correct mid-steps. In contrast, GPT-3.5-Turbo and Claude generate short but incorrect plans for most of these difficult tasks. Claude is slightly worse than GPT-3.5-Turbo in Exec@1 and Pass@1 but exhibits better A50/A90, which shows that Claude is a strong competitor of GPT-3.5-Turbo. See more detailed failure analysis in Sec. F.

To evaluate the category-specific performances, We further break down the subset into the six categories (defined in Sec. 3.1). The four metrics in each category are illustrated in Fig. 3. The radar charts demonstrate that the two GPT models both achieve 100% Exec@1 and Pass@1 in the Management and Entry & manipulation categories. Interestingly, The three models exhibit different patterns of A50/A90: GPT-3.5-Turbo, GPT-4, and Claude reach their best efficiency in the Formula, Management, and Pivot Table category, respectively. This suggests that it is difficult for these models to excel in all task categories.

### Ablation Studies of State Machine

We conduct ablation studies for GPT-3.5-Turbo on the full dataset to analyze the impact of the two types of feedback and external document insertion. The results are shown in Tab. 2.

**A) Closed-loop control generally boosts functional correctness** Individually adding the sheet state feedback at the proposing stage increases the Exec@1 and Pass@1 (rows 3 vs. row 2) since the model no longer needs to implicitly infer the sheet state. Individually adding error feedback obtains the highest Exec@1 (row 4 vs. row 2) as a longer context window can be used to re-plan without the sheet state feedback, increasing the probability of completely finishing a plan. The combination of the two feedback types further improves Pass@1 but at the cost of slightly increasing A50 and A90 (row 7 vs. row 2), probably because the model solves more difficult tasks but with plenty of steps. It is noticeable that without the external document and usage examples, the improvement in Exec@1 becomes narrow and Pass@1 even drops slightly (row 5 vs. row 1). This is because the

   No. & State feedback & Error feedback & External doc. & Usage examples & Exec@1\(\) & Pass@1\(\) & A50\(\) & A90\(\) \\ 
1 & & & & & & 56.6\% & 18.6\% & 1.50 & 4.00 \\
2 & & & & âœ“ & âœ“ & 67.9\% & 18.6\% & 1.00 & 2.50 \\
3 & âœ“ & & & âœ“ & âœ“ & 75.6\% & 24.4\% & 1.50 & 3.00 \\
4 & & âœ“ & âœ“ & âœ“ & âœ“ & 92.8\% & 23.5\% & 1.00 & 2.00 \\ 
5 & âœ“ & âœ“ & & & & 68.3\% & 18.1\% & 1.50 & 3.78 \\
6 & âœ“ & âœ“ & âœ“ & âœ“ & & 85.5\% & 28.1\% & 1.50 & 3.00 \\
7 & âœ“ & âœ“ & âœ“ & âœ“ & âœ“ & 87.3\% & 44.3\% & 1.50 & 3.00 \\   

Table 2: Ablation studies of the observe-propose-revise-act framework proposed in Sec. 4.3. The sheet state and error feedback increase the Exec@1 and Pass@1 when individually applied (rows 3, 4 vs. 2) and bring a significant improvement when both are applied (row 7 vs. 2). Inserting the external atomic action document with usage examples boosts the Exec@1 and Pass@1 and increases efficiency (row 2 vs. 1 and row 7 vs. 5). The synergy of the four components witnesses a large increase (30.7%) in Exec@1 over the baseline (row 7 vs. 1).

Figure 3: The four metrics decomposed in the six categories. The two GPT models both achieve 100% Exec@1 and Pass@1 in the Management and Entry & manipulation categories. The three models obtain their own best efficiency in different categories, suggesting that it is difficult for these models to excel in all task categories.

simple action list in the initial prompt (shown in Tab. 1) fails to provide sufficient information about how to determine correct action arguments even if a detailed sheet description is provided.

**B) Inserting the external document improves both functional correctness and efficiency** Merely adding the external atomic action document enjoys clear improvements of **17.2%** in Exec@1 and **10.0%** in Pass@1 (row 6 vs. row 5). This result demonstrates that presenting this document to the model enables it to less frequently hallucinate illegal actions (which improves Exec@1) and to more accurately determine the action arguments according to the sheet state (which improves Pass@1). Further adding usage examples in the document reaches the highest performance (row 7). Additionally, without any feedback, the improvements over the baseline become narrower (row 2 vs. row 1) since it is hard to determine correct action arguments without knowing the exact properties of the spreadsheet elements even if more details of atomic actions are provided. Adding the external document and usage examples reduces A50/A90 (row 2 vs. row 1 and rows 6, 7 vs. row 5), showing that more information about atomic action usage helps to avoid redundant steps.

**C) SheetCopilot surpasses the VBA-based method** To further demonstrate the advantages of our method, we compare it with a method that generates and runs VBA code. Tab. 1 shows that SheetCopilot with GPT-3.5-Turbo as its backend outperforms the VBA-based method by a large margin, increasing the Exec@1 by **9.5%** and Pass@1 by **28.0%**. This result shows that prompting powerful LLMs with our method to control spreadsheets is a better alternative to directly translating natural language requests to VBA code.

### Ablation Study of Atomic Action Names

To inspect the potential confusion problem stated in Sec. 4.4, we conduct another ablation study for GPT-3.5-Turbo on the full dataset by comparing the impact of adopting the official names and the synonyms. The results in Tab. 3 surprisingly show that using the synonyms increases the Pass@1 and obtains lower A50/A90, which means that the model learns to use the synonyms to generate more correct solutions with fewer steps. This result demonstrates the flexibility of our method: it is possible for users to define their own atomic actions and prompt LLMs to use them.

   Models & Exec@1\(\) & Pass@1\(\) & A50\(\) & A90\(\) \\  Official names & 87.3\% & 44.3\% & 1.50 & 3.00 \\ Synonyms & 86.9\% & 45.2\% & 1.33 & 2.79 \\   

Table 3: Ablation study of the atomic action names. Utilizing the synonyms far away from the official names brings an increase in Pass@1 and slightly better efficiency (lower A50).

Figure 4: Stability experiments results obtained by conducting evaluation 3 times for each temperature except 0.0. The line charts show that SheetCopilot achieves stable performances even if the GPT-3.5 API temperature changes from 0.0 to 1.0.

### The Influence of LLM Temperature on Task Plan Generation

We evaluate the stability of our method by running the full method three times with temperatures from 0.0 to 1.0. The results in Fig. 4 show that the metrics are stable with slight deviations from the values for temperature=0.0.

### Atomic Action at Different Granularity

A natural question is how to determine the granularity of atomic actions, i.e. the number of workbook elements an action manipulates.

To investigate this question, two experiments are conducted: 1) The actions that set chart properties are incorporated into the CreateChart action, and the original separate actions are removed, with the expectation that SheetCopilot will set chart properties when creating charts. 2) In another experiment, SetFormat, which is originally used to set multiple format properties, is split into finer-grained format-setting actions. Please refer to Sec. D.2 for details.

We conduct these two experiments with GPT-3.5-Turbo backend on the chart and format-related tasks. The results in Tab. 4 show that using an integrated CreateChart action to handle chart creation and property setting simultaneously obtains slightly higher efficiency (lower A50 and A90). However, this variant encounters significantly inferior Exec@1 and Pass@1. In contrast, splitting the original SetFormat action witnesses considerable gains in Exec@1 and Pass@1.

After analyzing the results, we found that an integrated CreateChart encounters lower functional correctness as its complex document makes it difficult for SheetCopilot to understand the action usage, thus being less able to correctly determine action arguments. In addition, the lengthy documentation of this integrated action frequently exceeds GPT-3.5's token limit. In contrast, we observed that after splitting SetFormat, the LLM can easily understand the simple finer-grained actions, thereby encountering fewer hallucination cases. See Sec. D.2 for detailed analysis.

These results suggest that it is more desirable to use finer-grained atomic actions instead of integrated high-level actions in terms of functional correctness.

## 6 Conclusion

We propose SheetCopilot, a spreadsheet agent based on the observe-propose-revise-act framework that decomposes a high-level task into step-by-step solutions for software control. To evaluate our agent, we curate a realistic and diverse dataset representing the typical demand of non-expert spreadsheet users. The experimental results show that SheetCopilot can perform spreadsheet manipulation tasks with a high pass rate and acceptable efficiency, outperforming VBA-based methods. The ablation studies show that the closed-loop control and external document insertion used by SheetCopilot bring clear improvements over the baseline and that adopting a set of atomic action names dissimilar to the official names achieves a surprising performance gain. We also find that utilizing finer-grained atomic actions instead of integrated high-level actions can notably improve functional correctness. We hope our work provides a useful roadmap for researchers interested in the field of LLM-based autonomous agents and sheds light on future research.

   Experiment & Method & Exec@1\(\) & Pass@1\(\) & A50\(\) & A90\(\) \\   & Ours full & **91.7\%** & **43.3\%** & 1.25 & 1.67 \\  & Ours + Integrated CreateChart & 79.1\% & 37.2\% & **1.00** & **1.50** \\   & Ours full & 70.7\% & 9.8\% & 2.75 & 6.65 \\  & Ours + Split SetFormat & **80.5\%** & **12.2\%** & **2.00** & **5.60** \\   

Table 4: Ablation study of atomic action granularity on the chart-related and format-related tasks. The results show that using an integrated CreateChart action achieves slightly lower A50 and A90 but encounters significantly inferior Exec@1 and Pass@1. Additionally, splitting the SetFormat into finer-grained format-setting actions leads to higher Exec@1 and Pass@1.