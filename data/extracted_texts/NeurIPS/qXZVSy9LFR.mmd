# Emotion-LLaMA: Multimodal Emotion Recognition

and Reasoning with Instruction Tuning

 Zebang Cheng\({}^{12}\) Zhi-Qi Cheng\({}^{3}\) Jun-Yan He\({}^{4}\) Jingdong Sun\({}^{3}\)

**Kai Wang\({}^{5}\) Yuxiang Lin\({}^{1}\) Zheng Lian\({}^{6}\) Xiaojiang Peng\({}^{12}\) Alexander G. Hauptmann\({}^{3}\)**

\({}^{1}\)Shenzhen Technology University \({}^{2}\)Shenzhen University \({}^{3}\)Carnegie Mellon University \({}^{4}\)Alibaba Group

\({}^{5}\)National University of Singapore \({}^{6}\)Institute of Automation, Chinese Academy of Sciences

**Project: https://zebangcheng.github.io/Emotion-LLaMA**

**Demo: https://huggingface.co/spaces/ZebangCheng/Emotion-LLaMA**

Equal contribution, listed in random order, completed during Zebang Cheng's internship at CMU.Corresponding author (zhiqic@cs.cmu.edu, pengxiaojiang@sztu.edu.cn).

###### Abstract

Accurate emotion perception is crucial for various applications, including human-computer interaction, education, and counseling. However, traditional single-modality approaches often fail to capture the complexity of real-world emotional expressions, which are inherently multimodal. Moreover, existing Multimodal Large Language Models (MLLMs) face challenges in integrating audio and recognizing subtle facial micro-expressions. To address this, we introduce the _MERR dataset_, containing 28,618 coarse-grained and 4,487 fine-grained annotated samples across diverse emotional categories. This dataset enables models to learn from varied scenarios and generalize to real-world applications. Furthermore, we propose _Emotion-LLaMA_, a model that seamlessly integrates audio, visual, and textual inputs through emotion-specific encoders. By aligning features into a shared space and employing a modified LLaMA model with _instruction tuning_, Emotion-LLaMA significantly enhances both emotional recognition and reasoning capabilities. Extensive evaluations show Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap (7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations on DFEW dataset.

## 1 Introduction

Emotion perception plays a vital role in applications such as human-computer interaction , educational assistance , and psychological counseling . While single-modality approaches, including _facial expression recognition_, _text emotion analysis_, and _audio emotion recognition_, have shown effectiveness, real-world emotional data is often multimodal, integrating text, audio, and images.

Despite extensive multimodal fusion methods having achieved promising improvements , they mainly focus on feature interaction and modality completion, remaining under-explored for knowledge-level interaction which is essential for emotional reasoning of humans. Recently, _Multimodal Large Language Models (MLLMs)_ have excelled in tasks such as visual-language understanding , visual question answering , and video understanding . However, for emotion recognition , models like GPT-4 with Vision (GPT-4V) still face two main challenges: the inability to process audio and the failure to recognize micro-expressions.

We argue that the lack of specialized multimodal emotion instruction datasets is the main factor limiting MLLMs' effectiveness. These issues stem from the inability of previous methods to effectively integrate audio inputs, which are crucial for capturing vocal tones and auditory cues, and their difficulty in recognizing subtle facial micro-expressions. These limitations lead to sub-optimal performance in real-world scenarios.

To address these challenges, we introduce the _MERR dataset_ (Sec. 3.1), which enables multimodal large models and supports _instruction tuning_ to learn from diverse scenarios and generalize to real-world applications. We also propose the _Emotion-LLaMA model_ (Sec. 3.2), which integrates audio, visual, and textual inputs through emotion-specific encoders. By employing _instruction tuning_ (Sec. 3.3), _Emotion-LLaMA_ significantly enhances both the accuracy of emotional recognition and the depth of emotional reasoning, setting a new benchmark for multimodal emotion analysis. Extensive experiments and evaluations (Sec. 4) demonstrate Emotion-LLaMA's superiority, achieving top scores on EMER, MER20231, MER20242, and DFEW datasets. Our main contributions are as follows:

* We constructed the _MERR dataset_, which includes 28,618 coarse-grained and 4,487 fine-grained annotated samples, covering a wide range of emotional categories such as "doub" and "contempt". Unlike previous datasets, MERR's diverse emotional contexts allow models to learn from varied scenarios and generalize to real-world applications, serving as a valuable resource for advancing large-scale multimodal emotion model training and evaluation.
* We developed the _Emotion-LLaMA model_, which incorporates HuBERT for audio processing and multiview visual encoders (MAE, VideoMAE, EVA) for capturing facial details, dynamics, and context. By aligning these features into a modified LLaMA language model, _Emotion-LLaMA_ enhances emotional recognition and reasoning capabilities.
* Extensive experiments demonstrate that _Emotion-LLaMA_ significantly outperforms other MLLMs across multiple datasets, establishing it as the current state-of-the-art model in public competitions. It achieved top scores on the EMER dataset (Clue Overlap: 7.83, Label Overlap: 6.25) and attained F1 scores of 0.9036 on MER2023-SEMI1 and 0.8452 on MER2024-NOISE2. _Emotion-LLaMA_ also surpassed ChatGPT-4V in zero-shot evaluations, including DFEW (+4.37%) and MER2024-OV2 (+8.52%).

## 2 Related Work

To highlight our contributions, we review existing multimodal large language models and instruction tuning methods, emphasizing their limitations in emotional understanding.

**Multimodal Large Language Models (MLLMs).** MLLMs  have gained substantial attention due to their powerful inferential capabilities. Research primarily focuses on leveraging pretrained models like CLIP , Q-Former , and ImageBind  for general domain applications . However, even advanced models like GPT-4V  struggle with understanding audio emotional cues and recognizing facial micro-expressions due to the lack of specialized training on multimodal emotional datasets and emotion-related knowledge. Recently, researchers have begun training MLLMs on multimodal emotional datasets to identify emotion-triggering utterances in dialogues , though these studies often lack detailed explanations. _In contrast, our proposed Emotion-LLaMA employs emotion-specific encoders to extract multimodal features, enhancing emotional recognition and reasoning capabilities._

**Instruction Tuning.** Language instructions have been widely used across diverse NLP tasks . Studies like InstructionGPT , FLAN , and OPT-IML  have explored instruction-tuning methods  that significantly enhance the zero-shot and few-shot capabilities of LLMs. The vision field has also embraced language instructions for various vision-language tasks . LLAVA  converted image-text pairs into instruction-following data using a language-only model, while EmoVIT  generated visual emotion instruction data using paired annotations. However, these approaches often lack audio information, which is crucial for understanding human emotions. Due to high annotation costs, AffectGPT  manually annotated only 100 samples with emotion clues. _To address the scarcity of emotion-related instruction-following data, our approach generates multimodal descriptions using prior knowledge._Methodology

This section presents our proposed Emotion-LLaMA model, which consists of three key components: the MERR dataset construction (Sec. 3.1), the Multimodal Emotion-LLaMA model architecture (Sec. 3.2), and the training procedures (Sec. 3.3).

### MERR Dataset Construction

The MERR dataset is constructed through a comprehensive process of emotion annotation in video data, as outlined in Algorithm 1 and Figure 1. First, human faces are extracted from each video frame using the OpenFace toolkit, which detects and scores Action Units (AUs)  to identify the frame with the maximum cumulative intensity:

\[_{peak}=_{k}_{i}S_{au_{i}}^{k},\] (1)

where \(S_{au_{i}}\) represents the intensity of each AU. These AUs are mapped to facial expression descriptions \(C_{ved}\) (Tables 10 and 11) to accurately depict facial movements. Next, MiniGPT-v23 analyzes the peak frame to extract contextual information \(C_{vod}\), such as activities and environment (Figure 1), facilitating the identification of latent emotional elements within the background context. Qwen-Audio4 processes audio segments to extract nuances in speech and vocal tone, generating emotion-related descriptions \(C_{atd}\). Visual and audio information are concatenated into a raw multimodal description, integrating sensory inputs to enhance the contextual supplementation for lexical subtitles. Lexical subtitles \(C_{ls}\) are integrated into the multimodal description, providing textual context that complements the audio and visual data. LLaMA-35 refines these annotations by aggregating unimodal descriptions (\(C_{ved}\), \(C_{vod}\), \(C_{atd}\), \(C_{ls}\)) into a detailed multimodal description \(C_{md}\), following instructions and examples in Table 12. Finally, the comprehensive description \(C_{md}\) is used to annotate the peak frame, ensuring the video is annotated with detailed emotional descriptors.

The MERR dataset extends the range of emotional categories and annotations beyond those found in existing datasets (Table 15). Each sample is annotated with an emotion label and described in terms of its emotional expression. The dataset was initially auto-annotated with coarse-grained labels for 28,618 samples from a large pool of unannotated data using LLaMA-3, and was later refined to include 4,487 samples with fine-grained annotations, carefully selected by experts. Figure 5 shows that, compared to other datasets, MERR encompasses a wider range of emotional categories. More details of MERR dataset construction are provided in the project homepage and the Appendix A.

### Multimodal Emotion-LLaMA Model

The proposed Multimodal Emotion Large Language Model (Emotion-LLaMA) architecture, depicted in Figure 2, comprises an audio encoder \(^{aud}\), a visual encoder \(^{vis}\), and a multimodal large language

Figure 1: Example of the MERR dataset: It includes audio tone description, lexical subtitle, visual objective description, visual expression description, classification label, and multimodal description.

model \(\). Given an input tuple \(P=,,\), Emotion-LLaMA is formulated as:

\[ =(,,,P)\] (2) \[=(^{aud}(),^{vis}(( )),^{tex}())\]

where \(\), \(\), and \(\) denote the LLaMA language model , vision pre-processor, and multimodal encoder, respectively. \(\) represents the formatted output text result. The multimodal encoder \(\) consists of audio, vision, and text prompt encoders. Input \(Video\) is pre-processed to construct the frame sequence \(V\) and \(Frame_{}\) (Sec. 3.1).

**Multimodal Prompt Template.** To address the intricate needs of emotional understanding, we craft a structured multimodal prompt template incorporating descriptive captions and emotion flags (as detailed in Table 16 and 17), directing the LLM to decipher latent correlations between emotional states and corresponding visual or auditory content. The template is denoted as:

_[INST] \(<\)VideoFeature\(>\)\(<\)AudioFeature\(>\) [Task Identifier] Prompt [INST]_

**Multiview Multimodal Encoder**. To capture emotional cues in audio and visual modalities, we leverage the HuBERT  model as our audio encoder \(^{aud}\) and a multiview visual encoder \(^{vis}\). HuBERT extracts a comprehensive auditory representation \(u^{aud}\) from the input audio signal, exhibiting remarkable performance in emotion recognition tasks.

We use a vision preprocessor to unify vision modalities, including facial sequences and peak frame extracted from the input video. Three visual encoders \(^{vis}=^{vis}_{glo},^{vis}_{loc},^{ vis}_{temp}\) are employed to comprehensively extract complementary multi-view visual emotional features:

* _Local Encoder_: A ViT-structured model pre-trained by the MAE scheme  extracts static facial expression features. A facial sequence is fed into the local encoder, and the output frame-wise features are fused by average pooling, producing the local visual feature \(u^{vis}_{loc}=(^{vis}_{loc}(V))\).
* _Temporal Encoder_: A VideoMAE  model, produces the temporal feature \(u^{vis}_{temp}=^{vis}_{temp}(V)\) of a facial sequence, learning facial dynamics that indicate emotional states and offering a temporal dynamic view of human emotion.
* _Global Encoder_: A ViT-structured model, EVA , initialized with official pre-trained weights, produces the visual feature \(u^{vis}_{glo}=^{vis}_{glo}(_{peak})\), capturing not only facial expressions but also background context.

**Multimodal Integration and Tokenization.** We use the LLaMA tokenizer, employing a byte-pair encoding (BPE) model based on SentencePiece , to address open vocabulary challenges and facilitate efficient processing of textual inputs. For multimodal emotional reasoning, a modified generate method iteratively selects the most probable tokens, producing contextually appropriate and emotionally nuanced responses.

To integrate audio and visual features with text tokens, we introduce a linear projection mechanism that transforms these features into a common dimensional space. This involves trainable linear mappings \(\), which include \(^{aud}\) for the audio token, and \(^{vis}_{glo}\), \(^{vis}_{loc}\), and \(^{vis}_{temp}\) for the visual tokens. Specifically, we apply \(\) to convert multimodal feature \(u\) into language embedding tokens \(\):

\[= u,\ u=u^{aud},u^{vis}_{glo},u^{ vis}_{loc},u^{vis}_{temp}\] (3)

The resulting multimodal tokens \(\) comprise a single audio token \(^{aud}\), three visual tokens \(^{vis}_{glo}\), \(^{vis}_{loc}\), and \(^{vis}_{temp}\), and a sequence of text tokens \(^{tex}_{0},,^{tex}_{N}\). These tokens are fused through the inner cross-attention mechanism of Emotion-LLaMA, enabling it to capture and reason about the emotional content in the multimodal input.

By employing this linear projection and multimodal token representation, Emotion-LLaMA processes and integrates information from various modalities, leveraging the strengths of the underlying LLaMA model while incorporating essential emotional cues from audio and visual sources. Further details of the Emotion-LLaMA Model are provided in the code repository.

### Training of Emotion-LLaMA Model

We design a multi-task learning scheme to simultaneously supervise the model in learning emotional reasoning and recognition. The ground truth output and labels are converted and concatenated as standard text by a formatted template for autoregressive loss calculation . Iterative random instruction sampling (see Table 16 and 17 for full list of instructions) for emotional reasoning and recognition tasks during training guides the model to develop a comprehensive understanding of emotions. Typically, Emotion-LLaMA is trained in a coarse-to-fine manner, consisting of the _Pre-training_ and _Multimodal Instruction Tuning_:

**Stage 1: Pretraining.** Initially, the model is trained on 28,618 coarse-grained samples from the MERR dataset. Distinct tasks help the model grasp emotions from multiple perspectives. This phase involves simple descriptions or classifications, facilitating the rapid alignment of multimodal feature tokens (\(^{aud}\), \(^{vis}_{glo}\), \(^{vis}_{loc}\), and \(^{vis}_{temp}\)) to the word embedding space [10; 97].

**Stage 2: Multimodal Instruction Tuning.** The pretrained Emotion-LLaMA model is then refined using fine-grained instructional datasets to enhance its capacity for emotion recognition and reasoning. This stage utilizes multimodal instruction tuning datasets, incorporating 4,487 fine-grained annotated descriptions for comprehensive reasoning from the MERR dataset. The tuning process is extended to diverse sources, including MER2023  and DFEW , which feature precisely annotated emotional categories. This phase ensures that the model not only identifies emotions accurately but also understands the underlying context and reasoning behind each emotion. More details are in the code repository and the Appendix B.

Figure 2: Architecture of Emotion-LLaMA, which integrates audio, visual, and text inputs for multimodal emotional recognition and reasoning.

Experiments

### Experimental Setup

To verify the effectiveness of Emotion-LLaMA, we conducted extensive evaluations across four different datasets: MER2023 , MER2024 , DFEW , and EMER . Notably, we utilized the MERR dataset for pre-training the model and then fine-tuned it on target datasets for evaluation.

**Emotion Recognition Evaluation**. We performed instruction tuning on the MER2023 and DFEW datasets, allowing the model to integrate the emotional knowledge acquired during pretraining. To test the generalizability of our model, we used three datasets: MER2023, MER2024, and DFEW. These datasets are multimodal emotion recognition datasets composed of movie and TV series clips, each annotated with various emotion categories. For fair comparisons, we evaluated Emotion-LLaMA on MER2023-SEMI and MER2024-NOISE using the F1 score. We also compared it with other MLLMs and state-of-the-art (SOTA) methods using unweighted average recall (UAR) and weighted average recall (WAR) on the DFEW dataset. Additionally, we used the average of accuracy and recall scores as evaluation metrics on the MER2024-OV dataset.

**Emotion Reasoning Evaluation**. The EMER dataset differs from traditional emotion datasets by including emotion trigger labels, such as facial micro-expressions, tone of speech, and video context information, in addition to emotion categories. To assess the emotional reasoning capabilities of different MLLMs on the EMER dataset, we employ ChatGPT to score their predictions, focusing on three key aspects: (1) the degree of overlap between emotion-related clues, (2) the degree of overlap between summarized emotional states, and (3) the completeness of the reasoning process across modalities. This multi-faceted evaluation provides a rigorous and in-depth assessment of the models' ability to understand and explain emotions in a multimodal context.

### Implementation Details

For the global visual encoder, we employ the EVA model with full images sized at 448\(\)448 pixels as input. For the local and temporal visual encoders, we first crop and align the faces within the images, then hierarchical sample 16 facial images as inputs for the MAE and VideoMAE models. The audio is handled by the HuBERT-Chinese large model. The extracted emotional features are transformed into a 4096-dimensional space via linear layers before being concatenated with text tokens.

During the tuning process, we froze the visual and audio backbones, focusing on training the linear projection layer. For the language model (LLM), we utilize LLaMA2-chat (7B) equipped with LoRA for parameter-efficient tuning. Following the Minigpt-v2 approach, we fine-tune the query and value projection matrices (\(_{q}\) and \(_{v}\)) by setting \(r=64\) and \(=16\). Consequently, the trainable parameters of Emotion-LLaMA totaled only 34 million, representing a mere 0.495% of the overall parameter count. We train on 4*A100 GPUs for 300,000 steps, which takes around 20 hours. Detailed information can be found on the project homepage and in the code repository.

### Comparison with State-of-the-Art Methods

To comprehensively evaluate the performance of Emotion-LLaMA, we compared it with several state-of-the-art (SOTA) methods across different datasets.

**Multimodal Emotion Reasoning Results.** We compared Emotion-LLaMA with contemporary MLLMs such as Video-LLaMA, Video-ChatGPT, PandaGPT, VideoChat, and Valley, presenting the results in Table 1. VideoChat demonstrates that aligning visual data directly with textual embedding space (VideoChat-Embed) significantly outperforms converting it into textual format (VideoChat-Text), supporting our method of mapping audio and visual features to textual embedding space. Notably, other MLLMs that accept audio inputs, like PandaGPT and Video-LLaMA, show no standout performance, suggesting inefficiencies in

  
**Models** & **Clue Overlap** & **Label Overlap** \\  VideoChat-Text  & 6.42 & 3.94 \\ Video-LLaMA  & 6.64 & 4.89 \\ Video-ChatGPT  & 6.95 & 5.74 \\ PandaGPT  & 7.14 & 5.51 \\ VideoChat-Embed  & 7.15 & 5.65 \\ Valley  & 7.24 & 5.77 \\
**Emotion-LLaMA (ours)** & **7.83** & **6.25** \\   

Table 1: Comparison of multimodal emotion reasoning results on the EMER dataset. Clue Overlap and Label Overlap scores range from 0 to 10.

extracting rich emotional content from audio. Emotion-LLaMA excels beyond these models across both Clue Overlap and Label Overlap evaluation metrics, highlighting our model's unparalleled ability to extract direct emotional features and engage in logical emotional reasoning. The scoring criteria and cases are presented in the Appendix C.1.

**Multimodal Emotion Recognition Results.** Table 2 presents the comparison results on the DFEW dataset. In the zero-shot scenario, Emotion-LLaMA demonstrates superior capabilities compared to all other MLLMs, showcasing its strong generalization ability. Notably, the majority of MLLMs scored zero in the disgust category, with GPT-4V achieving only 10.34%. This may be attributed to safety constraints on the term "disgust" within large language models, indicating a need for further exploration. Additionally, different MLLMs tend to favor predicting a specific emotion category, resulting in higher scores for those categories but lower recall scores overall. In contrast, Emotion-LLaMA maintains a more balanced prediction across all categories, ultimately achieving the highest WAR score of 59.37%. After fine-tuning, Emotion-LLaMA achieves the highest Unweighted Average Recall (UAR) and Weighted Average Recall (WAR) scores, further indicating its exceptional performance in emotion recognition tasks. These results highlight the effectiveness of our model in adapting to new datasets and accurately identifying emotions across various modalities. Overall, the results of Emotion-LLaMA's performance highlight the effectiveness of our approach in accurately recognizing emotions from multimodal data.

### Multimodal Emotion Recognition Challenge

To further validate the effectiveness of our proposed Emotion-LLaMA model, we conducted experiments on the MER2023 and MER2024 Challenge, comparing it with previous state-of-the-art methods.The results, presented in Table 3, demonstrate that our model, which maps audio and visual features to the textual space, achieves the highest F1 score across various modalities. This approach significantly enhances the context of the textual modality by providing a more comprehensive understanding of the information, thereby outperforming other models. By integrating audio, visual, and textual data, Emotion-LLaMA can better capture the nuances of emotional expression, leading to more accurate and reliable emotion recognition.

The MER2024 Challenge introduced a new Open-Vocabulary Multimodal Emotion Recognition (MER-OV) task. Unlike traditional tasks, MER-OV focuses on recognizing any number of labels across diverse categories, aiming for a more nuanced and precise description of emotional states. As shown in Table 4, Emotion-LLaMA outperforms other mainstream multimodal models, yielding an 8.52% improvement in average accuracy and recall compared to GPT-4V, and achieving the highest zero-shot score among all participating large multimodal models. These results showcase the robustness and versatility of our approach in handling complex multimodal data for emotion recognition tasks, making it a promising solution for real-world applications.

   Method & Hap & Sad & Neu & Ang & Sur & Dis & Fea & UAR & WAR \\ 
**Zero-Shot** & & & & & & & & & & \\ Qwen-Audio  & 25.97 & 12.93 & 67.04 & 29.20 & 6.12 & 0.00 & 35.36 & 25.23 & 31.74 \\ LLaVA-NEXT  & 57.46 & **79.42** & 38.95 & 0.00 & 0.00 & 0.00 & 0.00 & 25.12 & 33.75 \\ MiniGPT-v2  & **84.25** & 47.23 & 22.28 & 20.69 & 2.04 & 0.00 & 0.55 & 25.29 & 34.47 \\ Video-LLaVA(image)  & 37.09 & 27.18 & 26.97 & 58.85 & 12.97 & 0.00 & 3.31 & 20.78 & 31.10 \\ Video-LLaVA(video)  & 51.94 & 39.84 & 29.78 & 58.85 & 0.00 & 0.00 & 2.76 & 26.17 & 35.24 \\ Video-Llama  & 20.25 & 67.55 & **80.15** & 5.29 & 4.76 & 0.00 & 9.39 & 26.77 & 35.75 \\ GPT-4V  & 62.35 & 70.45 & 56.18 & 50.69 & 32.19 & 10.34 & **51.11** & **47.69** & 54.85 \\ Emotion-LLaMA(ours) & 71.98 & 76.25 & 61.99 & **71.95** & **33.67** & 0.00 & 3.31 & 45.59 & **59.37** \\ 
**Fine-tuning** & & & & & & & & & \\ EC-STFI  & 79.18 & 49.05 & 57.85 & 60.98 & 46.15 & 2.76 & 21.51 & 45.35 & 56.51 \\ Former-DFER  & 84.05 & 62.57 & 67.52 & 70.03 & 56.43 & 3.45 & 31.78 & 53.69 & 65.70 \\ IAL  & 87.95 & 67.21 & 70.10 & 76.06 & 62.22 & 0.00 & 26.44 & 55.71 & 69.24 \\ MAE-DFER  & 92.92 & 77.46 & 74.56 & 76.94 & 60.99 & **18.62** & 42.35 & 63.41 & 74.43 \\ VideoMAE  & 93.09 & 78.78 & 71.75 & 78.74 & 63.44 & 17.93 & 41.46 & 63.60 & 74.60 \\ S2D  & **93.62** & **80.25** & **77.14** & 81.09 & 64.53 & 1.38 & 34.71 & 61.82 & 76.03 \\ Emotion-LLaMA(ours) & 93.05 & 79.42 & 72.47 & **84.14** & **72.79** & 3.45 & **44.20** & **64.21** & **77.06** \\   

Table 2: Comparison of multimodal emotion recognition results on DFEW. The upper part shows zero-shot performance, while the lower part shows results after fine-tuning.

### Qualitative Analysis of Emotion Reasoning

To illustrate the qualitative performance of Emotion-LLaMA, we present a detailed comparison of emotion reasoning results across different models. Table 5 displays the emotion reasoning results of the four highest-scoring models. The video shows a person smiling while questioning another individual, an expression of dissatisfaction that suggests an angry emotional state. Accurate emotion reasoning for this sample necessitates integrating information from multiple modalities. PandaGPT and Valley captured the correct visual features but failed to incorporate information from other modalities, incorrectly classifying the emotion as happy. In contrast, VideoChat-Embed eventually reached the correct inference, but its reasoning was compromised by hallucinations. Emotion-LLaMA went a step further by recognizing the tone of the person and combining subtle facial expressions with multimodal information for accurate emotion reasoning. This example demonstrates the superiority of our model in understanding and integrating emotional cues from various modalities, resulting in more precise and contextually relevant emotion recognition.

  
**Model** & }\)} & \(}\) &  \\  Empty & 0.00 & 0.00 & 0.00 \\ Random & 13.42 & 24.85 & 19.13 \\ Ground Truth & 93.37 & 52.51 & 72.94 \\  Valley  & 20.16 & 13.26 & 16.71 \\ Outer  & 29.64 & 23.04 & 26.34 \\ PandaGPT  & 35.75 & 31.57 & 33.66 \\ Video-LLaMA  & 31.08 & 32.26 & 31.67 \\ VideoChat  & 43.17 & 44.92 & 44.05 \\ VideoChat2  & 46.91 & 34.78 & 40.85 \\ Video-ChatGPT  & 46.20 & 39.33 & 42.77 \\ SALMONN  & 42.20 & 44.75 & 43.47 \\ Qwen-Audio  & 55.12 & 32.91 & 44.02 \\ mPLUG-Owl  & 44.80 & 46.54 & 45.67 \\ AffectGPT  & 66.14 & 46.56 & 56.35 \\ GPT-4V  & 56.19 & 58.97 & 57.58 \\ Emotion-LLaMA & **69.61** & **62.59** & **66.10** \\   

Table 4: Performance (%) of Multimodal Large Language Models on MER2024 Challenge track 3: MER-OV. The “avg” column represents the average of “\(_{}\)” and “\(_{}\)”

  
**Method** & **Modality** & **F1 Score** \\  wav2vec 2.0  & A & 0.4028 \\ VGG3h  & A & 0.5481 \\ HuBERT  & A & 0.8511 \\ ResNet  & V & 0.4132 \\ MAE  & V & 0.5547 \\ VideoMAE  & V & 0.6068 \\ RoBERTA  & T & 0.4061 \\ BERT  & T & 0.4360 \\ MacBERT  & T & 0.4632 \\ MER2023-Baseline  & A, V & 0.8675 \\ MER2023-Baseline  & A, V, T & 0.8640 \\ Transformer  & A, V, T & 0.8853 \\ FBP  & A, V, T & 0.8855 \\ VAT  & A, V & 0.8911 \\ Emotion-LLaMA & A, V & 0.8905 \\ Emotion-LLaMA & A, V, T & **0.9036** \\   

Table 3: Comparison of multimodal emotion recognition results on MER2023. The table shows the performance of different models across various modalities, with the highest F1 scores achieved by our proposed method.

  
**An Example of Multimodal Emotion Reasoning** \\  Subtitle (En) & Are you really stupid or just pretending? Do I care about spending this money? It’s just a joke. \\  Ground Truth & In the video, although she had a smile on her face, she questioned the other person, “Are you really stupid or just pretending?” expressing her dissatisfaction and anger towards the other person’s act of pretending to be foolish. Therefore, it can be inferred that his emotional state is angry. \\  PandaGPT & The emotional state of this person appears to be positive and relaxed. They are smiling and laughing, indicating that they find the situation amusing or enjoyable. The woman’s reaction to the man’s joke suggests she is not offended or angry but finds it many or amusing. This means that the interaction between the two individuals is friendly, not tense or uncomfortable. \\  VideoChat-Embed & Based on the video, the Lady appears to be quite upset and angry. She discusses her financial losses and how her boyfriend seems indifferent to her feelings. She expresses a desire to spend money. The lady’s facial expressions and gestures, such as frowing and clutching her purse tightly... \\  Valley & From the video, we can infer that the woman might be in a neutral or mixed emotional state. In some scenes, she is smiling or making amusing faces, suggesting she feels happy or amused... \\  Emotion-LLaMA & In the video, the woman’s facial expressions, including a furroduced bow, lifted eyebrows, and a slightly lowered lip, suggest a mix of emotions. Her tone and intonation convey a sense of frustration, and her words reveal that she is questioning whether the other person is silly, indicating a sense of annoyance. Therefore, it is inferred that her emotional state is anger.

  
**Method** & **Modality** & **F1 Score** \\  wav2vec 2.0  & A & 0.4028 \\ VGG3h  & A & 0.5481 \\ HuBERT  & A & 0.8511 \\ ResNet  & V & 0.4132 \\ MAE  & V & 0.5547 \\ VideoMAE  & V & 0.6068 \\ RoBERTA  & T & 0.4061 \\ BERT  & T & 0.4360 \\ MacBERT  & T & 0.4632 \\ MER2023-Baseline  & A, V & 0.8675 \\ MER2023-Baseline  & A, V, T & 0.8640 \\ Transformer  & A, V, T & 0.8853 \\ FBP  & A, V, T & 0.8855 \\ VAT  & A, V & 0.8911 \\ Emotion-LLaMA & A, V & 0.8905 \\ Emotion-LLaMA & A, V, T & **0.9036** \\   

Table 5: An example of multimodal emotion reasoning comparing Emotion-LLaMA with other MLLMs. Incorrect reasoning is marked in red, correct reasoning in blue, and hallucinations in gray.

Figure 3 displays the recognition results of Emotion-LLaMA in comparison to other models. The left samples show that even when characters exhibit subtle emotional fluctuations lacking distinct emotional features, our Emotion-LLaMA model accurately discerns the true intentions behind the text. The right samples demonstrate that, unlike other language models, Emotion-LLaMA can extract multimodal emotional features to enhance the text, thereby accurately understanding the true emotions conveyed by the text. These qualitative results further illustrate the effectiveness of our model in capturing and interpreting emotional nuances across different modalities, leading to a more comprehensive and accurate understanding of human emotions.

### Ablation Evaluation

We conducted a series of ablation experiments to explore the effectiveness of each component of the proposed Emotion-LLaMA. More ablation experiments, which examine factors affecting instruction-tuning performance, including data quantity and quality as well as hyperparameters, are presented in Appendix C.

**Investigation of Encoders**. We explore various combinations of encoders. As shown in Table 6, the combinations listed in the upper part are fused by the part of the modalities, while the rest capture all the modalities. The best combination of the audio and multiview visual encoders is the HuBERT+MAE+VideoMAE+EVA, which obtained a 0.891 F1 score. Two observations are worth noting: 1) multiple modalities, including audio, static, and dynamic vision, are compensated for in the emotion capturing process; 2) the spatial, spatial-context, and temporal information are fully considered by the multiview visual module, which exhibits a significant improvement over single-view approaches. These findings highlight the importance of incorporating diverse modalities and considering multiple aspects of visual information for accurate emotion recognition.

  
**Audio Encoder** & **Visual Encoder** & **F1 Score** \\  Wav2Vec & - & 0.4893 \\ VGGish & - & 0.5944 \\ whisper & - & 0.5324 \\ HuBERT & - & 0.8394 \\ - & MAE & 0.6366 \\ - & VideoMAE & 0.6762 \\ - & EVA & 0.6635 \\ - & MAE, VideoMAE, EVA & 0.7122 \\  HuBERT & MAE & 0.8800 \\ HuBERT & VideoMAE & 0.8805 \\ HuBERT & EVA & 0.8757 \\ HuBERT & MAE, VideoMAE & 0.8880 \\ HuBERT & MAE, EVA & 0.8896 \\ HuBERT & VideoMAE, EVA & 0.8802 \\ HuBERT & MAE, VideoMAE, EVA & **0.8910** \\   

Table 6: Ablation Study Results for Different Encoders.

Figure 3: Visualization of the output probability distribution for multimodal emotion recognition by different models. Each sample is represented by two bar graphs: the left graph displays the results from other models, and the right graph shows the results from Emotion-LLaMA.

In Table 7, we present the impact of different instruction data on the instruction-tuning of Emotion-LLaMA. 'Raw' refers to the direct concatenation of visual and audio descriptions as instructions for training Emotion-LLaMA, which yielded the poorest performance. When trained using the coarse-grained set from the MERR dataset, Emotion-LLaMA achieved scores of 7.41 and 5.56 for clue and label overlap, respectively. This marks an improvement of 1.87 and 1.25 over the 'Raw' approach, demonstrating that coarse-grained annotations generated by the LLaMA-3 model effectively integrate emotional cues to capture genuine emotional expressions. Notably, further instruction-tuning using the fine-grained set from the MERR dataset resulted in additional gains of 0.42 and 0.69 in clue and label overlap, respectively, indicating that fine-grained annotations offer higher quality data and further enhance the performance of instruction-tuning.

To better understand the effect of the sample selection strategy, we compared our strategy against traditional semi-supervised approaches, as shown in Table 8. Due to the limited size of the MER2023 training dataset, which contains only 3,373 samples, pre-training on this dataset can lead to difficulties in fitting models with transformer structures, resulting in a low F1 score of 0.7977. We then compared traditional semi-supervised approaches, which involve assigning pseudo-labels to all unlabeled samples or selectively giving pseudo-labels to those samples that exhibit high softmax scores during inference. There are 73,148 and 36,490 samples selected by these two strategies, respectively. During the pre-training phase, a substantial increase in data volume can significantly enhance performance, even introducing considerable noise. Ultimately, the model tuning on our automatically annotated MERR dataset achieves the best model performance. This demonstrates the effectiveness and robustness of the proposed Emotion-LLaMA in leveraging large-scale, diverse datasets for improved emotion recognition and reasoning.

## 5 Ethics and Conclusion

**Ethics.** All datasets used in this study are governed by signed usage agreements, strictly restricting their use to academic research. The MERR dataset is derived from MER2023 and includes over 70,000 unannotated samples from diverse movies and TV series. We have obtained the necessary End User License Agreements (EULA) and explicit permissions from the original data providers. The open-source MERR dataset contains only emotion description JSON files, intentionally excluding source videos. Researchers must apply directly to the original providers and fully comply with the EULA to access the dataset. We ensure that the MERR dataset exclusively comprises multimodal emotion descriptions without any discriminatory or biased content.

**Conclusion.** We introduced _Emotion-LLaMA_, a novel multimodal large language model designed to accurately recognize and interpret human emotions in real-world scenarios. Utilizing robust open-source tools, we automated the selection and annotation of the Multimodal Emotion Recognition and Reasoning (MERR) dataset for pre-training purposes. Instruction-tuning on comprehensive datasets such as MER2023 and DFEW enabled us to achieve state-of-the-art performance metrics. Comparative analyses with other advanced multimodal large language models (MLLMs) demonstrated Emotion-LLaMA's superior generalization capabilities in emotion recognition and reasoning tasks.

**Acknowledgments.** This work was partially supported by the National Natural Science Foundation of China (Grant No. 62176165), Stable Support Projects for Shenzhen Higher Education Institutions (Grant No. 20220718110918001), and the Natural Science Foundation of Top Talent at SZTU (Grant No. GDRC202131). Additional support was provided by NSF CISE (Grant No. 1937998), the Air Force Research Laboratory (Grant No. FA8750-19-2-0200), U.S. Department of Commerce (Grant No. 60NANB17D156), IARPA (Grant No. D17PC00340), and DARPA (Grant No. HR00111990063). The research findings and conclusions presented in this paper are solely those of the authors and do not necessarily represent the views of the funding agencies.

  
**Stage** & **Clue Overlap** & **Label Overlap** \\  Raw & 5.54 & 4.31 \\ Coarse-grained & 7.41 & 5.56 \\ Fine-grained & 7.83 & 6.25 \\   

Table 7: Ablation Study Results for Different Stages of the MERR Dataset Instructions.

  
**Strategy** & **F1 Score** \\  Standard Training & 0.7917 \\ High-Confidence Training & 0.8831 \\ Pseudo-Label Training & 0.8950 \\ Instruction Tuning & **0.9036** \\   

Table 8: Ablation Study Results for Different Training Strategies.