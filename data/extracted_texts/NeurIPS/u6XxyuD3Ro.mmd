# Online Convex Optimisation:

The Optimal Switching Regret for all Segmentations Simultaneously

 Stephen Pasteris

The Alan Turing Institute

London UK

spasteris@turing.ac.uk Chris Hicks

The Alan Turing Institute

London UK

c.hicks@turing.ac.uk Vasilios Mavroudis

The Alan Turing Institute

London UK

vmavroudis@turing.ac.uk Mark Herbster

University College London

London UK

m.herbster@cs.ucl.ac.uk

###### Abstract

We consider the classic problem of online convex optimisation. Whereas the notion of static regret is relevant for stationary problems, the notion of switching regret is more appropriate for non-stationary problems. A switching regret is defined relative to any segmentation of the trial sequence, and is equal to the sum of the static regrets of each segment. In this paper we show that, perhaps surprisingly, we can achieve the asymptotically optimal switching regret on every possible segmentation simultaneously. Our algorithm for doing so is very efficient: having a space and per-trial time complexity that is logarithmic in the time-horizon. Our algorithm also obtains novel bounds on its dynamic regret: being adaptive to variations in the rate of change of the comparator sequence.

## 1 Introduction

We consider the classic problem of online convex optimisation: a problem with numerous real-world applications. In this problem we have an _action_ set which is a bounded convex subset of some euclidean space. On each trial we select an action from this set and then receive a convex function of bounded gradient, which associates the action with a _loss_. The aim is to minimise the cumulative loss. The static regret is defined as the cumulative loss of the algorithm minus that of the best constant action in retrospect. It has been shown that the minimax static regret is \(()\) where \(T\) is the time horizon, and that it is achieved by the classic _mirror descent_ family of algorithms . However, in dynamic environments a more sensible notion of regret is the _switching regret_, which is defined relative to any segmentation of the trial sequence and is equal to the sum of the static regrets over all segments. Clearly, if the segmentation is known a-priori then the minimax switching regret is \((_{k}})\) where \(_{k}\) is the length of the \(k\)-th segment, and it is obtained by running mirror descent independently on each segment. _Tracking algorithms_, instead, attempt to bound the switching regret on every possible segmentation of the trial sequence simultaneously. However, as far as we are aware, the best such bound until now was \((_{k}(T)})\) which is a factor of \(()\) higher than the optimal if we knew the segmentation a-priori. In this paper we (quite remarkably) get rid of this factor: hence obtaining the asymptotically optimal switching regret of \((_{k}})\) for every possible segmentation simultaneously. Not only is our algorithm optimal, but it is also parameter-free and efficient: having both a space and per-trial time complexity of \(((T))\).

In fact, our algorithm ReSeT is a meta-algorithm which utilises any _base algorithm_ for the online convex optimisation problem at hand. Using _online gradient descent_ as our base algorithm gives us the above \((_{k}})\) switching regret bound. However, the constant under the \(\) is dependent on the action set and the possible gradients. By choosing a more appropriate base algorithm that is tailored to the specific problem we can achieve lower constant factors. In particular, when faced with the classic problem of _prediction with expert advice_ with \(N\) experts, using _Hedge_ as our base algorithm yields the asymptotically optimal \((_{k}(_{k}\,,}))\) switching regret (a novel result in itself).

We note that although, like _strongly adaptive_ algorithms , we are adaptive to heterogeneous segment lengths, we are not necessarily strongly adaptive: in that we do not bound the static regret on any particular segment.

Whilst switching regret models discrete changes in the environment, a continuously changing environment is better modeled by the notion of _dynamic regret_, which is the difference between the loss of the algorithm and that of any comparator sequence of actions. It is known that algorithms exist which bound the dynamic regret by \(()\) where \(P\) is the _path length_ of the comparator sequence. However, this bound is not adaptive to variations in the rate that the comparator sequence is changing. ReSeT, with online gradient descent as the base algorithm, rectifies this: improving the dynamic regret to \((_{k})_{k}})\) for any segmentation in which the path length in the \(k\)-th segment is \(P_{k}\). We note that this implies the \((_{k}})\) bound on switching regret. However, since we are forced to use online gradient descent as our base algorithm here, this result is not strictly more general than our switching regret result.

**Related works:**_Mirror descent_ was introduced in  to find minimisers of convex functions in convex sets. The same algorithm, however, can also be applied to online convex optimisation: the _Hedge_ algorithm of  implementing a special case when the convex set is a simplex and the convex functions are linear (the so-called _experts_ problem). The \(()\) static regret of Mirror descent was shown to be optimal in . The work  studied the non-stationary case in the experts setting: modifying Hedge to give an algorithm _Fixed share_ which takes a parameter \(\) and has a switching regret of \(()\) for any segmentation with \(\) segments. One issue with Fixed share, however, is that it does not adapt to heterogeneous segment lengths. In order to remedy this,  gave a strongly adaptive algorithm which achieved a static regret of \(((T))\) on any segment of length \(\). This was improved to \(()\) in . The work  took parameters \(a,b\) and achieved a static regret of \(()\) for any segment of length \([a,b]\). However, this still leads to a switching regret of \((_{k}(T)})\) for general segmentations. Our work finally achieves the optimal \((_{k}})\). The work  showed that gradient descent achieves a dynamic regret of \(((1+P))\), which was improved to \(()\) in . However, prior to both these works, the work  obtained the \(()\) bound subject to an optimal parameter tuning. Our work dramatically improves on this bound by being adaptive to variations in the rate of change of the comparator sequence.

**Notation:** Let \(\) be the set of natural numbers excluding \(0\). Given \(A\) we define \([A]:=\{a\,|\,a A\}\), we define \(_{A}:=\{^{A}\,|\,_{i[A]}a_{i}=1\},\) and we define \(A\) to be the set of natural numbers that are multiples of \(A\). Given a predicate \(p\) we define \([\!p]:=0\) if \(p\) is false and define \([\![p]\!]:=1\) if \(p\) is true. Given \(q,s\) with \(s q\) let \( q,s:=\{a\,|\,q a s\}\).

## 2 Problem and Results

In this section we introduce the online convex optimisation problem and state the results of this paper. In particular we define and compare the notions of switching and dynamic regret, giving the bounds obtained by our meta-algorithm ReSeT. Another common notion of regret, not necessarily bounded by ReSeT, is _strongly adaptive regret_ which we discuss in Section 3.3.

### Online Convex Optimisation

Here we describe the classic problem of _online convex optimisation_, which our meta-algorithm ReSeT solves. In this problem we have known bounded convex subsets \(,\) of some euclideanspace. We define \(\) to be the set of all convex functions that map \(\) into \(\) and whose sub-gradients lie in \(\). The problem proceeds in \(T\) trials. On each trial \(t[T]\) the following happens:

1. We choose some _action_\(_{t}\).
2. We receive some _loss function_\(_{t}\).

Our aim is to minimise the cumulative loss:

\[_{t[T]}_{t}(_{t})\,.\]

Without loss of generality we shall assume that for all \(t[T]\) and all \(\) we have \(_{t}()\). This is without loss of generality as both \(\) and the sub-gradients of \(_{t}\) are bounded and our algorithm ReSet, when using mirror descent as the base algorithm, is invariant to any constant addition to any loss function. Also, without loss of generality, assume that \(T\) is an integer power of two.

An example of online convex optimisation is _prediction with expert advice_. Here we have some \(N\) and a set of \(N\)_experts_: where on each trial each expert is associated with a loss in \(\). On each trial we must select an expert (incurring the loss associated with that expert) and then observe the vector of losses for that trial. For this problem we choose \(:=_{N}\) and \(:=^{N}\). On each trial \(t\) we draw our expert from the probability vector \(_{t}\) and define the loss function \(_{t}\) to be the linear function such that for all \(i[N]\) we have that \(_{t}(_{i})\) (that is, the loss of the \(i\)-th basis element of \(^{N}\)) is the loss associated with expert \(i\) on trial \(t\). Note that \(_{t}(_{t})\) is our expected loss on trial \(t\).

### Switching Regret

We now define the notion of _switching regret_. A _segment_ is any set of the form \( q,s\) for \(q,s[T]\) with \(s q\). The _static regret_ with respect to such a segment \(\) is defined as:

\[R():=_{^{*}}_{t}(_{t }(_{t})-_{t}(^{*}))\]

which is the total loss incurred on the segment minus that which would have been obtained by always choosing the best constant action in retrospect. A _segmentation_\(\) is defined as any partition of \([T]\) into segments. The _switching regret_ with respect to such a segmentation \(\) is defined as:

\[R^{}():=_{}R()\]

which is the sum of the static regrets on each segment of \(\). Note that \(R^{}()\) is the total loss of the algorithm minus that which would have been obtained by the best sequence of actions which is constant over each segment of \(\). The following theorem establishes a lower bound on the switching regret with respect to any fixed segmentation, even in the special case in which all the loss functions are linear:

**Theorem 2.1**.: _For any segmentation \(\) and any algorithm for the online convex optimisation problem, there exists (except in trivial cases) a sequence:_

\[_{t}\,|\,t[T]\]

_of linear functions, in which:_

\[R^{}()(_{}|})\]

_where the constant under the \(\) is dependent only on \(\) and \(\)._

Proof.: Apply the static regret lower bound of  to each segment independently. 

In this paper we develop an algorithm ReSet which has an upper-bound that matches this lower bound for every possible segmentation \(\) simultaneously. ReSet utilises any algorithm (called the _base algorithm_) for the online convex optimisation problem at hand. The base algorithm must take a parameter \([T]\) and guarantee that \(R([])()\) if it were used directly. We note that online gradient descent  is always one such possibility. Computationally, to use online gradient descent, we must be able to compute subgradients of the loss functions and euclidean projections into the set \(\). The following theorem bounds the switching regret of ReSet.

**Theorem 2.2**.: _Suppose \(\) is such that for all \([T]\), when the base algorithm is run with parameter \(\), it is guaranteed that:_

\[R([])\,.\]

_Then, for any segmentation \(\), \(\) achieves a switching regret of:_

\[R^{}()(c+d)_{}|}\]

_where:_

\[c:=/(-1)\;; d:=/(3-2)\,.\]

Proof.: See Section 4. 

Clearly, theorems 2.1 and 2.2 show that, for any fixed pair \((,)\), \(\) has the asymptotically optimal switching regret for every segmentation simultaneously. However, our result is stronger. For example, take the problem of prediction with expert advice defined above. Here the Hedge algorithm attains:

\[R([])((,))\]

which is shown to be optimal via (the proofs of) theorems 3.6 and 3.7 of  and by noting that we can always force \(()\) regret if \((N)\). Although there is a slight technicality here when segments have length less than \((N)\), the proof of Theorem 2.2 still works in exactly the same way to show that (by applying ReSet to Hedge) we have the asymptotically optimal switching regret for every value of \(N\) and every segmentation simultaneously.

\(\) is also very efficient, as shown in the following theorem.

**Theorem 2.3**.: _Given that the base algorithm runs in a time of \(\) per trial and requires a space of \(^{}\), \(\) has a per-trial time complexity of \(((T))\) and space complexity of \((^{}(T))\)._

Proof.: Immediate from the ReSet algorithm. 

### Dynamic Regret

Switching regret measures the performance of the algorithm against the best comparator sequence of actions that is constant in each segment. Dynamic regret, on the other hand, measures the performance of the algorithm against any comparator sequence. Specifically, given any sequence of actions:

\[=_{t}\,|\,t[T+1] \]

then the _dynamic regret_ with respect to \(\) is defined as:

\[R^{*}():=_{t[T]}(_{t}(_{t})-_{t}( _{t}))\,.\]

To bound the dynamic regret of \(\) we introduce the following notion of _path length_. Specifically, given the above sequence \(\) and a segment \(\), the _path length_ of \(\) in the segment \(\) is defined as:

\[P(,):=_{t}\|_{t+ 1}-_{t}\|_{2}\,.\]

The current state of the art for dynamic regret is the algorithm \(\) which achieves a dynamic regret of:

\[R^{*}()(,[T]))T})\,.\]

In this paper we significantly improve on this result, as shown in the following theorem.

**Theorem 2.4**.: _When using online gradient descent  as the base algorithm, \(\) achieves, for any comparator sequence \(\) and any segmentation \(\), a dynamic regret of:_

\[R^{*}()(_{},))||})\]

_where the constant under the \(\) is dependent only on \(\) and \(\)._Proof.: See Section 4 

Note that, for any segmentation, the dynamic regret bound of ReSeT is asymptotically equal to that of running Ader on each segment independently. To achieve this with Ader one would need to know the specific segmentation a-priori. As for the switching regret, ReSeT achieves this for every segmentation simultaneously. We note that our bound is a significant improvement on that of Ader since it is adaptive to variation in the rate of change of the comparator sequence.

We note that, given a segmentation \(\), the switching regret \(R^{}()\) is equal to the maximum dynamic regret \(R^{*}()\) across all sequences \(\) that are constant in each segment of \(\). For all \(\), the fact that such an \(\) is constant on \(\) implies that the path length \(P(,)\) is in \((1)\). This means that Theorem 2.4 implies the switching regret bound of Theorem 2.2 up to a constant factor (dependent on \(,\) and \(\)). However, to obtain Theorem 2.4 we must use online gradient descent as our base algorithm. For prediction with expert advice, for example, online gradient descent is not asymptotically optimal for every value of \(N\) simultaneously. Hence, when considering switching regret only, Theorem 2.2 is a stronger result.

## 3 The Algorithm

In this section we describe our meta-algorithm ReSeT (**R**ecursion over **S**egment **T**ree). We first introduce the notation that we will use to describe the base algorithm.

### The Base Algorithm

We now define the notation that we use to describe our base algorithm. The base algorithm utilises a data-structure \(\) (which contains the parameter) and is composed of the following three subroutines:

* Given \([T]\), the subroutine Initialise\(()\) returns the initial data-structure with parameter \(\).
* At the start of a trial, given the current data-structure \(\), the subroutine Query\(()\) returns the output action of the base algorithm for that trial.
* At the end of a trial \(t\), given the current data-structure \(\) and the loss function \(_{t}\), the subroutine Update\((,_{t})\) returns the updated data-structure (ready for the next trial).

The assumption in Theorem 2.2 implies the following. Suppose we have trials \(q,s[T]\) with \(s q\), and on each trial \(t s,q\) we do the following:

1. If \(t=s\) then \(_{t}(q-s+1)\).
2. \(_{t}(_{t})\).
3. \(_{t+1}(_{t},_{t})\).

Then we have:

\[_{^{*}}_{t=q}^{s}(_{t}( _{t})-_{t}(^{*}))\,.\] (1)

### ReSeT

We now introduce our meta-algorithm ReSeT. First let \(:=_{2}(T)\) and define the function \(:\) by:

\[(,a,b,):=)} {(-a)+(1-)(-b\,)}\,.\]

The pseudocode of ReSeT is given in Algorithm 1.

We now describe ReSeT. We have a set of \(+1\)_levels_, where each level \(i[]\{0\}\) hosts an instance of the base algorithm with parameter \(2^{i}\) and which will reset every \(2^{i}\) trials. On each trial \(t\)we denote the data-structure of the base algorithm associated with level \(i\) by \(^{i}_{t}\). Each level also has an associated number in \(\) called the _mixing weight_. On each trial \(t\), we denote the mixing weight associated with level \(i\) by \(^{i}_{t}\). We note that the mixing weight \(^{0}_{t}\) is not necessary.

We now describe the creation of the action \(_{t}\) on trial \(t\). Note first that each level \(i[]\{0\}\) has an associated action \(^{i}_{t}\) which is defined as the output of \((^{i}_{t})\), so is the action selected by the base algorithm for level \(i\) on trial \(t\). We call these actions the _base actions_. The action \(_{t}\) is created by the following recursive process. For each level \(i\) in order we construct an action \(^{i}_{t}\) called the _propagating action_. This action is constructed by a convex combination of the preceding propagating action \(^{i-1}_{t}\) and the base action \(^{i}_{t}\). Specifically, we start by setting:

\[^{0}_{t}^{0}_{t}\]

and then, for all levels \(i[]\), once \(^{i-1}_{t}\) has been constructed we set:

\[^{i}_{t}^{i}_{t}^{i}_{t}+(1-^{i}_{t})^{i-1} _{t}\,.\]

Finally, we output:

\[_{t}^{}_{t}\,.\]

We now turn to the update at the end of trial \(t\). For all levels \(i[]\{0\}\) we have the following two cases.

If \(t 2^{i}\) then we set:

\[^{i}_{t+1} 1/2;^{i}_{t+1} (\,2^{i}\,)\]

so that the mixing weight and instance of the base algorithm hosted by level \(i\) are reset. Note that the parameter of the base algorithm is \(2^{i}\). This is since it is reset every \(2^{i}\) trials.

On the other hand, if \(t 2^{i}\) then we set:

\[^{i}_{t+1}(\,^{i}_{t}\,,\,_{t}(^{i}_{t})\,,\, _{t}(^{i-1}_{t})\,,\,2^{i}\,);^{i}_{t+1} (\,^{i}_{t}\,,\,_{t}\,)\,.\]

Note that the update of the mixing weight is based on the losses of \(^{i}_{t}\) and \(^{i-1}_{t}\). If \(^{i-1}_{t}\) has a higher loss than \(^{i}_{t}\), in that the base action performs better than the lower-level propagating action, then the mixing weight increases. This means that the weight of the base action, in the convex combination forming the propagating action of level \(i\), increases. If \(^{i}_{t}\) has a higher loss than \(^{i-1}_{t}\) then the opposite happens.

Figure 1 illustrates ReSeT.

### Comparison to Strongly Adaptive Online Learner

ReSeT has some similarities to the SAOL algorithm of . Unlike ReSeT, SAOL is _strongly adaptive_, in that it bounds the static regret on any segment. Specifically, for any segment \(\), SAOL achieves:

\[R()((T)|})\,.\]

This, however, leads to a switching regret bound that is a factor \(((T))\) off the optimal. This additional factor was improved to \(()\) by .

Like ReSeT, SAOL has \(+1\) levels and utilises a base algorithm which constructs, for every trial \(t\) and level \(i\), a base action \(_{t}^{i}\) in exactly the same way as ReSeT. It then generates, for each trial \(t\), the final action \(_{t}\) as a convex combination of the base actions. We note that in ReSeT, the action \(_{t}\) is also a convex combination of the base actions, where for each level \(i[]\), the coefficient of \(_{t}^{i}\) is equal to:

\[_{t}^{i}_{j=i+1}^{}(1-_{t}^{j})\,.\]

The crucial difference between SAOL and ReSet is that, whilst SAOL updates each coefficient in the convex combination directly, the coefficients in ReSet are updated by updating each mixing weight directly. It is due to this, and the particular way that the mixing weights are updated, that ReSeT attains, unlike SAOL, the optimal switching regret.

## 4 Analysis

Here we prove Theorem 2.2. We will show how to modify this proof in order to prove Theorem 2.4 at the end of this section. All lemmas stated in this section are proved in Appendix A.

Choose any segmentation \(\). Let \(:=||\) and for all \(k[+1]\) define \(_{k}\) such that the \(k\)-th segment in \(\) is \(_{k},_{k+1}-1\). We define a comparator sequence \(}_{t}\,|\,t[T]\) as follows. For all \(k[]\) define the action:

\[}_{k}:=*{argmin}_{^{*}} _{t=_{k}}^{_{k+1}-1}_{t}(^{*})\]

and then, for all \(t_{k},_{k+1}-1\), define \(_{t}:=}_{k}\). Note that:

\[R^{}()=_{t[T]}(_{t}(_{t})-_{t}(_{t}))\,.\] (2)

In addition let \(:=2/(-1)\). With these definitions in hand we now begin the analysis.

Figure 1: ReSeT with \(8\) trials.

### Hedge

The updates for our mixing weights follow the classic algorithm Hedge. In particular, for each level \(i[]\) we maintain an instance of Hedge (which restarts every \(2^{i}\) trials) with two _experts_. On each trial \(t\), the weight of the first expert is \(_{t}^{i}\) and the weight of the second is \(1-_{t}^{i}\). The loss of the first expert is \(_{t}(_{t}^{i})\) and the loss of the second is \(_{t}(_{t}^{i-1})\). The purpose of the function \(\) is then to update the weights according to the Hedge algorithm. The following lemma is a classic result about Hedge.

**Lemma 4.1**.: _Given trials \(q,s[T]\) with \(q s\), and a sequence \((a_{t},b_{t})\,|\,t q,s\) such that for all \(t q,s\) we have \(a_{t},b_{t}\), and a sequence \(_{t}\,|\,t q,s\) defined recursively such that for all \(t q,s-1\) we have:_

\[_{q}:=1/2\;;_{t+1}:=(_{t}\,,\,a_{t}\,,\,b_{t}\,,\,s- q+1)\]

_then we have:_

\[_{t=q}^{s}(_{t}a_{t}+(1-_{t})b_{t})\{_{t=q}^{s}a _{t}\,,\,\,_{t=q}^{s}b_{t}\}+\,.\]

### The Segment Tree

In this subsection we define the _segment tree_, which is the geometrical structure that our analysis is based on. The segment tree is a full, balanced, binary tree \(\) whose leaves are the elements of \([T]\) in order from left to right. Given any internal vertex \(v\), let \((v)\) and \((v)\) be its left and right child respectively. Given any vertex \(v\), let \((v)\) and \((v)\) be its left-most and right-most descendent respectively (noting that these are both elements of \([T]\)). Let \(r\) be the root of \(\). Given a vertex \(v\{r\}\), let \((v)\) be the parent of \(v\). Given a vertex \(v\), let \(h(v)\) be equal to the height of \(v\) (that is, the height of the tree \(\) minus the depth of \(v\), so that leaves have height \(0\)).

Each vertex \(v\) represents the segment \((v),(v)\). i.e. Each vertex represents a segment in the left hand side of Figure 1 (when \(t=8\)). We call a vertex \(v\)_stationary_ iff there exists \(k[]\) with \(_{k}(v)\) and \((v)<_{k+1}\). Let \(\) be the set of all stationary vertices. We call a vertex \(v\)_fundamental_ iff both:

* \(v\,\).
* \(v=r\) or \((v)\,\).

Let \(\) be the set of all fundamental vertices. We call a vertex \(v\)_relevant_ iff it is an ancestor of a fundamental vertex. Let \(\) be the set of all relevant vertices. For all relevant vertices \(v\) we define \((v)\) to be the set of descendants of \(v\) that are contained in \(\).

We have, from the algorithm, the following lemma about the vertices of the segment tree:

**Lemma 4.2**.: _Given any vertex \(v\) we have:_

\[(v)-(v)+1=2^{h(v)}\]

_and:_

\[_{(v)}^{h(v)}=1/2\;;\;\;\;_{ (v)}^{h(v)}=(\,2^{h(v)}\,)\]

_and for all \(t[(v),(v)-1]\) we have:_

\[_{t+1}^{h(v)}=(\,_{t}^{h(v)}\,,\,_{t}(_{t}^{h( v)})\,,\,_{t}(_{t}^{h(v)-1})\,,\,2^{h(v)}\,)\;; _{t+1}^{h(v)}=(\,_{t}^{h(v)}\,,\, _{t}\,)\,.\]

Note that this lemma shows that for all \(v\) we run a single instance of both the base algorithm and Hedge over the segment \((v),(v)\), as illustrated in Figure 1.

### The Recursive Equations

We now derive the recursive equations that our analysis is based on. First note that for all \(v\) there exists \(\) such that \(_{t}=\) for all \(t(v),(v)\). Hence, Lemma 4.2 and Equation (1), and the fact that \(_{t}^{h(v)}\) is the output of \((_{t}^{h(v)})\), lead to the following lemma.

**Lemma 4.3**.: _For all \(v\) we have:_

\[_{t=}}{}}{}}{}}}(v)}_{t} (_{t}^{h(v)})_{t=}}{}}{}}{}}}(v)}_{t}(_{t})+}\,.\]

Note that, from the algorithm and the convexity of the loss functions, we have, for all \(t[T]\) and all \(v\) with \(h(v) 0\), that:

\[_{t}(_{t}^{h(v)})_{t}^{h(v)}_{t}(_{ t}^{h(v)})+(1-_{t}^{h(v)})_{t}(_{t}^{h(v)-1} )\,.\]

So, by lemmas 4.2 and 4.1, we have the following lemma.

**Lemma 4.4**.: _For all vertices \(v\) with \(h(v) 0\) we have:_

\[_{t=}}{}}{}}{}}}(v)}^{(v)}_{t}(_{t}^{h(v)}) \{_{t=}}{}}{}}}(v)}_{t}(_{t}^{h(v)})\} \{_{t=}}{}}{}}{}}}(v)}_{t}(_{t}^{h(v)}))\,_{t=}}{}}{}}{}}}(v)}}_{t}(_{t}^{h(v)})_{t=}}{}}{}}{}}}(v)}}_{t}(_{t}^{h(v)})_{t=}}{}}{}}{}}}(v)}}_{t}(_{t}^{h(v)})_{t=}}{}}{}}{}}}(v)}}_{t}(_{t}^{h(v)})+}\,.\] (4)

### Performing the Recursion

We now utilise equations (3) and (4) to perform the recursion. Specifically, we have the following inductive hypothesis for vertices in \(\), which is proved by induction up the tree \(\) from the vertices in \(\) to the root. The reason it holds for vertices in \(\) comes direct from Equation (3). For a vertex in \(\), once the inductive hypothesis has been shown to hold for both its children, it is then shown to hold for the vertex itself by Equation (4). The inductive hypothesis is given in the following lemma.

**Lemma 4.5**.: _For all \(v\) we have:_

### Dynamic Regret Analysis

We now prove Theorem 2.4. Let the base algorithm be online gradient descent as in . Take any segmentation \(^{*}\) and any comparator sequence \(\). Let \(=|^{*}|\) and for all \(j[]\) let \(_{j}\) be the \(j\)-th segment in \(^{*}\). For all \(j[]\), note that we can choose a natural number:

\[N_{j} 1+P(,_{j})\]

and a partition \(^{}_{j}\) of \(_{j}\) into \(N_{j}\) segments such that for all \(^{}_{j}\) we have:

\[P(,)(1)\] (5)

Now define the segmentation:

\[:=_{j[]}^{}_{j}\]

Note that Equation (5) implies that for all \(\) we have:

\[P(,)(1)\,.\] (6)

We now modify the analysis of the switching regret as follows. In the analysis of the switching regret we defined a comparator sequence \(_{t}\,|\,t[T]\). In this analysis we instead define this comparator sequence as equal to \(\). Using the segmentation \(\), construct the segment tree and the sets \(\) and \(\) as in the analysis of the switching regret. Since our base algorithm is gradient descent we have, direct from , the following lemma.

**Lemma 4.8**.: _Suppose we have trials \(q,s[T]\) with \(s q\), and on each trial \(t s,q\) we do the following:_

1. _If_ \(t=s\) _then_ \(_{t}(q-s+1)\) _._
2. \(_{t}(_{t})\)_._
3. \(_{t+1}(_{t},_{t})\)_._

_Then we have:_

\[_{t=q}^{s}(_{t}(_{t})-_{t}(_{t} ))((1+P(, q,s)))\,.\]

This lemma, along with Lemma 4.2 and Equation (6), gives us the following.

**Lemma 4.9**.: _For all \(v\) we have:_

\[_{t=}(v)}^{}(v)} _{t}(_{t}^{h(v)})_{t=}(v)}^{}(v)}_{t}(_{t})+(})\,.\]

This lemma is essentially identical to, and will be used instead of, Lemma 4.3. Following the rest of the analysis of the switching regret gives us:

\[R^{*}()(_{}|})=(_{j[]}_{ ^{}_{j}}|})\,.\]

Note that for all \(j[]\) we have \(|^{}_{j}|=N_{j}\) and hence:

\[_{^{}_{j}}|}_{^{}_{j}}||}=| _{j}|},_{j}))|_{j}|}\]

so we have proved Theorem 2.4.