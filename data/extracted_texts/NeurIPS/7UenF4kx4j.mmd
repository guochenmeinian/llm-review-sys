# SMART: Towards Pre-trained Missing-Aware Model

for Patient Health Status Prediction

 Zhihao Yu\({}^{1,3}\) Xu Chu\({}^{1,2,4,5}\) Yujie Jin\({}^{1,3}\) Yasha Wang\({}^{3,4,5}\) Junfeng Zhao\({}^{1,3,4,6}\)

\({}^{1}\)School of Computer Science, Peking University

\({}^{2}\)Center on Frontiers of Computing Studies, Peking University, Beijing, China

\({}^{3}\)National Research and Engineering Center of Software Engineering, Peking University

\({}^{4}\)Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China

\({}^{5}\)Peking University Information Technology Institute (Tianjin Binhai)

\({}^{6}\)Nanhu Laboratory, Jiaxing, China

yuzhihao@stu.pku.edu.cn, chu_xu@pku.edu.cn, wangyasha@pku.edu.cn

Corresponding author.

###### Abstract

Electronic health record (EHR) data has emerged as a valuable resource for analyzing patient health status. However, the prevalence of missing data in EHR poses significant challenges to existing methods, leading to spurious correlations and suboptimal predictions. While various imputation techniques have been developed to address this issue, they often obsess difficult-to-interpolate details and may introduce additional noise when making clinical predictions. To tackle this problem, we propose SMART, a Self-Supervised Missing-Aware Representation Learning approach for patient health status prediction, which encodes missing information via missing-aware temporal and variable attentions and learns to impute missing values through a novel self-supervised pre-training approach which reconstructs missing data representations in the latent space rather than in input space as usual. By adopting elaborated attentions and focusing on learning higher-order representations, SMART promotes better generalization and robustness to missing data. We validate the effectiveness of SMART through extensive experiments on six EHR tasks, demonstrating its superiority over state-of-the-art methods. Our code is available at https://github.com/yzhHoward/SMART.

## 1 Introduction

The rapid accumulation of electronic health record (EHR) data, driven by the widespread adoption of health information systems, has opened up new avenues for analyzing patient health status. EHR data in Intensive Care Units (ICUs) primarily captures patients' laboratory tests and vital signs, providing a rich resource for describing and analyzing their health conditions. These time series data have been leveraged to predict patient prognosis and physical status, attracting significant attention from both computer scientists and medical researchers. Various deep learning methods have been developed to exploit the patterns from EHR data [1; 2; 3; 4; 5; 6; 7; 8; 9], aiming to assist doctors in making informed decisions, improving work efficiency, and ultimately enhancing patient outcomes.

Recent advancements in EHR analysis can be divided into two categories. The first category focuses on improving the learning ability of time series by capturing feature correlations and exploring the collaborative relationships between variables with various techniques, such as convolutions , attentions [11; 12; 13; 14; 15], and graph neural networks [16; 17]. These methods have achieved promising results in patient health status prediction tasks, providing valuable insights into modeling the underlying patterns. However, a critical challenge is the prevalence of missing data in EHRs.

Due to the fact that patients do not undergo all tests during each visit, EHR data is often highly sparse. These missing values can compromise the integrity of learned representations and potentially mislead models to capture spurious correlations, leading to erroneous predictions of patient health status. To address this issue, the other works [18; 19; 20] attempt to interpolate missing values in the input space by capturing the dynamics and regularizing the time series. Nevertheless, these methods may struggle with the complexity of the EHR data and concentrate on difficult-to-interpolate details instead of capturing the implicit semantic information. Furthermore, while these studies have demonstrated the potential of imputation methods to fill in missing values and enhance performance on clinical tasks in their settings, these approaches face limitations in maximizing the predictive power of the available data. By masking certain observations to serve as targets for imputation, a portion of the data is withheld from the model and cannot be fully utilized for predicting patient outcomes. Imperfect imputations can introduce additional noise into the data, causing unintended shifts in the data distribution and hindering the performance of subsequent tasks .

To tackle these challenges, we propose SMART, a Self-Supervised Missing-Aware RepresenTation Learning approach for patient health status prediction. The primary objective of our approach is to enable the model to encode missing information effectively. It performs variable-independent encoding on multivariate time series inputs and utilizes Missing-Aware RepresenTation learning blocks, namely MART blocks, to capture temporal and variable interactions, with both modules incorporating missing information. Instead of deploying the vanilla self-attention mechanism , we develop novel missing-aware attentions inside the MART block to cope with sparse data. Through stacking multiple blocks, the model can adequately learn correlations while perceiving missingness.

Another key innovation of our method lies in its two-stage training strategy. Inspired by previous work [18; 23; 24; 25] on imputing missing values, we adopt a self-supervised pre-training stage to enhance the ability to cope with sparse data, after which we fine-tune the model to accomplish the EHR tasks, such as mortality prediction. In contrast to previous works that perform reconstruction in the input space, our model conducts missing data reconstruction in the latent space. By randomly removing a portion of observations and training the model to reconstruct its representations, we enable the model to focus on learning higher-order representations which contain more semantic features rather than struggling with unnecessary details. This approach promotes better generalization and robustness to missing data. After pre-training, the embedding decoder for reconstruction is replaced with a task-specific decoder for patient health status prediction during fine-tuning. To further bridge the gap between these two tasks, we introduce a learnable vector that serves as the query for the proposed attention mechanism and as the basis for patient health status prediction.

We validate the effectiveness of SMART through extensive experiments on six different EHR tasks, including in-hospital mortality, sepsis, decompensation, phenotyping, and length of stay. Our results demonstrate that SMART, with its specific designs tailored to the missing characteristic of EHR data, significantly outperforms existing methods on all the metrics, setting a new state-of-the-art on these tasks. Furthermore, we showcase the robustness of our model under settings with higher missing rates, highlighting its potential for real-world applications in healthcare. Comparisons on the model efficiency illustrate that the proposed SMART is highly efficient, with lightweight parameters and fast training time among existing methods. Our work underscores the importance of integrating missing data awareness into representation learning for enhancing patient health status prediction and paves the way for more accurate and reliable clinical decision support systems.

## 2 Related Work

**Clinical Predictive Models for EHR:** Analyzing EHR data has become an increasingly popular research topic in the medical domain [15; 26; 27]. Numerous deep learning models have been developed to mine and leverage information from EHR data [28; 29; 30; 31; 32]. Early methods used recurrent networks and attentions to capture temporal information [11; 33; 34; 35]. For learning better representation and achieving higher performance, some works attempt to learn feature correlations via sophisticated network architectures [36; 37; 38]. For example, Baytas et al.  and Gao et al.  refine the design of recurrent networks by incorporating sampling intervals and disease progression in EHR data, thereby learning more comprehensive associations. Zhang et al.  design hierarchical recurrent encoders to capture both fragmented and global temporal variance. Ma et al.  and Ma et al.  adopt attention mechanisms to enhance biomarkers that have strong connections with outcomes. Other works try to incorporate medical knowledge from EHR data or human expertise [6; 40; 41; 42]. For example, Zhang et al.  and Yu et al.  discover similar patients in the dataset and utilize their information to enhance learned representations and provide interpretations. Xu et al.  and Lu et al.  combine information from knowledge graphs for medical code data to improve predictions. Ye et al.  integrate medical knowledge graphs with patient disease progression paths to obtain better health representations. However, as discussed in the Introduction, all these methods face challenges in handling missing values. Most of them merely populate missingness with mean or front values during EHR data pre-processing, although it may be implausible and mislead the model to make wrong decisions. To address this challenge, we introduce missing-awareness mechanisms at multiple positions in SMART, enabling it to encode missingness and avoid the negative impact of missing values on representation learning.

**Imputation Models for EHR Analysis:** Imputation models are widely used in EHR analysis to handle missing values. Some works make efforts to interpolate missingness for learning better representations via clinical predictive tasks [21; 46; 47]. Specifically, Neil et al. , Che et al. , and Tan et al.  incorporate missing information into recurrent methods. Horn et al.  use a set-based approach and transform time series into sets of observations modeled by set functions insensitive to misalignment. Zhang et al.  consider sampling frequency and unify irregular time series in multiple scales. Nevertheless, these methods do not apply reconstruction losses and do not perform real imputations. The other works impute irregularly observed values to aligned reference points, notably recurrent methods , variational auto-encoders [23; 18], generative adversarial networks [24; 52], ordinary differential equations [19; 53], and probabilistic interpolation methods . However, this paradigm randomly masks existing observations and conducts both clinical tasks and interpolation simultaneously, making already sparse data even sparser when predicting health status, leading to unsatisfactory performance compared to methods that do not mask observations. Recently, a pre-training approach  has been proposed for empowering models to impute missing values through self-supervised pre-training and then perform clinical tasks, allowing the model to execute EHR analyses on intact data while possessing imputation capabilities. Nonetheless, interpolating in the input space can lead to getting bogged down in optimizing some details rather than capturing the implicit information in the entire sequence, as well as skewing the underlying data distribution . In contrast, SMART conducts missing data reconstruction in the latent space, enabling the model to focus on learning more semantic representations.

## 3 Methodology

In this section, we present the proposed SMART, a self-supervised missing-aware representation learning approach for predicting patients' health status. In Figure 1, we give an overview of the information flow and highlight our designs.

### Variable Independent Encoder

**Notation:** Let \((,,y)\) denotes the EHR data of a specific patient, where the visit sequence \(^{T N}\) contains \(T\) visits with \(N\) variables. \(_{n}^{t}\) records the value of the \(n\)-th indicator for the patient at visit \(t\), accompanied by a binary mask \(_{n}\) indicating whether the value is observed. For different patients, the visit length \(T\) can vary. To avoid the possible negative effects of varying intervals between visits, the observation intervals are aligned to hours following [13; 16; 17; 43]. Every patient corresponds to a label \(y\), indicating the diagnosis or the outcome. Our goal is to improve the model's representation learning capability to achieve better performance on \(y\).

Given a patient's EHR data, we apply a variable-independent encoding strategy to map \((,)\) into a latent representation \(^{T N d}\), inspired by the success of previous works [15; 51; 54], where \(d\) is the dimension of latent space. Although variable-dependent approaches [9; 10] capture variable interactions and compress dimensions when embedding, such methods cannot learn further associations, i.e., the attention weight of each variable . Unlike variable-independent recurrent models [13; 17; 43], we adopt linear projections to encode data, allowing the entire encoding process to be parallel and without accumulating noise from past visits. When handling variable values \(\) and masks \(\), we combine them directly and let the model capture the interactions between them.

**CLS Vector:** When encoding the EHR data, we introduce a learnable vector which is concatenated before the time series. The role of this vector is to learn the information of the whole sequence in subsequent interactions and act as the pooled hidden state used for prediction (similar to the [CLS] token in the language model such as BERT ). This design also bridges the gap between pre-training and fine-tuning tasks, as the information in the vector is used for both reconstruction and prediction. Specifically, we concatenate the CLS vector \(^{N d}\) at the temporal dimension before the hidden representation \(\), which can be formulated as \(_{v}=[,]\), where \(_{v}^{(T+1) N d}\). This process can be interpreted as concatenating a learnable parameter before the time series \(\) and a True vector before the mask \(\), making the consultation sequence one step longer (as illustrated in Figure 1). Here, we concatenate on the embedding \(\) instead of adding vectors to the input data since this procedure provides more flexibility in the learned representation space. At the end of this module, we introduce positional information to the representation \(_{v}\) for subsequent interactions in the MART blocks using sinusoidal positional encoding .

### MART Block

The MART block is the core module elaborated to learn the patient's health representation. It is mainly composed of two attention mechanisms operating on the temporal and variable dimensions. To further mitigate the impact of missing data on representation learning, we introduce masks into the attention mechanism and strengthen the attention weights of existing observations.

**Temporal Attention:** The variable-independent encoder described above embeds patient information into representation \(_{v}^{(T+1) N d}\). In this section, we introduce how we calculate temporal weights leveraging mask information. Following the convention for self-attention mechanisms, we compute the query, key, and value via linear transformations of \(_{v}\) for the temporal attention, denoted as \(_{temp}\), \(_{temp}\), and \(_{temp}\). To incorporate mask information, we construct the temporal attention bias \(^{(T+1)(T+1) N}\). The bias \(_{n}^{i,j}\) between visit \(i\) and \(j\) for the \(n\)-th variable can be computed by

\[_{n}^{i,j}=2,&_{n}^{ i}=\ \ _{n}^{ j}=,\\ 1&_{n}^{ i}\ \ _{n}^{ j}=,\\ 0&_{n}^{ i}=\ \ _{n}^{ j}= ,\] (1)

where \(_{n}^{}=[,_{n}]\) and \([,]\) denotes concatenation of matrices. Then we obtain the temporal attention weights through \(_{temp}_{temp}^{}}{}+ \), where both matrix multiplication and softmax are imposed on the temporal dimension. In this way, we strengthen information from

Figure 1: **Overview of SMART. _Left_: Given EHR data with missingness, we randomly mask them on the existing observations and conduct reconstruction in the latent space. The reconstruction targets are generated by EMA updated parameters. _Right_: We illustrate the detailed architecture of the input encoder and the MART block. The input encoder embeds each variable (which can also be referred to as a biomarker) and missing mask into a separate hidden space. The MART block employs various techniques to capture feature interactions in both the temporal and variable dimensions while further encoding missing information.**

observed visits and suppress the others. At the same time, we do not completely block the missing visits, providing an opportunity for the model to interpolate them and utilize their information.

**Variable Attention:** We capture correlations between variables via the proposed variable attention. In contrast to previous approaches which calculate variable interactions separately for each visit [51; 54], we capture variable relationships from a global perspective of the patient. Given representation \(_{temp}^{(T+1) N d}\) from the temporal attention, we get query \(_{var}\), key \(_{var}\), and value \(_{var}\) for the variable attention mechanism as follows:

\[_{var} =(_{temp}^{0}),\] \[_{var} =_{t}_{temp}^{t}\ ^{ t}=,\] (2) \[_{var} =(_{temp}).\]

The query \(_{var}\) is only obtained using the vector at the first step (\(_{temp}^{0}\)), which is also the location of learnable parameter \(\) inserted in the input encoder. This operation motivates these vectors to learn the overall health state of the patient. The key \(_{var}\) are acquired using the averaged embedding of all observed visits to minimize the effect of missingness in visits. Then we calculate the time-invariant correlations between the variables and get attention output by \(_{var}_{var}^{}}{} _{var}\).

Between these attentions, we utilize layer normalizations  and skip connections  to avoid overfitting and accelerate convergence. Besides, a feed-forward constituted by linear projections and activation functions is used following the vanilla design of Transformer . The MART block can be stacked in multiple layers to allow for sufficient interactions. The final health status of patients embedded by the MART blocks is denoted as \(^{(T+1) N d}\).

### Two-Stage Training Strategy

**Pre-training Stage:** To empower the model with missing imputation capabilities to enhance the learned representation, we propose a self-supervised pre-training method that occludes some of the observations and reconstructs their representation in the hidden space. Different from previous methods , we do not seek complex manual data augmentation, but simply remove some of the observations as the targets whose representations will be reconstructed. When generating the targets, we generate them randomly with a probability interval rather than with a fixed probability. This encourages the model to achieve better generalization across sequences with different sampling rates, rather than overfitting on missing probabilities. For the purpose of strengthening the reconstructing capability additionally and avoiding the model being trapped in the local minimum, we randomly sample the targets in each epoch of pre-training instead of using fixed data.

To emphasize a nontrivial pre-training task, we apply a self-motivated paradigm with an asymmetric architecture inspired by , as illustrated in Figure 1. Specifically, given the EHR data \((,)\), we randomly generate a mask \(}\) to remove the existing observations partially and obtain augmented data \((^{*},^{*})\). Defining the modules being trained (input encoder, MART blocks, and embedding decoder) as \(f\) and the modules (input encoder and MART blocks) used to generate labels as \(\), the reconstructions \(_{pre-train}^{*}\) are generated by \(f\) using augmented data, while the reconstruction targets \(}\) are acquired by embedding the original data via \(\) whose parameters are updated by exponential moving average (EMA)  of the parameters from \(f\). The embedding decoder consists of an MLP with activation functions that uses the patient health representation \(^{*}^{(T+1) N d}\) from augmented data as input and outputs the reconstructed health state \(_{pre-train}^{*}^{(T+1) N d}\). This paradigm provides a smooth label update curve that avoids model oscillations and underfitted embedding decoder. The pre-training loss is computed by \(L_{1}\) distance considering only the features of the removed data (i.e., the position where \(}=\)) in the latent space:

\[_{pre-train}=}\|_{pre-train}^{*}-}\|_{1}.\] (3)

Since the model is trained to reconstruct the missing data, it is forced to learn the underlying structure of the data and the temporal relationships between variables, which can be beneficial for subsequent tasks. In particular, the CLS vector is not aligned in \(_{pre-train}\) (i.e., the position of the CLS vector is False in the mask \(}\)), which allows the model to store the information of the whole sequence in the CLS vector and exploit it in the fine-tuning stage.

**Fine-tuning Stage:** After pre-training, we replace the embedding decoder with a task-specific decoder for patient health status prediction, such as classification task. While making predictions, we only use the representation at the first step (\(s^{0}\)), which is the position of the CLS vector. The label decoder is an MLP with layer normalizations and activations which can be simplified as a projection function \(^{N d}^{|y|}\). In the fine-tuning stage, the parameters are updated by the task-specific loss (e.g. cross-entropy for classification). In the first few rounds of training, we freeze the parameters of the other modules and only update the label decoder to make the pre-trained parameters to be reserved. The proposed two-stage training procedure can be formulated in Algorithm 1:

```
1:Input: EHR data \(\) and its mask \(\).
2:Output: Patient health status prediction \(\).
3:for all\(E\) in pre-training epochs do
4: Sample a mask \(}\) with a probability interval \(p\) and generate augmented data \((^{*},^{*})\)
5: Generate reconstructions \(^{*}_{pre-train} f(^{*},^{*})\) and reconstruction targets \(}(,)\)
6: Update \(f\) by minimizing \(_{pre-train}\)
7: Update \((,f)\)
8:endfor
9: Freeze the parameters of the input encoder and the MART blocks
10:for all\(E\) in fine-tuning epochs do
11:if\(E\) = unfreeze epoch then
12: Unfreeze the parameters of the input encoder and the MART blocks
13:endif
14: Update parameters by minimizing the task-specific loss with prediction
15:endfor ```

**Algorithm 1** Algorithm of SMART

## 4 Experiments

### Experimental Settings

**Datasets:** We follow previous works [10; 13; 17; 51] to compare models on three EHR datasets, Cardiology [60; 61], Sepsis , and MIMIC-III . The **Cardiology** dataset consists of 37 vital signs and biomarkers describing patients admitted to cardiac, medical, surgical, and trauma ICUs. Each record contains sparse measurements from the first 48 hours after admission. We follow the preprocessing procedures of previous works [13; 21] where the observation times are aligned to hours. After preprocessing, there are 11,988 patients and the observed rate is 24.7%. The prediction target is to determine in-hospital mortality. 13.8% of examples are in the positive class. The **Sepsis** dataset contains 34 vital signs and laboratory values relevant to sepsis. The EHR data are recorded once an hour including 40,335 patients from ICUs. The prediction task is to identify the patient's risk of developing sepsis, with a positive rate of 7.3%. For convenience, we used only the records of the first 60 visits for each patient. The observed rate is only 19.8%. The **MIMIC-III** dataset is a multivariate time series dataset consisting of 17 physiological signals after pre-processing. It contains many clinical scenarios calling for accurate predictions over diversified clinical signals. We conduct four clinical tasks on the MIMIC-III including **in-hospital mortality**, **decompensation**, **phenotyping**, and **length of stay**. Additional details on these datasets can be found in Appendix A.1.

**Evaluation Protocols:** We assess the performance on the binary classification tasks (including Cardiology, Sepsis, in-hospital mortality, and decompensation) using the area under the precision-recall curve (AUPRC) and F1 Score. AUPRC is the most informative and primary evaluation metric when dealing with a highly imbalanced and skewed dataset  such as healthcare data. We calculate F1 Score focusing on positive patients, which is more relevant in clinical scenarios. Phenotyping is a multi-label classification, each label indicating the diagnosis of a phenotype. Therefore, we examine phenotyping on the macro and micro area under the receiver operating characteristic curve (AUROC), abbreviated as ma-ROC and mi-ROC, respectively. For length-of-stay prediction, we follow Harutyunyan et al.  to separate labels into multiple bins and evaluate the ma-ROC and 

[MISSING_PAGE_FAIL:7]

on AUPRC and F1 Score compared to the best baseline, respectively. The results demonstrate the effectiveness of SMART in learning representations and predicting patient health status. AdaCare performs the worst in the baseline, which may be due to the fact that its convolutional structure only averages the proximity visits and does not perceive the missing ones. Interestingly, although StageNet and PrimeNet show competitive performance on some tasks such as Cardiology and in-hospital mortality, they perform surprisingly poorly on the Sepsis dataset. This is because the Sepsis dataset has a lower observed rate, which makes it more challenging for models to learn effective representations. Some recurrent models, including ConCare, GRASP, and PPN, exhibit robust performance across all datasets among the baselines. Nevertheless, with the capability of encoding missingness, SMART achieves remarkable performance improvements. As we observed, on the datasets with higher missing rate (Cardiology and Sepsis), SMART outperforms the other methods by a larger margin, indicating its robustness to missing values. On the phenotyping and length-of-stay prediction, our proposed model achieves the best results as well, showing its generalization ability to different clinical scenarios. The results also show that the performance of SMART is more stable than other methods, as indicated by the smaller standard deviation.

#### 4.2.2 Ablation Study

To evaluate the effectiveness of each component in SMART, we conduct an ablation study on the Cardiology, Sepsis, and in-hospital mortality. Firstly, we investigate the effects of different pre-training strategies on model performance. We introduce two variants of the self-supervised pre-training strategy: (1) imputing the missing values in the input space like previous methods  (**w/ Imputation**), and (2) directly training the model for patient health status prediction without the proposed self-supervised pre-training (**w/o Pre-training**). As the results shown in Table 2, we observe that although combining the pre-training strategy in the input space can improve the performance compared to models without pre-training, it is still inferior to the proposed strategy that reconstructs in the representation space. These findings highlight the necessity of the pre-training strategy that integrates the missing imputation ability by reconstructing latent representations.

Additionally, we explore the importance of different components and designs in SMART, and the results are also presented in Table 2. In particular, we compare with the following reduced variants: (1) **w/o Mask**, which removes the mask information totally in SMART, including mask in the input encoder and the attentions in the MART blocks; (2) **w/o Temporal Attention**, which removes the temporal attention mechanism in SMART; (3) **w/o Variable Attention**, which removes the variable attention mechanism in SMART; and (4) **w/o CLS Vector**, which removes the CLS vector in the input encoder and using the representation at last observation as query in the variable attention and prediction. We observe that all components significantly contribute to the improvement. Notably, incorporating missing information is important for the model to learn high-quality representations. The results show that the temporal and variable attention mechanisms are crucial and fundamental in capturing temporal and variable dependencies, respectively. The CLS vector also plays a critical role in improving the effect of pre-training since it narrows the difference between the two training stages. By considering these components together, SMART is able to capture the intricate temporal relationships and characteristics inner sparse EHR data.

    &  &  &  \\  & AUPRC(\%) & F1 Score(\%) & AUPRC(\%) & F1 Score(\%) & AUPRC(\%) & F1 Score(\%) \\  Full & **53.84\(\)2.24** & **47.53\(\)2.33** & **81.67\(\)0.84** & **75.37\(\)2.62** & **53.30\(\)0.12** & **44.23\(\)2.03** \\  w/ Imputation & 52.77\(\)2.07 & 42.40\(\)2.69 & 81.04\(\)2.84 & 74.71\(\)2.61 & 52.91\(\)1.42 & 43.69\(\)3.24 \\ w/o Pre-training & 52.26\(\)3.14 & 46.60\(\)2.07 & 79.54\(\)3.22 & 74.26\(\)2.65 & 51.84\(\)0.95 & 41.69\(\)2.95 \\  w/o Mask & 49.03\(\)1.93 & 41.98\(\)1.71 & 76.35\(\)2.76 & 69.51\(\)2.11 & 50.43\(\)2.44 & 39.70\(\)1.60 \\ w/o Temporal Attention & 49.28\(\)2.94 & 40.53\(\)2.31 & 65.22\(\)1.44 & 58.47\(\)2.36 & 46.64\(\)1.63 & 33.37\(\)3.82 \\ w/o Variable Attention & 52.27\(\)2.53 & 44.75\(\)2.82 & 80.47\(\)2.36 & 74.65\(\)2.39 & 50.98\(\)0.62 & 41.70\(\)2.92 \\ w/o CLS Vector & 52.96\(\)0.34 & 46.42\(\)1.77 & 77.56\(\)2.89 & 71.53\(\)2.56 & 50.78\(\)1.64 & 43.86\(\)2.85 \\   

Table 2: Ablation study of SMART on the Cardiology, Sepsis, and MIMIC-III in-hospital mortality.

#### 4.2.3 Effect of Missingness

To further investigate the impact of missingness in the data on performance, we conduct a comprehensive experiment on the Cardiology, Sepsis, and in-hospital mortality tasks by varying the observed rate from 10% to 100%. The results of AUPRC are shown in Figure 2. AdaCare shows the most significant performance degradation as the observed rate decreases, which confirms that the convolutional architecture inside AdaCare is sensitive to missing values. Though Warpformer is stable on the Sepsis, it cannot handle the missingness well on the Cardiology and in-hospital mortality. Among the baselines, StageNet and PPN exhibit robustness to missing values. However, there is still a large margin compared to SMART. Especially, on the Sepsis, we observe only a very small decline on SMART despite only 10% of the available data. SMART performs outstandingly under different missing rate scenarios, demonstrating its fruitful results in missingness perception.

### Model Efficiency

We conduct runtime and parameter comparisons between our method and baselines on the Cardiology, Sepsis, and in-hospital mortality prediction tasks. The number of parameters, the averaged runtime (minutes) on GPU (counted at the start of training), and the AUPRC of different methods are reported in Figure 3 for comparisons. Since the Sepsis dataset contains a larger number of patients, the model trained on it takes longer on average. As illustrated, RainDrop has the largest number of parameters and its training time is several times that of some of the models, such as AdaCare, ConCare, GRASP, Warpformer, and SMART, yet its prediction performance is not satisfactory and unstable on different datasets. StageNet, PPN, and PrimeNet achieve competitive performance on some of the datasets, however their training times are very long. Although AdaCare has the fewest parameters and the shortest training time, it also has one of the worst performances. In a nutshell, SMART is lightweight, fast, and reliable among all the models, reaching the best performance with only fewer parameters and shorter training time, which confirms its efficiency and effectiveness. Nevertheless, it is worth

Figure 3: Training time, parameters, and AUPRC(%) of all models on the three datasets. The size of the circle represents the number of parameters. The GPU runtime is counted at the start of training.

Figure 2: Performance on different observed ratio of EHR.

mentioning that due to the temporal attention mechanism in SMART performs in the visit dimension, its training time may grow quadratically as the number of visits increases.

## 5 Conclusions and Limitations

In this work, we propose SMART, a self-supervised pre-trained model that is designed to handle the challenges of missingness and irregularity in EHR data. We introduce a two-stage training strategy that integrates imputing missingness in the representation space to enhance performance on clinical tasks. We elaborate on a novel MART block that captures temporal and variable interactions by introducing masks into the attention mechanism. Extensive experiments on six EHR tasks demonstrate that SMART outperforms existing baselines. The ablation study shows that all components in SMART contribute to the improvement. We further display that SMART is lightweight, efficient, and robust to missing values and achieves stable performance across different missing rates. In the future, we plan to provide more insights into the decision-making process and make it more explainable.

Despite the promising results obtained in our work, it is important to acknowledge its limitations. Like all models with temporal attention, the time and space complexity of our model is quadratic, which means that the model may face slow speed or insufficient memory when the input sequence is too long. Besides, we evaluated the experimental results with three datasets in reference to previous work, while there remain other datasets that could be taken into account.