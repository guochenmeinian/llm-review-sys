# DASH: Warm-Starting Neural Network Training in Stationary Settings without Loss of Plasticity

Baekrok Shin &Junsoo Oh &Haneul Cho &Chulhee Yun

Kim Jaechul Graduate School of AI, KAIST

{br.shin, junsoo.oh, jhs4015, chulhee.yun}@kaist.ac.kr

Authors contributed equally to this paper.

###### Abstract

Warm-starting neural network training by initializing networks with previously learned weights is appealing, as practical neural networks are often deployed under a continuous influx of new data. However, it often leads to _loss of plasticity_, where the network loses its ability to learn new information, resulting in worse generalization than training from scratch. This occurs even under stationary data distributions, and its underlying mechanism is poorly understood. We develop a framework emulating real-world neural network training and identify noise memorization as the primary cause of plasticity loss when warm-starting on stationary data. Motivated by this, we propose **Direction-Aware SHrinking (DASH)**, a method aiming to mitigate plasticity loss by selectively forgetting memorized noise while preserving learned features. We validate our approach on vision tasks, demonstrating improvements in test accuracy and training efficiency.12

## 1 Introduction

When training a neural network on a gradually changing dataset, the model tends to lose its _plasticity_, which refers to the model's ability to adapt to new information (Dohare et al., 2021; Lyle et al., 2023; Nikishin et al., 2022). This phenomenon is particularly relevant in scenarios with non-stationary data distributions, such as reinforcement learning (Igl et al., 2020; Nikishin et al., 2022) and continual learning (Chen et al., 2023; Kumar et al., 2023; Wu et al., 2021). While requiring to overwrite outdated knowledge as the environment changes, models overfitted to previously encountered environments often struggle to cumulate new information, which in turn leads to reduced generalization performance (Lyle et al., 2023). Under this viewpoint, various efforts have been made to mitigate the loss of plasticity, such as resetting layers (Nikishin et al., 2022), regularizing weights (Kumar et al., 2023), and modifying architectures (Lee et al., 2023; Lyle et al., 2023; Nikishin et al., 2023).

Perhaps surprisingly, a similar phenomenon occurs in supervised learning settings, even where new data points sampled from a stationary data distribution are added to the dataset during training. It is counterintuitive, as one would expect advantages in both generalization performance and computational efficiency when we _warm-start_ from a model pre-trained on data points of the same distribution. For a particular example, when a model is pre-trained using a portion of a dataset and then we resume the training with the whole dataset, the generalization performance is often worse than a model trained from scratch (i.e., _cold-start_), despite achieving similar training accuracy (Ash and Adams, 2020; Berariu et al., 2021; Igl et al., 2020). Liu et al. (2020) report a similar observation: training neural networks with random labels leads to a spurious local minimum which is challenging to escape from, even when retraining with a correctly labeled dataset. Interestingly, Igl et al. (2020) found that pre-training with random labels followed by the corrected dataset yieldsbetter generalization performance than pre-training with a small portion of the (correctly labeled) dataset and then training with the full, unaltered dataset. It is striking that warm-starting leads to such a severe loss of performance, even worse than that of a cold-started model or a model re-trained from parameters pre-trained with random labels, despite the stationarity of the data distribution.

These counterintuitive results prompt us to investigate the underlying reasons for them. While some studies have attempted to explain the loss of plasticity in deep neural networks (DNNs) under non-stationarity (Lewandowski et al., 2023; Lyle et al., 2023; Sokar et al., 2023), their empirical explanations rely on various factors, such as model architecture, datasets, and other variables, making it difficult to generalize the findings (Lewandowski et al., 2023; Lyle et al., 2023). Moreover, there is limited research that explores why warm-starting is problematic in stationary settings, highlighting the lack of a fundamental understanding of the loss of plasticity phenomenon in both stationary and non-stationary data distributions.

### Our Contributions

In this work, we aim to explain why warm-starting leads to worse generalization compared to cold-starting, focusing on the stationary case. We propose an abstract framework that combines the popular feature learning framework initiated by Allen-Zhu and Li (2020) with a recent approach by Jiang et al. (2024) that studies feature learning in a combinatorial and abstract manner. Our analysis suggests that warm-starting leads to overfitting by memorizing noise present in the newly introduced data rather than learning new features.

Inspired by this finding, we propose Direction-Aware SHrinking (DASH), which aims to encourage the model to forget memorized noise without affecting previously learned features. This enables the model to learn features that cannot be acquired through warm-starting alone, enhancing the model's generalization ability. We validate DASH using an expanding dataset setting, similar to the approach in Ash and Adams (2020), employing various models, datasets, and optimizers. As an example, Figure 1 shows promising results in terms of both test accuracy and training time.

### Related Works

Loss of Plasticity.Research has aimed to understand and mitigate loss of plasticity in non-stationary data distributions. Lewandowski et al. (2023) explain that loss of plasticity co-occurs with a reduction in the Hessian rank of the training objective, while Sokar et al. (2023) attribute it to an increasing number of inactive neurons during training. Lyle et al. (2023) find that changes in the loss landscape curvature caused by non-stationarity lead to loss of plasticity. Methods addressing this issue in non-stationary settings include recycling dormant neurons (Sokar et al., 2023), regularizing weights towards initial values (Kumar et al., 2023), and combining techniques (Lee et al., 2023) like layer normalization (Ba et al., 2016), Sharpness-Aware Minimization (SAM) (Foret et al., 2020), resetting layers (Nikishin et al., 2022), and Concatenated ReLU activation (Shang et al., 2016).

Figure 1: Performance comparison of various methods on Tiny-ImageNet using ResNet-18. The same hyperparameters are used across all methods. The dataset is divided into 50 chunks, with a constant number of data points added to the training dataset in each experiment (x-axis), reaching the full dataset at the 50th experiment. Models are trained until achieving 99.9% train accuracy before proceeding to the next experiment; the plot on the right reports the number of update steps executed in each experiment. Results are averaged over three random seeds. “Cold” refers to cold-starting and “Warm” refers to warm-starting. The Shrink & Perturb (S&P) method involves shrinking the model weights by a constant factor and adding noise (Ash and Adams, 2020). Notably, DASH, our proposed method, achieves better generalization performance compared to both training from scratch and S&P, while requiring fewer steps to converge.

However, these explanations and methods diverge from the behavior observed in stationary data distributions. Techniques aimed at mitigating loss of plasticity under non-stationarity are ineffective under stationary distributions, as shown in Appendix C.1, in line with the observations in Lee et al. (2023). While some works study the warm-starting problem in stationary settings, they rely on empirical observations without theoretical analysis (Achille et al., 2018; Ash and Adams, 2020; Berarita et al., 2021). The most relevant work by Ash and Adams (2020) introduces the Shrink and Perturb (S&P) method, which mitigates the loss of plasticity in stationary settings to some extent by shrinking all weight vectors by a constant factor and adding noise. However, they do not explain why this phenomenon occurs or why S&P is effective. We develop a theoretical framework explaining why warm-starting suffers even under stationary distribution. Based on findings, we propose a method that shrinks the weight vector in a direction-aware manner to maintain properly learned features.

Feature Learning in Neural Networks.Recent studies have investigated how training methods and network architectures influence generalization performance, focusing on data distributions with label-dependent features and label-independent noise (Allen-Zhu and Li, 2020; Cao et al., 2022; Deng et al., 2023; Jelassi and Li, 2022; Oh and Yun, 2024; Zou et al., 2023). In particular, Shen et al. (2022) examine a data distribution consisting of varying frequencies of features and large strengths of noise, emphasizing the significance of feature frequencies in learning dynamics. Jiang et al. (2024) propose a novel feature learning framework based on their observations in real-world scenarios, which also involves features with different frequencies but considers the learning process as a discrete sampling process. Our framework extends these ideas by incorporating features with varying frequencies, noise components, and the discrete learning process while introducing a more intricate learning process capturing the key aspects of feature learning dynamics in expanding datasets.

## 2 A Framework of Feature Learning

### Motivation and Intuition

We present the motivation and intuition behind our framework before delving into the formal description. Our framework captures key characteristics of image data, where the input includes both label-relevant information (referred to as _features_, e.g., cat faces in cat images) and label-irrelevant information (referred to as _noise_, e.g., grass in cat images). A key intuition is that minimizing training loss involves two strategies: _learning features_ and _memorizing noise_. This framework builds on insights from Shen et al. (2022) and integrates them into a discrete learning framework. We provide more detailed intuition on our framework, including the training process, in Appendix A.

Shen et al. (2022) consider a neural network trained on data with features of different frequencies and noise components stronger than the features. The negative gradient of the loss for each single data point aligns more with the noise than the features due to the larger scale of noise, making the model more likely to memorize noise rather than learn features. However, an identical feature appears in many data points, while noise appears only once and does not overlap across data points. Thus, if a feature appears at a sufficiently high frequency in the dataset, the model can learn the feature. Thus, the learning of features or noise depends on the frequency of features and the strength of the noise.

Inspired by Shen et al. (2022), we propose a novel discrete feature learning framework. This section introduces a framework describing a single experiment, while Section 3 analyzes expanding dataset scenarios. As our focus is on gradually expanding datasets, carrying out the (S)GD analysis over many experiments as in Shen et al. (2022) is highly challenging. Instead, we adopt a discrete learning process similar to Jiang et al. (2024) but propose a more intricate process reflecting key ideas from Shen et al. (2022). In doing so, we generalize the concept of plasticity loss and analyze it without assuming any particular hypothesis class for a more comprehensive understanding, whereas existing works are limited to specific architectures.

### Training Process

We consider a classification problem with \(C\) classes, and data are represented as \((,y)[C]\), where \(\) denotes the input space. A data point is associated with a combination of class-dependent features \(()_{y}\) where \(_{c}=\{v_{c,1},v_{c,2},,v_{c,K}\}\) is the set of all features for each class \(c[C]\). Also, every data point contains point-specific noise which is class-independent.

The model \(f:[C]\) sequentially learns features based on their frequency. The training process is described by the set of learned features \(_{c[C]}_{c}\) and the set of data points with non-zerogradients \(\), where \(=\{(_{i},y_{i})\}_{i[m]}\) denotes a training set. The set \(\), representing the data points with non-zero gradients, will be defined below. The frequency of a feature \(v\) in data points belonging to \(\) is denoted by

\[g(v;,)=|}_{(,y) }(v()),\]

where \(()\) is the indicator function, which equals \(\) if the condition inside the parentheses is true and 0 otherwise. At each step of training, if \(\) and \(\) are given, the model chooses the most frequent feature among the features not yet learned, i.e., arbitrarily choose \(v_{u}g(u;, )\).

The model decides whether to learn a selected feature \(v\) by comparing its signal strength, represented by \(|| g(v;,)\), with the signal strength of noise, given by \(\), which reflects the key ideas of Shen et al. (2022). If the frequency of the selected feature \(v\) is no less than the threshold \(/||\), i.e., \(g(v;,)/||\), the model learns \(v\) and adds it to its set of learned features \(\). The feature learning process continues until the model reaches a point where the selected feature \(v\) has \(g(v;,)</||\), indicating that the signal strength of every remaining feature is weaker than that of noise. At this point, the feature learning process ends.

We consider a data point \(\) to be _well-classified_ if the model \(f\) has learned at least \(\) features from \(()\), i.e., \(|()|\), where \(<K\). In this case, we consider \(\) to have a zero gradient, meaning it cannot further contribute to the learning process. Throughout the feature learning process, the set \(\) of data points with non-zero gradients is dynamically updated as new features are learned. At each step, when the model successfully learns a new feature, we update \(\) by removing the data points that satisfy \(|()|\), as they become well-classified due to the newly learned feature.

If the feature learning process ends and the model has learned as many features as it can, the remaining data points that have non-zero gradients will be _memorized_ by fitting the random noise present in them and will be considered to have zero gradients. This step concludes the training process. Pictorial illustration can be found in Appendix A and a detailed algorithm of the learning process can be found in Algorithm 2 in Appendix E.

### Discussion on Training Process

In our framework, the model selects features based on their frequency in the set of unclassified data points \(\). The intuition behind this approach is that features appearing more frequently in the set of data points will have larger gradients, leading to larger updates, and we treat \(g(v;,)\) as a proxy of the gradient for a particular feature \(v\). As a result, the model prioritizes sequentially learning these high-frequency features. However, if the frequency \(g(v;,)\) of a particular feature \(v\) is not sufficiently large, such that the total occurrence of \(v\) is less than the strength of the noise, i.e., \(|| g(v;,)<\), the model will struggle to learn that feature. Consequently, the model will prioritize learning the noise over the informative features. When this situation arises, the learning procedure becomes sub-optimal because the model fails to capture the true underlying features of the data and instead memorizes the noise to achieve high training accuracy.

The threshold \(\) determines when a data point is considered well-classified and acts as a proxy for the dataset's complexity. A higher \(\) requires the model to learn more features for correct predictions, while a lower \(\) allows accurate predictions with fewer learned features. Experiments in Appendix B Figure 11 and 12 support this interpretation.

**Remark 2.1**.: We believe our analysis can be extended to scenarios where feature strength varies across data by treating the set of features as a multiset, where multiple instances of the same element are allowed. The analyses in these cases are nearly identical to ours; therefore, we assume all features have identical strengths for notational simplicity.

## 3 Warm-Starting versus Cold-Starting, and a New Ideal Method

### Experiments with Expanding Dataset

In this section, we set up the scenario where the dataset grows after each experiment in our learning framework, allowing us to compare warm-start, cold-start, and a new ideal method, which will be defined later in Sections 3.3 and 3.4.

To better understand the loss of plasticity under stationary data distribution, we consider an extreme form of stationarity where the frequency of each feature combination remains constant in each chunk of additional data. We investigate if the loss of plasticity can manifest even under this strong stationarity. The detailed description of the dataset across the entire experiment is as follows:

**Assumption 3.1**.: In each \(j\)-th experiment, we are provided with a training dataset \(_{j}:=\{(_{i,j},y_{i,j})\}_{i[n]}\) with \(n\) samples. For each class \(c[C]\) and each possible feature combination \(_{c}\), we assume that \(_{j}\) contains exactly \(n_{} 1\) data points with associated feature set \(\), where the values of \(n_{}\) are independent of \(j\). Note that \(_{c[C],_{c}}n_{}=n\). In the \(j\)-th experiment, we use the cumulative dataset \(_{1:j}:=_{l[j]}_{l}\), the set of all training data up to the \(j\)-th experiment.

**Remark 3.2**.: In each experiment, the feature combinations remain the same across the dataset, but the individual data points differ. This is because each data point is associated with its specific noise, which varies across samples. Although the underlying features are the same, the noise component of each data point is unique. This approach ensures that the model is exposed to a diverse set of samples.

We define a technical term \(h(v;)_{c[C], _{c}}n_{}(v| |<)\) to denote the portion of data points containing \(v\) not-well-classified by feature set \(\). This leads to assumption:

**Assumption 3.3**.: For any learned feature set \(\), if \(v_{1},v_{2}_{c}\) for some class \(c[C]\) and \(h(v_{1};)=h(v_{2};)\), then \(v_{1}=v_{2}\). Also, for any class \(c[C]\), there exists some \(\) distinct features \(v_{1},,v_{}_{c}\) such that \(g(v_{1};_{j},_{j}),,g(v_{-1};_{j}, _{j})/n\) and \(g(v_{};_{j},_{j})</n\).

This assumption leads to Lemma D.2, stating that the order in which features are learned within a class is deterministic. This is just for simplicity of presentation and can be relaxed. The last assumption is justified by the moderate number of data points in each chunk \(_{j}\), ensuring the existence of both \(-1\) learnable features and a non-learnable feature within a class. Throughout the following discussion, we will proceed under the above assumptions unless otherwise specified.

Notation.We denote a model at step \(s\) of the \(j\)-th experiment as \(f^{(j,s)}\). We denote the set of learned features and the set of memorized data for the model \(f^{(j,s)}\) as \(^{(j,s)}\) and \(^{(j,s)}\), respectively. We also define the set of data points with non-zero gradients at step \(s\) of the \(j\)-th experiment as \(^{(j,s)}\). We define respective versions of these sets and the model, with different initialization methods, denoted by the subscripts (e.g., \(f^{(j,s)}_{}\), \(f^{(j,s)}_{}\), and \(f^{(j,s)}_{}\)). We emphasize that each method initializes \(f^{(j,0)},^{(j,0)},^{(j,0)}\), and \(^{(j,0)}\) differently at the start of the \(j\)-th experiment.

### Prediction Process and Training Time

We provide a comparison of three initialization methods based on test accuracy and training time. To evaluate these metrics within our framework, we define the prediction process and training time.

Prediction Process.The model predicts unseen data points by comparing the learned features with features present in a given data point \(\). If the overlap between the learned feature set \(\) and the features in \(\), denoted as \(()\), is at least \(\), i.e., \(|()|\), the model correctly classifies the data point. Otherwise, the model resorts to random guessing.

Training Time.Accurately measuring training time within our discrete learning framework is challenging. To address this, we introduce an alternative for training time of \(j\)-th experiment: the number of training data points with non-zero gradients at the start of \(j\)-th experiment, \(|^{(j,0)}|\). This represents the amount of "learning" required for the model to classify all data points correctly. We empirically validated this proxy in practical scenarios, as shown in Figures 6 and 7 in Appendix B.1. Additionally, Nakkiran et al. (2021) observe that in real-world neural network training, when other components are fixed, the training time increases with the number of data points to learn.

### Comparison Between Warm-Starting and Cold-Starting in Our Framework

Now we analyze the warm-start and cold-start initialization methods within our framework, focusing on test accuracy and training time. We note that, by definition, \(^{(j,0)}_{}\) and \(^{(j,0)}_{}\) are both empty sets, while \(^{(j,0)}_{}=^{(j-1,s_{j-1})}_{}\) and \(^{(j,0)}_{}=^{(j-1,s_{j-1})}_{},\) where \(s_{j}\) denotes the last step of \(j\)-th experiment. Besides, we use a shorthand notation for step \(s_{j}\) of the experiment \(j\) that we drop \(s\) if \(s=s_{j}\) (e.g., \(^{(j)}:=^{(j,s_{j})}\)). For the detailed algorithms based on our learning framework, see Algorithms 3 and 4 in Appendix E.

In the test data, a feature combination \(_{c}\) of data point with class \(c[C]\) appears with probability \(n_{}/n\) along with data-specific noise. By Section 3.2, test accuracy for a learned set \(\) and training time are defined as:

\[()  1-_{c[C], _{c}}n_{}(| |<)\] \[T_{}^{(J)}  _{j[J]}|_{}^{(j,0) }|,\;T_{}^{(J)}_{j[J]}| _{}^{(j,0)}|\]

Based on these definitions, the following theorem holds:

**Theorem 3.4**.: _There exists nonempty \(\) such that we always obtain \(_{}^{(1)}=_{}^{(1)}=\). For all \(J 2\), the following inequalities hold:_

\[(_{}^{(J)}) (_{}^{(J)}), T_{}^{(J)}< T_{}^{(J)}\]

_Furthermore, \((_{}^{(J)})<(_{ }^{(J)})\) holds when \(J>\) where \(_{v}h(v;)>0\)._

Proof Idea.: After the first experiment, the data points in \(_{1}\) cannot further contribute to the learning process of the warm-started model. Consequently, even when a new data chunk is provided in subsequent experiments, the feature frequencies are too small, resulting in a weak signal strength of features that cannot overcome the noise signal strength. As a result, the model memorizes individual noise components of the new data points. This procedure is repeated with every experiment, causing the learned feature set to remain _the same_ as at the end of the first experiment. In contrast, when receiving \(_{1:j}\) at once (cold-starting), the signal strength of features is large enough to overcome the noise signal strength, allowing the model to learn many more features. 

Theorem 3.4 highlights a trade-off between cold-starting and warm-starting. Regarding test accuracy, the theorem concludes that cold-starting can achieve strictly higher accuracy than warm-starting. However, warm-starting requires a strictly shorter training time compared to cold-starting.

Detailed proof is provided in Appendix D. Theorem 3.4 suggests that the loss of plasticity in the incremental setting under the stationary assumption can be attributed to the noise memorization process. A similar observation is made in real-world neural network training. It is widely believed that during the early stages of training, neural networks primarily focus on learning features from the dataset, and after learning these features, the model starts to memorize data points that it fails to classify correctly using the learned features. To investigate this phenomenon, we conducted an experiment where CIFAR-10 was divided into two chunks, each containing 50% of the training dataset. The model was pre-trained on one chunk and then further trained on the full dataset for 300 epochs. We used three-layer MLP and ResNet-18 with SGD optimizer across 10 random seeds.

Figure 2 shows the change in the model's performance based on the duration of pre-training. When pre-training is stopped at a certain epoch and the model is then trained on the full dataset, test accuracy is maintained. However, if pre-training continues beyond a specific threshold (approximately 50% pre-training accuracy in this case), warm-starting significantly impairs the model's performance as it increasingly memorizes training data points. We attribute this phenomenon to the neural network's memorization process after learning features. This is consistent with reports of a critical learning period where neural networks learn useful features in the early phase of learning (Achille et al., 2018; Frankle et al., 2020; Kleinman et al., 2024), and with findings that neural networks tend to learn features followed by memorizing noises (Arpit et al., 2017; Jiang et al., 2020). Using the same experimental settings as in Figure 2, we tested with a large-scale dataset, ImageNet-1k, and observed similar trends (see Figure 8 in Appendix B).

**Remark 3.5**.: Igl et al. (2020) find that training a model on random labels followed by corrected labels results in better generalization compared to pre-training on a subset of correctly labeled data and then further training on the full dataset with the same distribution. Achille et al. (2018) also observe that pre-training with slightly blurred images followed by original images yields worse test accuracy than pre-training with random label or random noise images. These findings align with our observations: re-training with corrected labels after random label learning "revives" gradients for most memorized data points, enabling new feature learning. Conversely, with static distributions, gradients for memorized data points remain suppressed, leading to learning from only a few data points with active gradients, causing memorization.

### An Ideal Method: Retaining Features and Forgetting Noise

In Section 3.3, we observed a trade-off between warm-starting and cold-starting. Cold-starting often achieves better test accuracy compared to warm-starting, while warm-starting requires less time to converge. The results suggest that neither retaining all learned information nor discarding all learned information is ideal. To address this trade-off and get the best of both worlds, we consider an ideal algorithm where we retain all learned features while forgetting all memorized data points. For any experiment \(J 2\), if we consider the ideal initialization, learned features \(_{}^{(J-1)}\) are retained, and memorized data points \(_{}^{(J-1)}\) are reset to an empty set. Pseudo-code for this method is given in Algorithm 5, which can be found in Appendix E. We define \(T_{}^{(J)}_{j[J]}|_{}^{(j,0)}|\) as the training time with the ideal method, where \(_{}^{(j,0)}\) represents the set of data points having a non-zero gradient at the initial step of the \(j\)-th experiment. Then, we have the following theorem:

**Theorem 3.6**.: _For any experiment \(J 2\), the following holds:_

\[(_{}^{(J)})=( _{}^{(J)}), T_{}^{(J)}<T_{ }^{(J)}<T_{}^{(J)}\]

The detailed proof is provided in Appendix D. The ideal algorithm addresses the trade-off between cold-starting and warm-starting. We conducted an experiment to investigate the performance gap between these initialization methods.

Synthetic Experiment.To verify our theoretical findings in more realistic scenarios, we conducted an experiment that more closely resembles real-world settings. Instead of fixing the frequency of each feature set, we sampled each feature's existence from a Bernoulli distribution to construct \(()\). This ensures that the experiment is more representative of real-world scenarios. Specifically, for each data point \((,y)\), we uniformly sampled \(y\{0,1\}\). From the feature set \(_{y}\) corresponding to the sampled class \(y\), we sampled features where each feature's existence follows a Bernoulli distribution, \((v_{y,k}())(p_{k})\), for all \(v_{y,k}_{y}\). This approach allows us to model the variability in feature occurrence that is commonly observed in real-world datasets while still maintaining the core principles of our learning framework. We set the number of features, \(K=50\), with \(p_{k}\) sampled from a uniform distribution, \((0,0.2)\). Each chunk contained 1000 data points with total 50 experiments, with \(=50\), \(=3\). We sampled 10000 test data from the same distribution.

As shown in Figure 3, the results align with the above theorems. Random initialization, i.e. cold-starting, and ideal initialization achieve almost identical generalization performance, outperforming warm initialization. However, with warm initialization, the model converges faster, as evidenced by the number of non-zero gradient data points, which serves as a proxy for training time. Ideal initialization requires less time compared to cold-starting, which is also consistent with Theorem 3.6. Due to the sampling process in our experiment, we observe a gradual increase in the number of learned features and test accuracy in warm-starting, mirroring real-world observations. These findings remained robust across diverse hyperparameter settings (see Figures 9-11 in the Appendix B).

Figure 2: The plot shows the test accuracy (left y-axis) when the model is pretrained for varying epochs (x-axis) and then fine-tuned on the full data, along with the pretrain accuracy (right y-axis) plotted in brown. We trained three-layer MLP (left) and ResNet-18 (right). Each transparent line and point corresponds to a specific random seed, and the median values are highlighted with opaque markers and solid lines. The ‘Random’ corresponds to training from random initialization (cold-start).

## 4 DASH: Direction-Aware SHrinking

The ideal method recycles memorized training samples by forgetting noise while retaining learned features. From now on, we shift our focus to a practical scenario: training neural networks with real-world data. This brings up the question of whether such an ideal approach can be applied in real-world settings. To address this, we propose our algorithm, **Direction-Aware SHrinking (DASH)**, which intuitively captures this idea in practical training scenarios. The outlined behavior is illustrated in Figure 4. When new data is introduced, DASH shrinks each weight based on its alignment with the negative gradient of the loss calculated from the training data, placing more emphasis on recent data.

If the degree of alignment is small (i.e., the cosine similarity is close to or below 0), we consider that the weight has not learned a proper feature and shrinks it significantly to make it "forget" learned information. This allows weights to forget memorized noises and easily change their direction. On the other hand, if the weight and negative gradient are well-aligned (i.e., the cosine similarity is close to 1), we consider it learned features and we shrink the weight to a lesser degree to maintain the learned information. This method aligns with the intuition of the ideal method, as it allows us to shrink weights that have not learned proper information while retaining weights that have learned commonly observed features.

```
0:
* Model \(f_{}\) with list of parameters \(\) after the \((j-1)\)-th experiment
0: Training data points \(_{1:j}\)
0: Averaging coefficient \(0< 1\)
0: Threshold \(>0\)
1:Initialize:
0:\(G_{}^{(0)} 0,\) in \(\)
2:for\(i 1:j\)do
3:\((f_{},_{i})\)
4:\(U_{}\ \)
5:for\(\) in \(\)do
6:\(G_{}^{(i)}(1-) G_{}^{(i-1)}+ U_ {}\)
7:endfor
8:endfor
9:for\(\) in \(\)do
10:\(s_{}(-G_{}^{(j)},)\)
11:\(\{,s_{}\}\)
12:endfor
13:return model \(f_{}\), initialized for the \(j\)-th experiment ```

**Algorithm 1** Direction-Aware SHrinking (DASH)

The shrinking is done _per neuron_, where the incoming weights are grouped into a weight vector denoted as \(\). For convolutional filters, the height and width of the kernel are flattened to form a single weight vector \(\) for each pair of input and output filters. DASH has two hyperparameters: \(\) and \(\). Hyperparameter \(\) is the minimum shrinkage threshold, as each weight vector is shrunk by \(\{,\}\), while \(\) denotes the coefficient of exponential moving average of per-chunk loss gradients. Lower \(\) value gives more weight to previous gradients, resulting in less shrinkage. This

Figure 4: Illustration of DASH. We compute the loss \(L\) with training data \(_{1:j}\) and obtain the negative gradient. Then, we shrink the weights proportionally to the cosine similarity between the current weight \(\) and \(_{}L\), resulting in \(\).

Figure 3: Comparison of random, warm, and ideal methods across 10 random seeds (mean \(\) std dev). The test accuracy (left) and the number of learned features across all classes (middle) are nearly identical for random and ideal initializations, causing their plots to overlap. Warm initialization, however, exhibits lower test accuracy compared to both methods. Regarding training time (right), there is a significant gap between random and warm initialization, which the ideal method addresses.

is because gradients of the previously seen data usually have high cosine similarity with learned weights; low \(\) is advantageous for simpler datasets where preserving learned features helps. Note that DASH is an initialization method applied _only once_ when new data is introduced. The detailed algorithm is presented in Algorithm 1.

We discuss our intuition regarding the connection between DASH and the spirit of the ideal method. Generally, features from previous data are likely to reappear frequently in new data, as they are relevant to the data class. In contrast, noise from previous data rarely reappears as it is not class-specific and varies with each data point. As a result, the negative gradient of the loss naturally aligns more closely with learning features rather than memorizing noise from older data. This suggests that when neurons are aligned with its negative gradient of the loss, we can assume they have learned important features and should not be shrunk.

To validate our intuition, we plotted the accuracy on previously learned data points a few epochs after applying DASH in Figure 13, Appendix B. Our experiments show that DASH recovers training accuracy on previous datasets more quickly than other methods, likely because it preserves learned features while discarding memorized noise. As experiments progress, the growing number of learned features allows DASH to retain more information, leading to improved training accuracy across successive experiments. We further validate our intuition behind DASH in Figure 14 in Appendix B.

## 5 Experiments

### Experimental Details

Our setup is similar to the one described in Ash and Adams (2020). We divided the training dataset into 50 chunks, and at the beginning of each experiment, a chunk is added to the existing training data. Models were considered converged and each experiment was terminated when training accuracy reached 99.9%, aligning with our learning framework. We conducted experiments with vanilla training i.e. without data augmentations, weight decay, learning rate schedule, etc. Appendix C.4 presents additional results on other settings, including the state-of-the-art (SoTA) settings that include the techniques mentioned above. We evaluated DASH on Tiny-ImageNet, CIFAR-10, CIFAR-100, and SVHN using ResNet-18, VGG-16, and three-layer MLP architectures with batch normalization layer. Models were trained using Stochastic Gradient Descent (SGD) and Sharpness-Aware Minimization (SAM) (Foret et al., 2020), both with momentum.

DASH was compared against baselines (cold-starting, warm-starting, and S&P (Ash and Adams, 2020)) and methods addressing plasticity loss under non-stationarity (L2 INIT (Kumar et al., 2023) and Reset (Nakkiran et al., 2021)). Layer normalization (Ba et al., 2016) and SAM (Foret et al., 2020), known to mitigate plasticity loss in reinforcement learning (Lee et al., 2023), were applied to both warm and cold-starting. Consistent hyperparameters were used across all methods, with details provided in Appendix C.3. S&P, Reset, and DASH were applied whenever new data was introduced. We report two metrics for both test accuracy and number of steps required for convergence: the value from the final experiment and the average across all experiments.

### Experimental Results

We first experimented with CIFAR-10 on ResNet-18 to determine if methods from previous works for mitigating plasticity based on non-stationarity can be a solution to our incremental setting with stationarity. Appendix C.1 shows that L2 INIT, Reset, layer normalization, and reviving dead neurons, are not effective in our setting. Thus, we conducted the remaining experiments without these methods. Additionally, Table 1 shows that warm-starting with SAM does not outperform cold-starting with SAM, indicating that SAM alone is not an effective method in our case. Table 1 shows that DASH surpasses cold-starting (Random Init) and S&P in most cases. Training times were often shorter compared to training from scratch, and when longer, the performance gap in test accuracy was more pronounced. Omitted results are in Tables 3-6 located in Appendix C.2. Additionally, we confirm that DASH is computationally efficient, with details on the computation and memory overhead comparisons provided in Appendix C.5.

We argue that S&P can cause the model to forget learned information, including important features, due to shrinking every weight uniformly and perturbing weights. This leads to increased training time and relatively lower test accuracy, especially in SoTA settings (see Appendix C.4.1). In contrast, DASH addresses these issues by preserving learned features with direction-aware weight shrinkage.

Theorem 3.6 shows that ideal initialization can achieve the same test accuracy as cold-starting. Yet in practice, DASH surpasses cold-starting in test accuracy. This could be due to the difference between the discrete learning process in our framework and the continuous learning process in real-world neural network training. Even if features have already been learned, DASH can learn them in greater strength compared to learning from scratch by preserving previously learned features during training.

Further insights into the applicability of DASH can be found in Appendix C.4. We evaluate DASH in various settings beyond our original setup. In the SoTA setting, different observations are made: DASH achieves test accuracy close to (but does not outperform) cold-starting, without requiring additional hyperparameter tuning, which aligns more closely with our theoretical analysis. We demonstrate DASH's scalability on large-scale datasets such as ImageNet-1k. We also examine two additional practical scenarios: a data-discarding setting and a situation where new data are continuously added. In such cases, applying DASH with an interval (rather than upon every arrival of new data) proves effective. Finally, we explore DASH's behavior in non-stationary environments, specifically in Class Incremental Learning (CIL) with data accumulation settings.

## 6 Discussion and Conclusion

In this work, we defined an abstract framework for feature learning and discovered that warm-starting benefits from reduced training time compared to random initialization but can hurt the generalization performance of neural networks due to the memorization of noise. Motivated by these observations, we proposed Direction-Aware SHrinking (DASH), which shrinks weights that learned data-specific noise while retaining weights that learned commonly appearing features. We validated DASH in real-world model training, achieving promising results for both test accuracy and training time.

Loss of plasticity is problematic in situations where new data is continuously added daily, which is the case in many real-world application scenarios. Our research aimed to interpret and resolve this issue, preventing substantial waste of energy, time, and the environment. By elucidating the loss of plasticity phenomenon in stationary data distributions, we have taken a crucial step towards addressing challenges that may emerge in real-world AI, where the continuous influx of additional data is inevitable.

We hope our fundamental analysis of the loss of plasticity phenomenon sheds light on understanding this issue as well as providing a remedy. To generalize our findings to any neural network architecture, we treated the learning process as a discrete abstract procedure and did not assume any hypothesis class. Future research could focus on understanding the loss of plasticity phenomenon via optimization or theoretically analyzing it in non-stationary data distributions, such as in reinforcement learning.

   } &  &  &  &  \\  &  &  &  &  \\  _T-ImageNet_ & SGD & SAM & SGD & SAM & SGD & SAM & SGD & SAM \\  Random Init & 25.69 (0.13) & 31.30 (0.09) & 30237 (368) & 40142 (368) & 17.37 (0.06) & 21.95 (0.11) & 17503 (53) & 22513 (74) \\ Warm Init & 9.57 (0.24) & 13.94 (0.37) & 3388 (368) & 5474 (0) & 6.70 (0.04) & 9.88 (0.21) & 1785 (5) & 2773 (77) \\ S\&P & 34.34 (0.48) & 37.39 (0.18) & 13873 (508) & 26066 (106) & 25.43 (0.02) & 28.47 (0.08) & 7940 (15) & 13172 (182) \\ DASH & **46.11 (0.34)** & **49.57 (0.36)** & **83.84 (368)** & **12251 (368)** & **33.06 (0.15)** & **35.93 (0.17)** & **44.39 (48)** & **7900 (136)** \\   _CIFAR-10_ &  &  &  &  &  &  &  &  &  \\ Random Init & 67.32 (0.51) & 75.68 (0.39) & **5161 (156)** & 17125 (292) & 57.66 (0.11) & 66.27 (0.13) & 2916 (37) & **8121 (26)** \\ Warm Init & 63.53 (0.56) & 70.99 (0.59) & 1173 (0) & 3910 (247) & 54.87 (0.18) & 63.27 (0.55) & 665 (11) & 2153 (23) \\ S\&P & 81.25 (0.14) & 85.53 (0.22) & 5395 (625) & 32649 (978) & 71.74 (0.16) & 76.19 (0.04) & **2766 (53)** & 15552 (1558) \\  _DASH_ & **84.08 (0.52)** & **86.75 (0.53)** & 6490 (399) & **11886 (2771)** & **75.21 (0.33)** & **77.59 (0.69)** & 3454 (55) & 8689 (527) \\  _CIFAR-100_ &  &  &  &  &  &  &  &  \\ Random Init & 35.52 (0.14) & 40.27 (0.31) & 10557 (247) & 14310 (191) & 25.72 (0.11) & 29.90 (0.06) & 5803 (79) & 7588 (54) \\ Warm Init & 25.12 (0.59) & 32.02 (0.31) & 1173 (0) & 2346 (0) & 19.18 (0.52) & 24.01 (0.33) & 854 (23) & 1294 (12) \\ S\&P & 50.08 (0.23) & 52.95 (0.36) & 4926 (191) & 12277 (1226) & 37.32 (0.14) & 40.36 (0.18) & 2929 (27) & **5954 (187)** \\ DASH & **57.99 (0.28)** & **60.88 (0.29)** & **3519 (0)** & **11730 (1211)** & **43.99 (0.14)** & **46.15 (0.58)** & **2041 (51)** & 6675 (797) \\   _SVHN_ &  &  &  &  &  &  &  &  \\ Random Init & 86.27 (0.46) & 89.84 (0.24) & 5552 (156) & **10869 (156)** & 78.01 (0.10) & 83.31 (0.14) & 3099 (15) & **5546 (44)** \\ Warm Init & 84.01 (0.41) & 88.85 (0.29) & 938 (191) & 1329 (19) & 75.37 (0.50) & 81.16 (0.54) & 642 (18) & 993 (15) \\ S\&P & 92.67 (0.17) & 94.27 (0.07) & **3597 (156)** & 1573 (191) & 87.35 (0.14) & 89.35 (0.05) & **1858 (12)** & 5548 (94) \\ DASH & **93.67 (0.13)** & **95.19 (0.09)** & 5161 (672) & 14467 (089) & **89.59 (0.07)** & **91.67 (0.03)** & 2619 (68) & 8613 (728) \\   

Table 1: Results of training with various datasets using ResNet-18. Bold values indicate the best performance. For the number of steps, bold formatting is used for all methods _except_ warm-starting. Results are averaged across five random seeds, except for Tiny-ImageNet which uses three random seeds. Standard deviations provided in parentheses.