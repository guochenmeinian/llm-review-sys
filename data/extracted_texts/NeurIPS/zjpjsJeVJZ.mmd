# Comparing Apples to Oranges:

Learning Similarity Functions for Data Produced by Different Distributions

 Leonidas Tsepenekas

JPMorganChase AI Research

New York, USA

leonidas.tsepenekas@jpmchase.com Ivan Brugere

JPMorganChase AI Research

New York, USA

ivan.brugere@jpmchase.com Freddy Lecue

JPMorganChase AI Research

New York, USA

freddy.lecue@jpmchase.com Daniele Maggazeni

JPMorganChase AI Research

London, UK

daniele.magazzeni@jpmorgan.com

###### Abstract

Similarity functions measure how comparable pairs of elements are, and play a key role in a wide variety of applications, e.g., notions of Individual Fairness abiding by the seminal paradigm of Dwork et al. (2012), as well as Clustering problems. However, access to an accurate similarity function should not always be considered guaranteed, and this point was even raised by Dwork et al. (2012). For instance, it is reasonable to assume that when the elements to be compared are produced by different distributions, or in other words belong to different "demographic" groups, knowledge of their true similarity might be very difficult to obtain. In this work, we present an efficient sampling framework that learns these across-groups similarity functions, using only a limited amount of experts' feedback. We show analytical results with rigorous theoretical bounds, and empirically validate our algorithms via a large suite of experiments.

## 1 Introduction

Given a feature space \(\), a similarity function \(:^{2}_{ 0}\) measures how comparable any pair of elements \(x,x^{}\) are. The function \(\) can also be interpreted as a distance function, where the smaller \((x,x^{})\) is, the more similar \(x\) and \(x^{}\) are. Such functions are crucially used in a variety of AI/ML problems, _and in each such case \(\) is assumed to be known_.

The most prominent applications where similarity functions have a central role involve considerations of individual fairness. Specifically, all such individual fairness paradigms stem from the the seminal work of Dwork et al. (2012), in which fairness is defined as _treating similar individuals similarly_. In more concrete terms, such paradigms interpret the aforementioned abstract definition of fairness as guaranteeing that for every pair of individuals \(x\) and \(y\), the difference in the quality of service \(x\) and \(y\) receive (a.k.a. their received treatment) is upper bounded by their respective similarity value \((x,y)\); the more similar the individuals are, the less different their quality of service will be. Therefore, any algorithm that needs to abide by such concepts of fairness, should always be able to access the similarity score for every pair of individuals that are of interest.

Another family of applications where similarity functions are vital, involves Clustering problems. In a clustering setting, e.g, the standard \(k\)-means task, the similarity function is interpreted as a distancefunction, that serves as the metric space in which we need to create the appropriate clusters. Clearly, the aforementioned metric space is always assumed to be part of the input.

Nonetheless, it is not realistic to assume that a reliable and accurate similarity function is always given. This issue was even raised in the work of Dwork et al. (2012), where it was acknowledged that the computation of \(\) is not trivial, and thus should be deferred to third parties. The starting point of our work here is the observation that there exist scenarios where computing similarity can be assumed as easy (in other words given), while in other cases this task would be significantly more challenging. Specifically, we are interested in scenarios where there are multiple distributions that produce elements of \(\). We loosely call each such distribution a "demographic" group, interpreting it as the stochastic way in which members of this group are produced. _In this setting, computing the similarity value of two elements that are produced according to the same distribution, seems intuitively much easier compared to computing similarity values for elements belonging to different groups_. We next present a few motivating examples that clarify this statement.

**Individual Fairness:** Consider a college admissions committee that needs access to an accurate similarity function for students, so that it provides a similar likelihood of acceptance to similar applicants. Let us focus on the following two demographic groups. The first being students from affluent families living in privileged communities and having access to the best quality schools and private tutoring. The other group would consist of students from low-income families, coming from a far less privileged background. Given this setting, the question at hand is **"Should two students with comparable feature vectors (e.g., SAT scores, strength of school curriculum, number of recommendation letters) be really viewed as similar, when they belong to different groups?"** At first, it appears that directly comparing two students based on their features can be an accurate way to elicit their similarity only if the students belong to the same demographic (belonging to the same group serves as a normalization factor). However, this trivial approach might hurt less privileged students when they are compared to students of the first group. This is because such a simplistic way of measuring similarity does not reflect potential that is undeveloped due to unequal access to resources. Hence, accurate across-groups comparisons that take into account such delicate issues, appear considerably more intricate.

**Clustering:** Suppose that a marketing company has a collection of user data that wants to cluster, with its end goal being a downstream market segmentation analysis. However, as it is usually the case, the data might come from different sources, e.g., data from private vendors and data from government bureaus. In this scenario, each data source might have its own way of representing user information, e.g., each source might use a unique subset of features. Therefore, eliciting the distance metric required for the clustering task should be straightforward for data coming from the same source, while across-sources distances would certainly require extra care, e.g., how can one extract the distance of two vectors containing different sets of features?

As suggested by earlier work on computing similarity functions for applications of individual fairness (Ilvento, 2019), when obtaining similarity values is an overwhelming task, one can employ the advice of domain experts. Such experts can be given any pair of elements, and in return produce their true similarity value. However, utilizing this experts' advice can be thought of as very costly, and hence it should be used sparingly. For example, in the case of comparing students from different economic backgrounds, the admissions committee can reach out to regulatory bodies or civil rights organizations. Nonetheless, resorting to these experts for every student comparison that might arise, is clearly not a sustainable solution. Therefore, our goal in this paper is to learn the across-groups similarity functions, using as few queries to experts as possible.

## 2 Preliminaries and contribution

For the ease of exposition, we accompany the formal definitions with brief demonstrations on how they could relate to the previously mentioned college applications use-case.

Let \(\) denote the feature space of elements; for instance each \(x\) could correspond to a valid student profile. We assume that elements come from \(\) known "demographic" groups, where \(\), and each group \([]\) is governed by an unknown distribution \(_{}\) over \(\). We use \(x_{}\) to denote a randomly drawn \(x\) from \(_{}\). Further, we use \(x_{}\) to denote that \(x\) is an element in the support of \(_{}\), and thus \(x\) is a member of group \(\). In the college admissions scenario where we have two demographic groups, there will be two distributions \(_{1}\) and \(_{2}\) dictating how the profiles of privileged and non-privileged students are respectively produced. Observe now that for a specific \(x\), we might have \(x_{}\) and \(x_{^{}}\), for \(^{}\). Hence, in our model group membership is important, and every time we are considering an element \(x\), we know which distribution produced \(x\), e.g., whether the profile \(x\) belongs to a privileged or non-privileged student.

For every group \([]\) there is an intra-group similarity function \(d_{}:^{2}_{ 0}\), such that for all \(x,y_{}\) we have \(d_{}(x,y)\) representing the true similarity between \(x,y\). In addition, the smaller \(d_{}(x,y)\) is, the more similar \(x,y\). Note here that the function \(d_{}\) is only used to compare members of group \(\) (in the college admissions example, the function \(d_{1}\) would only be used to compare privileged students with each other). Further, a common assumption for functions measuring similarity is that they are metric1[Yona and Rothblum, 2018; Kim et al., 2018; Ilvento, 2019; Mukherjee et al., 2020; Wang et al., 2019]. _We also adopt the metric assumption for the function \(d_{}\)_. Finally, based on the earlier discussion regarding computing similarity between elements of the same group, we assume that \(d_{}\) is known, and given as part of the instance.

Moreover, for any two groups \(\) and \(^{}\) there exists an _unknown_ across-groups similarity function \(_{,^{}}:^{2}_{ 0}\), such that for all \(x_{}\) and \(y_{^{}}\), \(_{,^{}}(x,y)\) represents the true similarity between \(x,y\). Again, the smaller \(_{,^{}}(x,y)\) is, the more similar the two elements, and for a meaningful use of \(_{,^{}}\) we must make sure that \(x\) is a member of group \(\) and \(y\) a member of group \(^{}\). In the college admissions scenario, \(_{1,2}\) is the way you can accurately compare a privileged and a non-privileged student. Finally, to capture the metric nature of a similarity function, we impose the following mild properties on \(_{,^{}}\), which can be viewed as across-groups triangle inequalities:

1. **Property \(_{1}\):**\(_{,^{}}(x,y) d_{}(x,z)+_{,^{}}(z,y)\) for every \(x,z_{}\) and \(y_{^{}}\).
2. **Property \(_{2}\):**\(_{,^{}}(x,y)_{,^{}}(x,z)+d_{^{ }}(z,y)\) for every \(x_{}\) and \(y,z_{^{}}\).

In terms of the college admissions use-case, \(_{1}\) and \(_{2}\) try to capture reasonable assumptions of the following form. If a non-privileged student \(x\) is similar to another non-privileged student \(z\), and \(z\) is similar to a privileged student \(y\), then \(x\) and \(y\) should also be similar to each other.

_Observe now that the collection of all similarity values (intra-group and across-groups) in our model does not axiomatically yield a valid metric space._ This is due to the following reasons. **1)** If \(_{,^{}}(x,y)=0\) for \(x_{}\) and \(y_{^{}}\), then we do not necessarily have \(x=y\). **2)** It is not always the case that \(d_{}(x,y)_{,^{}}(x,z)+_{,^{}}(y,z)\) for \(x,y_{}\), \(z_{^{}}\). **3)** It is not always the case that \(_{,^{}}(x,y)_{,^{}}(x,z)+ _{^{},^{}}(z,y)\) for \(x_{}\), \(y_{^{}}\), \(z_{^{}}\).

However, not having the collection of similarity values necessarily produce a metric space is not a weakness of our model. On the contrary, we view this as one of its strongest aspects. For one thing, imposing a complete metric constraint on the case of intricate across-groups comparisons sounds unrealistic and very restrictive. Further, even though existing literature treats similarity functions as metric ones, the seminal work of Dwork et al. (2012) mentions that this should not always be the case. Hence, our model is more general than the current literature.

**Goal of Our Problem:** We want for any two groups \(,^{}\) to compute a function \(f_{,^{}}:^{2}_{ 0}\), such that \(f_{,^{}}(x,y)\) is our estimate of similarity for any \(x_{}\) and \(y_{^{}}\). Specifically, we seek a PAC (Probably Approximately Correct) guarantee, where for any given accuracy and confidence parameters \(,(0,1)\) we have:

\[_{x_{},y_{^{}}}f_ {,^{}}(x,y)-_{,^{}}(x,y)> \]

The subscript in the above probability corresponds to two independent random choices, one \(x_{}\) and one \(y_{^{}}\). In other words, we want for any given pair our estimate to be \(\)-close to the real similarity value, with probability at least \(1-\), where \(\) and \(\) are user-specified parameters.

As for tools to learn \(f_{,^{}}\), we only require two things. At first, for each group \(\) we want a set \(S_{}\) of i.i.d. samples from \(_{}\). Obviously, the total number of used samples should be polynomial in the input parameters, i.e., polynomial in \(,\) and \(\). Secondly, we require access to an expert oracle, which given any \(x S_{}\) and \(y S_{^{}}\) for any \(\) and \(^{}\), returns the true similarity value \(_{,^{}}(x,y)\). We refer to a single invocation of the oracle as a query. Since there is a cost to collecting expert feedback, an additional objective in our problem is minimizing the number of oracle queries.

### Outline and discussion of our results

In Section 4 we present our theoretical results. We begin with a simple and very intuitive learning algorithm which achieves the following guarantees.

**Theorem 2.1**.: _For any given parameters \(,(0,1)\), the simple algorithm produces a similarity approximation function \(f_{,^{}}\) for every \(\) and \(^{}\), such that:_

\[[_{(,^{})}] _{x_{}^{ },\\ y_{^{}},}f_{ ,^{}}(x,y)-_{,^{}}(x,y)=( )\] \[=O+p_{}(,)+p_{^{}}( ,)\]

_The randomness here is of three independent sources. The internal randomness \(\) of the algorithm, a choice \(x_{}\), and a choice \(y_{^{}}\). The algorithm requires \(}\) samples from each group, and utilizes \(}^{2}}\) oracle queries._

In plain English, the above theorem says that with a polynomial number of samples and queries, the algorithm achieves an \(O()\) accuracy with high probability, i.e., with probability \(1--p_{}(,)-p_{^{}}(, )\). The definition of the functions \(p_{}\) is presented next.

**Definition 2.2**.: For each group \(\), we use \(p_{}(,)\) to denote the probability of sampling an \((,)\)-rare element of \(_{}\). We define as \((,)\)-rare for \(_{}\), an element \(x_{}\) or which there is a less than \(\) chance of sampling \(x^{}_{}\) with \(d_{}(x,x^{})\). Formally, \(x_{}\) is \((,)\)-rare iff \(_{x^{}_{}}[d_{}(x,x^{})]<\), and \(p_{}(,)=_{x_{}}[x(,) _{}]\). Intuitively, a rare element should be interpreted as an "isolated" member of the group, in the sense that it is at most \(\)-likely to encounter another element that is \(\)-similar to it. For instance, a privileged student is considered isolated, if only a small fraction of other privileged students have a profile similar to them.

Clearly, to get a PAC guarantee where the algorithm's error probability for \(\) and \(^{}\) is \(O()\), we need \(p_{}(,),p_{^{}}(,)=O()\). We hypothesize that in realistic distributions each \(p_{}(,)\) should indeed be fairly small, and this hypothesis is actually validated by our experiments. The reason we believe this hypothesis to be true, is that very frequently real data demonstrate high concentration around certain archetypal elements. Hence, this sort of distributional density does not leave room for isolated elements in the rest of the space. Nonetheless, we also provide a strong _no free lunch_ result for the values \(p_{}(,)\), which shows that any practical PAC-algorithm necessarily depends on them. _This result further implies that our algorithm's error probabilities are indeed almost optimal_.

**Theorem 2.3** (No-Free Lunch Theorem).: _For any given \(,(0,1)\), any algorithm using finitely many samples, will yield similarity approximations \(f_{,^{}}\) with \([|f_{,^{}}(x,y)-_{,^{}}(x,y)|= ()]=(\{p_{}(,),\,p_{^{ }}(,)\}-)\); the probability is over the independent choices \(x_{}\) and \(y_{}^{}\) as well as any potential internal randomness of the algorithm._

In plain English, Theorem 2.3 says that any algorithm using a finite amount of samples, can achieve \(\)-accuracy with a probability that is necessarily at most \(1-\{p_{}(,),\,p_{^{}}(,)\}+\), i.e., if \(\{p_{}(,),\,p_{^{}}(,)\}\) is large, learning is impossible.

Moving on, we focus on minimizing the oracle queries. By carefully modifying the earlier simple algorithm, we obtain a new more intricate algorithm with the following guarantees:

**Theorem 2.4**.: _For any given parameters \(,(0,1)\), the query-minimizing algorithm produces similarity approximation functions \(f_{,^{}}\) for every \(\) and \(^{}\), such that:_

\[[_{(,^{})}]=O(+p_{}(,)+p _{^{}}(,))\]

\([_{(,^{})}]\) _is as defined in Theorem 2.1. Let \(N=}\). The algorithm requires \(N\) samples from each group, and the number of oracle queries used is at most_

\[_{[]}Q_{}_{^{}[]:\,\,^{ }}Q_{^{}}\]

_where \(Q_{} N\) and \([Q_{}]+p_{}(,)N\) for each \(\)._

At first, the confidence, accuracy and sample complexity guarantees of the new algorithm are the same as those of the simpler one described in Theorem 2.1. Furthermore, because \(Q_{} N\), thequeries of the improved algorithm are at most \((-1)N\), which is exactly the number of queries in our earlier simple algorithm. However, the smaller the values \(p_{}(,)\) are, the fewer queries in expectation. Our experimental results indeed confirm that the improved algorithm always leads to a significant decrease in the used queries.

Our final theoretical result involves a lower bound on the number of queries required for learning.

**Theorem 2.5**.: _For all \(,(0,1)\), any learning algorithm producing similarity approximation functions \(f_{,^{}}\) with \(_{x_{},y_{^{}}}|f_{,^{}}(x,y)-_{,^{}}(x,y)|=()=O ()\), needs \((}{^{2}})\) queries._

Combining Theorems 2.4 and 2.5 implies that when all \(p_{}(,)\) are negligible, i.e., \(p_{}(,) 0\), _the expected queries of the Theorem 2.4 algorithm are asymptotically optimal._

Finally, Section 5 contains our experimental evaluation, where through a large suite of simulations we validate our theoretical findings.

## 3 Related work

Metric learning is a very well-studied area (Bellet et al., 2013; Kulis, 2013; Moutafis et al., 2017; Suarez-Diaz et al., 2018). There is also an extensive amount of work on using human feedback for learning metrics in specific tasks, e.g., image similarity and low-dimensional embeddings (Frome et al., 2007; Jamieson and Nowak, 2011; Tamuz et al., 2011; van der Maaten and Weinberger, 2012; Wilber et al., 2014). However, since these works are either tied to specific applications or specific metrics, they are only distantly related to ours.

Our model is more closely related to the literature on trying to learn the similarity function from the fairness definition of Dwork et al. (2012). This concept of fairness requires treating similar individuals similarly. Thus, it needs access to a function that returns a non-negative value for any pair of individuals, and this value corresponds to how similar the individuals are. Specifically, the smaller the value the more similar the elements that are compared. Even though the fairness definition of Dwork et al. (2012) is very elegant and intuitive, the main obstacle for adopting it in practice is the inability to easily compute or access the crucial similarity function. To our knowledge, the only papers that attempt to learn this similarity function using expert oracles like us, are Ilvento (2019); Mukherjee et al. (2020) and Wang et al. (2019). Ilvento (2019) addresses the scenario of learning a general metric function, and gives theoretical PAC guarantees. Mukherjee et al. (2020) give theoretical guarantees for learning similarity functions that are only of a specific Mahalanobis form. Wang et al. (2019) simply provide empirical results. The first difference between our model and these papers is that unlike us, they do not consider elements coming from multiple distributions. However, the most important difference is that these works only learn **metric** functions. **In our case the collection of similarity values (from all \(d_{}\) and \(_{,^{}}\)) does not necessarily yield a complete metric space**; see the discussion in Section 2. Hence, our problem addresses more general functions.

Regarding the difficulty in computing similarity between members of different groups, we are only aware of a brief result by Dwork et al. (2012). In particular, given a metric \(d\) over the whole feature space, they mention that \(d\) can only be trusted for comparisons between elements of the same group, and not for across-groups comparisons. In order to achieve the latter for groups \(\) and \(^{}\), they find a new similarity function \(d^{}\) that approximates \(d\), while minimizing the Earthmover distance between the distributions \(_{},_{^{}}\). This is completely different from our work, since here we assume the existence of across-groups similarity values, which we eventually want to learn. On the other hand, the approach of Dwork et al. (2012) can be seen as an optimization problem, where the across-groups similarity values need to be computed in a way that minimizes some objective. Also, unlike our model, this optimization approach has a serious limitation, and that is requiring \(_{},_{^{}}\) to be explicitly known (recall that here we only need samples from these distributions).

Finally, since similarity as distance is difficult to compute in practice, there has been a line of research that defines similarity using simpler, yet less expressive structures. Examples include similarity lists Chakrabarti et al. (2022), graphs Lahoti et al. (2019) and ordinal relationships Jung et al. (2019).

## 4 Theoretical results

All proofs for this section can be found in the supplementary material 

### A simple learning algorithm

Given any confidence and accuracy parameters \(,(0,1)\) respectively, our approach is summarized as follows. At first, for every group \(\) we need a set \(S_{}\) of samples that are chosen i.i.d. according to \(_{}\), such that \(|S_{}|=}\). Then, for every distinct \(\) and \(^{}\), and for all \(x S_{}\) and \(y S_{^{}}\), we ask the expert oracle for the true similarity value \(_{,^{}}(x,y)\). The next observation follows trivially.

**Observation 4.1**.: The algorithm uses \(}\) samples, and \(}^{2}}\) queries to the oracle.

Suppose now that we need to compare any \(x_{}\) and \(y_{^{}}\). Our high level idea is that the properties \(_{1}\) and \(_{2}\) of \(_{,^{}}\) (see Section 2), will actually allow us to use the closest element to \(x\) in \(S_{}\) and the closest element to \(y\) in \(S_{^{}}\) as proxies. Thus, let \((x)=_{x^{} S_{}}d_{}(x,x^{})\) and \((y)=_{y^{} S_{^{}}}d_{^{}}(y,y^{})\). The algorithm then sets

\[f_{,^{}}(x,y)_{,^{}}((x),(y))\]

where \(_{,^{}}((x),(y))\) is known from the earlier queries.

Before we proceed with the analysis of the algorithm, we need to recall some notation which was introduced in Section 2.1. Consider any group \(\). An element \(x_{}\) with \(_{x^{}_{}}[d_{}(x,x^{})]<\) is called an \((,)\)-rare element of \(_{}\), and also \(p_{}(,)_{x_{}}[x(,) _{}]\).

**Theorem 4.2**.: _For any given parameters \(,(0,1)\), the simple algorithm produces similarity approximation functions \(f_{,^{}}\) for every \(\) and \(^{}\), such that_

\[[_{(,^{})}]=O(+p_{}(,)+ p_{^{}}(,))\]

_where \([_{(,^{})}]\) is as in Theorem 2.1._

Observation 4.1 and Theorem 4.2 directly yield Theorem 2.1.

A potential criticism of the algorithm presented here, is that its error probabilities depend on \(p_{}(,)\). However, Theorem 2.3 shows that such a dependence is unavoidable (see appendix for proof).

### Optimizing the number of expert queries

Here we modify the earlier algorithm in a way that improves the number of queries used. The idea behind this improvement is the following. Given the sets of samples \(S_{}\), instead of asking the oracle for all possible similarity values \(_{,^{}}(x,y)\) for every \(,^{}\) and every \(x S_{}\) and \(y S_{^{}}\), we would rather choose a set \(R_{} S_{}\) of representative elements for each group \(\). Then, we would ask the oracle for the values \(_{,^{}}(x,y)\) for every \(,^{}\), but this time only for every \(x R_{}\) and \(y R_{^{}}\). The choice of the representatives is inspired by the \(k\)-center algorithm of Hochbaum and Shmys (1985). Intuitively, the representatives \(R_{}\) of group \(\) will serve as similarity proxies for the elements of \(S_{}\), such that each \(x S_{}\) is assigned to a nearby \(r_{}(x) R_{}\) via a mapping function \(r_{}:S_{} R_{}\). Hence, if \(d_{}(x,r_{}(x))\) is small enough, \(x\) and \(r_{}(x)\) are highly similar, and thus \(r_{}(x)\) acts as a good approximation of \(x\). The full details for the construction of \(R_{}\), \(r_{}\) are presented in Algorithm 1.

Suppose now that we need to compare some \(x_{}\) and \(y_{^{}}\). Our approach will be almost identical to that of Section 4.1. Once again, let \((x)=_{x^{} S_{}}d_{}(x,x^{})\) and \((y)=_{y^{} S_{^{}}}d_{^{}}(y,y^{})\). However, unlike the simple algorithm of Section 4.1 that directly uses \((x)\) and \((y)\), the more intricate algorithm here will rather use their proxies \(r_{}((x))\) and \(r_{^{}}((y))\). Our prediction will then be

\[f_{,^{}}(x,y)_{,^{}}r_{ }(x),r_{^{}}(y)\]

where \(_{,^{}}(r_{}((x)),r_{^{}}((y)))\) is known from the earlier queries.

**Theorem 4.3**.: _For any given parameters \(,(0,1)\), the new query optimization algorithm produces similarity approximation functions \(f_{,^{}}\) for every \(\) and \(^{}\), such that_

\[[_{(,^{})}]=O(+p_{}(,)+p _{^{}}(,))\]

_where \([_{(,^{})}]\) is as in Theorem 2.1._

Since the number of samples used by the algorithm is easily seen to be \(}\), the only thing left in order to prove Theorem 2.4 is analyzing the number of oracle queries. To that end, for every group \([]\) with its sampled set \(S_{}\), we define the following Set Cover problem.

**Definition 4.4**.: Let \(_{x}^{}\{x^{} S_{}:d_{}(x,x^{}) 4 \}\) for all \(x S_{}\). Find \(C S_{}\) minimizing \(|C|\), with \(_{c C}_{c}^{}=S_{}\). We use \(OPT_{}\) to denote the optimal value of this problem. Using standard terminology, we say \(x S_{}\) is _covered_ by \(C\) if \(x_{c C}_{c}^{}\), and \(C\) is _feasible_ if it covers all \(x S_{}\).

**Lemma 4.5**.: _For every \([]\) we have \(|R_{}| OPT_{}\)._

**Lemma 4.6**.: _Let \(N=}\). For each group \([]\) we have \(OPT_{} N\) with probability \(1\), and \([OPT_{}]+p_{}(,)N\). The randomness here is over the samples \(S_{}\)._

The proof of Theorem 2.4 follows since all pairwise queries for the elements in the sets \(R_{}\) are

\[_{}|R_{}|_{^{}}|R_{^{}}| _{}OPT_{}_{^{}}OPT_{^ {}}\] (1)

where the inequality follows from Lemma 4.5. Combining (1) and Lemma 4.6 proves Theorem 2.4.

**Remark 4.7**.: _The factor \(8\) in the definition of \(H_{x}^{}\) at line 2 of Algorithm 1 is arbitrary. Actually, any factor \(=O(1)\) would yield the same asymptotic guarantees, with any changes in accuracy and queries being only of an \(O(1)\) order of magnitude. Specifically, the smaller \(\) is, the better the achieved accuracy and the more queries we are using._

Finally, we are interested in lower bounds on the queries required for learning. Thus, we give Theorem 2.5, showing that any algorithm with accuracy \(O()\) and confidence \(O()\) needs \((^{2}/^{2})\) queries. This immediately leads to the following corollary characterizing the optimality of our algorithm.

**Corollary 4.8**.: _When for every \([]\) the value \(p_{}(,)\) is arbitrarily close to \(0\), the algorithm presented in this section achieves an expected number of oracle queries that is asymptotically optimal._

## 5 Experimental evaluation

We implemented all algorithms in Python 3.10.6 and ran our experiments on a personal laptop with Intel(R) Core(TM) i7-7500U CPU @ 2.70GHz 2.90 GHz and 16.0 GB memory.

**Algorithms:** We implemented the simple algorithm from Section 4.1, and the more intricate algorithm of Section 4.2. We refer to the former as Naive, and to the latter as Cluster. For the training phase of Cluster, we set the dilation factor at line 2 of Algorithm 1 to \(2\) instead of \(8\). The reason for this, is that a minimal experimental investigation revealed that this choice leads to a good balance between accuracy guarantees and oracle queries. As explained in Section 3, neither the existing similarity learning algorithms (Ilvento, 2019; Mukherjee et al., 2020; Wang et al., 2019) nor the Earthmover minimization approach of Dwork et al. (2012) address the problem of finding similarity values for heterogeneous data. Furthermore, if the across-groups functions \(\) are not metric, then no approach from the metric learning literature can be used. Hence, as baselines we used three general regressionmodels; an MLP, a Random Forest regressor (RF) and an XGBoost regressor (XGB). The MLP uses 4 hidden layers of 32 relu activation nodes, and both RF and XGB use 200 estimators.

**Number of demographic groups:** All our experiments are performed for two groups, i.e., \(=2\). The following reasons justify this decision. At first, this case captures the essence of our algorithmic results; the \(>2\) case can be viewed as running the algorithm for \(=2\) multiple times, one for each pair of groups. Secondly, as the theoretical guarantees suggest, the achieved confidence and accuracy of our algorithms are completely independent of \(\).

**Similarity functions:** In all our experiments the feature space is \(^{d}\), where \(d\) is case-specific. In line with our motivation which assumes that the intra-group similarity functions are simple, we define \(d_{1}\) and \(d_{2}\) to be the Euclidean distance. Specifically, for \(\{1,2\}\), the similarity between any \(x,y_{}\) is given by

\[d_{}(x,y)=(x_{i}-y_{i})^{2}}\]

For the across-groups similarities, we aim for a difficult to learn _non-metric_ function; we purposefully chose a non-trivial function in order to challenge both our algorithms and the baselines. Namely, for any \(x_{1}\) and \(y_{2}\), we assume

\[(x,y)=|_{i} x_{i}-_{i} y_{i} +_{i}|^{3}}\] (2)

where the vectors \(,,^{d}\) are basically the hidden parameters to be learned (of course non of the learners we use has any insight on the specific structure of \(\)).

The function \(\) implicitly adopts a paradigm of feature importance (Nino-Adan et al., 2021). Specifically, when \(x\) and \(y\) are to be compared, their features are scaled accordingly by the expert using the parameters \(,\), while some offsetting via \(\) might also be necessary. For example, in the college admissions use-case, certain features may have to be properly adjusted (increase a feature for a non-privileged student and decrease it for the privileged one). In the end, the similarity is calculated in an \(_{3}\)-like manner. To see why \(\) is not a metric and why it satisfies the necessary properties \(_{1}\) and \(_{2}\) from Section 2, refer to the last theorem in the Appendix.

In each experiment we choose all \(_{i},_{i},_{i}\) independently. The values \(_{i},_{i}\) are chosen uniformly at random from \(\), while the \(_{i}\) are chosen uniformly at random from \([-0.01,0.01]\). Obviously, the algorithms do not have access to the vectors \(,,\), which are only used to simulate the oracle and compare our predictions with the corresponding true values.

**Datasets:** We used 2 datasets from the UCI ML Repository, namely Adult (48,842 points - 14 features) (Kohavi, 1996) and Credit Card Default (30,000 points - 23 features) (Yeh and Lien, 2009), and the publicly available Give Me Some Credit dataset (150,000 points - 11 features) (Credit Fusion, 2011). We chose these datasets because this type of data is frequently used in applications of issuing credit scores, and in such cases fairness considerations are of utmost importance. For Adult, where categorical features are not encoded as integers, we assigned each category to an integer in \(\{1,\#\}\), and this integer is used in place of the category in the feature vector (Ding et al., 2021). Finally, in every dataset we standardized all features through a MinMax re-scaler. Due to space constraints, the figures for Adult and Give me Some Credit are moved to the supplementary material. However, the observed traits there the same as the ones shown here for Credit Card Default.

**Choosing the two groups:** For Credit Card Default and Adult, we defined groups based on marital status. Specifically, the first group corresponds to points that are married individuals, and the second to points that are not married (singles, divorced and widowed are merged together). In Give Me Some Credit, we partition individuals into two groups based on whether or not they have dependents.

**Choosing the accuracy parameter \(\):** To use a meaningful value for \(\), we need to know the order of magnitude of \((x,y)\). Thus, we calculated the value of \((x,y)\) over \(10,000\) trials, where the randomness was of multiple factors, i.e., the random choices for the \(,,\), and the sampling of \(x\) and \(y\). Figure 1 shows histograms for the empirical frequency of \((x,y)\) over the \(10,000\) runs. In addition, in those trials the minimum value of \((x,y)\) observed was **1)**\(0.1149\) for Credit Card Default, **2)**\(0.1155\) for Adult, and **3)**\(0.0132\) for Give Me Some Credit. Thus, aiming for an accuracy parameter that is at least an order of magnitude smaller than the value to be learned, we choose \(=0.01\) for Credit Card Default and Adult, and \(=0.001\) for Give Me Some Credit.

**Confidence \(\) and number of samples:** All our experiments are performed with \(=0.001\). In Naive and Cluster, we follow our theoretical results and sample \(N=}\) points from each group. Choosing the training samples for the baselines is a bit more tricky. For these regression tasks a training point is a tuple \((x,y,(x,y))\). In other words, these tasks do not distinguish between queries and samples. To be as fair as possible when comparing against our algorithms, we provide the baselines with \(N+Q\) training points of the form \((x,y,(x,y))\), where \(Q\) is the maximum number of queries used in any of our algorithms.

**Testing:** We test our algorithms over \(1,000\) trials, where each trial consists of independently sampling two elements \(x,y\), one for each group, and then inputting those to the predictors. We are interested in two metrics. The first is the relative error percentage; if \(p\) is the prediction for elements \(x,y\) and \(t\) is their true similarity value, the relative error percentage is \(100|p-t|/t\). The second metric we consider is the absolute error divided by \(\); if \(p\) is the prediction for two elements and \(t\) is their true similarity value, this metric is \(|p-t|/\). We are interested in the latter metric because our theoretical guarantees are of the form \(|f(x,y)-(x,y)|=O()\).

Table 1 shows the average relative and absolute value error, together with the standard deviation for these metrics across all \(1,000\) runs. It is clear that our algorithms dominate the baselines since they exhibit smaller errors with smaller standard deviations. In addition, Naive appears to have a tiny edge over Cluster, and this is expected (the queries of Cluster are a subset of Naive's). However, as shown in Table 2, Cluster leads to a significant decrease in oracle queries compared to Naive, thus justifying its superiority. To further demonstrate the statistical behavior of the errors, we present Figure 2. This depicts the empirical CDF of each error metric through a bar plot. Specifically, the height of each bar corresponds to the fraction of test instances whose error is at most the value in the x-axis directly underneath the bar. Once again, we see that our algorithms outperform the baselines, since their corresponding bars are always higher than those of the baselines.

## Disclaimer

This paper was prepared for informational purposes in part by the Artificial Intelligence Research group of JPMorganChase. and its affiliates ("JP Morgan"), and is not a product of the Research Department of JPMorganChase. JPMorganChase makes no representation and warranty whatsoever and

 
**Algorithm** & **Average Relative Error \%** & **SD of Relative Error \%** & **Average (Absolute Error)/\(\)** & **SD of (Absolute Error)/\(\)** \\  Naive & 1.558 & 3.410 & 0.636 & 1.633 \\  Cluster & 1.593 & 3.391 & 0.646 & 1.626 \\  MLP & 3.671 & 4.275 & 1.465 & 1.599 \\  RF & 3.237 & 4.813 & 1.306 & 2.318 \\  XGB & 3.121 & 4.606 & 1.273 & 1.869 \\  

Table 1: Error statistics for Credit Card Default

Figure 1: Frequency counts for \((x,y)\)disclaims all liability, for the completeness, accuracy or reliability of the information contained herein. This document is not intended as investment research or investment advice, or a recommendation, offer or solicitation for the purchase or sale of any security, financial instrument, financial product or service, or to be used in any way for evaluating the merits of participating in any transaction, and shall not constitute a solicitation under any jurisdiction or to any person, if such solicitation under such jurisdiction or to such person would be unlawful.