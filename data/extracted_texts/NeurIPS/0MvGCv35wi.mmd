# Aegis2.0: A Diverse AI Safety Dataset and

Risks Taxonomy for Alignment of LLM Guardrails

Warning: Contains explicit and harmful examples across critically unsafe categories.

 Shaona Ghosh

NVIDIA

shaonag@nvidia.com

&Prasoon Varshney

NYIDIA

prasoonv@nvidia.com

&Makesh Narsimhan Sreedhar

makeshn@nvidia.com

&Aishwarya Padmakumar

NVIDIA

apadmakumar@nvidia.com

&Traian Rebedea

NVIDIA

trebedea@nvidia.com

&Jibin Rajan Varghese

NVIDIA

jibinv@nvidia.com

&Christopher Parisien

NVIDIA

cparisien@nvidia.com

\(Authorscontributedequally\)\(Authorscontributedequally\)\(Authorscontributedequally\)\(Authorscontributedequally\)\(Authorscontributedequally\)

###### Abstract

As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into \(12\) top-level hazard categories with an extension to \(9\) fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM "jury" system to assess the safety of responses, we obtain Aegis2.0, a carefully curated collection of \(30,947\) samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets. Additionally, we introduce a novel training blend that combines safety with topic following data. This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis2.0 data and models to the research community to aid in safety guardrailing of LLMs.

## 1 Introduction

Systems designed to ensure safe interactions between humans and large language models (LLMs) generally adopt one of two strategies. The first strategy includes alignment-based approaches likereinforcement learning from human feedback (RLHF) (Ouyang et al., 2022) and Constitutional AI (Bai et al., 2022b) which embed adherence to ethical guidelines within the model parameters. Both techniques are resource-intensive and require predefined classifications of harmful content. Despite these efforts, aligned models remain susceptible to various vulnerabilities (Bhardwaj and Poria, 2023; Varshney et al., 2023), and achieving an optimal balance between safety and helpfulness remains an active research challenge. The second strategy has been to use content moderation systems such as OpenAI's Content Moderation (Markov et al., 2023) and Google's Perspective API (Lees et al., 2022) which rely on classifiers with predefined safety labels. However, the closed-source nature of these systems limits their adaptability to emerging risks, including those related to self-harm and illegal activities.

More recent approaches to content moderation like Meta's Llama Guard (Inan et al., 2023) and Google's ShieldGemma (Zeng et al., 2024) look to improve flexibility in content moderation systems by leveraging the ability of LLMs to utilize their internal knowledge and zero-shot generalization capabilities to handle new safety risks. However, these models are trained on closed source datasets, limiting the possibility of iterating over them by the larger research community.

The development of Aegis2.0 addresses the need for a safety-focused dataset suitable for commercial applications, featuring a diverse collection of samples curated from a comprehensive taxonomy of harms. Our scalable content safety risk taxonomy, consisting of \(12\) core categories and \(9\) fine-grained risks, captures critical safety concerns in human-LLM interactions. Designed for flexibility, the taxonomy allows human annotators to provide free-text input for unclassified risks, which are later standardized into fine-grained categories, enabling the discovery of new hazards and ensuring scalability without predefined constraints. The dataset includes a variety of prompts covering critical risks, adversarial jailbreaks, and cultural contexts, with responses generated by unaligned LLMs. Annotations are performed at the dialogue level, with prompt and response labels extracted using weak supervision from a "jury" of LLMs, aligned with human judgments. Through this process, Aegis2.0 is created, and models fine-tuned on this data demonstrate performance comparable to recent models like WildGuard(Han et al., 2024) that have been trained on larger datasets that leverage powerful, non-commercial sources like GPT4.

Our key contributions are as follows:

* We define an extensive and scalable content safety risk taxonomy that identifies \(12\) core categories and an additional \(9\) fine-grained risks. The taxonomy encompasses the most pertinent safety risks encountered in interactions between humans and LLMs.
* The taxonomy was uniquely designed to be scalable and flexible. As part of the human annotation exercise, we facilitated new risk discoverability by allowing annotators to add free-text input, if the content does not belong to the predefined taxonomy. All collected free-text was later standardized into the \(9\) fine-grained categories in our taxonomy. This enabled (i) appropriate handling of any deficiencies in annotation guidelines and (ii) new hazard discoverability to flexibly scale the taxonomy without exhaustively defining it a priori.
* The prompts in our dataset are curated to ensure coverage over diverse critical risks, adversarial jailbreaks, and geographical and cultural risks. These prompts are then used to generate synthetic responses from unaligned open-source LLMs at scale, complementing dialog level human annotations with response level annotations from a "jury" of LLMs.
* Our experimental results show that parameter-efficient fine-tuning on the Aegis2.0 using Llama3.1-8B-Instruct as a base model surpasses LlamaGuard3-8B- a model that is instruction-tuned on the same starting backbone, and performs at par with the state-of-the-art WildGuard model.
* We investigate the parallels between topic following (Sreedhar et al., 2024) and content moderation tasks and show that training on a combined blend of dialogue topic following and safety data can add robustness to safety models and enable better adherence to novel safety policies.

## 2 Related Work

As LLM safety is becoming an area of growing research and commercial interest, there are an increasing number of datasets available to benchmark LLM safety for evaluation. However many of these are of small size and not intended to be used for training content moderation models [Rottger et al., 2023, Markov et al., 2023, Mazeika et al., 2024]. An earlier available dataset suitable for training safety classifiers is ToxicChat [Lin et al., 2023], but its use of Vicuna [Zheng et al., 2023] for generating responses limits commercial use due to licensing of the ShareGPT 4 data used to train it. A more recent dataset for content moderation is WildGuardMix[Han et al., 2024] which covers wide-ranging safety risks, response refusals, and adversarial jailbreak data to have a total of \(92\)K samples. However, \(85\%\) of its training split (WildGuardTrain) is generated using GPT4, the use of which constrains commercial use.

Additionally, both ToxicChat and WildGuardTrain do not include annotated categories of safety hazards, thus providing a binary label annotation only. We argue that prediction of categories from a diverse taxonomy is important in production use cases in a Guardrails system [Rebedea et al., 2023], as the orchestration layer usually needs to generate a reason to relay to the user on why a request was blocked. Finally, a topic modeling based Grootendorst  analysis of the WildGuardMix dataset shows that important safety risk categories such as "Sexual abuse in Children" and "Suicide and Self-Harm" are not well represented in it (more details in Appendix C.6). These are extremely critical to moderate and have direct implications to mental health crisis and crimes against children. Another recent dataset similar to ours is BeaverTails [Ji et al., 2024] which includes human annotations over a safety taxonomy of 14 categories, broadly aligned with ours (see Appendix C.1). However BeaverTails also poses issues for commercial use due to employing Alpaca-7B Taori et al. for response generation, which is trained using Self-Instruct [Wang et al., 2022] style data generated from OpenAI models. Additionally, our dataset includes more sources of adversarial prompts [Shen et al., 2024, Wang et al., 2023, Radharapu et al., 2023] which we expect to result in more robust content moderation models.

Meta's Llama Guard [Inan et al., 2023] was one of the first content moderation LLMs, and the more recent LlamaGuard3-8B, based on the Llama 3.1 family of models [Llama Team, 2024a] is the latest in a series of content moderation models by Meta. However, the Llama Guard family of models are instruction-tuned on an unreleased internal dataset. Our experimental results show that parameter-efficient fine-tuning on the Aegis2.0 using Llama3.1-8B-Instruct as a base model surpasses LlamaGuard3-8B, a model that is instruction-tuned with the same starting backbone, providing evidence of the utility of Aegis2.0 as a training blend. ShieldGemma [Zeng et al., 2024] from Google is also trained on a closed dataset and covers a safety risk taxonomy of only \(4\) categories.

  
**Dataset** & **Train Split** & **Adversarial Prompts** &  **Human Labeled** \\ **Train Set** \\  &  **Human Labeled Risk** \\ **Labeled Risk** \\ **Categories** \\  & 
 **Commer-** \\ **cial Use** \\ **For** \\ **Training** \\  \\   XSTest [Rottger et al., 2023] & ✗ & ✓ & - & ✗ & - \\ OpenAI Mod. [Markov et al., \\
2023] & ✗ & ✗ & - & ✓ & - \\ HarmBench [Mazeika et al., 2024] & ✗ & ✓ & - & ✗ & - \\ ToxicChat [Lin et al., 2023] & ✓ & ✗ & ✓ & ✗ & ✗ \\ WildGuardMix [Han et al., \\
2024] & ✓ & ✓ & ✗ & ✗ & ✗ \\ BeaverTails [Ji et al., 2024] & ✓ & ✗ & ✓ & ✓ & ✗ \\  Aegis2.0 (Ours) & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Aegis2.0 is the first content moderation training dataset fully suitable for commercial use. It sources prompts from diverse datasets including datasets of adversarial prompts and obtains responses from a model suitable for commercial use, Mistral-7B-v0.1 [Jiang et al., 2023]. It includes human annotated safety labels on all data splits, including fine grained risk categories.

In addition, it is difficult to adjust ShieldGemma to novel safety policies on the fly as it is optimized to handle one policy at a time with multiple inferences needed to handle several risk categories. More recently, several content moderation LLMs (Ji et al., 2024; Han et al., 2024) have been trained using the WildGuardTrain and BeaverTails datasets. Therefore we compare our results to these models as well in our experiments.

## 3 Content Safety Risk Taxonomy

We define an extensive and scalable content safety risk taxonomy comprising 12 core categories and 9 additional fine-grained risks, as outlined in Table 2. Our taxonomy is informed by leading LLM safety and content moderation frameworks, including OpenAI's Content Moderation API5, Google's Perspective API6, Llama Guard (Inan et al., 2023), and the MLCommons AI Safety Benchmark (Vidgen et al., 2024). We begin by selecting categories that directly overlap with these established taxonomies, focusing on the most relevant risk areas to simplify evaluation and ensure consistency. These overlapping categories form the basis of our core risk categories. Notably, samples can be assigned to multiple risk categories, allowing for comprehensive risk representation.

Our aim during annotation was to develop a scalable, tiered taxonomy that allows for revisiting policy definitions, minimizing errors, and discovering new risks. In addition to the core risk categories, we included a top-tier category, called Other, which annotators selected when no predefined category applied under the given policy. For instances labeled as Other, annotators were asked to choose from a set of potentially unsafe categories not yet included in the taxonomy or to provide a free-text description of the most relevant hazard, along with an explanation. As demonstrated by Zhang et al. (2023), free-text explanations significantly improve the identification of subtle unsafe content. This approach facilitates risk discovery, enhances scalability, and supports policy updates. We later integrate these free-text annotations into the fine-grained categories outlined in Table 2.

In addition to deciding whether samples are Safe or violate specific risk categories, annotators were given the option to label ambiguous instances as Needs Caution, in order to prevent the unnecessary classification of uncertain content as unsafe. By incorporating this label, we enable the design of a system that can either adopt a defensive approach - blocking the request or response - or remain permissive while still being helpful, depending on how the Needs Caution designation is interpreted.

## 4 Creation of Aegis2.0 Dataset

Aegis2.0 comprises a diverse collection of benign and adversarial prompts, covering both safe and harmful scenarios, alongside LLM-generated responses. Unlike other safety datasets (Han et al., 2024), which rely on synthetic prompt generation, we source potentially harmful prompts from

    \\   &  \\  Hate/Identity Hate & Sexual & Illegal Activity \\ Suicide and Self Harm & Violence & Immoral/Unethical \\ Guns/Illegal Weapons & Threat & Unauthorized Advice \\ PII/Privacy & Sexual Minor & Political/Misinformation/Conspiracy \\ Criminal Planning/Confessions & Harassment & Fraud/Deception \\ Controlled/Regulated substances & Profanity & Copyright/Trademark/Plagiarism \\ Other & & High Risk Gov. Decision Making \\  & & Malware \\  & & Manipulation \\   

Table 2: Content safety risk taxonomy for Aegis2.0 dataset. Conversation labels from human annotators are standardized to one of Safe, Needs Caution or one of the risk categories listed here.

real-world interactions. To ensure prompt diversity, we selected prompts from Anthropic/hh-r1hf dataset , Do-Anything-Now DAN , AI-assisted Red-Teaming (AART) , and Do-Not-Answer 

For each of the selected prompts, we generated responses using Mistral-7B-v0.1 . The coherence of synthetically generated responses was not validated, as the primary goal was to curate a balanced set of benign and unsafe LLM responses. Upon inspection of the model responses, we found that Mistral-7B-v0.1 chose to engage and comply with many harmful user queries. Current state-of-the-art LLMs  that include safety alignment as part of post-training do not easily engage with harmful user queries and refuse to respond and redirect the conversation. In order to include such refusal samples as part of our dataset, we generate responses that avoid engaging with harmful queries using Gemma-2-27B  prompted to refuse unsafe prompts.

The final dataset comprises 35,947 samples, including 16,954 standalone prompts, 17,225 prompt and single-turn response pairs (of which 5,000 are synthetic refusals), and 1,768 multi-turn examples. We also provide a train-test split of the dataset, by selecting 1,984 samples with stratified sampling, to be able to test the performance of other models on our test dataset and to ensure a correct usage of Aegis2.0 dataset when comparing different models. More statistics of the dataset are included in C.7.

### Data Annotation

A team of 12 annotators, supported by a data quality team, conducted the annotation process. Each instance received at least three annotations, resulting in a total of 86,736 annotations. For quality assurance, the dataset was divided into 11 chunks, each containing 1,000 to 3,000 samples, with 10-15% of each chunk audited for data quality. Additionally, the research and engineering teams ran automated tests every few days to ensure data quality. Annotations were applied at the dialogue level, rather than separately for prompts and responses.

Inter-annotator agreement reached approximately 74%. The research and data teams maintained close communication throughout the project to provide immediate feedback and address any issues. Further details on ethical considerations, the annotation process, task instructions, and guidelines can be found in C.3, along with a sample of data generated by Mistral and corresponding annotations.

#### 4.1.1 Splitting the Conversation-level Annotation across Prompt and Response

We obtain a binary safe/unsafe conversation level majority vote from human labels, which is also used for prompt classification. Recognizing that if either the prompt itself was unsafe, or the prompt was of a type that solicited a harmful response from an LLM (for example jailbreaking attempts that would otherwise be marked as safe under a safety taxonomy alone), then, the prompt should be marked as unsafe.

For response classification, if the conversation-level human vote is safe, we use the label as-is, however, if the vote is unsafe, then we use a separate jury of LLM judges (see SS4.1.2) to ascertain whether the response is harmful or not, recognizing that if the conversation was marked unsafe by annotators because the user turn was unsafe, then the response might be a refusal instead of complying with the user turn. And in case of a refusal, the response label should be marked safe.

#### 4.1.2 Synthetic Response Label Annotations Using Jury-of-LLM Evaluators

When deploying safety guard models in end-user applications, it is important for the model to make predictions at the turn level, especially when distinguishing between safe and unsafe responses to unsafe prompts. Given that our dataset includes annotations at the dialogue level, we investigate the effectiveness of using LLM annotations to assess safety at the individual response level.

We obtain safety annotations for responses in our dataset from three different LLMs: Mistral-8x22B-v0.17, Mistral-NeMo8, and Gemma-2-27B-it Team . We selected the optimal ensemble of LLMs and prompt templates based on the correlation between the predicted labels and those from WildGuard9. Each LLM was instructed to generate a response JSON containing a binary safe/unsafe prediction and, if unsafe, a list of harm categories. The final labels were determined by a majority vote on the safe/unsafe classification and the union of harm categories predicted by the three LLMs. These annotations were especially useful in identifying cases where LLMs refused to engage with prompts containing or soliciting harmful content. More details about the response labels including prompt templates used are included in Appendix C.8.

### Refusal Data Generation

Recent advancements in model alignment pipelines have emphasized safety alignment as a critical component of post-training procedures to mitigate harmful interactions. LLMs are designed to avoid engaging with user inputs that are malicious, unsafe, or harmful. When presented with such queries, these models typically decline to respond directly or attempt to steer the conversation toward safer topics, thus prioritizing responsible usage. However, earlier models like Mistral-7B-v0.1 tend to engage with and comply with a significant number of harmful queries, leading to an underrepresentation of refusal and redirection strategies in AEGis2.0.

To address this imbalance, we generate synthetic refusals and redirections using Gemma-2-27B . The model is explicitly prompted to avoid engaging with harmful queries, following predefined strategies to produce diverse deflection responses. These strategies include direct refusals, offering alternative forms of assistance, explaining potential negative consequences, providing educational insights, and reframing the conversation toward safer topics. Using this method, we generate 5,000 prompt-response pairs, which are incorporated into AEGis2.0.

## 5 Safety Guard Models on AEGis2.0

We train safety guard models by performing parameter-efficient fine-tuning (PEFT) with Llama3.1-8B-Instruct as our backbone. The model is trained on the AEGis2.0 train split using the majority label inferred from the human and LLMs annotations to predict a label of safe/unsafe for each of the prompt and response. Details about the training setup and hyperparameters can be found in Appendix C.4.4.

We compare the performance of our best model Llama3.1-AEGisGuard against industry baselines in Table 3. Llama3.1-AEGisGuard outperforms LlamaGuard3-8B , which is instruction-tuned by Meta on an internal dataset using the same base model, as well as LlamaGuard3-1B , LlamaGuard2-8B , and the OpenAI Mod API10. Additionally, it performs on par with WildGuard in terms of average harmfulness F1 scores across WildGuardTest , XSTest , Han et al., 2024), and the OpenAI Moderation Dataset . Results for our models are reported as an average over three runs, while Table 7 also includes the standard deviations based on three different random seed trials.

We additionally notice from ablations that the binary safe/unsafe prediction performance improves with the adding fine-grained categories in the prompt template, compared to the core category taxonomy alone. This can be attributed to the fact that the WildGuardMix dataset contains many fine-grained risks like phishing, malware, and unauthorized advice, and privacy issues that are not present in the core categories. This demonstrates that a more detailed taxonomy not only enhances Llama3.1-AEGisGuard's ability to predict specific hazard categories but also improves its accuracy in distinguishing between safe and unsafe examples. We additionally notice that using weakly supervised (\(-\) Jury of LLMs) instead of the conversation-level annotations for responses substantially boosts performance on response moderation. Additionally, adding refusal data (SS4.2) and topic following data (SS6) also provide important increases in performance. At last, Table 4 shows that all the baselines are lacking in performance when used out-of-distribution on AEGis2.0 test split.

### Category Prediction Performance

While achieving performance on par with state-of-the-art moderation performance for binary safety predictions, our models also accurately predict the hazard categories based on which the user prompt or the model response was unsafe based on the Aegis2.0 taxonomy with accuracy as high as \(94\%\).

For the OpenAI Mod API, the task of category prediction becomes a multi-class classification problem since the ground truth in the OpenAI Mod dataset can have multiple categories. Heatmap visualizations capture model performance in a convenient manner and Figure 1 shows the heatmap for OpenAI Mod.

To calculate numeric accuracy, we rely on the simplifying assumption that categories within safety taxonomies often overlap. For example, content containing profane or disturbing language may also qualify as hate speech or violence. Based on this intuition, we compile and group fine-grained categories into broader themes. We provide this map in the Appendix C.9. Based on this grouping, we assess the accuracy of category predictions for unsafe samples, labeling them as correct or incorrect depending on whether they fall within the same theme as the ground truth. The heatmap on the left in Figure 1 illustrates this collapsed version showing that our model performs well in predicting unsafe categories.

We include plots comparing the distributions of categories predicted by our model against ground truth categories in OpenAI Mod and WildGuardTest in AppendixC.6 which further fortify the idea that the categorical predictions from our models are good, as they match the underlying distributions and the aforementioned top categories in each dataset.

## 6 Improving Content Moderation via Topic Following

In this section, we elaborate on combining topic-following with safety data and examine the impact of training on this data blend on content safety classification.

    &  &  &  **Un-weighted** \\ **Average** \\ **Across** \\ **Datasets** \\  } \\
**Evaluation Dataset-\(\)** & OAI Mod & WGTest & WGTest & XSTest & 
 **Un-weighted** \\ **Average** \\ **Across** \\ **Datasets** \\  \\   OpenAI Mod API & \(0.789\) & \(0.121\) & \(0.214\) & \(0.558\) & \(0.385\) \\  LLAMeGuard2-8B & \(0.759\) & \(0.704\) & \(0.658\) & \(0.908\) & \(0.723\) \\  LLAMeGuard3-1B & \(0.374\) & \(0.472\) & \(0.261\) & \(0.245\) & \(0.359\) \\  LLAMeGuard3-8B & \(0.788\) & \(0.768\) & \(0.700\) & \(0.904\) & \(0.764\) \\  BeaverDam\(\) & – & – & \(0.634\) & \(0.836\) & – \\  WildGuard\(\) & \(0.721\) & \(\) & \(0.754\) & \(\) & \(\) \\    \\   LLAMa3.1-AegisGuard + TF (86) & \(\) & \(0.816\) & \(\) & \(0.862\) & \(0.816\) \\  LLAMa3.1-AegisGuard & \(0.770\) & \(0.821\) & \(0.757\) & \(0.883\) & \(0.808\) \\  – Refusal Data (§4.2) & \(0.759\) & \(0.833\) & \(0.771\) & \(0.847\) & \(0.803\) \\  – Fine-Grained Risks (§3) & \(0.789\) & \(0.816\) & \(0.753\) & \(0.789\) & \(0.787\) \\  – LLM Jury Labels (§4.1.2) & \(0.793\) & \(0.787\) & \(0.511\) & \(0.521\) & \(0.653\) \\   

Table 3: Performance on out-of-domain benchmarks against baselines. Mean harmfulness F1 scores over 3 random seeds reported. Note that WGTest and XSTest are in-domain for WildGuard and OpenAI Mod is in-domain for OpenAI Mod API. Double dashes (\(--\)) represent a nested ablation.

### Topic-Following as Dialogue Moderation

Topic-following (TF) is a task that evaluates instruction-tuned large language models (LLMs) on their ability to follow detailed guidelines in task-oriented dialogues, as introduced by Sreedhar et al. (2024). Although primarily designed to train and evaluate chatbots for task-oriented interactions, TF can be viewed as a form of dialogue moderation with rules on allowed topics, conversation flow, and style. The dataset includes both on-topic (safe) turns and off-topic distractors (unsafe) with 1,080 multi-turn dialogues across nine domains. Similar to the task of content moderation, the model must decide for each user turn whether to engage with the query or deflect from responding based on its compliance to the dialogue task at hand. More details about these prompts and categories can be found in C.10.2.

Models trained on the TF task have demonstrated strong zero-shot performance in LLM safety moderation (Sreedhar et al., 2024), achieving results comparable to specialized safety-tuned models like LlamaGuard (Inan et al., 2023). Building on these findings, we want to explore the enhancements in safety moderation achieved by training on a combined dataset of TF and safety-specific samples.

### Safety Robustness with Topic Following

The TF dataset primarily involves classification decisions on whether to engage with the current user turn, closely resembling the prompt classification task in content moderation. Therefore, we evaluate models trained on the combined dataset using benchmarks previously applied in this context. Furthermore, since topic-following introduces data on adapting to various scenarios and conversational settings, we aim to investigate whether this improves performance in handling new safety categories specified at run-time. Specifically, we assess the model's ability to adapt to new

  
**Aegis2.0 Test Set** & **Prompt Classification** & **Response Classification** \\   OpenAI Mod API & \(0.378\) & \(0.474\) \\  LlamaGuard2-8B & \(0.768\) & \(0.674\) \\  LlamaGuard3-1B & \(0.496\) & \(0.529\) \\  LlamaGuard3-8B & \(0.773\) & \(0.657\) \\  WildGuard & \(0.819\) & \(0.835\) \\   _Ours_ & & \\   Llama3.1-AegisGuard & \(0.868\) & \(0.866\) \\  — Refusal Data (4.2) & \(\) & \(\) \\  

Table 4: Performance on our Aegis2.0 test split, scores are reported over 3 random seeds.

Figure 1: Heatmap of ground truth vs. category predictions on the OpenAI Moderation Dataset. **Left:** Summarized version collapsing categories into allowed, other, and safe. **Right:** Detailed version showing predicted (x-axis) against OpenAI taxonomy (y-axis). All abbreviations used are listed in Appendix C.9, Tables 10 and 11.

policy categories not included in the training taxonomy. For this evaluation, we introduce four new categories -- Financial, Medical, and Legal Advice -- as well as prompts related to NSFW generation from multimodal models that are not part of the safety policy used for training. These categories cover user prompts that seek advice or make unhinged, or controversial statements related to these topics. We synthetically generate prompts that violate the guidelines for each category, alongside positive examples that adhere to the guidelines and do not constitute violations. More details can be found in Appendix C.10.1 and Appendix C.11.

### Results on using Topic-Following with Aegis2.0

The results in Table 3 indicate that integrating the TF component boosts the model's overall performance in out-of-domain prompt classification tasks such as the challenging OpenAI Mod. On the safety evaluation benchmarks, the Llama3.1-AegisGuard + TF gets slightly higher scores than the Llama3.1-AegisGuard. However, the key advantage of the TF-enhanced model is its adaptability to the newly introduced categories -- Financial, Medical, and Legal Advice as well as prompts related to NSFW generations from multimodal models -- which were not part of the training policy. The results for these categories can be found in Table 5. In these categories, the Llama3.1-AegisGuard + TF shows substantial improvements, suggesting that the combination of dialogue and content moderation enhances the model's ability to generalize and adapt more effectively to new categories defined after training. More details about the content moderation setting for multimodal image generation can be found in Appendix C.11.

## 7 Conclusion

This paper introduces Aegis2.0, a dataset usable by commercial applications designed to address diverse safety risks in large language models using a taxonomy of \(12\) core and \(9\) fine-grained risk categories. By leveraging a hybrid approach that combines human and LLM-generated annotations, we demonstrate the effectiveness of the Aegis2.0 dataset by using it to train the Llama3.1-AegisGuard, which performs at par with the WildGuard on the WildGuardTest set and substantially outperforms it on the OpenAI Mod, all while using a much smaller training dataset and only open-source, commercial-usable LLMs for weak supervision, unlike the use of GPT4 for supervision in the WildGuardTrain dataset. We also show substantially improved performance compared to other baselines like LlamaGuard3-8B and OpenAI Mod API, providing conclusive evidence for the utility of Aegis2.0 for training content moderation models. Thus, we hope that the release of Aegis2.0 and associated AegisGuard models offers valuable resources for advancing LLM safety systems.

Our ablation studies show improved performance on inclusion of fine-grained risk categories, as compared to only the core categories in our taxonomy, providing evidence of the benefit of allowing annotators to use a flexible free-text input on unsafe samples that did not fit into the core categories. Finally, our experiments on augmenting our content moderation data with topic following dialogue moderation data show enhanced model robustness and improved performance on prompt safety tasks. Moreover, TF-improved models are much more adaptable to new risk categories not part of the safety datasets.

Future extensions to this work, based on the limitations discussed in Appendix B, include expanding the dataset to include responses from more LLMs and increasing representation of important risk categories that are currently underrepresented, expanding to multiple languages, and addition of more types of prompts designed to jailbreak moderation models to further enhance robustness.

    &  \\
**Evaluation Dataset** & Financial & Legal & Medical & NSFW \\   Llama3.1-AegisGuard & \(0.722\) & \(0.875\) & \(0.895\) & 0.699 \\  Llama3.1-AegisGuard + TF & \(\) & \(\) & \(\) & \(\) \\   

Table 5: Content moderation performance shows that models trained on Aegis2.0 + TF help improve performance for new categories defined during inference.