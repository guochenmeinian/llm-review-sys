# What functions can Graph Neural Networks compute on random graphs? The role of Positional Encoding

Nicolas Keriven

CNRS, IRISA, Rennes, France

nicolas.keriven@cnrs.fr &Samuel Vaiter

CNRS, LJAD, Nice, France

samuel.vaiter@cnrs.fr

###### Abstract

We aim to deepen the theoretical understanding of Graph Neural Networks (GNNs) on large graphs, with a focus on their expressive power. Existing analyses relate this notion to the graph isomorphism problem, which is mostly relevant for graphs of small sizes, or studied graph classification or regression tasks, while prediction tasks on _nodes_ are far more relevant on large graphs. Recently, several works showed that, on very general random graphs models, GNNs converge to certains functions as the number of nodes grows. In this paper, we provide a more complete and intuitive description of the function space generated by equivariant GNNs for node-tasks, through general notions of convergence that encompass several previous examples. We emphasize the role of input node features, and study the impact of _node Positional Encodings_ (PEs), a recent line of work that has been shown to yield state-of-the-art results in practice. Through the study of several examples of PEs on large random graphs, we extend previously known universality results to significantly more general models. Our theoretical results hint at some normalization tricks, which is shown numerically to have a positive impact on GNN generalization on synthetic and real data. Our proofs contain new concentration inequalities of independent interest.

## 1 Introduction

Machine learning on graphs with Graph Neural Networks (GNNs)  is now a well-established domain, with application fields ranging from combinatorial optimization  to recommender systems , physics , chemistry , epidemiology , physical networks such as power grids , and many more. Despite this, there is still much that is not properly understood about GNNs, both empirically and theoretically, and their performances are not always consistent , compared to simple baselines in some cases. It is generally admitted that a better theoretical understanding of GNNs, especially of their fundamental limitations, is necessary to design better models in the future.

Theoretical studies of GNNs have largely focused on their _expressive power_, kickstarted by a seminal study  that relates their ability to _distinguish non-isomorphic graphs_ to the historical Weisfeiler-Lehman (WL) test . Following this, many works have defined improved versions of GNNs to be "more powerful than WL" , often by augmenting GNNs with various features, or by implementing "higher-order" versions of the basic message-passing paradigm. Among the simplest and most effective idea to "augment" GNNs is the use of _Positional Encodings_ (PE) as input to the GNN, inspired by the vocabulary of Transformers . The idea is to equip nodes with carefully crafted input features that would help break some of the indeterminancy in the subsequent message-passing framework. In early works, unique and/or random node identifiers have been used , but they technically break the permutation-invariance/equivariance - consistency with a reordering of the nodes in the graph - of the GNN. Most PEs in the current literature are based on eigenvectors of the adjacency matrix or Laplacian of the graph  (with recent variants to handle the sign/basis indeterminancy), random-walks , node metrics [56; 30], or subgraphs . Some of these have been shown to have an expressive power beyond WL [30; 4; 31].

In some contexts however, WL-based analyses have limitations: they pertain to tasks on graphs (e.g. graph classification or regression) and have limited to no connections to tasks on nodes; and they are mostly relevant for small-scale graphs, as _medium or large graphs have a negligible chance of being exactly isomorphic to one another_, but exhibit different characteristics (e.g. community structures) that might be useful for learning. At the other end of the spectrum, the properties of GNNs on _large_ graphs have been analysed in the context of latent positions Random Graphs (RGs) [24; 25; 43; 44; 29; 2; 36], a family of models slightly more general than graphons . Such statistical models of large graphs are classically used in graph theory [18; 9] to model various data such as epidemiological [27; 39], biological , social , or protein-protein interaction  networks, and are still an active area of research . For GNNs, the use of such models has shed light on their stability to deformations of the model [44; 29; 24], expressive power , generalisation [15; 36], or some phenomena such as oversmoothing [23; 3]. One basic idea is that, as the number of nodes in a random graph grows, GNNs converge to "continuous" equivalents [24; 8], whose properties are somewhat easier to characterize than their discrete counterpart. As prediction tasks on _nodes_ are far more common and relevant on large graphs modelled by random graphs, this paper will focus on _permutation-equivariant_ GNNs, rather than permutation-invariant. In the limit, it has been shown that their output converge to _functions_ over some latent space to label the nodes, but the descriptions of this space of functions and its properties are still very much incomplete. A partial answer was given in , in which some universality properties are given for specific models of GNNs, but for limited models of random graphs _with no random edges_, and specific models of GNNs that no not include node features or PEs. We will in particular extend some of their results to random edges, with the proper choice of PEs.

Contributions.In this paper, we significantly extend existing results by providing a **complete description of the function space generated by permutation-equivariant GNNs** (Theorem 1), in terms of simple stability rules, and show that it is equivalent to previous implicit definitions that were based on convergence bounds. We outline the **role of the input node features**, and particularly of Positional Encodings (PEs). We then study the several representative examples of PEs on large random graphs. In particular, we analyze SignNet  (eigenvector-based) PEs (Theorem 2), and distance-based PEs  (Theorem 3). We derive simple normalization rules that are necessary for convergence, and illustrate them on real data. Finally, our proofs contain new universality results for square-integrable functions and new concentration inequalities that are of independent interest. All technical proofs are available in the Appendix. The code to reproduce the figures can be found at https://github.com/nkeriven/random-graph-gnn.

## 2 Background on Random Graphs and Graph Neural Networks

Let us start with generic notations and definitions. The norm \(\) is the Euclidean norm for vectors and the operator norm for matrices and compact operators between Hilbert spaces. The latent space \(\) is a compact metric set with a probability distribution \(P\) over it. Square-integrable functions from \(\) to \(^{q}\) w.r.t. \(P\) are denoted by \(L_{q}^{2}\), and are equipped with the filtration norm \( f_{L^{2}}^{2}}}{{=}} _{} f(x)^{2}dP(x)\). The (disjoint) union of multidimensional functions \(L_{}^{2}}}{{=}}_{q }\), \(L_{q}^{2}\) is a metric space for a metric defined as \( f-g_{L^{2}}\) if \(f,g L_{q}^{2}\) for some \(q\), and \(1\) otherwise. Continuous _Lipschitz_ functions between metric spaces \(\) are denoted by \(_{}(,)\). For \(X=\{x_{1},,x_{n}\}\) where \(x_{i}\), we define the sampling of \(f:^{d}\) as \(_{X}f=[f(x_{i})]_{i=1}^{n}^{n d}\). Given \(Z^{n d}\), the Frobenius norm is \( Z_{}\) and we define the normalized Frobenius norm as \( Z_{}=n^{-} Z _{}\). The notation comes from the fact that \(_{X}(f-f^{})_{}^{2}=n^{-1}_{i}  f(x_{i})-f^{}(x_{i})^{2}\) which is akin to a Mean Square Error.

Latent position Random Graphs.In this paper, we consider _latent position random graphs_[20; 28; 33], a family of models that includes Stochastic Block Models (SBM), graphons, random geometric graphs, and many other examples. They are the primary models used for the study of GNNs in the literature [24; 25; 29; 43]. We generate a graph \(G=(X,A,Z)\), where \(X^{n d}\) are _unobserved_ latent variables, \(A\{0,1\}^{n n}\) its symmetric adjacency matrix, and \(Z^{n p}\) are (optional) observed node features. The latent variables and adjacency matrix are generated as such:

\[ i,\;x_{i}}{{}}P, i<j,\; a_{ij}(_{n}w(x_{i},x_{j}))\] (1)where \(w:\) is a continuous _connectivity kernel_ and \(_{n}\) is the _sparsity-level_ of the graph, such that the expected degrees are in \((n^{2}_{n})\). Non-dense graph can be obtain with \(_{n}=o(1)\), here we will go down to the _relatively sparse_ case \(_{n}( n)/n\), a classical choice in the literature . Note that the continuity hypothesis of the kernel \(w\) is not really restrictive: neither \(\) nor the support of the distribution \(P\) need be connected. For instance, SBMs can be obtained by taking \(\) to be a finite set. We do not specify a model for the node features yet, see Sec. 4.

Graph shift matrix and operator.When the number of nodes \(n\) grows on random graphs, it is known that certain discrete operators associated to the graph converge to their continuous version, as well as the GNNs that employ them . Here, some of our results will be valid under quite general assumptions, hence we use generic notations for our graph representations. When the results are only valid for particular examples, this will be specifically expressed.

We consider a **graph shift matrix**\(S=S(G)^{n n}\), which can be either directly the adjacency matrix of the graph or various notions of graph Laplacians. We define an associated **graph shift operator**\(:L_{}^{2} L_{}^{2}\) such that the restriction \(_{|L_{q}^{2}}\) is a compact linear operator of \(L_{q}^{2}\) onto itself. Note that we reserve "matrix" and "operator" respectively for the discrete and continuous versions. The results in Sec. 3 will be valid under generic convergence assumptions from \(S\) to \(\), while the results of Sec. 4 will focus on the following two representative examples.

**Example 1** (Normalized adjacency matrix and kernel operator).: _Here \(S==(n_{n})^{-1}A\) and \(f=f= w(,x)dP(x)\). This choice requires to know, or estimate, the sparsity level \(_{n}\). In this case, our results will hold whenever \(_{n}( n)/n\) with an arbitrary multiplicative constant._

Note that this choice requires to know (or estimate) the parameter \(_{n}\), otherwise we will not have convergence between \(S\) and \(\), which can be limiting. This is not the case of the next example.

**Example 2** (Normalized Laplacian matrix1 and operator).: _Here \(S=L=D_{A}^{-1/2}AD_{A}^{-1/2}\) where \(D_{A}=(A1_{n})\) is the degree matrix of \(G\), and \(f=f=}dP(x)\) where \(d()= w(,x)dP(x)\) is the degree function. Whenever we opt for this choice, we assume that \(d_{}}}{{=}}_{}d(x)>0\), and our results will hold whenever \(_{n} C( n)/n\) with a multiplicative constant \(C\) that depends (in a non-trivial way) on \(w\), see Thm. 9 in App. D._

To sometimes unify notations, when we adopt these examples, we define \(w_{}\) such that \(w_{}(x,y)=w(x,y)\) in the adjacency case and \(w_{}(x,y)=}\) in the normalized Laplacian case. Therefore for these two examples the continuous operator has a single expression \(f= w_{}(,x)dP(x)\).

Graph Neural Network.As mentioned in the introduction, we focus on _equivariant_ GNNs that can compute functions over _nodes_, as this makes the most sense on large graphs that RGs seek to model. Recall that we observe a graph shift operator \(S\) and node features \(Z^{n p}\), and we return a vector per nodes \((S,Z)^{n d_{L}}\). We adopt a traditional GNN that uses the graph shift matrix \(S\): given input features \(Z^{(0)}^{n d_{0}}\),

\[Z^{()} =(Z^{(-1)}_{0}^{(-1)}+SZ^{(-1)} _{1}^{(-1)}+1_{n}(b^{()})^{})^{n d_{ }},\] \[_{}(S,Z^{(0)}) =Z^{(L-1)}^{(L-1)}+1_{n}(b^{(L)})^{}\] (2)

where \(\) is the ReLU function applied element-wise, and \(_{i}^{()}^{d_{} d_{+1}}\), \(b^{()}^{d_{}}\) are learnable parameters gathered in \(\). We denote by \(\) the set of all possible parameters. For all classic choices of \(S\), our definition of GNNs are a special case of message-passing NN (MPNN), which can be defined with a more general "aggregation" function. For the two examples above (adjacency and Laplacian), the aggregation function used is a sum, or a normalized sum.

We note that here we employ the ReLU function as a non-linearity, as some of our results will use its specific properties. Multi-Layer Perceptrons (MLP, densely connected networks) using the ReLU activation, and with potentially more than one hidden layer, will be denoted by \(f_{}^{}\), where \(\) gathers their parameters.

Following recent literature [12; 13], we consider inputing _Positional Encoding_ (PE) at each node. Such PE are generally computed using only the graph structure and concatenated to existing node features \(Z\), here we simply introduce a generic notation:

\[Z^{(0)}=_{}(S,Z)^{n d_{0}}\] (3)

with some parameter \(\). In our notations, the PE module uses the node features \(Z\), generally by concatenating them to its output. For short, we may denote the whole architecture with PE and GNN as \(_{,}(S,Z)}}{{=}}_{ }(S,_{}(S,Z))\). It is not difficult to see that if the PE computation is equivariant, then the whole GNN is equivariant: denoting by \(\) a permutation matrix of \(\{1,,n\}\),

\[,\ _{,}( S^{}, Z)= _{,}(S,Z),\ _{}( S^{}, Z)= _{}(S,Z).\]

All the examples of PEs examined in Sec. 4 are equivariant.

## 3 Function spaces of Graph Neural Networks

In this section, we provide a complete and intuitive description of the function space approximated by equivariant GNNs applied on RGs. All technical proofs are provided in App. A. It has been shown [24; 25; 8; 29] that GNNs converge to functions over the latent space: when the node features are a sampling of a certain function \(_{X}f^{(0)}\), then the output of the GNN is close to being a sampling of another function \(_{X}f^{(L)}\). Assuming the node features or PEs approximate some function set \( L^{2}_{}\), we define the space of functions that a GNN can approximate as follows.

**Definition 1**.: _Given a base set \( L^{2}_{}\), the **set of functions approximated by GNNs \(_{}()\)** is formed by all the functions \(f L^{2}_{}\) such that: for all \(>0\), there are \(,f^{(0)}\) such that_

\[\|_{}(S,_{X}f^{(0)})-_{X}f\|_ {}[n]{}0.\] (4)

In other words, \(_{}()\) are the functions whose sampling can be \(\)-approximated by the output of a GNN, with probability going to \(1\) as \(n\) grows. Note that if the quantifiers of \(,f^{(0)}\) and \(\) were reversed, the MSE would converge to \(0\) in probability. Here this is _not_ the case: \(,f^{(0)}\)_may depend on_\(\), which is akin to an approximation level. Similar to the permutation equivariance of GNNs, there is a notion of continuous equivariance for functions well-approximated by GNNs [24; 25; 8], where the permutations are replaced by bijections over the latent space \(\). We adopt the notations \(_{}()=_{}(,w,P)\). For all continuous bijections \(\) over \(\), we define \(w_{}(x,y)=w((x),(y))\), \(P_{}=^{-1} P\) where \(\) is the push-forward operation, and \(_{}=\{f f\}\). Then, we have the following result.

**Proposition 1**.: _Let \(S=S(A)\) be a graph shift operator that only depends on the adjacency matrix of the graph in a permutation-equivariant manner. Then, for all continuous bijections \(:\),_

\[_{}(_{},w_{},P_{})=\{f  f_{}(,w,P)\}.\]

That is, if one "permutes" the kernel \(w\), the distribution \(P\) and the base set \(\), then the function space \(_{}\) contains exactly the permuted version of the original space.

The goal of this section is to provide a more intuitive description of the space \(_{}\), which we will do under some basic convergence assumption from \(S\) to \(\). GNNs (2) basically include two components: dense connections and MLPs that can approximate any continuous function by the universality theorem , and applications of \(S\). Hence, we define the following function space.

**Definition 2**.: _We define \(_{}() L^{2}_{}\) the (minimal) \(\)-extension of a base set \( L^{2}_{}\) by the following rules:_

1. _Base space:_ \(_{}()\)_;_
2. _Stability by composition with continuous functions:_ _for all_ \(f_{}()\) _with a_ \(p\)_-dimensional output and_ \(g_{}(^{p},^{q})\)_, it holds_2 _that_ \(g f_{}()\)_;_ 3. _Stability by graph operator:_ _for all_ \(f_{}()\)_, it holds that_ \(f_{}()\)_;_ 
_
* _Linear span__: for all_ \(q\)_,_ \(_{}() L^{2}_{q}\) _is a vector space;_
* _Closure__:_ \(_{}()\) _is closed in_ \(L^{2}_{}\)_;_
* _Minimality__: for all_ \( L^{2}_{}\) _satisfying all the properties above,_ \(_{}\)_._

In words, \(_{}\) take a base set \(\), and extend it to be stable by composition with Lipschitz functions, application of the graph operator, and linear combination (of its elements with the same dimensionality). Our result will use the following assumption, which is naturally true for our running examples.

**Assumption 1**.: _With probability going to 1, \(\|S\|\) is bounded. Moreover, for all \(f L^{2}_{}\),_

\[\|S_{ X}f-_{X}f\|_{}[n] {}0\]

_where \([n]{}\) indicates convergence in probability._

**Proposition 2**.: _Assumption 1 is true for the adjacency matrix (ex. 1) and normalized Laplacian (ex. 2)._

Under this assumption, the main result of this section states that the functions well-approximated by GNNs are exactly the \(\)-extension of the base input features \(\).

**Theorem 1**.: _Under Assumption 1, for all \( L^{2}_{}\), we have:_

\[_{}()=_{}()\]

Given the definition of GNNs (2) and construction of \(_{}\), Theorem 1 appears quite natural. Its proof, provided in App. A.3, is however far from trivial. The inclusion \(_{}()_{}( )\) is similar in spirit to previous convergence results , since one has to construct a GNN that approximates a particular function. It involves however a new extended universality theorem for MLPs for square-integrable functions (Lemma 3 in App. A.3), which uses _the special properties of ReLU_. The reverse inclusion \(_{}()_{}( )\) is quite different from previous work on GNN convergence: given \(f_{}()\) whose only property is to be well-approximated by GNNs, one must construct a sequence of functions in \(_{}()\) that converge to \(f\), and uses the closure of \(_{}()\). The need to work within square-integrable function is here obvious, as we only have convergence of the MSE, an approximation of the \(L^{2}\)-norm. For instance, this inclusion would not be true in the space of continuous functions.

Using composition with continuous functions, if \(_{}()\) contains a continuous bijection \(:()\), then \(_{}()\) contains all continuous functions, and by density all square integrable functions. That is, the equivariant GNNs are then **universal** over \(\): they can generate any function to label the nodes. Another criterion using the Stone-Weierstrass theorem (e.g. ), similar to the proofs in , is the following.

**Proposition 3**.: _Assume that for all \(x x^{}\) in \(\), there is a continuous function \(f_{}()_{}( ,)\) such that \(f(x) f(x^{})\). Then, \(_{}()=L^{2}_{}\)._

In the rest of the paper, we study several examples of PEs and corresponding set \(\), that will generalize the results of . We expect many other interesting characteristics of \(_{}\) to be derived in the future.

## 4 Node features and Positional encodings

In the previous section, we have provided a complete description of the function space generated by equivariant GNNs when fed samplings of functions as node features, and the set of \(\) is thus crucial for the properties of \(_{}()\). For instance, in the absence of node features and PEs, it is classical to input _constant features_ to GNNs , such that the space of interest is \(_{}(1)\). However, similar to the failure of the WL test on regular graphs, if \(1 1\) (e.g. constant degree function), then \(_{}(1)\)_contains only constant functions_! The role of PEs is often to mitigate such situations.

**Definition 3**.: _The **set of functions approximated by PEs**\(_{}\) is formed by all the functions \(f L^{2}_{}\) such that: for all \(>0\), there is \(\) such that_

\[\|_{}(S,Z)-_{X}f\|_{}[n]{}0\,.\] (5)

Note that, as before, \(\) may depend on \(\). When passing PEs as input to GNNs, \(_{}\) serves as the base space \(\), and the space of interest to characterize the functions well approximated by the whole architecture \(_{,}\) is therefore \(_{}(_{})\). In fact, by simple Lipschitz property: for any \(f_{}(_{})\) and \(>0\), there are \(,\) such that

\[\|_{,}(S,Z)-_{X}f\|_{}[n]{}0\]

In the rest of the section, we therefore aim to characterize \(_{}\) for several representative examples. We first briefly comment on observed node features, then move on to PEs. Proofs are in App. B.

### Node features

A first, simple example, is when observed node features are actually a sampling of some function \(Z=_{X}f^{(0)}\). This is a convenient choice that is often adopted in the literature . In this case, by adopting the identity \(_{}(S,Z)=Z\), it is immediate that \(_{}=\{f^{(0)}\}\). A more realistic example is the presence of centered noise:

\[Z=_{X}f^{(0)}+^{n d_{0}}\] (6)

where \(=[_{1},,_{n}]\) and the \(_{i}\) are i.i.d. noise vectors with \(_{i}=0\) and \((_{i})=C_{}\). This time, \(_{}\) cannot contain directly \(f^{(0)}\), as the Law of Large Numbers (LLN) gives

\[\|Z-_{X}f^{(0)}\|_{}^{2}=\|\|_{}^{2}[n]{}(C_{})>0\]

However, when applying the graph shift matrix at least once, one obtains convergent PEs.

**Proposition 4**.: _Consider the adjacency matrix (ex. 1) or normalized Laplacian (ex. 2). If the node features are a noisy sampling (6) and the PE are defined \(_{}(S,Z)=SZ\), then, \(_{}=\{f^{(0)}\}\)._

Of course this may not be the only possibility for removing noise from node features, and moreover it is not clear how realistic the node features model (6) actually is. The study of more refined models linking graph structure and node features is a major path for future work.

### Positional Encodings

In this section, we consider classical PEs computed solely from the graph structure and show how they articulate with our framework. We consider two examples that are the most-often used in the literature: PEs as eigenvectors of the graph shift matrix  (actually a recent variant that account for sign indeterminancy ), and PE based on distance-encoding  (again a variant that, as we will see, generalize other architectures ). For most of the results below, we will focus on two representative cases of kernels, that include many practical examples. We remark that these yield _sufficient_ conditions to establish our results, but by no mean necessary. Other cases could be examined in future work.

**Example a** (Stochastic Block Models).: _In this case, the space of latent variables \(=\{1,,K\}\) is finite, each element correspond to a community label. The kernel \(w\) is represented by a matrix \(C}}}{{=}}[w(,k)]_{+} ^{K K}\) that gives the probability of connection between communities \(\) and \(k\), and \(P_{+}^{K}\) is a probability vector of size \(K\) that sum to \(1\)._

**Example b** (P.s.d. kernel).: _Here we assume that \(w\) is positive semi-definite (p.s.d.). This includes for instance the Gaussian kernel._

For any symmetric matrix (resp. self-adjoint compact operator) \(M\), we denote by \(_{i}^{M}\) its eigenvalues and \(u_{i}^{M}\) its eigenvectors (resp. eigenfunctions), with any arbitrary choice of sign or basis here. Since in all our examples operators are either p.s.d. or finite-rank, the eigenvalues are ordered as such: first the non-zero eigenvalues by decreasing order (from positive to negative), then all zero eigenvalues.

#### 4.2.1 Eigenvectors and SignNet

It has been proposed  to feed the first \(q\) eigenvectors of the graph into the GNN, for a fixed \(q\). A potential problem with this approach is the sign ambiguity of the eigenvectors, or even the basis ambiguity in case of eigenvalues with multiplicities. Here we consider only the sign ambiguity for simplicity: we will assume that the first eigenvalue of \(\) are distinct. The sign ambiguity was alleviated in  by taking a _sign-invariant_ function: considering an eigenvector \(u_{i}^{S}\) of \(S\),

\[(f)(u_{i}^{S})}}}{{=}}f(u_{ i}^{S})+f(-u_{i}^{S})^{n p}\] (7)where \(f:^{p}\) is a function applied to each coordinate of \(u_{i}^{S}\) to preserve permutation-equivariance. The resulting function is sign-invariant, and one can parameterized \(f\). Given the first \(q\) eigenvectors \(u_{i}^{S}\) and a collection of MLPs \(f_{_{i}}^{}:^{p_{i}}\) for some output dimensions \(p_{i}\), the PE considered in this subsection concatenates the outputs:

\[_{}(S)=[(f_{_{i}}^{})(u_{i}^{S})]_{i=1}^{q}^{n p}\] (8)

where \(p=_{i=1}^{q}p_{i}\) and the MLP are applied element-wise. The parameter \(\) gathers the \(_{i}\). The equation (8) involves a renormalization of the eigenvectors \(u^{S}\) by the square root of the size of the graph \(\): indeed, as \(u_{i}^{S}\) is normalized _in_\(^{n}\), this is necessary for consistency across different graph sizes. See Sec. 4.2.3 for a discussion and some numerical illustrations.

As can be expected, the eigenvectors of \(S\) generally converge to the eigenfunctions of \(\), under a spectral gap assumption. We provide the theorem below which handles all of our running examples. We suppose that the relevant eigenvalues have single multiplicities, to only have sign ambiguity.

**Theorem 2**.: _Consider either SBM (ex. \(a\)) or p.s.d. kernel (ex. \(b\)), and either adjacency matrix (ex. 1) or normalized Laplacian (ex. 2). Fix \(q\), assume the first \(q+1\) eigenvalues \(_{1}^{},,_{q+1}^{}\) of \(\) are two-by-two distinct. We define_

\[_{}}}{{=}}\{ [(f_{i}) u_{i}^{}]_{i=1}^{q} f_{i} _{}(,^{p_{i}}),p_{i}^{*}\}\] (9)

_Then \(_{}=_{}}\)._

Hence \(_{}\) contains the eigenfunctions of \(\), modified by the SignNet architecture to account for the sign indeterminancy. We further discuss this space in Sec. 4.2.3. An illustration is provided in Fig. 1.

#### 4.2.2 Distance-encoding PEs

In , the authors propose to define PEs through the aggregation of a set of "distances" \((i,j)\) from each node \(i\) to a set \(j V_{T}\) of target nodes (typically, labelled nodes in semi-supervised learning, or anchor nodes selected randomly ):

\[(_{})_{i,:}=(\{(i,j) j V_{T}\})\]

where AGG is an _aggregation_ function that acts on (multi-)sets, and \((i,j)\) is selected in  as random-walk based distances \((i,j)=[(AD_{A}^{-1})_{ij},,((AD_{A}^{-1})^{q})_{ij}]^{q}\). For simplicity, since here we do not consider any particular set of target nodes, we just consider \(V_{T}=V\) the set of all nodes. Moreover, to use our convergence results, we replace the random walk matrix with our graph shift matrix \(S\). As aggregation, we opt for the deep-set architecture , which applies an MLP on each \((i,j)\) then a sum. Deep sets can approximate any permutation-invariant function. As we will see below, with the proper normalization to ensure convergence, we obtain:

\[_{}=_{j}f_{}^{}(n [Se_{j},,S^{q}e_{j}])^{n q}\]

where \(f_{}^{}:^{q}^{p}\) is applied row-wise and \(e_{j}^{n}\) are one-hot basis vectors. We note that a similar architecture was proposed in a different line of work: it was called Structured Message Passing by , or Structured GNN by . In these works, the inspiration is to give nodes unique identifiers,

Figure 1: Illustration of the role of the SignNet architecture and of the renormalization by \(\) of the eigenvectors on synthetic data, with a latent space \(=[-1,1]\) (\(x\)-axis), a Gaussian kernel \(w\), and uniform distribution \(P\). Blue dots represent a graph from the training set, orange dot a test graph that is twice bigger. **From left to right:** eigenvectors with renormalization (with a different sign for the two graphs), eigenvectors without, PEs with, and PEs without, with the regression test errors of a GNN trained using these PE with or without renormalization. We observe that SignNet indeed fixed the sign ambiguity. The absence of renormalization yields unconsistent PEs across graphs of different sizes, which results in a high test error on test graphs than training graphs.

_e.g._, one-hot encodings \(e_{i}\). However, this process is not equivariant. To restore equivariance,  propose a deep-set pooling in the "node-id" dimension \(_{}(S)=_{j}_{}(S,e_{j})\), where \(_{}\) is itself a permutation-equivariant GNN, and the equivariance of \(_{}\) is restored. By choosing \(_{}(,e_{j})=n^{-1}f_{}^{}(n[Se_{ j},,S^{q}e_{j}])\) (which is a valid choice for a message-passing GNN), we obtain exactly distance-encoding PEs above.

In , powerful universality results were shown for this choice of architecture _in the case of non-random edges \(a_{ij}=w(x_{i},x_{j})\)_ and \(q=1\). With our notations, they implicitely studied PE functions of the following form: \( f(w(,x))dP(x)\). This allows to _modify the values of the kernel_ before computing the degree function, and can therefore break potential indeterminancy such as constant degrees. Unfortunately, their proof technique and the concentration inequalities they use are _not true anymore for Bernoulli random edges_, which are far more realistic than deterministic weighted edges. Here we show that for a large class of kernels, concentration can be restored when we add an MLP filter on the eigenvalues of \(S\) with ReLU. Our definition of distance-encoding PEs is therefore:

\[_{}=_{j}f_{_{1}}^{}(n [S_{_{2}}e_{j},,S_{_{2}}^{q}e_{j}])\] (10)

where \(S_{_{2}}}}{{=}}h_{f_{_{ 2}}^{}}(S)\) is a filter that applies an MLP \(f_{_{2}}^{}\) on the eigenvalues of \(S\).

**Theorem 3**.: _Consider either SBM (ex. \(a\)) or p.s.d. kernel (ex. \(b\)), and either adjacency matrix (ex. 1) or normalized Laplacian (ex. 2). Consider the PE (10). We define_

\[_{}}}{{=}} \{ f([_{x}(),,^{q}_{x}( )])dP(x) f_{}(^{q},^{p}),p ^{*}\}\] (11)

_where \(_{x}}}{{=}}\{z w _{}(z,x)\}\) by abuse of notation. Then \(_{}_{}\)._

Note that here we only have an inclusion \(_{}_{}\) instead of an equality as in Thm. 2: indeed, we show that the PE (10) can approximate functions in \(_{}\), but they may converge to other functions. Nevertheless, as a consequence of our analysis, all the universality results of [25, Sec. 5.3] are valid with the choice of PE (10), see Appendix C for a reminder using our notations. This is a strict, and non-trivial improvement over , as their results were only derived for non-random edges. For this, Theorem 3 relies mostly on a new concentration inequality for Bernoulli matrices with ReLU filters in Frobenius norm, that we give below since it is of independent interest.

**Theorem 4**.: _Consider either SBM (ex. \(a\)) or p.s.d. kernel (ex. \(b\)), and either adjacency matrix (ex. 1) or normalized Laplacian (ex. 2). Define the Gram matrix \(W=[w_{}(x_{i},x_{j})/n]_{ij}\). For all \(>0\), there is an MLP filter \(S_{}=h_{f_{}^{}}(S)\) such that_

\[(\|S_{}-W\|_{})  0.\]

The proof of this theorem, given in appendix B.3, is inspired by the so-called USVT estimator . One notes that the use of an MLP graph filter is quite unconventional. A more classical choice is polynomial filters: this avoids the diagonalization of \(S\) by computing \(_{k}a_{k}S^{k}\), it is for instance the basis for the ChebNet architecture . For the purpose of Theorems 3 and 4, _polynomial filters do not work, and ReLU is of crucial importance_: indeed, we need the filter to zero-out \((n)\) eigenvalues _uniformly_ in some interval \([-,]\). This cannot be done with polynomials with a fixed number of parameters and growing \(n\). On the other hand, when choosing \(f\) as an MLP with ReLU, due to the shape of this non-linearity, \(f_{_{2}}^{}\) can be _uniformly_\(0\)_on a whole domain_. Of course, polynomial filters offer great computational advantages, and perform well in practice, despite their flaw in our asymptotic analysis. Moreover, ReLU is technically non-differentiable. Designing filters that offer both computational advantages and exact approximation is still an open question. In practice, we observe that the ReLU-filter _does_ learn to approximate its expected shape, when we minimize the reconstruction error \(\|S_{}-W\|_{}\) on synthetic data where \(W\) is known, see Fig. 2.

Figure 2: Illustration of Theorem 4 on synthetic data where \(W\) is known, with a Gaussian kernel. Unfiltered eigenvalues of \(S\) are represented by blue crosses, filtered ones obtained by minimizing \(_{_{2}}\|S_{_{2}}-W\|_{}\) by orange dots, and the ideal ReLU-filter used in the proof of Thms. 3 and 4 is represented by a red line.

#### 4.2.3 Discussion

Approximation power.As mentioned above, in the absence of node features, one may opt for constant input, but this may lead to degenerate situations. PEs aim to counteract that, by increasing GNNs' approximation power. We quickly verify that this is indeed the case for our two examples.

**Proposition 5**.: _There are cases where \(_{}(1)\!\!_{}(_{ })\) or \(_{}(1)\!\!_{}(_{ })\) with strict inclusions._

Moreover, as mentioned in the previous section, existing universality results  can be generalized in our case, see App. C. Another interesting question is somewhat the opposite: given the already rich class of functions generated by PEs, are GNNs really more powerful?

**Proposition 6**.: _There are cases where \(_{}_{}(_{ })\) or \(_{}_{}(_{ })\), with strict inclusions._

The proof, which is not so trivial, invokes functions with at least one round of message-passing after the computation of PEs, so the additional approximation power does not come only from MLPs. Intuitively, it seems natural that message-passing rounds are useful for other reasons, e.g. noise reduction or smoothing . We leave these complementary lines of investigation for future work.

Renormalization.A striking point in our variants of PEs is the presence of various normalization factors by the graph size \(n\) to ensure convergence: the equation (8) involves a renormalization of the eigenvectors \(u^{S}\) by the square root of the size of the graph \(\), while (10) involves a multiplicative factor \(n\)_inside_ the MLP \(f^{}_{_{1}}\) (the \(1/n\) outside of the sum is more classical). Our analysis shows that these normalization factors are necessary for convergence when \(n\), and more generally for consistency across different graph sizes.

In practice, this is generally not used. Indeed, if the training and testing graphs have roughly the same "range" of sizes \(n[n_{},n_{}]\), then a GNN model can _learn_ the proper normalization to perform, which is not the point of view of our analysis \(n\). While in-depth benchmarking of PEs has been done in the literature  and is out-of-scope of this paper, we give a small numerical illustration of the effect of normalization in Table 1. We consider a synthetic dataset with a classic classification problem on (unobserved) latent variables \(x_{i}\) and a Gaussian kernel \(W\). To emphasize the effect of the normalization, we also examine a situation where the test graphs are much larger than the training graphs while following the same model, which we denote by out-of-dist. Concerning real data, since there are practically no datasets for node-classification with _several_ graphs of sufficiently different size to test the renormalization, we artificially extract many subgraphs from a single large graph (Citeseer) with labelled nodes to create such a dataset, denoted by Citeseer-subgraphs. We also directly look at two graph-classification datasets with many graphs of different sizes. Note that, to emphasize the effect of PEs, we discard eventual node features and use only the graph structure.

On synthetic data exactly formed of random graphs of vastly different sizes, the renormalization is of course necessary to obtain good performance, as predicted by our theory: without it, the PEs do not converge when \(n\) grows. On real data, we see that renormalization generally improve performance, and this is more true for IMDB-BINARY, which contains a larger range of graph sizes, and distance-based PEs. Note that here we use relatively small GNNs that are _not state-of-the-art_ (in particular since we do not use node features), as well as a different train/test split than most papers (\(K=5\) CV-folds instead of \(K=10\)): indeed, we do not want our models to _learn_ the proper normalization on the limited range of sizes \(n\) in the dataset, so we limit their number of parameters and use a smaller training set. We do not expect our simple renormalization process to make a significant difference on large-scale benchmarks with state-of-the-art models , but this is a pointer in an interesting direction to be explored in the future. In particular, this type of normalization may be useful in real-world scenarii where the test graphs are far larger than the labelled training graphs.

## 5 Conclusion

On large random graphs, the manner in which GNNs label _nodes_ can be modelled by functions. The analysis of the resulting function spaces is still in its infancy, and of a very different nature to the studies of _graph-tasks_, both discrete  or in the limit . In this paper, we clarified significantly the nature of the space of functions well-approximated by GNNs on large-graphs, showing that it can be defined by a few extension rules within the space of square-integrable functions. We then showed the usefulness of Positional Encodings by analyzing two popular examples, established new universality results, aswell as some concentration inequalities of independent interest. Our theory hinted at some process for consistency across graphs of different sizes that can help generalization in practice.

This paper, which in large part consisted in _properly defining_ the objects of interest, is without doubt only a first step in their analysis. Future studies might look at specific settings and derive more useful properties of the space \(_{}\), more powerful PEs, a better understanding of their limitations, or more realistic models for node features. In particular, a better connection with the existing WL-based theory for _finite small_ graphs, and associated "powerful" architectures, is a major path for future work.