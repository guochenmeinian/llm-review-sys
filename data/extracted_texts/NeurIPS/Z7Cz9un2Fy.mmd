# NEO-KD: Knowledge-Distillation-Based Adversarial Training for Robust Multi-Exit Neural Networks

Seokil Ham\({}^{1}\)  Jungwuk Park\({}^{1}\)  Dong-Jun Han\({}^{2}\)  Jaekyun Moon\({}^{1}\)

\({}^{1}\)KAIST \({}^{2}\)Purdue University

{gkatjrdlf, savertm}@kaist.ac.kr, han762@purdue.edu, jmoon@kaist.edu

Corresponding author.

###### Abstract

While multi-exit neural networks are regarded as a promising solution for making efficient inference via early exits, combating adversarial attacks remains a challenging problem. In multi-exit networks, due to the high dependency among different submodels, an adversarial example targeting a specific exit not only degrades the performance of the target exit but also reduces the performance of all other exits concurrently. This makes multi-exit networks highly vulnerable to simple adversarial attacks. In this paper, we propose NEO-KD, a knowledge-distillation-based adversarial training strategy that tackles this fundamental challenge based on two key contributions. NEO-KD first resorts to neighbor knowledge distillation to guide the output of the adversarial examples to tend to the ensemble outputs of neighbor exits of clean data. NEO-KD also employs exit-wise orthogonal knowledge distillation for reducing adversarial transferability across different submodels. The result is a significantly improved robustness against adversarial attacks. Experimental results on various datasets/models show that our method achieves the best adversarial accuracy with reduced computation budgets, compared to the baselines relying on existing adversarial training or knowledge distillation techniques for multi-exit networks.

## 1 Introduction

Multi-exit neural networks are receiving significant attention [9; 13; 26; 27; 28; 32] for their ability to make dynamic predictions in resource-constrained applications. Instead of making predictions at the final output of the full model, a faster prediction can be made at an earlier exit depending on the current time budget or computing budget. In this sense, a multi-exit network can be viewed as an architecture having multiple submodels, where each submodel consists of parameters from the input of the model to the output of a specific exit. These submodels are highly correlated as they share some model parameters. It is also well-known that the performance of all submodels can be improved by distilling the knowledge of the last exit to other exits, i.e., via self-distillation [15; 20; 24; 27]. There have also been efforts to address the adversarial attack issues in the context of multi-exit networks [3; 12].

Providing robustness against adversarial attacks is especially challenging in multi-exit networks: since different submodels have high correlations by sharing parameters, an adversarial example targeting a specific exit can significantly degrade the performance of other submodels. In other words, an adversarial example can have strong _adversarial transferability_ across different submodels, making the model highly vulnerable to simple adversarial attacks (e.g., an adversarial attack targeting a single exit).

**Motivation.** Only a few prior works have focused on adversarial defense strategies for multi-exit networks [3; 12]. The authors of  focused on generating adversarial examples tailored to multi-exit networks (e.g., generate samples via max-average attack), and trained the model to minimize the sum of clean and adversarial losses of all exits. Given the adversarial example constructed in , the authors of  proposed a regularization term to reduce the weights of the classifier at each exit during training. However, existing adversarial defense strategies [3; 12] do not directly handle the high correlations among different submodels, resulting in high adversarial transferability and limited robustness in multi-exit networks. To tackle this difficulty, we take a knowledge-distillation-based approach in a fashion orthogonal to prior works [3; 12]. Some previous studies [8; 23; 33; 34] have shown that knowledge distillation can be utilized for improving the robustness of the model in conventional single-exit networks. However, although there are extensive existing works on self-distillation for training multi-exit networks using clean data [15; 20; 24; 27], it is currently unknown how distillation techniques should be utilized for adversarial training of multi-exit networks. Moreover, when the existing distillation-based schemes are applied to multi-exit networks, the dependencies among submodels become higher since the same output (e.g., the knowledge of the last exit) is distilled to all sub-models. Motivated by these limitations, we pose the following questions: _How can we take advantage of knowledge-distillation to improve adversarial robustness of multi-exit networks? At the same time, how can we reduce adversarial transferability across different submodels in multi-exit networks?_

**Main contributions.** To handle these questions, we propose NEO-KD, a knowledge-distillation-based adversarial training strategy highly tailored to robust multi-exit neural networks. Our solution is two-pronged: neighbor knowledge distillation and exit-wise orthogonal knowledge distillation.

* Given a specific exit, the first part of our solution, neighbor knowledge distillation (NKD), distills the ensembled prediction of neighbor exits of clean data to the prediction of the adversarial example at the corresponding exit, as shown in Figure 0(a). This method guides the output of adversarial examples to follow the outputs of clean data, improving robustness against adversarial attacks. By ensembling the neighbor predictions of clean data before distillation, NKD provides higher quality features to the corresponding exits compared to the scheme distilling with only one exit in the same position.
* The second focus of our solution, exit-wise orthogonal knowledge distillation (EOKD), mainly aims at reducing adversarial transferability across different submodels. This part is another unique contribution of our work compared to existing methods on robust multi-exit networks [3; 12] (that suffer from high adversarial transferability) or self-distillation-based multi-exit networks [15; 20; 24; 27] (that further increase adversarial transferability). In our EOKD, the output of clean data at the \(i\)-th exit is distilled to the output of the adversarial sample at the \(i\)-th exit, in an exit-wise manner. During this exit-wise distillation process, we encourage the non-ground-truth predictions of individual exits to be mutually orthogonal, by providing orthogonal soft labels to each exit as described in Figure 0(b). By weakening the dependencies among different exit outputs, EOKD reduces the adversarial transferability across all submodels in the network, which leads to an improved robustness against adversarial attacks.

The NKD and EOKD components of our architectural solution work together to reduce adversarial transferability across different submodels in the network while correctly guiding the predictions of the adversarial examples at each exit. Experimental results on various datasets show that the proposed strategy achieves the best adversarial accuracy with reduced computation budgets, compared to existing adversarial training methods for multi-exit networks. Our solution is a plug-and-play method, which can be used in conjunction with existing training strategies tailored to multi-exit networks.

## 2 Related Works

**Knowledge distillation for multi-exit networks.** Multi-exit neural networks [9; 13; 26; 27; 28; 32] aim at making efficient inference via early exits in resource-constrained applications. In the multi-exit network literature, it is well-known that distilling the knowledge of the last exit to others significantly improves the overall performance on clean data without an external teacher network, i.e., via self-distillation [15; 20; 24; 27]. However, it is currently unclear how adversarial training can benefit from self-distillation in multi-exit networks. One challenge is that simply applying existing self-distillation techniques increases adversarial transferability across different submodels, since the same knowledgefrom the last exit is distilled to all other exits, increasing dependency among different submodels in the network. Compared to the existing ideas, our contribution is to develop a self-distillation strategy that does not increase the dependency of submodels as much; this helps reduce adversarial transferability of the multi-exit network for better robustness.

**Improving adversarial robustness.** Most existing defense methods [6; 7; 31] have mainly focused on creating new adversarial training losses tailored to single-exit networks. Several other works have utilized the concept of knowledge distillation [8; 23; 33; 34] showing that distilling the knowledge of the teacher network can improve robustness of the student network. Especially in , given a teacher network, it is shown that robustness of the teacher network can be distilled to the student network during adversarial training. Compared to these works, our approach can be viewed as a new self-distillation strategy for multi-exit networks where teacher/student models are trained together. More importantly, adversarial transferability across different submodels has not been an issue in previous works as the focus there has been on the single-exit network. In contrast, in our multi-exit setup, all submodels sharing some model parameters require extra robustness against adversarial attacks; this motivates us to propose exit-wise orthogonal knowledge distillation, to reduce adversarial transferability among different submodels.

Some prior works [19; 22; 29; 30] aim at improving adversarial robustness of the ensemble model, by reducing adversarial transferability across individual models. Specifically, the adaptive diversity-promoting regularizer proposed in  regularizes the non-maximal predictions of individual models to be mutually orthogonal, and the maximal term is used to compute the loss as usual. While the previous work focuses on reducing the transferability among different models having independent parameters, in a multi-exit network setup, the problem becomes more challenging in that all submodels have some shared parameters, making the models to be highly correlated. To handle this issue, we specifically take advantage of knowledge distillation in an exit-wise manner, which can further reduce the dependency among different submodels in the multi-exit network.

**Adversarial training for multi-exit networks.** When focused on multi-exit networks, only a few prior works considered the adversarial attack issue in the literature [3; 10; 11; 12]. The authors of [10; 11] focused on generating slowdown attacks in multi-exit networks rather than defense strategies. In , the authors proposed an adversarial training strategy by generating adversarial examples targeting a specific exit (single attack) or multiple exits (average attack and max-average attack). However, (i)  does not take advantage of knowledge distillation during training and (ii)  does not directly handle the high correlations among different submodels, which can result in high adversarial transferability. Our solution overcomes these limitations by reducing adversarial transferability while correctly guiding the predictions of adversarial examples at each exit, via self knowledge distillation.

## 3 Proposed NEO-KD Algorithm

Consider a multi-exit network with \(L\) exits, which is composed of \(L\) blocks \(\{_{i}\}_{i=1}^{L}\) and \(L\) classifiers \(\{w_{i}\}_{i=1}^{L}\). Given the input data \(x\), the output of the \(i\)-th exit is denoted as \(f_{_{i}}(x)\), which is parameterized by the \(i\)-th submodel \(_{i}=[_{1},,_{i},w_{i}]\) that consists of \(i\) blocks and one classifier. Note that all \(L\) submodels produce different predictions [\(f_{_{1}}(x),,f_{_{L}}(x)\)]. Here, since each submodel shares several blocks with other submodels, the predictions of any two submodels are highly correlated.

### Problem Setup: Adversarial Training in Multi-Exit Networks

The first step for adversarial training is to generate adversarial examples. Given \(L\) different submodels \(\{_{i}\}_{i=1}^{L}\), clean data \(x\), and the corresponding label \(y\), the adversarial example \(x^{adv}\) can be generated based on single attack, max-average attack or average attack, following the process of . More specifically, we have

\[x^{adv}_{single,i}=*{arg\,max}_{x^{}\{x:|z-x|_{ } t\}}|(f_{_{i}}(x^{}),y)|,\] (1)

\[x^{adv}_{max}=x^{adv}_{i^{*}},\ \ \ i^{*}=*{arg\,max} _{i}_{j=1}^{L}(f_{_{j}}(x^{adv}_{single,i}),y ),\] (2)

\[x^{adv}_{avg}=*{arg\,max}_{x^{}\{x:|z-x|_{} t \}}_{j=1}^{L}(f_{_{j}}(x^{}),y),\] (3)which correspond to the adversarial example generated by single attack targeting exit \(i\), max-average attack, and average attack, respectively. \(\) denotes the perturbation degree. In the single attack of (1), the adversarial example \(x_{i}^{adv}\) is generated to maximize the cross-entropy loss \((,)\) of the target exit utilizing an _attacker algorithm_ (e.g., PGD ). In the max-average attack of (2), among the adversarial examples generated by the single attack for all exits \(i=1,2,,L\), the sample that maximizes the average loss of all exits is selected. Finally, the average attack in (3) directly generates an adversarial sample that maximizes the average loss of all exits. Based on the generated \(x^{adv}\), a typical strategy is to update the model considering both the clean and adversarial losses of all exits as follows:

\[=_{j=1}^{N}_{i=1}^{L}[(f_{_{i}}(x_{j}),y_{j})+(f_{_{i}}(x_{j}^{adv}),y_{j})],\] (4)

where \(N\) is the number of samples in the training set and \(x_{j}^{adv}\) is the adversarial example corresponding to clean sample \(x_{j}\) generated by one of the attacks described above. However, the loss in (4) does not directly consider the correlation among submodels, which can potentially increase adversarial transferability of the multi-exit network.

### Algorithm Description

To tackle the limitations of prior works [3; 12], we propose neighbor exit-wise orthogonal knowledge distillation (NEO-KD), a self-distillation strategy tailored to robust multi-exit networks. To gain insights, we divide our solution into two distinct components with different roles - neighbor knowledge distillation and exit-wise orthogonal knowledge distillation - and first describe each component separately, and then put together into the overall NEO-KD method. A high-level description of our approach is given in Figure 1.

**Neighbor knowledge distillation (NKD).** The first component of our solution, NKD, guides the output feature of adversarial data at each exit to mimic the output feature of clean data. Specifically, the proposed NKD loss of the \(j\)-th train sample at the \(i\)-th exit is written as follows:

\[NKD_{i,j}=(f_{_{1}}(x_{j}^{adv}),_{k=1} ^{2}f_{_{k}}(x_{j}))&i=1\\ (f_{_{k}}(x_{j}^{adv}),_{k=L-1}^{L}f_{_{k}}(x _{j}))&i=L\\ (f_{_{i}}(x_{j}^{adv}),_{k=i-1}^{i+1}f_{_{k}}( x_{j}))&,\] (5)

which can be visualized with the colored arrows as in Figure 0(a). Different from previous self-knowledge distillation methods, for each exit \(i\), NKD generates a teacher prediction by ensembling (averaging) the neighbor predictions (i.e., from exit \(i-1\) and exit \(i+1\)) of clean data and distills it to each prediction of adversarial examples.

Compared to other distillation strategies, NKD takes advantage of only these _neighbors_ during distillation, which has the following key advantages for improving adversarial robustness. First,

Figure 1: NEO-KD consists of two parts that together improve the adversarial robustness: NKD and EOKD. (a) NKD guides the output of the adversarial data to mimic the ensemble outputs of neighbor exits of clean data. (b) EOKD reduces adversarial transferability of the network by distilling orthogonal knowledge of the clean data to adversarial data for the non-ground-truth predictions, in an exit-wise manner. Although omitted in this figure, EOKD normalizes the likelihood before distilling the soft labels. The overall process operates in a single model, although we consider two cases depending on the input (clean or adversarial example) for a clear presentation.

by ensembling the neighbor predictions of clean data before distillation, NKD provides a higher quality feature of the original data to the corresponding exit; compared to the naive baseline that is distilled with only one exit in the same position (without ensembling), NKD achieves better adversarial accuracy, where the results are provided in Table 7. Secondly, by considering only the neighbors during ensembling, we can distill different teacher predictions to each exit. Different teacher predictions of NKD also play a role of reducing adversarial transferability compared to the strategies that distill the same prediction (e.g., the last exit) to all exit; ensembling other exits (beyond the neighbors) increases the dependencies among submodels, resulting in higher adversarial transferability. The corresponding results are also shown via experiments in Section 4.3.

However, a multi-exit network trained with only NKD loss still has a significant room for mitigating adversarial transferability further. In the following, we describe the second part of our solution that solely focuses on reducing adversarial transferability of multi-exit networks.

**Exit-wise orthogonal knowledge distillation (EOKD).** EOKD provides orthogonal soft labels to each exit for the non-ground-truth predictions, in an exit-wise manner. As can be seen from the red arrows in Figure 0(b), the output of clean data at the \(i\)-th exit is distilled to the output of adversarial example at the \(i\)-th exit. During this exit-wise distillation process, some predictions are discarded to encourage the non-ground-truth predictions of individual exits to be mutually orthogonal. We randomly allocate the classes of non-ground-truth predictions to each exit for every epoch, which prevents the classifier to be biased compared to the fixed allocation strategy. The proposed EOKD loss of the \(j\)-th sample at the \(i\)-th exit is defined as follows:

\[EOKD_{i,j}=(f_{_{i}}(x_{j}^{adv}),O(f_{_{i}}(x_{j}))).\] (6)

Here, \(O()\) is the orthogonal labeling operation to make the non-ground-truth predictions to be orthogonal across all exits. For each exit, \(O()\) randomly selects \((C-1)/L\) labels among a total of \(C\) classes so that the selected labels are non-overlapping across different exits (except for the ground-truth label), as in Figure 0(b). Lastly, the probabilities of selected labels are normalized to sum to one. To gain clearer insights, consider a toy example with a 3-exit network (i.e., \(L=3\)) focusing on a \(4\)-way classification task (i.e., \(C=4\)). Let \([p_{1}^{i},p_{2}^{i},p_{3}^{i},p_{4}^{i}]\) be the softmax output of the clean sample at the \(i\)-th exit, for \(i=1,2,3\). If class 1 is the ground-truth, the orthogonal labeling operation \(O()\) jointly produces the following results from each exit: \([_{1}^{1},_{2}^{1},0,0]\) from exit 1, \([_{1}^{2},0,_{3}^{2},0]\) from exit 2, \([_{1}^{3},0,0,_{4}^{3}]\) from exit 3, where \(\) indicates the normalized probability of \(p\) so that the values in each vector sum to one.

Based on Eq. (6), the output of the orthogonal label operation \(O(f_{_{i}}(x_{j}))\) for the clean data \(x_{j}\) is distilled to \(f_{_{i}}(x_{j}^{adv})\) which is the prediction of the adversarial example of the \(j\)-th sample at the \(i\)-th exit. This encourages the model to self-distill orthogonally distinct knowledge in an exit-wise manner while keeping the essential knowledge of the ground-truth class. By taking this exit-wise orthogonal distillation approach, EOKD reduces the dependency among different submodels, reducing the adversarial transferability of the network.

**Overall NEO-KD loss.** Finally, considering the proposed loss functions in Eq. (5), (6) and the original adversarial training loss, the overall objective function of our scheme is written as follows:

\[\ =\ _{j=1}^{N}_{i=1}^{L}[(f_{_{i}}(x _{j}),y_{j})\,+\,(f_{_{i}}(x_{j}^{adv}),y_{j})\,+\,_{i}( \,\,NKD_{i,j}\,+\,\,\,EOKD_{i,j})],\] (7)

where \(,\) control the weights for each component and \(_{i}\) is the knowledge distillation weight for each exit \(i\). Since later exits have lower knowledge distillation loss than the earlier exits, we impose a slightly higher \(_{i}\) for the later exits than \(_{i}\) of the earlier exits. More details regarding hyperparameters are described in Appendix.

By introducing two unique components - NKD and EOKD - the overall NEO-KD loss function in Eq. (7) reduces adversarial transferability in the multi-exit network while correctly guiding the output of the adversarial examples in each exit, significantly improving the adversarial robustness of multi-exit networks, as we will see in the next section.

## 4 Experiments

In this section, we evaluate our method on five datasets commonly adopted in multi-exit networks: MNIST , CIFAR-10, CIFAR-100 , Tiny-ImageNet , and ImageNet . For MNIST, we use SmallCNN  with 3 exits. We trained the MSDNet  with 3 and 7 exits using CIFAR-10 and CIFAR-100, respectively. For Tiny-ImageNet and ImageNet, we trained the MSDNet with 5 exits. More implementation details are provided in Appendix.

### Experimental Setup

**Generating adversarial examples.** To generate adversarial examples during training and testing, we utilize max-average attack and average attack proposed in . We perform adversarial training using adversarial examples generated by max-average attack, where the results for adversarial training via average attack are reported in Appendix. During training, we use PGD attack  with \(7\) steps as attacker algorithm to generate max-average and average attack while PGD attack with \(50\) steps is adopted at test time for measuring robustness against a stronger attack. We further consider other strong attacks in Section 4.3. In each attacker algorithm, the perturbation degree \(\) is \(0.3\) for MNIST, and \(8/255\) for CIFAR-10/100, and \(2/255\) for Tiny-ImageNet/ImageNet datasets during adversarial training and when measuring the adversarial test accuracy. Other details for generating adversarial examples and additional experiments on various attacker algorithms are described in Appendix.

**Evaluation metric.** We evaluate the adversarial test accuracy as in , which is the classification accuracy on the corrupted test dataset compromised by an attacker (e.g., average attack). We also measure the clean test accuracy using the original clean test data and report the results in Appendix.

**Baselines.** We compare our NEO-KD with the following baselines. First, we consider the scheme based on adversarial training without any knowledge distillation, where adversarial examples are generated by max-average attack or average attack . The second baseline is the conventional self-knowledge distillation (SKD) strategy [20; 24] combined with adversarial training: during adversarial training, the prediction of the last exit for a given clean/adversarial data is distilled to the predictions of all the previous exits for the same clean/adversarial data. The third baseline is the knowledge distillation scheme for adversarial training , which distills the prediction of clean data to the prediction of adversarial examples in single-exit networks. As in , we distill the last output of clean data to the last output of adversarial data. The last baseline is a regularizer-based adversarial training strategy for multi-exit networks , where the regularizer restricts the weights of the fully connected layers (classifiers). In Appendix we compare our NEO-KD with the recent work TEAT , a general defense algorithm for single-exit networks.

**Inference scenarios.** At inference time, we consider two widely known setups for multi-exit networks: (i) anytime prediction setup and (ii) budgeted prediction setup. In the anytime prediction setup, an appropriate exit is selected depending on the current latency constraint. In this setup, for each exit, we report the average performance computed with all test samples. In the budgeted prediction setup, given a fixed computational budget, each sample is predicted at different exits depending on the predetermined confidence threshold (which is determined by validation set). Starting from the first exit, given a test sample, when the confidence at the exit (defined as the maximum softmax value) is larger than the threshold, prediction is made at this exit. Otherwise, the sample proceeds to the next exit. In this scenario, easier samples are predicted at earlier exits and harder samples are predicted at later exits, which leads to efficient inference. Given the fixed computation budget and confidence threshold, we measure the average accuracy of the test samples. We evaluate our method in these two

Table 2: **Anytime prediction setup**: Adversarial test accuracy on CIFAR-10.

Table 1: **Anytime prediction setup**: Adversarial test accuracy on MNIST.

settings and show that our method outperforms the baselines in both settings. More detailed settings for our inference scenarios are provided in Appendix.

### Main Experimental Results

**Result 1: Anytime prediction setup.** Tables 1, 2, 3, 4, 5 compare the adversarial test accuracy of different schemes under max-average attack and average attack using MNIST, CIFAR-10/100, Tiny-ImageNet, and ImageNet, respectively. Note that we achieve adversarial accuracies between 40% - 50% for CIFAR-10, which is standard considering the prior works on robust multi-exit networks . Our first observation from the results indicates that the performance of SKD  is generally lower than that of _Adv. w/o Distill_, whereas ARD  outperforms _Adv. w/o Distill_. This suggests that the naive application of self-knowledge distillation can either increase or decrease the adversarial robustness of multi-exit networks. Consequently, the method of knowledge distillation

Table 4: **Anytime prediction setup: Adversarial test accuracy on Tiny-ImageNet.**

Table 5: **Anytime prediction setup: Adversarial test accuracy on ImageNet.**significantly influences the robustness of multi-exit networks (i.e., determining which knowledge to distill and which exit to target). To further enhance robustness, we investigate strategies for distilling high-quality knowledge and mitigating adversarial transferability.

By combining EOKD with NKD to mitigate dependency across submodels while guiding a multi-exit network to extract high quality features from adversarial examples as original data, NEO-KD achieves the highest adversarial test accuracy at most exits compared to the baselines for all datasets/attacks. The overall results confirm the advantage of NEO-KD for robust multi-exit networks.

**Result 2: Budgeted prediction setup.** Different from the anytime prediction setup where the pure performance of each exit is measured, in this setup, we adopt _ensemble strategy_ at inference time where the predictions from the selected exit (according to the confidence threshold) and the previous exits are ensembled. From the results in anytime prediction setup, it is observed that various schemes tend to show low performance at the later exits compared to earlier exits in the model, where more details are discussed in Appendix. Therefore, this _ensemble strategy_ can boost the performance of the later exits. With the ensemble scheme, given a fixed computation budget, we compare adversarial test accuracies of our method with the baselines.

Figures 2 and 3 show the results in budgeted prediction setup under average attack and max-average attack, respectively. NEO-KD achieves the best adversarial test accuracy against both average and max-average attacks in all budget setups. Our scheme also achieves the target accuracy with significantly smaller computing budget compared to the baselines. For example, to achieve \(41.21\%\) of accuracy against average attack using CIFAR-10 (which is the maximum accuracy of _Adv. w/o Distill_), the proposed NEO-KD needs \(10.59\) MFlops compared to Adv. w/o Distill that requires \(20.46\) MFlops, saving \(48.24\%\) of computing budget. Compared to ARD, our NEO-KD saves \(25.27\%\) of computation, while SKD and LW are unable to achieve this target accuracy. For CIFAR-100 and Tiny-ImageNet, NEO-KD saves \(81.60\%\) and \(50.27\%\) of computing budgets compared to _Adv. w/o Distill_. The overall results are consistent with the results in anytime prediction setup, confirming the advantage of our solution in practical settings with limited computing budget.

**Result 3: Adversarial transferability.** We also compare the adversarial transferability of our NEO-KD and different baselines among exits in a multi-exit neural network. When measuring adversarial transferability, as in , we initially gather all clean test samples for which all exits produce correct predictions. Subsequently, we generate adversarial examples targeting each exit using the collected clean samples (We use PGD-\(50\) based single attack). Finally, we assess the adversarial transferability as the attack success rate of these adversarial examples at each exit. Figure 4 shows the adversarial transferability map of each scheme on CIFAR-100. Here, each row corresponds to the target exit for generating adversarial examples, and each column corresponds the exit where attack success rate is measured. For example, the \((i,j)\)-th element in the map is adversarial transferability measured at exit

Figure 3: **Budgeted prediction setup**: Adversarial test accuracy under max-average attack. The result for LW is excluded since the performance is too low and thus hinders the comparison between baselines and NEO-KD.

Figure 2: **Budgeted prediction setup**: Adversarial test accuracy under average attack. The result for LW is excluded since the performance is too low and thus hinders the comparison between baselines and NEO-KD.

[MISSING_PAGE_FAIL:9]

enables to distill high-quality features while lowering dependencies among submodels, confirming our intuition.

**Robustness against stronger adversarial attack.** We evaluate NEO-KD against stronger adversarial attacks; we perform average attack based on PGD-100 , Carlini and Wagner (CW) , and AutoAttack . Table 8 shows that NEO-KD achieves higher adversarial test accuracy than _Adv. w/o Distill_ in most of cases. Typically, CW attack and AutoAttack are stronger attacks than the PGD attack in single-exit networks. However, in the context of multi-exit networks, these attacks become weaker than the PGD attack when taking all exits into account. Details for generating stronger adversarial attacks are described in Appendix.

**Additional results.** Other results including clean test accuracy, results with average attack based adversarial training, results with varying hyperparameters, and results with another baseline used in single-exit network, are provided in Appendix.

## 5 Conclusion

In this paper, we proposed a new knowledge distillation based adversarial training strategy for robust multi-exit networks. Our solution, NEO-KD, reduces adversarial transferability in the network while guiding the output of the adversarial examples to closely follow the ensemble outputs of the neighbor exits of the clean data, significantly improving the overall adversarial test accuracy. Extensive experimental results on both anytime and budgeted prediction setups using various datasets confirmed the effectiveness of our method, compared to baselines relying on existing adversarial training or knowledge distillation techniques for multi-exit networks.