# Accelerating Transformers with Spectrum-Preserving Token Merging

Hoai-Chau Tran\({}^{*}\)\({}^{1,2}\), Duy M. H. Nguyen\({}^{*}\)\({}^{1,3,4}\), Duy M. Nguyen\({}^{5}\), TrungTin Nguyen\({}^{6}\), Ngan Le\({}^{7}\), Pengtao Xie\({}^{8,9}\), Daniel Sonntag\({}^{1,10}\), James Zou\({}^{11}\), Binh T. Nguyen\({}^{1}\)\({}^{2}\), Mathias Niepert\({}^{1}\)\({}^{3,4}\)

Co-first author, Corresponding Authors.

###### Abstract

Increasing the throughput of the Transformer architecture, a foundational component used in numerous state-of-the-art models for vision and language tasks (e.g., GPT, LLaVa), is an important problem in machine learning. One recent and effective strategy is to merge token representations within Transformer models, aiming to reduce computational and memory requirements while maintaining accuracy. Prior works have proposed algorithms based on Bipartite Soft Matching (BSM), which divides tokens into distinct sets and merges the top \(k\) similar tokens. However, these methods have significant drawbacks, such as sensitivity to token-splitting strategies and damage to informative tokens in later layers. This paper presents a novel paradigm called PiToMe, which prioritizes the preservation of informative tokens using an additional metric termed the _energy score_. This score identifies large clusters of similar tokens as high-energy, indicating potential candidates for merging, while smaller (unique and isolated) clusters are considered as low-energy and preserved. Experimental findings demonstrate that PiToMe saved from 40-60% FLOPs of the base models while exhibiting superior off-the-shelf performance on image classification (0.5% average performance drop of ViT-MAEH compared to 2.6% as baselines), image-text retrieval (0.3% average performance drop of CLIP on Flickr30k compared to 4.5% as others), and analogously in visual questions answering with LLaVa-7B. Furthermore, PiToMe is theoretically shown to preserve intrinsic spectral properties to the original token space under mild conditions. Our implementation is available at this link.

## 1 Introduction

Vision Transformers (ViTs)  have been integral to recent advancements in computer vision, leading to state-of-the-art deep learning architectures for representing images and videos . However, these transformer-based architectures incur substantial memory costs and have a quadratic time complexity in the number of tokens due to the self-attention layers. This challenge becomes particularly severe as model sizes increase, as observed in Large Language Models (LLMs) .

To address such limitations, several efforts focus on designing a more _efficient attention_ mechanism by making it linearly scale with input tokens , integrating vision or language domain-specific modules , or pruning the head numbers in ViT . Others propose _dynamically pruning_ less important tokens _w.r.t._ pre-defined metrics using learnable masks . However, a primary downside of these novel methodologies lies in the necessity to retrain the model from scratch, therefore hindering the leveraging of well-trained models such as LLMs. Moreover, most pruning-based techniques may not accelerate the training process, which arises from the dynamic removalof tokens in each sample, resulting in a mismatch of dimensions and consequently preventing the batching of samples with consistent dimensions.

Recent research has introduced a novel _token merging_ technique. Instead of pruning, this method combines tokens with high semantic similarity, removing background tokens and merging less informative foreground ones. Its versatility extends to training and non-training scenarios, drastically reducing compute and memory usage. A notable example is ToMe , which introduced the Bipartite Soft Matching (BSM) algorithm, prominent for its simplicity and effectiveness in merging highly similar tokens. Since ToMe, several works, including ToFu , Pumer , LTPM , and DiffRate , have built upon BSM with various adaptations in vision and language domains. In BSM, tokens representing image patches are separated into sets \(\) and \(\), and their pairwise cosine similarity is computed. The top \(k\) similar pairs of tokens between the sets \(\) and \(\) are merged. However, the performance of this algorithm is sensitive to the token-splitting strategy. For instance, ToMe's approach, which first splits tokens based on index parity, can lead to incorrect merging since tokens in \(\) can subsequently only be merged with those in \(\) (Figure 1). Moreover, while BSM excels in initial layers with many redundant tokens, deeper layers risk merging informative tokens due to latent object correlations. Though current enhancements  mitigated this by considering token attention scores in BSM , their adaptability to different ViT architectures, each with potentially distinct attention score distributions , remains a challenge.

In this work, we propose PiToMe (Protect Informative Tokens before Merging), a method designed to safeguard crucial information-bearing tokens prior to the merging step. Our method prioritizes preserving informative tokens by utilizing an additional metric termed the _energy score_ inspired by connections to _graph energy_ in spectral graph theory  (Theorem 1). Specifically, our energy score assesses large clusters of similar tokens as possessing high energy (like background and repeated textures), thereby marking them as suitable candidates for merging, while smaller, distinct regions (foreground) are deemed low-energy and thus treated as protected informative tokens. The proposed energy term operates on the graph built for input tokens, taking into account their relationships and aggregating information from nearby neighbors when their similarities exceed certain thresholds. This approach facilitates a deeper contextual comprehension compared to previous works  that rely solely on attention scores or feature embedding per token. Subsequently, we only select the highest-scoring tokens and pass them on for merging in the next steps, ensuring the preservation of important tokens, particularly in the latter stages when only a few remaining ones. During the merging process, we continue leveraging sorted energy vectors from earlier stages by distributing tokens with similar energy into two sets, \(\) and \(\), resulting in candidates in \(\) having a high probability of finding compatible matches in \(\). Matched tokens are then merged using a weighted average feature embedding to create a new token representation.

The empirical results demonstrate that despite the increased computational cost associated with energy score calculations, PiToMe exhibits comparable speed to other BSM-based approaches since the matching is performed on a smaller, high-energy token set. At the same time, it consistently shows superior accuracy across various experimental scenarios. Additionally, we present theoretical insights into PiToMe, showing that, under moderate assumptions -- such as the discriminative nature of feature embeddings generated by ViT for node pairs within and across distinct objects -- our algorithm efficiently preserves the spectral properties of the initial input tokens, maintaining the eigenvalues derived from normalized Laplacian matrices of the original tokens . To summarize, our contributions encompass:

* A new token merging procedure for accelerating ViT architectures is designed to protect crucial yet small-region tokens while identifying redundant ones for merging based on contextual token correlations captured by our energy score functions.

Figure 1: A comparison of token merging methods. Patches of the same color are merged. Green arrows highlight incorrect merges, avoided by PiToMe. Position of tokens with high attention scores (cyan borders, zoom for clarity) in PiToMe are maintained proportionality akin to ViT-base 384.

* Our PiToMe runs as fast as other BSM-based approaches while achieving SOTA performance on diverse tasks, ranging from image-text retrieval (Sec. 4.1), visual question answering with LLMs (Sec. 4.2), image classification (Sec. 4.3), and text classification (Sec. 4.4). In several cases, PiToMe is shown to reduce up to \(40-60\%\) FLOPs of base models while only dropping performance around \(~{}0.3-0.5\%\) (CLIP model on Flick30k).
* We also present theoretical findings indicating that, given reasonable assumptions, PiToMe can effectively approximate the spectral distance between the initial token spaces and the merged token set. This sheds light on why PiToMe tends to outperform baselines in practical applications and contributes to a better understanding of the potential limitations inherent in BSM-based methods, such as those in [15; 16; 19; 17; 27].

## 2 Related Work

**Efficient Attention Mechanisms.** Various efforts have sought to enhance the efficiency of transformers in both NLP and Vision domains. Some concentrate on accelerating attention computation [28; 29; 8] through approximation techniques involving hashing , low-rank , or sparse approximations . Others explore strategies such as head or feature pruning [11; 33] or the integration of domain-specific modules [9; 5; 34; 10]. However, many of them necessitate joint training with the backbone model from scratch. For instance, DynamicViT  runs approximately 150 hours of fine-tuning on an NVIDIA A100 GPU to prune the DeiT-S model . In contrast, we focus on accelerating existing ViT models by token merging, which applies to training and non-training scenarios.

**Dynamic Token Pruning.** Several studies have explored token pruning in transformer models across NLP [37; 38; 39] and vision domains [40; 41; 27; 42]. However, like efficient transformers, these methods typically require training. Additionally, most pruning techniques are dynamic, meaning the number of tokens varies across different inputs, which improves accuracy but complicates batching for practical deployment. To address this, numerous pruning methods employ masks during the training phase rather than directly eliminating tokens; however, it yields to cancel out the speed advantages associated with pruning.

**Token Merging.** Leading techniques such as ToMe  and its improvements [17; 43; 18; 19; 16; 44], build upon lightweight Bipartite Soft Matching (BSM). These methods exhibit speeds comparable to pruning while achieving superior performance. They have demonstrated the ability to double the throughput of state-of-the-art Vision Transformers (ViT) on both images and videos with minimal accuracy degradation in various scenarios. However, BSM-based approaches are sensitive to the selection of sets in the matching process, potentially resulting in the loss of informative tokens due to heuristic merging procedures. To address these issues, methods like DiffRate  and Crossget  leverage attention scores in ViT or cross-modal guidance to identify important tokens during the matching process, though they remain sensitive to the distribution of the token space, especially with imbalanced clusters. Another direction involves adapting more intricate algorithms, such as k-means , spectral clustering , graph pooling , or graph coarsening [24; 48], to merge similar tokens. While these strategies offer some guarantees and well-controlled outputs, their iteration schemes are highly complex and may not align with the goal of reducing model complexity in ViT layers. Our PiToMe, on the other hand, enables the advantages of both approaches. It maintains efficiency comparable to BSM, remains robust to token partitioning strategies, and offers a reasonable trade-off between speed and accuracy. Moreover, PiToMe is theoretically proved to approximate the spectral spectrum of the original token space under reasonable assumptions, resembling the behavior of other spectral clustering methods.

## 3 Methodology

### Token Merging Formulation

We apply token merging to each transformer block of the ViT architecture (Figure 2-a). Given the input token of the \(l\)-th block \(^{l}^{N h}\) where \(N\) and \(h\) are the token length and token hidden embeddings, a forward step in one Transformer block can be formulated as follows:

\[}^{l}=^{l}+(^{l}_ {Q},^{l}_{K},^{l}_{V}),~{}^ {l+1}=}^{l}+(}^{l})\] (1)

where Attention and MLP are the self-attention and multiple layer perceptron components. We then apply merge operations on \(}^{l}\) and compute the output of the reduced MLP block as:

\[^{l+1}=}^{l}_{m}+(}^{l}_{m} ),~{}~{}}^{l}_{m}=_{}(}^{l},^{l}_{K},r).\] (2)Here \(_{}(.)\) is the merging operation that receives \(}^{l}\) as input for compressing, \(^{l}_{K}\) (key matrices) as the token features of \(}^{l}\) following prior work , and \(r\) is the fraction of remaining tokens. The output \(}^{l}_{m}^{rN h}\) serves as input for the MLP layer to produce \(^{l+1}^{rN h}\). We present the PiToMe \(_{}(.)\) function in the next section.

### Energy-based Merging

We propose to use a new term called _energy score_ to evaluate the redundancy of each token, which is then used to protect informative or isolated tokens (low energy scores) while considering tokens that are in the large cluster as high energy scores and characterizing them as merging candidates. Figure 2-b illustrates the main steps in PiToMe.

**Token Graph Construction**: Given a set of \(N\) token inputs in \(}^{l}\), we build a weighted graph \((,,)\) with \(\) a set of \(N=||\) nodes, \(\) a set of \(M=||\) edges defined by connecting one token to the remaining ones in \(\), \(^{N N}\) be a weighted adjacency matrix. We opt for using the _key_ vectors \(=^{l}_{K}^{N h}\) as node features of \(\), i.e., \(v_{i}\) has \(h\) feature dimensions. The weight \([i,j]\) assigned to an edge \(e_{ij}\) connects \(v_{i}\) and \(v_{j}\) is computed by cosine distance:

\[[i,j]=1-(v_{i},v_{j}),(v_{i},v_{j})=  v_{j}}{\|v_{i}\|\|v_{j}\|}, v_{i},v_{j} .\] (3)

For simplicity, \([i,]\) and \([,i]\) denote the i-th row and column, _resp._; \([N]\) stands for \(\{1,,N\}\).

**Token Energy Scores**: In this step, the _energy score_, denoted as \(=(E_{i})_{i[N]}\), is computed for each node (Figure 2-a, Step 2). The term is inspired by the concept of _graph energy_ in spectral graph theory , defined as the sum of the absolute eigenvalues of the adjacency matrix \(\). We also leverage such structures of \(\) to find correlations among tokens and to estimate token redundancy. Instead of using independent token values such as attention scores , our energy leads to better performance (Figure 6, Appendix) and provides theoretical connections to the spectral properties of the original token graphs (Theorem 1).

Let \(i\) be the index of the current node and \((i)\) represent the set of neighbor nodes. The energy score \(E_{i} E_{i}(v_{i},[i,:])\) of node \(v_{i}\) is calculated using the following equation:

\[E_{i}(v_{i},[i,:])=_{j(i)}f_{m}((v _{i},v_{j})),\;f_{m}(x)=x&x m\\ ((x-m)-1)&.\] (4)

Rather than accumulating all \((v_{i},v_{j})\) values, the function \(f_{m}(.)\) in Eq.(4) mimics the exponential linear unit activation function , focusing on similar tokens even if they are far apart, while ignoring dissimilar ones. Here, \(m\) is a dynamic margin value varying at each layer in the ViT model. Nodes within this margin, i.e., (\(x>m\)) with high cosine similarity \((v_{i},v_{j})\) are considered true neighbors, potentially representing tokens belonging to the same object. Nodes outside this margin have \((v_{i},v_{j})\) replaced by a constant \(\), providing a lower bound for minimal edge weights. The

Figure 2: **a) PiToMe** can be inserted inside transformer block; **b)** Energy scores are computed to identify mergeable and protective tokens; **c)** Our algorithm gradually merges tokens in each block.

term \((x-m)-1<0\) smooths the function \(f(x)\) for neighboring nodes near the margin \(m\). In experiments, we set \(=1.0\) and \(m=0.9-0.9 l_{i}/l\), where \(l_{i}\) is the current layer index and \(l\) is the total number of encoder layers, indicating an increasing margin as tokens move to deeper layers. The ablation studies for the \(\) and \(m\) values are presented in Section 4.5.

Intuitively, Eq.(4) reflects the number of tokens potentially representing the same object. Tokens belonging to large objects (e.g., background) will have high energy scores, indicating potential candidates for merging, while smaller ones (e.g., foreground) will have low energy scores and are considered to be protected. This guides us to sort the energy vectors \(\) in descending order and choose only the top \(2k\) nodes with the highest scores as mergeable candidates and the remaining ones as protective tokens, i.e, \(=()\), _merge_\([:2k],\)\([2k:],k=N-Nr\).

**Ordered Energy-based Bipartite Soft Matching**: Having identified mergeable tokens in the _merge_ set, we continue exploit the sorted order in \(\) to form two sets \(\) and \(\) in BSM, each containing \(k\) nodes. Specifically, tokens with odd and even indices in _merge_ are selected for \(\) and \(\), _resp._ given the fact that those in the same object should have similar energy scores, resulting in likely distributing in consecutive positions in \(()\). In other words, our choosing has a high probability that one token in \(\) always finds its best match in the same object in \(\). This sets us apart with random partitions based on spatial indices in images like [15; 16].

**Tracking Token Sizes** All nodes in set \(\) are then merged with their nearest neighbors in set \(\) through the _fast_ BSM algorithm. Following prior works [15; 16], we also add proportional attention to balance the effect of the merged token on the output of the softmax function: \(=(^{l}}( ^{l}})^{T}/+)\) where \(\) is a row vector containing the size of each token, i.e., the number of data patches the token represents. The pseudo-code for our method is provided in Algorithm 1 (Appendix) with complexity analysis.

### Connection to Graph Coarsening with Spectral Preservation

In this section, we employ tools from spectral graph theory to show a spectral distance preservation of PiToMe. We note that similar properties can be obtained by using more complicated clustering algorithms such as K-mean  or spectral clustering [46; 47; 24]; however, these methods are typically loop-based algorithms, which are computationally expensive and not suitable for batch-type data. Our PiToMe, in contrast, is as fast as BSM methods but theoretically preserves spectral properties of input token graphs.

We begin by introducing Definitions 1 and 2 of graph coarsening and lifting, _resp._, to justify the spectral distance constructed in equation (5), measuring the similarity between the original and coarse graphs. For more thorough coverage of the mathematics of graph coarsening and graph lifting, we refer the reader to [50; 51; 52; 53]. In short, _we treat the result of token merging as a graph coarsening process_ (Figure 8, Appendix). We then create the _lifted graph_ as a reconstruction from this coarsened version to assess the spectral distance to the original token graph.

**Definition 1** (Graph Coarsening).: _Given a weighted graph \((,,)\), we denote \(=\{_{i}\}_{i[n]}\) where \(=_{i[n]}_{i}\), be a partition of its node into \(n\) disjoint sets. The coarsened graph of \(\) w.r.t. \(\) is the weighted graph \(_{c}\), where each partition in \(\) is aggregated into a single node, denoted \(\{_{i}\}_{i[n]}\), by averaging the elements within each partition. The elements of the adjacency matrix are given by \(_{c}[i,j]=_{v_{i}_{i}}_{v_{j}_{ j}}[i,j]/(|_{i}||_{j}|)\). We denote the combinatorial and normalized Laplacians of \(\) by \(=-\) and \(=_{N}-^{-1/2}^{-1/2}\), resp., where \(\) is the diagonal degree matrix with \([i,i]=d_{i}:=_{j=1}^{N}[i,j]\). Similarly, the definition of the coarsened Laplacian matrices follows directly: \(_{c}=_{c}-_{c}\) and \(_{c}=_{n}-_{c}^{-1/2}_{c}_{ c}^{-1/2}\). Finally, the eigenvalues and eigenvectors of \(\) (resp. \(_{c}\)) are denoted as \(\) and \(\) (resp. \(_{c}\) and \(_{l}\))._

**Definition 2** (Graph Lifting).: _We call \(_{l}(_{l},_{l},_{l})\) the lifted graph of \(\) if the adjacency matrix elements are given by \(_{l}[i,j]=_{c}[i,j]\). We denote the node degree of \(v_{li}_{l}\) by \(d_{li}=_{j=1}^{N}_{l}[i,j]\). The combinatorial and normalized Laplacians of \(_{l}\) is then defined as \(_{l}=_{l}-_{l}\) and \(_{l}=_{N}-_{l}^{-1/2}_{l}_{ l}^{-1/2}\), resp., where \(_{l}\) is the diagonal degree matrix with \([i,i]=d_{li}\). Then, we denote, resp., the eigenvalues and eigenvectors of \(_{l}\) by \(_{l}\) and \(_{l}\)._

**Lemma 1** (Eigenvalue Preservation, see _e.g.,_).: _The normalized Laplacian eigenvalues of the lifted graph \(_{l}\) contain all the eigenvalues of the coarse graph \(_{c}\) and additional eigenvalues \(1\) with \((N-n)\) multiplicity._

Through Lemma 1, we can use the lifted graph \(_{l}\) as a proxy for the coarse graph \(_{c}\), and define:

\[(,_{c})=\|-_{l}\|_{1}= _{i=1}^{N}|_{i}-_{li}|\] (5)

Next, we present our main theoretical result demonstrating how spectral distance characterizes the superiority of our novel PiToMe paradigm over the state-of-the-art approaches as ToMe . The Theorem 1 quantifies how similar the original \(\) is to its coarsened counterpart \(_{c}\), and is proved in Appendix E.

**Theorem 1** (Spectrum Consistent of Token Merging).: _Suppose the graphs \(_{0}^{(s)}\), \(_{}^{(s)}\), and \(_{}^{(s)}\) are coarsened from the original graph \(\) by iteratively merging pairs of nodes \(v_{a_{s}}\) and \(v_{b_{s}}\) w.r.t. the true partition \(_{0}^{(s)}=\{_{0i}^{(s)}\}_{i[s]}\), the PiToMe-partition \(_{}^{(s)}=\{_{}^{(s)}\}_{i [s]}\), defined by PiToMe in Algorithm 1, and the ToMe-partition , \(_{}^{(s)}=\{_{}^{(s)}\}_{i[s]}\), for \(s=N,,n+1\). We assume some standard mild assumptions: (A1) \([(v_{a_{s}},v_{b_{s}})] 1, v_{a_{s}} _{0i}^{(s)}, v_{b_{s}}_{0i}^{(s)},i[s]\); (A2) there exists a margin \(m\) s.t., \((v_{a_{s}},v_{b_{s}}) m>(v_{a_{s}},v_{c_{s}}), v_{a_{s }}_{0i}^{(s)}, v_{b_{s}}_{0i}^{(s)}, v _{c_{s}}_{0j}^{(s)}, i j[s]\); and (A3) there is an order of cardinality in the true partition, without loss of generality, we assume \(N_{1}^{(s)} N_{2}^{(s)} N_{s}^{(s)}\), where \(N_{i}^{(s)}=|_{0i}^{(s)}|, i[s]\). Then it holds that:_

1. _The spectral distance between the original_ \(_{0}^{(N)}\) _and the PiToMe-coarse_ \(_{}^{(n)}\) _graphs converges to_ \(0\)_, i.e.,_ \((,_{}^{(n)}) 0\)_,_
2. _The spectral distance between the original_ \(\) _and the ToMe-coarse_ \(_{}^{(n)}\) _graphs converges to a non-negative constant_ \(C\)_, with a high probability that_ \(C>0\)_._

Intuitively, Theorem 1 states that, given assumptions (i) tokens are closely embedded within classes and distinct between classes \((\)_A1_, _A2_\()\), and (ii) the number of tokens per class follows certain orders \((\)A3\()\), the spectral distance between PiToMe and the original tokens in Eq.(5) will converge to \(0\). In contrast, with ToMe partitions, a non-eliminable constant likely remains.

## 4 Experiments

We focus on two settings: _Off-the-Shelf Performance_, where we evaluate the models' performance immediately after compression without training, and _Retrained_, where we treat the compression algorithms as pooling functions and retrain the models on downstream tasks. The experiments cover four tasks: (i) _image & text retrieval_, (ii) _visual question answering (VQA)_, (iii) _image classification_, and (iv) _text classification_. We use the number of floating-point operations (FLOPS) needed for inference on one sample as the main metric to benchmark memory footprint and speed. Higher FLOPS indicate greater memory requirements and longer training and inference times.

### Image & Text Retrieval

We evaluate PiToMe on the image-text retrieval task using three different backbone models CLIP , ALBEF , and BLIP  on two frequently used Flickr30k  and MSCOCO  datasets. Our experiment is benchmarked using \(\), where a higher \(\) indicates the model's effectiveness in retrieval. In Figure 3, we benchmarked PiToMe against other SOTA _merging_ or _pruning_-based methods such as ToMe , ToFu , DiffRate , and DCT  on _off-the-shelf_ setting when varying amount of merged tokens at each layer. Given the same FLOPS, it is clear that PiToMe consistently outperforms previous compression algorithms across all backbones. The performance gap increases as we decrease the percentage \(r\) of tokens retained in each layer. The same behavior remains consistent in Table 2, where we set \(r=0.925\) and _retrain_ pre-trained checkpoints of BLIP and CLIP. For more details about the training method, please refer to Li et al. .

In Table 1, we compare PiToMe using compression ratios of \(r\{0.95,0.975\}\) on BLIP and BLIP-2 against other advanced architectures such as ViLT , LightningDOT , UNITER ,

[MISSING_PAGE_FAIL:7]

Let \(L\) denote the number of layers in the CLIP encoder and \(N\) the number of visually encoded tokens. In our experiment, we apply PiToMe to the ViT vision encoder of LLAVA, retaining only \(r\) percent of tokens in each layer. This results in \(r^{L}N\) tokens being fed into the LLM, significantly enhancing inference speed. We used LLaVA-1.5-7B and LLaVA-1.5-13B checkpoints to run off-the-shelf settings. Tables 3 and 4, along with Figure 4, illustrate that the PiToMe algorithm consistently achieves superior performance compared to other merging and pruning methods, as well as existing SOTA models such as BLIP-2 , InstructBLIP , IDEFICS-9B/80B , with _inference time nearly halved_. Remarkably, in some datasets like VisWiz and ScienceQA, the compressed model even surpasses the baseline model. We contend that this improvement stems from the merging of less significant tokens in PiToMe, potentially enhancing the robustness of the language model (LLM).

### Image Classification on Imagenet-1k

In this task, we employed five ViT backbones of varying sizes--tiny (ViT-T), small (ViT-S), base (ViT-B), large (ViT-L), and huge (ViT-H) - which are pre-trained using either MAE  or DEIT  styles. These backbones were utilized to assess both off-the-shelf and retrained performance. All experiments were conducted on the ImageNet-1k dataset, which is a subset of ImageNet  containing labeled images spanning 1000 categories.

Table 5 and Figure 5 present our experimental results, comparing PiToMe with recent works, including SOTA efficient transformers such as Swin-B , CSWin-B , MViT-B/L , MAE , and other token merging/pruning methods [84; 85; 41]. We observe that PiToMe maintains high accuracy with an average performance drop of only \(0.5\%\) after reducing up to \(44\%\) of FLOPS (MAE-H), showcasing superior performance with comparable throughput. It is important to note that _dynamic pruning-based methods_ such as A-ViT , Dynamic ViT , and SP-ViT  do not accelerate training speed due to using additional masks for padding tokens into a same dimension. On the retraining settings, we note that models compressed by PiToMe also surpass merging/pruning methods by a large margin and approach the performance of the original models.

  
**Model** & **VQA\({}^{ 1}\)** & **GOA\({}^{}\)** & **VisWiz\({}^{}\)** & **ScienceQA\({}^{}\)** & **TextVQA\({}^{}\)** & **MME\({}^{}\)** \\  LLaVA-1.5-7B & 09h.05m & 10m.25s & 04m-36s & 01m-50s & 10m-12s & 02m-32s \\ ToMe & 05h.38m & 06m-33s & 05m-26s & 10m-07s & 07m-37s & 01m-24s \\ ToFi & 05h.35m & 06m-33s & 08m-25s & 01m-06s & 07m-40s & 01m-24s \\ DCT & 05h.59m & 06m-41s & 03m-28s & 01m-08s & 08m-16s & 01m-27s \\ DiRRate & 05h.59m & 06m-39s & 02m-26s & 01m-06s & 07m-53s & 01m-21s \\
**PrToMe** & 05h.40m & 06m-37s & 00m-26s & 01m-07s & 07m-57s & 01m-23s \\  ILa-1.5-13B & 11m-13m & 05h.07m-36s & 04m-54s & 15m-06s & 02m-59s \\ ToMe & 05h.28m & 06m-35s & 05h.58s & 03m-25s & 11m-48s & 02m-15s \\ ToMi & 09h.26m & 09h.35s & 00m-45s & 12m-57s & 02m-15s \\ DCT & 100.02m & 09h.35s & 00m-46s & 12m-57s & 02m-34s \\ DiRRate & 09h.33m & 09h.44s & 06m-01s & 03m-37s & 11m-52s & 02m-18s \\
**PrToMe** & 09h.32m & 09h-39s & 06m-03s & 03m-35s & 12m-06s & 02m-17s \\   

Table 4: **Inference time** of LLaVA-1.5-7B and LLaVA-1.5-0.13B models when running on _five V100-GPUs_ and _five A100-GPUs_.

Figure 4: **Off-the-shelf performance of PiToMe on LLaVA-1.5-7B with different compressing ratio \(r\).**

[MISSING_PAGE_FAIL:9]

the performance of PiToMe, with energy-based operations playing a particularly significant role. Additionally, reducing tokens with a ratio \(r\) effectively eliminates redundant tokens in early layers while preserving informative ones in later layers.

**Margin \(m\) and \(\) hyper-parameters.** To validate the roles of these parameters in our energy score function in Eq.(4), we conduct ablation studies on image-text retrieval task with (v) adaptive margin \(m\) compared with a fixed value \(m\{0.9,0.45,0.0,-1.0\}\) when varying the ratio \(r\) and (vi) given a fixed value of \(r\), changing the smooth constant value \(\) in \(((x-m)-1)\) with \(x<m\). Results for these settings are summarized in Figure 7 and Table 8, respectively. We observe that while models with fixed tend to have the accuracy drop sharply when it is lower than some threshold, the adaptive margins achieve the best results across cases. We hypothesize that as the token space becomes sparser in deeper layers, PiToMe's fixed \(m\) approach likely assigns the same energy score to all tokens, making it difficult to isolate and protect tokens during merging. Table 8 also shows that \(=1.0\) is the best choice across margin values.

Further details, including additional ablation study results, visualizations (output merging, open-chat with LLaVa), and extra PiToMe experiments, are provided in the Appendix.

## 5 Conclusion

This paper introduces PiToMe, a novel algorithm that employs energy concepts to protect informative tokens during the token merging process. Our algorithm matches the efficiency of heuristic merging methods while maintaining a theoretical connection to the spectral properties of the input token space. In experiments on image classification, image-text retrieval, and VQA with LLaVA-1.5 7B/13B, PiToMe consistently outperforms recent token merging and pruning methods, given the equivalent runtime and memory usage.

**Limitations and Future Works** Although our focus has been on tasks using ViT encoders for a variety of applications, we believe it is important to extend PiToMe to generative tasks such as image generation (e.g., stable diffusion) or segmentation. This extension, however, necessitates the development of an _unmerge mechanism_ in the decoder, which remains an open question. Additionally, our energy score relies on a fully connected graph of input tokens, which can increase complexity as the input size grows. Constructing sparse graphs, therefore, might be beneficial for scaling in more challenging settings. Finally, designing a differentiable learning mechanism to optimize the reducing rate \(r\) for token merging could enhance robustness and versatility across different downstream tasks.