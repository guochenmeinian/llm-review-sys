# DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field

Chenyangguang Zhang\({}^{1*}\), Yan Di\({}^{2*}\), Ruida Zhang\({}^{1*}\), Guangyao Zhai\({}^{2}\),

Fabian Manhardt\({}^{3}\), Federico Tombari\({}^{2,3}\), Xiangyang Ji\({}^{1}\)

\({}^{1}\)Tsinghua University, \({}^{2}\)Technical University of Munich, \({}^{3}\) Google,

{zcyg22@mails., zhangrd21@mails., xyji@tsinghua.edu.cn, {yan.di@}tum.de

Authors with equal contributions.

###### Abstract

Reconstructing hand-held objects from a single RGB image is an important and challenging problem. Existing works utilizing Signed Distance Fields (SDF) reveal limitations in comprehensively capturing the complex hand-object interactions, since SDF is only reliable within the proximity of the target, and hence, infeasible to simultaneously encode local hand and object cues. To address this issue, we propose DDF-HO, a novel approach leveraging Directed Distance Field (DDF) as the shape representation. Unlike SDF, DDF maps a ray in 3D space, consisting of an origin and a direction, to corresponding DDF values, including a binary visibility signal determining whether the ray intersects the objects and a distance value measuring the distance from origin to target in the given direction. We randomly sample multiple rays and collect local to global geometric features for them by introducing a novel 2D ray-based feature aggregation scheme and a 3D intersection-aware hand pose embedding, combining 2D-3D features to model hand-object interactions. Extensive experiments on synthetic and real-world datasets demonstrate that DDF-HO consistently outperforms all baseline methods by a large margin, especially under Chamfer Distance, with about \(80\%\) leaf forward. Codes are available at https://github.com/ZhangCYG/DDFHO.

## 1 Introduction

Hand-held object reconstruction refers to creating a 3D model for the object grasped by the hand. It is an essential and versatile technique with many practical applications, _e.g._ robotics , augmented and virtual reality , medical imaging . Hence, in recent years, significant research efforts have been directed towards the domain of reconstructing high-quality shapes of hand-held objects, without relying on object templates or depth information. Despite the progress made, most existing methods rely on the use of Signed Distance Fields (SDF) as the primary shape representation, which brings about two core challenges in hand-held object reconstruction due to the inherent characteristics of SDF.

**First**, SDF is an undirected function in 3D space. Consequently, roughly determining the nearest point on the target object to a sampled point in the absence of object shape knowledge is infeasible. This limitation poses a significant challenge for single image hand-held object reconstruction as it is difficult to extract the necessary features to represent both the sampled point and its nearest neighbor on the object surface. Previous methods  have attempted to address this challenge by aggregating features within a local patch centered around the projection of the point, as shown in Fig. 1 (S-2). However, this approach is unreliable when the sampled point is far from the object surface since the local patch may not include the information of its nearest point. Therefore, for hand-held objectreconstruction, SDF-based methods either directly encode hand pose as a global cue  or propagate information between hand and object in 2D feature space , which fails to model hand-object interactions in 3D space. **Second**, SDF is compact and can not naturally encapsulate the inherent characteristics of an object such as symmetry. However, many man-made objects in everyday scenes exhibit some degree of (partial-)symmetry, and the inability of SDF to capture this information results in a failure to recover high-quality shapes, especially when the object is heavily occluded by the hand.

To overcome aforementioned challenges, we present DDF-HO, a novel **D**irected **D**istance **F**ield (DDF) based **H**and-held **O**bject reconstruction framework, which takes a single RGB-image as input and outputs 3D model of the target object. In contrast to SDF, DDF maps a ray, comprising an origin and a direction, in 3D space to corresponding DDF values, including a binary visibility signal determining whether the intersection exists and a scalar distance value measuring the distance from origin to target along the sampled direction.

As shown in Fig. 1, we demonstrate the superiority of DDF over SDF in modeling hand-object interactions. For each sampled ray, we collect its features to capture hand-object relationship by combining 2D-3D geometric features via our 2D ray-based feature aggregation and 3D intersection-aware hand pose embedding. We first project the ray onto the image, yielding a 2D ray or a dot (degeneration case), and then aggregate features of all the pixels along the 2D ray as the 2D features, which encapsulate 2D local hand-object cues. Then we collect 3D geometric features, including direct hand pose embedding as global information  and ray-hand intersection embedding as local geometric prior. In this manner, hand pose and shape serve as strong priors to enhance the object reconstruction, especially when there is heavy occlusion. Additionally, we also introduce a geometric loss term to exploit the symmetry of everyday objects. In particular, we randomly sample two bijection sets of 3D rays, where corresponding rays have identical origin on the reflective plane but with opposite directions. Thus the DDF predictions of corresponding rays in the two sets should be the same, enabling a direct supervision loss for shape.

Figure 1: **SDF** _vs_**DDF based hand-held object reconstruction. Given an input RGB image (a) and estimated hand and camera pose (b), SDF-based and DDF-based reconstruction pipelines vary from sampling spaces (S-1, D-1) and feature aggregation techniques (S-2, D-2 and S-3, D-3). SDF points sampling space must stay close to the object surface (S-1) or would lead to degraded network prediction results , while DDF ray sampling space (D-1) can be large enough to encapsulate the hand and object meshes. SDF methods typically aggregate features for the sampled point \(P\) in its local patch, which is not reliable when \(P\) is far from the object surface (S-2). DDF, however, aggregates features along the projection line \(r^{}\) for ray \(R\), which naturally captures both the information of the point and its intersection with the object surface (D-2). SDF methods cannot directly yield the contact points on the hand surface, so that only global relative hand joints encoding is used (S-3). On the contrast, DDF can get the intersection region of sampled rays and hand surface, leading to more representative local intersection-aware hand encoding (D-3). Due to these characteristics, we demonstrate that DDF is more suitable to model hand-object interactions. Consequently, DDF-based method achieves more complete and accurate hand-held object reconstruction results (S-4, D-4).

In summary, our main contributions are as follows. **First**, we present DDF-HO, a novel hand-held object reconstruction pipeline that utilizes DDF as the shape representation, demonstrating superiority in modeling hand-object intersections over SDF-based competitors. **Second**, we extract local to global features capturing hand-object relationship by introducing a novel 2D ray-based feature aggregation scheme and a 3D intersection-aware hand pose embedding. **Third**, extensive experiments on synthetic and real-world datasets demonstrate that our method consistently outperforms competitors by a large margin, enabling real-world applications requiring high-quality hand-held object reconstruction.

## 2 Related Works

**Hand Pose Estimation.** Hand pose estimation methods from RGB(-D) input can be broadly categorized into two streams: model-free and model-based methods. Model-free methods typically involve lifting detected 2D keypoints to 3D joint positions and hand skeletons [32; 44; 45; 46; 53; 52; 80]. Alternatively, they directly predict 3D hand meshes [11; 21; 48]. On the other hand, model-based methods [3; 55; 58; 77; 79] utilize regression or optimization techniques to estimate statistical models with low-dimensional parameters, such as MANO . Our approach aligns with the model-based stream of methods, as they tend to be more robust to occlusion .

**Single-view Object Reconstruction.** The problem of single-view object reconstruction using neural networks has long been recognized as an ill-posed problem. Initially, researchers focus on designing category-specific networks for 3D prediction, either with direct 3D supervision [7; 35; 16] or without it [25; 34; 38; 68; 17]. Some approaches aim to learn a shared model across multiple categories using 3D voxel representations [12; 22; 64; 65; 57], meshes [24; 27; 63; 51], or point clouds [19; 40]. Recently, neural implicit representations have emerged as a powerful technique in the field [42; 31; 2; 1; 47; 33; 70]. These approaches have demonstrated impressive performance.

**Hand-held Object Reconstruction.** Accurately reconstructing hand-held objects presents a significant challenge, yet it plays a crucial role in understanding human-object interaction. Prior works [20; 28; 59; 61] aim to simplify this task by assuming access to known object templates and jointly regressing hand poses and 6DoF object poses [15; 18; 75; 76; 60; 71]. Joint reasoning approaches encompass various techniques, including implicit feature fusion [9; 23; 41; 56], leveraging geometric constraints [4; 6; 13; 26; 74], and encouraging physical realism [62; 49]. Recent researches focus on directly reconstructing hand-held object meshes without relying on any prior assumptions. These methods aim to recover 3D shapes from single monocular RGB inputs. For instance,  designs a joint network that predicts object mesh vertices and MANO parameters of the hand, while  predicts them in the latent space. Additionally,  and  utilize Signed Distance Field (SDF) as the representation of hand and object shapes. In contrast, our method introduces a novel representation called Directed Distance Field (DDF) and demonstrates its superiority in reconstructing hand-held objects, surpassing the performance of previous SDF-based methods.

## 3 Method

### Preliminaries

**SDF**. Consider a 3D object shape \(\), where \(^{3}\) denotes the bounding volume that will act as the domain of the field, SDF maps a randomly sampled point \(\) to a a scalar value \(d\) representing the shortest distance from \(\) to the surface of the 3D object shape \(\). This scalar value can be positive, negative, or zero, depending on whether the point lies outside, inside, or on the surface of the object, respectively.

**From SDF to DDF**. SDF is widely used in the object reconstruction, however, due to its inherent undirected and compact nature, it is hard to effectively represent the complex hand-object interactions, as explained in Fig. 1. Hence, in this paper, we propose to utilize DDF, recently proposed and applied by [2; 31; 69], as an extension of SDF for high-quality hand-held object reconstruction.

**DDF**. Given a 3D ray \(L_{,}(t)=+t\), consisting of an origin \(\) and a view direction \(^{2}\), where \(^{2}\) denotes the set of 3D direction vectors having 2 degree-of-freedom. If this ray intersects with the target object \(\) at some \(t 0\), it is considered as _visible_, and DDF maps it to a non-negative scalar field \(:^{2}_{+}\), measuring the distance from the origin \(\) towards the first intersection with the object along the direction \(\). To conveniently model the _visibility_ of a ray, a binary visibility field is introduced as \((,)=[L_{,}\) is visible], _i.e._ for a visible ray, \((,)=1\). Moreover, [2; 31] provide several convenient ways to convert DDF to other 3D representations including point cloud, mesh and vanilla SDF.

### DDF-HO: Overview

**Objective**. Given a single RGB image \(\) containing a human hand grasping an arbitrary object, DDF-HO aims at reconstructing the 3D shape \(\) of the target object, circumventing the need of object template, category or depth priors.

**Initialization**. As shown in Fig. 2 (I-A)-(I-C), we first adopt an off-the-shelf framework [29; 55] to estimate the hand articulation \(_{H}\) and the corresponding camera pose \(_{C}\) for the input image \(\), where \(_{H}\) is defined in the parametric MANO model with 45D articulation parameters  and \(_{C}\) denotes the 6D pose, rotation \((3)\) and translation \(t^{3}\), of the perspective camera with respect to the world frame.

**Image Feature Encoding**. Hierarchical feature maps of image \(\) are extracted via ResNet  to encode 2D cues, as shown in Fig. 2 (2-A) and (2-B).

**Ray Sampling**. We sample 3D rays \(\{_{,}\}\), with origins \(\) and directions \(^{2}\), and transform the rays into the normalized wrist frame with the predicted hand pose \(_{H}\), as in IHOI . The specific ray sampling algorithm adopted in the training stage is introduced in detail in the Supplementary Material.

**Ray Feature Aggregation**. To predict corresponding DDF values of \(\{_{,}\}\), we collect and concatenate three sources of information: basic ray representations \(\{,\}\), 2D projected ray features \(_{2D}\), 3D intersection-aware hand features \(_{3D}\). For \(_{2D}\), we project each 3D ray onto the feature maps extracted from \(\), yielding a 2D ray \(\{l_{p,^{*}}\}\) or a dot (degeneration case). Note that in the degeneration case, the sampled 3D ray passes through the camera center, we only need to collect \(_{2D}\) inside the patch centered at \(p\), as in the SDF-based methods . For other non-trivial cases, we aggregate features along \(\{l_{p,^{*}}\}\) using the 2D Ray-Based Feature Aggregation technique, introduced in detail in Sec. 3.3. For \(_{3D}\), besides global hand pose embedding as in , we also encapsulate the intersection of the ray with the hand joints as local geometric cues to depict the relationship of hand-object interaction, which is further introduced in detail in Sec. 3.4.

Figure 2: **Overview of DDF-HO. Given an RGB-image (I-A), we first employ an off-the-shelf pose detector to predict camera pose \(_{C}\) and hand pose \(_{H}\) (45D parameters defined in MANO model ), as shown in (I-B) and (I-C) respectively. For the input of DDF, we sample multiple rays (R-A) in 3D space, and project them onto the 2D image (R-B). The corresponding intersections with the hand skeleton are also calculated (R-C). Then for each ray \(_{,}\), we collect 2D ray-based feature \(_{2D}\) from (2-A) to (2-F), and 3D intersection-aware hand embedding \(_{3D}^{}\) and \(_{3D}^{}\) from (3-A) to (3-D). Finally, we concatenate all features and ray representation as \(=\{,,_{2D},_{3D}^{ },_{3D}^{}\}\) to predict corresponding DDF values.**

**DDF Reconstruction**. Concatenating \(\{,,_{2D},_{3D}\}\) as input, we employ an 8-layer MLP network  to predict corresponding DDF values.

### Ray-Based Feature Aggregation

Previous SDF-based methods  typically aggregates feature for each sampled point within a local patch centered at its projection, posing a significant challenge for hand-held object reconstruction as the aggregated feature may not contain necessary information for predicting the intersection, as illustrated in Fig. 1 (S-2). When the sampled point is far from the object surface, its local feature may even be completely extracted from the background, making it infeasible to predict corresponding SDF values. As a consequence, SDF-based methods either leverage hand pose as a global cue  or only propagate hand-object features in 2D space , failing to capture hand-object interactions in 3D space. In DDF-HO, besides the ray representation \(\{,\}\), we combine two additional sources of features \(_{2D}\) and \(_{3D}\) for each sampled 3D ray to effectively aggregate all necessary information for predicting the DDF value.

We first collect \(_{2D}\) from the input image \(\), by employing our 2D Ray-Based Feature Aggregation technique. Given a 3D ray \(_{,}\), as shown in Fig. 2 (R-A) and (R-B), the origin is projected via \(p=K(+t)/_{z}\), where \(_{z}\) denotes \(z\) component of \(\), and the direction \(^{*}\) is determined as the normal vector from \(p\) towards the projection of another point \(^{*}\) on the 3D ray, yielding the projected 2D ray \(l_{p,^{*}}\). Then we sample \(K_{l}\) points \(\{p_{i}^{l},i=1,...,K_{l}\}\) on the 2D ray, and extract local patch features \(_{2D}^{l}=\{^{i}\}\) for all \(K_{l}\) points as well as the feature \(_{2D}^{p}\) of origin projection \(p\) via bilinear interpolation on the hierarchical feature maps of \(\). Finally, we leverage the cross-attention mechanism to aggregate 2D ray feature \(_{2D}\) for \(L_{,}\) as,

\[_{2D}=_{2D}^{p}+MultiH(_{2D}^{p},_ {2D}^{l},_{2D}^{l})\] (1)

where \(MultiH\) refers to the multi-head attention and \(Q=_{2D}^{p},K=V=_{2D}^{l}\).

Comparing with single point based patch features used in SDF methods, \(_{2D}\) naturally captures more information, leading to superior reconstruction quality. First, 2D features from the origin of the 3D ray towards its intersection on the object surface are aggregated, enabling reliable DDF prediction for the ray whose origin is far from the object surface. In this manner, we can sample 3D rays in the whole domain, as shown in Fig. 1 (D-1), encapsulating and reconstructing hand and object simultaneously with a single set of samples. Second, features related to the hand along the projected 2D ray are also considered, providing strong priors for object reconstruction.

### Hand-Object Interaction Modelling

We model hand-object interactions in two aspects. First, in 2D features maps, hand information along the projected 2D ray is encoded into \(_{2D}\), as introduced in Sec. 3.3. Second, we collect \(_{3D}\), which encodes both global hand pose embedding \(_{3D}^{}\) as in  (details in Supplementary Material) and local geometric feature \(_{3D}^{}\) indicating the intersection of each ray with the hand. As shown in Fig. 2 (3-C) and (3-D), for each 3D ray \(_{,}\), \(_{3D}^{}\) is collected in three steps. First, we calculate the

Figure 4: **Construction of the symmetry loss.**

Figure 3: **3D intersection-aware local geometric feature \(_{3D}^{}\).** We collect it by resolving the nearest neighboring hand joints of ray-hand intersection.

shortest path from \(_{,}\) towards the hand skeleton, constructed by the MANO model and predicted hand articulation parameter \(_{H}\), yielding starting point \(^{}\) on \(_{,}\) and endpoint \(^{}\) on the hand skeleton. Then we detect \(K_{3D}\) nearest neighboring hand joints of \(^{}\) on the hand skeleton, using geodesic distance. Finally, \(^{}\) is transformed to the local coordinates of detected hand joints (Fig. 2 3-C), indicated by the MANO model, and thereby obtaining \(^{}_{3D}\) by concatenating all these local coordinates of \(^{}\). In summary, \(_{3D}\) is represented as \(_{3D}=\{^{G}_{3D},^{}_{3D}\}\).

Our hand-object interaction modeling technique has two primary advantages over IHOI , in which sampled points are only encoded with all articulation points of the hand skeleton, as shown in Fig. 1 (S-3). First, our technique extracts and utilizes 2D features \(_{2D}\) that reflect the interaction between the hand and object, providing more useful cues to reconstruct hand-held objects. Second, we incorporate 3D intersection-based hand embeddings \(_{3D}\) that offer more effective global to local hand cues as geometric priors to guide the learning of object shape, especially when a 3D ray passing through the contact region between the hand and object. In such case, the intersection with the hand skeleton is closer to the intersection with the object than the origin of the ray, thereby encoding the local hand information around the intersection provides useful object shape priors, as shown in Fig. 3. In other cases, our method works similarly to IHOI, where the embeddings serve as a global locator to incorporate hand pose.

### Conditional DDF for Hand-held Object Reconstruction

Given concatenated feature \(=\{,,_{2D},_{3D}\}\) for each 3D ray \(_{,}\), we leverage an 8-layer MLP to map \(\) to the corresponding DDF value: distance \(D\) and binary visible signal \(\). The input \(\{,\}\) is positional encoded by \(\) function as . \(\) is output after the 3rd layer to leave the network capacity for the harder distance estimation task. We also introduce a skip connection of the input 3D ray \(\{,\}\) to the 4th layer to preserve low-level local geometry.

The loss functions of our conditional directed distance field network consist of depth term \(_{D}=|-D|\), visibility term \(_{}=BCE(,)\) and symmetry term \(_{s}=|_{1}-_{2}|\), with \(,\) being the predictions of \(D,\) respectively. For symmetric objects, we randomly sample two bijection sets of 3D rays, where corresponding rays have identical origin on the reflective plane but with opposite directions. Specifically, before our experiments, all symmetric objects are preprocessed to be symmetric with respect to the XY plane \(\{X=0,Y=0\}\). To determine whether the object is symmetric, we first flipping the sampled points \(P\) on the object surface w.r.t the XY plane, yielding \(P^{}\). Then we compare the Chamfer Distance between the object surface and \(P^{}\). If the distance lies below a threshold (1e-3), the object is considered symmetric. As for building two bijection sets \(B_{1}:\{P_{1},_{1}\}\) and \(B_{2}:\{P_{2},_{2}\}\), we first randomly sample origins \(P_{1}:\{(x_{1},y_{1},z_{1}))\}\) and directions \(_{1}:\{(_{1},_{1},_{1}))\}\) to construct \(B_{1}\). Then, we flip \(B_{1}\) with respect to the reflective plane to generate \(B_{2}\) by \(P_{2}:\{(x_{1},y_{1},-z_{1}))\}\), \(_{2}:\{(_{1},_{1},-_{1}))\}\). Since the object is symmetric, the DDF values \(_{1},_{2}\) of corresponding rays in \(B_{1}\) and \(B_{2}\) should be the same, which establishes our symmetry loss term. The process of constructing the symmetry loss is shown in Fig. 4.

The final loss is defined as \(=_{}+_{1}_{D}+_{2}_{s}\), where \(_{1},_{2}\) are weighting factors.

## 4 Experiments

### Experimental Setup

**Datasets.** A synthetic dataset ObMan  and two real-world datasets HO3D(v2) , MOW  are utilized to evaluate DDF-HO in various scenarios. ObMan consists of 2772 objects of 8 categories from ShapeNet , with 21K grasps generated by GraspIt . The grasped objects are rendered over random backgrounds through Blender 1. We follow [29; 67] to split the training and testing sets. HO3D(v2)  contains 77,558 images from 68 sequences with 10 different persons manipulating 10 different YCB objects . The pose annotations are yielded by multi-camera optimization pipelines. We follow  to split training and testing sets. MOW  comprises a total of 442 images and 121 object templates, collected from in-the-wild hand-object interaction datasets [14; 56]. The approximated ground truths are generated via a single-frame optimization method . The training and testing splits remain the same as the released code of .

[MISSING_PAGE_FAIL:7]

which allows for more exquisite hand-held object reconstruction. Notably, DDF-HO's CD metric is reduced by 85% compared to IHOI and 77% compared to HO, indicating that our predicted object surface contains much fewer outliers.

Fig. 6 presents improved visualizations of DDF-HO, showcasing enhanced and more accurate surface reconstruction of hand-held objects. While IHOI can achieve decent object surface recovery within the camera view, the reconstructed surface appears rough with numerous outliers when observed from novel angles. This suggests that IHOI lacks the ability to perceive 3D hand-held objects due to its limited modeling of hand-object interaction in 3D space. In contrast, DDF-HO utilizes a more suitable DDF representation, resulting in smooth and precise reconstructions from any viewpoint of the object.

### Evaluation on Real-world Scenarios

In addition to synthetic scenarios, we conduct experiments on two real-world datasets, HO3D(v2) and MOW, to evaluate DDF-HO's performance in handling real-world human-object interactions. Table 1 constitutes the evaluation results on HO3D(v2) after finetuning (with related settings described in Section 4.1), as well as the zero-shot generalization results, where we directly conducted inference on HO3D(v2) using the weights from training on ObMan. Furthermore, Table 2 presents results on MOW under the same setting.

Our method, after finetuning on the real-world data, achieves state-of-the-art performance on both datasets. Specifically, on HO3D(v2), we observe a considerable improvement compared to IHOI and other methods, with an increase in F-5 by 7%, F-10 by 4%, and a significant decrease in CD by 73%. On MOW, our approach also outperforms the previous state-of-the-art methods, achieving a remarkable performance gain in terms of increased F-5 (6%), F-10 (3%), and reduced CD (80%) compared with IHOI . Fig. 7 shows the visualization comparison results on MOW. DDF-HO performs well under the real-world scenario, yielding more accurate reconstruction results.

Furthermore, the zero-shot experiments demonstrate that DDF-HO has a stronger ability for synthetic-to-real generalization. Specifically, on HO3D(v2), DDF-HO yields superior performance in terms of F-5 by 7%, F-10 by 5%, and a decreased CD by 82%. Moreover, the results on MOW also indicate

  Method & F-5 \(\) & F-10 \(\) & CD \(\) & F-5 \(\) & F-10 \(\) & CD \(\) \\  IHOI  & 0.10 & 0.19 & 7.83 & 0.09 & 0.17 & 8.43 \\ Ours & **0.16** & **0.22** & **1.59** & **0.14** & **0.19** & **1.89** \\  

Table 2: Results on MOW  dataset, with the setting of finetuning (left) and zero-shot generalization from ObMan  dataset (right). Overall best results are **in bold**.

  Method & F-5 \(\) & F-10 \(\) & CD \(\) \\  HO  & 0.23 & 0.56 & 0.64 \\ GF  & 0.30 & 0.51 & 1.39 \\ IHOI  & 0.42 & 0.63 & 1.02 \\ Ours & **0.55** & **0.67** & **0.14** \\  

Table 3: Results on ObMan  dataset. Overall best results are **in bold**.

Figure 6: **Visualization results on ObMan . Our method consistently outperforms IHOI .**

that our method, trained only on synthetic datasets, can still achieve decent performance in real-world scenarios, thanks to the generic representation ability of DDF for hand-object interaction modeling.

### Efficiency

To demonstrate the efficiency of DDF-HO, we compare the network size and the running speed with IHOI, which is a typical SDF-based hand-held object reconstruction method. All experiments are conducted on a single NVIDIA A100 GPU.

DDF-HO runs at 44FPS, which is slower than IHOI with 172 FPS, but still achieves real-time performance. The slower inference comes from the attention calculation for 2D ray-based feature aggregation and the more elaborated 3D feature generation. For model size, the two methods share a similar scale (24.5M of DDF-HO and 24.0M of IHOI). Generally, the increased parameters mainly come from the cross attention mechanism in 2D ray-based feature aggregation. Other modules are only adopted to collect features to model hand-object interactions and do not significantly increase the network size.

### Ablation Studies

We conduct ablation studies on the ObMan and HO3D(v2) datasets to evaluate the impact of three key assets of DDF representation: Ray-Based Feature Aggregation (RFA), Intersection-aware Hand Feature (IHF), and Symmetry Loss (SYM). The results of the ablation studies are presented in Tab. 4.

On ObMan, we first replace the SDF representation with DDF without any modifications to feature aggregation, resulting in a slight improvement over the SDF-based IHOI with a \(3\%\) increase in F-5 metric. This indicates that although DDF is more suitable for representing hand-object interaction (with almost an \(80\%\) decrease in CD metric), more sophisticated feature aggregation designs are required. Next, we add RFA considering the characteristics of sampled rays, leading to a \(5\%\) increase in F-5 and a \(4\%\) increase in F-10. Subsequently, adding IHF, which models hand-object interaction locally by considering the intersection information of the hand, resulted in a \(5\%\) increase in F-5 and a \(3\%\) increase in F-10. This indicates that considering the intersection information of the hand can improve the accuracy of hand-held object reconstruction. Finally, adding the SYM loss, which captures the symmetry nature of everyday objects and handles self-occluded scenarios caused by hands, results in another \(2\%\) increase in F-5 and a \(1\%\) increase in F-10. On the HO3D(v2) dataset, RFA, IHF, and SYM modules play similar roles as on ObMan.

Additionally, we evaluate the influence of input hand pose on DDF-HO by adding Gaussian noise to the estimated hand poses (Pred) or ground truth hand poses (GT) in the input. The results of the ablation studies (Tab. 5) demonstrate the robustness of DDF-HO in handling noisy input hand poses.

## 5 Conclusion

In this paper, we present DDF-HO, a novel pipeline that utilize DDF as the shape representation to reconstruct hand-held objects, and demonstrate its superiority in modeling hand-object interactions

Figure 7: **Visualization results on MOW . Our method consistently surpasses IHOI .**

over competitors. Specifically, for each sampled ray in 3D space, we collect its features capturing local-to-global hand-object relationships by introducing a novel 2D ray-based feature aggregation and 3D intersection-aware hand pose embedding. Extensive experiments on synthetic dataset Obman and real-world datasets HO3D(v2) and MOW verify the effectiveness of DDF-HO on reconstructing high-quality hand-held objects.

**Limitations.** DDF-HO naturally inherits the shortcomings of DDF. First, the higher dimensional input of DDF makes it harder to train than SDF, resulting in more complex data, algorithm and network structure requirements. This may hinder the performance of DDF-based methods when scaling up to large-scale scenes, like traffic scenes. Second, to enable more photorealistic object reconstruction, there are other characteristics like translucency, material and appearance need to be properly represented. This requires further research to fill the gap.

**Acknowledgements.** This work was supported by the National Key R&D Program of China under Grant 2018AAA0102801, National Natural Science Foundation of China under Grant 61827804. We also appreciate Yufei Ye, Tristan Aumentado-Armstrong, Zerui Chen and Haowen Sun for their insightful discussions.