# Gradients of Functions of Large Matrices

Nicholas Kramer, Pablo Moreno-Munoz, Hrittik Roy, Soren Hauberg

Technical University of Denmark

Kongens Lyngby, Denmark

{pekra, pabmo, hroy, sohau}@dtu.dk

###### Abstract

Tuning scientific and probabilistic machine learning models - for example, partial differential equations, Gaussian processes, or Bayesian neural networks - often relies on evaluating functions of matrices whose size grows with the data set or the number of parameters. While the state-of-the-art for _evaluating_ these quantities is almost always based on Lanczos and Arnoldi iterations, the present work is the first to explain how to _differentiate_ these workhorses of numerical linear algebra efficiently. To get there, we derive previously unknown adjoint systems for Lanczos and Arnoldi iterations, implement them in JAX, and show that the resulting code can compete with Diffrax when it comes to differentiating PDEs, GPyTorch for selecting Gaussian process models and beats standard factorisation methods for calibrating Bayesian neural networks. All this is achieved without any problem-specific code optimisation. Find the code at https://github.com/phraemer/experiments-lanczos-adjoints and install the library with pip install matfree.

## 1 Introduction

Automatic differentiation has dramatically altered the development of machine learning models by allowing us to forego laborious, application-dependent gradient derivations. The essence of this automation is to evaluate Jacobian-vector and vector-Jacobian products without ever instantiating the full Jacobian matrix, whose column count would match the number of parameters of the neural network. Nowadays, everyone can build algorithms around matrices of unprecedented sizes by exploiting this _matrix-free_ implementation. However, differentiable linear algebra for Jacobian-vector products and similar operations has remained largely unexplored to this day. _We introduce a new matrix-free method for automatically differentiating functions of matrices_. Our algorithm yields the exact gradients of the forward pass, all gradients are obtained with the same code, and said code runs in linear time- and memory-complexity.

For a parametrised matrix \(A=A()^{N N}\) and an analytic function \(f:\), we call \(f(A)\) a function of the matrix (different properties of \(A\) imply different definitions of \(f(A)\); one of them is applying \(f\) to each eigenvalue of \(A\) if \(A\) is diagonalisable; see ). However, we assume that \(A\) is the Jacobian of a large neural network or a matrix of similar size and never materialise \(f(A)\). Instead, we only care about the values and gradients of the matrix-function-vector product

\[(,v) f[A()]v\] (1)

assuming that \(A\) is only accessed via differentiable matrix-vector products. Table 1 lists examples.

Evaluating Equation 1 is crucial for building large machine learning models, e.g., Bayesian neural networks: A common hyperparameter-calibration loss of a (Laplace-approximated) Bayesian neural network involves the log-determinant of the generalised Gauss-Newton matrix 

\[A()_{(x_{i},y_{i})}[D_{}g](x_{i})^{ }[D_{}^{2}](y_{i},g(x_{i}))[D_{}g](x_{i})+^{2}I,\] (2)where \(D_{}g\) is the parameter-Jacobian of the neural network \(g\), \(D_{g}^{2}\) is the Hessian of the loss function \(\) with respect to \(g(x_{i})\), and \(\) is a to-be-tuned parameter. The matrix \(A()\) in Equation 2 has as many rows and columns as the network has parameters, which makes traditional, cubic-complexity linear algebra routines for log-determinant estimation entirely unfeasible. To compute this log-determinant, one chooses between either (i) simplifying the problem by pretending that the Hessian matrix is more structured than it actually is, e.g., diagonal ; or (ii) approximating \((A)\) by combining stochastic trace estimation 

\[(A)=[v^{}Av] _{=1}^{L}v_{}^{}Av_{}, [vv^{}]=I,\] (3)

with a Lanczos iteration \(A() QHQ^{}\), to reduce the log-determinant to [17; 18]

\[(A)=( A)_{ =1}^{L}v_{}^{}(A)v_{}_{=1}^{L} v_{}^{}Q(H)Q^{}v_{}.\] (4)

The matrix \(H\) in \(A QHQ^{}\) has as many rows/columns as we are willing to evaluate matrix-vector products with \(A\); thus, it is small enough to evaluate the matrix-logarithm \((H)\) in cubic complexity.

ContributionsThis article explains how to differentiate not just log-determinants but any Lanczos and Arnoldi iteration so we can build loss functions for large models with such matrix-free algorithms (thereby completing the pipeline in Figure 1). This kind of functionality has been sorely missing from the toolbox of differentiable programming until now, even though the demand for functions of matrices is high in all of probabilistic and scientific machine learning [e.g. 2; 3; 4; 5; 6; 7; 8; 9; 10; 11; 12; 19; 20; 21; 22; 23; 24; 25; 26; 27; 28; 29; 30; 31].

## 2 Related work

Here, we focus on applications in machine learning and illustrate how prior work avoids differentiating matrix-free decomposition methods like the Lanczos and Arnoldi iterations. Golub and Meurant  discuss applications outside machine learning.

_Generative models_, e.g., normalising flows [27; 33], rely on the change-of-variables formula, which involves the log-determinant of the Jacobian matrix of a neural network. Behrmann et al.  and Chen et al.  combine stochastic trace estimation with a Taylor-series expansion for the matrix logarithm. Ramesh and LeCun  use Chebyshev expansions instead of Taylor expansions. That said, Ubaru et al.  demonstrate how both methods converge more slowly than the Lanczos iteration when combined with stochastic trace estimation.

_Gaussian process model selection_ requires values and gradients of log-probability density functions of Gaussian distributions (which involve log-determinants), where the covariance matrix \(A()\) has as many rows and columns as there are data points . Recent work [6; 7; 11; 19; 23; 36] all uses some

   Application & Function \(f\) & Matrix \(A\) & Vector \(v\) & Parameter \(\) \\  PDEs \& flows [2; 3; 4; 5] & \(e^{}\) & PDE discret. & PDE initial value & PDE \\ Gaussian process [6; 7; 8] & \(()\) & Kernel matrix & \(v\) Rademacher & Kernel \\ Invert. ResNets [9; 10] & \((1+)\) & Jacobian matrix & \(v\) Rademacher & Network \\ Gaussian sampler  & \(\) & Covariance matrix & \(v N(0,I)\) & Covariance \\ Neural ODE  & \(^{2}\) & Jacobian matrix & \(v\) Rademacher & Network \\   

Table 1: Some applications for functions of matrices. Log-determinants apply by combining \((A)=((A))\) with stochastic trace estimation, which is why most vectors in this table are Rademacher samples. “PDE” / “ODE” = “Partial/Ordinary differential equation”.

Figure 1: Values (down) and gradients (up) of functions of large matrices.

combination of stochastic trace estimation with the Lanczos iteration, and unanimously identifies gradients of log-determinants as ("\(\)" shall be an infinitesimal perturbation; see Section 4)

\[:=(K()),=K()^{- 1}K()\,.\] (5)

Another round of stochastic trace estimation then estimates \(\)[6; 23; 36]. In contrast, our contribution is more fundamental: not only do we derive the exact gradients of the forward pass, but our formulation also applies to, say, matrix exponentials, whereas Equation 5 only works for log-determinants. Section 5 shows how our black-box gradients match state-of-the-art code for Equation 5.

_Laplace approximations and neural tangent kernels_ face the same problem of computing derivatives of log-determinants but with the generalised Gauss-Newton (GGN) matrix from Equation 2. In contrast to the Gaussian process literature, prior work on Laplace approximations prefers structured approximations of the GGN by considering subsets of network weights [37; 38; 39], or algebraic approximations of the GGN via diagonal, KFAC, or low-rank factors [31; 40; 41; 42; 43; 44]. All such approximations imply simple expressions for log-determinants, which are straightforward to differentiate automatically. Unfortunately, these approximations discard valuable information about the correlation between weights, so a linear-algebra-based approach leads to superior likelihood calibration (Section 7).

_Linear differential equations_, for instance \((t)=Ay(t)\), \(y(0)=y_{0}\) are solved by matrix exponentials, \(y(t)=(At)y_{0}\). By this relation, matrix exponentials have frequent applications not just for the simulation of differential equations [e.g. 2; 45], but also for the construction of exponential integrators [3; 26; 29], state-space models [5; 46], and in generative modelling [4; 26; 28; 47]. There are many ways of computing matrix exponentials [48; 49], but only Al-Mohy and Higham  consider the problem of differentiating it and only in forward mode. In contrast, differential equations have a rich history of adjoint methods [e.g. 51; 52] with high-performance open-source libraries [53; 54; 55; 56]. Still, the (now differentiable) Arnoldi iteration can compete with state-of-the-art solvers in JAX (Section 6).

## 3 Problem statement

Recall \(A=A()^{N N}\) from Section 1. The focus of this paper is on matrices that are too large to store in memory, like Jacobians of neural networks or discretised partial differential equations:

**Assumption 3.1**.: \(A()\) _is only accessed via differentiable matrix-vector products \((,v) A()v\)._

The _de-facto_ standard for linear algebra under Assumption 3.1 are matrix-free algorithms [e.g. 57, Chapters 10 & 11], like the conjugate gradient method for solving large sparse linear systems . But there is more to matrix-free linear algebra than conjugate gradient solvers: Matrix-free implementations of matrix decompositions usually revolve around variations of the Arnoldi iteration , which takes an initial vector \(v^{N}\) and a prescribed number of iterations \(K\) and produces a column-orthogonal \(Q^{N K}\), structured \(H^{K K}\), residual vector \(r^{N}\), and length \(c\) such that

\[AQ=QH+r(e_{K})^{}, Qe_{1}=cv\] (6)

hold (Figure 2; \(e_{1},e_{K}^{K}\) are the first and last unit vectors). If \(A\) is symmetric, \(H\) is tridiagonal, and the Arnoldi iteration becomes the _Lanczos iteration_. Both iterations are popular for implementing matrix-function-vector products in a matrix-free fashion [1; 57], because the decomposition in Equation 6 implies \(A QHQ^{}\), thus

\[(,v) f(A())v Qf(H)Q^{}v=c^{-1}Qf(H)e_{1}.\] (7)

The last step, \(Q^{}v=c^{-1}e_{1}\), is due to the orthogonality of \(Q\). Since the number of matrix-vector products \(K\) rarely exceeds a few hundreds or thousands, the following Assumption 3.2 is mild:

**Assumption 3.2**.: _The map \(H f(H)e_{1}\) is differentiable, and \(Q\) fits into memory._

In summary, we evaluate functions of large matrices by firstly decomposing a large matrix into a product of small matrices (with Lanczos or Arnoldi) and, secondly, using conventional linear algebra to evaluate functions of small matrices. Functions of small matrices can already be differentiated efficiently [60; 61; 62]. _This work contributes gradients of the Lanczos and Arnoldi iteration under Assumptions 3.1 and 3.2, and thereby makes matrix-free implementations of matrix decompositions and functions of large matrices (reverse-mode) differentiable.

Figure 2: Lanczos/Arnoldi iteration.

Automatic differentiation, i.e. "backpropagating through" the matrix decomposition, is far too inefficient to be a viable option (Figure 3; setup in Appendix A). Our approach via implicit differentiation or the adjoint method, respectively, leads to gradients that inherit the linear runtime and memory-complexity of the forward pass.

Limitations and future workThe landscape of Lanczos- and Arnoldi-style matrix decompositions is vast, and some adjacent problems cannot be solved by this single article: (i) Forward-mode derivatives would require a derivation separate from what comes next. Yet, since functions of matrices map many to few parameters (matrices to vectors), reverse-mode is superior to forward-mode anyway [66, p. 153]. (ii) We only consider real-valued matrices (for their relevance to machine learning), even though the decompositions generalise to complex arithmetic with applications in physics . (iii) We assume \(Q\) fits into memory, which relates to combining Arnoldi/Lanczos with full reorthogonalisation . Relaxing this assumption requires gradients of partial reorthogonalisation (among other things), which we leave to future work.

## 4 The method: Adjoints of the Lanczos and Arnoldi iterations

Numerical algorithms are rarely differentiated automatically, and usually, some form of what is known as "implicit differentiation"  applies. The same is true for the Lanczos and Arnoldi iterations. However, and perhaps surprisingly, we differentiate the iterations like a dynamical system using the "adjoint method" , a variation of implicit differentiation that uses Lagrange multipliers , and not like a linear algebra routine . To clarify this distinction, we briefly review implicit differentiation before the core contributions of this work in Sections 4.1 and 4.2.

NotationLet \(x\) be an infinitesimal perturbation of some \(x\). \(D\) is the Jacobian operator, and \(,\) the Euclidean inner product between two equally-sized inputs. For a loss \(\) that depends on some \(x\), the linearisation \(=D_{x}\,x\) and the gradient identity \(=_{x},x\) will be important .

Implicit differentiationLet \(: x\) be a numerical algorithm that computes some \(x\) from some \(\). Assume that the input and output of \(()\) satisfy the constraint \((,())=0\). For instance, if \(()\) solves \(Ax=b\), the constraint is \((A,b;x)=Ax-b\) with \(\{A,b\}\). We can use \(()\) in combination with the chain rule to find the derivatives of \(()\), \((=0\) implies \(=0)\)

\[0=(,x)=D_{x}(,x)x+D_{ }(,x).\] (8)

In other words, we "linearise" the constraint \((,x)=0\). The adjoint method  proceeds by "transposing" this linearisation as follows. Let \(\) be a loss that depends on \(y\) with gradient \(_{y}\) and recall the gradient identity from the "Notation" paragraph above. Then, for all Lagrange multipliers \(\) with the same shape as the outputs of \(()\), we know that since \(=0\) implies \(=0\),

\[=_{x},x=_{x},x+,= _{x}+(D_{x})^{},x+(D_{ })^{},\] (9)

must hold. By matching Equation 9 to \(=_{},\) (this time, regarding \(\) as a function of \(\), not of \(x\); recall the "Notation" paragraph), we conclude that if \(\) solves the adjoint system

\[_{x}+(D_{x})^{}=0,\] (10)

then \(_{}(D_{})^{}\) must be the gradient of \(\) with respect to input \(\). This is the adjoint method [66, Section 10.4]. In automatic differentiation frameworks like JAX , this gradient implements a vector-Jacobian product with the Jacobian of \(()\) - implicitly via the Lagrange multiplier \(\), without differentiating "through" \(()\) explicitly. In comparison to approaches that explicitly target vector-Jacobian products with implicit differentiation [like 66, Proposition 10.1], the adjoint method shines when applied to highly structured, non-vector-valued constraints, such as dynamical systems or the Lanczos and Arnoldi iterations. The reason is that the adjoint method does not change if \(()\) becomes matrix- or function-space-valued, as long as we can define inner products and adjoint operators, whereas other approaches (like what Blondel et al.  use for numerical optimisers) would become increasingly laborious in these cases. In summary, to reverse-mode differentiate a

Figure 3: Backpropagation vs our adjoint method on a sparse matrix .

[MISSING_PAGE_FAIL:5]

Sketch of the proof.: This theorem is proven similarly to that of Theorem 4.1, but instead of a few equations involving matrices, we have many equations involving scalars because for symmetric matrices, \(H\) must be tridiagonal , and we expand \(AQ=QH+r(e_{K+1})^{}\) column-wise. The coefficients \(a_{k}\) and \(b_{k}\) are the tridiagonal elements in \(H\). We rename \(q_{k}\) from Arnoldi to \(x_{k}\) for Lanczos to make it easier to distinguish the two different sets of constraints. Details: Appendix C. 

### Matrix-free implementation

Solving the adjoint systemsTo compute \(_{A}\) and \(_{v}\), we need to solve the adjoint systems. When comparing the forward constraints to the adjoint systems, similarities emerge: for instance, the adjoint system of the Arnoldi iteration follows the same \(A^{()}H- H^{()}+=0\) structure as the forward constraint. This structure suggests deriving a recursion for the backward pass that mirrors that of the forward pass. Appendix E contains this derivation and contrasts the resulting algorithm with that of the forward pass. The main observation is that the complexity of the adjoint passes for Lanczos and Arnoldi mirrors that of the forward passes. Gradients can be implemented purely with matrix-vector products, which is helpful because it makes our custom backward pass as matrix-free as backpropagation "through" the forward pass would be. This matrix-free implementation in combination with the efficient recursions in Theorems 4.1 and 4.2 explains the significant performance gains of our method compared to naive backpropagation, observed in Figure 3.

Theorems 4.1 and 4.2's expressions for \(_{A}\) are not directly applicable when we only have matrix-vector products with \(A\). Fortunately, parameter-gradients emerge from matrix-gradients:

**Corollary 4.3** (Parameter gradients).: _Under Assumption 3.1 and the assumptions of Theorem 4.1, and if \(A\) is parametrised by some \(\), the gradients of \(\) with respect to \(\) are_

\[_{}=_{k=1}^{K}[(e_{k})^{}Q^{ }A()^{} e_{k}],\] (19)

_which can be assembled online during the backward pass. For the Lanczos iteration, we assume the conditions of Theorem 4.2 instead of Theorem 4.1, replace \(Qe_{k}\) and \( e_{k}\) with \(x_{k}\) and \(_{k}\), let the sum run from \(k=0\) to \(k=K\), and the rest of this statement remains true._

Sketch of the proof.: The proof of this identity combines the expression(s) for \(_{A}\) from Theorems 4.1 and 4.2 with \(A=D_{}A\). The derivations are lengthy and therefore relegated to Appendix D. 

ReorthogonalisationIt is well known that the Lanczos and Arnoldi iterations suffer from a loss of orthogonality and that reorthogonalisation of the columns in \(Q\) is often necessary . Reorthogonalisation does not affect the forward constraints, so the adjoint systems remain the same with and without reorthogonalisation. But adjoint systems also suffer from a loss of orthogonality: The equivalent of orthogonality for the adjoint system is the projection constraint in Equation 13b, which constrains the Lagrange multipliers \(\) to a hyperplane defined by \(Q\) and other known quantities. The constraint can - and should (Table 2) - be used whenever the forward pass requires reorthogonalisation.1 In the case studies below, we always use full reorthogonalisation on the forward and adjoint pass, also for the Arnoldi iteration [71, Table 7.1], even though this is slightly less common than for the Lanczos iteration.

Summary (before the case studies)The main takeaway from Sections 4.1 and 4.2 is that now, we do not only have closed-form expressions for the gradients of Arnoldi and Lanczos iterations (Theorems 4.1 and 4.2), but that we can compute them in the same complexity as the forward pass, in a numerically stable way, and evaluate parameter-gradients in linear time- and space-complexity (Corollary 4.3). While some of the derivations are somewhat technical, the overall

    & Loss of accuracy \\  Adjoint w/o proj. & \(5.83 10^{-3}\) \\ Adjoint w/ proj. & \(}\) \\ Backprop. & \(}\) \\   

Table 2: Accuracy loss when differentiating the Arnoldi iteration on a Hilbert matrix in double precision (\(:\) decompose with a full-rank Arnoldi iteration, then reconstruct the original matrix; measure \(\|-I\|\); details in Appendix F).

approach follows the general template for the adjoint method relatively closely. The resulting algorithm beats backpropagation "through" the iterations by a margin in terms of speed (Figure 3) and enjoys the same stability gains from reorthogonalisation as the forward pass (Table 2). Our open-source implementation of reverse-mode differentiable Lanczos and Arnoldi iterations can be installed via "pip install matfree". Next, we put this code to the test on three challenging machine-learning problems centred around functions of matrices to see how it fares against state-of-the-art differentiable implementations of exact Gaussian processes (Section 5), differential equation solvers (Section 6), and Bayesian neural networks (Section 7).

## 5 Case study: Exact Gaussian processes

Model selection for Gaussian processes has arguably been the strongest proponent of the Lanczos iteration and similar matrix-free algorithms in recent years [6; 7; 11; 19; 20; 23; 80], and most of these efforts have been bundled up in the GPTorch library . For example, GPTorch defaults to choosing a Lanczos iteration over a Cholesky decomposition as soon as the dataset exceeds 800 data points.2 Calibrating hyperparameters of Gaussian process models involves optimising log-marginal-likelihoods of the regression targets, which requires computing \(x^{}A^{-1}x\) and \((A)\) for a covariance matrix \(A\) with as many rows and columns as there are data points. Recent works [6; 7; 11; 19; 20; 23; 80] unanimously suggest to differentiate log-determinants via \(\ ((A))\) and \(=\ (A^{-1}A)\) (Equation 5). Since we seem to be the first to take a different path, benchmarking Gaussian processes in comparison to GPTorch is a good first testbed for our gradients.

Setup: Like GPyTorch's defaultsWe mimic recent suggestions for scalable Gaussian process models [7; 19]: we implement a pivoted Cholesky preconditioner  and combine it with conjugate gradient solvers for \(x^{}K^{-1}x\) (which can be differentiated efficiently). We estimate the log-determinant stochastically via \((A)=\ ((A))=E[v^{}(A)v]\), and compute \((A)v\) via the Lanczos iteration. While all of the above is common for "exact" Gaussian processes [6; 7; 19] ("exact" as opposed to variational approaches, which are not relevant for this comparison), there are three key differences between our code and GPyTorch's: (i) GPyTorch is in Pytorch and uses KeOps  for efficient kernel-matrix-vector products. We use JAX and must build our own low-memory matrix-vector products (Appendix G). (ii) GPyTorch runs all algorithms adaptively (we specify tolerances and maximum iterations as much as possible). We use adaptive conjugate gradient solvers and fixed ranks for everything else. _(iii) GPytorch differentiates the log-determinant with a tailored approximation of Equation 5; we embed our gradients of the Lanczos iteration into automatic differentiation._ To keep the benchmark objective, we mimic the parameter suggestions from GPyTorch's default settings, and optimise hyperparameters of a Matern\(()\) model on UCI datasets with the Adam optimiser . Appendix H lists parameters and discusses the datasets.

   Dataset & Size & Dim. & Method & RMSE \(\) & Final training loss \(\) & Runtime (s/epoch) \(\) \\   &  &  & Adjoints & 0.09 \(\) 0.002 &   } & **-0.91 \(\) 0.025** & 1.69 \(\) 0.000 \\  & & GPyTorch & 0.09 \(\) 0.003 & -0.63 \(\) 0.062 & \(\) \\   &  &  & Adjoints & 0.39 \(\) 0.005 & 0.73 \(\) 0.300 & 12.62 \(\) 0.172 \\  & & GPyTorch & 0.39 \(\) 0.005 & 0.73 \(\) 0.075 & \(\) \\   &  &  & Adjoints & 0.12 \(\) 0.004 &   } & **-0.30 \(\) 0.078** & 8.27 \(\) 0.004 \\  & & GPyTorch & 0.10 \(\) 0.010 & -0.26 \(\) 0.094 & \(\) \\   &  &  & Adjoints & 0.12 \(\) 0.002 & -0.59 \(\) 0.295 & 13.25 \(\) 0.005 \\  & & GPyTorch & 0.12 \(\) 0.005 & -0.41 \(\) 0.054 & \(\) \\   &  &  & Adjoints & 0.12 \(\) 0.002 &   } & **-0.69 \(\) 0.263** & 24.20 \(\) 0.004 \\  & & GPyTorch & 0.12 \(\) 0.003 & -0.40 \(\) 0.039 & \(\) \\   

Table 3: Our method yields the same root-mean-square errors (RMSEs) as GPyTorch. It reaches lower training losses but is \(\) 20\(\) slower per epoch due to different matrix-vector-product backends (see Appendix G). Three runs, significant improvements in bold. We use an 80/20 train/test split.

Analysis: Trains like GPyTorch; large scale only limited by matrix-vector-product backendsIn this benchmark, we are looking for low reconstruction errors, fast runtimes and well-behaved loss functions. Table 3 shows that this is the case for both implementations: the reconstruction errors are essentially the same, and both methods converge well (we achieve lower training losses). This result shows that by taking a numerically exact gradient of the Lanczos iteration, and leaving everything else to automatic differentiation, matches the performance of state-of-the-art solvers. Larger datasets are only limited by the efficiency of our matrix-vector products (in comparison to KeOps); Appendix G discusses this in detail. Overall, this result strengthens the democractisation of exact Gaussian processes because it reveils a simple yet effective alternative to GPyTorch's domain-specific gradients.

## 6 Case study: Physics-informed machine learning with PDEs

Much of the paper thus far discusses functions of matrices in the context of log-determinants. So, in order to demonstrate performance for (i) a problem that is not a log-determinant and (ii) for a non-symmetric matrix which requires Arnoldi instead of Lanczos, we learn the coefficient field \(\) of

\[}{ t^{2}}u(t;x_{1},x_{2})=(x_{1},x_{2})^{2} [}{ x_{1}^{2}}u(t;x_{1},x_{2})+}{ x_{2}^{2}}u(t;x_{1},x_{2})]\] (20)

subject to Neumann boundary conditions. We discretise this equation on a \(128 128\) grid in space and transform the resulting \(128^{2}\)-dimensional second-order ordinary differential equation into a first-order differential equation, \(=Aw\), \(w(0)=w_{0}\), with solution operator \(w(t)=(At)w_{0}\). The system matrix \(A\) is sparse, asymmetric, and has \(32,768\) rows and columns. We sample a true \(\) from a Gaussian process with a square exponential kernel and generate data by sampling \(256\) initial conditions and solving the equation numerically with high precision. Details are in Appendix I.

Setup: Arnoldi vs Diffrax's Runge-Kutta methods for a 250k parameter MLPWe learn \(\) with a multi-layer perceptron (MLP) with approximately 250,000 parameters. We had similar reconstructions with fewer parameters but use 250,000 to display how gradients of the Arnoldi iteration scale to many parameters. We compare an implementation of the solution operator \((,w_{0})(A())w_{0}\) with the Arnoldi iteration to Diffrax's  implementation of "Dopri5" [84; 85] with a differentiate-then-discretise adjoint  as well as "Tsit5"  with a discretise-then-differentiate adjoint (recommended by [53; 54]). All methods receive equal matrix-vector products per simulation.

Analysis: All methods train, but Arnoldi is more accurate for fixed matrix-vector-product budgetsWe evaluate the approximation errors in computing the values and gradients of a mean-squared error loss for all three solvers and then use the solvers to train the MLP. We are looking for low approximation errors for few matrix-vector products and for a good reconstruction of the truth. Figure 5 shows the results. The Arnoldi iteration has the lowest forward-pass and gradient error, but Table 4 demonstrates how all ap

Figure 4: All methods find the truth.

Figure 5: Arnoldi’s superior convergence on the forward pass (A1) is inherited by the gradients (A2; mind the shared \(y\)-axis) and ultimately leads to fast training (B). For training, Arnoldi uses ten matrix-vector products, and the other two use 15 (so they have equal error \( 10^{-4}\) in A1 and A2.)

proaches lead to low errors on \(\) as well as on a test set (a held-back percentage of the training data); see also Figure 4. The adjoints of the Arnoldi iteration match the efficiency of the differentiate-then-discretise adjoint , and both outperform the discretise-then-differentiate adjoint by a margin. This shows how linear-algebra solutions to matrix exponentials can compete with highly optimised differential equation solvers. We anticipate ample opportunities of using the now-differentiable Arnoldi iteration for physics-based machine learning.

## 7 Case study: Calibrating Bayesian neural networks

Next, we differentiate a function of a matrix on a problem that is native to machine learning: marginal likelihood optimisation of a Bayesian neural network (a high-level introduction is in Appendix J).

Setup: Laplace-approximation of a VAN pre-trained on ImageNetWe consider as \(g_{}(x)\) a "Visual Attention Network"  with 4,105,800 parameters, pre-trained on ImageNet . We assume \(p()=N(0,^{-2}I)\), and Laplace-approximate the log-marginal likelihood of the data as

\[ p(y x) p(y, x)-(A())+ \] (21)

where \(A()\) is the generalised Gauss-Newton matrix (GGN) from Section 1 (recall Equation 2). We optimise \(\) via Equation 21, implementing the log-determinant via stochastic trace estimation in combination with a Lanczos iteration (like in Section 5). Contemporary works  rely on sparse approximations of the GGN (such as diagonal or KFAC approximations), so we compare our implementation to a diagonal approximation of the GGN matrix, which yields closed-form log-determinants. The exact diagonal of the 4-million-column GGN matrix would require 4 million GGN-vector products with unit vectors, and like Deng et al. , we find this too expensive and resort to stochastic diagonal approximation (similar to trace estimation; all details are in Appendix J). We give both the stochastic diagonal approximation and our Lanczos-based estimator exactly 150 matrix-vector products to approximate Equation 21. We compare the evolution of the loss function over time and various uncertainty calibration metrics. Figure 6 demonstrates training and Table 5 shows results.

Analysis: Lanczos uses matrix-vector products better (by a margin)The results suggest how, for a fixed matrix-vector-product budget, Lanczos achieves a drastically better likelihood at a similar computational budget and already shows significant improvement with a much smaller budget. Lanczos outperforms the diagonal approximation on all metrics except ECE. The subpar performance of the diagonal approximation matches the observations of Ritter et al. ; see also . The main takeaway from this study is that differentiable matrix-free linear algebra unlocks new techniques for Laplace approximations and allows further advances for Bayesian neural networks in general.

    & Arnoldi (adjoints; ours) & Dopri5 (diff. \(\) disc.) & Tsit5 (disc. \(\) diff.) \\  Loss on test set & 6.1e-03 \(\) 3.3e-04 & 6.3e-03 \(\) 5.7e-04 & 5.9e-03 \(\) 2.2e-04 \\ Parameter RMSE & 2.9e-04 \(\) 4.4e-05 & 2.6e-04 \(\) 5.0e-05 & 2.7e-04 \(\) 5.2e-05 \\ Runtime per epoch & **7.7e-02 \(\) 1.8e-05** & **7.2e-02 \(\) 3.4e-05** & 2.7e-01 \(\) 1.1e-05 \\   

Table 4: All three methods reconstruct the parameter well (std.-deviations exceed differences for test-loss and RMSE), but Arnoldi and Dopri5 are faster than Tsit5. Dopri5 uses the BacksolveAdjoint, and Tsit5 the RecursiveCheckpointAdjoint in Diffrax . We contribute Arnoldi’s adjoints.

Figure 6: Lanczos vs diagonal approx. for a Bayesian VAN.