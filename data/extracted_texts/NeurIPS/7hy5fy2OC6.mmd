# Invisible Image Watermarks Are Provably Removable Using Generative AI

Xuandong Zhao

UC Berkeley

Equal contribution. Email addresses: xuandongzhao@berkeley.edu kexunz@andrew.cmu.edu

&Kexun Zhang1

Carnegie Mellon University

&Zihao Su

UC Santa Barbara

&Saastha Vasan

UC Santa Barbara

&Ilya Grishchenko

UC Santa Barbara

&Christopher Kruegel

UC Santa Barbara

&Giovanni Vigna

UC Santa Barbara

&Yu-Xiang Wang

UC San Diego

&Lei Li

Carnegie Mellon University

###### Abstract

Invisible watermarks safeguard images' copyrights by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack. Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality. However, watermarks that keep the image semantically similar can be an alternative defense against our attacks. Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks. Code is available at [https://github.com/XuandongZhao/WatermarkAttacker](https://github.com/XuandongZhao/WatermarkAttacker).

## 1 Introduction

Generative models like DALL-E , Imagen , and Stable Diffusion  can produce images that are often visually indistinguishable from those created by human photographers and artists, potentially leading to misunderstandings and false beliefs due to their visual similarity. To address this, government leaders  advocate for the responsible use of AI, emphasizing the importance of identifying AI-generated content. In response, leading AI companies such as Google , Microsoft , and OpenAI  have pledged to incorporate _watermarks_ into their AI-generated images. _Invisible_ watermarks  are preferred as they preserve image quality and are less likely to be removed by laypersons. However, abusers are aware of these watermarks and may attempt to remove them, making it crucial for invisible watermarks to be robust against evasion attacks.

Existing attacks can be categorized into two types. The first is _destructive_, where the watermark is removed by corrupting the image. Typical destructive attacks include modifying the brightness, JPEG compression, and adding Gaussian noise. These approaches are effective at removing watermarks, butthey result in significant quality loss. The second type of attack is _constructive_, where the watermark is treated as some noise on the original image and removed by purifying the image. Constructive attacks include image-denoising techniques like Gaussian blur , BM3D , and learning-based approaches . However, they cannot remove resilient watermarks easily. To counter these attacks, learning-based watermarking methods  were proposed to explicitly train against the known attacks to be robust. But how about other attacks? What is the end of this cat-and-mouse game? In this paper, we ask a more fundamental question:

_Is an invisible watermark_ necessarily _non-robust?_

To be more precise, is there a fundamental trade-off between the invisibility of a watermark and its resilience to any attack that preserves the image quality to a certain-level?

To address this question, we propose a regeneration attack that leverages the strengths of both destructive and constructive approaches. The pipeline of the attack is given in Figure 1. Our attack first corrupts the image by adding Gaussian noise to its latent representation. Then, we reconstruct the image from the noisy embedding using a generative model. The proposed regeneration attack is flexible in that it can be instantiated with various regeneration algorithms, including traditional denoisers and deep generative models such as diffusion models . Ironically, the recent advances in generative models that created the desperate need for invisible watermarks are also making watermark removal easier when integrated into the proposed regeneration attack.

Surprisingly, we prove that the proposed attack guarantees the removal of certain invisible watermarks such that _no_ detection algorithm could work. In other words, our attack is provably effective in removing any watermarks that perturb the image within a limited range of \(_{2}\)-distance, regardless of whether they have been proposed or have not yet been invented. We also show that our attack maintains image quality comparable to the unwatermarked original.

To validate our theory, we conduct extensive experiments on four widely used invisible watermarks . Our proposed attack method significantly outperforms five baseline methods in terms of both image quality and watermark removal. For the resilient watermark scheme RivaGAN, our regeneration attacks successfully remove 98% of the invisible watermarks while maintaining a PSNR above 30 compared to the original images. With the empirical results and the theoretical guarantee, we claim that pixel-level invisible image watermarks are vulnerable to our regeneration attacks.

Given the vulnerability of invisible watermarks, we explore another option: _semantic_ watermarks. Semantic watermarks do not limit the perturbation to be within an \(_{2}\)-distance. As long as the watermarked image looks similar and contains similar content, it is considered suitable for use. One instance of such semantic watermarks is Tree-Ring , which has shown resilience against our attacks. While not a perfect solution, as the watermark becomes somewhat "visible", semantic watermarking offers a potential path forward for protecting the proper use of AI-generated images when invisible watermarks are provably ineffective.

Figure 1: Removing invisible watermarks: The proposed attack first maps the watermarked image to its embedding, which is another representation of the image. Then the embedding is noised to destruct the watermark. After that, a regeneration algorithm reconstructs the image from the noisy embedding.

Our contributions can be summarized as follows:

* We propose a family of regeneration attacks for image watermark removal that can be instantiated with many existing denoising algorithms and generative models.
* We prove that the proposed attack is guaranteed to remove certain pixel-based invisible watermarks and that the regenerated images are close to the original unwatermarked image.
* We evaluate the proposed attack on various invisible watermarks to demonstrate their vulnerability and its effectiveness compared with strong baselines.
* We explore other possibilities to embed watermarks in a visible yet semantically similar way. Empirical results indicate that this approach works better under our attack and is worth investigating as an alternative.

## 2 Related Work and Background

### Related Work

**Image watermarking and steganography.** Steganography and invisible watermarking are key techniques in information hiding, serving diverse purposes such as copyright protection, privacy-preserved communication, and content provenance. Early works in this area employ hand-crafted methods, such as Least Significant Bit (LSB) embedding , which subtly hides data in the lowest order bits of each pixel in an image. Over time, numerous techniques have been developed to imperceptibly embed secrets in the spatial  and frequency [24; 44] domains of an image. Additionally, the emergence of deep learning has contributed significantly to this field. Deep learning methods offer improved robustness against noise while maintaining the quality of the generated image. SteganoGAN  uses generative adversarial networks (GAN) for steganography and perceptual image optimization. RivaGAN , further improves GAN-based watermarking by leveraging attention mechanisms. SSL watermarking , trained with self-supervision, enhances watermark features through data augmentation. Stable Signature  fine-tunes the decoder of Latent Diffusion Models to add the watermark. Tree-Ring  proposes a semantic watermark, which watermarks generative diffusion models using minimal shifts of their output distribution. This work focuses on the removal of invisible watermarks, as opposed to visible watermarks, for several reasons. Visible watermarks are straightforward for potential adversaries to visually identify and locate within an image. Additionally, removal techniques for visible watermarks are already extensively studied in prior work such as [35; 21; 10]. In contrast, invisible watermarks do not have explicit visual cues that reveal their presence or location. Developing removal techniques for imperceptible embedded watermarks presents unique challenges.

**Deep generative models.** The high-dimensional nature of images poses unique challenges to generative modeling. In response to these challenges, several types of deep generative models have been developed, including Variational Auto-Encoders (VAEs) [56; 55], Generative Adversarial Networks (GANs) , flow-based generative models , and diffusion models [23; 47]. These models leverage deep latent representations to generate high-quality synthetic images and approximate the true data distribution. One particularly interesting use of generative models is for data purification, i.e., removing the adversarial noise from a data sample. The purification is similar to watermark removal except that purification is a defense strategy while watermark removal is an attack. The diffusion-based approach in  is similar to an instance of our regeneration attack, but the usage is different in our paper and our theoretical guarantee of watermark removal is stronger. In this paper, we aim to demonstrate the capability of these deep generative models in removing invisible watermarks from images by utilizing the latent representations obtained through the encoding and decoding processes.

### Problem Setup

This section defines _invisible_ watermarks and the properties of an algorithm for their detection. It then discusses the threat model for the removal of invisible watermarks.

**Definition 2.1** (Invisible watermark).: Let \(x\) be the original image and \(x_{w}=(x,)\) be the watermarked image for a watermarking scheme that is a function of \(x\) and any auxiliary information \(\), e.g., a secret key. We say that the watermark is \(\)_-invisible_ on a clean image \(x\) w.r.t. a "distance" function \(:_{+}(x,x_{w})\).

**Definition 2.2** (Watermark detection).: A watermark detection algorithm \(:\{0,1\}\) determines whether an image \(\) is watermarked with the auxiliary information (secret key) being \(\). \(\) may make two types of mistakes, false positives (classifying an unwatermarked image as watermarked) and false negatives (classifying a watermarked image as unwatermarked). \(\) could be drawn from either the null distribution \(P_{0}\) or watermarked distribution \(P_{1}\). We define Type I error (or false positive rate) \(_{1}:=_{x P_{0}}[(x)=1]\) and Type II error (or false negative rate) \(_{2}:=_{x P_{1}}[(x)=0]\).

A watermarking scheme is typically designed such that \(P_{1}\) is different from \(P_{0}\) so the corresponding (carefully designed) detection algorithm can distinguish them almost perfectly, that is, to ensure \(_{1}\) and \(_{2}\) are nearly \(0\). An attack on a watermarking scheme aims at post-processing a possibly watermarked image which changes both \(P_{0}\) and \(P_{1}\) with the hope of increasing Type I and Type II error at the same time, hence evading the detection.

We consider the following threat model for removing invisible watermarks from images:

**Adversary's capabilities.** We assume that an adversary only has access to the watermarked images. The watermarking scheme \(\), the auxiliary information \(\), and the detection algorithm Detect are unknown to the adversary. The adversary can make modifications to these already watermarked images it has access to using arbitrary side information and computational resources, but it cannot rely on any specific property of the watermarking process and it cannot query \(\).

**Adversary's objective.** The primary objective of an adversary is to render the watermark detection algorithm ineffective. Specifically, the adversary aims to produce an image \(\) from the watermarked image \(x_{w}\) which causes the \(\) algorithm to always have a high Type I error (false positive rate) or a high Type II error (false negative rate). Simultaneously, the output image \(\) should maintain comparable quality to the original, non-watermarked image. The adversary's objective will be formally defined later.

### Invisible Watermark Detection

Watermarking methods implant \(k\)-bit secret information into images. The detection algorithm \(\) uses an extractor for this hidden data, applying a statistical test to ascertain if the extracted message matches the secret. This test evaluates the number of matching bits \(M(m,m^{})\) between the extracted \(m^{}\) and original \(m\{0,1\}^{k}\) messages. A watermark is detected if \(M(m,m^{})\) for a predefined threshold \(\). Formally, we distinguish whether an image \(x\) was watermarked (\(H_{1}\)) or not (\(H_{0}\)). Assuming under null hypothesis \(H_{0}\) that the extracted bits behave like independent Bernoulli variables with a 0.5 probability, \(M(m,m^{})\) follows a binomial distribution \(B(k,0.5)\). The false positive rate (\(_{1}\)) corresponds to the probability that \(M(m,m^{})\) exceeds \(\). This has a closed form using the regularized incomplete beta function \(I_{x}(a;b)\):

\[_{1}()=(M(m,m^{})> H_{0} )=}_{i=+1}^{k}=I_{1/2}(+1,k-).\]

**Decision threshold.** We consider a watermark to be detected, if we can reject the null hypothesis \(H_{0}\) with a \(p\)-value less than \(0.01\). In practice, for a \(k=32\)-bit watermark, we require at least \(23\) bits to be extracted correctly in order to confirm the presence of a watermark. This provides a reasonable balance between detecting real watermarks and avoiding false positives.

## 3 The Proposed Regeneration Attack

Our attack method first destructs a watermarked image by adding noise to its representation, and then reconstructs it from the noised representation.

Specifically, given an embedding function \(:^{n}^{d}\), a regeneration function \(:^{d}^{n}\), and a noise level \(\), the attack algorithm takes a watermarked image \(x_{w}^{n}\) and returns

\[=)+(0, ^{2}I_{d})}^{}}_{}. \]```
1:The watermarked image \(x_{w}\), a time step \(t^{*}\) determining the level of noise added.
2:A reconstructed clean image \(\).
3:\(z_{0}(x_{w})\) // map the watermarked image \(x_{w}\) to latent space
4:\((0,I_{d})\) // sample a random normal Gaussian noise
5:\(z_{t^{*}})}z_{0}+)}\) // add noise to the latent, noise level determined by \(t^{*}\)
6:\(_{0}(z_{t^{*}},t^{*},s,f,g)\) // denoise the noised latent to reconstruct a clean latent
7:\((_{0})\) // map the reconstructed latent back to a watermark-free image
8:return\(\)
```

**Algorithm 1** Regeneration Attack Instance: Removing invisible watermarks with a diffusion model

The first step of the algorithm is _destructive_. It maps the watermarked image \(x_{w}\) to an embedding \((x_{w})\) (which is a possibly different representation of the image), and adds i.i.d. Gaussian noise. The explicit noise shows the destructive nature of the first step. The second step of the algorithm is _constructive_. The corrupted image representation \((x_{w})+(0,^{2}I_{d})\) is passed through a regeneration function \(\) to reconstruct the original clean image.

There are various different choices for \(\) and \(\) that can instantiate our attack. \(\) can be as simple as identity map, or as complicated as deep generative models including variational autoencoders . \(\) can be traditional denoising algorithms from image processing and recent AI models such as diffusion . The choice of \(\) and \(\) may change the empirical results, but it does not affect the theoretical guarantee. In the following sections, we introduce three combinations of \(\) and \(\) to instantiate the attack. Among the three, the diffusion instantiation is the most complicated and we describe it with pseudocode in Algorithm 1.

### Attack Instance 1: Identity Embedding with Denoising Reconstruction

Set \(\) to be identity map, then \(\) can be any image denoising algorithm, e.g., BM3D , TV-denoising , bilateral filtering , DnCNNs , or a learned natural image manifold . A particular example of interest is a "denoising autoencoder" , which takes \(\) to be identity, adds noise to the image deliberately, and then denoises by attempting to reconstruct the image. Observe that for "denoising autoencoder" we do not need to add additional noise.

### Attack Instance 2: VAE Embedding and Reconstruction

The regeneration attack in Equation 1 can be instantiated with a variational autoencoder (VAE). A VAE  consists of an encoder \(q_{}(z|x)\) that maps a sample \(x\) to the latent space \(z\) and a decoder \(p_{}(x|z)\) that maps a latent \(z\) back to the data space \(x\). Both the encoder and decoder are parameterized with neural networks. VAEs are trained with a reconstruction loss that measures the distance from the reconstructed sample to the original sample and a prior matching loss that restricts the latent to follow a pre-defined prior distribution.

Instead of mapping \(x\) directly to \(z\), the encoder maps it to the mean \((x)\) and variance \((x)\) of a Gaussian distribution and samples from it. Therefore, VAE already adds noise during the encoding stage (though its variance depends on the sample \(x\), which is not exactly the same as defined in Equation 1), so there is no need to add extra noise. Note that this is similar to the situation of denoising autoencoders described in Section 3.1, as the denoising autoencoder is a trivial case of VAE where \((x)\) is identity.

### Attack Instance 3: Diffusion Embedding and Reconstruction

The regeneration attack can also be instantiated with diffusion models. Diffusion models  define a generative process that learns to sample from an unknown true distribution \(p(z_{0})\). This process is learned by estimating original samples from randomly noised ones. In other words, diffusion models are trained to denoise, which makes them candidates for the regeneration function \(\) in the proposed attack. The embedding function \(\) can either be identity  or a latent embedding , depending on the space where diffusion is trained.

For diffusion models, the process of adding noise to a clean sample is known as the _forward process_. Likewise, the process of denoising a noisy sample is known as the _backward process_.

The forward process is defined by the following stochastic differential equation (SDE): \(dz=f(z,t)dt+g(t)dw\) (2), where \(t,z^{d}\), \(w(t)^{d}\) is a standard Wiener process, and \(f,g\) are real-valued functions. The backward process can then be described with its reverse SDE: \(d=[f(,t)-g(t)^{2}_{z} p_{t}()]dt+g(t)d \) (3), where \(\) is a reverse Wiener process. Diffusion models parameterize \(_{} p_{t}()\) with a neural network \(s(z,t)\). By substituting \(s(z,t)\) into Equation 3, the backward SDE becomes known and solvable using numerical solvers [34; 36], \(_{0}=(z_{t},t,s,f,g)\).

Among many ways to define \(f\) and \(g\) in Equation 2, variance preserving SDE (VP-SDE) is commonly used [23; 47]. Under this setting, the conditional distribution of the noised sample is the following Gaussian : \(p(z_{t}|z_{0})=(z_{0},1-(t))\) (4), where \((t)\) a pre-defined noise schedule. The variance of the original distribution \(p(z_{0})\) is preserved at any step.

As defined in Algorithm 1, our algorithm removes the watermark from the watermarked image \(x_{w}\) using diffusion models. \(x_{w}\) is first mapped to the latent representation \(z_{0}\), which is then noised to the time step \(t^{*}\). A latent diffusion model is then used to reconstruct the latent \(_{0}\), which is mapped back to an image \(\).

Similar to denoising autoencoders, in either diffusion or VAEs, the noise-injection is integral to the algorithms themselves, and no additional noise-injection is needed.

## 4 Theoretical Analysis

We show in this section that the broad family of regeneration attacks as defined in Equation 1 enjoy provable guarantees on their ability to remove invisible watermarks while retaining the high quality of the original image. Our proofs are deferred to Appendix D. More discussion on the implications and interpretation of our theoretical analysis can be found in Appendix C.

### Certified Watermark Removal

How do we quantify the ability of an attack algorithm to remove watermarks? We argue that if after the attack, no algorithm is able to distinguish whether the result is coming from a watermarked image or the corresponding original image without the watermark, then we consider the watermark certifiably removed. More formally:

**Definition 4.1** (\(f\)-Certified-Watermark-Free).: We say that a watermark removal attack is \(f\)-Certified-Watermark-Free (or \(f\)-CWF) against a watermark scheme for a non-increasing function \(f:\), if for any detection algorithm \(:\{0,1\}\), the Type II error (false negative rate) \(_{2}\) of \(\) obeys that \(_{2} f(_{1})\) for all Type I error \(0_{1} 1\).

Let us also define a parameter to quantify the effect of the embedding function \(\).

**Definition 4.2** (Local Watermark-Specific Lipschitz property).: We say that an embedding function \(:^{d}\) satisfies \(L_{x,w}\)-Local Watermark-Specific Lipschitz property if for a watermark scheme \(w\) that generates \(x_{w}\) with \(x\),

\[\|(x_{w})-(x)\| L_{x,w}\|x_{w}-x\|.\]

The parameter \(L_{x,w}\) measures how much the embedding compresses the watermark added on a particular clean image \(x\). If \(\) is identity, then \(L_{x,w} 1\). If \(\) is a projection matrix to a linear subspace then \(0 L_{x,w} 1\) depending on the magnitude of the component of \(x_{w}-x\) in this subspace. For a neural image embedding \(\), the exact value of \(L_{x,w}\) is unknown but given each \(x_{w}\) and \(x\) it can be computed and estimated efficiently. We defer more discussion to the Appendix A.

**Theorem 4.3**.: _For a \(\)-invisible watermarking scheme with respect to \(_{2}\)-distance. Assume the embedding function \(\) of the diffusion model is \(L_{x,w}\)-Locally Lipschitz. The randomized algorithm \((()+(0,^{2}I_{d}))\) produces a reconstructed image \(\) which satisfies \(f\)-CWF with_

\[f(_{1})=(^{-1}(1-_{1})-}{ }),\]

_where \(\) is the Cumulative Density Function function of the standard normal distribution._Figure 2 illustrates what the tradeoff function looks like. The result says that after the regeneration attack, it is impossible for any detection algorithm to correctly detect the watermark with high confidence. In addition, it shows that such detection is as hard as telling the origin of a single sample \(Y\) from either of the two Gaussian distributions \((0,1)\) and \((L_{x,w}/,1)\). Specifically, when there is a uniform upper bound \(L L_{x,w}\), we can calibrate \(\) such that the attack is provably effective for a _prescribed_\((^{-1}(1-)-L/)\)-CWF guarantee for _all_ input images and _all_\(\)-invisible watermarks (see specific constructions in Appendix A).

The proof, deferred to Appendix D, leverages an interesting connection to a modern treatment of differential privacy  known as the Gaussian differential privacy . The work of  itself is a refinement and generalization of the pioneering work of  and  which established a tradeoff-function view.

### Utility Guarantees

In this section, we prove that the regenerated image \(\) is close to the original (unwatermarked) image \(x_{0}\). This is challenging because the denoising algorithm only gets access to the noisy version of the watermarked image. Interestingly, we can obtain a general extension lemma showing that for any black-box generative model that can successfully denoise a noisy yet unwatermarked image with high probability, the same result also applies to the watermarked counterpart, except that the failure probability is slightly larger.

**Theorem 4.4**.: _Let \(x_{0}\) be an image with \(n\) pixels and \(:^{n}^{d}\) be an embedding function. Let \(\) be an image generation / denoising algorithm such that with probability at least \(1-\), \(\|((x_{0})+(0,^{2}I_{d}))-x_{0}\|_{x_{0 },,}\). Then for any \(\)-invisible watermarking scheme that produces \(x_{w}\) from a clean image \(x_{0}\), then \(=((x_{w})+(0,^{2}I_{d}))\) satisfies that_

\[\|-x_{0}\|_{x_{0},,}\]

_with a probability at least \(1-\), where \(=_{w}\{ e^{v}+(}{2}-)-e^{v}(-}{2}-)\}\) in which \(\) denotes the standard normal CDF and \(:=L_{x_{0},w}\)._

The theorem says that if a generative model is able to denoise a noisy version of the original image, then the corresponding watermark-removal attack using this generative model provably produces an image with similar quality.

**Corollary 4.5**.: _The expression for \(\) above can be (conservatively) simplified to \( e^{^{2}^{2}}{^{2}}}^{1 /2}\). For example if \( L_{x,w}\), then this is saying that if \(_{x_{0},,}\) depends logarithmically on \(1/\), the same exponential tail holds for denoising the watermarked image._

The above result is powerful in that it makes no assumption about what perturbation the watermarking schemes could inject and which image generation algorithm we use. We give a few examples below.

For denoising algorithms with theoretical guarantees, e.g., TV-denoising [27, Theorem 2], our results imply provable guarantees on the utility for the watermark removal attack of the form, "w.h.p., \(\|-x_{0}\|^{2}=(_{2d}(x_{ 0})}{n})\)", i.e., vanishing mean square error (MSE) as \(n\) gets bigger.

Figure 2: Theoretical and empirical trade-off functions for DwtDctSvd watermark detectors after our attack. Trade-off functions indicate how much less Type II error (false negative rate) the detector gets in return by having more Type I error (false positive rate). Theoretically, after the attack, no detection algorithm can fall in the _Impossibility Region_ and have both Type I error and Type II error at a low level. Empirically, the watermark detector performs even worse than the theory, indicating the success of our attack and the validity of the theoretical bound. We use 500 watermarked MS-COCO images with an empirically valid upper bound of \(L=1\) and noise level \(=1.16\). An additional example for the RivaGAN watermark is provided in Figure 12.

For modern deep learning-based image denoising and generation algorithms where worst-case guarantees are usually intractable, Theorem 4.4 is still applicable for each image separately. That is to say, as long as their empirical denoising quality is good on an unwatermarked image, the quality should also be good on its watermarked counterpart.

## 5 Evaluation

Datasets.We evaluate our attack on two types of images: real photos and AI-generated images. For real photos, we use 500 randomly selected images from the MS-COCO dataset . For AI-generated images, we employ the Stable Diffusion-v2.1 model2 from Stable Diffusion , a state-of-the-art generative model capable of producing high-fidelity images. Using prompts from the Stable Diffusion Prompt (SDP) dataset3, we generate 500 images encompassing both photorealistic and artistic styles. This diverse selection allows for a comprehensive evaluation of our attack on invisible watermarks. All experiments are conducted on Nvidia A6000 GPUs.

Watermark settings.We evaluate four publicly available pixel-level watermarking methods: DwtDctSvd , RivaGAN , StegaStamp , and SSL watermark . These methods represent a variety of approaches, ranging from traditional signal processing to recent deep learning techniques, as introduced in Section 2.1. To account for watermarks of different lengths, we use \(k=32\) bits for DwtDctSvd, RivaGAN, and SSL watermark, and \(k=96\) bits for StegaStamp. For watermark detection, we set the decision threshold to reject the null hypothesis with \(p<0.01\), requiring the detection of 23 out of 32 bits or 59 out of 96 bits, respectively, for the corresponding methods, as described in Section 2.3. For watermark extraction, we use the publicly available code for each method with default inference and fine-tuning parameters specified in their papers.

Attack baselines.To thoroughly evaluate the robustness of our proposed watermarking method, we test it against a diverse set of baseline attacks representing common image perturbations. We select both geometric/quality distortions and noise manipulations that could potentially interfere with embedded watermarks. Specifically, the baseline attack set includes: brightness adjustments with enhancement factors of , contrast adjustments with enhancement factors of , JPEG compression at quality levels , Gaussian noise addition with a

Figure 3: Examples of attacks on DwtDctSvd watermarking, including destructive attacks (e.g., brightness change and JPEG compression), constructive attacks (e.g., Gaussian blur), and regeneration attacks using VAEs and diffusion models. Brightness change, JPEG compression, VAE attack, and diffusion attack successfully remove the watermark. The VAE attack over-smooths the image, resulting in blurriness. The diffusion attack maintains high image quality while removing the watermark. Additional attack examples for other watermarking schemes are in Figures 8, 9, 10, 11.

mean of 0 and standard deviations of , and Gaussian blur with radii of . Further details are provided in Appendix B.

Proposed attacks.For regeneration attacks using variational autoencoders, we evaluate two pre-trained image compression models from the CompressAI library : Bmshj2018  and Cheng2020 . Compression factors are set to , where lower factors correspond to more heavily degraded images. For diffusion model attacks, we use the Stable Diffusion-v2.1 model. The number of noise steps is set to  (with \(=\)[0.10, 0.17, 0.23, 0.34, 0.46, 0.57]), and we employ pseudo numerical methods for diffusion models (PNDMs)  to generate samples. By adjusting the compression factors and noise steps, we achieve varying levels of perturbation for regeneration attacks.

Evaluation metrics.We evaluate the quality of attacked and watermarked images compared to the original cover image using two common metrics: Peak Signal-to-Noise Ratio (PSNR) defined as \((x,x^{})=-10_{10}( (x,x^{}))\), for images \(x,x^{}^{c h w}\), and Structural Similarity Index (SSIM)  which measures perceptual similarity. To evaluate the diversity and quality of watermarked images, we use Frechet Inception Distance (FID)  between the distributions of watermarked and unwatermarked images. To evaluate the robustness of the watermark, we compute the True Positive Rate (TPR) at a fixed False Positive Rate (FPR), specifically TPR@FPR=0.01. The detection threshold corresponding to FPR=0.01 is set according to each watermark's default configuration: correctly decoding 23/32, 59/96, and 32/48 bits for the respective watermarking methods, as described in the watermark settings.

### Results and Analysis

This section presents detailed results and analysis of the regeneration attack experiments on different watermarking methods. Some attacking examples are shown in Figure 3.

Figure 4: Quality-detectability tradeoff for four watermarking schemes under eight attack methods on the MS-COCO dataset. Regeneration attacks (Diffusion model, VAE-Cheng2020, and VAE-Bmshj2018) are highlighted for their performance. The x-axis shows image quality metrics (SSIM and PSNR, higher values indicate better quality), while the y-axis represents the detection metric True Positive Rate at 1% False Positive Rate (TPR@FPR=0.01, lower values are better for attackers). The strongest attacker should appear in the lower right corner of these plots. Regeneration attacks demonstrate superior performance compared to other attack methods, achieving both lower TPR and higher image quality. Quality-detectability tradeoff results for the SDP dataset are in Figure 7.

Watermarking performance without attacks.Table 1 summarizes the watermarked image quality and detection rates for images watermarked by the four methods without any attacks. Each method--DwtDctSvd, RivaGAN, SSL, and StegaStamp--successfully embeds and retrieves messages from the images. These methods are post-processing techniques, adding watermarks to existing images. Among them, SSL achieves the highest PSNR and SSIM values, indicating superior perceptual quality and minimal visual distortion compared to the original images. DwtDctSvd achieves the lowest FID scores, suggesting the watermarked images maintain fidelity similar to clean images. In contrast, StegaStamp exhibits a noticeable drop in quality, with the lowest PSNR and highest FID scores. As illustrated in Figure 6, StegaStamp introduced noticeable blurring artifacts.

Watermark removal effectiveness and image quality reservation.Figure 4 and 7 present the quality-detectability tradeoff results from applying various regeneration attacks to remove watermarks. The VAE and diffusion model-based attacks (VAE-Bmshj2018, VAE-Cheng2020, Diffusion) consistently achieve over 99% removal rates for the DctDwtSvd, RivaGAN, and SSL watermarking methods, demonstrating their high effectiveness. In contrast, StegaStamp exhibits the highest robustness, with effective removal only achieved by the diffusion model with substantial noise. This resilience is partially attributed to StegaStamp's lower visual quality and higher perturbation levels (as indicated in Table 1, Figure 6 and Figure 5). Yet, increasing the noise level in diffusion models reduces StegaStamp's resistance. As demonstrated in Figure 4 and 7, higher noise levels improve the removal rate on StegaStamp, aligning with the theory that larger perturbations necessitate more noise for effective removal. This also aligns with the trade-off of lower image quality at higher noise levels. Overall, the consistently high removal rates across various watermarking schemes demonstrate the effectiveness of regeneration attacks for watermark removal. In terms of image quality preservation, the regeneration attacks generally maintain high quality, as indicated by PSNR and SSIM metrics. VAE models yield higher PSNR and SSIM scores, suggesting superior perceptual quality from a GAN-based perspective. However, qualitative inspection of example images in Figure 3 reveals the VAE outputs exhibit some blurring compared to the diffusion outputs. Since PSNR and SSIM are known to be insensitive to blurring artifacts , we conclude that the choice of using regeneration with diffusion models should be guided by the specific requirements of each application.

Potential defense.Although we show that the proposed attack is guaranteed to remove any pixel-based invisible watermarks, it is not impossible to detect AI-generated images. Semantic watermarks offer a viable detection method, with further details and results discussed in Appendix E.

## 6 Conclusion

We proposed a regeneration attack on invisible watermarks that combines destructive and constructive attacks. Our theoretical analysis proved that the proposed regeneration attack is able to remove certain invisible watermarks from images and make the watermark undetectable by any detection algorithm. We showed with extensive experiments that the proposed attack performed well empirically. The proofs and experiments revealed the vulnerability of invisible watermarks. Given this vulnerability, we explored an alternative defense that uses visible but semantically similar watermarks. Our findings on the vulnerability of invisible watermarks underscore the need for shifting the research/industry emphasis from invisible watermarks to their alternatives.

    &  &  \\ Watermark & PSNR & SSIM & FID & TPR/PR-0.01 & [PSNR] & SSIM & FID & TPR/PR-0.01 \\  DwtDctSvd & 30.38 & 0.903 & 5.28 & 1.000 & 37.73 & 0.272 & 0.52 & 1.000 \\ RivaGAN & 30.35 & 0.978 & 10.83 & 1.000 & 40.64 & 0.972 & 13.36 & 1.000 \\ SSL & 41.79 & 0.984 & 18.86 & 1.000 & 41.88 & 0.980 & 23.