# Demystifying the Optimal Performance

of Multi-Class Classification

 Minoh Jeong

Electrical and Computer Engineering

University of Minnesota

Minneapolis, MN 55455

jeong316@umn.edu

&Martina Cardone

Electrical and Computer Engineering

University of Minnesota

Minneapolis, MN 55455

mcardone@umn.edu

&Alex Dytso

Qualcomm Flarion Technology, Inc.

Bridgewater, NJ 08807

odytso2@gmail.com

###### Abstract

Classification is a fundamental task in science and engineering on which machine learning methods have shown outstanding performances. However, it is challenging to determine whether such methods have achieved the Bayes error rate, that is, the lowest error rate attained by any classifier. This is mainly due to the fact that the Bayes error rate is not known in general and hence, effectively estimating it is paramount. Inspired by the work by Ishida et al. (2023), we propose an estimator for the Bayes error rate of supervised multi-class classification problems. We analyze several theoretical aspects of such estimator, including its consistency, unbiasedness, convergence rate, variance, and robustness. We also propose a denoising method that reduces the noise that potentially corrupts the data labels, and we improve the robustness of the proposed estimator to outliers by incorporating the median-of-means estimator. Our analysis demonstrates the consistency, asymptotic unbiasedness, convergence rate, and robustness of the proposed estimators. Finally, we validate the effectiveness of our theoretical results via experiments both on synthetic data under various noise settings and on real data.

## 1 Introduction

Supervised classification problems are typical tasks in various fields of science and engineering, such as machine learning, statistical signal processing, estimation, and detection. In supervised classification, a dataset consisting of several input feature-label pairs is given. The goal of a supervised classification task is to design effective classifiers, by leveraging the given dataset, to suitably label future input features, or equivalently, to classify future input features into one of the classes.

As the dataset is the only available resource on the data distribution, the performance of a classifier is typically measured by its empirical misclassification rate on the test dataset (which is a subset of the given dataset). The best performance of an existing classifier, that is, the so-called state-of-the-art (SOTA) performance, tends to be the point of reference to measure the improvement of a new classifier. However, there are no guarantees that the SOTA performance is close to the theoretical minimum misclassification rate, namely the Bayes error rate (BER). Thus, comparing the empirical misclassification rate with the SOTA error rate provides only a relative improvement.

Having the knowledge of the BER is essential in theory and practice. The BER indeed provides a fundamental limit on the misclassification rate, which is critical for designing high-performingclassifiers. Moreover, one can leverage the BER to assess how good the SOTA performance is with respect to the theoretically optimal error rate. If the SOTA performance is nearly close to the BER, we can avoid wasting time and effort in designing a new classifier. Furthermore, the BER indicates the inherent hardness of a task and hence, it can be seen as a benchmark for comparing the hardness of different tasks [13; 46; 83]. Knowing the BER also brings the opportunity to detect whether test dataset overfitting occurs (which has sporadically happened [3; 51; 59; 91]); this overfitting can be detected since the BER provides the minimum misclassification rate and hence, no classifier will perform strictly better than it. We refer an interested reader to Appendix A for a thorough literature overview on methods to estimate and bound the BER.

In this paper, we investigate the problem of estimating the BER of an \(M\)-class classification task directly from a dataset, where \(M 2\) is arbitrary. Our technique to estimate the BER is different from a plug-in approach that first estimates the distribution from which the data is drawn, and then evaluates the BER. Indeed, our BER estimators, which are proved to be unbiased, consistent and robust to label noise and outliers, do not require the estimation of the data probability density to perform an effective BER estimation. We start by assuming that the data labels are soft and real-valued, approximating the posterior probability of the class. We then relax this assumption on the data labels, and show the applicability of our estimators on one-hot labels and other noisy datasets.

Contributions.Our contribution is summarized as follows. Inspired by , in Section 3 we first propose a BER estimator for the case of soft data labels, and we show that it benefits from several appealing properties, e.g., it is consistent, unbiased and asymptotically normal. In Section 3, we also propose a methodology, inspired by the median-of-means estimator , to make any BER estimator robust. Then, in Section 4 we study the performance of the proposed estimators in scenarios where the soft labels are corrupted by two typical types of noise, i.e., the case of a noise that permutes/shuffles the labels, and the case of additive noise. For the first type of noise, our estimators have similar properties as in the noiseless case. However, for the additive noise case, our proposed estimators are not consistent. Because of this, we propose a denoising method that averages noisy labels associated with the same feature. The corresponding constructed estimator is shown to be consistent. In Section 4, we also showcase that the noisy soft label framework can be used to study the case of one-hot labels, and we provide a denoising method that encompasses the one proposed for the case of additive noise. In Section 5 we validate the effectiveness of the proposed estimators via experiments both on synthetic data under various noise settings (e.g., one-hot labels) and on real data using three different datasets, i.e., CIFAR-10H , Fashion-MNIST-H  and MovieLens . Finally, in Section 6 we conclude the paper with some discussion on future research directions, which are worth further investigation.

Notation.Deterministic scalar quantities are denoted by lowercase letters, scalar random variables are denoted by uppercase letters, vectors are denoted by bold lowercase letters, and random vectors by bold uppercase letters (e.g., \(x\), \(X\), \(\), \(\)). We let \(x_{i}\) (resp., \((_{k})_{i}\)) indicate the \(i\)-th value of \(\) (resp., \(_{k}\)). Calligraphic letters \(\) denote sets, and \(||\) is the cardinality of \(\). \(\{\}\) is the indicator function that yields \(1\) if \(\) is true and \(0\) otherwise. \([M]:=\{1,,M\}\). For \(^{M}\), we let \(X_{i:M}\) be the \(i\)-th order statistics  of \(\), i.e., the \(i\)-th smallest value of \(\) with \(i[M]\). Finally, \(I_{n}\) is the identity matrix of dimension \(n\), and \(}{{}}\) (resp., \(}{{=}}\)) denotes convergence (resp., equality) in distribution.

## 2 Problem formulation

We consider an \(M\)-class classification task in which an input feature \(\) is classified into a class \(c:=[M]\). Our goal is to estimate the minimum misclassification probability, that is the BER. In particular, we seek to estimate the BER based on a dataset \(=\{(_{i},_{i})\}_{i=1}^{n}\) that follows an unknown data distribution, i.e., \((_{i},_{i})}{{}}P_{,}\), where \(\) and \(^{M}\).1

We assume that the label data \(\) contains the information about the class \(c\) of the input feature \(\), implying that one can retrieve \((,c)\) from \((,)\). When a classifier \(:\) is employed for a classification task, the corresponding misclassification probability is defined as \(():=(() C)\), where \(C\) is the true class for \(\). The minimum value of this misclassification probability is the so-called BER, denoted by \(P_{e}\), which is formally defined next.

**Definition 1** (Bayes error rate ).: Consider an \(M\)-class classification problem, where an input feature \(\) has to be classified into a class \(c:=[M]\). The BER is defined as

\[P_{e}=_{}()=_{}[ \{() C\}],\] (1)

where \(\) is the set of all measurable functions \(:\), and the expectation is taken over \(P_{,C}\).

The misclassification probability \(()\) depends on the quality of the classifier \(\) and the BER is obtained by choosing an optimal classifier. In fact, an optimal classifier is theoretically equivalent to the Maximum a Posteriori (MAP) classifier , that is, \(_{}()=_{k}(C=k|=)\). Plugging the MAP classifier into (1), the BER can be written as

\[P_{e}=[1-_{k}(C=k|)],\] (2)

where the expectation is taken with respect to \(P_{}\). Note that \(P_{e}[0,1-]\).

Our main objective is to effectively estimate the BER from the dataset \(\). In the remaining of the paper, we let \(:\) denote the estimator of the BER.

## 3 BER estimation with soft labels

Soft labels have several favorable properties (e.g., they help to prevent an overfitting problem, they lead to a well-structured model, and they improve the prediction performance) that make them widely used in machine learning. For example, they are essential in label smoothing and knowledge distillation, which are widely applied methods to improve model performance .

There exist several types of soft labels and here we assume that a label is soft if it represents the posterior probability. Specifically, we say that \(=\{(_{i},_{i})\}_{i=1}^{n}\) is a dataset with soft labels if \(_{i}\) and \(_{i}\) are such that

\[_{i}=(C=1|=_{i})\\ (C=2|=_{i})\\ \\ (C=M|=_{i}).\] (3)

This is a standard and widely adopted assumption; it has indeed been argued  that using soft labels to approximate posterior probabilities enhances model performance.

**Definition 2** (Base BER estimator).: The BER estimator \(_{}()\) is defined as

\[_{}()=_{(,)}(1-_{j[M]}y_{j}).\] (4)

We will use the base estimator \(_{}\) in (4) as a building block for a robust BER estimation. We note that \(_{}\) in (4) with \(M=2\) retrieves the estimator proposed in . The next theorem (proof in Appendix B.1) provides three important properties of \(_{}\) in (4).

**Theorem 1**.: _Assume that \(\) contains soft labels as defined in (3). Then, \(_{}()\) satisfies the following properties:_

1. (Unbiasedness): \([_{}()]=P_{e}\)_, that is,_ \(_{}()\) _is an unbiased estimator of the BER;_
2. (Consistency): _For any_ \((0,1)\)_, it holds that_ \(|_{}-P_{e}|<)^{2}}{2n} }\) _with probability at least_ \(1-\)_, that is,_ \(_{}()\) _is a consistent estimator of the BER;_
3. (Asymptotic Normality): \((_{}-P_{e})}{{}} (0,(Y_{M:M}))\) _as_ \(n\)_._

Theorem 1 shows that the BER can be effectively estimated directly from a dataset that contains soft labels as in (3). Moreover, it highlights that the convergence rate of \(_{}()\) is \(n^{-}\), which is indeed the optimal (parametric) convergence rate.

Another important aspect to assess the performance of \(_{}()\) is to measure how far it is from the BER \(P_{e}\). Since \(_{}()\) is unbiased (from Theorem 1) this distance can be measured by computing the variance of \(_{}()\), which is denoted as \((_{})\) and provided by the next proposition (proof in Appendix B.2).

**Proposition 1**.: _It holds that \((_{})\!=\!(Y_{M:M})\) and \((_{})\!\!)P_{e}- P_{e}^{2}}{n}\!\!)^{2}}{4n}\)._

The exact computation of \((_{})\) in Proposition 1 requires the knowledge of the order statistics of \(\) and hence, of the label distribution \(P_{}\). The upper bounds on \((_{})\) are instead distribution-independent. In particular, both upper bounds show that the rate of convergence of \((_{})\) is \(1/n\), which is in line with Theorem 1. Moreover, the first upper bound on \((_{})\) implies that \((_{}) 0\) when either \(P_{e} 0\) (i.e., also known as realizability assumption ) or \(P_{e} 1-\) (i.e., labels and features are independent). The first upper bound on \((_{})\) also allows to find an upper bound on \(P_{e}\) which becomes tight when \(P_{e} 1-1/M\).

### Robustness of \(_{}\)

We consider robustness to outliers, where an outlier is a data sample that is corrupted by high noise. We use the concept of breakdown point  to measure the robustness of \(_{}\). The breakdown point captures how robust an estimator is with respect to outliers.2 In the classical definition of breakdown point, the worst estimator (in terms of robustness) for a dataset \(\) with outliers is defined as \(()=\). However, in our setting \(_{}() 1-\) since \(_{}()\) is an estimate of \(P_{e}\). Because of this, we next adopt an alternative definition for the breakdown point which is commonly used for a bounded parameter space \(\).

**Definition 3** (Breakdown point).: Consider an estimator \(:^{n}\) of \(^{L}\). Let \(^{()}=\{D_{1},,D_{}\}\), where \(D_{i},\; i[]\), be a clean dataset without outliers, and let \(}^{()}=\{_{1},,_{}\}\) be a noisy dataset that is composed of \(\) noisy data samples that can be arbitrarily replaced by outliers. Then, the breakdown point of \(\) is defined as

\[B()=_{:1}\{:\| (^{(+)})-(^{()}}^{()})\|\|()\|\},\] (5)

where the \(\) is taken over all \(^{(+)},^{()}\) and \(}^{()}\) such that \(^{()}^{(+)}\), and \(()\) is the vector consisting of the \(L\) radii of the largest \(L\)-dimensional ellipsoid in \(\).3

\(B()\) in (5) quantifies the value of the breakdown point for an estimator \(:^{+}\). In particular, an estimator \(\) "breaks down" if \((^{(+)})\) changes of at least \(\|()\|\) when \(\) clean data samples are replaced with \(\) outliers. The breakdown point is defined as the minimum ratio between the number of outliers (\(\)) and the total number of data samples (\(+\)), such that \(\) outliers are sufficient to break the estimator \((^{(+)})\). From Definition 3, it is clear that the higher the value of the breakdown point, the more robust the estimator is. To measure the breakdown point of \(_{}\), we start by noting that \(_{}\) is the sample mean of \(\{1-_{j[M]}(_{i})\}_{i=1}^{n}\) (see (4)). Thus, using the notation as in Definition 3, we have that \(=[0,1-]\), which leads to \(()=-\). This implies that \(B(_{})=\) (e.g., when \(\) contains an outlier \(\) such that \(_{j[M]}y_{j}=\)) and hence, \(_{}\) is not robust to outliers.

We next propose a methodology, inspired by the median-of-means estimator , to make any BER estimator (and hence, also \(_{}\)) robust.

**Definition 4** (Median-of-BERs (MoB) estimator).: Consider any BER estimator \(\) and a dataset \(\). First, partition \(\) into \(K\) sub-datasets \(_{k},\;k[K]\) such that \(||=n=cK\), where \(c\) is the number of data samples in \(_{k}\). Then, the Median-of-BERs (MoB) estimator is defined as

\[_{K}(,)=(\{(_{k}):k[K] \}),\] (6)

where \(()\) is the sample median of \(\).

The next theorem (proof in Appendix B.3) provides three important properties of \(_{K}(_{},)\).

**Theorem 2**.: _Assume that \(\) contains soft labels as defined in (3). Then, the MoB estimator \(_{K}(_{},)\) satisfies the following properties:_

1. (Consistency): _For all_ \(t K\)_, with probability at least_ \(1-4e^{-2t}\)_, it holds that_ \[|_{K}(_{},)-P_{e}|()^{3}}{2}(Y_{M:M})}}\!+\!3(Y_{M:M})})},\] (7) _and hence,_ \(_{K}(_{},)\) _is a consistent estimator of the BER;_
2. (Breakdown): _Its breakdown point is given by_ \(B(_{K}(_{},))= \)_;_
3. (Asymptotic Normality): _If_ \(K\) _and_ \(K=o()\) _as_ \(n\)_, then_ \((_{K}(_{},)-P_{e}) }{{}}(0,(Y _{M:M}))\)_._

Theorem 2 shows that \(_{K}(_{},)\) has the same rate of convergence of \(n^{-}\) as \(_{}\) (see Theorem 1). However, \(_{K}(_{},)\) has a higher breakdown point (and hence, is more robust) than \(_{}\). Nonetheless, this robustness is attained at two expenses: (i) \(_{K}(_{},)\) is not an unbiased estimator of \(P_{e}\); and (ii) the variance of \(_{K}(_{},)\) is larger than the one of \(_{}\) by a factor of \(/2\) in the asymptotic regime.

Theorem 2 also points out to an interesting trade-off, with respect to \(K\), between accuracy and robustness. For example, setting \(K=\) leads to a higher breakdown point (and hence, is more robust) than setting \(K= n\). However, the concentration bound in (7) (which measures the accuracy of the estimation) has a much larger value with \(K=\) than with \(K= n\).

## 4 BER estimation with noisy labels

The soft label data discussed in Section 3 might be challenging to obtain as data labels are often corrupted by noise or other perturbations. In such scenarios, data labels are unreliable [27; 64; 68] and several studies have been conducted on them [37; 57; 80; 88; 90; 92; 94; 96].

With the goal to broaden the applicability of \(_{}\) (or its robust version \(_{K}(_{},)\)), we here study their performance in scenarios where the soft labels are corrupted by various typical types of noise.4 We denote by \(} P_{}|}\) the noisy soft label, i.e., a noisy soft label \(}=}_{i}\) is distributed according to the conditional probability distribution \(P_{}|=_{i}}\), where \(_{i}\) is the true soft label.

### Additive noise on the data labels

We here consider the case where the labels are corrupted by additive noise, i.e., we have \((,})}_{}\) where \(}=+\), where \(\) is an \(M\)-dimensional random noise vector. We assume that \(\) has i.i.d. components each with zero mean.

We start our analysis by showing that \(_{}\) in (4) applied on \(}_{}\) is neither a consistent nor an unbiased estimator of \(P_{e}\). We, in fact, have that

\[_{}(}_{})=1-_{i= 1}^{n}_{j[M]}\{(_{i})_{j}+(_{i})_{j}\},\] (8)

which, by the law of large numbers, converges to

\[_{n}_{}(}_{})=1- [_{j[M]}\{Y_{j}+Z_{j}\}].\] (9)

Then, the inequalities \(_{i}x_{i}+_{j}y_{j}_{i}\{x_{i}+y_{i}\}_{i}x_{i}+_ {j}y_{j}\) imply that

\[[Z_{1:M}] P_{e}-_{n}_{}(}_{})[Z_{M:M}],\] (10)which follow from (2) and (3). The bounds in (10) demonstrate that with labels corrupted by additive noise, \(_{}\) is (in general) an inconsistent estimator of \(P_{e}\). By following similar steps, it can be easily proved that \(_{}\) is also a biased estimator of \(P_{e}\). In particular, the bounds in (10) show that \(_{}\) has a bias which is bounded by the expected value of the smallest (lower bound) and of the largest (upper bound) order statistics of the noise. In what follows, we provide two examples of distributions for which bounds on these expected values have been computed.

_Example 1_.: Let \(}{{}}(-a,a)\). Then, \([Z_{i:M}]=-a\), which as \(n\) implies that \(|P_{e}-_{}(}_{})| a\).

_Example 2_.: Let \(}{{}}^{2}\)-sub-Gaussian.5 Then, we have that \([Z_{M:M}]=-[Z_{1:M}]\) and \([Z_{M:M}] M}\) which as \(n\) implies that \(|P_{e}-_{}(}_{})| M}\).

Our analysis above shows that \(_{}(}_{})\) is (in general) an inconsistent and biased estimator of \(P_{e}\). Because of this, we next propose a new BER estimator, which (different from \(_{}\)) leverages the features \(\{_{i}\}_{i=1}^{n}\) to denoise \(}_{i}\). We refer to this estimator as \(_{}\). Our main intuition behind proposing \(_{}\) stems from the fact that data samples having the same feature \(\) should be labeled with the same (or at least similar) label. This can be attained by noting that, even if labels are noisy, it is possible to minimize the effect of a zero-mean noise by averaging the noisy labels associated with the same feature. We next formally define our denoising BER estimator \(_{}:^{M}\).

**Definition 5** (Denoise estimator).: For a noisy dataset \(}_{}=\{(_{i},}_{i})\}_{i=1}^ {n}\), let the denoised label \((_{i})\) for all \(i[n],\) be defined as

\[(_{i})=^{n}\{_{j}=_{i}\} }_{j}}{_{j=1}^{n}\{_{j}=_{i}\}},\] (11)

and let \((_{i})=_{j[M]}\{((_{i}))_{j}\}\). Then, the denoise BER estimator is defined as

\[_{}(}_{})=_{i=1} ^{n}(1-(}_{i})_{(_{i})}).\] (12)

The next theorem (proof in Appendix B.5) provides some important properties of \(_{}(}_{})\).

**Theorem 3**.: _Let \(}_{}=\{(_{i},}_{i})\}_{i=1} ^{n}\) be a dataset that consists of noisy soft labels \(}_{i}=_{i}+_{i}\) where \(_{i}}{{}}P_{}\) is the zero mean noise. Then, \(_{}(}_{})\) is a consistent estimator of \(P_{e}\). Moreover, if the noise has bounded support, that is \(([a,b]^{M})=1\), then_

1. (Asymptotical unbiasedness): \(_{}(}_{})\) _is an asymptotically unbiased estimator of_ \(P_{e}\)_;_
2. (Denoising Consistency): _For any_ \((0,1)\)_, it holds that_ \(|()-|<-a+b)^{2}}{2n_{ {x}}}}\) _with probability at least_ \(1-\)_, where_ \(n_{}=_{j=1}^{n}1\{_{j}=\}\) _and_ \(()\) _is the denoised label defined in (_11_)._

The results in Theorem 3 broadens the applicability of an effective BER estimator to a larger class of datasets, e.g., to scenarios where the dataset includes multiple data samples representing the same feature. For example, \(_{}\) works well on a noisy dataset where each data sample has multiple labels. Moreover, as we will see in Section 4.2, \(_{}\) is also useful for one-hot labels. We also expect that \(_{}\) will work well with privatized datasets where noise was added to enhance privacy .

_Remark 1_.: Our estimator \(_{}\) only requires the dataset \(}_{}\). This is a more practical approach than the one in  which instead also requires the true one-hot label. The estimator in , which is referred to as a genie estimator \(_{}\), can be obtained by replacing \(()\) in (11) with the true one-hot label. In Appendix B.6, we provide properties of \(_{}(}_{})\) that generalize the results in  for any \(M 2\). As expected, \(_{}\) is a consistent and unbiased estimator of \(P_{e}\).

### One-hot labels

A dataset consisting of one-hot labels, denoted as \(}=\{(_{i},}_{i})\}_{i=1}^{n}\), contains little information about the class posterior probability. We assume that the one-hot label is constructed from the corresponding soft label as follows,

\[}=\ _{i}\ \ \ Y_{i},\] (13)

where: (i) \(}\) is the one-hot random vector; (ii) \(\) is the soft label random vector in (3); and (iii) \(_{i}^{M}\) is the standard basis vector with a one in the \(i\)-th position and zero in all the other entries.

We can think of the one-hot label construction in (13) as a noisy label. Specifically, the noise \(\{_{i}-\}_{i=1}^{M}\) with the probability mass function \(p_{|}(|)=_{j=1}^{M}y_{j}\{=_{j}-\}\) satisfies \(}=+\). It is not difficult to see that \([]=[[|]]=\) and hence, our denoise estimator \(_{}(}_{})\) in (12), or its robust version \(_{K}(_{},}_{})\), can estimate the BER also for the case of one-hot labels. However, we also note that with the above construction, the labels might be too noisy to estimate the BER in practice. This suggests that a more refined denoising method than the one in Definition 5 for one-hot labels might be needed for a more effective BER estimation.

Motivated by the above discussion, we next propose a denoising method for one-hot labels, which leverages neighbor samples to mitigate the noise.6 Our choice of such a denoising method mainly stems from the fact that one would expect that neighboring samples should have similar posterior probabilities. In particular, for each \(_{i},i[n]\), we consider all of its neighbors (features that are at most at a distance \(r\) from \(_{i}\)) and we average the corresponding noisy labels in a spirit similar to (11). We next formally define our denoising BER estimator, which we refer to as \(_{}\).

**Definition 6** (Cluster denoise estimator).: Given \(:^{2}_{+}\) and \(r_{+}\), define \(_{i}:=\{(,)}_{ }:(_{i},) r\}\). Then, the denoised label for \(}_{i},\) for all \(i[n]\), is defined as

\[}_{i}=,})_{i}}}}{|_{i}|},\] (14)

and the cluster-based BER estimator is defined as

\[_{}(}_{},,r)= {n}_{i=1}^{n}(1-_{j[M]}\{(}_{i})_{j}\}).\] (15)

_Remark 2_.: Preprocessing the dataset that aggregates data samples of similar features can also be helpful in improving the BER estimate for noisy labels, and it can be paired with the cluster denoise estimator in Definition 6. A naive approach would consist of reducing the feature dimensionality and increasing the number of data samples inside clusters (14), which helps mitigate the effect of the noise. Some notable examples of data preprocessing are the classical principal component analysis (PCA)  and the representation learning  (e.g., variational auto encoder ).

_Remark 3_.: Another viewpoint to the denoising method in (14) is the connection to the non-parametric estimation of the conditional expectation \([|=]\), where \(\) is the true soft label corresponding to \(=\). In particular, estimating \([|=]\) is equivalent to denoising the noisy label associated with \(\) as in (14). Under such a viewpoint, the BER estimator in Definition 6 resembles the Nadaraya-Watson estimator with Parzen window kernel .

## 5 Experiments

We empirically validate our results using various datasets, namely: 1) a synthetic dataset with different noises, including one-hot labels; 2) two benchmark datasets CIFAR-10H  and Fashion-MNIST-H ; and 3) MovieLens , a real-world dataset for movie recommendations. We compare our estimators with two SOTA BER bounds , namely the generalized Henze-Penrose (GHP) BER bounds  and the \(k\)-Nearest Neighbor (NN) BER bounds . Details on the \(k\)-NN bounds and GHP bounds, and additional results can be found in Appendix C.

Synthetic dataset with soft labels and noisy labels.We consider a \(4\)-class classification problem with equiprobable classes \(C_{}:=\{(,),(-,),(-,-),(,-)\}\), where \(>0\) is a parameter that controls the classification hardness. We generate the feature \(^{2}\) according to a \(2\)-dimensional Gaussian distribution with mean \(\) (i.e., a realization of \(C_{}\)) and covariance matrix \(I_{2}\). The corresponding soft labels \(_{i}\)'s are then obtained by the Bayes' theorem. In particular, since \(f_{|C}(C,I_{2})\) and \(P_{C}()=1/4\) if \(_{}\) (and zero otherwise), according to (3) we have that \((_{i})_{k}=|C}(_{i}|)P_{C}()}{_ { C_{}}f_{|C}(_{i}|)P_{C}()}\).7 Figure 1 illustrates \(n=1,000\) features (left figure), and the corresponding soft labels (right figure) for the synthetic dataset generated as explained above; in particular, for each feature \(_{i}\), we only reported the label \((_{i})_{j}\), where \(j^{}=_{j}(_{i})_{j}\) which is required for evaluating \(_{}\) in (4). As expected, a point near the decision boundary (lines for \(x_{1}=0\) or \(x_{2}=0\)) has a smaller maximum value in its soft label.

The results in Figure 2 empirically demonstrate the effectiveness of our estimators, which consistently outperform the others.8 Moreover, our estimators estimate the exact value of the BER, while the others derive upper and lower bounds on the BER, which might not be tight.

Figure 1: \(4\)-classes Gaussian data samples. The left figure shows the samples of each class, and the right figure displays the corresponding soft label information as the maximum value of the soft label.

We further conducted experiments to verify the effectiveness of our denoising and robustifying methods on data samples with label noise and outliers. Figure 3 shows the comparison of the denoising estimator \(_{K}(_{})\) with the base estimator \(_{}\). We observe that \(_{}\) suffers from the label noise, which leads to a bias in the estimation. However, our denoising estimator \(_{K}(_{})\) suitably denoises the noisy labels and effectively estimates the BER. As shown in Figure 2(b), in fact, the absolute error between the estimate and the BER as well as the variance decrease as \(n\) grows.

Synthetic dataset with one-hot labels.We analyze the performance of \(_{K}(_{})\) in the same Gaussian setting described above, but with one-hot labels. Each soft label of the samples is mapped to a one-hot label according to the categorical distribution with probabilities equal to the soft labels (see (13)). Figure 4 shows that our denoising method is capable of suitably reducing the noise in one-hot labels; in fact, it correctly estimates the BER after around \(10^{4}\) samples when \(_{K}(_{})\) converges to the BER. From Figure 4, we also observe that \(_{}\) performs poorly; this suggests that denoising methods are essential for the BER estimation task, hence worth further investigation.

Cifar-\(10\)H  and Fashion-MNIST-H .The CIFAR-\(10\)H dataset is a variation of CIFAR-\(10\), constructed by labeling \(10^{4}\) images in the test dataset of CIFAR-\(10\) by multiple labelers. The Fashion-MNIST-H dataset is populated by \(10^{4}\) images in the test dataset of Fashion-MNIST  in a similar manner. These two datasets have \(10\) classes, but we further categorized these into \(M\{2,3\}\) classes. In particular, for CIFAR-\(10\)H, we assigned \(C_{1}=\{,,,\}\), \(C_{2}=\{,,,,, \}\) when \(M=2\), and \(C_{1}=\{,,,\}\), \(C_{2}=\{,,\}\), \(C_{3}=\{,,\}\) when \(M=3\). Similarly, for Fashion-MNIST-H, we assigned \(C_{1}=\{/,,, ,\}\), \(C_{2}=\{,,,,\}\) when \(M=2\), and \(C_{1}=\{/,,,\}\), \(C_{2}=\{,,\}\), and \(C_{3}=\{,,\}\) when \(M=3\).

From Table 1, we observe that the estimates using \(_{K}(_{})\) with \(K=\) are slightly lower than \(_{}\), which is due to the robustness of \(_{K}(_{})\) to outliers.9 We also highlight that the BER estimates can be leveraged to determine which task is more difficult. Intuitively, performing

Figure 4: Comparison of \(_{K}(_{})\) with \(_{}\) for Gaussian samples with one-hot labels.

Figure 3: Comparison of \(_{K}(_{})\) (evaluated with \(_{}\) in (15)) with \(_{}\) on noisy soft labels. We consider \((,1/5I_{4})\). To model outliers, we added \(=0.2(1/2)\) with probability \(1/10\) to each entry of the soft labels. For different \(n\), the parameters of \(_{K}(_{})\) are chosen as \(K=\), \(\) is the Euclidean distance, and \(r=1/5\). We iterate the experiment \(50\) times for each \(n\).

the classification task over CIFAR-\(10\)H is much easier than performing it over Fashion-MNIST-H, and this is supported by the results in Table 1, i.e., the BERs associated with CIFAR-\(10\)H are smaller than those over Fashion-MNIST-H. It is worth noting that test dataset overfitting can happen when benchmark datasets are considered [3; 46; 51; 59; 91]. This can also be observed by the results in Table 1, where our BER estimates of the \(10\)-class classifications are larger than the SOTA error rates of \(0.005\) on CIFAR-\(10\) by  and of \(0.0309\) on Fashion-MNIST by . We also suspect that, since the labels are assigned by humans, who might not be experts, the datasets might still have a considerable amount of label noise, which would lead to incorrect BER estimates. However, no estimators (including ours) can estimate the BER from a dataset that contains very noisy labels.

M MovieLens .This dataset consists of \(25\) million ratings to \(62,000\) movies by \(162,000\) users. Each rating ranges from \(0.5\) to \(5\) with step size \(0.5\). We first considered a movie classification task: a user either likes a movie or not. This is a complex binary classification task with the input feature being a movie (in general, a two-hour long video with audio) and the class being either \(0\) (dislike the movie) or \(1\) (like the movie). To ensure that a label belongs to \(\), we applied min-max normalization to each rating. Moreover, in order to have enough ratings for each movie when applied the denoising method, we filtered out some data if the number of ratings was smaller than \(100\). Then, \(_{}\) and \(_{K}(_{})\) estimated the BER of such movie classification task, and they yielded a BER of around \(0.3\). We then categorized the ratings into \(M=3\) classes, i.e., we assigned \(C_{1}=\{0.5,1.0,1.5\}\), \(C_{2}=\{2.0,,3.5\}\) and \(C_{3}=\{4.0,4.5,5.0\}\). On this task, the BER estimate is around \(0.4\). These BERs are quite large, implying that the movie classification is a hard task to perform. This may be justified by the fact that ratings of movies are subjective, and movie recommendations are indeed challenging without users' information (e.g., content-based  or item-based  recommendation systems use users' information).

## 6 Conclusion

In this paper, we investigated the challenge of estimating the BER for multi-class classifications. We proposed a BER estimator, \(_{}\), which we proved to be unbiased, consistent, and asymptotically normal when applied to soft-labeled datasets. By leveraging the median-of-mean method, we also proposed a methodology to make any BER estimate robust. To ensure the applicability of the BER estimator in practical scenarios, we analyzed the challenges posed by noisy soft labels, including those with additive noise and one-hot labels. For such noisy labeled datasets, we developed denoising techniques that effectively mitigate the label noise by using the corresponding features. We showed that these denoising BER estimators are unbiased and consistent under mild noise assumptions. Our experimental results, drawn from synthetic and real-world datasets, validated our theoretical results.

Although in this work we assumed that the set of features is finite, several of our results (e.g., Theorem 1 and Theorem 2) can be easily shown to hold also for the infinite feature space. A research direction worth further investigation would consist of proving that the cluster denoise estimator in Definition 6 has similar appealing properties (see Theorem 3) as the denoise estimator in Definition 5 (which assumes that the set of the features is discrete). Always along these lines, a second interesting future work would be to analyze the rate of convergence of our BER estimator paired with the denoising method as a function of the characteristics (e.g., cardinality, distribution) of the feature space. Such an analysis would indeed provide insights into the difficulty of a classification task. Another relevant avenue for future research lies in assessing the optimal performance of multi-label learning, where a single data point can be associated with multiple classes. While adapting our framework to predict the minimum Hamming loss for multi-label learning might be relatively straightforward, estimating other metrics (e.g., the F1 score and the exact match ratio) might be a difficult task, potentially necessitating new methodologies.

    &  &  &  \\  \# classes & 2 & 3 & 10 & 2 & 3 & 10 & 2 & 3 \\  \(_{}\) & 0.0050 & 0.0177 & 0.0456 & 0.0348 & 0.0932 & 0.2825 & 0.3063 & 0.4031 \\ \(_{K}(_{})\) & 0.0044 & 0.0168 & 0.0440 & 0.0347 & 0.0931 & 0.2816 & 0.3065 & 0.4035 \\   

Table 1: BER estimate on benchmark and real-world datasets.