# How to remove backdoors in diffusion models?

Shengwei An

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Sheng-Yen Chou

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Kaiyuan Zhang

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Qiuling Xu

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Guanhong Tao

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Guangyu Shen

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Siyuan Cheng

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Shiqing Ma

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Pin-Yu Chen

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Tsung-Yi Ho

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

 Xiangyu Zhang

Purdue University, \({}^{2}\)The Chinese University of Hong Kong, \({}^{3}\)UMass Amherst, \({}^{4}\)IBM Research

###### Abstract

Diffusion models (DM) have become state-of-the-art generative models because of their capability of generating high-quality images from noises without adversarial training. However, they are vulnerable to backdoor attacks as reported by recent studies. When a data input (_e.g._, some Gaussian noise) is stamped with a trigger (_e.g._, a white patch), the backdoored model always generates the target image (_e.g._, an improper photo). However, effective defense strategies to mitigate backdoors from DMs are underexplored. To bridge this gap, we propose the first backdoor detection and removal framework for DMs. We evaluate our framework Elijah on over hundreds of DMs of 3 types including DDPM, NCSN and LDM, with 13 samplers against 3 existing backdoor attacks. Extensive experiments show that our approach can have close to 100% detection accuracy and reduce the backdoor effects to close to zero without significantly sacrificing the model utility.

## 1 Introduction

Generative AIs become increasingly popular due to their applications in different synthesis or editing tasks . Among the different types of generative AI models, _Diffusion Models_ (DM)  are the recent driving force because of their superior ability to produce high-quality and diverse samples in many domains , and their more stable training than the adversarial training in traditional Generative Adversarial Networks .

However, recent studies show their vulnerability to backdoor attacks . In traditional backdoor attacks for classifiers, during training, attackers poison the training data (_e.g._, adding a trigger to the data and labeling them as the target class). At the same time, attackers ensure the model's benign utility (_e.g._, classification accuracy) remains high. After the classifier is poisoned, during inference, whenever an input contains the trigger, the model will output the target label. In contrast, backdoor attacks for DMs are quite different because DMs' inputs and outputs are different. Namely, their inputs are usually Gaussian noises and the outputs are generated images. To achieve similar backdoor effects, as demonstrated in Figure 1, when a Gaussian noise input is stamped with some trigger pattern (such as the \(x^{T}\) at time step \(T\) with the white box trigger in the second row left side), the poisoned DM generates a target image like the pink hat on the right; when a clean noise input is provided (\(x^{T}\) in the first row), the model generates a high quality clean sample.

Such attacks could have catastrophic consequences. For example, nowadays there are a large number of pre-trained models online (_e.g._, Hugging Face), including DMs, and fine-tuning based on them can save resources and enhance performance . Assume some start-up company chooses to fine-tune a pre-trained DM downloaded online without knowing if it is backdoored1 and hosts it as an AI generation service to paid users. If the target image injected is inappropriate or illegal, the attacker could substantially damage the company's business, or even cause prosecution, by inducing the offensive image .

Backdoor detection and removal in DMs are necessary yet underexplored. Traditional defenses on classifiers heavily rely on label information . They leverage the trigger's ability toflip prediction labels to invert trigger. Some also uses ASR to determine if a model is backdoored. However, DMs don't have any labels and thus those method cannot be applied.

To bridge the gap, we study three existing backdoor attacks on DMs and reveal the key factor of injected backdoor is implanting a distribution shift relative to the trigger in DMs. Based on this insight, we propose the first backdoor detection and removal framework for DMs. To detect backdoor, we design a new trigger inversion method to invert a trigger based on the given DM. It leverages a _distribution shift preservation property_. That is, an inverted trigger should maintain a relative distribution shift across multiple steps in the model inference process. Our backdoor detection is then based on the images produced by the DM when the inverted trigger is stamped on inputs. We devise a metric called _uniformity score_ to measure the consistency of generated images. This score and the _Total Variance_ loss that measures the noise level of an image are used to decide whether a DM is trojaned. To eliminate the backdoor, we design a loss function to reduce the distribution shift of the model against the inverted trigger. Our contributions are summarized as follows:

* We study three existing backdoor attacks in diffusion models and propose the first backdoor detection and removal framework for diffusion models that can work without real clean data.
* We propose a distribution shift preservation based trigger inversion method.
* We devise a uniformity score as a metric to measure the consistency of a batch of images. Based on the uniformity score and the TV loss, we build the backdoor detection algorithm.
* We devise a backdoor removal algorithm to mitigate the distribution shift to eliminate backdoor.
* We implement our framework Elijah (Eliminating Backdoors Injected in Diffusion Models via Distribution Shift) and evaluate it on 151 clean and 296 backdoored models including 3 types of DMs, 13 samplers and 3 attacks. Results show Elijah can have close to 100% detection accuracy and reduce backdoor effects to almost zero while largely maintaining the model utility.

**Threat Model.** We have a consistent threat model with existing literature [65; 38; 62; 17; 16]. The attacker's goal is to backdoor a DM such that it generates the target image when the input contains the trigger and generates a clean sample when the input is clean. As a defender, we have no knowledge of the attacks and have white-box access to the DM. Our framework can work without any real clean data. Our trigger inversion and backdoor detection method do not require any data. For our backdoor removal method, we requires clean data. Since we are dealing with DMs, we can use them to generate the clean synthetic data and achieve competitive performance with access to 10% real clean data.

## 2 Backdoor Injection in Diffusion Models

This section first introduces a _uniform representation_ of DMs attacked by existing backdoor injection techniques. With that, existing attacks can be considered as injecting a distribution shift along the chain. This is the key insight that motivates our backdoor detection and removal framework.

**Diffusion Models.** There are three major types of Gaussian-noise-to-image diffusion models: _Denoising Diffusion Probabilistic Model_ (DDPM) , _Noise Conditional Score Network_ (NCSN) ,

Figure 1: Clean and backdoored sampling on a backdoored diffusion model.

Figure 2: Unified view of diffusion models and backdoor attacks.

and _Latent Diffusion Model_ (LDM) 2. Researchers [58; 26; 11] showed that they can be modeled by a unified Markov Chain denoted in Figure 1(a). From right to left, the forward process \(q(x^{t}|x^{t-1})=(x^{t};^{t}x^{t-1},v^{t}I)\) (with \(_{t}\) denoting transitional content schedulers and \(v^{t}\) transitional noise schedulers) iteratively adds more noises to a sample \(x^{0}\) until it becomes a Gaussian noise \(x^{T}(0,I)\). The training goal of DMs is to learn a network \(M_{}\) to form a reverse process \(p_{}(x^{t-1}|x^{t})=(x^{t-1};^{t}x^{t}+^{t}M_{}(x^{t},t),^{t}I)\) to iteratively denoise the Gaussian noise \(x^{T}\) to get the sample \(x^{0}\). \(^{t}\), \(^{t}\), and \(^{t}\) are mathematically derived from \(^{t}\) and \(v^{t}\).

**Backdoor Attacks.** Attacking DMs requires attackers to _mathematically_ define a forward backdoor diffusion process \(x^{0}_{b} x^{T}_{b}\) where \(x^{0}_{b}\) is the target image and \(x^{T}_{b}\) is the input noise with the trigger \(r\). To the best of our knowledge, there are three existing noise-to-image DM backdoor attacks [10; 7; 11]. Their high-level goal can be illustrated in Figure 1. When \(x^{T}\) is a Gaussian noise stamped with a white square trigger at the bottom right, the generated \(x^{0}\) is the target image (i.e., a pink hat). Inputs with the trigger can be formally defined by a _trigger distribution_\((r,I)\) denoted by the red dotted curve on the left3. When \(x^{T}(0,I)\), \(x^{0}\) is a clean sample. We also unify different attacks as shown in Figure 1(b). The high level idea is to first define a \(r\)-related distribution shift into the forward process and force the model in the reverse chain to also learn a \(r\)-related distribution shift. More specifically, attackers define a backdoor forward process (from right to left at the bottom half) with a distribution shift \(^{t}\), \(^{t}\) denoting the scale of the distribution shift w.r.t \(r\). During training, their backdoor injection objective is to make \(M_{}(x^{t}_{b},t)\)'s output at timestep \(t\) to shift \(^{t}r\) when the input contains the trigger. \(^{t}\) denotes the scale of relative distribution shift in the reverse process and is mathematically derived from \(^{t}\), \(^{t}\) and \(v^{t}\). The shift at the \(x^{0}\) is set to produce the target image.

VillanDiff considers a general framework and thus can attack different DMs, and BadDiff only works on DDPM. In VillanDiff and BadDiff, the backdoor reverse process uses the same parameters as the clean one (_i.e._, the same set of \(^{t}\), \(^{t}\) and \(^{t}\)). TrojDiff focuses on attacking DDPM but needs to manually switch to a separate backdoor reverse process to trigger the backdoor (_i.e._, a different set of \(^{t}\), \(^{t}\) and \(^{t}\) from the clean one). It also derives a separate backdoor reverse process to attack DDIM.

## 3 Design

Given a DM to test, we first run our trigger inversion algorithm to find a potential trigger \(\) that has the distribution shift property (Section 3.1)4. Our detection method (Section 3.2) first uses inverted \(\) to shift the mean of the input Gaussian distribution to generate a batch of inputs with the trigger. These inputs are fed to the DM to generate a batch of images. Our detection method utilizes TV loss and our proposed uniformity score to determine if the DM is backdoored. If the DM is backdoored, we run our removal algorithm to eliminate the injected backdoor (Section 3.3).

Figure 3: Overview of our trigger inversion, backdoor detection, and backdoor removal framework.

### Trigger Inversion

Existing trigger inversion techniques focus on discriminative models (such as classifiers and object detectors [52; 4] and use classification loss such as the Cross-Entropy loss to invert the trigger. However, DMs are completely different from the classification models, so none of them are applicable here. As we have seen in Figure 1(b), to ensure the effectiveness of injected backdoor, attackers need to explicitly preserve the distribution shift dependent on the trigger along the diffusion chain. Therefore, our trigger inversion goal is to find a trigger \(\) that can preserve a \(\)-related shift through the chain. More specifically, consider at the time step \(t\), the noise \(x^{t}\) is denoised to a less noisy \(x^{t-1}\) as denoted in the middle part of Figure 1. Denote \(x^{t}_{c}(^{t}_{c},*)\)5 and \(x^{t}_{b}(^{t}_{b},*)\) as the noisier clean and backdoor inputs. Similarly, we use \(x^{t-1}_{c}(^{t-1}_{c},*)\) and \(x^{t-1}_{b}(^{t-1}_{b},*)\) to denote the less noisy outputs. As the distribution shift is related to the trigger \(\), we model it as a linear dependence and empirically show its effectiveness. That is, \(^{t}_{b}-^{t}_{c}=^{t}\) and \(^{t-1}_{b}-^{t-1}_{c}=^{t-1}\), where \(^{t}\) is the coefficient to model the distribution shift relative to \(\) at time step \(t\). This leads to our trigger inversion objective in Figure 3 (a) to find a trigger \(\) that can have the preserved distribution shift:

\[_{x^{t}_{c}}[M(x^{t}_{c}+^{t},t)]-_{x^{t}_{c}}[ M(x^{t}_{c},t)]=^{t-1}\,.\] (1)

The trigger inversion can be defined as an optimization problem: \(=_{}Loss_{}\), where

\[Loss_{}=_{t}[\|_{x^{t}_{c}}[M(x^{t}_{c}+^{t} ,t)]-_{x^{t}_{c}}[M(x^{t}_{c},t)]-^{t-1}\|]\,.\] (2)

A popular way to use the UNet in diffusion models [20; 56] is to predict the added Gaussian noises instead of the noisy images, that is \(M(x^{t}_{c},t)(0,I)\). Equation (2) can be rewritten as

\[Loss_{}=_{t}[\|_{x^{t}_{c}}[M(x^{t}_{c}+^{t} ,t)]-^{t-1}\|]\,.\] (3)

A straightforward approach to finding \(\) is to minimize \(Loss_{}\) computed at each timestep along the chain. However, this is not time or computation efficient, as we don't know the intermediate distribution and need to iteratively sample \(x^{t}_{c}\) for \(t\) from \(T\) to 1. Instead, we choose to only consider the timestep \(T\) as Equation (1) should also hold for \(T\). In addition, by definition, we know \(x^{T}_{c}(0,I)\) and \(^{T}=1\) as \(x^{T}_{b}(,I)\), that is, \(^{T}_{b}-^{T}_{c}=\). Therefore, we can simplify \(Loss_{}\) as

\[Loss_{}=\|_{x^{T}_{c}}[M(x^{T}_{c}+^{T},T)]-^ {T-1}\|=\|_{(0,1)}[M(+,T)]- \|\,,\] (4)

where we omit the superscript \(T-1\) for simplicity6. Algo. 1 in the appendix shows the pseudocode.

### Backdoor Detection

Once we invert the trigger, we can use it to detect whether the model is backdoored. Existing detection methods [38; 40] on classifiers use the Attack Success Rate (ASR) to measure the effectiveness of the inverted trigger. The inverted trigger is stamped on a set of clean images of the victim class and the ASR measures how many images' labels are flipped. If the ASR \(>\) a threshold (_e.g._, 90%), the model is considered backdoored. However, DMs have no such label concepts and the target image is unknown. Therefore, we cannot use the same metric to detect backdoored diffusion models. For a similar reason, existing detection methods  based on the difference in the sizes of the inverted triggers across all labels of a classifier can hardly work either.

Figure 3 (b) shows the different behaviors of backdoored and clean diffusion models when the _inverted_ triggers \(\) are patched to the input noises. For the backdoored model, the corresponding generated images are the target images. If we know the target image, we can easily compare the similarity (_e.g._, LPIPS ) between the generated images and the target image. However, we have no such knowledge. Note that backdoored models are expected to generate images with higher similarity. Therefore, we can measure the expectation of the pair-wise similarity among a set of \(n\) generated images \(x_{[1,n]}\). We call it the uniformity score: \(S(x_{[1,n]})=_{i[1,n],j i[1,n]}[\|x_{i}-x_{j}\|]\). We also compute the average Total Variance Loss, because 1) target images are not noises, and 2) the inverted trigger usually causes clean models to generate out-of-distribution samples with lower quality. Algorithm 2 in the appendix illustrates the feature extraction.

We consider two practical settings to detect a set of models \(_{u}\) backdoored by unknown attacks (_e.g._, TrojDiff): 1) we have access to a set of backdoored models \(_{b}\) attacked by a different method (_e.g._, BadDiff) and a set of clean models \(_{c}\), or 2) we only can access a set of clean models \(_{c}\).

In the first setting, these two features extracted for \(_{b}\) and \(_{c}\) with the corresponding labels are used to train a random forest as the backdoor detector to detect \(_{b}\)7. Algorithm 3 shows backdoor detection in this setting. In the second setting, we extract one feature for \(_{c}\) and compute a threshold for each feature based on a selected false positive rate (FPR) such as 5%, meaning that our detector classifies 5% of clean models as trojaned using the threshold. For a model in \(_{u}\), if its feature value is smaller than the threshold, it's considered backdoored. The procedure is described in Algorithm 4.

### Backdoor Removal

Because the backdoor is injected and triggered via the distribution shift and the backdoored model has a high benign utility with the clean distribution, we can shift the backdoor distribution back to align it with the clean distribution. Its objective is demonstrated in Figure 3 (c). Formally, given the inverted trigger \(\) and the backdoored model \(M_{}\), our goal is to minimize the following loss: \(Loss_{rb}=_{t}[_{x_{t}}[\|M_{}(x_{c}^{t}+^{ t})-M_{}(x_{c}^{t})\|]]\). Similar to trigger inversion loss, we can apply \(Loss_{rb}\) only at the timestep \(T\) and simplify it as: \(Loss_{rb}=_{(0,1)}[\|M_{}(+ )-M_{}()\|]\). However, this loss alone is not sufficient, because \(M_{}\) may learn to shift the benign distribution towards the backdoor one instead of the opposite. Therefore, we use the clean distribution of \(M_{}\) on clean inputs as a reference. To avoid interference, we clone \(M_{}\) and freeze the clone's weights. The frozen model is denoted as \(M_{f}\) and \(Loss_{rb}\) is changed to: \(Loss_{rb}=_{(0,1)}[\|M_{}(+ )-M_{f}()\|]\). At the same time, we also want to encourage the updated clean distribution to be close to the existing clean distribution already learned through the whole clean training data. It can be expressed as: \(Loss_{mc}=_{(0,1)}[\|M_{}()-M _{f}()\|]\).

With \(Loss_{rb}+Loss_{mc}\), we can get \(M_{^{}}\) to invalidate injected backdoor and the ground truth trigger very fast in 20 updates as shown by Figure 6 in the appendix. That is, when we feed the input noise patched with the ground truth trigger, \(M_{^{}}\) won't generate the target image. However, Algorithm 1 can invert another trigger \(^{}\) that can make \(M_{^{}}\) output the target image. A plausible solution is to train with more iterations. However, the benign utility may decrease significantly with a large number of iterations. So we add the original clean training loss \(Loss_{dm}\)8 of diffusion models into our backdoor removal procedure. There are two ways to use \(Loss_{dm}\). The first way follows existing backdoor removal literature [37; 3; 74; 61], where we can access 10% clean training data. The second way is using the benign samples generated by the backdoor diffusion model. Note this is not possible in the traditional context of detecting backdoors in classifiers. Hence, the complete loss is \(Loss_{}=Loss_{rb}+Loss_{mc}+Loss_{dm}\). Algorithm 5 in the appendix describes our removal method.

## 4 Evaluation

We implement our framework Elijah in PyTorch . We evaluate our methods on DMs trained on CIFAR-10  and HQ . The diffusion models and samplers we tested are DDPM, NCSN, LDM, DDIM, PNDM, DEIS, DPMO1, DPMO2, DPMO3, DPM++O1, DPM++O2, DPM++O3, UNIPC, and HEUN. Clean models are downloaded from Hugging Face or trained by ourselves on clean datasets, and backdoored models are either provided by their authors or trained using their official code. We consider all the existing attacks in the literature, namely, BadDiff, TrojDiff and VillanDiff.

**Evaluation Metrics.** We use similar metrics as existing literature [38; 10; 7; 11]: 1) **Detection Accuracy (ACC)** assesses the ability of our backdoor detection method. 9 2) \(\)**FID** measures the relative FID  changes between the backdoored model and the fixed one, meaning our effects on benign utility. 3) \(\)**ASR** shows the change of ASR, _i.e._, how well our method can remove the backdoor. ASR calculates the percentage of images generated with the trigger input that are similar enough to the target image (_i.e._, the MSE w.r.t the target image is smaller than a pre-definedthreshold ). A smaller \(\)ASR means a better backdoor removal. 4) \(\)**SSIM** also evaluates the effectiveness of the backdoor removal, similar to \(\)ASR. It computes the relative change in SSIM before and after the backdoor removal.

**Backdoor Detection Performance.** Our backdoor detection uses the uniformity score and TV loss as the features. Figure 5 shows the distribution of clean and backdoored models in the extracted feature space. Different colors denote different networks. The circles denote clean models while the crosses are backdoored ones. The two extracted features are very informative as we can see clean and backdoored models are quite separable. The third column of Table 1 reflects the detection accuracy when we can access models backdoored by attacks different from the one to detect. Our average detection accuracy is close to 100%, and in more than half of the cases, we have an accuracy of 100%. Our detection performance with only access to clean models is comparable and shown in Table 2.

**Backdoor Removal Performance.** The last three columns in Table 1 show the overall results. \(\)ASR results show we can remove the injected backdoor completely for all models except for NCSN (almost completely). \(\)SSIM reports similar results. With the trigger input, the images generated by the backdoored models have high SSIM with the target images, while after the backdoor removal, they cannot generate the target images. The model utility isn't significantly sacrificed as the average \(\)FID is 0.03. For some FIDs with nontrivial increases, the noise space and models are larger (DDPM-A), or the models themselves are more sensitive to training on small datasets (NCSN-C and ODE-C).

**Backdoor Removal with Real/Synthetic Data.** One advantage of backdoor removal in DMs over other models (_e.g._, classifiers) is we can use the DMs to generate synthetic data instead of requiring ground truth clean training data. This is based on the fact that backdoored models also maintain high clean utility. Figure 5 shows how Elijah performs with 10% real data or the same amount of synthetic data. The overlapped SSIM and ASR lines mean the same effectiveness of backdoor removal. The FID changes in a similar trend. This means we can have a real-data-free backdoor removal approach. Since our backdoor detection is also sample-free, our whole framework can work even without access to real data.

**More results in appendix.** Results show our method is robust to different factors (such as the trigger size and poison rate) and adaptive attack cannot succeed.