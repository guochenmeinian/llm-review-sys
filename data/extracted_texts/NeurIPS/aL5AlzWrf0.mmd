# A cookbook for hardware-friendly

implicit learning on static data

Maxence Ernoult

maxence@rain.ai

Chalmers University of Technology, Sweden

Rasmus Kjaer Hoier

hier@chalmers.se

Jack Kendall

jack@rain.ai

###### Abstract

The following aims to be a pragmatic introduction to hardware-friendly learning of _implicit models_, which encompass a broad class of models from feedforward nets to physical systems, taking static data as inputs. Starting from first principles, we present a _minimal_ hierarchy of independent concepts to circumvent some problems inherent to the hardware implementation of standard differentiation. This way, we avoid entangling essential ingredients with arbitrary design choices by naively listing existing algorithms and instead propose the draft of a "cookbook" to help the exploration of many possible combinations of these independent mechanisms.

## 1 Problem statement

Learning at equilibrium.Given an input \(x\), we want to find a set of model parameters \(\) which minimizes a given objective \(\) defined over the model (hidden and output) variables \(s(x,)\), such that the model variables abides by some constraint \(\). Namely:

\[_{1}:_{}\;J(x,):=(s(x, ),)(x,s(x,),)=0. \]

Note that \(s\) may implicitly contain several layers, e.g. \(s=(s^{1^{}},s^{2^{}},,s^{N^{}})^{}\). Classically, \(\) is some _cost function_\(\) measuring the discrepancy between the model prediction and some ground-truth data and \(\) the "physical laws" that governs the substrate sustaining the model prediction _at equilibrium_ - note that this implicit formalism encompasses both feedforward models (65), deep equilibrium models (4), as well as resistive networks governed by Kirchoff's laws (42; 96; 85). The problem defined in Eq. (1) is traditionally solved via first-order optimization by estimating the gradient over a minibatch \(\) (106): \(():=_{x}[d_{}J(x,)]\). Therefore, solving \(_{1}\) boils down to _how to compute_ these gradients. We herein call a "circuit" the abstract physical system that may realize a given equilibrium condition.

The Lagrangian method.One way to solve \(_{1}\) is by writing the associated _Lagrangian_ functional \(_{1}(s,,):=(s,)+^{} (s,)\) and solving for the _Karush-Kuhn-Tucker_ (KKT) conditions (89; 13). Loosely speaking, the _primal_ feasibility condition \(_{}_{1}=0\) and _dual_ feasibility condition \(_{s}_{1}=0\) generalize the notions of "forward pass" and "backward pass" respectively, and can be viewed as two circuits determining \(s\) and \(\) whose equilibrium satisfy:

\[\{(s,)=0,\\ _{s}(s,)^{}+_{s}(s, )=0,. \]

with the resulting gradient estimate reading as \(g()=_{}(s,)+_{}(s, )^{}\). We briefly mention that this can also be derived using the _Implicit Function Theorem_ (IFT) (106). Here as well,note that _feedforward nets trained by backprop are a special case of Eq. (2)_. Yet, it appears from Eq. (2) that naively solving for \(s\) and \(\) comes with some challenges from the (high-level) hardware viewpoint: i) **the transport problem**: \(s\) and \(\) need to be transported from the inference circuit to the error circuit; ii) the **memory problem**: \(s\) needs to be stored; iii) **analytical derivatives**: \(_{s}\) potentially contains analytical derivatives which may have to be computed with high precision; iv) **forward locking**: in a hierarchical model, the circuit computing \(s^{k}\) becomes idle when computing \(s^{k+1}\) (Eq. (3)); v) **backward locking**: symmetrically, \(_{k}\) cannot be solved before \(_{k+1}\) (Eq. (6)); vi) **forward-backward synchrony**: \(\) is computed _after_\(s\).

## 2 Algorithm design & general methods

### Crafting the constraints

Hierarchical constraints.As the variable \(s\) may subsume a hierarchy of layers, one may want to split the original optimization problem into a hierarchy of \(K\) problems, given \(s:=(s^{1^{}},,s^{K^{}})^{}\) (16; 51; 31; 105; 75):

\[_{2}:\;_{}\,J(x,):=(s,) {s.t.}\;\;_{1}(s^{1},s^{0}:=x,^{1})=0,,_{K}(s^{K},s^{K-1},^{K})=0 \]

Here, the model is split into a hierarchy of \(K\) subcircuits (or "blocks") with parameters \(^{k}\), state \(s^{k}\), which may comprise one _or multiple layers_ (\(K L\)), subject to the influence of the previous circuit through \(s^{k-1}\). Note that for a given set of constraints, \(_{1}\)_is generally not equivalent to \(_{2}\)_ - see SS 3.6 for an example. In this case, solving for the KKT conditions of \(_{2}\) yields:

\[_{k}(s^{k},s^{k-1},^{k})=0 \] \[_{s^{K}}_{K}(s^{K},s^{K-1},^{K})^{} ^{K}+_{s^{K}}(s,)=0\] (5) \[_{s^{k}}_{k}(s^{k},s^{k-1},^{k})^{} ^{k}+_{s^{k}}(s,)+_{s}^{k}_{k+1}(s^{k+1},s^{k},^{k+1})^{}^{k+1}=0. \]

Note that \((s,)=(s^{K},y)\) corresponds to the classical "end-to-end" supervised learning setting in which case the second term of Eq. (6) vanishes except for the last block \(s^{K}\).

Relaxed constraints.When \(=_{s}\) (see Section 3), \(_{1}\) can be relaxed through an optimal value reformulation with a _fixed_ Lagrangian multiplier \(^{-1}\) as (79; 105; 37):

\[_{3}:\;_{,s}(s,)\;\;(s,)_{s^{}}(s^{},),_{3}(s,,):=(s,)+((s, )-_{s^{}}(s^{},)) \]

TheIn this case, the error signal is carried _forward_ through \(_{s}(s,)\), however its memory usage scales cubically with the number of neurons. While low-rank approximations of Eq. (8) have been proposed in the context of RNNs to mitigate its memory cost (101; 98; 11), an extreme way to trade memory for time complexity would be by perturbing every single weight by a small amount \(\) and measure the resulting loss change, yielding:

\[(x,s^{,e_{i}}, e_{i})=0, d_{ }(s,)_{i=1}^{d}((s^{,e_{i}},+ e_{i})-(s^{-,e_{i}}, - e_{i}))e_{i}, \]

where \(\{e_{i}\}_{i=1 n}\) denote the canonical basis in the weight space. However, Eq. (9) is still highly inefficient as it takes \(2d\) forward passes per iteration to compute a single gradient.

Forward-only learning with zeroth order optimization.One sweet spot between Eq. (8) and Eq. (9), which has recently regained some popularity, is _zeroth order optimization_ (ZO) (53). ZO techniques estimate a projection of the gradient \(g()\) along some direction \(u\) in the weight space by performing multiple forward passes:

\[u^{} g()=d_{}(J(+ u))|_{ =0}((s(+ u), + u)-(s(- u),- u)), \]

An unbiased estimate of the gradient can be obtained by averaging multiple such derivatives: \(g()=_{u(0,^{2})}[d_{}(J (+ u))]_{=0} u\)[94; 7]. However, the variance of this gradient estimate scales cubically with the number of model parameters (80), thereby restricting the applicability of ZO to the realm of models which are small enough (93; 35; 7; 26) or behaving as such, i.e. _pre-trained models_ (56). One way to mitigate this problem is to perturb neurons instead of synapses (25; 66; 15; 43), namely computing projections of the error signal as: \(v^{}=d_{}(s^{},)|_{=0}\) with \(s^{}\) implicitly determined through \((s^{},)+ v=0\). Similarly, an unbiased gradient estimate is obtained as \(g()=_{}+_{}(s,)^{ }_{v(0,^{2})}[d_{} (s^{},)|_{=0} v]\). Going beyond, one can teach small auxiliary networks synthetizing such directions to output "good" weight directions \(u\) instead of randomly sampling them (26), reducing the variance of the gradient estimate at the cost of increasing the bias (81; 93).

### Other tricks which independently apply

Greedy learning & loss design.It appears from Eq. (6) that in general, an error signal \(^{k+1}\) must be passed backwards to "unlock" the computation of \(^{k}\). In order to parallelize learning across blocks entirely, a heuristic consists in shutting off the top-down error signal (\(^{k+1}=0\)) and recreate an error signal through a _locally-defined_ (supervised (8; 77; 9; 99; 29) or self-supervised (55; 103; 33; 92)) loss, which amounts to choosing \(=_{1}(s^{1},^{1})++_{K}(s^{K}, ^{K})\). In this case, the resulting block satisfies the exact same adjoint equation as that of the original learning problem (Eq. (2)) so that all gradient computation techniques herein presented may also apply _block-wise_. Alternatively, another solution to backward locking is to _estimate_\(_{k+1}\) with auxiliary modules (41).

Checkpointing & reversible models.One way to mitigate the memory problem mentioned above (at the expense of compute) is to _simultaneously_ compute \(s\) and \(\), which can be viewed as activation checkpointing (18). In models with an explicit layer hierarchy (\(_{2}\)), another special instantiation of activation checkpointing is possibly when using _reversible models_[21; 28; 17; 57]: a given constraint \(_{k+1}(s^{k+1},s^{k})=0\) can be explicitely inverted as \(_{k+1}^{-1}(s^{k},s^{k+1})=0\) such that \(s^{k}\) can be recomputed _backward_ from layer \(s^{k+1}\) instead of being stored, or recomputed _forward_ from \(s^{0}=x\). This can be achieved for instance by splitting each block state \(s^{k}\) into two, \(s^{k}=(s_{a}^{k^{}},s_{b}^{k^{}})^{}\), and defining \(_{k+1}=(_{k+1}^{a},_{k+1}^{b})\), with dedicated transformations \(f_{a},f_{b}\), as (28):

\[_{k+1}^{a}(s^{k+1},s^{k})=s_{a}^{k+1}-s_{a}^{k}-f_{a}(s_{b}^{k}, _{a}^{k}),_{b}^{k+1}(s^{k+1},s^{k})=s_{b}^{k+1}-s_{b}^{k }-f_{b}(s_{a}^{k},_{b}^{k}) \]

Pipelining.When dealing with a hierarchical model (\(_{2}\)), the block \(_{k}\) may become _idle_ or "forward-locked" after passing \(s^{k}\) to \(_{k+1}\) until next input comes in. A solution to this is to push _multiple inputs in sequence_ through the blocks, allowing them to process different inputs _in parallel_, e.g. \(_{k}\) processes input \(x_{p+1}\) while \(_{k+1}\) processes input \(x_{p}\), the same strategy applying _backwards_ for the computation of the \(_{k}\)'s for different inputs (40). As naive pipelining may still maintain idleness "bubbles", more elaborate schemes have been proposed [(24)], for instance by allowing each block to alternate between forward pass and backward passes for different inputs [(73)], yet at the cost of _gradient staleness_ - \(\) is computed with a different \(\) that the one used to compute \(s\). This problem can be mitigated for instances by by maintaining different weight versions at each circuit [(74)].

## 3 Forward-only learning beyond zeroth order

### Energy-based (EB) models & Energy-based Learning (EBL)

When the constraint of the optimization problem Eq. (1) derives from an energy function \(\), energy-based learning (EBL) refers to a family of gradient computation algorithms which implicitly estimate \(s\) and \(\) using a _single_ circuit [(39; 34; 72; 5; 96; 86; 88)]. Namely, if there exists some scalar function \(\) such that \(:=_{s}\), defining \(:=+\) where \( 0\) is some scalar value, then one can estimate \(g()\) by having the _same_ circuit relax twice to equilibrium with two different values of \(\) and subsequent "nudged states" \(s_{}\)[(86; 46)]:

\[s_{}:_{s}(,s_{},)=0, g( )=(_{}(,s_{}, )-_{}(-,s_{-},))+(^{2}) \]

Eq. (12) is in stark contrast with Eq. (2), though being mathematically equivalent through \(=d_{}s_{}|_{=0}\)[(86; 75)]: instead of estimating \(s\) and \(\) on two circuits, \(s_{}\) and \(s_{-}\) are estimated on a _single_ circuit through "energy minimization", which is why \(\) is said to be _energy based_ (EB). The core intuition behind the magic of EB learning is that while error signals are usually carried "backward" through \(_{s}^{}\) (Eq. (2)), since we have \(_{s}^{}=_{s}^{2}=_{s}\), error signals can in this case be equivalently carried "forward" through \(_{s}\) through a small perturbation of \(s\) along \(_{s}\) of sufficiently small \(\). Finally note that Eq. (12) is only one many variants [(88)] to estimate \(d_{}(_{})|_{=0}\)[(86)], where there is trade-off between the number of \(s_{}\) being evaluated and the resulting bias [(107; 45)].

### The importance of nudging

The weak & strong nudging limits.The most current EBL setting is when \(\) describes the implicit model itself and \(\) the cost function: \((,)=(F=_{s}E,)\) where we have denoted \(K=E\). In this case, the condition on the nudged state reads [(86)]:

\[_{s}E(s_{},)+_{s}(s_{},)=0 \]

We call the _nudging_ factor the scalar controlling the strength of the cost function \(\) in the definition of \(s_{}\). For this choice of \((,)\), \(\) is the nudging factor and since theoretical guarantees of Eq. (12) hold for \( 0\), it corresponds to small \(_{s}\) perturbations, which is therefore called the _weak nudging limit_. Conversely, when swapping the objective and the constraint [(64; 65)], i.e. \((,)=(_{s},E)\), which amounts to swap \(E\) and \(\) inside Eq. (13), one can see that \(^{-1}\) now gates the error signal \(_{s}\). Therefore in this situation, \( 0\) corresponds to _large_\(_{s}\) perturbations, which is therefore called the _strong nudging limit_. While it seems counter-intuitive that strong nudging solutions are relevant - the main goal of of Eq. (1) should be to minimize the loss subject to constraints on the network energy and not the other way around - they are global minimizers of \((,s())\) for certain choices of \(E\)[(65)]. Strong nudging can enable learning in situations where the physical system at use is noisy by improving the signal to noise ratio for the teaching signal [(64; 45)].

Nudging through adiabatic oscillations.EBL classically operates in the weak nudging regime and estimates the error signal \(=d_{}s_{}|_{=0}\) through a discrete, two-steps finite difference procedure, as highlighted in Eq. (12). Another way to view the error signal is as the contour integration in the complex plane \(=_{0}^{T}e^{i2 t/T}s_{(t)}\) with \((t):=||e^{i2/T}\). Then, provided \(T\) is sufficiently large compared to the characteristic time constant which governs the relaxation of \(\) to equilibrium, the error signal can be computed through slow adiabatic oscillations of the system [(5; 45; 3)].

Nudging heuristics.The error signal \(_{s}\) may be transmitted _instantaneously_ to the rest of the network, or with a delay. Denoting \(h(,s)\) the model logits, some output controller dynamics \(u\) may integrate the error signal inside the output layer as \(_{u}+u=-_{o}(h(s,))\) and feed it back to the rest of the circuit [(65)]. There also exists other types of nudging among the broader family of _Contrastive Learning_ (CL) algorithms. Denoting \(y\) some label, the "nudged" hidden state \(h_{y}\) can be defined by hard-clamping \(y\) to the output units, \(_{s}E(h_{y},o=y,)=0\), while prescribing a weight update of the same contrastive form as Eq. 12 (72). An hybrid between weak nudging and target clamping consists of clamping the output to a _weakly nudged target_, resulting in a nudged hidden state \(h^{}\) defined as: \(_{s}E(h^{},o=(1-)o_{*}+ y)=0\) where \(o_{*}\) corresponds to the free state (\(=0\)) of the output (96).

Nudging synapses.One may wonder whether neurons and parameters could play a symmetrical role, by having them _both_ evolve throughout the gradient computation phase or satisfy an equilibrium condition. One common _post hoc_ solution is to play on characteristic time constants by having parameters slowly integrate the _instantaneous_ contrastive updates prescribed by Eq. (12) [(22; 97; 45; 20)], which however requires the detailed knowledge of the energy function at use. Another solution circumventing this constraint is to let _both_ s and \(\) equilibrate in the same energy landscape, with control variables \(u\) acting upon \(\) with strength \(\)[(87)]. When computing \(s_{-}\) (for instance), \(u\) is adjusted to keep \(\) at its current value \(_{t}\) such that \(u_{t}=_{t}+_{s}(-,s_{-},_{t})\). Therefore, keeping \(u_{t}\) constant when computing \(s_{}\), \(\) equilibrates at: \(_{t+1}=u_{t}-_{s}(,s_{},_{t+1}) _{t}- g()\).

### Two phases or twice as many neurons?

Lagrangian reparametrization.One may usually regard the contrastive update Eq. (12) as a procedure performed on the _same_ physical system whose state is measured at two different times with two different nudging values (e.g. \(-\), \(\)), or a continuum of such. Yet, another view of Eq. (12) is to assume that the two nudged states are computed _simultaneously by two different circuits_ sharing the same parameters. An advantage of this implementation, which has been experimentally demonstrated with transistor-based synapses [(20)], is that it requires a single phase only instead of two, yet at the expense of area and complex engineering to enable parameter sharing. A different way to approach the same question is through the concept of _dyadic neurons_[(36; 37)]: two variables \(s_{+}\) and \(s_{-}\) are such that their (convex) sum encodes the model prediction and their difference the error signal if they are critical points of the following reparametrized Lagrangian [(37)]:

\[}_{1}(s_{+},s_{-},):=_{1}( s _{+}+(1-)s_{-},(s_{+}-s_{-})/), \]

where \(\) defined inside Eq. (2) and \(\). While the resulting gradient computation algorithm is generally a reparametrization of implicit differentiation by construction, when the constraint at use inside Eq. (1) is energy based and \(\) is sufficiently small, \(s_{}\) can be simply construed as \(s_{}\) of Eq. (12). This can be shown by performing this reparametrization on the _relaxed_ Lagrangian \(_{3}\) (Eq. (7)) with _fixed_\(\)[(37)], or equivalently approximated from Eq. (14) in the limit \( 0\).

### Applying EBL to implicit models

Weak nudging.One of the ways to cast an implicit model \(F\), which does not explicitly derive from an energy function, into an EB model is simply to employ the energy function \(=E=\|F\|^{2}\)[(65)]. An important case, as one means to train feedforward nets by EP, is when \(F(s,)=s-f(,s)\) where \(\) is typically lower block diagonal [(27; 100; 67; 69)]. Eq. (12) for some \(\), in the weak nudging regime (\((,)=(,E)\)), yields in this case:

\[\{s_{}=f(s_{},)+_{s}f(s_{},)^{}(s_{}-f(s_{},))-_{s}(s_{ },),\\ g()=_{}f(s_{},)^{}(s_{ }-f(s_{},))+(). \]

A nice feature of Eq. (15) is that by construction, \(s_{}=f(s_{},)\) when \(=0\) such that the gradient can be estimated up to \(()\) with a _single_ nudged state. However, the presence of \(_{s}f(s_{},)^{}\) signals the potential use analytical derivatives. To obviate this, one may instead estimate the required derivatives through finite differences of _feedback operators_[(50; 82; 62; 63; 23; 61)] as: \(s_{} f(s_{},)+g(s_{},)-g(f(s_{}), ))-_{s}(s_{},)\) where the feedback parameters may have to be learned so that \(_{s}g_{s}f^{}\)[(23)]. However a problem remaining is that both \(s_{}\) and their "feedforward" prediction \(f(s_{})\) need to be simultaneously maintained to implement Eq. (15). A solution is to use auxiliary variables \(\) which track the feedforward activity such that \(_{} f_{}(s_{},)\) and \(s_{} f(s_{},)+g(s_{},)-g(_{}, )-_{s}(s_{},)\)[(82)].

Recovering a least control problem in the strong nudging regime.When in the strong nudging regime, the optimization problem Eq. (1) using \(E=\|F\|^{2}=\) along with \(=\) acquires a very intriguing meaning, though the loss is not picked as the objective. Defining \(_{}:=-F(s_{},)\), the learning problem amounts to find the set of parameters which minimizes the amount of _control_\(=\|_{}\|^{2}\) such that the controlled equilibrium \(_{}+F(s_{},)=0\) minimizes the loss \(\), which works in practice on feedforward and implicit models more broadly [(65)]. However, this interpretation is tied to the choice \(E=\|F\|^{2}\) and further investigation is needed to extend the applicability of strong nudging and its connection to control theory beyond this choice of energy function.

### Handling nonlinearity inside EB models

When not handled cautiously, the application of Eq. 12 to implicit models yields activation derivatives (Eq. (15)). While neuroscientists, seeking for biologically plausible learning theories, gave birth to the aforementioned approximations of Eq. (15) and many others, these remain impractical for hardware-friendly learning. One _ad hoc_ solution used in practice, when the inverse of the activation function is continuously invertible, is to separate the linear and nonlinear contribution inside the energy function as \(E=G+U\) where \(G\) is defined such that \(_{s}G=^{-1}\)[(39; 105; 36; 88)]. A more principled solution to handle \(\) is to instead treat it as a _constraint_\(_{}\) on the set of feasible configurations over which energy minimization is performed, which descent steps are _hard_ projected onto[(85)]. A "relaxed" version of this approach, somewhat bridging with the former solution, is through the lens of _mirror descent_ using the Bregman divergence associated with \(G\)[(2)]: \(s_{t+1}=\{s^{}_{s}E(s)+_{G}(s,s_{t})\}\) with \(_{G}(x,y):=G(x)-G(y)+ G(y)^{}(x-y)\). Note that many other fixed point algorithms could be use for the same purpose [(14)].

### Casting feedforward nets into hierarchical energy-based models

An important example.Another way to cast feedforward nets training by EBL algorithms is to break them into multiple, hierarchical energy-based constraints based on Eq. (3) [(105)]. The following example, which shows how to do so, is also a good illustration of why the problems \(_{1}\)[Eq. (1)] and \(_{2}\)[Eq. (3)] are generally not equivalent. Taking \(E:=_{k=1}^{K}E_{k}\) with \(E_{k}(s^{k},s^{k-1},^{k}):=G(s^{k})-s^{k^{}}^{k} s^{ k-1}\), we consider \(_{1}\) with \(=_{s}E\) and \(_{2}\) with \(_{k}=_{s^{k}}E^{k}\). In this case, \(s^{k}\)_is a single layer_ and not a block of several layers. Setting the constraint to zero in these two cases yields:

\[_{1}:\;s^{k}=(^{k} s^{k-1}+^{k+1^{ }} s^{k+1}),_{2}:\;s^{k}=(^{k}  s^{k-1}) \]

In one case, the model retained at inference time is a _Hopfield model_ and in the other one a _feedforward_ model. This difference is instrumental to understand why works revolving around "contrastive learning" may actually apply to feedforward models through the \(_{2}\) problem formulation [(36)]. Likewise, the nudged state defined by Eq. (12) reads for any layer _until penultimate_ the same as Eq. (16) for problem \(_{1}\) and as follows for \(_{2}\), taking \(\) sufficiently small [(105; 75)]:

\[_{2}:\;s^{k}_{}=(^{k} s^{k-1} ^{k+1^{}} d_{}s^{k+1}_{}|_{=0}) (^{k} s^{k-1}^{k+1^{}}(s^{k+1}_{ }-s^{k+1}_{-})/2), \]

Note that many other choices of energy functions are possible in order to learn feedforward nets as a hierarchical energy-based model [(51; 105; 31; 16)]

Breaking the forward-backward synchrony.As seen from Eq. (17), computing \(s^{k}_{}\) still requires the storage of \(s^{k-1}\). A trick is to notice that for sufficiently small \(\), \(s^{k}(s^{k}_{}+s^{k}_{-})/2:=_{}\) such that \(s^{k}_{}(^{k}_{}^{k+1^{ }} s^{k+1}_{})\) with \( s^{k+1}_{}:=s^{k+1}_{}-s^{k+1}_{-}\). Implemented this way, the gradient corresponding to problem \(^{2}\) with the above choice of energy function can be computed by _simultaneously_ solving for \(s^{k}_{}\)'s and \(s^{k}_{-}\)'s [(36)]. This implementation can also be derived by picking hierarchical constraints (Eq. (3)), relaxing these (Eq. (7)) and reparametrizing them (Eq. (14)) [(37)].

## 4 Looking ahead & take-aways

Going out of equilibrium?One of the ways to leverage equilibrium techniques towards learning from sequences is to treat the neuron _velocity_\(\) as a variable of its own inside the energy function (32; 90), or to put it differently to pick the _Lagrangian_ of the physical system as an energy function such that the resulting equilibrium is an actual trajectory (84). Another exciting route beyond equilibrium models is to consider system which have physical access to their _adjoint_ trajectory through _time-reversible_, Hamiltonian-based dynamics (54). This requirement can be regarded as the condition to be self-adjoint in the situation where the constraint \(\) at use is a _trajectory_ rather than an equilibrium state. In such a case, the associated algorithmic baseline is no longer implicit differentiation but the _continuous adjoint state method_ (83).

Forward-only learning beyond first order.One may wonder if (and how) _second-order optimization_ could be emulated using a _single_ circuit. A second-order update generally prescribes \(_{}}_{x}[C(x, )]^{-1}_{x}[g(x,)]\) where \(\) is a curvature matrix, i.e. the loss _Hessian_ or p.s.d. approximations thereof (58). As such, computing \(_{}}\) generally subsumes (_Hessian-free_ approaches being a notable exception (60)): i) computing \(g\), ii) computing \(C\), iii) inverting \(C\), iv) computing \(_{}}\). However, an interesting insight arises when \(||=1\) (unit batch size), picking the Fisher information curvature matrix \(C=(_{}s^{}_{}s)\) (59) and assuming \(\) does not explicitly depend on \(\) for simplicity. Then, denoting \(A^{}\) the _Moore-Penrose pseudo-inverse_ of a matrix \(A\):

\[_{}}(_{}s^{} _{}s)^{-1}_{}s^{}_{s} =_{}s^{}_{s}. \]

Therefore, moving to second order optimization here amounts to route the error signal \(_{s}\) backward through the system's _inverse_ rather than its adjoint. While this viewpoint has motivated several implementations of _self-invertible_ (rather than _self-adjoint_, or equivalently energy-based) circuits (50; 62; 12; 63), it may not straightforwardly generalize to the mini-batch regime (\(||>1\)).

Finding energy functions that map to physical systems.Given a system described by the implicit mapping \(=0\), one can always pick \(E=1/2\|\|^{2}\) (65), or even other variants picking other distances beyond the Euclidean metric, as a valid energy function. Also, the hierarchical energy-based setting depicted above offers even more flexibility to cast many feedforward architectures into an energy-based model. Beyond these tricks, other important computational primitives such as batch normalization or attention can also be construed as energy-based operations (38), yet to be investigated in the context of forward-only energy-based learning. So at first glance, "everything" seems to be energy-based and therefore trainable by the aforementioned forward-only techniques. However, all energy functions may not necessarily map to a physical system, namely a lot of these energy functions may _not_ be physical. For instance, the fact that layered resistive networks can compute MLP operations at equilibrium _and_ are governed by the minimization of an energy function that is itself the pseudo-power of these circuits (42; 85) is truly remarkable. Therefore, finding an energy function which realizes some core computational primitive _while being physically feasible_ is difficult.

Analog-digital codesign.While _fully analog_ learning experimental proof-of-concepts have been realized on small systems (20; 104), bringing analog systems at chip scale inherently requires digital circuitry for registers, tile-to-tile routing, analog non idealities mitigation (ADCs & DACs), and more fundamentally for the implementation of analytical and possibly non-weight stationnary operations (95; 49; 104). Beyond this practical aspect, one may wonder if we even _should_ map all operations into analog. For instance, attention operations are known to be memory-bounded both at inference and training time when using large context windows rather than bottlenecked by the digital compute itself (91; 19). So it is unclear whether or not analog computing may necessarily bring an advantage for this particular example. Overall this observation calls for digital-analog codesign, and to a further extent for the design of new theoretical building blocks that would account for both digital _and_ analog parts of a circuit with associated learning algorithms (75).

Conclusion.This "cookbook" is an attempt to complement recent algorithm unification papers (105; 106; 68; 78; 70) using pragmatic, ingredient-based logics. For instance, "Predictive coding" (100) amounts to apply Eq. (12) with a specific choice of energy function (3.4) and nudging scheme (3.2). Likewise, the "Forward-forward" algorithm (33) is a greedy learning strategy, with each block comprising a single layer, using local self-supervised loss (2.3). Conversely, one may want to split an architecture into blocks (Eq. (3)), apply standard implicit differentiation in some (2.2, SS1), ZO in others (2.2, SS1), along with a pipelining mechanism (2.3) depending on the constraints at hand, which does not correspond to any known algorithm in the literature.