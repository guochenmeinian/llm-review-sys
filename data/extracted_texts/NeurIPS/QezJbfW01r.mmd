# Adaptive Privacy Composition for Accuracy-first Mechanisms

Ryan Rogers

LinkedIn

Gennady Samorodnitsky

Cornell University

Zhiwei Steven Wu

Carnegie Mellon University

Aaditya Ramdas

Carnegie Mellon University

###### Abstract

In many practical applications of differential privacy, practitioners seek to provide the best privacy guarantees subject to a target level of accuracy. A recent line of work by Ligett et al. (2017), Whitehouse et al. (2022) has developed such accuracy-first mechanisms by leveraging the idea of _noise reduction_ that adds correlated noise to the sufficient statistic in a private computation and produces a sequence of increasingly accurate answers. A major advantage of noise reduction mechanisms is that the analysts only pay the privacy cost of the least noisy or most accurate answer released. Despite this appealing property in isolation, there has not been a systematic study on how to use them in conjunction with other differentially private mechanisms. A fundamental challenge is that the privacy guarantee for noise reduction mechanisms is (necessarily) formulated as _ex-post privacy_ that bounds the privacy loss as a function of the released outcome. Furthermore, there has yet to be any study on how ex-post private mechanisms compose, which allows us to track the accumulated privacy over several mechanisms. We develop privacy filters (Rogers et al., 2016; Feldman and Zrnic, 2021; Whitehouse et al., 2023) that allow an analyst to adaptively switch between differentially private and ex-post private mechanisms subject to an overall differential privacy guarantee.

## 1 Introduction

Although differential privacy has been recognized by the research community as the de-facto standard to ensure the privacy of a sensitive dataset while still allowing useful insights, it has yet to become widely applied in practice despite its promise to ensure formal privacy guarantees. There are notable applications of differential privacy, including the U.S. Census (Abowd et al., 2022), yet few would argue that differential privacy has become quite standard in practice.

One common objection to differential privacy is that it injects noise and can cause spurious results for data analyses. A recent line of work in differential privacy has focused on developing _accuracy-first_ mechanisms that aim to ensure a target accuracy guarantee while achieving the best privacy guarantee (Ligett et al., 2017; Whitehouse et al., 2022). In particular, these _accuracy-first_ mechanisms do not ensure a predetermined level of privacy, but instead provide _ex-post privacy_, which allows the resulting privacy loss to depend on the outcome of the mechanism. This is in contrast to the prevalent paradigm of differential privacy that fixes the scale of privacy noise in advance and hopes the result is accurate. With accuracy-first mechanisms, practitioners instead specify the levels of accuracy that would ensure useful data analyses and then aim to achieve such utility with the strongest privacy guarantee.

However, one of the limitations of this line of work is that it is not clear how ex-post privacy mechanisms _compose_, so if we combine multiple ex-post privacy mechanisms, what is the overallprivacy guarantee? Composition is one of the key properties of differential privacy (when used in a _privacy-first_ manner), so it is important to develop a composition theory for ex-post privacy mechanisms. Moreover, how do we analyze the privacy guarantee when we compose ex-post privacy mechanisms with differentially private mechanisms?

Our work seeks to answer these questions by connecting with another line work in differential privacy on _fully adaptive privacy composition_. Traditional differential privacy composition results would require the analyst to fix privacy loss parameters, which is then inversely proportional to the scale of noise, for each analysis in advance, prior to any interaction. Knowing that there will be noise, the data scientist may want to select different levels of noise for different analyses, subject to some overall privacy budget. Privacy filters and odometers, introduced in Rogers et al. (2016), provide a way to bound the overall privacy guarantee despite adaptively selected privacy loss parameters. There have since been other works that have improved on the privacy loss bounds in this adaptive setting, to the point of matching (including constants) what one achieves in a nonadaptive setting (Feldman and Zrnic, 2021; Whitehouse et al., 2023).

A natural next step would then be to allow an analyst some overall privacy loss budget to interact with the dataset and the analyst can then determine the accuracy metric they want to set with each new query. As a motivating example, consider some accuracy metric of \(\%\) relative error to different counts with some overall privacy loss parameters \(,\), so that the entire interaction will be \((,)\)-differentially private. The first true count might be very large, so the amount of noise that needs to be added to ensure the target \(\%\) relative error can be huge, and hence very little of the privacy budget should be used for that query, allowing for potentially more results to be returned than an approach that sets an a priori noise level.

A baseline approach to add relative noise would be to add a large amount of noise and then check if the noisy count is within some tolerance based on the scale of noise added, then if the noisy count is deemed good enough we stop, otherwise we scale the privacy loss up by some factor and repeat. We refer to this approach as the _doubling approach_ (see Section 7.1 for more details), which was also used in Ligett et al. (2017). The primary issue with this approach is that the accumulated privacy loss needs to combine the privacy loss each time we add noise, even though we are only interested in the outcome when we stopped. Noise reduction mechanisms from Ligett et al. (2017), Whitehouse et al. (2022) show how it is possible to only pay for the privacy of the last noise addition. However, it is not clear how the privacy loss will accumulate over several noise reduction mechanisms, since each one ensures ex-post privacy, not differential privacy.

We make the following contributions in this work:

* We present a general (basic) composition result for ex-post privacy mechanisms that can be used to create a privacy filter when an analyst can select arbitrary ex-post privacy mechanisms and (concentrated) differentially private mechanisms.
* We develop a unified privacy filter that combines noise reduction mechanisms -- specifically the Brownian Mechanism (Whitehouse et al., 2022) -- with traditional (concentrated) differentially private mechanisms.
* We apply our results to the task of releasing counts from a dataset subject to a relative error bound comparing the unified privacy filter and the baseline doubling approach, which uses the privacy filters from Whitehouse et al. (2023).

Our main technical contribution is in the unified privacy filter for noise reduction mechanisms and differentially private mechanisms. Prior work (Whitehouse et al., 2022) showed that the privacy loss of the Brownian noise reduction can be written in terms of a scaled Brownian motion at the time where the noise reduction was stopped. We present a new analysis for the ex-post privacy guarantee of the Brownian mechanism that considers a reverse time martingale, based on a scaled standard Brownian motion. Composition bounds for differential privacy consider a forward time martingale and apply a concentration bound, such as Azuma's inequality (Dwork et al., 2010), so we show how we can construct a forward time martingale from the stopped Brownian motions, despite the stopping times being adaptive, not predictable, at each time. See Figure 1 for a sketch of how we combine reverse time martingales into an overall forward time filtration.

There have been other works that have considered adding relative noise subject to differential privacy. In particular, iReduct (Xiao et al., 2011) was developed to release a batch of queries subject to a relative error bound. The primary difference between that work and our setting here is that we do not want to fix the number of queries in advance and we want to allow the queries to be selected in an adaptive way. Xiao et al. (2011) consider a batch of \(m\) queries and initially adds a lot of noise to each count and iteratively checks whether the noisy counts are good enough. The counts that should have smaller noise are then identified and a Laplace based noise reduction algorithm is used to decrease the noise on the identified set. They continue in this way until either all counts are good enough or a target privacy loss is exhausted. There are two scenarios that might arise: (1) all counts satisfy the relative error condition and we should have tried more counts because some privacy loss budget remains, (2) the procedure stopped before some results had a target relative error, so we should have selected fewer queries. In either case, selecting the number of queries becomes a parameter that the data analyst would need to select in advance, which would be difficult to do a priori. In our setting, no such parameter arises. Furthermore, they add up the privacy loss parameters for each count to see if it is below a target privacy loss bound at each step (based on the \(_{1}\)-general sensitivity), however we show that adding up the privacy loss parameters can be significantly improved on. Other works have modified the definition of differential privacy to accommodate relative error, e.g. Wang et al. (2022).

## 2 Preliminaries

We start with some basic definitions from differential privacy, beginning with the standard definition from Dwork et al. (2006b, a). We first need to define what we mean by _neighboring datasets_, which can mean adding or removing one record to a dataset or changing an entry in a dataset. We will leave the neighboring relation arbitrary, and write \(x,x^{}\) to be neighbors as \(x x^{}\).

**Definition 2.1**.: _An algorithm \(A:\) is \((,)\)-differentially private if, for any measurable set \(E\) and any neighboring inputs \(x x^{}\), \([A(x) E] e^{}[A(x^{}) E]+\). If \(=0\), we say \(A\) is \(\)-DP or simply pure DP._

We define a statistic's _sensitivity_ as the following: \(_{p}(f):=_{x,x^{}:x x^{}}\{||f(x)-f(x^{} )||_{p}\}.\)

A central actor in our analysis will be the _privacy loss_ of an algorithm, which will be a random variable that depends on the outcomes of the algorithm under a particular dataset.

**Definition 2.2** (Privacy Loss).: _Let \(A:\) be an algorithm and fix neighbors \(x x^{}\) in \(\). Let \(p^{x}\) and \(p^{x^{}}\) be the respective densities of \(A(x)\) and \(A(x^{})\) on the space \(\) with respect to some reference measure. Then, the privacy loss between \(A(x)\) and \(A(x^{})\) evaluated at point \(y\) is:_

\[_{A}(y;x,x^{}):=((y)}{p^{x^{}}(y) }).\]

Figure 1: Our privacy filter tracks an accumulated privacy loss over many different mechanisms (forward filtration, X-axis) and stopping when it exceeds a predefined \(\) (dotted line). Each mechanism satisfies approximate zCDP (blue dot) or is a noise reduction mechanism (black dots). The latter itself involves several rounds of interaction in a reverse filtration (red arrow) until a stopping criterion based on utility is met (red box). Later queries/mechanisms can depend on past ones.

_Further, we refer to the privacy loss random variable to be \(_{A}(x,x^{}):=_{A}(A(x);x,x^{})\). When the algorithm \(A\) and the neighboring datasets are clear from context, we drop them from the privacy loss, i.e. \((y)=_{A}(y;x,x^{})\) and \(=_{A}(x,x^{})\)._

Many existing privacy composition analyses leverage a variant of DP called (approximate) _zero-concentrated DP_ (zCDP), introduced by Bun and Steinke (2016). Recall that the Renyi Divergence of order \( 1\) between two distributions \(P\) and \(Q\) on the same domain is written as the following where \(p()\) and \(q()\) are the respective probability mass/density functions,

\[D_{}(P||Q):=(_{y P}[ ()^{-1}]).\]

Since we study fully adaptive privacy composition (where privacy parameters can be chosen adaptively), we will use the following _conditional_ extension of approximate zCDP, in which the zCDP parameters of a mechanism \(A\) can depend on prior outcomes.

**Definition 2.3** (Approximate zCDP (Whitehouse et al., 2023)).: _Suppose \(A:\) with outputs in a measurable space \((,)\). Suppose \(,:_{ 0}\). We say the algorithm \(A\) satisfies conditional \((z)\)-approximate \((z)\)-zCDP if, for all \(z\) and any neighboring datasets \(x,x^{}\), there exist probability transition kernels \(P^{},P^{},Q^{},Q^{}: \) such that the conditional outputs are distributed according to the following mixture distributions:_

\[A(x;z) (1-(z))P^{}( z)+(z)P^{ }( z)\] \[A(x^{};z) (1-(z))Q^{}( z)+(z)Q^{ }( z),\]

_where for all \( 1\), \(D_{}(P^{}( z)\|Q^{}( z))(z)\) and \(D_{}(Q^{}( z)\|P^{}( z))(z)\), \( z\)._

The following results establish the relationship between zCDP and DP and the composition of zCDP.

**Lemma 2.1** (Bun and Steinke (2016)).: _If \(M:\) is \((,)\)-DP, then \(M\) is \(\)-approximate \(^{2}/2\)-zCDP. If \(M\) is \(\)-approximate \(\)-zCDP, then \(M\) is \((+2)},+^{})\)-DP for \(^{}>0\)._

**Lemma 2.2** (Bun and Steinke (2016)).: _If \(M_{1}:\) is \(_{1}\)-approximate \(_{1}\)-zCDP and \(M_{2}:^{}\) is \(_{2}\)-approximate \(_{2}\)-zCDP in its first coordinate, then \(M:^{}\) where \(M(x)=(M_{1}(x),M_{2}(x,M_{1}(x)))\) is \((_{1}+_{2})\)-approximate \((_{1}+_{2})\)-zCDP._

## 3 Privacy Filters

In order for us to reason about the overall privacy loss of an interaction with a sensitive dataset, we will use the framework of _privacy filters_, introduced in Rogers et al. (2016). Privacy filters allow an analyst to adaptively select privacy parameters as a function of previous outcomes until some stopping time that determines whether a target privacy budget has been exhausted. To denote that privacy parameters may depend on previous outcomes, we will write \(_{n}(x)\) to mean the privacy parameter selected at round \(n\) that could depend on the previous outcomes \(A_{1:n-1}(x)\), and similarly for \(_{n}(x)\). We now state the definition of privacy filters in the context of approximate zCDP mechanisms.

**Definition 3.1** (Privacy Filter).: _Let \((A_{n}:)_{n 1}\) be an adaptive sequence of algorithms such that, for all \(n 1\), \(A_{n}(\ ;y_{1:n-1})\) is \(_{n}(y_{1:n-1})\)-approximate \(_{n}(y_{1:n-1})\)-zCDP for all \(y_{1:n-1}^{n-1}\). Let \(>0\) and \( 0\) be fixed privacy parameters. Then, a function \(N:^{}\) is an \((,)\)-privacy filter if_

1. _for all_ \((y_{1},y_{2},)^{}\)_,_ \(N(y_{1},y_{2},)\) _is a stopping time with respect to the natural filtration generated by_ \((A_{n}(x))_{n 1}\)_, and_
2. \(A_{1:N()}()\) _is_ \((,)\)_-differentially private where_ \(N(x)=N(A_{1}(x),A_{2}(x),)\)_._

Whitehouse et al. (2023) showed that we can use composition bounds from traditional differential privacy (which required setting privacy parameters for the interaction prior to any interaction) in the more adaptive setting, where privacy parameters can be set before each query.

**Theorem 1**.: _Suppose \((A_{n}:)_{n 1}\) is a sequence of algorithms such that, for any \(n 1\), \(A_{n}(\ ;y_{1:n-1})\) is \(_{n}(y_{1:n-1})\)-approximate \(_{n}(y_{1:n-1})\)-zCDP for all \(y_{1:n-1}\). Let \(>0\) and \( 0\)\(^{}>0\) be fixed privacy parameters. Consider the function \(N:^{}_{ 0}\) given by the following where \(<_{m n+1}_{m}(y_{1:m-1})\) for all \(y_{1},y_{2},\) and_

\[N(y_{1},y_{2},):=\{n:<2})_{m n+1}_{m}(y_{1:m-1})}+_{m n+1} _{m}(y_{1:m-1})\}.\]

_Then, the algorithm \(A_{1:N()}()\) is \((,+^{})\)-DP, where \(N(x):=N((A_{n}(x))_{n 1})\). In other words, \(N\) is an \((,)\)-privacy filter._

## 4 Ex-post Private Mechanisms

Although privacy filters allow a lot of flexibility to a data analyst in how they interact with a sensitive dataset while still guaranteeing a fixed privacy budget, there are some algorithms that ensure a bound on privacy that is adapted to the dataset. Ex-post private mechanisms define privacy loss as a probabilistic bound which can depend on the algorithm's outcomes, so some outcomes might contain more information about an individual than others (Ligett et al., 2017; Whitehouse et al., 2022). Note that ex-post private mechanisms do not have any fixed a priori bound on the privacy loss, so by default they cannot be composed in a similar way to differentially private mechanisms.

**Definition 4.1**.: _Let \(A:\) be an algorithm and \(:_{ 0}\) a function. We say \(A(\,;y)\) is \(((\,;y),(y))\)-ex-post private for all \(y\) if, for any neighboring inputs \(x x^{}\), we have \([(A(x;y))>(A(x;y);y)](y)\) for all \(y\)._

We next define a single noise reduction mechanism, which will interactively apply sub-mechanisms and stop at some time \(k\), which can be random. Each iterate will use a privacy parameter from an increasing sequence of privacy parameters \((^{(n)}:n 1)\) set in advance and the overall privacy will only depend on the last privacy parameter \(^{(k)}\), despite releasing noisy values with parameters \(^{(i)}\) for \(i k\). Noise reduction algorithms will allow us to form ex-post private mechanisms because the privacy loss will only depend on the final outcome. We will write \(M:^{*}\) to be any algorithm mapping databases to sequences of outputs in \(\), with intermediate mechanisms written as \(M^{(k)}:\) for the \(k\)-th element of the sequence and \(M^{(1:k)}:^{k}\) for the first \(k\) elements. Let \(\) be a probability measure on \(\), for each \(k 1\) let \(_{k}\) be a probability measure on \(^{*}\). We assume that the law of \(M^{(k)}(x)\) on \(\) is equivalent to \(\), the law of \(M(x)\) on \(^{*}\) is equivalent to \(^{*}\), and the law of \(M^{(1:k)}(x)\) on \(^{k}\) is equivalent to \(_{k}\) for every \(k\) and every \(x\). Furthermore, we will write \(\) to be the privacy loss of \(M\), \(^{(k)}\) be the privacy loss of \(M^{(k)}\), and \(^{(1:k)}\) to be the privacy loss of the sequence of mechanisms \(M^{(1:k)}\). We then define noise reduction mechanisms more formally.

**Definition 4.2** (Noise Reduction Mechanism).: _Let \(M:^{}\) be a mechanism mapping sequences of outcomes and \(x,x^{}\) be any neighboring datasets. We say \(M\) is a noise reduction mechanism if for any \(k 1\), \(^{(1:k)}=^{(k)}\)._

We will assume there is a fixed grid of time values \(t^{(1)}>t^{(2)}>>t^{(k)}>0\). We will typically think of the time values as being inversely proportional to the noise we add, i.e. \(t^{(i)}=(1/^{(i)})^{2}\) where \(^{(1)}<^{(2)}<\). An analyst will not have a particular stopping time set in advance and will instead want to stop interacting with the dataset as a function of the noisy answers that have been released. It might also be the case that the analyst wants to stop based on the outcome and the privatized dataset, but in this work we consider stopping times that can only depend on the noisy outcomes or possibly some public information, not the underlying dataset.

**Definition 4.3** (Stopping Function).: _Let \(A:^{}\) be a noise reduction mechanism. For \(x\), let \((^{(k)}(x))_{k}\) be the filtration given by \(^{(k)}(x):=(A^{(i)}(x):i k)\). A function \(T:^{}\) is called a stopping function if for any \(x\), \(T(x):=T(A(x))\) is a stopping time with respect to \((^{(k)}(x))_{k 1}\). Note that this property does not depend on the choice of measures \(\), \(^{*}\) and \(_{k}\)._

We now recall the noise reduction mechanism with Brownian noise (Whitehouse et al., 2022).

**Definition 4.4** (Brownian Noise Reduction).: _Let \(f:^{d}\) be a function and \((t^{(k)})_{k 1}\) be a sequence of time values. Let \((B^{(t)})_{t 0}\) be a standard \(d\)-dimensional Brownian motion and \((^{d})^{}\) be a stopping function. The Brownian mechanism associated with \(f\), time values \((t^{(k)})_{k 1}\), and stopping function \(T\) is the algorithm \(:((0,t^{(1)}]^{d})^{*}\) given by_

\[(x):=(t^{(k)},f(x)+B^{(^{(k)})})_{k T(x)}.\]

We then have the following result.

**Lemma 4.1** (Whitehouse et al. (2022)).: _The Brownian Noise Reduction mechanism \(\) is a noise reduction algorithm for a constant stopping function \(T(x)=k\). Furthermore, we have for any stopping function \(T:^{}\), the noise reduction property still holds, i.e. \(^{(1:T(x))}=^{(T(x))}\)._

Another noise reduction mechanism uses Laplace noise from Koufogiannis et al. (2017); Ligett et al. (2017), which we consider in Appendix C.

## 5 General Composition of Ex-post Private Mechanisms

We consider combining zCDP mechanisms with mechanisms that satisfy ex-post privacy. We consider a sequence of mechanisms \((A_{n}:)_{n 1}\) where each mechanism may depend on the previous outcomes. At each round, an analyst will use either an ex-post private mechanism or an approximate zCDP mechanism, in either case the privacy parameters may depend on the previous results as well. We will write \(_{n}(x):=_{n}(A_{1}(x),,A_{n-1})(x))\), \(_{n}(x):=(A_{1}(x),,A_{n-1})(x))\), and \(_{n}(A_{n}(x);x):=_{n}(A_{n}(x);A_{1}(x),,A_{n-1} (x))\).

**Definition 5.1** (Approximate zCDP and Ex-post Private Sequence).: _Consider a sequence of mechanisms \((A_{n})_{n 1}\), where \(A_{n}:\). The sequence \((A_{n})_{n 1}\) is called a sequence of approximate zCDP and ex-post private mechanisms if for each round \(n\), the analyst will select \(A_{n}()\) to be \(_{n}()\)-approximate \(_{n}()\)-zCDP given previous outcomes \(A_{i}()\) for \(i<n\), or the analyst will select \(A_{n}()\) to be \((_{n}(A_{n}()\,;),^{}_{n}())\)-ex-post private conditioned on \(A_{i}()\) for \(i<n\). In rounds where zCDP is selected, we will simply write \(_{i}(A_{i}();) 0\), while in rounds where an ex-post private mechanism is selected, we will set \(_{i}()=0\)._

We now state a composition result that allows an analyst to combine ex-post private and zCDP mechanisms adaptively, while still ensuring a target level of privacy. Because we have two different interactive systems that are differentially private, one that uses only zCDP mechanisms and the other that only uses ex-post private mechanisms, we can then use concurrent composition, see Appendix A for more details.

**Theorem 2**.: _Let \(,^{},,^{},^{}>0\). Let \((A_{n})_{n 1}\) be a sequence of approximate zCDP and ex-post private mechanisms. We require that the ex-post private mechanisms that are selected at each round \(n\) to have ex-post privacy functions \(_{n}\) that do not exceed the remaining budget from \(^{}\). Consider the following function \(N:^{}\) as the following for any sequence of outcomes \((y_{n})_{n 1}\)_

\[N((y_{n})_{n 1},(y_{n})=\{N_{}((y_{1:n-1})_{n 1}), N_{}((y_{n})_{n 1})\},\]

_where \(N_{}((y_{n})_{n 1})\) is the stopping rule given in Theorem 1 with privacy parameters \(,,^{}\) and \(N_{}((y_{n})_{n 1})\) is the stopping rule where the sum of realized \(_{n}(y_{n};y_{1},,y_{n-1})\) values cannot go above \(^{}\) nor can the sum of \(^{}_{n}(y_{1},,y_{n-1})\) go above \(^{}\) (see Lemma B.2 with privacy parameters \(^{},^{}\)). Then, the algorithm \(A_{1:N()}()\) is \((+^{},+^{}+^{})\)-DP, where \(N(x)=N((A_{n}(x))_{n 1}).\)_

Although we are able to leverage _advanced composition_ bounds from traditional differential privacy for the mechanisms that are approximate zCDP, we are simply adding up the ex-post privacy guarantees, which seems wasteful. Next, we consider how we can improve on this composition bound for certain ex-post private mechanisms.

## 6 Brownian Noise Reduction Mechanisms

We now consider composing specific types of ex-post private mechanisms, specifically the Brownian Noise Reduction mechanism. From Theorem 3.4 in Whitehouse et al. (2022), we can decompose the privacy loss as an uncentered Brownian motion, even when the stopping time is adaptively selected.

**Theorem 3** (Whitehouse et al. (2022)).: _Let \(\)be the Brownian noise reduction mechanism associated with time values \((t^{(k)})_{k 1}\) and a function \(f\). All reference measures generated by the mechanism are those generated by the Brownian motion without shift (starting at \(f(x)=0\)). For neighbors \(x x^{}\) and stopping function \(T\), the privacy loss between \(^{(1:T(x))}(x)\) and \(^{(1:T(x^{}))}(x^{})\) is given by_

\[^{(1:T(x))}_{^{T}}(x,x^{})=)- f(x)||_{2}^{2}}{2t^{(T(x))}}+)-f(x)||_{2}}{t^{(T(x))}} )-f(x)}{||f(x^{})-f(x)||_{2}},B^{(t^{(T(x ))})}.\]

This decomposition of the privacy loss will be very useful in analyzing the overall privacy loss of a combination of Brownian noise reduction mechanisms. In Appendix C we provide a more general analysis than in Whitehouse et al. (2022) to show that both the Laplace noise reduction from Ligett et al. (2017) and the Brownian noise reduction Whitehouse et al. (2022) are indeed noise reduction mechanisms and give the form of the privacy loss at an adaptive stopping time.

### Backward Brownian Motion Martingale

We now present a key result for our analysis of composing Brownian noise reduction mechanisms. Although in Whitehouse et al. (2022), the ex-post privacy proof of the Brownian mechanism applied Ville's inequality (for proof, cf. Howard et al., 2020, Lemma 1) to the (unscaled) standard Brownian motion \((B^{(t)})_{t>0}\), it turns out that the scaled standard Brownian motion \((B^{(t)}/t)\) forms a backward martingale (cf. Revuz and Yor, 2013, Exercise 2.16) and this fact is crucial to our analysis.

**Lemma 6.1** (Backward martingale).: _Let \((B^{(t)})\) be a standard Brownian motion. Define the reverse filtration \(^{(t)}=(B^{(u)};u t)\), meaning that \(^{(s)}^{(t)}\) if \(s<t\). For every real \(\), the process_

\[M^{(t)}:=( B^{(t)}/t-^{2}/(2t)); t>0\]

_is a nonnegative (reverse) martingale with respect to the filtration \(=(^{(t)})\). Further, at any \(t>0\), \([M^{(t)}]=1\), \(M^{()}=1\) almost surely, and \([M^{()}] 1\) for any stopping time \(\) with respect to \(\) (equality holds with some restrictions). In short, \(M^{()}\) is an "e-value" for any stopping time \(\) -- an e-value is a nonnegative random variable with expectation at most one._

Let \(B_{1}=(B_{1}^{(t)})_{t 0},B_{2}=(B_{2}^{(t)})_{t 0},\) be independent, standard Brownian motions, with corresponding backward martingales \(M_{1}=(M_{1}^{(t)})_{t 0},M_{2}=(M_{2}^{(t)})_{t 0},\) and (internal to each Brownian motion) filtrations \(_{1}=(_{1}^{(t)})_{t 0},_{2}=( _{2}^{(t)})_{t 0},\) as defined in the previous lemma. Select time values \((t_{1}^{(k)})_{k 1}\). Let a Brownian noise reduction mechanism \(_{1}\) be run using \(B_{1}\) and stopped at \(_{1}\). Then \(E[M_{1}^{(_{1})}] 1\) as per the previous lemma. Based on outputs from the \(_{1}\), we choose time values \((t_{2}^{(k)})_{k 1}(B_{1}^{(t)};t_{1}):=_{1}\). Now run the second Brownian noise reduction \(_{2}\) using \(B_{2}\), stopping at time \(_{2}\). Since \(B_{1},B_{2}\) are independent, we still have that \([M_{2}^{(_{2})}|_{1}] 1\). Let \(_{2}:=((B_{1}^{(t)})_{t_{1}},(B_{2}^{(t)})_{t _{2}})\) be the updated filtration, based on which we choose time values \((t_{3}^{(k)})_{k 1}\). Because \(B_{3}\) is independent of the earlier two, at the next step, we still have \([M_{3}^{(_{3})}|_{2}] 1\). Proceeding in this fashion, it is clear that the product of the stopped e-values \(E_{m}\), where

\[E_{m}:=_{s=1}^{m}M_{s}^{(_{s})}=(_{s=1}^{m}^{(_{s})}}{_{s}}-}{2}_{s=1}^{m}}),\] (1)

is itself a (forward) nonnegative supermartingale with respect to the filtration \(=(_{n})_{n 1}\), with initial value \(E_{0}:=1\). Applying the Gaussian mixture method (cf. Howard et al., 2021, Proposition 5), we get that for any \(,>0\) and with \((t;,):= })}\),

\[[_{m 1}\{|_{s=1}^{m}^{(_{s})}}{ _{s}}|_{s[m]}}+(_{s[m] }1/_{s};,)\}].\]

This then provides an alternate way to prove Theorem 3.6 in Whitehouse et al. (2022).

**Theorem 4**.: _Let \((T_{i})_{i 1}\) be a sequence of stopping functions, as in Definition 4.3, and a sequence of time values \((t_{i}^{(j)}:j[k_{i}])_{i 1}\). Let \(_{i}\) denote a Brownian noise reduction with statistic \(f_{i}\) that can be adaptively selected based on outcomes of previous Brownian noise reductions and \(f_{i}\) has \(_{2}\)-sensitivity 1. We then have, for any \(,>0\),_

\[_{x x^{}}[_{i=1}^{}_{_{ i}}^{(T_{i}(x))}(_{i=1}^{}1/t_{i}^{(T_{i}(x))};, )].\]

_In other words, \((_{i}^{(1:T_{i}())})_{i 1}\) is \(((_{i=1}^{}1/t_{i}^{(T_{i}())};, ),)\)-ex post private._

### Privacy Filters with Brownian Noise Reduction Mechanisms

Given Lemma 6.1 and the decomposition of the privacy loss for the Brownian mechanism given in Theorem 3, we will be able to get tighter composition bounds of multiple Brownian noise reduction mechanisms rather than resorting to the general ex-post privacy composition (see Lemma B.2 in the appendix). It will be important to only use time values with the Brownian noise reduction mechanisms that cannot exceed the remaining privacy loss budget. We then make the following condition on the time values \((t_{n}^{(j)})_{j=1}^{k_{n}}\) that are used for each Brownian noise reduction given prior outcomes \(y_{1},,y_{n-1}\) from the earlier Brownian noise reductions with time values \((t_{i}^{(j)})_{j=1}^{k_{i}}\) and stopping functions \(T_{i}\) for \(i<n\) and overall budget \(>0\)

\[^{(k_{n})}}-_{i<n}^{(T_{i}(y_{1:i})) }}.\] (2)

**Lemma 6.2**.: _Let \(>0\) and consider a sequence of \((_{n})_{n 1}\) each with statistic \(f_{n}:^{d_{n}}\) with \(_{2}\) sensitivity \(1\), stopping function \(T_{n}\), and time values \((t_{n}^{(j)})_{j=1}^{k_{n}}\) which can be adaptively selected and satisfies (2). Consider the function \(N:^{}\{\}\) where \(\) contains all possible outcome streams from \((_{n})_{n 1}\) as the following for any sequence of outcomes \((y_{n})_{n 1}\):_

\[N((y_{n})_{n 1})=\{n:=_{i[n]}^{(T_{i}(y_{1 :i}))}}\}.\]

_Then, for \(>0\), \(_{1:N()}()\) is \((+2,)\)-DP, where \(N(x)=N((_{n}(x))_{n 1})\)._

One approach to defining a privacy filter for both approximate zCDP and Brownian noise reduction mechanisms would be to use concurrent composition, as we did in Lemma B.2 in the appendix. However, this would require us to set separate privacy budgets for approximate zCDP mechanisms and Brownian noise reduction, which is an extra (nuissance) parameter to set.

We now show how we can combine Brownian noise reduction and approximate zCDP mechanisms with a single privacy budget. We will need a similar condition on the time values selected at each round for the Brownian noise reduction mechanisms as in (2). Note that at each round either an approximate zCDP or Brownian noise reduction mechanism will be selected. Given prior outcomes \(y_{1},,y_{n-1}\) and previously selected zCDP parameters \(_{1},_{2}(y_{1}),,_{n}(y_{1:n-1})\) -- noting that at round \(n\) where \(\) is selected we have \(_{n}(y_{1:n-1})=0\) or if a zCDP mechanism is selected we simply set \(k_{n}=1\) and \(^{(1)}}=0\) -- we have the following condition on \(_{n}(y_{1:n-1})\) and the time values \((t_{n}^{(j)})_{j=1}^{k_{n}}\) if we select a \(\) at round \(n\) and have overall budget \(>0\),

\[0^{(k_{n})}}+_{n}(y_{1:n-1})-_{i<n}( _{i}(y_{1:i-1})+^{(T_{i}(y_{1:i}))}}).\] (3)

**Theorem 5**.: _Let \(>0\) and \( 0\). Let \((A_{n})_{n 1}\) be a sequence of approximate zCDP and ex-post private mechanisms where each ex-post private mechanism at round \(n\) is a Brownian Mechanism with an adaptively chosen stopping function \(T_{n}\), a statistic \(f_{n}\) with \(_{2}\)-sensitivity equal to 1, and time values \((t_{n}^{(j)})_{j=1}^{k_{n}}\) that satisfy the condition in (3). Consider the function \(N:^{}\) as thefollowing for any sequence of outcomes \((y_{n})_{n 1}\) :_

\[N((y_{n})_{n 1})=\{n:<_{i[n+1]}_{i}(y_{1:i-1}) =_{i[n]}(_{i}(y_{1:i-1})+^{(T_{i}(y_{1:i )})}})\}.\]

_Then for \(^{}>0\), the algorithm \(A_{1:N()}()\) is \((+2)},+^{})\)-DP, where the stopping function is \(N(x)=N((A_{n}(x))_{n 1})\)._

## 7 Application: Bounding Relative Error

Our motivating application will be returning as many counts as possible subject to each count being within \(\%\) relative error, i.e. if \(y\) is the true count and \(\) is the noisy count, we require \(||/y|-1|<\). It is common for practitioners to be able to tolerate some relative error to some statistics and would like to not be shown counts that are outside a certain accuracy. Typically, DP requires adding a predetermined standard deviation to counts, but it would be advantageous to be able to add large noise to large counts so that more counts could be returned subject to an overall privacy budget.

### Doubling Method

A simple baseline approach would be to use the "doubling method", as presented in Ligett et al. (2017). This approach uses differentially private mechanisms and checks whether each outcome satisfies some condition, in which case you stop, or the analyst continues with a larger privacy loss parameter. The downside of this approach is that the analyst needs to pay for the accrued privacy loss of all the rejected values. However, handling composition in this case turns out to be straightforward given Theorem 1, due to Whitehouse et al. (2023). We then compare the 'doubling method" against using Brownian noise reduction and applying Theorem 5.

We now present the doubling method formally. We take privacy loss parameters \(^{(1)}<^{(2)}<\), where \(^{(i+1)}=^{(i)}\). Similar to the argument in Claim B.1 in Ligett et al. (2017), we use the \(\) factor because the privacy loss will depend on the sum of square of privacy loss parameters, i.e. \(_{i=1}^{m}(^{(i)})^{2}\) up to some iterate \(m\), in Theorem 1 as \(_{i}=(^{(i)})^{2}/2\) is the zCDP parameter. This means that if \(^{*}\) is the privacy loss parameter that the algorithm would have halted at, then we might overshoot it by \(^{*}\). Further, the overall sum of square privacy losses will be no more than \(4(^{*})^{2}\). Hence, we refer to the doubling method as doubling the square of the privacy loss parameter.

### Experiments

We perform experiments to return as many results subject to a relative error tolerance \(\) and a privacy budget \(,>0\). We will generate synthetic data from a Zipf distribution, i.e. from a density \(f(k) k^{-a}\) for \(a>0\) and \(k\). We will set a max value of the distribution to be \(300\) and \(a=0.75\). We will assume that a user can modify each count by at most 1, so that the \(_{0}\)-sensitivity is 300 and \(_{}\)-sensitivity is 1. See Figure 3 in Appendix B for the data distribution we used.

In our experiments, we will want to first find the top count and then try to add the least amount of noise to it, while still achieving the target relative error, which we set to be \(=10\%\). To find the top count, we apply the Exponential Mechanism (McSherry and Talwar, 2007) by adding Gumbel noise with scale \(1/_{}\) to each sensitivity 1 count (all 300 elements' counts, even if they are zero in the data) and take the element with the top noisy count. From Cesar and Rogers (2021), we know that the Exponential Mechanism with parameter \(_{}\) is \(_{}^{2}/8\)-zCDP, which we will use in our composition bounds. For the Exponential Mechanism, we select a fixed parameter \(_{}=0.1\).

After we have selected a top element via the Exponential Mechanism, we then need to add some noise to it in order to return its count. Whether we use the doubling method and apply Theorem 1 for composition or the Brownian noise reduction mechanism and apply Theorem 5 for composition, we need a stopping condition. Note that we cannot use the true count to determine when to stop, but we can use the noisy count and the privacy loss parameter that was used. Hence we use the following condition based on the noisy count \(\) and the corresponding privacy loss parameter \(^{(i)}\) at iterate \(i\):

\[1-<|(+1/^{(i)})/(-1/^{(i)}) | 1+||>1/^{(i)}.\]Note that for Brownian noise reduction mechanism at round \(n\), we use time values \(t_{n}^{(i)}=1/(_{n}^{(i)})^{2}\). We also set an overall privacy budget of \((,^{})=(10,10^{-6})\). To determine when to stop, we will simply consider the sum of squared privacy parameters and stop if it is more than roughly 2.705, which corresponds to the overall privacy budget of \(=10\) with \(^{}=10^{-6}\). If the noisy value from the largest privacy loss parameter does not satisfy the condition above, we discard the result.

We pick the smallest privacy parameter squared to be \((_{n}^{(1)})^{2}=0.0001\) for each \(n\) in both the noise reduction and the doubling method and the largest value will change as we update the remaining sum of square privacy loss terms that have been used. We then set 1000 equally spaced parameters in noise reduction to select between \(0.0001\) and the largest value for the square of the privacy loss parameter. We then vary the sample size of the data in \(\{8000,16000,32000,64000,128000\}\) and see how many results are returned and of those returned, how many satisfy the actual relative error, which we refer to as _precision_. Note that if 0 results are returned, then we consider the precision to be 1. Our results are given in Figure 2 where we give the empirical average and standard deviation over 1000 trials for each sample size.

We also evaluated our approach on real data from Reddit comments from https://github.com/heyyjudes/differentially-private-set-union/tree/ea7b39285dace35cc5e9029692802759f3e1c8e8/data. This data consists of comments from Reddit authors. To find the most frequent words from distinct authors, we take the set of all distinct words contributed by each author, which can be arbitrarily large and form the resulting histogram which has \(_{}\)-sensitivity 1 yet unbounded \(_{2}\)-sensitivity. To get a domain of words to select from, we take the top-1000 words from this histogram. We note that this step should also be done with DP, but will not impact the relative performance between using the Brownian noise reduction and the Doubling Gaussian Method.

We then follow the same approach as on the synthetic data, using the Exponential Mechanism with \(_{}=0.01\), minimum privacy parameter \(_{n}^{(1)}=0.0001\), relative error \(=0.01\), and overall (\(=1,=10^{-6}\))-DP guarantee. In 1000 trials, the Brownian noise reduction precision (proportion of results that had noisy counts within 1% of the true count) was on average 97% (with minimum 92%) while the Doubling Gaussian Method precision was on average 98% (with minimum 93%). Furthermore, the number of results returned by the Brownian noise reduction in 1000 trials was on average 152 (with minimum 151), while the number of results returned by the Doubling Gaussian method was on average 109 (with minimum 108).

## 8 Conclusion

We have presented a way to combine approximate zCDP mechanisms and ex-post private mechanisms while achieving an overall differential privacy guarantee, allowing for more general and flexible types of interactions between an analyst and a sensitive dataset. Furthermore, we showed how this type of composition can be used to provide overall privacy guarantees subject to outcomes satisfying strict accuracy requirements, like relative error. We hope that that this will help extend the practicality of private data analysis by allowing the release of counts with relative error bounds subject to an overall privacy bound.

Figure 2: Precision and number of results returned on Zipfian data for various sample sizes.

Acknowledgements

We would like to thank Adrian Rivera Cardoso and Saikrishna Badrinarayanan for helpful comments. This work was done while G.S. was a visiting scholar at LinkedIn. ZSW was supported in part by the NSF awards 2120667 and 2232693 and a Cisco Research Grant.