# Deductive Verification of Chain-of-Thought Reasoning

Zhan Ling\({}^{1}\) Yunhao Fang\({}^{1}\) Xuanlin Li\({}^{1}\) Zhiao Huang\({}^{1}\) Mingu Lee\({}^{2}\)

**Roland Memisevic\({}^{2}\) Hao Su\({}^{1}\)**

\({}^{1}\)UC San Diego, \({}^{2}\)Qualcomm AI Research\({}^{}\)

Equal contributionQualcomm AI Research is an initiative of Qualcomm Technologies, Inc

All datasets and models were solely downloaded and evaluated by the University of California San Diego.

###### Abstract

Large Language Models (LLMs) significantly benefit from Chain-of-Thought (CoT) prompting in performing various reasoning tasks. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations and accumulated errors, thereby limiting models' ability to solve complex reasoning tasks. Inspired by how humans engage in careful and meticulous deductive logical reasoning processes to solve tasks, we seek to enable language models to perform _explicit and rigorous deductive reasoning_, and also ensure the _trusworthiness_ of their reasoning process through self-verification. However, directly verifying the validity of an entire deductive reasoning process is challenging, even with advanced models like ChatGPT. In light of this, we propose to decompose a reasoning verification process into a series of step-by-step subprocesses, each only receiving their necessary context and premises. To facilitate this procedure, we propose **Natural Program**, a _natural language-based_ deductive reasoning format. Our approach enables models to generate precise reasoning steps where subsequent steps are more rigorously grounded on prior steps. It also empowers language models to carry out reasoning self-verification in a _step-by-step_ manner. By integrating this verification process into each deductive reasoning stage, we significantly enhance the rigor and trustfulness of generated reasoning steps. Along this process, we also improve the answer correctness on complex reasoning tasks. Code will be released at https://github.com/lzloceani/verify_cot.

## 1 Introduction

The transformative power of large language models, enhanced by Chain-of-Thought (CoT) prompting , has significantly reshaped the landscape of information processing , fostering enhanced abilities across a myriad of disciplines and sectors. While CoT allows models to produce more comprehensive reasoning processes, its emphasis on intermediate reasoning steps can inadvertently introduce hallucinations  and accumulated errors , thereby limiting models' ability to produce cogent reasoning processes.

In fact, the pursuit of reliable reasoning is not a contemporary novelty; indeed, it is an intellectual endeavor that traces its roots back to the time of Aristotle's ancient Greece. Motivated by the desire to establish a rigorous reasoning process, in his "Organon," Aristotle introduced principles of _logic_, in particular, syllogism, a form of logical argument that applies deductive reasoning to arrive at a conclusion based on two or more propositions assumed to be true. In disciplines that rigorous reasoning is critical, such as judicial reasoning and mathematical problem solving, documents must be written in a formal language with a logical structure to ensure the validity of the reasoning process.

We yearn for this sequence of reliable knowledge when answering questions. Our goal is to develop language models that can propose potential solutions through reasoning in logical structures. Simultaneously, we aim to establish a verifier capable of accurately assessing the validity of these reasoning processes. Despite recent significant explorations in the field, such as 's emphasis on self-consistency and [27; 5]'s innovative use of codes to represent the reasoning process, these approaches still exhibit considerable limitations. For example, consistency and reliability are not inherently correlated; as for program codes, they are not powerful enough to represent many kinds of reasoning process, e.g., in the presence of quantifiers ("for all", "if there exists") or nuances of natural language (moral reasoning, "likely",...).

We propose leveraging the power of natural language to achieve the deductive reasoning emphasized in ancient Greek logic, introducing a _"natural program"_. This involves retaining natural language for its inherent power and avoiding the need for extensive retraining with large data sets. A natural program

Figure 1: An overview of our proposed deductive reasoning and verification process. In response to an input question, LLMs generate deductive reasoning chains using the _Natural Program_ format (bottom 3 boxes), a natural language-based deductive reasoning approach. The Natural Program format allows individual reasoning steps (an example in purple) and their corresponding minimal set of premises (an example in orange) to be easily extracted. This streamlined extraction process facilitates the step-by-step decomposition and verification of deductive reasoning (top-right box).

Figure 2: Through our Natural Program-based deductive reasoning verification approach, we identify and eliminate reasoning chains that contain errors in reasoning and grounding (we define grounding error as utilizing information that is not present in cited premises). By alleviating such errors, we significantly enhance the rigor, trustworthiness, and interpretability of the generated reasoning outputs.

represents a rigorous reasoning sequence, akin to a computer program. We expect implementations of the idea to have two properties: 1) that natural programs are generated with minimal effort from an existing language model capable of CoT reasoning, preferably through in-context learning; 2) that the natural program can be easily verified for reliability in the reasoning process.

Through a step-by-step investigation, we discovered that large language models have the potential to meet our expectation. Naive CoT prompts like "Let us think step by step." has many flaws, and entrusting the entire verification process to a large model like ChatGPT can still lead to significant error rates. However, we found that, if the reasoning process is very short, and only based on necessary premises and contexts, the verification of existing large language models is already quite reliable. Therefore, our approach is to design prompts that induce CoT processes comprised of rigorous premises/conditions and conclusions with statement labels, and verification can be done by gradually isolating very few statements within the long thought chain. Experimentally, we found that most reasoning that passed the verification was rigorous, and many that did not pass had elements of imprecision in the reasoning process, even if they occasionally arrived at correct answers.

It is worth emphasizing that, we are not looking for a method to just maximize the correctness rate of final answers; instead, we aspire to generate a cogent reasoning process, which is more aligned with the spirit of judicial reasoning. When combined with sampling-based methods, our method can identify low-probability but rigorous reasoning processes. When repeated sampling fails to yield a rigorous reasoning process, we can output "unknown" to prevent hallucinations that mislead users.

We demonstrate the efficacy of our natural program-based verification approach across a range of arithmetic and common sense datasets on publicly-available models like OpenAI's GPT-3.5-turbo. Our key contributions are as follows:

1. We propose a novel framework for rigorous deductive reasoning by introducing a "**Natural Program**" format (Fig. 1), which is suitable for verification and can be generated by just in-context learning;

2. We show that reliable self-verification of long deductive reasoning processes written in our Natural Program format can be achieved through step-by-step subprocesses that only cover necessary context and premises;

3. Experimentally, we demonstrate the superiority of our framework in improving the rigor, trustworthiness, and interpretability of LLM-generated reasoning steps and answers (Fig. 2).

## 2 Related work

**Reasoning with large language models.** Recent large language models (LLMs) [3; 8; 57; 47; 38; 18; 9; 37] have shown incredible ability in solving complex reasoning tasks. Instead of letting LLMs directly generate final answers as output, prior work have shown that by encouraging step-by-step reasoning through proper prompting, such as Chain-of-Thought (CoT) prompting  and many others [21; 59; 58; 44; 48; 60; 25; 54], LLMs exhibit significantly better performance across diverse reasoning tasks. To further improve the step-by-step reasoning process, some recent studies have investigated leveraging external solvers such as program interpreters [39; 5; 27], training and calling external reasoning modules , or performing explicit search to generate deductive steps [2; 46]. Parallel to these works, we do not rely on external modules and algorithms, and we directly leverage the in-context learning ability of LLMs to generate more precise and rigorous deductive reasonings.

**Large language models as verifiers.** Using language models to evaluate model generations has been a long standing idea [22; 36; 40; 4]. As LLMs exhibit impressive capabilities across diverse tasks, it becomes a natural idea to use LLMs as evaluation and verification tools. For example, [10; 11; 33] finetune LLMs to verify solutions and intermediate steps. LLMs aligned with RLHF [32; 31; 48] have also been employed to compare different model generations. In addition, recent works like [43; 52; 28; 6] leverage prompt designs to allow LLMs to self-verify, self-refine, and self-debug without the need for finetuning. However, these works do not focus on the rigor and trustworthiness of the deductive reasoning processes at every reasoning step. In this work, we propose a natural language-based deductive reasoning format that allows LLMs to self-verify _every_ intermediate step of a deductive reasoning process, thereby improving the rigor and trustfulness of reasoning.

[MISSING_PAGE_FAIL:4]

given a (premise, conclusion) pair, and we are interested in determining whether the conclusion follows from the premises. In the context of reasoning-based QA tasks, for each reasoning step \(s_{i}\), we define its _deductive validity_\(V(s_{i})\) as a binary variable. A reasoning step is **deductively valid** (\(V(s_{i})=1\)) if and only if \(s_{i}\) can be logically deduced from its corresponding premises \(p_{i}\), which consist of the context \(C\), the question \(Q\), and all the previous reasoning steps \(s_{j}(j<i)\). Then, we can also define the deductive validity for the entire reasoning chain \(S\) as \(V(S)=_{i=1}^{M}V(s_{i})\). Compared to evaluating answer correctness, which can be accomplished by simple functions such as exact string match, evaluating deductive validity is a lot more challenging. Thanks to the recent progress on LLMs, which demonstrate impressive in-context learning capabilities across diverse scenarios, we propose to use LLMs to examine reasoning chains and predict the deductive reasoning validity.

## 4 Deductively Verifiable Chain-of-Thought Reasoning

In this section, we introduce our specific approaches to performing deductive verification of reasoning chains. Specifically, we first introduce our motivation and method for decomposing a deductive verification process into a series of step-by-step processes, each only receiving contexts and premises that are necessary. Then, we propose **Natural Program**, a natural language-based deductive reasoning format, to facilitate local step-by-step verification. Finally, we show that by integrating deductive verification with unanimity-plurality voting, we can improve the trustworthiness of reasoning processes along with final answers. An overview of our approach is illustrated in Fig. 1 and Fig. 2.

### Decomposition of Deductive Verification Process

Given a reasoning chain \(S=(s_{1},s_{2},,s_{n})\), a straightforward idea to verify its deductive validity is to ask LLMs to examine the _entire_ reasoning chain at once. To assess the effectiveness of this approach, we conduct a preliminary experiment: for a dataset problem and its reasoning chain \(S\) generated by ChatGPT, we prompt ChatGPT with "Do you think the above reasoning process is correct? Let's think step by step" such that its outputs whether there exists any mistake among any reasoning step in \(S\). However, as demonstrated in Tab. 2, the verification accuracy is 50% for most datasets, and ChatGPT struggles at finding out mistaken reasonings. Notably, it persistently outputs "Correct" for most reasoning chain queries, regardless of their actual validity.

We conjecture that such phenomenon is caused by the abundance of irrelevant premises for each reasoning step. Recall that the premises \(p_{i}\) for a reasoning step \(s_{i}\) consist of the the question \(Q\), the question context \(C\), along with the prior reasoning steps \(s_{ j}=\{s_{j}:j<i\}\). For \(Q\) and \(C\), we can further extract and decompose \(Q C\) into a set of "question-related premises" \(QC=\{qc_{1},qc_{2},,qc_{m}\}\), where \(qc_{i}\) is a premise or condition inferred from \(Q C\). Then, it is often the case that most elements of \(p_{i}=QC s_{ j}\) are irrelevant to the validity of \(s_{i}\), leading to erroneous verifications from language models. A very recent work  also observes a similar phenomenon where LLMs are easily distracted by irrelevant context.

Hence, we propose a decomposition of the reasoning chain verification process into a series of step-by-step processes, where each step only considers the premises that are _necessary_. The overall validity

   Prompting & Reasoning Correctness & GSM8K & AQuA & MATH & AddSub & Date & Last Letters \\   & Correct & 98\% & 96\% & 100\% & 98\% & 98\% & 100\% \\  & Incorrect & 4\% & 6\% & 4\% & 2\% & 4\% & 4\% \\  & (Average) & 51\% & 51\% & 52\% & 50\% & 51\% & 52\% \\   & Correct & 98\% & 96\% & 100\% & 92\% & 100\% & 96\% \\  & Incorrect & 2\% & 4\% & 0\% & 6\% & 26\% & 6\% \\   & (Average) & 50\% & 50\% & 50\% & 49\% & 63\% & 51\% \\   

Table 2: Zero-shot and two-shot reasoning chain verification accuracy for GPT-3.5-turbo (ChatGPT), where an entire reasoning chain is verified at once. The two shot prompt we used is presented in Appendix D.1. To generate verification inputs, for each dataset, we perform Chain-of-Thought (CoT) prompting and randomly sample 50 reasoning chains that are valid and 50 reasoning chains that exhibit mistakes. We observe that when given an _entire_ reasoning process, where the deductive graphs for all reasoning steps are entangled together, it is challenging even for strong language models like ChatGPT to verify its validity.

of the reasoning chain, denoted as \(V(S)=_{i=1}^{M}V(s_{i})\), can be naturally decomposed into individual step validity \(V(s_{i})\). However, achieving such decomposition is highly challenging without imposing constraints on the format of reasoning chains. Additionally, for each \(s_{i} S\), we aim to ensure that it _explicitly_ lists the minimal subset of premises \(_{i} p_{i}\) required for deductive reasoning to avoid potential ambiguities during verification. This motivates us to introduce a natural-language-based deductive reasoning format in Section 4.2.

### Natural Program Deductive Reasoning Format

As previously mentioned in Sec. 4.1, we desire LLMs to output deductive reasoning processes that can be easily verified by themselves, specifically by listing out the minimal set of necessary premises \(p_{i}\) at each reasoning step \(s_{i}\). To accomplish its goal, we propose to leverage the power of natural language, which is capable of rigorously representing a large variety of reasoning processes and can be generated with minimal effort. In particular, we introduce **Natural Program**, a novel deductive reasoning format for LLMs. More formally, Natural Program consists of the following components:

* An instruction for models to extract question-related premises \(QC\). We use the following instruction: "First, let's write down all the statements and relationships in the question with labels".
* A numbered-list of question-related premises, each prefixed with "#{premise_number}".
* An instruction for models to generate the reasoning chain \(S\) based on the question-related premises \(QC\). We use the following instruction: "Next, let's answer the question step by step with reference to the question and reasoning process".
* A list of prefixed reasoning steps \(S_{i}\). The prefix has the following format: #{number} (by {list_of_premises_used}). Here "number" equals \(|QC|+i\), and "list_of_premises_used" consists of numbers from the smallest subset of premises among \(QC s_{ j}\) that are used for the deductive reasoning of \(s_{i}\). In addition, for the last reasoning step \(s_{m}\), we ensure that it (1) includes a special tag Final Step; (2) refers to the premise number of the target question to be answered; (3) explicitly gives the final answer to a question.

To encourage language models to reason in the Natural Program format, we have designed one-shot prompts for different datasets, which are shown Appendix D.2. Given that LLM's reasoning outputs follow the Natural Program format, we can then verify the deductive validity of a _single_ reasoning step \(s_{i}\) through an instruction that consists of (1) the full descriptions of premises used for the reasoning of \(s_{i}\); (2) the full description of \(s_{i}\); (3) an instruction for validity verification, such as "Double-check the reasoning process, let's analyze its correctness, and end with "yes" or "no"." Note that throughout this verification process, we only retain the minimal necessary premise and context for \(s_{i}\), thereby avoiding irrelevant context distraction and significantly improving the effectiveness of validation. Additionally, we employ a one-shot prompt for this verification process, which we find very helpful for improving the verification accuracy. The prompt is shown in Appendix D.3.

Figure 1 provides an overview of the complete Natural Program-based deductive reasoning and verification process. By using the Natural Program approach, we demonstrate that LLMs are capable of performing explicit, rigorous, and coherent deductive reasoning. Furthermore, Natural Program enables LLMs to self-verify their reasoning processes more effectively, enhancing the reliability and trustworthiness of the generated responses.

### Integrating Deductive Verification with Unanimity-Plurality Voting

Given that we can _effectively_ verify a deductive reasoning process, we can naturally integrate verification with LLM's sequence generation strategies to enhance the trustworthiness of both the intermediate reasoning steps and the final answers. In this work, we propose Unanimity-Plurality Voting, a 2-phase sequence generation strategy described as follows. Firstly, similar to prior work like , we sample \(k\) reasoning chain candidates along with their final answers. In the unanimity phase, we perform deductive validation on each reasoning chain. Recall that a chain \(S\) is valid (i.e., \(V(S)=1\)) if and only if all of its intermediate reasoning steps are valid (i.e., \( i,V(s_{i})=1\)). For _each_ intermediate reasoning step \(s_{i}\), we perform majority voting over \(k^{}\) sampled single-step validity predictions to determine its final validity \(V(s_{i})\). We then only retain the verified chain candidates \(\{S:V(S)=1\}\). In the plurality voting stage, we conduct a majority-based voting among the verified chain candidates to determine the final answer. This voting process ensures that the final answer is selected based on a consensus among the trustworthy reasoning chains.

## 5 Experiments

In this section, we perform evaluations to demonstrate the effectiveness of our Natural Program-based deductive reasoning verification approach over diverse reasoning datasets. Firstly, we show that our deductive verification process leads to substantial improvements in the rigor and reliability of reasoning chains. Subsequently, we will examine the impact of deductive verification on the accuracy of final answers. Our findings reveal that by adopting our Natural Program reasoning format without verification, we improve answer correctness on challenging benchmarks. Further applying deductive verification leads to slight reductions in final answer accuracy. One reason for this phenomenon is that the verification process effectively identifies and eliminates flawed reasoning chains that still produce correct answers.

### Experimental Setup

**Benchmarks.** We evaluate the deductive verification accuracy and the answer correctness of reasoning chains over a diverse set of reasoning tasks: arithmetic reasoning, symbol manipulation, and date understanding. For arithmetic reasoning, we utilize the following benchmarks: 1) AddSub ; 2) GSM8K ; 3) MATH ; 4) AQuA . Among these benchmarks, the AddSub and GSM8K datasets involve middle school-level multi-step calculations to arrive at a single number as the final answer. The MATH dataset presents more challenging problems that require expressing the answer as a mathematical expression in LaTeX format. These problems involve concepts from linear algebra, algebra, geometry, calculus, statistics, and number theory. AQuA also features similarly challenging problems, except that questions are in a multiple-choice format. For symbol manipulation, we use Last Letter Concatenation , where the model is tasked with concatenate the last letters of all the words provided in the question. For date understanding, we use the one from BIG-bench 

**Deductive verification evaluation setup.** For each of the above benchmarks, we select 100 reasoning chains, where 50 of them are deductively valid and 50 of them exhibit reasoning mistakes. The ground-truth deductive validity of each reasoning chain is determined by human annotators.

**Answer extraction.** To extract answers from reasoning solutions, we first perform text splitting based on answer prefix patterns such as "answer is" or "option is". Then, using problem type-specific regular expressions, we extract the final answer. To extract the validity results from deductive verification processes, we only keep the last sentence of model response. We then extract the validity answer with regular expressions to obtain attitude words, e.g., "yes" or "no", to determine the validity answer. Sometimes, language models may not provide a direct answer and instead output phrases like "not applicable" at the end of the response. In such cases, we consider the answer from the model as "yes". Please refer to Appendix C for more details.

**Model and Hyperparameters.** We conduct our main experiments with GPT-3.5-turbo (Chat-GPT) . We also present results for the LLama model-family ) in Appendix A, where we find the deductive verification accuracy to be worse than larger models even after finetuning. For ChatGPT, we use a generation temperature of \(T=0.7\). For Unanimity-Plurality Voting, we set \(k=10\) and \(k^{}=3\) by default. We use 1-shot prompting for both reasoning chain generation and deductive verification (except reasoning chain generation for the date understanding task where we use 2-shot). See Appendix D.2 and Appendix D.3 for more details.

### Comparison of Deductive Verification Accuracy

We compare the verification accuracy of reasoning chains using two methods: (1) verifying the entire reasoning chain at once (as described in Section 4.1) without utilizing the Natural Program, and (2) our Natural Program-based verification approach with step-by-step decomposition. The results, presented in Table 3, indicate that our approach achieves significantly higher reasoning verification accuracy across most datasets. It effectively identifies erroneous reasoning in faulty chains while maintaining a low rate of false positives for valid chains. However, we observe that our approach's effectiveness is limited on the "Last Letters" task. We hypothesize that this is due to the task's nature, where each subsequent reasoning step is conditioned on _all_ previous steps, presenting greater challenges for reasoning verification due to the increased dependency among premises.

### Impact of Natural Program and Deductive Verification on Final Answer Correctness

We then investigate the impact of our Natural Program reasoning format and our deductive verification process on final answer correctness. We conduct two experiments: (1) for each problem, we instruct language models to generate \(k=10\) reasoning chain candidates in the Natural Program (NP) format and perform simple majority voting on final answers, _without_ using deductive verification to filter out reasoning chain candidates; (2) applying our deductive verification approach to filter out reasoning chain candidates, and apply Unanimity-Plurality Voting (UPV) along the process to determine the final answer. As a reference, we also report the performance of Chain-of-Thought (CoT)  and Faithful CoT . For these baselines, we perform simple answer-based majority voting with \(k=10\) for fair comparison.

Results are presented in Tab. 4. While our major goal is to improve the trustworthiness and reliability of deductive reasoning, we find that prompting language models to reason in our Natural Program format achieves on-par or better final answer accuracy than baselines over many reasoning tasks. Upon further applying our deductive verification approach to filter out invalid reasoning chains, we observe a slight decrease in final answer accuracy. One major contributing factor to this decrease is the filtering out of reasoning chain candidates that provide correct answers but exhibit incorrect reasoning. We illustrate an example in Table 5, where ChatGPT generates the correct final answer but assigns incorrect premise numbers to support the first reasoning step. We note that in many such cases, our approach effectively identifies these reasoning errors, thereby enhancing the rigor and

    &  &  \\  Methods & GSM8K & AQuA & MATH\({}^{*}\) & AddSub & Date & Last Letters \\  CoT + Voting & **87.62\%** & 70.18\% & 35.93\% & 92.36\% & 69.97\% & 81.60\% \\ Faithful CoT + Voting & 75.80\% & 61.80\% & 31.78\% & 88.35\% & **73.50\%** & - \\  Ours (Natural Program (NP), No Verification) & 87.05\% & **70.34\%** & **36.75\%** & **93.67\%** & 72.49\% & **92.98\%** \\ Ours (NP + Deductive Verification + UPV) & 86.01\% & 69.49\% & 36.48\% & 93.54\% & 71.45\% & 92.60\% \\   

Table 4: Final answer accuracy comparison on GPT-3.5-turbo (ChatGPT). All approaches generate \(k=10\) reasoning chains for each problem before performing majority voting or reasoning chain filtering with our deductive verification approach.

   Verification Method & Reasoning Correctness & GSM8k & AQuA & MATH & AddSub & Date & Last Letters & Overall \\   & Correct & 98\% & 96\% & 100\% & 92\% & 100\% & 96\% & 97\% \\  & Incorrect & 2\% & 4\% & 0\% & 6\% & 26\% & 6\% & 7\% \\ Two-shot & (Average) & 50\% & 50\% & 50\% & 49\% & 63\% & 51\% & 52\% \\   & Correct & 84\% & 72\% & 70\% & 95\% & 90\% & 96\% & 85\% \\  & Incorrect & 84\% & 62\% & 76\% & 40\% & 56\% & 6\% & 54\% \\ One-shot & (Average) & **84\%** & **67\%** & **73\%** & **68\%** & **73\%** & 51\% & **69\%** \\   

Table 3: Comparison of deductive verification accuracy of reasoning chains for GPT-3.5-turbo (ChatGPT). We compare two approaches: (1) verifying entire reasoning chains generated by Chain-of-Thought prompting; (2) verifying reasoning chains generated in the Natural Program format with step-by-step decomposition. In the latter case, when we verify each reasoning step \(s_{i}\), we only keep the necessary subset of premises \(} p_{i}\). To calculate verification accuracy, for each dataset, we randomly sample 50 reasoning chains that are deductively valid and 50 reasoning steps exhibiting incorrect reasonings.

reliability of the language models' reasoning processes, albeit with a slight negative impact on the overall final answer correctness. Further discussions are presented in Appendix B.

### Ablation Study

In addition, we perform several ablation studies to gain further insights into the designs of our deductive verification approach. In Tab. 6, we compare two different approaches to verify a single reasoning step \(s_{i} S\) following our Natural Program format. The first approach utilizes all premises \(p_{i}=QC S_{ j}\) for verification regardless of their relevance to \(s_{i}\), potentially introducing irrelevant contexts. The second approach follows our design in Sec. 4.1 and only includes the necessary context and premises \(} p_{i}\). We observe that removing irrelevant premises significantly improves the reasoning chain verification accuracy on many datasets, highlighting the importance of this technique.

We also ablate on our Unanimity-Plurality Voting strategy by investigating the impact of different \(k^{}\). Recall that \(k^{}\) determines the number of votes to produce validity predictions of single-step reasoning. Results are shown in Tab. 7. We observe that increasing \(k^{}\) generally enhances reasoning validation accuracy, though we note that this is at the expense of more compute.

## 6 Limitations

While we have demonstrated the effectiveness of Natural Program-based deductive reasoning verification to enhance the trustworthiness and interpretability of reasoning steps and final answers, it is

   Premise Context & \# Shots & Reasoning Correctness & GSM8K & AQuA & MATH & AddSub & Date & Last Letters & Average \\   &  & Correct & 64\% & 54\% & 58\% & 95\% & 26\% & 96\% & 66\% \\  & & Wrong & 56\% & 68\% & 56\% & 24\% & 76\% & 5\% & 48\% \\  & & (Average) & 60\% & 61\% & 57\% & 60\% & 51\% & 51\% & 57\% \\   &  & Correct & 84\% & 78\% & 90\% & 96\% & 90\% & 12\% & 75\% \\  & & Wrong & 26\% & 12\% & 28\% & 20\% & 20\% & 80\% & 31\% \\  & & (Average) & 55\% & 45\% & 59\% & 58\% & 55\% & 46\% & 53\% \\   &  & Correct & 84\% & 72\% & 70\% & 95\% & 90\% & 96\% & 85\% \\  & & Wrong & 84\% & 62\% & 76\% & 40\% & 56\% & 6\% & 54\% \\   & & (Average) & **84\%** & **67\%** & **73\%** & **68\%** & **73\%** & 51\% & **69\%** \\   

Table 6: Ablation study on the impact of (1) premise context and (2) zero-shot vs. few-shot scenarios on deductive verification accuracy using our Natural Program-based approach with step-by-step reasoning chain decomposition. To verify each reasoning step \(s_{i}\), we either the full premises \(p_{i}=QC S_{ j}\), or use the minimal subset of premises \(} p_{i}\) necessary as outlined in Sec. 4.1 The one-shot prompt we used is shown in Appendix D.3. For each dataset, we randomly sample 50 reasoning chains that are deductively valid and 50 reasoning steps exhibiting incorrect reasonings.

  
**Question:** In March it rained 0.81 inches. It rained 0.35 inches less in April than in March. \\ How much did it rain in April? \\
**Ground Truth:** It rained 0.81 - 0.35 = 0.46 inches. \\ 
**GPT-3.5-turbo (ChatGPT):** \\
**Question-related premises:** \\
**\#1. It rained 0.81 inches in March. \\
**\#2. It rained 0.35 inches less in April than in March. \\
**\#3. How much did it rain in April? \\
**The first reasoning step:** \\
**\#4. (by \#2) Step 1: Calculate how much it rained in April by subtracting the difference of 0.35 inches from the amount in March. \\ Amount of rain in April: 0.81 - 0.35 = 0.46 inches \\   

Table 5: For this reasoning chain candidate of this GSM8k problem, ChatGPT generates the correct final answer but provides incorrect premise numbers for grounding the first reasoning step. In “Step 1”, the correct premise numbers should be #1 and #2. Our deductive reasoning verification approach effectively identifies these reasoning errors, enhancing the rigor and trustworthiness of the reasoning process. It is worth noting that removing a correct answer from the candidate reasoning chains has a slightly negative impact on the overall final answer correctness.

important to acknowledge that our approach has limitations. In this section, we analyze a common source of failure cases to gain deeper insights into the behaviors of our approach. The failure case, as shown in Tab. 8, involves the ambiguous interpretation of the term "pennies," which can be understood as either a type of coin or a unit of currency depending on the context. The ground truth answer interprets "pennies" as coins, while ChatGPT interprets it as a unit of currency. In this case, our deductive verification process is incapable of finding such misinterpretations. Contextual ambiguities like this are common in real-world scenarios, highlighting the current limitation of our approach.

## 7 Conclusion

In this paper, we aim to enable Large Language Models (LLMs) to perform explicit and rigorous deductive reasoning while ensuring the trustworthiness of their reasoning processes through self-verification. To this end, we have proposed a novel framework based on "Natural Program", a natural language-based deductive reasoning format that facilitates reasoning verification and can be easily generated through in-context learning. Within this framework, we decompose the verification process of complex reasoning chains into step-by-step subprocesses that focus solely on necessary context and premises, allowing us to significantly enhance the accuracy of verification. Additionally, we introduce a Unanimity-Plurality Voting strategy to further improve verification accuracy. Experimentally, we demonstrate the superiority of our framework in improving the rigor, trustworthiness, and interpretability of reasoning steps and answers.

**Broader Impact.** While our deductive verification approach can mitigate hallucinations and reasoning errors of Large Language Models (LLMs), it does not completely eliminate these phenomena. LLMs can still produce harmful and biased content, make incorrect claims, and produce wrongful advice. This issue becomes particularly significant when LLMs engage in complex reasoning chains, increasing the risk of misleading users. Consequently, it is still crucial for users to exercise great caution when interacting with, deploying, or developing LLM-based applications.

#5. (by #1) Step 1: Calculate the number of pennies Melanie had initially. \\ Number of pennies in 10 quarters: 10 * 25 = 250 \\ Number of pennies initially: 250 + 17 = 267 \\   

Table 8: An example question with ambiguous wordings. The term "pennies" in this question can be interpreted as either a type of coin or a unit of currency. In this particular question, "pennies" is treated as a type of coin. However, the initial reasoning step by ChatGPT mistakenly treats "pennies" as a unit of currency, resulting in the conversion of all Melanie’s money into "pennies" (highlighted in red). Consequently, all subsequent reasoning steps follow this flawed logic, leading to an incorrect reasoning trace. Our deductive verification is not yet able to detect such errors.