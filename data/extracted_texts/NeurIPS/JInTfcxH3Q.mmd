# PowerPM: Foundation Model for Power Systems

Shihao Tu

Zhejiang University

shihao.tu@zju.edu.cn

&Yupeng Zhang

Zhejiang University

yuppzhang@zju.edu.cn

&Jing Zhang

Renmin University of China

zhang-jing@ruc.edu.cn

&Zhendong Fu

Zhejiang University

zhendongfu@zju.edu.cn

&Yin Zhang

Zhejiang University

yinzh@zju.edu.cn

&Yang Yang

Zhejiang University

yangya@zju.edu.cn

These authors contributed equally to this work.Corresponding authors.

###### Abstract

The proliferation of abundant electricity time series (ETS) data presents numerous opportunities for various applications within power systems, including demand-side management, grid stability, and consumer behavior analysis. Deep learning models have advanced ETS modeling by effectively capturing sequence dependence. However, learning a generic representation of ETS data for various applications is challenging due to the inherently complex hierarchical structure of ETS data. Moreover, ETS data exhibits intricate temporal dependencies and is susceptible to the influence of exogenous variables. Furthermore, different instances exhibit diverse electricity consumption behavior. In this paper, we propose a foundation model PowerPM for ETS data, providing a large-scale, off-the-shelf model for power systems. PowerPM consists of a temporal encoder and a hierarchical encoder. The _temporal encoder_ captures temporal dependencies within ETS data, taking into account exogenous variables. The _hierarchical encoder_ models correlations between different levels of hierarchy. Furthermore, PowerPM leverages a novel self-supervised pre-training framework consisting of _masked ETS modeling_ and _dual-view contrastive learning_. This framework enables PowerPM to capture temporal dependency within ETS windows and aware the discrepancy across ETS windows, providing two different perspectives to learn generic representation. Our experiments span five real-world scenario datasets, including both private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Notably, when transferred to public datasets, PowerPM retains its edge, showcasing its remarkable generalization ability across various tasks and domains. Moreover, ablation studies and few-shot experiments further substantiate the effectiveness of our model.

## 1 Introduction

The volume of Electricity Time Series (ETS) data has recently increased rapidly due to the emergence of advanced power systems known as smart grids . This abundance of data has paved the way for diverse applications in power systems, including demand-side management , grid stability  and consumer behavior analysis , etc. Meanwhile, these applications have spawned various tasks, as shown in Fig. 1(d). These include load forecasting , clock anomaly detection , electricity theft  and and the detection of elderly individuals living alone .

As society progresses towards modernization, electricity consumption is rapidly increasing, presenting opportunities and challenges for the development and application of smart grids. On one hand, the substantial economic benefits that accompany this significant electricity usage are considerable. On the other hand, unreasonable electricity planning can have a detrimental impact on the environment. Therefore, given the large volume of data and the variety of tasks, there is an urgent need to study effective ETS data modeling methods for these tasks, so as to improve economic efficiency while adhering to low-carbon principles.

Recently, numerous research studies on pre-training approaches for ETS data have emerged. These approaches adopt the "pre-training then fine-tuning" paradigm to deal with the dilemma of limited annotation data, and the pre-trained model to easily adapt to new tasks, such as PatchTST , TS2Vec , CoST , etc. However, these pre-training methods only utilize small-scale of data with a small number of instances (e.g. users), resulting in poor performance on downstream tasks. As the same time, many researcher begin to apply Large Language Models (LLMs) to assist time series modeling by using pre-trained LLM to encode time series  or incorporating additional descriptions related to the time series [17; 20]. Nevertheless, these models have limited ability in the power system scenario due to insufficient pre-training data of power systems and the lack of sufficient domain-specific knowledge. Additionally, none of these models are tailored for the scenario of power systems, so they neglect the unique characteristics of ETS data. Consequently, there remains a significant research gap in existing power systems literature regarding the modeling of ETS data using a foundation model.

In our scenario, the ETS data contains numerous instances and naturally exhibits a complex hierarchy [41; 23]. As depicted in Fig. 1(a), a city ETS can be disaggregated into district ETS according to the administrative divisions, which can further be disaggregated into user ETS in this district. For the complex hierarchy of ETS data, modeling ETS data entails the consideration of several challenges:

**(1) Hierarchical Dependency Modeling.** The hierarchy of ETS data facilitates information interaction across different granularities. Fine-grained ETS provides detailed insights into individual electricity usage, while coarse-grained ETS for districts and cities captures broader factors and indicates overall trends. For example, user-level data reflects user-specific behaviors and city-level data encompasses demographics and policy effects [29; 35]. Integrating these levels of granularity to provide both macro and micro perspectives is a complex task that requires sophisticated modeling.

**(2) Temporal Dependencies within ETS Window.** An ETS window refers to a piece of electricity time series over a period of time. The temporal dependencies within an ETS window refer to the correlations and dependencies between observations at different timestamps. As shown in Fig. 1(b), the city-level ETS exhibits daily and weekly dependency. Moreover, the temporal dependencies are often influenced by exogenous variables, such as weather, temperature, and seasonal effects. Integrating these factors into the model is challenging because their impact may interact with the temporal dynamics in complex ways. Accurately capturing the temporal dependencies with the impact of exogenous variables is a key challenge in modeling ETS data.

**(3) Discrepancy across ETS Windows.** The patterns observed in ETS windows can vary significantly across different instances and different timestamps. For instance, as shown in Fig. 1(c), residential electricity consumption (_User A_) reaches its peak in the mornings and evenings, used for lighting, appliances, and heating. However, electricity usage typically declines during the day because residents

Figure 1: (a) The hierarchical structure of ETS data. (b) The temporal dependency within ETS data and the influence of exogenous variables. (c) Different electricity consumption behaviors exist across time and instances. (d) Various tasks in power systems.

are generally absent, being engaged in work or education activities outside the home. Moreover, industries (_User B_) have high power demand during specific daytime periods for machinery and production lines, with lower load requirements during nighttime and weekends. These variations in behavior highlight the challenge of achieving consistency across ETS windows in personalized modeling.

To address these challenges, we propose a foundation model for power systems named **P**ower **P**re-trained **M**odel (PowerPM), as illustrated in Figure 3. PowerPM contains about \(250\)M parameters and is pre-trained on large-scale hierarchical ETS data with \(987.42\)GB. Specifically, we employ the "pre-training then fine-tuning" paradigm to learn generic representations by pre-training on hierarchical ETS data and to unify various tasks by fine-tuning on downstream data. During pre-training stage, we propose a novel self-supervised pre-training framework consisting of _masked ETS modeling_ and _dual-view contrastive learning_, which enables PowerPM to capture temporal dependency within ETS windows and aware the discrepancy across ETS windows, so as to provide two different perspectives to learn universal representations. PowerPM mainly consists of two modules, namely, _temporal encoder_ and _hierarchical encoder_. The _temporal encoder_ employs Transformer encoders to capture the temporal dependency in ETS data, and incorporates exogenous variables to make the modeling process more robust. Moreover, to model hierarchical dependency, _hierarchical encoder_ utilizes R-GCN  to propagate information about the correlation between hierarchy. According to the message that passes through the hierarchies, the micro and macro information can effectively assist in modeling the ETS data. In summary, the main contributions of our work include:

1. We propose a foundation model for power systems named PowerPM, which is pre-trained on large-scale ETS data and provide an off-the-shelf model for power systems.
2. To the best of our knowledge, PowerPM is the first to date that considers temporal dependency and hierarchical dependency simultaneously. In addition, we present a novel self-supervised pre-training framework that combines masked ETS modeling and dual-view contrastive learning, enhancing the model's ability to learn temporal dependencies within ETS windows and aware the discrepancy across ETS windows.
3. Extensive experiments show that PowerPM generalizes well to \(44\) downstream tasks. Fig. 2 summarizes the results of all the downstream tasks, showing its great potential in ETS data modeling. Moreover, when transferred to the public dataset, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Further analysis illustrates the effectiveness of PowerPM as well.

## 2 Methodology

**Overview.** As shown in the middle part of Fig. 3: Firstly, the hierarchical graph \(\) is constructed according to the naturally existing hierarchical relationship of ETS data. The ETS windows in \(\) and its corresponding exogenous variables are denoted as \(\{_{i}\}_{i=1}^{N}\) and \(\{_{i}\}_{i=1}^{N}\), where \(N\) is the number of instances, \(_{i}^{T_{w}}\), \(_{i}^{T_{w} K}\), and each instance ETS window spans \(T_{w}\) time points starting at \(T_{a}\) and ending at \(T_{b}\). Each time point has \(K\) kinds of exogenous variables. Our objective is to perform

Figure 2: Performance comparison of our model and other baseline models on all downstream tasks in our scenario. Model performances are plotted on \(3\) radar subfigures for clarity with the same coordinate range.

pre-training on an encoder \(f()\) to encode each window into a latent representation \(_{i}^{N d}\), where \(d\) indicates the dimension of the latent representation. More specific, PowerPM consists of an exogenous variable enhanced temporal encoder \(f_{T}()\) and a hierarchical encoder \(f_{H}()\), with the process: \(_{i}=f(_{i},_{i},)=f_{H}(f_{T}(_{i}, _{i}),)\). In addition, a novel self-supervised strategy which combines masked ETS modeling and dual-view contrastive learning is used for pre-training PowerPM. Next, we will detail the techniques in both model architecture and pre-training strategy.

### Hierarchical Graph Construction

The data of cities, districts, and users in ETS data naturally form a hierarchical relationship, based on which we can construct a hierarchical graph. However, the imbalance in the number of users and districts means there will be multitude of edges between user nodes and district nodes, which significantly increases the complexity of graph modeling. To address this, we employ a clustering strategy to create intermediary nodes, which is a common approach to implement graph sparsification  and a user group policy in the power systems [36; 44; 12]. As depicted in Fig. 3 (c), we use clustering method to categorize users into several clusters, the detailed process can be found in App. B.1. The cities are bidirectionally connected to districts, and these user clusters are also bidirectionally connected to districts but are unidirectionally connected to districts. By sparsifying the edges, we enhance the efficiency of graph modeling. Mathematically, we represent the hierarchy as a directed graph \(=(,,)\), where \(\) is the set of nodes, each node corresponds to an instance, \(\) is the set of directed edges, and \(\) is the set of type of edges (e.g. user cluster \(\) district, district \(\) user, etc.).

### Temporal Encoder with Exogenous Variables

**Patching.** In the \(\), each node's feature \(_{i}\) is a window of ETS data corresponding to instance \(i\). Due to the semantic sparsity of time series, we patch each window \(_{i}\) into \(N_{p}\) segments, each of length \(P\), resulting in \(_{i}^{N_{p} P}\), where \(N_{p}=-P}{S}+1\), and this method proved its validity in many works [21; 17; 20]. Subsequently, a linear projection is applied to each segment to obtain the window representation \(_{i}^{N_{p} d}\).

**Exogenous Variables Encoding.** To efficiently interact with exogenous variables, we model these variables using learnable embeddings \(^{(_{k=0}^{K-1}M_{k}) d}\), where \(K\) indicates the number of exogenous variables (e.g. weather type and temperature), \(M_{k}\) represents the number of value types of the \(k\)-th exogenous variable (e.g. sunny and rainy in weather type variable). The exogenous variables \(_{i}^{(k)}^{N_{p} P}\) corresponding to \(_{i}\) of the \(k\)-th exogenous variable are used to obtain representations of the exogenous variables from \(\), indexing out \(_{i}^{(k)}^{N_{p} d}\), as illustrated in

Figure 3: The pre-training framework of PowerPM. For simplicity, we take the windows of each instance in the same time range for illustration, and the window process at other times is the same.

Fig. 3 (b). Subsequently, we derive a representation \(_{i}^{N_{p} d}\) that considers the window's exogenous variable influence: \(_{i}=_{i}+_{k=0}^{K-1}_{i}^{(k)}\).

**Temporal Encoder.** To model the complex temporal dependency and interaction with exogenous variables, we use the vanilla Transformer encoder  to encode \(_{i}\), resulting in an augmented temporal representation \(}_{i}^{N_{p} d}\).

### Hierarchical Encoder

To model the complex correlation across different hierarchies, we employ Graph Neural Networks (GNNs). GNNs have recently become increasingly popular for modeling relationships within time series data, which enhances temporal representation [7; 26; 40]. In addition, considering that the correlation relationships of different edges are distinct, we adopt R-GCN  to integrate information across various hierarchies and instances, as depicted in Fig 3 (a). Specifically, we use R-GCN to update the representation \(}\) by considering its neighboring nodes in \(\), with the final node representation denoted as \(_{i}^{N_{p} d}\). Moreover, we use \(_{i}\) to perform self-supervised pre-training.

### Self-supervised Pre-training

#### 2.4.1 Masked ETS Modeling

To model temporal dependency within an ETS window, we have adopted the widely utilized masked reconstruction strategy. Nevertheless, existing random masking methods may face a significant challenge: they reconstruct the missing part based on the known surrounding part [21; 8], without considering the prediction of future parts relying solely on the past part. This approach not only diminishes the difficulty of the pre-training stage but also lacks consistency across pre-training task and forecasting task.

To address this issue, we propose a novel masking approach that combines random and casual masking, as shown in Fig. 3 (d) (_left_). Specifically, we randomly select one of the masking approaches for a given patched window \(_{i}\), resulting in **masked \(_{i}\)**. This approach not only retains the benefits of the random masking strategy but also ensures that the model learns to predict future parts based solely on past information, thereby it can more comprehensively capture the temporal dependencies within a window. Mathematically, this can be formulated as: **masked \(_{i}=_{r}(_{i})&<0.5\\ _{c}(_{i})&\)**, where \(_{r}\) and \(_{c}\) denote the random and causal masking, respectively, and \(\) is a uniformly distributed variable. Specifically, after the \(_{i}\) is inputted into PowerPM for masked ETS modeling, we will obtain a reconstructed \(}_{i}\). The corresponding reconstruction loss is: \(_{MSE}=_{i=1}^{N}(_{i}-}_{i})^{2}\).

#### 2.4.2 Dual-view Contrastive Learning

The objective of contrastive learning is to learn representations by bringing positive pairs closer and pushing negative pairs farther apart in the latent space [5; 6]. Motivated by this, to make PowerPM aware of the discrepancy across ETS windows, we employ dual-view contrastive learning (DVCL) to discern subtle differences in electricity usage behavior.

**Positive and Negative Sample Pairs.** These pairs are determined from two views: one is _temporal view_, which is based on the time difference between the two windows. Another is the _instance view_, which depends on whether two windows belong to the same instance. For the same instance, the closer the time difference between two windows, the closer their representations are likely to be. This idea is also presented in [31; 42]. Conversely, windows from different instances or the same instance with a larger time difference are likely to have more distinct representations. Overall, we consider adjacent windows from the same instance as positive samples, while windows from different instances or non-adjacent windows from the same instance are negative samples. As depicted in Fig. 3 (d) (_right_), for the district node \(V\) in \(\), the original start timestamp about this window is \(T_{a}\). After shifting several time steps \(\) on, we obtain another window \(V^{+}\) starting at \(T_{a}+\), which serves as a positive sample. Meanwhile, we select windows from other nodes in \(\), such as city \(P\), starting at \(T_{a}\), as well as windows from the same node \(V\) but starting at \(T_{c}\), where \(|T_{c}-T_{a}|\). These windows serve as instance and temporal negative samples, respectively, and are denoted as \(P^{-}\) and \(V^{-}\).

Mathematically, given an ETS window \(_{i}\), we obtain a positive sample \(_{i}^{+}\) by shifting it by \(\) time steps. The other samples in this batch serve as negative samples, totaling \(B-1\) negative samples, where \(B\) is the batch size during pre-training. The DVCL loss is: \(_{DVCL}=-_{i=1}^{N}(f(_ {i}),f(_{i}^{+}))/)}{_{m=1}^{B}( {sim}(f(_{i}),f(_{m}))/)}\), where \(\) is the boolean vector to select the negative pairs and \(()\) is cosine similarity function.

## 3 Experiments

### Experiment Setup

**Pre-training Dataset.** PowerPM is pre-trained on a mount of ETS data, a private dataset from the real scenario1. This pre-training dataset encompasses ETS data of cities, districts, and users, covering over \(3\) years records. The ETS data is collected at a frequency of one data point every \(15\) minutes. More details are in App. A

Downstream Dataset.To evaluate the performance of PowerPM, we conduct comprehensive experiments on eleven downstream private and public datasets. And seven private datasets are also collected from real scenario. These datasets have different labels for different tasks. Among them, the solar generation dataset does not have a hierarchical structure due to its particularity. Four public datasets are obtained from CSISO 2, ISONE3, NYISO 4, and PJM 5, and they all exhibit a hierarchical structure. Further details can be found in Appendix A.

Settings.For the model configurations, the temporal encoder contains a \(26\)-layer Transformer encoder with model dimension \(1024\), inner dimension (FFN) \(2048\) and \(16\) attention heads, and the hierarchical encoder contains \(2\)-layer R-GCN. PowerPM contains about 250M parameters. During pre-training, the \(40\%\) segments in each input window are masked in the form of random mask and casual mask, the user cluster numbers is set to \(12\). See further details in App. B.1

Baselines.We compare with \(8\) state-of-the-art methods: Large Language Model (LLM) enhanced models: GPT4TS , Time-LLM , UniTime ; pre-train models: PatchTST , CoST , TS2Vec ; supervised models: DLinear , TimesNet . More implementation details are provided in App. B.2.

Evaluation Metrics.For forecasting and imputation tasks, we use mean squared error (MSE): \(_{i=1}^{n}(-})^{2}\) and mean absolute error (MAE): \(_{i=1}^{n}|-}|\) as the evaluation metric. For classification tasks, we use accuracy as the metric. The metric of the anomaly detection task includes precision, recall, \(F0.5\), and \(F1\) scores. The \(F\) is a metric defined as the weighted harmonic mean of precision and recall, with the following equation: \(F=) precision recall}{^{2} precision +recall}\). We use \(F0.5\) for anomaly detection, since precision is more important than recall in power systems scenario .

### Downstream Tasks

Demand-side Management.Demand-side management aims to optimize and balance the power system by managing and adjusting the electricity demand of end-users. We develop tasks to predict load at different levels (such as cities and users) and tasks to forecast solar generation. With demand-side management, we can better plan and schedule power resources, improve energy efficiency, promote the development of renewable energy, and achieve sustainable energy management.

Grid Stability.To ensure the stability of the power grid, we have implemented a series of tasks, including electricity theft detection, load imputation, and clock anomaly detection, to address the impact of potential appliance failures within the grid and external electricity theft on the quality of power data and grid operations. Internal appliance malfunctions within the grid such as clock anomalies or the inability to record electricity usage accurately decrease the accuracy of power data, making it challenging for power dispatch and management. Additionally, external electricity theft 

[MISSING_PAGE_FAIL:7]

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

Therefore, the utilization of a larger model with higher capacity and large ETS data enables better generalization across a wide range of downstream tasks.

## 4 Related Work

Self-supervised Pre-training Model.Large-scale model based on self-supervised pre-training has become more significant in both industrial and academic domains due to the versatility and impressive performance. It initially developed in the fields of computer vision  and natural language processing [8; 11]. Self-supervised pre-training in time series is typically classified into two paradigms: contrastive learning and mask modeling. The objective of contrastive learning is to learn representation by pushing positive pairs closer and pull negative pairs away in the embedding space . TS2Vec  proposes contextual consistency for positive pair selection. Then, CoST  extracts the trend and seasonal feature representations, and takes advantage of both time and frequency domain contrastive loss to encourage discriminative seasonal representation. And TF-C  applies time-frequency consistency for embedding time-based and frequency-based neighbors. In mask modeling, to extract the contextual semantic information, PatchTST  masks at the series-level.

Supervised Learning Model.Since the self-attention mechanism in Transformer  showed the great ability to seize global dependencies between input and output, recently many variants have been proposed to tackle power system tasks. LogTrans , Informer  reduce the complexity by optimizing the vanilla self-attention mechanism. Autoformer  leverages auto-correlation mechanism to achieve series-wise representation aggregation. FEDformer  incorporates frequency-domain information to enhances prediction performance while reducing complexity to linear levels. Besides, DLinear  questions the effectiveness of transformers as it outperforms most Transformer-based SOTAs, with a simple linear model. TimesNet  has treated time series as a \(2D\) signal and utilized a convolution-based inception net backbone to function as a comprehensive time series model.

Large Language Models Enhanced Model.Recently, the advancement of Large Language Models (LLMs) has opened up new horizons in time series modeling. Many LLMs, such as llama , GPT-3 , GPT-4 , ChatGLM  have the capability to capture complex dependencies and understand varied textual data, yielding sensible reasonable generation results. Therefore, many researchers begin to apply LLMs to assist time series modeling. Time-LLM  and TEXT  employ reprogrammed input time series with text prototype embedding and incorporate textual prompts for time series. GPT4TS  and UniTime  apply fine-tuning to selected components of LLMs to improve performance in time series analysis tasks. TEMPO  incorporates the decomposition of time series and retrieval-based prompt design for non-stationary time series data.

However, despite numerous methods for self-supervised and supervised time series, the research on foundation models specifically designed for power systems remains relatively sparse. And LLMs are limited in power systems scenario, lacking enough textual descriptions for domain knowledge.

## 5 Conclusion

This paper introduces the PowerPM, a foundational model designed to model ETS data within power systems. PowerPM consists of a _temporal encoder_ and a _hierarchical encoder_. Furthermore, PowerPM leverages a novel self-supervised pre-training framework consisting of _masked ETS modeling_ and _dual-view contrastive learning_. Our experiments involve two real-world scenario datasets, comprising private and public data. Through pre-training on massive ETS data, PowerPM achieves SOTA performance on diverse downstream tasks within the private dataset. Moreover, when transferred to the public dataset, PowerPM maintains its superiority, showcasing its remarkable generalization ability across various tasks and domains. Further analysis shows the effectiveness of a foundation model in the field of power system. Also, PowerPM is an off-the-shelf model with its code and weights. This feature greatly mitigates the challenges associated with sample and label efficiency, allowing it to be directly integrated into various power system applications.