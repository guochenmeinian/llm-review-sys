# MARPLE: A Benchmark for Long-Horizon Inference

Emily Jin & Zhuoyi Huang & Jan-Philipp Franken & Weiyu Liu

Hannah Cha & Erik Brockbank & Sarah Wu & Ruohan Zhang

Jiajun Wu & Tobias Gerstenberg

Stanford University

Equal contribution.

###### Abstract

Reconstructing past events requires reasoning across long time horizons. To figure out what happened, humans draw on prior knowledge about the world and human behavior and integrate insights from various sources of evidence including visual, language, and auditory cues. We introduce MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence. Our benchmark features agents interacting with simulated households, supporting vision, language, and auditory stimuli, as well as procedurally generated environments and agent behaviors. Inspired by classic "whodunit" stories, we ask AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened. The goal is to correctly identify the culprit as early as possible. Our findings show that human participants outperform both traditional Monte Carlo simulation methods and an LLM baseline (GPT-4) on this task. Compared to humans, traditional inference models are less robust and performant, while GPT-4 has difficulty comprehending environmental changes. We analyze factors influencing inference performance and ablate different modes of evidence, finding that all modes are valuable for performance. Overall, our experiments demonstrate that the long-horizon, multimodal inference tasks in our benchmark present a challenge to current models. Project website: https://marple-benchmark.github.io/.

## 1 Introduction

Long-horizon inferences are critical for solving "whodunit" problems in our every day lives. For example, we may wonder, "Who left the fridge open?", "Who spilled the food?", or "Who turned on the light?" To find out what happened and who did it, humans rely on an intuitive understanding of the physical world and how people interact with their environment to pursue their goals. Importantly, humans readily combine evidence across sensory modalities to figure out what happened . Long-horizon inferences are critical for solving "whodunit" problems in our every day lives. For example, we may wonder, "Who left the fridge open?", "Who spilled the food?", or "Who turned on the light?" To find out what happened and who did it, humans rely on an intuitive understanding of the physical world and how people interact with their environment to pursue their goals. Importantly, humans readily combine evidence across sensory modalities to figure out what happened .

Developing AI models capable of performing such long-horizon reasoning and event reconstruction from multimodal information is critical for bridging the gap between human and machine intelligence. While the field of AI has developed increasingly powerful, general-purpose inference models , the ability of these models to solve long-horizon inference problems, such as reasoning about "whodunit" scenarios, remains unclear. Existing benchmarks for evaluating inference capabilities focus on problems that require reasoning over short time horizons about physical events  and over agent behaviors . In addition, they focus on visual stimuli, with only recent ones supporting language and audio . However, these benchmarks lack coverage of long-horizon,multimodal inference in complex, everyday scenarios, a setting that is crucial for evaluating human-like reasoning abilities.

We propose MARPLE (in reference to Agatha Christie's Miss Marple) - a benchmark for long-horizon inference based on multimodal evidence. The main goal of MARPLE is to evaluate a model's ability to answer "whodunit"-style questions in daily household scenarios, such as "Who turned on the laundry?", by leveraging visual, text, and/or audio evidence. This inference problem challenges one to select the correct agent from two potential suspects, based on prior knowledge about agent behaviors and the state of the environment, as shown in Fig. 1.

In addition, we provide diverse training and inference data, along with well-defined evaluation metrics for our inference tasks. To systematically generate data, MARPLE builds upon the Mini-BEHAVIOR simulator , which simulates semantically rich daily activities in procedurally generated household Gridworld environments. We extend Mini-BEHAVIOR to support autonomous agents using hierarchical planners, enabling them to interact with the environment and generate multimodal evidence (vision, language, and audio). As a Gridworld, MARPLE facilitates the development of models focused on understanding high-level agent behavior, with the benefits of fast prototyping and training.

Using MARPLE, we benchmark two baselines against human performance. The **first baseline** uses traditional Monte Carlo tree search with learned agent models, while the **second baseline** uses a language model (GPT-4). To provide a comparison standard, we also conduct a behavioral study with **human participants**. Our findings reveal that that both baselines fall short in long-horizon, multimodal inference tasks compared to humans. The first baseline struggles to accurately predict future states and generalize to new environments, while the second one has difficulty reasoning about changes in the environment. Overall, we make the following key **contributions**:

1. We introduce a Gridworld simulator to procedurally generate household environments and diverse agent behaviors that yield multimodal evidence (visual, auditory, and language);
2. Using our simulator, we propose a set of long-horizon inference tasks for a) machine learning research on event reconstruction and multimodal reasoning and b) cognitive science

Figure 1: Illustrative example of an inference task in MARPLE: a “whodunit”-inspired benchmark for long-horizon inference. Given a query state change, the challenge is to decide which agent caused the change by leveraging visual, text, and/or audio evidence of both agents A and B up to some timestep \(t\). The inference accuracy, probability of choosing the correct agent, is calculated at every timestep and used to evaluate performance.

research on the processes underlying human inference in complex scenarios. We also provide pre-collected datasets and evaluation metrics;
3. Lastly, we benchmark the performance of machine learning methods (Monte Carlo simulation and LLM) and human experts on the inference tasks.

## 2 Related Work: Cognition-Inspired AI Inference Benchmarks

Understanding how humans reason about causal relationships remains an active research area in cognitive science [31; 34]. To model this process, prior works have developed various frameworks, including the force dynamics model , mental models [16; 23], causal models [17; 35], and counterfactual simulation models [12; 13]. These frameworks provide insights into the cognitive mechanisms that underlie human inference abilities.

Inspired by human cognition, many machine learning benchmarks focus on problems that require the ability to reason about agents' interactions with their environment. These benchmarks emphasize different types of inference problems, such as reasoning about physical events [1; 3; 7; 15; 27; 45], agent behaviors [10; 33], and multi-agent social behaviors . While most of these benchmarks rely on visual stimuli, some recent ones support multimodal stimuli [19; 21], integrating both audio and vision . Furthermore, several of these benchmarks provide human-annotated judgments and performance baselines [27; 28; 33], which are helpful for assessing the performance gap between humans and machines.

To address the inference problems in MARPLE, inference models must leverage knowledge about both the agents and the world. When these models are unknown, they can be learned from training data. The agent model allows the inference model to predict agent goals or actions, which can be learned through imitation learning [20; 32]. Meanwhile, the world model helps predict the consequences of taking an action from a given state. Recently, significant advancements in AI inference abilities have been made by machine learning-based models, such as large language models (LLMs) [36; 44], especially when combined with traditional search methods . Our work presents and analyzes the performance of both a traditional search-based approach and an LLM-based method.

Despite progress in existing benchmarks, they primarily focus on short-term reasoning or single-modality stimuli, limiting their ability to evaluate models' performance in more complex, real-world scenarios. Our benchmark, MARPLE, addresses these shortcomings by providing a comprehensive framework for evaluating whether recent inference methods can solve long-horizon, multimodal inference tasks. In doing so, MARPLE aims to support the development of more robust and human-like AI reasoning capabilities.

   Benchmark & Time (seconds) & Video & Text & Audio & Ecological & High-Level & Physical & Controlled & Cognition \\  & - & - & ✓ & - & - & - & - & - & ✓ & - \\ MovieQA & 202.7 & ✓ & ✓ & - & ✓ & ✓ & - & - & - \\ TGIF-QA & 1.6 & ✓ & ✓ & - & ✓ & - & - & - & - \\ TVQA+ & 7.2 & ✓ & ✓ & - & ✓ & - & - & - & - \\ AGQA & 30 & ✓ & ✓ & - & ✓ & ✓ & - & - & - \\ MultiPLY & - & - & ✓ & ✓ & ✓ & - & - & ✓ & - \\  IntPhys & 7 & ✓ & - & - & - & - & ✓ & ✓ & - \\ Galileo & - & ✓ & - & - & - & - & ✓ & ✓ & - \\ CATER & 12.5 & ✓ & - & - & - & ✓ & ✓ & ✓ & - \\ CoPhy & 6 & ✓ & - & - & - & - & ✓ & ✓ & ✓ & - \\ CRAFT & 10 & ✓ & - & - & - & - & ✓ & ✓ & - \\ CLEVRET & 5 & ✓ & - & - & - & - & ✓ & ✓ & - \\ Comphy & 5 & ✓ & - & - & - & - & ✓ & ✓ & - \\  CLEVRET-Humans & 5 & ✓ & - & - & - & - & ✓ & ✓ & ✓ \\ AGENT & 15.4 & ✓ & - & - & - & - & - & ✓ & ✓ \\ BIB & 55 & ✓ & - & - & - & ✓ & - & ✓ & ✓ \\ PHASE & 17.5 & ✓ & - & - & ✓ & ✓ & ✓ & ✓ & ✓ \\ MMToM-QA & 63.4 & ✓ & ✓ & - & ✓ & ✓ & - & ✓ & ✓ \\ 
**MARPLE** & **52.5** & ✓ & ✓ & ✓ & ✓ & ✓ & - & ✓ & ✓ \\   

Table 1: Comparing MARPLE with other visual reasoning, causal reasoning, and cognition-inspired benchmarks. MARPLE is long-horizon, high-level, and with multimodal (vision, text, audio) support. _Time_ refers to average stimuli length, _ecological_ refers to object diversity, and _controlled generation_ refers to annotated data generation.

## 3 MAPLE Benchmark

**Overview.** As shown in Table 1, MAPLE focuses on inference problems in long-horizon settings with multimodal support. It is highly configurable, with support for procedural generation of rich agent behaviors and diverse environment states at an abstract, semantic level. Specifically, MAPLE provides a variety of _inference scenarios_ for "whodunit"-type questions, in which two agents, A and B, each perform a _mission_: a common household activity that humans perform in real life. To carry out a mission, an agent interacts with the environment, causing changes in the world and leaving evidence of its activity. A "whodunit" question is constructed by selecting a unique state that only appears in one agent's trajectory. For example, consider an inference scenario where agents A and B have completed the missions do laundry and get snack, respectively. A state that is unique to agent A is "laundry machine is on," so we pose the following question: "Who turned on the laundry?" To answer "whodunit" questions, models must leverage evidence in the form of multimodal observations from each agent's activity history. An example of the inference process is shown in Figure 1.

**Problem Formulation.** We formalize the inference problem using a Partially Observable Markov Decision Process (POMDP), denoted by the tuple \(,,,,,,\), where \(\) is the state space, \(\) is the action space, \(\) is the reward function, \(\) is the transition function, \(\) is a set of observations, \(\) is the observation function, and \(\) is the discount factor. The state at time step \(t\) is \(s_{t}\), and visual, auditory, and language observations are denoted by \(o_{t}=\{o_{t}^{t},o_{t}^{A},o_{t}^{L}\}\). The action space \(\) consists of low-level agent actions, and an agent's action trajectory is determined by its mission. A mission is decomposed into a sequence of mid-level subgoals \(g\), which are further decomposed into low-level actions. Each subgoal relies on the completion of past ones and is necessary for completing future ones, creating strong multi-step dependencies between the actions. We represent agent \(i\)'s behavior using a policy \(^{t}:\) that maps observations \(\) to a probability distribution over actions in \(\), while the transition function \(:\) determines the effects of agent actions.

In each scenario, the objective is to infer whether agent A or B is more likely to have caused a particular query state (e.g., "laundry is on"). We formulate this as predicting the probability \(P(s_{T}|^{i},o_{0:})\) for agent \(i\) at any intermediate time step \(\), where \(s_{T}\) is the state in query, and \(o_{0:}\) are observations until time step \(\). Different instantiations of \(o_{0:}\) affect the horizon, and hence inference difficulty. For example, when \(=T\), inference is trivial.

Solving an inference scenario requires knowledge about the world model \((s^{}|s,a)\), observation model \((o|s)\), and policy \(^{i}(a|o)\) for both agents. The true agent policies are unknown to the inference model and need to be learned in a training stage. A training dataset of previous agent behaviors \(_{i}=\{_{1},_{2},...,_{n}\}\) is collected, where each trajectory \(\) is a sequence of agent actions \(\{a_{0},a_{1},..,a_{T}\}\) paired with observations \(\{o_{0},o_{1},..,o_{T}\}\). We assume that agents can perform multiple missions, with their preferences for the missions represented as a prior distribution over all possible ones. For example, an agent might prefer to get snack with probability \(0.8\), pickup the plant with probability \(0.2\), and all other missions with probability 0. When simulating the agent's trajectories, the missions are sampled according to their mission preferences.

Figure 2: **MAPLE Household Simulator (backend)**. The simulator contains a list of pre-defined Missions, each mission consists of a list of Subgoals, and each subgoal is a representation of a Action-State_change-Object-Furniture-Room combination. Given the mission definition and corresponding environment configuration file, we can procedurally generate the environment.

**Evaluation.** In our problem setting, inference ability is measured by the probability of correctly choosing the agent responsible for the query state. We are interested in how much evidence is needed to make the correct inference: stronger models require less evidence and achieve high inference accuracy at earlier time points. Other factors that affect performance include inference scenario difficulty, environment complexity, agent behavior similarities, and inference horizon.

## 4 MAPLE Household Simulator

To generate our benchmark, we introduce the MAPLE Household Simulator, shown in Figure 2. The simulator supports a wide variety of complex scenarios and generates diverse data. It consists of two components: a multimodal environment simulator and a hierarchical agent planner. Our simulation environment is built on top of Mini-BEHAVIOR , which supports 20 household activities, fast simulation, and procedural generation of room layouts. By abstracting away low-level physical details, MAPLE enables researchers to efficiently prototype and evaluate their high-level, long-horizon inference models. Additional details about the simulator and computational resources are in Appendix D. Our simulator extends Mini-BEHAVIOR to support multimodal stimuli, procedural generation of diverse agent behaviors, and a human experiment user interface (UI).

**Multimodal Environment Simulator.** Our simulator additionally supports language and auditory stimuli. The _language_ modality is a natural language description of the subgoal that the agent intends to perform next. For example, the subgoal ToggleOn(light) is described as _"I am going to toggle on the light."_ This modality offers insight into the agent's future intentions without revealing its mission until the ultimate subgoal. The challenge lies in effectively leveraging this information to understand how these intentions relate to the final state. We carefully constructed scenarios where the language modality helps to varying degrees. For example, in the scenario "Who picked up the snack?", language evidence reveals early on that agent A intends to "open refrigerator" while agent B intends to "pickup towel from closet." From this, a strong inference model should be able to reason that agent A is more likely to pick up the snack. On the other hand, consider the scenario "Who toggle on the laundry", where both agents share many subgoals. Agent A performs: "pickup clothes from bed", "open laundry", "drop clothes", "close laundry", "toggle-on laundry", while Agent B performs: "open closet", "pickup clothes from closet", "close closet", "open laundry", "drop clothes", "close laundry". In this case, language evidence only helps distinguish between agents at the end.

The audio modality is generated by mapping each possible agent action to a corresponding sound. This mapping is not one-to-one; for example, all navigation actions (left, right, forward) share the same _step_ sound. Such audio evidence reveals partial information about the agent's low-level actions, which can be useful for resolving state uncertainty in inferential settings [14; 24; 40]. For example, consider the scenario "Who turned on the laundry?", where visual evidence reveals that Agent A is in the same room as the laundry, just 5 steps away, while Agent B is in a bedroom 20 steps away with the door closed. Based solely on this, one might infer that Agent A was the likely culprit due to proximity. However, if audio evidence reveals a long sequence of steps or a door closing, it might suggest that Agent B was responsible. Leveraging audio to infer what happened presents a challenging research direction. For details on language and auditory simulation generation, see Appendix B.

**Procedural Generation of Agent Behaviors.** To generate agent behaviors, we use a hierarchical planner with high-, mid-, and low-level components, as illustrated in Figure 3. The high-level planner first selects a _mission_ based on the agent's mission preferences, and the mid-level planner breaks the mission down into a sequence of subgoals. Each subgoal is defined by an _action_, _object_, and _state_. The low-level planner further decomposes each subgoal into a sequence of atomic actions to perform, which includes actions for navigation (turn left, turn right, and move forward) and the action specified by the subgoal itself. In particular, the low-level planner uses the A-star algorithm  to plan the shortest path to navigate to the subgoal position, perform the subgoal _action_ on the _object_, and ultimately produce the desired _state_. When multiple

Figure 3: A hierarchical planner for procedural generation of agent behaviors. A high-level planner samples a mission, a finite state machine breaks it into subgoals, and a low-level planner determines actions.

optimal paths exist, the planner randomly selects one to introduce variability. This approach avoids unnecessary actions or random walks, ensuring that every action in the trajectory directly contributes to completing the mission. Our planner is able to generate large amounts of diverse, long-horizon agent trajectories based on the specified mission, subgoals, room layouts, and initial positions.

**Human Experiment User Interface.** Mini-BEHAVIOR's visualization is suitable for machine learning research but not human studies. Hence, we develop a more intuitive, aesthetically pleasing interface, as shown in Appendix K. This extension allows us to collect human data to establish performance baselines, as well as support future cognitive science experiments using MARPLE.

**Inference Scenarios and Dataset.** With these new features, we define the MARPLE Benchmark with ten diverse, long-horizon missions and provide both training and testing data. We construct various inference scenarios by combining missions and assigning mission preferences to each agent. In experiments, we demonstrate the simplest case by only having A perform one mission and B another. We pair up all 10 missions to define 5 distinct _inference scenarios_ with a query state selected to be a meaningful subgoal unique to one agent, as shown in Table A.1. These 5 scenarios offer a manageable representation of the diversity and complexity offered by pairing missions. Details on the inference scenarios and selection process are provided in Appendix A.

For each mission, we provide a test dataset with 500 diverse agent trajectories, generated in 10 environments featuring different room layouts and object placements. We also provide two training datasets with 5000 trajectories each: one with 500 agent trajectories in each of the 10 test environments, and the other with one trajectory per 5000 procedurally generated environments. The test environments are unseen by models trained on the second dataset, enabling evaluation of generalization capabilities. These datasets offer diverse scenarios for training and evaluating inference models.

## 5 Inference Methods and Baselines

### Simulation with Learned Agent Models

Our first inference method (Appendix E) uses Monte Carlo Tree Search (MCTS)  with learned agent policy models. At inference time, this method performs Monte Carlo rollouts starting from time \(\), assuming that it has access to the ground truth world model (provided by the simulator). An agent-specific policy for agent \(i\), \(^{i}:\), is first learned through imitation learning from a dataset of past agent behaviors. We perform \(m\) Monte Carlo rollouts for each agent \(i\) starting from the current state \(s_{}\) and observation \(o_{}\), and the model predicts agent action \(a_{}\) using the learned policy model. Then, the predicted action is passed to the simulator to query for \(s_{+1}\) and \(o_{+1}\), and the model predicts the next action. The probability of reaching the query state \(s_{T}\), given by \(P(s_{T}|^{i},o^{i}_{0:})\), corresponds to the fraction of the \(m\) sampled rollouts that reach \(s_{T}\). Assuming Boltzmann rationality , normalized predictions are obtained by applying a softmax function to the probability for each agent. For example, for agent A, the prediction is (\(=5\) is the temperature parameter): \(P(A)=( P(s_{T}|^{A},o^{A}_{0:}))/( P(s_ {T}|^{A},o^{A}_{0:}))+( P(s_{T}|^{B},o^{B}_{0: }))\). Now, we discuss four variants of this baseline, each of which uses different types of observations.

**Vision-Only Model.** The first variant learns to predict the next low-level action \(a_{t+1}\) given the current visual observation \(o^{V}_{t}\). It uses a vision transformer  as an encoder and a policy head that outputs a probability distribution over all possible actions \(P(a|o^{V}_{t})\). The network is trained using supervised learning, i.e., through behavioral cloning .

**Audio-Augmented Model.** Our second implementation leverages both visual \(o^{V}_{t}\) and audio \(o^{A}_{t}\) observations. Audio information is used here in a limited setting to improve the prediction accuracy of the first action in the rollout, as it reveals partial information about the agent's next low-level action. We first obtain a predicted action distribution from the vision-only model, and then leverage audio evidence to refine the distribution. We then obtain the probability of the next action being an action \(a\), conditioned on the visual and audio observations, by using Bayes' rule: \(P(a|o^{V}_{t},o^{A}_{t}) P(o^{A}_{t}|a)P(a|o^{V}_{t})\), where the probability \(P(a|o^{V}_{t})\) is predicted by the vision-only model, and \(P(o^{A}_{t}|a)\) is computed using a mapping from the action to the audio observation that is given.

**Language-Conditioned Model.** The third variant uses language observations \(o^{L}_{t}\), which reveal information about the subgoal that the agent is aiming to achieve at time \(t\). Intuitively, the intended subgoal reveals future information that will improve low-level action prediction accuracy. At time \(t\), the language-conditioned model predicts the next low-level action \(a_{t}\) by conditioning on both the visual observation \(o^{V}_{t}\) and the subgoal revealed by the language observation \(o^{L}_{t}\).

**Audio-Augmented Language-Conditioned Model.** The final variant uses observations from all three modalities - vision, language, and audio. At test time \(t\), this variant uses the language-conditioned model to predict the next action \(a_{t}\), conditioned on both the visual \(o_{t}^{V}\) and language observation \(o_{t}^{L}\). Audio evidence \(o_{t}^{A}\) is then leveraged to refine the distribution over possible actions.

### Additional Baselines

**LLM.** For our second class of baselines, we ask state-of-the-art large language models to predict which agent is more likely to have caused the query state, given visual observations of both agents at two consecutive timesteps, \(o_{-1}^{V}\) and \(o_{}^{V}\). We benchmark GPT-4-0613 with a standard zero-shot "let's think step-by-step" prompt [39; 42] as our primary LLM baseline. In addition, we evaluate the performance of top open-source models, Llama-3.1-8B-Instruct  and Qwen2-7B-Instruct , chosen due to their large context length.

To perform the inference task, these LLMs must reason about changes in consecutive states and consider how the agent may reach the query state \(s_{T}\). Both the evidence and query states are provided to the model using a standard scene graph representation , containing a set of nodes and directed edges. Each node represents an agent or object, along with the states of that entity (e.g., a drawer is open). The directed edges represent physical relations between entities, e.g., "onTop" (object-object relation) and "inRoom" (object-room relation). See Appendix G for a simplified prompt. For a more comprehensive analysis, we also benchmark GPT-4 with in-context learning on select inference scenarios. We modify our zero-shot prompt and include examples from two other trajectories. Each example contains the inference answer and scene graphs of the current, previous, and query states of both agents at the same time step.

**Human Baseline.** As a third baseline, we run an experiment with two human experts. Each participant is provided with a habituation phase, in which they are familiarized with MARPLE domain knowledge, the inference setup, and a few examples of agent trajectories. During experiments, participants answer the inference question, given side-by-side visual observations of agent trajectories, presented one step at a time from \(t=0\) to \(\) (as in Figure K.1). This allows participants to build an incremental understanding of agent trajectories and compare agent behaviors within the scenario.

## 6 Experiments and Results

### Benchmarking Model Performance in Long-Horizon Inference Scenarios

For each inference method and baseline, we run experiments on all five inference scenarios shown in Table A.1. We test on 10 randomly generated environments of each inference scenario, resulting in 50 total trials (see Appendix D for more details). For each trial, we ask the model to answer the inference question and obtain its inference accuracy given evidence at various time steps, namely

Figure 4: Performance for each baseline across scenarios. Results are included for the simulation baseline trained both in-distribution and out-of-distribution (ood). Inference scenarios are presented in order of increasing difficulty from left to right, top to bottom. Error bands correspond to 95% CI intervals across tested trajectories.

\(=0,=T/10,...,=T\). The inference problem becomes easier at later time steps, as more evidence is revealed, and the inference horizon decreases. Thus, we expect accuracy to increase as \(\) increases. We are especially interested in how much evidence is required to choose the correct agent.

For our MCTS baseline, we focus on two variants: vision-only and audio-augmented language-conditioned. In this setup, each agent always performs one mission, and the agent models are trained on a dataset of agent trajectories for that mission. The dataset contains 500 trajectories in each of the 10 environments seen at test time. The number of rollouts is set to be \(m=100\). For our second baseline, we use GPT-4-0613 at temperature \(T=0.5\) using \(n=10\) completions for each API call.

**Main Results.** Our key results are summarized in Figure 4. Across all five inference scenarios, the accuracies of all baselines increase over time and eventually converge at the end of the trajectory (except GPT-4, as discussed below). Our evaluation, however, is centered on how early the methods are able to make the correct inference, rather than on convergence itself. With this in mind, we see that MARPLE presents a challenging benchmark for all baselines. Overall, human participants provide a strong upper bound on performance, even without extensive prior knowledge about the agents' preferences and past behaviors. Humans consistently outperform all models, achieving higher accuracies with less evidence and demonstrating stronger robustness.

**Analysis of Simulation Methods.** When contrasting simulation methods (vision-only and vision+audio+language) with GPT-4, we observe that simulation-based models generally achieve higher accuracy and always converge to \(1.0\) by the end of the trajectory. This highlights the benefit of explicitly modeling agent behaviors and performing step-by-step simulations. As a concrete example, we examine an instance of the scenario: "Who picked up the plant?" Evidence shown \(50\%\) into the trajectory reveals that the two agents are in the same state - next to the turned-on light - as shown in Figure 5. In this case, GPT-4 doesn't make the correct inference, as it only considers the evidence at the current and last time steps. Meanwhile, the simulation baseline achieves a 0.9 accuracy. The state \(\) is a meaningful one that always occurs before Pickup(plant), and the simulation baseline leverages its knowledge of agent behaviors to successfully estimate future states.

**Analysis of LLM Performance.** While GPT-4 performs competitively on some inference scenarios, GPT-4 fails to converge on two in particular: "Who turned on the shower?" and "Who turned on the laundry?". In Appendix I, we provide additional results of GPT-4 with in-context learning (ICL) applied to these two scenarios where it does not converge. We find that although ICL improves GPT-4's performance, it still struggles to converge. An analysis of GPT-4's chain-of-thought reasoning reveals that the model was biased toward changes in agent states, such as position, direction, or whether the agent was carrying an object. We speculate that this prevented GPT-4 from converging for these two tasks because their query states were only reflected as a change in the environment state and _not_ the agent state. By contrast, in the other three scenarios, the agent was holding an object in the query state, making it easier for GPT-4 to infer the correct answer. Examples of zero-shot and ICL reasoning mistakes are provided in Appendix J.

Additionally, we provide the results of the open-source LLMs Llama-3.1-8B-Instruct and Qwen2-7B-Instruct in Appendix H. Both demonstrate lower perform than GPT-4 and exhibit similar inconsistencies in performance, including difficulties in comprehending changes in the environment and ultimately failing to converge.

Despite the abilities of these LLMs to perform strong general reasoning, their failure modes reveal important opportunities for future work that better leverage in-context examples  or additional scaffolds  to study language models on our benchmark.

Figure 5: Example rollouts performed by our simulation model, starting from the initial state to possible future states. For agent A, this rollout reaches the inference state: Pickup(plant).

**Analysis of Human Performance.** Humans consistently outperform the other baselines, on average reaching 0.8 accuracy given only \(48\%\) of the evidence. Even without significant training, humans require \(10\%\) and \(47\%\) less evidence than the best MCTS variant in-distribution and GPT-4 (Table 2).

### Benchmarking Generalization Capabilities of Simulation Models

We run additional experiments on all five inference scenarios to evaluate the generalization capabilities of the simulation approach. We train models under two settings: one with trajectories in the same 10 environments as the test set, and the other using procedurally generated environments and tested in 10 unseen environments. While the models perform well in distribution, they struggle to generalize to novel environments (Table 2). Even the vision + audio + language variant, the strongest MCTS method, suffers a significant performance drop in unseen environments (Figure 4). This is primarily because the learned agent model does not generalize well to novel environments, leading to decreased accuracy in action prediction and rollouts. In sharp contrast, humans achieve strong performance even without prior training. As shown in Table 2, the performance gap between humans and the best simulation method increases from \(10\%\) to \(33\%\) less evidence out-of-distribution, highlighting significant room for improvement in building robust and generalizable inference models.

### Benchmarking in Multimodal Settings

We now study how incorporating multimodal observations can improve the simulation model's performance. We conduct experiments on the four variants of the simulation baseline: vision-only, audio-augmented, language-conditioned, and audio-augmented and language-conditioned. The results for "Who turned on the shower?" are shown in Figure 6. While language seems more valuable than audio in our setting, the baseline using all three modalities consistently outperforms the others. This suggests that audio and language provide different signals and are both beneficial.

**Effect of Audio Evidence.** In all settings, audio evidence slightly improves performance over the vision-only model, as correctly predicting the current action results in a more accurate distribution of the rollouts. This demonstrates the benefit of including audio evidence, but note that the benefits are limited under this setup as we only leverage one timestep of audio evidence for one action prediction.

**Effect of Language Evidence.** We find that the language-conditioned model significantly outperforms other baselines and stays consistent even when others' performances decrease. As expected, knowing the subgoal leads to more accurate action prediction. When evaluated on the inference trajectories, the language-conditioned policy achieves 0.92 accuracy, as compared to 0.86 for the vision-only policy. This advantage is critical for boosting performance in long-horizon rollouts due to compounding errors and is even more salient under challenging inference settings, as discussed next.

### Additional Benchmarking Experiments

In contrast to our primary experiments, where we assume that each agent is dedicated to a single mission, this time, we allow agents to undertake both their own mission and the other's. We vary agent preferences to be 1.0, 0.8, and 0.6 for their own mission and 0.0, 0.2, and 0.4 for the other, respectively. We use the inference scenario where the agents perform feed dog and do laundry due to the substantial differences between the two missions. The distinct subgoals of the two agents

    & Human &  Vision + Audio \\ + Language \\  &  Vision + \\ Language \\  & 
 Vision + \\ Audio \\  & Vision-Only & LLM \\  In-Distribution \(\) & 0.48 & 0.58 & 0.64 & 0.80 & 0.85 & 0.95 \\ Out-of-Distribution \(\) & 0.48 & 0.81 & 0.85 & 0.91 & 0.92 & 0.95 \\   

Table 2: Evidence needed for the baselines to achieve a 0.8 inference accuracy, quantified by the fraction of trajectories shown. Humans consistently make more accurate predictions earlier, particularly out-of-distribution.

Figure 6: Performance for all variants of the simulation baseline, for one inference scenario: “Who turned on the shower?”. The error bands correspond to 95% CI intervals across test trajectories.

result in divergent agent behaviors when each has a 1.0 preference for their primary mission. As agent preferences converge - such as 0.6 for their own mission and 0.4 for the other - agent behaviors become increasingly similar, thereby increasing inference difficulty.

**Effect of Agent Preferences.** As agent preferences converge and agent behaviors become more similar, we see that performance worsens for the vision-only and audio-augmented models. When agents have a preference of 1.0 for their primary missions, both models reach 0.6 inference accuracy when observing around \(40\%\) of the trajectory. When the primary mission preferences are 0.6 though, model performance decreases. The audio-augmented and vision-only models require evidence up to \(70\%\) and \(85\%\) of the whole trajectory, respectively, to reach the same accuracy of 0.6.

## 7 Limitations and Conclusion

**Limitations.** While our benchmark is well-suited for studying high-level, commonsense reasoning and inference, it has several limitations. First, our simulation environment is a GridWorld, which lacks physical realism and is therefor not suitable for low-level physical reasoning tasks. Second, our language and audio stimuli are limited as they are generated from a defined library and mapping. In the future, we plan to enhance our simulator with free-form natural language descriptions and realistic audio renderings to create a more comprehensive and realistic testbed. Lastly, there are some limitations of our problem setup. For example, we focus on "whodunit" scenarios where the selected state is unique to one agent's trajectory, simplifying the inference problem. Future work could explore scenarios where the query state could potentially be caused by either agent. Furthermore, although our current setup involves only two agents, our simulator is capable of supporting multiple agents acting in parallel. However, it does not yet support agent interactions, which would introduce additional complexity to the inference process. Despite these limitations, our current setup remains challenging, as it requires models to understand diverse agent behaviors and generalize across various environments, making it a strong foundation for benchmarking inference methods.

**Conclusion.** We introduced MARPLE, a novel benchmark for evaluating long-horizon, multimodal inference capabilities. We find that current AI models, including Monte Carlo tree search and LLM methods fall short of humans in leveraging multimodal stimuli and performing long-horizon inference. We hope that MARPLE facilitates further AI and cognitive science research to bridge the gap between artificial and human cognitive abilities in complex, real-world inference scenarios.