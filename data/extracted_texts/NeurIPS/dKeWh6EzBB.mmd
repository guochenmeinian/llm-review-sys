# SwiFT: Swin 4D fMRI Transformer

Peter Yongho Kim

Seoul National University

Junbeom Kwon

Seoul National University

Sunghwan Joo

SungKyunKwan University

Sangyoon Bae

Seoul National University

&Donggyu Lee

SungKyunKwan University

&Yoonho Jung

Seoul National University

&Shinjae Yoo

Brookhaven National Lab

&Jiok Cha

Seoul National University

&Taesup Moon

Seoul National University

Equal ContributionCo-corresponding author

###### Abstract

Modeling spatiotemporal brain dynamics from high-dimensional data, such as functional Magnetic Resonance Imaging (fMRI), is a formidable task in neuroscience. Existing approaches for fMRI analysis utilize hand-crafted features, but the process of feature extraction risks losing essential information in fMRI scans. To address this challenge, we present SwiFT (**Swin** 4D **fMRI**Transformer), a Swin Transformer architecture that can learn brain dynamics directly from fMRI volumes in a memory and computation-efficient manner. SwiFT achieves this by implementing a 4D window multi-head self-attention mechanism and absolute positional embeddings. We evaluate SwiFT using multiple large-scale resting-state fMRI datasets, including the Human Connectome Project (HCP), Adolescent Brain Cognitive Development (ABCD), and UK Biobank (UKB) datasets, to predict sex, age, and cognitive intelligence. Our experimental outcomes reveal that SwiFT consistently outperforms recent state-of-the-art models. Furthermore, by leveraging its end-to-end learning capability, we show that contrastive loss-based self-supervised pre-training of SwiFT can enhance performance on downstream tasks. Additionally, we employ an explainable AI method to identify the brain regions associated with sex classification. To our knowledge, SwiFT is the first Swin Transformer architecture to process dimensional spatiotemporal brain functional data in an end-to-end fashion. Our work holds substantial potential in facilitating scalable learning of functional brain imaging in neuroscience research by reducing the hurdles associated with applying Transformer models to high-dimensional fMRI. Project page: [https://github.com/Transconnectome/SwiFT](https://github.com/Transconnectome/SwiFT)

## 1 Introduction

The human brain is a dynamic system characterized as an extensive (feedback) network generating complex spatiotemporal dynamics of its activity. Analyzing such dynamics and linking them to normative adaptive cognition and behaviors, as well as their maladaptive forms in a disease condition is of paramount importance in neuroscience and medicine. However, the fields have been hampered by a lack of predictive power owing to the gap between the complexity of the brain network and the contrasting simplicity of brain imaging analytics . Hence, developing an accurate predictive model using high-dimensional brain imaging data and learning rich representations of brain function will help close this gap and advance toward precision neuroscience.

Among many brain imaging modalities, functional Magnetic Resonance Imaging (fMRI) provides a unique opportunity to model the spatiotemporal patterns of brain electrochemical activity. Functional MRI captures a temporal sequence of 3D images (a stack of 2D slices) of Blood Oxygenation Level Dependent (BOLD) signals with time resolution ranging from 0.5 to 3 seconds, rendering in 4D data. It has accelerated the discovery of detailed functional anatomy of the human brain and significantly contributed to the understanding of brain diseases, such as Alzheimer's disease and psychiatric disorders. To better process the fMRI data, statistical methods were conventionally used to estimate the spatiotemporal patterns related to cognition [2; 3] and brain diseases [4; 5; 6; 7]. More recently, deep neural networks (DNN) have been applied to fMRI to investigate the nonlinear relationship of brain dynamics with human cognition and behaviors [8; 9; 10; 11; 12].

Regarding the DNN-based approaches, researchers have broadly pursued two distinct lines of work. The first approach is the so-called _ROI-based method_, in which the high-dimensional fMRI data (with around 300,000 voxels) is clustered into the temporal sequence of hundreds of pre-defined brain regions (ROIs) using anatomical segmentation  or statistical clustering  before learning a predictive DNN model. Despite the computational efficiency, these pre-processing approaches may be prone to losing information important to capture subtle variability across the individual brains . The second DNN-based approach is the _two-step method_, in which the raw fMRI data is used as input, and specialized architectures for spatial and temporal domains are separately used. Namely, for learning spatial features, convolutional neural networks (CNNs) are used, and for temporal, long short-term memory (LSTM)  or Transformers [17; 18] are used. This separation of DNN architecture necessarily results in two-step learning for better computational and memory efficiency. However, it could also limit the capability of capturing comprehensive information among brain regions across the temporal dimension. Therefore, a critical unresolved issue is whether an efficient, end-to-end DNN that utilizes raw 4D fMRI input can be formulated to better model and learn the brain dynamics compared to previous approaches.

To that end, we propose **Swin** 4D **fMRI** Transformer **(SwiFT)**, a 4D extension of the Swin Transformer  architecture, which can jointly learn the spatiotemporal representations of the brain's intrinsic activity directly from high-dimensional fMRI in an end-to-end fashion. The main gist of our method is to employ the 4D local window attention structure, which makes SwiFT readily applicable to process large-scale, high-dimensional 4D data with low computational complexity. We note that while 3D variants of Swin Transformers have been proposed before [20; 21; 22; 23] to take video or medical image inputs, to the best of our knowledge, this is the first work to extend Swin Transformer to take 4D data input and to apply it to the fMRI data.

Our experimental results show that the end-to-end learning capability of SwiFT unlocks its potential to effectively learn complex spatiotemporal patterns in fMRI. Specifically, we evaluate SwiFT's performance on three representative fMRI benchmarks: the Human Connectome Project (HCP) , the Adolescent Brain Cognitive Development (ABCD) , and the UK Biobank (UKB) [26; 27]. Across various classification and regression tasks, including sex classification and age/intelligence prediction, SwiFT significantly outperforms the recent baselines mentioned above. Furthermore, we also demonstrate that applying the widely successful "pre-train and fine-tune" framework to SwiFT would be feasible. We believe this capability has the potential to empower researchers to construct large-scale foundation models for fMRI, akin to those utilized in several other application domains. Finally, to provide a comprehensive analysis, we present the interpretation results using IG-SQ for SwiFT's predictions and conduct ablation studies to substantiate our modeling choices.

## 2 Related Work

ROI-based methodsTo analyze the brain network, researchers typically reduce the sequences of brain volumes into low-dimensional multivariate time series by aggregating voxel intensities in specific regions of interest (ROI) based on standardized brain atlases, considering pairwise correlations between the time series of each ROI as functional connectivity [28; 29]. The majority of the DNNs, such as graph neural networks (GNN), were designed to treat the brain network as a graph, considering each ROI as node and pairwise correlations between the ROIs as edges. For instance, BrainNetCNN, consisting of multiple graph convolutional filters, was proposed to model various levels of topological interactions in structural  and functional connectivity . Kan et al.  proposed a Transformer-based model that learns the individual connectivity strengths between each ROI. Namely, they apply an orthonormal clustering readout operation for functional connectivity to locate functional clusters related to specific human behaviors, and those clusters serve as informative embeddings for predicting psychiatric and biological outcomes. Some recent studies proposed methods to capture spatiotemporal dynamics directly from extracted fMRI time series, utilizing Transformer by separating spatial and temporal attention units  and focusing on local representations with fused window multi-head self-attention (FW-MSA) .

Two-step methods for 4D fMRI inputsExisting DNNs that take raw 4D fMRI as input typically process spatial and temporal information separately. Li et al.  integrated 3D convolutional neural networks (CNNs) to extract spatial embeddings in each 3D volume of a 4D fMRI and then feed the spatial embeddings to an LSTM for temporal encoding. Transformer Framework for fMRI (TFF) by Malkiel et al.  replaces the LSTM with a Transformer to extract temporal features and propose reconstruction-based pre-training steps. To learn spatial features before the downstream task, TFF concatenates decoder layers after the Transformer and minimizes three reconstruction-based losses; L1, perceptual, and intensity losses. Nguyen et al.  suggested a pre-training method to predict the types of cognitive tasks performed during an fMRI scanning. A 3D CNN encoder is then trained to predict the target variable from fMRI volumes. Furthermore, the pre-trained encoder learns temporal features by attaching multi-head self-attention layers. The aforementioned models may have issues of unstable training of spatiotemporal data, which involves multiple training steps and large memory usage. These limitations may result in sub-optimal model computation and learning capability.

Transformers for vision tasksFollowing the success of Transformers in natural language processing [33; 34], many works applied the Transformer architecture in vision tasks. One of the major challenges for such an extension is the computation complexity scaling quadratically with respect to the number of tokens. Namely, as images have a much larger number of tokens (pixels), rendering the direct pixel-to-token use of Transformers infeasible in most cases. Dosovitskiy et al.  tackled this issue with ViT, converting image patches into tokens rather than treating each individual pixel as a token, but it still did not resolve the quadratic scaling of the self-attention layers. Liu et al.  suggested the Swin Transformer, a model that reduces the computational complexity to be linear in the number of tokens by running self-attention only within a local window instead of running it globally. Along with the local windowed attention, Swin Transformer also introduces shifting windows to add cross-window connections and patch merging (downsampling) steps to produce hierarchical representations. Swin UNETR  extended the Swin Transformer to handle 3D images and demonstrated its utility by coupling a Swin Transformer encoder with CNN-based decoders. VAT , a 4D Convolutional Swin Transformer, extended the Swin Transformer model to accept a 4D correlation map of two CNN-extracted 2D image features, utilizing the model for cost aggregation. While these works show that the Swin Transformer model can be extended to handle more spatial dimensions, Video Swin Transformer  shows that the Swin Transformer model can also be extended to the temporal dimension by processing 3D video inputs. Overall, these works present the feasibility of applying Swin Transformers to higher spatiotemporal dimensions, but to the best of our knowledge, such a method has yet to be applied to 4D functional brain imaging.

## 3 Main Method

### Swin 4D fMRI Transformer (SwiFT)

Overall architectureIn line with the recent studies [23; 20], which introduce 3D extensions to enhance the capabilities of the Swin Transformer , we propose an advancement in the architecture to add an additional temporal dimension, thereby enabling its application to 4D data.

The overall architecture of our model is depicted in Figure 0(a). The SwiFT architecture utilized in our study consists of four distinct stages. Each stage is constructed through the implementation of patch merging, with linear embedding employed in the case of Stage 1. Additionally, positional embedding is incorporated, and multiple (Swin) Transformer blocks are applied repeatedly within the stages. The model processes an input fMRI data of size \(T H W D 1\), which consists of a length \(T\) sequence of fMRI volumes (\(H W D\)) with a single channel. During the initial patch partitioning step, the input fMRI data is partitioned into \(T\) patches with \(P^{3}\) voxels. In this study, \(H\), \(W\), and \(D\) are 96, and the initial patch size \(P\) is 6. During the linear embedding process, patches with size \(P^{3}\) are transformed into \(C\)-dimensional tokens. This transformation effectively maps the spatially neighboring pixels within a patch onto a token.

Next, following an absolute positional embedding layer, multiple layers of 4D Swin Transformer blocks are applied on the embedded patches, forming Stage 1 of SwiFT. Starting from Stage 2, a patch merging layer at the beginning of each stage reduces the number of tokens by merging 8 spatially neighboring patches. After the patch merging layer, an absolute positional embedding layer followed by multiple layers of 4D Swin Transformer blocks is applied, forming Stage 2 and onward. In the final stage, Stage 4, the 4D Swin Transformer blocks are replaced by global attention Transformer blocks which carry out global attention instead of local window attention. This computationally expensive global attention is made possible by the significant reduction in the number of tokens achieved through the patch merging steps executed in the preceding stages. Global attention Transformer blocks allow each token to globally attend to all other tokens rather than being restricted to the tokens within the local window.

Patch mergingFollowing prior works , the patch merging step is only performed for the three spatial dimensions (\(H,W,D\)) and not for the temporal dimension (\(T\)), thereby merging a group of \(8=2 2 2\) neighboring patches into a single patch for each time frame. During the patch merging operation, the spatial dimensions are reduced by half, and the channel size (\(\)) is doubled as compensation. Thus, in Figure 0(a), the numbers \(P_{2},P_{3},C_{2},\) and \(C_{3}\) are \(12,24,2C\), and \(4C\), respectively.

As a general example, consider a tensor with arbitrary dimensions \(T H^{} W^{} D^{} C^{}\) before passing through the patch merging layer. During patch merging, this tensor is reshaped into a new tensor with dimensions \(T}{2}}{2}}{ 2} 8C^{}\), where \(2 2 2\) spatially neighboring patches are concatenated along the channel dimension. Then, each \(8C^{}\) channel in the resulting tensor is projected onto a \(2C^{}\) dimensional space by applying a single fully connected layer, resulting \(T}{2}}{2}}{ 2} 2C^{}\) in total. The process of patch merging facilitates the hierarchical feature-extraction structure of SwiFT and reduces the computational complexity of the subsequent layers. We clarify that while the patch merging is operated only on the spatial dimensions, the temporal information is still well-incorporated via the windowed attention.

Figure 1: Figures depicting the structure of SwiFT and its components.

4D window multi-head self-attentionThe core of the Swin Transformer model is the window multi-head self-attention (W-MSA) layer, which allows the model to process a large number of tokens by limiting self-attention only within a local window. In SwiFT, the 3D window mechanism is extended to 4D windows; given \(T H^{} W^{} D^{}\) input tokens, the tokens are partitioned by a window of size \(P M M M\), resulting in \(}{M} }{M}}{M}\) non-overlapping windows.

However, simply stacking multiple window self-attention layers would be undesirable since there would be no crosstalk across different windows. To that end, a shifted window multi-head self-attention (SW-MSA) layer enables cross-window connections. Namely, we extend the 3D shifted window mechanism to 4D shifted windows as well; in \(P M M M\) windows obtained from the W-MSA layer, we shift the windows of the successive layer by \((,,,)\) tokens.

The detailed operations of our 4D W-MSA and SW-MSA are shown in Figure 0(c). In this example, the applied size of input tokens and the windows are \(T H^{} W^{} D^{}=4 8 8 8\) and \(P M M M=2 4 4 4\), respectively. Then, by following the window partitioning methods described above, the numbers of grouped windows in W-MSA and SW-MSA become \(2 2 2 2=16\) and \(3 3 3 3=81\), respectively. Such separately applied window self-attention is essential in effectively extracting spatiotemporal feature representation from the 4D fMRI data. Although the number of windows increases in SW-MSA, the actual computation cost remains similar by leveraging the cyclic-shifting batch computation proposed in .

Combining the W-MSA layer and the SW-MSA layer, two successive 4D Swin Transformer blocks, as shown in Figure 0(b), are computed as the following:

\[^{l}=4((z^{l-1}))+z^{l-1}, z^{l }=((^{l}))+^{l}\] \[^{l+1}=4((z^{l}))+z^{l},\ z^{l +1}=((^{l+1}))+^{l+1},\]

in which 4D(S)W-MSA, LN, and MLP denote the 4D (Shifted) Window Multi-head Self-Attention, Layer Normalization, and Multi-Layer Perceptron module, respectively. Moreover, \(^{l}\) and \(z^{l}\) denote the output features of the 4D(S)W-MSA module and the following MLP module for block \(l\), respectively.

4D absolute positional embeddingEven though previous models utilize relative position biases to encode positional information [19; 23], we have opted instead for an absolute position embedding scheme for SwiFT. While absolute positional embeddings are more computationally expensive for low-dimensional data , since we are dealing with much larger scale 4D data, absolute positional embeddings become more cost-effective than relative positional bias. To that end, we add a learnable embedding at the beginning of each stage of the Transformer right after the patch merging step. In line with , we separately add positional embeddings for the spatial and temporal dimensions. Specifically, given an input tensor with dimensions of \(T H^{} W^{} D^{} C^{}\), we define spatial and temporal positional embedding tensors with dimensions of \(1 H^{} W^{} D^{} C^{}\) and \(T 1 1 1 C^{}\), respectively. These tensors are then added to the input tensor using broadcasting. The effect of this switch is investigated in Appendix C.1, and overall absolute positional embedding seems to be the superior method of choice.

### Self-supervised Pre-training

Our proposed end-to-end model structure allows efficient self-supervised pre-training of SwiFT, which can then be fine-tuned for specific tasks. This unique capability sets our method apart from other approaches in Section 2 relying on ROI-based brain data or a two-step learning approach for 4D fMRI. We achieve this by using two different contrastive loss-based pre-training objectives adapted from . Figure 2 depicts the positive and negative pairs for the two loss functions, where the InfoNCE  loss of the pairs is calculated for the final loss function.

Instance contrastive lossTo allow the model to distinguish fMRI scans that come from _different subjects_, the instance contrastive loss (\(_{IC}\)) is a type of contrastive-based loss function that considers a representation to be positive if two distinct fMRI sub-sequences come from the same subject and negative if they come from different subjects. The feature representation passes through three layers: SwiFT, global average pooling, and a multi-layer perceptron (MLP) head. To clearly define this loss function, we denote the feature representation as \(f_{i,p}\), where \(i\{1,...,B\}\) refers to the subject index for a given batch size \(B\), and \(p\) refers to the fMRI sub-sequence index (either 1 or 2). Since we are sampling two sub-sequences for each of the \(B\) subjects, this amounts to a total of \(2B\) representations. For each subject \(i\), the anchor, positive, and negative representations are set as \(f_{i,1}\), \(f_{i,2}\), and the remaining \(2B-2\) feature representations, respectively. Using this setup, the instance contrastive loss for subject \(i\) is denoted and defined as

\[_{IC}^{i}=-,f_{i,2})}{_{j=1}^{B}[_ { j i}(h(f_{i,1},f_{j,1})+h(f_{i,1},f_{j,2}))]},\]

in which \(h\) denotes the exponential of the cosine similarity between two vectors, and \(\) denotes an indicator function that equals one if the condition is true and zero otherwise.

Local-local temporal contrastive lossTo allow the model to distinguish fMRI scans that come from _different timestamps within the same subject_, the local-local temporal contrastive loss (\(_{LL}\)) determines both positive and negative pairs within a single subject. Namely, a positive representation is derived from the same fMRI sub-sequence but is applied with different random augmentations. On the other hand, negative representations come from distinct fMRI sub-sequences from the same subjects. The feature representations are obtained in the same manner as the instance contrastive loss. To clearly define this loss function, we use the same notation of feature representation as \(f_{i,p}\), but the range of \(p\) is changed to \(\{1,2,...,N\}\), where \(N\) is the number of fMRI sub-sequences from a single subject. We denote an fMRI sub-sequence of the same subject \(i\) and fMRI sub-sequence \(p\) but different random augmentation as \(_{i,p}\). Since we are sampling two differently augmented versions for each of the \(N\) sub-sequences, this amounts to a total of \(2N\) representations. Using this setup, the local-local temporal contrastive loss for subject \(i\) is denoted as \(_{LL}^{i}\) and defined as

\[_{LL}^{i}=-_{p=1}^{N},_{i,p})}{ _{q=1}^{N}[_{ q p}(h(f_{i,p},f_{i,q})+h(f_{i,p}, _{i,q}))]},\]

in which \(h\) denotes the exponential of the cosine similarity between two vectors.

## 4 Experiments

### Experimental Settings

DatasetsThe Adolescent Brain Cognitive Development (ABCD) study is a longitudinal, multi-site investigation of brain development and related behavioral outcomes in children . The dataset is open to the scientific community but requires authorization. After quality control, we used the resting state fMRI of 9,128 children (\(=118.95 7.46\) months, 52.4% female) from release 2. For fMRI preprocessing, we used a well-established pipeline, fMRIprep [42; 43], which includes reducing the bias field, skull-stripping, alignment to structural image, and spatial normalization to standard space for a pediatric brain . After fMRIprep, we applied low pass filtering, head movement correction, and artifact removal, regressing out signals from non-grey matters (aCompcor) .

We also used the resting-state fMRI of 1,084 healthy young adults (\(=28.80 3.70\) years, 54.4% female) from the Human Connectome Project (HCP) (S1200 data) [46; 47], and 5,935 middle and old aged adults (\(=54.971 7.53\) years) from the UK Biobank (UKB) . We used preprocessed data provided by Human Connectome Project  and UK Biobank [26; 27], which follows the fMRI volume pipeline, including reducing the bias field, skull-stripping, cross-modality registration, and spatial normalization to standard space.

Figure 2: Illustration of two different contrastive losses for the pre-training of SwiFT.

For each of the 4D fMRI volumes, we globally normalized brain images over the four dimensions except for the background regions. Then we filled the background with the minimal voxel intensity value. To easily divide the volume into patches for SwiFT, we changed the 3D volume into a shape of \(96 96 96\) by cropping and padding on the background. To evaluate the performances of ROI-based models, following the preprocessing steps of  as closely as possible, we applied the HCP MMP1 atlas  to each fMRI volume to obtain the time series data for each ROI. Subsequently, we processed this ROI series to generate functional connectivities, which involves computing the Pearson correlation coefficient to construct a correlation matrix. The correlation matrix is then Fisher Transformed, serving as the input for the ROI-based models in Section 4.1.2.

To evaluate our models, we constructed three random splits with a ratio of (train: validation: test) = (\(0.7:0.15:0.15\)) and reported the average performances across the three splits.

TargetsWe chose the sex , age , and cognitive intelligence (NIH Toolbox  for ABCD, HCP datasets, and "fluid intelligence" for UKB dataset) of each subject as the prediction target for our models. These targets are significant since the relationship between the brain and these targets represents a fundamental brain-biology and brain-cognition association. Also, the capability to predict these outcomes can prove the model's capability to process fMRI volumes, possibly leading to the prediction of clinical outcomes of debilitating brain disorders, such as Alzheimer's disease, schizophrenia, autism, and bipolar disorder . For these reasons, predicting these outcomes from brain imaging has been an important benchmark task in recent computational neuroscience .

The regression targets (age, intelligence) were z-normalized to bring stable training regardless of the range of the target variable. Since the age has a unit (e.g., years or months), when reporting the performance metrics, we transformed the z-scaled age back to its original scale of months or years.

For the binary classification task, balanced accuracy and AUC (Area Under ROC Curve) were used to evaluate model performances. For the regression tasks, Mean Squared Error (MSE) and Mean Absolute Error (MAE) were used to evaluate model performances.

#### 4.1.1 Implementation Details

For SwiFT, we use the same architecture across all of our experiments, using the architecture corresponding to the Swin Transformer-variant from previous work  with a channel number of \(C=36\). The numbers of layers are fixed to \(\{L_{1},L_{2},L_{3},L_{4}\}=\{2,2,6,2\}\) which corresponds to a model with three stages with 2, 2, 6 consecutive 4D Swin Transformer blocks for each stage and a final stage with 2 consecutive global attention Transformer blocks. In the cases of 4D(S)W-MSA, we set \(P=M=4\). The final output of the model is obtained by applying a global average pooling layer on the output feature map of Stage 4, followed by an MLP head. For training, the Binary Cross Entropy (BCE) loss was used for the binary classification task, and the Mean Squared Error (MSE) loss was used for regression tasks.

For the ABCD dataset, input training images were randomly augmented to prevent the model from overfitting. The augmentations include affine transformation, adding Gaussian noise, and Gaussian smoothing, and they were also applied for the contrastive pre-training in Section 4.3.

Instead of inputting the entire fMRI volume of a subject, we divided the volume into 20-frame subsequences and used them as the input, mainly due to memory constraints. However, we believe this choice may benefit the model, as discussed in Appendix C.2 in detail. For training, each individual sub-sequence was treated as a data point, meaning the appropriate loss function was calculated and backpropagated for each sub-sequence. For inference, the logits from the sub-sequences of each particular subject were averaged, yielding a single output for each subject.

Computational complexityThe computational complexities of a single global attention Transformer block (denoted as MSA & MLP) and a 4D Swin Transformer block (denoted as W-MSA & MLP) for input with a dimension of \(T H^{} W^{} D^{} C^{}\) can be calculated as

\[()=12NC^{ 2}+2N^{2}C^{}\ \ \ \ \ ()=12NC^{ 2}+2PM^{3}NC^{},\]

in which the number of tokens \(N=TH^{}W^{}D^{}\). In practice, on Stage 1 of SwiFT, setting the values used for the experiments \(C^{}=36,T=20,H^{}=W^{}=D^{}=16,P=M=4\), the two terms \(12NC^{ 2}\) and \(2PM^{3}NC^{}\) are balanced with the second term only being 1.19 times the first term.

Compared to this, with global attention the \(2N^{2}C^{}\) term becomes 379 times larger than the \(12NC^{ 2}\) term, taking up most of the computation budget and creating a bottleneck. For successive stages, \(N\) is reduced by a factor of 8, and \(C^{}\) is increased by a factor of 2, resulting in Stage 1 being the most computationally expensive.

#### 4.1.2 Baselines

ROI-based modelsWe used ROI-based deep learning methods as baseline models, which are listed as BrainNetCNN , VanillaTF , and Brain Network Transformer (BNT) . These models utilize functional connectivity data as input, which is computed using temporal correlations (Pearson correlation) of every pair of brain regions. To evaluate such methods, we followed the hyper-parameter and implementations of these three models from . In addition, we also employed XGBoost (eXtreme Gradient Boosting)  in conjunction with the features described in  to compare a traditional machine learning model with that of deep learning-based models. We used the flattened upper triangular correlation matrix as the input for XGBoost.

TFF (a two-step method)As mentioned in Section 2, TFF  consists of 3D CNNs to reduce the dimensionality of fMRI volumes, which are then passed to a transformer encoder layer for temporal processing. It has been reported that the model achieves state-of-the-art performances in sex classification and age regression in HCP datasets compared to other deep neural networks . However, due to the separate processing of spatial and temporal information, TFF may lose important spatio-temporal information and suffer from performance degradation. Furthermore, since TFF also accepts the fMRI volume as its input, due to memory constraints, we also used the technique of dividing the volume into 20-frame sub-sequences described in Section 4.1.1.

### Classification and Regression Results

In Table 1, we compared the performance of SwiFT, which was trained from scratch as in Section 4.1.1, against various baselines mentioned above on sex classification and age, intelligence regression tasks. On the sex classification task, SwiFT outperforms all of the baselines on the HCP and UKB datasets while showing competitive results against the best ROI-based models on the ABCD dataset. On the regression tasks, we observe SwiFT outperforms all baselines significantly for the age prediction tasks, although all models still have a large room for improvement on the UKB intelligence prediction task. We believe our quantitative results clearly underscore the effectiveness of the end-to-end training of our SwiFT model.

    &  &  &  \\   &  &  &  &  &  &  &  &  \\  & ACC & AUC & MSE & MAE & ACC & AUC & MSE & MAE & MSE & MAE & ACC & AUC & MSE & MAE & MSE & MAE \\  XGBoost & 69.5 & 76.7 & 0.97 & 0.770 & 68.5 & 75.5 & 14.3 & 3.12 & 0.991 & 0.813 & 79.5 & 87.6 & 48.8 & 5.85 & 1.055 & 0.816 \\ BrainNetCNN30 & **80.1** & 87.9 & 0.969 & 0.767 & 77.1 & 84.9 & 12.6 & 2.97 & 0.984 & 0.805 & 86.8 & 93.8 & 42.7 & 5.36 & 1.001 & 0.800 \\ VanillaTF & 77.4 & 85.1 & 0.961 & 0.764 & 77.9 & 85.2 & 12.5 & 2.95 & 0.987 & 0.812 & 87.0 & 95.4 & 14.1 & 5.26 & 0.999 & 0.799 \\ BNTL & 79.1 & **88.9** & 0.955 & 0.767 & 81.0 & 88.0 & 12.8 & 2.98 & 1.001 & 0.830 & 87.0 & 94.8 & 39.6 & 5.17 & 0.998 & 0.798 \\ TFF & 73.8 & 80.2 & 0.968 & 0.768 & 92.5 & 97.5 & 13.8 & 3.11 & 0.953 & 0.795 & 96.8 & 99.5 & 42.1 & 5.10 & 0.997 & **0.783** \\  SwiFT (ours) & 79.3 & 87.8 & **0.932** & **0.756** & **92.9** & **98.0** & **8.6** & **2.36** & **0.903** & **0.786** & **97.7** & **99.8** & **18.2** & **3.40** & **0.992** & 0.796 \\   

Table 1: Performance comparison to baselines on classification and regression tasks

Figure 3: Effect of UKB pre-training on (a) HCP and (b) ABCD intelligence prediction tasks.

### Effects of Pre-training on Downstream Tasks

To demonstrate the effectiveness of contrastive pre-training described in Section 3.2, SwiFT pre-trained on a larger dataset (UKB) was fine-tuned on a smaller dataset (HCP), and a comparable-sized dataset (ABCD) for the intelligence prediction task, which has room for improvement compared to sex and age prediction tasks. The model was pre-trained using the combination of the instance contrastive loss function (\(_{IC}\)) and the local-local temporal contrastive loss function (\(_{LL}\)) such that the training objective is to minimize the sum (\(_{IC}+_{LL}\)). The feature representations used in the loss calculation were obtained in the same manner as other tasks; by applying a global average pooling layer on the output feature map of Stage 4, followed by an MLP head. Figure 3 depicts the model's performance for each training epoch during the fine-tuning process. The performances are averaged over three splits. The results from a model trained from scratch (Section 4.2) are also shown for comparison. In HCP, the pre-trained model consistently performs better during the early stage of the training compared to the model trained from scratch. In ABCD, on the other hand, the pre-trained model exhibits dramatic drops in performance at the early stage of training and gradually attains a better performance at the later stage of fine-tuning. This initial worse performance might result from the sub-optimal training hyper-parameters for fine-tuning, such as the sub-optimal initial learning rate. Overall, we believe our results suggest that the contrastive pre-training of SwiFT on a larger dataset has a promising effect on improving downstream performance, and we plan to investigate more extensively on this topic in our future research.

### Interpretation Results

Using an Integrated Gradient with Smoothgrad sQuare (IG-SQ) implemented in Captum framework [15; 61; 62], we identified the brain regions showing high explanatory power on the sex classification task. We acquired 4D IG-SQ maps from test sets and filtered out incorrectly predicted samples. To find the spatial patterns of the brain showing explanatory power across subjects, we normalized the IG-SQ maps, smoothing the maps with a Gaussian filter. Then, we averaged the maps over time dimensions and across subjects.

Figure 4 shows the brain regions contributing to successful sex classification. These include, in children (ABCD), the medial prefrontal gyrus (mPFC), posterior cingulate cortex (PCC), precuneus (PCu), and parietal gyrus (the default mode network). In young adults (HCP), similar brain regions were observed with a broader and higher intensity in mPFC, while showing unique brain regions such as the thalamus and insular cortex. In middle and old-aged adults (UKB), we acquired the highest IG values in the inferior temporal gyrus and medial orbitofrontal cortex. Of note, the results confirm those regions implicated in prior brain sex difference literature [63; 64; 65; 66].

### Model Efficiency

In Table 2, we compared the computational efficiency of SwiFT against TFF by using dummy data with random numbers. Each sample consists of 20 volumes with the shape of \(96 96 96\)

   Method & \# Param. & FLOPs & Throughput (samples/sec) \\  TFF & 729M & 40.72G & 53.60 \\ SwiFT & 4.64M & 2.62G & 104.46 \\   

Table 2: Efficiency of 4D fMRI Transformers

Figure 4: Interpretation maps with Integrated Gradients (IG) for sex classification. (Sagittal plane) (a) ABCD (b) HCP (c) UKBFloating point operations (FLOPs) were used to estimate the amounts of multiply-add computations required for processing the 4D fMRI volumes. The throughput (samples/s) denotes the number of 20-volume samples processed per second, calculated using a single NVIDIA A100 GPU. For an accurate measurement, the throughput was measured using synchronized timing with an initial GPU warmup step and was repeated 100 times. The results show that SwiFT has 158.4 times fewer parameters, requires 15.5 times fewer multi-add operations, and processes input data 1.94 times faster than TFF. This result shows SwiFT is much more computationally efficient compared to TFF while also attaining better performance as given in Table 1.

### Effect of Input Time Sequence Length

To justify our use of a 4D model and investigate the consequences of dividing the input fMRI volume into sub-sequences, we evaluated the effect of the input fMRI volume's sequence length (number of time frames). To ensure a fair comparison, we kept the model architecture and hyper-parameters, such as the local window size, constant. In Figure 5, we compared the model performance by input sequence lengths (4, 8, 16, 32 time frames) on intelligence and age prediction tasks, respectively. In the intelligence prediction task -- which is a challenging task considering the MAE of about 0.8 (z-score) was only slightly better than the variance of one -- longer sequence lengths (16 and 32) led to better results in both young adults (HCP) and elders (UKB). Also, in age prediction, for young adults, we found the performance peaking at 16 time frames. However, in predicting the age of elders, longer sequence lengths (16, 32) resulted in poorer performance in age prediction. Namely, the findings of the former three cases showed that the longer sequence lengths enabled better learning of temporal dynamics needed to predict intelligence in young adults and elders and age in young adults. However, the last case of the age prediction task in elders showed that the benefit might not be generalizable to all the cases.

These findings point out that the optimal value of the input time sequence length may vary depending on the given task and dataset, and our choice to employ constant-sized sub-sequences may have proven beneficial in light of this variability. A more detailed analysis can be found in Appendix C.2.

## 5 Concluding Remarks

Investigating the spatiotemporal dynamics of the human brain poses a formidable challenge owing to the lack of powerful analytics permitting individual-level prediction of cognitive or clinical outcomes such as psychiatric or neurological diseases. In this study, we present SwiFT, an efficient Transformer model designed for high-dimensional 4D brain functional MRI, aimed at learning these spatiotemporal dynamics and predicting biological and cognitive outcomes. Throughout various tasks, our method consistently outperforms the state-of-the-art ROI-based and two-step based methods in the domain. Furthermore, compared to TFF, a Transformer-based baseline that takes raw fMRI as input, SwiFT uses significantly less memory and training time. Lastly, our IG-SQ interpretation results show the feasibility of interpreting the spatial patterns of the functional representations from SwiFT that contribute to a given task. We believe the simple and effective end-to-end learning capability of our SwiFT has the potential to significantly contribute to enhancing the scalability of spatiotemporal learning of fMRI in both computational and clinical neuroscience research. For future work, we plan to more extensively test the effectiveness of pre-training SwiFT on large-scale datasets.

Figure 5: Effect of the number of time frames of the input fMRI volume on (a) intelligence prediction and (b) age prediction tasks.

Acknowledgements

This work was supported by the U.S. Department of Energy (DOE), Office of Science (SC), Advanced Scientific Computing Research program under award DE-SC-0012704 and used resources of the National Energy Research Scientific Computing Center, a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231 using NERSC award ASCR-ERCAP0023081. Also, this research used resources of the Argonne Leadership Computing Facility, which is a U.S. Department of Energy Office of Science User Facility supported under Contract DE-AC02-06CH11357.

This work was also supported in part by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) [No. 2021R1C1C1006503, 2021K1A3A1A2103751212, 2021M3E5D2A01022515, RS-2023-00265406, 2021M3E5D2A01024795, 2021R1A2C2007884], by Creative-Pioneering Researchers Program through Seoul National University (No. 200-20230058), and by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) [No.2021-0-01343, No.2021-0-02068, No.2022-0-00113, No.2022-0-00959]. Taesup Moon is also supported in part by ASRI / INMC at Seoul National University.

This research has been conducted using data from UK Biobank, a major biomedical database ([https://www.ukbiobank.ac.uk](https://www.ukbiobank.ac.uk)). Our project ID of the UK Biobank is 32575. Data used in the preparation of this article were obtained from the Adolescent Brain Cognitive Development (ABCD) Study ([https://abcdstudy.org](https://abcdstudy.org)), held in the NIMH Data Archive (NDA). This is a multisite, longitudinal study designed to recruit more than 10,000 children aged 9-10 and follow them over 10 years into early adulthood. The ABCD Study(r) is supported by the National Institutes of Health and additional federal partners under award numbers U01DA041048, U01DA050989, U01DA051016, U01DA041022, U01DA051018, U01DA051037, U01DA050987, U01DA041174, U01DA041106, U01DA041117, U01DA041028, U01DA041134, U01DA050988, U01DA051039, U01DA041156, U01DA041025, U01DA041120, U01DA051038, U01DA041148, U01DA041093, U01DA041089, U01DA041123, U124DA041147. A full list of supports is available at ([https://abcdstudy.org/federal-partners.html](https://abcdstudy.org/federal-partners.html)). A listing of participating sites and a complete listing of the study investigators can be found at ([https://abcdstudy.org/consortium_members/](https://abcdstudy.org/consortium_members/)). ABCD consortium investigators designed and implemented the study and/or provided data but did not necessarily participate in the analysis or writing of this report. This manuscript reflects the views of the authors and may not reflect the opinions or views of the NIH or ABCD consortium investigators. The ABCD data repository grows and changes over time. The ABCD data used in this report came from [http://dx.doi.org/10.15154/1503209](http://dx.doi.org/10.15154/1503209). DOIs can be found at [https://nda.nih.gov/abcd/abcd-annual-releases.html](https://nda.nih.gov/abcd/abcd-annual-releases.html). Data were provided [in part] by the Human Connectome Project, WU-Minn Consortium (Principal Investigators: David Van Essen and Kamil Ugurbil; 1U54MH091657) funded by the 16 NIH Institutes and Centers that support the NIH Blueprint for Neuroscience Research; and by the McDonnell Center for Systems Neuroscience at Washington University.