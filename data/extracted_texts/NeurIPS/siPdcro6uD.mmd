# OneRef: Unified One-tower Expression Grounding

and Segmentation with Mask Referring Modeling

 Linhui Xiao\({}^{1,2,3}\), Xiaoshan Yang\({}^{1,2,3}\), Fang Peng\({}^{1,2,3}\), Yaowei Wang\({}^{2,4}\), Changsheng Xu\({}^{1,2,3}\)

\({}^{1}\)MAIS, Institute of Automation, Chinese Academy of Sciences \({}^{2}\)Pengcheng Laboratory

\({}^{3}\)School of Artificial Intelligence, University of Chinese Academy of Sciences

\({}^{4}\)Harbin Institute of Technology (Shenzhen)

{xiaolinhui16, pengfang21}@mails.ucas.ac.cn,

{xiaoshan.yang, csxu}@nlpr.ia.ac.cn, wangyw@pcl.ac.cn

Corresponding author.

###### Abstract

Constrained by the separate encoding of vision and language, existing grounding and referring segmentation works heavily rely on bulky Transformer-based fusion en-/decoders and a variety of early-stage interaction technologies. Simultaneously, the current mask visual language modeling (MVLM) fails to capture the nuanced referential relationship between image-text in referring tasks. In this paper, we propose _OneRef_, a minimalist referring framework built on the modality-shared one-tower transformer that unifies the visual and linguistic feature spaces. To modeling the referential relationship, we introduce a novel MVLM paradigm called _Mask Referring Modeling (MRefM)_, which encompasses both referring-aware mask image modeling and referring-aware mask language modeling. Both modules not only reconstruct modality-related content but also cross-modal referring content. Within MRefM, we propose a referring-aware dynamic image masking strategy that is aware of the referred region rather than relying on fixed ratios or generic random masking schemes. By leveraging the unified visual language feature space and incorporating MRefM's ability to model the referential relations, our approach enables direct regression of the referring results without resorting to various complex techniques. Our method consistently surpasses existing approaches and achieves SoTA performance on both grounding and segmentation tasks, providing valuable insights for future research. Our code and models are available at https://github.com/linhuixiao/OneRef.

## 1 Introduction

Visual Grounding (VG) aims to ground a region referred by a expression query text in a specific image. The generalized VG / referring tasks include Referring Expression Comprehension (REC) , Phrase Grounding (PG) , and Referring Expression/Image Segmentation (RES/RIS) . In REC/PG, the grounding region is represented by a rectangular boundary box, while in RES/RIS, it is represented by an irregular fine-grained segmented mask of the referred object. Unlike object detection  or instance segmentation , which usually relies on a close-set of categories to detect or segment multiple regions that satisfy the object label, visual grounding is not limited to fixed categories. It requires understanding the semantics of the query text and then grounding or segmenting specific areas. Therefore, visual grounding is a task that strongly relies on the multimodal interaction and alignment of visual and linguistic features.

Since the introduction of BERT  and ViT , the state-of-the-art (SoTA) grounding works have widely adopted a pre-training and fine-tuning paradigm. As illustrated in Fig. 1, existing studiesemploying pre-trained models, either utilizing uni-modal pre-trained models to separately transfer visual and language knowledge  or utilizing multimodal pre-trained models , primarily fall into three typical architectures: _(i)_ two modality encoders combined with a cross-modal fusion encoder, exemplified by TransVG _etc_. ; _(ii)_ additionally incorporating a decoder, exemplified by MDETR _etc_. ; _(iii)_ direct regression based on language-guided visual features, such as LAVT, TransVG++, _etc_. . However, incorporating modality-dependent encoders in these studies presents a challenge for seamlessly integrating the two modalities into a unified feature space. Consequently, these works not only require an additional cross-modal Transformer-based  en-/decoder (_(i)_ and _(ii)_), but also propose a variety of careful-designed interaction structures for modality-dependent encoders to facilitate early-stage fine-grained cross-modal alignment , such as adapter , cross-modal bridge , weight generation , image-text cross-attention , _etc_. Therefore, these methods not only entail a large number of parameters but also involve intricate processes. Considering these critical limitations, we aim to explore simpler modality-shared grounding frameworks that can unify vision and language within a unified feature space, thereby obviating the necessity of the elaborate interaction modules, bulky fusion Transformer en-/decoders, as well as the special grounding tokens.

With the advancement of pre-training , several studies have been conducted to explore unified modality-shared multimodal frameworks. YORO  implemented a shared encoder based on ViLT . However, its modeling approach tends to overshadow the uni-modal knowledge and requires the encoder to incorporate additional query anchors, limiting its applicability for transfer with common pre-trained models. ONE-PEACE  has designed seven expert branches based on Mix-of-Expert (MoE)  to construct a three-modality foundation model to realize the integration of image, text, and audio modalities. However, their research employed extensive tri-modal data without exploring the potential utilization of MVLM for modeling the referring tasks. BEiT-3  is built upon multi-way Transformer , which adopts three MoE heads (_i_.\(e\)., vision, language, vision-language) and a modality-shared structure that effectively unifies vision and language within a shared feature space. It demonstrates notable advantages across various classification-like cross-modal fields (_e_.\(g\)., Retrieval, VQA _etc_. ). However, no prior research has explored the utilization of BEiT-3 for achieving transfer in referring tasks. Consequently, our objective is to explore more concise and efficient referring grounding and segmentation transfer within a unified feature space on the one-tower model of BEiT-3. However, BEiT-3 model is pre-trained utilizing a generic Mask Vision Language Modeling (MVLM) approach, and this masking paradigm lacks fine-grained cross-modal referring ability and cannot effectively model the intricate referential relationship between images and text. As a result, there exists a significant gap when applying BEiT-3 to the regression-like referring tasks. Therefore, exploring how to incorporate fine-grained cross-modal referring capability into the mask modeling paradigm becomes an important research issue that has not been addressed yet.

In this paper, we propose a novel paradigm called _Mask Referring Modeling (MRefM)_, as well as a unified and extremely concise grounding and referring segmentation framework named _OneRef_ that no longer requires the fusion or interaction Transformer structure and the special grounding tokens.

**Firstly**, we propose MRefM paradigm to enhance the referring capability of BEiT-3 in a flexible manner. MRefM consists of two components: Referring-aware Mask Image Modeling (**Referring MIM**) and Referring-aware Mask Language Modeling (**Referring MLM**). The conventional MVLM is typically trained alternately or randomly with uni-modal MIM and MLM. In contrast, Referring MIM and Referring MLM are required to reconstruct two distinct types of content: their own modality-related content and cross-modal referring information. Specifically, _(i) Referring MIM_ employs visual tokens after the dot product operation with the aggregated text token for reconstruction purposes. This not only entails reconstructing masked visual features itself but also necessitates

Figure 1: Comparison between our proposed approach and the mainstream REC/RES architectures.

reconstructing the visual target-relation score, which indicates the distance between the current token and the grounding region. The score encompasses four dimensions: horizontal and vertical distance to the grounding center, as well as width and height of the grounding region. In order to enhance the model's understanding capability for referred regions, we propose a referring-aware dynamic image masking strategy that replaces traditional ratio-fixed random masking so that referred regions are reconstructed with a relatively high mask ratio. _(ii) Referring MLM_ employs text tokens after the dot product operation with the aggregated visual token for reconstruction purposes. This not only involves reconstructing masked text itself but also requires reconstructing semantic target-relation scores that represent the correlation degrees between current text tokens and referred image regions.

**Secondly**, existing grounding and segmentation models commonly employ a [Region] token and multiple query anchors to regress results. However, embedding the region token in backbone will disrupt the pre-trained model , and the query anchor also depends on the decoder . With the unified feature space established by modality-shared encoder, we no longer need additional cross-modal en-/decoders to fuse uni-modal features, enabling us to more effectively leverage the knowledge acquired by pre-trained backbone. Benefiting from MRefM paradigm, the visual token inherently contains referring information. Consequently, we can discard special grounding token/anchors and directly construct lightweight and highly concise grounding and segmentation task heads based on the dot product operation within Referring MIM to unify the referring framework.

_Contributions:_ Our contributions are threefold: _(i)_ We pioneer the application of mask modeling to referring tasks by introducing a novel paradigm called mask referring modeling. This paradigm effectively models the referential relation between visual and language. _(ii)_ Diverging from previous works, we propose a remarkably concise one-tower framework for grounding and referring segmentation in a unified modality-shared feature space. Our model eliminates the commonly used modality interaction modules, modality fusion en-/decoders, and special grounding tokens. _(iii)_ We extensively validate the effectiveness of MRefM in three referring tasks on five datasets. Our method consistently surpasses existing approaches and achieves SoTA performance across several settings, providing a valuable new insights for future grounding and referring segmentation research.

## 2 Related work

### Referring expression comprehension (REC) and segmentation (RES)

_(i) Rec._ The recent supervised REC task, also known as visual grounding in a narrow sense, can be broadly categorized into **five main approaches**: **(1)** Fine-tuning with a uni-modal pre-trained language model and a closed-set detector. This setting is exemplified by TransVG , which builds upon the two-stage [102; 56; 52; 30] and one-stage [96; 95; 106] methods from the CNN era. It is considered the most conventional and extensively studied approach. **(2)** Fine-tuning with a pre-trained uni-modal language model and an open-set detection model pre-trained on box-level datasets mixed with multiple data sources. MDETR  represents this type of setting, where Fig. 1-(a)-(ii) plays a dominant role in its model structure. **(3)** Fine-tuning with multimodal self-supervised pre-trained models. CLIP-VG  serves as an example for this category, introduced primarily through the proposal of CLIP . **(4)** Multimodal and multi-task mix-supervised pre-trained models. These methods typically combine multiple tasks while mixing datasets from each downstream task, employing mixed pre-training that incorporates both self-supervision and fine-grained supervision. UniTAB , OFA, _etc._ represent such approaches where visual grounding often acts as one of the pre-training tasks. **(5)** Grounding multimodal large language models (GMLLMs). These methods influenced by works like GPT  or LLAMA _etc_. These models integrate visual backbones into Large Language Models (LLMs) to generate grounding results rather than relying on regression techniques. Our approach mainly falls under type (3). _(ii) RES._ The development and approach categories of RES [49; 13; 35; 89; 79; 94; 93; 88] are generally similar to those of REC. However, the key distinction lies in the finer granularity of RES's output, which necessitates separate study from REC. In terms of model architecture, RES works predominantly employ two modality-dependent encoders and a decoder to generate the segmentation mask. Our work stands out as the first endeavor to explore RES within a unified multimodal feature space under a one-tower structure.

### Mask vision language modeling

Motivated by the success of MLM  in BERT , MAE  and BEiT  have primary shifted their attention to MIM [21; 83; 3]. Subsequently, exemplified by BEiT-3 , numerous MVLMworks [61; 47; 36; 104; 2] have emerged, with most of these works implementing randomly alternating uni-modal MIM and MLM. Most relevant to our work are mask region modeling (known as MRM) [64; 83], which can be either unimodal MIM (_e.g_., R-MAE ) or employ more fine-grained regional data and contrastive learning to reconstruct the alignment between regions and object labels (_e.g_., ConLIP , VLT [17; 18] _etc_. ). However, our work focuses on modeling the fine-grained referential relationship within image and text, so as to enhance the cross-modal referring capability, which is significantly different from these works.

## 3 Methodology

In this section, we propose our multimodal Mask Referring Modeling (_MRefM_) paradigm, which includes Referring MIM and Referring MLM, as well as a feature space unified grounding and segmentation framework _OneRef_. We will introduce these methods in the following sections.

Following BEiT-3 , we employ a multimodal modality-shared Transformer  as the underlying backbone network. Initially, we perform mask-then-predict MRefM pre-training, and followed by transfer fine-tuning on the referring tasks. As shown in Fig. 2, the MRefM pre-training stage consists of two components: Referring-aware Mask Image Modeling (**Referring MIM**) and Referring-aware Mask Language Modeling (**Referring MLM**). Both modules aim to reconstruct two types of content: modality-related content within each modality and cross-modal fine-grained referring content.

### Preliminaries

BEiT-3  utilizes MIM, MLM, and MVLM for processing image, text, and image-text pairs respectively to facilitate the acquisition of general representations through MoE heads and shared multi-head self-attention. Notably, MVLM involves alternate training of MIM and MLM. Specifically:

**(i) Vanilla mask image modeling.** We denote \(^{H W 3}\) as the input image, and it is tokenized by a convolution projection to \(N_{v}=HW/P^{2}\) patches \(\{_{i}^{p}\}_{i=1}^{N_{v}}\), where \(^{p}^{N_{v} D}\), \(H,W\) are the image size, and \(P\) is the patch size, \(D\) is the hidden dimension of the unified feature space. Then, we leverage a specific masking strategy to mask a specific number of image patches. The masked position is termed as \(_{v}\). Thus, a shared learnable embedding \(_{[]}\) is used to replace the masked image patch embeddings \(_{i}^{p}\) if \(i_{v}\). Subsequently, we prepend a learnable [CLS] token to the input, _i.e_., \([_{},\{_{i}^{p}\}_{i=1}^{N_{v}}]\), and feed them to the one-tower Transformer. Next, we utilize a MIM head which consists of a linear projection and a softmax classifier to predict the visual tokens of the masked positions based on the corrupted image \(^{}\). The visual tokens are obtained by the image tokenizer VQ-KDCLIP proposed in BEiT v2 , which provides supervisions for the MIM self-supervised learning procedure. The visual tokens of the original image are denote as \(\{_{i}\}_{i=1}^{N_{v}}\), and \(\) denotes the pre-training images. Then, the training loss of MIM is defined as:

\[_{}=-_{}_{i _{v}} p(_{i}|_{i}^{}).\] (1)

Figure 2: Illustration of our multimodal Mask Referring Modeling (MRefM) paradigm, which includes Referring-aware mask image modeling and Referring-aware mask language modeling.

**(ii) Vanilla mask language modeling.** The input text is tokenized and projected to the word embeddings \(\{_{i}\}_{i=1}^{M}\) by a SentencePiece tokenizer  with vocabulary size of 64010, where \(^{M D}\), \(M\) is the length of tokenized text sequence. Then, following BEiT-3 , we randomly mask the text tokens with a fixed masking ratio \(\). The masked position is termed as \(_{w}\). Thus, a shared learnable embedding \(_{[]}\) is used to replace the masked word tokens \(_{i}\) if \(i_{w}\). We prepend a learnable special tokens [SEP] and an end-of-sequence token [EOS] to the sequence, _i.e._, \([_{},\{_{i}\}_{i=1}^{M},_{}]\), and feed them to the one-tower Transformer. Similarly, we utilize a MLM head which consists of a linear projection to predict the text tokens of masked positions based on the corrupted text data \(^{}\). The original textual tokens are denoted as \(\{_{i}\}_{i=1}^{M}\), and \(\) denotes the pre-training text sequences. Then, the training loss of MLM is defined as:

\[_{}=-_{}_{i_ {w}} p(_{i}|_{i}^{}).\] (2)

### Referring-aware mask image modeling

After concatenating the visual and text tokens and feeding them into the modality-shared encoder, the vanilla MVLM is commonly implemented through the alternating use of MIM and MLM . Despite the multimodal features are interact within the modality-shared encoder, it fundamentally remains a unimodal information reconstruction. Additionally, MVLM acquires general knowledge by randomly masking images and texts, it fails to effectively model the referential relationship. Hence, we propose Referring MIM and Referring MLM methods. Specifically, as shown in Fig. 2, our proposed Referring MIM incorporates two additional components: the reconstruction of visual target-relation score and a referring-aware dynamic masking strategy.

In Referring MIM (Fig. 2), instead of using uni-modal visual tokens [87; 47; 2], we propose to employ visual tokens that dot product with the aggregated text token \(_{}^{1 D}\) for the reconstruction purpose. The reconstruction of Referring MIM involves not only the modality-related content \(\{_{i}\}_{i_{v}}\) but also the visual target-relation scores \(\{_{i}^{v}\}_{i=1}^{N_{v}}^{N_{v} 4}\). We utilize a visual target-relation head which consists of a three-layer perceptron \(\) to predict the scores. The scores represent the distance between each patch token \(\{_{i}^{}\}_{i=1}^{N_{v}}\) and the referred region \(=(x_{c},y_{c},w_{r},h_{r})\), where \((x_{c},y_{c},w_{r},h_{r})\) denote the center coordinate and the width and height of the referred region. It encompasses four masks, _i.e._, _x-y-, w-, h- masks_, which represent the normalized horizontal and vertical distances from the referred center, _i.e._, \(((x-x_{c})/W,\ (y-y_{c})/H)\), and the proportion of width and height on the the referred region, _i.e._, \((P/w_{r},\ P/h_{r})\), respectively, where \((x,y)\) denote the center coordinate of each patch. We denote \(\) as dot product operation. Finally, the training loss of Referring MIM is defined as:

\[_{}=-_{}_{i _{v}} p(_{i}|(_{i}^{}_{}))-_{}_{i[1,N_{v}]} p(_{i}^{v }|(_{i}^{}_{})).\] (3)

**Referring-aware dynamic image masking strategy.** As shown in Fig. 4, among the existing masking strategies, MAE  adopts a high-ratio random masking while BEiT-3  uses a low-ratio block-wise random masking, neither of which effectively directs attention to the referred region. SemMAE  proposes a semantic-guided masking that requires additional bulky semantic models and limits its generality. To enhance the model's understanding of the referred region through surrounding visual context and text semantics, we propose a referring-aware dynamic masking strategy as shown in Algo. 1.

The strategy avoids the drawbacks of the aforementioned methods and directs the model's attention to the referred region. Specifically, we denote the shape after patch reshaping of the image as \((h,w)\), where \(h=H/P\), \(w=W/P\), and \(N_{v}=h w\). To maximize the masking of thereferred region \((x_{s},y_{s},w_{r},h_{r})\), where \(x_{s}\), \(y_{s}\) represent the starting coordinates of the referred region, we introduce a margin \(m\) to its surroundings and denote its patch coordinates as \((x_{sp},y_{sp},w_{rp},h_{rp})\), _i.e._, \(x_{sp}= x_{s}/P-m\), \(w_{rp}= w_{r}/P+m\), \(y_{sp}\) and \(h_{rp}\) are similar to that of \(x_{sp}\) and \(w_{rp}\), where \(\) indicates rounding down to an integer. Thus, the number of referred patches is denote as \(N_{r}=h_{rp} w_{rp}\). Then, as shown in Algo. 1, to ensure that the model allocates appropriate attention to the in-contextual information around the referred region, we utilize a random masking with a relatively low ratio \(\) for its surroundings. Simultaneously, we employ a block-wise masking approach with a high ratio \(\) in the extended area of the region. Since referred regions vary across different image-text pairs, each sample's entire masking ratio \(\) is dynamically determined:

\[=[(N_{v}-N_{r})+ N_{r}]/N_{v}.\] (4)

### Referring-aware mask language modeling

Similarly, in Referring MLM, instead of using uni-modal linguistic tokens [87; 47; 2], we propose to employ linguistic tokens that dot product with the aggregated visual token \(}}^{1 D}\) for the reconstruction purpose. The reconstruction of Referring MLM involves not only the modality-related content \(\{_{i}\}_{i_{w}}\) but also the semantic target-relation scores \(\{_{i}^{st}\}_{i=1}^{M}\). The score represents the correlation between the referred target and the language token, which is obtained by a teacher model (_i.e._, a BEiT-3 model with performed image-text contrastive intermediate tuning) with calculating the weighted sum of the normalized similarity between the language token \(\{_{i}\}_{i=1}^{M}\) and the aggregated visual token \(}^{reg}}\) of referred region, as well as the aggregated visual token \(}^{img}}\) of entire image:

\[^{st}=_{reg}(<}^{reg}},\ \{_{i}\}_{i=1}^{M}>)+ _{img}(<}^{img}}^{},\ \{_{i}\}_{i=1}^{M}>),\] (5)

where \(<,>\) denotes cosine similarity operation, \(\) denotes the softmax normalization.As shown in Fig. 2, we utilize a semantic target-relation head which consists of a three-layer MLPs and a softmax normalization to predict the scores. Finally, the training loss of Referring MLM is defined as:

\[_{}=-_{}_{i _{w}} p(_{i}|(_{i}^{}}}))-_{}_{i[1,M]} p_{kl}(_{i}^{st }|(_{i}^{}}})),\] (6)

where \(p_{kl}\) represents a probabilistic prediction with Kullback-Leibler divergence .

### Referring-based grounding and segmentation transfer

The modeling of visual and language in a unified feature space eliminates the need for the commonly-used Transformer-based fusion en-/decoder [14; 54; 55] and various early-stage interaction techniques [15; 79; 92] to further uniform the visual and language features. Additionally, since the referential relationship is modeled by MRefM during pre-training, we can accurately regress the results of grounding and referring segmentation using the output tokens, without relying on the widely-used special grounding tokens (_e.g._, [Region] token [14; 15; 98; 91; 92], query anchors [55; 54]).

**Referring expression comprehension.** As illustrated in Fig. 3-(a), based on Referring MIM, we initially perform a similarity operation between visual tokens \(\{_{i}\}_{i=1}^{N_{v}}^{N_{v} D}\) and aggregated language token \(}}^{1 D}\) to obtain a softmax-normalized similarity mask \(_{sim}^{h w}\). This mask is then replicated and multiplied back to each hidden dimension of the visual tokens. Subsequently, the visual tokens are summed to yield reduced tokens, which are finally subjected to regress the prediction box \(}=(_{c},_{c},_{r},_{r})\) using a 3-layer MLPs:

\[}=(_{i[1,N_{v}]}(((<}}^{},\ \{_{i}\}_{i=1}^{N_{v}}>))(\{_{i}\}_{i=1}^{N_ {v}}))).\] (7)

To enhance the accuracy of cross-modal similarity, we propose treating the similarity as a coarse-grained downsampling bounding box mask \(_{box}^{h w}\) and imposing segmentation loss (_i.e._,

Figure 3: Illustration of the referring-based grounding and segmentation transfer.

[MISSING_PAGE_FAIL:7]

while in REC and RES, the query refers to a reference expression. The text of RefCOCO+/g exhibits greater length and complexity in comparison to that of RefCOCO. In REC/PG, we follow previous works  that employs Intersection-over-Union (IoU) as the evaluation metric, \(i.e.\), a prediction is deemed accurate only when its IoU exceeds or equals 0.5. We compute the prediction accuracy for each dataset as a performance indicator. While in RES, we follow previous works  that employs mean IoU (mIoU) and overall IoU (oloU) for each dataset as the indicators. The detailed statistics information regarding these five datasets are provided in the Appendix B.

**Experimental details.** Since MReftM is proposed on the basis of the traditional MVLM, considering the pre-training cost of MVLM, we adopt BEiT-3  base and large model as our initial weights and then perform intermediate MReftM pre-training on the task-relevant dataset. Such intermediate pre-training is common in existing grounding works . As described in Sec. 2.1, to verify the effectiveness of our MReftM approaches, **we conduct extensive experiments on three settings**: **(1) The basic single-dataset fine-tuning setting.** This setting does not require additional training data and aligns with existing supervised and self-supervised transfer approaches . In this setting, we perform supervised single-dataset intermediate MReftM pre-training before fine-tuning. **(2) The setting of fine-tuning with supervised dataset-mixed intermediate pre-training.** This setting aligns with existing grounded pre-trained approaches, such as Grounding-DINO , DQ-DETR \(etc.\) we perform an MReftM intermediate pre-training before fine-tuning. **(3) To verify the generality of MReftM, we perform **the setting of fine-tuning with unsupervised intermediate MReftM pre-training.** There are several ways to obtain the regions in the regional masking modeling works , such as Felzenswalb-Huttenlocher (FH) algorithm , SAM \(etc.\) Thus, we adopt the unsupervised, fast, image-computable FH algorithm  to generate regions following R-MAE . We then select the referred one using a BEiT-3 model with performed image-text contrastive intermediate tuning. More details about the selection of the unsupervised regions, network architecture, training and inference, model hyperparameters \(etc.\) are provided in the Appendix C.

### Comparison with state-of-the-art methods

**Referring expression comprehension.** As shown in Tab. 1 and Tab. 2, we conducted experiments for the REC and PG tasks across **three settings**. **(1)** In the single-dataset fine-tuning setting, our base model surpasses the current SoTA method HiVG  by 2.07\(\%\)(testB), 6.15\(\%\)(testB), 4.73\(\%\)(test),

   &  &  &  &  &  &  \\  & &  &  &  &  &  &  &  &  &  \\   \\  & NIPS-21 & RN101 & RN101 & RN12 & RN- & 74.34 & 76.77 & 70.87 & 66.75 & 70.58 & 94.00 & 66.63 & 67.39 \\ SegTA  & ECV-22 & DNSV2/Bi-GRU & – & 71.70 & 73.31 & 69.82 & 63.04 & 67.38 & 58.97 & 64.69 & 65.74 \\ LAVT  & CVPR-22 & Swin-B BERT-B & – & 74.46 & 76.89 & 70.94 & 65.81 & 70.97 & 59.23 & 63.43 & 63.62 \\ VGA-LAW  & CVPR-23 & ViT-Det /BERT-B & – & 75.05 & 77.36 & 71.69 & 66.61 & 70.30 & 58.14 & 65.36 & 65.13 \\   \\  & CVPR-22 & L\(\)L/C-ILP-1 & – & 70.47 & 73.18 & 66.61 & 62.27 & 68.06 & 53.68 & 59.87 & 60.36 \\ JMCIL & EMU-23 & L\(1.95\(\%\)(test), and 1.50\(\%\)(test) on the five datasets respectively, while also significantly outperforming the traditional uni-modal detector-based approach TransVG++  by 4.37\(\%\)(testB), 7.98\(\%\)(testB), 7.22\(\%\)(test), 2.47\(\%\)(test), and 2.12\(\%\)(test), respectively. **(2)** In dataset-mixed pre-training setting, our base model outperforms HiVG  by 1.35\(\%\), 2.79\(\%\), and 2.63\(\%\) on RefCOCO/+/g testB/testB/test splits, outperforms Grounding-DINO  by 2.59\(\%\), 4.76\(\%\), and 2.38\(\%\), exceeds OFA by 5.28\(\%\), 5.18\(\%\),and 5.01\(\%\), and even surpasses LION  - a GMLLM model that is 20-60 times larger than ours - by 3.76\(\%\), 2.13\(\%\) and 1.69\(\%\). Note that among these works, UniTAB , OFA, LION  also utilize the MVLM on the pre-training stage. **(3)** Furthermore, we achieve competitive performance in the unsupervised setting, which shows the generality of MReFM paradigm. Additionally, our large-size model exhibits remarkable scalability with further substantial improvements in performance. More detailed results are provided in the Appendix E.

**Referring expression segmentation.** As presented in Tab. 3 (mIoU metric), we conducted experiments for RES task under **three settings**. **(1)** In the single-dataset fine-tuning setting, our base model surpasses the SoTA self-supervised method RISCLIP  by 2.65\(\%\), 4.77\(\%\), and 1.73\(\%\) on RefCOCO/+/g testB/testB/test splits, respectively, while also significantly outperforming the traditional uni-modal detector-based approach VG-LAW  by 3.42\(\%\), 7.31\(\%\), and 4.57\(\%\), respectively. **(2)** In the dataset-mixed pre-training setting, our base model achieves superior performance compared to the SoTA method RISCLIP  with improvements of 4.53\(\%\), 8.21\(\%\), and 5.39\(\%\). **(3)** In the unsupervised pre-training setting, we also achieve competitive performance. Additionally, our large-size model also exhibits remarkable scalability and demonstrates a substantial improvement in performance. For oIoU metric, the results are presented in Appendix E.2 (Tab. 13).

### Ablation study

**The Mask Referring Modeling.** In Tab. 4, we conducted ablation studies on MReFM, which included Referring MIM ('Ref MIM'), Referring MLM ('Ref MLM'), and referring-aware dynamic image masking ('referring-aware'). The 'vanilla' denotes the vanilla MVLM described in Sec. 3.1. As shown in Tab. 4, referring MIM, referring MLM, and dynamic masking strategy resulted in improvements of 3.70\(\%\), 2.16\(\%\), and 1.05\(\%\) on the RefCOCOg-test dataset, and with an overall improvement of 6.21\(\%\), demonstrates the effectiveness of our methods. More results are provided in the Appendix E.4.

**The referring-aware dynamic masking strategy.** Fig. 4 presents a schematic of the three masking strategies. In our experiments, as illustrate in Fig. 4-(c), \(\) and \(\) demonstrate optimal performance at values of 0.35 and 0.75, respectively. More detailed results are provided in the Appendix E.5.

**The referring-based task heads.** We conducted ablation studies on the design of two referring-based task heads. Tab. 5 reveals that our modeling method effectively captures referring information at the backbone stage, benefiting from the one-tower structure. This approach is significantly more efficient

   &  &  &  \\  & & val & test & val & test \\  TransVG & DETR / BERT-B & 68.67 & 67.73 & 75.73 & 75.86 \\ MReFM-TransVG & DETR / BERT-B & **71.51** & **70.84** & **78.71** & **78.69** \\ CLIP-VG & CLIP-B / CLIP-B & 73.18 & 72.54 & 78.67 & 78.54 \\ MReFM-CLIP-VG & CLIP-B / CLIP-B & **74.22** & **74.50** & **80.48** & **80.83** \\  

Table 6: Generality study of MReFM on RefCOCOg.

   &  \\  & val & test \\  TransVG++ (Reproduced by us) & 75.04 & 75.55 \\ TransVG++ \(w\): our REC head & **76.65** & **77.09** \\  LAVT  & 63.34 & 63.62 \\ LAVT w. our RES head & **64.84** & **65.35** \\  

Table 7: Generality of the task heads.

Figure 4: Illustrations of random masking (MAE) , block-wise masking (BEiT) , and our referring-aware dynamic masking. \(\) denotes the entire masking ratio, while \(\) and \(\) denote the masking ratio beyond and within the referred region.

than the traditional fusion encoder and special token-based method. Additionally, our proposed box mask loss also contributes to a performance gain of 1.50\(\%\)(test).

### Generality study

**The generality of MRefM.** Firstly, we perform an unsupervised MRefM pre-training in Tab. 2 and Tab. 3, both of which achieve competitive performance. Secondly, we replace the backbone and apply MRefM on DETR and CLIP by using TransVG  and CLIP-VG  under the two settings. Since the two frameworks do not interact at backbone stage, we build MRefM on the fusion encoder. In Tab. 6, MRefM can effectively learn referring representation, resulting in an overall performance gain of about 2.0\(\%\). All these findings demonstrate the validity and generality of the MRefM paradigm.

**The generality of referring-based task heads.** Since both TransVG++  and LAVT  have modality interactions at backbone stage, we attempted to apply our task heads to both frameworks. TransVG++ is reproduced by us since its code is not available. Tab. 7 shows that our proposed task heads achieve a 1.5+\(\%\) improvement in both REC and RES, offering a new avenue for future research.

## 5 Conclusion

In this paper, we propose a novel, highly concise, and feature space unified one-tower referring framework. Additionally, we pioneer the exploration of mask modeling in referring tasks by introducing MRefM paradigm to capture the referential relationships between vision and text. We demonstrate the effectiveness and generality of MRefM across three settings on REC, PG, and RES tasks, consistently achieving groundbreaking results. Furthermore, leveraging unsupervised methods enables potential large-scale pre-training of MRefM in the future, presenting a new direction for referring tasks.