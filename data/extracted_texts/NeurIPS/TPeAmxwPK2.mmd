# Parameter and Computation Efficient Transfer Learning for Vision-Language Pre-trained Models

Qiong Wu\({}^{12}\), Wei Yu\({}^{12}\), Yiyi Zhou\({}^{12}\), Shubin Huang\({}^{1}\), Xiaoshuai Sun\({}^{12}\), Rongrong Ji\({}^{12}\)

\({}^{1}\) Key Laboratory of Multimedia Trusted Perception and Efficient Computing,

Ministry of Education of China, Xiamen University, 361005, P.R. China.

\({}^{2}\) Institute of Artificial Intelligence, Xiamen University, 361005, P.R. China.

{qiong, weiyu}@stu.xmu.edu.cn, zhouyiyi@xmu.edu.cn,

shubinhuang@stu.xmu.edu.cn, {xssun, rrji}@xmu.edu.cn

Corresponding Author.

###### Abstract

With ever increasing parameters and computation, vision-language pre-trained (VLP) models exhibit prohibitive expenditure in downstream task adaption. Recent endeavors mainly focus on parameter efficient transfer learning (PETL) for VLP models by only updating a small number of parameters. However, excessive computational overhead still plagues the application of VLPs. In this paper, we aim at _parameter and computation efficient transfer learning_ (PCETL) for VLP models. In particular, PCETL not only needs to limit the number of trainable parameters in VLP models, but also to reduce the computational redundancy during inference, thus enabling a more efficient transfer. To approach this target, we propose a novel _dynamic architecture skipping_ (DAS) approach towards PCETL. Instead of directly optimizing the intrinsic architectures of VLP models, DAS first observes the significances of their modules to downstream tasks via a reinforcement learning (RL) based process, and then skips the redundant ones with lightweight networks, _i.e._, adapters, according to the obtained rewards. In this case, the VLP model can well maintain the scale of trainable parameters while speeding up its inference on downstream tasks. To validate DAS, we apply it to a bunch of representative VLP models, and conduct extensive experiments on a set of VL tasks. The experimental results not only show the great advantages of DAS in reducing computational complexity, _e.g._\(-11.97\%\) FLOPs of METER on VQA2.0, but also confirm its competitiveness against existing PETL methods in terms of parameter scale and performance. Our source code is given in https://github.com/DoubtedSteam/DAS.

## 1 Introduction

Inspired by the great success in natural language processing (NLP) [8; 19; 32; 36], large-scale pre-training on massive image-text pairs also becomes the _de-facto_ standard in vision-language research [4; 24; 35; 65]. To accommodate the large-scale pre-training corpora, vision-language pre-trained (VLP) models [4; 70; 10; 24; 28; 35; 56; 74] often adopt Transformer-based networks with sheer sizes of parameters and computations. In this case, directly transferring these VLP models to downstream tasks is excessively expensive in terms of memory footprint and computation overhead.

To reduce the costs of pre-training models, recent advances resort to _parameter efficient transfer learning_ (PETL) for affordable downstream task adaptions [16; 13; 26; 37; 59; 72; 74; 75]. In particular, the PETL methods aim to save the memory usage for downstream tasks by only updatingor inserting a small number of trainable parameters rather than fully tuning the entire model. For instance, prompt-tuning methods [1; 5; 26; 36; 37; 50; 52; 74; 75] expand the input sequence with hand-craft or learnable tokens to bridge the gap between pre-training and downstream tasks. Practitioners also insert lightweight neural networks called _Adapter_[13; 20; 46; 47; 59; 72; 41] into the pre-trained models, thereby projecting the hidden features onto the semantic space of downstream tasks. More recently, these PETL methods have been successfully introduced to VLP models [13; 33] for either prompt-based image classification [26; 52; 74; 75] or conventional VL tasks like _visual question answering_[58; 59; 43]. Despite the great successes, PETL methods still cannot reduce the computation complexity of VLP models, which is of more significance for applications.

In this paper, we study a novel problem called _parameter and computation efficient transfer learning_ (PCETL). To achieve more efficient downstream task adaptions, PCETL is not only expected to maintain the scale of trainable parameters similar to PETL, but more importantly, also needs to reduce the computation complexity of pre-training models, thereby speeding up their inference on downstream tasks. In existing works, the efficiency of the network itself is largely attributed to its manually [6; 22; 44; 54] or automatically structural designs [62; 66; 78; 45]. Although the computation complexity can be further reduced by the compression methods, such as _pruning_[67; 49; 3; 31; 7; 12; 30; 9; 64; 69; 55], _quantification_[11; 29; 73] or _distiliation_[2; 25], these approaches usually require retraining after optimizing the network architecture, which is not applicable to the VLP models that are well pre-trained on massive data. On one hand, the large-scale pre-training data still needs a certain model capacity to learn these prior knowledge, thus it is hard to obtain a good trade-off between performance and computation overhead for the pre-training objectives. On the other hand, devising a small and effective model for each downstream task is still laborious and expensive, which also contradicts the target of PETL [20; 36], since fully fine-tune is often required.

In this case, we argue that the key to PCETL is to explore the parameter and computation redundancy in existing VLP models. It is generally assumed that the model scale is proportional to the complexity of the task [77; 1; 18; 37; 76]. To robustly serve a variety of downstream tasks, VLP models are pre-trained by multiple pre-training objectives based on tens of millions of image-text pairs [14; 51; 52; 57]. In this case, the excessive parameters are suitable for pre-training, but prone to redundant for a downstream task. As shown in Fig. 1-(a), the performance of METER  on VQA is barely affected when skipping a certain number of its Transformer layers. This empirical result also suggests that exploring a short-cut pathway in existing VLP models is a feasible way for PCETL.

To this end, we propose a novel _Dynamic Architecture Skipping_ (DAS) approach towards efficient transfer learning for VLP models. By observing the module redundancy of VLP models, DAS can realize the optimal subnetwork routing of VLP models for a downstream task, thereby reducing the computation during inference. In practice, DAS regards this process as a \(k\)-armed bandit problem, and evaluates the importance of each VL layer/block to the downstream task via numerous subnetwork samplings and quick validations. Thus, the obtained rewards can be used to reflect the redundancy of VL modules and determine which layers to be skipped. Meanwhile, to achieve parameter efficiency,

Figure 1: (a) The performance of METER  is barely affected when skipping a certain number of its Transformer layers. (b) The comparison on VQA2.0 between the conventional PETL methods [17; 21; 26; 36; 59] and the proposed _Dynamic Architecture Skipping_ (DAS) for METER. The circle size represents the memory footprint. DAS is the only method faster than the original VLP model.

we also adopt lightweight networks, _i.e._ Adapter [20; 59], to serve the hidden feature adaptions and the short-cut connections of DAS for VLP models.

To validate DAS, we apply it to a set of VLP models, namely including , ViLT  and LaVIN 2, on three VL benchmarks, namely VQA2.0 , NLVR2 and Flickr30K . The experimental results not only show the competitive performance of DAS against the fully finetune and PETL methods [17; 21; 59; 26], but also witness its great advantage in reducing the computation complexity of VLP models. For instance, DAS can help METER achieve \(96.60\%\) performance of full tuning on the VQA2.0 benchmark with only \(1.65\%\) trainable parameters, while decreasing \(11.97\%\) FLOPs. For the practical deployment of a specific VL task, DAS can reduce up to \(93.75\%\) parameters of the VLP models 3. These results well confirm our assumption about the redundancy of VLP models on downstream tasks, and also validated the design of the proposed DAS.

Overall, our contributions can be summarized as three-fold:

* We raise a new problem called _parameter and computation efficient transfer learning_ (PCETL) for VLP models, which not only requires to keep the scale of training parameters but also needs to reduce the computation complexity of VLP models on downstream tasks.
* We propose a novel _Dynamic Architecture Skipping_ (DAS) for PCETL, which can explore the optimal short-cut pathway in VLP models with the combination of parallel adapters.
* On two VLP models and three benchmark datasets, the proposed DAS not only reduces the computation overhead by a large extent, _e.g._, \(-11.97\%\) FLOPs of METER on VQA2.0, but also is on par with existing PETL methods in terms of parameter and performance.

## 2 Related Work

### Vision-Language Pre-training

In recent years, the advancement in natural language processing (NLP) [32; 36] also sparks the prevalence of large-scale pre-training in vision-language (VL) research [4; 10; 24; 28; 35; 56; 74]. In particular, VL pre-training also accomplishes self-supervised learning on massive image-text pairs based on the generative prediction tasks, _e.g. masked language modeling_ (MLM) and _masked image modeling_ (MIM). Furthermore, _Image Text Matching_ (ITM) [10; 28] is also applied to align two modalities. In terms of network architecture, most VLP models are equipped with two modality encoders to extract the visual and text features, _e.g._ BERT  and Faster-RCNN , respectively, based on which a stack of Transformer-based layers are deployed for cross-modal fusions [4; 10; 23; 24; 28; 34; 38; 39; 56; 61; 63]. For instance, ViL-BERT  and LXMERT  contain two independent Transformer-based branches  for region and text feature extractions, and another Transformer block is used for cross-modal interaction. To simplify the framework, Visual-BERT , VL-BERT  and UNITER  abandon additional branches and directly feed features into a single Transformer network. Then Pixel-BERT , CLIP-ViL , and METER  break the limitation of object detection backbones by directly applying grid features for multi-modal pre-training. To further simplify the model complexity, ViLT  directly feeds the word embeddings and image patches into the Transformer blocks. Additionally, CLIP  applies cross-modal contrastive learning for vision-language alignment with a shallow fusion layer for prediction. Overall, these VLP models often require more network branches due to the increase of modalities, resulting in more parameter and computation overheads. In this paper, we present the first attempt to evaluate their network redundancy on downstream tasks.

### Parameter Efficient Transfer Learning

Parameter Efficient Transfer Learning (PETL) [13; 17; 20; 21; 46; 47; 48; 58; 59; 60; 71; 72] aims to approach the fully-tuned performance on downstream by updating a small number of parameters. One of the main methodology in PETL is prompt-tuning [1; 5; 26; 36; 37; 50; 52; 74; 75], which is originally designed for large pre-trained language models such as GPT-3 . Concretely, the hand-craft prompts [50; 52] expand the original input sequence with natural language and regard all problems as a generation task. To better fit downstream tasks, soft prompt tuning methods [26; 36; 75] replace the handcraft prompts with a sequence of trainable embeddings. In addition to prompt-tuning, adapter-based [13; 20; 46; 47; 59; 72] methods insert lightweight feed-forward networks into VLP models, and these methods transfer VLP models by projecting hidden features onto the downstream distributions [20; 59]. Furthermore, LoRA  is proposed to transfer VLP models without additional calculation overhead in the inference stage by updating the low-rank parts of the original parameters. Besides, Side-tuning  runs in parallel with the pre-trained models to adapt downstream tasks while overcoming the constraint from the concrete structure. In addition, LST  stacks the outputs of the pre-trained modules in a parallel path. Without feedback to the VLP model, LST alleviates the memory requirement in the transfer while increasing the computation overhead. Compared to fine-tuning the entire model, PETL methods significantly improve the efficiency in transferring VLP models to downstream tasks. However, all of the above methods take the original VLP model as the upper bound of inference efficiency. In this paper, the proposed DAS method is the first to reduce the computation of VLP models while maintaining competitive performance. In terms of computation efficiency, network compression methods can also reduce the computation overhead,but they often require to fully tune the model on the downstream tasks, such as LayerDrop , EfficientVLM  and J2C . This setting make them against the target of PCETL about parameter efficiency.

## 3 Preliminary

We first revisit the principle of PETL methods for VLP models. Given a vision-language pre-trained (VLP) model, denoted as \(G()\), the target of PETL is to achieve the parameter-efficient adaption on the downstream task, which can be summarized as

\[*{argmin}_{}G(I,T|[,]),\] (1)

where \(=\{_{1},_{2},..,_{n}\}\) represent the parameters of \(n\) layers in the VLP model, and \(\) is usually frozen in PETL \((I,T)\) denotes the image-text pair, and \(\) is a small number of updated parameters. Since all VLP layers are reserved on downstream tasks, PETL methods can only reduce the parameter expenditure but not the computation of VLP models. Moreover, most PETL methods often incur non-negligible latency during inference [21; 40].

According to the observation in Fig 1-(a), there exists obvious redundancy in the VLP models. To this end, the objective of the proposed task of _parameter and computation efficient transfer learning_ (PCETL) can be defined by

\[*{argmin}_{,}G(I,T|[_{},]),\] (2)

Figure 2: Illustration of _Dynamic Architecture Skipping_ (DAS). DAS regards the network skipping as a \(k\)-armed bandit problem, and evaluates the redundancy of each VL layer/block via numerous subnetwork samplings. The accumulated rewards are used to determine which layers can be skipped, and adapters are also used for feature adaptions and short-cut connections.

where \(}=\{_{k_{1}},_{k_{2}},...,_{k_{m}}\}\) is the parameters of VLP modules except the skipped ones. Via skipping the redundant layers in VLP models and the combination of PETL methods, VLP models can accelerate the inference speed and maintain the scale of updated parameters.

## 4 Dynamic Architecture Skipping

### Redundancy Estimation

In this paper, we propose a novel transfer learning approach called _Dynamic Architecture Skipping_ (DAS) towards the parameter and computation efficient adaption of VLP models.

DAS first observes the model redundancy to downstream tasks before skipping the layers of VLP models. In practice, we regard this process as a \(k\)-armed bandit problem, as illustrated in Fig. 2. Firstly, we define the degree of redundancy as \(^{n}\), where \(n\) is the number of VLP modules. To correctly estimate the redundancy, we equally initialize \(_{i}=0\) and update it momentously.

In each training step, we skip \(m\) modules according to uniform distribution based on \(\), and train the sampled architectures on the downstream data. For \(t\)-th step, the action policy \(_{i}^{(t)}\) for the \(i\)-th VLP module follows the distribution:

\[_{i}^{(t)} U(0,(_{i}^{(t)})),\] (3)

where \(U(a,b)\) is the uniform distribution between \(a\) and \(b\). And \(\) represent the Sigmoid function. We randomly pick a probability from the \(_{i}^{(t)}\) of each module as the score \(s_{i}^{(t)}\). According to the score \(s_{i}^{(t)}\), the sampled subnetwork can be defined by

\[ G_{s}&=g_{0} g_{1}...  g_{n},\\ where& g_{i}=\{_{i},i\{j|s _{j}^{(t)}<s_{m}^{(t)}\},\\ _{i},i\{j|s_{j}^{(t)} s_{m}^{(t)}\}..\] (4)

Here, \(g_{i} g_{i+1}\) represents the compositional function \(g_{i}(g_{i+1}())\). \(_{i}\) denotes the original VL module, and \(_{i}\) is the lightweight module like adapter for short-cut connection. And \(s_{m}^{(t)}\) are the \(m\) largest values in the picked scores. Here, the module with a larger \(_{i}^{(t)}\) is more likely to be skipped during training. Meanwhile, Eq. 4 also help \(_{i}\) learn pre-trained knowledge from \(_{i}\) in a distillation way .

Then, DAS observes the redundancy of VLP modules in a reinforcement learning manner, as shown in 2. DAS samples \(c\) candidate network structures and calculates their rewards according to their loss values during validation, _i.e._ reward \(v=e^{-loss}\). Based on the rewards, the degree of redundancy \(\) can be updated by

\[_{i}^{(t+1)}=_{i}^{(t)}+(v_{h}-_{j=1}^{c} v_{j}).\] (5)Here, \(v_{h}\) denotes the reward of the sampled subnetwork, where the \(i\)-th module is skipped. When its validation loss is larger than the mean value, it suggests that this skipped module is more redundant. Eq. 5 is conducted at short training intervals to makes sure that most subnetworks can be sufficiently validated via numerous samplings, and the theorem of large numbers can guarantee the optimality of search results. The detailed search procedure of DAS is illustrated in Algorithm. 1. Finally, according to the degree of redundancy \(\), we can select top-\(m\) layers to be skipped, thereby reducing the computation complexity of VLP models.

### Model Adapting

To reduce the updated parameter scale during adaptation, DAS also introduces lightweight adapters [20; 59] to serve the hidden feature transformations as well as the short-cut connections in VLP models. Typically, an adapter is constructed by two linear projection layers and an activation function in between:

\[adapter()=ReLU(_{in})_{out}.\] (6)

Here, \(_{in}^{d h}\) and \(_{out}^{h d}\) are two trainable matrices, where \(h d\). For the \(i\)-th VLP module, the adaptation can be defined by

\[_{i}=_{i-1}+VLP(_{i-1})+adapter(_{i- 1}),\] (7)

where \(_{i}\) is the output of the \(i\)-th component. In this manner, DAS can freeze most parameters in the VLP models during adaption, similar to existing PETL methods .

Notably, directly removing the redundant modules will make the subsequent layers to receive the hidden features with drastic changes. Meanwhile, we do not expect the fully tuning of the whole model. In this case, we apply the adapter to serve the short-cut connection of the skipped layers:

\[_{i}=_{i-1}+adapter_{r}(_{i-1}).\] (8)

In this way, DAS can not only bridge the gap between feature transformations, but also retain parameter efficiency. Based on the estimated redundancy, DAS skips the redundant modules and finds out the optimal pathway for the downstream task with the helps of adapters, as shown in Fig. 2.

## 5 Experiment

### Datasets and Experimental Setup

**Visual Question Answering**. We conduct experiments on VQA2.0 . Instead of answering the question in open-ended natural language, it is converted into a classification task with \(3,129\) classes. Following the previous setting [10; 28], the PETL methods and DAS are trained on the train and validation sets of VQA2.0, and we report the _test-dev_ results from the online evaluation 4.

**Natural Language for Visual Reasoning**. The NLVR\({}^{2}\) is a dataset for classifying triplets of two images and a question into two classes. Because its form is different from the setup of VLP models, which has two images in one VL example, we feed these triplet examples to the model following the default setting of ViLT  and METER . Under this setting, the paired images and the question are input to the network, respectively. And the classifier predicts the results according to the concatenation of two representations.

**Retrieval Task**. For cross-modal retrieval, we measure the performance on Flickr30K  re-splited by Karpathy _et al._. We initialize the predictor for similarity measurement from the pre-trained ITM head. During the training, we randomly sample \(15\) instances as negative samples.

### Implementation details

We validate DAS on two deep-fusion based VLP models, which are ViLT  and METER . In terms of ViLT, we update the parameters of the additional components, classifier, class token and modal-type embeddings, while the rest are frozen. Following the most conventional setting [17; 59], the width of hidden states in adapters is set to \(96\). And the hidden dimension of the adapter used for the skip connection is set to \(192\) to retain a certain capacity. The VLP model is first warmed up for one epoch. In this epoch, the subnetwork is randomly sampled according to the skipped number \(m\). Then the search runs \(2\) epochs and the redundancy observation is executed at \(10\)-th step per interval. Finally, the optimal architecture will be trained for another \(10\) epochs. Notably, the validation set is used during training for all methods. In terms of METER, we split its fusion layer into two modules, _i.e._ the vision and language ones, which are skipped separately. The hidden dimension of the adapter used as the skip connection is set to \(192\) for encoders and \(288\) for fusion layers. The rest settings are the same as ViLT. We conduct all experiments with a single NVIDIA Tesla A100 GPU and the settings not mentioned are the same as ViLT  and METER .

### Experimental results

#### 5.3.1 Quantitative analysis

**Comparation with PETL methods.** We first compare DAS with a bunch of PETL methods [17; 21; 26; 36; 59] on the VLP models, of which results are given in Tab. 1. Here, **the suffix of DAS** denotes the number of skipped layers, and "_fusion_" and "_global_" refer to the range of network skipping, _i.e._, only the fusion branch or the complete model. From Tab. 1, the first observation is that existing PETL methods can largely reduce the amount of updated parameters for VLP models. For instance, the prompt-based methods only require about 1M parameters for two VLP models, nearly 300 times less than full tuning. Meanwhile, their performance gap to fully tuning is also marginal, _e.g._, Scaled PA . However, we can also see that all these PETL methods cannot reduce the computation of VLP models, and some of them incurs obvious increases in FLOPs, _e.g._ +\(28.71\)G by Shallow Prompt  on METER. The most computation efficient one is LoRA , which applies the re-parameterization technique to merge the additional modules into the VLP model, taking no extra computations. However, its performance obviously lags behind other PETL methods and our DAS. In stark contrast, DAS is the only method that can reduce the computation overhead on the downstream VL tasks, _e.g._, -11.16G FLOPs by DAS\({}_{4}\)-Fusion on VQA. More importantly, its updated parameter scale is only slightly larger than Adapter and Scaled PA, while the overall performance is still competitive. These results well confirm the effectiveness of DAS towards PCETL.

**Ablation of the number of skipped layers.** In Tab. 2, we report the results of skipping different numbers of layers by DAS. In terms of METER, we can first observe that skipping a few layers

has limited impact on its performance, _e.g._, skipping up to 8 fusion layers only has about \(2.2\%\) performance drops, strongly suggesting the redundancy of this VLP model. However, DAS can only reduce about one layer for ViLT without notable performance degradations. To explain, METER is a deep VLP model with two independent encoders to extract the features of image and text, while ViLT processes the multi-modal information from image pixels and word embeddings. In this case, ViLT is more compact than METER for downstream tasks, and this is also reflected in their parameter scales and performance. In terms of METER, we can also see the difference in optimizing the fusion branch and the entire model, _i.e._ DAS-Fusion and DAS-Global. With the same number of skipped layers, DAS-Fusion can reduce more FLOPs since the length of multi-modal inputs is often larger than the single-modal one. Meanwhile, when evaluating the entire model, DAS often tends to reduce the layers in the language encoders 5, which also suggests that natural language understanding is often less difficult in the VL tasks. Overall, these results well confirm our motivation about the redundancy of VLP models in downstream VL tasks, especially the ones with independent encoders like METER.

**Reliability of DAS.** Considering that DAS is an RL-based search approach, we also examine its stability and reliability via comparing to random skipping, of which results are given in Fig. 3. It can be seen that DAS is consistently better than random skipping without regard to the number of skipping layers, well confirming its effectiveness. In particular, when the number of skipped layers increases, the performance deviation of random skipping will become much more obvious, _e.g._\( 1.33\) for skipping 8 layers. Instead, DAS is still more stable and superior than the random solution, which also suggests the importance of correct redundancy estimations. Overall, these results further confirm the effectiveness and reliability of the proposed DAS.

**Inference Efficiency.** We further compare the actual inference efficiencies of DAS and other methods. The computation overhead during both the training and testing stages are reported in Tab. 3. We can first observe that the PETL methods, _i.e._ LoRA, Adapter and Scaled PA, significantly reduce the computational burden during the training stage. However, during test, these methods lose their efficiency advantage. For instance, Scaled PA is \(-5.04\%\) slower compared to the full tuning method. In contrast, the proposed DAS enhances the efficiency in both phases. Specifically, DAS only takes similar computation overhead to the PETL methods in the training stage and improves inference efficiency by \(+19.23\%\). Overall, these results well confirm the effectiveness of the proposed DAS in the PCETL task.

   &  **Shipped** \\ **Number** \\  } &  &  &  \\   & & **total-dev** & **FLOPs** & **total-P** & **FLOPs** & **Per.** & **FLOPs** \\  Baseline & 0 & 75.28 & 1.686 & 81.28 & 0.996 & 78.28 & 1.346 \\   & 2 & 74.92 & -9.060 & 80.07 & -2.660 & 77.30 & -3.866 \\  & 4 & 74.80 & -11.166 & 80.11 & -1.446 & 77.46 & -7.656 \\  & 6 & 74.67 & -17.586 & -18.16 & -9.976 & 76.42 & 1.1376 \\  & 8 & 73.70 & -24.00G & 79.30 & -11.45G & 76.50 & -17.72G \\   & 2 & 75.24 & -3.960 & 81.37 & -2.196 & 78.31 & -3.086 \\  & 75.13 & -4.51G & 81.34 & -3.67G & -82.4 & -4.096 \\  & 6 & 75.02 & -5.066 & 80.04 & -4.22G & 77.53 & -4.646 \\  & 8 & 74.95 & -5.61G & 79.61 & -8.34G & 77.28 & -6.976 \\   \\  Baseline & 0 & 70.13 & 0.730 & 76.26 & 0.736 & 73.20 & 0.736 \\  DAS & 1 & 69.28 & -1.05G & 74.89 & -1.036 & 72.09 & -1.036 \\  & 2 & 67.64 & -2.79 & 73.00 & -2.796 & 70.32 & -2.796 \\  

Table 2: Ablation study of different numbers of skipped layers by DAS. “Fusion” denotes the skipping is only for the fusion branch, while “Global” refers to the entire METER.

Figure 3: The comparison between DAS and random skipping for METER on the VQA2.0.

   &  \\   &  **VQA** \\ **test-dev** \\  } &  **Additional** \\ **FLOPs** \\  } &  &  \\   & & **total-dev** & **FLOPs** & **Memory(G)** & **Time(h)** & **Memory(G)** & **Speed(Samples)** \\  Full Tuning & 77.43 & 0 & \(\)40 & N/A & 6.8 & 4.16 \\ LoRA & 74.00 & 0 & 21.5 & 27 & 6.8 & 4.16 (+0.00\%) \\ Adapter & 74.70 & +1.64G & 22.9 & 28 & 7.2 & 4.09 (-1.68\%) \\ Scaled PA & 75.11 & +1.12G & 23.1 & 30 & 7.1 & 3.95 (-5.04\%) \\  DAS\({}_{+}\)Global & 75.09 & -4.51G & 21.7 (search) \& 20.6 (train) & 10 (search) + 20 (train) & 6.5 & 4.57 (+9.35\%) \\ DAS\({}_{+}\)Fusion & 74.80 & -11.16G & 21.7 (search) \& 18.1 (train) & 10 (search) + 18 (train) & 6.5 & 4.96 (+19.23\%) \\  

Table 3: Computation overhead of different methods for METER on the VQA2.0.

[MISSING_PAGE_FAIL:9]

automatic network skipping. Second, DAS only regards the complete Transformer layers in VLP models as the skipping candidates, limiting its potential in pathway routing. In the future, we will extend the search space with more detailed components, such as MHA and FFN.

## 7 Conclusion

In this paper, we propose a new problem for vision-language pre-trained (VLP) models termed _parameter and computation efficient transfer learning_ (PCETL). Existing transfer learning solutions for VLP models can only save the parameter expenditure during downstream task adaption, _e.g._, the PETL ones, while the excessive computation is still a unconquered problem. To this end, we propose a novel approach called _Dynamic Architecture Skipping_ (DAS) towards effective PCETL. DAS can observe the redundancies of VLP modules to downstream tasks via a reinforcement learning based process, and then skip the redundant ones to speed up inference. Meanwhile, DAS also adopts lightweight adapters to serve the hidden feature adaptions and the short-cut connections, thereby reducing the scale of trainable parameters. On two VLP models and three VL tasks, DAS not only shows a great superiority in reducing computation, but is also on par with the PETL methods in terms of parameter overhead and performance.

## 8 Acknowledge

This work was supported by National Key R&D Program of China (No.2022ZD0118201), the National Science Fund for Distinguished Young Scholars (No.62025603), the National Natural Science Foundation of China (No. U21B2037, No. U22B2051, No. 62176222, No. 62176223, No. 62176226, No. 62072386, No. 62072387, No. 62072389, No. 62002305 and No. 62272401), the Natural Science Foundation of Fujian Province of China (No.2021J01002, No.2022J06001) and the China Fundamental Research Funds for the Central Universities (Grant No. 20720220068).