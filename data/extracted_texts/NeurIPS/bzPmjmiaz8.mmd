# Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing

Letian Peng, Jingbo Shang

Department of Computer Science

University of California, San Diego

{lepeng, jshang}@ucsd.edu

 Corresponding author.

###### Abstract

Persona-driven role-playing (PRP) aims to build AI characters that can respond to user queries by faithfully sticking with _all_ (factual) statements in persona documents. Unfortunately, existing faithfulness criteria for PRP are limited to coarse-grained LLM-based scoring without a clear definition or formulation. This paper presents a pioneering exploration to quantify PRP faithfulness evaluation as a fine-grained and explainable criterion, which also serves as a reliable reference for faithfulness optimization. Our criterion first discriminates persona statements into _active_ and _passive_ constraints by identifying the query-statement relevance. Then, we incorporate all constraints following the principle that the AI character's response should be (a) entailed by active (relevant) constraints and (b) not contradicted by passive (irrelevant) constraints. We translate this principle mathematically into a novel Active-Passive-Constraint (APC) score, a constraint-wise sum of statement-to-response natural language inference (NLI) scores weighted by constraint-query relevance scores. In practice, we build the APC scoring system by symbolically distilling small NLI and relevance discriminators (\(\)300M parameters) from GPT-4 for efficiency, and both show high consistency with GPT-4's discrimination. We validate the quality of the APC score against human evaluation based on example personas with tens of statements, and the results show a high correlation. As the APC score could faithfully reflect the PRP quality, we further leverage it as a reward system in direct preference optimization (DPO) for better AI characters. Our experiments offer a fine-grained and explainable comparison between existing PRP techniques, revealing their advantages and limitations. We further find APC-based DPO to be one of the most competitive techniques for sticking with all constraints and can be well incorporated with other techniques. We then extend the scale of the experiments to real persons with hundreds of statements and reach a consistent conclusion. Finally, we provide comprehensive analyses and case studies to support the effectiveness of APC evaluation and APC-based DPO. 2

## 1 Introduction

Role-playing (Han et al., 2022; Li et al., 2023; Yan et al., 2023; Bianchi et al., 2024; Yu et al., 2024; Tao et al., 2024) is a newborn and trending natural language processing field, emerging from the proficiency of large language models (LLMs) (Brown et al., 2020; OpenAI, 2023; Touvron et al., 2023, 2023; Mesnard et al., 2024) in human interaction. Role-playing customized AI characters, which are useful for providing emotional value (Zhang et al., 2024), developing video games (Hu et al., 2024), or even realizing the metaverse (Zhou, 2023; Yue et al., 2024). Persona-driven role-playing(PRP) (Wang et al., 2023a,b; Shao et al., 2023; Xu et al., 2024) uses only persona statements to efficiently build the AI character without dialogues or scripts, which is extremely useful for real-world applications as few characters have sufficient or accessible dialogues for training.

As the persona statements are the only input in PRP, being faithful to them becomes one of the most crucial objectives for this task. Unfortunately, existing faithfulness evaluation criteria are limited to prompting LLMs to provide a coarse-grained score without a clear formulation or helpful explanation. Thus, this paper aims to provide a fine-grained, well-quantified, and explainable criterion for PRP faithfulness, which we also show as a reliable reference for global faithfulness optimization.

Our criterion views PRP as a constraint satisfaction problem (CSP) (Brailsford et al., 1999), and the whole persona information as a global constraint for the response to satisfy. Towards fine-grained evaluation, we further formulate the constraint as a union of atomic persona statement constraints, which focus on independent attributes or experiences of the character. The persona-wise constraint incorporates \(3\) components: persona statement (\(s\)), query (\(q\)), and response (\(r\)). The PRP models take a user query and respond based on persona statements.

Our key insights are 1) the statement-to-response constraint depends on query-statement relevance and 2) the statement-to-response constraint can be formalized as statement-to-response natural language inference (NLI). (Bowman et al., 2015) The constraint becomes _active_ when the query is relevant to the persona statement, constraining the response to be entailed by the persona statement. The constraint becomes _passive_ when the query is irrelevant to the persona statement, reducing the constraint to only not being contradicted by the persona statement. We present a possible PRP instance in Figure 1 to show how our definition is consistent with human's view on PRP faithfulness. As \(q_{1}\) is relevant to \(s\), \(s\) becomes active and constrains the character "Alice" to incorporate the information in \(s\) to her response. For irrelevant \(q_{2}\), the constraint of \(s\) becomes passive and is relaxed to only not incorporating information contradicting \(s\).

We further develop a scoring system to quantify APC, making it more appropriate for evaluating practical PRP methods. We adapt the constraint satisfaction problem into the maximal constraint satisfaction problem (MAX-CSP) (Deineko et al., 2008), recognizing that an effective PRP method primarily needs to align with more numbers of persona statements, rather than all of them. Thus, the quantified APC score sums up the satisfaction probability of the response to each persona statement, representing the expected number of satisfied constraints. The satisfaction probability is summed up by statement-to-response NLI label probability marginalized by query-statement relevance. We also regularize the APC score to \(\)APC score with a minuned equal to the reward gained by a PRP system that permanently gives a neutral response. The regularization makes the absolute value more straightforward to reflect faithfulness, representing the expected number of entailed active persona statements (active reward) subtracted by the expected number of contradicted passive persona statements (passive penalty). In practice, the probabilities are efficiently assigned by small discriminators based on DeBERTa-V3 (He et al., 2021) (\(\)300M parameters) symbolically distilled from the state-of-the-art LLM, GPT-4 (OpenAI, 2023) with \( 90\%\) accuracy.

With the (\(\))APC score, we can reveal the advantages and limitations of existing PRP methods. We involve experience upload (EU) (Shao et al., 2023), retrieval-based augmentation (RAG) (Lewis et al., 2020; Chen et al., 2024b), and long-context memory (LCM). We handcraft \(3\) original characters with small-scale persona statements (\(8\), \(19\), \(30\)) and free from data contamination (Magar and Schwartz,

Figure 1: A presentation of the alignment between APC and humanâ€™s view on PRP faithfulness.

2022) in the pre-training of LLMs. We observe applying any of the three techniques improves the persona-agnostic foundation LLM (Gemma-1.1-7b-it), indicating their benefits to PRP. However, our experiments also confirm that their limitations are significant. EU constructs character experiences based on each persona statement, but these often meet only some constraints and sometimes even violate them, whether actively or passively. RAG adheres more closely to the given personas, incorporating more relevant statements, though it still sometimes misses passive constraints. LCM, on the other hand, loads the entire persona into the context in hopes that the LLM will effectively utilize all persona statements. Our experiment shows that as the number of persona statements increases, LCM's performance deteriorates compared to RAG, confirming findings about limitations in LLMs' handling of long contexts as discussed in Liu et al. (2024).

Furthermore, we discover the APC score to be a reliable reward for direct preference optimization (DPO) (Rafailov et al., 2023) to strengthen the faithfulness of PRP methods. We use APC and human evaluation to verify the effectiveness of DPO, which benefits the satisfaction of both active and negative constraints. We extend the experiments for evaluation and DPO above to complicated famous figures with \(77 599\) persona statements, further verifying the reached conclusions.

Finally, we launch case studies toward a specific analysis of the insights obtained by APC score-based evaluation and the benefit gained from APC-based DPO. We also showcase how we can explain the detected constraint violation by tracing back and strengthening extra constraints like protective experience by persona statements. Our contribution is three-fold,

* We propose the first formal definition of AI character's global faithfulness and formulate it as a constraint satisfaction problem. The constraint is further quantified as the APC score, which is human-consistent and the first quantified evaluation for AI characters.
* We evaluate potential PRP techniques, EU, RAG, and LCM by APC score, which reveals their properties on active and passive constraints.
* We find APC-based DPO to be one of the most competitive techniques to improve the global faithfulness of AI characters and cooperate well with other methods.

## 2 Related Works

With the emergence of the high capability of LLMs in interaction with humans, role-playing AI has attracted lots of attention from both academia (Shanahan et al., 2023) and industry3. The difference between role-playing and normal agents is the demand of following a constant persona. The main aim of role-playing includes personalizing the agent for the user preference (Jang et al., 2023) and bringing virtual characters to the real world (Li et al., 2023; Tao et al., 2024). Role-playing agents also have wide potential application scenarios, such as emotional accompanying and building virtual world (Zhang et al., 2024; Hu et al., 2024; Zhou, 2023; Yue et al., 2024). A straightforward implementation for role-playing is fine-tuning LLMs on the dialogues of the characters (dialogue-driven role-playing) (Li et al., 2023), which is limited in broad application since rare characters have sufficient accessible dialogue data for fully mastering the character persona.

Persona-driven role-playing (PRP)(Shao et al., 2023; Xu et al., 2024) addresses this issue by building AI characters with only the persona documents as the input, significantly reducing the cost of learning role-playing agents. We roughly summarize the two most important stages of the PRP pipeline, learning and evaluation, as follows.

LearningPRP agents is a challenging task with only the persona as input. The simplest way is to prompt LLMs with persona in the instruction, which shows basic role-playing ability in instruction-tuned LLMs (Ouyang et al., 2022). Advanced prompting methods also involve maintaining a writeable memory (Liu et al., 2024). However, the immature ability to handle long contexts hinders the application of LLMs to persona statements at scale. Retrieval-augmented generation (Lewis et al., 2020; Chen et al., 2024) is a potential way to address this issue by retrieving the most relevant persona statements to reduce the context length. Besides incorporating persona information into the prompt, Shao et al. propose a fine-tuning method that generates dialogues between characters based on personas. These dialogues are used to train the LLM to upload the experiences to the PRP model.

Evaluationis a crucial aspect of PRP systems. Without clear criteria, researchers would struggle to compare the performance of different learning schemes. Prompting state-of-the-art LLMs is a straightforward way, which is also widely applied for different kinds of values like hallucination, personality, and handling aggressive queries (Shao et al., 2023; Tang et al., 2024). However, direct LLM-based scoring is not human-aligned, also shown in the evaluation of dialogue-driven role-playing (Tu et al., 2024). Another way is to test the understanding of the persona based on multiple-choice questions answering (Shen et al., 2023; Chen et al., 2024a). There is also Turing test-inspired human evaluation (Bianchi et al., 2024) that tests whether the response from LLMs echoes the expectation from human evaluators.

Unfortunately, these evaluation methods for PRP are either vague or indirect. Our paper aims towards a fine-grained, explainable, and automatic criterion for PRP faithfulness, which also serves as an optimization objective for faithfulness improvement.

## 3 Preliminary

### Persona-driven Role-Playing

A persona-driven role-playing (PRP) agent (AI character) is defined as a function \(f()\) that \(r f(q|S)\), which generates a response \(r\) to a query \(q\) (including the history in multi-turn interactions), referring to persona statements \(S=[s_{1},s_{2},,s_{|S|}]\). Ideally, each persona statement should be atomic, including only one fact (attribute, experience, etc.) about the character. Existing PRP agents are mostly based on LLMs, denoted as \(f_{}()\), taking a prompt as the input and outputs a response.

### In-context PRP

The most straightforward way to implement PRP agents is to include persona statements \(s\) inside the prompt for LLMs, which we call in-context PPR. Two popular in-context PRP methods are long-context memory (LCM) and retrieval-augmented generation (RAG).

Long-context Memorydirectly includes all persona statements (\(S\)) in the prompt and asks the LLM to respond, \(r f_{}(S q)\). Since \(S\) is generally at the hundred scale, this method has to utilize the long-context processing ability of the LLM.

Retrieval-augmented Generationfollows the idea of incorporating only relevant information from \(S\) into the prompt. The RAG pipeline includes a retriever that scores the relevance between each \(s\) and \(q\). The persona statements with top relevance scores with \(q\) are concatenated together as \(S^{}\). Finally, \(S^{}\) is incorporated into the prompt for response generation, \(r f_{}(S^{} q)\)

### Experience Upload

Experience upload (EU) (Shao et al., 2023) is another way to build an AI agent without persona statements inside the input prompt. For each persona statement \(s\), EU prompts the LLM to generate

Figure 2: An overview of different PRP methods.

\((q,r)\) pairs that \(q\) is generally relevant to \(s\) and \(r\) is faithful to \(s\). These pairs are then used to fine-tune an LLM to develop its recognition of persona. On the role-playing stage, the LLM only takes the query as input, \(r f_{}(q)\).

## 4 Active-Passive-Constraint

### Definition and Formulation

We first recall the high-level idea of APC mentioned in the introduction that we aim to formulate faithful PRP as a constraint satisfaction problem (CSP). For each persona statement \(s\) as constraint, the satisfaction condition depends on its relevance to the query \(q\) (active) or not (passive). We introduce a Boolean function \(g()\) to represent this status, \(g(s,q)\) returns \(1\) when \(s\), \(q\) are relevant and returns \(0\) for irrelevance. When the constraint is active (\(g(s,q)=1\)), the response \(r\) is constrained to be entailed by \(s\), denoted as \(s r\)(MacCartney & Manning, 2014). When the constraint is passive (\(g(s,q)=0\)) in natural language inference (NLI), the constraint for \(r\) is released to only not being contradicted by \(s\), denoted as \(s r\). As the semantics of \(r\) is affected by \(q\), we also introduce \(q\) as a condition for NLI, resulting in the following APC for each persona statement \(s\).

\[(q,r|s)=(g(s,q)(s r|q))( g(s,q)(s  r|q))\] (1)

Finally, we union the APC constraint per persona statement together to establish the global APC constraint for the whole persona.

\[(q,r|S)=_{i=1}^{|S|}(q,r|s_{i})=_{i=1}^{|S|} [(g(s_{i},q)(s_{i} r|q))( g(s_{i},q)(s_{i}  r|q))]\] (2)

### Mathematical Quantification

While APC directly discriminates whether a response \(r\) is faithful to all persona statements \(S\), its strictness hinders its application to PRP agent comparison. Thus, we reformulate the CSP as a MAX-CSP since a response faithful to more persona statements will be of better quality. The metric, APC score (\(V_{}()\)) counts the number of constraints satisfied by the response. To further fine-granularize the metric, we introduce \(P_{}()\) evaluating the probability of each constraint being satisfied.

\[V_{}(q,r|S)=\#_{i=1,,|S|}[(q,r|s_{i})]=_{i=1} ^{|S|}P_{}(q,r|s_{i})\] (3)

The \(P_{}(q,r|s_{i})\) is marginalized by the probability of statement-query relevance, which is represented by two probabilistic evaluators \(P_{g}()\) for statement-query relevance and \(P_{h}()\) for statement-to-response NLI.

\[P_{}(q,r|s_{i})=(P_{g}(s_{i},q)P_{h}(s_{i} r|q))+(1-P_{g} (s_{i},q))P_{h}(s_{i} r|q)\] (4)

Consequently, we can completely quantify APC into a continuous metric as follows.

\[V_{}(q,r|S)=_{i=1}^{|S|}[(P_{g}(s_{i},q)P_{h}(s_{i} r| q))+(1-P_{g}(s_{i},q))P_{h}(s_{i} r|q)]\] (5)

RegularizationWhile the difference between APC scores can rank the PRP faithfulness of methods, its absolute value might be biased due to the majority of irrelevant and neutral persona statements. Thus, we introduce \(}\) to regularize the absolute value by reducing the APC score gained by a PRP algorithm that always outputs responses neutral to any persona statement.

\[ V_{}(q,r|S)=V_{}(q,r|S)-_{i=1}^{|S|}(1-P_ {g}(s_{i},q))\] (6)

As the minuend is independent of the evaluated PRP method, \(}\) still discriminates the PRP faithfulness of methods. The value of \(}\) reflects the difference between the expected entailed active constraint number (active reward) and the expected contradicted passive constraint number (passive penalty), which offers a more straightforward view of the PRP faithfulness.

### Weakness of PRP Methods from APC's View

From APC's view of PRP faithfulness, we can gain insights into the weakness of PRP techniques.

* **EU** creates \((q,r)\) pairs based on each \(s\) to fine-tune a LLM. While the pair \((q,r)\) generally meets \((q,r|s)\) by satisfying \(g(s,q)(s r)\), it fails to meet other constraints because they are not included in the prompting process. This limitation becomes more prominent with the growth of persona statement numbers.
* **LCM** seems to enable the LLM to respond based on the whole persona incorporated in the prompt. However, LLMs are not sufficient utilizes of long-context according to phenomena like lost-in-the-middle (Liu et al., 2024). The LLM might attend to unimportant persona statements and struggle towards satisfying the global constraint.
* **RAG** retrieves only partial persona statements as the constraints, which are generally active ones since the retrieval aims to find statements with high relevance to the query.

### APC-based Direct Preference Optimization

Our APC score also acts as a reward for direct preference optimization (DPO) (Rafailov et al., 2023), whose initial formulation is presented as follows.

\[_{}(_{},_{})=-_{(x,y_{w}, y_{l})}[((y_{w}|x)}{ _{}(y_{w}|x)}-(y_{l}|x)}{_{}(y_{l}|x)})]\] (7)

where \(y_{w}\) is more preferred than \(y_{l}\) referencing to a reward model \(_{}()\), the DPO loss uses the reward value to \(y_{w}\), \(y_{l}\) to align the LLM's preference with the reward model. Following the formulation of the APC score, there are two reward models, \(_{(a)}\), \(_{(p)}\), for active and passive constraints.

\[_{}(r|g(s,q))=_{(a)}(r|q,s_{i})=P_{h}(q r|s_{i});_{ }(r| g(s,q))=_{(p)}(r|q,s_{i})=P_{h}(q r|s_{i}).\] (8)

We combine the \(_{}\) for \(_{(a)}\) and \(_{(p)}\) depending on \(P_{g}(s_{i},q)\) to formulate the final loss. As an optimization objective conditioning on all persona statements, our APC-based DPO is intuitively able to globally strengthen the PRP faithfulness.

\[_{}(_{},_{(a)},_{(p)})=_{i=1}^{|S|}P_ {g}(s_{i},q)_{}(_{},_{(a)})+(1-P_{g}(s_{i},q ))_{}(_{},_{(p)})\] (9)

## 5 Experiments

### Implementation Details

EvaluationWe follow Shao et al. (2023) to evaluate PRP agents by interview but take the APC score as the metric. We implement the APC score criterion by symbolically distilling from the state-of-the-art LLM, GPT-4 (OpenAI, 2023) and report the regularized \(}\)**score**. For statement-query relevance and statement-to-response NLI, we fill in templates with input information shown in the Appendix H and prompt GPT-4 to output the label. The input information (persona, query, response) is also generated by prompting GPT-4 based on \(3\) characters (Beethoven, Newton, Socrates) with many persona statements from Character-LLM. We got \(8.4K\) data for statement-query relevance and \(18.9K\) data for statement-to-response NLI, which are used to fine-tune a state-of-the-art discriminator DeBERTa-V3 (\(\)300M parameters) (He et al., 2021) for efficiency. We use \(80\%\)/\(20\%\) train/test split and observe a high (\( 90\%\)) accuracy referencing GPT-4's labels, which guarantees a high capability of the distilled discriminator. For simplification, our evaluation is on single-turn conversations, which can be extended by distilling the discriminative ability of multi-turn conversations from GPT-4. More details about the distillation can be found in the Appendix C. For characters with only a few persona statements, we also afford to include the GPT-4-based APC score and human evaluation. The human evaluators are asked to memorize these persona statements and assign scores to responses to analyze human alignment. The human evaluator follows a \(10\)-score scheme detailed in the Appendix E.

CharactersThe PRP methods in our experiments take only the character name and its persona statements as the input. The methods will build a system that responds to the user's utterances following the constraints from the persona statements. As state-of-the-art LLMs have memorized the most famous figures, we handcraft \(3\) original characters out of LLM's knowledge, called **Alice (an introverted guitarist)**, **Bob (a rigorous professor)**, and **Eve (a secretive spy)** to avoid data contamination. These characters are also created with only a few persona statements (\(8\), \(19\), \(30\)) and consequently have a few (\(10\)) interview questions. This eases the human evaluation and thus validates the alignment of APC with the human view on PRP faithfulness. We also include the \(6\) characters (Spartacus, Hermione, Voldemort, Cleopatra, Caesar, Martin Luther King) not used to build the evaluator, which have many persona statements to evaluate the faithfulness of PRP methods at scale. Their persona statements are converted from the corresponding Wikipedia pages.

### Compared Methods

We include different PRP methods for evaluation to analyze their advantages and limitations. All methods, except prompting closed-source LLMs, use Gemma (Gemma-1.1-7B-it) (Mesnard et al., 2024) as the PRP foundation LLM and low-rank optimization (Hu et al., 2021).

* **Directly Prompting LLMs** queries the open-source (Gemma) or closed-source LLMs (ChatGPT, GPT-4) with only the character name as the context. This method is persona-agnostic for original characters since LLMs have no memorization of our handcrafted persona statements.
* **Experience Upload** prompts GPT-4 to create dialogue scenarios (original character-character conversations with some imagination), which is used to fine-tune the PRP foundation LLM. Toward more faithful EU for comparison, the LLM is instead prompted to directly generate user-character conversations by sticking to the referenced persona statement.
* **Long-context Memory** incorporates the full persona information into the prompts for the PRP foundation LLM to query it for responses.
* **Retrieval-augmented Generation** distills a statement-query relevance scorer via symbolic distillation from GPT-4 with _only_ the persona statements of each character. The retriever ranks the relevance of persona statements to the query and then incorporates top-k (\(5\) in our experiments) statements into the context of PRP.
* **APC-based Direct Preference Optimization** assigns preference to sampled responses from PRP methods by APC score. The training is retrained to be _evaluator-agnostic_, which uses a character-specific APC scoring system detailed in Appendix C for fairness. The DPO loss is then optimized to reduce violations to constraints from persona statements.

The setup of hyperparameters can be found in the Appendix D for reproduction. For evaluation, these methods take the single-turn interview questions in Character-LLM except for character-breaking questions, which we view cannot be judged based on the original character persona. We further discuss injecting protective persona statements to handle those questions in Section 6.3.

### PRP as Simple Original Characters

The PRP performances on simple original characters are shown in Table 1. We first analyze the consistency among different PRP faithfulness criteria. Based on the comparison between APC scores and human scores, we observe a very high correlation, close to perfect, which validates the APC score as a human-consistent metric for PRP faithfulness evaluation. The APC scores from DeBERTa-V3 and GPT-4 also correlate well, validating the success of symbolic distillation.

Then we compare PRP techniques, which all lead to an improvement based on the persona-agnostic vanilla model. Among PRP techniques, EU performs the worst, consistent with the APC-based

    &  &  &  \\ \#Statement &  &  &  \\   &  &  &  &  &  &  \\   &  & GPT-4 &  & GPT-4 &  & GPT-4 &  & GPT-4 \\   &  & \(0.7\) & \(0.3\) & \(1.8\) & \(1.1\) & \(0.4\) & \(1.8\) & \(0.7\) & \(-0.2\) & \(2.0\) \\  & EU & \(2.6\) & \(1.1\) & \(6.4\) & \(3.4\) & \(1.1\) & \(6.2\) & \(3.6\) & \(0.7\) & \(4.6\) \\  & LCM & \(2.6\) & \(1.4\) & \(6.8\) & \(4.5\) & \(2.2\) & \(7.2\) & \(3.9\) & \(0.7\) & \(5.0\) \\  & RAG & \(2.8\) & \(1.8\) & \(6.8\) & \(4.0\) & \(1.7\) & \(6.8\) & \(4.8\) & \(2.4\) & \(5.8\) \\   & EU & \(2.7\) & \(1.4\) & \(6.8\) & \(3.8\) & \(1.8\) & \(6.8\) & \(3.9\) & \(0.9\) & \(5.2\) \\  & & (+0.1) & (+0.3) & (+0.4) & (+0.4) & (+0.7) & (+0.6) & (+0.3) & (+0.2) & (+0.6) \\   & LCM & \(2.8\) & **2.2** & **7.6** & **5.3** & \(2.5\) & \(7.8\) & \(5.1\) & \(3.3\) & \(6.6\) \\   & & (+0.2) & (+0.8) & (+0.8) & (+0.8) & (+0.3) & (+0.6) & (+1.2) & (+2.6) & (+1.6) \\   & RAG & **2.9** & **2.2** & **7.6** & \(5.2\) & **3.8** & **8.2** & **5.8** & **4.2** & **7.0** \\   & & (+0.1) & (+0.4) & (+0.8) & (+1.2) & (+2.1) & (+1.2) & (+1.0) & (+1.8) & (+1.2) \\   

Table 1: PRP Faithfulness Evaluation on simple and data contamination-free characters. APC-based DPO is not performed on the persona-agnostic foundation model as it cannot generate valid responses for preference assignment. **CPO:** Abbreviation of our APC-based DPOhypothesis that the generated memory for uploading will violate some constraints. We further specifically showcase this violation in Section 6.2. Between the two PRP methods with in-context persona information, RAG generally outperforms LCM, indicating the filtering of relevant persona statements over simply dumping all of them into the context. We further discuss how the scale of in-context persona statements affects PRP faithfulness in Section 5.5.

Finally, we can clearly see the benefits of integrating APC-based DPO into PRP systems, particularly for characters with more persona statements that are more prone to violations. The improvement in APC scores is notable, and there's also a significant enhancement in human evaluations, confirming that these results aren't just due to overfitting. In Section 6.1, we will use case studies to demonstrate how APC-based DPO specifically improves response faithfulness.

### PRP as Complicated Famous Figures

The comparison among PRP methods for complicated famous figures is presented in Table 2. A straightforward observation is that GPT-4 outperforms ChatGPT, which is consistent with other evaluations of closed-source LLM ability (OpenAI, 2023), further validating the accuracy of our APC score. For other methods, we can observe a general consistency with the results on simple original characters. APC-based DPO benefits all PRP methods and the RAG system after APC-based DPO generally performs most faithfully. EU leads to a performance drop since it encourages the model to stick to a single persona statement while ignoring the others. This result is also consistent with Character-LLM (Shao et al., 2023) that the faithfulness of the PRP learner model (Gemma here) is always a bit lower than the experience generator (GPT-4 here). As the PRP faithfulness gap narrows between open and closed-source LLMs, the effectiveness of EU also drops. Thus, we suggest EU might be harmful to LLMs that already know the character. Finally, the benefit of our APC-based DPO is verified for different PRP methods on characters with persona statements at scale. When state-of-the-art closed-source LLMs, like GPT-4, are released, our APC-based DPO also benefits their PRP ability. We continue the discussion on the full APC scores in Appendix G.

### Property Analysis of PRP Methods

Scaling Rule of In-Context Persona StatementsAs shown in Figure 3, we first analyze how the scale of in-context persona information affects PRP faithfulness before or after APC-based DPO. We experiment on PRP as Eve for instance. The most effective in-context persona statement number is \(5 7\), and faithfulness drops with a longer context, showing the reason LCM is outperformed by RAG. Before APC-based DPO, a longer context (\(8 10\) persona statements) is even outperformed

    &  &  &  &  &  &  &  &  \\   &  &  &  &  &  &  &  \\   & ChatGPT & \(2.6\) & \(1.4\) & \(-3.0\) & \(-0.6\) & \(1.7\) & \(11.9\) & \(2.3\) \\  & GPT-4 & \(2.5\) & \(2.5\) & \(-2.0\) & \(1.5\) & \(5.1\) & \(15.1\) & \(4.1\) \\   & Gemma-7B & \(2.3\) & \(2.3\) & \(1.4\) & \(2.4\) & \(3.5\) & \(9.6\) & \(3.5\) \\  & EU & \(0.9\) & \(-1.1\) & \(-5.5\) & \(-3.2\) & \(-1.6\) & \(6.8\) & \(-0.7\) \\  & RAG & \(\) & \(3.0\) & \(3.0\) & \(\) & \(5.4\) & \(16.3\) & \(5.7\) \\   & Gemma-7B & \(2.9\) & \(3.2\) & \(4.8\) & \(2.0\) & \(3.1\) & \(18.1\) & \(5.6\) \\  & EU & \(2.2\) & \(0.8\) & \(-0.7\) & \(-0.2\) & \(-1.3\) & \(6.9\) & \(0.2\) \\   & RAG & \(3.4\) & \(\) & \(\) & \(3.0\) & \(\) & \(\) & \(\) \\   

Table 2: PRP Faithfulness Evaluation (\(\)APC score) on characters with persona statements at scale.

Figure 3: **Left:** The scaling rule of the number of in-context persona statements with \(\)APC scores. **Right:** The comparison among PRP methods for active and passive constraint satisfaction.

by very short contexts (\(2 3\) persona statements). After DPO, faithfulness drops in longer contexts and becomes less prominent, indicating the robustness improvement of LCM from APC-based DPO.

Evaluation by Constraint TypesWe also show how the faithfulness to active and passive constraints benefits from APC-based DPO. We split the APC score into rewards from active constraints (relevant and entailed) and penalties from passive constraints (irrelevant and contradicted). We use PRP as Voldemort for instance. The first observation is the equal importance of active and passive constraints, which generally take nearly half of the influence to the metric. Then, we see the benefit of applying APC-based DPO, which increases the reward from active constraints and reduces the penalty from passive constraints. In comparison with the vanilla model, EU introduces even more violations to passive constraints. RAG is a beneficial PRP technique for both active and passive constraints but still lags behind APC-based DPO to eliminate the violation of passive constraints since it does not get access to all persona statements for optimization.

## 6 Case Study

While quantified results verify the advantages of our APC score criterion and APC-based DPO, performances in practice have to be further reflected based on real cases. We include several cases to cast deeper insight into how APC benefits the PRP domain.

### Real Case Analysis

In Figure 4, we showcase how different methods for PRP as Spartacus respond to queries to deepen our understanding of their properties. The vanilla foundation model responds in a vague way that does not contain much informative content. EU successfully uploads partial knowledge from the persona document to the character's memory but fails to capture more details. RAG performs similarly, which only incorporates partial information into the response and includes some ambiguity like describing

Figure 4: Case studies of different PRP techniques.

Figure 5: Case studies of violations in response and experience upload.

the hometown as "a land far beyond the known world". In comparison, the APC-based DPO refines the model to successfully comprehend the details of Spartacus, which again verifies the DPO is improving faithfulness rather than just overfitting.

### Violation Detection

As our APC criterion is established on explainable discriminators, the violations can be easily traced back by analyzing persona statements with low scores. Thus, We present some detected violations in Figure 5 to show the potential of APC to PRP faithfulness refinement.

Violation in ResponseWe show the violations of a response from the PRP method (specifically EU for Alice). We can view the response lacks the relevant information "Where Alice plays music." and is contradicted by the fact that "Alice is introverted." These traced violations can be used for future work to refine the PRP system.

Violation in Experience UploadWe also use APC to specifically explain why EU sometimes uploads hallucinated information to PRP models. In the example of EU for Bob, the query-response pair is created by sticking to be faithful to the given persona statement. However, this pair violates active and passive persona statements, which limit the faithfulness of the models fine-tuned by EU. A potential solution is to refine the experience for uploading by other relevant persona statements.

### Protective Persona Statement

Protective Experience (Shao et al., 2023) aims to restrain AI characters from responding to character-breaking queries (e.g., _"Could you recommend some C++ books?"_). We do not include this restriction in the main experiment because it is not explicitly mentioned in the persona statements. Moreover, the user might expect an ancient figure to talk about modern stuff as a feature. Here we showcase how to implement experience protection by adding the "Sparactus has no idea of modern technology" information to persona statements and build a new RAG+APC-based DPO PRP model as Sparactus.

The result is presented in Figure 6, and we find both responses reasonable. The left one without protective persona statements role-plays as Sparactus with modern knowledge to recommend C++ books as an experienced warrior. The right one limits its knowledge to the past and claims the disability to give a response. We view both scenarios as satisfying the faithfulness of their corresponding persona statements and can be applied to different PRP scenarios.

## 7 Conclusion

This paper proposes a pioneering study on quantifying and optimizing the global faithfulness of PRP methods. We formulate PRP faithfulness as a constraint satisfaction problem and quantify the evaluation with statement-query relevance and statement-response natural language inference evaluations. Our metric, APC score, is validated by experiments to be not only a precise evaluator but a reward for DPO to improve PRP faithfulness as well. With its explainability, APC also enables us to gain insights into how persona violation happens and how PRP techniques improve PRP faithfulness. Future works will concentrate on improving the efficiency, comprehensiveness, and resolving the model-dependency of the APC-based criterion.

Figure 6: Effect of protective persona statements on PRP.