# Optimal Convergence Rate for Exact Policy Mirror Descent in Discounted Markov Decision Processes

Emmeran Johnson

Department of Mathematics

Imperial College London

emmeran.johnson17@imperial.ac.uk

&Ciara Pike-Burke

Department of Mathematics

Imperial College London

c.pike-burke@imperial.ac.uk

&Patrick Rebeschini

Department of Statistics

University of Oxford

patrick.rebeschini@stats.ox.ac.uk

###### Abstract

Policy Mirror Descent (PMD) is a general family of algorithms that covers a wide range of novel and fundamental methods in reinforcement learning. Motivated by the instability of policy iteration (PI) with inexact policy evaluation, PMD algorithmically regularises the policy improvement step of PI. With exact policy evaluation, PI is known to converge linearly with a rate given by the discount factor \(\) of a Markov Decision Process. In this work, we bridge the gap between PI and PMD with exact policy evaluation and show that the dimension-free \(\)-rate of PI can be achieved by the general family of unregularised PMD algorithms under an adaptive step-size. We show that both the rate and step-size are unimprovable for PMD: we provide matching lower bounds that demonstrate that the \(\)-rate is optimal for PMD methods as well as PI, and that the adaptive step-size is necessary for PMD to achieve it. Our work is the first to relate PMD to rate-optimality and step-size necessity. Our study of the convergence of PMD avoids the use of the performance difference lemma, which leads to a direct analysis of independent interest. We also extend the analysis to the inexact setting and establish the first dimension-optimal sample complexity for unregularised PMD under a generative model, improving upon the best-known result.

## 1 Introduction

The problem of finding an optimal policy in tabular discounted Markov Decision Processes (MDPs) was classically solved using dynamic programming approaches such as policy iteration (PI) and value iteration (VI) . These methods are well understood theoretically and are guaranteed to converge linearly to the optimal policy in the tabular setting with a rate equal to the discount factor \(\) of the MDP . Recently, increased interest has been devoted to the study of policy-gradient (PG) approaches based on optimising a parameterised policy with respect to an objective .

Given their popularity, it is of interest to better understand PG methods and determine if their guarantees match those of classical algorithms in tabular MDPs. Among the recent works focused on understanding these methods in the tabular setting,  study a general family of algorithms known as Policy Mirror Descent (PMD). PMD algorithmically regularises the policy improvement step of PI and as such can be seen as a version of regularised PI, without actually regularising the objective of interest. It is also viewed as a policy-gradient method through its connection to mirror descent . Linear convergence of PMD was established by , though their rate depends on an instancedependent factor that can scale with the dimension of the problem, such as the size of the state space. For a specific instance of PMD known as Natural Policy Gradient (NPG),  showed that an instance-independent \(\)-rate is achievable, although their results do not cover general PMD. In MDPs where the objective is regularised, the \(\)-rate has been established for PMD . The classical approaches (PI and VI) achieve the \(\)-rate without regularisation, revealing that regularisation is, in general, not necessary for algorithms to reach the \(\)-rate. This motivates the following questions:

_Can the classical linear \(\)-rate be matched by unregularised policy-gradient algorithms? And what is the best rate that unregularised policy-gradient methods can achieve?_

For PMD, our work answers the first question positively and answers the second by establishing that the \(\)-rate is in fact the best rate achievable for PMD as well as for a more general family of algorithms (see Section 4.1). PMD allows for a choice of a mirror map that specifies different algorithms. Among these, NPG and PI are two ubiquitous instances of PMD each corresponding to their own mirror map. However, PMD is much more general and other mirror maps will lead to alternative algorithms endowed with the guarantees of PMD that we establish in this paper. In particular, the correspondence of mirror maps with exponential families  allows us to specify a wealth of valid mirror maps. This illustrates that PMD is a general framework that encompasses a wide range of novel but also fundamental algorithms, and motivates the study of its convergence guarantees. In this work, we make the following contributions and summarise them in Table 1,

* We recover the \(\)-rate for the general family of PMD algorithms with exact policy evaluations under an adaptive size (see the third bullet point below). In particular, Theorem 4.1 establishes the following bound in \(_{}\)-norm for the value \(V^{^{k}}\) of the policy \(^{k}\) after \(k\) iterations of PMD compared to the value \(V^{^{}}\) of an optimal policy \(^{}\), \[\|V^{^{}}-V^{^{k}}\|_{}^{k},\] providing guarantees for any starting-state distribution. This matches the rate of VI and PI as well as the best known rates for PMD on regularised MDPs. This is also the first fully dimension-independent linear convergence result for unregularised PMD, by which we mean that there is no dependence on the size of the state space or the action space.
* We provide a matching non-asymptotic lower-bound in Theorem 4.2, establishing the \(\)-rate as the optimal rate for PMD methods in early iterations. Our results show that a particular choice of learning rate allows PMD to reach this lower-bound exactly. Note that an asymptotic lower-bound is not possible due to the exact convergence of PI in finite iterations (see Section 4.1 for a discussion of the significance of this non-asymptotic lower-bound).
* The \(\)-rate for PMD in Theorem 4.1 relies on an adaptive step-size, where the adaptivity comes from the fact that the step-size depends on the policy at the current iteration (see Section 4). In Theorem 4.3 we show that this adaptivity is necessary for PMD to achieve the \(\)-rate, establishing our step-size as both sufficient and necessary.
* We establish a novel theoretical analysis that avoids the use of the performance difference lemma . This leads to a simple analysis and avoids needing to deal with visitation distribution mismatches that lead to dimension dependence in prior work.
* By extending our analysis to the inexact setting, with an approach similar to that of in , we establish an instance-independent sample complexity of \((||||(1-)^{-8}^{-2})\) under a generative model, where the notation \(()\) hides poly-logarithmic factors, \(\) is the state space of the MDP, \(\) is the action space and \(\) is the required accuracy. This improves on the previous best known sample complexity for PMD by removing the dependence on a distribution mismatch coefficient that can scale with problem-dependent quantities such as the size of the state space. More generally, we highlight that the analysis we establish in the exact setting can easily be combined with any other scheme for estimating the Q functions (see Section 5), paving the way for further improvements in instance-independent sample complexity results should more efficient estimation procedures be developed.

Our contributions are primarily on establishing the optimal rate for general (not just NPG) _exact_ PMD where we assume access to the true action-values of policies, and the simplicity of the analysis. The sample complexity result in the inexact setting illustrates how our analysis can be easily extended to obtain improved results for inexact PMD. These contributions advance the theoretical understanding of PMD and rule out searching for instances of PMD that may improve on PI or NPG in the exact setting. The algorithmic differences between instances of PMD do not affect the performance of the algorithm beyond the step-size condition.

## 2 Related work

### Convergence rates for exact policy mirror descent

We first consider the setting where exact policy evaluation is assumed. In this setting, several earlier works have sub-linear convergence results for PMD [17; 33] and NPG specifically [3; 1], though these have since been improved to linear convergence results as discussed below. Note that the work of  refers to their algorithm as "Politex", which shares the same update as NPG.

A line of work has considered PG methods applied to regularised MDPs. In this setting, linear convergence has been established for NPG with entropy regularisation , PMD with strongly-convex regularisers  and PMD with convex non-smooth regularisers . The rates of convergence are either exactly \(\) or can be made arbitrarily close to \(\) by letting the step-size go to infinity.

In the setting of unregularised MDPs, which is the focus of this paper, linear convergence of the special case of NPG was established [11; 22] under an adaptive step-size similar to ours that depends on the current policy at each step. The bound of  has an additive asymptotic error-floor that can be made arbitrarily small by making the step-size larger, while for a similar step-size  does not have this term so we focus on this work. Their analysis relies on a link between NPG and PI and consists of bounding the difference in value between iterates of both methods.  also establish linear convergence for a number of algorithms including PMD, although it is in the idealised setting of choosing the step size at each iteration that leads to the largest increase in value. This step-size choice will make PMD at least as good as PI since arbitrarily large step-sizes can be chosen and PMD with an infinite step-size converges to a PI update. Since PI converges linearly, so will PMD. This does not establish linear convergence of PMD for step-sizes with a closed-form expression. However, linear convergence for unregularised general PMD was recently established by  under a geometrically increasing step-size. In general, their rate is instance-dependent and may scale with problem dependent quantities such as the size of the state space. For general starting-state distributions, this same instance-dependent rate was established by  for a variant of PMD which augments the update with an added regularisation term. We focus our comparison on the work of  rather than this work as the guarantees are equivalent in both but  do not directly study PMD and have a more complicated algorithm. A summary of our results compared to those of  and  is presented in Table 1 and discussed in more detail in Section 4. In terms of optimality,  provide a lower-bound for constant step-size NPG, though it only applies to MDPs with a single-state, which can be solved in a single iteration with exact policy evaluation as the step-size goes to infinity (for which the lower-bound goes to \(0\)). We provide a lower-bound in Theorem 4.2 that applies to PMD with arbitrary step-size on an MDP with any finite state space. To the best of our knowledge, prior to this work no lower-bound has been established in this general setting.

    & Linear & General & \(_{}\) & Dimension & Matching & Step-Size \\  & \(\)-Rate & Mirror Map & Bound & Independent & Lower-Bound & Necessity \\ 
 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\
 & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ This work & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\   

Table 1: Comparison of contributions with prior work that study PMD.  focus on NPG, an instance of PMD for a specific mirror map (see Section 3). Their analysis is fundamentally different to ours as it exploits the closed-form update of NPG. Their step-size is similar to ours, although it has a dependence on a sub-optimality gap (see Section 4). Linear \(\)-rate holds if the linear convergence is with the \(\)-rate, not only linear convergence. The \(_{}\)-bound is satisfied if it holds for \(\|V^{^{*}}-V^{^{*}}\|_{}\). Dimension independence is satisfied when there is no instance for which the bound can scale with the size of the state or action space. We compare these works in more detail in Section 4.

### Sample complexity of inexact policy mirror descent

Sample complexity in the inexact policy evaluation setting refers to the number of samples needed to guarantee an \(\)-optimal policy is returned when we no longer have access to the Q-values exactly. We here give an outline of results, typically established in high-probability, under a generative model that we formally present in Section 5. The lower bound on the sample complexity in this setting was shown to be of \(|||}{(1-)^{3}^ {2}}\) by . This lower-bound can be reached by model-based approches [2; 26] and model-free approaches [34; 37].

The sample-complexity for PG methods has been recently studied in . Under a generative model, some works have considered PMD or NPG under various types of regularisation [13; 24; 41]. We focus on unregularised methods, for which results for PMD or its instances on tabular MDPs under a generative model are limited. There are works that obtain sample complexity results for NPG [3; 28] and for PMD  though they do not attain the optimal \(\)-dependence of \((^{-2})\).  show that a variant of PI, a special case of PMD, achieves the optimal \(\)-dependence of \((^{-2})\). More recently,  show that the general family of PMD methods match the \((^{-2})\) sample complexity with a factor of \((1-)^{-8}\). Our result for the inexact setting shares the same dependence on \(\) and \(1-\) as  but removes an instance-dependent quantity which can depend on the size of the state space. Further comparison to the result in  is given in Section 5. Beyond tabular MDPs and generative models,  study NPG under linear function approximation and off-policy sampling, though their results imply worse sample complexities when restricted to tabular MDPs under a generative model. PMD under linear function approximation [40; 4] and general function approximation  have also been studied and results similar to  were obtained in those settings.

## 3 Preliminaries

A Markov Decision Process (MDP) is a discrete-time stochastic process, comprised of a set of states \(\), a set of actions \(\), a discount factor \([0,1)\) and for each state-action pair \((s,a)\) a next-state transition function \(p(|s,a)()\) and a (assumed here deterministic) reward function \(r(s,a)\). \(()\) denotes the probability simplex over a set \(\). We consider both \(\) and \(\) to be finite, which is known as the tabular setting. In a state \(s\), an agent chooses an action \(a\), which gives them a reward \(r(s,a)\) and transitions them to a new state according to the transition function \(p(|s,a)\). Once they are in a new state, the process continues. The actions chosen by an agent are formalised through policies. A policy \(:()\) is a mapping from a state to a distribution over actions. We will often write it as an element in \(=()^{||}\). In each state \(s\), an agent following policy \(\) chooses an action \(a\) according to \(_{s}=(|s)()\).

In this work, the goal is to learn how to behave in a \(\)-discounted infinite-horizon MDP. We measure the performance of a policy with respect to the value function \(V^{}:\),

\[V^{}(s)=_{t=0}^{}^{t}r(s_{t},a_{t})|,s _{0}=s,\]

where \(s_{t},a_{t}\) are the state and action in time-step \(t\) and the expectation is with respect to both the randomness in the transitions and the choice of actions under policy \(\). This is a notion of long-term reward that describes the discounted rewards accumulated over future time-steps when following policy \(\) and starting in state \(s\). For a distribution over starting states \(()\), we write \(V^{}()=_{s}(s)V^{}(s)\) for the expected value when starting in a state distributed according to \(\). It is also useful to work with the state-action value \(Q^{}:\):

\[Q^{}(s,a)=_{t=0}^{}^{t}r(s_{t},a_{t})|, s_{0}=s,a_{0}=a,\]

which is similar to \(V^{}\), with the additional constraint of taking action \(a\) in the first time-step. We will often write \(V^{}^{||}\) (resp. \(Q^{}^{||||}\)) to refer to the vector form, where each entry represents the value (resp. action-value) in that state (resp. state-action pair). Similarly, we write \(Q^{}_{s}^{||}\) for the vector of action-values in state \(s\). The following expressions, which relate \(Q^{}\) and \(V^{}\) in terms of each other and when combined give the Bellman equations , follow from their definitions above,

\[V^{}(s)= Q^{}_{s},_{s}, Q^{}(s,a)=r(s,a)+ _{s^{}}p(s^{}|s,a)V^{}(s^{}).\]We now define the discounted visitation-distribution for starting state \(s^{}\) and policy \(\),

\[d^{}_{s^{}}(s)=(1-)_{t=0}^{}^{}^{ }(s_{t}=s|s_{0}=s^{}),\] (1)

which plays an important part in the study of PG methods. Note that \(^{}(s_{t}=s|s_{0}=s^{})\) is the probability of being in state \(s\) at time \(t\) when starting in state \(s^{}\) and following policy \(\). We also write \(d^{}_{}(s)=_{s^{}}(s^{})d^{}_{s^{ }}(s)\).

One of the main aims of reinforcement learning is to find a policy \(\) that maximises \(V^{}\). It is known that there exists a deterministic policy that simultaneously maximises \(V^{}\) and \(Q^{}\) for all states and actions . We call such a policy an optimal policy and denote it by \(^{*}\). We are interested in finding an \(\)-optimal policy, i.e a policy \(\) such that \(\|V^{^{*}}-V^{}\|_{}<\).

### Exact policy mirror descent

We are interested in PG methods that are based on optimising a parameterised policy \(_{}\) with respect to \(V^{_{}}()\) for some \(()\). In the tabular setting, we can use the direct parameterisation of a policy \(_{}\), which associates a parameter to each state-action pair, i.e. we have \(_{}(a|s)=_{s,a}\). We will drop the subscript \(\) for notational convenience. The gradient of the value function with respect to this parameterisation  is given for each state-action pair (s,a) by

\[V^{}()=d^{}_{ }(s)Q^{}(s,a).\] (2)

Mirror Descent (MD, ) carries out gradient descent in a geometry that is non-Euclidean. Using \(-V^{}()\) as the minimising objective, the proximal perspective of MD gives an update of the form

\[^{k+1}=_{p}-_{k} V^{^{k}}( ),p+D_{h}(p,^{k})}\] (3)

where \(h:h\) is the mirror map (with \(h\)) and \(D_{h}\) is the Bregman divergence generated by \(h\). We require \(h\) to be of Legendre type , i.e strictly convex and essentially smooth (differentiable and \(\| h(x_{k})\|\) for any sequence \(x_{k}\) converging to a point on the boundary of \(h\)) on the relative interior (rint) of \(h\). The Bregman Divergence is defined as

\[D_{h}(,^{})=h()-h(^{})- h(^{}), -^{},^{}h.\]

As the objective \(V^{}()\) is non-convex in general , usual techniques from convex theory  are not applicable.

The presence of the visitation-distribution term \(d^{}_{}(s)\) in the gradient of the objective in (2) can slow down learning because it can lead to vanishingly small gradients when states are infrequently visited under the current policy \(\). To circumvent this issue, Policy Mirror Descent (PMD)  applies a variant of update (3) with a weighted Bregman divergence \(D^{}_{h}\) that matches the visitation distribution factors of the gradient \(D^{}_{h}(p,^{k})=_{s}d^{^{k}}_{}(s)D_{h}(p_{s},^{k}_ {s})\) where the mirror map \(h\) is now defined on a subset of \(^{||}\). The resulting update has for all states a factor of \(d^{^{k}}_{}(s)\) in both terms. The minimisation can then be applied for each state individually to get the PMD update

\[^{k+1}_{s}=_{p()}-_{k} Q ^{^{k}}_{s},p+D_{h}(p,^{k}_{s})}\] (4)

for all states \(s\). We will often add a superscript \(k\) to any quantity that is associated to \(^{k}\). For example, \(V^{k}(s)=V^{^{k}}(s)\). Similarly for \(^{*}\) and the superscript \(\). Exact PMD iteratively applies update (4) for some sequence of step-sizes \(_{k}>0\) and initial policy \(^{0}\). We call this algorithm exact because we assume access to the true state-action values \(Q^{k}\).

The update (4) of PMD considered in this work uses the true action-value \(Q^{}\). In prior work, PMD has sometimes been applied to regularised MDPs  where the action-value is augmented with regularisation and is no longer the true action-value. This is a different algorithm that converges to a policy that is optimal in the regularised MDP but not in the original unregularised MDP.

PMD is a general family that covers many algorithms, specified by the choice of mirror map \(h\). These will inherit the guarantees of PMD, which motivates the study of the convergence guarantees of PMD beyond specific instances. Taking \(h\) to be the negative entropy yields NPG, whose theoretical properties have attracted a lot of interest [3; 13; 22]. With a null Bregman Divergence, PMD recovers PI. PI is generated by a constant mirror map, which is not of Legendre type but the analysis still applies so all results on PMD remain valid for PI. In fact, PMD can be viewed as a form of regularised PI since the update (4) converges to a PI update as \(_{k}\), regardless of the mirror map. Beyond these, providing mirror maps that generate other Bregman Divergences will lead to different algorithms. In particular, every exponential family has a corresponding mirror map generating a unique Bregman Divergence , highlighting the generality of PMD.

## 4 Main results for exact policy mirror descent

In this section, we present our main results on the convergence of exact PMD. We first introduce some relevant notation. Fix a state \(s\) and an integer \(k 0\). Let \(_{s}^{k}=\{a:Q^{k}(s,a)=_{a^{}}Q^{k}(s,a^{})\}\) denote the set of optimal actions in state \(s\) under policy \(^{k}\). Denote by \(_{s}^{k+1}\) the set of greedy policies w.r.t \(Q_{s}^{k}\) in state s, i.e \(_{s}^{k+1}=p():_{a_{s}^{k}}p(a)=1}\). We are now ready to state our main result in the setting of exact PMD, which is proved in Section 6.

**Theorem 4.1**.: _Let \(\{c_{k}\}_{k_{ 0}}\) be a sequence of positive reals. Consider applying iterative updates of (4) with \(^{0}\) and step-sizes satisfying for all \(k 0\),_

\[_{k}}_{s}_{ _{s}^{k+1}_{s}^{k+1}}D_{h}(_{s}^{k+1},_ {s}^{k})}.\] (5)

_Then we have for all \(k 0\),_

\[\|V^{}-V^{k}\|_{}^{k}\|V^{}-V^{0 }\|_{}+_{i=1}^{k}^{-i}c_{i-1}.\] (6)

The sequence \(\{c_{k}\}_{k_{ 0}}\) plays an important role in both the step-size constraint (5) and the bound (6). In particular, different choices will lead to different guarantees. We focus on \(c_{i}=^{2(i+1)}c_{0}\) for some \(c_{0}>0\), giving a step-size with a geometrically increasing component. The resulting bound is

\[\|V^{}-V^{k}\|_{}^{k}\|V^{}-V^{0 }\|_{}+}{1-},\]

which converges linearly with the \(\)-rate, and matches the bounds of PI and VI as \(c_{0}\) goes to \(0\). PMD cannot do better as we will show in Theorem 4.2. We discuss other choices of \(\{c_{k}\}\) in Appendix C. We provide some simulations that demonstrate the behaviour of our step-size (5) and validate its benefits compared to the step-size of  in Appendix D.

**Comparison to :** Linear convergence of unregularised PMD was first established by  under a geometrically increasing step-size \(_{k}=_{0}/^{k}\). We discuss this step-size further and show the necessity of adaptivity to achieve the \(\)-rate in Section 4.2. Their rate of convergence is \(1-}\) where \(_{}\) is an instance-dependent term defined as follows

\[_{}=^{}}{ }_{},\]

where \(d_{}^{}\) is the visitation distribution defined in (1) under an optimal policy and \(\) is the starting-state distribution to which the bound applies, i.e the bound is on \(V^{}()-V^{k}()\). This \(_{}\) is at best \(\) when we use \(\) to be the stationary distribution of the optimal policy. However, in this case, the guarantee only applies to states on the support of this stationary distribution and provides no guarantees for other states. In general, it is unclear how \(_{}\) may scale in a specific MDP. In particular, it is possible to construct an MDP where \(_{}\) scales linearly with the size of the state space \(||\) (Appendix H.1). Though this MDP is somewhat trivial, it nonetheless illustrates how \(_{}\) can easily be large leading to slow rates of convergence. It is also not straightforward to obtain convergence in individual states from the bound in  due to the presence of \(\) in the denominator of the mismatch coefficient in \(_{}\). In contrast, we obtain the optimal \(\)-rate of convergence and our result holds in \(_{}\)-norm over all states so avoids having to deal with a starting-state distribution \(\) altogether.

This distribution mismatch coefficient commonly appears in convergence bounds in the literature [19; 32; 10; 33; 3], both under exact and inexact policy evaluation. For many of these papers mentioned, removing it would be of great interest though often does not appear possible. Our results show that it is removable for the general family of PMD algorithms to obtain dimension-free linear convergence. The techniques we use may be of interest for removing this coefficient in other settings.

**Comparison to :** The \(\)-rate was established by  for NPG, a specific instance of PMD for which the Bregman Divergence is the KL-divergence. The bound shown in their work is similar to the one implied by our result with \(c_{i}=^{2(i+1)}c_{0}\). Defining \(^{k}(s)=_{a}Q^{k}(s,a)-_{a _{s}^{k}}Q^{k}(s,a)\), the minimal sub-optimality gap in state \(s\) under \(^{k}\), then the step-size corresponding to their bound with the KL as Bregman Divergence is

\[_{k}_{s,_{s}^{k+1}_{s}^{k+ 1}}L_{k}+||+D(_{s}^{k+1}, _{s}^{k})(s)}},\]

where \(L_{k}=Lk\) for some constant \(L>0\). This highlights the connection with our step-size condition (5). In particular, they both have an adaptive component that depends linearly on the Bregman divergence between the current policy and the greedy policy and a non-adaptive component on which the bound depends. An important difference is that our step-size is independent of the sub-optimality gap \(_{k}(s)\), and will be robust to situations where this gap is small. We can construct a general family of MDPs for which we can make \(_{k}(s)\) arbitrarily small and the step-size of  will correspondingly become arbitrarily large (Appendix H.2). Despite the apparent similarities with our results, their analysis is significantly different to ours as it exploits the specific closed-form update of NPG to bound the difference in value with an update of PI. Our analysis applies to PMD for a general mirror map (not just NPG) and as such does not utilize specific properties of the mirror map and does not require the analytic solution of the update to be known. Our analysis also easily extends to inexact PMD (see Section 5), which theirs does not.

Computing the step-size:The expression (5) for the step-size can be simplified. The minimum over \(_{s}^{k+1}_{s}^{k+1}\) can be removed by taking any \(_{s}^{k+1}_{s}^{k+1}\). It was stated with the minimum to have the smallest condition but that is not necessary. In fact, there will often only be one greedy action, i.e. \(_{s}^{k}\) will contain a single action and there will just be one policy in \(_{s}^{k+1}\). As for the maximum over \(s\), this condition is imposed because we are using the same step-size in all states for simplicity. If we allow different step-sizes in each state, then the step-size in state \(s\), denoted \(_{k}(s)\) would just have to satisfy (choosing any \(_{s}^{k+1}_{s}^{k+1}\))

\[_{k}(s)}D_{h}(_{s}^{k+1},_{s}^{k}).\]

The computational complexity of the step-size is then that of the Bregman divergence between two policies. Note that the step-size (5) is always finite. It can be unbounded when \(h\) is of Legendre type on the relative interior of \(()\) (as for the negative entropy) but then all iterates will belong to the relative interior of \(()\) and the Bregman divergences are well defined so the step-size is finite.

### Optimality of PMD

We have established in Theorem 4.1 that PMD achieves a linear \(\)-rate. The following result shows that this rate is in fact optimal in a worst-case sense. The proof can be found in Appendix E.

**Theorem 4.2**.: _Fix \(n>0\). There exists a class of MDPs with \(||=2n+1\) and a policy \(^{0}\ \) such that running iterative updates of (4) for any positive step-size regime, we have for \(k<n\):_

\[\|V^{}-V^{k}\|_{}^{k}\|V^{}-V^{0}\|_{ }.\] (7)

A key feature of this result is that the bound holds for \(k<n\). For a fixed iteration budget, Theorem 4.2 implies that there exists an MDP on which PMD will not do better than the linear \(\)-rate for any step-size. The \(\)-rate for PMD that we prove in Theorem 4.1 is optimal in this sense.

Lower-bound beyond \(\)?For a fixed state-space size \(||=2n+1\), it is not known what lower-bound holds when \(k n\). However, PI is an instance of PMD (see Section 3.1) and thus a lower bound that scales with \(^{k}\) for all \(k>0\) cannot hold since PI converges exactly in finite-iterations(in fact with a number of iterations that scales linearly with \(||\)[31, Theorem 3]). However, an asymptotic lower-bound for PMD under arbitrary step-size will not hold even if we exclude PI from the class of considered algorithms. As the step-size tends to infinity, any PMD update recovers a PI update. This implies that general PMD can be arbitrarily close to PI's exact convergence for the same finite number of iterations. Thus, any lower-bound on the convergence of PMD must be limited to finite iterations. Since the finite iteration convergence of PI only scales linearly with \(||\)[31, Theorem 3], the number of iterations guaranteed by Theorem 4.2 has the same dependence on \(||\) as the number of iterations needed for exact convergence of PI. To the best of our knowledge, this lower bound on the value convergence of PMD scaling with \(^{k}\) is new. We expect this result may have been known for the special case of PI, though we could not find a proof of it in the literature. The works that establish a lower bound for PI do so in the setting of exact convergence to the optimal policy , not \(\)-accurate convergence, and for undiscounted MDPs .

Rate-OptimalityTheorem 4.2 establishes that the \(\)-rate is optimal in the first \(||/2\) iterations. This non-asymptotic optimality is justified by the discussion above, which highlights that a rate for PMD under arbitrary step-size can only be optimal for a finite number of iterations, and specifically up until the exact convergence of PI.

There are some results on the super-linear convergence of NPG in the literature, though these apply once you have a policy within some neighbourhood of the optimal policy or value.  establish such a result for NPG in the regularised case, and  in the unregularised case under additional conditions. Theorem 4.2 does not contradict this latter result as for the MDP considered in the proof, the super-linear convergence would kick-in for iterations beyond the \(k<n\) considered here.

Lower-bound for general PG methods:The lower bound of Theorem 4.2 in fact applies to any algorithm that at each iteration increases the probability of the current greedy action. The greedy action is the action with highest action-value for the current policy. This covers algorithms more general than PMD and in particular, includes the vanilla PG algorithm.

### Adaptive step-size necessity

We have established in Theorem 4.1 that PMD under an adaptive step-size achieves a linear \(\)-rate and in Theorem 4.2 that this rate is optimal for PMD. We now show the adaptivity is in fact necessary to achieve the \(\)-rate. This strengthens the notion of optimality from the previous section - both the rate and step-size are unimprovable. The proof can be found in Appendix F.

**Theorem 4.3**.: _Fix \(n>0\) and \(>0.2\). There exists an MDP with state-space of size \(||=2n+1\) and a policy \(^{0}\,\,\) such that running iterative updates of NPG (PMD with h as the negative entropy) that satisfy \(\|V^{}-V^{k}\|_{}^{k}(\|V^{}-V^{0}\|_{}+ {1-}{8})\) requires_

\[_{k_{i}} KL(_{s_{i}}^{k_{i}+1},_{s_{i}}^{k_{i}})/2 ^{k_{i}},_{s_{i}}^{k_{i}+1}_{s}^{k+1}\] (8)

_for \(i=1,...,n\) s.t \(k_{1}<k_{2}<...<k_{n}\) where \(\{s_{1},...,s_{n}\}\) are distinct states of the considered MDP._

This theorem states that there are at least \(n\) distinct iterations with \(n\) distinct states where the step-size has to be bigger than a quantity depending on the Bregman divergence between the current policy and its greedy policy in the considered state in order to achieve a linear \(\)-rate. This is precisely the notion of adaptivity that appears in the step-size condition of Theorem 4.1 and . Theorem 4.3 shows we cannot improve on this in general and provides justification for using an adaptive step-size instead of the one from . Beyond its necessity, the adaptivity of our step-size can be a strength: it is large when needed, small when not. This is illustrated by the simulations we provide in Appendix D.

Theorem 4.3 only holds for the particular case of the negative entropy (NPG). This limitation is due to technical reasons rather than fundamental ones. The proof relies on the closed form update of NPG and we don't have this for general mirror map (see Appendix F). However, this result for NPG is enough to establish the necessity of the step-size for PMD in general.

Sample complexity of inexact policy mirror descent under generative model

In the previous sections, we have assumed access to the state-action values \(Q_{s}^{k}\) to carry out the PMD update. In Inexact PMD (IPMD), we replace \(Q_{s}^{k}\) with an estimate \(_{s}^{k}\) giving the update

\[_{s}^{k+1}=_{p()}-_{k} _{s}^{k},p+D_{h}(p,_{s}^{k})}.\] (9)

Similarly to the exact case, IPMD iteratively applies update (9) for some sequence of \(_{k}>0\) and initial policy \(^{0}\,\), this time only assuming access to an inexact estimator of \(Q^{k}\).

We consider the setting of a generative model , which is a sampling model where we can draw samples from the transition probabilities \(p(|s,a)\) for any pair \((s,a)\). We borrow an estimator common in the literature (see e.g. ): for all state-actions pairs \((s,a)\), draw \(M_{k}\) trajectories of length or horizon \(H\), i.e samples of the form \((s_{0}^{(i)},a_{0}^{(i)}),(s_{1}^{(i)},a_{1}^{(i)}),...,(s_{H-1}^{(i)}, a_{H-1}^{(i)})_{i=1,...,M_{k}}\), where \(a_{t}^{(i)}\) is drawn from \(^{k}(|s_{t}^{(i)})\), \(s_{t+1}^{(i)}\) is drawn from \(p(|s_{t}^{(i)},a_{t}^{(i)})\) and \((s_{0}^{(i)},a_{0}^{(i)})=(s,a)\). Using these samples, we can define a truncated Monte-Carlo estimate of the values as follows,

\[^{k}(s,a)=}_{i=1}^{M_{k}}_{(i)}^{k}(s,a),_{(i)}^{k}(s,a)=_{t=0}^{H-1}^{ t}r(s_{t}^{(i)},a_{t}^{(i)}).\] (10)

We use these \(^{k}(s,a)\) to replace \(Q^{k}(s,a)\) in the PMD update step.  present a bound on the accuracy of this estimator which is restated in Appendix G. Following the same ideas as , we can extend Theorem 4.1 to the inexact setting. The following theorem establishes a sample complexity result, which is the sufficient number of calls to the generative model to obtain an \(\)-optimal policy. For simplicity, we focus on the step-size following from the choice \(c_{i}=^{2(i+1)}\).

**Theorem 5.1**.: _Consider applying iterative updates of (9) using the Q-estimator in (10) given access to a generative model with \(^{0}\,\) and step-sizes satisfying for all \(k 0\) (with the definitions of \(_{s}^{k}\) and \(_{s}^{k+1}\) suitably adjusted with \(_{s}^{k}\) instead of \(Q_{s}^{k}\)),_

\[_{k}_{s}_{_{s}^{k+1}_{ s}^{k+1}}_{s}^{k+1},_{s}^{k})}{^{2k+1}}}.\]

_Fix \(>0\). For any \((0,1)\), suppose the following are satisfied for all \(k 0\),_

\[K>, H} M _{k}=M}{2}|||}{ }.\]

_Then with probability at least \(1-\), \(\|V^{}-V^{k}\|_{}^{k}\|V^{}-V^{0}\|_{} ++}^{H}<\). Choosing \(K\), \(H\) and \(M\) to be tight to their lower-bounds, the corresponding sample complexity is \(|||}{(1-)^{3}^{2 }}\), where the notation \(()\) hides poly-logarithmic factors._

The proof can be found in Appendix G.1. The sample complexity established by  (Theorem 16) under a generative model and the same Q-estimator is \(|||}{(1-)^{3}^{2 }}^{}}{}_{}^{3}\).

In their work,  stresses the interest in reducing the dependence on both \(1/(1-)\) and the distribution mismatch coefficient in order to scale the PMD guarantees to more relevant settings such as function approximation. Theorem 5.1 partially resolves this matter by removing the dependence on the distribution mismatch coefficient, which may scale with the size of the state space (Appendix H.1). This makes the result dimension-optimal, which is crucial when scaling the results to large or infinite state or action spaces. The dependence on \(1/(1-)\) remains distant from the \(1/(1-)^{3}\) lower-bound of  (see Section 2). Whether this can be reached by PMD methods remains open, though using a more suitable Q-estimator than (10) with our step size regime and analysis, which extends to arbitrary Q-estimators, could bring the sample complexity closer to this.

## 6 Analysis

In this section, we present the proof of Theorem 4.1. A key component in establishing the \(\)-rate is avoiding the performance difference lemma that we state in Appendix B. In prior works, the quantity that we are looking to bound \(V^{}()-V^{k}()\) arises through the performance difference lemma. In particular,  use the lemma on \(_{s d_{s}^{*}}[ Q_{s}^{k},_{s}^{}-_{s}^{k+1}]\), which introduces a distribution mismatch coefficient in order to get a recursion. On the other hand, we extract the value sub-optimalities \(V^{}(s)-V^{k}(s)\) and \(\|V^{}-V^{k+1}\|_{}\) directly from \( Q_{s}^{k},_{s}^{}-_{s}^{k+1}\) in (12). This leads to an elegant analysis that may be of interest in the study of other methods, and ultimately allows us to remove distribution mismatch factors and obtain an exact \(\)-rate.

**Proof of Theorem 4.1:** Fix \(s\) and \(k 0\). From Lemma A.2, we have that \( Q_{s}^{k},_{s}^{k+1} Q_{s}^{k+1},_{s}^{k+1} =V^{k+1}(s)\). This decouples the dependencies on \(^{k}\) and \(^{k+1}\) below and is one of the ingredients that allows us to bypass the performance difference lemma. Using this,

\[ Q_{s}^{k},_{s}^{}-_{s}^{k+1} Q _{s}^{k},_{s}^{}-V^{k+1}(s) = Q_{s}^{k}-Q_{s}^{},_{s}^{}+ Q _{s}^{},_{s}^{}-V^{k+1}(s)\] \[-\|Q_{s}^{}-Q_{s}^{k}\|_{}+V^{}(s)-V^{k+1}(s),\] (11)

where the last step uses Holder's inequality. Now we use that the difference in state-action values of different policies for the same state-action pair propagates the error to the next time-step, which is discounted by a factor of \(\). Formally, for any state-action pair \((s,a)\),

\[Q^{}(s,a)-Q^{k}(s,a) =_{s^{}}p(s^{}|s,a)(V^{}(s^{})-V ^{k}(s^{}))\] \[_{s^{}}p(s^{}|s,a)\|V^{}-V^{k}\|_ {}=\|V^{}-V^{k}\|_{},\]

which is the same phenomenon that is responsible for the contraction of the Bellman operator. This gives \(\|Q_{s}^{}-Q_{s}^{k}\|_{}\|V^{}-V^{k}\|_{}\). Plugging into Equation (11),

\[V^{}(s)-V^{k+1}(s)-\|V^{}-V^{k}\|_{} Q_{s}^ {k},_{s}^{}-_{s}^{k+1}.\]

The rest of the proof relies on making the right-hand side of the above arbitrarily small by taking a large enough step size. Choose any greedy policy with respect to \(Q_{s}^{k}\), \(_{s}^{k+1}_{s}^{k+1}\),

\[V^{}(s)-V^{k+1}(s)-\|V^{}-V^{k}\|_{}  Q_{s}^{k},_{s}^{}-_{s}^{k+1}\] (12) \[ Q_{s}^{k},_{s}^{k+1}-_{s}^{k+1}\] (13)

where we use that \(_{s}^{k+1}\) is greedy with respect to \(Q_{s}^{k}\). We then apply Lemma A.1 or (16) to \(p=_{s}^{k+1}\),

\[ Q_{s}^{k},_{s}^{k+1}-_{s}^{k+1}_{s}^{k+1},_{s}^{k})-D(_{s}^{k+1},_{s}^{k+ 1})-D(_{s}^{k+1},_{s}^{k})}{_{k}} D(_{s}^{k+1}, _{s}^{k})/_{k}.\]

Combining with (13) and noting that this holds for any \(_{s}^{k+1}_{s}^{k+1}\), we have

\[V^{}(s)-V^{k+1}(s)-\|V^{}-V^{k}\|_{}}_{_{s}^{k+1}_{s}^{k+1}}D(_{s}^ {k+1},_{s}^{k}) c_{k}\]

from the step-size condition in the statement of the theorem. Rearranging and recalling that \(s\) and \(k\) were arbitrary, we can choose \(s\) where \(V^{}(s)-V^{k+1}(s)\) reaches its maximum value. We get

\[\|V^{}-V^{k+1}\|_{}\|V^{}-V^{k}\|_{}+c_{k},\]

and unravelling this recursion completes the proof. 

## 7 Conclusion

In this paper, we have shown that the general family of exact policy mirror descent algorithms in tabular MDPs under an adaptive step-size achieve the same dimension-free linear \(\)-rate of convergence of classical algorithms such as policy iteration. We provide matching lower-bounds that establish this rate as optimal for PMD and the adaptive step-size as necessary. We exploit a new approach to study the convergence of PMD, for which avoiding the performance difference lemma is a key element. Though the focus of our work is on the exact policy evaluation setting, the analysis naturally extends to the inexact setting, given access to an estimator of the action-value of a policy. We provide a result for a simple estimator under a generative model that improves upon the best-known sample complexity, although it still does not match the lower bound. Our method is general and applies to any estimator, meaning our result could be improved by a better estimator. Exploiting further algorithmic properties of PMD in the inexact setting may be needed to bridge the gap to the optimal sample complexity, and determine if PMD can match the lower bound in the inexact setting.