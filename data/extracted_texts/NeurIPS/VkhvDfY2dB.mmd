# Efficient Exploration in Continuous-time

Model-based Reinforcement Learning

 Lenart Treven

ETH Zurich

trevenl@ethz.ch &Jonas Hubotter

ETH Zurich

jhuebotter@student.ethz.ch &Bhavya Sukhija

ETH Zurich

sukhijab@ethz.ch &Florian Dorfler

ETH Zurich

dorfler@ethz.ch &Andreas Krause

ETH Zurich

krausea@ethz.ch

###### Abstract

Reinforcement learning algorithms typically consider discrete-time dynamics, even though the underlying systems are often continuous in time. In this paper, we introduce a model-based reinforcement learning algorithm that represents continuous-time dynamics using nonlinear ordinary differential equations (ODEs). We capture epistemic uncertainty using well-calibrated probabilistic models, and use the optimistic principle for exploration. Our regret bounds surface the importance of the _measurement selection strategy_ (MSS), since in continuous time we not only must decide how to explore, but also _when_ to observe the underlying system. Our analysis demonstrates that the regret is sublinear when modeling ODEs with Gaussian Processes (GP) for common choices of MSS, such as equidistant sampling. Additionally, we propose an _adaptive_, data-dependent, practical MSS that, when combined with GP dynamics, also achieves sublinear regret with significantly fewer samples. We showcase the benefits of continuous-time modeling over its discrete-time counterpart, as well as our proposed _adaptive_ MSS over standard baselines, on several applications.

## 1 Introduction

Real-world systems encountered in natural sciences and engineering applications, such as robotics (Spong et al., 2006), biology (Lenhart and Workman, 2007; Jones et al., 2009), medicine (Panetta and Fister, 2003), etc., are fundamentally continuous in time. Therefore, ordinary differential equations (ODEs) are the natural modeling language. However, the reinforcement learning (RL) community predominantly models problems in discrete time, with a few notable exceptions (Doya, 2000; Yildiz et al., 2021; Lutter et al., 2021). The discretization of continuous-time systems imposes limitations on the application of state-of-the-art RL algorithms, as they are tied to specific discretization schemes.

Discretization of continuous-time systems sacrifices several essential properties that could be preserved in a continuous-time framework (Nesic and Postoyan, 2021). For instance, exact discretization is only feasible for linear systems, leading to an inherent discrepancy when using standard discretization techniques for nonlinear systems. Additionally, discretization obscures the inter-sample behavior of the system, changes stability properties, may result in uncontrollable systems, or requires an excessively high sampling rate. Multi-time-scale systems are particularly vulnerable to these issues (Engquist et al., 2007). In many practical scenarios, the constraints imposed by discrete-time modeling are undesirable. Discrete-time models do not allow for the independent adjustment of measurement and control frequencies, which is crucial for real-world systems that operate in different regimes. For example, in autonomous driving, low-frequency sensor samplingand control actuation may suffice at slow speeds, while high-speed driving demands faster control. How to choose aperiodic measurements and control is studied in the literature on the event and self-triggered control (Astrom and Bernhardsson, 2002; Anta and Tabuada, 2010; Heemels et al., 2012). They show that the number of control inputs can be significantly reduced by using aperiodic control. Moreover, data from multiple sources is often collected at varying frequencies (Ghysels et al., 2006) and is often not even equidistant in time. Discrete-time models struggle to exploit this data for learning, while continuous-time models can naturally accommodate it.

Continuous-time modeling also offers the flexibility to determine optimal measurement times based on need, in contrast to the fixed measurement frequency in discrete-time settings, which can easily miss informative samples. This advantage is particularly relevant in fields like medicine, where patient monitoring often requires higher frequency measurements during the onset of illness and lower frequency measurements as the patient recovers (Kaandorp and Koole, 2007). At the same time, in fields such as medicine, measurements are often costly, and hence, it is important that the most informative measurements are selected. Discrete-time models, limited to equidistant measurements, therefore often result in suboptimal decision-making and their sample efficiency is fundamentally limited. In summary, continuous-time modeling is agnostic to the choice of measurement selection strategy (MSS), whereas discrete-time modeling often only works for an equidistant MSS.

ContributionsGiven the advantages of continuous-time modeling, in this work, we propose an optimistic continuous-time model-based RL algorithm - OCoRL. Moreover, we theoretically analyze OCoRL and show a general regret bound that holds for any MSS. We further show that for common choices of MSSs, such as equidistant sampling, the regret is sublinear when we model the dynamics with GPs (Williams and Rasmussen, 2006). To our knowledge, we are the first to give a no-regret algorithm for a rich class of nonlinear dynamical systems in the continuous-time RL setting. We further propose an _adaptive_ MSS that is practical, data-dependent, and requires considerably fewer measurements compared to the equidistant MSS while still ensuring the sublinear regret for the GP case. Crucial to OCoRL is the exploration induced by the _optimism in the face of uncertainty_ principle for model-based RL, which is commonly employed in the discrete-time realm (Auer et al., 2008; Curi et al., 2020). We validate the performance of OCoRL on several robotic tasks, where we clearly showcase the advantages of continuous-time modeling over its discrete-time counterpart. Finally, we provide an efficient implementation1 of OCoRL in JAX (Bradbury et al., 2018).

## 2 Problem setting

In this work, we study a continuous-time deterministic dynamical system \(^{*}\) with initial state \(_{0}^{d_{x}}\), i.e.,

\[(t)=_{0}+_{0}^{t}^{*}((s),(s))\, ds.\]

Here \(:[0,)^{d_{u}}\) is the input we apply to the system. Moreover, we consider state feedback controllers represented through a policy \(:^{d_{u}}\), that is, \((s)=((s))\). Our objective is to find the optimal policy with respect to a given running cost function \(c:^{d_{x}}^{d_{u}}\). Specifically, we are interested in solving the following optimal control (OC) problem over the policy space \(\):

\[^{*}}}{{=} }*{argmin}_{}C(,^{*})=* {argmin}_{}_{0}^{T}c(,())\,dt\\ }=^{*}(,()), (0)=_{0}.\] (1)

The function \(^{*}\) is _unknown_, but we can collect data over episodes \(n=1,N\) and learn about the system by deploying a policy \(_{n}\) for the horizon of \(T\) in episode \(n\). In an (overly) idealized continuous time setting, one can measure the system at any time step. However, we consider a more practical setting, where we assume taking measurements is costly, and therefore want as few measurements as necessary. To this end, we formally define a measurement selection strategy below.

**Definition 1** (Measurement selection strategy).: _A measurement selection strategy \(S\) is a sequence of sets \((S_{n})_{n 1}\), such that \(S_{n}\) contains \(m_{n}\) points at which we take measurements, i.e., \(S_{n}[0,T],|S_{n}|=m_{n}\).2_

During episode \(n\), given a policy \(_{n}\) and a MSS \(S_{n}\), we collect a dataset \(_{n}(_{n},S_{n})\). The dataset is defined as

\[_{n} }}{{=}}\{(_{n}(t_{n,i}), }_{n}(t_{n,i})) t_{n,i} S_{n},i\{1,m_{n}\}\} \] \[_{n}(t_{n,i}) }}{{=}}(_{n}(t_{n,i}), _{n}(_{n}(t_{n,i}))),}_{n}(t_{n,i})}}{{=}}}_{n}(t_{n,i})+_{n,i}.\]

Here \(_{n}(t),}_{n}(t)\) are state and state derivative in episode \(n\), and \(_{n,i}\) is i.i.d, \(\)-sub-Gaussian noise of the state derivative observations. Note, even though in practice only the state \((t)\) might be observable, one can estimate its derivative \(}(t)\) (e.g., using finite differences, interpolation methods, etc. Cullum (1971); Knowles and Wallace (1995); Chartrand (2011); Knowles and Renka (2014); Wagner et al. (2018); Treven et al. (2021)). We capture the noise in our measurements and/or estimation of \(}(t)\) with \(_{i,n}\).

In summary, at each episode \(n\), we deploy a policy \(_{n}\) for a horizon of \(T\), observe the system according to a proposed MSS \(S_{n}\), and learn the dynamics \(^{*}\). By deploying \(_{n}\) instead of the optimal policy \(^{*}\), we incur a regret,

\[r_{n}(S)}}{{=}}C(_{n},^{*})- C(^{*},^{*}).\]

Note that the policy \(_{n}\) depends on the data \(_{1:n-1}=_{i<n}_{i}\) and hence implicitly on the MSS \(S\).

Performance measureWe analyze OCoRL by comparing it with the performance of the best policy \(^{*}\) from the class \(\). We evaluate the _cumulative regret_\(R_{N}(S)}}{{=}}_{n=1}^{N}r_{n}(S)\) that sums the gaps between the performance of the policy \(_{n}\) and the optimal policy \(^{*}\) over all the episodes. If the regret \(R_{N}(S)\) is sublinear in \(N\), then the average cost of the policy \(C(_{n},^{*})\) converges to the optimal cost \(C(^{*},^{*})\).

### Assumptions

Any meaningful analysis of cumulative regret for continuous time, state, and action spaces requires some assumptions on the system and the policy class. We make some continuity assumptions, similar to the discrete-time case (Khalil, 2015; Curi et al., 2020; Sussex et al., 2023), on the dynamics, policy, and cost.

**Assumption 1** (Lipschitz continuity).: _Given any norm \(\), we assume that the system dynamics \(^{*}\) and cost \(c\) are \(L_{}\) and \(L_{c}\)-Lipschitz continuous, respectively, with respect to the induced metric. Moreover, we define \(\) to be the policy class of \(L_{}\)-Lipschitz continuous policy functions and \(\) a class of \(L_{}\) Lipschitz continuous dynamics functions with respect to the induced metric._

We learn a model of \(^{*}\) using data collected from the episodes. For a given state-action pair \(=(,)\), our learned model predicts a mean estimate \(_{n}()\) and quantifies our epistemic uncertainty \(_{n}()\) about the function \(^{*}\).

**Definition 2** (Well-calibrated statistical model of \(^{*}\), Rothfuss et al. (2023)).: _Let \(}}{{=}}\). An all-time well-calibrated statistical model of the function \(^{*}\) is a sequence \(\{_{n}()\}_{n 0}\), where_

\[_{n}()}}{{=}}\{: ^{d_{x}},  j\{1,,d_{x}\}:|_{n,j}()-f_{j}()|_{n}( )_{n,j}()\},\]

_if, with probability at least \(1-\), we have \(^{*}_{n 0}_{n}()\). Here, \(_{n,j}\) and \(_{n,j}\) denote the \(j\)-th element in the vector-valued mean and standard deviation functions \(_{n}\) and \(_{n}\) respectively, and \(_{n}()_{ 0}\) is a scalar function that depends on the confidence level \((0,1]\) and which is monotonically increasing in \(n\)._

**Assumption 2** (Well-calibration).: _We assume that our learned model is an all-time well-calibrated statistical model of \(^{*}\). We further assume that the standard deviation functions \((_{n}())_{n 0}\) are \(L_{}\)-Lipschitz continuous._This is a natural assumption, which states that we are with high probability able to capture the dynamics within a confidence set spanned by our predicted mean and epistemic uncertainty. For example, GP models are all-time well-calibrated for a rich class of functions (c.f., Section 3.2) and also satisfy the Lipschitz continuity assumption on \((_{n}())_{n 0}\)(Rothfuss et al., 2023). For Bayesian neural networks, obtaining accurate uncertainty estimates is still an open and active research problem. However, in practice, re-calibration techniques (Kuleshov et al., 2018) can be used.

By leveraging these assumptions, in the next section, we propose our algorithm OCoRL and derive a generic bound on its cumulative regret. Furthermore, we show that OCoRL provides sublinear cumulative regret for the case when GPs are used to learn \(^{*}\).

## 3 Optimistic Continuous-time RL Algorithm

``` **OnCoRL:** Optimistic Continuous-time RL

``` **Init:** Statistical model \(_{0}\), Simulator Sim, MSS \(S\), Probability \(\) for episode \(n=1,,N\)do \[_{n}=*{argmin}_{}_{ _{n-1}}C(,)\] /* Select optimistic policy /* \[_{n}=\{(_{n}(t_{n,i}),}_{n}(t_{n,i}))  t_{n,i} S_{n}\}(_{n},S_{n})\] /* Measurement collection /* \[_{n}(_{n},_{n},_{n}( ))_{1:n}\] /* Update statistical model/* ```

**Optimistic policy selection** There are several strategies we can deploy to trade-off exploration and exploitation, e.g., dithering (\(\)-greedy, Boltzmann exploration (Sutton and Barto, 2018)), Thompson sampling (Osband et al., 2013), upper-confidence RL (UCRL) (Auer et al., 2008), etc. OCoRL is a continuous-time variant of the Hallucinated UCRL strategy introduced by Chowdhury and Gopalan (2017); Curi et al. (2020). In episode \(n\), the optimistic policy is obtained by solving the optimal control problem:

\[(_{n},_{n})}}{{=}}* {argmin}_{,\ _{n-1}}C(,)\] (2)

Here, \(_{n}\) is a dynamical system such that the cost by controlling \(_{n}\) with its optimal policy \(_{n}\) is the lowest among all the plausible systems from \(_{n-1}\). The optimal control problem (2) is infinite-dimensional, in general nonlinear, and thus hard to solve. For the analysis, we assume we can perfectly solve it. In Appendix B, we present details on how we solve it in practice. Specifically, in Appendix B.1, we show that our results seamlessly extend to the setting where the optimal control problem of Equation (1) is discretized w.r.t. an _arbitrary_ choice of discretization. Moreover, in this setting, we show that theoretical guarantees can be derived without restricting the models to \(_{n-1}\). Instead, a practical optimization over all models in \(_{n-1}\) can be performed as in Curi et al. (2020); Pasztor et al. (2021).

Model complexityWe expect that the regret of any model-based continuous-time RL algorithm depends both on the hardness of learning the underlying true dynamics model \(^{*}\) and the MSS. To capture both, we define the following model complexity:

\[_{N}(^{*},S)}}{{=}} _{_{1},,_{N}\\ _{n}}_{n=1}^{N}_{0}^{T}_{n-1}(_{n}(t))^{2}\,dt.\] (3)

The model complexity measure captures the hardness of learning the dynamics \(^{*}\). Intuitively, for a given \(N\), the more complicated the dynamics \(^{*}\), the larger the epistemic uncertainty and thereby the model complexity. For the discrete-time setting, the integral is replaced by the sum over the uncertainties on the trajectories (c.f., Equation (8) of Curi et al. (2020)). In the continuous-time setting, we do not observe the state at every time step, but only at a finite number of times wherever the MSS \(S\) proposes to measure the system. Accordingly, \(S\) influences how we collect data and update our calibrated model. Therefore, the model complexity depends on \(S\). Next, we first present the regret bound for general MSSs, then we look at particular strategies for which we can show convergence of the regret.

**Proposition 1**.: _Let \(S\) be any MSS. If we run OCoRL, we have with probability at least \(1-\),_

\[R_{N}(S) 2_{N}L_{c}(1+L_{})T^{}e^{L_{}(1+L_{ })T}_{N}(^{*},S)}.\] (4)

We provide the proof of Proposition 1 in Appendix A. Because we have access only to the statistical model and any errors in dynamics compound (continuously in time) over the episode, the regret \(R_{N}(S)\) depends exponentially on the horizon \(T\). This is in line with the prior work in the discrete-time setting (Curi et al., 2020). If the model complexity term \(_{N}(^{*},S)\) and \(_{N}\) grow at a rate slower than \(N\), the regret is sublinear and the average performance of OCoRL converges to \(C(^{*},^{*})\). In our analysis, the key step to show sublinear regret for the GP dynamics model is to upper bound the integral of uncertainty in the model complexity with the sum of uncertainties at the points where we collect the measurements. In the next section, we show how this can be done for different measurement selection strategies.

### Measurement selection strategies (MSS)

In the following, we present different natural MSSs and compare the number of measurements they propose per episode. Formal derivations are included in Appendix A.2.

OracleIntuitively, if we take measurements at the times when we are the most uncertain about dynamics on the executed trajectory, i.e., when \(\|_{n-1}(_{n}(t))\|\) is largest, we gain the most knowledge about the true function \(^{*}\). Indeed, when the statistical model is a GP and noise is homoscedastic and Gaussian, observing the most uncertain point on the trajectory leads to the maximal reduction in entropy of \(^{*}(_{n}(t))\) (c.f., Lemma 5.3 of Srinivas et al. (2009)). In the ideal case, we can define an MSS that collects only the point with the highest uncertainty in every episode, i.e., \(S_{n}^{}}{=}\{t_{n,1}\}\) where \(t_{n,1}}{=}*{argmax}_{0 t T}\| _{n-1}(_{n}(t))\|^{2}\). For this MSS we bound the integral over the horizon \(T\) with the maximal value of the integrand times the horizon \(T\):

\[_{N}(^{*},S_{n}^{})_{ _{1},,_{N}\\ _{n}}T_{n=1}^{N}\|_{n-1} (_{n}(t_{n,1}))\|^{2}\] (5)

The _oracle_ MSS collects only one measurement per episode, however, it is impractical since it requires knowing the most uncertain point on the true trajectory a priori, i.e., before executing the policy.

EquidistantAnother natural MSS is the equidistant MSS. We collect \(m_{n}\) equidistant measurements in episode \(n\) and upper bound the integral with the upper Darboux integral.

\[_{N}(^{*},S_{n}^{})_{ _{1},,_{N}\\ _{n}}_{n=1}^{N}}_{i=1}^{m_{n }}(\|_{n-1}(_{n}(t_{n,i}))\|^{2}+ ^{2}}}{m_{n}}).\] (6)

Here \(L_{^{2}}\) is the Lipschitz constant of \(\|_{n-1}()\|^{2}\). To achieve sublinear regret, we require that \(_{n=1}^{N}} o(N)\). Therefore, for a fixed equidistant MSS, our analysis does not ensure a sublinear regret. This is because we consider a continuous-time regret which is integrated (c.f., Equation (1)) while in the discrete-time setting, the regret is defined for the equidistant MSS only (Curi et al., 2020). Accordingly, we study a strictly harder problem. Nonetheless, by linearly increasing the number of observations per episode and setting \(m_{n}=n\), we get \(_{n=1}^{N}}((N))\) and sublinear regret. The equidistant MSS is easy to implement, however, the required number of samples is increasing linearly with the number of episodes.

AdaptiveFinally, we present an MSS that is practical, i.e., easy to implement and at the same time requires only a few measurements per episode. The core idea of receding horizon adaptive MSS is simple: simulate (hallucinate) the system \(_{n}\) with the policy \(_{n}\), and find the time \(t\) such that \(\|_{n-1}(}_{n}(t))\|\) is largest. Here, \(}_{n}(t)\) is the state-action pair at time \(t\) in episode \(n\) for the hallucinated trajectory.

However, the hallucinated trajectory can deviate from the true trajectory exponentially fast in time, and the time of the largest variance on the hallucinated trajectory can be far away from the time of the largest variance on the true trajectory. To remedy this technicality, we utilize a receding horizon MSS. We split, in episode \(n\), the time horizon \(T\) into uniform-length intervals of \(_{n}\) time, where \(_{n}(})\), c.f., Appendix A. At the beginning of every time interval, we measure the true state-action pair \(\), hallucinate with policy \(_{n}\) starting from \(\) for \(_{n}\) time on the system \(_{n}\), and calculate the time when the hallucinated trajectory has the highest variance. Over the next time horizon \(_{n}\), we collect a measurement only at that time. Formally, let \(m_{n}= T/_{n}\) be the number of measurements in episode \(n\) and denote for every \(i\{1,,m_{n}\}\) the time with the highest variance on the hallucinated trajectory in the time interval \([(i-1)_{n},i_{n}]\) by \(t_{n,i}\):

\[t_{n,i} }}{{=}}(i-1)_{n}+}{}\|_{n-1}(}_{n}(t))\|\] s.t. \[}_{n}=_{n}(}_{n},_{n}( }_{n}))\] \[}_{n}(0)=_{n}((i-1)_{n})\]

For this MSS, we show in Appendix A the upper bound

\[_{N}(^{*},S_{n}^{})_{1},,_{N}\\ _{n}}{}9_{n=1}^{N}}_{i=1} ^{m_{n}}\|_{n-1}(_{n}(t_{n,i}))\|^{2}.\] (7)

The model complexity upper bound for the adaptive MSS in Equation (7) does not have the \(L_{^{2}}\)-dependent term from the equidistant MSS (6) and only depends on the sum of uncertainties at the collected points. Further, the number of points we collect in episode \(n\) is \((_{n})\), and \(_{n}\), e.g., if we model the dynamics with GPs with radial basis function (RBF) kernel, is of order \((n)\) (c.f., Lemma 2).

### Modeling dynamics with a Gaussian Process

We can prove sublinear regret for the proposed three MSSs coupled with optimistic exploration when we model the dynamics using GPs with standard kernels such as RBF, linear, etc. We consider the vector-valued dynamics model \(^{*}()=(f_{1}^{*}(),,f_{d_{x}}^{*}())\), where scalar-valued functions \(f_{j}^{*}_{k}\) are elements of a Reproducing Kernel Hilbert Space (RKHS) \(_{k}\) with kernel function \(k\), and their norm is bounded \(\|f_{j}^{*}\|_{k} B\). We write \(^{*}_{k,B}^{d_{x}}}}{{= }}\{(f_{1},,f_{d_{x}})\|f_{j}\|_{k} B\}\).

To learn \(^{*}\) we fit a GP model with a zero mean and kernel \(k\) to the collected data \(_{1:n}\). For ease of notation, we denote by \(}_{1:n}^{j}\) the concatenated \(j\)-th dimension of the state derivative observations from \(_{1:n}\). The posterior means and variances of the GP model have a closed form (Williams and Rasmussen (2006)):

\[_{n,j}() =_{n}^{}()(_{n}+^{2} )^{-1}}_{1:n}^{j}\] \[_{n,j}^{2}() =k(,)-_{n}^{}()(_{n}+ ^{2})^{-1}_{n}(),\]

where we write \(_{n}=[k(_{l},_{m})]_{_{l},_{m}_{1:n}}\), and \(_{n}()=[k(_{l},)]_{_{l}_{1:n}}\). The posterior means and variances with the right scaling factor \(_{n}()\) satisfy the all-time well-calibrated Assumption 2.

**Lemma 2** (Lemma 3.6 from Rothfuss et al. (2023)).: _Let \(^{*}_{k,B}^{d_{x}}\) and \((0,1)\). Then there exist \(_{n}()(+(1/)})\) such that the confidence sets \(_{n}\) built from the triplets \((_{n},_{n},_{n}())\) form an all-time well-calibrated statistical model._

Figure 1: In episode \(n\) we split the horizon \(T\) into intervals of \(_{n}\) time. We hallucinate the trajectory in every interval and select time \(t_{n,i}\) in the interval \(i\) where the uncertainty on the hallucinated trajectory is the highest.

Here, \(_{n}\) is the _maximum information gain_ after observing \(n\) points (Srinivas et al., 2009), as defined in Appendix A, where we also provide the rates for common kernels. For example, for the RBF kernel, \(_{n}=((n)^{d_{x}+d_{u}+1})\).

Finally, we show sublinear regret for the proposed MSSs for the case when we model dynamics with GPs. The proof of Theorem 3 is provided in Appendix A.

**Theorem 3**.: _Assume that \(^{*}_{k,B}^{d_{x}}\), the observation noise is i.i.d. \((;^{2})\), and \(\|\|\) is the Euclidean norm. We model \(^{*}\) with the GP model. The regret for different MSSs is with probability at least \(1-\) bounded by_

\[R_{N}(S^{}) (_{N}T^{2}e^{L_{}(1+L_{}) T}}), m_{n}^{}=1\] \[R_{N}(S^{}) (_{N}T^{2}e^{L_{}(1+L_{}) T}+(N))}), m_{n}^{}=n\] \[R_{N}(S^{}) (_{N}T^{2}e^{L_{}(1+L_{}) T}}), m_{n}^{}=(_{n})\]

The optimistic exploration coupled with any of the proposed MSSs achieves regret that depends on the maximum information gain. All regret bounds from Theorem 3 are sublinear for common kernels, like linear and RBF. The bound for the adaptive MSS with RBF kernel is \((+d_{u}+1)}})\), where we hide the dependence on \(T\) in the \(\) notation. We reiterate that while the number of observations per episode of the oracle MSS is optimal, we cannot implement the oracle MSS in practice. In contrast, the equidistant MSS is easy to implement, but the number of measurements grows linearly with episodes. Finally, adaptive MSS is practical, i.e., easy to implement, and requires only a few measurements per episode, i.e., \((n)\) in episode \(n\) for RBF.

SummaryOCoRL consists of two key and orthogonal components; _(i)_ optimistic policy selection and _(ii)_ measurement selection strategies (MSSs). In optimistic policy selection, we optimistically, w.r.t. plausible dynamics, plan a trajectory and rollout the resulting policy. We study different MSSs, such as the typical equidistant MSS, for data collection within the framework of continuous time modeling. Furthermore, we propose an adaptive MSS that measures data where we have the highest uncertainty on the planned (hallucinated) trajectory. We show that OCoRL suffers no regret for the equidistant, adaptive, and oracle MSSs.

## 4 Related work

Model-based Reinforcement LearningModel-based reinforcement learning (MBRL) has been an active area of research in recent years, addressing the challenges of learning and exploiting environment dynamics for improved decision-making. Among the seminal contributions, Deisenroth and Rasmussen (2011) proposed the PILCO algorithm which uses Gaussian processes for learning the system dynamics and policy optimization. Chua et al. (2018) used deep ensembles as dynamics models. They coupled MPC (Morari and Lee, 1999) to efficiently solve high dimensional tasks with considerably better sample complexity compared to the state-of-the-art model-free methods SAC (Haarnoja et al., 2018), PPO (Schulman et al., 2017), and DDPG (Lillicrap et al., 2015). The aforementioned model-based methods use more or less greedy exploitation that is provably optimal only in the case of the linear dynamics (Simchowitz and Foster, 2020). Exploration methods based on Thompson sampling (Dearden et al., 2013; Chowdhury and Gopalan, 2019) and Optimism (Auer et al., 2008; Abbasi-Yadkori and Szepesvari, 2011; Luo et al., 2018; Curi et al., 2020), however, provably converge to the optimal policy also for a large class of nonlinear dynamics if modeled with GPs. Among the discrete-time RL algorithms, our work is most similar to the work of Curi et al. (2020) where they use the reparametrization trick to explore optimistically among all statistically plausible discrete dynamical models.

Continuous-time Reinforcement LearningReinforcement learning in continuous-time has been around for several decades (Doya, 2000; Vrabie and Lewis, 2008, 2009; Vamvoudakis et al., 2009). Recently, the field has gained more traction following the work on Neural ODEs by Chen et al. (2018). While physics biases such as Lagrangian (Cranmer et al., 2020) or Hamiltonian (Greydanus et al., 2019) mechanics can be enforced in continuous-time modeling, different challenges such as vanishing \(Q\)-function (Tallec et al., 2019) need to be addressed in the continuous-time setting. Yildiz et al. (2021) introduce a practical episodic model-based algorithm in continuous time. In each episode, they use the learned mean estimate of the ODE model to solve the optimal control task with a variant of a continuous-time actor-critic algorithm. Compared to Yildiz et al. (2021) we solve the optimal control task using the optimistic principle. Moreover, we thoroughly motivate optimistic planning from a theoretical standpoint. Lutter et al. (2021) introduce a continuous fitted value iteration and further show a successful hardware application of their continuous-time algorithm. A vast literature exists for continuous-time RL with linear systems (Modares and Lewis, 2014; Wang et al., 2020), but few, only for linear systems, provide theoretical guarantees (Mohammadi et al., 2021; Basei et al., 2022). To the best of our knowledge, we are the first to provide theoretical guarantees of _convergence to the optimal cost in continuous time_ for a large class of RKHS functions.

Aperiodic strategiesAperiodic MSSs and controls (event and self-triggered control) are mostly neglected in RL since most RL works predominantly focus on discrete-time modeling. There exists a considerable amount of literature on the event and self-triggered control (Astrom and Bernhardsson, 2002; Anta and Tabuada, 2010; Heemels et al., 2012). However, compared to periodic control, its theory is far from being mature (Heemels et al., 2021). In our work, we assume that we can control the system continuously, and rather focus on when to measure the system instead. The closest to our adaptive MSS is the work of Du et al. (2020), where they empirically show that by optimizing the number of interaction times, they can achieve similar performance (in terms of cost) but with fewer interactions. Compared to us, they do not provide any theoretical guarantees. Umlauft and Hirche (2019); Lederer et al. (2021) consider the non-episodic setting where they can continuously monitor the system. They suggest taking measurements only when the uncertainty of the learned model on the monitored trajectory surpasses the boundary ensuring stability. They empirically show, for feedback linearizable systems, that by applying their strategy the number of measurements reduces drastically and the tracking error remains bounded. Compared to them, we consider general dynamical systems and also don't assume continuous system monitoring.

## 5 Experiments

We now empirically evaluate the performance of OCoRL on several environments. We test OCoRL on _Cancer Treatment and Glucose in blood systems_ from Howe et al. (2022), _Pendulum_, _Mountain Car_ and _Cart Pole_ from Brockman et al. (2016), _Bicycle_ from Polack et al. (2017), _Furuta Pendulum_ from Lutter et al. (2021) and _Quadrotor in 2D and 3D_ from Nonami et al. (2010). The details of the systems' dynamics and tasks are provided in Appendix C.

Comparison methodsTo make the comparison fair, we adjust methods so that they all collect the same number of measurements per episode. For the equidistant setting, we collect \(M\) points per episode (we provide values of \(M\) for different systems in Appendix C). For the adaptive MSS, we assume \(_{n} T\), and instead of one measurement per episode we collect a batch of \(M\) measurements such that they (as a batch) maximize the variance on the hallucinated trajectory. To this end, we consider the _Greedy Max Determinant_ and _Greedy Max Kernel Distance_ strategies of Holzmuller et al. (2022). We provide details of the adaptive strategies in Appendix C. We compare OCoRL with the described MSSs to the optimal discrete-time zero hold control, where we assume the access to the true discretized dynamics \(_{d}^{*}(,)=+_{0}^{T/(M-1)}^{*}((t), )\,dt\). We further also compare with the best continuous-time control policy, i.e., the solution of Equation (1).

Does the continuous-time control policy perform better than the discrete-time control policy?In the first experiment, we test whether learning a continuous-time model from the finite data coupled with a continuous-time control policy on the learned model can outperform the discrete-time zero-order hold control on the true system. We conduct the experiment on all environments and report the cost after running OCoRL for a few tens of episodes (the exact experimental details are provided in Appendix C). From Table 1, we conclude that the OCoRL outperforms the discrete-time zero-order hold control on the true model on every system if we use the adaptive MSS, while achieving lower cost on 7 out of 9 systems if we measure the system equidistantly.

Does the adaptive MSS perform better than equidistant MSS?We compare the adaptive and equidistant MSSs on all systems and observe that the adaptive MSSs consistently perform better than the equidistant MSS. To better illustrate the difference between the adaptive and equidistant MSSs we study a 2-dimensional Pendulum system (c.f., Figure 2). First, we see that if we use adaptive MSSs we consistently achieve lower per-episode costs during the training. Second, we observe that while equidistant MSS spreads observations equidistantly in time and collects lots of measurements with almost the same state-action input of the dynamical system, the adaptive MSS spreads the measurements to have diverse state-action input pairs for the dynamical system on the executed trajectory and collects higher quality data.

Does optimism help?For the final experiment, we examine whether planning optimistically helps. In particular, we compare our planning strategy to the mean planner that is also used by Yildiz et al. (2021). The mean planner solves the optimal control problem greedily with the learned mean model \(_{n}\) in every episode \(n\). We evaluate the performance of this model on the Pendulum system for all MSSs. We observe that planning optimistically results in reducing the cost faster and achieves better performance for all MSSs.

    &  &  \\  System &  Continuous \\ time OC \\  &  Discrete zero- \\ order hold OC \\  &  Max Kernel \\ Distance \\  & 
 Max \\ Determinant \\  & Equidistant \\  Cancer Treatment & 20.57 & 21.05 & \(20.70 0.06\) & \(\) & \(21.05 1.60\) \\ Glucose in Blood & 15.23 & 15.30 & \(\) & \(15.24 0.01\) & \(15.25 0.01\) \\ Pendulum & 20.16 & 20.59 & \(\) & \(\) & \(20.29 0.03\) \\ Mountain Car & 34.63 & 35.04 & \(\) & \(\) & \(34.64 0.01\) \\ Cart Pole & 17.49 & 19.96 & \(\) & \(17.53 0.04\) & \(17.63 0.05\) \\ Bicycle & 9.45 & 10.24 & \(\) & \(9.53 0.03\) & \(9.67 0.05\) \\ Furuta Pendulum & 23.31 & 25.11 & \(23.64 0.22\) & \(\) & \(314.77 411.01\) \\ Quadrotor 2D & 3.54 & 4.01 & \(\) & \(\) & \(3.57 0.01\) \\ Quadrotor 3D & 7.38 & 7.84 & \(\) & \(7.54 0.28\) & \(9.41 1.43\) \\   

Table 1: OCoRL with adaptive MSSs achieves lower final cost \(C(_{N},^{*})\) compared to the discrete-time control on the true system on all tested environments while converging towards the best continuous-time control policy. While equidistant MSS achieves higher cost compared to the adaptive MSS, it still outperforms the discrete-time zero-order hold control on the true model for most systems.

Figure 2: All MSSs coupled with continuous-time control achieve lower cost than the optimal discrete zero-order hold control on the true model. Adaptive MSSs (Greedy Max Kernel Distance and Greedy Max Determinant) reduce the suffered cost considerably faster than equidistant MSS and are converging towards the best possible continuous-time control. Whereas equidistant MSS spreads the measurements uniformly over time, the adaptive MSSs spread the data over the state-action space (dynamical system’s input) and collect higher quality data.

## 6 Conclusion

We introduce a model-based continuous-time RL algorithm OCoRL that uses the _optimistic paradigm_ to provably achieve sublinear regret, with respect to the best possible continuous-time performance, for several MSSs if modeled with GPs. Further, we develop a practical _adaptive_ MSS that, compared to the standard equidistant MSS, drastically reduces the number of measurements per episode while retaining the regret guarantees. Finally, we showcase the benefits of continuous-time compared to discrete-time modeling in several environments (c.f. Figure 1, Table 1), and demonstrate the benefits of planning with _optimism_ compared to greedy planning.

In this work, we considered the setting with deterministic dynamics where we obtain a noisy measurement of the state's derivative. We leave the more practical setting, where only noisy measurements of the state, instead of its derivative, are available as well as stochastic, delayed differential equations, and partially observable systems to future work.

Our aim with this work is to catalyze further research within the RL community on continuous-time modeling. We believe that this shift in perspective could lead to significant advancements in the field and look forward to future contributions.