# Regret Matching\({}^{+}\): (In)Stability and Fast Convergence in Games

Gabriele Farina

MIT

gfarina@mit.edu

&Julien Grand-Clement

ISOM, HEC Paris

grand-clement@hec.fr

&Christian Kroer

IEOR, Columbia University

christian.kroer@columbia.edu

&Chung-Wei Lee

Department of Computer Science

University of Southern California

leechung@usc.edu

&Haipeng Luo

Department of Computer Science

University of Southern California

haipeng1@usc.edu

###### Abstract

Regret Matching\({}^{+}\) (RM\({}^{+}\)) and its variants are important algorithms for solving large-scale games . However, a theoretical understanding of their success in practice is still a mystery. Moreover, recent advances  on fast convergence in games are limited to no-regret algorithms such as online mirror descent, which satisfy _stability_. In this paper, we first give counterexamples showing that RM\({}^{+}\) and its predictive version  can be unstable, which might cause other players to suffer large regret. We then provide two fixes: restarting and chopping off the positive orthant that RM\({}^{+}\) operates in. Combined with RM\({}^{+}\) with predictions, we show that restarting is sufficient to get \(O(T^{1/4})\) individual regret and that chopping off achieves \(O(1)\) social regret in normal-form games. We also apply our stabilizing techniques to clativyoural updates in the uncoupled learning setting for RM\({}^{+}\), introduced _Extragradient RM\({}^{+}\)_, and prove desirable results akin to recent works for Clairvoyant online mirror descent . Our experiments show the advantages of our algorithms over vanilla RM\({}^{+}\)-based algorithms in matrix and extensive-form games.

## 1 Introduction

Regret minimization is an important framework for solving games. Its connection to game theory provides a practically efficient way to approximate game-theoretic equilibria . Moreover, it provides a scaleable way to solve large-scale sequential games, for example using the _Counterfactual Regret Minimization_ (CFR) decomposition . Consequently, regret minimization algorithms are a central component in recent superhuman poker AIs . _Regret Matching\({}^{+}\) (RM\({}^{+}\))_ is the most prevalent regret minimizer in these applications. In theory, it guarantees an \(O(1/)\) convergence rate after \(T\) iterations, but its practical performance is usually significantly faster.

On the other hand, a line of recent works show that regret minimizers based on follow the regularized leader (FTRL) or online mirror descent (OMD) enjoy faster convergence rates in theory when combined with the concept of optimism/predictiveness . The result was originally provenin matrix games , and later extended to multiplayer normal-form games [34; 6; 7], extensive-form games [8; 10; 15; 1], and general convex games [22; 13]. However, despite their favorable properties in theory, optimistic algorithms based on FTRL/OMD are usually numerically inferior to \(^{+}\) when applied to solving large-scale sequential games. It remains a mystery whether some optimistic variant of \(^{+}\) enjoys a theoretically faster convergence rate, considering the strong empirical performance of \(^{+}\). It is also an open question whether there exists an algorithm that has both favorable theoretical guarantees similar to FTRL/OMD algorithms and practical performance comparable to \(^{+}\). Inspired by recent work on the connection between OMD and \(^{+}\), we provide new insights on the theoretical and empirical behavior of \(^{+}\)-based algorithms, and we show that the analysis of fast convergence for OMD can be extended to \(^{+}\) with some simple modifications to the algorithm. Specifically, our main contributions can be summarized as follows.

1. We provide a detailed theoretical and empirical analysis of the potential for slow performance of \(^{+}\) and predictive \(^{+}\). We start by showing that, in stark contrast to FTRL/OMD algorithms that are stable inherently, there exist loss sequences that make \(^{+}\) and its variants unstable, leading to cycling between very different strategies. The key reason for such instability is that the decisions of these algorithms are chosen by normalizing an _aggregate payoff vector_; thus, in a region close to the origin, two consecutive aggregate payoffs may point in very different directions, despite being close, resulting in unstable iterations. Surprisingly, note that this can only happen when the aggregate payoff vectors, which essentially measure the algorithm's regret against each action, are small, so instability can only happen when one's regret is small and thus is seemingly not an issue. However, in a game setting, such instability might cause other players to suffer large regret because they have to learn in an unpredictable environment. Indeed, we identify a \(3 3\) matrix game where this is the case and both \(^{+}\) and predictive \(^{+}\) converge slowly at a rate of \(O(1/)\) (Fig. 1). We emphasize that very little is known about the properties of (predictive) \(^{+}\) and we are the first to show concrete examples of stability issues in matrix games and in the adversarial setting.
2. Motivated by our counterexamples, we propose two methods to stabilize \(^{+}\): _restarting_, which reinitializes the algorithms when the aggregate payoffs are all below a threshold, and _chopping off_ the origin from the nonnegative orthant to smooth the algorithms. When applying these techniques to online learning with \(^{+}\), we show improved regret and fast convergence similar to predictive OMD: we obtain \(O(T^{1/4})\) individual regrets for _Stable Predictive \(^{+}\)_ (which uses restarting) and \(O(1)\) social regret for _Smooth Predictive \(^{+}\)_ (which chops off the origin). We also consider _conceptual prox_ and _extragradient_ versions of \(^{+}\) for normal-form games. We show that our stabilizing ideas also provide the required stability in these settings and thus give strong theoretical guarantees: Conceptual \(^{+}\) achieves \(O(1)\) individual regrets (Theorem 5.3) while Extragradient \(^{+}\) achieves \(O(1)\) social regret (Theorem 5.6). See Table 1 for a summary of our results for normal-form games. We further extend Conceptual \(^{+}\) to extensive-form games (EFG), yielding \(O(1)\) regret in \(T\) iterations with \(O(T(T))\) gradient computation. The key step here is to show the Lipschitzness of the CFR decomposition (Lemma J.1).
3. We apply our algorithms to solve matrix games and EFGs. For the \(3 3\) matrix game instability counterexample, our algorithms indeed perform significantly better than (predictive)

 
**Algorithms** & **Social regret in multi-player NFGs** \\   \(^{+}\) & \(O(T^{1/2})\) \\ Predictive \(^{+}\) & \(O(T^{1/2})\) \\ Stable Predictive \(^{+}\) (Alg. 1) & \(O(T^{1/4})\) \\ Smooth Predictive \(^{+}\) (Alg. 2) & \(O(1)\) \\ Conceptual \(^{+}\) (Alg. 3 ) & \(O(1)\) \\ Approximate Conceptual \(^{+}\) (Alg. 4 with \(k=(T)\)) & \(O(1)\) \\ Extragradient \(^{+}\) (Alg. 5) & \(O(1)\) \\  

Table 1: Summary of regret guarantees for the algorithms studied in this paper. The constants hidden in the \(O()\) notations depends on initialization and the dimensions of the games and are given in our theorems.

RM\({}^{+}\). For random matrix games, we find that Stable and Smooth Predictive RM\({}^{+}\) have very strong empirical performance, on par with (unstabilized) Predictive RM\({}^{+}\), and greatly outperforming RM\({}^{+}\) in all our experiments; Extragradient RM\({}^{+}\) appears to be more sensitive to the choice of step sizes and sometimes performs only as well as RM\({}^{+}\). Our experiments on \(4\) different EFGs show that our implementation of clairvoyant CFR outperforms predictive CFR in some, but not all, instances.

## 2 Preliminaries

**Notations.** For \(d\), we write \(_{d}^{d}\) the vector with \(1\) on every component. The simplex of dimension \(d-1\) is \(^{d}=\{_{+}^{d},_{d} =1\}\). The vector \(\) has \(0\) on every component and its dimension is implicit. For \(x\), we write \([x]^{+}\) for the positive part of \(x:[x]^{+}=\{0,x\}\), and we overload this notation to vectors component-wise. For two vectors \(\) and \(\), \(\) means \(\) is at least \(\) component-wise. We write \(\|\|_{*}\) for the dual norm of a norm \(\|\|\).

**Online Linear Minimization.** In online linear minimization, at every decision period \(t 1\), an algorithm chooses a decision \(^{t}\) from a convex decision set \(\). A loss vector \(^{t}\) is chosen arbitrarily and an instantaneous loss of \(^{t},^{t}\) is incurred. The regret of an algorithm generating the sequence of decisions \(^{1},...,^{T}\) is defined as the difference between the cumulative loss generated and that of any fixed strategy \(}\): \(^{T}(})=_{t=1}^{T}^{t}, ^{t}-}\). A _regret minimizer_ guarantees that \(^{T}(})=o(T)\) for any \(}\).

**Online Mirror Descent.** A famous regret minimizer is Online Mirror Descent (OMD) , which generates the decisions \(^{1},...,^{T}\) as follows (with a learning rate \(>0\)):

\[^{t+1}=_{^{t},}(^{t})\] (OMD)

where for any \(\), and any loss \(\), the _proximal operator_\(_{,}()\) is defined as \(_{,}()=_{} ,}+D(},)\) where \(D\) is the _Bregman divergence_ associated with \(:\), a \(1\)-strongly convex regularizer (with respect to some norm \(\|\|\)): \(D(},)=(})-()- (),}-,\ },\). OMD guarantees that the worst-case regret against any \(}\) grows as \(O()\) (omitting other dependence for simplicity; the same below). Other popular regret minimizers include Follow-The-Regularized-Leader (FTRL), and adaptive variants of OMD and FTRL; we refer the reader to  for an extensive survey on regret minimizers.

**Regret Matching and Regret Matching\({}^{+}\).** Regret Matching (RM) and Regret Matching\({}^{+}\) (RM\({}^{+}\)) are two regret minimizers that achieve \(O()\) worst-case regret when \(=^{d}\). RM  maintains a sequence of _aggregate payoffs_\((^{t})_{t 1}\): \(^{t}=R_{0}_{d},\) and for \(t 1\),

\[^{t}=[^{t}]^{+}/\|[^{t}]^{+}\|_{1},^{t+1}=^ {t}+^{t},^{t}_{d}- ^{t},\]

where \(R_{0} 0\) specifies an initial point and \(/0\) is defined as the uniform distribution for convenience. The original RM sets \(R_{0}=0\), making the algorithm completely _parameter-free_, a very appealing property in practice. RM\({}^{+}\) is a simple variation of RM, where the aggregate payoffs are thresholded at every iteration . In particular, RM\({}^{+}\) only keeps track of the non-negative components of the aggregate payoffs to compute a decision: \(^{1}=R_{0}_{d},\) and for \(t 1\),

\[^{t}=^{t}/\|^{t}\|_{1},^{t+1}=[^{t}+ ^{t},^{t}_{d}-^{t}]^{+}.\]

We highlight that very little is known about the theoretical properties of RM\({}^{+}\), despite its strong empirical performances:  show that RM\({}^{+}\) is a regret minimizer (and enjoys the stronger \(K\)-tracking regret property), and  show that it can safely be combined with alternation ( prove strict improvement when using alternation). Farina et al.  show an interesting connection between RM\({}^{+}\) and Online Mirror Descent: the update \(^{t+1}=[^{t}+^{t},^{t} _{d}-^{t}]^{+}\) of RM\({}^{+}\) can be rewritten as

\[^{t+1}=_{^{t},}((^{t}, ^{t}))\]

for \(=_{+}^{d},=\|\|_{2}^{2}\), \(=1\), and \((^{t},^{t})\) defined as \((^{t},^{t})=^{t}-^{t}, ^{t}_{d}.\) Therefore, RM\({}^{+}\) generating a sequence of decisions \(^{1},...,^{T}\) facing a sequence of losses \((^{t})_{t 1}\), is closely connected to OMD instantiated with the non-negative orthant as the decision set and facing a sequence of losses \(((^{t},^{t}))_{t 1}\). We have the following relation for the regret in \(^{1},...,^{T}\) and the regret in \(^{1},...,^{T}\) (the proof follows  and is deferred to the appendix).

**Lemma 2.1**.: _Let \(^{1},...,^{T}^{d}\) be generated as \(^{t}=^{t}/\|^{t}\|_{1}\) for some sequence \(^{1},...,^{T}_{+}^{d}\). The regret \(^{T}(})\) of \(^{1},...,^{T}\) facing a sequence of losses \(^{1},...,^{T}\) is equal to \(^{T}(})\), the regret of \(^{1},...,^{T}\) facing the sequence of losses \((^{1},^{1}),...,(^{T},^{T})\), compared against \(}=}\): \(^{T}(})=_{t=1}^{T} (^{t},^{t}),^{t}-}.\)_

Since OMD is a regret minimizer guaranteeing \(^{T}(})=O()\), Lemma 2.1 directly shows that \(^{+}\) is also a regret minimizer: \(^{T}(})=O()\).

**Multiplayer Normal-Form Games.** In a multiplayer normal-form game, there are \(n\) players. Each player \(i\) has \(d_{i}\) strategies and their decision space \(^{d_{i}}\) is the probability simplex over the \(d_{i}\) strategies. We denote \(=_{i=1}^{n}^{d_{i}}\) as the joint decision space of all players and \(d=d_{1}++d_{n}\). The utility function for player \(i\) is a concave function \(u_{i}:[-1,1]\) that maps every joint strategy profile \(=(_{1},...,_{n})\) to a payoff. We assume bounded gradients and \(L_{u}\)-smoothness for the utilities of the players: there exists \(B_{u}>0,L_{u}>0\) such that for any \(\), \(^{}\) and any player \(i\),

\[\|_{_{i}}u_{i}()\|_{2} B_{u},\|_{_{i}}u_{i}( )-_{_{i}}u_{i}(^{})\|_{2} L_{u}\|- {x}^{}\|_{2}.\] (1)

The function mapping joint strategies to negative payoff gradients for all players is a vector-valued function \(G:^{d}\) such that \(G()=(-_{_{n}}u_{1}(),,-_{_{n}}u_{n} ())\). It is well known that running a regret minimizer for \((_{1}^{t},...,_{n}^{t})=_{i=1}^{n}^{d_{i}}\) facing the loss \(G(^{t})=(_{1}^{t},,_{n}^{t})\) leads to strong game-theoretic guarantees (e.g., the average iterate being an approximate coarse correlated equilibrium). However, in light of Lemma 2.1, we will instead perform regret minimization on \((_{1}^{t},...,_{n}^{t})=_{i=1}^{n}_{+ }^{d_{i}}\) with the losses \(((_{1}^{t},_{1}^{t}),,(_{n}^{t},_{n}^{t}))\). For conciseness, we thus define the operator \(F:^{d}\) as, for \(=(_{1},...,_{n})\), \(F()=((_{1},_{1}),...,(_{n},_{ n}))\) where \(_{i}=_{i}/\|_{i}\|_{1},\ i=1,...,n,(_{i} )_{i[n]}=G()\).

**Predictive OMD and Its RVU Bounds.** The predictive version of OMD proceeds as follows:

\[^{t}=_{}^{t},}(^{t}) }^{t+1}=_{}^{t},}(^{ t})\]

When setting \(^{t}=^{t-1}\), predictive OMD satisfies \(^{T}(})},}^{1 })}{}+_{t=1}^{T}\|^{t}-^{t-1}\|_{}^{2}- _{t=1}^{T-1}\|^{t+1}-^{t}\|^{2}\). This regret bound satisfies the _RVU_ (regret bounded by variation in utilities) condition, introduced in . The authors show that this type of bound guarantees that the social regret (i.e., sum of the regrets of all players) is \(O(1)\) when all players apply this special instance of predictive OMD. Syrgkanis et al.  further prove that each player has improved \(O(T^{1/4})\) individual regret by the _stability_ of predictive OMD. Specifically, they show that predictive OMD guarantees \(\|^{t+1}-^{t}\|=O()\) against any adversarial loss sequence, i.e., the algorithm is stable in the sense that the change in the iterates can be controlled by choosing \(\) appropriately.

**Predictive RM\({}^{+}\)** Similar to OMD, we can generalize RM\({}^{+}\) to Predictive Regret Matching\({}^{+}\): define \(^{1}=^{1}=R_{0}_{d}\) (with \(R_{0}=0\) by default), and for \(t 1\),

\[^{t} =}^{t}/\|}^{t}\|_{1},\ \ }^{t}=[^{t}+^{t}]^{+},\] \[^{t+1} =[^{t}-(^{t},^{t})]^{+},\ \ (^{t},^{t})=^{t}-^{t},^{t}_{d}.\]

We call the algorithm predictive RM\({}^{+}\) (PRM\({}^{+}\)) when \(^{t}=-(^{t-1},^{t-1})\), and it recovers RM\({}^{+}\) when \(^{t}=\). A regret bound with a similar RVU condition is attainable for predictive RM\({}^{+}\) by its connection to predictive OMD , but only in the non-negative orthant space instead of the actual strategy space. To make a connection between them, stability is required as we show later. A natural question is then whether (predictive) RM\({}^{+}\) is also always stable. We show that the answer is no by giving an adversarial example in the next section.

## 3 Instability of (Predictive) Regret Matching\({}^{+}\)

We start by showing that there exist adversarial loss sequences that lead to instability for both RM\({}^{+}\) and predictive RM\({}^{+}\). Our construction starts with an unbounded loss sequence \(^{t}\) so that \(^{t}\) alternates between \((1/2,1/2)\) and \((0,1)\): we set \(^{t}=(^{t},0)\), where \(^{1}=2\), and for \(t 2\), \(^{t}=-2^{(t-2)/2}\) if \(t\) is even and \(^{t}=2^{(t-1)/2}\) if \(t\) is odd. Our proof is completed by normalizing the losses to \([-1,1]\) given a fixed time horizon (see Appendix B for details).

**Theorem 3.1**.: _There exist finite sequences of losses in \(^{2}\) for RM\({}^{+}\) and its predictive version such that \(^{t}=(,)\) when \(t\) is odd and \(^{t}=(0,1)\) when \(t\) is even._

This is in stark contrast to OMD which always ensures \(\|^{t+1}-^{t}\|=O()\) and is thus inherently stable. However, a somewhat surprising property about (predictive) RM\({}^{+}\) is that _instability actually implies low regret_. To see this, we first present the following Lipschitz property of the normalization function \(:/\|\|_{1}\) for \(^{d}_{+}\).

**Proposition 1**.: Let \(,^{d}_{+}\), with \(^{} 1\). Then, \(\|()-()\|_{2}\|-\|_{2}\).

This proposition shows that the normalization step has a reasonable Lipschitz constant (\(\)) as long as its input is not too close to the origin, which further implies the following corollary.

**Corollary 3.2**.: _RM\({}^{+}\) with \(\|^{t}\|_{1} R_{0}\) satisfies \(\|^{t+1}-^{t}\|_{2}}{R_{0}} \|^{t+1}-^{t}\|_{2}}{R_{0}}\)._

Put differently, the corollary states that instability can happen only when the cumulative regret vector \(^{t}\) is small. For example, if \(\|^{t+1}-^{t}\|=(1)\), then we must have \(\|^{t}\|_{1}=O(dB_{u})\) and thus the regret at that point is at most \(O(dB_{u})\). A similar argument holds for predictive RM\({}^{+}\) as well. Therefore, instability is in fact not an issue for these algorithms' own regret.

However, when using these algorithms to play a game, what could happen is that such instability leads to other players learning in an unpredictable environment with large regret. We show this phenomenon via an example of a \(3 3\) matrix game \(_{(3)}_{(3)},\), where \(=((3,0,-3),(0,3,-4),(0,0,1))\). The first column of Fig. 1 shows the squared \(_{2}\) norm of the consecutive difference of the last \(100\) iterates of Predictive RM\({}^{+}\) for the \(x\) player (top) and the \(y\) player (bottom). The iterates of the \(x\) player are rapidly changing in a periodic fashion while the iterates of the \(y\) player are stable with changes on the order of \(10^{-5}\). In the center plots where we show the individual regret for each player, we indeed observe that the cumulative regret of the \(x\) player is near zero as implied by instability, but it causes large regret (close to \(T^{0.5}\) empirically) for the \(y\) player. (We show the same plots for RM\({}^{+}\) in Fig. 4 in Appendix B; there, the iterates of both players are stable, but since RM\({}^{+}\) lacks predictivity, it still leads to larger regret for one player.)

The right column of Fig. 1 shows the duality gap achieved by the linear average \((_{t},_{t})=(_{t=1}^{T}t^{t}, {2}{T(T+1)}_{t=1}^{T}t^{t})\), when the iterates are generated by RM\({}^{+}\) with alternation (top) and predictive RM\({}^{+}\) (bottom). For both algorithms the convergence rate slows down around \(10^{4}\) iterations. A linear regression estimate on the rate for the last \(10^{6}\) iterates shows rates of \(-0.497\) and \(-0.496\) for RM\({}^{+}\) and predictive RM\({}^{+}\) respectively. To the best of our knowledge, this is the

Figure 1: Left plots show the iterate-to-iterate variation in the last \(100\) iterates of predictive RM\({}^{+}\). Center plots show the regret for the \(x\) and \(y\) players under predictive RM\({}^{+}\). Right plots show empirical convergence speed of RM\({}^{+}\) (top row) and Predictive RM\({}^{+}\) (bottom row).

first known case of empirical convergence rates on the order of \(T^{-0.5}\) for either \(^{+}\) or predictive \(^{+}\); the worst prior instance for \(^{+}\) was \(T^{-0.74}\) in Farina et al. ; no hard instance was known for predictive \(^{+}\).

## 4 Stabilizing \(^{+}\) and Predictive \(^{+}\).

Based on the discussions in the previous section, we aim to make _every_ player stable despite the fact that being an unstable player may actually be good for that particular player. By Corollary 3.2, it suffices to make sure that \(\|^{t}\|_{1}\) is never too small. We provide two approaches to ensure this property and thereby stabilize (predictive) \(^{+}\).

**Stable Predictive \(^{+}\).** One way to maintain the required distance to the origin is via _restarting_: We initialize the algorithm with the cumulative regret vector equal to some non-zero amount, instead of the usual initialization at zero. Then, when the cumulative regret vector gets below the initialization point, we _restart_ the algorithm from the initialization point. Applying this idea to predictive \(^{+}\) yields Algorithm 1. Player \(i\) starts with \(^{1}_{i}=R_{0}_{d_{i}}\), runs predictive \(^{+}\), and restarts whenever \(^{t}_{i} R_{0}_{d_{i}}\). In the algorithm we write \((^{t}_{1},...,^{t}_{n})\) compactly as \(^{t}\) (similarly for \(^{t}\)). Note, though, that the updates are decentralized for each player, as in vanilla predictive \(^{+}\).

Given this modification, Stable \(^{+}\) achieves improved individual regret in multiplayer games, as stated in Theorem 4.1. We defer the proof to the appendix. One key step in the analysis is to note that by definition, the regret against any action is negative when the restarting event happens, so it is sufficient to consider the regret starting from the last restart. Thanks to the stability enforced by the restarts, the regret from the last restart is also well controlled and the results follow by tuning \(\) and \(R_{0}\) optimally. In fact, since the algorithm is scale-invariant up to the relative scale of the two parameters, it is without loss of generality to always set \(R_{0}=1\).

```
1:Input:\(R_{0}>0\), step size \(>0\)
2:Initialization:\(^{0}=R_{0}_{d}\)
3:for\(t=1,,T\)do
4:\(^{t}=_{^{t-1},}(^{t})\)
5:\(^{t}=_{^{t-1},}( F(^{t}))\)
6:\((^{t}_{1},,^{t}_{n})=((^{t}_{1}),, (^{t}_{n}))\)
7:for\(i=1,...,n\)do
8:if\(^{t}_{i} R_{0}_{d_{i}}\)then
9:\(^{t}_{i}=R_{0}_{d_{i}}\) ```

**Algorithm 1** Stable Predictive \(^{+}\)

**Theorem 4.1**.: _Let \(=(d^{2}T)^{-1/4}\) and \(R_{0}=1\). Let \((^{t}_{i})_{i[n]}=F(^{t})\) for \(t 1\). For each player \(i\), set the sequence of predictions \(^{t}_{i}=\) when \(t=0\) or restart happens at \(t-1\); otherwise, \(^{t}_{i}=^{t-1}_{i},\;t 1\). Then Algorithm 1 guarantees that the individual regret \(^{T}_{i}(}_{i})=_{t=1}^{T}_ {_{i}}u_{i}(^{t}),}_{i}-^{t}_{i}\) of each player \(i\) is bounded by \(O(d^{3/2}T^{1/4})\) in multiplayer normal-form games._

Although the restarting idea successfully stabilizes the \(^{+}\) algorithm, the discontinuity created by asynchronous restarts causes technical difficulty for bounding the social regret by \(O(1)\). Next we introduce an alternative stabilization idea to fix this issue.

**Smooth Predictive \(^{+}\).** Our second stabilization idea is to restrict the decision space to a subset where we "chop off" the area that is too close to the origin, that is, project the vector \(^{t}_{i}\) onto the set \(^{d_{i}}_{}=\{^{d_{i}}_{+}\|\|_{1} 1\}\). We denote the joint chopped-off decision space as \(_{}=_{i=1}^{n}^{d_{i}}_{}\). We call the resulting algorithm smooth predictive \(^{+}\) (Algorithm 2). Besides a similar result to Theorem 4.1 on the individual regret (omitted for simplicity), Algorithm 2 also guarantees that the social regret is bounded by a game-dependent constant, as shown in Theorem 4.2.

**Theorem 4.2**.: _Let \(=(2(n-1)_{i}\{d_{i}^{3/2}\})^{-1}\). Using the sequence of predictions \(^{0}=,^{t}=F(^{t-1}),\;t 1\), Algorithm 2 guarantees that the so cial regret_\(_{i=1}^{n}_{i}^{T}(}_{i})=_{i=1}^{n}_{t=1}^{T} _{_{i}}u_{i}(^{t}),}_{i}-_{i}^{t}\) is upper bounded by \(O(n^{2}_{i=1,...,n}\{d_{i}^{3/2}\}_{i=1,...,n}\{\|_{i}^{0}- }_{i}\|_{2}^{2}\})\) in multiplayer normal-form games.

Algorithm 2 dominates Algorithm 1 in terms of our theoretical results so far, but it has one drawback: it requires occasional projection onto \(_{}\). In Appendix K we show that this can be done with a sorting trick in \(O(d d)\) time, whereas the restarting procedure is implementable in linear time.

```
1:Input: Step size \(>0\) with \(<1/L_{F}\)
2:Initialization:\(^{0}_{}\)
3:for\(t=1,,T\)do
4:\(^{t}=_{^{t-1},_{}}( F(^{t}))\)
5:\((_{1}^{t},,_{n}^{t})=((_{1}^{t}),,( _{n}^{t}))\) ```

**Algorithm 3** Conceptual RM\({}^{+}\)

## 5 Conceptual Regret Matching\({}^{+}\)

In this section, we depart from the predictive OMD framework and develop new smooth variants of RM\({}^{+}\) from a different angle. Instead of using predictive OMD to compute the iterates \((_{i}^{t})_{t 1}\), we consider the following regret minimizer that we call _cheating OMD_, defined for some arbitrary closed decision set \(\) and an arbitrary sequence of losses \((^{t})_{t 1}\): \(^{t}=_{^{t-1},}(^{t})\) for \(t 1\), and \(^{0}_{}\). Cheating OMD is inspired by the Conceptual Prox method for solving variational inequalities associated with monotone operators [5; 23; 29]. We call it _cheating_ OMD because at iteration \(t\), the decision \(^{t}\) is chosen as a function of the current loss \(^{t}\), which is revealed _after_ the decision \(^{t}\) has been chosen. It is well-known that cheating OMD yields a sequence of decisions with constant regret; we show it for our setting in the following lemma.

**Lemma 5.1**.: _The Cheating OMD iterates \(\{^{t}\}_{t}\) satisfy \(_{t=1}^{T}^{t},^{t}-} \|^{0}-}\|_{2}^{2},} \)._

To instantiate RM\({}^{+}\) with Cheating OMD as a regret minimizer for the sequence \((_{i}^{t})_{t 1}\) of each player \(i\), we need to show the existence of a vector \(^{t}_{}\) such that

\[^{t}=_{^{t-1},_{}}( F(^{t})).\] (2)

Equation (2) can be interpreted as a fixed-point equation for the map \(_{^{t-1},_{}}( F())\). For any \(^{}_{}\), the map \(_{^{},_{}}( F())\) is \( L\)-Lipschitz continuous _as long as_\(F\) is \(L\)-Lipschitz continuous. Therefore, it is a contraction when \(<1/L\), and then the fixed-point equation \(=_{^{},_{}}( F())\) has a unique solution. Recall that for \(=(_{1},...,_{n})_{}\), the operator \(F\) is defined as \(F()=((_{1},_{1}),,(_{n},_ {n}))\) where \(_{i}=(_{i})\) and \(_{i}=-_{_{i}}u_{i}(),\) for all \(i\{1,...,n\}\). We now show the Lipschitzness of \(F\) over \(_{}\) for normal-form games.

**Lemma 5.2**.: _For a normal-form game, the operator \(F\) is \(L_{F}\)-Lipschitz continuous over \(_{}\), with \(L_{F}=(_{i}d_{i})^{2}+4L_{u}^{2}}\) with \(B_{u},L_{u}\) defined in (1)._

For \(L_{F}\) defined as in Lemma 5.2 and \(<1/L_{F}\), the existence of the fixed-point \(^{t}=_{^{t-1},_{}}( F(^{t}))\) is guaranteed. This yields _Conceptual RM\({}^{+}\)_, defined in Algorithm 3. In the following theorem, we show that Conceptual RM\({}^{+}\) ensures constant regret for each player.

**Theorem 5.3**.: _Let \(L_{F}>0\) be defined as in Lemma 5.2. For \(<1/L_{F}\), Algorithm 3 guarantees that the individual regret \(_{i}^{T}(}_{i})=_{t=1}^{T}_{ _{i}}u_{i}(^{t}),}_{i}-_{i}^{t}\) of each player \(i\) is bounded by \(\|_{i}^{0}-}_{i}\|_{2}^{2}\) in multiplayer normal-form games._Note that the requirement of \(<1/L_{F}\) in Theorem 5.3 and Algorithm 3 is only needed in order to ensure existence of a fixed-point. If the fixed-point condition holds for some larger \(\), then the algorithm is still well-defined and the same convergence guarantee holds.

**Remark 5.4**.: _Piliouras et al.  propose the clairvoyant multiplicate weights updates (MWU) algorithm, based on the classical MWU algorithm, but where the rescaling at iteration \(t\) involves the payoff of the players at iteration \(t\). The connection with the conceptual prox method is made explicit by , where they show how to extend clairvoyant MWU for normal-form games to clairvoyant OMD for general convex games. Our algorithm uses the same idea but for RM\({}^{+}\)._

For \(^{}_{}\), we can approximate the fixed-point of \(_{^{},_{}}( F())\) by performing \(k\) fixed-point iterations. This results in Algorithm 4. We give the guarantees for Algorithm 4 below.

**Theorem 5.5**.: _Let \(L_{F}>0\) be defined as in Lemma 5.2 and \(<1/L_{F}\). Assume that in Algorithm 4, we ensure \(\|^{k}-_{^{-1},_{}}( F(^{k }))\|_{2}^{(t)}\), for all \(t 1\). Then Algorithm 4 guarantees that the individual regret \(_{i}^{T}(}_{i})=_{t=1}^{T} _{_{i}}u_{i}(^{t}),}_{i}-_{i}^{t}\) of each player \(i\) is bounded by \(\|_{i}^{0}-}_{i}\|_{2}^{2}+2B_{u}} _{t=1}^{T}^{(t)}\) in multiplayer normal-form games._

By Theorem 5.5, if we ensure error \(^{(t)}=1/t^{2}\) in Algorithm 4 then the individual regret of each player is bounded by a constant. Since \(_{^{-1},_{}}( F())\) is a contraction for \(<1/L_{F}\), this only requires \(k=O((t))\) fixed-point iterations at each time \(t\). If the number of iterations \(T\) is known in advance, we can choose \(k=O((T))\), to ensure \(^{(t)}=O(1/T)\) and therefore that the individual regret of each player \(i\) is bounded by the constant \(\|_{i}^{0}-}_{i}\|_{2}^{2}+O(2B_{u} {d_{i}}).\)

Recall that the uniform distribution over a sequence of strategy profiles \(\{^{t}\}_{t=1}^{T}\) is a \((_{i}_{i}^{T})/T\)-approximate coarse correlated equilibrium (CCE) of a multiplayer normal-form game (see e.g. Theorem 2.4 in Piliouras et al. ). Therefore, Algorithm 3 guarantees \(O(1/T)\) convergence to a CCE after \(T\) iterations. With the setup from Theorem 5.5 and \(k=O((T))\), Algorithm 4 guarantees \(O((T)/T)\) convergence to a CCE after \(T\) evaluations of the operator \(F\).

Extragradient RM\({}^{+}\).We now consider the case of Algorithm 4 but with only one fixed-point iteration (\(k=1\)). This is similar to the mirror prox algorithm  or the extragradient method . We call this algorithm extragradient RM\({}^{+}\)(ExRM\({}^{+}\), Algorithm 5). We show that one fixed-point iteration (\(k=1\)) at every iteration ensures constant social regret.

**Theorem 5.6**.: _Define \(L_{F}\) as in Lemma 5.2 and let \(=(L_{F})^{-1}\). Algorithm 5 guarantees that the social regret \(_{i=1}^{n}_{i}^{T}(}_{i})=_{i=1}^{n}_ {t=1}^{T}_{_{i}}u_{i}(^{t}),}_{i}- _{i}^{t}\) is bounded by \(_{i=1}^{n}\|_{i}^{0}-}_{i}\|_{2}^{2}\) in multiplayer normal-form games._

We now apply Theorem 5.6 to the case of matrix games, where the goal is to solve

\[_{^{d_{1}}}_{^{d_{2}}},\]

for \(^{d_{1} d_{2}}\). The operator \(F\) is defined as

\[F_{1}\\ _{2}=((_{1}), (_{2}))\\ ((_{2}),-^{}(_{1}))\]

and \(_{}=_{}^{d_{1}}_{}^{d_{2}}\). The next lemma gives the Lipschitz constant of the operator \(F\) in the case of matrix games.

**Lemma 5.7**.: _For matrix games, the operator \(F\) is \(L_{F}\)-Lipschitz over \(_{}\), with \(L_{F}=\|\|_{op}\{d_{1},d_{2}\}\) with \(\|\|_{op}=\{\|\|_{2}/\|\|_{2}^{d_{2}},\}.\)_

Combining Lemma 5.7 with Theorem 5.6, ExRM\({}^{+}\) for matrix games with \(_{}\) as a decision set and \(=(L_{F})^{-1}\) guarantees constant social regret, so that the average of the iterates computed by ExRM\({}^{+}\) converges to a Nash Equilibrium at a rate of \(O(1/T)\).

Extensive-form gamesOur convergence results for Conceptual RM\({}^{+}\) apply beyond normal-form games, to EFGs. Briefly, a EFG is a game played on a tree, where each node belongs to some player, and the player chooses a probability distribution over branches. Moreover, players have _information sets_, which are groups of nodes belonging to a player such that they cannot distinguish among them, and thus they must choose the same probability distribution at all nodes in an information set. As is standard, we assume that each player never forgets information. Below, we describe the main ideas behind the extension; details are given in Appendix J.

In order to extend our results, we use the CFR regret decomposition [37; 9]. CFR defines a notion of local regret at each information set, using so-called _counterfactual values_. By minimizing the regret incurred at each information set with respect to counterfactual values, CFR guarantees that the overall regret over tree-form strategies is minimized. Importantly, counterfactual values are multilinear in the strategies of the players, and therefore they are Lipschitz functions of the strategies of the other players. Hence, using Algorithm 4 at each information set with counterfactual value and applying Theorem 5.5 begets a smooth-RM\({}^{+}\)-based algorithm that computes a sequence of iterates with regret at most \(\) in at \(O(1/)\) iterations and using \(((1/)/)\) gradient computations.

## 6 Numerical experiments

**Matrix games.** We compute the performance of ExRM\({}^{+}\), Stable and Smooth PRM\({}^{+}\) on the \(3 3\) matrix game instance from Section 2 (with step size \(=0.1\)) and on \(30\) random matrix games of size \((d_{1},d_{2})=(30,40)\) with normally distributed coefficients of the payoff matrix and with step sizes \(\{0.1,1,10\}\). We initialize our algorithms at \((1/d_{1})_{d}\), all algorithms use linear averaging, and all algorithms (except ExRM\({}^{+}\)) use alternation. The results are shown in Figure 2. Our new algorithms greatly outperform RM\({}^{+}\) and PRM\({}^{+}\) in the \(3 3\) matrix game; linear regression finds an asymptotic convergence rate of \(O(1/T^{2})\). More detailed results for this instance are given in Appendix K.1. For random matrix games, our algorithms ExRM\({}^{+}\), Smooth PRM\({}^{+}\) and Stable PRM\({}^{+}\) all outperform RM\({}^{+}\) for stepsize \(=0.1\). ExRM\({}^{+}\) performs on par with RM\({}^{+}\) for larger values of \(\), while Stable PRM\({}^{+}\) and Smooth PRM\({}^{+}\) remain very competitive, performing on par with the unstabilized version of PRM\({}^{+}\). We note that we use step sizes that are larger than the theoretical ones since the latter may be overly conservative [10; 25].

**Extensive-form games.** We implemented and evaluated our CFR-based clairvoyant algorithm (henceforth 'Clairvoyant CFR') for extensive-form games. To our knowledge, it is the first time that clairvoyant algorithms are evaluated in extensive-form games. Overall, we were unable to observe the same strong performance observed in normal-form games (Figure 2), for a combination of reasons. First, we observe that the stepsize \(\) calculated in Appendix J to make the operator \(F\) a contraction in extensive-form games is prohibitively small in the games we test on, each of which has a number of sequences on the order of tens of thousands. At the same time, we observe that ignoring the issue by setting a large constant stepsize in practice often leads to non-convergence of the fixed point iterations. To sidestep both issues, we considered a variant of the algorithm which only performs a single fixed-point iteration, and uses a stepsize hyperparameter \(\), where we pick the best from the set \(\{1,10,20\}\). We remark that this variant of the algorithm is clairvoyant only in spirit, and while it is a sound regret-minimization algorithm, we expect that the strong theoretical

Figure 2: Empirical performances of RM\({}^{+}\), PRM\({}^{+}\), ExRM\({}^{+}\), Stable PRM\({}^{+}\) and Smooth PRM\({}^{+}\)on our \(3 3\) matrix game (left plot) and on random instances for different step sizes.

guarantees of constant per-player regret do not apply. Nevertheless, in Fig. 3 we show that we are able to sometimes observe superior performance to (non-clairvoyant) predictive CFR in the four games we tried, which are described in the appendix. For both algorithms, we ignore the first 100 iterations, in which the iterates are very far from convergence. To compensate for the increased amount of computation needed at each iteration by our clairvoyant algorithm, we plot on the x-axis not the number of iterations but rather the number of gradients of the utility functions computed for each player. On the y-axis, we measure the gap to a coarse correlated equilibrium, which is equal to the maximum regret across the players, divided by the number of iterations.

## 7 Conclusion

We initiated the study of stability for \(^{+}\), and showed that both \(^{+}\)and predictive \(^{+}\)suffer from stability issues that can lead to slow convergence in games. We introduced two simple ideas, _restarting_ and _chopping off_, that ensure stability. Consequently, we introduced stable/smooth Predictive \(^{+}\), conceptual \(^{+}\) and Extragradient \(^{+}\), all with strong regret guarantees. Our results yield the first \(^{+}\)-based algorithms with better than \(O()\) regret guarantees, thus partially resolving the open question of whether optimism can yield theoretical speedup for \(^{+}\). Future directions include understanding whether our stability observations can be leveraged more directly in \(^{+}\) without adding our stability tricks, extending our results to general convex games, for which a regret minimizer based on Blackwell approachability similar to \(^{+}\) has been proposed recently , and combining clairvoyant updates with alternation.

Funding.J. Grand-Clement is supported by the Agence Nationale de la Recherche [Grant 11-LABX-0047] and by Hi! Paris. Christian Kroer is supported by the Office of Naval Research awards N00014-22-1-2530 and N00014-23-1-2374, and the National Science Foundation awards IIS-2147361 and IIS-2238960. Haipeng Luo and Chung-Wei Lee are supported by National Science Foundation award IIS-1943607.