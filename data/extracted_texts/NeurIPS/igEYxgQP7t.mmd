# RVD: A Handheld Device-Based Fundus Video Dataset

for Retinal Vessel Segmentation

MD Wahiduzzaman Khan\({}^{1*}\) Hongwei Sheng\({}^{12*}\) Hu Zhang\({}^{2*}\) Heming Du\({}^{3}\)

**Sen Wang\({}^{2}\) Minas Theodore Coroneo\({}^{4}\) Farshid Hajati\({}^{5}\) Sahar Sharifiou\({}^{1}\)**

**Michael Kalloniatis\({}^{6}\) Jack Phu\({}^{4}\) Ashish Agar\({}^{4}\) Zi Huang\({}^{2}\)**

**Mojtaba Golzan\({}^{1}\) Xin Yu\({}^{2}\)**

\({}^{1}\)University of Technology Sydney \({}^{2}\)University of Queensland \({}^{3}\)Australian National University

\({}^{4}\)University of New South Wales \({}^{5}\)Victoria University \({}^{6}\)University of Houston-Downtown

mdwahiduzzaman. khan@student.uts.edu.au

\({}^{*}\) Equal Contribution \({}^{}\) Corresponding Author

###### Abstract

Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices. The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility. Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition. The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old. It delivers comprehensive and precise annotations of retinal structures in both spatial and temporal dimensions, aiming to advance the landscape of vasculature segmentation. Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fine-grained vein-artery masks for further characterizing the granularities of each artery and vein. In addition, the dataset offers temporal annotations that capture the vessel pulsation characteristics, assisting in detecting ocular diseases that require fine-grained recognition of hemodynamic fluctuation. In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus posing great challenges to existing methods. Thanks to rich annotations and data scales, our dataset potentially paves the path for more advanced retinal analysis and accurate disease diagnosis. In the experiments, we provide evaluation metrics and benchmark results on our dataset, reflecting both the potential and challenges it offers for vessel segmentation tasks. We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early prevention. The dataset is available at \(}\) RVD.

## 1 Introduction

The observation of the retinal vasculature patterns serves as a reliable approach to tracking the morphological changes of eyes over time. These morphological changes have been found to be closely associated with a spectrum of ocular diseases, _e.g._, diabetic retinopathy, age-related macular degeneration, and glaucoma . Retinal vessel segmentation aims to provide pixel-level extraction of the visible vasculature from a fundus image . It is the initial yet fundamental step in objectively assessing vasculature in fundus images and quantitatively interpreting associated morphometrics. Thus, this task plays a pivotal role in understanding and diagnosing ocular diseases.

Existing methods for retinal vessel segmentation are designed on image-based datasets [38; 72; 69; 25], as shown in Fig. 1 (a). Although these datasets have contributed valuable vessel annotations for studying retinal segmentation, the static nature of images inherently limits their ability to portray dynamic retinal characteristics, _e.g._, vessel pulsations. These dynamic phenomena play a vital role in facilitating comprehensive and in-depth understanding of retinal functionality and vasculature structure. Moreover, image-based datasets are captured by expensive bench-top ophthalmic equipment, which is operated by professionally trained clinicians [31; 34]. Such requirements potentially limit the scale of the datasets and data diversity, thereby adversely affecting the generalization ability of the models trained on these datasets.

In recent years, advances in imaging technology have enabled the usage of smartphone-based devices for retinal observation [79; 32]. They offer better flexibility and portability, allowing for scalable data collection. In this paper, we introduce the first video-based retinal vessel dataset (RVD), a collection of 635 smartphone-based videos with detailed vessel annotation. These videos are recorded from four clinics, including patients from 50 to 75 years old. Some examples of our dataset are shown in Fig. 1 (b). The sequential frames capture the continuous changes in retinal vessels and thus significantly facilitate the analysis of subtle fluctuations in the retinal structure. Therefore, the use of portable devices for data acquisition and the provision of the video modality remarkably overcome the limitations of existing datasets.

The annotations provided in our dataset span two dimensions: spatial and temporal. In the spatial dimension, we offer three distinct levels of annotations: binary vessel masks, general vein-artery masks, and fine-grained vein-artery masks. Each kind of annotation is tailored to specific clinical purposes. Specifically, for the binary vessel masks, we identify the sharpest and most representative frame from the video clip and generate binary masks representing the skeletal structure of the vessels. This mask primarily targets the holistic vessel structure but neglects the difference between arteries and veins. For general vein-artery masks, we differentiate veins and arteries based on their respective vessel calibres and generate separate masks for them respectively. Lastly, in contrast to the general differentiation between arteries and veins, the fine-grained vein-artery masks further divide each retinal artery and vein into sections based on a set of pre-defined vessel widths. We thus generate eight different vein-artery masks for each sample and these masks precisely reflect the granularities of retinal vessels. These sophisticated masks are highly demanded when detecting ocular diseases [3; 9].

In the temporal dimension, we enrich our dataset with annotations of the complex dynamics of retinal vasculature. For each video, we focus on the optic disk regions where the retinal vessel fluctuation normally occurs. We then select and annotate frames with the maximal and minimal pulse widths as well as label the existence of spontaneous retinal venous pulsations (SVP). The existence and extent of vessel changes signify vascular pulsations and cranial pressure-related alterations. Clinically, the signals of pulsation facilitate the detection of abnormalities in retinal vessels, while precise identification of pressure-related alterations aids in detecting temporally-dependent ocular diseases. Our integration of temporal annotations thus increases its potential for ocular disease diagnosis.

The distinction between smartphone-based and benchtop devices and data modality differences result in domain gaps, as illustrated in Fig. 1. Furthermore, since our data are collected by handheld devices

Figure 1: **(a)** Samples from existing image based retinal vessel datasets: DRIVE , STARE , HRF , and CHASE_DB1 . **(b)** Video samples from our retinal vessel dataset. Different from existing image-based datasets, our dataset captures continuous changes in retinal vessels and facilitates the analysis of vessel dynamics in the retina. **(c)** The intensity distributions of our dataset and existing ones. The differences imply the domain gaps between our dataset and existing ones.

in clinics, our dataset also involves more realistic factors, _e.g._, the operations of the clinicians, surrounding illumination conditions, and eye movements of patients during video capture. Consequently, our dataset presents more challenges for existing vessel segmentation methods. More importantly, the large number of training samples and detailed annotations in our dataset will likely pave the way toward more advanced yet portable retinal analysis and more accurate disease diagnosis. In the experiments, we delve into an in-depth analysis of our dataset and provide benchmark results of different tasks on our newly curated dataset.

The main contributions of our paper are summarized as follows:

* **Dataset construction**: We construct a new video-based retinal vessel dataset (RVD) with rich spatial and temporal annotations for vessel segmentation tasks. To the best of our knowledge, RVD is the first mobile-device based dataset for retinal vessel segmentation.
* **Three-level spatial annotations**: Our dataset introduces three levels of annotations in spatial, comprising binary vessel masks, general vein-artery masks, and fine-grained vein-artery masks. The hierarchical and diverse descriptions of spatial annotations enable us to better analyze the vessel structure.
* **Temporal annotations**: Our dataset also provides temporal annotations of spontaneous retinal venous pulsations (SVP) to reveal the dynamic changes in retinal vessels. This enables the assessment of pulsatile variations in retinal vessels.
* **Benchmarking**: We investigate the gap between our dataset and previous retinal datasets by assessing the performance of several state-of-the-art methods. The experimental results will shed some light on mobile-device based retinal vessel segmentation.

## 2 Related Work

**Existing Retinal Datasets:** In the realm of retinal vessel segmentation, various retinal vessel datasets have been proposed. Existing datasets can be roughly categorized into two streams: binary vessel based ones and artery-vein based ones. Among the datasets with binary vessel masks, DRIVE , STARE , HRF , and CHASE_DB1  have emerged as the most frequently used datasets. In fact, each of these datasets only comprises dozens of images. For example, DRIVE consists of 40 images captured in the 45-degree field of view, with an image size of 584\(\)565 pixels. Besides, DRiDB , ARIA , IOSTAR , and RC-SLO  are another publicly available datasets for retinal vessel segmentation. However, they are less used in recent years considering their data quality and maintenance. Recently, the FIVES dataset has been introduced , with data distributed across four categories: normal retinas, retinas affected by Diabetic Retinopathy, Glaucoma, and Age-related Macular Degeneration. It comprises 800 retinal images.

Regarding the datasets with artery-vein masks, RITE , AV-DRIVE , INSPIRE-AVR , and WIDE  are the available ones. The AV-DRIVE dataset, derived from DRIVE, consists of 40 images and offers separate ground truth masks for arteries and veins. The INSPIRE-AVR is an independently constructed dataset with artery-vein ground truth masks. It consists of 40 color images in total. The WIDE dataset provides 30 scanning laser ophthalmoscope (SLO) images.

In contrast to existing datasets which are collected with cumbersome bench-top devices and are composed of static images, our dataset is constructed with portable handheld devices and is video-based. Our dataset preserves the dynamic characteristics of vessels. Besides, existing datasets typically provide only one type of annotation for a specific research purpose, whereas our dataset offers annotations in both spatial and temporal dimensions. The spatial annotations include binary vessel masks, general vein-artery masks, and fine-grained vein-artery masks, respectively. The temporal annotations reveal the state of SVP, an important signal for diagnosing various diseases.

**Methods for Retinal Vessel Segmentation:** In the past years, a variety of methods have been developed for retinal vessel segmentation. Traditional methods mainly depend on handcrafted features , which are less discriminative and effective . With the unprecedented breakthroughs of deep neural networks (DNNs) in the image classification, detection, and segmentation tasks, researchers have explored the potential of DNNs in retinal vessel segmentation . Many works  adopt fully convolutional networks  to produce more accurate segmentation of retinal images by combining semantic information from deep layers with appearance information from shallow layers. Several works have focused on modifying the U-Net structure [74; 64; 85] for vessel segmentation.  first introduces the residual connection into U-net to detect vessels. This idea has been adopted in later studies [4; 41; 26; 78].  introduces the local-region and cross-dataset contrastive learning losses in training to explore a more powerful feature embedding space. Besides, several other methods employ various networks and strategies for retinal vessel segmentation, such as generative and adversarial networks [40; 70], ensemble learning [49; 75; 44], and graph convolutional network .

The aforementioned methods are mainly conducted on datasets DRIVE, STARE, CHASE_DB1, and HRF, with binary vessel masks as supervision. Thanks to INSPIRE-AVR, AV-DRIVE, and WIDE, many works have been proposed to distinguish artery and vein [18; 21; 87; 50]. In , a multi-task deep neural network with spatial activation is proposed. The constructed network is able to segment full retinal vessels, arteries, and veins simultaneously. More recently, transformer based models, _e.g._, ViT , Swin transformer , and Mask2Former , have been proposed. These models have demonstrated their superior performance in capturing visual concepts and become popular backbones in visual understanding tasks . We thus choose these models in the experiments to study the characteristics of our proposed dataset.

## 3 Our Proposed RVD

In this section, we first describe our data collection process and data sources1 Concerning privacy and ethic, we perform this study in accordance with the guidelines of the Tenets of Helsinki. Written consent was obtained from all participants prior to any data collection, and all examination protocols adhered to the tenets of the Declaration of Helsinki. Once clinical data have been collected, we need to clean and pre-process the data in order to facilitate clinicians' annotations and neural network training. In our work, the annotations are provided by professionally well-trained clinicians, and they have been asked to not only annotate conventional spatial segmentation masks but also temporal segmentation masks for dynamic biomarkers, such as Spontaneous retinal Venous Pulsations (SVPs). Last, we will introduce the data split and relevant tasks that are supported by our dataset.

### Data Collection

For data collection, the employed hand-held fundus imaging devices are constructed by connecting a smartphone to the fundus camera lens. Then, clinicians are trained to operate the hand-held devices to examine patients' retinas while collecting fundus videos. These participants are fully aware of data collection when they undergo their annual medical examinations. With the help of clinicians, a total of 415 patients from four different clinics participate in the data collection process. As data are collected in different clinics over the past five years, the employed smartphones are different, thus increasing the diversity of data sources. More specifically, 264 males and 151 females are included here. Their ages range from 50 to 75. People of these ages are commonly considered to be at high risk for eye-related diseases, such as glaucoma and hypertension . Our dataset involves both videos recorded from healthy eyes and videos from eyes with ocular diseases.

During the collection, one eye of each patient is recorded at a time. In this manner, at least one fundus video of each patient has been recorded and some participate multiple times in video recording. As a

  
**Dataset** & **Resolution** & **Modality** & **Device** & **Num** & **Dimension** & **Annotation type** \\  STARE  & 605\(\)700 & Image & Benchtop & 20 & Spatial & Binary \\ DRIVE  & 768\(\)584 & Image & Benchtop & 40 & Spatial & Binary \\ ARIA  & 576\(\)768 & Image & Benchtop & 161 & Spatial & Binary \\ CHASEDB1  & 990\(\)960 & Image & Benchtop & 28 & Spatial & Binary \\ INSPIRE-AVR  & 293\(\)2048 & Image & Benchtop & 40 & Spatial & Multi-class \\ HRF  & 3304\(\)236 & Image & Benchtop & 45 & Spatial & Binary \\ RITE  & 768\(\)584 & Image & Benchtop & 40 & Spatial & Multi-class \\ FIVES  & 2048\(\)2048 & Image & Benchtop & 800 & Spatial & Binary \\ RAVIR  & 768\(\)768 & Image & IR Laser & 42 & Spatial & Multi-class \\ 
**RVD (ours)** & **1800\(\)1800** & **Video** & **Hand-held** & **1,270** & **Spatial** & **Multi-class** \\   

Table 1: Comparisons of different retinal vessel segmentation datasets. “Num” denotes the number of annotated image frames.

result, a total of 635 RGB videos have been captured. All captured videos have a frame rate of 25 frames per second, with the duration varying between 2 to 30 seconds. The total number of frames in our dataset is over 130,000. This collection process ensures the generality and diversity of our dataset for retinal vessel analysis. The detailed information of our dataset is shown in Table 1 and some examples could be found in Fig. 1 (b).

### Data Cleaning and Preprocessing

Although we have tried our best to minimize environmental interference during collection, the original videos still exhibit various noise, such as video jittering and motion blur. Such noise will severely degrade the quality of collected videos and impose more difficulties in annotations. Hence, we eliminate the noise in the footage to improve the quality of our dataset and facilitate annotations.

**Data cleaning:** Considering that blood vessel dynamics mostly appear in ODR, we remove the video segments without ODR and ensure the existence of ODR in all videos. To this end, we employ an ODR detection method to localize ODR. Specifically, we label ODR regions by bounding-boxes, and for each video, we only annotate one frame per 25 frames (_i.e._, 1 second), similar to . Then, we leverage the labeled ODR as supervision to train the Faster-RCNN detection network . After that, the remaining frames are labeled by the trained Faster-RCNN. Manual check is also conducted to modify erroneous detection results by annotators. We only select video segments in the dataset if their ODRs are detectable in a minimum of 30 continuous frames. Such operations help maintain the overall quality of our video data.

To further improve the data quality, we leverage the optical flow to pinpoint frames with a high level of blur. Optical flow captures the spatial alterations between distinct frames, and thus it could serve as an indicator of spatial sharpness. Frames with large optical flow are subsequently discarded, as they likely correspond to instances of blurring. Similar to ODR detection, annotators (non-experts) also manually remove frames that undergo severe blur but have not been spotted by optical flow.

**Data Preprocessing:** In retinal vessel segmentation, ODR and its surrounding area are the most representative regions of the eye and provide extensive details about retinal vessels. However, the inherent ocular movement results in varying ODR positions across different frames. Such variations can impede the precise observation and annotation of SVP by clinicians. To tackle this issue, we employ the template matching algorithm  to stabilize the ODRs across video segments, ensuring a consistent ODR placement and a fixed field of view across frames. This facilitates human observation and machine perception of dynamic changes surrounding the ODR, thus greatly enhancing annotations and clinical diagnosis.

### Data Annotation

To ensure the annotation quality, six clinicians are involved in this process. In the process of annotation, we also incorporate inter-rater reliability using the IoU metric to ensure the consistency of annotations across different annotators. Specifically, after every 100 annotations, we will randomly select 5 frames from each clinician and then reassign the frames to another clinician for annotations. If the IoU is lower than 0.95, the annotations will be manually reviewed by a third clinician. For the temporal SVP annotations, each video has been annotated by three clinicians to reduce annotation bias.

#### 3.3.1 Spatial segmentation

To mitigate the redundancy of annotating similar video frames, we currently select and annotate two frames for each video. Here, we adopt a three-fold strategy to identify the most representative frame from a video. **(1)** A frame is in high image quality (high sharpness/contrast) and contains the most vessels in a video; **(2)** A frame can cover ODR, fovea, and macula regions without obstructions. These regions pathologically have a significant number of capillaries; **(3)** Based on the first frame, we choose another frame that not only has visible ODR with high-density vessels but also has a maximum spatial distance between its ODR and the previously selected ones. In this way, the annotated two frames will cover most of the retinal vasculature and any pathological regions of the retina. We then elaborate on creating three types of spatial annotations: binary vessel masks, general vein-artery masks, and fine-grained vein-artery masks, as follows:

**Binary vessel masks:** To generate the binary vessel masks, we adopt a similar method proposed in . For each frame, we first draft a centerline-level annotation using the ImageJ software  and generate the delineation of vessel boundaries to obtain the main structure of vessels. Then we employ our experts to manually refine the structure by correcting the boundaries and improving the details of small capillaries. We can obtain the binary vessel masks by assigning the label to the refined structure (see Fig. 2 (b)).

**General vein-artery masks:** Many intracranial vascular diseases are found to be related to retinal vessels and affect the arteries and veins differently . Thus, distinguishing between the retinal artery and vein plays a critical role in the clinical biomarker study of how various systemic and cardiovascular diseases affect the retinal vessels. In practice, the arteries and veins can be distinguished based on their difference in three aspects: color, light reflection, and calibres. The veins generally have a darker color than arteries and show a smaller central light reflex. Meanwhile, the veins are also wider than adjacent arteries. Then, clinicians only need to assign labels (_i.e._, vein and artery) to the vessels and obtain the vein-artery masks, as shown in Fig. 2 (c).

**Fine-grained vein-artery masks:** Vascular morphology holds substantial clinical significance, as alterations in vessel diameters frequently signify the presence of various diseases. For example, damage of the small retinal vessels could result in diabetic retinopathy . Similarly, glaucoma pathogenesis is postulated to be linked to alterations in the retinal vasculature, such as retinal arteriolar narrowing and decreased fractal dimension . Despite the clinical importance of such information, existing datasets scarcely provide this type of labels. Therefore, we consider the morphological characteristics of each artery and vein in our dataset and thus provide fine-grained vein-artery masks based on the vessel widths.

Specifically, we first measure the vessel diameters automatically via "Vessel Diameters" plugin in ImageJ.2 Then, we divide the arteries into multiple small vessel segments based on the diameters of the vessels. Based on the largest diameter among these artery segments, we define four levels of widths according to specific ratios. More specifically, a vessel segment within the range of 0-25% of the largest diameter is categorized as level 0. Similarly, levels 1, 2, and 3 correspond to vessel widths in the ranges of 25%-50%, 50%-75%, and 75%-100% of the largest diameter, respectively. Afterward, we obtain four-class masks for arteries based on vessel widths. The same operation is also applied to veins. After the automatic processing, clinicians will validate the quality of the fine-grained

Figure 2: **Left: Illustration of our multi-grained segmentation annotations. For each given fundus image (a), we provide three different kinds of segmentation masks including a conventional binary mask (b), a general artery-vein mask (c) and a fine-grained artery-vein mask (d) (VL: vein width level, AL: artery width level, the numbers (0 to 3) indicate four increasing width levels). Right: Overview of the temporal annotations (e), including ODR locations, presence and absence of SVP, temporal localization of SVP, and “peak” and “trough” of SVP.**segmentation masks. This process ultimately yields eight-class masks for both arteries and veins (see Fig. 2 (d)). Those masks significantly enrich the granularity of our dataset.

#### 3.3.2 Temporal localization

**Existence of SVP:** Based on the results of data cleaning and preprocessing, we utilize the stabilized videos and further annotate the dynamic state of vessel pulsations. Spontaneous retinal Venous Pulsation (SVP) plays a crucial role as a biomarker in retina assessments. Specifically, SVP is characterized by rhythmic pulsations evident in the central retinal vein and its branches, typically observable within the optic disc region (ODR) of the retinas. The absence of SVP holds substantial clinical significance, as it is correlated with certain pathologies. For example, the absence of SVP is associated with progressive glaucoma , and it is indicative of increased intracranial pressure . Considering the requirement of specialized knowledge, we have invited multiple clinicians and finished the annotation of SVP presence or absence for each video in our dataset. Once the annotation process completes, we obtain 335 "SVP-present" videos and 300 "SVP-absent" videos respectively. This annotation establishes a fundamental task of SVP detection, facilitating further analysis and investigation on the relationship between SVP and eye diseases.

**Temporal duration of SVP:** After annotating the existence of SVP in the stabilized videos, some "SVP-present" videos may not contain SVP throughout the whole video. This means in some frames SVP is not visible. Using these videos to train an SVP classification model would suffer ambiguity especially when an entire video cannot be fed into a neural network. Therefore, we further provide temporal emergence annotations of SVP by indicating the starting and ending frames of retinal vessel fluctuation (see Fig. 2 (e)). The detailed duration of SVP serves two purposes: it acts as a valuable signal to improve the performance of SVP detection tasks and concurrently sets a new task for SVP temporal localization. We obtain videos in three distinct groups: 156 videos containing intermittent SVPs, 179 videos demonstrating persistent SVPs, and the remaining 300 videos without SVPs. These temporal annotations allow us to better understand retinal vessel dynamics.

**"Peak" and "Trough" annotations of SVP:** As discussed above, SVP reflects the temporal dilation and contraction in retinal vessels. The state with maximal dilation is characterized as "peak", whereas those with maximal contraction are termed "trough". Here, we select frames corresponding to the "peak" and "trough" states from each "SVP-present" video. Subsequently, we generate corresponding masks for these selected frames, yielding a total of 670 annotated masks. This annotation allows us to quantitatively measure the extent of pulsations and occurring positions of vessel pulsations.

### Data Protocols

**Data split:** When partitioning our dataset for training and evaluation, we also take into account the similarity of the recorded videos of the same person. Specifically, we ensure that videos captured from the same patient are allocated to the same subset during the partitioning process. This strategy aims to decrease the similarity between training data and testing data and thus minimizes performance bias. In practice, we divide the data based on patient IDs. In this manner, the same patient's videos will not appear simultaneously in both the training and testing sets. Then, we select 517 videos from the 635 videos for training and validation, and the rest 118 videos are used for testing. We also cross-validate a method with three different data splits and will release the dataset and data splits.

**Metrics:** Based on our annotations, we can conduct tasks in two major categories. (1) Retinal vessel segmentation metrics: Since the binary, general artery-vein and fine-grained vessel segmentation tasks are essentially semantic segmentation, we adopt the mean Intersection over Union (mIoU), mean Accuracy (mAcc), and macro-averaged F1-score (mFscore) to evaluate the performance of models on our dataset. Note that, mFscore is the average of F1-score by treating all classes equally and the F1 score is also known as the DICE score in binary classification or segmentation tasks3, the mFscore thus essentially equals the mean of DICE scores for segmentation tasks. (2) SVP recognition and temporal localization metrics: SVP recognition is to classify whether SVP exists in a video. SVP localization task is to identify the time period where SVP appears in a video. We adopt the Accuracy (Acc), Area Under the Receiver Operating Characteristic Curve (AUROC), and Recall for SVP recognition. The frame-mAP (F-mAP), video-mAP (V-mAP)  under IoU threshold 0.5 and mean Intersection over Union (mIOU) are adopted for the task of SVP localization.

[MISSING_PAGE_FAIL:8]

binary vessel segmentation performance, we include the following datasets: CHASE DBI (C-DB.), DRIVE (DRI.), HRF, and STARE (STA.). Then, we also conduct general artery-vein segmentation on the RITE dataset. The results are shown in Table 4. Note that existing datasets do not support fine-grained eight-class segmentation, and thus we did not test our data in this setting. Due to the domain gap, the models suffer performance drop. The results also indicate that our dataset provides unique data samples. The visualization is illustrated in the Appendix. From our experimental results above, we can tell that the retinal vessel segmentation is far from being solved. Our RVD dataset will serve as a valuable resource, motivating future explorations in retinal vessel segmentation.

## 5 Conclusion

In this work, we propose the first video-based retinal vessel segmentation dataset by employing hand-held devices for data acquisition. Our dataset significantly complements the current benchtop-based datasets for retinal vessel segmentation and enables SVP detection and localization. More importantly, it offers rich annotations for both spatial vessel segmentation and temporal SVP localization. In comparison to existing datasets, our dataset is not only the largest scale one with the most diverse annotations but also more challenging. The domain gaps between our dataset and existing ones allows researchers to investigate how to minimize the domain gaps in vessel segmentation. Therefore, our curated dataset RVD is valuable for retinal vessel segmentation and would facilitate the clinical diagnosis of eye-related diseases.

    &  &  &  \\  & &  \\   & & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\  & & & & & & & & & & & & \\    & UNet & \(70.23 0.2\) & \(62.17 0.4\) & \(65.21 0.2\) & \(62.21 0.3\) & \(70.89 0.3\) & \(56.81 0.5\) & \(60.95 0.2\) & \(68.92 0.4\) & \(51.05 0.2\) & \(28.83 0.4\) \\  & ResNet50 & \(71.56 0.3\) & \(63.41 0.2\) & \(66.34 0.1\) & \(63.04 0.2\) & \(71.27 0.2\) & \(57.76 0.4\) & \(68.73 0.3\) & \(64.95 0.2\) & \(51.84 0.3\) & \(29.87 0.3\) \\  & ResNet10 & \(71.89 0.4\) & \(63.59 0.2\) & \(66.72 0.3\) & \(63.98 0.1\) & \(71.98 0.3\) & \(58.17 0.9\) & \(64.81 0.2\) & \(65.26 0.3\) & \(52.56 0.2\) & \(30.30 0.4\) \\   & ResNet50 & \(72.32 0.2\) & \(65.71 0.3\) & \(67.03 0.2\) & \(66.74 0.1\) & \(73.05 0.6\) & \(65.65 0.2\) & \(71.00 1.8\) & \(68.98 0.4\) & \(45.45 0.2\) & \(34.94 0.3\) \\  & ResNet101 & \(73.27 0.2\) & \(67.20 0.1\) & \(67.57 0.7\) & \(67.25 0.3\) & \(76.70 0.6\) & \(65.66 0.1\) & \(65.27 0.2\) & \(69.39 0.3\) & \(55.23 0.12.4\) \\   & Swin-71 & \(74.97 0.2\) & \(67.59 0.3\) & \(69.99 0.3\) & \(67.74 0.3\) & \(47.71 0.3\) & \(63.89 0.3\) & \(67.40 0.3\) & \(69.64 0.1\) & \(55.65 0.4\) & \(58.88 0.2\) \\   & Swin-71 & \(75.12 0.2\) & \(67.98 0.2\) & \(69.93 0.1\) & \(67.51 0.3\) & \(73.97 0.7\) & \(67.05 0.3\) & \(72.87 0.6\) & \(69.53 0.4\) & \(56.47 0.2\) & \(51.42 0.3\) \\   & Swin-81 & \(74.93 0.2\) & \(68.13 0.2\) & \(71.21 0.7\) & \(67.40 0.3\) & \(76.03 0.6\) & \(68.84 0.1\) & \(71.30 0.2\) & \(69.57 0.7\) & \(57.44 0.2\) & \(52.99 0.4\) \\   & Swin-82 & \(76.95 0.6\) & \(70.47 0.2\) & \(73.95 0.3\) & \(68.36 0.2\) & \(68.76 0.2\) & \(64.06 0.4\) & \(74.37 0.3\) & \(69.34 0.4\) & \(57.05 0.2\) & \(52.98 0.5\) \\   & Swin-71 & \(76.56 0.1\) & \(70.45 0.2\) & \(73.02 0.1\) & \(67.72 0.4\) & \(76.15 0.3\) & \(67.31 0.3\) & \(73.56 0.9\) & \(69.64 1.5\) & \(57.88 0.2\) & \(50.98 0.3\) \\   

Table 4: Evaluation of domain gaps between different datasets.

Figure 3: Visualization in the binary, general artery-vein, and fine-grained artery-vein segmentation.