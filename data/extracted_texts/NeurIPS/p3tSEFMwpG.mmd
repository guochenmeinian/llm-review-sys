# Drift-Resilient TabPFN: In-Context Learning Temporal Distribution Shifts on Tabular Data

Kai Helli\({}^{*,1,2}\) David Schnurr\({}^{*,1,3}\) Noah Hollmann\({}^{1,4}\) Samuel Muller\({}^{1}\) Frank Hutter\({}^{5,1}\)

\({}^{1}\) University of Freiburg, \({}^{2}\) Technical University of Munich, \({}^{3}\) ETH Zurich,

\({}^{4}\) Charite University Medicine Berlin, \({}^{5}\) ELLIS Institute Tubingen, \({}^{*}\) Equal contribution.

Correspondence to kai.helli@tum.de

###### Abstract

While most ML models expect independent and identically distributed data, this assumption is often violated in real-world scenarios due to distribution shifts, resulting in the degradation of machine learning model performance. Until now, no tabular method has consistently outperformed classical supervised learning, which ignores these shifts. To address temporal distribution shifts, we present Drift-Resilient TabPFN, a fresh approach based on In-Context Learning with a Prior-Data Fitted Network that learns the learning algorithm itself: it accepts the entire training dataset as input and makes predictions on the test set in a single forward pass. Specifically, it learns to approximate Bayesian inference on synthetic datasets drawn from a prior that specifies the model's inductive bias. This prior is based on structural causal models (SCM), which gradually shift over time. To model shifts of these causal models, we use a secondary SCM, that specifies changes in the primary model parameters. The resulting Drift-Resilient TabPFN can be applied to unseen data, runs in seconds on small to moderately sized datasets and needs no hyperparameter tuning. Comprehensive evaluations across 18 synthetic and real-world datasets demonstrate large performance improvements over a wide range of baselines, such as XGB, CatBoost, TabPFN, and applicable methods featured in the Wild-Time benchmark. Compared to the strongest baselines, it improves accuracy from 0.688 to 0.744 and ROC AUC from 0.786 to 0.832 while maintaining stronger calibration. This approach could serve as significant groundwork for further research on out-of-distribution prediction.

## 1 Introduction

In traditional machine learning the train and test data are assumed to be sampled from the same distribution . However, this assumption of independent and identically distributed (i.i.d.) data is commonly violated in real-world scenarios due to distribution shifts, resulting in performance degradation of standard machine learning (ML) models over time . Research in the area of temporal domain generalization (Temporal DG) tries to address these shifts by developing methods that perform consistently across temporal domains and generalize beyond the training regimen, i.e. into the future. In fields such as healthcare, climate science, or finance, data is most often organized in a tabular format . Here shifts are driven by hidden variables such as policy or climate changes, equipment updates, seasonal changes, or activity cycles, limiting real-world model deployment .

Young and Steele  describe declining mortality quantification in a hospital system while Pasterkamp et al.  show deterioration of cardiovascular risk models over time, leading to increased mortality; Ganesan et al.  show the COVID-19 pandemic exacerbated this issue, as ICU mortality prediction models failed to adapt to the unique characteristics of COVID-19 patients; environmental models need to continually adapt to climate changes ; fraud detection models need to continuously adaptas the strategies of fraudsters adapt to the models themselves ; these feedback loops often arise in practice, where model deployment inherently causes a system change .

Robustness to such distribution shifts stands as a prominent challenge in current ML research . So far multiple approaches have been proposed to address temporal distribution shifts using neural networks (NNs) . However, modeling distribution shifts in tabular data presents a two-fold challenge: (i) NNs have struggled to model and extrapolate distribution shifts to date  (ii) approaches for modeling distribution shifts have mostly employed NNs, while tree-based methods have consistently outperformed NNs in handling tabular data  - leaving a wide methodological gap in addressing this common real-world scenario.

We provide a fresh perspective on predicting given distribution shifts by leveraging in-context-learning (ICL) to learn the prediction algorithm itself - bypassing many challenges encountered in this setting. Our approach builds on the foundation of Prior-Data Fitted Networks (PFNs; 18) and TabPFN (19; see Section 3.1). PFNs leverage large-scale ML and ICL techniques to approximate Bayesian inference accurately for any prior that can be sampled from. They are trained on millions of synthetic datasets sampled from this prior. For each such dataset, a supervised learning task is constructed, and the model is asked to predict on held out test samples. Then, the PFN is able to apply the principles learned on this synthetic data to real-world datasets, effectively having learned a prediction algorithm.

This paper introduces _Drift-Resilient TabPFN_, an adaptation of the TabPFN framework tailored for tabular datasets exhibiting temporal distribution shifts. Our idea is as follows: Data distribution shifts can be modeled as gradual changes to the structural causal model (SCM; 20; 21) underlying the data. By including the assumption that underlying models change over time into the approximated prior, our models learn to estimate, adapt to, and extrapolate these model changes. While likely no causal model exactly underlies real-world data, we find this approximation to be empirically and intuitively useful. Specifically, we suggest shifting edge weights between nodes that affect causal relationships while sampling instances in each dataset. To accurately model these edge shifts, we propose using a \(2^{}\)-order SCM, a secondary model that adjusts the primary SCM, that takes a time indicator as input and outputs the weight shifts. To represent our temporal domain indicators, we use Time2Vec .

Figure 1: High-level overview of our method. We train a transformer that accepts entire datasets as input to learn the learning algorithm itself by training on millions of synthetic datasets once as part of algorithm development. The trained model can be applied to arbitrary real-world datasets. In (b), X, c, and y refer to features, time domain, and label respectively. In (c), we show predictions on test domains 4 (left) and 5 (right), where we see a distribution shift. Drift-Resilient TabPFN accurately updates decision boundaries in this example.

To validate the robustness and adaptability of our approach, we conduct a comprehensive evaluation of 18 tabular datasets. Among these, 8 are synthetic toy problems, that allow us to analyze model behavior in detail. The remaining 10 are real-world datasets, each representative of different temporal distribution shifts. Our model is benchmarked against (i) state-of-the-art methods for handling temporal domain generalization (implemented in the Wild-Time benchmark ), as well as (ii) state-of-the-art methods for tabular data, that do not handle distribution shifts explicitly (the unmodified version of TabPFN, well-established classifiers such as XGBoost, Catboost, and LightGBM [23; 24; 25]). In addition, we qualitatively analyse the model's behavior and prediction characteristics.

Our model consistently outperforms all baselines on out-of-distribution data across both synthetic and real-world datasets while simultaneously exhibiting strong calibration. Qualitatively, we find that Drift-Resilient TabPFN dynamically adjusts its decision boundary over time, surpassing existing models in leveraging temporal domain information to extrapolate shifts far into the future and capture trends effectively.

Key contributions of this paper are:

1. We provide a novel perspective to learning under distribution shifts: Rather than specifying how exactly out-of-distribution data should be handled, we leverage in-context learning to learn an algorithm that handles such scenarios.
2. We propose a prior for temporal shifts, based on sparse, non-linear, and correlated mechanism shifts in structural causal models. This unifies shift types (covariate shift, prior probability shift, concept shift, and combinations thereof) and allows to extrapolate these changes.
3. We release Drift-Resilient TabPFN, a tabular data model for predicting under temporal distribution shifts, that automatically recognizes and adapts to shifts in the data and requires no hyperparameter tuning.
4. We extensively evaluate our models and state-of-the-art baselines on 18 synthetic and real-world datasets demonstrating improved out-of-distribution performance while requiring, on average, only 10.9s for training and prediction combined.

## 2 Background

### Distribution Shift Settings

Consider \(\), \(\), and \(\), the sample spaces for features, labels, and domain indices, respectively. In these spaces, specific instances are represented by \(\), \(y\), and \(\). The corresponding random variables for these instances are denoted as \(X\), \(Y\), and \(C\).

A dataset is then a collection of \(n\) tuples, denoted as \(:=\{(_{i},y_{i},}_{k})\}_{i=1}^{n}\). Each tuple is thereby drawn from a conditional distribution \((X,Y C=_{k})\) over the spaces \(\). Here, \(_{k}\) serves as a domain index that conditions the distribution from which the respective sample is drawn. Given that the true underlying temporal domain is mostly unknown in real-world data, it often is approximated and represented as \(}_{k}\) within the dataset. To isolate samples from a specific domain \(}_{k}\), we introduce the sub-dataset \(_{}_{k}}\), defined as \(_{}_{k}}:=\{(,y,})|}=}_{k}\}\). This adapted notation allows for a more nuanced analysis of datasets with domain shifts, building upon the frameworks presented by Wang et al.  and Sheth et al. .

Domain Generalization (DG).DG aims to train a model that generalizes across a continuum of source domains \(^{}\) to the target domains \(^{}\) without access to samples of the target domains. The objective is to learn a mapping function \(f:\) that minimizes the expected loss when applied to new, previously unseen target domains.

Temporal Domain Generalization (Temporal DG).In temporal DG, a special case of DG, the domain index set \(\) is one-dimensional and follows a total ordering, \(c_{1} c_{2}\). In this framework, the training set is limited to source domains that precede target domains in this ordering, namely \(^{}=\{c_{1},c_{2},,c_{t}\}\). In this setting, the objective is to learn a predictive model \(f\) that performs well on these source domains while also robustly generalizing to future, unseen domains \(^{}=\{c_{t+1},c_{t+2},,c_{n}\}\). The indices of all training domains \(^{}\) and the index of the current testing domain \(c_{k}^{}\) are provided to the model.

### Related Work

While DG has drawn increasing attention in the research community [28; 29; 30; 31; 32; 33; 34; 35; 36; 37; 38], its temporal variant remains under-explored. We review existing DG benchmarks on tabular data, DG methods, the temporal DG benchmark Wild-Time , temporal DG methods, as well as relevant TaboPFN studies.

**Tabular Distribution Shift Benchmarks.** Several benchmarks have been introduced to assess methods for tabular data under distribution shifts. (1) **Shifts** and **Shifts 2.0**[39; 40] are uncertainty focused benchmarks. **Shifts 2.0** includes five tasks, two of which involve tabular data subject to temporal and spatio-temporal shifts. (2) **WhyShift**, focusing on spatio-temporal shifts, offers five real-world tabular datasets, including the ACS dataset , which we also use in a subsampled form. Evaluating 22 methods like Gradient Boosted Decision Trees (GBDTs), MLPs, and robustness techniques, WhyShift finds that robustness methods do not consistently enhance out-of-distribution (OOD) performance. They also find that while GBDTs perform better, the gap between in-distribution (ID) and OOD performance persists, likely due to GBDTs being better fitted to the ID distribution. (3) **TableShift** includes 15 tabular binary classification tasks, with 10 relevant to DG. Their work evaluates 19 model types, including several DG methods, finding that methods tailored for distribution shifts do not consistently outperform GBDTs. While generalization gaps can be slightly reduced, each robustness method comes at the cost of ID performance. (4) **Wild-Tab** focuses on domain generalization within tabular regression using three large datasets. Their study compares 10 generalization techniques against standard Empirical Risk Minimization (ERM) applied to MLPs. Similar to previous work , they find that ERM was not consistently outperformed by specialized DG methods. Notably, no advantage of GBDTs over ERM on MLPs was observed in their datasets.

Unlike these benchmarks, which focus on large-scale datasets, our work addresses the overlooked challenges of small-scale temporal distribution shift datasets. However, their insights on generalization and robustness methods align with our findings, providing valuable context for our approach.

**DG Methods.** Several techniques have been proposed to improve robustness to distribution shifts in DG . Key approaches include domain-invariant learning methods, such as **Deep CORAL**, **IRM**, and **DANN**, which aim to learn representations that generalize across domains. Data augmentation strategies, like **Mixup** and **LISA**, contribute to generalization by generating synthetic data variations. Additionally, robust optimization techniques, including **VRev**, **GroupDRO**, **EQRM**, and **SWA**, aim to improve performance under distributional shifts by optimizing for worst-case scenarios or incorporating model uncertainty.

In relation to our work, data augmentation strategies are conceptually similar, as we also teach the model DG rather than adding invariances to the architecture. In contrast to augmentation, though, we learn to generalize using completely artificial data on the meta level rather than through manipulation of the target dataset. The other methods focus on designing models specifically for DG, while our approach completely relies on the model to learn to handle distribution shifts. While similar to continual meta-learning (CML) approaches, our method uses domain identifiers to generalize to unseen domains, whereas CML continuously adapts to evolving contexts without identifiers .

**Wild-Time Benchmark.** Wild-Time  is a benchmark of five datasets designed to study the real-world effects of temporal distribution shifts - an area largely overlooked by previous benchmarks. While Wild-Time primarily uses non-tabular data, it evaluates a wide array of techniques on its tabular dataset, including classical supervised learning (ERM), fine-tuning, and several previously mentioned general DG methods adapted to temporal distribution shifts. Despite the diversity of methods evaluated, Wild-Time reveals a significant performance gap between ID and OOD data, with none of the 13 tested methods consistently outperforming the standard ERM approach.

In our evaluation, we employ their evaluation strategy _Eval-Fix_ and also benchmark our approach against the methods they considered. An in-depth overview of these methods is given in Appendix A.6.

**Temporal DG methods.** Recent specialized temporal DG methods include: (1) **DRAIN**, which employs a Bayesian framework alongside a recurrent neural network for predicting the dynamics of the model parameters across temporal domains. (2) **GI**, which explicitly incorporates the temporal domain as a feature, using a specialized Gradient Interpolation loss function, a time-sensitive activation, and enhanced domain reasoning via Time2Vec preprocessing . However, both DRAIN and GI are limited in their ability to extrapolate beyond the near future, leading to their exclusion from our main evaluation. Notably, on the Rotated Two Moons Dataset - where DRAIN and GI have demonstrated their capabilities - our approach outperforms both. See Appendix A.4.3.4 for details.

Recent TabPFN Studies.To overcome the limitations of TabPFN in handling large samples and feature sets, typically found in DG benchmarks, several improvements have been proposed. Ma et al.  introduced a data distillation approach, where a distilled dataset serves as the model's context, optimized to maximize the training-data likelihood. Similarly, TuneTables , uses prompt-tuning to compress large datasets into a smaller context. Either of these methods could potentially be combined with Drift-Resilient TabPFN, as our modifications focus primarily on the pre-training phase, which remains unchanged in their approaches.

Another TabPFN variation, ForecastPFN , also introduces time dependence, but it does not consider any features. It only models simple time series data. Unlike our approach, which builds on TabPFN's SCM architecture for pre-training to handle a large set of features, ForecastPFN models synthetic time series using a single handcrafted function with sampled hyperparameters, simplifying the architecture while trading off diversity of the synthetic datasets.

## 3 Methodology

Our approach is built on PFNs, which use ICL to learn the learning algorithm itself. This approach also has a theoretical foundation as described by Muller et al. : It can be viewed as approximating Bayesian prediction for a prior defined by the synthetic datasets. The trained PFN will approximate the posterior predictive distribution (PPD) and thus return a Bayesian prediction for the specified distribution over artificial datasets used during PFN training.

### Structural Causal Model Prior

Hollmann et al.  introduce a prior based on Structural Causal Models (SCMs; 20; 21) to model complex feature dependencies and potential causal mechanisms underlying tabular data. To sample one dataset, this prior samples an SCM which is then used to sample the examples in the dataset. In this approach, each causal representation of a sampled SCM is converted into a functional representation to enable forward computation and dataset sampling.

Consider a sampled SCM graph \(=(Z,R)\), where \(Z=\{z_{1},,z_{n}\}\) are the nodes (mechanisms) and \(R=\{r_{1},,r_{m}\}\) the edges (relationships). Each node is set using an individual assignment function \(f_{i}\), \(z_{i}=f_{i}(\{z_{j} j_{i}\},_{i})\), with PA\({}_{i}\) being the parent nodes of \(z_{i}\) and \(_{i}\) random noise.

In the following, we will detail the underlying functional graph representation \(}\) of an SCM \(\) sampled from the prior. We will later on apply distribution shifts on it. This representation is one level below the SCM and all nodes represent a single scalar value and all edges correspond to linear connections. The values of each node \(v\) in \(}\) are set just like neurons in a neural network as a simple weighted sum and an activation function \(v:=h_{j}(_{v^{}_{j}}w_{i,j}v,_{j})\), where the scalar weights \(w_{i,j}\) and the activation function \(h_{j}\) are randomly chosen at graph creation and the noise term \(_{j}\) is sampled in each forward propagation to create a variety of samples within a dataset.

The functional graph representation \(}\) of an SCM \(\) is defined as follows:

1. **Node Expansion.** Each node \(z_{i}\) is expanded into a set of subnodes \(Z_{i}:=\{_{i}^{1},,_{i}^{k}\}\), each representing a scalar value.
2. **Graph Expansion.** The following steps are performed for each node \(z_{j}\) with a non-empty parent set PA\({}_{j}\): 1. A new set of subnodes \(F_{j}:=\{_{j}^{1},,_{j}^{l}\}\) is added. 2. We add the edges \(E_{j}:=\{Z_{i} i PA_{j}\} F_{j} F_{j} Z_{j}\).

Finally, the functional graph representation is defined as \(}:=(_{i}Z_{i}_{j }F_{j},_{i}E_{i})\).

The features of our dataset are defined by a random subset \(X_{i}Z_{i}\) and the target is defined by a randomly chosen \(y_{i}Z_{i} X\). Samples in a dataset are now generated by (i) sampling all noise terms \(_{j}\) in \(}\), (ii) propagating the values of all nodes through the graph, and finally (iii) retrieving values at the feature and target nodes. The resulting datasets, thus abide the computational flow of both \(\) and \(}\). For a more intuitive understanding of this construction, refer to the simplified example provided in Figure 2.

### Inducing Temporal Robustness in SCMs

We extend TabPFN's prior to model distribution shifts, allowing the model to expand its posterior predictive distribution (PPD) calculations to incorporate temporal domain information. We propose modeling the dynamic of edge shifts using a _\(2^{}\)-order SCM_. This \(2^{}\)-order SCM is itself an SCM with feature nodes specifying the magnitude of edge shifts in the base SCM's functional graph.

Specifically, to extend the scope of the functional representation of the SCM \(}\), we introduce a version \(}_{c_{k}}\), depending on the temporal domain \(c_{k}\). Our objective is to construct a time-dependent dataset \(\) which is an aggregation of individual datasets \(_{c_{1}},_{c_{2}},,_{c_{n}}\), where each dataset \(_{c_{k}}:=\{(_{i},y_{i},c_{k})\}_{i=1}^{n_{c_{k}}}\) contains \(n_{c_{k}}\) samples. To do this, we sample the temporal domains \(=\{c_{1},c_{2},,c_{n}\}\) and for each domain \(c_{k}\), we sample the number of samples \(n_{c_{k}}\) it contains. An illustration of exemplary domain distributions across datasets is shown in Figure 14.

We then select a sparse subset of relationships in the causal representation of the SCM \(\) to undergo temporal shifts based on the evidence that sparse shifts allow for causal reasoning . For these selected edges, the \(2^{}\)-order SCM \(\) is used to sample shift parameters that govern the corresponding edges in the functional representation \(}_{c_{k}}\). See Algorithm 1 for a high-level overview.

Employing a \(2^{}\)-order SCM for Edge Shifting.The \(2^{}\)-order SCM \(\) takes temporal domains \(\) as input and, through a single forward pass on the corresponding functional representation \(}\), produces dynamic edge shifts for each edge weight \(w_{i,j}\) in \(}\) that corresponds to an edge in \(\) that should be shifted. Note that although we perform a forward pass, there is no backward pass associated with it. Each \(2^{}\)-order SCM is randomly generated and used solely for sampling the weight shifts. This design allows for Bayesian reasoning over the edge shifts and enables \(}\) to generate complex, often correlated shifts over time. While we emphasize this construction does not reflect the true underlying dynamics of edge shifts in many real-world datasets, we use this heuristic prior construction for the following reasons: (a) Features in the SCM are correlated to each other in varying degrees, mimicking real-world processes of correlated changes in the underlying generating mechanisms of real-world data (b) An SCM with NN based causal mechanisms and nonlinear activation functions can extrapolate values outside of the data distribution, generalizing the underlying function to future domains (c) As demonstrated by TabPFN, a causal model prior is a sufficiently general approximation to many real-world datasets. We have visualized this approach in Figure 3 and a selection of the functions generated by a \(2^{}\)-order SCM in Figure 15.

Shifting SCM edges can model various Types of Distribution Shifts.A distribution shift is characterized by changing distributions between contexts (\((X,Y C=_{i})(X,Y C=_{k})\)). This definition can be further broken down into the following scenarios:

_Covariate Shift_: Changes in the feature distribution (\((X)\)) while the conditional label distribution (\((Y|X)\)) remains constant.

_Prior Probability Shift_: Changes in the label distribution (\((Y)\)) while the feature distribution given the labels (\((X|Y)\)) remains constant.

_Concept Shift_: Changes in the relationship between features and labels (\((Y|X)\) or \((X|Y)\)) while the marginal distributions of features or labels remain constant.

Figure 2: Illustrative transformation of an SCM to one exemplary functional representation. Shaded nodes indicate that their activations cannot be sampled. Feature nodes are blue, the target node is green, input/noise nodes are purple, and all others are gray. The figure also shows the mapping of shifted edges between a causal relationship and its functional form in red, ensuring that shifts specifically target the intended causal relationships without affecting others.

These shifts can also be viewed as Bayesian networks shown in Figure 4 - all arising in our prior by sampling features and targets at varying SCM positions. For further visualizations, refer to Figure 13.

Encoding the Temporal Domain.When encoded as inputs to our model, the temporally-dependent datasets \(=_{c_{k}}_{c_{k}}\) are partitioned at a particular instance into \(^{}\) and \(^{}\).

Temporal domains and features are normalized using only the training data, and so, to effectively encode temporal information and provide normalized inputs when projecting into the future, we use Time2Vec . It converts each temporal domain index into an \(m\)-dimensional vector, using linear and sinusoidal functions characterized by learned parameters \(_{i}\) and \(_{i}\). Specifically, the Time2Vec transformation for a temporal index \(c_{k}\) is formulated as:

\[(c_{k})[i]=_{i}c_{k}+_{i},&\; \;i=0.\\ (_{i}c_{k}+_{i}),&\;\;1 i<m.\] (1)

## 4 Experiments

Evaluation Strategy.We evaluate analogous to the Eval-Fix setting outlined in Wild-Time , measuring both, ID and OOD performance. Here, each dataset \(\) is split into three subsets: \(^{}\), \(^{}\), and \(^{}\). Splits are based on a randomly sampled temporal domain \(c_{k}\) that serves as the boundary between the train and test (OOD) portion. We only use such splits, where \(^{}\) comprises between 30% and 80% of the total domains and samples. To assess ID performance, we subsample 10% of the instances in each domain of \(^{}\) as the ID test set and the remainder as the train set. An illustration of the Eval-Fix strategy is provided in Figure 16.

Each class in the training set is required to be represented in both \(^{}\) and \(^{}\), and vice versa. For all datasets, we generate three random splits and average metrics across these splits. We had to limit the number of splits to three due to the constrained number of available domains and the requirement for classes to be present in both the train and test splits. Each method is trained three times, and we report the average and 95%-confidence intervals calculated across model initializations.

Figure 4: Types of distribution shifts based on the definitions by Moreno-Torres et al.  represented as Bayesian networks as defined by Kull and Flach . Here \(X\), \(Y\), and \(C\) denote the random variables of the features, label, and context, respectively. Note that all these types of shifts naturally arise in our prior, since we sample feature and target positions, as well as the locations of shifted edges, randomly at various positions in the synthetic datasets.

Figure 3: Diagram illustrating the integration of a \(2^{}\)-order SCM for adaptive edge shifting across evolving temporal domains. On the right, the primary network \(}\) generates data samples over multiple time domains, with red arrows indicating shifted edges. On the left, the \(2^{}\)-order SCM - an auxiliary network \(}\) - takes an input domain \(c_{k}\) and outputs parameters to adaptively shift each edge weight \(w_{i}\) in the base network.

Metrics.We evaluate Accuracy, F1-Score (Harmonic mean of precision and recall, useful in imbalanced datasets), ROC AUC (Area under the receiver operating characteristic curve), and ECE (Expected Calibration Error; reflects the reliability of the model's probability outputs).

Datasets.Our benchmark comprises 18 test datasets, 8 synthetic and 10 real-world. In addition, 12 validation datasets, 4 synthetic and the remaining 8 real-world, were used to optimize the hyperparameters of our approach via random search. While some of these datasets have been analyzed in previous work, there has been no comprehensive benchmark focusing on small tabular datasets undergoing distribution shifts. To address this gap, we carefully selected and generated a diverse range of datasets that exhibit temporal distribution shifts. The selected datasets originate from open dataset platforms or previous work in DG. Ground truth domain indices \(c_{k}\) are known for synthetic datasets. For real-world datasets, we approximated domain indices \(}\) based on features that encode temporal information, which we transformed into discrete intervals. Also, some real-world datasets required subsampling due to their large size, which was beyond the current architecture of TabPFN. We provide full details, including descriptions for each dataset and pre-processing steps in Section A.7 of the Appendix.

Baseline Setup.Our baselines include state-of-the-art methods for tabular prediction. These include advanced GBDTs like CatBoost , XGBoost , and LightGBM , which have demonstrated superior performance to standard neural network approaches in handling tabular data . We also include TabPFN in its unmodified form (TabPFN\({}_{}\); 19). Methods from the Wild-Time benchmark are examined separately and detailed in Section A.4.4.3 of the Appendix. However, we have added the two best-performing Wild-Time methods, ERM and Stochastic Weight Averaging (SWA), to the main results table. All baseline methods besides TabPFN are subject to a time budget of 1,200 seconds on 8 CPUs and 1 GPU. For each method except TabPFN, which does not require tuning, a random hyperparameter search with 3-fold time series cross-validation was used. We chose the best-performing hyperparameters based on OOD ROC AUC within the allocated time.

Among our baselines, we considered three strategies:

1. Providing the full dataset \(^{}\) along with the corresponding domain indices \(^{}\) as a feature, aiming to allow for better reasoning of the shifts in the dataset (all dom. w. ind.).
2. Using the dataset without domain indices \(^{}=\{(_{i}^{},y_{i}^{ {train}})\}_{i=1}^{n}\) (all dom. w. ind.).
3. Limiting the training set to samples from the last training domain \(c_{t}\). In this setting, we also omit the corresponding domain indices, resulting in the set \(^{}_{c_{t}}=\{(_{i}^{},y_{i}^ {})\}_{i=1}^{n_{c_{t}}}\) (last dom. w. ind.). The rationale behind the last scenario is to provide only training data closest to the subsequent test distribution. This strategy is not used for distribution shift baselines.

TabPFN Setup.For the TabPFN variants, both the original and our modified method (TabPFN\({}_{}\)) were pre-trained for 30 epochs across 8 GPUs. This results in a total of 30,720,000 synthetically generated datasets processed during pre-training. While this pre-training step is moderately expensive, it is done offline, in advance, and only once as part of our algorithm development. Furthermore, the preprocessing parameters of both methods were optimized once on the validation datasets by random search over 300 configurations. We chose the configurations that yielded the best OOD ROC AUC performance. The resulting model and hyperparameters are used for all datasets, resulting in, on average, 110 times faster training and prediction time on our benchmark.

Quantitative Evaluation.Our method demonstrates superior predictive performance across all metrics for OOD data in 18 test datasets, for synthetic datasets and real-world datasets, as detailed in Table 1. Compared to the strongest baseline, our method improves accuracy from 0.665 to 0.754 on synthetic datasets and from 0.712 to 0.736 on real-world datasets. It also enhances the F1 score from 0.588 to 0.697 on synthetic datasets and from 0.668 to 0.682 on real-world datasets. Additionally, it increases the ROC AUC from 0.749 to 0.844 on synthetic datasets and from 0.82 to 0.822 on real-world datasets. Furthermore, our method shows much stronger calibration on OOD samples, reducing ECE from 0.164 to 0.126 on synthetic datasets and from 0.083 to 0.062 on real-world datasets. While baselines are often overconfident on OOD data, our method is able to predict uncertainty accurately. Compared to TabPFN and GBDTs, the NN-based methods in the Wild-Time Benchmark (Section A.4.4.3) show a substantial drop in performance, likely due to the limited training data. Since our method focuses on enhancing OOD robustness rather than optimizing ID tasks, we find lower predictive performance on ID tasks. While performance gains are observed on both, real-world and synthetic data, we observe stronger improvements on synthetic datasets. This can be partly attributedto the, on average, stronger distribution shifts between ID and OOD data in our synthetic benchmark. Furthermore, real-world datasets often show multifaceted and complex shifts that are much more difficult to extrapolate into the future. Combined results across all datasets are provided in Table 5.

Qualitative Analysis.Next, we take an in-depth look at the predictions made by our method. Figure 5 illustrates the decision boundaries of our method and TabPFN\({}_{}\) on the synthetic Intersecting Blobs dataset. In this evaluation, we restrict the training domains to \(^{}=\{0,1,2,3\}\) and aim to predict samples in test domains \(^{}=\{4,5,6\}\) without adding additional data to the training set. This setup requires the model to extrapolate the temporal shifts into the future based solely on existing training data. In this setting, our model accurately extrapolates decision boundaries to future domains, while TabPFN\({}_{}\) tends to retain its initial boundary. Our analysis reveals two key attributes of our model: (i) The model decreases prediction certainty over time, improving calibration. (ii) Our model adjusts the decision boundary dynamically, boosting accuracy. Further visualizations, including decision boundaries for the Rotated Two Moons dataset, are available in Figure 7. Likewise, plots illustrating the overall shifts in these datasets are provided in Figure 6.

    &  &  &  &  &  \\  & & & OOD & ID & OOD & ID & OOD & ID & OOD & ID & OOD & ID \\ 
**TabPFN\({}_{}\)** & all dom. w. ind. & \(0.754_{~{}032}\) & \(0.959_{~{}011}\) & \(0.697_{~{}048}\) & \(0.935_{~{}033}\) & \(0.844_{~{}03}\) & \(0.987_{~{}002}\) & \(0.126_{~{}018}\) & \(0.038_{~{}003}\) \\  _{}\)**} & all dom. w. ind. & \(0.658_{~{}018}\) & \(0.963_{~{}006}\) & \(0.567_{~{}015}\) & \(0.935_{~{}02}\) & \(0.749_{~{}017}\) & \(0.986_{~{}007}\) & \(0.164_{~{}014}\) & \(_{~{}003}\) \\  & all dom. w. ind. & \(0.571_{~{}014}\) & \(0.901_{~{}006}\) & \(0.467_{~{}016}\) & \(0.848_{~{}006}\) & \(0.631_{~{}01}\) & \(0.955_{~{}03}\) & \(0.322_{~{}016}\) & \(0.053_{~{}005}\) \\  & last dom. w. ind. & \(0.651_{~{}002}\) & \(0.939_{~{}015}\) & \(0.574_{~{}002}\) & \(0.918_{~{}031}\) & \(0.727_{~{}006}\) & \(0.975_{~{}001}\) & \(0.271_{~{}011}\) & \(0.066_{~{}001}\) \\   & all dom. w. ind. & \(0.665_{~{}008}\) & \(0.985_{~{}003}\) & \(0.058_{~{}008}\) & \(0.089_{~{}009}\) & \(0.731_{~{}008}\) & \(0.109_{~{}002}\) & \(0.297_{~{}006}\) & \(0.037_{~{}006}\) \\  & all dom. wo. ind. & \(0.575_{~{}006}\) & \(0.885_{~{}003}\) & \(0.476_{~{}008}\) & \(0.831_{~{}002}\) & \(0.613_{~{}013}\) & \(0.942_{~{}007}\) & \(0.325_{~{}006}\) & \(0.063_{~{}014}\) \\  & last dom. wo. ind. & \(0.639_{~{}004}\) & \(0.932_{~{}017}\) & \(0.564_{~{}005}\) & \(0.916_{~{}021}\) & \(0.684_{~{}005}\) & \(0.962_{~{}012}\) & \(0.301_{~{}019}\) & \(0.065_{~{}006}\) \\   & all dom. w. ind. & \(0.645_{~{}018}\) & \(0.936_{~{}012}\) & \(0.57\) & \(0.109_{~{}019}\) & \(0.005_{~{}005}\) & \(0.705_{~{}011}\) & \(0.968_{~{}02}\) & \(0.253_{~{}032}\) & \(0.075_{~{}013}\) \\  & all dom. wo. ind. & \(0.582_{~{}07}\) & \(0.872_{~{}031}\) & \(0.487_{~{}047}\) & \(0.818_{~{}039}\) & \(0.621_{~{}006}\) & \(0.926_{~{}035}\) & \(0.245_{~{}008}\) & \(0.097_{~{}034}\) \\  & last dom. wo. ind. & \(0.645_{~{}008}\) & \(0.916_{~{}005}\) & \(0.565_{~{}009}\)Impact of Time2Vec Preprocessing on Model Performance.To examine Time2Vec's contributions to improved OOD performance, we conduct an ablation study in Appendix A.4.1. The ablation reveals that while Time2Vec provides at most slight improvements, the substantial performance gains are to be attributed to the prior construction used during the model's pre-training phase.

## 5 Conclusions & Limitations

In this work, we presented a Bayesian approach to address the issue of temporal domain generalization in tabular data. Specifically, we focused on enhancing TabPFN to improve its robustness to temporal distribution shifts. Within this framework, we introduced a novel approach that changes the causal relationships in the SCM prior over time, thereby enabling TabPFN to inherently adapt to these shifts. Our method outperforms all baselines on the evaluated datasets and demonstrates notable improvements both qualitatively and quantitatively, particularly on synthetic OOD datasets. Furthermore, it requires no hyperparameter tuning, is not limited to particular types of distribution shifts and takes only 10.9s for training and prediction combined.

Despite these advancements, our methodology inherits certain limitations from the underlying TabPFN model. (1) Due to the quadratic scaling of the attention mechanism with respect to the number of samples, our method does not scale to large datasets. Here, our research will benefit from the continued improvements of TabPFN, ICL, and sequence-based models in general. (2) The TabPFN, like many transformer-based models, acts as a "black box", making it challenging to interpret the model's predictions and understand the recognized distribution shifts. (3) The underlying prior for structural causal models with sparse mechanism shifts may not accurately describe all real-world datasets. While we find it to be empirically and intuitively useful, real-world shifts might have underlying complexities that our prior currently does not capture.

For future work, next to addressing the existing limitations, we have identified in initial experiments promise in extending our model to (1) transductive and (2) online continual learning settings. Also, our prior could be adapted to support (3) spatial or spatio-temporal distribution shifts. Employing a prior that (4) models shifts in the underlying causal model could improve the robustness of the baseline TabPFN in standard classification tasks where temporal shifts are often implicit.

We provide code, pre-trained models, and a Colab notebook at https://github.com/automl/Drift-Resilient_TabPFN. We further describe reproducibility, the release of code and models, the broader impact of our models, and the computational resources used for method development in Appendix A.1, A.2 and A.3. We add a discussion of baselines in A.6, an in-detail discussion of the evaluated datasets in A.7, and additional quantitative and qualitative evaluations in A.4.

Figure 5: This figure displays the predictive behavior of TabPFN\({}_{}\) in the top row and TabPFN\({}_{}\) in the bottom row on the Intersecting Blobs dataset. It illustrates how each model adapts to unseen test domains when trained on domains \(^{}=\{0,1,2,3\}\). The baseline is given the domain indices as a feature in train and test. The coloring indicates the probability of the most likely class at each point. Incorrectly classified samples are highlighted in red.

Acknowledgments

Frank Hutter acknowledges the financial support of the Hector Foundation. This research was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under grant number 417962828.

We acknowledge funding by the European Union (via ERC Consolidator Grant DeepLearning 2.0, grant no. 101045765). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union or the European Research Council. Neither the European Union nor the granting authority can be held responsible for them.