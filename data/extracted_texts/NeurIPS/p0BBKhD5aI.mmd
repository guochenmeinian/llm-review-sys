# Infinite Limits of Multi-head Transformer Dynamics

Blake Bordelon, Hamza Chaudhry, Cengiz Pehlevan

John A. Paulson School of Engineering and Applied Sciences

Center for Brain Science

Kempner Institute for the Study of Natural and Artificial Intelligence

Harvard University

Cambridge, MA 02138

blake_bordelon@g.harvard.edu

hchaudhry@g.harvard.edu

cpehlevan@seas.harvard.edu

###### Abstract

In this work, we analyze various scaling limits of the training dynamics of transformer models in the feature learning regime. We identify the set of parameterizations that admit well-defined infinite width and depth limits, allowing the attention layers to update throughout training-a relevant notion of feature learning in these models. We then use tools from dynamical mean field theory (DMFT) to analyze various infinite limits (infinite key/query dimension, infinite heads, and infinite depth) which have different statistical descriptions depending on which infinite limit is taken and how attention layers are scaled. We provide numerical evidence of convergence to the limits and discuss how the parameterization qualitatively influences learned features.

## 1 Introduction

Increasing the scale of transformer models has continued to improve performance of deep learning systems across many settings including computer vision [1; 2; 3; 4] and language modeling [5; 6; 7; 8; 9]. However, understanding the optimization stability and limiting behavior of these models under increases in model scale remains a core challenge.

One approach to scaling up systems in a stable and predictable way is to identify parameterizations of neural networks that give approximately scale-independent feature updates during training [10; 11; 12]. The mean field parameterization, commonly referred to as \(\)P, is a well-known example that satisfies this property [13; 14; 15]. When such parameterizations are adopted, the learned internal representations in hidden layers of the network are very similar across model scales [16; 17], but performance tends to improve with model scale [10; 11; 12]. Further, theoretical results about their limits can often be obtained using Tensor Programs  or dynamical mean field theory (DMFT) techniques [15; 17].

In this work, we develop a theoretical treatment of randomly initialized transformers. We study various scaling limits of the training dynamics of these models including the infinite key/query dimension limit, the infinite head limit, and the infinite depth limit. Concretely, our contributions are the following:

1. We derive a DMFT for feature learning in randomly initialized transformers with key/query dimension \(N\), attention head count \(\) and depth \(L\). From the derived DMFT action, we identify large \(N\), large \(\) and large \(L\) limits of the training dynamics.

2. We analytically show that the large key-query \(N\) limit requires the \(\)P scaling of key/query inner product with \(1/N\), even if key/queries are reparameterized to decrease the size of their updates from gradient descent.
3. From the limiting equations, we show that this \(N\) limit causes multi-head self attention trained with stochastic gradient descent (SGD) to effectively collapse to single-head self attention since all heads follow identical dynamics.
4. To overcome this limitation, we analyze the infinite head \(\) limit while fixing \(N\). We show there is a limiting _distribution_ of attention variables across heads at each layer throughout training. Despite \(N\) being finite, the infinite-head \(\) limit leads to concentration of the network's output logits and learned residual stream feature kernels, giving deterministic training dynamics.
5. Finally, we examine large depth limits of transformers with residual branch scaling. We illustrate and discuss the tension between parameterizing a model so that it has a non-trivial kernel at initialization while maintaining feature learning within the multi-head self attention (MHSA) and multi-layer perceptron (MLP) blocks.

### Related Works

Hron et al.  studied the Neural Network Gaussian Process limit of multi-head self attention in the infinite-head \(\) limit. They showed that, at initialization, there is a limiting distribution over attention matrices and that the outputs of the multi-head attention block follow a Gaussian process, establishing a connection to kernel methods. Dinan et al.  develop a similar theory of transformers at initialization and compute the Neural Tangent Kernel associated with this architecture as the dimensions per head \(N\) using a \(}\) scaling of the key-query inner product within each attention layer. One of our key theoretical results is showing that this picture of a _distribution over learned attention heads_ persists throughout training in the feature-learning regime as \(\) (though the distribution of residual stream variables generally becomes non-Gaussian).

Several works have analyzed the signal propagation properties of transformers at initialization at large key/query dimension \(N\) and large depth \(L\)[20; 21; 22; 23] including providing modifications to the standard transformer architecture [22; 24]. In this work, we pursue large depth limits of transformers by scaling the residual branch as \(L^{-_{L}}\) with \(_{L}[,1]\), which has been shown to converge to a limit not only at initialization [25; 26; 27], but also throughout training in the feature learning regime [11; 12; 27]. However, we argue that in transformers that \(_{L}=1\) is preferable as it enables the attention layers to update non-negligibly as \(L\).

Yang et al.  introduced the \(\)P scaling for attention layers which multiplies the key/query inner product with \(\) rather than the more commonly used \(}\). They show empirically that this change improves stability of training and transfer of optimal hyperparameters across different values of \(N\). Vyas et al.  empirically found that such \(\)P transformers learn attention matrices that become approximately consistent across different heads and model sizes, suggesting that models parameterized in \(\)P learn similar representations across scales.

In addition to work on infinite width and depth limits of deep networks, there is also a non-asymptotic approach to optimizer design and scaling based on controlling the norm of weight updates . This approach coincides with \(\)P width-scaling when the spectral norm of the weights is used as the measure of distance , and can achieve hyperparameter transfer for a wide array of optimizers and initialization schemes [30? ].

## 2 Parameterizations with Feature Learning Limits

We consider a transformer architecture with \(L\) layers, \(\) heads per layer, and \(N\) dimensional keys/ queries per head. Transformers are often defined in terms of \(d_{}=d_{}=N\) which can be increased by scaling the number of heads or the dimension of each head, where \(N\) is often written \(d_{}\). Our goal is to determine the set of parameterizations that allow the attention layers to undergo non-trivial feature learning in the various \(N,,L\) limits. The analysis of these limits is performed with batch size and number of training steps \(t\) fixed while the other architectural parameters are taken to infinity.

### Model Scalings

The network's output is computed by a depth \(L\) recursion through hidden layers \([L]\) starting with the first layer \(_{}^{1}()=}^{0}_{}^{N}\) where \(_{}^{D}\) is the input at spatial/token position \(\).

Preatvations in subsequent layers \(^{}\) are determined by a forward pass through the residual stream which contains an attention layer and a MLP layer

\[_{}^{+1}=}_{}^{}+}{L^{ _{L}}}(}_{}^{})\;,\;}_{}^{}=_{}^{}+}{L^{_{L}} }(^{})_{}.\] (1)

The constants \(_{0}\) and \(_{0}\) control the rate of feature learning and the _effective depth_ respectively 1. We will consider \(_{L}[,1]\).2 The multi-head self attention layer (MHSA) with pre-layer-norm3 is

\[(^{})_{}=}}_{[]}_{Ob}^{}_{ }^{}\;,_{}^{}=_{ ^{}[]}_{^{}^{}}^{}_{^{}^{ }}^{}\]

\[_{}^{}=}}_{Vb}^{} }_{}^{}\;,}_{}^{}=( _{}^{})\] (2)

where \(_{}^{}^{}\) are the attention matrices passed through a matrix-valued nonlinearity \((}_{}^{})^{4}\).

For a sequence of length \(\), the pre-attention matrix \(}_{}^{}^{ }\) is defined as

\[_{^{}}^{}=}}} _{}^{}_{^{}}^{}\;,\; _{}^{}=-_{}} }}_{Kb}^{}}_{}^{}\;,\;_{ }^{}=-_{}}}}_{Qb}^{}}_{}^{}.\] (3)

The exponent \(_{}\) will alter the scale of the pre-attention variables \(_{}^{}\) at initialization. The input matrices have shape \(_{Vb}^{}\), \(_{Kb}^{},_{Ob}^{}^{N}\), while the output matrices have shape \(_{Ob}^{}^{N N}\). All of the weights \(_{Ob^{}}^{},_{Ob}^{},_{Kb}^{}\) are initialized with \((1)\) entries while \(_{Kb}^{},_{Ob}^{}\) have entries of size \((N^{1-_{}})\) which ensures that all key and query \(,\) vectors are \((1)\) at initialization. The pre-attention variables \(}_{}^{}^{ }\) at each head \(\) are determined by key \(_{}^{}\) and query \(_{^{}}^{}\) inner products. The MLP layer consists of two linear layers with an element-wise nonlinearity \(\) applied in between, where \(^{,2},^{,1}^{N N}\) are initialized with \((1)\) entries:

\[(}_{}^{})=}}^{ ,2}(}_{}^{,1})\;,\;}_{ }^{,1}=}}^{,1}}_{ {s}}^{}\;,\;}_{}^{}=(}_{ }^{}).\] (4)

\(\)P scaling  downscales the readout of the last layer compared to standard and NTK parameterization . Thus, we define the output of the model as

\[f=N}^{L}(}_ {}_{}^{L})\] (5)

5 where \(_{}^{L}^{N}\) are the final layer preactivations at spatial/token position \([]\). The parameter \(_{0}\) is an additional scalar that controls the rate of change of the internal features of the network relative to the network output .

### Learning Rate Scalings

In order to approximately preserve the size of internal feature updates, we must scale the learning rate \(\) appropriately with \((N,,L)\). However, this scaling depends on the optimizer. In Table 2, we provide the appropriate scaling of learning rates for SGD and Adam for any \(_{L}[,1]\) and \(_{}[,1]\). In what follows, we focus on the SGD scaling and theoretically analyze the \(N\), \(\), and \(L\) limits of the training dynamics. We also provide in Table 2 details about what prefactor the first layer should be multiplied by and the initial weights divided by to ensure convergence to the \(L\) limit. Example FLAX implementations of these parameterizations for vision and language modeling transformers are provided in Appendix B.

Our analysis assumes that at each step \(t\) of SGD or Adam a mini-batch \(_{t}\) of size \((1)\) is used to estimate the loss gradient. We assume that the minibatches are fixed. Further, the number of total training steps is assumed to not be scaled jointly with the model size. Therefore the analysis provided here can cover both online SGD for a fixed number of steps or full batch GD (repeating data) with a \((1)\) sized dataset.

## 3 Infinite Limits of Learning Dynamics

In this section, we first analyze the infinite dimension-per-head \(N\) limit of training. We find that for this limit, the \(\)P rule of \(_{}=1\) is necessary and show that all heads collapse to the same dynamics. To counteract this effect, we next analyze the infinite head \(\) limit of the training dynamics at fixed \(N,L\), where we find a limiting _distribution_ over attention heads. We will conclude by analyzing the statistical descriptions of various infinite depth \(L\) limits. 6.

### Mean Field Theory Treatment of the Learning Dynamics

To obtain the exact infinite limits of interest when scaling dimension-per-head \(N\), the number of heads \(\), or the depth \(L\) to infinty, we work with a tool from statistical physics known as dynamical mean field theory (DMFT). Classically, this method has been used to analyze high dimensional disordered systems such as spin glasses, random recurrent neural networks, or learning algorithms with high dimensional random data [34; 35; 36; 37; 38; 39]. Following [15; 11], we use this method to reason about the limiting dynamics of randomly initialized neural networks by tracking a set of deterministic correlation functions (feature and gradient kernels) as well as additional linear-response functions (see Appendix D). The core conceptual idea of this method is that in the infinite limit and throughout training, all neurons remain statistically independent and only interact through collective variables (feature kernels, neural network outputs, etc). Further the collective variables can be computed as _averages_ over distribution of neurons in each hidden layer or along the residual stream. This DMFT description can be computed using a path integral method that tracks the moment generating function of the preactivations or with a dynamical cavity method (see Appendix D).

### Scaling Dimension-Per-Head \(N\)

One way of obtaining a well-defined infinite parameter limit of transformers is to take the \(N\) limit, where \(N\) is the dimension of each head. A priori, it is unclear if there are multiple ways of scaling the key/query inner product. Concretely, it is unknown what values for the exponent \(_{}\) are admissible for the pre-attention \(=}}}\). The keys and queries are uncorrelated at initialization which motivated the original choice of \(_{}=\)[5; 18]. Yang et al.  assume the entries of the key and query vectors move by \((1)\), implying \(_{}=1\) is necessary since the

  Optimizer & Global LR & First/Last Layer Rescale Multiplier & First/Last Layer Std. Dev. \\  SGD & \(_{0}NL^{2_{L}-1}\) & \(L^{-_{L}}\) & \((1)\) \\  Adam & \(}{}}L^{-1+_{L}}\) & \(L^{1-_{L}}}\) & \(}}L^{-1+_{L}}\) \\  

Table 1: The learning rates which should be applied to obtain the correct scale of updates for SGD or Adam optimizers. In addition, the weight variance and multiplier for the first layer may need to be rescaled (relative to eq (5)) with width/depth depending on the parameterization and optimizer.

Figure 1: Schematic representations of the transformer architecture we model. (a) The forward pass through the residual stream is an alternation of MHSA and MLP blocks scaled by \(_{0}L^{-_{L}}\). (b) The MHSA block computes keys, queries, values, and attention variables to produce a concatenated output of dimension \(d_{}=N\).

update to \(\) is correlated to \(\) and vice versa. However, it is possible to obtain \((1)\) updates to the attention variable for alternative values of \(_{}\) if we choose the change to key (also query) entries after gradient descent to be \( k_{i}(N^{-1+_{}})\). We show that this scaling can approximately preserve optimal hyperparameters across \(N\) in Figure 2 (a) and give similar dynamics under SGD Appendix C. However, as we show in Appendix E.1.2, any well defined \(N\) limit of SGD requires \(_{}=1\). The reason is not that keys and queries become correlated, but rather that the scale of the backward pass must be controlled to ensure the dynamics remain stable (non-divergent) under SGD training. After performing two or more gradient descent steps, we demonstrate that the backpropagation signals will diverge as \(N\) unless initial key and query weight matrices are downscaled to have variance of order \(_{N}(1)\). In Appendix E, we provide a DMFT analysis of the \(N\) limit of the transformer training dynamics. We summarize the result of that analysis informally below.

**Result 1** (Infinite Dimension-Per-Head \(N\)): _(**Informal**) A stable feature learning \(N\) limit of transformer SGD training requires taking \(_{}=1\) (\(\)P scaling), even if key/query updates are allowed to be rescaled to account for their correlation. The limiting dynamics of training are governed by the residual stream kernel \(H^{}_{^{}}(,^{},t,t^{})= {N}^{}_{}(,t)^{}_{ ^{}}(^{},t^{})\) and a collection of inner product kernels in each head \(\) that concentrate as \(N\)_

\[V^{}_{^{}}(,^{},t,t^{ }) =^{}_{}(,t)^{ }_{^{}}(^{},t^{})\;,\;Q^{}_{ }(,^{},t,t^{})=^{}_{ }(,t)^{}_{^{}}(^{ },t^{})\] (6) \[K^{}_{^{}}(,^{},t,t^{ }) =^{}_{}(,t)^{}_{ ^{}}(^{},t^{})\;,\;^{}_{ ^{}}(,t)=^{}_{}(,t)^{}_{^{}}(,t),\] (7)

_alongside residual-stream gradient kernels and response functions in the sense of . The NN output logits \(f(,t)\) evolve deterministically according to the above kernels as well as kernels for the gradient vectors \(^{}_{0}N}\) which appear in the backward pass. These variables become identical across heads such that for any \(,^{}[]\), \(^{}_{^{}}(,t)=^{}_{ ^{}}(,t)\). All preactivations on the residual stream and key/query/value variables within a MHSA block are statistically independent across neurons and can be described by a single scalar stochastic process_

\[h^{+1}_{}(,t)=h^{}_{}(,t )+_{0}L^{-_{L}}^{}_{}(,t)+_{0}L^{ -_{L}}u^{+1}_{}(,t)\] \[+_{0}_{0}_{0}^{2}L^{-1}_{t^{}<t} _{^{}[S]} d^{}[^{}_{ ^{}}(,^{},t,t^{})^{}_ {^{}}(^{},t^{})+C^{}_{^{ }}(,^{},t,t^{})g^{}_{^{}}( ^{},t^{})]\] \[k^{}_{}(,t)=u^{}_{K}(,t )+_{t^{}^{}} d^{}C^{k^{}}_{ ^{}}(,^{},t,t^{})q^{}_{^{}}(^{},t^{})\] (8)

_where \(^{},u^{},u^{}_{K}\) are Gaussian processes with covariances \(^{,1},V^{},H^{}\) respectively. Analogous equations hold for the queries and values. In the limit, the kernels \(H^{}_{^{}}(,^{},t,t^{})=<h^{ }_{}(,t)h^{}_{^{}}(^{},t^{ })>\). \(^{}_{^{}}(,t)=<k^{}_{} (,t)q^{}_{^{}}(,t)>\), etc. are computed as averages \(\) over

Figure 2: Increasing dimension-per-head \(N\) with heads fixed for \(_{}=\{1,\}\). (a) Both \(_{}=1\) and \(_{}=\) exhibit similar hyperparameter transfer for vision transformers trained on CIFAR-5M over finite \(N\) at \(=16\). (b) The variance of attention variables across the different heads of a vision transformer after training for \(2500\) steps on CIFAR-5M. For \(_{}=1\) the variance of attention variables decays at rate \((N^{-2})\) and for \(_{}=\) the variance does not decay with \(N\).

these random variables. The deterministic kernels \(C^{},^{}\) can also be expressed in terms of single site averages of residual variables and head averages of MHSA variables. The kernels \(C^{},^{},C^{k^{}}\) depend on the precise mini-batches of data \(_{t}\) presented at each step \(t\) which we assume are known._

We derive this result using a Martin-Siggia-Rose path integral formalism  for DMFT in Appendix E. Full DMFT equations can be found in Appendix E.2. Following prior works on DMFT for infinite width feature learning, the large-\(N\) limit can be straightforwardly obtained from a saddle point of the DMFT action [15; 11; 41; 17].

Collapse of Attention HeadsAs \(N\), multi-head self-attention will effectively compute the same outputs as a single-head self-attention block. We theoretically derive this effect in Appendix E.2.1 and demonstrate it empirically in Figure 2 (b). This experiment shows that in \(_{}=1\) (\(\)P) transformers trained for \(2500\) steps on CIFAR-5M , the variance of attention matrices across heads decreases with \(N\). However, we note that if the scaling exponent is chosen instead as \(_{}=\) there is non-decreasing diversity of attention variables across heads. This result is consistent with recent empirical findings that attention variables in \(\)P transformers converge to the same limiting quantities at large \(N\) with \(\) fixed for different initializations and also across model sizes . This aspect of transformers in the large-\(N\) limit is potentially undesirable as some tasks could require learning multiple attention mechanisms. Furthermore, this suggests scaling the model in this limit could increase computational cost without improving performance. To circumvent this, we explore if there exist well defined limits at finite \(N\) with a diversity of attention variables across heads.

### Scaling Number of Heads

In this section, we take \(\) with the inner dimension \(N\) fixed. Rather than having all kernels concentrate, the kernel of each head of the MHSA block follows a statistically independent stochastic process. This picture was shown to hold at initialization by Hron et al. . Here, using a DMFT analysis, we show that it continues to hold throughout training in the feature learning regime.

**Result 2 (Infinite Head Limit)**: _(Informal) The \(\) limit of SGD training dynamics in a randomly initialized transformer at any key/query dimension \(N\), scaling exponents \(_{},_{L}\), and any depth \(L\) is governed by head-averaged kernels for pairs of input data \(,^{}\) at training times \(t,t^{}\) and spatial/token positions \(,^{}\) such as_

\[V^{,}_{^{}}(,^{},t,t^{ })=}_{=1}^{}^{ }_{}(,t)^{}_{ ^{}}(^{},t^{})\] (9)

_which converge to deterministic values as \(\). The attention variables \(\{^{}_{}(,t),^{}_{}(,t), ^{}_{}(,t),^{}_{}(,t)\}\) within each head become statistically independent across heads and decouple in their dynamics (but not across dimensions within a head). Each residual stream neuron becomes independent and obeys a single site stochastic process analogous to Result 1, but with different kernels._

We derive this and the full DMFT in Appendix E.3, showing that the joint distribution of head-averaged dynamical quantities satisfies a large deviation principle and the limit can be derived as a saddle point of a DMFT action.

To gain intuition for this result, we first examine variables \(H^{}\) and \(^{}_{}\) at initialization. In Figure 3, we plot the convergence of a \(N=4,L=8\) vision transformer's residual stream kernel \(H^{}\) to its \(\) limit at rate \((^{-1})\) in square error, consistent with perturbative analysis near the limit . Next, we plot the distribution (over heads) of \(_{}\) at a fixed pair of spatial/token positions for a fixed sample. This is a non-Gaussian random variable for finite \(N\), but as \(N\) the distribution of \(\) will approach a Gaussian with variance \((N^{1-2_{}})\).

We then investigate training dynamics as we approach the \(\) limit. In Figure 4 (a) we show the test loss on CIFAR-5M as a function of the number of training iterations. The performance tends improve as \(\) increases and the model approaches its limit. In Figure 4 (b) we show that all of the models are converging in function space by measuring the squared error between finite \(\) head models and a proxy for the infinite \(\) model. Since the \(\) limit is essentially uncomputable, we approximate it as the ensemble averaged predictor of the widest possible models, a technique used in prior works [16; 11]. We again see that at early time, the logits of \(\) head models converge to the limit proxy at a rate \((^{-1})\), but after continued training the convergence rate weakens. This effect has been observed in \(\)P networks in many settings  and a theoretical model of this was provided in recent work which argues it arises from low-rank effects in the finite \(\) kernels .

### Infinite Depth Limits

We next describe the infinite depth limits which depend on the choice of \(_{L}\). Below we informally describe the main finding which again uses a DMFT formalism and is based on analyses in recent works on infinite depth networks from Bordelon et al.  and Yang et al. .

**Result 3** (Infinite Depth Limit): _(**Informal**) The training dynamics for \(,L\) with \(L^{-_{L}}\) branch scaling with \(_{L}[,1]\) is described by a differential equation for residual variables \(h_{}(,t)\) in layer time \(=_{L}\) for the residual stream_

\[h_{}(,,t) =_{0}\;_{_{L},}_{0}^{}du_{ }(^{},,t)\] \[+_{0}_{0}_{0}^{2}_{^{}<t } d^{}_{0}^{}d^{}C_{^{}}( ^{},,^{},t,t^{})g_{^{}}( ^{},^{},t^{})\] (10)

_where the Brownian motion term \(du_{}(,,t)\) survives in the limit only if \(_{L}=\) and has covariance_

\[ du_{}(,,t)du_{^{}}(^{}, ^{},t^{})=(-^{})d d^{ }[_{^{}}(,,^{},t,t^{ })+V^{}_{^{}}(,,^{},t,t^{ })]\] (11)

Figure 4: Approaching the large head limit \(\) in early portion of SGD dynamics for a vision transformer trained on CIFAR-5M with \((L,N)=(2,4)\) and \((_{0},,_{})=(0.05,4,)\) and losses averaged over \(10\) random inits (colored error bars are standard deviations). (a) As \(\) increases the loss and the variability over random initial seed decreases. (b) The mean square difference between output logits for \(\) head models and a proxy for the infinite head model on a held out batch of test examples. Following prior works, our proxy for the limit is the ensemble averaged outputs of the widest models [16; 11].

_and the deterministic kernel \(C_{^{}}(,,^{},t,t^{})\) can be expressed in terms of head-averaged kernels and response functions. The weights inside each hidden MHSA layer or each MLP layer are frozen in the \(L\) limit unless \(_{L}=1\). All response functions are suppressed at \(L\) unless \(_{L}=\)._

Below we provide a couple of short comments about this result. The proof and full DMFT is provided in Appendix E.4.

1. At initialization \(t=0\), the only term which contributes to the residual stream layer dynamics is the integrated Brownian motion \(_{0}^{}du(^{})\) which survives at infinite depth for \(_{L}=\). For \(_{L}=1\) this term disappears in the limit. The structure of \(C()\) is also modified by additional response functions at \(_{L}=\) which we show disappear for \(_{L}=1\).
2. The weights within residual blocks (including the MHSA block) can be treated as completely frozen for \(_{L}<1\) in the \(L\) limit, which leads to the simplified statistical description of the preactivations in those layers. However, the residual stream variables \(h()\) do still obtain \(_{L}(1)\) updates. At \(_{L}=1\) the weights in the MHSA blocks evolve by \(_{L}(1)\), causing additional feature evolution in the model.
3. A consequence of our large \(N\) and large \(L\) result is that the \(N,L\) limit with \(_{L}=\) (the parameterization studied by [11; 12]) would lead to \(^{}_{^{}}(,t)=0\) for all time \(t\). Thus the MHSA blocks would only involve average pooling operations over the spatial indices, despite the residual stream kernels \(H^{}\) updating from feature learning.

First, we note in Figure 5 that the weights within each attention block freeze as \(L\) with \(_{L}=\) case but move at a constant scale for \(_{L}=1\). As a consequence, the loss at large \(L\) can be lower in the \(=1\) parameterization.

We can see some numerical evidence for the first of these effects in Figure 6 (a)-(b) where initially training at large \(L\) is slower than the base model and the initial kernel appears quite different for \(L=4\) and \(L=64\). The initial kernel will decrease in scale as \(L\) for \(_{L}=1\) since the preactivation vectors lose variance as we discuss in Appendix E.4, resulting in slower initial training. However, we note that the final learned feature kernels are quite similar after enough training.

In summary, our results indicate that the \(_{L}=1\) parameterization is the one that allows attention layers to actually be learned in the limit \(L\), but that this parameterization leads to a less structured kernel at initialization.

## 4 Experiments in Realistic Settings

In practice, large scale neural networks do not generally operate close to their limit. Given the costs of training large networks, one would ideally operate in a regime where there is a guarantee of consistent improvements with respect to model scale. In pursuit of this goal, we apply our theoretical findings of this paper to training language models on a larger natural language dataset, a Transformer with causal attention blocks trained on the C4 dataset  with Adam optimizer. As mentioned in 2.2, while our exact theoretical description of these infinite limits focus on SGD, we can implement an appropriate scaling for Adam which preserves the scale of internal feature updates. This allows us

Figure 5: Depth scaling in a vision transformer on CIFAR-5M with \(_{L}\{,1\}\). (a) The key and query weights move by \(1/\). (b) The compute scaling laws with models at fixed width \(N,\) and varying depth \(L\). At large \(L\), the \(_{L}=1\) (dashed) models perform better at fixed compute.

to investigate realistic training dynamics of our LLM as we take the \(N,L,\) limits. Training details are provided in Appendix F

In Figure 7 (a), we sweep over each of the model dimensions independently for each parameterization of \(_{}\{1,\}\) on the left and right respectively. For fixed \(N\) and \(L\), scaling \(\) provides a similar increase in performance in both parameterization and appear to start converging to a final loss around \(5\), with slight benefit to \(_{}=\). For fixed \(\) and \(L\), scaling \(N\) provides a similar increase in performance to scaling heads in when \(_{}=1\), but a substantial increase when \(_{}=\). This is in line with our predictions in Section 3.2 about the benefits of diversity across attention heads. Next, for fixed \(N\) and \(\), scaling \(L\) provides little to no benefit in either parameterization as predicted in Section 3.4. Finally, we inspect the sample and spatial residual stream kernels of these models before and after training and find that the kernels are identical for both \(_{}\), except for a slight difference for large \(N\). Furthermore, they are extremely similar for large \(N\) and large \(\).

Taken together, these results suggest that scaling different model dimensions do indeed have different effects on training dynamics and final performance. This provides groundwork for future large-scale experiments systematically investigating their trade-offs, thereby identifying compute-optimal scaling of realistic architectures in parameterizations with well-defined limits.

## 5 Discussion

This paper provided analysis of the infinite head, depth and key/query dimension limits of transformer training in the feature learning regime. We showed that feature learning in \(\)P multi-head transformers in the limit of \(N\) collapses to single-head self-attention. At finite \(N\) and infinite heads \(\)

Figure 6: Initial and final representations are converging as model scale increases after one pass of training on the full CIFAR-5M with SGD+momentum. The base model is a \((N,,L)=(16,16,4)\) and \((_{},_{L},_{0},_{0})=(1,1,4,0.1)\). (a) The test loss dynamics for one pass through CIFAR-5M. The dynamics are very similar across different head-counts \(\) but the early dynamics are changed for large depth \(L\), consistent with our theory. (b) The initial and final feature kernels after spatial pooling at the last layer of the residual stream. The initial kernel at large \(L\) is quite different for \(_{}=1\) due to suppression of Brownian motion on the forward pass, which we explain in Section 3.4. (c) The residual stream kernel across pairs of spatial positions for a single randomly chosen input sample. (d) The distribution of attention entries across heads at a fixed pair of spatial locations and data point. The initial variance of \(\) decreases for \(_{}=1\) but the update is roughly consistent across \(N\). For \(_{}=\) both initial and final distributions for \(_{}\) are consistent across \(N\).

we showed that there is an alternative limit which maintains a _distribution over attention heads_. We discussed two different large depth limits of transformer training that reduce to differential equations in the residual layer time \(\). The depth scaling that maintains feature learning within all MHSA blocks (\(_{L}=1\)) causes the initial kernel to lose structure from the initialization as \(L\), but allows learning of the self-attention variables, whereas the depth scaling that preserves structure from initialization (\(_{L}=\)) leads to static layers.

Limitations and Future DirectionsCurrently exact theoretical analysis of the limit is focused on SGD (and can be easily extended to SGD+momentum ) while Adam is currently only reasoned with rough scaling arguments rather than an exact theoretical description of the limit. Since Adam is most commonly used to train transformers, a theory of the limiting dynamics of Adam in Transformers would be an important future extension. In addition, while we provide an exact asymptotic description of network training, the limiting equations are compute intensive for realistic settings which is why we focus our empirical investigations on training large width networks in the appropriate parameterizations. Lastly our techniques assume that the number of training steps is fixed as the scaling parameters of interest \((N,,L)\) are taken to infinity. However, it would be important to understand learning dynamics in the regime where model size and training times are chosen to balance a compute optimal tradeoff (or perhaps even training longer than compute optimal) . In this regime, harmful finite model-size effects become significant and comparable to the finite training horizon . Thus stress testing the ideas in this work at larger scales and longer training runs would be an important future direction of research into scaling transformer models.

Figure 7: Training dynamics and initial/final representations of decoder only language models trained on C4 converge with increasing model scale. The base model has \((N,,L)=(8,8,4)\) and \((_{L},_{0},_{0})=(1,4,0.25)\) and \(_{}\{1,\}\). (a) Train loss dynamics after \(10000\) steps on C4 using Adam optimizer. The dynamics improve consistently when scaling \(\) for both values of \(_{}\), with slight benefit to \(_{}=\). Scaling \(N\) reveals a significant advantage to setting \(_{}=\). Scaling \(L\) provides little improvement for either parameterization of \(_{}\). (b) Initial and final residual stream kernels for the final token across samples for Base, \(=128\), \(N=128\), and \(L=64\) models. The first row is at initialization. The second and third rows are after training with \(_{}\{1,\}\) respectively. (c) Initial and final feature kernels across pairs of tokens for a single randomly chosen input sample. Note both types of kernels are identical across \(_{}\) except for a slight difference at large \(N\).