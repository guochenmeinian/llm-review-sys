# Local Linearity: the Key for No-regret Reinforcement Learning in Continuous MDPs

Davide Maran

Politecnico di Milano, Milan, Italy

davide.maran@polimi.it &Alberto Maria Metelli

Politecnico di Milano, Milan, Italy

albertomaria.metelli@polimi.it &Matteo Papini

Politecnico di Milano, Milan, Italy

matteo.papini@polimi.it &Marcello Restelli

Politecnico di Milano, Milan, Italy

marcello.restelli@polimi.it

###### Abstract

Achieving the no-regret property for Reinforcement Learning (RL) problems in continuous state and action-space environments is one of the major open problems in the field. Existing solutions either work under very specific assumptions or achieve bounds that are vacuous in some regimes. Furthermore, many structural assumptions are known to suffer from a provably unavoidable exponential dependence on the time horizon \(H\) in the regret, which makes any possible solution unfeasible in practice. In this paper, we identify _local linearity_ as the feature that makes Markov Decision Processes (MDPs) both _learnable_ (sublinear regret) and _feasible_ (regret that is polynomial in \(H\)). We define a novel MDP representation class, namely _Locally Linearizable MDPs_, generalizing other representation classes like Linear MDPs and MDPS with low inherent Belmman error. Then, i) we introduce Cinderella, a no-regret algorithm for this general representation class, and ii) we show that all known learnable and feasible MDP families are representable in this class. We first show that all known feasible MDPs belong to a family that we call _Mildly Smooth MDPs_. Then, we show how any mildly smooth MDP can be represented as a Locally Linearizable MDP by an appropriate choice of representation. This way, Cinderella is shown to achieve state-of-the-art regret bounds for all previously known (and some new) continuous MDPs for which RL is learnable and feasible.

## 1 Introduction

_Reinforcement learning_ (RL)  is a paradigm of artificial intelligence in which an agent interacts with an environment, which is typically assumed to be a Markov Decision Process (MDP) , to maximize a reward signal in the long term. By interacting with the environment, an RL algorithm tries to make the agent play actions leading to the highest possible expected reward; RL theory is the field that designs algorithms to be provably efficient, i.e., to work with probability close to one. This idea is formalized in a performance metric called the _(cumulative) regret_, which measures the cumulative difference between actions played by the algorithm and the optimal ones in terms of expected reward.

For the case of episodic _tabular_ MDPs, when both the state and the action space are finite, an optimal result was first proved by , who showed a bound on the regret of order \((||||K})\), where \(\) is the state space, \(\) is the action space, \(K\) is the number of episodes of interaction, and \(H\) the time horizon of every episode. This regret is minimax-optimal in the sense that no algorithm can achieve smaller regret for every arbitrary tabular MDP. This result is not useful in many real-world scenarios, where \(\) and \(\) are huge, or even continuous . In fact, all applications of RLto the physical world, like robotics  and autonomous driving , have to deal with continuous state spaces. Furthermore, the most common benchmarks used to evaluate practical RL algorithms  have continuous state spaces.

One of the first studied families of MDPs with continuous spaces is the Linear Quadratic Regulator (LQR) , which goes back to control theory. LQR is a model where the state of the system evolves according to linear dynamics, and the reward is quadratic. Regret guarantees of order \(}()\)for this problem were obtained by  (with a computationally inefficient algorithm) and, then, by . Still, this parametric model of the environment is very restrictive and does not capture the vast majority of continuous MDPs. A much wider and non-parametric family is given by _Lipschitz MDPs_, which assume that bounded differences in the state-action pair \((s,a)\) correspond to bounded differences in the reward \(r(s,a)\) and in the transition function \(p(|s,a)\) (e.g., in Wasserstein metric). Lipschitz MDPs have been applied to several scenarios, like policy gradient methods , RL with delayed feedback , configurable RL , and auxiliary tasks for imitation learning . While this model is very general, its regret guarantees are weak, both in terms of dependence on \(K\) and on \(H\). In fact, very recently,  showed a regret lower bound of order \((2^{H}K^{})\), where \(d\) is the dimension of the state-action space, which makes this family of problems _statistically unfeasible_.

Another part of the literature has focused instead on _representation classes_ of MDPs. In this paper, we call "representation class" a family of problems that depend both on an MDP and on an exogenous element, usually a _feature map_. One example can be found in the popular class of _Linear MDPs_, which assumes that both the transition and the reward function can be factorized as a scalar product of a known feature map \(\) of dimension \(d_{}\) and some unknown vectors. Regret bounds of order \(}(H^{2}d_{}^{3/2})\) are possible , which succeed in moving the complexity of the problem into the dimension \(d_{}\) of the feature map. Unfortunately, this representation class is very restrictive: i) linearity is assumed on both the \(p_{h}\) and \(r_{h}\) functions; ii) the same linear factorization must be constant along the state-action space \(\), which goes in the opposite direction w.r.t. the locality principle introduced by Lipschitz MDPs. The first issue is solved by , which significantly extends this class of process by assuming linearity on the Bellman optimality operator, which turns out to be much weaker. This representation class is known as MDPs with _low inherent Bellman error_, a generalization of linear MDPs that further allows for a small approximation error \(\).

Very recently, different kinds of assumptions for continuous spaces were introduced. In _Kernelized MDPs_, both the reward function and the transition function belong to a _reproducing kernel Hilbert space_ (RKHS) induced by a known kernel coming from the Matern covariance function with parameter \(>0\). The higher the value of \(\), the more stringent the assumption, as the corresponding RHKS contains fewer functions. This kind of assumption enforces the smoothness of the process, which is stronger for higher values of \(\). A generalization of this family can be found in _Strongly smooth MDPs_, which require the transition and reward functions to be \(-\)times continuously differentiable. Although it is a wide, non-parametric family of processes, enforcing the smoothness of the transition function is rather demanding as it implies that the state \(s^{}\) is affected by a smooth noise. For this reason,  also defines the larger family of _Weakly smooth MDPs_, which only requires the smoothness of the Bellman optimality operator. This model is extremely general, as it can also capture Lipschitz MDPs. Still, for the same reason, it is affected by an exponential lower bound in \(H\). Lastly, note that for the last three kinds of MDPs, _Kernelized_, _Strongly and Weakly Smooth_, the best-known regret bounds are linear in \(K\) for some values of \(d\) and \(\). As the regret is trivially bounded by \(K\), these bounds are vacuous, not guaranteeing convergence to the performance of the optimal policy.

**Our contributions.** In this paper, we argue that the aspect that makes many continuous RL problems both learnable and feasible is local linearity, i.e., the possibility to locally approximate the MDP as a process exhibiting some sort of linearity for a well-designed feature map. To support this thesis, in the first part of the paper, (\(i\)) we introduce _Locally Linearizable MDPs_, a novel _representation_ class of MDPs depending on both a feature map \(_{h}\) and a partition \(_{h}\) of the state-action space of the MDP. This class generalizes both LinearMDPs and low inherent Bellman Error while also allowing the feature map to be local. (\(ii\)) We design an algorithm, Cinderella, which enjoys satisfactory regret bounds for this representation class. In the second part of the paper, we explore how this approach can be compared to the state of the art on continuous MDP; (\(iii\)) we show that all families all families that were defined in the continuous RL literature, to the best of our knowledge, for which learnable and feasible RL is possible are included in a novel family of _Mildly Smooth MDPs_ defined in this paper; (\(iv\)) we show that this family is, in turn, a special instance of our _Locally Linearizable MDPs_ representation class, for an appropriate choice of the feature map. Therefore, Cinderella can be applied to learning on all these families of continuous MDPs. (\(v\)) we finally prove that the regret bound of Cinderella outperforms several state-of-the-art results in this field.

## 2 Background and set-up

Markov Decision processes.We consider a finite-horizon Markov decision process (MDP) \(M=(,,p,r,H)\), where \(\) is the state space, \(\) is the action space,1\(p=\{p_{h}\}_{h=1}^{H-1}\) is the sequence of transition functions mapping, for each step \(h[H-1]\{1,,H-1\}\), a pair \(z=(s,a)\) to a probability distribution \(p_{h}(|z)\) over \(\), while the initial state \(s_{1}\) may be arbitrarily chosen by the environment at each episode; \(r=\{r_{h}\}_{h=1}^{H}\) is the sequence of reward functions, mapping, for each step \(h[H]\), a pair \(z=(s,a)\) to a real number \(r_{h}(z)\), and \(H\) is the horizon. At each episode \(k[K]\), the agent chooses a policy \(_{k}=\{_{k,h}\}_{h=1}^{H}\), which is a sequence of step-dependent mappings from \(\) to probability distributions over \(\). For each step \(h[H]\), the action is chosen as \(a_{h}_{k,h}(|s_{h})\) and the agent gains reward of mean \(r_{h}(z_{h})\) and independent on the past, then the environment transitions to the next state \(s_{h+1} p_{h}(|z_{h})\). For a summary of this notation see A.

Value functions and Bellman operators.The state-action value function (or _Q-function_) quantifies the expected sum of the rewards obtained under a policy \(\), starting from a state-step pair \((s,h)[H]\) and fixing the first action to some \(a\):

\[Q_{h}^{}(s,a)_{}[_{=h}^{H}r_{}(s_{ },a_{})|s_{h}=s,a_{h}=a]=_{}[_{ =h}^{H}r_{}(z_{})|z_{h}=(s,a)],\] (1)

where \(_{}\) denotes expectation w.r.t. to the stochastic process \(a_{h}_{h}(|s_{h})\) and \(s_{h+1} p_{h}(|z_{h})\) for all \(h[H]\). The state value function (or _V-function_) is defined as \(V_{h}^{}(s)_{a_{h}(|s)}[Q_{h}^{}(s,a)]\), for all \(s\). The supremum of the value functions across all the policies is referred to as the optimal value function: \(Q_{h}^{}(z)_{}Q_{h}^{}(z)\) for the Q-function and \(V_{h}^{}(s)_{}V_{h}^{}(s)\) for the V-function.

In this work, as often done in the literature , we make the following

**Assumption 1**.: _The instantaneous reward minus its mean \(r_{h}(s,a)\) is \(1-\)subgaussian, and normalized so that \(0 Q_{h}^{}(z) 1\) for every policy \(\), \(z\) and \(h[H]\)._

Passing to the case where the _per-step_ reward is in \(\) requires multiplying all upper bounds by \(H\). An explicit way to find the optimal value function is given by the _Bellman optimality operator_, which is defined, for every function \(f:\), as \(_{h}f(s,a) r_{h}(s,a)+_{s^{} p_{h}( |s,a)}[_{a^{}}f(s^{},a^{}) ].\) In fact, it is easy to show that \(Q_{h}^{}=_{h}Q_{h+1}^{}\) at every step, while the optimal state-value function is obtained simply as \(V_{h}^{}(a)=_{a}Q_{h}^{}(s,a)\).2

Agent's regret.We evaluate the performance of an agent, i.e., of a policy \(_{k}\) played at episode \(k[K]\), with its expected total reward, i.e., the V-function evaluated in the initial state \(V_{1}^{_{k}}(s_{1}^{k})\). The goal of the agent is to play a sequence of policies \(\{_{k}\}_{k=1}^{K}\) to minimize the cumulative difference between the optimal performance \(V_{1}^{}(s_{1}^{k})\) and its performance \(V_{1}^{_{k}}(s_{1}^{k})\), given the initial state \(s_{1}^{k}\) chosen by the environment. This quantity takes the name of _(cumulative) regret_, \(R_{K}_{k=1}^{K}(V_{1}^{}(s_{1}^{k})-V_{1}^{_{k}}(s_{1 }^{k})).\) This quantity is non-negative, and by the normalization condition, we can see that it cannot exceed \(K\) as every term in the sum is bounded by \(1\). Note that if \(R_{K}=o(K)\), then the average performance of the chosen policies will converge to optimal performance. An algorithm choosing a sequence of policies with this property is called _no-regret_.

Representation classes of MDPs.As anticipated in the introduction, we call "representation class" a family of MDPs that is defined through its relation with a feature map or another exogenous element. While the most popular representation class is the Linear MDP, assuming the exact factorization of both \(p_{h}\) and \(r_{h}\), no-regret learning is possible for a much wider family, only requiring a form of _approximate_ linearity on the application of Bellman's optimality operator. This class was introducedby  as MDPs with _low inherent Bellman error_. Given a sequence of compact sets \(_{h}^{d_{}}\), and calling \(Q_{h}[](s,a)\) the linear function \(_{h}(s,a)^{}\), the inherent Bellman error w.r.t. \(\{_{h}\}_{h}\) is defined as:

\[():=_{h[H]}_{_{h+1}}_{ ^{}_{h}}\|_{h}()^{}^{}-_{h}Q_{h+1}[]()\|_{L^{}},\] (2)

where the supremum norm \(\|\|_{L^{}}\) indicates the maximum of the function in absolute value over \(\). In the realizable case (i.e., \(=0\)), these processes are a strict generalization of Linear MDPs . To achieve regret guarantees for continuous state-action MDPs that go beyond this linear case, we will need to borrow some concepts of smoothness from mathematical analysis, as presented below.

**Smooth functions.** Let \([-1,1]^{d}\) and \(f:\). We define a multi-index \(\) as a tuple of non-negative integers \((_{1},_{d})\). We say that \(f^{}()\), for \((0,+)\), if it is \(_{*}-\)times continuously differentiable for \(_{*}:=-1\), and there exists a constant \(L_{}(f)\) such that:

\[:\ \|\|_{1}=_{*}, x,y:|D^{}f(x)-D^{}f(y)|  L_{}(f)\|x-y\|_{}^{-_{*}}\] (3)

the multi-index derivative is defined as \(D^{}f:=++_{d}}}{  x_{1}^{_{1}} x_{d}^{_{d}}}\). The previous set becomes a normed space when endowed with a norm \(\|f\|_{^{}}\) defined as \(\{_{||_{*}}\|D^{}f \|_{L^{}},L_{}(f)\}.\) Note that, when \(\), this norm reduces to \(\|f\|_{^{}}=_{||}\|D^{ {}}f\|_{L^{}},\) since the Lipschitz constant \(L_{}(f)\) of the derivatives up to order \(_{*}=-1\) correspond exactly to the upper bound of the derivatives of order \(\) (which exists as a Lipschitz function is differentiable almost everywhere). For these values of \(\), the spaces defined here are equivalent to the spaces \(^{-1,1}()\) defined in .

## 3 Locally Linearizable MDPs

As we have stated in the introduction, the main limitation of the low inherent Bellman error assumption is that it cannot model scenarios where the linear parameter \(\) changes across the state-action space. In fact, \(_{h}(s,a)^{}\) must be an approximation of the Q-function \(Q_{h}(s,a)\)_uniformly_ over \((s,a)\). To overcome this limitation, we introduce a novel concept of _locality_ to enable the feature map to be associated with different parameters \(\) in different regions of the state-action space.

Therefore, from this point on, we are going to associate to a given Markov Decision Process \(M\) the following two entities:

1. \(_{h}:^{d_{h}}\): a feature map which is allowed to depend on the current stage (also for its dimension.

Figure 1: In Locally Linearizable MDPs, we have that, as shown in (a), the space \(\) is partitioned into several regions, which do not need to be convex nor connected. On each of these regions, as shown in (b), the result of the Bellman optimality operator can be well approximated by a \(Q\) function that is linear in the feature map, with a parameter \(\) that may depend on the region itself.

2. \(_{h}\): a sequence of partitions of the state-action space \(\) in \(N_{h}\) regions, so that \(_{h}\{_{h,n}\}_{n=1}^{N_{h}}\). We call \(_{h}:[N_{h}]\) the map linking every element \(z\) to the index of its set in the partition \(_{h}\).

These two elements are necessary to introduce the function class we are going to use as function approximator in this setting. Calling \(_{h}=\{_{h,n}\}_{n=1}^{N_{h}}\) a list of vectors in \(^{d_{h}}\), one for each of the regions \(_{h,n}\), we employ, as function approximator for the state-action value function, the following set

\[_{h}\{Q_{h}[_{h}]()=_{h}()^ {}_{h,_{h}()},_{h}=\{ _{h,n}\}_{n=1}^{N_{h}},\ _{h,n}_{h,n}\}.\] (4)

Coherently, we will call \(_{h}\{V(s)=_{a}Q(s,a):\ Q_{h},\,s\}\). For fixed \(z\), we have \(Q_{h}[_{h}](z)=_{h}(z)^{}_{h,_{h}(z)}\), so that the feature map is allowed to depend directly on \(z\), while the linear parameter only depends on \(_{h}(z)\), the function indicating in which of the \(N_{h}\) regions we are. \(_{h,n}\) are arbitrary compact sets which contain the candidate values for the linear parameters \(_{h,n}\). Two relevant quantities for this model are the \(2-\)norm of the feature map and the diameter of the sets \(_{h,n}\),

\[L_{}_{h[H],z}\|_{h}(z)\|_{2} _{h,n}(_{h,n}),\]

Respectively. Analog normalization constants appear in . The low inherent Bellman error property can be redefined in this setting as follows.

**Definition 1**.: _(Inherent Bellmann Error) Given a family of compact sets \(_{h,n}^{d_{h}}\) depending on \(h[H],n[N_{h}]\), and their Cartesian product \(_{h}=_{n=1}^{N_{h}}_{h,n}\), we define:_

\[(,)_{h[H]}_{_{h+1} _{h+1}}_{_{h}_{h}}\|Q_{h}[_{h}]()-_{h}Q_{h+1}[_{h+1}]()\|_{L^{ }}.\] (5)

Note that, for \(N_{h}=1\), our definition exactly reduces to Equation (2), as expected. As in that case, the term \(\) plays the role of an approximation error. By assuming a bound on \((,)\), we can now give a formal definition of the class of MDPs we are going to study in this paper.

**Definition 2**.: _An \(-\)Locally Linearizable MDP3 is a triple \((M,\{_{h}\}_{h=1}^{H},\{_{h}\}_{h=1}^{H})\) where \(M\) is an MDP, \(_{h}:^{d_{h}}\) is a feature map and \(_{h}\) a sequence of partitions of the state-action space \(\) in \(N_{h}\) regions such that the corresponding inherent Bellman error (definition 1) satisfies \((,)\)._

As for the class of MDPs with Low Inherent Bellmann error, \(\) (and \(\) in our case) must be known to the agent, while \(\) is not needed. The novel aspect this class is that, as a result of definition 1, linearity is independently enforced in separate regions of the state-action space. We provide a visualization of this concept in Figure 1. Note that, in principle, any MDP belongs to the Locally Linearizable representation class for \(=1\). In fact, if we take \(_{h}=\{\}\), the trivial partition containing just the state-action space as an element, and \(_{h}(z)=0\) (a feature map mapping everything to \(0\)), Equation (5) is satisfied with \(=1\). Nonetheless, this class is _interesting_ only if \(\) is small, as it is easy to show that the regret of any algorithm in this class grows at least as \(K\).

**Limitations of known approaches.** Formally, no algorithm in theoretical RL literature can achieve no-regret learning on this class of problems, as it is a superclass of the low inherent Bellman error, which, to the best of our knowledge, is not included in any other setting that has been tackled. Still, a clever strategy called feature extension (example 2.1 from ) may allow us to solve this class of MDPs. In fact, consider Eleanor, the only algorithm able to deal with MDPs with low inherent Bellman error. This trick employs a newly defined feature map:

\[_{h}(z)[(z)_{h}(z)}_{N_{h }},\ _{h,2}(z)_{h}(z),,_{h,N_{h}}(z)_{h}(z)]^{}, _{h,n}(z)1&_{h}(z)=n\\ 0&\]

so that its dimension expands from \(d_{h}\) to \(N_{h}d_{h}\). This way, any function of \(_{h}\) (as defined in Equation 4) is linear in \(_{h}(z)\), with a single \(_{h}\) independent of the region \(_{n,h}\). Indeed, we have for \(Q_{h}[_{h}]_{h}\):\[Q_{h}[_{h}](z)=_{h}(z)^{}_{h,_{h}(z)}= _{h}(z)^{}[_{h,1},_{h,2},,_{h,N_{h}}].\]

This shows every Locally Linearizable MDP with feature map \(_{h}\) of dimension \(d_{h}\) is also an MDP with low inherent Bellman error w.r.t. \(_{h}\) of dimension \(N_{h}d_{h}\) and the same value for \(\). If we apply the regret bound for Eleanor[42, Theorem 1], we obtain:

\[R_{K}=}(_{h=1}^{H}N_{h}d_{h}+_{h=1} ^{H}d_{h}}K),\] (6)

holding with high probability. The issue is that the second term, growing linearly in \(K\), depends on the number of regions as \(}\). As we will see in the second part of this paper, the application of this model to Smooth MDPs requires \(N_{h}\) to be very large and also dependent on \(K\). For this reason, in the next section, we introduce an _ad hoc_ algorithm for Locally Linearizable MDPs to improve this dependence.

### Algorithm

As we have seen, even a wise application of the Eleanor algorithm is not enough to solve our setting of Locally Linearizable MDPs satisfyingly. Thus, we introduce a novel algorithm, Cinderella (Algorithm 1). Before analyzing it, we need to introduce some notation. Let us call \(s_{h}^{k},a_{h}^{k},r_{h}^{k}\) the state, action, and reward relative to step \(h\) of episode \(k\). Moreover, we denote \(z_{h}^{k}(s_{h}^{k},a_{h}^{k})\) and \(_{h}^{k}=_{h}(z_{h}^{k})\). At every episode \(k[K]\), we compute an optimistic estimation of the \(Q\)-function for every step \(h[H]\). Then, we choose actions in order to maximize this function while fixing \(s_{1}^{k}\) (line 7). Clearly, what is really important is how this surrogate \(Q\)-function is computed (line 4). Here, Cinderella relies on solving an optimization problem (7), which follows an idea similar to the one of . We want to optimize over three sets of variables: \(_{h,n},_{h,n},_{h,n}\), the first one representing ridge regression of the linear parameter for region \(n\) at step \(h\), the second representing the uncertainty relative to this estimation, and the third one an "optimistic" estimate. Under this view, the objective is to maximize the surrogate \(V\)-function in the first state, and the constraints are designed so that all variables match their intuitive interpretation. Formally, the optimization problem is defined as:

\[_{_{h,n},_{h,n},_{h,n}} _{a}_{1}(s_{1}^{k},a)^{}_{1, _{1}(s_{1}^{k},a)}\] (7) s.t. \[_{h,n}=_{h,n}^{k}{}^{-1}_{=1}^{k- 1}\{_{h}(z_{h}^{})=n\}_{h}^{}(r_{h}^{}+ _{a}_{h+1}(s_{h+1}^{},a)^{}_{h +1,(s_{h+1}^{},a)})\] \[_{h,n}=_{h,n}+_{h,n}\] \[\|_{h,n}\|_{_{h,n}^{k}}^ {k}}.\]

Where \(_{h,n}^{k}_{=1}^{k}\{_{h}(z_{h}^{}) =n\}_{h}^{}_{h}^{}{}^{}+ I,\) is the design matrix of \(-\)regularized ridge regression, and \(_{h,n}^{k}\) is a constant determining the exploration rate which will be fixed in the analysis B.6. As it is for Eleanor, this algorithm turns out to be computationally inefficient, an issue that we discuss in the Appendix D. The first constraint enforces that \(_{h,n}\) is estimated with ridge regression having as target the (optimistic) value function estimated for step \(h+1\), and the second constrain \(_{h,n}=_{h,n}+_{h,n}\) ensures that the optimistic estimate in every region is given by its mean estimate plus the uncertainty, while the third one bounds the magnitude of \(_{h,n}\) so that this uncertainty shrinks the more data we collect.

Although the structure of the algorithm is directly inherited from Eleanor, Cinderella distinguishes itself by dividing the samples across the various regions of the state-action space of the MDP. This allows parameters associated with different regions to be learned independently. Despite introducing another layer of technical difficulty in proving the regret bound, this procedure is relatively natural given the characteristics of our class of problems.

**Cinderella: Regret bound.** Having defined the algorithm, we can state a theorem showing a high probability regret bound in our setting.

**Theorem 2**.: _Assume to be in an \(-\)Locally Linearizable MDP with \(L_{}=(1)\), \(_{n[N_{h}]}_{h,n}=(_{h})\) and that Assumption 1 holds. Then, with probability at least \(1-\), Cinderella (Algorithm 1), with \(=1\) achieves a regret bound of order_

\[R_{K}=}(_{h=1}^{H}N_{h}d_{h}+_{h=1 }^{H}_{h}K).\]

The proof of this result is long, and is deferred to sections B and C of the appendix. Note that the two normalization assumptions \(L_{}=(1)\) and \(_{h[H],n[N_{h}]}_{h,n}=(_{h})\) correspond to the ones enforced by . While for \(N_{h}=1\) the two reported bounds coincide, Theorem 2 proves superior to the regret bound obtained for Eleanor in our setting (Equation 6), as it prevents the second term, which is linear in \(K\), to depend on \(N_{h}\). This dramatically changes the potential of the regret bounds and, as we will see in the next sections, represents the key to achieving sub-linear regret bounds for RL in continuous state-action spaces.

## 4 From local linearity to Mildly Smooth MDPs

Having proved that our Cinderella algorithm enjoys an improved regret bound on Locally Linearizable MDPs, we need to see how powerful this new class is once applied to problems with continuous state-action spaces for which a representation is not given a priori. To this aim, we are going to define a family of continuous-space MDPs, and prove that they are included in the Locally Linearizable class for a particular choice of \(_{h},_{h}\). From now on, we assume, without loss of generality, that \(=[-1,1]^{d_{S}}\) and \(=[-1,1]^{d_{A}}\), so that \(=[-1,1]^{d}\). We call Mildly Smooth MDP a process where the Bellman optimality operator outputs functions that are smooth.

Figure 2: Relation between the setting described in this paper and the other settings proposed for reinforcement learning in continuous state-action spaces. The dashed line means that inclusion holds, but passing to the larger family brings a \((H)\) lower bound on the regret. As we can see, the Mildly smooth MDP is the largest known setting for which regret of order poly(H) is possible. Note that the Strongly Smooth family also contains known families like LQRs and Linear MDPs with smooth feature map .

**Definition 3**.: _(Mildly Smooth MDP). An MDPs is Mildly Smooth of order \(\) if, for every \(h[H]\), the Bellman optimality operator \(_{h}\) is bounded on \(L^{}()^{}()\)._

Boundedness over \(L^{}()^{}()\) means that the operator transforms functions that are bounded (i.e., belong to \(L^{}()\)) into functions that are \(\)-times differentiable (i.e., belong to \(C^{}()\)). Moreover, there exists a constant \(C_{}<+\) such that \(\|_{h}f\|_{^{}} C_{}(\|f\|_{L^{ }}+1)\) for every \(h[H]\) and every function \(f L^{}()\). Intuitively, this condition means that by applying the Bellman operator to bounded (possibly non-smooth) functions, we always get functions that are smooth.

In order to reduce this family of processes to Locally Linearizable MDPs, we have to design the partition of the state-action space into sets \(_{h,n}\). Since \([-1,1]^{d}\), we can find, for every \(>0\), a set \(^{}\) which is an \(\)-cover of \(\) in the infinity norm, such that \(|^{}|:N(2/)^{d}\). Now, for every \(z^{n}^{}\), we define recursively \(_{n}\) to be the set of points which are near to \(z^{n}\), formally:

\[_{1}:=\{z:\|z-z^{1}\|_{} \},_{n}:=\{z:\|z-z^{n}\|_{} \}_{=1}^{n-1}_{}.\] (8)

By definition, every point of \(\) is matched with a point \(z^{n}\) of the cover \(^{}\) and, importantly, is assigned to exactly one subset \(_{n}\) of \(\). This way, we have defined a partition \(\{_{n}\}_{n=1}^{N}\) of \(\), one that does not depend on the step \(h\). Secondly, we have to choose the feature map \(_{h}()\). To this end, we define \(_{h}(z)\) as the vector of _Taylor polynomials of degree \(_{*}\)_ centered in \(z^{n}^{}\) for \(n=(z)\) (just "Taylor feature map" in the following). This means that, fixed \(z\) such that \((z)=n\), the map \((z)\) will contain terms of the form \((z-z^{n})^{}\) for every multi-index \(\|\|_{1}_{*}\). Note that the feature map does not depend on the time-step \(h\) either. The dimension of this feature map, which we call \(d_{_{*}}\), corresponds to the number of non-negative multi-indexes such that \(\|\|_{1}_{*}\), which is well-known to be \(d_{_{*}}=+d}{_{*}}_{*}^{d}\). The power of this choice lies in the fact that, as it is well known from mathematical analysis, any \(^{}\) function can be approximated by a Taylor polynomial of degree \(_{*}\) in a neighborhood of diameter \(\) with an error of order \(^{}\). Seeing the regions \(_{n}\) we have defined as neighborhoods of the points \(z^{n}\) of the cover, the analogy is complete. This argument is made formal in the following result.

**Theorem 3**.: _Let \(M\) be a Mildly Smooth MDP, and \( 1/(2C_{}H)^{1/}\). Then, choosing \(_{h,n}\) as in Equation (8) and taking \(_{h}\) as the Taylor feature map on the same regions, the tuple \((M,\{_{h,n}\}_{n,h},\{_{h}\}_{h})\) is an \(-\)Locally Linearizable MDP with_

\[L_{}=1+2_{_{*}},_{h,n}=2_{_{*}}C_{ }, 2C_{}^{}.\]

This reduction shows how general the class of Locally Linearizable MDPs is and enables us to tackle Mildly Smooth MDPs with the Cinderella algorithm, originally designed for Locally Linearizable MDPs. By appropriately selecting the parameter \(\), we can prove the following theorem, bounding the regret of a Locally Linearizable MDP with smoothness \(\) and state-action space of dimension \(d\).

**Theorem 4**.: _Let \(M\) be a Mildly smooth MDP of parameter \(>0\) satisfying Assumption 1. With probability at least \(1-\), Cinderella, initialized with \(=1\), \(_{h,n}\) as in Equation (8) and \(_{h}\) given by the Taylor feature map on the same regions, achieves a regret bound of order:_

\[R_{K}}(Hd_{_{*}}K^{}+H^ {}).\]

Before comparing our result with the state of the art, some comments are due. First note that the exponent of \(K\) is \(\), which is always in \((1/2,1)\). This means that the no-regret property is achieved _in every regime_. Two elements in this regret bound are undesirable: i) the exponential dependence in \(d\) (as \(d_{_{*}}_{*}^{d}\)) and ii) the lower-order term \(H^{}\), which has an exponent that may be very large, albeit polynomial in \(H\). The first issue is discussed in the appendix (Section E.5). We show that even for the much simpler continuous _bandit_ problem, lower bounds entail that the problem is not learnable unless \(d=o((K))\). Therefore, terms of order \(2^{d}\) can still be seen as \(o(K^{})\) for an arbitrarily small \(>0\). For the second issue, note that the exponent of \(H\) in the lower order term is significantly large only if \(d\). This regime is known to be very difficult, and in the literature before this paper it was not even possible to achieve the no-regret property (even just for \(d>2\)).

We end this section with a simple corollary showing how to deal with the "large \(\)" regime.

**Corollary 5**.: _Under the assumption of theorem 4, for \(>(K)\) we have_

\[R_{K}=}(H(K)^{d}K^{}+H^{2}).\]Proof.: Applying theorem 4 for \(\) arbitrarily large does not work, as \(d_{_{*}}^{d}\) makes the bound vacuous. Still, note that, being \(\|\|_{^{^{}}}\|\|_{^{^{}}}\) for \(^{}\), under the assumption of the theorem we can take \(=(K)\). At this point, theorem 4 ensures

\[R_{K}}(H(K)^{d}K^{}+H^{2})=}(H(K)^{d}K^{}+H^ {2}),\]

due to the fact that \(K^{}=K^{}K^{}=K^{ }2^{} 2^{d}K^{}\). 

The appearance of \((K)^{d}\) should not scare: this term is necessary even in the simpler case of bandits with squared exponential kernel, as the lower bound in  shows.

## 5 Comparison with related works

Regret bounds for continuous MDPs have been an area of intense research in recent years. While many parametric families like LQRs have been shown to achieve \((H)\) regret bounds , tackling this problem in more general cases has been proved to be very challenging.

Kernelized MDPs [8; 41; 13] are a representation class of processes assuming that the transition function \(p_{h}()\) as well as the reward function \(r_{h}()\) belong to a Reproducing Kernel Hilbert Space (RKHS) with given kernel \(k(,)\). Most of the literature deals with the case of the Matern kernel. The smoothness of this kernel is determined by a parameter \(>0\); by fixing it we can compare this family with the other families of continuous MDPs. The best-known result  in this setting only achieves regret \(}(K^{})\), which is vacuous if \(d>2\). Very recently  presented a regret bound of order \(}(K^{})\), but we were not able to verify the correctness of this result. We discuss a possible subtle issue of the proof in Appendix F. Another family that is based on assuming that \(p_{h}\) and \(r_{h}\) belong to some given functional space is the Strongly Smooth MDP . This family assumes that \(p_{h}(s^{}|),r_{h}()^{}()\). A subtle difference is that in , the smoothness index \(\) was restricted to be an integer, while in our case, it is a generic real \(>0\). Regret bounds for this family were shown of order \(}(K^{})\), which is vacuous even for \(d>\). Since, for the same \(\), the Matern kernel RKHS is a subset of \(^{}()\) (see Appendix E.3), Strongly Smooth MDPs are more general than the former family. Further increasing the generality, we have Lipschitz MDPs . This is only a superset of the Strongly Smooth MDPs for \(=1\), as Lipschitz MDPs do not admit higher levels of smoothness. A regret guarantee of order \(}(K^{})\), which turns out to be optimal for this setting, was achieved by different algorithms [33; 34; 32; 21] in recent years. Unfortunately, it was recently shown that an _exponential dependence on the horizon \(H\)_ is unavoidable . Therefore, the Lipschitz MDPs, as any of their generalizations, are intrinsically unfeasible.

   Algorithm & Weakly & Lipschitz & **Mildly** & Strongly & Kernelized \\ 
 Legendre-Eleanor & \(K^{}\) & \(K^{}\) & \(K^{}\) & \(K^{}\) & \(K^{}\) \\
 Golf & \(K^{}\) & \(K^{}\) & \(K^{}\) & \(K^{}\) & \(K^{}\) \\
 Net-Q-Learning & � & \(K^{}\) & � & \(K^{}\) & \(K^{}\) \\  & � & � & \(K^{}\) & \(K^{}\) \\
 Legendre-LSVI & � & � & � & \(K^{}\) & \(K^{}\) \\
 KOVI & � & � & � & � & \(K^{}\) \\   & **Yes** & **Yes** & No & No & No \\   

Table 1: Table containing the order w.r.t. \(K\) of the regret guarantee of each algorithm for each setting discussed in the paper. Columns correspond to different smoothness assumptions: Weakly and Strongly Smooth MDPs were defined in , Lipschitz MDPs in , and Kernelized MDPs in . Rows correspond to algorithms with no-regret guarantees for some of the settings. [25; 34; 38] represented the state of the art for Strongly smooth MDPs, Lipschitz MDPs, and Kernelized MDPs, respectively. The last row indicates whether the corresponding setting is feasible or if there exists an \((H)\) lower bound for the regret.

Increasing the generality even further, we find the family of Weakly Smooth MDPs , which imposes the Bellman optimality operator \(_{h}\) to be bounded on \(^{}()^{}()\). For fixed \(\), this assumption generalizes Strongly smooth MDPs (with the same \(\)), and, for \(=1\), the Lipschitz MDPs. For this reason, also this family is affected by an \((H)\) regret lower bound, while in terms of \(K\) its regret has been bounded as \(}(K^{})\), the same as Kernelized MDPs. Introducing a different notion of dimension, we can also define the family of MDPs with bounded Bellman-Eluder dimension . This family is, in a certain sense, even wider, but admits worse regret bounds .

**Key point: locality.** Our approach leverages the concept of locality. We approximate the continuous problem by constructing a feature map that captures information from local neighborhoods. This is a novel approach in the context of Reinforcement Learning (RL), with only  employing a remotely similar idea. Nonetheless, the effectiveness of this strategy has been well-established in the field of Continuous Armed Bandits, as demonstrated by [16; 23].

**Comparison with our work.** As the name suggests, the family of Mildly Smooth MDPs occupies an intermediate position between Weakly and Strongly Smooth processes. In Appendix E.2, we formally prove this relation, showing that both inclusions are strict. A graphical representation of this relationship is shown in Figure 2. Note that, even if our family is not the largest that has been studied in the literature, it is indeed the largest known one for which RL is feasible.

We have summarized the comparison between the regret bound of Cinderella with state-of-the-art algorithms for the various MDP families in Table 1, where settings (columns) are listed from the most general (Weakly Smooth) to the least (Kernelized). All inclusions are intended to hold for a fixed parameter \(\), which is shared by all the MDP families apart from the Lipschitz one, which in some sense has fixed \(=1\). For every column, the best regret guarantee is colored in green, with \(\)meaning that the algorithm (row) is unsuitable for the setting (column). As we can see, Cinderella i) works in every setting where there is no \((H)\) lower bound for the regret, achieving the best guarantees ii) is the only algorithm to exploit smoothness fully, i.e. achieving regret \(\) in the limit \(\) while being non-vacuous for all finite values of \(d,\).

## 6 Conclusion

In this paper, we have significantly enlarged the set of MDPs for which no-regret guarantees are possible. After defining the representation class of Locally Linearizable MDPs (Section 3), we have introduced a new algorithm called Cinderella (Algorithm 1), which is able to achieve a strong regret guarantee on this setting, being a "local" generalization of the Eleanor algorithm. In the second part of the paper, we have introduced a family called Mildly Smooth MDPs (Section 4), which generalizes all the known continuous MDP families where no-regret learning is feasible. Through an argument based on local Taylor polynomial approximation, we have proved that this family is a special instance of the Locally Linearizable MDPs, with a specific partition and feature map. Therefore, we were able to achieve, in Theorem 4, a regret bound for Cinderella in the family of Mildly Smooth MDPs, which constitutes the main result of this paper. Not only this bound surpasses the state-of-the-art in terms of generality for feasible settings, but is also able to achieve improved regret bounds for Strongly Smooth MDPs and Kernelized MDPs. Crucially, our work proves the first regret bound that is non-vacuous in any regime for these two families.