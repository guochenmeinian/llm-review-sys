# An NLP Benchmark Dataset for Assessing Corporate Climate Policy Engagement

Gaku Morio and Christopher D. Manning

Stanford University

{gaku,manning}@stanford.edu

Also a researcher of Hitachi America, Ltd., Santa Clara, California.

###### Abstract

As societal awareness of climate change grows, corporate climate policy engagements are attracting attention. We propose a dataset to estimate corporate climate policy engagement from various PDF-formatted documents. Our dataset comes from LobbyMap (a platform operated by global think tank InfluenceMap) that provides engagement categories and stances on the documents. To convert the LobbyMap data into the structured dataset, we developed a pipeline using text extraction and OCR. Our contributions are: (i) Building an NLP dataset including 10K documents on corporate climate policy engagement. (ii) Analyzing the properties and challenges of the dataset. (iii) Providing experiments for the dataset using pre-trained language models. The results show that while Longformer outperforms baselines and other pre-trained models, there is still room for significant improvement. We hope our work begins to bridge research on NLP and climate change.

## 1 Introduction

Climate change is one of the most critical challenges confronting our society today . As societal awareness of climate change heightens, how corporations minimize their environmental impact has come under tight scrutiny by the public . Consumers are increasingly warming up to "eco-friendly" products  and investors are placing a premium on investments yielding environmental benefits, as evidenced by the popularity of Environment, Social, and Governance (ESG) funds .

Nonetheless, skepticism about whether companies that claim environmental benefits are truly effective in mitigating the impact of climate change persists. Controversies have been sparked by dubious practices such as using vague terms to guide consumers to certain irrelevant conclusions (e.g., using 'all-natural' to imply beneficial to the environment)  or engaging in lobbying activities to mislead the public and policy-makers . Such practices are often known as _greenwashing_ - more formally defined as "_behavior or activities that make people believe that a company is doing more to protect the environment than it really is_". Greenwashing can help a company to enhance their corporate legitimacy, if it is not exposed . To prevent that from happening, instances of corporate greenwashing have been flagged by environmental, non-profit, and non-governmental organizations. The ongoing public scrutiny deters corporate attempts to mislead stakeholders with deceptive messages  and encourages truthful engagement with the public.

Monitoring and identifying greenwashing at scale is difficult as it requires experts with domain knowledge to analyze corporate documents. Natural language processing (NLP) can help to alleviate the issue by automating the process of extracting relevant data quickly. As Stammbach et al.  mentioned, the initial step in identifying greenwashing would involve recognizing corporate environmental claims - something that NLP would be well-suited to handle. Previous NLP effortsin the area include ClimateBERT , a domain-adapted language model that incorporates climate-related data, that has been used to label whether a paragraph is climate-related or not  and to identify Task Force on Climate-Related Financial Disclosures (TCFD) categories such as governance, strategy and risk management for sentences in corporate reports .

However, NLP research in the context of climate change and greenwashing is still in its infancy due to the lack of principles on task definitions and model development. This is primarily due to the lack of open datasets suitable for training and evaluating models within this context. While others have provided datasets in this field [43; 15; 38], they are either narrowly defined, lacking explainability in tasks, not diverse in terms of data source and geography, or small.

Our paper proposes a dataset representing corporations' climate policy engagement collected from a plethora of corporate-related documents that can potentially be used to automatically detect greenwashing. We construct the dataset from LobbyMap , a platform operated by global think tank InfluenceMap . As illustrated in Figure 1, our dataset poses a task that takes the text extracted from a PDF file, such as a corporate sustainability report, as input and outputs a set of (query, stance, evidence page indices) triplets. The query represents high-level climate policy issues, such as _'Renewable energy'_. The stance represents a scale with five levels ranging from _'strongly supporting'_ to _'opposing.'_ The evidence page indices give the supporting pages for the query and stance.

We invested effort in collecting data, and aligning evidence to transform the raw data of LobbyMap into the dataset. Consequently, our dataset differentiates itself from others in terms of its size, label richness, and diversity. Our contributions can be summarized as follows:

**NLP Dataset on Climate Change.** We have assembled a high-quality, large-scale (i.e., including over 10K documents), and diverse dataset, designed for the task of evidence-based assessment of corporate climate policy engagement. We anticipate that our dataset will stimulate research on NLP and climate change, steering it towards more accurate detection of greenwashing.

**Dataset Analysis.** We conducted analyses of the dataset properties, demonstrating that the task of our dataset is challenging as an NLP task.

**Benchmark Experiments.** We evaluated the performance of pre-trained language models such as BERT , ClimateBERT, and Longformer  on the task. While we obtained promising results, e.g., about 70% F-score for evidence-page detection, there remains ample room for improvement in prediction performance. Furthermore, to establish a benchmark for interpretability and explainability in this task, we introduce and evaluate a supplementary task where the model provides a scrutiny comment for the given query, stance, and evidence page indices.

Overall, we hope that our dataset will stimulate research on NLP and climate change and possibly serves as a foundation for the detection of corporate greenwashing. The code and dataset are available at https://climate-nlp.github.io.

## 2 Related Work

**Background on Greenwashing Controversy.** The term 'greenwashing' was introduced by environmentalist Jay Westerveld in 1986 . The field of greenwashing is extensive and has been the subject of much research in recent years. Some of the existing literature delves into the dynamics between consumers and firms. As the marketplace becomes saturated with an increasing number of "green"

Figure 1: Schematic overview of the proposed task.

products that tout positive environmental advantages, allegations of greenwashing have concurrently risen . A survey study by de Freitas Netto et al.  suggests that certain research highlights _selective disclosure_ wherein greenwashing manifests when corporations selectively highlight positive environmental impact. The renowned _seven sins of greenwashing_ have been widely discussed and are identified as product-level greenwashing in various studies [12; 13]. For instance, _the sin of the lesser of two evils_ pertains to a claim that might be accurate within a specific product category but could potentially divert consumers from the broader environmental implications inherent to that entire category .

Nemes et al.  contributed a more exhaustive survey, underscoring the gaps in establishing a definition for greenwashing and setting definitive behavioral standards to identify it. A potential connection between our research and their definition of greenwashing might be found in the contexts of _corporate responsibility in action_ (where a claim is categorized as greenwashing if it is not mirrored by consistent organizational practices) and _political spin_ (deemed as greenwashing when corporations express green undertakings while concurrently lobbying against environmental legislation) . This is because our dataset covers claims from a variety of sources, from official reports to political documents, which can spot potential contradictions in claims.

**Greenwashing-related NLP Research.**

In a few recent years, a burgeoning interest has emerged in leveraging NLP or fact-checking methodologies for the analysis of climate change-centric documents [43; 15]. While not directly addressing greenwashing, there exist Question Answering (QA) systems  and bots  tailored to facilitate the acquisition of credible climate-related knowledge. These systems typically utilize document retrieval techniques sourcing content from news articles by media and publications disseminated by global institutions to generate responses to user queries. Within the greenwashing context, researchers have implemented keyword analyses in annual reports to analyze mismatches in discourse, actions, and investments . Detecting environmental claims is perceived as a first step towards a greenwashing evaluation [38; 37]. Stammbach et al.  introduced a sentence-level classification task for environmental claims in sources such as sustainability reports, earnings calls, and annual reports. Notably, ClimateBERT, a specialized model tailored for the climate domain, has been employed for detecting climate-related paragraphs  and for classifying TCFD categories . Bingler et al.  provided an analysis that firms are primarily selective in reporting immaterial climate risk information. We believe our research aligns most closely with the detection of environmental claims, while concurrently performing the role of potential fact-checking.

## 3 Understanding LobbyMap

LobbyMap (or its organization InfluenceMap) has been referenced in media outlets [40; 42] as well as in academic research , typically in the context of corporate greenwashing. LobbyMap analyzes a wide range of diverse information about companies and industry associations, categorizing each corporation's stance on specific topics. (See also their methodology  reproduced in the supplementary material.) LobbyMap features various platform categories, such as regional categories (Japan, United States, South Korea, Australia, and European Union) and a specific corporate group known as the Climate Action 100+ (CA100+) Investor Hub , which we focus on in our dataset.

The LobbyMap system consists of several interconnected components, as illustrated in Figure 2. These components work together to offer a comprehensive understanding of a corporation's stance on climate-related issues. Any key terms will be defined and explained in detail later.

**Company Profile.** Each company on LobbyMap has a profile summary page that contains relevant company information, ratings, and a summary review by an expert. These assessments are substantiated by a scoring matrix described below.

**Scoring Matrix.** As illustrated in Figure 2b, the scoring matrix contains 13 _query_ rows and 7 _data source_ columns. Each cell in the matrix links to a page that contains evidence items pertinent to the selected query and data source.

**Evidence Item.** Each evidence item, as depicted in Figure 2c, records the corporate _stance_ for the query along with an excerpt (we call this _evidence snippet_) quoted from an attached data source file in PDF format. This page also includes an analyst's _comment_ summarizing the reasons for the assigned stance. One may also obtain other metadata such as the year, creation date, and region associated with the evidence.

### Key Terms in an Evidence Item

**Query.** The query gives the subcategory of the climate policy engagement agenda. For instance, _Energy transition & zero carbon technologies'_ relates to the economy's transition away from carbon-emitting technologies in line with the IPCC's guidelines. A comprehensive list of query definitions can be found in Appendix A.7.

**Data Source.** The data source is characterized by seven distinct document types: _Main Website_, _Social Media_, _CDP Response_, _Direct Consultation with Governments_, _Media Reports_, _CEO Messaging_, and _Financial Disclosures_. PDF files from this data source are attached to the evidence item. Depending on the analyst's approach, a PDF file attached may contain excerpts of only relevant pages, or the entire document. Occasionally, PDF files are screenshots of a website or social media.

**Stance.** A stance is a position expressed on a five-level discrete scale between \(+2\) and \(-2\). This scale represents whether the company is _'strongly supporting'_ (+2), _'supporting'_ (+1), expressing _'no position or mixed position'_ (0), _'not supporting'_ (-1), or _'opposing'_ (-2) the query.

**Evidence Snippet.** An evidence snippet is an excerpt extracted by a human analyst from the attached PDF file in the evidence item. Snippets can be sentences, clauses, or even paragraphs, and may span multiple pages. We use these excerpts to identify the page indices of the PDF file that contain the evidence.

**Comment.** A comment is a brief, human-generated content pertaining to the query, stance, and evidence snippet. It provides an insight into the rationale behind the assigned stance. An example of a comment for a query _'Land use'_ is _"Supports forestry sequestration for carbon offsetting but is unclear if supports regulations."_ This comment is used in our supplementary task in Section 6.4.

## 4 Dataset

Our dataset is meticulously curated to offer value to various research purposes. For NLP, this dataset poses a real-world challenge concerning corporate documents in the sustainability domain. For sustainability and finance, models trained on this dataset provide a systematic way to predict a corporate stance on environmental categories.

### Dataset Design

Our dataset adheres to two key design principles: First, it should contain all the necessary information to evaluate a corporation's climate policy engagements. Secondly, the data should be stored in a standard format so that it can quickly and easily be used by downstream tasks.

Figure 3: Example data representation of our dataset.

Figure 2: Example screenshots of LobbyMap pages. The profile page contains a summary of the company, as well as a scoring matrix consists of 13 query labels and 7 data sources. Each matrix cell contains evidence items for the selected query and data source.

Information Included in the Dataset.The dataset meets the first design principle by recording corporate climate policy engagements, which will be used in our task as in Figure 1. Specifically, our dataset will contain text extracted from a document and its associated triplets \((P,Q,S)\), where \(Q\), \(S\), and \(P\) denote a query label, the stance label, and evidence page indices supporting the query and stance, respectively.2 Our dataset also includes the comment, which is used in the supplementary task described later. The above information provides sufficient information to assess a corporate position on a particular climate issue category.

**Data Representation.** In order to make the dataset easy to use, we store the data in the JSON Lines format since this is widely used in tasks such as fact-verification . Each line contains an instance of our task. An example instance is shown in Figure 3. The _sentences_ field contains the text data extracted and sentence-tokenized from the PDF files, including relevant details such as sentence ID and page numbers. Therefore, researchers can directly use the field as task input. The _evidence_ field contains information obtained from evidence items (i.e., \(P\), \(Q\), and \(S\).) The _meta_ field contains any other metadata, providing origin information for the evidences items.

### Dataset Construction

We invested effort in PDF parsing and evidence alignment to transform the raw data of LobbyMap into the dataset. As depicted in Figure 4, the dataset construction primarily consisted of three stages: (i) data collection, (ii) establishing alignments (i.e., correspondences) between the evidence snippet and text in the PDF to identify the evidence page indices, and (iii) splitting data into training, validation, and test sets. Note that we omit pragmatic technique here, but that can be found in Appendix A.5.

**Data Collection.** From February to March 2023, we collected evidence items from companies listed under CA100+ that includes firms key for climate-change. A comprehensive list of these companies can be found in Appendix A.8.

**Text Extraction from PDF Files.** PDF files have varying layouts. In particular, some files contain only embedded images without embedded text. We employed three different PDF text extraction parsers to obtain textual data from the PDF files robustly: Fitz in PyMuPDF , docTR , and Tesseract . The first is a tool that extracts embedded text from PDF file whereas the last two are Optical Character Recognition (OCR) based software. Our approach of using three parsers to extract textual data minimizes the chances of not obtaining any usable information. We also preserved the order of the text based on the layout produced by each parser.

**Alignment between Evidence Snippet and PDF File.** We need to construct \(P\), the indices of the pages containing evidence snippets, because the evidence snippet does not explicitly tell us which part of the PDF file was extracted for the snippet. We used NLTK  to split the evidence snippet and text extracted from the PDF file into sentences to align the evidence snippet and PDF at the sentence level. After obtaining alignments from each of the three aforementioned parsers, we selected the one with the highest number of alignment. Finally, we obtained the page indices containing the alignments and designated them as \(P\).

**Data Splitting.** We split the data obtained by the previous steps into training, validation and test sets based on the _year_ metadata included in the evidence items. Documents before 2022 were assigned into the validation set and train set, while those in or after 2022 were designated as test data. This decision ensures that our model evaluation hinges on more recent data, mirroring a realistic scenario where data from the future is employed for evaluation.

Figure 4: Schematic overview of the dataset construction procedure.

## 5 Dataset Analysis

This section provides data analyses and demonstrates the values of the dataset for benchmarking.

**Basic Statistics.** Table 1 shows statistics on the dataset. The training set contains over 7K documents, totalling 28M words distributed across over 60K pages. Also, the number of pages per document is approximately 6-9 and the word counts per document exceed 2,500. This property suggests that our task can be positioned as long document understanding, which is a challenging aspect of NLP. Besides, models must generate multiple triplets per document, increasing the complexity of the task.

**Label Distribution.** We analyzed the label distribution of query and stance. (The full table is in Appendix A.9.1.) We found that the labels are imbalanced. There are about 1K training samples of _Energy transition & zero carbon technologies'_ for _'supporting'_, while the query _'Land use'_ shows a sparse presence of labels. The imbalances introduce another level of challenge, where models must predict less frequent labels accurately. We also found a general trend of companies leaning towards a positive stance. This trend may be explained by the fact that companies often seek to enhance their public reputation.

**Diversity.** We found that our dataset is diverse in terms of:

**(i) Time.** The dataset includes evidence items spanning over years. (See Appendix Figure 8.) The increasing number of samples collected each year suggests that the reporting and/or scrutiny have increased over time.

**(ii) Data Source.** The most frequent data source is _Main Web Site_ and the least is _Financial Disclosures_ (See Appendix Figure 9.) Interestingly, _Social Media_ and _CEO Messaging_ are also frequent, showing our dataset contains various sources.

**(iii) Geographic Diversity Regarding the Company.** Figure 5 shows the geographical distribution of the corporate headquarters in our dataset. We can see that our dataset contains companies not only from major economic powers such as United States, but also from resource-rich countries such as Australia, which has been accused of state-sponsored greenwashing . The dataset covers all the continents except Antarctica, allowing researchers to perform their analysis across various type of corporations.

**(iv) Geographic Diversity Regarding the Evidence Item Region.** In LobbyMap, each evidence item is associated with a specific regional target. For example, a document from an Australian media article sometimes describe regional events and can be associated within Australia. In the case of a multinational company, it may be associated within 'Global' that is not bound to a specific region. We examined such geographic diversity of evidence items (See Appendix Figure 11 for more details.) We found that 'Global' appears frequently, suggesting most companies in our dataset are multinational. The number of evidence items from Europe and United States is enormous.

**(v) Sector.** There are 14 different types of company sectors in our dataset: automobiles, chemicals, construction materials, consumer staples, energy, food products, healthcare, industrials, information

   & Train & Validation & Test \\  \# Doc & 7,425 & 825 & 2,354 \\ \# Output triplets & 11,159 (1.50) & 1,229 (1.49) & 3,336 (1.42) \\ \# Word & 28,434,661 (3829.58) & 3,067,156 (3717.76) & 6,244,125 (2652.56) \\ \# Page & 67,091 (9.04) & 7,289 (8.84) & 15,755 (6.69) \\  

Table 1: Basic statistics of the dataset (avg. num. per document in parentheses.)

Figure 5: Country distribution of corporate headquarters in our dataset.

technology, metals & mining, paper & forest products, retailing, transportation, and utilities. The wide variety of sectors represented in our dataset suggests that it may be of interest to researchers studying diverse industrial fields.

## 6 Benchmark Experiments

Here, we benchmark the proposed task on the dataset. Conceptually, our task projects an input text extracted from a document into a set of output triplets \(Y\), where \((P,Q,S) Y\).

### Models

First, we provide a most-frequent baseline that always outputs majority labels: \((P=\{0\},Q=\)_Energy transition & zero carbon technologies'_, \(S=\)_'supporting'_) for each document. This is similar to the majority class baseline and is useful as a simple consideration of the task's lower bounds.

Next, given that our benchmark will be used as baselines for future work, we decided to use pretrained language models that are widely used in current state-of-the-art studies. The challenge of this is that it is difficult to handle dozens of pages at the same time because most pre-trained language models have a limited context length. To this end, we employ a page-wise classification approach, where each document is split into pages, and we feed text of each page into the model and obtain output labels, gathering all the outputs in the document.

We have the following three page-wise classifiers as a pipeline: (i) The _evidence page detector_ is fine-tuned to predict whether each page contains evidence or not. (ii) The _query classifier_ predicts query labels (i.e., multi-label classification) given a page detected by (i). Gold page indices are used for fine-tuning. (iii) The _stance classifier_ predicts one of the five stance labels given the detected page and the predicted query label. Gold page indices and query labels are used for fine-tuning.

As basis of the classifiers, BERT  and its variant models, RoBERTa , ClimateBERT  and Longformer , with a classification head, are used. BERT and RoBERTa are known for the strong baseline in text classification tasks. ClimateBERT is pre-trained on sustainability and climate domain so we can verify the effectiveness of domain adaptation. We also provide Longformer, which is specialized for long-document understanding. These pre-trained language models are used as initial weights for the parameters of each of the three classifiers. The optimizer is Adam .

For comparison, we introduce a simple linear model using logistic regression  and tf-idf . While this model also employs the page-wise approach, it provides a distinct logistic regression binary classifier for each query and stance label.

**Implementation and Hyperparameters.** The input text for each page is created by concatenating all the sentences in the page. During inference for evidence page detection, if none of the pages are identified as containing evidence, the page with the highest probability is considered as the evidence page. For the stance classification, the input text is created by inserting query label in front of the page sentences. If the query classification generates multiple query labels for a single page, we create separate input text for each query label. Finally, we post-process the outputs by gathering evidence page indices which share the same query and stance labels. The implementation detail and hyperparameters are shown in Appendix A.10.

### Evaluation Metrics

We evaluate the model outputs in the aspects of evidence page detection, query classification, and stance classification. We provide three types of F-score metrics as follows:

**Strict.** This is a standard F-score, based on the set of tuples produced by a model and gold. The F-score for evidence page indices is evaluated using the set of output tuples and the set of gold tuples. The F-score is calculated for the page indices by singletons \((P)\), for the query by tuples \((P,Q)\) and for the stance by tuples \((P,S)\).

**Page overlap.** The above metric is "too strict" and can not capture how close are the model predictions to gold. To this end, we provide an evidence page overlap-based metric. This is based on the work of Barnes et al. , which calculates graph-based structured sentiment F-scores using the ratio of word token overlap between predicted and gold outputs. In this study, the F-scores are calculated by overlap ratio of gold and predicted evidence page indices. Thus, the more overlap, the better the F-score is. For query and stance labels scores, the overlap is considered only for cases which share the same query or stance label. For more detail, refer to Appendix A.10.

**Document.** This is the most rough metric where F-scores for query, stance, and evidence page indices are evaluated independently. The motivation of this metric is that there can be cases where the user wants to know just overall trends of query or stance of a company, not fine-grained evidence-based outputs, e.g., one can examine whether energy sector firms are increasingly mentioning renewable energy. For the F-score of evidence page indices, a set of output tuples \(s\) and a set of gold tuples \(g\), where each tuple represents \((,i)\) and \(i\) denotes an evidence page index, are used to calculate F-score. Similarly, F-score is calculated for query by output tuples \((,Q)\) and for stance by \((,S)\). In other words, an output of query/stance is considered correct if the query/stance label is correct, even if the evidence page indices for the output are incorrect.

### Result and Discussion

Table 2 provides an overview of the F-score for the test set. For the results of validation set, refer to Appendix Table 8. The results illustrate that all models can detect evidence page indices, with an F-score of about 70% in the document or page overlap \(P\) metrics. Given that most PDF files contain only one page (See Appendix Figure 10), this result may be generous. Nevertheless, given the F-score in the document \(P\) metric of the most-frequent baseline, which always outputs a page index of 0, shows a low F-score, we can see that training the model is highly effective. On the other hand, F-scores of the strict metric suggests the difficulty of our task to exactly identify evidences. This insight will be further explained later. Query \((Q)\) and stance \((S)\) classification proves to be challenging. All models demonstrate lower F-scores in these aspects across all the metrics, which might reflect the intricate nature of these tasks.

In terms of pre-trained language models, ClimateBERT outperformed BERT. This indicates that pre-training on the sustainability domain is effective in our task. However, the better trained but not climate-specific RoBERTa partially outperformed ClimateBERT. In turn, Longformer outperformed other models like RoBERTa, showing its robustness in handling long documents. Interestingly, the linear model outperformed BERT in the strict metric for \(P\) and \(S\). The linear model had higher precision but lower recall than BERT. The strict metric deems predicted page indices incorrect if they deviate at all from the gold indices, potentially disadvantageing high-recall models like BERT. These findings highlight the importance of using multiple metrics, including document and page overlap.

**Error Analysis - Classification.** We investigate representative error patterns of the Longformer-large model in the document metric. For the query, one of the most frequent errors is that the model

    &  &  &  \\  & \(P\) & \(Q\) & \(S\) & \(P\) & \(Q\) & \(S\) & \(P\) & \(Q\) & \(S\) \\  Most-frequent & 46.7 & 52.6 & 36.8 & 51.8 & 25.6 & 19.8 & 41.2 & 19.6 & 17.5 \\ Linear & 66.0 & 61.9 & 50.3 & 71.4 & 44.5 & 36.1 & 52.0 & 31.2 & 27.0 \\ BERT-base & 71.0 & 63.5 & 51.6 & 73.6 & 48.1 & 37.2 & 50.2 & 31.9 & 25.8 \\ ClimateBERT & 71.8 & 64.0 & 52.8 & 74.4 & 48.9 & 39.0 & 50.2 & 32.2 & 26.8 \\ RoBERTa-base & 71.6 & 64.5 & 53.1 & 73.8 & 49.6 & 38.3 & 50.4 & 33.4 & 26.6 \\ Longformer-base & 73.7 & 66.9 & 54.6 & 75.9 & 53.0 & 40.8 & 52.5 & 36.1 & 28.6 \\ Longformer-large & 73.9 & 68.8 & 57.3 & 76.5 & 55.0 & 44.1 & 53.6 & 38.7 & 31.5 \\   

Table 2: Evaluation results, measuring test F-score (%). \(Q\), \(S\), and \(P\) represent query, stance and evidence page indices, respectively.

Figure 6: For two companies in the energy sector, the distributions of gold and predicted stances for _‘Energy transition & zero carbon technologies’_ are shown. The model used is Longformer-large.

predicted '_Energy transition & zero carbon technologies'_ and '_Renewable energy_', while the gold is _Energy transition & zero carbon technologies.'_ This might be because the label '_Energy transition & zero carbon technologies_' sometimes includes the topic of '_Renewable energy_', and the model could not distinguish that. For the stance, the frequent error is that the model predicted '_supporting_', while the gold is '_no position or mixed position._' This suggests that the model finds it difficult to distinguish different nuances regarding the stance.

**Error Analysis - Longer Document.** The lower strict F-score, as noted earlier, seems to be influenced by the length of the documents. Figure 7 illustrates a decline in the prediction performance for \(P\) in the Longformer-large model with an increase in the number of pages per document. Inherently, due to the metric's nature, \(Q\) and \(S\) scores are as low as \(P\), as they are constrained by \(P\). This indicates that our model struggles with longer documents in terms of the fine-grained (i.e., strict) metric. This issue might be because our model, which operates training and inference on the page-by-page manner, does not consider the context of the entire document.

The above results and error analyses suggest the necessity of a domain-adapted model that is capable of comprehending lengthy contexts. Specifically, improving the model's focus on the overall document context and the consistency of multi-label output could potentially enhance its performance.

**Pilot Study.** We present a pilot study aimed at greenwashing detection. As depicted in Figure 6, we select two major companies (A and B) from the energy sector and aggregate the predicted stances towards the query '_Energy transition & zero carbon technologies_.3 The figure shows that our predictions are reasonably capable of replicating the trends present in the gold. The important finding here is that the trend of companies A and B is different even in the same sector. The company A tends to present negative stances, whereas the company B exhibits a more ambiguous stance. Company B's presentation of conflicting stances raises the possibility of greenwashing. Even if this were not the case, such comparisons allow researchers to uncover hypotheses worth exploring further. Thus, our dataset can also be used to model and test hypotheses about the corporate climate policy engagement.

### Comment Generation: A Supplementary Task for More Explanation

One can recognize that the output triplet \((P,Q,S)\) may not provide sufficient context in some cases, specifically when there is a need to understand how the evidence page is interpreted and how the query and stance are derived from the evidence. Therefore, we introduce a supplementary task: generating comments (as described in Section 3) for a given query, stance, and evidence page indices. We fine-tune FlanT5 [(]), where the model is trained by the comments as reference. The generated comments are evaluated by ROUGE [(26)]. The input text for the model is formatted as '_Generate a reason why the corporate climate policy engagement for "QUERY" is "STANCE". <PAGE TEXTS>_', where _QUERY_ and _STANCE_ represent the query and stance label respectively. <PAGE TEXTS>_ represents the concatenated sentences from all reference evidence pages (\(P\).)

Table 3 represents evaluation results, suggesting the potential of using generative models to generate rationale comments for given triplets \((,Q,)\). The FlanT5-XL model outperforms the FlanT5-large model on test data. Performance on the test set is lower than on the validation set. Since the validation and train sets are sampled from the same temporal span, this result suggests that generation performance can be undermined due to changes in the distribution of comments over time.

    & R-1 & R-2 & R-L \\  Test & & & \\ FlanT5-large & 38.4 & 22.1 & 34.8 \\ FlanT5-XL & 39.5 & 22.7 & 35.6 \\ Validation & & & \\ FlanT5-large & 43.4 & 28.1 & 40.5 \\ FlanT5-XL & 42.7 & 27.6 & 39.7 \\   

Table 3: Comment generation evaluation in ROUGE (%.)

Figure 7: Strict F-score based on the number of pages in the document.

Conclusion and Future Work

The field of research aimed at effectively utilizing NLP to contribute to solving climate change issues is still in its infancy. However, endeavors undertaken in recent years have demonstrated the considerable scope for advancement in this realm . We introduced an NLP dataset to predict corporate climate policy engagement. This study is one of the attempts to provide fundamental knowledge in this budding research area. We hope that the proposed dataset will stimulate research on NLP and climate change by laying the foundation for the detection of corporate greenwashing.

We recognize that there is future work to be done. The benchmark experiments revealed room for improvement in prediction performance. We describe several promising directions for future research: (i) Multi-modal. The development of multi-modal models capable of processing not only text from PDF files but also embedded images could enhance the prediction performance.

(ii) Multi-lingual. While we only considered English text in this work, the capability to work with other languages will allow us to create a more diverse dataset.

(iii) Few-shot learning. By leveraging the capabilities of Large Language Models (LLMs), we can explore strategies for few-shot learning, potentially enabling more efficient training with smaller amounts of data.