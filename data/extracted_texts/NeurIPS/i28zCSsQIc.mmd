# GlortiNets: Scalable Non-Convex Optimization

with Certificates

Gaspard Beugnot

gaspard.beugnot@inria.fr

Inria, Ecole normale superieure, CNRS, PSL Research University, 75005 Paris, France

Julien Mairal

julien.mairal@inria.fr

Univ. Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, 38000 Grenoble, France

Alessandro Rudi

alessandro.rudi@inria.fr

Inria, Ecole normale superieure, CNRS, PSL Research University, 75005 Paris, France

###### Abstract

We present a novel approach to non-convex optimization with certificates, which handles smooth functions on the hypercube or on the torus. Unlike traditional methods that rely on algebraic properties, our algorithm exploits the regularity of the target function intrinsic in the decay of its Fourier spectrum. By defining a tractable family of models, we allow _at the same time_ to obtain precise certificates and to leverage the advanced and powerful computational techniques developed to optimize neural networks. In this way the scalability of our approach is naturally enhanced by parallel computing with GPUs. Our approach, when applied to the case of polynomials of moderate dimensions but with thousands of coefficients, outperforms the state-of-the-art optimization methods with certificates, as the ones based on Lasserre's hierarchy, addressing problems intractable for the competitors.

## 1 Introduction

Non-convex optimization is a difficult and crucial task. In this paper, we aim at optimizing globally a non-convex function defined on the hypercube, by providing a certificate of optimality on the resulting solution. Let \(h\) be a smooth function on \([-1,1]^{d}\). Here we provide an algorithm that given \(\), an estimate of the minimizer \(x_{}\) of \(h\)

\[x_{}=*{arg\,min}_{x[-1,1]^{d}}h(x),\]

produces an \(\), that constitutes an explicit _certificate_ for the quality of \(\), of the form

\[|h(x_{})-h()|_{},\]

with probability \(1-\). The literature abounds of algorithms to optimize non-convex functions. Typically they are either _(a)_ heuristics, very smart, but with no guarantees of global convergence Moscato et al. (1989); Horst and Pardalos (2013)_(b)_ variation of algorithms used in convex optimization, which can guarantee convergence only to _local_ minima Boyd and Vandenberghe (2004)_(c)_ algorithms with only asymptotic guarantees of convergence to a global minimum, but no explicit certificates Van Laarhoven et al. (1987). In general, the methods recalled above are quite fast to produce somesolution, but don't provide guarantees on its quality, with the result that the produced point can be arbitrarily far from the optimum, so they are used typically where non-reliable results can be accepted.

On the contrary, there are contexts where an explicit quantification of the incurred error is crucial for the task at hand (finance, engineering, scientific validation, safety-critical scenarios Lasserre (2009)). In these cases, more expensive methods that provide certificates are used, such as _polynomial sum-of-squares_ (poly-SoS) Lasserre (2001, 2009). These kinds of techniques are quite powerful since they provide certificates in the form above, often with machine-precision error. However, _(a)_ they have reduced applicability since \(h\) must be a multivariate polynomial (possibly sparse, low-degree) and must be known in its analytical form _(b)_ the resulting algorithm is a semi-definite programming optimization on matrices whose size grows very fast with the number of variables and the degree of the polynomial, becoming intractable already in moderate dimensions and degrees.

Our approach builds and extends the more recent line of works on _kernel sum-of-squares_, and in particular the work of Woodworth et al. (2022) based on the Fourier analysis. It mitigates the limitations of poly-SoS methods in both aspects: _(a)_ we can deal with any function \(h\) (not necessarily a polynomial) for which the Fourier transform is known and _(b)_ the resulting algorithm leverages the smoothness properties of the objective function as Woodworth et al. (2022) rather than relying on its algebraic structure leading to way more compact representations than poly-SoS. Contrary to Woodworth et al. (2022), we fully leverage the power of the certificate allowing for a drastic reduction of the computational cost of the method. Indeed, we cast the minimization in terms of a way smaller problem, similar to the optimization of a small neural network that, albeit again non-convex, produces efficiently a good candidate on which we then compute the certificate.

Notably, our focus lies on a posteriori guarantees: we define a family of models that allow for efficient computation of certificates. Once the model structure is established, we have ample flexibility in training the model, offering various possibilities to achieve good certificates in practical scenarios, while still using well-established and effective techniques in the field of deep neural networks (DNN) Goodfellow et al. (2016) to reduce the computational burden of the approach.

Our contributions can be summarized as follows:

* We propose a new approach to global optimization _with certificates_ which drastically extends the applicability domain allowed by the state of the art, since it can be applied to any function for which we can compute the Fourier transform (not just polynomials).
* The proposed approach is naturally tailored for GPU computations and provides a refined control of time and memory requirements of the proposed algorithm, contrary to poly-SoS methods (whose complexity scales dramatically and in a rigid way with dimension and degree of the polynomial).
* From a technical viewpoint, we improve the results in Woodworth et al. (2022), by developing a fast stochastic approach to recover the certificate in high probability (theorem 3), and we generalize the formulation of the problem to allow the use of powerful techniques from DNN, still providing a certificate on the result (section 3, in particular alg. 1)
* In practical applications, we are able to provide certificates for functions in moderate dimensions, which surpasses the capabilities of current state-of-the-art techniques. Specifically, as shown in the experiments we can handle polynomials with thousands of coefficients. This achievement marks an important milestone towards utilizing these models to provide certificates for more challenging real-life problems.

### Previous work

Polynomial SoS.In the field of certificate-based polynomial optimization, Lasserre's hierarchy plays a pivotal role Lasserre (2001, 2009). This hierarchy employs a sequence of SDP relaxations with increasing size proportional to \(O(r^{d})\) (where \(d\) is the dimension of the space and \(r\) is a parameter that upper bounds the degree of the polynomial) and that ultimately converges to the optimal solution when \(r\). While Lasserre's hierarchy is primarily associated with polynomial optimization, its applicability extends beyond this domain. It offers a specific formulation for the more general moment problem, enabling a wide range of applications; see Henrion et al. (2020) for an introduction. For polynomial optimization problems such as in eq. (1), a significant amount of research has been dedicated to leveraging problem structure to improve the scalability of the hierarchy. This research has predominantly focused on exploiting very specific sparsity patterns among the variables of the polynomial, enabling the handling in these restricted scenarios of instances ranging from a few variables to even thousands of variables Waki et al. (2006); Wang et al. (2021b, a). There have been theoretical results regarding optimization on the hypercube Bach and Rudi (2023); Laurent and Slot (2022), but there are no algorithms handling them natively. Furthermore, alternative approaches exist that exploit different types of structure, such as the constant trace property Mai et al. (2022).

Kernel SoS.Kernel Sum of Squares (K-SoS) is an emerging research field that originated from the introduction of a novel parametrization for positive functions in Marteau-Ferey et al. (2020). This approach has found application in various domains, including Optimal Control Berthier et al. (2022); Optimal Transport Muzellec et al. (2021) and modeling probability distribution Rudi and Ciliberto (2021). In the context of function optimization, two types of theoretical results have been explored: _a priori_ guarantees Rudi et al. (2020) and _a posteriori_ guarantees Woodworth et al. (2022). A priori guarantees offer insights into the convergence rate towards a global optimum of the function, giving a rate on the number of parameters and the complexity necessary to optimize a function up to a given error. For example, Rudi et al. (2020) proposes a general technique to achieve the global optimum, with error \(\) of a function that is \(s\)-times differentiable, by requiring a number of parameters essentially in the order of \(O(^{-s/d})\), allowing to avoid the curse of dimensionality in the rate, when the function is very regular, i.e., \(s d\), while typical black-box optimization algorithms have a complexity that scales as \(^{-d}\). A-posteriori guarantees focus on providing a certificate for the minimum found by the algorithm. In particular, Woodworth et al. (2022), provides both a-priori guarantee and a-posteriori certificates; however, the model considered makes it computationally infeasible to provide certificates in dimension larger than \(2\).

To conclude, approaches based on kernel-SoS allow to extend the applicability of global optimization with certificates methods to a wider family of functions and on exploiting finer regularity properties beyond just the number of variables and the degrees of a polynomial. By comparison, we focus on making the optimization amenable to high-performance GPU computation while retaining an a posteriori certificate of optimality.

## 2 Computing certificates with extended k-SoS

Without loss of generality (see next remark), with the goal of simplifying the analysis and using powerful tools from harmonic analysis, we cast the problem in terms of minimization of a _periodic_ function \(f\) over the torus, \(^{d}\) (we will denote it also as \(^{d}\)). In particular, we are interested in minimizing periodic functions for which we know (or we can easily compute) the coefficients of its Fourier representation, i.e.

\[f_{}=_{z^{d}}f(z), f(z)=_{ ^{d}}_{}e^{2 z}, z ^{d},\] (1)

where \(\) is the set of integers. This setting is already interesting on its own, as it encompasses a large class of smooth functions. It includes notably trigonometric polynomials, _i.e._ functions which have only a finite number of non-zero Fourier coefficients \(_{}\). Optimization of trigonometric polynomials arises in multiple research areas, such as the optimal power flow Van Hentenryck (1988) or quantum mechanics Hilling and Sudbery (2010). Note that this problem is already NP-hard, as it encompasses for instance the Max-Cut problem Waldspurger et al. (2013). Even so, we will consider the more general case where we can evaluate function values of \(f\), along with its Fourier coefficient \(_{}\), and we have access to its norm in a certain Hilbert space. This norm can be computed numerically for trigonometric polynomials, and more generally reflects the regularity (degree of differentiability) of the function, and thus the difficulty of the problem.

**Remark 1** (No loss of generality in working on the torus).: _Given a (non-periodic) function \(h:[-1,1]^{d}\) we can obtain a periodic function whose minimum is exactly \(h_{*}\) and from which we can recover \(x_{}\). Indeed, following the classical Chebychev construction, define \((2 z)\) as the componentwise application of \(\) to the elements of \(2 z\), i.e. \((2 z):=((2 z_{1}),,(2 z_{d}))\) and define \(f\) as \(f(z):=h((2 z))\) for \(z^{d}\). It is immediate to see that_ (a)_\(f\) is periodic, and,_ (b) _since \((2 z)\) is invertible on \(^{d}\) and its image is exactly \([-1,1]^{d}\), we have \(h_{*}=h(x_{})=f(z_{})\) where_

\[x_{}=(2 z_{}), z_{}=_{z ^{d}}f(z).\]

### Certificates for global optimization and k-SoS

A general "recipe" for obtaining a certificates was developed in Woodworth et al. (2022) where, in particular, it was derived the following bound (Woodworth et al., 2022, see Thm. 2)

\[f_{}_{c,\ g_{+}}c-\|f-c-g\|_ {F}\,\] (2)

where \(\|u\|_{F}\) is the \(_{1}\) norm of the Fourier coefficients of a periodic function \(u\), i.e.

\[\|u\|_{F}:=_{^{d}}|_{ }|,\] (3)

and the \(\) is taken over \(_{+}\) that is a class of non-negative functions. The paper Woodworth et al. (2022) then chooses \(_{+}\) to be the set of _positive semidefinite models_, leading to a possibly expensive convex SDP problem. Our approach instead starts from the following two observations: _(a)_ the lower bound in eq. (2) holds for any set \(_{+}\) of non-negative functions, _not necessarily convex_, moreover _(b)_ any candidate solution \((g,c)\) of the supremum in eq. (2) would constitute a lower bound for \(f_{}\), so there is no need to solve eq. (2) exactly. This yields the following theorem

**Theorem 1**.: _Given a point \(^{d}\) and a non-negative and periodic function \(g_{0}:^{d}_{+}\), we have_

\[|f()-f(x_{})|\|f-f()-g_{0} \|_{F}\] (4)

Proof.: Since \(x_{}\) is the minimizer of \(f\), then \(f(x_{}) f()\). Moreover, since \(c_{0}=f()\) and \(g_{0}\) are feasible solutions for the r.h.s. of eq. (2), we have

\[f() f(x_{})_{c,\ g_{+ }}c-\|f-c-g\|_{F} c_{0}-\|f-c_{0}-g_{0}\|_{F},\]

from which we derive that \(0 f()-f(x_{})\|f-f()-g_{0}\|_{F}\). 

In particular, since any good candidate \(g_{0}\) is enough to produce a certificate, we consider the following class of non-negative functions that can be seen as a _two-layer neural network_.

**Definition 1** (extended K-SoS model on the torus).: _Let \(K:^{d}^{d}\) be a periodic function in the first variable and let \(m,r\). Given a set of anchors \(=(_{1},,_{})^{d}\) and a matrix \(R^{r m}\), we define the K-SoS model \(g\) with_

\[^{d}, g()=\|RK_{ }()\|_{2}^{2}, K_{}( )=(K(,_{1}),,K(,_{m}) )^{m}.\] (5)

The functions represented by the model above are non-negative and periodic. The model is an extension of the k-SoS model presented in Marteau-Ferey et al. (2020), where the points \((_{1},,_{})\) cannot be optimized. Moreover it has the following benefits at the expense of the convexity in the parameters:

1. The extended k-SoS models benefit of the good approximation properties of k-SoS models described in Marteau-Ferey et al. (2020) and especially Rudi and Ciliberto (2021), since they are a super-set of the k-SoS, that have optimal approximation properties for non-negative functions.
2. The extended model can have a _reduced number of parameters_, by choosing a matrix \(R\) with \(r=1\) or \(r m\). This will drastically improve the cost of the optimization, while not impacting the approximation properties of the model, since a good approximation is still possible with already \(r\) proportional to \(d\)(Rudi et al., 2020, see Thm. 3).
3. The extended model _does not require any positive semidefinite constraint_ on the matrix (contrary to the base model) that is typically a well-known bottleneck to scale up the optimization in the number of parameters Marteau-Ferey et al. (2020). In the extended model we trade the positive semidefinite constraint with non-convexity. However this allows us to use all the advanced and effective techniques we know for unconstrained (or box-constrained) non-convex optimization for (two-layers) neural networks Goodfellow et al. (2016).

To conclude the picture on the k-SoS models, a critical aspect of the model is the choice of \(K\), since it must guarantee good approximation properties and at the same time we need to compute easily its Fourier coefficients since we need to evaluate \(\|f-c-g\|_{F}\). To this aim, a good candidate for \(K\) are the _reproducing kernels_ defined on the torus Steinwart and Christmann (2008). We use shift-invariant kernels, enabling a convenient analysis of the associated RKHS through their Fourier Transform.

**Definition 2** (Reproducing kernel on the torus).: _Let \(q\) be a real function on \(^{d}\), with positive Fourier Transform and \(q(0)=1\). Let \(K\) be the kernel defined with_

\[ x,y^{d},\ \ K(x,y)=q(x-y)=_{^{d}} _{}e^{2(x-y)}.\] (6)

_Then, \(K\) is a r.k bounded by \(1\). We denote \(\) its Reproducing kernel Hilbert Space (RKHS) and by \(\|\|_{}\) the associated RKHS norm_

\[\|f\|_{}^{2}=_{^{d}}|_{}|^{ 2}/_{}.\]

_Define \((x)=q(x)^{2}\). We assume that we can compute (and sample from, see next section) \(_{}\), i.e., the Fourier transform of \(\), corresponding to \(()_{}\), for all \(^{d}\)._

By choosing such a \(K\), the models inherit the good approximation properties derived in Marteau-Ferey et al. (2020); Rudi and Ciliberto (2021). We conclude by recalling that shift-invariant r.k kernel have a positive Fourier transform due to Bochner's theorem Rudin (1990). The fact that \(K\) is bounded by \(1\) can be seen with \(|K(x,x)|=|q(0)|=_{}_{}=1\). Finally, note that the Fourier coefficients of an extended k-SoS model can be computed exactly, as in shown _e.g._ later in lemma 1.

### Providing certificates with the \(F\)-norm

As discussed in the previous section our approach for providing a certificate on \(f\) relies on first obtaining \(\) using a fast algorithm without guarantees and solving approximately eq.2 to obtain the certificate (see theorem1). With this aim, now we need an efficient way to compute the norm \(\|\|_{F}\). We use here a stochastic approach. Introducing a probability \(_{}\) (that later will be chosen as a rescaled version of \(_{}\) in definition2) on \(^{d}\) we rewrite the \(F\)-norm

\[\|u\|_{F}=_{^{d}}_{ }_{}|}{_{}}=_{ _{}}[_{}|}{ _{}}]\] (7)

which yields an objective that is amenable to stochastic optimization. From there, Woodworth et al. (2022) computes a certificate by truncating the sum to a hypercube \(\{;\|\|_{} N\}\) of size \(N^{d}\) and bounding the remaining terms with a smoothness assumption on \(u=f-c-g\), which enables to control the decay of \(_{}\). We want to avoid this cost exponential in the dimension so we proceed differently.

Probabilistic estimates with the \(\) norm.Given that the \(F\)-norm can be written as an expectation in eq.7, we approximate it with an empirical mean \(\) given with \(N\) i.i.d samples \((_{i})_{1 i n}_{}\). Now, note that the variance of \(\) can be upper bounded by a Hilbert norm, as

\[=_{i=1}^{N}_{_{i}}|}{ _{_{i}}},\ \ \ \,\,(_{ }|}{_{}})^{2}=_{ ^{d}}_{}|^{2}}{_{}}= \|u\|_{_{}}^{2},\] (8)

with \(_{}\) the RKHS from definition2 with kernel \(K(x,x^{})=_{^{d}}_{}e^{2 (x-x^{})}\). This allows to quantify the deviation of \(\) from \([]=\|u\|_{F}\), with _e.g._ Chebychev's inequality, as shown in next theorem.

Figure 1: \(f-f_{}\) is a trigonometric polynomial approximated by an extended k-SoS model \(g\). The \(L_{}\) norm of the difference _(blue)_ is upper-bounded by the \(F\)-norm _(red)_, which is itself upper bounded by the MoM inequality in theorem3, with probability \(98\%\), here showed with respect to the number \(N\) of sampled frequencies. Shaded area shows min/max values across \(10\) estimations.

**Theorem 2** (Certificate with Chebychev Inequality).: _Let \((_{})_{}\) be a probability distribution on \(^{d}\), \((0,1)\) and \(g\) a positive function. Let \(N>0\) and \(\) be the empirical mean of \(|_{}-c-_{}|/_{}\) obtained with \(N\) samples \(_{i}_{}\). Then, a certificate with probability \(1-\) is given with_

\[f_{} c--_{}} }{},\ \ =_{i=1}^{N}_{_{i}}-c _{_{i}=0}-_{_{i}}|}{_{ _{i}}}.\] (9)

Proof.: From its definition in eq. (7), we see that an unbiased estimator of the \(F\)-norm is given by \(\). Then, Chebychev's inequality states that \(|-\|u\|_{F}|^{2}\,/\) with probability at least \(1-\). Using the computation of the variance in eq. (8), it follows that \(\|u\|_{F}+\|f-c-g\|_{_{ }}/\) with probability at least \(1-\). Plugging this expression into eq. (2), we obtain the result. 

Note that the norm in \(_{}\) can be developed with (assuming for conciseness that \((-c)\) is comprised in the \(0\)-frequency of \(f\))

\[\|u\|_{_{}}^{2}=_{^{d}} _{}^{*}(_{}-2_{})}{ _{}}+\|g\|_{_{}}^{2}( \|f\|_{_{}}+\|g\|_{_{ }})^{2}.\] (10)

Thus, theorem 2 provides a certificate of \(f_{}\) as long as we can _(i)_ evaluate the Fourier transform \(_{}\) of \(g\) and _(ii)_ compute its Hilbert norm in some r.k \(_{}\) induced by \(_{}\). In next section, we detail the choice we make to achieve this efficiently, with kernels amenable to GPU computation, scaling to thousands of coefficients.

**Remark 2** (Using a RKHS norm instead of the \(F\)-norm).: _Note that since \((_{})_{}\) sums to \(1\), the associated kernel is bounded by \(1\). Hence \(\|u\|_{L_{}}\|u\|_{_{}}\), and the latter could be used instead of the \(F\)-norm in eq. (2). There are two reasons for taking our approach instead. Firstly, the \(F\)-norm is always tighter that a RKHS norm (see e.g.[Woodworth et al., 2022, Lem. 4]); secondly, we cannot compute \(\|u\|_{_{}}\) efficiently and have to rely instead on another upper bound. However, taking the number of samples \(N=O(\|u\|_{_{}}^{2})\) alleviates this issue._

Exponential concentration bounds with MoM.The scaling in \(1/\) in theorem 2 can be prohibitive if one requires a high probability on the result (\( 1\)). Hopefully, alternative estimator exist for those cases. The Median-of-Mean estimator is an example, illustrated in theorem 3.

**Theorem 3** (Certificate with MoM estimator).: _Let \((_{})_{}\) be a probability distribution on \(^{d}\), and \((0,1)\). Draw \(N>0\) frequencies \(_{i}_{}\). Define the MoM estimator with the following: for \(K\) s.t. \(=e^{-K/8}\) and \(N=Kb\), \(b 1\), write \(B_{1},,B_{K}\) a partition of \([N]\); then_

\[_{}(|_{_{i}}|/_{_{i}})=\{_{i B_{j}}_{_{i}}-c _{_{i}=0}-_{_{i}}|}{_{_{i}}} \}_{1 j K}.\] (11)

_A certificate on \(f_{}\) with probability \(1-\) follows, with_

\[f_{} c-_{}(|_{_{i}}|/_{ _{i}})-4\|f-c-g\|_{_{}}}.\] (12)

Proof.: Using _e.g._ Theorem 4.1 from Devroye et al. (2016) we get that the deviation of the MoM estimator from the expectation is bounded by

\[|\|u\|_{F}-_{}(|_{_{i}}|/ _{_{i}})| 4(|_{}|/ _{})}\ \ \ 1-.\] (13)

Using the upper bound on the variance with the \(_{}\) norm given in eq. (8) and plugging the resulting expression into eq. (2), we obtain the result.

To conclude this section, bounding the \(L_{}\) norm from above with the \(F\)-norm in eq. (3) enables to obtain a certificate on \(f\), as shown in theorem 1. The \(F\)-norm requires an infinite number of computation in the general case, but can be bounded efficiently with a probabilistic estimate, given by theorem 2 or theorem 3. This is summed up in fig. 1. Note that the difference \(_{F}-_{L_{}}\) is a source of conservatism in the certificate which we do not quantify - yet, the \(F\)-norm is optimal for a class of norms, see (Woodworth et al., 2022, Lemma 3).

## 3 Algorithm and implementation

### Bessel kernel

We now detail the specific choice of kernel we make in order to compute the certificate of theorem 2 or theorem 3 efficiently. Our first observation is to use a kernel stable by product, so that we can easily characterize a Hilbert space the model \(g\) belongs to. This restricts the choice to the exponential family. That's why we define, for a parameter \(s>0\),

\[ x,\ \ q_{s}(x)=e^{s((2 x)-1)}=_{ }e^{-s}I_{||}(s)e^{2 x},\] (14)

with \(I_{||}()\) the modified Bessel function of the first kind (Watson, 1922, p.181). Then, define \(K_{s}(x,y)=q_{s}(x-y)\) as in definition 2, and take a tensor product to extend the definition of \(K\) to multiple dimension, _i.e._\(K_{}(,)=_{=1}^{d}K_{_{}}( _{},_{})\) for any \(,^{d}\). We refer to this kernel as the _Bessel kernel_, and the associated RKHS as \(_{}\). It is stable by product as \(K_{}(x,y)=K_{/2}(x,y)^{2}\). This is key to compute the Fourier transform of the model \(g\), and in contrast to previous approaches which used the exponential kernel with \(_{} e^{-s||}\)Woodworth et al. (2022); Bach and Rudi (2023).

In the following, \(g\) is a K-SoS model defined as in definition 1, with the Bessel kernel of parameter \(_{+}^{d}\) defined in eq. (14).

**Lemma 1** (Fourier coefficient of the Bessel kernel).: _For \(^{d}\), the Fourier coefficient of \(g\) in \(\) can be computed in \(O(drm^{2})\) time with_

\[_{}=_{i,j=1}^{m}R_{i}^{}R_{j}_{=1}^{d}e^{-2 _{}}I_{|_{}|}(2s(_{i}- _{j}))e^{-_{}(_{i}+_{j })}.\] (15)

Proof.: From its definition in eq. (5), we rewrite \(g\) as

\[g(x)=_{i,j=1}^{m}R_{i}^{}R_{j}_{=1}^{d}K_{s}(x,_{i })K_{s}(x,_{j}).\] (16)

Now, from the definition of the Bessel kernel in eq. (14), we have that for any \((x,y,z)\), \(K(x,y)K(x,z)=e^{-2s}e^{2s(2(y-z)/2) 2(x-(y+z)/2)}\). By definition of the modified Bessel function, the Fourier coefficient of this expression are given by \(I_{||}(2s(2(y-z)/2))\). Using this into eq. (16), we get the result. 

The second necessary ingredient for using the certificate of theorem 2 is computing a RKHS norm for \(g\). It relies on the inclusion of \(_{2}\) into the bigger space of symmetric operator \((_{})\).

**Lemma 2** (Bound on the RKHS norm of \(g\)).: \(g\) _belongs to \(_{2}\), and \(\|g\|_{_{2}}\) is bounded by the Hilbert-Schmidt norm of \((_{})\), which can be computed in \(O(dm^{2}+mr^{2})\) time, with_

\[\|g\|_{_{2}}^{2}\|g\|_{(_{s})} ^{2}=\,(RK_{s,}R^{})^{2}.\] (17)

Proof.: Assume that \(d=1\); the reasoning can be extended to multiple dimensions with the tensor product. From the computation of the Fourier coefficient in lemma 1 and the fact that \(I_{||}(2s(2)) I_{||}(2s)\), we have that \(_{}=O(I_{||}(2s))\) hence \(g_{2s}\). Finally, since the kernel is stable by product, \(_{2s}=_{s}_{s}\), so we can use _e.g._(Paulsen and Raghupathi, 2016, Thm. 5.16), with \(_{1}=_{2}=_{s}\) and \((_{s})=_{s}_{s}\), with the operator \(A=((_{1}),,(_{m}))R^{}R(( _{1}),,(_{m}))^{*}(_ {s})\).

With lemma 2, we have that the model \(g\) belongs to \(_{2}\), so we will naturally use \(_{}=_{=1}^{d}e^{-2_{}}I_{}( _{})\) in theorem 2; said differently, the space \(_{}\) introduced in eq. (8) is simply \(_{2}\) defined in eq. (14).

### The algorithm: GloptiNets

We can now describe how GloptiNets yields a certificate on \(f\). The key observation is that no matter how is obtained our model \(g(R,)\) from definition 1, we will always be able to compute a certificate with theorems 2 and 3. Thus, even though optimizing eq. (9) w.r.t \((c,R,)\) is highly non-convex, we can use any optimization routine and check empirically its efficiency by looking at the certificate. Finally, thanks to its low-rank structure it is cheaper to evaluate \(g\) than evaluating its Fourier coefficient. This is formally shown in proposition 2 in appendix A, where a block-diagonal structure for the model is also introduced. That's why we first optimize \(_{c,g}c- f-c-g_{}\), where \(_{}\) is a proxy for the \(L_{}\) norm, _e.g._ the log-sum-exp on a random batch of \(N\) points1:

\[ f-c-g_{L_{}}_{i[N]}|f(x_{i})- c-g(x_{i})|(f(x_{i})-c-g(x_{i}))_{i[N]}.\] (18)

This optimization can be carried out by any deep learning libraries with automatic differentiation and any flavour of gradient ascent. Only afterwards do we compute the certificate with theorems 2 and 3. This procedure is summed up in alg. 1.

``` Data: A trigonometric polynomial \(f\), a candidate \(z\) s.t. \(c=f(z)\), a model \(g\), and a probability \(\). Result: A certificate \( f_{}-f(z)_{}\) with proba. \(1-\). /* Optimize \(g\) with function values forepoch =1:nepochsdo  Sample \(x_{1},,x_{N}^{d}\) ; \(L, L=((f(x_{i})-c-g(x_{i}))_{i [N]})\) ; \(,R( L)\) ; /* Compute a certificate */ \(_{}\): probability distribution on \(^{d}\) with \(_{}=_{=1}^{d}e^{-2_{}}I_{} (_{})\);  Sample \(=(_{1},,_{N})_{}\) ;  Compute \(M=_{}(_{_{i}}-c_{ _{i}=0}-_{_{i}}/_{_{i}})_{i [n]}\) and \(= g_{(_{})}\);  Returns \(_{}=c-M-4}}{{N}}}\); ```

**Algorithm 1**GloptiNets

**Remark 3** (Providing a candidate).: _In alg. 1, a candidate estimate \(c\) for the minimum value \(f(x_{})\) is necessary. However, it is possible to overcome this requirement by incorporating \(c\) as a learnable parameter within the training loop. Moreover, \(x_{}\) can be learned using techniques similar to those in Rudi et al. (2020): by replacing the lower bound \(c\) with a parabola centered at \(z\), \(z\) becomes a candidate for \(x_{}\) with precision corresponding to the tightness of the certificate. Note however that this method introduces additional hyperparameters._

### Specific implementation for the Chebychev basis

As already observed in Bach and Rudi (2023), a result on trigonometric polynomial on \(^{d}\) directly extends to a real polynomials on \([-1,1]^{d}\). The reason for that is that minimizing \(h\) on \([-1,1]^{d}\) amounts to minimizing the trigonometric polynomial \(f=h(( 2 x_{1},, 2 x_{d}))\) on \(^{d}\). Note however that \(f\) is an even function in all dimension, as for any \(x^{d}\), \(f(x)=f(x_{1},,-x_{i},,x_{d})\). Thus, approximating \(f-f_{}\) with a K-SoS model of definition 1 is suboptimal, in the sense that we could approximate \(f\) only on \([0,1/2]^{d}\), which is \(2^{-d}\) smaller. Put differently, the Fourier coefficient of \(f\) are real by design: it would be convenient to enforce this structure in the model \(g\). This is achieved with proposition 1.

**Proposition 1** (Kernel defined on the Chebychev basis).: _Let \(q\) be a real, even function on the torus, bounded by \(1\), as in eq. (6). Let \(K\) be the kernel defined on \([-1,1]\) by_

\[(u,v)(0,}{{2}}),K( 2 u, 2 v)=(q(u+v )+q(u-v)).\] (19)

_Then \(K\) is a symmetric, p.d., hence reproducing kernel, bounded by \(1\), with explicit feature map given by_

\[(x,y)[-1,1],K(x,y)=_{0}+_{}2 _{}H_{}(x)H_{}(y).\] (20)

The proof is available in appendix B. It simply relies on expanding the definition of \(K\) in eq. (19). The resulting expression in eq. (20) exhibits only cosine terms (in the decomposition of \(x K( 2 x,y)\)). This enables to directly extend the PSD models from definition 1 with such kernels. Finally, when used with the Bessel kernel of eq. (14), we recover an easy computation of the Chebychev coefficient, as shown in lemma 3, in \(O(drm^{2})\) time. This enables to approximate any function expressed on the Chebychev basis. Note that polynomials expressed in other basis can be certified too, by first operating a change of basis.

## 4 Experiments

The code to reproduce these experiments is available at

github.com/gaspardbb/GloptiNets.jl

Settings.Given a function \(h\), we compute a candidate \(\) with gradient descent and multiple initializations. The goal is then to certify that \(\) is indeed a global minimizer of \(h\). This is a common setup in the Polynomial-SoS literature Wang and Magron (2022). To illustrate the influence of the number of parameters, the positive model \(g\) defined in definition 1 for GloptiNets designates either a small model GN-small with 1792 parameters, or a bigger model GN-big with \(22528\) parameters. The latter should have higher expressivity and better interpolate positive functions, leading to tighter certificates. All results for GloptiNets are obtained with confidence \(1-=1-e^{-4} 98\%\). All other details regarding the experiments are reported in appendix C.

Polynomials.We first consider the case where \(h\) is a random trigonometric polynomial. Note that this is a restrictive analysis, as GloptiNets can handle any smooth functions (_i.e._ with infinite non-zero Fourier coefficients). Polynomials have various dimension \(d\), degree \(p\), number of coefficients \(n\), but a constant RKHS norm \(_{2_{d}}\). We compare the performances of GloptiNets to TSSOS, in its complex polynomial variant Wang and Magron (2022). The latter is used with parameters such that it executes the fastest, but without guarantees of convergence to the global minimum \(f_{}\). Table 1 shows the certificates \(h(x_{})-h()\) and the execution times (lower is better, \(t\) in seconds) for TSSOS, GN-small and GN-big. Figure 2 provides certificate on a random polynomial, function of the number of parameters in \(g\).

Kernel mixtures.While polynomials provide ground for comparison with existing work, GloptiNets is not confined to this function class. This is evidenced by experiments on kernel mixtures, where our approach stands as the only viable alternative we are aware of. The function we certify are of the form \(h(x)=_{i=1}^{n}_{i}K(x_{i},x)\), where \(K\) is the Bessel kernel of eq. (14). Kernel mixtures are ubiquitous in machine learning and arise _e.g._ when performing kernel ridge regression. Certificates obtained on mixtures are compared with those obtained on polynomials in fig. 2, function of the model size \(g\).

Figure 2: Certificate _vs._ number of parameters in \(g\), for a given function \(h\). The higher the RKHS norm of \(h\), the more difficult it is to approximate uniformly and the looser the certificate, independently of the function type. The more parameters in the k-SoS model, the tighter the certificates obtained with theorem 3.

Results.There are two key hindsight about the performances of GloptiNets. Firstly, its certificate _does not depend on the structure_ of the function to optimize. Thus, although GloptiNets does not match the performances of TSSOS on small polynomials, it can tackle polynomials which cannot be handled by competitors, with arbitrarily as many coefficients (\(n=\)). For instance, TSSOS cannot handle problems with \(n\{489,833\}\) in table 1. More importantly, GloptiNets can certify a richer class of functions than polynomials, among which kernel mixtures. The performances of GloptiNets mostly depends on the complexity of the function to certify, as measured with its RKHS norm.

Secondly, note that _a bigger model yields tighter certificate_. This is detailed in fig. 2, where the same function \(f\) is optimized with various models. The dependency of the certificate on the norm of \(f\) is shown in fig. 3 in appendix C, along with experiments with Chebychev polynomials.

## 5 Limitations

One limitation of GloptiNets is the trade-off resulting from its high flexibility for obtaining a certificate as in alg. 1. While this flexibility offers numerous advantages, it also introduces the need for an extensive hyperparameter search. Although we have identified a set of hyperparameters that align with deep learning practices - utilizing a Momentum optimizer with cosine decay and a large initial learning rate - the optimal settings may vary depending on the specific problem at hand.

In the same vein, the certificates given by GloptiNets are of moderate accuracy. While adding more parameters into the k-SoS model certainly helps (as shown in fig. 2), alternative optimization scheme to interpolate \(h-h()\) with \(g\) might provide easier improvement. For instance, we found that using approximate second-order scheme in alg. 1 is key to obtaining good certificates.

In the specific settings of polynomial optimization, we highlight that our model is not competitive on problems which exhibits some algebraic structure, as for instance term sparsity or the constant trace property. Typically, problems with coefficients of low degrees (less or equal than \(2\)), which encompass notably the OPF problem, are really well handled by the family of solvers TSSOS belongs to. Finally, GloptiNets does not handle constraints yet.

## 6 Conclusion

The GloptiNets algorithm presented in this work lays the foundation for a new family of solvers which provide certificates to non-convex problems. While our approach does not aim to replace the well-established Lasserre's hierarchy for sparse polynomials, it offers a fresh perspective on tackling a new set of problems at scale. Through demonstrations on synthetic examples, we have showcased the potential of our approach. Further research directions include extensive parameter tuning to obtain tighter certificates, with the possibility of leveraging second-order optimization schemes, along with warm-restart schemes for application which requires solving multiple similar problems sequentially.

    &  &  &  &  &  \\   & & & Certif. & \(t\) & Certif. & \(t\) & Certif. & \(t\) \\   & \(5\) & \(85\) & \(5.3 10^{-11}\) & \(3\) & \(8.35 10^{-4}\) & \(6 10^{3}\) & \(2.64 10^{-4}\) & \(9 10^{3}\) \\  & \(7\) & \(231\) & \(4.7 10^{-13}\) & \(120\) & \(9.51 10^{-4}\) & \(6 10^{3}\) & \(2.90 10^{-4}\) & \(9 10^{3}\) \\  & \(9\) & \(489\) & out of memory! & - & \(1.18 10^{-3}\) & \(6 10^{3}\) & \(3.34 10^{-4}\) & \(9 10^{3}\) \\   & \(3\) & \(33\) & \(3.1 10^{-10}\) & \(0.1\) & \(2.46 10^{-2}\) & \(1 10^{4}\) & \(3.45 10^{-3}\) & \(2 10^{4}\) \\  & \(5\) & \(225\) & \(4.8 10^{-12}\) & \(53\) & \(3.71 10^{-2}\) & \(1 10^{4}\) & \(3.59 10^{-3}\) & \(2 10^{4}\) \\   & \(7\) & \(833\) & out of memory! & - & \(4.76 10^{-2}\) & \(1 10^{4}\) & \(4.85 10^{-3}\) & \(2 10^{4}\) \\   

Table 1: GloptiNets and TSSOS on random trigonometric polynomials. While TSSOS provides machine-precision certificates, its running time grows exponentially with the problem size, and eventually fails on problems 3 and 6. On the other hand, GloptiNets has constant running time no the problem size, and its certificates can be tightened by increasing the model size.

Acknowledgments.AR acknowleges support of the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). AR acknowledges support of the European Research Council (grant REAL 947908). JM was supported by the ERC grant number 101087696 (APHE-LAIA project) and by ANR 3IA MIAI@Grenoble Alpe (ANR-19-P3IA-0003).