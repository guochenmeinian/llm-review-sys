# SciCode: A Research Coding Benchmark

Curated by Scientists

 Minyang Tian\({}^{1,2*{}}\), Luyu Gao\({}^{3*}\), Shizhuo Dylan Zhang\({}^{1}\), Xinnan Chen\({}^{1{}}\), Cunwei Fan\({}^{1{}}\),

**Xuefei Guo\({}^{1{}}\), Roland Haas\({}^{1{}}\), Pan Ji\({}^{4{}}\), Kittithat Krongchon\({}^{1{}}\),Yao Li\({}^{1{}}\), Shengyan Liu\({}^{1{}}\), Di Luo\({}^{5,6,11{}}\),Yutao Ma\({}^{7{}}\), Hao Tong\({}^{1{}}\), Kha Trinh\({}^{7{}}\), Chenyu Tian\({}^{8{}}\), Zihan Wang\({}^{1{}}\), Bohao Wu\({}^{1{}}\), Yanyu Xiong\({}^{9{}}\), Shengzhu Yin\({}^{1{}}\), Minhui Zhu\({}^{1{}}\), Kilian Lierei\({}^{10}\), Yanxin Lu\({}^{1}\), Genglin Liu\({}^{1}\), Yufeng Du\({}^{1}\), Tianhua Tao\({}^{1}\), Offr Press\({}^{10}\), Jamie Callan\({}^{3}\), Eliu Huerta\({}^{1,2,7{}}\), Hao Peng\({}^{1{}}\)\({}^{1}\)University of Illinois Urbana-Champaign \({}^{2}\)Argonne National Laboratory

\({}^{3}\)Carnegie Mellon University \({}^{4}\)University of North Carolina at Chapel Hill

\({}^{5}\)Massachusetts Institute of Technology \({}^{6}\)Harvard University \({}^{7}\) University of Chicago

\({}^{8}\)University of Texas at Austin \({}^{9}\)Stanford University \({}^{10}\) Princeton University

\({}^{11}\)The NSF AI Institute for Artificial Intelligence and Fundamental Interactions

\({}^{*}\) Equal contribution lead authors. \({}^{{}}\) Data curation, alphabetical order.

\({}\) Correspondence to {mtian8, haopeng}@illinois.edu, elihu@anl.gov

###### Abstract

Since language models (LMs) now outperform average humans on many challenging tasks, it is becoming increasingly difficult to develop challenging, high-quality, and realistic evaluations. We address this by examining LM capabilities to generate code for solving real scientific research problems. Incorporating input from scientists and AI researchers in 16 diverse natural science sub-fields, including mathematics, physics, chemistry, biology, and materials science, we create a scientist-curated coding benchmark, **SciCode**. The problems naturally factorize into multiple subproblems, each involving knowledge recall, reasoning, and code synthesis. In total, SciCode contains 338 subproblems decomposed from 80 challenging main problems, and it offers optional descriptions specifying useful scientific background information and scientist-annotated gold-standard solutions and test cases for evaluation. OpenAI o1-preview, the best-performing model among those tested, can solve only 7.7% of the problems in the most realistic setting. We believe that SciCode demonstrates both contemporary LMs' progress towards realizing helpful scientific assistants and sheds light on the building and evaluation of scientific AI in the future. 1

## 1 Introduction

The development of evaluations in tandem with language models (LMs) has substantially contributed to the rapid advancement of these models . Because LMs now surpass the performance of most humans except domain experts, evaluating them becomes increasingly challenging. Many established benchmarks struggle to keep pace with the advancements in LM performance and have quickly become saturated , leading to discrepancies between the models' perceived and actual capabilities . As a consequence, researchers are developing synthetic challenging benchmarks, often involving models in the construction of evaluation instances. For example, some subsample instances from existing benchmarks that cannot be solved by current models , or augment them to construct more challenging evaluations . However, it is unclear whether such efforts accurately reflect real-world applications and the models' performance in practical scenarios. Realistic, high-quality, and challenging evaluations are crucial for the continued advancement of LMs.

We therefore propose SciCode, a benchmark containing code generation problems drawn from diverse natural science fields, including mathematics, physics, chemistry, biology, and materials science. SciCode contains 80 main problems, each decomposed into multiple subproblems, totaling 338. Each problem provides the scientific background when necessary as well as detailed instructions. To solve it, the model must implement multiple Python functions--one for each subproblem--and then integrate them into a complete solution for the main problem. For every main problem and subproblem, SciCode provides gold-standard solutions and multiple test cases, facilitating easy and reliable automatic evaluation. Figure 1 shows an example.

SciCode aims to overcome the challenges of current LM evaluations by introducing the following value-added design choices.

* _Intentional focus on natural science fields_, such as computational mechanics, quantum information and computing, quantum chemistry, ecology, and molecular modeling.
* _Abundant high-quality data not usually made available to current LMs_, enabling a more robust evaluation of the models' ability to generalize to less familiar scenarios.
* _High annotation quality_, with all problems, including gold solutions and test cases, annotated, revised, and verified by at least two senior researchers (PhD student level or above) in represented scientific domains.
* _Realistic and current problems_ sourced from scientists' everyday research tasks or influential papers. This ensures SciCode's relevance to real-world applications.
* _Problems curated to have zero overlap with publicly available datasets_ to prevent potential data contamination.2 * _Problems that test LM's comprehensive and all-around capabilities_. Solving the main problems requires deep scientific background knowledge, strong analytical capabilities to decompose complex problems into simpler ones and correctly solve each, and the ability to integrate partial into complete solutions.
* _Opportunities to evaluate various model capabilities in varied setups_ by toggling options, e.g., whether to provide scientific background information or to condition on gold or generated solutions to previous subproblems.

Figure 1: A SciCode main problem is decomposed into multiple smaller and easier subproblems. Docstrings specify the requirements and input-output formats. When necessary, scientific background knowledge is provided, written by our scientist annotators. The full problem is shown in subsection A.3

Further, we believe that the availability of this well-designed benchmark can _motivate research into developing new AI methods for accelerating scientific research_, an area that has thus far benefited less from recent LM advancements partly due to a lack of commercial incentive.

We use SciCode to evaluate state-of-the-art proprietary and open models. Results show that SciCode is a very challenging benchmark: in the most realist evaluation setup, Claude3.5-Sonnet, the best-performing model in our experiments, can solve only 4.6% of the main problems, while other strong models, such as Claude3-Opus and GPT-4o, solve only 1.5%. Similarly, the best open source model under test, Deepseek-Coder-v2, can only solve 3.1% of the problems. The other open-source LLMs under test (e.g., Llama-3-70B-Instruct and Mistral-8x22B-Inst) fail to complete any problems despite successfully solving some subproblems correctly. Our analysis finds that all models can benefit from the background knowledge written by our scientist annotators, achieving substantial and consistent improvements. However, even with background, the best model can solve only 12.3% of the main problems.

## 2 SciCode

This section examines the design principles and annotation process we chose for SciCode, describing: research-level coding problems from various natural science fields (SS2.1); how we decomposed main problems into multiple, simpler subproblems (SS2.2); our design choices for the annotation process (SS2.3); and various evaluation setups that SciCode facilitates (SS2.4).

### Challenging and Realistic Scientific Coding Problems

SciCode sources challenging and realistic research-level coding problems across natural science disciplines, including mathematics, physics, chemistry, biology, and material science, covering a total of 16 subfields. This diverse selection ensures a comprehensive representation of the natural sciences, where extensive code development is essential.

SciCode is mainly drawn from the scripts that scientists use in their everyday workflow. Many of these have been used in one or more publications, demonstrating their robustness and correctness. However, they are primarily for internal use, which means that they are seldomly open-sourced and often poorly annotated. Consequently, unlike general-domain coding problems, natural science problems have less exposure in most current LMs' training data. This offers a unique opportunity to evaluate the models' ability to generalize to less familiar contexts. In total, SciCode consists of 80 main problems, decomposed into 338 subproblems.

Table 1 lists the subfields SciCode covers along with the number of main problems in each. Each main problem has a median of 3 subproblems, with a maximum of 15. We reserve 15 main problems (50 subproblems) for the development split and use the remaining 65 main problems (288 subproblems) as the test data. The 15 main development problems cover all five domains; over half of these have less than 4 subproblems each for easier few-shot settings.

### A Main Problem with Multiple Subproblems

In their everyday workflow, scientists often decompose a complex problem into multiple smaller, more manageable parts. They may write relatively independent code for each part and then integrate these parts into a complete solution to the main problem. In developing our dataset, we leverage

  
**Fields** & **Subfields** \\  Mathematics & Numerical linear Algebra (8), Computational Mechanics (5), Computational Finance (1) \\  Physics & Condensed Matter Physics (13), Optics (10), Quantum Information/Computing (6), \\  & Computational Physics (5), Astrophysics (2), Particle Physics (1) \\  Chemistry & Quantum Chemistry (5), Computational Chemistry (3) \\  Biology & Ecology (6), Biochemistry (1), Genetics (1) \\  Material Science & Semiconductor Materials (7), Molecular Modeling (6) \\   

Table 1: SciCode fields and subfields, with the number of main problems in each.

this natural and intuitive structure and further standardize our dataset by instructing the scientists to adhere to the following format.

Main ProblemA _main problem_ is a primary task that needs to be addressed. It defines the overall objective of the research and guides the direction of the study. The main problem encompasses all subproblems, with detailed instructions on required inputs and expected outputs articulated in a docstring block. With the main problem defined, scientists have sufficient guidance to solve the task.

Subproblem Decomposition_Subproblems_ focus on questions derived from the main problem. They decompose the complex main problem into smaller, more manageable parts, enabling a more detailed and systematic investigation. Detailed docstrings for each subproblem describe the required input and expected output, ensuring clarity and aiding in accurate code generation. This structured decomposition simplifies problem-solving and facilitates a more granular evaluation of the models' scientific coding capabilities.

### Data Annotation

This process consists of three main stages:

1. **Problem selection:** Deciding on question topics related to the research domain (SS2.3.1).
2. **Evaluation design:** Designing both numerical and domain-specific test cases to ensure the problem's validity (SS2.3.2).
3. **Problem validation:** Iterating on the problems through three rounds of revisions to further enhance question design (SS2.3.3).

We now examine the design choices for each stage.

#### 2.3.1 Problem Selection

Throughout the research project cycle, various coding needs arise, such as data processing, fitting, and plotting. To use SciCode, scientists select the problems that require intense scientific knowledge and reasoning to optimally test LM's science capability. This approach ensures that both the breadth and depth of frontier research are addressed. We focus on:

* **Numerical methods.** Analytical forms are usually impossible to achieve for very complicated systems. Therefore, scientists must derive numerical models and algorithms that describe physical phenomena , chemical reactions , biological systems , or statistical behaviors.
* **Simulation of systems.** In fields of natural science, scientists write code to simulate systems and processes. These simulations are based on theoretical principles and empirical data,

Figure 2: Distributions of (a) main problems and (b) subproblems.

reflecting deep scientific insights into the system being studied [78; 101; 63; 57; 75; 21; 92; 100; 39; 70; 42; 60; 73].
* **Scientific calculation.** During data post-processing and visualization, scientists often perform many transformations based on scientific formulas to get physical observable of interest instead of raw experimental data [11; 90; 31; 40; 41; 69; 6; 32].

We also include several research problems that are built upon or reproduce methods used in Nobel Prize-winning studies to highlight current trends in scientific research: the self-consistent field (SCF) method for density functional theory (DFT) calculations  (**The Nobel Prize in Chemistry 1998**), the PMNS matrix for neutrino oscillation in matter [55; 62] (**The Nobel Prize in Physics 2015**), the Haldane model for the anomalous quantum Hall effect  (**The Nobel Prize in Physics 2016**), optical tweezer [47; 7] simulations for microscopic thermodynamics [51; 25; 48] (**The Nobel Prize in Physics 2018**), and the replica method for spin glasses [81; 71; 58; 56] (**The Nobel Prize in Physics 2021**).

#### 2.3.2 Evaluation Design

To facilitate evaluation, we have scientist annotators use only widely adopted and well-documented packages such as NumPy, SciPy, and SymPy when writing the solution code for their problems, as shown in Figure 4.

Our test suite involves two key components. (1) **Numerical tests** list input-output pairs to check if the generated code produces the same outputs as ground truth. (2) **Domain-specific test cases**, introduced as an additional stage, evaluate whether model-generated solutions align with scientists' practical needs and further ensure the correctness and applicability of each solution within its specific field. These tests are extracted from real scientific workflows: scientists must design domain-specific test cases to verify code accuracy by reproducing results published in academic papers or matching analytical solutions derived from theoretical models. For example, we reproduce the phase transition at around \(kT/J=2.269\) for the 2D square Ising model problem , derive the surface plasmon mode in a 2D layered electron gas [11; 33], verify the ballistic Brownian motion in optical tweezer , etc. By doing so, we validate that the code not only functions correctly but also accurately represents the underlying scientific problem.

Overall, the evaluation design aims to balance the fidelity of the scientific problem with the practicality of the evaluation process, ensuring that the solutions are both accurate and accessible.

#### 2.3.3 Problem Validation for Quality Control

We conduct three rounds of validation and revision for each problem:

1. **In-domain scientist validation.** At least two scientists in the same research domain cross-check the _question design_, _solution code_, and _domain-specific test cases_, providing detailed feedback. The scientists who design the workflows iterate on them based on this feedback to ensure the problems are scientifically accurate.
2. **Out-of-domain scientist validation.** One scientist from a different domain reviews the _question design_ to ensure it is clear and that the information provided is precise and sufficient to solve the problem (e.g., all scientific constants are given). This helps to identify any assumptions that might be unclear to those outside the immediate field of study.
3. **GPT-4 validation.** GPT-4 assists with the final review round. The previously validated sub-questions are input to GPT-4 to generate code solutions. Scientists perform error analysis for the generated solutions and redesign the _numerical test cases_ if necessary to prevent false positives.Based on the code solutions from GPT-4, the scientist may also revise the _entire workflow_ a third time to addressany potential ambiguity.

This multi-round validation approach ensures that the problems are scientifically rigorous, clear, and unambiguous, facilitating accurate and effective evaluation.

### Various Types of Evaluations

SciCode offers unique opportunities for evaluating LMs across diverse settings, comprehensively testing their coding capabilities.

* **Without vs. with scientific background.** A subproblem can provide scientific background knowledge to guide LMs in solving the coding task. SciCode's scientific background for each problem offers two modes of evaluation. (1) When models are evaluated _without_ scientific background, it tests their inherent scientific knowledge and reasoning along with their coding capability. (2) For models not designed to handle scientific problems, background provides the necessary knowledge and reasoning steps to solve the problems, shifting the evaluation's focus towards the models' coding and instruction-following capabilities. As we show in the experiments (SS3), all models substantially improve performance when background is provided, indicating their lack of knowledge and reasoning capability in these natural science fields.
* **Gold vs. generated solutions to previous subproblems.** Each main problem in SciCode factorizes into multiple subproblems, and solutions to previous problems provide vital information for solving the current one. SciCode enables use of _gold_ or _generated_ solutions to previous subproblems. Gold solutions focus only on the current problem, while generated ones provide a more realistic evaluation setting and are more challenging due to error accumulation.
* **Main vs. subproblem levels.** (1) The LM is considered to have successfully solved the main problem when all subproblem solutions are correct and the integrated solution to the main problem is correct. (2) Alternatively, SciCode can assess at a subproblem level, evaluating a subproblem independently of other subproblems or its main problem.

Among these setups, evaluation _without background_ carrying over _generated_ solutions to previous problems is the closest to scientists' real use case of LMs. Therefore, we dub this the _standard setup_. Our experiments indicate that this setup is very challenging for even the best models available today: Claude3.5-Sonnet, the best performing one, can solve only 4.6% of the main problems.

To make SciCode useful for evaluating less capable or developing models, we also consider less challenging settings in our experiments.

## 3 Experiments

Prompts.We evaluate our model using zero-shot prompts. We keep the prompts general and design different ones for different evaluation setups only to inform the model about the tasks. We keep prompts the same across models and fields, and they contain the model's main and sub-problem instructions and code for previous subproblems. We also instruct the model to recall useful knowledge when gold background knowledge is not provided. SSA.1 presents an example.

### Evaluated Models

Since SciCode is a challenging benchmark, we mainly consider strong language models.3

* **OpenAI o1-preview**: A new OpenAI model designed to spend more time thinking before they respond
* **OpenAI o1-mini**: An efficient version of OpenAI o1-preview
* **GPT-4o**: An optimized version of GPT-4  by OpenAI with multi-modal capability.
* **GPT-4-Turbo**: A faster and more cost-effective variant of GPT-4 . We use the 'gpt-4-turbo-2024-04-09' snapshot.
* **Claude3.5-Sonnet (new)**: The upgraded model (20241022) from Claude3.5-Sonnet.
* **Claude3.5-Sonnet**: The latest model from the Claude 3.5 family from Anthropic.
* **Claude3-Opus**: The most capable model from the Claude 3 family from Anthropic.
* **Claude3-Sonnet**: The second most capable model from the Claude 3 family.
* **Gemini 1.5 Pro**: A model from the Gemini 1.5 family by Google and the largest with open access at the time of writing.
* **Llama-3-70B-Instruct**: The instruction-tuned version of the largest available model from the Llama-3 family.
* **Llama-3.1-70B-Instruct**: The instruction-tuned version of the largest available model from the Llama-3 family.

* **Llama-3.1-405B-Instruct**: The instruction-tuned version of the largest available model from the Llama-3 family.
* **Mixtral-8x22B-Instruct**: The instruction-tuned version of Mistral AI's largest publicly accessible Mixture-of-Expert Model.
* **Deepseek-Coder-v2**: Mixture-of-Experts (MoE) code language model continue pre-trained on DeepSeek-V2
* **Qwen2-72B-Instruct**: The largest instruction-tuned Qwen-2 model.

### Main Results

Table 2 presents results under the _standard setup_.4 For the easier subproblem-level evaluation, the state-of-the-art models we test solve 14%-28.5% of the subproblems. Among them, OpenAI o1-preview achieves the best performance, with a 28.5% pass@1 rate. However, all models perform much worse on the more realistic and challenging main problem evaluation. OpenAI o1-preview still performs the best in this setting, but with only a 7.7% pass@1 rate.

These results show that SciCode is a difficult benchmark for current LMs. Consistent with our observations on proprietary models, open-weight LMs under test also showed their lack of capabilities in solving any main problem despite being able to solve a number of sub-problems correctly.

### Additional Results with Other Evaluation Settings

Providing gold scientific background knowledge.Table 3 presents results when background text authored by scientists is provided to the LMs and generated solutions to previous subproblems are used. This setting evaluates both the models' capabilities to faithfully follow the instructions provided in the background as well as their code-generation performance. The \(\) columns indicate performance differences compared to the standard setup.

  
**Models** & **Main Problem** & **Subproblem** \\  _Proprietary Models_ & & \\ OpenAI o1-preview & **7.7** & **28.5** \\  Claude3.5-Sonnet & 4.6 & 26.0 \\  Claude3.5-Sonnet (new) & 4.6 & 25.3 \\  GPT-4o & 1.5 & 25.0 \\  GPT-4-Turbo & 1.5 & 22.9 \\  OpenAI o1-mini & 1.5 & 22.2 \\  Gemini 1.5 Pro & 1.5 & 21.9 \\  Claude3-Opus & 1.5 & 21.5 \\  Claude3-Sonnet & 1.5 & 17.0 \\  _Open Models_ & & \\ Deepseek-Coder-v2 & 3.1 & 21.2 \\  Llama-3.1-405B-Chat & 1.5 & 19.8 \\  Qwen2-72B-Instruct & 1.5 & 17.0 \\  Llama-3.1-70B-Chat & 0.0 & 17.0 \\  Mixtral-8x22B-Instruct & 0.0 & 16.3 \\  Llama-3-70B-Chat & 0.0 & 14.6 \\   

Table 2: Model performance in pass@1 rate on SciCode under the standard setup: without background knowledge and carrying over generated solutions to previous subproblems.

All models substantially improve performance for both subproblem and main problem evaluations when given scientific background knowledge.For the subproblem evaluation, Claude3.5-Sonnet (new) performs the best with a 37.2% pass@1 rate. Llama-3.1-405B-Instruct benefits the most from the provided scientific background and reasoning with an increase of 13.2 %. However, Open models still improves less compared to proprietary models which might indicate weaker Instruction following capability. Interestingly, the comparison between Llama-3-70B-Instruct and Mixral-8x22B-Instruct reveals a trend that differs from the standard setup: Llama-3-70B-Instruct benefits more from the scientific background knowledge and reaches the performance of Mixral-8x22B-Instruct in this setting.

For the main problem evaluation, the trend remains similar to the standard setup. OpenAI o1-mini performs best, with a 13.8% pass@1 rate, followed closely by Claude3.5-Sonnet at 12.3%. OpenAI o1-mini improves most from background content, at 12.3%. Nonetheless, all models still fall short of satisfactory performance even with the background knowledge provided. This reaffirms that SciCode is challenging even when focusing on code generation rather than testing the models' scientific knowledge.

With gold subproblem solutions.Figure 3 plots the subproblem pass@1 rates conditioning on various numbers of previous subproblems and their gold solutions. Background knowledge is _not_ provided. The intuition behind this analysis is that later steps can leverage gold solutions from previous steps to gain a richer understanding of the problem. Instructions and solutions from earlier steps serve as in-context demonstrations, enabling the model to rely less on its instruction-following capability. By focusing on later steps, we can more precisely assess the models' inherent capabilities.

_Overall, all three models show similar trends, with their performance generally improving as they condition on more gold solutions from previous steps_. However, there is a notable exception when conditioning on 7 previous gold subproblem solutions. Additionally, performance starts to decline when models condition on more than 9 previous solutions, possibly due to the increased difficulty of managing long contexts.

    &  &  \\  & **Pass@1** & \(\) & **Pass@1** & \(\) \\  _Proprietary Models_ & & & & \\ OpenAI o1-mini & **13.8** & **12.3** & 34.4 & 12.2 \\  Claude3.5-Sonnet & 12.3 & 7.7 & 35.4 & 9.4 \\  Claude3.5-Sonnet (new) & 10.8 & 6.4 & **37.2** & 12.1 \\  OpenAI o1-preview & 10.8 & 3.1 & 34.0 & 5.5 \\  GPT-4o & 9.2 & 7.7 & 35.4 & 10.4 \\  GPT-4-Turbo-2024-04-09 & 9.2 & 7.7 & 33.7 & 10.8 \\  Gemini 1.5 Pro & 7.7 & 6.2 & 30.6 & 8.7 \\  Claude3-Opus & 4.7 & 3.0 & 26.7 & 5.2 \\  Claude3-Sonnet & 4.7 & 3.0 & 25.7 & 8.7 \\  _Open Models_ & & & & \\ Llama-3.1-405B-Instruct & 10.8 & 9.3 & 33.0 & **13.2** \\  Deepseek-Coder-v2 & 4.6 & 1.5 & 27.1 & 5.9 \\  Llama-3.1-70B-Instruct & 4.6 & 4.6 & 25.7 & 8.7 \\  Qwen2-72B-Instruct & 4.6 & 3.1 & 22.2 & 5.2 \\  Mixral-8x22B-Instruct & 3.1 & 3.1 & 20.8 & 4.5 \\  Llama-3-70B-Instruct & 1.5 & 1.5 & 20.8 & 6.3 \\   

Table 3: Pass@1 with generated solutions for previous subproblems and scientific background texts provided. The \(\) columns show the performance differences compared to the _standard setting_ in Table 2, i.e., where background content is _not_ provided.

## 4 Related Work

Language models for code.Code has long been an active field of research, and code LMs have co-evolved with foundation LMs since the era of BERT . Earlier works include CodeBert  and CodeT5 , while Codex  arguably kick-started the LLM era for code-generation models. Since Codex, the field has experienced rapid growth in quantity and quality of large code generation models, including specially trained models like Codegen , StarCoder models [46; 53], and generalist models with code adapation  such as CodeLlama , CodeQwen , and DeepSeek-Coder . As code generation gains more attention and becomes increasingly useful, contemporary generalist models often include non-trivial coding capabilities [68; 87].

Evaluating code generation.Before the emergence of very capable code synthesis models, when most models struggled to produce executable code, datasets like CoNaLa typically included n-gram-based metrics . Soon after model capabilities improved, execution-based evaluation gained in popularity [29; 8; 12]. While n-gram or general text-based evaluation still exists, we opted to omit them from SciCode due to obvious limitations of surface form matching in scientific coding.

Code generation benchmarks now take various forms. For simple function completion, MBPP  and HumanEval  are two widely used benchmarks that contain basic programming questions, mainly evaluating LMs' ability to turn natural language instructions into Python programs. Other benchmarks assess the models' competence in real-world programming scenarios, such as writing data science code [43; 103], repository-level code completion , and more complex tasks in real-world software engineering . Though our work is similar to MTPB  in terms of using a multi-turn setup, our subproblem instructions correspond to a high-level task, while theirs correspond to specific code actions (e.g., replace X with Y in the string).

Language models for science.Scientific tasks are complex due to their demands for reasoning and knowledge. However, Recent advances in general and specialized language models have revolutionized the processing of text and other data modalities, such as molecules and proteins, in scientific fields. Galactica , a general-purpose scientific model, can perform tasks like citation prediction, scientific reasoning, document generation, and molecular property prediction. Many models focus on one single domain or task, like math (e.g., Minerva  and Deepseek-Math  ), protein structure prediction (e.g., ESM-2 ), medical reasoning (e.g., Med-PaLM , BioGPT ), and others.

Figure 3: Subproblem pass@1 rate when conditioning on various numbers of previous subproblems and their gold solutions. Background knowledge is _not_ provided.

Conclusion

We introduce SciCode, a scientific research benchmark curated by professional natural scientists. We designed SciCode for scientific problem evaluation and collected problems representing 16 diverse domains. By assessing SciCode with ten contemporary state-of-the-art AI models, we demonstrated that our benchmark is within reach but remains very challenging. We believe SciCode will serve as a helpful guideline for building future code language models for varied scientific applications.