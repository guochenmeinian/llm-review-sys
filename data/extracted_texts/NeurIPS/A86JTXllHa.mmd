# On the Importance of Feature Separability

in Predicting Out-Of-Distribution Error

 Renchunzi Xie\({}^{1}\) Hongxin Wei\({}^{2}\)1 Lei Feng\({}^{1}\) Yuzhou Cao\({}^{1}\) Bo An\({}^{1}\)

\({}^{1}\) School of Computer Science and Engineering, Nanyang Technological University, Singapore

\({}^{2}\) Department of Statistics and Data Science, Southern University of Science and Technology, China

{xier0002,yuzhou002}@e.ntu.edu.sg

weihx@sustech.edu.cn

lfengaqq@gmail.com

boan@ntu.edu.sg

###### Abstract

Estimating the generalization performance is practically challenging on out-of-distribution (OOD) data without ground-truth labels. While previous methods emphasize the connection between distribution difference and OOD accuracy, we show that a large domain gap not necessarily leads to a low test accuracy. In this paper, we investigate this problem from the perspective of feature separability empirically and theoretically. Specifically, we propose a dataset-level score based upon feature dispersion to estimate the test accuracy under distribution shift. Our method is inspired by desirable properties of features in representation learning: high inter-class dispersion and high intra-class compactness. Our analysis shows that inter-class dispersion is strongly correlated with the model accuracy, while intra-class compactness does not reflect the generalization performance on OOD data. Extensive experiments demonstrate the superiority of our method in both prediction performance and computational efficiency.

## 1 Introduction

Machine learning techniques deployed in the open world often struggle with distribution shifts, where the test data are not drawn from the training distribution. Such issues can substantially degrade the test accuracy, and the generalization performance of a trained model may vary significantly on different shifted datasets (Quinonero-Candela et al., 2008; Koh et al., 2021). This gives rise to the importance of estimating out-of-distribution (OOD) errors for AI Safety (Deng and Zheng, 2021). However, it is prohibitively expensive or unrealistic to collect large-scale labeled examples for each shifted testing distribution encountered in the wild. Subsequently, predicting OOD error becomes a challenging task without access to ground-truth labels.

In the literature, a popular direction in predicting OOD error is to utilize the model output on the shifted dataset (Jiang et al., 2021; Guillory et al., 2021; Garg et al., 2022), which heavily relies on the model calibration. Yet, machine learning models generally suffer from the overconfidence issue even for OOD inputs (Wei et al., 2022), leading to suboptimal performance in estimating OOD performance. Many prior works turned to measuring the distribution difference between training and OOD test set, due to the conventional wisdom that a higher distribution shift normally leads to lower OOD accuracy. AutoEval (Deng and Zheng, 2021) applies Frechet Distance to calculate the distribution distance for model evaluation under distribution shift. A recent work (Yu et al., 2022)introduces Projection Norm (ProjNorm) metric to measure the distribution discrepancy in network parameters. However, we find that the connection between distribution distance and generalization performance does not always hold, making these surrogate methods to be questionable. This motivates our method, which directly estimates OOD accuracy based on the feature properties of test instances.

In this work, we show that feature separability is strongly associated with test accuracy, even in the presence of distribution shifts. Theoretically, we demonstrate that the upper bound of Bayes error is negatively correlated with inter-class feature distances. To quantify the feature separability, we introduce a simple dataset-level statistic, Dispersion Score, which gauges the inter-class divergence from feature representations, i.e., outputs of feature extractor. Our method is motivated by the desirable properties of embeddings in representation learning (Bengio et al., 2013). To achieve high accuracy, we generally desire embeddings where different classes are relatively far apart (i.e., high inter-class dispersion), and samples in each class form a compact cluster (i.e., high intra-class compactness). Surprisingly, our analysis shows that intra-class compactness does not reflect the generalization performance, while inter-class dispersion is strongly correlated with the model accuracy on OOD data.

Extensive experiments demonstrate the superiority of Dispersion Score over existing methods for estimating OOD error. First, our method dramatically outperforms existing training-free methods in evaluating model performance on OOD data. For example, our method leads to an increase of the \(R^{2}\) from 0.847 to 0.970 on TinyImageNet-C (Hendrycks and Dietterich, 2019) - a 14.5\(\%\) of relative improvement. Compared to the recent ProjNorm method (Yu et al., 2022), our method not only achieves superior performance by a meaningful margin, but also maintains huge advantages in computational efficiency and sample efficiency. For example, using CIFAR-10C dataset as OOD data, Dispersion Score achieves an \(R^{2}\) of 0.972, outperforming that of ProjNorm (i.e., 0.947), while our approach only takes around 3% of the time consumed by ProjNorm.

Overall, using Dispersion Score achieves strong performance in OOD error estimation with high computational efficiency. Our method can be easily adopted in practice. It is straightforward to implement with deep learning models and does not require access to training data. Thus our method is compatible with modern settings where models are trained on billions of images.

We summarize our main contribution as follows:

1. We find that the correlation between distribution distance and generalization performance does not always hold, downgrading the reliability of existing distance-based methods.
2. Our study provides empirical evidence supporting a significant association between feature separability and test accuracy. Furthermore, we theoretically show that increasing the feature distance will result in a decrease in the upper bound of Bayes error.
3. We propose a simple dataset-level score that gauges the inter-class dispersion from feature representations, i.e., outputs of feature extractor. Our method does not rely on the information of training data and exhibits stronger flexibility in OOD test data.
4. We conduct extensive evaluations to show the superiority of the Dispersion Score in both prediction performance and computational efficiency. Our analysis shows that Dispersion Score is more robust to various data conditions in OOD data, such as limited sample size, class imbalance and partial label set. Besides, we show that intra-class compactness does not reflect the generalization performance under distribution shifts (SubSection 4.4).

## 2 Problem Setup and Motivation

### Preliminaries: OOD performance estimation

SetupIn this work, we consider multi-class classification task with \(k\) classes. We denote the input space as \(\) and the label space as \(=\{1,,k\}\). We assume there is a training dataset \(=\{_{i},y_{i}\}_{i=1}^{n}\), where the \(n\) data points are sampled _i.i.d._ from a joint data distribution \(_{}\). During training, we learn a neural network \(f:^{k}\) with trainable parameters \(^{p}\) on \(\).

In particular, the neural network \(f\) can be viewed as a combination of a feature extractor \(f_{g}\) and a classifier \(f_{}\), where \(g\) and \(\) denote the parameters of the corresponding parts, respectively. The feature extractor \(f_{g}\) is a function that maps instances to features \(f_{g}:\), where \(\) denotes the feature space. We denote by \(_{i}\) the learned feature of instance \(_{i}\): \(_{i}=f_{g}(_{i})\). The classifier \(f_{}\) is a function from the feature space \(\) to \(^{k}\), which outputs the final predictions. A trained model can be obtained by minimizing the following expected risk:

\[_{}(f) =_{(,y)_{}} [(f(;),y)]\] \[=_{(,y)_{}} [(f_{}(f_{g}()),y)]\]

Problem statementAt test time, we generally expect that the test data are drawn from the same distribution as the training dataset. However, distribution shifts usually happen in reality and even simple shifts can lead to large drops in performance, which makes it unreliable in safety-critical applications. Thus, our goal is to estimate how a trained model might perform on the shifted data without labels, i.e., unlabeled out-of-distribution (OOD) data.

Assume that \(}=\{}_{i}\}_{i=1}^{m}\) be the OOD test dataset and \(\{_{i}\}_{i=1}^{m}\) be the corresponding unobserved labels. For a certain test instance \(}_{i}\), we obtain the predicted labels of a trained model by \(_{i}^{}= f(}_{i})\). Then the ground-truth test error on OOD data can be formally defined as:

\[(})=_{i=1}^{m}(_{i}^{}_{i}),\] (1)

To estimate the real OOD error, the key challenge is to formulate a score \(S(})\) that is strongly correlated with the test error across diverse distribution shifts without utilization of corresponding test labels. With such scores, a simple linear regression model can be learned to estimate the test error on shifted datasets, following the commonly used scheme (Deng and Zheng, 2021; Yu et al., 2022).

While those output-based approaches suffer from the overconfidence issue (Hendrycks and Gimpel, 2016; Guillory et al., 2021), other methods primarily rely on the distribution distance between training data \(\) and test data \(}\)(Deng and Zheng, 2021; Tzeng et al., 2017), with the intuition that the distribution gap impacts classification accuracy. In the following, we motivate our method by analyzing the failure of those methods based on feature-level distribution distance.

### The failure of distribution distance

In the literature, distribution discrepancy has been considered as a key metric to predict the generalization performance of the model on unseen datasets (Deng and Zheng, 2021; Tzeng et al., 2017; Gao and Kleywegt, 2022; Sinha et al., 2017; Yu et al., 2022). AutoEval (Deng and Zheng, 2021) estimates model performance by quantifying the domain gap in the feature space:

\[S(,})=d(,}),\]

where \(d()\) denotes the distance function, such as Frechet Distance (Dowson and Landau, 1982) or maximum mean discrepancy (MMD) (Gretton et al., 2006).

ProjNorm measures the distribution gap with the Euclidean distance in the parameter space:

\[S(,})=\|-\|_{2},\]

where \(\) and \(\) denote the parameters fine-tuned on training data \(\) and OOD data \(}\), respectively.

The underlying assumption is inherited from the conventional wisdom in domain adaptation, where a representation function that minimizes domain difference leads to higher accuracy in the target domain (Tzeng et al., 2014; Ganin and Lempitsky, 2015; Tzeng et al., 2017). Yet, the relationship between distribution distance and test accuracy remains controversial. For example, the distribution shift may not change the model prediction if the classifier's outputs are insensitive to changes in the shifted features. There naturally arises a question: given a fixed model, does a larger distribution distance always lead to a lower test accuracy?

To verify the correlation between distribution distance and model performance, we compare the model performance on different OOD test sets of CIFAR-10C, using the ResNet-50 model trained on CIFAR-10. For each OOD test set, we calculate the distribution distances in the feature space \(\) via Frechet distance (Dowson and Landau, 1982) and MMD (Gretton et al., 2006), respectively.

Figure 1 presents the classification accuracy versus the distribution distance. The results show that, on different test datasets with similar distances to the train data in the feature space. the performance of the trained model varies significantly with both the two distance metrics. For example, calculated by Frechet distance, the distribution gap varies only a small margin from 223.36 to 224.04, but the true accuracy experiences a large drop from 61.06% to 46.99%. Similar to MMD distance, when the distribution gap changes from 87.63 to 88.35, the OOD accuracy drops from 66.53% to 47.73%. The high variability in the model performance reveals that, **the distribution difference is not a reliable surrogate for generalization performance under distribution shifts**.

## 3 Proposed Method

In this section, we first introduce the intuition of our method with a natural example. Next, we characterize and provide quantitative measure on the desirable properties of feature representations for predicting OOD error. We then present the advantages of our proposed method.

### Motivation

In representation learning, it is desirable to learn a separable feature space, where different classes are far away from each other and samples in each class form a compact cluster. Therefore, separability is viewed as an important characteristic to measure the feature quality. For example, one may train a nearest neighbor classifier over the learned representation using labeled data and regard its performance as the indicator. In this paper, we investigate how to utilize the characteristic of separability for estimating the model performance under distribution shifts, without access to ground-truth labels.

Intuitive exampleIn Figure 2, we present a t-SNE visualization of the representation for training and test data. In particular, we compare the separability of the learned representation on shifted datasets with various severity. While the feature representation of training data exhibits well-separated clusters, those of the shifted datasets are more difficult to differentiate and the cluster gaps are well correlated with the corruption severity, as well as the ground-truth accuracy. It implies that, a model with high classification accuracy is usually along with a well-separated feature distribution, and vice versa. With the intuition, we proceed by theoretically showing how the feature separability affects the classification performance.

Theoretical explanationRecall that we obtain the model prediction by \(y^{}_{i}= f_{}(_{i})\), where the intermediate feature \(_{i}=f_{g}(_{i})\). Let \(P(y=i)\) denote the prior class probability of class \(i\), and \(p(|y=i)\) denote the class likelihood, i.e., the conditional probability density of x given that it belongs to class \(i\). Then the Bayes error (Young and Calvert, 1974; Devijver and Kittler, 1982; Webb and Copsey, 1990; Duda et al., 2006) can be expressed as:

Figure 1: Distribution Gap Vs. OOD accuracy on CIFAR10-C via (a) Fréchet distance (Dowson and Landau, 1982) and (b) MMD (Gretton et al., 2006). All colors indicate 14 types of corruption. The test accuracy varies significantly on shifted datasets with the same distribution distances.

\[E_{}\,=1-_{i=1}^{k}_{C_{i}}P(y=i)p( y= i)d,\]

where \(C_{i}\) is the region where class \(i\) has the highest posterior. The Bayes error rate provides a lower bound on the error rate that can be achieved by any pattern classifier acting on derived features. However, the Bayes error is not so readily obtainable as class priors and class-conditional likelihoods are normally unknown. Therefore, many studies have focused on deriving meaningful upper bounds for the Bayes error (Devijver and Kittler, 1982; Webb and Copsey, 1990).

For a binary classification problem where \(k=2\), we have

\[E_{}\,,\] (2)

where \(\) denotes the distance between features from the two classes (Devijver and Kittler, 1982; Tumer and Ghosh, 2003), such as _Mahalanobis distance_(Chandra et al., 1936) or _Bhattacharyya distance_(Fukunaga, 2013). Based on the Bayes error for 2-class problems, the upper bound can be extended to the multi-class setting (\(k>2\)) by the following equation (Garber and Djouadi, 1988).

\[E_{}^{k}\,_{\{0,1\}}( _{i=1}^{k}(1-P(y=i))E_{\,;i}^{k-1}+ {1-}{k-2}),\] (3)

where \(\) is an optimization parameter. With Equations 2 and 3, we obtain an upper bound of Bayes error, which is negatively correlated with the inter-class feature distances. Specifically, increasing the feature distance will result in a decrease in the upper bound of Bayes error. In this manner, we provide a mathematical intuition for the phenomenon shown in Figure 2. However, Computing the upper bound of Bayes error directly, as indicated above, is challenging due to its computational complexity. To circumvent this issue, we propose a straightforward metric as a substitute, which does not require access to label information.

### Dispersion score

In our previous analysis, we show a strong correlation between the test accuracy and feature separability under different distribution shifts. To quantify the separability in the feature space \(\), we introduce _Dispersion score_ that measures the inter-class margin without annotation information.

First, we allocate OOD instances \(\{}\}_{i=1}^{m}\) into different clusters \(j\) based on the model predictions, i.e., their pseudo labels from the trained classifier \(f_{}\): \(j=_{i}^{}= f_{}(_{i})\).

With these clusters, we compute the _Dispersion score_ by the average distances between each cluster centroid \(}_{j}=}_{i=1}^{m_{j}}_{i}\{_{i}^{}=j\}\) and the center of all features \(}=_{i=1}^{m}_{i}\), weighted by the sample size of each cluster \(m_{j}\). Formally, the _Dispersion score_ is defined as:

\[S(})=_{j=1}^{k}m_{j}(},}_{j})\]

Figure 2: t-SNE visualization of feature representation on training and OOD test datasets (CIFAR-10C) with _contrast_ corruption under different severity levels. The first left figure shows feature representation of the training dataset, while the rest of three figures illustrate those of the OOD datasets. From this figure, we observe that different clusters tends to be more separated as the corruption severity level gets smaller.

where \(k-1\) is the degree of freedom and \(\) denotes the distance function. With the weight \(m_{j}\), the induced score receives a stronger influence from those larger clusters, i.e., the majority classes. This enables our method to be more robust to the long-tailed issue, which naturally arises in the unlabeled OOD data. In subsection 4.3, we explicitly show the advantage of our method in the class-imbalanced case and analyze the importance of the weight in Appendix 4.5.

In particular, we use square Euclidean distances to calculate the distance in the feature space \(\). So it converts to:

\[S(})=^{k}m_{j}\|}-}_{j}\|_{2}^{2}}{k-1}\] (4)

Following the common practice (Jiang et al., 2019), we adopt a log transform on the final score, which corresponds to multiplicative combination of class statistics.

We summarize our approach in Appendix A. Notably, the Dispersion score derived from the feature separability offers several compelling advantages:

1. **Training data free.** The calculation procedure of our proposed score does not rely on the information of training data. Thus, our method is compatible with modern settings where models are trained on billions of images.
2. **Easy-to-use.** The computation of the Dispersion score only does forward propagation for each test instance once and does not require extra hyperparameter, training a new model, or updating the model parameters. Therefore, our method is easy to implement in real-world tasks and computational efficient, as demonstrated in Tables 2 and 3.
3. **Strong flexibility in OOD data**. Previous state-of-the-art methods, like ProjNorm (Yu et al., 2022), usually requires a large amount of OOD data for predicting the prediction performance. Besides, the class distribution of unlabeled OOD datasets might be severely imbalanced, which makes it challenging to estimate the desired test accuracy on balanced data. In Subsection 4.3, we will show the Dispersion score derived from the feature separability exhibits stronger flexibility and generality in sample size, class distribution, and partial label set (see Subsection 4.3 and Appendix E).

## 4 Experiments

In this subsection, we first compare the proposed score to existing training-free methods. Then, we provide an extensive comparison between our method and recent state-of-the-art method - ProjNorm. We then show the flexibility of our method under different settings of OOD test data. Additionally, we present an analysis of using ground-truth labels and K-means in Appendixes C and 4.6. Finally, we discuss the limitation of the proposed score for adversarial setting in Appendix F.

### Experimental Setup

**Train datasets.** During training, we train models on the CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009) and TinyImageNet (Le and Yang, 2015) datasets. Specifically, the train data of CIFAR-10 and CIFAR-100 contain 50,000 training images, which are allocated to 10 and 100 classes, respectively. The TinyImageNet dataset contains 100,000 64 \(\) 64 training images, with 200 classes.

**Out-of-distribution (OOD) datasets.** To evaluate the effectiveness of the proposed method on predicting OOD error at test time, we use CIFAR-10C and CIFAR-100C (Hendrycks and Dietterich, 2019), which span 19 types of corruption with 5 severity levels. For the testing of TinyImageNet, we use TinyImageNet-C (Hendrycks and Dietterich, 2019) that spans 15 types of corruption with 5 severity levels as OOD dataset. All the datasets with certain corruption and severity contain 10,000 images, which are evenly distributed in the classes.

**Evaluation metrics.** To measure the linear relationship between OOD error and designed scores, we use coefficients of determination (\(R^{2}\)) and Spearman correlation coefficients (\(\)) as the evaluation metrics. For the comparison of computational efficiency, we calculate the average evaluation time (\(T\)) for each test set with certain corruption and severity.

**Training details.** During the training process, we train ResNet18, ResNet50 (He et al., 2016) and WRN-50-2 (Zagoruyko and Komodakis, 2016) on CIFAR-10, CIFAR-100 (Krizhevsky et al., 2009) and TinyImageNet (Le and Yang, 2015) with 20, 50 and 50 epochs, respectively. We use SGD with the learning rate of \(10^{-3}\), cosine learning rate decay (Loshchilov and Hutter, 2016), a momentum of 0.9 and a batch size of 128 to train the model.

**The compared methods.** We consider 7 existing methods as benchmarks for predicting OOD error: _Rotation Prediction_ (Rotation) (Deng et al., 2021), _Averaged Confidence_ (ConfScore) (Hendrycks and Gimpel, 2016), _Entropy_(Guillory et al., 2021), _Agreement Score_(AgreeScore) (Jiang et al., 2021), _Averaged Threshold Confidence_ (ATC) (Garg et al., 2022), _AutoEval_(Frechet) (Deng and Zheng, 2021), and _ProjNorm_(Yu et al., 2022). Rotation and AgreeScore predict OOD error from the view of unsupervised loss constructed by generating rotating instances and measuring output agreement of two independent models, respectively. ConfScore, Entropy, and ATC formulate scores by model predictions. ProjNorm measures the parameter-level difference between the models fine-tuned on train and OOD test data respectively, while Frechet quantifies the distribution difference in the feature space between the training and test datasets. We present the related works in Appendix B.

### Results

Can Dispersion score outperform existing training-free approaches?In Table 1, we present the performance of OOD error estimation on three model architectures and three datasets. We find that Dispersion Score dramatically outperforms existing training-free methods. For example, averaged across three architectures on TinyImageNet, our method leads to an increase of the \(R^{2}\) from 0.847 to 0.970 - a 14.5\(\%\) of relative improvement. In addition, Dispersion Score achieves consistently high performance over the three datasets with a \(R^{2}\) higher than 0.950, while scores of other approaches such as Rotation varying from 0.787 to 0.924 are not stable. We observe a similar phenomenon on \(\), where the Entropy method achieves performance that is ranging from 0.842 to 0.994, while the performance of our method fluctuates around 0.988.

    &  &  &  &  &  &  &  &  \\   & & \(R^{2}\) & \(\) & \(R^{2}\) & \(\) & \(R^{2}\) & \(\) & \(R^{2}\) & \(\) & \(R^{2}\) & \(\) & \(R^{2}\) & \(\) & \(R^{2}\) & \(\) \\   & ResNet18 & 0.822 & 0.951 & 0.869 & 0.985 & 0.899 & 0.987 & 0.663 & 0.929 & 0.884 & 0.985 & 0.950 & 0.971 & **0.968** & **0.990** \\  & ResNet50 & 0.835 & 0.961 & 0.935 & 0.993 & 0.945 & **0.994** & 0.835 & 0.985 & 0.946 & **0.994** & 0.858 & 0.964 & **0.987** & 0.990 \\  & WRN-50-2 & 0.862 & 0.976 & 0.943 & **0.994** & 0.942 & **0.994** & 0.856 & 0.986 & 0.947 & **0.994** & 0.814 & 0.973 & **0.962** & 0.988 \\   & Average & 0.840 & 0.963 & 0.916 & 0.991 & 0.930 & **0.992** & 0.785 & 0.967 & 0.926 & 0.991 & 0.874 & 0.970 & **0.972** & 0.990 \\   & ResNet18 & 0.860 & 0.936 & 0.916 & 0.985 & 0.891 & 0.979 & 0.902 & 0.973 & 0.938 & 0.986 & 0.888 & 0.968 & **0.952** & **0.988** \\  & ResNet50 & 0.908 & 0.962 & 0.919 & 0.984 & 0.884 & 0.977 & 0.922 & 0.982 & 0.921 & 0.984 & 0.837 & 0.972 & **0.951** & **0.985** \\  & WRN-50-2 & 0.924 & 0.970 & 0.971 & 0.984 & 0.968 & 0.981 & 0.955 & 0.977 & 0.978 & **0.993** & 0.865 & 0.987 & **0.980** & 0.991 \\   & Average & 0.898 & 0.956 & 0.936 & 0.987 & 0.915 & 0.983 & 0.927 & 0.982 & 0.946 & **0.988** & 0.864 & 0.976 & **0.962** & **0.988** \\   & ResNet18 & 0.786 & 0.946 & 0.670 & 0.869 & 0.592 & 0.842 & 0.561 & 0.853 & 0.751 & 0.945 & 0.826 & 0.970 & **0.966** & **0.986** \\  & ResNet50 & 0.786 & 0.947 & 0.670 & 0.869 & 0.651 & 0.892 & 0.560 & 0.853 & 0.751 & 0.945 & 0.826 & 0.971 & **0.977** & **0.986** \\   & WRN-50-2 & 0.878 & 0.967 & 0.757 & 0.951 & 0.704 & 0.935 & 0.654 & 0.904 & 0.635 & 0.897 & 0.884 & 0.984 & **0.968** & **0.986** \\   & Average & 0.805 & 0.959 & 0.727 & 0.920 & 0.650 & 0.890 & 0.599 & 0.878 & 0.693 & 0.921 & 0.847 & 0.976 & **0.970** & **0.987** \\    & &  & & & & & & & & & & & & & & & \\ 

Table 1: Performance comparison of training free approaches on CIFAR-10, CIFAR-100 and TinyImageNet, where \(R^{2}\) refers to coefficients of determination, and \(\) refers to Spearman correlation coefficients (higher is better). The best results are highlighted in **bold**.

Figure 3: OOD error prediction versus True OOD error on CIFAR-10 with ResNet50. We compare the performance of Dispersion Score with that of ProjNorm and Fréchet via scatter plots. Each point represents one dataset under certain corruption and certain severity, where different shapes represent different types of corruption, and darker color represents the higher severity level.

Dispersion score is superior to ProjNorm.In Table 2, we compare our method to the recent state-of-the-art method - ProjNorm (Yu et al., 2022) in both prediction performance and computational efficiency. The results illustrate that Dispersion Score could improve the prediction performance over ProjNorm with a meaningful margin. For example, with trained models on CIFAR-10 dataset, using Dispersion score achieves a \(R^{2}\) of 0.953, much higher than the average performance of ProjNorm as \(0.873\). On CIFAR-100, our method also achieves comparable (slightly better) performance with ProjNorm (0.961 vs. 0.948). Besides, Dispersion score obtains huge advantages to ProjNorm in computational efficiency. Using WRN-50-2, ProjNorm takes an average of 575 seconds for estimating the performance of each OOD test dataset, while our method only requires 11 seconds. Since the computation of Dispersion score does not needs to update model parameters or utilize the training data, our method enables to predict OOD error with large-scale models trained on billions of images.

To further analyze the advantage of our method, we present in Figure 3 the scatter plots for Frechet, ProjNorm and Dispersion Score on CIFAR-10C with ResNet50. From the figure, we find that Dispersion Score estimates OOD errors linearly w.r.t. true OOD errors in all cases. In contrast, we observe that those methods based on distribution difference tends to fail when the classification error is high. This phenomenon clearly demonstrates the reliable and superior performance of the Dispersion score in predicting generalization performance under distribution shifts.

### Flexibility in OOD data

In previous analysis, we show that Dispersion score can outperform existing methods on standard benchmarks, where the OOD test datasets contain sufficient instances and have a balanced class distribution. In reality, the unlabeled OOD data are naturally imperfect, which may limit the performance of predicting OOD error. In this part, we verify the effectiveness of our method with long-tailed data and small data, compared to ProjNorm (Yu et al., 2022).

    &  &  &  \\   & & \(R^{2}\) & \(\) & \(T\) & \(R^{2}\) & \(\) & \(T\) \\   & ResNet18 & 0.936 & 0.982 & 179.616 & **0.968** & **0.990** & **10.980** \\  & ResNet50 & 0.944 & 0.989 & 266.099 & **0.987** & **0.990** & **11.259** \\  & WRN-50-2 & 0.961 & **0.989** & 575.888 & **0.962** & 0.988 & **11.017** \\   & Average & 0.947 & 0.987 & 326.201 & **0.972** & **0.990** & **11.085** \\   & ResNet18 & **0.979** & 0.980 & 180.453 & 0.952 & **0.988** & **6.997** \\  & ResNet50 & **0.988** & **0.991** & 262.831 & 0.953 & 0.985 & **11.138** \\  & WRN-50-2 & **0.990** & **0.991** & 605.616 & 0.980 & **0.991** & **12.353** \\   & Average & **0.985** & 0.987 & 349.63 & 0.962 & **0.988** & **10.163** \\   & ResNet18 & **0.970** & 0.981 & 182.127 & 0.966 & **0.986** & **7.039** \\  & ResNet50 & **0.979** & 0.987 & 264.651 & 0.977 & **0.990** & **13.938** \\   & WRN-50-2 & 0.965 & 0.983 & 590.597 & **0.968** & **0.986** & **11.235** \\   & Average & **0.972** & 0.984 & 345.792 & 0.970 & **0.987** & **10.737** \\    & Average & **0.972** & 0.984 & 345.792 & 0.970 & **0.987** & **10.737** \\   

Table 2: Performance comparison between ProjNorm (Yu et al., 2022) and our Dispersion score on CIFAR-10, CIFAR-100 and TinyImageNet, where \(R^{2}\) refers to coefficients of determination, \(\) refers to Spearman correlation coefficients (higher is better), and \(T\) refers to average evaluation time (lower is better). The best results are highlighted in **bold**.

Figure 4: Prediction performance \(R^{2}\) vs. sample size of OOD data (subsets of CIFAR-10C) with (a) ResNet18 and (b) ResNet50.

Class imbalance.We first evaluate the prediction performance under class imbalanced setting. In particular, given a trained model, we aim to estimate its balanced accuracy under distribution shift while we only have access to a long-tailed test data, where a few classes (majority classes) occupy most of the data and most classes (minority classes) are under-represented. We conduct experiments on CIFAR-10C and CIFAR-100C with imbalance rate 100.

Our results in Table 3 show that Dispersion Score achieves better performance than ProjNorm (Yu et al., 2022) under class imbalance. For example, our approach achieves an average \(R^{2}\) of 0.953 on CIFAR-10C with 2.780 seconds, while ProjNorm obtains an \(R^{2}\) of 0.873 and takes 398.972 seconds. In addition, the performance of our method is more stable across different model architectures and datasets with the \(R^{2}\) ranging from 0.932 to 0.986 than ProjNorm (0.799 - 0.980). Overall, Dispersion score maintains reliable and superior performance even when the OOD test set are class imbalanced.

**Sampling efficiency.** During deployment, it can be challenging to collect instances under a specific distribution shift. However, existing state-of-the-art methods (Yu et al., 2022) normally require a sufficiently large test dataset to achieve meaningful results in predicting OOD error. Here, we further validate the sampling efficiency of Dispersion score, compared with ProjNorm.

Figure 4 presents the performance comparison between Dispersion Score and ProjNorm on subsets of CIFAR10C with various sample sizes. The results show that our method can achieve excellent performance even when only 50 examples are available in the OOD data. In contrast, the performance of ProjNorm decreases sharply with the decrease in sample size. The phenomenon shows that Dispersion score is more efficient in exploiting the information of OOD instances.

### Intra-class compactness Vs Inter-class dispersion

In Section 3, we empirically show that feature separability is naturally tied with the final accuracy and propose an effective score based on inter-class dispersion. However, the connection between intra-class compactness and generalization performance is still a mystery. In this analysis, we show that compactness is not a good indicator of OOD accuracy. Specifically, we define the compactness score:

\[S(})=-^{k}_{i=1}^{m_{j}}||_{i} -}_{j}||^{2}}{n-k}\]

where \(}_{j}\) denotes the centroid of the cluster \(j\) that the instance \(_{i}\) belongs to. Therefore, the compactness score can measure the clusterability of the learned features, i.e., the average distance between each instance and its cluster centroid. Intuitively, a high compactness score may correspond to a well-separated feature distribution, which leads to high test accuracy. Surprisingly, in Figure 5, we find that the compactness score is largely irrelevant to the final OOD error, showing that it is not an effective indicator for predicting generalization performance under distribution shifts.

    &  &  &  \\   & & \(R^{2}\) & \(\) & \(T\) & \(R^{2}\) & \(\) & \(T\) \\   & ResNet18 & 0.799 & 0.968 & 204.900 & **0.959** & **0.982** & **2.076** \\  & ResNet50 & 0.897 & 0.980 & 430.406 & **0.968** & **0.982** & **3.295** \\  & WRN-50-2 & 0.922 & 0.978 & 561.611 & **0.932** & **0.978** & **2.970** \\   & Average & 0.873 & 0.973 & 398.972 & **0.953** & **0.980** & **2.780** \\   & ResNet18 & 0.886 & 0.968 & 210.05 & **0.941** & **0.982** & **1.864** \\  & ResNet50 & **0.980** & **0.988** & 433.860 & 0.956 & 0.982 & **2.974** \\   & WRN-50-2 & 0.978 & 0.982 & 768.883 & **0.986** & **0.994** & **3.242** \\    & Average & 0.948 & 0.980 & 470.931 & **0.961** & **0.986** & **2.693** \\   

Table 3: Summary of prediction performance on **Imbalanced** CIFAR-10C and CIFAR-100C (Hendrycks and Dietterich, 2019), where \(R^{2}\) refers to coefficients of determination, \(\) refers to Spearman correlation coefficients (higher is better), and \(T\) refers to average evaluation time (lower is better). The best results are highlighted in **bold**. More results can be found in Appendix D.

Figure 5: Compactness vs. test error on CIFAR-10C with ResNet50.

### The importance of the weight

As introduced in Section 3, Dispersion Score can be viewed as a weighted arithmetic mean of the distance from centers of each class to the center of the whole samples in the feature space, where the weight is the total number of samples in the corresponding class. To verify the importance of the weight in long-tailed setting, we consider a variant that removes the weight:

\[S(})=_{j=0}^{k}(}, }_{j}).\]

The results are shown in Table 4, where we compare performances of Dispersion score and the variant without weight respectively under **imbalanced** CIFAR-10C and CIFAR-100C with ResNet10 and ResNet50. From this table, we could observe that the weight enhance the robustness significantly in long-tail conditions.

### K-means vs. Pseudo labels.

While our Dispersion score derived from pseudo labels has demonstrated strong promise, a question arises: _can a similar effect be achieved by alternative clustering methods?_ In this ablation, we show that labels obtained by K-means (Lloyd, 1982, MacQueen, 1967) does not achieve comparable performance with pseudo labels obtained from the trained classifier. In particular, we allocate instances into clusters by using K-means instead of pseudo labels from the classifier.

We present the performance comparison of our method and the variant of K-means in Figure 6. The results show that our Dispersion score performs better than the K-means variant and the gap is enlarged with more classes. On the other hand, the variant of K-means does not require a classifier, i.e., the linear layer in the trained model, which enables to evaluate the OOD performance of representation learning methods, e.g., self-supervised learning.

## 5 Conclusion

In this paper, we introduce Dispersion score, a simple yet effective indicator for predicting the generalization performance on OOD data without labels. We show that Dispersion score is strongly correlated to the OOD error and achieves consistently better performance than previous methods under various distribution shifts. Even when the OOD datasets are class imbalanced or have limit number of instances, our method maintains a high prediction performance, which demonstrates the strong flexibility of dispersion score. This method can be easily adopted in practical settings. It is straightforward to implement with trained models with various architectures, and does not require access to the training data. Thus, our method is compatible with large-scale models that are trained on billions of images. We hope that our insights inspire future research to further explore the feature separability for predicting OOD error.

    &  &  &  \\   & & \(R^{2}\) & \(\) & \(R^{2}\) & \(\) \\   & ResNet18 & 0.675 & 0.930 & **0.959** & **0.982** \\  & ResNet50 & 0.748 & 0.948 & **0.968** & **0.982** \\   & Average & 0.712 & 0.939 & **0.978** & **0.990** \\   & ResNet18 & 0.595 & 0.838 & **0.941** & **0.982** \\  & ResNet50 & 0.395 & 0.733 & **0.956** & **0.982** \\    & Average & 0.494 & 0.785 & **0.952** & **0.986** \\   

Table 4: Summary of prediction performance on **Imbalanced** CIFAR-10C and CIFAR-100C. The best results are highlighted in **bold**.

Figure 6: Compare with K-means.

Acknowledgements

This research is supported by the Ministry of Education, Singapore, under its Academic Research Fund Tier 1 (RG13/22). Hongxin Wei gratefully acknowledges the support of Center for Computational Science and Engineering at Southern University of Science and Technology for our research. Lei Feng is supported by the National Natural Science Foundation of China (Grant No. 62106028), Chongqing Overseas Chinese Entrepreneurship and Innovation Support Program, Chongqing Artificial Intelligence Innovation Center, CAAI-Huawei MindSpore Open Fund, and Openl Community (https://openi.pcl.ac.cn).