# _EpiCare_: A Reinforcement Learning Benchmark for Dynamic Treatment Regimes

Mason Hargrave

Center for Studies in Physics and Biology

The Rockefeller University

New York, NY, USA

mhargrave@rockefeller.edu

&Alex Spaeth

Dept. of Electrical and Computer Engineering

University of California, Santa Cruz

Santa Cruz, CA, USA

atspaeth@ucsc.edu

&Logan Grosenick

Dept. of Psychiatry and BMRI

Weill Cornell Medicine, Cornell University

New York, NY, USA

log4002@med.cornell.edu

To whom correspondence should be addressed.

###### Abstract

Healthcare applications pose significant challenges to existing reinforcement learning (RL) methods due to implementation risks, limited data availability, short treatment episodes, sparse rewards, partial observations, and heterogeneous treatment effects. Despite significant interest in using RL to generate dynamic treatment regimes for longitudinal patient care scenarios, no standardized benchmark has yet been developed. To fill this need we introduce _Episodes of Care_ (_EpiCare_), a benchmark designed to mimic the challenges associated with applying RL to longitudinal healthcare settings. We leverage this benchmark to test five state-of-the-art offline RL models as well as five common off-policy evaluation (OPE) techniques. Our results suggest that while offline RL may be capable of improving upon existing standards of care given sufficient data, its applicability does not appear to extend to the moderate to low data regimes typical of current healthcare settings. Additionally, we demonstrate that several OPE techniques standard in the the medical RL literature fail to perform adequately on our benchmark. These results suggest that the performance of RL models in dynamic treatment regimes may be difficult to meaningfully evaluate using current OPE methods, indicating that RL for this application domain may still be in its early stages. We hope that these results along with the benchmark will facilitate better comparison of existing methods and inspire further research into techniques that increase the practical applicability of medical RL.

## 1 Introduction

Most human diseases evolve over time, many with trajectories that can be influenced by the right treatment . Dynamic treatment regimes (DTRs) are adaptive medical policies which define a set of decision rules to determine the treatment to apply to a patient given the patient's medical history, including past treatments and observations . Although latent biology drives disease progression, physicians lack direct access to the true biological state of any given patient and instead must rely on indirect and often partial clinical observations that correlate with this hidden state [1; 3; 4; 5; 6].

Sparved by previous work applying reinforcement learning (RL) to other types of medical problems , numerous authors have expressed interest in using offline RL to generate DTRs, especially in the case of longitudinal patient care with multi-treatment selection .

Medical RL models are faced with a chicken-and-egg problem: the RL models cannot be deployed until they are evaluated for safety, and cannot be directly evaluated except by being deployed. To address this, indirect pre-deployment validation methods are commonly used to evaluate the real-world readiness of various RL techniques. This pre-deployment validation can be approached in three ways. First, models can be trained on historical data, and their performance predicted via off-policy evaluation (OPE). Second, models can limit themselves to directly mimicking the behavior policy under which the historical data was collected, a process known as behavior cloning (BC) , which can avoid some issues with OPE by restricting the RL model's behavioral repertoire . Finally, RL models can be trained on a simulated environment designed to capture the challenges expected in the real-world environment of interest. This simulation approach enables direct evaluation of RL policies on the simulated environment without ethical concerns. This approach also makes it possible to compare OPE performance predictions against the actual online performance of RL policies, providing a performance benchmark for OPE methods themselves. Despite the distinct advantages of the simulation-based approach, to date no such simulated environments have been developed for longitudinal healthcare applications -- instead, most previous work has focused on simulating the effect of controlling individual drug dosages over short periods of time (See Section 2).

In this paper we introduce _Episodes of Care_ (_EpiCare_), the first benchmark for RL in longitudinal patient care. We compare the performance of five state-of-the-art offline RL models on our benchmark. Additionally, we evaluate five common OPE methods to determine whether they reliably predict the performance of RL models when trained on _EpiCare's_ simulated clinical trial data. Our findings indicate that these OPE methods cannot be trusted to accurately predict RL performance in longitudinal medical scenarios, calling into question their use for benchmarking RL performance in real-world clinical applications.

Key design considerations include:

**Realistic Difficulty.**_EpiCare_ presents significant challenges for existing RL methods, including short episodes with varied initial conditions, unknown transition dynamics, and observation distributions that overlap between multiple distinct hidden states. Our benchmark also includes healthcare-specific challenges such as heterogeneous treatment effects (HTEs) and adverse events . As we are chiefly interested in offline RL, we generate our off-policy datasets by way of simulated clinical trials which emulate the real-world collection of clinical data. While the challenges present in _EpiCare_ are germane to the field of healthcare, _EpiCare_ is designed as a benchmark capable of representing a class of medically inspired problems rather than a disease-specific simulation.

**Patient Safety.** One of the most important considerations in deployment of any new DTR is that it should not reduce patient safety relative to the existing standard of care (SoC). Therefore, in addition to mean returns, we also measure patient welfare statistics such as the adverse event rate and mean time to remission. For comparison, we model the SoC via a policy designed to emulate performance of a hypothetical clinician following best practice but without access to the latent disease states.

**Reproducibility and Configurability.** As an open source tool available on GitHub and conforming to OpenAI Gym standards , _EpiCare_ aims to encourage the reproducibility and comparability of results critical to advancing the field of medical RL. The environment's configurability ensures that researchers can simulate a wide array of procedurally generated disease treatment scenarios of variable difficulty. We would like to stress that no such longitudinal medical treatment simulation environments exist and thus our work represents a first-in-class example of such a benchmark.

**Standardized Benchmarks.** While configurability is useful, having a standard benchmark is also important. To this end we have chosen some specific environment hyperparameters in close collaboration with medical professionals which reflect the realities of longitudinal patient treatment scenarios. As online RL has historically been too risky for most medical contexts , we focused our benchmarking efforts on offline RL methods, as well as off-policy evaluation (OPE).

## 2 Related Work

Reviews on both offline RL [17; 16], and medical RL  comprehensively cover a large scope of related work. An enormous fraction of the offline RL literature cites healthcare as a core motivation [19; 20; 21; 22], but evaluations typically use standard RL benchmarks that are unrelated to medicine [15; 23; 24]. This highlights a significant need for a healthcare-oriented RL benchmark like _EpiCare_.

A central problem in RL-generated DTRs is that of validating their real-world performance [25; 26; 27]. RL-generated DTRs are typically evaluated online; the DTR is applied to an environment for some number of episodes and the rewards are reported. In medical RL however, online evaluation is too risky prior to employing alternate initial validation strategies . Instead, DTRs are evaluated by either off-policy evaluation (OPE) or via simulation, each having advantages and drawbacks.

**Off-Policy Evaluation on Real-World Data.** OPE is a class of techniques for predicting real-world performance of a policy by way of historical data [28; 29]. However, OPE is plagued by high data overheads and significant variance in the predicted performance . Consequently, it has been claimed that most available medical datasets are not large enough for OPE . Despite these challenges, numerous exciting RL contributions have emerged in the medical context, not only for discrete treatment selection in longitudinal patient care [9; 10; 11], but also for problems including propofol infusion control during surgery , mechanical ventilation for intensive care , sepsis treatment [34; 35; 36; 37], and chemotherapy . Due to the widespread use of OPE to evaluate RL models trained on real-world data, much of the previous research on medical RL hinges on the quality of OPE methods themselves. Short of the ethically dubious proposition of deploying RL models directly on patient populations, simulated patient care models are the only other available pathway to validating OPE techniques. _EpiCare_ represents such a benchmark and provides an unambiguous evaluation of OPE efficacy in longitudinal patient care scenarios.

**Simulated Environments.** In contrast to OPE, simulation-based methods evaluate the performance of RL algorithms on domain-specific pathogenesis models. Most simulated environments in the medical RL literature are chiefly concerned with the continuous control of drug dosages. For example, an HIV drug dosage model  has been used by a number of researchers as a test bed for various RL techniques [40; 41; 42; 43]. Similarly, researchers have simulated blood glucose control for diabetes [44; 45; 46], anti-seizure medications for epilepsy , and levidopa dosage for Parkinson's disease . Despite this focus on continuous control, it is common for clinicians to model disease progression dynamics as a set of discrete states which evolve over time (Figure 0(a)). While continuous models of medical scenarios like propofol infusion can be modulated to represent well-understood HTEs (especially those arising from known risk factors), we are not aware of any simulation which uses a discrete hidden state model to represent cryptic disease states. More broadly, there are no existing RL environments for longitudinal healthcare applications. This is despite the wealth of literature focused on the challenge of developing longitudinal treatment protocols for conditions specifically characterized by HTEs, such as acute respiratory effect syndrome , atrial fibrillation , osteoarthritis , and borderline personality disorder . In this way _EpiCare_ fills a critical gap in the existing medical RL literature.

## 3 Environment

_EpiCare_ represents longitudinal patient care scenarios by modeling disease progression and treatment response over time (Figure 0(b)) using a Partially Observable Markov Decision Process (POMDP) framework (Figure 0(c)). The environment contains a state space representing various disease states including remission and adverse events, an observation space capturing clinical indicators (symptons), and an action space representing the set of available therapeutic interventions. The probabilistic state transition dynamics are influenced by both the current state and selected treatment, while observations are emitted based on state-specific symptom distributions and modified by treatment effects. The reward function of _EpiCare_ aligns with medical objectives to account for symptom management, treatment costs, and achieving remission. Each episode begins with a patient initialized in a random initial state, and the goal is to manage the patient's symptoms effectively through a sequence of treatment decisions until remission is achieved or the episode ends. _EpiCare_ is highly configurable, allowing researchers to simulate a wide range of disease dynamics and treatment scenarios, providing a comprehensive benchmark for evaluating RL methods in longitudinal medical contexts. For the full modeling details of _EpiCare_, see Appendix A.

An important feature of this model is that all of the POMDP parameters are generated pseudorandomly according to an "environment seed", which is separate from the random seed controlling the stochastic transitions within an episode. As a result, _EpiCare_ defines a class of related environments indexed by the environment seed. The performance of an RL method should be evaluated across multiple environments in order to assess its generalizability. In this paper, we report the performance of each algorithm on eight different environment instantiations.

## 4 Policies

_EpiCare_ includes three non-RL policies which serve two different purposes. First, they can be used to generate the datasets from which we train our offline RL algorithms of interest. When used in this way, the policies are referred to as "behavior policies". Second, they can be used as performance baselines against which to compare the performance of our RL models. When used in this way, the policies are referred to as "baseline policies".

These policies are not trained from data; instead, their behavior is computed directly from the parameters of the POMDP. These policies simulate medical decision-making (1) with complete state and state-specific treatment response knowledge (Oracle Policy), (2) without state or state-specific treatment response knowledge (SoC), and (3) using a popular real-world approach (SMART) for clinical trial randomization [54; 55]. For policies without state knowledge, it is possible to mis-estimate the efficacy of treatments, leading to worse performance compared to situations where states are identifiable (see Section 4.2). Overall performance of these policies is compared in Appendix B.3.

### Oracle Policy (OP)

The oracle policy (OP) provides direct access to the hidden state, and at each timestep chooses the action which greedily maximizes the instantaneous expected reward given that state. This policy is not fully optimal, as it does not take into account multi-step treatment strategies (e.g. biasing

Figure 1: (a) A simple real-world example of the state transition graph for liver disease . (b) A diagram representing a simple two-state disease. Inside the dashed boxes is a Markov model representing disease states. For each disease state, there exists a set of treatments which if applied may lead to remission (as indicated by the blue table), as well as a distribution of symptom severities. At the beginning of each episode, a patient is initialized in one of the disease states, and an initial observation of that patient’s symptoms is collected. An agent then uses that observation to select a treatment to apply, which affects the transition probabilities out of the current state. This process continues until remission is achieved or a maximum number of timesteps is reached. (c) A graphical model of a POMDP complete with observations, rewards, states, and actions. The dashed lines from actions to observations indicate that in _EpiCare_, actions can directly affect observations.

transition probabilities towards a disease state that would be easier to treat on the next step).2 Still, the OP operates with significant advantage and can thus be used to establish a reasonable floor on best-case DTR performance.

### Standard of Care (SoC)

The SoC policy aims to provide a facsimile of real clinician performance. Because our treatment scenarios are procedurally generated, however, there is no such thing as a real-world SoC to compare against. Therefore, we have made some assumptions about what such an SoC would look like. Because we are focused on the challenges associated with generating DTRs in scenarios with cryptic latent disease states and HTEs, we assume our idealized clinician does not have a way to estimate latent state or state-specific treatment effects. Instead, their knowledge of the medical literature and best practices is modeled by use of the ground truth expected reward of each action without hidden state information. Our SoC clinician also assumes that the reward distribution during each episode is non-stationary and patient-dependent. Thus each individual episode is a non-stationary multi-armed bandit with some known prior information, which we address using the common technique of an exponentially recency-weighted value estimate  that resets to the stationary expected reward at the beginning of each episode.3 For implementation details, see Appendix B.1.

Given the importance of safety as a performance metric, a meaningful baseline policy must be able to take adverse events into account when selecting actions. Since adverse events occur when symptoms reach extreme values, our SoC policy simply avoids prescribing treatments which would worsen any symptom that is currently above a given threshold. The result is a greedy policy that simulates a plausible clinical SoC in the face of incomplete information about disease state.4 This provides a conservative benchmark against which the performance of RL algorithms can be assessed.

### Sequential Multiple Assignment Randomized Trial (SMART)

The SMART policy models treatment selection for a simulated sequential multiple assignment randomized trial (SMART) [54; 55]. This widely-used clinical trial strategy randomizes patients across multiple treatment arms. The policy adheres to a weighted random selection process where each treatment's likelihood of selection is based on its expected reward (for details, see Appendix B.2). This weighted sampling approach is inspired by Thompson sampling, a simple but effective heuristic approach for balancing exploration and exploitation . The SMART policy allows us to generate synthetic clinical trial data which can be used to provide our RL approaches with relevant and realistic off-policy datasets.

## 5 Results

We employed _EpiCare_ to benchmark five recent, high-impact offline RL methods: AWAC , EDAC , TD3+BC , IQL , and CQL . Our implementations of these models are derivative of the CORL library . Most of these models are usable for discrete control simply by optimizing the logits of a one-hot-encoded action output, but for TD3+BC and EDAC, it is necessary to propagate gradients through the chosen action; to convert these implementations for the discrete control case, we used Gumbel-Softmax reparameterization [65; 66]. Additionally, we benchmarked two simpler methods as baselines: behavior cloning (BC), and a deep \(Q\) network (DQN) . The input to each model consisted of not only the current symptoms, but also the entire observation history of the current episode as well as the last action selected. Hyperparameters were derived from sweeps carried out on each model according to ranges established in the literature (Appendix C.3). A diagram detailing the benchmarking process can be found in Figure 2.

### Online Evaluation

We assessed the performance of our chosen RL methods across variations of the environment by generating a dataset of \(2^{17}=131{,}072\) episodes from each of 8 different environment seeds collected under the SMART policy defined in Section 4.3.5 Because the underlying POMDP is generated from the environment parameters, these datasets can be thought of as being drawn from sequentially randomized clinical trials of 8 unrelated diseases. These datasets, consisting of observation, action, and reward trajectories, were then used to train four replicates of each of our models of interest. Each trained model was evaluated on 1,000 episodes of online interactions. Online evaluations of OP and SoC are also reported, with SoC representing a lower bound on the acceptable performance of an RL algorithm. A policy which takes uniform random actions at all timesteps (Rand) was also assessed. The outcomes of this experiment can be found in Figure 2(a) and Table 4.

The RL methods we benchmarked fit broadly into two categories: value-based (CQL, DQN, and IQL) and actor-critic (TD3+BC, AWAC, and EDAC).6 In our online evaluation metrics, all value-based methods outperformed all actor-critic methods for all metrics when averaged across environments. This makes some sense considering that our benchmarks used a relatively small action space of 16 treatments, and one of the main advantages of actor-critic methods is their ability to efficiently manage large (high-dimensional or continuous) action-spaces . Indeed the advantage of value-based methods over actor-critic methods in discrete action spaces is well documented in other, non-medical domains . Interestingly, TD3+BC, which is a hybrid between the actor-critic method TD3 and BC performs significantly better across the board than either method type in isolation (Figure 2(a)). We see similar relationships between model performance when it is quantified in terms of ability to achieve remission (Table 6). Overall, CQL, DQN, IQL, and to a lesser extent TD3+BC all outperform our SoC policy baseline, indicating that they learn to distinguish between the latent states. Of these, CQL has the best performance overall.

This advantage continues to a lesser degree in terms of adverse event rates, which we use to evaluate the safety of each RL method, i.e. the degree to which they avoid rare but negative consequences (Figure 2(b)). The adverse event rate metric also reveals that while DQN may achieve higher overall reward than IQL, IQL manages to trigger fewer adverse events. The safety disadvantage of DQN can be ascribed to its tendency to overestimate future rewards , a tendency which IQL and CQL are

Figure 2: A diagram of the benchmarking process. The SMART policy was used to generate a synthetic clinical trial dataset from our environment. Once trained, the offline RL methods were evaluated both online and by way of OPE.

both designed to correct against [62; 63]. Despite optimizing only for mean returns, CQL, IQL, and DQN all outperform SoC's heuristic approach in terms of adverse event rates, demonstrating that these methods have some ability to avoid actions which would lead to dangerous outcomes.

### Data Restriction

For the results presented in Figure 3 we used \(2^{17}=131,072\) episodes worth of training data per environment. This quantity of simulated patients is well in excess of the typical size of clinical trials. Although clinical trials of individual treatments have in some cases had in the millions of patients, a more typical sample size would be in the hundreds, with the largest SMART trial including \(2{,}876\) patients [71; 72]. As such, it is important to evaluate how RL models perform in a restricted data regime. To test this, we trained the four top performing models from the initial evaluation with varying training set sizes to see how performance degrades as offline training data size decreases (Figure 4)7. We compare these to the OP, SoC, and Rand (random) policies, whose performance curves are constant horizontal lines because they are based on known environment parameters rather than learned from data.

DQN is the first model to beat SoC performance at 2,048 patients worth of data, very close to the size of the largest ever SMART clinical trial. In the low data regime, below 256 patients worth of data, DQN performance degrades below random. This is likely due to the fact that DQN has no mechanism by which to correct against reward overestimates, a problem that becomes more pronounced as data availability decreases. IQL in particular lags in terms of relative performance for a unique reason: the optimal number of IQL training steps varies as as function of data availability. For a full discussion of this peculiarity, see Appendix C.4. TD3+BC also exhibits an interesting phenomenon where the mean and variance of its returns decrease substantially near \(N=256\). We suspect that this may correspond to a double-descent-like effect, where the model (whose layers each have 256 neurons) moves out of the overparametrization regime, as was recently recorded in TD models . We carry out the same analysis but with median remission rate instead of episode reward in Appendix C.5.

### Off-policy Evaluation

A significant amount of previous work in medical RL is dependent on the belief that existing OPE methods behave as faithful estimators of the true real-world performance of a learned policy . In the context of medicine however, the number of interactions with any given patient is usually

Figure 3: Performance evaluations in terms of (a) returns and (b) adverse event rates for all learning methods. These metrics are reported as their respective means across 4 replicates each of 8 structurally different _EpiCare_ environments generated from environment seeds 1–8. The error bars represent the mean (across environments) of the standard deviation (across replicates). For comparison, the SoC baseline performance is shown as a horizontal dashed red line. See Tables 4 and 5 for full results.

quite small compared to existing RL benchmarks, a regime which is out of scope for existing OPE benchmarks [29; 74]. Therefore _EpiCare_, which incorporates unique challenges associated with healthcare including short episode lengths, can provide us with an optimistic picture of how well OPE is likely to work in the clinical setting. To this end we implemented five common OPE methods: IS, WIS, PDIS, WPDIS , and a simple direct method (DM)  based on a regression model of returns at each timestep. These methods were chosen based on their prevalence in the medical RL literature [9; 11; 34; 37]. In order to evaluate these OPE methods, we took the final model checkpoints for all RL models and conducted OPE on a \(131,072\) episode withheld test set. For the four importance sampling methods (IS, WIS, PDIS, and WPDIS), we used the mean value of the estimator across 8 bootstrap resamples of the test set. On the other hand, since DM is based on a trained model, instead of bootstrapping, we simply trained 4 replicates. Root-mean-square error (RMSE) between OPE estimates and online evaluations of each checkpointed model was then used to assess the degree to which the OPE estimates were indicative of the actual online evaluation results (Table 1).

We find that OPE estimates of online performance on _EpiCare_ are poor overall, in line with the high reported variance of these estimators . Furthermore, another key limitation of OPE is that its accuracy is dependent on an _effective_ sample size, which can become orders of magnitude smaller than the number of data points available when the policy being tested differs significantly from the behavior policy , a fact which has been used to argue that RL in healthcare should be limited to behavior cloning policies . Indeed, our OPE methods performed reasonably only on AWAC and BC, the two policies most likely to select the same action as the behavior policy (Appendix D).

    & EDAC & AWAC & BC & TD3+BC & IQL & DQN & CQL \\  IS & 32.7 & 4.3 & 2.3 & 81.0 & 37.1 & 35.4 & 37.7 \\ WIS & 61.4 & 4.3 & 2.3 & 36.2 & 35.0 & 10.9 & 10.7 \\ PDIS & 35.6 & 4.4 & 2.3 & 112.8 & 38.3 & 57.9 & 56.0 \\ WPDIS & 36.7 & 8.4 & 6.7 & 46.3 & 30.7 & 44.4 & 46.9 \\ DM & 23.0 & 11.8 & 12.3 & 50.6 & 46.9 & 93.5 & 106.4 \\   

Table 1: RMSE between the OPE estimates and the true online returns evaluated on 1,000 episodes for each combination of OPE method and RL model, across 8 seeds with 4 replicates. A plot of these results can be found in Appendix Figure 13.

Figure 4: Median returns during the data restriction trials for four top performing RL models, compared to _EpiCare_’s three baseline policies (whose median per-episode performance is dictated only by the environment parameters and not by data availability).

### Effect of Training Data

The quality of training data significantly affects offline RL performance. We evaluated this by training models on _EpiCare_ data generated by three policies: the SoC policy (expert clinician behavior), the SMART policy (clinical trial simulation), and an online-trained DQN policy with the same hyperparameters as above (but with \(2^{19}\) episodes) which we refer to simply as Online.

Results in Table 2 show that most models trained on SMART data outperform those trained on SoC or Online data, likely due to SMART's increased state space exploration through randomization. While SoC and Online policies are more effective for individual patients, their exploitative nature limits the diversity of training data. BC and AWAC by contrast, which both aim to replicate training data behavior, show improved performance when trained on higher-performing policies (Online \(>\) SoC \(>\) SMART), benefiting from the consistent, expert-driven behavior in SoC data and the patient-optimized decisions in the Online data.

These findings emphasize that more exploratory datasets may outperform expert-driven data for training robust RL policies in DTR healthcare, despite the latter's apparent advantages. Furthermore, methods like DQN, while effective with exploratory data, degrade significantly with less exploratory data likely due to overoptimism in unobserved contexts [63; 62].

## 6 Limitations

**Generalizability to Real-World Clinical Scenarios.**_EpiCare_, while sophisticated, clearly cannot capture all of the complexities of the clinic. We caution against attempting to use _EpiCare_ as a model of any one particular disease without appropriate domain expertise both in terms of the disease of interest and the modeling details of the environment. The results of any disease-specific benchmarking should be audited independently by experts and ethicists for bias prior to deployment.

**Dependence on Simulation Parameters.** The performance of RL and OPE methods in _EpiCare_ is influenced by the environment's parameters. Variations in parameters, such as the number of disease states or the connectivity of the states, could impact the relevance of our findings to specific contexts. In particular, we report results for 8 distinct _EpiCare_ disease environments generated randomly from the same parameters. This leaves open the possibility that other parameters could yield more consistent OPE performance or faster RL convergence. However, we expect that real longitudinal medical care applications represent a greater challenge to existing methods than _EpiCare_ such that our results act as a ceiling on real-world performance of both offline RL and OPE.

**OPE Methods.** Though we test a comprehensive list of the most common OPE methods in the medical RL literature, our list is not exhaustive. Still, we expect that our list is representative and that the same limitations would likely apply to OPE methods not included.

## 7 Guidance on Usage and Interpretation for Researchers

First and foremost, _EpiCare_ is designed as a standalone medically inspired benchmark for RL and OPE methods. Any RL or OPE methods that cite longitudinal care as a motivating use-case should leverage _EpiCare_ to validate the efficacy of the method in longitudinal healthcare contexts. To accomplish this, offline RL algorithms should be trained on the provided offline training datasets as generated by the SMART behavior policy, while online RL algorithms should simply train until

    & EDAC & AWAC & BC & TD3+BC & IQL & DQN & CQL \\  SMART & 7.2(17.5) & 30.1(1.8) & 24.4(1.0) & 71.3(5.2) & 76.5(0.7) & 77.0(1.6) & 79.4(0.7) \\ SoC & -26.8(19.9) & 40.7(1.6) & 41.5(0.9) & _49.5(1.8)_ & 42.6(0.9) & -59.6(7.0) & _54.3(0.4)_ \\ Online & -3.2(0.8) & 64.4(0.7) & 66.7(0.8) & 9.6(1.0) & 68.7(0.7) & -32.6(0.7) & 63.9(0.7) \\   

Table 2: Mean return comparison between policies trained on SMART data vs. policies trained on SoC data. Mean (standard deviation) across 4 replicates on _EpiCare_ environment 1. Only CQL and TD3+BC (italicized) outperform SoC when trained on SoC data.

convergence on _EpiCare_ itself.8 RL algorithms that surpass the SoC baseline in performance are demonstrating clear evidence for the ability to distinguish between hidden states and associate effective treatments. Furthermore, RL algorithms with lower adverse event rates than the SoC baseline are in so doing demonstrating the ability to identify state and select safe treatments.

Given the high-degree of configurability in _EpiCare_, it may be tempting to set or fit the parameters of _EpiCare_ to match some medical dataset or model some specific disease for sim-to-real applications. We caution against using _EpiCare_ in this way as the configurable parameters are predominantly related to the random generation of different ensembles of fictitious disease environments. In this way _EpiCare_ parameters are used to define a set of medically-inspired problems for RL to solve, indexed by environment seeds, rather than a single disease. Anyone interested in modeling a specific disease would likely be better off designing a more detailed simulation of a disease of interest including any disease-specific challenges not well-represented in the _EpiCare_ benchmark.

A better way to use _EpiCare_ in the context of applied medical RL research would be to set the benchmark parameters to ranges which are relevant to the disease of interest by asking questions like "How many unique treatments exist for my disease?", "What is the cure-rate for each treatment and how do they vary?", "How distinguishable are the hidden states believed to be and how many are there?", "How many symptoms or clinical measurements are associated with the disease?" and "How many time points do we typically have per patient?". These questions should guide the setting of the _EpiCare_ parameters and allow researchers to titrate the challenges represented by _EpiCare_. Setting the parameters in this way should provide researchers with a _rough_ estimate of the amount of data that would be necessary for any given RL method to be effective for a given medical use-case,9 though we still caution as above that researchers may need to go beyond simply setting parameters to incorporate any disease-specific phenomena which are not well-accounted for with _EpiCare_.

## 8 Conclusion

Here we have introduced _EpiCare_, a comprehensive Python library designed to benchmark reinforcement learning (RL) methods in the context of medical treatment. We hope this work represents a significant stride towards benchmarking and realizing the practical application of RL in healthcare.

Our results demonstrate that existing OPE methods fail to provide reliable performance estimates even in our simplified model of clinical settings (inherently easier than real-world scenarios). This suggests that these methods are even less likely to succeed in the noisy and complex environments of actual clinical practice. The poor performance of OPE methods in our study calls into question the practical validity of much existing research that relies on these techniques for evaluating RL in clinical contexts. If OPE cannot reliably estimate model performance in _EpiCare_, the utility of OPE in more complex real-world scenarios is dubious--especially given that OPE depends on large data availability , and our simulated trials were orders of magnitude larger than standard and SMART clinical trials. Additionally, we show that while some RL methods can outperform our SoC baseline in terms of both efficacy and safety given sufficient data, this advantage disappears in the data-restricted regime typical of real clinical settings. Additionally, the superior performance of value-based methods over actor-critic approaches demonstrates the importance of method selection for medical applications. Finally, we show that of the value-based methods, both CQL and IQL have advantages over DQN with regards to safety (by way of lower adverse event rates) and with regards to learning from low-entropy training data as collected by highly exploitative behavior policies.

The medical community's increasing interest in RL-based dynamic treatment regimes demands rigorous evaluation methods. _EpiCare_ addresses this need by providing a first-in-class benchmark that captures the key challenges of longitudinal healthcare settings. By enabling the systematic comparison of RL methods and evaluation techniques, we hope this work will facilitate more reliable assessment of RL's readiness for clinical applications and inspire new approaches better suited to the unique demands of healthcare settings.