# Unified lower bounds for interactive high-dimensional estimation under information constraints

Jayadev Acharya

Cornell University

acharya@cornell.edu

&Clement L. Canonne

University of Sydney

clement.canonne@sydney.edu.au

Ziteng Sun

Google Research, New York

zitengsun@google.com

&Himanshu Tyagi

Indian Institute of Science, Bangalore

htyagi@iisc.ac.in

###### Abstract

We consider distributed parameter estimation using interactive protocols subject to _local information constraints_ such as bandwidth limitations, local differential privacy, and restricted measurements. We provide a unified framework enabling us to derive a variety of (tight) minimax lower bounds for different parametric families of distributions, both continuous and discrete, under any \(_{p}\) loss. Our lower bound framework is versatile and yields "plug-and-play" bounds that are widely applicable to a large range of estimation problems. In particular, our approach recovers bounds obtained using data processing inequalities and Cramer-Rao bounds, two other alternative approaches for proving lower bounds in our setting of interest. Further, for the families considered, we complement our lower bounds with matching upper bounds.

## 1 Introduction

We consider the problem of parameter estimation under _local information constraints_, where the estimation algorithm has access to only limited information about each sample. These constraints can be of various types, including communication constraints, where each sample must be described using a few (_e.g._, constant number of) bits; (local) privacy constraints, where each sample is obtained from a different user and the users seek to reveal as little as possible about their specific data; as well as many others, _e.g._, noisy communication channels, or limited types of data access such as linear measurements. Such problems have received significant attention in recent years, motivated by applications such as data analytics in distributed systems and federated learning.

Our main focus is on information-theoretic lower bounds for the minimax error rates (or, equivalently, the sample complexity) of these problems. Several recent works have provided bounds that apply to specific constraints or work for specific parametric estimation problems, sometimes without allowing for interactive protocols. Indeed, handling interactive protocols is technically challenging, and several results in prior work exhibit flaws in their analysis. In particular, even the most basic Gaussian mean estimation problem using interactive communication remains, surprisingly, open.

We present general, "plug-and-play" lower bounds for parametric estimation under information constraints that can be used for any local information constraint and allows for _interactive_ protocols. Our abstract bound requires very simple (and natural) assumptions to hold for the underlying parametric family; in particular, we do not require technical "regularity" conditions that are common in asymptotic statistics.

We apply our general bound to canonical problems of high-dimensional mean estimation and distribution estimation, under privacy and communication constraints, for the entire family of \(_{p}\) loss functions for \(p 1\). In addition, we provide complementary schemes that show that our lower bounds are tight for most settings of interest.

### Our results

Our main contribution is a general approach to establish lower bounds in distributed information-constrained parameter estimation. The setup is described in detail in Section 2 and is illustrated in Fig. 1. In short, independent samples \(X^{n}=(X_{1},,X_{n})\) are generated from an unknown distribution \(\) from a parametric family \(_{}=\{_{},\}\) of distributions. Only limited information \(Y_{i}\) about datum \(X_{i}\) is available to the algorithm. The goal is to estimate the underlying parameter \(\) associated with \(\). Furthermore, we consider interactive estimation, wherein \(Y_{i}\) can depend on \(Y_{1},,Y_{i-1}\). Our general lower bound, which we develop in Section 3, takes the following form: Consider a collection of distributions \(\{_{z}\}_{z\{-1,+1\}^{k}}_{}\) contained in the parametric family. This collection represents a "difficult subproblem" that underlies the parametric estimation problem being considered; such constructions are often used when deriving information-theoretic lower bounds (_e.g._, in Assouad's method ). Note that each coordinate of \(z\) represents, in essence, a different "direction" of uncertainty for the parameter space. The difficulty of the estimation problem can be related to the difficulty of determining a randomly chosen \(z\) (or most of the coordinates of \(z\)), denoted \(Z\), by observing samples from \(_{z}\). Once \(Z=z\) is fixed, \(n\) independent samples \(X^{n}=(X_{1},,X_{n})\) are generated from \(_{z}\) and the limited information \(Y_{i}\) about \(X_{i}\) is passed to an estimator.

Our most general result, stated as Theorem 1, is an upper bound for the average discrepancy, an average distance quantity related to average probability of error in determining coordinates of \(Z\) by observing the limited information \(Y^{n}=(Y_{1},,Y_{n})\). Our bounding term reflects the underlying information constraints using a quantity that captures how "aligned" we can make our information about the sample to the uncertainty in different coordinates of \(Z\). Importantly, our results hold under minimal assumptions. In particular, in contrast to many previous works, our results do not require any "bounded ratio" assumption on the collection \(\{_{z}\}_{z\{-1,+1\}^{k}}\), which would ask that the density function change by at most a constant factor if we modify one coordinate of \(z\). When we impose additional structure for \(_{z}\) - such as orthogonality of the random changes in density when we modify different coordinates of \(z\) and, more stringently, independence and subgaussianity of these changes - we get concrete bounds which are readily applicable to different problems. These plug-and-play bounds are stated as consequences of our main result in Theorem 2. The interested reader can also directly consider the applications to local privacy (Corollary 1) or communication constraints (Corollary 2).

We demonstrate the versatility of the framework by showing that it readily yields tight (and in some cases nearly tight) bounds for parameter estimation (both in the sparse and dense cases) for several fundamental families of continuous and discrete distributions, several families of information constraints such as communication and local differential privacy (LDP), and for the family of \(_{p}\) loss functions for \(p 1\), all when interactive protocols are allowed. To complement our lower bounds, we provide algorithms (protocols) which attain the stated rates, thus establishing optimality of our results.1We discuss these results in Section 5, where we provide the corresponding statements. In terms of the applications, our contributions are two-fold:

1. We obtain several results from a diverse set of prior works in a unified fashion as simple corollaries of our main result: As discussed further in Section 1.2, the lower bounds for mean estimation for product Bernoulli under \(_{2}\) loss and those for estimation of discrete distributions under \(_{1}\) and \(_{2}\) losses, for both communication and LDP constraints, were known from previous work. However, our approach allows us to easily recover those results and extend them to arbitrary \(_{p}\) losses, with interaction allowed, in a unified fashion.
2. Our bounds also yield new lower bounds for some canonical problems. The prototypical example being mean estimation for high-dimensional Gaussian distributions under information constraints. As discussed in the next section, while some prior work claimed lower bounds for this problem, their arguments appear to be flawed - at a high level, due to the "bounded ratio" assumption their techniques rely on, which Gaussian distributions do not satisfy, and which our framework does not require. To the best of our knowledge our work is the first to obtain those lower bounds for interactive mean estimation of high-dimensional Gaussian distributions under communication or local privacy constraints.

### Previous and related work

There is a significant amount of work in the literature dedicated to parameter estimation under various constraints and settings. Here, we restrict our discussion to works that are most relevant to the current paper, with a focus on the interactive setting (either the sequential or blackboard model; see Section 2 for definitions).

The work arguably closest to ours is the recent work , which focuses on density estimation and goodness-of-fit testing of discrete distributions, under the \(_{1}\) metric, for sequentially interactive protocols under general local information constraints (including, as special cases, local privacy and communication constraints, as in the present paper). This work can be seen as a significant generalization of the techniques of , allowing us to obtain lower bounds for estimation in a variety of settings, notably high-dimensional parameter estimation.

Among other works on high-dimensional mean estimation under communication constraints, [17; 10] consider communication-constrained Gaussian mean estimation in the blackboard communication model, under \(_{2}\) loss. The protocols for the upper bounds in these works do not require interactivity and are complemented with lower bounds which show that the bounds are tight up to constant factors in the dense case and up to logarithmic factors in the sparse case. However, the proof of the lower bound in  seems to present a gap (specifically, in the truncation argument of [10, Theorem 4.3]), as confirmed in personal communication with the authors. Correcting the issue in the truncation argument would lead to a result significantly weaker than the claimed lower bound, and it is unclear whether this can be fixed using the techniques from that paper.

In this work, we present interactive protocols for the sparse case which improve over the noninteractive protocols and strenghten the upper bounds by a logarithmic factor in the interactive case (see Remark 1). Further, using our general framework, we establish a nearly-matching lower bound for the problem, recovering the rate lower bound originally claimed in  up to a logarithmic factor. In a slightly different setting,  considers the mean estimation problem for product Bernoulli distributions when the mean vector is \(1\)-sparse, under \(_{2}\) loss. The lower bound in , too, allows sequentially interactive protocols. In the blackboard communication model,  and  obtained tight bounds for mean estimation and density estimation under \(_{2}\) and \(_{1}\) loss, respectively.

Turning to local privacy,  provide upper bounds (as well as some partial lower bounds) for one-dimensional Gaussian mean estimation under LDP under the \(_{2}\) loss, in the sequentially interactive model. Recent works of  and  obtain lower bounds for mean estimation in the blackboard communication model and under LDP, respectively, for both Gaussian and product Bernoulli distributions; as well as density estimation for discrete distributions. Their approach is based on the classic Cramer-Rao bound and, as such, is tied inherently to the use of the \(_{2}\) loss. In a recent independent work,  extended these methods to obtain lower bounds under general \(_{p}\) loss under communication constraints, which are tight for Gaussian mean estimation under noninteractive protocols. , by developing a locally private counterpart of some of the well-known information-theoretic tools for establishing statistical lower bounds (namely, Le Cam, Fano, and Assouad), establish tight or nearly tight bounds for several mean estimation problems in the LDP setting.

More recently, drawing on machinery from the communication complexity literature,  develop a methodology for proving lower bounds under LDP constraints in the blackboard communication model. They obtain lower bounds for mean estimation of product Bernoulli distributions under general \(_{p}\) losses which match ours (in the high-privacy regime, _i.e._, small \(\)). Similar to the results under communication constraints [17; 10], their approach relies heavily on the assumption that the distributions on each coordinate are independent, which fails to generalize to discrete distributions. Further, their bounds are tailored to the LDP constraints and do not seem to extend to arbitrary information constraints. Finally, while  also claims tight bounds for mean estimation of Gaussian and sparse Gaussian distributions under \(_{2}\) loss, their argument invokes the analogous (flawed) result from  and thus it is unclear whether the stated lower bound can be shown using their techniques.

Finally, we mention that very recently, following the appearance of , an updated version of  appeared online as , which has similar results as ours for the high-dimensional mean estimation problem under communication constraints. Both our work and  build upon the framework presented for the discrete setting in . Moreover, their work still need the "bounded ratio", and hence their lower bound for sparse Gaussian family only works for noninteractive protocols.

**Notation.** Hereafter, we write \(\) and \(\) for the binary and natural logarithms, respectively. For distributions \(_{1},_{2}\) over \(\), denote their Kullback-Leibler divergence (in nats) by \((_{1}\|_{2})\), and their Hellinger distance by \(_{}(_{1},_{2}):=(( _{1}}{}}-_{2}}{}})^{2}\,)^{1/2}\,,\) where we assume \(_{1},_{2}\) for some underlying measure \(\) on \(\). Further, we denote the Shannon entropy of a random variable \(X\) by \(H(X)\) and the mutual information between \(X\) and \(Y\) by \(I(X;Y)\); we will sometimes write \(H()\) for the entropy of a random variable with distribution \(\). We refer the reader to  for details on these notions and their properties, which will be used throughout. Given two functions \(f,g\), we write \(f g\) if there exists an absolute constant \(C>0\) such that \(f(x) Cg(x)\) for all \(x\); and \(f g\) if \(f g\) and \(f g\) both hold. Finally, we use the standard asymptotic notation \(O(f)\), \((f)\), \((f)\).

**Organization.** In Section 2, we formalize our setting of interactive inference under local information constraints. The general lower bound framework and results are presented in Section 3, where we provide implications of the general result under additional structures and specific information constraints such as local privacy (LDP) and communication constraints. Finally, we use our framework to readily derive lower bounds for a wide range of applications in Section 5. Due to space constraints, all proofs, as well as our upper bounds (algorithms) are provided in the Supplement, where we also discuss how our techniques compare with other existing approaches for proving lower bounds under information constraints, namely, those based on strong data processing inequalities (SDPI) or on the van Trees inequality.

## 2 The setup

We consider standard parametric estimation problems. For some \(^{d}\), let \(_{}=\{_{},\}\) be a family of distributions over some measurable space \((,)\), namely each \(_{}\) is a distribution over \((,)\). Suppose \(n\) independent samples \(X^{n}=(X_{1},,X_{n})\) from an unknown \(_{}_{}\) are obtained. The goal in parametric estimation is to design estimators \(:^{n}\), and form estimates \((X^{n})\) of \(\) using independent samples \(X^{n}\) from \(_{}\). We illustrate our results using two specific distribution families: discrete probability mass functions (pmfs) and high-dimensional product distributions with unknown mean vectors. We will describe the precise minimax setting in detail later in this section.

We are interested in an information-constrained setting, where we do not have direct access to the samples \(X^{n}\) from \(_{}\). Instead, we can only obtain limited information about each datapoint \(X_{i}\). Following , we model these information constraints by specifying an allowed set of _channels_\(\) with input alphabet \(\) and some output space \(\).2 Each sample \(X_{i}\) is passed through a channel from \(\), chosen appropriately, and its output \(Y_{i}\) is the observation we get. This setting is quite general and captures as special cases the popular communication and privacy constraints, as we will describe momentarily.

We now formally describe the setting, which is illustrated in Fig. 1. \(n\) i.i.d. samples \(X_{1},,X_{n}\) from an unknown distribution \(_{}_{}\) are observed by players (users) where player \(t\) observes \(X_{t}\). Player \(t[n]\) selects a channel \(W_{t}\) and sends the message \(Y_{t}\) to a referee, where \(Y_{t}\) is drawn from the probability measure \(W_{t}( X_{t},Y_{1},,Y_{t-1})\). The referee observes \(Y^{n}:=(Y_{1},,Y_{n})\) and seeks to estimate the parameter \(\).

The freedom allowed in the choice of \(W_{t}\) at the players gives rise to various communication protocols. We focus on _interactive protocols_, where channels are chosen by one player at a time, and they can use all previous messages to make this choice. We describe this class of protocols below, where we further allow each player \(t\) to have a different set of constraints \(_{t}\) (_e.g._, a different communication budget), and \(W_{t}\) must be in \(_{t}\). For simplicity of exposition, and as these already encapsulate most of the difficulties, we focus here on the case of _sequentially_ interactive protocols, which has been widely considered in the literature and captures many settings of interest. However, we emphasize that our results extend to the more general class of fully interactive protocols (see Supplement).

_Definition 1_ (Sequentially Interactive Protocols).: Let \(X_{1},,X_{n}\) be i.i.d. samples from \(_{}\), \(\). A _sequentially interactive protocol \(\) using \(^{n}=(_{1},,_{n})\)_ involves mutually independent random variables \(U,U_{1},,U_{n}\) (independent of the input \(X_{1},,X_{n}\)) and mappings \(g_{t}(U,U_{t}) W_{t}_{t}\) for selecting the channel in round \(t[n]\). In round \(t\), player \(t\) uses the channel \(W_{t}\) to produce the message (output) \(Y_{t}\) according to the probability distribution \(W_{t}( X_{t},Y_{1},,Y_{t-1})\). The messages \(Y^{n}=(Y_{1},,Y_{n})\) received by the referee and the public randomness \(U\) (available to all players) constitute the _transcript_ of the protocol \(\); the private randomness \(U_{1},,U_{n}\) (where \(U_{t}\) is local to player \(t\)) is not part of the transcript. In other words, the channel at player \(t\) as a (randomized) mapping \(W^{t-1}\), which depends on input \(x\) and the previous \(t-1\) messages \(y^{t-1}^{t-1}\) outputs some \(y\).

For concreteness, we now instantiate this definition for the two aforementioned types of information constraints, communication and (local) privacy.

Communication constraintsLet \(:=\{0,1\}^{*}=_{m=0}^{}\{0,1\}^{m}\). For \( 1\) and \(t 1\), let

\[^{,}:=\{W^{*} \{0,1\}^{}\}\] (1)

be the family of channels with input alphabet \(\) and output alphabet the set of all \(\)-bit strings. This captures the constraint where the message from each player can be at most \(\) bits long, and corresponds to the choice \(^{n}=(^{,},,^{ ,})\). Note that allowing a different communication budget to each player can be done by setting \(^{n}=(^{,_{1}},,^{ ,_{n}})\).

Local differential privacy constraintsFor \(>0\) and \(t 1\), a channel \(W^{t-1}\) is _\(\)-locally differentially private (LDP)_ if

\[_{S}_{y^{t-1}^{t-1}}, y^{t-1})}{W(S x_{2},y^{t-1})} e^{}, x_{1},x_{2} .\] (2)

We denote by \(^{,}\) the set of all \(\)-LDP channels. For sequentially interactive protocols, the \(\)-LDP condition is captured by setting \(^{n}=(^{,},,^{ ,})\). As before, one can allow different privacy parameters for each player by setting \(^{n}=(^{,_{1}},,^{,_{n}})\).

Finally, we formalize the interactive parametric estimation problem for the family \(_{}=\{_{},\}\). We consider the problem of estimating \(\) under \(_{p}\) loss. For \(p[1,)\), the \(_{p}\) distance between

Figure 1: The information-constrained distributed model. In the interactive setting, \(W_{t}\) can depend on the previous messages \(Y_{1},,Y_{t-1}\) (dotted, upwards arrows).

\(u,v^{d}\) is \(_{p}(u,v)=\|u-v\|_{p}=(_{i=1}^{d}|u_{i}-v_{i}|^{p}) ^{1/p}.\) This definition extends in a natural way to \(p=\) by taking the limit.3

_Definition 2_ (Sequentially Interactive Estimates).: Fix \(d\) and \(p[0,]\). Given a family \(_{}\) of distributions on \(\), with \(^{d}\), an _estimate_ for \(_{}\) consists of a sequentially interactive protocol \(\) with transcript \((Y^{n},U)\) and estimator \((Y^{n},U)(Y^{n},U)\). The referee observes the transcript \((Y^{n},U)\) and forms the estimate \((Y^{n},U)\) of the unknown \(\). Further, for \(n\) and \((0,1)\), \((,)\) constitutes an \((n,)\)-_estimator_ for \(_{}\) using \(\) under \(_{p}\) loss if for every \(\) the transcript \((Y^{n},U)\) of \(\) satisfies

\[_{_{}^{n}}_{p}(,(Y^{n},U))^{p}^{1/p}.\]

Note that the expectation is over the input \(X^{n}_{}^{n}\) for the protocol \(\) and the randomness of \(\).

## 3 Main result: The information contraction bound

Our main result is a unified framework to bound the information revealed about the unknown \(\) by the transcript of the messages obtained via the constraints defined by the channel family \(\). The framework is versatile and provides tight bounds for several families of continuous and discrete distributions, several families of information constraints such as communication and local differential privacy, and for the family of \(_{p}\) loss functions for \(p 1\).

Our approach at a high-level proceeds as below: We first consider the "pertubation space" \(:=\{-1,+1\}^{k}\), for some suitable \(k\). We associate with each \(z\) a parameter \(_{z}\), and refer to \(_{_{z}}\) simply as \(_{z}\). These distributions are designed in a way that the distance between \(_{z}\) and \(_{z^{}}\) is large when the Hamming distance between \(z\) and \(z^{}\) is large. With this, the difficulty of estimating \(\) will be captured in the difficulty of estimating the associated \(z\). This will make our approach compatible with the standard Assouad's method for deriving lower bounds (_cf_. ).

Then, we let \(Z=(Z_{1},,Z_{k})\) be a random variable over \(\). Under some assumptions on the distribution of \(Z\), we will bound the information between the individual \(Z_{i}\)s and the transcript \((Y^{n},U)\) induced by a family of channels \(\). Combining the two steps above provides us with the desired lower bounds. Formally, let \(:=\{-1,+1\}^{k}\) for some \(k\) and \(\{_{z}\}_{z}\) (where \(_{z}=_{_{z}}\)) be a collection of distributions over \(\), indexed by \(z\). For \(z\), denote by \(z^{ i}\) the vector obtained by flipping the sign of the \(i\)th coordinate of \(z\). To bound the information that can be obtained about the underlying \(z\) from the observations, we make the following assumptions:

**Assumption 1** (Densities Exist).: _For every \(z\) and \(i[k]\) it holds that \(_{z^{ i}}_{z}\), and there exist measurable functions \(_{z,i}\) such that \(_{z^{ i}}}{_{z}}=1+ _{z,i}\)._

The functions \(_{z,i}\) capture the change in density when the coordinate \(i\) is flipped. In our applications below, we will have discrete distributions or continuous densities, and the Radon-Nikodym derivatives above can be replaced with the corresponding ratios between the pmfs and pdfs, respectively.

**Assumption 2** (Orthogonality).: _There exists some \(^{2} 0\) such that, for all \(z\) and distinct \(i,j[k]\), \(_{_{z}}[_{z,i}_{z,j}]=0\) and \(_{_{z}}[_{z,i}^{2}]^{2}\)._

Note that from Assumption 1 we have that \(_{_{z}}[_{z,i}]=0\) for each \(i\). In conjunction with Assumption 2 this implies that for any fixed \(z\), the family \((1,_{z,1},,_{z,k})\) is orthogonal and uniformly bounded in \(L^{2}(,_{z})\). Taken together, Assumption 1 and Assumption 2 roughly say that the densities can be decomposed into uncorrelated "perturbations" across coordinates of \(\). In later sections, we will show that for several families, such as discrete distributions, product Bernoulli distributions, and spherical Gaussians, well-known constructions for lower bounds satisfy these assumptions.

Our first bound given in (3) only requires Assumption 1; by imposing the additional structure of Assumption 2, we obtain the more specialized bound given in (4). Interestingly, (3) can be strengthened further when the following subgaussianity assumption holds.

**Assumption 3** (Subgaussianity).: _There exists some \( 0\) such that, for all \(z\), the random vector \(_{z}(X):=(_{z,i}(X))_{i[k]}^{k}\) is \(^{2}\)-subgaussian for \(X_{z}\).4_

Let \(Z=(Z_{1},,Z_{k})\) be a random variable over \(\) such that \([\,Z_{i}=1\,]=\) for all \(i[k]\) and the \(Z_{i}\)s are all independent; we denote this distribution by \(()^{ k}\). Our main result is an upper bound on the average amount of information that can be obtained about a coordinate of \(Z\) from the transcript \((Y^{n},U)\) of a sequentially interactive protocol, as a function of the information constraint channels and \(_{Z,i}\)s. This result only requires Assumption 1, and is presented below in its most general form, suited to applications beyond those discussed in the current paper.

**Theorem 1** (Information contraction bound: Technical form).: _Fix \((0,1/2]\). Let \(\) be a sequentially interactive protocol using \(^{n}\), and let \(Z\) be a random variable on \(\) with distribution \(()^{ k}\). Let \((Y^{n},U)\) be the transcript of \(\) when the input \(X_{1},,X_{n}\) is i.i.d. with common distribution \(_{Z}\), with density function \(_{Z}^{Y^{n}}\). Then, under Assumption 1,_

\[(_{i=1}^{k}_{}\!(_{+i}^{Y^{n}},_{-i}^{Y^{n}}))^{2}_{i= 1}^{n}_{z}_{W_{i}}_{i=1}^{k}_{ }_{_{z}}[_{z,i}(X)W(y X)]^{2}}{ _{_{z}}[W(y X)]}\!\,\] (3)

_where \(_{+i}^{Y^{n}}:=\![_{Z}^{Y^{n}} Z_{i}=+ 1]\), \(_{-i}^{Y^{n}}:=\![_{Z}^{Y^{n}} Z_{i}=- 1]\)._

We now instantiate this result, invoking Assumptions 2 and 3, to give simple "plug-and-play" bounds which can be applied readily to several inference problems and information constraints.

**Theorem 2**.: _Fix \((0,1/2]\). Let \(\) be a sequentially interactive protocol using \(^{n}\), and let \(Z\) be a random variable on \(\) with distribution \(()^{ k}\). Let \((Y^{n},U)\) be the transcript of \(\) when the input \(X_{1},,X_{n}\) is i.i.d. with common distribution \(_{Z}\). Then, under Assumptions 1 and 2, we have_

\[(_{i=1}^{k}_{}\!(_{+i}^{Y^{n}},_{-i}^{Y^{n}}))^{2}^{2} _{t=1}^{n}_{z}_{W_{t}}_{}_{_{z}}[W(y X)]}{_{_{z}}[W(y X)]}\!\.\] (4)

_Moreover, if Assumption 3 holds as well, we have_

\[(_{i=1}^{k}_{}\!(_{+i}^{Y^{n}},_{-i}^{Y^{n}}))^{2}^{2} _{t=1}^{n}_{z}_{W_{t}}I(_{z };W),\] (5)

_where \(I(_{z};W)\) denotes the mutual information \(I(X;Y)\) between the input \(X_{z}\) and the output \(Y\) of the channel \(W\) with \(X\) as input._

As an illustrative and important corollary, we now derive the implications of this theorem for communication and privacy constraints. For both constraints our tight (or nearly tight) bounds in Section 5 follow directly from these corollaries.

**Corollary 1** (Local privacy constraints).: _For \(=^{,}\) and any family of distributions \(\{_{z},z\{-1,+1\}^{k}\}\) satisfying Assumptions 1 and 2, with the notation of Theorem 2, we have_

\[(_{i=1}^{k}_{}\!(_{+i}^{Y^{n}},_{-i}^{Y^{n}}))^{2}n^{2} (e^{}-1)^{2} e^{}.\] (6)

_Moreover, if Assumption 3 holds as well, we have_

\[(_{i=1}^{k}_{}\!(_{+i}^{Y^{n}},_{-i}^{Y^{n}}))^{2}n^{2}.\] (7)

**Corollary 2** (Communication constraints).: _For any family of channels \(\) with finite output space \(\) and any family of distributions \(\{_{z},z\{-1,+1\}^{k}\}\) satisfying Assumptions 1 and 2, with the notation of Theorem 2, we have_

\[(_{i=1}^{k}_{}\!(_{+i}^{Y^{n}},_{-i}^{Y^{n}}))^{2}n^{2}| |.\] (8)_Moreover, if Assumption 3 holds as well, we have_

\[(_{i=1}^{k}_{}_{+i}^{Y^ {n}},_{-i}^{Y^{n}})^{2}n^{2}| |.\] (9)

## 4 An Assouad-type bound

In the previous section we provided an upper bound on \(_{i=1}^{k}_{}_{+i}^{Y^{n} },_{-i}^{Y^{n}}\). We now prove a lower bound for this quantity in terms of the parameter estimation task we set out to solve. This is an "Assouad's lemma-type" bound, which when combined with Theorem 2 will establish the bounds for \(n\); and, reorganizing, the minimax rate lower bounds. To state the result, we require the following assumption, which relates the \(_{p}\) distance between parameters \(_{z}\)s to the distance between \(z\)s.

**Assumption 4** (Additive loss).: _Fix \(p[1,)\). For every \(z,z^{}\{-1,+1\}^{k}\),_

\[_{p}(_{z},_{z^{}})=4_{ }(z,z^{})}{ k}^{1/p},\]

_where \(_{}(z,z^{}):=_{i=1}^{k}\{z_{i} z _{i}^{}\}\) denotes the Hamming distance._

**Lemma 1** (Assouad-type bound).: _Let \(p 1\) and assume that \(\{_{z},z\}\), \([0,1/2]\) satisfy Assumption 4. Let \(Z\) be a random variable on \(=\{-1,+1\}^{k}\) with distribution \(()^{ k}\). Suppose that \((,)\) constitutes an \((n,)\)-estimator of \(_{}\) using \(^{n}\) under \(_{p}\) loss (see Definition 2) and \(_{Z}[_{Z}_{}\,] 1-/4\). Then the transcript \((Y^{n},U)\) of \(\) satisfies_

\[_{i=1}^{k}_{}_{+i}^{Y^{n }},_{-i}^{Y^{n}},\]

_where \(_{+i}^{Y^{n}}:=_{Z}^{Y^{n}}Z_{i} =+1\), \(_{-i}^{Y^{n}}:=_{Z}^{Y^{n}}Z_{i} =-1\)._

## 5 Applications

We now consider three distribution families: product Bernoulli and Gaussian distributions with identity covariance matrix (and \(s\)-sparse mean vectors), and discrete distributions (multinomials), to illustrate the generality and efficacy of our bounds. We describe these three families below, before addressing each of them in their respective subsection. Due to space constraints, we present the proof of the upper bounds in Appendix C, and proof of the lower bounds in Appendix G.

**Sparse Product Bernoulli (\(_{d,s}\)).**: Let \(1 s d\), \(=\,[-1,1]^{d}:\,\|\|_{0} s\,}\), and \(=\{-1,1\}^{d}\). Let \(_{}:=_{d,s}\) be the family of \(d\)-dimensional \(s\)-sparse product Bernoulli distributions over \(\). Namely, for \(=(_{1},,_{d})\), the distribution \(_{}\) is equal to \(_{j=1}^{d}\,((_{j}+1))\): a distribution on \(\{-1,+1\}^{d}\) such that the marginal distributions are independent, and for which the mean of the \(j\)th marginal is \(_{j}\).
**Sparse Gaussian (\(_{d,s}\)).**: Let \(1 s d\), \(=\,[-1,1]^{d}:\,\|\|_{0} s\,}\), and \(=^{d}\). Let \(_{}:=_{d,s}\) be the family of \(d\)-dimensional spherical Gaussian distributions with bounded \(s\)-sparse mean. That is, for \(\), \(_{}=(,)\) with mean \(\) and covariance matrix \(\). We note that this general formulation assumes \(\|\|_{} 1\) (from the choice of \(\)).5

**Discrete distributions (\(_{d}\)).**: Let \(\ =\ \,^{d}:\,_{i=1}^{d}_{i}=1\,}\ \ ^{d}\) and \(\ =\{1,2,,d\}\). Let \(_{}:=_{d}\), where \(_{d}\) is the standard \((d-1)\)-simplex of all probability mass functions over \(\). Namely, the distribution \(_{}\) is a distribution on \(\), where, for \(j[d]\), the probability assigned to the element \(j\) is \(_{}(j)=_{j}\). For a unified presentation, we view \(\) as the mean vector of the "categorical distribution," namely the distribution of vector \((\{X=x\},x)\) for \(X\) with distribution \(_{}\).

We now define our measure of interest, the minimax error rate of mean estimation.

_Definition 3_ (Minimax rate of mean estimation).: Let \(\) be a family of distributions parameterized by \(^{d}\). For \(p[1,]\), \(n\), and a family of channels \(\), the minimax error rate of mean estimation for \(\) using \(^{n}\) under \(_{p}\) loss, denoted \(_{p}(,,n)\), is the least \((0,1]\) such that there exists an \((n,)\)-estimator for \(\) using \(\) under \(_{p}\) loss (see Definition 2).

We obtain lower bounds on the minimax rate of mean estimation for the different families above by specializing our general bound. Importantly, our methodology is not specific to \(_{p}\) losses, and can be used for arbitrary additive losses such as (squared) Hellinger or, indeed, for any loss function for which an analogue of Lemma 1 can be derived.

Product Bernoulli family.We first establish the following bounds for \(_{d,s}\) under privacy and communication constraints.

**Theorem 3** (Product Bernoulli).: _Fix \(p[1,)\). For \(4 d s d\), \((0,)\), and \( 1\),_

\[}{n(^{2})}} 1 _{p}(_{d,s},^{,},n) }{n(^{2} 1)}}\] (10)

_and_

\[}{n}}{n}} 1 _{p}(_{d,s},^{,},n) }{n}}{n}}\] (11)

_For \(p=\), we have the upper bounds_

\[_{}(_{d,s},^{, },n)=O}} _{}(_{d,s},^{,}, n)=O},\]

_while the lower bounds given in Eqs. (10) and (11) hold for \(p=\), too.6_

_Remark 1_.: Previous work had shown, in the simpler _noninteractive_ model, a rate lower bound scaling as \(\) for the specific case of \(_{2}\) loss (see, for instance, [21, Theorem 7] for the sparse Gaussian case, which implies the Bernoulli one). An analogous phenomenon was observed for local privacy (_e.g._, ). Thus, by removing this logarithmic factor from the upper bound, our result establishes the first (to the best of our knowledge) separation between interactive and noninteractive protocols for sparse mean estimation under communication or local privacy constraints.

_Remark 2_.: Although we stated for simplicity the lower bounds of Theorem 3 in the case where all \(n\) players have a similar local constraints (_i.e._, same privacy parameter \(\), or same bandwidth constraint \(\)), it is immediate to check from the application of Theorem 2 that the result extends to different constraints for each player; replacing \(n(^{2})\) and \(n\) in the statement by \(_{t=1}^{n}_{t}^{2}_{t}\) and \(_{t=1}^{n}_{t}\), respectively. A similar remark applies to the Gaussian and discrete families.

Gaussian family.We derive a lower bound for \(_{p}(_{d,s},,n)\) under local privacy (captured by \(=^{,}\)) and communication (captured by \(=^{,}\)) constraints.7 Recall that for product Bernoulli mean estimation we had optimal bounds for both privacy and communication constraints for all finite \(p\). For Gaussians, we will obtain tight bounds for privacy constraints for \((0,1]\). However, for communication constraints and privacy constraints when \( 1\), our bounds for Gaussian distributions lose a (single) logarithmic factor in some parameter regimes.

**Theorem 4** (Gaussian distributions).: _Fix \(p[1,)\). For \(4 d s d\), under LDP constraints, when \((0,1]\),_

\[}{n^{2}}} 1_{p}( _{d,s},^{,},n)}{n^{2}}}\] (12)

_and when \(>1\),_

\[}{n(nd)}} 1 _{p}(_{d,s},^{,},n) }{n}}\] (13)_Under communication constraints,_

\[}{n(dn)}}{ n}} 1_{p}(_{d,s},^{,},n) }{n}}{n}}\] (14)

_The same bound as in Theorem 3 hold for \(p=\)._

We emphasize that, as discussed in Sections 1.1 and 1.2, to the best of our knowledge our results provides the first lower bounds for interactive Gaussian mean estimation under these constraints.

**Discrete distribution estimation.** We are able to derive a lower bound for \(_{p}(_{d},,n)\), the minimax rate for discrete density estimation, under local privacy and communication constraints. In the interest of space, we focus here on two important corollaries; first, for the case of total variation distance (\(_{1}\)), where combining it with known upper bounds we obtain optimal bounds for all \(>0\). In particular, for \((0,1]\) (high-privacy regime) we retrieve the lower bound established in , which matches the upper bound from . For \(>1\) (low-privacy regime), our bound matches the upper bound for the noninteractive case, established in , showing that even in this low-privacy regime interactivity cannot lead to better rates, except maybe up to constant factors.

**Corollary 3** (Total variation distance).: _For \(>0\), we have_

\[_{1}(_{d},^{,},n)}{n((e^{}-1)^{2} e^{}) }} 1\,.\] (15)

_For \( 1\),_

\[_{1}(_{d},^{,},n)  d)}} 1\,.\] (16)

For the case of \(_{2}\) estimation, we also obtain order-optimal bounds:

**Corollary 4** (\(_{2}\) density estimation).: _For \(>0\), we have_

\[_{2}(_{d},^{,},n)-1)^{2} e^{})}} -1)^{2} e^{})}}  1\,.\] (17)

_For \( 1\),_

\[_{2}(_{d},^{,},n)  d)}}  d)}} 1\,.\] (18)

## 6 Acknowledgment

The authors would like to thank Yanjun Han, and the anonymous reviewers for helpful comments on an earlier version of this paper.