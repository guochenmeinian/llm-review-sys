# Long Sequence Hopfield Memory

Hamza Tahir Chaudhry\({}^{1,2}\), Jacob A. Zavatone-Veth\({}^{2,3}\),

**Dmitry Krotov\({}^{5}\), Cengiz Pehlevan\({}^{1,2,4}\)**

\({}^{1}\)John A. Paulson School of Engineering and Applied Sciences,

\({}^{2}\)Center for Brain Science, \({}^{3}\)Department of Physics,

\({}^{4}\)Kempner Institute for the Study of Natural and Artificial Intelligence,

Harvard University

Cambridge, MA 02138

\({}^{5}\)MIT-IBM Watson AI Lab, IBM Research,

Cambridge, MA 02142

hchaudhry@g.harvard.edu, jzavatoneveth@g.harvard.edu,

krotov@ibm.com, cpehlevan@seas.harvard.edu

###### Abstract

Sequence memory is an essential attribute of natural and artificial intelligence that enables agents to encode, store, and retrieve complex sequences of stimuli and actions. Computational models of sequence memory have been proposed where recurrent Hopfield-like neural networks are trained with temporally asymmetric Hebbian rules. However, these networks suffer from limited sequence capacity (maximal length of the stored sequence) due to interference between the memories. Inspired by recent work on Dense Associative Memories, we expand the sequence capacity of these models by introducing a nonlinear interaction term, enhancing separation between the patterns. We derive novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and verify these theoretical results with numerical simulation. Moreover, we introduce a generalized pseudoinverse rule to recall sequences of highly correlated patterns. Finally, we extend this model to store sequences with variable timing between states' transitions and describe a biologically-plausible implementation, with connections to motor neuroscience.

## 1 Introduction

Memory is an essential ability of intelligent agents that allows them to encode, store, and retrieve information and behaviors they have learned throughout their lives. In particular, the ability to recall sequences of memories is necessary for a large number of cognitive tasks with temporal or causal structure, including navigation, reasoning, and motor control .

Computational models with varying degrees of biological plausibility have been proposed for how neural networks can encode sequence memory . Many of these are based on the concept of associative memory, also known as content-addressable memory, which refers to the ability of a system to recall a set of objects or ideas when prompted by a distortion or subset of them. Modeling associative memory has been an extremely active area of research in computational neuroscience and deep learning for many years, with the Hopfield network becoming the canonical model .

Unfortunately, a major limitation of the traditional Hopfield Network and related associative memory models is its capacity: the number of memories it can store and reliably retrieve scales linearly with the number of neurons in the network. This limitation is due to interference between different memories during recall, also known as crosstalk, which decreases the signal-to-noise ratio. Large amounts of crosstalk results in the recall of undesired attractor states of the network .

Recent modifications of the Hopfield Network, known as Dense Associative Memories or Modern Hopfield Networks (MHNs), overcome this limitation by introducing a strong nonlinearity when computing the overlap between the state of the network and memory patterns stored in the network [30; 31]. This leads to greater separation between partially overlapping memories, thereby reducing crosstalk, increasing the signal-to-noise ratio, and increasing the probability of successful recall .

Most models based on the Hopfield Network are autoassociative, meaning they are designed for the robust storage and recall of individual memories. Thus, they are incapable of storing sequences of memories. In order to adapt these models to store sequences, one must utilize asymmetric weights in order to drive the network from one activity pattern to the next. Many such models use temporally asymmetric Hebbian learning rules to strengthen synaptic connections between neural activity at one time state and the next time state, thereby learning temporal association between patterns in a sequence [1; 3; 10; 11; 16; 17; 22].

In this paper, we extend Dense Associative Memories to the setting of asymmetric weights in order to store and recall long sequences of memories. We work directly with the update rule for the state of the network, allowing us to provide an analytical derivation for the sequence capacity of our proposed network. We find a close match between theoretical calculation and numerical simulation, and further establish the ability of this model to store and recall sequences of correlated patterns. Additionally, we examine the dynamics of a model containing both symmetric and asymmetric terms. Finally, we describe applications of our network as a model of biological motor control.

## 2 DenseNets for Sequence Storage

Traditional Hopfield Networks and MHNs, as described in Appendix B, are capable of storing individual memories. What about storing sequences? Assume that we want to store a sequence of \(P\) patterns, \(^{1}^{2}^{P}\), where \(_{j}^{}\{ 1\}\) is the \(j^{th}\) neuron of the \(^{th}\) pattern and the network will transition from pattern \(^{}\) to \(^{+1}\). Let \(N\) be the number of neurons in the network and \((t)\{-1,+1\}^{N}\) be the state of the network at time \(t\). We want to design a network with dynamics such that when the network is initialized in pattern \(^{1}\), it will traverse the entire sequence.1 We define a network, \(\), which follows a discrete-time synchronous update rule2:

\[T_{SN}()_{i}:=[_{j i}J_{ij}S_{j} ]=[_{=1}^{P}_{i}^{+1}m_{i}^{} ], m_{i}^{}:=_{j i}_{j}^{}S_{j},\] (1)

Figure 1: \(\) and Polynomial \(\) (\(d=2\)) are simulated with \(N=300\) neurons and \(P=100\) patterns. One hundred curves are plotted as a function of time, each representing the overlap of the network state at time \(t\) with one of the patterns, \(m^{}=(1/N)_{i=1}^{N}_{i}^{}S_{i}\). The curves are ordered using the color code described on the right (patterns in the beginning and end of the sequence are shaded in yellow and red respectively). \(\). \(\) quickly loses the correct sequence, indicated by the lack of alignment of the network state with the correct pattern in the sequence (\(m^{} 1\)). \(\). The Polynomial \(\) faithfully recalls the entire sequence and maintains alignment with one of the patterns at any moment in time, \(m^{} 1\).

where \((t+1)=T_{SN}()\) and \(J_{ij}=_{=1}^{P}_{i}^{+1}_{j}^{}\) is an asymmetric matrix connecting pattern \(^{}\) to \(^{+1}\). Note that we are excluding self-interaction terms \(i=j\). We also rewrote the dynamics in terms of \(m_{i}^{}\), the overlap of the network state \(\) with pattern \(^{}\). When the network is aligned most closely with pattern \(^{}\), the overlap \(m_{i}^{}\) is the largest contribution in the sum and pushes the network to pattern \(^{+1}\). When multiple patterns have similar overlaps, meaning they are correlated, then there will be low signal-to-noise ratio. This correlation between patterns limits the capacity of the network, limiting the SeqNet's capacity to scale linearly relative to network size.

To overcome the capacity limitations of the SeqNet, we use inspiration from Dense Associative Memories  to define the DenseNet update rule:

\[T_{DN}()_{i}:=[_{=1}^{P}_{i}^{+1}f (m_{i}^{})]\] (2)

where \(f\) is a nonlinear monotonically increasing interaction function. Similar to MHNs, \(f\) reduces the crosstalk between patterns and, as we will analyze in detail, leads to improved capacity. Figure 1 demonstrates this improvement for \(f(x)=x^{2}\).

### Sequence capacity

To derive analytical results for the capacity, we must choose a distribution to generate the patterns. As is standard in studies of the classic HN and MHNs [26; 27; 28; 29; 30; 31; 33; 34; 35; 36], we choose this to be the Rademacher distribution, where \(_{j}^{}\{-1,+1\}\) with equal probability for all neurons \(j\) in all patterns \(\), and calculate the capacity for different update rules. If one is allowed to specially engineer the patterns, even the SeqNetcan store a sequence of length \(2^{N}\), but this construction is not relevant to associative recall of realistic sequences. Rademacher patterns are a more appropriate model for generic patterns while remaining theoretically tractable.

We consider both the robustness of a single transition, and the robustness of propagation through the full sequence. For a fixed network size \(N\{2,3,\}\) and an error tolerance \(c[0,1)\), we define the single-transition and sequence capacities by

\[P_{T}(N,c)=\{P\{2,,2^{N}\}:[_{DN}( ^{1})=^{2}] 1-c\}\] (3)

and

\[P_{S}(N,c)=\{P\{2,,2^{N}\}:[_{=1}^{P} \{_{DN}(^{})=^{+1}\}]  1-c\},\] (4)

respectively, where the probability is taken over the random patterns. Note that for the single-transition capacity we could focus on any pair of subsequent patterns due to translation invariance arising from periodic boundary conditions. Also note that the full sequence capacity is defined by demanding that all transitions are correct. For perfect recall, we want to take the threshold \(c 0\). In the thermodynamic limit in which \(N,P\), we expect for there to exist a sharp transition in the recall probabilities as a function of \(P\), with almost-surely perfect recall below the threshold value and vanishing probability of recall above [26; 27; 28; 29; 31; 33; 34; 35; 36]. Thus, we expect the capacity to become insensitive to the value of \(c\) in the thermodynamic limit; this is known rigorously for the classic Hopfield network from the work of Bovier .

As we detail in Appendix C, all of our theoretical results are obtained under two approximations. We will validate the accuracy of the resulting capacity predictions through comparison with numerical experiments. First, following Petritis 's analysis of the classic Hopfield network, we use union bounds to control the single-transition and full-sequence capacities in terms of the single-biffling error probability \([T_{DN}(^{1})_{1}_{1}^{2}]\). Using the fact that the patterns are i.i.d., this gives \([_{DN}(^{})=^{+1}]  1-N[T_{DN}(^{1})_{1}_{2}^{1}]\) and \([_{=1}^{P}\{_{DN}(^{})= ^{+1}\}] 1-N[T_{DN}(^{1})_{1} _{2}^{1}]\), respectively, resulting in the lower bounds

\[P_{T}(N,c) \{P\{2,,2^{N}\}:N[T_{DN}( ^{1})_{1}_{2}^{1}] c\},\] (5) \[P_{S}(N,c) \{P\{2,,2^{N}\}:N[T_{DN} (^{1})_{1}_{2}^{1}] c\}.\] (6)

From studies of the classic Hopfield network, we expect for these bounds to be tight in the thermodynamic limit (\(N\)), but we will not attempt to prove that this is so [33; 34]. Second, our theoretical results are obtained under the approximation of \([T_{HN}(^{1})_{1}_{1}^{2}]\) in the regime \(N,P 1\) by a Gaussian tail probability. Concretely, we write the single-bitflip probability as

\[[T_{DN}(^{1})_{1}_{1}^{2}]=[C<-f(1)]\] (7)

in terms of the crosstalk

\[C=_{=2}^{P}_{1}^{2}_{1}^{+1}f_{j=2}^{ N}_{j}^{}_{j}^{1},\] (8)

which represents interference between patterns that can lead to a bitflip. Then, as the crosstalk is the sum of \(P-1\) i.i.d. random variables, we approximate its distribution as Gaussian. We then extract the capacity by determining how \(P\) should scale with \(N\) such that the error probability tends to zero as \(N\), corresponding to taking \(c 0\) with increasing \(N\). Within the Gaussian approximation, we can also estimate the capacity at fixed \(c\) by using the asymptotics of the inverse Gaussian tail distribution function to determine how \(P\) should scale with \(N\) such that the error probability is asymptotically bounded by \(c\) as \(N\). This predicts that the effect of non-negligible \(c\) should vanish as \(N\).

For \(P\) large but finite, this Gaussian approximation amounts to retaining only the leading term in the Edgeworth expansion of the tail distribution function [38; 39; 40; 41]. We will not endeavour to rigorously control the error of this approximation in the regime of interest in which \(N\) is also large. To convert our heuristic results into fully rigorous asymptotics, one would want to construct an Edgeworth-type series expansion for the tail probability \([C<-f(1)]\) that is valid in the joint limit with rigorously-controlled asymptotic error, accounting for the fact that the crosstalk is a sum of discrete random variables [38; 40; 41]. As a simple probe of Gaussianity, we will consider the excess kurtosis of the crosstalk distribution, which determines the leading correction to the Gaussian approximation in the Edgeworth expansion, and describes whether its tails are heavier or narrower than Gaussian [38; 39; 40; 41].

### Polynomial DenseNet

Consider the DenseNet with polynomial interaction function, \(f(x)=x^{d}\), which we will call the Polynomial DenseNet. In Appendix C.1, we argue that the leading asymptotics of the transition and sequence capacities for perfect recall are given by

\[P_{T}}{2(2d-1)!!(N)}, P_{S}}{2(d+1)(2 d-1)!!(N)}.\] (9)

Note that this polynomial scaling of the single-transition capacity with network size coincides with the capacity scaling of the symmetric MHN . Indeed, as we have excluded self-interaction terms in the update rule, the single-bitflip probabilities for these two models coincide exactly for unbiased Radamacher patterns (Appendix C.1). This allows us to adapt arguments from Demircigil et al.  to show that (9) is in fact a rigorous asymptotic lower bound on the capacity (Appendix D). We compare our results for the single-transition and sequence capacities to numerical simulation in Figure 2. The simulation matches theoretical prediction for large network size \(N\). For smaller \(N\), there are finite-size effects that result in deviation from theoretical prediction. The crosstalk has non-negligible kurtosis in finite size networks which leads to deviation from the Gaussian approximation.

Furthermore, we point out that for fixed \(N\), the network capacity does not monotonically increase in the degree \(d\). Since the factorial function grows faster than the exponential function, every finite network of size \(N\) has a polynomial degree \(d_{max}\) after which the capacity will actually decrease. This is also true for the standard MHN. We demonstrate this numerically in Figure 2B, again noting mild deviations between theory and simulation due to finite-size effects.

### Exponential DenseNet

We have shown the DenseNet's capacity can scale polynomially with network size. Can it scale exponentially? Consider the DenseNet with exponential interaction function, \(f(x)=e^{(N-1)(x-1)}\), which we call the Exponential DenseNet. This function reduces crosstalk dramatically: \(f(m^{}())=1\) when \(m^{}()=1\) and is otherwise sent to zero exponentially fast. In Appendix C.2, we show that under the abovementioned approximations one has the leading asymptotics

\[P_{T}}{2 N} P_{S}}{2()N},=  1.964\] (10)In Figure 2, numerical simulations confirm this model scales significantly better than the Polynomial DenseNet and enables one to store exponentially long sequences relative to network size. While the ratio between transition and sequence capacities remains bounded for the Polynomial DenseNet, where \(P_{T}/P_{S} d+1\), the gap for the Exponential DenseNet diverges with network size.

However, we can see in Figure 2A that the empirically measured capacity--particularly the sequence capacity--of the Exponential DenseNet deviates substantially from the predictions of our approximate Gaussian theory. Due to computational constraints, our numerical simulations are limited to small network sizes (Appendix G). Computing the excess kurtosis of the crosstalk distribution with a number of patterns comparable to the capacity predicted by the Gaussian theory reveals that, for the range of system sizes we can simulate, the distribution should deviate strongly from a Gaussian. In particular, if take \(P^{N-1}/( N)\) for some constant factor \(\), then the excess kurtosis increases with network size up to around \(N 56\) (Appendix C.2). Increasing the size of an Exponential DenseNet therefore has competing effects: for a fixed sequence length \(P\), increasing network size \(N\) decreases the crosstalk variance, which should reduce the bitflip probability, but also increases the excess kurtosis, which reflects a fattening of the crosstalk distribution tails that should increase the bitflip probability. This is illustrated in Figure 2C.

The competition between increasing \(P\) and \(N\) for the Exponential DenseNet is easy to understand intuitively. For a fixed \(N\), increasing \(P\) means that the crosstalk is equal in distribution to the sum of an increasingly large number of i.i.d. random variables, and thus by the central limit theorem should become increasingly Gaussian. Conversely, for a fixed \(P\), increasing \(N\) means that each of the

Figure 2: Testing the transition and sequence capacities of DenseNets with polynomial and exponential nonlinearities. **A**. Scaling of transition capacity (\(_{10}(P_{T})\), _left_) and sequence capacity (\(_{10}(P_{S})\), _right_) with network size. As network size increases, the variance of the crosstalk decreases and the theoretical approximations become more accurate, resulting in a tight match between theory (solid lines) and simulation (points with error bars). The theory curves are given by Equations 9 and 10. Error bars are computed across realizations of the random patterns (see Appendix G). There is significant deviation between theory and simulation for the sequence capacity of the Exponential DenseNet. We show that this is due to finite-size effects in Section 2.3. **B**. Transition capacity of Polynomial DenseNets as a function of degree. For any finite network size \(N\), there is a degree \(d\) that maximizes the transition capacity. The same would be true for the sequence capacity. **C**. Crosstalk variance (_left_) and excess kurtosis (_right_) for the Exponential DenseNet as a function of \(P\) and \(N\). Variance is proportional to \(P\) and inversely proportional to \(N\), while the opposite is true for excess kurtosis. See Appendix G for details of our numerical methods.

\(P-1\) contributions to the crosstalk is equal in distribution to the _product_ of an increasing number of i.i.d. random variables--as \(f_{j=2}^{N}_{j}^{}_{j}^{1}=_{j=2}^{N }(_{j}^{}_{j}^{1})\)--and thus by the multiplicative central limit theorem each term should tend to a lognormal distribution. In this regime, then, the crosstalk is roughly a mixture of lognormals, which is decidedly non-Gaussian. In contrast, for a Polynomial DenseNet, memorization is easy in the limit where \(N\) tends to infinity for fixed \(P\), as the crosstalk should tend almost surely to zero as each term \(f_{j=2}^{N}_{j}^{}_{j}^{1} 0\) almost surely.

### Recalling Sequences of Correlated Patterns

The full-sequence capacity scaling laws for these models were derived under the assumption of i.i.d Rademacher random patterns. While theoretically convenient, this is unrealistic for real-world data. We therefore test these networks in more realistic settings by storing correlated sequences of patterns, which will lead to greater crosstalk in each transition and thus smaller single-transition and full-sequence capacities relative to network size [26; 36]. However, the nonlinear interaction functions should still assist in separating correlated patterns to enable successful sequence recall.

For demonstration, we store a sequence of \(200000\) highly-correlated images from the MovingMNIST dataset and attempt to recall this sequence using DenseNets with different nonlinearities . The entire sequence is composed of \(10000\) unique subsequences concatenated together, where each subsequence is composed of 20 images of two hand-written digits slowly moving through one another. This means there is significant correlation between patterns which will result in large amounts of crosstalk. The results of the DenseNets are shown in Figure 3A, where increasing the nonlinearity of the Polynomial DenseNets slowly improves recall but not entirely, while the exponential network achieves perfect recall. The SeqNet and DenseNets, up until approximately \(d=50\), are entirely unable to recall any part of any image, despite the DenseNets being well within the capacity limits predicted by theoretical calculations on uncorrelated patterns.

### Generalized pseudoinverse rule

Can we overcome the DenseNet's limited ability to store correlated patterns? Drawing inspiration from the pseudoinverse learning rule introduced by Kanter and Sompolinsky  for the classic Hopfield network, we propose a generalized pseudoinverse (GPI) transition rule

\[T_{GPI}()_{i}:=[_{=1}^{P}_{i}^{+1}f (_{=1}^{P}(O^{+})^{}m^{}())], O ^{}=_{j=1}^{N}_{j}^{}_{j}^{},\] (11)

Figure 3: **A**. Recall of a sequence of \(200000\) correlated images from the MovingMNIST dataset using DenseNets of size \(N=784\). We showcase a 10 image subsequence. The top row depicts the true sequence, the second row depicts SeqNet’s performance, the next rows depict the Polynomial DenseNets’ performance which increases with degree \(d\), and the final row depicts the Exponential DenseNet’s performance which yields perfect recall. **B**. Transition capacity of Polynomial DenseNets of size \(N=100\) relative to pattern bias \(\). Increasing \(\) monotonically decreases capacity. Networks with stronger nonlinearities maintain high capacity for large correlation strength. Implementing the generalized pseudoinverse rule decorrelates these patterns and maintains high sequence capacity for much larger correlation. See Appendix G for details of numerical methods.

where the overlap matrix \(O^{}\) is positive-semidefinite, so we can define its pseudoinverse \(^{+}\) by inverting the non-zero eigenvalues. With \(f(x)=x\), this reduces to the pseudoinverse rule of .

If the patterns are linearly independent, such that \(\) is full-rank, we can see that this rule can perfectly recall the full sequence (Appendix E). This matches the classic pseudoinverse rule's ability to perfectly store any set of linearly independent patterns; this is why we choose to sum over \(\) inside the separation function in (11). For i.i.d. Rademacher patterns, linear independence holds almost surely in the thermodynamic limit provided that \(P<N\).

In Figure 3B, we demonstrate the effect of correlation on the Polynomial DenseNet through studying the recall of biased patterns \(_{i}^{}\) with \((_{i}^{}= 1)=(1)\) for \([0,1)\).3 We see that the Polynomial DenseNet has better recall at all levels of bias \(\) as degree \(d\) increases, although we still expect there to be a maximum degree as described before. However, at large correlation values, they all have low recall, suggesting the need for alternative methods to decorrelate these patterns. This failure is easy to understand theoretically, following van Hemmen and Kuhn 's analysis of the classic Hopfield model: for patterns with bias \(\), the Polynomial DenseNet update rule expands as

\[T_{DN}(^{})_{i}=[_{i}^{+1}+(P-1)^{2d+1} +()].\] (12)

Therefore, even if \(N\) is large, for \( 0\) there must be some value of \(P\) for which the constant bias overwhelms the signal. If \(N\) for any fixed \(P\), then we must have \(P<^{-(2d+1)}+1\) for the signal to dominate. In Figure 3B, we show the generalized pseudoinverse update rule is more robust to large correlations than the Polynomial DenseNet. While this rule can also be applied to the Exponential DenseNet, simulations fail due to numerical instability coming from small values in the pseudoinverse.

## 3 MixedNets for variable timing

Thus far, we have considered sequence recall in purely asymmetric networks. These networks transition to the next pattern in the sequence at every timestep, preventing the network from storing sequences with longer timing between elements. In this section, we aim to construct a model where the network stays in a pattern for \(\) steps. Our starting model will be an associative memory model for storing sequences known as the Temporal Association Network (TAN) , defined as:

\[T_{TAN}()_{i}:=[_{=1}^{P}[_{i}^{ }m_{i}^{}+_{i}^{+1}_{i}^{}]],_{i}^{}:=_{j i}_{j}^{}_{j}\] (13)

where \(_{i}^{}\) represents the normalized overlap of each pattern \(^{}\) with a weighted time-average of the network over the past \(\) timesteps, \(_{i}(t)=_{=0}^{}w()S_{i}(t-)\). The weight function, \(w(t)\), is generally taken to be a low-pass convolutional filter (e.g. Heaviside step function, exponential decay).

This network combines a symmetric and asymmetric term for robust recall of multiple sequences. The symmetric term containing \(m_{i}^{}(t)\), also referred to as a "fast" synapse, stabilizes the network in pattern \(^{}\) for a desired amount of time. The asymmetric term containing \(_{i}^{}(t)\), also referred to as a "slow" synapse, drives the network transition to pattern \(^{+1}\). The \(\) parameter controls the strength of the transition signal. If \(\) is too small, no transitions will occur since the symmetric term will overpower it. If \(\) is too large, transitions will occur too quickly for the network to stabilize in a desired pattern and the sequence will quickly destabilize.

For TAN, Sompolinsky and Kanter  used numerical simulations to estimate the capacity as approximately \(P_{TAN} 0.1N\), defining capacity as the ability to recall the sequence in correct order with high overlap (meaning that a small proportion of incorrect bits are allowed in each transition). Note that this model can fail in two ways: (i) it can fail to recall the correct sequence of patterns, or (ii) it can fail to stay in each state for the desired amount of time.

To address these issues, we consider the following dynamics:

\[T_{MN}()_{i}:=[\,_{=1}^{P}[_{i}^{ }f_{S}(m_{i}^{})+_{i}^{+1}f_{A}(_{i}^{ })\,]]\] (14)We call this model the MixedNet, and seek to analyze the relationship between the symmetric and asymmetric terms in driving network dynamics and their impact on sequence capacity. As before, the asymmetric term will try to push the network to the next state at every timestep, while the symmetric term tries to maintain it in its current state for \(\) timesteps. We will allow different nonlinearities for \(f_{S}\) and \(f_{A}\), and analyze their effect on transition and sequence capacity.

We demonstrate the effectiveness of the Polynomial MixedNet, where for simplicity we set \(f_{S}(x)=f_{A}(x)=x^{d}\), in Figure 4A. While TAN fails completely, a polynomial nonlinearity of \(d=2\) enables recall of pattern order but the network does not stay in each pattern for \(=5\) timesteps. Further increasing the nonlinearity to \(d=10\) recovers the desired sequence with correct order and timing.

Theoretical analysis of the capacity of the MixedNet (14) for general memory length \(\) is challenging due to the extended temporal interactions. We therefore consider single-step memory (\(=1\)), and show that even in this relatively tractable special case new complications arise relative to our analysis of the DenseNet. Alternatively, we can interpret the MixedNet with \(=1\) as an imperfectly-learned DenseNet. If one imagines the network learns its weights through a temporally asymmetric Hebbian rule with an extended plasticity kernel, and its state is not perfectly clamped to the desired transition, the coupling from \(^{}\) to \(^{+1}\) could be corrupted by coupling \(^{}\) to itself .

We first consider the setting where both interaction functions are polynomial, \(f_{S}(x)=x^{d_{S}}\) and \(f_{A}(x)=x^{d_{A}}\), and refer to this network as the Polynomial MixedNet. This model is analyzed in detail in Appendix F.1. Interestingly, this model's crosstalk variance forms a bimodal distribution, as shown in Figure F.1. This complicates the analysis, but once bimodality is accounted for one can approximate the capacity using a similar argument to that of the DenseNet. We find that

\[P_{T}}{2_{d_{S},d_{A}}},d_ {A}\}}}{ N}, P_{S}}{2(\{d_{S},d_{A}\}+ 1)_{d_{S},d_{A}}},d_{A}\}}}{ N},\] (15)

Figure 4: Capacity of the Polynomial MixedNet. **A**. We simulate MixedNets with \(N=100\), \(=5\), and attempt to store \(P=40\) patterns. The Temporal Association Network (_left_), corresponding to a linear MixedNet with \(d_{S}=1=d_{A}\), fails to recover the sequence. Increasing the nonlinearities to \(d_{S}=2=d_{A}\) (_center_) recovers the correct sequence order, but not the timing. Increasing the nonlinearities to \(d_{S}=10=d_{A}\) (_right_) recovers the correct sequence order and timing. **B**. Transition capacity \(_{10}(P_{T})\) of the Polynomial MixedNet as a function of network size. Each panel has a fixed symmetric nonlinearity \(f_{S}(x)=x^{d_{S}}\) indicated by the panel’s title. As network size increases, crosstalk variance decreases and theoretical approximations in Equation 3 become more accurate to tightly match the simulations. Note that as expected, the capacity scales according to the minimum of \(d_{S}\) and \(d_{A}\). **C**. As in **B**, but for the sequence capacity \(_{10}(P_{S})\).

where \(_{d_{S},d_{A}}\) is a multiplicative factor defined as

\[_{d_{S},d_{A}}=(2d_{S}-1)!!&d_{S}<d_{A}\\ (^{2}+1)(2d_{S}-1)!!+2[(d_{S}-1)!!]^{2}\{d_{S}\}&d_{S}=d_{A}\\ ^{2}(2d_{A}-1)!!&d_{S}>d_{A}.\] (16)

In Figure 4B-C, we show that simulations match the theory curves well as \(N\) increases. We demonstrate theoretical and simulations results for the Exponential MixedNet in Appendix F.2.

## 4 Biologically-Plausible Implementation

Since biological neural networks must store sequence memories [2; 5; 6; 7; 8], one naturally asks if these results can be generalized to biologically-plausible neural networks. A straightforward biological interpretation of the DenseNet is problematic, as a network with polynomial interaction function of degree \(d\) is equivalent to having a neural network with many-body synapses between \(d+1\) neurons. This can be seen by expanding the Polynomial DenseNet in terms of a weight tensor of \(d+1\) neurons:

\[S_{i}(t+1)=[_{j_{1},,j_{d}}J_{ij_{1} j _{d}}S_{j_{1}}(t) S_{j_{d}}(t)], J_{i,j_{1},,j_{d}}= }_{=1}^{P}_{i}^{+1}_{j_{1}}^{}_{j_{d}} ^{}\] (17)

This is biologically unrealistic as synaptic connections usually occur between two neurons . In the case of the Exponential DenseNet, one can interpret its interaction function via a Taylor series expansion, implying synaptic connections between infinitely many neurons which is even more problematic. Similar difficulties arise in models with sum of terms with different powers .

To address this issue, we again take inspiration from earlier work in MHNs. Krotov and Hopfield  addressed this concern for symmetric MHNs by reformulating the network using two-body synapses, where the network was partitioned into a bipartite graph with visible and hidden neurons (see  for an extension of this idea to deeper networks). The visible neurons correspond to the neurons in our network dynamics, \(_{j}\), while the hidden neurons correspond to the individual memories stored within the network. They are connected through a weight matrix. Since we are working with an asymmetric network, we modify their approach and define two sets of synaptic weights: \(W_{j}\) connects visible neuron \(v_{j}\) to hidden neuron \(h_{}\), of DenseNet with two-body synapses.

\(M_{ j}\) connects hidden neuron \(h_{}\) to visible neuron \(v_{j}\). This yields the same dynamics exhibited in Equation (2), absorbing the nonlinearity into the hidden neurons' dynamics.

For the DenseNet, we define the weights as \(W_{j}:=_{j}^{}\) and \(M_{ j}:=_{j}^{+1}\). For the MixedNet, we redefine the weight matrix \(M_{ j}=_{j}^{}+_{j}^{+1}\). The update rules for the neurons are as follows:

\[h_{}(t):=f_{j}W_{j}v_{j}(t), v_{j}(t+1):= _{}M_{ j}h_{}(t)\] (18)

Note that these networks' transition and sequence capacities, \(P_{T}\) and \(P_{S}\), now scale linearly with respect to the total number of neurons in this model, \(N\) visible neurons and \(P\) hidden neurons. However, the network capacity still scales nonlinearly with respect to the number of visible neurons.

Finally, we remark that this network is reminiscent of recent computational models for motor action selection and control via the cortico-basal ganglia-thalamo-cortical loop, in which the basal ganglia inhibits thalamic neurons that are bidirectionally connected to a recurrent cortical network [5; 49; 50]. This relates to our model as follows: the motor cortex (visible neurons) executes an action, each

Figure 5: Biologically-plausible implementation of DenseNet with two-body synapses.

thalamic unit (hidden neurons) encodes a motor motif, and the basal ganglia silences thalamic neurons (external network modulating context). In particular, the role of the basal ganglia in this network suggests a novel mechanism of context-dependent gating within Hopfield Networks . Rather than modulating synapses or feature neurons in a network, one can directly inhibit (activate) memory neurons in order to decrease (increase) the likelihood of transitioning to the associated state. Similarly, thalamocortical loops have been found to be important to song generation in zebra finches . Thus, the biological implementation of the DenseNet can provide insight into how biological agents reliably store and generate complex sequences.

## 5 Discussion and Future Directions

We introduced the DenseNet for the reliable storage and recall of long sequences of patterns, derived the scaling of its single-transition and full-sequence capacity, and verified these results in numerical simulation. We found that depending on the choice of nonlinear interaction function, the DenseNet could scale polynomially or exponentially. We tested the ability of these models to recall sequences of correlated patterns, by comparing the recall of a sequence of MovingMNIST images with different nonlinearities. As expected, the network's reconstruction capabilities increased with the nonlinearity power \(d\), with perfect recall achieved by the exponential nonlinearity. To further increase the capacity, we introduced the generalized pseudoinverse rule and demonstrated in simulation its ability to maintain high capacity for highly correlated patterns. We also introduced and analyzed the MixedNet to maintain patterns within sequences for longer periods of time. Finally, we described a biologically plausible implementation of the models with connections to motor control.

There has recently been a renewed interest in storing sequences of memories. Steinberg and Sompolinsky  store sequences in Hopfield networks by using a vector-symbolic architecture to bind each pattern to its temporal order in the sequence, thus storing the entire sequence as a single attractor. However, this model suffers from the same capacity limitations as the Hopfield Network. Whittington et al.  suggest a mechanism to control sequence retrieval via an external controller, analogous to the role we ascribe to the basal ganglia for context-dependent gating. Herron et al.  investigate a mechanism for robust sequence recall within complex systems more broadly, reducing crosstalk by directly modulating interactions between neurons rather than the inputs into neurons. Tang et al.  propose a model for sequential recall akin to SeqNet with an implicit statistical whitening process. Karuvally et al.  introduce a model closely related to the biologically-plausible implementation of our MixedNet and analyze it in the setting of continuous-time dynamics, allowing for intralayer synapses within the hidden layer and different timescales between the hidden and feature layers.

While we have focused on a generalization of the fixed-point capacity for sequence memory, this is not the only notion of capacity one could consider. In other studies of MHNs, instead of considering stability as the probability of staying at a fixed point, researchers quantify the probability that the network will reach a fixed point within a single transition [58; 59; 31]. This approach allows one to quantify noise-robustness and the size of each memory's basin of attraction . More broadly, one could consider other definitions of associative memory capacity not addressed here, including those that depend only on network architecture and not on the assumption of a particular learning rule [60; 61]. However, as compared to the relatively simple analysis that is possible for the fixed-point capacity of a Hopfield network using a Hebbian learning rule, analyzing these alternative notions of capacity in nonlinear networks can pose significant technical challenges [61; 62; 63].

In this work, we limited ourselves to theoretical analysis of discrete-time networks storing binary patterns. An important direction for future research would be to go beyond the Gaussian theory in order to develop accurate predictions of the Exponential DenseNet capacity. There are also many potential avenues for extending these models and methods to continuous-time networks, continuous-valued patterns, computing capacity for correlated patterns, testing different weight functions, and examining different network topologies. Finally, we hope to take inspiration from the recent resurgence of RNNs in long sequence modeling to use this model for real-world tasks [64; 65].