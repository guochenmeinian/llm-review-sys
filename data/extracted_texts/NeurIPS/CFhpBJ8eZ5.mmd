# Dual Mean-Teacher: An Unbiased Semi-Supervised Framework for Audio-Visual Source Localization

Yuxin Guo\({}^{1,2,3}\), Shijie Ma\({}^{1,2}\), Hu Su\({}^{1,2}\), Zhiqing Wang\({}^{1,2}\), Yuhao Zhao\({}^{1,2}\), Wei Zou\({}^{1,2}\)

Siyang Sun\({}^{3}\), Yun Zheng\({}^{3}\)

\({}^{1}\)School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China

\({}^{2}\)State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS),

Institute of Automation of Chinese Academy of Sciences, Beijing, China

\({}^{3}\)DAMO Academy, Alibaba Group

{guoyuxin2021, wei.zou}@ia.ac.cn

Corresponding author.

###### Abstract

Audio-Visual Source Localization (AVSL) aims to locate sounding objects within video frames given the paired audio clips. Existing methods predominantly rely on self-supervised contrastive learning of audio-visual correspondence. Without any bounding-box annotations, they struggle to achieve precise localization, especially for small objects, and suffer from blurry boundaries and false positives. Moreover, the naive semi-supervised method is poor in fully leveraging the information of abundant unlabeled data. In this paper, we propose a novel semi-supervised learning framework for AVSL, namely **Dual Mean-Teacher (DMT)**, comprising two teacher-student structures to circumvent the _confirmation bias_ issue. Specifically, two teachers, pre-trained on limited labeled data, are employed to filter out noisy samples via the consensus between their predictions, and then generate high-quality pseudo-labels by intersecting their confidence maps. The sufficient utilization of both labeled and unlabeled data and the proposed unbiased framework enable DMT to outperform current state-of-the-art methods by a large margin, with CIoU of **90.4%** and **48.8%** on Flickr-SoundNet and VGG-Sound Source, obtaining **8.9%**, **9.6%** and **4.6%**, **6.4%** improvements over self- and semi-supervised methods respectively, given only \(<3\%\) positional-annotations. We also extend our framework to some existing AVSL methods and consistently boost their performance. Our code is available at https://github.com/gyx-gloria/DMT.

## 1 Introduction

Visual and auditory perception is crucial for observing the world. When we hear a sound, our brain will extract semantic information and locate the sounding source. In this work, we focus on Audio-Visual Source Localization (AVSL) , with the purpose of accurately locating sounding objects in frames based on their paired audio clips. Beyond this scope, AVSL also plays a crucial role in many downstream tasks including environmental perception , navigation , sound separation  and event localization . Therefore, accurate localization is of utmost importance.

In the literature of AVSL , the conventional paradigm is to employ self-supervised contrastive learning based on audio-visual correspondence. However, most of them suffer from some serious challenges. From the performance perspective, there are issues such as blurry boundaries, inability to converge to specific objects, and the predicted sounding regions that are too large to accurately locate objects, especially _small objects_. In terms of the learning stage, a single model alone is unable to recognize and filter out _false positives_, _i.e_., noisy samples with no visible sounding sources, which could affect the entire learning process of the model.

In essence, AVSL is a dense prediction task, which can not be directly accomplished from a shared global image representation , requiring models to capture fine local features in order to accurately predict object locations, _i.e_., achieving precise pixel-level localization is not feasible without positional annotations. Unluckily, the number of samples with location labels is extremely limited. As a result, we resort to Semi-Supervised Learning  (SSL) to fully leverage the labeled data.

Considering that self-supervised AVSL is not fully learnable, Attention10k [14; 15] extended the self-supervised model to an SSL model by directly appending a supervised loss on labeled data, which is the first semi-supervised attempt in the field. Nevertheless, simply leveraging labeled data might lead to overfitting and neglect to fully harness the underlying unlabeled data. Given these issues, we resort to pseudo-labeling . However, directly introducing pseudo-labeling could lead to _confirmation bias_ which cannot be adequately rectified by a single model.

To tackle these challenges, we break away from traditional self-supervised learning and propose a more sophisticated Semi-Supervised Audio-Visual Source Localization (SS-AVSL) framework, called Dual Mean-Teacher (DMT), which adopts a double teacher-student structure in a two-stage training manner. We consider previous AVSL methods as a single student unit. To fully leverage positional annotations and training data, we extend it to a classic semi-supervised framework Mean-Teacher . To address the issue of _confirmation bias_, we expand it into a dual independent teacher-student structure with designed modules of Noise Filtering, Intersections of Pseudo-Labels (IPL), as shown in Figure 2. Specifically, teachers are pre-trained on a limited amount of labeled data in Warm-Up stage, establishing a solid foundation, in the subsequent Unbiased-Learning Stage, dual teachers filter out noisy samples and rectify pseudo-labels. In more detail, the Noise Filtering module effectively rejects noise samples by leveraging consensus, _i.e_., agreement, between dual teachers, ensuring high-quality training data, then IPL module generates precise pseudo-labels by intersecting the predictions from both teachers. DMT eliminates the influence of _confirmation bias_ by rejecting noisy samples and improving the quality of pseudo-labels, which effectively tackles the issues of false positives and greatly improves localization performance.

In summary, our method contributes to the following three aspects. **Firstly,** we introduce a novel unbiased framework based on a pseudo-labeling mechanism for semi-supervised AVSL, which could maximize the utilization of both labeled and unlabeled data, effectively address the challenge of limited annotated data, and mitigate the issue of _confirmation bias_. **Moreover,** compared to existing approaches, DMT achieves much remarkable localization performance, with better small object localization and stronger generalization capability, which significantly elevate the performance of current methods. **Finally,** DMT can be summarized as a semi-supervised learning paradigm and could be combined with existing (weakly-supervised) AVSL methods to consistently boost their performance.

## 2 Related Works

Semi-Supervised Learning.Semi-Supervised Learning (SSL) [13; 19] leverages a small amount of labeled data to unlock the potential of unlabeled data for better model learning. One line of work relies on consistency regularization [18; 20; 21] to encourage models to behave similarly under different perturbations. An orthogonal idea is to generate high-quality pseudo-labels [16; 22] on

Figure 1: Comparison of existing Audio-Visual Source Localization (AVSL) methods and the proposed Dual Mean-Teacher (DMT). **Left:** DMT has greatly addressed severe issues including inaccurate small object localization, blurry boundaries, and instability. **Right:** DMT outperforms previous by a large margin on Flickr and VGG-ss datasets.

unlabeled data to retrain models for better performance. The quality of pseudo-labels is crucial. Current methods [23; 24; 25] combine the above two paradigms to achieve remarkable results.

Audio-Visual Source Localization.The key to Audio-Visual Sound Localization (AVSL) endeavors to establish the correspondence between visual objects and their corresponding sounds by contrastive learning [26; 27]. Most existing methods predominantly utilize self-supervised or weakly-supervised approaches (for all of them employ pre-trained backbone). Some classical works, such as Attention10k [14; 15], DMC , LVS , EZVSL , SSPL , SSL-TIE  achieve improving performance over time. Other methods like DSOL , CoarsetoFine , mix and localize , and AVGN  pay attention to multi-source localization. In addition, some studies also address the issue of false positives and false negatives in self-supervised learning. For example, SLAVC  focuses on false positives and effectively overcomes the overfitting issue. IER  proposes a label-free method that targets the suppression of non-sounding objects, while Robust  considers both false positive and false negative issues. AVID  detects false negatives by defining sets of positive and negative samples via cross-modal agreement. Source separation [37; 38] and generative models  also achieve good results. However, most AVSL methods exhibit subpar performance in the absence of annotation.

Semi-Supervised Learning in Localization.Semi-Supervised Object Detection (SSOD) is one of the few applications of SSL in the localization field. The majority of SSOD methods, such as [40; 41; 42], utilize pseudo-labeling to enhance the localization performance. Moreover, some works like [43; 44] focus on the _confirmation bias_ in SSOD. Similar to object detection, AVSL is a pixel-wise dense prediction task, heavily reliant on high-quality pseudo-labels. Attention10k [14; 15] is the first SS-AVSL work. It extends a self-supervised model to a semi-supervised framework by simply adding a supervised loss, aiming at fixing the false conclusions generated by weakly-supervised methods. However, this naive method may lead to overfitting and neglects the full utilization of unlabeled data. In contrast, we introduce a novel SS-AVSL framework based on pseudo-label mechanism, which can address _confirmation bias_ and maximize the utilization of both labeled and unlabeled data, to achieve stronger localization performance.

## 3 Background

Problem Definition.Audio-Visual Source Localization (AVSL) aims to accurately locate the sound source within a given visual scene. We denote audio-visual pairs as \((a_{i},v_{i})\), where \(a_{i}\) and \(v_{i}\) represent the audio and visual modality, respectively. The objective is to generate a pixel-wise confidence map \(\) indicating the location of the sound source.

Contrastive Learning in AVSL.Self-supervised AVSL methods commonly leverage audio-visual correspondence to maximize the similarity between frames and their corresponding audio clips (positive pairs) while minimizing the similarity among unpaired ones (negative pairs):

\[_{}=-_{(a_{i},v_{i})_{u}} [),f(v_{i}))/_{t})}{_{j=1}^{n}( s(g(a_{i}),f(v_{j}))/_{t})}+),g(a_{i}))/ _{t})}{_{j=1}^{n}(s(f(v_{i}),g(a_{j}))/_{t} )}].\] (1)

where \(_{u}\) are unlabeled datasets, \(g()\) and \(f()\) are audio and visual feature extractors. \(_{t}\) is the temperature coefficient. \(s()\) is consistency matching criterion. The predicted map \(_{i}\) is typically calculated with cosine similarity \(()\) to represent the confidence of the presence of sounding objects:

\[_{i}=(g(a_{i}),f(v_{i}))=),f(v_{i })}{\|g(a_{i})\|\|f(v_{i})\|}.\] (2)

Learning with (Pseudo) Labels in AVSL.When labeled data are available, one could apply supervised loss directly to learn to localize:

\[_{}=_{i}H(_{i}, _{i}).\] (3)

where \(_{i}\) could be the ground truth or generated pseudo-labels, both \(_{i}\) and \(_{i}\) are in the form of binary confidence map. \(H(,)\) is cross-entropy function across the two-dimensional spatial axes.

## 4 Dual Mean-Teacher

Overview.In this section, we mainly describe Dual Mean-Teacher (DMT) in the order of the learning process. Specifically, in the Warm-Up Stage (Section 4.1), two teachers are pre-trained on bounding-box annotated data to obtain a stable initialization. In the subsequent Unbiased-Learning Stage (Section 4.2), the Noise Filtering module and Intersection of Pseudo-Label (IPL) module collectively filter out noisy samples and generate high-quality pseudo-labels by two teachers to guide students' training, teachers are in turn updated by exponential moving average (EMA) of students.

From a unified perspective, existing AVSL methods can be viewed as a single student, which is later expanded into the semi-supervised classical framework Mean-Teacher , in order to fully utilize limited labeled data. To effectively address the _confirmation bias_ issue, we further extend it to a double teacher-student framework, as shown in Figure 2. DMT adopts a teacher-student architecture, where each teacher and student contains two independent AVSL pipelines for audio-visual contrastive learning and generating localization results. Teachers provide students with stable unlabeled samples after Noise Filtering and their generated IPL. Students pass back the parameters to the teachers.

General Notations.Cross-entropy function \(H(,)\) and two feature extractors \(f()\) and \(g()\) are already discussed in Section 3. By default, subscript \(i\) indicates the \(i\)-th sample, while superscript \(A\), \(B\) denote two teacher-student pairs with \(t\) and \(s\) indicating teacher and student, respectively. \(_{l}\) and \(_{u}\) are labeled and unlabeled datasets. \(_{i}\) and \(_{i}\) are ground-truth and predicted confidence maps, both with size of \(H W\). We apply strong \(()\) or weak \(()\) augmentation on visual inputs.

### Warm-Up Stage

The quality of pseudo-labels is crucial to SSL, especially for localization tasks. Before generating pseudo-labels, we first pre-train dual teachers with bounding-box annotated data to achieve preliminary localization performance. In order to avoid overfitting, we apply strong augmentation and get augmented labeled dataset \(_{l}=\{(a_{i},(v_{i})),_{i}\}\). After extracting visual features \(f^{t}((v_{i}))\) and auditory features \(g^{t}(a_{i})\), the predicted map \(_{i}^{t}\) can be obtained by Eq. (2). Then we utilize bounding-box annotations \(_{i}\) as supervision:

\[_{}=_{(a_{i},v_{i})_{l}}H( _{i},_{i}^{t}).\] (4)

### Unbiased-Learning Stage

Noise Filtering.To mitigate _confirmation bias_, it is crucial to filter out noisy samples that are more likely to be false positives. As depicted in Figure 2, two predicted maps of the same sample are generated by dual teachers. It is clear that samples with higher reliability can be identified when the two predicted maps exhibit higher similarity, _i.e._, there is more agreement and consensus between dual teachers, then the sample is reserved for pseudo-labelling. Conversely, when there is a significant discrepancy between the two maps, the sample will be considered as a false positive, such as frames without distinguishable sound objects or sounds that cannot be accurately represented by a bounding box (_e.g._, wind sounds), such samples are rejected and discarded.

Figure 2: Overview of the proposed Dual Mean-Teacher framework. **Left:** Overall learning process of dual teacher-student pairs, two students are guided by both ground-truth labeled data and filtered unlabeled data with the Intersection of Pseudo-Labels (IPL). **Upper-right:** Details of Noise Filtering and IPL. Dual teachers reject noise samples based on their consensus and generate pseudo-labels on filtered data. **Lower-right:** Details of AVSL pipeline. Students are learned through contrastive learning and predict confidence maps for supervised learning with (pseudo) labels.

Intersection of Pseudo-Labels (IPL).By intersecting the foreground regions of two predicted maps on the filtered samples, one can generate positional pseudo-labels, named Intersection of Pseudo-Labels (IPL), to guide students' learning. With the pre-defined foreground threshold \(\), two predicted maps \(_{i}^{t,A},_{i}^{t,B}\) could be transferred to binary maps \(_{i}^{t,A}\) and \(_{i}^{t,B}\). Weak augmentation \(()\) is employed for teachers to generate high-quality pseudo-labels:

\[_{i}^{t} =(g^{t}(a_{i}),f^{t}((v_{i}))),\] (5) \[_{i}^{t} =(_{i}^{t}).\] (6)

We adopt the Intersection over Union (IoU) metric to quantify the similarity between the two maps \(_{i}^{t,A}\), \(_{i}^{t,B}\). If the IoU score exceeds the threshold \(\), the sample will be accepted, and the intersection of those two maps will be generated as its pseudo-label (IPL). Otherwise, it will be filtered out as a noise sample.

\[_{u}^{}=(a_{i},v_{i})\;\;( _{i}^{t,A},_{i}^{t,B}),\;(a_{i},v_{i}) _{u}}.\] (7)

\[(a_{i},v_{i})=_{i}^{t,A}_{i}^{t,B}.\] (8)

The newly selected unlabeled dataset is applied to the student model along with the corresponding high-quality IPL.

Students Learning without bias.To suppress _confirmation bias_ more effectively, we mix labeled and new unlabeled datasets. Both ground-truth annotations and high-quality IPL are employed to train the student models:

\[_{mix}=_{l}_{u}^{}=\{(a_{i},v_{i}), }_{i}\}\;,\;\;}_{i}=\{ &_{i}&(a_{i},v_{i})_{l}\\ &(a_{i},v_{i})&(a_{i},v_{i})_{u}^{} .\] (9)

In addition, we incorporate consistency regularization  in the semi-supervised learning process. Specifically, for a given sample, we obtain IPL from the teachers on weakly augmented images while strong augmentations are applied for samples of students. By enforcing consistency between IPL and students' predictions, DMT could be more stable with better generalization ability.

\[_{i}^{s} =(g^{s}(a_{i}),f^{s}((v_{i}))),\] (10) \[_{} =_{i_{mix}}H(}_{i}, _{i}^{s}).\] (11)

Similar to the AVSL method mentioned in Section 3, students are also trained by audio-visual correspondence of contrastive learning loss. Here, we introduce an attention module to add attention to the sounding region in the frame:

\[f_{att}(v_{i})=_{i}(x,y))}{_{x,y} (_{i}(x,y))} f(v_{i}).\] (12)

Then, the full semi-supervised loss could be derived with \(_{}\) (see Eq. (11)) on \(_{mix}\) and \(_{}\) (see Eq. (1)) on \(_{u}\):

\[_{}=(_{}^{A}+_{ }^{B})+_{u}(_{}^{A}+ _{}^{B}).\] (13)

Update of Students and Teachers.Students are updated via gradient descent of \(_{}\), while dual teachers are updated through the exponential moving average (EMA) of corresponding students:

\[_{m}^{s}_{m-1}^{s}-_{ }}{_{m-1}^{s}},_{m}^{t} _{m-1}^{t}+(1-)_{m}^{s}.\] (14)

The slowly progressing teachers can be regarded as the ensemble of students in recent training iterations, which enables stable progress in training.

### Unbiased Superiority of Dual Mean-Teacher

For dense prediction tasks such as AVSL, employing pseudo-labels for model training can easily accumulate errors and lead to sub-optimal results. In our DMT framework, The unbiased characteristics could be attributed to the following three factors: (i). Noise Filtering ensures that only stable samples are utilized to train. (ii). IPL generates high-quality pseudo-labels. (iii). Pre-train dual teachers on bounding-box annotated data with strong augmentation in Warm-Up Stage. The above conclusion will be validated in subsequent ablation studies in Section 5.4.

## 5 Experiments

With limited annotated data, DMT could significantly raise the performance of AVSL and address the severe issues, _e.g_., false positives and poor localization ability on small objects. Then, we direct our focus towards answering the following questions with ablation experiments 5.4:

* What is the individual contribution of each module to the performance gains?
* How does annotation enhance localization performance?
* Why can DMT outperform the existing semi-supervised AVSL method?
* Is it necessary to warm up dual teachers?
* How to effectively mitigate _confirmation bias_ in AVSL?

### Experimental Settings

Datasets.We conduct experiments on two large-scale audio-visual datasets: Flickr-SoundNet  and VGG Sound Source , where there are 5,000 and 5,158 bounding-box annotated samples, respectively. For labeled data, we randomly select 4,250 for training, 500 for validating, and keep the same test sets with 250 samples as previous works . Moreover, we select a subset of 10k and 144k unlabeled samples to train as before. Details can be found in Appendix B.1.

Audio and Visual Backbones.For visual backbones, we followed prior work and used ResNet-18  pre-trained on ImageNet . For audio backbones, we select the pre-trained VGGish  and SoundNet  with semantic audio information. Further details can be found in Appendix B.2.

Metrics.We report the Consensus Intersection over Union (CIoU) and Area Under Curve (AUC), following previous settings . CIoU represents the localization accuracy, samples with IoU above the threshold \(=0.5\) are considered to be accurately located. Considering small objects, we introduce Mean Square Error (MSE), which measures the average pixel-wise difference between two maps without binarization, making it more suitable for evaluating dense prediction tasks on small objects. More details are shown in Appendix B.3.

Implementation details.For audio clips, we pass \(96 64\) log-mel spectrograms to VGGish, and the output is a 512D vector, while the raw waveform of the original 3s audio clip is sent to SoundNet. For frames, we used an input image of size \(256 256 3\), with \(224 224 512\) as output. We choose RandAug  as strong augmentation, while random cropping, resizing, and random horizontal flip as weak augmentation. We set \(\) as 0.6 and \(\) as 0.7. More experiments of hyperparameters are shown in Appendix C.5.

    &  &  \\   & CIoU & AUC & CIoU & AUC \\  Attention10k  & 43.60 & 44.90 & 66.00 & 55.80 \\ CoarseoteFine  & 52.20 & 49.60 & – & \\ DMC  & – & – & 67.10 & 56.80 \\ LVS  & 58.20 & 52.50 & 69.90 & 57.30 \\ EZVSL  & 62.65 & 54.89 & 72.69 & 58.70 \\ SLAVC\({}^{}\) & 66.80 & 56.30 & 73.84 & 58.98 \\ SSPL  & 74.30 & 58.70 & 75.90 & 61.00 \\ SSL-TIE\({}^{}\) & 75.50 & 58.80 & 81.50 & 61.10 \\  Attention10k-SSL  & 82.40 & 61.40 & 83.80 & 61.72 \\  Ours (\(|_{l}|=256\)) & 87.20 (84.40) & 65.77 (59.60) & 87.60 (84.40) & 66.28 (59.60) \\ Ours (\(|_{l}|=2k\)) & 87.80 (85.60) & 66.20 (63.18) & 88.20 (85.60) & 66.63 (63.18) \\ Ours (\(|_{l}|=4k\)) & **88.80** (86.20) & **67.81** (65.56) & **90.40** (86.20) & **69.36** (65.56) \\   

Table 1: Comparison results on Flickr-SoundNet. Models are trained on Flickr 10k and 144k. \(\) indicates our reproduced results, others are borrowed from original papers. Attention10k-SSL is of 2k labeled data supervision. We report the proposed DMT results from both stages as stage-2(stage-1). \(|_{l}|\) denotes the number of labeled data.

### Comparison with the State-of-the-art Methods

Comprehensive experiments show that DMT achieves the state-of-the-art performance among all existing methods on both datasets, and showcases several advantages.

Effective Utilization of Finite Annotations and Remarkable Performance.We tested DMT's localization performance with varying amounts of labeled data and found that it consistently outperforms state-of-the-art methods when with 256, 2k, and 4k labeled data. Notably, even with just 256 labeled data, DMT achieved an accuracy of \(87.2\%\) to \(87.6\%\), showing a significant improvement in CIoU by around 10 absolute points compared to preceding models. Additionally, our model shows a \(3\%\) absolute improvement in CIoU compared to a supervised-only model. Furthermore, DMT maintains superior performance in complex and open environments, as demonstrated in Table 2 and Table 3(c), indicating strong generalization capabilities. These results highlight DMT's ability to improve localization performance by utilizing more unlabeled data.

Significant Advancement in Small Subset Localization.We categorize objects based on their bounding box pixel area into small (\(1 32^{2}\)), medium (\(32^{2} 96^{2}\)), large (\(96^{2} 144^{2}\)) and huge (\(144^{2} 224^{2}\)). We tested different methods on small and medium objects in the VGG-Sound dataset, focusing on the challenges of detecting small objects and reducing false positives mentioned earlier. The results in Table 3(a) show that DMT significantly improves performance, especially in terms of MSE metric. Despite some errors in the IoU metric, DMT still outperforms previous methods. The results in Figure 1 show that DMT accurately locates sounding objects with clear boundaries and precisely convergence to object contours, unlike previous methods that often have excessive or insufficient foreground regions, especially for small objects. These results demonstrate the effective and precise localization of small objects achieved by DMT. More experiments of different object sizes are in Appendix C.7.

Capability of Learning Rich Semantic Information.We present visualized predictions for testsets of varying sizes in Figure 1 (Left). It is evident that our approach achieves remarkable precision in localizing sounding sources. It accurately locates the position of sounding objects and precisely converges to their boundaries, while prior methods usually have excessive or insufficient foreground regions, particularly in the case of small objects.

Table 2: Comparison results on VGG-ss. Models are trained on VGG-Sound 10k and 144k.

Table 3: Performance comparisons in existing issues (small objects localization and false positives) and complex scenarios (open set). The results of small objects and open set are tested on the VGG-ss dataset, while false positives are reported on the Flickr dataset.

It is worth noting that our method can even find out sounding objects overlooked in the manual annotations. For instance, in Figure 1 (Left, \(4\)-th row), the heatmap reveals the presence of a piano, which is omitted in the manual annotation process. Furthermore, we assessed the model's capability to identify false positives, signifying instances where sounding objects are occasionally not visually observable within the image (off-screen), as shown in (b)b. This reflects the ability of Dual Mean-Teacher to extract audio semantic information and effectively localize multiple sounding objects within a scene, a feat that eludes other methods. We attribute this capability to the semantic alignment of audio-visual features achieved through the pre-trained VGGish and SoundNet backbone.

Capacity for Cross-Domain Generalization and Multi-Source Localization.We tested DMT's generalization across different domains and its ability to localize multiple objects. Models trained using VGG-ss 144k were directly evaluated on MUSIC-solo , MUSIC-duet , and MUSIC-synthetic datasets . Figure 3 demonstrates DMT's strong generalization performance in the music domain, outperforming other method. As shown in Figure 3, the previous method struggles to accurately localize multiple sounding objects, either missing them or including all sounding objects within a large foreground area. In contrast, DMT localizes each instrument accurately and separately. However, without category information for fine-grained training, it leads to sub-optimal performance in differentiating between multiple active and silent instruments. There is still significant room for improvement with multiple sounding objects and we plan to address this issue in future work.

### Extensions of Dual Mean-Teacher

We replicated several existing methods and integrated them into our framework. Notably, the integration of the Dual Mean-Teacher showcases its ability to significantly enhance the performance of other existing methods. In Table 4, one can observe a noteworthy improvement in the CIoU of EZVSL from 62.65% to 87.20%, and SLAVC rising from 66.80% to 88.80%, which further reinforces the efficacy of our framework and highlight its flexible extensibility.

### Ablation Studies

What is the individual contribution of each module to the performance gains?In this section, we progressively analyze the performance gain from each module in detail. We choose a self-supervised approach as our baseline. Table 5 presents the results on the Flickr 144k training set with \(10\%\) annotated samples.

   Methods & Backbones & CIoU\(\) & AUC\(\) & MSE\(\) \\  EZVSL w/o DMT & R & 62.65 & 54.89 & 0.428 \\ EZVSL w/ DMT & R+V & 85.30 & 65.80 & 0.312 \\ EZVSL w/ DMT & R+S & 85.95 & 66.12 & 0.298 \\ ZVSL w/ DMT & V+S & **87.20** & **67.74** & **0.256** \\  SLAVC w/o DMT & R & 66.80 & 56.30 & 0.386 \\ SLAVC w/ DMT & R+V & 86.10 & 66.24 & 0.288 \\ SLAVC w/ DMT & R+S & 86.30 & 66.58 & 0.283 \\ SLAVC w/ DMT & V+S & **88.80** & **68.69** & **0.247** \\   

Table 4: Extension results of DMT with various audio backbones, with ’R’, ’V’ and ‘S’ denoting ResNet, VGGish and SoundNet.

Figure 3: Performance on music-domain.

    &  \\  \# Teachers & Backbone & Filter & IPL & EMA & CIoU\(\) & AUC\(\) & MSE\(\) \\   & (a). S & ✗ & ✗ & ✗ & 80.20 & 53.57 & 0.458 \\  & (b). V & ✗ & ✗ & ✗ & 81.80 & 55.92 & 0.379 \\   & (c). S+V & ✗ & ✗ & ✗ & 82.20 & 55.16 & 0.382 \\  & (d). S+V & ✗ & ✗ & ✓ & 82.80 & 59.38 & 0.375 \\  & (e). S+V & ✗ & ✓ & ✗ & 83.60 & 62.83 & 0.359 \\   & (f). S+V & ✓ & ✗ & ✗ & 84.80 & 65.58 & 0.259 \\   & (g). S+V & ✓ & ✗ & ✓ & 86.20 & 66.56 & 0.260 \\   & (h). S+V & ✗ & ✓ & ✓ & 86.60 & 66.35 & 0.274 \\   & (i). S+V & ✓ & ✓ & ✗ & 88.60 & 66.68 & 0.260 \\   & (j). S+V & ✓ & ✓ & ✓ & **90.40** & **69.36** & **0.237** \\   

Table 5: Main ablation study results. Models are trained on Flickr 144k and tested on Flickr-SoundNet testset, where ‘S’ and ‘V’ denote SoundNet and VGGish respectively.

(i). Initially, we apply it under the semi-supervised pseudo-labeling mechanism, using only one backbone, as shown in Table 5(a) and Table 5(b). The localization performance improves with annotated data supervision, but the gain is limited due to poor-quality pseudo-labels.

(ii) Next, we extend it to a two-backbone architecture and sequentially introduce the filter, IPL, and EMA modules. The results demonstrate that all three modules contribute to performance improvement (\(3\%\), \(1.8\%\), \(1\%\)). Notably, Filter shows the most significant impact on the model's improvement, for it effectively rejects noisy samples, ensuring the stability of the model.

(iii) Finally, we can observe that optimal performance is achieved through the joint integration of the three modules by effectively suppressing _confirmation bias_.

How does annotation help localization?We aim to demonstrate that even with extremely limited labeled data, significant performance can still be achieved. To this end, we investigated the performance of our model from \(0.5\%\) to \(10\%\), and report the results in Table 6. Our model consistently outperforms state-of-the-art approaches across all ratios. Furthermore, with a constant amount of unlabeled data, as the proportion of labeled data increases, our model's performance continues to improve, highlighting the significant impact of labeled data.

We investigated the impact of varying amounts of unlabeled data while keeping labeled data constant. Experimental results in Table 6 show that increasing the amount of unlabeled data improves localization performance, which seems contradictory to the previous conclusion about the proportion of labeled data, but actually, it demonstrates that labeled data can effectively leverage the unlabeled data. Based on our analysis, labeled data not only provides annotation information but also effectively enhances the power of unlabeled data, resulting in significant performance improvements.

Why can DMT outperform the existing semi-supervised AVSL method?Both naive SSL and DMT have utilized labeled and unlabeled data. However, a key distinction is that naive SSL employs unlabeled data only for contrastive loss, whereas DMT leverages pseudo-labels to incorporate unlabeled data into both contrastive loss and supervised loss, which amplifies the utilization of unlabeled data, thus enhancing generalization capability.

Data Utilization.We supplement the comparison experiments with fixed labeled data and an increase in unlabeled data from 10k to 200k, as shown in left part of 7. As the amount of unlabeled data increases, naive SSL exhibits only marginal improvement, whereas DMT shows more performance gains, indicating DMT can better use unlabeled data.

Generalization Ability.The right part of 7 highlights the limitations of naive SSL in the open set and in-the-wild datasets, suggesting that adding a supervised loss alone may lead to overfitting and weaken generalization. In contrast, DMT effectively leverages pseudo-label for improved generalization capability.

Is it necessary to warm up dual teachers?We believe that the initialization of teachers and students is crucial, for the quality of pseudo-labels has a significant impact on model performance. To validate the effectiveness of this idea, we experimented to study the warm-up stage's impact on the model. We find that without the Warm-Up Stage, the model's improvement is very slow, and the performance eventually deteriorates. This indicates that without a good initialization, the model can

   Labeled ratio \(\%\) & CIoU & AUC \\  \(0.5\%\) (200/40k) & 84.80 & 63.58 \\ \(1\%\) (400/40k) & 86.20 & 65.16 \\ \(2\%\) (800/40k) & 87.20 & 65.94 \\ \(5\%\) (2k4/40k) & 87.60 & 67.44 \\ \(10\%\) (4k/40k) & **88.40** & **68.12** \\   Multiple \(\) & CIoU & AUC \\  \(2.5\) (4k/10k) & 88.00 & 67.80 \\ \(5\) (4k/20k) & 88.20 & 67.91 \\ \(10\) (4k/40k) & 88.40 & 68.12 \\ \(20\) (4k/80k) & 89.20 & 68.44 \\ \(40\) (4k/200k) & **91.20** & **71.36** \\   

Table 6: Performance on various labeled ratios \(\%\) and multiple \(\) on Flickr 144k.

    & 2.5k/10k & 2.5k/144k & 2.5k/200k & open set & cross-datasets \\  attention10k + naive SSL & 84.00\({}^{}\)83.68 & 84.40\({}^{}\)84.08 & 84.24 & 19.60 & 62.20 \\ attention10k + DMT (ours) & **88.00** & **89.52** & **90.40** & **42.64** & **87.26** \\ sim-avsl + naive SSL & 83.84 & 84.24 & 84.40 & 20.80 & 60.60 \\ sim-avsl + DMT (ours) & **88.24** & **89.76** & **91.12** & **43.10** & **89.80** \\   

Table 7: Comparison of two SS-AVSL methods. * denotes the results from the original paper. ’sim-avsl’ denotes the simple self-supervised AVSL model we use. We report the CIoU below.

accumulate errors, leading to _confirmation bias_ issues. Therefore, we can conclude that Warm-Up is essential as it effectively suppresses _confirmation bias_ in the early stages of training.

How to effectively mitigate confirmation bias in AVSL?In Section 4.3, we present the origins and mitigation strategies for _confirmation bias_ in the localization task. In this section, we will demonstrate this. Figure 4(a) depicts the quality of pseudo labels before and after applying the filter module. It is evident that the quality of pseudo labels can be significantly improved after filtering. This observation highlights the effectiveness of the filter in eliminating noisy samples, which tend to be false-positive instances. Figure 4(b) depicts the comparison between using the direct outputs of each teacher as pseudo labels and utilizing their intersection, known as IPL. Where the purple dashed line represents the initialization value. The results clearly indicate that employing the direct outputs alone leads to the accumulation of bias, causing a deterioration in the quality of pseudo labels throughout the training process. Conversely, IPL consistently ensures the preservation of high-quality pseudo labels, thus mitigating the impact of bias.

Furthermore, Figure 4(c) visually presents the trend of model performance, revealing that the absence of any of these three modules results in a decline in model performance. However, we can see that EMA only affects the final performance of the model, and without the Filter module, the model's performance will be significantly affected by noise. Without the IPL module, the model will experience a continuous decline in performance due to erroneous estimation of pseudo-labels. Therefore, we find that the Noise Filtering module and IPL modules play a significant role in addressing the _confirmation bias_ problem. Moreover, Figure 4(d) reflects that under the joint action of the three modules, DMT generates more accurate pseudo-labels and its performance continues to improve steadily.

## 6 Conclusion

In this paper, we advance the naive SS-AVSL work and propose a novel Semi-Supervised Audio-Visual Source Localization (SS-AVSL) framework, namely Dual Mean-Teacher (DMT), considering the importance of both limited annotated and abundant unlabeled data. From a unified perspective, existing self-supervised (weakly-supervised) AVSL methods could be referred to as a single student structure, while DMT employs dual teacher-student pairs to filter out noisy samples via the agreement of two teachers and generate high-quality pseudo labels to avoid _confirmation bias_. DMT has greatly enhanced AVSL performance and addressed intractable issues like false positives and inaccurate localization of tiny objects. Moreover, DMT is a learning paradigm and could be seamlessly incorporated into existing AVSL methods and consistently boost their performance.

We hope this work will bring more attention to SS-AVSL, provoke a reconsideration of pseudo-labeling, bias avoidance, and better utilization of the underlying unlabeled data, and thus stimulate more semi-supervised learning research in this dense prediction task.

Figure 4: Effect of Warm-Up Stage.

Figure 5: The effect of each component (Noise Filtering, IPL and EMA) in DMT to suppress confirmation bias, together with the number of filtered samples for pseudo labeling depicted in (d).