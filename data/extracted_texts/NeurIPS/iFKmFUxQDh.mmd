# ReFIR: Grounding Large Restoration Models

with Retrieval Augmentation

 Hang Guo\({}^{1}\) Tao Dai\({}^{*}\)\({}^{2}\) Zhihao Ouyang\({}^{3}\) Taolin Zhang\({}^{1}\)

**Yaohua Zha\({}^{1}\) Bin Chen\({}^{4}\) Shu-tao Xia\({}^{1,5}\)**

\({}^{1}\)Tsinghua University \({}^{2}\)Shenzhen University \({}^{3}\)Aitist.ai

\({}^{4}\)Harbin Institute of Technology \({}^{5}\)Peng Cheng Laboratory

https://github.com/csguoh/ReFIR

Corresponding author: Tao Dai (daitao.edu@gmail.com).

###### Abstract

Recent advances in diffusion-based Large Restoration Models (LRMs) have significantly improved photo-realistic image restoration by leveraging the _internal knowledge_ embedded within model weights. However, existing LRMs often suffer from the _hallucination_ dilemma, _i.e._, producing incorrect contents or textures when dealing with severe degradations, due to their heavy reliance on limited internal knowledge. In this paper, we propose an orthogonal solution called the **R**etrieval-augmented **F**ramework for **I**mage **R**estoration (ReFIR), which incorporates retrieved images as _external knowledge_ to extend the knowledge boundary of existing LRMs in generating details faithful to the original scene. Specifically, we first introduce the nearest neighbor lookup to retrieve content-relevant high-quality images as reference, after which we propose the cross-image injection to modify existing LRMs to utilize high-quality textures from retrieved images. Thanks to the additional external knowledge, our ReFIR can well handle the hallucination challenge and facilitate faithfully results. Extensive experiments demonstrate that ReFIR can achieve not only high-fidelity but also realistic restoration results. Importantly, our ReFIR requires no training and is adaptable to various LRMs.

## 1 Introduction

Restoring a high-quality image (HQ) from its low-quality counterpart (LQ) is a well-known ill-posed problem and has been studied over the years . Previous efforts attempt to handle this problem through employing various neural network architectures, including CNNs, GANs and Transformers. Recently, diffusion models  have emerged as a promising alternative, delivering noteworthy results in real-world image restoration . In particular, some works  have successfully leveraged the powerful generative prior of pre-trained text-to-image (T2I) diffusion models for scaling up, to obtain the Large Restoration Model (LRM) with billions of parameters, bringing significant progress in restoring photo-realistic images.

Although scaling up restoration models has achieved remarkable success, existing LRMs may not always produce results that are faithful to the original scene, particularly when faced with heavily degraded images that surpass the LRMs' capabilities (see Fig. 1). This issue is similar to the hallucination problem observed in large language models (LLMs) , _e.g._ ChatGPT might generate nonsense responses when highly specialized questions exceed its knowledge boundary. Similarly, if one LRM has never seen a specific scene, it will struggle to restore corresponding images faithfully. By analogizing LLM to LRM, we define the phenomenon where LRMs generate textures inconsistent with the original scene when facing hard samples as the hallucination of LRMs.

To address the hallucination problem in LRMs, simply expanding the _internal knowledge_ through additional training data and parameters might seem straightforward, but it can significantly increasecomputational and storage costs. Instead, this work considers another orthogonal strategy that enhances the _external knowledge_ of LRMs without adding parameter counts. Drawing inspiration from the retrieval-augmented generation (RAG) used in LLMs [23; 24; 25], we aim to use the retrieved high-quality content-relevant images as external knowledge to alleviate the hallucination of LRMs. However, applying RAG to image restoration poses specific challenges. Specifically, in natural language, simply feeding the retrieved documents along with the original user query to LLMs can allow it to produce grounded responses. However, in the context of image restoration, allowing low-quality images to attend to retrieved images during their restoration process is non-trivial, which motivates us to develop novel techniques to enable LRMs to utilize external knowledge in restoration.

To this end, we delve deep into the working mechanisms of LRMs for insightful observations. Details of the experimental setup are described in Sec. 3. Our key findings indicate that the workflow of LRMs can be divided into two distinct stages: the **Denoising Structure Reconstruction** stage, during which the self-attention in the ControlNet  reconstructs a clear overall structure from the noised representation. After that, in the **Detail Texture Restoration** stage, the self-attention in the UNet  decoder fills scene-specific textures based on the denoised structure map. Based on these findings, a natural solution emerged: we can transfer high-quality, scene-specific textures from the retrieved images to the low-quality images during the detail texture restoration stage. In this way, the restored image is allowed a consistent texture with the retrieved image, thus mitigating the hallucination.

Inspired by the above observation, in this work, we propose the Retrieval-augmented Framework for Image Restoration, dubbed ReFIR, to offer a simple but effective way to expand the knowledge boundary of LRMs using the external knowledge from the retrieved images. Specifically, we first construct the retriever which employs the nearest neighbor lookup in the semantic embedding space to retrieve content-relevant reference images in the high-quality image database. After that, we develop the cross image injection which modifies the self-attention layer of original LRMs to enable the queries from the low-quality denoising chain to attend to the keys and values from the denoising chain of retrieved reference. To avoid the domain preference problem during injection, we propose separate attention to perform intra-chain and inter-chain attention, respectively. Given the spatial misalignment between the LQ and the retrieved HQ, we further adopt spatial adaptive gating to mask meaningless pixels during injection. At last, we employ the distribution alignment to narrow the domain gap between LQ and retrieved images. Thanks to the proposed ReFIR, the restoration of the LQ image can make full use of the external knowledge from the reference to generate high-fidelity images. Notably, the proposed pipeline is training-free and can be applied to multiple LRMs.

The contribution of this paper can be summarized as: **(i)** We introduce retrieval-augmented restoration, a novel concept to mitigate the hallucination problems in existing LRMs. **(ii)** We conduct an in-depth analysis of the working mechanisms of LRMs, based on which we propose a training-free framework to utilize the retrieved images. **(iii)** Extensive experiments validate that our proposed method effectively mitigates hallucination and is applicable to a broad spectrum of existing LRMs.

Figure 1: Existing LRMs encounter hallucination issues, _i.e._, generating contents or details that deviate from the original scene, when dealing with challenging degradations. By incorporating the proposed ReFIR to existing LRMs  without any training, the additional external knowledge facilitates producing more faithful results. Please zoom in for better visualization.

## 2 Related Works

### Diffusion Model for Image Restoration

Diffusion models have recently achieved significant advancements across various computer vision tasks [28; 29; 30; 31; 32; 33; 34; 35; 36; 37]. In the realm of image restoration, early explorations often involved training diffusion models from scratch to obtain the restoration tailored models [38; 39; 40; 41]. While these models are capable of producing high-fidelity results, they usually fall short of generating perceptually pleasing images. To leverage the powerful generative capabilities of large pre-trained text-to-image diffusion models like Stable Diffusion , recent attempts [15; 16; 17; 42; 18; 19] have focused on using the ControlNet  with a LQ image as the condition to generate HQ images. Benefiting from the scaling law , these large restoration models with billions of parameters have shown impressive restoration results with photo-realistic textures and details. However, similar to the large language models, when the user query, _i.e._, the LQ image in this setting, exceeds the knowledge boundary of the large models, the models often fail to generate meaningful or correct responses, which is unacceptable for image restoration tasks that pursue high-fidelity.

### Reference-based Image Super-resolution

Compared with single image super-resolution [1; 4; 2; 3], Reference-based Image Super-Resolution (RefSR) can achieve enhanced performance by employing content-similar reference images as the additional input, and has attracted great research interests in the past few years [45; 46; 47]. For instance, C2-Matching  introduces a teacher-student correlation distillation and a dynamic DCN aggregation module for more precise alignment between low-quality and reference images. Following this, DATSR  employs reciprocal learning and SwinTransformer to further boost performance. Additionally, MRefSR  introduces a simple baseline to facilitate RefSR with multiple reference images. It is worth mentioning that despite both using additional images as references, our proposed retrieval augmented restoration pipeline differs from previous RefSR methods in several key aspects. Firstly, current RefSR models are typically small-scale due to limited training data, leading to performance degradation under challenging real-world conditions. Secondly, most RefSR methods can only use one single reference image and even fail to work in the absence of reference images. Thirdly, different from RefSR models that require training, our method can inject image-specific external knowledge into LRMs in a training-free manner. We give a detailed discussion about the difference in Appendix B.

### Retrieval Augmented Generation

In the domain of natural language processing, Retrieval-Augmented Generation (RAG) leverages the strengths of pre-trained Large Language Models (LLMs) combined with knowledge retrieved from an external document database to enhance the quality of generated content [21; 22]. Typically, a RAG system initially retrieves documents relevant to the user's query from the knowledge base and then integrates the retrieved document along with the original user query into the LLMs without any tuning to generate a response. Even when no relevant document is available, this system can still operate by using the internal knowledge embedded in the LLMs' parameters. The integration of RAG allows LLMs to produce outputs that are not only contextually rich but also factually accurate, effectively mitigating the hallucination problem in knowledge-intensive tasks [23; 24; 25]. In this

Figure 2: In-depth visualization about the working mechanism of LRM. **Left**: we use PCA to visualize the top three principal components of latent extracted from the self-attention layer of the ControlNet and UNet decoder. **Right**: quantitative power spectrum of the corresponding latent using Fourier analysis. More visualization can be found in Appendix H.

work, we extend the concept of RAG to image processing and propose retrieval-augmented restoration to alleviate the hallucination issues in LRMs. By utilizing external textures embedded in the retrieved reference images, our tuning-free framework significantly facilitates faithful restoration results.

## 3 Probing Large Restoration Models

In order to manipulate the LRM so that it can utilize the retrieved reference images as external knowledge, we first delve into the underlying mechanism of existing LRMs to find useful insights. We choose the current popular LRM method SUPIR  as a representative. Inspired by previous image editing efforts [50; 31; 30; 29], which show that the self-attention layer of diffusion models contains important spatial correlation of an image, we thus follow this clue and employ the PCA to visualize the principal components of the latent from self-attention layers of SUPIR. We further utilize the Fourier analysis  to allow for quantitative results. The results are shown in Fig. 2.

It can be seen that the ControlNet of the LRM can denoise the latent as the layers deepen, facilitating the reconstruction of a clear overall structure. However, this process is accompanied by a reduction in the high-frequency meaningful texture of the original image. This qualitative visualization can be also verified by the frequency characteristic plots, with high-frequency components decaying as layer number increases. On the other hand, the role of the UNet decoder is significantly different. Based on the previous clear structural map, the decoder restores the high-frequency details and textures with the help of skip connections, which is also shown through the strengthening high-frequency component in the decoder's frequency curve.

Considering the above observations, we can divide the image restoration process of the LRM into two phases: the Denosing Structure Reconstruction phase in the ControlNet, and the Detail Texture Restoration phase in the UNet decoder. Inspired by these probing experiments, in this work, we employ the detail texture restoration nature in the self-attention layer of the decoder to inject the high-fidelity textures of retrieved images into the restoration process of the low-quality image.

## 4 Methodology

This work considers using retrieved reference images as an explicit part of the model. In contrast to the existing restoration pipeline, our ReFIR is parameterized by not only the internal knowledge from the network weights but also the external knowledge retrieved from suitable data representations. Fig. 3 gives an overview of our ReFIR. In the following part, we will first give the technical details of the retriever for reference image retrieval in Sec. 4.1, followed by the cross image injection to inject the external data knowledge into the restoration process of LRMs in Sec. 4.2.

### Nearest Neighbor Lookup for Reference Image Retrieval

Our reference image retrieval system can be represented as a binary set \(\{,\}\), where \(\) is a fixed database containing a large number of HQ images, and \(\) denotes a non-parametric retriever to obtain the retrieved image set \(}\) which consists of \(k\) elements and is a subset of \(\) given a query LQ image \(I_{LQ}^{3 H W}\), _i.e._, \(:I_{LQ},}\), where \(}\) and \(|}|=k\). Ideally, \(\) has to be

Figure 3: Our ReFIR consists of two stages: the **Reference Image Retrieval** stage employs the retriever \(\) to search content-relevant images from high-quality image database \(}\), and then the **High-fidelity Image Restoration** stage restores HQ image with reference images \(}\) as condition. The proposed framework is highly generic and can be applied to multiple existing LRMs without any training or fine-tuning.

designed such that it provides the model with beneficial data representations from \(\) to help restore images containing details faithful to the original scenes.

In this work, we implement a conceptually simple solution of \(\), which uses the query image \(I_{LQ}\) to retrieve its \(k\) nearest neighbor in \(\) using cosine similarity in the compact feature space derived from any feature extractors, such as VGG , ResNet  or CLIP . Since the \(\) is fixed, in practice, we can pre-extract and store the compact feature before training. Given a sufficiently large database \(\), this strategy ensures that the set of neighbors \(}\) shares sufficient semantic consistency with \(I_{LQ}\) and thus provides useful visual information for the restoration. Although this scheme seems simple, we show that it is efficient and effective, please see Sec. 5.3 for discussion.

### Cross Image Injection for High-fidelity Image Restoration

Given the retrieved reference images \(}=(I_{LQ},)\), we further propose the cross image injection to allow the original LRMs to use the external knowledge from \(}\). As shown in Fig. 4, we first construct two parallel denoising chains: the target restoration chain \(_{T}\) which is used to restore \(I_{LQ}\), and the source reference chain \(_{S}\) which unfolds \(}\) into denoising time steps. After that, we introduce separate attention to separately perform attention within and between chains, followed by spatial adaptive gating to filter out irrelevant pixels. At last, we use the distribution alignment to mitigate the domain gap between chains. More details are given below.

**Separate attention.** To allow the \(_{T}\) to learn the knowledge from the \(_{S}\), an effective interaction between the latents is crucial. Inspired by the observation in Sec. 3, we aim to transfer the knowledge embedded in the self-attention layer of \(_{S}\)'s decoder to the counterpart of \(_{T}\). To this end, we modify the original self-attention in \(_{T}\) to our proposed separate attention. The core idea of our separate attention is to add "inter-chain cross-attention" to the original "intra-chain self-attention" so that \(_{T}\) can attend high-quality texture knowledge from \(_{S}\) while preserving its original features. As shown in Fig. 4(a), formally, denote \(Q_{T}\), \(K_{T}\), \(V_{T}\) as the query, key and value from the \(_{T}\), and \(K_{S}\), \(V_{S}\) as the key and value from the \(_{S}\), the intra-chain self-attention preserves the original attention of \(_{T}\) to obtain the output \(O_{intra}\), and the inter-chain cross-attention uses the \(Q_{T}\) to query the \(K_{S}\) and \(V_{S}\) to facilitate \(_{T}\) utilizing the knowledge from \(_{S}\) to get the result \(O_{inter}\). In short, the proposed separate attention can be formalized as follows:

\[O_{intra}=(Q_{T},K_{T},V_{T}), O_{inter}=(Q_{T},K_{S},V_{S}).\] (1)

It is worth mentioning that directly using \(Q_{T}\) to query the concatenate results of \(K_{T}\) and \(K_{S}\) can only yield sub-optimal results due to the domain preference issue, _i.e._, \(Q_{T}\) will prefer latent from the same domain \(_{T}\) even though \(_{S}\) is more helpful for reconstruction. By using the proposed separate attention, the \(Q_{T}\) is separated to attend \(K_{T}\) and \(K_{S}\), thus effectively mitigating this problem. We give more discussion in Sec. 5.3.

**Spatial adaptive gating.** We then consider fusing the separate attention results \(O_{intra}\) and \(O_{inter}\). The main challenge is the spatial misalignment between \(I_{LQ}\) and \(}\). For instance, the same objects

Figure 4: An illustration of cross image injection. Both \(_{T}\) and \(_{S}\) share the same model weights.

may appear in different locations in \(I_{LQ}\) and \(}\), or some objects in \(I_{LQ}\) may not present in \(}\) and vice versa. As a result, some pixels in \(Q_{T}\) may not find the corresponding reference in \(K_{S}\), resulting in some pixels in \(O_{inter}\) meaningless.

To address this spatial misalignment, we propose the spatial adaptive gating to selectively fuse \(O_{intra}\) and \(O_{inter}\) without introducing additional parameters (Fig. 4(b)). Specifically, given latents at specific denoising blocks from \(_{T}\) and \(_{S}\), respectively, we first flatten them along the spatial dimension to obtain \(},}^{C HW}\). Next, we compute their pixel-wise cosine similarity to obtain the similarity matrix \(^{HW HW}\). Since the \(i\)-th row of \(\) represents the similarity of the \(i\)-th pixel in \(}\) to all the pixels in \(}\), therefore, a large sum of the \(i\)-th row indicates a large impact of \(}\) in restoring the \(i\)-th pixel of \(}\). Following this idea, we summation over the \(i\)-th row of the \(\) to approximate the utility of \(}\) to the \(i\)-th pixel of \(}\). Finally, we reshape this summation results back to 2D shape and use min-max normalization to restrict the range to \(\), to get the pixel-wise mask \(\) for adaptive gated fusion:

\[O_{fuse}=(-s) O_{intra}+s O_{ inter},\] (2)

where \(s\) is a user-defined scalar to control the degree to which the restored image attends the retrieved images, \(\) denotes the Hardamard product, and \(\) is an all one tensor with the same shape as \(\).

**Distribution alignment.** Using the \(O_{fuse}\) to replace the original intra-chain self-attention results \(O_{intra}\) seems to be a promising way to integrate useful external knowledge from \(_{S}\). However, it should be noticed that there is a domain gap between \(_{T}\) and \(_{S}\) due to the image quality and content differences, and thus a direct insertion of \(O_{intra}\) into \(_{T}\) will result in a distribution shift of the original denoising chain in \(_{T}\).

To this end, we propose the distribution alignment as a complementary to calibrate the distribution shift. Specifically, considering the latent in the diffusion chain is a Gaussian, we propose to use the Adaptive Instance Normalization (AdaIN)  to align the mean and variance of \(O_{fuse}\) to the original statistics of \(O_{intra}\):

\[O^{}_{fuse}=(O_{fuse},O_{intra}),\] (3)

where \((u,v)\) denotes replacing the mean and variance of \(u\) with the corresponding part of \(v\). Finally, we replace the original self-attention result in \(_{T}\) with the well-aligned \(O^{}_{fuse}\) to finish the cross image injection process.

## 5 Experiments

### Experiments Setup

**Datasets and metrics.** In this work, we include experiments with two difficulty levels for performance evaluation. The first setup considers restoration with manually provided ideal reference images, which share a high content similarity with the LQ image, to evaluate the ability to utilize the reference knowledge. The datasets for this setting employ the widely used RefSR dataset including CUFED5 [56; 57] and WR-SR , in which the reference images are already provided. Since these datasets only contain HQ images, we thus use the second-order degradation model from Real-ESRGAN  with \( 4\) down-sampling scale to generate the real-world degraded images. The second setup turns to more challenging practice where the reference images have to be retrieved using the retriever, and we use the RealPhoto60  which contains 60 real-world degraded images without ground truth for evaluation. And we use DIV2K  as the high-quality image database for retrieval and employ the image encoder of VGG16  as the feature extractor. As for the evaluation metrics, we use both the fidelity metrics containing PSNR and SSIM, as well as the perceptual metrics including LPIPS , NIQE , FID , MUISQ , and CLIPIQA , to assess the performance of the different methods.

**Implementation details.** For a fair comparison, we use one reference image if not specified. Experiments with multiple reference images are given in Appendix A. Following the common practice of existing LRMs [19; 17; 18; 16], the \(I_{LQ}\) is up-sampled to the desired size using Bicubic before going through the LRMs. We use reflective padding to ensure the input size of \(_{T}\) and \(_{S}\) are the same. We use fixed random seeds for results reproducibility in all experiments. The hyperparameters of different baselines follow their original settings. We apply the proposed retrieval augmented restoration framework to two popular LRMs, namely SeeSR  and SUPIR , and denoted the models augmented with our ReFIR as "SeeSR+ReFIR" and "SUPIR+ReFIR", respectively.

### Comparison to State-of-the-Arts

**Restoration with ideal reference.** We first compare on the RefSR dataset with real-world degradation. The compared methods includes state-of-the-art RefSR methods [47; 48; 49], GAN-based methods [9; 8], and recent Diffusion-based methods [16; 19; 18; 17; 15]. Tab. 1 gives the results. It can be seen that our method brings significant gains in _all_ metrics on both fidelity (PSNR, SSIM) and perceptual quality (LPIPS, NIQE, FID) for the LRMs. Taking SUPIR as an example, our method brings a FID improvement of even 19.57 on the CUFED5 dataset. Moreover, similar performance gains can also be observed in SeeSR. For instance, equipping our ReFIR to SeeSR can lead to 0.38dB PSNR improvement, demonstrating the generalization of our ReFIR. It is noteworthy that the above superiority is obtained without any training or fine-tuning. Moreover, we also give visual comparisons in Fig. 5, and it can be seen that our method can generate details that are faithful to the original scene with the help of external knowledge from retrieved reference images.

**Restoration in the wild.** The above experiments on RefSR datasets focus on utilizing the already provided reference images from the dataset, which applies when the user has relevant HQ images. In this section, we turn to more challenging scenarios in which the reference image has to be obtained by retrieval. Since the ground truth of RealPhoto datasets is unavailable, we use non-reference image quality assessment metrics, _i.e._ NIQE, MUSIQ, and CLIPIQA for evaluation. As shown in Tab. 2, our approach continues to produce significant gains over its non-ReFIR counterparts. For instance, our SeeSR+ReFIR surpasses the original SeeSR by 0.2866 NIQE and 1.59 MUSIQ. Since the retrieved image can not serve as an ideal reference, the above favorable results demonstrate the robustness of our ReFIR in the face of real-world retrieved images. We also give quantitative results in Fig. 6. Even under severe real-world degradation, our method maintains good perceptual quality.

    &  &  \\  & PSNR\(\) & SSIM\(\) & LPIPS\(\) & NIQE\(\) & FID\(\) & PSNR\(\) & SSIM\(\) & LPIPS\(\) & NIQE\(\) & FID\(\) \\  C2-Matching  & 20.77 & 0.5169 & 0.7282 & 8.4438 & 282.43 & 22.63 & 0.5627 & 0.7177 & 8.3238 & 157.61 \\ DATSR  & 20.75 & 0.5130 & 0.7301 & 8.6765 & 282.19 & 22.62 & 0.5620 & 0.7210 & 8.4329 & 157.54 \\ MerSR  & 20.84 & 0.5218 & 0.7853 & 9.6524 & 286.44 & 22.68 & 0.5703 & 0.7748 & 9.7742 & 156.57 \\  BSRGAN  & 20.22 & 0.5256 & 0.4135 & 4.2204 & 203.17 & 22.07 & 0.5735 & 0.4073 & 3.8703 & 133.50 \\ Real-ESRGAN  & 20.31 & 0.5543 & 0.3698 & 3.8832 & 175.91 & 22.14 & 0.5974 & 0.3631 & 3.7001 & 97.88 \\ StablesR  & 20.46 & 0.4480 & 0.6532 & 6.3433 & 292.69 & 21.22 & 0.4421 & 0.5899 & 5.2040 & 145.07 \\ DiffBIR  & 19.76 & 0.4886 & 0.3820 & 3.5629 & 154.75 & 21.30 & 0.5284 & 0.3938 & 3.8736 & 76.05 \\ PASD  & 20.22 & 0.4959 & 0.5252 & 5.4828 & 208.64 & 21.12 & 0.5254 & 0.4292 & 4.2505 & 98.16 \\  SeeSR  & 19.94 & 0.5195 & 0.3660 & 3.7912 & 142.92 & 21.73 & 0.5658 & 0.3501 & 4.0155 & 65.78 \\ SeeSR+ReFIR & 20.32 & 0.5289 & 0.3338 & 3.7831 & 134.62 & 21.86 & 0.5664 & 0.3460 & 3.9089 & 61.22 \\ \(\)_improvement_ & +0.38 & +0.0094 & +0.0322 & +0.0081 & +8.30 & +0.13 & +0.0006 & +0.0041 & +0.1066 & +4.56 \\  SUPIR & 18.97 & 0.4665 & 0.4807 & 4.5624 & 168.26 & 20.91 & 0.5426 & 0.3791 & 3.7587 & 75.85 \\ SUPIR+ReFIR & 19.00 & 0.4729 & 0.4341 & 4.2085 & 148.69 & 21.02 & 0.5497 & 0.3785 & 3.7478 & 71.82 \\ \(\)_improvement_ & +0.03 & +0.0064 & +0.0466 & +0.3539 & +19.57 & +0.11 & +0.0071 & +0.0006 & +0.0109 & +4.03 \\   

Table 1: Quantitative comparison with state-of-the-art RefSR methods, GAN-based methods, and Diffusion-based methods on real-world image super-resolution. Our ReFIR achieves consistent performance improvements in both fidelity and perceptual quality.

Figure 5: Quantitative comparison on RefSR dataset. The results using our ReFIR are **bolded**. Please zoom in for better visualization.

**Complexity analysis.** Tab. 3 gives the comparison of the computational complexity, including the number of parameters, GPU cost, and the inference latency. We also give the restoration performance for a more comprehensive comparison. As for the parameters, our ReFIR can facilitate both fidelity and realistic image restoration using the same #param as the original base LRMs. For the GPU memory, since our ReFIR uses two images as input, _i.e._, one LQ image, and one reference image, the GPU cost will become larger than the original one. For instance, it rises 1.38 times the increase of SUPIR+ReFIR than the original SUPIR model. Moreover, the inference time also increases due to more inputs as well as the additional interaction between two chains. In the future, we will delve deep into the effective utilization of retrieved images while maintaining efficiency.

### Ablation Studies

**Effectiveness of the reference retriever.** In order to obtain content-relevant retrieved images, we present a simple but inference-efficient retriever \(\) that uses the high-level semantic vectors from the pre-trained deep models for similarity matching in the high-quality image dataset \(\). Despite the simple design, we here demonstrate its effectiveness in Fig. 7. Since semantically consistent images usually contain similar textures, _e.g._, the texture in the first elephant image can help in the restoration of the LQ elephant image, and thus the proposed retriever can yield satisfactory retrieval results. Although texture-based retrieval may be a better choice for image restoration, it usually necessitates additional training of new retrieval models. For simplicity, we adopt semantic-based retrieval and leave the exploration of more advanced reference retrievers for future work.

**Ablation on cross image injection.** In the proposed cross image injection, we use separate attention (SA), spatial adaptive gating (SG), and distribution alignment (DA) for effective external knowledge injection. Here, we ablate to validate the effectiveness of different components. We use SUPIR+ReFIR as a representative on the CUFED5 dataset and use the scalar weighted sum when SG is removed. The results are shown in Tab. 4. One can see that using fixed scalar weights instead of spatial adaptive gating results in a 0.18 NIQE drop. This is because not all pixels of the reference image are useful, and thus fine-grain gated mask is needed. Moreover, removing the distribution alignment also impairs performance, _e.g._, 4.36 FID drop, since the distribution of raw fusion results \(O_{fuse}\) does not match \(_{T}\), and directly inject \(O_{fuse}\) to the denoising chain of \(_{T}\) can cause sub-optimal results.

    & StableSR & DiffBIR & PASD & CCSR & SeeSR & SUPIR & SeeSR+ReFIR & SUPIR+ReFIR \\  &  &  &  &  &  &  & (Ours) & (Ours) \\  NIQE\(\) & 3.7695 & 2.8458 & 5.1603 & 5.5082 & 4.7432 & 3.5076 & 4.4566(+0.2866) & 3.4593(+0.0483) \\ MUSIQ\(\) & 51.95 & 65.20 & 49.01 & 32.26 & 55.54 & 59.84 & 57.13(+1.59) & 60.49(+0.65) \\ CLIPIQAT\(\) & 0.6852 & 0.7845 & 0.5863 & 0.4568 & 0.6575 & 0.5692 & 0.6732(+0.0157) & 0.5722(+0.003) \\   

Table 2: Quantitative comparison on real-world degradation with RealPhoto datasets.

Figure 6: Quantitative comparison on RealPhoto dataset. More results are provided in Appendix H.

Domain preference problem.The motivation behind the proposed separate attention is to address the domain preference problem, _i.e._, the attention in \(_{T}\) will prefer to use latent from the same chain even though the latent from \(_{S}\) is more helpful for reconstruction. To verify the existence of the domain preference, we use the ground truth \(I_{HR}\) as the input of \(_{S}\) and compute the normalized attention scores between \(Q_{T}\) and \(K_{T}\), \(Q_{T}\) and \(K_{S}\). It can be seen in Fig. 8 that even using the spatially strictly aligned \(I_{HQ}\) as the reference, \(Q_{T}\) still has significantly high attention for the latent from the same chain, indicating that the domain preference problem interferes with the \(_{T}\)'s utilization of external knowledge in \(_{S}\). By contrast, the proposed separate attention can effectively mitigate this problem by forcing the \(Q_{T}\) to separately attend \(K_{T}\) and \(K_{S}\).

Other choices on injection position.In Sec. 3, we find the diffusion decoder is responsible for restoring textures. Based on this observation we propose to apply cross-image injection on the UNet decoder. Here, we ablate to analyze the impact of different cross-image injection positions. The results are shown in Tab. 5. It can be seen that performing cross-image injection only on the encoder will cause 19.57 FID drops. This is because the encoder focuses on the structure reconstruction, thus transferring the structure of \(_{S}\) will destroy the layout of the \(_{T}\). Moreover, performing injection only in the decoder achieves the best results since it can transfer the high-quality textures from the \(_{S}\). Due to the page limit, more ablation experiments can be seen in Appendix C.

### Discussions

What is the impact of the control scale?The scale \(s\) in Eq. (2) can control the extent to which the LRMs use external knowledge from the retrieved reference image for restoration. Here, we conduct an ablation study to explore the effect of \(s\). The results are shown in Fig. 9. It can be seen that when \(s\) takes smaller values, the model mainly uses the internal knowledge embedded in its own parameters, which can make the model hallucinate when the degradation is severe. For example, the model produces incorrect textures when \(s=0\). As \(s\) increases, the model starts to use external knowledge from the retrieved reference image, from which the model's hallucination problem can be alleviated. We also provide quantitative ablation experiments on \(s\) in Appendix C.

How much do the reference images affect performance?In the proposed framework, the retrieved images \(_{}\) is crucial in alleviating hallucinations. Here, we try to answer the role of \(_{}\) during restoration process, by manually controlling different types of retrieved images. As shown in Tab. 6,

   SA & SG & DA & PSNR\(\) & SSIM\(\) & NIQE\(\) & FID\(\) \\   & & 18.97 & 0.4665 & 4.5624 & 168.26 \\ ✓ & ✓ & 19.09 & 0.4799 & 4.3893 & 150.81 \\ ✓ & ✓ & 19.12 & 0.4724 & 4.2275 & 153.05 \\ ✓ & ✓ & ✓ & 19.00 & 0.4729 & 4.2085 & 148.69 \\   

Table 4: Effectiveness of different components in cross image injection.

Figure 8: The normalized attention scores are obtained by averaging all samples and all time steps.

Figure 7: The retrieval results with RealPhoto60 dataset  as the query images and DIV2K  as the HQ image database.

we find that using the exact ground truth \(I_{HQ}\) as the \(}\) can further improve the performance, which can be seen as an ideal up-bound. Interestingly, using \(I_{LQ}\) itself as its own retrieved image instead brings a slight improvement compared with no retrieval, which we attribute to the regularization effect from the distribution alignment strategy. Finally, randomly selecting a high-quality reference image even resulted in a huge performance degradation, suggesting that the content correlation is more important than the image quality for a favorable retrieved reference image.

**How does the proposed ReFIR work?** Extensive experiments have shown the state-of-the-art performance of our ReFIR. However, it seems not straightforward to understand how the retrieved reference images influence the image restoration process of the original LRMs. Here, we give an intuitive explanation. As shown in Fig. 10, for the latent at the \(t\)-th time step on the latent manifold, there are two forces in different directions pulling it to produce the latent at the next \(t-1\)-th time step. One force is from the internal knowledge of frozen weights in LRMs, and the other is the external knowledge from the retrieved reference image through the proposed cross image injection mechanism. These two forces ultimately determine the latent of the next time step. Therefore, a restored image from our ReFIR can utilize both the internal knowledge in the original LRMs as well as the external knowledge in the retrieved image, thus alleviating the hallucination of the LRMs.

## 6 Conclusion

This paper presents ReFIR, a training-free and generic framework that can alleviate the hallucination of LRMs to facilitate high-fidelity and photo-realistic restoration results through retrieval augmentation. We introduce the nearest neighbor lookup as a simple retriever to obtain relevant high-quality images and further propose the cross-image injection which employs separate attention to transfer knowledge while avoiding the domain preference problem, the spatial adaptive gating to address the spatial misalignment, and the distribution alignment to mitigate the domain gap during injection. Through expanding the knowledge boundary using the additional external knowledge from retrieved images, our ReFIR exhibits significant improvements on both fidelity and perceptual quality, as demonstrated through extensive qualitative and quantitative evaluations. Moreover, with its training-free and generic nature, our ReFIR can be easily applied to multiple LRMs.

  Settings & PSNR\(\) & SSIM\(\) & LPIPS\(\) & NIQE\(\) & FID\(\) \\  NoRef & 18.97 & 0.4665 & 0.4807 & 4.5624 & 168.26 \\ HQRef & 19.41 & 0.5033 & 0.3928 & 4.0764 & 137.52 \\ SelfRef & 19.16 & 0.4795 & 0.4761 & 4.5501 & 163.94 \\ Random & 19.53 & 0.5138 & 0.5354 & 5.3796 & 223.47 \\ Baseline & 19.00 & 0.4729 & 0.4341 & 4.2085 & 148.69 \\  

Table 6: The performance impact of reference images. NoRef means no reference image is used. HQRef denotes the corresponding \(I_{HQ}\) is used as the reference. SelfRef represents using \( 4\) bicubic upsampling of \(I_{LQ}\) for reference. Random means randomly selecting a high-quality image as the reference.

Figure 10: An explanation of how the proposed retrieval augmented framework affects the restoration process of existing LRMs.

Figure 9: Ablation visualization on the control scale \(s\). As \(s\) increases, the LRM utilizes the external knowledge from retrieved reference images to mitigate hallucination. Zoom in for better effects.