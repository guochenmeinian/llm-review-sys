# Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning

Gaussian Approximation and Multiplier Bootstrap for Polyak-Ruppert Averaged Linear Stochastic Approximation with Applications to TD Learning

 Sergey Samsonov

HSE University

svsamsonov@hse.ru

&Eric Moulines

Ecole Polytechnique,

MBUZAI

&Qi-Man Shao

Department of Statistics and Data Science,

Shenzhen International Center of Mathematics,

Southern University of Science and Technology

&Zhuo-Song Zhang

Department of Statistics and Data Science,

Shenzhen International Center of Mathematics,

Southern University of Science and Technology

&Alexey Naumov

HSE University,

Steklov Mathematical Institute

of Russian Academy of Sciences

###### Abstract

In this paper, we obtain the Berry-Esseen bound for multivariate normal approximation for the Polyak-Ruppert averaged iterates of the linear stochastic approximation (LSA) algorithm with decreasing step size. Moreover, we prove the non-asymptotic validity of the confidence intervals for parameter estimation with LSA based on multiplier bootstrap. This procedure updates the LSA estimate together with a set of randomly perturbed LSA estimates upon the arrival of subsequent observations. We illustrate our findings in the setting of temporal difference learning with linear function approximation.

## 1 Introduction

Stochastic approximation (SA) methods are a central component for solving various optimization problems that arise in machine learning [32; 26], empirical risk minimization  and reinforcement learning [42; 67]. There is a vast number of contributions in the literature, which cover both asymptotic [48; 53] and non-asymptotic [45; 15; 36] properties of the SA estimates. The primarily important property among the asymptotic ones of the SA estimates is their asymptotic normality , which is important due to its role in constructing (asymptotic) confidence intervals and hypothesis testing . However, a natural question of the rate of convergence in the appropriate central limit theorems (CLT) is not well addressed in literature even in the relatively simple setting of the linear stochastic approximation (LSA) [21; 34; 9].

Alternatively, confidence sets for SA algorithms can be constructed in a non-asymptotic manner based on concentration inequalities . These bounds are often regarded as loose , yielding suboptimal performance of the statistical procedures based on the latter estimates . In contrast, for statistical inference procedures based on independent and identically distributed (i.i.d.) observations, such as \(M\)-estimators , there is a machinery of non-parametric methods for constructing confidence sets with the bootstrap [19; 58]. This approach is accompanied with theoretical guarantees, showing the non-asymptotic validity of the bootstrap-based confidence intervals for parameters in linear regression  and statistical tests . Extending theoretical guarantees to a non-classical situation with online learning algorithms encounters serious problems, essentially related to the problem of obtaining rate of convergence in the corresponding CLTs. At the same time, many phenomena arising in the analysis of nonlinear SA algorithms already appear in the analysis of LSA problems.

The LSA procedure aims to find an approximate solution for the linear system \(}^{}=}\) with a unique solution \(^{}\) based on a sequence of observations \(\{((Z_{k}),(Z_{k}))\}_{k}\). Here \(:^{d d}\) and \(:^{d}\) are measurable functions and \((Z_{k})_{k}\) is a sequence of noise variables taking values in some measurable space \((,)\) with a distribution \(\) satisfying \([(Z_{k})]=}\) and \([(Z_{k})]=}\). We focus on the setting of independent and identically distributed (i.i.d.) observations \(\{Z_{k}\}_{k}\). With a sequence of decreasing step sizes \((_{k})_{k}\) and the starting point \(_{0}^{d}\), we consider the estimates \(\{_{n}\}_{n}\) given by

\[_{k}=_{k-1}-_{k}\{(Z_{k})_{k-1}-( Z_{k})\}\;,\;\;k 1,_{n}=n^{-1}_{k=n}^{2n-1} _{k}\;,\;\;n 1\;.\] (1)

Here, we have fixed the size of the _burn-in_ period (see, e.g., [16; 44]) to \(n_{0}=n\). Provided that \(n\) is large enough, the burn-in size affects only a constant factor in the subsequent bounds. The sequence \(\{_{k}\}_{k}\) corresponds to the standard LSA iterates, while \(\{_{n}\}_{n}\) corresponds to the Polyak-Ruppert (PR) averaged iterates [59; 53]. It is known that \(_{n}\) is asymptotically normal with a minimax-optimal covariance matrix (see  and  for discussion). Specifically, under appropriate technical conditions on the step sizes \(\{_{k}\}\) and noisy observations \(\{(Z_{k})\}\), it holds that

\[(_{n}-^{})}{{ }}(0,_{})\;,\]

where \(_{}\) is the asymptotic covariance matrix defined later in Section 3.1. There is a long list of contributions to the non-asymptotic analysis of \(_{n}\), particularly  and , which study moment and Bernstein-type concentration bounds for \((_{n}-^{})\). Unfortunately, such bounds do not imply Berry-Esseen type inequalities for \((_{n}-^{})\), that is, they do not allow us to control the quantity

\[_{n}^{}=_{B(^{d})}|(_{n}-^{}) B-( _{}^{1/2} B)|\;,\] (2)

where \((^{d})\) refers to the set of convex sets in \(^{d}\). While the Berry-Esseen bounds are a popular subject of study in probability theory, starting from the classical work , most results are obtained for sums of random variables or martingale difference sequences [52; 8]. We can only mention a few results for SA algorithms, see Section 2 for more details. This paper aims to provide the latter bounds for the specific setting of the LSA procedure. Our primary contribution is twofold:

* We establish a BerryEsseen bound for accuracy of normal approximation of the distribution of Polyak-Ruppert averaged LSA iterates with a polynomially decreasing step size. Our results suggest that the best rate of normal approximation, in the sense of (2), is of order \(n^{-1/4}\) up to logarithmic factors in \(n\), where \(n\) denotes the number of samples. Interestingly, this rate is achieved with an aggressive step size, \(_{k}=c_{0}/\). Our proof technique follows the Berry-Esseen bounds for nonlinear statistics provided in .
* We provide non-asymptotic confidence bounds for the distribution of the PR-averaged statistic \((_{n}-^{})\) using the multiplier bootstrap procedure. In particular, our bounds imply that the quantiles of the exact distribution of \((_{n}-^{})\) can be approximated at a rate of \(n^{-1/4}\), where \(n\) is the number of samples used in the procedure, provided that \(n\) is sufficiently large (see A4 for exact conditions). To the best of our knowledge, this is the first non-asymptotic bound on the accuracy of bootstrap approximation in SA algorithms. We apply the proposed methodology to the temporal difference learning (TD) algorithm for policy evaluation in reinforcement learning.

The rest of the paper is organized as follows. In Section 2, we provide a literature review on the non-asymptotic analysis of the LSA algorithm and bootstrap methods. Next, in Section 3, we analyze the convergence rate of Polyak-Ruppert averaged LSA iterates to the normal distribution. In Section 4, we discuss the multiplier bootstrap approach for LSA and establish bounds on the accuracy of approximating the quantiles of the true distribution. Finally, we apply our findings to TD learning and present numerical illustrations in Section 5.

**Notations.** For matrix \(A^{d d}\) we denote by \(\|A\|\) its operator norm. For symmetric matrix \(Q=Q^{} 0\;,\;Q^{d d}\) and \(x^{d}\) we define the corresponding norm \(\|x\|_{Q}=Qx}\), and define the respective matrix \(Q\)-norm of the matrix \(B^{d d}\) by \(\|B\|_{Q}=_{x 0}\|Bx\|_{Q}/\|x\|_{Q}\). For sequences \(a_{n}\) and \(b_{n}\), we write \(a_{n} b_{n}\) if there exist a constant \(c>0\) such that \(a_{n} cb_{n}\) for \(c>0\). For simplicity we state the main results of the paper up to constant factors.

Related works

Among contributions to the analysis of the LSA algorithm, we should mention the papers [53; 34; 9; 6]. These works investigate the asymptotic properties of the LSA estimates (such as asymptotic normality and almost sure convergence) under i.i.d. and Markov noise. Non-asymptotic results for the LSA and PR-averaged LSA estimates were obtained in [55; 47; 7; 35; 44], where MSE bounds were established, and in [43; 17; 16], which provided high-probability error bounds. The latter results enable the construction of Bernstein-type confidence intervals for the error \(_{n}-^{}\). Unfortunately, the corresponding bounds typically depend on unknown problem properties of (1), related to the design matrix \(}\) and the noise variables \((Z_{k})\), \((Z_{k})\). For this reason, applying these error bounds in practice is complicated. Furthermore, concentration bounds for the LSA error [43; 17; 16] do not imply convergence rates of the rescaled error \((_{n}-^{})\) to the normal distribution in Wasserstein or Kolmogorov distance. Non-asymptotic convergence rates were previously studied in  using the Stein method, but the resulting rate corresponds to a smoothed Wasserstein distance. Recent work  investigates convergence rates to the normal distribution in Wasserstein distance for LSA with Markovian observations. Both papers yield bounds that are less tight with respect to their dependence on trajectory length \(n\) than those presented in the present work, see a detailed comparison after Theorem 2.

A popular method for constructing confidence intervals in the context of parametric estimation is based on the bootstrap approach (). Its analysis has attracted many contributions, in particular a series of papers  and  that validate a bootstrap procedure for a test based on the maximum of a large number of statistics. Their study shows a close relationship between bootstrap validity results, Gaussian comparison and anticoncentration bounds for rectangular sets. The papers  and  investigate the applicability of likelihood-based statistics for finite samples and large parameter dimensions under possible model misspecification. The important step in proving bootstrap validity is again based on Gaussian comparison and anticoncentration bounds, but now for spherical sets. The bootstrap procedure for spectral projectors of covariance matrices is discussed in  and . The authors follow the same steps to prove the validity of the bootstrap.

Extending the classical bootstrap approach to online learning algorithms is a challenge. For example, the iterates \(\{_{k}\}_{k}\) determined by (1) are not necessarily stored in memory, which makes the classical bootstrap inapplicable. This problem can be solved by performing randomly perturbed updates of the online procedure, as proposed in  for the iterates of the Stochastic Gradient Descent (SGD) algorithm. The authors in  used the same procedure for the case of Markov noise and policy evaluation algorithms in reinforcement learning, but in both papers the authors only consider the asymptotic validity. In our paper we use the same multiplier bootstrap approach (see Section 4), but we provide an explicit error bound for the bootstrap approximation of the distribution of the statistics \((_{n}-^{})\).

In addition to the bootstrap approach, one can also use the pivotal statistics [37; 40; 41] or various estimates of the asymptotic covariance matrix  to construct the confidence intervals for \(^{}\). The latter approach can be based on the plug-in estimators , batch mean estimators  or in combination with the multiplier bootstrap approach . However, the theoretical guarantees for mentioned methods remain purely asymptotic.

## 3 Accuracy of normal approximation for LSA

We first study the rate of normal approximation for the tail-averaged LSA procedure. When there is no risk of ambiguity, we use simply the notations \(_{k}=(Z_{k})\) and \(_{k}=(Z_{k})\). Starting from the definition (1), we get with elementary transformations that

\[_{k}-^{}=(-_{k}_{k})(_{k-1}- ^{})-_{k}_{k}\,\] (3)

where we have set \(_{k}=(Z_{k})\) with

\[(z)=}(z)^{}-}(z)\,}(z)=(z)-}\,}(z)=(z)-}\ \.\]

Here the random variable \((Z_{k})\) can be viewed as a noise, measured at the optimal point \(^{}\). We now assume the following technical conditions:

**A 1**.: _Sequence \(\{Z_{k}\}_{k}\) is a sequence of i.i.d. random variables defined on a probability space \((,,)\) with distribution \(\)._

**A2**.: \(_{Z}{ A}(z){ d}(z)=\) _and \(_{Z}{ b}(z){ d}(z)=\), with the matrix \(-\) being Hurwitz. Moreover, \(\|\|_{}=_{z}\|(z)\|<+\), and the mapping \(z(z)\) is bounded, that is,_

\[{ C}_{ A}=_{z}\|{ A}(z)\|_{z}\|(z)\|<\.\] (4)

_Moreover, for the noise covariance matrix_

\[_{}=_{Z}(z)(z)^{}{ d}(z)\] (5)

_it holds that its smallest eigenvalue is bounded away from \(0\), that is,_

\[_{}:=_{}(_{})>0\.\] (6)

It is possible to change (4) to the moment-type bound as it was previously considered in  and , see the detailed discussion after Theorem 2. The fact that the matrix \(-\) is Hurwitz implies that the linear system \(=\) has a unique solution \(^{}\). Moreover, this fact is sufficient to show that the matrix \({ I}-\) is a contraction in an appropriate matrix \(Q\)-norm for small enough \(>0\). Precisely, the following result holds:

**Proposition 1**.: _Let \(-\) be a Hurwitz matrix. Then for any \(P=P^{}\), there exists a unique matrix \(Q=Q^{}\), satisfying the Lyapunov equation \(^{}Q+Q=P\). Moreover, setting_

\[a=(P)}{2\|Q\|}\,_{}= (P)}{2_{Q}\|{ A}\|_{Q}^{2}}(P)}\,\] (7)

_where \(_{Q}=_{}(Q)/_{}(Q)\), it holds for any \([0,_{}]\) that \( a 1/2\), and_

\[\|{ I}-\|_{Q}^{2} 1- a\.\] (8)

The proof of Proposition 1 is provided in Appendix D.1. Note that it is possible to set \(P={ I}\) as in , yet it is possible that other choices of \(P\) could be more beneficial for particular applications. Now consider an assumption on the step sizes \(_{k}\) and number of observations \(n\):

**A3**.: _The step sizes \(\{_{k}\}_{k}\) has a form \(_{k}=c_{0}/k^{}\), where \([1/2;1)\) and \(c_{0}(0;_{} a(1-)]\). Moreover, we assume that \(n d\), and_

\[}{(1+ n) n}_{Q}\,{  C}_{}^{}}{a(1-)}(1-)}\,\ \ =1/2\,\\ }{ n}_{Q}\,{ C}_{} ^{}}{a(2-1)(1-(1/2)^{1-})}(1-(1/2)^{1-}}\,\ \ 1/2<<1\.\] (9)

The main aim of lower bounding \(n\) is to ensure that the number of observations is large enough in order that the LSA error related to the choice of initial condition \(_{0}-^{}\) becomes small.

### Central limit theorem for Polyak-Ruppert averaged LSA iterates.

It is known that the assumptions A1-A3 guarantee that the CLT applies to the iterates of \(_{n}\), namely,

\[(_{n}-^{})}{{}}{  N}(0,_{})\,\] (10)

where the asymptotic covariance matrix \(_{}\) has a form

\[_{}=^{-1}_{}^{-},\] (11)

and \(_{}\) is defined in (5). This result can be found for example in  and . We are interested in the Berry-Esseen type bound for the rate of convergence in (10), that is, we aim to bound \(_{n}^{ Conv}\) defined in (2) w.r.t. the available sample size \(n\). We control \(_{n}^{ Conv}\) using a method from  based on randomized multivariate concentration inequality. Below we briefly state its setting and required definitions. Let \(X_{1},,X_{n}\) be independent random variables taking values in \({ X}\) and \(T=T(X_{1},,X_{n})\) be a general \(d\)-dimensional statistics such that \(T=W+D\), where

\[W=_{=1}^{n}_{}, D:=D(X_{1},,X_{n})=T-W,\] (12)\(_{}=h_{}(_{})\) and \(h_{}:^{d}\) is a Borel measurable function. Here the statistics \(D\) can be non-linear and is treated as an error term, which is "small" compared to \(W\) in an appropriate sense. Assume that \([_{}]=0\) and \(_{=1}^{n}[_{}_{}^{}]=_{d}\). Let \(=_{n}=_{=1}^{n}[\|_{}\|^{3}]\). Then, with \((0,_{d})\),

\[_{B(^{d})}|(T A)-(  A)| 259d^{1/2}+2[\|W\|\|D\|]+2_{=1}^{n} [\|_{}\|\|D-D^{()}\|],\] (13)

where \(D^{()}=D(X_{1},,X_{-1},X_{}^{},X_{+1},,X_{n})\) and \(X_{}^{}\) is an independent copy of \(X_{}\). This result is due to [63, Theorem 2.1]. One can modify the bound (13) for the setting when \(_{=1}^{n}[_{}_{}^{}]= 0\). This result due to [63, Corollary 2.3]. Following the construction (12), we set \(T=}(_{n}-^{})\) and consider it as a nonlinear statistic of i.i.d. random variables \(Z_{1},,Z_{2n}\), which drive the LSA dynamics (1). We can exactly represent \(T\) as a sum of linear (\(W\)) and non-linear parts (\(D\)), where

\[W=-}_{k=n}^{2n-1}_{k+1}, D =}-^{}}{_{n}}-}-^{}}{_{2n}}-}_{k= n+1}^{2n}(_{k}-})(_{k-1}-^{})\\ +}_{k=n+1}^{2n}(_{k-1}-^{ })(}-}).\]

The proof of this result can be bound in Proposition 3. To obtain a bound for the approximation accuracy in (2) using the bound (13), we need to upper bound \(^{1/2}[\|D(Z_{1},,Z_{2n})\|^{2}]\) and \([\|D-D^{(i)}\|]\). The first result below provides a second moment bound on \(D\):

**Theorem 1**.: _Assume A1, A2, and A3. Then we obtain the following error bound:_

\[^{1/2}[\|D(Z_{1},,Z_{2n})\|^{2}] }\|\|_{}}{}}(}+\,_{}}{ }})\] \[+}_{1}-an^{1 -}}{2(1-)}}\|_{0}-^{}\|\,\]

_where \(\) stands for inequality up to an absolute constant, and \(_{1}=_{1}(n,a,_{},c_{0})\) is a polynomial function defined in Appendix A.3, eq. (29)._

The proof of Theorem 1 is provided in Appendix A.3. Now it remains to upper bound the term \([\|D-D^{(i)}\|]\), which is done in Appendix B.1 using the synchronous coupling methods . Combining these bounds, we obtain the following theorem:

**Theorem 2**.: _Assume A1, A2, and A3. Then the following bound holds:_

\[_{n}^{}\|\|_{}^{3}}{ _{}^{3/2}}+}(_{1 }}{n^{(1-)/2}}+_{2}}{n^{/2}})+}{_{}}-an^{1-}}{2(1-)}} \|_{0}-^{}\|\,\] (14)

_where \(_{2}=_{2}(n,a,_{},_{ },c_{0})\) is a polynomial function defined in (35), and constants \(_{1},_{2}\), depending upon \(a,_{},_{Q},_{},c_ {0}\), are defined in Appendix B, eq. (36)._

The proof of Theorem 2 is provided in Appendix B. Note that the assumption A2 requires that \((Z_{1})\) is almost sure bounded. It is a strong assumption, but it can be partially relaxed. Following the stability of matrix products technique, used in [17, Proposition 3], it is possible to consider the setting when the random variable \(\|}(Z_{1})\|\) has only finite number of moments. In particular, we expect that assuming finite third moment of \(\|}(Z_{1})\|\) and \(\|(Z_{1})\|\) is sufficient to obtain a counterpart to Theorem 1. However, this generalization requires non-trivial technical work on generalizing the stability of matrix products result (see Corollary 4 in Appendix D ).

Note that the bound of Theorem 2 predicts the optimal error of normal approximation for Polyak-Ruppert averaged estimates of order \(n^{-1/4}\), which is achieved with the aggressive step size \(_{k}=c_{0}/\), that is, when setting \(=1/2\) in (14). In this case we obtain the optimized bound

\[_{n}^{}_{3}}{_{}n^{1/4}}+ \|\|_{}^{1}}{_{}^{3/2}}+-c_{0}a}}{_{}}\|_{0}- ^{}\|\,\] (15)

where \(_{3}=_{3}(a,_{},_{Q}, _{},\|\|_{})\) is provided in (36).

Discussion.Our proof technique of Theorem 2 reveals an interesting feature: fastest rate of convergence in the convex distance \(_{n}^{}\) corresponds to the learning rate schedule that admits the fastest decay of the second-order term in the MSE bound for remainder statistics \(D\) (see Theorem 1). Results similar to the one of Theorem 2 have been recently obtained in the literature in  and . The author in  considers the LSA problem specified to the temporal-difference learning (see Section 5) with Markov noise and obtains convergence rate in Wasserstein distance of order \(n^{-1/4}\), which corresponds to the "optimal" step size schedule \(_{k}=c_{0}/k^{3/4}\). Using the bound of [49, eq. (3)] (see also section 2 in ), this result yield a suboptimal bound of order \(n^{-1/8}\) for the convex distance \(_{n}^{}\). Such an upper bound may be loose for some classes of distributions, but it is not clear if in particular setting of LSA the bound of  could imply scaling of order \(n^{-1/4}\) for \(_{n}^{}\). At the same time, in case of \(X_{1},,X_{n}\) forming a Markov chain in (12) there is no available counterpart of the bound (13). Generalizing (13) is an interesting research direction that would allow to obtain a counterpart of Theorem 2 in case of Markovian dynamics. Similarly, the result of  holds for much stronger metrics, which controls the convergence of moments of twice differentiable functions. We provide additional details about connections between this metric and \(_{n}^{}\) in Appendix B.2. At the same time, the authors in  cover the non-linear setting of PR-averaged iterates of stochastic gradient descent algorithm under strong convexity.

**Remark 1**.: _The leading (with respect to \(n\)) terms of the bound from Theorem 1 have an implicit dependence on the problem dimension \(d\) due to the presence of \(_{}\). Yet the result of Theorem 1 can be improved in a sense of dependence in dimension if one is interested not in the rates of convergence for \((_{n}-^{})\), but in the projected iterated \(^{}(_{n}-^{})\) for some \(^{d m}\), \(m d\). If this is the case, one may apply (13) for the class \(_{m}=(^{m})\) of convex sets in \(^{m}\) and obtain, setting step size \(_{k}=c_{0}/\), and \(_{}^{()}=_{}^{}\), that_

\[_{n}^{}_{4}}{_{}n ^{1/4}}+\|\|_{}^{3}}{_{}^{3/2 }}+^{-c_{0}a}}{_{ }}\|_{0}-^{}\|\,\]

_and the constant \(_{4}=_{4}(a,_{},_{Q}, \,_{}^{()},\|\|_{})\) is provided in (36)._

**Remark 2**.: _Results similar to Theorem 1 can be obtained not only for the Polyak-Ruppert averaged estimator \(_{n}\), but also for the last iterate \(_{n}\). In particular, it is known (see e.g. ), that the last iterate error \(_{n}-^{}\) is also asymptotically normal:_

\[-^{}}{}}(0,_{ })\,\]

_where the covariance matrix \(_{}\) is different from \(_{}\). In such a case \(_{}\) can be found as a solution to appropriate Lyapunov equation, see . Then, we expect that it is possible to use the perturbation-expansion approach from  together with randomized concentration inequalities  (see (13)), in order to obtain the Berry-Esseen bound_

\[_{B(^{d})}|-^{}}{}} B-(_{ }^{1/2} B)|}\.\]

_We leave the detailed derivation for future work._

## 4 Multiplier bootstrap for LSA

In order to perform statistical inference with the Polyak-Ruppert estimator \(_{n}\), we propose an online bootstrap resampling procedure, which recursively updates the LSA estimate as well as a large number of randomly perturbed LSA estimates, upon the arrival of each data point. The suggested procedure follows the one outlined in . It has the following advantages: it does not rely on the asymptotic distribution of the error \((_{n}-^{})\), does not require to know the moments of \((_{n}-^{})\) or its asymptotic covariance matrix \(_{}\), and does not involve any data splitting.

We state the suggested procedure as follows. Let \(^{2n}=\{W_{}\}_{1 2n}\) be a set of i.i.d. random variables, independent of \(^{2n}=\{Z_{}\}_{1 2n}\), with \([W_{1}]=1\) and \([W_{1}]=1\). We write, respectively, \(^{}=(|^{2n})\) and \(^{}=(|^{2n})\) for the corresponding conditional probability and expectation. In parallel with procedure (1) that generates \(\{_{k}\}_{1 k 2n}\) and \(_{n}\), we generate \(M\) independent samples \((w_{n}^{},,w_{2n}^{})\), \(1 M\) distributed as \(^{2n}\), and recursively update \(M\) randomly perturbed LSA estimates, that is,

\[_{k}^{,}&=_{k -1}^{,}-_{k}w_{k}^{}\{(Z_{k})_{k-1}^{ ,}-(Z_{k})\}\,\ \ k n+1\,\ _{n}^{,}=_{n}\,\\ _{n}^{,}&=n^{-1}_{k=n}^ {2n-1}_{k}^{,}\,\ \ n 1\.\] (16)We use a short notation \(_{n}^{}\) for \(_{n}^{,1}\). The key idea of the procedure (16) is that the "Bootstrap-world" distribution (that is, the one conditional on \(^{2n}\)) of the perturbed samples \((_{n}^{}-_{n})\) is close to the distribution of the quantity of interest, that is, \((_{n}-^{})\). Precisely, the main result of this section will show that the quantity

\[_{B(^{d})}|^{}( (_{n}^{}-_{n}) B)-((_{n}-^{}) B)|\] (17)

is small. Although an analytic expression for \(^{}((_{n}^{}-_{n})  B)\) is not available, one can approximate it from numerical simulations according to (16) by generating sufficiently large number \(M\) of perturbed trajectories. Standard arguments, see e.g. [62, Section 5.1] suggest that the accuracy of Monte-Carlo approximation is of order \(M^{-1/2}\). To analyze the suggested procedure, we shall impose an additional assumption on the trajectory length \(n\):

**A4**.: _Assumption A3 holds with \(=1/2\), and \(c_{0} 1/(_{}^{2}\,_{Q})\). Moreover, setting_

\[h(n)=(_{}\,_{Q}^{1/2}}{( -1)a})^{2}(1+2)})^{2}\,,\] (18)

_it holds that_

\[}{h(n)}-1)}}{_{ }}\,,}{^{2}n}(1\,_{}^{2}) }{a} c_{0}a\,_{}^{2}}\] (19)

_Moreover, we assume that for \(_{}\) defined in (6) it holds that_

\[_{} 8\|\|_{} \| n}{n}}+\|+\|\|_{}^{2})  n}{n}\] (20)

Note that the new bound (19) simply states that \(/^{2}(n)\) is sufficiently large, since \(h(n)\) scales as \(^{2}n\). We discuss the assumption A4 in more details in the proof scheme. Now we formulate the main result of this section. We analyze only the setting of polynomially decaying step size with \(=1/2\), since decay rate of (17) essentially depends on the approximation rate of Theorem 2, with the fastest rate achieved when \(=1/2\). For other learning rates the decay rate of right-hand side in Theorem 3 will be slower. For simplicity, we do not trace the dependence of the bound below on the parameter \(c_{0}\).

**Theorem 3**.: _Assume A1, A2, A3 with \(=1/2\), and A4. Then with \(\) - probability at least \(1-6/n\) it holds that_

\[_{B(^{d})}|^{}( (_{n}^{}-_{n}) B)-(( {}_{n}-^{}) B)|^{2}(_{ }^{4}\,\, 1)(1+\|\|_{}^{2}) n}{a^{5/2} _{}n^{1/4}}\]

_where \(_{3}=_{3}(n,a,_{},\|\|_{})\) is a polynomial function defined in Appendix C, eq. (46)._

The proof of Theorem 3 is based on the Gaussian approximation performed both in the "real" world and bootstrap world together with an appropriate Gaussian comparison inequality. The main steps of the proof are illustrated by the following scheme:

\[}(_{n}-^{ })\ }}(0,_{ })\\ }}\]

\[}(_{n}^{}-_{n})}}^{}(0,_{ }^{})\]

In the above scheme we have denoted by \(_{}^{}=n^{-1}_{=n}^{2n-1}_{} _{}^{}\) the sample covariance matrix approximating \(_{}\). Gaussian approximation for the true distribution of \(}(_{n}-^{})\) follows from Theorem 2. Proof of Gaussian approximation in the Bootstrap world Theorem 4 is also based on the inequality (13), but is more complicated and involves the expansion analysis of the LSA error from . This technique allows to separate the LSA error into different scales with respect to the step sizes \(\{_{k}\}\), see Appendix C.4 for details. However, this technique requires to impose additional assumption A4 - eq. (19). Proof of the Gaussian comparison part of Theorem 5 is based on Pinsker's inequality and matrix Bernstein inequality. The latter result requires that \(n\) is large enough to ensure that minimal eigenvalue of \(_{}^{}\) is close to \(_{}\), justifying the assumption A4 - eq. (20). Detailed proof if provided in Appendix C.

**Discussion.** We emphasize that the Gaussian approximation result of Theorem 2 (with Bootstrap world generalization in Theorem 4) is a key result to prove the above bootstrap validity. This argument was missing in the earlier works studying confidence intervals for stochastic optimization algorithms , where the authors considered procedures to estimate \(_{}\) in (11). They combine _non-asymptotic_ bounds on the accuracy of recovering \(_{}\) with only _asymptotic_ validity of the resulting confidence intervals. We expect that our proof technique for Theorem 2 can be used to provide similar non-asymptotic validity results for outlined approaches for constructing confidence intervals based on the estimation of the asymptotic covariance matrix.

**Corollary 1**.: _(Set of Euclidean balls or ellipsoids) Suppose that we are interested in estimating quantile of a given order \((0,1)\) and some matrix \(B^{d d}\), that is, the quantity_

\[t_{}=\{t>0:(\|B(_{n}-^{}) \| t)\}.\]

_We define its counterpart in the Bootstrap world, \(t_{}^{}\), as_

\[t_{}^{}=\{t>0:^{}(\|B(_{n}^{}-_{n})\| t)\}.\]

_Note that \(t_{}^{}\) is defines with respect to the bootstrap measure, therefore, it depends on the data \(^{2n}\). This bootstrap critical value \(t_{}^{}\) is applied in the Bootstrap world to build the confidence set_

\[()=\{^{d}:\|B(- _{n})\| t_{}^{}\}\;.\]

_Theorem 3 justifies this construction and evaluate the coverage probability of the true value \(^{}\) by this set. It states that_

\[(^{}())=(\|B( _{n}-^{})\|>t_{}^{})\;,\]

_with the error of order \(n^{-1/4}\) in the right-hand side. Although an analytic expression for \(t_{}^{}\) is not available, one can approximate it by generating a large number \(M\) of independent samples of \(_{n}\) and computing from them the empirical distribution function of \(\|B(_{n}^{}-_{n})\|\), following (16)._

**Remark 3**.: _A natural question that arises after Theorem 3 is whether it is possible to prove similar bounds for the iterates of first-order stochastic optimization algorithms. There are several MSE bounds for corresponding algorithms with explicit dependence on the step size \(_{k}\); see, for example, . Therefore, we expect that it is possible to obtain a counterpart to Theorem 2. At the same time, for general first-order stochastic optimization algorithms, unlike LSA, there are no counterparts to the precise error expansions of . Thus, proving the counterpart of Theorem 3 in this setting is more challenging. Similarly, we emphasize that generalizations of the procedure (16) to cases where \(\{Z_{k}\}_{k}\) are dependent, for example, form a Markov chain, are complicated. The approach of  is not directly applicable in this setting, and appropriate generalization of (13) is a separate and challenging research direction._

## 5 Applications to the TD learning and numerical results

We illustrate our findings for the setting of temporal difference (TD) learning algorithm  for policy evaluation in RL. Non-asymptotic error bounds for this algorithm attracted lot of contributions . At the same time, confidence intervals for TD were studied in  only in terms of their asymptotic validity. In the TD algorithm we consider a discounted MDP (Markov Decision Process) given by a tuple \((,,,r,)\). Here \(\) and \(\) stand for state and action spaces, and \((0,1)\) is a discount factor. Assume that \(\) is a complete metric space with metric \(_{}\) and Borel \(\)-algebra \(()\). \(\) stands for the transition kernel \((B|s,a)\), which determines the probability of moving from state \(s\) to a set \(B()\) when action \(a\) is performed. Reward function \(r\) is assumed to be deterministic. _Policy_\((|s)\) is the distribution over action space \(\) corresponding to agent's action preferences in state \(s\). We aim to estimate _value function_

\[V^{}(s)=_{k=0}^{}^{k}r(s_{k},a_{k})|s_{0}=s \;,\]where \(a_{k}(|s_{k})\), and \(s_{k+1}(|s_{k},a_{k})\) for any \(k\). Define the transition kernel under \(\),

\[_{}(B|s)=_{}(B|s,a)(a|s)\,\] (21)

which corresponds to the \(1\)-step transition probability from state \(s\) to a set \(B()\). The state space \(\) here can be arbitrary. It is a common option to consider the _linear function approximation_ for \(V^{}(s)\), defined for \(s\), \(^{d}\), and a feature mapping \(^{d}\) as \(V^{}_{q}(s)=^{}(s)\). Here \(d\) is the dimension of feature space. Our goal is to find a parameter \(^{}\) which is defined as a unique solution to the projected Bellman equation, see . We denote by \(\) the invariant distribution over the state space \(\) induced by \(^{}(|s)\) in (21). We define the _design matrix_\(_{}\) as

\[_{}=_{}[(s)(s)^{}]^{ d d}\.\] (22)

Consider the following assumptions on the generative mechanism and on the feature mapping \(()\):

**TD 1**.: _Tuples \((s,a,s^{})\) are generated i.i.d.with \(s\), \(a(|s)\), \(s^{}(|s,a)\)._

**TD 2**.: _Matrix \(_{}\) is non-degenerate with the minimal eigenvalue \(_{}(_{})>0\). Moreover, the feature mapping \(()\) satisfies \(_{s}\|(s)\| 1\)._

In the setting of linear function approximation the estimation of \(V^{}(s)\) reduces to estimating \(^{}^{d}\), which can be done via the LSA procedure. Here, the \(k\)-th step randomness is given by the tuple \(Z_{k}=(s_{k},a_{k},s^{}_{k})\). Then, the corresponding LSA update can be written as

\[_{k}=_{k-1}-_{k}(_{k}_{k-1}-_{k})\,\] (23)

where \(_{k}\) and \(_{k}\) are given, respectively, by

\[_{k}=(s_{k})\{(s_{k})-(s^{}_{k})\} ^{}\,_{k}=(s_{k})r(s_{k},a_{k})\.\]

We provide the expressions for the corresponding system matrix \(}=[_{k}]\) and the right-hand side \(}\) in Appendix E. We verify that assumption A2 holds and, furthermore, we provide a tighter counterpart to the result of Proposition 1. This result closely follows  and .

**Proposition 2**.: _Let \(\{\}_{k}\) be a sequence of TD updates generated by (23) under **TD 1** and **TD 2**. Then this update scheme satisfies assumption A2 with_

\[_{}=2(1+)\,\|\|_{}=2(1+ )(\|^{}\|+1)\,\]

_moreover, one can check that \(\|-\|^{2} 1- a\) with_

\[a=(1-)_{}(_{})\,_{}=(1- )/(1+)^{2}\,\]

_that is, Proposition 1 holds with \(Q=\)._

Proof of Proposition 2 is provided in Appendix E. Since all the assumptions in A2 are fulfilled, we can verify tightness of the bound Theorem 2 for different learning rate schedules \(_{k}\) in (23).

Numerical results.Efficiency of the multiplier bootstrap approach (16) to the problems of constructing confidence sets in online algorithms has been demonstrated in the works  and . We aim to illustrate the tightness of our bounds for normal approximation outlined in Theorem 2 in the setting of TD learning with linear function approximation. To this end, we consider the classical Garnet problem , in the simplified version proposed by . This problem is characterized by the number of states \(N_{s}\), number of actions \(a\), and branching factor \(b\) (i.e. the number of neighbors of each state in the MDP). We set these values to \(N_{s}=10\), \(a=2\) and \(b=3\), and aim to evaluate the value function of the randomly generated policy \((|s)\). Details on the way the policy \(\) is set can be found in Appendix F. We consider the problem of policy evaluation in this MDP using the TD learning algorithm with identity feature mapping, that is, \((s)=e_{s}\) (that is, \(s\)-th coordinate vector) for \(s\{1,,N_{s}\}\). We run the procedure (23) with the learning rates \(_{k}=c_{0}/k^{}\) and different powers \(\{0.5,0.65,0.7\}\). For each of the experiments we aim to estimate the supremum

\[_{n}:=_{x}(\|_{n}- ^{}\| x)-(\|_{}^{1/2}\| x)\,\] (24)

\((0,_{N_{s}})\), and show that this supremum scales as \(n^{-1/4}\) when \(=1/2\) and admits slower decay for other powers of \(\). We approximate true probability \((\|_{}^{1/2}\| x)\) by the corresponding empirical probabilities based on sample of size \(M n\). Second, for \(n\{1600,,1638400\}\), where next sample size is twice larger than the previous one, we generate \(N=6553600\) trajectories of TD algorithm and approximate the distribution of \(\|_{n}-^{}\|\) based on the corresponding empirical distribution. We report our results in Figure 1, showing that the smallest values of \(_{n}\) correspond to the step size schedule \(=1/2\), moreover, the decay rate \(n^{-1/4}\) seems to be tight, otherwise one should expect further decay of \(_{n}n^{1/4}\). Additional simulations are provided in Appendix F.

## 6 Conclusion

In this paper, we have established, to the best of our knowledge, the first fully non-asymptotic confidence bounds for parameter estimation in the LSA algorithm using the multiplier bootstrap. This result is based on a novel Berry-Esseen bound for the Polyak-Ruppert averaged LSA iterates, which is of independent interest. Our paper suggests several interesting directions for further research. First, our Berry-Esseen bounds are obtained using the randomized concentration inequality , and it would be valuable to generalize this approach to the setting of Markov chains. Second, it is natural to extend our results to the first-order gradient methods, both for stochastic optimization and variational inequalities. Third, it becomes possible to prove the fully non-asymptotic validity of confidence intervals obtained with plug-in techniques or other estimators of the asymptotic covariance matrix of \(_{n}\). These could then be compared with the multiplier bootstrap confidence intervals in terms of their dependence on problem dimension \(d\) and other instance-dependent quantities.