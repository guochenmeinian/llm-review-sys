# Relationship Prompt Learning is Enough for

Open-Vocabulary Semantic Segmentation

Jiahao Li\({}^{1}\), Yang Lu\({}^{1,2,3}\), Yuan Xie\({}^{*,4,5}\) and Yanyun Qu\({}^{*}\)\({}^{1,2,3}\)

\({}^{1}\)School of Informatics, Xiamen University

\({}^{2}\)Institute of Artificial Intelligence, Xiamen University

\({}^{3}\)Key Laboratory of Multimedia Trusted Perception and Efficient Computing, Ministry of Education of China,

Xiamen University

\({}^{4}\)School of Computer Science and Technology, East China Normal University

\({}^{5}\)Chongqing Institute of East China Normal University

###### Abstract

Open-vocabulary semantic segmentation (OVSS) aims to segment unseen classes without corresponding labels. Existing Vision-Language Model (VLM)-based methods leverage VLM's rich knowledge to enhance additional explicit segmentation-specific networks, yielding competitive results, but at the cost of extensive training cost. To reduce the cost, we attempt to enable VLM to directly produce the segmentation results without any segmentation-specific networks. Prompt learning offers a direct and parameter-efficient approach, yet it falls short in guiding VLM for pixel-level visual classification. Therefore, we propose the **R**elationship **P**rompt **M**odule (**RPM**), which generates the relationship prompt that directs VLM to extract pixel-level semantic embeddings suitable for OVSS. Moreover, RPM integrates with VLM to construct the **R**elationship **P**rompt Network (**RPN**), achieving OVSS without any segmentation-specific networks. RPN attains state-of-the-art performance with merely about **3M** trainable parameters (2% of total parameters).

## 1 Introduction

Open-vocabulary semantic segmentation (OVSS)  aims to segment novel classes without corresponding training images, which is still a challenging task in computer vision. Vision-Language Model (VLM)  has emerged as a powerful approach, acquiring comprehensive knowledge via large-scale image-caption matching training. Several VLM-based OVSS methods  achieve promising results. These methods employ the rich image-text representation knowledge inherent in VLM to improve segmentation performance and are categorized into two types: two-stage and one-stage methods. Two-stage methods  first generate image-level masks without semantics via a well-designed mask proposal network  and then classify these masks via the image-level classification ability of VLM. One-stage methods  employ a semantic decoding network to distill VLM's comprehensive knowledge from the image to the pixel level, thereby producing pixel-level segmentation results.

However, both of these VLM-based methods rely on additional explicit segmentation-specific networks to obtain segmentation results, resulting in extensive training cost. To reduce the training cost, an intuitive idea is to make VLM directly produce segmentation results without these segmentationspecific networks1. In this context, prompt learning [5; 21; 22; 23] emerges as a practical approach, guiding VLM to transform image-text pair embeddings into pixel-level semantic embeddings suitable for OVSS, thereby directly achieving OVSS.

Without explicit segmentation-specific networks, applying prompt learning solely to VLM for OVSS is straightforward yet challenging. Typically, existing prompt learning methods fine-tune task-specific models to enhance performance on that task, yet they fail to secure cross-task performance gains. These methods employ either fixed or trainable vision prompt tokens , or they construct complex ViT-based networks for generating prompt [25; 26; 20; 27]. The limitations of such prompt include: 1) providing only image-level granularity, which restricts VLMs from performing tasks related to pixel-level visual classification, and 2) a lack of an image-text relationship, which hinders the exploration of VLMs' potential for open-vocabulary scene understanding. Therefore, it is difficult for these methods to enable VLM suitable for image-level classification to achieve open-vocabulary semantic segmentation directly.

In summary, addressing the above-mentioned issue lies in refining the granularity of prompt and strengthening the image-text relationship within them. By analyzing the outputs of VLM's encoder layer, we find they can construct an image-text relationship attention map via the attention mechanism, guiding the encoder to focus on relevant pixels. Thus, we propose the **R**elationship **P**rompt **M**odule (**RPM**) that utilizes the outputs of image and text encoding layers to enable pixel-level relationship prompting, enhancing prompt's granularity and image-text relationship. Moreover, we implement a layer-by-layer guidance mode in VLM, enabling a progressive transfer of embeddings from the image to pixel level. As illustrated in Figure 1, each layer's relationship attention map is continuously refined following an image-to-pixel attention scheme. To obtain the segmentation results, we propose the **L**inear **P**rojection **M**odule (**LPM**) comprising merely two individual linear layers, which maps the image and text feature into a shared space, and then computes their Matrix product to produce the results. Finally, we propose the **R**elationship **P**rompt **N**etwork (**RPN**), which consists of RPM, LPM and VLM. Figure 2 shows the comparison between RPN and other OVSS methods. RPN employs VLM to directly output pixel-level predictions by prompt learning, while other methods use VLM to assist explicit segmentation-specific networks to obtain predictions. In these VLM-based methods, VLM transfers its rich knowledge to the mask proposal network by knowledge distillation or enables the semantic decoder to output segmentation masks by feature adaptation. The key difference between RPN and existing VLM-based methods is that RPN does

Figure 1: **Visualization of relationship attention map \(\) from the well-trained RPN.** The degree of attention from low to high is marked by colors from dark blue to red. The more attention, the darker red; the less attention, the darker blue. As the layers deepen, the attention maps exhibit increasingly precise pixel-level semantics. The images correspond to the attention maps for an aeroplane, a sheep, and a train, respectively.

Figure 2: **Our method _vs_ existing VLM-based methods.** Our method employs VLM to directly perform OVSS by prompt learning, while the other methods relies on additional explicit segmentation-specific networks.

not require any explicit segmentation-specific networks; it only adapts VLM to perform OVSS. Our contributions are summarized as follows:

* We propose the **R**elationship **P**rompt **M**odule (**RPM**), which generates pixel-level relationship prompt to guide VLM in transforming image-level embeddings to pixel-level ones suitable for OVSS.
* We propose the **R**elationship **P**rompt **N**etwork (**RPN**), employing prompt learning solely to adapt VLM for OVSS without explicit segmentation-specific networks.
* RPN attains state-of-the-art results on four public benchmarks by optimizing about **3M** trainable parameters (2% of total parameters).

## 2 Related Works

Open-Vocabulary Semantic Segmentation.Open-vocabulary semantic segmentation [28; 29] aims to leverage the knowledge from representation distributions of seen categories to classify unseen categories. Existing methods can be divided into two types: generative and discriminative. Generative methods [30; 31; 28; 32] require the segmentation network to be aware of which categories are unseen during training, while discriminative methods [11; 8; 19; 33] directly transfer semantics from seen to unseen categories, which is more straightforward. SPNet  introduces the zero-shot task for the first time and proposes an end-to-end training paradigm, in which the visual embeddings are composed with the uniform semantic word embeddings to obtain the semantic logits. ZS3Net  utilizes a generative approach to project the text embeddings into visual space and generate visual embeddings for unseen categories. Subsequently, many works following the generative method have been proposed. STRICT  assumes that the pixels of unseen categories can be present during training the model and adopts the self-training strategy to optimize the model for classifying the unseen categories. GaCNet  proposes a novel context-aware feature generation method based on ZS3Net, in which pixel-wise contextual knowledge can be utilized to guide the feature generation process of unseen categories. CLIP-based approaches have also made great progress. ZegFormer  proposes two sub-tasks, i.e., class-agnostic grouping and segment-level zero-shot classification and presents the CLIP-based method for the first time. MaskCLIP  utilizes the frozen CLIP to make a minimal adaptation by fine-tuning a lightweight classifier and replacing it with that of the segmentation network. Zsseg  proposes a two-stage CLIP-based method, in which a proposal generator is used to generate binary masks and CLIP is required to classify them. ZegCLIP  presents a one-stage method in which CLIP directly transfers knowledge to a lightweight decoder.

Vision-Language Models for Vision Tasks.Vision-language models for vision tasks [5; 21; 22; 23] are optimized with a large scale of image-text pair data on the internet. There are three categories: contrastive, generative, and aligned objectives. CLIP  first proposes the paradigm of pre-trained vision-language model. DeCLIP  argues that CLIP is data-intensive and proposes a data-efficient training paradigm. UniCL  combines the two data sources to build a new image-text-label field and proposes unified contrastive learning. ZeroVL  proposes debiased sampling to deal with biased representation distributions and a new mixup method for the image and text models. OTTER  uses optimal transport to find the soft label for contrastive learning and handle the problem of noisy image-text pairs.

Visual Prompt Learning.Visual Prompt Learning [37; 38; 39] is a technique that assists in adapting CLIP-like vision-language models for various visual tasks. CoOp  adopts trainable vectors as word prompt to adapt CLIP for vision classification. VP  utilizes perturbations as visual prompt. VPT  proposes trainable visual prompt to adapt each layer of the visual embeddings. UPT  constructs unified prompt modeling to extract trainable visual and textual prompt for adapting CLIP. MaPLe  adopts trainable prompt to guide both visual and textual embeddings and proposes a coupling function as a bridge to build a multi-modal prompt. DenseCLIP  uses the contextual information from the image to prompt the language model. Probabilistic prompt  applies multiple prompt sampled from probabilistic text embeddings to better understand the image. SegPrompt  proposes a category-level prompt to improve the model's class-agnostic segmentation ability.

## 3 Approach

Overview.Our objective is to adopt prompt learning to develop a VLM-based OVSS method without any explicit segmentation-specific networks, thereby reducing training cost. As shown in Figure 3, the **R**elationship **P**rompt **N**etwork (**RPN**) is an end-to-end system comprising text, image and image-text relationship prompt branches. In the text branch, the text encoder inputs text to yield the class embeddings \(^{C d}\), where \(C\) and \(d\) represent the number of classes and dimension, respectively. In the image branch, the plain encoder inputs images to obtain the image embeddings \(^{(N+1) d}\), which includes the patch embeddings \(^{N d}\) and the [CLS] token \(^{1 d}\), with \(N\) representing the number of patches. Concurrently, in the image-text relationship prompt branch, the proposed **R**elationship **P**rompt **M**odule (**RPM**) alongside the encoder takes the class and image embeddings to generate pixel-level relationship prompt, which is subsequently concatenated with the image embeddings to serve as the input for the next image layer. To obtain segmentation results, the last class and image embeddings are fed into the proposed **L**inear **P**rojection **M**odule (**LPM**) to calculate their Matrix product.

Relationship Prompt Module.RPM can guide the plain encoder of VLM to directly produce pixel-level semantic embeddings suitable for OVSS, due to its acquisition of three distinct types of knowledge. Firstly, it acquires multi-scale vision knowledge to locate objectives at distinct scales. Secondly, its image-text relationship knowledge enables the plain encoder to learn open-vocabulary semantics from text features. Thirdly, it introduces dynamic pixel-level knowledge, which is adaptive for the relationship knowledge, enabling pixel-level relationship prompt learning and thus transforming VLM's image-level embeddings into pixel-level ones. Therefore, RPM comprises three blocks, each dedicated to capturing one of the aforementioned knowledge types.

It is crucial for OVSS to obtain multi-scale image embeddings to locate targets of different scales. However, the image embeddings maintain a consistent scale across each image layer. Therefore, we propose multi-scale mixture-of-experts (M2oE) block to aggregate the patch embeddings across distinct scales. As illustrated in Figure 4, M2oE comprises a gating network and several

Figure 4: **M2oE. \(\) and \(\) denote matrix product and addition.**

Figure 3: **Overview of RPN. The end-to-end architecture is delineated into four principal components: 1) the frozen plain encoder, which adopts ViT architecture to encode visual knowledge with relationship prompt; 2) the frozen text encoder, which adopts CLIP text encoder architecture to encode class knowledge with text templates; 3) Relationship Prompt Module (RPM), which generates relationship prompt to guide the plain encoder to output pixel-level semantic embeddings; 4) Linear Projection Module (LPM), which consists of two individual linear layers to output OVSS results.**expert networks as in . The gating network aims to dynamically activate different experts, each responsible for scaling the input to various extents. M2oE of the \(i\)-th layer is formulated as follows:

\[^{i}^{i}+_{j=1}^{n}G(^{i})_{j} _{}(E_{j}(^{i}))\] (1)

where \(G()\) and \(E_{j}()\) represent the gating network and the \(j\)-th expert of all \(n\) experts, respectively. Note that the patch embeddings first reduce the dimension from \(d\) to \(r=3\). And \(()\) maps the output of each expert back to the original dimension \(d\). See Appendix B for more details on M2oE.

It is crucial for OVSS to fuse the image embeddings with open-vocabulary semantics of the class embeddings. The key is to construct the image-text relationship that bridges the class and image embeddings. To achieve this, we propose image-to-pixel semantic attention (ITP) block, which utilizes the image embeddings and the last class embeddings to form the relationship attention map \(^{N}\). As illustrated in Figure 5, we first calculate the **Hadamard product** between the [CLS] token \(\) and the last class embeddings \(t\) after dimension alignment. Then, the **Matrix product** between the Hadamard product result and the patch embeddings \(\) yields the relationship attention map \(\). Consequently, the relationship attention map \(^{i}\) of the \(i\)-th layer is formulated as follows:

\[^{i}=^{i}(t^{i})^{}\] (2)

Intuitively, the first Hadamard product operation assigns weights to images in a batch, with their sum being one (the more important the image, the larger the weight), by fusing the class embeddings used to identify different classes and the [CLS] token used to identify each image in a batch, thus attaining image-level attention. The subsequent Matrix product operation weights the pixels, with the sum of pixel weights normalized to one (the more important the pixel, the larger the weight), by integrating the patch embeddings that contain pixel-level visual information, thus securing pixel-level attention. Therefore, we refer to the training-free operation (i.e., Eq. 2) as the image-to-pixel attention scheme, in which the first and the subsequent products extract image-level and pixel-level information, respectively. As illustrated in Figure 1, the relationship attention map construction process from the shallow to the deep layer demonstrates the effectiveness of the image-to-pixel attention scheme.

The construction of the adaptive image-text relationship for each pixel enables VLM to directly output pixel-level semantic embeddings. To achieve pixel-level dynamic tuning, we propose adaptive prompt generation (APG) block. As illustrated in Figure 5, we first initialize a trainable parameter \(^{N d}\), representing the dynamic pixel-level knowledge. The adaptive relationship prompt \(}^{C d}\) for each pixel is then derived from the Matrix product between the dynamic pixel-level knowledge \(\) and the relationship attention map \(\). Consequently, the adaptive relationship prompt \(}^{i}\) of the \(i\)-th layer is formulated as follows:

\[}^{i}=^{i}{}^{}^{i}\] (3)

The role of the dynamic pixel-level knowledge \(\) is twofold: 1) it projects the relationship attention map from a lower to a higher dimension to serve as an input for the plain encoder, 2) it fine-tunes the relationship prompt for each pixel. It is the fine-tuning of the relationship prompt for each pixel in a high-dimensional space that enables the plain encoder to directly obtain pixel-level semantics.

In addition, we integrate RPM in parallel within each layer of VLM. The image embeddings and the last class embeddings are fed into RPM to generate the relationship prompt, which is then merged with the image embeddings and fed into the next image layer. The prompt output from the image layer is discarded. The processing of each image layer is formulated as follows:

\[[^{i},\_]=^{i}([^{i-1},}^{i- 1}])\] (4)

The notation \([,]\) represents the concatenation operation. See Appendix C for more details on RPM.

Figure 5: **ITP and APG.** Expand, Einsum and Mul denote expanding class dimension, Hadamard product and Matrix product.

Linear Projection Module.Given that RPM enables the plain encoder to directly obtain pixel-level semantics, LPM aims to map the last image and class embeddings into a common space and calculate their Matrix product as the segmentation results. To this end, there are three intuitive designs as illustrated in Figure 6. All three designs share a common structure, consisting of an image branch and a text branch, each equipped with a linear layer and a normalization layer. The image branch processing (the blue solid line) remains consistent, with the last patch embeddings \(p\) sequentially passing through the linear layer and the normalization layer. The difference lies in the text branch processing (the orange solid line). In LPM\({}_{a}\) (as shown in Figure 6 (a)), only the last class embeddings \(t\) are factored into the text branch. In LPM\({}_{b}\) (as shown in Figure 6 (b)), the last class embeddings \(t\) and the last [CLS] token \(g\) first produce the Hadamard product before entering into the linear layer. In LPM\({}_{c}\) (as shown in Figure 6 (c)), the last class embeddings \(t\) is concatenated with the Hadamard product result and then fed into the linear layer. Thus, the segmentation results \(_{a}\), \(_{b}\) and \(_{c}\) of them are defined as follows:

\[_{a} =(p)(t)^{}\] (5) \[_{b} =(p)(t g)^{}\] (6) \[_{c} =(p)([t g,t])^{}\] (7)

For simplicity, the normalization layers are omitted. We select LPM\({}_{c}\) as the proposed LPM experimentally (as shown in Table 6).

Optimization.There are two types of loss functions: the cross-entropy loss with the Softmax function and the combination loss between the focal loss and the dice loss with the Sigmoid function. The former employs one-hot encoding to render the class distribution as mutually exclusive in the embedding space, while the latter utilizes multi-label encoding to permit class distribution overlap. Practically, these losses are typically selected based on the used semantic decoders, such as the former with FPN and the latter with a transformer decoder. Considering that we employ LPM (comprising just two individual linear layers) to obtain the final segmentation results without any well-designed semantic decoders, we evaluate the aforementioned two loss functions. We refer to the former as the Softmax loss and the latter as the Sigmoid loss. We select the Sigmoid loss experimentally (as shown in Table 6).

The Softmax loss is formulated as follows:

\[_{softmax}=-_{k=1}^{h w}y_{k}_{k}\] (8)

where \(y_{k}\) and \(_{k}\) are the ground truth and the prediction vectors, respectively, and \(h\) and \(w\) represent the height and width of the input image. The Sigmoid loss is formulated as follows:

\[_{focal} =-_{k=1}^{h w}(-_{k})^{ } y_{k}(_{k})\] (9) \[_{dice} =1-^{h w}_{k} y_{k}^{}}{ _{k=1}^{h w}_{k}_{k}^{}+_{k=1}^{h w }y_{k} y_{k}^{}}\] (10) \[_{sigmoid} =_{1}_{focal}+_{2}_{dice}\] (11)

where \(\), \(\), \(_{1}\) and \(_{2}\) are hyperparameters. In Eq. 9, when \(y_{k}\) equals the zero vector, \(y_{k}\) and \(_{k}\) are substituted with \(-y_{i}\) and \(-_{i}\), respectively.

Figure 6: **Three kinds of LPMs.**\(\) and \(\) denote element-wise product and matrix product.

Experiments

We evaluate our method in both zero-shot and open-vocabulary settings. See Appendix D for more details on the settings.

### Implementation Details

Datasets and Evaluation Metrics.ADE20K consists of 25k images for training and 2k images for validation. Pascal VOC 2012 includes 10,582 augmented training images and 1,449 validation images. COCO-Stuff164K contains 118,287 training images and 5,000 validation images, with 171 classes in total. Pascal Context consists of 10,100 images, of which 4,996 are used for training and 5,104 for validation, covering 60 classes. We employ pixel-wise classification accuracy (pAcc) and the mean of class-wise intersection over union (mIoU) for seen classes (mIoU\({}_{s}\)), unseen classes (mIoU\({}_{u}\)), and their harmonic mean (hIoU).

Training Strategy.We conduct all experiments on eight NVIDIA GTX 3090 GPUs using the MMSegmentation tool . If not specified, we employ the pre-trained CLIP ViT-B/16 model for both the plain encoder and the text encoder. We set the batch size of 4 for each GPU and set the input resolution to \(512 512\) pixels. The data augmentation strategy adheres to the default settings in MMSegmentation, which includes random image resizing with a short-side range of  and a crop size of \(512 512\). The optimizer is AdamW, initialized with a learning rate of \(2 10^{-5}\) and a weight decay of \(1 10^{-2}\). The learning rate follows a polynomial decay schedule with a power of 0.9. The number of iterations is set to 20K for the VOC dataset, 80K for the COCO dataset, and 40K for the Context dataset. We set \(_{1}\) and \(_{2}\) in Eq. 11 to 100 and 1, respectively.

### System Level Comparison

Efficiency Comparison.We present an efficiency comparison with state-of-the-art methods in Table 1. The results of compared methods are derived from . To ensure a fair comparison, we report our results based on the open-source code from  and evaluate them with an input resolution of \(512 512\) on a single NVIDIA GTX 1080 Ti GPU. Our method outperforms the other methods in efficiency, achieving the lowest number of trainable parameters and the smallest FLOPs.

Comparison in the Zero-Shot Setting.We show the performance comparison with the state-of-the-art methods in the zero-shot setting in Table 2, and conduct the comparison under three scenarios: without self-training, with self-training, and fully supervised. In the absence of self-training, our method surpasses FreeSeg  with +6.0% mIoU\({}_{u}\) on the VOC dataset and +0.6% mIoU\({}_{u}\) on the COCO dataset. With self-training, our method outperforms ZegCLIP  with +3.7% mIoU\({}_{u}\) on the VOC dataset, +1.3% mIoU\({}_{u}\) on the COCO dataset, and +2.3% mIoU\({}_{u}\) on the Context dataset. Under the fully supervised scenario, our method exceeds ZegCLIP  with an average of +2.0% mIoU\({}_{u}\) across the three datasets. We attribute the modest improvement on the COCO dataset to the bias from the extremely unbalanced training and validation set ratios (more training data and less validation data), which contrasts with the performance on the Context dataset (less training data and more validation data), reflecting the robust zero-shot learning ability of our method.

Comparison in the Open-Vocabulary Setting.We show the performance comparison with the state-of-the-art methods in the open-vocabulary setting in Table 3. Our method does not require an additional training dataset. The results indicate that no method can consistently outperform others across all validation datasets; however, our method attains state-of-the-art performance on the A-847, A-150 and the PAS-20 datasets. As analyzed in , the Context dataset and the ADE20K dataset

  
**Methods** & **\#Params(M)** & **FLOPs(G)** & **FPS** \\   \\  ZegFormer  & 60.3 & 1829.3 & 1.7 \\ ZegCLIP  & 13.8 & 110.4 & 9.0 \\
**RPN(ours)** & **3.2** & **84.2** & **10.6** \\   \\  ZegFormer  & 60.3 & 1875.1 & 1.5 \\ ZegCLIP  & 14.6 & 123.9 & 6.7 \\
**RPN(ours)** & **3.2** & **101.3** & **10.2** \\   

Table 1: **Efficiency comparison with state-of-the-art methods. #Params(M)** represents the total number of trainable parameters.

exhibit the highest and lowest label-set similarities with the training dataset, respectively. Therefore, our method showcases a more comprehensive open-vocabulary learning ability.

### Ablation Study

We conduct the ablation experiments on the VOC and the COCO datasets. If not specified, most are conducted on the VOC dataset. See Appendix D for more details on the experiments.

Image-to-Pixel Attention Scheme.RPM aims to transform image-level embeddings from VLM into pixel-level semantic embeddings, enabling direct OVSS. To illuminate its functionality, we employ the Mean Attention Distance (MAD) [56; 57] as a metric, reflecting the granularity of information aggregated within the self-attention head. As illustrated in Figure 7, we analyze MAD of each self-attention head during the initial and advanced stages of training. A higher point indicates a larger receptive field, and greater point spacing signifies richer feature diversity. In the initial training phase, shallow and deep layer information exhibit marked differences: the former concentrates on the local field with fine granularity and high diversity, while the latter focuses on the global field with coarse granularity and limited diversity. During training with RPM, deep layer information maintains attention on both local and global fields without sacrificing granularity or diversity. Clearly, deep layer information, augmented with relationship prompt learning, is more suitable for pixel-level semantic segmentation tasks, thereby diminishing the need for additional segmentation-specific networks. In addition, we visualize the relationship attention map in Figure 1. The first line denote the relationship attention maps for seen classes across each layer; the last two lines for unseen classes. For seen classes, the model has prior pixel-level semantic, so the relationship attention map only needs to focus on a few pixels to guide the model to make predictions for these pixels (e.g., the relationship attention map for airplane has fewer highlighted areas). For unseen classes, the model lacks corresponding semantic, so the relationship attention map needs to focus on more pixels to provide the model with

    &  &  &  \\   & **pAcc** & **mIoU\({}_{}\)** & **mIoU\({}_{}\)** & **mIoU\({}_{}\)** & **pAcc** & **mIoU\({}_{}\)** & **mIoU\({}_{}\)** & **mIoU\({}_{}\)** & **mIoU\({}_{}\)** & **mIoU\({}_{}\)** & **mIoU\({}_{}\)** \\   \\   \\ ZegFormer & - & 86.4 & 63.6 & 73.3 & - & 36.6 & 33.2 & 34.8 & - & - & - & - \\ ZegFormer+MAFT  & - & 91.5 & 80.7 & 85.7 & - & 36.4 & 40.1 & 38.1 & - & - & - \\ ZSSg & 90.0 & 83.5 & 72.5 & 77.5 & 60.3 & 39.3 & 36.3 & 37.8 & - & - & - \\ ZSSg+MAFT  & 87.1 & 76.1 & 81.2 & - & 36.1 & 35.9 & 36.0 & - & - & - & - \\ ZegCLIP  & 94.6 & 91.9 & 77.8 & 84.3 & 62.0 & 40.2 & 41.4 & 40.8 & 76.2 & 46.0 & 54.6 & 49.9 \\ FreeSeg  & - & 91.9 & 78.6 & 84.7 & - & **42.4** & 42.2 & **42.3** & - & - & - \\
**RPN(ours)** & **95.8** & **93.1** & **84.6** & **88.6** & **64.4** & 40.8 & **42.8** & 41.8 & **76.4** & **47.7** & **58.7** & **52.6** \\   \\   \\ ZegCLIP  & 95.1 & 91.8 & 82.2 & 86.7 & 68.8 & 40.6 & 54.8 & 46.6 & 77.2 & 46.6 & 65.4 & 54.4 \\ MaskCLIP  & - & 88.8 & 86.1 & 87.4 & - & 38.1 & 54.7 & 45.0 & - & 44.4 & 66.7 & 53.3 \\ ZegCLIP  & 96.2 & 92.3 & 89.9 & 91.1 & 69.2 & **40.7** & 59.9 & 48.5 & 77.3 & 46.8 & 68.5 & 55.6 \\
**RPN(ours)** & **97.1** & **93.1** & **93.6** & **93.3** & **69.3** & 40.6 & **61.2** & **48.8** & **78.3** & **48.1** & **70.8** & **57.3** \\   \\  \\  \\  \\  \\  \\  \\  \\  
    \\ 
**RPN(ours)** & **97.2** & **94.0** & **94.6** & **94.3** & **70.8** & **41.1** & **64.1** & **50.5** & **78.7** & **48.5** & **80.1** & **60.4** \\   

Table 2: **Performance comparison in the zero-shot setting (unit:%). Here, the best results are shown in bold and the second-best results are underlined.**

  
**Methods** & **VLM** & **Training Set** & **A-847** & **PC-459** & **A-150** & **PC-59** & **PAS-20** \\  OVSeg  & ViT-B/16 & COCO-Stuff+COCO Caption & 7.1 & 11.0 & 24.8 & 53.3 & 92.6 \\ CAT-Seg & ViT-B/16 & COCO-Stuff & 8.4 & 16.6 & 27.2 & **57.5** & 93.7 \\ SAN & ViT-B/16 & COCO-Stuff & 10.1 & 12.6 & 27.5 & 53.8 & 94.0 \\ SED & ConvNeXt-B & COCO-Stuff & 11.4 & **18.6** & **31.6** & 57.3 & 94.4 \\
**RPN(ours)** & ViT-B/16 & COCO-Stuff & 11.4 & 17.3 & 31.5 & 57.1 & **95.2** \\  OVSeg  & ViT-L/14 & COCO-Stuff+COCO Caption & 9.0 & 12.4 & 29.6 & 55.7 & 94.5 \\ CAT-Seg & ViT-L/14 & COCO-Stuff & 10.8 & 20.4 & 31.5 & **62.0** & 96.6 \\ SAN & ViT-L/14 & COCO-Stuff & 13.7 & 17.1 & 33.3 & 60.2 & 95.5 \\ FC-CLIP & ConvNeXt-L & COCO Panoptic & 14.8 & 18.2 & 34.1 & 58.4 & 95.4 \\ SED & ConvNeXt-L & COCO-Stuff & 13.9 & **22.6** & 35.2 & 60.6 & 96.1 \\
**RPN(ours)** & ViT-L/14 & COCO-Stuff & **14.9** & 22.1 & **36.4** & 61.9 & 96.6 \\   

Table 3: **Performance comparison in the open-vocabulary setting (unit:%). Here, the best results are shown in bold and the second-best results are underlined.**more sufficient pixel-level semantic (e.g., the relationship attention map for sheep highlights the complete semantic at shallow layers).

Impact of Different Modules.Table 4 shows the impact of various modules. RPM (the first line) denotes the combination of M2oE, ITP and APG. RPM without M2oE (the second line) denotes the ablation about APG and ITP. Utilizing RPM and LPM yields the best performance. The performance improvement attributed to RPM (+62.6%) is significantly greater than that of LPM (+35.5%). Furthermore, integrating LPM on top of RPM yields a modest performance gain of 1.9%. In contrast, incorporating RPM based on LPM results in a substantial 29% improvement. Therefore, we conclude that VLM with relationship prompt learning is enough for OVSS without any explicit segmentation-specific networks.

Exploration of Different Designs in RPM.Firstly, we explore various designs of the ITP block. As discussed in Section 3, removing the patch embeddings, which contain pixel-level visual information, results in a loss of pixel-level attention for ITP. Conversely, omitting the [CLS] token, responsible for identifying each image, results in a loss of image-level attention. To assess the significance of the image-to-pixel attention scheme, we define the 'without pixel-level attention' and the 'without image-level attention' scenarios by excluding the patch embeddings \(^{i}\) and the [CLS] token \(^{i}\) from Eq. 2, respectively. Note that employing the 'without pixel-level attention' scenario necessitates altering the dimension of

    &  &  \\   &  &  &  & _{s}\)**} & _{u}\)**} &  &  & _{s}\)**} & _{u}\)**} &  \\   & & & 77.1 & 76.3 & 14.3 & 24.1 & 48.3 & 31.8 & 16.4 & 21.6 \\  & ✓ & & 94.7 & 92.1 & 81.2 & 86.3 & 63.3 & 39.2 & 40.3 & 39.7 \\ ✓ & & & 95.1 & 92.4 & 81.7 & 86.7 & 63.7 & 39.5 & 40.3 & 39.9 \\  & & ✓ & 88.8 & 87.3 & 45.3 & 59.6 & 49.3 & 31.3 & 20.3 & 24.6 \\  & ✓ & ✓ & 95.6 & 92.9 & 83.4 & 87.9 & 64.1 & 39.7 & 41.6 & 40.6 \\ ✓ & & & ✓ & 95.8 & 93.1 & 84.6 & 88.6 & 64.4 & 40.8 & 42.8 & 41.8 \\   

Table 4: **Impact of different modules (unit:%). Methods without LPM represent eliminating the linear layers in LPM, i.e., discarding \(()\) in Eq.7.**

    & **Attention** & **pAcc** & **mIoU\({}_{s}\)** & **mIoU\({}_{u}\)** & **hIoU\({}_{u}\)** \\   & w/o image-level & 73.1 & 74.2 & 21.4 & 33.2 \\  & w/o pixel-level & 87.7 & 77.6 & 68.1 & 72.5 \\  & w/ both & 95.8 & 93.1 & 84.6 & 88.6 \\   & **Modes** & **pAcc** & **mIoU\({}_{s}\)** & **mIoU\({}_{u}\)** & **hIoU** \\   & nn.Linear & 95.1 & 92.1 & 80.6 & 86.0 \\  & nn.Parameter & 95.8 & 93.1 & 84.6 & 88.6 \\   & **Dimension** & **pAcc** & **mIoU\({}_{s}\)** & **mIoU\({}_{u}\)** & **hIoU** \\   & \(r=3\) & 95.8 & 93.1 & 84.6 & 88.6 \\   & \(r=6\) & 95.8 & 93.4 & 84.5 & 88.7 \\   & \(r=12\) & 95.9 & 93.8 & 84.3 & 88.8 \\    & **Modes** & **pAcc** & **mIoU\({}_{s}\)** & **mIoU\({}_{u}\)** & **hIoU** \\    & Multi-Scale & 94.9 & 92.8 & 84.1 & 88.2 \\   & M2oE & 95.8 & 93.1 & 84.6 & 88.6 \\   

Table 5: **Ablation study on different designs in RPM (unit:%).**

Figure 7: **Mean Attention Distance of each self-attention head.**

the trainable parameter \(^{i}\) from \(^{N d}\) to \(^{d d}\). The results presented in Table 5 (ITP) corroborate the efficacy of the image-to-pixel attention scheme. Secondly, we explore different configurations of the APG block and evaluate two distinct trainable modes: nn.Linear and nn.Parameter. The outcomes in Table 5 (APG) advocate for the implementation of the nn.Parameter mode. Thirdly, we explore the dimension \(r\) of M2oE and different multi-scale aggregation modes in Table 5 (M2oE). The results show that performance enhancements are marginal with increasing dimensions. Given the trade-off between performance gain and parameter increase, we adopt \(r=3\). The 'Multi-Scale' mode refers to removing the gating network and directly aggregating the features processed by all expert networks (See Appendix B for more details).

Exploration of Different Designs in **LPM.** We evaluate three different LPM designs (shown in Figure 6) using Softmax and Sigmoid losses. The results in Table 6 reveal that LPM\({}_{c}\) with Sigmoid loss is the most effective strategy. In addition, using Sigmoid loss is significantly better than using Softmax loss. This reflects that the relationship prompt does not directly focus on pixels, but follows the image-to-pixel process.

Impact of Training-Free Projection Modules. Note that our method comprises two trainable modules: RPM and LPM. To further explore the performance of RPM, we eliminate all linear layers in LPM to construct three variants of training-free projection networks. Following the sequence depicted in Figure 6, we denote the training-free projection modules as TFPM\({}_{a}\), TFPM\({}_{b}\) and TFPM\({}_{c}\). The results in Table 7 demonstrate that both TFPM\({}_{b}\) and TFPM\({}_{c}\) can achieve the state-of-the-art performance. The suboptimal results with TFPM\({}_{a}\) suggests that the relationship prompt guides the plain encoder to perform pixel-level classifications by encoding semantics in the [CLS] token, which should not be disregarded when obtaining segmentation results.

## 5 Conclusion

In this work, we propose the **R**elationship **P**rompt **M**odule (**RPM**) to guide VLM to transform its image-level embeddings into pixel-level semantic ones. RPM and VLM combine to form **R**elationship **P**rompt **N**etwork (**RPN**), a VLM-based OVSS method that directly performs OVSS without any explicit segmentation-specific networks. To the best of our knowledge, we are the first to give a straightforward solution for OVSS that applies prompt learning solely to VLM. We evaluate our method on four public benchmark datasets in both zero-shot and open-vocabulary settings, and achieve the state-of-the-art performance with only about **3M** trainable parameters (2% of total parameters). Therefore, it is concluded that VLM with relationship prompt learning is enough for open-vocabulary semantic segmentation without any explicit segmentation-specific networks.

Limitations.Although we meticulously design an effective prompt learning method for directly using VLM to achieve pixel-level OVSS, there are several ways for prompt learning to achieve further improvement: 1) directly acting on the attention map (more direct); 2) dynamically reorganizing multi-head attention map (more lightweight).