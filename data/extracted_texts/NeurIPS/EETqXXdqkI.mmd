# PICProp: Physics-Informed Confidence Propagation

for Uncertainty Quantification

 Qianli Shen

National University of Singapore

Singapore

shenqianli@u.nus.edu

&Wai Hoh Tang

National University of Singapore

Singapore

waihoh.tang@nus.edu.sg

&Zhun Deng

Columbia University

USA

zhun.d@columbia.edu

&Apostolos Snaros

Brown University

USA

a.psaros@brown.edu

&Kenji Kawaguchi

National University of Singapore

Singapore

kenji@nus.edu.sg

###### Abstract

Standard approaches for uncertainty quantification in deep learning and physics-informed learning have persistent limitations. Indicatively, strong assumptions regarding the data likelihood are required, the performance highly depends on the selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost. This paper introduces and studies confidence interval (CI) estimation for deterministic partial differential equations as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We propose a method, termed Physics-Informed Confidence Propagation (PICProp), based on bi-level optimization to compute a valid CI without making heavy assumptions. We provide a theorem regarding the validity of our method, and computational experiments, where the focus is on physics-informed learning. Code is available at https://github.com/ShenQianli/PICProp.

## 1 Introduction

Combining data and physics rules using deep learning for solving differential equations  and learning operator mappings  has recently attracted increased research interest. In this context, physics-informed machine learning is transforming the computational science field in an unprecedented manner, by solving ill-posed problems, involving noisy and multi-fidelity data as well as missing functional terms, which could not be tackled before . Nevertheless, uncertainty - mainly due to data noise and neural network over-parametrization - must be considered for using such models safely in critical applications . Despite extensive work on uncertainty quantification (UQ) for deep learning , standard UQ has persistent limitations. Indicatively, although for most practical applications where only confidence intervals (CIs) are required, the harder problem of UQ is often solved first using simplifying assumptions, and the CIs are subsequently computed. Further, strong assumptions regarding the data likelihood are required, the performance highly depends onthe selection of priors, and the posterior can be sampled only approximately, which leads to poor approximations because of the associated computational cost.

In this paper, we introduce and study CI estimation for addressing deterministic problems involving partial differential equations (PDEs) as a novel problem. That is, to propagate confidence, in the form of CIs, from data locations to the entire domain with probabilistic guarantees. We name our framework Physics-Informed Confidence Propagation (PICProp) and propose a practical method based on bi-level optimization to identify valid CIs in physics-informed deep learning. Our method can be applied in diverse practical scenarios, such as propagating data CIs, either estimated or given by experts, to the entire problem domain without distributional assumptions. Indicative applications are provided in Appendix A.

PICProp provides distinct advantages over traditional UQ approaches in the context of physics-informed deep learning. Specifically, PICProp does not rely on strong assumptions regarding the data likelihood and constructs theoretically valid CIs. Further, PICProp offers a unique capability to construct joint CIs for continuous spatio-temporal domains. In contrast, UQ methods typically construct marginal intervals for discrete locations. Furthermore, due to the conservative propagation of uncertainty, PICProp produces conservative CIs, which is preferred in critical and risk-sensitive applications [7; 13], considering the significant harm of non-valid confidence intervals. This sets it apart from commonly used UQ methods like deep ensembles  and Monte-Carlo dropout [28; 10], which are known to be overconfident in practice [9; 7].

The paper is organized as follows: In Section 2 (and its Appendices B & C), we introduce the PDE problem setup and discuss related work. In Section 3 (and its Appendices D-F), we introduce the problem of CI estimation for PDEs and extensively describe joint solution predictions. Next, we formulate this novel problem as a bi-level optimization problem in Section 4 and Appendices G & H, and adopt various algorithms for computing the required implicit gradients in Appendix I. Finally, we evaluate our method on various forward deterministic PDE problems in Section 5 and Appendices J-L, and discuss our findings in Section 6.

## 2 Background

### Problem Setup

Consider the following general PDE describing a physical or biological system:

\[[u(x),(x)]=0,x,[u(x),(x)]= b(x),x,\] (1)

where \(x\) is the \(D_{x}\)-dimensional space-time coordinate, \(\) is a bounded domain with boundary \(\), and \(\) as well as \(\) are general differential operators acting on \(\) and \(\), respectively. The PDE coefficients are denoted by \((x)\), the boundary term by \(b(x)\), and the solution by \(u(x)\).

In this paper, we focus on the data-driven forward deterministic PDE problem, where (i) operators \(,\) and coefficients \(\) are deterministic and known; (ii) datasets \(_{f}=\{x_{f}^{j}\}_{i=1}^{N_{f}}\) for the force condition and \(_{b}=\{(x_{b}^{j},b^{j})\}_{j=1}^{N_{b}}\) for the boundary condition are given; (iii) uncertainty only occurs in the boundary term due to data noise, i.e., \(b^{j}=b(x_{b}^{j})+^{j}\) for \(j=1,...,N_{b}\), and the rest of the dataset is considered to be deterministic without loss of generality. We denote the function obtained by utilizing the dataset \(z=\{_{f},_{b}\}\) for approximating the solution \(u(x)\) of Equation (1) with a data-driven solver \(\) as \(u_{}(x;z)\).

Clearly, different datasets \(z\) contaminated by random noise lead to different solutions \(u(x;z)\) and thus, to a distribution of solutions at each \(x\). It follows that the solution \(u(x;Z)\) can be construed as stochastic, where \(Z\) is used to denote the dataset as a random variable and \(z\) to denote an instance of \(Z\). In this regard, this paper can be viewed as an effort towards a universal framework, with minimal assumptions, for constructing CIs for solutions of nonlinear PDEs.

### Related Work

#### Physics-Informed Neural Networks

The physics-informed neural network (PINN) method, developed by , addresses the forward deterministic PDE problem of Equation (1) by constructing a neural network (NN) approximator \(u_{}(x)\), parameterized by \(\), for modeling the approximate solution \(u(x;z)\). The approximator \(u_{}(x)\) is substituted into Equation (1) via automatic differentiation for producing \(f_{}=[u_{},]\) and \(b_{}=[u_{},]\). Finally, the optimization problem for obtaining \(\) in the forward problem scenario given a dataset \(z\) is cast as

\[=*{argmin}_{}_{pinn}(,z),  L_{pinn}:=}{N_{f}}_{i=1}^{N_{f}}||f_{ }(x_{f}^{i})||_{2}^{2}+}{N_{b}}_{i=1}^{N_{b}}||b_{} (x_{b}^{j})-b^{j}||_{2}^{2},\] (2)

and \(\{w_{f},w_{b}\}\) are objective function weights for balancing the various terms in Equation (2); see . Following the determination of \(\), \(u_{}\) can be evaluated at any \(x\) in \(\). PINN can be called up for high-dimensional PDEs .

**Uncertainty Quantification for Physics-Informed Learning** Although UQ for physics-informed learning is not extensively studied, the field is gaining research attention in recent years. Indicatively,  employed dropout to quantify the uncertainty of NNs in approximating the modal functions of stochastic differential equations;  presented a model based on generative adversarial networks (GANs) for quantifying and propagating uncertainty in systems governed by PDEs;  developed a new class of physics-informed GANs to solve forward, inverse, and mixed stochastic problems;  further studied Wasserstein GANs for UQ in solutions of PDEs;  proposed Bayesian PINNs for solving forward and inverse problems described by PDEs with noisy data. Although this field is always evolving, we refer to it in this paper as "standard" UQ in order to distinguish it from our proposed method in Section 4. In Appendices B-C, we summarize the limitations of standard UQ, and we direct interested readers to the review paper  for more information.

**Interval Predictor Model (IPM)** The Interval Predictor Model (IPM) in regression analysis has a similar motivation as our approach. A function of interest is approximated by an IPM and the bounds on the function are obtained.  proposed a technique using a double-loop Monte Carlo algorithm to propagate a CI using the IPM. The technique requires sampling epistemic and aleatory parameters from uniform distributions to estimate the upper and lower bounds of the IPM. Our approach is different for two reasons. First, we fit a PINN model directly and thus, sampling epistemic parameters may not be feasible. Second, we do not require a sampling procedure. Instead, we cast the CI estimation as a bi-level optimization problem and solve it using hyper-gradient methods.

## 3 Confidence Intervals for Solutions of Nonlinear Partial Differential Equations

In this section, we provide the definition of CI for the exact PDE solution and introduce our main assumption.

**Definition 3.1** (Joint CI for PDE Solution).: A pair of functions \((L_{Z},U_{Z})^{}^{}\) is a \(p\)-CI for the exact PDE solution \(u(x)\) if the random event \(:=\{\{L_{Z}(x) u(x) U_{Z}(x),\; x\}\) holds with probability at least \(p\).

Note that this interval refers to the exact solution \(u(x)\). Therefore, a novel problem is posed in Definition 3.1, pertaining to propagating randomness, in the form of CIs, from the data locations, typically on the boundary, to the rest of the domain. The subscript \(Z\) in \((L_{Z},U_{Z})\) signifies that the CI is random as it depends on the random dataset \(Z\). The available data can be an instance of \(Z\), denoted as \(z\), or a collection of instances; see Appendix D for the considered problem scenarios. Such instances can, for example, be drawn from repeating independent experiments. The subscript \(Z\) in \((L_{Z},U_{Z})\) is dropped if there is no ambiguity.

We also note that the joint CI (also referred to as confidence sequence [17; 32]) in Definition 3.1 corresponds to joint predictions \(u()\) over the whole domain \(\), rather than to marginal predictions \(u(x)\) for a given \(x\). The importance of joint predictions as an essential component in UQ has been emphasized in recent works for active learning , sequential decision making , and supervised learning . Nevertheless, the present paper is the first work to consider joint predictions, and general CI estimation, in the context of physics-informed learning. In Appendix E, we provide several theoretical remarks delineating the differences between marginal and joint predictions, as well as unions of bounds, which are typically used in standard UQ. In Appendix F, we highlight these differences with a toy example.

Next, to relate the PINN solution with the exact PDE solution \(u(x)\) of Definition 3.1, we make the following assumption regarding the noise at the boundary.

**Assumption 3.2** (Zero-mean Noise).: The noise on the boundary is zero-mean, i.e., \([^{j}]=0, j\{1,,N_{b}\}\).

Next, a clean dataset (without noise), denoted as \(\), can be considered as the expectation of \(Z\), i.e., \(=[Z]\). In this regard, consider a data-driven PDE solver \(\), which yields an approximate solution \(u_{}(x;z)\) given a dataset \(z\). We define the properness of \(\) based on the concept of a clean dataset.

**Definition 3.3** (Proper PDE Solver).: A PDE solver \(\) is \(\)-proper with respect to a clean dataset \(\), if \(_{x}|u_{}(x;)-u(x)|\).

Henceforth, we only consider proper PDE solvers. That is, a powerful solver is available such that the absolute error of the solution obtained by the solver with a clean dataset is bounded. In all cases, we assume that such an informative clean dataset exists, but is not available. Further, the constant \(\), which is only related to \(\) and \(\), is considered known and fixed in the computational experiments. Note that no assumptions are made about \(u(x;z)\) for \(z\), i.e., there are no assumptions about the PINN solver robustness against noisy data. Finally, in all NN training sessions we consider no NN parameter uncertainty, although it is straightforward to account for it by combining our method with Deep Ensembles , for instance. The aim of this paper is to propose a principled and theoretically guaranteed solution approach and thus, we refrain from applying such ad hoc solutions in the computational experiments and plan to consider parameter uncertainty rigorously in a future work. Nevertheless, we provide in Appendix L a comparison with Bayesian PINNs for completeness.

## 4 Method

In this section, we introduce our PICProp method and provide the accompanying theorem, as well as practical implementation algorithms. Further, we discuss the computational cost of our method and propose a modification for increasing efficiency. Specifically, based on Definition 3.1, we address the problem of propagating the uncertainty from the boundary to arbitrary locations \(x\) of the domain. Equivalently, this relates to obtaining the corresponding \(p\)-CIs, i.e., obtaining the functions \((L,U)^{}^{}\) in Definition 3.1.

### Physics-Informed Confidence Propagation (PICProp)

Our method comprises two steps: (i) a \(p\)-CI \(}_{p}\) for the clean data \(\) is constructed. (ii) \(}_{p}\) is propagated to any point \(x\) of the domain by solving the following problem,

\[L(x)=_{z}_{p}}u_{}(x;z)-, U(x)= _{z}_{p}}u_{}(x;z)+,\] (3)

which searches for configurations of points \(z\) within the interval \(}_{p}\) such that the predicted values for \(u(x)\) given by the \(\)-proper solver \(\) are minimized or maximized, respectively.

The CI in the first step can either be given or constructed from data. The former case relates to various practical scenarios (e.g., the error ranges of measurement sensors with a certain confidence degree are known or given by experts), whereas the latter serves as a data-driven scheme, where less prior knowledge is required. Techniques to construct the CI in the first step depend on the data (noise) distribution. For example, if the noise is assumed to be Gaussian, the corresponding CI can be identified by chi-squared or Hotelling's \(T\)-squared statistics. If the noise distribution is not known, a concentration inequality can be used to identify the CI. Relevant details are provided in Appendix D. See also Figure 1 for an illustration.

The interval constructed via Equation (3) is a joint \(p\)-CI for the whole domain \(\) of \(u\). This fact is formalized in the next theorem (see Appendix G for the corresponding proof).

**Theorem 4.1**.: _Consider a \(p\)-CI for \(\) denoted as \(}_{p}\), constructed such that \((}_{p}) p\). Then the functions \((L,U)\) obtained by solving Equation (3) define a \(p\)-CI for \(u\)._

Although the robustness of the PDE solver does not affect the validity of Theorem 4.1, it nevertheless affects the quality of intervals constructed via Equation (3). Specifically, although valid CIs can always be constructed with arbitrary proper PDE solvers by solving Equation (3), a robust PDE solver less sensitive to data noise is expected to yield tighter CIs compared to a less robust solver.

Note that the confidence guarantee of the constructed CIs corresponds to a probability that is greater than, rather than equal to, \(p\), which means that the constructed CIs are conservative. From a design perspective, conservative uncertainty estimation is the preferred route for critical and risk-sensitive applications [7; 13], although it can be more costly.

Next, a brute force and computationally expensive approach to solve Equation (3) pertains to traversing \(z}_{p}\) in an exhaustive search (ES) manner and invoking a PDE solver for each candidate \(z\). Alternatively, we propose to first employ a NN for obtaining \(u_{(z)}\) by minimizing the physics-informed loss of Equation (2) and approximating \(u(x;z)\) for each \(z}_{p}\). In this regard, if a PINN is assumed to be an \(\)-proper PDE solver, Equation (3) can be further extended to a bi-level optimization problem cast as

\[ L(x)=_{z}_{p}}u_{(z)}(x)-, U(x)=_{z}_{p}}u_{(z)}(x)+,\\ s.t.(z)=_{}_{pinn}( ,z).\] (4)

Nevertheless, explicit gradient \(_{z}u\) calculation is intractable, as multiple steps of gradient descent for the NN are required for a reliable approximation. Despite this fact, bi-level optimization involving large-scale parameterized models such as NNs, with main applications in meta-learning, has been extensively studied in recent years and has resulted to numerous implicit gradient approximation methods (see Appendix I). In this regard, Algorithm 1 in Appendix H summarizes our method for solving efficiently Equation (3) by using such approximations. In passing, note that \(=0\) is considered for all implementations in this work.

### Efficient Physics-Informed Confidence Propagation (EffiPICProp)

If joint CIs are required for multiple query points, denoted by the set \(_{q}\), Algorithm 1 has to be used multiple times to solve Equation (4) for each and every point \(x\) in the set. This clearly incurs a high computational cost. To address this issue, a straightforward approach pertains to training an auxiliary model parameterized by \(\) that models the CI as \((L_{}(x_{q}),U_{}(x_{q}))\) for all query points \(x_{q}_{q}\), or more generally of the entire domain \(\). However, this naive regression using solutions of the bi-level optimization at a set of locations, which we refer to as SimPICProp, does not provide accurate results when the size of the query set is small; see computational results in Section 5.1.

For improved generalization properties with limited query points, we propose to train a two-input parameterized meta-model \(u_{}(x_{q},x)\) that models the resulting PDE solution \(u(x)\), via Equation (4), for each different query point. For example, note that \(L(x)\) in Equation (4) evaluates the NN \(u_{(z)}\) at the location of the query point and \(z\) minimizes the corresponding value. As a result, if a trained model \(u_{}(x_{q},x)\) minimizes the value of \(u\) at \(x_{q}\), the sought \(p\)-CI can be simply obtained by evaluating \(u_{}\) at \((x_{q},x_{q})\). More generally, given a query set consisting of \(K\) randomly selected query points \(_{q}=\{x_{q}^{k}\}_{k=1}^{K}\), the model is trained such that it solves the problem

\[&_{}_{k=1}^{K}_{ pinn}(u_{}^{L}(x_{q}^{k},),z^{L}(x_{q}^{k}))+_{pinn}(u_{ }^{U}(x_{q}^{k},),z^{U}(x_{q}^{k})),\\ & z^{L}(x_{q}^{k})=*{arg\,min}_{z }_{p}}u_{(z)}(x_{q}^{k}),z^{U}(x_{q}^{k})= *{arg\,max}_{z}_{p}}u_{(z)}(x_ {q}^{k}),\\ & s.t.(z)=*{arg \,min}_{}_{pinn}(u_{}(),z).\] (5)

The estimated \(p\)-CI is, subsequently, given by

\[L(x)=u_{}^{L}(x,x), U(x)=u_{}^{U}(x,x).\] (6)

In fact, we can train a single model for both upper- and lower-bound predictions because the parameters are shared as in Equation (5). This is achieved by encoding an indicator into the input vector, so that \(u_{}^{L}(x_{q},)=u_{}(x_{q},,-1)\) and \(u_{}^{U}(x_{q},)=u_{}(x_{q},,1)\). Furthermore, to balance the weights of the terms in the objective of Equation (5) more effectively, PINNs are separately trained with boundary conditions \(z^{L}(x_{q}^{1:K})\) and \(z^{U}(x_{q}^{1:K})\). In this regard, \(u_{}\) is obtained by \[_{} \ \ _{}\ (x_{q}^{1:K};),\ \ \ _{}:=_{k=1}^{K}(1-)(x_{q}^{k},x_{q}^{k};)+\ }_{x}(x_{q}^{k},x;),\] (7) \[s.t. \ \ \ (x_{q}^{k},x;):=(u_{}(x_{q}^{k},x,-1)-u_{ (z^{L}(x_{q}^{k}))}(x))^{2}+(u_{}(x_{q}^{k},x,1)-u_{(z^{U}(x_{q}^{k}))}(x))^{2},\]

and \(\) is a weight to balance the two loss terms, of which the first aims to enforce close predictions of CIs at query points, and the second to utilize extra information in PDE solutions for better generalization. We refer to this model as EffiPICProp and provide the corresponding implementation in Algorithm 2 in Appendix H. Specifically, the EffiPICProp version with \(=0\) is called SimPICProp, as it simply focuses on close predictions of confidence intervals at query points, while disregarding additional information at other locations.

## 5 Computational experiments

We provide three computational experiments that are relevant to physics-informed machine learning. We will start with a 1D pedagogical example to illustrate our proposed approaches. Next, we will move to more complicated problems involving standard representative equations met in many real-world problems including heat transfer, fluid dynamics, flows through porous media, and even epidemiology [21; 38]. Details regarding implementation, query points, and computational times can be found in Appendix J.

### A Pedagogical Example

We consider the following equation, which has been adopted from the pedagogical example of ,

\[u_{xx}-u^{2}u_{x}=f(x), x[-1,1],(),\] (8)

where \(f(x)=-^{2}( x)-( x)^{2}( x).\) For the standard BCs \(u(-1)=u(1)=0\), the exact solution is \(u(x)=( x).\) The purpose of this one-dimensional PDE with only two boundary points, i.e., with a two-dimensional search space in Equation (4), is to demonstrate the applicability of our method in different data scenarios (see Appendix D). Further, the small size of the problem is ideal for comparing computationally expensive methods, such as exhaustive search (ES) and employing PICProp separately at all domain locations, with the more efficient implementation of Algorithm 2.

We consider two types of noisy BCs (Figure 1). First, we consider BCs with Gaussian noise given as

\[u(-1)(0,_{1}^{2}), u(1)(0,_{2} ^{2}),\] (9)

Figure 1: **Pedagogical example: Boundary \(95\%\) CIs to be propagated to the rest of the domain. (a-b) Random samples follow \((,^{2})\), where \(=0.05\). The black crosses correspond to 5,000 random samples from the distribution. A \(^{2}\) CI is constructed using the only one available sample and a \(T^{2}\) CI is constructed using the five available samples. (c) Random samples follow \(([-1.5,1.5][-1.5,1.5])\). The black crosses correspond to 2,000 random samples from the distribution. A Hoeffding CI is constructed using the five available samples. All available samples are shown with red crosses.**

where the standard deviations are \(_{1}=_{2}=0.05\), i.e., the data is assumed to be Gaussian and the standard deviations are known (or estimated if unknown). We sample one or five datapoint(s) for each side of the boundary, for each case, respectively. Given the dataset, a \(^{2}\) CI and a \(T^{2}\) CI are constructed according to the corresponding problem scenarios discussed in Appendix D; see also Figure 1(a),(b).

Second, we consider BCs with uniform noise given as

\[u(-1)(-D_{1},D_{1}), u(1)(-D_{2},D_{2}),\] (10)

where \(D_{1}=D_{2}=1.5\) are assumed to be known. We sample five datapoints for each boundary side. Given the dataset, a Hoeffding CI is constructed; see also Figure 1(c).

For the query set, six points are used for implementing SimPICProp and EffiPICProp in Section 4.2, whereas 41 points are used for implementing the brute-force version of PICProp, which amounts to solving the bi-level optimization problem of Equation (4) for each point. The implementation details of the PICProp-based methods are summarized in Table J.1. Further, 5,000 BCs are sampled for solving Equation (4) with ES, which can be considered as a reference solution.

The CI predictions for the three cases are visualized in Figure 2, whereas the meta-learning curves can be found in Appendix K.1. Note that the sizes of the CIs are approximately equal to \(0.1-0.2\) near the boundaries, which are comparable with the propagated CI sizes of Figure 1, and become larger around \(x\) of \( 0.5\). In all cases, the PICProp results match those of ES, despite the fact that PICProp requires solving a challenging bi-level optimization method with approximate implicit gradient methods. Further, the efficient method EffiPICProp performs accurately as compared to its reference method PICProp, while retaining a significantly lower computational cost (see Figure 3 for \(^{2}\) results and Figure K.4 K.5 for \(T^{2}\) and Hoeffding results). On the other hand, the predictions of SimPICProp match the brute-force PICProp results only on the query points, but generalize poorly on other locations. In contrast, EffiPICProp, which is based on a meta-model that fits the PDE solutions for multiple query points, generalizes well on unseen locations.

Some additional experimental results are summarized in Appendix K.1 for the completeness of our empirical study: Firstly, a comparison among PICProp and EffiPICProp with various \(\)s is provided in Figure K.3 to better demonstrate how the balance between two EffiPICProp loss terms in Equation (7) affects the CI prediction. Next, we conduct an additional experiment to empirically verify the validity of EffiPICProp CIs in Appendix K.1.4. Finally, a brief comparison with Bayesian PINNs is presented in Appendix L.

### Two-dimensional Poisson equation

Next, we consider a two-dimensional Poisson equation given as

\[ u_{xx}+u_{yy}=f(x,y),(x,y)[-1,1][-1, 1],(),\\ u(x,y)=e^{x}+e^{y}, x= 1y= 1,( ).\] (11)

Figure 2: **Pedagogical example:** CI predictions for the entire domain \(x[-1,1]\). Left: Noise is Gaussian with known standard deviation, and a \(^{2}\) CI is constructed at the boundary. Middle: Noise is Gaussian with unknown standard deviation, and a \(T^{2}\) CI is constructed at the boundary. Right: Uniform noise with known bounds, and a Hoeffding CI is constructed at the boundary.

The exact solution is \(u(x,y)=e^{x}+e^{y}\), and the corresponding \(f(x,y)\) is obtained by substituting \(u(x,y)\) into Equation (11). This example serves to demonstrate the effectiveness of our method for treating cases with large numbers of noisy BC datapoints. In such cases, an exhaustive search approach for solving the bi-level optimization of Algorithm 1 generally fails to identify the global optimum, unless we increase the number of trials and the computational cost at prohibitive levels.

We propagate the following fixed CI from the boundary to the rest of the domain \((x,y)(-1,1)(-1,1)\):

\[u(x,y)[e^{x}+e^{y}-0.05,e^{x}+e^{y}+0.05], x= 1y= 1.\] (12)

To solve the problem, 10 boundary points are sampled for each side of the domain, i.e., 40 boundary points in total are used for training and uncertainty is considered at all points. The search space is thus 40-dimensional. The implementation details of the PICProp-based methods are summarized in Table J.1, whereas 4,000 steps of standard PINN training are run for each BC for ES. For a fair comparison, 121 query points for PICProp are selected in the domain, see Figure J.1, and the computational time is approximately the same as of ES with 1,000 trials. Moreover, we include the ES result with 5,000 trials, which is considered to be exhaustive, as a reference.

Figure 4 plots the slices of CI predictions for \(x\{-0.5,0.0,0.5,1.0\}\), whereas K.2 includes slices along the \(y\) axis and the heatmaps over the entire domain. For reference, the size

Figure 4: **2-D Poisson equation**: Slices of CI predictions for \(x\{-0.5,0.0,0.5,1.0\}\). Effi PICProp can identify the solution to the bi-level optimization problem more efficiently and accurately, as compared with the computationally expensive, search-based method ES.

Figure 3: **Pedagogical example**, \(^{2}\): Although the results of both SimPICProp and Effi PICProp match PICProp results on the query points, Effi PICProp generalizes better on unseen locations.

of the CIs obtained with EffiPICProp is approximately 0.1 across the domain, which is equal to the size of the propagated CI of Equation (12). The predicted CI size is also larger in most cases than that of the ES-based CIs. We conclude that 1,000 trials for ES are clearly not sufficient, as the intervals of ES with 5,000 trials are significantly wider; thus, a global optimum is not reached with 1,000 trials. In contrast, EffiPICProp converges to similar or wider intervals compared to ES with 5,000 trials. Overall, the optimization-based method EffiPICProp can identify the solution to the bi-level optimization problem more efficiently and accurately, as compared with the computationally expensive, search-based method ES.

### One-dimensional, time-dependent Burgers equation

In this example, we study the Burgers equation given as

\[ u_{t}+u_{x}- u_{xx}=0,(x,t)[-1,1] ,(),\\ u(x,t=0)=-( x), u(x=-1,t)=u(x=1,t)=0,(), \] (13)

where \(=\) is the viscosity parameter. We consider clean data for the BCs and noisy data for the initial conditions (ICs). Specifically, for the ICs, we propagate the following fixed CI from \(t=0\) to the rest of the time domain \(t(0,1]\) and for all \(x[-1,1]\):

\[u(x,t=0)[-( x)-0.2,-( x)+0.2].\] (14)

This problem serves to demonstrate the reliability of our method even in challenging cases with steep gradients or discontinuities of the PDE solution. For solving the problem using PINNs, we consider 200 boundary points, 256 initial points, and 10,000 samples in the interior domain. The 256 initial points are evenly spaced along \([-1,1]\). The implementation details of the PICProp-based methods are summarized in Table 1, whereas 2,500 and 5,000 trials are considered for ES. The 246 PICProp query points are visualized in Figure 1 and the respective computational time is approximately equal to that of ES with 2,500 trials.

Slices of CI predictions for \(t\{0.0,0.25,0.5,0.75\}\) are plotted in Figure 5, whereas Appendix K.3 includes the respective meta-learning curves. We conclude that EffiPICProp, although computationally cheaper, in most cases provides wider, i.e., more conservative, CIs, as compared to ES with varying numbers of trials. It also succeeds in capturing the expected increase in uncertainty as the query points move away from the source of uncertainty, which corresponds to larger \(t\) values. Specifically, the size of the CIs at \(t=0\) is approximately 0.4, which is equal to the size of the propagated CI in Equation (14), and becomes approximately 2 at \(t=0.75\) and near \(x=0\). On the other hand, since we consider 256 random initial points, the search space of ES is 256-dimensional. It requires a prohibitively large number of trials to identify the global optima of Equation (4). Finally, note that the results of EffiPICProp in Figure 5 correspond to \(=0\) in Equation (7), because we found that the additional information provided by the second loss term in Section 4.2 adversely affects generalization. This is partly due to the large number of query points in this problem.

## 6 Conclusion

Physics-informed machine learning is transforming the computational science field in an unprecedented manner, by solving ill-posed problems, involving noisy and multi-fidelity data as well as missing functional terms, which could not be tackled before. However, uncertainty quantification is necessary for deploying such methods in critical applications. In this work, we considered a novel problem pertaining to propagating randomness, in the form of CIs, from data locations (at the boundary but can easily be extended to inner or collocation data points) to the entire domain, with probabilistic guarantees. We proposed PICProp, a method based on bi-level optimization, and its efficient version, with a theorem guaranteeing the validity of the obtained CIs (Algorithms 1-2 and Theorem 4.1). To the best of our knowledge, this is the first work on joint CI estimation with theoretical guarantees in the context of physics-informed learning, while our method can be applied in diverse practical scenarios without imposing any distributional assumptions.

In the computational experiments of Section 5, involving various partial differential equations, we demonstrated the effectiveness and efficiency of our proposed methods. Specifically, the predicted CIs are wider, i.e. more conservative, in most cases, as compared with the more computationally demanding exhaustive search for solving the bi-level optimization. Although not encountered in the experiments, a potential limitation of our method is that the obtained CIs can be loose in some cases, even when propagating a tight CI from the boundaries, due to the max/min operations in the bi-level optimization of Equation (3). Because of the theoretical challenges involved, we plan to study this limitation and perform pertinent experiments in a future work.

## 7 Acknowledgements

This material is based upon work supported by the Google Cloud Research Credit program with the award (6NW8-CF7K-3AG4-1WH1). The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).