# Factorized Contrastive Learning:

Going Beyond Multi-view Redundancy

 Paul Pu Liang\({}^{1}\)1, Zihao Deng\({}^{2}\)2, Martin Q. Ma\({}^{1}\)1

**James Zou\({}^{3}\), Louis-Philippe Morency\({}^{1}\), Ruslan Salakhutdinov\({}^{1}\)**

\({}^{1}\)Carnegie Mellon University, \({}^{2}\)University of Pennsylvania, \({}^{3}\)Stanford University

pliang@cs.cmu.edu, zihaoden@cs.cmu.edu, qianlim@cs.cmu.edu

###### Abstract

In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of _multi-view redundancy_ - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks.

## 1 Introduction

Learning representations from different modalities is a central paradigm in machine learning . Today, a popular learning method is to first pre-train general representations on unlabeled multimodal data before fine-tuning on task-specific labels . These current multimodal pre-training approaches have largely been inherited from prior work in multi-view learning  that exploit a critical assumption of _multi-view redundancy_: the property that shared information between modalities is almost exactly what is relevant for downstream tasks . When this assumption holds, approaches based on contrastive pre-training to capture shared information , followed by fine-tuning to keep task-relevant shared information , have seen successful applications in learning from images and captions , video and audio , speech and transcribed text , and instructions and actions . However, our paper studies two fundamental limitations in the application of contrastive learning (CL) to broader real-world multimodal settings (see Figure 1 for a visual depiction and experimental results showing the performance drop of CL):

1. **Low _shared_ information** relevant to tasks: There exists a wide range of multimodal tasks involving small amounts of shared information, such as between cartoon images and figurative captions (i.e., not literal but metaphoric or idiomatic descriptions of the images ). In these situations, standard multimodal CL will only receive a small percentage of information from the learned representations and struggle to learn the desired task-relevant information.
2. **High _unique_ information relevant to tasks: Many real-world modalities can provide unique information not present in other modalities. Examples include healthcare with medical sensors or robotics with force sensors . Standard CL will discard task-relevant unique information, leading to poor downstream performance.

In light of these limitations, how can we design suitable multimodal learning objectives that work beyond multi-view redundancy? In this paper, starting from the first principles in information theory, we provide formal definitions of shared and unique information via conditional mutual information and propose an approach, Factorized Contrastive Learning (FactorCL for short), to learn these multimodal representations beyond multi-view redundancy using three key ideas. The first idea is to explicitly _factorize_ shared and unique representations. The second idea is to _capture task-relevant_ information via maximizing lower bounds on MI and _remove task-irrelevant_ information via minimizing upper bounds on MI, resulting in representations with sufficient and necessary information content. Finally, a notion of task relevance without explicit labels in the self-supervised setting is achieved by leveraging _multimodal augmentations_. Experimentally, we evaluate the effectiveness of FactorCL on a suite of synthetic datasets and large-scale real-world multimodal benchmarks involving images and figurative language , prediction of human sentiment , emotions , humor , and sarcasm , as well as patient disease and mortality prediction from health indicators and sensor readings , achieving new state-of-the-art performance on six datasets. Overall, we summarize our key technical contributions here:

1. A new analysis of contrastive learning performance showing that standard multimodal CL fails to capture task-relevant unique information under low shared or high unique information cases.
2. A new contrastive learning algorithm called FactorCL: 1. FactorCL factorizes task-relevant information into shared and unique information, expanding contrastive learning to better handle low shared or high unique information. 2. FactorCL optimizes shared and unique information separately, by removing task-irrelevant information via MI upper bounds and capturing task-relevant information via lower bounds, yielding optimal task-relevant representations. 3. FactorCL leverages multimodal augmentations to approximate task-relevant information, enabling self-supervised learning from our proposed FactorCL.

## 2 Analysis of Multi-view Contrastive Learning

We begin by formalizing definitions of four types of information: shared, unique, task-relevant, and task-irrelevant information in multimodal data. To formalize the learning setting, we assume there exist two modalities expressed as random variables \(X_{1}\) and \(X_{2}\) with outcomes \(x_{1}\) and \(x_{2}\), and a task with the random variable \(Y\) and outcome \(y\). We denote \(X_{-i}\) as the other modality where appropriate.

**Shared and unique information**: We formalize shared and unique information by decomposing the total multimodal information \(I(X_{1},X_{2};Y)\) into three conditional mutual information (MI) terms:

\[I(X_{1},X_{2};Y)=;X_{2};Y)}_{S=}+;Y|X_{2})}_{U_{1}=X_{1}}+;Y|X_{1})}_{U_{2}=X_{2}},\] (1)

where \(I(X_{1},X_{2};Y)= p(x_{1},x_{2},y),x_{2},y)}{p(x_{1},x_{ 2}p(y))}dx_{1}dx_{2}dy\) is the total MI between the joint random variable \(X_{1},X_{2}\) and the task \(Y\), \(S=I(X_{1};X_{2};Y)=I(X_{1};X_{2})-I(X_{1};X_{2}|Y)= p(x_{1},x_{2}) {p(x_{1},x_{2})}{p(x_{1})p(x_{2})}dx_{1}dx_{2}-I(X_{1};X_{2}|Y)\) is the task-relevant shared in

Figure 1: **Left**: We define \(S=I(X_{1};X_{2};Y)\) as task-relevant shared information and \(U_{1}=I(X_{1};Y|X_{2})\), \(U_{2}=I(X_{2};Y|X_{1})\) as task-relevant unique information. **Right**: On controllable datasets with varying ratios of \(S\), \(U_{1}\), and \(U_{2}\), standard CL captures \(S\) but struggles when there is more \(U_{1}\) and \(U_{2}\). Our FactorCL approach maintains best performance, whereas SimCLR  and SupCon  see performance drops as unique information increases, and Cross+Self [33; 36; 44; 89] recovers in fully unique settings but suffers at other ratios.

formation, \(I(X_{1};X_{2}|Y)= p(x_{1},x_{2},y),x_{2}|y)}{p(x_{1}|y)p(x_{ 2}|y)}dx_{1}dx_{2}dy\) is the task-irrelevant shared information, and \(U_{1}=I(X_{1};Y|X_{2})\), \(U_{2}=I(X_{2};Y|X_{1})\) denote unique task-relevant information.

**Limitations of CL**: Current approaches for CL maximize mutual information \(I(X_{1};X_{2})\) (and subsequently task-relevant shared information \(I(X_{1};X_{2};Y)\) during supervised fine-tuning), without modeling unique information. These methods generally learn a pair of representations ,

\[Z_{1}=*{arg\,max}_{Z_{1}=f_{}(X_{1})}I(Z_{1};X_{2}), Z _{2}=*{arg\,max}_{Z_{2}:=f_{}(X_{2})}I(X_{1};Z_{2}).\] (2)

For example, \(Z_{1}\) could encode images \(X_{1}\) and \(Z_{2}\) encodes text \(X_{2}\) via maximizing a lower bound on \(I(X_{1};X_{2})\) using the NCE objective . The NCE objective falls into a broader class of contrastive learning methods  that model the ratio between joint densities \(p(x_{1},x_{2})\) and product of marginal densities \(p(x_{1})p(x_{2})\) using positive and negative samples  or probabilistic classifiers . It has been shown that contrastive learning works well under the assumption of multi-view redundancy :

**Definition 1**.: _(Multi-view redundancy) \(>0\) such that \(I(X_{1};Y|X_{2})\) and \(I(X_{2};Y|X_{1})\)._

In other words, the task-relevant information in data is mostly shared across both views and the unique information is at most a small \(\). From a representation perspective, Tian et al.  further introduces the assumption that the optimal representation is minimal and sufficient, where all learned task-relevant information is shared information: \(I(Z_{1};Y|X_{2})=I(Z_{2};Y|X_{1})=0\). While the multi-view redundancy is certainly true for particular types of multimodal distributions, it crucially ignores settings that display _multi-view non-redundancy_ and unique information can be important, such as when health indicators, medical sensors, and robotic visual or force sensors each provide unique information not present in other modalities .

**Definition 2**.: _(Multi-view non-redundancy) \(>0\) such that \(I(X_{1};Y|X_{2})>\) or \(I(X_{2};Y|X_{1})>\)._

Under multi-view non-redundancy, we show that standard CL only receives a weak training signal since it can only maximize a lower bound on shared information \(I(X_{1};X_{2})\), and struggles to learn task-relevant unique information. We formalize this intuition with the following statement:

**Theorem 1**.: _(Suboptimality of standard CL) When there is multi-view non-redundancy as in Definition 2, given optimal representations \(\{Z_{1},Z_{2}\}\) that satisfy Eq.(2 and \(I(Z_{1};Y|X_{2})=I(Z_{2};Y|X_{1})=0\), we have that_

\[I(Z_{1},Z_{2};Y)=I(X_{1},X_{2};Y)-I(X_{1};Y|X_{2})-I(X_{2};Y|X_{1})=I(X_{1};X_{ 2})-I(X_{1};X_{2}|Y)<I(X_{1},X_{2};Y).\] (3)

_Correspondingly, the Bayes error rate \(P_{e}(Z_{1},Z_{2}):=1-_{p(z_{1},z_{2})}[_{y Y}P( =y z_{1},z_{2})]\) of contrastive representations \(\{Z_{1},Z_{2}\}\) for a downstream task \(Y\) is given by:_

\[P_{e}  1-[I(X_{1},X_{2};Y)-I(X_{1};Y|X_{2})-I(X_{2};Y|X_{1}) -H(Y)]\] (4) \[=1-[I(X_{1};X_{2};Y)-H(Y)]\] (5)

We include proofs and a detailed discussion of the assumptions in Appendix B. Based on Eq.(3), \(I(Z_{1},Z_{2};Y)\) decreases with higher task-relevant unique information \(I(X_{1};Y|X_{2})\) and \(I(X_{2};Y|X_{1})\); we call this the difference \(I(X_{1},X_{2};Y)-I(Z_{1},Z_{2};Y)\) the _uniqueness gap_. The uniqueness gap measures the loss in task-relevant information between the input and encoded representation: as task-relevant unique information grows, the uniqueness gap increases. In addition, \(I(Z_{1},Z_{2};Y)\) also drops with lower \(I(X_{1};X_{2})\) (i.e., two modalities sharing little information to begin with), or with higher \(I(X_{1};X_{2}|Y)\) (i.e., when the shared information is mostly task-irrelevant). Similarly, in Eq.(5), the Bayes error rate of using \(\{Z_{1},Z_{2}\}\) for prediction is directly related to the task-relevant information in \(\{Z_{1},Z_{2}\}\): error on the downstream task increases with higher unique information and lower shared information.

## 3 Factorized Contrastive Learning

We now present a suite of new CL objectives that alleviate the challenges above and work at all ranges of shared and unique information. At a high level, we aim to learn a set of factorized representations \(Z_{S_{1}},Z_{S_{2}},Z_{U_{1}},Z_{U_{2}}\) representing task-relevant information in \(X_{1}\) shared with \(X_{2}\), in \(X_{2}\) shared with \(X_{1}\), unique to \(X_{1}\), and unique to \(X_{2}\) respectively. As common in practice , we define neural networks \(f_{}\) with trainable parameters \(\) to extract representations from inputs \(X_{1}\) and \(X_{2}\). Learning these parameters requires optimizing differentiable and scalable training objectives to capture task-relevant shared and unique information (see overview in Figure 2):

\[Z_{S_{1}} =*{arg\,max}_{Z_{1} f_{}(X_{1})}I(Z_{1};X_{2 };Y), Z_{S_{2}} =*{arg\,max}_{Z_{2} f_{}(X_{2})}I(Z_{2};X_{1} ;Y),\] (6) \[Z_{U_{1}} =*{arg\,max}_{Z_{1} f_{}(X_{1})}I(Z_{1};Y|X _{2}), Z_{U_{2}} =*{arg\,max}_{Z_{2} f_{}(X_{2})}I(Z_{2};Y|X_ {1}).\] (7)

where \(I(Z_{1};X_{2};Y)=I(Z_{1};X_{2})-I(Z_{1};X_{2}|Y)\) is the shared information and \(I(Z_{2};X_{1};Y)=I(Z_{2};X_{2})-I(Z_{2};X_{1}|Y)\) is the unique information. One important characteristic of our framework is that when unique information is zero: \(I(X_{1};Y|X_{2})=0\) and \(I(X_{2};Y|X_{1})=0\), or all shared information is task-relevant: \(I(X_{1};X_{2};Y)=I(X_{1};X_{2})\), our framework recovers standard CL as in Eq.(2). However, as we have previously indicated and will show empirically, these assumptions can easily be violated, and our framework enlarges Eq.(2) to cases where unique information is present.

The learned \(Z\)s can then be used as input to a linear classifier and fine-tuned to predict the label for multimodal classification or retrieval tasks. However, the shared and unique MI terms above are often intractable in practice. In the next section, we will build up our method step by step, eventually showing that each term in Eqs.(6- 7) can be approximated as follows:

\[S =I(X_{1};X_{2};Y) I_{}(X_{1};X_{2})-I_{}(X_{1};X_{2}|X_{1}^{},X_{2}^{})\] (8) \[U_{i} =I(X_{i};Y|X_{-i}) I_{}(X_{i};X_{i}^{})-I_{ }(X_{1};X_{2})+I_{}(X_{1};X_{2}|X_{1}^{},X_{2}^ {})\] (9)

where \(I_{}\) and \(I_{}\) are scalable contrastive estimators (Section 3.1) and \(X_{1}^{},X_{2}^{}\) are suitable data augmentations (Section 3.2) on each modality. Overall, these equations can be interpreted as both positive and negative signals to learn representations for \(S\) and \(U\). For shared information \(S\), the estimator maximizes task-relevant shared information via \(I_{}(X_{1};X_{2})\) while removing task-irrelevant shared information via a novel upper bound \(-I_{}(X_{1};X_{2}|X_{1}^{},X_{2}^{})\). For unique information \(U_{i}\), we capture task-relevant uniqueness via \(+I_{}(X_{i};X_{i}^{})\) while non-unique information is removed via \(-(I_{}(X_{1};X_{2})-I_{}(X_{1};X_{2}|X_{1}^{},X _{2}^{}))\). In the following sections, we derive this final objective step-by-step: (1) approximating the MI objectives in \(S\) and \(U\) with CL estimators, (2) relaxing the dependence on labels \(Y\) with self-supervised data augmentations, finally (3) discussing overall training and implementation details of end-to-end self-supervised learning.

### Supervised FactorCL with shared and unique information

To capture shared and unique information via an objective function, we will need to maximize lower bounds for all terms with a positive sign in Eq.(8) and (9) (\(I(X_{1};X_{2}),I(X_{i};Y),I(X_{1};X_{2}|Y)\)) and minimize upper bounds for all terms with a negative sign (\(I(X_{1};X_{2}),I(X_{1};X_{2}|Y)\)). Our first theorem derives general lower and upper bounds for MI terms as variants of contrastive estimation:

Figure 2: FactorCL: We propose a self-supervised CL method to learn _factorized_ representations \(Z_{S_{1}}\), \(Z_{S_{2}}\), \(Z_{U_{1}}\), and \(Z_{U_{2}}\) to capture task-relevant information shared in both \(X_{1}\) and \(X_{2}\), unique to \(X_{1}\), and unique to \(X_{2}\). By starting with information-theoretic first principles of shared and unique information, we design contrastive estimators to both _capture task-relevant_ and _remove task-irrelevant_ information, where a notion of task-relevance without explicit labels is afforded by a new definition of _multimodal augmentations_\(X_{1}^{},X_{2}^{}\). Lower bounds are in green and upper bounds are in red.

**Theorem 2**.: _(Contrastive estimators for \(I(X_{1};X_{2})\)) Defining the NCE and NCE-CLUB estimators,_

\[I_{}(X_{1};X_{2}) =_{x_{1},x_{2}^{+}p(x_{1},x_{ 2})\\ x_{2}^{-}p(x_{2})}[,x_{2}^{+} )}{_{k} f(x_{1},x_{2}^{-})}]\] (10) \[I_{}(X_{1};X_{2}) =_{x_{1},x_{2}^{+}p(x_{1},x_{ 2})}[f^{*}(x_{1},x_{2}^{+})]-_{ {c}x_{1}p(x_{1})\\ x_{2}^{-}p(x_{2})}[f^{*}(x_{1},x_{2}^{-})]\] (11)

_where \(f^{*}(x_{1},x_{2})\) is the optimal critic from \(I_{}\) plugged into the \(I_{}\) objective . We call the proposed plug-in objective Eq.(11) \(I_{}\), and obtain lower and upper bounds on \(I(X_{1};X_{2})\):_

\[I_{}(X_{1};X_{2}) I(X_{1};X_{2}) I_{}(X_{1}; X_{2}).\] (12)

Proof.: The lower bound \(I_{}(X_{1};X_{2}) I(X_{1};X_{2})\) follows from Oord et al. : optimizing the objective leads to an optimal critic \(f^{*}= p(x_{1}|x_{2})+c(x_{1})\), with a deterministic function \(c()\). Plugging optimal critic \(f^{*}\) into \(I_{}(X_{1};X_{2})\) cancels out the \(c(x_{1})\) term and yields \(I_{}(X_{1};X_{2})\) and \(I(X_{1};X_{2}) I_{}\). We include a detailed proof in Appendix C.1. 

\(I_{}(X_{1};X_{2})\) gives a desired upper bound of \(I(X_{1};X_{2})\) "for free" while avoiding separately optimizing lower bound and upper bounds. In Figure 3, we show these two bounds in practice across two Gaussian distributions \(X_{1}\) and \(X_{2}\) with varying amounts of MI \(I(X_{1};X_{2})\). We use the second formulation of \(I_{}\), which assumes \(p(x_{1}|x_{2})\) to be unknown. Our upper bound is empirically tighter (see Figure 3) and comes for "free" via jointly maximizing the lower bound \(I_{}\). These lower and upper bounds can be seen as new contrastive objectives over positive and negative \((x_{1},x_{2})\) pairs, enabling a close integration with existing pre-training paradigms. Finally, we can similarly obtain bounds for the conditional MI \(I_{}(X_{1};X_{2}|Y) I(X_{1};X_{2}|Y) I_{}(X_{1 };X_{2}|Y)\):

\[I_{}(X_{1};X_{2}|Y) =_{p(y)}[_{x_{1},x_{ 2}^{+}p(x_{1},x_{2}|y)\\ x_{2}^{-}p(x_{2}|y)}[,x_{2}^{ +},y)}{_{k} f(x_{1},x_{2}^{-},y)}]]\] (13) \[I_{}(X_{1};X_{2}|Y) =_{p(y)}[_{x_{1},x_{ 2}^{+}p(x_{1},x_{2}|y)}[f^{*}(x_{1},x_{2}^{+},y) ]-_{x_{1}p(x_{1}|y)\\ x_{2}^{-}p(x_{2}|y)}[f^{*}(x_{1},x_{2}^{-},y)]]\] (14)

These two bounds result in _conditional CL_ objectives [51; 74; 78] - they differ critically from standard CL methods since they capture task-irrelevant shared information that remains between \(X_{1}\) and \(X_{2}\) after observing \(Y\). This task-irrelevant shared information is removed by minimizing its upper bound. Note that \(f(x_{1},x_{2},y)\) here denotes a different function from \(f(x_{1},x_{2})\) in Eq.(10), as the general forms are different (taking in \(x_{1},x_{2}\) versus \(x_{1},x_{2},y\)). \(f(x_{1},x_{2},y)\) can be implemented in different ways, e.g., \(g([x_{1},y])^{T}h(x_{2})\) where \(g(),h()\) are trainable encoders and \([x_{1},y]\) denotes concatenation .

### Self-supervised FactorCL via multimodal augmentations

The derivations above bring about supervised CL objectives with access to \(Y\). For unsupervised CL [58; 72], we derive similar objectives without access to \(Y\) by leveraging semantic augmentations on each modality. Denote \(X^{}\) as some augmentation of \(X\) (e.g., rotating, shifting, or cropping). Under

Figure 3: Estimated \(I_{}\) lower bound  and our proposed upper bound \(I_{}\) on sample distributions with changing mutual information: our upper bound is tighter, more accurate, and more stable than \(I_{}\) upper bound , and also comes for ‘free’ via jointly estimating both lower and upper bounds simultaneously. We find that as dimension increases, the \(I_{}\) estimator collapses to zero and no longer tracks true MI.

[MISSING_PAGE_FAIL:6]

## 4 Experiments

We run comprehensive experiments on a suite of synthetic and large-scale real-world datasets with varying requirements of shared and unique task-relevant information, comparing our FactorCL method to key baselines:

1. SimCLR [(13)]: the straightforward method of cross-modal \((X_{1},X_{2})\) contrastive learning.
2. Cross+Self [(33; 36; 44; 64; 85; 89)]: captures a range of methods combining cross-modal \((X_{1},X_{2})\) CL with additional unimodal \((X_{i},X_{i}^{})\) CL objectives. This category also includes other ways of preserving unique information, such as through (variational) autoencoder reconstructions [(81)].
3. Cross+Self+Fact [(86; 89)]: A factorized extension of Cross+Self, which is approximately done in prior work that adds separate (typically pre-trained) unimodal encoders for each modality.
4. SupCon [(41)], which learns \(I(X_{1};X_{2}|Y)\) using CL conditioned on \(Y\) from labeled data.

We also carefully ablate each component of our method and investigate factors, including training data size and choice of augmentations. The intermediate ablations that emerge include:

1. FactorCL-SUP: The supervised CL version which uses labels \(Y\) in Eqs.(13) and (14).

Figure 4: Standard vs. unique augmentations for the figurative language [(88)] dataset. After augmenting text modality \(X_{1}\) independently (same for both augmentation types), we illustrate their differences for image augmentation: unique augmentation on images should avoid removing information referred to by \(X_{1}\) (the text). The text mentions that the car is fast so unique augmentation for images should _not_ remove the highway pixels of the image which can suggest the car is fast.

2. FactorCL-SSL: The fully self-supervised version of our approach replacing \(Y\) with multimodal augmentations \(X_{1}^{}\) and \(X_{2}^{}\) to approximate the task.
3. OurCL-SUP: FactorCL-SUP but removing the factorization so only two features \(Z_{1}\) is optimized for both \(I(X_{1};X_{2};Y)\) and \(I(X_{1};Y|X_{2})\), \(Z_{2}\) optimized for both \(I(X_{1};X_{2};Y)\) and \(I(X_{2};Y|X_{1})\).
4. OurCL-SSL: FactorCL-SSL but also removing the factorization in the self-supervised setting.

The formulation of each ablation and implementation can be found in Appendix D.1.

### Controlled experiments on synthetic datasets

**Synthetic data generation**: We begin by generating data with controllable ratios of task-relevant shared and unique information. Starting with a set of latent vectors \(w_{1},w_{2},w_{s}(0_{d},_{d}^{2}),d=50\) representing information unique to \(X_{1},X_{2}\) and common to both respectively, the concatenated vector \([w_{1},w_{s}]\) is transformed into high-dimensional \(x_{1}\) using a fixed transformation \(T_{1}\) and likewise \([w_{2},w_{s}]\) to \(x_{2}\) via \(T_{2}\). The label \(y\) is generated as a function (with nonlinearity and noise) of varying ratios of \(w_{s}\), \(w_{1}\), and \(w_{2}\) to represent shared and unique task-relevant information.

**Results**: In Figure 1, we show our main result on synthetic data comparing FactorCL with existing CL baselines. FactorCL consistently maintains the best performance, whereas SimCLR  and SupCon  see performance drops as unique information increases. Cross+Self [33; 36; 44; 89] recovers in fully unique settings (x-axis\(=1.0\)) but suffers at other ratios.

**Representation probing information**: We run a probing experiment to compute how well different contrastive representations capture shared and unique information. In Table 1, for the \(Z_{i}\)'s learned by each method, we approximately compute \(I(Z_{i};w_{1})\), \(I(Z_{i};w_{2})\), and \(I(Z_{i};w_{s})\) with respect to ground truth generative variables \(w_{s}\), \(w_{1}\), and \(w_{2}\). As expected, existing methods such as SimCLR capture smaller amounts of unique information (roughly \(4\) bits in \(I(Z_{i};w_{1})\) and \(I(Z_{i};w_{2})\)), focusing instead on learning \(I(Z_{i};w_{s})\) (12 bits). Cross+self captures slightly larger \(I(Z_{i};w_{2})=4.26\), and SupCon with labeled data captures up to \(5\) bits of unique information. Our FactorCL approach captures \(7\) bits of unique information and maintains \(10\) bits of shared information, with total information captured higher than the other approaches. Furthermore, \(\{Z_{S_{1}},Z_{S_{2}}\}\) capture more information about \(w_{s}\), \(Z_{U_{1}}\) about \(w_{1}\), and \(Z_{U_{2}}\) about \(w_{2}\), indicating that factorization in our approach is successful.

### Self-supervised multimodal learning with low redundancy and high uniqueness

**Multimodal fusion datasets**: We use a large collection of real-world datasets provided in MultiBench , where we expect varying ratios of shared and unique information important for the task, to compare FactorCL with other CL baselines:

1. MIMIC : mortality and disease prediction from \(36,212\) medical records (tabular patient data and medical time-series sensors from ICU).
2. MOSEI : multimodal sentiment and emotion benchmark with \(23,000\) monologue videos.
3. MOSI : multimodal sentiment analysis from \(2,199\) YouTube videos.
4. UR-FUNNY : a dataset of humor detection from more than \(16,000\) TED talk videos.
5. MUSTARD : a corpus of \(690\) videos for research in sarcasm detection from TV shows.
6. IRFL : \(6,697\) matching images and figurative captions (rather than literal captions).

Together, these datasets cover seven different modalities from the healthcare, affective computing, and multimedia research areas and total more than \(84,000\) data points. For MIMIC with tabular and medical sensor inputs, we train self-supervised CL models on top of raw modality inputs. For IRFL with image and caption inputs, we start with a pretrained CLIP model  and perform continued pre-training to update CLIP weights with our FactorCL objectives, before linear classifier testing. For the remaining four video datasets, we train self-supervised CL models starting from standard pre-extracted text, video, and audio features . Please refer to Appendix D.2 for experimental details. We release our code and models at https://github.com/pliang279/FactorCL.

   Model &  &  &  &  \\ Representations & \(Z_{1}\) & \(Z_{2}\) & \(Z_{1}\) & \(Z_{2}\) & \(Z_{1}\) & \(Z_{2}\) & \(Z_{U_{1}}\) & \(Z_{U_{2}}\) & \(Z_{S_{1}}\) & \(Z_{S_{2}}\) \\  \(I(Z;w_{1})\) & 4.45 & 0.16 & 4.39 & 0.14 & 5.17 & 0.19 & **7.83** & 0.03 & 6.25 & 0.04 \\ \(I(Z;w_{2})\) & 0.17 & 3.92 & 0.13 & 4.26 & 0.23 & 5.17 & 0.06 & **7.17** & 0.05 & 5.79 \\ \(I(Z;w_{s})\) & 12.61 & 12.06 & 11.30 & 11.47 & 7.48 & 7.17 & 9.47 & 9.89 & 10.13 & 9.40 \\   

Table 1: We probe whether contrastive representations learned by classic CL methods and FactorCL contain shared \(w_{s}\) or unique \(w_{1},w_{2}\) information. FactorCL captures the most unique information.

**Multimodal fusion results**: From Table 2, FactorCL significantly outperforms the baselines that do not capture both shared and unique information in both supervised and self-supervised settings, particularly on MuStARD (where unique information expresses sarcasm, such as sardonic facial expressions or ironic tone of voice), and on MIMIC (with unique health indicators and sensor readings). In Table 3, we also show that FactorCL substantially improves the state-of-the-art in classifying images and figurative captions which are not literally descriptive of the image on IRFL, outperforming zero-shot and fine-tuned CLIP  as well as continued pre-training baselines on top of CLIP.

**Modeling ablations**: In Table 2, we also carefully ablate each component in our method and indicate either existing baselines or newly-run ablation models.

1. **Factorized representations**: In comparing FactorCL-SSL with OurCL-SSL, and also FactorCL-SUP with OurCL-SUP, we find that factorization is critical: without it, performance drops on average \(6.1\%\), with performance drop as high as \(8.6\%\) for MIMIC.
2. **Information removal via upper bound**: By comparing FactorCL with SimCLR, Cross+Self, and Cross+Self+Fact, and SupCon that only seek to capture task-relevant information via contrastive lower bounds on MI, we find that separately modeling the task-relevant information (to be captured) and task-irrelevant information (to be removed) is helpful. Without removing task-irrelevant information via the upper-bound objective, performance drops on average \(13.6\%\), with performance drops as high as \(23.5\%\) for the MOSI dataset. We also found that training was more difficult without this objective, which is expected due to overwhelming superfluous information from the dataset .
3. **Multimodal augmentations**: Finally, we investigate the differences between separate unimodal augmentations (FactorCL-IndAug in Table 3) versus a joint multimodal augmentation (FactorCL-SSL) on the IRFL dataset. We choose this dataset since its images and captions are the easiest to visualize (see Figure 4 for augmentations from both strategies). In the self-supervised setting, we find that multimodal augmentations achieve \(95\%\) performance, higher than the \(92\%\) for separate unimodal augmentations, and both outperform baselines SimCLR and Cross+Self.

**Ablations on \(S,U_{1}\) and \(U_{2}\)**: In Table 4, we also test FactorCL when training linear classifiers on top of only shared \(\{Z_{S_{1}},Z_{S_{2}}\}\) and unique \(Z_{U_{1}}\), \(Z_{U_{2}}\) separately. We call these models FactorCL-\(S\), FactorCL-\(U_{1}\), and FactorCL-\(U_{2}\). Immediately, we observe that performance drops as compared to the full FactorCL model, indicating that both shared and unique information are critical in real-world multimodal tasks. As expected, the best-performing submodel is the one that captures the region with the largest amount of task-relevant information: MOSEI and MOSI are known to include a lot of redundancy and unique information since language is very important for detecting sentiment , so FactorCL-\(S\) and FactorCL-\(U_{2}\) perform best. For sarcasm detection on MuStARD, video information is most important with FactorCL-\(U_{1}\) performing best (\(59.4\%\)), and ablation models are also the furthest away from full multimodal performance (\(69.9\%\)). This is aligned with intuition where sarcasm is expressed through tone of voice and visual gestures (high \(U_{1}\)), as well as from contradictions between language and video (higher multimodal performance).

   Model & \((X_{1};X_{2})\) & \((X_{i};X_{j}^{})\) & \((X_{1};X_{2}|Y)\) & \((X_{2}^{})\) & Fact & MIMIC & MOSEI & MOSI & UR-FUNNY & MUStARD \\  SimCLR  & ✓ & ✗ & ✗ & ✗ & ✗ & 66.67\% & 71.03\% & 46.21\% & 50.09\% & 53.48\% \\ Cross+Self  & ✓ & ✓ & ✗ & ✗ & ✗ & 65.20\% & 71.04\% & 46.92\% & 56.52\% & 53.91\% \\ Cross+Self+Fact  & ✓ & ✓ & ✗ & ✗ & ✓ & 65.49\% & 71.07\% & 52.37\% & 59.91\% & 53.91\% \\ OurCL-SSL & ✓ & ✓ & ✓ & ✓ & ✗ & 65.22\% & 71.16\% & 48.98\% & 58.79\% & 53.98\% \\ FactorCL-SSL & ✓ & ✓ & ✓ & ✓ & ✓ & **67.34\%** & **74.88\%** & **52.91\%** & **60.50\%** & **55.80\%** \\  SupCon  & ✗ & ✗ & ✓ & ✗ & ✗ & 67.37\% & 72.71\% & 47.23\% & 50.98\% & 52.75\% \\ OurCL-SUP & ✓ & ✓ & ✓ & ✗ & ✗ & 68.16\% & 71.15\% & 65.32\% & 58.32\% & 65.05\% \\ FactorCL-SUP & ✓ & ✓ & ✓ & ✗ & ✓ & **76.79\%** & **77.34\%** & **70.69\%** & **63.52\%** & **69.86**\% \\   

Table 2: Results on MultiBench  datasets with varying shared and unique information: FactorCL achieves strong results vs self-supervised (top \(5\) rows) and supervised (bottom \(3\) rows) baselines that do not have unique representations, factorization, upper-bounds to remove irrelevant information, and multimodal augmentations.

   Task & IRFL \\  Zero-shot CLIP  & 89.15\% \\ SimCLR  & 91.57\% \\ Cross+Self  & 95.18\% \\ FactorCL-IndAug & 92.77\% \\ FactorCL-SSL & **95.18\%** \\  Fine-tuned CLIP  & 96.39\% \\ SupCon  & 89.16\% \\ FactorCL-SUP & **98.80\%** \\   

Table 3: Continued pre-training on CLIP with our FactorCL objectives on classifying images and figurative language.

**Additional results**: In Appendix D.3, we also verify FactorCL in settings with abundant shared information, where we expect to recover the same performance as standard CL [13; 58; 72].

## 5 Related Work

**Contrastive learning** is a successful self-supervised learning paradigm for computer vision [11; 13; 14; 25; 28; 58], natural language [24; 54; 56], speech [5; 58; 63], and multimodal tasks [1; 37; 61]. Its foundational underpinnings are inspired by work in multiview information theory [23; 41; 70; 72; 76] studying the shared information between two views and whether they are necessary or sufficient in predicting the label. Recently, Wang et al.  and Kahana and Hoshen  discuss the limitations of assuming multiview redundancy and propose autoencoder reconstruction or unimodal contrastive learning to retain unique information, which resembles the Cross+self baselines in our experiments. We refer the reader to Shwartz-Ziv and LeCun  for a comprehensive review on multiview and contrastive learning. Our work also relates to conditional contrastive learning [17; 51; 78; 87], where positive or negative pairs are supposed to sample from conditional distributions.

**Multimodal contrastive learning** aims to align related data from different modalities, typically provided as positive pairs. This could be done via optimizing a contrastive objective for inter-modality pairs [1; 2; 37; 61], or both intra- and inter-modality data pairs [33; 36; 42; 44; 89]. Our work also relates to factorized representation learning, which primarily studies how to capture modality-specific information primarily in each modality and multimodal information redundant in both modalities [32; 75]. Prior work has used disentangled latent variable models [8; 30; 32; 75], mixture-of-experts , or product-of-experts  layer to explain factors in multimodal data.

**Information theory**[18; 65] has been used to study several phenomena in multimodal learning, including co-learning [62; 92] and multi-view learning [34; 76]. Due to its theoretical importance, several lower and upper bounds have been proposed for practical estimation [58; 59; 60; 84]. We build on the CLUB upper bound  to create a more accurate and stable bound. Our characterizations of shared and unique information are also related to partial information decomposition , co-information [7; 80], interaction information , and cross-domain disentanglement  research.

## 6 Conclusion

This paper studied how standard CL methods suffer when task-relevant information lies in regions unique to each modality, which is extremely common in real-world applications such as sensor placement, medical testing, and multimodal interaction. In response, we proposed FactorCL, a new method expanding CL techniques through the use of factorized representations, removing task-irrelevant information via upper bounds on MI, and multimodal data augmentations suitable for approximating the unobserved task. Based on FactorCL's strong performance, there are several exciting directions in extending these ideas for masked and non-contrastive pre-training; we further discuss broader impacts and limitations of this line of work in Appendix A.