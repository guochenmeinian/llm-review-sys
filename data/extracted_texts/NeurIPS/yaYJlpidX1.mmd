# Metalearning to Continually Learn In Context

Anonymous Author(s)

Affiliation

Address

email

###### Abstract

General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF)--previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own _in-context_ continual (meta-)learning algorithms. ACL encodes continual learning desiderata--good performance on both old and new tasks--into its meta-learning objectives. Our experiments demonstrate that, in general, in-context learning algorithms also suffer from CF but ACL effectively solves such "in-context catastrophic forgetting". Our ACL-learned algorithms outperform hand-crafted ones and popular meta-continual learning methods on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple few-shot and standard image classification datasets. Going beyond, we also highlight the limitations of in-context continual learning, by investigating the possibilities to extend ACL to the realm of state-of-the-art CL methods which leverage pre-trained models.1

## 1 Introduction

Enemies of memories are other memories . Continually-learning artificial neural networks (NNs) are memory systems in which their _weights_ store memories of task-solving skills or programs, and their _learning algorithm_ is responsible for memory read/write operations. Conventional learning algorithms--used to train NNs in the standard scenarios where all training data is available _at once_--are known to be inadequate for continual learning (CL) of multiple tasks where data for each task is available _sequentially and exclusively_, one at a time. They suffer from "catastrophic forgetting" (CF; [2; 3; 4; 5]); the NNs forget, or rather, the learning algorithm erases, previously acquired skills, in exchange of learning to solve a new task. Naturally, a certain degree of forgetting is unavoidable when the memory capacity is limited, and the amount of things to remember exceeds such an upper bound. In general, however, capacity is not the fundamental cause of CF; typically, the same NNs, suffering from CF when trained on two tasks sequentially, can perform well on both tasks when they are jointly trained on the two tasks at once instead (see, e.g., ).

The real root of CF lies in the learning algorithm as a memory mechanism. A "good" CL algorithm should preserve previously acquired knowledge while also leveraging previous learning experiences to improve future learning, by maximally exploiting the limited memory space of model parameters. All of this is the _decision-making problem of learning algorithms_. In fact, we can not blame the conventional learning algorithms for causing CF, since they are not aware of such a problem. They are designed to train NNs for a given task at hand; they treat each learning experience independently (they are stationary up to certain momentum parameters in certain optimizers), and ignore anypotential influence of current learning on past or future learning experiences. Effectively, more sophisticated algorithms previously proposed against CF [7; 8], such as elastic weight consolidation [9; 10] or synaptic intelligence , often introduce manually-designed constraints as regularization terms to explicitly penalize current learning for deteriorating knowledge acquired in past learning.

Here, instead of hand-crafting learning algorithms for continual learning, we train self-referential neural networks [12; 13] to meta-learn their own "in-context" continual learning algorithms. We train them through gradient descent on learning objectives that reflect desiderata for continual learning algorithms--good performance on both old and new tasks, including forward and backward transfer. In fact, by extending the standard settings of few-shot or meta-learning based on sequence-processing NNs [14; 15; 16; 17; 18], the continual learning problem can also be formulated as a long-span sequence processing task . Corresponding CL sequences can be obtained by concatenating multiple few-shot/meta-learning sub-sequences, where each sub-sequence consists of input/target examples corresponding to the task to be learned in-context. As we'll see in Sec. 3, this setting also allows us to seamlessly express classic desiderata for CL as part of objective functions of the meta-learner.

Once formulated as such a sequence-learning task, we let gradient descent search for CL algorithms achieving the desired CL behaviors in the program space of NN weights. In principle, all typical challenges of CL--such as the stability-plasticity dilemma --are automatically discovered and handled by the gradient-based program search process. Once trained, CL is automated through recursive self-modification dynamics of the trained NN, without requiring any human intervention such as adding extra regularization or setting hyper-parameters for CL. Therefore, we call our method, Automated Continual Learning (ACL).

Our experiments focus on supervised image classification, making use of standard few-shot learning datasets for meta-training, namely, Mini-ImageNet [21; 22], Omniglot , and FC100 , while we also meta-test on other datasets including MNIST , FashionMNIST  and CIFAR-10 .

**Our core contribution** is a set of focused experiments showing various facets of in-context CL: (1) We first reveal the "in-context catastrophic forgetting" problem using two-task settings (Sec. 4.1) and analyse its emergence (Sec. 4.2). We are not aware of any prior work discussing this problem. (2) We show very promising results of our ACL-trained learning algorithm on the classic Split-MNIST [6; 28] benchmark, outperforming hand-crafted learning algorithms and prior meta-continual learning methods [29; 30; 31]. (3) We experimentally illustrate the limitations of ACL on 5-datasets  and Split-CIFAR100 by comparing to more recent prompt-based state-of-the-art CL methods [33; 34].

## 2 Background

### Continual Learning

The main focus of this work is on continual learning [35; 36] in _supervised_ learning settings even though high-level principles we discuss here also transfer to reinforcement learning settings . In addition, we focus on the realm of CL methods that keep model sizes constant (unlike certain CL methods that incrementally add more parameters as more tasks are presented; see, e.g., ), and do not make use of any external replay memory (used in other CL methods; see, e.g., [40; 41; 42; 43; 39; 44]).

Classic desiderata for a CL system (see, e.g., [44; 45]) are typically summarized as good performance on three metrics: _classification accuracies_ on each dataset (their average), _backward transfer_ (i.e., impact of learning a new task on the model's performance on previous tasks; e.g., catastrophic forgetting is a negative backward transfer), and _forward transfer_ (impact of learning a task for the model's performance on a future task). From a broader perspective of meta-learning systems, we may also measure other effects such as _learning acceleration_ (i.e., whether the system leverages previous learning experiences to accelerate future learning); here our primary focus remains the classic CL metrics above.

### Few-shot/meta-learning via Sequence Learning

In Sec. 3, we'll formulate continual learning as a long-span sequence processing task. This is a direct extension of the classic few-shot/meta learning formulated as a sequence learning problem. In fact, since the seminal works [14; 15; 16; 17] (see also ), many sequence processing neural networks (see, e.g., [47; 48; 49; 50; 51; 52; 53; 54; 55; 56; 57; 58]) including Transformers [59; 18]) have been trained as a meta-learner [13; 12] that learn by observing sequences of training examples (i.e., pairs of inputs and their labels) in-context.

Here we briefly review such a formulation. Let \(d\), \(N\), \(K\), \(P\) be positive integers. In sequential \(N\)-way \(K\)-shot classification settings, a sequence processing NN with a parameter vector \(^{P}\) observes a pair (\(_{t}\), \(y_{t}\)) where \(_{t}^{d}\) is the input and \(y_{t}\{1,...,N\}\) is its label at each step \(t\{1,...,N K\}\), corresponding to \(K\) examples for each one of \(N\) classes. After the presentation of these \(N K\) examples (often called the _support set_), one extra input \(^{d}\) (often called the _query_) is fed to the model without its true label but with an "unknown label" token \(\) (number of input labels accepted by the model is thus \(N+1\)). The model is trained to predict its true label, i.e., the parameters of the model \(\) are optimized to maximize the probability \(p(y|(_{1},y_{1}),...,(_{N K},y_{N K}),(, );)\) of the correct label \(y\{1,...,N\}\) of the input query \(\). Since class-to-label associations are randomized and unique to each sequence (\((_{1},y_{1}),...,(_{N K},y_{N K})\)), \((,)\), each such a sequence represents a new (few-shot or meta) learning example to train the model. To be more specific, this is the _synchronous_ label setting of Mishra et al.  where the learning phase (observing examples, \((_{1},y_{1})\) etc.) is separated from the prediction phase (predicting label \(y\) given \((,)\)). We opt for this variant in our experiments as we empirically find this (at least in our specific settings) more stable than the _delayed_ label setting  where the model has to make a prediction for every input, and the label is fed to the model with a delay of one time step.

### Self-Referential Weight Matrices

Our method (Sec. 3) can be applied to any sequence-processing NN architectures in principle. Nevertheless, certain architectures naturally fit better to parameterize a self-improving continual learner. Here we use the _modern self-referential weight matrix_ (SRWM; [19; 60]) to build a generic self-modifying NN. An SRWM is a weight matrix that sequentially modifies itself as a response to a stream of input observations [12; 61]. The modern SRWM belongs to the family of linear Transformers (LTs) a.k.a. Fast Weight Programmers (FWPs; [62; 63; 64; 65; 66; 67; 68]). Linear Transformers and FWPs are an important class of the now popular Transformers : unlike the standard ones whose computational requirements grow quadratically and whose state size grows linearly with the context length, LTs/FWPs' complexity is linear and the state size is constant w.r.t. sequence length (like in the standard RNNs). This is an important property for in-context CL, since, conceptually, we want such a CL system to continue to learn for an arbitrarily long, lifelong time span. Moreover, the duality between linear attention and FWPs --and likewise, between linear attention and gradient descent-trained linear layers [69; 70]--have played a key role in certain theoretical analyses of in-context learning capabilities of Transformers [71; 72].

The dynamics of an SRWM  are described as follows. Let \(d_{}\), \(d_{}\), \(t\) be positive integers, and \(\) denote outer product. At each time step \(t\), an SRWM \(_{t-1}^{(d_{}+2*d_{}+1) d_{}}\) observes an input

Figure 1: An illustration of meta-training in Automated Continual Learning (ACL) for a self-referential/modifying weight matrix \(_{0}\). Weights \(_{}\) obtained by observing examples for Task A (_blue_) are used to predict a test example for Task A. Weights \(_{,}\) obtained by observing examples for Task A then those for Task B (_yellow_) are used to predict a test example for Task A (backward transfer) as well as a test example for Task B (forward transfer).

\(_{t}^{d_{}}\), and outputs \(_{t}^{d_{}}\), while also updating itself to \(_{t}\) as:

\[[_{t},_{t},_{t},_{t}]=_{t-1}_{t}\] (1) \[_{t}=_{t-1}(_{t});\,}_{t}= _{t-1}(_{t})\] (2) \[_{t}=_{t-1}+(_{t})(_{t}-} _{t})(_{t})\] (3)

where \(_{t},}_{t}^{(d_{}+2*d_{}+1)}\) are value vectors, \(_{t}^{d_{}}\) and \(_{t}^{d_{}}\) are query and key vectors, and \((_{t})\) is the learning rate. \(\) and \(\) denote sigmoid and softmax functions respectively. \(\) is typically also applied to \(_{t}\) in Eq. 1; here we follow Irie et al. 's few-shot image classification setting, and use the variant without it. Eq. 3 corresponds to a rank-one update of the SRWM, from \(_{t-1}\) to \(_{t}\), through the _delta learning rule_[73; 67] where the self-generated patterns, \(_{t}\), \((_{t})\), and \((_{t})\), play the role of _target_, _input_, and _learning rate_ of the learning rule respectively. The delta rule is crucial for the performance of LTs [67; 68; 74; 75].

The initial weight matrix \(_{0}\) is the only trainable parameters of this layer, that encodes the initial self-modification algorithm. We use the layer above as a direct replacement to the self-attention layer in the Transformer architecture ; and use the multi-head version of the computation above .

## 3 Method

**Task Formulation.** We formulate continual learning as a long-span sequence learning task. Let \(D\), \(N\), \(K\), \(L\) denote positive integers. Consider two \(N\)-way classification tasks \(\) and \(\) to be learned sequentially (as we'll see, this can be straightforwardly extended to more tasks). The formulation here applies to both "meta-training" and "meta-test" phases (see Appendix A.1 for more on this terminology). We denote the respective training datasets as \(\) and \(\), and test sets as \(^{}\) and \(^{}\). We assume that each datapoint in these datasets consists of one input feature \(^{D}\) of dimension \(D\) (generically denoted as vector \(\), but it is an image in all our experiments) and one label \(y\{1,...,N\}\). We consider two sequences of \(L\) training examples \((_{1}^{},y_{1}^{}),...,(_{L}^{ },y_{L}^{})\) and \((_{1}^{},y_{1}^{}),...,(_{L}^{ },y_{L}^{})\) sampled from the respective training sets \(\) and \(\). In practice, \(L=NK\) where \(K\) is the number of training examples for each class. By concatenating these two sequences, we obtain one long sequence representing CL examples to be presented as an input sequence to a (left-to-right) auto-regressive model. At the end of the sequence, the model is tasked to make predictions on test examples sampled from both \(^{}\) and \(^{}\); we assume a single test example for each task (hence, without index): \((^{^{}},y^{^{}})\) and \((^{^{}},y^{^{}})\) respectively; which we simply denote as \((_{}^{},y_{}^{})\) and \((_{}^{},y_{}^{})\) instead.

Our model is a self-referential NN that modifies its own weight matrices as a function of input observations. To simplify the notation, we denote the _state_ of our self-referential NN as a _single_ SRWM \(_{*}\) (even though it may have many of them in practice) where we'll replace \(*\) by various symbols representing the context/inputs it has observed. Given a training sequence \((_{1}^{},y_{1}^{}),...,(_{L}^{ },y_{L}^{}),(_{1}^{},y_{1}^{}),...,(_{L}^{},y_{L}^{})\), our model auto-regressively consumes one input at a time, from left to right, in the auto-regressive fashion. Let \(_{}\) denote the state of the SRWM that has consumed the first part of the sequence, i.e., the examples from Task \(\), \((_{1}^{},y_{1}^{}),...,(_{L}^{},y _{L}^{})\), and let \(_{,}\) denote the state of our SRWM having observed the entire sequence.

**ACL Meta-Training Objectives.** The ACL meta-training objective function tasks the model to correctly predict the test examples of all tasks learned so far at each task boundaries. That is, in the case of two-task scenario described above (learning Task \(\) then Task \(\)), we use the weight matrix \(_{}\) to predict the label \(y_{}^{}\) from input \((_{}^{},)\), and we use the weight matrix \(_{,}\) to predict the label \(y_{}^{}\) from input \((_{}^{},)\)_as well as_ the label \(y_{}^{}\) from input \((_{}^{},)\). By letting \(p(y|;_{*})\) denote the model's output probability for label \(y\{1,..,N\}\) given input \(\) and model weights/state \(_{*}\), the ACL objective can be expressed as:

\[}-(p(y_{}^{} |_{}^{};_{}))+(p(y_{}^{}|_{}^{};_{, }))+(p(y_{}^{}|_{}^{ };_{,}))\] (4)

for an arbitrary input meta-training sequence \((_{1}^{},y_{1}^{}),...,(_{L}^{ },y_{L}^{}),(_{1}^{},y_{1}^{}),...,(_{L}^{},y_{L}^{})\) (which is extensible to mini-batches with multiple such sequences), where \(\) denotes the model parameters (for the SRWM layer, it is the initial weights \(_{0}\)). Figure 1 illustrates the overall meta-training process of ACL.

The ACL objective function above (Eq. 4) is simple but encapsulates desiderata for continual learning (Sec. 2.1). The last term of Eq. 4 with \(p(y_{}^{A}|_{}^{A};_{A,})\) or schematically \((^{}|,)\), optimizes for _backward transfer_: (1) remembering the first task \(\) after learning \(\) (combatting catastrophic forgetting), and (2) leveraging learning of \(\) to improve performance on the past task \(\). The second term of Eq. 4, \(p(y_{}^{}|_{}^{};_{A, })\) or schematically \((^{}|,)\), optimizes _forward transfer_ leveraging the past learning experience of \(\) to improve predictions in the second task \(\), in addition to simply learning to solve Task \(\) from the corresponding training examples. To complete, the first term of Eq. 4 is the single-task meta-learning objective for Task \(\).

**Overall Model Architecture.** As we mention in Sec. 2, in our NN architecture, the core sequential dynamics of CL are learned by the self-referential layers. However, as an image-processing NN, our model makes use of a vision backend. We use the "Conv-4" architecture  (typically used in the context of few-shot learning) in all our experiments, except in the last one where we use a pre-trained vision Transformer . Overall, the model takes an image as input, process it through a feedforward vision NN, whose output is fed to the SRWM-layer block. Note that this is one of the limitations of this work: more general ACL should also learn to modify the vision components.2

Another crucial architectural choice that is specific to continual/multi-task image processing is normalization layers (see also Bronskill et al. ). Typical NNs used in few-shot learning (e.g., Vinyals et al. ) contain batch normalization (BN; ) layers. All our models use instance normalization (IN; ) instead of BN because in our preliminary experiments, we expectably found IN to generalize much better than BN layers in the CL setting.

## 4 Experiments

### Two-Task Setting: Comprehensible Study

We first reveal the problem of "in-context catastrophic forgetting" and show how our ACL method (Sec. 3) can overcome it. As a minimum setting for this, we focus on the two-task "domain

    & & &  \\  Meta-Training Tasks & & & A & A \(\) B & B & B \(\) A \\  Task A & Task B & ACL & A & B & A & B & A & B \\  Omniglot & Mini-ImageNet & No & 97.6 \(\) 0.2 & 52.8 \(\) 0.7 & 22.9 \(\) 0.7 & 52.1 \(\) 0.8 & 97.8 \(\) 0.3 & 20.4 \(\) 0.6 \\  & & Yes & 98.3 \(\) 0.2 & 54.4 \(\) 0.8 & **98.2**\(\) 0.2 & 54.8 \(\) 0.9 & 98.0 \(\) 0.3 & **54.6**\(\) 1.0 \\  FC100 & Mini-ImageNet & No & 49.7 \(\) 0.7 & 55.0 \(\) 1.0 & 21.3 \(\) 0.7 & 55.1 \(\) 0.6 & 49.9 \(\) 0.8 & 21.7 \(\) 0.8 \\  & & Yes & 53.8 \(\) 1.7 & 52.5 \(\) 1.2 & **46.2**\(\) 1.3 & 59.9 \(\) 0.7 & 45.5 \(\) 0.9 & **53.0**\(\) 0.6 \\   

Table 1: 5-way classification accuracies using 15 (meta-test training) examples for each class in the context. Each row is a single model. **Bold** numbers highlight cases where in-context catastrophic forgetting is avoided through ACL.

    & & &  \\  Meta-Training Tasks & & & MNIST & MNIST \(\) CIFAR-10 & CIFAR-10 & CIFAR-10 \(\) MNIST \\  Task A & Task B & ACL & MNIST & CIFAR-10 & MNIST & CIFAR-10 & MNIST & CIFAR-10 \\  Omniglot & Mini-ImageNet & No & 71.1 \(\) 4.0 & 49.4 \(\) 2.4 & 43.7 \(\) 2.3 & 51.5 \(\) 1.4 & 68.9 \(\) 4.1 & 24.9 \(\) 3.2 \\  & & Yes & 75.4 \(\) 3.0 & 50.8 \(\) 1.3 & **81.5**\(\) 2.7 & 51.6 \(\) 1.3 & 77.9 \(\) 2.3 & **51.8**\(\) 2.0 \\  FC100 & Mini-ImageNet & No & 60.1 \(\) 2.0 & 56.1 \(\) 2.3 & 17.2 \(\) 3.5 & 54.4 \(\) 1.7 & 58.6 \(\) 1.6 & 21.2 \(\) 3.1 \\  & & Yes & 70.0 \(\) 2.4 & 51.0 \(\) 1.0 & **68.2**\(\) 2.7 & 59.2 \(\) 1.7 & 66.9 \(\) 3.4 & **52.5**\(\) 1.3 \\   

Table 2: Similar to Table 1 above but using MNIST and CIFAR-10 (unseen domains) for meta-testing.

incremental" CL setting (see Appendix A.1). We consider two meta-training task combinations: Omniglot  and Mini-ImageNet [21; 22] or FC100  (which is based on CIFAR100 ) and Mini-ImageNet. The order of appearance of two tasks within meta-training sequences is alternated for every batch. Appendix A.2 provides further details. We compare systems trained with or without the backward transfer term in the ACL loss (the last term in Eq. 4).

Unless otherwise indicated (e.g, later for classic Split-MNIST; Sec. 4.3), all tasks are configured to be a 5-way classification task. This is one of the classic configurations for few-shot learning tasks, and also allows us to evaluate the principle of ACL with reasonable computational costs (like any sequence learning-based meta-learning methods, scaling this to many more classes is challenging; we also discuss this in Sec. 5). For standard datasets such as MNIST, we split the dataset into sub-datasets of disjoint classes : for example for MNIST which is originally a 10-way classification task, we split it into two 5-way tasks, one consisting of images of class '0' to '4' ('MNIST-04'), and another one made of class '5' to '9' images ('MNIST-59'). When we refer to a dataset without specifying the class range, we refer to the first sub-set. Unless stated otherwise, we concatenate 15 examples from each class for each task in the context for both meta-training and meta-testing (resulting in sequences of length 75 for each task). All images are resized to \(32 32\)-size \(3\)-channel images, and normalized according to the original dataset statistics. We refer to Appendix A for further details.

Table 1 shows the results when the models are meta-tested on the test sets of the corresponding few-shot learning datasets used for meta-training. We observe that for both pairs of meta-training tasks, the models without the ACL loss _catastrophically forget_ the first task after learning the second one: the accuracy on the first task is at the chance level of about 20% for 5-way classification after learning the second task in-context (see rows with "ACL No"). The ACL loss clearly addresses this problem: the ACL-learned CL algorithms preserve the performance of the first task. This effect is particularly pronounced in the Omniglot/Mini-ImageNet case (involving two very different domains).

Table 2 shows evaluations of the same models but using two standard datasets, 5-way MNIST and CIFAR-10, for meta-testing. Again, ACL-trained models better preserve the memory of the first task after learning the second one. In the Omniglot/Mini-ImageNet case, we even observe certain positive backward tranfer effects: in particular, in the "MNIST-then-CIFAR10" continual learning case, the performance on MNIST noticeably improves after learning CIFAR10 (possibly leveraging'more data' provided in-context).

### Analysis: Emergence of In-Context Catastrophic Forgetting

Now we closely look at the emergence of "in-context catastrophic forgetting" during meta-training for the baseline models trained **without** the backward transfer term (the last/third term in Eq. 4) in

Figure 2: **ACL/No**-case meta-training curves displaying 6 individual meta-training loss terms, when the last term of the ACL objective (the backward tranfer loss; “_Task A ACL bwd_” and “_Task B ACL bwd_” in the legend) is **not** minimized (**ACL/No** case in Tables 1 and 2). Here Task A is Omniglot and Task B is Mini-ImageNet. We observe that, in both cases, without explicit minimization, backward transfer capability (_purple_ and _brown_ curves) of the learned learning algorithm gradually degrades as it learns to learn a new task (all other colors), causing in-context catastrophic forgetting. Note that _blue/orange_ and _green/red_ curve pairs almost overlap; indicating that when a task is learned, the model can learn it whether it is in the first or second segment of the continual learning sequence.

[MISSING_PAGE_FAIL:7]

incremental setting, while it largely outperforms them (all but another meta-CL method, GeMCL) in the 2-task class-incremental setting. The same model can be further meta-finetuned using the 5-task version of the ACL loss (here we only used Omniglot as the meta-training data). The resulting model (the last row of Table 3) outperforms all other methods in all settings studied here. Note that on the 'in-domain' Omniglot test set, ACL and GeMCL perform similarly (see Appendix B.2/Table 9). We are not aware of any existing hand-crafted CL algorithms that can achieve ACL's performance without any replay memory. We refer to Appendix A.7/B for further discussions and ablation studies.

**Evaluation on diverse task domains.** Using the setting of Sec. 4.1, we also evaluate our ACL-trained models for CL involving more tasks/domains; using meta-test sequences made of MNIST, CIFAR-10, and Fashion MNIST. We also evaluate the impact of the number of tasks in the ACL objective: in addition to the model meta-trained on Omniglot/Mini-ImageNet (Sec. 4.1), we also meta-train a model (with the same architecture and hyper-parameters) using 3 tasks, Omniglot, Mini-ImageNet, and FC100, using the 3-task ACL objective (see Appendix A.5); which is meta-trained not only on longer CL sequences but also on more data. The full results of this experiment can be found in Appendix B.4. We find that the two ACL-trained models are indeed capable of retaining the knowledge without catastrophic forgetting for multiple tasks during meta-testing, while the performance on prior tasks gradually degrades as the model learns new tasks, and performance on new tasks becomes moderate (see also Sec. 5 on limitations). The 3-task version outperforms the 2-task one overall, encouragingly indicating a potential for further improvements even with a fixed parameter count.

**Going beyond: limitations and outlook.** The experiments presented above effectively demonstrate the possibility to encode a continual learning algorithm into self-referential weight matrices, that outperforms handcrafted learning algorithms and existing metalearning approaches for CL. While we consider this as an important result for metalearning and in-context learning in general, we note that current state-of-the-art CL methods use neither regularization-based CL algorithms nor meta-continual learning methods we mention above, but the so-called _learning to prompt_ (L2P)-family of methods  that leverage pre-trained models, namely a vision Transformer (ViT) pre-trained on ImageNet . A natural question we should ask is whether we could foresee ACL beyond the scope considered so far, and evaluate it in such a setting. To study this, we take a pre-trained (frozen) vision model, and add self-referential layers (to be meta-trained from scratch) on top of it to build a continual learner. This allows us to highlight an important challenge of in-context CL in what follows.

We use two tasks from the L2P works above : 5-datasets  and Split-CIFAR-100, in the class-incremental setting, but we focus on a _"mini"_ versions thereof: we only use the two first classes within each task (i.e., _2-way_ version) and for Split-CIFAR100, we only use the 5 first tasks; as we'll see, this setting is enough to illustrate an important limitation of in-context CL. Again following L2P , we use ViT-B/16  (available via PyTorch) as the pre-trained vision model, which we keep frozen. We use the same configuration for the self-referential component from the Split-MNIST experiment. We meta-train the resulting model using Mini-ImageNet and Omniglot with the 5-task ACL loss. Table 4 shows the results. Even in this simple "mini" version of the tasks, ACL's performance is far behind that of L2P methods. Notably, the frozen ImageNet-pre-trained features with the meta-learner trained on Mini-ImageNet and Omniglot are not enough to perform well on the 5-th task of Split-CIFAR100, and SVHN and notMNIST of 5-datasets. This shows the necessity to meta-train on more diverse tasks for in-context CL to be possibly successful in more general settings.

    &  &  \\  L2P  & _83.9*_\(\) 0.3 & _81.1*_\(\) 0.9 \\ DualPrompt  & _86.5*_\(\) 0.3 & _88.1*_\(\) 0.4 \\    & Task 1 & 95.9 \(\) 0.9 & CIFAR10 & 91.3 \(\) 1.2 \\  & Task 2 & 85.6 \(\) 3.6 & MNIST & 98.9 \(\) 0.3 \\  & Task 3 & 93.4 \(\) 1.4 & Fashion & 93.5 \(\) 2.0 \\  & Task 4 & 97.0 \(\) 0.7 & SVHN & 66.1 \(\) 9.4 \\  & Task 5 & 67.6 \(\) 7.0 & notMNIST & 76.3 \(\) 6.7 \\   &  &  \\    & & & \\ 

Table 4: Experiments with “_mini_” Split-CIFAR100 and 5-datasets tasks. Meta-training is done using **Mini-ImageNet** and **Omniglot**. All meta-evaluation images are therefore from unseen domains. Numbers marked with * are _reference_ numbers (evaluated in the more challenging, original version of these tasks) which can not be directly compared to ours.

Discussion

**Other Limitations.** In addition to the limitations already mentioned above, here we discuss others. First of all, as an in-context/learned learning algorithm, there are challenges in terms of both domain and length generalization (we qualitatively observe these to some extent in Sec. 4; further discussion and experimental results are presented in Appendix B.3 & B.5). Regarding the length generalization, we note that unlike the standard "quadratic" Transformers, linear Transformers/FWPs-like SRWMs can be trained by _carrying over states_ across two consecutive batches for arbitrarily long sequences. Such an approach has been successfully applied to language modeling with FWPs . This possibility, however, has not been investigated here, and is left for future work. Also, directly scaling ACL for real-world tasks requiring many more classes does not seem straightforward: it would require very long training sequences. That said, it may be possible that ACL could be achieved without exactly following the process we propose; as we discuss below for the case of LLMs, certain real-world data may naturally give rise to an ACL-like objective. This work is also limited to the task of image classification, which can be solved by feedforward NNs. Future work may investigate the possibility to extend ACL to continual learning of sequence learning tasks, such as continually learning new languages. Finally, ACL learns CL algorithms that are specific to the pre-specified model architecture; more general meta-learning algorithms may aim at achieving learning algorithms that are applicable to any model, as is the case for many classic learning algorithms.

**Related work.** There are several recent works that are catagorized as'meta-continual learning' or 'continual meta-learning' (see, e.g., [29; 82; 83; 84; 851]). For example, Javed and White , Beaulieu et al.  use "model-agnostic meta-learning" (MAML; [85; 86]) to meta-learn _representations_ for CL while still making use of classic learning algorithms for CL; this requires tuning of the learning rate and number of iterations for optimal performance during CL at meta-test time (see, e.g., Appendix A.7). In contrast, our approach learn _learning algorithms_ in the spirit of Hochreiter et al. , Younger et al. ; this may be categorized as 'in-context continual learning.' Several recent works (see, e.g., [88; 87]) mention the possibility of such in-context CL but existing works [19; 89; 90] that learn multiple tasks sequentially in-context do not focus on catastrophic forgetting which is one of the central challenges of CL. Here we show that in-context learning also suffers from catastrophic forgetting in general (Sec. 4.1-4.2) and propose ACL to address this problem. We also note that the use of SRWM is relevant to 'continual meta-learning' since with a regular sequence processor with slow weights, there remains the question of how to continually learn the slow weights (meta-parameters). In principle, recursive self-modification as in SRWM is an answer to this question as it collapses such meta-levels into single self-reference . We also refer to [91; 92; 93] for other prior work on meta-continual learning.

**Artificial v. Natural ACL in Large Language Models?** Recently, "on-the-fly" few-shot/meta learning capability of sequence processing NNs has attracted broader interests in the context of large language models (LLMs; ). In fact, the task of language modeling itself has a form of _sequence processing with error feedback_ (essential for meta-learning ): the correct label to be predicted is fed to the model with a delay of one time step in an auto-regressive manner. Trained on a large amount of text covering a wide variety of credit assignment paths, LLMs exhibit certain sequential few-shot learning capabilities in practice . This was rebranded as _in-context learning_, and has been the subject of numerous recent studies (e.g., [97; 98; 99; 100; 101; 72]). Here we explicitly/artificially construct ACL meta-training sequences and objectives, but in modern LLMs trained on a large amount of data mixing a large diversity of dependencies using a large backpropagation span, it is conceivable that some ACL-like objectives may naturally appear in the data.

## 6 Conclusion

Our Automated Continual Learning (ACL) trains sequence-processing self-referential neural networks (SRNNs) to learn their own in-context continual (meta-)learning algorithms. ACL encodes classic desiderata for continual learning (e.g., forward and backward transfer) into the objective function of the meta-learner. ACL uses gradient descent to deal with classic challenges of CL, to automatically discover CL algorithms with good behavior. Once trained, our SRNNs autonomously run their own CL algorithms without requiring any human intervention. Our experiments reveal the original problem of in-context catastrophic forgetting, and demonstrate the effectiveness of the proposed approach to combat it. We demonstrate very promising results on the classic Split-MNIST benchmark where existing hand-crafted algorithms fail, while also discussing its limitations in more general scenarios. We believe this comprehensive study to be an important step for in-context CL research.