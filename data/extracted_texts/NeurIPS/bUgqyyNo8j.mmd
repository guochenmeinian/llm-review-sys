# Model-Based Reparameterization Policy Gradient Methods: Theory and Practical Algorithms

Shenao Zhang\({}^{1}\) &Boyi Liu\({}^{1}\) &Zhaoran Wang\({}^{1}\) &Tuo Zhao\({}^{2}\)

\({}^{1}\)Northwestern University \({}^{2}\)Georgia Tech

shenao@u.northwestern.edu

###### Abstract

ReParameterization (RP) Policy Gradient Methods (PGMs) have been widely adopted for continuous control tasks in robotics and computer graphics. However, recent studies have revealed that, when applied to long-term reinforcement learning problems, model-based RP PGMs may experience chaotic and non-smooth optimization landscapes with exploding gradient variance, which leads to slow convergence. This is in contrast to the conventional belief that reparameterization methods have low gradient estimation variance in problems such as training deep generative models. To comprehend this phenomenon, we conduct a theoretical examination of model-based RP PGMs and search for solutions to the optimization difficulties. Specifically, we analyze the convergence of the model-based RP PGMs and pinpoint the smoothness of function approximators as a major factor that affects the quality of gradient estimation. Based on our analysis, we propose a spectral normalization method to mitigate the exploding variance issue caused by long model unrolls. Our experimental results demonstrate that proper normalization significantly reduces the gradient variance of model-based RP PGMs. As a result, the performance of the proposed method is comparable or superior to other gradient estimators, such as the Likelihood Ratio (LR) gradient estimator. Our code is available at https://github.com/agenification/RP_PGM.

## 1 Introduction

Reinforcement Learning (RL) has seen tremendous success in a variety of sequential decision-making applications, such as strategy games  and robotics , by identifying actions that maximize long-term accumulated rewards. As one of the most popular methodologies, the policy gradient methods (PGM)  seek to search for the optimal policy by iteratively computing and following a stochastic gradient direction with respect to the policy parameters. Therefore, the quality of the stochastic gradient estimation is essential for the effectiveness of PGMs.

Two main categories have emerged in the realm of stochastic gradient estimation: (1) Likelihood Ratio (LR) estimators, which perform zeroth-order estimation through the sampling of function evaluations , and (2) ReParameterization (RP) gradient estimators, which harness the differentiability of the function approximation . Despite the wide adoption of both LR and RP PGMs in practice, the majority of the literature on the theoretical properties of PGMs focuses on LR PGMs. The optimality and approximation error of LR PGMs have been heavily investigated under various settings . Conversely, the theoretical underpinnings of RP PGMs remain to be fully explored, with a dearth of research on the quality of RP gradient estimators and the convergence of RP PGMs.

RP gradient estimators have established themselves as a reliable technique for training deep generative models such as variational autoencoders . From a stochastic optimization perspective, previousstudies [48; 40] have shown that RP gradient methods enjoy small variance, which leads to better convergence and performance. However, recent research [42; 37] has reported an opposite observation: When applied to long-horizon reinforcement learning problems, model-based RP PGMs tend to encounter chaotic optimization procedures and highly non-smooth optimization landscapes with exploding gradient variance, causing slow convergence.

Such an intriguing phenomenon inspires us to delve deeper into the theoretical properties of RP gradient estimators in search of a remedy for the issue of exploding gradient variance in model-based RP PGMs. To this end, we present a unified theoretical framework for the examination of model-based RP PGMs and establish their convergence results. Our analysis implies that the smoothness and accuracy of the learned model are crucial determinants of the exploding variance of RP gradients: (1) both the gradient variance and bias exhibit a polynomial dependence on the Lipschitz continuity of the learned model and policy w.r.t. the input state, with degrees that increase linearly with the steps of model value expansion, and (2) the bias also depends on the error of the estimated model and value.

Our findings suggest that imposing smoothness on the model and policy can greatly decrease the variance of RP gradient estimators. To put this discovery into practice, we propose a spectral normalization method to enforce the smoothness of the learned model and policy. It's worth noting that this method can enhance the algorithm's efficiency without substantially compromising accuracy when the underlying transition kernel is smooth. However, if the transition kernel is not smooth, enforcing smoothness may lead to increased error in the learned model and introduce bias. In such cases, a balance should be struck between model bias and gradient variance. Nonetheless, our empirical study demonstrates that the reduced gradient variance when applying spectral normalization leads to a significant performance boost, even with the cost of a higher bias. Furthermore, our results highlight the potential of investigating model-based RP PGMs, as they demonstrate superiority over other model-based and Likelihood Ratio (LR) gradient estimator alternatives.

## 2 Background

**Reinforcement Learning.** We consider learning to optimize an infinite-horizon \(\)-discounted Markov Decision Process (MDP) over repeated episodes of interaction. We denote by \(^{d_{s}}\) and \(^{d_{a}}\) the state and action space, respectively. When taking an action \(a\) at a state \(s\), the agent receives a reward \(r(s,a)\) and the MDP transits to a new state \(s^{}\) according to \(s^{} f(\,|\,s,a)\).

We aim to find a policy \(\) that maps a state to an action distribution to maximize the expected cumulative reward. We denote by \(V^{}:\) and \(Q^{}:\) the state value function and the state-action value function associated with \(\), respectively, which are defined as follows,

\[Q^{}(s,a)=(1-)_{,f}_{i=0}^{} ^{i} r(s_{i},a_{i})\,\,s_{0}=s,a_{0}=a, V^{ }(s)=_{a}Q^{}(s,a).\]

Here \(s\), \(a\), and the expectation \(_{,f}[\,\,]\) is taken with respect to the dynamic induced by the policy \(\) and the transition probability \(f\). We denote by \(\) the initial state distribution. Under policy \(\), the state and state-action violation measure \(_{}(s)\) over \(\) and \(_{}(s,a)\) over \(\) are defined as

\[_{}(s)=(1-)_{i=0}^{}^{i}(s_{ i}=s),_{}(s,a)=(1-)_{i=0}^{}^{i} (s_{i}=s,a_{i}=a),\]

where the summations are taken with respect to the trajectory induced by \(s_{0}\), \(a_{i}(\,|\,s_{i})\), and \(s_{i+1} f(\,|\,s_{i},a_{i})\). The objective \(J()\) of RL is defined as the expected policy value as follows,

\[J()=_{s_{0}}V^{}(s_{0})=_{(s,a)_{}}r(s,a).\] (2.1)

**Stochastic Gradient Estimation.** The underlying problem of policy gradient, i.e., computing the gradient of an expectation with respect to the parameters of the sampling distribution, takes the form \(_{}_{p(x;)}[y(x)]\). To restore the RL objective, we can set \(p(x;)\) as the trajectory distribution conditioned on the policy parameter \(\) and \(y(x)\) as the cumulative reward. In the sequel, we introduce two commonly used gradient estimators.

Likelihood Ratio (LR) Gradient (Zeroth-Order): By leveraging the _score function_, LR gradients only require samples of the function values. Since \(_{} p(x;)=_{}p(x;)/p(x;)\), the LR gradient is

\[_{}_{p(x;)}y(x)=_{p(x; )}y(x)_{} p(x;).\] (2.2)ReParameterization (RP) Gradient (First-Order): RP gradient benefits from the structural characteristics of the objective, i.e., how the overall objective is affected by the operations applied to the sources of randomness as they pass through the measure and into the cost function . From the simulation property of continuous distribution, we have the following equivalent sampling processes:

\[ p(x;)=g(,),\,  p(),\] (2.3)

which states that an alternative way to generate a sample \(\) from the distribution \(p(x;)\) is to sample from a simpler base distribution \(p()\) and transform this sample using a deterministic function \(g(,)\). Derived from the _law of the unconscious statistician_ (LOTUS) , i.e., \(_{p(x;)}[y(x)]=_{p()}[y(g(;))]\), the RP gradient can be formulated as \(_{}_{p(x;)}[y(x)]=_{p()}[ _{}y(g(;))]\).

## 3 Analytic Reparameterization Gradient in Reinforcement Learning

In this section, we present two fundamental _analytic_ forms of the RP gradient in RL. We first consider the Policy-Value Gradient (PVG) method, which is model-free and can be expanded sequentially to obtain the Analytic Policy Gradient (APG) method. Then we discuss potential obstacles that may arise when developing practical algorithms.

We consider a policy \(_{}(s,)\) with noise \(\) in continuous action spaces. To ensure that the first-order gradient through the value function is well-defined, we make the following continuity assumption.

**Assumption 3.1** (Continuous MDP).: We assume that \(f(s^{}\,|\,s,a)\), \(_{}(s,)\)1, \(r(s,a)\), and \(_{a}r(s,a)\) are continuous in all parameters and variables \(s\), \(a\), \(s^{}\).

Policy-Value Gradient.: The reparameterization PVG takes the following general form,

\[_{}J(_{})=_{s, p} _{}Q^{_{}}s,_{}(s,) .\] (3.1)

In sequential decision-making, any immediate action could lead to changes in all future states and rewards. Therefore, the value gradient \(_{}Q^{_{}}\) possesses a recursive structure. Adapted from the deterministic policy gradient theorem [52; 34] by considering stochasticity, we rewrite (3.1) as

\[_{}J(_{})=_{s_{},} _{}_{}(s,)_{a}Q^{_{}}(s,a) _{a=_{}(s,)}.\]

Here, \(_{a}Q^{_{}}\) can be estimated using a critic, which leads to model-free frameworks [25; 4]. Notably, as a result of the recursive structure of \(_{}Q^{_{}}\), the expectation is taken over the state visitation \(_{}\) instead of the initial distribution \(\).

By sequentially expanding PVG, we obtain the analytic representation of the policy gradient.

Analytic Policy Gradient.From the Bellman equation \(V^{_{}}(s)=_{}[(1-)r(s,_{}(s, ))+_{^{*}}[V^{_{}}(f(s,_{}(s, ),^{*}))]]\), we obtain the following backward recursions:

\[_{}V^{_{}}(s) =_{}(1-)_{a}r_{ }_{}+_{^{*}}_{s^{}}V^{ _{}}(s^{})_{a}f_{}_{}+_{ }V^{_{}}(s^{}),\] (3.2) \[_{s}V^{_{}}(s) =_{}(1-)(_{s}r+_{a}r _{s}_{})+_{^{*}}_{s^{}}V^ {_{}}(s^{})(_{s}f+_{a}f_{s}_{}) .\] (3.3)

See SSA for detailed derivations of (3.2) and (3.3). Now we have the RP gradient backpropagated through the transition path starting at \(s\). By taking an expectation over the initial state distribution, we obtain the Analytic Policy Gradient (APG) \(_{}J(_{})=_{s}[_{}V^{_ {}}(s)]\).

There remain challenges when developing practical algorithms: (1) the above formulas require the gradient information of the transition function \(f\). In this work, however, we consider a common RL setting where \(f\) is unknown and needs to be fitted by a model. It is thus natural to ask how the properties of the model (e.g., prediction accuracy and model smoothness) affect the gradient estimation and the convergence of the resulting algorithms, and (2) even if we have access to an accurate model, unrolling it over full sequences faces practical difficulties. The memory and computational cost scale linearly with the unroll length. Long chains of nonlinear mappings can also lead to exploding or vanishing gradients and even worse, chaotic phenomenons  and difficulty in optimization [43; 36; 58; 38]. These difficulties demand some form of truncation when performing RP PGMs.

Model-Based RP Policy Gradient Methods

Through the application of Model Value Expansion (MVE) for model truncation, this section unveils two RP policy gradient frameworks constructed upon MVE.

### \(h\)-Step Model Value Expansion

To handle the difficulties inherent in full unrolls, many algorithms employ direct truncation, where the long sequence is broken down into short sub-sequences and backpropagation is applied accordingly, e.g., Truncated BPTT . However, such an approach over-prioritizes short-term dependencies, which leads to biased gradient estimates.

In model-based RL (MBRL), one viable solution is to adopt the \(h\)-step Model Value Expansion , which decomposes the value estimation \(^{}(s)\) into the rewards gleaned from the learned model and a residual estimated by a critic function \(_{}\), that is,

\[^{_{}}(s)=(1-)_{i=0}^{h-1}^{ i} r(_{i},_{i})+^{h}_{}( _{h},_{h}),\]

where \(_{0}=s\), \(_{i}=_{}(_{i},)\), and \(_{i+1}=_{}(_{i},_{i},)\). Here, the noise variables \(\) and \(\) can be sampled from the fixed distributions or inferred from the real samples, which we now discuss.

### Model-Based RP Gradient Estimation

Utilizing the pathwise gradient with respect to \(\), we present the following two frameworks.

**Model Derivatives on Predictions (DP).** A straightforward way to compute the first-order gradient is to link the reward, model, policy, and critic together and backpropagate through them. Specifically, the differentiation is carried out on the trajectories simulated by the model \(_{}\), which serves as a tool for _both_ the prediction of states and the evaluation of derivatives. The corresponding RP-DP estimator of gradient \(_{}J(_{})\) is denoted as \(^{}_{}J(_{})\), which takes the form of

\[^{}_{}J(_{})=_{n=1}^{ N}_{}_{i=0}^{h-1}^{i} r(_{i,n}, _{i,n})+^{h}_{}(_{h,n}, _{h,n}),\] (4.1)

where \(_{0,n}_{_{}}\), \(_{i,n}=_{}(_{i,n},_{n})\), and \(_{i+1,n}=_{}(_{i,n},_{i,n}, _{n})\) with noises \(_{n} p()\) and \(_{n} p()\). Here, \(_{_{}}\) is the distribution where the initial states of the simulated trajectories are sampled. In Section 5, we study a general form of \(_{_{}}\) that is a mixture of the initial state distribution \(\) and the state visitation \(_{_{}}\).

Various algorithms can be instantiated from (4.1) with different choices of \(h\). When \(h=0\), the framework reduces to model-free policy gradients, such as RP\((0)\) and the variants of DDPG , e.g., SAC . When \(h\), the resulting algorithm is BPTT [22; 13; 6] where only the model is learned. Recent model-based approaches, such as MAAC  and related algorithms [42; 4; 33], require a carefully selected \(h\).

**Model Derivatives on Real Samples (DR).** An alternative approach is to use the learned differentiable model solely for the calculation of derivatives, with the aid of Monte-Carlo estimates obtained from _real_ samples. By replacing \(_{a}f,_{s}f\) in (3.2)-(3.3) with \(_{a}_{},_{a}_{}\) and setting the termination of backpropagation at the \(h\)-th step as \(V^{_{}}(_{h,n})=_{ }(_{h,n})\), we are able to derive a dynamic representation of \(_{}V^{_{}}\), which we defer to SSA. The corresponding RP-DR gradient estimator is

\[^{}_{}J(_{})=_{n=1}^{N }_{}V^{_{}}(_{0,n}),\] (4.2)

where \(_{0,n}_{_{}}\). Equation (4.2) can be specified as (A.9), which is in the same format as (4.1), but with the noise variables \(_{n}\), \(_{n}\) inferred from the real data sample \((s_{i},a_{i},s_{i+1})\) via the relation \(a_{i}=_{}(s_{i},_{n})\) and \(s_{i+1}=_{}(s_{i},a_{i},_{n})\) (see SSA for details). Algorithms such as SVG  and its variants [1; 5] are examples of this RP-DR method.

### Algorithmic Framework

The pseudocode of model-based RP PGMs is presented in Algorithm 1, where three update procedures are performed iteratively. In other words, the policy, model, and critic are updated at each iteration \(t[T]\), generating sequences of \(\{_{_{t}}\}_{t[T+1]}\), \(\{_{_{t}}\}_{t[T]}\), and \(\{_{_{t}}\}_{t[T]}\), respectively.

```
0: Number of iterations \(T\), learning rate \(\), batch size \(N\), empty dataset \(\)
1:for iteration \(t[T]\)do
2: Update the model parameter \(_{t}\) by MSE or MLE
3: Update the critic parameter \(_{t}\) by performing Temporal Difference
4: Sample states from \(_{_{t}}\) and estimate \(_{}J(_{_{t}})=_{}^{ }J(_{_{t}})\) (4.1) or \(_{}^{}J(_{_{t}})\) (4.2)
5: Update the policy parameter \(_{t}\) by \(_{t+1}_{t}+_{}J(_{ _{t}})\)
6: Execute \(_{_{t+1}}\) and save data to \(\) to obtain \(_{t+1}\)
7:endfor ```

**Algorithm 1** Model-Based Reparameterization Policy Gradient

**Policy Update.** The update rule for the policy parameter \(\) with learning rate \(\) is as follows,

\[_{t+1}_{t}+_{}J(_{ _{t}}),\] (4.3)

where \(_{}J(_{_{t}})\) can be specified as \(_{}^{}J(_{_{t}})\) or \(_{}^{}J(_{_{t}})\).

**Model Update.** By predicting the mean of transition with minimized mean squared error (MSE) or fitting a probabilistic model with maximum likelihood estimation (MLE), e.g., \(_{t}=*{argmax}_{}_{_{t}}[ _{}(s_{i+1}|s_{i},a_{i})]\), canonical MBRL methods learn forward models that predict how the system evolves when an action is taken at a state.

However, accurate state predictions do not imply accurate RP gradient estimation. Thus, we define \(_{f}(t)\) to denote the model (gradient) error at iteration \(t\):

\[_{f}(t)=_{i[h]}\,_{(s_{i},a_{i}),(_{i},_{i})}}{  s_{i-1}}-_{i}}{_{i-1}} _{2}+}{ a_{i-1}}-_{i}}{_{i-1}}_{2},\] (4.4)

where \((s_{i},a_{i})\) is the true state-action distribution at the \(i\)-th timestep by following \(s_{0}_{_{_{t}}}\), \(a_{j}_{_{t}}(\,|\,s_{j})\), \(s_{j+1} f(\,|\,s_{j},a_{j})\), with policy and transition noise sampled from a fixed distribution. Similarly, \((_{i},_{i})\) is the model rollout distribution at the \(i\)-th timestep by following \(_{0}_{_{_{t}}}\), \(_{j}_{_{t}}(\,|\,_{j})\), \(_{j+1}_{}(\,|\,_{j},_ {j})\), where the noise is sampled when we use RP-DP gradient estimator and is inferred from real samples when we use RP-DR gradient estimator (in this case \((_{i},_{i})=(s_{i},a_{i})\)).

In MBRL, it is common to learn a state-predictive model that can make multi-step predictions. However, this presents a challenge in reconciling the discrepancy between minimizing state prediction error and the gradient error of the model. Although it is natural to consider regularizing the models' directional derivatives to be consistent with the samples , we contend that the use of state-predictive models does _not_ cripple our analysis of gradient bias based on \(_{f}\): For learned models that extrapolate beyond the visited regions, the gradient error can still be bounded via finite difference. In other words, \(_{f}\) can be expressed as the mean squared training error with an additional measure of the model class complexity to capture its generalizability. This same argument can also be applied to the case of learning a critic through temporal difference.

**Critic Update.** For any policy \(\), its value function \(Q^{}\) satisfies the Bellman equation, which has a unique solution. In other words, \(Q=^{}Q\) if and only if \(Q=Q^{}\). The Bellman operator \(^{}\) is defined for any \((s,a)\) as

\[^{}Q(s,a)=_{,f}(1-) r(s,a)+  Q(s^{},a^{}).\]

We aim to approximate the state-action value function \(Q^{}\) with a critic \(_{}\). Due to the solution uniqueness of the Bellman equation, it can be achieved by minimizing the mean-squared Bellman error \(_{t}=*{argmin}_{}_{_{t}}[ (_{}(s,a)-^{}_{}(s,a))^{2}]\) via Temporal Difference (TD) [55; 10]. We define the critic error at the \(t\)-th iteration as follows,

\[_{v}(t)=^{2}_{(s_{h},a_{h}),( _{h},_{h})}}}}{ s}-_{_{t}}}{ }_{2}+}}}{  a}-_{_{t}}}{} _{2},\] (4.5)where \(=(1-)/^{h}\) and \((s_{h},a_{h})\), \((_{h},_{h})\) are distributions at timestep \(h\) with the same definition as in (4.4). The inclusion of \(^{2}\) ensures that the critic error remains in alignment with the single-step model error \(_{f}\): (1) the critic estimates the tail terms that occur after \(h\) steps in the model expansion, therefore the step-average critic error should be inversely proportional to the tail discount summation \(_{i=h}^{}^{i}=1/\), and (2) the quadratic form shares similarities with the canonical MBRL analysis - the cumulative error of the model trajectories scales linearly with the single-step prediction error and quadratically with the considered horizon (i.e., tail after the \(h\)-th step). This is because the cumulative error is linear in the considered horizon and the maximum state discrepancy, which is linear in the single-step error and, again, the horizon .

## 5 Main Results

In what follows, we present our main theoretical results, whose detailed proofs are deferred to SSB. Specifically, we analyze the convergence of model-based RP PGMs and, more importantly, study the correlation between the convergence rate, gradient bias, variance, smoothness of the model, and approximation error. Based on our theory, we propose various algorithmic designs for MB RP PGMs.

To begin with, we impose a common regularity condition on the policy functions following previous works [68; 46; 69; 3]. The assumption below essentially ensures the smoothness of the objective \(J(_{})\), which is required by most existing analyses of policy gradient methods [60; 6; 2].

**Assumption 5.1** (Lipschitz and Bounded Score Function).: We assume that the score function of policy \(_{}\) is Lipschitz continuous and has bounded norm \((s,a)\), that is,

\[_{_{1}}(a\,|\,s)-_{_{2}}(a\,|\,s)_{2 } L_{1}\|_{1}-_{2}\|_{2},_{}(a \,|\,s)_{2} B_{}.\]

We characterize the convergence of RP PGMs by first providing the following proposition.

**Proposition 5.2** (Convergence to Stationary Point).: We define the gradient bias \(b_{t}\) and variance \(v_{t}\) as

\[b_{t}=_{}J(_{_{t}})-_{}J(_{_{t}})_{2}, v_{t}= _{}J(_{_{t}})-_{}J(_{_{t}})_{2}^{2}.\]

Suppose the absolute value of the reward \(r(s,a)\) is bounded by \(|r(s,a)| r_{}\) for \((s,a)\). Let \(=\|\|_{2}\), \(L=r_{} L_{1}/(1-)^{2}+(1+) r_{} B_{ }^{2}/(1-)^{3}\), and \(c=(-L^{2})^{-1}\). It then holds for \(T 4L^{2}\) that

\[_{t[T]}_{}J(_{_{t}}) _{2}^{2}J(_{_{T }})-J(_{_{1}})+_{t=0}^{T-1}c(2 b _{t}+ v_{t})+b_{t}^{2}+v_{t}.\]

Proposition 5.2 illustrates the interdependence between the convergence and the variance, bias of the gradient estimators. In order for model-based RP PGMs to converge, it is imperative to maintain both the variance and bias at sublinear growth rates. Prior to examining the upper bound of \(b_{t}\) and \(v_{t}\), we make the following Lipschitz assumption, which has been implemented in a plethora of preceding studies [46; 12; 33].

**Assumption 5.3** (Lipschitz Continuity).: We assume that \(r(s,a)\) and \(f(s,a,^{*})\) are \(L_{r}\) and \(L_{f}\) Lipschitz continuous, respectively. Formally, for any \(s_{1},s_{2}\), \(a_{1},a_{2}\), and \(^{*}\),

\[r(s_{1},a_{1})-r(s_{2},a_{2})  L_{r}(s_{1}-s_{2},a_{1}-a_{2})_{2},\] \[f(s_{1},a_{1},^{*})-f(s_{2},a_{2},^{*})_{2}  L_{f}(s_{1}-s_{2},a_{1}-a_{2})_{2}.\]

Let \(_{g}=\{L_{g},1\}\), where \(L_{g}\) is the Lipschitz of function \(g\). We have the following result for gradient variance.

**Proposition 5.4** (Gradient Variance).: Under Assumption 5.3, for any \(t[T]\), the gradient variance of the estimator \(_{}J(_{})\), which can be specified as \(_{}^{}J(_{})\) or \(_{}^{}J(_{})\), can be bounded by

\[v_{t}=Oh^{4}}{1-}^{2} {L}_{}^{4h}_{}^{4h}/N+^{2h}h^{4}_ {}^{4h}_{}^{4h}/N,\]

where \(L_{}=_{,s_{1},s_{2},a_{1},a_{2} }\|_{}(s_{1},a_{1},)-_{}(s_{2},a_{2 },)\|_{2}/\|(s_{1}-s_{2},a_{1}-a_{2})\|_{2}\) and \(L_{}=_{,s_{1},s_{2},}\|_{}(s_{1 },)-_{}(s_{2},)\|_{2}/\|s_{1}-s_{2}\|_{2}\).

We observe that the variance upper bound exhibits a polynomial dependence on the Lipschitz continuity of the model and policy, where the degrees are linear in the model unroll length. This makes sense intuitively, as the transition can be highly chaotic when \(L_{}>1\) and \(L_{}>1\). This can result in diverging trajectories and variable gradient directions during training, leading to significant variance in the gradients.

**Remark 5.5**.: Model-based RP PGMs with non-smooth models and policies can suffer from large variance and highly non-smooth loss landscapes, which can lead to slow convergence or failure during training even in simple toy examples . Proposition 5.4 suggests that one can add smoothness regularization to avoid exploding gradient variance. See our discussion at the end of this section for more details.

_Model-based_ RP PGMs possess unique advantages by utilizing proxy models for variance reduction. By enforcing the smoothness of the model, the gradient variance is reduced without a burden when the underlying transition is smooth. However, in cases of non-smooth dynamics, doing so may introduce additional bias due to increased model estimation error. This necessitates a trade-off between the model error and gradient variance. Nevertheless, our empirical study demonstrates that smoothness regularization improves performance in robotic locomotion tasks, despite the cost of increased bias.

Next, we study the gradient bias. We consider the case where the state distribution \(_{}\), which is used for estimating the RP gradient, is a mixture of the initial distribution \(\) of the MDP and the state visitation \(_{}\). In other words, we consider \(_{}=_{}+(1-)\), where \(\). This form is of particular interest as it encompasses various state sampling schemes that can be employed, such as when \(h=0\) and \(h\): When not utilizing a model, such as in SVG(0)  and DDPG , states are sampled from \(_{}\); while when unrolling the model over full sequences, as in BPTT, states are sampled from the initial distribution.

Given that the effects of policy actions extend to all future states and rewards, unless we know the exact policy value function, its gradient \(_{}Q^{_{}}\) cannot be simply represented by quantities in any finite timescale. Hence, the differentiation of the critic function does not align with the true value gradient that has recursive structures. To tackle this issue, we provide the gradient bias bound that is based on the measure of discrepancy between the initial distribution \(\) and the state visitation \(_{}\).

**Proposition 5.6** (Gradient Bias).: We denote \(=_{}_{_{}}[(/_{} (s))^{2}]^{1/2}\), where \(/_{}\) is the Radon-Nikodym derivative of \(\) with respect to \(_{}\). Let \(^{}=+(1-)\). Under Assumption 5.3, for any \(t[T]\), the gradient bias is bounded by

\[b_{t}=O^{}h^{2}(1-^{h})_{ {f}}^{h}_{}^{h}_{}^{2h}_{f,t} /(1-)+^{}h^{3h}_{}^{h} _{}^{h}_{v,t}/(1-)^{2},\]

where \(_{f,t}\) and \(_{v,t}\) are the shorthand notations of \(_{f}(t)\) defined in (4.4) and \(_{v}(t)\) in (4.5), respectively.

The analysis above yields the identification of an optimal model expansion step \(h^{*}\) that achieves the best convergence rate, whose form is presented by the following proposition.

**Proposition 5.7** (Optimal Model Expansion Step).: Given \(L_{f} 1\), if we regularize the model and policy so that \(L_{} 1\) and \(L_{} 1\), then when \( 1\), the optimal model expansion step \(h^{*}\) at iteration \(t\) that minimizes the convergence rate upper bound satisfies \(h^{*}=\{h^{*},0\}\), where \(h^{*}=O(_{v,t}/((1-)(_{f,t}+_{v,t})))\) scales linearly with \(_{v,t}/(_{f,t}+_{v,t})\) and the effective task horizon \(1/(1-)\).

In Proposition 5.7, the Lipschitz condition of the underlying dynamics, i.e., \(L_{f} 1\), ensures the stability of the system. This can be seen in the linear system example, where the transitions are determined by the eigenspectrum of the family of transformations, leading to exponential divergence of trajectories w.r.t. the largest eigenvalue. In cases where this condition is not met in practical control systems, finding the best model unroll length may require trial and error. Fortunately, we have observed through experimentation that enforcing smoothness offers a much wider range of unrolling lengths that still provide satisfactory results.

**Remark 5.8**.: As the error scale \(_{v,t}/(_{f,t}+_{v,t})\) increases, so too does the value of \(h^{*}\). This finding can inform the practical algorithms to rely more on the model by performing longer unrolls when the model error \(_{f,t}\) is small, while avoiding long unrolls when the critic error \(_{v,t}\) is small.

**A Spectral Normalization Method.** To ensure a smooth transition and faster convergence, we propose using a Spectral Normalization (SN)  model-based RP PGM that applies SN to all layers of the deep model network and policy network. While other techniques, such as adversarial regularization , exist, we focus primarily on SN as it directly regulates the Lipschitz constant of the function. Specifically, the Lipschitz constant \(L_{g}\) of a function \(g\) satisfies \(L_{g}=_{x}_{}( g(x))\), where \(_{}(W)\) denotes the largest singular value of the matrix \(W\), defined as \(_{}(W)=_{\|x\|_{2} 1}\|Wx\|_{2}\). For neural network \(f\) with linear layers \(g(x)=W_{i}x\) and \(1\)-Lipschitz activation (e.g., ReLU and leaky ReLU), we have \(L_{g}=_{}(W_{i})\) and \(L_{}_{i}_{}(W_{i})\). By normalizing the spectral norm of \(W_{i}\) with \(W_{i}^{}=W_{i}/_{}(W_{i})\), SN guarantees that the Lipschitz of \(f\) is upper-bounded by \(1\).

Finally, we characterize the algorithm convergence rate.

**Corollary 5.9** (Convergence Rate).: Let \((T)=_{t=0}^{T-1}b_{t}\). We have for \(T 4L^{2}\) that

\[_{t[T]}_{}J(_{_{t}}) _{2}^{2} 16(T)/+4^{ 2}(T)/T+O1/.\]

The convergence rate can be further clarified by determining how quickly the errors of model and critic approach zero, i.e., \(_{t=0}^{T-1}_{f}(t)+_{v}(t)\). Such results can be accomplished by conducting a more fine-grained investigation of the model and critic function classes, such as utilizing overparameterized neural nets with width scaling with \(T\) to bound the training error, as done in [10; 35], and incorporating complexity measures of the model and critic function classes to bound \(_{f}(t)\) and \(_{v}(t)\). Their forms, however, are beyond the scope of this paper.

## 6 Related Work

**Policy Gradient Methods.** Within the RL field, the LR estimator is the basis of most policy gradient algorithms, e.g., REINFORCE  and actor-critic methods [56; 31; 30; 14]. Recent works [3; 60; 7; 35] have shown the global convergence of LR policy gradient under certain conditions, while less attention has been focused on RP PGMs. Remarkably, the analysis in  is based on the strong assumptions on the _chained_ gradient and ignores the impact of value approximation, which oversimplifies the problem by reducing the \(h\)-step model value expansion to single-step model unrolls. Besides, _only_ focused on the gradient bias while still neglecting the necessary visitation analysis. Despite the utilization in our method of Spectral Normalization on the learned model to control the gradient variance, SN has also been applied in deep RL to _value_ functions in order to enable deeper neural nets  or regulate the value-aware model error .

**Differentiable Simulation.** This paper delves into the model-based setting [11; 28; 29; 70; 24], where a learned model is employed to train a control policy. Recent approaches [41; 53; 54; 67] based on differentiable simulators [18; 27] assume that gradients of simulation outcomes w.r.t. actions are explicitly given. To deal with the discontinuities and empirical bias phenomenon in the differentiable simulation caused by contact dynamics, previous works proposed smoothing the gradient adaptively with a contact-aware central-path parameter , using penalty-based contact formulations [20; 66] or adopting randomized smoothing for hard-contact dynamics [53; 54]. However, these are not in direct comparison to our analysis, which relies on model function approximators.

## 7 Experiments

### Evaluation of Reparameterization Policy Gradient Methods

To gain a deeper understanding and support the theoretical findings, we evaluate several algorithms originating from our RP PGM framework and compare them with various baselines in Figure 1. Specifically, RP-DP-SN is the proposed SN-based algorithm; RP-DP, as described in (4.1), is implemented as MAAC  with entropy regularization ; RP-DR, as described in (4.2), is implemented as SVG ; the model-free RP(0) is described in SS4.2. Details and discussions are deferred to SSC.

The results indicate that RP-DP consistently outperforms or matches the performance of existing methods such as MBPO  and LR PGMs, including REINFORCE , NPG , ACKTR , and PPO . This highlights the significance and potential of model-based RP PGMs. Due to space limitations, we refer the readers to SSC.5 for larger versions of the figures in the experiment section.

### Gradient Variance and Loss Landscape

Our prior investigations have revealed that vanilla MB RP PGMs tend to have highly non-smooth landscapes due to the significant increase in gradient variance. We now conduct experiments to validate this phenomenon in practice. In Figure 2, we plot the mean gradient variance of the vanilla RP-DP algorithm during training. To visualize the loss landscapes, we plot in Figure 3 the negative value estimate along two directions that are randomly selected in the policy parameter space of a training policy.

We can observe that for vanilla RP policy gradient algorithms, the gradient variance explodes in exponential rate with respect to the model unroll length. This results in a loss landscape that is highly non-smooth for larger unrolling steps. This renders the importance of smoothness regularization. Specifically, incorporating Spectral Normalization (SN)  in the model and policy neural nets leads to a marked reduction in mean gradient variance for all unroll length settings, resulting in a much smoother loss surface compared to the vanilla implementation.

### Benefit of Smoothness Regularization

In this section, we investigate the effect of smoothness regularization to support our claim: The gradient variance has polynomial dependence on the Lipschitz continuity of the model and policy, which is a contributing factor to training. Our results in Figure 4 show that SN-based RP PGMs achieve equivalent or superior performance compared to the vanilla implementation. Importantly, for longer model unrolls (e.g., \(10\) in walker2d and \(15\) in hopper), vanilla RP PGMs fail to produce reliable performance. SN-based methods, on the other hand, significantly boost training.

Additionally, we explore different choices of model unroll lengths and examine the impact of spectral normalization, with results shown in Figure 5. We find that by utilizing SN, the curse of chaos can be mitigated, allowing for longer model unrolls. This is crucial for practical algorithmic designs: The most popular model-based RP PGMs such as [12; 4] often rely on a carefully chosen (small) \(h\) (e.g., \(h=3\)). When the model is good enough, a small \(h\) may not fully leverage the accurate gradient information. As evidence, approaches [67; 41] based on differentiable simulators typically adopt longer unrolls compared to model-based approaches. Therefore, with SN, more accurate multi-step predictions should enable more efficient learning without making the

Figure 4: Performance of vanilla and SN-based MB RP PGMs with varying \(h\). The vanilla method only works with a small \(h\) and fails when \(h\) increases, while the SN-based method enables a larger \(h\).

Figure 5: Performance with different \(h\).

Figure 3: The loss surface of hopper.

Figure 2: Gradient variance of the vanilla RP-DP explodes while adding spectral normalization solves this issue.

underlying optimization process harder. SN-based approaches also provide more robustness since the return is insensitive to \(h\) and the variance of return is smaller compared to the vanilla implementation when \(h\) is large.

**Ablation on Variance.** By plotting the gradient variance of RP-DP during training in Figure 6, we can discern that for walker \(h=10\) and hopper \(h=15\), a key contributor to the failure of vanilla RP-DP is the exploding gradient variances. On the contrary, the SN-based approach excels in training performance as a result of the drastically reduced variance.

**Ablation on Bias.** When the underlying MDP is itself contact-rich and has non-smooth or even discontinuous dynamics, explicitly regularizing the Lipschitz of the transition model may lead to large error \(_{f}\) and thus large gradient bias. Therefore, it is also important to study if SN causes such a negative effect and if it does, how to trade off between the model bias and gradient variance. To efficiently obtain an accurate first-order gradient (instead of via finite difference in MuJoCo), we conduct ablation based on the _differentiable_ simulator dFlex , where Analytic Policy Gradient (APG) described in Section 3 can be implemented.

Figure 7 illustrates the crucial role SN plays in locomotion tasks. It is worth noting that the higher bias of the SN method does _not_ impede performance, but rather improves it, indicating that the primary obstacle in training RP PGMs is the large variance in gradients. Therefore, even if the simulation is differentiable, learning a smooth proxy model can be beneficial when the dynamics have bumps or discontinuous jumps, which is usually the case in robotics systems, sharing similarities with the gradient smoothing techniques  for APG.

## 8 Conclusion & Future Work

In this work, we study the convergence of model-based reparameterization policy gradient methods and identify the determining factors that affect the quality of gradient estimation. Based on our theory, we propose a spectral normalization (SN) method to mitigate the exploding gradient variance issue. Our experimental results also support the proposed theory and method. Since SN-based RP PGMs allow longer model unrolls without introducing additional optimization hardness, learning more accurate multi-step models to fully leverage their gradient information should be a fruitful future direction. It will also be interesting to explore different smoothness regularization designs and apply them to a broader range of algorithms, such as using proxy models in differentiable simulation to obtain smooth policy gradients, which we would like to leave as future work.