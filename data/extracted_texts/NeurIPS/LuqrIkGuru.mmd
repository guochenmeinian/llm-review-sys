# Are Your Models Still Fair? Fairness Attacks on

Graph Neural Networks via Node Injections

 Zihan Luo, Hong Huang, Yongkang Zhou, Jiping Zhang, Nuo Chen, Hai Jin

Huazhong University of Science and Technology, China

{zihanluo,honghuang,yongkangzhou_, jipingz,nuo_chen,hjin}@hust.edu.cn

 Hong Huang is the corresponding author. Zihan Luo, Hong Huang, Yongkang Zhou, Jiping Zhang and Hai Jin are affiliated with the National Engineering Research Center for Big Data Technology and System, Service Computing Technology and Systems Laboratory, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology.

###### Abstract

Despite the remarkable capabilities demonstrated by _Graph Neural Networks_ (GNNs) in graph-related tasks, recent research has revealed the fairness vulnerabilities in GNNs when facing malicious adversarial attacks. However, all existing fairness attacks require manipulating the connectivity between existing nodes, which may be prohibited in reality. To this end, we introduce a _Node Injection-based Fairness Attack_ (NIFA), exploring the vulnerabilities of GNN fairness in such a more realistic setting. In detail, NIFA first designs two insightful principles for node injection operations, namely the uncertainty-maximization principle and homophily-increase principle, and then optimizes injected nodes' feature matrix to further ensure the effectiveness of fairness attacks. Comprehensive experiments on three real-world datasets consistently demonstrate that NIFA can significantly undermine the fairness of mainstream GNNs, even including fairness-aware GNNs, by injecting merely 1% of nodes. We sincerely hope that our work can stimulate increasing attention from researchers on the vulnerability of GNN fairness, and encourage the development of corresponding defense mechanisms. Our code and data are released at: https://github.com/CGCL-codes/NIFA.

## 1 Introduction

Due to the strong capability in understanding graph structure, _Graph Neural Networks_ (GNNs) have achieved much progress in graph-related domains such as social recommendation  and bioinformatics . Nevertheless, despite the impressive capabilities demonstrated by GNNs, more and more in-depth research has revealed shortcomings in the fairness of GNN models, which greatly restricts their applications in the real world.

In fact, studies  have found that the biases and prejudices existed in training data would be further amplified through the message propagation mechanism of GNNs, leading to model predictions being correlated with certain sensitive attributes, such as _gender_ and _race_. Such correlations are usually undesired and can result in fairness issues and societal harm. For instance, in online recruitment, a recommender based on GNNs may be associated with the gender of applicants, leading to differential treatments towards different demographics and consequently giving rise to group unfairness. To address fairness issues in GNNs, researchers have proposed solutions such as adversarial learning , data augmentation  and others, which have achieved promising results.

However, recent research in the machine learning domain indicates that fairness is actually susceptible to adversarial attacks . Given this, we cannot help but wonder: _"Is the fairness of GNNmodels also highly vulnerable?_" For example, in e-commerce, if attackers could exacerbate performance disparities between male and female user groups by attacking GNN-based recommendation models, they could ultimately cause the e-commerce platform to provoke dissatisfaction from specific user demographics and gradually lose its appeal among these users. Several studies [11; 13; 41] have explored the vulnerability of GNN fairness and proposed effective attack strategies. Unlike conventional attacks, these fairness attacks aim to undermine GNN fairness without excessively compromising its utility. However, all these works require altering connectivity between existing nodes, whose authority is typically limited in the real world, such as modifying the relationship between real users. In contrast, injecting fake nodes into the original graph is a more practical way to launch an attack without manipulating the existing graph [12; 32; 34], which is still under-explored in the field of GNN fairness attack. To address this gap, we aim to be the first to launch an attack on GNN fairness via node injection, examining their vulnerabilities under such a more realistic setting.

Specifically, launching a node injection-based fairness attack on GNNs is non-trivial, whose challenges can be summarized as follows: **RQ1:**_How to determine the node injection strategy?_ The node injection can be decomposed into two steps, including selecting appropriate target nodes and connecting the injected nodes with them, both will impact the effectiveness of the attack. **RQ2:**_How to determine the features of the injected nodes after node injection?_ Like the real nodes, injected nodes will also participate in the message propagation process of GNNs, thereby affecting their neighbors and even the whole graph. Given the key role of massage propagation in GNN fairness [40; 45], proposing suitable strategies to determine the features of injected nodes is also important.

To address these challenges, we propose a gray-box poisoning attack method namely _Node Injection-based Fairness Attack_ (NIFA) during the GNN training phase. In detail, for the two steps in the first challenge, NIFA innovatively designs two corresponding principles. The first is the _uncertainty-maximization principle_, which asks to select real nodes with the highest model uncertainty as target nodes for injection. The idea is that nodes with higher uncertainty are typically more susceptible to attacks, thereby ensuring the attack's effectiveness. After selecting target nodes, NIFA follows the second principle, the _homophily-increase principle_, to connect target nodes with injected nodes. This principle aims to deteriorate GNN fairness by enhancing message propagation within sensitive groups [22; 40]. For the second challenge, multiple novel objective functions are proposed after node injection to guide the optimization of the injected nodes' features, which could further impact the victim GNN's fairness from a feature perspective. In summary, our contributions are as follows:

* To the best of our knowledge, we are the first to conduct fairness attacks on GNNs via node injections, and our work successfully highlights the vulnerability of GNN fairness. We also summarize several key insights for the future defense of GNNs' fairness attacks from the success of NIFA.
* We propose a node injection-based gray-box attack named NIFA. To be concrete, NIFA first designs two novel principles to guide the node injection operations from a structure perspective, and then proposes multiple objective functions for the injected nodes' feature optimization.
* We conduct extensive experiments on three real-world datasets, which consistently show that NIFA can effectively attack existing GNN models with only a 1% perturbation rate and an unnoticeable utility compromise, even including fairness-aware GNN models. Comparisons with other state-of-the-art baselines also verify the superiority of NIFA in achieving fairness attacks.

## 2 Related work

**Fairness on GNNs.** Researchers have discovered various fairness issues of GNNs, which often lead to societal harms [22; 37] and performance deterioration [21; 33] in practical applications. Algorithmic fairness on GNNs can be categorized into two main types based on the definition: individual fairness [1; 6] and group fairness [5; 37; 46]. Individual fairness requires that similar individuals should receive similar treatment, while group fairness aims to protect specific disadvantaged groups . In detail, many researchers have delved into studies focusing on fairness grounded in sensitive attributes. For instance, Dai et al.  reduce the identifiability of sensitive attributes in node embeddings through adversarial training to enhance fairness. FairVGNN  goes a step further by introducing a feature masking strategy to address the problem of sensitive information leakage during the feature propagation process in GNNs. Graphair  achieves fairness through an automated data augmentation method and FairSIN  designs a novel sensitive information neutralization method for fairness. Beyond fairness related to sensitive attributes, some researchers also direct attention to fairness related to graph structures, like DegFairGNN  and Ada-GNN . In this work, we mainly focus on attacks on the group fairness of GNNs based on sensitive attributes.

**Attacks on GNNs.** Finding out potential vulnerabilities thus improving the security of GNNs remains a pivotal concern in the field of trustworthy GNNs . From the perspective of attackers, they aim to compromise the GNNs' performance on graph data via manipulating graph structures [24; 32; 47], node attributes , or node labels . Among these methods, node injection attacks, given the attackers' limited authority to manipulate the connectivity between existing nodes, emerge as one of the most prevalent methods [32; 34; 43]. However, existing attacks, including node injection attacks, mainly focus on undermining GNN's utility, with little attention to the vulnerability of GNN fairness. Different from attacks on GNN utility, fairness-targeted attacks aim to deteriorate the fairness without significantly compromising the accuracy. FA-GNN , FATE , and G-FairAttack  stand out as the few ones that we are aware of to explore attacks on GNN fairness. FA-GNN's empirical findings suggest that adding edges with certain strategies can significantly compromise GNN fairness without affecting its performance . FATE  formulates the fairness attacks as a bi-level optimization problem and proposes a meta-learning-based framework. G-FairAttack  designs a novel surrogate loss with utility constraints to launch the attacks in a non-gradient manner. Nevertheless, all these works require modifying the link structure between existing nodes, which may be prohibited in reality due to the lack of authority.

## 3 Preliminary

Here we will introduce some basic notations and concepts, and then give our problem definition.

### Notations

A graph is denoted as \(=(,,)\), where \(\) is the node set, and \(^{||||}\) represents the adjacency matrix. \(^{|| D}\) denotes the feature matrix, in which \(D\) is the feature dimension. Under the settings of node classification, each node \(v\) will be assigned with a label \(y_{v}\), and a GNN-based mapping function \(f_{}:\{,\}\{1,2,...,||\}^{| |}\) with parameters \(\) is learned to leverage the graph signals for label prediction, where \(\) represents the true label set.

### Fairness-related concepts

In alignment with prior works [5; 7; 17], we mainly focus on group fairness where each node will be assigned with a binary sensitive attribute \(s\{0,1\}\), although our attack could also be generalized to the settings of multi-sensitive groups and we leave this as our future work. Based on the sensitive attributes, the nodes can be divided into two non-overlapped groups \(=\{_{0},_{1}\}\), and we employ the following two kinds of fairness related definitions:

**Definition 1**.: _Statistical Parity (SP). The Statistical Parity requires the prediction probability distribution to be independent of sensitive attributes, i.e. for any class \(y\) and any node \(v\):_

\[P(_{v}=y|s=0)=P(_{v}=y|s=1),\] (1)

_where \(_{v}\) denotes the predicted label of node \(v\)._

**Definition 2**.: _Equal Opportunity (EO). The Equal Opportunity requires that the probability of predicting correctly is independent of sensitive attributes, i.e. for any class \(y\) and any node \(v\), we can have:_

\[P(_{v}=y|y_{v}=y,s=0)=P(_{v}=y|y_{v}=y,s=1).\] (2)

Based on the above definitions, we can define two kinds of metrics \(_{SP}\) and \(_{EO}\) to quantitatively measure fairness. For both metrics, smaller values indicate better fairness:

\[_{SP}=|P(=y|s=0)-P(=y|s=1)|,\] (3)

\[_{EO}=|P(=y|y=y,s=0)-P(=y|y=y,s=1)|.\] (4)

### Problem definition

In this paper, our goal is to launch fairness-targeted attacks on GNN models through the application of node injection during the training phase, i.e. poisoning attack. Following the line of previous attacks on GNNs [32; 41], our attack is under the prevalent gray-box setting, where the attackers can obtain the graph \(\) with node labels \(\), and the sensitive information \(s\), but can not access the model architecture and parameters \(\). Detailed introduction to our attack settings is provided in Appendix B. Specifically, through injecting malicious node set \(_{I}\) into the graph, the original graph \(=(,,)\) is poisoned as \(^{}=(^{},^{},^{ })\), where

\[^{}=_{I},\ \ ^{}= [\\ _{I}],_{I}^{|_{I }| D},\] (5)

\[^{}=[&_{I}\\ _{I}^{T}&_{I}],_{I}^{|||_{I}|},_{I}^{| _{I}||_{I}|}.\] (6)

Both \(_{I}\) and \(_{I}\) are matrices for illustrating the connectivity related to injected nodes, and \(_{I}\) is the feature matrix of injected nodes \(_{I}\). The true label set \(\) will not be poisoned by injected nodes in our settings, as such information is typically hard to modify in reality. For conciseness, we denote \(()\) and \(()\) as the evaluation functions on fairness and utility for the learned mapping function \(f_{}\), respectively. Then our goal as an injection-based attack on fairness could be formulated as:

\[_{^{}}|(f_{^{*}}(,^{}))|\] (7) \[\ *{arg\,max}_{^{*}}(f_{ ^{*}}(,^{})),\ \ ^{}=(^{},^{},^{ }),\ \ |_{I}| b,\ \ deg(v)_{v_{I}} d.\]

As a poisoning attack, the first constraint in Eq. (7) requires to train the victim model \(f_{^{*}}\) with parameters \(^{*}\) on the poisoned graph \(^{}\), so that the predictions of \(f_{^{*}}\) are as correct as possible before evaluating the attack performance. The following constraints in Eq. (7) make sure that the proposed attack is unnoticeable and deceptive to the defenders, i.e. the number of injected nodes is below a predefined budget \(b\)1 and the degrees of injected nodes are constrained by a budget \(d\). Our goal is to find a poisoned graph \(^{}\) to deteriorate the fairness of victim models \(f_{^{*}}\) as severely as possible, i.e. maximize the fairness metrics \(_{SP}\) and \(_{EO}\) introduced previously.

## 4 Methodology

In this section, we first give an overview of our attack method NIFA. Then we will elaborate on the details of each module and summarize the implementation algorithms at last.

### Framework overview

The overall framework of NIFA is illustrated in Figure 1. As mentioned before, NIFA first employs two principles to guide the node injection operations. For the first uncertainty-maximization principle, NIFA utilizes the Bayesian GNN for model uncertainty estimation of each node, and then selects target nodes with the highest uncertainty (a). As for the second homophily-increase principle, NIFA requires each injected node can only establish connections to target nodes from one single sensitive group (b), thus increasing the homophily-ratio and enhancing information propagation within sensitive groups. After node injection, multiple objective functions are designed to guide the optimization of injected nodes' feature matrix, where we introduce an iterative optimization strategy for avoiding over-fitting issues (c). The details of each part will be introduced later.

### Node injection with principles

The first step of NIFA is conducting node injections, which aims to ensure the effectiveness of NIFA from a structure perspective. In detail, we propose two novel principles to guide the node injection operations, namely _Uncertainty-maximization principle_ and _Homophily-increase principle_.

**Uncertainty-maximization principle.** Intuitively, nodes with higher model uncertainties are positioned closer to the decision boundary, which means their predicted labels are more vulnerable and easier to flip when facing adversarial attacks. We acknowledge that the model uncertainty may not be the only method to measure the vulnerability of nodes, and we will discuss potential alternative approaches in Appendix H.2. Inspired by , we utilize a Bayesian GNN to estimate the model uncertainty of each node, where we employ the Monte Carlo dropout approach  to approximate the distributions of the sampled model parameters. Given a GNN with parameters \(_{}\), we obtain different model parameters through \(T\) times independent Bernoulli dropout sampling processes, i.e.:

\[P(M_{i}) \ (p)\] \[_{_{i}} =\ M_{i}_{},i\{1,2,,T\},\] (8)

where \(M_{i}\) is the \(i\)th sampled binary mask following the Bernoulli distribution with parameter \(p\), and \(\) denotes the dot production operation. Here we take a two-layer GCN  with parameters \(_{}\) as the Bayesian GNN for estimating the uncertainty of each node, and \(_{}\) is optimized by minimizing the following objective function, which consists of a cross-entropy loss plus a regularization term:

\[L(_{})=-_{i=1}^{T}(f_{_{ _{i}}}(,))+\|_{}\|_{2}^{2},\] (9)

where \(\) denotes the true labels, \(\|\|_{2}^{2}\) denotes the L2 regularization, \(T\) is the number of sampling processes and \(f_{_{_{i}}}()\) is the mapping function with the \(i\)th sampled parameters \(_{_{i}}\). After the training process, model uncertainty can be estimated by calculating the variance of \(T\) times predictions with the sampled parameters \(\{_{_{i}}\}_{i=1}^{T}\). Intuitively, nodes with lower variance are more confident in their predictions and vice versa. Thus, the model uncertainty scores \(U^{||}\) are positively correlated with the model prediction variance, and we simply estimate \(U\) with the following formulation:

\[U=(f_{_{_{i}}}(,)).\] (10)

Under the guidance of the uncertainty-maximization principle, we will select nodes with the top \(k\%\) model uncertainty \(U\) in each sensitive group as the target nodes, where \(k\) is a hyper-parameter.

**Homophily-increase principle.** After selecting target nodes, the next step is to connect the injected nodes with them. In particular, we first present our strategy in this step, with more rationales provided later: the injected nodes \(_{I}\) are first equally assigned to each sensitive group in the graph, then each injected node will exclusively connect to \(d\) random target nodes with the same sensitive attribute, as illustrated in Figure 1(b), where \(d\) is a hyper-parameter. At this stage, the node injection operations are completed, with the structure of the original graph \(\) manipulated.

Intuitively, compared with random node injection, our strategy prevents information propagation between nodes of different sensitive groups through the injected nodes, making it easier to accentuate

Figure 1: The overall framework of NIFA: (a) Utilizing uncertainty estimation, nodes exhibiting high uncertainty (depicted as shaded nodes) are designated as targeted nodes. (b) Injected nodes are equally assigned to each sensitive group, and only connect targeted nodes with the same sensitive attribute. (c) After node injection, the injected feature matrix and surrogate model are optimized iteratively by diverse objective functions.

differences in embeddings between groups and thereby exacerbate unfairness issues . We also provide a brief theoretical analysis to show that such a strategy could lead to the increase of node-level homophily ratio. Similar to , we define the node-level homophily-ratio \(_{u}\) as the ratio of neighbors of node \(u\) that have the same sensitive attribute as node \(u\), i.e. \(_{u}=_{u}}(s_{u}=s_{v})}{| _{u}|}\), where \(_{u}\) denotes the neighbors of node \(u\) and \(()\) is an indicator function. Then we can have:

**Lemma 1**.: _For target node \(u\) that will connect with injected nodes, our proposed node injection strategy will lead to the increase of node-level homophily-ratio \(_{u}\)._

Due to space limitation, the proof for Lemma 1 is provided in Appendix C. It is worth noting that \(_{u}\) is also equivalent to the probability of choosing neighbors with the same sensitive attribute for node \(u\). From the perspective of message propagation, higher node-level homophily-ratio indicates that more sensitive-related information will be aggregated to the target node, thus leading to more severe unfairness issues on sensitive attributes2. We believe that such characteristics could empower our node injection strategy with stronger capability on fairness attacks.

### Feature optimization

In this part, we will introduce the details of optimizing injected nodes' features \(_{I}\), which helps further advance the effectiveness of NIFA. Generally, under a gray-box attack setting, there is no visible information about the victim models for the attackers, thus requiring attackers to propose a surrogate GNN model \(\) at first for assessing their attacks. To be specific, similar to the training process of victim models as described in Eq. (7), the surrogate model \(\) will be trained on the poisoned graph \(^{}\) and optimize its parameter \(_{}\) to maximize the utility. Conversely, \(_{I}\) is designed to mislead \(\), ensuring that even a well-trained surrogate model will still maintain high unfairness under attacks. Instead of employing a pre-trained frozen surrogate model \(\), NIFA asks two components, i.e. \(\) and \(_{I}\) to be trained iteratively with different objective functions, which avoids the attack being over-fitting to specific model parameters. In detail, the surrogate model \(\) follows the common training procedure of a GNN classifier with cross-entropy loss, while for the injected nodes' feature optimization, we devise multiple effective objective functions as follows:

**Classification loss.** Although our primary goal is to maximize the unfairness of a GNN model, it is crucial to ensure that the utility of the victim model will not experience a significant decrease after training on a poisoned graph [11; 13; 41], thus being unnoticeable for utility-based attack detection. To this end, we set cross-entropy loss as our first objective function, i.e.:

\[L_{CE}=-^{tr}|}_{u^{tr}}y_{u} h_{u},\] (11)

where \(^{tr}\) denotes the original training node set, and \(h_{u}\) denotes the output logits of node \(u\).

**Fairness loss.** Aiming at enlarging the unfairness on GNNs, we then design two kinds of fairness loss based on the definitions of \(_{SP}\) and \(_{EO}\), which are formulated as:

\[L_{SP}=-\|^{tr}_{0}|}_{u^{tr}_{0}}h_{ u}-^{tr}_{1}|}_{u^{tr}_{1}}h_{u}\|^{2}_{2},\] (12)

\[L_{EO}=-\|_{y}(^{tr}_{0,y}|}_{u ^{tr}_{0,y}}h_{u,y}-^{tr}_{1,y}|}_{u ^{tr}_{1,y}}h_{u,y})\|^{2}_{2},\] (13)

where \(h_{u}^{||}\) denotes the raw output of node \(u\), and \(h_{u,y}\) denotes the raw output of node \(u\) on class \(y\). \(^{tr}_{i,y}\) denotes the training nodes with sensitive attribute \(i\) and label \(y\). By minimizing \(L_{SP}\) and \(L_{EO}\), the gap in output between different groups increases, leading to high unfairness.

**Constraint of feature.** To further accentuate the differences between different sensitive groups, it is important to ensure that the information introduced by injected nodes for different sensitive groupsis distinct during the message propagation process. To this end, we devise the following constraint function on the injected node feature matrix \(_{I}\):

\[L_{CF}=-\|_{I,0}|}_{u_{I,0}}_{I,u}-_{I,1}|}_{u_{I,1}}_{I,u}\| _{2}^{2},\] (14)

where \(_{I,i}\) is the injected node set linking to the \(i\)th sensitive group during the node injection.

**Overall loss.** By combining the aforementioned objective terms, the overall loss \(L\) for injected nodes' features optimization can be formulated as:

\[L=L_{CE}+ L_{CF}+(L_{SP}+L_{EO}),\] (15)

where \(\) and \(\) are two hyper-parameters to control the weights of different objective functions.

### Implementation algorithm

**Training process.** Due to space limitation, we summarize the pseudo-code of NIFA in Algorithm 1 in Appendix D. Initially, we perform node injection operations based on two proposed principles (lines 2-4). Subsequently, an iterative training strategy is utilized to optimize the surrogate model and injected nodes' feature (lines 5-15). Specifically, after each inner loop for \(_{I}\) training, it is clamped to fit the range of the original feature **X** (line 14) so that the defenders cannot filter out the injected nodes easily through abnormal feature detection. For datasets with discrete features, \(_{I}\) is rounded to the nearest integer at the end of the training process (lines 16-18).

**Inference process.** As a poisoning attack, the original clean graph \(\) is poisoned as \(^{}\) after malicious node injection and feature optimization. The victim models will re-train on the poisoned graph \(^{}\) normally, and we take the predictions from the poisoned victim model for final evaluation.

## 5 Experiments

### Experimental settings

**Datasets.** Experiments are conducted on three real-world datasets namely Pokec-z, Pokec-n, and DBLP. Both **Pokec-z** and **Pokec-n** are subgraphs sampled from Pokec, one of the largest online social networks in Slovakia, according to the provinces of users . Each node in these graphs represents a user, while each edge represents an unidirectional following relationship. The datasets provide node attributes including age, gender, and hobbies, and the classification task is to predict the working fields of users. **DBLP** is a coauthor network dataset , where each node represents an author and two authors will be connected if they publish at least one paper together. The node features are constructed based on the words selected from the corresponding author's published papers. The final classification task is to predict the research area of the authors. The detailed dataset statistics are summarized in Table 1.

**Victim models.** As a gray-box attack method, we target multiple classical GNNs as victim models, including GCN , GraphSAGE , APPNP , and SGC . We also include three well-established fairness-aware GNNs -- FairGNN , FairVGNN , and FairSIN  as our selected victim models. The details of these victim models will be elaborated in Appendix E.

**Baselines.** Depending on the attack goals, we mainly consider the following two kinds of graph attack methods as our baselines, including _1) Utility attack_: AFGSM , TDGIA , and G\({}^{2}\)A\(2\)C , and _2) Fairness attack_: FA-GNN , FATE , and G-FairAttack . The details of these baselines will be further introduced in Appendix F.

**Implementation details.** As shown in Table 1, only a part of the nodes have the label information, and we randomly select 50%, 25%, and 25% labeled nodes as the training set, validation set, and test set, respectively. In line with the prior work, , we choose _region_ as the sensitive attribute for Pokec-z and Pokec-n, and _gender_ for DBLP. For all victim models, we employ a two-layer GCN model as the surrogate model. Due to space limitations, please refer to Appendix G for more reproducibility details.

  
**Dataset** & **Pokec-z** & **Pokec-n** & **DBLP** \\  \# of nodes & 67,796 & 66,569 & 20,111 \\ \# of edges & 617,958 & 517,047 & 57,508 \\ feature dim. & 276 & 265 & 2,530 \\ \# of labeled nodes & 10,262 & 8,797 & 3,196 \\   

Table 1: Dataset statistics 

[MISSING_PAGE_FAIL:8]

to deploy on Pokec-z and Pokec-n due to scalability issues3. Results after repeating five times are shown in Table 3.

It can be seen that NIFA consistently achieves the state-of-the-art fairness attack performance on three datasets. The reasons might be two-fold: 1) the utility attack methods are mainly designed to impact the accuracy of victim models, while overlooking the fairness objectives. 2) As pioneering works in fairness attacks on GNNs, all baselines on fairness attacks need to modify the original graph, such as adding or removing some edges or modifying features of real nodes. For decotiveness consideration, their modifications are usually constrained by a small budget. However, NIFA introduces new nodes into the original graph through node injection and can optimize the injected nodes in a relatively larger feature space. Such superiority of node injection attack helps NIFA have a greater impact on the original graph from the feature perspectives.

### Ablation study

In this part, we conduct ablation experiments to prove the effectiveness of the uncertainty-maximization principle, homophily-increase principle, and iterative training strategy, respectively. In detail, we consider the following three variants of NIFA. 1) _NIFA-U_: the uncertainty-maximization principle is removed, and we randomly choose targeted nodes from the labeled nodes. 2) _NIFA-H_: we still choose real nodes with the top k% model uncertainty as the targeted nodes, but the homophily-increase principle is removed, i.e. each injected node may connect with targeted nodes from different sensitive groups simultaneously. 3) _NIFA-1_: we remove the iterative training strategy here, which means that the surrogate model is trained on the clean graph in advance, and the feature optimization process will only involve the training process of injected feature matrix. For all variants, we set GCN as the victim model.

The results are reported in Figure 2, where we can have the following observations. Firstly, after removing the uncertainty-maximization principle (NIFA-U), the fairness attack performance consistently decreases on three datasets. This is expected since the concept of uncertainty helps NIFA find more vulnerable nodes, thus improving the attack effectiveness. Secondly, after removing the homophily-increase principle (NIFA-H), the attack performance drops obviously, which verifies the homophily-ratio is crucial in GNN fairness. Finally, without iterative training during feature optimization (NIFA-I), the attack performance decreases slightly on all datasets. The main reason is that the iterative training strategy could help NIFA to have better robustness to dynamic victim models.

## 6 Defense discussion to fairness attacks on GNNs

As previously emphasized, our intrinsic aim is to unveil the vulnerabilities of existing GNN models in terms of fairness, thereby inspiring related defense research. In fact, as an emerging field that is just beginning to be explored, defense strategies against GNN fairness attacks are relatively scarce. However, we still can summarize several key insights from NIFA for further careful study:

**Reliable training nodes.** One key assumption in NIFA is that the nodes with high model uncertainty will be much easier to be attacked, which can also be supported by the ablation study in Section 5.4. In this way, administrators can pay more attention to these nodes and their abnormal neighbors for defense

Figure 3: Defense performance on Pokec-z with masking \(\) training nodes with the highest uncertainty

Figure 2: Ablation study of each module in NIFA

purposes. For example, engineers can pre-train a model to detect the abnormal nodes or edges in advance, especially those that emerged recently in the training data, and weaken their impacts on the model by randomly masking these nodes or edges in the input graph or decreasing their weights in the message propagation during the training of GNNs.

To verify our assumption, we conduct a simple experiment by removing a proportion of nodes (\(\)) with the highest uncertainty \(U\) from supervision signals after the attack. Similarly, GCN  is employed as the victim model and we gradually tune \(\) from 0 to 0.6 with step 0.1, where \(=0\) means no defense is involved. The performance of NIFA on the Pokec-z dataset with different \(\) is illustrated in Figure 3. It can be seen that, since NIFA mainly focuses on attacking nodes with high uncertainty, after masking a part of these nodes during the training stage, the fairness attack performance of NIFA gradually decreases with a small fluctuation in accuracy. However, it is worth noting that although such an intuitive strategy can defend the attack from NIFA to some extent, there is still obvious fairness deterioration compared with the performance of clean GCN in Table 2 (\(_{SP}\)=7.13, \(_{EO}\)=5.10). More dedicated and effective defense mechanisms in the future are still in demand.

**Strengthen the connections among groups.** One main reason behind the success of NIFA in fairness attack is the guidance of the homophily-increase principle during node injection. The ablation study in Appendix 5.4 also provides empirical evidence for this claim. As we analyze in Section 4.2, NIFA will lead to the increase of node-level homophily-ratio, which means more sensitive-related information will be aggregated and enlarged within the group. Given this, we believe that an effective defensive strategy is to strengthen the information propagation among different sensitive groups, thus preventing the risks of information cocoons [22; 25] and fairness issues.

**Fairness auditing.** At last, we find that a crucial assumption in NIFA and other research [11; 13; 41] is that GNN model administrators will only audit the utility metrics of the models, such as accuracy or F1-scores. Therefore, as long as attackers can ensure that the model utility is not affected excessively, it will be hard for administrators to realize the attack. Consequently, we strongly suggest that model administrators should also incorporate fairness-related metrics into their monitoring scopes, especially before model deployment or during the beta testing phase, thus, mitigating the potential broader negative impacts and social risks. For instance, if an updated GNN model suddenly demonstrates obvious fairness deterioration compared with the previous versions, the model administrators should be careful about the potential fairness attacks. However, the challenge of this approach mainly lies in the diverse definitions of fairness, such as group fairness [5; 23], individual fairness , etc., and group fairness based on different sensitive attributes [5; 38] or structures [21; 22] may further lead to different definitions. Therefore, model administrators might need prior knowledge or expertise to determine what kinds of fairness metrics to be included in their monitoring scopes.

## 7 Conclusion

In this work, we aim to examine the vulnerability of GNN fairness under adversarial attacks, thus mitigating the potential risks when applying GNNs in the real world. All existing fairness attacks on GNNs require modifying the connectivity or features of existing nodes, which is typically infeasible in reality. To this end, we propose a node injection-based poisoning attack namely NIFA. In detail, NIFA first proposes two novel principles for node injection operations and then designs multiple objective functions to guide the feature optimization of injected nodes. Extensive experiments on three datasets demonstrate that NIFA can effectively attack most mainstream GNNs and fairness-aware GNNs with an unnoticeable perturbation rate and utility degradation. Our work highlights the vulnerabilities of GNNs to node injection-based fairness attacks and sheds light on future research about robust fair GNNs and defensive mechanisms for potential fairness attacks.

**Limitations.** Firstly, NIFA is still under the settings of gray-box attacks, which requires accessibility to the labels and sensitive attributes. We acknowledge that such information may not always be available and we leave the extensions to the more realistic black-box attack settings as future work. Moreover, although we present some insights on the defense strategies of GNN fairness, more effective defense measures are still under-explored, calling for more future research efforts. At last, currently we only focus on fairness based on sensitive attributes, while neglecting the fairness based on graph structures. Since different fairness may stem from different sources, we leave this as our future work.