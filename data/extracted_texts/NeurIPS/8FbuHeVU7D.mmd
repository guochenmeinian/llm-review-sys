# Differentially Private Statistical Inference

through \(\)-Divergence One Posterior Sampling

 Jack Jewson

Department of Economics and Business

Universitat Pompeu Fabra

Barcelona, Spain

jack.jewson@upf.edu

&Sahra Ghalebikesabi

Department of Statistics

University of Oxford

Oxford, UK

sahra.ghalebikesabi@univ.ox.ac.uk

Joint first authorship. Order decided based on coin flip.

Chris Holmes

The Alan Turing Institute

Department of Statistics

University of Oxford

Oxford, UK

chris.holmes@stats.ox.ac.uk

###### Abstract

Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose \(\)D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the \(\)-divergence between the model and the data generating process. This provides private estimation that is generally applicable without requiring changes to the underlying model and consistently learns the data generating parameter. We show that \(\)D-Bayes produces more precise inference estimation for the same privacy guarantees, and further facilitates differentially private estimation via posterior sampling for complex classifiers and continuous regression models such as neural networks for the first time.

## 1 Introduction

Statistical and machine learning analyses are increasingly being done with sensitive information, such as personal user preferences , electronic health care records , or defence and national security data . It thus becomes more and more important to ensure that data-centric algorithms do not leak information about their training data. Let \(D=\{D_{i}\}_{i=1}^{n}=\{y_{i},X_{i}\}_{i=1}^{n}\) denote a sensitive data set with \(d\) dimensional features \(X_{i}^{d}\), and labels \(y_{i}\). Here, we are considering inference problems where a trusted data holder releases model parameters \((D)\), that describe the relationship between \(X\) and \(y\), based on an arbitrary likelihood model \(f(;)\). Differential privacy(DP) provides a popular framework to quantify the extent to which releasing \((D)\) compromises the privacy of any single observation \(D_{i}=\{y_{i},X_{i}\} D\).

**Definition 1** (Differential Privacy, ).: _Let \(D\) and \(D^{}\) be any two neighbouring data sets differing in at most one feature label pair. A randomised parameter estimator \((D)\) is \((,)\)-differentially private for \(>0\) and \(\) if for all \(\), \(P((D))()P((D^{ }))+\)._

The privacy guarantee is controlled by the privacy budget \((,)\). While \(\) bounds the log-likelihood ratio of the estimator \((D)\) for any two neighbouring data sets \(D\) and \(D^{}\), \(\) is the probability of outputs violating this bound and is thus typically chosen smaller than \(1/n\) to prevent data leakage. DP estimation requires the parameter estimate to be random even after conditioning on the observed data. Thus, noise must be introduced into the learning or release procedure of deterministic empirical risk minimisers to provide DP guarantees.

The _sensitivity method_ is a popular privatisation technique in DP where the functional that depends on the sensitive data (i.e. a sufficient statistic, loss objective or gradient) is perturbed with noise that scales with the functional's _sensitivity_. The sensitivity \(S(h)\) of a functional \(h:\) is the maximum difference between the values of the function on any pair of neighboring data sets \(D\) and \(D^{}\), \(S(h)=_{D,D^{}}\|h(D)-h(D^{})\|\). As the bound of statistical functionals given arbitrary data sets is typically unknown, their sensitivity is determined by assuming bounded input features  or parameter spaces . In practise, noise is added either directly to the estimate \(\) or the empirical loss function, skewing the interpretation of the released statistical estimates in ways that cannot be explained by probabilistic modelling assumptions. In differentially private stochastic gradient descent [DPSGD; 1], for example, the sensitivity of the mini-batch gradient in each update step is bounded by clipping the gradients of single batch observations before averaging. The averaged mini-batch gradient is then perturbed with noise that scales inversely with the gradients' clipping norm. The repercussions of so doing can be detrimental. The effects of statistical bias within DP estimation have been subject to recent scrutiny, and bias mitigation approaches have been proposed .

An alternative to the sensitivity method, is one-posterior sampling [OPS; 81]. Instead of artificially introducing noise that biases the learning procedure, OPS takes advantage of the inherent uncertainty provided by sampling from Bayesian posterior distributions . Given prior \(()\) and likelihood \(f(D;)\), the _random_ OPS estimate corresponds to a single sample from the Bayesian posterior \((|D) f(D;)()\). If one accepts the Bayesian inference paradigm, then a probabilistic interpretation of the data generative distribution can be leveraged to sample interpretable DP parameter estimates. Additionally, Bayesian maximum-a-posteriori estimates are generally associated with regularised maximum likelihood estimates, and OPS has been shown to consistently learn the data generating parameter . Therefore, OPS provides a compelling method for DP estimation independently of the analyst's perspective on the Bayesian paradigm. Current approaches to OPS, however, only provide DP under unrealistic conditions where the log-likelihood is bounded  or unbounded but Lipschitz and convex  limiting their implementation beyond logistic regression models - see Table 1.

In this paper, we extend the applicability of OPS to more general prediction tasks, making it a useful alternative to the sensitivity method. To do so, we combine the ability of OPS to produce consistent estimates with a robustified general Bayesian posterior aimed at minimising the \(\)-divergence  between the model \(f(D_{i};)\) and the data generating process. We henceforth refer to this privacy mechanism as \(\)D-Bayes. While previous research has studied the benefits of the \(\)-divergence for learning model parameters that are robust to outliers and model misspecification , we leverage its beneficial properties for DP parameter estimation: A feature of the \(\)D-Bayes posterior is that it naturally provides a pseudo-log likelihood with bounded sensitivity for popular classification and regression models without having to modify the underlying model \(f(;)\). Further, such a bound is often independent of the predictive model, which allows, for instance, the privatisation of neural networks without an analysis of their sensitivity or perturbation of gradients.

Contributions.Our contributions can thus be summarised as follows:

* By combining OPS with the intrinsically bounded \(\)-divergence, we propose \(\)D-Bayes, a DP mechanism that applies to a general class of inference models.
* \(\)D-Bayes bounds the sensitivity of a learning procedure without changing the model, getting rid of the need to assume bounded feature spaces or clipping statistical functionals. If the model is correctly specified, the data generating parameter can be learned consistently.

* \(\)D-Bayes improves the efficiency of OPS leading to improved precision for DP logistic regression.
* The general applicability of \(\)D-Bayes facilitates OPS estimation for complex models such as Bayesian neural network (BNN) classification and regression models.
* We provide extensive empirical evidence by reporting the performance of our model and four relevant baselines on ten different data sets, for two different tasks; additionally analysing their sensitivity in the number of samples and the available privacy budget.

## 2 Background and related work

Here, we focus on the OPS literature that analyses the inherent privacy of perfect sampling from a Bayesian posterior . Such a DP mechanism is theoretically appealing as it does not violate generative modelling assumptions. Current approaches have, however, been limited in their application due to strong likelihood restrictions. We start by outlining the weaknesses of traditional DP methods and the state-of-the-art for OPS, before we introduce \(\)D-Bayes OPS.

OPS is distinct from other work on DP Bayesian inference that can be categorised as either analysing the DP release of posterior samples using variational inference , or Monte Carlo procedures . We show that these methods can be extended to approximately sample from \(\)D-Bayes in Section 4.

DP via the sensitivity methodAccording to the exponential mechanism , sampling \(\) with probability proportional to \((-(D,)/(2S(h))\) for a loss function \(:\) is \((,0)\)-DP. A particularly widely used instance is the sensitivity method  which adds Laplace noise with scale calibrated by the sensitivity of the estimator. For example, consider empirical risk minimisation for a \(p\) dimensional parameter \(^{p}\):

\[(D):=*{arg\,min}_{}_{i =1}^{n}(D_{i},f(;))+ R()\] (1)

where \((D_{i},f(;))\) is the loss function, \(R()\) is a regulariser, and \(>0\) is the regularisation weight. Chaudhuri et al.  show that \(=(D)+z,^{p} z=(z_{1},,z_{p}) }}{{}}(0,)\), where \((,s)\) is a Laplace distribution with density \(f(z)=\{-|z-|/s\}\), is \((,0)\)-DP provided \(R()\) is differentiable and 1-strongly convex, and \((y_{i},)\) is convex and differentiable with gradient \(|^{}(D_{i},)|<1\). The negative log-likelihood of logistic regression with a \(L_{2}\) regulariser satisfies these conditions when the features are standardised between 0 and 1 - see Section A.4.1 for more details. Relaxing the conditions of convexity, DPSGD , which adds calibrated noise to gradient evaluations within stochastic gradient descent, has arisen as a general purpose tool for empirical risk minimisation . To achieve bounded sensitivity, DPSGD clips each single gradient within the update step. Instead of bounding the sensitivity artificially by assuming bounded feature spaces or clipping data functions, Dwork and Lei  identified the promise of robust methods for DP-estimation. Robust estimation procedures [e.g 41, 35] provide parameter estimates that are less sensitive to any one observation and therefore the scale of the noise that needs to be added to privatise such an estimate is reduced. While the connection between robust statistical estimators and DP has thus been subject to extensive research , we are the first to consider it within Bayesian sampling to produce a generally applicable method for consistent estimation of model parameters.

DP via Gibbs one-posterior samplingSampling from a Bayesian posterior constitutes a particular case of the exponential mechanism , leading to the proposal of OPS through Gibbs posterior sampling. If the log-likelihood \( f(D;)\) is such that \(_{D,}| f(D;)| B\), then one sample from the Gibbs posterior

\[^{w f}(|D)()\{w_{i=1}^{n} f(D_{ i};)\}\] (2)

with \(w=\) is \((,0)\)-DP . Note \(w=1\) recovers the standard posterior and \(w 1\) provides flexibility to adapt the posterior to the level of privacy required. However, standard models for inference do not usually have bounded log-likelihoods. Even discrete models with bounded support such as logistic regression do not. To overcome this, Wang et al.  assume a bounded parameter space and accordingly'scale-down' the data. However, parameter spaces are typically unbounded, and data scaling has to be done carefully to avoid information leakage or loss inefficiency. Nevertheless,their algorithm is shown to outperform the perturbation approach proposed by . Some works [21; 22; 86] overcome the assumption of a bounded log-likelihood by relaxing the definition of DP allowing distant observations according to some metric to be distinguishable with greater probability. Such a relaxation, however, no longer guarantees individual privacy. Geumlek et al.  consider versions of the Gibbs posterior for exponential family models and generalised linear models but in the context of Renyi-DP (RDP) , a relaxation of DP that bounds the Renyi divergence with parameter \(\) between the posterior when changing one observation. Minami et al.  generalise the result of Wang et al.  to show that one sample from (2) with \(w=/(1+2(1/))}\) for convex \(L\)-Lipschitz log-likelihoods with \(m_{}\)-strongly convex regulariser is \((,)\)-DP. For logistic regression, we have \(L=2\) if the features are bounded between \(0\) and \(1\). This, however, requires a relaxation to \(>0\).

Sufficient statistics perturbationFoulds et al.  identified that OPS mechanisms based on the Gibbs posterior are data inefficient in terms of asymptotic relative efficiency. For exponential family models, Foulds et al.  and Zheng  propose a Laplace mechanism that perturbs the data's sufficient statistics and considers conjugate posterior updating according to the perturbed sufficient statistics. The privatisation of the statistics allows for the whole posterior to be released rather than just one sample, and the perturbation of the statistics is independent of \(n\) allowing the amount of noise to become smaller relative to the sufficient statistics as \(n\) providing consistent inference. They compare directly to  showing improved inference for exponential family models. However, these results require bounded sufficient statistics. Extensions include [12; 13; 67; 40; 71; 51]. These methods are further limited to exponential families which do not include popular machine learning models e.g. logistic regression or neural networks. In this paper, we set out to generalise the applicability of OPS and also address its efficiency issues by moving away from the Gibbs posterior, making use of tools used within generalised Bayesian updating.

## 3 \(\)D-Bayes one-posterior sampling

Generalised Bayesian Updating and the beta-DivergenceOPS has struggled as a general purpose tool for DP estimation as bounding the sensitivity of \( f(x;)\) is difficult. Motivated by model misspecification, Bissiri et al.  showed that the posterior update

\[^{}(|D)()\{-_{i=1}^{n} (D_{i},f(;))\},\] (3)

assigning high posterior density to parameters that achieved small loss on the data, provides a coherent means to update prior beliefs about parameter \(_{0}^{}:=*{arg\,min}_{}(D,f( ;))g(D)dz\) after observing data \(D g()\). The Gibbs posterior in (2) is recovered using the weighted negative log-likelihood \((D,f(;))=-w f(D;)\), and the standard posterior for \(w=1\). This demonstrates that Bayesian inference learns about \(_{0}^{}:=*{arg\,min}_{}  f(D;)g(D)dD=*{arg\,min}_{}(g ||f(;))\)[11; 80].

The framework of  provides the flexibility to choose a loss function with bounded sensitivity. An alternative, loss function that continues to depend on \(\) through the likelihood \(f(;)\) is the

    & **Unbounded** & **Likelihood** & **Prior** & \(\) & **Unbiased+** \\  & **Features** & **Restriction** & **Restriction** & & **Consistent** \\  Foulds et al.  & ✗ & exponential family + & conjugate & \(0\) & ✓ \\  & & bounded sufficient statistics & & & \\ Bernstein and Shel-don [12; 13] & ✗ & exponential family + & conjugate & \(0\) & ✓ \\  & & bounded sufficient statistics & & & \\ Minami et al.  & ✗ & convex + Lipschitz & strongly convex & \(>0\) & ✓ \\  & & log-density & & & \\ Wang et al.  & ✗ & bounded log-density & & proper & \(0\) & ✓ \\  & & convex log-density & & strongly convex & \(0\) & ✓ \\  & & bounded gradients & & none & \(>0\) & ✗ \\  & & bounded density & & proper & \(0\) & ✓ \\   

Table 1: Requirements of different DP estimation techniques; sorted in decreasing strength of the restrictions imposed on the likelihood.

\(\)-divergence loss  for \(>1\)

\[^{()}(D,f(;))=-f(D;)^{-1}+ f(;)^{}d,\] (4)

so called as \(_{}_{D g}[^{()}(D,f(;)) ]=_{}D_{B}^{()}(g||f(;))\) where \(D_{B}^{()}(g||f)\) is the \(\)-divergence defined in Section A. The first term in (4) contains the negative likelihood, therefore parameters that make the data likely achieve low loss. However, it is raised to the power \(-1>1\) prescribing relatively smaller loss to observations unlikely under that parameter than the log-likelihood. A key feature of (4) is that while \(_{f 0}- f=\), \(_{f 0}-}{-1}=0\) for \(>1\). The second integral term only depends on the parameters and ensures the \(\)D loss can learn the data generating parameters. Setting \(=1\) recovers the negative log-likelihood. We refer to updating using (3) and loss (4) as \(\)D-Bayes. Note that this update is general, and can be applied for any choice of prior \(()\), and density/mass function \(f(y;)\). Using (4) for inference was first proposed by Basu et al.  and extended by Ghosh and Basu  to the Bayesian paradigm. Because of its favourable robustness properties, the \(\)-divergence has since been deployed for a variety of examples within modern statistical inference [e.g. 47, 34, 48, 33, 79] and deep learning [e.g. 3, 36, 2, 19, 48].

A particularly convenient feature of inference based on divergences is that they are uniquely minimised to \(0\) when \(f=g\). Therefore, if there exists \(_{0}\) such that \(g()=f(;_{0})\), i.e. the model is correctly specified for \(g\), then \(_{}D_{B}^{()}(g()||f(;))=_ {}(g()||f(;))=_{0},\) and the \(\)D-Bayes posterior will learn about the same parameter as the Gibbs posterior (2). Further, the general Bernstein-von-Mises theorem for generalised posteriors [Theorem 4; 65] can be applied to the \(\)D-Bayes posterior (see Theorem 3) to show that \(^{()}(|D)\) is asymptotically Gaussian and concentrates around \(_{0}^{^{()}}:=_{}D_{B}^{()}(g( )||f(;))\) as \(n\). This proves useful when establishing consistency and asymptotic efficiency, see Section A.2. When the model is misspecified, i.e. there exists no \(_{0}\) such that \(g()=f(;_{0})\), then \(_{0}^{^{()}}_{}(g() ||f(;))\) and the \(\)D-Bayes posterior learns a different parameter to the standard posterior. However, several works have argued that it provides more desirable inference  and decision-making  under model misspecification, and stability  to the specification of the model. We now show that we can leverage the favourable robustness properties of \(\)D-Bayes to obtain general-use DP OPS estimates.

DP one-posterior-samplingRather than bounding \( f(;)\), we replace it in (3) with the \(\)D loss from (4) which is naturally bounded when the density is bounded.

**Condition 1** (Boundedness of the model density/mass function).: _The model density or mass function \(f(;)\) is such that there exists \(0<M<\) such that \(f(;) M,\)._

**Lemma 1** (Bounded sensitivity of the \(\)D-Bayes loss).: _Under Condition 1 the sensitivity of the \(\)D-Bayes-loss for any \(>1\) is \(|^{()}(D,f(;))-^{()}(D^{},f(; ))|}{-1}\)._

Bounding \(f\) rather than its logarithm is considerably more straightforward: discrete likelihoods are always bounded by 1, while continuous likelihoods can be guaranteed to be bounded under mild assumptions. We will see in Example 2 that such a bound can be provided in a Gaussian regression model by truncating the support of the variance parameter \(^{2}\) from below. Under Condition 1, Theorem 1 proves \((,0)\)-DP of \(\)D-Bayes OPS. Theorem 2 establishes the consistency of \(\)D-Bayes OPS and Proposition 1 establishes its efficiency, based on the definitions given by . Condition 3, stated fully in Section A.2 requires that the \(\)D-Bayes loss can be approximated by a quadratic form and requires that as \(n\) grows the \(\)D-Bayes loss is uniquely minimied.

**Theorem 1** (Differential privacy of the \(\)D-Bayes posterior).: _Under Condition 1, a draw \(\) from the \(\)D-Bayes posterior \(^{^{()}}(|D)\) in (3) is \((}{-1},0)\)-differentially private._

**Theorem 2** (Consistency of \(\)D-Bayes sampling).: _Under Condition 3, stated in Section A.2,_

1. _a posterior sample_ \(^{^{()}}(|D)\) _is a consistent estimator of_ \(_{0}^{^{()}}\)_._
2. _if data_ \(D_{1},,D_{n} g()\) _were generated such that there exists_ \(_{0}\) _with_ \(g(D)=f(D;_{0})\)_, then_ \(^{^{()}}(|D)\) _for all_ \(1<<\) _is consistent for_ \(_{0}\)

**Proposition 1** (Asymptotic efficiency).: _Under Condition 3, stated in Section A.2, \(^{^{()}}(|D)\) is asymptotically distributed as \((-_{0}^{^{()}})}}{{}}(0,(H_{0}^{^{()}})^{-1} K_{0}^{^{()}}(H_{0}^{()})^{-1}+(H_{0}^{^{()}})^{-1}),\) where \(K_{0}^{^{()}}\) and \(H_{0}^{^{()}}\) are the gradient cross-product and Hessian matrices for the \(\)D loss and are defined in (5) and (6)._

OPS for classification and regressionNote, unlike other methods, this DP guarantee does not require bounding features or response. We consider two explicit examples.

**Example 1** (Binary classification).: _Consider a classifier for \(y\{0,1\}\) taking the logistic value of arbitrary function \(m:\), \(p(y=1;X,)=1/(1+(-m(X,)))\), depending on predictors \(X\) and parameters \(\). Clearly, \(0 f(y=1;X,) 1\) independently of the functional form of \(m(X,)\), which guarantees \((,0)\)-DP of the \(\)D-Bayes ops._

Taking \(m(X,)=X^{T}\) recovers logistic regression. Both the output perturbation of  and the Gibbs posterior (2) taking the Gibbs weight of  can also provide DP estimation for logistic regression. We show in Section 5 that \(\)D-Bayes provides superior inference to these methods for the same privacy budget. Figure 1 (left) compares the standard Bayesian posterior for logistic regression with the Gibbs posterior (\(w=0.09\)) and the \(\)D-Bayes posterior (\(=1.33\)); both methods achieve DP estimation with \(=6\). The \(y\)-axis is \((y=1;X,)\{(y=1;X,)\}\), the term that multiplies the prior in the updates (2) and (3). The figure shows that for the same privacy, the \(\)D-Bayes update more closely resembles the standard Bayesian update allowing \(\)D-Bayes ops to produce more precise inference. Unlike  and , \(\)D-Bayes also does not require bounding the features to guarantee DP.

Modern machine learning, however, often requires non-linear models, and \(\)D-Bayes allows DP estimation for these as well. For example, we can choose \(m(X,)=(X,)\) where NN is a neural network parameterised by \(\). See  for more details on Bayesian inference for neural networks. Unlike logistic regression, the log-likelihood of a neural network classifier is not convex and therefore the methods of  and  cannot be applied. This necessitates the application of DPSGD which adds noise to minibatch gradient evaluations in SGD and clips the gradients at some value to artificially bound their sensitivity-see  for a Bayesian extension. In contrast, \(\)D-Bayes gets rid of the need to bound the neural network's sensitivity. It further allows for DP estimation beyond classification.

**Example 2** (Gaussian regression).: _Consider a Gaussian model \(f(y;X,,^{2})=(y;m(X,),^{2})\) regressing univariate \(y\) on predictors \(X\) using any mean function \(m:\) and parameter \(\) where \(^{2}>0\) is the residual variance. Provided there exists a lower bound \(0<s^{2}<^{2}\), then \(0 f(y;X,,^{2}) 1/(s)\) independent of \(m(X,)\). \(\)D-Bayes ops for such a model is \((,0)\)-DP with \(=2/((-1)(s)^{(-1)})\)._

Ensuring Condition 1 for a Gaussian likelihood model requires that the support of the variance parameter is bounded away from 0. Such a bound is not limiting. For example, the standard conjugate

Figure 1: **Left:** Comparison of \((y=1;X,)\{(y=1;X,)\}\) for the Gibbs and \(\)D-Bayes posteriors achieving DP with \(=6\) with the standard logistic function for _binary classification_ with \(m(X,)=X\). The \(\) associated with the \(\)D-Bayes posterior is closer to that of the Bayes posterior for any \(m(X,)\) than that of the Gibbs posterior for the same privacy level. **Right:** Comparison of \((y;X,)\) for the Gibbs posterior and the \(\)D-Bayes posterior for _Gaussian regression_ with the standard log-likelihood. The \(\)D-Bayes posterior allows for DP estimation while the Gibbs posterior does not as any choice for \(w>0\) fails to bound the loss sensitivity.

inverse-gamma prior puts vanishing prior density towards \(0\), and in situations where a natural lower bound is not available, adding independent and identically distributed zero-mean Gaussian noise with variance \(s^{2}\) to the observed responses \(y\) ensures this bound without changing the mean estimation.

One popular choice for the mean function is \(m(X,)=X^{T}\) corresponding to standard linear regression. This is an exponential family model with conjugate prior, and DP estimation can be done using e.g. [13; 12]. They, however, require artificial bounds on the feature and response spaces. Again, the \(\)D-Bayes estimation also holds for more complex mean functions such as neural networks - see  for \(\)D-Bayes neural network regression. Figure 1 (right) illustrates how \(\)D-Bayes bounds the sensitivity for Gaussian regression models while the Gibbs posterior cannot. The log-likelihood of the Gaussian distribution is unbounded and while multiplying this with \(w<1\) reduces the slope, this does not change its bound. The \(\)D-Bayes, on the other hand, provides a bounded loss function.

## 4 Extending the privacy beyond one-posterior sampling

While Ops has been shown to provide DP guarantees for perfect samples, the Ops posterior is typically not available in closed form and as a result, some approximate sampling methods such as Markov Chain Monte Carlo (MCMC) are required. Note that this is not only a limitation of OPS, but sampling from the intractable exponential mechanism [63; 75] in general. Proposition 2 taken from  investigates the DP properties of the approximation.

**Proposition 2** (Proposition 12 of ).: _If sampling from \((|D)\) is \((,)\)-DP and for all \(D\) there exists approximate sampling procedure \(p_{D}()\) with \(|(|D)-p_{D}()|d\), then sampling from \(p_{D}()\) is \((,^{})\)-DP with \(^{}=+(1+e^{})\)._

Proposition 2 establishes that if the MCMC chain has converged to the target distribution i.e. \( 0\), then the DP of the exact posterior is shared by the approximate sampling. Proposition 3 of  is the same result for \(=0\). For the sufficiently well-behaved Gibbs posteriors (i.e. with Lipschitz convex loss function and strongly convex regulariser), Minami et al.  provide an analytic stepsize and number of iterations, \(N\), that guarantees an (unadjusted) Langevin Monte-Carlo Markov Chain is within \(\) of the target. While the Gibbs posterior for logistic regression satisfies these conditions, the Gibbs posterior for more general tasks and the \(\)D-Bayes loss will in general not be convex.

Previous contributions [81; 66; 29] have assumed that the MCMC kernel has converged. Seeman et al.  observed that if the MCMC algorithm is geometrically ergodic achieving a \(^{}\) smaller than order \(1/n\) and preventing data leakage requires the chain to be run for at least order \(N=(n)\) iterations. For our experiments we used the No-U-turn Sampler [NUTS; 38] version of Hamiltonian Monte Carlo [HMC; 24] implemented in the Stan probabilistic programming language . The geometric ergodicity of HMC was established in  and Stan provides a series of warnings that indicate when the chain does not demonstrate geometric ergodicity . Running Stan for sufficiently many iterations to not receive any warnings provides reasonable confidence of a negligible \(^{}\). As an alternative to measuring convergence, we below review approximate DP sampling approaches where \(\)D-Bayes can be applied to guarantee bounded density. We hope that the emergence of \(\)D-Bayes, as a general purpose OPS method to provide consistent DP estimation, motivates further research into private sampling methods for Ops.

DP MCMC methodsAlternatively to attempting to release one sample from the exact posterior, much work has focused on extending OPS to release a Markov chain that approximates the Gibbs posterior, incurring per iteration privacy costs. Examples include the privatisation of Stochastic Gradient Langevin Dynamics [SGLD; 82; 81], the penalty algorithm , Hamiltonian Monte Carlo [DP-HMC 73], Baker's acceptance test [37; 8; 76], and rejection sampling . Similar to DPSGD, these algorithms all rely on either subsampled estimates of the model's log-likelihood or its gradient to introduce the required noise and run the chain until the privacy budget has been used up. Just like the Gibbs posterior (2), the \(\)D-Bayes posterior-using (4) and (3)-also contains a sum of loss terms that can be estimated via subsampling, making the \(\)D-Bayes similarly amenable. What is more, many of these algorithms previously listed [85; 83; 37; 73; 81; 56; 84] require the boundedness of the sensitivity of the log-likelihood, or its derivative. Proposition 3 shows that under Condition 1 which ensures that \(\)D-Bayes OPS is DP, or for gradient based samplers Condition 2, many of these DP samplers can be used to sample from the \(\)D-Bayes posterior without compromising DP.

**Condition 2** (Boundedness of the model density/mass function gradient).: _The model density or mass function \(f(;)\) is such that there exists \(0<G^{()}<\) such that \(_{}f(D;) f(D;)^{-2} _{} G^{()},\)._

**Proposition 3** (dp-mcmc methods for the \(\)D-Bayes-Posterior).: _Under Condition 1, the penalty algorithm [Algorithm 1; 83], dp-hmc[Algorithm 1; 73] and dp-Fast MH [Algorithm 2; 85] and under Condition 2 dp-sgld[Algorithm 1; 56] can produce \((,)\)-dp estimation from the \(\)D-Bayes posterior with \(>0\) without requiring clipping._

Perfect samplingAn alternative approach is to seek to modify MCMC chains to allow for exact samples from the posterior. Seeman et al. Priviste the perfect sampling algorithm of , which introduces an 'artificial atom' \(a\) into the support of the target and uses an augmented MCMC kernel to move between the atom and the rest of the support. While we believe these approaches to be very promising and in principle trivially applicable to \(\)D-Bayes OPS, Lee et al.  find considerable instability to choices for the underlying MCMC chain, and therefore more investigation is required.

Attacking one-posterior samplesIn order to quantify the data leakage from approximate sampling schemes, we run the strongest privacy attacks for dp auditing, namely membership inference attacks (mia). In mia, an adversary tries to predict whether an observation was part of the training data set or not. This attack corresponds directly to the dp guarantee presented in Definition 1: Given any two neighbouring data sets, \(D\) and \(D^{}\), an attacker should not be able to confidently predict which data set was used in training if they observe the final statistical estimate \(\). Indeed, Jagielski et al.  have shown that the false positive and false negative rates of mia attacks can be used to audit the dp of an algorithm directly. In the pursuit of tight auditing of dp algorithms, Nasr et al.  have proposed worst-case attacks where \(|D|+|D^{}|=1\) are chosen to maximise attack performance. These attacks are beneficial to uncover whether an algorithm violates its promised dp guarantees as published works have repeatedly been shown to suffer from faulty implementations or mistakes in proving dp[69; 78]. We are the first to consider such attacks for ops.

For a number of rounds, we 1) generate two neighbouring data sets, \(D\) and \(D^{}\), 2) sample \(m(0.5)\), 3) if \(m=1\) return \((D^{})\) or if \(m=0\) return \((D)\), and then 4) predict given Remark 1 which data set \(\) was trained on. Without loss of generality, we assume \(m=1\). We follow  in defining the objective of the mia attack as \((,D,D^{}):=p(m=1;,D,D^{})\), i.e. the probability that \(\) was trained on \(D^{}\) after observing \(,D\), and \(D^{}\).

**Remark 1**.: _Let \(p(|D)\) be the density of the privacy mechanism--i.e the Laplace density for  or the posterior (i.e. (2) or (3)) for ops. An attacker estimating \((,D,D^{})=|D^{})} {(p(|D)+p(|D^{}))}\) is Bayes optimal. For ops, \((,D,D^{})=\{(D^{}_{l};f(; ))-(D;)\}\{(D_{l};f(; ))-(D^{}_{l};f(;))\}(|D)d\) where \(D,D^{}\) s.t. \(D D^{}=\{D_{l}\}\) and \(D^{} D=\{D^{}_{l}\}\) (see Appendix A.5)._

## 5 Experimental results

Appendix B contains additional experimental details and results. Our code can be found at https://github.com/sghalebikesabi/beta-bayes-ops.

Data setsThe evaluations are conducted on simulated and UCI  data sets. For the former, we generate \(d\)-dimensional features from a multivariate normal distribution with the identity matrix as covariance matrix, sample the model parameters from a normal distribution with mean 0 and standard deviation 3 (i.e. a \(d\) dimensional vector for the logistic regression), and simulate the label according to the assumed likelihood model. For the latter, we have included the two classification data sets that were previously analysed in other applications of ops (adult and abalone) [66; 81], in addition to other popular UCI data sets. We report the test performance on random test splits that constitute 10% of the original data. We further scale the features in all data sets to lie within 0 and 1. This is a requirement for the methods proposed by [81; 66; 18], whereas \(\)D-Bayes guarantees DP for unbounded data.

Logistic regressionFigure 2 compares \(\)D-Bayes, Chaudhuri et al.  and Minami et al.  (with \(=10^{-5}\)). Note that  still presents a widely-used implementation of DP logistic regression . We consider two implementations of : one where \(=1/(9n)\) in (1) decreases in thenumber of samples, consistent with the Bayesian paradigm the effect of the regulariser diminishes as \(n\) and corresponds to a Gaussian prior with variance of 3, and another where \(=1/9\) is fixed - see A.4.1 for further discussion. The first leads to unbiased but inconsistent DP-inference while the second is consistent at the cost of also being biased. We report the RMSE between the estimated parameter \((D)\) and the true data generating parameter on the simulated data sets, and the ROC-AUC on UCI data. For a compairson of each method with its non-private equivalent, please see Figure 6 in the appendix. We also introduce a new metric, termed _correct sign accuracy_ that computes the proportion of coefficients that are 'correctly' estimated. Please refer to Figures 7 and 8 in the appendix.

In simulations, we observe that as \(n\) increases, \(\)D-Bayes achieves the smallest RMSE, illustrating the increased efficiency we argued for in Section 3. The extent to which \(\)D-Bayes dominates is greater for large \(\). The ROC-AUC curves show that this better estimation of parameters generally corresponds to greater ROC-AUC. For the UCI data we see that for increasing \(n\)\(\)D-Bayes outperforms the other methods and Figure 6 shows it achieves performance that is very close to the unprivatised analogue.

Figure 3: Test set predictive log RMSE of DP estimation for neural network regression as the number of observations \(n\) increases on simulated and UCI data. See Appendix Figure 6 for a comparison of each method with its private counterpart.

Figure 2: Parameter log RMSE and test-set ROC-AUC of DP estimation for logistic regression as the number of observations \(n\) increases. We have upper bounded the axis of the log RMSE \(()\), as  performs poorly when the regularisation term decreases in \(n\).

Neural network regressionFigure 3 compares \(\)D-Bayes and DPSGD  (with \(=10^{-5}\)) for training a one-layer neural network with 10 hidden units. The learning rate, number of iterations, and clipping norm for DPSGD were chosen using validation splits to maximise its performance. For small \(n\) the DPSGD is preferable in both simulations and the UCI data, but as the number of observations increases, the test set predictive RMSE of \(\)D-Bayes outperforms that of DPSGD. Note that DPSGD is currently the best-performing optimiser for DP neural network training . It privatises each gradient step, guaranteeing privacy for each parameter update, while \(\)D-Bayes only guarantees DP for a perfect sample from the Bayesian posterior. DPSGD thus provides stronger privacy guarantees, and is more computationally efficient as MCMC scales poorly to high-dimensional feature spaces. We hyperparameter-tune the number of epochs, learning rate, and batch size of DP-SGD on a validation data set and use the same parameters on SGD for a fair comparison. Thus, the performance of SGD could be improved by a different choice of hyperparameters.

Membership inference attacksWe implement the Bayes optimal attacker for the case of logistic regression. As the solution of the logistic regression is not defined when only observations from a single class are present, we choose \(D=\{(1,1),(0,0)\}\) and \(D^{}=\{(-1,1),(0,0)\}\) to achieve optimal attack results. Figure 4 compares the attack success rate with the log RMSE achieved on simulated data (\(n=1000,d=2\)). As \(\) increases, the attack success rate of all methods increases and their RMSE decreases, except  with \(=1/9\) whose RMSE does not decrease because of bias. Fixing the attack success rates, \(\)D-Bayes generally achieves the lowest RMSE. Importantly, \(\)D-Bayes is more efficient than  for the same attack success rate, where  has exact DP guarantees.

## 6 Discussion

We showed that \(\)D-Bayes OPS produces consistent parameter estimates that are \((,0)\)-DP provided that the model's density or mass function \(f(;)\) was bounded from above. \(\)D-Bayes OPS improved the precision of DP inference for logistic regression and extends to more complex models, such as neural network classification and regression. Such extensions facilitate DP estimation for semi-parametric models where consistent inference for a linear predictor is required, but a complex model is used to capture the remaining variation . Extensions of this work could consider dividing the privacy budget to release more than one sample paving the way for parameter inference as well as estimation. Bayesian inference with different divergences or discrepancies is becoming increasingly popular [28; 42; 61; 4], and their suitability for DP estimation could be investigated following our example and results. Further, the \(\)D-loss is applicable beyond Bayesian methods, it could also be used in place of clipping gradients in algorithms such as DP-SGD .

The main limitation of OPS methods is that they prove DP for a sample from the exact posterior which is never tractable. Instead, MCMC samples approximate samples from the exact posterior and convergence of the MCMC sampler must be ensured to avoid leaking further privacy. Future work should investigate whether convergence guarantees are possible for \(\)D-Bayes, taking advantage of its natural boundedness, and consider which of the DP-MCMC methods introduced in Section 4 makes best use of the privacy budget to sample a chain from \(\)D-Bayes. A further limitation of OPS is the computational burden required to produce posterior samples, particularly in larger neural networks with many parameters. Such a cost and is mitigated by the fact that inference can only be run once to avoid leaking privacy and that only one posterior sample is required. But further research is required to tackle these computational challenges, including scaling MCMC to large neural networks or developing DP variational inference approaches [40; 71; 44] for the \(\)D-Bayes posterior. We hope that the existence of a general-purpose method for DP estimates through Bayesian sampling with improved performance can stimulate further research in these directions.