# CoDet: Co-Occurrence Guided Region-Word Alignment for Open-Vocabulary Object Detection

Chuofan Ma\({}^{1}\) Yi Jiang\({}^{2}\) Xin Wen\({}^{1}\) Zehuan Yuan\({}^{2}\) Xiaojuan Qi\({}^{1}\)

\({}^{1}\)The University of Hong Kong \({}^{2}\)ByteDance Inc.

###### Abstract

Deriving reliable region-word alignment from image-text pairs is critical to learn object-level vision-language representations for open-vocabulary object detection. Existing methods typically rely on pre-trained or self-trained vision-language models for alignment, which are prone to limitations in localization accuracy or generalization capabilities. In this paper, we propose CoDet, a novel approach that overcomes the reliance on pre-aligned vision-language space by reformulating region-word alignment as a co-occurring object discovery problem. Intuitively, by grouping images that mention a shared concept in their captions, objects corresponding to the shared concept shall exhibit high co-occurrence among the group. CoDet then leverages visual similarities to discover the co-occurring objects and align them with the shared concept. Extensive experiments demonstrate that CoDet has superior performances and compelling scalability in open-vocabulary detection, _e.g._, by scaling up the visual backbone, CoDet achieves 37.0 AP\({}^{}_{}\) and 44.7 AP\({}^{}_{}\) on OV-LVIS, surpassing the previous SoTA by 4.2 AP\({}^{}_{}\) and 9.8 AP\({}^{}_{}\). Code is available at https://github.com/CVMI-Lab/CoDet.

## 1 Introduction

Object detection is a fundamental vision task that offers object-centric comprehension of visual scenes for various downstream applications. While remarkable progress has been made in terms of detection accuracy and speed, traditional detectors  are mostly constrained to a fixed vocabulary defined by training data, _e.g._, 80 categories in COCO . This accounts for a major gap compared to human visual intelligence, which can perceive a diverse range of visual concepts in the open world. To address such limitations, this paper focuses on the open-vocabulary setting of object detection , where the detector is trained to recognize objects of arbitrary categories.

Recently, vision-language pretraining on web-scale image-text pairs has demonstrated impressive open-vocabulary capability in image classification . It inspires the community to adapt this paradigm to object detection , specifically by training an open-vocabulary detector using region-text pairs in detection or grounding annotations . However, unlike free-form image-text pairs, human-annotated region-text pairs are limited and difficult to scale. Consequently, a growing body of research  aims to mine additional region-text pairs from image-text pairs, which raises a new question: _how to find the alignments between regions and words?_ (Figure 0(a))

Recent studies typically rely on vision-language models (VLMs) to determine region-word alignments, for example, by estimating region-word similarity . Despite its simplicity, the quality of generated pseudo region-text pairs is subject to limitations of VLMs. As illustrated in Figure 0(b), VLMs pre-trained with image-level supervision, such as CLIP , are largely unaware of localization quality of pseudo labels . Although detector-based VLMs  mitigate this issue to some extent, they are initially pre-trained with a limited number of detection or grounding concepts,resulting in inaccurate alignments for novel concepts . Furthermore, this approach essentially faces a chicken-and-egg problem: obtaining high-quality region-word pairs requires a VLM with object-level vision-language knowledge, yet training such a VLM, in turn, depends on a large number of aligned region-word pairs.

In this work, instead of directly aligning regions and words with VLMs, we propose leveraging region correspondences across images for co-occurring concept discovery and alignment, which we call CoDet. Figure 1c illustrates the idea. Our key motivation is that objects corresponding to the same concept should exhibit consistent visual similarity across images, which provides visual clues to identity region-word correspondences. Based on this intuition, we construct semantic groups by sampling images that mention a shared concept in their captions, from which we can infer that a common object corresponding to the shared concept exists across images. Subsequently, we leverage cross-image region similarity to identify regions potentially containing the common object, and construct a prototype from them. The prototype and the shared concept form a natural region-text pair, which is then adopted to supervise the training of an open-vocabulary object detector.

Unlike previous works, our method avoids the dependence on a pre-aligned vision-language space and _solely relies on the vision space_ to discover region-word correspondences. However, there could exist multiple co-occurring concepts in the same group, and even the same concept may still exhibit high intra-category variation in appearance, in which case general visual similarity would fail to distinguish the object of interest. To address this issue, we introduce text guidance into similarity estimation between region proposals, making it concept-aware and more accurately reflecting the closeness of objects concerning the shared semantic concept.

The main contributions of this paper can be summarized as follows:

* We introduce a novel perspective in discovering region-word correspondences from image-text pairs, which bypasses the dependence on a pre-aligned vision-language space by reformulating region-word alignment as a co-occurring object discovery problem.
* Building on this insight, we propose CoDet, an open-vocabulary detection framework that learns object-level vision-language alignment directly from web-crawled image-text pairs.
* CoDet consistently outperforms existing methods on the challenging OV-LVIS benchmark and demonstrates superior performances in cross-dataset detection on COCO and Objects365.
* it achieves \(23.4/29.4/37.0\) mask \(_{}\) with ResNet50/Swin-B/EVA02-L backbone, outperforming previous SoTA at a comparable model size by \(0.8/3.1/4.2\) mask \(_{}\), respectively.

## 2 Related Work

Zero-shot object detection (ZSD) leverages language feature space for generalization to unseen objects. The basic idea is to project region features to the pre-computed text embedding space (_e.g_., GloVe ) and use word embeddings as the classifier weights [2; 10; 36]. This presents ZSD with the flexibility of recognizing unseen objects given its name during inference. Nevertheless,

Figure 1: **Illustration of different region-text alignment paradigms. (a): example image-text pair, and region proposals generated by a pre-trained region proposal network; (b): a pre-trained VLM (_e.g_., CLIP ) is used to retrieve the box with the highest region-word similarity concerning the query text, which yet exhibits poor localization quality; (c) our method overcomes the reliance on VLMs by exploring visual clues, _i.e_., object co-occurrence, within a group of image-text pairs containing the same concept (_e.g_., frisbee ). _Best viewed in color._**

ZSD settings restrict training samples to come from a limited number of seen classes, which is not sufficient to align the feature space of vision and language. Although some works [63; 42] try to overcome this limitation by hallucinating novel classes using Generative Adversarial Network , there is still a large performance gap between ZSD and its supervised counterparts.

Weakly supervised object detection (WSD)exploits data with image-level labels to train an object detector. It typically treats an image as a bag of proposals, and assigns the image label to these proposals through multiple instance learning [4; 9; 3; 46]. By relieving object detection from costly instance-level annotations, WSD is able to scale detection vocabulary with cheaper classification data. For instance, recent work Detic  greatly expands the vocabulary of detectors to twenty-thousand classes by leveraging image-level supervision from ImageNet-21K . However, WSD still requires non-trivial annotation efforts and has a closed vocabulary during inference.

Open-vocabulary object detection (OVD)is built upon the framework of ZSD, but relaxes the stringent definition of novel classes from 'not seen' to 'not known in advance', which leads to a more practical setting . Particularly, with recent advancement of vision-language pre-training [35; 25; 55; 54], a widely adopted approach of OVD is to transfer the knowledge of pre-trained vision-language models (VLMs), _e.g._, CLIP , to object detectors through distillation [18; 50] or weight transfer [33; 26]. Despite its utility, performances of these methods are arguably restricted by the teacher VLM, which is shown to be largely unaware of fine-grained region-word alignment [59; 6]. Alternatively, another group of works utilize large-scale image-text pairs to expand detection vocabulary [57; 59; 56; 14; 27; 28; 52], sharing a similar idea as WSD. Due to the absence of regional annotations in image-caption data, these methods typically rely on pre-trained or self-trained VLMs to find region-word correspondences, which are prone to limitations in localization accuracy or generalization capabilities. Our method is orthogonal to all the aforementioned approaches in the sense that it does not explicitly model region-word correspondences, but leverage region correspondences across images to bridge regions and words, which greatly simplifies the task.

Cross-image Region Correspondenceis widely explored to discover semantically related regions among a collection of images [20; 41; 45; 44; 30]. Based on the observation that modern visual backbones provide consistent semantic correspondences in the feature space [58; 20; 47; 24; 60; 1], many works take a heuristic approach  or use clustering algorithms [20; 8; 23; 49] for common region discovery. Our method takes inspiration from these works to discover co-occurring objects across image-text pairs, with newly proposed text guidance.

## 3 Method

In this section, we present CoDet, an end-to-end framework exploiting image-text pairs for open-vocabulary object detection. Figure 2 gives an overview of CoDet. We first provide a brief introduction to the OVD setup (Sec. 3.1). Then we discuss how to reformulate region-word alignment as a co-occurring object discovery problem (Sec. 3.2), which is subsequently addressed by CoDet (Sec. 3.3). Finally, we summarize the overall training objectives and inference pipelines of CoDet (Sec. 3.4).

### Preliminaries

Task formulation.In our study, we adopt the classical OVD problem setup as in OVR-CNN . Specifically, box annotations are only provided for a predetermined set of base categories \(^{}\) during training. While at the test phase, the object detector is required to generalize beyond \(^{}\) to detect objects from novel categories \(^{}\). Notably, \(^{}\) is not known in advance to simulate open-world scenarios. This implies the object detector needs to possess the capability to recognize potentially any object based on its name. To achieve this goal, we additionally leverage image-text pairs with an unbounded vocabulary \(^{}\) to extend the lexicon of the object detector.

OVD framework.Our method is built on the two-stage detection framework Mask-RCNN . To adapt it for the open-vocabulary setting, we follow the common practice [18; 61] to decouple localization from classification, by replacing the class-specific localization heads with class-agnostic ones that produce a single box or mask prediction for each region proposal. Besides, to enable open-vocabulary classification of objects, the fixed classifier weights are substituted with dynamic text embeddings of category names, which are generated by the pre-trained text encoder of CLIP .

### Aligning Regions and Words by Co-occurrence

Due to the absence of box annotations, a core challenge of utilizing image-text pairs for detection training is to figure out the fine-grained alignments between regions and words. To put it formally, for an image-text pair \( I,T\), \(R=\{r_{1},r_{2},...,r_{|R|}\}\) denotes the set of regions proposals extracted from image \(I\), and \(C=\{c_{1},c_{2},...,c_{|C|}\}\) denotes the set of concept words extracted from caption \(T\). Under weak caption supervision, we can assume the presence of a concept \(c\) in \(T\) indicates the existence of at least one region \(r\) in \(I\) containing the corresponding object. However, the concrete correspondence between \(c\) and \(r\) remains unknown.

To solve the problem, we propose to explore the global context of caption data and align regions and words via co-occurrence. Specifically, for a given concept \(c\), we construct a concept group \(G\) comprising all image-text pairs that mention \(c\) in their captions. This grouping effectively clusters images that contain objects corresponding to concept \(c\). Consequently, if \(G\) is large enough, we can induce that the objects corresponding to concept \(c\) will be the most common objects among images in \(G\). This natural correlation automatically aligns the co-occurring objects in \(G\) to concept \(c\). We thus reduce the problem of modeling cross-modality correspondence (region-word) to in-modality correspondence (region-region), which we address in the next section.

### Discovering Co-occurring Objects across Images

Intuitively, candidate region proposals containing the co-occurring object should exhibit consistent and similar visual patterns across the images. We thus take a similarity-driven approach to discover these proposals. As illustrated in Figure 2, during training, we only sample a mini-group of images from the concept group as inputs in consideration of efficiency. In the mini-group, we _iteratively_ choose one image as query image, and leave the rest as support images. Note that the regional proposals for each image are cached to avoid re-computation when swapping query and support images. The basic idea is to discover co-occurring objects in the query image from region proposals that have close neighbors across the support images. To fulfill this purpose, we introduce text-guided similarity estimation and similarity-based prototype discovery in the following paragraphs.

Similarity-based prototype discovery.Since modern visual backbones provide consistent feature correspondences for visually similar regions across images [58; 20; 47; 24; 60; 1], a straightforward way to identify co-occurring objects is to measure the cosine similarity between features of region proposals. Concretely, we calculate the pairwise similarity of region proposals between the query image and the support images. This yields a similarity matrix \(^{n mn}\), where \(n\) stands for the number of proposals per image, and \(m\) stands for the number of support images. Intuitively,

Figure 2: **Overview of CoDet. Our method learns to jointly discover region-word pairs from a group of image-text pairs that mention a shared concept in their captions (_e.g._, dog in the figure). We identify the co-occurring objects and the shared concept as natural region-word pairs. Then we leverage inter-image region correspondences, _i.e._, region-region similarity, with text guidance to locate the co-occurring objects for region-word alignment.**

co-occurring regions should exhibit high responses (similarities) in the last dimension of \(\). But instead of using hand-crafted rules as in , we employ a two-layer MLP, denoted as \(\), to derive co-occurrence from \(\). \(\) is trained to estimate the probability of each region proposal in the query images as a co-occurring region, solely conditioned on \(\). Here, we do not explicitly supervise the output probability since there is no ground-truth annotation, but \(\) is encouraged to assign high probabilities for co-occurring regions to minimize the overall region-word alignment loss. Based on the estimated probability vector \(^{n}\), we obtain the prototypical region features for the co-occurring object via simple weighted sum:

\[_{p}=_{i=1}^{N}_{i}_{i},\ =_{N}(())\;.\] (1)

As the text label for this prototype naturally corresponds to the shared concept \(c\) in the mini-group. We can thus learn region-word alignment with a binary cross-entropy (BCE) classification loss:

\[_{}=_{}(_{p},c),\ _{}(,c)=-(_{c})- _{k c}(1-(_{k}))\;,\] (2)

where \(\) the classifier weight derived from \(^{}\), and \(()\) stands for the sigmoid function.

Text-guided region-region similarity estimation.However, general cosine similarity may not always truly reflect closeness of objects in the semantic space, as objects of the same category may exhibit significant variance in appearance. Moreover, there could exist multiple co-occurring concepts among the sampled images, which incurs ambiguity in identifying co-occurrence. To address the problems, we introduce text guidance into similarity estimation to make it concept-aware. Concretely, given features of two region proposals \(_{i}\), \(_{j}^{d}\), where \(d\) is the dimension of feature vectors, we additionally introduce \(_{c}^{d}\), the text embedding of concept \(c\) (the shared concept in the mini-group), to re-weight similarity calculation:

\[s_{ij}=}_{c}^{}(_{i}}{\|_{i}\|}_{j}}{\|_{j}\|}),\ }_{c}=_{c}|}{\|_{c}\|}\,,\] (3)

where "\(\)" denotes Hadamard product, "\(||\)" denotes absolute value operation, and "\(\|\|\)" denotes \(_{2}\)-normalization, respectively. Here, the rationale is that the relative magnitude of text features at different dimensions indicates their relative importance in the classification of concept \(c\). By weighting the image feature similarities with the text feature magnitudes, the similarity measurement can put more emphasis on feature dimensions that reflect more of the text features. Therefore, the re-weighted similarity metric provides a more nuanced and tailored measure of the proximity between objects in the context of a particular concept. It is noteworthy that we choose regional features from the output of the penultimate layer of the classification head (the last layer is the text embedding) so that they naturally reside in the shared feature space as text embeddings.

### Training and Inference

Following [61; 28], we train the model simultaneously on detection data and image-text pairs to acquire localization capability and knowledge of vision-language alignments. In addition to learning region-level alignments from region-word pairs discovered in Sec. 3.3, we treat image-text pairs as a generalized form of region-word pairs to learn image-level alignments. Particularly, we use a region proposal covering the entire image to extract image features, and encode the entire caption into language embeddings. Similar to CLIP , we consider each image and its original caption as a positive pair and other captions in the same batch as negative pairs. We then use a BCE loss similar to Eq. (2) to calculate the image-text matching loss \(_{}\). The overall training objective for this framework is:

\[(I)=_{}+_{}+ _{},&I^{}\\ _{}+_{},&I ^{}\,,\] (4)

where \(_{},_{},_{}\) are standard losses in the two-stage detector. For inference, CoDet does not require cross-image correspondence modeling as in training. It behaves like a normal two-stage object detector by forming the classifier with arbitrary language embeddings.

Experiments

### Benchmark Setup

#### Ov-Lviis

is a general benchmark for open-vocabulary object detection, built upon LVIS  dataset which contains a diverse set of 1203 categories of objects with a long-tail distribution. Following standard practice [18; 59], we set the 866 common and frequent categories in LVIS as base categories, and leave the 337 rare categories as novel categories. Besides, we choose CC3M  which contains 2.8 million free-from image-text pairs crawled from the web, as the source of image-text pairs. The main evaluation metric on OV-LVIS is the mask AP of novel (rare) categories.

#### Ov-Coco

is derived from the popular COCO  benchmark for evaluation of zero-shot and open-vocabulary object detection methods [2; 57]. It splits the categories of COCO into 48 base categories and 17 novel categories, while removing the 15 categories without a synset in the WordNet . As for image-caption data, following existing works [57; 61; 28], we use COCO Caption  training set which provides 5 human-generated captions for each image for experiments on OV-COCO. The main evaluation metric on OV-COCO is the box AP\({}_{50}\) of novel categories.

### Implementation Details

We extract object concepts from the text corpus of COCO Caption/CC3M using an off-the-shelf language parser . Remarkably, we filter out concepts without a synset in WordNet or outside the scope of the 'object' definition (_i.e_., not under the hierarchy of 'object' synset in WordNet) to clean the extracted concepts. For phrases of more than one word, we simply apply the filtering logic to the last word in the phrase. Subsequently, we remove concepts with a frequency lower than 100/20 in COCO-Caption/CC3M. This leaves 634/4706 concepts for COCO/CC3M.

For experiments on OV-LVIS, unless otherwise specified, we use CenterNet2  with ResNet50 as the backbone, following [61; 28]. For OV-COCO, Faster R-CNN with a ResNet50-C4  backbone is adopted. To achieve faster convergence, we initialize the model with parameters from the base class detection pre-training as in [61; 28]. The batch size on a single GPU is set to 2/8 for COCO/LVIS detection data and 8/32 for COCO Caption/CC3M caption data. The ratio between the detection batch and caption batch is set to 1:1 during co-training. Notably, a caption batch by default contains four mini-groups, where each mini-group is constructed by sampling 2/8 image-text pairs from the same concept group in COCO Caption/CC3M. We train the model for 90k iterations on 8 GPUs.

   Method & Backbone & Supervision & Strict & AP\({}_{}^{}\) & AP\({}_{}^{}\) & AP\({}_{}^{}\) & AP\({}_{}^{}\) \\  ViLD  & RN50-FPN & CLIP & ✓ & 16.6 & 24.6 & 30.3 & 25.5 \\ RegionCLIP  & RN50-C4 & Caption & ✓ & 17.1 & 27.4 & 34.0 & 28.2 \\ DetPro  & RN50-FPN & CLIP & ✓ & 19.8 & 25.6 & 28.9 & 25.9 \\ OV-DETR  & RN50-C4 & Caption & ✗ & 17.4 & 25.0 & 32.5 & 26.6 \\ PromptDet  & RN50-FPN & Caption & ✗ & 19.0 & 18.5 & 25.8 & 21.4 \\ Detic  & RN50 & Caption & ✗ & 19.5 & - & - & 30.9 \\ F-VLM  & RN50-FPN & CLIP & ✓ & 18.6 & - & - & 24.2 \\ VLDet  & RN50 & Caption & ✓ & 21.7 & 29.8 & 34.3 & 30.1 \\ BARON  & RN50-FPN & CLIP & ✓ & 22.6 & 27.6 & 29.8 & 27.6 \\ CoDet (Ours) & RN50 & Caption & ✓ & **23.4** & 30.0 & 34.6 & 30.7 \\  RegionCLIP  & R50x4 (87M) & Caption & ✓ & 22.0 & 32.1 & 36.9 & 32.3 \\ Detic  & SwinB (88M) & Caption & ✗ & 23.9 & 40.2 & 42.8 & 38.4 \\ F-VLM  & R50x4 (87M) & CLIP & ✓ & 26.3 & - & - & 28.5 \\ VLDet  & SwinB (88M) & Caption & ✓ & 26.3 & 39.4 & 41.9 & 38.1 \\ CoDet (Ours) & SwinB (88M) & Caption & ✓ & **29.4** & 39.5 & 43.0 & 39.2 \\  F-VLM  & R50x64 (420M) & CLIP & ✓ & 32.8 & - & - & 34.9 \\ CoDet (Ours) & EVA02-L (304M) & Caption & ✓ & **37.0** & 46.3 & 46.3 & 44.7 \\   

Table 1: **Comparison with state-of-the-art open-vocabulary object detection methods on OV-LVIS**. Caption supervision means the method learns vision-language alignment from image-text pairs, while CLIP supervision indicates transferring knowledge from pre-trained CLIP. The column ‘Strict’ indicates whether the method follows a strict open-vocabulary setting.

### Benchmark Results

Table 1 presents our results on OV-LVIS. We follow a strict open-vocabulary setting where novel categories are kept unknown during training, to ensure we obtain a generic open-vocabulary detector not biased towards specific novel categories. It can be seen that CoDet consistently outperforms SoTA methods in novel object detection. Especially, among the group of methods learning from caption supervision, CoDet surpasses all alternatives which rely on CLIP (RegionCLIP , OV-DETR), max-size prior (Detic ), or self-trained VLM (PromptDet , VLDet ) to generate pseudo region-text pairs, demonstrating the superiority of visual guidance in region-text alignment.

In addition, we validate the scalability of CoDet by testing with more powerful visual backbones, _i.e._, Swin-B  and EVA02-L . It turns out that our method scales up surprisingly well with model capacity - it leads to a +6.0/13.6 AP\({}_{}^{}\) performance boost by switching from ResNet50 to Swin-B/EVA02-L, and continuously enlarges the performance gains over the second best method with a comparable model size. We believe this is because stronger visual representations provide more consistent semantic correspondences across images, which are critical for discovering co-occurring objects among the concept group.

Table 2 presents our results on OV-COCO, where CoDet achieves the second best performance among existing methods. Compared with the leading method VLDet, the underperformance of CoDet can be mainly attributed to the human-curated bias in COCO Caption data distribution. That is, images in COCO Caption contain at least one of the 80 categories in COCO, which leads to highly concentrated concepts. For instance, roughly 1/2 of the images contain 'people', and 1/10 of the images contain 'car'. This unavoidably incurs many hard negatives for identifying co-occurring objects of interest. Ablation studies on the concept group size of CoDet in Table 5 reveals the same problem. But we believe this would not harm the generality of our method as we show CoDet works well on web-crawled data (_e.g._, CC3M), which is a more practical setting and can easily be scaled up.

### Transfer to Other Datasets

In simulation to detection in the open world, where test data may come from different domains, we conduct cross-dataset transfer detection experiments in Table 3. Specifically, we transfer the open-vocabulary detector trained on OV-LVIS (LVIS base + CC3M) to COCO and Objects365 v1, by plugging in the vocabulary of test datasets without further model fine-tuning. In comparison with existing works, CoDet outperforms the second-best method ViLD  which uses a 32\(\) training schedule by 2.0% and 2.1% AP on COCO and Objects365, validating the generalization capability of CoDet across image domains and vocabularies.

### Visualization and Analysis

Discovering reliable region-word alignments is critical to learn object-level vision-language representations from image-text pairs. In this section, we investigate different types of alignment strategies that are primarily based on: 1) region-word similarity, _i.e._, assigning words to regions of the highest similarity [38; 14]; 2) hand-crafted prior, _i.e._, assigning words to regions of the maximum size ;

   Method & AP\({}_{50}^{}\) & AP\({}_{50}^{}\) & AP\({}_{50}^{}\) \\  OVR-CNN  & 22.8 & 46.0 & 39.9 \\ ViLD  & 27.6 & 59.5 & 51.3 \\ RegionCLIP  & 26.8 & 54.8 & 47.5 \\ Detic  & 27.8 & 47.1 & 42.0 \\ OV-DETR †  & 29.4 & 61.0 & 52.7 \\ PB-OVD  & 29.1 & 44.4 & 40.4 \\ VLDet & **32.0** & 50.6 & 45.8 \\ CoDet (Ours) & 30.6 & 52.3 & 46.6 \\   

Table 2: **Comparison with state-of-the-art methods on OV-COCO**. †: implemented with Deformable DETR .

    &  &  \\   & AP & AP\({}_{50}\) & AP\({}_{75}\) & AP & AP\({}_{50}\) & AP\({}_{75}\) \\  Supervised  & 46.5 & 67.6 & 50.9 & 25.6 & 38.6 & 28.0 \\  ViLD  & 36.6 & 55.6 & 39.8 & 11.8 & 18.2 & 12.6 \\ DetPro †  †: & 34.9 & 53.8 & 37.4 & 12.1 & 18.8 & 12.9 \\ F-VLM  & 32.5 & 53.1 & 34.6 & 11.9 & 19.2 & 12.6 \\ BARON  & 36.2 & 55.7 & 39.1 & 13.6 & **21.0** & 14.5 \\ CoDet (Ours) & **39.1** & **57.0** & **42.3** & **14.2** & 20.5 & **15.3** \\   

Table 3: **Cross-datasets transfer detection from OV-LVIS to COCO and Objects365. †:** Detection-specialized pre-training with SoCo .

and 3) region-region similarity, _i.e_., assigning words to regions of the maximum weight, derived by CoDet from region-region similarity matrix (one may refer to Figure 2 for clarity).

Figure 3 shows a comparison of these strategies. Specifically, we employ the model trained with different strategies on OV-COCO benchmark to generate pseudo-labels for novel categories in COCO validation set. Note that the pseudo-labels are generated with the same strategy as in training, not by prediction results. We evaluate the quality of pseudo-labels by cover rate, which is defined as the ratio of pseudo-labels whose assigned region has a mIoU > 0.5 with the closest ground-truth box. At the end of training, we can see that alignments based on region-region similarity produce much more accurate pseudo labels compared with the other two strategies, manifesting the reliability of visual guidance. Another intriguing finding is that, our method can benefit from self-training which leads to steadily increasing pseudo-label quality, while such pattern is not observed in similarly self-trained model relying on region-word similarity for alignment. We conjecture this is because the model gets stuck in the aforementioned chicken-and-egg problem, which is reflected in its unstable training curve (see Appendix B). This also aligns with the finding in 3. Further qualitative comparisons are presented in Figure 4.

### Ablation Study

Text-guidance.We ablate the impact of text guidance in estimating inter-region similarity on OV-COCO. As shown in Table 3(a), introducing text guidance leads to a significant performance boost on novel AP. This finding aligns with our intuition that putting region similarity estimation in a semantic context can provide better measurements of semantic closeness. Figure 5 further demonstrates how text guidance facilitates mitigating interference from irrelevant concepts.

Figure 4: Visualization of pseudo bounding box labels generated by different region-word alignment strategies. From top to bottom, each row shows results of strategies based on region-region similarity, region-word similarity, and hand-crafted prior, respectively. Zoom in for a better view.

Figure 3: **A comparison of different alignment strategies on OV-COCO.** Cover rate is the ratio of assigned proposal covering the ground-truth box.

Heuristic vs. Prototype-based co-occurring object discovery.To investigate the effectiveness of prototype-based strategy in co-occurring object discovery, we adopt a heuristic strategy in image co-segmentation  for comparison. Results are presented in Table 3(b). The heuristic strategy works to select a single region proposal that has close neighbors across support images following hand-crafted rules (please refer to Appendix A for more details). Our prototype-based strategy makes a substantial improvement over this simple baseline by 3.7 AP on novel categories. We speculate the gains mainly come from two aspects: 1) robustness to noisy similarity estimations; We notice that some region proposals in the background may be estimated with high similarity in the early stage, which disturbs the selection by the heuristic strategy. While our prototype-based strategy avoids hard selection by assigning soft weights to each region proposal to construct a prototype, thus is more robust to such noises. 2) the ability to harness multiple instances; considering that there may be multiple instances corresponding to the shared concept in an image, prototype-based strategy can effectively make use of region proposals of different instances to construct a prototype, which we show in Appendix C).

Size of concept group.In CoDet, co-occurring object discovery is based on a mini-group of images sampled from the same concept group during training. Since the size of the mini-group is generally small, sometimes there could be more than one co-occurring concept in a group, in which case the model will be confused about which concept to discover, as illustrated in Figure 5. Besides introducing text guidance, intuitively, increasing the group size can also effectively reduce this ambiguity, as verified by results on OV-LVIS in Table 5. However, contrary results are observed in experiments on OV-COCO. We speculate these abnormal results are probably caused by the aforementioned human-curated bias in COCO Caption (See Sec. 4.3). Due to the highly concentrated concepts, increasing the group size will undesirably introduce more concurrent concepts that harm the model performances.

Table 4: **Ablation study on effective components.** We show that both text guidance and prototype-based strategy substantially facilitate co-occurring object discovery.

Figure 5: There can be more than one co-occurring concept among sampled images. Text guidance helps filter out the distracting concept (chair legs) and focus on the concept of interest (pigeons).

    &  &  \\   & \(_{50}^{}\) & \(_{50}^{}\) & \(_{50}^{}\) & \(_{}^{}\) & \(_{}^{}\) & \(_{}^{}\) & \(_{}^{}\) \\ 
2 & 30.6 & 52.3 & 46.6 & 21.9 & 30.3 & 35.0 & 30.7 \\
4 & 29.9 & 51.2 & 45.6 & 21.8 & 30.2 & 34.9 & 30.6 \\
8 & 29.1 & 50.9 & 45.2 & 22.7 & 30.3 & 34.7 & 30.7 \\   

Table 5: **Ablation study on concept group size.** CoDet shows different preferences of concept group size on human-curated caption data (OV-COCO) and web-crawled image-text pairs (OV-LVIS).

## 5 Limitations and Conclusions

In this paper, we make the first attempt to explore visual clues, _i.e._, object co-occurrence, to discover region-word alignments for open-vocabulary object detection. We present CoDet, which effectively leverages cross-image region correspondences and text guidance to discover co-occurring objects for alignment, achieving state-of-the-art results on various OVD benchmarks. On the other hand, our method is orthogonal to previous efforts in aligning regions and words with VLMs. Combining the advantages of both sides is a promising direction of research but is under-explored here. We leave this for further investigation.

AcknowledgementsThis work has been supported by Hong Kong Research Grant Council - Early Career Scheme (Grant No. 27209621), General Research Fund Scheme (Grant No. 17202422), RGC Theme-based research (T45-701/22-R) and RGC Matching Fund Scheme (RMGS). Part of the described research work is conducted in the JC STEM Lab of Robotics for Soft Materials funded by The Hong Kong Jockey Club Charities Trust.