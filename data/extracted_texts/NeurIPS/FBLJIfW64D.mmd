# Dimension-free deterministic equivalents and scaling laws for random feature regression

Leonardo Defilippis

Departement d'Informatique

Ecole Normale Superieure - PSL & CNRS

leonardo.defilippis@ens.psl.eu&Bruno Loureiro

Departement d'Informatique

Ecole Normale Superieure - PSL & CNRS

bruno.loureiro@ens.psl.eu&Theodor Misiakiewicz

Department of Statistics and Data Science

Yale University

theodor.misiakiewicz@yale.edu

###### Abstract

In this work we investigate the generalization performance of random feature ridge regression (RFRR). Our main contribution is a general deterministic equivalent for the test error of RFRR. Specifically, under a certain concentration property, we show that the test error is well approximated by a closed-form expression that only depends on the feature map eigenvalues. Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension--allowing for infinite-dimensional features. We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets. As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay. In particular, we provide a tight result for the smallest number of features achieving optimal minimax error rate.

## 1 Introduction

At odds with classical statistical intuition, overparametrized neural networks are able to generalize while perfectly interpolating the training data. This observation, which defies the canonical mathematical understanding of generalization based on complexity measures and uniform convergence, appears surprising at first (Zhang et al., 2017). However, recent progress in our mathematical understanding of generalization has taught us that this _benign overfitting_ property of overparametrized neural networks is shared by a plethora of simpler learning tasks (Bartlett et al., 2021; Belkin, 2021). Among them, the investigation of the following class of _random feature models_ has been at the forefront of this progress:

\[_{}=(;)=} _{j[p]}a_{j}(,_{j})\ :\ =(a_{j})_{j[p]}^{p}}.\] (1)

Here \(\) denotes the inputs and \(=(_{j})_{j[p]}\) a set of weight vectors which are taken to be random \(_{j}^{d}_{}_{w}\). Hence, as the name suggests the feature map \(:\) defines a random function. A few examples of random feature maps include the fully connected neural network features \((,)=(,)\), \(:\), and the convolutional features with global average pooling \((,)=}{{d}}_{=1}^{d}( ,g_{})\) where \(:\) and\(g_{}=(x_{+1},,x_{d},x_{1},,x_{})\) is the \(\)-shift operator with cyclic boundary conditions (both with \(=^{d}\)). Random features (Balcan et al., 2006; Rahimi and Recht, 2007) were originally introduced as a computationally efficient approximation for the limiting kernel: \[K(,^{})=_{_{w}}[(, )(^{},)].\] (2) Although this can reduce the computational cost of kernel methods, it introduces an approximation error. Rahimi and Recht (2008) showed that in a supervised setting with \(n\) samples, \(p=O(n)\) features are sufficient to achieve an excess risk \(O(n^{-}{{n}}})\). Rudi and Rosasco (2017) improved this result under standard power-law assumptions on the asymptotic kernel spectrum, showing that for fast decays, less features are needed to achieve the minimax rate. In Section 4 we will revisit this question, where we will derive a tight result for the minimum number of features. More recently, the random feature model has gained in popularity as a proxy model for studying the generalization properties of two-layer neural networks in the lazy regime of training (Jacot et al., 2018; Chizat et al., 2019). Indeed, for particular choices of feature maps such as \((,)=(,)\), it can also be seen as a two-layer neural network with fixed first-layer weights. Exact asymptotic results for the generalization error of eq. (1) were derived for different supervised learning tasks under the proportional scaling regime \(n,p=(d)\) in (Mei and Montanari, 2022; Gerace et al., 2021; Dhifallah and Lu, 2020; Hu and Lu, 2023; Goldt et al., 2022; Loureiro et al., 2022; Bosch et al., 2023, 2024; Schroder et al., 2023, 2024) and under more general polynomial scaling \(n,p=(d^{})\) in (Simon et al., 2023, Aguirre-Lopez et al., 2024; Hu et al., 2024). As discussed above, these works played a fundamental role in our current mathematical understanding of the relationship between overparametrization and generalization, demystifying different phenomena such as double descent Belkin et al. (2019) and benign overfitting Bartlett et al. (2020). It also led to fundamental separation results between lazy and trained networks (Ghorbani et al., 2019, 2020; Mei et al., 2022), recently motivating the investigation of corrections to the random limit (Ba et al., 2022; Dandi et al., 2023; Moniri et al., 2024; Cui et al., 2024). With the exception of (Simon et al., 2023), which is based on non-rigorous arguments, the results in all the works cited above are derived in the asymptotic limit of large data dimension. However, the relative scaling of the \(n,p,d\) is fundamentally artificial, and in practice it is hard to unambiguously define the regime of interest. Our main goal in this manuscript is to provide a dimension-free characterization of the generalization error allowing us to give tight answers to questions which cannot be addressed asymptotically. More precisely, our main contributions are:

1. Under a concentration assumption on the feature map eigenfunctions, we prove a non-asymptotic deterministic approximation for the RFRR risk \(_{}_{n,p}\) which is independent of the feature map dimension. More precisely, with high-probability over the input data and random weights: \[|_{}-_{n,p}|(p^{-1/2}+n^{-1/2}) _{n,p}.\] (3) where the _deterministic equivalent_\(_{n,p}\) can be computed by solving a set of self-consistent equations of the type \(x=f(x)\), with \(f\) a contractive map. This result unifies the long list of asymptotic formulas in the RFRR literature, and proves a conjecture by Simon et al. (2023). We numerically validate the results on various real and synthetic datasets. The precise statement of the theorem and the assumptions are discussed in Section 3.
2. Leveraging our formula, we investigate the error scaling laws in a setting where the target function and feature spectrum decay as a power-law, also known as _source and capacity_ conditions. We provide a full picture of the different scaling regimes and the cross-overs between them, summarized in Figure 2. Our result is closely related to the neural scaling laws literature (Kaplan et al., 2020), and provides the first rigorous, non-linear extension of (Bahiri et al., 2024; Maloney et al., 2022).
3. We provide a sharp expression for the minimum number of features required to achieve the minimax optimal decay rate of Caponnetto and De Vito (2007), closing the gap of previous lower-bounds in the literature (Rudi and Rosasco, 2017). Further related works --Deterministic equivalents have been derived for a wide range of learning problems, such as ridge regression (Dobriban and Wager, 2018; Hastie et al., 2022; Cheng and Montanari, 2022; Wei et al., 2022), kernel regression (Misiakiewicz and Saeed, 2024), shallow (Liaoand Couillet, 2018; Mei and Montanari, 2022; Chouard, 2022; Bach, 2024; Atanasov et al., 2024) and deep random feature regression (Fan and Wang, 2020; Schroder et al., 2023, 2024; Chouard, 2023) and spiked random features (Wang et al., 2024). Scaling laws under source and capacity conditions were studied by several authors in the context of kernel ridge regression (Bordelon et al., 2020; Spigler et al., 2020; Cui et al., 2022; Simon et al., 2023; Li et al., 2023; Misiakiewicz and Mei, 2022; Favero et al., 2021; Cagnetta et al., 2023; Dohmatob et al., 2024) and classification (Cui et al., 2023).

## 2 Setting

In this work, we focus on the generalization properties of the random feature class \(_{}\) defined in eq. (1) in a supervised regression setting. More precisely, consider a data set \(=\{(_{i},y_{i})_{i[n]}\}\) composed of \(n\) independent and identically distributed samples from a joint distribution \(_{x,y}\) on \(\). Let \(f_{}()=[y|]\) denote the target function. We assume \(f_{} L_{2}(_{x})\), where \(_{x}\) is the marginal distribution over \(\). Moreover, we assume the noise \( y-f_{}()\) has zero mean and finite variance \([^{2}]=_{}^{2}<\). Note this is equivalent to:

\[y_{i}=f_{}(_{i})+_{i}, f_{} L_{2}( _{x}).\] (4)

Given the training data, we are interested in the properties of the minimiser:

\[}_{}(,)*{arg\,min}_{^{p}}_{i[n]}y_{i}-(_{i};)^{2}+\|\|_{2}^{2}}=(^{}+ _{p})^{-1}^{},\] (5)

where we have defined the feature matrix \(_{ij}=p^{-}{{2}}}(_{i};_{j})\) and the label vectors \(=(y_{i})_{i[n]}\). In particular, we are interested in its capacity of generalising to unseen data, as quantified by the _excess population risk_:

\[(f_{},,,,)_{_{x}}f_{}()-(;}_{ })^{2}.\] (6)

It will be convenient to decompose the excess risk above in terms of the standard bias and variance:

\[(f_{};,,)_{}[(f_{};,,, )]=(f_{};,,)+(,,),\] (7)

where:

\[(f_{};,,) _{_{x}}[f_{}()-_{}[(;}_{})]^{2} ],\] (8) \[(,,) _{_{x}}[_{ }((;}_{}))].\] (9)

Note that to simplify the exposition, we have explicitly taken an expectation over the training data noise \(=(_{i})_{i[n]}\). Indeed, it can be shown that the excess risk eq. (6) concentrates on its expectation over \(\) under mild assumptions (see for example Misiakiewicz and Saeed (2024)).

## 3 Deterministic equivalents

The excess risk eq. (6) is a function of the covariates \(\) and the weights \(\), and therefore it is a random quantity. Our main result in what follows is a sharp characterization of the bias and variance in terms of a _deterministic equivalent_ depending only on the model parameters and spectral properties of the features. Consider a square-integrable \( L_{2}()\), and define the Fredholm integral operator \(:L_{2}() L_{2}()\):

\[h()_{}(;)h() _{x}(), h L_{2}(),\] (10)

where we define \(=()\). This is a compact operator, and therefore can be diagonalized:

\[=_{k=1}^{}_{k}_{k}_{k}^{},\] (11)where \((_{k})_{k 1}\) are the eigenvalues and \((_{k})_{k 1}\) and \((_{k})_{k 1}\) are orthonormal bases of \(L_{2}()\) and \(\) respectively:

\[_{k},_{k^{}}_{L_{2}()}=_{kk^{ }},_{k},_{k^{}}_{L_{2}()}=_{kk^{}}.\] (12)

Without loss of generality, we assume the eigenvalues are ordered in non-increasing absolute values \(|_{1}||_{2}|\), and for simplicity of presentation we assume that all eigenvalues are non-zero, i.e., \(()=\{0\}\). Denote \(=(_{1}^{2},_{2}^{2},)^ {}\) the diagonal matrix of the squared eigenvalues. Similarly, since \(f_{} L_{2}(_{x})\), it admits the following decomposition in \(()_{k 1}\):

\[f_{}=_{k 1}_{,k}_{k}\] (13)

Our formal results will assume the following concentration property over the eigenfunctions.

**Assumption 3.1** (Concentration of the eigenfunctions).: _Denote the (infinite-dimensional) random vectors1\(:=(_{k}_{k}())_{k 1}\) and \(:=(_{k}_{k}())_{k 1}\). There exists a constant \(_{x}>0\) such that for any deterministic p.s.d. matrix \(^{}\), i.e. a linear operator acting on an infinite-dimensional Hilbert space, with \(()<\), we have_

\[(|^{}- ()| 1\|^{1/2}^{1/2}\|_{F}) _{x}\{-t/_{x}\},\] (14) \[(|^{}- ()| 1\|^{1/2}^{1/2}\|_{F}) _{x}\{-t/_{x}\}.\] (15)

While Assumption 3.1 is restrictive and will not be satisfied by many non-linear settings, it covers a number of popular theoretical models studied in the literature: 1) independent sub-Gaussian entries, 2) verifying a log-Sobolev inequality or convex Lipschitz concentration (see Cheng and Montanari (2022)). We expect that Assumption 3.1 can be relaxed using the same procedure as in Misiakiewicz and Saeed (2024) to cover classical examples such as data and weights uniformly distributed on the sphere or hypercube. Such a relaxation is involved and we leave it to future work. We will further assume that:

**Assumption 3.2**.: _There exists \(\) such that_

\[p_{+1}^{2}_{k=+1}^{} _{k}^{2}.\] (16)

_Furthermore, we will assume that for some \(_{*}>0\) that we have_

\[((+_{2})^{-1})}{ {Tr}(^{2}(+_{2})^{-2})}_{*}, _{},(+_{2})^{-1}_{ }}{_{2}_{},(+_{2})^{-2} {}_{}}_{*}.\] (17)

Assumption 3.2 is technical, and we believe it can be removed at the cost of a more involved analysis. For instance, eq. (16) is always satisfied for \(_{k}^{2} k^{-}\) if we take \(=O(p^{2})\). Condition (17) was also considered in Cheng and Montanari (2022), and is satisfied in many settings of interest, for example under source and capacity conditions \(_{k} k^{-}\) and \(_{k}^{2} k^{-}\) considered in Section 4.

Main result --Our main result concerns a dimension-free characterization of the risk eq. (6) in terms of deterministic equivalents. We start by defining them.

**Definition 1** (Deterministic equivalents).: _Given integers \(n,p\), covariance matrix \(\) and regularization parameter \( 0\). Consider the parameter \(_{2}_{>0}\) defined as the unique solution of the self-consistent equation:_

\[1+-)^{2}+4}}= ((+_{2})^{-1}),\] (18)

_and \(_{1}_{>0}\) is given by:_

\[_{1}:=}{2}[1-+)^ {2}+4}}].\] (19)_We introduce the short-hand:_

\[(_{1},_{2}) [(1-}{_{2}})^{2}+ (}{_{2}})^{2}(^{2} (+_{2})^{-2})}{p-(^{2}(+ _{2})^{-2})}],\] (20) \[(_{2}) ((+_{2} )^{-2})}{p-(^{2}(+_{2})^{-2})}.\] (21)

_Then, the deterministic equivalents for the bias, variance and test error are defined as:_

\[_{n,p}(_{*},) ^{2}}{1-(_{1},_{2})} _{*},(+_{2})^{-2}_{*}+( _{2})_{*},(+_{2})^{-2} _{*},\] (22) \[_{n,p}() _{}^{2},_{2}) }{1-(_{1},_{2})},\] (23) \[_{n,p}(_{*},) _{n,p}(_{*},)+_{n,p} ().\] (24)

Our main result provides precise conditions for when the deterministic equivalents defined in definition 1 are a good approximation for the test error eq. (6), as a function of the dimensions \(n,p\), feature covariance \(\), and regularization \(>0\). More precisely, the approximation rates will depend on them through the following quantities:

\[r_{}(k) _{j}^{2}}{_{k}^{2}}, M_{ }(k) 1+}(_{*} k)  k}{k}(r_{}(_{*} k) k ),\] (25) \[_{}(p)  1+ p}^{2}}{ }M_{}(p),\] (26) \[_{}(n,p)  1+1[n p/_{*}]\{  n}^{2}}{}+_{}(p)\}M_{}(n),\] (27)

Below we denote \(C_{a_{1},,a_{k}}\) constants that only depend on the values of \(\{a_{i}\}_{i[k]}\). We use \(a_{i}=`*\)' to denote the dependency on the constants in Assumptions 3.1 and 3.2.

**Theorem 3.3** (Test error of RFRR).: _Under Assumptions 3.1, 3.2 and for any \(D,K>0\), there exist constants \(_{*}(0,1/2)\) and \(C_{*,D,K}>0\) such that the following holds. For any \(n,p C_{*,D,K}\), regularization \(>0\), and target function \(f_{} L_{2}(_{x})\), if_

\[ n^{-K},_{} p^{-K}, _{}(n,p)^{5/2}^{3/2}(n)  K,\] (28) \[_{}(n,p)^{2}_{_{+}}(p)^{8 }^{4}(p)  K,\] (29)

_then with probability at least \(1-n^{-D}-p^{-D}\), we have_

\[|(f_{};,,)-_{n,p}(_{*}, )| C_{*,D,K}(n,p)_{n,p}(_ {*},),\] (30)

_where \(_{n,p}(_{*},)\) has been defined in eq. (24) and:_

\[_{}=+_{k=m+1}^{}_{k}^{2}, _{+}=p_{1}+_{k=m+1}^{}_{k}^{2}.\] (31)

_and the approximation rate is given by_

\[(n,p) :=_{}(n,p)^{6}^{7/2}(n)}{}+_{}(n,p)^{2}_{_{+}}(p)^{8} ^{7/2}(p)}{}.\] (32)

For typical settings, with regularly varying spectrum, \(_{}(p)(p)^{C}/\) and \(_{}(n,p)(n p)^{C}/\). In this case, the approximation rate scales as \((n,p)=(n^{-1/2}+p^{-1/2})\), which matches the optimal rates expected from local law fluctuations. A few remarks on this theorem are in order:

1. Theorem 3.3 provides fully non-asymptotic approximation bounds for the population risk and its deterministic equivalent. They hold pointwise and for a large class of functions. In particular, they do not require probabilistic assumptions over the target function coefficients \(_{}\), as for instance in (Dobriban and Wager, 2018; Richards et al., 2021; Wu and Xu, 2020).

2. They are not explicitly dependent on the feature map dimension.
3. They are multiplicative, and therefore relative to the scale of the risk. In particular, they hold even if \( n^{-}\), which will be crucial to the discussion in section 4.
4. Theorem 3.3 is considerably more general than previous results. First, it extends the dimension-free results of Cheng and Montanari (2022) for well-specified ridge regression and Misiakiewicz and Saeed (2024) for KRR (see \(p\) discussion below) to the case of feature maps \(:\), which, as discussed in Section 2, comprises several cases of interest in machine learning. Moreover, the deterministic equivalent recovers as particular cases the asymptotic results derived under proportional \(n,p=(d)\)Mei and Montanari (2022); Loureiro et al. (2022); Schroder et al. (2023) and polynomial \(n,p=(d^{})\)Xiao et al. (2022); Hu et al. (2024); Aguirre-Lopez et al. (2024) scaling.
5. The bounds depend on \(^{-1}\) and \(_{>m}^{-1}\). Following similar arguments as in Cheng and Montanari (2022); Misiakiewicz and Saeed (2024), this assumption could be removed at the cost of a lengthier analysis and worse rates \(n^{-C}+p^{-C}\) with \(C<1/2\). Figure 1 illustrates Theorem 3.3 in two different settings with real and synthetic data. On the left, we show the population risk of learning a single-index target function with a spiked random features model. This model was recently shown to be equivalent to the first-step of training in a fully-connected two-layer network Ba et al. (2022), and it was recently studied by several authors Moniri et al. (2024); Cui et al. (2024); Wang et al. (2024). On the right, we apply our formulas directly to a real data set. In both cases, the theoretical curves show excellent agreement with the numerical simulations. In Appendix C we present additional plots, together with a discussion of how these plots were generated.

Particular limits --We now discuss some particular limits of interest of the deterministic equivalent eq. (24). First, note that at the interpolation threshold \(n=p\), we have \(1-(_{1},_{2})\). Therefore, the risk \(_{n,p}^{-1/2}\) diverges as \( 0^{+}\), a well-known behaviour known as the _interpolation peak_ in the random feature literature Hastie et al. (2022); Mei and Montanari (2022); Gerace et al. (2021) and observed in neural networks Spigler et al. (2019); Nakkiran et al. (2021).

Another limit of interest is \(p\) where, in the generic case, the features span an infinite-dimensional RKHS. Typically, the resulting kernel will be universal, implying it can approximate any function in \(L_{2}(_{x})\). In this limit, the risk bottleneck is given by the finite amount of data \(n\).

**Corollary 3.4** (Kernel limit).: _In the \(p\) limit both \(_{1}\) and \(_{2}\) converge to a single \(_{}\) which is the unique positive solution to the following self-consistent equation_

\[n-}}=(+_{})^{-1}.\] (33)

_Moreover, the bias eq. (22) and variance eq. (23) terms simplify to:_

\[_{,n}(_{*},)=}^{2} _{*},(+_{})^{-2}_{*}} {1-(^{2}(+_{}) ^{-2})},_{,n}()=_{}^{2} (^{2}(+_{})^{-2})}{ n-(^{2}(+_{})^{-2})}.\] (34)

_We denote the corresponding test error \(_{,n}(_{*},)=_{,n}( _{*},)+_{,n}()\)._

Note that eq. (23) exactly agrees with the dimension-free deterministic equivalents for kernel methods in Cheng and Montanari (2022), Misiakiewicz and Saeed (2024). Finally, the third limit of interest is the \(n\) where data is abundant. In this case, the empirical risk eq. (5) converge to the population risk, and therefore the bottleneck in the risk is given by the capacity of the random feature class \(_{}\) eq. (1) to approximate the target \(f_{*}\).

**Corollary 3.5** (Approximation limit).: _In the \(n\) limit, we have \(_{1} 0\) and \(_{2}_{}\) satisfying the following simplified self-consistent equation:_

\[p=(+_{})^{-1} .\] (35)

_Moreover, the bias eq. (22) and variance eq. (23) terms simplify to:_

\[_{,p}(_{*})=_{}_{ *},(+_{})^{-1}_{*}, _{,n}=0.\] (36)

_We denote the risk in this case \(_{,p}(_{*})=_{,p}(_{*})\), which as expected does not depend on \(\)._

## 4 Scaling laws

Our exact characterization of the excess risk in Theorem 3.3 shows that the bottleneck in the model performance stems either from its approximation capacity (as measured by the "width" \(p\)) and the availability of data (as measured by the number of samples \(n\)). In other words, for a fixed data budget \(n\), increasing \(p\) might not improve the error besides a certain point, yielding a waste of computational resources. This raises an important question: _given a fixed data budged \(n\), what is the optimal choice of model size \(p_{*}\)?_

Context --This is a fundamental question in the random feature literature, and was investigated already in the pioneering works of Rahimi and Recht (2007, 2008), who showed that to achieve an excess risk of \(O(n^{-1/2})\) requires at most \(p=O(n)\) features. This upper bound was considerably refined by Rudi and Rosasco (2017) under classical power law scaling assumptions, also known as _source_ and _capacity_ conditions in the kernel literature:

\[\!^{1/}<,|| ^{-r}_{*}||_{2}<.\] (37)

where \((1,)\) and \(r(0,)\), with the case \(r=}{{2}}\) corresponding to \(f_{*}\) belonging to the RKHS of the asymptotic random feature kernel eq. (2). The optimal minmax rate \(O(n^{-})\) for ridge regression under source and capacity conditions were obtained by Caponnetto and De Vito (2007). Rudi and Rosasco (2017) showed that this optimal rate can be attained by the random feature hypothesis eq. (1) with \(p>p_{0}=O(n^{})\) features. However, this is only an upper bound, and understanding how tight it is, as well as the full picture in the hard regime \(r(0,}{{2}})\), remains an open question. In this section, we leverage our tight characterization of the excess risk in Theorem 3.3 to provide a sharp answer to this question.

Results --Without loss of generality, we can assume the covariance is a diagonal matrix \(=(_{k}^{2})_{k 1}\), and we consider the case where the exponents exactly saturate the source and capacity conditions eq. (37):

\[_{k}^{2} =k^{-}, _{*,k} =k^{-}.\] (38)

Further, we assume a relative scaling of the number of features \(p\) and the regularization \(\) with the number of samples \(n\):

\[p=n^{q},  =n^{-(-1)}.\] (39)

with \(q 0\) and \( 0\).

**Theorem 4.1** (Excess risk rates).: _Under source and capacity conditions eq.38 and scaling assumptions eq.39, the deterministic equivalent eq.24 rate is given by:_

\[_{n,p}(_{*},)=(n^{-_{}( ,q)}+_{e}^{2}n^{-_{}(,q)})=(n^ {-(,q)}),\] (40)

_where \((,q)_{}(,q)_{} (,q)\) for non-zero noise variance \(_{}^{2} 0\), otherwise \((,q)=_{}(,q)\). The exponents \(_{}\) and \(_{}\) are respectively the decay rates of the bias and variance terms eqs.22 and 23, and are explicitly given by_

\[_{} [2( q 1 )(r 1)][(2(r )-1)( q 1)+q],\] (41) \[_{}  1-( q 1).\] (42)

**Remark 4.1**.: _Under the scaling in eqs.38 and 39, one can check that the approximation rates \((n,p)\) in Theorem3.3 are vanishing for \(+}{{12}}\) if \(q 1\), and for \( q((+}{{16}})}{{16}}(-1))\) if \(q<1\), which includes the optimal vertical line \(=_{}\). Hence, for these regions of scaling, Theorem3.3 readily implies that the excess risk eq.6 indeed has the decay rates described in Theorem4.1. As discussed in the previous section, we expect that these approximation guarantees can be improved to include a larger region of decay rates, but we leave it to future work._

A detailed derivation of the result above from the deterministic equivalent characterization from Theorem3.3 is discussed in AppendixD. The expressions in eq.41 are easier to visualise in a diagram. Figure2 shows the excess risk exponent \((,q)\) as a function of the parameters \(\) and \(q\), in the case where \(_{}^{2} 0\) for \(r}{{2}}\) (left) and \(r<}{{2}}\) (right). Note that the key difference between the diagrams is the presence of an additional region for \(r}{{2}}\).2 Defining the following shorthand:

\[_{}, q_{}  1-_{}(2r 1),=q_{}\] (43)

we can identify two main regions in the \((,q)\) plane, corresponding to a trade-off between the bias \(_{}\) and variance \(_{}\) terms:

1. **Variance dominated region** (\(_{}<_{}\)): if \(>_{}\), \(q>\) and \(p>\), the excess risk is dominated by the variance term, provided the number of samples is large enough \(n_{}^{-}{{_{}(,q)}}- _{}(,q)}\).3 Inside this region it is possible to further distinguish between two regimes:* **slow decay regime** (orange and brown): for \(<\) and \(q<1\) (\(p n\)), \(_{}=1-(}{{}} q)\), hence the decay depends on the interplay between regularization strength and number of random features and it is slower as \((/ q)\) increases;
* **plateau regime** (red): for \(\) and \(q 1\) (\(p n\)) the excess risk converges to a constant value and does not decay as \(n\) increases.
* **Bias dominated region** (\(_{}>_{}\)): if \(<_{}\), \(q<\) and \(p<\), the excess risk is dominated by the bias term, whose decay is faster as \((}{{}} q)\) increases (cyan, emerald and teal). Note that in the limit of large number of random features \(p\), we recover the same rates found by Cui et al. (2022) for kernel ridge regression. Of particular interest is the rate for which the excess risk decays the fastest with the number of samples \(n\), and what is the minimum number of random features \(p_{}\) required to achieve this rate.

**Corollary 4.2** (Optimal rates).: _The optimal excess risk rate achieved by the random features hypothesis eq. (1) under source and capacity conditions eq. (38) and scaling assumptions eq. (39):_

\[_{}=_{,q}(,q)=,\] (44)

_and it is attained for:_

\[=_{} n^{-(_{}-1)}&r}{{2}},\\ p p_{} n^{q_{}}=_{}&r }{{2}},\\ \] (45)

\[=_{}&\{_{ }.\\ p p_{}=(_{}^{-1}n)^{}{{}}}& r<}{{2}}\] (46)

_corresponding to the bold red line (\(\)) in Fig. 2. In particular, the minimal number of random features \(p_{}=n^{q_{}}\) required to achieve the optimal rate \(_{}\) is given by:_

\[q_{}=1-\] (47)

_and corresponds to the bold red dot (\(\)) in Fig. 2._

A few comments on Corollary 4.2 are in place. 1) The optimal excess error rate eq. (44) is consistent with the minimax optimal rates for ridge regression from Caponnetto and De Vito (2007), as also discussed by Rudi and Rosasco (2017). 2) The minimal number of random features \(p_{}=n^{q_{}}\) in eq. (47) achieving the optimal rate eq. (44) in the \(r}{{2}}\) regime is strictly smaller than the lower bound \(p>p_{0}\) of Rudi and Rosasco (2017). More precisely, letting \(p_{0}=n^{q_{0}}\), for \(r[}{{2}},1)\):

\[q_{0}-q_{}=>0,>1.\] (48)

Relationship to scaling laws --The empirical observation that the performance of large scale neural networks decreases as a power law with respect to the number of samples, parameter and computing time has sparked a renewed wave of interest in the theoretical investigation of power laws (Kaplan et al., 2020). Despite being a mature topic in the statistical learning literature, different recent works have turned to the study of linear models under source and capacity conditions as a playground to understand the emergence of different bottlenecks in the excess error rates (Bahi et al., 2024; Maloney et al., 2022).

The model studied in these works is given by ridge regression on data \(y_{i}=_{},_{i}\) with \((,((}{{k}})^{}))\) and \(_{}(,}{{d}} _{d})\) with a linear projection model \((,)=, \), where \(\) is an i.i.d. Gaussian matrix. Note this model is a particular case of the one studied here, corresponding to a linear feature map and random target function. Moreover, since the variance of the target is constant, the source is entirely determined by the capacity \(\) of the asymptotic kernel, here controlled by the decay of the covariance of the input data.

The approximation limit from Corollary 3.5 and the kernel limit from Corollary 3.4 are known in this literature as _Variance_ and _Resolution limited regimes_, respectively (Bahi et al., 2024). They correspond precisely to the bottlenecks in the excess risk arising from the limited approximation capacity of the random feature model or the limited availability of training data. As this model is a particular case of ours, the rates in the variance limited regime can also be obtained from Theorem 4.1, and correspond to particular cases in Fig. 2, see Appendix E for a detailed discussion. Contemporary to our work, Atanasov et al. (2024) has extended the analysis in this linear model to the case where \(_{}\) also has a power-law decay, and provided a comprehensive discussion of the different scaling regimes for this model. Their rates can be put in a one-to-one correspondence with the rates derived in section 4. We refer the interested reader to Section VI.6 of Atanasov et al. (2024) for a detailed discussion of this relationship. We stress, however, that beside being rigorous, our results hold for features in infinite-dimensional Hilbert spaces and are not restricted to a particular asymptotic limit in the dimensions.

Complementary to the sample and model complexity bottlenecks, Kaplan et al. (2020) also observed the emergence of computational scaling laws in the risk as a function of flops used in training. A recent line of work has investigated this question on the aforementioned linear random feature model under different training algorithms, such as gradient flow (Bordelon et al., 2024) and SGD (Paquette et al., 2024; Lin et al., 2024; Bordelon et al., 2024). Due to the simplicity of this setting, the risk of ridge regression with a particular choice of regularization \(\) is closely related to the risk of different descent algorithms for least-squares at a fixed running horizon (Ali et al., 2019, 2020; Sonthalia et al., 2024). A similar analogy allows us to compare our results to the ones obtained in (Paquette et al., 2024; Lin et al., 2024). In particular, our setting cover three of the phases identified by Paquette et al. (2024), and correspond to the result in Theorem 4.1 with \(=1\) (\(=1\)). Similarly, the rates of Lin et al. (2024) are obtained by taking \(\) to be the inverse of the learning rate. A detailed connection to this line of work is discussed in Appendix E.

## 5 Conclusion

In this paper, we have investigated the generalization properties of random feature models, deriving a non-asymptotic deterministic equivalent for the risk of random feature ridge regression--which recovers (and unifies) previous asymptotic findings as special limits. Our results provide a rigorous multiplicative approximation rate, enabling us to analyze error scaling laws under source and capacity conditions, and offers a complete view of the different scaling regimes and their cross-overs. Our analysis relies on Assumption 3.1 which, while popular in theoretical investigations, excludes more realistic random feature models, such as \((,)=(^{})\) with \(,\) Gaussian vectors and non-linear \(\). Although restrictive, this assumption allowed us to derive tight multiplicative approximation bounds for a generic random feature model with infinite-dimensional features--which was essential for obtaining the rigorous excess risk rates that are the primary motivation of our work. We further note that numerical simulations in Figure 1 and Appendix C suggest that the predictions of Theorem 3.3 remain accurate much beyond Assumption 3.1. We consider lifting this technical condition--e.g., by following the approach in Misiakiewicz and Saeed (2024)--to be an important direction for future research.

Figure 3: Excess risk eq. (6) of RFRR as a function of the number of samples \(n\) under source and capacity conditions eq. (37) and power-law assumptions \(=n^{-(-1)}\), \(p=n^{q}\), with noise variance \(_{e}^{2}=0.1\). Solid lines are obtained from the deterministic equivalent Theorem 3.3. In the figure on the left, points are finite size numerical experiments. Dashed and dotted lines are the analytical rates from Theorem 4.1, stated in the legend. The colour scheme corresponds to the regions of Fig. 2.

#### Acknowledgements

We would like to thank Yasaman Bahri, Hugo Cui and Florent Krzakala for stimulating discussions. BL & LD acknowledges funding from the _Choose France - CNRS AI Rising Talents_ program.