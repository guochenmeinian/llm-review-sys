# Mitigating Covariate Shift in Behavioral Cloning

via Robust Stationary Distribution Correction

Seokin Seo\({}^{1}\), Byung-Jun Lee\({}^{2,3}\), Jongmin Lee\({}^{4}\), HyeongJoo Hwang\({}^{1}\), Hongseok Yang\({}^{1}\), Kee-Eung Kim\({}^{1}\)

\({}^{1}\)KAIST, \({}^{2}\)Korea University, \({}^{3}\)Gauss Labs Inc., \({}^{4}\)UC Berkeley

siseo@ai.kaist.ac.kr, byungjunlee@korea.ac.kr,

jongmin.lee@berkeley.edu, hjhwang@ai.kaist.ac.kr,

hongseok.yang@kaist.ac.kr, kekim@kaist.ac.kr

###### Abstract

We consider offline imitation learning (IL), which aims to train an agent to imitate from the dataset of expert demonstrations without online interaction with the environment. Behavioral Cloning (BC) has been a simple yet effective approach to offline IL, but it is also well-known to be vulnerable to the covariate shift resulting from the mismatch between the state distributions induced by the learned policy and the expert policy. Moreover, as often occurs in practice, when expert datasets are collected from an arbitrary state distribution instead of a stationary one, these shifts become more pronounced, potentially leading to substantial failures in existing IL methods. Specifically, we focus on covariate shift resulting from arbitrary state data distributions, such as biased data collection or incomplete trajectories, rather than shifts induced by changes in dynamics or noisy expert actions. In this paper, to mitigate the effect of the covariate shifts in BC, we propose DrilDICE, which utilizes a distributionally robust BC objective by employing a stationary distribution correction ratio estimation (DICE) to derive a feasible solution. We evaluate the effectiveness of our method through an extensive set of experiments covering diverse covariate shift scenarios. The results demonstrate the efficacy of the proposed approach in improving the robustness against the shifts, outperforming existing offline IL methods in such scenarios.

## 1 Introduction

Imitation learning (IL) aims to recover the expert behavior from the dataset of demonstrations. The standard IL setting assumes that the imitator is allowed to interact with the environment during training, as it provides valuable information regarding state transitions. On the other hand, offline IL requires training without online interactions, reflecting scenarios where the interactions are either infeasible or expensive [5; 12; 13; 28]. Despite recent works on offline IL that explore scenarios involving supplementary datasets, such as suboptimal demonstrations [5; 14], behavioral cloning (BC) remains a compelling option in practice since BC does not require additional datasets except expert demonstrations. However, the efficacy of BC can be compromised when the imitator policy's behavior deviates from the underlying data distribution. This phenomenon, known as covariate shift, presents a significant challenge, often resulting in performance degradation. Consequently, a substantial body of IL research is dedicated to addressing this covariate shift issue [5; 24].

A common approach for solving offline IL problems is to use the so-called distribution matching objective or its variant [11; 12; 14; 15; 31], which aims to align the stationary distribution of the imitator policy with that of the expert policy. However, this approach crucially assumes that expert demonstrations are sampled from the stationary distribution of the expert policy. In practice, thisassumption may break, that is, sampled expert demonstrations may be from a shifted version of the stationary distribution. For instance, expert demonstrations may be collected by first sampling states from a distribution that is different from the stationary distribution of the expert policy, and then labeling sampled states with expert actions. In this case, expert demonstrations are not samples from the stationary distribution. As a result, the offline IL algorithms based on the distribution-matching approach might perform badly in this covariate shift case.

In this paper, we address the issue of the covariate shift in the dataset of expert demonstrations, particularly when the data distribution does not match the stationary distribution of the expert policy. We begin by arguing that BC objective is more natural to consider the shift than the distribution matching objective. Then, inspired by the principle of distributionally robust optimization [2; 6; 20; 25], we propose an adversarial objective for offline IL that addresses covariate shifts in BC training. Instead of simply considering the worst-case with respect to all possible distributions on state-action pairs, our objective considers only those distributions that arise as the stationary distributions of policies, i.e., distributions that satisfy the Bellman flow constraint. Leveraging the techniques from the stationary distribution correction ratio (DICE) algorithm, we introduce DriIDICE, which efficiently solves our optimization problem and computes an imitator robust to covariate shifts of the dataset. In addition, we suggest the practical covariate shift scenarios that may arise in offline IL applications. Under those problem settings, we compare our approach with baselines and demonstrate that our approach can imitate the agent robust to covariate shift of our interests.

## 2 Background and Related Work

### Offline Imitation Learning with Arbitrary State Distributions

We consider a Markov decision process (MDP) without rewards, which is defined as a tuple of \(,,T,p_{0},\) where \(\) is a state space, \(\) is an action space, \(T:()\) is a transition distribution, \(p_{0}()\) is an initial state distribution and \([0,1)\) is a discounted factor. We focus on the class of deterministic policies \(=\{:\}\). Given a policy \(\), the stationary distribution \(d_{}\) of \(\) is defined as \(d_{}(s,a)=(1-)_{t=0}^{}^{t}(s_{t}=s,a_{t}=a)\) where \(s_{0} p_{0},a_{t}=(s_{t}),s_{t+1}(s_{t},a_{t})\). For convenience, we use \(_{E}\) to denote the policy of an expert and write simply \(d_{E}\) for the stationary distribution \(d_{_{E}}\) of \(_{E}\).

Assuming the existence of an expert policy \(_{E}\), the goal of imitation learning (IL) is to recover \(_{E}\) by utilizing some demonstrations of the expert. Specifically, offline IL prohibits online interactions and relies solely on a given offline dataset that consists of demonstrations of the expert. We consider that the demonstration dataset \(\) is the collection of \((s,a,s^{})\) triplets where \(s\) is sampled from an arbitrary state distribution \(d_{D}(s)\), \(a\) is determined by \(_{E}(s)\) and \(s^{}\) is sampled from \(T(s^{}|s,a)\). Note that conventional IL approaches assume that \(d_{D}\) is close to the expert's state distribution \(d_{E}\). However, unlike previous studies, we focus the imitation learning with an arbitrary state distribution \(d_{D}\), without assuming that \(d_{D}\) to be a state stationary distribution of any policy.

As we mentioned in the introduction, one common approach for offline IL is to use the distribution matching objective or its variant [11; 12; 14; 15] and to find an imitator policy whose stationary distribution matches that of the expert policy. But as we will show later in the paper, the algorithms based on this approach do not perform well when there is a covariate shift in the given demonstration dataset \(d_{D}(s,a)\) in a way explained in the previous paragraph. In the paper, we propose an offline IL algorithm that achieves good performance in the presence of such a covariate shift by using a version of distributionally robust optimization.

### Covariate Shift in Imitation Learning

Covariate shift is a widely used term in machine learning [1; 7]. Traditionally, it refers to the phenomenon where the distribution of covariates (inputs) during training differs from that during testing, i.e. given a covariate variable \(X\) and a response variable \(Y\),

\[p_{}(Y|X=x)=p_{}(Y|X=x) x,  p_{}(X) p_{}(X).\]

The distribution shift that is considered in this paper has this form where state \(s\) and action \(a\) correspond to \(X\) and \(Y\), and \(p_{}\) and \(p_{}\) correspond to the distribution \(d_{D}\) of the given demonstration dataset and the stationary distribution \(d_{E}\) of the expert policy \(_{E}\).

One significant branch of research addressing the issue of covariate shift is distributionally robust optimization (DRO). DRO involves considering an uncertainty set of the distribution and optimizing for the worst-case loss within this set [6; 20]. Recent studies have proposed method to attain the worst-case supremum by using kernel methods  or the integral probability metric such as Wasserstein metric . Additionally, in the field of imitation learning, DRO has been applied to achieve robust learning in the presence of noisy experts  or transition shifts .

A closely related problem studied in IL is robust imitation learning, which focuses on learning a policy that can tolerate various shifts in the transition distribution \(T\) of the MDP [4; 19]. Another related scenario is the noisy expert scenario, where the demonstration dataset is corrupted by noise, resulting in the distribution \(d_{D}\) of the dataset being a noisy version of \(d_{E}\)[6; 23]. These issues are usually addressed by using robust optimization techniques [6; 19], which are also utilized in our approach. However, it is important to note that the types of distribution shifts considered in our work and in prior researches differ within the context of offline IL.

To emphasize, we are not focused on addressing covariate shifts induced by transition shifts or noisy demonstrations. Instead, our work investigates a type of covariate shift where the dataset still consists of expert actions, but states are not sampled from the stationary distribution of the expert policy: the distribution \(d_{D}(s)\) of the samples in the given demonstration dataset differs from \(d_{E}(s)\), but both distributions share the expert policy given states, i.e., \(_{D}(s)=_{E}(s)\) for all \(s\).

## 3 Mitigating Covariate Shift in BC

### Robustness to Covariate Shift

Assume that we have a free access to \(_{E}\), such that we can optimize our trained policy \(\) to be close to it on any state. We can write the BC objective for optimizing \(\) as:

\[_{}\ _{s d_{E}}[((s),_{E}(s))]\] (1)

for some supervised learning loss \(\) (e.g. squared error, cross-entropy,...). Previous studies on imitation learning have assumed that \(d_{D} d_{E}\). Under this assumption, BC approach of:

\[_{}_{s d_{D}}[((s),_{E}(s))]\] (2)

does show strong performance if we have sufficient data. Otherwise, if the expert data is not sufficient, we can also incorporate transition information and benefit from the distribution matching (DM) approach, which aims to get a stationary distribution \(d_{}\) that is close to data distribution \(d_{D}\) with the following objective:

\[d_{}:=*{arg\,max}_{d()} \ -(d(s,a)\|d_{D}(s,a))\] \[\ _{a}d(s,a)=(1-)_{0}(s)+_{ ,}T(s|,)d(,) s \]

where \(\) is any divergence between two probability distributions. The solution of the optimization problem \(d_{}\) can be used for the state distribution for BC instead of \(d_{E}\), and it is widely known that this can complement lack of data with additional transition information, leading to improved performance. Nevertheless, under harsh covariate shift, i.e., when the state distribution of \(d_{D}\) diverges from that of \(d_{E}\), both two approaches above can fail miserably, as there is no guarantee on improving policy performance when \(d_{E}\) and \(d_{D}\) are significantly different. See Section A for an example of failure cases.

To overcome the covariate shift, we adopt a distributionally robust objective, i.e., for an arbitrary distribution \(d\), we know \(_{s d_{E}}[((s),_{E}(s))]_{d(S)} _{s d}[(_{E}(s),(s))]\), which holds with \(d_{}\) as well, and we can instead optimize for

\[_{}_{d}_{s d}[((s),_{E}(s))],\] (3)

which corresponds to an upper-bound minimization for optimizing \(\) with respect to some uncertainty set \(()\). However, when \(\) is choosed as \(()\), this objective can easily lead to overly pessimistic solutions, as we seek for any extreme state distribution \(d\) that maximizes the policy loss, and it will easily place all its probability on a single state. Hence, a choice of \(\) is crucial to obtain an appropriate solution of distributionally robust optimization.

### Towards Less Pessimistic Robust Objective

For notational convenience, denote \(C_{}(s):=((s),_{E}(s))\). In the objective (3), the worst-case policy loss is considered among an uncertainty set \(\). However, when \(}=()\), it would be too pessimistic and the objective can be improved when we consider a smaller choice of \(\). To keep the fact that it is an upper-bound minimization for Eq. 1, which is the loss with \(d_{E}\), the expert state distribution \(d_{E}\) should still be contained in \(\), even if we aim to use a smaller set.

To facilitate this, we impose two constraints on \(\) with respect to a state distribution \(d\): (1) \(d\) should satisfy Bellman flow constraint, (2) \(d\) should be close enough to \(d_{D}\) to prevent \(d\) diverging too far from \(d_{D}\)1. The first constraint is a constraint that should be satisfied by \(d_{E}\), and thus it effectively reduces the potential distribution set for \(d\) if we impose it properly. The second constraint may not seem necessary, but as we estimate the policy loss from finite samples from \(d_{D}\), diverging too far from \(d_{D}\) will reduce effective number of samples. Then, by introducing \(f\)-divergence regularization to enforce the second constraint, we consider the following constrained optimization problem:

\[_{}&_{d()}_{s d}[C_{}(s)]-_{f}(d(s,a)||d_{D}(s,a))\\ &_{a}d(s,a)=(1-)_{0}(s)+ _{,}T(s|,)d(,) s \] (4)

where \(_{f}\) denotes \(f\)-divergence with a convex function \(f\). We develop a practical algorithm in a stationary distribution correction ratio estimation (DICE) framework style. From Eq. 4, we can take Lagrangian \(\) to solve the inner constrained optimization, i.e.

\[_{d()}&_{ }_{s,a}d(s,a)C_{}(s)-_{f}(d(s,a)||d_{D}(s,a))\\ &+_{s}(s)[(1-)_{0}(s)+ _{,}T(s|,)d(,)-_{a}d( s,a)]\\ =&_{w}_{}(1-)_{s_{0}}[(s)]+_{(s,a,s^{}) d_{D}}[- f (w(s,a))+w(s,a)e_{,}(s,a,s^{})]\] (5)

where \(e_{,}:=(C_{}(s)+_{s^{}}T(s^{}|s,a)(s^ {})-(s))\), \(w(s,a):=(s,a)}\) and \(\) is a class of functions \(w:_{ 0}\).

By Slater's condition , since the strong duality holds on Eq. 5, we can change the minimax into a maximin optimization as follows:

\[_{}_{w}(1-)_{s_{0}}[(s)]+ _{(s,a,s^{}) d_{D}}[- f(w(s,a))+w(s, a)e_{,}(s,a,s^{})],\] (6)

and the closed-form solution \(w^{*}\) can be obtained by solving for the inner maximization.

**Proposition 1**.: _(Lee et al. ) The closed-form solution for the inner maximization of Eq. (6) is_

\[w^{*}_{,}(s,a)=(0,(f^{})^{-1}((s,a )}{})) s,a\] (7)

After obtaining \(w^{*}_{,}\), the minimax optimization (6) is converted into a minimization problem with respect to \(\). As a result, each objective for \(,\) can be summarized as follows:

\[_{}&(1-)_{s _{0}}[(s)]+_{(s,a,s^{}) d_{D}}[- f( w^{*}_{,}(s,a))+w^{*}_{,}(s,a)e_{,}(s,a,s^{}) ]\\ _{}&_{(s,a) d_{D}}[w^{*}_{ ,}(s,a)C_{}(s)]\] (8)

Hence, the problem can be practically addressed by alternatively optimizing \(\) and \(\) following recent developments in DICE approaches . We call this distributionally robust optimization approach to covariate shift in offline IL as DrilDICE (**D**istributionally **R**obust **I**mitation **L**earning via **DICE**).

### Soft TV-Distance

To facilliate a fair comparison with the uncertainty set defined by total variation (TV) distance, as utilized in , we introduce the soft-TV distance as a specific choice of \(f\)-divergence of DriIDICE. It is important to note that **Proposition 1** requires \((f^{})^{-1}\), the invertable derivative of \(f\). However, the generator function of TV-distance \(f_{}(x):=|x-1|\), lacks an invertible derivative as illustrated in Figure 1, rendering the direct application of TV distance in DriIDICE challenging.

We technically overcome this limitation by relaxing the derivative function of TV-distance. Given that the derivative function of TV-distance manifests as a step function, we choose to relax this function by employing the \(\) function. Specifically, we utilize the log-cosh function  to \(f\) as follows:

\[f_{}(x)=(),(f_{}^{ })^{-1}(y)=^{-1}{(2y)}+1.\] (9)

By plugging \(f_{}^{}\) into Eq. 7, we can obtain the closed-form solution \(w_{,}^{*}\) of the inner maximization objective (6) tailored to a specific choice of \(f_{}\) as follows:

\[w_{,}^{*}(s,a)=(^{-1}(}(s, a,s^{})}{})+1)\] (10)

where \((x):=(x,0)\). This choice of \(f\) enables DriIDICE to obtain a closed form solution of the inner maximization problem while maintaining similar properties of TV-distance. For other possible choice of \(f\)-divergence in DriIDICE, see Section B in the supplementary material.

## 4 Experiments

### Comparison on Baselines Objectives

To provide clear descriptions about the baselines used for comparison in our experiments, we summarize the relevant objectives in Table 1.

Adversarial Weighted BC (AW-BC)We refer to adversarial weighting by following the terminology from  as the minimax objective without constraints. As we discussed in Section 3.2, the adversarial weighting objective is also an upper-bound of the target objective. However, it tends to be overly pessimistic because it considers the entire state distribution space, including distributions that do not correspond to the stationary distribution of any policy.

Distributionally Robust BC (DR-BC)To adjust the level of the robustness, DR-BC  can be employed in this context. Despite originally designed to address transition shifts, DR-BC remains relevant to our scenario since it considers the uncertainty set over state distributions by adopting a robustness radius hyperparameter \(\). Notably, the \(f\)-divergence constraint of DriIDICE is functionally analogous to the robustness radius constraint \(D_{}(d\|d_{D})\). However, our approach, which incorporates Bellman flow constraints, addresses a more restricted set when constrained by the equivalent radius level, thereby offering a tighter upper-bound to Eq. 1.

Figure 1: Illustration of soft TV-distance. (left) \(f\) functions, (right) corresponding derivatives \(f^{}\).

**Best-case Weighting** If the sign of the cost function in the worst-case weighting objective is converted, the objective seeks to find a cost-minimizing stationary distribution that is close to the data collection policy. We call this objective the best-case weighting. When \(d_{E}\), the best-case weighting minimizes the lower bound of the target objective since \(_{d}_{d}[C_{}(s)]_{d_{E}}[C_{} (s)]\), which is not relevant to minimize the target objective. Despite there is no direct connection, to ensure that performance of our method is not due to side effects of algorithm's implementation, we include this method for performance comparison in the following experiment section.

### Toy Domain Experiment: Four Rooms domain with Imbalanced Datasets

#### 4.2.1 Experiment Settings

Four Rooms environmentFour Rooms is a grid-world environment which aims to find a path from a starting state to a goal state. Each cell in the grid represents a state and the starting state and the goal state are marked by the orange and the green box in Figure 2 respectively. The agent can choose one of 4 actions from each state: UP,DOWN, LEFT, RIGHT, and the rooms are numbered as shown in Figure 2. We pre-collect expert dataset \(D_{E}\) by using a deterministic expert policy \(_{E}\) and online interactions.

Covariate shift scenariosTo investigate that our approach can effectively addresses the covariate shift, we need the dataset that notably deviates from the expert stationary distribution \(d_{E}\). A possible realistic scenario of the dataset deviation occurs when data collectors gather data at different frequencies for each state or action. For example, if a data collection device (e.g. cameras or sensors) operates at different recording frequencies in each room, the frequency of observing a room in the dataset will not match \(d_{E}\). To simulate this scenario, we design datasets where the marginal room (or action) distribution of dataset deviates from \(d_{E}\). By manipulating the marginal proportion of a predetermined variable in dataset, we generate a deviated dataset from the original dataset. We consider a total of 8 problem settings by manipulating the marginal distribution of four rooms and four actions. In each scenario, we set a marginal distribution \(p_{i}(u)\) of the variable to be manipulated \(u\) to a certain fixed probability and resample transitions to configure the dataset \(D_{i}\) through the following process. (See Section C in the supplementary material)

\[D_{i}=\{(s,a)|u p_{i}(u),s D_{E}(s|u),a=_{E}(s)\}\]

Implementations2 and baselinesSince we are not interested in trivial scenarios where the policy can easily minimize the supervised loss over entire state space to zero (i.e. \(_{s}C_{}(s)=0\)), we conduct experiments using function approximators instead of a tabular setting. We initialize \(_{0}\) for the cost function \(C_{}\) as BC policy. To solve optimization problems, each algorithm is implemented

  
**Functionality** & **Objective** \\  Distribution matching & \(_{}_{s d_{}}[C_{}(s)]\) & s.t. \(d_{}=}{}- _{f}(d(s,a)\|d_{D}(s,a))\) \\  AW-BC objective & \(_{}}_{s d}[C_{ }(s)]\) \\  DR-BC objective & \(_{}}(d\|d_{D})}{} _{s d}[C_{}(s)]\) \\  Best-case weighting & \(_{}}}{}_{s  d}[-C_{}(s)]-_{f}(d(s,a)\|d_{D}(s,a))\) \\  Worst-case weighting (Ours) & \(_{}}}{}_{s  d}[C_{}(s)]-_{f}(d(s,a)\|d_{D}(s,a))\) \\   

Table 1: Objective comparisons for related approaches. Denote a stationary distributions class as \(}\), i.e., \(}:=\{d():d(s)=(1-)_{0}(s)+ _{,}T(s|,)d(,)  s\}\)

Figure 2: Four Rooms environment and a deterministic expert (red arrows).

with a two-step procedure: (1) optimize each objective by using the cost \(C_{_{0}}\) determined by an initial policy \(_{0}\) and obtain \(w^{*}_{_{0}}\), (2) train \(\) with the weighted BC. For policy and weight models, we use linear approximators with RBF features, which exploit distances from representative points. For more details, see Section C. For choices of \(f\)-divergence for OptiDICE-BC and DrilDICE, we select KL-divergence instead of the soft TV-distance for a simplicity of convex optimization solver implementation. We consider following baselines:

* **BC**: a standard behavioral cloning without any regularization.
* **DemoDICE** : a representative distribution matching approach to offline IL. We compare a special case of DemoDICE that does not exploits supplementary datasets.
* **AW-BC**: adversarial weighting method without Bellman flow constraints and robustness radius.
* **DR-BC**: distributionally robust BC method without Bellman flow constraints.
* **OptiDICE-BC** : a representative method as the best-case weighting.

Evaluation metricsThe following metrics are measured with 100 episodes for each trained policy.

* **Normalized score**: a normalized episode return that scales from 0 (random score) up to 100 (expert score) averaged by 100 episodes.
* **Worst-25% performance** : the normalized scores averaged by the worst 25% episodes.
* **Target 0-1 loss**: the averaged 0-1 loss (i.e. \(_{d_{}}[[(s)_{E}(s)])\)3

    & **Scenario** & **Metrics** & **BC** & **DemoDICE** & **AW-BC** & **DR-BC** & **OptiDICE-BC** & **DrilDICE (Ours)** \\   &  & Normalized score & 90.84 \(\) 0.69 & 91.62 \(\) 0.65 & 90.84 \(\) 0.58 & 91.38 \(\) 0.73 & 94.30 \(\) 0.41 & **95.04 \(\) 0.45** \\  & & & Worst-25\% & 63.36 \(\) 2.74 & 66.48 \(\) 2.99 & 63.42 \(\) 2.31 & 65.68 \(\) 2.84 & 77.70 \(\) 1.64 & **80.16 \(\) 1.92** \\  & & Target 0-1 loss (\(\)10\({}^{}\)) & 8.69 \(\) 0.06 & 8.28 \(\) 0.06 & 7.92 \(\) 0.05 & 8.42 \(\) 0.55 & **7.85 \(\) 0.05** & 8.31 \(\) 0.07 \\   &  & Normalized score & 89.16 \(\) 1.07 & 89.72 \(\) 1.00 & 8.29 \(\) 0.78 & 89.28 \(\) 1.08 & 94.06 \(\) 0.65 & **94.44 \(\) 0.62** \\  & & & Worst-25\% & 58.44 \(\) 3.90 & 83.38 \(\) 57.28 & 88.52 \(\) 3.23 & 76.42 \(\) 2.61 & 77.76 \(\) 2.47 \\  & & & Target 0-1 loss (\(\)10\({}^{}\)) & 10.75 \(\) 1.05 & 10.51 \(\) 0.14 & 9.34 \(\) 0.09 & 17.2\(\) 1.03 & 6.55 \(\) 0.08 & **6.46 \(\) 0.07** \\   &  & Normalized score & 85.80 \(\) 1.27 & 89.66 \(\) 1.21 & 88.68 \(\) 1.16 & 87.80 \(\) 1.26 & 94.20 \(\) 0.98 & **95.04 \(\) 0.86** \\  & & & Worst-25\% & 52.82 \(\) 4.74 & 59.00 \(\) 4.56 & 55.52 \(\) 4.40 & 55.92 \(\) 4.75 & 76.80 \(\) 3.92 & **80.16 \(\) 3.44** \\  & & & Target 0-1 loss (\(\)10\({}^{}\)) & 13.55 \(\) 0.16 & 13.22 \(\) 0.15 & 11.75 \(\) 0.16 & 13.74 \(\) 1.11 & 96.64 \(\) 0.14 & **8.51 \(\) 0.13** \\   &  & Normalized score & 90.94 \(\) 0.75 & 91.34 \(\) 0.70 & 90.42 \(\) 0.75 & 90.94 \(\) 0.57 & 94.26 \(\) 0.54 & **94.92 \(\) 0.37** \\  & & & Worst-25\% & 64.00 \(\) 2.89 & 65.36 \(\) 2.81 & 62.72 \(\) 2.91 & 64.00 \(\) 2.89 & 77.04 \(\) 2.15 & **79.68 \(\) 1.49** \\  & & & Target 0-1 loss (\(\)10\({}^{}\)) & 9.30 \(\) 0.93 & 93.40 \(\) 0.80 & 7.00 \(\) 0.90 & 93.60 \(\) 2.81 \(\) 0.07 & **97.97 \(\) 0.05** \\   &  &  & 84.96 \(\) 1.33 & 87.20 \(\) 1.17 & 89.44 \(\) 1.27 & 83.96 \(\) 1.33 & 92.06 \(\) 0.69 & **93.22 \(\) 0.61** \\  & & & Worst-25\% & 43.92 \(\) 4.43 & 51.04 \(\) 4.04 & 34.44 \(\) 4.18 & 43.92 \(\) 4.43 & 68.32 \(\) 2.73 & **72.88 \(\) 4.3** \\  & & & Target 0-1 loss (\(\)10\({}^{}\)) & 13.80 \(\) 1.18 & 0.18 \(\) 0.16 & 12.64 \(\) 1.66 & 13.30 \(\) 1.26 & 8.29 \(\) 0.09 & **81.88 \(\) 0.08** \\   &  &  &  & 89.96 \(\) 9.66 & 91.28 \(\) 0.84 & 95.20 \(\) 0.90 & 89.66 \(\) 0.96 & 93.62 \(\) 0.62 & **94.06 \(\) 0.39** \\  & & & Worst-25\% & 60.80 \(\) 3.45 & 65.36 \(\) 3.27 & 62.24 \(\) 3.53 & 60.80 \(\) 3.45 & 74.48 \(\) 2.47 & **78.40 \(\) 1.55** \\  & & & Target 0-1 loss (\(\)10\({}^{}\)) & 10.45 \(\) 0.13 & 9.53 \(\) 0.12 & 7.89 \(\) 0.12 & 10.45 \(\) 0.99 & 7.73 \(\) 0.70 & **7.33 \(\) 0.06** \\   &  &  & 90.18 \(\) 1.11 & 93.03 \(\) 1.10 & 8.04 \(\) 1.11 & 90.84 \(\) 1.11 & 91.86 \(\) 1.03 & **92.42 \(\) 0.95** \\  & & & Target 0-2 loss (\(\)10\({}^{}\)) & 11.40 \(\) 0.02 & 11.16 \(\) 0.15 & 10.56 \(\) 0.15 & 11.42 \(\) 1.02

### Results

As depicted Table 2, DriIDICE outperformed baselines in the Four Rooms environment under various covariate shift scenarios. Our approach achieved the highest normalized scores across all scenarios and consistently demonstrated superior performance in the worst-25% performance. In scenarios involving Room 3, where the original dataset \(D_{E}\) had only 8.9% coverage, the probability of observing states in Room 3 was increased to 40%, causing significant covariate shifts. This led to a significant degradation in the robust performance of BC (worst-25%). However, DriIDICE successfully improved the worst-25% performance in Room 3 to levels comparable to other scenarios, demonstrating its robustness to covariate shifts induced by dataset deviations.

Figure 3 illustrates the behaviors of BC and DriIDICE in the Room 3 scenario. With an increased dataset proportion visiting Room 3, as shown in Figure 3-(a), \(_{}\) performs accurately in Room 3, predicting the correct expert actions in nearly all states (26 out of 27) in Room 3. However, its performance remains suboptimal in other rooms. DriIDICE, leveraging the cost derived from \(_{}\), computes the corrected state distribution through the worst-case weighting, denoted as \(w_{}^{*}\), as depicted in Figure 3-(c). Notably, states mispredicted by \(_{}\) or those with high probabilities for the dataset distribution tend to obtain relatively higher weight values, contributing more to loss optimization. Consequently, DriIDICE effectively trains the agent by correcting suboptimal behaviors.

### D4RL Dataset with Covariate-Shifted Expert Demonstrations

#### 4.4.1 Covariate Shift Scenarios

To investigate our approach also benefits distributionally robust training in more complex tasks, we conduct experiments on continuous control domains. In addition, we devise three practical problem settings that could possibly occur in the real-world and simulate those scenarios by restructuring the given dataset to simulate these dataset deviation scenarios (For experiments on standard scenarios, refer to Section E). We utilize hopper-expert, walker2d-expert, halfcheeta-expert datasets included in D4RL benchmark  as original datasets. We utilize the soft TV-distance defined in Section 3.3 as \(f\)-divergence for OptiDICE-BC and DriIDICE for these scenarios (For experiments with another choice of \(f\)-divergence, see Section E.4). See Section D for implementation details.

Scenario 1: Rebalanced datasetFrom a practical perspective, a data collector may encounter scenarios in which the costs associated with taking an action or visiting a specific state exhibit substantial costs varying across different states or actions. In such scenarios, the data collected in states or actions with lower costs is likely to be observed, while those with higher costs tend to be underrepresented. To simulate these circumstances, we partition the state or action space and manipulate their mixture ratio to generate an imbalanced dataset, under assuming the costs of collecting data points in each group is significantly different, hence their proportion is shifted from the original dataset.

    & **Task** & \(p(D_{1})\) & **BC** & **DemoDICE** & **AW-BC** & **DR-BC** & **OptiDICE-BC** & **DriIDICE (Ours)** \\    } &  & 0.1 & \(24.7 4.2\) & \(25.8 3.2\) & \(17.9 2.0\) & \(27.0 4.3\) & \(12.7 1.3\) & \(\) \\  & & 0.5 & \(35.4 4.1\) & \(37.6 1.5\) & \(24.6 1.7\) & \(3.6 3.6\) & \(8.6 2.0\) & \(\) \\  & & 0.9 & \(11.2 2.5\) & \(15.0 3.4\) & \(13.3 4.8\) & \(27.4 4.9\) & \(10.4 1.8\) & \(\) \\   &  & 0.1 & \(18.9 4.1\) & \(15.1 2.4\) & \(7.3 1.2\) & \(14.7 3.3\) & \(4.9 1.1\) & \(\) \\  & & 0.5 & \(22.9 3.1\) & \(30.2 3.7\) & \(27.8 2.8\) & \(45.1 10.0\) & \(8.1 0.4\) & \(\) \\  & & 0.9 & \(30.4 7.1\) & \(21.6 3.3\) & \(35.8 5.0\) & \(46.0 8.4\) & \(7.7 0.5\) & \(\) \\   &  & 0.1 & \(49.3 5.2\) & \(38.1 4.6\) & \(\) & \(32.9 3.8\) & \(7.1 1.5\) & \(52.5 3.6\) \\  & & 0.5 & \(38.0 3.1\) & \(43.9 3.4\) & \(32.6 1.9\) & \(26.2 4.9\) & \(6.1 1.2\) & \(\) \\  & & 0.9 & \(15.5 3.1\) & \(5.0 1.1\) & \(7.8 1.2\) & \(9.0 3.3\) & \(1.0 1.1\) & \(\) \\    } &  & 0.1 & \(29.7 4.0\) & \(26.8 3.0\) & \(24.6 4.8\) & \(25.9 2.5\) & \(11.7 2.1\) & \(\) \\  & & 0.5 & \(26.4 4.9\) & \(33.4 4.4\) & \(30.8 2.3\) & \(35.1 5.4\) & \(11.8 1.1\) & \(\) \\   & & 0.9 & \(30.5 3.6\) & \(16.5 1.4\) & \(19.3 2.9\) & \(36.6 2.3\) & \(19.4 2.8\) & \(\) \\   &  & 0.1 & \(23.6 5.1\) & \(22.6 6.5\) & \(20.4 4.2\) & \(31.2 4.2\) & \(7.3 0.5\) & \(\) \\  & & 0.5 & \(32.3 6.7\) & \(33.8 7.5\) & \(25.7 3.5\) & \(30.5 3.9\) & \(6.4 1.0\) & \(\) \\   & & 0.9 & \(16.9 2.8\) & \(12.0 1.0\) & \(16.9 2.5\) & \(37.6 9.0\) & \(4.6 1.0\) & \(\) \\   &  & 0.1 & \(41.9 4.8\) & \(34.9 3.1\) & \(41.7 3.9\) & \(2.75 1.0\) & \(8.4 3.4\) & \(\) \\   & & 0.5 & \(45.8 4.5\) & \(32.5 2.0\) & \(30.7 2.3\) & \(33.4 6.5\) & \(4.0 0.8\) & \(\) \\   & & 0.9 & \(25.9 3.4\) & \(8.8 3.1\) & \(14.2 0.7\) & \(12.1 2.0\) & \(0.6 0.7\) & \(\) \\   

Table 3: Performance comparison on Scenario 1 (rebalanced dataset). \(p(D_{1})\) determines the proportion of dataset \(D_{1}\), which is close to the representative point. Each experiment is repeated with 5 times and the average normalized scores with their standard errors are reported. The highest mean performance scores are highlighted in bold.

In order to implement this scenario, we measure the distance between each state and the representative point of the states, then split the dataset into two groups \(D_{1}\), \(D_{2}\) based on the statistics of distances: \(D_{1}\) comprises states close to the point, while \(D_{2}\) is consists of states farther from the point. Subsequently, we resample datasets according to a predetermined proportion of \(p(D_{1})\) in \(\{0.1,0.5,0.9\}\) by uniformly sampling different numbers of samples from each group.

Scenario 2: Time-dependently collected datasetOne of well-known practical issues of covariate shift in supervised learning and time-series forecasting communities is dataset shift caused by seasonal or time-dependent variables [8; 18; 26]. In real-world scenarios, data collection agents affected by seasonality (e.g., temperature, humidity) can cause the data to be collected far from the expert agent's full demonstration. For instance, consider a sensor with the collecting frequency is sensitive to the temperature, making it prone to frequent breakdowns in the summer, and this sensor should collect expert data throughout the entire year. However, due to its frequent failures during the summer, the collected dataset will deviate from \(d_{E}\).

Motivated by these scenarios, we simulate time-dependent covariate shift scenarios by subsampling timesteps of the trajectory in extensive manners. To model the timestep sampling distribution, we utilize Beta distribution \((a,b)\). Since the support of \((a,b)\) is \(\), with multiplying with the maximum timestep and discretization, we can sample timesteps in various shapes of distributions by adjusting parameters \(a,b\). We conduct experiments on four parameter combinations, \((a,b)\{(1,1),(1,5),(5,1),(5,5)\}\) and the timestep distribution notably varies as depicted in Figure 3(a).

Scenario 3: Segmented trajectory datasetAs similarly explored in , when decision-making horizons are extensively long, it is not feasible to gather multiple long trajectories keep tracking from initial states to ensure the stationarity of the expert dataset. Instead, it is more practical to gather shorter segments of expert demonstrations in such scenarios. We simulate this scenario by utilizing short segments of the pre-collected trajectories. To collect this, for each trajectory in the original dataset, we sample the starting timestep of the segment by using a subsampling method similar to Scenario 2. Then, we extract consequent segments with fixed-length timesteps and configure the segmented trajectory dataset similar to Figure 3(b). In this scenario, we adjust the number of segments used for training to investigate the relationship between performance and the number of segments.

    & \((a,b)\) & **BC** & **DemoDICE** & **AW-BC** & **DR-BC** & **OptiDICE-BC** & **DrilDICE** (Ours) \\    } &  & (1, 1) & \(28.9 3.8\) & \(26.4 5.8\) & \(18.0 3.1\) & \(21.1 2.3\) & \(22.8 3.9\) & \(\) \\  & & (1, 5) & \(31.0 0.9\) & \(25.7 2.8\) & \(24.9 1.6\) & \(25.0 1.7\) & \(19.3 1.2\) & \(\) \\  & & (5, 1) & \(26.8 7.1\) & \(23.0 5.4\) & \(23.2 4.6\) & \(17.5 3.4\) & \(25.7 6.0\) & \(\) \\  & & (5, 5) & \(27.7 6.7\) & \(\) & \(27.7 7.8\) & \(23.2 6.3\) & \(14.1 3.6\) & \(25.6 6.0\) \\   & & (1, 1) & \(29.0 5.3\) & \(21.7 4.1\) & \(27.6 3.7\) & \(45.7 9.9\) & \(6.1 1.0\) & \(\) \\  & & (1, 5) & \(61.5 5.2\) & \(52.0 6.5\) & \(34.7 5.9\) & \(57.3 4.8\) & \(17.6 2.7\) & \(\) \\  & & (5, 1) & \(8.1 0.7\) & \(7.4 0.8\) & \(8.8 2.2\) & \(18.0 2.6\) & \(4.4 0.5\) & \(\) \\  & & (5, 5) & \(6.7 1.2\) & \(7.4 1.2\) & \(10.6 3.3\) & \(12.5 1.8\) & \(5.5 0.7\) & \(\) \\   & & (1, 1) & \(33.7 3.0\) & \(35.0 6.2\) & \(33.4 5.5\) & \(17.1 2.4\) & \(9.9 3.6\) & \(\) \\  & & (1, 5) & \(72.7 2.6\) & \(74.2 1.4\) & \(71.1 2.2\) & \(61.8 2.5\) & \(24.4 3.3\) & \(\) \\  & & (5, 1) & \(2.4 0.5\) & \(4.6 1.5\) & \(4.2 0.4\) & \(3.8 1.3\) & \(1.3 1.1\) & \(\) \\  & & (5, 5) & \(2.0 0.9\) & \(2.9 2.2\) & \(0.9 0.7\) & \(2.9 1.1\) & \(-1.2 0.4\) & \(\) \\   

Table 4: Performance comparison on Scenario 2 (time-dependently collected dataset). Each experiment is repeated 5 times, and the average normalized scores with their standard errors are reported. The highest mean performance scores are highlighted in bold.

Figure 4: Illustrative examples for generating datasets for Scenario 2 and 3.

#### 4.4.2 Results

Scenario 1Table 3 summarizes the performance comparison on the rebalanced datasets. From the table, we observe that DriiDICE overallly outperforms other methods across different proportions and tasks and outperforms 14 out of 15 problem settings with signficant margins. DemoDICE, AW-BC and OptiDICE-BC fail to outperform BC more than half of the problem settings, which demonstrates the distribution matching and the best-case weighting are not robust to these shift scenarios. Moreover, we can conclude a robustness of DriiDICE is not gifted by side effects from the implementation of DICE algorithms at least this scenario. DR-BC, which does not incorporate Bellman flow constraints, also exhibits improved robustness, outperforming BC in 9 out of 15 problem settings. However, DriiDICE consistently surpasses DR-BC across all settings, illustrating that the inclusion of Bellman flow constraints is crucial for effectively addressing the covariate shift of our interest.

Scenario 2As depicted in Table 4, DriiDICE shows robust overall performance, achieving the highest mean performance in 11 out of 12 settings. Remarkably, while all baseline methods, including DR-BC, fail to surpass BC more than half of the problem settings, only DriiDICE consistently demonstrates exceptional robustness. This emphasizes that the critical role of incorporating Bellman flow constraint to enhance robustness in scenarios involving this type of covariate shift.

Scenario 3Figure 5 presents a performance comparison across three metrics while varying the number of segments. As illustrated, DriiDICE's normalized score consistently increases as the number of segments grows, and the target MSE exhibits a steady decreases maintaining levels that are lower or comparable to those of other approaches. The robust performance metrics also display consistent behavior, with an exception of walker2d task. Interestingly, we observed that a weak correlation between the target MSE and the episode return (see Figure E in the supplementary material).

In summary, by constraining the uncertainty set to a plausible set, DriiDICE effectively minimizes target MSE and enables BC to imitate the agent robust to various covariate shift scenarios.

## 5 Conclusions and Limitations

We propose DriiDICE, a offline IL approach that is robust to the covariate shift caused by the data distribution deviated from the stationary distribution of the expert. By optimizing the Bellman-flow-constrained worst-case objective, our approach effectively minimize the surrogate loss of the expected error w.r.t non-shifted target distribution. We also suggests an extensive set of practical covariate shift scenarios of our interest and empirically show that DriiDICE successfully imitate the expert robust to those shift scenarios. For a limitation, we don't consider uncertainty from transition shift or noisy demonstrations. We expect that extending our approach into such scenario will be beneficial on many practical scenarios, such as Sim2Real or transfer learning tasks. Moreover, we believe exploring a real-world application of our method is also an attractive extend of the future work.

Figure 5: Performance comparison on Scenario 3 (segmented dataset) along the number of segments. The points and shaded areas indicate the means and standard errors measured over 5 repetitions.