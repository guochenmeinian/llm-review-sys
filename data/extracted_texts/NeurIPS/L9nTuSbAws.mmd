# GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients

GradOrth: A Simple yet Efficient Out-of-Distribution Detection with Orthogonal Projection of Gradients

 Sima Behpour &Thang Doan &Xin Li &Wenbin He &Liang Gou &Liu Ren

Bosch Research North America, Bosch Center for Artificial Intelligence (BCAI)

{sima.behpour, thang.doan, xin.li9, wenbin.he2, liang.gou, liu.ren}@us.bosch.com

###### Abstract

Detecting out-of-distribution (OOD) data is crucial for ensuring the safe deployment of machine learning models in real-world applications. However, existing OOD detection approaches primarily rely on the feature maps or the full gradient space information to derive OOD scores neglecting the role of **most important parameters** of the pre-trained network over in-distribution (ID) data. In this study, we propose a novel approach called GradOrth to facilitate OOD detection based on one intriguing observation that the important features to identify OOD data lie in the lower-rank subspace of ID data. In particular, we identify OOD data by computing the norm of gradient projection on _the subspaces considered **important for the in-distribution data**. A large orthogonal projection value (i.e., a small projection value) indicates the sample as OOD as it captures a weak correlation of the ID data. This simple yet effective method exhibits outstanding performance, showcasing a notable reduction in the average false positive rate at a 95% true positive rate (FPR95) of up to 8% when compared to the current state-of-the-art methods.

## 1 Introduction

The issue of identifying out-of-distribution (OOD) data, which falls outside training data distributions, has become a significant focus in deep learning. OOD data challenges real-world model deployment, as it can lead to unreliable or incorrect predictions, particularly in safety-critical applications such as healthcare, autonomous vehicles, and physical sciences . This problem arises because modern Deep Neural Networks (DNNs) produce overconfident predictions on OOD inputs, complicating the separation of in-distribution (ID) and OOD data . The main goal of OOD detection is to develop methods that can accurately detect when a model encounters OOD data, allowing the model to either reject these inputs or provide more informative responses, such as uncertainty indication or confidence measures.

Many studies have investigated approaches to detecting OOD in deep learning . The majority of prior work focused on calculating OOD uncertainty from the activation space of a neural network, for example, by using model output  or feature representations . Another line of studies like ODIN , GradNorm , and ExGrad  leverage the gradient information of deep neural network models to compute OOD uncertainty score and achieve performant results. GradNorm  investigates the richness of the gradient space and presents that gradients provide valuable information for OOD detection. In particular, GradNorm utilizes the vector norm of gradients explicitly as an OOD scoring function. GradNorm, however, considers the full gradient space information, which might be noisy and lead to sub-optimal solutions.

A recent direction of research employs network parameter sparsification to improve OOD detection performance like DICE  and ASH . ASH removes a majority of the activation by obtaining the \(p\)th-percentile of the entire representation. However, the potential consequence may result in diminished performance due to the partial removal of critical parameters within the pre-trained network. Also, this is an empirical approach without a principled way to sparsify models.

Based on key observations presented in _gradient-based_ and _sparcification-based_ OOD detection methods, an intriguing insight emerges: the crucial discriminative features for OOD data identification reside within the gradient subspace of the ID data. This suggests that by focusing on the gradient information in the subspace of the ID data, which captures the most salient information, we can enhance the accuracy and reliability of OOD data detection algorithms. However, it takes non-trivial work to identify such an efficient gradient subspace.

Inspired by recent low-rank factorization research for DNNs [59; 52; 20; 51], indicating the intrinsic model information resides in a few low-rank dimensions, we introduce a novel approach named **GradOrth**. More specifically, the proposed method, GradOrth, distinguishes OOD samples by employing _orthogonal gradient projection_ in the _low-rank subspaces_ of ID data (figure 1). These ID subspaces (\(S^{L}\) in figure 1) are derived through singular value decomposition (SVD) of pre-trained network activations, specifically on a small subset of randomly selected ID data. By leveraging SVD, GradOrth effectively computes and identifies the relevant subspaces associated with the ID data, enabling accurate discrimination of OOD samples through orthogonal gradient projection. A large magnitude (figure 1-a) of orthogonal projection (i.e., small projection) serves as a significant criterion for classifying a sample as OOD since it captures a weak correlation with the ID data.

Our key results and contributions are:

- We present GradOrth, a novel and efficient method for out-of-distribution (OOD) detection. Our approach leverages the **most important** parameter space of a pre-trained network and its **gradients** to accomplish this task. To the best of our knowledge, GradOrth is the pioneering endeavor to investigate and showcase the efficacy of the subspace of a DNN's gradients in OOD detection.

- We evaluate the performance of GradOrth on widely-used benchmarks, and it demonstrates competitive results compared to other post-hoc OOD detection baselines. Notably, GradOrth outperforms the strong baseline methods by consistently reducing the false positive rate at the 95th percentile (FPR95) by a margin ranging from 2.71% to 8.05%. Moreover, our experiments highlight that GradOrth effectively enhances OOD detection capabilities while maintaining high accuracy in classifying in-distribution (ID) data.

- We present a comprehensive analysis, including ablation experiments and theoretical investigation, aimed at enhancing the understanding of our proposed method for OOD detection. Through these rigorous analyses, we aim to provide valuable insights and improve the overall comprehension of the intricacies and effectiveness of our OOD detection approach.

## 2 Background and Our Notations

We consider a neural network with \(L\) layers and a set of learning parameters \(=\{^{l}\}_{l=1}^{L}\) where \(^{l}\) present the learning parameters of layer \(l\). \(x_{i}^{l}\) denotes the **representation of input**\(x_{i}\) at layer \(l\) in the successive layers given the data input \(x_{i}\). The network performs the following computation at each layer:

\[x_{i}^{l+1}=(f(^{l},x_{i}^{l})), l=1,...L,\] (1)

where, \((.)\) is a non-linear function and \(f(.,.)\) is a linear function. We leverage matrix notation for input (\(_{i}\)) in convolutional layers and vector notation for input (\(x_{i}\)) in fully connected layers. In the first layer, \(x_{i}^{1}=x_{i}\) refers to the raw input data. It is noteworthy to mention that our approach holds applicability across all layers of the network. However, our experimental investigations reveal that the last layer yields the most optimal performance, please refer to appendix, section B for our

Figure 1: The main idea of _GradOrth_: Measuring the orthogonal projection of the gradient of a testing sample on a \(k\)-dimension (e.g \(k\)=2 here) subspace of pre-trained network on ID data. We show the online \(\) between \(9(_{x}^{l})=_{^{l}}(})\) and \(^{l})}=P_{i}^{L}(_{^{l}}(}))\). If \(\) is large (\(k\),..., small projection on subspace \(^{L}\)), shown in (a), the sample \(x_{i}\) is weakly correlated to ID data, and therefore it is recognized as OOD. Otherwise, it is ID data, shown in (b).

empirical studies report. This preference is advantageous as it alleviates significant time complexity arising from gradient computations across multiple network layers.

### Input and Gradient Space

Our algorithm capitalizes on the intrinsic property of stochastic gradient descent (SGD) updates lying within the span of input data points, as validated by . The subsequent subsections present this relationship, specifically in fully connected layers. We present the details regarding convolutional layers in the appendix, section A.

**Fully Connected Layer** Consider a single-layer linear neural network in a supervised learning setup where each (input, label) training data pair is driven from a training dataset, \(\). We use \(^{n}\) to present the input vector, \(^{m}\) to present the label vector in the dataset, and \(^{m n}\) to express the learning parameters (weights) of the network. In general, the network is trained by minimizing a loss function (e.g. mean-squared error) as follows:

\[=||-||_{2}^{2}.\] (2)

Following stochastic gradient optimization, we can present the gradient of this loss with respect to weights as:

\[_{}=(-)^{T}=^{T},\] (3)

Here, \(^{m}\) denotes the error vector. Consequently, the gradient update will reside within the input span (\(\)), wherein the elements in \(\) exhibit varying magnitudes, thus influencing the scaling of \(\) accordingly, please refer to section E for the proof. For simplicity, we have considered per-example loss (batch size of 1) and mean-squared loss function here. Furthermore, it is important to mention that the aforementioned relationship remains applicable even in the context of the mini-batch setting or when utilizing alternative loss functions such as cross-entropy loss, where the calculation of \(\) may differ. For more comprehensive information on this subject, please consult the appendix, specifically section D. The input-gradient relationship in equation 3 can be applied to any fully connected layer of a neural network where \(\) is the input to that layer and \(\) is the error coming from the next layer. In addition, this equation also applies to the networks with non-linear units (such as ReLU) and cross-entropy losses, though \(\) will be calculated differently.

### Matrix Approximation with SVD:

In our algorithm, we utilize singular value decomposition (SVD) for matrix factorization. Specifically, a rectangular matrix \(=^{T}^{m n}\) can be factorized using SVD into the product of three matrices. Here, \(\) represents a matrix containing the singular values sorted along its main diagonal, while \(^{m m}\) and \(^{n n}\) denote orthogonal matrices . In the case where the rank of the matrix \(\) is \(r\) (\(r(m,n)\)), the matrix \(\) can be represented as \(=_{i=1}^{r}_{i}_{i}_{i}^{T}\), where \(_{i}()\) denotes the singular values and \(_{i}\) as well as \(_{i}\) represent the left and right singular vectors, respectively. Furthermore, we can formulate the \(k\)-rank approximation to the matrix as \(_{k}=_{i=1}^{k}_{i}_{i}_{i}^{T}\), where \(k r\). The specific value of \(k\) can be determined as the smallest value that satisfies the condition \(||_{k}||_{F}^{2}_{th}||||_{F}^{2}\). In this equation, \(||.||F\) represents the Frobenius norm of the matrix, and \(_{th}\) (\(0<_{th} 1\)) serves as the threshold . For a comprehensive explanation of the SVD method, more explanation regarding \(k\)-rank matrix approximation, and the impact of \(_{th}\) in method performance please refer to the appendix sections F, H, I, respectively.

### Problem Statement: Out-of-distribution Detection

OOD is typically characterized by a distribution that represents unknown scenarios encountered during deployment. These scenarios involve data samples originating from an irrelevant distribution, whose label set has no intersection with the predefined set. Consider the supervised setting where a neural network is given access to a set of training data \(=\{(_{i},y_{i})\}_{i=1}^{N}\) drawn from an unknown joint data distribution \(P\) defined on \(\) in the training phase. We denote the input space and output space by \(=^{n}\) and \(=\{1,2,...,m\}\), respectively.

\[z()=,&O()\\ ,&O()<.\] (4)

The parameter \(\) is typically selected to ensure a high percentage of correct classification for in-distribution (ID) data, such as 95%. A major hurdle is to establish a scoring function \(O()\) that effectively captures the uncertainty associated with OOD samples. Prior approaches have predominantly relied on various factors, including the model's output, gradients, or features, to estimate OOD uncertainty [23; 12]. In our proposed approach, we aim to compute the scoring function \(O()\) by leveraging _orthogonal gradient projection_ on _parameter subspace_ of a pre-trained network over ID data. The details of our methodology are described in the subsequent section.

### Orthogonal Projection

In this section, we discuss the concept of orthogonal projection and our notation, which holds significant importance in our methodology. To simplify the explanation, we will present it in a 2D space, but it can be extended to higher dimensions. Orthogonal projection serves as a metric that we utilize to calculate the distance between a vector \(\) and a space W (represented as a matrix in this case).

The result of the orthogonal projection of vector \(\) onto space W consists of three essential components: (1) The orthogonal projection vector \(\), (2) the projection vector \(\), and (3) the angle \(\). These three components' values can be utilized to determine the correlation between the vector \(\) and the space W. As depicted in the figure 2, a larger value of \(\) indicates a weaker correlation. On the other hand, \(\) exhibits the opposite pattern, where a larger value of \(\) indicates a stronger correlation. Depending on the specific application requirements, any of these values can be chosen to compute the correlation. In order to align with the OOD score, where a smaller value indicates a higher degree of OODness as presented in equation 4, we incorporate the projection vector (\(\)) into our computations.

## 3 Our Method: GradOrth

In this section, we describe our method GradOrth where we recognize ID Vs. OOD data from a different view of previous studies, computing the norm of gradient projection on _the subspaces considered important for the in-distribution data_.

We define a sample as OOD data if its orthogonal projection value is large (i.e., small projection value), indicating a weak correlation with the ID data.

GradOrth OOD detection is developed following these steps:

1. **Pre-trained Network subspace Computation:** Our \(L\)-layer neural network with learning parameter \(\) is trained using **ID** data. Upon completion of the training process, the model parameters \(\) are frozen, resulting in a pre-trained network specialized in ID data. It is worth mentioning that we can also leverage the existing pre-trained network over our interest ID data. To retain the most significant parameters of the pre-trained network with respect to the ID data, we compute the network's last layer (\(L\)) subspace. For this purpose, we construct a representation matrix denoted as \(^{L}_{ID}=[^{L}_{1},^{L}_{2},...,^{L}_{n}]\), which concatenates \(n\) representations obtained from the network's last layer (\(L\)) through the forward pass of \(n\) randomly selected samples (a small subset, \(n N\)) of the ID data. Next, we perform SVD on \(^{L}_{ID}\), resulting in \(^{L}_{ID}=^{L}_{ID}^{L}_{ID}(^{L}_{ID})^{T}\). We then proceed to approximate its rank \(k\) by obtaining \((^{L}_{ID})_{k}\), guided by the given criteria that rely on a specified threshold, denoted as \(_{th}\): \[\|(^{L}_{ID})_{k}\|^{2}_{F}_{th}\|^{L}_{ID}\|^{2}_{F}.\] (5) The pre-trained network subspace, denoted as \(S^{L}=span\{^{L}_{1},^{L}_{2},...,^{L}_{k}\}\), is defined as the **space of significant representation** for the pre-trained network at the last layer \(L\). This subspace is spanned by the first \(k\) vectors in \(U^{L}_{ID}\) and encompasses all directions associated with the highest singular values in the representation. We store this subspace, \(S^{L}\), and leverage it in the next step. We present the algorithm to compute the ID subspace in Algorithm 1.
2. **Inference with OOD Data:** During the inference phase, the pre-trained model is exposed to an OOD sample \(x_{i}\). The OOD sample is propagated through the pre-trained network, and subsequently, its gradient at layer L is computed which is presented as \(g(x_{i})=_{^{L}}(^{L})\).

Figure 2: Orthogonal Projection

In accordance with the GradNorm approach, we calculate the cross-entropy loss by comparing the model's predicted softmax probability to a uniform vector used as the target. Consequently, during testing, we employ an all-one vector as the ground truth, assuming a uniform distribution for the target data.
3. **Detector Construction:** The model is transformed into a _detector_ by generating a **score** based on its output, enabling the differentiation between ID and OOD inputs. To this end, we compute the norm of sample gradient projection onto the subspace of the pre-trained network (\(S\)). We compute projection of the gradients \(_{^{L}}(^{L})\) onto the subspace \(^{L}\) as follows: \[P_{S^{L}}(_{^{L}}(^{L}))=(_{^{L}}(^{L}))^{L}(^{L})^{ }.\] (6) Here, \((.)^{}\) presents the matrix transpose. Next, we define the OOD score for the sample as follows by computing the projection norm: \[O(x_{i})=\|P_{S^{L}}(_{^{L}}(^{L}))\|\] (7) This score serves as a surrogate to characterize the correlation between the sample and ID data that the pre-trained network trained on it. As presented in figure 1, it implies a weak correlation between the new sample \(x_{i}\) and ID when the gradient \(g(x_{i})=_{^{L}}(^{L})\) has a small projection (large orthogonal projection) onto the subspace of the pre-trained network (large angle \(\)) due to the fact that stochastic gradient descent (SGD) updates lie in the span of input data points , please refer to the appendix, section E for the proof. Algorithm 2 presents OOD score computation.

```
1:function Compute subspace (\(f_{}\), \(_{ID}\), \(_{th}\) )
2://Initialization
3:\(^{L}[\ ]\)
4:\(B_{n}\) Sample a mini-batch of size \(n\) from ID data (\(_{ID}\))
5:\(\) Pre-trained network \(f\) learning parameters
6:// subspace Computation
7:\(^{L}_{ID}\) forward\((B_{n},f())\)
8:\(^{L}_{ID}\) SVD\((}^{L}_{ID})\)
9:\(k\) criteria\((}^{L}_{ID},^{L}_{ID},^{t}_{th})\) // Refer to equation 5
10:\(^{L}^{L}_{ID}[0:k]\)
11:return\(^{L}\) ```

**Algorithm 1** ID Subspace Computation

## 4 Experiments

In this section, we evaluate the performance of our method _GradOrth_ running extensive experiments considering different ID/OOD datasets and network architectures. We follow the experiment setting in general OOD baselines and explain the experimental setup in section 4.1. These empirical studies demonstrate the superior performance of _GradOrth_ over existing state-of-the-art baselines that are reported in section 4.1. We report extensive ablations and analyses that provide a deeper understanding of our methodology, please refer to the appendix, section G.

### Experimental Setup

**Dataset** We leverage 2 benchmarks proposed by  and  for detecting OOD images that are based on the large-scale ImageNet dataset and CIFAR dataset. To provide a fair comparison, we adopt an average results-over-5-run approach. In each run, distinct random seeds are employed to select random samples from each class, generating small subsets of in-distribution data. Subsequently, we compute the subspace of the pre-trained network based on these subsets. The OOD scores of the test data are then calculated, and FPR95 and AUROC scores are derived. This process is repeated five times, and the average of these five runs is reported as the final score.

**ImageNet Benchmark:** This benchmark is more challenging than others because it has higher-resolution images and a larger label space of 1,000 categories. To test our approach, we evaluate four OOD test datasets, including subsets of iNaturalist , SUN , Places , and Textures. These datasets have non-overlapping categories compared to the ImageNet-1k dataset and cover a diverse range of domains including fine-grained, scene, and textural images. We follow the experimental setting reported in  and use the Resnet-50 model  pre-trained on ImageNet-1k. For a fair comparison, all the methods use the same pre-trained backbone, without regularizing with auxiliary outlier data. Details and hyperparameters of baseline methods can be found in appendix J.1. The outcomes of this study present results obtained by applying GradOrth after the last fully connected layer in all the experiments. In this configuration, the feature size is \(2048\) for ResNet-50 and \(1280\) for MobileNetV2. For subspace computation, we choose 10 random samples per class and set the SVD threshold to \(0.97\).

**CIFAR Benchmark:** We evaluate our approach on the commonly used CIFAR-10 , and CIFAR-100  benchmarks as in-distribution data following the experimental setting in [12; 50]. We employ the standard split with 50,000 training images and 10,000 test images. For subspace computation, we choose 5 random samples per class and set the SVD threshold to \(0.97\). We assess the model on six widely used OOD benchmark datasets: Textures , SVHN , Places365 , LSUN-Crop , LSUN-Resize , and iSUN . Regarding pre-trained network architecture, we use DenseNet-101 architecture . We leverage pre-trained networks over ID datasets. Please refer to section J.1 in the appendix for more details regarding the experiment setting. It is important to note that no modifications were made to the network parameters during the OOD detection phase.

Evaluation MetricsWe assess the effectiveness of our proposed method by utilizing threshold-free metrics that are commonly used for evaluating OOD detection, as standardized in . These metrics include (i) AUROC, which stands for the Area Under the Receiver Operating Characteristic curve; and (ii) FPR95, which is the false positive rate. FPR95 represents the probability that a negative (i.e., OOD) example is misclassified as positive (i.e., ID) when the true positive rate is as high as 95 .

### Results and Discussion

Our experimental studies present the promising performance of OrthoGrad in OOD detection on two benchmarks, ImageNet and CIFAR benchmarks.

#### 4.2.1 Experimental Results on Out-of-Distribution Detection

ImageNet Benchmark:Our method demonstrates competitive performance, reaching the state-of-the-art level, as indicated in table 1. On the Resnet pre-trained network, GradOrth surpasses ASH-S, ASH-B, and ASH-S by \(0.45\%\), \(2.47\%\), and \(0.93\%\) in terms of FPR95 on the iNaturalist, SUN, and Textures OOD datasets, respectively. When evaluated on the Places OOD dataset, our method achieves an FPR95 of \(33.67\%\) and secures the second rank after ASH-B. Furthermore, GradNorm demonstrates an average FPR95 performance of \(18.57\%\), outperforming ASH-B by \(3.98\%\).

It is important to acknowledge the fact that GradOrth boasts a _low computational complexity_. It only requires computing the subspace of the pre-trained network once and can be conveniently utilized through a simple gradient calculation, without the need for hyper-parameter tuning or additional training during OOD detection. In contrast, certain methods like Mahalanobis  require collecting feature representations from intermediate layers for the entire training set, which can be computationally expensive for large-scale datasets like ImageNet. Additionally, GradOrth presents a _stable performance_ across most datasets whereas the performance of ASH versions varies across the four OOD datasets. ASH-B outperforms other baselines on the Places dataset but ranks third, second, and third on the other three datasets. A similar pattern is observed for ASH-S in terms of FPR95, where it ranks second, sixth, sixth, and second across the iNaturalist, SUN, Places, and Textures datasets, respectively.

GradOrth also exhibits superior performance in terms of AUROC, outperforming ASH-S by an average of \(2.80\%\) across the four datasets. Particularly, GradOrth surpasses ASH-S, ASH-B, and ASH-S by \(0.13\%\), \(0.66\%\), and \(0.46\%\) on the iNaturalist, SUN, and Textures OOD datasets, respectively.

For the pre-trained MobileNet model, our GradOrth approach also demonstrates outstanding performance. We present experimental results on leveraging MobileNet as the pre-trained ID network and evaluate the OOD detection performance on the iNaturalist, SUN, Places, and Textures datasets (the bottom section of table 1). In these experiments, GradOrth demonstrates outstanding performance. In terms of FPR95, GradOrth outperforms ASH-B, DICE+ReAct, DICE+ReAct, and ASH-S by \(4.65\%\), \(0.40\%\), \(6.50\%\), and \(0.43\%\), respectively, across the four datasets. Regarding AUROC, GradOrthoutperforms other baselines on average by at least \(4.01\%\) and \(0.57\%\) in terms of FPR95 and AUROC, respectively.

CIFAR Benchmark: In this research study, we further investigate the performance of GradOrth by conducting additional experimental studies on the CIFAR10 and CIFAR100 datasets. The key observation is that no single method consistently outperforms all other methods across diverse datasets. However, it is noticeable that GradOrth rank is always among the top three across six OOD datasets. This feature presents its promising performance for OOD detection. On the CIFAR10 dataset, GradOrth demonstrates superior performance compared to other baseline methods across six OOD datasets, namely SVHN, LSUN-c, LSUN-r, iSUN, Textures, and Places365. On average, GradOrth outperforms these baselines by \(2.71\%\) and \(0.32\%\) in terms of FPR95 and AUROC, respectively. Detailed experimental results can be found in table 2. In the LSUN-c OOD dataset, DICE demonstrates superior performance with an impressive \(0.26\%\) FPR95, placing it at the top. Our method, on the other hand, ranks second with a respectable \(0.81\%\) FPR95. However, the ranking differs when examining the Textures and Places365 datasets. Notably, Gradorth outperforms other baseline methods in both cases, achieving noteworthy FPR95 values of \(20.63\%\) and \(38.22\%\), respectively. In contrast, DICE attains the sixth and ninth positions in these datasets, displaying comparatively higher FPR95 rates of \(41.90\%\) and \(48.59\%\).

On the CIFAR100 dataset, GradOrth surpasses its competitors in both FPR95 and AUROC by an average margin of \(8.0\%\) and \(2.80\%\), respectively, across six well-known OOD datasets. Detailed experimental results are provided in table 3. For a comprehensive discussion and analysis of the CIFAR benchmark, please refer to the appendix, specifically section C.

   &  &  &  &  &  &  \\   & & FPR95 & AUROC & **FPR95** & AUROC & **FPR95** & AUROC & **FPR95** & AUROC & **FPR95** & AUROC & FPR95** & AUROC \\   & Softmax score & 54.99 & 87.74 & 70.83 & 80.86 & 73.99 & 79.76 & 68.00 & 79.61 & 66.95 & 81.99 &  \\  & ODN & 47.66 & 89.96 & 60.15 & 84.59 & 67.89 & 81.78 & 50.23 & 85.62 & 56.48 & 85.41 & 85.41 \\  & Mahalanobis & 97.00 & 52.65 & 89.50 & 42.41 & 98.40 & 41.79 & 58.30 & 85.01 & 87.43 & 55.47 &  \\  & Entropy score & 55.72 & 89.95 & 59.26 & 83.89 & 64.92 & 82.86 & 53.27 & 55.99 & 54.41 & 86.17 \\  & GradOrth & 42.46 & 90.33 & 40.73 & 89.96 & 43.48 & 80.66 & 34.38 & 88.43 & 40.29 & 57.34 \\  & Erdöf & 54.11 & 76.91 & 46.73 & 69.74 & 50.62 & 74.27 & 38.12 & 79.37 & 47.40 & 75.90 \\  & Erdöf & 20.38 & 96.22 & 24.02 & 94.20 & 33.55 & 91.58 & 47.30 & 89.80 & 31.43 & 92.95 &  \\  & DICE & 25.63 & 94.49 & 25.51 & 90.83 & 46.69 & 87.48 & 31.72 & 90.30 & 34.75 & 90.77 \\  & DICE + ReAct & 18.64 & 96.24 & 25.45 & 93.94 & 36.36 & 90.67 & 26.87 & 92.74 & 27.25 & 93.40 \\  & VRA-DN & 16.82 & 96.92 & 30.63 & 96.56 & 39.94 & 90.75 & 26.72 & 95.90 & 28.53 & 94.09 \\  & VRA-P & 15.70 & 97.12 & 26.94 & 94.12 & 94.25 & 73.95 & 91.27 & 21.47 & 95.62 & 24.59 & 94.57 \\  & AISH-P & 44.57 & 92.51 & 52.88 & 83.35 & 61.79 & 85.58 & 42.06 & 89.70 & 50.32 & 89.04 \\  & AISH-B & 14.21 & 97.32 & 22.08 & 95.10 & **30.45** & **92.31** & 21.17 & 95.50 & 22.73 & 95.66 \\  & AISH-B & 11.49 & 97.87 & 27.98 & 94.02 & 39.78 & 90.98 & 11.93 & 97.60 & 22.80 & 95.12 \\  & OntoOrth (Ours) & **18.04\(\)10.20** & 98.01 & **19.41\(\)12.76** & **98.76\(\)10** & 93.76 & 91.78 \(\)10.21 & **11.90\(\)10** & **97.92\(\)10** & **18.75\(\)10** & **93.43\(\)10** \\   & Softmax score & 64.29 & 85.32 & 70.72 & 77.02 & 77.19 & 79.23 & 76.27 & 73.51 & 77.30 & 73.51 & 79.00 \\  & ODN & 55.39 & 87.62 & 84.07 & 85.88 & 57.36 & 84.71 & 49.96 & 85.03 & 54.20 & 85.81 \\  & Mahalanobis & 62.11 & 81.00 & 47.82 & 86.53 & 52.09 & 86.53 & 92.38 & 30.66 & 63.00 & 71.01 \\  & Energy score & 59.50 & 88.91 & 62.68 & 84.50 & 69.87 & 81.19 & 58.05 & 85.08 & 62.39 & 84.91 \\  & ReAct & 42.40 & 91.53 & 47.69 & 88.16 & 51.56 & 86.64 & 84.24 & 91.53 & 45.02 & 89.47 \\  & DICE & 43.09 & 90.83 & 86.96 & 90.46 & 53.11 & 85.81 & 35.90 & 91.30 & 41.92 & 89.60 \\  & DICE + ReAct & 32.30 & 90.57 & 35.12 & 92.26 & 98.66 & 47.68 & 80.02 & 16.28 & 92.52 & 31.64 & 92.68 \\  & AISH-P & 54.92 & 90.46 & 58.61 & 86.72 & 66.69 & 83.27 & 44.48 & 88.72 & 37.15 & 87.34 \\  & AISH-B & 31.46 & **92.48** & 34.58 & 91.61 & 51.80 & 87.56 & 20.92 & 50.70 & 35.66 & 92.13 \\  & AISH-B & 39.10 & 91.91 & 4.362 & 90.02 & 58.84 & 84.73 & 11.12 & 97.01 & 38.67 & 90.95 \\  & GradOrth (Ours) & **26.81\(\)11** & 93.17 & 17.12 & **30.31\(\)12** & **93.18\(\)11** & **40.27\(\)11** & **42.69\(\)11** & **97.25\(\)11** & **27.68\(\)10** & **93.22\(\)11** \\  

Table 1: OOD detection results with **ImageNet-1k** as ID. GradOrth present outstanding performance on average and across most datasets. We adopted the identical table format and evaluation metrics as introduced in . The ResNet and MobileNet models are pre-trained solely with ID data from the ImageNet-1k dataset. We use \(\) denote the latter values are preferable, and \(\) to denote that smaller values are preferable. All values are presented as percentages. All values in the table are directly taken from table \(\) (12) except for the gradient-based methods. (GradNorm, ExGrad, GradOrth (ours)). For GradNorm and ExGrad, we run this experiment leveraging the code provided by the authors.

 \

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_FAIL:9]

input perturbation, and the OOD scores were still calculated based on the output space of the perturbed inputs. GradNorm  also utilizes gradient information from a neural network to detect distributional shifts between ID and OOD samples. By measuring the norm of gradients with respect to the network's input, it quantifies uncertainty and identifies OOD samples causing significant output changes. ExGrad, proposed by , introduces a method akin to GradNorm with two notable distinctions. Firstly, the label distribution of \(y\) is derived from the model's predicted distribution (\(P\)) as opposed to the uniform distribution. Secondly, ExGrad computes the expected norm of the gradient, in contrast to GradNorm which calculates the norm of the expected gradient.

Discriminative Models for OOD Uncertainty EstimationThe problem of classification with rejection has a long history, dating back to early works on abstention such as  and , which considered simple model families like SVMs . However, the phenomenon of neural networks' overconfidence in OOD data was not revealed until the work of .

Early efforts aimed to improve OOD uncertainty estimation by proposing the ODIN score  and Mahalanobis distance-based confidence score . More recently,  proposed using an energy score derived from a discriminative classifier for OOD uncertainty estimation, showing advantages over the softmax confidence score both empirically and theoretically.  demonstrated that an energy-based approach can improve OOD uncertainty estimation for multi-label classification networks. Additionally,  revealed that approaches developed for common CIFAR benchmarks might not effectively translate into a large-scale ImageNet benchmark, highlighting the need to evaluate OOD uncertainty estimation in a large-scale real-world setting. These developments have brought renewed attention to the problem of classification with rejection and the need for effective OOD uncertainty estimation.

Generative Models for OOD Uncertainty EstimationDetection of OOD inputs is a crucial problem in machine learning. One popular approach is to use generative models that estimate the density directly. Such models can identify OOD inputs as those lying in low-likelihood regions. To this end, a plethora of literature has emerged to leverage generative models for OOD detection. However, recent studies have shown that deep generative models can assign high likelihoods to OOD data, rendering such models less effective in OOD detection. Additionally, these models can be challenging to train and optimize, and their performance may lag behind their discriminative counterparts. In contrast, our approach relies on a discriminative classifier, which is easier to optimize and achieves stronger performance. While some recent works have attempted to improve OOD detection with generative models using improved metrics, likelihood ratios, and likelihood regret, our approach leverages the energy score from a discriminative classifier and has demonstrated significant advantages over generative models in OOD detection.

Distributional ShiftsThe problem of distributional shift has garnered significant attention in the research community. It is essential to recognize and distinguish between different types of distributional shift problems. In the literature on OOD detection, the focus is typically on ensuring model reliability and detecting label-space shifts [18; 32; 34], where OOD inputs have labels that are disjoint from the ID data, and as such, should not be predicted by the model. On the other hand, some studies have examined covariate shifts in the input space [17; 37; 42], where inputs may be subject to corruption or domain shifts. However, covariate shifts are commonly used to evaluate model robustness and domain generalization performance, where the label space \(\) remains the same during test time. It is worth noting that our work focuses on the detection of shifts where the model should not make any predictions, as opposed to covariate shifts where the model is expected to generalize.

## 6 Conclusion

In this paper, we propose GradOrth, a novel OOD uncertainty estimation approach utilizing information extracted from the _important parameter space for ID data_ and _gradient space_. Extensive experimental results show that our gradient-based method can improve the performance of OOD detection by up to \(8.05\%\) in FPR95 on average, establishing superior performance. We hope that our research brings to light the informativeness of gradient subspace, and inspires future work to utilize it for OOD uncertainty estimation. In our future research, our objective is to investigate GradOrth's capabilities considering different directions like influence functions , novelty detection in open-world context , data pre-selection , and underspecification .