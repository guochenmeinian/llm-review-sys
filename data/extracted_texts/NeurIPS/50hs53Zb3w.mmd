# Recovering Simultaneously Structured Data via Non-Convex Iteratively Reweighted Least Squares

Christian Kummerle

Department of Computer Science

University of North Carolina at Charlotte

Charlotte, NC 28223, USA

kuemmerle@uncc.edu

&Johannes Maly

Department of Mathematics

Ludwig-Maximilians-Universitat Munchen

80799 Munich, Germany and

Munich Center for Machine Learning (MCML)

maly@math.lmu.de

###### Abstract

We propose a new algorithm for the problem of recovering data that adheres to multiple, heterogeneous low-dimensional structures from linear observations. Focusing on data matrices that are simultaneously row-sparse and low-rank, we propose and analyze an iteratively reweighted least squares (IRLS) algorithm that is able to leverage both structures. In particular, it optimizes a combination of non-convex surrogates for row-sparsity and rank, a balancing of which is built into the algorithm. We prove locally quadratic convergence of the iterates to a simultaneously structured data matrix in a regime of minimal sample complexity (up to constants and a logarithmic factor), which is known to be impossible for a combination of convex surrogates. In experiments, we show that the IRLS method exhibits favorable empirical convergence, identifying simultaneously row-sparse and low-rank matrices from fewer measurements than state-of-the-art methods.

## 1 Introduction

Reconstructing an image from (noisy) linear observations is maybe the most relevant inverse problem for modern image processing and appears in various applications like medical imaging and astronomy . If the latent image is \(n\)-dimensional, for \(n\), it is well-known that \((n)\) observations are required for robust identification in general. In practice, imaging problems are however often ill-posed, i.e., the number of observations is smaller than \(n\) or the operator creating the observations is defective . In such situations, the fundamental lower bound of \((n)\) can be relaxed by leveraging structural priors of the latent image in the reconstruction process.

Of the various priors that are used for solving ill-posed inverse problems in the literature, sparsity1 and low-rankness are most prevalent. This prominent role can be explained with their competitive performance in imaging tasks and the rigorous mathematical analysis they allow . For instance, consider the recovery of an \(n_{1} n_{2}\)-dimensional image \(_{}^{n_{1} n_{2}}\) from linear observations

\[=(_{})+^{m},\] (1)

where \(^{n_{1} n_{2}}^{m}\) is a linear operator modeling the impulse response of the sensing device and \(^{m}\) models additive noise. Whereas this problem is ill-posed for \(m<n_{1}n_{2}\), it has been established  that it becomes well-posed if \(_{}\) is sparse or of low rank. The aforementioned works prove that \(m=(s_{1}s_{2})\) observations suffice for robust reconstruction if \(_{}\) is \(s_{1}\)-row-sparseand \(s_{2}\)-column-sparse, and that \(m=(r(n_{1}+n_{2}))\) observations suffice if \(_{}\) is a rank-\(r\) matrix. These bounds, which relax the general lower bound of \(m=(n_{1}n_{2})\), agree with the degrees of freedom of sparse and low-rank matrices, respectively.

A number of computationally challenging problems in signal processing and machine learning can be formulated as instances of (1) with \(_{}\) being _simultaneously structured_, i.e., \(_{}\) is both of rank \(r\) and \(s_{1}\)-row-sparse/\(s_{2}\)-column-sparse. Examples encompass sparse phase retrieval , sparse blind deconvolution , hyperspectral imaging , sparse reduced-rank regression , and graph denoising and refinement . In these settings, the hope is that due to leveraging the simultaneous structure, \((r(s_{1}+s_{2}))\) observations suffice to identify the data matrix. For \(r s_{1},s_{2} n_{1},n_{2}\), these bounds are significantly smaller than the bounds for single-structured data matrices.

From an algorithmic point of view, however, the simultaneously structured recovery problem poses obstacles that are _not_ present for problems where \(_{}\) is only of low-rank _or_ (group) sparse: In the latter case, variational methods  that formulate the reconstruction method in terms of optimizing a suitable objective with a structural regularization term involving \(_{p}\)/\(_{2,p}\)-(quasi-)norms and and \(S_{p}\)-Schatten (quasi-)norms have been well-understood, leading to tractable algorithms in the information theoretically optimal regime .

For simultaneously structured problems, on the other hand, Oymak et al. showed in  that a mere linear combination of _convex_ regularizers for different sparsity structures -- in our case, nuclear and \(_{2,1}\)-norms -- cannot outperform recovery guarantees of the "best" one of them alone. While this indicates that leveraging two priors at once is a way more intricate problem than leveraging a single prior, it was also shown in  that minimizing a linear combination of rank and row sparsity _can_ indeed lead to guaranteed recovery from \((r(s_{1}+s_{2}))\) measurements. The downside is that the combination of these _non-convex and discontinuous_ quantities does not lend itself directly to practical optimization algorithms, and to the best of our knowledge, so far, there have been no works directly tackling the optimization of a combination of _non-convex surrogates_ that come with any sort of convergence guarantees.

### Contribution

In this work, we approach the reconstruction of simultaneously sparse and low-rank matrices by leveraging the positive results of  for non-convex regularizers. To this end, we introduce a family of non-convex, but continuously differentiable regularizers that are tailored to the recovery problem for simultaneously structured data. The resulting objectives lend themselves to efficient optimization by a novel algorithm from the class of _iteratively reweighted least squares_ (_IRLS_) , the convergence of which we analyze in the information theoretically (near-)optimal regime. Specifically, our main contributions are threefold:

1. In Algorithm 1, we propose a novel IRLS method that is tailored to leveraging both structures of the latent solution, sparsity and low-rankness, at once. The core components of the algorithm are the weight operator defined in Definition 2.1 and the update of the smoothing parameters in (12). Notably, the algorithm automatically balances between its low-rank and its sparsity promoting terms, leading to a reliable identification of \(s_{1}\)-row-sparse and rank-\(r\) ground truths2. 2. Under the assumption that \(\) behaves almost isometrically on the set of row-sparse and low-rank matrices, we show in Theorem 2.5 that locally Algorithm 1 exhibits quadratic convergence towards \(_{}\). Note that if \(\) is, e.g., a Gaussian operator, the isometry assumption is (up to log factors) fulfilled in the information theoretic (near-)optimal regime \(m=(r(s_{1}+n_{2}))\).
3. Finally, in Section 2.3 we identify the underlying family of objectives that are minimized by Algorithm 1. To make this precise, we define for \(>0\) and \(e\) denoting Euler's number the real-valued function \(f_{}:\) such that \[f_{}(t)=^{2}(et^{2}/^{2}),&|t|>,\\ t^{2},&|t|,,\] (2)which is quadratic around the origin and otherwise logarithmic in its argument. Using this definition, we define for \(>0\) the \((-)\)_smoothed_\(\)_-determinant_ objective \(_{lr,}:^{n_{1} n_{2}}\) and for \(>0\) the \((-)\)_smoothed sum of logarithmic row-wise \(_{2}\)-norms_ objective \(_{sp,}:^{n_{1} n_{2}}\) such that

\[_{lr,}()=_{r=1}^{\{n_{1},n_{2}\}}f_{ }(_{r}()),_{sp,}( )=_{i=1}^{n_{1}}f_{}(\|_{i,:}\|_{2}).\] (3)

Combining the above, we further define the \((,-)\)_smoothed logarithmic surrogate_ objective \(_{,}:^{n_{1} n_{2}}\) as

\[_{,}():=_{lr,}( )+_{sp,}().\] (4)

In Theorem 2.6, we prove for _any_\(\) that the iterates of Algorithm 1 minimize quadratic majorizations of \(_{,}\) and form a non-increasing sequence on \(_{,}\). To the best of our knowledge, the proposed method is the so far only approach for recovering simultaneously sparse and low-rank matrices which combines local (quadratic) convergence with a rigorous variational interpretation.

The numerical simulations in Section 4 support our theoretical findings and provide empirical evidence for the efficacy of the proposed method.

### Related Work

Sparse and Low-Rank Recovery.Whereas leveraging a single matrix structure like sparsity _or_ low-rankness in the reconstruction process can easily be obtained by convex regularizers [78; 16], Oymak et al.  showed that, if one is interested in near-optimal sampling rates, one cannot expect comparably simple solutions for identifying simultaneously structured objects; a minimization of (4) with _convex_ terms \(_{lr,}()\) and \(_{sp,}()\) would be only as good as using the one structure that is information theoretically more favorable. A closely related problem3 that appears in statistical literature under the name _Sparse Principal Component Analysis_ (SPCA) [104; 25] is known to be NP-hard in general . Despite the the intrinsic hardness of simultaneously structured recovery problems, promising empirical results for hyperspectral image demixing were shown in , where minimization problems involving the sum of _reweighted_ convex surrogates are solved by a proximal scheme based on ADMM for the case of simultaneously sparse, low-rank and non-negative matrices. For the problem of simultaneously sparse and low-rank matrix recovery, there exist only a handful approaches that come with rigorous theoretical analysis. The first line of works [4; 37] aims to overcome the aforementioned limitations of purely convex methods in a neat way. They assume that the operator \(\) has a nested structure such that basic solvers for low-rank resp. row/column-sparse recovery can be applied in two consecutive steps. Despite being an elegant idea, this approach clearly restricts possible choices for \(\) and is of hardly any practical use.

In a second line of work, Lee et al.  consider general impulse response operators that satisfy a suitable restricted isometry property for \(s_{1}\)-row- and \(s_{2}\)-column-sparse rank-\(r\) matrices. They propose and analyze a highly efficient, greedy method, the so-called _Sparse Power Factorization_ (SPF) which is a modified version of power factorization  and uses hard thresholding pursuit  to enforce sparsity in addition. In particular, they show that if \(_{}\) is rank-\(R\), has \(s_{1}\)-sparse columns and \(s_{2}\)-sparse rows, then \(m R(s_{1}+s_{2})(\{en_{1}/s_{1},en_{2}/s_{2}\})\) Gaussian observations suffice for robust recovery, which is up to the log-factor at the information theoretical limit we discussed above. The result however assumes a low noise level and requires that SPF is initialized by a, in general, intractable method. Only in the special case that \(_{}\) is spiky, which means that the norms of non-zero rows/columns exhibit a fast decay, a tractable substitute for the initialization method works provably. The analysis of SPF has been extended to the blind deconvolution setup in . In , a related approach that combines gradient descent of a smooth objective with hard thresholding is considered, for which the authors show linear convergence from a suitable initialization if the measurement operator satisfies a restricted strong convexity and smoothness assumption.

A third line of work, approaches the problem from a variational point of view. In [33; 69] the authors aim at enhancing robustness of recovery by alternating minimization of an \(_{1}\)-norm based multi-penalty functional. In essence, the theoretical results bound the reconstruction error of global minimizers of the proposed functional depending on the number of observations. Although the authors only provide local convergence guarantees for the proposed alternating methods, the theoretical error bounds for global minimizers hold for arbitrarily large noise magnitudes and a wider class of ground-truth matrices than the one considered in .

The works [37; 29], which build upon generalized projection operators to modify iterative hard thresholding to the simultaneous setting, share the lack of global convergence guarantees.

In , the authors examine the use of atomic norms to perform recovery of simultaneously sparse and low-rank matrices, which uses a related, but different sparsity assumption compared to the row or column sparsity studied here. From a practical point of view, the such norms are hard to compute and the paper only proposes a heuristic polynomial time algorithm for the problem.

Finally, the alternative approach of using optimally weighted sums or maxima of convex regularizers  requires optimal tuning of the parameters under knowledge of the ground-truth.

Iteratively Reweighted Least Squares.The herein proposed iteratively reweighted least squares algorithm builds on a long line of research on IRLS going back to Weiszfeld's algorithm proposed in the 1930s for a facility location problem [95; 5]. IRLS is a practical framework for the optimization of non-smooth, possibly non-convex, high-dimensional objectives that minimizes quadratic models which majorize these objectives. Due to its ease of implementation and favorable data-efficiency, it has been widely used in compressed sensing [42; 18; 26; 57; 34; 55], robust statistics [45; 2; 72], computer vision [19; 61; 84], low-rank matrix recovery and completion [35; 70; 56; 54], and in inverse problems involving group sparsity [21; 101; 20]. Recently, it has been shown  that dictionary learning techniques can be incorporated into IRLS schemes for sparse and low-rank recovery to allow the learning of a sparsifying dictionary while recovering the solution. Whereas IRLS can be considered as a type of majorize-minimize algorithm , optimal performance is achieved if intertwined with a smoothing strategy for the original objective, in which case globally linear (for convex objectives) [26; 1; 72; 55; 75] and locally superlinear (for non-convex objectives) [26; 56; 54; 75] convergence rates have been shown under suitable conditions on the linear operator \(\).

However, there has only been little work on IRLS optimizing a sum of heterogenous objectives  -- including the combination of low-rank promoting and sparsity-promoting objectives -- nor on the convergence analysis of any such methods. The sole algorithmically related approach for our setting has been studied in , where a method has been derived in a sparse Bayesian learning framework, the main step of which amounts to the minimization of weighted least squares problems. Whereas the algorithm of  showcases that such a method can empirically identify simultaneously structured matrices from a small number of measurements, no convergence guarantees or rates have been provided in the information-theoretically optimal regime. Furthermore,  only focuses on general sparsity rather than row or column sparsity.

### Notation

We denote matrices and vectors by bold upper- and lower-case letters to distinguish them from scalars and functions. We furthermore denote the \(i\)-th row of a matrix \(^{n_{1} n_{2}}\) by \(_{i,:}\) and the \(j\)-th column of \(\) by \(_{:,j}\). We abbreviate \(n=\{n_{1},n_{2}\}\). We denote the \(r\)-th singular value of a matrix \(^{n_{1} n_{2}}\) by \(_{r}()\). Likewise, we denote the in \(_{2}\)-norm \(s\)-largest row of \(\) by \(_{s}()\). (To determine \(_{s}()\), we form the in \(_{2}\)-norm non-increasing rearrangement of the rows of \(\) and, by convention, sort rows with equal norm according to the row-index.) We use \(\) to denote the Hadamard (or Schur) product, i.e., the entry-wise product of two vectors/matrices.

We denote the Euclidean \(_{2}\)-norm of a vector \(^{n}\) by \(\|\|_{2}\). For \(^{n_{1} n_{2}}\), the matrix norms we use encompass the operator norm \(\|\|\ :=\ _{\|\|_{2}=1}\|\|_{2}\), the row-sum \(p\)-quasinorm \(\|\|_{p,2}\ :=\ _{i=1}^{n_{1}}\|_{i,:}\|_{2}^{p}^{1/p}\), the row-max norm \(\|\|_{,2}\ :=\ _{i[n_{1}]}\|_{i,:}\|_{2}\), and the Schatten-p quasinorm \(\|\|_{S_{p}}\ :=\ (_{r=1}^{n}_{r}()^{p})^{1/p}\). Note that two special cases of Schatten quasinorms are the nuclear norm \(\|\|_{}\ :=\ \|\|_{S_{1}}\) and the Frobenius norm \(\|\|_{F}\ :=\ \|\|_{S_{2}}\).

## 2 Irls for Sparse and Low-Rank Reconstruction

Recall that we are interested in recovering a rank-\(r\) and \(s\)-row-sparse matrix \(_{}^{n_{1} n_{2}}\) from \(m\) linear observations

\[=(_{})^{m},\] (5)

i.e., \(\): \(^{n_{1} n_{2}}^{m}\) is linear. We write \(_{}_{r,s}^{n_{1},n_{2}}:=_{r}^{n_{1}, n_{2}}_{s}^{n_{1},n_{2}}\), where \(_{r}^{n_{1},n_{2}}^{n_{1} n_{2}}\) denotes the set of matrices with rank at most \(r\) and \(_{s}^{n_{1},n_{2}}^{n_{1} n_{2}}\) denotes the set of matrices with at most \(s\) non-zero rows. For convenience, we suppress the indices \(n_{1}\) and \(n_{2}\) whenever the ambient dimension is clear from the context. In particular, we know that \(_{}=_{}_{}_{} ^{*}\), where \(_{}_{s}^{n_{1},r}\), \(_{}^{r r}\), and \(_{}^{n_{2} r}\) denote the reduced SVD of \(_{}\). Furthermore, the row supports of \(_{}\) and \(_{}\) (the index sets of non-zero rows of \(_{}\) resp. \(_{}\)) are identical, i.e., \((_{})=(_{})=S_{} [n_{1}]:=\{1,,n_{1}\}\).

### How to Combine Sparse and Low-Rank Weighting

As discussed in Section 1, the challenge in designing a reconstruction method for (5) lies in simultaneously leveraging both structures of \(_{}\) in order to achieve the optimal sample complexity of \(m r(s+n_{2})\). To this end, we propose a novel IRLS-based approach in Algorithm 1. The key ingredient for computing the \((k+1)\)-st iterate \(^{(k+1)}^{n_{1} n_{2}}\) in Algorithm 1 is the multi-structural weight operator \(W_{^{(k)},_{k},_{k}}:^{n_{1} n_{2}} ^{n_{1} n_{2}}\) of Definition 2.1, which depends on the current iterate \(^{(k)}\).

**Definition 2.1**.: _For \(_{k},_{k}>0\), \(_{i}^{k}:=_{i}(^{(k)})\), and \(^{(k)}^{n_{1} n_{2}}\), let_

\[r_{k}:=|\{i[n]:_{i}^{(k)}>_{k}\}|\] (6)

_denote the number of singular values \(^{(k)}=(_{i}^{(k)})_{i=1}^{r_{k}}\) of \(^{(k)}\) larger than \(_{k}\) and furthermore be_

\[s_{k}:=|\{i[n_{1}]:\|_{i}^{(k)},\|_{2}> _{k}\}|.\] (7)

_the number of rows of \(^{(k)}\) with \(_{2}\)-norm larger than \(_{k}\). Define the \(r_{k}\) left and right singular vectors of \(^{(k)}\) as columns of \( R^{n_{1} r_{k}}\) and \(^{n_{2} r_{k}}\), respectively, corresponding to the leading singular values \(^{(k)}=(_{i}^{(k)})_{i=1}^{r_{k}}:=( _{i}(^{(k)}))_{i=1}^{r_{k}}\). We define the weight operator \(W_{^{(k)},_{k},_{k}}:^{n_{1} n_{2}} ^{n_{1} n_{2}}\) at iteration \(k\) of Algorithm 1 as_

\[W_{^{(k)},_{k},_{k}}()=W_{^{(k) },_{k}}^{lr}()+_{^{(k)},_{k}}^ {sp}\] (8)

_where \(W_{^{(k)},_{k}}^{lr}:^{n_{1} n_{2}} ^{n_{1} n_{2}}\) is its low-rank promoting part_

\[W_{^{(k)},_{k}}^{lr}()=[ _{}]_{_{k}}^{-1}[ _{}^{*}][_{}] _{_{k}}^{-1}[^{*}_{}]\] (9)

_with \(_{_{k}}=(_{i}^{(k)}/_{k},1)\) and \(_{^{(k)},_{k}}^{sp}^{n_{1} n_{1}}\) is its sparsity-promoting part, which is diagonal with_

\[(_{^{(k)},_{k}}^{sp})_{ii}=( \|(^{(k)})_{i,}\|_{2}^{2}/_{k}^{2},1)^{-1},i[n_{1}].\] (10)

_The matrices \(_{}\) and \(_{}\) are arbitrary complementary orthogonal bases for \(\) and \(\) that do do not need to be computed in Algorithm 1._

**Remark 2.2**.: _For the sake of conciseness, we only consider row-sparsity here. Algorithm 1 and its analysis can however be modified to cover row- and column-sparse matrices as well. For instance, in the symmetric setting \(_{}=_{}^{T}\) (naturally occurring in applications like sparse phase retrieval) one would define the weight operator \(W_{^{(k)},_{k},_{k}}\) as in (8), but with an additional term that multiplies \(_{^{(k)},_{k}}^{sp}\) from the right to \(\), which corresponds to minimizing the sum of three smoothed logarithmic surrogates. In this case, the solving modified weighted least squares problem (11) will have similar complexity (potentially smaller complexity, as additional symmetries can be exploited)._Recall \(_{_{k},_{k}},_{l_{r},_{k}}\), and \(_{,_{k}}\) from (3)-(4). The high-level idea of Algorithm 1, as for other IRLS methods, is to minimize quadratic functionals, which we call \(_{l_{r},_{k}}(\,\,|^{(k)}):\,^{n _{1} n_{2}}\) and \(_{,_{k}}(\,\,|^{(k)}):^{n _{1} n_{2}}\) and define them by

\[_{l_{r},_{k}}(|^{(k)}) \!:=\!_{l_{r},_{k}}(^{(k)})+ _{l_{r},_{k}}(^{(k)}),- ^{(k)}+-^{(k)},W^{lr}_{ ^{(k)},_{k}}(-^{(k)}),\] (13) \[_{,_{k}}(|^{(k)}) \!:=_{,_{k}}(^{(k)})+ _{,_{k}}(^{(k)}),- ^{(k)}+-^{(k)},^{}_{^{(k)},_{k}}(-^{(k)}),\]

that majorize \(_{_{k},_{k}}()\) (see Theorem 2.6 below) for any iteration \(k\). This minimization leads to the weighted least squares problem (11) in Algorithm 1. This step can be implemented by standard numerical linear algebra (see the supplementary material for a discussion of its computational complexity). As a second ingredient of the method, the smoothing parameters \(_{k}\) and \(_{k}\) of \(_{_{k},_{k}}\) are updated (i.e., decreased) in step (12) before the weight operator is updated according to the current iterate information. For the weight operator update, it is only necessary to compute row norms and leading singular triplets of \(^{(k)}\).

**Remark 2.3**.: _The particular form of the low-rank promoting part of the weight operator \(W^{lr}_{^{(k)},_{k}}\) in (9) is due to [53; 54] and captures optimally spectral information both in the column and row space, unlike prior work on low-rank IRLS [35; 70], while retaining the property that the induced quadratic model \(_{l_{r},_{k}}(\,\,|^{(k)})\) majorizes \(_{l_{r},_{k_{r}}}()\) (see proof of Theorem 2.6). This choice is critical to enable a fast local rate as established in Theorem 2.5._

### Local Quadratic Convergence of IRLS

Our first main result states that Algorithm 1 exhibits quadratic convergence in a local neighborhood of \(_{}\), a property Algorithm 1 shares with several methods from the IRLS family. We only need to assume that \(\) acts almost isometrically on the set \(_{r,s}\).

**Definition 2.4**.: _We say that a linear operator \(^{n_{1} n_{2}}^{m}\) satisfies the rank-\(r\) and row-\(s\)-sparse restricted isometry property (or \((r,s)\)-RIP) with RIP-constant \((0,1)\) if_

\[(1-)\|\|_{F}^{2}\|() \|_{2}^{2}(1+)\|\|_{F}^{2},\]

_for all \(_{r,s}\)._

It is worth highlighting that Gaussian operators satisfy the above RIP with high-probability if \(m cr(s+n_{2})(en_{1}/s)\), for some absolute constant \(c>0\), see for instance . Up to log-factors, this is at the information theoretic limit which we discussed in the beginning. The convergence result for Algorithm 1 now reads as follows.

**Theorem 2.5** (Local Quadratic Convergence).: _Let \(_{}_{r,s}\) be a fixed ground-truth matrix that is \(s\)-row-sparse and of rank \(r\). Let linear observations \(=(_{})\) be given and assume that \(\) has the \((r,s)\)-RIP with \((0,1)\). Assume that the \(k\)-th iterate \(^{(k)}\) of Algorithm 1 with \(=r\) and \(=s\) updates the smoothing parameters in (12) such that one of the statements \(_{k}=_{r+1}(^{(k)})\) or \(_{k}=_{s+1}(^{(k)})\) is true, and that \(r_{k} r\) and \(s_{k} s\)._

_If \(^{(k)}\) satisfies_

\[\|^{(k)}-_{}\|_{\|\|_{2 2}}^{3}}\{(_{})}{r},( _{})}{s}\}\] (14)

_where \(c_{\|\|_{2 2}}=\|_{2 2}^{2}}{(1- )}}\) and \(n=\{n_{1},n_{2}\}\), then the local convergence rate is quadratic in the sense that_

\[\|^{(k+1)}-_{}\|\{\|^{(k)}- _{}\|^{2},0.9\|^{(k)}-_{}\|\},\]

_for_

\[=4.179c_{\|\|_{2 2}}^{2}( _{})}+(_{})},\] (15)

_and \(^{(k+)}_{}\)._

The proof of Theorem 2.5 is presented in the supplementary material. To the best of our knowledge, so far no other method exists for recovering simultaneously sparse and low-rank matrices that exhibits local quadratic convergence. In particular, the state-of-the-art competitor methods  reach a local linear error decay at best.

On the other hand, (14) is rather pessimistic since for Gaussian \(\) the constant \(c_{\|\|_{2 2}}\) scales like \(n_{2})/m}\), which means that the right-hand side of (14) behaves like \(m^{3/2}/(n(n_{1}n_{2})^{3/2})\), whereas we observe quadratic convergence in experiments within an empirically much larger convergence radius. Closing this gap between theory and practical performance is future work.

It is noteworthy that the theory in  -- to our knowledge the only other related work explicitly characterizing the convergence radius -- holds on a neighborhood of \(_{}\) that is independent of the ambient dimension. The authors of  however assume that the RIP-constant decays with the conditioning number \(\) of \(\), a quantity that might be large in applications. Hence, ignoring log-factors the sufficient number of measurements in  scales like \(m=(^{2}r(s_{1}+n_{2}))\). In contrast, Theorem 2.5 works for any RIP-constant less than one which means for \(m=(r(s_{1}+n_{2}))\).

### Irls as Quadratic Majorize-Minimize Algorithm

With Theorem 2.5, we have provided a local convergence theorem that quantifies the behavior of Algorithm 1 in a small neighbourhood of the simultaneously row-sparse and low-rank ground-truth \(_{}^{n_{1} n_{2}}\). The result is based on sufficient regularity of the measurement operator \(\), which in turn is satisfied with high probability if \(\) consists of sufficiently generic random linear observations that concentrate around their mean.

In this section we establish that, for _any_ measurement operator \(\), Algorithm 1 can be interpreted within the framework of iteratively reweighted least squares (IRLS) algorithms , which implies a strong connection to the minimization of a suitable smoothened objective function. In our case, the objective \(_{,}\) in (4) is a linear combination of sum-of-logarithms terms penalizing both non-zero singular values  as well as non-zero rows of a matrix \(\).

We show in Theorem 2.6 below that the IRLS algorithm Algorithm 1 studied in this paper is based on minimizing at each iteration quadratic models that majorize \(_{,}\), and furthermore, that the iterates \((^{(k)})_{k 1}\) of Algorithm 1 define a non-increasing sequence \(_{_{k},_{k}}(^{(k)})_{k  1}\) with respect to the objective \(_{,}\) of (4). The proof combines the fact that for fixed smoothing parameters \(_{k}\) and \(_{k}\), the weighted least squares and weight update steps Algorithm 1 can be interpreted as a step of a Majorize-Minimize algorithm , with a decrease in the underlying objective (4) for updated smoothing parameters.

**Theorem 2.6**.: _Let \(^{m}\), let the linear operator \(^{n_{1} n_{2}}^{m}\) be arbitrary. If \((^{(k)})_{k 1}\) is a sequence of iterates of Algorithm 1 and \((_{k})_{k 1}\) and \((_{k})_{k 1}\) are the sequences of smoothing parameters as defined therein, then the following statements hold._1. _The quadratic model functions_ \(_{l,_{k}}(|^{(k)}.)\) _and_ \(_{,_{k}}(|^{(k)}.)\) _defined in (_13_) globally majorize the_ \((_{k},_{k})-\)_smoothed logarithmic surrogate objective_ \(_{_{k},_{k}}\)_, i.e., for_ any__\(^{n_{1} n_{2}}\)_, it holds that_ \[_{_{k},_{k}}()_{l,_{k}}(|^{(k)})+_{,_ {k}}(|^{(k)}).\] (16)
2. _The sequence_ \((_{_{k},_{k}}(^{(k)}))_{k  1}\) _is non-increasing._
3. _If_ \(:=_{k}_{k}>0\) _and_ \(:=_{k}_{k}>0\)_, then_ \(_{k}\|^{(k)}-^{(k+1)}\|_{F}=0\)_. Furthermore, in this case, every accumulation point of_ \((^{(k)})_{k 1}\) _is a stationary point of_ \[_{:()=}_{,}().\]

The proof of Theorem 2.6 is presented in the supplementary material.

## 3 Discussion of Computational Complexity

It is well-known that the solution of the linearly constrained weighted least squares problem (11) can be written as

\[^{(k)}=W_{k-1}^{-1}^{*}(W_{k-1}^{-1} ^{*})^{-1}\] (17)

where \(W_{k-1}:=W_{^{(k-1)},_{k-1},_{k-1}}\) is the weight operator (8) of iteration \(k-1\)[26; 54]. In [54; Theorem 3.1 and Supplementary Material], it was shown that in the case of low-rank matrix completion without the presence of a row-sparsity inducing term, this weighted least squares problem can be solved by solving an equivalent, well-conditioned linear system via an iterative solver that uses the application of a system matrix whose matrix-vector products have time complexity of \(O(mr+r^{2}(n_{1},n_{2}))\).

In the case of Algorithm 1, the situations is slightly more involved as we cannot provide an explicit formula for the inverse of the weight operator \(W_{k-1}\) as it amounts to the sum of the weight operators \(W_{^{(k-1)},_{k-1}}^{}\) and \(_{^{(k-1)},_{k-1}}^{}\) that are diagonalized by different, mutually incompatible bases. However, computing this inverse is facilitated by the _Sherman-Morrison-Woodbury_ formula 

\[(^{*}+)^{-1}=^{-1}-^{-1}( ^{-1}+^{*}^{-1})^{-1}^{*} ^{-1}\]

for suitable matrices of compatible size \(,\) and invertible \(,\) and the fact that both \(W_{^{(k-1)},_{k-1}}^{}\) and \(_{^{(k-1)},_{k-1}}^{}\) exhibit a "low-rank plus (scaled) identity" or a "sparse diagonal plus (scaled) identity" structure. After a simple application of the SMW formula, (17) can be rewritten such that the computational bottleneck becomes the assembly and inversion of a \(O(r_{k}(n_{1},n_{2}))\) linear system. We note that in general, this can be done exactly in a time complexity of \(O(r_{k}^{3}max(n_{1},n_{2})^{3})\) using standard linear algebra. A crucial factor in the computational cost of the method is also the structure of the measurement operator \(\) defining the problem, as the application of itself and its adjoint can significantly influence the per-iteration cost of IRLS; for dense Gaussian measurements, just processing the information of \(\) amounts to \(mn_{1}n_{2}\) flops. If rank-one or Fourier-type measurements are taken, this cost can significantly be reduced, see [29; Table 1 and Section 3] for an analogous discussion.

We refer to the MATLAB implementation available in the repository https://github.com/ckuemmerle/simirls for further details. While our implementation is not optimized for large-scale problems, the computational cost of Algorithm 1 was observed to be comparable to the implementations of SPF or RiemAdaIHT provided by the authors [69; 29]. We leave further improvements and adaptations to large-scale settings to future work.

## 4 Numerical Evaluation

In this section, we explore the empirical performance of IRLS in view of the theoretical results of Theorems 2.5 and 2.6, and compare its ability to recover simultaneous low-rank and row-sparse data matrices with the state-of-the-art methods Sparse Power Factorization (SPF)  and Riemannian adaptive iterative hard thresholding (RiemAdaIHT) , which are among the methods with the best empirical performance reported in the literature. The method ATLAS and its successor  are not used in our empirical studies since they are tailored to robust recovery and yield suboptimal performance when seeking high-precision reconstruction in low noise scenarios. We use spectral initialization for SPF and RiemdaIHT. The weight operator of IRLS is initialized by the identity as described in Algorithm 1, solving an unweighted least squares problem in the first iteration. A detailed description of the experimental setup can be found in Appendix A.1.

Performance in Low-Measurement Regime.Figures 1 and 2 show the empirical probability of successful recovery when recovering \(s\)-row sparse ground-truths \(_{}^{256 40}\) of rank \(r=1\) (resp. \(r=5\)) from Gaussian measurements under oracle knowledge on \(r\) and \(s\). The results are averaged over 64 random trials. As both figures illustrate, the region of success of IRLS comes closest to the information theoretic limit of \(r(s+n_{2}-r)\) which is highlighted by a red line, requiring a significantly lower oversampling factor than the baseline methods.

In Appendix A.2 and Appendix A.3 in the supplementary material, we report on similar experiments conducted for other measurement operators than dense Gaussians, in which cases the empirical relative behavior of the methods is comparable.

Sensitivity to Parameter Choice.In applications of our setting, the quantities \(r\) and \(s\) might be unknown or difficult to estimate. In the second row of Figure 2, we repeat the experiment of the first row (rank-\(5\) ground truth), but run the algorithms with rank and sparsity estimates of \(=2r\) and \(= 1.5s\). Whereas all considered methods suffer a deterioration of performance, we observe that IRLS deteriorates relatively the least by a large margin. Furthermore, we observe that even if IRLS does not recovery \(_{}\), it converges typically to a matrix that is still low-rank and row-sparse (with

Figure 1: Left column: RiemdaIHT, center: SPF, right: IRLS. Phase transition experiments with \(n_{1}=256\), \(n_{2}=40\), \(r=1\), Gaussian measurements. Algorithmic hyperparameters informed by model order knowledge (i.e., \(=r\) and \(=s\) for IRLS). White corresponds to empirical success rate of \(1\), black to \(0\).

Figure 2: Left column: RiemdaIHT, center: SPF, right: IRLS. First row: As in Figure 1, but for data matrix \(_{}\) of rank \(r=5\). Second row: As first row, but hyper-parameters \(r\) and \(s\) are overestimated as \(=2r=10\), \(= 1.5s\)

larger \(r\) and \(s\)) satisfying the data constraint, while the other methods fail to convergence to such a matrix.

Convergence Behavior.Finally, we examine the convergence rate of the iterates to validate the theoretical prediction of Theorem 2.5 in the setting of Figure 2. Figure 3 depicts in log-scale the approximation error over the iterates of SPF, RiemAdaIHT, and IRLS. We observe that the IRLS indeed exhibits empirical quadratic convergence within a few iterations (around \(10\)), whereas the other methods clearly only exhibit linear convergence. The experiment further suggests that the rather pessimistic size of the convergence radius established by Theorem 2.5 could possibly be improved by future investigations.

Further experiments.In Appendix A.4, we provide additional experiments investigating the self-balancing property of the objective (4), as well as the experiments on the noise robustness of the method in Appendix A.5.

## 5 Conclusion, Limitations and Future Work

Conclusion.In this paper, we adapted the IRLS framework to the problem of recovering simultaneously structured matrices from linear observations focusing on the special case of row sparsity and low-rankness. Our convergence guarantee Theorem 2.5 is hereby the first one for any method minimizing combinations of structural surrogate objectives that holds in the information-theoretic near-optimal regime and exhibits local quadratic convergence. The numerical experiments we conducted for synthetic data suggest that, due to its weak dependence on the choice of hyperparameters, IRLS in the form of Algorithm 1 can be a practical method for identifying simultaneously structured data even in difficult problem instances.

Limitations and Future Work.As in the case of established IRLS methods that optimize non-convex surrogate objectives representing a single structure [26; 56; 54; 75], the radius of guaranteed quadratic convergence in Theorem 2.5 is the most restrictive assumption. Beyond the interpretation in terms of surrogate minimization as presented in Theorem 2.6, which holds without any assumptions on the initialization, our method shares a lack of a global convergence guarantees with other non-convex IRLS algorithms [70; 54; 75].

The generalization and application of our framework to combinations of structures beyond rank and row- (or column-)sparsity lies outside the scope of the present paper, but could involve subspace-structured low-rankness [31; 24; 99; 87] or analysis sparsity [30; 77]. A generalization of the presented IRLS framework to higher-order objects such as low-rank tensors is of future interest as convexifications of structure-promoting objectives face similar challenges [71; 74; 98] in this case.

In parameter-efficient deep learning, both sparse [39; 44; 32; 91] and low-rank [96; 94; 82] weight parameter models have gained considerable attention due to the challenges of training and storing, e.g., in large transformer-based models [92; 9]. It will be of interest to study whether in this non-linear case IRLS-like preconditioning of the parameter space can find network weights that are simultaneously sparse and low-rank, and could potentially lead to further increases in efficiency.

Figure 3: Comparison of convergence rate. Setting as in Figure 2 with \(s=40\) and \(m=1125\).