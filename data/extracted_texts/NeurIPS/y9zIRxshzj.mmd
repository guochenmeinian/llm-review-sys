# Causal Discovery from Event Sequences

by Local Cause-Effect Attribution

Joscha Cuppers

CISPA Helmholtz Center

for Information Security

joscha.cueppers@cispa.de &Sascha Xu

CISPA Helmholtz Center

for Information Security

sascha.xu@cispa.de &Ahmed Musa

Institute for Structural Analysis

TU Dresden

ahmed.musa@tu-dresden.de &Jilles Vreeken

CISPA Helmholtz Center

for Information Security

jv@cispa.de

###### Abstract

Sequences of events, such as crashes in the stock market or outages in a network, contain strong temporal dependencies, whose understanding is crucial to react to and influence future events. In this paper, we study the problem of discovering the underlying causal structure from event sequences. To this end, we introduce a new causal model, where individual events of the cause trigger events of the effect with dynamic delays. We show that in contrast to existing methods based on Granger causality, our model is identifiable for both instant and delayed effects.

We base our approach on the Algorithmic Markov Condition, by which we identify the true causal network as the one that minimizes the Kolmogorov complexity. As the Kolmogorov complexity is not computable, we instantiate our model using Minimum Description Length and show that the resulting score identifies the causal direction. To discover causal graphs, we introduce the Cascade algorithm, which adds edges in topological order. Extensive evaluation shows that Cascade outperforms existing methods in settings with instantaneous effects, noise, and multiple colliders, and discovers insightful causal graphs on real-world data.

## 1 Introduction

Suppose we are considering a multivariate event sequence. What caused a specific event to happen? Which variables are causes of each other? Data-driven methods can infer causal relationships from observed data. Existing methods for discovering causal networks from event sequence data  are based on _Granger causality_. This purely predictive notion defines a variable \(X\) to be a cause of another variable \(Y\) if the past of \(X\) helps to predict the future \(Y\). It is a relatively weak notion of causality that excludes instantaneous effects and is often unable to discover true causal dependencies; in Granger causality, baking a cake is causal to a birthday.

In this paper, we instead build upon Pearl's model of causality, which assumes the existence of an an underlying causal structure in the form of a directed acyclic graph (DAG) . In our context, such a graph describes the causal relationships between types of events, such as alarms in a network. We propose a new causal model for event sequences based on a one-to-one _matching_ of individual events, where we model the process of one individual event of a certain type possibly causing anindividual event of another type. In our model, we take into account the uncertainty of whether an event is actually caused or independently generated, the uncertainty of an event actually causing an effect or failing to do so, and the uncertainty of the delay between cause and effect. As we will show, our model has several advantages, such as a clear notion of what event caused another and the identifiability for both instant and non-instant effects.

We base our theory on the Algorithmic Markov Condition (AMC) , which postulates that the true causal model achieves the lowest Kolmogorov complexity. As Kolmogorov complexity is not computable, we instantiate it via the Minimum Description Length (MDL) principle . We show that our score is consistent, identifies the true causal direction for both instantaneous and delayed effects, and formally connect it to Hawkes processes. To discover causal networks in practice, we introduce the Cascade algorithm, which adds edges in topological order. Through extensive empirical evaluation, we show that Cascade performs well in practice and outperforms the state of the art by a wide margin. On synthetic data, Cascade recovers the ground truth without reporting spurious edges, and on real-world data, it returns graphs that correspond to existing knowledge.

## 2 Preliminaries

We write \(i j\) when \(S_{i}\) is a cause of \(S_{j}\) and \((j)\) for the set of parents of node \(j\). We assume faithfulness, sufficiency, and the causal Markov condition .

Information-Theoretic Causal DiscoveryThe Algorithmic Markov Condition (AMC) postulates that the factorization of the joint distribution according to the true causal network achieves the lowest Kolmogorov complexity . The Kolmogorov complexity \(K(x)\) of a binary string \(x\) is the length of the shortest program \(p\) for a universal Turing machine \(\) that computes \(x\) and halts . For a distribution \(P\), it is the length of the shortest program that uniformly approximates \(P\) arbitrarily well,

\[K(P)=_{p 0,1^{*}}\{|p|:_{y}|(p,y,q)-P(y)|\}\.\]

The AMC states that the Kolmogorov complexity of the joint distribution \(P(X)\) is the sum of the complexities of the conditional distributions \(P(X_{i}|(i))\) of the true DAG \(G^{*}\), i.e.

\[K(P(X))=_{i=1}^{p}K(P(X_{i}|(i)))\,\]

up to a constant independent of the input. Due to, among others, the halting problem, Kolmogorov complexity is not computable, but we can approximate it from above. A statistically well-founded way to do so is by Minimum Description Length (MDL) . For a fixed class of models \(\), MDL identifies a description length \(L\) of encoding data \(X\) together with its optimal model,

\[L(X)=_{h}(L(X h)+L(h))\.\]

Next, we introduce the assumed data generating process, its corresponding model class \(\) and encoding length function \(L\), and show under which conditions it can be identified.

## 3 Theory

To be able to infer causal relationships from observational data, we need to make assumptions about the underlying data-generating process . The key assumption we make here is that an individual event of type \(i\) at time \(t\) with probability \(_{i,j}\) causes an individual event of type \(j\) at time \(t^{} t\). To illustrate, we give a toy example in Fig. 1 in which event sequence \(S_{i}\) causes event sequence \(S_{j}\). The individual events in \(S_{i}\) occur uniformly at random. The first and third events in \(S_{i}\) cause events in \(S_{j}\), resp. with a delay of 0.2 and 0.3. The other two events in \(S_{i}\) do not cause events in \(S_{j}\), denoted by a delay of \(\). The final event in \(S_{j}\) is due to noise, marked by \(N_{j}\).

Figure 1: Cause-effect matching, where \(S_{i}\) causes \(S_{j}\).

Next, we formally describe the causal mechanism. We differentiate between source and effect nodes.

_Source_ nodes are nodes \(i\) in \(G^{*}\) with an empty parent set \(pa(i)\). For source nodes \(i\) we assume that the events in \(S_{i}\) occur uniformly at random with a rate of \(_{i}\) events per time unit. This mechanism, commonly known as a homogeneous _Poisson process_, is used, for example, as a model for accident rates requiring hospital admission . In this work, we focus on the delay times between individual events, denoted as \(d_{k}\) for the delay \(t_{k}-t_{k-1}\) and as \(_{i i}=\{d_{k}\}_{k=1}^{n_{i}}\) for the sequence. For a Poisson process, the delay times are independently and exponentially distributed. Thus, we model a source event sequence \(S_{i}\) as

\[S_{i}=\{t_{k}\}_{k=1}^{n_{i}}\,,t_{k}=_{l=1}^{k}d_{l}, _{i i}=\{d_{k}(_{i})\,iid\}_{k=1}^{n_{i}}.\] (1)

_Effect_ nodes are nodes \(j\) in \(G^{*}\) with at least one parent \(pa(j)\). For each effect node \(j\), the individual events in \(S_{j}\) are either caused by an individual event in an \(S_{i}\) with \(i(j)\) or due to noise. That is, reasoning from the causing node \(i\), every event \(t_{k} S_{i}\) may trigger an event of the effect \(S_{j}\) with a probability of \(_{i,j}\). If triggered, an individual event in \(S_{j}\) will occur after a random delay \(d_{k}\), drawn from a cause-effect specific delay distribution \(_{i,j}\) parameterized by \(_{i,j}\), e.g. the rate \(\) of an exponential distribution, and \(_{i,j}\). If no event is triggered, then we model the delay as infinite, i.e. \(d_{k}=\). The sequence of delays \(_{i j}\) from \(S_{i} S_{j}\) is modeled as

\[_{i j}=\{d_{k}\}_{k=1}^{n_{i}}\,, d_{k}_{i,j}( _{i,j},_{i,j})\,iid\,_{i,j}(d)=1-_{i,j}&d= \\ p(d;_{i,j})_{i,j}&\] (2)

where \(_{i,j}(d)\) denotes the density of the delay distribution. Thus, given the event sequence \(S_{i}\) of the cause and delays \(_{i j}\), the individual events in \(S_{j}\) caused by \(S_{i}\) are obtained by adding the delays \(d_{k}\) to the time stamps \(t_{k}\) of the individual cause events, with the reconstruction function \(f\) as

\[f_{i,j}(S_{i},_{i j})=\{t_{k}+d_{k} d_{k}\},k=1,,n_{i}\.\]

In addition, individual events in \(S_{j}\) can also be due to noise. Like for source nodes, we assume these a Poisson process as per Eq. (1) with rate \(_{j}\), i.e. \(N_{j}(_{j})\). Putting this together, given causal structure \(G^{*}\), an effect event sequence \(S_{j}\) is generated by taking the union of the individual delays from the causal parents \(pa(j)\) and the time stamps due to noise \(N_{j}\),

\[S_{j}=(_{i pa(j)}f_{i,j}(S_{i},_{i j} )) N_{j}\.\] (3)

Next, we instantiate an MDL score for this causal model and consider its identifiability.

### Minimum Description Length Instantiation

We now develop a score for our causal model using MDL . It consists of the cost of the data given the model, \(L(S)\), i.e. the negative log-likelihood of the data, and the cost of the model, i.e. that of the parameters, \(L()\), and that of the graph, \(L(G)\), all measured in bits.

Data CostThe cost of data in bits directly corresponds to its negative log-likelihood, i.e. the likelihood of each delay as per Eq. (2) over all the event sequences corresponding to the parents of node \(j\) and that of the noise events. Formally, we have

\[L(S_{j} S_{pa(j)},\,)=_{i pa(j)}_{d_{k} _{i j}}-(_{i,j}(d_{k}))+_{d_{l}_{j j}}- (_{j,j}(d_{l}))\.\]

The first term encodes those events that were caused by the parent \(S_{i}\) through the delays \(_{i j}\). Here, we use a Shannon-optimal coding that requires \(-(_{i,j}(d_{k}))\) bits per sample . In the second term, we encode all remaining events as noise using the delay distribution of a Poisson process. For source events, i.e. variables without any parents, only the noise term is present.

The cost of all sequences is then simply \(L(S G,)=_{j[p]}L(S_{j} S_{pa(j)},)\).

Parameter CostNext, we define the costs of the DAG, \(L(G)\), and that of the parameters, \(L()\). We encode the DAG in topological order. Per node we encode its number of parents \(|pa(i)|\) and identify which those are, i.e. \(L(G)=_{k=0}^{d-1}((k)+)\). Depending on their type, we encode the parameters \(\). For parameters \(\) we use \(L_{}\), the MDL-optimal encoding for integers . It is defined as \(L_{}(z)=^{*}z+ c_{0}\,\) where \(^{*}z\) is the expansion, \( z+ z+\) in which we only include positive terms. To ensure this is a valid encoding, i.e. one that satisfies the Kraft inequality, we set \(c_{0}=2.865064\). For parameters \(\) we use \(L_{}()=L_{}(d)+L_{}( 10^{d} )+1\) as the number of bits needed to encode a real number up to a user-specified precision . For an edge \(i j\), the parameters are the trigger probability \(_{i,j}\) and those of the delay distribution \(\). For the cost of an edge we hence have \(L(i j)=L_{}(_{i,j})+_{_{i,j}}L()\). For \(\) as a whole, we have \(L()=_{i j G}L(i j)\).

The overall MDL score is then

\[L(S G,)+L(G)+L()\.\]

### Identifiability

We now study the identifiability guarantees of our model and score, i.e. under what conditions we can identify from a given pair which is the cause and which the effect. Consider a pair of event sequences \(S_{i}\) and \(S_{j}\), where \(S_{i} S_{j}\) and the cause \(S_{i}\) is a source event while \(S_{j}\) is an effect event.

Instant EffectsWe begin with the case of instant effects only. Instant effects are observed when the sampling frequency of the data, e.g. a daily time scale, is insufficient to pick up a difference in time, such as a financial crash that can spread across the globe within hours. It is well-known that Granger causality cannot identify the causal direction for instant effects . In Pearl's causal framework, on the other hand, the causal direction between two binary variables is identifiable [14; 15; 16]. We can build upon these results and show that our causal model and MDL-based score can identify the causal direction for non-deterministic instant effects.

**Theorem 1**.: _Let \(S_{i}\) be an event sequence generated by a Poisson process as per Eq. (1) and \(S_{j}\) be an effect of \(S_{i}\) as per Eq. (3), with, low noise \(_{j}<(1-_{i,j})_{i}\), and a trigger probability \(_{i,j}<1\)._

_In the case of exclusively instant effects, i.e. \(_{i,j}(d)=(d)\), where \((d)\) is the Dirac delta function, the MDL score in the true causal direction is lower than in the anti-causal direction, i.e._

\[_{n_{i}}L(S_{j} S_{i},_{1})+L(S_{i}_{1})<L(S _{i} S_{j},_{2})+L(S_{j}_{2})\.\]

We provide the full proof in the Appx. A.1, the general idea is under a non-deterministic trigger mechanism, i.e. \(_{i,j}<1\). Then, in the causal direction, we can fully explain \(S_{j}\) with \(S_{i}\), but not vice-versa, as the cause is generated by a Poisson process. If \(_{i,j}=1\), i.e. the process is deterministic, we always observe cause and effect together, making them indistinguishable.

Delayed EffectsNext, we consider the case of exclusively delayed effects. Here, there is an inherent asymmetry in the benefit of knowing the cause versus the effect. As shown by Didelez  for marked point processes, and later used by Xu et al. , Eichler et al.  for Granger causality in Hawkes processes, the intensity of observing the cause after an event of the effect is unchanged. That is, the future of the cause is independent of the past of the effect, while if a cause triggers an effect, the intensity of the effect is increased by the cause. We have the following identifiability guarantee.

**Theorem 2**.: _Let \(S_{i}\) be an event sequence generated by a Poisson process as per Eq. (1) and \(S_{j}\) be an effect of \(S_{i}\) as per Eq. (3), such that \(H(_{j,j})>H(p(;_{i,j}))+_{i,j}^{-1}H( (_{i,j}))+_{j,j}^{-1}H((_{j, j}))\), where \(H\) denotes the entropy and \(\) the Bernoulli distribution._

_Then the matching in the anti-causal direction \(_{j i}\) of the effect \(S_{j}\) to the cause \(S_{i}\) has a worse MDL score than the true matching \(_{i j}\), i.e._

\[L(S_{j} S_{i},_{i j})+L(S_{i}_{i})<L(S_{i} S_{j}, _{j i})+L(S_{j}_{j})\.\]

We provide the full proof in the Appx. A.2. In the anti-causal direction \(S_{j} S_{i}\), the delay times follow the same exponential distribution of \((_{i})\), leading to no gain in score compared to the self-delay encoding. On the other hand, in the true causal direction, knowing the times of the cause leads to a better knowledge of the delay and hence a lower cost, so long as the delay distribution \(_{i,j}\) provides a better description than treating it as noise. This requirement is closely related to the algorithmic Markov condition, which postulates that the shortest description of a variable is given through its parents.

### Connection to Hawkes Processes

Hawkes processes  are analytically convenient and well-suited for modeling real-world processes where events trigger further events, e.g. earthquakes triggering aftershocks. Consequently, the majority of methods focusing on Granger causality are based on Hawkes processes [1; 2; 3]. The Hawkes process extends the Poisson process by incorporating the influence of past events on the intensity, i.e. the rate of occurrence of future events. This is done by means of excitation functions \(v_{i,j}(t-t_{k})\), which increase/inhibit the intensity of future events based on past events. The intensity function of a Hawkes process under a DAG structure is given by

\[_{j}(t)=u_{j}+_{i pa(j)}_{t_{k}<t,t_{k} S_{i}}v_{i,j}( t-t_{k})\.\]

Each event \(t_{k} S_{i}\) increases the intensity of seeing an effect by \(v_{i,j}(t-t_{k})\). The main difference between our model and a Hawkes process is our direct trigger model from cause to effect. In a Hawkes process, an event of type \(i\) increases the intensity and, therewith, the probability of effect events occurring. That is, contrary to our framework, in a Hawkes process there is no explicit one-to-one relationship between causing and effect events, i.e. no one event can be attributed solely to causing another. Nonetheless, in Appendix A.5 we show how to identify \(S_{i}\) as a parent of \(S_{j}\) by constructing a sequence of delays \(_{i j}\) with the most-influential past event and therewith \(_{i,j}\). If \(_{i,j}\) fulfills Theorem 2, we can identify \(S_{i}\) as a parent of \(S_{j}\). Hence, should the data be generated by a Hawkes process, our method can still pick up the causal relationship between the two event classes, so long as there are sufficiently many events where \(S_{i}\) is the primary cause.

## 4 Algorithm

With our model in place, we now turn to the problem of discovering the underlying causal structure from an observed sequence of events. In recent years, several methods that find and proceed on a topological ordering of the true graph have been introduced [20; 21; 22], which outperform other score-based frameworks such as GES  in terms of accuracy. We here propose the Cascade algorithm that instantiates this idea for information-theoretic scores. We prove that in the limit, it recovers not only the correct topological ordering but also the correct parent set of each node.

Cascade derives its guarantees from the _gain_ in bits of adding an edge \(i j\) to the model, i.e.

\[g(i j)=L(S_{j} S_{pa(j)},)-L(S_{j} S_{pa(j) i},_{i,j})+L(i j)\.\]

The edge cost \(L(i j)\) is constant and independent of the number of samples \(n_{i}\). In the limit \(n_{i}\), the gain inherits the identifiability guarantees from Sec. 3.2, such that \(g(i j)>g(j i)\) if \(S_{i}\) is a true ancestor of \(S_{j}\). In other words, the gain of an edge is greater in the causal than in the anti-causal direction.

### High Level Overview

Cascade initializes the model \(\) with an empty graph \(G\) and without any causal edges. During the search, we maintain a set of nodes \(C=[p]\), from which we remove nodes in a topological order of \(G^{*}\). We iterate over the following four steps until \(C\) is empty.

1. **Source Node Selection**: Select that node \(i C\) with minimal gain for any edge \(j i\), \(j C\), i.e. \[*{arg\,min}_{i C}\ *{max}_{j C}g(j i)-g(i j)\.\] (4)
2. **Edge Adding**: Add all _outgoing_ edges from \(i j\), \(j C\), to \(G\) that _improve_ our score.
3. **Edge Pruning**: Remove all _incoming_ edges \(j i\) from \(G\) that harm our score.

## 4 Node Set Update Remove \(i\) from \(C\).

Each iteration, Cascade selects that node \(i\), which has the minimal achievable gain when adding any edge \(j i\) to the current graph \(G\), expressed in Eq. (4); below, we will show that under our causal model this node is guaranteed to be a true source of the graph \(G^{*}\). We then add all edges from \(i\) to nodes \(j C\) that improve our score; provided that all true causal edges \(i j\) were added, there is now at least one node \(j C\) whose parents are all accounted for, that in the next iteration can be identified as a source. We remove edges \(j i\) from \(G\) to remove shortcuts. By repeating this process, Cascade proceeds in a topological order of the true graph \(G^{*}\). In total, Cascade requires \(p\) iterations, leading to an overall cubic complexity \(O(p^{3})\).

Source Node Selection.To identify a source node in the graph, we can use the identifiability guarantees from Sec. 3.2. They show that the gain \(g(i j)\) correctly orients the edge \(i j\) in the unconfounded bi-variate case. We additionally require that the edge gain is _pathwise oracle_, i.e. it can identify the direction of the path from \(i\) to \(k\).

**Theorem 3**.: _Given an event sequence \(S\) generated by a causal structure \(G^{*}\), let \(S_{i}\) be a source node of \(G^{*}\) and \(S_{v}\) be a descendant of \(S_{i}\), where there exists a path \(i j v\) in \(G^{*}\)._

_Then, the gain in the causal direction of the path \(g(i v)-g(v i)\) is greater._

We provide the proof in Appx. A.3. We can now show that the criterion in Eq. (4) selects nodes in a topological ordering of \(G^{*}\). Initially, Cascade has to identify a true source of \(G^{*}\), i.e. a node \(i\) without parents. For that node \(i\), all other nodes \(j\) are either ancestors or independent of \(i\). If \(i\) is an ancestor of \(j\), then \(g(j i)-g(i j)<0\), i.e. the gain in the anti-causal direction is lower. If \(i\) is independent of \(j\), then \(g(j i)=0\) and \(g(i j)=0\). Hence, the maximum achievable gain for a node without parents is zero.

Now consider a node \(v\) which does have a parent. For this node, there exists an ancestor \(u\) which is a true source. Hence, for that pair \(g(u v)-g(v u)>0\). Consequently, the maximum achievable gain is positive, whilst for a source node, we can maximally achieve zero, allowing us to identify true sources with Eq. (4).

In the next step, we add all outgoing edges from the source \(i\) to \(G\) that improve the score. As \(G^{*}\) is a DAG, we are now guaranteed to have another node \(j\), whose incoming edges are all accounted for in \(G\). Then, as per the causal model from Eq. (3), the only events that remain are those of the noise \(N_{j}\). Hence, \(j\) is now a source node for which the guarantees from above apply. By repeating this process, Cascade thus follows a topological order of \(G^{*}\).

Edge AdditionGiven a source \(i\), Cascade adds all outgoing edges \(i j\) that improve the score. We restrict the set to nodes \(j C\) from the candidate set only, i.e. to nodes further down the topological order. By the Algorithm Markov Condition, the description length of the true set of parents of a node \(j\) is smaller than the description length of any other set of parents, and hence the gain of the true edge is positive in the limit of \(n_{i}\).

When adding an edge \(i j\), where there is already an edge \(v j\), we use an Expectation Maximization approach to attribute all events to their respective cause. That is, we first find the bi-variate alignment \(_{i j}\) using all events in \(S_{j}\). Now, it is very likely that there are conflicts between \(_{i j}\) and \(_{v j}\), as the same event can be attributed to both \(i\) and \(v\). In those cases, we choose that event where the density \(_{i,j}/_{v,j}\) is higher and set the delay to infinity in the other matching. After re-assigning all events, we refit the delay distribution function \(_{i,j}\) using the new matching.

Edge PruningLastly, we deal with removing any shortcuts that have been added in the previous iteration. With the previous two steps, we are guaranteed to have a superset of all true causal edges incoming to \(i\). Fortunately, we can prune such edges directly with MDL by removing any incoming edge \(i j\) that does not improve the MDL score. In the chain graph \(i j v\), we would remove \(i v\) as the edge \(j v\) is sufficient to explain the data. In practice, given the current set of parents of \(i\) in \(G\), we search for the true set of parents by starting with the empty set and greedily adding only those edges that improve the score. As we show in Appx. A.3, a shortcut always has a lower gain than the true edge and hence will not be

Figure 2: Causal chainre-added. In this manner, we are asymptotically left with only the true causal parents. We can now finally show the consistency of Cascade.

**Theorem 4**.: _Given an event sequence \(S\), where each individual subsequence \(S_{i}\) was generated as per Eq. (3) by an underlying causal graph \(G^{*}\). Assuming all \(_{i j}\) are the true causal matchings. Under the Algorithmic Markov Condition, Cascade recovers the true graph \(G^{*}\) for \(n\)._

We postpone the proof to Appx. A.7. In the experiment section, we show that Cascade recovers the true DAG even in challenging settings and works well on real-world data.

## 5 Related Work

Causal discovery on observational data is an active research topic. Two main research directions exist: constraint-based  and score-based [23; 24] methods. Our approach belongs to the latter and is based on the Algorithmic Markov Condition . While Kolmogorov complexity is uncomputable, Marx and Vreeken  formally showed that if we instantiate the AMC with two-part MDL , we, on expectation, achieve the same results. MDL has been successfully used for bivariate causal inference [25; 15; 26], causal discovery , identifying hidden confounding , identifying mechanisms shifts , and identifying selection bias .

In this paper, we consider point processes. Particularly close to our method are Hawkes processes  as a way to model the influence of past events onto future events. As such, our work is also related to the concept of transfer entropy , which measures the influence in terms of Shannon entropy. Budathoki and Vreeken  proposed an MDL-based method for bivariate causal inference on event sequences, which is unsuitable for learning a global causal structure.

Existing methods for discovering causal graphs from event sequence data focus on different instantiations of Granger causality and can mostly be categorized by different intensity functions. Most common are parametric approaches with different regularizing [34; 2; 1]. ADM4  uses the nuclear matrix norm in combination with lasso, THP  uses BIC for regularization. The method MDLH by Jalaldoust et al.  is most closely related, as they also use MDL for regularization. NPHC  takes a non-parametric approach by using a moment matching method to fit second and third-order integrated cumulants. A recent development is neural point processes. Mei and Eisner  propose a deep neural network that learns the dependencies , which Xiao et al.  extended to include attention mechanisms. Zhang et al.  first learn a neural point process and then use a feature importance attribution method to obtain a weight matrix of pairwise variable influence.

## 6 Experiments

We evaluate Cascade on both synthetic and real-world data. Cascade is implemented in Python. We provide the source code, along with the synthetic data generator and the used real-world datasets online.3 We compare our method to four of state of the art methods: THP  as representative for the regularized parametric approaches, CAUSE  as representative for the neural point processes and NPHC  as a representative non-parametric approach, and MDLH  who also rely on MDL, as our most closely related competitor. CAUSE and NPHC do not return a graph but rather a weight matrix where the weight indicates the strength of the causal relation. On synthetic data, we can obtain a graph by thresholding such that we optimize the \(F1\) score.

### Evaluation

We evaluate the estimated graphs in terms of structural similarity by the Structural Hamming Distance (SHD) , in terms of causal similarity by the Structural Intervention Distance (SID) , and predictive performance by F1 score. To compare graphs of different sizes, we report the scores normalized by the maximally achievable SHD/SID and show the unnormalized scores in Appendix B. NPHC, CAUSE, and MDLH can and often do return cyclic graphs. As SID is strictly only defined for acyclic graphs, we omit these methods from the SID evaluation.

### Synthetic Data

We begin by comparing all methods on data with known ground truth. To this end, we generate synthetic data. We generate both data within and outside our causal model and vary aspects such as noise intensity, number of event types and the number of parents of a variable. We describe the full data-generating process in Appendix B.

Sanity CheckWe start with a sanity check on data without any structure over 20 variables, Cascade correctly does not report any causal edge. THP reports in 45% of the cases at least one spurious edge. We omit the results of CAUSE and NPHC as it is unclear how to choose a meaningful, non-trivial threshold, in this setting. MDLH did not terminate within 96 hours.

ScalabilityWe evaluate how well each method scales under an increasing number of variables. We vary the number of nodes, which correspond to the number of unique event types, from 5 to 50 and report the results in Fig. 2(a). As MDLH did not terminate within 96 hours for 15 variables, we omit it from here on out. For a lower number of nodes, both Cascade and THP obtain far better results than NPHC and CAUSE. With increasing event types, all methods SID and F1 scores decrease. Amongst all methods, Cascade scales best with an increasing number of nodes, whereas Granger causality based methods such as THP and NPHC find many spurious edges of connected but not causal variables. On the other hand, Cascade is the most accurate method for a higher number of nodes, showing the efficacy of its causal model and MDL-based approach.

NoiseNext, we assess the impact of noise, which are events that is not caused by any parent. To this end, we vary the 'cause' probability and the fraction of events due to additive noise. We do so by varying a noise parameter \(a\), adding an additional \(n_{i} a\) events to \(S_{i}\) (additive noise), and by setting the 'cause' probability \(=1-a\), i.e. we decrease additive noise and increase trigger probability. We show the results in Fig. 2(b). We observe that Cascade does quite well for high noise and that for noise levels of \(a=0.7\) and lower, it (mostly) recovers the true DAG. All other methods perform considerably worse.

Figure 3: DAG recovery in different settings. We show normalized SHD, normalized SID, and \(F1\) score, the \(Y\)-axis are truncated for better visualization. In (a) we vary the number of event types, on the SID score we observe that the graph reported by Cascade is casually, the most similar to the true DAG. In (b) we decrease the noise, Cascade does recover a close causal graph, even under high noise. Finally, in (c) we increase the number of parents of a collider, we observe that a high number of parents does not pose a problem for Cascade.

CollidersMatching an effect event to the correct parent, resp. modeling the correct excitation, becomes increasingly challenging for a larger number of parents. We test this through a setting where half (\(\)) the variables converge into a collider, and the other half (\(\)) are independent. We vary the total number of variables, \(p\) and we show the results in Fig. 3c. We observe that Cascade achieves almost perfect results. THP is robust, but with an increasing number of nodes, it starts to miss edges. Beyond 100 variables, it does not terminate within 24 hours. To validate that our method can recover structures with multiple colliders, we repeat the same experiment where 10% of nodes are colliders. That is, for 50 event types, 5 are colliders and 23 direct causes of all 5 colliders. The remaining 22 are independent. Resulting in an \(F1\) score of 0.97 for 50 event types, slightly decreasing to 0.82 for 200 event types; as such Cascade can deal well with multiple colliders.

Instantaneous EffectsNext, we evaluate performance under instantaneous effects. First, we consider data with exclusively instant effects. Cascade achieves an average unnormalized SHD of 32.8. The second best-performing method, NPHC, achieves 46.85. Next, we generate a setting where 90% of the effects are instantaneous and the others occur with a small delay. Cascade improves to an SHD of 19.45, while NPHC achieves the second lowest average with 47.5. We provide all results in the Appendix B.

Hawkes ProcessesFinally, we evaluate how effectively Cascade recovers the true DAG on data generated by a Hawkes process. We vary the intensity of the excitation function, i.e., the expected number of events generated per cause. We show the results in Figure 4. We observe that Cascade performs best when our assumptions hold, when there is one effect per cause or fewer, but still demonstrates strong performance across all settings.

### Real-World Data

We evaluate Cascade on three distinct datasets of real-world event sequences. We begin by evaluating Cascade on a dataset of network alarms, where the causal structure is known.

Network AlarmsThis data was provided by Huawei for the NeurIPS 2023 CSL-competition4 and consists of data from a simulated network of devices in which alarms can cause other alarms. We run all methods and get an (unnormalized) SHD score of 42 for Cascade, 127 for THP, 214 for NPHC, and 1564 for CAUSE. As the network connectivity structure is known, we can take it into account during the search. THP supports this natively, Cascade can be trivially constrained to only consider the given edges. Cascade correctly identifies 142 out of 147 causal edges, THP 20. Neither method reports spurious edges. We show the full recovered graph in Appendix B.5.

Global BanksSecond, we run Cascade on a daily return volatility dataset , we follow the preprocessing of Jalaldoust et al. , specifically we turn the time series into an event sequence by rolling a one year window over the data and register an event if the last value is among the top 10%. The dataset includes the 96 world's largest publicly traded banks. We show the largest discovered subgraph in Fig. 5. In addition, three unconnected sub-graphs are discovered, one covering two banks in Australia and two others connecting banks in Japan, which we provide in Appendix B.5.

Daily ActivitiesWe run Cascade on a dataset of recorded daily activities . Our method reports plausible causal connections such as _Sleeping End \(\) Showering Start \(\) Showering End \(\) Breakfast Start \(\) Breakfast End_, etc. We show the complete graph in the Appendix B.5. This result reinforces the suitability of our causal model and Cascade for real-world data, and illustrates the potential of our method to discover causal structures in a wide range of applications.

## 7 Conclusion

We studied the problem of causal discovery from event sequences, we propose a cause-effect matching approach to learn a fully directed acyclic graph (DAG). To this end, we introduced a new causal model

Figure 4: DAG recovery on data generated by a Hawkes process.

and an MDL based score. We proposed the Cascade algorithm to discover causal graphs through a topological search from observational data. Finally, we evaluated Cascade on synthetic and realistic data. On synthetic data, we find that Cascade is either the best or close to the best-performing method across all settings, both within and outside our causal model. In particular, whenever conditions get challenging, e.g. due to noise or with multiple colliders, Cascade outperforms all other methods by a significant margin. We examined how Cascade performs on real world event sequences, where the true data-generating process may lie outside our causal model. We found that Cascade recovers meaningful graphs that match with a common understanding of the world.

**Limitations** As is necessary, we have to make causal assumptions. The most prominent in our work is the direct matching between a cause event and an effect event - which precludes modeling of a single event causing multiple other events, as well as multiple events jointly causing a single effect event - and that we only consider excitatory effects - which precludes modeling the absence of events due to a cause. Our proof of identifiability for instantaneous effects depends on the strengths of the trigger resp. noise probabilities. The identifiability of the model seems provable via the independence of these, but how to operationalize this into an effective score and search algorithm are open questions.

**Future Work** Currently, our structural equations are 'or' relations over the parent's variables. An interesting future direction would be to explore 'and' relations, e.g., A and B together cause C. This raises several questions, like how close to each other A and B have to occur or if the order matters. Another interesting future direction is to allow matching of multiple causing events to one event, where each parent _could_ have caused the event. This would allow us to answer counterfactual questions, such as if a causing event had not occurred, would we nevertheless observe its effect? This strongly relates to the firing squad example by Pearl , where multiple guards shoot a prisoner at the same time; if one guard did not shoot, the prisoner would still have died.

Figure 5: Result of Cascade on the _Global Banks_ dataset, we show the largest subgraph, we highlight the 10 largest, by assets, banks. We clearly see Cascade recovers locality and that larger banks have a strong influence on the market, both information not provided in the input.