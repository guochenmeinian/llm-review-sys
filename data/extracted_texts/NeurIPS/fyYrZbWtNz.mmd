# Rethinking Imbalance in Image Super-Resolution

for Efficient Inference

 Wei Yu\({}^{1}\), Bowen Yang \({}^{1}\), Qinglin Liu \({}^{1}\), Jianing Li \({}^{2}\), Shengping Zhang \({}^{1,}\), Xiangyang Ji \({}^{2,}\)

\({}^{1}\) School of Computer Science and Technology, Harbin Institute of Technology

\({}^{2}\) School of Information Science and Technology, Tsinghua University

20b903014@stu.hit.edu.cn, 2022211119@stu.hit.edu.cn, qlliu@hit.edu.cn, lijianing@pku.edu.cn, s.zhang@hit.edu.cn, xyji@tsinghua.edu.cn

Corresponding author.

###### Abstract

Existing super-resolution (SR) methods optimize all model weights equally using \(_{1}\) or \(_{2}\) losses by uniformly sampling image patches without considering dataset imbalances or parameter redundancy, which limits their performance. To address this issue, we formulate the image SR task as an imbalanced distribution transfer learning problem from a statistical probability perspective and propose a plug-and-play Weight-Balancing framework (WBSR) for image SR to achieve balanced model learning without changing the original model structure or training data. Specifically, we develop a Hierarchical Equalization Sampling (HES) strategy to address data distribution imbalances, enabling better feature representation from texture-rich samples. To tackle model optimization imbalances, we propose a Balanced Diversity Loss (BDLoss) function, focusing on learning texture regions while disregarding redundant computations in smooth regions. After joint training of HES and BDLoss to rectify these imbalances, we present a gradient projection dynamic inference strategy to facilitate accurate and efficient reconstruction during inference. Extensive experiments across various models, datasets, and scale factors demonstrate that our method achieves comparable or superior performance to existing approaches with approximately a 34% reduction in computational cost. The code is available at https://github.com/apiixel/WBSR.

## 1 Introduction

Image super-resolution (SR) aims to reconstruct high-resolution (HR) images with more details from low-resolution (LR) images. Recently, deep learning-based image SR methods have made significant progress in reconstruction performance through deeper network models and large-scale training datasets, but these improvements place higher demands on both computing power and memory resources, thus requiring more efficient solutions. Therefore, various techniques such as pruning [36; 53; 55], quantization [34; 39; 5], knowledge distillation [18; 45], and designing lightweight architectures [17; 25; 40] have been widely researched to accelerate inference and meet the requirements of deployment inference on resource-constrained platforms. However, these methods rely on static networks to process all input samples fairly, ignoring the different requirements of diverse samples for network computational cost, which limits the representation ability of the model.

In contrast, dynamic neural network based methods [4; 42; 54; 48] can dynamically adjust the network structure or parameters and reduce the average computational cost, becoming a mainstream research focus in recent years. These methods can adaptively allocate networks with suitable computationalcosts according to the content of the input samples during inference. Despite the advancements in these dynamic network solutions, practical applications are still hindered by two prevalent limitations:

**Data Distribution Imbalance.** Existing SR methods  mostly use uniformly sampled LR-HR patch pairs instead of the entire image to train models due to the limitation of memory resources. However, they ignore the underlying fact that patch contents in images exhibit imbalanced distributions (i.e., the abundant easily reconstructed smooth flat patches and rare hardly reconstructed edge texture patches), resulting in inherent data bias. Figure 1 (a) shows that the number proportion of easy flat patches (48.8\(\%\)) is much larger than that of hard textured patches (16.6\(\%\)).

**Model Optimization Imbalance.** Current SR methods  typically employ \(_{1}\) or \(_{2}\) losses to treat all patch areas and optimize each weight equally, which lacks reasonable optimization for their model training. Since the details lost in low-resolution images mainly exist in edges and texture locations, fewer computational resources of the model are required for those flat patches. Therefore, existing SR methods involve redundant calculations in flat areas, which leads to imbalanced inference performance where the model overfits in simple areas and underfits in complex ones and results in uneven distribution of model computational resources as shown in Figure 1 (b). For the same image, the optimized RCAN  model exhibits overfitting in the smooth background area (green box, with error pixels accounting for only 0.08\(\%\)), while it shows obvious underfitting in the textured foreground area (red box, with error pixels accounting for up to 52\(\%\)).

Overall, these prevalent imbalance problems of data distribution and model optimization in the real world limit the performance of current image SR algorithms. As motivated, although this imbalance is a well-known observation in the classification task , we formulate the image SR task as an imbalanced distribution transfer learning problem from a statistical probability perspective. To mitigate the gap, we propose a plug-and-play weight-balancing framework, dubbed WBSR, to achieve balanced model learning without additional computation costs, which improves the restoration effect and inference efficiency of models without changing the original model structure and training data, as shown in Figure 1 (c). Specifically, to address the imbalance problem of data distribution, we develop a Hierarchical Equalization Sampling (HES) strategy, enabling better feature representation from texture-rich samples to mitigate data biases. Then, to solve the imbalance problem of model optimization, we propose a Balanced Diversity Loss (BDLoss) function, focusing on learning texture areas while disregarding redundant computations in smooth areas. After joint training of HES and BDLoss within WBSR to rectify these imbalances, we present a gradient projection dynamic inference strategy to facilitate accurate and efficient inference.

In summary, we make the following three key contributions: **(1)** This paper is the first attempt to explore the imbalance in the image super-resolution field and gives a reasonable analysis from a perspective of probability statistics, i.e., the imbalance of data distribution and model optimization limits the algorithm performance. **(2)** We propose a plug-and-play weight-balancing framework dubbed WBSR upon HES and BDLoss to achieve balance training without additional computation costs, which improves the restoration effect and inference efficiency of models without changing the original model structure and training data. **(3)** Extensive experiments across various models,

Figure 1: Illustration of (a) the data distribution from the widely used DIV2k  training set, (b) the reconstruction results of RCAN  model, and (c) the proposed weight-balancing framework.

datasets, and scale factors demonstrate that our achieves comparable or superior performance to existing methods with less computational cost.

## 2 Related Work

### Deep Imbalanced Learning

Deep imbalanced learning has attracted widespread attention due to the imbalanced data distribution caused by the difficulty of data acquisition in practical applications [20; 47]. The data imbalance problem presents a significant challenge in deep learning, when some classes have fewer samples than others, resulting in poorer model prediction performance for the minority class. Previous imbalanced learning methods [43; 50] have mainly studied data resampling techniques to solve this problem. For example, over-sampling minority classes [30; 3] and under-sampling common classes [2; 44]. However, oversampling increases memory storage and training time, while under-sampling causes overfitting problems [10; 5; 29]. Recently, several works [41; 49] attempt to develop data resampling methods as data augmentation strategies for the image super-resolution task to compensate for the imbalance of training patches between different classes.

Another category of class-imbalanced learning methods is reweighting techniques. Recent reweighting methods assign weights to different classes [14; 7; 28] and training examples [15; 13; 38], which aim to modify their gradients to make models balance.  process from a domain adaptation perspective and enhance classic class-balanced learning by explicitly estimating the differences between different class distributions using meta-learning methods. In contrast, these methods balance the data loss by reweighting each class instead of sampling to achieve a balanced data distribution.

### Dynamic Network for Efficient Image Super-Resolution

Recent researches address this problem with efficient dynamic network frameworks, which mostly adopt content-aware modules to dynamically send image patches to sub-networks with different complexities to accelerate model inference. ClassSR  combines classification and SR in a unified framework, which uses an additional class module to classify image patches into different classes, and then applies the subnets to perform SR on different classes. ARM  further adopts the validation set to build an Edge-to-PSNR lookup table by mapping edge scores of image patches to the performance of each sub-network to select appropriate subnets to further improve efficiency. PathRestore  introduces a pathfinder to implement a multi-path CNN, which can dynamically select appropriate routes for different image areas according to the difficulty of restoration. However, these techniques still have two key problems. One is the additional amount of parameters and calculations brought by the introduction of classifiers or selectors, and the other is the neglect of data and network imbalance that affects the performance of the model.

## 3 Theoretical Analysis

Let \(x\) and \(y\) denote LR and HR patches and \(_{1}\) loss as an example (Note that the theoretical applies to the \(_{2}\)), the optimization object of SR task can be written as

\[_{}_{(x,y)|p_{data}}||y-||_{1}\] (1)

where \(=f_{}(x)\) represents the SR result estimated from LR \(x\) with SR model \(f_{}\). \(\) denotes the model parameters. \(p_{data}\) indicates the data distribution space. It aims to minimize all absolute errors between predicted images and ground-truth images from the whole data. From the natural assumption that the distribution of the training set is imbalanced, whereas the independent testing set is balanced [11; 14; 10], so we set the training data and testing data are drawn from different joint data distributions, \(p_{ train}(x,y)\) and \(p_{ bal}(x,y)\), respectively. The conditional probability \(p(x|y)\) is the same in both training and testing sets due to the fixed downsampling degradation in the SR task.

From the probabilistic view, the prediction \(\) of the SR network is considered as the mean of a noisy prediction distribution, which can be modeled as a Gaussian distribution

\[p(y|x;)=(y;,^{2}_{ noise})\] (2)

where \(^{2}_{ noise}\) indicates the variance of the independently distributed error term. The prediction \(\) can be treated as the mean of a noisy prediction distribution. Eq. 2 can be interpreted as the distribution form of Eq. 1, corresponding to the maximized negative log-likelihood (NLL) loss in the regression of the prediction distribution. Consequently, the prediction model trained by \(_{1}\) actually captures the mean value of the entire solution space, i.e., the distribution of the training set.

**Theorem 1** (Distribution Transformation).: _Considering the discordance between \(p_{}(y|x)\) and \(p_{}(y|x)\) attributable to the distribution shift. Given the identical conditional probability \(p(x|y)\) across both the training and testing sets, we leverage the Bayes rule \(p(y|x) p(x|y) p(y)\) to establish the relationship through variable substitution as follows_

\[p_{}(y|x)=p_{}(y)}(y|x)} {p_{}(y)}}(x)}{p_{}(x)}\] (3)

This theorem reveals that the existence of imbalance issues stems from the direct proportionality between \(p_{}(y|x)\) and \(p_{}(y)\) with a ratio of \(}(x)}{p_{}(x)}\). When a specific type of patch sample is infrequently present in the training set, i.e., when \(p_{}(y)\) is low, the value of \(p_{}(y|x)\) decrease as well, which results in a decrease in the accuracy of predictions. As a consequence, the trained SR model tends to underestimate the occurrence of rare patches during prediction. Meanwhile, considering that the integral of \(p_{}(y|x)\) equals \(1\), we can obtain

\[p_{}(y|x)=}(y|x)}{_{Y}p_{}(y^{}|x)dy^{}}\] (4)

where \(Y\) denotes the entire training sample space. Then, we substitute Eq. 3 into Eq. 4 to model the relationship between the two distributions through explicit distribution transformation

\[p_{}(y|x)=}(y|x) p_{}(y )}{_{Y}p_{}(y^{}|x) p_{}(y^{} )dy^{}}\] (5)

where \(y^{}\) denotes the integral variable. Diverging from previous works that focus on modeling \(p_{}(y|x)\), our objective is to estimate \(p_{}(y|x)\) for achieving balanced prediction on the testing set. The detailed proof is available in the supplementary materials. The aforementioned theory proves that the imbalanced model optimization caused by imbalanced data distribution and loss function is reasonable. Therefore, our approach aims to correct this imbalance without introducing additional datasets or computational costs.

## 4 Methodology

### Weight-Balancing Training Framework

Based on the observed phenomenon and analysis, the imbalanced model optimization of image SR undoubtedly limits the reconstruction performance of the model, especially for rare hard texture patches. We consider attaining a robust model representation with balanced weights from the perspective of two aspects: data sampling and optimization function. Figure 2 (a) illustrates the training process of the proposed framework, dubbed WBSR, which consists of two main components: Hierarchical Equalization Sampling (HES) and Balanced Diversity Loss (BDLoss). Given input LR patches from the training set, we employ HES to sample a batch of approximately balanced patches to optimize each subnet model with our BDLoss \(_{bd}\). The overall optimization objective is

\[_{}_{(x,y)|p_{}}_{bd}(y-_{m_{}}(x))\] (6)

where \(S_{m_{}}\) represents the \(m\)-th subnet in the supernet with parameters \(_{m}\). We employ a divide-and-conquer optimization strategy to learn nearly balanced weights, minimizing the overall objective by ensuring that each individual subnet within the supernet is well-optimized. Each subnet with varying computational cost shares the weights of the supernet and is intended to handle image patches of different complexities, which does not introduce additional complexity that impedes the inference speed. In the following, we describe the details of our HES and BDLoss, respectively.

#### 4.1.1 Hierarchical Equalization Sampling

Without prior data classification, we propose a simple yet effective Hierarchical Equalization Sampling (HES) strategy, which utilizes inherent gradient information of patches to perform sample-levelsampling and class-level sampling of difficult and easy classes to achieve equalization between the abundant simple samples and rare difficult samples.

**Sample-Level Sampling** refers to uniformly sampling patches from the training dataset. Each sample is sampled with equal probability during the training stage, whose probability is \(P_{i}=\). \(i\) indicates the \(i\)-th samples. \(N\) denotes the total number of training patch samples. It ensures that the model learns stable initial weights early in training, capturing general features across different sample types.

**Class-Level Sampling** aims to assign a higher sampling probability to rare difficult samples. Unlike the image classification task where the number of categories is determined, samples in image SR are unclassified and the number is unknown. To address it, we calculate the gradient vectors online consisting of the mean and standard deviation of the gradient magnitude of the input samples in the horizontal and vertical directions, which assess the reconstruction difficulty of samples and then classify them using vector thresholds \(t\) to obtain the sampling probability. The threshold for the \(k\)-th class is defined as follows

\[t_{k}=t[], k[1,K]\] (7)

where \(K\) is the number of classes. \(t_{1}\) and \(t_{K}\) represent the gradient threshold of the simplest and most difficult classes, respectively. The number of samples for the \(k\)-th class corresponds to the \(N_{k}\) samples whose gradient vectors fall within the range from \(t_{k-1}\) to \(t_{k}\). The sampling possibility \(P_{k}\) can be calculated by

\[P_{k}=^{K}}}{N_{k}^{k}}\] (8)

where \((0,1)\) indicates the exponential factor to avoid overfitting simple data by reducing the number of samples. It enables the sampled batch training data containing samples from difficult classes, thereby achieving equalized data sampling.

The core concept of the proposed hierarchical equalization sampling strategy is to reconcile the data bias caused by the inherent imbalance, i.e., difficult samples are visually more important than smooth samples. During training and testing, the gradient vectors of image patches can be quickly exported using existing gradient operator . Therefore, our HES method does not impose any additional computational burden and effectively leverages dataset information to enhance the model's feature representation capabilities for hard samples.

Figure 2: Illustration of the proposed weight-balancing framework. (a) The training stage combines hierarchical equalization sampling and balanced diversity loss to jointly train a supernet model with balanced weights. (b) The testing stage adopts the gradient projection dynamic inference with a gradient projection map and multiple dynamic subnets for efficient inference.

#### 4.1.2 Balanced Diversity Loss

The commonly used \(_{1}\) and \(_{2}\) losses of previous methods treat all patches equally and perform gradient updates on each weight parameter, which ignores parameter redundancy and leads to overfitting on simple patches and underfitting on rare hard patches. To achieve reasonable optimization of models for diversity patches, we propose a novel Balanced Diversity Loss (BDLoss) to learn approximate balanced model weights, which performs distribution transformation by exploiting the training distribution without additional data to achieve balanced predictions. In accordance with Theorem 1, we first estimate the desired \(p_{}(y|x)\) by minimizing the NLL loss

\[p_{}(y|x;)=(y;,_{}^{2} )\] (9)

**Definition 1**.: _To balance the uncertainty of model diversity predictions and avoid excessive optimization, our BDLoss is defined as the likelihood function_

\[_{bd}=- p_{}(y|x;)+||||_{2}\] (10)

where \( p_{}(y|x;)\) denotes the converted conditional probability aimed to obtain balanced model weights \(\). \(||||_{2}\) indicates the \(L_{2}\) regularization function to prevent model overfitting. \(\) represents a regularization coefficient. Next, we derive the implementation of \(_{bd}\) based on Eq. 9

\[& p_{}(y|x;)=}(y|x;) p_{}(y)}{_{Y}p_{}( y^{}|x;) p_{}(y^{})dy^{}}\\ &=(y;,_{}^{2} )+ p_{}(y)-_{Y}(y^{};, _{}^{2}) p_{}(y^{})dy ^{}\] (11)

where \( p_{}(y)\) is constant term that can be omitted. The first remaining term is a probability form of \(_{1}\) loss as Eq. 2. The last term of \(_{Y}(y^{};,_{}^{2} ) p_{}(y^{})dy^{}\) indicates the key diversity balancing term that obeys Gaussian distribution, which involves the integral operation and necessitates finding a closed-form expression.

Building upon the design of previous classification tasks , we utilize the Gaussian Mixture Model (GMM) technique to represent the constant term

\[p_{}(y)=_{i=1}^{L}_{i}(y;_{i},_{i})\] (12)

where \(L\) denotes the number of Gaussian components. \(\), \(\), \(\) indicate the weights, means and covariances of multi-dimensional GMM, respectively. As the multiplication of two Gaussian functions results in another unnormalized Gaussian, the diversity balancing term can be expressed as

\[_{Y}(y;,_{}^{2}) _{i=1}^{L}_{i}(y;_{i},_{i})dy=_{i=1}^{L}_{i }s_{i}_{Y}(y;_{i},_{i})dy\] (13)

where \(s_{i}\), \(\), and \(\) are the norms, means, and covariances of the resulting unnormalized Gaussian, respectively. Now, the integral of the balanced diversity term adheres to a Gaussian distribution and is solved straightforwardly, so the BDLoss of Eq. 10 can be derivable as follows

\[_{bd}=-(y;,_{}^{2} )+_{i=1}^{L}_{i}(;_{i},_{ i}+_{}^{2})+||||_{2}\] (14)

### Gradient Projection Dynamic Inference

Figure 2 (b) illustrates the testing process of our WBSR framework, we propose a gradient projection dynamic inference strategy to achieve a dynamic balance of efficiency and performance. It adaptively allocates the subnet model without any increase in additional parameters by calculating the gradient projection map based on the input content.

**Gradient Projection.** We observe that patches with complex (simple) structures exhibit high (low) image gradient magnitude and do not suffer more (less) score degradation with SR scale changes. Following the approach described in Section 4.1.1, we calculate gradient vectors to measure thecomplexity of the patch contents and construct a gradient projection map online to project the gradient vector of an image patch to the selection of each subnet model. At inference time, each patch can select a suitable subnet upon its gradient vector. When low-resolution noise exists in image patches, the edge detection methods [37; 8] ignore the local complexity of the patch and result in missed detections, thereby erroneously categorizing the patch as a simple sample. We count the changes in gradient strength by calculating the standard deviation directly, when there is a large amount of noise or texture changes of varying intensity in the local area of the patch, it can still be correctly assigned as a difficult sample. As shown in Figure 3, yellow boxes represent areas of local texture change, such as the clouds in the previous row and the railings in the next row. It can be intuitively seen that our gradient projection method can accurately distinguish local smooth regions or textured regions and assign them to the corresponding small or large subnets.

**Dynamic Inference.** To facilitate the deployment of the model across any hardware resources, our dynamic supernet contains multiple subnets by gradually shrinking the model calculation with structured iteration to dynamically adapt various computational and performance requirements. During inference, we adopt the dynamic supernet to individually distribute image patches of \(K\) classes to \(M\) subnets to obtain better computational performance trade-offs. Given a new LR patch, we first calculate its gradient vector and derive its class \(\) according to the threshold \(t\). Then, the selected subnet for inference can be easily obtained by equally splitting the gradient vector interval into a total of \(M\) subintervals, which can be expressed as

\[m= M}{K}\] (15)

where \(m[1,M]\) denotes the index of the selected subnet to reconstruct this LR patch. \(\) indicates the ceiling function that tends to select the larger subnet. However, the larger subnet selection leads to better performance with heavier computation, we further consider selecting the inference subnet under the limited computational resources \(C_{t}\)

\[=_{m}| C_{m}}{K}-C_{t}|\] (16)

where \(\) indicates the selected optimal subnet under resource constraints. \(C_{m}\) denotes the computational cost of the \(m\)-th subnet. \(\) is a hyperparameter that is utilized to strike a balance between the computational cost and performance, where higher values prioritize improved performance, while lower values favor reduced computational overhead. Consequently, our WBSR framework can be flexibly adjusted to accommodate diverse application scenarios based on actual performance and hardware resource requirements.

## 5 Experiments

### Experimental Details

**Datasets and Metrics.** Following the previous works [23; 4], we apply DIV2K  as the training dataset widely used for image SR, which includes 800 high-quality images with diverse contents and texture details. To verify the model performance under different image content distributions, four datasets are employed for model testing, including B100 , Urban100 , Test2K, and Test4K. Test2K and Test4K are downsampled from DIV8K . For metrics, we adopt peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) to quantitatively evaluate all methods. Additionally, the FLOPs are calculated as the average results of all patches in the entire test dataset images.

**Implementation Details.** Our proposed WBSR can be easily incorporated into existing CNN-based SR networks to achieve efficient inference. SRResNet  and RCAN  are selected as two baselines in our experiments for a fair comparison, we conduct extensive experiments on four datasets of different SR scales to verify the effectiveness of our framework. During training, we set nine

Figure 3: Visualizations of (a) the edge detection results, (b) the gradient magnitude results, and (c) the projected subnet selection. For ease of observation, we visualize three assigned subnets with small, medium, and large computational costs as green, yellow, and red, respectively.

subnets (i=9) with different parameters \(_{i}\) in each supernet. For SRResNet, the widths and depths of the subnets are set as ([36; 52; 64]) and ([4; 8; 16]), respectively. As for RCAN, the widths and depths of the subnets are configured as ([36; 52; 64]) and ([5; 10; 20]), respectively. The compared width adaptation algorithms [23; 4] also follow such model width configuration to ensure a fair comparison. All methods are implemented using PyTorch and trained on an NVIDIA GeForce RTX 3090 for 100 epochs with 16 batch sizes, where the first 70 epochs are sample-level sampling and the rest are class-level sampling. The former aims to maintain the original data distribution of the entire dataset, ensuring a stable and comprehensive feature representation. The latter focuses on correcting the imbalance of dataset and enhancing the model's ability to represent difficult texture samples. The training times of SRResNet and RCAN are 25 and 28 GPU hours using a single GPU, respectively The training patch size is set to 128 \(\) 128 and augmented by horizontal and vertical flipping to enhance its robustness. We utilize our \(_{bd}\) loss along with the Adam optimizer , setting \(_{1}=0.9\) and \(_{2}=0.999\). To adjust the learning rate, we apply a cosine annealing learning strategy, starting with an initial learning rate of \(2 10^{-4}\) and decaying to \(10^{-7}\).

### Comparison results

Table 1 shows the quantitative performance of our approach coupled with various SR baselines in terms of the metrics, parameter number, and computational cost. ClassSR  with an additional classifier module has more parameters, resulting in additional computational and parameter costs. ARM  adopts the validation set to build an Edge-to-PSNR lookup table, which generates additional inference time and parameter storage overhead. In comparison, our framework achieves superior performance with less computation (average 62\(\%\)) than baselines, without incurring additional parameters or computational costs. When tested on unseen datasets such as B100 and Urban100, which lie outside the trained distribution, the compared methods exhibit performance degradation due to overfitting to specific features of the original training dataset and a lack of generalization ability to diverse data. In contrast, our method maintains comparable performance to the original model with lower computational costs (averaging 70\(\%\)), benefiting from our balanced sampling and optimization strategies during training.

Figure 4 shows visual comparisons across four testing datasets. The SR images produced by ClassSR and ARM exhibit structural blur and noise. In contrast, our method recovers more detailed information, resulting in better outcomes that are more faithful to the HR ground truth.

### Ablation Studies

To verify the effectiveness of our WBSR upon HES and BDLoss, we conduct ablation studies on Tesla2K and Test4K with sclae factor of \(\)4, as shown in Table 2.

**Effectiveness of HES.** During model training, we replace the original uniform sampling in baseline with our HES strategy. As the method "+HES" shown in Table 2, HES achieves a 0.05 dB average improvement in terms of PSNR compared with the baseline, which benefits from enhanced feature representations in hard texture patches.

   &  &  &  &  &  &  \\  & & &  &  &  &  &  &  &  &  \\   & SRResNet  & 1.52 & 32.919 & 20.78 (1005) & 32.11 & 20.78 (1005) & 30.30 & 20.78 (1005) & 31.90 & 20.78 (1005) \\  & +ClassSR  & 3.12 & 31.68 & 14.755(315) & 31.15 & 62.818(305) & 30.24 & 14.143(1058) & 31.89 & 18.51(0555) \\  & +ASMM  & 1.52 & 31.69 & 16.12 (74.1) & 16.13 (61.1) & 30.26 & 15.59 (75) & 31.90 & 13.11 (664) \\  & +WSBR (Ours) & 1.52 & 32.14 & 12.26 (59.93) & 31.98 & 13.36(649) & 30.41 & 12.05 (580) & 32.20 & 12.68(65.5) \\   & RCAN  & 15.59 & 32.40 & 130.40 (1005) & 32.283 & 130.40 (1005) & 30.86 & 13.04 (1005) & 32.26 & 13.04 (1005) \\  & +ClassSR  & 30.10 & 31.85 & 91.208(704) & 31.72 & 10.050(295) & 30.79 & 33.636(645) & 32.24 & 8.346(645) \\  & +ASMM  & 15.59 & 31.89 & 99.106(764) & 31.74 & 109.54 (34) & 30.80 & 105.62 (811) & 32.24 & 97.80 (754) \\  & +WBSR (Ours) & 15.59 & 32.34 & 38.68(1068) & 32.21 & 96.50(743) & 30.91 & 95.65(1589) & 32.27 & **77.65(1062)** \\    & SRResNet  & 1.52 & 22.34 & 5.19 (1005) & 25.30 & 5.59 (1005) & 26.19 (1005) & 27.65 (1005) & 27.65 & 3.19 (1005) \\  & +ClassSR  & 3.12 & 3.53 & 33.874(105) & 24.53 & 42.923(105) & 26.20 & 36.02 (2078) & 27.66 & 3.30 (6559) \\   & +ASMM  & 1.52 & 26.53 & 43.43 (134) & 24.54 & 41.48 (86.26) & 26.12 & 3.76 (724) & 27.66 & 3.33 (644) \\   & +WSBR (Ours) & 1.52 & **27.36** & 33.997(725) & 25.22 & 43.46 (3649) & 26.26 & 33.67 (6559) & 27.73 & **22.62(25)** \\   & RCAN  & 15.59 & **27.76** & 32.60 (1005) & **25.52** & 32.60 (1005) & 26.39 & 32.60 (1005) & 27.89 & 32.60 (1005) \\   & +ClassSR  & 30.10 & 26.70 & **22.82** & 25.75 (794) & 25.14 & 25.36 (875) & 26.39 & 22.122 (659) & 27.88 & 19.49 (605) \\   & +ASMM  & 15.59 & 26.74 & 25.75 (794) & 25.14 & 25.36 (875) & 26.39 & 26.70 (824) & 27.88 & 25.10 (774) \\   & +WSBR (Ours) & 15.59 & 27.75 & 25.10(774) & 25.81 & 27.01(839) & 26.45 & 18.52(529) & 27.94 & 19.40(59) \\  

Table 1: Quantitative comparison results of our method and other SOTA methods on the GoPro and H2D datasets. The optimal and suboptimal results are highlighted.

Furthermore, we conduct additional experiments to compare our HES with existing sampling works [41; 49; 29] in Table 3. It shows that our HES outperforms the previous best sampling method BSPA of an average of 0.1dB in terms of PSNR and demonstrates the superiority and generalization capabilities of our HES. In "+WBSR\(\)", we can achieve even greater performance gains of 0.18 dB by integrating our HES with our BDLoss. HES first performs more sample-level sampling to learn generalized feature representations followed by fewer selective class-level sampling to focus on texture-rich regions to correct sample bias with stable learning and prevent the model's overfitting, which mitigates the model oscillation and addresses the overfitting problem. Furthermore, our HES achieves balanced stable training with our BDLoss for diverse samples in each training step, which solves the instability and training bias issues.

**Effectiveness of BDLoss.** To demonstrate the effect of BDLoss, we train the SR model using uniform sampling and replace only the \(_{1}\) loss with \(_{bd}\). As shown for method "+\(_{bd}\) " in Table 2, the PSNR

    &  &  \\  & PSNR \(\) & SSIM\(\) & \#FLOPs (G) & PSNR\(\) & SSIM\(\) & \#FLOPs (G) \\  SRResNet  & 26.19 & 0.7624 & 5.19 (100\%) & 27.65 & 0.7966 & 5.19 (100\%) \\ +HES & 26.24 & 0.7665 & 3.58 (69\%) & 27.71 & 0.7986 & 3.43 (66\%) \\ +\(_{bd}\) & 26.21 & 0.7658 & 3.58 (69\%) & 27.70 & 0.7984 & 3.43 (66\%) \\ +WBSR & 26.26 & 0.7673 & 3.37 (65\%) & 27.73 & 0.7993 & 3.22 (62\%) \\ +WBSR\(\) & 26.38 & 0.7684 & 5.19 (100\%) & 27.80 & 0.8026 & 5.19 (100\%) \\  RCAN  & 26.39 & 0.7706 & 32.60 (100\%) & 27.89 & 0.8058 & 32.60 (100\%) \\ +HES & 26.43 & 0.7748 & 20.86 (64\%) & 27.92 & 0.8086 & 19.89 (61\%) \\ +\(_{bd}\) & 26.42 & 0.7746 & 20.86 (64\%) & 27.91 & 0.8077 & 19.89 (61\%) \\ +WBSR & 26.45 & 0.7755 & 18.52 (57\%) & 27.94 & 0.8106 & 19.40 (59\%) \\ +WBSR\(\) & 26.51 & 0.7756 & 32.60 (100\%) & 28.10 & 0.8138 & 32.60 (100\%) \\   

Table 2: Ablation studies of our WBSR on two benchmarks of \(\) 4 SR. \(\) indicates using the whole network with 100\(\%\) FLOPs for inference. The optimal and suboptimal results are highlighted.

Figure 4: Qualitative comparison results of our method with other methods for \(\) 4 SR on the four testing datasets. Please zoom in for details.

improved by balanced training is 0.06 dB with an average 65\(\%\) computing cost compared to the baseline model, which demonstrates the superiority of our BDLoss.

**Effectiveness of Joint Training.** When applying joint training of HES and BDLoss within WBSR to the baseline network, we can further improve the PSNR and SSIM results by 0.13 dB and 0.0043, respectively, which achieve an overall performance improvement with average \(66\%\) calculation. Furthermore, to fully demonstrate the effectiveness of our WBSR, we adopt the weight-balancing framework to retrain the full baseline model instead of the dynamic supernet model. It can be seen from the "+WBSR"method of Table 2, our WBSR with a computational cost comparable to baseline obtains average PSNR and SSIM gains of 0.33 dB and 0.0078, respectively. Models trained using our WBSR show consistent performance improvements that are not affected by the skewness of the training sample distribution. In Figure 5, the SR performance on rare samples obtains gains, while the performance on abundant samples remains the same or slightly decreases, which proves that our weight-balancing strategy not only enhances the learning of texture areas and reduces redundant computation of flat areas. Additional experimental results are placed in supplementary materials.

## 6 Conclusion

In this paper, we rethink the imbalance problem in image SR from a statistical probability perspective and propose a plug-and-play Weight-Balancing framework (WBSR) to achieve balanced model learning without changing the original model structure and training data. Specifically, to tackle the imbalance problem of data distribution, we propose a Hierarchical Equalization Sampling strategy (HES) to enhance the model's capability to extract features from difficult samples to mitigate inherent data biases. Then, to solve the imbalance problem of model optimization, we propose a Balanced Diversity Loss (BDLoss) function to focus on learning texture regions and ignore redundant computations in those smooth regions. After joint training of HES and BDLoss, our WBSR rectifies the imbalance to achieve accurate and efficient inference via a gradient projection dynamic inference strategy. Extensive qualitative and quantitative experiments across various models, datasets, and scaling factors demonstrate that our method achieves comparable or superior performance to existing approaches with less computational cost.

## 7 Acknowledgements

This work was supported in part by the National Natural Science Foundation of China under Grants 62272134 and 62072141, in part by the National Science and Technology Major Project under Grant 2021ZD0110901.

Figure 5: Illustration of the gain of our weight-balancing framework relative to the baseline model and its weight rectification diagram.

    &  &  \\  & PSNR\(\) & SSIM\(\) & \#FLOPs (G) & PSNR\(\) & SSIM\(\) & \#FLOPs (G) \\  RCAN & 27.40 & 0.7306 & 32.60 (100\%) & 25.54 & 0.7684 & 32.60 (100\%) \\ +BSPA  & 27.54 & 0.7348 & 32.60 (100\%) & 26.02 & 0.7839 & 32.60 (100\%) \\ +SamplingAug  & 27.47 & 0.7323 & 32.60 (100\%) & 25.80 & 0.7771 & 32.60 (100\%) \\ +DDA  & 27.51 & - & 32.60 (100\%) & 25.89 & - & 32.60 (100\%) \\  +HES & 27.73 & 0.7388 & 32.60 (100\%) & 26.04 & 0.7863 & 32.60 (100\%) \\ +WBSR\({}^{}\) & 27.81 & 0.7402 & 32.60 (100\%) & 26.10 & 0.7889 & 32.60 (100\%) \\ +WBSR & 27.77 & 0.7391 & 26.41 (81\%) & 26.03 & 0.7850 & 29.67 (91\%) \\   

Table 3: Quantitative comparison results of our method with other sampling strategies.