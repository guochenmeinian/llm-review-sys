# Turbulence in Focus:

Benchmarking Scaling Behavior of 3D Volumetric

Super-Resolution with BLASTNet 2.0 Data

 Wai Tong Chung\({}^{*,1}\), Bassem Akoush\({}^{1}\), Pushan Sharma\({}^{1}\), Alex Tamkin\({}^{1}\),

**Ki Sung Jung\({}^{2}\), Jacqueline H. Chen\({}^{2}\), Jack Guo\({}^{1}\), Davy Brouzet\({}^{1}\), Mohsen Talei\({}^{3}\), Bruno Savard\({}^{4}\), Alexei Y. Poludnenko\({}^{5}\), Matthias Ihme\({}^{*,1,6}\)**

\({}^{1}\)Stanford University, \({}^{2}\)Sandia National Laboratory, \({}^{3}\)University of Melbourne,

\({}^{4}\)Polytechnique Montreal, \({}^{5}\)University of Connecticut, \({}^{6}\)SLAC National Accelerator Laboratory

Corresponding authors: {wtchung,mihme}@stanford.edu

###### Abstract

Analysis of compressible turbulent flows is essential for applications related to propulsion, energy generation, and the environment. Here, we present **BLASTNet 2.0**, a **2.2 TB** network-of-datasets containing **744 full-domain samples** from **34 high-fidelity direct numerical simulations**, which addresses the current limited availability of 3D high-fidelity reacting and non-reacting compressible turbulent flow simulation data. With this data, we benchmark a total of **49 variations** of five deep learning approaches for 3D super-resolution - which can be applied for improving scientific imaging, simulations, turbulence models, as well as in computer vision applications. We perform neural scaling analysis on these models to examine the performance of different machine learning (ML) approaches, including two scientific ML techniques. We demonstrate that (i) predictive performance can scale with model size and cost, (ii) architecture matters significantly, especially for smaller models, and (iii) the benefits of physics-based losses can persist with increasing model size. The outcomes of this benchmark study are anticipated to offer insights that can aid the design of 3D super-resolution models, especially for turbulence models, while this data is expected to foster ML methods for a broad range of flow physics applications. This data is publicly available with download links and browsing tools consolidated at https://blastnet.github.io.

## 1 Introduction

In recent years, machine learning (ML) has offered new modeling approaches for natural and engineering sciences. For example, 5 PB of ERA5 data  was employed towards GraphCast , an ML model that outperformed conventional weather modeling techniques. As such, efforts in curating large datasets for scientific ML have been growing across numerous domains including agricultural science , geophysics , and biology . In many of these examples, the volume of data can significantly exceed the free limit of open-source repositories such as Kaggle (100 GB) - typically used to store and share language and image data - due to potentially high dimensions within scientific data. As such, significant resources are required for building and maintaining data storage capabilities, either through institutional collaborations [3; 5; 6] or cloud service providers .

In previous work [8; 9], we proposed the Bearable Large Accessible Scientific Training Network-of-datasets (BLASTNet), a cost-effective community-driven weakly centralized framework (see Section 3.1) that utilizes Kaggle for increasing access to scientific data, which provided access to 110 full-domain samples from 10 configurations (225 GB) of 3D high-fidelity flow physics directnumerical simulations (DNS). In this work, we present the updated BLASTNet 2.0 dataset, which is extended to 744 full-domain samples from 34 DNS configurations, as shown in Figure 1. This dataset aims to address limitations in data availability for compressible turbulent non-reacting and reacting flows, which is found in automotive [10; 11], propulsion [12; 13], energy [14; 15], and environmental [16; 17] applications. BLASTNet data has previously been employed for solving ML tasks related to dimensionality reduction, regime classification, and turbulence-chemistry closure modeling . Beyond this, BLASTNet is potentially suited for ML problems involving predictions of physical quantities found in turbulent non-reacting and reacting flows, which can also involve inverse problems [18; 19] and physics discovery [20; 21].

In this work, we demonstrate the utility of BLASTNet data for 3D super-resolution (SR) of turbulent flows. From BLASTNet 2.0, we pre-process DNS data to form the Momentum128 3D SR dataset for benchmarking this task. To this end, we:

* Curate BLASTNet 2.0, a diverse public 3D compressible turbulent flow DNS dataset.
* Benchmark performance and cost of five 3D ML approaches [22; 23; 24; 19; 25] for SR with this publicly accessible dataset.
* Show that SR model performance can scale with the logarithm of model size and cost.
* Demonstrate the persisting benefits of a popular physics-based gradient loss term  with increasing model size.

We provide an overview of related efforts in Section 2. In Section 3, we provide information on the BLASTNet 2.0 and Momentum128 3D SR datasets. Our benchmark setup is described in Section 4, with results discussed in Section 5, before the conclusions in Section 6.

## 2 Related Work

Flow Physics Simulation DatasetsNumerical simulations accurately describe flow physics, as long as the simulation grid resolves the smallest lengthscales associated with turbulent dissipation . With up to \((10^{9})\) voxels, \((10^{6})\) core-hours of simulation time, and \((10^{4})\) cores on parallel computing facilities [27; 28; 29; 30], high-fidelity DNS of many real-world flows cannot be performed

Figure 1: Summary of this study.

due to prohibitive costs. Thus, it is common to employ coarser grids with large-eddy simulations (LES) [15; 31], or by only evolving time-/ensemble-averaged quantities with Reynolds-averaged Navier-Stokes (RANS) simulations [32; 33] - both of which rely on turbulence models that can be discovered from DNS data. DNS data is also useful for applications involving scientific imaging , spatio-temporal modeling , and solving inverse problems . As shown in Table 1, many existing flow simulation datasets focus on LES and RANS simulations due to data storage constraints. McConkey et al.  released a dataset for improving turbulence models in incompressible non-reacting RANS. AirfRANS  provides both 2D incompressible and compressible non-reacting RANS data, specifically on airfoil configurations. For reacting flows, Huang  released a 2D LES dataset for developing reduced-order models. The largest flow physics dataset, the Johns Hopkins Turbulence Database (JHTDB) , provides 3D DNS data from turbulent incompressible non-reacting flow simulations. Since these datasets are either 2D, incompressible, or non-reacting, they are not suitable for many applications involving aerodynamics, propulsion, or chemical processes. This is one reason why ML studies involving these applications employ self-generated inaccessible datasets [37; 38] - introducing challenges to open model evaluation. To address these gaps, we curate BLASTNet 2.0, a 3D turbulent compressible reacting and non-reacting flow dataset, which targets a balance of diversity, fidelity, and size.

SR for Turbulent FlowsWithin experimental flow measurements, deep learning-based SR approaches have been employed towards improving Schlieren , particle-image-velocimetry , and tomography  techniques, which can often be limited by resolution restrictions. In many practical engineering analyses, coarse-grained simulations with under-resolved grid sizes are often used to bypass the extensive costs of fully-resolved DNS. A dominant source of error from this approach involves missing physics that arise from the under-resolved grid, as will be detailed in Section 4.2. While algebraic turbulence models have been traditionally used to represent the under-resolved physics [43; 44], specific algebraic models are effective only in specific flow configurations . SR and related upsampling approaches have been proposed as a versatile alternative for correcting under-resolved information from coarse-grid simulations between numerical time-stepping [37; 46; 47], which would require considerations of real-time inferencing. Studies on turbulent SR have focused on demonstrating feasibility [37; 41; 46; 47; 48], mostly by modifying existing image SR models. Due to memory constraints, many of these studies focus on 2D configurations [41; 46; 48], with 3D SR investigations only demonstrated recently [37; 47]. As such, there has not yet been a detailed and reproducible benchmark study comparing SR models for 3D turbulent flows [38; 49].

    &  &  &  McCConkey \\ et al.  \\  } &  &  \\    & **2.0** & 1.0  &  AirfRANs  \\  } &  \\  Size [TB] & **2.2** & 0.225 & 490 & 0.014 & 0.066 & 0.209 \\ Configs. & **34** & 10 & 10 & 29 & 1,000 & 1 \\ Full-domain Samples & **744** & 120 & 15,791 & 29 & 1,000 & 30,000 \\  Fidelity &  & DNS & RANS & RANS & LES \\ Spatial Dimensions & **3D** & 3D & 2D,3D & 2D & 2D & 2D \\ Primitive Variables2  & \(u_{i}\),\(p\),\(\),\(b_{i}\) & \( u_{i}\),\( p\) & \( u_{i}\),\( p\) & \( T\),\(\) & \(_{i}\),\(\),\(\),\(\),\(_{k}\) \\ Number of PDEs3  & **5 to 27** & 4 to 7 & 3 to 4 & 4 to 5 & 7 \\ Compressible Flow4  & **Yes** & No & No & Yes5  & **Yes** \\ Multi-physics6  & **Reactions6  & **MHD7 [FOOTNOTE:7]
Table 1: Comparison of BLASTNet 2.0 (in **bold**) with selected flow simulation datasets.

Scientific ML ApproachesScientific ML approaches can involve developing custom architectures with implicit biases that suit specific problems and modifying loss functions with constraints related to governing equations . Firstly, model architectures such as NUNet , MeshGraphNet , and Fourier Neural Operators (FNO)  employ graph and spectral convolution layers to ensure that flow predictions are mesh invariant. While FNOs have been successfully employed towards canonical laminar flow configurations, previous studies have demonstrated that these models are insufficiently expressive for complex configurations, involving turbulent  and multi-physics  flows. This has been attributed to regularization properties of spectral convolution layers by the FNO's original developers . Recently, convolution FNO (Conv-FNO)  models have been proposed to ameliorate these underfitting issues by embedding convolutional blocks within FNO blocks. Secondly, modifying the loss functions can involve adding a regularization term based on the residuals of the entire governing equations . A softer approach involves regularizing with individual operators within the governing equation (such as continuity, advection, diffusion, and source terms) . In this work, we benchmark models based on both loss function and architecture approaches, _i.e._, a model with a gradient-based loss function  (which biases ML optimization towards turbulence applications; see Appendix F.3) and a Conv-FNO model, respectively.

SR BenchmarksSR via deep learning has been subjected to numerous competition and benchmark studies that target various aspects of 2D image SR, including 2K-images , night photography , spectral recovery , and satellite images . For 3D SR, benchmarks for video , medical resonance imaging , and 3D microscopy applications  have been performed. Another method for studying the behavior of deep learning models involves the construction of empirical scaling relationships . This type of analysis is useful for studying resource requirements of large language models, and was briefly been employed for studying 2D SR with a U-Net model . Given the potential real-time computing applications of turbulent SR, this analysis provides useful information on the relationship between model size, cost, and predictive performance.

## 3 Dataset

### BLASTNet 2.0

BLASTNet 2.0 consists of turbulent compressible flow DNS data, on Cartesian spatial grids, generated by solving governing equations for mass, momentum, energy, and chemical species, respectively:

\[_{t}+() =0\;,\] (1a) \[_{t}()+( ) =- p+\;,\] (1b) \[_{t}( e^{t})+[( e^{t}+p)] =-+[()]\;,\] (1c) \[_{t}( Y_{k})+(Y_{k}) =-_{k}+_{k}\;,\] (1d)

with density \(\), velocity vector \(\), pressure \(p\), specific total energy \(e^{t}\), stress tensor \(\), and heat flux \(\). \(Y_{k}\), \(_{k}\), and \(_{k}\) are the mass fraction, diffusion flux, and source term for chemical species \(k\;=\;[1,N_{s}-1]\), where \(N_{s}\) is the number of species. Molecular fluxes are typically modeled using the mixture-averaged diffusion model.

The BLASTNet 2.0 dataset is developed with these properties in mind:

FidelityAll DNS data is collected from well-established numerical solvers  with spatial discretization schemes ranging from 2nd- to 8th-order accuracy, while time-advancement accuracy range from 2nd- to 4th-order. Low-order schemes require finer discretizations compared to high-order schemes, to achieve similar accuracy and numerical stability . However, all simulations are spatially resolved to the order of the Kolmogorov lengthscale, ranging from 3.9 to 41 \(\)m depending on the configuration, with a corresponding temporal discretization that ensures numerical stability.

Size and DiversityBLASTNet 2.0 contains a total of 744 full-domain samples (2.2 TB) from a diverse collection of 34 simulation configurations: non-reacting decaying homogeneous isotropic turbulence (HIT) , reacting forced HIT , two parametric variations of reacting jet flows , six configurations of non-reacting transcritical channel flows , a reacting channel flow , a partially-premixed slot burner configuration , and 22 parametric variations (with different turbulent and chemical timescales) of a freely-propagating flame configuration .

Community-involvementBLASTNet 2.0 consists of data contributions from six different institutions. As mentioned in Appendix C, our long-term vision and maintenance plan for this dataset involves seeking additional contributions from members of the broader flow community.

Cost-effective Storage, Distribution, and BrowsingTo circumvent Kaggle storage constraints, we partition the data into a network of \(<100\) GB subsets, with each subset containing a separate simulation configuration. This partitioned data can then be uploaded as separate datasets on Kaggle. To consolidate access to this data, all Kaggle download links are presented in https://blastnet.github.io, with the inclusion of a bash script for downloading all data through the Kaggle API. In addition, Kaggle notebooks are attached to each subset to enable convenient data browsing on Kaggle's cloud computing platform. This approach enables cost-effective distribution of scientific data that adheres to FAIR principles , as further detailed in Appendix C.

Consistent FormatData, generated from different numerical solvers, initially exists in a range of formats (.vtk,.vtu,.tec, and.dat) that are not readily formatted for training ML models. Thus, all flowfield data are processed into a consistent format - little-endian single-precision binaries that can be read with np.fromfile/np.memmap. The choice of this data format enables high I/O speed in loading arrays. We provide.json files that store additional information on configurations, chemical mechanisms and transport properties. See Appendix D for more details.

Licensing and EthicsAll data is generated by the present authors and licensed via CC BY-NC-SA 4.0. Other than the contributors' names and institutions, no personal-identifiable information is published in this data. No offensive content is published with this flow physics dataset. Further discussion on negative impact is provided in Section 6.

### Momentum128 3D SR Dataset

BLASTNet 2.0 is further processed for training due to constraints in (i) memory and (ii) grid properties. Currently, the single largest sample (92 GB) in BLASTNet 2.0 contains 1.3B voxels and 15 channels, which cannot fit into typical GPU memory. In addition, the spatial grid is stretched depending on the resolution requirements of the flow domain. As shown in Figure 1, we circumvent these two issues by sampling \(128^{3}\) sub-volumes of density \(\) and velocity \(\) from the uniform-grid regions from all BLASTNet data. This results in 12,750 sub-volume samples (427 GB). We choose this sub-volume size to enable 32\(\) SR (the resulting feature sub-volume is \(4^{3}\) which is larger than a kernel size of 3), while maintaining a low memory footprint. In order to develop a compressible turbulence benchmark dataset that can be easily downloaded, we select 2,000 sub-volumes to form a 67 GB dataset that can fit into a single Kaggle repository. To ensure that these 2,000 samples are representative of the different flows encountered in each configuration, we:

1. Extract mean, variance, skewness, and kurtosis (statistical moments for characterizing turbulence ) from the three velocity components of 12,750 sub-volumes.
2. Apply k-means clustering with the elbow method (using the statistical moments as features) to partition the sub-volumes in 18 clusters.
3. Select 2,000 samples while ensuring that the proportion of clusters are well-balanced.

The resulting sub-volumes form the labels of BLASTNet Momentum128 3D SR dataset. Figure 2 demonstrate the mean and standard deviation of the specific kinetic energy:

\[ e^{k}=({u_{1}}^{2}+{u_{2}}^{2}+{u_{3}}^{2})/2\,,\] (2)

which we use to characterize all channel variables. Each distinct marker represents a different simulation configuration. Since flows from the same configuration possess similar statistics, the different configurations from BLASTNet 2.0 can result in a dataset with a variety of flow conditions.

Figure 2: Statistics of the specific kinetic energy \( e^{k}\) of each 128\({}^{3}\) sub-volume in the Momentum128 3D SR dataset. Marker type represents DNS configuration. See detailed legend in Appendix E.1.5

Due to stochastic and chaotic nature of turbulence , it is not possible to obtain matching pairs of coarse (also known as implicit LES ) and fine DNS data. Thus, we employ a canonical method [37; 45; 73] for obtaining implicit LES surrogates, known as finite-volume optimal LES  (see Appendix E.1.6 on their validity). Specifically, we Favre-filter  and downsample the labels by 8, 16, and 32\(\) to obtain a representative range of coarse resolution samples (LES is typically an order of magnitude coarser than DNS [26; 75]) to generate inputs for turbulent SR:

\[=V_{f}}_{V_{f}}\,dV_{f}\] (3)

where \(\) denotes a uniform-filtered quantity, \(\) is a Favre-filtered quantity, and \(V_{f}\) is a subvolume with the size of the filter width. In our SR dataset, the channels of each label correspond to \(=\{\),\(u_{1}\),\(u_{2}\),\(u_{3}\}\), while the feature channels consist of \(_{f}=\{\),\(_{1}\),\(_{2}\),\(_{3}\}\). For the purpose of the present benchmark study, we further split the 2,000 sub-volumes as follows:

Train, Validation, and Baseline Test Sets80:10:10 split via random selection with a uniform distribution. The training set contains 1,382 samples, and both validation and baseline test sets contain 173 samples each.

Parametric Variation SetA 144-sample subset for model evaluation from an unseen parametric variation configuration with approximately \(15\%\) higher mean velocities and velocity fluctuations than the train, validation, and baseline test sets.

Forced HIT SetA 128-sample subset for model evaluation from an unseen flow type (forced HIT) with 30-fold higher pressure and 34-fold lower velocity fluctuations.

## 4 Benchmark Configuration

### Models and Methods

As shown in Figure 1, three well-studied 2D ResNet-based  SR models are modified from their original repositories for 3D SR: (i) Residual-in-Residual Dense Block (RRDB) , (ii) Enhanced Deep Residual Super-resolution (EDSR) , and (iii) Residual Channel Attention Networks (RCAN) . Convolution networks possess inductive biases that are suitable for problems involving spatial grids such as in flow physics [50; 77]. We choose to study these models due to their differences in architecture paradigms. Specifically, RRDB employs residual layers within residual layers; EDSR features an expanded network width; RCAN utilizes long skip connections and channel attention mechanisms. In addition, we consider two additional scientific ML approaches: (i) a Conv-FNO model , modified for SR (see details in Appendix F.2), and (ii) an RRDB model regularized with the weighted MSE of the gradient of channel variables \(^{2}_{k=1}^{3}[()_{k},()_ {k}]\) to the loss \((1-)(,)\), where \(\) is the distance between each voxel. Details on the gradient-based loss and its weighting factor \(\) are provided in Appendix F.3. We compare model predictions with a baseline approach, _i.e._, tricubic interpolation. To investigate the scaling behavior of the model architectures, we vary the number of parameters by changing the network depth and width.

Similar to other turbulent SR studies [37; 46], all models are trained with mean-squared-error (MSE) loss, unless otherwise stated. For evaluation, we select models with the best MSE after training for 1,500 epochs with a batch size of 64 across 16 Nvidia V100 GPUs. Learning rate is initialized at le-4, and halved every 300 epochs. Both the number of training iterations and learning scheduling are chosen to match other SR studies [22; 23; 24] and are found to be sufficient for the SR predictions, as will be shown in Section 5. All other hyperparameters are maintained from their original studies, with He initialization  used on all initial model weights. Data augmentation is performed via variants of random rotation and flip - modified to ensure that augmented data remains consistent with continuity (Equation (1a)). Training is performed with automatic mixed-precision from Lightning 1.6.5 . Prior to training, data is normalized with means and standard deviations of density and velocity extracted from the train set. During evaluation, this normalization resulted in poor accuracy for the Forced HIT set, due to the significantly different magnitudes of density and velocity. However, Section 5 will show that good performance can be achieved when normalization is performed with the mean and standard deviation of each distinct evaluation set. Thus, all evaluation sets are normalizedwith their own mean and standard deviation, prior to testing. All 40 model variations are trained with three different seeds, resulting in a total computational cost of approximately 15,000 GPU-hours on the Lassen Supercomputer . Further information on model hyperparameters, data augmentation, training, normalization, as well as links to code and model weights are found in Appendix A, F, and G.

### Metrics

We compare the performance of each model by examining local and global quantities of each sample. For the local quantities, we employ \(=\{,\}\), where SSIM is the 3D extension  of the structural similarity image measure  and NRMSE is the normalized root-mean-squared error (see Appendix F.1). For quantities with multiple channels:

\[_{,} [(,)+_{i=1} ^{3}(_{i},u_{i})]\,,\] (4a) \[_{sgs} _{k=1}^{3}[(}^{})_{k},(^{ })_{k}]\,.\] (4b)

with \(\) denoting an arbitrary predicted quantity. SSIM is a common image metric, but has also become a popular ML metric for evaluating flow simulations due to its employment of mean, variance, and covariance quantities - suited for evaluating the statistical nature of turbulence . In addition, this metric is intuitive for both readers familiar and unfamiliar with turbulent flows - SSIM of 0 denotes dissimilar fields while SSIM of 1 denotes highly similar fields. \(_{,}\) evaluates each channel of the predictions via macro-averaging. To measure the suitability of SR for turbulence modeling in coarse-grid simulations, we measure \(_{sgs}\), which evaluates the predicted divergence of the subgrid-scale (SGS) stress \(^{}\). \(^{}\) represents physics information lost during coarse-graining, and originates from the Favre-filtered/LES momentum equation (Equation (1b)):

\[_{t}(})+ (}})=- +(}+^{ })\,,\] \[^{}=} }-}})\,.\] (5)

We evaluate global physical properties of ML predictions by considering the \(_{\{E^{k},\}}\) of turbulent dissipation rate \(\) (rate of conversion of turbulent kinetic energy to heat) and volume-averaged kinetic energy \(E^{k}\) (momentum component in energy conservation of a fixed control volume):

\[E^{k} =}_{V} e^{k}\,dV\,,\] (6a) \[ =_{V}}{} \,dV\,,\] (6b)

with sample volume \(V\) and velocity fluctuation \(^{}\).

## 5 Experiment Results

We summarize SSIMs of RRDB, EDSR, RCAN, and Conv-FNO in Table 2, along with model parameters \(N_{p}\) and inferencing cost (in FLOPs for a batch size of 1; see Appendix F.6 for details). The 8\(\) SR models shown here possess the best SSIMs across different sizes for a given model approach, as shown in Appendix G.1. Models with the same network depth and width, are then initialized and trained for 16 and 32\(\) SR. For 8 and 16\(\) SR, RRDB (with gradient loss) performs the best across most of the metrics and evaluation sets, with RCAN demonstrating the highest SSIM\({}_{,}\) at 8\(\) SR. At 32\(\) SR, all shown models exhibit lower SSIM\({}_{,}\) than tricubic interpolation in the baseline test set, indicating that SR is difficult to learn at high ratios. However, all models exhibit higher SSIM\({}_{sgs}\) than tricubic interpolation for all SR ratios. This indicates that SR models may still be useful for turbulence modeling at high SR ratios. Figure 3, demonstrates that model predictions of specific kinetic energy \( e^{k}\) (Equation (2)) (a physical quantity that combines predictions of all four channels) from all models presented in Table 2 increasingly lose fine turbulent structures as SR ratio increase. Nevertheless, when compared to tricubic interpolation, the SR models can still recover the magnitudes of the energy at these SR ratios. Further examination of the NRMSE metrics from the 8\(\) SR models (see Appendix G.1 for 16, 32\(\) SR), in Table 3, also demonstrates that all ML models significantly outperform tricubic interpolation on baseline test and forced HIT sets. Here, gradient loss RRDB performs best in most of the metrics. However, EDSR outperforms with NRMSE\({}_{E^{k}}\), as the gradient loss only offers minor improvements to \(E^{k}\).

Scaling behavior of RRDB is shown in Figure 4, which compares ground truth and input values of \( e^{k}\) (shown in the first column) with \(8\) SR predictions from tricubic interpolation and variations of RRDB models. For the model predictions, the first row visualizes the specific kinetic energy \(^{k}\), while the second row shows the error \(|_{ e^{k}}|=|^{k}- e^{k}|\) normalized by \( e^{k}_{max}\). Our discussion

    &  &  &  &  &  \\   & \(\$SSIM$_{,}\) & \(\$SSIM$_{gg}\) & \(\$SSIM$_{,}\) & \(\$SSIM$_{gg}\) & \(\$SSIM$_{,}\) & \(\$SSIM$_{gg}\) & \(N_{p}\) & \(\)GFLOPs \\  Tricubic 8\(\) & 0.820 & 0.431 & 0.800 & 0.418 & 0.951 & 0.711 & \(-\) & **23** \\ RRDB 8\(\) & 0.907\(\)0.003 & 0.715\(\)0.004 & 0.898\(\)0.003 & 0.755\(\)0.002 & 0.997\(\)0.000 & 0.891\(\)0.003 & \(50.2\)M & 1430 \\ (+ Grad. Loss) & **0.936\(\)0.003** & **0.802\(\)0.003** & **0.929\(\)0.001** & **0.825\(\)0.001** & **0.998\(\)0.000** & **0.944\(\)0.005** & 50.2M & 1430 \\ EDSR 8\(\) & 0.928\(\)0.004 & 0.748\(\)0.012 & 0.916\(\)0.005 & 0.775\(\)0.010 & 0.999\(\)0.000 & 0.937\(\)0.005 & 34.6M & 2122 \\ RCAN 8\(\) & 0.928\(\)0.000 & 0.753\(\)0.002 & 0.916\(\)0.001 & 0.778\(\)0.001 & **0.999\(\)0.000** & 0.941\(\)0.003 & 16.4M & 671 \\ Conv-FNO 8\(\) & 0.846\(\)0.016 & 0.566\(\)0.019 & 0.845\(\)0.011 & 0.614\(\)0.015 & 0.993\(\)0.001 & 0.845\(\)0.008 & 33.0M & 1276 \\  Tricubic 16\(\) & 0.652 & 0.175 & 0.620 & 0.173 & 0.876 & 0.432 & \(-\) & **23** \\ RRDB 16\(\) & 0.724\(\)0.001 & 0.506\(\)0.004 & 0.700\(\)0.001 & 0.512\(\)0.002 & 0.971\(\)0.000 & 0.805\(\)0.003 & 1074 \\ (+ Grad. Loss) & **0.739\(\)0.008** & **0.554\(\)0.001** & **0.719\(\)0.004** & **0.556\(\)0.002** & **0.973\(\)0.000** & **0.816\(\)0.001** & 50.3M & 1074 \\ EDSR 16\(\) & 0.716\(\)0.005 & 0.447\(\)0.018 & 0.693\(\)0.005 & 0.481\(\)0.019 & 0.969\(\)0.001 & 0.783\(\)0.008 & 37.8M & 1944 \\ RCAN 16\(\) & 0.672\(\)0.039 & 0.408\(\)0.066 & 0.665\(\)0.024 & 0.415\(\)0.058 & 0.961\(\)0.009 & 0.737\(\)0.050 & 17.3M & 573 \\ Conv-FNO 16\(\) & 0.629\(\)0.020 & 0.343\(\)0.027 & 0.640\(\)0.013 & 0.355\(\)0.022 & 0.951\(\)0.006 & 0.690\(\)0.022 & 34.6M & 1068 \\  Tricubic 32\(\) & **0.508** & 0.060 & 0.476 & 0.087 & 0.758 & 0.156 & \(-\) & **23** \\ RRDB 32\(\) & 0.503\(\)0.001 & **0.194\(\)0.005** & 0.482\(\)0.000 & 0.186\(\)0.006 & 0.845\(\)0.001 & 0.494\(\)0.011 & 50.4M & 1030 \\ (+ Grad. Loss) & 0.505\(\)0.001 & 0.184\(\)0.009 & **0.483\(\)0.001** & **0.188\(\)0.002** & **0.855\(\)0.000** & **0.516\(\)0.012** & 50.4M & 1030 \\ EDSR 32\(\) & 0.502\(\)0.004 & 0.173\(\)0.006 & 0.481\(\)0.002 & 0.187\(\)0.004 & 0.845\(\)0.001 & 0.463\(\)0.005 & 40.9M & 1921 \\ RCAN 32\(\) & 0.473\(\)0.006 & 0is focused on the predictions in the cyan box. At \(N_{p}=0.6\)M, RRDB is unable to reconstruct \( e^{k}\) accurately. RRDB's prediction is more accurate than tricubic interpolation at \(N_{p}=4.9\)M, but spurious structures that originate from the coarse grid can be seen. For \(N_{p}=50.2\)M, the model is sufficiently expressive for eliminating the spurious structures from the flow. The addition of the gradient loss term is shown to reduce prediction errors from RRDB 50.2M. This trend in improvement is also visible in the bottom row, which shows the mean divergence of SGS stresses (Equation (5)).

Scaling behavior of RRDB (with and without gradient loss), EDSR, RCAN, and Conv-FNO models are examined in Figure 5. SSIM\({}_{sgs}\) scales differently compared to SSIM\({}_{,}\), indicating the importance of evaluating derived physical quantities from model predictions in flow physics applications. For both SSIMs across all evaluation sets, RCAN models demonstrate better performance than EDSR and vanilla RRDB models for \(N_{p}<17\)M, but performance deteriorates after this model size. The gradient loss term improves RRDB predictions for all model parameters explored, resulting in SSIM\({}_{sgs}\) that exceeds RCAN after \(N_{p}=1.4\)M for the baseline test and Parametric Variation sets. Thus, this loss term is shown to benefit moderately sized models (\(N_{p}=50.2\)M) and data (67 GB), which is in contrast to the notion that physics-based losses are mostly helpful for small models and datasets . Conv-FNO is seen to outperform the baseline tricubic prediction after approximately 20M parameters. FNO layers are memory-intensive due to high number of dimensions found in the spectral convolution weights (six in total: one for batches, two for channels, and three for Fourier modes). This memory-intensive nature has been acknowledged by FNO's original developers, with attempts to address this remaining an active research pursuit .

Figure 4: Predictions from various RRDB models, showing gradual improvement in the cyan box.

Figure 5: Scaling behavior of RRDB (with and without gradient-based loss), EDSR, RCAN and Conv-FNO. RRDB, EDSR and Conv-FNO models continue to scale at large model sizes.

For all models, both SSIMs are found to scale with \(_{10}N_{p}\). All ResNet-based models share similar slopes in the scaling relationship between SSIM\({}_{sgs}\) and log\({}_{10}N_{p}\) in the test and Parametric Variation set. However, these slopes can differ when evaluated on another flow configuration. This is seen with the idealized flows in the Forced HIT set, where higher SSIMs from all predictions and baseline are observed.

Figure 6 shows the relationship between SSIM\({}_{sgs}\) and inference cost (in FLOPs) for the five model approaches. SSIM\({}_{sgs}\) for EDSR, RCAN, and RRDB (with gradient loss) models scales with cost in a similar fashion, after approximately 100 GFLOPs. A steeper scaling relationship is observed for both Conv-FNO and vanilla RRDB. Vanilla RRDB models also do not demonstrate a strong linear relationship with log\({}_{10}\) GFLOPs when tested on the Forced HIT set.

## 6 Conclusion

In this work, we released BLASTNet 2.0, a public 3D compressible turbulent reacting and non-reacting flow dataset. From this data, we extracted the Momentum128 3D SR dataset, which we employed for benchmarking 3D SR models at 8, 16, and 32\(\) SR. SR models are shown to score well in SSIM-based metrics and capture fine turbulent structures at 8\(\) SR. For the higher SR ratios, these fine structures cannot be captured, but the SR models can still recover the magnitude of large flow structures. Through our scaling analysis, we demonstrate that benefits from a gradient-based physics-based loss persist with model scale - providing empirical evidence that disagrees with the postulated notion that physics-based methods are useful mostly in small model scenarios . However, we recognize that this observation is applicable only to one type of physics-based ML technique, and is not necessarily extendable to other physics-based ML approaches. We observe that model performance scales with the logarithm of model parameters, and that the scaling relationship between SSIM\({}_{sgs}\) and inference cost are similar for RRDB (with gradient loss), EDSR, and RCAN. We also demonstrate that the choice of model architecture can matter significantly, especially when developing small models for real-time scientific computing applications, and that physics-based losses can improve some metrics of poorly performing architectures. With this work, we demonstrate that BLASTNet 2.0 can provide a rich resource for evaluating models for scientific and engineering turbulent flows.

Limitations and Negative ImpactThe simple geometry, skeletal finite-rate mechanisms and mixture-averaged transport used in these DNS provide high-fidelity information of fundamental processes, but are not fully representative of real-world systems. However, rectifying this would require complex geometry, detailed mechanisms (introducing an order of magnitude more PDEs) and multi-component transport that can result in intractable calculations . Another limitation is that the DNS data originate from proprietary-licensed numerical solvers (see Appendix D.2), resulting in data that cannot be thoroughly inspected through open-source means. However, expertise, peer review from published research, and solver reputation ensure that these DNS data meet community-accepted standards. In this work, we are limited to using Favre-filtered (Equation (3)) DNS to generate low-resolution inputs in the Momentum128 3D SR dataset, as it is not feasible to obtain pairs of coarse simulation sample that matches a corresponding ground truth DNS sample, as the chaotic nature of turbulence will result in uncorrelated pairs . However, Favre-filtered DNS is a canonical surrogate  for coarse simulations with strong theoretical foundations . Further discussion on the validity of employing Favre-filtered DNS is provided in Appendix E.1.6. Data generation incurs up to \((10^{6})\) CPU-hours per case, while this study used 15,000 GPU-hours - resulting in significant carbon emissions. However, we attempted to ameliorate this by curating previously unreleased already-generated DNS from existing publications, and employing mixed-precision training for this study. In addition, this work can improve fundamental knowledge on carbon-free combustion, which can reduce society-wide reliance on hydrocarbons.

Figure 6: Scaling behavior with cost.