# Activity Grammars for Temporal Action Segmentation

Dayoung Gong1  Joonseok Lee1  Deunsol Jung  Suha Kwak  Minsu Cho

Pohang University of Science and Technology (POSTECH)

{dayoung.gong, jameslee, deunsol.jung, suha.kwak, mcho}@postech.ac.kr

http://cvlab.postech.ac.kr/research/KARI

Equal contribution.

###### Abstract

Sequence prediction on temporal data requires the ability to understand compositional structures of multi-level semantics beyond individual and contextual properties. The task of temporal action segmentation, which aims at translating an untrimmed activity video into a sequence of action segments, remains challenging for this reason. This paper addresses the problem by introducing an effective activity grammar to guide neural predictions for temporal action segmentation. We propose a novel grammar induction algorithm that extracts a powerful context-free grammar from action sequence data. We also develop an efficient generalized parser that transforms frame-level probability distributions into a reliable sequence of actions according to the induced grammar with recursive rules. Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence prediction and discover its compositional structure. Experimental results demonstrate that our method significantly improves temporal action segmentation in terms of both performance and interpretability on two standard benchmarks, Breakfast and 50 Salads.

## 1 Introduction

Human activities in videos do not proceed by accident; they are structured being subject to generative rules imposed by the goal of activities, the properties of individual actions, the physical environment, and so on. Comprehending such a compositional structure of multi-granular semantics in human activity poses a significant challenge in video understanding research. The task of temporal action segmentation, which aims at translating an untrimmed activity video into a sequence of action segments, remains challenging due to the reason. The recent methods based on deep neural networks  have shown remarkable improvement in learning temporal relations of actions in an implicit manner, but often face out-of-context errors that reveal the lack of capacity to capture the intricate structures of human activity, and the scarcity of annotated data exacerbates the issue in training. In this work, we address the problem by introducing an effective activity grammar to guide neural predictions for temporal action segmentation.

Grammar is a natural and powerful way of explicitly representing the hierarchical structure of languages  and can also be applied to express the structure of activities. Despite the extensive body of grammar-based research for video understanding , none of these approaches have successfully integrated recursive rules. Recursive rules are indispensable for expressing complex and realistic structures found in action phrases and activities. To achieve this, we introduce a novel activity grammar induction algorithm, _Key-Action-based Recursive Induction_ (KARI), that extracts a powerful probabilistic context-free grammar while capturing the characteristics of the activity. Since an activity is composed of multiple actions, each activity exhibits a distinctive temporal structure based on pivotal actions, setting it apart from other activities. The proposed grammar inductionenables recursive rules with flexible temporal orders, which leads to powerful generalization capability. We also propose a novel activity grammar evaluation framework to evaluate the generalization and discrimination power of the proposed grammar induction algorithm. To incorporate the induced activity grammar into temporal action segmentation, we develop an effective parser, dubbed BEP, which searches the optimal rules according to the classification outputs generated by an off-the-shelf action segmentation model. Our approach can be combined with any neural network for temporal action segmentation to enhance the sequence prediction and discover its compositional structure.

The main contribution of this paper can be summarized as follows:

* We introduce a novel grammar induction algorithm that extracts a powerful context-free grammar with recursive rules based on key actions and temporal dependencies.
* We develop an effective parser that efficiently handles recursive rules of context-free grammar by using Breadth-first search and pruning.
* We propose a new grammar evaluation framework to assess the generalization and discrimination capabilities of the induced activity grammars.
* We show that the proposed method significantly improves the performance of temporal action segmentation models, as demonstrated through a comprehensive evaluation on two benchmarks, Breakfast and 50 Salads.

## 2 Related work

**Grammar for activity analysis.** Grammar is an essential tool to represent the compositional structure of language  and has been mainly studied in the context of natural language processing (NLP) [21; 22; 20; 37]. Grammar has been extensively studied in various research areas [28; 29; 32; 8; 43; 13; 42; 11; 27; 12]. Similarly, a grammatical framework can be used to express the structure of activities. Several work [23; 24; 33; 35] have defined context-free grammars based on possible temporal transitions between actions for action detection and recognition. Vo and Bovick  propose a stochastic grammar to model a hierarchical representation of activity based on AND-rules and OR-rules. Richard et al.  propose a context-free grammar defined on action sequences for weakly-supervised temporal action segmentation. Qi et al. [30; 32; 31] utilize a grammar induction algorithm named ADIOS  to induce grammar from action corpus. However, none of the proposed grammar for activity analysis includes recursive rules, which are a fundamental factor in expressing repetitions of actions or action phrases. In this paper, we propose a novel action grammar for temporal action segmentation based on key action and temporal dependency between actions considering recursive temporal structure.

**Temporal action segmentation (TAS).** Various methods have been proposed to address the task. Early work utilizes temporal sliding windows [36; 19] to detect action segments, and language-based methods [24; 23] has been proposed to utilize a temporal hierarchy of actions during segmentation. Recently, a deep-learning-based model named the temporal convolutional networks (TCN) has been proposed with an encoder-decoder architecture [25; 9]. Moreover, transformer-based models [45; 2] are recently introduced to leverage global temporal relations between actions based on self-attention and cross-attention mechanisms . Other researches have been proposed to improve the accuracy of temporal action segmentation based on existing models [9; 45]. Huang _et al._ introduce a network module named Graph-based Temporal Reasoning Module (GTRM) that is applied on top of baseline models to learn temporal relations of action segments. Ishikawa _et al._ suggest an action segment refinement framework (ASRF) dividing a task into frame-wise action segmentation and boundary regression. They refine frame-level classification results with the predicted action boundaries. Gao _et al._ propose a global-to-local search scheme to find appropriate receptive field combinations instead of heuristic respective fields. Ahn and Lee  recently propose a hierarchical action segmentation refiner (HASR), which refines segmentation results by applying multi-granular context information from videos. A fast approximate inference method named FIFA for temporal action segmentation and alignment instead of dynamic programming is proposed by Souri _et al._. Other researches [5; 6] reformulate TAS as a cross-domain problem with different domains of spatio-temporal variations, introducing self-supervised temporal domain adaptation. Xu _et al._ proposes differentiable temporal logic (DTL), which is a model-agnostic framework to give temporal constraints to neural networks. In this paper, we propose a neuro-symbolic approach where the activity grammar induced by the proposed grammar induction algorithm guides a temporal action segmentation model to refine segmental errors through parsing.

## 3 Our approach

Given a video of \(T\) frames \(=[F_{1},F_{2},...,F_{T}]\) and a predefined set of action classes \(\), the goal of temporal action segmentation is to translate the video into a sequence of actions \(=[a_{1},a_{2},...,a_{N}]\) and their associated frame lengths \(=[l_{1},l_{2},...,l_{N}]\) where \(N\) is unknown, \(a_{i}\) for \(1 i N\), \(a_{i} a_{i+1}\) for \(1 i N-1\), and \(_{i=1}^{N}l_{i}=T\).2 The resultant output of \(\) and \(\) indicates that the video consists of \(N\) segments and each pair \((a_{i},l_{i})\) represents the action and length of \(i_{ th}\) segment.

In this work, we introduce an activity grammar that guides neural predictions for temporal action segmentation through parsing. We propose a novel activity grammar induction algorithm named KARI and an efficient parser called BEP. The overall pipeline of the proposed method consists of three steps, as illustrated in Fig. 1. First of all, KARI induces an activity grammar from action sequences in the training data. Using the KARI-induced grammar, BEP then takes the frame-level class prediction \(^{T||}\) from the off-the-shelf temporal action segmentation model [45; 9] and produces a grammar-consistent action sequence \(^{*}\). Finally, segmentation optimization is performed to obtain optimal action lengths \(^{*}\) based on \(^{*}\) and \(\). In the following, we introduce the activity grammar as a probabilistic context-free grammar (Section 3.1), present KARI (Section 3.2) and BEP (Section 3.3), and describe a segmentation optimization method for final outputs (Section 3.4).

### Activity grammar

We define the _activity grammar_ as a probabilistic context-free grammar (PCFG) , designed to derive diverse action sequences pertaining to a specific activity class. The activity grammar, denoted as \(G=(,,,S)\), follows the conventional PCFG which consists of four components: a finite set of variables \(\), a finite set of terminals \(\), a finite set of production rules \(\), and the start symbol \(S\). In our context, the set of terminals \(\) becomes the set of action classes \(\), and the production rules \(\) are used to generate action sequences from the start variable \(S\). We use two types of production rules, 'AND' and 'OR', defined as follows:

\[: V \,V\,\,( )^{*},\] (1) \[: V V_{1}[p_{1}]|\,V_{2}[p_{2} ]\,|\,|\,V_{n}[p_{n}]. \,V,V_{1},...,V_{n}.\] (2)

The AND rule replaces a head variable \(V\) with a sequence of variables and terminals \(\), determining the order of the terminals and variables. In contrast, the OR rule converts a head variable \(V\) to a sub-variable \(V_{i}\) with the probability \(p_{i}\), providing multiple alternatives for replacement; '!' denotes 'OR' operation. These two types of rules allow us to generate action sequences hierarchically.

### Grammar induction: Key-Action-based Recursive Induction (KARI)

Grammar induction refers to the process of learning grammars from data . In our context, it takes action sequences of a specific activity in the training set and produces an activity grammar that is

Figure 1: **Overall pipeline of the proposed method.** (a) KARI induces an activity grammar \(G\) from action sequences in the training data, (b) BEP parses neural predictions \(\) from the off-the-shelf temporal action segmentation model given a video \(\) by using the KARI-induced grammar \(G\), and (c) the final output of optimal action sequences and lengths \((^{*},^{*})\) is achieved through segmentation optimization. It is best viewed in color.

able to parse action sequences of the activity; the induced grammar should be able to parse unseen sequences of the activity as well as the sequences in the training set for generalization. To obtain an effective activity grammar avoiding under-/over-generalization, we introduce two main concepts for grammar induction: _key action_ and _temporal dependency_.

The key actions for a specific activity are those consistently present in every action sequence from the training dataset. Specifically, the top \(N^{}\) most frequently occurring actions among these are selected as the key actions. The hyperparameters of the number of key actions \(N^{}\) affects the degree of generalization achieved by the induced grammar. The temporal dependency refers to the relevance of temporal orders across actions. Temporally independent actions do not occur in a specific temporal order. This concept of temporal dependency can also be extended to groups of actions, meaning that some groups of actions can be temporally dependent on others.

We induce an activity grammar based on the key actions and the temporal dependency. Action sequences are divided into sub-sequences using the key actions as reference points, and the temporal dependencies between actions within the sub-sequences are established; temporally dependent actions are represented using AND rules (Eq. 1), while temporally independent actions are expressed with OR rules (Eq. 2). We give an example of grammar induction in Fig. 2; four action sequences are given in Fig. 2a, where the action class 'pour coffee' is chosen as the key action with the number of key actions \(N^{}\) set to 1.

Given the action sequences from the training dataset \(\), we begin grammar induction by identifying a set of key actions \(\) with the pre-defined hyperparameter \(N^{}\). Using the key actions, each action sequence \(\) is divided into three parts: \(=[^{},^{},^{}]\). The sub-sequences \(^{},^{}\), and \(^{}\) denote the portions of the original action sequence that occurred _before_, _between_, and _after_ the key actions, respectively; the sub-sequence \(^{}\) starts from the first key action and includes up to the last key action in \(\). An example in Fig. 2b shows that the action sequence \(_{3}\) is divided into three sub-sequences using the key actions. For notational convenience, we will use the superscript \(\{,,\}\) to denote one of the three parts. All action sub-sequences \(^{}\) in a specific part \(\) are grouped to consist in a corresponding set of sub-sequences \(^{}\) (_cf._ Fig 2c). The action set \(^{}\)

Figure 2: **Example of activity grammar induction of KARI. (a) Example action sequences are provided with ‘pour coffee’ as the key action with \(N^{}\) set to 1. (b) Action sequence, _e.g._, \(_{3}\), is segmented into sub-sequences based on key actions. (c) All action sub-sequences \(^{}\) consist in a set of sub-sequences \(^{}\). (d) The action set \(^{}\) contains all the actions occurring in \(^{}\). (e) Temporally independent actions are grouped, where each action group is temporally dependent in the action group sequence \(^{}\). (f) The resultant KARI-induced activity grammar is shown. For simplicity, we omit the probability, and it is best viewed in color.**

is then defined to contain all the actions occurring in \(^{}\) (_cf._ Fig 2d). To determine the temporal dependencies among the actions of \(^{}\), pairwise temporal orders are considered as follows. If one action always occurs before the other in \(^{}\), then the two actions are temporally dependent and otherwise temporally independent. Based on the concepts, we construct the action group sequence \(^{}\) by collecting the temporally independent actions as an action group and arranging such action groups according to their temporal dependencies (_cf._ Fig 2e).

In the following, we describe how to construct the production rules \(\) of the activity grammar \(G\).

**Start rule**. We first create the rule for the start variable \(S\):

\[S V^{}\,V^{}\,V^{}\,,\] (3)

where \(V^{}\), \(V^{}\), and \(V^{}\) are variables used to derive left, middle, and right parts of the action sequence, respectively.

**Rule for the variable \(V^{}\)**. For \(V^{}\), \(\{,\}\), we construct an AND rule of action groups based on action group sequence \(^{}\):

\[V^{} V^{}_{1}\,V^{}_{2}\,\,V^{}_{|^{ }|},\] (4)

where the variable \(V^{}_{i}\) represents the \(i_{}\) action group in the action group sequence \(^{}_{i}\). Since actions in an action group are considered temporally independent, we construct an OR rule for each action group:

\[V^{}_{i} d^{}_{i,1}\,V^{}_{i}\,[p^{}_{i,1} ]|\,d^{}_{i,2}\,V^{}_{i}\,[p^{}_{i,2}]|\, |\,d^{}_{i,|^{}|}\,V_{i}\,[p^{}_{i,|^{ }|}]\,|\,\,[p^{}_{i,}]\,,\] (5)

where \(d^{}_{i,j}\) denotes the \(j_{}\) action from the action group \(^{}_{i}\). The variable \(V^{}_{i}\) yields \(d^{}_{i,j}\,V^{}_{i}\) with the probability \(p^{}_{i,j}\). This rule can be recursively used to proceed to the variable \(V^{}_{i}\) in the next step. This recursive structure allows for repeated selection of actions within the same action group, leading to the generation of diverse action sequences, which is effective for generalization. To avoid an infinite loop of the recursion, the empty string \(\) with the escape probability \(p^{}_{i,}\) is added to Eq. 5. For the details, refer to the transition probability \(p^{}_{i,j}\) and the escape probability \(p^{}_{i,}\) in Appendix A.1.

**Rule for the middle variable \(V^{}\).** Since the temporal order of key actions might vary, we consider all the possible temporal orders between key actions in \(\). A set of temporal permutations of actions is denoted as \(\), where each possible temporal permutation is represented by the OR rule:

\[V^{} V^{}_{1}\,[p^{}_{1}]\,|\,V^{} _{2}\,[p^{}_{2}]\,|\,\,|\,V^{}_{||}\,[p^{}_{}]\,|\,\,[p^{}_{}].\] (6)

The rule for the permutation variable \(V^{}_{i}\) is defined by the AND rule:

\[V^{}_{i}_{i,1}\,V^{(i,1)}\,\,_{i,| _{i}|}\,V^{(i,|_{i}|)}\,V^{}\,,\] (7)

where all the key actions are included. Note that \(_{i,j}\) represents the \(j_{}\) action of the permutation \(_{i}\), and the variable \(V^{(i,j)}\) derives action sub-sequences between actions \(_{i,j}\) and \(_{i,j+1}\). The production rule for \(V^{(i,j)}\) adheres to the rules specified in Eq. 4 and 5. The resultant KARI-induced grammar from the example is shown in Fig. 2f, highlighting the compositional structure of actions.

### Parser: Breadth-first Earley Parser (BEP)

The goal of the parser is to identify the optimal action sequence \(^{}\) by discovering the most likely grammatical structure based on the output of the action segmentation model [9; 45]. In other words, the parser examines the production rules of the activity grammar to determine whether the given neural prediction \(\) can be parsed by the grammar \(G\). However, when the grammar includes recursive rules, the existing parser struggles to complete the parsing within a reasonable time due to the significant increase in branches from the parse tree. To address this challenge, we introduce an effective parser dubbed BEP, integrating Breadth-first search (BFS) and a pruning technique into a generalized Earley parser (GEP) . Since the BFS prioritizes production rules closer to the start variable, it helps the parser understand the entire context of the activity before branching to recursive iterations. Simultaneously, pruning effectively reduces the vast search space generated by OR nodes and recursion, enabling the parser to focus on more relevant rules for the activity.

For parsing, we employ two heuristic probabilities introduced in  to compute the probability of variables and terminals within the parse tree. Specifically, let \(_{t,x}\) denote the probability of frame being labeled as \(x\). In this context, we denote the last action in the action sequence \(\) as \(x\), _i.e._\(x=a_{N}\), where \(=[a_{1},a_{2},...,a_{N}]\), for simplicity. The transition probability \(g(x\,|\,_{1:N-1},G)\) determines the probability of parsing action \(x\) given the \(_{1:N-1}\) and the grammar \(G\).

The parsing probability \(p(_{1:T}\,|\,G)\) computes the probability of \(\) being the action sequence for \(_{1:T}\). The probability at \(t=1\) is initialized by:

\[p(F_{1}\,|\,G)=g(x\,|\,,G)\,_{1,x }&$ contains only $x$},\\ 0&\] (8)

where \(\) indicates an empty string.

Since we assume that the last action of \(\) is classified as \(x\), the parsing probability \(p(_{1:t}\,|\,G)\) can be represented with the probability of the previous frames:

\[p(_{1:t}\,|\,G)=_{t,x}(p(_{1:t-1} \,|\,G)+g(x\,|\,_{1:N-1},G)\,p(_{1:t-1} _{1:N-1}\,|\,G)\,).\] (9)

The prefix probability \(p(_{1:T}...\,|\,G)\) represents the probability of \(\) being the prefix of \(^{*}\). This probability is computed by measuring the probability that \(\) is the action sequence for the frame \(_{1:t}\) with \(t\) in the range \([1,T]\):

\[p(_{1:T}...\,|\,G)=p(F_{1}\,|\,G)+g (x\,|\,_{1:N-1},G)_{t=2}^{T}_{t,x}\,p(_{1:t-1} _{1:N-1}\,|\,G).\] (10)

The parsing operation is structured following the original Earley parser , consisting of three key operations: prediction, scanning, and completion. These operations involve the update and generation of states, where every state comprises the rule being processed, the parent state, the parsed action sequence denoted as \(\), and the prefix probability denoted as \(p(...)\). The states are enqueued and prioritized by their depth \(d\) within the parse tree.

* **Prediction**: for every state \(Q(m,n,d)\) of the form \((A B,Q(i,j,k),,p(...))\), add \((B,Q(m,n,d),,p(...))\) to \(Q(m,n,d+1)\) for every production rule in the grammar with \(B\) on the left-hand side.
* **Scanning**: for every state in \(Q(m,n,d)\) of the form \((A w,Q(i,j,k),,p(...))\), append the new terminal \(w\) to \(\) and compute the probability \(p((+w)...)\). Create a new set \(Q(m+1,n^{},d)\) where \(n^{}\) is the current size of \(Q(m+1)\). Add \((A w,Q(i,j,k),+w,p((+w)...))\) to \(Q(m+1,n^{},d)\).
* **Completion**: for every state in \(Q(m,n,d)\) of the form \((A Q,(i,j,k),,p(...))\), find states in \(Q(i,j,k)\) of the form \((B A,Q(i^{},j^{},k^{}),^{ },p(^{}...))\) and add \((B A,Q(i^{},j^{},k^{}),,p( ...))\) to \(Q(m,n,d-1)\).

The symbols \(\), \(\), and \(\) represent arbitrary strings consisting of terminals and variables, _i.e._\(,,( V)^{*}\). The symbols \(A\) and \(B\) refer to the variables, while \(w\) denotes a single terminal. The symbol \(Q\) represents the set of states, and the dot (\(\)) denotes the current position of the parser within the production rule.

Additionally, we introduce a pruning technique of limiting the queue size to reduce the vast search space in the parse tree, similar to the beam search. Specifically, the parser preserves only the top \(N^{ queue}\) elements from the queue in order of the parsing probability of each state. The parsing process terminates when the parser identifies that the parsed action sequence \(^{*}\) has a higher parsing probability than the prefix probabilities of any other states in the queue. For the further details, refer to Appendix B.

### Segmentation optimization

The objective of segmentation optimization is to determine the optimal alignment between the input classification probability matrix \(\) and the action sequence \(^{*}\). In other words, the entire frames are allocated within the action sequences \(^{*}=a_{1}^{*},a_{2}^{*},...,a_{N}^{*}\), obtained from the parser, to determine the optimal action lengths \(^{*}=[l_{1}^{*},l_{2}^{*},...,l_{N}^{*}]\). In this work, we utilize dynamic programming-based Viterbi-like algorithm  for activity parsing. Similar to [26; 32], the optimizer explores all possible allocations and selects the one with the maximum product of probabilities:

\[^{*}=_{}(p(\,|\,^{*},_{1:T})),\] (11)

\[p(\,|\,,_{1:t})=_{i<t}(p(_{1:N-1}\,|\,_{1:N- 1},_{1:i})_{j=i}^{t}_{j,a_{N}}).\] (12)

## 4 Experimental evaluation and analysis

### Datasets and evaluation metrics

**Datasets.** We conduct experiments on two widely used benchmark datasets for temporal action segmentation: Breakfast  and 50 Salads . The Breakfast dataset, consisting of 1,712 videos, involves 52 individuals preparing 10 different breakfast activities comprised of 48 actions in 18 different kitchens. Similarly, the 50 Salads dataset comprises 50 egocentric videos of people preparing salads of a single activity with 17 fine-grained actions from 25 people. We used I3D  features provided by .

**Evaluation metrics.** For evaluation metrics, we report edit score, F1@\(\{10,25,50\}\) scores, and frame-wise accuracy following the previous work [9; 45].

### Implementation details

For KARI, we set the hyperparameters of the number of key actions \(N^{}\) to 4 for Breakfast, and 3 for 50 Salads. We individually induce separate activity grammar for the ten activity classes within Breakfast and subsequently merge them into a unified grammar. For the comparison with the existing grammar used in the previous work [32; 31], we induce activity grammars of ADIOS  provided by . Two types of ADIOS-induced grammar are induced: ADIOS-AND-induced grammar, primarily composed of AND rules with limited generalization capabilities, and ADIOS-OR-induced grammar, predominantly incorporating OR rules, offering improved generalization. Please refer to Appendix C.1 for grammar induction details.

For BEP, we configured the queue size \(N^{}\) to be 20. For efficiency, we adjust the sampling rate of the input video features to 50 for Breakfast and 100 for 50 Salads. We use two widely used models for the temporal action segmentation: ASFormer  based on Transformer and MS-TCN  based on CNNs. Since we apply the proposed method to the reproduced temporal action segmentation models, we directly compare and evaluate the performance based on the reproduced results.

### Evaluation framework for activity grammar

We propose a novel evaluation framework to assess the generalization and discrimination capabilities of the activity grammar. Figure 3 shows the overall process of the grammar evaluation framework. We first generate a set of synthetic activity grammars \(^{}\) randomly. Action sequences \(_{i}^{}\) are generated from each synthetic grammar \(G^{}_{i}^{}\), and these sequences are randomly divided into two sets: seen _seen_ and _unseen_. For each seen set, a grammar induction algorithm is applied, resulting in the induced grammar \(G^{}_{i}\) consisting in a corresponding set of induced grammars \(^{}\). For grammar evaluation, the induced grammar \(G^{}_{i}^{}\) parses action sequences from the entire unseen sets. The induced grammar should accurately parse the action sequences generated by the original synthetic grammar from which it was induced, while also effectively discriminating those generated by other synthetic grammars.

To simulate real-world video action sequences, we generate the synthetic activity grammars assuming temporal dependencies across actions. This indicates that certain actions follow a temporal order

   grammar & precision & recall \\  ADIOS-AND & 0.97 & 0.08 \\ ADIOS-OR & **0.99** & 0.25 \\ KARI (ours) & 0.93 & **0.98** \\   

Table 1: **Synthetic \(G\) I**

Figure 4: **Confusion matrix of activity grammars. The results of KARI-induced grammar are similar to the synthetic grammar, showing high recall with comparable precision.**

Figure 3: **Grammar evaluation**

[MISSING_PAGE_FAIL:8]

[MISSING_PAGE_EMPTY:9]

and 'add saltnpepper', (blue bar in Fig. 4(a)), which are omitted in the results obtained by using the ADIOS-OR induced grammar. The results show that KARI-induced grammar allows a more flexible temporal structure between actions. Furthermore, our method effectively removes actions such as 'put pancake2plate' that do not correspond to the intended activity. Similarly, qualitative results on 50 Salads in Fig. 4(b) show the effectiveness of the proposed method with complex action sequences. The overall results show that activity grammar-based refinement for the temporal action segmentation model is effective for correcting the neural predictions by using the grammar as a guide.

## 5 Conclusion

We have shown that the proposed approach enhances the sequence prediction and discovers its compositional structure, significantly improving temporal action segmentation in terms of both performance and interpretability. However, the improvement is limited by the initial output of the action segmentation network, which remains further research in the future. We believe that the grammar induction and parsing methods can be easily applied to other sequence prediction tasks.

Figure 5: **Qualitative results.** KARI-induced grammar efficiently insert missing actions and removes out-of-context actions in ASFormer .

    & scrambled & pancake & salad & fried egg & juice & coffee & sandwich & cereal & milk & tea & total \\  & egg (11.9) & (11.1) & (9.9) & (9.5) & (7.2) & (6.7) & (6.0) & (5.1) & (5.0) & (5.0) & (7.7) \\  Kuehne _et al._ & 0.25 & 0.24 & 0.0 & 0.32 & 0.53 & 0.80 & 0.63 & 0.96 & 0.78 & 0.91 & 0.53 \\ Richard _et al._ & 0.25 & 0.24 & 0.0 & 0.32 & 0.53 & 0.80 & 0.63 & 0.96 & 0.78 & 0.91 & 0.54 \\ ADIOS-AND  & 0.25 & 0.24 & 0.0 & 0.32 & 0.53 & 0.80 & 0.63 & 0.96 & 0.78 & 0.91 & 0.54 \\ ADIOS-OR  & 0.39 & 0.30 & 0.37 & 0.53 & 0.55 & 0.80 & 0.73 & 0.96 & 0.78 & 0.92 & 0.63 \\ KARI & **0.84** & **0.71** & **0.90** & **0.70** & **0.77** & **1.00** & **0.91** & **0.96** & **0.90** & **0.98** & **0.87** \\   

Table 8: **Grammar evaluation on real data.** We evaluate the proposed KARI-induced-grammar on Breakfast, demonstrating the superior high recall on unseen action sequences from each activity. The average length of action sequences of each activity is shown in parentheses.

Acknowledgements

This work was supported by the IITP grants (2022-0-00264: Comprehensive video understanding and generation with knowledge-based deep logic (\(50\%\)), 2022-0-00290: Visual intelligence for space-time understanding and generation based on multi-layered visual common sense (\(20\%\)), 2022-0-00959: Few-shot learning of causal inference in vision and language (\(20\%\)), and 2019-0-01906: AI graduate school program at POSTECH (\(10\%\))) funded by the Korea government (MSIT).