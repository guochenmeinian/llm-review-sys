# Transferable Boltzmann Generators

Leon Klein

Freie Universitat Berlin

leon.klein@fu-berlin.de &Frank Noe

Microsoft Research AI4Science

Freie Universitat Berlin

Rice University

franknoe@microsoft.com

###### Abstract

The generation of equilibrium samples of molecular systems has been a long-standing problem in statistical physics. Boltzmann Generators are a generative machine learning method that addresses this issue by learning a transformation via a normalizing flow from a simple prior distribution to the target Boltzmann distribution of interest. Recently, flow matching has been employed to train Boltzmann Generators for small molecular systems in Cartesian coordinates. We extend this work and propose a first framework for Boltzmann Generators that are transferable across chemical space, such that they predict zero-shot Boltzmann distributions for test molecules without being retrained for these systems. These transferable Boltzmann Generators allow approximate sampling from the target distribution of unseen systems, as well as efficient reweighting to the target Boltzmann distribution. The transferability of the proposed framework is evaluated on dipeptides, where we show that it generalizes efficiently to unseen systems. Furthermore, we demonstrate that our proposed architecture enhances the efficiency of Boltzmann Generators trained on single molecular systems.

## 1 Introduction

Generative models have demonstrated remarkable success in the physical sciences, including protein structure prediction , generation of de novo molecules , and efficiently generating samples from the Boltzmann distribution . In this work, we will focus on the latter for molecular systems, which represents a promising avenue for addressing the sampling problem. The sampling problem refers to the long-standing challenge in statistical physics to generate samples from equilibrium Boltzmann distributions \((x)(-U(x)/k_{B}T)\), where \(U(x)\) is the potential energy of the system, \(k_{B}\) the Boltzmann constant, and \(T\) the temperature. Traditionally, samples are generated with sequential sampling algorithms such as Markov Chain Monte Carlo and Molecular Dynamics (MD) simulations. However, these algorithms require a significant amount of time to generate uncorrelated samples from the target distribution. This is due to the necessity of performing small update steps, in the order of femtoseconds, for stability. This is especially challenging in the presence of well-separated metastable states, where transitions are unlikely due to high energy barriers. In recent years, numerous machine learning methods have emerged to address this challenge . One such method is the Boltzmann Generators (BG) . In this work, we refer to BGs as a model that allows for the approximate sampling of the Boltzmann distribution of interest and the subsequent reweighting to the unbiased target distribution. If the model is only capable of generating approximate samples, which may stem from a subset of the Boltzmann distribution, we refer to them as Boltzmann Emulators1. Boltzmann Generators transform a typically simple prior distribution into an approximation of the target Boltzmann distribution through a normalizing flow . Once generated, samples can be reweighted to align with the unbiased targetdistribution. The effectiveness of this reweighting hinges on how closely the generated distribution approximates the target. As a result, it is possible to obtain uncorrelated and unbiased samples from the target Boltzmann distribution, potentially achieving significant speed-ups compared to classical MD simulations.

There are numerous ways to construct a Boltzmann Generator due to the variety of realizations of normalizing flows available. In this work, we concentrate on continuous normalizing flows (CNFs) [16; 17], as opposed to coupling flows . Recently, flow matching [19; 20; 21; 22] has emerged as an alternative training method for CNFs. This approach is simulation-free, enabling more efficient training of CNFs.

Thus far, Boltzmann Generators have been limited by the requirement to train them on the specific system of interest. This training process demands a significant amount of time, making it difficult to achieve substantial speed-ups over classical MD simulations. Consequently, there is a strong desire for a transferable Boltzmann Generator that can be trained on one set of molecules and effectively generalize to another, enabling efficient generation of Boltzmann samples at inference time without the need for retraining. Recently, reliable Boltzmann Generators in Cartesian coordinates for molecules have been introduced [23; 24], paving the way for transferable Boltzmann Generators. This advancement is particularly significant because these models do not rely on molecule-specific internal coordinate representations, which have traditionally made the construction of transferable models challenging.

In this work, we present a framework for _transferable_ Boltzmann Generators based on CNFs, enabling effective sample generation from previously unseen Boltzmann distributions. Transferable Boltzmann Generators are particularly advantageous as they do not require retraining for similar systems and can be trained on shorter trajectories that may not fully capture all metastable states.

We make the following main contributions:

1. To the best of our knowledge, we introduce the first _transferable_ Boltzmann Generator. We demonstrate its transferability on dipeptides, successfully generating unbiased samples from the Boltzmann distributions of unseen dipeptides.
2. We outline a general framework for training and sampling with transferable Boltzmann Generators based on continuous normalizing flows, which also includes the post-processing of generated samples.
3. We conduct several ablation studies to investigate the effects of different architectures, training set sizes, and biasing of the training data. The results reveal that even small training sets can suffice to train transferable Boltzmann Generators. Additionally, we compare our model with Timewarp , which employs large time steps instead of generating independent samples as our method does.

## 2 Related work

The initial work on Boltzmann Generators  has led to a great deal of subsequent research. The most common application of BGs is to generate samples from Boltzmann distributions of molecules [25; 26; 27; 28; 29; 30], as well as lattice systems [26; 31; 32; 33]. Most BGs and related methods for molecular systems require system-specific featurizations such as internal coordinates [8; 34; 26; 27; 35; 36; 30; 37; 38]. Only recently, BGs for small molecular systems in Cartesian coordinates were introduced [23; 24], using CNFs and coupling flows, respectively. Equivariant normalizing flows [39; 40; 41; 28; 23; 42] played a pivotal role in the success of Boltzmann Generators in Cartesian coordinates, not only for molecular systems. The majority of BGs employ a Gaussian prior distribution, but it is also possible to start from prior distributions close to the target distribution [43; 36; 44], which makes the learning task simpler. However, all previous Boltzmann Generators are not transferable. Arguably, the work of [7; 45] represents an exception, as they are able to generate samples from unseen conditional (Boltzmann) distributions in torsion space. However, the distribution is conditioned on a single local structure for each molecule, namely fixed bonds and angles. Consequently, in contrast to our work, they are unable to generate samples from the full Boltzmann distribution in Euclidean space. The first transferable deep generative model that was able to generate asymptotically unbiased samples from the Boltzmann distribution is . Instead of generating independent samples, they learn large time steps and combine these with MetropolisHastings acceptance steps, to ensure asymptotically unbiased samples. However, in contrast to our work, they do not generate uncorrelated samples.

Boltzmann Emulators are analogous to Boltzmann Generators, yet they are not designed to generate unbiased equilibrium samples from the target Boltzmann distribution. Instead, they are intended to generate approximate samples that do not undergo reweighting. Furthermore, the generation of all metastable states may not be a necessary requirement, depending on the system. Boltzmann Emulators do not need to be based on flow models, as they do not aim to do reweighing to the target distribution. They are often similar to Boltzmann Generators and use normalizing flows or diffusion models for the architecture, but due to removing the constraint of sampling the unbiased Boltzmann distribution, they can target significantly larger systems or are transferable. One example is , who propose a three stage transferable CNF model to learn peptide ensembles.  use flow matching to learn distributions of proteins, while  utilize diffusion models.  build a transferable Boltzmann Emulator for small molecules. Others aim to additionally also capture the correct dynamics of the molecular systems, such as , who use a diffusion model to predict transition probabilities. Scaling to larger systems often requires coarse graining [51; 52; 47], e.g. describing amino acids by a single bead rather than the individual atoms. However, this approach precludes the possibility of reweighting to the Boltzmann distribution.

A distinct, though related, learning objective is to generate novel molecular conformations. However, approximations from the Boltzmann distribution are not necessary; it is sufficient to generate a few (or even a single) conformation per molecule. The utilized architectures are once again analogous, as flow and diffusion models are employed [5; 6; 53; 54; 55; 7].

## 3 Boltzmann Generators and Normalizing Flows

Here, we describe Boltzmann Generators and normalizing flows, which are a central part of our proposed transferable Boltzmann Generator framework. We follow the notation of .

### Boltzmann Generators

Boltzmann Generators (BGs)  combine an exact likelihood deep generative model and a reweighting algorithm to reweight the generated distribution to the target Boltzmann distribution. The exact likelihood deep generative model is trained to generate samples from a distribution \((x)\) that is close to the target Boltzmann distribution \((x)\). A common choice for the exact likelihood model are normalizing flows.

The Boltzmann Generator can be used to generate unbiased samples by first sampling \(x(x)\) with the exact likelihood model and then computing corresponding importance weights \(w(x)=(x)/(x)\) for each sample. These allow to reweight generated samples to the target Boltzmann distribution \((x)\). It is possible to estimate observables of interest (asymptotically unbiased) using the weights \(w(x)\) with importance sampling via

\[ O_{}=_{x(x)}[w(x)O(x)]}{ _{x(x)}[w(x)]}.\] (1)

Furthermore, these reweighting weights can be employed to assess the efficiency of trained BGs by computing the effective sample size (ESS) with Kish's equation . In this work, we will compute the relative ESS, rather than the absolute one, and refer to it as ESS.

### Continuous Normalizing Flows (CNFs)

Normalizing flows [15; 57] are a type of deep generative model used to learn complex probability densities \((x)\) by transforming a prior distribution \(q(x)\) through an invertible transformation \(f_{}:^{n}^{n}\), resulting in the push-forward distribution \((x)\).

Continuous Normalizing Flows (CNFs) [16; 17] are a specific kind of normalizing flow. For CNFs, the invertible transformation \(f_{}^{t}(x)\) is defined by the ordinary differential equation

\[^{t}(x)}{dt}=v_{}(t,f_{}^{t}(x)), f _{}^{0}(x)=x_{0},\] (2)where \(v_{}(t,x):^{n}^{n}\) is a time-dependent vector field. The solution to this initial value problem provides the transformation equation

\[f_{}^{t}(x)=x_{0}+_{0}^{t}dt^{}v_{}(t^{},f_{ }^{t^{}}(x)),\] (3)

with \(f_{}^{1}(x)=^{t}(x)\). The corresponding change in log density from the prior to the push-forward distribution is described by the continuous change of variable equation

\[(x)= q(x)-_{0}^{1}dt v_{}(t,f_{ }^{t}(x)).\] (4)

Equivariant flowsThe energy of molecular systems is typically invariant under permutations of interchangeable particles and global rotations and translations. Consequently, it is advantageous for the push-forward distribution of a Boltzmann Generator to possess the same symmetries as the target system. In [39; 40] the authors demonstrate that the push-forward distribution \((x)\) of a permutation and rotation equivariant normalizing flow with a permutation and rotation invariant prior distribution, is again rotation and permutation invariant. Furthermore,  present a method to construct such equivariant CNFs by using an equivariant vector field \(v_{}\).

### Flow matching

Flow matching [19; 20; 21; 22] enables efficient, simulation-free training of CNFs. The conditional flow matching training objective allows for the direct training of the vector field \(v_{}(t,x)\) through

\[_{}()=_{t,x p_{t}(x|z)} ||v_{}(t,x)-u_{t}(x|z)||_{2}^{2}.\] (5)

There are many possible parametrizations for the conditional vector field \(u_{t}(x|z)\) and the conditional probability path \(p_{t}(x|z)\). One of the most simple, but powerful possible parametrization is

\[z =(x_{0},x_{1}) p(z)=q(x_{0})(x_{1})\] (6) \[u_{t}(x|z) =x_{1}-x_{0} p_{t}(x|z)=(x|t x _{1}+(1-t) x_{0},^{2}),\] (7)

which we use in this work to train our models. For a more detailed description refer to [19; 22; 23; 53].

## 4 Transferable Boltzmann Generators

This section presents our proposed framework for transferable Boltzmann Generators (TBGs).

### Architecture

Our proposed transferable Boltzmann Generator is based on a CNF. The corresponding vector field \(v_{}(t,x)\) is parametrized by an \(O(D)\)- and \(S(N)\)-equivariant graph neural network (EGNN) [41; 58; 46], as commonly used in prior work, e.g. [23; 41]. Although, less expressive than other equivariant networks such as [59; 60; 61; 62], it is faster to evaluate, which is important for CNFs as there can be hundreds of vector field calls during inference.

The vector field \(v_{}(t,x)\) consists of \(L\) consecutive layers. The position of the \(i\)-th particle \(x_{i}\) is updated according to the following equations:

\[h_{i}^{0} =(t,a_{i},b_{i},c_{i}), m_{ij}^{l}=_{e}(h_{i}^{l},h_ {j}^{l},d_{ij}^{2}),\] (8) \[x_{i}^{l+1} =x_{i}^{l}+_{j i}^{l}-x_{j}^{l})}{d_{ij}+1} _{d}(m_{ij}^{l}),\] (9) \[h_{i}^{l+1} =_{h}(h_{i}^{l},m_{i}^{l}), m_{i}^{l}=_{ j i}_{m}(m_{ij}^{l})m_{ij}^{l},\] (10) \[v_{}(t,x^{0})_{i} =x_{i}^{L}-x_{i}^{0}-_{j}^{N}(x_{j}^{L}-x_{j}^{0}),\] (11)where \(_{}\) represents different neural networks, \(d_{ij}\) is the Euclidean distance between particle \(i\) and \(j\), \(t\) is the time, \(a_{i}\) is an embedding for the particle type, \(b_{i}\) for the amino acid, and \(c_{i}\) or the amino acid position in the peptide. In the final step, the geometric center is subtracted to ensure that the center of positions is conserved. When combined with a symmetric mean-free prior distribution, the push-forward distribution of the CNF will be \(O(D)\)- and \(S(N)\)-invariant, as demonstrated in .

The embedding of each atom is constructed from three parts. The first part is the atom type \(a_{i}\), which is a one-hot vector of \(54\) classes. The classes are defined based on the atom types in the peptide topology. Therefore, only a few atoms are indistinguishable, such as hydrogen atoms that are bound to the same carbon or nitrogen atom. The second part is the amino acid to which the atom belongs, which is divided into 20 classes. The third part is the position of the amino acid in the peptide sequence. This embedding is similar to the embedding used in  for the rotamer embeddings. The amino acid and positional embeddings are only used for the transferable experiments. For more details see Appendix B.5. In this study, we refer to this transferable Boltzmann Generator architecture as _TBG + full_, and we use this name even when we apply it to a non-transferable setting.

The proposed architecture in  uses distinct encodings for all backbone atoms and the atom types for all other atoms. This represents a special case of our architecture, wherein \(b_{i}\) and \(c_{i}\) are omitted and \(a_{i}\) encodes the atom type or a backbone atom. Hence, there are \(13\) classes for \(a_{i}\). We refer to this architecture as _TBG + backbone_. Furthermore, we refer to the specific architecture employed in  as BG + backbone for the alanine dipeptide experiments. Note that using only \(a_{i}\) causes problems for transferability, see Appendix A.4 for more details.

Moreover, we employ a model that utilizes the atom type as the sole encoding (there are only five distinct atom types). This model is referred to as simply _TBG_.

### Training transferable Boltzmann Generators

All transferable Boltzmann Generators utilize flow matching for training. Given the variation in peptides across batches, the flow matching loss for each peptide is normalized by the number of atoms it contains. This training procedure is applied to all different architectures. For further details, refer to Appendix B.

### Inference with transferable Boltzmann Generators

Sampling with a transferable Boltzmann Generator, especially on unseen peptides, poses multiple challenges: (i) Some generated samples may not correspond to the molecule of interest, but rather to a molecule that contains the same atoms but has a different bonding graph. Some of these configurations might even be valid molecules. For some examples see Appendix A.4. However, as we are in this work interested in sampling from the equilibrium Boltzmann distribution for a given molecular bonding graph, rather than sampling distinct molecules, we would like to avoid these cases. Nevertheless, this effect can be largely mitigated by our proposed TBG + full architecture. (ii) When working with classical force fields, the correct ordering with respect to the topology is crucial for evaluating energies. This is not a concern for semi-empirical force fields, as they respect the permutation symmetry of particles of the same type. As we typically use a Gaussian prior distribution, it is common that the generated samples are not arranged according to the topology. Consequently, in order to evaluate the energy, it is necessary to reorder the generated samples according to the topology. (iii) It is possible that the chirality of generated samples differs from that of the peptide of interest.

We resolve (i) and (ii) by generating a bond graph for the generated samples, based on empirical bond distances and atom types. This graph is then compared with a reference bond graph. If the two graphs are isomorphic, we can conclude that the configuration is correct. For more details, see Appendix B.1. For (iii), we employ the code of  to check all chiral centers. If all chirality centers of a peptide are flipped, this can be resolved by mirroring. Otherwise, these samples are assigned high energies, as they are not from the target Boltzmann distribution of interest. It is important to note that only generated samples with the correct configuration and chirality are considered valid samples from the Boltzmann distribution of interest.

## 5 Experiments

In this section, we compare our model with similar previous work on equivariant Boltzmann Generators  for alanine dipeptide. Moreover, we show the transferability of our model on dipeptides, where we compare our model with the transferable Timewarp model . More experimental details, such as dataset details, the specifics of the employed models, and the utilized computing infrastructure can be found in Appendix B.

### Alanine dipeptide

In our first experiment, we investigate the single molecule alanine dipeptide in implicit solvent, described in Cartesian coordinates. The dataset was introduced in , for more details see Appendix B.2. The training trajectory was generated by sampling with respect to a classical force field, and subsequently, \(10^{5}\) random samples were relaxed with respect to the semi-empirical _GFN2-xTB_ force-field  for \(100\)fs each. The objective is to train a Boltzmann Generator capable of sampling from the equilibrium Boltzmann distribution defined by the semi-empirical _GFN2-xTB_ force-field efficiently and to recover the free energy surface along the slowest transition, i.e. the \(\) angle. Following the methodology outlined in , the training data is biased towards the less probable (positive) \(\) state. It is evident that any trained model on this set will be biased in comparison to the true Boltzmann distribution defined by the semi-empirical energy. However, the reweighting technique allows for the debiasing of the samples. The model is trained in the same way as described in . Overall, the likelihoods and ESS values observed for the TGB + full model are superior to those reported in  (Table 1). This is achieved with nearly the same amount of parameters and maintaining comparable training and inference times (see Appendix B.3). Furthermore, the correct free energy difference is recovered, as demonstrated in Appendix A.1.

In  the authors utilized a semi-empirical potential to avoid the required ordering of the atoms to the topology for classical force fields. As the prior distribution of the Boltzmann Generator is usually a multivariate standard Gaussian distribution, generated samples will almost certainly not have the correct ordering. As we have introduced an efficient way to reorder samples in Section 4.3, we can now also evaluate alanine dipeptide for a classical force field. Therefore, we retrain the model in  on the classical MD trajectory and compare with our TBG + full architecture. We bias the training

    & **NLL** (\(\)) & **ESS** (\(\)) \\   &  \\  BG + backbone  & \(-107.56 0.09\) & \(0.50 0.13\%\) \\ TBG + full (ours) & \(\) & \(\) \\   &  \\  BG + backbone  & \(-109.02 0.01\) & \(1.56 0.30\%\) \\ TBG + full (ours) & \(\) & \(\) \\   

Table 1: Comparison of Boltzmann Generators with different architectures for the single molecular system alanine dipeptide. Errors are computed over five runs. The results for the Boltzmann Generator and backbone encoding (BG + backbone) for the semi-empirical force field are taken from .

Figure 1: Results for the alanine dipeptide system simulated with a classical force field (a) Ramachandran plots for the biased MD distribution (left) and for samples generate with the TBG + full model (right). (b) Energies of samples generated with different methods. (c) Free energy projection along the slowest transition (\(\) angle), computed with different methods.

data as before towards the unlikely \(\) state. As expected, the likelihood and ESS for the classical force field are much better than for the semi-empirical one, as the training data stems from the target distribution. Our proposed architecture again performs significantly better, as shown in Table 1 and Figure 1. The majority of generated samples with the TBG + full model and the BG + backbone sample nearly exclusively correct configurations, i.e. configurations with the correct bond graph, namely nearly \(100\%\) and about \(98\%\), respectively. As presented in Figure 1, both models recover the free energy landscape correctly.

### Dipeptides (2AA)

In our second experiment, we evaluate our model on dipeptides and show transferability. The dataset was introduced in . The training set consists of \(200\) dipeptides, which were simulated each with a

Figure 3: Results for the GN dipeptide (a) Sample generated with the TBG + full model (b) Ramachandran plot for the weighted MD distribution (left) and for samples generate with the TBG + full model (right). (c) TICA plot for the weighted MD distribution (left) and for samples generate with the TBG + full model (right). (d) Energies of samples generated with different methods and architectures. (e) Free energy projection along the \(\) angle. (f) Free energy projection along the slowest transition (TIC0).

Figure 2: Results for the KS dipeptide (a) Sample generated with the TBG + full model (b) Ramachandran plot for the weighted MD distribution (left) and for samples generate with the TBG + full model (right). (c) TICA plot for the weighted MD distribution (left) and for samples generate with the TBG + full model (right). (d) Energies of samples generated with different methods and architectures. (e) Free energy projection along the \(\) angle. (f) Free energy projection along the slowest transition (TIC0).

classical force field for \(50\) ns and, therefore, may not have reached convergence. Nevertheless, as previously demonstrated, it is not necessary to train on unbiased data in order to obtain unbiased samples with a Boltzmann Generator.

We compare the three different transferable architectures described in Section 4.1 and use the same training procedure for all of them. Similar to the alanine dipeptide experiments, we obtain significantly better results for the TBG + full model in terms of ESS (Table 2 and Figure 4a), energies (Figure 2d), the ratio of correct configurations (Table 2), and likelihoods of test set samples (Appendix A.5). In particular, the extremely low number of correct configurations for numerous test peptides for the TBG and TBG + backbone models renders them unsuitable as Boltzmann Generators for this setting (Table 2 and Appendix A.5). Furthermore, the TBG + full model always finds all metastable states for unseen test peptides (see also Appendix A.5).

The results for the well-performing TBG + full model are presented for two example peptides from the test set in Figure 2 and Figure 3. They were chosen as all architectures sample relevant amounts of valid configurations. Detailed results for other evaluated test peptides are shown in Appendix A.5.

The TBG + full model is an exemplary Boltzmann Emulator, as it is capable of capturing all metastable states of the target Boltzmann distribution (Figure 2b,c and Figure 3b,c). However, it is furthermore also a capable Boltzmann Generator, as it allows for efficient reweighting (Figure 2d,e,f and Figure 3d,e,f) with good ESSs (Table 2). To identify different metastable states, we employ time-lagged independent component analysis (TICA) , a dimensionality reduction technique that separates metastable states. We show this analysis in addition to the Ramachandran plots for the dihedral angles.

Moreover, we investigate the influence of the training set in two ablation studies.

Training on a biased training setOur alanine dipeptide results as well as  indicate that it can be advantageous to bias the training data towards states that are less probable, such as positive \(\) states, to recover free energy landscapes. Therefore, we bias the training data by weighting positive \(\) states for each training peptide, such that they have nearly equal weight to the negative states (see also Appendix B.4). We show that a TBG + full model trained on this dataset (TBG + full (biased)) produces even more accurate free energy landscapes for both the Ramachandran and TICA projections (Figure 4bc). Notably, the unweighted projection shows a clear bias, as expected. However, as the training data is now biased, the effective sample size (ESS) is generally lower (Table 2 and Figure 4a).

Training on a smaller training setAdditionally, we examine the impact of smaller training sets on the generalization results. To this end, we train the TBG + full model on two smaller datasets with shorter simulation times: (i) \(5\)ns for each training simulation and (ii) only \(500\)ps of each training simulation. Consequently, the training trajectories are \(10\) times and \(100\) times smaller than before. As we utilize only the initial portion of each trajectory, a greater number of metastable states are missed during the brief simulations, as illustrated in Appendix A.2. Training on the tenfold smaller trainings set, we refer to the model as TBG + full (smaller), shows slightly worse results compared to training on the whole trainings set (Table 2 and Appendix A.5). The even smaller trainings set leads to inferior results, with several metastable states being missed as presented inAppendix A.3.

Figure 4: (a) Effective samples sizes (ESS) for the first 8 test peptides for different transferable architectures and training sets. (b) Free energy projection along the \(\) angle for the TBG + full model trained on the _biased_ dataset for the KS dipeptide. The weighted free energy projection demonstrates a superior fit compared to the TBG + full model (see Figure 2e). (c) Free energy projection along the \(\) angle for the TBG + full model trained on the _biased_ dataset for the GN dipeptide. The weighted free energy projection demonstrates a superior fit compared to the TBG + full model (see Figure 3e).

Nevertheless, these findings show that transferable Boltzmann Generators can be effectively trained using very small datasets, even when individual trajectories lack metastable states.

### Comparison with Timewarp 

In this section, we compare our TBG model with the Timewarp model  on the dipeptide dataset. Unlike our approach, which generates independent samples, the Timewarp model predicts large time steps, which can be cobined with Metropolis-Hastings acceptance steps to ensure asymptotically unbiased sampling. Additionally, Timewarp employs a coupling flow rather than a continuous flow. Similar to our TBG + full model, the Timewarp model also treats most particles as distinguishable. It does so by conditioning the learned probability on the current state, implicitly capturing the topology information.

We compare the Wasserstein distance between the generated Ramachandran plot and the MD Ramachandran plot for different computational budgets for dipeptides: (a) 30,000 energy evaluations, (b) 12 hours, and (c) 24 hours of wall-clock simulation time on an A-100 GPU. As shown in Figure 5 the TBG + full model outperformed the Timewarp model across all budgets, particularly in terms of energy evaluations. Energy evaluations are especially critical, as they become a major computational bottleneck with more complex force fields. Moreover, the unweighted samples of the TBG + full model have lower energies, and are therefore closer to the actual target Boltzmann distribution than samples generated with Timewarp without Metropolis-Hastings acceptance (exploration mode). Additional results and details can be found in Appendix A.7.

## 6 Discussion

For the first time, we demonstrated the feasibility of training _transferable_ Boltzmann Generators. We introduced a general framework for training and evaluating transferable Boltzmann Generators based on continuous normalizing flows. Furthermore, we developed a transferable architecture based on equivariant graph neural networks and demonstrated the importance of including topology information in the architecture to enable efficient generalization to unseen, but similar systems. The transferable Boltzmann Generator was evaluated on dipeptides, where significant effective sample sizes were demonstrated on unseen test peptides and accurate sampling of physical properties, such

   Model &  &  \\  & Mean & Range & Mean & Range \\  TBG & \(0.48 0.59\%\) & \((0.0\%,1.47\%)\) & \(13 18\%\) & (1\%,48\%) \\ TBG + backbone & \(0.58 1.04\%\) & \((0.0\%,3.24\%)\) & \(17 21\%\) & \((1\%,52\%)\) \\ TBG + full & \(\) & \(\) & \(\) & \(\) \\ TBG + full (smaller) & \(6.13 3.13\%\) & \((1.93\%,11.16\%)\) & \(96 3\%\) & \((88\%,100\%)\) \\ TBG + full (biased) & \(\) & \(\) & \(\) & \(\) \\   

Table 2: Effective samples size and correct configuration rate for unseen dipeptides across different transferable Boltzmann Generator (TBG) architectures. The values are computed for \(8\) test dipeptides. See Appendix A.5 for more results.

Figure 5: TBG / Timewarp MCMC  sampling experiments. Wasserstein distance between the generated Ramachandran plot and the MD Ramachandran plot for different computational budgets for dipeptides. Lower is better. (a) After 30000 energy evaluations (b) After 12h wall-clock-time. (c) After 24h wall-clock-time.

as the free energy difference between metastable states, was achieved. Moreover, we have shown in ablation studies that transferable Boltzmann Generators can be extremely data efficient, with even small training trajectories being sufficient. Future research will determine whether and how transferable Boltzmann Generators can be scaled to larger systems.

## 7 Limitations / Future work

Scaling transferable Boltzmann Generators to larger systems remains for future work. Notably, this usually requires large amounts of computational resources, as e.g. shown in , where they are able to train their transferable model on tetrapeptides, but use more than \(100\) times more parameters than us. Scaling to larger systems often involves coarsegraining, which typically results in the loss of an explicit energy function. In such cases, the transferable Boltzmann Generator would effectively become a transferable Boltzmann Emulator. Depending on the specific application, samples from a distribution close to the target Boltzmann distribution may be sufficient. Our results show that unweighted samples from the TBG + full model already closely resemble the target Boltzmann distribution.

Additionally, small systems can still be highly relevant, especially when paired with more expensive force fields, such as semi-empirical or first-principles quantum-mechanical force fields. In these scenarios energy evaluations become a primary bottleneck. Boltzmann Generators are particularly advantageous in this context, as they require several orders of magnitude fewer energy evaluations compared to MD simulations or other iterative methods, such as Timewarp . One potential real world application would therefore be the simulation of small molecules with expensive force fields.

Instead of flow matching, one could use optimal transport flow matching  or equivariant optimal transport flow matching [23; 53] for training, but as indicated in  the gains for molecular systems, especially in the presence of many distinguishable particles, are small.

Throughout our work, we utilize a standard Gaussian prior distribution. However, as recently introduced, an alternative is to use a Harmonic prior distribution [66; 67], where atoms that are close in the bond graph are sampled in the vicinity of each other. Notably, We experimented with this Harmonic prior but found no significant improvements for our transferable model. This aligns with findings by , indicating that chemically informed priors do not enhance performance substantially compared to simpler uninformed priors for flow matching in molecular systems. Instead, the network architecture and inductive bias play a more crucial role. However, such priors might become more important for larger systems.

Despite conducting a series of ablation studies, we did not pursue the impact of a training set comprising a smaller number of peptides. Instead, we opted for investigating shorter trajectories. Another potential direction could be relaxing the 2AA dataset using a semi-empirical force field and training on this modified version, similar to the alanine dipeptide experiment. However, this approach incurs additional computational costs, as the entire 2AA dataset would need to be relaxed with respect to the semi-empirical force field.

We used the EGNN architecture for the vector field due to its fast evaluation capabilities. Future research could explore alternative architectures for the vector field, such as those proposed by [59; 60; 61; 62; 69; 70; 67], to determine if they improve performance and enable scaling to larger systems. We hope that our framework will facilitate the scaling of transferable Boltzmann Generators to larger systems in future research.

## 8 Broader Impact

This work represents foundational research with no immediate societal impact. However, if our method is scalable to larger, more relevant systems, it could facilitate the acceleration of drug and material discovery by replacing or enhancing MD simulations, which often play a crucial part in the process. A potential risk is that this method then might be used to identify new diseases or develop biological weapons. Another risk is the lack of a known convergence criterion, making it impossible to confirm that all potential configurations have been identified, even with an infinite number of samples. This could lead to false claims about the results, potentially affecting subsequent applications.