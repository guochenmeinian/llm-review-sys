# Iteratively Learn Diverse Strategies with State Distance Information

Wei Fu\({}^{1}\), Weihua Du\({}^{*1}\), Jingwei Li\({}^{*1}\), Sunli Chen\({}^{1}\), Jingzhao Zhang\({}^{12}\), Yi Wu\({}^{12}\)

\({}^{1}\) IIIS, Tsinghua University, \({}^{2}\) Shanghai Qi Zhi Institute

\({}^{}\) fuwth17@gmail.com, \({}^{}\)jxwuyi@gmail.com

Equal Contribution

###### Abstract

In complex reinforcement learning (RL) problems, policies with similar rewards may have substantially different behaviors. It remains a fundamental challenge to optimize rewards while also discovering as many _diverse_ strategies as possible, which can be crucial in many practical applications. Our study examines two design choices for tackling this challenge, i.e., _diversity measure_ and _computation framework_. First, we find that with existing diversity measures, visually indistinguishable policies can still yield high diversity scores. To accurately capture the behavioral difference, we propose to incorporate the state-space distance information into the diversity measure. In addition, we examine two common computation frameworks for this problem, i.e., population-based training (PBT) and iterative learning (ITR). We show that although PBT is the precise problem formulation, ITR can achieve comparable diversity scores with higher computation efficiency, leading to improved solution quality in practice. Based on our analysis, we further combine ITR with two tractable realizations of the state-distance-based diversity measures and develop a novel diversity-driven RL algorithm, _State-based Intrinsic-reward Policy Optimization_ (SIPO), with provable convergence properties. We empirically examine SIPO across three domains from robot locomotion to multi-agent games. In all of our testing environments, SIPO consistently produces strategically diverse and human-interpretable policies that cannot be discovered by existing baselines.

## 1 Introduction

A consensus in deep learning (DL) is that different local optima have similar mappings in the functional space, leading to similar losses to the global optimum . Hence, via stochastic gradient descent (SGD), most DL works only focus on the final performance without considering _which_ local optimum SGD discovers. However, in complex reinforcement learning (RL) problems, the policies associated with different local optima can exhibit significantly different behaviors . Thus, it is a fundamental problem for an RL algorithm to not only optimize rewards but also discover as many diverse strategies as possible. A pool of diversified policies can be further leveraged towards a wide range of applications, including the discovery of emergent behaviors , generating diverse dialogues , designing robust robots , and enhancing human-AI collaboration .

Obtaining diverse RL strategies requires a quantitative method for measuring the difference (i.e., _diversity_) between two policies. However, how to define such a measure remains an open challenge. Previous studies have proposed various diversity measures, such as comparing the difference between the action distributions generated by policies , computing probabilistic distances between the state occupancy of different policies , or measuring the mutual information between states andpolicy identities . However, it remains unclear which measure could produce the best empirical performance. Besides, the potential pitfalls of these measures are rarely discussed.

In addition to diversity measures, there are two common computation frameworks for discovering diverse policies, including population-based training (PBT) and iterative learning (ITR). PBT directly solves a constrained optimization problem by learning a collection of policies simultaneously, subject to policy diversity constraints [52; 38; 9]. Although PBT is perhaps the most popular framework in the existing literature, it can be computationally challenging  since the number of constraints grows quadratically with the number of policies. The alternative framework is ITR, which iteratively learns a single policy that is sufficiently different from previous policies [43; 74]. ITR is a greedy relaxation of the PBT framework and it largely simplifies the optimization problem in each iteration. However, the performance of the ITR framework has not been theoretically analyzed yet, and it is often believed that ITR can be less efficient due to its sequential nature.

We provide a comprehensive study of the two aforementioned design choices. First, we examine the limitations of existing diversity measures in a few representative scenarios, where two policies outputting very different action distributions can still lead to similar state transitions. In these scenarios, state-occupancy-based measures are not sufficient to truly reflect the underlying behavior differences of the policies either. By contrast, we observe that diversity measures based on _state distances_ can accurately capture the visual behavior differences of different policies. Therefore, we suggest that an effective diversity measure should explicitly incorporate state distance information for the best practical use. Furthermore, for the choice of computation framework, we conduct an in-depth analysis of PBT and ITR. We provide theoretical evidence that ITR, which has a simplified optimization process with fewer constraints, can discover solutions with the same reward as PBT while achieving _at least half_ of the diversity score. This finding implies that although ITR is a greedy relaxation of PBT, their optimal solutions can indeed have comparable qualities. Furthermore, note that policy optimization is much simplified in ITR, which suggests that ITR can result in much better empirical performances and should be preferred in practice.

Following our insights, we combine ITR and a state-distance-based diversity measure to develop a generic and effective algorithm, _State-based Intrinsic-reward Policy Optimization (SIPO)_, for discovering diverse RL strategies. In each iteration, we further solve this constrained optimization problem via the Lagrange method and two-timescale gradient descent ascent (GDA) . We theoretically prove that our algorithm is guaranteed to converge to a neighbor of \(\)-stationary point. Regarding the diversity measure, we provide two practical realizations, including a straightforward version based on the RBF kernel and a more general learning-based variant using Wasserstein distance.

We evaluate SIPO in three domains ranging from single-agent continuous control to multi-agent games: Humanoid locomotion , StarCraft Multi-Agent Challenge , and Google Research Football (GRF) . Our findings demonstrate that SIPO surpasses baselines in terms of population diversity score across all three domains. Remarkably, our algorithm can successfully discover 6 distinct human-interpretable strategies in the GRF 3-vs-1 scenario and 4 strategies in two 11-player GRF scenarios, namely counter-attack, and corner, without any domain-specific priors, which are beyond the capabilities of existing algorithms.

## 2 Related Work

**Diversity in RL.** It has been shown that policies trained under the same reward function can exhibit significantly different behaviors [10; 35]. Merely discovering a single high-performing solution may not suffice in various applications [12; 64; 25]. As such, the discovery of a diverse range of policies is a fundamental research problem, garnering attention over many years [44; 13; 28]. Early works are primarily based on multi-objective optimization [45; 55; 39; 47; 54], which assumes a set of reward functions is given in advance. In RL, this is also related to reward shaping [46; 2; 15; 61]. We consider learning diverse policies without any domain knowledge.

**Population-based training (PBT)** is the most popular framework for diverse solutions by jointly learning separate policies. Representative works include evolutionary computation [65; 37; 52], league training [64; 23], computing Hessian matrix  or constrained optimization with a population diversity measure [38; 73; 29; 36; 9]. An improvement is to learn a latent variable policy instead of separate ones. Prior works have incorporated different domain knowledge to design the latent code,such as action clustering , agent identities [29; 21] or prosocial level [53; 3]. The latent variable can be also learned in an unsupervised fashion, such as in DIYAN  and its variants [25; 49]. Zahavy et al.  learns diverse policies with hard constraints on rewards to ensure the derived policies are (nearly) optimal, potentially hindering policies with disparate reward scales. On the other hand, our method prioritizes diversity and fully accepts sub-optimal strategies.

**Iterative learning (ITR)** simplifies PBT by only optimizing a single policy in each iteration and forcing it to behave differently w.r.t. previously learned ones [43; 60; 74]. While some ITR works require an expensive clustering process before each iteration  or domain-specific features , we consider domain-agnostic ITR in an end-to-end fashion. Besides, Pacchiano et al.  learns a kernel-based score function to iteratively guide policy optimization. The score function is conceptually similar to SIPO-WD but is applied to a parallel setting with more restricted expressiveness power.

**Diversity Measure.** Most previous works considered diversity measures on action distribution and state occupancy. For example, measures such as Jensen-Shannon divergence  and cross-entropy  are defined over policy distributions to encourage different policies to take different actions on the same state, implicitly promoting the generation of diverse trajectories. Other measures such as maximum mean discrepancy  maximize the probability distance between the state distributions induced by two policies. However, these approaches can fail to capture meaningful behavior differences between two policies in certain scenarios, as we will discuss in Section 4.1. There also exist specialized measures, such as cross-play rewards , which are designed for cooperative multi-agent games. It is worth noting that diversity measures are closely related to exploration criteria [4; 20; 6; 27] and skill discovery [8; 32; 24], where a diversity surrogate objective is often introduced to encourage broad state coverage. However, this paper aims to explicitly discover mutually distinct policies. Our diversity measure depends on a function that computes the distance between states visited by two policies.

## 3 Preliminary

**Notation:** We consider POMDP  defined by \(=,,,r,P,O,,H\). \(\) is the state space. \(\) and \(\) are the action and observation space. \(r:\) is the reward function. \(:\) is the observation function. \(H\) is the horizon. \(P\) is the transition function. At timestep \(h\), the agent receives an observation \(o_{h}=O(s_{h})\) and outputs an action \(a_{h}\) w.r.t. its policy \(:()\). The RL objective \(J()\) is defined by \(J()=_{(s_{h},a_{h})(P,)}[_{h=1}^{H}r(s_{h},a_{h}) ].\)

The above formulation can be naturally extended to cooperative multi-agent settings, where \(\) and \(R\) correspond to the joint policy and the shared reward. We follow the standard POMDP notations for conciseness. Our method will be evaluated in both single-agent tasks and complex cooperative multi-agent scenarios. Among them, multi-agent environments encompass a notably more diverse range of potential winning strategies, and hence offer an apt platform for assessing the effectiveness of our method. Moreover, in this paper, we **assume access to object-centric information and features** rather than pure visual observations to simplify our discussion. We remark that although we restrict the scope of this paper to states, our method can be further extended to high-dimensional inputs (e.g. images, see App. B.4.1) or tabular MDPs via representation learning [68; 14].

Finally, to discover diverse strategies, we aim to learn a set of \(M\) policies \(\{_{i}\}_{i=1}^{M}\) such that all of these policies are locally optimal under \(J()\) but mutually distinct subject to some diversity measure \(D(,):\), which captures the difference between two policies.

**Existing Diversity Measures:** We say a diversity measure \(D\) is defined over action distribution if it can be written as

\[D(_{i},_{j})=_{s q(s)}[_{}( _{i}( s)\|_{j}( s))],\] (1)

where \(q\) is an occupancy measure over states, \(_{}:\) measures the difference between action distributions. \(_{}\) can be any probability distance as defined in prior works [60; 38; 74; 52].

Denote the state occupancy of \(\) as \(q_{}\). We say a diversity measure is defined over state occupancy if it can be written as

\[D(_{i},_{j})=_{}(q_{_{i}}\|q_{_{j}} ),\] (2)which can be realized as an integral probability metric . We remark that \(q_{}\) is usually intractable.

In addition to diversity measures, we present two popular computation frameworks for this purpose.

**Population-Based Training (PBT):** PBT is a straightforward formulation by jointly learning \(M\) policies \(\{_{i}\}_{i=1}^{M}\) subject to pairwise diversity constraints, i.e.,

\[_{\{_{i}\}}_{i=1}^{M}J(_{i})\ \ \ D(_{j},_{k}) , j,k[M],\,j k,\] (3)

where \(\) is a threshold. In our paper, we consistently refer to the aforementioned computation framework as "PBT", rather than adjusting hyperparameters . Despite a precise formulation, PBT poses severe optimization challenges due to mutual constraints.

**Iterative Learning (ITR):** ITR is a greedy approximation of PBT by iteratively learning novel policies. In the \(i\)-th (\(1 i M\)) iteration, ITR solves

\[_{i}^{}=_{_{i}}J(_{i})\ \ \ D(_{i},_{j}^{ }), 1 j<i.\] (4)

\(_{j}^{}\) is recursively defined by the above equation. Compared with PBT, ITR trades off wall-clock time for less required computation resources (e.g., GPU memory) and performs open-ended training (i.e., the population size \(M\) does not need to be fixed at the beginning of training).

## 4 Analysis of Existing Diversity-Discovery Approaches

In this section, we conduct both quantitative and theoretical analyses of existing approaches to motivate our method. We first discuss diversity measures in Sec. 4.1 and then compare computation frameworks, namely PBT and ITR, in Sec. 4.2.

### A Common Missing Piece in Diversity Measure: State Distance

The perception of diversity among humans primarily relies on the level of dissimilarity within the state space, which is measured by a distance function. However, the diversity measures outlined in Eq. (1) and Eq. (2) completely fail to account for such crucial information. In this section, we provide a detailed analysis to instantiate this observation with concrete examples and propose a novel diversity measure defined over state distances.

First, we present a synthetic example to demonstrate the limitations of current diversity measures. Our example consists of a grid-world environment with a single agent and grid size \(N_{G}\). The agent starts at the top left of the grid-world and must navigate to the bottom right corner, as shown in Fig. 1. While \(N_{G}\) can be large in general, we illustrate with \(N_{G}=5\) for simplicity. We draw five distinct policies, denoted as \(_{1}\) through \(_{5}\), which differ in their approach to navigating the grid-world. Consider \(_{1}\), \(_{2}\), and \(_{3}\) first. Although humans may intuitively perceive that policies \(_{1}\) and \(_{2}\), which move along the diagonal, are more similar to each other than to \(_{3}\), which moves along the boundary, diversity measures based on actions can fail to reflect this intuition, as shown in Table 1. Then, let's switch to policies \(_{3}\), \(_{4}\), and \(_{5}\). We find that state-occupancy-based diversity measures are unable to differentiate between \(_{4}\) and \(_{5}\) in contrast to \(_{3}\). This is because the states visited by \(_{3}\) are entirely disjoint from those visited by both \(_{4}\) and \(_{5}\). However, humans would judge \(_{5}\) to be more distinct from \(_{3}\) than \(_{4}\) because both \(_{3}\) and \(_{4}\) tend to visit the upper boundary.

    & human &  &  \\   & & KL & JSD\({}_{1}\) & JSD\({}_{0}\)/EMD & \(L_{2}\) norm & \(L_{2}\) norm & EMD \\  \(D(_{1},_{2})\) & small & \(+\)\(\) & **log 2** & **1/2** & \(\) & \(2\) & \(5.7\) \\ \((_{1},_{3})\) & large & \(+\)\(\) & **log 2** & \(1/8\) & \(1\) & \(}\) & \(\) \\   

Table 1: Diversity measures of the grid-world example. Computation details can be found in App. B.

Next, we consider a more realistic and complicated multi-agent football scenario, i.e., the Google Research Football  environment, in Fig. 3, where an idle player in the backyard takes an arbitrary action without involving in the attack at all. Although the idle player stays still with no effect on the team strategy, action-based measures can produce high diversity scores. This example underscores a notable issue. If action-based measures are leveraged to optimize diversity, the resultant policies can produce visually similar behavior. While it can be possible to exclude idle actions by modifying task rewards, it requires domain-specific hacks and engineering efforts. The issue of idle actions exists even in such popular MARL benchmarks. Similar issues have also been observed in previous works .

To summarize, existing measures suffer from a significant limitation -- they only compare the behavior trajectories _implicitly_ through the lens of action or state distribution without _explicitly measuring state distance_. Specifically, action-based measures fail to capture the behavioral differences that may arise when similar states are reached via different actions. Similarly, state occupancy measures do not quantify _the degree of dissimilarity_ between states. To address this limitation, we propose a new diversity measure that explicitly takes into account the distance function in state space:

\[D(_{i},_{j})=_{(s,s^{})}[g(d(s, s^{}))],\] (5)

\(d\) is a distance metric over \(\). \(g:^{+}\) is a monotonic cost function. \((q_{_{i}},q_{_{j}})\) is a distribution over state pairs. \((q_{_{i}},q_{_{j}})\) denotes the collection of all distributions on \(\) with marginals \(q_{_{i}}\) and \(q_{_{j}}\) on the first and second factors respectively. The cost function \(g\) is a notation providing a generalized and unified definition. It also contributes to training stability by scaling the raw distance. We highlight that Eq. (5) computes the cost on individual states before taking expectation, and therefore prevents information loss of taking the average over the entire trajectory (e.g. the DvD score ). We also note that states are consequences of performed actions. Hence, a state-distance-based measure also implicitly reflects the (meaningful) differences in actions between two policies. We compute two simple measures based on state distance, i.e., the \(L_{2}\) norm and the Earth Moving Distance (EMD), for the grid-world example and present results in Table 1. These measures are consistent with human intuition.

### Computation Framework: Population-Based or Iterative Learning?

We first consider the simplest motivating example to intuitively illustrate the optimization challenges. Let's assume that \(_{i}\) is a scalar, \(J(_{i})\) is linear in \(_{i}\), and \(D(_{i},_{j})=|_{i}-_{j}|\). In our definition, where \(M\) denotes the number of diverse policies, PBT involves \((M^{2})\) constraints in a single linear programming problem while ITR involves \((M)\) constraints in each of \(M\) iterations. Given that the complexity of linear programming is a high-degree polynomial (higher than 2) of the number of constraints, solving PBT is harder (and probably slower) than solving ITR in a total of \(M\) iterations,despite PBT being parallelized_. This challenge can be more severe in RL due to complex solution space and large training variance.

Although ITR can be optimized efficiently, it remains unclear whether ITR, as a greedy approximation of PBT, can obtain solutions of comparable rewards. Fig. 4 shows the worst case in the 1-D setting when the ITR solutions (green) can indeed have lower rewards than the PBT solution (red) subject to the same diversity constraint. However, we will show in the next theorem that ITR is guaranteed to have no worse rewards than PBT by trading off half of the diversity.

**Theorem 4.1**.: _Assume \(D\) is a distance metric. \(T_{2}=_{i=1}^{M}J(_{i})\) where_

\[_{i} =_{_{i}}J(_{i})\;D(_{i},_{j}) /2, 1 j<i\] (6)

_for \(i=1,,M\), then \(T_{2} T_{1}\)._

Please see App. E.1 for the proof. The above theorem provides a quality guarantee for ITR. The proof can be intuitively explained by the 1-D example in Fig. 4, where green points represent the worst case with threshold \(\) and blue points represent the solutions with threshold \(/2\). Thm. 4.1 shows that, for any policy pool derived by PBT, we can always use ITR to obtain another policy pool, which has _the same rewards_ and _comparable diversity scores_.

**Empirical Results:** We empirically compare PBT and ITR in a 2-D navigation environment with 1 agent and \(N_{L}\) landmarks in Fig. 3. The reward is 1 if the agent successfully navigates to a landmark and 0 otherwise. We train \(N_{L}\) policies using both PBT and ITR to discover strategies toward each of these landmarks. More details can be found in App. D. Table 2 shows the number of discovered landmarks by PBT and ITR. ITR performs consistently better than PBT even in this simple example. We intuitively illustrate the learning process of PBT and ITR in Fig. 3. ITR, due to its computation efficiency, can afford to run longer iterations and tolerate larger exploration noises. Hence, it can converge easily to diverse solutions by imposing a large diversity constraint. PBT, however, only converges when the exploration is faint, otherwise it diverges or converges too slowly.

### Practical Remark

Based on the above analyses, we suggest ITR and diversity measures based on state distances be _preferred_ in RL applications. We also acknowledge that, by the no-free-lunch theorem, they cannot be universal solutions and that trade-offs may still exist (see discussions in Sec.7 and App.F). Nonetheless, in the following sections, we will show that the effective implementation of these choices can lead to superior performances in various challenging benchmarks. We hope that our approach will serve as a starting point and provide valuable insights into the development of increasingly powerful algorithms for potentially more challenging scenarios.

## 5 Method

In this section, we develop a diversity-driven RL algorithm, _State-based Intrinsic-reward Policy Optimization (SIPO)_, by combining ITR and state-distance-based measures. SIPO runs \(M\) iterations to discover \(M\) distinct policies. At the \(i\)-th iteration, we solve equation (4) by converting it into unconstrained optimization using the Lagrange method. The unconstrained optimization can be written as:

\[_{_{i}}_{_{j} 0,\;1 j<i}-J(_{i})-_{j=1}^{i-1} _{j}(D_{}(_{i},_{j}^{*})-)\] (7)

   setting & PBT & ITR \\  \(N_{L}=4\) & 2.0 (1.0) & **3.5** (0.5) \\ \(N_{L}=5\) & 2.2 (0.9) & **4.5** (0.5) \\   

Table 2: The number of discovered landmarks across 6 seeds with standard deviation in the bracket.

Figure 4: 1-D worst case of ITR. With threshold \(\), ITR finds solutions with inferior rewards. However, ITR can find optimal solutions if the threshold is halved.

\(_{j}\) (\(1 j<i\)) are Lagrange multipliers. \(\{_{j}^{*}\}_{j=1}^{i-1}\) are previously obtained policies. We adopt two-timescale Gradient Descent Ascent (GDA)  to solve the above minimax optimization, i.e., performing gradient descent over \(_{i}\) and gradient ascent over \(_{j}\) with different learning rates. In our algorithm, we additionally enforce the dual variables \(_{j}\) to be bounded (i.e., in an interval \([0,]\) for a large number \(\)), which plays an important role both in the theoretical analysis and in empirical convergence. However, \(D_{}(_{i},_{j}^{*})\) cannot be directly optimized w.r.t. \(_{i}\) through gradient-based methods because it depends on the states traversed by \(\), rather than its output (e.g. actions). Therefore, we cast \(D_{}(_{i},_{j}^{*})\) as the cumulative sum of intrinsic rewards, specifically the intrinsic return. This allows us to leverage policy gradient techniques for optimization. The pseudocode of SIPO can be found in App. G.

An important property of SIPO is the convergence guarantee. We present an informal illustration in Thm. 5.1 and present the formal theorem with proof in App. E.2.

**Theorem 5.1**.: _(Informal) Under continuity assumptions, SIPO converges to an \(\)-stationary point._

**Remark:** We assumed that the return \(J\) and the distance \(D_{}\) are smooth in policies. In practice, this is true if (1) policy and state space are bounded and (2) reward function and system dynamics are continuous in the policy. (Continuous functions are bounded over compact spaces.) The key step is to analyze the role of the bounded dual variables \(\), which achieves an \(\)-approximation of constraint without hurting the optimality condition.

Instead of directly defining \(D_{}\), we define intrinsic rewards as illustrated in Sec. 5, such that \(D_{}(_{i},_{j}^{*})=_{s_{h}_{i}}[_{ h=1}^{H}r_{}(s_{h};_{i},_{j}^{*})]\).

**RBF Kernel:** The most popular realization of Eq. (5) in machine learning is through kernel functions. Herein, we realize Eq. (5) as an RBF kernel on states. Formally, the intrinsic reward is defined by

\[r_{}^{}(s_{h};_{i},_{j}^{*})=_{ s^{}_{_{j}^{*}}}[-(--s^{}\|^{2}} {2^{2}})]\] (8)

where \(\) is a hyperparameter controlling the variance.

**Wasserstein Distance:** For stronger discrimination power, we can also realize Eq. (5) as \(L_{2}\)-Wasserstein distance. According to the dual form , we define

\[r_{}^{}(s_{h};_{i},_{j}^{*})=_{\|f\|_ {L} 1}f(s_{h})-_{s^{}_{_{j}^{*}}}[f(s^{ })]\] (9)

where \(f:\) is a \(1\)-Lipschitz function. This realization holds a distinct advantage due to its interpretation within optimal transport theory [63; 1]. Unlike distances that rely solely on specific summary statistics such as means, Wasserstein distance can effectively quantify shifts in state distributions and remains robust in the presence of outliers . We implement \(f\) as a neural network and clip parameters to \([-0.01,0.01]\) to ensure the Lipschitz constraint. Note that \(r_{}^{}\) incorporates representation learning by utilizing a learnable scoring function \(f\) and is more flexible in practice. We also show in App. B.4 that \(r_{}^{}\) is robust to different inputs, including states with random noises and RGB images.

We name SIPO with \(r_{}^{}\) and \(r_{}^{}\)_SIPO-RBF_ and _SIPO-WD_ respectively.

**Implementation:** To incorporate temporal information, we stack the recent 4 global states to compute intrinsic rewards and normalize the intrinsic rewards to stabilize training. In multi-agent environments, we learn an agent-ID-conditioned policy  and share the parameter across all agents. Our implementation is based on MAPPO  with more details in App. D.

## 6 Experiments

We evaluate SIPO across three domains that exhibit multi-modality of solutions. The first domain is the humanoid locomotion task in Isaac Gym , where diversity can be quantitatively assessed by well-defined behavior descriptors. We remark that the issues we addressed in Sec. 4.1 may not be present in this task where the action space is small and actions are highly correlated with states. Further, we examine the effectiveness of SIPO in two much more challenging multi-agent domains, StarCraft Multi-Agent Challenge (SMAC)  and Google Research Football (GRF) , where well-defined behavior descriptors are not available and existing diversity measures may produce misleading diversity scores. We provide introductions to these environments in App. C.

First, we show that SIPO can efficiently learn diverse strategies and outperform several baseline methods, including DIPG , SMERL , DvD , and RSPO . Then, we qualitatively demonstrate the emergent behaviors learned by SIPO, which are both _visually distinguishable_ and _human-interpretable_. Finally, we perform an ablation study over the building components of SIPO and show that both the diversity measure, ITR, and GDA are critical to the performance.

All algorithms run for the same number of environment frames on a desktop machine with an RTX3090 GPU. Numbers are average values over 5 seeds in Humanoid and SMAC and 3 seeds in GRF with standard deviation shown in brackets. More algorithm details can be found in App. D. Additional visualization results can be found on our project website (see App. A).

### Comparison with Baseline Methods

**Humanoid Locomotion.** Following Zhou et al. , we train a population of size \(4\). We assess diversity by the pairwise distance of joint torques, a widely used behavior descriptor in recent Quality-Diversity works . Torque states are not included as the input of diversity measures and we only use them for evaluation to ensure a fair comparison. Results are shown in Table 3. We can see that both variants of SIPO can outperform all baseline methods except that SIPO-RBF achieves comparable performance with RSPO, even if RSPO explicitly encourages the output of different actions/forces.

**SMAC** Following Zhou et al. , we run SIPO and all baselines on an easy map, _2m_vs_1z_, and a hard map, _2c_vs_64zg_, both across 4 iterations. We merge all trajectories produced by the policy collection and incorporate a \(k\)-nearest-neighbor state entropy estimation  to assess diversity. Intuitively, a more diverse population should have a larger state entropy value. We set \(k=12\) following Liu and Abbeel  and show results in Table 4. On these maps, two agents are both involved in the attack. Therefore, RSPO, which incorporates an action-based cross-entropy measure, can perform well across all baselines. However, SIPO explicitly compares the distance between resulting trajectories and can even outperform RSPO, leading to the most diverse population.

**GRF** We consider three academy scenarios, specifically _3v1_, _counterattack_ (_CA_), and _corner_. The GRF environment is more challenging than SMAC due to the large action space, more agents, and the existence of duplicate actions. We determine a population size \(M=4\) by balancing resources and wall-clock time across different baselines. Table 5 compares the number of distinct policies (in terms of ball-passing routes, see App. B.3) discovered in the population. Due to the strong adversarial power of our diversity measures and the application of GDA, SIPO is the most efficient and robust -- even in the challenging 11-vs-11 _corner_ and _CA_ scenario, SIPO can effectively discover different winning strategies in just a few iterations across different seeds. By contrast, baselines suffer from learning instability in these challenging environments and tend to discover policies with slight distinctions. We also calculate the estimated state entropy as we did in SMAC. However, we find that this metric cannot distinguish fine-grained ball-passing behaviors in GRF (check our discussions in App. B).

**Remark:** In GRF experiments, when \(M\) is small, even repeated training with different random seeds (PG) is a strong baseline (see Table 5). Hence, the numbers are actually restricted in a small interval (with a lower bound equal to PG results and an upper bound equal to \(M=4\)), which makes the improvements by SIPO seemingly less significant. However, achieving clear improvements in these challenging applications remains particularly non-trivial. With a population size \(M=10\), SIPO clearly outperforms baselines by consistently discovering one or more additional strategies.

### Qualitative Analysis

For SMAC, we present heatmaps of agent positions in Fig. 5. The heatmaps clearly show that SIPO can consistently learn novel winning strategies to conquer the enemy. Fig. 6 presents the learned

    & \(2m\_vs\_1z\) & \(2c\_vs\_64zg\) \\  SIPO-RBF & **0.0350.002** & **0.0720.003** \\ SIPO-WD & 0.0360(0.001) & 0.0560(0.003) \\ RSPO & 0.0320(0.003) & 0.0700(0.001) \\ DIPG & 0.0320(0.002) & 0.0560(0.004) \\ SMERL & 0.0280(0.002) & 0.0420(0.002) \\ DvD & 0.030(0.002) & 0.057(0.003) \\   

Table 4: State entropy estimated by \(k\)-nearest-neighbor in SMAC. (\(k=12\))

   SIPO-RBF & SIPO-WD & RSPO \\ 
0.53(0.17) & **0.71(0.23)** & 0.53(0.05) \\   DIPG & DvD & SMERL \\ 
0.12(0.04) & 0.40(0.22) & 0.01(0.00) \\   

Table 3: Pairwise distance of joint torques (i.e., diversity scores) in the humanoid locomotion task.

behavior by SIPO in the GRF _3v1_ scenario of seed 1. We can observe that agents have learned a wide spectrum of collaboration strategies across merely 7 iterations. The strategies discovered by SIPO are both _diverse_ and _human-interpretable_. In the first iteration, all agents are involved in the attack such that they can distract the defender and obtain a high win rate. The 2nd and the 6th iterations demonstrate an efficient pass-and-shoot strategy, where agents quickly elude the defender and score a goal. In the 3rd and the 7th iterations, agents learn smart "one-two" strategies to bypass the defender, a prevalent tactic employed by human football players. We note that _NONE_ of the baselines have ever discovered this strategy across all runs, while SIPO is consistently able to derive such strategies for all random seeds. Visualization results in _CA_ and _corner_ scenarios can be found in App. B.

### Ablation Study

We apply these changes to SIPO-WD:

* _fix-L:_ Fixing the multiplier \(_{i}\) instead of applying GDA.

    &  &  &  \\   & & SIPO-RBF & SIPO-WD & DIPG & SMERL & DvD1  & RSPO & PG \\  _3v1_ & 4 & **3.0 (0.8)** & **3.0 (0.0)** & 2.7 (0.5) & 1.3 (0.5) & **3.0 (0.8)** & 2.0 (0.0) & 2.7 (0.5) \\ _CA_ & 4 & **3.3 (0.5)** & 3.0 (0.8) & 2.3 (0.5) & 1.3 (0.5) & - & 2.0 (0.0) & 1.7 (0.5) \\ _corner_ & 4 & 2.7 (0.5) & **3.0 (0.8)** & 1.7 (0.5) & 1.0 (0.0) & - & 1.6 (0.5) & 2.0 (0.8) \\ _3v1_ & 10 & 4.3 (0.5) & **5.7 (0.5)** & 3.7 (0.5) & - & - & 2.3 (0.5) & - \\   

Table 5: Number of distinct strategies in GRF discovered by different methods in terms of the ball-passing route. Details of the evaluation protocol can be found in App. B.3.

Figure 5: Heatmaps of agent positions in SMAC across 4 iterations with SIPO-RBF.

Figure 6: Learning curves and discovered strategies by SIPO-WD in the _3v1_ scenario over 7 iterations. Strategies of seed 1 are shown.

* _CE_: The intrinsic reward is replaced with cross-entropy, i.e., \(r_{}^{}(s_{h},a_{h})=-_{j}^{*}(a_{h} s_{h})\), where \(_{j}^{*}\) denotes a previously discovered policy. Additionally, GDA is still applied.
* _filter_: Optimizing the extrinsic rewards on trajectories that have intrinsic returns exceeding \(\) and optimizing intrinsic rewards defined by Eq. (9) for other trajectories .
* _PBT_: Simultaneously training \(M\) policies with \(M(M-1)/2\) constraints (i.e., directly solving Eq. (3)) with intrinsic rewards defined by Eq. (9) and GDA.

We report the number of visually distinct policies discovered by these methods in Table 6. Comparison between SIPO and CE demonstrates that the action-based cross-entropy measure may suffer from duplicate actions in GRF and produce nearly identical behavior by overly exploiting duplicate actions, especially in the _CA_ and _corner_ scenarios with 11 agents. Besides, the fixed Lagrange coefficient, the filtering-based method, and PBT are all detrimental to our algorithm. These methods also suffer from significant training instability. Overall, the state-distance-based diversity measure, ITR, and GDA are all critical to the performance of SIPO.

## 7 Conclusion

We tackle the problem of discovering diverse high-reward policies in RL. First, we demonstrate concrete failure cases of existing diversity measures and propose a novel measure that explicitly compares the distance in state space. Next, we present a thorough comparison between PBT and ITR and show that ITR is much easier to optimize and can derive solutions with comparable quality to PBT. Motivated by these insights, we combine ITR with a state-distance-based diversity measure to develop SIPO, which has provable convergence and can efficiently discover a wide spectrum of human-interpretable strategies in a wide range of environments.

**Limitations:** First, we assume direct access to an object-centric state representation. When such a representation is not available (e.g., image-based observations), representation learning becomes necessary and algorithm performance can be affected by the quality of the learned representations. Second, because ITR requires sequential training, the wall clock time of SIPO can be longer than the PBT alternatives when fixing the total number of training samples. The acceleration of ITR remains an open challenge.

**Future Directions:** Besides addressing the above limitations, we suggest three additional future directions based on our paper. First, a consensus on the best algorithmic formulation of distinct solutions in RL remains elusive. It is imperative to understand diversity in a more theoretical manner. Second, while this paper focuses on single-agent and cooperative multi-agent domains, extending SIPO to multi-agent competitive games holds great potential. Finally, although SIPO/ITR enables open-ended training, it is worth studying how to determine the optimal population size to better balance resources and the diversity of the resulting population.