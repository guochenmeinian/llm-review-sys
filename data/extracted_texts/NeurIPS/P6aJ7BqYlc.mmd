# GACL: Exemplar-Free Generalized Analytic

Continual Learning

 Huiping Zhuang\({}^{1}\)1 Yizhu Chen\({}^{1}\)1 Di Fang\({}^{1}\) Run He\({}^{1}\) Kai Tong\({}^{1}\)

**Hongxin Wei\({}^{2}\) Ziqian Zeng\({}^{1}\)2 Cen Chen\({}^{1,3,4}\)3\({}^{1}\)**South China University of Technology, China

\({}^{2}\)Southern University of Science and Technology, China

\({}^{3}\)Shenzhen Institute, Hunan University, China

\({}^{4}\)Pazhou Lab, China

###### Abstract

Class incremental learning (CIL) trains a network on sequential tasks with separated categories in each task but suffers from catastrophic forgetting, where models quickly lose previously learned knowledge when acquiring new tasks. The generalized CIL (GCIL) aims to address the CIL problem in a more real-world scenario, where incoming data have mixed data categories and unknown sample size distribution. Existing attempts for the GCIL either have poor performance or invade data privacy by saving exemplars. In this paper, we propose a new exemplar-free GCIL technique named generalized analytic continual learning (GACL). The GACL adopts analytic learning (a gradient-free training technique) and delivers an analytical (i.e., closed-form) solution to the GCIL scenario. This solution is derived via decomposing the incoming data into exposed and unexposed classes, thereby attaining a weight-invariant property, a rare yet valuable property supporting an equivalence between incremental learning and its joint training. Such an equivalence is crucial in GCIL settings as data distributions among different tasks no longer pose challenges to adopting our GACL. Theoretically, this equivalence property is validated through matrix analysis tools. Empirically, we conduct extensive experiments where, compared with existing GCIL methods, our GACL exhibits a consistently leading performance across various datasets and GCIL settings. Source code is available at https://github.com/CHEN-YIZHU/GACL.

## 1 Introduction

Class incremental learning (CIL) , an important form of continual learning, aims to effectively tune an off-the-shelf network on incoming new datasets, with data excluding various categories from its previous states. The CIL has gained significant traction due to its ability to refine learned models for new and unfamiliar data classes, eliminating the need to start the training process from scratch. This elimination of retraining saves valuable computational resources, which is especially important in the era of pre-trained models that have absorbed a massive amount of data.

One significant challenge in CIL is _catastrophic forgetting_, which causes trained models to lose existing knowledge when gaining new information quickly. This can be attributed to the fundamental property of gradient-based iterative algorithms that impose a _task-recency_ bias, i.e., predictions favor recently updated categories . To the authors' knowledge, no solutions exist for these gradient-trained CIL models to fully tackle catastrophic forgetting.

On the other hand, traditional CIL assumes that the number of samples in each task is fixed and that new tasks are entirely disjoint from previous ones. This paradigm does not align with real-world scenarios, where training data may include both new and previously encountered categories, and the number of data points often exhibits arbitrariness in each task. This extended CIL setting is referred to as generalized CIL (GCIL) [5; 6]. Such an uneven task-wise distribution of training samples and data categories further complicates the forgetting issue. For instance, GCIL may lead to the neglect of minority samples within a batch, thereby undermining representation during the training process.

To mitigate catastrophic forgetting, a simple but effective approach is to replay historical samples. Replay-based CIL [1; 4] mitigates forgetting by storing a small number of samples from historical categories for the model to review while learning new information. However, this replay mechanism poses risks to data privacy. Thus, the exemplar-free CIL (EFCIL) without saving old exemplars gains prominence due to the increasing concern for privacy. However, many EFCIL methods perform poorly due to the task-recency bias caused by the nature of gradient-based algorithms . Recently, this dilemma has been alleviated by the analytic continual learning (ACL) [7; 8], an emerging EFCIL branch that first achieves comparable or even more competitive performance over the replay-based CIL. This improvement occurs because, for the first time, ACL achieves a near "complete non-forgetting" by allowing an equivalence between the incremental learning and its joint training (i.e., the weight-invariant property).

The ACL provides a powerful toolbox for traditional EFCIL scenarios where data categories among training tasks are mutually exclusive. However, an apparent gap exists between the existing ACL techniques and the more desired and real-world GCIL scenario. Exploring the possibility of incorporating the weight-invariant property into the GCIL framework is both a significant and natural motivation, as it has the potential to enhance overall performance. To achieve this, we propose a generalized analytic continual learning (GACL), a new and compensated ACL member, offering a weight-invariant property solution to the GCIL. The key contributions are summarized as follows.

* We present the GACL, an exemplar-free technique that achieves the equivalence between the GCIL (with split incoming data) and its joint training (with data centralized in a single task).
* We theoretically establish the GACL's weight-invariant property. It is achieved and proved by separating the incoming data into exposed and unexposed components and aligning them structurally with matrix decomposition techniques.
* We isolate the distinctive component of the GACL, namely the _exposed class label gain_ (ECLG), from the existing ACL. This module explains the feasibility of achieving GCIL's analytic learning, offering a high interpretability in the GCIL realm.
* Experiments on various benchmark datasets are presented, showing that the GACL outperforms the existing EFCIL by a large margin. It also exceeds most state-of-the-art replay-based methods.

## 2 Related Works

This section reviews existing methods for CIL and its more real-world counterpart, i.e., GCIL.

### CIL Techniques

Existing CIL methods can be roughly divided into three categories: replay-based methods, regularization-based methods, and prototype-based methods.

The _replay-based CIL_ methods such as iCaRL , LUCIR , PODNet , AANets , FOSTER , and OHO , retain past training samples as exemplars and utilize them during the learning of new ones. However, storing original training samples presents a significant challenge, particularly in scenarios with strict data privacy requirements.

The _regularization-based CIL_ aims to design a loss function that prevents the change of activations or important weights. Methods such as the Less-forgetting learning  and the LwF  introduce knowledge distillation  into their loss function to prevent the forgetting caused by activation drift. EWC , EWC++, RWalk , and Rotate your Networks , introduce regularization that slows down learning on the weights important for old tasks by calculating the Fisher information matrix.

The _prototype-based CIL_ maintains distinct prototypes for each category, which prevents overlapping representations of new and old categories. For example, the PASS  distinguishes prior categoriesby augmenting feature prototypes. The SSRE technique  enhances the dissimilarity between old and new categories via selecting prototypes to incorporate with new samples into a distillation process. The FeTrIL  uses new representations to generate pseudo-features of old categories.

### Analytic Continual Learning

The ACL is a recently developed EFCIL branch inspired by the analytic learning [22; 23; 24] where the training of neural networks yields a closed-form solution using least squares. The ACIL  first converts a continual learning problem to a batch recursive least-squares problem, eliminating the need to store samples by preserving the correlation matrix, and the RanPAC  applies this trick to pre-trained models. The GKEAL  focuses on the few-shot CIL scenarios by leveraging a Gaussian kernel projection. The DS-AL  introduces an additional linear classifier to learn the residue of the ACIL to enhance the plasticity, while the REAL  introduces the representation enhancing distillation to improve the backbone's generalization capabilities. The AFL  extends the ACL to federated learning, transitioning from temporal increment to spatial increment, and similar techniques are applied to the reinforcement learning . The ACL is an emerging competitive CIL branch with a closed-form solution that leads to a valuable weight-invariant property, securing the equivalence between CIL and its joint learning. However, existing ACL methods are designed for the CIL scenario in which the categories of samples in each task must be entirely distinct. This restricts their applicability in real-world scenarios.

### The Generalized Class Incremental Learning

The GCIL simulates real-world incremental learning, as distributions of data category and size could be unknown in one task. The GCIL arouses problems such as intra- and inter-task forgettings and the class imbalance problem . The key GCIL properties can be summarized as follows: (i) the number of classes across different tasks is not fixed; (ii) classes shown in prior tasks could reappear in later tasks; (iii) training samples are imbalanced across different classes in each task  (See Appendix B).

There are several GCIL settings. In the BlurryM setting , \(a\%\) of the classes are disjoint between tasks, while the remaining classes appear in every task. The i-Blurry-N-M  setting has blurry task boundaries and requires the model to perform anytime inference. However, the i-Blurry scenario has a fixed number of classes in each task with the same proportion of new and old classes. The Si-Blurry  is the most complex and realistic GCIL setting satisfying all three GCIL properties since it has an ever-changing number of classes and is capable of effectively simulating newly emerging or disappearing data, highlighting the problem of uneven distribution in real-world scenarios.

To address the issue of the GCIL, gradient-based sample selection methods such as the GSS-IQP and the GSS-Greedy are proposed by . The RM  proposes a memory management strategy based on per-sample classification uncertainty and data augmentation, while the management in the CLIB  eliminates samples based on a per-sample importance score. The DualPrompt , as an EFCIL method, introduces the prompt-based learning to the CIL problem, and the MVP  proposes an instance-wise logit masking and contrastive visual prompt tuning loss.

## 3 The Proposed Method

In this section, we deliver details of the proposed GACL. We first define the learning problem. Then, we derive the GACL by employing matrix decomposition techniques. A corresponding theoretical analysis follows to indicate the interpretability of our work. An overview is depicted in Figure 1.

### Problem Definition

We denote the complete set of available data as \(\). When \(\) is partitioned into a sequence of GCIL tasks, we assume that \(_{k}^{}\{_{k}^{},_{k}^{ }\}\) is the set of training samples that are present in task \(k\). The training dataset \(_{k}^{}\) consists of labeled samples, where \(_{k}^{}^{N_{k} c} h}\) represents \(N_{k}\) input image samples with a shape of \(c w h\). \(_{k}^{}^{N_{k} d_{y_{k}}}\) represents \(N_{k}\)-stacked one-hot encoded label tensors with \(d_{y_{k}}\) classes that have been seen from task \(1\) to task \(k\). \(_{k}^{}\{_{k}^{},_{k}^{}\}\)is the test dataset in task \(k\). The goal of GCIL in task \(k\) is to train networks using \(_{k}^{}\) and evaluate their performance on the test dataset \(_{1:k}^{}\). Here, \(_{1:k}\) denotes the joint dataset spanning tasks \(1\) to \(k\).

### Exposed-unexposed Class Split

In each GCIL task, classes may not appear exclusively. Hence, in any GCIL task \(k\), we refer to classes that have appeared in previous tasks 1 to \(k-1\) as the _exposed classes_ of task \(k\), while classes making their initial appearance are the _unexposed classes_ of task \(k\) as shown in Figure 1 (a). This distinction helps to characterize the evolving nature of class occurrences throughout different GCIL tasks.

In a task-wise GCIL scenario, we can involve all class labels in a set \(\). In task \(k\), the set of the exposed class labels is denoted as \(_{}\), while the set of unexposed class labels is marked by \(_{}\), where \(_{}_{}=\). Note that \(_{}\) and \(_{}\) may evolve from task \(k-1\) to task \(k\), that is

\[_{}=S_{}_{}=S_{} S_{}_{}.\]

From the scope of exposed-unexposed classes, the \(d_{y_{k}}\) can be represented as \(d_{y_{k}}=|_{}|+|_{}|=d_{y_{k-1}}+|_{}|\), where \(||\) denotes the cardinality of a set.

In task \(k\), given training dataset \(_{k}^{}\{_{k}^{},_{k}^{ }\}\), class labels \(_{k}^{}\) can be partitioned due to the _exposed-unexposed split_ as follows:

\[_{k}^{}=[}_{k}^{}}_{k}^{}],\] (1)

where \(}_{k}^{}^{N_{k} d_{y_{k-1}}}\) is the _exposed class label matrix_ and \(}_{k}^{}^{N_{k}(d_{y_{k}}-d_{y_{ k-1}})}\) is the _unexposed class label matrix_. They correspond to segments displaying the appearance of exposed classes and unexposed classes.

### Buffered Embedding Extraction

The power of pre-trained models allows the GACL to adopt a frozen backbone from structures such as ViT  to extract the features of images shown in Figure 1 (b). Let

\[^{()}=f_{}(,_{})\] (2)

Figure 1: An overview of our proposed GACL. (a) Labels of the _exposed class_ and the _unexposed class_ are extracted in each GCIL task (see definition in Section 3.2), respectively. (b) A frozen pre-trained ViT and a buffer layer are utilized to extract features from the inputs. (c) The key to the recursively updated formulation of the GACL contains two components. The \(}_{}^{(k)}\) takes in the contribution of unexposed class data (see (11)). The other is contributed by the ECLG module \(}_{}^{(k)}\) (e.g., see (12)), which reflects the gain of exposed class data on the seen categories. The recursive formulation flows aided by the _autocorrelation memory matrix_\(\) throughout the GCIL.

be the features extracted by the backbone, where \(_{}\) indicates the backbone weight. Then, we use a buffer layer to project features, i.e.,

\[_{i}^{}=f_{}(^{}),\] (3)

where \(f_{}\) indicates the operation of the buffer layer. Several options for the buffer layer exist, including a randomly initialized linear mapping in the ACIL  or a kernel embedding projection in the GKEAL . The selection of the buffer layer is not our focus. For convenience, we follow the ACIL, taking the random linear projection followed by a non-linear activation function as the buffer layer, i.e. \(f_{}(^{})=(^{} _{})\), where the elements of the buffer layer weight \(_{}\) are randomly sampled from a normal distribution.

### Generalized Analytic Class Incremental Learning

Here, we derive the GACL by partitioning training samples into unexposed and exposed categories, as shown in Figure 1 (c). Let \(_{1:k}^{}\) and \(_{1:k}^{}\) be the accumulated feature and label matrices in task \(k\), which can be extended from the accumulated matrices \(_{1:k-1}^{}\) and \(_{1:k-1}^{}\) in task \(k-1\) as follows.

\[_{1:k}^{}=_{1:k-1}^{}\\ _{k}^{},_{1:k}^{}= _{1:k-1}^{}&\\ }_{k}^{}&}_{k}^{}.\]

Subsequently, one could formulate the learning problem in task \(k\) by a fully connected network (FCN) as the classifier

\[*{argmin}_{_{}^{(k)}}\ \|_{1:k}^{}-_{1:k}^{}_{}^{(k)}\|_{}^{2}+ \|_{}^{(k)}\|_{}^{2},\] (4)

where \(\|\|_{}\) is Frobenius-norm, \( 0\) is the regularization term and \(_{}^{(k)}\) indicates the FCN layer weight. The optimal solution to (4) is

\[}_{}^{(k)}=(_{1:k}^{}_{1:k}^{ }+)^{-1}_{1:k}^{}_{1:k}^{ }.\] (5)

The goal of the GACL is then to obtain \(}_{}^{(k)}\) recursively from \(}_{}^{(k-1)}\) without directly involving historical samples (e.g., \(_{1:k-1}^{}\) and \(_{1:k-1}^{}\)). That is to solve

\[*{argmin}_{_{}^{(k)}}\ \|_{1:k-1}^{ }&\\ }_{k}^{}&}_{k}^{} -_{1:k-1}^{}\\ _{k}^{}_{}^{(k)}\|_{}^{2}+\|_{}^{(k)}\|_{}^{2}\] (6)

by recursively updating the previous-task weight \(}_{}^{(k)}\). To achieve this, we define an _autocorrelation memory matrix_ as follows.

\[_{k}=(_{1:k}^{}_{1:k}^{}+)^{-1}.\] (7)

Accordingly, we summarize the recursive formulation of the proposed GACL in Theorem 3.1.

**Theorem 3.1**.: _Let \(}_{}^{(k)}\) be the optimal estimation of (6) with all the training data from task \(1\) to task \(k\). Then \(}_{}^{(k)}\) is equivalent to its recursive form_

\[}_{}^{(k)}=[}_{}^{(k-1)}- _{k}_{k}^{}_{k}^{}}_{}^ {(k-1)}+_{k}_{k}^{}}_{k}^{} _{k}_{k}^{}}_{k}^{} ],\] (8)

_where_

\[_{k}=_{k-1}-_{k-1}_{k}^{}(+_{k }^{}_{k-1}_{k}^{})^{-1}_{k}^{}_{k-1}.\] (9)

Proof.: See Appendix A. 

As indicated in Theorem 3.1, the weight \(}_{}^{(k)}\) in task \(k\) recursively obtained using the previous-task weight \(}_{}^{(k-1)}\) is identical to its joint-learning counterpart formulated in (6). That is, the GACL maintains the same _weight-invariant property_ in the GCIL scenario as other ACL methods.

The pseudo-code of the GACL is listed in Algorithm 1.

``` Input: GCIL tasks \(_{1}^{},,_{K}^{}\) with \(_{k}^{}\{_{k}^{},_{k}^{}\}\), the pre-trained backbone with frozen weight \(_{}\) Initialization:\(_{0}\), \(_{}^{(0)}\) for task \(k=1\) to \(K\)do \(_{k}^{()}_{}(_{k}^{ },_{})\) (2) \(_{k}^{()}_{}(_{k}^{( )})\) (3)  Decompose \(_{k}^{}\) into exposed and unexposed class components \(}_{k}^{}\) and \(}_{k}^{}\) \(_{k}_{k-1}-_{k-1}_{k}^{()}( +_{k}^{()}_{k-1}_{k}^{()})^{-1}_{k }_{k-1}\) (9) \(_{}^{(k)}[_{}^{(k-1)}- _{k}_{k}^{()}_{k}^{()}}_{ }^{(k-1)}.\)\(_{k}_{k}^{()}}}_{k}^{}\) (11) \(_{}^{(k)}[_{k}_{k}^{() }}}_{k}^{}]\) (12) \(_{}^{(k)}_{}^{(k)}+_{ }^{(k)}\) endfor ```

**Algorithm 1** The pseudo-code of GACL.

**Exemplar-free.** The recursive formulation is aided by \(_{k}\) as indicated in (9). Note that this autocorrelation memory matrix records the inverse of inner products among the historical embedding matrices as shown in (7). Hence, the embeddings (e.g., \(_{k}^{()}\)) are not reversible. Saving \(_{k}\) instead of used samples is a safe alternative to preserve past knowledge. That is, our GACL is an _exemplar-free_ technique without the need to keep any historical samples.

To more properly explain our GACL, as indicated in Figure 1 (c), the recursive solution in (8) can be rewritten as the sum of the unexposed-class contributed weight \(}_{}^{(k)}\) and the ECLG weight \(}_{}^{(k)}\), i.e.,

\[}_{}^{(k)}=}_{}^{(k)}+}_{}^{(k)},\] (10)

where

\[}_{}^{(k)}=[}_{}^{(k-1)}-_{k}_{k}^{()}_{k}^{()}}_{ }^{(k-1)}_{k}_{k}^{()}}} _{k}^{}],\] (11) \[}_{}^{(k)}=[_{k}_{k}^{( )}}}_{k}^{}].\] (12)

Unevposed-class Contributed Weight.The unexposed-class contributed weight \(}_{}^{(k)}\) is recursively updated by the data of the unexposed class only. Note that the unexposed class label \(}_{k}^{}\) is applied on the concatenated weight along with new data \(_{k}^{()}\), which is reasonable as historical information should not intervene with the weight update of unseen classes. On the other hand, new data \(_{k}^{()}\) could also affect historical knowledge. This is marked by the gain of \(-_{k}_{k}^{()}_{k}^{()}}_{ }^{(k-1)}\) to the original weight \(}_{}^{(k-1)}\) as indicated in (11).

Exposed-class Label Gain Weight.The ECLG module indicated in (12) captures knowledge from exposed-class labels. The supervision of this weight component marked by \(_{k}_{k}^{()}}}_{k}^{}\) is mainly contributed by the exposed-class labels (i.e., \(}_{k}^{}\)). It is important to note that when \(}_{k}^{}\) is empty (i.e., no classes reappear in task \(k\)), this component does not contribute to the update of \(}_{}^{(k)}\). This module is also isolated to distinguish GACL's difference from the existing ACL methods in a mathematical analysis manner (indicated as follows).

Difference from Existing ACL Methods.Overall, the GACL can be treated as a nontrivial generalization of ACIL , GKEAL , and various other ACL methods. For instance, in conventional CIL where no classes reappear in new tasks (i.e., \( k,}_{k}^{}^{* 0}\)), the classifier of the GACL \(}_{}^{(k)}=}_{}^{(k)}\), which is equivalent to the recursive classifier of the ACIL. That is, the ACIL is a special case of our proposed GACL. The major difference lies in the ECLG module, corresponding to the exposed-class gain. This pattern makes sense as there must be compensation on top of ACIL updates (specifically designed for traditional CIL) when exposed data (out of setting) participate.

## 4 Experiments

### Experimental Setup

In the section, we conduct experiments on various benchmark datasets and compare the GACL with both EFCIL and replay-based state-of-the-art methods, including LwF , L2P , DualPrompt , ER , EWC++ , SLDA , RM , MVP , and MVP-R (MVP with exemplars).3

Datasets.We conduct experiments on three datasets: CIFAR-100 , ImageNet-R , and Tiny-ImageNet . We evaluate each method under the Si-Blurry setting  (the most complex GCIL setting) with 5 independent seeds. For the Si-Blurry setting, we set the disjoint class ratio \(_{}\) to \(50\%\) and the blurry sample ratio \(_{}\) to \(10\%\). More details about Si-Blurry are listed in Appendix C.

Implementation Details.We utilize the DeiT-S/16  as our backbone. Following [42; 43], we pre-train the backbone on 611 ImageNet classes after excluding 389 classes that overlap with CIFAR and Tiny-ImageNet to prevent data leakage. To ensure a fair comparison, all methods utilize a frozen backbone. All methods under comparison are implemented as specified in . The memory sizes of compared relay-based methods are set to 500 and 2000.

There are two hyperparameters in the GACL, the regularization term \(\) and the size of the buffer layer. Here, we adopt \(=100\), which is determined by the grid search of {0, 10, 100, 500, 1000, 10000} on CIFAR-100 (by a 90%-10% train-val split). As the regularization term \(\) is not sensitive in a proper range , we adopt this value for all datasets for convenience. We relocate its analysis to Appendix E. The size for the buffer layer \(_{}\) is set to 5000 for both the GACL and ACIL for convenience.

Evaluation Protocol.Three metrics are adopted to evaluate GCIL tasks. The real-time performance is evaluated by the _area under the curve of accuracy_\(_{}\), i.e., \(_{}=_{i=1}^{k}f(i n) n\), where \( n\) is the number of samples observed between evaluation and \(f()\) is the curve in the accuracy-to-{number of training samples} plot, measuring anytime inference performance during training. A higher \(_{}\) corresponds to a method consistently maintaining high accuracy throughout the training. The overall performance is evaluated by the _average incremental accuracy_ (or average accuracy) \(_{}=^{K}}_{k}\), where the task-wise accuracy \(_{k}\) indicates the average test accuracy in task

    &  &  &  &  &  \\   & & & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) & \(_{}\) \\   & EWC++  & ✕ & 53.31\(\)1.70 & 50.95\(\)1.50 & 52.55\(\)0.73 & 36.31\(\)0.72 & 39.87\(\)1.35 & 29.52\(\)0.43 & 52.43\(\)0.92 & 54.61\(\)1.54 & 37.67\(\)0.77 \\  & ER  & ✕ & 56.17\(\)1.54 & 53.80\(\)1.46 & 55.60\(\)0.69 & 39.31\(\)0.70 & 43.03\(\)1.93 & 32.09\(\)0.44 & 55.69\(\)0.47 & 57.87\(\)1.42 & 41.10\(\)0.57 \\  & RM  & ✕ & 53.22\(\)1.82 & 52.99\(\)1.69 & 55.25\(\)0.61 & 32.34\(\)1.88 & 36.46\(\)2.23 & 25.26\(\)1.08 & 49.28\(\)0.84 & 57.74\(\)1.57 & 41.79\(\)0.34 \\  & MVP-R  & ✕ & 60.62\(\)1.03 & 57.58\(\)0.56 & 34.03\(\)0.29 & 47.16\(\)1.60 & 50.36\(\)0.99 & 42.05\(\)0.15 & 61.15\(\)0.86 & 62.41\(\)0.50 & 51.12\(\)0.67 \\   & EWC++  & ✕ & 48.31\(\)1.81 & 44.56\(\)0.69 & 40.52\(\)0.68 & 32.81\(\)0.76 & 35.54\(\)1.69 & 23.43\(\)0.61 & 45.30\(\)0.61 & 46.34\(\)2.08 & 27.05\(\)1.35 \\  & ER  & ✕ & 51.59\(\)1.94 & 48.03\(\)0.80 & 44.09\(\)0.80 & 35.96\(\)0.72 & 39.01\(\)1.54 & 26.14\(\)0.44 & 48.95\(\)0.58 & 50.44\(\)1.21 & 29.97\(\)0.75 \\  & RM  & ✕ & 41.07\(\)1.30 & 38.10\(\)0.93 & 32.66\(\)0.34 & 22.45\(\)0.622 & 22.08\(\)1.78 & 9.61\(\)0.13 & 36.66\(\)0.40 & 38.83\(\)2.33 & 18.23\(\)0.22 \\  & MVP-R  & ✕ & 56.20\(\)1.47 & 53.61\(\)0.94 & 55.35\(\)0.43 & 43.28\(\)1.41 & 45.74\(\)0.97 & 35.60\(\)1.18 & 55.28\(\)1.42 & 55.54\(\)1.12 & 40.12\(\)0.40 \\   & LwF  & ✕ & 40.71\(\)2.13 & 38.49\(\)0.56 & 27.03\(\)2.92 & 29.41\(\)0.83 & 31.95\(\)1.86 & 19.67\(\)1.27 & 39.88\(\)0.904 & 41.35\(\)2.59 & 24.93\(\)0.21 \\  & L2P  & ✕ & 42.68\(\)2.70 & 39.89\(\)0.45 & 28.95\(\)3.34 & 30.21\(\)0.91 & 32.21\(\)1.73 & 18.01\(\)0.37 & 41.67\(\)1.17 & 42.53\(\)2.52 & 24.78\(\)2.31 \\  & DualPrompt  & ✓ & 41.34\(\)2.59 & 38.59\(\)0.68 & 22.74\(\)3.30 & 30.44\(\)0.83 & 25.41\(\)1.84 & 16.07\(\)3.30 & 39.16\(\)1.33 & 39.81\(\)3.02 & 20.42\(\)3.37 \\  & MVP  & ✓ & 45.07\(\)2.43 & 44.93\(\)0.54 & 39.94\(\)0.47 & 35.77\(\)2.55 & 35.58\(\)1.20 & 22.06\(\)5.01 & 46.43\(\)3.37 & 45.41\(\)1.19 & 28.21\(\)2.89 \\  & SLDA  & ✓ & 53.00\(\)3.85 & 50.09\(\)2.77 & 61\(k\) by testing the network on \(_{}^{}\). A higher \(_{}\) score is preferred when evaluating algorithms. The last evaluation metric is the _last-task accuracy_\(_{}\) evaluating the network's last-task performance after completing all tasks.

### Comparison with State-of-the-arts

As shown in Figure 2, we comprehensively compare the GACL with both EFCIL and replay-based methods.

Compare with EFCIL Methods.EFCIL methods address privacy concerns and mitigate catastrophic forgetting without exemplars. Among EFCIL methods, our GACL consistently exhibits superior performance across all three datasets, as illustrated in the lower panel of Table 1.

For instance, on CIFAR-100, our method surpasses the second-best method SLDA, by **4.99\(\%\)**, **6.15\(\%\)**, and **8.52\(\%\)** for \(_{}\), \(_{}\), and \(_{}\), respectively. On Tiny-ImageNet, the GACL achieves impressive results with \(_{}\), \(_{}\), and \(_{}\) reaching 63.14\(\%\), 69.32\(\%\), and 62.68\(\%\), respectively, surpassing the previous best EFCIL by **13.97\(\%\)**, **21.39\(\%\)**, and **9.55\(\%\)**. Similar patterns are evident in the results of ImageNet-R, further confirming that the GACL is an exceptional tool for GCIL.

Owing to the weight-invariant property, the GACL exhibits more accurate and stable evolutions as \(k\) increases as observed in Figure 2 (a). All compared EFCIL methods exhibit sharp declines in accuracy, while the GACL delivers nearly non-declining curves. In particular, on CIFAR-100, the GACL shows an unnatural improvement of task-wise accuracy throughout the learning tasks, with the GACL initially lagging behind other EFCIL methods. This is because the Si-Blurry samples more than 70% of the CIFAR-100 categories in the first two tasks (see Appendix F), constructing a scenario where gradient-based algorithms could largely avoid the forgetting issue. Moreover, our method produces more stable predictions across diverse scenarios, as indicated by much smaller standard errors (colored shades in Figure 2 (a)). In summary, the experimental results demonstrate that our proposed GACL is exceedingly accurate and robust, exhibiting exceptional generalization ability.

Compare with Replay-based Methods.Replay-based methods are considerably competitive as they leverage historical samples. The memory size is a key adjustment, as increasing it typically leads to performance improvements by allowing more historical knowledge to be reviewed. For instance, the MVP-R achieves 4.42%, 3.97%, and 8.95% gains for \(_{}\), \(_{}\), and \(_{}\) (see Table 1) on CIFAR-100 when increasing the memory size from 500 to 2000.

Figure 2: The task-wise accuracy \(_{k}\) of the GACL with EFCIL methods (top) and replay-based methods (bottom) on benchmark datasets with the \(K=5\).

As an exemplar-free technique, our GACL avoids re-using the historical samples. However, as indicated in Table 1, the GACL still outperforms most existing replay-based results. For instance, the GACL achieves the best \(_{}\) results among all settings. The GACL's \(_{}\) and \(_{}\) results are also mostly superior, except that our performance is slightly weaker than that of MVP-R with a memory size of 2000 on CIFAR-100 and ImageNet-R. Although increasing the number of exemplars can further improve the results of replay-based methods, this approach could lead to higher training and memory costs and, more importantly, more severe privacy invasion.

As indicated in Figure 2 (b), replay-based methods experience accuracy declines similar to those observed in the EFCIL case. This decline is due to an inherent limitation of gradient-based iterative algorithms, which tend to favor recently trained categories and thus lead to catastrophic forgetting. The GACL is iterative-free and then not constrained by this forgetting issue, thereby achieving nearly no performance reduction as \(K\) increases.

Why the GACL Gives Leading Performance.The above comparisons show that the proposed GACL is a powerful GCIL technique. Its competitive performance can be explained as follows. (i) Weight-invariant property. As shown in Theorem 3.1, the weight obtained recursively is equal to its joint-learning counterpart, indicating that the GACL is a "completely non-forgetting" technique (under the condition of a frozen backbone). (ii) Analytical solution. Existing GCIL techniques are gradient-based iterative algorithms prone to catastrophic forgetting by nature. The GACL is a new member of the ACL and inherits its non-iterative gradient-free essence with an analytical solution, thereby avoiding the task-recency bias to address forgetting.

### Ablation Study on the ECLG Module

The ECLG module is a core component that allows the GACL to obtain the weight-invariant property in the GCIL scenario. Here, we conduct an ablation study to justify the ECLG's contributions under various blurry sample ratios \(r_{B}\) with \(_{}=50\%\). Larger \(_{}\) indicates more complex data distributions in the Si-Blurry setting. As shown in Table 2, the GACL without ECLG exhibits poor performance with a visible gap for \(_{}\), \(_{}\), and \(_{}\). For instance, on CIFAR-100 with \(r_{B}\) = 10%, the ECLG contributes a 23.01% \(_{}\) gain to the GACL.

As claimed in Theorem 3.1, the classifier without the ECLG module fails to absorb knowledge from joint classes in each task (i.e., classes that reappear), leading to substantial information loss under the GCIL setting. The GACL, equipped with the ECLG module, demonstrates competence in handling overlapping classes in realistic scenarios.

### Robustness Analysis in Si-Blurry Setting

Here, we conduct a robust analysis by varying the disjoint class ratio \(_{}\) and the blurry sample ratio \(_{}\). The comparison happens among the GACL, the second-best EFCIL method SLDA, and the top-performing replay-based method MVP-R with a memory size of 500.

   _{}\)} &  &  &  \\   & & \(_{}\)(\%) & \(_{}\)(\%) & \(_{}\)(\%) & \(_{}\)(\%) & \(_{}\)(\%) & \(_{}\)(\%) \\   & CIFAR-100 & \(_{}\) & \(_{}\) & \(_{}\) & \(45.68_{}\) & \(42.04_{}\) & \(47.30_{}\) \\  & ImageNet-R & \(_{}\) & \(_{}\) & \(_{}\) & \(40.29_{}\) & \(46.95_{}\) & \(41.67_{}\) \\  & Tiny-ImageNet & \(_{}\) & \(_{}\) & \(_{}\) & \(60.21_{}\) & \(65.80_{}\) & \(60.13_{}\) \\   & CIFAR-100 & \(_{}\) & \(_{}\) & \(_{}\) & \(42.53_{}\) & \(42.26_{}\) & \(45.49_{}\) \\  & ImageNet-R & \(_{}\) & \(_{}\) & \(_{}\) & \(42.01_{}\) & \(46.95_{}\) & \(41.67_{}\) \\  & Tiny-ImageNet & \(_{}\) & \(_{}\) & \(_{}\) & \(60.63_{}\) & \(57.03_{}\) & \(60.13_{}\) \\   & CIFAR-100 & \(_{}\) & \(_{}\) & \(_{}\) & \(40.91_{}\) & \(47.25_{}\) & \(58.61_{}\) \\  & ImageNet-R & \(_{}\) & \(_{}\) & \(_{}\) & \(40.44_{}\) & \(42.50_{}\) & \(39.05_{}\) \\   & Tiny-ImageNet & \(_{}\) & \(_{}\) & \(_{}\) & \(60.32_{}\) & \(60.70_{}\) & \(56.97_{}\) \\   

Table 2: Ablation study on the ECLG module of our GACL.

We evaluate our method under various \(_{ D}\), including extreme cases where each task shares classes (\(_{ D}=0\%\)) and traditional CIL scenarios (\(_{ D}=100\%\)). Table 3 illustrates that our GACL consistently outperforms the compared methods (e.g., leads the SLDA by 2%-10%) and produces near-identical \(A_{}\) values with varying \(_{ D}\). This shows the accurate and robust traits of the GACL.

We also evaluate our method using various \(_{ B}\) values, as shown in Table 4. Similar patterns observed here align with those in Table 3, further demonstrating the robustness of the proposed GACL, which delivers exceptional performance across different GICL settings.

### Limitation and Future Work

Overall, the GACL exhibits various good characteristics as an exemplar-free GCIL technique. The major limitation here is the need for a well-trained backbone because the GACL does not update backbone weights. This could motivate the exploration of adjustable backbones to continuously improve their feature extraction abilities, thereby further enhancing GACL's performance.

## 5 Conclusion

In this paper, we introduce the exemplar-free generalized analytic class incremental learning (GACL) approach to address the GCIL problem. Building upon analytic learning, the GACL delivers closed-form solutions to GCIL through the decomposition of GCIL data into exposed and unexposed classes. The GACL achieves the weight-invariant property that provides identical solutions for GCIL to its joint learning counterpart. We theoretically validate this property and provide high interpretability through the matrix analysis tool. Various experiments are conducted under the Si-Blurry setting, demonstrating that our proposed GACL achieves remarkable performance with high robustness compared to state-of-the-art EFCIL and replay-based methods.