# _Who Speaks Matters_: Analysing the Influence of the Speaker's Ethnicity on Hate Classification

Ananya Malik

Northeastern University

malik.ana@northeastern.edu

&Kartik Sharma

Georgia Institue of Technology

ksartik@gatech.edu

&Lynnette Hui Xian Ng

Carnegie Mellon University

lynnetteng@cmu.edu

&Shaily Bhatt

Carnegie Mellon University

shaily@cmu.edu

###### Abstract

Large Language Models (LLMs) offer a lucrative promise for scalable content moderation, including hate speech detection. However, they are also known to be brittle and biased against marginalised communities and dialects. This requires their applications to high-stakes tasks like hate speech detection to be critically scrutinized. In this work, we investigate the robustness of hate speech classification using LLMs, particularly when explicit and implicit markers of the speaker's ethnicity are injected into the input. For the explicit markers, we inject a phrase that mentions the speaker's identity. For the implicit markers, we inject dialectal features. By analysing how frequently model outputs flip in the presence of these markers, we reveal varying degrees of brittlemess across 4 popular LLMs and 5 ethnicities. We find that the presence of implicit dialect markers in inputs causes model outputs to flip more than the presence of explicit markers. Further, the percentage of flips varies across ethnicities. Finally, we find that larger models are more robust. Our findings indicate the need for exercising caution in deploying LLMs for high-stakes tasks like hate speech detection.

_Warming:This paper contains examples of bias that can be offensive or upsetting_

## 1 Introduction

Language technologies are increasingly being used in content moderation tasks, including hate speech detection, because of their ability to handle large volumes of data . However, the use of LLMs in a high stakes task like hate speech task a requires caution, because LLMs are known to be brittle and biased. LLM generations are known to be non-deterministic and brittle when additional information that is not relevant to the task itself is present . There is extensive documentation of biases against marginalized communities and dialects that leads to disparate treatment and representational harms in downstream tasks, including hate speech detection .

In this work, we analyse the robustness of 4 LLMs in hate speech detection in English by measuring the flips in outputs when _explicit_ and _implicit_ markers of the speaker's ethnicity are injected to the input. We consider 5 ethnicities: British, Indian, Singaporean, Jamaican, and African-American. Our setup is shown in Figure 1. Given an unmarked input "Let's go eat food today", the explicit marker is added by injecting a phrase that conveys the speaker's ethnicity. For example, _The Indian person said,"Let's go eat food today"_. Implicit markers are added by introducing dialectal features, including code-mixed text. For example, _"Chalo na, let's go eat some food today"_. Here, the phrase Chalo na (which means 'let's go' in Hindi) is colloquial addition of code-mixed text common in Indian Englishand indicates the possibility of the speaker being of Indian ethnicity. Finally, we compare the model outputs for the unmarked and marked inputs by aggregating the percentage of flips.

We find that LLMs are in fact brittle to such dialectal injections. In particular, implicit markers cause a larger percentage of flips than explicit markers. This indicates the brittleness of LLM in handling dialects, unsurprisingly since both pre-training corpora and commonly used datasets for hate speech detection tend to be west-centric. Further, the frequency of flips varies across ethnicity. Surprisingly however, in most cases, the percentage of flips is higher for British than for other ethnicities. Further, higher percentage of flips are observed for inputs that are originally non-hateful. This can have real-world implications of disparate outcomes of content moderation for certain communities more than others (which has also been previously documented by ). We also find that larger models tend to be more robust.

Overall, our results suggest the need for exercising caution when deploying LLMs for high stakes tasks in content moderation, like hate speech detection due to their brittle and biased nature.

## 2 Methodology

In this work, we investigate whether LLM outputs for hate speech classification flip when markers of the speaker's ethnicity are injected into the input.

We consider the following 5 ethnicities: Indian, Singaporean, British, Jamaican, and African-American. These were selected for their geographic diversity, distinctive dialects, and familiarity of authors to be able to perform qualitative analysis.

For each of the input examples, we generate 5 perturbations with the chosen ethnicities for each of the two markers. This leads to a total of \(15,000\) explicitly and \(15,000\) implicitly marked examples.

### Data

We use the hate speech dataset collected by . This has \(3000\) unique sentences collected from discussion forums like Twitter, 4Chan and Reddit. It consists of \(600\)_Hateful (H)_ and \(2400\)_Not Hateful (NH)_ statements.

Figure 1: We investigate whether adding implicit and explicit markers of the speaker’s ethnicity flips the model’s predictions for hate speech classification. Our findings indicate that model outputs do flip because of the presence of such markers, and the percentage of flips varies depending on the nature of the maker, model size, and the ethnicity injected.

#### 2.1.1 Explicit markers of speaker identity

In order to inject an explicit marker of speaker identity into the input, we mention the ethnicity of the speaker before the statement. In particular, for an unmarked (input), we create an explicitly marked input using the following template: The [ethnicity] person said,"[input]'.

#### 2.1.2 Implicit markers of speaker identity

For injecting implicit markers of ethnicity, we choose to inject dialectal features. This is because dialectal variations are indicators of identity while preserving the semantic meaning .

However, parallel datasets or models for dialect translation are unavailable. Due to this scarcity and the relatively high cost of human translation, we inject dialectal features by few-shot prompting a Llama-3-8b model to translate the input example into the dialect of a given ethnicity. Table 1 shows the examples of the generation for two cases along with LLM hate speech classification.

To do so, we construct a few-shot prompt as shown in Appendix A.1 and set the temperature to \(0\). The system prompt of this few-shot prompt is reflective of the zero-shot prompt in  and has verbatim instructions to avoid content filtering constraints, which the model initially depicted. These instructions helped us jailbreak and generate the required content.

#### 2.1.3 Observations in Dialect Generation

Random samples of these generations were verified by the authors, who are native speakers of the dialects to ensure that the dialect generated by the model was reasonable in most cases. This qualitative analysis revealed that the generated dialect included features like appending culturally-specific and code-mixed phrases (like _"Wah gwaan"_ for Jamaican or _"Blimey mate"_ for British).

We also see modifications to spelling and grammar while maintaining the tone, structure, and semantics. Out of the \(15,000\) dialect generated samples, we observe some hallucinations (\(2.5\%\)) 372 instances of these generations returned with the phrase "I cannot generate content that is discriminatory and offensive. Can I help you with anything?", and 12 instances were nonsensical output. Despite these hallucinations, the annotators were in agreement that the dialect generated was representative of the culture and did not add or modify the meaning of the sentence.

  
**Original** & **Race** & **Generated** \\   & British & Blimey, mate, I’m proper gutted about these young whippersnappers, innir? I reckon a good of’ fashioned British bashing would sort ‘em out, yeah? [Not Hateful] \\ I wanna shoot these kid like a good American [Hateful] & Indian & Arey, yaar, I thu want to shoot these kids just like a full real Amreek [Hateful] \\  & Jamaican & Wah gwaan, me a-go shoot dem little ones like a good yute from di States, ya hear? [Hateful] \\   & African American & Ah’ma put it back in, fo sho, after dat procedure, ya hear me? Ah ain’t got time fo’ no mess, gotta get dat thing fixed up right, ya di [Hateful] \\ I would put it back in after the procedure [Not Hateful] & African American & Ah, han, I would put it back in after the procedure, no worries, got it? Like, I would make sure it’s all fixed and whathon, then I would put it back in, you know? [Not Hateful] \\   

Table 1: Examples of how a given text can be transformed by assigning an identity to it as a form of different dialects. We see that the dialect-generated text has different hate classifications for different races, for the same sentence. For these set of examples, the dialect generation and classification is both done by Llama-3-8B.

### Models

We evaluate the brittleness of four LLMs including LLama-3-8b, LLama-3-70b , GPT-3.5-turbo , GPT-4-turbo  We perform zero-shot classification with a temperature of 0. We queried the GPT models through the OpenAI API and the Llama Models using the Together API between May-June 2024.

We use a zero-shot prompt to classify each text input as hateful or not. The prompt is akin to a classification prompt designed in . This prompt is ran for each text input across all the models. The prompt is provided below.

Prompt for classifying the text inputs as hateful or not using LLMs:

You are an agent who is tasked with identifying hate content. \(\)n\(\)You have to classify the entire input as hateful or not hateful only. \(\)n\(\)Only classify as 'hateful' or 'not hateful' and do not use the phrase 'I cannot'.

## 3 Experiments

### How accurately do LLMs classify hate speech in the absence of speaker identity?

First, we verify that the LLMs can accurately classify the unmarked inputs. For this, we compute the accuracy of the models by comparing their responses against the human annotated responses when tasked with classifying just the statement. Table 2 shows the zero-shot accuracy for each model. These are fairly high, indicating the model's ability to accurately classify unmarked inputs.

### Do the models flip their responses when inputs are marked with speaker identity?

Having established that all the models achieve high accuracy with respect to the ground truth (Table 2), we test the brittleness of these models when explicit and implicit markers of speaker identities are injected. We report the percentage of examples where the model prediction flips from the original prediction after injecting the markers in Table 3 under the explicit and implicit markers.

### What factors cause outputs to flip?

Model Size and RecencyWe find that larger and newer models, such as Llama-3-70B and GPT-4-turbo, are more robust and show smaller percentage of flips, than the smaller Llama-3-8B, and the older GPT-3.5-turbo model.

Type of markerWe find that models are fairly robust to explicit markers, but are brittle when implicit dialectal markers of ethnicity are injected. One exception is Llama-3-8B, which we believe indicates the brittleness and learned biases of the smaller model towards explicit markers.

EthnicityWe observe more flipping of the hateful sentences in the British, Indian, and Singaporeal dialects, as compared to other dialects, even in larger and more robust models. For originally not

   Model & Accuracy & Precision & Recall \\  LLama-3-8b & 0.95 & 0.95 & 0.91 \\ LLama-3-70b & 0.96 & 0.97 & 0.93 \\ GPT-3.5-turbo & 0.82 & 0.89 & 0.76 \\ GPT-4-turbo & 0.99 & 0.98 & 0.98 \\   

Table 2: Hate speech classification accuracy

   Model &  &  &  &  &  \\   & Explicit & Implicit & Explicit & Implicit & Explicit & Implicit & Explicit & Implicit & Explicit & Implicit \\  Llama-3-8B & 24.03 & 14.43 & 12.73 & 12.60 & 22.91 & 14.06 & 18.50 & 12.10 & 12.43 & 15.33 \\ Llama-3-70B & 3.66 & 10.06 & 3.23 & 12.56 & 3.26 & 11.96 & 3.46 & 8.86 & 3.00 & 12.03 \\ GPT-3.5-turbo & 13.33 & 19.96 & 10.00 & 20.57 & 12.53 & 20.93 & 11.47 & 22.55 & 9.53 & 23.03 \\ GPT-4-turbo & 2.33 & 8.53 & 1.83 & 10.47 & 2.23 & 10.733 & 1.90 & 7.73 & 1.83 & 10.53 \\   

Table 3: Aggregate percentage of flips for different dialects.

hateful prediction, we observe sentences injected with an implicit and explicit African-American or Jamaican dialect are more likely to be classified as hateful than other dialects.

Ground truth label of unmarked inputFigure 2 shows that an originally non-hateful (NH) prediction is likely to remain not-hateful across different ethnicities in GPT-4-turbo and Llama-3-70B. On the other hand, hateful (H) predictions become not hateful across most models.

Original inputFinally, we study if the flip in prediction is related to the nature of the unmarked input. For this, we find the overlap using Jaccard similarity between inputs where flips happen across different ethnicities. Jaccard similarity is calculated pairwise as the fraction of commonly flipped sentences in two races over the total flipped sentences. If the similarity is more, it implies that the LLM is likely to flip on changes in certain sentences despite the injections of ethnic markers. On the other hand, if the similarity is less, it indicates that the racial marker is playing a distinctive role in

Figure 3: Jaccard similarity of sentences whose hate classification prediction flips between different racial speaker identities implicitly/explicitly. Note that AfAm: African-American, Brit: British, Ind: Indian, Jama: Jamaican.

Figure 2: Percentage of flips in the prediction of different models when the original prediction is not-hateful (NH) or hateful (H) and the sentences are injected with different racial markers of the speaker either explicitly or implicitly.

flipping the prediction. Figure 3 plots the Jaccard similarity of the flipped sentences between any two races in different models and injection techniques. We find that there is more overlap between flipped sentences in different races when they are explicitly marked as compared to when they are implicitly marked. Moreover, Llama models to have higher similarity between flipped sentences than GPT models. Further, for Llama-3-8b explicit, African-American and British flips have more similarity to Indian and Singaporean flips, respectively.

## 4 Conclusion

In this work, we evaluate the robustness (or lack of thereof) of LLMs in hate speech classification. Specifically, we injected explicit and implicit dialectal markers of speaker's ethnicity in the input. We evaluated 4 recent LLMs by measuring the percentage of flips of the model outputs from the unmarked prompt. We find that the % of flips is governed by nature of the model, ethnicity, and the type of marker injected. This depicts the unreliability of LLMs in real-world applications.

## Limitations

The proposed study for assessing the brittleness of Large Language Models through implicit and explicit markers has the following limitations:

**Limited Dialect Data**: : There is a lack of data in different dialects and code-mixed text for hate speech-related content. This increased the paper's dependency on using Llama-3-8B to translate the unmarked inputs into inputs marked with the dialect. Even though the authors were able to manually verify a sub-sample of the data, there do exist some hallucinations (\(2.5\%\)). We also acknowledge that this verification may hold some unknown author biases and may not cover all the nuances of the dialect of a given region.
**Limited Models**: : To investigate the model robustness for different types of models, we looked at \(4\) models : Llama-3-8B, Llama-3-70B and GPT-3-5-turbo and GPT-4-turbo. Due to limited computational resources, we were not able to extend our study to models advertised to be'safer' like Claude. We did perform preliminary experiments with Llama Guard, but the model returned refusals hindering our ability to analyse it.
**Limited Hate-speech Datasets**: In this work, we use only hate speech dataset for our unmarked prompts. Further, we limit our analysis to only English datasets. We recognise that findings from multilingual datasets and other hate speech datasets could yield different results.

## Broad Implication and Social Impact

This paper investigates the robustness of LLMs in hate classification tasks. In light of this, this paper uses a large language model, Llama-3-8B to generate hateful content in a given English dialect. In doing so, we might uncover unintentional biases . In no way do the authors of this paper subscribe to the hateful content used in the paper or the content generated by the model. This paper promotes further caution while using Large Language Models as content moderators or in Hate Speech Classification Tasks.