# Large Language Models as Commonsense Knowledge for Large-Scale Task Planning

Zirui Zhao  Wee Sun Lee  David Hsu

National University of Singapore

{ziruiz, leews, dyhsu}@comp.nus.edu.sg

###### Abstract

Large-scale task planning is a major challenge. Recent work exploits large language models (LLMs) directly as a _policy_ and shows surprisingly interesting results. This paper shows that LLMs provide a _commonsense model_ of the world in addition to a policy that acts on it. The world model and the policy can be combined in a search algorithm, such as Monte Carlo Tree Search (MCTS), to scale up task planning. In our new LLM-MCTS algorithm, the LLM-induced world model provides a commonsense prior belief for MCTS to achieve effective reasoning; the LLM-induced policy acts as a heuristic to guide the search, vastly improving search efficiency. Experiments show that LLM-MCTS outperforms both MCTS alone and policies induced by LLMs (GPT2 and GPT3.5) by a wide margin for complex, novel tasks. Further experiments and analyses on multiple tasks--multiplication, travel planning, object rearrangement--suggest _minimum description length_ (MDL) as a general guiding principle: if the description length of the world model is substantially smaller than that of the policy, using LLM as a world model for model-based planning is likely better than using LLM solely as a policy.1

## 1 Introduction

Consider, for example, an autonomous robot butler in a household environment. The human user sits in the living room and asks the robot to "Put fruits into the fridge." The robot looks for fruits, such as apples, peaches, etc., which may be on a table in the dining room, on the kitchen counter, but unlikely in a wardrobe in the bedroom. To complete the task, the robot has to consider fruits' likely locations as well as their spatial relations, traverse these locations efficiently, and finally put them into the fridge. A household environment typically contains hundreds of movable items and locations, resulting in a huge search space that makes the task very challenging for the robot.

Recently, multiple attempts exploiting pre-trained large language models (LLMs) show surprisingly interesting results for such tasks . Their underlying idea is simple: treat the LLM as a policy and query it directly for the next actions, given the history of past actions and observations. We call this strategy _L-Policy_, which exploits LLMs' vast commonsense knowledge to circumvent the challenge of searching a very large space. In our example task, L-Policy may simply instruct the robot to move to the hallway and then to the kitchen. Even though LLMs are trained on internet-scale data, this strategy shows limits in generalization , especially when encountering uncommon, complex tasks. Alternatively, we may use LLMs' knowledge to build a world model and apply a planning algorithm to the model. The world model may contain, e.g., a belief over the target object's location, which biases the search and drastically improves search efficiency. We call this strategy _L-Model_. L-Model's performance depends on two critical preconditions: the accuracy of the world model and the efficiency of the planning algorithm. The former is a question of sample complexity for learning, and the latter is that of computational complexity.

This paper presents LLM-MCTS (shown in Fig 1), which combines the ideas of L-Model and L-Policy for large-scale task planning. Like L-Model, LLM-MCTS uses an LLM to build a commonsense world model; it then uses the model to perform Monte Carlo Tree Search (MCTS)  online for the next actions. During the tree search, LLM-MCTS chooses the promising action branches heuristically by querying the LLM. This is similar in spirit to L-Policy. While L-Policy commits to the actions chosen by the LLM for execution, LLM-MCTS uses these choices only as a search heuristic.

We evaluate LLM-MCTS in VirtualHome , a standard household activity simulation platform widely used in earlier work . The evaluation set consists of 800 randomly generated large-scale, partially observable object rearrangement tasks. In each task, a robot aims to fetch a common household item with an unknown location and place it in a designated container. Our main experimental findings are summarized below:

1. _L-Model performs poorly._ There are two possible reasons. One is model inaccuracy: the robot has an incorrect belief of the target object's location. The other is huge search space size, beyond the reach of even the state-of-the-art MCTS algorithm. Further experiments indicate that search space size is the main cause.
2. _L-Policy performs reasonably with both GPT2 and GPT3.5, but the performance degrades quickly for novel, complex tasks._ This generally corroborates with earlier results .
3. _LLM-MCTS outperforms L-Model._ This clearly shows the benefit of using the LLM as a heuristic policy to guide the search.
4. _LLM-MCTS outperforms L-Policy, especially for novel, complex tasks._ LLM-MCTS basically combines L-Model and L-Policy. Since the L-Model performs very poorly on its own, why does the combination outperform L-Policy? One explanation is that search space size is the main cause of L-Model's poor performance. The LLM-induced world model is sufficiently accurate; tree search with this world model, when limited to the neighbourhood of the LLM-induced policy, provides improved performance over L-Policy.

The explanation for (F4) bogs a new question: with an efficient planning algorithm for large search space, _would L-Model outperform L-Policy?_ To answer this question, we study two related, simpler tasks: multiplication of two large numbers and multi-hop travel planning. We discuss multiplication here and travel planning in Section 4.1. A decimal number is described as a sequence of \(n\) digits, \((d_{n-1},d_{n-2},,d_{0})\). There are two methods of implementing multiplication with an LLM. The first one corresponds to L-Policy. We represent the multiplication function as a table. Each row or column corresponds to a number. The table entry is the multiplication of two numbers, obtained by querying an LLM. Experimentally, GPT4 performs single-digit multiplication perfectly with \(100\%\) accuracy, 2-digit multiplication with \(99\%\) accuracy, 4-digit multiplication with merely \(4\%\) accuracy, and fails almost completely on 5-digit multiplication . The second approach uses LLM-derived small single-digit multiplication tables, which GPT4 performs with 100% accuracy. To multiply multi-digit numbers, it applies the long multiplication algorithm with the single-digit table. This method corresponds to L-Model. While long multiplication differs from planning in algorithmic details, it plays the same role in L-Model and is highly efficient. Clearly, this second method achieves \(100\%\) accuracy for arbitrarily large numbers, provided that the single-digit multiplication table is accurate. So, the L-Model outperforms L-Policy for the multiplication task, contrary to the finding for object rearrangement tasks.

_How do we choose between L-Model and L-Policy then?_ One idea is the minimum description length (MDL) principle. Theoretical analysis suggests that a hypothesis with a shorter description length has a smaller generalization error and is preferred . For multiplication, the method corresponding to L-Policy uses a large table of \(O(10^{2n})\) entries. It takes \(O(n10^{2n})\) bits to represent it. The method corresponding to the L-Model uses a single-digit multiplication table of constant size. The long multiplication algorithm can be encoded in any reasonable programming language with constant size. So, the total representation size is constant. According to MDL, the L-Model has a smaller generalization error than the L-Policy for multiplication, with sufficiently large \(n\). This is fully consistent with experimental results . The analysis of travel planning provides further evidence (Section 4.1).

In summary, LLM-MCTS combines the ideas of L-Model and L-Policy, outperforming either alone for complex task planning, particularly, object rearrangement (Sections 2 and 3). To choose between L-Model and L-Policy, MDL provides a useful guiding principle (Section 4). In essence, simplicity is preferred, a well-known general principle in machine learning.

## 2 LLM-MCTS: Monte Carlo planning with commonsense knowledge

We aim to solve task-planning problems in large-scale domains with partial observation. One example is object rearrangement tasks  in household environments. It is a meaningful and challenging problem with a large-scale and long-term planning horizon. It has many practical implications in everyday life , such as setting the table, tidying up the room, loading the dishwasher, etc.. To solve the problem, we present LLM-MCTS (shown in Fig 1), which combines the L-Model and L-Policy for large-scale planning. It uses LLM to build a commonsense world model to perform MCTS for reasoned planning and uses L-Policy to guide the MCTS and reduce the large search space.

### Task planning

We focus on task-planning problems with partial observation and large-scale domains. The problem can be formulated as a Partially Observable Markov Decision Process (POMDP): \((S,A,,T,O,R,)\). The state space \(S\) define the state of the robot and its environment. The action space \(A\) defines the action that the robot can do. \(\) is the observation space. \(T\) defines the transition function of states, which we assume to be given. \(O\) is the observation function that provides partial information about a state. \(R(s,a)\) is the reward function determined by the action \(a\) taken at the state \(s\). The discount factor is specified by \(\). The history trajectory \(h_{t}\) at time step \(t\) consists of a sequence of executed actions and received observations up to time \(t-1\), \(h_{t}=(o_{0},a_{0},o_{1},a_{1},,o_{t-1},a_{t-1})\). The objective is to find an optimal policy \(^{*}(h_{t})\) that maximize the expected cumulative rewards \(^{*}(h_{t})=_{a A}[_{i=0}^{}^{i} R(s_{t+i},a_{t+i})|a_{t}=a]\).

In this work, we focus on the object rearrangement task, though our approach is a general method for large-scale task planning. Object rearrangement is a representative embodied AI task  with various daily applications, such as setting the table, tidying up the room, loading the dishwasher, and more. It is a challenging task  as the robot must navigate, locate target objects and positions, and execute multi-step planning. As with hundreds of items and containers in the domain, identifying target objects can be challenging. Even with known state information, it requires long-horizon planning to achieve the goal. In addition, the vast number of domain objects leads to a large action space, given the actions are to interact with objects. The vast action space produces an exponentially large search tree, making the planning extremely challenging.

Following the approach of , we model the task as a POMDP as outlined above. The state \(S\) comprises variables denoting the positions of the robot, movable items, and containers. Actions \(A\) encompass five predefined actions from VirtualHome, parameterized by object/container/rooms: (1) _pick(object)_, where the robot collects an observed, proximate object; (2) _place(object,placement)_, allowing the robot to set a picked object nearby or inside an open container; (3) _open(container)_ and (4) _close(container)_, for interacting with an observed, nearby containers; and (5) _move(room)object/container)_, where the robot relocates within a room or near an observed object-t/container. Given our assumption of the robot's familiarity with house structures and the manageable size of the house, the robot can move directly to designated rooms. The deterministic transition \(T\) is pre-defined by actions. Partial observation \(O\) enables the robot to discern object/container positions within its room or an opened container at its location. The objective is to reorganize household items based on verbal instructions, represented by a reward of \(R\) for achieving the desired item arrangement.

Figure 1: Overview of LLM-MCTS. For each simulation in the MCTS, we sample from the commonsense belief to obtain an initial state of the world and use the LLM as heuristics to guide the trajectory to promising parts of the search tree.

### LLM as a commonsense world model

A commonsense prior belief of states can improve the effectiveness of object and location searches by prioritizing the search to appropriate locations. Our approach utilizes LLM's commonsense knowledge to generate the initial belief of states, which is updated with each action and observation in the real world. MCTS samples from the belief in simulation to estimate the value of the action.

**Initial belief of state.** We use object-centric state representation and categorize the objects in the house as moveable objects (e.g., apples), containers (e.g., fridge), and surfaces (e.g., kitchen table). The states of a moveable object might be inside the containers or on the surfaces. The containers and surfaces should be inside a room. Similar to [22; 25], we maintain the belief in object-centric graphs, where nodes are objects and edges describe abstract-level relationships (e.g., apples are inside fridge, fridge is inside kitchen) between objects and rooms. Details are in the Appendix C.2.

Assume a dataset \(\) is accessible, containing expert actions and observations in similar household environments to solve daily tasks. LLMs can use the observations in the data to know what are the objects in the house and predict their positions, forming the commonsense belief of the state. To achieve this, we find all the objects, containers, and surfaces that appeared in the dataset \(\) to form a list of objects \(_{}\) using a unique name for all of them. To approximate \(b(s_{0})\), we ask the LLMs to sample the positions of objects \(M\) times. For each sample, we ask the LLM to predict the position of objects using \(_{}\) and a fixed prompt. For instance, we ask LLM to complete "_The containers in the apartment are: fridge,...; The surfaces in the apartment are: kitchen counter,...; Question: what are the possible positions of strawberry? Answer: inside fridge, inside pantry...Question: what are the possible positions of apple? Answer:---_" We use three prompt examples to provide example formats of the response. The exact prompts we used are provided in the appendix. As the responses from LLM are free-form natural language, we have to precisely map those expressions to \(_{}\) for consistent state representation. Thus, we encode the names of objects in the LLM's response into embeddings using sentence-BERT \(f()\) and examine their cosine similarity to the unique name of objects in \(_{}\): \((e_{i},e)=)f(e)}{\|(f_{e})\|\|f(e)\|}\), where \(e\) is the name of objects, containers, or surfaces in the LLM's response, and \(e_{i}_{}\) are the unique names in the object list. We select the most similar expressions in \(_{}\) to form the sampled state. For example, when querying the position of an apple, the LLM's response is "on the kitchen table," we use the above technique to translate "the kitchen table" to "kitchentable," a unique name in \(_{}\).

**Goal.** Similar to , we use LLMs to translate the natural language goal into a formal goal for MCTS. We use a fixed set of prompt examples for LLM to interpret natural language goals, such as "put one apple into the fridge" is translated as a tuple "(_apple, inside, fridge_)." For compositional instructions, it will translate it into multiple tuples, such as "put one apple on the kitchen table and one plate inside the dishwasher" is translated as "(_apple, on, kitchentable_), (_plate, inside, dishwasher_)." We precisely map the LLM-generated goal into the admissible expressions in \(_{}\) for search using the same representation as the state. In MCTS, the goal is used to identify the reward. As the representations are the same, we can directly check whether the object's state is the same as the goal by string matching. If the goal is reached, it will receive a large positive reward, or 0 otherwise.

### LLM as a heuristic policy

We use LLMs to play the role of \((a|h)\) in PUCT to guide the action selection in the simulation procedure. In this procedure, the LLM takes as input the examples in the dataset, the goal description, the current observation, and the history of actions, and then outputs the suggested action plan (e.g., "_Next actions: move to the kitchen, open the fridge,..._"). Similar to , the observations and goal description are translated into English sentences. As the answer of LLM is from the conditional distribution of the following words given the context, it can also be viewed as a commonsense policy of actions to take conditioned on the context of tasks, observations, and completed actions. However, direct implementation and access to the probability value of the GPT-3.5 is not available. Thus, we propose an empirical policy distribution \(\) that uses sampling to approximate the policy distribution.

We sample the LLM for \(M\) times to approximate the policy probability distribution. For each sample, we query the LLM with prompt and trajectory history \(h\) and receive an answer of the following actions to take \(_{i}(h,)\), where \(_{i}\) is the first action of the answer. The prompt examples are retrieved from the dataset according to the similarity to the current language instruction \(\). We use  to translate the instructions in the dataset \(_{i}\) into embedding and examine theircosine similarity to the current instruction: \((_{i},)\). In experiments, we use a subset of \(\) to show its performance when restricted to a small training set. We select the top \(K\) similar instructions and use the corresponding expert trajectories as a \(K\)-shot prompt. However, the answer \(_{i}\) is a free-formed natural language sentence that cannot be mapped to admissible actions for the agent directly. To ensure that the action can be executed, we follow the method in prior works  to represent the actions and admissible actions by embeddings from  and evaluate their cosine similarity \((_{i},a)\). The empirical policy distribution is formulated as follows: \((a|h)=+(1-)\{_{i=1}^{M }(_{i},a)-\}\), where \(\) is the average value of \(_{i}(_{i},a)\) and \(|A|\) is the size of the admissible action space. \(\) is a hyper-parameter that adds randomness to the belief, as the sampled actions from LLM could be very deterministic. Therefore, the empirical policy distribution is a mixture of approximated policy from LLM and uniform distribution. The example prompts are provided in Appendix F.

### Monte Carlo tree search

We integrate the commonsense world belief and policy from LLM in MCTS, presented in Alg 1. For each simulation, MCTS samples a state from the belief \(b(s)\) at the root (line 4). It independently samples one position for each object to construct a state \(s\). This sampled state \(s\) is then employed in the simulation, generating a new tree trajectory. An action \(a^{*}\) is chosen during the simulation based on the \(Q\) value, visit counts, and LLM policy (lines 28 and 29). The observation and transition function, denoted as \(\) (lines 15 and 30), predict the next state \(s^{}\) given the selected action \(a^{*}\) and the sampled state \(s\), thus progressing to the subsequent step in the simulation (lines 30 and 31). When encountering leaf nodes in the tree, MCTS expands the tree and performs a random rollout for the corresponding node (lines 23 to 26). A uniform policy is employed to sample actions in the rollout, and the discounted reward is then returned (lines 14 to 17). Upon completing the task or reaching the maximum depth, the accumulated rewards are backpropagated, updating each node's estimated \(Q\) value (lines 32 to 35). Following \(N\) simulations, the output action is determined based on the estimated \(Q\) value (lines 3 to 8). Upon completion of the search process, the agent will execute an action and receive a new observation. For simplicity, we assume that the observation and transition functions are deterministic and known. In cases where an object is detected, its corresponding position within the belief will be updated with the observed position. Conversely, if the object remains undetected at certain positions, the belief regarding its presence in those positions will be rendered null, denoted by a zero value.

Experiments

### Experimental setup

**VirtualHome.** We proceed with our experiments in the VirtualHome , a large household simulated environment with a large domain, partial observations, and large action space. It contains hundreds of interactive items and containers with various types of rooms. It is a well-suited platform for evaluating embodied decision-making for solving daily tasks in household environments.

**Data.** To generate data for prompting and baseline training, we follow  to create 2000 tasks with randomly initialized scenes and expert trajectories. There are several settings for the evaluation. _Simple_ tasks are the tasks that only require the rearrangement of one item generated from the same distribution as the training dataset. _Comp._ refers to the composition of simple tasks in order to rearrange multiple objects sampled from the same distribution as the dataset (e.g., "Put plate on kitchen table and chicken inside fridge" is the composition of "put plate on kitchen table" and "put chicken inside fridge," ). The composition of tasks increases the planning horizon, making it more challenging to complete. In evaluation, we also use the _Novel Simple_ tasks with seen items (e.g., in the dataset, we have "put one plate on the kitchen table" and "put one chicken inside the fridge," and we use "put one plate inside the fridge" and "put one chicken on the kitchen table" to evaluate; these tasks are not included in the training dataset). For compositional tasks, we include _Novel Comp_ositional tasks, with 2 or 3 primary tasks composed, denoted as _NovelComp(2)_ and _NovelComp(3)_ (e.g., we have "put plate on kitchen table" and "put chicken inside fridge," in dataset but their composition "Put plate on kitchen table and chicken inside fridge" is not.) We also generate scenes at a _Novel Apartment_ for testing, where the distribution of object positions differs from the dataset.

The expert data are generated by an Oracle agent implemented in . The expert has the full knowledge of the environment (hence, does not need to understand where objects are likely to be placed) and uses handcrafted heuristics for completing various tasks. It uses regression planning to search for solutions to a task. We collect the actions and observations of the expert completing the tasks in the VirtualHome simulator as the dataset. There are 10,000 trajectories in total for training the baseline. To show the capability of LLMs when using a small training set, we only select 200 instances uniformly at random from the dataset as prompt candidates for the LLM model and policy when used in a few-shot mode with no fine-tuning. We also generated 800 tasks in total for evaluation.

**Evaluation.** We evaluate the success rate of completing the tasks within 30 steps, while a typical task can be finished within at most 15 steps. The task is considered successful if all the requirements of object positions are satisfied. For example, given the instruction "Put one apple inside the fridge," the task is successful if any apple is in the fridge. For simplicity, we don't consider the task of rearranging a very specific object, e.g., putting the leftmost apple in the fridge.

**Baselines.** We evaluate several baselines to compare. _UCT_: We use the UCT algorithm to conduct planning without commonsense knowledge and use the ground-truth reward function in simulation. We use uniform distribution as the initial belief for states of objects. It is to provide evidence that commonsense knowledge improves planning efficiency. _Finetuned GPT2 policy_: we use the training dataset with 10000 trajectories to fine-tune GPT-2 as the planning policy. This is to show that larger pre-trained LLM without fine-tuning outperforms the smaller model fine-tuned in specific tasks. _GPT3.5 Policy_: LLM takes as input the instructions and history of actions and the currently visible objects to generate the next action. We use the LLM as the policy only, with a few examples as prompts to interact with the environments. This baseline demonstrates the benefits of additional information from the commonsense model and algorithmic benefits from MCTS.

### Results

**Main result.** The main results of the experiments are shown in Table 1, reporting the success rate of our method

    & &  \\  Method & Simple & Comp. & NovelSimple & NovelComp.(2) & NovelComp.(3) \\  UCT  & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 \\ finetuned GPT2 policy  & 81.3\(\)2.4 & 59.0\(\)0.7 & 41.2\(\)7.1 & 30.9\(\)2.8 & 2.3\(\)1.5 \\ GPT3.5 Policy  & 83.4\(\)6.8 & 47.0\(\)7.8 & 74.4\(\)4.0 & 48.2\(\)8.8 & 5.4\(\)2.0 \\ GPT3.5-MCTS (Ours) & 91.4\(\)3.3 & 71.2\(\)6.2 & 88.1\(\)4.3 & 72.6\(\)6.9 & 33.6\(\)3.1 \\    
    & &  \\  Method & Simple & Comp. & NovelSimple & NovelComp.(2) & NovelComp.(3) \\  UCT  & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 \\ finetuned GPT2 policy  & 65.5\(\)3.4 & 39.9\(\)5.2 & 33.4\(\)6.4 & 12.8\(\)3.9 & 1.1\(\)0.9 \\ GPT3.5 Policy  & 74.3\(\)5.0 & 43.3\(\)4.0 & 67.8\(\)4.9 & 54.0\(\)3.0 & 6.9\(\)2.1 \\ GPT3.5-MCTS (Ours) & 82.9\(\)3.2 & 71.9\(\)5.6 & 79.3\(\)3.3 & 70.4\(\)6.4 & 38.8\(\)3.4 \\   

Table 1: Main results: mean \(\) standard error of success rate (%)and baselines in completing the tasks in VirtualHome environments. In this result, GPT3.5-MCTS outperforms all the compared baselines, especially for unseen situations. UCT works poorly in all conditions, as the poor model and the huge search tree make the planning intractable. Thus, we focus our discussion on comparing the finetuned GPT2 policy and GPT3.5 policy. For _Simple_, in-distribution tasks, the planning horizon is relatively short. Finetuned GPT2 policy, GPT3.5 Policy, and our method work reasonably well, but our method still outperforms the baselines. For _Novel Simple_ tasks, finetuned GPT2 policy works significantly worse than GPT3.5 Policy and GPT3.5-MCTS. This is because the fine-tuning of narrow tasks results in a biased distribution of the policy and compromises generalizability. GPT3.5 Policy and GPT3.5-MCTS work better due to the LLM's few-shot planning capability. GPT3.5-MCTS works better for both situations. It benefits from the MCTS' look-ahead search that explore possible states for potential outcomes in order to make reasoned decisions.

For the _Comp_ositional, in-distribution tasks, the finetuned GPT2 policy and GPT3.5 policy get significantly worse performance, while GPT3.5-MCTS works far better. The finetuned GPT2 policy is trained by behavior cloning that suffers from compounding errors. Therefore, when the planning horizon gets longer, the influence of the errors accumulates and compromises the overall performance significantly. As for GPT3.5 Policy, the longer horizon potentially introduces more errors during planning, which might not be included in the prompt examples. Without suitable guidance from prompt, we cannot guarantee the GPT3.5 Policy will carry out suitable replanning when encountering errors. MCTS encourages exploration to a certain extent of different possible actions during searching, introducing additional guidance to the GPT3.5 policy to look into other possible solutions. This is because the action selection procedure in GPT3.5-MCTS is not purely determined by GPT3.5 Policy but also by the \(Q\) value and visit counts. Thus, MCTS encourages GPT3.5 Policy to explore other possible search directions instead of excessively applying certain actions sampled by itself.

**Ablation study.** We conduct ablation studies to see the individual contributions of different components within the GPT3.5-MCTS framework. The _No Heuristic Policy_ version of GPT3.5-MCTS refers to the absence of PUCT guided by the GPT3.5 Policy for action selection. Instead, it solely relies on UCT with an initial commonsense belief derived from LLM. The variant employing the _Uniform State Prior_ utilizes a uniform prior belief regarding states, in contrast to the LLM-generated initial belief employed during the search process. Lastly, the variant operating in a _Fully Observable_ environment aims to assess the accuracy of LLM's knowledge in modeling the world.

Table 2 shows the results of our ablation study. The outcomes obtained under the _No Heuristic Policy_ version highlight the significance of heuristic policies in facilitating MCTS to conduct efficient searches for complex and large-scale planning tasks. Conversely, the results of the _Uniform State Prior_ row indicate that incorrect world models compromise search performance. This is because the model of the world determines the \(Q\) value. The wrong model results in an inaccurate estimation of the \(Q\) value, misleading the search process toward irrelevant locations. The _Fully Observable_ results demonstrate that GPT3.5-MCTS with perfect knowledge of the environment only slightly outperforms its counterpart without it, implying that the commonsense knowledge of LLM regarding world modelling suffices for practical purposes.

**Failure analysis.** Policy, model, and translation errors are the primary causes of failures. Among these, policy errors are responsible for the majority of the failures. Oftentimes, the policy produces unreasonable behaviours that mislead the search procedure. For example, it usually outputs inadmissible actions, such as "_walk to the cutleryfork_" where the "_cutleryfork_" is not in the observation. It also produces back-and-forth behaviours, resulting in an unreasonable heuristic and slowing the search procedure. For example, when putting objects inside the microwave, it is sometimes struck by repeatedly opening and closing the microwave. For model error, the predicted positions of objects are not always correct. Since a random rollout policy is employed, incorrect object states can result in higher \(Q\)-values than correct states, leading to misguided exploration. The wrong translation also

    &  \\   & Simple & Comp. & NovelSimple & NovelComp.(2) & NovelComp.(3) \\  GPT3.5-MCTS (No Heuristic Policy) & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 \\ GPT3.5-MCTS (Uniform State Prior) & 3.2\(\)1.1 & 0.0\(\)0.0 & 1.1\(\)0.4 & 0.0\(\)0.0 & 0.0\(\)0.0 \\ GPT3.5-MCTS (Fully Observable) & 94.0\(\)2.1 & 80.7\(\)3.3 & 94.3\(\)2.4 & 78.3\(\)4.0 & 34.0\(\)4.4 \\ GPT3.5-MCTS (Ours) & 91.4\(\)3.3 & 71.2\(\)6.2 & 8.81\(\)4.3 & 72.6\(\)6.9 & 33.6\(\)3.1 \\   \\   &  &  &  &  &  \\   & Simple & Comp. & NovelSimple & NovelComp.(2) & NovelComp.(3) \\  GPT3.5-MCTS (No Heuristic Policy) & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 \\ GPT3.5-MCTS (Uniform State Prior) & 1.1\(\)0.2 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 & 0.0\(\)0.0 \\ GPT3.5-MCTS (Fully Observable) & 85.1\(\)5.0 & 7.7\(\)3.2 & 82.5\(\)3.3 & 76.6\(\)3.1 & 37.9\(\)2.9 \\ GPT3.5-MCTS (Ours) & 82.9\(\)3.2 & 71.9\(\)5.6 & 79.3\(\)3.3 & 70.4\(\)6.4 & 38.8\(\)3.4 \\   

Table 2: Ablation Study: mean \(\) standard error of success rate (%)compromises the performance as we translate the response from LLM to admissible action or object names to ensure executability. This is partly caused by the VirtualHome environments, as the policy might not understand the underlying logic of the actions in VirtualHome, such as you have to walk close to interact with the object. Thus, if the LLM outputs "open fridge" but is not close enough to the fridge, the action will be translated to other admissible actions ("open fridge" is not inside the admissible actions for this case as it is invalid due to the setting of VirtualHome).

## 4 LLM as a model or a policy?

When would using LLM as a model outperform using LLM as a policy, and vice versa? We propose using the minimum description length (MDL) principle, also known as Occam's Razor from the philosophy of science, to gain insights into the issue. The MDL principle suggests choosing the method that has a shorter description when both methods fit the training data well. MDL has been formalized in various ways. One formal statement (from section 7.3 of ) is provided here:

**Theorem 4.1** (Occam's Razor).: _Let \(\) be a hypothesis class and let \(d:\{0,1\}^{*}\) be a prefix-free description language for \(\). Then, for every sample size, \(m\), every confidence parameter, \(>0\), and every probability distribution, \(D\), with probability greater than \(1-\) over the choice of \(S D^{m}\) we have that, \( h,L_{D}(h) L_{S}(h)+)/2m}\) where \(L_{S}(h)\) is the empirical loss of \(h\) on the \(S\), \(L_{D}(h)\) is the expected loss of \(h\), and \(|h|\) is the length of \(d(h)\)._

According to Theorem 4.1, we can bound the expected loss of a solution \(h\) by the description length \(|h|\) and the training loss \(L_{S}(h)\). We do not know the LLM training loss for using it as a model or as a policy, but for the purpose of gaining insights, it is reasonable to assume that they are both small. In that case, the MDL principle suggests selecting between a model or policy depending on which of them has the smaller description length given the description language.

Numerous caveats should be observed when using Theorem 4.1 to gain insights into the behaviour of LLM as a model and policy. The theorem assumes that the training data is independent and identically distributed (id), which is likely not true in practice. Nonetheless, the qualitative behaviour is often similar for non-id data. As the training data of GPT is unknown, when comparing the description length, we also assume that the training data for each subproblem is roughly the same. In addition, using the theorem to gain insights requires major assumptions on the predictor classes \(\) for model and policy; here, we assume that \(\) is the model (or policy) class that we are analysing and assume that LLM training using powerful approximators such as transformers has similar behaviour to training using the model (or policy) class. Finally, depending on how the model and policy are used, error propagation may need to be analysed separately.

Besides the multiplication example described earlier, we discuss an air travel planning task and the VirtualHome object rearrangement task examined earlier.

### Travel planning

Consider planning for air travel from a starting city to the destination city. To solve the problem through L-Model, we need to have the model - the direct flights out of each city - together with a shortest path algorithm. The model can be represented as a graph, which is likely to be sparse in the real world. For a sparse graph, an adjacency list will give a compact representation. Assuming that the total number of edges grows proportionally to the number of cities, \(O(n n)\) bits would be sufficient to describe a graph with \(n\) cities, with approximately \( n\) bits used to describe each city in the adjacency list structure. The shortest path algorithm can be described by any reasonable programming language with a constant size. The method regarding L-Policy can be represented as a 2-dimensional table, where each row and column denotes the current and destination city, and the table entry describes the next city to fly to on the shortest path from the current city to the destination. The next city in the table can be described with \( n\) bits. As such, with \(n\) cities in the rows and columns, there should be approximately \(n^{2} n\) bits in total. Thus, according to MDL, the L-Model has a shorter description length and should make fewer generalization errors than the L-Policy for travel planning with sufficiently large \(n\).

**Experiment.** We conducted experiments about planning for air travel from a starting city to a destination city, which we analyzed above. We utilized GPT-3.5 to generate flight paths between cities. We compare it to the GPT-3.5 model-based approach: we use GPT-3.5 to predict neighbouringcities connected by a direct flight, which feeds into the uniform-cost search (i.e., replace node expansion by GPT-3.5 as the world model).

We use the data from the Kaggle World cities database, select 68 cities with populations exceeding 5 million in different countries and 62 middle-size cities with populations between 500 thousand and 2 million, and use the Virtual Radar Server to get the flight routes dataset as ground truth. In our tests, we sampled 400 city pairs among large cities and 400 pairs among mid-size cities, evaluating path accuracy by verifying each direct flight exists. Paths were accepted if all flights were valid, even if they extended beyond our selected source and target cities. We evaluate the methods in two settings: predicting the flight route given two large cities and two mid-size cities.

**Results.** The main result shown in Fig 2 suggests that the LLM model + search algorithm consistently outperforms the LLM policy, supporting our analysis2. Furthermore, the performance gaps for mid-size cities are larger than for large cities. This is consistent with the fact that there are more mid-size cities than large cities (the gap between the description lengths of \(n^{2} n\) for policies vs \(n n\) for models grows with the number of cities). Note that performance decreases as the path length increases for both methods. As the number of predictions required for each path increases with path length, the probability of incorrect path prediction also increases.

### Object rearrangement task

Consider a house with \(n\) movable objects, \(m\) containers, and \(k\) rooms. If we use L-Model, we must describe the model and search algorithm (i.e., MCTS). The model can be represented as a sparse graph with objects, containers, and rooms as nodes, and weighted directed edges signifying a "located in" relation between the nodes, with the weights specifying the probabilities. Assume that each weight is specified with a constant number of bits. Each of the \(n\) objects can be located within the \(m+k\) containers or rooms, so each object edge would require approximately \((m+k)\) bits to describe. Each of the \(m\) containers can be located in \(k\) rooms, so each container edge would require approximately \((k)\) bits to describe. Assume further that the degree of each object and container node in the graph is bounded by a constant; the entire graph then requires \(O(n(m+k)+m(k))=O((m+n)(m+k))\) bits to describe. The MCTS algorithm should be described by a programming language with a constant size. For the L-Policy, tasks can be designated using object-container pairs. Each object-container policy can be defined by a sequence of actions, e.g. "walk to the fridge, open the fridge," until the object is found, followed by a sequence of actions until the destination container is found. Each action takes \(O(m+k)\) bits to describe. Assuming search sequences and the size of each object-container policy are bounded by a constant. Describing the policies for all \(mn\) object-container pairs requires \(O(mn(m+k))\) bits.

The composed tasks provide further challenges for L-Policy. Composing tasks increases the description complexity of policies to \(O((mn)^{N}(m+k))\), where \(N\) is the number of composed tasks if the composition is not exploited in the policies. For the L-Model, decomposition is automatically done in MCTS at the expense of more computation, whereas for the L-Policy, the LLM must learn to do the decomposition. This may make the problem of learning the decomposed policy computationally more difficult and less likely to be approximated by the LLM in practice.

This analysis indicates that the L-Model has a shorter description length than the L-Policy. According to the MDL principle, the L-Model will likely have a lower error rate than the L-Policy. However, in this case, we do not have an algorithm that is guaranteed to be efficient for solving L-Model. Instead, we use the L-Policy as a search heuristic to obtain a practically effective search algorithm. As predicted by the MDL principle, the search with the LLM-induced world model, when limited to the neighbourhood of the LLM-induced policy, provides improved performance over L-Policy.

### Discussion

While we have discussed problems where the L-Model outperforms the L-Policy, we would also expect the L-Policy to outperform the L-Model when the description length of policies is shorter than

Figure 2: Flight planning results.

the models. For example, when recommending a tourist itinerary for a city, the description length of the itinerary should be shorter than describing all the places of interest plus the reward and the length of time recommended for visiting each location. In such a case, the LLM may be able to do better in recommending an itinerary than in providing accurate information on all places of interest for planning itineraries.

In the multiplication problem, the model-based approach used a known efficient multiplication algorithm. In the air travel planning task, an efficient shortest-path algorithm is used. However, for the object rearrangement problem, we do not have an efficient search algorithm. In this case, we demonstrate another strength of LLMs - as a policy, it can be used as a heuristic for improving the efficiency of search algorithms.

## 5 Related work

Task planning has a long-standing history in the AI research community. In the early stage, many studies [1; 15; 14; 12] focused on task planning in a discrete state space with deterministic transitions. These methods are intractable for large-scale, long-horizon problems. Recently, researchers have used learning-based methods to learn planning policies directly  or to learn search heuristics to accelerate planning [32; 31; 41; 7; 29]. Those policies or heuristics are not generalizable to other unseen settings. Most recently, the pre-trained LLMs have been applied as few-shot policies for task planning [18; 2; 19]. However, the planning policy may not have good compositional generalizablity. In the robotics community, many studies proposed that task planning should be integrated with physical-level motion planning, i.e., task and motion planning (TAMP) [11; 12; 9]. This paper focuses on large-scale task planning with partial observations and large, object-centric domains, in which classical planning is intractable and learning-based methods require massive data.

Various approaches are proposed to scale up to large-scale planning problems. Initially, the Monte Carlo method [21; 8; 6] is proposed to tackle intractable large-scale problems using random sampling in the tree search. However, further scaling up to the problem with a large domain with sparse rewards requires massive sampling. Silver et al. [32; 31] integrate MCTS with deep learning to bias the action selection and reduce sampling. The idea has been successfully applied in large-scale planning scenarios [32; 31; 7; 27]. Most recently, studies show that LLM can be a few-shot open-loop [18; 2] and closed-loop [19; 37] planning policy in the large domain, while it may suffer from hallucinations. In this paper, we show that LLMs' commonsense knowledge can guide a search algorithm, reducing the search space sufficiently for practical planning and producing reasoned results.

How to use LLMs for a planning problem? LLMs have been used as a few-shot policy for language-conditioned task planning [18; 2; 19], or a policy for multi-step reasoning problems [38; 42]. However, recent research  also suggests that transformer LLMs are inherently limited for solving multi-step reasoning problems. In addition, some studies try to use LLMs as heuristics  or transition function  in MCTS, boosting the performance in coding or small-scale reasoning. However, the literature has not discussed utilizing LLMs as a world model in depth. We show the benefits of using LLM to model the world, as well as using MDL analysis to decide how to use LLM for planning problems.

## 6 Conclusion

We use LLMs as the commonsense world model and the heuristic policy within MCTS to achieve better-reasoned decision-making for daily tasks. MCTS enables LLM to leverage its world modelling knowledge for informed reasoning and explore new combinations of actions to tackle novel tasks. LLM helps MCTS through the biased sampling of states and actions, improving its efficiency in resolving complex task-planning problems. Our analysis and empirical evidence suggest that, for certain real-world domains, if the description length of the world is substantially shorter than policy, using LLM as a model in a model-based approach is a better option than using LLM as policy.

**Limitations.** The runtime of LLM-MCTS is currently hindered by multiple LLM calls. The details of runtime performance are in Appendix E. While our method requires multiple LLM calls, it provides substantially improved results. There are also various ways to enhance runtime performance like using smaller LLMs like Llama [35; 36] or distilling LLM's knowledge into a smaller model [30; 16; 23]. Those are interesting avenues for future research.

**Broader impact.** There might be concerns about the inherent biases of LLMs that may lead to unfair or risky decisions in some domains. Further study about the fairness and bias of LLMs' knowledge would be beneficial.