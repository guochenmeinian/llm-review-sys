# Robust Neural Contextual Bandit against

Adversarial Corruptions

 Yunzhe Qi, Yikun Ban, Arindam Banerjee, Jingrui He

University of Illinois at Urbana-Champaign

Champaign, IL 61820

{yunzheq2,yikunb2,arindamb,jingrui}@illinois.edu

###### Abstract

Contextual bandit algorithms aim to identify the optimal arm with the highest reward among a set of candidates, based on the accessible contextual information. Among these algorithms, neural contextual bandit methods have shown generally superior performances against linear and kernel ones, due to the representation power of neural networks. However, similar to other neural network applications, neural bandit algorithms can be vulnerable to adversarial attacks or corruptions on the received labels (i.e., arm rewards), which can lead to unexpected performance degradation without proper treatments. As a result, it is necessary to improve the robustness of neural bandit models against potential reward corruptions. In this work, we propose a novel neural contextual bandit algorithm named R-NeuralUCB, which utilizes a novel context-aware Gradient Descent (GD) training strategy to improve the robustness against adversarial reward corruptions. Under over-parameterized neural network settings, we provide regret analysis for R-NeuralUCB to quantify reward corruption impacts, without the commonly adopted arm separateness assumption in existing neural bandit works. We also conduct experiments against baselines on real data sets under different scenarios, in order to demonstrate the effectiveness of our proposed R-NeuralUCB.

## 1 Introduction

Contextual bandits refer to one specific type of multi-armed bandit (MAB) problems, where the learner can access the arm context information during the decision-making process. Contextual bandit algorithms have been commonly applied in various real-world applications, including online content recommendation , and medical experiments . While these algorithms have been proved effective for numerous online learning tasks, they can be susceptible to the malicious feedback from the environment, such as malicious user feedback in recommender systems , and corrupted labels under active learning settings . This can potentially impair the model performance and interfere with the internal decision-making logic. One renowned research direction formulates this problem as _contextual bandits with adversarial corruptions_, where received _arm rewards_ can be potentially "corrupted" by the unknown adversary. In this case, bandit algorithms need to be robust against such adversarial corruptions, otherwise they can lead to sub-optimal results. Existing works on contextual bandits with corruptions are mainly based on linear  and kernelized bandits , where the unknown reward mapping function is assumed to be linear, or lies in a specified Reproducing Kernel Hilbert Space (RKHS). However, one key challenge is that these assumptions can evidently fail under real-world application scenarios , when we have little prior knowledge regarding this mapping function, or it becomes increasingly complex.

In the face of this challenge, neural bandit algorithms  have been proposed to relax the assumptions on reward functions. By leveraging the representation power of neural networks, neural contextual bandit algorithms are able to deal with complex reward functions irrespective of whetherthey are linear or non-linear, along with suitable exploration strategies for tackling the exploitation-exploration dilemma [5; 59]. While neural bandit algorithms have been proved effective [9; 68; 60], they can be sensitive to adversarial corruptions as well. From perspectives of trustworthiness, it is well known that neural models can be susceptible to "label attacks" [71; 62; 65], which is akin to reward corruptions in bandit settings. Failure to comply with robustness requirements can impair the feasibility under real-world application scenarios like recommender systems [82; 27; 87; 78], and therefore it is necessary for neural bandit methods to be robust against potential adversarial corruptions. Furthermore, existing neural bandit works generally require _arm separateness assumptions_ (e.g., assuming a positive-definite Neural Tangent Kernel [NTK] Gram matrix [86; 84; 26], or positive arm Euclidean distances [11; 67]), which will require no duplicate arm contexts are observed (or chosen) by the learner. This can lead to additional vulnerabilities when arm contexts are intentionally chosen by the adversary (e.g., assigning duplicate arms in the candidate pool across different rounds), making the arm separateness assumption fail in such adversarial environments.

Motivated by aforementioned challenges, in this paper, we propose a novel neural contextual bandit algorithm named Robust Neural-UCB (R-NeuralUCB), which can model the discrepancy among candidate arms and adopt arm-specific context-aware Gradient Descent (GD) to enhance model robustness against reward corruptions. Instead of applying ordinary GD to update the network parameters, R-NeuralUCB utilizes a fine-grained GD strategy by modeling the importance level of training samples, to reduce the impact of potential adversarial reward corruptions. Meanwhile, to improve the model performance from both the theoretical and empirical perspectives, R-NeuralUCB simultaneously perceives the uncertainty levels of candidate arms, and adaptively customizes network parameters for each of these candidates. To deal with the exploitation-exploration dilemma, R-NeuralUCB is equipped with an informative exploration mechanism based on Upper Confidence Bound (UCB) to achieve principled exploration. In addition, we present regret analysis without the commonly adopted _arm separateness assumption_, which reinforces R-NeuralUCB's theoretical robustness under adversarial scenarios. Our contributions can be summarized as follows:

* **Problem Settings and Proposed algorithm:** We study a novel neural bandit problem, where the received arm reward can be potentially corrupted by the unknown adversary. To deal with this problem, we propose a novel neural bandit algorithm called R-NeuralUCB, which leverages a refined context-aware Gradient Descent training strategy to improve the model robustness against potential arm reward corruptions. While we consider all the observed arms are governed by the same unknown reward mapping function as in (1) similar to existing neural bandit works, our R-NeuralUCB interestingly maintains separate model parameters specific to the different candidate arms, for a fine-grained way of improving the robustness. This casts lights on our contributions of novel algorithmic designs, compared to related existing methods without an arm-specific modeling (e.g.,  and our base algorithm NeuralUCB-WGD in Appendix E).
* **Theoretical Analysis:** With over-parameterized neural networks, we present the regret analysis for R-NeuralUCB. Given finite horizon \(T\), effective dimension of NTK Gram matrix \(\), and corruption level \(C\), R-NeuralUCB enjoys a data-dependent regret bound of \(}(+C)\). In addition, to ensure R-NeuralUCB is capable of handling contexts specified by the adversary (e.g., duplicate arms across different rounds), our analysis removes the _arm separateness assumption_ dependency, a widely adopted assumption for neural bandit literature, which can be of independent interest.
* **Experiments:** We conduct experiments on publicly available real-world data sets with various specifications. Under different types of reward corruptions, our R-NeuralUCB can achieve better performance, and is less vulnerable to reward corruptions than baselines.

## 2 Related Works

**Contextual Bandits with Adversarial Corruptions.** To begin with, there have been numerous studies [76; 17; 29; 57; 63; 31; 22; 54] working on tackling adversarial reward corruptions under linear contextual bandit settings [59; 24]. A related topic is bandits with mis-specifications [36; 55; 33; 53; 53; 83; 75], where the deviation of reward estimation comes from problem modeling instead of the adversary. On the other hand, kernelized bandits [15; 40; 16] extend the adversarial corruption problem to non-linear cases by assuming the reward mapping is a functional in the specified RKHS , while comparable ideas are also applicable for robust Bayesian Optimization . Adversarial corruptions are also studied for other formulations, such as Lipschitz bandits [49; 89] and MAB without contexts [18; 80]. However, compared with neural bandit methods, these works generally require assumptions on the reward function prior, which may not be satisfied in real-world scenarios.

**Neural Contextual Bandits.** Neural contextual bandits algorithms are proposed to leverage the representation power of neural networks, and relax the assumptions on the reward mapping functions that can be linear or non-linear. Neural-UCB  applies a fully-connected (FC) neural network for reward estimation and utilizes corresponding network gradients for principled exploration. Comparable ideas have been leveraged by other neural bandit works [84; 50; 25; 10; 37; 46; 9; 60; 11], and adopted under various application scenarios such as active learning [74; 13; 6], and bandit-based graph learning [66; 67; 51] with graph neural networks [77; 34; 35]. Alternatively,  utilizes the neural network to embed original arm contexts for regression.  utilizes inverse reward gap for exploration, and  achieves exploration with the reward perturbation. However, as these methods are not designed to defend against reward corruptions and widely require arm separateness assumptions, they can fail to meet the robustness requirements in an adversarial environment.

## 3 Problem Definition

Let \(T\) be the finite horizon. In round \(t[T]\), the learner receives \(K\) candidate arms \(_{t}\), \(|_{t}|=K\), and each arm \(_{i,t}_{t}\) with arm index \(i[K]\) is described by a \(d\)-dimensional vector \(_{i,t}^{d}\). The learner will then choose one arm \(_{t}_{t}\) and receive its reward \(r_{t}\). The index of \(_{t}\) is denoted by \(i_{t}[K]\), s.t. \(_{t}=_{i_{t},t}\). Here, similar to existing works (e.g., [42; 16; 15; 86; 84]), we define _corruption-free_ arm reward \(_{i,t}\) for each candidate arm \(_{i,t}_{t}\), as well as _corrupted_ arm reward \(r_{t}\) for chosen arm \(_{t}_{t}\), as

\[_{i,t}=h(_{i,t})+_{i,t}, r_{t}=_{t}+c_{t}=h(_{t})+_{t}+c_{t},\] (1)

where \(h:^{d}\) is an unknown reward mapping function that can be either linear or non-linear. \(_{i,t}\) stands for zero-mean \(\)-sub-Gaussian random noise which is standard for stochastic contextual bandit works (e.g., [24; 72; 86]), and \(c_{t}\) is the unknown adversarial corruption imposed by the adversary. While kernelized bandit works (e.g., [16; 72]) assume \(h()\) belongs to the RKHS induced by specified kernels, we alternatively consider \(h()\) as an arbitrary unknown function, and utilize the neural model to learn this mapping with flexibility.

Taking expectation w.r.t. zero-mean noise \(\), for the chosen arm \(_{t}_{t}\), we denote its expected perturbed reward \([r_{t}]=h(_{t})+c_{t}\); meanwhile, for each candidate arm \(_{i,t}_{t}\), its expected corruption-free reward \([_{i,t}]=h(_{i,t})\). Here, we consider \([r]\) and \([]\) both fall into value range \(\), analogous to existing works (e.g., [86; 84; 11; 50]). This is intuitive as numerous real-world applications work with bounded rewards (e.g., online recommendation tasks with normalized rating  or binary feedback ); and the adversary also needs its attack to be stealthly, by ensuring perturbed rewards fall into the normal value range. With previously chosen arms \(\{_{}\}_{[t]}\) up to round \(t\), we denote received context-reward tuples with _perturbed rewards_ as \(_{t}:=\{_{},_{}\}_{[t]}=\{_{i_{}, },r_{i_{},}\}_{[t]}\), and the corresponding context-reward tuples with _corruption-free rewards_ as \(}_{t}:=\{_{},_{}\}_{[t] }=\{_{i_{},},_{i_{},}\}_{[t]}\), where each corruption-free but imaginary unobserved reward is \(_{}=h(_{})+_{},[t]\) based on (1).

**Learning Objective.** Our objective is to minimize cumulative pseudo-regret for \(T\) rounds:

\[R(T)=_{t=1}^{T}[_{t}^{*}-_{t}],\] (2)

where \([_{t}]=h(_{t})\) is the expected corruption-free reward of the chosen arm \(_{t}_{t}\), and \([_{t}^{*}]=_{_{i,t}_{t}}[h( _{i,t})]\) stands for that of the optimal arm \(_{t}^{*}_{t}\).

**Corruption Level.** If the adversary determines the reward corruption \(c_{i,t}\) for each candidate arm \(_{i,t}_{t}\) beforehand, without observing the learner's choice \(_{t}\), some works (e.g., ) formulate the corruption level measurement as \(C^{}=_{t[T]}[_{i[K]}|c_{i,t}|]\). In this work, similar to [42; 16], we alternatively consider reward corruptions are determined w.r.t. particular chosen arms \(\{_{t}\}_{t[T]}\), and formulate the corruption level as \(C=_{t[T]}|c_{t}|\). This leads to \(C C^{}\).

## 4 Proposed Algorithm: Robust Neural-UCB (R-NeuralUCB)

Recall that in (1), arm rewards under neural bandit settings are governed by the unknown reward mapping function \(h()\), where \(h()\) can be an arbitrary function. For our proposed R-NeuralUCB, we adopt a neural network \(f()\) to approximate \(h()\) for reward estimation.

**Network Structure.** We use \(f(;)\) to denote an FC network with depth \(L 2\) and width \(m^{+}\):

\[f(;):=_{L}(_{L-1}(_{L-2}(_{1})))\] (3)where \(()\) is element-wise ReLU activation, and we have trainable weight matrices \(_{1}^{m d}\), \(_{l}^{m m}\), \(2 l L-1\), \(_{L}^{1 m}\). For the ease of notation, we denote vectorized parameters

\[:=[(_{1})^{},(_ {2})^{},,_{L}]^{}^{p},\]

with the dimensionality of \(p\), and randomly initialized parameters are denoted by \(_{0}\). Then, we let \(g(;)=(_{}f(;)) ^{p}\) be vectorized network gradients w.r.t. input \(\) and parameters \(\).

We motivate our proposed R-NeuralUCB by first mentioning a base algorithm named NeuralUCB with Weighted GD (NeuralUCB-WGD), which is elaborated in Appendix E. To begin with, NeuralUCB-WGD measures the uncertainty level of training samples (i.e., previously received arm-reward pairs) through their UCB values, as the UCB essentially measures _arm uncertainty levels_ in terms of reward estimation [24; 72; 86]. Then, different from conventional neural bandit methods that treat all training samples equally [86; 84], inspired by , NeuralUCB-WGD utilizes a weighted GD process to train neural model \(f()\) for estimating arm rewards, where training samples with high uncertainty levels will be downplayed. The main idea is that although we do not know which training samples are corrupted, we instead aim to reduce potentially severe impacts caused by adversarial corruptions, by paying relatively more attention on the training samples (arm-reward pairs) with low uncertainty levels, for a stable GD training process. We also present corresponding regret analysis for NeuralUCB-WGD in Appendix E.2, as well as experiments in Section 6.

However, notice that the neural model will also perceive varying uncertainty levels for different candidate arms in terms of reward estimation. In this case, simply applying the same exploitation-exploration strategy across all candidate arms can overlook this discrepancy, leading to insufficient granularity w.r.t. reward estimation. For instance, regarding candidate arms with low uncertainty levels, it can be more beneficial to adequately leverage existing training samples for estimating their rewards, instead of sharing an identical exploitation-exploration strategy with other high-uncertainty candidate arms. Meanwhile, analogous to existing works (e.g., [42; 16; 15]), NeuralUCB-WGD supposes a known corruption level \(C\) for regret analysis (Theorem E.1), which can be difficult to satisfy if we have limited knowledge regarding the unknown adversary. With the above motivations, we propose R-NeuralUCB as a refined solution to further enhance neural model robustness against potential reward corruptions. For readers' reference, we also compare our proposed R-NeuralUCB and NeuralUCB-WGD with some regret results from existing works in Table 1.

### R-NeuralUCB: Robust Neural-UCB

Our R-NeuralUCB formulates a novel context-aware GD process, by taking the uncertainty information of both candidate arms and training samples into account, for neural network training and decision making. Here, R-NeuralUCB customizes individual sets of network parameters \(_{i,t-1}\) for each candidate arm \(_{i,t}_{t},i[K]\), before the actual arm recommendation. Afterwards, these arm-specific networks are applied for arm reward estimation, along with an informative arm-specific UCB-based exploration mechanism. The pseudo-code is presented in Algorithm 1.

**Arm Weight Formulation.** Inspired by NTK-based exploration mechanisms [86; 51; 66; 11; 84], we measure arm uncertainty levels with the weighted gradient norm of arms. With a regularization parameter \(>0\), we first define a weight-free gradient covariance matrix \(}_{t-1}=+_{[t-1]}g(_{}; _{-1})g(_{};_{-1})^{}/m\). Here, \(_{-1}\) is the shorthand of \(_{i_{},-1}\), representing the network parameters of the previously chosen arm \(_{}=_{i_{},},[t-1]\). Then, for each

  
**Algorithm** & Reward Function & Corruption \(C\) & Regret Bound* \\  Robust OFUL  & Linear & Known & \(}(d+^{T}c^{2}})\) \\ CW-OFUL  & Linear & Known & \(}(d+Cd)\) \\ COBE + OFUL  & Linear & Unknown & \(}(d+^{T}c^{2}})\) \\ CW-OFUL (\(=\))  & Linear & Unknown & \(}(d)\), if \(C\). Otherwise \((T)\) \\  Fast-slow GP-UCB  & Kernelized & Known & \(}(+C)\) \\ RGB-PE  & Kernelized & Known & \(}(+C^{}{{2}}})\) \\ Fast-slow GP-UCB  & Kernelized & Unknown & \(}(+C)\) \\  NeuralUCB-WGD (Thm. E.1) & Arbitrary & Known & \(}(+C^{}{{2}}})\) \\ R-NeuralUCB (Thm. 5.6) & Arbitrary & Unknown & \(}(+C^{}{{2}}})\) \\   

* \(d\): context dimension; \(\): NTK matrix effective dimension or kernel information gain; \(\): data-dependent gradient deviation term.

Table 1: Comparison of \(T\)-round regret bounds with adversarial corruption level \(C\).

candidate arm \(_{i,t}_{t}\), we formulate its weight w.r.t. previously chosen arm \(_{},[t-1]\) as

\[w_{i,t}^{()}=\{1,\ _{t} }\|g(;_{t-1})/\|_{}_{t-1}^{-1}}^{2} }{g_{}\|g(_{i,t};_{t-1})/\|_{(}_{t-1}^{()})^{-1}}}\},\] (4)

with \(^{2}\)-scaled covariance matrix being \(}_{t-1}^{()}:=+^{2}_{ [t-1]}g(_{};_{-1})g(_{};_{ -1})^{}/m\), for a constant \((0,1)\). Alternatively, we also denote \(w_{i,t}^{()}=\{1,_{}(_{i,t}; _{t},}_{t-1})\}\), with \(_{}()\) being a shorthand that integrally represents the fraction term in (4). A tunable parameter \(>0\) and the squared round-wise minimum weighted norm \(_{_{t}}\|g(;_{t-1})/\|_{}_{t-1}^{-1}}^{2}\) in the numerator are applied for scaling purposes. We also include complementary discussions for arm weight scaling in Appendix B.5. Meanwhile, we have \(g_{}=\|g(_{};_{-1})/\|_{(}_{t-1}^{()})^{-1}},[t-1]\) quantifying uncertainty levels of previously chosen arms (training samples) motivated by UCB-based exploration strategies (e.g., ). Since previous \(\{g_{}\}_{[t-1]}\) values can be reused, we only need to compute and store \(g_{t}\) for current round \(t\). As a result, if candidate arm \(_{i,t}\) is of high uncertainty (i.e., large \(\|g(_{i,t};_{t-1})/\|_{}_{t-1}^{-1}}\) value), its arm weights \(\{w_{i,t}^{()}\}_{[t-1]}\) will become small.

```
1:Input: Time horizon \(T\). GD iterations \(J\). Learning rate \(\). Exploration coefficient \(\). Scaling parameter \(\). Norm parameter \(S\). Regularization parameter \(\).
2:Initialization: Parameters \(_{0}\). Weight-free covariance matrix \(}_{0}=\). Records \(_{0}=\).
3:for each round \(t[T]\)do
4: Observe a collection of \(K\) candidate arms \(_{t}=\{_{i,t}\}_{i[K]}\).
5:for each candidate arm \(_{i,t}_{t}\)do
6:if\(t\) equals to \(1\)then
7:\(_{i,t-1}_{0}\).
8:else
9: With arm weights \(\{w_{i,t}^{()}\}_{[t-1]}\) in (4), train parameters \(_{i,t-1}\) with GD and the arm-specific loss function in (5) based on received records \(_{t-1}\).
10:endif
11: For candidate arm \(_{i,t}\), calculate its benefit score \(U(_{i,t})\) based on (6).
12:endfor
13: Choose arm \(_{t}=_{_{i,t}_{t}}[U(_{i,t})]\) with the highest benefit score.
14: Receive arm reward \(r_{t}\), and update the records, such that \(_{t}_{t-1}\{(_{t},r_{t})\}\).
15: Update the shorthand \(_{t-1}_{i_{t},t-1}\), and matrix \(}_{t}}_{t-1}+g(_{t}; _{t-1})g(_{t};_{t-1})^{}/m\).
16:endfor ```

**Algorithm 1** Robust Neural-UCB (R-NeuralUCB)

**Model Training with Context-aware GD.** According to line 9 in Algorithm 1, we perform model training _before_ the actual arm recommendation in each round \(t\{2,,T\}\). For each candidate \(_{i,t}_{t}\), we train its arm-specific parameters \(_{i,t-1}\) with \(J\) iterations of GD and received records \(_{t-1}=\{(_{},r_{})\}_{[t-1]}\). Starting from initialization \(_{i,t-1}^{(0)}=_{0}\), we have \(j\)-th GD iteration (\(j[J]\)) being \(_{i,t-1}^{(j)}=_{i,t-1}^{(j-1)}-_{} _{i,t}(_{t-1};_{i,t-1}^{(j-1)})\), where \(>0\) refers to the learning rate. We formulate a loss function \(_{i,t}(;),i[K]\) specified to candidate arm \(_{i,t}_{t}\) as

\[_{i,t}(_{t-1};)=_{(_{},r _{})_{t-1}}^{()}}{2}f(_{ };)-r_{}^{2}+\|- _{0}\|_{2}^{2},\] (5)

where the \(L_{2}\) loss is scaled by arm weights \(w_{i,t}^{()},[t-1]\) from (4). Intuitively, if arm weights \(w_{i,t}^{()}\) are large (i.e., low uncertainty level), we proceed to train a neural model that adequately fits the collected training data (i.e., previously received records) \(_{t-1}\), instead of staying around the random initialization \(_{0}\) given the \(L_{2}\) regularization. On the other hand, if arm weights \(w_{i,t}^{()}\) are small, it means that the uncertainty level in terms of reward estimation is high. In this case, we prefer being relatively conservative to prevent potentially large impacts caused by adversarial corruptions. As a result, R-NeuralUCB will focus more on the training samples in \(_{t-1}\) with low uncertainty levels, and stay relatively close to the random initialization \(_{0}\) due to the regularization term in (5).

In practice, instead of starting from \(_{0}\) in each round \(t\{2,,T\}\), we can alternatively initiate the GD process from the existing trained parameters to reduce computational cost, inspired by the concept of warm-start GD . Here, we can start from \(_{t-2}\), the parameters of the previously chosen arm \(_{t-1}\), and fine-tune arm-specific parameters \(_{i,t-1}\) for each candidate arm \(_{i,t}_{t}\), based on its loss function \(_{i,t}()\) and a small batch of samples from \(_{t-1}\). Further details are elaborated in Appendix B.6, and this approach is also applied for the experiments in Section 6.

**Arm Selection.** For candidate arm \(_{i,t}_{t}\) and arm weights \(\{w^{()}_{i,t}\}_{[t-1]}\), we formulate its arm-specific gradient covariance matrix \(_{i,t-1}=+_{[t-1]}w^{()}_{i,t} g (_{};_{-1})g(_{};_{-1})^{ }/m\). Here, if the variance proxy value \(\) in (1) is unknown, similar to existing works (e.g., [86; 84]), we deem \( 0\) as a tunable parameter to control the exploration intensity. With our UCB-type exploration motivated by Appendix Lemma C.11, we formulate the benefit score for arm \(_{i,t}_{t}\) as

\[U(_{i,t})=f(_{i,t};_{i,t-1})+_{i,t-1}_{i,t};_{i,t-1})^{}_{i,t-1}^{-1}g(_ {i,t};_{i,t-1})/m},\] (6)

where the confidence coefficient \(_{i,t-1}=(_{i,t-1})}{ ()}-2()}+S)\), along with a constant \(>0\) from Lemma C.11. Afterwards, we choose \(_{t}=_{_{i,t}_{t}}[U(_{i,t})]\) [line 13, Algorithm 1], based on calculated arm benefit scores in (6). After receiving reward \(r_{t}\), the collected records will be updated by \(_{t}_{t-1}\{(_{t},r_{t})\}\) (line 14, Algorithm 1). We also update the shorthand for model parameters of the chosen arm as \(_{t-1}_{i_{t},t-1}\), and the weight-free covariance matrix \(}_{t}}_{t-1}+g(_{t};_{t-1})g(_{t};_{t-1})^{}/m\) for next round \(t+1\) (line 15, Algorithm 1).

In summary, the primary goal of R-NeuralUCB is to customize individual learning objectives (i.e., loss functions) for different candidate arms by leveraging arm uncertainty information before pulling an arm. For candidate arms with high uncertainty levels, the neural model may lack confidence in estimating rewards based on current records, due to potential reward corruptions, which can lead to significant estimation errors. In this situation, by using the regularization term \(\|-_{0}\|_{2}^{2}\), we prefer to adopt a relatively conservative approach, training a model close to random initialization to mitigate the potentially large impacts of adversarial corruptions. This approach is inspired by existing work on enhancing model robustness through regularization techniques (e.g., [69; 23]). On the other hand, for candidate arms with low uncertainty, we aim to train neural models that fully utilize the received records for reward estimation. Since the model is confident in its estimation, the received samples can provide adequate reference. With larger arm weights, the loss function can focus more on the training samples, instead of staying closely around \(_{0}\).

## 5 Theoretical Analysis

To the best of our knowledge, we provide the first theoretical results under the neural bandit settings with adversarial reward corruptions, and our proof flow is distinct from those of linear and kernelized bandit works. In particular, as our ReLU activation in (3) is not Lipschitz smooth [3; 21], it leads to additional challenges for our theoretical analysis, since a small perturbation on rewards can lead to drastic changes of network gradients. As a result, even with a small corruption level \(C\), the corrupted model parameters trained by GD can significantly deviate from the imaginary network parameters trained with corresponding corruption-free rewards. Therefore, it is non-trivial to quantify corruption impacts from theoretical perspectives, which simultaneously makes our proof flow differ significantly from that of the vanilla Neural-UCB . We include additional discussions on analysis distinctions and our contributions in Appendix B.3. To begin with, we first introduce some preliminaries.

**Parameter Initialization.** Analogous to existing works [86; 21; 3; 9; 84], for an \(L\)-layer network of width \(m\) in (3), we let its intermediate-layer matrices \(_{l}=(&\\ &)\), \(l[L-1]\), where each element of matrix \(\) is drawn from Gaussian distribution \((0,4/m)\). Similarly, let \(_{L}=(^{},-^{})\), where each element of vector \(\) is drawn from \((0,2/m)\).

**Arm Context Normalization.** To ensure arm contexts are of unit length (i.e., \(\|_{i,t}\|=1, i[K],t[T]\)) as in existing neural bandit works [86; 84; 11; 67; 50], we can apply the following transformation inspired by existing works [3; 86; 84] without loss of generality: with unprocessed context \(}_{i,t}\), we formulate the corresponding normalized arm context \(_{i,t}=[}_{i,t}}{2\|}_{i,t }\|_{2}},\ ,}_{i,t}}{2\|}_{i,t} \|_{2}},\ ]\). It can be verified that we have three properties: (i) \(\|_{i,t}\|_{2}=1\); (ii) no two normalized arm contexts will be in opposite directions; and (iii) \(f(_{i,t};_{0})=0\) with the randomly initialized \(_{0}\).

**Definitions of NTK Matrices.** First, we denote imaginary corruption-free models as \(f(;}_{t-1}),t[T]\) which are trained on corruption-free records \(}_{t}=\{_{},_{}\}_{[t]},t [T]\), for the sake of theoretical analysis, and the learner does not need to own the imaginary model in practice. Let \(\{_{t}\}_{t[T]}=\{_{i_{t},t}\}_{t[T]}\) be arms chosen by the corrupted model \(f(;)\), and \(\{}_{t}\}_{t[T]}=\{_{_{t,t}}\}_{t[T]}\) be those chosen by the corruption-free model \(f(;})\) respectively. Then, define a union set \(}_{T}:=(\{_{t}\}_{t=1}^{T}\{_{t}^{*}\}_{t=1 }^{T}\{}_{t}\}_{t=1}^{T})\), based on: (i) the chosen arms \(\{_{t}\}_{t=1}^{T}\), (ii) the optimal arms \(\{_{t}^{*}\}_{t=1}^{T}\) according to (2), and (iii) arms \(\{}_{t}\}_{t=1}^{T}\) chosen by the imaginary corruption-free models. Here, \(_{T}\) naturally contains _unique arms_ from these three arm collections, with cardinality \(|}_{T}| 3T\). Meanwhile, we simply merge these three arm collections to form \(_{T}\) (with cardinality \(|_{T}|=3T\)), which allows duplicate arms. Afterwards, we have the following _two formulations_ of the NTK Gram matrix: (i) The NTK Gram matrix \(\) with _possibly duplicate_ arms based on the collection \(_{T}\); (ii) the NTK matrix for _non-duplicate_ arms \(}\) built upon the set \(}_{T}\).

**Definition 5.1** (NTK Gram Matrix with _Possibly Duplicate_ Arms).: Let \(\) be the Gaussian distribution. With layer index \(l[L]\) and subscripts \(i,j\{1,,|_{T}|\}\) for enumerating across arms, comparable to , define the following recursive process

\[^{0}_{i,j} =^{0}_{i,j}=_{i},_{j}, ^{l}_{i,j}=^{l}_{i,i}&^{l }_{i,j}\\ ^{l}_{j,i}&^{l}_{j,j},\] (7) \[^{l}_{i,j} =2_{a,b(,^{l-1}_{i,j })}[(a)(b)], ^{l}_{i,j}=2^{l-1}_{i,j}_{a,b (,^{l-1}_{i,j})}[^{}(a)^{ }(b)]+^{l}_{i,j}.\]

With \(_{T}\) containing _possibly duplicate arms_, we denote the NTK Gram matrix \(=(^{L}+^{L})/2^{3T 3T}\), and expected reward vector \(=[h()]_{_{T}}^{3T}\). Existing works with the _arm separateness assumption_ (e.g., ) generally assume \(\), while we do not.

**Definition 5.2** (NTK Gram Matrix with _Non-duplicate_ Arms).: Follow the recursive process in (7). With set \(}_{T}\) containing _non-duplicate arms_, we denote the corresponding NTK matrix \(}=(}^{L}+}^{L})/2^{|}_{T}||}_{T}|}\), and expected reward vector \(}=[h()]_{}_{T}}^{| }_{T}|}\), with \(|}_{T}| 3T\).

**Remark 5.3** (No Arm Separateness Assumption).: Existing neural bandit works generally impose separateness assumptions regarding the arm contexts: NTK-based approaches (e.g., ) commonly assume \(\) which requires no two arms are parallel among \(\{_{i,t}\}_{i[K],t[T]}\); meanwhile, some other works (e.g., ) assume the Euclidean separateness: \(\|_{i,t}-_{i^{},t^{}}\|_{2}>0\) if \((i,t)(i^{},t^{}), i,i^{}[K],t,t^{}[T]\). To avoid the arm separateness assumption, since \(}_{T}\) contains all the unique arms from \(_{T}\), we alternatively build the confidence ellipsoid upon the NTK matrix \(}\), and the ellipsoid will also hold for all the arms in \(_{T}\) for regret analysis (Lemma C.1). This also leads to our tighter definition of NTK norm term \(S\) (Theorem 5.6, Remark 5.8).

**Fact 5.4**.: Let \(_{0}\) be the minimum eigenvalue of matrix \(}\), and \(_{0}\) be that of NTK matrix \(\). We have (i) \(_{0}=_{}(})>0\); and, (ii) \(_{0}_{0} 0\).

For (i) in Fact 5.4, since \(}_{T}\) contains no parallel arms, matrix \(}\) will be full-rank, leading to \(_{0}>0\). For (ii), if \(}_{T}_{T}\), then \(_{T}\) contains duplicate arms and matrix \(\) will be singular, s.t. \(_{0}>_{0}=0\). Otherwise, if \(}_{T}=_{T}\), it will naturally lead to \(}=\) and \(_{0}=_{0}\). Next, similar to existing neural bandit works (e.g., ), we define the NTK Gram matrix effective dimension \(\), which essentially measures the vanishing speed of NTK Gram matrix eigenvalues.

**Definition 5.5** (Effective Dimension of NTK Matrix ).: Given the NTK matrix \(\) with possibly duplicate arms (Def. 5.1), its effective dimension is defined as \(=+/)}{(1+TK/ )}\).

### Regret Analysis for R-NeuralUCB

We follow the pseudo-regret \(R(T)=_{t=1}^{T}[_{t}^{*}-_{t}]\) in (2), which is defined based on the expected corruption-free reward of chosen arms and optimal arms across \(T\) rounds.

**Instance-dependent Gradient Deviation Term \(\).** Recall that for a candidate arm \(_{i,t}_{t}\) in round \(t[T]\), its arm weight w.r.t. previously chosen arm \(_{},[t-1]\) in (4) can be represented by \(w_{i,t}^{()}=\{1,_{}(_{i,t}; _{t},}_{t-1})\}\), with the scaling parameter \(>0\). Here, we define a minimum fraction value as \(=_{t[T],[t-1]}\{_{}(_{t}; _{t},}_{t-1}),\ _{}(}_{t}; _{t},}_{t-1})\}\), whichis formulated to quantify the gradient deviation among arms. Here, the learner is not required to know \(\), and we can adjust the scaling parameter \(\) in each round \(t[T]\) to constrain the round-wise minimum weight value \(\{w_{i,t}^{()}\}_{i[K],[t-1]}\) (Subsection B.5), which leads to Theorem 5.6.

**Theorem 5.6**.: _With finite horizon \(T^{+}\), denote \(S}^{}}^{-1}}},>0\). Suppose \( S^{-2}\), \(((TmL+m)^{-1})\), \(J}(TL/)\). Let \(f()\) be an \(L\)-layer FC network with width \(m\), and adjust the scaling parameter \(\), s.t. \(\{w_{i,t}^{()}\}_{i[K],[t-1]}=^{2},\  t[T]\), for a tunable constant \((0,1)\) from (4). With \((0,1)\), let network width \(m((T,L,^{-1},_{0}^{-1},^{-1},S^{-1})(^{-1}))\). With probability at least \(1-\), R-NeuralUCB achieves the regret bound of_

\[R(T)\ ()}-2()+S}( /^{2}})+C ^{-1}^{2}().\]

The proof of Theorem 5.6 is in Appendix C. The first term on the RHS refers to the corruption-independent regret upper bound, which comparably matches the bound \(}(+S})\) in existing corruption-free neural bandit works [86; 84]. Here, our corruption-dependent term is free of the NTK norm \(S\), which measures the complexity of reward mapping \(h()\) (Appendix B.4). This is different from existing works (e.g., ) that include a parameter norm (similar to our NTK norm \(S\)) in their corruption-dependent terms, as the estimation error of confidence ellipsoids. In addition, inspired by , we can derive a \(T\)-independent upper bound for the \(^{-1}\) term, when the arm contexts are nearly spreading within some low-dimensional subspace of the NTK-induced RKHS (Appendix C.9), as it will lead to small effective dimension \(\) and small eigenvalues of NTK matrix \(}\). Meanwhile, compared with the regret bound of our base algorithm NeuralUCB-WGD (Theorem E.1), Theorem 5.6 removes the assumption of known corruption \(C\); and, reduces the order of effective dimension \(\) as well as the dependency of NTK norm \(S\) for corruption-dependent terms.

**Remark 5.7** (Unknown corruption level \(C\)).: For Theorem 5.6, we do not assume \(C\) is known to the learner in advance, as practitioners can have little prior knowledge regarding the unknown adversary. This makes our regret analysis more challenging, compared with the existing works (e.g., ) where \(C\) is assumed known for setting hyper-parameters to achieve tight regret bounds.

**Remark 5.8** (Tighter definition for NTK norm \(S\)).: For existing works (e.g., [86; 84]), the NTK Gram matrix is generally defined with all the \(TK\) observed candidate arms, i.e., \(\{_{i,t}\}_{i[K],t[T]}\), while our NTK matrices (Def. 5.1 and 5.2) only rely on arm collection \(_{T}\) and set \(}_{T}\), with cardinality \(|}_{T}||_{T}|=3T\). This results in our parameter norm \(S\) that can be tighter compared to existing works (e.g., [84; 86]), because when constructing the confidence ellipsoid around the initialization \(_{0}\) in Lemma C.1, our ellipsoid is intuitively tighter, as it only needs to ensure Eq. C.1 holds for arms in \(}_{T}\) (with cardinality \(|}_{T}| 3T\)), rather than for all \(TK\) candidate arms.

**Remark 5.9** (Reducing the order of \(\) and removing the dependency of \(S\) for corruption-dependent terms).: For corruption-dependent terms involving \(C\), we have \(}(C^{-1})\). Using NTK to align the information gain definition  with our effective dimension \(\), our result improves latest kernelized bandit works from \(}(^{3/2})\) to \(}()\) for corruption-dependent terms, given the NTK-induced RKHS and an indefinite arm space (Corollary 7 in ). Meanwhile, our corruption-dependent term is free of the NTK norm \(S\), while for some existing works with UCB-type exploration (e.g., ), they involve comparable parameter norms in their corruption-dependent regret terms, in order to quantify corruption impacts w.r.t. the reward mapping function complexity.

## 6 Experiments

We evaluate R-NeuralUCB and the base algorithm NeuralUCB-WGD (Appendix E) with experiments on three real data sets, under different adversarial corruption scenarios. Following definition in (2), we record the cumulative regret in terms of corruption-free rewards \(R(T)=_{t[T]}_{t}^{*}-_{t}\). Note that the learner will still only have access to the potentially corrupted rewards \(r_{t},t[T]\). Our baselines consist of linear algorithms: Lin-UCB , CW-OFUL ; and conventional neural algorithms: Neural-UCB , Neural-TS . Complementary experiment details are in Appendix A.

**MovieLens and Amazon Data Sets.** From _"MovieLens 20M rating data set"_, we choose 5,000 movies and 10,000 users with most reviews to form the user-movie matrix, and the entries are user ratings. Then, we consider the arm (user-item pair) features as the concatenation of corresponding user features and item features, which are obtained by singular value decomposition (SVD) and extracting item genome-scores respectively, with \(K=10\) and \(d=41\). The corruption-free arm rewards \(_{i,t}\) are user ratings normalized into range \(\). Here, we consider the "exaggerated reward corruption". If one pulled arm \(_{t}\) is attacked and its corruption-free reward \(_{t} 0.5\), we exaggerate its reward to \(r_{t}=1\). Otherwise, if one pulled arm \(_{t}\) is attacked and \(_{t}<0.5\), we set its reward \(r_{t}=0\). _Amazon Recommendation data set_ consists of user reviews and corresponding ratings. With each piece of review (user-item pair) as an arm, we vectorize the review as the arm features using the "Sentire" package [85; 85], with \(K=10\) and \(d=41\). Similarly, the corruption-free arm rewards \(_{i,t}\) are normalized user ratings with the value range \(\). Different from MovieLens data set, we here consider the "reverse exaggerated corruption": if the pulled arm \(_{t}\) is attacked and its corruption-free reward \(_{t} 0.5\), we downplay its reward to \(r_{t}=0\); or if the pulled arm \(_{t}\) is attacked and \(_{t}<0.5\), we alternatively set the corrupted reward \(r_{t}=1\).

**MNIST Data Set.** To perform online classification with bandit feedback experiment, we adopt the MNIST data set  which consists of \(10\) image classes. Similar to previous works (e.g., [86; 84]), given a sample \(^{d^{}}\) in each round, we transform it into \(K=10\) arms, denoted by \(_{1}=(,,,),_{2}=(,,,),,_{10}=(,,,)^{10 d^{ }}\), s.t. \(d=10 d^{}\). The arm index that the learner chooses will be its predicted class, and the reward is \(1\) if the sample \(\) belongs to this class; otherwise, the reward will be \(0\). Here, we consider the symmetric "label-flipping" attack . For example, when a sample from digit class 2 is attacked, its corrupted label will be switched to digit class \(9-2=7\), and the corrupted arm rewards will also change accordingly.

**Experiment Results.** The experiment results are shown in Fig. 1, and we also include a parameter study in Appendix A.2. Due to the representation power of neural networks, neural algorithms generally perform better than linear ones. In particular, for the MNIST data set, since the reward mapping can be relatively more complex, neural algorithms manage to achieve more significant improvements over the linear algorithms. Here, compared with conventional neural methods, our proposed NeuralUCB-WGD and R-NeuralUCB are more robust against adversarial reward corruptions. In particular, we see that R-NeuralUCB outperforms NeuralUCB-WGD on these three data sets, which helps support our claim that it is beneficial to involve the uncertainty information in terms of both training samples and candidate arms. When we increase the corruption intensity (three figures on the right), the overall results tend to be consistent with previous findings. Notice that the performance gap among algorithms on the Amazon data set tends to be smaller, as this setting becomes significantly more difficult (i.e., with up to \( 8000\) regret) when we increase the corruption probability to \(50\%\). Meanwhile, for MNIST, when we increase \(C\) to 4000, the performance gap between our proposed algorithms and the conventional neural methods tends to increase, as the task becomes increasingly more complex. We also see that R-NeuralUCB still outperforms NeuralUCB-WGD given the increased corruption intensity, showing the benefit of involving the candidate arm information and customizing arm-specific model parameters.

## 7 Conclusion and Future Direction

In this paper, we propose a novel neural bandit algorithm named R-NeuralUCB to address potential adversarial corruption issues on arm rewards. To enhance model robustness against reward corruptions, R-NeuralUCB applies a refined, context-aware Gradient Descent procedure that incorporates arm uncertainty information. To demonstrate its effectiveness, we present a regret analysis of R-NeuralUCB to quantify the impacts of adversarial corruption. Furthermore, to ensure that R-NeuralUCB can handle arm contexts deliberately chosen by an adversary (e.g., duplicate arms across different rounds), our analysis avoids the commonly adopted arm separateness assumption in neural bandit literature, which can be of independent interest. Empirical evaluations on real datasets with varied specifications show the effectiveness of our proposed solution over baseline methods. A challenging future direction is to derive the theoretical lower bound for neural bandits with corruption, and we provide complementary discussions in Appendix B.7.

Figure 1: Regret results on real data sets. (Left three figures: For MovieLens and Amazon, corrupt the chosen arm reward with \(20\%\) probability. For MNIST, consider \(C=2000\) and randomly sample 2000 rounds for attack); (Right three figures: For MovieLens and Amazon: we corrupt reward with \(50\%\) probability; For MNIST: \(C=4000\) and randomly sample 4000 corrupted rounds).