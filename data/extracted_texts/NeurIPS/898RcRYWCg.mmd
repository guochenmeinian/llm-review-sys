# Tame a Wild Camera:

In-the-Wild Monocular Camera Calibration

 Shengjie Zhu, Abhinav Kumar, Masa Hu, and Xiaoming Liu

Department of Computer Science and Engineering,

Michigan State University, East Lansing, MI, 48824

{zhusheng, kumarab6, huynshen}@msu.edu, liuxm@cse.msu.edu

###### Abstract

3D sensing for monocular in-the-wild images, _e.g._, depth estimation and 3D object detection, has become increasingly important. However, the unknown intrinsic parameter hinders their development and deployment. Previous methods for the monocular camera calibration rely on specific 3D objects or strong geometry prior, such as using a checkerboard or imposing a Manhattan World assumption. This work instead calibrates intrinsic via exploiting the monocular 3D prior. Given an undistorted image as input, our method calibrates the complete \(4\) Degree-of-Freedom (DoF) intrinsic parameters. First, we show intrinsic is determined by the two well-studied monocular priors: monocular depthmap and surface normal map. However, this solution necessitates a low-bias and low-variance depth estimation. Alternatively, we introduce the incidence field, defined as the incidence rays between points in 3D space and pixels in the 2D imaging plane. We show that: 1) The incidence field is a pixel-wise parametrization of the intrinsic invariant to image cropping and resizing. 2) The incidence field is a learnable monocular 3D prior, determined pixel-wisely by up-to-scale monocular depthmap and surface normal. With the estimated incidence field, a robust RANSAC algorithm recovers intrinsic. We show the effectiveness of our method through superior performance on synthetic and zero-shot testing datasets. Beyond calibration, we demonstrate downstream applications in image manipulation detection & restoration, uncalibrated two-view pose estimation, and 3D sensing. Codes/models are held here.

## 1 Introduction

Camera calibration is typically the first step in numerous vision and robotics applications  that involve 3D sensing. Classic methods enable accurate calibration by imaging a specific 3D structure such as a checkerboard . With the rapid growth of monocular 3D vision, there is an increasing focus on 3D sensing from in-the-wild images, such as monocular depth estimation, 3D object detection , and 3D reconstruction . While 3D sensing techniques over in-the-wild images are developed, camera calibration for such in-the-wild images continues to pose significant challenges.

Classic methods for monocular calibration use strong geometry prior, such as using a checkerboard. However, such 3D structures are not always available in in-the-wild images. As a solution, alternative methods relax the assumptions. For example,  and  calibrate using common objects such as human faces and objects' 3D bounding boxes. Another significant line of research  is based on the Manhattan World assumption , which posits that all planes within a scene are either parallel or perpendicular to each other. This assumption is further relaxed  to estimate the lines that are either parallel or perpendicular to the direction of gravity. The intrinsic parameters are recovered by determining the intersected vanishing points of detected lines, assuming a central focal point and an identical focal length.

While the assumptions are relaxed, they may still not hold true for in-the-wild images. This creates a contradiction: although we develop robust models to estimate in-the-wild monocular depthmap, generating its 3D point cloud remains infeasible due to the missing intrinsic. A similar challenge arises in monocular 3D object detection, as we face limitations in projecting the detected 3D bounding boxes onto the 2D image. In AR/VR applications, the absence of intrinsic precludes placing multiple reconstructed 3D objects within a canonical 3D space. The absence of a reliable, assumption-free monocular intrinsic calibrator has become a bottleneck in deploying these 3D sensing applications.

Our method is motivated by the consistency between the monocular depthmap and surface normal map. In Fig. 1 (c) - (e), an incorrect intrinsic distorts the back-projected 3D point cloud from the depthmap, resulting in distorted surface normals. Based on this, intrinsic is optimal when the estimated monocular depthmap aligns consistently with the surface normal. We present a solution to recover the complete \(4\) DoF intrinsic by leveraging the consistency between the surface normal and depthmap. However, the algorithm is numerically ill-conditioned as its computation depends on the accurate gradient of depthmap. This requires depthmap estimation with low bias and variance.

To resolve it, we propose an alternative approach by introducing a novel 3D monocular prior in complementation to the depthmap and surface normal map. We refer to this as the incidence field, which depicts the incidence ray between the observed 3D point and the projected 2D pixel on the imaging plane, as shown in Fig. 1 (b). The combination of the incidence field and the monocular depthmap describes a 3D point cloud. Compared to the original solution, the incidence field is a direct pixel-wise parameterization of the camera intrinsic. This implies that a minimal solver based on the incidence field only needs to have low bias. We then utilize a deep neural network to perform the incidence field estimation. A non-learning RANSAC algorithm is developed to recover the intrinsic parameters from the estimated incidence field.

We consider the incidence field is also a monocular 3D prior. Similar to depthmap and surface normal, the incidence field is invariant to the image cropping or resizing. This encourages its generalization over in-the-wild images. Theoretically, we show the incidence field is pixel-wisely determined by depthmap and normal map. Empirically, to support our argument, we combine multiple public datasets into a comprehensive dataset with diverse indoor, outdoor, and object-centric images captured by different imaging devices. We further boost the variety of intrinsic by resizing and cropping the images in a similar manner as . Finally, we include zero-shot testing samples to benchmark real-world monocular camera calibration performance.

We showcase downstream applications that benefit from monocular camera calibration. In addition to the aforementioned 3D sensing tasks, we present two intriguing additional applications. One is detecting and restoring image resizing and cropping. When an image is cropped or resized, it disrupts the assumption of a central focal point and identical focal length. Using the estimated

Figure 1: In (a), our work focuses on monocular camera calibration for in-the-wild images. We recover the intrinsic from monocular 3D-prior. In (c) - (e), an estimated depthmap is converted to surface normal using a groundtruth and noisy intrinsic individually. Noisy intrinsic distorts the point cloud, consequently leading to inaccurate surface normal. In (e), the normal presents a different color to (c). Motivated by the observation, we develop a solver that utilizes the **consistency** between the two to recover the intrinsic. However, this solution exhibits numerical instability. We then propose to learn the incidence field as an alternative 3D monocular prior. The incidence field is the collection of the pixel-wise incidence ray, which originates from a 3D point, targets at a 2D pixel, and crosses the camera origin, as shown in (b). Similar to depthmap and normal, a noisy intrinsic leads to a noisy incidence field, as in (e). By same motivation, we develop neural network to learn in-the-wild incidence field and develop a RANSAC algorithm to recover intrinsic from the estimated incidence field.

intrinsic parameters, we restore the edited image by adjusting its intrinsic to a regularized form. The other application involves two-view uncalibrated camera pose estimation. With established image correspondence, a fundamental matrix  is determined. However, there does not exist an injective mapping between the fundamental matrix and camera pose . This raises a counter-intuitive fact: inferring the pose from two uncalibrated images is infeasible. But our method enables uncalibrated two-view pose estimation by applying monocular camera calibration.

We summarize our contributions as follows:

\(\) Our approach tackles monocular camera calibration from a novel perspective by relying on monocular 3D priors. Our method makes no assumption for the to-be-calibrated undistorted image.

\(\) Our algorithm provides robust monocular intrinsic estimation for in-the-wild images, accompanied by extensive benchmarking and comparisons against other baselines.

\(\) We demonstrate its benefits on diverse and novel downstream applications.

## 2 Related Works

Monocular Camera Calibration with Geometry.One line of work  assumes the Manhattan World assumption , where all planes in 3D space are either parallel or perpendicular. Under the assumption, line segments in the image converge at the vanishing points, from which the intrinsic is recovered. LSD  and EDLine  develop robust line estimators. Others jointly estimate the horizon line and the vanishing points . In Tab. 1, recent learning-based methods  relax the assumption by using the gravity-aligned panorama images during training. Such data provides the groundtruth vanishing point and horizon lines which are originally computed with the Manhattan World assumption. Still, the assumption constrains  in modeling intrinsic as 1 DoF camera. Recently,  relaxes the assumption to \(3\) DoF via regressing the focal point. In comparison, our method makes no assumption except for undistorted images. This enables us to calibrate \(4\) DoF intrinsic and train with any calibrated images.

Monocular Camera Calibration with Object.Zhang's method  based on a checkerboard pattern is widely regarded as the standard for camera calibration. Several works generalize this method to other geometric patterns such as 1D objects , line segments , and spheres . Recent works  and  extend camera calibration to real-world objects such as human faces. Optimizers, including BPnP  and PnP  are developed. However, the usage of specific objects restricts their applications. In contrast, our approach is applicable to any image.

Image Cropping and Resizing.Detecting photometric image manipulation  is extensively researched. But few studies detecting geometric manipulation, _e.g._, resizing and cropping. On resizing,  regresses the image aspect ratio with a deep model. On cropping, a proactive method  is developed. We demonstrate that calibration also detects geometric manipulation. Our method does not need to encrypt images, complementing photometric manipulation detection.

Uncalibrated Two-View Pose Estimation.With the fundamental matrix estimated, the two-view camera pose is determined up to a projective ambiguity if images are uncalibrated. Alternative solutions  exist by employing deep networks to regress the pose. However, regression hinders the usage of geometric constraints, which proves crucial in calibrated two-view pose estimation . Other works  use more than two uncalibrated images for pose estimation. Our work complements prior studies by enabling an uncalibrated two-view solution.

Learnable Monocular 3D Priors.Monocular depth  and surface normals  are two established 3D priors. Numerous studies have shown a learnable mapping between monocular images and corresponding 3D priors. Recently,  introduces the perspective field as a monocular 3D

   &  &  &  &  & Ours \\  Degree of Freedom (DoF) & \(4\) & \(1\) & \(1\) & \(3\) & \(4\) \\ Assumption & Specific-Objects & Manhattan & Train with Panorama Images & Undistorted Images \\  

Table 1: **Camera Calibration Methods from Strong to Relaxed Assumptions.** Non-learning methods  rely on strong assumptions. Learning-based methods  relax the assumptions to using gravity-aligned panorama images during training. Our method makes no assumptions except for undistorted images. This enables training with any calibrated images and calibrates 4 DoF intrinsic.

prior for inferring gravity direction. In this work, we suggest the incidence field is also a learnable monocular 3D prior. A common characteristic of monocular 3D priors is their exclusive relationship with 3D structures, making them invariant to 2D manipulations, _e.g._, cropping, and resizing. In contrast, camera intrinsic changes on spacial manipulation. Unlike other monocular priors, computing the groundtruth incidence field is straightforward, relying solely on intrinsic and image coordinates.

## 3 Method

Our work focuses on intrinsic estimation from in-the-wild monocular images captured by modern imaging devices. Hence we assume an undistorted image as input. In this section, we first show how to estimate intrinsic parameters by using monocular 3D priors, such as the surface normal map and monocular depthmap. We then introduce the incidence field and show it a learnable monocular 3D prior invariant to 2D spacial manipulation. Next, we describe the training strategy and the network used to learn the incidence field. After estimating the incidence field, we present a RANSAC algorithm to recover the \(4\) DoF intrinsic parameters. Lastly, we explore various downstream applications of the proposed algorithm. Fig. 2 shows the framework of the proposed algorithm.

### Monocular Intrinsic Calibration

Our method aims to use generalizable monocular 3D priors without assuming the 3D scene geometry. Hence, we start with monocular depthmap \(\) and surface normal map \(\). Assume there exists a learnable mapping from the input image \(\) to depthmap \(\) and normal map \(\): \(,=_{}()\), where \(_{}\) is a learned network. We denote the intrinsic \(_{}\), \(\), and its inverse \(^{-1}\) as:

\[_{}=f&0&w/2\\ 0&f&h/2\\ 0&0&1,=f_{x}&0&b_{x}\\ 0&f_{y}&b_{y}\\ 0&0&1,^{-1}=1/f_{x}&0&-b_{x}/f_{x} \\ 0&1/f_{y}&-b_{y}/f_{y}\\ 0&0&1.\] (1)

The notation \(_{}\) suggests a simple camera model with the identical focal length and central focal point assumption. Given a 2D homogeneous pixel location \(=[x y 1]^{}\) and its depth value \(d=()\), the corresponding 3D point \(=[X Y Z]^{}\) is defined as:

\[=X\\ Y\\ Z=d^{-1}x\\ y\\ 1=d}{f_{x}}\\ }{f_{y}}\\ 1=dv_{x}\\ v_{y}\\ 1=d,\] (2)

where the vector \(\) is an incidence ray, originating from the 3D point \(\), directed towards the 2D pixel \(\), and passing through the camera's origin. The incidence field \(\) is the collection of incidence rays \(\) associated with all pixels at location \(\), where \(=()\).

### Monocular Intrinsic Calibration with Monocular Depthmap and Surface Normal

In this section, we explain how to determine the intrinsic matrix \(\) using the estimated surface normal map \(\) and depthmap \(\). Given the estimated depth \(d=()\) and normal \(=()\) at 2D pixel

Figure 2: We illustrate the framework for the proposed monocular camera calibration algorithm. In (a), a deep network maps the input image \(\) to the incidence field \(\). A RANSAC algorithm recovers intrinsic from \(\). In (b), we visualize a single iteration of RANSAC. An intrinsic is computed with two incidence vectors randomly sampled at red pixel locations. From Eq. (2), an intrinsic determines the incidence vector at a given location. The optimal intrinsic maximizes the consistency with the network prediction (blue and orange). Subfigure (c) details the RANSAC algorithm. Different strategies are applied depending on if a simple camera is assumed. If not assumed, we independently compute \((f_{x},b_{x})\) and \((f_{y},b_{y})\). If assumed, there is only \(1\) DoF of intrinsic. We proceed by enumerating the focal length within a predefined range to determine the optimal value.

location \(\), a local 3D plane is described as:

\[} d+c=0.\] (3)

By taking derivative in \(x\)-axis and \(y\)-axis directions, we have:

\[}_{x}(d)=0, }_{y}(d)=0.\] (4)

Note the bias \(c\) of the 3D local plane is independent of the camera projection process. Without loss of generality, we show the case of our method for \(x\)-direction. Expanding Eq. (4), we obtain:

\[n_{1}_{x}(d}{f_{x}})+n_{2}}{f_{y}}_{ x}(d)+n_{3}_{x}(d)=0,\] (5)

where \(_{x}(d)\) represents the gradient of the depthmap \(\) in the \(x\)-axis and can be computed, for example, using a Sobel filter . Next, re-parametrize the unknowns in Eq. (5) to get:

\[a_{1}f_{x}f_{y}+a_{2}f_{x}b_{y}+a_{3}f_{y}b_{x}+a_{4}f_{y}+a_{5}f_{x}=0.\] (6)

Divide both sides of the equation by \(f_{x}\) to get:

\[a_{1}f_{y}+a_{2}b_{y}+a_{3}rb_{x}+a_{4}r+a_{5}=0,\] (7)

where \(r=}{f_{x}}\). By stacking Eq. (7) with \(N 4\) randomly sampled pixels, we acquire a linear system:

\[_{N 4}\ _{4 1}=_{4 1},\] (8)

where the intrinsic parameter to be solved is stored in a vector \(_{4 1}=[f_{y} b_{y} rb_{x} r]^{}\). This solves the other intrinsic parameters as:

\[f_{y}=f_{y},\,b_{y}=b_{y},\,f_{x}=}{r},\,b_{x}=}{r}.\] (9)

The known constants are stored in matrix \(_{N 4}\) and \(_{4 1}\). If we choose \(N=4\) in Eq. (8), we obtain a minimal solver where the solution \(\) is computed by performing Gauss-Jordan Elimination. Conversely, when \(N>4\), the linear system is over-determined, and \(\) is obtained using a least squares solver. The above suggests the intrinsic is recoverable from the monocular 3D prior.

### Monocular Incidence Field as Monocular 3D Prior

Eq. (9) relies on the consistency between the surface normal and depthmap gradient, which may require a low-variance depthmap estimate. From Fig. 3, even groundtruth depthmap leads to spurious normal due to its inherent high variance. Thus, minimal solver in Eq. (9) can lead to a poor solution.

Figure 3: In (a) and (b), we highlight the ground truth depthmap of a smooth surface, such as a table’s side. Even with the ground truth depthmap, the resulting surface normals exhibit noise patterns due to the inherent high variance. This makes the intrinsic solver based on the consistency of the depthmap and surface normals numerically unstable. Further, (a)-(d) demonstrate a scaling and cropping operation applied to each modality. In (c), the intrinsic changes per operation, leading to ambiguity if a network directly regresses the intrinsic values. Meanwhile, the FoV is undefined after cropping. In comparison, the incidence field remains invariant to image editing, the same as the surface normal and depthmap.

As a solution, we propose to directly learn the incidence field \(\) as a monocular 3D prior. In Eq. (2) and Fig. 1, the combination of the incidence field \(\) and the monocular depthmap \(\) creates a 3D point cloud. In Eq. (3), the incidence field \(\) can measure the observation angle between a 3D plane and the camera. Similar to depthmap \(\) and surface normal map \(\), the incidence field \(\) is invariant to the image cropping and resizing. Consider an image cropping and resizing described as:

\[^{}=~{},=  f_{x}&0& c_{x}\\ 0& f_{y}& c_{y}\\ 0&0&1,^{}=,\] (10)

where \(^{}\) is the intrinsic after transformation. The surface normal map \(\) and depthmap \(\) after transformation is defined as:

\[^{}(^{})=()=( ^{-1}^{}),^{}( ^{})=()=(^{-1} ^{}).\] (11)

Similarly, the incidence field after transformation is:

\[^{}(^{})=(^{})^{-1} ^{}=^{-1}()^{-1}~{}^{}= ^{-1}~{}==().\] (12)

Eq. (12) shows that the incidence field \(\) is a parameterization of the intrinsic matrix that is **invariant** to image resizing and image cropping. Other invariant parameterizations of the intrinsic matrix, such as the camera field of view (FoV), rely on the central focal point assumption and only cover a \(2\) DoF intrinsic matrix. An illustration is shown in Fig. 3.

Next, we prove the incidence field \(\) is a monocular 3D prior pixel-wisely determined by monocular depth \(\) and surface normal \(\). Expanding Eq. (4) using the incidence vector \(\) in Eq. (2), we get:

\[n_{1}_{x}(d)v_{x}+n_{1}(})+n_{2}v_{y}_{x}(d)+n_{3} _{x}(d)=0.\] (13)

Combined with the axis-\(y\) constraint, the \(2\) DoF incidence vector \(=[v_{x} v_{y} 1]^{}\) is uniquely solved. Eq. (13) gives identical solutions when depthmap \(\) is adjusted by a scalar. This implies that the incidence field \(\) is **pixel-wise determined** by the up-to-scale depth map and surface normal map, indicating a learnable mapping from the monocular image \(\) to the incidence field \(\) exists.

Given the strong connection between the monocular depthmap \(\) and camera incidence field \(\), we adopt NewCRFs , a neural network used in monocular depth estimation, for incidence field estimation. We change the last output head to output a three-dimensional normalized incidence field \(}\) with the same resolution as the input image \(\). We adopt a cosine similarity loss defined as:

\[}=_{}(), L=_{ i=1}^{N}}^{}(_{i})}_{ }(_{i}).\] (14)

We normalize the last dimension of the incidence field to one before feeding to the RANSAC algorithm. That is to say, \((_{i})=[_{1}/_{3}_{2}/ _{3} 1]^{}=[v_{1} v_{2} 1]^{}\).

### Monocular Intrinsic Calibration with Incidence Field

Since the network inference executes on GPU device, we adopt a GPU-end RANSAC algorithm to recover the intrinsic \(\) from the incidence field \(\). Unlike a CPU-based RANSAC, we perform fixed \(K_{r}\) iterations of RANSAC without termination. In RANSAC, we use the minimal solver to generate \(K_{c}\) candidates and select the optimal one that maximizes a scoring function (see Fig. 2).

**RANSAC _w.o_ Assumption.** From Eq. (2), the incidence vector \(\) relates to the intrinsic \(\) as:

\[=^{-1}=}{f_{x}}& }{f_{y}}&1^{}.\] (15)

From Eq. (15), a minimal solver for intrinsic is straightforward. In the incidence field, randomly sample two incidence vectors \(^{1}=v_{x}^{1}&v_{y}^{1}&1^{}\) and \(^{2}=v_{x}^{2}&v_{y}^{2}&1^{}\). The intrinsic is:

(16)

Similarly, the scoring function is defined in \(x\)-axis and \(y\)-axis, respectively:

\[_{x}(f_{x},b_{x},\{\},\{\})=_{i=1}^{N_{k}}( \|-b_{x}}{f_{x}}-v_{x}^{i}\|<k_{x}),_{y}(f_{y},b_{y}, \{\},\{\})=_{i=1}^{N_{k}}(\|-b_{y}}{f_ {y}}-v_{y}^{i}\|<k_{y}),\] (17)where \(N_{k}\) and \(k_{x}\)/ \(k_{y}\) are the number of sampled pixels and the threshold for axis-\(x/y\) directions.

**RANSAC _w/_ Assumption.** If a simple camera model is assumed, _i.e._, intrinsic only has an unknown focal length, it only needs to estimate \(1\)-DoF intrinsic. We enumerate the focal length candidates as:

\[\{f\}=\{f_{}+}(f_{}-f_{}) 0 i  N_{f}\}.\] (18)

The scoring function under the scenario is defined as the summation over \(x\)-axis and \(y\)-axis:

\[(f,\{\},\{\})=_{x}(f_{x},w/2,\{\},\{ \})+_{y}(f_{y},h/2,\{\},\{\}).\] (19)

### Downstream Applications

**Image Crop & Resize Detection and Restoration.** Eq. (10) defines a crop and resize operation:

\[}=\,,}= \,,}(})= }(\,)=().\] (20)

When a modified image \(}\) is presented, our algorithm calibrates its intrinsic \(}\) and then:

Case 1: The original intrinsic \(\) is known. _E.g._, we obtain \(\) from the image-associated EXIF file . Image manipulation is computed as \(=}\,}\). A manipulation is detected if \(\) deviates from an identity matrix. The original image restores as \(()=}(\,)\). Interestingly, the four corners of image \(}\) are mapped to a bounding box in original image \(\) under manipulation \(\). We thus quantify the restoration by measuring the bounding box. See Fig. 4.

Case 2: The original intrinsic \(\) is unknown. We assume the genuine image possess an identical focal length and central focal point. Any resizing and cropping are detected when matrix \(}\) breaks this assumption. Note, the rule cannot detect aspect ratio preserving resize or centered crop. We restore the original image by defining an inverse operation \(\) restore \(}\) to an intrinsic fits the assumption.

**3D Sensing Related Tasks.** Intrinsic estimation can enable multiple applications. _E.g._, depthmap to point cloud, uncalibrated two-view pose estimation, etc.

## 4 Experiments

**Implementation Details** Our network is trained using the Adam optimizer  with a batch size of \(8\). The learning rate is \(1e^{-5}\), and the training process runs for \(20,000\) steps. Our RANSAC algorithm

   &  &  &  &  &  &  &  \\  & & & & \(e_{f}\) & \(e_{b}\) & \(e_{f}\) & \(e_{b}\) & \(e_{f}\) & \(e_{b}\) \\  NuScenes  & Calibrated & Driving & ✗ & ✔ & \(0.378\) & \(0.286\) & \(\) & \(\) & \(0.402\) & \(0.400\) \\ KITTI  & Calibrated & Driving & ✗ & ✔ & \(0.631\) & \(0.279\) & \(\) & \(\) & \(0.383\) & \(0.368\) \\ Cityscapes  & Calibrated & Driving & ✗ & ✔ & \(0.624\) & \(0.316\) & \(\) & \(\) & \(0.387\) & \(0.367\) \\ NYUv2  & Calibrated & Indoor & ✗ & ✔ & \(0.261\) & \(0.348\) & \(\) & \(\) & \(0.376\) & \(0.379\) \\ ARKitScenes  & Calibrated & Indoor & ✗ & ✔ & \(0.325\) & \(0.367\) & \(\) & \(\) & \(0.400\) & \(0.377\) \\ SUN3D  & Calibrated & Indoor & ✗ & \(0.260\) & \(0.385\) & \(\) & \(\) & \(0.389\) & \(0.383\) \\ MVImgNet  & SfM & Object & ✗ & ✔ & \(0.838\) & \(0.272\) & \(\) & \(\) & \(0.072\) \\ Objectron  & SfM & Object & ✗ & ✔ & \(0.601\) & \(0.311\) & \(\) & \(\) & \(0.088\) & \(0.079\) \\  MegDepth  & SfM & Outdoor & ✗ & ✔ & \(0.319\) & \(\) & \(0.137\) & \(0.406\) & \(\) & \(\) \\  Waymo  & Calibrated & Driving & ✗ & \(0.444\) & \(\) & \(0.210\) & \(0.053\) & \(\) & \(\) \\ RGBD  & Pre-defined & Indoor & ✗ & \(0.166\) & \(\) & \(0.097\) & \(0.039\) & \(\) & \(\) \\ ScanNet  & Calibrated & Indoor & ✗ & \(0.189\) & \(\) & \(0.128\) & \(0.041\) & \(\) & \(\) \\ MVS  & Pre-defined & Indoor & ✗ & \(0.185\) & \(\) & \(0.170\) & \(0.028\) & \(\) & \(\) \\ Scenes11  & Pre-defined & Synthetic & ✗ & \(0.211\) & \(\) & \(0.170\) & \(0.044\) & \(\) & \(\) \\ 

Table 2: **In-the-Wild Monocular Camera Calibration.** We benchmark in-the-wild monocular camera calibration. We use the first \(9\) datasets for training, and test on all \(14\) datasets. Except for MegaDepth, we synthesize novel intrinsic by cropping and resizing during training. Note the synthesized images violate the focal point and focal length assumption. **[Key: ZS = Zero-Shot, Asn. = Assumptions, Syn. = Synthesized]**

  ^{}\))} &  &  &  &  &  \\  & & & & & \(\)21 & & & \\  Mean & \(9.47\) & \(4.37\) & \(3.59\) & \(3.07\) & \(2.49\) & \(\) \\ Median & \(4.42\) & \(3.58\) & \(2.72\) & \(2.33\) & \(1.96\) & \(\) \\ 

Table 3: **Comparisons to Monocular Camera Calibration with Geometry on GSV dataset**. We follow the training and testing protocol of . For a fair comparison, we convert the estimated intrinsic to camera FoV on the \(y\)-axis direction, following , and report our results _w/_ and _w/o_ the assumptions.

[MISSING_PAGE_FAIL:8]

as shown in Tab. 2. Second, it constrains the baselines to a simple camera parameterized by FoV. We consider the proposed incidence field a more generalizable and invariant parameterization of intrinsic. _E.g._, while FoV remains invariant to image resizing, it still changes after cropping. However, the incidence field is unaffected in both cases. In Tab. 3, the substantial improvement we achieved (\(0.60=3.07-2.47\)) over the recent SoTA  empirically supports our argument. Third, our method calibrates the \(4\) DoF intrinsic with a non-learning RANSAC algorithm. Baselines instead regress the intrinsic. This renders our method more robust and interpretable. In Fig. 2 (b), the estimated intrinsic quality is visually discerned through the consistency achieved between the two incidence fields.

### Monocular Camera Calibration with Objects

We compare to the recent object-based camera calibration method FaceCalib . The baseline employs a face alignment model to calibrate the intrinsic over a video. Both  and ours perform zero-shot prediction. We report performance using Tab. 2 model. Compared to , our method is more general as it does not assume a human face present in the image. Meanwhile,  calibrates over a **video**, while ours is a **monocular** method. For a fair comparison, we report the video-based results as an averaged error over the videos. We report results _w/_ and _w/o_ assuming a simple camera model. Since the tested image has a central focal point, when the assumption applied, the error of the focal point diminished. In Tab. 4, we outperform SoTA substantially.

### Downstream Applications

Image Crop & Resize Detection and Restoration.In Sec. 3.5, our method also addresses the detection and restoration of geometric manipulations in images. Using the model reported in Tab. 2, we benchmark its performance in Tab. 5. Random manipulations following Sec. 4.1 contribute to \(50\%\) of both train and test sets, and the other \(50\%\) are genuine images. In Tab. 5, we evaluate restoration with mIOU and report detection accuracy (_i.e._ binary classification of genuine vs edited images). From the table, our method generalizes to the unseen dataset, achieving an averaged mIOU of \(0.680\). Meanwhile, we substantially outperform the baseline, which is trained to directly regresses the intrinsic. The improvement suggests the benefit of the incidence field as an invariant intrinsic parameterization. Beyond performance, our algorithm is interpretable. In Fig. 4, the perceived image geometry is interpretable for humans.

Figure 4: **Image Crop & Resize Detection and Restoration. Image editing, including cropping and resizing changes intrinsic. As in Sec. 3.5, monocular calibration is applicable to detect and restore image manipulations. We visualize the zero-shot samples on ScanNet and Waymo. More examples are in Supp.**

   &  &  &  &  &  &  &  \\  & mIOU & Acc & mIOU & Acc & mIOU & Acc & mIOU & Acc & mIOU & Acc & mIOU & Acc & mIOU & Acc \\  Baseline & \(0.686\) & \(0.795\) & \(0.621\) & \(0.710\) & \(0.586\) & \(0.519\) & \(0.581\) & \(0.721\) & \(0.636\) & \(0.681\) & \(0.597\) & \(0.811\) & \(0.595\) & \(0.667\) \\ Ours & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 

Table 5: **Image Crop and Resize Restoration.** Stated in Sec. 3.5, our method also encompasses the restoration of image manipulations. Use model reported in Tab. 2, we conduct evaluations on both seen and unseen datasets.

**Uncalibrated Two-View Camera Pose Estimation.** With correspondence between two images, one can infer the fundamental matrix . However, the pose between two uncalibrated images is determined by a projective ambiguity. Our method eliminates the ambiguity with monocular camera calibration. In Tab. 6, we benchmark the uncalibrated two-view pose estimation and compare it to recent baselines. The result is reported using the model benchmarked in Tab. 2 and assumes unique intrinsic for **both** images. We perform zero-shot in ScanNet. For MegaDepth, it includes images collected over the Internet with diverse intrinsics. Interestingly, in ScanNet, our uncalibrated method outperforms a calibrated one . In Supp, we plot the curve between pose performance and intrinsic quality. The challenging setting suggests itself an ideal task to evaluate the intrinsic quality.

**In-the-Wild Monocular 3D Sensing.** In-the-wild 3D sensing, _e.g._, monocular 3D object detection , requires intrinsic to connect the 3D and 2D space. In Fig. 5, despite 3D bounding boxes being accurately estimated, incorrect intrinsic produces suboptimal 2D projection. Our calibration results suggest monocular incidence field estimation is another essential monocular 3D sensing task.

## 5 Conclusion

We calibrate monocular images through a novel monocular 3D prior referred as incidence field. The incidence field is a pixel-wise parameterization of intrinsic invariant to image resizing and cropping. A RANSAC algorithm is developed to recover intrinsic from the incidence field. We extensively benchmark our algorithm and demonstrate robust in-the-wild performance. Beyond calibration, we show multiple downstream applications that benefit from our method.

**Limitation.** In real application, whether to apply the assumption still requires human input.

**Broader Impacts.** Our method detects image spacial editing. If the Exir files are provided, it further localizes the modification region. Shown in Fig. 6, it helps improve press image authenticity.

  Methods & Calibrated &  &  \\  & & ^{}\)} & ^{}\)} & @20\({}^{}\) & @5\({}^{}\) & @10\({}^{}\) & @20\({}^{}\) \\  SuperGlue cvwww19 & ✔ & \(16.2\) & \(33.8\) & \(51.8\) & \(42.2\) & \(61.2\) & \(75.9\) \\ DRC-Net cASSP22 & ✔ & \(7.7\) & \(17.9\) & \(30.5\) & \(27.0\) & \(42.9\) & \(58.3\) \\ LoFTR cvww21 & ✔ & \(22.0\) & \(40.8\) & \(57.6\) & \(52.8\) & \(69.2\) & \(81.2\) \\ ASpanFormer scv22 & ✔ & \(25.6\) & \(46.0\) & \(63.3\) & \(55.3\) & \(71.5\) & \(83.1\) \\ PMatch cvw23 & ✔ & \(29.4\) & \(50.1\) & \(67.4\) & \(61.4\) & \(75.7\) & \(85.7\) \\  PMatch cvww23 & ✔ & \(11.4\) & \(29.8\) & \(49.4\) & \(16.8\) & \(30.6\) & \(47.4\) \\  

Table 6: **Uncalibrated Two-View Camera Pose Estimation.** We use the model reported in Tab. 2 and assume distinct camera models for **both** frames. During calibration, we apply the simple camera assumption. The last two rows ablate the performance using GT intrinsic and our estimated intrinsic.

Figure 5: **In-the-Wild Camera Calibration Benefits In-the-Wild 3D Object Detection . Our estimated intrinsic improves 2D projection of estimated 3D bounding boxes , highlighted with red arrows.**

Figure 6: **Broader Impact. Our method detects geometric manipulation and helps press image authenticity. In the figure, our method restores cropping and the aspect ratio. Orange arrows mark estimated incidence field.**