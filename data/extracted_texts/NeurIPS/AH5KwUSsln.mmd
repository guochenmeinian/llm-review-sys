# Credal Learning Theory

Michele Caprio

Department of Computer Science

University of Manchester, Manchester, UK

michele.caprio@manchester.ac.uk

&Maryam Sultana   Eleni G. Elia   Fabio Cuzzolin

School of Engineering Computing & Mathematics

Oxford Brookes University, Oxford, UK

{msultana,eelia,fabio.cuzzolin}@brookes.ac.uk

###### Abstract

Statistical learning theory is the foundation of machine learning, providing theoretical bounds for the risk of models learned from a (single) training set, assumed to issue from an unknown probability distribution. In actual deployment, however, the data distribution may (and often does) vary, causing domain adaptation/generalization issues. In this paper we lay the foundations for a 'credal' theory of learning, using convex sets of probabilities (credal sets) to model the variability in the data-generating distribution. Such credal sets, we argue, may be inferred from a finite sample of training sets. Bounds are derived for the case of finite hypotheses spaces (both assuming realizability or not), as well as infinite model spaces, which directly generalize classical results.

## 1 Introduction

Statistical Learning Theory (SLT) considers the problem of predicting an output \(y\) given an input \(x\) using a mapping \(h:\), \(h\), called _model_ or hypothesis, belonging to a model (or hypotheses) space \(\). The loss function \(l:()\) measures the error committed by a model \(h\). For instance, the zero-one loss is defined as \(l((x,y),h)[y h(x)]\), where \(\) denotes the indicator function, and assigns a zero value to correct predictions and one to incorrect ones. Input-output pairs are usually assumed to be generated i.i.d. by a probability distribution \(P^{}\), which is unknown. The _expected risk_ - or _expected loss_ - of the model \(h\), \(L(h) L_{P^{}}(h)_{P^{}}[l((x,y),h)]=_{ }l((x,y),h)P^{}((x,y))\), measures the expected value - taken with respect to \(P^{}\) - of loss \(l\). The expected risk minimizer \(h^{}_{h}L(h)\) is any hypothesis in the given model space \(\) that minimizes the expected risk. Given a training dataset \(D=\{(x_{1},y_{1}),,(x_{n},y_{n})\}\) whose elements are drawn independently and identically distributed (i.i.d.) from probability distribution \(P^{}\), the _empirical risk_ of a hypothesis \(h\) is the average loss over \(D\). The _empirical risk minimizer_ (ERM), i.e., the model \(\) one actually learns from the training set \(D\), is the one minimizing the empirical risk . Statistical Learning Theory seeks upper bounds for the expected risk \(L()\) of the ERM \(\), and in turn, for the _excess risk_, that is, the difference between \(L()\) and the lowest expected risk \(L(h^{})\). This endeavor is pursued under increasingly more relaxed assumptions about the nature of the hypotheses space \(\). Two common such assumptions are that either the model space is finite, or that there exists a model with zero expected risk (_realizability_).

In real-world situations, however, the data distribution may (and often does) vary, causing issues of _domain adaptation_ (DA)  or _generalization_ (DG) . Domain adaptation and generalization are interrelated yet distinctive concepts in machine learning, as they both deal with the challenges oftransferring knowledge across different domains. The main goal of DA is to adapt a machine learning model trained on source domains to perform well on target domains. In opposition, DG aims to train a model that can generalize well to unseen data/domains not available during training. In simple terms, DA works on the assumption that our source and target domains are related to each other, meaning that they somehow follow a similar data-generating probability distribution. DG, instead, assumes that the trained model should be able to handle unseen target data.

Attempts to derive generalization bounds under more realistic conditions within classical SLT have been made (see Section 2). Those approaches, however, are characterized by a lack of generalizability, and the use of strong assumptions. A more detailed account of the state of the art and their limitations is discussed in Section 2. In opposition to all such proposals, our learning framework leverages _Imprecise Probabilities_ (IPs) to provide a radically different solution to the construction of bounds in learning theory.

A hierarchy of formalisms aimed at mathematically modeling the 'epistemic' uncertainty induced by sources such as lack of data, missing data or data which is imprecise in nature [24; 62; 63], IPs have been successfully employed in the design of neural networks providing both better accuracy and uncertainty quantification to predictions [17; 47; 48; 49; 64; 73]. To date, however, they have never been considered as a tool to address the foundational issues of statistical learning theory associated with data drifting.

**Contributions**. This paper provides two innovative contributions: (1) the formal definition of a new learning setting in which models are inferred from a (finite) sample of training sets (via either objectivist or subjectivist modeling techniques, as explained in Section 3), rather than a single training set, each assumed to have been generated by a single data distribution (as in classical SLT); (2) the derivation of generalization bounds to the expected risk of a model learned in this new learning setting, under the assumption that the epistemic uncertainty induced by the available training sets can be described by a _credal set_, i.e., a convex set of (data generating) probability distributions.

The overall framework is illustrated in Figure 1. Generalized upper bounds under credal uncertainty are derived under three increasingly realistic sets of assumptions, mirroring classical statistical learning theory treatment: (i) finite hypotheses spaces with realizability, (ii) finite hypotheses spaces without realizability, and (iii) infinite hypotheses spaces. We show that the corresponding classical results in SLT are special cases of the ones derived in the present paper.

**Paper outline**. The paper is structured as follows. First (Section 2) we present the existing work addressing data distribution shifts in learning theory. We then introduce our new learning framework (Section 3). In Section 4 we illustrate the bounds derived under credal uncertainty and show how classical results can be recovered as special cases. Section 5 concludes and outlines future undertakings. We prove our results in Appendix A, and we provide synthetic experiments on our first two main results (Theorems 4.1 and 4.5) in Appendix B.

## 2 Related Work

The standard statistical approach to generalization is based on the assumption that test and training data are i.i.d. according to an unknown distribution. This assumption can fall short in real-world applications: as a result, many recent papers have been focusing on the "Out of Distribution" (OOD) generalization problem, also known as domain generalization (DG), to address the discrepancy between test and training distribution(s) [31; 40; 68]. Extensive surveys of existing methods and

Figure 1: Graphical representation of the proposed learning framework. Given an available finite sample of training sets, each assumed to be generated by a single data distribution, one can learn a credal set \(\) of data distributions in either a frequentist or subjectivist fashion (Section 3). This allows us to derive generalization bounds under credal uncertainty (Section 4).

approaches to DG can be found in Wang et al. , Zhou et al. . Although several proposals for learning bounds with theoretical guarantees have been made within DA, only a few attempts have been made in the field of DG [28; 60]. Most theoretical attempts have focused on kernel methods, starting from the seminal work of Blanchard et al. , spanning to a body of later work (see for example Deshmukh et al. , Hu et al. , Muandet et al. ). In this line of work, assumptions related to boundedness of kernels and continuity of feature maps and loss function render the approaches not directly applicable to broader scenarios.

Other work has focused on providing theoretical grounds using domain-adversarial learning method; in this approach, the authors use a convex combination of source domains in order to approximate the target distribution leveraging H-divergence . Ye et al.  have attempted to relax assumptions to provide more general bounds, focusing on feature distribution characteristics; the authors have introduced terms related to stability and discriminative power to calculate the error bound on unseen domains, through the use of an expansion function. Nonetheless, as the authors acknowledge, practical challenges arise concerning the estimation of the expansion function and the choice of a constraint on the top model to improve convergence.

Researchers have also focused on adaptation to new domains over time, treating DG as an online game and the model as a player minimizing the risk associated with introducing new distributions by an adversary at each step . However, in scenarios where the training distribution is significantly outside the convex hull of training distributions , or because of unmet strong convexity loss function assumption , they fall short from achieving robust generalization. Causality principles have been leveraged in this sense, for example by Bellot and Bareinboim , Sheth et al. , to provide distributional robustness guarantees using causal diagrams and source domain data. However, causal approaches for improving model robustness across varying domains pose important challenges including reliance on domain knowledge. Researchers have also explored generalization bounds for DG based on the Rademacher complexity, allowing for the approach to be applicable to a broader range of models . Though this simplification has a number of practical benefits, models trained under covariate shift assumptions might suffer in terms of robustness to other distribution shift types. On the empirical analysis side, Gulrajani and Lopez-Paz  have provided a comprehensive review of the state of the art. Though a simple ERM was found to outperform other more sophisticated methods in benchmark experiments , this approach has been criticized for its non-generalizability. In this direction, Izmailov et al.  have highlighted the importance of searching for flat minima in the training process for improved generalization.

All the aforementioned approaches take a point estimate-like, stance (i.e., assuming a single training set) to the derivation of generalization bounds. In this paper, in opposition, we explicitly acknowledge the uncertainty inherent to domain variation in the form of a sample of training sets, each assumed to be generated by a different distribution, and propose a robust and flexible approach representing the resulting epistemic uncertainty via credal sets. Related works on the computational complexity specific to the use of credal sets are discussed in Appendix E.

## 3 Credal Learning

Let us formalize the notion of learning a model from a collection of (training) sets of data, each issued from a different 'domain' characterized by a single, albeit unknown, data-generating probability distribution. Assume that we wish to learn a mapping \(h:\) between an input space \(\) and an output space \(\) (where, once again, the mapping \(h\) belongs to a hypotheses space \(\)), having as evidence a finite sample of training sets, \(D_{1},,D_{N}\), \(D_{i}=\{(x_{i,1},y_{i,1}),,(x_{i,n_{i}},y_{i,n_{i}})\}\). Assume also that the data in each set \(D_{i}\) has been generated by a distinct probability distribution \(P_{i}^{}\). The question we want to answer is: What sort of guarantees can be derived on the expected risk of a model learned from such a sample of training sets? How do they relate to classical Probably Approximately Correct (PAC) bounds from statistical learning theory?

### Objectivist Modeling

While in classical statistical learning theory results are derived assuming no knowledge about the data-generating process, the theorems and corollaries in this paper do require some knowledge, although incomplete, of the true distribution. To be more specific, we will posit that, by leveraging the available evidence \(D_{1},,D_{N}\), the agent is able to elicit a credal set - i.e., a closed and convex set of probabilities - that contains the true data generating process \(P^{} P^{*}_{N+1}\) for a _new_ set of data \(D_{N+1}\) (that we call the _test set_), possibly different from \(D_{1},,D_{N}\). As we shall see in Section 4, though, this extra modeling effort allows us to derive stronger results.

There are at least two ways in which such a credal set can be derived, that is, via either an _objectivist_ or a _subjectivist_ modeling stance. In this section, we present the former. We start by inspecting the frequentist approach to objectivist modeling, considering in particular epsilon-contamination models (Section 3.1.1) and belief functions models (Sections 3.1.2, 3.1.3). A further objectivist model based on fiducial inference  is outlined in Appendix D.

#### 3.1.1 Epsilon-contamination models

In classical frequentist statistics, given the available dataset, the agent assumes the analytical form of a likelihood \(\) (not to be confused with the expected loss function, which we denote by a Roman letter \(L\)), e.g., a Normal or a Gamma distribution. As shown by Huber and Ronchetti , though, small perturbations of the specified likelihood can induce substantial differences in the conclusions drawn from the data. A _robust frequentist_ agent is thus interested in statistical methods that may not be fully optimal under the ideal 'true' likelihood model, but still lead to reasonable conclusions if the ideal model is only approximately true .

To account for this, the agent specifies the class of _\(\)-contaminated_ distributions

\[=\{P:P=(1-)+ Q,\, Q\},\]

where \(\) is some positive quantity in \((0,1)\), and \(Q\) is any distribution on \(\). Wasserman and Kadane  show that \(\) is indeed a (nonempty) credal set. In view of this robust frequentist goal, then, requiring that the true data generating process belongs to \(\) is a natural assumption.

In our framework, in which a finite sample of \(N\) training sets \(\{D_{i}\}_{i=1}^{N}\) is available, one approach to building the desired credal set is to specify \(N\) many likelihoods \(\{_{i}\}_{i=1}^{N}\) and \(_{i}\)-contaminate each of them to obtain \(_{i}=\{P:P=(1-_{i})_{i}+_{i}Q,\, Q\}\), \(i\{1,,N\}\). The credal set \(\) can then be derived by setting

\[=(_{i=1}^{N}_{i}),\]

where \(()\) denotes the convex hull operator.1 An immediate consequence of Wasserman and Kadane  and references therein is that \(=(_{i=1}^{N}_{i})=\{P:P(A)}(A), A\}\), where \(}(A)=_{i\{1,,N\}}(1-_{i})_{i}(A)\), for all \(A\). A simple numerical example for such a procedure is given in Appendix C.

#### 3.1.2 Belief functions as lower probabilities

An alternative way to derive a credal set from the sample training evidence can be formulated within the framework of the Dempster-Shafer theory of evidence .

A _random set_ is a set-valued random variable, modeling random experiments in which observations come in the form of sets. In the case of finite sample spaces, they are called _belief functions_. While classical discrete mass functions assign normalized, non-negative values to the _elements_\(\) of their sample space, a belief function independently assigns normalized, non-negative mass values to _subsets_ of the sample space: \(m(A) 0\), for all \(A\), \(_{A}m(A)=1\). The belief function associated with a mass function \(m\) then measures the total mass of the subsets of each event \(A\), \((A)=_{B A}m(B)\).

Crucially, a belief function can be seen as the lower probability (or lower envelope) of the credal set

\[()=\{P::(A) P(A),  A\},\]

where \(P\) is a data distribution. The dual upper probability to Bel is \((A) 1-(A^{c})\), for all \(A\). When restricted to singleton elements, it is called the _contour function_, \(()=(\{\})\).

#### 3.1.3 Inferring belief functions from data

There are various ways one can infer a belief (or, equivalently, a plausibility) function from (partial) data, such as a sample of training sets. If a classical likelihood \(\) having probability density or mass function (pdf/pmf) \(\) is available (as assumed in the frequentist paradigm),2 one can build a belief function by using the normalized likelihood as its contour function. That is, \(()}( ^{})}\), for all \(\), where \(=\) is the space where the training pairs live.

As before, in our framework in which a finite sample of \(N\) training sets \(\{D_{i}\}_{i=1}^{N}\) is available, we can specify \(N\) many likelihoods \(\{_{i}\}_{i=1}^{N}\), and their corresponding pdf/pmf's \(\{_{i}\}_{i=1}^{N}\). Then, we can compute \(()=_{i\{1,,N\}}_{i}()\), for all \(\), and in turn

\[()=()/sup_{^{}} (^{}),\] (1)

for all \(\).3 In turn, our credal set is derived as \(=\{P:P/=p\}\), where \(P/=p\) is the pdf/pmf associated with distribution \(P\) via its Radon-Nikodym derivative with respect to a sigma-finite dominating measure \(\). Such construction means that \(\) includes all distributions whose pdf/pmf's are element-wise dominated by plausibility contour pl.

**Numerical example.** Let \(=\{_{1},_{2},_{3}\}\), where \(_{j}=(x_{j},y_{j})\), \(j\{1,2,3\}\). Suppose also that we observed four sample training sets \(D_{1},,D_{4}\) and that we specified the likelihood pmf's \(_{1},,_{4}\) as in Table 1.4 There, we see e.g. how pmf \(_{1}\) assigns a probability of \(0.3\) to the element \(_{1}\) of the state space \(\), and similarly for the other pmf's and the other elements of the state space. It is immediate to see that \(=(0.4,0.8,0.6)^{}\).5 Then, by Equation (1), we have that \(=(0.5,1,0.75)^{}\).

We can then derive the lower \(P\) and upper \(\) probabilities of \(=\{P:P/=p\}\) on \(2^{}\) as in Table 2, using the results in Augustin et al. [8, Section 4.4]. That is, \((A)=\{_{ A}P(),1-_{ A^{ }}()\}\) and \((A)=\{_{ A}(),1-_{  A^{}}P()\}.\) As we can see from the visual representation of \(\) (the yellow convex region in Figure 2), the probability bounds imposed by the credal set are not too stringent, and in line with the evidence encapsulated in \(_{1},,_{4}\). Hence, the assumption that \(P^{} P_{5}^{}\) is quite plausible.

### Subjectivist Modeling

Another way of specifying a credal set is by taking a _personalistic_ (or subjectivist) route . In this approach, let \(\{A_{}\}\) be a finite collection of subsets of \(=\). The agent first specifies the lower probability \(_{}\) on the power set \(2^{}\), where \(= A_{}\) - i.e., the smallest value that the probability of any subset of \(\) can take on. This can be done, for example, as a result of the empirical distribution, as described below.

In our framework in which a finite sample of \(N\) training sets \(\{D_{i}\}_{i=1}^{N}\) is available, we have that \(\{A_{}\}=\{D_{i}\}_{i=1}^{N}\), and so \(=_{i=1}^{N}D_{i}\). Recall that we originally denoted by \(P_{i}^{}\) the true data generating process for training set \(D_{i}\), \(i\{1,,N\}\): the empirical distribution \(P_{i}^{}\) is a (non-parametric) estimation of \(P_{i}^{}\). On the other hand, recall that we denoted by \(P^{} P_{N+1}^{}\) the true data generating process for the test set of data \(D_{N+1}\).

The lower probability \(_{}\) is defined as follows. For every element \((x,y)\) in \(=_{i=1}^{N}D_{i}\), we let \(_{}(\{(x,y)\})\{P_{i}^{}(\{(x,y) \}):P_{i}^{}(\{(x,y)\})>0\}\). Requiring \(_{}(\{(x,y)\})=_{i}P_{i}^{}(\{(x,y)\})\) is not enough because, if the training data sets do not overlap, we would end up having lower probability \(0\) for some singleton that we observed at training time, and hence we would be neglecting some collected evidence. The lower probability \(_{}\) of all the other non-singleton elements \(B\) of \(\) is computed according to (8, Equation (4.6a)), that is,

\[_{}(B)=\{_{(x,y) B} _{}(\{(x,y)\}),1-_{(x,y) B^{c}}_{i}P_{i}^{ }(\{(x,y)\})\}.\] (2)

**Numerical example.** Suppose for simplicity that \(=\{x\}\), so that \(=\{x\}\), and let \(=\{1,,10\}\). Suppose \(N=2\), and let \(D_{1}\) be a collection of three \(4\)'s, three \(5\)'s, and three \(6\)'s. Let also \(D_{2}\) be a collection of six \(5\)'s and two \(6\)'s. Then, \(=\{4,5,6\}\) and \(2^{}=\{,\{4\},\{5\},\{6\},\{4,5\},\{5,6\},\{4,6\},\{4,5,6\}\}\). In turn, \(_{}(\{4\})=1/3\), \(_{}(\{5\})=1/3\), and \(_{}(\{6\})=1/4\). By (2), this implies that \(_{}(\{4,5\})=2/3\), \(_{}(\{5,6\})=2/3\), and \(_{}(\{4,6\})=\{1/3+1/4,1-_{i\{1,2\}}P_{i}^ {}(\{5\})\}=\{7/12,1-\{1/3,3/4\}=\{7/12,1-3/4\}=7/12\). Of course, \(_{}()=0\) and \(_{}()=1\).

#### 3.2.1 Walley's Natural Extension

Once a lower probability \(_{}\) on \(2^{}\) is inferred, it can be (coherently uniquely) extended to a lower probability \(\) on the whole sigma-algebra endowed to \(\) through an operator called _natural extension_, (70, Sections 3.1.7-3.1.9). The resulting extended lower probability is such that \((B)=_{}(B)\), for all \(B 2^{}\), and a lower probability value \((A)\) is assigned to all the other subsets \(A\) of \(\) that are not in \(\). It is also _coherent_ - in Walley's terminology - because, in the behavioral interpretation of probability derived from de Finetti [25; 26], its values cannot be used to construct a bet that would make the agent lose for sure, no matter the outcome of the bet itself.

Once \(\) is obtained, the agent can consider the _core_ of \(\), \(()\{P:(A) P(A), A \}\), i.e., the collection of all the (countably additive) probabilities that set-wise dominate \(\). Scholars [20; 50] have shown that \(=()\) is indeed a (nonempty) credal set.

#### 3.2.2 Properties of the Core

As shown in Amarante and Maccheroni (5, Example 1) and Amarante et al. (6, Examples 6, 7, 8), given a generic credal set \(\) whose lower envelope is \(\) - i.e., a credal set \(\) for which \(()=_{Q}Q()\) - we have that \(()\). From an information-theoretic perspective, this means that the uncertainty encapsulated in the core of a lower probability \(\) is larger than that in any credal set whose lower envelope is \(\)[13; 15; 16; 18; 30]. In turn, \(}()\) is the largest credal set the agent can build which represents their partial knowledge. In our learning framework, given the available evidence \(D_{1},,D_{N}\) that the agent uses to derive \(_{}\), if the agent is confident that \(P^{}(B)_{}(B)\), for all \(B 2^{}\), then it is natural to assume \(P^{} P_{N+1}^{}()\).

## 4 Generalization Bounds under Credal Uncertainty

Consider a credal set \(\) on \(\) derived as in Section 3, and assume that we collect new evidence in the form of a test set of data \(D_{N+1}=\{(x_{N+1,1},y_{N+1,1}),,(x_{N+1,n_{N+1}},y_{N+1,n_{N+1}})\}\). To ease notation, in the following we refer to the newly acquired evidence as \((x_{1},y_{1}),,(x_{n},y_{n})\).

An assumption that is common to all the results we present in this section is that \((x_{1},y_{1}),,(x_{n},y_{n}) P P^{} P_{N+1}^ {}\) i.i.d., and \(P\). This means that either the new evidence comes from one of the distributions that generated \(D_{1},,D_{N}\), or that it is at least _compatible_ with the credal set we built from past evidence . That is, either \(P\) set-wise dominates the lower probability \(\) of \(\) (as in Sections 3.1, 3.1.2, and 3.2), or it is set-wise dominated by the upper probability \(\) of \(\) (induced, e.g., by a contour function as in Section 3.1.3). This is a rather natural assumption, especially when the stream of training sets we collect pertains to similar experiments or tasks. For example, this is the case in Continual Learning (CL), where it is customary to assume _task similarity_, that is, to posit that the oracle distributions pertaining to all the tasks of interest are all contained in a TV-ball of radius chosen by the user. A more complete discussion on the relation between our credal approach and CL can be found in Appendix F. It is also the case in the healthcare setting, where experts' opinions can be incorporated alongside empirical data (plausible probability distributions) to represent the probability uncertainty, for example, for the prognosis of a disease given a set of patient characteristics/biomarkers . To make sure that the credal set constructed encapsulates most of the "potential distributions needed", a number of approaches can be taken including incremental learning; in this approach the AI model learns and updates knowledge incrementally. As a result, the credal set can be continuously updated (via incremental learning) as new data become available. In this direction, learning health systems are being implemented in practice. These are health systems "in which internal data and experience are systematically integrated with external evidence, and that knowledge is put into practice".

We note, in passing, that assuming \(P\) is less stringent than what is typically done in frequentist statistics, where the data-generating process is assumed to be perfectly captured by the likelihood. We instead posit that the true data generating process for the new evidence available belongs to a credal set \(\), that was derived by the sample of training sets \(D_{1},,D_{N}\).

Formal ways of checking whether the assumption \(P\) holds exist, e.g., by following what Cella and Martin [19, Section 7] and Javanmardi et al.  do for credal-set-based conformal prediction methods, or more general approaches .6 That being said, deriving PAC-like guarantees on the correct distribution \(P\) being an element of the credal set \(\) is a desirable objective - a task that we defer to future work. Here, we focus on formally deriving what the consequences are in terms of generalization bounds.

### Realizability and Finite Hypotheses Space

**Theorem 4.1**.: _Let \((x_{1},y_{1}),,(x_{n},y_{n}) P\) i.i.d., where \(P\) is any element of the credal set \(\). Let the empirical risk minimizer be_

\[*{arg\,min}_{h}_{i=1}^{n} l((x_{i},y_{i}),h).\] (3)

_Assume that there exists a realizable hypothesis, that is, \(h^{}\) such that \(L_{P}(h^{})=0\), and that the model space \(\) is finite. Let \(l\) denote the zero-one loss, and fix any \((0,1)\). Then, \([L_{P}()^{}()] 1-\), where \(^{}()\) is a well-defined quantity that depends only on \(\) and on extreme elements ex\(\) of \(\), i.e., those that cannot be written as a convex combination of one another._

Under the assumptions of finiteness and realizability, Theorem 4.1 gives us a tight probabilistic bound for the expected risk \(L_{P}()\) of the empirical risk minimizer \(\). The bound holds for any possible distribution \(P\) in the credal set \(\) that generated the stream of training data. A slightly looser bound depending on the diameter of credal set \(\) holds if we calculate \(L_{Q}()\) in place of \(L_{P}()\).

**Corollary 4.2**.: _Retain the assumptions of Theorem 4.1. Denote by \(Q\), \(Q P\), a generic distribution in \(\) different from \(P\). Let \(_{}\) denote the space of all distributions over \(\), and endow it with the total variation metric \(d_{TV}\). Then, pick any \(_{>0}\). If the diameter of \(\), denoted by diam\({}_{TV}()\), is equal to \(\), we have that_

\[[L_{Q}()^{}()+] 1-,\]

_where \(^{}()\) is the same quantity as in Theorem 4.1._

Corollary 4.2 gives us a probabilistic bound for the expected risk \(L_{Q}()\) of the empirical risk minimizer \(\), calculated with respect to a "wrong" distribution \(Q\) - that is, any distribution in \(\)different from the one generating the new test set of data \(D_{N+1}\). We can also give a looser - but easier to compute - bound for \(L_{P}()\).

**Corollary 4.3**.: _Under the assumptions of Theorem 4.1, \(^{}()_{}() 1/n[| |+()]\) and_

\[[L_{P}()_{}()] 1-,  P_{}.\]

Notice how \(_{}()\) is a uniform bound, that is, a bound that holds for all possible distributions on \(\), not just those in \(\). Strictly speaking, this means that we do not need to come up with a credal set \(\) to find such a bound. Observe, though, that the bound \(^{}()\) in Theorem 4.1 is tighter, as it leverages the training evidence encoded in the credal set \(\). A synthetic experiment confirming this, and studying other interesting properties of \(^{}()\) and \(_{}()\), can be found in Appendix B.

By the proof of Theorem 4.1, we have that \(L_{P}()\) behaves as \((|_{P^{}}B_{P^{}}|/n)\), which is faster than the rate \((||/n)\) that we find in Corollary 4.3, since \(_{P^{}}B_{P^{}}\).7 Roughly, \(B_{P^{}}\) is the set of "bad hypotheses" according to \(P^{}\). That is, those \(h\)'s for which \(L_{P^{}}(h)\) is larger than \(0\). A formal definition is given in the proof of Theorem 4.1. The modeling effort required by producing credal set \(\) is therefore rewarded with a tighter bound and a faster rate.

Notice that Corollary 4.3 corresponds to Liang [44, Theorem 4]: we obtain a classical result as a special case of our more general theorem.

Let us now allow for distribution drift in the new test set of data \(D_{N+1}\).

**Corollary 4.4**.: _Consider a natural number \(k<n\). Let \((x_{1},y_{1}),,(x_{k},y_{k}) P_{1}\) i.i.d., and \((x_{k+1},y_{k+1}),,(x_{n},y_{n}) P_{2}\) i.i.d., where \(P_{1},P_{2}\) are two generic elements of credal set \(\). Retain the other assumptions of Theorem 4.1. Then,_

\[[L_{P_{1}}(_{1})+L_{P_{2}}(_{2})^{ }()}{k(n-k)}] 1-,\] (4)

_where \(^{}()\) is the same quantity as in Theorem 4.1, and_

\[_{1}*{arg\,min}_{h}\{ _{i=1}^{k}l((x_{i},y_{i}),h)\},_{2}*{ arg\,min}_{h}\{_{i=k+1}^{n}l((x_{i},y_{i}),h) \}.\]

Corollary 4.4 gives us a bound similar to the one in Theorem 4.1 when distribution drift is allowed. The price we pay for it is that it is looser. As a result of Corollary 4.3, for a looser but easier to compute bound, we can substitute \(^{}()\) with \(_{}()\).

### No Realizability and Finite Hypotheses Space

Let us now relax the realizability assumption in Theorem 4.1.

**Theorem 4.5**.: _Let \((x_{1},y_{1}),,(x_{n},y_{n}) P\) i.i.d., where \(P\) is any element of the credal set \(\). Assume that the model space \(\) is finite. Let \(l\) be the zero-one loss, \(\) the empirical risk minimizer, and \(h^{}\) the best theoretical model. Fix any \((0,1)\). Then, \([L_{P}()-L_{P}(h^{})^{}()]  1-\), where \(^{}()\) is a well-defined quantity that depends only on \(\) and on the elements of ex\(\)._

As we did in Section 4.1, we can also show that the "wrong" expected risk \(L_{Q}()\) - that is, the expected risk computed according to \(Q\) different from the one generating the new evidence \(D_{N+1}\) - concentrates around the expected risk \(L_{P}(h^{})\) evaluated at the best theoretical model \(h^{}\).

**Corollary 4.6**.: _Retain the assumptions of Theorem 4.5. Denote by \(Q\), \(Q P\), a generic distribution in \(\) different from \(P\). Pick any \(_{>0}\); if \(_{TV}()=\), we have that_

\[[L_{Q}()-L_{P}(h^{})^{}()+ ] 1-,\]

_where \(^{}()\) is the same quantity as in Theorem 4.5._

Similarly to Corollary 4.3, we can give a looser - but easier to compute - bound for \(L_{P}()-L_{P}(h^{})\).

**Corollary 4.7**.: _Retain the assumptions of Theorem 4.5. Then, \(^{}()^{}_{}() |+() )}{n}}\). In turn, \([L_{P}()-L_{P}(h^{})^{}_{}( )] 1-\), for all \(P_{}\)._

The main difference with respect to Theorem 4.1 is that in Theorem 4.5, \(L_{P}()-L_{P}(h^{})\) behaves as \((_{}|/n})\), which is slower than what we had in Theorem 4.1. This is due to the relaxation of the realizability hypothesis. Just like before, though, we have that \((_{}|/n})\) is faster than the rate \((|/n})\) that we find in Corollary 4.7. This is because \(B^{}_{}\).

Roughly, \(B^{}_{}\) is the set of "bad hypotheses" according to at least one \(P^{}\). That is, those \(h\)'s for which \(|(h)-L_{P^{}}(h)|\) is larger than \(0\), for at least one \(P^{}\). A formal definition is given in the proof of Theorem 4.5. Notice that Corollary 4.7 corresponds to Liang [44, Theorem 7]: we obtain a classical result as a special case of our more general theorem.

Let us now allow for distribution drift. To improve notation clarity, in the following we let \(h^{}_{P}\) denote an element of \(_{h}L_{P}(h)\), for a distribution \(P\).

**Corollary 4.8**.: _Consider a natural number \(k<n\). Let \((x_{1},y_{1}),,(x_{k},y_{k}) P_{1}\) i.i.d., and \((x_{k+1},y_{k+1}),,(x_{n},y_{n}) P_{2}\) i.i.d., where \(P_{1},P_{2}\) are two generic elements of credal set \(\). Retain the other assumptions of Theorem 4.5. Then,_

\[(L_{P_{1}}(_{1})-L_{P_{1}}(h^{}_{P_{1}}))+(L_{P_ {2}}(_{2})-L_{P_{2}}(h^{}_{P_{2}}))^{}( )}(+) 1-,\]

_where \(^{}()\) is the same quantity as in Theorem 4.5, and \(_{1}\) and \(_{2}\) are defined as in Corollary 4.4._

Corollary 4.8 tells us that the excess risk is also bounded in the presence of distribution drift. The price we pay for allowing distribution shift is a looser bound. As a result of Corollary 4.7, for a looser but easier to compute bound, we can substitute \(^{}()\) with \(^{}_{}()\).

### No Realizability and Infinite Hypotheses Space

We now relax also the finite hypotheses space assumption in Theorem 4.1.

**Theorem 4.9**.: _Let \((x_{1},y_{1}),,(x_{n},y_{n}) P\) i.i.d., where \(P\) is any element of credal set \(\). Let \(l\) denote the zero-one loss, \(\) the empirical risk minimizer and \(h^{}\) the best theoretical model. Fix any \((0,1)\). Then,_

\[[L_{P}()-L_{P}(h^{})^{} ()] 1-,\] (5)

_for all \(P\). Here, \(^{}() 4_{n,P^{}}() +}\), where \(_{n,P^{}}()_{P^{} }R_{n,P^{}}()\) and_

\[R_{n,P^{}}()_{P^{}}[_{h }_{i=1}^{n}_{i}l((x_{i},y_{i}),h)].\] (6)

_In (6), \(_{1},,_{n}(\{-1,1\})\), and \(\{(x,y) l((x,y),h):h\}\)._

\(R_{n,P^{}}()\) is a slight modification of the classical _Rademacher complexity_ of class \(\),8 given by

\[R_{n}() R_{n,P}()_{P}[_{h }_{i=1}^{n}_{i}l((x_{i},y_{i}),h)],\]

where the expectation is taken with respect to the same distribution \(P\) from which the data points \((x_{1},y_{1}),,\)\((x_{n},y_{n})\) are drawn. We consider \(R_{n,P^{}}()\) instead of \(R_{n,P}()\) because, since \(\) is a credal set, \(P\) can be written as a convex combination of the extreme elements of \(\), and \(_{P}R_{n,P}()=_{P^{} }R_{n,P^{}}()\). If the credal set is _finitely generated_, that is, if it hasfinitely many extreme elements (see footnote 7), then it is easier to compute \(^{}()\): we only need to compute a maximum in place of a supremum.

As we show in Corollary 4.10, Theorem 4.9 generalizes Liang [44, Theorem 9]. This latter focuses only on the "true" probability \(P_{N+1}^{} P^{}\) on \(\), while our result holds for all the plausible distributions in credal set \(\). This grants us to hedge against distribution misspecification.

Let us pause here to add a clarification. In real-world applications, we effectively cannot compute \(R_{n,P^{}}()\), since the distribution \(P^{}\) is unknown. While \(R_{n,P^{}}()\) can be approximated via the _empirical Rademacher complexity_\(_{n}()\)[44, Equation (219)], whose expected value is indeed \(R_{n,P^{}}()\), doing so has at least two drawbacks: **(1)** When the number of data points \(n n_{D_{N+1}}\) is not "large enough", this may lead to a poor approximation of the classical bound (Equation (10) in Appendix A); **(2)** The test set of data \(D_{N+1}=\{(x_{i},y_{i})\}_{i=1}^{n}\) may well be a realization from the tail of distribution \(P^{} P_{N+1}^{*}\). The empirical Rademacher complexity \(_{n}()\), then, would be a poor approximation of \(R_{n,P^{}}()\). In opposition, while \(_{n,P^{}}()\) is more conservative, it can be computed explicitly - since we know the credal set \(\) and its extreme elements ex\(\) - and it leads to a bound that, although looser, holds for all \(P\).

**Corollary 4.10**.: _Retain the assumptions of Theorem 4.9. If \(\) is the singleton \(\{P^{}\}\) (i.e., all the training datasets \(D_{1},,D_{N}\) are generated by the same distribution as the new test set \(D_{N+1}\)), we retrieve Liang [44, Theorem 9]._

We then derive a more general version of Corollary 4.6.

**Corollary 4.11**.: _Retain the assumptions of Theorem 4.9. Denote by \(Q\), \(Q P\), a generic distribution in \(\) different from \(P\). Pick any \(_{>0}\); if \(_{TV}()=\), we have that_

\[[L_{Q}()-L_{P}(h^{})^{}( )+] 1-,\]

_where \(^{}()\) is the same quantity as in Theorem 4.9._

Finally, we once again allow for distribution drift.

**Corollary 4.12**.: _Consider a natural number \(k<n\). Let \((x_{1},y_{1}),,(x_{k},y_{k}) P_{1}\) i.i.d., and \((x_{k+1},y_{k+1}),,(x_{n},y_{n}) P_{2}\) i.i.d., where \(P_{1},P_{2}\) are two generic elements of credal set \(\). Retain the other assumptions of Theorem 4.9, and let \(^{}_{} 4[_{k,P^{}}( )+_{n-k,P^{}}()]+}(+)\). Then,_

\[[(L_{P_{1}}(_{1})-L_{P_{1}}(h^{}_{P_{1}}))+(L_{P_{2}}( _{2})-L_{P_{2}}(h^{}_{P_{2}}))^{}_{ }] 1-,\]

_where \(_{1}\) and \(_{2}\) are defined as in Corollary 4.4._

Similar considerations as the ones after Corollaries 4.4 and 4.8 hold in this more general case as well.

## 5 Conclusions

In this paper, we laid the foundations of a more general Statistical Learning Theory (SLT), that we called Credal Learning Theory (CLT). We generalized some of the most important results of classical SLT to allow for drift and misspecification of the data-generating process. We did so by considering sets of probabilities (credal sets), instead of single distributions. The modeling effort needed to elicit credal sets is paid off in terms of the tightness of the resulting bounds.

**Limitations.** (i) We only consider the zero-one loss in our results (we did so to be able to directly build on the classical results in Liang [44, Chapter 3]). (ii) We assume that the true distribution which the elements of the new test set \(D_{N+1}\) are sampled from, belongs to the credal set that we derive at training time.

**Future work.** In the future, we plan to further our undertaking, for instance by (i) modeling the epistemic uncertainty induced by domain variation through random sets rather than credal sets, (ii) comparing our method with robust learning , (iii) extending our results to different losses, and (iv) deriving PAC-like guarantees on the correct distribution \(P\) being an element of the credal set \(\). We also intend to validate our findings on real datasets.