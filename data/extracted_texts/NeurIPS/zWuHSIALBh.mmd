# Flame+: Factuality-Aware Alignment

for Large Language Models

 Sheng-Chieh Lin\({}^{1}\)+, Luyu Gao\({}^{2}\), Barlas Oguz\({}^{3}\), Wenhan Xiong\({}^{3}\),

**Jimmy Lin\({}^{1}\)**, Wen-tau Yih\({}^{3}\), Xilun Chen\({}^{3}\)+

Footnote †: dagger}\) Xilun and Sheng-Chieh contributed equally to this work.

###### Abstract

Alignment is a procedure to fine-tune pre-trained large language models (LLMs) to follow natural language instructions and serve as helpful AI assistants. We have observed, however, that the conventional alignment process fails to enhance the factual accuracy of LLMs, and often leads to the generation of more false facts (i.e., _hallucination_). In this paper, we study how to make the LLM alignment process more factual, by first identifying factors that lead to hallucination in both alignment steps: supervised fine-tuning (SFT) and reinforcement learning (RL). In particular, we find that training the LLM on new or unfamiliar knowledge can encourage hallucination. This makes SFT less factual as it trains on human-labeled data that may be novel to the LLM. Furthermore, reward functions used in standard RL often inadequately capture factuality and favor longer and more detailed responses, which inadvertently promote hallucination. Based on these observations, we propose _FactuaLity-aware AlignMEnt_ (Flame+), comprised of _factuality-aware SFT_ and _factuality-aware RL_ through direct preference optimization. Experiments show that our proposed Flame+ guides LLMs to output more factual responses while maintaining their instruction-following capability.

Footnote †: dagger}\) Xilun and Sheng-Chieh contributed equally to this work.

## 1 Introduction

Alignment  is a procedure to make pre-trained large language models (LLMs)  follow human instructions and serve as helpful AI assistants. Despite significant progress in general LLM alignment , state-of-the-art aligned LLMs are still prone to generate false claims . In this work, we therefore attempt to advance the understanding of the underlying causes of LLM hallucination as well as its relation to the alignment procedure.

We consider the commonly seen alignment process consisting of two training phases: (1) supervised fine-tuning (SFT) ; (2) reinforcement learning (RL) with human  or automated feedback . In our study, we find that both the SFT and RL steps in the standard alignment process may actually _encourage_ LLMs to hallucinate. First, in the SFT stage, LLMs are fine-tuned with diverse instructions paired with human-created high-quality responses. While this leads to strong instruction-following capability , our study shows that such human-labeled responses may present _new or unknown information_ to the LLM. This, in turn, may inadvertently promote hallucination. Second, we find that the standard reward used in the RL stageoften prefers longer and more detailed responses (Singhal et al., 2023; Chen et al., 2024; Yuan et al., 2024). Consequently, a reward-hacking model ends up with a tendency to produce longer claims with more non-factual information, as shown in the black dots in Figure 1. One possible reason is that most existing RLHF or RLAIF approaches rely on a single scalar reward to represent preference, which struggles to cover multiple alignment skill sets (Ye et al., 2024) and is likely to under-present the aspect of factuality (Hosking et al., 2024).

To address the aforementioned issues, we study the key factors which impact factuality during alignment. In particular, we first conduct a pilot study on the biography generation task (Min et al., 2023) in a more controlled setting where the alignment process focuses solely on factuality (Section 3). Our key observation is that an LLM hallucinates more if it is fine-tuned on new knowledge in either the SFT or the RL stage. For example, an LLM becomes significantly less factual when fine-tuned on responses produced by a model with access to external knowledge (e.g. a retrieval augmented LLM), even though those responses are more factual themselves. Similarly, hallucination is greatly increased if RLAIF is performed on preference pairs that consist of retrieval-augmented LLM output as positive examples and the LLM's own output as negative examples. In comparison, we discover that fine-tuning a pre-trained LLM on a subset of its _own_ generations selected by factuality yields more factual responses and reduces hallucinations.

Next, we apply our findings to improve the factuality of the general LLM alignment process, which is more challenging due to the diversity of instructions. As shown in Figure 2, we observe that some instructions require factual responses while the others do not, and therefore would require different alignment treatments. We first identify fact-based instructions that require factual responses and leverage the findings in our pilot study to create additional training data at both SFT and RL stages to explicitly guide LLMs to output factual responses. Specifically, at the SFT stage, for fact-based instructions, instead of using human created seed training data, we elicit knowledge from the pre-trained LLM and construct training data using its own pre-trained knowledge. This can prevent fine-tuning the LLM on knowledge unknown to itself. At the RL stage, we create additional preference pairs focused on factuality for fact-based instructions, which are combined with the standard preference pairs for instruction following during Direct Preference Optimization (DPO; Rafailov et al., 2023).

We evaluate models on Alpaca Eval (Dubois et al., 2024) and Biography, using win rate for instruction-following capability and FActScore(Min et al., 2023) for factuality evaluation. As shown in Figure 1, using our Flame(r) method (\(^{}\) + \(^{}\)), a significantly higher FActScore (+5.6 pts) is achieved compared to the standard alignment process (\(\) + \(\)), without sacrificing the LLM's instruction-following capability (51.2% win rate). Our ablation study also indicates that identifying fact-based instructions is the key to factual alignment in the general alignment setting.

Figure 1: Models’ helpfulness on Alpaca Eval vs factuality on biography. Helpfulness is measured by models’ win rate over our baseline \(\) + \(\) on Alpaca Eval. Dot size represents average length of bio generation.

Related Work

Alignment.Since pre-trained LLMs cannot accurately follow human instructions, a bunch of work has been proposed to improve LLM alignment through SFT and RL. Some propose to improve SFT through data curation (Zhou et al., 2023; Chen et al., 2024), diverse instruction augmentation (Wang et al., 2023; Li et al., 2024) while others focus on RL with human feedback (Ouyang et al., 2022; Bai et al., 2022), AI feedback (Bai et al., 2023; Sun et al., 2024; Yuan et al., 2024). The main goal of these alignment approaches is instruction-following capability (or helpfulness), which may guide LLMs to output detailed and lengthy responses (Singhal et al., 2023) but inevitably encourage hallucination.

Factuality.Prior work has highlighted the issue of hallucination in LLMs (Gao et al., 2022; Kandpal et al., 2023; Mallen et al., 2023). To address the issue, important research lines are factuality evaluation (Min et al., 2023; Wang et al., 2023; Chern et al., 2023) and improvement. Some training-free approaches to improve LLMs' actuality include external knowledge augmentation (Gao et al., 2022; Kandpal et al., 2023; Cheng et al., 2023; Jiang et al., 2023) and specialized decoding (Li et al., 2023; Chuang et al., 2024).

Recent studies apply RL to improve LLMs' factuality. For example, Tian et al. (2024) propose to construct factuality preference pairs for direct preference optimization (DPO; Rafailov et al., 2023), which is closely related to our work. However, they focus solely on enhancing LLMs' factuality through DPO but overlook its potential impact on the models' instruction-following capability, as demonstrated in our experiments. In contrast, our work provides a comprehensive examination of improving LLMs' factuality and instruction-following ability through fine-tuning approaches encompassing both SFT and DPO. Concurrent to our work, Kang et al. (2024) find that LLMs tend to hallucinate when facing unfamiliar queries. They consider improving LLMs' factuality as teaching LLMs to output abstaining or less detailed responses on such unfamiliar queries, a similar behavior observed from our LLMs fine-tuned with Flame (see case studies in Section 6.5). It is worth mentioning that both prior studies focus on a simplified scenario as our pilot study in Section 3: fine-tuning LLMs to improve factuality on a single task (e.g., fine-tuning and evaluating on biography generation). In contrast, we consider the general alignment task, where LLMs are given diverse and complex instructions.

## 3 A Pilot Study on Factual Alignment

In this section, we first study how to align large language models (LLMs) to be more factual. We use biography generation as the task of our pilot study for two main reasons: (1) Biography generation is a simplified setting where factuality is the sole focus of the alignment process. As we will discuss in Section 4, studying factual alignment on diverse human instructions is more complex, as the alignment process encompasses aspects beyond factuality, such as helpfulness and safety. (2) Evaluating the factuality of biography generation is relatively easy since Wikipedia covers sufficient information for public figures and most of the facts about a person are non-debatable (Min et al., 2023).

### Alignment for Biography Generation

A standard alignment procedure consists of supervised fine-tuning (SFT) and reinforcement learning (RL). In this pilot study, our main goal is to teach LLMs to generate biography with reduced misinformation. For the experiment, we compile training and evaluation datasets comprising 500 and 183 diverse human entities, respectively (further details provided in Appendix A.1). We employ FActScore (FS; Min et al., 2023) as the automated metric for assessing factuality, given its fine-grained evaluation capabilities for long-form text generation and its strong correlation with human judgments.3 To study factuality alignment in this pilot study, we posit that training data is needed where the responses are more factual than the LLM's own generations. Thus, we use retrieval-augmented LLMs (RAG; Lewis et al., 2020) to generate training data, which has been shown to output more factual responses (Mialon et al., 2023).

Throughout the paper, we refer to the pre-trained (PT), supervised fine-tuned (SFT), and direct preference optimization (DPO) fine-tuned LLMs as \(\), \(\), and \(\), respectively.4Sft.We explore two sources of supervision to generate training data (detailed in Appendix A.1): (1) using \(^{}\) with few-shot demonstration to generate biographies for each name entity in training data, where \(^{}\) is \(\) augmented with an off-the-shelf retriever [Lin et al., 2023]; (2) using vanilla \(\) with few-shot demonstration to generate training data as a baseline. As shown in Table 1, \(^{}\) is indeed much more factual than \(\). However, a surprising discovery in the pilot study is that _fine-tuning on such more factual instruction-biography pairs generated by \(^{}\) results in a less factual \(\) model_ (row 4 vs 3).

Dpo.We further fine-tune the LLMs to be more factual through DPO. An intuitive way to create factuality preference pairs is to directly use the samples from \(^{}\) and \(\) as positives and negatives since \(^{}\) generates more factual biographies than \(\) (row 2 vs 1). Another approach is to employ FactScore (FS) as the reward to select positive and negative samples among the generations from \(\) itself [Tian et al., 2024] (detailed in Appendix A.1). As shown in Table 1, DPO fine-tuned on self-generated data with FS reward guides models to generate more factual responses (row 5 vs 3); however, DPO fine-tuned with the supervision of \(^{}\) makes the models hallucinate even more than its \(\) counterpart (6 vs 4).

This outcome suggests that compelling models to generate responses akin to \(^{}\) prompts increases hallucination. Conversely, fine-tuning LLMs on their own generations appears to be crucial for factual alignment, a finding applicable to both SFT and DPO fine-tuning.

### Strategies for Factual Alignment

From the pilot study, we find that better quality data (in terms of factuality) for SFT and DPO does not necessarily yield models with better factual alignment. This is likely because the supervision from RAG contains information unknown to the LLM; thus, fine-tuning on RAG generated responses may inadvertently encourage the LLM to output unfamiliar information. To avoid unknown knowledge from being presented to the LLM, a viable strategy is to create SFT and DPO training data using the generated responses from the LLM itself.

## 4 Factuality-Aware Alignment

In the section, we further extend our discussion of factual alignment to encompass more general instructions. Unlike biography generation in Section 3, where factuality is the main alignment objective, human instructions are diverse and complex, necessitating a range of alignment skill sets beyond factuality alone; e.g., logical thinking, problem handling and user alignment [Ye et al., 2024]. Thus, conducting factual alignment with the diverse instructions face two main challenges: (1) different instructions may demand distinct skill sets. For example, in Figure 2, instruction 3, "Please give me a brief history of coffee", necessitates factual accuracy and concise summarization,

   } &  &  \\   & Pos. & Neg. & FS & \# Corr. / Err. \\  \((1)\)\(^{}\) & - & - & 39.1 & 14.4 / 22.0 \\ \((2)\)\(^{}\) & - & - & 55.4 & 18.6 / 15.9 \\  \((3)\)\(\) & \(\) & - & 37.9 & 13.4 / 21.8 \\ \((4)\)\(\) & \(^{}\) & - & 35.7 & 13.5 / 23.7 \\  \((5)\)\(\) & \(^{}\) & \(^{}\) & 41.6 & 15.4 / 20.7 \\ \((6)\) & \(^{}\) & \(^{}\) & 23.5 & 12.7 / 34.9 \\   ^{}\)FactScore is used to select positives and negatives.} \\ 

Table 1: Pilot study on bio generation. Pos. denotes the positives for SFT or DPO. Neg. denotes the negatives for DPO. FS denotes FactScore.

Figure 2: Instructions from Open Assistant dataset. The instructions are classified with \(\) model using the prompt in Appendix Figure 4.

while instruction 8, "Tell me a story about a pig who goes to the moon", prioritizes creativity and imagination over strict factuality. (2) As recent studies have emphasized , using a single scalar for reward modeling fails to adequately address multiple alignment skill sets and often under-presents the aspect of factuality.

To tackle the aforementioned challenges, we propose _factuality-aware alignment_ (Flame*). To address the first challenge, we propose to prompt LLMs to classify whether a given instruction demands the response to be factual, as shown in Figure 2. We then apply the factuality fine-tuning strategy for SFT and DPO discussed in Section 3.2 to those fact-based instructions. Furthermore, to address the second challenge, we employ separate rewards to evaluate the factuality and instruction-following capability of an LLM. For simplicity, our work only considers two alignment skill sets: instruction following and factuality. We leave more comprehensive reward modeling to future work.

In the following, we first describe our baseline alignment approach and introduce our proposed factuality-aware alignment built on top of the baseline alignment procedure.

### Baseline Alignment

We initialize \(\) from Llama-2 70B pre-trained model5 and build our baseline alignment procedure following self-rewarding language models  due to its simplicity and independence of other strong LLMs (e.g., GPT4) or human evaluators as a reward model. The alignment comprises two steps: (1) building \(\) model fine-tuned on a high-quality seed data consisting of 3,200 instructions and each instruction is paired with the best response created by humans from Open Assistant dataset ; (2) further fine-tuning \(\) through DPO on instruction-following preference data \((x,y_{+},y_{-})\) constructed by itself (\(\)) as the reward model, \(^{}\), where \(y_{+}\) and \(y_{-}\) are the positive and negative responses for a given prompt \(x\), respectively. The resulting fine-tuned model is denoted as \(\) + \(\). Note that, following , we use additional augmented 20K instructions to create the preference training data for DPO fine-tuning. Further details are provided in Appendix A.3.

### Our Approach

#### 4.2.1 Factuality-Aware SFT (\(^{}\))

Although leveraging human created high-quality seed data is a reasonable choice for SFT , our study in Section 3 suggests that fine-tuning on such high-quality data generated by models other than the LLM itself may present unknown information to the LLM, which may in turn encourage hallucination. To address the above issue, for each instruction from the seed data, we elicit the knowledge from the pre-trained LM itself by generating the responses with a few-shot demonstration. Furthermore, to better use the knowledge from both humans and the pre-trained LLM itself, we propose to utilize human generated responses for non-fact-based instructions, while leveraging the responses sampled from pre-trained LLMs for fact-based instructions to mitigate the introduction of unknown knowledge.

Figure 3: Illustrations of (a) response generation using a pre-trained LLM (\(\)) with few-shot demonstration; (b) factuality-aware alignment.

Specifically, we create factuality-aware alignment training data for SFT with two steps. (1) Classifying instructions: we first prompt \(\) to judge whether an instruction from the seed data is fact-based (\(x X^{}\)) or not.6 (2) Eliciting knowledge from \(\): as illustrated in Figure 3(a), we sample 10 responses from \(\) with 5-shot demonstration, \((x_{0},(x_{0}))(x_{4},(x_{4}))\), where \(x_{k}\) is the top-\(k\) similar instruction to \(x\) retrieved by DRAGON+ (Lin et al., 2023) from the seed data. \((x_{k})\) denotes the corresponding human response to \(x_{k}\) in the seed data. As illustrated in Figure 3(b) (upper), the resulting training data for SFT is \((x X^{},(x)),(x X^{},(x))\), where \((x)\) denotes the set of responses to \(x\) sampled from \(\). The fine-tuned model is denoted as \(^{}\).

#### 4.2.2 Factuality-Aware DPO (\(^{}\))

At the second stage of alignment with DPO, we use \(^{}\) to generate multiple responses \(y_{0},y_{1},\) for a given instruction \(x\); then, using \(^{}\) itself as the reward model (\(^{}\)) to create a preference pair: \((x,y_{+},y_{-})\).7 The above data creation procedure is the same as the second stage of our baseline alignment in Section 4.1. However, recent studies (Saha et al., 2024; Hosking et al., 2024; Ye et al., 2024) indicate that a single scalar reward from human feedback or LLM reward models may underrepresent the aspect of factuality. To address this limitation, we introduce another factuality reward model (\(^{}\)) to evaluate factuality of responses and create a factuality preference pair for fact-based instructions: \((x X^{},y_{},y_{})\).

Specifically, we build \(^{}\) with retrieval augmentation to measure the percentage of facts in a response that are correct. \(^{}\) comprises two main components: atomic fact decomposition and retrieval augmented claim verification. We detail the components and ablate their impacts on the quality of \(^{}\) in Appendix A.5. We compute factuality reward for the same responses sampled from \(^{}\): \(^{}(x,y_{0}),^{}(x,y_{1}),\). The response with the highest (lowest) factuality reward is chosen as \(y_{}\) (\(y_{}\)). Note that if the chosen paired responses show large difference in instruction-following reward, we discard the pair; i.e., \(|^{}(x,y_{})-^{}(x,y_{ })|>0.5\). As illustrated in Figure 3(b) (lower), in factuality-aware DPO training, the model is initialized from \(^{}\) and the fine-tuned model is our final factuality-aware aligned model, denoted \(^{}\) + \(^{}\). The specific procedures for fine-tuning models in both the SFT and DPO are described in Appendix A.6.

## 5 Experiments

### Evaluation Datasets and Metrics

Instruction Following.We use the 805 instruction-following tasks from Alpaca Eval (Dubois et al., 2024) to evaluate models head-to-head win rate against our baselines using the recommended evaluator: alpaca_eval_gpt4_turbo_fn. We use \(\) and \(\) + \(\) described in Section 4.1 as the baselines for win rate comparisons.

Factuality.We evaluate models on three datasets with diverse knowledge-intensive instructions for factuality. (1) Biography: a knowledge insensitive sub-task of instruction-following tasks. Following our pilot study in Section 3, we use the 183 human entities provided by Min et al. (2023) with the prompt "Tell me a bio of entity name". (2) Alpaca Fact: we extract the fact-based instructions from the 803 instructions using our SFT model (with the prompt shown in Appendix Figure 4), resulting in 241 instructions. (3) FAVA (Mishra et al., 2024)8: the 141 knowledge-intensive instructions from multiple sources, including Open Assistant (Kopf et al., 2023), No Robots (Rajani et al., 2023), WebNLG (Gardent et al., 2017) and manually created datasets. We report FActScore (FS) without length penalty as the metric for all the three datasets. Note that original FS computes proportion of correct facts with additional penalty on short generations with less than 10 atomic facts. This penalty aims to address situations where models provide insufficiently detailed answers. We assume that this aspect is considered in the evaluation of instruction following in Alpaca Eval. In addition, we also report the number of correct and erroneous facts. All the numbers reported are averaged over the instructions in each dataset.

In addition, we also evaluate our fine-tuned models' truthfulness using TruthfulQA [Lin et al., 2022]. We evaluate model performance in the generation task and use ROUGE [Lin, 2004] and BLEU [Papineni et al., 2002] to measure the quality of responses.

### Comparisons of SFT

Table 2 compares the pre-trained Llama-2 70B fine-tuned on OASST dataset with responses from different sources. We list the FActScore (FS) of biography generation using the pre-trained model through Bio 5-shot demonstration as reference (row 0) and \(\), which is fine-tuned on our seed data with human-created responses, is our baseline (row 1). We first notice that \(\) shows significant FActScore degradation (53.1 vs 44.7) compared to Bio 5-shot with the pre-trained model. It seems that \(\) tends to generate more lengthy responses but with more erroneous facts.

When eliciting the knowledge from \(\) by fine-tuning on its own generated responses, \(^{}\) generates more factual responses in Biography and Alpaca (row 2 vs 1). However, it shows slightly inferior instruction-following capability in Alpaca Eval. This result demonstrates that human responses indeed teach LLMs how to better follow instructions but also encourage LLMs to output more false facts. On the other hand, eliciting the knowledge from the pre-trained model itself avoids the encouragement of hallucination albeit with a slight reduction in instruction-following capability. Finally, \(^{}\) combining supervision from humans and \(\), shows comparable instruction-following capability and output more factual responses on fact-based instructions (row 3 vs 1).

### Comparisons of DPO

Table 3 compares different DPO training recipes. First, we conduct DPO fine-tuning on our SFT baseline, \(\). When further aligning the model to follow instructions, DPO sees a significant improvement in instruction-following capability (row 2 vs 1) with win rate 72.9 over \(\); however, the instruction aligned model tends to output lengthy responses with more factual errors (see examples in Appendix Figure 10). On the other hand, when only aligned with factual preference data, DPO\({}^{}\) shows less improvement in instruction-following capability (row 1 vs 3). These results indicate that

    &  &  &  &  &  \\   & Human & PT & win rate over (1) & FS & \# Corr. / Err. & FS & \# Corr. / Err. & FS & \# Corr. / Err. \\  (0) PT & - & - & - & 53.1 & 15.3 / 13.5 & - & - & - & - \\  (1) SFT & \(\) & ✗ & 50.0 & 44.7 & 21.1 / 26.8 & 38.6 & 16.7 / 29.0 & **54.4** & 21.2 / 25.8 \\ (2) SFT\({}^{}\) & ✗ & ✓ & 48.1 & 48.5 & 19.6 / 20.6 & **42.0** & 17.5 / 28.4 & 53.3 & 18.3 / 24.2 \\ (3) SFT\({}^{}\) & ✓ & ✓ & **51.2** & **49.5** & 19.9 / 19.5 & 41.4 & 18.3 / 27.7 & 54.2 & 19.3 / 22.4 \\   ^{*}\) SFT\({}^{}\) uses supervision from \(\) and \(\) for non-fact-based and fact-based instructions, respectively.} \\ 

Table 2: Experimental results of supervised fine-tuning on Open Assistant dataset. \(\) denotes pre-trained Llama2 70B with 5-shot demonstration. \(^{}\) denotes the variant which only optimizes factuality. FS denotes FActScore.

    &  &  &  &  &  \\   & IF & Fact. & win rate over (2) & FS & \# Corr. / Err. & FS & \# Corr. / Err. & FS & \# Corr. / Err. \\  (0) Chat & Proprietary data & 66.2 & 33.2 & 23.4 / 43.6 & 39.3 & 22.3 / 36.4 & 47.5 & 28.0 / 31.3 \\  (1) SFT & - & - & 27.1 & 44.7 & 21.1 / 26.8 & 38.6 & 16.7 / 29.0 & 54.4 & 21.2 / 25.8 \\ (2) + DPO\({}^{}\) & ✓ & ✗ & 50.0 & 42.3 & 24.6 / 35.0 & 41.6 & 22.9 / 34.6 & 52.9 & 28.1 / 26.8 \\ (3) + DPO\({}^{}\) & ✗ & ✓ & 40.8 & 47.1 & 19.8 / 23.9 & 48.2 & 17.5 / 19.0 & 57.9 & 20.0 / 15.9 \\ (4) + DPO\({}^{}\) & ✓ & ✓ & **51.7** & 44.9 & 23.7 / 30.3 & 45.0 & 23.1 / 28.7 & 56.4 & 27.1 / 23.3 \\  (5) SFT\({}^{}\) & - & - & 29.1 & **49.5** & 19.9 / 19.5 & 41.4 & 18.3 / 27.7 & 54.2 & 19.3 / 22.4 \\  (6) + DPO & ✓ & ✗ & 50.4 & 46.3 & 24.0 / 28.7 & 43.9 & 21.6 / 28.8 & 55.0 & 25.4 / 22.0 \\ (7) + DPO\({}^{}\) & ✓ & ✓ & 51.2 & 47.9 & 25.9 / 28.5 & **48.7** & 24.1 / 25.5 & **58.9** & 29.0 / 22.2 \\   

Table 3: Experiments of direct preference optimization (DPO). IF. and Fact. denote instruction following \((x,y_{+},y_{-})\) and factuality \((x X^{},y_{},y_{})\) preference data, where \(X^{}\) denotes the set of fact-based instructions. \(^{}\) denotes the variant which only optimizes factuality. The preference data statistics is listed in Appendix, Table 11.

preference optimization for either instruction following or factuality alone may come at the expense of the other since the former encourages models to output long and detailed responses while the later discourages models to output false claims. When jointly conducting instruction and factuality alignment, \(^{}\) not only better follows instructions but also outputs more factual responses (row 4 vs 1, 2). Finally, initializing from \(^{}\), the DPO fine-tuned models are more factual than their counterparts (i.e., 6 vs 2 and 7 vs 4) without instruction-following capability degrade. We also list the results from Llama-2-Chat 70B (row 0) and observe that despite of its strong instruction-following capability, it tends to output many more incorrect facts. These results demonstrate that standard alignment, even on proprietary commercial data, may encourage LLMs to hallucinate. In contrast, our factuality-aware alignment guides LLMs to output more factual responses without degradation in their general instruction-following capabilities.

It is worth noting that \(^{}\) and \(^{}\) are similar to SFT and DPO fine-tuning proposed by Tian et al. (2024), which improve LLMs' factuality but degrade their instruction-following capability. Also, we do not observe our SFT and DPO variants outperform the pre-trained model with few-shot demonstrations on biography generation (row 0 in Table 2. This is possibly due to the alignment tax found in previous work (Ouyang et al., 2022), which degrades LLMs' accuracy on the standard knowledge benchmarks. How to improve both models' instruction-following capability and their accuracy on standard knowledge benchmarks is worth exploring, which we leave for future work.

### Results on TruthfulQA

Table 4 compares models performance on TruthfulQA. Generally, we observe that our factuality-aware alignment training guides LLMs to output more truthful responses. For example, factuality-aware SFT improves LLMs' truthfulness (row 5 vs 1). In addition, DPO fine-tuning on the factuality preference data guides LLMs to output more truthful responses (rows 3,4 vs 2 and 7 vs 6). Note that we observe that \(\) and \(\) models show a reverse trend in BLUE and ROUGE. This is likely because \(\) models tend to generate shorter responses than the \(\) ones do.

In addition, Table 5 reports models' accuracy in tasks of multiple choices from TruthfulQA. No significant differences between models are observed. This is possibly because we mainly focus on the tasks of long-form response generation while TruthfulQA-MC task is formed by short-form answers. The discrepancy between improving LLMs' factuality on long-form and short-form generation is also found by the previous work (Chuang et al., 2024). Appendix Table 9 reports more evaluation results on other NLP benchmarks.

## 6 Discussion

### Effects of Fact-Based Instruction Classification

In our factuality-aware alignment, we prompt \(\) to judge whether an instruction requires a factual response and apply our factuality alignment strategy to the fact-based instruction. Without the instruction classification, in our factuality-aware SFT, we cannot create supervision from \(\) and \(\) responses for respective non-fact-based and fact-based instructions. Instead, for each instruction, we create instruction-response pairs from 1 and 10 responses from \(\) and \(\) as supervisions, respectively. Note that, during fine-tuning, for each instruction, we randomly sample instruction-response pair either created from \(\) or \(\) with same probability. The SFT model shows

    &  &  \\   & IF. & Fact. & BLUE & ROUGE \\  (0) Chat &  & 0.21 & 1.16 \\  (1) \(\) & - & - & 0.37 & 0.20 \\  (2) + \(\) & ✓ & ✗ & 0.03 & 0.54 \\ (3) + \(^{}\) & ✗ & ✓ & 0.30 & 1.12 \\ (4) + \(^{}\) & ✓ & ✓ & 0.15 & 0.80 \\  (5) \(^{}\) & - & - & 0.39 & 0.51 \\ (6) + \(^{}\) & ✓ & ✗ & 0.07 & 0.91 \\ (7) + \(^{}\) & ✓ & ✓ & 0.20 & 0.96 \\   

Table 4: Results on TruthfulQA.

    &  &  \\   & IF. & Fact. & MC1 & MC2 & MC3 \\  (0) Chat &  & 32.50 & 52.54 \\  (1) \(\) & - & - & 30.8 & 45.7 & 23.9 \\ (2) + \(\) & ✓ & ✗ & 30.5 & 46.0 & 23.4 \\ (3) + \(^{}\) & ✗ & ✓ & 31.8 & 46.8 & 24.3 \\ (4) + \(^{}\) & ✓ & ✓ & 30.8 & 46.0 & 23.6 \\ (5) \(^{}\) & - & - & 29.9 & 44.8 & 22.5 \\ (6) + \(\) & ✓ & ✗ & 31.5 & 47.0 & 24.0 \\ (7) + \(^{}\) & ✓ & ✓ & 30.5 & 45.4 & 23.1 \\   

Table 5: Results on TruthfulQA multiple choices.

[MISSING_PAGE_FAIL:9]

puts responses with similar length as \(\) on Alpaca Eval, \(^{}\) generates a slightly shorter responses for the fact-based instructions in the other three datasets. This results show that our factuality-aware DPO training mainly impacts models' responses for fact-based instructions. The impact is mainly to reduce the false claims, evidenced by the numbers of erroneous facts in rows 2 and 4 of Table 3).

### Case Studies

Figure 10 (in Appendix) showcases the generations of different models, \(\), \(\) + \(\) and \(^{}\) + \(^{}\), on Alpaca Eval and Biography. Given the instruction, "What are the names of some famous actors that started their careers on Broadway?", \(\) only lists some names of Broadway actors while DPO fine-tuned models generate detailed information for each listed Broadway actor. As for biography generations, we observe that given the instruction to generate a biography for a rare name entity, Marianne McAndrew, \(\) + \(\) generates a detailed response but with many wrong facts while \(\) and \(^{}\) + \(^{}\) give relatively short responses. For the frequent entity, Ji Sung, all the models generate detailed and mostly correct responses. This qualitative analysis shows that \(^{}\) + \(^{}\) tends to generate detailed responses for most instructions, but for those instructions required tailed knowledge (e.g., rare entity) likely unknown to LLMs (Mallen et al., 2023), it reduces erroneous facts by giving less detailed responses, which is also observed by Kang et al. (2024).

## 7 Conclusion and Future Work

In this paper, we present a study to enhance the factuality of large language models (LLMs). We first identify that the standard alignment approach, comprising SFT and RLAIF with DPO, may inadvertently encourage LLMs to produce more erroneous facts. Specifically, during the SFT stage, fine-tuning LLMs with high-quality human responses may introduce unfamiliar information, prompting LLMs to output unknown facts. Additionally, during the DPO stage, enhancing LLMs' ability to follow instructions may result in more detailed and lengthy responses but often leads to increased hallucination. To tackle the shortcomings of the standard alignment, we propose a factuality-aware alignment method, which includes factuality-aware SFT and DPO. Quantitative and qualitative analyses demonstrate that our factuality-aware alignment not only guides LLMs to generate detailed and helpful responses but also helps prevent the generation of false claims.

While we have successfully integrated factuality into standard alignment procedure, our work only considers two alignment skill sets: instruction following (or helpfulness) and factuality. In practice, each instruction may require consideration of multiple and distinct alignment skill sets (Saha et al., 2024). The method to optimize for these skill sets tailored to each query requires further study. In our experiments, we note that optimizing preferences solely for instruction following or factuality could potentially compromise the other. While our factuality-aware alignment demonstrated improvements in both aspects, it is uncertain whether there is a trade-off between the two aspects when integrating our approach to large-scale alignment (Touvron et al., 2023). Finally, as shown in Appendix Figure 7, not all the claims (or sentences) in a response require fact verification, a more accurate factuality reward model should take this factor into account. While our preliminary experiment, which removes non-fact-based sentences from the factuality reward modeling (Section 6.2), shows suboptimal performance, we believe that further study can bring more insights.