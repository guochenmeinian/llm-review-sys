# Online Feature Updates Improve Online (Generalized) Label Shift Adaptation

Ruihan Wu

UC San Diego

ruu076@ucsd.edu

Equal contributionWork done while at Cornell University

&Siddhartha Datta

University of Oxford

siddhartha.datta@cs.ox.ac.uk

&Yi Su

Google DeepMind

yisumtv@google.com

&Dheeraj Baby

UC Santa Barbara

dheeraj@ucsb.edu

&Yu-Xiang Wang

UC San Diego

yuxiangw@ucsd.edu

&Kilian Q. Weinberger

Cornell University

kilian@cornell.edu

###### Abstract

This paper addresses the prevalent issue of label shift in an online setting with missing labels, where data distributions change over time and obtaining timely labels is challenging. While existing methods primarily focus on adjusting or updating the final layer of a pre-trained classifier, we explore the untapped potential of enhancing feature representations using unlabeled data at test-time. Our novel method, Online Label Shift adaptation with Online Feature Updates (OLS-OFU), leverages self-supervised learning to refine the feature extraction process, thereby improving the prediction model. By carefully designing the algorithm, theoretically OLS-OFU maintains the similar online regret convergence to the results in the literature while taking the improved features into account. Empirically, it achieves substantial improvements over existing methods, which is as significant as the gains existing methods have over the baseline (i.e., without distribution shift adaptations).

## 1 Introduction

The effectiveness of most supervised learning models relies on a key assumption that the training data and test data share the same distribution. However, this assumption rarely holds in real-world scenarios, leading to the phenomenon of _distribution shift_. Previous research has primarily focused on understanding distribution shifts in offline or batch settings, where a single shift occurs between the training and test distributions . In contrast, real-world applications often involve test data arriving in an _online_ fashion, and the distribution shift can continuously evolve over time. Additionally, there is another challenging issue of _missing and delayed_ feedback labels, stemming from the online setup, where gathering labels for the streaming data in a timely manner becomes a challenging task.

To tackle the distribution shift problem, prior work often relies on additional assumptions regarding the nature of the shift, such as label shift or covariate shift . In this paper, we focus on the common _(generalized) label shift_ problem in an online setting with missing labels . Specifically, the learner is given a fixed set of labeled training data \(D_{0}^{}\) in advance and trains a model \(f_{0}\). During test-time, only a small batch of unlabelled test data \(S_{t}^{}_{t}\) arrives in an online fashion (\(t=1,2,\)). For the online label shift, we assume the label distribution \(^{}_{t}(y)\) may change over time \(t\) while the conditional distribution remains the same, i.e. \(^{}_{t}(x|y)=^{}(x|y)\). For example, employing MRI image classifiers for concussion detection becomes challenging as label shifts emerge fromseasonal variations in the image distribution. A classifier trained during skiing season may perform poorly when tested later, given the continuous change in image distribution between skiing and non-skiing seasons. In contrast to label shift, the _generalized_ label shift relaxes the assumption of an unchanged conditional distribution on \(x\) given \(y\). Instead, it assumes that there exists a transformation \(h\) of the covariate, such that the conditional distribution \(_{t}^{}(h(x)|y)=^{}(h(x)|y)\) stays the same. Reiterating our example, consider an MRI image classifier that undergoes training and testing at different clinics, each equipped with MRI machines of varying hardware and software versions. As a result, the images may display disparities in brightness, resolution, and other characteristics. However, a feature extractor \(h\) exists, capable of mapping these variations to the same point in the transformed feature space, such that the conditional distribution \((h(x)|y)\) remains the same. In both settings, the goal of the learner is to adapt to the (generalized) label shift within the non-stationary environment, continually adjusting the model's predictions in real-time.

Existing algorithms for online label shift adaptation (OLS) primarily adopt one of two approaches: either directly reweighting of the pretrained classifier \(f_{0}\), or re-training only the final linear layer of \(f_{0}\) -- while keeping the feature extractor frozen. Recent work  have demonstrated the potential for improving feature extractors, even during test-time and in the absence of labeled data. We hypothesize that a similar effect can be harnessed in the context of (generalized) label shift, leading to the idea of _improving feature representation learning during testing_. In online label shift, updating the feature extractor offers two potential advantages. First, it utilizes the additional unlabeled samples, hence enhancing the sample efficiency of the feature extractor. Second, it enables the feature extractor to adapt to label shift, which is crucial to the learning process as the optimal feature extractor may depend on the underlying label distribution. Particularly in generalized label shift scenarios, where the feature transformation \(h\) is often unknown, the integration of extra unlabeled test samples facilitates the learning of \(h\).

Building upon this insight, this paper introduces the _Online Label Shift adaptation with Online Feature Updates_ (OLS-OFU) framework, aimed at enhancing feature representation learning in the context of online label shift adaptation. Specifically, each instantiation of OLS-OFU incorporates a self-supervised learning method associated with a loss function denoted as \(l_{}\) for feature representation learning, and an online label shift adaptation (OLS) algorithm to effectively address distribution shift. By carefully checking the existing OLS methods and SSL methods, we identify three principles for algorithm design: maintain the theoretical guarantee, obey the underlying assumption of the existing OLS methods and fit the required condition of SSL techniques while avoiding heavy additional computational costs. Within the principles, OLS-OFU is designed as three main steps: at each time step, OLS-OFU first executes a revised OLS algorithm and then every \(\) steps, OLS-OFU updates the feature extractor through self-supervised learning and subsequently refines the final linear layer.

In addition to its ease of implementation and seamless integration with existing OLS algorithms, OLS-OFU also shows strong theoretical guarantee and empirical performance. Theoretically, we demonstrate that OLS-OFU effectively reduces the loss of the overall algorithm by leveraging self-supervised learning (SSL) techniques to enhance the feature extractor, thereby improving predictions for test samples at each time step \(t\). Empirical evaluations on various datasets, considering both online label shift and online generalized label shift scenarios, validate the effectiveness of OLS-OFU. Our OLS-OFU method achieves substantial improvements over existing OLS methods, which is _as significant as the gains existing OLS methods have over the baseline (i.e., without distribution shift adaptations)_. This demonstrates that integrating online feature updates is as effective in solving online distribution shift as the fundamental online label shift method itself. Moreover, the improvement is consistent on various datasets, all existing OLS methods and the various choices of SSL techniques. This consistency underscoring its robustness across different scenarios and its generality to incorporate future OLS methods with more advanced online learning techniques and better SSL techniques.

Problem Setting & Related Work

We start with some basic notations. Let \(^{K-1}\) be the probability simplex. Let \(f:^{K-1}\) denote a classifier. Given an input \(x\) from domain \(\), \(f(x)\) outputs a probabilistic prediction over \(K\) classes. For example, \(f\) can be the output from the softmax operation after any neural network. If we reweight a model output from \(f\) by a vector \(p^{K}\), we refer to this model as \(g(;f,p)\) with \(g\) denotes the method of reweighting. For any two vectors \(p\) and \(q\), \(p/q\) denotes the element-wise division.

**Online distribution shift adaptation.** The effectiveness of any machine learning model \(f\) relies on a common assumption that the train data \(D_{0}\) and test data \(D_{}\) are sampled from the same distribution, i.e., \(^{}=^{}\). However, this assumption is often violated in practice, which leads to _distribution shift_. This can be caused by various factors, such as data collection bias and changes in the data generation process. Moreover, once a well-trained model \(f_{0}\) is deployed in the real world, it moves into the testing phase, which is composed of a sequence of periods or time steps. The test distribution at time step \(t\), \(^{}_{t}\), from which test data \(x_{t}\) is sampled, may vary over time. One example is that an MRI image classifier might be trained on MRI images collected during skiing season (which may have a high frequency of head concussions) but tested afterward (when the frequency of concussion is lower). The test stage can last several months until the next classifier is trained. During this test period, the distribution of MRI images may undergo continuous changes, transitioning between non-skiing and skiing seasons.

As the test-time distribution changes over time, the challenge lies in how to adjust the model continuously from \(f_{t-1}\) to \(f_{t}\) in an online fashion to adapt to the current distribution \(^{}_{t}\). We call this problem _online distribution shift adaptation_ and illustrate it in Figure 1. Given a total of \(T\) steps in the online test stage, we define the average loss for any online algorithm \(\) through the loss of the sequence of models \(f_{t}\), \(t[T]\) that are produced from \(\), i.e.,

\[L(;^{}_{1},,^{}_{ T})=_{t=1}^{T}(f_{t};^{}_{t}), \]

where \((f;)=_{(x,y)}_{}(f(x),y)\) and \(_{}\) is the loss function, for example, \(0\)-\(1\) loss or cross-entropy loss for classification tasks.

In this paper, we consider the challenging scenario where at each time step \(t\), only _a small batch of unlabeled samples \(S_{t}=\{x^{1}_{t},,x^{B}_{t}\}\)_ is received. We formalize the algorithm \(\) as: \( t[T]\),

\[f_{t}:=(\{S_{1},,S_{t-1}\},\{f_{1},,f_{t-1}\},D_{ 0},D^{}_{0}). \]

In contrast to the classical online learning setup, this scenario presents a significant challenge as classical online learning literature usually relies on having access to either full or partial knowledge of the loss at each time step, i.e., \(_{}(f(x_{t}),y_{t})\). In this setting, however, only a batch of unlabeled samples is provided at each time step and this lack of access to label information and loss values presents a significant challenge in accurately estimating the true loss defined in Equation 1.

**Online label shift (OLS) adaptation.** Online label shift assumes the marginal distribution of the label \(^{}_{t}(y)\) changes over time, while the conditional distribution keeps invariant:

\[ t[T],^{}_{t}(x|y)=^{} (x|y).\]

This assumption is most typical when the label \(y\) is the causal variable and the feature \(x\) is the observation . The aforementioned example of concussion detection from MRI images fits this scenario, where the presence or absence of a concussion (label) causes the observed MRI image features. Most previous methods tackle this problem through a non-trivial reduction to the classical online learning problem. Consequently, most of the online label shift algorithms [49; 7; 6] study the theoretical guarantee of the algorithm via the convergence of the regret function, which could be either static regret or dynamic regret. In previous studies, the hypothesis class \(\) of the prediction function \(f\) is typically chosen in one of two ways:

1. \(\) is defined as a family of post-hoc reweightings of \(f_{0}\), with the parameter space comprising reweight vectors. Examples within this category include ROGD , FTH , and FLHFTL .
2. \(\) is defined as a family of functions that share the same parameters in \(f_{0}\) except the last linear layer, such as UOGD  and ATLAS .

Notice that the existing OLS methods don't update the feature extractor at the online test stage, but focus on how to leverage advanced online learning techniques to update the remaining part of the model under certain theoretical guarantees. Our method will be orthogonal to them: it will focus on how to leverage the self-supervised learning techniques to update the feature extractor and, therefore, can improve each of them. Our empirical results in fact show that the improvement from the feature extractor update is as significant as the improvement from online learning techniques.

**Online _generalized_ label shift adaptation.** In the context of MRI image classification, where head MRI images serve as the feature \(x\), variations in software or hardware across different clinics' MRI machines can introduce discrepancies in image characteristics like brightness, contrast, and resolution. In such scenarios, the conditional probability distribution \((x|y)\) is no longer invariant. However, when a feature extractor \(h\) is robust enough, it can map images into a feature space where the images from different machines have the same distributions \((h(x)|y)\) in the transformed feature space. The concept of generalized label shift, as introduced in Tachet des Combes et al. , formalizes this by postulating the existence of an unknown function \(h\), such that \((h(x)|y)\) remains invariant. The primary challenge in this context is to find the underlying transformation \(h\). Building upon this, online generalized label shift assumes that there exists an unknown function \(h\) such that

\[ t[T],_{t}^{}(h(x)|y)=^{}(h(x)|y).\]

Thus, to apply the OLS methods for this more general problem, it is important to learn a proper feature extractor as this underlying function \(h\).

**Motivations of deploying self-supervised feature updates.** As we reviewed above, OLS methods in the literature don't update the pretrained feature extractor in the online stage. However, the pretrained feature extractor in \(f_{0}\) can be suboptimal for the online test stage in online (generalized) label shift adaptation, because of the three potential reasons:

1. The amount of pretrained data doesn't achieve the learning capacity of the feature extractor structure, i.e. learning with more data can improve the feature extractor.
2. The optimal feature extractors can be different for two different distributions. In our problem, the distribution shifts overtime and the optimal feature extractor can be dynamic too.
3. Particularly in online generalized label shift adaptation, the domain of the data can be dramatically changed between train and test, and the pretrained feature extractor could have unpredictable performance for the test data.

Fortunately, in online (generalized) label shift adaptation problem, the learner receives many unlabeled test samples \(S_{1} S_{t-1}\) before the prediction for time step \(t\). On the other hand, self-supervised learning is very powerful for representation learning [18; 32; 30; 16; 37; 9; 23; 20; 24] and domain adaptation [46; 48; 36; 39]. In this paper, we will introduce how to deploy these self-supervised learning techniques to the existing OLS methods such that we can still enjoy the theoretical guarantees from online learning techniques and, more importantly, take the advantage of self-supervised learning to achieve better empirical performance.

## 3 Method

In this paper, we introduce a novel online label shift adaptation algorithm OLS-OFU, that leverages self-supervised learning (SSL) to improve representation learning and can be seamlessly integrated with any existing online label shift (OLS) method. By carefully designing how to place the SSL techniques, our algorithm maintains a similar theoretical guarantee from the existing OLS methods, obeys the underlying assumption in existing OLS methods that is important for the effectiveness, and fits the required condition of the SSL techniques while avoiding heavy additional time cost. Through the derived theoretical results, we further understand how the online learning techniques and SSL techniques contribute together to reduce the test loss in the online label shift adaptation problem. Lastly, we demonstrate how the SSL techniques help with the more challenging problem online generalized label shift adaptation.

### Three Principles of Algorithm Design.

To combine the step of feature extractor update with any OLS method, the most straightforward thought is to directly insert this step right before or after the step of original OLS in each time step;see the summarization of the online process in Figure 1. Starting from this thought, there are three remaining questions before finalizing the algorithm: **Q1**: Inserting this step before or after the original OLS step, which option is better? **Q2**: Besides this feature extractor update step, are any other steps also necessary? **Q3**: How frequently (in terms of time step \(t\)) should we update the feature extractor? We are going to introduce three principles when designing our algorithm, which helps answer the three questions and together lead to our final design.

**Principle 1: maintain the theoretical guarantee.** One main advantage of existing OLS methods is that they have theoretical guarantees for the performance of any unknown label shifts in the online test stage. We would like to keep this advantage after deploying the feature extractor update step. By carefully checking the theoretical analysis of the existing OLS methods, we find that when we deploy the feature extractor update into ROGD, UOGD, or ATLAS, if we would like to update the feature extractor by the unlabeled test samples including \(S_{t}\), it is necessary to execute this update _after_ the step of these OLS methods at time \(t\) to maintain the similar theoretical guarantee. This is because the main idea of ROGD, UOGD, and ATLAS is to construct an _unbiased_ estimator for the gradient \(_{f}(f_{t};)\) using a batch of samples \(S_{t}_{t}^{}\). The estimator has the form of

\[_{y}s_{t}[y]_{f}_{(x,y) ^{}}_{}(f_{t}(x),y), \]

where \(s_{t}\) is an unbiased estimator for the label marginal distribution \(q_{t}\) and it depends on samples \(S_{t}\) as constructed. If the feature extractor update according to \(S_{t}\) happened before the step of ROGD, UOGD or ATLAS, \(f_{t}\) would not be independent of \(s_{t}\) in Equation 3 and this can break their main idea, as illustrated in the following proposition.

**Proposition 1**.: _If \(f_{t}\) is not independent of the samples \(S_{t}\), the gradient estimator in Equation 3 is not guaranteed to be an unbiased estimator of the gradient \(_{f}(f_{t};)\)._

Thus, to maintain a similar theoretical guarantee after updating the feature extractor by the online test samples, _we should insert the feature update involving \(S_{t}\) after the step of original ROGD, UOGD or ATLAS_, and this answers Q1. In the experiment, we will validate the necessity of this design.

**Principle 2: obey the underlying assumption of the existing OLS methods.** We find that the existing OLS methods ROGD, FTH, and FLHFTL opt for the hypothesis \(f\) to be a post-hoc reweight of the pre-trained model \(f_{0}\). The underlying assumption behind this design is that \(f_{0}\) is a good approximation of \(^{}(y|x)\). Within this assumption, because \(^{}(y|x)\) is a post-hoc reweight of \(^{}(y|x)\) in the case of label shift , reweighting such \(f_{0}\) can approach \(^{}(y|x)\). Therefore, after updating the feature extractor, we expect that the base model, which is to be reweighted, still approximates the training distribution \(^{}(y|x)\). This can be done by _re-training the linear layer under the training data after the feature extractor update_, and this answers Q2.

**Principle 3: fit the required condition of the SSL techniques while avoiding heavy additional computational costs.** Given a set of unlabeled samples \(S\), we denote the loss of an SSL technique for a model \(f\) as \(_{}(S;f)\) and the update to this model would be in the form of gradient descent: \(^{}^{}-_{ ^{}}_{}(S;f)\), where \(\) is the learning rate. We notice that the batch size \(|S|\) cannot be very small. Otherwise, the update can be too noisy due to the data variance or some types of SSL whose benefit replies on large batch sizes, such as contrastive learning , would lose this benefit. However, online (generalized) label shift problem assumes we only have a small batch of unlabeled samples \(S_{t}\) at time \(t\); the batch size is set as \(1\) or \(10\) in the experiment of literature. Therefore, for the effectiveness of SSL techniques, instead of updating the feature extractor at each time step by \(S_{t}\), we opt to accumulate the sample batch \(S_{ c} S_{(c+1)}\) and update the feature extractor once every \(\) time steps, and we call it _batch accumulation_.

Besides the help of effectiveness, _batch accumulation_ also helps with time efficiency. The existing OLS methods only involve the updates for the small reweighting vector or the linear layer and the time cost is low. However, updating the feature extractor and re-training the linear layer on the full training set, which are introduced in Principle 2, are much computationally heavier. For example, when we evaluated the methods in the experiment with ResNet18 on CIFAR10 dataset, one step of FLHFTL only took \(0.069\) second, while one step of FLHFTL together with feature extractor update and linear layer re-training took \(17.1\) seconds, which is \(247\) times. After applying _batch accumulation_ and we select \(=100\), the additional time costs will be reduced by \((-1)/=99\%\).

_Batch accumulation_ answers \(Q_{3}\) - _we should update the feature extractor every \(\) steps for effectiveness and time efficiency_. We will study how _batch accumulation_ helps in the experiments.

```
Require: An online label shift adaptation algorithm OLS, a self-supervised learning loss \(_{}\). A pre-trained model \(f_{0}\) and initialize \(f_{1}=f^{}_{1}:=f_{0}\). for\(t=1,,T\)do Input at time \(t\): Samples \(S_{1} S_{t}\), models \(\{f_{1},,f_{t}\}\), train set \(D_{0}\), validation set \(D^{}_{0}\).  1. Run the revised version of OLS, that is, OLS-R, and get \(f^{}_{t+1}\). If\(t\% 0\), \(f_{t+1}:=f^{}_{t+1}\), \(f^{}_{t+1}:=f^{}_{t}\) Else, go to the next step 2-3.  2. Update the feature extractor \(^{}_{t}\) in \(f^{}_{t+1}\) by Equation 4. Replace \(^{}_{t}\) in \(f^{}_{t+1}\) by \(^{}_{t+1}\).  3. Re-train the last linear layer by Equation 5, calibrate the model and get \(f^{}_{t+1}\). Output at time \(t\): For the reweighting OLS methods, denote the latest reweighting vector from Step 1 is \(p_{t+1}\) and we define \(f_{t+1}:=g(;f^{}_{t+1},p_{t+1})\); else, we define \(f_{t+1}:=f^{}_{t+1}\). endfor
```

**Algorithm 1** Online label shift adaptation with online feature updates (OLS-OFU).

### Online Label Shift Adaptation with Online Feature Updates

We have discussed three principles and now we can finalize our algorithm _Online Label Shift adaptation with Online Feature Updates_ (OLS-OFU; Algorithm 1), which requires a self-supervised learning loss \(_{}\) and one of existing OLS methods in the literature, which either reweights the offline pre-trained model \(f_{0}\) or updates the last linear layer. In the training stage, we train \(f_{0}\) by minimizing the supervised and self-supervised loss together defined on train data. In the test stage, OLS-OFU comprises three steps at each time step \(t\): (1) running the refined version of OLS, which we refer to as OLS-R, (2) updating the feature extractor, and (3) re-training the last linear layer. As suggested by Principle 3 in Section 3.1, steps (2-3) only run every \(\) steps. We illustrate the details of these three steps are elaborated below.

**(1) Running the Revised OLS.** At the beginning of the time \(t\), we use \(f^{}_{t}\) to denote the model within the latest feature extractor and the re-trained linear model (using data \(\{S_{0},,S_{t-1}\}\)). The high level idea of our framework centers on substituting the pre-trained model \(f_{0}\) used in existing OLS methods with our updated model \(f^{}_{t}\), which we call _OLS-R_. To provide an overview, we examine common OLS methods (FLHFTL, FTH, ROGD, UOGD, and ATLAS) and identify two primary use cases of \(f_{0}\). Firstly, all OLS methods rely on an unbiased estimator \(s_{t}\) of the label distribution \(q_{t}\) and \(f_{0}\) is a part of the estimator. Secondly, the hypothesis \(f\) is some weights of reweighting the \(f_{0}\) output or only the last linear layer in \(f_{0}\) is updated. We illustrate all revised OLS methods formally in Appendix C. We use \(f^{}_{t+1}\) to denote the model after running the OLS-R.

**(2) Updating the Feature Extractor.** As guided by Principle 1 in Section 3.1, the step of updating the feature extractor should be after the Revised-OLS module (step (1)) for theoretical guarantees. We now introduce how to utilize the SSL loss \(_{}\) to update the feature extractor based on the accumulated batch of unlabeled test data \(S_{rc} S_{(c+1)}\) at timestep \(t=(c+1)\). Specifically, let \(^{}_{t}\) denote the parameters of the feature extractor in \(f^{}_{t+1}\). The update of \(^{}_{t+1}\) is given by:

\[^{}_{t+1}:=^{}_{t}-_{ ^{}}_{}(S_{ c} S_{(c+1)};f^{ }_{t+1}). \]

Then we update the feature extractor in \(f^{}_{t+1}\) with \(^{}_{t+1}\).

**(3) Re-training Last Linear Layer.** Principle 2 in Section 3.1 suggests this step of re-training the last linear layer on the training distribution. The re-training starts from random initialization while the feature extractor remains frozen. The training objective of \(^{}_{t+1}\) is to minimize the average cross-entropy loss over train data \(D_{0}\). We denote the model with the frozen feature extractor \(^{}_{t+1}\) as \(f(|^{}_{t+1},^{})\). The objective for re-training the last linear layer can be written as follows:

\[^{}_{t+1}:=_{_{}}_{(x,y) D _{0}}_{}f(x|^{}_{t+1},^{}),y. \]

We calibrate the model \(f(|^{}_{t+1},^{}_{t+1})\) by temperature calibration  using the validation set \(D^{}_{0}\) and denote the model after calibration as \(f^{}_{t+1}\).

**Output at time \(t\).** Finally, we define \(f_{t+1}\) for the next time step prediction. For the reweighting OLS methods (ROGD, FTH, FLHFTL), denote the latest reweighting vector from Step 1 as \(p_{t+1}\) and define \(f_{t+1}:=g(;f^{}_{t+1},p_{t+1})\). For those which optimize the last linear layer (UOGD, ATLAS), we define \(f_{t+1}:=f^{}_{t+1}\) - as stated in Principle 2 in Section 3.2, the \(f^{}_{t}\) including retrained last linear layer serves specially for the reweighting OLS method.

**The requirement of the training data.** Notice that in step (3), we need the training data to retrain the linear classifier, which potentially brings the additional cost of memory and computation in the test time. As discussed in Principle 2 and lines 248-252, retraining the last linear layer is designed only for three previous OLS methods (ROGD, FTH, FLHFTL); our algorithm for two other OLS methods (UOGD and ATLAS) in the literature are independent of this step. As for ROGD, FTH, or FLHFTL, we interestingly find that OLS-OFU without this re-training step, which means that we update the feature extractor but reuse the pretrained linear classifier, still has a certain advantage over the original OLS methods - although more stored training data results in the more significant benefit of OLS-OFU. This suggests that in practice, if we reduce the amount of stored training data due to the memory or computational constraint, OLS-OFU is still effective.

### Performance Guarantee for Online Label Shift Adaptation

One main advantage of the original OLS methods is that they exhibit theoretical guarantees in terms of regret convergence for online label shift settings. Next, we will show how OLS-OFU demonstrates analogous theoretical guarantees with the incorporation of additional online feature updates. Due to limited space, we illustrate the theoretical results pertaining to FLHFTL-OFU here, and present the results for ROGD-OFU, FTH-OFU, UOGD-OFU, and ATLAS-OFU in Appendix D.

**Theorem 1**.: _[**Regret convergence for FLHFTL-OFU**] Suppose we choose the OLS-R to be FLHFTL-R (Algorithm 6) from Baby et al. . Let \(f_{t}^{}\) be the output at time step \(t-1\) from Algorithm 1, that is \(g(;f^{}_{t},}{q_{0}})\). Under Assumptions 1 and 2 in Baby et al. , FLHFTL-OFU has the guarantee:_

\[[_{t=1}^{T}(f_{t}^{};^{}_{t})][_{t=1}^{T}(g( ;f^{}_{t},}{q_{0}});^{}_{t} ]+O(V_{T}^{1/3}}{T^{1/3}}+}), \]

_where \(V_{T}:=_{t=1}^{T}\|q_{t}-q_{t-1}\|_{1}\), \(K\) is the number of classes, and the expectation is taken w.r.t. randomness in the revealed co-variates. This result is attained without prior knowledge of \(V_{T}\)._

**How do online feature updates contribute to the bound?** We observe that the upper bound of the test loss has two terms. The first term is the loss of the model within the up-to-date feature extractor when the knowledge of label distribution \(q_{t}\) is known. Any improvement from SSL could be reflected in this first term. The second term is to quantify the loss gap between the knowledge of label distribution \(q_{t}\) and the estimation of the label distribution by the online learning technique.

**When do online feature updates improve the guarantee from FLHFTL to FLHFTL-OFU?** If we do not make any update to the feature extractor (i.e. \(f^{}_{t}=f_{0}, t\)), the upper bound in Equation 6 would be naturally reduced to the theoretical guarantee of FLHFTL . Moreover, in the following event of \(f^{}_{t}\) (\(t[T]\)):

\[[_{t=1}^{T}(g(;f^{}_{t}, }{q_{0}});^{}_{t}]<_{t=1} ^{T}(g(;f_{0},}{q_{0}});^{}_{t}), \]

our theorem will guarantee that the loss of FLHFTL-OFU converges to a smaller value than the one of the original FLHFTL, resulting in a better upper bound. We substantiate this improvement through empirical evaluation in Section 4.

### Online Feature Updates Improve Online Generalized Label Shift Adaptation

The generalized label shift is harder because the feature map \(h\) is unknown and naively applying OLS methods might raise the challenge due to the violation of the label shift assumption. Fortunately, existing research in test-time training (TTT) [46; 48; 36; 39] demonstrates that feature updates driven by SSL align the source and target domains in feature space. When the source and target domains achieve perfect alignment, such feature extractor effectively serves as the feature map \(h\) as assumed in generalized label shift. Therefore, the sequence of feature extractors in \(f_{1},,f_{T}\) generated by Algorithm 1 progressively approximates the underlying \(h\). This suggests that, compared to the original OLS, OLS-OFU experiences a milder violation of the label shift assumption within the feature space and is expected to have better performance in online generalized label shift settings.

## 4 Experiment

In this section, we initiate OLS-OFU with three popular SSL techniques and empirically evaluate how OLS-OFU improves the original OLS methods on both online label shift and online generalized label shift on various datasets and shift patterns3.

### Experiment Set-up

**Dataset and Label Shift Settings.** For online label shift, we evaluate the efficacy of our algorithm on CIFAR-10 , STL10 , CINIC , and EuroSAT . For each dataset, we split the original train set into the offline train (i.e., \(D_{0}\)) and validation sets (i.e., \(D_{0}^{}\)) following a ratio of \(4:1\). At the online test stage, unlabeled batches are sampled from the test set. For online generalized label shift, the offline train and validation sets are the CIFAR-10 images. The test unlabeled batches are drawn from CIFAR-10C , a benchmark with the same objects as CIFAR-10 but with various types of corruption. We experiment with three types of corruptions with CIFAR-10C: Gaussian noise, Fog,

   & FTFWH & FTH & ROGD & ATLAS & UOGD & FLH-FTL \\  OLS & 11.9\% / 0.441 & 12.04\% / 0.705 & 13.65\% / 0.937 & 12.18\% / 43.2 & 11.54\% / 297 & 12.02\% / 1.15 \\  OLS-OFU (\(=1\)) & 11.3\% / 233 & 11.2\% / 286 & 13.9\% / 385 & 11.6\% / 47 & 11.4\% / 319 & 11.2\% / 285 \\ OLS-OFU (\(=10\)) & 9.93\% / 28.8 & 10.2\% / 29.6 & 13.1\% / 41.9 & 11.6\% / 45.1 & 10.5\% / 306 & 9.93\% / 29.9 \\ OLS-OFU (\(=50\)) & 8.15\% / 6.47 & 8.89\% / 6.91 & 12\% / 9.49 & 11.3\% / 44.8 & 9.27\% / 303 & 8.27\% / 7.08 \\ OLS-OFU (\(=100\)) & 7.33\% / 3.57 & 8\% / 3.85 & 10.8\% / 5.29 & 10.1\% / 44.2 & 8.35\% / 303 & 7.45\% / 4.15 \\ OLS-OFU (\(=500\)) & 11.6\% / 1.27 & 11.7\% / 1.53 & 13.5\% / 2.09 & 12.1\% / 43.6 & 11.3\% / 298 & 11.7\% / 1.94 \\  

Table 1: Average error/time (minutes) of 6 original OLS methods versus OLS-OFU with various frequency \(\) in _batch accumulation_. The SSL in OLS-OFU is the rotation degree prediction.

Figure 2: Evaluation of OLS and OLS-OFU.

and Pixelate. We follow Bai et al.  and Baby et al.  to simulate the online label distribution shift with two shift patterns: Sinusoidal shift and Bernoulli shift. We experiment with \(T=1000\) and batch size \(B=10\) at each time step, following Baby et al. . See more details of dataset set-up and online shift patterns in Appendix E.1.

**Evaluation Metric.** We report the average error during test, i.e., \(_{t=1}^{T}_{x_{t} S_{t}}\ (f_{t}(x_{t}) y_{t})\), where \((x_{t},y_{t})_{t}^{}\), to approximate \(_{t=1}^{T}(f_{t};_{t}^{})\) for the evaluation efficiency.

**Self-supervised learning methods in OLS-OFU.** In the experiment, we narrow our focus on three particular SSL techniques in the evaluation for classification tasks: _rotation degree prediction_, _entropy minimization_ and _MoCo_. It is important to note that this concept extends beyond these three SSL techniques, and the incorporation of more advanced SSL techniques to further elevate the performance. Appendix E.1 gives more details of these SSL techniques.

**Set-ups of OLS methods, OLS-OFU and baselines.** We perform an extensive evaluation of 6 OLS methods in the literature: FTFWH, FTH, ROGD, UOGD, ATLAS, and FLHFTL by following the implementation in Baby et al. . We report the performance of our method OLS-OFU (Algorithm 1) applied on top of each OLS and 3 SSL methods introduced above. The frequency parameter \(\) is fixed as \(100\) for most experiments unless we particularly mention it. Additionally, by following the setup in , we report one baseline score _Base_, which uses the fixed pre-trained model \(f_{0}\) to predict the labels at all time steps.

### Results

**Main Results: comparison between OLS-OFU and OLS under online (generalized) label shift.** Figure 2(a) shows the performance comparison between OLS-OFU, implemented with _three SSL methods_, and their corresponding OLS counterparts on CIFAR-10 under the scenario of _classical online label shift_. Figure 2(b) shows the results on _three more datasets_ with SSL technique in OLS-OFU being rotation degree prediction. Figure 2(c) shows the results on CIFAR-10C datasets for evaluating methods on _online generalized label shift_. The online shift pattern in Figure 2 is the sinusoidal shift and similar results on the Bernoulli shift are in Appendix E.2. We have two main observations from the results. First, we find our OLS-OFU method achieves substantial improvements over existing OLS methods, which is _as significant as to the gains existing OLS methods have over the baseline (i.e., without distribution shift adaptations)_. This demonstrates that integrating online feature updates is as effective in solving online distribution shifts as the fundamental online label shift method itself. Second, the improvement is consistent across all six original OLS methods and three SSL techniques on all datasets, which further demonstrates our OLS-OFU is general enough to incorporate future OLS methods and more advanced SSL techniques as well.

**Validating Principle 1: ablation study on the order between OLS and the feature update step in OLS-OFU.** We compare OLS-OFU with its other variant named OLS-OFU-difforder where we update SSL first and run OLS later (which violates Principle 1). We compare these two algorithms across all previous 6 OLS methods and two choices of batch accumulation \(=1\) and \(=100\). The dataset is CIFAR-10, the SSL is rotation degree prediction and the shift pattern is the sinusoidal shift. We report the average error every \(\) time step when the feature extractor is updated, so the order matters. From Table 2, we can observe that benefiting from Principle 1, the error OLS-OFU is consistently lower than OLS-OFU-difforder. This means that Principle 1 indeed is not only necessary for the correctness of theoretical guarantee but also plays an important role in practice.

**Validating Principle 3: ablation study on frequency \(\) in _batch accumulation_ in OLS-OFU.** In section 3.1, we introduce the _batch accumulation_ to flavor the effectiveness of the SSL and reduce the additional time cost of OLS-OFU compared with the original OLS methods. In Table 1, we

    & FTFWH & FTH & ROGD & ATLAS & UOGD & FLHFTL \\  OLS-OFU(\(=1\)) & 11.3\% & 11.2\% & 13.9\% & 11.6\% & 11.4\% & 11.2\% \\ OLS-OFU-difforder(\(=1\)) & 12.33\% & 12.12\% & 14.35\% & 12.10\% & 11.91\% & 12.08\% \\ OLS-OFU(\(=100\)) & 7.23\% & 7.91\% & 10.81\% & 10.12\% & 8.33\% & 7.53\% \\ OLS-OFU-difforder(\(=100\)) & 8.02\% & 8.63\% & 11.19\% & 10.55\% & 8.80\% & 8.29\% \\   

Table 2: Ablation study on the order between OLS and the feature update step in OLS-OFU.

evaluate OLS-OFU on CIFAR-10 with various \(\) while initiating OLS-OFU with rotation degree prediction and compare the average error and time with OLS. We have these observations across all OLS (columns): 1. OLS-OFU with all \(\) outperforms OLS; 2. The average error of OLS-OFU decreases from \(=1\) (_w/o_ batch accumulation) to \(=100\) and starts to increase after and this is because the effectiveness of SSL benefits from \(>1\) while larger \(\) means less updates and hence hurts the long term performance; 3. the additional time cost of OLS-OFU with larger \(\) is smaller and \(=100\) gives a great balance of performance and time cost for all OLS (and SSL; the similar tables for other two SSL are presented in the Appendix E.3). From the results, we recommend \(=100\) as a good starting point for choosing this parameter.

**Ablation study on the amount of stored training data in step (3).** We further study how the amount of training data stored influences the effectiveness of our OLS-OFU when the OLS is ROGD, FTH, or FLHFTL, which is dependent on step (3). We experiment with the CIFAR-10 dataset and the shift pattern of sinusoidal shift; the SSL is chosen as rotation degree prediction. The percentage of stored training data (out of the whole training set of size 10,000) is varied in \(\{100\%,80\%,60\%,40\%,20\%,10\%,5\%,0\%\}\); \(0\%\) means that we still update the feature extractor but reuse the pretrained linear classifier. The results are reported in Table 3. We can observe that with less stored training data for retraining the last linear layer, the error of OLS-OFU would increase gradually. However, an important finding is that even with \(0\%\) stored training data, the error of OLS-OFU is still lower than the OLS without feature extractor updates. This can actually explained by the original test-time training papers , where they only update the feature extractor without refining the last linear layer and still have substantial benefit. The results suggest that a large amount of stored training data is not necessary for the effectiveness of OLS-OFU, while more stored data can bring more benefits.

**Empirical validation of Equation 7.** In Section 3.3, we argued that when the inequality in Equation 7 holds, the loss of FLHFTL-OFU exhibits a tighter upper bound compared to FLHFTL. Figure 3 presents the RHS (corresponds to OLS) and LHS (corresponds to OLS-OFU with SSL loss as rotation degree prediction) of Equation 7. We perform the study over eight different settings and vary the domain shift and online shift patterns. It is evident that OLS-OFU yields improvements on the _baseline_ of the regret as shown in Equation 7. Appendix E.4 validates this inequality for other SSL.

## 5 Conclusion and Future Work

We focus on online (generalized) label shift adaptation and present a novel framework OLS-OFU, which harnesses the power of self-supervised learning to enhance feature representations dynamically during testing, leading to improved predictive models and better test time performance.

**Discussion and future work.** One promising direction is to extend the idea of this paper to online covariate shift, for example, the algorithm in Zhang et al.  freezes the feature extractor and only updates the linear layer. Another possible direction is to consider a more realistic domain shift scenario within the generalized label shift setting -- domain shift types may vary over time or be even more challenging, such as shifting from cartoon images to realistic images.

Figure 3: Empirical validation of Equation 7. Clean denotes the experiment on CIFAR-10, and others denote the corruption type. They are paired with two online shift patterns: Sinusoidal and Bernoulli.

 \% Stored Training Data & 100\% & 80\% & 60\% & 40\% & 20\% & 10\% & 5\% & 0\% & OLS only \\ 
**FTH-OFU** & 8\% & 8.18\% & 8.68\% & 9.49\% & 9.54\% & 9.67\% & 9.81\% & 10.24\% & 12.04\% \\
**ROGD-OFU** & 10.8\% & 10.93\% & 11.84\% & 12.50\% & 12.63\% & 12.82\% & 12.94\% & 13.50\% & 13.65\% \\
**FLHFTL-OFU** & 7.45\% & 7.62\% & 8.04\% & 8.91\% & 9.51\% & 10.11\% & 10.34\% & 10.43\% & 12.02\% \\
**FTW-OFU** & 7.33\% & 7.48\% & 7.92\% & 8.40\% & 8.91\% & 9.86\% & 10.03\% & 10.41\% & 11.9\% \\  

Table 3: The effectiveness of OLS-OFU versus the percentage of stored training data in step (3).