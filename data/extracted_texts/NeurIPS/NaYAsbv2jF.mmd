# Geometric Neural Diffusion Processes

Emile Mathieu

University Of Cambridge

&Vincent Dutordoir\({}^{*}\)

University Of Cambridge

Secondmind Labs

&Michael J. Hutchinson

University Of Oxford

&Valentin De Bortoli

Center for Science of Data, ENS Ulm

&Yee Whye Teh

University Of Oxford

&Richard E. Turner

University Of Cambridge,

Microsoft Research

###### Abstract

Denoising diffusion models have proven to be a flexible and effective paradigm for generative modelling. Their recent extension to infinite dimensional Euclidean spaces has allowed for the modelling of stochastic processes. However, many problems in the natural sciences incorporate symmetries and involve data living in non-Euclidean spaces. In this work, we extend the framework of diffusion models to incorporate a series of geometric priors in infinite-dimension modelling. We do so by a) constructing a noising process which admits, as limiting distribution, a geometric Gaussian process that transforms under the symmetry group of interest, and b) approximating the score with a neural network that is equivariant w.r.t, this group. We show that with these conditions, the generative functional model admits the same symmetry. We demonstrate scalability and capacity of the model, using a novel Langevin-based conditional sampler, to fit complex scalar and vector fields, with Euclidean and spherical codomain, on synthetic and real-world weather data.

## 1 Introduction

Traditional denoising diffusion models are defined on finite-dimension Euclidean spaces (Song and Ermon, 2019; Song et al., 2021; Ho et al., 2020; Dhariwal and Nichol, 2021). Extensions have recently been developed for more exotic distributions, such as those supported on Riemannian manifolds (De Bortoli et al., 2022; Huang et al., 2022; Lou and Ermon, 2023; Fishman et al., 2023), and on function spaces of the form \(f:^{n}^{d}\)(Dutordoir et al., 2022; Kerrigan et al., 2022; Lim et al., 2023; Pidstrigach et al., 2023; Franzese et al., 2023; Bond-Taylor and Willcocks, 2023) (i.e. stochastic processes). In this work, we extend diffusion models to further deal with distributions over functions that incorporate non-Euclidean geometry in two different ways. This investigation of geometry also naturally leads to the consideration of symmetries in these distributions, and as such we also present methods for incorporating these into diffusion models.

Firstly, we look at _tensor fields_. Tensor fields are geometric objects that assign to all points on some manifold a value that lives in some vector space \(V\). Roughly speaking, these are functions of the form \(f: V\). These objects are central to the study of physics as they form a generic mathematical framework for modelling natural phenomena. Common examples include the pressure of a fluid in motion as \(f:^{3}\), representing wind over the Earth's surface as \(f:^{2}^{2}\), where \(^{2}\) is the _tangent-space_ of the sphere, or modelling the stress in a deformed object as \(f:^{3}^{3}\), where \(\) is the _tensor-product_ of the tangent spaces. Given the inherent symmetry in the laws of nature, these tensor fields can transform in a way that preserves these symmetries. Any modelling of these laws may benefit from respecting these symmetries.

Secondly, we look at fields with manifold codomain, and in particular, at functions of the form \(f:\). The challenge in dealing with manifold-valued output, arises from the lack of vector-space structure. In applications, these functions typically appear when modelling processes indexed by time that take values on a manifold. Examples include tracking the eye of cyclones moving on the surface of the Earth, or modelling the joint angles of a robot as it performs tasks.

The lack of data or noisy measurements in the physical process of interest motivates a _probabilistic_ treatment of such phenomena, in addition to its functional nature. Arguably the most important framework for modelling stochastic processes are Gaussian Processes (GPs) (Rasmussen, 2003), as they allow for exact or approximate posterior prediction (Titsias, 2009; Rahimi and Recht, 2007; Wilson et al., 2020). In particular, when choosing equivariant mean and kernel functions, GPs are invariant (i.e. stationary) (Holderrieth et al., 2021; Azanuglov et al., 2022; Azanuglov et al., 2023). Their limited modelling capacity and the difficulty in designing complex, problem-specific kernels motivates the development of neural processes (NPs) (Garnelo et al., 2018), which learn to approximately model a conditional stochastic process directly from data. NPs have been extended to model translation invariant (scalar) processes (Gordon et al., 2020) and more generic \((n)\)-invariant processes (Holderrieth et al., 2021). Yet, the Gaussian conditional assumption of standard NPs still limits their flexibility and prevents such models from fitting complex processes. Diffusion models provide a compelling alternative for significantly greater modelling flexibility. In this work, we develop geometric diffusion neural processes which incorporate geometrical prior knowledge into functional diffusion models.

Our contributions are three-fold: (a) We extend diffusion models to more generic function spaces (i.e. tensor fields, and functions \(f:\)) by defining a suitable noising process. (b) We incorporate group invariance of the distribution of the generative model by enforcing the covariance kernel and the score network to be group equivariant. (c) We propose a novel Langevin dynamics scheme for efficient conditional sampling.

## 2 Background

**Denoising diffusion models.** We briefly recall here the key concepts behind diffusion models on \(^{d}\) and refer the readers to (Song et al., 2021) for a more detailed introduction. We consider a forward _noising_ process \((_{t})_{t 0}\) defined by the following Stochastic Differential Equation (SDE)

\[_{t}=-_{t}t+ _{t},_{0} p_{0},\] (1)

where \((_{t})_{t 0}\) is a \(d\)-dimensional Brownian motion and \(p_{0}\) is the data distribution. The process \((_{t})_{t 0}\) is simply an Ornstein-Ulhenbeck (OU) process which converges with geometric rate to \((0,)\)(Durmus and Moulines, 2017). Under mild conditions on \(p_{0}\), the time-reversed process \((}_{t})_{t 0}=(_{T-t})_{t[0,T]}\) also satisfies an SDE (Cattiaux et al., 2021) given by

\[}_{t}=\{}_{t}+ p _{T-t}(}_{t})\}t+_{t}, {}_{0} p_{T},\] (2)

where \(p_{t}\) denotes the density of \(_{t}\). Unfortunately we cannot sample exactly from (2) as \(p_{T}\) and the scores \(( p_{t})_{t[0,T]}\) are unavailable. First, \(p_{T}\) is substituted with the limiting distribution \((0,)\) as it converges towards it. Second, one can easily show (Kallenberg, 2021, Thm 5.1) that the score \( p_{t}\) is the minimiser of \(_{t}()=[\|(_{t})-_{y_{t}}  p_{t|0}(_{t}|_{0})\|^{2}]\) over functions \(:[0,T]^{d}^{d}\) where the expectation is over the joint distribution of \(_{0},_{t}\), and as such can readily be approximated by a neural network \(_{}(t,y_{t})\) by minimising this functional. Finally, a discretisation of (2) is performed (e.g. Euler-Maruyama) to obtain approximate samples of \(p_{0}\).

**Steerable fields.** We now define steerable feature fields which are collections of tensor fields. We focus on the Euclidean group \(G=(d)\), which elements \(g\) admits a unique decomposition \(g=uh\) where \(h(d)\) is a \(d d\) orthogonal matrix and \(u(d)\) is a translation which can be identified as an element of \(^{d}\); for a vector \(x^{d}\), \(g x=hx+u\) denotes the action of \(g\) on \(x\), with \(h\) acting from the left on \(x\) by matrix multiplication. This special case simplifies the presentation, but can be extended to the general case is discussed in App. C.1.

Figure 1: Illustration of a vector field \(f:^{2}^{2}\) with representation \((h)=h\) being steered by a group element \(h=90^{}(2)(2)\).

We are interested in learning a probabilistic model over functions of the form \(f:^{d}\) such that a group \(G\) acts on \(\) and \(^{d}\). We call a feature field a tuple \((f,)\) with \(f:^{d}\) a mapping between input \(x\) to some feature \(f(x)\) with associated representation \(:G(^{d})\)(Scott and Serre, 1996). This feature field is said to be \(G\)-steerable if it is transformed for all \(x,g G\) as \(g f(x)=(g)f(g^{-1} x)\). In this setting, the action of \((d)=(d)(d)\) on the feature field \(f\) yields \(g f(x)=(uh) f(x)(h)f(h^{-1}(x-u))\). Typical examples of feature fields include scalar fields with \(_{}(g) 1\) transforming as \(g f(x)=f(g^{-1}x)\) such as temperature fields, and vectors or potential fields with \(_{}(g) h\) transforming as \(g f(x)=hf(g^{-1}x)\) as illustrated in Fig. 1, such as wind or force fields.

For many natural phenomena, a priori we do not want to express a preference for a particular conformation of the feature field and thus want a prior \(p\) to place the same density on all the transformed fields \((g f)=(f),\  g G\). Leveraging this symmetry can drastically reduce the amount of data required to learn from and reduce training time.

## 3 Geometric neural diffusion processes: GeomNDPs

### Continuous diffusion on function spaces

We construct a diffusion model on functions \(f:\), with \(=^{d}\), by defining a diffusion model for every finite set of marginals. Most prior works on infinite-dimensional diffusions consider a noising process on the space of functions (Kerrigan et al., 2022; Pidstrigach et al., 2023; Lim et al., 2023b). In theory, this allows the model to define a consistent distribution over all the finite marginals of the process being modelled. In practice, however, only finite marginals can be modelled on a computer and the score function needs to be approximated, and at this step lose consistency over the marginals. The only work to stay fully consistent in implementation is Phillips et al. (2022), at the cost of limiting functions that can be modelled to a finite-dimensional subspace. With this in mind, we eschew the technically laborious process of defining diffusions over the infinite-dimension space and work solely on the finite marginals following Dutordoir et al. (2022). We find that in practice consistency can be well learned from data see Sec. 5, and this allows for more flexible choices of score network architecture and easier training.

**Noising process.** We assume we are given a data process \((_{0}(x))_{x}\). Given any \(x=(x^{1},,x^{n})^{n}\), we consider the following forward _noising_ process \((_{t}(x))_{t 0}(_{t}(x^{1}),, _{t}(x^{n}))_{t 0}=(_{t}^{1},,_{t}^{n})_{t  0}^{n}\) defined by the following SDE

\[_{t}(x)=\{m(x)-_{t}(x)\} _{t}t+_{t}^{1/2}(x,x)^{1/2}_ {t},\] (3)

where \((x,x)_{i,j}=k(x^{i},x^{j})\) with \(k:\) a kernel and \(m:\). The process \((_{t}(x))_{t 0}\) is a multivariate Ornstein-Uhlenbeck process--with drift \(b(t,x,_{t}(x))=m(x)-_{t}(x)\) and diffusion coefficient \((t,x,_{t}(x))=(x,x)\)--which converges with geometric rate to \((m(x),(x,x))\). Using Phillips et al. (2022), it can be shown that this convergence extends to the _process_\((_{t})_{t 0}\) which converges to the Gaussian Process with mean \(m\) and kernel \(k\), denoted \(_{}\).

In the specific instance where \(k(x,x^{})=_{x}(x^{})\), then the limiting process \(_{}\) is simply Gaussian _white noise_, whilst other choices such as the squared-exponential or Matern kernel would lead to the associated Gaussian limiting process \(_{}\). Note that the _white noise_ setting is not covered by the existing theory of functional diffusion models, as a Hilbert space and a square integral kernel are required, see Kerrigan et al. (2022) for instance.

**Denoising process.** Under mild conditions over \(}^{2}\), the time-reversal process \((}_{t}(x))_{t 0}\) satisfies

\[}_{t}(x)=\{-(m(x)-}_{t}(x ))+(x,x) p_{T-t}(}_{t}(x))\}_{T-t} t+_{T-t}^{1/2}(x,x)^{1/2}_{t},\] (4)

with \(}_{0}(m,k)\) and \(p_{t}\) the density of \(_{t}(x)\) w.r.t. Lebesgue. In practice, the \( p_{T-t}\) term-known as the Stein score, is not tractable and must be approximated by a neural network. We then consider the generative stochastic process model defined by first sampling \(}_{0}(m,k)\) and then simulating the reverse diffusion (4) (e.g. via Euler-Maruyama discretisation).

**Manifold valued outputs.** So far we have defined our generative model with \(=^{d}\), we can readily extend the methodology to manifold-valued functional models using _Riemannian_ diffusion models such as De Bortoli et al. (2022) and Huang et al. (2022), see App. C. One of the main notable difference is that in the case where \(\) is a _compact_ manifold, we replace the Ornstein-Uhlenbeck process by a Brownian motion which targets the uniform distribution.

**Training.** As the reverse SDE (4) involves the preconditioned score \((x,x) p_{t}\), we directly approximate it with a neural network \(()_{}:[0,T]^{n}^{n} ^{n}\), where \(\) is the tangent bundle of \(\),see App. C. The conditional score of the noising process (3) is given by

\[_{_{t}} p_{t}(_{t}(x)|_{0}(x))=- _{t|0}^{-1}(_{t}(x)-m_{t|0})=-_{t|0}^{-1}(x,x)^{-1/2},\] (5)

since \(_{t}=m_{t|0}+_{t|0}^{1/2}\) with \((0,)\), and \(_{t|0}=_{t|0}^{2}\) with \(_{t|0}=(1-^{-_{0}^{t}(s)s})^{1/2}\), see App. B.1. We learn the preconditioned score \(()_{}\) by minimising the following denoising score matching (DSM) loss (Vincent et al., 2010) weighted by \((t)=_{t|0}^{2}\ ^{}\)

\[(;(t))=[\|s_{}(t,_{t})-  p_{t}(_{t}|_{0})\|_{(t)}^{2}]=[\|_{t|0}()_{}(t,_{t})+^{1/2} \|_{2}^{2}],\] (6)

where \(\|x\|_{}^{2}=x^{} x\). Note that when targeting a unit-variance white noise, then \(=\) and the loss (6) reverts to the DSM loss with weighting \((t)=1/_{t|0}^{2}\)(Song et al., 2021). In App. B.2, we explore several preconditioning terms and associated weighting \((t)\). Overall, we found the preconditioned score \( p_{t}\) parameterisation, in combination with the \(_{2}\) loss, to perform best, as shown by the ablation study in App. F.1.3.

### Invariant neural diffusion processes

In this section, we show how we can incorporate geometrical constraints into the functional diffusion model introduced in the previous Sec. 3.1. In particular, given a group \(G\), we aim to build a generative model over steerable feature fields as defined in Sec. 2.

**Invariant process.** A stochastic process \(f\) is said to be \(G-\)invariant if \((g)=()\) for any \(g G\), with \(((,))\), where \(\) is the space of probability measure on the space of continuous functions and \((,)\) measurable. From a sample perspective, this means that with input-output pairs \(=\{(x^{i},y^{i})\}_{i=1}^{n}\), and denoting the action of \(G\) on this set as \(g\{(g x^{i},(g)y^{i})\}_{i=1}^{n}\), \(f\) is \(G-\)invariant if and only if \(g\) has the same distribution as \(\). In what follows, we aim to derive sufficient conditions on the model introduced in Sec. 3 so that it satisfies this \(G\)-invariance property. First, we recall such a necessary and sufficient condition for Gaussian processes.

**Proposition 3.1**.: _Invariant (stationary) Gaussian process (Holderrieth et al., 2021). We have that a Gaussian process \((m,k)\) is \(G\)-invariant if and only if its mean \(m\) and covariance \(k\) are suitably \(G\)-equivariant--that is, for all \(x,x^{},g G\)_

\[m(g x)=(g)m(x)\;\;\;\;k(g x,g x^{})=(g )k(x,x^{})(g)^{}.\] (7)

Trivial examples of \((n)\)-equivariant kernels include diagonal kernels \(k=k_{0}\,\) with \(k_{0}\) invariant (Holderrieth et al., 2021), but see App. F.2 for non trivial instances introduced by Macedo and Castro (2010). Building on Prop. 3.1, we then state that our introduced neural diffusion process is also invariant if we additionally assume the score network to be \(G\)-equivariant.

**Proposition 3.2**.: _Invariant neural diffusion process (Yim et al., 2023, Prop 3.6). We denote by \((}_{t}(x))_{x,t[0,T]}\) the process induced by the time-reversal SDE (4) where the score is approximated by a score network \(_{}:[0,T]^{n}^{n} ^{n}\), and the limiting process is given by \((}_{0})=(m,k)\). Assuming \(m\) and \(k\) are respectively \(G\)-equivariant per Prop. 3.1, if we additionally have that the score network is \(G\)-equivariant vector field, i.e. \(_{}(t,g x,(g)y)=(g)_{}(t,x,y)\) for all \(x,g G\), then for any \(t[0,T]\) the process \((}_{t}(x))_{x}\) is \(G\)-invariant._

This result can be proved in two ways, from the probability flow ODE perspective or directly in terms of SDE via Fokker-Planck, see App. D.2. In particular, when modelling an invariant scalar data process \((_{0}(x))_{x}\) such as a temperature field, we need the score network to admit the invariance constraint \(_{}(t,g x,y)=_{}(t,x,y)\).

**Equivariant conditional process.** Often precedence is given to modelling the predictive processgiven a set of observations \(=\{(x^{c},y^{c})\}_{c C}\). In this context, the conditional process (Pollard, 2002, p.117) inherits the symmetry of the prior process in the following sense. A stochastic process with distribution \(\) given a context \(\) is said to be conditionally \(G-\)equivariant if the conditional satisfies \((|g)=(g|)\) for any \(g G\) and \((,)\) measurable, as illustrated in Fig. 2.

**Proposition 3.3**.: _Equivariant conditional process. Assume a stochastic process \(f\) is \(G-\)invariant. Then the conditional process \(f|\) given a set of observations \(\) is \(G\)-equivariant._

Originally stated in Holderrieth et al. (2021) in the case where the process is over functions of the form \(f:^{n}^{d}\) and marginals with density w.r.t. Lebesgue, we prove Prop. 3.3 for stochastic processes over generic fields on manifolds in terms only of the measure of the process (App. D.3).

### Conditional sampling

There exist several methods to perform conditional sampling in diffusion models such as: replacement sampling, amortisation and conditional guidance, which we discuss in App. E.1. Here we propose a new method for sampling from exact conditional distributions of NDPs using only the score network for the joint distribution. Using the fact that the conditional score can be written as \(_{x} p(x|y)=_{x} p(x,y)-_{x} p(y)=_{x} p (x,y)\) we can therefore, for any point in the diffusion time, conditionally sample using Langevin dynamics, following the SDE \(_{s}= p_{T-t}(_{s} )s+}_{s}\), by only applying the diffusion to the variables of interest and holding the others fixed. While we could sample directly at the end time this proves difficult in practice. Similar to the motivation of Song and Ermon (2019), we sample along the reverse diffusion, taking a number of conditional Langevin steps at each time. In addition, we apply the forward noising SDE to the conditioning points at each step, as this puts the combined context and sampling set in a region that the score function will be well learned in training. Our procedure is illustrated in Alg. 1. In App. E.1 we draw links with RePaint of Lugmayr et al. (2022).

### Likelihood evaluation

Similarly to Song et al. (2021), we can derive a deterministic ODE which has the same marginal density as the SDE (3), which is given by the 'probability flow' Ordinary Differential Equation (ODE), see App. B. Once the score network is learnt, we can thus use it in conjunction with an ODE solver to compute the likelihood of the model. A perhaps more interesting task is to evaluate the predictive posterior likelihood \(p(y^{*}|x^{*},\{x^{i},y^{i}\}_{i C})\) given a context set \(\{x^{i},y^{i}\}_{i C}\). A simple approach is to simply rely on the conditional probability rule evaluate \(p(y^{*}|x^{*},\{x^{i},y^{i}\}_{i C})=p(y^{*},\{y^{i}\}_{i C}|x^{*},\{x^{ i}\}_{i C})/p(\{y^{i}\}_{i C}|\{x^{i}\}_{i C})\). This can be done by solving two probability flow ODEs: one over the joint evaluation and context set, and another only over the context set.

Figure 3: Illustration of Langevin corrected conditional sampling. The black line represents the noising process dynamics \((p_{t})_{t[0,T]}\). The time reversal (i.e. predictor) step, is combined with a Langevin corrector step projecting back onto the dynamics.

Figure 2: Samples from equivariant neural diffusion processes conditioned on context set \(\) (in red) and evaluated on a regular grid \(x^{*}\) for scalar (_Left_) and 2D vector (_Right_) fields. Same model is then conditioned on transformed context \(g\), with group element \(g\) being a translation of length \(2\) (_Left_) or a \(90^{}\) rotation (_Right_).

Related work

**Gaussian processes and the neural processes family.** One important and powerful framework to construct distributions over functional spaces are Gaussian processes (Rasmussen, 2003). Yet, they are restricted in their modelling capacity and when using exact inference they scale poorly with the number of datapoints. These problems can be partially alleviated by using neural processes (Kim et al., 2019; Garnelo et al., 2018; Garnelo et al., 2018; Jha et al., 2022; Louizos et al., 2019; Singh et al., 2019), although they also assume a Gaussian likelihood. Recently introduced autoregressive NPs (Bruinsma et al., 2023) alleviate this limitation, but they are disadvantaged by the fact that variables early in the auto-regressive generation only have simple distributions (typically Gaussian). Finally, (Dupont et al., 2022) model weights of implicit neural representation using diffusion models.

Stationary stochastic processes.The most popular Gaussian process kernels (e.g. squared exponential, Matern) are stationary, that is, they are translation invariant. These lead to invariant Gaussian processes, whose samples when translated have the same distribution as the original ones. This idea can be extended to the entire isometry group of Euclidean spaces (Holderrieth et al., 2021), allowing for modelling higher order tensor fields, such as wind fields or incompressible fluid velocity (Macedo and Castro, 2010). Later, Azangulov et al. (2022) and Azangulov et al. (2023) extended stationary kernels and Gaussian processes to a large class of non-Euclidean spaces, in particular all compact spaces, and symmetric non compact spaces. In the context of neural processes, (Gordon et al., 2020) introduced ConvCNP so as to encode translation equivariance into the predictive process. They do so by embedding the context into a translation equivariant functional representation which is then decoded with a convolutional neural network. Holderrieth et al. (2021) later extended this idea to construct neural processes that are additionally equivariant w.r.t. rotations or subgroup thereof.

Spatial structure in diffusion models.A variety of approaches have also been proposed to incorporate spatial correlation in the noising process of finite-dimensional diffusion models leveraging the multiscale structure of data (Jing et al., 2022; Guth et al., 2022; Ho et al., 2022; Saharia et al., 2021; Hoogeboom and Salimans, 2022; Rissanen et al., 2022). Our methodology can also be seen as a principled way to modify the forward dynamics in classical denoising diffusion models. Hence, our contribution can be understood in the light of recent advances in generative modelling on soft and cold denoising diffusion models (Daras et al., 2022; Bansal et al., 2022; Hoogeboom and Salimans, 2022). Several recent work explicitly introduced a covariance matrix in the Gaussian noise, either on a choice of kernel (Bilos et al., 2022), based on Discrete Fourier Transform of images (Voleti et al., 2022), or via empirical second order statistics (squared pairwise distances and the squared radius of gyration) for protein modelling (Ingraham et al., 2022). Alternatively, (Guth et al., 2022) introduced correlation on images leveraging a wavelet basis.

Functional diffusion models.Infinite dimensional diffusion models have been investigated in the Euclidean setting in (Kerrigan et al., 2022; Pidstrigach et al., 2023; Lim et al., 2023; Bond-Taylor and Willcocks, 2023; Hagemann et al., 2023; Franzese et al., 2023; Dutordoir et al., 2022; Phillips et al., 2022). Most of these works are based on an extension of the diffusion models techniques (Song et al., 2021; Ho et al., 2020) to the infinite-dimensional space, leveraging tools from the Cameron-Martin theory such as the Feldman-Hajek theorem (Kerrigan et al., 2022; Pidstrigach et al., 2023) to define infinite-dimensional Gaussian measures and how they interact. We refer to (Da Prato and Zabczyk, 2014) for a thorough introduction to Stochastic Differential Equations in infinite dimension. (Phillips et al., 2022) consider another approach by defining countable diffusion processes in a basis. All these approaches amount to learn a diffusion model with spatial structure. Note that this induced correlation is necessary for the theory of infinite dimensional SDE (Da Prato and Zabczyk, 2014) to be applied but is not necessary to implement diffusion models (Dutordoir et al., 2022). Several approaches have been considered for conditional sampling. (Pidstrigach et al., 2023; Bond-Taylor and Willcocks, 2023) modify the reverse diffusion to introduce a guidance term, while (Dutordoir et al., 2022; Kerrigan et al., 2022) use the replacement method. Finally (Phillips et al., 2022) amortise the score function w.r.t. the conditioning context.

## 5 Experimental results

### 1D regression over stationary scalar fields

We evaluate GeomNDPs on several synthetic 1D regression datasets. We follow the same experimental setup as Bruinsma et al. (2020) which we detail in App. F.1. In short, it contains Gaussian (Squared Exponential (SE), Matern\(()\), Weakly Periodic) and non-Gaussian (Sawtooth and Mixture) sample paths, where Mixture is a combination of the other four datasets with equal weight. Fig. 9 shows samples for each of these dataset. The Gaussian datasets are corrupted with observation noise with variance \(^{2}=0.05^{2}\). Table 1 reports the average log-likelihood \(p(y^{*}|x^{*},)\) across \(4096\) test samples, where the context set size is uniformly sampled between \(1\) and \(10\) and the target has fixed size of \(50\). All inputs \(x^{c},x^{*}\) are chosen uniformly within their input domain which is \([2,2]\) for the training data and 'interpolation' evaluation and \(\) for the 'generalisation' evaluation.

We compare the performance of GeomNDP to a GP with the true hyperparameters (when available), a (convolutional) Gaussian NP (Bruinsma et al., 2020), a convolutional NP (Gordon et al., 2020) and a vanilla attention-based NDP (Dutordoir et al., 2022) which we reformulated in the continuous diffusion process framework to allow for log-likelihood evaluations and thus a fair comparison--denoted \(^{*}\). We enforce translation invariance in the score network for GeomNDP by subtracting the centre of mass from the input \(x\), inducing stationary scalar fields.

On the GP datasets, GNP, ConvNPs and GeomNDP methods are able to fit the conditionals perfectly--matching the log-likelihood of the GP model. GNP's performance degrades on the non-Gaussian datasets as it is restricted by its conditional Gaussian assumption, whilst NDPs methods still performs well as illustrated on Fig. 4. In the bottom rows of Table 1, we assess the models ability to generalise outside of the training input range \(x[2,2]\), and evaluate them on a translated grid where context and target points are sampled from \(\). Only convolutional NPs (GNP and ConvNP) and \((1)-\)GeomNDP are able to model stationary processes and therefore to perform as well as in the interpolation task. The \(^{*}\), on the contrary, drastically fails at this task.

    & & SE & Matern\(()\) & Weakly Per. & Sawtooth & Mixture \\   & GP (optimum) & \(0.70_{0.00}\) & \(0.31_{0.00}\) & \(-0.32_{0.00}\) & - & - \\  & \((1)-\)**GeomNDP** & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\  & \(^{*}\) & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\  & GNP & \(}\) & \(}\) & \(-0.47_{0.01}\) & \(0.42_{0.01}\) & \(0.10_{0.02}\) \\  & ConvNP & \(-0.46_{0.01}\) & \(-0.67_{0.01}\) & \(-1.02_{0.01}\) & \(1.20_{0.01}\) & \(-0.50_{0.02}\) \\   & GP (optimum) & \(0.70_{0.00}\) & \(0.31_{0.00}\) & \(-0.32_{0.00}\) & - & - \\  & \((1)-\)**GeomNDP** & \(}\) & \(}\) & \(}\) & \(}\) & \(}\) \\   & NDP\({}^{*}\) & \(*\) & \(*\) & \(*\) & \(*\) & \(*\) \\   & GNP & \(}\) & \(}\) & \(-0.47_{0.01}\) & \(0.42_{0.01}\) & \(0.10_{0.02}\) \\   & ConvNP & \(-0.46_{0.01}\) & \(-0.67_{0.01}\) & \(-1.02_{0.01}\) & \(1.19_{0.01}\) & \(-0.53_{0.02}\) \\   

Table 1: Mean test log-likelihood (TLL) \(\) 1 standard error estimated over 4096 test samples are reported. Statistically significant best non-GP model is in **bold**. ‘*’ stands for a TLL below \(-10\). NP baselines from Bruinsma et al. (2020). \((1)-\)GeomNDP indicates our proposed method with a translation invariant score.

Figure 4: Prior and posterior samples (in blue) from the data process and the GeomNDP model, with context points in red and posterior mean in black.

**Non white kernels for limiting process.** The NDP methods in the above experiment target the white kernel \((x=x^{})\) in the limiting process. In App. F.1.3, we explore different choices for the limiting kernel, such as SE and periodic kernels with short and long lengthscales, along with several score parameterisations, see App. B.3 for a description of these. We observe that although choosing such kernels gives a head start to the training, it eventually yield slightly worse performance. We attribute this to the additional complexity of learning a non-diagonal covariance. Finally, across all datasets and limiting kernels, we found the preconditioned score \( p_{t}\) to result in the best performance.

**Conditional sampling ablation.** We employ the SE dataset to investigate various configurations of the conditional sampler as we have access to the ground truth conditional distribution through the GP posterior. In Fig. 11 we compute the Kullback-Leibler divergence between the samples generated by GeomNDP and the actual conditional distribution across different conditional sampling settings. Our results demonstrate the importance of performing multiple Langevin dynamics steps during the conditional sampling process. Additionally, we observe that the choice of noising scheme for the context values \(y_{c}\) has relatively less impact on the overall outcome.

### Regression over Gaussian process vector fields

We now focus our attention to modelling equivariant vector fields. For this, we create datasets using samples from a two-dimensional zero-mean GP with one of the following \((2)\)-equivariant kernels: a diagonal Squared-Exponential (SE) kernel, a zero curl (Curl-free) kernel and a zero divergence (Div-free) kernel, as described in App. D.1.

We equip our model, GeomNDP, with a \((2)\)-equivariant score architecture, based on steerable CNNs (Thomas et al., 2018; Weiler and Cesa, 2021). We compare to NDP\({}^{*}\) with a non-equivariant attention-based network (Dutordoir et al., 2022). We also evaluate two neural processes, a translation-equivariant ConvCNP (Gordon et al., 2020) and a \(4^{2}(2)\)-equivariant SteerCNP (Holderrieth et al., 2021). We also report the performance of the data-generating GP, and the same GP but with diagonal posterior covariance GP (Diag.). We measure the predictive log-likelihood of the data process samples under the model on a held-out test dataset.

We observe in Fig. 5 (Left), that the CNPs performance is limited by their diagonal predictive covariance assumption, and as such cannot do better than the GP (Diag.). We also see that although NDP\({}^{*}\) is able to fit well GP posteriors, it does not reach the maximum log-likelihood value attained by the data GP, in contrast to its equivariant counterpart GeomNDP. To further explore gains brought by the built-in equivariance, we explore the data-efficiency in Fig. 5 (Right), and notice that E(2)-GeomNDP requires few data samples to fit the data process, since effectively the dimension of the (quotiented) state space is dramatically reduced.

### Global tropical cyclone trajectory prediction

Finally, we assess our model on a task where the domain of the stochastic process is a non-Euclidean manifold. We model the trajectories of cyclones over the earth, modelled as sample paths of the form \(^{2}\) coming from a stochastic process. The data is drawn from the International Best Track Archive for Climate Stewardship (IBTrACS) Project, Version 4 ((Knapp et al., 2010; Knapp et al., 2018)) and preprocessed as per App. F.3, where details on the implementation of the score function, the ODE/SDE solvers used for the sampling, and baseline methods can be found.

Figure 5: Quantitative results for experiments on GP vector fields. Mean predictive log-likelihood (\(\)) and confidence interval estimated over 5 random seeds. _Left_: Comparison with neural processes. Statistically significant results are in **bold**. _Right_: Ablation study when varying the number of training data samples.

Fig. 6 shows some cyclone trajectories samples from the data process and from a trained GeomNDP model. We also demonstrate how such trajectories can be interpolated or extrapolated using the conditional sampling method detailed in Sec. 3.3. Such conditional sample paths are shown in Fig. 7. Additionally, we report in Table 2 the likelihood and MSE for a series of methods. The interpolation task involves conditioning on the first and last 20% of the cyclone trajectory and predicting intermediary positions. The extrapolation task involves conditioning on the first 40% of trajectories and predicting future positions. We see that the GPs (modelled as \(f:^{2}\), one on latitude/longitude coordinates, the other via a stereographic projection, using a diagonal RBF kernel with hyperparameters fitted with maximum likelihood) fail drastically given the high non-Gaussianity of the data. In the interpolation task, the NDP performs as well as the GeomNDP, but the additional geometric structure of modelling the outputs living on the sphere appears to significantly help for extrapolation. See App. F.3 for more fine-grained results.

## 6 Discussion

In this work, we have extended diffusion models to model invariant stochastic processes over tensor fields. We did so by (a) constructing a continuous noising process over function spaces which correlate input samples with an equivariant kernel, (b) parameterising the score with an equivariant neural network. We have empirically demonstrated the ability of our introduced model GeomNDP to fit complex stochastic processes, and by encoding the symmetry of the problem at hand, we show that it is more data efficient and better able to generalise.

We highlight below some current limitations and important research directions. First, evaluating the model is slow as it relies on costly SDE or ODE solvers, as existing diffusion models. Second, targeting a white noise process appears to over-perform other Gaussian processes. In future work, we would like to investigate the practical influence of different kernels. Third, strict invariance may sometimes be too strong, we thus suggest softening it by amortising the score network over extra spatial information available from the problem at hand.

    &  &  &  \\  & Likelihood & Likelihood & MSE (km) & Likelihood & MSE (km) \\  GeomNDP (\(^{2}\)) & \(_{ 5}\) & \(_{ 4}\) & \(_{ 6}\) & \(_{ 4}\) & \(_{ 14}\) \\ Stereographic GP (\(^{2}/\{0\}\)) & \(393_{ 3}\) & \(266_{ 3}\) & \(2619_{ 13}\) & \(245_{ 2}\) & \(6587_{ 55}\) \\ NDP (\(^{2}\)) & - & - & \(166_{ 22}\) & - & \(769_{ 48}\) \\ GP (\(^{2}\)) & - & - & \(685_{ 41}\) & - & \(8138_{ 87}\) \\   

Table 2: Comparative results of different models on the cyclone dataset, comparing test set likelihood, interpolation likelihood and mean squared error (MSE), and extrapolation likelihood and mean squared error. These are estimated over 5 random seeds. We only report likelihoods of models defined w.r.t the uniform measure on \(^{2}\).

Figure 6: _Left:_ 1000 samples from the training data. _Right:_ 1000 samples from the trained model.

Figure 7: _Top:_ Examples of conditional trajectories sampled from the GeomNDP model. _Blue:_ Conditioned sections of the trajectory. _Green:_ The actual trajectory of the cyclone. _Red:_ conditional samples from the model. _Purple:_ closest matching trajectories in the dataset to the conditioning data.