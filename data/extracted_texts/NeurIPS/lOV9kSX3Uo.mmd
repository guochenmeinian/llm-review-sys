# Optimizing over Multiple Distributions under Generalized Quasar-Convexity Condition

Shihong Ding\({}^{1}\)

dingshihong@stu.pku.edu.cn

&Long Yang\({}^{1}\)

YANGLONG001@pku.edu.cn

&Luo Luo\({}^{2,4}\)

luoluo@fudan.edu.cn

&Cong Fang\({}^{1,3}\)

fangcong@pku.edu.cn

Corresponding author.

\({}^{1}\) State Key Lab of General AI, School of Intelligence Science and Technology, Peking University

\({}^{2}\) School of Data Science, Fudan University

\({}^{3}\) Institute for Artificial Intelligence, Peking University

\({}^{4}\) Shanghai Key Laboratory for Contemporary Applied Mathematics

###### Abstract

We study a typical optimization model where the optimization variable is composed of multiple probability distributions. Though the model appears frequently in practice, such as for policy problems, it lacks specific analysis in the general setting. For this optimization problem, we propose a new structural condition/landscape description named generalized quasar-convexity (GQC) beyond the realms of convexity. In contrast to original quasar-convexity , GQC allows an individual quasar-convex parameter \(_{i}\) for each variable block \(i\) and the smaller of \(_{i}\) implies less block-convexity. To minimize the objective function, we consider a generalized oracle termed as the internal function that includes the standard gradient oracle as a special case. We provide optimistic mirror descent (OMD) for multiple distributions and prove that the algorithm can achieve an adaptive \(}((_{i=1}^{d}1/_{i})^{-1})\) iteration complexity to find an \(\)-suboptimal global solution without pre-known the exact values of \(_{i}\) when the objective admits "polynomial-like" structural. Notably, it achieves iteration complexity that does not explicitly depend on the number of distributions and strictly faster (\(_{i=1}^{d}1/_{i}\) v.s. \(d_{i[1:d]}1/_{i}\)) than mirror decent methods. We also extend GQC to the minimax optimization problem proposing the generalized quasar-convexity-concavity (GQCC) condition and a decentralized variant of OMD with regularization. Finally, we show the applications of our algorithmic framework on discounted Markov Decision Processes problem and Markov games, which bring new insights on the landscape analysis of reinforcement learning.

## 1 Introduction

We study a common class of generic minimization problem

\[_{}f(),\] (1)

where the optimization variable \(\) is composed of \(d\) probability distributions \(\{_{i}\}_{i=1}^{d}\) and \(\) denotes the product space of the \(d\) probability simplexes. Problem (1) meets widespread applications inreinforcement learning optimization [62; 2; 35], multi-class classification  and model selection type aggregation . In this paper, we are particularly interested in the case where \(d\) is reasonably large and we manage to obtain complexities dependent of \(d\) non-explicitly.

When \(f\) is convex with respect to \(\), many efficient algorithms can be powerful tools for solving Problem (1). One well-known algorithm is mirror descent (MD)  which is based on Bregman divergence. The wide choices of Bregman divergence enable the algorithm to iterate and converge under specifically constrained region . In particular, if one applies the usual Euclidean distance, the algorithm reduces to project gradient descent . One common and more sophisticated selection is the Kullback-Leibler (KL) divergence, the algorithm thereby becoming the variant of multiplicative weights update (MWU)  over probability distribution.

Turning to the non-convex world, specific analysis for Problem (1) is rare. In general, finding an approximate global solution suffers from the curse of dimensionality [51; 46]. And one interesting direction is to consider suitable relaxations for the desired solutions, such as an approximate local stationary point of smooth functions [31; 19]. However, for many cases, local solutions may not be sufficient. Moreover, the algorithms often converge much faster in practice than the theoretic lower bounds in non-convex optimization suggest. This observed discrepancy can be attributed to the fairly weak assumptions underpinning these generic bounds. For example, many generic non-convex optimization theories, e.g. Carmon et al. [7; 8] only focus on the consideration of Lipschitz continuity of the gradient and some higher-order derivatives. In practice, the objective is often more "structured". For example, the recent progress in neural networks shows that systems of neural networks approximate convex kernel systems when the model is overparameterized . As pointed out by Hinder et al. , much more research is needed to characterize structured sets of functions for which minimizers can be efficiently found; It was also noted by Yurii Nesterov  that lots of functions are essentially convex; Our work follows this research line.

We propose generalized quasar-convexity (GQC) for the class of "structure". The original quasar-convex functions  is parameterized by a constant \((0,1]\) and requires \(f()-f(^{*}) f(),- ^{*}\). These functions are unimodal on all lines that pass through a global minimizer and so all critical points are minimizers. We extend quasar-convexity by introducing individual quasar-convex parameter \(_{i}\) for each distribution \(_{i}\). Therefore GQC is parameterized by \(d\) constants \(\{_{i}\}_{i=1}^{d}\) and implies quasar-convexity in the case \(d=1\). The main intuition of the generalization is the observation that \(d/_{i[1:d]}_{i}\) often depends on the number of distributions \(d\) in real problems, whereas, \(_{i=1}^{d}1/_{i}\) may not. That is to say, the hardness for distribution \(i\) diverges according to the magnitude of \(_{i}\). The larger of \(_{i}\) implies more convexity and the simpler to solve \(_{i}\). In general, one always have \(_{i=1}^{d}1/_{i} d_{i[1:d]}1/_{i}\). In the worst case, \(_{i=1}^{d}1/_{i}\) can be \(d\) times smaller than \(d_{i[1:d]}1/_{i}\) (see discussions in Section 3.3), which motivates us to study the GQC condition.

We then study designing efficient algorithms to solve (1). One simple case is when \(\{_{i}\}_{i=1}^{m}\) is pre-known by the algorithms. The possible direction is to impose a \(_{i}\)-dependent update rule, such as by non-uniform sampling. However, in general cases, \(\{_{i}\}_{i=1}^{m}\) is not known and determining \(\{_{i}\}_{i=1}^{m}\) require non-negligible costs.

In this paper, we consider a generalized oracle, which we refer to as the internal function. Here the standard gradient oracle can be viewed as a special case of the internal function. We provide the optimistic mirror descent algorithm for multiple distributions, which makes sure that each probability distribution is updated according to its own internal function. We first establish an \(((d_{})^{1/2}(_{i=1}^{d}_{i}^{-1})^{3/2}L ^{-1}(N))\) complexity with \(N=_{i[1:d]}n_{i}\) and \(_{}=_{i[1:d]}_{i}\) when \(_{}<\). However, such an complexity depends on \(d_{}\) and requires the step size rely on pre-known \(_{}_{i=1}^{d}_{i}^{-1}\). We then consider \(f\) satisfies "polynomial-like" structural (see Assumption 3.3). We show the assumption can be achieved in a variety of function classes and important machine learning problems. Under the assumption, we show the algorithm can adapt to the values of \(\{_{i}\}_{i=1}^{m}\) and guarantees an reduced iteration complexity \(((_{i=1}^{d}1/_{i})^{-1}(N)^{4.5}( ^{-1}))\). In the following, the \(}()\) notation hides factors that are polynomial in \((^{-1})\) and \((N)\).

We also extend our framework to the minimax optimization

\[_{ X}_{}f(,),\] (2)where both \(\) and \(\) are composed of \(d\) probability distributions, and \(=\) is a joint region. In the general non-convex and non-concave setting, it is known that finding even an approximated local solution for (2) is computationally intractable . We introduce the generalized quasar-convexity-concavity (GQCC) condition analogous to GQC and demonstrate the feasibility of obtaining an \(\)-approximate Nash equilibrium with \(((1-)^{-2.5}_{}(_{i=1}^{d}_{ i}())^{-1}(M)(^{-1}))\) iteration complexities, where \(_{}(_{i=1}^{d}_{i}())\) is analogous to \((_{i=1}^{d}1/_{i})\) with \(_{i}()\) defined in the GQCC condition; \(\) is the discount parameter; \(M=_{i[1:d]}\{m_{i}+n_{i}\}\). Intuitively, the GQCC condition can be viewed as the generalization of convexity-concavity condition. Similarly, the \(}()\) notation hides factors that are polynomial in \((^{-1})\) and \((M)\).

Finally, we demonstrate the applications of our framework. For problem (1), we consider both infinite horizon discounted and finite horizon MDPs problem. For problem (2), we study the infinite horizon two-player zero-sum Markov games. We prove the learning objectives admit the GQC and GQCC conditions, respectively. This provides new landscape description for RL problems, thereby bringing new insights. Accordingly, our algorithms achieve state-of-the-art iteration complexities up to logarithmic factors. We provide \(}(^{-1})\) iteration bound for finding an \(\)-approximate Nash equilibrium of infinite horizon two-player zero-sum Markov games, which outperforms the \(}(||^{3}^{-2})\) bound of Wei et al.  and the \(}(||^{-1})\) bound of Cen et al.  by factors of \(||^{3}^{-1}\) and \(||\), respectively, up to a logarithmic factor.

### Contribution

1. We introduce new structural conditions GQC for minimization problems and GQCC for minimax problems over multiple distributions.
2. We provide adaptive algorithm that achieves \(}((_{i=1}^{d}1/_{i})^{-1})\) iteration complexities to find an \(\)-suboptimal global minimum of "polynomial-like" function under GQC. We also provide an implementable minimax algorithm, given a generalized quasar-convex-concave function with proper conditions, uses \(}((1-)^{-2.5}\,_{}(_{ i=1}^{d}_{i}())\;^{-1})\) iterations to find an \(\)-approximate Nash equilibrium.
3. We show that discounted MDP and infinite horizon two-player zero-sum Markov games admit the GQC and GQCC conditions, respectively, and also satisfy our mild assumptions. In addition, we provide \(}((1-)^{-2.5}^{-1})\) iteration bound for finding an \(\)-approximate Nash equilibrium of infinite horizon two-player zero-sum Markov games. Detailed comparisons between our method and prior arts are provided in Table 1.

### Related Works

**Minimization:** Convexity condition has been studied at length and plays a critical role in optimizing minimization problems [59; 44; 25; 60; 6; 49]. Several other "convexity-like" conditions have

   Solution type & Related work & Iteration complexity & Single loop \\   & **Cen et al. ** & \(}}\) & ✗ \\   & **Cen et al. ** & \(}|^{3}}{(1-)^{8} ^{2}}\) & ✓ \\   & **Wei et al. ** & \(}|^{3}}{(1-)^{8} ^{2}}\) & ✓ \\   & **Cen et al. ** & \(}|}{(1-)^{4}} \) & ✓ \\   & **This Work** & \(}|}{(1-)^{2.5}} \) & ✓ \\   

Table 1: Comparison of policy optimization methods for finding an \(\)-approximate NE of infinite horizon two-player zero-sum Markov games in terms of the max-min gap (see Eq. (4)). Since the iteration complexity of several research works (such as Zhao et al. , Alacaoglu et al.  and Zeng et al. ) involve concentrability coefficient and initial distribution mismatch coefficient, we will not delve into them here.

attracted considerable attention, which provide opportunity for designing algorithmic framework to achieve global convergence. Star-convexity  is a typical example that relaxes convexity, showing potential in machine learning recently [32; 76]. Quasi-convexity, which admits that the highest point along any line segment is one of the endpoints, is also an important condition . Following this, the concept of weak quasi-convexity is proposed by Hardt et al.  which is an extension of star-convexity in the differentiable case, and Hinder et al.  provides lower bound for the number of gradient evaluations to find an \(\)-minimizer of a quasar-convex function (a linguistically clearer redefinition of weak quasi-convex function claimed by Hinder et al.  ).

**Minimax Optimization:** Minimax problem attracted considerable attention in machine learning. There exist a variety of algorithms to find the approximate Nash equilibrium points [63; 43; 48; 45; 40; 33; 55; 66; 27] or stationary points  for convex-concave functions. Without convex-concave assumption, there exist related work considered specific structures in objective, including nonconvex-(strongly-)concave assumption [39; 73; 50], Kurdyka-Lojasiewicz condition (or specific PL condition) [68; 11; 69; 38], interaction dominant condition  and negative comonotonicity [17; 36].

**RL Landscape Descriptions:** For the policy gradient based model of infinite horizon reinforcement learning problems, Agarwal et al.  provides a convergence proof for the natural policy gradient descent, which is the same as the mirror descent-modified policy iteration algorithm  with negative entropy as the Bregman divergence. Subsequently, Lan  focuses on exploring the structural properties of infinite horizon reinforcement learning problems with convex regularizers. For two-player zero-sum Markov games [61; 42] under full information setting, there are various algorithms [26; 54; 64; 18; 42; 67; 9; 74; 70] have been proposed. Specifically, Cen et al.  focus on finding approximate minimax soft \(Q\)-function in regularized infinite horizon setting; Zhao et al.  focus on finding one-sided approximate Nash equilibrium in standard infinite horizon setting with \(}(^{-1})\) iteration bound which depends on the concentrability coefficient; Yang and Ma  focus on finding approximate Nash equilibrium in standard finite horizon setting with \(}(^{-1})\) iteration bound.

**Related Works on Optimistic Mirror Descent (OMD) and Optimistic Multiplicative Weights Update (OMWU):** The connection between online learning and game theory [58; 4; 23; 1] has since led to the discovery of broad learning algorithms such as multiplicative weights update (MWU) . Rakhlin and Sridharan  introduces an optimistic variant of online mirror descent [56; 14]-optimistic mirror descent. Daskalakis et al.  shows that the external regret of each player achieves near-optimal growth in multi-player general-sum games, with all players employ the optimistic multiplicative weights update.

## 2 Preliminary

**Notation:** Let \(=(_{1},,_{d})^{_{i=1}^{d}n_{i}}\) be the joint vector variable, for every vector variable \(_{i}^{n_{i}}\). Let \(=((1),,(n))\) be the multi-indices, where \((i)_{+}\), we define \(||=_{i=1}^{n}(i)\) and \(!=(1)!(n)!\). For any vector \(=((1),,(n))^{n}\), we define \(^{}=(1)^{(1)}(n)^{(n)}\). Let \(f:^{n}\) be a smooth function, we expand its Taylor expansion with Lagrange remainder \(R_{K,}^{f}()\) as follows,

\[R_{K,}^{f}()=f()-_{i=0}^{K}_{||=i}}f()}{!}(-)^{}.\] (3)

Given matrices \(\) and \(\) in \(^{_{1}_{2}}\) we claim that \(\) if \([]_{i,j}-[]_{i,j} 0\) for every \(i,j\). For a sequence of vector-valued functions \(\{_{i}\}_{i=1}^{d}\), we say that \(\{_{i}\}_{i=1}^{d}\) is uniformly \(L\)-Lipschitz continuous with respect to \(\|\|\) under \(\|\|\) if \(\|_{i}(_{i})-_{i}(_{i}) \|\), \( L\|_{i}-_{i}\|\) for every \(i[1:d]\) and any \(,\). We denote by \(\|\|\), the dual norm of \(\|\|\). Let \(:^{_{1}_{2}}^{n_{1} n_{2}}\) be a matrix function, we say that \(\) is a \(\)-contraction mapping under \(\|\|\) if \(\|(_{1})-(_{2})\|_{} \|_{1}-_{2}\|\) for any \(_{1},_{2}^{_{1}_{2}}\). For matrix-valued function \(:^{n}^{_{1}_{2}}\),we define \(_{}(,^{})=()-( ^{})\) for any \(,^{}^{n}\). The KL divergence \((\|)=_{j=1}^{n}()((j)}{(j)})\) between distributions \(\) and \(\) is defined on probability simplex \(_{n}\). And the variance of \(\) over \(\) is defined by \(_{}()=_{j=1}^{n}(j)((j)- _{j^{}}[(j^{})])^{2}\). We define max-min gap of function \(f:\) as follows,

\[_{f}(,):=_{^{}}f(, ^{})-_{^{}}f(^{},).\] (4)

We claim that \((,)\) is an \(\)-approximate Nash equilibrium (\(\)-approximate NE) if \(_{f}(,)\). When \(=0\), \((,)\) is a Nash equilibrium.

**Infinite Horizon Discounted Markov Decision Process:** We consider the setting of an infinite horizon discounted Markov decision process (MDP), denoted by \(:=(,,,,,_{0})\). \(\) is a finite state space; \(\) is a finite action space; \((s|s^{},a^{})\) denotes the probability of transitioning from \(s\) to \(s^{}\) under playing action \(a^{}\); \(:\) is a cost function, which quantifies the cost associated with taking action \(a\) in state \(s\); \([0,1)\) is a discount factor; \(_{0}\) is an initial state distribution over \(\).

\(:_{}\) (where \(_{}\) is the probability simplex over \(\)) denotes a stochastic policy, i.e., the agent play actions according to \(a(|s)\). We use \(_{t}^{}(s^{}|s)=^{}(s_{t}=s^{ }|s_{0}=s)\) to denote the probability of visiting the state \(s^{}\) from the state \(s\) after \(t\) time steps according to policy \(\). Let trajectory \(=\{(s_{t},a_{t})\}_{t=0}^{}\), where \(s_{0}_{0}\), and, for all subsequent time steps \(t\), \(a_{t}(|s_{t})\) and \(s_{t+1}(|s_{t},a_{t})\). The value function \(V^{}:\) is defined as the discounted sum of future cost starting at state \(s\) and executing \(\), i.e.

\[V^{}(s)=(1-)[._{t=0}^{}^{t} (s_{t},a_{t})|,s_{0}=s].\]

Moreover, we define the action-value function \(Q^{}:\) and the advantage function \(A^{}:\) as follows:

\[Q^{}(s,a)=(1-)[._{t=0}^{}^{ t}(s_{t},a_{t})|,s_{0}=s,a_{0}=a],\;\;A^{}(s,a)=Q^{ }(s,a)-V^{}(s).\]

It's also useful to define the discounted state visitation distribution \(_{s_{0}}^{}\) of a policy \(\) as \(_{s_{0}}^{}(s)=(1-)_{t=0}^{}^{t}_{t}^{}(s|s_{0})\). In order to simplify notation, we write \(_{_{0}}^{}(s)=_{s_{0}_{0}}[ _{s_{0}}^{}(s)]\), where \(_{_{0}}^{}\) is the discounted state visitation distribution under initial distribution \(_{0}\).

## 3 Minimization Optimization

In this section, we propose the generalized quasar-convexity (GQC) condition, and analyze a related algorithmic framework for minimization over \(=_{i=1}^{d}_{n_{i}}\), under mild assumptions.

### Generalized Quasar-Convexity (GQC)

We provide a novel depiction of function structure-generalized quasar-convexity, which is defined as follows:

**Definition 3.1** (Generalized Quasar-Convexity (GQC)).: Let \(^{*}^{_{i=1}^{d}n_{i}}\) be a minimizer of the function \(f:\). We say that \(f\) is generalized quasar-convex on \(\) with respect to \(^{*}\) if for all \(\), there exist a sequence of vector-valued functions \(\{_{i}:^{n_{i}}\}_{i=1}^{d}\) and a sequence of positive scalars \(\{_{i}\}_{i=1}^{d}\) such that

\[f(^{*}) f()+_{i=1}^{d}} _{i}(),_{i}^{*}-_{i}.\] (5)

If Eq. (5) holds, we say that \(=(_{1}^{},,_{d}^{})^{}\) is the internal function of \(f\). Given \(i[1:d]\) we say that \(_{i}\) is the internal function of \(f\) for variable block \(_{i}\).

Our proposed GQC condition concerns the multi-variable generalized extension of the quasar-convexity condition. In the case \(d=1\), the GQC condition degenerates into the \(\)-quasar-convexity condition as studied in Hinder et al.  with the gradient \( f()\) belongs to the internal functions of \(f\). In the case \(d>1\), the GQC condition is instrumental in capturing the crucial characteristic of those optimization applications with each variable block has difficulty to be optimized.

### Main Results

Recall that GQC condition provides a perspective to bound function error \(f()-f(^{*})\) based on internal function, which is different from that based on gradient oracle. We therefore aim to provide an algorithmic framework for finding an approximate suboptimal global solution using internal function. Given an objective function \(f:\) with internal function \(\), our algorithm (Algorithm 1) independently computes points \(_{i}^{t}\) and \(_{i}^{t}\) following OMD over each block. If \(_{i[1:d]}_{i}<\) and internal function \(\) has Lipschitz continuity, we have following basic and primary convergence result of Algorithm 1,

**Theorem 3.2**.: _Assuming that \(\) is \(L\)-Lipschitz continuous with respect to \(\|\|_{*}\) under \(\|\|\) and \(_{}=_{i[1:d]}_{i}<\), and setting \(=(L^{2}d_{}_{i=1}^{d}_{i}^{-1})^{-1/2}/2\), we have_

\[_{t=1}^{T}(f(^{t})-f(^{*}))(n_{i})(d_{})^{1/2}(_{i=1}^{d}_{i} ^{-1})^{3/2}}{T}.\] (6)

However, the estimation provided by Theorem 3.2 depends on \(d_{}\). And the step size relying on \(_{}(_{i=1}^{d}_{i}^{-1})\) might be difficult to set when \(\{_{i}\}_{i=1}^{d}\) is unknown.

We then hope to propose an alternative analytical method that can adapt to unknown \(\{_{i}\}_{i=1}^{d}\) and obtain complexity which does not depends on block dimension \(d\) explicitly. The challenges includes: 1) The algorithm does not know the weight \(1/_{i}\); 2) every \(_{i}\) has dependence on the joint variable \(\) instead of depending on \(_{i}\). Before we present the details of convergence analysis, we need the following notations and assumptions:

Denote \(P_{K,}^{}())=_{i=0}^{K}_{||=i}}f()|}{!}(||+||)^{}\) and let \(_{K,}^{}()=(P_{K,}^{(1)}(), ,\)\(P_{K,}^{()}())\) for any vector-valued function \(:^{n}^{}\). Recalling the definition of \(R_{K,}^{}\) in Eq. (3), we shall also define \(_{K,}^{}()=(R_{K,}^{(1)}(), ,R_{K,}^{()}())\).

**Assumption 3.3**.: Let \(\) be the internal function of \(f\). There exists \(_{1},_{2}>0\), \(K_{0}_{+}\), and \([0,1)\), and a fixed \(^{_{i=1}^{d}n_{i}}\) such that

\[\ \|_{K,}^{}()\|_{ } _{1}^{K}K>K_{0}.\] \[\ \|_{K,}^{}()\|_{ } _{2}\ K_{+}.\]

Assumption 3.3 is a characterization of "polynomial-like" functions. We clarify this view as follows. For a standard polynomial function \(p\), it's clear that \(p\) satisfies Assumption 3.3, since the Taylor expansion of \(p\) after order \(K_{0}\) is always equal to 0 (**IA1**) in Assumption 3.3 holds) and \(\) is a bounded and closed set (**IA2** in Assumption 3.3 holds). Assumption 3.3 is easy to achieve. Shown in Proposition B.2 and Remark B.3 in Appendix B, Assumption 3.3 can be satisfied by many smooth functions defined on bounded region \(\). In addition, we introduce a simple machine learning example: learning one single neuron network over a simplex in the realizable setting.

_Example 3.4_.: The objective function is written as \(f(,)=_{,y}(_{i=1}^{n}_{i} (^{}_{i})-y)^{2}\), where \(_{m}\) and \(=(_{1},,_{m})_{i=1}^{m}_{d}\) and the target \(y\) given \([-C,C]^{d}\) admits \(y=(^{}_{1}^{*})\) for some \(_{1}^{*}_{d}\). For activation function \((x)=\{x\}\), \(f\) satisfies GQC condition and Assumption 3.3 with the internal functions \(}=\{[(_{j=1}^{m}_{j}(^{}_{j})-y)(^{}_{i})]\}_{i=1}^{m}\) for block \(\) and \(_{i}}}=[((^{}_{i})-y) {x}]\) for block \(_{i}\).

Note previous work  studies single neuron learning by considering \(_{1}^{*}\) in the sphere and assuming \(\) follows from a Gaussian distribution. To our knowledge, there is no evidence shows that objective function of Example 3.4 has quasar-convexity. This example demonstrates the advantage of studying the GQC framework over the previous approach. The proof of Example 3.4 is in Section B.2.

Parameter SettingBefore stating the convergence result, we set the parameters as follows:

\[&=_{1}+_{2}+1, H= (T),_{0}=(4H)^{-1},=\{/8}}{H^{3}},\},\\ &=e^{2}+(_{2}),=\{ )+(_{1})}{(^{-1})},K_{0}\}, =\{\{,1\}},^{4}}{()}\}.\] (7)

**Theorem 3.5**.: _Let \(f\) satisfies the GQC condition and denote \(N=_{i[1:d]}\{n_{i}\}\). Under Assumption 3.3, the following estimation holds for Algorithm 1's output \(\{^{t}\}_{t=1}^{T}\)_

\[_{t=1}^{T}(f(^{t})-f(^{*}))(_{i=1}^{d }1/_{i})[(N)+^{3}(6+330240 H ^{5})]T^{-1},\] (8)

Theorem 3.5 implies that for any generalized quasar-convex function \(f\) satisfies Assumption 3.3, the \(T\)-step random solution outputted by Algorithm 1 is a \(((_{i=1}^{d}1/_{i})T^{-1}(N)^{4.5}(T))\)-suboptimal solution. Ignoring the logarithmic factor, the iteration complexity of our algorithm is competitive to the state-of-the-art algorithm when applied to specific application (i.e. policy optimization of reinforcement learning ). Moreover, our algorithm makes iteration complexity depend on \(_{i=1}^{d}1/_{i}\) linearly. In some common applications, \(_{i=1}^{d}1/_{i}\) has no dependence on \(d\), which is the number of variable blocks (see discussions in Section 3.3).

### Application to Reinforcement Learning

This section reveals that GQC condition provides a novel analytical approach to reinforcement learning. We show how to leverage Algorithm 1 to find \(\)-suboptimal global solution for infinite horizon reinforcement learning problem. And in Appendix B.3.2, we show how to leverage Algorithm 1 to minimize finite horizon reinforcement learning problem.

The infinite horizon reinforcement learning is formulated as the following policy optimization problem:

\[_{}J^{}(_{0}),\] (9)

where \(J^{}(_{0})=_{s_{0}}[V^{}(s_{0})]\) and \(=_{i=1}^{||}_{}\) denotes \(||\) probability simplexes. We write \(=\{s_{i}\}_{i=1}^{||}\) and denote the action-value vector on state \(s_{i}\) by \(}(s_{i},)\). The next Proposition 3.6 states that \(J^{}(_{0})\) satisfies the GQC condition for any initial state distribution \(_{0}\).

**Proposition 3.6**.: _Let \(\{^{*}(|s)_{}\}_{s}\) denote the optimal global solution of problem (9). We have that \(J^{}(_{0})\) satisfies the GQC condition in Eq. (5) with internal function \(_{i}()=}(s_{i},)\) for variable block \(_{i}\) and \(\) satisfies Assumption 3.3 with \(_{1}=,_{2}=1\) and \(K_{0}=1\)._

According to Theorem 3.5, if we apply Algorithm 1 to the infinite horizon reinforcement learning basing action-value vector \(}\) with parameter selection Eq. (7), which is actually a simple variant of natural policy gradient descent , then the iterations \(T\) we need to find an \(\)-suboptimal global solution is upper-bounded by \((\{1,^{-1}(^{-1})\}(1-)^{-1}^{-1} ^{4.5}(^{-1})(||))\) under Agarwal et al. 's setting. Therefore, the iteration complexity of Algorithm 1 does not depend on the size of states, since the summation of \(}_{_{0}}}\) over \(\) (\(_{i=1}^{||}1/_{i}=_{i=1}^{||} }_{_{0}}}(s_{i})=1\)) mollifies the accumulation of the maximum of \(_{_{0}}^{^{*}}\) over \(\) with \(||\) times. Specifically, if we take into account the loosest upper bound \(||_{i[1:||]}_{_{0}}^{^{*}}(s _{i})\), then the iteration complexity of algorithm may suffer from the linear dependence on \(||\), since \(_{i[1:||]}_{_{0}}^{^{*}}(s_{i})(1 -)_{i[1:||]}_{0}(s_{i})\). Previous research [2, Theorem 5.3] has demonstrated that utilizing the information of joint variables to separately update each variable block ensures global convergence for problem (9) with \(((1-)^{-2}^{-1})\) iteration complexity. However, their analytical approach is carefully designed for infinite horizon reinforcement learning problems.

## 4 Minimax Optimization

In this section, we introduce the generalized quasar-convexity-concavity (GQCC) condition, which can be verified in real applications such as two-player zero-sum Markov games. We provide a related algorithm for minimax optimization (minimizing \(_{f}(,)\) has been defined in Eq. (4)) over \(=_{i=1}^{d}_{i}=_{i=1}^{d}(_{n_{i}} _{m_{i}})\), under proper assumptions. We specify the divergence-generating function \(v\) as \(v()=_{i()}[((i))]\) in probability simplexes setting. We also provide a framework for minimax problem over the general compact convex regions in Appendix C.

### Generalized Quasar-Convexity-Concavity (GQCC)

We provide a new notion called generalized quasar-convexity-concavity for nonconvex-nonconcave minimax optimization, which is defined as follows:

**Definition 4.1** (Generalized Quasar-Convexity-Concavity (GQCC)).: Denote \(_{i}=_{i}_{i}\) for any \(i[1:d]\), and let \(f:\) be the objective function. We say that \(f\) is generalized quasar-convex-concave on \(\) if for all \(=(,)\), there exist a sequence of functions \(\{f_{i}:^{ d}_{i}\}_{i=1}^{d}\), a sequence of non-negative functions \(\{_{i}:_{+} 0\}_{i=1}^{d}\) and a matrix-valued function \(=(_{1},,_{d}):^{  d}\) where every \(_{i}\) is a \(\)-dimensional vector-valued function, such that

\[_{f}(,)_{i=1}^{d}_{i}()_{ f_{i}((),,)}(_{i},_{i}),\] (10)

where each \(f_{i}(,,)\) is convex-concave for a fixed \(=(_{1},,_{d})^{ d}\). We denote the internal operator of \(f\) for variable block \(_{i}\) by \(_{i}\) where \(_{i}(,_{i})=((_{_{i}},f_{i}(,_{i}))^{},(-_{_{i}}f_{i}(,_{i}))^{})^{}\). Moreover, we say that \(=(_{1}^{},,_{d}^{})^{}\) is the internal operator of \(f\).

The GQCC condition is an extension of the GQC condition in minimax optimization setting. The specific connection between them can be found in Appendix C. The GQCC condition can be viewed as an extension of the convexity-concavity condition in multi-variable optimization; it seamlessly reduces to the convexity-concavity condition with \(f_{1}((),)=f()\) and \(_{1}() 1\), in the case \(d=1\). Assuming every \(_{i}\) is bounded, \(f_{i}((),_{i}) f_{i}(,_{i})\) with Lipschitz continuous gradient and is convex-concave with respect to \(_{i}\), then finding the Nash equilibrium point of \(f\) is reduced to finding the Nash equilibrium points of \(d\) independent convex-concave minimax problems. However, how to find the approximate Nash equilibrium points in more general case has not been well-studied. Most of existing work for minimax optimization without convex-concave assumption are focused on finding the approximate stationary points.

### Main Results

For simplicity, we denote by \(_{i}^{}\) and \(_{i}^{}\) the projection of \(_{i}\) in the \(_{i}\) and \(_{i}\) directions, respectively, i.e., \(_{i}^{}=((_{i}^{})^{},(_{i}^{})^{ })\). Given an objective function \(f:\) with internal operator \(\), our algorithm (Algorithm 2) employs regularized OMD over each distribution independently basing on \(_{i}\) and updates matrix \(^{t}\) to track the behavior of function \(\) iteratively. It's worth noting that each iteration of Algorithm 2 provides explicit expressions for \(_{i}^{t}\) and \(_{i}^{t}\) (see the proof of Theorem 3.5 in Appendix B). Consequently, Algorithm 2 essentially operates as a single-loop algorithm.

**Assumption 4.2**.: In Definition 4.1, we assume that matrix-valued function \(\) has the form of \((^{},)\) where \(^{}^{ d}\) depends on \(\), and \(\) satisfies the following properties on region \(\{^{ d}|\ \|\|_{} C\} \) for some constant \(C>0\):

**Input:**\(\{_{i}^{0}\}_{i=1}^{d}=\{_{i}^{0}\}_{i=1}^{d}=\{(1/n_ {i},,1/n_{i}),(1/m_{i},,1/m_{i})\}_{i=1}^{d}\), \(\{_{t} 0\}_{t=1}^{T}\) with \(_{i=1}^{T}_{t}=1\), \(\{_{t} 0\}_{t=1}^{T}\), \(\) and \(^{0}=\).

**Output:**\(}_{T}=_{t=1}^{T}_{t}^{t}\).

```
1:while\(t T\)do
2:\(^{t}=(1-_{t-1})^{t-1}+_{t-1}(^{t-1},_{t-1})\).
3:for all\(i[1:d]\)do
4:\(_{i}^{t}=_{i}_{i}}{} \ _{i}^{}(^{t-1},_{i}^{t-1}),_ {i}+_{t}(_{i}\|(_{i}^{ })^{t-1})+_{t}v(_{i})\),
5:\(_{i}^{t}=_{i}_{i}}{} \ _{i}^{}(^{t-1},_{i}^{t-1}),_ {i}+_{t}(_{i}\|(_{i}^{ })^{t-1})+_{t}v(_{i})\),
6:\((_{i}^{})^{t}=_{i}^{}_{i}}{ }\ _{i}^{}(^{t},_{i}^{t}),_{i} ^{}+_{t}(_{i}^{}\|( _{i}^{})^{t-1})+_{t}v(_{i}^{})\),
7:\((_{i}^{})^{t}=_{i}^{}_{i}}{ }\ _{i}^{}(^{t},_{i}^{t}),_{i} ^{}+_{t}(_{i}^{}\|( _{i}^{})^{t-1})+_{t}v(_{i}^{})\).
8:endfor
9:\(t t+1\).
10:endwhile ```

**Algorithm 2** Optimistic Mirror Descent with Regularization for Multiple Distributions

**[A\({}_{1}\)]**: There exist constants \(L_{1},L_{2} 0\) such that \(_{i}(,_{i})\) is uniformly \(L_{1}\)-Lipschitz continuous with respect to \(\|\|_{}\) under \(\|\|_{}\), and \(_{i}(,)\) is uniformly \(L_{2}\)-Lipschitz continuous with respect to \(\|\|_{}\) under \(\|\|_{1}\).
**[A\({}_{2}\)]**: There are a positive constant \(>0\) and a set of non-negative constant matrices \(\{_{i},_{i}\}_{i=1}^{d}\) satisfying \(\|_{i=1}^{d}(_{i}+_{i})\|_{}\), such that \(_{(,,)}(,\ ^{})_{i=1}^{d}_{i} _{i}^{}(,_{i}),_{i}-_{i}^{ }\) and \(_{(,,)}(,^{}) _{i=1}^{d}_{i}_{i}^{}(,_{i}), _{i}^{}-_{i}\).
**[A\({}_{3}\)]**: There exists \([0,1)\) such that \((,)\) is a \(\)-contraction mapping under \(\|\|_{}\), and \(\|(,)\|_{} C\) for any \(\).

We present Lemma 4.3 to demonstrate that there exist \(^{*}^{ d}\), \(^{*}\) and \(^{*}\) satisfy the saddle point and fixed point conditions of function \(\), i.e., Eq. (11), under proper assumptions.

**Lemma 4.3**.: _Assuming that Assumption 4.2 holds, \([(,,)]_{k,j}\) is continuous, convex with respect to \(\), concave with respect to \(\) for any \((k,j)\), and \(_{k,j,i}_{i}\}_{k,j},[_{i}]_{k,j}}{[ _{i}]_{k,j}+[_{i}]_{k,j}} C^{}\) for some \(C^{}>0\), then there exist \(^{*}^{ d}\) and \(^{*}\) such that_

\[^{*}=(^{*},^{*},^{*}),^ {*}(^{*},,^{*}),^{*}(^{*},^{*},).\] (11)

For Algorithm 2, we let \(_{T,t}=_{t}_{j=t+1}^{T}(1-_{j})\) for any \(T t\) and \(_{T,}=_{T}\), and set parameters

\[c=2(1-)^{-1},\,}{16L_{2}(( L_{1})^{1/2}+ 1)},\,_{t}=,\,_{t}=_{T,t},\,_{t}=}{_{t}},\,_{t}=1-_{t}.\] (12)

Then we have the following convergence result by denoting \(M=_{i[1:d]}\{m_{i}+n_{i}\}\).

**Theorem 4.4**.: _For any generalized quasar-convex-concave function \(f\) which satisfies Assumption 4.2 with \(^{*}\), where \(^{*}\) satisfies Eq. (11). Algorithm 2's output \(}_{T}=(}_{T},}_{T})\) satisfies_

\[_{f}(}_{T},}_{T}) 60_{ }(_{i=1}^{d}_{i}())(1-)^{-1}( (M)+ L_{1}^{2}+L_{1}Y_{T}^{})T^{-1},\]

_where \(Y_{T}^{}=8(c+1)[(M)+160 L_{2}+2 L _{1}^{2}(1+64C^{2})]((c+T)+1)\)._

Similar to minimization Algorithm 1, the iteration complexity of minimax Algorithm 2 linearly depends on the upper bound of \(_{i=1}^{d}_{i}\) over \(\). Generally, the upper bound of \(_

### Application to Infinite Horizon Two-Player Zero-Sum Markov Games

In this section, we show how to leverage Algorithm 2 to achieve accelerated rates for optimizing infinite horizon two-player zero-sum Markov games. Our algorithm use \(}(^{-1})\) iteration bound to find an \(\)-approximate Nash equilibrium of infinite horizon two-player zero-sum Markov games.

As similar as the definition of discounted MDP in Preliminary, we utilize \(=(,,,,,, _{0})\) to define a infinite horizon two-player zero-sum Markov game. The difference here compared to Section 3.3 is that the cost function \(\) is defined on \(\) with values in \(\), and the transition model \((s|s^{},a^{},b^{})\) denotes the probability of transitioning into state \(s\) upon player 1 taking action \(a^{}\) and player 2 taking action \(b^{}\) in state \(s^{}\). We can define the value function \(V^{}\) and action-value function \(Q^{}\) on the joint distribution \(=(,)=_{i=1}^{| |}_{}_{i=1}^{||}_{ }\). The infinite horizon two-player zero-sum Markov games consider the following policy optimization problem:

\[_{}_{}J^{ ,}(_{0}),\] (13)

where \(J^{}(_{0})=_{s_{0}_{0}}[V^{ }(s_{0})]\). The following proposition indicates that \(J^{}\) is general quasar convex-concave, and satisfies Assumption 4.2 and the condition of Theorem 4.4,

**Proposition 4.5**.: _For any \(=(_{1},,_{||})\) with every \(_{i}^{||||}\), define function \(f_{i}(,_{i}):=_{i}^{}_{i} _{i}\) for any \(i[1:||]\). There exists a tensor-valued function \(\) such that \(J^{}(_{0})\) satisfies GQCC condition with \(f_{i}((),_{i})=f_{i}(^{*}, _{i})\) for any \(_{0}_{}\), where \(^{*}\) satisfies the conditions mentioned in Eq. (11). Moreover, \(\) satisfies Assumption 4.2._

According to Proposition 4.5 and Theorem 4.4, if we apply Algorithm 2 to the infinite horizon two-player Markov games basing internal operator \(_{i}(,)=(_{i}^{} _{i}^{},-_{i}^{}_{i})^{}\) for block \(_{i}\) with parameter selection Eq. (12), which is actually a variant of optimistic gradient descent/ascent for Markov games , then the iterations \(T\) we need to find an \(\)-approximate Nash equilibrium is upper-bounded by \(}((1-)^{-2.5}^{-1})\). To the best of our knowledge, our iteration bound matches state-of-the-art iteration bound and is a factor of \((1-)^{-1.5}||\) better than \(}((1-)^{-4}||^{-1})\) bound of Cen et al. . Since the upper bound of \(_{i=1}^{||}_{i}\) over feasible region \(\) in infinite horizon two-player zero-sum Markov games' setting satisfies \(_{i=1}^{||}_{i}()_{i=1}^{||}[_{_{0}}^{,^{*} ()}(s_{i})+_{_{0}}^{ {x}^{*}(),}(s_{i})] 2\) for any \(\), our algorithm's iteration bound does not depend on the size of states.

## 5 Conclusion

In this work, we introduce two function structures: GQC and GQCC and provide related algorithmic frameworks with convergence result. To complement our result, we also show that discounted MDP and infinite horizon two-player zero-sum Markov games admit the GQC and GQCC condition, respectively, and satisfy our mild assumptions.

## 6 Acknowledgements

C. Fang was supported by National Key R&D Program of China (2022ZD0114902) and the NSF China (No.62376008). L. Luo was supported by National Natural Science Foundation of China (No. 62206058), Shanghai Sailing Program (22YF1402900), Shanghai Basic Research Program (23JC1401000), and the Major Key Project of PCL under Grant PCL2024A06.