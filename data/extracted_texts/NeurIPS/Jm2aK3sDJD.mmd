# VLG-CBM: Training Concept Bottleneck Models

with Vision-Language Guidance

 Divyansh Srivastava, Ge Yan1, Tsui-Wei Weng

{ddivyansh, geyan, lweng}@ucsd.edu

UC San Diego

Equal contribution

###### Abstract

Concept Bottleneck Models (CBMs) provide interpretable prediction by introducing an intermediate Concept Bottleneck Layer (CBL), which encodes human-understandable concepts to explain models' decision. Recent works proposed to utilize Large Language Models and pre-trained Vision-Language Models to automate the training of CBMs, making it more scalable and automated. However, existing approaches still fall short in two aspects: First, the concepts predicted by CBL often mismatch the input image, raising doubts about the faithfulness of interpretation. Second, it has been shown that concept values encode unintended information: even a set of random concepts could achieve comparable test accuracy to state-of-the-art CBMs. To address these critical limitations, in this work, we propose a novel framework called Vision-Language-Guided Concept Bottleneck Model (VLG-CBM) to enable faithful interpretability with the benefits of boosted performance. Our method leverages off-the-shelf open-domain grounded object detectors to provide visually grounded concept annotation, which largely enhances the faithfulness of concept prediction while further improving the model performance. In addition, we propose a new metric called Number of Effective Concepts (NEC) to control the information leakage and provide better interpretability. Extensive evaluations across five standard benchmarks show that our method, VLG-CBM, outperforms existing methods by at least 4.27% and up to 51.09% on _Accuracy at NEC=5_ (denoted as ANEC-5), and by at least 0.45% and up to 29.78% on _average accuracy_ (denoted as ANEC-avg), while preserving both faithfulness and interpretability of the learned concepts as demonstrated in extensive experiments2.

## 1 Introduction

As deep neural networks become popular in real-world applications, it is crucial to understand the decision of these black-box models. One approach to provide interpretable decisions is the Concept Bottleneck Model (CBM) , which introduced an intermediate concept layer to encode human-understandable concepts. The model makes final predictions based on these concepts. Unfortunately, one major limitation of this approach is that it requires concept annotations from human experts, making it expensive and less applicable in practice as concept labels may not always be available.

Recently, a line of works utilized the powerful Vision-Language Models (VLMs) to replace manual annotation . They used Large Language Models (LLMs) to generate set of concepts, and then trained the models in a post-hoc manner under the guidance of VLMs or neuron-level interpretability tool . By eliminating the expensive manual annotations, some of these CBMs  could be scaled to large datasets such as ImageNet . However, these CBMs  still face two critical challenges:1. **Challenge #1: Inaccurate concept prediction.** The concept predictions in these CBMs often contain factual errors i.e. the predicted concepts do not match the image. Moreover, as concepts are generated by LLMs, there are some non-visual concepts, for example "loud music" or "location" used in LF-CBM , which further hurt the faithfulness of concept prediction.
2. **Challenge #2: Information leakage.** Recently, [13; 12] observed the information leakage in CBMs through empirical experiments - they found that the concept prediction encodes unintended information for downstream tasks, even if the concepts are irrelevant to the task.

In this paper, we propose a new framework called **V**ision-**L**anguage-**G**uided **C**oncept **B**ottleneck **M**odel (**VLG-CBM**) to address these two major challenges. Our contributions are summarized below:

1. To address **Challenge #1**, we propose to use the open-domain grounded object detection model to generate localized, visually recognizable concept annotations in Section 3. This approach automatically filters the non-visual concepts. Furthermore, the location information is utilized to augment the data. As far as we know, our VLG-CBM is the first end-to-end pipeline to build CBM with vision guidance from open-vocabulary object detectors.
2. To address **Challenge #2**, we provide the first rigorous theoretical analysis which proves that CBMs have serious issues on information leakage in Section 4.1, whereas previous study on information leakage [12; 25] only provides empirical explanations. Building on our theory, we further propose a new metric called the Number of Effective Concepts (NEC) in Section 4.2, which facilitates fair comparison between different CBMs. We also show that using NEC can help to effectively control information leakage and enhance interpretability in our VLG-CBM.
3. We conduct a series of experiments in Section 5 and demonstrate that our VLG-CBM outperforms existing methods across 5 standard benchmarks by at least 4.27% and up to 51.09% on _Accuracy at NEC=5_ (denoted as ANEC-5), and by at least 0.45% and up to 29.78% on _average accuracy across different NECs_ (denoted as ANEC-avg). Our learned CBM achieves a high sparsity of 0.2% in the final layer even on large datasets including Places365, preserving interpretability even with a large number of concepts. Additionally, we qualitatively demonstrate that our method provides more accurate concept attributions compared to existing methods.

Figure 1: We compare the decision explanation of VLG-CBM with existing methods by listing top-5 contributions for their decisions. Our observations include: (1) VLG-CBM provides _concise_ and _accurate_ concept attribution for the decision; (2) LF-CBM  frequently uses negative concepts for explanation, which is less informative; (3) LM4CV attributes the decision to concepts that do not match the images, a reason for this is that LM4CV uses a limited number of concepts, which hurts CBM’s ability to explain diverse images; (4) Both LF-CBM and LM4CV have a significant portion of contribution from non-top concepts, making decisions less transparent. Full figure is in Appendix Fig. D.1.

## 2 Related work

**Concept Bottleneck Model (CBM).** The seminal paper  first proposed self-explaining models by leveraging the idea of autoencoder to learn interpretable basis concepts in an unsupervised manner without pre-specified concepts. Later,  proposed to learn interpretable concepts with human specifications (labels) in the concept bottleneck layer (CBL), and coin the term Concept Bottleneck Models (CBM). CBL is followed by a linear prediction layer, which maps concepts to classes, enabling interpretable final decisions. Formally, let feature representation generated by a frozen backbone represented by \(z=(x)\), CBL concept prediction as \(g(z)=W_{c}z\), and the final prediction layer as \(h()=W_{F}g(z)+b_{F}\). The final class prediction of the CBM is given by \(=h(g(z))=h g(x)\).

Under this setting, the key in training a CBM is obtaining an annotated {(image, concept)} paired dataset for training concept bottleneck layer \(g\). In , the authors used human-specified labels to train the CBL in a supervised way. However, obtaining labels with human annotators could be very tedious and costly. Recently, , , and  proposed to utilize Large Language Models (LLM) to generate a set of concepts \(S\), then train CBL by aligning image and concepts with the guidance of vision language models (e.g. CLIP). For example, Oikarinen et al.  proposed LF-CBM to train CBM by directly learning a mapping from the embedding space of backbone to concept values in the CLIP space using cosine cubed loss function with the neuron interpretability tool, and then mapping concepts to classes using sparse linear layer.  proposed LM4CV, a task-guided concept searching method that learns text embeddings in the CLIP space, and then maps the learned embeddings to concepts obtained from LLM using nearest neighbor. Yang et al.  proposed LaBo, using submodular optimization to reduce the concept set, followed by using CLIP backbone for obtaining concept values. However, as we show in Sections 5.1 and 5.3, these methods suffer from multiple issues: (i) The concept prediction is often incorrect and does not capture the visual attributes required for downstream class prediction (e.g. see Fig. 1 b )(ii) VLMs like CLIP suffer from modality gap between image and text embeddings  resulting in encoding unintended information, and even random concepts can achieve high accuracy . To address these issues, we explicitly ground the concepts on the training dataset using an open-domain object detection model and then using the obtained concepts for learning CBL - this can ensure a more faithful representation of fine-grained concepts and avoids the modality gap issues introduced by VLMs. Table 1 demonstrates the superiority of VLG-CBM over existing methods [15; 25; 27] on properties including controlling information leakage, flexibility to use any backbone, and accurate concept prediction.

There are some recent works aim at addressing the challenges of CBMs. Similar to us, Pham et al.  uses an open-vocabulary object detection model to provide an explainable decision. However, their model is directly adapted from an OWL-ViT model, while our VLG-CBM uses an open-vocabulary object detection model to train a CBL over any base model, providing more flexibility. Additionally, their model requires pretraining to get best performance, while our VLG-CBM could be applied post-hoc to any pretrained model. Kim et al.  proposed to filter non-visual concepts by adding a vision activation term to the concept selection step, whereas VLG-CBM uses an open-vocabulary object detectors in multiple stage of CBM pipeline: for filtering non-visual concepts and the guiding training of concept bottleneck layer. Sun et al.  aims at eliminating the information leakage, and the authors evaluate the information leakage by measuring the performance drop speed after

    &  &  &  \\  Method &  Control on \\ information leakage \\  &  Unlimited concept \\ numbers \\  &  Flexible \\ backbone \\  &  Accurate concept \\ prediction \\  &  Vision-guided \\ concept filtering \\  & 
 Interpretable \\ decision \\  \\  
**Baselines:** & & & & & & \\ LF-CBM & \(\) & ✓ & ✓ & \(\) & \(\) & \(\) \\ LaBo & \(\) & ✓ & \(\) & \(\) & \(\) & \(\) \\ LM4CV & ✓ & \(\) & \(\) & \(\) & \(\) & \(\) \\ 
**This work:** & & & & & & \\ VLG-CBM & ✓ & ✓ & ✓ & ✓ & ✓ & ✓ \\   

Table 1: Comparative analysis of methods based on evaluation, flexibility, and interpretability. Here, ✓ denotes the method satisfies the requirement, \(\) denotes the method partially satisfies the requirement, and \(\) denotes the method does not satisfy the requirement. We compare with SOTA methods including LF-CBM , Labo  and LM4CV .

removing top-contributing concepts. This metric can be controlled by our proposed NEC metric, because the performance reach minimum after removing all contributing concepts. Roth et al.  demonstrate that random words and characters achieve comparable CLIP zero-shot performance on visual classification tasks. However, their work does not address information leakage problem and is a very different setting from our work. To date, most of the CBMs focused on vision domains, including this work. There are some recent work applying CBM approach to different domains and different tasks, e.g. interpretable language models for text classifications [21; 22; 11] and for continual learning . We refer the interested readers to their papers for more details.

**Open Domain Language Grounded Object Detection.** Recent works, including GLIP , GLIPv2 , and GroundingDINO  detect objects in images in an open-vocabulary manner conditioned on natural language queries. In this work we propose to utilize open-vocabulary object detectors for automatically generating grounded concept dataset for training CBMs. This removes the need for human labelers, which is costly, tedious, and does not scale to large datasets. Further, the detected objects provide necessary vision-guidance for CBMs training as demonstrated in our experiments.

## 3 Method

In this section, we describe our novel automated approach to train a CBM with both Vision and Language Guidance to ensure faithfulness, which is currently lacking in the field. Our approach, abbreviated as VLG-CBM in the paper, generates an auxiliary dataset grounded on fine-grained concepts present in images for training a sequential CBM. Section 3.1 describes our approach to generating an auxiliary dataset used in training CBM, Section 3.2 describes our approach to training concept bottleneck layer, and Section 3.3 describes the training of sparse layer to obtain class labels from concepts in an interpretability-preservable manner. The overall pipeline is shown in Fig. 2.

### Automated generation of auxiliary dataset

Here we describe our novel automated approach for generating labeled datasets for training CBMs. Let \(f:\) be the neural network mapping images to corresponding class labels, where \(=^{H W 3}\) denotes the input image space and \(=\{1,2,,C\}\) denotes the label space, \(C\) is the number of classes. Denote \(D=\{(x_{i},y_{i})\},x_{i},y_{i}\) the dataset used for training \(f\), where \(x_{i}\) is the \(i\)-th image and \(y_{i}\) is the corresponding label. Let \(S\) be a set of natural-language concepts describing the fine-level visual details from which classes are composed. We propose to generate a modified and auxiliary dataset \(D^{}\) from \(D\) such that each image contains finer-grained concepts that

Figure 2: VLG-CBM pipeline: We design automated Vision+Language Guided approach to train Concept Bottleneck Models.

are useful in predicting the classes, along with the target class. The overall process of obtaining the modified dataset \(D^{}\) can be divided into two steps:

* **Language supervision from LLMs to generate a set of candidate concepts**: We follow the steps proposed in LF-CBM  for generating candidate concepts \(S_{c}\) for each class \(c\) by prompting LLM to obtain visual features describing the class.
* **Vision supervision from Open-domain Object Detectors to ground candidate concepts to spatial information**: We propose using Grounding-DINO Swin-B, current state-of-the-art grounded object detector, for obtaining bounding boxes of candidate concepts in the dataset. For each image \(x_{i}\) with class label \(c\) and candidate concepts \(S_{c}\), we prompt Grounding DINO model with \(S_{c}\) and obtain \(K_{i}\) bounding boxes: \[B_{i}=\{(b_{j},t_{j},s_{j})\}_{j=1}^{K_{i}},\] (1) where \(b_{j}^{4 2}\) is the \(j\)-th bounding box coordinates, \(t_{j}\) is the corresponding confidence given by the model and \(s_{j} S_{c}\) is the concept of this bounding box. We define a confidence threshold \(T\) and remove bounding boxes with confidence less than \(T\) to get filtered bounding boxes for each image: \[_{i}=\{(b,t,s) B_{i} t>T\}.\] (2)

After collecting bounding boxes for every image, we filter out the concepts that do not appear in any bounding box, and get our final concept set \(\):

\[=\{s S(,,s)_{i=1}^{|D|}_{i}\}.\] (3)

The one-hot encoded concept label vector \(o_{i}\{0,1\}^{||}\) for image \(x_{i}\) is thus defined as:

\[(o_{i})_{j}=1,&$ appears in $_{i}$},\\ 0,&.\] (4)

Our final concept-labeled dataset \(D^{}\) for training CBM can be written as:

\[D^{}=\{(x_{i},o_{i},y_{i})\}_{i=1}^{|D|}\] (5)

### Training Concept Bottleneck Layer

After constructing the concept-labeled dataset \(D^{}\), we now define our approach to train the concept bottleneck layer for predicting the fine-grained concepts in the input image in a multi-label classification setting. Let \(:^{d}\) be a backbone that generates \(d\)-dimensional embeddings \(z=(x)\) for input image \(x\). Note that \((x)\) can be a pre-trained backbone or trained from scratch. Define \(g\) to be the Concept Bottleneck Layer (CBL) which maps embeddings to concept logits. We train a sequential CBM [6; 13]\(g((x))\) to predict concepts in an image using Binary Cross Entropy (BCE) loss for multi-label prediction. Additionally, to improve the diversity of the concept-labeled dataset \(D^{}\), we augment the training dataset by cropping images to a randomly selected bounding box and modifying the target one-hot vector to predict the concept corresponding to the bounding box. Our optimization objective in terms of BCE loss can be written as:

\[_{g}_{CBL},\ _{CBL}=|}_{i=1}^{ |D^{}|}BCE[g(x_{i}),o_{i}]\] (6)

### Mapping Concept to Classes

In this section, we define our approach to training a sparse linear layer to obtain class labels from concepts in an interpretability-preservable manner. Let \(h:^{d}^{C}\) be the sparse linear layer with weight matrix \(W_{F}\) and bias \(b_{F}\), which maps concept logits to class logits. We train the sparse layer using the original dataset \(D\) by first obtaining concept logits from the trained CBL(frozen), normalizing the concept logits with the mean and variance on training set, and then using them to predict class logits. Our optimization objective in terms of Cross Entropy (CE) loss can be written as:

\[_{h}_{SL},\ _{SL}=_{(x,y) D} CE[h g (x),y]+ R_{},\] (7)where \(R_{}=(1-)\|W_{F}\|_{2}^{2}+\|W_{F}\|_{1}\) is the elastic-net regularization  on weight matrix \(W_{F}\), \(\) is a hyperparameter controlling regularization strength. We use GLM-SAGA solver to solve this optimization problem.

## 4 Unifying CBM evaluation with Number of Effective Concepts (NEC)

Besides training, another important challenge for CBM is: _how to evaluate the semantic information learned in the CBL?_ Conventionally, the classification accuracy for final class labels is an important metric for evaluating CBMs, with the intuition that a good classification accuracy indicates that useful semantic information is learned in the CBL. However, purely using accuracy as the evaluation metric could be problematic, as it has been shown that information leakage exists in jointly or sequentially trained CBM [13; 12]. That is to say, the CBL could contain _unintended information_ that could be used for downstream classification hence achieving high classification accuracy, even if the concept is irrelevant to the task. In fact, recently  showed that, when increasing the number of concepts, a randomly selected concept set could even approach the accuracy of the concept set chosen with sophistication, supporting the existence of information leakage.

To better understand this phenomenon, in section 4.1, we conduct a first theoretical analysis to investigate random CBL and its capability. To the best of our knowledge, this is the first formal analysis of random CBL. Next, inspired by our theoretical result, we propose a new evaluation metric for CBM, named NEC in section 4.2. NEC provides a way to control information leakage and enhance the interpretability of model decisions.

### Theoretical analysis of the Random CBL

We start by defining the notations. Denote \(k\) the number of concepts in CBL. We assume that the CBL \(g\) consists of a single linear layer: \(g(z)=W_{c}z\), where \(W_{c}^{k d}\) and \(z^{d}\), and the final layer \(h\) is also linear: \(h g(z)=W_{F}g(z)+b_{F}\), where \(W_{F}^{C k}\), \(b_{F}^{C}\). This is the common setting for CBMs. The following theorem suggests a surprising conclusion: _a linear classifier upon random (i.e. untrained) CBL could accurately approximate any linear classifier trained directly on the representation, as the number of concepts in the CBL goes up._

**Theorem 4.1**.: _Suppose \(^{d d}\) is the variance matrix of the representation \(z\) which is positive definite, \(_{max}\) is the largest eigenvalue of \(\), and the weight matrix \(W_{c}^{k d}\) is sampled i.i.d from a standard Gaussian distribution. For any linear classifier \(f\) which is built directly on the representation \(z\), i.e. \(f:^{d},f(z)=w^{}z+b\), it could be approximated by another linear classifier \(\) on concept logits \(g(z)=W_{c}z\), i.e. \(f(z)(z)=^{}g(z)+\), with the expected square error \(E(k)\) upper-bounded by_

\[E(k)_{max}(1-)\|w\|_{2}^{2},&k<d;\\ 0,&k d.\] (8)

_Here \(E(k)=_{W_{c}}[_{(,)}_{z}\ [|f(z)-(z)|^{2}]]\) denotes the average square error, \(w^{d}\), \(^{k}\), \(k\) is the number of concepts in CBL._

_Remark 4.1_.: In Theorem 4.1, we consider a 1-D regression problem where we use a linear combination of concept bottleneck neurons to approximate any linear function. The multi-class classification result could be derived by applying Theorem 4.1 to each class logit (see Corollary A.1). From Eq. (8), we could see that the expected error goes down linearly when concept number \(k\) increases, and achieves 0 when \(k d\), where \(d\) is the dimension of backbone representation \(z\). This suggests that, even with a random CBL (i.e. \(W_{c}\) is simply drawn from a standard Gaussian distribution without any training), the classifier could still approximate the original classifier well and achieve good accuracy, when concept number \(k\) is large enough. We defer the formal proof of Theorem 4.1 to Appendix A.

### A New Evaluation Metric for CBM: Number of Effective Concepts (NEC)

Theorem 4.1 provides a formal theoretical explanation on 's observation. Moreover, it raises a concern on the evaluation of CBMs: _model classification accuracy may not be a good metric for evaluating the semantical information learned in CBL, because a random CBL could also achieve high accuracy._ To address this concern, we need to control the concept number \(k\) so that the semantically meaningful CBLs can be distinguished with random CBLs w.r.t. the final classification performance.

We notice that previous works mainly use two approaches to control \(k\):

1. Control the total number of concepts:  used a more concise concept layer, i.e. reduce the total number of concepts. However, this approach may miss some important concepts due to limitations in total concept numbers. Additionally, they used a dense final layer which is less interpretable for humans, as each decision is related to the whole concept set.
2.  suggested using a sparse linear layer for final prediction to enhance interpretability. Though sparsity is initially introduced to enhance interpretability, we note that this also reduces the number of concepts used in the decision, thus controlling the information leakage. However, the problem is, these works lack the quantification for sparsity, which is necessary for fair comparison between methods.

To provide a unified metric for both approaches, we propose to measure the Number of Effective Concepts (NEC) for final prediction as a sparsity metric. It is defined as

\[NEC(W_{F})=_{i=1}^{C}_{j=1}^{k}\{(W_{F})_{ij} 0\}\] (9)

Intuitively, NEC measures the average number of concepts the model uses to predict a class. Using NEC to evaluate CBM provides the following benefits:

1. A smaller NEC reduces the information leakage. As shown in Fig. 3, with large NEC, even random CBL could achieve near-optimal accuracy, suggesting potential leakage in information. However, by reducing NEC, the accuracy of random concepts drops quickly. This implies enforcing a small NEC could help to control information leakage.
2. A model with a smaller NEC provides more interpretable decision explanations. Humans can recognize an object with several important visual features. However, models can utilize tens or hundreds of concepts for the final prediction. By using a smaller NEC, the model's decision could be attributed mainly to several concepts, making it more interpretable to human users.
3. NEC enables fair comparison between CBMs. Comparing the performance of CBMs has long been a challenging problem, as different models use different numbers of concepts and different styles of final layers (sparse/dense). NEC considers both, thus providing a fair metric to compare different models.

Given these benefits, we suggest to control the NEC when comparing the performance of CBMs. In Section 5, we provide experiments with controlled NEC, where we observed our VLG-CBM  outperforms other baselines.

Figure 3: Accuracy comparison between our VLG-CBM, LF-CBM and randomly initialized concept bottleneck layer under different NEC. The experiment is conducted on the CIFAR10 dataset. From the results, we could see that (1) for NEC large enough, even a random CBL could achieve near-optimal accuracy, supporting the existence of information leakage; (2) when NEC decreases, the accuracy of LF-CBM and random weights begin to drop, while our VLG-CBM does not have significant decrease.

Experiments

In this section, we conduct a series of experiments to evaluate our method, including illustrating the faithfulness of concept prediction, interpretability of model decisions, and performance with controlled NEC.

### Performance comparison

**Setup.** Following prior work , we conduct experiments on five image recognition datasets: CIFAR10, CIFAR100, CUB, Places365 and ImageNet. For the choice of backbone, we categorize the experiments into two categories:

1. CLIP backbone: For CLIP backbone, we choose CLIP-RN50 for all datasets.
2. non-CLIP backbone: We follow the choice of  to use ResNet-18 for CUB and ResNet-50 (trained on ImageNet) for Places365 and ImageNet as the backbone.

The reason of this categorization is some previous works (e.g. LaBo only supports CLIP backbone.

**Baselines.** We compare our method with three major baselines when applicable: LF-CBM, LaBo, and LM4CV. These are SOTA methods for constructing scalable CBMs, with  most flexible and [27; 25] limited by specific architecture and not available for certain dataset. Additionally, we present the results from a randomly initialized CBL for comparison.

**Metrics.** As discussed in Section 4, in order to evaluate the final classification power of CBM, we should acculate the Accuracy under specified NEC(ANEC). Therefore, we measure the following two metrics:

1. **Accuracy at NEC=5(ANEC-5):** This metric is designed to show the performance of CBM which could provide an interpretable prediction. We choose the number 5 so that human users could easily inspect all concepts related to the decision without much effort.
2. **Average accuracy(ANEC-avg):** To evaluate the trade-off between interpretability and performance, we also calculate the average accuracy under different NECs. In general, higher NEC indicates a more complex model, which may achieve better performance but also hurt interpretability. We choose six different levels: \(5,10,15,20,25,30\) and measure the average accuracy.

**Controlling NEC.** As we discussed in Section 4, there are two approaches to control NEC: (1) using a dense final layer and directly controlling the number of concepts and (2) training a sparse final layer with appropriate sparsity. LM4CV used the first approach, where the number of concepts could be directly set as target NEC. For LF-CBM and our VLG-CBM, the second approach is utilized: To achieve target sparsity, we control the regularization strength \(\) in GLM-SAGA. GLM-SAGA provides a regularization path, which allows us to gradually reduce regularization strength and get a series of weight matrices with different sparsity. Specifically, we start with \(_{0}=_{max}\) which gives the sparsest weight. Then, we gradually reduce the \(\) by \(_{t+1}=_{t}\) until we achieve the desired NEC. We choose the weight matrix with the closest NEC to our target and prune the weights from smallest magnitude to largest to enforce accurate NEC. LaBo  did not provide a NEC control method. Hence, we apply sparse final layer training of LaBo's concept prediction to control NEC.

**Results.** We summarize the test results for with backbone in Table 2 and results with non-CLIP backbones in Table 3. From the results, we could see that:

1. The accuracy at NEC \(=5\) provides a good metric for evaluating the semantic information in CBL: As shown in the table, the accuracy of random CBL is much lower with NEC \(=5\), which implies the information leakage is controlled and the accuracy could better reflect the useful semantic information learned in the CBL.
2. The performance of LM4CV is even worse than random CBL. An explanation to this is LM4CV utilizes a dense final layer, which is intrinsically inefficient to interpret as each class is connected to all the concepts, including the irrelevant ones. When limiting the NEC to a small value to provide a concise explanation, the model has to largely reduce the concept number which sacrifices the prediction power.

3. Our method significantly outperforms all the baselines at least 4.27% and up to 51.09% on accuracy at NEC=5, and by at least 0.45% and up to 29.78% on average accuracy across different NECs, illustrating both high performance and good interpretability.

### Visualization of Cbl neurons

In order to examine whether our CBL learns concepts aligned with human perception, we list the top-5 activated images for example concept neurons on the model trained on the CUB dataset in Fig. 4. As shown in the figure, our CBL faithfully captures the corresponding concept. We provide more visualization results in Appendix K.

### Case study

In this section, we conduct a case study to compare the concept prediction between our VLG-CBM, LF-CBM  and LM4CV as shown in Fig 1. We provide extended results and comparison with LaBo  in Appendix G.2. For our method, we use the final layer with NEC \(=5\). We show that our method provides more accurate concept prediction and more interpretable decisions for users.

**Decision interpretability.** We examine the explanation of each CBM model on example images by showing the top-5 concept contributions. The contribution of each concept is calculated as the product of the concept prediction value and corresponding weight. Formally, the contribution of

   Dataset &  &  &  &  \\  Metrics & ANEC-5 & ANEC-avg & ANEC-5 & ANEC-avg & ANEC-5 & ANEC-avg & ANEC-5 & ANEC-avg \\   Random & 67.55\% & 77.45\% & 29.52\% & 47.21\% & 18.04\% & 39.63\% & 25.37\% & 40.13\% \\  LF-CBM & 84.05\% & 85.43\% & 56.52\% & 62.24\% & 52.88\% & 62.24\% & 31.35\% & 52.70\% \\ LM4CV & 53.72\% & 69.02\% & 14.64\% & 36.70\% & 3.77\% & 26.65\% & 3.63\% & 15.25\% \\ LaBo & 78.69\% & 82.05\% & 44.82\% & 55.18\% & 24.27\% & 45.53\% & 41.97\% & 59.27\% \\
**VLG-CBM(Ours)** & **88.55\%** & **88.63\%** & **65.73\%** & **66.48\%** & **59.74\%** & **62.70\%** & **60.38\%** & **66.03\%** \\   

Table 2: Performance comparison with CLIP-RN50 backbone. We compare our method with a random baseline, LF-CBM, LM4CV and LaBo. The random baseline has 1024 neurons for CIFAR10 and CIFAR100, 512 for CUB and 4096 for ImageNet.

Figure 4: Top-5 activated images of example concepts neurons in VLG-CBM on CUB dataset.

   Dataset &  &  &  \\  Metrics & ANEC-5 & ANEC-avg & ANEC-5 & ANEC-avg & ANEC-5 & ANEC-avg \\   Random & 68.91\% & 73.44\% & 17.57\% & 28.62\% & 41.49\% & 61.97\% \\  LF-CBM & 53.51\% & 69.11\% & 37.65\% & 42.10\% & 60.30\% & 67.92\% \\
**VLG-CBM (Ours)** & **75.79\%** & **75.82\%** & **41.92\%** & **42.55\%** & **73.15\%** & **73.98\%** \\   

Table 3: Performance comparison with non-CLIP backbone. We compare against LF-CBM and a random baseline, as LM4CV, LaBo does not support non-CLIP backbone. The random baseline has 512 neurons for CUB, 2048 for Places365, and 4096 for ImageNet.

\(i\)-th concept to \(j\)-th class is defined as: \((i,j)=g_{i}(z)(W_{F})_{ji}\). We pick the top-5 contributing concepts for the final predicted class and visualize it in Fig. 1. We could see that:

1. For other CBMs, a large portion of the final decision is attributed to the "Sum of other features". This part hurts the interpretability of CBM, as it's difficult for users to manually inspect all these concepts in practice. We conduct further study on this in Section 5.4. Our model, however, provides a concise explanation from a few concepts because we apply the constraint NEC=5. This ensures users can understand model decisions without difficulties.
2. Our VLG-CBM provides explanation more aligned with human perception. From the example, we can also see that our model explains the decision with clear visual concepts. Other CBMs attribute the decision to non-visual concepts (e.g. LaBo), concepts that do not match the image (e.g. LM4CV), or negative concepts (LF-CBM).

### Do Top-5 concepts fully explain the decision?

Besides training a final layer with a small NEC, another common approach to provide a concise explanation is showing only the top contribution concepts. However, we argue that this approach may not faithfully explain the model's behavior, as the non-top concepts also make a significant contribution to the decision. To verify this, we conduct the following experiment: On the CUB dataset, we prune the final weight matrix \(W_{F}\) to leave only the top-5 concepts for each class, whose weight has the largest magnitude. Then, we use the pruned model to make predictions and compare them with the prediction results from the original model. Table 4 shows results for our VLG-CBM which uses NEC\(=5\) to control sparsity, and other three baselines, LaBo, LF-CBM and LM4CV, without any constraint on NEC. As shown in the table, for all three baselines, a large portion of predictions changes after pruning. This suggests that without explicitly controlling NEC, only showing top-5 contributing concepts does not faithfully explain all of the model decisions. Hence, we recommend training the final layer with NEC controlled to obtain a concise and faithful explanation as we proposed in Section 4.

## 6 Conclusion, Potential Limitations and Future work

In this work, we study how to improve the interpretability and performance of concept bottleneck models. We introduce a novel approach VLG-CBM based on both vision and language guidance, which successfully improves both the interpretability and utility of existing CBMs in prior work. Additionally, our theoretical analysis show that information leakage may exist on even in the untrained CBLs, serving the foundations for our proposed new NEC-controlled metrics (ANEC-5 and ANEC-avg). We show that NEC not only allow fair evaluation of CBMs but also can be used to effectively control information leakage of CBM and ensure interpretability. Extensive experiments on image classification benchmarks demonstrated our VLG-CBM largely outperform previous baselines especially for small NEC, providing more interpretable decisions for users.

Despite the superior performance of VLG-CBM over prior work as demonstrated in extensive experiments, one potential limitation is the dependence on large pretrained models (e.g. the success of open-domain grounded object detection model that we use to enforce vision guidance). However, prior work (e.g. LaBo, LM4CV, LF-CBM) also shared similar limitation on the reliance of large pre-trained models (e.g. CLIP). Nevertheless, it also means that our techniques have the potential to be further improved with the advancement of large pre-trained models. In the future, we plan to explore training CBL with even more vision guidance, such as using segmentation maps of concepts.

   Method & **VLG-CBM (Ours)** & LF-CBM & LM4CV & LaBo \\  \% changed decisions & **0.12\%** & 49.21\% & 98.34\% & 81.40\% \\   

Table 4: Portion of model predictions that changes after pruning. The results suggest that for existing methods (LF-CBM, LM4CV, LaBo) without NEC control, a large portion of predictions changes with top-5 concepts, implying potential risk when using top-5 contributions to explain model decisions.