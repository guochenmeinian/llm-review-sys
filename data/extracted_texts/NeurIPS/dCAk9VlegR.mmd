# This Looks Like Those: Illuminating Prototypical Concepts Using Multiple Visualizations

Chiyu Ma

Dartmouth College

chiyu.ma.gr@dartmouth.edu &Brandon Zhao1

Caltech

byzhao@caltech.edu &Chaofan Chen

University of Maine

chaofan.chen@maine.edu &Cynthia Rudin

Duke University

cynthia@cs.duke.edu

Denotes equal contribution

###### Abstract

We present ProtoConcepts, a method for interpretable image classification combining deep learning and case-based reasoning using prototypical parts. Existing work in prototype-based image classification uses a "this looks like that" reasoning process, which dissects a test image by finding prototypical parts and combining evidence from these prototypes to make a final classification. However, all of the existing prototypical part-based image classifiers provide only one-to-one comparisons, where a single training image patch serves as a prototype to compare with a part of our test image. With these single-image comparisons, it can often be difficult to identify the underlying concept being compared (e.g., "is it comparing the color or the shape?"). Our proposed method modifies the architecture of prototype-based networks to instead learn prototypical concepts which are visualized using multiple image patches. Having multiple visualizations of the same prototype allows us to more easily identify the concept captured by that prototype (e.g., "the test image and the related training patches are all the same shade of blue"), and allows our model to create richer, more interpretable visual explanations. Our experiments show that our "this looks like those" reasoning process can be applied as a modification to a wide range of existing prototypical image classification networks while achieving comparable accuracy on benchmark datasets.

## 1 Introduction

As machine learning models are increasingly adopted in high-impact, high-stakes domains such as healthcare , self-driving cars , facial recognition , etc., the design of human-interpretable models has become essential for ensuring accountability and transparency of their decisions. In particular, a class of models called prototype networks have combined the power of deep learning with the explainability of case-based reasoning to make accurate, human-interpretable decisions on fine-grained image classification tasks . These models make decisions by dissecting an image into informative feature patches, then comparing them to prototypical parts learned during training. Evidence of similarity to each prototypical part is then combined to make a final classification.

When evaluating the interpretability of an image classification network, it is important to consider not only the model's capability to explain its reasoning process, but also the quality of its explanations. The image features analyzed by prototype networks can encode a variety of visual properties, such as color, shape, texture, contrast, or saturation. Because each prototype in the existing prototype network corresponds exactly to a single patch of a single training image, it can be difficult for human users to understand the exact semantic content underlying a prototype's visualization. Although there has been some work in quantifying the importance of each of these feature categories in prototypes , the exact semantic content in an image patch can still remain ambiguous (e.g. "Texture isimportant in this prototype, but what kind of texture in particular?"). The ambiguity underlying prototype visualizations has been shown to cause a gap between visual explanations from prototype networks and human understanding of visual similarity . Additionally, a variety of prototype networks have been proposed in which prototypical parts from a single image can serve as evidence for many different classes, thus reducing the total number of prototypes needed for the network to make classifications [19; 26; 25]. The rationale behind this prototype sharing is that certain distinctive features (e.g. "long beak, solid black head, spotted belly") can be found in multiple classes, and thus do not need to be represented as distinct class-specific copies. However, while reducing the number of prototypes can simplify visual explanations by reducing the amount of considered evidence, these methods can often produce seemingly nonsensical explanations when a training patch from an image is used spuriously as evidence for a different class.

Inspired by the difficulty of identifying the meaning behind single-image prototypes, we propose ProtoConcepts, a novel modification to prototype geometry enabling visualizations of prototypical concepts from multiple training images. This allows human users to better understand the conceptual content of a prototype by observing the features shared among its visualizations. An example of a visual explanation produced by our ProtoConcepts is shown in Figure 1 (right). Unlike previous prototype networks where a prototype is visualized using a single training image patch (Figure 1, left), our ProtoConcepts visualizes each prototype with multiple visualizations. Because our model offers many visual examples for each prototype, the semantic meaning of each prototype can be determined with less ambiguity - we can look at the commonality between the visualized training image patches to infer the semantic meaning of each prototype. Our method can be applied to a wide range of prototype-based networks, resulting in better human interpretability while achieving comparable classification accuracy. Our code is available at https://github.com/Henrymachiyu/This-looks-like-those_ProtoConcepts

## 2 Related Work

Attempting to understand deep neural networks, _posthoc_ explanation techniques such as activation maximization [7; 21; 37; 38], image perturbation [8; 12], and saliency visualizations [30; 31; 27; 27] fail to explain a network's reasoning process, and their results can be risky and unreliable [18; 1]. On the other hand, prototype-based networks compare learned latent feature representations, called

Figure 1: Image of a Brown Thrasher and how the ProtoPool (left) and ProtoPool-Concepts (ours, right) explain their decisions. Prototype classifications are made by finding patches in the image similar to learned prototypical parts. Single-visualization methods such as ProtoPool can make visually ambiguous decisions when the semantic features underlying a prototype are unclear. Our method provides clearer explanations by providing multiple visualizations of prototypical concepts found in the test image.

prototypes, to the latent representations of an image to perform classification. These models are constrained to be intrinsically interpretable. The Prototypical Part Network (ProtoPNet)  uses class-specific prototypes, where a pre-determined number of prototypes are assigned to each class. Each prototype is trained to be similar to feature patches from images of its own class and dissimilar to patches from images of other classes. This gives such class-specific models high discriminative power suitable for fine-grained image classification. In the end, prototype similarity scores are aggregated as positive evidence for each class in a "scoresheet." The Transparent Embedding Space Network (TesNet)  improved on the class-specific ProtoPNet architecture by transforming the latent space geometry into a hypersphere, and representing prototypes as near-orthogonal basis vectors specific to a class subspace in the Grassmannian manifold.

Other prototype-based networks found that abandoning class-specific prototype constraints allowed for a significant reduction in the number of prototypes, making explanations simpler and allowing prototypes to represent similar visual concepts shared in images of different classes. In , a ProtoPNet is first trained with enforced class specificity, and then a novel data-based similarity metric is used to "merge" highly similar prototypes across classes into a single prototype representing multiple classes. In Neural Prototype Trees (ProtoTree) , the "scoresheet" of the ProtoPNet is replaced by a binary decision tree, in which the scored presence or absence of a prototype in each node determines the traversal path of the tree. Thus, each prototype in the tree represents either positive or negative evidence for the classes at the leaf nodes of the subtree for which the prototype is in the root node. Finally, in the ProtoPool network , the soft assignment of prototypes to classes is learned throughout training, and is eventually transformed into a hard assignment at test time.

Our work also relates to works that try to understand the semantic meaning behind a learned prototype of a prototype-based network. For example, in , Nauta et al. developed a method to quantify the importance of various visual characteristics relevant to each prototype. Our approach is complementary to, yet differs from theirs in that we illuminate the concept behind each prototype by presenting multiple visualizations of the same prototype, thereby allowing one to better infer the semantic meaning behind each prototype.

Finally, our work is related to posthoc methods in concept visualization such as TCAV  and ACE . _Posthoc_ methods do not show any part of the model's actual reasoning process. Worse, their visual explanations may not be faithful to the model's reasoning process. Sometimes, multiple concepts have the same concept vector [as discussed in 5], and concepts may not be clustered in the latent space (since they are not trained to be that way), leading to meaningless concept vectors. Instead, our method is inherently interpretable, and derives its visual concept-based explanations directly from the decision-making process.

## 3 Methods

We begin with a brief general review of traditional prototype-based models, then propose our method, which represents prototypes as interpretable concepts with multiple visual explanations. We show how our method can be applied as a modification to existing prototype-based networks, and introduce a novel training algorithm to learn meaningful prototypical concepts. Details for the implementation of specific prototype-based classification models are in the Experiments section.

### Prototype Architectures

Existing prototype-based networks [3; 19; 25; 26; 36] can be understood architecturally as the combination of three components. First is a convolutional neural network (CNN) \(f\) parameterized by weights \(w_{}\), which acts as a latent feature extractor on input images \(\). Next the prototype layer \(p\) considers the convolutional output \(f()\) as \(\) "patch" vectors \(z_{i}^{D}\), each of which corresponds to features from a patch of the original image space. Each patch is compared to \(m\) learned prototype vectors \(=\{_{j}\}_{j=1}^{m}\), where each prototype \(_{j}^{D}\) lies in the same space as the image's latent feature patches. The prototype layer \(p\) calculates a similarity score \(g_{_{j}} f()\) for each prototype \(_{j}\) which is monotonically decreasing with respect to the distance between \(_{j}\) and the closest image patch \(} f()\) in the model's feature space. Finally, the prototype layer \(g_{}\) is followed by a prototype class assignment mechanism \(h\) which assigns evidence logits to each class based on the calculated prototype similarity values \(g_{_{j}} f()\) and class assignment weights \(w_{h}\). The evidence logits are then normalized by a softmax to yield predicted probabilities for a given image belonging to each class.

In the final model, each prototype vector \(_{j}\) is restricted so that it is equal to a latent patch \(} f(_{i})\) from some training image \(_{i}\). Thus, the model's reasoning process can be interpreted visually as an evidence scoresheet of comparisons between test image patches and salient features underlying previously-seen training image patches. We propose to modify the geometry of prototypes \(_{j}\) by instead representing them as a _ball_ in the latent space encompassing _multiple_ training image patches. Mathematically, each prototype is a ball \(_{j}=B(_{j},r_{j})\) around a central vector \(_{j}^{D}\) with radius \(r_{j}^{+}\). This ball representation has the property that the similarity scores of any latent patch vector \(_{i}^{D}\) and any vector \(_{j}_{j}\) in the prototypical ball are constrained to be the same. Hence, with this design, as long as a prototypical ball \(_{j}\) contains a number of latent training patches, we can visualize the prototype \(_{j}\) by visualizing any of those latent training patches it contains. This set representation allows our prototypes to represent concepts found in all latent training patches contained within a prototypical ball, thereby providing richer semantic explanations than that of visualizing just a single latent training patch. Implementing our concept geometry on a prototype-based model entails minor changes to the architecture and loss; model-specific implementation details are described in Sec. 4.1.1.

### Prototypical Concept Representations

The prototype layer \(g_{_{j}}\) of previous prototype-based classification models uses one of two activation functions for calculating similarity scores between prototypes and image patches: a log-based activation on inverse \(^{2}\) distance, and a cosine similarity score. In this section, we formalize our ball representation of prototypical concepts with respect to the two corresponding latent spaces.

Log-based Activation:Models such as ProtoPNet  and ProtoPool  use a log-based function \(g^{}\) to compute a similarity score, which is monotonically decreasing with respect to the \(^{2}\) distance. In particular, for the latent representation \(=f()\) of an input image \(\), this function computes \(g^{}_{_{j}}()=_{}(})}((\|}-p_{j}\|_{2}^{2}+1)/(\| }-p_{j}\|_{2}^{2}+))\) with a small constant \(\) to avoid numerical overflow. For prototypical concepts, we propose an analogous similarity function

Figure 2: Architecture of ProtoConcepts. The convolutional layer includes all types of architectures, including ResNet, VGG and DenseNet. Evidence Layer \(h\) consists of a prototype class assignment mechanism and a fully connected layer. Depending on the specific prototype-based architecture, TesNet and ProtoPNet would have a one-to-one prototype class assignment mechanism, while ProtoPool and ProtoPShare would have a shareable prototype mechanism among the classes.

corresponding to the distance to center \(_{j}\) of prototypical ball \(_{j}=B(_{j},r_{j})\), bounded below by \(r_{j}\):

\[_{_{j}}^{}()=_{} ()}((}- _{j}\|_{2}^{2}+1}{\|}-_{j}\|_{2}^{2}+ }),(+1}{r_{j}+})).\] (1)

Cosine Similarity:Models such as TesNet  and Deformable ProtoPNet  have found that representing prototypes as unit vectors on a hypersphere can improve the accuracy of prototype-based models. In these models, the prototype layer compares prototypes to latent image patches via the cosine similarity function \(g_{_{j}}^{}()=_{}()}}^{T}_{j}/(\|}\|_{2} \|_{j}\|_{2})\). This cosine similarity can be interpreted as a cosine activation function on the angular distance between normalized vectors on the hypersphere. For models with a hypersphere-based latent space, we replace the cosine similarity function \(g_{_{j}}^{}\) by calculating the cosine similarity to the ball \(B(_{j},r_{j})\) on the hypersphere :

\[_{_{j}}^{}()=_{} ()}(}^{T} _{j}}{\|}\|_{2}\|_{j}\|_{2}},(r_{j})).\] (2)

Both Eqs. (1) and (2) ensure that for any patch vector \(_{i}^{D}\), if \(_{i}\) lies outside the ball \(_{j}\), its distance to \(_{j}\) is the distance to its center \(_{j}\), and if \(_{i}\) lies inside \(_{j}\), its distance to \(_{j}\) is kept the same as the radius of the ball. During training, a pass-through estimator of each activation function is used to avoid a zero gradient when training patches lie within the prototypical ball.

### Training Algorithm

The training of prototype-based networks is typically divided into three stages: (1) stochastic gradient descent (SGD) of layers before the last layer; (2) projection of prototypes; (3) optimization of the last layer \(h\). Because of the ball representation of prototypical concepts, the second stage can be skipped as prototypical balls are visualized by the training patches they contain at the end of the first step.

SGD of layers before last layer:The first training stage prototype-based networks aims to learn a meaningful latent space that clusters important training patches near semantically similar prototypes. To do this, a joint optimization problem is solved to optimize the network parameters using SGD while keeping the last layer weight matrix fixed. To learn semantically meaningful prototypical concepts, we introduce two novel loss terms, \(_{}\) and \(_{}\):

\[_{}=_{i=1}^{n}_{j=1}^{k}_{ _{j}_{y_{i}}\\ _{j}_{1},,_{j-1}}_{ }(f(_{i}))\\ _{j}=(_{j},r_{j})}d(},_{j}),\;_{}=_{_{j}\\ _{j}=(_{j},r_{j})}r_{j}^{2}.\] (3)

Here \(d\) is used as the distance function corresponding to the respective latent space geometry of the ProtoConcepts module. The minimization of \(_{}\) encourages \(k\) prototypical concepts of the correct class to be close to at least one latent patch of each training image. The specific value of \(k\) is treated as a hyperparameter and is chosen by cross-validation. As opposed to the traditional prototype cluster loss , which only penalizes the distance of the closest prototype to a latent training patch, we find that our top-\(k\) formulation is essential for ensuring that prototypical concept balls contain multiple training patches. The minimization of \(_{}\) encourages prototypical concept balls to be compact. This allows learned prototypical concepts to encapsulate more specific semantic features important for classification.

Prototypical Concept Visualisation:In previous prototype-based networks, the trained prototype vector \(p_{j}\) is projected (pushed) to the nearest training patch to produce a single patch visualization. However, since we use a set representation for prototypical concepts, it is no longer reasonable to project the prototypical ball or its central vector to the training patches. Instead, we visualize each prototypical concept by the training patches that are contained within each ball at the end of training. Thus, we can produce multiple visualizations from each prototypical concept and skip the projection step altogether.

Pruning and Fine-tuning:Because the number of prototypical concepts is fixed at the beginning of training, it is possible for a trained ProtoConcepts model to have a small number of prototypical concepts that do not contain any latent training image patches. In other words, these prototypical concepts do not capture any visualizable latent features, and should not be considered when making predictions. To prune the non-visualized prototypical concepts, we design a 0/1 mask on the prototype class assignment mechanism so that the given non-visualized prototypical concepts do not provide evidence for any image classes. After pruning these prototypical concepts, we finally perform a sparse convex optimization on the last layer weight matrix \(w_{h}\) as in previous prototype-based methods .

## 4 Experiments

### Case Study 1: Bird Species Identification

To demonstrate and examine the effectiveness of our method, we implement the ProtoConcepts module on the ProtoPNet, ProtoPool, and TesNet networks for the Caltech-UCSD Birds-200-2011 (CUB 200-2011) dataset . The dataset contains 5,994/5,794 images for training and testing across 200 different bird species. We perform offline data augmentation, and crop training and testing images using bounding boxes provided with the dataset.

#### 4.1.1 Implementation

In this section, we describe the implementation changes made to each prototype-based model for our experiments to incorporate prototypical concepts. A full list of exact parameter settings and training schedule for each model can be found in the Appendix.

ProtoPNet:The ProtoPNet model  consists of a CNN backbone, a log-based prototype layer \(g^{}\), and a fully connected layer \(h\). Each of 200 classes is assigned 10 class-specific prototypes, which are constrained to provide evidence only for their corresponding class through the last layer weighting. We train a ProtoPNet-Concepts model by substituting the prototype layer as in Eq. (1), and then training with additional losses \(_{}\) and \(_{}\) with \(k=10\), radius initialization \(7.5\), and loss weights \(0.01\) and \(0.8\), respectively. The training schedule and other hyperparameters are unchanged from the original paper.

TesNet:The TesNet model  uses a cosine similarity function in its prototype layer to represent the latent space as a hypersphere and incorporates several hypersphere-based loss functions during training. After modifying the activation function as in Eq. (2), we train TesNet-Concepts with a radius initialization of \(8.05\), and a top-\(k\) cluster loss \(_{}\) with \(k=3\). We set the weight for the radius loss as \(3 10^{-5}\) and the weight for top-\(k\) cluster loss as \(0.8\) with a learning rate of \(1 10^{-4}\) during the warmup stage, and reweight the loss functions described in its original paper with a new training schedule.

ProtoPool:The ProtoPool architecture  employs a log-based activation on the prototype layer, and adds a prototype class assignment mechanism that allows sharing prototypes among multiple classes, thereby reducing the number of prototypes. After modifying the prototype layer as in Eq. (1), we train ProtoPool-Concepts using a radius initialization of \(4.5\) and top-\(k\) cluster loss \(_{}\) with \(k=10\). We set a radius loss weight \(3 10^{-3}\), a top-\(k\) cluster weight \(0.8\), and learning rate for the radius as \(0.5 10^{-3}\). The training schedule and other hyperparameters are left unchanged from the original paper.

For baseline comparisons, we retrain CNN backbones and previous prototype-based models using publicly available code. As shown in Table 1, for the same CNN architecture, our ProtoConcepts network achieves similar test accuracy to corresponding baseline models.

Radius Study:We performed an ablation on ProtoPool-Concepts with a different set of radius initialization and top-\(k\) cluster loss with \(k=10\) under the same training settings described above. Intuitively, a larger radius would encourage more prototypical concepts to capture information and an infinitesimal radius would be equivalent to a single visualization. As shown in Table 2, the number of visualizable prototypical concepts increases with the radius. In addition, radius also influences the accuracy of our ProtoConcepts model after pruning and fine-tuning. If the radius is too small, the number of visualizable prototypical concepts also tends to be small - this means that pruning will remove too many prototypes, and consequently adversely impact performance. On the other hand, if the radius is too large, each prototypical ball would capture too many latent representations whose semantic meanings may be too diverse, thereby making the prototypes less meaningful. Hence, we need to tune the radius to be within a reasonable range.

#### 4.1.2 Interpretability

The reasoning process of a ProtoConcepts network mirrors that of a traditional prototype network, but provides multiple visualizations of the prototypes used in its explanations. Figure 3 shows how our model classifies a testing image of a Baltimore Oriole into the correct class. It is worth noting that because of the shareable prototypes mechanism inherited from ProtoPool, the visualizations of a learned prototypical concept now come from different classes. The most representative features that an ornithologist would use to classify an adult male Baltimore Oriole are the bright orange and black plumage and the slender beak . As shown in Figure 3, our model is able to compare the test image patch containing orange and black plumage with the learned prototypical concept that captures similar features from its own class. Additionally, our model compares the patch containing the wings to the prototypical concept shared among Baltimore Orioles and other similar birds such as Orchard Orioles. Lastly, our model compares the part of image that contains a slender beak along with its black head to the prototypical concept that captures the slender beak and black head of birds such as Orchard Orioles and Eastern Towhees.

Figure 1 shows an example of how a test image of a Brown Thrasher is correctly classified by ProtoPool and ProtoPool-Concepts. Because of the prototype sharing mechanism, it is frequently the case that training image patches from birds of one class are used as evidence of a test image belonging to another class. As a result, the similarities presented by ProtoPool's visual explanations can be unintuitive. As shown in the plot, it is unclear how ProtoPool found the head of a Brown Thrasher to be similar to the blue head of a Barn Swallow. Moreover, it is hard to determine whether the model is comparing the background or the belly of the Brown Thrasher, as shown at the bottom. However, comparisons made by our method are much clearer thanks to multi-visualizations. Although both models attempt to compare the lower part of the bird to the learned prototypes, it is clear that our ProtoPool-Concepts is looking at the pattern on the belly, which is an identifying feature of the Brown Thrasher .

  Arch. & Model & Prototype \(p\)\(\#\) & Acc.[\(\%\)] \\   & **ProtoPNet-Concepts (ours)** & 1993 \(\) 3 & 77.9 \(\) 0.2 \\  & ProtoPNet  & 2000 & 78.0 \(\) 0.2 \\  & **TestNet-Concepts (ours)** & 1862 \(\) 20 & 78.2 \(\) 0.8 \\  & TesNet  & 2000 & 77.9 \(\) 0.1 \\  & Baseline (given in ) & N/A & 75.1 \(\) 0.4 \\   & **ProtoPNet-Concepts (ours)** & 1980 \(\) 2 & 80.7 \(\) 0.4 \\  & ProtoPNet  & 2000 & 80.1 \(\) 0.3 \\  & **TestNet-Concepts (ours)** & 1994 \(\) 2 & 81.4 \(\) 0.3 \\  & TesNet  & 2000 & 81.5 \(\) 0.3 \\  & **ProtoPool-Concepts (ours)** & 199 \(\) 1 & 81.5 \(\) 0.6 \\  & Protopool  & 202 & 80.3 \(\) 0.3 \\  & Baseline (given in ) & N/A & 82.2 \(\) 0.2 \\   & **Test-Concepts (ours)** & 1957 \(\) 10 & 79.8 \(\) 0.4 \\  & TesNet  & 2000 & 80.7 \(\) 0.3 \\  & **ProtoPool-Concepts (ours)** & 185 \(\)4 & 80.4 \(\) 0.1 \\  & ProtoPool  & 202 & 80.3 \(\) 0.2 \\   & **ProtoPool-Concepts (ours)** & 188 \(\) 2 & 85.2 \(\) 0.2 \\  & ProtoPool  & 202 & 85.5 \(\) 0.1 \\  & ProtoTree  & 202 & 82.2 \(\) 0.7 \\   & **ProtoPNet-Concepts (ours)** & 1972 \(\) 10 & 78.0 \(\) 0.1 \\  & ProtoPNet  & 2000 & 78.0 \(\) 0.3 \\   & Baseline (given in ) & N/A & 81.5 \(\) 0.4 \\  

Table 1: Comparison of ProtoConcepts implementation on ProtoPNet, ProtoPool, TesNet with the corresponding baselines and various backbone architectures. All algorithms on the same architecture perform similarly; the advantage is instead in interpretability.

Figure 4 shows examples of how the two models correctly classify a female and a male ruby-throated hummingbird. In the top row, both of the models found the neck of a female ruby-throated hummingbird from the given test image similar to the white feathers around neck from the training examples. On the other hand, although having a red throat differentiates the male ruby-throated hummingbird from the female , the significance of a red-throat to correctly classify a male ruby-throated hummingbird is unclear. As shown in the bottom left of Figure 4, the learned prototype from ProtoPNet is able to compare the prototype that contains a red throat to that of the test image. However, the visualizations from ProtoPNet-Concepts demonstrate that a red throat may not be all that the prototypical concept is representing. It appears to represent broader information about the bird's gray head and the transition to its body, where the additional red or gray color is located on the throat, and the white on the breast.

Figure 4: Reasoning of a correctly classified Female Ruby Throated Hummingbird (top) and Male Ruby Throated Hummingbird (bottom) by ProtoPNet (left) and ProtoPNet-Concepts (right).

Figure 3: The reasoning process of our network in classifying a test image as a Baltimore Oriole. Test image patches are compared to prototypical concepts in latent space, then visual similarity scores are compiled as evidence for each class.

### Case Study 2: Car Model Identification

In this case study, we implement our method for the Stanford Cars dataset  of 196 car types, using similar procedures and training algorithms as in the CUB-2011-200 experiment. We trained a ProtoPool-Concepts model with radius initialization \(4.5\), keeping other parameter settings the same as described in Sec. 4.1.1. We compare our method to self-reported accuracies of other shared prototype models ProtoPool  and ProtoPShare . Our model achieves similar test accuracy to other shared prototype models, with the advantage of multiple visualizations for prototypical concepts. More details, including visual explanations, can be found in the Appendix.

## 5 User Study

To show the reduction of ambiguity and resulting improvement in user interpretability, we created a distinction user study similar to HIVE  to compare our ProtoConcepts method with ProtoPNet; results are shown in Table 4. We randomly picked ten samples from the test set and calculated the top two predicted classes (i.e., the classes with the highest predicted probabilities according to the model) for each test sample. We then provided visual explanations from the most actively prototypes for these classes by ProtoPNet and ProtoPNet-Concepts. A test-taker was then asked to choose which class the model is actually predicting, looking only at the visual explanations without the class probabilities. Examples of our user study are shown in Figure 7. We released our user study on Amazon Mechanical Turk and collected 50 responses from test takers with a 98\(\%\) survey approval rate to ensure the quality of responses, and removed 1 response from both surveys after screening for nonsensical free response answers. We first ran a two-sided \(t\)-test on self-rated ML experience for the test takers from the ProtoPNet and ProtoPNet-Concept. The \(p\)-value is 1, and we are assured that there is no statistically significant difference in machine learning experience between the two groups on average. From the results of our survey, we are able to conclude that users with visual explanations from ProtoConcepts outperform those with visual explanations from ProtoPNet to a statistically significant degree \((p=0.003)\). Moreover, users given visual explanations from ProtoPNet were unable to outperform \(50\%\) random guessing to a statistically significant degree \((p=0.289)\), whereas users given visual explanations from our model did \((p=2.85 10^{-5})\). Our survey results show not only that our model provides a notable improvement in user interpretability, but is able to improve non-expert user performance in a difficult fine-grained classification task whereas the previous ProtoPNet model cannot.

## 6 Limitations

The interpretability of prototype-based models comes from _visual_ explanations of classification decisions. Hence, the exact semantics (e.g., "this bird has a long beak" or "this bird has a spotted belly") underlying these visual explanations depend on the user's visual system to determine. We present a method for clarifying the user's semantic inference by offering multiple visualizations whose

  Radius & Acc.\([\%]\) before pruning & Acc.\([\%]\) after pruning \& finetuning & Prototype \(p\)\(\#\) \\ 
4.0 & 85.4 \(\) 0.1 & 81.5 \(\) 1.0 & 61 \(\) 6 \\
4.5 & 85.2 \(\) 0.1 & 85.2 \(\) 0.2 & 188 \(\) 2 \\
5.0 & 85.2 \(\) 0.3 & 85.0 \(\) 0.3 & 202 \\  

Table 2: Performance of ProtoPool-Concepts with different radius initializations using ResNet-50(iNat) backbone

  Arch. & Model & Prototype \(p\)\(\#\) & Acc.\([\%]\) \\   & **ProtoConcepts (ours)** & 150 \(\) 6 & 89.4 \(\) 0.5 \\  & ProtoPShare  & 480 & 86.4 \\  & Protopool  & 195 & 89.3 \(\) 0.1 \\  Densenet121  & **ProtoConcepts (ours)** & 194 \(\) 1 & 87.1 \(\) 0.4 \\  & Protopool  & 195 & 86.4 \(\) 0.1 \\  

Table 3: Accuracy comparison of ProtoPool-Concepts to other shared prototype models on the Stanford Cars datasetcommonalities can disambiguate our model's exact reasoning. While we find that our concept-based representation is helpful for this process, it is still unable to provide explicit semantics to explain its classifications. With recent progress in large vision-language hybrid models [22; 16; 4; 34], a technique offering explicit semantics for visual explanations could be explored in future work. It is important to note that in some visual domains, concepts may not have natural textual descriptions (e.g., in mammography or other radiology applications where it might look at a patch of a particular type of breast tissue that is difficult to describe and has no associated terminology).

## 7 Broader Impact

Interpretability is an essential ingredient to trustworthy AI systems . Our work presents a major improvement in interpretability to prototype-based networks, which are one of the leading techniques for interpretable neural networks in computer vision. Thus, our technique can be used for important computer vision applications to discover new knowledge and create better human-AI interfaces for difficult, high-impact applications.

## 8 Conclusion

In this work, we present an interpretable method for image classification which incorporates prototypical concepts with multiple visualizations to explain its predictions (_this_ looks like _those_). Unlike previous works in prototype-based classification which offer only single prototype visualizations, our method allows the user to use commonalities between visualizations of prototypical concepts to better infer the semantic meaning of prototypical concepts. We compare the visual explanations offered by our and previous prototype-based methods and show that our model can achieve comparable accuracy to previous methods.

  Model & Mean Acc.[\(\%\)]\(\)1.96 Std. & p-value (1) & p-value (2) \\ 
**ProtoConcepts (Ours)** & **62.1 \(\) 5.4** & \(0.003\) & \(2.85 10^{-5}\) \\ ProtoPNet  & 51.5 \(\) 5.2 & & \(0.288\) \\  

Table 4: Results of the user study comparing ProtoPNet to ProtoConcepts. We report the mean user accuracy for each model with a range of plus/minus 1.96 standard deviations. In addition, we conduct two 1-sided t-tests on the results, with the alternative hypotheses that (1) users with ProtoConceptsâ€™ visual explanations score higher on average than with ProtoPNet and (2) Each model, respectively, achieves a higher accuracy than random guessing. We conclude that users presented with visual explanations from ProtoConcepts outperform those with explanations from ProtoPNet to a statistically significant degree.

Figure 5: User study example questions with architecture ProtoPNet (left), and ProtoConcepts (right). We showed each user 2 possible class explanations for a given test image and asked the user to pick the correct class. We compared the results from two separate surveys, one in which visual explanations are generated by ProtoPNet, and one in which they are generated by ProtoConcepts.