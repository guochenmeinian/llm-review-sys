# Multi-Agent Imitation Learning:

Value is Easy, Regret is Hard

 Jingwu Tang

Carnegie Mellon University

jingwutang@cmu.edu

&Gokul Swamy

Carnegie Mellon University

gswamy@cmu.edu

&Fei Fang

Carnegie Mellon University

feifang@cmu.edu

&Zhiwei Steven Wu

Carnegie Mellon University

zstevenwu@cmu.edu

###### Abstract

We study a multi-agent imitation learning (MAIL) problem where we take the perspective of a learner attempting to _coordinate_ a group of agents based on demonstrations of an expert doing so. Most prior work in MAIL essentially reduces the problem to matching the behavior of the expert _within_ the support of the demonstrations. While doing so is sufficient to drive the _value gap_ between the learner and the expert to zero under the assumption that agents are non-strategic, it does not guarantee robustness to deviations by strategic agents. Intuitively, this is because strategic deviations can depend on a counterfactual quantity: the coordinator's recommendations outside of the state distribution their recommendations induce. In response, we initiate the study of an alternative objective for MAIL in Markov Games we term the _regret gap_ that explicitly accounts for potential deviations by agents in the group. We first perform an in-depth exploration of the relationship between the value and regret gaps. First, we show that while the value gap can be efficiently minimized via a direct extension of single-agent IL algorithms, even _value equivalence_ can lead to an arbitrarily large regret gap. This implies that achieving regret equivalence is harder than achieving value equivalence in MAIL. We then provide a pair of efficient reductions to no-regret online convex optimization that are capable of minimizing the regret gap _(a)_ under a coverage assumption on the expert (MALICE) or _(b)_ with access to a queryable expert (BLADES).

## 1 Introduction

We consider the problem of a _mediator_ learning to _coordinate_ a group of strategic agents via _recommendations_ of actions to take without knowledge of their underlying utility functions (e.g. routing a group of drivers through a road network). Given the difficulty of manually specifying the quality of a recommendation in such situations, it is natural to provide the mediator with data of desired coordination behavior, turning our problem into one of _multi-agent imitation learning_ (MAIL, ). In our work, we explore the nuances of a fundamental MAIL question:

_What is the right objective for the learner in a multi-agent imitation learning problem?_

We can begin to answer this question by exploring the following scenario: consider developing a routing application to provide personalized route recommendations (\(\)) to a group of users with joint policy \(\) (e.g. the routing policy that underlies the recommendations provided in Google Maps ). As usual in imitation learning (IL), we assume we are given access to _demonstrations_ from an _expert_\(_{E}\) (e.g. a past iteration of the application). We can imagine two kinds of users of our application (i.e.

agents_): _non-strategic_ users who blindly follow the recommendations of our routing application and _strategic_ users who will _deviate_ from our recommendations if they have the incentive to do so under their (unknown) personal utility function (e.g. we recommend a long route to a busy driver). We use \(J_{i}(_{})\) below to denote the value of the mediator's learned policy \(\) under the \(i\)th agent's utility.

**Case 1: No Strategic Agents.** In the idealized situation where all agents in the population are perfectly obetient, we can essentially treat a MAIL problem as a _single-agent_ IL (SAIL) problem over joint policies. It is therefore natural to use a direct extension of the well-studied _value gap_ criterion from the SAIL literature  to the multi-agent setting:

\[_{i[m]}J_{i}(_{_{E}})-J_{i}(_{}).\]

Intuitively, driving the value gap to 0 (i.e. achieving _value equivalence_ in the terminology of ) implies that along as long as all agents blindly follow our recommendations, we have learned a policy that performs at least as well as that of the expert from the perspective of _any_ agent in the population. In our running routing application example, this means that if no driver deviates from the previous behavior, all drivers will be at least as happy as they were with the prior iteration of the application.

**Case 2: Strategic Agents.** Of course for any MAIL problem where agents actually have _agency_, we need to account for the fact that agents may deviate from our recommendations if it appears beneficial to do so from their subjective perspective. Let us denote the class of deviations (i.e. policy modifications) for agent \(i\) as \(_{i}\). Then, we can define the _regret_ induced by the mediator's policy as

\[_{}():=_{i[m]}_{_{i}_{i}}(J_{i}( _{,_{i}})-J_{i}(_{})),\]

where \(_{i}\) is a strategic deviation of agent \(i\) and \(_{,_{i}}\) is the _joint agent policy induced by all agents other than \(i\) following \(\)'s recommendations_. Intuitively, regret captures the maximum incentive any agent in the population has to deviate from the mediator's recommendations. We can then compare this metric between the expert and learner policies to arrive at the notion of a _regret gap_:

\[_{}()-_{}(_{E}).\]

Driving the regret gap to zero (i.e. achieving _regret equivalence_) implies that _even if agents are free to deviate, our learned policy is at least as good as the expert's from the perspective of an arbitrary agent in the population_. In our preceding example, this means that despite the fact that they are not forced to follow our application's recommendations, all agents would have no more incentive to take an alternate route than they did under the previous iteration of the application.

A simple decomposition allows us to show that a small value gap does not in general imply a small regret gap. Consider the performance difference between the learner's policy under all obetient (\(J_{i}(_{})\)) and a deviating \(i\)th agent (\(J_{i}(_{,_{i}})\)). We can decompose this quantity into the following:

\[J_{i}(_{,_{i}})-J_{i}(_{})=(_{ ,_{i}})-J_{i}(_{_{E},_{i}}))}_{$)}}+(_{_{E},_{i}}))-J_{i}(_{ _{E}})}_{$)}}+(_{_{E}})-J_{i}(_{ }))}_{},\]

where we use \(_{_{E},_{i}}\) to denote agent joint behavior under expert recommendations and deviation \(_{i}\). Term III is the standard single-agent value gap (i.e. the performance difference under the assumption that no agents deviate). Term II is the expert's regret under deviation \(_{i}\) (i.e. a quantity we cannot control). Thus, the difference between the regret gap and value gap objectives can be boiled down to Term I: \(J_{i}(_{,_{i}})-J_{i}(_{_{E},_{i}})\). Observe that because of the state distribution shift induced by deviation \(_{i}\), minimizing Term III doesn't give us any guarantees with respect to Term 1. This underlies our key insight: _regret is hard in MAIL as it requires knowing what the expert would have done in response to an arbitrary agent deviation_. More explicitly, our contributions are three-fold:

**1. We initiate the study of the _regret gap_ for MAIL in Markov Games.** Unlike the value gap - the standard objective in single-agent IL - the regret gap captures the fact that agents in the population may choose to deviate from the mediator's recommendations. The shift from value to regret gap captures what is fundamentally different about the SAIL and the MAIL problems.

**2. We investigate the relationship between regret gap and the value gap.** We show that under the assumption of complete reward and deviation function classes, regret equivalence implies value equivalence. However, we also prove that value equivalence provides essentially no guarantees on the regret gap, establishing a fundamental limitation of applying SAIL algorithms to MAIL problems.

**3. We provide a pair of efficient algorithms to minimize the regret gap under certain assumptions.** While regret equivalence is hard to achieve in general as it depends on counter-factual expert recommendations, we derive a pair of efficient reductions for minimizing the regret gap that operate under different assumptions: MALICE (which operates under a coverage assumption) and BLADES (which requires access to a queryable expert). We prove that both algorithms can provide \(O(H)\) bounds on the regret gap, where \(H\) is the horizon, matching the strongest known results for the value gap in single-agent IL. See Table 1 for a summary of our regret gap bounds.

## 2 Related Work

**Single-Agent Imitation Learning.** Much of the theory of imitation learning focuses on the single-agent setting . Offline approaches like behavioral cloning (BC, ) reduce the problem of imitation to mere supervised learning. Ignoring the covariate shift in state distributions between the expert and learner policies can cause _compounding errors_[17; 21] and associated poor performance. In response, interactive IL approaches like inverse reinforcement learning (IRL, [1; 28]) allow the learner to observe the consequences of their actions during the training procedure, preventing compounding errors . However, such approaches can be rather sample-inefficient due to the need to repeatedly solve a hard RL problem [25; 16]. Alternative approaches include interactively querying the expert to get action labels on the learner's induced state distribution (DAgger, ) or, assuming full coverage of the demonstrations, using importance weighting to correct for the covariate shift (ALICE, ). Our BLADES and MALICE algorithms can be seen as the regret gap analog of the value gap-centric DAgger and ALICE algorithms, operating under the same assumptions.

**Multi-Agent Imitation Learning.** The concept of the regret gap was first introduced in the exceptional work of Waugh et al. , though their exploration was limited to Normal Form Games (NFGs), in contrast to the more general Markov Games (MGs) we focus on. Fu et al.  briefly consider the regret gap in Markov Games (MGs) but do not explore its properties nor provide algorithms for efficient minimization. Most empirical MAIL work [19; 12; 4; 26; 11] is value gap-based, while we take a step back and ask what the right objective is for MAIL in the first place.

**Inverse Game Theory.** Another line of work focuses on inverse game theory in Markov Games [13; 8], where the goal is to recover a set of utility functions that rationalize the observed agent behavior, rather than learning to coordinate from demonstrations. A detailed comparison between the goals of our work at that of inverse game theory provided in Appendix F.

## 3 Preliminaries

We begin with the notation we will use in our paper. Throughout, we use \((X)\) denote the space of probability distribution over a set \(\). We will use \(\) to denote the loss function each algorithm optimizes, which should be thought of as a convex upper bound on the total variation distance \(\). We use \(_{}\) when the loss function is exactly the TV distance.

**Markov Games.** We use \(MG(H,,,,\{r_{i}\}_{i=1}^{m},_{0})\) to denote a _Markov Game_ (MG) between \(m\) agents. Here, \(H\) is the horizon, \(\) is the state space, and \(=_{1}..._{m}\) is the joint action space for all agents. We use \(:()\) to denote the transition function. Furthermore, the reward (utility) function for agent \(i[m]\) is denoted by \(r_{i}:[-1,1]\). Lastly, we use \(_{0}\) to denote the initial state distribution from which the initial state \(s_{0}_{0}\) is sampled.

   & Assumption & Upper Bound & (Matching) Lower Bound \\  J-BC & \(\)-Coverage & \(O( uH)\), Theorem 5.1 & \(( uH)\), Theorem 5.2 \\  J-IRL & \(\)-Coverage & \(O( uH)\), Theorem 5.3 & \(( uH)\), Corollary 5.4 \\  MALICE (ours) & \(\)-Coverage & \(O( uH)\), Theorem 5.5 & \(( uH)\), Theorem 5.6 \\  BLADES (ours) & Queryable Expert & \(O( uH)\), Theorem 5.7 & \(( uH)\), Theorem 5.8 \\  

Table 1: A summary of our results: upper and lower bounds on the regret gap (i.e. \(_{}()-_{}(_{E})\)) of various approaches to multi-agent IL. Here, \(\) is the coverage constant in Assumption 5.2, \(u\) is the recoverability constant in Assumption 5.1, \(H\) is the horizon.

**Learning to Coordinate.** Rather than considering the problem of learning individual agent policies in the MG, we take the perspective of a _mediator_ who is giving recommendations to each agent to help them coordinate their behavior (e.g. a smartphone mapping application providing directions to a set of users). At each time step, the mediator gives each agent \(i\) a private _action recommendation_\(a_{i}\) to take at the current state \(s\). Critically, no agent observes the recommendations the mediator provides to another agent. We can represent the mediator as a Markovian joint policy \(\), where \(:()\). We use \((|s)\) to denote the probability of recommending joint action \(\) in state \(s\). We use \(:()\) to denote the joint policy that agents play in response to the mediator's policy. When agents exactly follow the mediator's recommendations, we denote their joint policy as \(_{}\).

A trajectory \(=\{s_{h},_{h}\}_{h=1,...,H}\) refers to a sequence of state-action pairs generated by starting from \(s_{0}_{0}\) and repeatedly sampling joint action \(_{h}\) and next states \(s_{h+1}\) from \(\) and \(\) for \(H-1\) time steps. Let \(d^{}_{h}\) denote the _state visitation distribution_ at timestep \(h\) following \(\) and let \(d^{}=_{h=1}^{H}d^{}_{h}\) be the average state distribution. Let \(^{}_{h}(s_{h},_{h})\) denote the _occupancy measure_ - i.e., probability of reaching state \(s\) and then taking action \(\) at time step \(h\). By definition, we know that \(_{h},_{s,}^{}_{h}(s,)=1\). Let \(^{}(s,)=_{h=1}^{H}^{}_{h}(s,)\) be the average occupancy measure.

We use \(V^{}_{i,h}\) to denote the expected cumulative reward of agent \(i\) under this policy from time step \(h\), i.e. \(V^{}_{i,h}(s)=_{}[_{t=h}^{H}r_{i}(s_{t},_{t} )|s_{h}=s]\). We define Q-value function of agent \(i\) as \(Q^{}_{i,h}(s,)=_{}[_{t=h}^{H}r_{i}(s_{t}, _{t})|s_{h}=s,_{h}=]\). We define advantage of an agent \(i\) to be the difference between its Q-value on a selected action and the V-value on the state, i.e. \(A^{}_{i,h}(s,)=Q^{}_{i,h}(s,)-V^{}_{i,h}(s)\). We also define the performance of a policy \(\) from the perspective of agent \(i\) as \(J_{i}()=_{s_{0}_{0}}[_{}[_{t=1}^ {H}r_{i}(s_{t},_{t})|s=s_{0}]]\). Observe that performance is the inner product between the occupancy measure and the agent's reward function, i.e. \(J_{i}()=H_{s,}^{}(s,)r_{i}(s,)\).

**Correlated Equilibria.** We now introduce the notion of a _correlated equilibrium_ (CE, Aumann ). First, we define a _strategy deviation_\(_{i}\) for the \(i\)-th agent as a map \(_{i}:_{i}_{i}\). Intuitively, a strategy deviation captures how the agent responds to the current state of the world and the recommendation of the mediator - they can either obey (in which case \(_{i}(s,a)=a\)) or defect (in which case \(_{i}(s,a) a\)). Let \(_{i}\) be the set of deviations for agent \(i\), which is a subset of all possible deviations. We use \(:=\{_{i}\}_{i=1}^{m}\) to denote deviations for all agents. We assume that for all \(i\), the identity mapping \(_{i}(s,a) a\) is in \(_{i}\). We use \(_{,_{i}}\) to denote \((_{i}_{,i})_{,-i}\): the joint agent policy induced by mediator policy \(\) being over-ridden by deviation \(_{i}\). We can now define a CE.

**Definition 3.1** (Regret and CE in General-Sum MGs).: _Let \(\) be the mediator's policy in a Markov Game, and \(_{i}\), \(i[m]\) be the deviation classes for each agent. Then,_

1. _We define the regret of a mediator policy_ \(\) _to be_ \[_{}():=_{i[m]}_{_{i}_{i}}(J_{i}( _{,_{i}})-J_{i}(_{})),\] (1)
2. _We say a mediator with policy_ \(\) _induces an_ \(\)_-approximate Correlated Equilibrium (CE) if_ \[_{}().\] (2)

Intuitively, regret captures the maximum utility any agent can gain by defecting from the mediator's recommendation. A CE is an induced joint policy where no agent has a large incentive to deviate.

## 4 On the Relationship between the Value Gap and the Regret Gap

As sketched above, we consider two potential objectives for the learner in MAIL:

**Definition 4.1** (Value Gap).: _We define the value gap between the expert's policy \(_{E}\) and the learner's policy \(\) as_

\[_{i[m]}(J_{i}(_{_{E}})-J_{i}(_{})).\] (3)

**Definition 4.2** (Regret Gap).: _We define the regret gap between the expert's policy \(_{E}\) and the learner's policy \(\) as_

\[_{}()-_{}(_{E})=_{i[m]}_{ _{i}_{i}}(J_{i}(_{,_{i}})-J_{i}(_{}))-_{k [m]}_{_{k}_{k}}(J_{k}(_{_{E},_{k}})-J_{k}(_{ _{E}})).\] (4)We say that the learner's policy satisfies _value / regret equivalence_ when the value / regret gap is \(0\). We now explore the relationship between the value and regret gap in MAIL, 1 summarized in Figure 1. We use \(J_{i}(_{},f)\) and \(_{}(,f)\) to denote the value/regret of policy \(\) under the reward function \(f\).

### Regret Equivalence \(+\) Complete Reward / Deviation Class \(\) Value Equivalence

First, we show that if the reward function class and deviation class are both _complete_, then regret equivalence implies value equivalence. We say that the reward function class is complete when \(=\{[-1,1]\}\) (i.e. all convex combinations of state-action indicators), and that the deviation class is complete if for every agent \(i\), \(_{i}=\{_{i}_{i}\}\) (i.e. all possible deviations).

**Theorem 4.1** (Complete Classes).: _If the reward function class \(\) and deviation class \(\) are complete and regret equivalence is satisfied (i.e. \(_{f}(_{}(,f)-_{}( _{E},f))=0\)), then value equivalence is also satisfied: \(_{f}_{i[m]}(J_{i}(_{_{E}},f)-J_{i}(_{ },f))=0\). [Proof]_

Next, we prove that large classes are needed for this implication to hold true.

**Theorem 4.2** (Incomplete Classes).: _There exists an MG, an expert policy \(_{E}\), and a trained policy \(\) such that even though the regret equivalence is satisfied under the true reward function \(r\), i.e. \(_{}(,r)-_{}(_{E},r)=0\), the value gap \(_{i[m]}(J_{i}(_{_{E}},r)-J_{i}(_{},r)) 0\). [Proof]_

Together, these results tell us that with an expressive enough class of reward functions / deviations, regret equivalence is stronger than value equivalence. We now turn our attention to the converse.

### Value Equivalence \(\) Regret Equivalence

We now show a surprising result: _value equivalence does not directly imply a low regret gap!_ In the worst case, value equivalence fails to provide _any_ meaningful guarantees on the regret gap. This reveals a critical distinction between SAIL and MAIL not fully addressed in the prior work.

**Theorem 4.3**.: _There exists a Markov Game, an expert policy \(_{E}\), and a learner policy \(\), such that even occupancy measure of \(_{}\) exactly matches \(_{_{E}}\), i.e. \((s,),^{_{}}(s,)=^{_{_{E}}}(s, )\) (i.e. we have value equivalence under all rewards), the regret gap \(_{}()-_{}(_{E})(H)\). [Proof]_

We leave the details of the proof for this theorem in Appendix E.3. As visualized in Figure 2, both the expert and learner policies only visit the states in the lower path \(s_{2},s_{4},...,s_{2H-2}\). The trained policy perfectly matches the occupancy measure of the expert by taking identical actions in visited states \(s_{2},s_{4},...,s_{2H-2}\). However, expert demonstrations lack coverage of state \(s_{1}\) as it is unreachable by executing \(_{E}\). This omission becomes critical when agent 1 deviates from the original policy, making \(s_{1}\) unreachable with high probability. Consequently, the trained policy may perform poorly in \(s_{1}\), in stark contrast to the expert playing a CE under the true reward function. This example highlights the key difference between value equivalence and regret equivalence: the former only depends on states actually visited by the policy, while the latter depends on the counterfactual recommendations the learner would make at unvisited states in response to an agent deviations.

**Remark 4.1**.: _As shown in Theorem 4.3, even if the learner has access to infinite samples on the equilibrium path from expert demonstrations, it is possible that the learner remains unaware of the expert's behavior in states unvisited by the expert (but reachable by the deviated agents joint policy). Thus, from an information theoretic perspective, it is impossible for the learner to minimize the regret gap without knowing how the expert would behave on those states. This demonstrates the fundamental difficulty of minimizing the regret gap, and thus, **regret is 'hard' in MAIL.** We therefore need a fundamentally new paradigm of MAIL algorithm to minimize the regret gap._

Figure 1: Under expressive enough reward function and deviation classes, regret equivalence implies value equivalence but not vice versa, making the regret gap a “stronger” objective than the value gap.

### Low Regret Gap \(\) CE, Low Value Gap \(\) Ce

Given the deep connections between regret and correlated equilibrium discussed above, it is perhaps intuitive that if the expert \(_{E}\) is playing a CE, a low regret gap means the learner is as well.

**Theorem 4.4** (Regret Gap Implies CE).: _If the expert policy \(_{E}\) induces a \(_{1}\)-approximate CE, and the learner policy \(\) satisfies \(_{}()-_{}(_{E})_{2}\), then \(\) induces a \(_{1}+_{2}\)-approximate CE. [Proof]_

Then, by combining our preceding result with Theorem 4.3, it follows that a low value gap does not imply that the learner is playing a CE.

**Corollary 4.5**.: _There exists a Markov Game, an expert policy \(_{E}\), and a learner policy \(\), such that \(_{E}\) induces a \(_{1}\)-approximate CE, and \(\) satisfies \(_{i[m]}(J_{i}(_{_{E}})-J_{i}(_{}))=_{2}\), \(\) induces a \((H)\)-approximate CE._

Together, these results imply that if we are interested in inducing a CE amongst the agents in the population, the regret gap is a more suitable objective.

### Efficient Algorithms for Minimizing the Value Gap

Although we have shown that the value gap is a 'weaker' objective in some sense, in many real-world scenarios, the agents may be non-strategic. In these scenarios, minimizing value gap can be a reasonable learning objective. As we will demonstrate here, the natural multi-agent generalization of single-agent IL algorithms can efficiently minimize the value gap--hence, _value is 'easy' in MAIL_.

Behavior Cloning (BC) and Inverse Reinforcement Learning (IRL) are two single-agent IL algorithms aimed at minimizing the value gap. By running these algorithms over joint policies, we can apply BC and IRL to the multi-agent setting, which we call Joint Behavior Cloning (J-BC) and Joint Inverse Reinforcement Learning (J-IRL). Doing so results in the same value gap bounds as in the single-agent setting. More details on of J-BC and J-IRL can be found in Appendix B.

**Theorem 4.6** (J-BC Value Gap Upper Bound).: _If J-BC returns a policy \(\) that satisfies \(_{s d^{_{E}}}[(_{E}(s),(s))]\), then the value gap \(_{i[m]}(J_{i}(_{_{E}})-J_{i}(_{})) O( H^ {2})\). [Proof]_

**Theorem 4.7** (J-IRL Value Gap Upper Bound).: _If J-IRL outputs a policy \(\) with moment-matching error_

\[_{f}_{_{_{E}}}[_{h=1}^{H}f(s_{h },_{h})]-_{_{}}[_{h=1}^{H}f(s_{h}, _{h})] H,\]

_then the value gap \(_{i[m]}(J_{i}(_{_{E}})-J_{i}(_{})) O( H)\). [Proof]_

As argued by Swamy et al. , satisfying the conditions for either of the above theorems can be achieved oracle-efficiently via a reduction to no-regret online learning. We now turn our attention to sufficient conditions for there to exist efficient algorithms for minimizing the regret gap.

Figure 2: Illustration of an Markov Game that captures why _“regret is hard”_. Here, \(_{E}(a_{1}a_{1}|s_{0})=1\). Observe that \(s_{1}\) is un-visited when all agents obediently follow \(_{E}\) but is with probability \(1\) under deviation \(_{1}\) (\(_{1}(s_{0},a_{1})=_{1}(s_{1},a_{1})=a_{2}\)). This means that unless we know what the expert \(_{E}\) would have recommended counter-factually in \(s_{1}\), we cannot minimize the regret gap.

Efficient Algorithms for Minimizing the Regret Gap

In our following analysis, we will make a _recoverability_ assumption: that a single-step agents deviation could at most cost the expert a fixed constant.

**Assumption 5.1** (\(u\)-recoverability).: _We say that an MG is \(u\)-recoverable if the expert advantage function is bounded for all deviations, i.e. \( s,,h,i,_{i},|A_{i,h}^{_{_{E},_{i}}}( s,)| u\)._

Intuitively, a small value of \(u\) means that we're not in a problem where a single agent can deviate and a joint mistake happens that even the expert couldn't recover from for the rest of the episode. In the worst case, \(u\) is \(O(H)\). However, we believe that in many cases \(u\) is small. For instance, in the route planning example, at some point many cars may miss their turns/intersections/exits, but this can be recovered within a constant time, even when a single driver chooses not to follow its recommendation, rather than \(u\) increasing as \(H\) increases.

This assumption can be thought of natural multi-agent generalization of the standard recoverability assumption in SAIL [18; 21; 20] which is necessary and sufficient to avoid compounding errors while maintaining computational efficiency. While we define recoverability with respect to the actual reward function for implicitly, one can instead easily define it with respect to the worst-case reward function in a class \((_{f})\) - _moment_ recoverability - as in  to avoid the need to know the ground truth set of agent reward functions \(r\) to bound \(u\).

In Section 4.2, we proved that for general MGs, J-BC and J-IRL don't give any guarantees on the regret gap. Fundamentally, without the ability to observe how the expert would have responded in the counter-factual state induced by a deviation, the learner cannot ensure that they match the expert's regret. We now explore two different sets of assumptions that give us this ability.

### Assumption 1: Full Coverage of Expert Demonstrations

In this section, we introduce a coverage assumption on the expert's state distribution \(d^{_{_{E}}}(s)\) which states that the expert visits every state with a positive probability. We will show that this assumption is sufficient to give a regret gap guarantee. The state coverage assumption is a common theoretical assumption in the analysis of learning in MDPs/MGs  and has been explored in SAIL .

**Assumption 5.2** (\(\)-coverage).: _There exists a constant \(>0\) such that for the expert's policy \(_{E}\), it holds that \(d^{_{_{E}}}(s)\) for all \(s\)._

Intuitively, this assumption implies that in the infinite sample limit, there are no states where we are unsure what the expert would recommend. As discussed in Remark 4.1, without the ability to interactively query the expert, a coverage assumption is necessary because we cannot minimize the regret gap without knowing the expert mediator's actions in counter-factual states.

We first show that under Assumption 5.2, J-BC and J-IRL get a (relatively weak) regret gap guarantee.

#### 5.1.1 Regret Gaps of J-BC and J-IRL under Full Demonstration Coverage

We begin by analyzing joint behavioral cloning (J-BC).

**Theorem 5.1** (J-BC Regret Gap Upper Bound).: _Under Assumption 5.1 and Assumption 5.2, if the J-BC algorithm returns a policy \(\) that satisfies \(_{s d^{_{_{E}}}}[(_{E}(s),(s)) ]\), then_

\[_{}()-_{}(_{E}) O( {} uH).\]

_[Proof]_

We leave the proof in Appendix E.7. It is worth to note that although the dependency of \(H\) is linear under our recoverability assumption, we still need to pay for the term \(\) in our regret gap bound. In general, this term can grow exponentially with the horizon, making this guarantee relatively weak. We can show its tightness by slightly modifying the example in Theorem 4.3 to satisfy the assumptions.

**Theorem 5.2** (J-BC Regret Gap Lower Bound).: _There exists a Markov Game, an expert policy \(_{E}\), and learner policy \(\) such that \(_{E}\) satisfies Assumption 5.1 and Assumption 5.2, \(\) achieves BC error \(_{s d^{_{_{E}}}}[_{}(_{E}(s),( s))]\), and_

\[_{}()-_{}(_{E})=( uH).\]

_[Proof]_

We now prove analogous results for joint inverse reinforcement learning (J-IRL).

**Theorem 5.3** (J-IRL Regret Gap Upper Bound).: _Under Assumption 5.2 and Assumption 5.1 and with a complete reward function class \(\), if J-IRL returns a policy \(\) with moment-matching error_

\[_{f}_{_{_{E}}}[^{H}f (s_{h},_{h})}{H}]-_{_{}}[^ {H}f(s_{h},_{h})}{H}],\]

_then \(_{}()-_{}(_{E}) O( {} uH)\). [Proof]_

There are two interesting features of this theorem. The first is that we needed to assume that the reward function class is complete - otherwise, a small value gap can still translate to a large regret gap. The second is that the upper-bound for J-IRL matches that for J-BC, which is in stark contrast to the single-agent setting, where IRL enjoys linear-in-\(H\) guarantees with respect to the value gap . We now show this is not an artifact of our analysis by providing a matching lower bound.

**Corollary 5.4** (J-IRL Regret Gap Lower Bound).: _There exists a Markov Game, an expert policy \(_{E}\), and a policy \(\) such that \(_{E}\) satisfies Assumption 5.1 and Assumption 5.2, the trained policy \(\) gets moment-matching error_

\[_{f}_{_{_{E}}}[^{H} f(s_{h},_{h})}{H}]-_{_{}}[^ {H}f(s_{h},_{h})}{H}],\]

_and \(_{}()-_{}(_{E})=( uH)\). [Proof]_

This result implies another fundamental distinction between SAIL and MAIL: _in contrast to the value gap, interactive training alone is not sufficient to effectively minimize the regret gap._

#### 5.1.2 Malice: Multi-agent Aggregation of Losses to Imitate Cached Experts

Observe that the upper bounds for both J-BC and J-IRL include a dependence on the inverse of the coverage coefficient \(\), which can be rather large for problems with long horizons or large action spaces. We now present an efficient algorithm that is able to avoid this dependence by extending the ALICE algorithm  to the multi-agent setting. ALICE is an interactive algorithm that, at each round, uses importance sampling to re-weight the behavior cloning (BC) loss based on the density ratio between the current learner policy and that of the expert. Accordingly, ALICE requires a full demonstration coverage assumption to ensure that these importance weights are finite. ALICE uses a no-regret algorithm to learn a policy that minimizes reweighed on-policy error, which guarantees a linear-in-\(H\) bound on the value gap under a recoverability assumption .

In Algorithm 1, we describe Multi-agent ALICE (MALICE), where adapt ALICE to the multi-agent setting (i.e. minimizing the regret gap). Specifically, we modify the ALICE loss function to include a maximum over all deviations. This gives us

\[_{}(,D_{E},)=_{i[m]}_{_{i} _{i}}_{s d^{_{_{E}}}}[}}(s)}{d^{_{_{E}}}(s)}(_{E}(s),(s))].\] (5)

Since \(_{s d^{_{E}}}[}}(s)}{d^{_{ _{E}}}(s)}(_{E}(s),(s))]\) is a convex loss function, and the maximum of convex functions is still a convex function, we know that \(_{}(,D_{E},)\) is a valid convex loss function with scales in \(\). As a result, we can run an (arbitrary) no-regret online convex optimization (OCO) algorithm to efficiently optimize it, giving us an _efficient reduction from regret gap minimization to no-regret online convex optimization under demonstration coverage_.

We now provide regret gap guarantees on the policy returned by MALICE.

**Theorem 5.5** (Malice Regret Gap Upper Bound).: _Let \(\) be a policy such that \(_{}(,D_{E},)\). Under Assumption 5.1 and Assumption 5.2, we have_

\[_{}()-_{}(_{E}) O( uH).\]

_[Proof]_

As promised, observe that adapting the importance sampling technique of Spencer et al.  to the multi-agent setting allows us to efficiently minimize the regret gap while avoiding an upper bound that depends on the coverage coefficient of the expert demonstrations.

We now show that the bound in Theorem 5.5 is tight by constructing a matching lower bound.

**Theorem 5.6** (Malice Regret Gap Lower Bound).: _There exists a Markov Game, an expert policy \(_{E}\) that satisfies Assumption 5.1, and a trained policy \(\) that gets error \(_{}(,D_{E},)\), and_

\[_{}()-_{}(_{E})=(  uH).\]

_[Proof]_

We now turn our attention to an alternate assumption and the corresponding regret gap algorithm.

### Assumption 2: Access to a Queryable Expert

For many problems, full coverage of expert demonstrations is not a reasonable assumption. Thus, we explore another natural assumption that allows us to observe expert recommendations at counterfactual states: access to a queryable expert. In their classic DAgger algorithm, Ross et al.  showed that access to a queryable expert allows one to eliminate the covariate shift that results from the difference between expert and learner induced state distributions. When we transition to the multi-agent setting, we can again use access to a queryable expert to handle yet another source of covariate shift: potential strategic deviations by agents in the population that push the learner outside of the support of the expert. We refer to our multi-agent extension of DAgger as BLADES.

In each iteration of BLADES, we request the expert to provide recommendations under all possible agent deviations, before training on the aggregated data. More formally, we minimize the following sequence of loss functions:

\[_{}(,)=_{i[m]}_{_{i} _{i}}_{s d^{_{_{i}}}}[(_{E}(s),(s ))].\] (6)

Similar to MALICE, we know that the loss \(_{}\) is also a valid convex loss function, and thus we can use a no-regret algorithm to efficiently minimize it. This gives us an _efficient reduction from regret gap minimization to no-regret online convex optimization with access to a queryable expert._ We now derive and upper and lower bounds on the regret gap of a policy returned by BLADES.

**Theorem 5.7** (Blades Regret Gap Upper Bound).: _Under Assumption 5.1, if a policy \(\) satisfies \(_{}(,)\), then_

\[_{}()-_{}(_{E}) O( uH).\]

_[Proof]_

**Theorem 5.8** (Blades Regret Gap Lower Bound).: _There exists a Markov Game, an expert policy \(_{E}\), and a trained policy \(\) such that \(_{E}\) satisfies Assumption 5.1, \(\) achieves error \(_{}(,)\), and_

\[_{}()-_{}(_{E})=( u H ).\]

_[Proof]_

In short, under either a demonstration coverage assumption or with access to a queryable expert, we are able to efficiently minimize the regret gap on a recoverable MAIL problem.

## 6 Conclusion

Our work focuses on the core question of what fundamentally distinguishes multi-agent IL problems from single-agent ones. In short, our answer is that on problems with strategic agents that are not mere puppets, we need to deal with another source of distribution shift: deviations by agents in the population. This new source of distribution shift cannot be efficiently controlled with environment interaction (i.e. inverse RL). Instead, we need to be able to estimate how the expert would act in counter-factual states. Based on this core insight, we derive two reductions that are able to minimize the regret gap under a coverage or queryable expert assumption. We leave the development and implementation of practical approximations of our idealized algorithms to future work.

## 7 Acknowledgements

We thank Drew Bagnell and Brian Ziebart for their incredible patience and detailed answers to a somewhat absurdly large number of questions about their prior work. We thank Simon Shaolei Du for a discussion on our coverage assumptions and Noah Golowich for references to relevant lower bounds in the MARL literature. We also thank Sanjiban Choudhury and Wen Sun for comments on our draft. ZSW, GS, and Jingwu Tang are supported in part by an Air Force STTR grant and the NSF Award #1763786. ZSW is supported in part by the NSF FAI Award #1939606, a Google Faculty Research Award, a J.P. Morgan Faculty Award, a Facebook Research Award, an Okawa Foundation Research Grant, and a Mozilla Research Grant. FF is supported in part by NSF grant IIS-2046640 (CAREER) and the Sloan Research Fellowship. GS is supported by his family and friends.

## 8 Contribution Statements

1. **JT** uncovered the distinction between the regret and value gaps, proved all of the core results in the paper, and drafted the initial version of the paper.
2. **GS** initially proposed the project, came up with the sufficient conditions and associated algorithms for minimizing the regret gap, and wrote most of the final version of the paper.
3. **FF** and **ZSW** advised the project.