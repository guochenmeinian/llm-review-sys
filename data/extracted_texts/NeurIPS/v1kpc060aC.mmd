# Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML

Tehila Dahan

Department of Electrical Engineering

Technion

Haifa, Israel

t.dahan@campus.technion.ac.il

&Kfir Y. Levy

Department of Electrical Engineering

Technion

Haifa, Israel

kfirylevy@technion.ac.il

###### Abstract

We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous computing resources. Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning. The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults. To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework. This allows for the extension of robust aggregators and a recent meta-aggregator to their weighted versions, mitigating the effects of delayed updates. By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment. Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems.

## 1 Introduction

In recent years, there has been significant growth in the development of large-scale machine learning (ML) models and the volume of data they require (Zhao et al., 2023). To efficiently accelerate large-scale training processes, Distributed ML has emerged as a crucial approach that can be categorized into synchronous and asynchronous paradigms. In synchronous learning, workers update the model simultaneously using the average of their outputs, similar to the Minibatch approach (Dekel et al., 2012). Asynchronous learning, however, allows workers to operate independently, sending updates as they are ready without waiting for others (Arjevani et al., 2020). This prevents slow workers from hindering the process, making it especially practical as the number of workers increases.

A major challenge of distributed ML is fault-tolerance, and Byzantine ML (Alistarh et al., 2018; Lamport et al., 2019; Guerraoui et al., 2023) is a powerful framework for tackling this aspect. Byzantine ML captures a broad spectrum of failures within distributed environments, including random malfunctions or even malicious workers aiming to disrupt the training process. This makes Byzantine ML widely applicable across various domains to ensure robust performance.

Addressing the Byzantine problem in synchronous distributed learning is well-established (Karimireddy et al., 2020, 2021; Allouah et al., 2023; Farhadkhani et al., 2022; Alistarh et al., 2018; Dahan and Levy, 2024). Two primary ingredients were found to be crucial towards tackling Byzantine ML in synchronous settings: **(i)**_Robust Aggregators_(Yin et al., 2018; Blanchard et al., 2017; Chen et al., 2017): such aggregators combine the gradient estimates sent by the workers to a single estimatewhile filtering out the outliers which may hinder the training process. While the use of robust aggregators is crucial, it was found to be insufficient, and an additional ingredient of **(ii)**_learning from history_ was shown to be vital in mitigating Byzantine faults (Karimireddy et al., 2021). And, the performance of robust aggregators was systematically explored within a powerful generic framework (Karimireddy et al., 2020, 2021, Allouah et al., 2023, Farhaddkhani et al., 2022, Dahan and Levy, 2024). Moreover, due to the diversity of Byzantine scenarios (Xie et al., 2020, Allen-Zhu et al., 2020, Baruch et al., 2019), it was found that relying on a single aggregator is insufficient, making the variety of robust aggregators essential. Unfortunately, many existing aggregators have sub-optimal performance. This drawback was elegantly resolved by the design of meta-aggregators (Karimireddy et al., 2020, Allouah et al., 2023, Dahan and Levy, 2024), that enable to boost the performance of baseline aggregators. Unfortunately, in the asynchronous case, the use of robust aggregators is not straightforward, as updates are typically applied individually per-worker, rather than averaging outputs from all workers at once (Arjevani et al., 2020).

Despite its advantages, asynchronous distributed learning presents unique challenges, particularly when dealing with Byzantine faults. The delays inherent in asynchronous settings introduce additional bias to the system and obscure the disruptions caused by Byzantine faults. In fact, in contrast to the synchronous Byzantine setting, all existing approaches towards the asynchronous Byzantine case do not ensure a generalization error (excess loss) that diminishes with the number of honest data-samples and updates. This applies to works for both convex (Fang et al., 2022) as well as non-convex scenarios (Xie et al., 2020, Yang and Li, 2023); as well as to works that further assume the availability of a _trusted dataset_ possessed by the central-server (Xie et al., 2020, Fang et al., 2022). Furthermore, the performance guarantees of all existing approaches towards that setting include an explicit dependence on the dimension of the problem -- a drawback that does not exist for SOTA synchronous Byzantine approaches.

**Contributions.** We explore the asynchronous Byzantine setting under the fundamental framework of Stochastic Convex Optimization (SCO) (Hazan et al., 2016). Our work is the first to achieve a convergence rate that diminishes with the number of honest data samples and updates and does not explicitly depend on the problem's dimension. In the absence of Byzantine workers, our rate matches the optimal performance of Byzantine-free asynchronous settings. This stands in contrast to previous efforts on Byzantine, which did not attain diminishing rates or dimensionality independence, even without Byzantine workers. We also show the effectiveness of our approaches in practice. Our contributions:

* We quantify the difficulty in asynchronous scenarios by considering the _number of Byzantine updates_, which is more natural than the standard measure of _number of Byzantine workers_.
* We identify the need to utilize weighted aggregators rather than standard ones in favor of asynchronous Byzantine problems. Towards doing so, we extend the robust aggregation framework to allow and include weights and develop appropriate (weighted) rules and a meta-aggregator.
* **Achieving Optimal Convergence**: We incorporate our weighted robust framework with a recent double momentum mechanism, leveraging its unique features to achieve an optimal convergence rate for the first time in asynchronous Byzantine ML.

Related Work.A long line of studies has explored the synchronous Byzantine setting (see e.g., Alistarh et al. (2018), Karimireddy et al. (2020, 2021), Allouah et al. (2023), Farhaddkhani et al. (2022), Allen-Zhu et al. (2020), El Mhamdi et al. (2021), Dahan and Levy (2024)). Alistarh et al. (2018), Karimireddy et al. (2021) demonstrated that historical information is crucial for optimal performance in Byzantine scenarios; and Karimireddy et al. (2021) introduced the idea of combining generic aggregation rules, together with standard momentum with a parameter of \(1/\) to effectively incorporates \(\) iterations of historical gradients. Additionally, Dahan and Levy (2024) showed that a double momentum approach is effective by taking a momentum parameter of \(1/T\), capturing the entire gradient history.

Robust aggregators such as Coordinate-wise Trimmed Mean (CWTM) (Yin et al., 2018), Krum (Blanchard et al., 2017), Geometric Median (GM) (Chen et al., 2017), CWMed (Yin et al., 2018), and Minimum Diameter Averaging (Guerraoui et al., 2018) have also proven to be highly beneficial in synchronous settings and have been evaluated within robust frameworks (Allouah et al., 2023, Karimireddy et al., 2020, Farhaddkhani et al., 2022, Dahan and Levy, 2024). However, not all robust aggregators achieve optimal performance, leading to the development of meta-aggregators[Karimireddy et al., 2020, Allouah et al., 2023, Dahan and Levy, 2024] to enhance their effectiveness. While standard aggregation works well in synchronous settings, where outputs are averaged across all workers, it is less suitable for asynchronous settings, where updates are processed individually as they arrive [Arjevani et al., 2020].

To adapt these approaches to asynchronous settings, Yang and Li  devised BASGDm, an extension of BASGD [Yang and Li, 2021], that groups worker momentums into buckets that are then aggregated using a robust aggregator. Other methods, like Zeno++ [Xie et al., 2020] and AFLGuard [Fang et al., 2022], rely on a trusted dataset on the central server, which hinders their practicality. Kardam [Damaskinos et al., 2018] uses the Lipschitzness of gradients to filter out outliers. Unfortunately, none of these approaches ensure a generalization error (excess loss) that diminishes with the number of honest data-samples and updates, and suffers from an explicit dependence on the problem's dimension. And this applies even in the absence of Byzantine faults.

Asynchronous Byzantine ML faces unique challenges as inherent delays add bias that obscures Byzantine disruptions. To mitigate this delay-bias in asynchronous, non-Byzantine scenarios, Cohen et al. , Aviv et al.  propose methods to keep model weights relatively close during iterations. Other approaches [Stich and Karimireddy, 2019, Arjevani et al., 2020, Mishchenko et al., 2022] suggest adjusting the step size proportionally to the delay. These strategies have proven useful in reducing the negative impact of delays, and achieve optimal performance.

Our work extends several concepts from Dahan and Levy  to the asynchronous scenario. We devise a novel generalization of their Centered Trimmed Meta Aggregator (CTMA) towards weighted meta-aggregation, making it amenable to asynchronous scenarios. In the spirit of Dahan and Levy , we also adopt a recent variance reduction technique called \(^{2}\)-SGD [Levy, 2023]. Nevertheless, while Dahan and Levy  used this technique in a straightforward manner, we found it crucial to appropriately incorporate individual per-worker weights to overcome the challenge of asynchronicity in Byzantine ML.

## 2 Setting

Our discussion focuses on the minimization of a smooth convex objective \(f:\):

\[f():=_{}[f(;)]\;,\]

where \(^{d}\) is a compact convex set and \(\) denotes an unknown distribution from which we can draw i.i.d samples \(\{_{t}\}_{t}\). Our work considers first-order methods that iteratively utilize gradient information to approach an optimal point. Such methods output a solution \(_{T}\), which is evaluated by the expected excess loss:

\[:=[f(_{T})-f(^{*})]\;,\]

where \(^{*}\) is a solution that minimizes \(f\) over \(\) and \(_{T}\) approximates this optimal solution.

**Asynchronous Training.** We explore these methods within a distributed environment involving multiple workers. Our discussion focuses on a _centralized_ distributed framework characterized by a central Parameter Server (\(\)) that may communicate with \(m\) workers. Each of these workers may draw i.i.d. samples \(\); and based on these samples, compute unbiased gradient estimate \(^{d}\) at a point \(\). Concretely, a worker may compute \(:= f(;)\); implying that \([|]= f()\). Specifically, our main focus is on _Asynchronous_ systems, where the \(\) does not wait to receive the stochastic gradient computations from all machines; instead, it updates its model whenever a worker completes a (stochastic) gradient computation. That worker then proceeds to compute a gradient estimate for the updated model, while the other workers continue to compute gradients based on'stale' models. This staleness leads to the use of staled (and therefore biased) gradient estimates, which is a major challenge in designing and analyzing asynchronous training methods.

**Asynchronous Byzantine Framework.** We assume that an unknown subset of the \(m\) workers are _Byzantine_, implying that these workers may transmit arbitrary or malicious information during the training process, and these "Byzantine" workers may even collaborate to disrupt the training. We assume that the fraction of updates that arrive from Byzantine workers during the asynchronous training process is bounded and strictly smaller than \(}{{2}}\) and denote this fraction by \(\).

**Remark 2.1** (Fraction of Byzantine Updates vs. Byzantine Workers).: _In both synchronous and asynchronous settings, it is common to consider a bound on the **fraction of Byzantine workers** (upto \(}{{2}}\)) (Allouah et al., 2023; Farhaddhani et al., 2022; Karimireddy et al., 2020, 2021; Yang and Li, 2023, 2021; Damaskinos et al., 2018). In synchronous scenarios this is meaningful since the server equally treats the information from all workers; which is done by equally averaging gradients of all workers in each iteration in a mini-batch fashion (Dekel et al., 2012). Conversely, in asynchronous scenarios, faster workers contribute to more updates than slower workers, leading to an unequal influence on the training process. In such scenarios, the fraction of Byzantine workers is less relevant; and it is therefore much more natural to consider the **fraction of Byzantine updates**. Interestingly, our definition aligns with the standard one (for the synchronous case), which considers the number of Byzantine workers._

**Notation.** For each worker \(i[m]\) and iteration \(t\), \(s_{t}^{(i)}\) represents the total number of updates by worker \(i\) up to \(t\), and \(_{t}^{(i)}\) is the delay compared to the current model. \(t^{(i)}\) is the last update before \(t\), making \(_{t}^{(i)}\) the time since the second last update (Figure 1). \(_{t}\) denotes the delay for the worker arriving at iteration \(t\), i.e., if worker \(j\) arrives at iteration \(t\) then \(_{t}=_{t}^{(j)}\).

For a given time (iteration) \(t\), let \(t^{(i)}\) be the last iteration when worker \(i\) made an update. We denote \(_{t}^{(i)}:=_{t^{(i)}}\), \(_{t}^{(i)}:=_{t^{(i)}}\), \(}_{t}^{(i)}:=}_{^{(i)}}\), and \(_{t}^{(i)}=_{t^{(i)}}\), where the latter are individual vectors that we will later define for any worker \(i\). Throughout, \(\|\|\) represents the \(L_{2}\)-norm. For any natural \(N\), \([N]=\{1,,N\}\). We use the compressed sum notation \(_{1:t}=_{k=1}^{t}_{k}\). For every \(^{d}\), the orthogonal projection of \(\) onto a set \(\) is denoted by \(_{}()=*{arg\,min}_{}\|-\|^{2}\). We denote \(\) and \(\) as the subsets of Byzantine workers and honest workers, respectively, such that \(|m|=||+||\).

Assumptions.We use the following conventional assumptions:

**Bounded Diameter**: we assume there exists \(D>0\) such that \(_{,}\|-\| D\). (1)

**Bounded Variance**: there exists \(>0\) such that \(\), \(\{\}\),

\[\| f(;)- f()\|^{2} ^{2}\;.\] (2)

**Expectation over Smooth Functions**: we assume that \(f()\) is an expectation of smooth functions, i.e. \(,\;,\{ \}\) there exist \(L>0\) such that,

\[\| f(;)- f(;)\| L\| -\|\;,\] (3)

The above assumption also implies that the expected loss \(f()\) is \(L\) smooth.

**Bounded Smoothness Variance**(Levy, 2023): in Appendix A we show that Eq. (3) implies that, \(,\), \(\{\}\) there exists \(_{L}^{2}[0,L^{2}]\) such,

\[\|( f(;)- f())-(  f(;)- f())\|^{2}_ {L}^{2}\|-\|^{2}\] (4)

**Bounded Delay**: \( K>0\) such that for each worker \(i[m]\), \(_{min}^{(i)}_{t}^{(i)} K_{min}^{(i)}\) (5)

where \(_{min}^{(i)}\) is the minimum delay of worker \(i\). \(K\) bounds the variance of the delay for each worker.

**Bounded Byzantine Iterations**: there exists \(0<}{{2}}\) such that \(t[T]\): \(t_{} t\) (6)

where \(t_{}\) is the total number of iterations made by Byzantine workers up to iteration \(t\).

**Sample-Arrival Independence**: we assume that the delays in the system (i.e. \(_{t}^{(i)}\)'s) are independent of the data samples. This is a standard assumption in asynchronous training scenarios, see e.g., Arjevani et al. (2020), Aviv et al. (2021).

Figure 1: Illustration of the delay interval \(_{t}^{(i)}\) for worker \(i\) at iteration \(t\), marking \(t\) (current iteration), \(t^{(i)}\) (most recent update from worker \(i\)), and \(t-_{t}^{(i)}\) (previous update from worker \(i\)).

Weighted Robust Aggregation Rules

As we have mentioned, robust aggregation rules have played a major role in designing fault-tolerant ML training methods for synchronous settings (see, e.g., Allouah et al. (2023); Karimireddy et al. (2020, 2021); Dahan and Levy (2024)). These existing aggregation rules treat inputs from all workers equally, which makes sense in synchronous cases where all workers contribute the same number of updates and data samples. Conversely, this symmetry breaks down in asynchronous settings, where faster (honest) workers contribute more updates and samples compared to slower workers.

Inspired by this asymmetry, we have identified the need to define a notion of weighted robust aggregators that generalizes the standard definition of robust aggregators. In this section, we provide such a definition, derive weighted variants of standard aggregators that satisfy our new definition, and design a generic meta-approach to derive optimal weighted aggregation rules. Later, in Section 4, we demonstrate the benefits of using weighted robust aggregators as a crucial building block in designing asynchronous fault-tolerant training methods (see Alg. 2).

### Robust Weighted Aggregation Framework

Below, we generalize the definition introduced by Dahan and Levy (2024); Karimireddy et al. (2020, 2021) to allow and associate weights to the inputs of the robust aggregation rule, therefore allowing the aggregator to unequally treat its inputs.

**Definition 3.1**.: \((c_{},)\)**-weighted robust**_. Assume we have \(m\) random vectors \(_{1},,_{m}^{d}\) and corresponding weights \(s_{1},,s_{m}>0\). Also assume we have an "honest" subset \([m]\), implying \(\{_{i}\}_{}\) are independent of each other. Finally, assume that there exists \([0,}{{2}})\) such that \(_{i}s_{i}(1-)s_{1:m}\). Moreover, assume that for any \(i\) there exist \(_{i} 0\) such that,_

\[\|_{i}-}_{}\|^{2}_{i}^{2 }, i\.\]

_Then an aggregation rule \(_{}\) is called \((c_{},)\)-weighted robust if for any such \(m\) random vectors and weights and \( 0\), it outputs \(}_{}(_{1},,_{m};s_{1},,s_{m})\) such that,_

\[\|}-}_{}\| c_{ }^{2}\]

_for some \(c_{} 0\). Above, \(}_{}:=}s_{i}}_{i }s_{i}_{i}\), \(^{2}:=}s_{i}}_{i}s_{i} _{i}^{2}\), and the expectation is w.r.t. \(\{_{i}\}_{i=1}^{m}\) and (possible) randomization in the \(_{}\)._

Here, \(\) represents the fraction of the sum of the non-honest vectors' weights, unlike the unweighted definition (in synchronous cases) (Karimireddy et al., 2020, 2021; Allouah et al., 2023; Karimireddy et al., 2022) where it indicates the fraction of non-honest vectors. These definitions align when all weights are equal (Dahan and Levy, 2024). Similarly to the unweighted version, the optimal \(c_{}\) should be \(c_{} O()\)(Dahan and Levy, 2024).

**Remark 3.1**.: _Note that our definition is generic and may be applied in both convex and non-convex scenarios. Moreover, it is natural to consider such weighted aggregators beyond asynchronous settings. For example, in synchronous settings where workers have varying batch sizes, weighted aggregation based on batch sizes may be more effective than uniform aggregation._

Next, we present two weighted variants of standard (non-weighted) aggregators that satisfy the above definition (we defer the proof into Appendix C). Table 1 summarizes their \(c_{}\) values.

### Weighted Variant of Geometric Median and Coordinate-Wise

Weighted Geometric Median (WeightedGM)The Weighted Geometric Median (WeightedGM) minimizes the weighted sum of Euclidean distances to a set of points. Formally, for points \(\{_{i}\}_{i=1}^{m}\) and corresponding weights \(\{s_{i}\}_{i=1}^{m}\), WeightedGM \(_{^{d}}_{i[m]}s_{i}\|- _{i}\|\).

Weighted Coordinate-Wise Median (WeightedCWMed)The Weighted Coordinate-Wise Median (WeightedCWMed) aggregates multi-dimensional data by finding the weighted median of each coordinate separately. Thus, for given coordinate if \(\{_{i}\}_{i=1}^{m}\) are sorted and weights \(\{s_{i}\}_{i=1}^{m}\), the weighted median \(_{j^{*}}\) is the element where: \(j^{*}=_{j[m]}\{_{i[j]}s_{i}>_{i[m]}s_ {i}\}\). If \(_{i=1}^{j}s_{i}=_{i=1}^{m}s_{i}\) for some \(j\), then: WeightedMedian \(=_{j}+_{j+1}}{2}\).

### Weighted Centered Trimmed Meta Aggregator (\(\)-CTMA)

Table 1 illustrates that \(\)-GM and \(\)-CWMed fail to achieve the desired optimal \(c_{}=O()\); typically for \(}{{3}}\), their \(c_{}\) remains \( O(1)\). To address this suboptimality, we propose \(\)-CTMA, a weighted extension of the Centered Trimmed Meta Aggregator (CTMA) (Dahan and Levy, 2024). This extension enables us to achieve the optimal bound \(c_{} O()\) for \(}{{3}}\) (see Table 1).

The \(\)-CTMA algorithm (Algorithm 1) operates on a set of vectors along with their associated weights, a threshold \([0,}{{2}})\), and a \((c_{},)\)-weighted robust aggregator. It sorts the distances between each vector and the weighted robust aggregator, trims the set based on the threshold to satisfy \(_{i S}s_{i}=(1-)s_{1:m}\), and calculates a weighted average of the vectors, excluding outliers based on their proximity to an anchor point--the weighted robust aggregator.

**Lemma 3.1**.: _Under the assumptions outlined in Definition 3.1, if \(\)-CTMA receives a \((c_{},)\)-weighted robust aggregator, \(_{}\); then the output of \(\)-CTMA, \(}\), is \((60(1+c_{}),)\)-robust._

For the complete analysis, please refer to Appendix C.2. Like CTMA (Dahan and Levy, 2024), \(\)-CTMA is highly efficient, with a computational complexity of \(O(dm+m m)\), similar to \(\)-GM, \(\)-CWMed, and weighted averaging, differing by at most an additional logarithmic factor.

## 4 Asynchronous Robust Training

We leverage the \(^{2}\)-SGD algorithm (Levy, 2023), a double momentum mechanism that enhances variance reduction. By seamlessly incorporating our weighted robust framework as a black box into the \(^{2}\)-SGD, we derive an optimal asynchronous Byzantine convergence rate.

\(^{2}\)-Sgd:The \(^{2}\)-SGD is a variant of standard SGD, incorporating several key modifications in its update rule:

\[_{t+1}=_{}(_{t}-_{t}_{t}),_{t+1}=}_{k[t+1]} _{k}_{k};_{1}=_{1},\;  t>1.\]

Here, \(\{_{t}>0\}_{t}\) are importance weights that emphasize different update steps, with \(_{t} t\) to place more weight on recent updates. The sequence \(\{_{t}\}_{t}\) represents weighted averages of the iterates \(\{_{t}\}_{t}\), and \(_{t}\) is an estimate of the gradient at the average point, \( f(_{t})\), differing from standard SGD, which estimates gradients at the iterates, \( f(_{t})\).

This approach relates to Anytime-GD (Cutkosky, 2019), which is strongly connected to momentum and acceleration concepts (Cutkosky, 2019; Kavis et al., 2019). While the stochastic version of Anytime-GD typically uses the estimate \( f(_{t};_{t})\), \(^{2}\)-SGD employs a variance reduction mechanism to produce a _corrected momentum_ estimate \(_{t}\)(Cutkosky and Orabona, 2019). Specifically, \(_{1}= f(_{1};_{1})\), and for \(t>2\):

\[_{t}= f(_{t};_{t})+(1-_{t})(_{t-1}- f(_{t-1};_{t})).\]

  Aggregation & \(\)-GM & \(\)-CWMed & \(\)-GM +\(\)-CTMA & \(\)-CWMed + \(\)-CTMA \\  \(c_{}\) & \((1+)^{2}\) & \((1+)^{2}\) & \((1+)^{2}\) & \((1+)^{2}\) \\  

Table 1: Summary of weighted aggregation rules and their respective \(c_{}\) values.

Here, \(_{t}\) are _corrected momentum_ weights. It can be shown by induction that \([_{t}]=[ f(x_{t})]\); however, in general, \([_{t}|x_{t}] f(x_{t})\), unlike standard SGD estimators. Nevertheless,  demonstrates that choosing _corrected momentum_ weights \(_{t}:=1/t\) results in significant error reduction, with \(\|_{t}\|^{2}:=\|_{t}- f(_{t})\|^{2} O(^{2}/t)\) at step \(t\), where \(^{2} O(^{2}+D^{2}_{L}^{2})\). This indicates that variance decreases with \(t\), contrasting with standard SGD where the variance \(\|_{t}^{}\|^{2}:=\|_{t}-  f(_{t})\|^{2}\) remains uniformly bounded.

### Asynchronous Robust \(^{2}\)-Sgd

Building upon these, we integrate the \(^{2}\)-SGD with a \((c_{},)\)-weighted robust aggregator \(_{}\), as described in Alg. 2. At each iteration \(t[T]\), the global \(\) receives an output from a certain worker and aggregates all workers' recent updates \(\{_{t}^{(i)}\}_{i=1}^{m}\) by employing weights accordingly to the number of updates of each worker \(\{s_{t}^{(i)}\}_{i=1}^{m}\). An honest worker \(i\) arriving at iteration \(t\) returns its corrected momentum \(_{t}^{(i)}\) to the \(\), computed as:

\[_{t}^{(i)}=_{t-_{t}}=_{t-_{t}}+(1- _{t-_{t}})(_{t-_{t}-_{t-_{t}}}-}_{ t-_{t}-_{t-_{t}}})\;,\]

where \(_{t}:= f(_{t};_{t})\), and \(}_{t-_{t}}:= f(_{t-_{t}};_ {t})\). Afterwards, the \(\) performs the AnyTime update step as follows:

\[_{t+1}=_{}(_{t}-_{t} _{}(\{_{t}^{(i)},s_{t}^{(i)}\}_{i=1}^{m})),\; _{t+1}=}_{k[t+1]}_{k} _{k}\;.\]

In the spirit of Levy , Dahan and Levy , we suggest employing \(_{t}:=1/s_{t}\), which effectively considers the entire individual gradient's history of each worker; this translates to a stochastic error bound of \(\|_{t}^{(i)}\| O(/s_{t})\) for an honest worker \(i\) arriving at iteration \(t\). To achieve an error corresponding to the total number of honest iterations \(t_{}\), specifically \(\|_{t}\| O(/t_{})\), as in the non-distributed setting , a weighted collective error across all honest workers should be considered with weights determined by the number of honest worker arrivals, as detailed in Theorem 4.1. The unique characteristics of \(^{2}\)-SGD make it well-suited for the asynchronous Byzantine setting, where \(<}{{2}}\) relates to the fraction of Byzantine iterations. The total iteration number \(t\) matches the sum of the workers' frequencies (\(_{i[]}s_{t}^{(i)}=t_{}\)), aligning with the weighted robust definition in Definition 3.1. Using other approaches like momentum  is less straightforward in the asynchronous Byzantine setting with the weighted robust definition. This complexity arises because an individual honest error \(\|_{t}^{(i)}\| O(/})\) implies that weights should be \(}\) instead of \(s_{t}\), which can be more challenging.

**Remark 4.1** (Memory and Computational Overhead of Algorithm 2).: _Algorithm 2 incurs additional memory and computational costs compared to the asynchronous Byzantine-free setting  is less straightforward in the asynchronous Byzantine setting._2020], where the server stores only one worker's output and the global model. Algorithm 2 stores the latest outputs from all workers, increasing memory usage to \(O(dm)\). Robust aggregation methods like \(\)-CWMed [Yin et al., 2018] and (\(\)-approximate) \(\)-GM [Chen et al., 2017, Acharya et al., 2022] add a computational cost of \(O(dm m)\) and \(O(dm+d^{-2})\), respectively. This is in contrast to Byzantine-free settings where worker outputs are used directly without aggregation. Comparable overheads are observed in synchronous Byzantine-resilient methods, which similarly aggregate outputs from all workers. This reflects a necessary trade-off: achieving robustness inherently requires leveraging information from all workers to counteract the influence of potentially faulty ones._

**Theorem 4.1**.: _For a convex set \(\) with bounded diameter \(D\) and a function \(f:\), and assume the assumptions in Equations (2),(3),(4). Then Alg. 2 with parameters \(\{_{t}=t\}_{t}\) and \(\{_{t}=1/s_{t}\}_{t}\) ensures the following for every \(t[T]\) and each honest worker \(i\):_

\[\|_{t}^{(i)}\|^{2}^{2 }}{s_{t}^{(i)}},\|}s_{t}^{(i )}}{_{i}s_{t}^{(i)}}{_{t}^{(i)}}\|^{2} ^{2}}{t_{}}\;,\]

_where \(_{t}^{(i)}=_{t}^{(i)}- f(_{t}^{(i)})\), \(^{2}=2^{2}+32D^{2}K^{2}_{L}^{2}\), and \(t_{}\) is the total number of honest iterations up to the \(t^{}\) iteration._

Proof Sketch of Thm. 4.1.: The complete analysis is provided in App. B.1. It involves several key steps for an honest \(i\) worker who arrives at iteration \(t\):

1. Following Lemma B.1, the distance between successive query points:\(\|_{t}^{(i)}-_{t-_{t}}^{(i)}\|^{(i) }-1}D\)
2. We analyze the recursive dynamics of the error term \(_{t}^{(i)}\) by setting \(_{t}=^{(i)}}\) and obtain: \[s_{t}^{(i)}_{t}^{(i)}=(_{t}^{(i)}- f(_{ t}^{(i)}))+(s_{t}^{(i)}-1)Z_{t}^{(i)}+(s_{t}^{(i)}-1)_{t-_{t}}^{(i)}\;,\] where \(Z_{t}^{(i)}:=_{t}^{(i)}- f(_{t}^{(i)})-(}_{t-_{t}}^{(i)}- f(_{t-_{t}}^{(i)}))\). Unrolling this recursion provides an explicit expression: \(s_{t}^{(i)}_{t}^{(i)}=_{k[s_{t}^{(i)}]}_{k}^{(i)}\), where \(_{s_{t}^{(i)}}^{(i)}:=_{t}^{(i)}- f(_{t }^{(i)})+(s_{t}-1)Z_{t}^{(i)}\); thus, \(\{_{k}^{(i)}\}_{k[s_{t}^{(i)}]}\) is a martingale difference sequence.
3. Employing the above with Eq. (2) and (4), we have: \(\|_{k}^{(i)}\|^{2} 2^{2}+32D^{2}K^{2} _{L}^{2}=^{2}\).
4. Leveraging the properties of a martingale difference sequence, we have: \[\|s_{t}^{(i)}_{t}^{(i)}\|^{2}= \|_{k[s_{t}^{(i)}]}_{k}^{(i)}\|^{2}= _{k[s_{t}^{(i)}]}\|_{k}^{(i)} \|^{2}^{2}s_{t}^{(i)}\;,\] \[\|_{i}s_{t}^{(i)}_{t}^{(i)} \|^{2}=\|_{i}_{k[s_{t}^{(i) }]}_{k}^{(i)}\|^{2}=_{i}_{k [s_{t}^{(i)}]}\|_{k}^{(i)}\|^{2} ^{2}_{i}s_{t}^{(i)}=^{2}t_{ }\;.\]

**Remark 4.2**.: _Compared to synchronous scenarios [Levy, 2023, Dahan and Levy, 2024], the variance \(\) in Thm. 2 additionally includes the variance in the delay, denoted as \(K\) (Eq. (5)). In balanced scheduling methods, like Round Robin [Langford et al., 2009], the impact of \(K\) on the error becomes minor, as the delay \(_{t}^{(i)}=m\) is constant. In the case of constant delays, the factor \(K\) equals \(1\)._

**Lemma 4.1**.: _Let \(_{}\) be \((c_{},)\)-weighted robust aggregation rule and let \(f:\), where \(\) is a convex set with bounded diameter \(D\), and presume that the assumption in Equations (2),(3),(4) hold. Then invoking Alg. 2 with \(\{_{t}=t\}_{t}\) and \(\{_{t}=1/s_{t}\}_{t}\), ensures the following for any \(t[T]\)._

\[\|}_{t}- f(_{t})\|^{2} O (^{2}}{m^{2}}{}}}}+^{max}DL)^{2}}{t^ {2}}+(_{t}^{max}DL)^{2}}{t^{2}}}_{})\]_where \(}_{t}=_{}(\{_{t}^{(i)},s_{t}^{(i)}\}_{i=1 }^{m})\), \(_{t}^{max}=_{i[m]}\{_{t}^{(i)}\}\), and \(^{2}=2^{2}+32D^{2}K^{2}_{L}^{2}\)._

Lemma 4.1 shows that the error between our gradient estimator \(}_{t}\) and the true gradient includes a bias term arising from the aggregation of delayed momentums. This is in contrast to the synchronous scenario (Dahan and Levy, 2024) where the error is solely variance-dependent without any bias component. However, this bias does not affect the overall excess loss (Theorem 4.2), which remains comparable to the optimal rate achieved in synchronous Byzantine settings (see Remark 4.5).

By integrating the weighted robust aggregator with the double momentum mechanism, we achieve the optimal convergence rate for the first time in an asynchronous Byzantine setting--a significant advancement over previous efforts.

**Theorem 4.2** (Asynchronous Byzantine \(^{2}\)-SGD Guarantees).: _Let \(_{}\) be \((c_{},)\)-weighted robust aggregation rule and let \(f\) be a convex function. Also, let us make the same assumptions as in Thm. 4.1, and let us denote \(G^{*}:=\| f(^{*})\|\), where \(^{*}_{ K}f()\). Then invoking Alg. 2 with \(\{_{t}=t\}_{t}\) and \(\{_{t}=1/s_{t}\}_{t}\), and using a learning rate \( 1/4LT\) guarantees,_

\[[f(_{T})-f(^{*})] O (D+LD^{2}_{max}(})}{T}+(}m)}{})\]

_where \(^{2}=2^{2}+32D^{2}K^{2}_{L}^{2}\), \(_{max}=_{t[T]}_{t}^{max}\), and \(_{t}^{max}=_{i[m]}\{_{t}^{(i)}\}\)._

**Remark 4.3**.: _In the absence of Byzantine iterations (\(=0\)), the parameter \(c_{}\) of a \((c_{},)\)-weighted robust aggregator can diminish to 0 when we use \(\)-CTMA (see Table 1). This aligns with the asynchronous SGD analysis (Arjevani et al., 2020) and represents the first work to achieve optimal convergence without Byzantine workers compared to previous efforts (Yang and Li, 2021, 2023; Fang et al., 2022; Zhu et al., 2023; Damaskinos et al., 2018; Xie et al., 2020; Zhu et al., 2024)._

**Remark 4.4**.: _Unlike previous works (Yang and Li, 2021, 2023; Fang et al., 2022; Zhu et al., 2023; Damaskinos et al., 2018; Xie et al., 2020; Zhu et al., 2024), our convergence rate is independent of data dimensionality \(d\) and is sublinear at \(T\), even in the presence of Byzantine workers._

**Remark 4.5**.: _This result is consistent with the synchronous scenario (Dahan and Levy, 2024), where the delay is constant \(_{t}=m\) as in Round Robin (Langford et al., 2009). In this case, the proportion of Byzantine workers is \(\), and the asynchronous excess loss is \( O(m}{T}+m}}{ })\). In comparison to the synchronous case, where \(m\) workers perform \(R\) rounds, here we make \(R\) query point updates and \(T=Rm\) data-samples, resulting in synchronous excess loss \( O(}{R}+}}{ })=O(m}{T}+}}{})\)(Dahan and Levy, 2024)._

## 5 Experiments

To evaluate the effectiveness of our proposed approach, we conducted experiments on MNIST (LeCun et al., 2010) and CIFAR-10 (Krizhevsky et al., 2014) datasets--two recognized benchmarks in image classification tasks. We employed a two-layer convolutional neural network architecture for both datasets, implemented using the PyTorch framework. The training was performed using the cross-entropy loss function, and all computations were executed on an NVIDIA L40S GPU. To ensure the robustness of our findings, each experiment was repeated with three different random seeds, and the results were averaged accordingly. Our experimental results demonstrate consistent performance across both datasets. Further details about the experimental setup and the complete results are provided in Appendix D.

**Weighted vs. Non-Weighted Robust Aggregators**. We evaluated the test accuracy of weighted and non-weighted robust aggregators in imbalanced asynchronous Byzantine environments. Our experiments show that weighted robust aggregators consistently achieved higher test accuracy than the non-weighted ones (see Figure 2 and Figure 5). This highlights the benefit of prioritizing workers who contribute more updates in asynchronous setups.

**Effectiveness of \(\)-CTMA**. We evaluated the test accuracy of weighted robust aggregators with and without the integration of \(\)-CTMA, as shown in Figure 3 and Figure 6. The results demonstrate that \(\)-CTMA can enhance the performance of weighted robust aggregators in various Byzantine scenarios for both datasets.

**Performance of \(^{2}\)-SGD vs. Standard Momentum and SGD**. We evaluated the test accuracy of \(^{2}\)-SGD in comparison to standard momentum  and SGD  within an asynchronous Byzantine setup. Figure 4 and Figure 7 show that \(^{2}\)-SGD performs on par with standard momentum, while SGD generally exhibits poorer performance relative to both. These results underscore the importance of utilizing historical information when addressing Byzantine scenarios.

## Conclusions and Future Work

This paper shows that using a double momentum approach, which incorporates the entire history of each honest worker, improves the stochastic error bound to be proportional to the total number of updates when considering their weighted average in asynchronous settings. By integrating this method with a weighted robust framework, \(^{2}\)-SGD achieves an optimal convergence rate, making it particularly effective for asynchronous Byzantine environments. However, integrating other optimization algorithms, like momentum, into this weighted robust framework can be challenging, as they do not achieve an error bound proportional to the total number of updates and may complicate the adjustment of weights based on the update count. This highlights the need for further research to adapt different methods to the spirit of this framework in non-convex and convex settings.

Figure 4: **CIFAR-10. Test Accuracy Comparison Among Different Optimizers**. This scenario involves 9 workers (4 Byzantine) with \(=0.4\), and workers’ arrival probabilities are proportional to their IDs. Left: _sign flipping_. Right: _label flipping_.

Figure 3: **CIFAR-10. Test Accuracy Comparison of Weighted Robust Aggregators With and Without \(\)-CTMA**. This scenario involves 9 workers, including either 1 or 3 Byzantine workers. The arrival probabilities of workers are proportional to their IDs, and we employed \(^{2}\)-SGD. On the _left_, the _label flipping_ and _sign flipping_ attacks are depicted with \(=0.3\) and \(=0.4\), respectively, using 3 Byzantine workers. On the _right_, the _little_ attack is shown with \(=0.1\) and 1 Byzantine worker, and the _empire_ attack is shown with \(=0.4\) and 3 Byzantine workers.

Figure 2: **CIFAR-10. Test Accuracy of Weighted vs. Non-Weighted Robust Aggregators**. This scenario involves 17 workers, including 8 Byzantine workers, with workers’ arrival probabilities proportional to the square of their IDs. We used the \(^{2}\)-SGD in this scenario. Left: _label flipping_, \(=0.3\). Right: _sign flipping_, \(=0.4\).