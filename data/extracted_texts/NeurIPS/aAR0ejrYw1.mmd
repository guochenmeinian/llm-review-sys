# Images that Sound:

Composing Images and Sounds on a Single Canvas

Ziyang Chen  Daniel Geng  Andrew Owens

University of Michigan

https://ificl.github.io/images-that-sound/

###### Abstract

Spectrograms are 2D representations of sound that look very different from the images found in our visual world. And natural images, when played as spectrograms, make unnatural sounds. In this paper, we show that it is possible to synthesize spectrograms that simultaneously look like natural images and sound like natural audio. We call these visual spectrograms _images that sound_. Our approach is simple and zero-shot, and it leverages pre-trained text-to-image and text-to-spectrogram diffusion models that operate in a shared latent space. During the reverse process, we denoise noisy latents with both the audio and image diffusion models in parallel, resulting in a sample that is likely under both models. Through quantitative evaluations and perceptual studies, we find that our method successfully generates spectrograms that align with a desired audio prompt while also taking the visual appearance of a desired image prompt. Please see our project page for video results: https://ificl.github.io/images-that-sound/

## 1 Introduction

The spectrogram is a ubiquitous low-dimensional representation for audio machine learning that plots the energy within different frequencies over time. But it is also widely used as a tool for converting

Figure 1: **Images that sound.** We use diffusion models to generate visual spectrograms (second row) that look like natural images, which we call _images that sound_. These spectrograms can be converted into natural sounds (third row) using a pretrained vocoder, or colorized to obtain more visually pleasing results (first row). Please refer to our website to listen to the sounds.

sound into a visual form that can be--at least partially--perceived by sight. For example, in this representation (Fig. 2), event onsets look to a human observer like lines, and speech looks like a sequence of waves and bands. This insight is commonly used within the audio community, which frequently repurposes pretrained visual networks for audio tasks, often with only relatively minor modifications .

We hypothesize that the success of spectrograms in these roles is due in part to the fact that they share many statistical properties with the distribution of natural images, providing visual structures like edges and textures that the human visual system can readily process. Given the statistical similarities between images and sounds, we ask whether it is possible to automatically generate examples that lie at the _intersection_ of both modalities. We create _images that sound_ (Fig. 1), 2D matrices that _look_ semantically meaningful when viewed as images, but that also _sound_ meaningful when played as a spectrogram. This generative modeling problem is challenging, because it requires modeling a distribution that is induced by two very different data sources, and no relevant paired data is available.

We are motivated by the "spectrogram art" that has been made by a variety of artists , most famously by musicians Aphex Twin , Venetian Snares  and Nine Inch Nails . These artists manipulate their songs to display a desired image when they are visualized as spectrograms, such as by showing the artist's face or album art. In current practice, there is a steep trade-off between the quality of the image and the sound, since it is difficult to simultaneously control the interpretation of a signal in both modalities. As a result, existing artwork often comes across to the listener as dissonant or as random noise, rather than as natural sounds.1 By contrast, we aim to generate signals that are as natural as possible in both modalities, such as towers that simultaneously sound like ringing bells or images of tigers that make a roaring sound (Fig. 1).

In this work, we pose this problem as a multimodal compositional generation task and propose a simple, zero-shot method that composes off-the-shelf text-to-spectrogram and text-to-image diffusion models from different modalities. Inspired by prior work on compositionality in diffusion models , we denoise using both a noise estimate from the spectrogram model and a noise estimate from the image model. This is possible because these two models perform diffusion in the same latent space. The result is a sample that is simultaneously likely under the (text-conditional) distribution of spectrograms and images. The spectrograms are then converted to waveforms using a pretrained vocoder. In addition, we show that these black-and-white images may be colorized, resulting in color images whose grayscale versions can be played as spectrograms.

Surprisingly, we find that off-the-shelf diffusion models _trained on different modalities_ can be composed together to obtain samples that function as both an image and a sound. Often these examples reuse visual elements in unexpected ways (_e.g._, in Fig. 1, a line is both the onset of a bell chime and the contour of a bell tower). We provide qualitative results, as well as quantitative comparisons and human study results against baselines, indicating that our method produces spectrograms that better align with both the audio and image prompts. Our contributions are summarized as follows:

* We propose _images that sound_, a type of multimodal art that can be both understood as an image or played as a sound.
* We show that we can compose pretrained diffusion models from different modalities in a zero-shot fashion to produce examples at the intersection of image and spectrogram distributions.

Figure 2: **Images vs. spectrograms.** We show grayscale images generated from Stable Diffusion  on the left, followed by log-mel spectrograms generated from Auffusion  in the middle, and our generated _images that sound_ results on the right.

* We propose alternative methods for generating _images that sound_, one based on score distillation sampling [11; 93] and another based on simply subtracting an image from a spectrogram.
* We find through qualitative and quantitative experiments that our method outperforms baseline approaches and generates high-quality samples.

## 2 Related Work

Diffusion models.Diffusion models [102; 54; 106; 27; 104] are a class of generative models that learn to reverse a forward process that iteratively corrupts data. Typically, this forward process adds Gaussian noise and the reverse process learns to denoise the data by predicting the added noise. Diffusion models have a variety of applications, including text-conditioned image generation [27; 96; 84; 26; 98], video generation [53; 56; 101; 6; 45; 115], image and video editing [94; 97; 79; 49; 9; 31; 40], audio generation [118; 70; 71; 32; 43; 76], 3D generation [74; 57; 12; 73; 8; 38], and camera pose estimation . In this work, we use Stable Diffusion , a latent diffusion model trained for text-conditioned image generation, as well as Auffusion , a text-conditioned audio generation model trained to produce log-mel spectrograms. Auffusion is finetuned from Stable Diffusion, similar to Riffusion , and as a result, the two methods share a latent space. This is crucial for our technique, which jointly diffuses these shared latents.

Compositional generation.One property of diffusion models is that they admit a straightforward technique to compose concepts by summing noise estimates. This may be understood by viewing noise estimates as gradients of a conditional data distribution [105; 106], and the sum of these gradients as pointing in the direction that maximizes multiple conditional likelihoods. This approach has been applied to enable compositions of text prompts globally , spatially [7; 29], transformations of images , and image components . We go beyond these works by showing that diffusion models _from two different modalities_ can successfully be composed together.

Audio-visual learning.A variety of works have learned cross-modal associations between vision and sound. Some approaches establish _semantic correspondence_, _i.e._, which sounds and visuals are commonly associated with one another [3; 108]. Previous work has used this cue to learn cross-modal representations [5; 83; 80; 46; 44; 69; 68] and audio-visual sound localization [4; 58; 81; 59; 100; 91; 75]. Some researchers focus on the _temporal correspondence_ between audio and visual streams [64; 87; 33; 16; 107; 62] to study source separation [122; 1; 37], Foley sound generation [61; 28; 117; 78], and action recognition [39; 60; 86]. Others also explore the _spatial correspondence_ between them [21; 35; 120; 19; 22; 77], including spatial sound generation [36; 82; 14; 67] and audio-visual acoustic learning [13; 103; 24; 18; 20]. Differing from the works above, our focus is to explore the intersection of the distributions between spectrograms and images, where we create spectrograms that can be understood as visual images and can also be played as sounds.

Audio steganography.Audio steganography is the practice of concealing information within an audio signal. Artists have explored it for creative expression [111; 110]. Aphex Twin embedded a visual of his face in the audio waveform of the track "Formula" . Noam Oxman creates animal portraits made of musical notations . Other work has proposed deep learning methods for steganography, such as hiding video content inside audio files with invertible generative models , hiding audio data inside an identity image , and audio watermarking [15; 89; 99]. Our approach can be viewed as a steganography method that hides an image within an audio track, and is only revealed when the track is converted to a spectrogram.

## 3 Method

Our goal is to generate spectrograms that simultaneously represent both a sound and an image, each of which is specified by a text prompt. When the spectrogram is converted into a waveform, the sound matches the audio prompt, while when it is visually inspected, it should take the appearance of a given visual prompt (Fig. 1). To do this, we sample from the joint distribution of images and spectrograms, using off-the-shelf diffusion models trained on each modality independently.

### Preliminaries

Diffusion models.Diffusion models [54; 106] iteratively denoise standard Gaussian noise, \(_{T}(,)\), to generate clean samples, \(_{0}\), from some learned data distribution. At timestep \(t\) in the reverse diffusion process, the noise predictor, \(_{}\), takes the intermediate noisy sample, \(_{t}\), and the condition \(y\), such as a text prompt embedding, to estimate the noise \(_{}(_{t};y,t)\). Following DDIM , we obtain the next, less noisy, sample \(_{t-1}\) at the previous timestep via:

\[_{t-1}=}(_{t}-}}_{}(_{t};y,t)}{ }})+-_{t}^{2}}}_{ }(_{t};y,t)+_{t}_{t},\] (1)

where \(_{t}\) is independent Gaussian noise, \(_{t}\) is a predefined coefficient, and \(_{t}\) controls the randomness level which we set to 0 for deterministic sampling. We may also optionally apply classifier-free guidance (CFG)  by modifying the noise estimate as:

\[}_{}(_{t};y,t)=_{}( _{t};,t)+(_{}(_{t };y,t)-_{}(_{t};,t)),\] (2)

where \(\) denotes the strength of the conditional guidance and \(\) is the unconditional embedding of the empty string. This often results in much higher-quality samples.

Latent diffusion.Latent Diffusion Models (LDMs)  perform the diffusion process in a latent space rather than in pixel space. A pretrained encoder and decoder pair, \(\) and \(\), translates between pixel space and latent space. The latent space is typically much more compact and information-dense, which makes diffusion in this space more efficient. We use pretrained LDMs in our approach, due to the availability of audio and visual models with the same latent space.

### Multimodal Denoising

Our goal is to generate an example \(^{m n}\) that would be likely to appear under both visual and audio distributions, \(p_{a}()\) and \(p_{v}()\). We formulate this as sampling from a product of expert models2: \(p_{av}() p_{a}()p_{v}()\). We follow recent work on the compositional generation that

Figure 3: **Composing audio and visual diffusion models.** We generate the visual spectrogram that can be visualized as an image or played as a sound. Given a noisy latent \(_{t}\), we apply visual and audio diffusion models, each guided by a text prompt, to compute noise estimates \(_{v}^{(t)}\) and \(_{a}^{(t)}\) respectively. We obtain the multimodal noise estimate \(}^{(t)}\) by a weighted average, then use it as part of the iterative denoising process. Finally, we decode the clean latent \(_{0}\) to a spectrogram and convert it into a waveform using a pretrained vocoder (or by Griffin-Lim ).

samples from this distribution using the score functions from pretrained diffusion models . In contrast to these approaches, however, our two models are trained on _two different modalities_.

We create our spectrograms using two pretrained latent diffusion models. One trained to generate images, \(_{,v}(,,)\), and the other to generate spectrograms, \(_{,a}(,,)\), both operating in the same latent space. We show an overview of our method in Fig. 3. Given a noisy latent, \(_{t}\), and text prompts \(y_{v}\) and \(y_{a}\) corresponding to the desired image and spectrogram prompt respectively, we compute two CFG noise estimates (Eq. (2)):

\[_{v}^{(t)} =_{,v}(_{t};,t)+_{v} (_{,v}(_{t};y_{v},t)-_{,v}( _{t};,t)),\] (3) \[_{a}^{(t)} =_{,a}(_{t};,t)+_{a} (_{,a}(_{t};y_{a},t)-_{,a}( _{t};,t)),\] (4)

where \(_{v}\) and \(_{a}\) are the corresponding visual and audio guidance scales. We then combine the noise estimates from both modalities by applying weighted averaging, producing a multimodal noise estimate that steers the denoising process toward a sample that is likely under the distribution of both images and spectrograms:

\[}^{(t)}=_{a}^{(t)}_{a}^{(t)}+_ {v}^{(t)}_{v}^{(t)},\] (5)

where \(_{a}^{(t)}\) and \(_{v}^{(t)}\) are the weights of the audio and visual noise estimates at timestep \(t\) respectively.

With this new noise estimate \(}^{(t)}\), we perform a step of DDIM (Eq. (1)) to obtain a less noisy latent, \(_{t-1}\). Repeating this process we obtain the clean latent \(_{0}\), which is then decoded using the decoder \(\) to obtain the spectrogram \(}=(_{0})\). This spectrogram can further be converted to a waveform using a pretrained vocoder or colorized to an RGB image whose grayscale version is the spectrogram.

Warm-starting.We find it useful to warm-start the denoising process. In Sec. 4.5, we experiment with warm-starting using only the spectrogram noise estimates or only the image noise estimates. This can be represented by using \(w_{a}^{(t)}\) and \(w_{v}^{(t)}\) as the relative weight on the audio and the visual noise estimates respectively. We let

\[_{a}^{(t)}=^{(t)}}{w_{a}^{(t)}+w_{v}^{(t)}},_{v }^{(t)}=^{(t)}}{w_{a}^{(t)}+w_{v}^{(t)}},\] (6)

with \(w_{a}^{(t)}=H(t_{a}T-t)\) and \(w_{v}^{(t)}=H(t_{e}T-t)\) being Heaviside step functions, and \(t_{a}\) and \(t_{v}\) indicating the _proportion_ of the reverse process that has audio or visual denoising respectively. When \(t_{a}<1.0\) and \(t_{v}=1.0\), we warm-start with only image denoising, and vice-versa. The above ensures that the weights \(_{a}^{(t)}\) and \(_{v}^{(t)}\) sum to one, and are equally weighted after warm-starting.

Colorization.After we generate a spectrogram, \(}\), we can optionally colorize it to create a more visually appealing result. Since our spectrograms fall outside the distribution of pre-trained colorization models, we use Factorized Diffusion  to colorize, which samples a diffusion model while projecting the noisy intermediate images such that they equal \(}\) when turned into grayscale. In doing so, the denoising process synthesizes only the "color component" of the sampled image, while the "grayscale component" is constrained to equal the generated spectrogram. Note that this method is similar to prior work [63; 106; 114; 23]. We choose this particular method due to its simplicity.

## 4 Experiments

We evaluate our methods using quantitative metrics and human studies. We also present qualitative comparisons and an analysis of our method, and why it works.

### Implementation Details

Models.We select a pair of off-the-shelf latent diffusion image and audio models that share the same latent space, encoder, and decoder. For the image model, we use Stable Diffusion v1.53. For the audio model, we use Auffusion4, which finetunes Stable Diffusion v1.5 on log-mel spectrograms. To synthesize audio from the log-mel spectrograms, we consider two options: following  and using off-the-shelf HiFi-GAN  vocoder, or the Griffin-Lim algorithm [92; 47]. We use HiFi-GAN for our main experiments. In Sec. 4.5, we evaluate the choice of vocoder and verify that our resultant waveforms do indeed encode to a visually interpretable spectrogram.

Hyperparameters.We begin the reverse process with random latent noise \(_{T}^{4 32 128}\), the same shape that Auffusion was trained on. Despite the image model not being trained on this specific size, we found that it nevertheless produces visually appealing results. We set the classifier guidance scales \(_{v}\) and \(_{a}\) to be between 7.5 and 10 and denoise the latents for 100 inference steps with warm-start parameters of \(t_{a}=1.0,t_{v}=0.9\) to preserve audio priors. We decode the latent variables into images of dimension \(3 256 1024\). By averaging across each channel, we obtain spectrograms corresponding to 10 seconds of audio. We re-normalize the spectrograms for visualization.

Baselines.As there is no previous work in this domain, we propose two baseline approaches. The first, inspired by Diffusion Illusions of Burgert _et al._, uses multimodal score distillation sampling (SDS). We optimize a single-channel image \(=g()\), where \(g\) is an implicit function parameterized by \(\), using two SDS losses: one from the image diffusion model \(_{v}\) and the other from the audio diffusion model \(_{a}\). This results in a gradient of:

\[_{}_{}(=g())= _{}_{t,}[_{v}(t)(_{v}^{(t)}-)}{} ]+_{t,}[_{a}(t)(_{a}^{(t )}-)}{}],\] (7)

where \(_{}\) is the weight of the image SDS gradient and \(\) is the noise added to the image or latents. We implement this with pixel-based diffusion model DeepFloyd IF , as we find it performs better than Stable Diffusion with the SDS loss, and Auffusion . This model thus does not require a shared latent space between vision and audio. We refer to this baseline as the _SDS_.

The second baseline involves taking existing images and subtracting them from existing spectrograms, multiplied by some scaling factor, inspired by . This works when the spectrograms have high power, as the subtraction does not significantly affect the audio but still imprints an image into the spectrogram. We obtain spectrograms and images for this baseline via Auffusion and Stable Diffusion. This approach, which we call _imprint_, is simple but can be surprisingly effective. All methods use the same vocoder and post-processing for fairness. Please see Appendix A.3 for more details.

### Quantitative Evaluation

We start by quantitatively evaluating the quality of our generated _images that sound_, examining how well the generated examples match the provided text prompts for each modality.

Experimental setup.Following the evaluation of Visual Anagrams , we create two sets of text prompt pairs. We randomly select 5 discrete (onset-based) and 5 continuous sound category names from VGGSound Common  as audio prompts. We randomly chose 5 objects and 5 scene classes for image prompts, formatted as "a painting of [class], grayscale". This yields a total of 100 prompt pairs. We report Stable Diffusion and Auffusion performances as single-modality

   Method & Modality & CLIP (\%) \(\) & CLAP (\%) \(\) & FID \(\) & FAD \(\) \\  Stable Diffusion  & \(\) & **34.5**\(\,\)(\( 0.1\)) & 2.2 (\( 0.2\)) & – & 41.74 \\ Auffusion  & \(\) & 22.5 (\( 0.1\)) & **48.3**\(\,\)(\( 0.6\)) & 290.29 & – \\  Imprint & \(\) \(\&\)\(\) & 27.2 (\( 0.2\)) & 32.3 (\( 1.0\)) & 244.84 & 29.42 \\ SDS & \(\) \(\&\)\(\) & 25.4 (\( 0.2\)) & **23.4**\(\,\)(\( 1.4\)) & 273.03 & 32.57 \\ Ours & \(\) \(\&\)\(\) & **28.2**\(\,\)(\( 0.1\)) & **33.5**\(\,\)(\( 0.9\)) & **226.46** & **19.21** \\   

Table 1: **Quantitative evaluation on images that sound. We report CLIP, CLAP, FID, and FAD metrics, along with 95% confidence intervals shown in gray. The best results are highlighted in bold.**

   Baseline & Metric & bell/castle & bark/dog & birds/garden & meow/kitten & racecar/racecar & tiger/tiger & train/train & Average \\   & audio quality & 53.1 (\( 1.9\)) & **69.4**\(\,\)(\( 4.2\)) & 95.9 (\( 0.5\)) & 75.5 (\( 3.7\)) & 88.8 (\( 2.0\)) & 70.4 (\( 4.1\)) & 88.8 (\( 2.0\)) & **77.4**\(\,\)(\( 0.7\)) \\  & visual quality & 60.2 (\( 1.7\)) & 51.0 (\( 0.9\)) & 98.0 (\( 0.3\)) & 32.7 (\( 0.4\)) & 69.4 (\( 2.4\)) & 68.4 (\( 4.5\)) & 94.9 (\( 1.0\)) & **67.8**\(\,\)(\( 0.8\)) \\  & alignment & 58.2 (\( 4.5\)) & 63.3 (\( 0.4\)) & 93.9 (\( 1.1\)) & 62.2 (\( 4.7\)) & 82.7 (\( 2.8\)) & 59.2 (\( 4.5\)) & 91.8 (\( 1.5\)) & **73.0**\(\,\)(\( 0.8\)) \\   & audio quality & 82.1 (\( 3.0\)) & 73.7 (\( 3.9\)) & 53.7 (\( 5.0\)) & 54.7 (\( 0.5\)) & 86.3 (\( 2.4\)) & 85.3 (\( 2.5\)) & 85.3 (\( 2.5\)) & **74.4**\(\,\)(\( 0.7\)) \\  & visual quality & 92.6 (\( 1.4\)) & 86.3 (\( 2.4\)) & 66.3 (\( 4.5\)) & 68.4 (\( 3.3\)) & 66.3 (\( 4.5\)) & 77.9 (\( 3.5\)) & 56.8 (\( 4.9\)) & **73.5**\(\,\)(\( 0.8\)) \\  & alignment & 88.4 (\( 2.1\)) & 87.4 (\( 2.2\)) & 60.0 (\( 1.5\)) & 65.3 (\( 0.6\)) & 86.3 (\( 2.4\)) & 80.0 (\( 3.2\)) & 85.3 (\( 2.5\)) & **78.9**\(\,\)(\( 0.6\)) \\   

Table 2: **Human study. We show win-rates of our spectrograms against those generated by the SDS and _imprint_ baselines. The first row indicates which audiovisual prompt pair is evaluated, formatted as [audio prompt] /[visual prompt], with the last column being the average of all seven prompt pairs. Note that 50% win-rate is chance performance, and as such our method outperforms the baselines in the vast majority of cases. Also note that this is a _best-case_ evaluation – please see Sec. 4.3 for details. All results reported are % win-rate against the baseline with a 95% confidence interval in gray (\(N=100\)).**benchmarks to establish upper and lower bounds. We generate 10 samples for each prompt pair, except for the SDS baseline, for which we generate 4 samples due to its slower speed.

Evaluation metric.Following , we use the CLIP  score to measure the alignment between spectrograms and image text prompts, and analogously we use the CLAP  score to evaluate the alignment of audio with audio text prompts. An ideal method should excel at both simultaneously. We also report FID  and FAD  to evaluate the quality of generated examples where we use the results of Stable diffusion and Auffusion as reference sets respectively.

Results.We show our quantitative results in Tab. 1. Our method outperforms baselines across all metrics and performs comparably to single-modality models, which serve as rough upper bounds for each modality. This demonstrates our approach's ability to generate meaningful _images that sound_, sampling from the intersection of natural image and spectrogram distributions. Stable Diffusion achieves a low CLAP score, indicating how poorly a randomly sampled natural image acts as a spectrogram. We observe that the SDS baseline often fails to optimize both modalities together. In contrast, our method achieves a higher success rate and generates more diverse results. Our method is significantly faster, generating one sample in 10 seconds compared to the SDS baseline's 2-hour optimization time using NVIDIA L40s. The _imprint_ baseline imprints the image onto the spectrogram, potentially degrading the sound pattern and leading to a lower CLAP score. Note that FID and FAD are distribution-based metrics, and as our task focuses on generating examples that lie in a small subset of the natural image and spectrogram distribution, higher FID scores, in general, are expected.

### Human Studies

Experimental setup.We also perform two-alternative forced choice (2AFC) studies to evaluate our results. We construct seven paired text prompts by hand, ensuring semantic correlations between image and audio prompts, such as pairing a visual of dogs with the sound of dogs barking. Using these prompts, we generate samples using our method, the SDS baseline, and the _imprint_ baseline, and hand-pick the best examples for evaluation. This _best-case evaluation_ is useful as participants from MTurk are not expected to have prior knowledge about spectrograms, let alone domain expertise. Moreover, this evaluation matches the intended use case of our method, in which a user repeatedly queries the model for a result that they prefer based on artistic merit and quality. Participants, are presented with one sample from our method, and a corresponding sample from a baseline, and are

Figure 4: **Qualitative comparison. We show our qualitative results along with the _imprint_ and SDS baselines given visual (first) and audio (second) prompts. Please zoom in for better viewing.**

[MISSING_PAGE_EMPTY:8]

[MISSING_PAGE_FAIL:9]

Guidance scale.We also explore different guidance scales \(_{v}\) and \(_{a}\) for our method. We present results in Tab. 3. We find that higher guidance scales generally yield better results on both modalities. We hypothesize that the higher guidance scales more strongly encourage the sample to come from the "intersection" of the conditional spectrogram and conditional image distributions, resulting in better alignment with both text prompts.

## 5 Discussion and Conclusion

In this work, we demonstrate that, perhaps surprisingly, there is a non-trivial overlap between the distribution of natural images and the distribution of natural spectrograms. We show this by sampling from the intersection of these two distributions, resulting in spectrograms that look like real images but also sound like real sounds. The method we proposed is simple and zero-shot, and leverages the compositional nature of diffusion with cross-modal models. We see our work as advancing multimodal compositional generation and opening up new possibilities for multimodal art.

Limitations.One limitation of our method is that it cannot generate examples that have both high-fidelity audio and image. We show failure cases, which occur for many prompts, in Fig. 9. Some of these failures may be due to the strict constraints of the problem, since realistic examples may not always exist at the intersection of both distributions. Our method is also limited by the quality of the audio diffusion model, whose performance lags behind that of visual models.

Potential negative societal impacts.The image and audio generation models that our method leverages are becoming progressively more powerful, and care must be taken in their deployment. Moreover, our method could potentially be used for steganography, secretly embedding images within audio. This capability may be used for deception, and we believe it deserves further consideration.

Acknowledgements.We thank Ang Cao, Linyi Jin, Jeongsoo Park, Chris Donahue, Alexei Efros, Prem Seetharaman, Justin Salamon, Julie Zhu, and John Granzow for their helpful discussions. This project is supported by the Sony Research Award and Cisco Systems. Daniel is supported by the National Science Foundation Graduate Research Fellowship under Grant No. 1841052.