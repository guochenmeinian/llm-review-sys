# ANTN: Bridging Autoregressive Neural Networks and Tensor Networks for Quantum Many-Body Simulation

Zhuo Chen\({}^{123}\) **Laker Newhouse\({}^{4}\)1 Eddie Chen\({}^{3}\)2 Di Luo\({}^{1235}\)3 Marin Soljacic\({}^{13}\)**

\({}^{1}\)NSF AI Institute for Artificial Intelligence and Fundamental Interactions

\({}^{2}\)Center for Theoretical Physics, Massachusetts Institute of Technology

\({}^{3}\)Department of Physics, Massachusetts Institute of Technology

\({}^{4}\)Department of Mathematics, Massachusetts Institute of Technology

\({}^{5}\)Department of Physics, Harvard University

{chenzhuo,lakern,ezchen,diluo,soljacic}@mit.edu

###### Abstract

Quantum many-body physics simulation has important impacts on understanding fundamental science and has applications to quantum materials design and quantum technology. However, due to the exponentially growing size of the Hilbert space with respect to the particle number, a direct simulation is intractable. While representing quantum states with tensor networks and neural networks are the two state-of-the-art methods for approximate simulations, each has its own limitations in terms of expressivity and inductive bias. To address these challenges, we develop a novel architecture, Autoregressive Neural TensorNet (ANTN), which bridges tensor networks and autoregressive neural networks. We show that Autoregressive Neural TensorNet parameterizes normalized wavefunctions, allows for exact sampling, generalizes the expressivity of tensor networks and autoregressive neural networks, and inherits a variety of symmetries from autoregressive neural networks. We demonstrate our approach on quantum state learning as well as finding the ground state of the challenging 2D \(J_{1}\)-\(J_{2}\) Heisenberg model with different systems sizes and coupling parameters, outperforming both tensor networks and autoregressive neural networks. Our work opens up new opportunities for quantum many-body physics simulation, quantum technology design, and generative modeling in artificial intelligence.

## 1 Introduction

Quantum many-body physics is fundamental to our understanding of the universe. It appears in high energy physics where all the fundamental interactions in the Standard Model, such as quantum electrodynamics (QED) and quantum chromodynamics (QCD), are described by quantum mechanics. In condensed matter physics, quantum many-body physics has led to a number of rich phenomena and exotic quantum matters, including superfluids, superconductivity, the quantum Hall effect, and topological ordered states (Girvin & Yang, 2019). As an application, quantum many-body physics is crucial for new materials design. The electronic structure problem and chemical reactions in quantum chemistry are governed by quantum many-body physics. The recent development of quantum computers is also deeply connected to quantum many-body physics. A multi-qubit quantum device is intrinsically a quantum many-body system, such that progress on quantum computer engineering is tied to our understanding of quantum many-body physics (Preskill, 2021).

All information of a closed quantum many-body system is captured by the wavefunction, whose properties are described by the famous Schrodinger equation. An important tool to study and understand quantum many-body physics is to classically simulate the wavefunction. However, the wavefunction is a high dimensional function in Hilbert space, whose dimension grows exponentially with the number of particles. For example, for qubit systems where each qubit has two degrees of freedom), the wavefunction of 300 qubits will have dimension \(2^{300}\), which is larger than the number of atoms in the observable universe. Furthermore, the Schrodinger equation is a complex-valued high-dimensional equation, which is challenging to solve or simulate in general.

A number of algorithms have been developed to simulate quantum many-body physics, including quantum Monte Carlo, tensor networks, neural network quantum states, and quantum computation. In particular, computing the ground state of quantum many-body systems is of great interest. One important approach is the variational principle, which provides an upper bound for the ground state energy. To apply the variational principle successfully, one must design an ansatz that can represent and optimize the wavefunction efficiently. Tensor networks and neural network quantum states (Carleo and Troyer, 2017) are the two main state-of-the-art methods that can be applied with the variational principle for quantum many-body simulation. However, tensor networks usually suffer from an expressivity issue in systems with more than one dimension, while neural network quantum states usually lack inductive bias from the underlying physics structure and have sign structure challenges in the representation.

In this paper, we develop a novel architecture, Autoregressive Neural TensorNet (ANTN), to bridge neural network quantum states and tensor networks, achieving the best of both worlds. In particular, our contributions are threefold:

* Develop ANTN with two variants called "elementwise" and "blockwise," which each naturally generalize the two state-of-the-arts ansatzes, tensor networks (TN) and autoregressive neural networks (ARNN), to provide proper inductive bias and high expressivity.
* Prove that ANTN is normalized with exact sampling, has generalized expressivity of TN and ARNN, and inherits multiple symmetries from ARNN.
* Demonstrate our methods on quantum state learning and variationally finding the ground state of the challenging 2D \(J_{1}\)-\(J_{2}\) Heisenberg model, outperforming both TN and ARNN.

## 2 Related Work

**Tensor Networks (TN)** represent high-dimensional wavefunctions using low-rank tensor decomposition, notably matrix product state (MPS) (Vidal, 2003, 2004), PEPS (Verstraete and Cirac, 2004), and MERA (Vidal, 2007). They capture the entanglement structure of physical systems and, with algorithms like the density matrix renormalization group (DMRG) (White, 1992), are used for state simulations and real-time dynamics. However, their expressivity can be limited in systems with more than one dimension and systems with high entanglement. In machine learning, TN appears as tensor train (Oseledets, 2011) and CP decomposition methods.

**Neural Network Quantum State (NNQS)** leverages neural networks for high dimensional wavefunction representation (Carleo and Troyer, 2017). It's been demonstrated that many quantum states can be approximated or represented by NNQS (Sharir et al., 2021; Gao and Duan, 2017; Lu et al., 2019; Levine et al., 2019; Luo et al., 2021, 2019; Deng et al., 2017; Huang and Moore, 2021; Vieijra et al., 2020), and it has yielded state-of-the-art results in computing quantum system properties (Gutierrez and Mendl, 2020; Schmitt and Heyl, 2020; Vicentini et al., 2019; Yoshioka and Hamazaki, 2019; Hartmann and Carleo, 2019; Nagy and Savona, 2019; Luo et al., 2021, 2022). Key advancements in NNQS include ARNN that improves sample efficiency and gradient estimation, and the development of

Figure 1: Diagrammatic representation of autoregressive neural network (ARNN), tensor network (TN) and our Autoregressive Neural TensorNet (ANTN).

neural networks that adhere to the underlying symmetries of quantum systems (Luo et al., 2021; Hibat-Allah et al., 2020; Choo et al., 2019; Luo and Clark, 2019; Hermann et al., 2019; Pfau et al., 2020; Luo et al., 2021, 2022b, 2021b; Chen et al., 2022). Despite its potential, NNQS faces challenges such as the lack of physics prior and the sign structure problem. While there are attempts to integrate NNQS with TN, including matrix product state with neural network backflow (Lami et al., 2022) and generalizing MPS to RNN (Wu et al., 2022), the former can not produce normalized wavefunctions, while the latter only builds on tensor contractions that lacks the nonlinear activation functions of standard neural network wavefunctions.

## 3 Background

### Quantum Preliminaries

In this work, we focus on qubit systems in quantum many-body physics. The wavefunction or quantum state \(\) of the system is a normalized function \(:_{2}^{n}\) with \(_{}|()|^{2}=1\), where \(n\) is the system size. The input to the wavefunction is an \(n\)-bit string \(\{0,1\}^{n}\). Therefore, the wavefunction \(\) can be viewed as a complex-valued vector of size \(2^{n}\), with Dirac notation \(|\) and \(|\) correspond to a conjugate row vector and a column vector respectively, and \(()=|\). Because the size of the wavefunction grows exponentially with the system size \(n\), a direct computation quickly becomes intractable as the system size increases. The goal of NNQS is to design a compact architecture that can approximate and optimize the wavefunction efficiently.

### Quantum State Learning

Given a quantum state \(|\), we are often interested in finding \(|_{}\) that is closest to \(|\) given the variational ansatz. In quantum mechanics, the closeness of two quantum states is measured by the quantum fidelity \(F=||_{}|^{2}=|_{ {x}}^{*}()_{}()|^{2}\) where \({}^{*}\) refers to complex conjugation. Quantum fidelity satisfies \(0 F 1\), with \(F=1\) corresponding to exact match, and \(F=0\) orthogonal quantum states. Finding \(F_{}\) for a given ansatz allows us to quantify how good the ansatz can be used to approximate that state. In practice, minimizing \(- F\) is usually better than maximizing \(F\) itself. This can be achieved by enumerating all the basis \(\) in small systems and stochastically in large systems such as quantum state tomography (see Appendix A.1).

### Variational Monte Carlo

For a given quantum system with \(n\) qubits, the Hamiltonian \(}\) can be written as a Hermitian matrix of size \(2^{n} 2^{n}\). The ground state energy \(E_{g}\) of the system is the smallest eigenvalue of \(}\) and the ground state is the corresponding eigenvector. For large system sizes, finding the ground state directly is usually impossible. In this case, the variational principle in quantum mechanics provides an upper bound on the ground state energy \(E_{g}\). For all (normalized) wavefunctions \(|\), it is evidental that \(E_{g}|}|\). Therefore, finding the \(|_{}\) that minimizes \(E_{}=_{}|}|_{}\) gives the lowest upper bound of the ground state energy. For large system sizes, we can stochastically evaluate and minimize

\[_{}|}|_{}=_{ ^{}}|_{}()|^{2}_{,^{}}_{}(^{})}{_{}( )}=_{|_{}|^{2}}^{ }}_{,^{}}_{}(^{})}{ _{}()},\] (1)

where \(_{,^{}}\) refers to the matrix element of \(}\) and we interpret \(|_{}()|^{2}\) as a probability distribution. The summation over \(^{}\) can be efficiently computed given \(\) since the Hamiltonian is usually sparse. The gradient \(_{}E_{}\) can also be calculated stochastically in a similar fashion (see Appendix A.2).

## 4 Method

### Preliminaries

**Autoregressive Neural Network Wavefunction.** The autoregressive neural network (ARNN) (Fig. 1 left) parameterizes the full probability distribution as a product of conditional probability distributionsas

\[p()=_{i=1}^{n}p(x_{i}|_{<i}),\] (2)

where \(=(x_{1},,x_{n})\) is a configuration of \(n\) qubits and \(_{<i}=(x_{1},,x_{i-1})\) is any configuration before \(x_{i}\). The normalization of the full probability can be guaranteed from the normalization of individual conditional probabilities. The ARNN also allows for exact sampling from the full distribution by sampling sequentially from the conditional probabilities (see Appendix B.1). The autoregressive constructions for probabilities can be easily modified to represent quantum wavefunction by (1) replacing \(p(x_{i}|_{<i})\) with a complex-valued conditional wavefunction \((x_{i}|_{<i})\) and (2) using the following normalization condition for the conditional wavefunctions: \(_{x}|(x_{i}|_{<i})|^{2}=1\)(Sharir et al., 2020; Hibat-Allah et al., 2020; Luo et al., 2021b). Similar to the case of probabilities, ARNN automatically preserves the normalization of wavefunctions and allows for exact sampling (see Appendix B.1). Because of this, ARNN is often more efficient when training and computing various quantum observables compared to other generative neural networks.

**Matrix Product State (MPS).** MPS (also known as tensor train) is a widely used TN architecture to study quantum many-body physics (Vidal, 2003, 2004). The MPS defines a wavefunction using \(n\) rank-3 tensors \(M_{x_{i}}^{_{i-1}_{i}}\) with \(_{i-1}\) (or \(_{i}\)) the index of the left (or right) bond dimensions and \(x_{i}\) the configuration of the \(i\)th qubit. Then, the full wavefunction is generated by first choosing a particular configuration \(=(x_{1},,x_{n})\) and then contracting the tensors selected by this configuration (Fig. 1 right) as

\[()=_{_{1},,_{n-1}}M_{x_{1}}^{_{0} _{1}} M_{x_{n}}^{_{n-1}_{n}}=_{}_{i=1}^ {n}M_{x_{i}}^{_{i-1}_{i}},\] (3)

where the left-and-right-most bond dimensions are assumed to be \((_{0})=(_{n+1})=1\) so are not summed.

### Autoregressive Neural TensorNet (ANTN)

**Elementwise and Blockwise Construction for ANTN.** Both ARNN and MPS are powerful ansatzes for parameterizing quantum many-body wavefunctions. Albeit very expressive, ARNN lacks the physics prior of the system of interest. In addition, since wavefunctions are complex-valued in general, learning the sign structure can be a nontrivial task (Westerhout et al., 2020). MPS, on the other hand, contains the necessary physics prior and can efficiently represent local or quasi-local sign structures, but its expressivity is severely limited. The internal bond dimension needs to grow exponentially to account for a linear increase in entanglement. It turns out that MPS representation is not unique in that many MPS actually represent the same wavefunction; and if we choose a particular constraint (without affecting the expressivity), MPS allows for efficient evaluation of conditional probability and exact sampling (Ferris and Vidal, 2012) (see Appendix B.2). Because both these ansatzes allow for exact sampling, it is natural to combine them to produce a more powerful ansatz. Therefore, we develop the Autoregressive Neural TensorNet (ANTN) (Fig. 1 (middle)). In the last layer of the ANTN, instead of outputting the conditional wavefunction \((x_{i}|_{<i})\), we output a conditional wavefunction tensor \(^{_{i-1}_{i}}(x_{i}|_{<i})\) for each site. Defining the left partially contracted tensor up to qubit \(j\) as \(^{_{j}}_{L}(_{ j}):=_{_{<j}}_{i =1}^{j}^{_{i-1}_{i}}(x_{i}|_{<i})\), we can define the (unnormalized) marginal probability distribution as

\[q(_{ j}):=_{_{j}}^{_{j}}_{L}(_{  j})^{_{j}}_{L}(_{ j})^{*},\] (4)

where \({}^{*}\) denotes complex conjugation. Then, the (normalized) conditional probability can be obtained as \(q(x_{j}|_{<j})=q(_{ j})/_{x_{j}}q(_{ j})\). We construct the overall wavefunction by defining both its amplitude and phase according to

\[():=)}e^{i()},\] (5)

with \(q()=:_{i=1}^{n}q(x_{i}|_{<i})\) and the phase \(()=:_{}_{i=1}^{n}^{_ {i-1},_{i}}(x_{i}|_{<i})\). In other words, we define the amplitude of the wavefunction through the conditional probability distributions and define the phase analogous to the standard MPS.

We develop two different constructions for ANTN that differ in the last layer on how to construct conditional wavefunction tensors.

The elementwise ANTN is given by

\[^{_{i-1}_{i}}(x_{i}|_{<i})=M_{x_{i}}^{_{i-1} _{i}}+f_{NN}(x_{i},_{i-1},_{i}|_{<i}),\] (6)

where \(f_{NN}(x_{i},_{i-1},_{i}|_{<i})\) is the complex-valued output. The blockwise ANTN is given by

\[^{_{i-1}_{i}}(x_{i}|_{<i})=M_{x_{i}}^{_{i- 1}_{i}}+f_{NN}(x_{i}|_{<i}),\] (7)

where the complex-valued output \(f_{NN}(x_{i}|_{<i})\) is broadcasted over \(\). This results in a lower complexity that allows us to use a larger maximum bond dimension.

**Transformer and PixelCNN Based ANTN.** Our construction above is general and can be applied to any standard ARNN. Depending on the application, we can use different ARNN architectures. In this work, we choose the transformer and PixelCNN depending on the specific tasks. The transformer (Vaswani et al., 2017) used here is similar to a decoder-only transformer implemented in (Luo et al., 2020), and The PixelCNN we use is the gated PixelCNN (Van den Oord et al., 2016) implemented in (Chen et al., 2022).

**MPS Initialization.** Since our ANTN generalizes from TN, both the elementwise and the blockwise can take advantage of the optimized MPS from algorithms such as DMRG (White, 1992) as an initialization. In practice, we can initialize the TN component with the optimized DMRG results of the same bond dimension (similar to (Lami et al., 2022; Wu et al., 2022)). The MPS Initialization can also be thought of as a pretraining of the ANTN. This is a nice feature since it provides a good sign structure and inductive bias from the physics structure, which does not exist in the conventional ARNN.

**Limitations.** In this work, we only integrated MPS into the ANTN, where MPS may not be the best TN in various settings. Besides MPS, many other TNs also allow efficient evaluation of conditional probabilities and exact sampling, such as MERA (Vidal, 2007) and PEPS (Verstraete and Cirac, 2004), or cylindrical MPS for periodic boundary conditions. In fact, the recently developed TensorRNN (Wu et al., 2022) can also be viewed as a type of TN and can be integrated into our construction for future work. In addition, while the ANTN construction is general in terms of the base ARNN used, our current study only focuses on transformer and PixelCNN. Lastly, as shown later in Sec. 5.1, the current implementation of ANTN has an additional sampling overhead that is linear in system size which can be avoided.

## 5 Theoretical Results

### Exact Sampling and Complexity Analysis of ANTN

**Theorem 5.1**.: _Autoregressive Neural TensorNet wavefunction is automatically normalized and allows for exact sampling._

Proof.: This is a direct consequence that we defined the amplitude of the wavefunction through normalized conditional probability distributions \(q(x_{i}|_{<i})\). (See Appendix B.1 for the detailed sampling procedure.) 

**Complexity Analysis.** We first note that for MPS, the number of parameters and computational complexity for evaluating a bitstring \(\) scales as \((n^{2})\), where \(n\) is the number of particles and \(=()\) is the (maximum) bond dimension of MPS. The sampling complexity scales as \((n^{2}^{2})\). The DMRG algorithm has a computational complexity of \((n^{3})\)(White, 1992). The number of parameters and computational complexity of ARNN depends on the specific choice. Assuming the ARNN has \(n_{}\) parameters and a single pass (evaluation) of the ARNN has a computational complexity of \(c_{}\), then the sampling complexity of the ARNN is \((nc_{})\) in our current implementation. The ANTN based on such ARNN would have \((n_{}+n^{2}h_{})\) parameters for the elementwise construction, and \((n_{}+n^{2}+nh_{})\) parameters for the blockwise construction. The computational complexity for evaluating and sampling bitstrings scales as \((n^{s}(c_{}+n^{2}h_{}))\) for elementwise construction and \((n^{s}(c_{}+n^{2}))\) for blockwise construction with \(s=0\) for evaluation and \(s=1\) for sampling.

We note that the complexity analysis above is based on our current implementation, where the sampling procedure for all algorithms has an \((n)\) overhead compared to evaluation. It is possible to remove it by storing the partial results during the sampling procedure.

Then, each gradient update of the variational Monte Carlo algorithm requires sampling once and evaluating \(N_{}\) times where \(N_{}\) is the number of connected (non-zero) \(^{}\)'s in \(_{,^{}}\) given \(\), which usually scales linearly with the system size. Then, the overall complexity is \((N_{s}(N_{}c_{}+c_{}))\) with \(N_{s}\) the batch size, \(c_{}\) the evaluation cost and \(c_{}\) the sampling cost. Usually, the first part dominates the cost.

The difference in computational complexities and number of parameters between elementwise ANTN and blockwise ANTN implies that blockwise ANTN is usually more economical than elementwise ANTN for the same hidden dimsnieen \(h_{}\) and bond dimension \(\). Therefore, for small bond dimensions, we use the elementwise ANTN for a more flexible parameterization with a higher cost. In contrast, for large bond dimensions, we use the blockwise ANTN, which saves computational complexity and improves the initial performance (with DMRG initialization) of the blockwise ANTN at the cost of less flexible modifications from the ARNN. We also note that compared to the state-of-the-art MPS simulation, even for our blockwise ANTN, a small bond dimension usually suffices. For example, in the later experimental section, we use bond dimension 70 for blockwise ANTN while the best MPS results use bond dimension 1024, which has much more parameters than our ANTN. In general, our ANTN has much fewer parameters compared to MPS due to the \((^{2})\) parameters scaling which dominates at large bond dimension.

### Expressivity Results of ANTN

**Theorem 5.2**.: _Autoregressive Neural TensorNet can have volume law entanglement, which is strictly beyond the expressivity of matrix product states._

Proof.: We proved in Thm. 5.3 that ANTN can be reduced to an ARNN, which has been shown to have volume law entanglement that cannot be efficiently represented by TN (Sharir et al., 2021; Levine et al., 2019). Hence, ANTN can represent states that cannot in general be represented by MPS efficiently; thus it has strictly greater expressivity. 

**Theorem 5.3**.: _Autoregressive Neural TensorNet has generalized expressivity over tensor networks and autoregressive neural networks._

Proof.: Thm B.2of Appendix B.4 shows that both TN and ARNN are special cases as ANTN and Thm B.3of Appendix B.4 shows that ANTN can be written as either a TN or an ARNN with an exponential (in system size) number of parameters. Thus, ANTN generalizes the expressivity over both TN and ARNN. 

### Symmetry Results of ANTN

Symmetry plays an important role in quantum many-body physics and quantum chemistry. Many of the symmetries can be enforced in ARNN via the two classes of the symmetries--_mask symmetry_ and _function symmetry_.

**Definition 5.1** (Mask Symmetry).: A conditional wavefunction \((x_{i}|_{<i})\) has a _mask symmetry_ if \((x_{i}|_{<i})=0\) for some \(x_{i}\) given \(_{<i}\).

**Definition 5.2** (Function Symmetry).: A conditional wavefunction tensor \((x_{i}|_{<i})\) has a _function symmetry_ over a function \(F\) if \((x_{i}|_{<i})=(F(x_{i};_{<i})|(_{<i}))\) where \((_{ i}):=\{F(x_{1}),F(x_{2};x_{1}),,F(x_{i};x_{<i})\}\).

Here, we list several symmetries that ANTN inherits from ARNN and show the proofs in Appendix B.4.

**Theorem 5.4**.: _Autoregressive Neural TensorNet inherits mask symmetry and function symmetry from autoregressive neural networks._

**Corollary 5.4.1** (Global U(1) Symmetry).: _Autoregressive Neural TensorNet can realize global U(1) symmetry, which conserves particle number._

**Corollary 5.4.2** (\(_{2}\) Spin Flip Symmetry).: _Autoregressive Neural TensorNet can realize \(_{2}\) spin flip symmetry such that the wavefunction is invariant under conjugation of the input._

**Corollary 5.4.3** (Discrete Abelian and Non-Abelian Symmetries).: _Autoregressive Neural TensorNet can realize discrete Abelian and Non-Abelian symmetries._

## 6 Experiments

### Quantum State Learning

As stated previously, ANTN generalizes both ARNN and TN to take advantage of both the expressivity and the inductive bias. Here, we test the ANTN on learning physically important quantum states to demonstrate this ability. In this task, we use the transformer neural network for ARNN and ANTN due to the 1D structure of the system.

**Experiments on expressivity.** We first test the expressivity of the ANTN by learning a class of well-known high-entangled states--random permuted Bell states. A 2-qubit Bell state is defined as \((|00+|11)/\), which has 1 bit of entanglement. For a system size of \(n\), we randomly pair qubits in the first half of the system with the qubits in the second half of the system to be Bell states. This process creates quantum states with \(n/2\) bit of entanglement between the first and second half of the system. It can be shown that a bond dimension of \(2^{n/2}\) is required for MPS to fully represent such a system. In Fig. 2 (a), we plot the quantum fidelity of learning 16-qubit random Bell states. As the figure shows, MPS cannot represent the states without reaching the required bond dimension of \(256=2^{8}\). The ARNN and ANTN, on the other hand, can represent such states without limitations from the bond dimension.

**Experiments on inductive bias.** We then test the physics inductive bias of the ANTN. One of the physics inductive biases of MPS is that it can represent wavefunctions with local or quasi-local sign structures that can be hard for neural networks to learn. These wavefunctions can have a fluctuating sign depending on the configuration \(\). Here, we use (real-valued) shallow random circuits to mimic the short-range interaction and generate states with sign structures. We test the algorithms on these states both with and without the "sign rule", which means that we explicitly provide the sign structure to the algorithm. As shown in Fig. 2 (b), the fidelity of MPS only depends weakly on the sign rule, whereas the fidelity of ARNN can change drastically. Our ANTN inherits the property of MPS and is thus not affected by the sign structure. Furthermore, being more expressive, ANTN with a bond dimension of 4 already performs better than MPS with a bond dimension of 16.

### Variational Monte Carlo

We further test our algorithm on finding the ground state of the challenging 2D \(J_{1}\)-\(J_{2}\) Heisenberg model with open boundary condition. The model has a rich phase diagram with at least three different phases across different \(J_{2}/J_{1}\) values (Capriotti et al., 2004) (Lante & Parola, 2006). In addition, the

Figure 2: Fidelity \(\) on quantum state learning with 16 qubits for TN (MPS), ARNN (transformer) and ANTN (elementwise construction with transformer+MPS). (a) Learning random Bell states. (b) Learning real-valued depth-4 random circuit with and without sign rule. The error bar denotes the standard deviation (not the standard error of the mean) of the fidelities over the random states sampled from the corresponding distribution. The mean and standard deviation are calculated from 10 random states. The numbers inside the parentheses denote the bond dimension.

complicated structure of its ground state makes it a robust model on which to test state-of-the-art methods. Here, we use the PixelCNN for ARNN and ANTN due to the 2D geometry of the system.

The 2D \(J_{1}\)-\(J_{2}\) Hamiltonian is given by

\[}=J_{1}_{ i,j}}}+J_{2}_{ i,j}} },\] (8)

where subscript refers to the site of the qubits, \(,\) is the nearest neighbour and \(,\) is the next nearest neighbour. \(}}=X_{i} X_{j}+Y_{i} Y_{j}+Z_ {i} Z_{j}\) with \(X\), \(Y\) and \(Z\) the Pauli matrices

\[X=0&1\\ 1&0, Y=0&-i\\ i&0, Z=1&0\\ 0&-1.\] (9)

We will fix \(J_{1}=1\) and vary \(J_{2}\) in our studies.

**Experiments on inductive bias of \(J_{1}\)-\(J_{2}\) Heisenberg Model.**

As shown previously, it can be challenging for neural networks to learn the sign structures. The ground state of the \(J_{1}\)-\(J_{2}\) model also has a sign structure, which can be partially captured by the Marshall sign rule(Marshall, 1955). The sign rule is exact at \(J_{2}=0\) and becomes worse for large \(J_{2}\). This could introduce bias to the neural network wavefunction if directly applied. We compare the restricted Boltzmann machine (RBM) (from NetKet (Vicentini et al., 2022)), our implementation of gated PixelCNN, and the two different constructions of ANTN with and without the sign rule.

   \\  Algorithms & \(J_{2}=0.2\) & \(J_{2}=0.3\) & \(J_{2}=0.4\) & \(J_{2}=0.5\) & \(J_{2}=0.6\) & \(J_{2}=0.7\) & \(J_{2}=0.8\) \\  RBM (NS) & -1.9609(16) & -1.5128(20) & -1.7508(19) & -1.7605(20) & -1.7508(19) & -1.7508(19) & -1.7508(19) \\ RBM (S) & -2.1914(17) & -1.8625(20) & -0.7267(29) & -1.8925(16) & -1.8726(26) & -1.8726(19) & -1.8726(19) \\ PixelCNN (NS) & -2.2317(19) & -1.8890(219) & -1.8367(226) & -1.8726(19) & -1.8726(19) & -1.8726(19) \\ Elementwise (8 NS) & **-2.23690(4)** & _-1.90318(8)_ & _-2.00036(16)_ & _-2.00036(1)_ & _-2.00026(11)_ \\ Elementwise (8 S) & **-2.3688** & **-1.93102(7)** & _-1.9004(16)_ & _-1.9914(13)_ & _-1.9018(13)_ \\ Blockwise (70 S) & -2.23517(6) & -1.92880(9) & -1.99246(13) & & & \\  

Table 1: Energy per site \(\) for \(8 8\) system with various algorithms where elementwise and blockwise are two constructions of ANTN (with PixelCNN + MPS). The bond dimensions for ANTN are labeled inside the parentheses. For each algorithm, we test it both with the sign rule (S) and without the sign rule (NS) The best energy is highlighted in boldface and the second in italic.

   &  \\  Algorithms & \(J_{2}=0.2\) & \(J_{2}=0.3\) & \(J_{2}=0.4\) & \(J_{2}=0.5\) & \(J_{2}=0.6\) & \(J_{2}=0.7\) & \(J_{2}=0.8\) \\  MPS (8) & -1.997537 & -1.893753 & -1.797675 & -1.734326 & -1.716253 & -1.768225 & -1.869871 \\ MPS (70) & -2.191048 & -2.069029 & -1.956480 & -1.861659 & -1.816249 & -1.854296 & -2.007845 \\ MPS (1024) & -2.255633 & -2.138591 & -2.031681 & -1.938770 & -1.865561 & **-1.894371** & **-2.062730** \\ PixelCNN & -2.22462(24) & -2.12873(14) & -2.02053(14) & -1.74098(29) & -1.78185(27) & -1.81800(13) & -1.98331(17) \\ Elementwise (8) & **-2.26034(6)** & **-2.14450(4)** & **-2.03727(7)** & **-1.94001(6)** & _-1.85684(10)_ & _-1.88643(7)_ & _-2.05707(43)_ \\ Blockwise (70) & **-2.55755** (8) & _-2.14152_ (8) & _-2.03373(10)_ & _-1.93842(12)_ & _-1.85270(12)_ & _-1.87853(13)_ & _-2.05088(14)_ \\   &  &  &  &  &  &  &  \\  Algorithms & \(J_{2}=0.2\) & \(J_{2}=0.3\) & \(J_{2}=0.4\) & \(J_{2}=0.5\) & \(J_{2}=0.6\) & \(J_{2}=0.7\) & \(J_{2}=0.8\) \\ MPS (8) & -1.989207 & -1.887531 & -1.800784 & -1.735906 & -1.720619 & -1.788652 & -1.893916 \\ MPS (70) & -2.185071 & -2.059443 & -1.944832 & -1.851954 & -1.812450 & -1.83650 & -2.030313 \\ MPS (1024) & _-2.264084_ & -2.141043 & -2.027736 & -1.931243 & **-1.858846** & _-1.913483_ & _-2.093013_ \\ PixelCNN & -2.24311(102) & -2.1261The result is shown in Table 1. Since our approach is based on the variational principle discussed in Sec. 3, it provides an upper bound on the exact ground state energy; therefore, a lower energy implies a better state. As shown in the results, both RBM and PixelCNN improve significantly with the application of the sign rule at \(J_{2}=0.2\) and \(J_{2}=0.5\), but the results deteriorate at \(J_{2}=0.8\). This is expected because the sign rule becomes increasingly inaccurate as \(J_{2}\) increases, especially past \(J_{2}=0.5\). Our ANTN, on the other hand, does not require the sign rule in both constructions. As an additional comparison, we note that both of our ANTN constructions achieved better results compared to the recently developed matrix product backflow state (Lami et al., 2022), which uses the sign rule and has a per site energy of \(-1.9267\) at \(J_{2}=0.5\).

**Ablation study on \(J_{1}\)-\(J_{2}\) Heisenberg Model.** We scan across many \(J_{2}\) for the model without using the sign rule. In this experiment, we compare the performance of six models to compute the ground state energy of the \(J_{1}\)-\(J_{2}\) Hamiltonian system with \(J_{1}=1\) fixed and \(J_{2}=0.2\)-\(0.8\) with 0.1 increments, covering three different phases of the model. The first three models are TN models, using the MPS with bond dimensions of 8, 70, and 1024. For up to \(4 4\) system, the MPS results with bond dimension 1024 are exact and can be regarded as a benchmark. The fourth model is a PixelCNN pure neural network; the fifth and sixth models are elementwise and blockwise ANTN models. Thus the experiment compares the ANTN against the two previous state-of-the-art techniques.

Table 2 summarizes the per site ground state energy computed by different models at different \(J_{2}\)'s in three distinct phases. Our results provide strong evidence that ANTN integrating TN and ARNN outperforms each base model, achieving state-of-the-art results.

**Comparison between ANTN and MPS.** In all cases the ANTN surpasses the MPS with the corresponding bond dimension on which they are based. For example, even in the \(10 10\) system with \(J_{2}>0.5\), where MPS with a bond dimension of 1024 outperforms the ANTN, the elementwise ANTN still significantly improves on the base MPS with a bond dimension of 8 and the blockwise ANTN improves on the base MPS with a bond dimension of 70. It is consistent with Theorem. 5.3 and Theorem. 5.2 that ANTN is more expressive than TN.

In addition, the ANTN models scale well for larger systems. Figure 3 visualizes the scalability of ANTN compared to MPS. The figure plots the difference in per site energy between the elementwise ANTN with bond dimension 8 and MPS with bond dimension 1024 for \(J_{2}=0.2,0.5,0.8\) where lower energies signify better performance for the elementwise ANTN. Compared to MPS, the ANTN models compute better ground state energies as the system size grows larger. As the system size increases, the elementwise ANTN with bond dimension 8 starts to outperform MPS with bond dimension 1024. Even at \(J_{2}=0.6\), where the elementwise ANTN is slightly worse than MPS, the difference gets significantly smaller at \(12 12\) compared to \(10 10\) (Table 2). In fact, the elementwise ANTN achieves such a performance using only \( 1\%\) the number of parameters of MPS. We note that for large \(J_{2}\), the system goes into a stripped phase (Nomura and Imada, 2021), which can be less challenging for MPS to represent. Nevertheless, in almost all cases our ANTN still outperforms MPS on the \(12 12\) system. MPS has a cost dominated by the bond dimension (quadratic for memory and cubic for computational), which limits its use in practice for large system sizes that require large bond dimensions. According to the complexity analysis in Sec. 5.1, ANTN has lower complexity than TN and thus scales better for larger systems.

**Comparison between ANTN and ARNN.** The elementwise and blockwise ANTN models also consistently outperform the pure neural network PixelCNN model. This agrees with Theorem 5.3 that both elementwise and blockwise ANTN have a generalized expressivity compared to ARNN. In Appendix C we further compare ANTN with ARNN and find that: (a) it is challenging to surpass ANTN by ARNN even with an increased number of parameters; (b) the improvement from ANTN mainly comes from the effective inductive bias of MPS structure instead of the DMRG initialization; (c) ANTN has a favorable scaling compared to ARNN and MPS for reaching accurate results.

The details of all experiment setups and hyperparameters can be found in Appendix D.

## 7 Conclusion

In this paper, we developed Autoregressive Neural TensorNet, bridging the two state-of-the-art methods in the field, tensor networks, and autoregressive neural networks. We proved that Autoregressive Neural TensorNet is self-normalized with exact sampling, naturally generalizes the expressivity of both tensor networks and autoregressive neural networks, and inherits proper physics inductive bias (e.g. sign structures) from tensor networks and various symmetries from autoregressive neural networks. We demonstrated our approach on quantum state learning and the challenging 2D \(J_{1}\)-\(J_{2}\) Heisenberg model with different system sizes and couplings. Our approach achieves better performance than both the original tensor network and autoregressive neural network while surpassing tensor networks with large bond dimensions as the system size increases. In addition, our approach is robust, independent of sign rule. Besides scientific applications, since both tensor networks and autoregressive neural networks have been applied to machine learning tasks such as supervised learning and generative modeling, our novel approach holds promise for better performance in these domains due to its exact sampling, expressivity, and symmetries.

## Broader Impact

Our Autoregressive Neural TensorNet, blending tensor networks and autoregressive neural networks, could advance our grasp of quantum phenomena, potentially fueling scientific breakthroughs. It enhances quantum material design and quantum computing through improved simulations and control of quantum states. This technology may also inspire new machine learning models for handling high-dimensional data. However, possible ethical and societal impacts, such as the use for chemical weapon development, require careful scrutiny.