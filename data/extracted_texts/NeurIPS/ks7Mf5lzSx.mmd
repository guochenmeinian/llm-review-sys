# SpatialRank: Urban Event Ranking with NDCG Optimization on Spatiotemporal Data

Bang An

Department of Business Analytics

University of Iowa

Iowa City, IA 52242

bang-an@uiowa.edu

&Xun Zhou

Department of Business Analytics

University of Iowa

Iowa City, IA 52242

xun-zhou@uiowa.edu

&Yongjian Zhong

Department of Computer Science

University of Iowa

Iowa City, IA 52242

yongjian-zhong@uiowa.edu

&Tianbao Yang

Department of Computer Science and Engineering

Texas A&M University

College Station, TX 77843

tianbao-yang@tamu.edu

corresponding author

###### Abstract

The problem of urban event ranking aims at predicting the top-\(k\) most risky locations of future events such as traffic accidents and crimes. This problem is of fundamental importance to public safety and urban administration especially when limited resources are available. The problem is, however, challenging due to complex and dynamic spatio-temporal correlations between locations, uneven distribution of urban events in space, and the difficulty to correctly rank nearby locations with similar features. Prior works on event forecasting mostly aim at accurately predicting the actual risk score or counts of events for all the locations. Rankings obtained as such usually have low quality due to prediction errors. Learning-to-rank methods directly optimize measures such as Normalized Discounted Cumulative Gain (NDCG), but cannot handle the spatiotemporal autocorrelation existing among locations. In this paper, we bridge the gap by proposing a novel spatial event ranking approach named SpatialRank. SpatialRank features adaptive graph convolution layers that dynamically learn the spatiotemporal dependencies across locations from data. In addition, the model optimizes through surrogates a hybrid NDCG loss with a spatial component to better rank neighboring spatial locations. We design an importance-sampling with a spatial filtering algorithm to effectively evaluate the loss during training. Comprehensive experiments on three real-world datasets demonstrate that SpatialRank can effectively identify the top riskiest locations of crimes and traffic accidents and outperform state-of-the-art methods in terms of NDCG by up to \(12.7\%\).

## 1 Introduction

Given a risk score defined based on historical urban events (e.g., crimes, traffic accidents) in a study area as well as the socio-environmental attributes (e.g., travel demands, weather, road network) associated with the events, the goal of the _urban event ranking problem_ is to learn a model that can predict the ranking of the top-\(k\) riskiest locations of future events.

The urban event ranking problem is a widely observed spatiotemporal prediction problem, which has critical applications in public safety, traffic management, and urban planning. For example, the National Highway Traffic Safety Administration , estimated a total of 42,939 deaths in motor vehicle traffic crashes in the United States in 2021. Crime control and property loss cost over $2.5 trillion in the United States in 2017 . Chicago Police Department has utilized criminal intelligence analysis and data science techniques to help command staff determine where best to deploy resources . However, according to the Police Executive Research Forum, there were \(42.7\%\) more resignations among law enforcement but a \(3.9\%\) decrease in hiring new officers in 2021 compared to 2019 . Meanwhile, the Federal Bureau of Investigation confirms that violent crime in 2020 has surged nearly \(30\%\) over 2019 . Therefore, given such growth of crimes and accidents, predicting the riskiest locations of traffic accidents or crimes helps law enforcement stakeholders allocate their limited resources strategically to reduce injuries, deaths, and property losses.

Despite its importance, the urban event ranking problem is technically challenging. **First**, there exist complex and dynamic spatiotemporal correlations between locations in terms of events and attributes such as traffic conditions and weather changes. Capturing such dynamic relationships is non-trivial. **Second**, due to the presence of spatial autocorrelation, nearby locations may share very similar socio-environmental attributes. It is thus very challenging to learn a good model that can correctly rank neighboring locations. **Finally**, urban events are usually sparse in space. Making a prediction with a low average error in count does not ensure a good ranking of the top-\(k\) locations.

Prior works on urban event prediction employed either classic machine learning methods  or deep learning techniques including recurrent neural networks  and convolutional neural networks . Recently, Li et al.  proposed a cross-region hypergraph structure network to address the data sparsity issues. Besides, Yuan et al.  proposed spatial ensembles to address spatial heterogeneity. The objective of the above methods is to make accurate predictions of the risk or count for all locations. However, urban events are often very sparse in space, which might guide the model to avoid predicting accidents in most locations to obtain a low average error. Such prediction does not truly benefit the users such as police officers. The top-\(k\) locations derived from such predictions are naturally inaccurate.

Our problem is also relevant to the learning-to-rank problem frequently studied in the field of recommendation systems. State-of-the-art methods in this area typically predict the top-\(k\) items most likely to be chosen by users through optimizing ranking-based metrics such as the Normalized Discounted Cumulative Gain (NDCG) . The rank operator of NDCG is non-differentiable, and previous studies have made noticeable progress in approximating NDCG by surrogate functions . In our problem, a straightforward adaptation would be to consider the locations as "items" and each time slot as a "user". However, NDCG is not a perfect objective function for our problems as it neither measures the local ranking quality nor considers spatial autocorrelation among locations. Instead, the existing approaches assume items are independent. Therefore, directly applying existing NDCG optimization solutions might not be the best solution to our problem.

In this paper, we bridge the gaps in both fields by formulating the urban event ranking problem as a spatial learning-to-rank problem and solving it by directly optimizing a "spatial" version of the NDCG measure. We propose **SpatialRank**, our deep learning model with three novel designs. To efficiently capture the spatial and temporal dependencies, we design an adaptive graph convolution layer that learns the correlations between locations dynamically from features and historical events patterns; to ensure the model balances both the global ranking quality on all the locations and the local ranking quality on subsets of locations, we propose a hybrid loss function combining both NDCG loss and a novel local NDCG loss; to improve the effectiveness of optimizing NDCG surrogates in the spatiotemporal setting of our problem, we design an importance-based location sampling with spatial filtering algorithm to iteratively adjust the weights of each location considered in the objective function, thereby guiding the model to concentrate on learning for more important locations. We conduct comprehensive experiments on three real-world datasets collected from Chicago and the state of Iowa. The results demonstrate that SpatialRank can substantially outperform baselines and achieve better ranking quality.

Our contributions are summarized below:

* To the best of our knowledge, this is the first paper to formulate urban event forecasting as a location ranking problem and learn a ranking model by optimizing its ranking quality.
* We propose SpatialRank with adaptive graph convolution layers learning from historical event patterns and spatiotemporal features to capture dynamic correlations over time and space.

* We propose a hybrid objective function to leverage the trade-off between global ranking quality and local ranking quality.
* We propose a ranking-based importance sampling algorithm to adaptively adjust the weights of different locations considered in the objective function of the prediction results from the last training epoch to help the model focus on important locations.

## 2 Preliminaries

### Formulation of the event ranking problem

A spatio-temporal field \( T\) is a three-dimensional partitioned space, where \(T=\{t_{1},t_{2},...,t_{T}\}\) is a study period divided into equal-length intervals (e.g., hours, days) and \(=\{s_{(0,0)},...,s_{(M,N)}\}\) is a \(M N\) two-dimension spatial grid partitioned from the study area. A set of socio-environmental features \(F\) are observed over \(S T\), which include temporal features \(F_{t}^{T}\) (e.g., day of week, holiday), spatial features \(F_{s}^{M N}\) (e.g., total road length, speed limit), and spatiotemporal features \(F_{st}^{M N T}\) (e.g., traffic volume, rainfall amount). A risk score \(y^{M N T}\) is a user-defined attribute over \( T\) measuring the risk level of each spatiotemporal location (e.g., number of crimes, injuries of traffic accidents). \(y(s,t)>0\) when any events occurred in \((s,t)\), and equals 0 when no events occurred.

Given the socio-environmental features \(F\) and risk score \(y\) for all the locations in time window \(\{t_{1},t_{2},..,t_{n}\}\), the urban event ranking problem is to predict the ranking of the top-\(k\) locations \(\{s_{1},...,s_{K}\} S\) in the next time interval \(t_{n+1}\) with the highest risk scores. The objective is to prioritize the ranking quality on the top-\(k\) riskiest locations. As a basic assumption of our problem, there exists spatial and temporal autocorrelation among locations in \(F\), meaning nearby locations tend to have more correlated values. In addition, events are sparse so \(y\) is 0 for the majority of the locations. A detailed example of data attributes and feature generation steps can be found in the supplementary materials Appendix A.

### Stochastic Optimization of NDCG in Event Ranking Problem

In the field of recommendation systems, learning to rank is substantially studied, and NDCG is widely used as the metric to measure the ranking quality of the foremost importance. In the following part, we use the terminologies from the event forecasting problem to define NDCG in our problem setting. For a ranked list of locations \(s\) in a period \(t T\), the NDCG score is computed as by:

\[_{t}=}_{s_{t}}}-1}{ _{2}(1+(s))},\] (1)

where \(y_{s}\) is the risk score of the location \(s\), \((s)\) denotes the rank of location \(s\) in the studied spatial domain \(_{t}\), and \(Z_{t}\) is the Discounted Cumulative Gain (DCG) score  of the perfect ranking of locations for time period \(t\). However, the rank operator in NDCG is non-differentiable in terms of model parameters, and thus cannot be optimized directly. A popular solution is to approximate the rank operator with smooth functions and then optimize its surrogates  as shown in Eq. 2.

\[(;,_{t})=_{s^{} _{t}}(h_{t}(s^{};)-h_{t}(;)),\] (2)

where rank operator \(r(s)\) in NDCG is approximated by a differentiable surrogate function \(()\), and the squared hinge loss \((x)=(0,x+c)^{2}\) is commonly used . In this way, the model parameters \(\) can be updated by a gradient-based optimizer. We can maximize over \(L()\):

\[_{^{d}}L():=_{t=1}^{T} _{s_{t}^{+}}^{t}}-1}{Z_{t}_{2}(( ;_{s}^{t},_{t})+1)}.\] (3)where \(^{t} S_{t}^{+}\) denotes a set of locations with positive risk scores to be considered in the objective function. In this way, the optimization solution on NDCG can be directly applied to the event ranking problem.

## 3 Methodology

Directly optimizing NDCG as described above might provide a solution to our problem but fails to capture the spatiotemporal autocorrelation in the data. Also, since NDCG is a global measure, the model might not be able to learn how to rank nearby locations with highly correlated features correctly. In this section, we present our SpatialRank method to address these limitations. Figure. 1 demonstrates the proposed model architecture.

### The Deep Learning Model

Many recent deep learning models for spatiotemporal could be used as the backbone architecture of our SpatialRank method. A key idea in these methods is to model the correlations between locations using a graph, with locations as nodes and edge weights representing the strength of correlations, and extract latent spatial dependency information through graph convolution layers . We follow a similar idea to build the overall network architecture, where spatial and spatiotemporal features are fed into graph convolutional layers. Then the extracted latent representations are concatenated with the temporal features and fed into LSTM layers to capture temporal representations before the final output layer.

A key novelty in our SpatialRank network is the design of a time-guided and traffic-aware graph-generating process for the graph convolution layers. Prior works typically use Pearson's correlation coefficients or similar measures of features (e.g., traffic volume, accident counts) between locations as their correlation strengths and pre-compute a **time invariant** adjacency matrix of locations . In fact, studies  demonstrate that the influence of traffic conditions on events varies over different periods. Inspired by a related work  on a different problem, we use historical events to generate a static graph and learn a time-variant graph from \(F_{ST}\) (e.g., traffic volume) in each time interval. Intuitively, we use \(F_{T}\) (e.g. hour of the day) to learn the weights of dynamic graph vs. static graph to be considered in the graph convolution. The key equations of generating the adaptive graph adjacency are shown below:

Figure 1: SpatialRank Architecture. The spatiotemporal features are used to generate adjacency matrices and then embedded by graph convolution layers. We use a fully connected layer to make final predictions. The hybrid objective function is combined with NDCG loss and local NDCG loss.

\[Z_{1} =( E_{1}W_{1})\] (4) \[Z_{2} =( E_{2}W_{2})\] (5) \[_{dynamic}=(((Z_{1}Z_{2}^ {T}-Z_{2}Z_{1}^{T})))\] (6) \[=(F_{T}W_{3})\] (7) \[ =_{dynamic}+(1-)_{static}\] (8)

Where \(W_{1}\), \(W_{2}\), and \(W_{3}\) are learnable parameters. \(E_{1}\) and \(E_{2}\) are randomly initialized node embeddings that can be learned during training. We represent those embeddings by the spatiotemporal features \(F_{ST}\) (e.g., traffic volume) to reveal the underlying dynamic connections between nodes. The subtraction and ReLU activation function in Eq. (6) lead to the asymmetric property of \(_{dynamic}\)-\(\) is a hyperparameter to control the saturation rate. \(_{static}\) is a pre-calculated adjacency matrix before training the model by computing the Pearson correlation coefficient between the risk scores of locations (nodes), where the correlation between node \(i\) and node \(j\), \(a_{ij}=(y_{i}-_{i})(y_{j}-_{j})}{_{i}(y_{i}- _{i})^{2}(y_{j}-_{j})^{2}}\). The final adjacency matrix \(\) is the weighted sum of \(_{dynamic}\) and \(_{static}\), and the weight \(\) is learned by a sigmoid activation function in Eq. 7 from the linear transformation of temporal feature \(F_{T}\). Intuitively, the static adjacency matrix indicates a baseline correlation between different locations and it is pre-computed before training. This is also what most of the related work has been done. However, inspired by many observations from related studies , it is evident that we think this correlation is not always constant. Therefore, we use locations' time-variant features to construct a new adjacency matrix, and this dynamic adjacency matrix varies with time. We learn the parameters in this dynamic matrix during the network training process. Finally, we combine the static and the learned dynamic matrices through a learned weight \(\). In this way, a combined adjacency matrix can be treated as an adaptation from a static adjacency matrix considering the influence of other features during different periods. Finally, extracted embeddings are fed into LSTM layers and make final predictions of the risk scores \(y\) for each location through a fully connected layer.

### Local Ranking Metric and Hybrid Loss function

As previously mentioned, ranking-based metrics such as NDCG are not designed to handle spatial correlations. According to the first law of geography , nearby locations may share very similar socio-environmental attributes, thus it is challenging to rank neighboring locations correctly. However, the locations nearby with uncertain event patterns are worthy to be focused on so that more potential events can be discovered. Moreover, using NDCG on event ranking problems can cause over-concentrating on top-ranked locations and sacrificing prediction accuracy on other locations due to lower priority. To address those issues, we design a novel local ranking measurement named Local Normalized Discounted Cumulative Gain (L-NDCG) to measure spatially local ranking quality over every sub-region of the study area. The L-NDCG is calculated as:

\[=_{t}|}_{t=1}^{T}_{s^{t} _{t}}_{(s^{t})}}^{t}} -1}{_{(s^{t})}^{t}_{2}(r(,(s^{t}) )+1)}.\] (9)

We use the same terminologies from Eq. 3. The unique part is that we compute an NDCG score for every location in the study area based on the local ranking of a subset of locations \((s^{t})\), where \(\) is a neighborhood of location \(s^{t}\). We define the \(\) as the Euclidean distance of coordinates smaller than \(R\) in this work. \(\) can be defined in other ways depending on the need of the problem. Essentially, L-NDCG is the average of NDCG scores for all subsets of locations. In this way, a few stationed hot-spot locations only take considerably large weight in their own NDCG scores and cannot over-influence the overall L-NDCG score. L-NDCG emphasizes ranking correctly on each subset of locations, and a greater L-NDCG score indicates that relatively more important locations can be distinguished from their nearby locations in a small region. Similar to optimizing NDCG, the ranking operator of L-NDCG is non-differentiable, thus we optimize its surrogates instead.

\[_{^{d}}L():=_{t}^{+ }|}_{t=1}^{T}_{s^{t}_{t}^{+}}_{(s^{t})}}^{t}}-1}{_{(s^{t})}^{t} _{2}((;_{}^{t},(s^{t}))+1)}.\] (10)Where \(g(;_{s}^{t},(s^{t}))\) is a surrogate loss function similar to Eq. 2, and \((s^{t})\) is a set of locations. Finally, to learn a trade-off between locally ranking quality and globally ranking quality we design a hybrid objective function consisting of both NDCG and L-NDCG

\[Loss=(1-)+\] (11)

Where \(\) is a hyperparameter controlling the preference between NDCG and L-NDCG.

### Importance-based Location Sampling with Spatial Filtering

To bridge the gap between capturing spatial correlations in a list of locations and optimizing the quality of predicted ranking, we propose a novel importance-based location sampling strategy with spatial filtering. Specifically, we first design an importance measure function to assign higher weights to locations with larger errors in predictions and higher ranking priority. Secondly, we sample locations based on the weights assigned by importance-measuring, so that important locations have a higher probability to be sampled. Afterward, only losses from sampled locations will be calculated in the objective function and considered during the optimization. In this way, the model pays attention to more important locations. Thirdly, we adjusted the importance scores every epoch so that the model learns to focus on different locations adaptively. Lastly, spatial filters are applied to smooth the importance scores in each epoch to achieve spatial-aware sampling. This allows nearby locations to be sampled in the same batch with high probability to help the model learn how to rank them. The training process is shown in algorithm 1

``` Input: feature tensor \(F\), event risk scores \(y\), hyperparameter \(\), standard deviation \(\) Output: learned model \(f_{}\)
1 Initialize probability set \(P\) = \(\{p_{1},p_{2},...,p_{l}\} L\);
2for each epochdo
3 Compute \(\) = \(f_{}(x)\)
4 Compute \(loss_{NDCG}\) = WeightedLoss\((y,,P)\)
5 Compute \(loss_{local}\) = LocalWeightedLoss\((y,,P)\)
6\(loss_{hybrid}\) = \((1-) loss_{NDCG}+ loss_{local}\)
7for\(s\)do
8\(E_{s}\) = \(_{t}^{t}-y_{s}^{t}|-1}{log_{2}(1+r(y_{s}))}\)
9\(E_{s^{}}^{*}=_{s}}{2^{2}}e^{-}{2^{2}}}\) for \(s\), \(x=dist(s^{},s)\)
10 Update P = \((E^{})\)
11 Compute gradient \( f()\) by \(loss_{hybrid}\)
12 Update \(\) by \( f()\) return\(f_{}\) ```

**Algorithm 1**SpatialRank Training

Algorithm 1 shows the details of the importance-based sampling mechanism. The inputs include feature tensor \(F\), event risk scores \(y\), hyperparameter \(\), and Gaussian standard deviation \(\). The output is a learned model. The algorithm starts with initializing a set of equal-importance scores, which means the probabilities of locations being sampled are equal in the first epoch. Line 3 is forward propagation with current model parameters. Line 4 calculates the weighted NDCG losses given true label \(y\), predicted labels \(\), and current importance scores for locations. \(Loss_{NDCG}\) is computed by Eq. 9, where importance scores \(\{p_{1},p_{2},...,p_{s}\}\) are normalized and treated as weights. Line 6 is our novel hybrid loss function discussed in Eq 10 to leverage the local ranking quality. The key step is to update the importance scores set based on current prediction errors and true ranking in Lines 7-8. We design a score function shown in Line 8 to leverage the errors made in predictions and the priority of true ranking for each location.

\[E_{s}=_{t}^{t}-y_{s}^{t}|-1}{log_{2}(1+r(y_{s}^{t} ))}\] (12)

where \(r()\) denotes a ranking function of the \(i\)-th location in the study area, and \(|y_{s}^{t}-_{s}^{t}|\) is the absolute difference between predicted injuries and true injuries. Basically, a higher score \(E_{l}\) indicates that there are larger prediction errors and higher true ranking in this location \(s\). Note that smaller \(r(y_{s}^{t})\) denotes a higher true ranking. To capture their geographical connections, we map the list of locations to their original locations on the grids, and then we apply a Gaussian filter to smooth the score distribution spatially in Line 9. \(\) is the standard deviation. In this way, the model can learn better spatially correlated patterns, and avoid over-focusing on a few standalone locations but ignoring the the area nearby. Next, the smoothed scores \(E^{*}\) are normalized into \(P\), where \(_{s}P_{s}=1\). Finally, we compute the gradient \( f()\) by \(loss_{hybrid}\) and update model parameters \(\). After a few iterations, we obtain the learned model \(f_{}\).

### Complexity Analysis

In each iteration complexity of SpatialRank, we need to conduct forward propagation \(h_{t}(s;), s_{t}^{+}_{t}\) and back-propagation for computing \( h_{t}(s;), s_{t}^{+}_{t}\). The complexity of forwarding is \(S=_{t T}(|_{t}^{+}|(s)+|_{t}|)d O (TSd)\), where \(d\) is the model size. To compute \(local_{NDCG}\) part, an extra cost on neighbor querying is needed to replace location list size on its complexity. The cost of computing \((;,_{t})\) is \(_{t T}|_{t}^{+}||_{t}| O(TS^{2})\) for NDCG and \(_{t T}|_{t}^{+}|^{2}(s) O(TS^{2}( s))\) for local NDCG. The size of \(^{2}(s)\) is small, therefore the total cost is reduced to \(O(TSd+TS^{2})\).

## 4 Experiments

We perform comprehensive experiments on three real-world traffic accident and crime datasets from Chicago2 and the State of Iowa3. Experiment results show that our proposed approach substantially outperforms state-of-art baselines on three datasets by up to \(12.7\%\), \(7.8\%\), and \(4.2\%\) in NDCG respectively. The case study and results on the state of Iowa are shown in Appendix C.

**Data.** In the Chicago dataset, we collect data from the year 2019 to the year 2021. The first 18 months of this period are used as the training set, and the last 6 months of 2020 are used as the validating set. The year 2021 is used as a testing set. The area of Chicago is partitioned by \(500\) m \(\)\(500\) m square cells and converted to a grid with the size of \(64 80\). For the crime dataset, we use total crimes as the risk score. For accident datasets, we use the number of injuries as the risk score. w **Baselines.** First, we use daily **Historical Average (HA)**. Next, we consider popular machine-learning methods including **Long Short-term Memory (LSTM)**, and **Convolutional LSTM Network (ConvLSTM)**. Thirdly, we compare with recent methods such as **GSNet**, **Hetero-ConvLSTM**, and **HintNet**. Moreover, we compared our optimization approach with other NDCG optimization solutions including **Cross Entropy (CE)**, **ApproxNDCG**, and **SONG**. The details of the baselines are described in Appendix C.

**Metrics.** To measure the ranking quality of foremost locations, we test \(K\) on the test data. We use the metrics including NDCG, L-NDCG, and top-K precision (Prec). We report the average performance and standard deviation over 3 runs for three datasets.

### Performance Comparison

In table 1, we can observe that our SpatialRank significantly outperforms other compared baselines in both datasets. We observe a similar trend among all metrics. On both datasets, Hetero-ConvLSTM and HintNet achieve similar results and outperform general machine learning methods such as LSTM and ConvLSTM. GSNet is designed on a dataset with a smaller grid size, thus performing worse on our larger grid. To study the effectiveness of learning the appropriate graph, we set the learning parameter \(\) as a fixed ratio of 0.5 in SpatialRank\({}^{\#}\). Oppositely, \(\) is a parameter to be learned based on temporal features in SpatialRank. In the Table 1 and Table. 2, each * indicates that the performance improvement of the proposed method over this baseline is statistically significant based on the student t-test with \(=0.05\) over three runs. The results show that the design of the time-aware graph convolution is able to improve performance and capture more dynamic variations in the graph, therefore performing better over different top-k rankings.

### Optimization Comparison

To evaluate the effectiveness of our proposed hybrid objective function and importance-based location sampling, we perform experiments on the same network architecture but with different optimization solutions including Cross Entropy (CE), ApproxNDCG, and SONG. The results are shown in table 2. Methods designed to optimize NDCG consistently perform better than Cross Entropy. SpatialRank substantially outperforms SONG and ApproxNDCG and made a noticeable improvement on L-NDCG as it is considered in the objective function.

### Ablation Study

We examine the effects of tuning hyper-parameter \(\) in the hybrid loss function. We present the results in table 3. Recall that \(\) decides the ratio of L-NDCG takes in the objective function. \(=0\) means L-NDCG is not considered in the objective function. Starting from \(=0\), we observe a trend that the overall performance improves steadily while \(\) goes up, and it reaches the best performance

    &  &  &  \\   & NDCG & Prec & l-NDCG & NDCG & Prec & l-NDCG & NDCG & Prec & l-NDCG \\  HA &.214\(\)0 &.332\(\)0 &.502\(\)0 &.225\(\)0 &.322\(\)0 &.497\(\)0 &.235\(\)0 &.316\(\)0 &.493\(\)0 \\ LSTM &.215\(\)2s\({}_{}\).392\(\)2s\({}_{}\).519\(\)3s\({}_{}\).225\(\)1s\({}_{}\).380\(\)2s\({}_{}\).543\(\)3s\({}_{}\).249\(\)1s\({}_{}\).368\(\)2s\({}_{}\).544\(\)3s\({}_{}\).

ConvLSTM &.225\(\)5s\({}_{}\).410\(\)4s\({}_{}\).558\(\)3s\({}_{}\).236\(\)1s\({}_{}\).388\(\)4s\({}_{}\).563\(\)8s\({}_{}\).252\(\)1s\({}_{}\).366\(\)2s\({}_{}\).540\(\)8s\({}_{}\).

GSNet &.194\(\)1s\({}_{}\).371\(\)2s\({}_{}\).493\(\)5s\({}_{}\).201\(\)2s\({}_{}\).371\(\)2s\({}_{}\).517\(\)5s\({}_{}\).231\(\)1s\({}_{}\).337\(\)3s\({}_{}\).499\(\)3s\({}_{}\).

Hetero-ConvLSTM &.229\(\)2s\({}_{}\).401\(\)1s\({}_{}\).557\(\)2s\({}_{}\).240\(\)1s\({}_{}\).395\(\)4s\({}_{}\).564\(\)3s\({}_{}\).255\(\)3s\({}_{}\).375\(\)2s\({}_{}\).551\(\)3s\({}_{}\).

HintNet &.282\(\)1s\({}_{}\).402\(\)5s\({}_{}\).555\(\)3s\({}_{}\).238\(\)2s\({}_{}\).390\(\)3s\({}_{}\).569\(\)3s\({}_{}\).256\(\)1s\({}_{}\).373\(\)4s\({}_{}\).561\(\)8s\({}_{}\).

SpatialRank * &.250\(\)2s\({}_{}\).438\(\)3s\({}_{}\).591\(\)3s\({}_{}\).256\(\)1s\({}_{}\).409\(\)2s\({}_{}\).593\(\)2s\({}_{}\).271\(\)2s\({}_{}\).394\(\)2s\({}_{}\).585\(\)1s\({}_{}\).

SpatialRank & **.257\(\)1s\({}_{}\).444\(\)3s\({}_{}\).621\(\)4s\({}_{}\).268\(\)1s\({}_{}\).420\(\)1s\({}_{}\).614\(\)2s\({}_{}\).278\(\)3s\({}_{}\).403\(\)1s\({}_{}\).599\(\)1s\({}_{}\)** \\   &  &  &  \\   & NDCG & Prec & l-NDCG & NDCG & Prec & l-NDCG \\  HA &.237\(\)0 &.348\(\)0 &.514\(\)0 &.250\(\)0 &.333\(\)0 &.506\(\)0 &.259\(\)0 &.322\(\)0 &.449\(\)0 \\ LSTM &.246\(\)1s\({}_{}\).327\(\)2s\({}_{}\).517\(\)3s\({}_{}\).257\(\)1s\({}_{}\).329\(\)2s\({}_{}\).521\(\)3s\({}_{}\).262\(\)3s\({}_{}\).314\(\)5s\({}_{}\).512\(\)3s\({}_{}\).

ConvLSTM &.313\(\)2s\({}_{}\).41\(\)5s\({}_{}\).41\(\)4s\({}_{}\).43\(\).352\(\)2s\({}_{}\).404\(\)1s\({}_{}\).607\(\)6s\({}_{}\).333\(\)2s\({}_{}\).387\(\)4s\({}_{}\).599\(\)3s\({}_{}\).

GSNet &.283\(\)3s\({}_{}\).388\(\)2s\({}_{}\).584\(\)3s\({}_{}\).584\(\)3s\({}_{}\).296\(\)3s\({}_{}\).374\(\)5s\({}_{}\).568\(\)3s\({}_{}\).

Hetero-ConvLSTM &.346\(\)3s\({}_{}\).468\(\)3s\({}_{}\).657\(\)4s\({}_{}\).365\(\)1s\({}_{}\).452\(\)3s\({}_{}\).642\(\)6s\({}_{}\).374\(\)4s\({}_{}\).433\(\)4s\({}_{}\).638\(\)4s\({}_{}\).

HintNet &.342\(\)3s\({}_{}\).468\(\)3s\({}_{}\).661\(\)4s\({}_{}\).358\(\)3s\({}_{}\).448\(\)4s\({}_{}\).

SpatialRank * &.361\(\)2s\({}_{}\).484\(\)1s\({}_{}\).670\(\)4s\({}_{}\).376\(\)4s\({}_{}\).463\(\)3s\({}_{}\).**655\(\)7s\({}_{}\).387\(\)1s\({}_{}\).**446\(\)1s\({}_{}\).**651\(\)7s\({}_{}\).

SpatialRank & **.373\(\)2s\({}_{}\).491\(\)3s\({}_{}\).665\(\)4s\({}_{}\).380\(\)2s\({}_{}\).467\(\)5s\({}_{}\).

at \(\) equals 0.1 in the accident and crime dataset. It indicates that considering a reasonable weight of L-NDCG in the objective function boosts overall performance. These results demonstrate the importance of considering local ranking because NDCG, L-NDCG, and precision can be all improved.

### Cross-K function

We use the Cross-K function with Monte Carlo Simulation to evaluate the accuracy of predicted locations. The Cross-K function measures the spatial correlation between the predicted locations and true locations. Specifically, we calculate the average density of predictions within every distance \(d\) of a true event in each day as shown in Eq. 13:

\[(d)=_{j}^{-1}_{i j}I(d_{ij} d)/n,\] (13)

where \(\) is global density of event \(j\), and \(I()\) is an identity function which equals one if real distance \(d_{ij}\) is smaller than \(d\), else equals zero. \(n\) is the number of events \(i\). The results are shown in Figure 2. The grey curve represents the complete spatial randomness and we use it as a reference baseline. Higher is better. Our SpatialRank achieves the best predictions in both datasets, which indicates that the predictions of SpatialRank are significantly spatially correlated with ground truth. The similar results on the other two datasets are shown in Appendix C in the supplementary materials.

**Additional Results** and a **Case Study** to demonstrate successful prediction examples are included in the Supplementary materials.

    &  &  &  \\   & NDCG & Prec & l-ndcg & NDCG & Prec & l-ndcg & NDCG & Prec & l-ndcg \\  \(=0.0\) & 0.251 & 0.439 & 0.610 & 0.263 & 0.407 & **0.612** & 0.274 & 0.289 & 0.605 \\ \(=0.05\) & 0.239 & 0.416 & 0.559 & 0.254 & 0.394 & 0.600 & 0.266 & 0.386 & 0.597 \\ \(=0.1\) & **0.256** & 0.443 & **0.621** & **0.268** & **0.421** & 0.608 & **0.276** & **0.400** & 0.599 \\ \(=0.2\) & 0.250 & **0.444** & 0.616 & 0.261 & 0.411 & 0.605 & 0.271 & 0.394 & **0.603** \\ \(=0.3\) & 0.234 & 0.422 & 0.606 & 0.243 & 0.388 & 0.59 & 0.253 & 0.374 & 0.591 \\   &  &  &  \\   & NDCG & Prec & l-ndcg & NDCG & Prec & l-ndcg & NDCG & Prec & l-ndcg \\  \(=0.0\) & 0.366 & 0.489 & 0.661 & 0.378 & 0.464 & 0.644 & 0.385 & 0.445 & 0.642 \\ \(=0.05\) & 0.366 & 0.489 & 0.659 & 0.378 & **0.467** & **0.649** & 0.382 & 0.447 & **0.643** \\ \(=0.1\) & **0.371** & **0.494** & **0.665** & **0.383** & **0.467** & 0.644 & **0.391** & **0.450** & 0.642 \\ \(=0.2\) & 0.356 & 0.474 & 0.654 & 0.367 & 0.448 & 0.643 & 0.372 & 0.429 & 0.623 \\ \(=0.3\) & 0.345 & 0.457 & 0.658 & 0.361 & 0.445 & 0.641 & 0.373 & 0.431 & 0.631 \\   

Table 3: Ablation study

Figure 2: Comparison of cross-K function in Chicago Accident.

## 5 Related Work

**Urban Event forecasting** has been widely studied in the past few decades. Most early studies  rely on small-scale and single-source datasets with limited types of features, thus the prediction accuracy is limited. Notably, Zhou et al.  proposed a differential time-varying graph convolution network capturing traffic changes and improving prediction accuracy. Similarly, Wang et al.  proposed GSNet with a geographical module and a weighted loss function to capture semantic spatial-temporal correlations among regions and solve data sparsity issues. Addressing the issue of spatial heterogeneity, Yuan et al.  proposed Hetero-ConvLSTM to leverage an ensemble of predictions from models learned from pre-selected sub-regions. Furthermore, An et al.  proposed HintNet partitions the study area hierarchically and transfers learned knowledge over different regions to improve performance. However, most existing models rely on optimizing cross-entropy and the objective is to make accurate predictions on every location. **Learning to rank** is an extensively studied area in recommendation systems and search engines . NDCG is a widely adopted metric to measure ranking quality. The prominent class of methods involves approximating ranks in NDCG using smooth functions and subsequently optimizing the resultant surrogates. Taylor et al.  tries to use rank distributions to smooth NDCG, but suffers from high computational cost. Similarly, Qin et al.  approximates the indicator function by a generalized sigmoid function with a top-k variant. Noticeably, Qiu et al.  develop stochastic algorithms optimizing the surrogates for NDCG and its top-K variant. Although it is possible to directly apply the existing ranking methods to the urban event ranking problem, the performance tends to be unsatisfactory as these methods commonly assume independence between items and queries and lack the ability to handle spatiotemporal autocorrelation in the data.

## 6 Conclusion and Limitations

In this work, we formulate event forecasting as a location ranking problem. We propose a novel method SpatialRank to learn from spatiotemporal data by optimizing NDCG surrogates. To capture dynamic spatial correlations, we design an adaptive graph convolution layer to learn the graph from features. Furthermore, we propose a hybrid loss function to capture potential risks around hot-spot regions, and a novel ranking-based importance sampling mechanism to leverage the importance of each location considered during the model training. Extensive experimental results on three real-world datasets demonstrate the superiority of SpatialRank compared to baseline methods.

**Limitations**: the model's performance might be affected by other properties of data, such as spatial heterogeneity and sparsity. We observe less improvement over baselines on the Iowa dataset, partially due to that the data is sparser over a large area with heterogeneity. These are issues addressed by some of the prior work and can be addressed in our future work. In addition, the new algorithm increases the training time complexity due to the sampling steps. This is acceptable due to the relatively small number of locations and time periods in urban event datasets but may require extra work to generalize to large datasets.