# DiffuserLite: Towards Real-time Diffusion Planning

Zibin Dong\({}^{1}\) Jianye Hao\({}^{1}\) Yifu Yuan\({}^{1}\) Fei Ni\({}^{1}\) Yitian Wang\({}^{2}\) Pengyi Li\({}^{1}\) Yan Zheng\({}^{1}\)

\({}^{1}\)College of Intelligence and Computing, Tianjin University

\({}^{2}\)UC San Diego Jacobs School of Engineering

Contact me at zibindong@outlook.comCorrespondence to: Jianye Hao (jiangve.hao@tju.edu.cn), Yan Zheng (yanzheng@tju.edu.cn)

###### Abstract

Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The capability of generating high-quality long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies due to the expensive iterative sampling cost. To alleviate this, we introduce **DiffuserLite**, a super fast and lightweight diffusion planning framework, which employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, significantly reducing the modeling of redundant information and leading to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite achieves a decision-making frequency of \(122.2\)Hz (\(112.7\)x faster than predominant frameworks) and reaches state-of-the-art performance on D4RL, Robomimic, and FinRL benchmarks. In addition, DiffuserLite can also serve as a flexible plugin to increase the decision-making frequency of other diffusion planning algorithms, providing a structural design reference for future works. More details and visualizations are available at project website.

**DiffuserLite: Towards Real-time Diffusion Planning**

## 1 Introduction

Diffusion models (DMs) are powerful generative models that demonstrate promising performance across various domains . Motivated by their remarkable capability in complex distribution modeling and conditional generation, researchers have developed a series of works applying diffusion models for decision-making tasks in recent years . DMs can play various roles in decision-making tasks, such as acting as planners to make better decisions from a long-term perspective , serving as policies to support complex multimodal-distribution modeling , and working as data synthesizers to assist reinforcement learning (RL) training , etc. Among these roles, diffusion planning is the most widely applied paradigm . Unlike auto-regressive planning in previous model-based RL approaches , diffusion planning avoids severe compounding errors by directly generating the entire trajectory rather than one-step transition . Also, its powerful conditional generation capability allows planning at the

Figure 1: **Performance overview. We present DiffuserLite, a lightweight framework that utilizes progressive refinement planning to reduce redundant information generation and achieves real-time diffusion planning. DiffuserLite significantly outperforms predominant frameworks, Diffuser and DD, regarding scores, inference time, and model size on three popular D4RL benchmarks. The decision-making frequency of DiffuserLite achieves \(\), which is \(\) higher than predominant frameworks.**trajectory level without being limited to step-wise shortsightedness. The diffusion planning paradigm has achieved state-of-the-art (SOTA) performance in various offline RL tasks, including single-agent RL , multi-agent RL , meta RL , and more.

One key issue that diffusion planning faces is the expensive iterative sampling cost. As depicted in fig. 1, the decision-making frequencies (number of actions inferred per second) of two predominant diffusion planning frameworks, Diffuser  and Decision Diffuser (DD) , are recorded as \(1.3\)Hz and \(0.4\)Hz, respectively. Such a low decision frequency fails to meet the requirements of numerous real-world applications, e.g. real-time robot control  and game AI . The low decision frequency is primarily attributed to modeling a denoising process for a long-horizon trajectory distribution, which requires a heavy neural network backbone and multiple forward passes. A question may arise whether the generation of complete long-horizon trajectories is necessary for successful planning. Experimental results indicate that it is not, for the detailed trajectory information in long-horizon segments is highly redundant. As shown in fig. 2, a motivation example in Antmaze, the disparities between plans increase as the horizon grows, leading to poor consistency between plans in consecutive steps. Besides, in practice, agents often struggle to reach the planned distant state. These facts argue that while long-horizon planning helps improve foresight, it introduces redundant information in distinct parts. The details in closer parts are more crucial. Ignoring the modeling of these redundant parts in the diffusion planning process will significantly reduce the complexity of the trajectory distribution to be fitted, making it possible to build a fast and lightweight diffusion planning framework.

Motivated by these insights, we propose to build a plan refinement process (PRP) to speed up diffusion planning. First, we perform "rough" planning, where jumpy planning is executed, only considering the states at intervals that are far apart and ignoring other individual states. Then, we refine a small portion of the plan, focusing on the steps closer to the current state. By doing so, we fill in the execution details between two states far apart, gradually refining the plan to the step level. This approach has three advantages: 1) It reduces the length of the sequences generated by the diffusion model, simplifying the complexity of the probability distribution to be fitted. 2) It significantly reduces the search space of plans, making it easier for the planner to find well-performed trajectories. 3) Since only the first action of each step is executed, rough planning of steps further away causes no noticeable performance drop.

Diffusion planning with PRP, which we call DiffuserLite, is simple, fast, and lightweight. Our experiments have demonstrated the effectiveness of PRP, significantly increasing decision-making frequency while achieving SOTA performance. Moreover, it can be easily adapted to other existing diffusion planning methods. In summary, our contributions are as follows:

* We introduce the plan refinement process (PRP) for coarse-to-fine-grained trajectory generation, reducing the modeling of redundant information.
* We introduce DiffuserLite, a lightweight diffusion planning framework, which significantly increases decision-making frequency by employing PRP.
* DiffuserLite is a simple and flexible plugin that can be easily combined with other diffusion planning algorithms.
* DiffuserLite achieves a super high decision-making frequency (\(122.2\)Hz, \(112.7\)x faster than predominant frameworks) and SOTA performance on multiple benchmarks in D4RL.

Figure 2: **Comparison of one-shot planning (top) and PRP (down) on Antmaze. The former directly generates plans with a temporal horizon of \(129\). The latter consists of three coarse to fine-grained levels with temporal horizons of \(0\)-\(128\), \(0\)-\(32\), and \(0\)-\(8\), and temporal jumps of \(32\), \(8\), and \(1\), respectively. The visualization in the figure illustrates the x-y coordinates of \(100\) plans. It shows that one-shot planning exhibits a significant amount of redundant information and a large search space. In contrast, PRP demonstrates better plan consistency and a smaller search space.**

## 2 Preliminaries

**Problem Setup:** Consider a system governed by discrete-time dynamics \(o_{t+1}=f(o_{t},a_{t})\) at state \(o_{t}\) given an action \(a_{t}\). A trajectory \(=[x_{0},,x_{T-1}]\) can be either a sequence of states \(x_{t}=o_{t}\) or state-action pairs \(x_{t}=(o_{t},a_{t})\), where \(T\) is the planning horizon. Each trajectory can be mapped to a property \(\). Diffusion planning aims to find a trajectory that exhibits a property closest to the target:

\[^{*}=*{arg\,min}_{}d((),_{ })\] (1)

where \(d\) is a certain distance metric, \(\) is a critic that maps a trajectory to the property it exhibits, and \(c_{}\) is the target property. The action to be executed \(a_{t}\) is then extracted from the selected trajectory (_state-action sequences_) or predicted by an inverse dynamic model \(a_{t}=h(o_{t},o_{t+1})\) (_state-only sequences_). In the context of offline RL, it is a common choice to define the property as the corresponding cumulative reward \(()=_{t=0}^{T-1}r(o_{t},a_{t})\) in previous works [20; 1].

**Diffusion Models** assume an unknown trajectory distribution \(q_{0}(_{0})\), DMs define a forward process \(\{x_{s}\}_{s[0,S]}\) with \(S>0\). Starting with \(_{0}\), previous work  proved that one can obtain any \(_{s}\) by solving the following stochastic differential equation (SDE):

\[_{s}=f(s)_{s}s+g(s)_{s},\; {x}_{0} q_{0}(_{0})\] (2)

where \(_{s}\) is the standard Wiener process, and \(f(s)=_{s}}{s}\), \(g^{2}(s)=^{2}}{s}-2_{s}^{2}_{s}}{s}\). Values of \(_{s},_{s}^{+}\) depend on the noise schedule but keep the _signal-to-noise-ratio_ (SNR) \(_{s}^{2}/_{s}^{2}\) strictly decreasing . While this SDE transforms \(q_{0}(_{0})\) into a noise distribution \(q_{S}(_{S})=(,)\), one can reconstruct trajectories from the noise by solving the reverse process of eq. (2). Previous work  proved that solving its associated _probability flow ODE_ can support faster sampling:

\[_{s}}{s}=f(s)_{s}-g^{2}(s) _{} q_{s}(_{s}),\;_{S} q_{S}(_{S})\] (3)

in which _score function_\(_{} q_{s}(_{s})\) is the only unknown term and estimated by a neural network \(-_{}(_{s},s)/_{s}\) in practice. The parameter \(\) is optimized by minimizing the following objective:

\[()=_{q_{0}(_{0}),q(),s}[||_{}(_{s},s)-||_{2}^{2}]\] (4)

where \( q()=(,)\), \(_{s}=_{s}_{0}+_{s}\). Various ODE solvers can be employed to solve Equation (3), such as the Euler solver , RK45 solver , DPM solver , etc.

**Conditional Sampling** helps to generate trajectories exhibiting certain properties in a priori. There are two main approaches: classifier-guidance (CG)  and classifier-free-guidance (CFG) . CG requires an additional classifier \( p_{}(|_{s},s)\) to predict the log probability that a noisy trajectory \(_{s}\) exhibits a given property \(\). The gradients from this classifier are then used to guide the solver:

\[}_{}(_{s},s,):=_{}( _{s},s)-w_{s}_{_{s}} p_{}(|_ {s},s)\] (5)

CFG does not require an additional classifier but uses a conditional noise predictor to guide the solver.

\[}_{}(_{s},s,):=w_{ }(_{s},s,)+(1-w)_{}(_{s},s)\] (6)

Increasing the value of guidance strength \(w\) leads to more property-aligned generation, but decreases the legality of the generated trajectories .

## 3 Efficient Planning via Refinement

Diffuser  and DD  are two pioneering frameworks, upon which a vast amount of diffusion planning works has been built [28; 26; 53]. Although the design details differ, they can be unified into one paradigm. In the inference step, multiple candidate trajectories are first conditionally sampled using the diffusion model. Then, a critic is used to select the optimal one that exhibits the closest property to the target. Finally, the action to be executed is extracted. This paradigm relies on multiple forwarding complex neural networks, resulting in extremely low decision-making frequencies (typically \(1\)-\(10\)Hz, or even less than \(1\)Hz), severely hindering its real-world deployment.

The fundamental reason is the requirement for highly complex neural networks to model the complex long-horizon trajectory distributions . Although some works have explored using advanced ODE solvers to reduce the sampling steps to around \(5\), the time consumption of network forwarding is still unacceptable. However, we notice that _ignoring some redundant distant parts of the generated plan can be a possible solution_ to address the issue. As shown in fig. 2, the disparities between plans increase as the horizon grows, leading to poor consistency between the plans selected in consecutive steps. Besides, agents often struggle to reach the distant states given by a plan in practice. Both facts indicate that terms in distant parts of a plan become increasingly redundant, whereas the closer parts are more crucial.

Regarding these findings, we aim to develop a progressive refinement planning (PRP) process. This process initially plans a rough trajectory consisting of only key points spaced at equal intervals and then progressively refines the first interval by generative interpolating. It is worth noting that this is orthogonal to methods of identifying key points based on task semantic information . Specifically, our proposed PRP consists of \(L\) planning levels. At each level \(l\{0,1,,L-1\}\), starting from the known first term \(x_{0}\), DMs plan rough trajectories \(_{0:H_{l}:I_{l}}\) with temporal horizon \(H_{l}\) and temporal jump \(I_{l}\). Then, the planned first key point \(x_{I_{l}}\) is passed to the next level as its terminal:

\[_{0:H_{l}:I_{l}} :=[x_{0},x_{I_{l}},x_{2I_{l}},x_{H_{l}-1}]\] (7) \[_{0:H_{l+1}:I_{l+1}} :=[x_{0},x_{I_{l+1}},x_{2I_{l+1}},,x_{H_{l+1}-1}]\] \[x_{I_{l}} =x_{H_{l+1}-1}\]

By this design, only the first planned intervals are refined in the next level, and the other redundant details are all ignored, resulting in a coarse-to-fine generation process. The progressive refinement continues until the last level to extract an action. To support conditional sampling for each level, we define the property of a rough trajectory \(_{0:H_{l}:I_{l}}\) as the property expectation over the distribution of all its completed trajectories \((_{0:H_{l}:I_{l}})\):

\[(_{0:H_{l}:I_{l}}):=_{( _{0:H_{l}:I_{l}})}[()]\] (8)

PRP ensures that long-term planning maintains foresight while alleviating the burden of modeling redundant information. As a result, it greatly contributes to reducing model size and improving planning efficiency:

**Simplifying the fitted distribution of DMs.** The absence of redundant details in PRP allows for a significant reduction in the complexity of the fitted distribution at each level. This reduction in complexity enables us to utilize a lighter neural network backbone, shorter network input sequence lengths, and a reduced number of denoising steps.

**Reducing the plan-search space.** Key points generated at former levels often sufficiently reflect the quality of the entire trajectory, which allows the planner to focus more on finding distant key points and planning actions for the immediate steps, reducing search space and complexity.

Figure 3: **Overview of DiffuserLite.** Observing the current state \(o_{t}\), level \(0\) of DiffuserLite fixes \(o_{t}\) as \(o_{0}\) and generates multiple candidate trajectories. A critic is then used to select the optimal one, in which \(o_{I_{l}}\) is then passed to the next level as its terminal \(o_{H_{1}-1}\). The plan refinement process continues iteratively until the last level with a temporal jump of \(I_{L-1}=1\). Finally, the action \(a_{t}\) to be executed is extracted using an inverse dynamic model \(a_{t}=(o_{0},o_{1})\).

A Lite Architecture for Real-time Diffusion Planning

Employing PRP results in a new lightweight architecture for diffusion planning, which we refer to as DiffuserLite. DiffuserLite can reduce the complexity of the fit distribution and significantly increase the decision-making frequency, achieving \(122\)Hz on average for the need of real-time control. We present the architecture overview in fig. 3, provide pseudocode for both training and inference in algorithm 1 and algorithm 2, and discuss detailed design choices in this section.

**Diffusion model for each level:** We train \(L\) diffusion models for all levels to generate state-only sequences. We employ DiT  as the noise predictor backbone, instead of the more commonly used UNet  due to the significantly reduced length of the generated sequences in each level (typically around \(5\)). This eliminates the need for 1D convolution to extract local temporal features. To adapt the DiT backbone for temporal generation, we make minimal structural adjustments following . For conditional sampling, we utilize CFG instead of CG, as the slow gradient computation process of CG reduces the frequency of decision-making. During the training phase, at each gradient step, we sample a batch of \(H_{0}\)-length trajectories, slice each of them into \(L\) sub-trajectories \([_{0:H_{0}:1},,_{0:H_{L-1}:1}]\), evaluate their properties \((_{0:H_{1}:1})\) as an estimation of \((_{0:H_{1}:I_{l}})\) for condition at each level, and then slice the sub-trajectories into evenly spaced training samples \(_{0:H_{1}:I_{l}}\) for training the diffusion models. During the inference phase, as shown in fig. 3, diffusion models generate multiple candidate plans level by level from 0 to \(L-1\), and the optimal one is selected by the critic \(\).

**Critic design:** The Critic \(\) in DiffuserLite plays two important roles: providing generation conditions during the diffusion training process and selecting the optimal plan from the candidates generated by the diffusion model during inference. In the context of Offline RL, both Diffuser and DD adopt the cumulative reward of a trajectory as the condition:

\[()=_{t=0}^{H-1}r(o_{t},a_{t}),\] (9)

where \(H\) is the temporal horizon. This design allows rewards from the offline RL dataset to be utilized as a ground-truth critic for acquiring generation conditions during training. During inference, an additional trained reward function is required to serve as the critic. The critic then helps select the plan that maximizes the cumulative reward, as depicted in the lower part of fig. 3. However, this design poses challenges in tasks with sparse rewards, as it can confuse diffusion models when distinguishing better-performing trajectories, especially for short-horizon plans. To address this challenge, we introduce an option to use the sum of discounted rewards and the value of the last state as an additional property design:

\[()=_{t=0}^{H-2}^{t}r(o_{t},a_{t})+^{H-1}V(o_{ H-1}),\] (10)

where \(V(o_{t})=_{}[_{=t}^{}^{-t}r_{}]\) represents the optimal value function  and can be estimated by a neural network through various offline RL methods. In the context of other domains, properties can be flexibly designed as needed, as long as the critic \(\) can evaluate trajectories of variable lengths. It is worth noting that previous diffusion planning algorithms widely support this flexibility. In addition, it is even possible to skip the critic selection during inference, which is equivalent to using a uniform critic, as used in DD.

**Action extraction:** After obtaining the optimal trajectory from the last level through critic selection, we utilize an additional inverse dynamic model \(a_{t}=h(o_{t},o_{t+1})\) to extract the action to be executed. This approach is suggested in .

**Further speedup with rectified flow:** DiffuserLite aims to achieve real-time diffusion planning to support its application in real-world scenarios. Therefore, we introduce _Rectified flow_ for further increasing the decision-making frequency. Rectified flow, an ODE on the time interval \(\), causalizes the paths of linear interpolation between two distributions. If we define the two distributions as trajectory distribution and standard Gaussian, we can directly replace the Diffusion ODE with rectified flow to achieve the same functionality. The most significant difference is that rectified flow learns a straight-line flow and can continuously straighten the ODE through _reflow_. This straightness property allows for consistent and stable gradients throughout the flow, enabling the generation of trajectories with very few sampling steps (in our experiments, we found that one-stepsampling is sufficient to produce good results). We consider rectified flow an optional backbone for cases that prioritize decision frequency. In appendix B, we offer comprehensive explanations of trajectory generation and training with rectified flow.

## 5 Experiments

We explored the performance of the DiffuserLite on various tasks on D4RL, Robomimic, and FinRL [12; 34; 41], and aimed to answer the following research questions (RQs): 1) To what extent can DiffuserLite reduce the **runtime cost**? 2) How is the **performance** of DiffuserLite on offline RL tasks? 3) Can DiffuserLite serve as a **flexible plugin** for other diffusion planning algorithms? 4) Can we summarize a list of simple and clear **design choices** for DiffuserLite?

### Experimental Setup

**Benchmarks:** We evaluate the algorithm on various offline RL domains, including locomotion in Gym-MuJoCo , real-world manipulation in FrankaKitchen  and Robomimic , long-horizon navigation in Antmaze , and real-world stock trading in FinRL . We train all models using publicly available datasets (see appendix A.1 for further details).

**Baselines:** Our comparisons mainly include the basic imitation learning algorithm BC, existing offline RL methods CQL  and IQL , as well as two pioneering diffusion planning frameworks, Diffuser  and DD . Additionally, we compare with a state-of-the-art algorithm, HDMI , which is improved based on DD. (Further details about each baseline and the sources of performance results for each baseline across the experiments are presented in appendix A.2).

**Backbones:** As mentioned in Section 4, we implement three variations of DiffuserLite, based on different backbones: 1) diffusion model, 2) rectified flow, and 3) rectified flow with an additional _reflow_ step. These variations will be used in subsequent experiments and indicated by the suffixes D, R1, and R2, respectively. All backbones utilize three levels, with a total temporal horizon of \(129\) in MuJoCo and Antmaze, and \(49\) in Kitchen. For more details about the hyperparameters selection for DiffuserLite, please refer to appendix D.

**Computing Power:** All runtime results across our experiments are obtained on a server equipped with an Intel(R) Xeon(R) Gold 6326 CPU @ 2.90GHz and an NVIDIA GeForce RTX3090.

  
**Environment** & **Metric** & **Diffuser** & **DD** & **HDMI** & **DiffuserLite-D** & **DiffuserLite-R1** & **DiffuserLite-R2** \\   & Runtime (s) & \(0.665\) & \(2.142\) & \(0.405\) & \(0.015\) & \(0.013\) & \(0.005\) \\  & Frequency (Hz) & \(1.5\) & \(0.47\) & \(2.5\) & \(68.2\) & \(79.7\) & \(200.7\) \\   & Runtime (s) & \(0.790\) & \(2.573\) & \(0.407\) & \(0.017\) & \(0.015\) & \(0.010\) \\  & Frequency (Hz) & \(1.3\) & \(0.4\) & \(2.5\) & \(58.7\) & \(66.0\) & \(103.2\) \\   & Runtime (s) & \(0.791\) & \(2.591\) & \(0.410\) & \(0.027\) & \(0.015\) & \(0.010\) \\  & Frequency (Hz) & \(1.3\) & \(0.39\) & \(2.4\) & \(37.3\) & \(65.7\) & \(101.7\) \\   & \(0.749\) & \(2.435\) & \(0.407\) & **0.020** & **0.014** & **0.008** \\  & 1.34 & 0.41 & \(2.46\) & **51.26** & **69.90** & **122.44** \\   

Table 1: **Time consumption per step and frequency in D4RL.** All results are obtained over 5 random seeds. DiffuserLite achieves an average decision frequency of about 122Hz on the R2 backbone and 811Hz averaging over three variations, which meets the requirements of real-time inference. Since the code for HDMI is not open-source, we make every effort to reproduce HDMI with the original settings to test its runtime cost and thus mark its results with underlines.

Figure 4: **Runtime and performance comparison in FrankaKitchen.** The y-axis represents the number of completed tasks (maximum of \(4\)), and the x-axis represents the required wall-clock time. Task success rates are presented in colored circles. All results are averaged over 250 rollouts. DiffuserLite demonstrates significant advantages in both wallclock time and success rate.

### Runtime Cost (RQ1)

The primary objective of DiffuserLite is to increase the decision-making frequency. Therefore, we first test the wall-clock runtime cost (time consumption for one action inference) of DiffuserLite under three different backbones, compared to Diffuser, DD, and HDMI, to determine the extent of the advantage gained. We present the test results in table 13, which shows that the runtime cost of DiffuserLite with D, R1, and R2 backbones is only \(1.23\%\), \(0.89\%\), and \(0.51\%\) of the average runtime cost of Diffuser and DD, respectively. **The remarkable improvement in decision-making frequency does not harm its performance.** As shown in fig. 4, compared to the average success rates of Diffuser (purple line) and DD (grey line) on four FrankaKitchen sub-tasks, DiffuserLite improves them by \([41.5\%,56.2\%,55.3\%,8.7\%]\), respectively, while being \(200\)-\(300\) times faster. These improvements are attributed to ignoring redundant information in PRP, which reduces the complexity of the distribution that the backbone generative model needs to fit, allowing us to employ a light neural network backbone and use fewer sampling steps to conduct _perfect-enough_ planning. Its success in FrankaKitchen, a realistic robot manipulation scenario, also reflects its potential application in real-world settings.

### Performance (RQ2)

DiffuserLite is then evaluated on various popular domains in D4RL, Robomimic, and FinRL, to test how well it can maintain the performance when significantly increasing the decision-making frequency. All results are presented in table 2, table 3, and table 4, and the detailed descriptions of the sources of all baseline results are listed in appendix A.2. Results in _D4RL_ table show significant performance improvements across all benchmarks with high decision-making frequency. This advantage is particularly pronounced in FrankaKitchen and Antmaze environments, indicating that the structure of DiffuserLite enables more accurate and efficient planning in long-horizon tasks, thus yielding greater benefits. In the MuJoCo environments, more notable advantages are shown on sub-optimal datasets, i.e., "medium" and "medium-replay" datasets. This sub-optimal advantage can be attributed to the PRP planning structure, which does not require one-shot generation of a consistent long trajectory, but explicitly demands stitching. This allows for better utilization of high-quality segments in low-quality datasets, leading to improved performance. Results in _Robomimic and

  
**Dataset** & **BC** & **COL** & **IRS** & **DiffuserLite-D** & **Dataset** & **BC** & **COL** & **MB-PRO** & **DD** & **HDMI** & **DiffuserLite-D** \\  FinRL-B999 & \(270\) & \(444\) & \(757\) & \(752\) & \(801\) & \(796\) \\ FinRL-M-999 & \(504\) & \(621\) & \(698\) & \(712\) & \(754\) & \(762\) \\ 
**Average** & \(387\) & \(533\) & \(743\) & \(747\) & \(778\) & \(\) \\   

Table 4: **FinkL Performance.**

  
**Dataset** & **Environment** & **BC** & **COL** & **IQL** & **Diffuser** & **DD** & **HDMI** & **DiffuserLite-D** & **DHMI** & **DiffuserLite-R2** \\   & HalCheetah & \(52.5\) & \(\) & \(86.7\) & \(79.8\) & \(\) & \(\) & \(85.5\) & \(0.94\) & \(\) & \(84.0\) & \(2.9\) \\  & Happer & \(52.5\) & \(105.4\) & \(91.5\) & \(107.2\) & \(\) & \(\) & \(\) & \(0.2\) & \(\) & \(\) & \(\) + \(\) \\  & Wake2d & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) + \(\) \\   & HalCheetah & \(42.6\) & \(41.0\) & \(47.4\) & \(44.2\) & \(49.1\) & \(84.0\) & \(\) & \(\) & \(\) & \(\) & \(4.3\) + \(0.5\) \\  & Hopper & \(52.9\) & \(55.8\) & \(66.3\) & \(55.9\) & \(73.6\) & \(76.4\) & \(\) & \(\) & \(\) + \(\) & \(\) + \(\) \\  & Wake2d & \(75.3\) & \(72.5\) & \(78.3\) & \(79.7\) & \(82.5\) & \(79.9\) & \(\) & \(\) & \(\) + \(\) & \(83.7\) + \(1.0\) \\   & HalCheetah & \(36.6\) & \(\) & \(\) & \(42.2\) & \(39.3\) & \(\) & \(41.6\) & \(0.4\) & \(42.9\) & \(0.4\) & \(39.6\) + \(0.4\) \\  & Happer & \(18.1\) & \(59.0\) & \(94.7\) & \(\) & \(\) & \(\) & \(\) & \(\) & \(\) + \(\) & \(93.2\) + \(0.7\) \\  & Wake2d & \(26.0\) & \(77.2\) & \(73.9\) & \(61.2\) & \(75.0\) & \(80.7\) & \(\) + \(\) & \(84.6\) + \(1.7\) & \(78.2\) + \(1.7\) \\   & \(51.9\) & \(77.6\) & \(77\) & \(75.3\) & \(\) & \(\) & \(\) & \(\) & \(\) \\ 

_FinRL_ table are obtained by models trained on real-world datasets, and demonstrate that DiffuserLite continues to exhibit its superiority in these real-world tasks, achieving performance comparable to SOTA algorithms. This illustrates the potential application of DiffuserLite in real-world scenarios.

### Flexible Plugin (RQ3)

To test the capability of DiffuserLite as a flexible plugin to support other diffusion planning algorithms, Aligndiff  is selected as a non-reward-maximizing algorithm backbone, and integrated with DiffuserLite plugin, referred to as AlignDiff-Lite. AlignDiff aims to customize the agent's behavior to align with human preferences and introduces an MAE area metric to measure this alignment capability (refer to appendix A.3 for more details), where a larger value indicates a stronger alignment capability. The comparison of AlignDiff and AlignDiff-Lite is presented in table 5, showing that AlignDiff-Lite achieves a \(560\%\) improvement in decision-making frequency compared to AlignDiff, while only experiencing a small performance drop of \(3.2\%\). This result demonstrates the potential of DiffuserLite serving as a plugin to accelerate diffusion planning across various domains.

### Ablations (RQ4)

**How to choose the number of planning levels \(L\) and the temporal horizons \(H_{l}\)?** We compared the performance of DiffuserLite with 2/3/4 planning levels and with four different temporal horizon designs, and reported the results in the left and right part of table 6, respectively. The list in the first row represents the temporal horizon for each level, with larger values on the left indicating more planning at a coarser granularity, while larger values on the right indicate more planning at a finer granularity. For planning level design, results show a performance drop with 2 planning levels, particularly in Antmaze, and a consistently excellent performance with 3 or 4 planning levels. This suggests that for longer-horizon tasks, it is advisable to design more planning levels. For temporal horizon design, results show a performance drop when the temporal horizon of one level becomes excessively long. This suggests the temporal horizon of each level is supposed to be similar and stay close. We summarized and presented a design choice list in appendix C.

**Has the last level (short horizon) of DiffuserLite already performed well in decision making?** This is equivalent to the direct use of the shorter planning horizon. If it is true, key points generated by former levels may not have an impact, making PRP meaningless. To address this question, we conduct tests using a one-level DiffuserLite with the same temporal horizon as the last level of the default model, referred to as **Lite**_w/ only last level_. The results are presented in table 7, column 2. The notable performance drop demonstrates the importance of a long-enough planning horizon.

**Can decision-making be effectively accomplished without progressive refinement planning (PRP)?** To address this question, we conduct tests using a one-level DiffuserLite with the same temporal horizon as the default model, referred to as **Lite**_w/o PRP_. This model only supports one-shot generation at the inference phase. We also test a "smaller" DD with the same network parameters and sampling steps as DiffuserLite, to verify whether one can speed up DD by simply

    \\ Planning horizon=\(129\) &  &  &  &  &  &  &  \\  HalfCheetah-me & \(75.6 8.3\) & \(\) & \(88.3 0.5\) & \(85.6 0.6\) & \(88.5 0.4\) & \(88.6 0.7\) & \(\) \\ Antmaze-ld & \(0.0 0.0\) & \(68.0 2.8\) & \(\) & \(34.7 4.1\) & \(\) & \(67.3 3.4\) & \(34.0 4.3\) \\  Planning horizon=\(49\) & [7.9] &  &  &  &  &  &  \\  Kitchen-p & \(72.8 0.5\) & \(\) & \(72.3 1.9\) & \(66.7 1.7\) & \(\) & \(74.2 0.6\) & \(31.7 2.7\) \\   

Table 6: **Performance of DiffuserLite with various PRP design choices. The left part shows a comparison with 2/3/4 planning levels and the right part shows a comparison with 4 temporal horizon designs. Results correspond to the mean and standard error over 5 random seeds, the highest scores are emphasized in bold, and the default design choices used across other experiments are underlined.**

  
**Metric** & **GC** & **AlignDiff** & **AlignDiff-Lite** \\  MAE Area & \(0.319 0.005\) & \(0.621 0.023\) & \(0.601 0.018\) (3.2\% ) \\ Frequency (Hz) & – & \(6.9\) & \(45.5\) (560\% 1) \\   

Table 5: **Integrate with DiffuserLite as a plugin. We refer to AlignDiff with DiffuserLite plugin as AlignDiff-Lite. A larger value of _MAE Area_ indicates a stronger alignment capability. AlignDiff-Lite greatly increases decision-making frequency, while only experiencing a small performance drop.**reducing the parameters, referred to as **DD-_small_**. The results are presented in Table 7, column \(3\)-\(4\). The large standard deviation and the significant performance drop provide strong evidence for the limitations of one-shot generation planning, having difficulties in modeling the distribution of detailed long-horizon trajectories. However, DiffuserLite can maintain high performance with fast decision-making frequency due to its life architecture and simplified fitted distribution.

**More ablations.** We conducted further ablation studies on model size, sampling steps, planning horizon, with or without value condition, and visual comparison to better elucidate DiffuserLite. Please refer to appendix E for these sections.

## 6 Related Works

Diffusion models are a type of score-based generative model . Two pioneering frameworks, Diffuser  and DD , were the first to attempt using diffusion models for trajectory generation and planning in decision-making tasks. Based on these two frameworks, diffusion planning has been continuously improved and applied to various decision-making domains [36; 10; 54; 8; 17; 26; 21]. However, long-horizon estimation and prediction often suffer from potential exponentially increasing variance concerning the temporal horizon, called the "curse of horizon" . To address this, HDMI  is the first algorithm that proposed a hierarchical decision framework to generate sub-goals at the upper level and reach goals at the lower level, achieving improvements in long-horizon tasks. However, HDMI is limited by cluster-based dataset pre-processing to obtain high-quality sub-goal data for upper training. Another concurrent work, HD-DA  introduces a similar hierarchical structure and allows the high-level diffusion model to automatically discover sub-goals from the dataset, achieving better results. However, the motivation behind DiffuserLite is completely different, which aims to increase the decision-making frequency of diffusion planning. Also, DiffuserLite allows for more hierarchy levels and refines only the first interval of the previous layer using PRP. Since HD-DA does not ignore redundant information, it fails to achieve a notable frequency increase. Compared to related works, DiffuserLite has more clean plugin design and undoubtedly contributes to increasing decision-making frequency and performance. We believe it can serve as a reference for the design of future diffusion planning frameworks.

## 7 Conclusion

In this paper, we introduce DiffuserLite, a super fast and lightweight diffusion planning framework that significantly increases decision-making frequency by employing the plan refinement process (PRP). PRP enables coarse-to-fine-grained trajectory generation, reducing the modeling of redundant information. Experimental results on various D4RL benchmarks demonstrate that DiffuserLite achieves a super-high decision-making frequency of \(122.2\)Hz (\(112.7\)x faster than previous mainstream frameworks) while maintaining SOTA performance. DiffuserLite provides three generative model backbones to adapt to different requirements and can be flexibly integrated into other diffusion planning algorithms as a plugin. However, DiffuserLite currently has limitations mainly caused by the classifier-free guidance (CFG). CFG sometimes requires adjusting the target condition, which becomes more cumbersome on the multi-level structure of DiffuserLite. In future works, designing better guidance mechanisms, devising an optimal temporal jump adjustment, or integrating all levels in one diffusion model to simplify the framework is worth considering. DiffuserLite may also have some societal impacts, such as expediting the deployment of robotic products that could be utilized for military purposes.

  
**Environment** & **Oracle** & **Lite w/** & **Lite w/o** & **DD** \\  & _only last level_ & _PRP_ & _small_ \\  Hopper-me & \(111.6 0.2\) & \(27.8 10.2\) & \(96.6 1.0\) & \(67.9 24.7\) \\ Hopper-m & \(100.9 1.1\) & \(19.4 2.0\) & \(66.4 20.2\) & \(16.7 8.0\) \\ Hopper-m & \(96.6 0.3\) & \(2.0 0.2\) & \(62.5 31.3\) & \(1.0 0.4\) \\ 
**Average** & \(103.1\) & \(16.4 1.5\%\) & \(75.2\) (\(27.1\%\)) & \(28.5\) (\(27.3\%\) \(\)) \\  Kitchen-m & \(73.6 0.7\) & \(48.2 1.4\) & \(26.9 1.0\) & \(0.0 0.0\) \\ Kitchen-p & \(74.4 0.6\) & \(38.6 3.1\) & \(23.9 0.5\) & \(1.8 0.8\) \\ 
**Average** & \(74.0\) & \(43.4\) (\(41.3\%\)) & \(25.4\) (\(65.7\%\) \(\)) & \(0.8\) (\(98.8\%\) \(\)) \\   

Table 7: **Ablation tests conducted to examine the effectiveness of PRP. Lite w/ only last level and Lite w/o**_PRP_ are two ablated versions of DiffuserLite, while DD-small_ is a version of DD that uses the same network parameters and sampling steps as DiffuserLite. All results are obtained over \(5\) seeds. The varying degrees of performance drop observed in each experiment highlight the importance of PRP.**Acknowledgements

This work is supported by the National Natural Science Foundation of China (Grant Nos. 62422605, 92370132, 62106172), the National Key R&D Program of China (Grant No. 2022ZD0116402) and the Xiaomi Young Talents Program of Xiaomi Foundation.