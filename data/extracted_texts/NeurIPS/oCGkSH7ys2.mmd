# Self-playing Adversarial Language Game

Enhances LLM Reasoning

 Pengyu Cheng\({}^{1}\) Tianhao Hu\({}^{1}\) Han Xu\({}^{1}\) Zhisong Zhang\({}^{1}\)

Yong Dai\({}^{1}\) Lei Han\({}^{3}\) Nan Du\({}^{1}\) Xiaolong Li\({}^{2}\)

Tencent AI Lab \({}^{1}\)Shenzhen & \({}^{2}\)Seattle \({}^{3}\)Tencent Robotics X Lab pengyucheng@tencent.com

###### Abstract

We explore the potential of self-play training for large language models (LLMs) in a two-player adversarial language game called Adversarial Taboo. In this game, an attacker and a defender communicate around a target word only visible to the attacker. The attacker aims to induce the defender to speak the target word unconsciously, while the defender tries to infer the target word from the attacker's utterances. To win the game, both players must have sufficient knowledge about the target word and high-level reasoning ability to infer and express in this information-reserved conversation. Hence, we are curious about whether LLMs' reasoning ability can be further enhanced by **S**elf-**P**laying this **A**dversarial language **G**ame (SPAG). With this goal, we select several open-source LLMs and let each act as the attacker and play with a copy of itself as the defender on an extensive range of target words. Through reinforcement learning on the game outcomes, we observe that the LLMs' performances uniformly improve on a broad range of reasoning benchmarks. Furthermore, iteratively adopting this self-play process can continuously promote LLMs' reasoning abilities. The code is available at https://github.com/Linear95/SPAG.

## 1 Introduction

Large language models (LLMs), such as GPT-4  and Gemini , have reformed the domain of artificial intelligence (AI) with astonishing language capacities, such as

Figure 1: Reasoning improvements from **S**elf-**P**laying of **A**dversarial language **G**ames (SPAG) on comprehensive reasoning benchmarks. With the SPAG epoch increasing, the LLM reasoning ability continuously improves. Each axis is normalized by the maximum answer-accuracy value.

natural language understanding (Yang et al., 2023; Touvron et al., 2023), text generation (Kocon et al., 2023; Anil et al., 2023), machine translation (Jiao et al., 2023), and programming (Surameery and Shakor, 2023; Tian et al., 2023). However, the reasoning ability of LLMs, which is essential for complex problem-solving (Pan et al., 2023) and advanced intelligence-developing (Yao et al., 2021), still retains being challenged by various criteria including correctness (Zhang et al., 2023) and faithfulness (Turpin et al., 2024).

To address the reasoning challenge of LLMs, plenty of works have contributed in-depth efforts from the perspectives of Chain-of-Thought (CoT) prompt engineering (Wei et al., 2022; Ding et al., 2023; Yao et al., 2024), and the usage of auxiliary reasoning tools (Pan et al., 2023). However, both prompt-based and tool-calling methods require additional prompt designs, which are inconsistent and sensitive to different prompt patterns and LLM checkpoints (Turpin et al., 2024; Chu et al., 2023). More fundamental and consistent reasoning-improving approaches are post-pretraining (Zarepaev et al., 2023) and fine-tuning (Dong et al., 2023), which trains LLMs with additional reasoning-related text corpus. Nevertheless, these methods demand sufficient high-quality textual data, which are difficult to collect due to the massive costs of human annotation efforts (Singh et al., 2023).

To improve LLM reasoning more efficiently, self-improvement methods, which enhance LLMs with model-generated synthetic data, have recently attracted increasing research attention (Singh et al., 2023; Huang et al., 2023; Burns et al., 2023; Chen et al., 2024). Self-improvement methods usually utilize the intrinsic language capability of LLMs to judge (Huang et al., 2023), filter (Yuan et al., 2024), or revise (Yuan et al., 2024) self-generated samples to enhance their quality. However, most self-improvement methods rely on a broad range of high-quality question queries to prevent over-fitting into a sub-domain of reasoning tasks, which still requires additional data collection and cleaning. Besides, the judgments from LLMs are not guaranteed objective (Raina et al., 2024). If an LLM already has an incorrect or biased recognition of a particular concept, the self-improvement process can reinforce and amplify the LLM's cognitive dissonance.

Towards more general and objective self-reasoning-improvement methods, we are inspired by the advancement from AlphaGO (Silver et al., 2016) to AlphaGO Zero (Silver et al., 2017), in which the game agents' intelligence continuously promotes via self-play without any human knowledge. Analogically, we expect to set up a language game where LLMs can improve their reasoning capacities via reinforcement learning (RL) during self-play. Although language games have attracted increasing attention in natural language processing (Lewis et al., 2017; Hausknecht et al., 2020; Xu et al., 2023; Wu et al., 2024), most of them are specially designed with customized game rules, in lack of the generalization to improve the general language capacities of LLMs. Among a few general-target language games including red-teaming (Ma et al., 2023), negotiation (Lewis et al., 2017), and bargain (Abdulhai et al., 2023), additional human judgments or reward models are required for outcome determination, which posts challenges on the efficiency and effectiveness of large-scale self-play RL training. Recent studies have raised interest in entity- or word-based language games, such as _20-Question_(Zhang et al., 2023) and _Guess-My-City_(Abdulhai et al., 2023), which provide not only straight-forward word-level outcomes but also language universality by traversing the game word from comprehensive vocabularies. However, unlike the GO game, these word-based games are out of the adversarial scheme, limiting the game intensity and self-play learning effectiveness.

With the above consideration, we select an adversarial language game called _Adversarial Taboo_(Yao et al., 2021), in which an attacker and a defender perform a conversation around a target word

Figure 2: Examples of _Adversarial Taboo_ with the same target word “conversation”. The left shows an attacker-winning game, in which the defender unconsciously speaks out the target word. The right is a defender-winning episode because the defender makes the correct inference from the dialogue.

only visible to the attacker. The attacker aims to induce the defender to speak out the target word unconsciously; the defender tries to avoid unconscious utterance of the target word and guess the word from the dialogue history. To win the adversarial game in information-limited conversations, both players are required to have high-level language capacities in terms of expression, upstanding, and reasoning. Moreover, by collecting target words from a vast vocabulary, this game can cover a broad range of topics providing sufficient language versatility. Besides, the game outcomes can be automatically and explicitly judged: we only need to check whether the target word appears in the defender's utterances (attacker wins) or its inference patterns (defender wins). We conduct the self-play on this adversarial game using open-source LLMs, LLAMA-2-7B (Touvron et al., 2023) and Baichuan-2-13B (Yang et al., 2023), with target words selected from a 50K top-frequency vocabulary (Davies, 2020). Next, we conduct offline reinforcement learning on the game outcomes and observe significant performance improvement on a broad range of LLM reasoning benchmarks. Furthermore, we iterate this sampling-learning process with three epochs, within which the LLMs' reasoning can continuously obtain improvement. We believe this novel training scheme, **S**elf-**P**lay of **A**dversarial **G**ame (SPAG), has great potential for developing advanced LLM capacities.

## 2 Preliminary

With the development of LLMs (OpenAI, 2023, 2022), reinforcement learning (RL) has played an increasingly important role in language model training (Ouyang et al., 2022, Ramamurthy et al., 2023). The prime application scenario for RL in LLM training is reinforcement learning from human feedback (RLHF) (Yuan et al., 2023, Cheng et al., 2023, Zeng et al., 2023). RLHF first learns a reward model \(r(,)\) from the human feedback preference pairs (Cheng et al., 2023), and then optimizes the LLM policy \(_{}(|)\) to maximize the expected reward value (Dong et al., 2023):

\[_{}(_{})=-_{, _{}(|)}[r(,)].\] (1)

To learn the above objective, proximal policy optimization (PPO) (Schulman et al., 2017) algorithm has been recognized as the mainstream solution. In each update to equation 1, PPO minimizes:

\[_{}(_{})=-_{, _{}(|)}(| )}{_{}(|)}^{_{}}-[_{ }\|_{}],\] (2)

where \(_{}\) is a copy of \(_{}\) before the update, \(^{_{}}\) is the estimated advantage value (Schulman et al., 2016) with respect to the reference policy \(_{}\), and \([_{}\|_{}]\) is the Kullback-Leibler (KL) (Kullback, 1997) divergence regularizing \(_{}\) with an appropriate updating step. However, PPO for LLMs has been continually challenged due to its inefficient natural-text online sampling and the unstable training processes (Baheti et al., 2024). Among the improvements to PPO (Rafailov et al., 2023; Yuan et al., 2023; Dong et al., 2023), Baheti et al. (2024) adopts the PPO objective into an offline scheme by using importance sampling (Neal, 2001), which is named Advantage-Leftower-Lunch (A-LoL):

\[_{}}_{}=_{ ,_{}(|)}^{_{ }}(|)}{_{}(| )}_{}_{}(|).\] (3)

Here the sample \(_{}(|)\) and advantage \(^{_{}}\) are both from the reference distribution \(_{}(|)\) and calculated offline. Besides, Gulcehre et al. (2023) proposed Reinforced Self-Training (ReST) to simplify the RLHF scheme. With a threshold \(\), ReST updates the LLM by the reinforcement on the selected samples \(_{}=\{(,):r(,)>\}\):

\[_{}(_{})=_{, _{}(|)}_{r(,)> }_{}(,)=_{_{ }}[_{}(,)],\] (4)

where \(_{}(,)\) could be any offline RL loss such as A-LoL or the vanilla language modeling loss.

## 3 Self-play of Adversarial Language Games

The game of Adversarial Taboo is first introduced by Yao et al. (2021), in which an attacker \(\) and a defender \(\) involve a multi-turn conversation. At the beginning of the game, the attacker is assigned a target word \(w_{}\), which is not informed to the defender. The attacker's target is to induce the defender to speak the target word unconsciously. To achieve this goal, the attacker can talk about any topic related to \(w\), except directly speaking out the target word. In contrast, the defender is required to infer the target word without any unconscious utterance of the word. If the defender has sufficient confidence to infer the word, it can spell "I know the word! It is {_target word_}!". Then the game terminates. If the guess is correct, the defender wins, otherwise the attacker wins. Besides, the game has a maximum number of turns \(T_{0}\). If nobody wins during \(T_{0}\) turns, there is a tie. The examples of Adversarial Taboo are shown in Figure 2.

``` Inputs: LLM policy \(_{}(|)\), target word \(w\), attacker and defender prompts \(f_{},f_{}\).  Set the initial state \(_{0}=(w)\). for\(t\) from \(1\) to \(T\)do  Sample an attacker utterance \(_{t}_{}(_{t}|_{t-1})=_{}(=_{ t}|=f_{}(_{t-1}))\).  Update state \(^{}_{t}=(w,_{1},_{1},,_{t-1},_{t-1}, _{t})\).  Sample a defender utterance \(_{t}_{}(_{t}|^{}_{t})=_{}( =_{t}|=f_{}(^{}_{t}))\).  Update state \(_{t}=(w,_{1},_{1},,_{t-1},_{t-1},_{t },_{t})\). endfor  Collect an episode \(=(_{0},^{}_{1},_{1},,^{}_{T}, _{T})\) ```

**Algorithm 1** Data collection of LLM self-plays for the adversarial language game.

### Adversarial Language Game Modeling

We view the Adversarial Taboo as a two-player zero-sum Markov game (Littman, 1994), which can be described by a tuple as \((,,F,r)\):

* The state space \(=\{_{t},^{}_{t}:1 t T_{0}\}\) contains two types of states, \(^{}_{t}=(w,_{1},_{1},_{2},,_{t})\) and \(_{t}=(w,_{1},_{1},_{2},,_{t},_{t})\), where \(\{_{i}\}_{i=1}^{t}\) and \(\{_{i}\}_{i=1}^{t}\) are the utterances of the attacker and defender, respectively. Games start at \(_{0}=(w)\) with a target word \(w_{}\) and end with \(T_{0}\) maximum turns. States \(^{}_{t}\) and \(_{t}\) end with utterances \(_{t}\) and \(_{t}\) for the defender and attacker to act, respectively.
* The action space \(\) is shared with both the attacker and defender, which is equivalent to the token sequence space of natural language \(=\{=(x_{1},x_{2},,x_{L})|x_{l}_{},L_{+}\}\).
* The transition function \(F:\) deterministically appends the utterance \(_{t}\) or \(_{t}\) at the end of the dialogue, and converts \(^{}_{t}=F(_{t-1},_{t})\) and \(_{t}=F(^{}_{t},_{t})\).
* The reward \(r:\) evaluates the actions \(,\) based on their corresponding states \(,^{}\) with rewards \(r(,)\) and \(r(^{},)\), respectively. Given a game episode \(=(_{0},^{}_{1},_{1},,^{}_{ T},_{T})\), we denote the _attacker's total reward_\(R()=_{t=1}^{T}r(_{t-1},_{t})\), so the _defender's total reward_ is \(_{t=1}^{T}r(^{}_{t},_{t})=-R()\) to satisfy the _zero-sum_ constraint. More detailed reward designs with heuristic rules for the Adversarial Taboo can be found in Appendix B.

In the above game, we denote \((|)\) and \((|^{})\) as the attacker's and defender's policies, respectively. Then each episode \(\) can be regarded as a trajectory with the probability:

\[P()=P(_{0})_{t=1}^{T}P(^{}_{t}|_{t-1}) _{t=1}^{T}P(_{t}|^{}_{t})=P(w)_{t=1}^{T}( _{t}|_{t-1})_{t=1}^{T}(_{t}|^{}_{t})=:( ),\] (5)

where \(P(w)\) is the data distribution of target word \(w_{}\). Then we can write the self-play objective of the Adversarial Taboo as:

\[_{}_{}_{}(,):=_{ }[R()],\] (6)

in which the attacker tries to maximize its total reward \(R()\) by optimizing policy \(\), and the defender seeks strategies \(\) to maximize the defender reward \(-R()\) (minimize \(R()\)). To play the above adversarial game with an LLM generation policy \(_{}(|)\), we first design prompt templates \(f_{},f_{}:\) for the attacker and defender respectively to convert the game states into natural language task descriptions. Next, we introduce the game policies for the two players:

\[_{}(|)=_{}(|f_{}()),_{}(|^{})=_{}(|f_{}(^{ })).\] (7)

The detailed prompt templates for the game are demonstrated in Appendix A.

### Imitation Learning

Due to the limited capability of current open-source LLMs, the generation policy \(_{}(|)\) can not guarantee the strict instruction-following of the game rules in prompts \(f_{}()\) and \(f_{}(^{})\). Therefore, before the self-play, we first conduct an imitation learning (behavior cloning) of GPT-4's behaviors to ensure that \(_{}(|f_{}())\) and \(_{}(|f_{}(^{}))\) act consistently with the game rules. To collect the game episodes of GPT-4 (Achiam et al., 2023), we use the data collection procedure described in Algorithm 1. Similar to the setups in equation 7, we also design attacker and defender prompts for GPT-4 to act as the game players, which can be found in Appendix A.1.

After collecting a group of GPT-4 game episodes for imitation learning as \(_{}\), we divide it into an attacker-winning set \(_{}^{}=\{_{}:R ()>0\}\) and a defender-winning set \(_{}^{}=\{_{}:R ()<0\}\). The imitation learning loss is to maximize the log-likelihood of winners' actions:

\[_{}^{}(_{})= -_{_{}^{}} _{t=1}^{T}_{}(_{t}|f_{}( _{t-1}))+_{1}[_{}\|_{}],\] (8) \[_{}^{}(_{})= -_{_{}^{} }_{t=1}^{T}_{}(_{t}|f_{}( _{t}^{}))+_{1}[_{}\|_{}],\] (9)

where the re-weighting parameter \(_{1}>0\) and the regularizer \([_{}\|_{}]\) prevents the model from over-fitting on the language game task and maintains the model's general language abilities. The reference model \(_{}\) is the initial checkpoint of the LLM before training. The overall imitation learning objective is \(_{}(_{})=_{}^{ }(_{})+_{}^{} (_{})\).

### Reinforcement Learning from Self-play

Imitation learning enables LLM to behave consistently following the game rules. Next, we conduct self-play by letting the LLM \(_{}(|)\) play alternatively as the attacker \(_{}(|)=_{}(|f_{}())\) and the defender \(_{}(|^{})=_{}(|f_{}( {s}^{}))\). Note that the self-play sampling process involves massive multi-turn auto-regressive text generation of the LLM, which causes heavy computational complexity and makes the on-policy RL training intensively inefficient. Therefore, we use an offline learning scheme: (1) make a copy \(_{}\) of current LLM policy \(_{}\); (2) collect self-play episodes \(_{}=\{_{}_{}\}\) from games between attacker \(_{}\) and defender \(_{}\); (3) update \(_{}\) via RL training with \(_{}\). The details of the collection of \(_{}\) are shown in Algorithm 1.

With a group of episodes \(_{}\), we first fix the defender policy \(_{}\) and consider updating the attacker policy \(_{}\) with respect to the self-play objective \(_{}(_{},_{})=_{ _{}_{}}[R()]\). We calculate the corresponding policy gradient for the attacker as:

\[_{}_{}(_{},_{})= _{_{}_{}} _{t=1}^{T}A_{t}^{_{}}_{}_{}(_ {t}|_{t-1})\] \[= _{_{}_{}} _{t=1}^{T}A_{t}^{_{}}(_{t}|_{t -1})}{_{}(_{t}|_{t-1})}_{}_{ }(_{t}|_{t-1}),\] (10)

where \(A_{t}^{_{}}=A^{_{}}(_{t-1},_{t})\) is the advantage of action \(_{t}\) for the attacker \(_{}\) in the self-play of \(_{}_{}\). Here we apply importance sampling to unbiasedly estimate the expectation _w.r.t_\(_{}(_{t}|_{t-1})\) with the sampled actions from \(_{}(_{t}|_{t-1})\) in \(_{}\). Inspired by TRPO (Schulman et al., 2015) and PPO (Schulman et al., 2017), we design the following loss to optimize \(_{}(_{},_{})\):

\[_{}^{}(_{})= -_{_{}}_{t=1}^{ T}(_{t}|_{t-1})}{_{}(_{t}|_{t-1 })}_{t}^{_{}}-_{2}[_{}\|_{}]\] \[= -_{_{}}_{t=1}^{ T}(_{t}|f_{}(_{t-1}))}{_{}(_{t}|f_{ }(_{t-1}))}_{t}^{_{}}-_{2}[ _{}\|_{}],\] (11)

where the re-weighting parameter \(_{2}>0\), and regularizer \((_{}\|_{})\) guarantees an appropriate policy update step size. Following TRPO (Schulman et al., 2015), we use empirical estimation \(_{t}^{_{}}\) of policy \(_{}\) to approximate the advantage \(A_{t}^{_{}}\). More details of advantage estimation are described in Appendix D. Similarly, from the perspective of the defender, we propose the corresponding loss:

\[_{}^{}(_{})=-_{ _{}}_{t=1}^{T}(_{t}|f_ {}(_{t}^{}))}{_{}(_{t}|f_{}( _{t}^{}))}_{t}^{_{}}-_{2}[_{ }\|_{}].\] (12)

In practice, we find when negative advantage values exist, the above policy gradient method can cause training instability and damage the LLMs' general language performance. To mitigate the issue and obtain more stable RL training, we utilize the methodology of ReST (Gulcehre et al., 2023) as in equation 4, which considers the offline learning with samples \(\{:R()>\}\) selected by a reward threshold \(\). More specifically, we set the reward threshold \(=0\) and select the attacker-winning episodes \(_{}^{}=\{_{}:R()>0\}\) and the defender-winning episodes \(_{}^{}=\{_{}:R()<0\}\) for the attacker and the defender training respectively. Similar techniques have also been studied in earlier RL literature to obtain stable policy updates, such as self-imitation learning (Oh et al., 2018) and the UPGO (Vinyals et al., 2019) methods. Therefore, the overall self-play of adversarial language games (SPAG) objective is:

\[_{}(_{})= -_{_{}^{}} _{t=1}^{T}(_{t}|f_{}(_{t -1}))}{_{}(_{t}|f_{}(_{t-1}))}_{t}^{ _{}}-_{2}[_{}\|_{}] \] (13) \[-_{_{}^{}} _{t=1}^{T}(_{t}|f_{}(_{ t}^{}))}{_{}(_{t}|f_{}(_{t }^{}))}_{t}^{_{}}-_{2}[_{}\|_ {}]-_{(,)_{ }}[_{}(|)],\]

where \(>0\) is a re-weighting hyper-parameter, and \(_{_{}}[_{}(|)]\) is the log-likelihood on a supervised fine-tuning (SFT) dataset \(_{}\) to prevent LLMs from losing general language abilities.

## 4 Experiments

To verify the effectiveness of SPAG, we select open-source pretrained LLMs of different sources and model sizes, particularly LLaMA-2-7B (Touvron et al., 2023) and Baichuan-2-13B (Yang et al., 2023). As introduced in Section 3, the training process includes two stages: imitation learning of GPT-4, and self-play learning on game episodes. For baseline comparison, we consider Chain-of-Thought (CoT) (Wei et al., 2022) and continuous supervised fine-tuning (SFT) methods. Besides, we also test another two keyword-based non-adversarial language games: _20-Question_ and _Guess-My-City_ as described in Abdulhai et al. (2023). More details about the two games are in Appendix E.

### Experimental Setups

Training Data PreparationThe training data preparation consists of the following three parts. More data collection details are in Appendix A.

* _Target Words:_ We aim to play the adversarial game with an extensive range of target words so that diverse topics can be discussed during the self-play processes, which helps maintain the generalization ability of LLMs. Hence, we collect the 50K most frequently used words from the Corpus of Contemporary American (CoCA) (Davies, 2020) as the target word list \(_{}\). Besides, stop words defined in NLTK (Bird, 2006), which are commonly used with insignificant semantic meaning, are filtered out of \(_{}\).
* _Imitation Learning Data:_ To enable the instruction-following ability of open-source LLMs on game rules, we use the same data collection process in Algorithm 1 via the GPT-4 (OpenAI, 2023) API and play the Taboo game one episode per target word. The attacker and defender prompts are in Appendix A.1. Due to the resource limitation, we only collect the GPT-4 self-play samples with the top 30K words from \(_{}\). The maximum interaction turn is randomly selected in the range \(\). The results are collected as \(_{}\) for the imitation learning.
* _Supervised Funetuning (SFT) Data:_ We also prepare general query-response SFT data to prevent LLMs from being over-fitted on the adversarial game. We use Alpaca (Taori et al., 2023) as the SFT set, which contains 52K instruction-following data from GPT-3 (Brown et al., 2020).

[MISSING_PAGE_FAIL:7]

non-adversarial setups: Imitation-_20Q_ (on _20-Question_) and Imitation-_GuessCity_ (on _Guess-MyCity_). From the results, we find the IM model on Adversarial Taboo outperforms models trained on non-adversarial games, highlighting the effectiveness of the adversarial game setups for reasoning improvement. Besides, we report the Chain-of-Thought (CoT) reasoning results of LLaMA-2 (Lama-2-Base-CoT) and Alpaca-2 (AlpacaSFT-3-CoT). Although the CoT method on LLaMA-2 reaches conspicuous performance on BBH and WinoGrande, the IM model can still surpass the CoT results in terms of overall reasoning performance (geometric mean).

Self-play TrainingAfter imitation learning, we conduct three epochs of SPAG training as described in Algorithm 2. As shown in Figure 1, on most of the reasoning benchmarks, both LLaMA-2-7B and Baichuan-2-13B have their performance steadily improved with the SPAG training epoch increasing. For LLaMA-2-7B, although the first-epoch SPAG model has a relatively lower performance than the imitation-learned (IM) model on WinoGrande, after an additional epoch of self-play iteration, SPAG-2 has achieved sufficient improvement to surpass the performance of the IM model. Considering the general language capability on MMLU, SPAG models can not guarantee continuous improvement, especially for Baichuan-2-13B whose performance slightly decays during the SPAG training. Compared to the improvements in the reasoning benchmarks, this language-understanding decline is still within an acceptable range, since the overall performance (GM score) maintains increasing significantly. For the baseline comparison, we report the continuous SFT on the IM models (IM+AlpacaSFT) and self-played models on non-adversarial games (SP-_20Q_ and SP-_GuessCity_). On both LLaMA-2 and Baichuan-2, continuous SFT models have lower performance scores compared with the corresponding SPAG-1 models. For non-adversarial self-play, SP-_GuessCity_ even performs worse than Imitation-_GuessCity_, which with a high probability is because the game _Guess-My-City_ has a more narrow topic range, insufficient to comprehensively improve the general LLM capacity.

Ablation StudySince the SPAG training loss includes the SFT data, we conduct an ablation study to test whether the performance gains on reasoning benchmarks come from the SFT data or the SPAG method. More specifically, we follow the training setups in Alpaca  and conduct SFT on LLaMA-2-7B and Baichuan-2-13B with three epochs. The checkpoint after the \(i\)th-epoch training is denoted as AlpacaSFT-\(i\) (\(i=1,2,3\)) and tested on the same evaluation sets. The SFT models' performances are also reported in Figure 1 and Table 1 & 2. With the LLaMA base, our SPAG-3 model can uniformly defeat the SFT baselines with clear performance gaps on all benchmarks. With the Baichuan base, except the Mutual set, the SPAG models maintain noticeable advantages on other metrics. Considering the distinct surpass of overall performance, we claim that the major contribution to the reasoning improvements should be credited to the SPAG scheme.

Moreover, we test the sample efficiency and hyper-parameter effectiveness of SPAG in Figure 3. For sample efficiency during imitation learning, we vary the imitation data size by collecting the GPT-4 game episodes on target words with top-iK frequency (\(i=1,2,3,5,10,15,20,30\)). The ablation results are shown in the first column of Figure 3. When game episode size increases larger than \(5\)K, imitation from GPT-4 cannot significantly provide additional reasoning gains. For the KL coefficient \(_{1}\), we test values in range \([0,0.4]\). In figure 3 second column, we found KL coefficients around \(_{1}=0.1\) have more satisfying performance regarding reasoning improvements. For self-play ablation in the most right column, we find that when the SFT coefficient \(>0.5\), it cannot bring remarkable benefits for reasoning improvements. The KL coefficient of the SPAG loss reaches the best performance with values around \(_{2}=0.2\). As for sample efficiency (third column of Figure 3), we find the performance gain from the increasing episode number is not as significant as in the imitation learning stage. However, more self-play episodes still bring higher reasoning scores.

    & MMLU & BBH & Mutual & ARC-e & ARC-c & LGQA2 & WGrande & PIQA & GM (Avg.) \\  Baichuan-2-13B & **59.00** & 39.03 & 53.72 & 77.36 & 46.84 & 30.73 & 69.93 & 77.42 & 54.21 \\  AlpacaSFT-1 & 52.94 & 36.52 & **58.35** & 74.12 & 44.88 & 33.33 & 71.35 & 77.20 & 53.68 \\ AlpacaSFT-2 & 51.27 & 36.60 & 57.67 & 73.36 & 44.28 & 33.46 & 69.77 & 75.90 & 53.00 \\ AlpacaSFT-3 & 52.14 & 36.57 & 55.08 & 69.44 & 42.15 & 33.91 & 66.61 & 74.59 & 51.79 \\ Imitation-AG & 58.37 & 39.49 & 57.11 & 76.60 & 47.53 & 33.65 & 70.59 & 78.49 & 55.45 \\  IM+AlpacaSFT & 57.45 & 39.60 & 58.01 & 77.61 & 48.55 & 34.10 & 70.60 & 78.50 & 55.80 \\ SPAG-1 & 57.93 & 39.81 & 57.45 & 78.20 & 48.55 & 35.05 & 70.67 & 78.69 & 56.09 \\ SPAG-2 & 57.99 & 39.97 & 57.67 & **78.32** & 49.83 & 35.62 & 71.03 & 78.83 & 56.52 \\ SPAG-3 & 57.75 & **40.30** & 57.79 & 78.11 & **50.00** & **36.26** & **71.43** & **79.05** & **56.75** \\   

Table 2: Reasoning Performance of SPAG on Baichuan-2-13B.

Game-Play PerformanceBesides the evaluation of LLMs' natural language abilities, we review the models' game-play performance in terms of the win rates in the testing set \(_{}\). We first test our IM models and SPAG models with GPT-4 as the opponent. For each testing word in \(_{}\), we play the language game twice, once GPT-4 as the attacker, and GPT-4 as the defender for another time. The average win rates are reported in the left-hand-side plot of Figure 4, in which one can observe uniform and continuous win rate improvement of SPAG models playing with GPT-4. We also let SPAG models play the game against each other and report the attacker win rate in the right-hand-side plot of Figure 4, in which we find the defender's performance continuously enhanced along with the SPAG epoch increasing. Besides, we also provide the self-play statistics including interaction number and average utterance length in supplementary Figure 6, in which both players choose to use less communication to achieve victory. Self-play examples are attached in Appendix G.

## 5 Conclusion

Towards more efficient reasoning-improving methods for LLMs, we introduce a novel training strategy called **S**elf-**P**lay learning in **A**dversarial language **G**ame (SPAG). In our method, a given LLM first learns to act as an attacker and a defender to play the language game named _Adversarial Taboo_ via imitation learning. Next, we collect self-play episodes from the LLM playing against a copy of itself in the adversarial game. Finally, the LLM is further reinforced on the selected winning episodes via our SPAG algorithm. We repeat this self-play & reinforcement process for three epochs and find that the LLM's reasoning performance continuously and uniformly improves on various benchmarks. The SPAG algorithm explores a new path to improve the fundamental capabilities of LLMs from the perspective of multi-agent self-play. With more elaborate language game designs under more comprehensive task setups, we believe the self-play approaches have great potential for developing a broader range of advanced language abilities of LLMs.

Figure 4: Game results on the testing word list. Left: average win rates of SPAG models playing against GPT-4. Right: average win rate of SPAG _attackers_ against different-epoch checkpoints.

Figure 3: Ablation study of hyper-parameters and data efficiency on imitation learning and first-epoch self-play training. The geometric mean (GM) scores overall reasoning benchmarks are reported. For episode-size ablations, the X-axis is in the logarithmic scale.

## 6 Limitations

Due to the limited computational resources, we only verified the effectiveness of the SPAG method on two open-source LLMs, LLaMA-2-7B and Baichuan-2-13B. The SPAG performances for LLMs with larger sizes have not been empirically evaluated. Besides, more effective estimation methods for the value function and advantages remain unexplored in the SPAG training. For example, Monte Carlo tree search (MCTS) (Coulom, 2006) can be applied to the value function estimation of the Adversarial Taboo game. Also, actor-critic algorithms (Konda and Tsitsiklis, 1999) can provide more accurate policy gradient estimation with lower variances, which have not been tested on SPAG.

Although self-playing adversarial language games can continuously improve the reasoning performance of LLMs, we haven't conducted sufficient studies about the harmful impact of this adversarial self-play training on LLMs. It remains unclear whether LLMs have learned unsafe behaviors such as cheating, bluffing, or other disgraceful tricks to win the adversarial games.

## 7 Broader Impacts

From our experiments, we have found the LLM capacities on a particular task can be continuously enhanced through self-play training. This indicates that we are closer to the LLMs' AlphaGo Zero moment: the intelligence level of AI agents can rapidly surpass human beings by self-playing on a particular language task without any supervision, as which already happened on the GO game (Silver et al., 2017). By designing various self-play environments for LLMs, we can expect that the LLMs can comprehensively surpass humans in terms of intelligence level. This raises the urgency to study the methods to ensure the safety of such super-AIs. Although some of the works have already been devoted to this direction, such as the SuperAlignment (Burns et al., 2023) from OpenAI, more research alertness is required from the LLM community. Besides, within adversarial language games such as Adversarial Taboo, LLMs have great potential to develop harmful language tricks (such as cheating and bluffing) to achieve victory. We warn developers to make security checks on the self-played LLMs as exhaustively as possible.