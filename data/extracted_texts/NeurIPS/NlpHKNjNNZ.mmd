# Just Add $100 More: Augmenting Pseudo-LiDAR Point Cloud for Resolving Class-imbalance Problem

Mincheol Chang

Korea University

mincheoree@korea.ac.kr

Work done during intern at NAVER LABS

Siyeong Lee

NAVER LABS

siyeong.lee@naverlabs.com

Jinkyu Kim

Korea University

jinkyukim@korea.ac.kr

&Namil Kim

NAVER LABS

namil.kim@naverlabs.com

Work done during intern at NAVER LABS

###### Abstract

Typical LiDAR-based 3D object detection models are trained with real-world data collection, which is often imbalanced over classes. To deal with it, augmentation techniques are commonly used, such as copying ground truth LiDAR points and pasting them into scenes. However, existing methods struggle with the lack of sample diversity for minority classes and the limitation of suitable placement. In this work, we introduce a novel approach that utilizes pseudo LiDAR point clouds generated from low-cost miniatures or real-world videos, which is called Pseudo Ground Truth augmentation (PGT-Aug). PGT-Aug involves three key steps: (i) volumetric 3D instance reconstruction using a 2D-to-3D view synthesis model, (ii) object-level domain alignment with LiDAR intensity simulation, and (iii) a hybrid context-aware placement method from ground and map information. We demonstrate the superiority and generality of our method through performance improvements in extensive experiments conducted on popular benchmarks, i.e., **nuScenes**, **KITTI**, and **Lyft**, especially for the datasets with large domain gaps captured by different LiDAR configurations. The project webpage is https://just-add-100-more.github.io.

## 1 Introduction

LiDAR-based 3D object detection has garnered growing interest thanks to its wide applications, including fully autonomous vehicles (or robots) where the LiDAR point cloud is a main reliable source for 3D scene understanding. There exists a large volume of literature on LiDAR-based 3D object detection models , and they have achieved remarkable performance improvements via better model architectures and efficient LiDAR points representation. Still, we observe that these models often suffer from the so-called class imbalance problem - they tend to be overfitted to frequently observed objects

Figure 1: **Overview. We present PGT-Aug, a novel cost-effective pipeline that generates and augments pseudo-LiDAR samples (from miniatures and web videos) to effectively reduce the performance gap between majority-class vs. minority-class objects.**(of majority classes) and underfitted to rarely observed objects (of minority classes), causing a severe performance gap. Such an imbalance problem is commonly observed in real-world data collections, such as KITTI  and nuScenes .

A naive solution for the imbalance problem is collecting more LiDAR data, but achieving a sufficient number of long-tail samples is still practically challenging. Due to the inherent imbalance in the natural distribution, the more data we collect, the worse the imbalance. In literature, one common strategy to deal with class imbalance is a copy-and-paste-based sample augmentation [10; 11; 12; 1], where objects (of minority classes) from other frames are copied into the current frame, balancing class-wise occurrences during training (see Fig. 1 top). However, these methods are still limited in that (i) the areas where objects can be pasted depend on their original position (thus limiting the positional diversity), and (ii) sample diversity where the same object (of minority classes) can be repeatedly copied and pasted.

An ideal sample augmentation method for balancing performance across classes may need the 3D shape information of diverse samples (of minority classes) to enable flexible placements into a given scene. Recent Neural Radiance Field (NeRF)-based approaches [13; 14; 15; 16] have demonstrated the ability to generate high-quality 3D scenes from multiple viewpoints at low computational costs. Here, we advocate utilizing NeRF-based approaches to obtain the 3D shape of objects from external sources (not from other scenes within the same data). However, generating diverse real-world 3D objects, especially of minority classes, can pose challenges due to the need for images from multiple viewpoints. To address this, we propose using miniatures and public videos to obtain 3D-rendered samples of target objects. Miniatures, commonly used for practical effects in the film industry, offer a practical solution to collect diverse samples, particularly for various types of vehicles.

In this work, we introduce Pseudo Ground Truth Augmentation (PGT-Aug), which generates point clouds of minority-class objects from two sources: (i) surround-view videos of given miniatures and (ii) public videos of real-world objects. As shown in Fig. 1 bottom, we first utilize a 2D-to-3D renderer to reconstruct an object's 3D volumetric representation. Then, we transform it into LiDAR-like 3D point clouds by rearranging and filtering points and estimating their intensities. During training, such generated pseudo-LiDAR points are sampled and placed into appropriate places of a scene without where-to-paste technical constraints. Our experiments with public datasets (i.e. nuScenes , KITTI , and Lyft ) demonstrate that our data augmentation with pseudo-LiDAR points effectively improves detection performance for minority classes. Our contributions can be summarized into three-fold:

* We introduce PGT-Aug, a novel cost-effective pipeline for LiDAR-based object detectors to solve the class imbalance problem by (i) generating pseudo-LiDAR samples (from multi-view images of miniatures and public videos of an object) and (ii) augmenting them during training to balance the performance gap across classes.
* Our pipeline involves a novel view-agnostic pseudo-LiDAR sample generation where we reduce the domain gap between the real-world and the generated LiDAR point clouds. A series of processes, such as spatial distribution matching and data-driven intensity adjustments, achieve this.
* We propose a novel map-aware augmentation technique that determines where to paste the pseudo-LiDAR samples based on map-based scene context (i.e. placing an object into the appropriate locations in the given scene).

## 2 Related Work

**LiDAR-based 3D Object Detection Datasets for Autonomous Driving.** The automotive industry has shown a surge of interest in LiDAR-based 3D object detection, enhancing system reliability compared to camera-based detectors. However, creating large-scale annotations face challenges due to sensor costs, spatial distribution variations, and annotation difficulties caused by reflections and occlusions. While several annotated datasets [9; 17; 18; 19; 20] exists, the limited quantity, compared to the image domain, leads to both generalization issues and class imbalance [21; 22]. To mitigate this, we propose a novel approach for generating high-quality rare objects inexpensively for data augmentation in 3D object detection, addressing the class imbalance. (see Fig. 7 in the Supplementary Material.)

**Data Augmentation of LiDAR Point Clouds.** Data augmentation plays a crucial role in increasing the diversity of training samples and addressing class imbalance in datasets. Two common strategiesfor mitigating overfitting and enhancing model robustness are data resampling and data synthesis. Data resampling [23; 24; 25; 26; 27] primarily focus on oversampling minority classes by replicating existing samples, thereby increasing the frequency of minority classes and balancing class distributions during model training. In contrast, data synthesis [28; 29; 30] relies on blending of original samples or utilizing synthetic data from generative models. This approach enhances the diversity of the training set, helping to address data scarcity and improve overall model generalization.

Recent works have tackled 3D annotation difficulties through LiDAR simulation [10; 31; 32; 33] for 3D renderings or data augmentation [1; 34; 35; 36; 37; 12] for utilizing existing data. Some works [10; 32; 38] attempted to reduce a domain gap between simulated and real-world data by combining them, as seen in SHIFT3D , which employs deep signed distance functions to refine object's shape and pose adversarially. GT-Aug  used copy-and-paste strategies, while Real-Aug  enhanced data utilization with realistic placement. Creating diverse, non-standard shapes requires considerable time and effort through 3D CAD modeling. On the other hand, 3D objects in the target dataset are partially observed, making free placement challenging. Different from previous approaches, we collect miniatures and real-world objects to render realistic 3D samples with low cost from diverse domains, and introduce an object-level augmentation with flexible placeability.

**Neural Radiance Fields and 3D Rendering.** Many works have attempted to represent 3D scenes as continuous implicit representations or differentiable structures [13; 39; 40; 41; 14]. NeRF  represented 3D geometry by approximating density and view-dependent RGB using a simple MLP architecture. Among many subsequent works [14; 15; 42; 43; 16], both Instant-NGP  and Plenoxels  contributed to faster training and rendering of 3D scenes. Instant-NGP accelerated MLPs using multilevel hash tables, while Plenoxels utilized sparse voxel grids for interpolating color and density field. These advances can make the reconstruction of 3D objects easier and more precise , facilitating their use in various perception tasks .

## 3 Method

We introduce PGT-Aug, a fast, realistic, and low-cost pipeline that generates (and collects) pseudo-LiDAR point clouds of minority-class objects from multi-view images of miniatures or real-world objects. Then, to reduce the class imbalance problem, the generated point clouds are augmented into the current scene, balancing the number of occurrences across classes during training. As shown in Fig. 2, our pipeline consists of three main modules: (i) **Volumetric 3D Instance Collection**, where we reconstruct objects' 3D volumetric representation from multi-view images of real-world miniatures or objects (Section 3.1). (ii) **Object-level Domain Alignment**, where we reduce the domain gap (i.e., between the generated vs. the collected from LiDAR sensors) by transforming the point clouds based on sensor configurations and intensity simulation models (Section 3.2). Lastly, (iii) **Pseudo LiDAR Point Clouds Augmentation**, where we augment the generated pseudo-LiDAR

Figure 2: **Overview of Pseudo GT (PGT)-Aug Framework. Given multiview images, we first reconstruct their volumetric representations (Section 3.1). We post-process RGB point clouds using spatial rearrangement and LiDAR intensity simulator (Section 3.2), producing pseudo-LiDAR point clouds. Such points are stored in a psuedo LiDAR bank, and we paste the sampled objects into the target scene with the proposed augmentation scheme (Section 3.3).**

point clouds into the current scene by determining more realistic object insertion areas based on the estimated ground areas and map information (Section 3.3).

### Volumetric 3D Instance Collection of Minority Class Objects

**Data Collection.** Conventional Ground truth (GT) sampling-based data augmentation approaches resample minority-class 3D instances (e.g., trucks, trailers, buses, construction vehicles) from in-domain collections, but their placement flexibility and contextual diversity are often limited. To resolve this issue, we want to augment minority-class 3D instances from out-of-domain sources, efficiently resolving the data imbalance problem by sampling more diverse types and shapes of minority-class samples in the wild. To this end, we advocate for utilizing the following two cost-effective sources: (i) videos capturing the surround view of miniatures and (ii) publicly available videos capturing multiple views of minority-class objects (see Fig. 2). As the title suggests, we purchased dozens of realistic miniatures _for just $100_. Plus, we collect public videos from YouTube, followed by manual filtering to ensure those videos capture an object from multiple viewpoints.

**Preprocessing.** Given collected video frames, we first estimate the following three pieces of information as a preprocessing for the later 2D-to-3D rendering: (i) a camera intrinsic matrix, (ii) camera poses (3D camera position and its orientation for each frame), and (iii) binary masks for a foreground object. For (i) and (ii), we use COLMAP , similar to the preprocess of existing NeRF-based approaches . For (iii), we use the off-the-shelf video segmentation model, i.e., Segment and Track Anything , to extract segmentation masks for foreground objects.

**2D-to-3D Rendering.** Our framework is built on advanced 2D-to-3D renderers, such as Plenoxels  and Gaussian-Splitting , known for efficient, high-quality 3D reconstruction. Unlike some 3D conditional generative models for specific classes such as Shap-E  and Zero-1-to-3 , these methods offer dense 3D point clouds suitable for increasing out-of-distribution samples. In general, these methods  predict view-dependent representation, but obtaining fully visible and uniformly high-density data from all viewpoints is necessary to generate pseudo-LiDAR objects and axis-aligned bounding boxes. Therefore, we propose an ad-hoc module to obtain representative colors for each voxel grid or point based on the estimated mean color values from different views. More specifically, iterating through \(N\) views, it determines which voxel grid or point's camera ray passes through and calculates the color value of voxels or points by doing a dot product between corresponding spherical harmonic coefficients and view-dependent harmonics basis. If a certain voxel or a point is hit multiple times by rays from different views, we update the existing value by adding the new color value and store the total number of passes. Notably, this straightforward module is applicable not only to 2D-to-3D rendering techniques such as Plenoxels and Gaussian Splatting, but also to renderers that do not rely on spherical harmonics2.

### Object-level Domain Alignment

We aim to augment real LiDAR data with simulated minority-class samples derived from RGB-colored point clouds. However, these point clouds lack crucial information regarding spatial distribution from sensor configurations and LiDAR's intensity, which is essential for accurate LiDAR modeling and evaluation. Thus, we propose _object-level domain alignment_ between RGB-colored point clouds \(_{rgb}\) and real-world point clouds \(_{int}\), employing two alignment techniques as (i) view-dependent points filtering and rearrangement and (ii) LiDAR intensity simulation.

**View-dependent Points Filtering.** This step simulates realistic data variations based on the object's relative position to the LiDAR sensors and their settings. Therefore, we require alignment parameters based on factors such as type, position, quantity, and specifications (Field of View and azimuth resolution) of LiDAR sensors in the target dataset. We apply the following three steps: (i) **transformation** of points to the range view representation, (ii) **filtering** of points on the invisible side, and (iii) **reprojection** of points into 3D space.

In (i), given points in the spherical coordinates system, we map points into a range view \((u,v)^{H W}\), where \(u\) and \(v\) are the spatial grid indexes. For each grid \((u,v)\), a point with minimum depth remains, and the others are filtered out (i.e., points on visible parts remain). The remaining points are irregularly located in 3D space, generally different from those of real-world LiDAR points. We therefore rearrange each point to be regularly spaced by adjusting their inclination \(\) and azimuth \(\) in the spherical coordinate system: \(^{}=[1-()]_{}-_{},^{}=[2( )-1]\), where \(_{}\) and \(_{}\) represent the down part and the total range of the field of view, respectively. Rearranged inclination and azimuth \(^{}\), \(^{}\) are calculated by backprojecting image coordinates \(u,v\) to spherical coordinates following the above equations. The results on different LiDAR settings are shown in Fig. 3.

**LiDAR Intensity Simulation.** Since object surface reflectivity cannot be derived from images, we use a data-driven model for intensity estimation. As no real-world LiDAR points match generated RGB points, we create an unpaired domain transfer model specifically for point clouds based on CycleGAN  framework. Also, we design a novel region matching loss that directly reduces the intensity error between two samples, even with varying point counts, as shown in Fig. 4. To compute the loss, both generated points \(G_{_{rgb}_{int}}(_{rgb})\) and real-world points \(_{int}\) are grouped into an equal number of ball patches whose centers are obtained via farthest point sampling. After dividing into \(N\) ball patches, Hungarian matching is used to find the optimal assignment of ball patch pairs by computing the center distances for all matchings \(_{N}\),

\[=*{arg\,min}_{_{N}}_{i}^{N}|| c_{i}-c_{(i)}||_{1},\] (1)

where \(c_{i}\) and \(c_{(i)}\) are centers of ball patches from \(G_{_{rgb}_{int}}(_{rgb})\) and \(_{int}\), respectively.

After finding optimal pairs, we reduce all intensity distances between fake and real pairs of patches. For normalized points \(x,y\) and a given ball radius \(r\),

\[_{group}=_{j}^{N}||_{x b^{r}_{j}}(x)-_{ y b^{r}_{(j)}}(y)||_{1},\] (2)

where \(_{x b^{r}_{j}}(x)\), \(_{y b^{r}_{(j)}}(y)\) denote the average of intensity values of optimal ball patch pairs \(b^{r}_{j},b^{r}_{(j)}\) and \(r\) is 0.1. The overall objective function is

\[_{total}=_{}+_{},\] (3)

where \(\) is a regularizing factor set to 0.1.

**Instance Size Setting.** We analyzed the average object size per class in the dataset. We determine the size by adding Gaussian noise with \(\) of 0.1. The noise exceeding one \(\) range was clipped to \(\).

### Pseudo LiDAR Point Clouds Augmentation

**Ground and Map Synthesis for Object Insertion.** To determine feasible insertion areas, Real-Aug  and Lidar-Aug  utilized estimated ground estimation. However, relying solely on this data may not cover areas adequately as shown in Fig. 5. To address this, we propose a method that combines map information and estimated ground areas for more realistic scene composition. We first create a rasterized map with a 0.128m per pixel resolution within a radius of 51.2m around the ego vehicle, assigning proper layouts (e.g., Road, Sidewalk, etc.) for inserted objects. Additionally, we construct a rasterized ground map based on estimated ground points. The two pieces of information may overlap at pixels where dynamic object are present or where ground area is not estimated due to

Figure 4: **Region Matching Loss.**

Figure 3: **Effect of Reprojection on Different Datasets.**

insufficient points. To deal with it, we prioritize map values for pixels with low point density, while using ground values otherwise. As shown in Fig. 5-(b), the proposed method can predict broader and feasible areas for more realistic data augmentation than ground-only composition.

**Aligning to Data Geometry.** The generated objects need to be axis-aligned to easily obtain accurate bounding box annotations for the detection task. For this, we first use PCA to align the generated points and rotate them around the normal direction in \(xy\), \(yz\), and \(zx\) planes to align its axes. We also need to classify objects' front and back for heading information, so we adopt PointNet++  as a heading classifier trained using binary cross-entropy loss. Since PointNet++ is rotation sensitive  and is trained without rotation augmentation, the model can distinguish between front and back.

**Virtual Object Sweeps.** LiDAR scan sweeping is commonly used to increase the point density in some popular benchmarks [8; 9]. Since our pipeline can generate objects that appear from a single scan, additional techniques are required to apply them to such datasets. We employ a rigid body motion model to stack points over time. Given the dataset, we first collect the velocity and acceleration of each class's center points at each time step and estimate a motion trajectory. We translate the center of generated object points along the selected trajectory. Ultimately, we can generate virtual objects that closely resemble what is acquired from the real LiDAR sweeps.

## 4 Experiments

**Implementation Details.** We use Plenoxels  to reconstruct a given object's 3D shape from multi-view images. Also, we utilize PointNeXt  in our CycleGAN-based LiDAR intensity simulator. Note that we sample 300 instances (that has at least 256 LiDAR points) for each minority class from the nuScenes dataset  to train our intensity simulator. We implement and evaluate our proposed

Figure 5: **Comparison of Ground-only and Ground+Map Scene Composition.** Blue and Pink-colored points denote the feasible location of insertion derived from (a) ground-only and (b) ground+map synthesized insertions, respectively.

Figure 6: **Examples of generated pseudo-LiDAR point samples** with different orientations and ranges given reconstructed 3D volumetric representations.

method based on LiDAR-based 3D object detection implementations from OpenPCDet  with default parameter settings. All detectors are trained with a batch size of 32 on 4\(\)A100 GPUs for 20 epochs. We conducted all experiments with the fixed seed for a fair comparison. Details on architectures and hyperparameters can be found in the Supplementary Material.

**Pseudo Object Bank Details.** To validate our system, we have chosen nuScenes  dataset. For the pseudo object bank, we generated minority class objects in 13 heading directions (from \(-180^{}\) to \(180^{}\) at intervals of \(30^{}\)) within the entire perception range (from -50m to 50m at intervals of 5 meters). During training, we discard objects with less than 16 points to prevent ambiguity between classes and add a small variation at the center. As a result, our bank has 36,960 trucks, 52,800 construction vehicles, 12,960 buses, 19,280 trailers, 25,279 motorcycles, and 4,300 bicycles.

**Performance on NuScenes _val_ set.** We assess the effect of adding pseudo-LiDAR samples during training using CP-Voxel  with a voxel size of \([0.075,0.075,0.2]\). As shown in Table 1, the incorporation of pseudo-LiDAR samples into the GT database significantly improves the overall detection performance, including minority classes, in terms of mAP and NDS (compare 1st vs. 2nd, 3rd vs. 4th, and 5th vs. 6th columns). Our placement approach surpasses existing methods (compare 1st, 3rd vs. 5th columns). Combining our two methods achieves the best performance, with 69.1% in NDS and 63.5% in mAP, suggesting that our high-quality pseudo-LiDAR samples effectively improve detection performance for minority classes without compromising majority classes.

**Performance on NuScenes _test_ set.** Lastly, we evaluate our model on nuScenes test set based on CP-Voxel  and Transfusion-L  as baseline detection models. We observe in Table 2 that our model consistently achieves the best NDS and mAP scores (compared to Real-Aug ). Notably, our method shows significant performance gains for minor classes such as Trailer, Truck, Motorcycle, etc. Note that we applied the same Test Time Augmentation (TTA) technique to CP-Voxel as their official submission to nuScenes leaderboard.

    &  &  &  &  \\    & & Bus & C.V & Trailer & Truck & M.C & B.C & \\   ^{}\)} & GT-Aug & 64.4 & **31.0** & 60.0 & 47.2 & 65.7 & 41.0 & 63.8 & 68.7 \\  & Real-Aug\({}^{}\) & 64.5 & 29.0 & 60.1 & 57.3 & 72.2 & 47.1 & 65.8 & 71.3 \\  & PGT-Aug & **68.1** & 29.0 & **61.7** & **57.7** & **74.0** & **48.6** & **67.1** (41.39) & **72.3** (41.09) \\   & GT-Aug & 63.7 & 29.0 & 58.7 & 46.3 & 67.1 & **44.2** & 63.9 & 68.6 \\  & Real-Aug\({}^{}\) & 64.3 & **31.0** & 60.0 & 47.3 & 65.7 & 41.0 & 63.8 & 68.7 \\   & PGT-Aug & **67.3** & 30.1 & **60.2** & **56.9** & **68.2** & 40.6 & **65.1** (41.38) & **69.9** (41.29) \\   

Table 2: **Detection performance comparison on nuScenes _test_ set in terms of minority class AP, mAP for all 10 classes, and NDS**. \({}^{}\): our reproduction, \({}^{}\): test time augmentation enabled.

    &  &  &  &  \\    & & Bus & C.V & Trailer & Truck & M.C & B.C & \\   ^{}\)} & GT-Aug & 64.4 & **31.0** & 60.0 & 47.2 & 65.7 & 41.0 & 63.8 & 68.7 \\  & Real-Aug\({}^{}\) & 64.5 & 29.0 & 60.1 & 57.3 & 72.2 & 47.1 & 65.8 & 71.3 \\  & PGT-Aug & **68.1** & 29.0 & **61.7** & **57.7** & **74.0** & **48.6** & **67.1** (41.39) & **72.3** (41.09) \\   & GT-Aug & 63.7 & 29.0 & 58.7 & 46.3 & 67.1 & **44.2** & 63.9 & 68.6 \\  & Real-Aug\({}^{}\) & 64.3 & **31.0** & 60.0 & 47.3 & 65.7 & 41.0 & 63.8 & 68.7 \\   & PGT-Aug & **67.3** & 30.1 & **60.2** & **56.9** & **68.2** & 40.6 & **65.1** (41.38) & **69.9** (41.29) \\   

Table 1: **Detection performance comparison on nuScenes _val_ set in terms of AP, mAP, and NDS**. Based on CV-Voxel , we compare different placement methods, such as random , ground-based , and our placement. We also report the effect of using our pseudo-LiDAR samples.

**Comparison with Different Model Architectures.** We compared detection performance among GT-Aug, Real-Aug, and PGT-Aug using five baselines, as detailed in Table 3. All detectors were tested under same conditions, ensuring that no additional information about the generated objects was utilized during the evaluation process. PGT-Aug brings significant improvements over other augmentations in all types of models, such as center-based models (CP-Pillar, CP-Voxel ), anchor-based model (SECOND ), and other types (VoxelNeXt , Transfusion-L ). This confirms that our method can generally be applied to various detection models, boosting the model's accuracy for minority classes without sacrificing accuracy for majority classes.

## 5 Ablation Studies

**Quality of Pseudo Labels.** To evaluate the quality of our generated objects, we measure FID scores using SE(3)-transformer  trained on the nuScenes dataset. As shown in Table 4, our generated pseudo-LiDAR objects show similar or lower FID scores (than true samples), which may confirm their plausibility and high quality (compare 5th and 6th vs. 7th and 8th columns). We also observe that the pseudo-LiDAR object quality improves with (i) similar Azimuth resolution to the target dataset sensor, (ii) the use of RGB values additionally as input, and (iii) the regularization by group

    &  &  & A2D2  \\   &  &  & - & - \\  Azimuth Resolution (px) & \(3600\) & \(1080\) & \(1080\) & \(1080\) & \(1080\) & \(1080\) & - & - \\ RGB features & ✓ & ✗ & ✗ & ✓ & ✓ & ✓ & - & - \\ Group intensity Loss & ✓ & ✗ & ✓ & ✗ & ✓ & ✓ & - & - \\   Bus & 17.7 & 14.6 & 13.1 & **13.0** & 13.2 & **11.2** & 8.7 & 19.8 \\ Construction Vehicle & **7.0** & 7.5 & 7.6 & 7.5 & 7.6 & **7.6** & - & 6.0 \\ Trailer & 20.6 & 12.7 & 11.9 & **11.9** & 12.2 & **13.7** & - & 36.5 \\ Truck & 8.9 & 8.4 & 7.6 & 7.6 & **7.3** & **6.9** & 6.6 & 13.4 \\ Motorcycle & 20.7 & 2.5 & 7.0 & 7.2 & **3.7** & **1.3** & 3.0 & 10.1 \\ Bicycle & 9.0 & 3.3 & 2.2 & 2.4 & **2.1** & **1.8** & 1.8 & 0.7 \\  Avg. FID Score & 14.2 & 8.2 & 8.3 & 8.3 & **7.7** & **7.1** & 4.8 & 14.4 \\  mAP (_for all 10 classes_) & 63.40 & 63.44 & 63.48 & 63.41 & **63.52** & **63.77** & 63.45 & 63.17 \\ NDS (_for all 10 classes_) & 68.83 & 68.99 & 69.02 & 68.87 & **69.11** & **69.35** & 68.88 & 68.73 \\   

Table 4: **Quality of Pseudo LiDAR Point Clouds. FID scores (squared Wasserstein distance between given samples and nuScenes samples, thus lower is better) comparison between variants of our models and public LiDAR datasets, Lyft  and A2D2 . Abbr. G.S: Gaussian Splatting**

    & &  &  &  \\  } \\  Model & Aug. & Car & Ped & Barrier & T.C & Bus & C.V & Trailer & Truck & M.C & B.C & mAP & NDS \\    & GT-Aug & 81.5 & 77.3 & 58.3 & 57.9 & 67.2 & 15.2 & 36.2 & 50.6 & 40.7 & 16.2 & 50.11 & 61.56 \\  & Real-Aug\({}^{1}\) & 84.5 & **80.1** & 61.8 & **67.4** & **72.0** & 24.1 & **44.2** & 58.7 & 61.5 & 36.6 & 59.09 & 67.23 \\  & PGT-Aug & **84.8** & 80.0 & **62.2** & 67.2 & 71.9 & **24.4** & 42.4 & **58.8** & **64.0** & **38.1** & **59.37** & **67.30** \\   & GT-Aug & 83.1 & 82.5 & 65.0 & 65.8 & 63.0 & 14.1 & 23.7 & 54.4 & 51.4 & 25.5 & 52.85 & 62.57 \\  & Real-Aug\({}^{1}\) & **84.9** & **84.0** & 65.7 & **67.9** & 64.9 & 19.5 & 26.3 & **57.2** & 64.6 & 46.2 & 58.03 & 64.85 \\  & PGT-Aug & **83.3** & 83.5 & **65.7** & 67.4 & **66.0** & **22.1** & **27.5** & 56.3 & **65.0** & **47.0** & **58.39** & **65.49** \\   & GT-Aug & 84.9 & **85.4** & **68.3** & 69.9 & 69.1 & 18.0 & 36.1 & 56.9 & 60.3 & 41.5 & 59.04 & 66.54 \\  & Real-Aug\({}^{1}\) & 84.7 & 85.0 & 67.5 & 70.0 & 71.8 & 22.7 & **41.6** & 57.3 & 67.1 & 54.9 & 62.27 & 68.39 \\  & PGT-Aug & **85.4** & **85.4** & 68.0 & **71.1** & **72.1** & **24.2** & **40.4** & **59.8** & **70.3** & **58.3** & **63.25** & **69.11** \\   & GT-Aug & 86.6 & **86.6** & 69.4 & 73.6 & 72.7 & 23.3 & 43.9 & 53.3 & 69.4 & 56.1 & 63.48 & 68.58 \\  & Real-Aug\({}^{1}\) & 86.6 & **86.9** & **69.3** & 73.6 & 73.1 & 25.9 & 42.2 & **53.8** & 68.3 & 55.1 & 63.54 & 68.57 \\  & PGT-Aug & **87.0** & 86.4 & 69.4 & **74.0** & **73.4** & **26.7** & **46.9** & 50.6 & **71.3** & **56.3** & **64.20** & **68.83** \\   & GT-Aug & 83.7 & 84.5 & **68.9** & 68.4 & 71.4 & 20.9 & 37.5 & 56.1 & 62.9 & 49.8 & 60.42 & 67.03 \\  & Real-Aug\({}^{1}\) & 83.4 & 85.0 & 67.8 & 69.6 & 70.9 & 22.6 & 38.7 & **58.0** & 69.8 & **56.7** & 62.25 & 67.62 \\   & PGT-Aug & **84.2** & **85.0** & 67.7 & **70.6** &intensity loss. Finally, we used objects generated by Plenoxel with the parameters that gave the best FID score for all 3D object detection tasks. We provide more details in the Supplementary Material.

**Performance based on FID scores.** We conducted experiments to assess how the quality of generated objects as FID scores affects detection performance. We generated objects in all comparison groups to have the same shape distribution. As shown in the last three rows of columns 1 to 5 of Table 4, we observe a positive correlation between the average FID scores and the detection performance.

**Samples from Other Datasets.** The evaluation was conducted not only through self-validation but also using other public datasets such as Lyft  and A2D2 . Despite differences in class ontology and sensor settings from nuScenes, we can prove the effectiveness of our augmentation and confirm a correlation between the FID score and the number of samples in performance gain as shown in Table 4. See the Supplementary Material for matching classes across datasets.

**Samples from Different Renderer.** To demonstrate the baseline capability of the proposed pipeline, we applied the results generated by replacing the rendering model with Gaussian-splatting  instead of Plenoxels . Due to Gaussian-splatting  generates higher-quality objects, so the detection performance improves accordingly, as shown in Table 4.

**Mixing Ratio for Domain Alignments.** Even when several methods are applied to reduce the domain gap between the original and generated pseudo objects, there is still a difference between the two sample groups. Therefore, when out-of-distribution data are only used, we can see that the detection performance might drop. To alleviate this discrepancy, we used a strategy that combines the original object bank with the pseudo object bank to reduce the gap. We experimentally demonstrate that this strategy can mitigate the domain gap, allowing pseudo objects to be used effectively for detection tasks. It is especially effective when the mixing ratio is 1:1, which can be confirmed through Table 5.

**Performance based on Bank Size.** We discuss the importance of the size of the object bank in the detection task. To examine the impact of varying object shapes, we adjusted the size of the generated bank by decreasing the number of object shapes per class by 1/2 and 1/4. As shown in Table 6, we found that as the types of object increased, the performance improved in the validation set.

**Comparison between other approaches for class imbalance problems.** As shown in Table 7, we provide the results of comparative experiments with [34; 57]. To align with the baseline of Dynamic Weight Average (DWA) , we conducted experiments on PointPillars . We re-implemented Class-Balanced Loss (CBLoss)  with a beta value of 0.9999 and resampled the number of objects. While we observed that loss-balancing methods like DWA  and CBLoss  effectively improve the performance on minority classes, PGT-Aug, a data-augmentation-based method, achieved the largest performance improvement among all methods. Furthermore, our experiments demonstrate that combining these two types of methods can yield a synergistic effect.

    &  &  &  &  \\    & & Car & Ped & Barrier & T.C & Bus & C.V & Trailer & Truck & M.C & B.C & \\    & CBLoss\({}^{}\) & 82.7 & **74.7** & 54.0 & 52.1 & 51.2 & 61.7 & 17.9 & 30.5 & 48.7 & 20.5 & 49.4 \\  & DWA & 81.0 & 72.3 & 50.2 & 50.1 & 49.0 & 63.4 & 10.7 & 34.3 & 32.9 & 6.9 & 44.6 \\  & PGT-Aug & **83.0** & 71.8 & 54.8 & 51.1 & **54.9** & **69.7** & 20.2 & **39.5** & 49.6 & 14.5 & 50.9 \\  & PGT Aug + CBLoss & 82.7 & **74.7** & **56.5** & **55.9** & 54.4 & 68.7 & **20.9** & 34.1 & **53.5** & **20.5** & **52.2** (41-39) \\   

Table 6: **Continuously increasing pseudo-LiDAR data.**

   GT-PGT & 0:1 & 1:3 & 1:1 & 3:1 & 1:0 \\   mAP (\(\)) & 61.29 & 63.35 & **63.52** & 63.40 & 63.34 \\ NDS (\(\)) & 67.63 & 69.08 & **69.11** & 68.78 & 68.71 \\   

Table 5: **Mixing ratio between GT and PGT objects.**

**Performance on Other Datasets.** Lastly, we applied the proposed framework to KITTI  and Lyft  with different sensor configurations, ranges, and the number of sweeps. Both datasets did not provide map information, so we modified our scene composition similar to the ground-only composition by using Patchworks  to estimate ground. As shown in Table 8 and Table 9, we can see that our framework demonstrates performance improvement across all datasets and is applicable in various environments.

## 6 Conclusion, Limitations, and Broader Impacts

In this paper, we propose PGT-Aug, a low-cost yet effective data augmentation framework for class imbalance in 3D object detection. To efficiently obtain rare class objects, we start by generating objects from miniatures or web videos at a low cost, then transform them to resemble real LiDAR data, and finally apply map-assistant data augmentation to insert them consistently into the scene information. Even though PGT-Aug achieves significant improvements on various 3D object detection benchmarks, we believe that domain discrepancies still exist in both datasets and categories. Furthermore, our approach must be extended to dynamic and deformable objects, such as animals. While the generated pseudo objects have the potential to render both images and point clouds, our current implementation is limited to point clouds. Regarding broader impact, our work applies not only to objects for autonomous driving but also to secure facilities or military equipment in the same data format. Thus, it could be used for the development of AI models in military or security applications.

    &  &  &  &  \\    & &  &  &  &  \\   & &  & 290 & 262 & 56 & 2980 & 5082 & 3116 & 1139 & 605 & 434 &  \\   & Difficulty & Easy & Mod. & Hard & Easy & Mod. & Hard & Easy & Mod. & Hard & \\   SECOND  & GT-Aug & 62.3 & 55.3 & 49.8 & **91.4** & **82.4** & **79.6** & 87.1 & 67.9 & 63.8 & 68.5 \\  & PGT-Aug & **63.3** & **56.2** & **50.2** & 90.7 & 82.1 & 79.3 & **90.3** & **72.1** & **67.7** & **70.1** (41.65) \\   

Table 8: **PGT Performance on KITTI _val_ set in terms of AP and mAP.**

    &  &  &  &  \\   & & Truck & Bus & O.V & M.C & B.C & Car & Ped. & \\   & & \# of objects in _val_ & 2,721 & 1,653 & 4,920 & 187 & 3,347 & 91529 & 4952 & \\    & GT-Aug & 19.15 & 20.48 & 31.91 & **4.54** & 5.31 & **37.14** & 6.00 & 13.84 \\  & PGT-Aug & **19.85** & **21.11** & **31.99** & 4.39 & **5.48** & 37.11 & **6.12** & **14.01** (40.175) \\   

Table 9: **PGT Performance on Lyft _val_ set. _Abbr._ E.V: Emergency Vehicle, O.V: Other Vehicle, M.C: Motorcycle, B.C: Bicycle. Ped: Pedestrian