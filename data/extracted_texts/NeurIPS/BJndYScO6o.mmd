# Model-Based Diffusion for Trajectory Optimization

Chaoyi Pan\({}^{*}\), Zeji Yi\({}^{*}\), Guanya Shi\({}^{}\), Guannan Qu\({}^{}\)

Carnegie Mellon University

{chaoyip,zejiy,guanyas,gqu}@andrew.cmu.edu

###### Abstract

Recent advances in diffusion models have demonstrated their strong capabilities in generating high-fidelity samples from complex distributions through an iterative refinement process. Despite the empirical success of diffusion models in motion planning and control, the model-free nature of these methods does not leverage readily available model information and limits their generalization to new scenarios beyond the training data (e.g., new robots with different dynamics). In this work, we introduce Model-Based Diffusion (MBD), an optimization approach using the diffusion process to solve trajectory optimization (TO) problems _without data_. The key idea is to explicitly compute the score function by leveraging the model information in TO problems, which is why we refer to our approach as _model-based_ diffusion. Moreover, although MBD does not require external data, it can be naturally integrated with data of diverse qualities to steer the diffusion process. We also reveal that MBD has interesting connections to sampling-based optimization. Empirical evaluations show that MBD outperforms state-of-the-art reinforcement learning and sampling-based TO methods in challenging contact-rich tasks. Additionally, MBD's ability to integrate with data enhances its versatility and practical applicability, even with imperfect and infeasible data (e.g., partial-state demonstrations for high-dimensional humanoids), beyond the scope of standard diffusion models. Videos and codes: https://lecar-lab.github.io/mbd/

## 1 Introduction

Trajectory optimization (TO) aims to optimize the state and control sequence to minimize a cost function while subject to specified dynamics and constraints. Given non-linear, non-smooth dynamics and non-convex objectives and constraints, traditional optimization methods like gradient-based methods and interior point methods are less effective in solving TO problems. In response, diffusion models have emerged as a powerful tool for trajectory generation in complex dynamical systems due to their expressiveness and scalability [12; 54; 34; 33; 40; 5].

Figure 1: MBD refines the trajectory by leveraging the dynamics model directly without relying on demonstration data.

Although diffusion models excel when learning from large-scale, high-dimensional, and high-quality demonstrations, their dependency on such data limits their practicality. For example, after training a manipulation task with a specific robotic arm, the model may struggle to generalize to new tasks with a different arm as the underlying dynamics change. This limitation arises from the model-free nature of existing diffusion-based methods, which do not leverage readily available model information to enhance adaptability. Moreover, existing diffusion-based approaches often require high-quality (in terms of optimality and feasibility) demonstration data, which limits their applications in various scenarios with imperfect data, such as dynamically infeasible trajectories (e.g., generated by high-level planners using simplified models) and partial demonstrations (e.g., lower-body-only demonstrations for a high-dimensional humanoid).

Fortunately, unlike diffusion model's applications in vision or language where data is from unknown distributions (e.g., internet-scale image data), in trajectory optimization, we often know the distribution of desired trajectories, which is described by the optimization objectives, constraints, and the underlying dynamics model, although such a distribution is intractable to directly sample from. Diffusion models offer a tantalizing new perspective, by iteratively refining samples from isotropic Gaussians to meaningful desired distributions in manageable steps, rather than directly learning the complex desired distribution. Inspired by this, we propose Model-Based Diffusion (MBD) that utilizes model information to approximate the gradient of the log probability density function (a.k.a. score function) and uses it to iteratively refine sampled trajectories to solve TO problems, as depicted in Fig. 1. This model-centric strategy allows for the generation of dynamically feasible trajectories in a data-free manner, and gradually moves them towards more optimal solutions. Furthermore, by using demonstrations as observations of the target distribution, MBD can be smoothly combined with data of different qualities to steer the diffusion process and enhance its effectiveness. Particularly, we merge the demonstration data into the sampling process by evaluating their likelihoods with the model and use them to improve the estimation of the score function. Our contributions are threefold:

* We introduce the Model-Based Diffusion (MBD) framework for trajectory optimization, utilizing the dynamics model to estimate the score function. This enables an effective trajectory planner given non-smooth dynamics and non-convex objectives, such as contact-rich manipulation tasks or high-dimensional humanoids.
* Our analysis and empirical evaluations demonstrate that MBD matches, and often exceeds, the performance of existing reinforcement learning and sampling-based TO methods. In particular, MBD outperforms PPO by 59% in various tasks within tens of seconds of diffusing.
* We demonstrate MBD's flexibility in utilizing diverse imperfect data to steer the diffusion process and further enhance the performance. Specifically, the resulting whole-body humanoid trajectory from MBD is more natural by utilizing the lower-body-state-only human motion data. Similarly, MBD can effectively address long-horizon sparse-reward Umaze navigation tasks by leveraging infeasible demonstrations generated by an RRT planner with simplified dynamics.

## 2 Related Work

**Diffusion Models.** Diffusion models have been widely adopted as generative models for high-dimensional data, such as image , audio , and text  through iterative refinement processes [50; 28]. The backward process can be viewed as gradient prediction  or score matching , which learns the score function to move samples towards the data distribution. We deliver new methods to perform the backward diffusion process using the available model information.

**Sampling-based Optimization.** Optimization involving black-box functions is widely applied across various fields, including hyperparameter tuning and experimental design [49; 27]. Evolutionary algorithms like CMA-ES are often used to tackle black-box optimization problems, dynamically modifying the covariance matrix to produce new samples . Such problems can also be efficiently addressed within the Bayesian optimization framework [50; 19], which offers greater efficiency. Nonetheless, traditional BO algorithms are generally restricted to low-dimensional problems.

**Trajectory Optimization.** Traditionally, trajectory optimization (TO) is solved using gradient-based optimization, which faces challenges such as non-convex problem structures, nonlinear or discontinuous dynamics, and high-dimensional state and control action spaces. As two equivalent formulations, direct methods  and shooting-based methods  are commonly used to solve TOproblems, where gradient-based optimizers such as Augmented Lagrangian , Interior Point , and Sequential Quadratic Programming [3; 48] are employed. To leverage the parallelism of modern hardware and improve global convergence properties, sampling-based methods like Cross-Entropy Motion Planning (CEM)  and Model Predictive Path Integral (MPPI) [58; 62] have been proposed to solve TO by sampling from target distributions. To solve stochastic optimal control problems, trajectory optimization has also been framed as an inference problem in a probabilistic graphical model, where system dynamics defines the graph structure [35; 39]. This perspective extends methods such as iLQG by integrating approximate inference techniques to improve trajectory optimization . The connection between diffusion and optimal control has been explored in , which motivates us to use diffusion models as solvers for trajectory optimization.

**Diffusion for Planning.** Diffusion-based planners have been used to perform human motion generation [12; 54] and multi-agent motion prediction . Diffusion models are capable of generating complete trajectories by folding both dynamics and optimization processes into a single framework, thus mitigating compounding errors and allowing flexible conditioning [33; 40; 5]. In addition, they have been adeptly applied to policy generation, enhancing the capability to capture multimodal demonstration data in high-dimensional spaces for long-horizon tasks [46; 15]. These works assume no access to the underlying dynamics, limiting the generalization to new environments. To enforce dynamics constraints, SafeDiffuser  integrates control barrier functions into the learned diffusion process, while Diffusion-CCSP  composes the learned geometric and physical conditions to guarantee constraint compliance. Our approach uses diffusion models directly as solvers, rather than simply distilling solutions from demonstrations.

**Langevin-based Markov Chain Monte Carlo for Global Optimization.** Gradient-based sampling algorithms have been widely used in global optimization, where the energy function \(J\) is optimized by sampling from the Boltzmann distribution \(p(-)\)[57; 43]. By annealing the temperature \(\) to zero, the sampling process converges to the global minimum of the energy function \(J\). The convergence of Langevin-based MCMC methods has been well studied in both continuous and discretized settings [16; 21], showing that the distribution will converge in probability to the target distribution with certain decreasing schedule of the step size and temperature \(\). In practice, the most common Langevin-based MCMC methods are unadjusted Langevin Monte Carlo (ULMC)  and Underdamped Langevin Monte Carlo (UdLMC) , with convergence rates of \(O(}())\) and \(O()\) given strongly convex and smooth energy functions, respectively. Recently, Langevin-based MCMC methods have been integrated into diffusion processes to improve global convergence and sampling efficiency [30; 6], where the score function is estimated by Monte Carlo to accelerate and stabilize the diffusion process. Our work differs from these methods in that we aim to sample from the high-probability region of the target distribution without assuming access to the gradient of the energy function, and without assuming the energy function is smooth or convex.

## 3 Problem Statement and Background

**Notations**: We use lower (upper) scripts to specify the time (diffusion) step, e.g., \(x_{t},u_{t},y_{t}\) represent the state, control and state-control pair at time \(t\), and \(Y^{(i)}\) represents the diffusion state at step \(i\).

This paper focuses on a class of trajectory optimization (TO) problems whose objective is to find the sequences \(\{x_{t}\}\) and \(\{u_{t}\}\) that minimize the cost function \(J(x_{1:T};u_{1:T})\) subject to the dynamics and constraints. The optimization problem 1 can be formulated as follows:

\[_{x_{1:T},u_{1:T}} J(x_{1:T};u_{1:T})=l_{T}(x_{T})+_{t=0}^{T-1}l_{t}(x_{t},u_{t})\] (1a) s.t. \[x_{0}=x_{}\] (1b) \[x_{t+1}=f_{t}(x_{t},u_{t}), t=0,1,,T-1,\] (1c) \[g_{t}(x_{t},u_{t}) 0, t=0,1,,T-1.3\] (1d)

where \(x_{t}^{n_{x}}\) and \(u_{t}^{n_{u}}\) are the state and control at time \(t\), \(f_{t}:^{n_{x}}^{n_{u}}^{n_{x}}\) represents the dynamics, \(g_{t}:^{n_{x}}^{n_{u}}^{n_{g}}\) are the constraint functions and \(l_{t}:^{n_{x}}^{n_{u}}\) are the stage costs. We use \(Y=[x_{1:T};u_{1:T}]\) to denote all decision variables. Traditionally, TO is solved using nonlinear programming, which faces challenges such as non-convex problem structures, nonlinear or discontinuous dynamics, and high-dimensional state and control action spaces. Recently, there has been a growing interest in bypassing these challenges by directly generating samples from the optimal trajectory distribution using diffusion models trained on optimal demonstration data [12; 40; 46; 61].

To use diffusion for TO, (1) is first transformed into a sampling problem. The target distribution \(p_{0}(Y^{(0)})\) is proportional to dynamical feasibility \(p_{d}(Y)_{t=1}^{T}(x_{t}=f_{t-1}(x_{t-1},u_{t-1}))\), optimality \(p_{J}(Y))}\) and the constraints \(p_{g}(Y)_{t=1}^{T}(g_{t}(x_{t},u_{t}) 0)\), i.e.,

\[p_{0}(Y) p_{d}(Y)p_{J}(Y)p_{g}(Y)\] (2)

Obtaining the solution \(Y^{*}\) from the TO problem in Eq. (1) is equivalent to sampling from Eq. (2) given a low temperature \( 0\). In fact, in Appendix A.2, we prove that the distribution of \(J(Y)\) with \(Y p_{0}()\) converges in probability to the optimal value \(J^{*}\) as \( 0\), under mild assumptions. However, it is generally difficult to directly sample from the high-dimensional and sparse target distribution \(p_{0}()\). To address this issue, the diffusion process iteratively refines the samples following a backward process, which reverses a predefined forward process as shown in Fig. 1. The forward process corrupts the original distribution \(p_{0}()\) to an isotropic Gaussian \(p_{N}()\) by incrementally adding small noise to it and scaling it down by \(}\) to maintain an invariant noise covariance (see Fig. 2(b) for an example). Mathematically, this means we iteratively obtain \(Y^{(i)} p_{i}()\) with \(p_{i|i-1}(|Y^{(i-1)})(}Y^{(i-1)},(1- _{i})I)\). Because the noise at each time step is independent, the conditional distribution of \(Y^{(i)}|Y^{(i-1)}\) also leads to that of \(Y^{(i)}|Y^{(0)}\):

\[p_{i|0}(|Y^{(0)})(_{i}}Y^{(0)},(1-_{i})I),_{i}=_{k=1}^{i}_{k}.\] (3)

The backward process \(p_{i-1|i}(Y^{(i-1)}|Y^{(i)})\) is the reverse of the forward process \(p_{i|i-1}(Y^{(i)}|Y^{(i-1)})\), which removes the noise from the corrupted distribution \(p_{N}()\) to obtain the target distribution \(p_{0}()\). The target distribution \(p_{0}()\) in the diffusion process is given by:

\[p_{i-1}(Y^{(i-1)}) = p_{i-1|i}(Y^{(i-1)}|Y^{(i)})p_{i}(Y^{(i)})\,dY^{(i)},\] (4) \[p_{0}(Y^{(0)}) = p_{N}(Y^{(N)})_{i=N}^{1}p_{i-1|i}(Y^{(i-1)}|Y^{(i)})dY ^{(1:N)}\] (5)

Standard diffusion models [33; 40; 61], which we refer to as Model-Free Diffusion (MFD), solve the backward process by learning score function merely from data. In contrast, we propose leveraging the dynamics model to estimate the score to improve the generalizability of the model and allow a natural integration with diverse quality data.

## 4 Model-Based Diffusion

In this section, we formally introduce our MBD algorithm that leverages model information to perform backward process. To streamline the discussion, in Section 4.1, we first present MBD with Monte Carlo score ascent to solve simplified and generic unconstrained optimization problems. In Section 4.2, we extend MBD to the constrained optimization setting to solve the TO problem given complex dynamics and constraints. Lastly, in Section 4.3, we augment the MBD algorithm with demonstrations to improve sample quality and steer the diffusion process.

### Model-based Diffusion as Multi-stage Optimization

We consider the reverse process for an unconstrained optimization problem \(_{Y}J(Y)\), where the target distribution is \(p_{0}(Y^{(0)})(-)}{})\). In our MBD framework, "model" implies that we can evaluate \(J(Y^{(0)})\) for arbitrary \(Y^{(0)}\), enabling us to compute the target distribution up to the normalizing constant.

MBD uses Monte Carlo score ascent instead of the commonly adopted reverse SDE approach in MFD. Specifically, when denoising from \(i\) to \(i-1\), MBD performs one step of gradient ascent on \( p_{i}(Y^{(i)})\) and then scales the sample by the factor \(}}\) as defined in the forward process:

\[Y^{(i-1)}=}}(Y^{(i)}+(1-_{i} )_{Y^{(i)}} p_{i}(Y^{(i)}))\] (6)

Critically, with the model-informed \(p_{0}(Y^{(0)})\), we can estimate the score function \(_{Y^{(i)}} p_{i}(Y^{(i)})\) by connecting \(p_{i}(Y^{(i)})\) to \(p_{0}(Y^{(0)})\) via Bayes' rule:

\[_{Y^{(i)}} p_{i}(Y^{(i)})= } p_{i0}(Y^{(i)} Y^{(0)})p_{0}(Y^{(0 )})\,dY^{(0)}}{ p_{i0}(Y^{(i)} Y^{(0)})p_{0}(Y^{(0)})\,dY^{(0)}}= {_{Y^{(i)}}p_{i0}(Y^{(i)} Y^{(0)})p_{0}(Y^{(0)})\,dY^{(0)}}{  p_{i0}(Y^{(i)} Y^{(0)})p_{0}(Y^{(0)})\,dY^{(0)}}\] (7a) \[= -}Y^{(0)}}{1- _{i}}p_{i0}(Y^{(i)} Y^{(0)})p_{0}(Y^{(0)})\,dY^{(0)}}{ p_{i0}(Y^{(i)}  Y^{(0)})p_{0}(Y^{(0)})\,dY^{(0)}}\] (7b) \[= -}{1-_{i}}+_{i} }}{1-_{i}}\,p_{i0}(Y^{(i)} Y^{(0)})p_{0}(Y^{(0)} )\,dY^{(0)}}{ p_{i0}(Y^{(i)} Y^{(0)})p_{0}(Y^{(0)})\,dY^{(0)}}\] (7c)

Between Eq. (7a) and Eq. (7b), we use the forward Gaussian density in Eq. (3): \(p_{i|0}(Y^{(i)} Y^{(0)})(-- {_{i}}Y^{(0)})^{}(Y^{(i)}-_{i}} Y^{(0)})}{1-_{i}})\). Its log-likelihood gradient is \(_{Y^{(i)}}p_{i|0}(Y^{(i)} Y^{(0)})=-_{i}}(Y^{( i)}-_{i}}Y^{(0)})p_{i|0}(Y^{(i)} Y^{(0)})\). Given \(Y^{(0)}\) as the integration variable in Eq. (7c), \(p_{i|0}(Y^{(i)} Y^{(0)})\) is evaluated as a function of \(Y^{(0)}\) parameterized by \(Y^{(i)}\). Based on that, we define the function \(_{i}(Y^{(0)})\) as:

\[_{i}(Y^{(0)}) p_{i|0}(Y^{(i)} Y^{(0)})(- -}{_{i}}})^{} (Y^{(0)}-}{_{i}}})}{_{i}}{_{i}}})(}{_{i}}},_{i}}-I)\] (8)

Figure 2: Reverse SDE vs. Monte Carlo score ascent (MCSA) on a synthetic highly non-convex objective function. (a) Synthesized objective function with multiple local minima. (b) The intermediate stage density \(p_{i}()\), where peaked \(p_{0}()\) is iteratively corrupted to a Gaussian \(p_{N}()\). (c) Reverse SDE vs. MCSA: Background colors represent the density of \(p_{i}()\) at different stages. MCSA converges faster due to larger step size and lower sampling noise while still capturing the multimodality.

This finding enables the Monte-Carlo estimation for computing the score function. We collect a batch of samples from \(_{i}()\) which we denote as \(^{(i)}\) and approximate the score as:

\[_{Y^{(i)}} p_{i}(Y^{(i)})=-}{1-_{ i}}+_{i}}}{1-_{i}}_{i}(Y^ {(0)})p_{0}(Y^{(0)})\,dY^{(0)}}{_{i}(Y^{(0)})p_{0}(Y^{(0)})\,dY^{(0)}}\] (9a) \[-}{1-_{i}}+_{i}}}{1-_{i}} ^{(i)}}Y^{(0)}p_{0}(Y^{(0)})}{_{Y^{(0)}^{(i) }}p_{0}(Y^{(0)})}}_{}:=-}{1-_{i}}+ _{i}}}{1-_{i}}^{(0)}(^ {(i)})\] (9b)

**Comparison between MFD and MBD.** Table 1 highlights the key differences between MBD and MFD, which originate from two assumptions made in MBD: (a) a known target distribution \(p_{0}(Y^{(0)})\) given the model; (b) the objective of sampling \(Y^{(0)}\) from the high-likelihood region of \(p_{0}(Y^{(0)})\) to minimize the cost function. For (a), MBD leverages \(p_{0}\) to estimate the score following Eq. (9a), whereas MFD learns that from the data. For (b), MBD runs Monte Carlo score ascent in Eq. (6) to quickly move the samples to the high-density region as depicted in Fig. 2(c), while MFD runs reverse SDE \(Y^{(i-1)}=}}(Y^{(i)}+}{2} _{Y^{(i)}} p_{i}(Y^{(i)}))+}_{i}\), where \(_{i}\) is Gaussian noise, to maintain the sample diversity. Given low temperature \(\), it can be shown that \( p_{i}(Y^{(i)})-_{i})}(Y^{(i)}-  p_{i}())\)2, i.e., the function \( p_{i}(Y^{(i)})\) is \(_{i})}\)-smooth. Therefore, choosing the step size \((1-_{i})\) in Eq. (6) is considered optimal, as for \(L\)-smooth functions, \(O()\) is the step size that achieves the fastest convergence .

**How diffusion helps?** The diffusion process plays an important role in helping Monte Carlo score ascent overcome the local minimum issue in highly non-convex optimization problems, as shown in Fig. 2(a). Compared with optimizing a highly non-convex objective, Monte Carlo score ascent is applied to the intermediate distribution \(p_{i}()= p_{0}(Y^{(0)})p_{i0}()dY^{(0)}\), which is made concave by convoluting \(p_{0}()\) with a Gaussian distribution \(p_{i|0}()\), as shown in Fig. 2(b). Starting from the strongly concave Gaussian distribution \(p_{N}(,I)\) with scale \(_{N} 0\), the density is easy to sample. The covariance of the sampling density \(_{_{i}}=(_{i}}-1)I\) is large when \(i=N\), implying that we are searching a wide space for global minima. In the less-noised stage, the intermediate distribution \(p_{i}()\) is more peaked and closer to the target distribution \(p_{0}()\), and \(_{i} 1\) produces a smaller sampling covariance \(_{_{i}}\) to perform a local search. By iteratively running gradient ascent on the smoothed distribution, MBD can effectively optimize a highly non-convex objective function as presented in Fig. 2. The MBD algorithm is formally depicted in Algorithm 1.

**Connection with Sampling-based Optimization.** When diffusion step is set to \(N=1\), MBD effectively reduces to the Cross-Entropy Method (CEM)  for optimization. To see this, we can plug the estimated score Eq. (9b) into the Monte Carlo score ascent Eq. (6) and set \(N=1\): \(Y^{(0)}=_{1}}{_{1}}^{(0)}(^{(1)})= ^{(0)}(^{(1)})=^{(1)} }Y^{(0)}w(Y^{(0)})}{_{Y^{(0)}^{(1)}}w(Y^{(0)})}\) where \(w(Y^{(0)})=p_{0}(Y^{(0)})(-)}{})\) and \(^{(1)}(}{_{0}},(}-1)I)\). This precisely mirrors the update mechanism in CEM, which aims to optimize the objective function \(f_{}(Y^{(0)})=J(Y^{(0)})\) and determine the sampling covariance \(_{}=(}-1)I\), thus linking the sampling strategy of CEM with the \(\) schedule in MBD. The advances that distinguish MBD from CEM-like methods are (1) the careful

 p{113.8pt} p{113.8pt}}  
**Aspect** & **Model-Based Diffusion (MBD)** & **Model-Free Diffusion (MFD)** \\  Target Distribution & Known (Eq. (2)), but hard to sample & Unknown, but have data \\ Objective & Sample \(Y^{(0)}\) from high-likelihood region of \(p_{0}()\) & Sample \(Y^{(0)} p_{0}()\) \\ Score Approximation & Estimated using the model (Eq. (9a)). Can be augmented with demonstrations (Eqs. (11) and (13)) & Learned from data \\ Backward Process & Perform Monte Carlo score ascent (Eq. (6)) to move samples towards most-likely states & Run reverse SDE to preserve sample diversity \\   

Table 1: Comparison of Model-Based Diffusion (MBD) and Model-Free Diffusion (MFD)scheduling of \(\) and (2) the intermediate refinements on \(p_{i}\), both following the forward diffusion process. This allows MBD to optimize for smoothed functions in the early stage and gradually refine the solution for the original objective. On the contrary, CEM's solution could either be biased given a large \(_{}\) which overly smoothes the distribution as in \(p_{20},p_{100}\) of Fig. 2(b), or stuck in local minima with a small \(_{}\) as in \(p_{1}\) of Fig. 2(b) where the distribution is highly non-concave.

### Model-based Diffusion for Trajectory Optimization

For TO, we have to accommodate the constraints in Eq. (1) which change the target distribution to \(p_{0}(Y^{(0)}) p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})\). Given that \(p_{d}(Y^{(0)})\) is a Dirac delta function that assigns non-zero probability only to dynamically feasible trajectories, sampling from \(_{i}(Y^{(0)})\) could result in low densities. To enhance sampling efficiency, we collect a batch of dynamically feasible samples \(_{d}^{(i)}\) from the distribution \(_{i}(Y^{(0)})p_{d}(Y^{(0)})\) with model information. Proceeding from Eq. (9a), and incorporating \(p_{0}(Y^{(0)}) p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})\), we show the score function is:

\[_{Y^{(i)}} p_{i}(Y^{(i)})= -}{1-_{i}}+_{i}} }{1-_{i}}_{i}(Y^{(0)})p_{d}(Y^{(0)})p_{g}(Y^ {(0)})p_{J}(Y^{(0)})\,dY^{(0)}}{_{i}(Y^{(0)})p_{d}(Y^{(0)})p_{g}(Y^{(0 )})p_{J}(Y^{(0)})\,dY^{(0)}}\] (10a) \[= -}{1-_{i}}+_{i} }}{1-_{i}}_{d}^{(i)}}Y^{(0)}p_{J }(Y^{(0)})p_{g}(Y^{(0)})}{_{Y^{(0)}_{d}^{(i)}}p_{J}(Y^{(0)} )p_{g}(Y^{(0)})}\] (10b) \[= -}{1-_{i}}+_{i} }}{1-_{i}}^{(0)},\] (10c) \[^{(0)}= _{d}^{(i)}}Y^{(0)}w(Y^{(0)})}{ _{Y^{(0)}_{d}^{(i)}}w(Y^{(0)})}, w(Y^{(0)})=p_{J}(Y^{(0 )})p_{g}(Y^{(0)})\] (10d)

The model plays a crucial role in score estimation by transforming infeasible samples \(^{(i)}\) from Line 3 in Algorithm 2 into feasible ones \(_{d}^{(i)}\). The conversion is achieved by putting the control part \(U=u_{1:T}\) of \(Y^{(0)}=[x_{1:T};u_{1:T}]\) into the dynamics Eq. (1c) recursively to get the dynamically feasible samples \(Y_{d}^{(0)}\) (Line 4), which shares the same idea with the shooting method  in TO. MBD then evaluates the weight of each sample with \(p_{g}(Y^{(0)})p_{J}(Y^{(0)})\) in Line 5. One common limitation of shooting methods is that they could be inefficient for long-horizon tasks due to the combinatorial explosion of the constrained space \(p_{g}(Y)_{t=1}^{T}(g_{t}(x_{t},u_{t}) 0)\), leading to low constraint satisfaction rates. To address this issue, we will introduce demonstration-augmented MBD in Section 4.3 to guide the sampling process from the state space to improve sample quality.

### Model-based Diffusion with Demonstration

With the ability to leverage model information, MBD can also be seamlessly integrated with various types of data, including imperfect or partial-state demonstrations by modeling them as noisy observations of the desired trajectory \(p(Y_{} Y^{(0)})(Y^{(0)},^{2}I)\). Given suboptimal demonstrations, sampling from the posterior \(p(Y^{(0)} Y_{}) p_{0}(Y^{(0)})p(Y_{} Y^{ (0)})\) could lead to poor solutions as the demonstration likelihood \(p(Y_{} Y^{(0)})\) could dominate the model-based distribution \(p_{0}(Y^{(0)}) p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})\) and mislead the sampling pro cess. Rather, we assess \(Y^{(0)}\) using \(p(Y_{} Y^{(0)})\), employing a similar technique to interchange the distribution's parameter with the random variable, as demonstrated in Eq. (8), to establish \(p_{}(Y^{(0)}) p(Y_{} Y^{(0)})(Y^{(0)} Y_{},^{2}I)\).

To accommodate demonstrations of varying qualities, instead of fixing target to \(p_{0}(Y^{(0)})p(Y_{} Y^{(0)})\), we propose seperating the \(p_{0}(Y^{(0)})\) from \(p_{}(Y^{(0)})\) to form a new target distribution3:

\[p_{0}^{}(Y^{(0)})(1-)p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0) })+ p_{}(Y^{(0)})p_{J}(Y_{})p_{g}(Y_{})\] (11)

where \(\) is a constant to balance the model and the demonstration. Here, we have introduced two extra constant terms \(p_{J}(Y_{})\) and \(p_{g}(Y_{})\) to ensure that the demonstration likelihood is properly scaled to match the model likelihood \(p_{0}(Y^{(0)})\). With these preparations, we propose to adaptively determine the significance of the demonstration by choosing \(\) as follows:

\[=1&p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)})<p_{ }(Y^{(0)})p_{J}(Y_{})p_{g}(Y_{})\\ 0&p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)}) p_{}(Y^{(0)})p_{J}( Y_{})p_{g}(Y_{}).\] (12)

When samples have a high model-likelihood \(p_{0}\), we ignore the demonstration and sample from the model. Otherwise, we trust the demonstration. With the demonstration-augmented target distribution, we modify the way to calculate \(^{(0)}\) in Eq. (10d) as follows to obtain the score estimate:

\[^{(0)}=_{d}^{(i)}}Y^{(0)}w(Y^{(0)})}{ _{Y^{(0)}_{d}^{(i)}}w(Y^{(0)})}, w(Y^{(0)})=\{ p_{d}(Y^{(0)})p_{J}(Y^{(0)})p_{g}(Y^{(0)}),\\ p_{}(Y^{(0)})p_{J}(Y_{})p_{g}(Y_{}) \}.\] (13)

## 5 Experimental Results

The experimental section will focus on demonstrating the capabilities of MBD in: (1) its effectiveness as a zeroth-order solver for high-dimensional, non-convex, and non-smooth trajectory optimization problems, and (2) its flexibility in utilizing dynamically infeasible data to enhance performance and regularize solutions. Our benchmark shows that MBD outperforms PPO by \(59\%\) in various control tasks with \(10\%\) computational time.

Beyond control problems, in Appendix A.3, we also show that MBD significantly improves sampling efficiency by an average of \(23\%\) over leading baselines in high-dimensional (up to 800d) black-box optimization testbeds [23; 18; 56; 42; 41; 44]. We also apply MBD to optimize an MLP network with 28K parameters in a _gradient-free_ manner, achieving 86% accuracy within 2s for the MNIST classification task , which is comparable to the gradient-based optimizer (SGD with momentum, 93% accuracy). To further extend MBD to closed-loop control, we employ receding horizon strategy to MBD in Appendix A.6 to update control sequence at each timestep, further improving the performance of MBD by \(9.6\%\) in terms of reward.

### MBD for Planning in Contact-rich Tasks

To test the effectiveness of MBD as a trajectory optimizer for systems involving non-smooth dynamics, we run MBD on both locomotion and manipulation tasks detailed in Appendix A.5.1. The locomotion 

[MISSING_PAGE_FAIL:9]

### Data-augmented MBD for Trajectory Optimization

We also evaluate the performance of MBD with data augmentation on the Car UMaze Navigation and Humanoid Jogging tasks to see how partial and dynamically infeasible data can help the exploration of MBD and regularize the solution by steering the diffusion process.

For Car UMaze Navigation, the map blocked by U-shaped obstacles is challenging to explore given a nonlinear dynamics model. Therefore, random shooting has a low chance of reaching the goal region. To sample with loosened dynamical constraints, we augment MBD with data from the RRT  algorithm through goal-directed exploration with simplified dynamics. Fig. 4(b) shows the difference between data-augmented MBD and data-free one: the former can refine the infeasible trajectory and further improve it to reach the goal in less time, while the latter struggles to find a feasible solution. The reason is that the infeasible trajectory from RRT serves as a good initialization for MBD, which can be further refined to minimize the cost function with MBD.

For Humanoid Jogging, we aim to regularize the solution for the task with multiple solutions to the desired one with partial state data. Due to the infinite possibilities for humanoid jogging motion, the human motion data provide a good reference to regularize MBD to converge to a more human-like and robust solution instead of an aggressive or unstable one [26; 45]. We use data from the CMU Mocap dataset , from which we extract torso, thigh, and shin positions and use them as a partial state reference. Fig. 4(a) demonstrates a more stable motion generated by data-augmented MBD.

## 6 Conclusion and Future Work

This paper introduces Model-Based Diffusion (MBD), a novel diffusion-based trajectory optimization framework that employs a dynamics model to approximate the score function. MBD not only outperforms existing methods in terms of sample efficiency and generalization, but also provides a new perspective on trajectory optimization by leveraging diffusion models as powerful samplers. Future directions involve theoretically understanding its convergence, optimizing the standard Gaussian forward process using the model information, adapting it to online tasks with receding horizon strategies, and exploring advanced sampling and scheduling techniques to further improve performance.

#### Acknowledgments

This work was supported by NSF Grant 2154171, NSF CAREER 2339112 and CMU CyLab Seed Funding.