# Quasi-Monte Carlo Graph Random Features

Isaac Reid

University of Cambridge

ir337@cam.ac.uk

Krzysztof Choromanski

Google DeepMind

Columbia University

kchoro@google.com

&Adrian Weller

University of Cambridge

Alan Turing Institute

aw665@cam.ac.uk

Senior lead.

###### Abstract

We present a novel mechanism to improve the accuracy of the recently-introduced class of _graph random features_ (GRFs) . Our method induces negative correlations between the lengths of the algorithm's random walks by imposing _antithetic termination_: a procedure to sample more diverse random walks which may be of independent interest. It has a trivial drop-in implementation. We derive strong theoretical guarantees on the properties of these _quasi-Monte Carlo GRFs_ (q-GRFs), proving that they yield lower-variance estimators of the \(2\)-regularised Laplacian kernel under mild conditions. Remarkably, our results hold for any graph topology. We demonstrate empirical accuracy improvements on a variety of tasks including a new practical application: time-efficient approximation of the graph diffusion process. To our knowledge, q-GRFs constitute the first rigorously studied quasi-Monte Carlo scheme for kernels defined on combinatorial objects, inviting new research on correlations between graph random walks.1

## 1 Introduction and related work

Kernel methods are ubiquitous in machine learning . Via the kernel trick, they provide a mathematically principled and elegant way to perform nonlinear inference using linear learning algorithms. The positive definite _kernel function_\(k:\), defined on an input domain \(\), measures the'similarity' between two datapoints. Examples in Euclidean space include the Gaussian, linear, Matern, angular and arc-cosine kernels .

Though very effective on small datasets, kernel methods suffer from poor scalability. The need to materialise and invert the _kernel matrix_ typically leads to a time-complexity cubic in the size of the dataset. Substantial research has been dedicated to improving scalability by approximating this matrix, notably including _random features_ (RFs) . These randomised mappings \(:^{d}^{s}\) construct low-dimensional feature vectors whose dot product equals the kernel evaluation in expectation:

\[k(,)=(()^{}()).\] (1)

This permits a low-rank decomposition of the kernel matrix which enables better time- and space-complexity than exact kernel methods. Random feature methods exist for a variety of Euclidean kernels with properties engineered for the desiderata and symmetries of the particular kernel beingapproximated (Dasgupta et al., 2010; Johnson, 1984; Choromanski et al., 2020; Goemans and Williamson, 2001; Rahimi and Recht, 2007).

Kernels can also be defined on discrete input spaces such as _graphs_, which are the natural way to represent data characterised by local relationships (e.g. social networks or interacting chemicals (Albert and Barabasi, 2002)) or when data is restricted to a lower-dimensional manifold than the original space (Roweis and Saul, 2000; Belkin and Niyogi, 2003). We consider _graph kernels_\(k:\) on the set of nodes \(\) of a graph \(\). Examples include the diffusion, regularised Laplacian, \(p\)-step random walk and cosine kernels (Smola and Kondor, 2003; Kondor and Lafferty, 2002; Chung and Yau, 1999), which have found applications including in bioinformatics (Borgwardt et al., 2005), community detection (Kloster and Gleich, 2014) and recommender systems (Yajima, 2006). More recently, these kernels have been used in manifold learning for deep generative modelling (Zhou et al., 2020) and for solving shortest path problems (Crane et al., 2017). Substantial research effort has also been devoted to developing and analysing graph kernels \(k:\), now taking entire graphs \(\) from graph spaces \(\) as inputs rather than their nodes (Shervashidze et al., 2009; Vishwanathan et al., 2006; Shervashidze and Borgwardt, 2009), but we stress that these are not the subject of this paper.

The problem of poor kernel scalability is exacerbated in the graph domain because even computing the kernel matrix \(\) is typically of at least cubic time-complexity in the number of nodes \(N\). In contrast to kernels defined on points in \(^{d}\), random feature methods for fixed graph kernels (c.f. kernel learning (Fang et al., 2021)) have proved challenging to construct. Only recently has a viable _graph random feature_ (GRF) mechanism been proposed, which uses a series of random walkers depositing 'load' at each node they pass through (Choromanski, 2023). GRFs provide an unbiased estimate of the matrix \((_{N}-)^{-d}\), where \(\) is a weighted adjacency matrix of the graph and \(d\). The decomposition supports subquadratic time-complexity (again with respect to the number of nodes \(N\)) in downstream algorithms applying regularised Laplacian kernels. Moreover, the computation of GRFs admits a simple distributed algorithm that can be applied if a large graph needs to be split across machines. The author demonstrates the strong empirical performance of GRFs in speed tests, Frobenius relative error analysis and a \(k\)-means graph clustering task.

In the Euclidean setting, significant research has been dedicated to developing _quasi-Monte Carlo_ (QMC) variants to RF methods that enjoy better convergence properties (Yang et al., 2014; Lyu, 2017; Dick et al., 2013). By using correlated ensembles rather than i.i.d. random variables in the feature maps, one can suppress the mean squared error (MSE) of the kernel estimator. For example, _orthogonal random features_ (ORFs) (Yu et al., 2016; Choromanski et al., 2020) improve the quality of approximation of the Gaussian kernel when using trigonometric or positive random features, and of the linear kernel in the _orthogonal Johnson-Lindenstrauss transformation_(Choromanski et al., 2017). With positive random features, the recently-introduced class of _simplex random features_ (SimRFs) performs even better (Reid et al., 2023). This has been used to great effect in estimating the attention mechanism of Transformers (Vaswani et al., 2017), overcoming its prohibitive quadratic time-complexity scaling with token sequence length.

This invites the central question of this work: **how can we implement a QMC mechanism for random walks on a graph?** What do we mean by a 'diverse' sample in this context? Choromanski (2023) first identified the challenge of constructing _quasi-Monte Carlo GRFs_ (q-GRFs). They suggested a high-level approach of reinforced random walks, but left to future work its theoretical and empirical analysis. In this paper, we provide a first concrete implementation of q-GRFs, proposing an unbiased scheme that correlates the length of random walks by imposing _antibletic termination_. We derive strong theoretical guarantees on the properties of this new class, proving that the correlations reduce the variance of estimators of the \(2\)-regularised Laplacian kernel under mild conditions. Our results hold for any graph topology. We also demonstrate empirical accuracy improvements on a variety of tasks. We hope our new algorithm (hereafter referred to as 'q-GRFs' for brevity) will spur further research on correlations between graph random walks in machine learning.

We emphasise that, although we present antithetic termination through the lens of q-GRFs (an analytically tractable and important use case), it is fundamentally a procedure to obtain a more diverse ensemble of graph random walks and may be of independent interest. To illustrate this, in App 8.1 we show how it can alternatively be used to improve approximations of the PageRank vector (Page et al., 1998). It could also be relevant e.g. for estimating graphlet statistics for kernels between graphs[Chen et al., 2016; Ribeiro et al., 2021] or in some GNN architectures [Nikolentzos and Vazirgiannis, 2020].

The remainder of the manuscript is organised as follows. In **Sec. 2** we introduce the mathematical concepts and existing algorithms to be used in the paper, including the \(d\)-regularised Laplacian and diffusion kernels and the GRF mechanism. **Sec. 3** presents our novel q-GRFs mechanism and discusses its strong theoretical guarantees - in particular, that it provides lower kernel estimator variance than its regular predecessor (GRFs) under mild conditions. We provide a brief proof-sketch for intuition but defer full technical details to App. 8.4. We conduct an exhaustive set of experiments in **Sec. 4** to compare q-GRFs to GRFs, including: (a) quality of kernel approximation via computation of the relative Frobenius norm; (b) simulation of the graph diffusion process; (c) kernelised \(k\)-means node clustering; and (d) kernel regression for node attribute prediction. q-GRFs nearly always perform better and in some applications the difference is substantial.

## 2 Graph kernels and GRFs

### The Laplacian, heat kernels and diffusion on graphs

An undirected, weighted graph \((,)\) is defined by a set of vertices \(\{1,...,N\}\) and a set of edges \(\) given by the unordered vertex pairs \((i,j)\) where \(i\) and \(j\) are neighbours, themselves associated with weights \(W_{ij}\). The _weighted adjacency matrix_\(^{N N}\) has matrix elements \(W_{ij}\): that is, the associated edge weights if \((i,j)\) and \(0\) otherwise. Let \((i)\{j|(i,j)\}\) denote the set of neighbours of node \(i\).

Denote by \(^{N N}\) the diagonal matrix with elements \(D_{ii}_{j}W_{ij}\), the sum of edge weights connecting a vertex \(i\) to its neighbours. The _Laplacian_ of \(G\) is then defined \(-\). The _normalised Laplacian_ is \(}^{-}^ {-}\), which rescales \(\) by the (weighted) number of edges per node. \(\) and \(}\) share eigenvectors in the case of an equal-weights \(d\)-regular graph and play a central role in spectral graph theory; their analytic properties are well-understood [Chung, 1997].

In classical physics, diffusion through continuous media is described by the equation

\[}{dt}=^{2}\] (2)

where \(^{2}=}{ x_{1}^{2}}+}{  x_{2}^{2}}+...}{ x_{N}^{2}}\) is the _Laplacian operator_ on continuous spaces. The natural analogue on discrete spaces is \(\), where we now treat \(\) as a linear operator on vectors \(^{N}\). This can be seen by noting that, if we take \(\) to be the _unweighted_ adjacency matrix, \(,=^{} =-_{(i,j)}(u_{i}-u_{j})^{2}\) so, like its continuous counterpart, \(\) measures the local smoothness of its domain. In fact, in this case \(-\) is exactly the finite difference discretisation of \(^{2}\) on a square grid in \(N\)-dimensional Euclidean space. This motivates the _discrete heat equation_ on \(G\),

\[}{dt}=-},\] (3)

where we followed literature conventions by using the normalised variant whose spectrum is conveniently contained in \(\)[Chung, 1997]. This has the solution \(_{t}=(-}t)_{0}\), where \((-}t)=_{n}(1-}}{n})^{n}\). The symmetric and positive semi-definite matrix

\[_{}(t)(-}t)\] (4)

is referred to as the _heat kernel_ or _diffusion kernel_[Smola and Kondor, 2003]. The exponentiation of the generator \(}\), which by construction captures the _local_ structure of \(G\), leads to a kernel matrix \(_{}\) which captures the graph's _global_ structure. Upon discretisation of Eq. 3 with the backward Euler step (which is generally more stable than the forward), we have that

\[_{t+ t}=(_{N}+ t})^{-1} _{t},\] (5)

where the discrete time-evolution operator \(_{}^{(1)}=(_{N}+ t}) ^{-1}\) is referred to as the \(1\)_-regularised Laplacian kernel_. This is a member of the more general family of \(d\)-regularised Laplacian kernels,

\[_{}^{(d)}=(_{N}+ t} )^{-d},\] (6)for which we can construct an unbiased approximation using GRFs [Choromanski, 2023]. We predominantly consider \(d=2\) with the understanding that estimators for other values of \(d\) (and the diffusion kernel) are straightforward to obtain - see App. 8.2. This demonstrates the intimate connection between the graph-diffusion and Laplacian kernels, and how a QMC scheme that improves the convergence of \(_{}^{(2)}\) will be of broad interest.

### Graph random features (GRFs)

Here we recall the GRF mechanism, which offers a rich playground for our novel antithetic termination QMC scheme. The reader should consult Choromanski  (especially Algorithm 1.1) for a full discussion, but for convenience we provide a cursory summary.

Suppose we would like to estimate the matrix \((_{N}-)^{-2}\), with \(^{N N}\) a weighted adjacency matrix of a graph with \(N\) nodes and no loops. Choromanski  proposed a novel algorithm to construct a randomised estimator of this matrix. The author uses _graph random features_ (GRFs) \((i)^{N}\), with \(i\) the index of one of the nodes, designed such that

\[(_{N}-)_{ij}^{-2}=((i)^{ }(j)).\] (7)

They construct \((i)\) by taking \(m\) random walks \(\{(k,i)\}_{k=1}^{m}\) on the graph out of node \(i\), depositing a 'load' at every node that depends on i) the product of edge weights traversed by the subwalk and ii) the marginal probability of the subwalk. Importantly, each walk terminates with probability \(p\) at every timestep. The \(x\)th component of the \(i\)th random feature is given by

\[(i)_{x}:=_{k=1}^{m}_{_{1}_{ i_{x}}}(_{1})}{p(_{1})}(_{1} (k,i)),\] (8)

where: \(k\) enumerates \(m\) random walks we sample out of node \(i\); \(_{1}\) denotes a particular walk from the set of all walks \(_{ix}\) between nodes \(i\) and \(x\); \((_{1})\) is the product of weights of the edges traversed by the walk \(_{1}\); \(p(_{1})\) is the marginal probability that a walk contains a prefix subwalk \(_{1}\), given by \(p(_{1})=((1-p)/d)^{(_{1})}\) in the simplest case of a walk of length \((_{1})\) on a \(d\)-regular graph; \((_{1}(k,i))\) is an indicator function that evaluates to \(1\) when the walk \(_{1}\) is a prefix subwalk of the \(k\)th random walk sampled from \(i\) (itself denoted \((k,i)\)) and is \(0\) otherwise.

It is simple to see how Eq. 8 satisfies Eq. 7. By construction

\[[(_{1}(k,i))(_{ 2}(l,j))]=p(_{1})p(_{2})\] (9)

for independent walks, whereupon

\[((i)^{}(j))=_{ x}_{_{1}_{ix}}_{_{2}_{ jx}}(_{1})(_{2})=_{ _{ij}}(()+1)()=( -)_{ij}^{-2}.\] (10)

This shows us that the estimator is unbiased. The central contribution of this work is a QMC scheme that induces correlations between the \(m\) walks out of each node to suppress the variance of the estimator \((i)^{}(j)\) without breaking this unbiasedness.

## 3 q-GRFs and antithetic termination

We will now present our novel antithetic termination mechanism. It generalises the notion of antithetic variates - a common, computationally cheap variance-reduction technique when sampling in Euclidean space [Hammersley and Morton, 1956] - to the termination behaviour of random walks.

We have seen that, in the i.i.d. implementation of the GRF algorithm, each walker terminates independently with probability \(p\) at every timestep. For a pair of i.i.d. walkers out of node \(i\), this is implemented by independently sampling two _termination random variables_ (TRVs) between \(0\) and \(1\) from a uniform distribution, \(t_{1,2}(0,1)\). Each walker terminates if its respective TRV is less than \(p\), \(t_{1,2}<p\). In contrast, we define the _antithetic_ walker as follows.

**Definition 3.1** (Antithetic walkers).: _We refer to a pair of walkers as antithetic if their TRVs are marginally distributed as \(t_{1,2}=(0,1)\) but are offset by \(\),_

\[t_{2}=_{1}(t_{1}+),\] (11)_such that we have the conditional probability density_

\[p(t_{2}|t_{1})=(_{1}(t_{2}-t_{1})-).\] (12)

In both schemes, the TRVs are resampled at every timestep until the corresponding walker terminates.

**Computational cost**: We note that the computational cost of generating an antithetic TRV according to Eq. 11 is no greater than the cost of generating an independent TRV, so this drop-in replacement in the GRF algorithm is cheap. We provide a schematic in Fig. 0(a).

Since the marginal distributions over \(t_{i}\) are unchanged our estimator remains unbiased, but the couplings between TRVs lead to statistical correlations between the walkers' terminations. Denoting by \(s_{1}\) the event that walker \(1\) terminates at some timestep, \(s_{2}\) the event that walker \(2\) terminates and \(_{1,2}\) their complements, it is straightforward to convince oneself that for \(p\)

\[ p(s_{1})=p(s_{2})=p,& p(_{1})=p(_{2})=1-p,\ p(s_{2}|s_{1})=0,\\ p(_{2}|s_{1})=1,& p(s_{2}|_{1} )=,& p(_{2}|_{1})=.\] (13)

This termination coupling modifies the joint probability distribution over walk lengths. In the i.i.d. scheme, the walks are independent and are of expected length

\[(())=.\] (14)

These _marginal_ expectations are preserved in the antithetic scheme, but now the expected length of one walk _conditioned on the length of the other_ is

\[((_{2})|(_{1})=m)=+2 ()^{m},\] (15)

which we derive in App. 8.3. It is straightforward to see that the two lengths are negatively correlated. Antithetic termination 'diversifies' the lengths of random walks we sample, preventing them from clustering together, and in the spirit of QMC this turns out to suppress the kernel estimator variance. See Fig. 0(b) for a schematic. We refer to random features constructed with antithetic walkers as _quasi-Monte Carlo graph random features_ (q-GRFs).

Though this paper focuses on antithetic termination for GRFs, it is a more general QMC scheme applicable in algorithms that sample walks with geometrically distributed lengths. To illustrate this, we again direct the reader to App. 8.1 where we show how it can be used to improve numerical estimates of the PageRank vector [Page et al., 1998].

Figure 1: _Left_: schematic of the i.i.d. (GRF) and antithetic (q-GRF) mechanisms in termination space. \(f(t)\) is the probability density of the termination random variable (TRV) \(t\). Vertical arrows represent draws of \(t\), with the walker terminating if they lie in the pink region where \(t<p\). With q-GRFs the TRVs are offset by \(\), modifying the joint distribution over walk lengths. _Right_: demonstration with \(4\) random walks on the karate graph, beginning at some node labelled \(i\). The blue pair of antithetic walks (q-GRFs) have very different lengths; they cannot terminate simultaneously. The red pair of i.i.d. walks (GRFs) have similar lengths. We prove that q-GRFs give lower variance estimators of the \(2\)-regularised Laplacian kernel.

### Theoretical results

In this section, we state and discuss our central theoretical results for the q-GRFs mechanism. Sec. 3.1.1 provides a sketch, but full proofs are deferred to App. 8.4. We remind the reader that results for \((_{N}-)^{-2}\) are trivially applied to \(_{}^{(2)}\) (see App. 8.2).

**Theorem 3.2** (Antithetic termination is better than i.i.d.).: _For any graph, q-GRFs will give lower variance on estimators of \((_{N}-)^{-2}\) than regular GRFs provided either i) the termination probability \(p\) is sufficiently small or ii) the spectral radius \(()\) is sufficiently small._

By'sufficiently small' we mean that for a fixed \(()\) there exists some value of \(p\) below which antithetic termination will outperform i.i.d.. Likewise, for fixed \(p\) there exists some value of \(()\). These conditions turn out to not be too restrictive in our experiments; antithetic termination is actually very effective at \(p=0.5\) which we use for practical applications.

Considering Eq. 11 carefully, it is easy to see that the termination probabilities in Eq. 13 are not particular to a TRV offset equal to \(\), and in fact hold for any offset \(\) satisfying \(p 1-p\). An immediate corollary is as follows.

**Corollary 3.3** (Maximum size of an antithetic ensemble).: _For a termination probability \(p\), up to \( p^{-1}\) random walkers can all be conditioned to exhibit mutually antithetic termination._

This is achieved by offsetting their respective TRVs by \(=p\). The resulting _antithetic ensemble_ will have lower kernel estimator variance than the equivalent number of i.i.d. walkers or a set of mutually independent antithetic pairs. We make one further interesting remark.

**Theorem 3.4** (Termination correlations beyond antithetic).: _A pair of random walkers with TRVs offset by \(p(1-p)<<p\) will exhibit lower variance on estimators of \((_{N}-)^{-2}\) than independent walkers, provided either i) the termination probability \(p\) is sufficiently small or ii) the spectral radius of the weighted adjacency matrix \(()\) is sufficiently small._

This provides an upper limit of \((p(1-p))^{-1}\) on the number of walkers we can simultaneously correlate before we can no longer guarantee that coupling in a further walker's TRV will be better than sampling it independently. Intuitively, Theorem 3.4 tells us that we can space TRVs even more closely than \(p\), allowing us to increase the number of simultaneously anticorrelated random walkers at the cost of the strength of negative correlations between walkers with neighbouring TRVs.

#### 3.1.1 Proof sketch

In this section, we will outline a proof strategy for the results reported earlier in this section. Full technical details are reported in App. 8.4.

From its Taylor expansion, the \((ij)\)-th element of \((_{N}-)^{-2}\) is nothing other than a sum over all possible walks between the nodes \(i\) and \(j\), weighted by their lengths and the respective products of edge weights. GRFs use a Monte Carlo scheme to approximate this sum by sampling such walks at random - concretely, by first sampling separate random walks out of nodes \(i\) and \(j\) and summing contributions wherever they intersect. In order to sample the space of walks between nodes \(i\) and \(j\) more efficiently, it follows that we should make our ensemble of walks out of each node more diverse. q-GRFs achieve this by inducing negative correlations such that they are different lengths.

In more detail, it is clear that the variance of the kernel estimator will depend upon the expectation of the square of \((i)^{}(j)\). Each term in the resulting sum will take the form

\[_{x,y}_{_{1}_{ ix}}_{_{2}_{jx}}_{_{3}_{1}}_{_{4} _{jy}}(_{1})}{p(_{1})}(_{2})}{p(_{2})}( _{3})}{p(_{3})}(_{4})}{p(_{4})}\\  p(_{1}(k_{1},i),_{3}(k_{2},i))p(_{2}(l_{1},j),_{4}(l_ {2},j)),\] (16)

where we direct the reader to Sec. 2.2 for the symbol definitions. Supposing that all edge weights are equal (an assumption we relax later), the summand is a function of the _length_ of each of the walks \(_{1,2,3,4}\). It is then natural to write the sum over all walks between nodes \(i\) and \(x\) as a sum over walk lengths \(m\), with each term weighted by a combinatorial factor that counts the number of walks of said length. This factor is \((^{m})_{ix}\), with \(\) the unweighted adjacency matrix. That is,

\[_{_{1}_{ix}}\,()=_{m=0}^{}(^{m})_{ ix}\,()\,.\] (17)\((^{m})_{ix}\) is readily written as its eigendecomposition, \(_{p=1}^{N}_{p}^{m}k_{pi}k_{px}\), with \(_{p}\) the \(p\)th eigenvalue and \(k_{pi}\) the \(i\)-th coordinate of the \(p\)-th eigenvector \(_{p}\). We put these into Eq. 16 and perform the sums over each walk length \(m_{1,2,3,4}\) from \(1\) to \(\), arriving at

\[_{x,y}_{k_{1},k_{2},k_{3},k_{4}}f(_{1},_{2 },_{3},_{4})k_{1i}k_{1x}k_{2j}k_{2x}k_{3i}k_{3y}k_{4j}k_{4y}\] (18)

where \(f(_{1},_{2},_{3},_{4})\) is a function of the eigenvalues of \(\) that depends on whether we correlate the terminations of the walkers. Since the eigenvectors are orthogonal we have that \(_{x}k_{1x}k_{2x}=_{12}\), so this reduces to

\[_{k_{1},k_{3}}f(_{1},_{1},_{3},_{3})k_{1i}k_{ 1j}k_{3i}k_{4j}.\] (19)

Our task becomes to prove that this expression becomes smaller when we induce antithetic termination. We achieve this by showing that a particular matrix (with elements given by the difference in \(f\) between the schemes) is negative definite: a task we achieve by appealing to Weyl's perturbation inequality (Bai et al., 2000).

## 4 Experiments

In this section we report on empirical evaluations of q-GRFs. We confirm that they give lower kernel estimator variance than regular GRFs and show that this often leads to substantially better performance in downstream tasks, including simulation of graph diffusion, \(k\)-means node clustering and kernel regression for node attribute prediction. We use ensembles of antithetic pairs (Def. 3.1).

### Estimation of the \(2\)-regularised Laplacian kernel

We begin with the simplest of tasks: estimation of the \(2\)-regularised Laplacian kernel,

\[_{}^{(2)}=(_{N}+^{2}})^{-2},\] (20)

where \(}^{N N}\) is the symmetrically normalised Laplacian. \(0<<1\) is a regulariser. We use both GRFs and q-GRFs to generate unbiased estimates \(}_{}^{(2)}\) (see App. 8.2), then compute the relative Frobenius norm \(\|_{}^{(2)}-}_{}^{(2)}\|_ {}^{2}/\|_{}^{(2)}\|_{}^{2}\) between the true and approximated kernel matrices. This enables us to compare the quality of the estimators. As a benchmark, we also include

Figure 2: Relative Frobenius norm error of estimator of the \(2\)-regularised Laplacian kernel with GRFs (red circle) and q-GRFs (green cross). Lower is better. We also include q-RRW-GRFs (Choromanski, 2023) (blue triangle), instantiated with \(f=\), as a benchmark. Our novel q-GRFs perform the best on every graph considered. \(N\) is the number of nodes and, for the Erdős-Rényi (ER) graphs, \(p\) is the edge-generation probability. One standard deviation is shaded but it is too small to easily see.

an implementation of the high-level reinforced random walk QMC mechanism suggested (but not tested) by Choromanski (2023). We choose the exponential mapping as the reinforcement function \(f\) (used to downweight the probability of traversing previously-visited edges), although the optimal choice remains an open problem. We refer to this mechanism as _q-RRW-GRFs_ to disambiguate from our instantiation of q-GRFs (which uses antithetic termination).

Fig. 2 presents the results for a broad class of graphs: small Erdos-Renyi, larger Erdos-Renyi, a binary rooted tree, a ladder, and four real-world examples from Ivashkin (2023) (karate, dolphins, football and eurosis). We consider \(2\), \(4\), \(8\) and \(16\) walks, taking \(100\) repeats for the variance of the approximation error. We use the regulariser \(=0.1\) and the termination probability \(p=0.5\).

The quality of kernel approximation naturally improves with the number of walkers. Inducing antithetic termination consistently reduces estimator variance, with our q-GRF mechanism outperforming regular GRFs in every case. The exact size of the gain depends on the particular graph (according to the closed forms derived in App. 8.4), but improvement is always present. It is intriguing that tree-like and planar graphs tend to enjoy a bigger gap; we defer a rigorous theoretical analysis to future work. Meanwhile, the q-RRW-GRF variant with exponential \(f\) is often worse than the regular mechanism and is substantially more expensive. We do not include it in later experiments.

### Scalable and accurate simulation of graph diffusion

In Sec. 2.1, we noted that the Laplacian \(\) (or \(}\)) is the natural operator to describe diffusion on discrete spaces and that the \(1\)-regularised Laplacian kernel constitutes the corresponding discrete time-evolution operator. Here we will show how, by leveraging q-GRFs to provide a lower-variance decomposition of \(_{}^{(2)}\), we can simulate graph diffusion in a scalable and accurate way.

Choosing a finite (even) number of discretisation timesteps \(N_{t}\), we can approximate the final state by

\[_{t}}_{t}[(_{N}+ }})^{-2}]^{}{2}}_{0}=_{}^{(2)}{2}}_{0}\] (21)

This is equivalent to approximating the action of the diffusion kernel \(_{}\) (see App. 8.2). We can efficiently compute this using our GRF or q-GRF decomposition of \(_{}^{(2)}\) and compare the accuracy of reconstruction of \(_{t}\).2 In particular, we take an initial one-hot state \(_{0}=(1,0,0,...,0)^{}\) and simulate diffusion for \(t=1\) divided into \(N_{t}=1000\) timesteps, using \(m=10\) walkers and a termination probability \(p=0.5\). Fig. 3 gives a schematic. We average the MSE of \(}_{t}\) over \(1000\) trials. Table 1 reports the results; q-GRFs approximate the time evolution operator more accurately so consistently give a lower simulation error. Since the time-evolution operator is applied repeatedly, even modest improvements in its approximation can lead to a substantially better reconstruction of the final state. In extreme cases the simulation error is halved.

   Graph & \(N\) & Sim error, \((}_{t})\) \\  & & GRFs & q-GRFs \\  Small ER & 20 & 0.0210(5) & **0.0160(3)** \\ Larger ER & 100 & 0.0179(9) & **0.0085(3)** \\ Binary tree & 127 & 0.0161(6) & **0.0106(3)** \\ Ladder & 100 & 0.0190(8) & **0.0105(3)** \\ karate & 34 & 0.066(2) & **0.054(1)** \\ dolphins & 62 & 0.0165(4) & **0.0139(3)** \\ football & 115 & 0.0170(3) & **0.0160(2)** \\ eurosis & 1272 & 0.089(1) & **0.084(1)** \\   

Table 1: MSE of the approximation of the final state \(_{t}\) by repeated application of an approximation of the discrete time-evolution operator, constructed with (q-)GRFs. Brackets give one standard deviation.

Figure 3: Schematic of diffusion on a small graph, with heat initially localised on a single node spreading under the action of the Laplace operator. We use (q-)GRFs to estimate the quantity on the right.

### Kernelised \(k\)-means clustering for graph nodes

Next we evaluate the performance of GRFs and q-GRFs on the task of assigning nodes to clusters using the graph kernel, following the approach described by Dhillon et al. (2004). We first run the algorithm to obtain \(N_{c}=2\) clusters with the exact \(2\)-regularised Laplacian kernel, then compare the results when we use its approximation via GRFs and q-GRFs. In each case, we report the _clustering error_, defined by

\[E_{c}}{N(N-1)/2}.\] (22)

This is simply the number of misclassified pairs (in the sense of being assigned to the same cluster when the converse is true or vice versa) divided by the total number of pairs. The results are less straightforward because curiously the clustering error does not generally vary monotonically with the variance on the kernel estimator, but nonetheless in six out of eight cases q-GRFs provide equally good or better results.

### Kernel regression for node attribute prediction

Lastly, we consider the problem of kernel regression on a triangular mesh graph. Each node is associated with a normal vector \(^{(i)}\), equal to the mean of the normal vectors of its surrounding faces. The task is to predict the directions of missing vectors (a random 5% split) from the remainder. We consider five meshes of different sizes, available from Dawson-Haggerty (2023). Our prediction for the direction of the normal vector corresponding to the \(i\)th node is

\[}^{(i)}_{}^{(2)}( i,j)^{(j)}}{_{j}_{}^{(2)}(i,j)},\] (23)

where \(j\) sums over the vertices with known vectors (a 95% split). This is simply a linear combination of all the training node normal vectors, weighted by their respective kernel evaluations. We compute the average angular error \(1-\) between the prediction \(}^{(i)}\) and groundtruth \(^{(i)}\) across the missing vectors, comparing the result when \(_{}^{(2)}\) is approximated with GRFs and q-GRFs with \(m=6\) random walks at a termination probability \(p=0.5\). The regulariser is \(=0.1\). q-GRFs enjoy lower kernel estimator variance so consistently give better predictions for the missing vectors.

## 5 Conclusion

We have proposed a novel class of quasi-Monte Carlo graph random features (q-GRFs) for unbiased and efficient estimation of kernels defined on the nodes of a graph. We have proved that our new algorithm, which induces negative statistical correlations between the lengths of graph random walks via antithetic termination, enjoys better convergence properties than its regular predecessor (GRFs). This very often permits better performance in empirical tasks and for some applications the improvement is substantial. Our work ushers in further research in this new domain of quasi-Monte Carlo methods for kernels on combinatorial objects. It may be of broader interest including for algorithms that sample random walks.

   Graph & \(N\) & \)} \\  & & GRFs & q-GRFs \\  karate & 34 & 0.11 & **0.05** \\ dolphins & 62 & 0.40 & **0.38** \\ polbooks & 105 & **0.28** & **0.28** \\ football & 115 & 0.10 & **0.09** \\ eu-core & 1005 & **0.04** & 0.05 \\ databases & 1046 & 0.17 & **0.11** \\ eurosis & 1285 & **0.15** & 0.19 \\ citeseer & 3300 & 0.02 & **0.01** \\   

Table 2: Errors in kernelised \(k\)-means clustering when approximating the Gram matrix with q-GRFs and GRFs. Lower is better.

   Graph & \(N\) &  \\  & & GRFs & q-GRFs \\  cylinder & 210 & 0.104(1) & **0.101(1)** \\ teapot & 480 & 0.0531(5) & **0.0493(5)** \\ idler-riser & 782 & 0.0881(6) & **0.0852(5)** \\ busted & 1941 & 0.00690(4) & **0.00661(4)** \\ torus & 4350 & 0.00131(1) & **0.00120(1)** \\   

Table 3: \(1-\) between true and predicted node vectors when approximating the Gram matrix with q-GRFs and GRFs. Lower is better. Brackets give one standard deviation.

## 6 Broader impacts and limitations

We envisage several possible **impacts** of our novel algorithm. First, graphs provide a natural way to describe systems characterised by complex biological interactions such as the proteins at the proteome scale or drugs in the body (Ingraham et al., 2019). Our novel QMC algorithm might be of translational impact in this **bioinformatics** setting. Antithetic termination is at its heart a procedure to improve the sampling efficiency of random walks, so it could also help mitigate the notoriously high **energy and carbon** cost of large models (Strubell et al., 2019). Lastly, we believe our results are of **intrinsic interest** as the first (to our knowledge) rigorously studied QMC scheme defined on a combinatorial object. They might spur further research in this new domain. Our work is **foundational** with no immediate direct negative societal impacts that we can see. However, it is important to note that increases in scalability afforded by GRF and q-GRF algorithms could amplify risks of graph-based machine learning, either from bad actors or as unintended consequences.

The work also has some **limitations** and **directions for future research**. First, although we have derived closed-form expressions for the kernel estimator variance with GRFs and q-GRFs in App. 8.4, they are still complicated functions of the spectra of the respective graph Laplacians. Understanding **what characterises graphs that particularly benefit** from antithetic termination (empirically, tree-like and planar graphs) is an important future direction. Moreover, our scheme only correlates walk lengths. A more sophisticated mechanism that **couples walk directions** might do even better. Lastly, further work is needed to fully understand the applicability of antithetic termination **beyond the GRF setting**.

## 7 Relative contributions and acknowledgements

IR devised the antithetic termination scheme, proved all theoretical results and ran the experiments in Secs 4.1, 4.2 and 4.4. KC provided crucial support throughout, particularly: running the clustering experiment in Sec. 4.3, showing how \(d\)-regularised Laplacian kernels can be used to approximate the graph diffusion process, and proposing to apply q-GRFs to the problems in Secs 4.2 and 4.4. AW gave important guidance and feedback on the manuscript.

IR acknowledges support from a Trinity College External Studentship. AW acknowledges support from a Turing AI fellowship under grant EP/V025279/1 and the Leverhulme Trust via CFI.

We thank Austin Tripp and Kenza Tazi for their thoughtful feedback on earlier versions of the text, and Michael Ren for his excellent suggestion to treat the negative definite property perturbatively. We are also grateful to the anonymous reviewers.