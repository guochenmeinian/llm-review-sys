# Online Nonstochastic

Model-Free Reinforcement Learning

 Udaya Ghai \({}^{}\)

Amazon

ughai@amazon.com

&Arushi Gupta \({}^{}\)

Princeton University & Google DeepMind

arushig@princeton.edu

&Wenhan Xia

Princeton University & Google DeepMind

wxia@princeton.edu

&Karan Singh

Carnegie Mellon University

karansingh@cmu.edu

&Elad Hazan

Princeton University & Google DeepMind

ehazan@princeton.edu

Work performed while at Princeton University and Google. \(\) denotes equal contribution.

###### Abstract

We investigate robust model-free reinforcement learning algorithms designed for environments that may be dynamic or even adversarial. Traditional state-based policies often struggle to accommodate the challenges imposed by the presence of unmodeled disturbances in such settings. Moreover, optimizing linear state-based policies pose an obstacle for efficient optimization, leading to nonconvex objectives, even in benign environments like linear dynamical systems.

Drawing inspiration from recent advancements in model-based control, we introduce a novel class of policies centered on disturbance signals. We define several categories of these signals, which we term pseudo-disturbances, and develop corresponding policy classes based on them. We provide efficient and practical algorithms for optimizing these policies.

Next, we examine the task of online adaptation of reinforcement learning agents in the face of adversarial disturbances. Our methods seamlessly integrate with any black-box model-free approach, yielding provable regret guarantees when dealing with linear dynamics. These regret guarantees unconditionally improve the best-known results for bandit linear control in having no dependence on the state-space dimension. We evaluate our method over various standard RL benchmarks and demonstrate improved robustness.

## 1 Introduction

Model-free reinforcement learning in time-varying responsive dynamical systems is a statistically and computationally challenging problem. In contrast, model based control of even unknown and changing linear dynamical systems has enjoyed recent successes. In particular, new techniques from online learning have been applied to these linear dynamical systems (LDS) within the framework of online nonstochastic control. A comprehensive survey can be found in Hazan and Singh (2022). The key innovation in the aforementioned framework is the introduction of a new policy class called Disturbance-Action Control (DAC), which achieves a high degree of representational capacity without compromising computational efficiency. Moreover, efficient gradient-based algorithms canbe employed to obtain provable regret bounds for this approach, even in the presence of adversarial noise. Crucially, these methods rely on the notion of disturbance, defined to capture unmodeled deviations between the observed and nominal dynamics, and its availability to the learner.

This paper explores the potential of applying these disturbance-based techniques, which have proven effective in model-based control, to model-free reinforcement learning. However, it is not immediately clear how these methods can be adapted to model-free RL, as the disturbances in model-free RL are unknown to the learner.

We therefore develop the following approach to this challenge: instead of relying on a known disturbance, we create a new family of signals, which we call "Pseudo-Disturbances", and define policies that use "Pseudo-Disturbance" features to produce actions. The advantage of this approach is that it has the potential to produce more robust policies. Again inspired by model-based methods, we aim to augment existing reinforcement learning agents with a "robustness module" that serves two purposes. Firstly, it can filter out adversarial noise from the environment and improve agent performance in noisy settings. Secondly, in cases where the environment is benign and simple, such as a linear dynamical system, the augmented module will achieve a provably optimal solution. We also empirically evaluate the performance of our method on OpenAI Gym environments.

### Our Contributions

In this work, we make the following algorithmic and methodological contributions:

* In contrast to state-based policies commonly used in RL, Section 3 defines the notion of a **disturbance-based policy**. These policies augment traditional RL approaches that rely strictly on state feedback.
* We develop **three distinct and novel methods** (Sections 3.1, 3.2, 3.3) to estimate the Pseudo-Disturbance in the model-free RL setting.
* We develop a **new algorithm**, MF-GPC (Algorithm 1), which adapts existing RL methods to take advantage of our Pseudo-Disturbance framework.
* We **empirically evaluate** our method on OpenAI Gym environments in Section 5. We find that our adaptation applied on top of a DDPG baseline performs better than the baseline, significantly so in same cases, and has better robustness characteristics.
* We prove that the proposed algorithm achieves **sublinear regret** for linear dynamics in Theorem 4. These regret bounds improve upon the best-known for bandit linear control in terms of their dependence on state space dimension (Appendix E). Notably, our bounds have **no dependence on the state dimension**, reducing the state-of-the-art regret bound by factors of \(}\) for convex losses and \(d_{x}^{2/3}\) if losses are additionally smooth, signalling that our methodology is better suited to challenging high-dimensional under-actuated settings.

### Pseudo-Disturbance based RL

A fundamental primitive of the non-stochastic control framework is the _disturbance_. In our RL setting, the system evolves according to the following equation

\[_{t+1}=f(_{t},_{t})+_{t}\;,\]

where \(_{t}\) is the state, \(_{t}\) is control signal, and \(_{t}\) is a bounded, potentially adversarially chosen, disturbance. Using knowledge of the dynamics, \(f\), non-stochastic control algorithms first compute \(_{t}\), and then compute actions via DAC, as follows

\[_{t}=_{}(_{t})+_{i=1}^{h}M_{i}^{t} _{t-i}\;.\]

Here \(_{}\) is a baseline linear controller, and \(M^{t}\) are matrices, learned via gradient descent or similar algorithms. For linear systems, the DAC law is a convex relaxation of linear policies, which allows us to prove regret bounds against powerful policy classes using tools from online convex optimization.

To generalize this approach, without a model or knowledge of the dynamics function \(f\), both defining and obtaining this disturbance in order to implement DAC or similar policies becomes unclear. Toaddress this, we introduce the concept of a _Pseudo-Disturbance_ (PD) and provide three distinct variants, each representing a novel signal in reinforcement learning. These signals have various advantages and disadvantages depending on the available environment:

1. The first notion is based on the gradient of the temporal-difference error. It assumes the availability of a value function oracle that can be evaluated or estimated online or offline using any known methodology.
2. The second notion also assumes the availability of a black-box value function oracle/generator. We assign artificial costs over the states and generate multiple auxiliary value functions to create a "value vector." The Pseudo-Disturbance is defined as the difference between the value vector at consecutive states. This signal's advantage is that it does not require any zero-order optimization mechanism for estimating the value function's gradient.
3. The third notion assumes the availability of an environment simulator. The Pseudo-Disturbance is defined as the difference between the true state and the simulated state for a specific action.

For all these Pseudo-Disturbance variants, we demonstrate how to efficiently compute them (under the appropriate assumption of either a value function oracle or simulator). We provide a reduction from any RL algorithm to a PD-based robust counterpart that converts an RL algorithm into one that is also robust to adversarial noise. Specifically, in the special case of linear dynamical systems our algorithm has provable regret bounds. The formal description of our algorithm, as well as a theorem statement, are given in Section 4. For more general dynamical systems, the learning problem is provably intractable. Nonetheless, we demonstrate the efficacy of these methods empirically.

### Related Work

Model-free reinforcement learning.Reinforcement learning (Sutton and Barto, 2018) approaches are classified as model-free or model-based (Janner et al., 2019; Ha and Schmidhuber, 2018; Osband and Van Roy, 2014), dependent on if they attempt to explicitly try to learn the underlying transition dynamics an agent is subject to. While the latter is often more sample efficient (Wang et al., 2019), model-free approaches scale better in that their performance does not prematurely saturate and keeps improving with number of episodes (Duan et al., 2016). In this paper, we focus on adaption to unknown, arbitrary disturbances for model-free reinforcement learning algorithms, which can be viewed as a tractable restriction of the challenging adversarial MDP setting (Abbasi Yadkori et al., 2013). Model-free approaches may further be divided into policy-based (Schulman et al., 2015, 2017), value-based approaches (Mnih et al., 2013), and actor-critic approaches (Barth-Maron et al., 2018; Lillicrap et al., 2016); the latter use a learnt value function to reduce the variance for policy optimization.

Robust and Adaptive reinforcement learning.Motivated by minimax performance criterion in robust control (Zhang et al., 2021; Morimoto and Doya, 2005) introduced to a minimax variant of Q-learning to enhance of he robust of policies learnt from off-policy samples. This was later extended to more tractable formulations and structured uncertainty sets in Tessler et al. (2019); Mankowitz et al. (2019); Pinto et al. (2017); Zhang et al. (2021); Tamar et al. (2013), including introductions of model-based variants (Janner et al., 2019). Another approach to enhance the robustness is Domain Randomization (Tobin et al., 2017; Akkaya et al., 2019; Chen et al., 2021), wherein a model is trained in a variety of randomized environments in a simulator, and the resulting policy becomes robust enough to be applied in the real world. Similarly, adversarial training (Mandlekar et al., 2017; Vinitsky et al., 2020; Agarwal et al., 2021) has been shown to improve performance in out-of-distribution scenarios. In contrast to the previously mentioned approaches, our proposed approach only adapts the policy to observed disturbances at test time, and does not require a modification of the training procedure. This notably means that the computational cost and sample requirement of the approach matches that of vanilla RL in training, and has the benefit of leveraging recent advances in mean-reward RL, which is arguably better understood and more studied. Adaption of RL agents to new and changing environments has been similarly tackled through the lens of Meta Learning and similar approaches (Wang et al., 2016; Nagabandi et al., 2018; Pritzel et al., 2017; Agarwal et al., 2021).

Online nonstochastic control.The presence of arbitrary disturbances during policy execution had been for long in the fields of robust optimization and control (Zhou and Doyle, 1998). In contrast to minimax objectives considered in robust control, online nonstochastic control algorithms (see Hazan and Singh (2022) for a survey) are designed to minimize regret against a benchmark policy class, and thus compete with the best policy from the said class determined posthoc. When the benchmark policy class is sufficiently expressive, this approach has the benefit of robustness against adversarially chosen disturbances (i.e. non-Gaussian and potentially adaptively chosen (Ghai et al., 2021)), while distinctly not sacrificing performance in the typical or average case. The first nonstochastic control algorithm with sublinear regret guarantees was proposed in Agarwal et al. (2019) for linear dynamical systems. It was subsequently extended to partially observed systems (Simchowitz et al., 2020), unknown systems (Hazan et al., 2020), multi-agent systems (Ghai et al., 2022) and the time-varying case (Minasyan et al., 2021). The regret bound was improved to a logarithmic rate in Simchowitz (2020) for strongly convex losses. Chen et al. (2021) extend this approach to non-linearly parameterized policy classes, like deep neural networks. Bandit versions of the nonstochastic control setting have also been studied (Gradu et al., 2020; Cassel and Koren, 2020; Sun et al., 2023) and are particularly relevant to the RL setting, which only has access to scalar rewards.

### Paper Outline

After some basic definitions and preliminaries in Section 2, we describe the new Pseudo-Disturbance signals and how to create them in a model-free reinforcement learning environment in Section 3. In Section 4 we give a unified meta-algorithm that exploits these signals and applies them as an augmentation to any given RL agent. In Section 5 we evaluate our methods empirically.

An overview of notation can be found in Appendix A. Appendix B contains additional experimental details. Generalization of our algorithm to discrete spaces is provided in Appendix C. Proofs for Section 3 are provided in Appendix D, while the main theore is proved in Appendix E.

## 2 Setting and Preliminaries

Consider an agent adaptively choosing actions in a dynamical system with adversarial cost functions. We use notation from the control literature: \(_{t}^{d_{x}}\) is a vector representation of the state2 at time \(t\), \(_{t}^{d_{u}}\) is the corresponding action. Formally, the evolution of the state will follow the equations

\[_{t+1}=f(_{t},_{t})+_{t},\]

where \(_{t}\) is an arbitrary (even adversarial) disturbance the system is subject to at time \(t\). Following this evolution, the agent suffers a cost of \(c_{t}(_{t},_{t})\).

In this work, we adapt model-free reinforcement learning algorithms to this more challenging case. The (easier) typical setting for model-free methods assume, in contrast, that the disturbance \(_{t}\) is sampled _iid_ from a distribution \(\), and that the cost functions \(c(,)\) is fixed and known. Central to the study of model-free methods are the notions of the state and state-action value functions, defined as the discounted sum of future costs acquired by starting at any state (or state-action pair) and thereafter following the policy \(\). For any policy \(\), we denote the state and state-action value functions, which are mappings from state or state/action pair to the real numbers, as

\[Q_{}(,)=[._{t=0}^{} ^{t}c(_{t}^{},_{t}^{})|_{0}^{ }=,_{0}^{}=]\,V_{}()= [._{t=0}^{}^{t}c(_{t}^{},_{t}^ {})|_{0}^{}=]\,\]

where expectations are taken over random transitions in the environment and in the policy.

A special case we consider is that of linear dynamical systems. In these special instances the state involves linearly according to a linear transformation parameterized by matrices \(A,B\), i.e.

\[_{t+1}=A_{t}+B_{t}+_{t}.\]Pseudo-Disturbance Signals and Policies

In this section we describe the three different Pseudo-Disturbance (PD) signals we can record in a general reinforcement learning problem. As discussed, the motivation for this signal comes from the framework of online nonstochastic control. We consider dynamical systems with an additive misspecification or noise structure,

\[_{t+1}=f(_{t},_{t})+_{t},\]

where the perturbation \(_{t}\) does not depend on the state. Using perturbations rather than state allows us to avoid recursive structure that makes the optimization landscape challenging and nonconvex. As discussed, we introduce Pseudo-Disturbance signals \(}_{t}^{d_{w}}\) in lieu of the true disturbances. We note that the PD dimensionality \(d_{w}\) need not be the same as that of the true disturbance, \(d_{x}\).

An important class of policies that we consider henceforth is linear in the Pseudo-Disturbance, i.e.

\[_{}=\{.(_{1:t})=_{}(_{t})+_{i=1}^{h}M_{i}}_{t-i}|M_{i}^{d_ {u} d_{w}}\}.\]

Here \(_{}\) denotes the policy class of Disturbance-Action-Control. The fact that \(_{t}\) does not depend on our actions allows for convex optimization of linear disturbance-action controllers in the setting of linear dynamical systems, see e.g. Hazan and Singh (2022).

We would like to capture the essence of this favorable phenomenon in the context of model free RL, but what would replace the perturbations \(_{t}\) without a dynamics model \(f\)? That's the central question of this section, and we henceforth give three different proposal for this signal.

An important goal in constructing these signals is that **in the case of linear dynamical systems, it recovers the perturbation**. This will enable us to prove regret bounds in the case the environment is an LDS.

### Pseudo-Disturbance Class I: Value-Function Gradients

The first signal we consider is based on the gradient of the value function. The value function maps the state onto a scalar, and this information is insufficient to recreate the perturbation even if the underlying environment is a linear dynamical system. To exact a richer signal, we thus consider the gradient of the value function with respect to the action and state. The basic goal is to implement the following equation

\[}_{t}=_{}( V_{}(f(_{t}, )+_{t})-(Q_{}(_{t},)-c(_ {t},))|_{=_{t}}\,\]

where \(f(_{t},)+_{t}\) represents the counterfactual next state after playing \(\) at state \(_{t}\). Note, this signal is a gradient of the temporal-difference error (Sutton and Barto, 2018), in fact being syntactically similar to expected SARSA. If \(_{t}\) was in fact (_iid_) stochastic with \(V_{}\), \(Q_{}\) as corresponding value functions, this term on expectation would be zero. Therefore, this signal on average measures deviation introduced in \(_{t+1}\) due to arbitrary or adversarial \(_{t}\). We can also view this expression as

\[}_{t}=_{}( V_{}(f(_{t}, )+_{t})- V_{}(f(_{t},)))|_{ =_{t}}\.\]

\(V_{}\) is quadratic in the linear quadratic regulator setting, so this becomes a linear function of \(_{t}\). Computing \(_{}V_{}(f(_{t},)+_{t})|_{ =_{t}}\) analytically would require knowledge of the dynamics, but luckily this can be efficiently estimated online. Using a policy \(\), with noised actions \(_{t}=(_{t})+_{t}\), for \(_{t}(0,)\) we have the following PD estimates:

\[}_{t}= V_{}(_{t+1})^{-1} _{t}-_{}(Q_{}(_{t},)-c( _{t},))|_{=_{t}}\,}\] (1)

\[}_{t}=(c(_{t},_{t})+ V_{}( _{t+1})-Q_{}(_{t},_{t}))^{-1}_ {t}\.}\] (2)

These are zeroth-order gradient estimators (see (Liu et al., 2020) for a more detailed exposition). Intuitively, the second estimator may have lower variance as the expected SARSA error can be much smaller than the magnitude of the value function. An additional benefit is that this implementation only requires a scalar cost signal without needing access to a differentiable cost function.

The most important property of this estimator is that it, in expectation, it produces a signal that is a linearly transformation of the true disturbance if the underlying setting is a linear dynamical system. This is formalized in the following lemma.

**Lemma 1**.: _Consider a time-invariant linear dynamical systems with system matrices \(A,B\) and quadratic costs, along with a linear baseline policy \(\) defined by control law \(_{t}=-K_{}_{t}\). In expectation, the pseudo disturbances (1) and (2) are linear transformations of the actual perturbation_

\[[}_{t}|_{t}]=T_{t},\]

_where \(T\) is a fixed linear operator that depends on the system._

### Pseudo-Disturbance Class II: Vector Value Functions

The second approach derives a signal from auxiliary value functions. Concretely, instead of scalar-valued cost function \(c:^{d_{x}}\), consider a vector-valued cost function \(:^{d_{x}}^{d_{w}}\). For such vector-valued cost, we introduce vectorized value and state-action value functions as

\[V^{}_{}:^{d_{x}}^{d_{w}}\,\ Q^{ }_{}:^{d_{x}}^{d_{w}}^{d_{w}}\.\]

In particular, we have

\[Q^{}_{}(,)=[._{t=0}^ {}^{t}(^{}_{t})|^{}_ {0}=,^{}_{0}=]\,V^{}_{}()= [._{t=0}^{}^{t}(^{ }_{t})|^{}_{0}=]\.\]

Our PD signal is then

\[}_{t}=(_{t})+^{ }_{}(_{t+1})-^{}_{}(_ {t},_{t})\.}\] (3)

In contrast to the first approach, for a fixed set of cost functions, this approach provides a deterministic PD-signal. This is very beneficial, as at inference time the DAC policy can be run without injecting additional noise and without requiring a high variance stochastic signal. This does come at a cost, as this method requires simultaneous off-policy evaluation for many auxiliary value functions (each corresponding to a different scalar cost) before DAC can be run via \(Q\)-function evaluations at inference, both of which can be significantly more expensive than the first approach.

For the case of linear dynamical systems, if we use _linear_ costs on top of a linear base policy, this approach can recover the disturbances up to a linear transformation. It can be seen that the values corresponding to a linear cost function \(c\) are linear functions of the state, and hence the vectorized versions are also linear functions of state. We formalize this as follows:

**Lemma 2**.: _Consider a time-invariant linear dynamical systems with system matrices \(A,B\), along with a linear baseline policy \(\) defined by control law \(_{t}=-K_{}_{t}\). Let \(^{}_{}\) and \(^{}_{}\) be value functions for \(\) for i.i.d. zero mean noise with linear costs \((x):=Lx\), then the PD-signal (3) is a linear transformation_

\[}_{t}=T_{t},\]

_where \(T\) is a fixed linear operator that depends on the system and baseline policy \(\). In addition, if \(L\) is full rank and the closed loop dynamics are stable, then \(T\) is full rank._

### Pseudo-Disturbance Class III: Simulator Based

The last Pseudo-Disturbance signal we consider requires a potentially inaccurate simulator. It is intuitive, particularly simple to implement, and yet comes with theoretical guarantees.

The Pseudo-Disturbance is taken to be the difference between the actual state reached in an environment, and the expected state, over the randomness in the environment. To compute the expected state, we require the simulator \(f_{}\) initialized at the current state. Formally,

\[}_{t}=_{t+1}-f_{}(_{t}, _{t}).}\] (4)

The simplicity of this PD is accompanied by a simple lemma on its characterization of the disturbance in a dynamical system, even if that system is time varying, as follows,

**Lemma 3**.: _Suppose we have a simulator \(f_{}\) such that \(,,\|f_{}(,)-f( ,)\|\ \ \), then Pseudo-Disturbance (4) is approximately equal to the actual perturbation \(\|}_{t}-_{t}\|\ \ \)._

### Merits of different Pseudo-Disturbance signals

Each of the three PD signals described in this section offers something a bit different. PD3 offers the most direct disturbance signal, but comes with the requirement of a simulator. If the simulator is very accurate, this is likely the strongest signal, though this method may not be suitable with a large sim-to-real gap. PD1 and PD2 on the other hand, do not require a simulator but also have a natural trade off. PD1 is simpler and easier to add on top of an existing policy. However, it uses zeroth-order estimation, so the guarantees only hold in expectation and it may have high variance. On the other hand, PD2 is not a stochastic estimate, but it requires auxiliary value estimation from the base policy. This may come at the cost of additional space and computational complexity. In many cases, this can be handled using the same deep Q-network except with a wider head, which may not be so onerous. We note that PD2 **does not** require specific domain engineered signals for the auxiliary rewards. For example, using the coordinates of the state representation was enough to demonstrate improvements over baselines in our experiments. For richer, higher dimensional (visual) state spaces, this can be generalized using neural representations of state as the auxiliary reward, achieved by modulating the PD2 disturbance dimension to account for the fact that the underlying dynamics are simpler.

## 4 Meta Algorithm and Main Theorem

In this section we define a meta-algorithm for general reinforcement learning. The algorithm takes as an input an existing RL method, that may or may not have theoretical guarantees. It adds an additional layer on top, which estimates the Pseudo-Disturbances according to one of the three methods in the previous section. It then uses an online gradient method to optimize a linear policy in the past Pseudo-Disturbances. This can be viewed as a zeroth-order model-free version of the Gradient Perturbation Controller (GPC) [Agarwal et al., 2019].

The algorithm is formally defined in Algorithm 1. A typical choice of the parametrization \((|M)\) is a linear function of a window of past disturbances (ie. Disturbance Action Control [Agarwal et al., 2019]).

\[(_{t-1:t-h}|M_{1:h})=_{i=1}^{h}M_{i}_{t-i}.\] (5)

```
1:Input: Memory parameter \(h\), learning rate \(\), exploration noise covariance \(\), initialization \(M_{1:h}^{1}^{d_{u} d_{u} h}\), initial value and \(Q\) functions, base RL algorithm \(\).
2:for\(t=1 T\)do
3: Use action \(_{t}=_{}(_{t})+(}_{t-1:t-h }|M^{t})+_{t}\), where \(_{t}\) is _iid_ Gaussian, i.e. \[_{t}(0,)\]
4: Observe state \(_{t+1}\), and cost \(c_{t}=c_{t}(_{t},_{t})\).
5: Compute Pseudo-Disturbance [see (2),(3),(4)] \[}_{t}=(_{t+1},_{t}, _{t},c_{t},_{t}).\]
6: Update policy parameters using the stochastic gradient estimate (see Section 4.1) \[M^{t+1} M^{t}-\;c_{t}(_{t},_{t})^{-1} _{j=0}^{h-1}_{t-i} J_{i}^{t},\] where \(\) is an outer product and \(J_{i}^{t}=}_{t-i-1:t-h-i}\) for (5), and more generally, \[J_{i}^{t}=.}_{t-i-1:t-h-i}|M_{i})}{  M}|_{M=M^{t}}.\]
7:endfor
8:Optionally, update the policy \(_{}\) and its \(Q,V\) functions using \(\) so that they are Bellman consistent, i.e. they satisfy the policy version of Bellman equation. ```

**Algorithm 1** MF-GPC (Model-Free Gradient Perturbation Controller)We prove the following theorem for the case of linear dynamics:

**Theorem 4** (Informal Statement (see Theorem 8)).: _If the underlying dynamics are linear with the state evolution specified as_

\[_{t+1}=A_{t}+B_{t}+_{t},\]

_with \(d_{}=\{d_{x},d_{u}\}\), then then as long as the Pseudo-Disturbance signal \(}_{t}\) satisfies \(}_{t}=T_{t}\), for some (possibly unknown) invertible map \(T\), Algorithm 1 generates controls \(_{t}\) such that for any sequence of bounded (even adversarial) \(_{t}\) such that the following holds_

\[_{t}c_{t}(_{t},_{t})\ \ _{^{O_{AC}}} _{t}c_{t}(_{t}^{},_{t}^{})+}(d_{}}T^{3/4}),\]

_for any any sequence of convex costs \(c_{t}\), where the policy class \(DAC\) refers to all policies \(\) that produce a control as a linear function of \(_{t}\). Further, if the costs \(c_{t}\) are \(L\)-smooth, the regret for Algorithm 1 admits an improved upper bound of \(}((d_{u}d_{}T)^{2/3})\)._

In particular, the above theorem implies the stated regret bounds when the Pseudo-Disturbance is estimated as described in Equations 3 (Vector Value Function-based) and 4 (Simulator-based).

The regret bounds in Theorem 4 are strict improvements over state-of-the-art bounds in terms of dimension dependence; the latter operate with explicit descriptions of disturbances. This is achieved by using a better choice of gradient estimator, using exploration in action-space rather than parameter space. As a result, our bounds have no dependence on the state dimension since \(d_{}\ \ d_{u}\). As an instructive case, for high-dimensional underactuated systems, where \(d_{u}<d_{x}\), our regret bounds scale as \((d_{u}T^{3/4})\) in contrast to \((d_{u}d_{x}^{1/2}T^{3/4})\) for convex costs from [Gradu et al., 2020, Cassel and Koren, 2020], and as \((d_{u}^{4/3}T^{2/3})\) for smooth costs improving over \((d_{u}^{4/3}d_{x}^{2/3}T^{2/3})\) from [Cassel and Koren, 2020]. Note that the ratio by which we improve here can be unbounded, with larger improvements for high-dimensional (\(d_{x} 1\)) systems. See Appendix E.2 for further details, comparisons and proofs.

### Derivation of update

In the algorithm, the key component is computing an approximate policy gradient of the cost. A complete theoretical analysis of our algorithm can be found in Appendix E, but we provide a brief sketch of the gradient calculation. Let \(J_{t}(M)\) denote the expected counterfactual cost \(c_{t}\) of following policy \(M\) with the same observed disturbances \(w_{t}\). We first note that if the dynamics are suitably stabilized (which should be done by \(_{}\)), the state and cost can be approximated as a function \(C\) of a small window of previous controls.

\[J_{t}(M)=_{_{1:t}}[c_{t}(_{t}^{M},_{t} ^{M})]_{_{t-h:t}}[C(_{t}(M)+_{ t},,_{t-h}(M)+_{t-h})]\;,\]

where we use \(u_{t-i}(M)\) as a shorthand for \((}_{t-i-1:t-h-i}|M)\). The expression here is that of a Gaussian smoothed function, which allows us to get the following unbiased single point gradient estimate

\[_{_{i}}_{_{t-h:t}}[C(_{t}+ _{t},,_{t-h}+_{t-h})]=_{_{t-h:t}}[^{-1}C(_{t}+_{t},,_{t-h}+ _{t-h})_{i}]\;.\]

We use a single sample to get a stochastic gradient. Using the chain rule, which involves an outer product due to the tensor structure of \(M\), we get stochastic gradients with respect to \(M\) as follows

\[}J_{t}(M) C(_{t}(M)+_{t},, _{t-h}(M)+_{t-h})^{-1}_{i=0}^{h-1}_{t- i}}_{t-i-1:t-h-i}|M)}{ M}\;.\]

Finally, we note that \(M^{t}\) is slowly moving because of gradient descent, so we can approximate

\[c_{t}(_{t},_{t}) C(_{t}(M^{t})+_{t},,_{t-h}(M^{t})+_{t-h}).\]

Putting everything together, we have

\[}J_{t}(M)_{M=M^{t}}\ c_{t}(_{t}, _{t})^{-1}_{i=0}^{h-1}_{t-i}.}_{t-i-1:t-h-i}|M)}{ M}|_{M=M^{t}}.\] (6)

## 5 Experiments

We apply the MF-GPC Algorithm 1 to various OpenAI Gym (Brockman et al., 2016) environments. We conduct our experiments in the research-first modular framework Acme (Hoffman et al., 2020). We pick \(h=5\) and use the DDPG algorithm (Lillicrap et al., 2016) as our underlying baseline. We update the \(M\) matrices every 3 episodes instead of continuously to reduce runtime. We also apply weight decay to line 6 of Algorithm 1. Our implementation of PD1 is based on Equation 2. PD2 can be implemented with any vector of rewards. We choose linear function \(L\) given in Lemma 2 to be the identity function. Hence \(\) in Equation 3 reduces to the state \(x_{t}\) itself. We pick \(\) and \(\) network architectures to be the first \(d_{x}\) units of the last layer of the critic network architecture. We train for 1e7 steps as a default (this is also the default in the Acme code) and if performance has not converged we extend to 1.5e7 steps. Because the \(M\) matrices impact the exploration of the algorithm, we tune the exploration parameter \(\) for both DDPG and MF-GPC. For the baseline DDPG, we typically explore \(\{0.15,0.2,0.25\}\). More experimental details may be found in Appendix Section B.

Results for Noisy Hopper, Walker 2D, and AntWe create a noisy Hopper, Walker 2D, and Ant environments by adding a Uniform random variable \(U[-0.1,0.1]\) to the state. The noise is added at every step for both the DDPG baseline and our MF-GPC. We plot the results for PD2, and PD3 in Figure 1. We find that PD2 and PD3 perform relatively well in these settings. Graphs depicting all runs for different \(\) are available in Appendix Section B. MF-GPC is not guaranteed to improve performance in realistic RL settings. We find that generally PD1 does not perform well e.g. in Figure 2 a) and some examples where applying it yields performance similar to baseline are given in Appendix Section B. This is likely due to the high variance of the PD estimate. We find that neither our method nor the baseline is too sensitive to our hyper-parameter tuning (Figure 2 b) ), possibly because we start with the default Acme parameters which are already well tuned for the noiseless environment.

Linear Dynamical SystemsWe evaluate our methods on both low dimensional (\(d_{x}=2,d_{u}=1\)) and a higher dimensional (\(d_{x}=10,d_{u}=5\)) linear systems with sinusoidal disturbances to demonstrate the improvements in dimension of our method (labeled RBPC) over BPC (Gradu et al., 2020). We use the full information GPC (Agarwal et al., 2019) and LQR as baselines using implementations from Gradu et al. (2021). While performance is comparable to BPC on the small system, on the larger system, BPC could not be tuned to learn while RBPC improves upon the LQR

Figure 1: Episode return for best performing MF-GPC model versus best performing baseline DDPG model for various OpenAI Gym environments and pseudo-estimation methods. Environment and pseudo-estimation method shown in title. Results averaged over 25 seeds. Shaded areas represent confidence intervals. We find that PD2 and PD3 perform well in these settings.

baseline (see Figure 3). In both experiments, \(h=5\) and the learning rate and exploration noise is tuned.

## 6 Conclusion

We have described a new approach for model-free RL based on recent exciting advancements in model based online control. Instead of using state-based policies, online nonstochastic control proposes the use of disturbance-based policies. To create a disturbance signal without a model, we define three possible signals, called Pseudo-Disturbances, each with its own merits and limitations. We give a generic (adaptable) REINFORCE-based method using the PD signals with provable guarantees: if the underlying MDP is a linear dynamical system, we recover and improve the strong guarantees of online nonstochastic control. Preliminary promising experimental results are discussed. We believe this is a first step in the exciting direction of applying tried-and-tested model-based control techniques for general reinforcement learning.