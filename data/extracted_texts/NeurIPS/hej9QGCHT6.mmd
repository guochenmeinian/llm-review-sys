# DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception

Xiaotong Li\({}^{1,2,*}\), Fan Zhang\({}^{2*}\), Haiwen Diao\({}^{3,2*}\), Yueze Wang\({}^{2}\), Xinlong Wang\({}^{2}\), Ling-Yu Duan\({}^{1}\)

\({}^{1}\)Peking University \({}^{2}\)Beijing Academy of Artificial Intelligence (BAAI)

\({}^{3}\)Dalian University of Technology

Dataset: [https://huggingface.co/datasets/BAAI/DenseFusion-1M](https://huggingface.co/datasets/BAAI/DenseFusion-1M)

Equal contribution. \(\) Correspondence to _lingyu@pku.edu.cn, wangxinlong@baai.ac.cn._

###### Abstract

Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, and spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose _Perceptual Fusion_, using a low-budget but highly effective caption engine for complete and accurate image descriptions. Specifically, _Perceptual Fusion_ integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs' perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs. The dataset and code are publicly available at [https://github.com/baaivision/DenseFusion](https://github.com/baaivision/DenseFusion).

## 1 Introduction

Multimodal Large Language Models (MLLMs)  have made remarkable strides in multi-modal understanding and reasoning by aligning the Large Vision Models (LVMs)  and Large Language Models (LLMs) . To fully harness the capabilities of MLLMs in comprehensive visual perception, there is a critical demand for high-quality image-text datasets that provide dense and thorough descriptions across a wide range of visual elements. Such hyper-detailed datasets are essential for training MLLMs to accurately interpret and interact with diverse visual information. However, the scarcity of such rich datasets currently hampers the progress of the MLLM community. Given these challenges, it is crucial to pioneer a practical and efficient route to craft highly detailed image descriptions for comprehensive perception.

As the saying goes, "an image is worth a thousand words". Images contain various visual elements of different granularities that are essential yet challenging to harness. Employing human labor  or advanced GPT-4V  is one feasible option to generate accurate, reliable, andhigh-quality image descriptions. Nevertheless, this approach is expensive and limits the scalability of the resulting dataset. Alternative strategies concentrate on caption engines , generating relatively detailed annotations over the web-crawled text. However, we observe that they often neglect many important visual details and still fall short of providing fine-grained descriptions with all visual clues. For example, the remarkable ShareGPT4V , struggles to accurately recognize various visual elements in Figure 1. The bottleneck lies in the limited perception capability of current caption engines for grasping diverse visual semantic information, including text recognition, object attributes, localization, and external knowledge, which hinders the sufficient exploration of visual information.

To address this issue, we empirically discover that incorporating diverse vision experts can effectively mitigate the limitations of caption engines' perceptual abilities. The perception information from specialized visual models can be considered as intermediate understanding of images. Typically, specialized perception models  outperform generalized MLLMs  within their respective visual specializations, _e.g.,_ small object recognition for detection models. Therefore, utilizing these experts as strong assistants facilitates the perception process, enabling the efficient extraction of various visual elements for comprehensive image understanding. However, there remains less exploration into integrating their capabilities and diverse visual information to achieve well-rounded visual perception.

In this paper, we meticulously design a pipeline for comprehensive multimodal understanding, named _Perceptual Fusion_, integrating diverse vision experts as image priors and adopting a low-budget MLLM as a centric pivot for information fusion. Under this strategy, we exploit LAION , a valuable public resource, and delicately extract 1 million diverse and high-quality data. Firstly, we feed supplements from visual experts into the advanced GPT-4V and acquire 100K intricately detailed descriptions. With this meta dataset as guidance, we can efficiently develop a strong captioning engine capable of integrating strengths from multiple sources, including object detection, image tagging, text recognition experts for thoroughly comprehending image content. Leveraging this multimodal pivot, we can further construct a scalable, reliable, and high-quality pre-trained dataset, named DenseFusion-1M, enriched with abundant text information, accurate object and position recognition, and external knowledge. The hyper-detailed image-text data, in turn, enhances the perception of existing MLLMs to achieve better vision-language alignment.

In summary, our contributions are listed as follows:

Figure 1: Illustration of the highly informative image description from DenseFusion-1M dataset and comparisons with state-of-the-art caption engine . It showcases comprehensive image understanding and captures all detailed visual clues (such as visual elements 1-10 in the image). For better visualization, information about objects/attributes, spatial positions, text information, and knowledge/reasoning are marked in individual colors.

* To promote comprehensive visual perception, we introduce a perceptual fusion pipeline that leverages multi-source experts as image priors, establishing a low-budget yet powerful caption engine to comprehend image elements and generate well-crafted descriptions.
* Through our perception fusion strategy, we construct a large hyper-detailed image-text dataset, DenseFusion-1M with informative images and dense descriptions, including rich text information, multiple objects, attributes, spatial relations, world knowledge, etc.
* Based on our DenseFusion-1M, we validate that the trained MLLM demonstrates superior performance against existing state-of-the-art MLLMs across 10 vision-language benchmarks, especially for detailed text recognition and high-resolution image perception.

## 2 Related Work

**Large Multi-Modality Models:** The development of Large Multi-Modality Models (MLLMs) has witnessed significant advances in the abilities of comprehension and reasoning [32; 31; 12; 72; 39; 37; 9; 33; 13], typically through aligning pre-trained Large Vision Models (LVMs) [25; 54; 26; 68] with Large Language Models (LLMs) [10; 57; 46]. The pioneer works BLIP [32; 31], LLaVA [39; 37; 38], and Qwen series [39; 37] bridge the modality gaps through resamplers or MLP projectors, and obtain promising performances. Besides, Emu series [55; 53] exhibits strong in-context learning ability for multimodal content. Recently, there has been an emergent trend in developing high-resolution MLLMs [62; 38; 49; 21; 35; 63]. Among them, Monkey  resizes input images to fixed resolutions and divides them into multiple 448x448 patches, which are then processed by a pre-trained vision encoder. Moreover, CogAgent  utilizes low-resolution and high-resolution image encoders to recognize tiny visual elements inside a large image. LLaVA-NEXT , dubbed as LLaVA-1.6, introduces dynamic image aspect ratios and partitions the original images into multiple sub-images to capture more visual details, while LLaVA-UHD  divides images into smaller variable-sized slices for efficient and extensible encoding. Notably, Scaling on Scales \((S^{2})\) straightly extracts multi-scale features through image wrapping and rescaling without increasing image tokens. These high-resolution MLLMs capture tiny visual clues and benefit from meticulous image descriptions.

Figure 2: Examples of highly informative image descriptions from the DenseFusion-1M dataset, composed with various visual details and knowledge.

Hence, we aim to create hyper-detailed image annotations to enhance understanding of intricate visual elements and provide more accurate visual-language alignment.

**Image-Text Datasets:** Large-scale image-text datasets, e.g. LAION [45; 23], CC12M , Visual Genome  and YFCC , have effectively facilitate the development of vision-language pre-training. Along this line, BLIP-LAION  presents the synthetic short descriptions by the BLIP model, while LLaVA  and LLaVAR  prompt the text-only GPT-4 with visual information to generate conversations. Moreover, LaCLIP  rewrites the caption via ChatGPT through its in-context learning capability, while CapsFusion  leverages fine-tuned large language models to consolidate and refine information from both web-crawled and synthetic captions. To acquire detailed description datasets, recent studies seek help for the advanced GPT-4V model or human-in-the-loop strategy [8; 7; 60; 59; 18]. Among them, ShareGPT4V  comprises 100K captions from GPT-4V and employs an advanced captioner to produce an additional 1.2 million synthetic captions, while ALLaVA  directly leverages the advanced GPT4V's capabilities to create a synthetic dataset with detailed captions and instructional data. For region-level vision recognition, GLaMM  and all-seeing projects [60; 59] advance conversation generation with detailed region-level understanding and semantic tags. Lastly, ImageInWords (IIW)  presents 9k hyper-detailed captions through a human-in-the-loop annotation framework. DOCCI  instructs human annotators to create 19k comprehensive descriptions. Despite detailed visual annotations, their human-in-the-loop strategies require expensive labor costs and restrict the dataset scales. In contrast, we construct a low-budget caption engine empowered by diverse vision experts that can automatically generate large-scale and hyper-detailed image-text datasets at a negligible cost.

## 3 Methodology

In this section, we introduce the methodology design for constructing the dataset DenseFusion-1M. Specifically, we detail the data pre-processing pipeline for filtering high-quality image sources, the perceptual fusion procedure from vision experts, and the construction of the caption engine.

### Data Processing

Establishing a high-quality dataset for comprehensive perception necessitates access to a large-scale data resource that encompasses a wide range of image categories and rich visual semantics.

Unlike methods such as ShareGPT4V, which meticulously curate images from specialized sources including COCO, SAM, Textcaps, etc, we opt to the widely-used LAION-2B  dataset, which naturally sources its diverse content directly from the wild internet, including different image categories like photos, posters, powerpoint, infographics, and more. Moreover, the LAION open-source dataset supports further academic research by offering readily accessible data that has been re-annotated by various studies [14; 8; 66; 23].

Despite its massive scale, LAION is still uncurated and contains significant issues about duplication , hindering both image diversity and quality. To address this, we mainly focus on two critical factors for data processing. Firstly, higher resolution images are prioritized since they generally provide richer visual content and more abundant semantics. Secondly, we emphasize the selection of representative images to preserve a greater diversity of visual content within the same data scale.

* **High Resolution Image Selection.** Images with a short-edge resolution less than 448 are filtered out to ensure the richness of the image content. Following this approach, approximately 500M images are retained from the initial 2B images, resulting in the subset named DenseFusion-500M.
* **Semantic Clustering and De-duplication.** To maximize the diversity of image distribution, we follow SemDeDup to remove semantically duplicated images from DenseFusion-500M. Specifically, we employ k-means clustering on images features extracted via EVA-CLIP  to create 50,000 clusters. We set the threshold \(=0.4\) to remove semantic duplicated images within each cluster, yielding an image set of 14 million images. Finally, we select the top 20 images from each cluster, which are closest to the cluster centers in the clustering process, to create our DenseFusion-1M from the deduplicated subset.

### Perceptual Fusion

Comprehensive visual perception is a prerequisite for multimodal understanding and reasoning. This perception ability can be achieved through extensive, detailed, and accurate alignments in image-text pre-training data. Despite the feasibility of current MLLMs [39; 32] for image captioning, they still struggle to provide meticulous descriptions. (1) Generalist MLLMs are designed for executing various instructions and are not intended for specific captioning tasks, especially for well-rounded image captioning. (2) Existing specialist caption engines lack a strong ability for comprehending and describing various visual elements inside high-resolution images, due to their inherent drawbacks in identifying all kinds of visual characteristics.

#### 3.2.1 Mixture of Visual Experts

With the advancements in computer vision, numerous visual experts of various perceptual tasks have emerged and demonstrate outstanding capabilities within their respective domains [42; 15; 70; 22]. These models provide valuable intermediate perceptual information for image understanding. Therefore, comprehensively understanding the diverse visual elements in complex scenes can benefit from the collaboration of different specialists. In this section, we develop a perceptual fusion strategy with assistance from a variety of vision experts.

This approach specifically targets areas where generalist MLLMs often show limited perceptual capabilities. Our strategy includes the application of expert techniques in image tagging, object detection, text recognition, and the incorporation of world knowledge. We meticulously select these vision experts based on several key aspects of perception, which are detailed as follows.

* **Image Tagging**: Initially, we attempt to produce scene-level understanding for holistic images, including objects and visual scenes. Specifically, we employ the pre-trained RAM++  that generates expansive tag descriptions over conventionally predefined tag categories. This approach enriches visual tag information and provides accurate scene annotations in overall image understanding, enhancing the recognition of diverse open-vocabulary concepts for object understanding.
* **Object Detection**: Comprehensive understanding relies on the perception ability of various object entities, while current MLLMs suffer from incomplete object perception and inaccurate positioning. Therefore, we utilize two types of specialized detection models to boost hyper-detailed recognition. (1) We employ the closed-set EVA02  detection model trained on LVIS  and COCO 

Figure 3: Pipeline of _Perceptual Fusion_ to acquire DenseFusion-1M, which comprises 1 million hyper-detailed image descriptions. This pipeline leverages various visual experts as image priors and employs a multimodal model as the central pivot for integrating multi-source information. Its capability is learned from a 100K meta dataset generated by advanced GPT-4V.

to precisely detect the objects with basis concepts and varying sizes. (2) Meanwhile, we employ the open-set OWL-ViTv2  detection model for capturing objects across broader categories constructed from the tagging classes. Afterward, we retain the objects with high confidence over the predefined threshold, and adopt a balanced sampling strategy to highlight small-scale objects, considering that the generalist MLLMs tend to focus on large-scale objects.
* **Text Recognition:** Text information is crucial for visual understanding, especially for documents, such as documents, posters, tables, and charts. However, generalist MLLMs often overlook some text elements and fail to identify text with various font styles and scales accurately. Meanwhile, we find that it is over 70% of the resulting images contain text information according to our statistics. Therefore, we employ OCR (Optical Character Recognition) models  to recognize all textual elements within each image, even those with vague text information.
* **World Knowledge**: Although LAION's short captions crawled from the internet sometimes misalign with image descriptions, they contain a wealth of world knowledge, including visual context, background information, and subtle details, etc. This can help boost the MLLMs' knowledge density and enhance the reasoning abilities. By incorporating these noisy yet rich captions, the models can achieve a deeper, more nuanced understanding of visual content, improving their performance in tasks requiring comprehensive visual and contextual understanding.

Here, we simultaneously integrate the image tags, objects, textual information, and external knowledge through the above vision experts . Through their powerful assistance, we facilitate the adaptive and meticulous perception capabilities of generalist MLLMs.

#### 3.2.2 Perceptual Fusion Engine

To obtain precise and comprehensive image descriptions, the widely-used advanced GPT-4V  serves as an ideal MLLM with strong visual perception and contextual understanding capabilities. It can generate image descriptions that are further enriched with various visual information from specialized vision experts. Considering its expensive cost of time and finance, we attempt to construct an open-sourced and low-budget caption engine to efficiently mimic its ability for large-scale image captioning. We empirically discover that the perception ability of existing open-sourced caption engine can be enhanced with the assistance of additional visual experts, where they can improve the recognition of small-scale objects and OCR information, guiding our caption engine to focus on often overlooked content and correcting inaccuracies caused by its limited visual perception.

Initially, we adopt the proficient GPT-4V via manual-tuning prompts to generate image captions with extra visual information as the perceptual fusion guidance. The detailed prompt template can be found in Appendix. We thereby obtain 100K hyper-detailed image descriptions, i.e. DenseFusion-4V-100K. Using this meta dataset as guidance, we train our caption engine to learn from GPT-4V's characteristics and generate highly detailed image descriptions, as depicted in Figure 3. Our caption engine is based on LLaVA-1.6 (7B) , utilizing high-resolution images as inputs to ensure better visibility of detailed visual clues. The expertise of visual specialists are extracted offline and adopted as contextual information for caption engine. This process allows our engine to capture various visual clues effectively, enhancing its perception abilities by incorporating insights from vision experts. Consequently, it accurately identifies a wide range of objects and detailed textual information, resulting in image annotations with high information density.

### Dataset Description

Utilizing the perceptual fusion pipeline, we incorporate insights from multiple visual experts into producing hyper-detailed image descriptions, resulting in the following datasets: (1) **DenseFusion-4V-100K.** GPT-4V generated 100K captions. (2) **DenseFusion-1M.** Scaling up to 1 million detailed captions by our caption engine. We conducted a statistical analysis to show the detailed dataset information in Table 1. On average, the captions are 190 words long and consist of 11 sentences with dense descriptions. As shown in the category distribution in Figure 4(b), the **DenseFusion** dataset contains diverse categories such as photos, visual art, commercial design, and infographics, making it a valuable resource with various image types. We employ LLaVA-1.5  as a generalist MLLM for the category classification task. Generating hyper-detailed captions is fundamental to various multi-modal research tasks, as it facilitates the translation of images into language seamlessly. This capability presents significant potential in applications, _e.g.,_ vision-language contrastive pre-training , multimodal alignment in MLLMs , and text-conditioned image generation .

## 4 Experiments

In this section, we introduce the implementation details and compare the model trained by our DenseFusion-1M dataset with state-of-the-art MLLMs across diverse vision-language benchmarks. Finally, we validate the effectiveness of perception fusion qualitatively and quantitatively.

### Implementation Details

**Caption Engine.** To explore the detailed visual clues inside each image, we adopt LLaVA-1.6 (7B)  to handle the high-resolution image inputs. For the meta dataset, we utilize GPT-4V to annotate the randomly selected 100K images from our picked 1M LAION data, thereby boosting our engine supported by various experts and producing high-quality annotations to mimic advanced GPT-4V. This supervised fine-tuning stage takes around \(\) 5.5 hours on 4 nodes of 8\(\)A100 (40G) for 2 epochs. The visual knowledge from diverse visual experts are extracted and integrated as contextual information for the perception fusion prompt. Then we utilize the caption engine with the efficient deployment tool SGLang  to generate 1M data with enhanced multimodal perception.

**Evaluation Benchmarks.** To verify the efficacy of the provided DenseFusion-1M, we adopt these captions during the pre-training stage and follow the setup of LLaVA-1.5  on various visual question answering (VQA) and multi-modality understanding benchmarks for evaluation, such as ScienceQA , TextQA , VQAv2 , GQA , SEED , MMBench , MME , POPE , MM-Vet , that covers a wide range dimensions for evaluating model abilities. The metric in Table 2 reflects the individual scores for each benchmark, typically represented as the percentage (%) of correct answers across all questions.

**Model Configuration.** To verify the effectiveness of DenseFusion-1M, we adopt it in the pre-training stage for vision-language alignment. The model is based on LLaVA-1.5 , using the vision encoder CLIP-ViT-L/14-336  and the large language model (LLM) Vicuna  respectively. The vision encoder and LLM are connected by a two-layer multi-layer perception (MLP) projector. We utilize the approach of \(S^{2}\) for training the high-resolution MLLM, which is efficient in handling high-resolution inputs without increasing image tokens. We follow LLaVA-1.5  that comprises a two-stage training stages. **(a) Pre-training Stage**. We first only train the projector for pre-alignment, then we conduct pre-training with a trainable vision encoder of the last 12 layers to further improve the perception ability. **(b) Instruction-tuning Stage.** For fair comparison, we follow LLaVA-1.5

   Dataset Name & Caption & Samples & Char. & Word & Sen. & Nouns & Adj. & Adv. & Verb. & Num. \\  DenseFusion-4V-100K & GPI-4V & 100K & 1253 & 206 & 11.2 & 27.9\% & 10.9\% & 1.8\% & 12.0\% & 0.83\% \\ DenseFusion-1M & Ours & 1059K & 1130 & 191 & 11.0 & 28.0\% & 10.6\% & 1.4\% & 12.0\% & 0.85\% \\   

Table 1: Statistical information on DenseFusion-1M: (a) average number of characters, words, and sentences per caption, and (b) the lexical composition of the captions.

Figure 4: DenseFusion-1M dataset description. We obtain 1 million DenseFusion images after pre-processing. Figure (a) demonstrates the individual samples of classes and Figure (b) displays the category distribution, highlighting diverse images with rich semantics.

 and adopt the original LLaVA-mix-665K for instruction tuning, including GPT-generated and academic-oriented datasets. The detailed training recipe is shown in supplementary material.

### Main Results

**Compared Models.** We report the experiment results against current state-of-the-art MLLMs, including Qwen-VL , InstructBLIP , mPLUG-Owl2 , InternVL , LLaVA-1.5 . In particular, we compare our strategies with existing caption datasets or engines, e.g. ShareGPT4V , LVIS-4V . To fully exploit its potential, we conduct comparisons under high-resolution settings with recent MLLMs, including Monkey , LLaVA-1.6 , and Scaling on Scales  (S\({}^{2}\)).

**Experiment Results.** (1) Conventionally, Table 2 demonstrates that our meticulous descriptions significantly improve baseline models, providing solid and consistent benefits across all vision-language benchmarks, particularly for text-recognition scenes, e.g. TextVQA. Notably, our dataset originates from the generic LAION, which has no direct connection to the validation domains. Despite this, our strategies outperform ShareGPT4V, which uses images from COCO and VG that share a similar image distribution with the evaluation benchmarks, like VQAv2 and GQA. (2) Additionally, we observe that the potential benefits of our dataset are not fully exploited due to limited input resolutions, making MLLMs challenging to extract hyper-detailed image clues. To address this, we conduct further experiments using the high-resolution MLLM, Scaling on Scales  (S\({}^{2}\)), which performs multi-scale aggregation on high-resolution inputs without increasing the number of image tokens. Even with a fifth of visual tokens of LLaVA-1.6 and requires no additional instruction tuning data, LLaVA-S\({}^{2}\) trained by our data achieves better performance than the state-of-the-art LLaVA-1.6 and exhibits higher forward efficiency. Besides, we reproduce LLaVA-S\({}^{2}\) using 1.2M pre-training data from ShareGPT4V , named ShareGPT4V-S\({}^{2}\), and we do not introduce additional supervised fine-tuning data for fair comparisons. Our dataset shows further gains compared to the low-resolution version, demonstrating our superiority in scenarios requiring hyper-detailed visual elements.

From the above results, we observe that (1) a high-quality image-text dataset is crucial during pre-training to enhance alignment across modalities before learning specific instruction patterns; (2) meticulous and accurate image descriptions are essential for high-resolution vision perception. Low-resolution MLLMs easily reach saturation due to blurred visuals and difficulty in exploring detailed clues. Therefore, meticulous image annotation is a promising direction for enhancing the hyper-detailed perception and reasoning capabilities of multimodal models.

### Ablation Study

**Perceptual Fusion.** Generalist MLLMs occasionally exhibit inherent drawbacks in comprehensive perception, _e.g.,_ omitting objects and weak in text recognition. For time saving, we performed

  Method &  & ^{T}\)} & \(^{T2}\) & GQA & \(^{T}\) &  &  & ^{T}\)} & \)} & \)} &  \\  _Low-resolution Multimodal Large Language Module_ &  & & & & & & & & & & & \\  InstructBLIP & Vicuna-7B & 60.5 & - & 49.2 & 34.5 & - & 36.0 & - & 53.4 & - & 26.2 \\ QwenVL & 67.1 & 78.8 & - & 35.2 & - & 38.2 & - & 56.3 & - & - & 56.3 & - \\ QwenVL-Chat & 60.7 & 72.8 & 72.5 & 57.5 & 61.5 & 1487 & 60.6 & - & 58.2 & - & - \\ mPLUG-Owl2 & LLaMA-27B & 68.7 & 79.4 & 56.1 & 58.2 & 1450 & 64.5 & - & 57.8 & - & 36.5 \\ InternVL-Chat & Vicuna-7B & - & 79.3 & 62.9 & 57.0 & 1525 & - & - & 86.4 & - & 31.5 \\ LVIS-4V & Vicuna-7B & 68.3 & 79.6 & 62.6 & 58.7 & 1528 & 66.2 & - & 60.6 & - & 31.5 \\ ShareGPT4V & Vicuna-7B & 68.4 & 80.6 & 63.3 & 60.4 & 1567 & 68.8 & 69.7 & 61.9 & 85.7 & 37.6 \\ LLaVA-1.5 & Vicuna-7B & 66.8 & 78.5 & 62.0 & 58.2 & 1510 & 64.3 & 66.2 & 58.6 & 85.9 & 30.5 \\ LLaVA-1.5 & Vicuna-7B & 69.3 & 80.8 & 64.0 & 62.0 & 1574 & 69.2 & 70.1 & 62.3 & 86.5 & 37.8 \\ LLaVA-1.5 & LLaMA-38B & 72.3 & 79.7 & 63.8 & 58.7 & 1553 & 72.8 & 69.2 & 61.8 & 85.0 & 34.9 \\ LLaVA-1.5 (Ours) & LLaMA-38B & 72.9 & 80.4 & 64.4 & 61.0 & 1560 & 73.4 & 71.6 & 63.7 & 85.3 & 40.0 \\ LLaVA-1.5 & Qwen-7B & 72.3 & 79.8 & 63.4 & 57.0 & 1566 & 72.9 & 70.0 & 62.5 & 85.7 & 35.8 \\ LLaVA-1.5 (Ours) & Qwen-7B & 73.5 & 80.5 & 64.0 & 58.9 & 1528 & 73.5 & 71.6 & 63.6 & 86.0 & 41.4 \\  _High-resolution Multimodal Large Language Module_ &  & & & & & & & & & & \\  Monkey & 69.4 & 80.3 & 60.7 & - & - & - & - & - & - & - & - \\ LLaVA-1.6 & Vicuna-7B & 70.1 & 81.8 & 64.2 & 64.9 & 1519 & 67.4 & 70.2 & - & 86.5 & 43.9 \\ ShareGPT4V-S\({}^{2}\) & Vicuna-7B & 69.7 & 81.5 & 63.8 & 64.4 & 1547 & 68.0 & 70.1 & 62.4 & 86.7 & 35.0 \\ LLaVA-S\({}^{2}\) & Vicuna-7B & 68.2 & 79.7 & 63.3 & 60.8 & 1520 & 66.4 & 67.2 & 59.9 & 86.7 & 34.6 \\ LLaVA-S\({}^{2}\) (Ours) & Vicuna-7B & 72.1 & 81.6 & 65.3 & 67.4 & 1551 & 70.7 & 71.1 & 63.3 & 87.2 & 37.5 \\  

Table 2: Comparisons with state-of-the-art approaches on 10 vision-language evaluation benchmarks, including SQA\({}^{I}\): ScienceQA-IMG , VQA\({}^{v2}\): VQA-v2, GQA , MME , POPE , VQA\({}^{T}\): TextVQA , MMB: MMBench , SEED , MM-Vet . DenseFusion-1M is adopted in the pre-training stage for alignment, bringing significant and consistent improvements.

the ablation study using a subset of 100K data points from DenseFusion-1M as default setting. It is observed in Table 3, our strategy can effectively alleviate these issues, bringing substantial improvements on different benchmarks, especially in TextVQA with rich OCR information. We note that the relative improvement for high-resolution MLLMs becomes more emphasized, indicating that these MLLMs can benefit more from the visual details.

**Vision Encoder.** As demonstrated by previous studies , unfreezing the vision encoder benefits from high-quality image-text alignment data. We verify the effectiveness of different training configurations: frozen vision encoder, half fine-tuning (last 12 layers), and full fine-tuning. Notably, fine-tuning improves performance, but full fine-tuning does not significantly outperform half fine-tuning. Therefore, we follow ShareGPT4V's approach of tuning the last 12 layers for fair comparisons.

**Visual Analysis.** We conduct the Visual analysis on specific contribution of visual experts for final description in Figure 6. Besides, We demonstrate caption examples from our perception fusion caption engine and the generalist MLLM LLaVA-1.6 7B  in Fig.5. Specially, the detected objects help the MLLM focus on individual objects, generating descriptions with more details and attributes. This integrated information allows the caption engine to achieve comprehensive image understanding for hyper-detailed captions. Note that even when not all additional information is provided, our caption engine can still focus on producing comprehensive captions, showcasing its robustness. More visualizations are included in the supplementary materials.

**Data Efficiency.** We conduct the experiment to verify the data efficiency of our high-quality image-text pairs across varying training samples. The experiment performances (%) demonstrate our superiority improvements than ShareGPT4V for equivalent data scale. This advantage becomes particularly significant with high-resolution inputs. The experiment indicates that the quality of detailed descriptions and input resolution significantly impact training effectiveness and hyper-detailed captions. As a result, the high-quality image-text data result in a more efficient training manner under the same data scale.

   Model & MMB & SEED & VQA\({}^{T}\) & SQA\({}^{I}\) \\  LLaVA-1.5 & 64.3 & 58.6 & 58.2 & 66.8 \\  Frozen & 65.7 & 59.8 & 59.6 & 68.8 \\ Half-tuning & 67.0 & 60.8 & 60.8 & 68.9 \\ Full-tuning & 67.3 & 60.9 & 61.0 & 67.2 \\   

Table 4: Different number of trainable layers of vision encoders.

Figure 5: Visualization of perceptual fusion for enhanced image descriptions, which illustrates that the caption engine leverages additional information _e.g.,_ text, object recognition, and world knowledge to produce detailed and comprehensive descriptions.

   Model & MMB & SEED & VQA\({}^{T}\) & SQA\({}^{I}\) \\  Ours w/o fusion & 66.3 & 60.3 & 59.9 & 68.3 \\ Ours w fusion & 67.0 & 60.8 & 60.8 & 68.9 \\  Ours (S\({}^{2}\)) w/o fusion & 66.9 & 60.8 & 61.7 & 68.7 \\ Ours (S\({}^{2}\)) w fusion & 68.2 & 61.4 & 63.0 & 69.4 \\   

Table 3: The effect of perceptual fusion with different resolution MLLMs.

## 5 Conclusion

In this paper, we tackle the challenge of limited high-quality image-text data by developing a low-budget caption engine for high-resolution images and hyper-detailed captions. Our strategy involves curating a dataset from the LAION-2B corpus, followed by a perceptual fusion pipeline that guides a multimodal model to integrate information from various vision experts and thereby yields one million well-rounded descriptions, dubbed DenseFusion-1M. We believe that such an extensive image-text dataset, characterized by its hyper-detailed nature, would substantially enhance the capabilities of MLLMs by enabling more effective alignment between visual and textual data.

Figure 6: Visualization on specific contribution of world knowledge, text recognition, and detection model to produce detailed and comprehensive descriptions.

Acknowledgement

This work was supported by the Program of Beijing Municipal Science and Technology Commission Foundation (No.Z241100003524010), in part by the National Natural Science Foundation of China under Grant 62088102 and the National Key R&D Program of China (2022ZD0116302), in part by AI Joint Lab of Future Urban Infrastructure sponsored by Fuzhou Chengtou New Infrastructure Group and Boyun Vision Co. Ltd, and in part by the PKU-NTU Joint Research Institute (JRI) sponsored by a donation from the Ng Teng Fong Charitable Foundation.