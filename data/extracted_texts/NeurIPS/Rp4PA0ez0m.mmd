# Unsupervised Video Domain Adaptation for Action Recognition: A Disentanglement Perspective

Pengfei Wei

AI Lab, ByteDance

pengfei.wei@bytedance.com &Lingdong Kong

National University of Singapore

lingdong@comp.nus.edu.sg &Xinghua Qu

AI Lab, ByteDance

quxinghua17@gmail.com &Yi Ren

AI Lab, ByteDance

ren.yi@bytedance.com &Zhiqiang Xu

MBZUAI

zhiqiang.xu@mbzuai.ac.ae &Jing Jiang

University of Technology Sydney

Jing.Jiang@uts.edu.au &Xiang Yin

AI Lab, ByteDance

yinxiang.stephen@bytedance.com

###### Abstract

Unsupervised video domain adaptation is a practical yet challenging task. In this work, for the first time, we tackle it from a disentanglement view. Our key idea is to handle the spatial and temporal domain divergence separately through disentanglement. Specifically, we consider the generation of cross-domain videos from two sets of latent factors, one encoding the static information and another encoding the dynamic information. A _**Transfer Sequential VAE** (_TranSVAE_) framework is then developed to model such generation. To better serve for adaptation, we propose several objectives to constrain the latent factors. With these constraints, the spatial divergence can be readily removed by disentangling the static domain-specific information out, and the temporal divergence is further reduced from both frame- and video-levels through adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and Epic-Kitchens datasets verify the effectiveness and superiority of _TranSVAE_ compared with several state-of-the-art approaches.1

## 1 Introduction

Over the past decades, unsupervised domain adaptation (UDA) has attracted extensive research attention . Numerous UDA methods have been proposed and successfully applied to various real-world applications, _e.g._, object recognition , semantic segmentation , and object detection . However, most of these methods and their applications are limited to the image domain, while much less attention has been devoted to video-based UDA, where the latter is undoubtedly more challenging.

Compared with image-based UDA, the source and target domains also differ temporally in video-based UDA. Images are spatially well-structured data, while videos are sequences of images with both spatial and temporal relations. Existing image-based UDA methods can hardly achieve satisfactory performance on video-based UDA tasks as they fail to consider the temporal dependency of video frames in handling the domain gaps. For instance, in video-based cross-domain action recognition tasks, domain gaps are presented by not only the actions of different persons in different scenarios but also the actions that appear at different timestamps or last at different time lengths.

Recently, few works have been proposed for video-based UDA. The key idea is to achieve _domain alignment_ by aligning both frame- and video-level features through adversarial learning , contrastive learning , attention , or combination of these mechanisms, _e.g._, adversarial learning with attention . Though they have advanced video-based UDA, there is still room for improvement. Generally, existing methods follow an _all-in-one_ way, where both spatial and temporal domain divergence are handled together, for adaptation (Fig. 1, \(\)). However, cross-domain videos are highly complex data containing diverse mixed-up information, _e.g._, domain, semantic, and temporal information, which makes the simultaneous elimination of spatial and temporal divergence insufficient. This motivates us to handle the video-based UDA from a _disentanglement_ perspective (Fig. 1, \(\)) so that the spatial and temporal divergence can be well handled separately.

To achieve this goal, we first consider the _generation_ process of cross-domain videos, as shown in Fig. 2, where a video is generated from two sets of latent factors: one set consists of a sequence of random variables, which are _dynamic_ and incline to encode the semantic information for downstream tasks, _e.g._, action recognition; another set is _static_ and introduces some domain-related spatial information to the generated video, _e.g._, style or appearance. Specifically, the blue / red nodes are the observed source / target videos \(^{}\) / \(^{}\), respectively, over \(t\) timestamps. Static latent variables \(_{d}^{}\) and \(_{d}^{}\) follow a joint distribution and combining either of them with dynamic latent variables \(_{t}\) constructs one video data of a domain.

With the above generative model, we develop a _Transfer **S**equential **V**ariational **A**uto**Encoder (_TranSVAE_) for video-based UDA. _TranSVAE_ handles the cross-domain divergence in two levels, where the first level removes the spatial divergence by disentangling \(_{d}\) from \(_{t}\); while the second level eliminates the temporal divergence of \(_{t}\). To achieve this, we leverage appropriate constraints to ensure that the disentanglement indeed serves the adaptation purpose. Firstly, we enable a good decoupling of the two sets of latent factors by minimizing their _mutual dependence_. This encourages these two latent factor sets to be mutually independent. We then consider constraining each latent factor set. For \(_{d}^{}\) with \(\{,\}\), we propose a _contrastive triplet loss_ to make them static and domain-specific. This makes us readily handle spatial divergence by disentangling \(_{d}^{}\) out. For \(_{t}\), we propose to align them across domains at both frame and video levels through _adversarial learning_ so as to further eliminate the temporal divergence. Meanwhile, as downstream tasks use \(_{t}\) as input, we also add the _task-specific supervision_ on \(_{t}\) extracted from source data (w/ ground-truth).

To the best of our knowledge, this is the first work that tackles the challenging video-based UDA from a domain disentanglement view. We conduct extensive experiments on popular benchmarks (UCF-HMDB, Jester, Epic-Kitchens) and the results show that _TranSVAE_ consistently outperforms previous state-of-the-art methods by large margins. We also conduct comprehensive ablation studies

Figure 1: Conceptual comparisons between the traditional _all-in-one view_ and the proposed _disentanglement view_. Prior works often seek to compress implicit domain information to obtain domain-indistinguishable representations; while in this work, we pursue explicit decouplings of domain-specific information from other information via generative modeling.

Figure 2: Graphical illustrations of the proposed _generative_ (1st figure) and _inference_ (2nd figure) models for video domain disentanglement.

and disentanglement analyses to verify the effectiveness of the latent factor decoupling. The main contribution of the paper is summarized as follows:

* We provide a generative perspective on solving video-based UDA problems. We develop a generative graphical model for the cross-domain video generation process and propose to utilize the sequential VAE as the base generative model.
* Based on the above generative view, we propose a _TranSVAE_ framework for video-based UDA. By developing four constraints on the latent factors to enable disentanglement to benefit adaptation, the proposed framework is capable of handling the cross-domain divergence from both spatial and temporal levels.
* We conduct extensive experiments on several benchmark datasets to verify the effectiveness of _TranSVAE_. A comprehensive ablation study also demonstrates the positive effect of each loss term on video domain adaptation.

## 2 Related Work

**Unsupervised Video Domain Adaptation**. Despite the great progress in image-based UDA, only a few methods have recently attempted video-based UDA. In , a temporal attentive adversarial adaptation network (TA\({}^{3}\)N) is proposed to integrate a temporal relation module for temporal alignment. Choi _et al._ proposed a SAVA method using self-supervised clip order prediction and clip attention-based alignment. Based on a cross-domain co-attention mechanism, the temporal co-attention network TCoN  focused on common key frames across domains for better alignment. Luo _et al._ pay more attention to the domain-agnostic classifier by using a network topology of the bipartite graph to model the cross-domain correlations. Instead of using adversarial learning, Sahoo _et al._ developed an end-to-end temporal contrastive learning framework named CoMix with background mixing and target pseudo-labels. Recently, Chen _et al._ learned multiple domain discriminators for multi-level temporal attentive features to achieve better alignment, while Turrisi _et al._ exploited two-headed deep architecture to learn a more robust target classifier by the combination of cross-entropy and contrastive losses. Although these approaches have advanced video-based UDA tasks, they all adopted to align features with diverse information mixed up from a compression perspective, which leaves room for further improvements.

**Multi-Modal Video Adaptation**. Most recently, there are also a few works integrating multiple modality data for video-based UDA. Although we only use the single modality RGB features, we still discuss this multi-modal research line for a complete literature review. The very first work exploring the multi-modal nature of videos for UDA is MM-SADA , where the correspondence of multiple modalities was exploited as a self-supervised alignment in addition to adversarial alignment. A later work, spatial-temporal contrastive domain adaptation (STCDA) , utilized a video-based contrastive alignment as the multi-modal domain metric to measure the video-level discrepancy across domains.  proposed cross-modal and cross-domain contrastive losses to handle feature spaces across modalities and domains.  leveraged both cross-modal complementary and cross-modal consensus to learn the most transferable features through a CIA framework. In , the authors proposed to generate noisy pseudo-labels for the target domain data using the source pre-trained model and select the clean samples in order to increase the quality of the pseudo-labels. Lin _et al._ developed a cycle-based approach that alternates between spatial and spatiotemporal learning with knowledge transfer. Generally, all the above methods utilize the flow as the auxiliary modality input. Recently, there are also methods exploring other modalities, for instance, A3R  with audios and MixDANN  with wild data. It is worth noting that the proposed _TranSVAE_ - although only uses single modality RGB features - surprisingly achieves better UDA performance compared with most current state-of-the-art multi-modal methods, which highlights our superiority.

**Disentanglement**. Feature disentanglement is a wide and hot research topic. We only focus on works that are closely related to ours. In the image domain, some works consider adaptation from a generative view.  learned a disentangled semantic representation across domains. A similar idea is then applied to graph domain adaptation  and domain generalization .  proposed a novel informative feature disentanglement, equipped with the adversarial network or the metric discrepancy model. Another disentanglement-related topic is sequential data generation. To generate videos, existing works [22; 50; 1] extended VAE to a recurrent form with different recursive structures. Inthis paper, we present a VAE-based structure to generate cross-domain videos. We aim at tackling video-based UDA from a new perspective: sequential domain disentanglement and transfer.

## 3 Technical Approach

Formally, for a typical video-based UDA problem, we have a source domain \(\) and a target domain \(\). Domain \(\) contains sufficient labeled data, denoted as \(\{(_{i}^{},y_{i}^{})\}_{i=1}^{N_{S}}\), where \(_{i}^{}\) is a video sequence and \(y_{i}^{}\) is the class label. Domain \(\) consists of unlabeled data, denoted as \(\{_{i}^{}\}_{i=1}^{N_{T}}\). For a sequence \(_{i}^{}\) from domain \(\{,\}\), it contains \(T\) frames \(\{_{i,-1}^{},...,_{i,T}^{}\}\) in total2. We further denote \(N=N^{}+N^{}\). Domains \(\) and \(\) are of different distributions but share the same label space. The objective is to utilize both \(\{(_{i}^{},y_{i}^{})\}_{i=1}^{N_{S}}\) and \(\{_{i}^{}\}_{i=1}^{N_{T}}\) to train a good classifier for domain \(\). We present a table to list the notations in the Appendix.

**Framework Overview**. We adopt a VAE-based structure to model the cross-domain video generation process and propose to regulate the two sets of latent factors for adaptation purposes. The reconstruction process is based on the two sets of latent factors, _i.e._, \(_{1}^{},...,_{T}^{}\) and \(_{d}^{}\) that are sampled from the posteriors \(q(_{t}^{}|_{<t}^{})\) and \(q(_{d}^{}|_{1:T}^{})\), respectively. The overall architecture consists of five segments including the encoder, LSTM, latent spaces, sampling, and decoder, as shown in Fig. 3. The vanilla structure only gives an arbitrary disentanglement of latent factors. To make the disentanglement facilitate adaptation, we carefully constrain the latent factors as follows.

**Video Sequence Reconstruction**. The overall architecture of _TranSVAE_ follows a VAE-based structure [22; 50; 1] with two sets of latent factors \(_{1}^{},...,_{T}^{}\) and \(_{d}^{}\). The generative and inference graphical models are presented in Fig. 2. Similar to the conventional VAE, we use a standard Gaussian distribution for static latent factors. For dynamic ones, we use a sequential prior \(_{t}^{}|_{<t}^{}( _{t},diag(_{t}^{2}))\), that is, the prior distribution of the current dynamic factor is conditioned on the historical dynamic factors. The distribution parameters can be re-parameterized as a recurrent network, _e.g._, LSTM, with all previous dynamic latent factors as the input. Denoting \(^{}=\{_{1}^{},...,_{T}^{ }\}\) for simplification, we then get the prior as follows:

\[p(_{d}^{},^{})=p(_{d}^{ })_{t=1}^{T}p(_{t}^{}|_{<t}^{ }).\] (1)

Figure 3: _TranSVAE_ overview. The input videos are fed into an encoder to extract the visual features, followed by an LSTM to explore the temporal information. Two groups of _mean_ and _variance_ networks are then applied to model the posterior of the latent factors, _i.e._, \(q(_{t}^{}|_{<t}^{})\) and \(q(_{d}^{}|_{1:T}^{})\). The new representations \(_{1}^{},...,_{T}^{}\) and \(_{d}^{}\) are sampled, and then concatenated and passed to a decoder for reconstruction. Four constraints are proposed to regulate the latent factors for adaptation.

Following Fig. 2 (the 1st subfigure), \(_{t}^{}\) is generated from \(_{d}^{}\) and \(_{t}^{}\), and we thus model \(p(_{t}^{}|_{d}^{},_{t}^{ })=(_{t}^{},diag(_{t}^{\,2}))\). The distribution parameters are re-parameterized by the decoder which can be flexible networks like the deconvolutional neural network. Using \(^{}=\{_{1}^{},...,_{T}^{ }\}\), the _generation_ can be formulated as follows:

\[p(^{})=p(_{d}^{})_{t=1}^{T}p( _{t}^{}|_{d}^{},_{t}^{ })p(_{t}^{}|_{<t}^{}).\] (2)

Following Fig. 2 (the 2nd subfigure), we model the posterior distributions of the latent factors as another two Gaussian distributions, _i.e._, \(q(_{d}^{}|^{})=( _{d},diag(_{d}^{\,2}))\) and \(q(_{t}^{}|_{<t}^{})=( _{t}^{},diag(_{t}^{\,2}))\). The parameters of these two distributions are re-parameterized by the encoder, which can be a convolutional or LSTM module. However, the network of the static latent factors uses the whole sequence as the input while that of the dynamic latent factors only uses previous frames. Then the _inference_ can be factorized as:

\[q(_{d}^{},^{}|^{})=q(_{d}^{}|^{})_{t=1}^{T}q( _{t}^{}|_{<t}^{}).\] (3)

Combining the above generation and inference, we obtain the VAE-related objective function as:

\[_{}=_{q(_{d}^{},^{}|^{})}[- _{t=1}^{T} p(_{t}^{}|_{d}^{}, _{t}^{})]+\\ (q(_{d}^{}|^{} )||p(_{d}^{}))+_{t=1}^{T}(q(_{t}^{ }|_{<t}^{})||p(_{t}^{}| _{<t}^{})),\] (4)

which is a frame-wise negative variational lower bound. Only using the above vanilla VAE-based loss cannot guarantee that the disentanglement serves for adaptation, and thus we propose additional constraints on the two sets of latent factors.

**Mutual Dependence Minimization** (Fig. 3, \(\)). We first consider explicitly enforcing the two sets of latent factors to be mutually independent. To do so, we introduce the mutual information  loss \(_{}\) to regulate the two sets of latent factors. Thus, we obtain:

\[_{}(_{d}^{},^{} )=_{t=1}^{T}(q(_{d}^{},_{t}^{ })||q(_{d}^{})q(_{t}^{} ))=_{t=1}^{T}[H(_{d}^{})+H(_{t}^{ })-H(_{d}^{},_{t}^{})].\] (5)

To calculate Eq. (5), we need to estimate the densities of \(_{d}^{},_{t}^{}\) and \((_{d}^{},_{t}^{})\). Following the non-parametric way in , we use the mini-batch weighted sampling as follows:

\[H()=-_{q()}[ q()]- _{i=1}^{M}[_{j=1}^{M}q(( _{i})|_{j})],\] (6)

where \(\) is \(_{d}^{},_{t}^{}\) or \((_{d}^{},_{t}^{})\), \(N\) denotes the data size and \(M\) is the mini-batch size.

**Domain Specificity & Static Consistency** (Fig. 3, \(\)). A characteristic of the domain specificity, e.g., the video style or the objective appearance, is its static consistency over dynamic frames. With this observation, we enable the static and domain-specific latent factors so that we can remove the spatial divergence by disentangling them out. Mathematically, we hope that \(_{d}^{}\) does not change a lot when \(_{t}^{}\) varies over time. To achieve this, given a sequence, we randomly shuffle the temporal order of frames to form a new sequence. The static latent factors disentangled from the original sequence and the shuffled sequence should be ideally equal or be very close at least. This motivates us to minimize the distance between these two static factors. Meanwhile, to further enhance the domain specificity, we enforce the dynamic latent factors from different domains to have a large distance. To this end, we propose the following contrastive triplet loss:

\[_{}=(D(_{d}^{^{+}}, {}_{d}^{^{+}})-D(_{d}^{^{+}},_{d}^{^{-}})+,0),\] (7)

where \(D(,)\) is Euclidean distance, \(\) is a margin set to \(1\) in the experiments, \(_{d}^{^{+}},}_{d}^{^{+}}\), and \(_{d}^{^{-}}\) are static latent factors of the anchor sequence from domain \(^{+}\), the shuffled sequence, and a randomly selected sequence from domain \(^{-}\), respectively. \(^{+}\) and \(^{-}\) represent two different domains.

**Domain Temporal Alignment** (Fig. 3, \(\)). We now consider to reduce the temporal divergence of the _dynamic_ latent factors. There are several ways to achieve this, and in this paper, we take advantage of the most popular adversarial-based idea . Specifically, we build a domain classifier to discriminate whether the data is from \(\) or \(\). When back-propagating the gradients, a gradient reversal layer (GRL) is adopted to invert the gradients. Like existing video-based UDA methods, we also conduct both frame-level and video-level alignments. Moreover, as TA\({}^{3}\)N  does, we exploit the temporal relation network (TRN)  to discover the temporal relations among different frames, and then aggregate all the temporal relation features into the final video-level features. This enables another level of alignment on the temporal relation features. Thus, we have:

\[_{f}=_{i=1}^{N}_{t=1}^{T}CE[G_{f} (_{i\_t}^{}),d_{i}],\] (8)

\[_{r}=_{i=1}^{N}_{n=2}^{T}CE[G_ {r}(TrN_{n}(_{i}^{})),d_{i}],\] (9)

\[_{v}=_{i=1}^{N}CE[G_{v}( _{n=2}^{T}TrN_{n}(_{i}^{})),d_{i}],\] (10)

where \(d_{i}\) is the domain label, \(CE\) denotes the cross-entropy loss function, \(_{i}^{}=\{_{i\_t}^{},...,_{i\_T}^{}\}\), \(TrN_{i}\) is the \(n\)-frame temporal relation network, \(G_{f}\), \(G_{r}\), and \(G_{v}\) are the frame feature level, the temporal relation feature level, and the video feature level domain classifiers, respectively. To this end, we obtain the domain adversarial loss by summing up Eqs. (8-10):

\[_{}=_{f}+_{r}+_{v}.\] (11)

We assign equal importance to these three levels of losses to reduce the overhead of the hyperparameter search. To this end, with \(_{}\), \(_{}\), and \(_{}\), the learned dynamic latent factors are expected to be domain-invariant (the three constraints are interactive and complementary to each other for obtaining the domain-invariant dynamic latent factor), and then can be used for downstream UDA tasks. In this paper, we specifically focus on action recognition as the downstream task.

**Task Specific Supervision** (Fig. 3, \(\)). We further encourage the dynamic latent factors to carry the semantic information. Considering that the source domain has sufficient labels, we accordingly design the task supervision as the regularization imposed on \(_{t}^{}\). This gives us:

\[_{}=}}_{i=1}^{N^{}} ((_{i}^{}),y_{i}^{ }),\] (12)

where \(()\) is a feature transformer mapping the frame-level features to video-level features, specifically a TRN in this paper, and \((,)\) is either cross-entropy or mean squared error loss according to the targeted task.

Although the dynamic latent factors are constrained to be domain-invariant, we do not completely rely on source semantics to learn features discriminative for the target domain. We propose to incorporate target pseudo-labels in task-specific supervision. During the training, we use the prediction network obtained in the previous epoch to generate the target pseudo-labels of the unlabelled target training data for the current epoch. However, to increase the reliability of target pseudo-labels, we let the prediction network be trained only on the source supervision for several epochs and then integrate the target pseudo-labels in the following training epochs. Meanwhile, a confidence threshold is set to determine whether to use the target pseudo-labels or not. Thus, we have the final task-specific supervision as follows:

\[_{}=(_{i=1}^{N^{}} ((_{i}^{}),y_{i}^{})+ _{i=1}^{N^{}}((_{i}^{}),_{i}^{})),\] (13)

where \(_{i}^{}\) is the pseudo-label of \(_{i}^{}\).

**Summary**. To this end, we reach the final objective function of our _TranVAE_ framework as follows:

\[^{}=_{}+_{1}_{}+_{2}_{}+_{3}_{}+ _{4}_{},\] (14)

where \(_{i}\) with \(i=1,2,3,4\) denotes the loss balancing weight.

Experiments

In this section, we conduct extensive experimental studies on popular video-based UDA benchmarks to verify the effectiveness of the proposed _TranSVAE_ framework.

### Datasets

**UCF-HMDB** is constructed by collecting the relevant and overlapping action classes from UCF\({}_{101}\) and HMDB\({}_{51}\). It contains 3,209 videos in total with 1,438 training videos and 571 validation videos from UCF\({}_{101}\), and 840 training videos and 360 validation videos from HMDB\({}_{51}\). This in turn establishes two video-based UDA tasks: \(\) and \(\).

**Jester** consists of 148,092 videos of humans performing hand gestures. Pan _et al._ constructed a large-scale cross-domain benchmark with seven gesture classes, and form a single transfer task \(_{}_{}\), where \(_{}\) and \(_{}\) contain 51,498 and 51,415 video clips, respectively.

**Epic-Kitchens** is a challenging egocentric dataset consisting of videos capturing daily activities in kitchens.  constructs three domains across the eight largest actions. They are \(_{1}\), \(_{2}\), and \(_{3}\) corresponding to P08, P01, and P22 kitchens of the full dataset, resulting in six cross-domain tasks.

**Sprites** contains sequences of animated cartoon characters with 15 action categories. The appearances of characters are fully controlled by four attributes, _i.e._, body, top wear, bottom wear, and hair. We construct two domains, \(_{1}\) and \(_{2}\). \(_{1}\) uses the _human_ body with attributes randomly selected from 3 top wears, 4 bottom wears, and 5 hairs, while \(_{2}\) uses the _alien_ body with attributes randomly selected from 4 top wears, 3 bottom wears, and 5 hairs. The attribute pools are non-overlapping across domains, resulting in completely heterogeneous \(_{1}\) and \(_{2}\). Each domain has 900 video sequences, and each sequence is with 8 frames.

### Implementation Details

**Architecture**. Following the latest works [31; 37], we use I3D  as the backbone3. However, different from CoMix which jointly trains the backbone, we simply use the pretrained I3D model on Kinetics , provided by , to extract RGB features. For the first three benchmarks, RGB features are used as the input of _TranSVAE_. For Sprites, we use the original image as the input, for the purpose of visualizing the reconstruction and disentanglement results. We use the shared encoder and decoder structures across the source and target domains. For RGB feature inputs, the encoder and decoder are fully connected layers. For original image inputs, the encoder and decoder are the convolution and deconvolution layers (from DCGAN ), respectively. For the TRN model, we directly use the one provided by . Other details on this aspect are placed in Appendix.

**Configurations**. Our _TranSVAE_ is implemented with PyTorch . We use Adam with a weight decay of \(1e^{-4}\) as the optimizer. The learning rate is initially set to be \(1e^{-3}\) and follows a commonly used decreasing strategy in . The batch size and the learning epoch are uniformly set to be 128 and 1,000, respectively, for all the experiments. We uniformly set 100 epochs of training under only source supervision and involved the target pseudo-labels afterward. Following the common protocol in video-based UDA , we perform hyperparameter selection on the validation set. The specific hyperparameters used for each task can be found in the Appendix. NVIDIA A100 GPUs are used for all experiments. Kindly refer to our Appendix for all other details.

 
**Method \& Year** & **Backbone** & \(\) & \(\) & **Average \(\)** \\  DANN (JML’16) & ResNet-101 & 75.28 & 76.36 & 75.82 \\ JAN (ICML’17) & ResNet-101 & 74.72 & 76.69 & 75.71 \\ AdaBN (PR’18) & ResNet-101 & 72.22 & 77.41 & 74.82 \\ MCD (CVPR’18) & ResNet-101 & 73.89 & 79.34 & 76.62 \\ TA’s (ICCV’19) & ResNet-101 & 78.83 & 81.79 & 80.06 \\ ABG (MM’20) & ResNet-101 & 79.17 & 85.11 & 82.14 \\ TCN (AAAI’20) & ResNet-101 & 87.22 & 89.14 & 88.18 \\ MA’1-TD (WAC’22) & ResNet-101 & 85.00 & 86.59 & 85.80 \\  Source-only (\(S_{}\)) & I3D & 80.27 & 88.79 & 84.53 \\  DANN (JML’16) & I3D & 80.83 & 88.09 & 84.46 \\ ADDA (CVPR’17) & I3D & 79.17 & 88.44 & 83.81 \\ TA’s (ICCV’19) & I3D & 81.38 & 90.54 & 85.96 \\ SAVA (ECC’20) & I3D & 82.22 & 91.24 & 86.73 \\ CoMix (NeurIPS’21) & I3D & 86.66 & 93.87 & 90.22 \\ CO\({}^{2}\)A (WAC’22) & I3D & 87.78 & 95.79 & 91.79 \\ 
**TranSVAE (Ours)** & **I3D** & **87.78** & **98.95** & **93.37** \\  Supervised-target (\(T_{}\)) & I3D & 95.00 & 96.85 & 95.93 \\  

Table 1: UDA performance comparisons on UCF-HMDB.

**Competitors**. We compared methods from three lines. We first consider the _source-only_ (\(_{}\)) and _supervised-target_ (\(_{}\)) which uses only labeled source data and only labeled target data, respectively. These two baselines serve as the lower and upper bounds for our tasks. Secondly, we consider five popular image-based UDA methods by simply ignoring temporal information, namely DANN , JAN , ADDA , AdaBN , and MCD . Lastly and most importantly, we compare recent SoTA video-based UDA methods, including TA\({}^{3}\)N , SAVA , TCoN , ABG , CoMix , CO\({}^{2}\)A , and MA\({}^{2}\)L-TD . All these methods use single modality features. We directly quote numbers reported in published papers whenever possible. There exist recent works conducting video-based UDA using multi-modal data, _e.g._ RGB + Flow. Although _TranSVAE_ solely uses RGB features, we still take this set of methods into account. Specifically, we consider MM-SADA , STCDA , CMCD , A3R , CleanAdapt , CycDA , MixDANN  and CIA .

### Comparative Study

**Results on UCF-HMDB**. Tab. 1 shows comparisons of _TranSVAE_ with baselines and SoTA methods on UCF-HMDB. The best result among all the baselines is highlighted using bold. Overall, methods using the I3D backbone  achieve better results than those using ResNet-101. Our _TranSVAE_ consistently outperforms all previous methods. In particular, _TranSVAE_ achieves \(93.37\%\) average accuracy, improving the best competitor CO\({}^{2}\)A , with the same I3D backbone , by \(1.38\%\). Surprisingly, we observe that _TranSVAE_ even yields better results (by a \(2.1\%\) improvement) than the supervised-target (\(_{}\)) baseline. This is because the \(\) task already has a good performance without adaptation, _i.e._, \(88.79\%\) for the source-only (\(_{}\)) baseline, and thus the target pseudo-labels used in _TranSVAE_ are almost correct. By further aligning domains and equivalently augmenting training data, _TranSVAE_ outperforms \(_{}\) which is only trained with target data.

**Results on Jester & Epic-Kitchens**. Tab. 3 shows the comparison results on the Jester and Epic-Kitchens benchmarks. We can see that our _TranSVAE_ is the clear winner among all the methods on all the tasks. Specifically, _TranSVAE_ achieves a \(1.4\%\) improvement and a \(9.4\%\) average improvement over the runner-up baseline CoMix  on Jester and Epic-Kitchens, respectively. This verifies the superiority of _TranSVAE_ over others in handling video-based UDA. However, we also notice that the accuracy gap between CoMix and \(_{}\) is still significant on Jester. This is because the large-scale Jester dataset contains highly heterogeneous data across domains, _e.g._, the source domain contains videos of the rolling hand forward, while the target domain only consists of videos of the rolling hand backward. This leaves much room for improvement in the future.

**Compare to Multi-Modal Methods**. We further compare with four recent video-based UDA methods that use multi-modalities, _e.g._ RGB features, and optical flows, although _TranSVAE_ only uses RGB features. Surprisingly, _TranSVAE_ achieves better average results than seven out of eight multi-modal methods and is only worse than CleanAdapt  on UCF-HMDB and A3R  on Epic-Kitchens. Considering _TranSVAE_ only uses single-modality data, we are confident that there exists great potential for further improvements of _TranSVAE_ with multi-modal data taken into account.

  
**Task** & \(_{}\) & MM-SADA & STCDA & CMCD & A3R & CleanAdapt & CycDA & MixDANN & CIA & **TranSVAE** \\  \(\) & \(86.1\) & \(84.2\) & \(83.1\) & \(84.7\) & / & \(\) & \(88.1\) & \(82.2\) & \(\) & \(87.8\) (\(\)**7.7** \\ \(\) & \(92.5\) & \(91.1\) & \(92.1\) & \(92.8\) & / & \(\) & \(98.0\) & \(92.8\) & \(94.1\) & \(99.0\) (\(\)**6.8**) \\ 
**Average**\(\) & \(89.3\) & \(87.7\) & \(87.6\) & \(88.8\) & / & \(\) & \(93.1\) & \(87.5\) & \(91.2\) & \(93.4\) (\(\)**4.1**) \\  \(_{1}_{2}\) & \(43.2\) & \(49.5\) & \(52.0\) & \(50.3\) & \(\) & \(52.7\) & / & \(56.0\) & \(52.5\) & \(50.5\) (\(\)**7.8**) \\ \(_{1}_{3}\) & \(42.5\) & \(44.1\) & \(45.5\) & \(46.3\) & \(\) & \(47.0\) & / & \(47.3\) & \(47.8\) & \(50.3\) (\(\)**7.8**) \\ \(_{2}_{3}\) & \(43.0\) & \(48.2\) & \(49.0\) & \(49.5\) & \(\) & \(46.2\) & / & \(50.3\) & \(49.8\) & \(50.3\) (\(\)**7.9**) \\ \(_{2}_{3}\) & \(48.0\) & \(52.7\) & \(52.5\) & \(52.0\) & \(55.5\) & \(52.7\) & / & \(52.4\) & \(53.2\) & \(\) (\(\)**10.0**) \\ \(_{3}_{1}\) & \(43.0\) & \(50.9\) & \(\) & \(48.7\) & \(51.5\) & 47.8 & / & \(51.0\) & \(52.2\) & \(48.0\) (\(\)**5.0**) \\ \(_{3}_{2}\) & \(55.5\) & \(56.1\) & \(55.6\) & \(56.3\) & \(\) & \(54.4\) & / & \(54.7\) & \(57.6\) & \(58.0\) (\(\)**2.6**) \\  
**Average**\(\) & \(45.9\) & \(50.3\) & \(51.2\) & \(51.0\) & \(\) & \(50.3\) & / & \(52.0\) & \(52.2\) & \(52.6\) (\(\)**6.7**) \\   

Table 2: UDA performance comparisons to approaches using multi-modality data as the input.

  
**Task** & \(_{}\) & DANN & ADDA & TA\({}^{3}\)N & CoMix & **TranSVAE** & \(_{}\) \\  \(_{s}_{}\) & \(51.5\) & \(55.4\) & \(52.3\) & \(55.5\) & \(64.7\) & \(\) (\(\)**4.8**) & \(95.6\) \\  \(_{1}_{2}\) & \(32.8\) & \(37.7\) & \(35.4\) & \(34.2\) & \(42.9\) & \(\) (\(\)**7.7**) & \(64.0\) \\ \(_{1}_{3}\) & \(34.1\) & \(36.6\) & \(34.9\) & \(37.4\) & \(40.9\) & \(\) (\(\)**10.6**) & \(63.7\) \\ \(_{2}_{3}\) & \(35.4\) & \(38.3\) & \(36.3\) & \(40.9\) & \(38.6\) & \(\) (\(\)**10.4**) & \(57.0\) \\ \(_{2}_{3}\) & \(39.1\) & \(41.9\) & \(40.8\) & \(42.8\) & \(45.2\) & \(\) (\(\)**10.6**) & \(63.7\) \\ \(_{3}_{1}\) & \(34.6\) & \(38.8\) & \(36.1\) & \(39.9\

### Property Analysis

**Disentanglement Analysis**. We analyze the disentanglement effect of _TranVAE_ on Sprites  and show results in Fig. 4. The left subfigure shows the original sequences of the two domains. The "Human" and "Alien" are completely different appearances and the former is _casting spells_ while the latter is _slashing_. The middle subfigure shows the sequences reconstructed only using \(\{_{1}^{},...,_{T}^{}\}\). It can be clearly seen that the two sequences keep the same action as the corresponding original ones. However, if we only focus on the appearance characteristics, it is difficult to distinguish the domain to which the sequences belong. This indicates that \(\{_{1}^{},...,_{T}^{}\}\) are indeed _domain-invariant_ and well encode the semantic information. The right subfigure shows the sequences reconstructed by exchanging \(_{d}^{}\), which results in two sequences with the same actions but exchanged appearance. This verifies that \(_{d}^{}\) is representing the appearance information, which is actually the _domain-related_ information in this example. This property study sufficiently supports that _TranVAE_ can successfully disentangle the domain information from other information, with the former embedded in \(_{d}^{}\) and the latter embedded in \(\{_{1}^{},...,_{T}^{}\}\).

**Complexity Analysis**. We further conduct complexity analysis on our _TranVAE_. Specifically, we compare the number of trainable parameters, multiply-accumulate operations (MACs), floating-point operations (FLOPs), and inference frame-per-second (FPS) with existing baselines including TA\({}^{3}\)N , CO\({}^{2}\)A , and CoMix . All the comparison results are shown in Tab. 4. From the table, we observe that _TranSVAE_ requires less trainable parameters than CO\({}^{2}\)A and CoMix. Although more trainable parameters are used than TA\({}^{3}\)N, _TranSVAE_ achieves significant adaptation performance improvement than TA\({}^{3}\)N (see Tab. 1 and Tab. 3). Moreover, the MACs, FLOPs, and FPS are competitive among different methods. This is reasonable since all these approaches adopt the same I3D backbone.

**Ablation Study**. We now analyze the effectiveness of each loss term in Eq. (14). We compare with four variants of _TranSVAE_, each removing one loss term by equivalently setting the weight \(\) to \(0\). The ablation results on UCF-HMDB, Jester, and Epic-Kitchens are shown in Tab. 5. As can be seen, removing \(_{}\) significantly reduces the transfer performance in all the tasks. This is reasonable as \(_{}\) is used to discover the discriminative features. Removing any of \(_{}\), \(_{}\), and \(_{}\) leads to an inferior result than the full _TranSVAE_ setup, and removing \(_{}\) is the most influential. This is because \(_{}\) is used to explicitly reduce the temporal domain gaps. All these results demonstrate that every proposed loss matters in our framework.

We further conduct another ablation study by sequentially integrating \(_{}\), \(_{}\), \(_{}\), and \(_{}\) into our sequential VAE structure using UCF-HMDB. We use this integration order based on the average positive improvement that a loss brings to _TranSVAE_ as shown in Tab. 5. We also take advantage of t-SNE  to visualize the features learned by these different variants. We plot two sets of t-SNE

  
**Methods** & **Trainable Params** & **MACs** & **FLOPs** & **FPS** \\  TA\({}^{3}\)N & \(7.6880\) M & \(18.2318\) G & \(36.4636\) G & \(0.0134\) s \\ CoMix & \(30.3688\) M & \(18.5640\) G & \(37.1280\) G & \(0.0157\) s \\ CO\({}^{2}\)A & \(23.6720\) M & \(18.1884\) G & \(36.3768\) G & \(0.0127\) s \\ 
**TranSVAE** & \(12.7419\) M & \(18.2657\) G & \(36.5314\) G & \(0.0133\) s \\   

Table 4: Comparison results on model complexity.

figures, one using the class-wise label and another using the domain label. Fig. 5 and Fig. 6 show the visualization and the quantitative results. As can be seen from the t-SNE feature visualizations, adding a new component improves both the domain and semantic alignments, and the best alignment is achieved when all the components are considered. The quantitative results further show that the transfer performance gradually increases with the sequential integration of the four components, which again verifies the effectiveness of each component in _TranSVAE_. More ablation study results can be found in the Appendix.

## 5 Conclusion and Limitation

In this paper, we proposed a _TranSVAE_ framework for video-based UDA tasks. Our key idea is to explicitly disentangle the domain information from other information during the adaptation. We developed a novel sequential VAE structure with two sets of latent factors and proposed four constraints to regulate these factors for adaptation purposes. Note that disentanglement and adaptation are interactive and complementary. All the constraints serve to achieve a good disentanglement effect with the two-level domain divergence minimization. Extensive empirical studies clearly verify that _TranSVAE_ consistently offers performance improvements compared with existing SoTA video-based UDA methods. We also find that _TranSVAE_ outperforms those multi-modal UDA methods, although it only uses single-modality data. Comprehensive property analysis further shows that _TranSVAE_ is an effective and promising method for video-based UDA.

We further discuss the limitations of the _TranSVAE_ framework. These are also promising future directions. Firstly, the empirical evaluation is mainly on the action recognition task. The performance of _TranSVAE_ on other video-related tasks, _e.g._ video segmentation, is not tested. Secondly, _TranSVAE_ is only evaluated on the typical two-domain transfer scenario. The multi-source transfer case is not considered but is worthy of further study. Thirdly, although _TranSVAE_ exhibits better performance than multi-modal transfer methods, its current version does not consider multi-modal data functionally and structurally. An improved _TranSVAE_ with the capacity of using multi-modal data is expected to further boost the adaptation performance. Fourthly, current empirical evaluations are mainly based on the I3D backbone, more advanced backbones, _e.g._, [36; 40], are expected to be explored for further improvement. Lastly, _TranSVAE_ handles the spatial divergence by disentangling the static domain-specific latent factors out. However, it may happen that spatial divergence is not completely captured by the static latent factors due to insufficient disentanglement.

Figure 5: Loss integration studies on \(\). **Left:** The t-SNE plots for class-wise (top row) and domain (bottom row, red source & blue target) features. **Right:** Ablation results (%) by adding each loss sequentially, _i.e._, row (a) - row (e).

Figure 6: Loss integration studies on \(\). **Left:** The t-SNE plots for class-wise (top row) and domain (bottom row, red source & blue target) features. **Right:** Ablation results (%) by adding each loss sequentially, _i.e._, row (a) - row (e).