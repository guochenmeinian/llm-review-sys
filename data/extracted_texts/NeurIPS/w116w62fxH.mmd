# Optimal Learners for Realizable Regression:

PAC Learning and Online Learning

 Idan Attias

Ben-Gurion University of the Negev

idanatti@post.bgu.ac.il

&Steve Hanneke

Purdue University

steve.hanneke@gmail.com

&Alkis Kalavasis

Yale University

alvertos.kalavasis@yale.edu

&Amin Karbasi

Yale University, Google Research

amin.karbasi@yale.edu

&Grigoris Velegkas

Yale University

grigoris.velegkas@yale.edu

###### Abstract

In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.

Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in this context.

Additionally, in the context of online learning we provide a dimension that characterizes the minimax instance optimal cumulative loss up to a constant factor and design an optimal online learner for realizable regression, thus resolving an open question raised by Daskalakis and Golowich in STOC '22.

## 1 Introduction

Real-valued regression is one of the most fundamental and well-studied problems in statistics and data science , with numerous applications in domains such as economics and medicine . However, despite its significance and applicability, theoretical understanding of the statistical complexity of real-valued regression is still lacking.

Perhaps surprisingly, in the fundamental **realizable** Probably Approximately Correct (PAC) setting  and the **realizable** online setting , we do not know of any characterizing dimension or optimal learners for the regression task. This comes in sharp contrast with binary and multiclass classification, both in the offline and the online settings, where the situation is much more clear . Our goal in this work is to make progress regarding the following important question:

[MISSING_PAGE_EMPTY:2]

by . The study of multiclass online classification was initiated by , who provided an optimal algorithm for the realizable setting and an algorithm that is suboptimal by a factor of \( k T\) in the agnostic setting, where \(k\) is the total number of labels. Recently,  shaved off the \( k\) factor. The problem of online regression differs significantly from that of online classification, since the loss function is not binary. The agnostic online regression setting has received a lot of attention and there is a series of works that provides optimal minimax guarantees .

To the best of our knowledge, the realizable setting has received much less attention. A notable exception is the work of  that focuses on realizable online regression using _proper_1 learners. They provide an optimal regret bound with respect to the _sequential fat-shattering dimension_. However, as they mention in their work (cf. Examples 1 and 2), this dimension does _not_ characterize the optimal cumulative loss bound.

Interestingly, Daskalakis and Golowich  leave the question of providing a dimension that characterizes online realizable regression open. In our work, we resolve this question by providing upper and lower bounds for the cumulative loss of the learner that are tight up to a constant factor of \(2\) using a novel combinatorial dimension that is related to (scaled) Littlestone trees (cf. Definition 13). Formally, the setting of realizable online regression is defined as follows:

**Definition 2** (Online Realizable Regression).: _Let \(:^{2}_{ 0}\) be a loss function. Consider a class \(^{}\) for some domain \(\). The realizable online regression setting over \(T\) rounds consists of the following interaction between the learner and the adversary:_

* _The adversary presents_ \(x_{t}\)_._
* _The learner predicts_ \(_{t}\)_, possibly using randomization._
* _The adversary reveals the true label_ \(y_{t}^{}\) _with the constraint that_ \( h_{t}^{}, t,h(x_{})=y_{}^{}\)_._
* _The learner suffers loss_ \((_{t},y_{t}^{})\)_._

_The goal of the learner is to minimize its expected cumulative loss \(_{T}=[_{t[T]}(_{t},y_{t}^{ })]\)._

We remark that in the definition of the cumulative loss \(_{T}\), the expectation is over the randomness of the algorithm (which is the only stochastic aspect of the online setting). As we explained before, the main question we study in this setting is the following:

Figure 1: Landscape of Realizable PAC Regression: the “deleted” arrows mean that the implication is _not_ true. The equivalence between finite fat-shattering dimension and the uniform convergence property is known even in the realizable case (see ) and the fact that PAC learnability requires finite scaled Natarajan dimension is proved in . The properties of the other three dimensions (scaled Graph dimension, scaled One-Inclusion-Graph (OIG) dimension, and scaled Daniely-Shalev Shwartz (DS) dimension) are shown in this work. We further conjecture that finite scaled Natajaran dimension is not sufficient for PAC learning, while finite scaled DS does suffice. Interestingly, we observe that the notions of uniform convergence, learnability by any ERM and PAC learnability are separated in realizable regression.

[MISSING_PAGE_FAIL:4]

### \(\)-Fat Shattering Dimension

Perhaps the most well-known dimension in the real-valued learning setting is the fat shattering dimension that was introduced in . Its definition is inspired by the pseudo-dimension  and it is, essentially, a scaled version of it.

**Definition 4** (\(\)-Fat Shattering Dimension ).: _Let \(^{}\). We say that a sample \(S^{n}\) is \(\)-fat shattered by \(\) if there exist \(s_{1},,s_{n}^{n}\) such that for all \(b\{0,1\}^{n}\) there exists \(h_{b}\) such that:_

* \(h_{b}(x_{i}) s_{i}+, i[n]\) _such that_ \(b_{i}=1\)_._
* \(h_{b}(x_{i}) s_{i}-, i[n]\) _such that_ \(b_{i}=0\)_._

_The \(\)-fat shattering dimension \(_{}^{}\) is defined to be the maximum size of a \(\)-fat shattered set._

In the realizable setting, finiteness of the fat-shattering dimension (at all scales) is sufficient for learnability and it is equivalent to uniform convergence2. However, the next example shows that it is **not** a necessary condition for learnability. This comes in contrast to the agnostic case for real-valued functions, in which learnability and uniform convergence are equivalent for all \(\).

**Example 1** (Realizable Learnability \(\#\) Finite Fat-Shattering Dimension, see Section 6 in ).: _Consider a class \(^{}\) where each hypothesis is uniquely identifiable by a single example, i.e., for any \(x\) and any \(f,g\) we have that \(f(x) g(x)\), unless \(f g\). Concretely, for every \(d\), let \(\{S_{j}\}_{0 j d-1}\) be a partition of \(\) and define \(_{d}=\{h_{b_{0},...,b_{d-1}}:b_{i}\{0,1\},0 i d-1 \}\), where_

\[h_{b_{0},...,b_{d-1}}(x)=_{j=0}^{d-1}_{S_{j}}(x)b_{ j}+_{k=0}^{d-1}b_{k}2^{-k}\,.\]

_For any \( 1/4,_{}^{}(_{d})=d\), since for a set of points \(x_{0},,x_{d-1}\) such that each \(x_{j}\) belongs to \(S_{j}\), \(0 j d-1\), \(_{d}\) contains all possible patterns of values above \(3/4\) and below \(1/4\). Indeed, it is not hard to verify that for any \(j\{0,,d-1\}\) if we consider any \(h^{}:=h_{b_{0},...,b_{d-1}}_{d}\) with \(b_{j}=1\) we have \(h^{}(x_{j}) 3/4\). Similarly, if \(b_{j}=0\) then it holds that \(h^{}(x_{j}) 1/4\). Hence, \(_{}^{}(_{d}_{d })=\). Nevertheless, by just observing one example \((x,h^{}(x))\) any ERM learner finds the exact labeling function \(h^{}\)._

We remark that this example already shows that the PAC learnability landscape of regression is quite different from that of multiclass classification, where agnostic learning and realizable learning are characterized by the same dimension .

To summarize this subsection, the fat-shattering dimension is a natural way to quantify how well the function class can interpolate (with gap \(\)) some fixed function. Crucially, this interpolation contains only inequalities (see Definition4) and hence (at least intuitively) cannot be tight for the realizable setting, where there exists some function that exactly labels the features. Example1 gives a natural example of a class with infinite fat-shattering dimension that can, nevertheless, be learned with a single sample in the realizable setting.

### \(\)-Natarajan Dimension

The \(\)-Natarajan dimension was introduced by  and is inspired by the Natarajan dimension , which has been used to derive bounds in the multiclass classification setting. Before explaining the \(\)-Natarajan dimension, let us begin by reminding to the reader the standard Natarajan dimension. We say that a set \(S=\{x_{1},...,x_{n}\}\) of size \(n\) is Natarajan-shattered by a concept class \(^{}\) if there exist two functions \(f,g:S\) so that \(f(x_{i}) g(x_{i})\) for all \(i[n]\), and for all \(b\{0,1\}^{n}\) there exists \(h\) such that \(h(x_{i})=f(x_{i})\) if \(b_{i}=1\) and \(h(x_{i})=g(x_{i})\) if \(b_{i}=0\). Note that here we have equalities instead of inequalities (recall the fat-shattering case Definition4).

From a geometric perspective (see ), this means that the space \(\) projected on the set \(S\) contains the set \(\{f(x_{1}),g(x_{1})\}...\{f(x_{n}),g(x_{n})\}\). This set is "isomorphic" to the Booleanhypercube of size \(n\) by mapping \(f(x_{i})\) to 1 and \(g(x_{i})\) to 0 for all \(i[n]\). This means that the Natarajan dimension is essentially the size of the largest Boolean cube contained in \(\).

[Sim97] defines the scaled analogue of the above dimension as follows.

**Definition 5** (\(\)-Natarajan Dimension [Sim97]).: _Let \(^{}\). We say that a set \(S^{n}\) is \(\)-Natarajan-shattered by \(\) if there exist \(f,g:[n]\) such that for every \(i[n]\) we have \((f(i),g(i)) 2,\) and_

\[|_{S}\{f(1),g(1)\}\{f(n),g(n)\}\,.\]

_The \(\)-Natarajan dimension \(_{}^{}\) is defined to be the maximum size of a \(\)-Natarajan-shattered set._

Intuitively, one should think of the \(\)-Natarajan dimension as indicating the size of the largest Boolean cube that is contained in \(\). Essentially, every coordinate \(i[n]\) gets its own translation of the \(0,1\) labels of the Boolean cube, with the requirement that these two labels are at least \(2\) far from each other. [Sim97] showed the following result, which states that finiteness of the Natarajan dimension at all scales is a necessary condition for realizable PAC regression:

**Informal Theorem 1** (Theorem 3.1 in [Sim97]).: \(^{}\) _is PAC learnable in the realizable regression setting only if \(_{}^{}()<\) for all \((0,1).\)_

Concluding these two subsections, we have explained the main known general results about realizable offline regression: (i) finiteness of fat-shattering is sufficient but not necessary for PAC learning and (ii) finiteness of scaled Natarajan is necessary for PAC learning.

### \(\)-Graph Dimension

We are now ready to introduce the \(\)-graph dimension, which is a relaxation of the definition of the \(\)-Natarajan dimension. To the best of our knowledge, it has not appeared in the literature before. Its definition is inspired by its non-scaled analogue in multiclass classification [Nat89, DSS14].

**Definition 6** (\(\)-Graph Dimension).: _Let \(^{},:^{2}\). We say that a sample \(S^{n}\) is \(\)-graph shattered by \(\) if there exists \(f:[n]\) such that for all \(b\{0,1\}^{n}\) there exists \(h_{b}\) such that:_

* \(h_{b}(x_{i})=f(i), i[n]\) _such that_ \(b_{i}=0\)_._
* \((h_{b}(x_{i}),f(i))>, i[n]\) _such that_ \(b_{i}=1\)_._

_The \(\)-graph dimension \(_{}^{}\) is defined to be the maximum size of a \(\)-graph shattered set._

We mention that the asymmetry in the above definition is crucial. In particular, replacing the equality \(h_{b}(x_{i})=f(i)\) with \((h_{b}(x_{i}),f(i))\) fails to capture the properties of the graph dimension. Intuitively, the equality in the definition reflects the assumption of realizability, i.e., the guarantee that there exists a hypothesis \(h^{}\) that exactly fits the labels. Before stating our main result, we can collect some useful observations about this new combinatorial measure. In particular, we provide examples inspired by [DSS14, DSSBOSS15] which show (i) that there exist gaps between different ERM learners (see Example 3) and (ii) that any learning algorithm with a close to optimal sample complexity must be improper (see Example 4).

Our first main result relates the scaled graph dimension with the learnability of any class \(^{}\) using a (worst case) ERM learner. This result is the scaled analogue of known multiclass results [DSS14, DSSBOSS15] but its proof for the upper bound follows a different path. For the formal statement of our result and its full proof, we refer the reader to Appendix B.

**Informal Theorem 2** (Informal, see Theorem 1).: _Any \(^{}\) is PAC learnable in the realizable regression setting by a worst-case ERM learner if and only if \(_{}^{}()<\) for all \((0,1)\)._

**Proof Sketch.** The proof of the lower bound follows in a similar way as the lower bound regarding learnability of binary hypothesis classes that have infinite VC dimension [VC71, BEHW89]. If \(\) has infinite \(\)-graph dimension for some \((0,1)\), then for any \(n\) we can find a sequence of \(n\) points \(x_{1},,x_{n}\) that are \(\)-graph shattered by \(\). Then, we can define a distribution \(\) that puts most of its mass on \(x_{1}\), so if the learner observes \(n\) samples then with high probability it will not observe at least half of the shattered points. By the definition of the \(\)-graph dimension this shows

[MISSING_PAGE_FAIL:7]

**Definition 8** (Orientation and Scaled Out-Degree).: _Let \(,n,^{[n]}\). An orientation of the one-inclusion graph \(G_{}^{}=(V,E)\) is a mapping \(:E V\) so that \((e) e\) for any \(e E\). Let \(_{i}(e)\) denote the \(i\)-th entry of the orientation._

_For a vertex \(v V\), corresponding to some hypothesis \(h\) (see Definition 7), let \(v_{i}\) be the \(i\)-th entry of \(v\), which corresponds to \(h(i)\). The (scaled) out-degree of a vertex \(v\) under \(\) is \((v;,)=|\{i[n]:(_{i}(e_{i,v}),v_{i})>\}|\). The maximum (scaled) out-degree of \(\) is \((,)=_{v V}(v;,)\)._

Finally, we introduce the following novel dimension in the context of real-valued regression. An analogous dimension was proposed in the context of learning under adversarial robustness .

**Definition 9** (\(\)-Oig Dimension).: _Consider a class \(^{}\) and let \(\). We define the \(\)-one-inclusion graph dimension \(_{}^{}\) of \(\) as follows:_

\[_{}^{}()=\{n : S^{n}G=(V,E)G_{|_{S}}^{}=(V_{n},E_{n})\] \[, v V,(v;,)>n/3\}\,.\]

_We define the dimension to be infinite if the supremum is not attained by a finite \(n\)._

In words, it is the largest \(n\) (potentially \(\)) such that there exists an (unlabeled) sequence \(S\) of length \(n\) with the property that no matter how one orients some finite subgraph of the one-inclusion graph, there is always some vertex for which at least \(1/3\) of its coordinates are \(\)-different from the labels of the edges that are attached to this vertex. We remark that the hypothesis class in Example 1 has \(\)-OIG dimension equal to \(O(1)\). Moreover, a finite fat-shattering dimension of hypothesis class implies a finite OIG dimension of roughly the same size (see Appendix C). We also mention that the above dimension satisfies the "finite character" property and the remaining criteria that dimensions should satisfy according to  (see Appendix F). As our main result in this section, we show that any class \(\) is learnable if and only if this dimension is finite and we design an (almost) optimal learner for it.

**Informal Theorem 3** (Informal, see Theorem 2).: _Any \(^{}\) is PAC learnable in the realizable regression setting if and only if \(_{}^{}()<\) for all \((0,1)\)._

The formal statement and its full proof are postponed to Appendix C.

**Proof Sketch.** We start with the lower bound. As we explained before, orientations of the one-inclusion graph are, in some sense, equivalent to learning algorithms. Therefore, if this dimension is infinite for some \(>0\), then for any \(n\), there are no orientations with small maximum out-degree. Thus, for _any_ learner we can construct _some_ distribution \(\) under which it makes a prediction that is \(\)-far from the correct one with constant probability, which means that \(\) is not PAC learnable.

Let us now describe the proof of the converse direction, which consists of several steps. First, notice that the finiteness of this dimension provides good orientations for _finite_ subgraphs of the one-inclusion graph. Using the compactness theorem of first-order logic, we can extend them to a good orientation of the whole, potentially infinite, one-inclusion graph. This step gives us a weak learner with the following property: for any given \(\) there is some \(n_{0}\) so that, with high probability over the training set, when it is given \(n_{0}\) examples as its training set it makes mistakes that are of order at least \(\) on a randomly drawn point from \(\) with probability at most \(1/3\). The next step is to boost the performance of this weak learner. This is done using the "median-boosting" technique  (cf. Algorithm 2) which guarantees that after a small number of iterations we can create an ensemble of weak learners such that, a prediction rule according to their (weighted) median will not make any \(\)- mistakes on the training set. However, this is not sufficient to prove that the ensemble of these learners has small loss on the distribution \(\). This is done by establishing _sample compression_ schemes that have small length. Essentially, such schemes consist of a _compression_ function \(\) which takes as input a training set and outputs a subset of it, and a reconstruction function \(\) which takes as input the output of \(\) and returns a predictor whose error on every point of the training set \(S\) is at most \(\). Extending the arguments of  from the binary setting to the real-valued setting we show that the existence of such a scheme whose compression function returns a set of "small" cardinality implies generalization properties of the underlying learning rule. Finally, we show that our weak learner combined with the boosting procedure admit such a sample compression scheme.

Before proceeding to the next section, one could naturally ask whether there is a natural property of the concept class that implies finiteness of the scaled OIG dimension. The work of  providesa sufficient and natural condition that implies finiteness of our complexity measure. In particular, Mendelson shows that classes that contain functions with bounded oscillation (as defined in ) have finite fat-shattering dimension. This implies that the class is learnable in the agnostic setting and hence is also learnable in the realizable setting. As a result, the OIG-based dimension is also finite. So, bounded oscillations are a general property that guarantees that the finiteness of OIG-based dimension and fat-shattering dimension coincide. We also mention that deriving bounds for the OIG-dimension for interesting families of functions is an important yet non-trivial question.

### \(\)-DS Dimension

So far we have identified a dimension (cf. Definition 9) that characterizes the PAC learnability of realizable regression. However, it might not be easy to calculate it in some settings. Our goal in this section is to introduce a relaxation of this definition which we conjecture that also characterizes learnability in this context. This new dimension is inspired by the DS dimension, a combinatorial dimension defined by Daniely and Shalev-Shwartz in . In a recent breakthrough result,  showed that the DS dimension characterizes multiclass learnability (with a possibly unbounded number of labels and the 0-1 loss). We introduce a scaled version of the DS dimension. To this end, we first define the notion of a scaled pseudo-cube.

**Definition 10** (Scaled Pseudo-Cube).: _Let \(\). A class \(^{d}\) is called a \(\)**-pseudo-cube** of dimension \(d\) if it is non-empty, finite and, for any \(f\) and direction \(i[d]\), the hyper-edge \(e_{i,f}=\{g:g(j)=f(j)\;\; j[d],i j\}\) satisfies \(|e_{i,f}|>1\) and \((g_{1}(i),g_{2}(i))>\) for any \(g_{1},g_{2} e_{i,f},g_{1} g_{2}\)._

Pseudo-cubes can be seen as a relaxation of the notion of a Boolean cube (which should be intuitively related with the Natarajan dimension) and were a crucial tool in the proof of . In our setting, scaled pseudo-cubes will give us the following combinatorial dimension.

**Definition 11** (\(\)-Ds Dimension).: _Let \(^{}\). A set \(S^{n}\) is \(\)-\(\) shattered if \(|_{S}\) contains an \(n\)-dimensional \(\)-pseudo-cube. The \(\)-\(\) dimension\(_{}^{}\) is the maximum size of a \(\)-\(\)-shattered set._

Extending the ideas from the multiclass classification setting, we show that the scaled-DS dimension is necessary for realizable PAC regression. Simon (Section 6, ) left as an open direction to "obtain supplementary lower bounds [for realizable regression] (perhaps completely unrelated to the combinatorial or Natarajan dimension)". Our next result is a novel contribution to this direction.

**Informal Theorem 4** (Informal, see Theorem 3).: _Any \(^{}\) is PAC learnable in the realizable regression setting only if \(_{}^{}()<\) for all \((0,1)\)._

The proof is postponed to Appendix D. We believe that finiteness of \(_{}^{}()\) is also a _sufficient_ condition for realizable regression. However, the approach of  that establishes a similar result in the setting of multiclass classification does not extend trivially to the regression setting.

**Conjecture 1** (Finite \(\)-DS is Sufficient).: _Let \(^{}\). If \(_{}^{}()<\) for all \((0,1)\), then \(\) is PAC learnable in the realizable regression setting._

## 3 Online Learnability for Realizable Regression

In this section we will provide our main result regarding realizable online regression. Littlestone trees have been the workhorse of online classification problems . First, we provide a definition for scaled Littlestone trees.

**Definition 12** (Scaled Littlestone Tree).: _A scaled Littlestone tree of depth \(d\) is a complete binary tree of depth \(d\) defined as a collection of nodes_

\[_{0<d}\{x_{u}:u\{0,1\}^{}\}= \{x_{}\}\{x_{0},x_{1}\}\{x_{00},x _{01},x_{10},x_{11}\}...\]

_and real-valued gaps_

\[_{0<d}\{_{u}:u\{0,1\}^{}\}= \{_{}\}\{_{0},_{1}\} \{_{00},_{01},_{10},_{11}\}...\]_such that for every path \(\{0,1\}^{d}\) and finite \(n<d\), there exists \(h\) so that \(h(x_{})=s_{+1}\) for \(0 n\), where \(s_{_{+1}}\) is the label of the edge connecting the nodes \(x_{_{}}\) and \(x_{_{+1}}\) and \((s_{,0},s_{,1})=_{ }\)._

In words, scaled Littlestone trees are complete binary trees whose nodes are points of \(\) and the two edges attached to every node are its potential classifications. An important quantity is the _gap_ between the two values of the edges. We define the online dimension \(^{}()\) as follows.

**Definition 13** (Online Dimension).: _Let \(^{}\). Let \(_{d}\) be the space of all scaled Littlestone trees of depth \(d\) (cf. Definition12) and \(=_{d=0}^{}_{d}\). For any scaled tree \(T=_{0(T)}\{(x_{u},_{u})(,):u\{0,1\}^{}\}\,,\) let \((T)=\{y=(y_{0},...,y_{(T)}):y_{i}\{0,1\}^{i}\}\}\) be the set of all paths in \(T\). The dimension \(^{}()\) is_

\[^{}()=_{T}_{y (T)}_{i=0}^{(T)}_{y_{i}}\,.\] (1)

In words, this dimension considers the tree that has the maximum sum of label gaps over its path with the smallest such sum, among all the trees of arbitrary depth. Note that we are taking the supremum (infimum) in case there is no tree (path) that achieves the optimal value. Providing a characterization and a learner with optimal cumulative loss \(_{T}\) for realizable online regression was left as an open problem by . We resolve this question (up to a factor of 2) by showing the following result.

**Informal Theorem 5** (Informal, see Theorem4).: _For any \(^{}\) and \(>0\), there exists a deterministic learner with \(_{}^{}()+\) and any, potentially randomized, learner satisfies \(_{}^{}()/2-\)._

The formal statement and the full proof of our results can be found in AppendixE. First, we underline that this result holds when there is no bound on the number of rounds that the learner and the adversary interact. This follows the same spirit as the results in the realizable binary and multiclass classification settings . The dimension that characterizes the minimax optimal cumulative loss for any given \(T\) follows by taking the supremum in Definition13 over trees whose depth is at most \(T\) and the proof follows in an identical way (note that even with finite fixed depth \(T\) the supremum is over infinitely many trees). Let us now give a sketch of our proofs.

**Proof Sketch.** The lower bound follows using similar arguments as in the classification setting: for any \(>0\), the adversary can create a scaled Littlestone tree \(T\) that achieves the \(\) bound, up to an additive \(\). In the first round, the adversary present the root of the tree \(x_{}\). Then, no matter what the learner picks the adversary can force error at least \(_{}/2\). The game is repeated on the new subtree. The proof of the upper bound presents the main technical challenge to establish Theorem4. The strategy of the learner can be found in Algorithm3. The key insight in the proof is that, due to realizability, we can show that in every round \(t\) there is some \(_{t}\) the learner can predict so that, no matter what the adversary picks as the true label \(y_{t}^{*}\), the online dimension of the class under the extra restriction that \(h(x_{t})=y_{t}^{*}\), i.e, the updated _version space_\(V=\{h:h(x_{})=y_{}^{*},1 t\}\), will decrease by \((y_{}^{*},_{t})\). Thus, under this strategy of the learner, the adversary can only distribute up to \(^{}()\) across all the rounds of the interaction. We explain how the learner can find such a \(_{t}\) and we handle technical issues that arise due to the fact that we are dealing with \(\) instead of \(\) in the formal proof (cf. AppendixE).

## 4 Conclusion

In this work, we developed optimal learners for realizable regression in PAC learning and online learning. Moreover, we identified combinatorial dimensions that characterize learnability in these settings. We hope that our work can lead to simplified characterizations for these problems. We believe that the main limitation of our work is that the OIG-based dimension we propose is more complicated than the dimensions that have been proposed in the past, like the fat-shattering dimension (which, as we explain, does not characterize learnability in the realizable regression setting). Nevertheless, despite its complexity, this is the first dimension that characterizes learnability in the realizable regression setting. More to that, our work leaves as an important next step to prove (or disprove) the conjecture that the (combinatorial and simpler) \(\)-DS dimension is qualitatively equivalent to the \(\)-OIG dimension. Another future direction, that is not directly related to this conjecture, is to better understand the gap between the fat-shattering dimension and the OIG-based dimension.