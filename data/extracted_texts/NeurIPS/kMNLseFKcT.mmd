# Planning By Active Sensing

Kaushik Lakshminarasimhan jl5649@columbia.edu Zuckerman Mind Brain Behavior Institute, Columbia University

Seren Zhu lt1686@nyu.edu Dora Angelaki da93@nyu.edu Center for Neural Science, New York University

###### Abstract

Flexible behavior requires rapid planning, but planning requires a good internal model of the environment. Learning this model by trial-and-error is impractical when acting in complex environments. How do humans plan action sequences efficiently when there is uncertainty about model components? To address this, we asked human participants to navigate complex mazes in virtual reality. We found that the paths taken to gather rewards were close to optimal even though participants had no prior knowledge of these environments. Based on the sequential eye movement patterns observed when participants mentally compute a path before navigating, we develop an algorithm that is capable of rapidly planning under uncertainty by active sensing i.e., visually sampling information about the structure of the environment. New eye movements are chosen in an iterative manner by following the gradient of a dynamic value map which is updated based on the previous eye movement, until the planning process reaches convergence. In addition to bearing hallmarks of human navigational planning, the proposed algorithm is sample-efficient such that the number of visual samples needed for planning scales linearly with the path length regardless of the size of the state space.

model-based reinforcement learning, sequential decision-making, navigation, maze, gaze

## 1 Introduction

Planning, or the ability to flexibly choose a sequence of actions in a goal-dependent manner, is a cornerstone of human intelligence. Signatures of human planning have been documented in a variety of sequential decision-making paradigms ranging from simple two-step decision-making tasks (Daw et al., 2011; Miller et al., 2017; da Silva and Hare, 2020) to more complex, multi-step navigation tasks (Simon and Daw, 2011; Anggraini et al., 2018; Zhu et al., 2022; de Cothi et al., 2022). A wealth of data suggests that human planning exhibits two key properties - _computational efficiency_ i.e., the convergence time of the planning algorithm should not scale rapidly with the size of the state space (Kool et al., 2017), and _noise robustness_ i.e., the algorithm should overcome any uncertainty in the representation about the model of the world (Hudson et al., 2008; Alhussein and Smith, 2021).

Early models of planning in artificial intelligence systems were formulated as heuristic forward search algorithms (Hart et al., 1968; Pearl, 1984; Korf, 1985) or their backward counterparts (LaValle, 2006) that operated on symbolic world models. The use of appropriate heuristics allows for focusing computations on the task-relevant parts of the statespace, making these algorithms computationally efficient. However, since these models express transition dynamics in symbolic terms, they are not inherently capable of dealing with subjective uncertainty in the transition dynamics.

An alternative formulation of the planning problem using the framework of Markov Decision Process (MDP) has emerged as a standard approach to deal with stochastic transition dynamics (Sutton and Barto, 2017). Planning in the MDP framework is typically solved via dynamic programming (DP) algorithms such as value iteration or policy iteration (Bellman, 1957; Howard, 1960) which entail alternating between policy evaluation and policy improvement. Unlike heuristic search, DP algorithms converge to the optimal solution even if the transition dynamics are stochastic. Model-based planning algorithms such as Dyna (Sutton, 1991; Moerland et al., 2020) allow for reducing stochasticity due to subjective uncertainty about the transition dynamics (epistemic uncertainty) by performing model updates in conjunction with policy updates. However, a common drawback of DP algorithms is that the policy updates are performed across the entire state space in an undirected manner, making them much less computationally efficient than heuristic search.

## 2 Related work and our contribution

Studies on planning have shown that humans minimize computational complexity using strategies such as pruning and decomposition of decision trees (Huys et al., 2012; Solway et al., 2014; Huys et al., 2015), resource-rational planning (Callaway et al., 2018; Ho et al., 2020), or by constructing simplified mental representations of the world (Ho et al., 2022). While such insights help identify useful model classes, they are insufficient to construct a granular algorithm of human planning at the level of individual planning steps. Moreover, above studies do not account for representational noise and thus do not explain how we might plan when our model of the environment is imprecise or wrong. Humans rely on structured eye movements to reduce uncertainty in simple discrimination/detection tasks (Renninger et al., 2007; Yang et al., 2016; Hoppe and Rothkopf, 2019) but it is not yet clear if and how such strategies are used when planning action sequences in complex environments. Recently, recurrent neural network models have been used to explain human eye movements during memory-guided (Lakshminarasimhan et al., 2018; Stavropoulos et al., 2023) and maze-solving (Li et al., 2023; Kadner et al., 2023) paradigms. Such models help identify the computational objective driving eye movements but do not explain the mechanisms by which individual eye movements are chosen in real time.

To develop a granular, algorithmic theory of human planning that accommodates a role for active sensing, here we first analyzed data gathered from a free-form behavioral experiment in which human participants used a joystick to navigate mazes in virtual reality. We found that nearly all trials comprised of an initial planning phase during which participants visually explored a small set of relevant states before navigating to the goal suggesting that planning was performed ahead of time via active sensing rather than in real time. Furthermore, visual exploration of those states unfolded in a sequential manner until participants were able to successfully connect the start and goal states. Based on this spatiotemporal pattern of visual sampling, we propose an algorithm for planning under uncertainty by active sensing whereby an internal model of the world is updated in conjunction with planning

[MISSING_PAGE_FAIL:3]

## 4 Results

### Behavioral task

To model human planning, we used data from a virtual reality (VR) task in which participants navigated to cued reward locations in hexagonal mazes. The experiments used a head-mounted VR system (HTC Vive Pro) with a wide field of view to provide an immersive experience. Participants (\(n=13\)) viewed the environment from a first-person perspective and freely rotated in a swivel chair and used an analog joystick to control their forward and backward motion along the direction in which they were facing (Figure 1A). The program recorded their position in the virtual maze as well as their gaze using a built-in eye tracker. To facilitate quantitative analyses, mazes were designed with a hidden underlying triangular tessellation where each triangular unit constituted a _state_ in a discrete state space (Supplementary Figure 1A) but this discretization was invisible to participants. A fraction of the edges of the tessellation was chosen to be impassable barriers (obstacles). Participants could take _actions_ using the joystick to achieve _transitions_ between adjacent states which were not separated by obstacles. Critically, participants experienced a relatively high vantage point and were able to gaze over the tops of all of the obstacles to gather information about distal transitions through visual exploration (Figure 1A). On each trial, participants had to collect a reward by navigating to a random reward location drawn uniformly from all states in the maze. They had to locate the reward (a banana) and navigate to it after which a new reward for the next trial was spawned without breaking the continuity of the task. In separate blocks, participants navigated to 50 goals in each of five different mazes of variable complexity (Supplementary methods). The simplest of these was an open maze that required no planning, while the rest were structured.

### Signatures of planning

We found that participants navigated to the goal along optimal trajectories in all mazes even though they had no prior experience in any of them i.e., they did not have a model of the environment (Figure 1B; summarized in Supplementary Figure 1B). This suggests that they _planned_ their trajectories before navigating, as confirmed by their velocity traces which showed that participants were typically stationary for a brief period at the beginning of each trial ('planning period') and then navigated without stopping (Figure 1C). Planning duration increased with navigation duration in structured (but not open) mazes implying that planning in these mazes was effortful and that the complexity of planning increased with the path length (Figure 1D).

### Active sensing strategy

Since participants did not have a model of the environment at the beginning of the trial, we reasoned that they built a model by visually exploring the maze during the planning period i.e., by _active sensing_. Therefore, the source of algorithmic complexity is twofold: sample complexity associated with the number of visual samples needed to build a sufficiently good model, and computational complexity associated with performing Bellman backups to infer a good policy. Since random visual exploration is sample inefficient, we hypothesized that participant's active sensing strategy could be dictated by task demands. To test this, weanalyzed their gaze positions during the planning period relative to the trajectory taken while navigating and found two salient features. In the spatial domain, there was a striking correspondence between the two in structured (but not open) mazes suggesting that the active sensing strategy was trial-specific (Figure 1E). Note that this correspondence was not perfect since the trajectories are determined only after the planning process is complete. In the temporal domain, participants performed visual sweeps during which they sampled the states that comprised the trajectory in a sequential manner (Supplementary Figure 2). The total duration of these sweeps increased with the navigation duration since more states need to be traversed as trajectories get longer (Figure 1F). Finally, we note that the planning duration was stable across trials within a block (Figure 1G) suggesting that information gathered about the model was not consolidate across trials but rather learned from scratch on each trial. This is most likely because even the least complex among the structured mazes had a fairly complex transition structure and the points of view differed across trials.

Taken together, the spatiotemporal pattern of gaze during planning suggests that the sampling strategy was: (i) influenced by the starting and reward locations, and (ii) informed by a heuristic which encouraged the exploration to evolve sequentially. In the next section, we propose an algorithm for planning by active sensing that incorporates this sparse sampling strategy to update the world model. However, we do not exactly know how humans reduce the computational complexity i.e., how are their backups organized? We make a parsimonious assumption that Bellman backups are performed only for the set of sam

Figure 1: **Human behavior.****A**. Humans navigated in unfamiliar mazes using a joystick. **B**. An example trial: navigated trajectory (color) with the optimal trajectory overlaid (dashed). Open and solid circles denote starting location and reward location respectively. **C**. Trials typically comprised an initial planning period followed by a navigation period. **D**. Mean planning duration as a function of mean navigation duration across trials. **E**. Example trials showing gaze positions during planning (yellow dots) and the trajectories navigated subsequently (blue traces). **F**. Mean sweeping duration as a function of mean navigation duration. **G**. Mean planning duration as a function of experience.

pled states and immediately after each model update. We demonstrate that this minimal assumption is sufficient to generate a successful plan.

### Planning algorithm

Based on the observations from human behavior, we developed a planning algorithm whose core components are described below and illustrated in Figure 2.

**Model uncertainty.** Without loss of generality, we consider navigation in deterministic mazes such as the ones used in the human experiments. For these mazes, the transition dynamics \(P_{}(s^{}|s,a)=1\) if an action \(a\) allows a transition from state \(s\) to a neighboring state \(s^{}\) and \(P_{}(s^{}|s,a)=0\) otherwise. In case an action fails to bring about a change of state due to the presence of an obstacle, then \(P_{}(s^{}|s,a)=(s^{}-s)\). Note that if the agent has a perfect model of the environment, the subjective transition dynamics upon which the planning algorithm operates would be identical to these deterministic transition dynamics. In the other extreme scenario where the agent has no knowledge of the model, \(P_{}(s^{}|s,a)=P_{}(s|s,a)=\), corresponding to equal probabilities of success and failure of an action to bring about a change of state from \(s\) to \(s^{}\). In practice, the subjective model might be somewhere in-between and we capture this by assuming that the subjective transition model is a weighted sum of the true and noisy transition models:

\[P(s^{}|s,a)=(1-)P_{}(s^{}|s,a)+ P_{}(s^{}|s,a)\] (4)

where \(\) denotes the level of uncertainty.

Figure 2: **Planning algorithm.** Each iteration of the algorithm updates the transition model locally around the state sampled by the gaze. The updated transition model is used to update the value function based on the current policy before performing a policy update.

**'Online' update.** In standard DP algorithms, policy improvements (equation (3)) are performed for all states \(s\). This ignores knowledge of the initial state \(s_{0}\) making them undirected and thus computationally inefficient. In contrast, adaptive real-time dynamic programming (RTDP) algorithm and its variants restrict model/policy updates to a single state and proceed by acting according to the current best policy at that state i.e., they interleave steps of planning and action selection. For this reason, they are considered as online planning strategies. We adopt a similar approach in our algorithm albeit with a subtle but important difference. Like human participants, we endow our agents with active sensing to learn about the consequences of actions at distal states by visually sampling them without physically navigating to those states. Although the resulting model/policy update equations at the sampled states are mathematically equivalent to online algorithms that perform the selected actions, we interpret them as simulated actions rather than real actions. This allows the agent to stay put at \(s_{0}\) throughout planning. This difference can be formally expressed by defining a simulated MDP \(}\) that is identical to the ground MDP \(\), running the algorithm on \(}\) to determine a policy \(\), and then finally setting \(=\) before applying it on the ground. We skip this formalism to keep notations simple.

**Greedy sampling for model and policy updates.** Each iteration includes a model update step in which the subjective transition matrix is updated locally around the chosen state. Model updates are followed by policy evaluation, and policy update in the chosen state. Updates are performed in the initial state \(s_{0}\) in the first iteration, and an action \(a\) is simulated according to the best action for that state according to the current policy. This determines a new sample state \(s_{1}\) for updating in the next iteration, following which the best action is simulated in that state to determine \(s_{2}\) and so on until the \(k^{}\) sample \(s_{k}=s_{G}\). Since new samples are generated by simulating actions at the currently sampled state, sampling takes place sequentially similar to human participants. However, since greedy sampling corresponds to ascending the value gradient, the sampled sequence may be sensitive to the assumptions we make about the value function before the transition model is learned. We demonstrate that a simple initialization strategy that depends only on the knowledge of the reward location works very well in practice (see below).

**Model update.** The transition model update in the \(k^{}\) iteration involves modifying the subjective transition dynamics \(P_{k-1}\) according to:

\[P_{k}(s^{}|s,a) P_{k-1}(s^{}|s,a)+W_{k}(s) P_{k-1} (s^{}|s,a)\] (5)

where \( P_{k-1}(s^{}|s,a)=P_{}(s^{}|s,a)-P_{k-1}(s^{ }|s,a)\) and \(W_{k}(s)=e^{)^{2}}{2^{2}}}\) is a Gaussian weight profile that decays as a function of distance \(d(s,s_{k})\) from the state \(s_{k}\) sampled in the \(k^{}\) iteration. This weight profile was chosen to restrict information gathering to the immediate vicinity of the sampled state and is meant to mimic the effects of filtering by the human fovea.

**Initialization.** The initial sample is assumed to be at the starting location \(s_{0}\). The initial transition model \(P_{0}\) is given by equation (4) where we assumed \(=1\) which corresponds to maximum uncertainty. The policy is initialized to be a random walk such that\(_{0}(a|s)=1/M\)\( s\) where \(M=3\) is the number of actions available in each state due to triangular tessellation of the state space. These choices imply an initial value function \(V_{_{0}}(s)=(I-P_{0}_{0})^{-1}R(s)\) where the reward function \(R(s)=(s-s_{G})\) is the only term that is trial-specific.

To understand how the algorithm plans a trajectory, consider the example task shown in Figure 3A where the objective is to determine the optimal policy to navigate between the states marked by the colored circles (from green to red). The panels in the top row show the

Figure 3: **Model performance.****A.** Example simulation. _Top row_: The true transition dynamics (_left_), and the corresponding optimal value function (_middle_) and optimal policy (_right_) computed using the standard dynamic programming approach for an example trial. Starting location and goal location are shown in green and red respectively. Policy unfolding from the starting location is highlighted in black. _Middle row_: The assumed transition dynamics, value function and (random walk) policy at initialization (\(k=0\)). _Bottom row_: Similar to middle panels, but after convergence of the proposed algorithm (\(k=\)). A video of the full simulation is available at https://tinyurl.com/planning-algo. **B.**_Top_: Comparison of the path length of the policy determined by the algorithm against the optimal path length across trials _Middle_: Path length (normalized by optimal, black) and number of planning steps (cyan) as a function of initial model uncertainty. _Bottom_: Success rate as a function of the number of planning steps.

mathematical quantities determined by a standard dynamic programming (DP) approach (policy iteration). This technique uses the true transition model (configuration of the maze, top row - left) to calculate the optimal state value function (heat map, top row - middle) and the corresponding optimal policy (vector field, top row - right). The value function and policy determined by DP serves as the ground truth for assessing the performance of the proposed algorithm. In contrast to DP techniques, we assume that the subjective transition model to be noisy such that the resulting model is very fuzzy with transitions between all neighbors having a probability close to 0.5 (middle row - left). Under this noisy transition model, a naive random walk policy assumed at initialization (middle row - right) evaluates to a value function that is markedly different from optimal (middle row - middle panel) before any updates are applied. Despite this conservative initialization, the algorithm converges to the true optimal policy (bottom row - right) unfolding from the start towards the goal and the corresponding value function resembles the optimal value function (bottom row - middle). In contrast, the subjective transition structure is still noisy in large swathes of the state space outside of the regions sampled by the algorithm (bottom row - left). Thus, the algorithm is able to determine the optimal policy with only a modest number of samples. A closer inspection of the policy function determined by this algorithm shows that, unlike the policy determined by dynamic programming, this policy is essentially random everywhere except for the sampled states. This demonstrates the directed nature of this algorithm imposed by the greedy sampling such that the final policy is only applicable to a subset of the state space. The specific subset depends on the subjective transition model, the starting state, and the goal state. Nevertheless, this specificity is precisely what makes the planning algorithm achieve low sample and computational complexity. An animation showing how the plan is gradually built up is available at https://tinyurl.com/planning-algo. Figure 3B summarizes the performance of the algorithm. The algorithm converges to the optimal solution on most trials (top) even when the model is completely unknown (\(=1\), middle) and across mazes of varying complexities (bottom).

The sampling strategy of the algorithm can be better understood by examining the spatiotemporal dynamics of the samples \(s_{k}\) (Figure 4A) alongside the dynamics of the state value function, \(V(s_{k})\), of the sampled states (Figure 4B). Since the algorithm follows a greedy sampling strategy that follows the spatial gradient of the value function, the value of the sampled states generally increases. However, there is a precipitous drop in the middle of planning after about \(k=17\) iterations in example 1 (left panel, black triangle). Why does this happen? Notice that the new sample encountered on this iteration of planning reveals a (previously unknown) obstacle preventing access towards the goal state. When this new information about the transition model is used to perform model update (Equation 5), it results in downgrading of the values of the states affected by that obstacle during the policy evaluation step (Equation 2) which triggers a visual detour in the sampling strategy after the policy improvement step (Equation 3). On the other hand, value increases monotonically when samples are relatively unsurprising as in example 3 (right panel). Across simulations, we found that non-monotonic value dynamics are more prevalent in the most complex arena (65% of the trials) compared to the simplest one (12%). These findings illustrate how planning in the real-world can benefit from a rich interplay between information gathering and incremental Bellman backups. Importantly, the subjective value dynamics accompanying visual exploration can serve as a prediction for neuroscience experimentsthat probe the neural basis of visually-guided navigation at a single-trial resolution (Gulli et al., 2020; Noel et al., 2022; Lakshminarasimhan et al., 2023). Specifically, we predict that activity dynamics of neurons encoding the subjective value should follow a temporal profile shown in Figure 4B when the spatiotemporal profile of gaze is given by Figure 4A.

### Model comparison

To test whether the particular strategy of sampling information in a sequential manner is in fact efficient, we constructed a variant of the algorithm in which the sampling strategy corresponded to random exploration. The policy updates still happened in a sequential manner in this variant. Across simulations of trials across mazes used to test human participants, we found that the median number of backups (planning steps) needed for convergence was indeed substantially reduced when sampling was performed sequentially in a coordinated manner with the policy updates (Figure 5A; sequential sampling: \(16 2\) planning steps, random sampling: \(57 6\) planning steps).

Figure 4: **Interplay between information gathering and value updating.****A**. Spatial locations (jittered to avoid overlap) sampled by the proposed algorithm during three example trials. **B**. Subjective value of the sampled states, \(s_{k}\) as a function of planning iteration, \(k\) on the same trials. The example simulated in Figure 3 is shown in the leftmost panels. Note the decrease in value during the course of planning in spite of following a greedy sampling strategy in two of these examples. This decrease happens when newly gathered information unexpectedly reveals the presence of a previously unknown obstacle on the path towards the goal. Large red and cyan circles denote starting location and goal location. Gaze samples are color coded to denote the sequence of planning iterations (blue to red).

Techniques that use incremental backups such as prioritized sweeping are particularly useful for reducing the computational complexity of planning in large state spaces. Since the proposed algorithm does not use a prioritization scheme, we wanted to know whether it

Figure 5: **Efficiency of the proposed algorithm.****A**. Cumulative probability distribution of the number of backups across trials simulated using sequential information sampling (proposed algorithm, black) and using random information sampling (gray) strategies. Policy updates were performed sequentially in both cases. **B**. Mean number of backups as a function of the total number of states in the simulated mazes for the two algorithms shown in A. **C**. Number of planning steps of the proposed algorithm, compared against those obtained when ablating (by randomization) one feature of the algorithm (initialization, order of model updates, order of Bellman backups).

could scale to large state spaces. To test this, we simulated trials by constructing mazes that differed in the total number of states ranging from 6 to 294. We found that the number of planning steps increased only marginally with the number of states (Figure 5B - black), and increase that was entirely accounted for by an increase in the average path length of trials in larger mazes. Moreover, the algorithm outperformed the variant with random sampling (Figure 5B - gray) suggesting that both sample efficiency and computationally efficiency are robust to the size of the state space. Finally, we probed whether the performance of the algorithm largely depended on appropriate initialization of the value function, ordering of model updates according the the value-gradient heuristic, or the sequence of Bellman backups by ablating each feature separately. We found that all three features were critical for efficient convergence (Figure 5C).

## 5 Limitations and future work

Although the proposed algorithm was successful at finding the optimal path in the environments we tested, it may yield suboptimal policies under certain conditions. Since active sensing is guided by local value gradients, one could construct mazes in which the initial direction of the gradient pushes the exploration towards a long-winding path. A rigorous theoretical treatment is required to understand the precise conditions under which this strategy is reasonable. Another direction we have not yet explored is a systematic model comparison against alternative algorithms such as prioritized sweeping or \(n\)-step look-ahead which may also co-exist with active sensing. Moreover, it would be intriguing to investigate how this algorithm is implemented neurally. Planning is associated with sequential neural activity in the hippocampus (Brown et al., 2016; Miller et al., 2017; Mattar and Daw, 2018; George et al., 2021; Zhu et al., 2023). Testing whether such neural dynamics could serve as a substrate for planning by active sensing is an important topic for future work. Finally, while the current study focused on visually-guided navigation, visual information might be either unnecessary or insufficient for some planning problems. Whether the proposed model can inspire useful computational strategies in such settings remains to be seen.

## 6 Conclusion

Planning and active sensing have traditionally been modeled separately. Planning has traditionally been characterized as a covert, internal information search over past experiences while active sensing is an overt, external search to gather new information. There is a growing realization that these two search strategies may be intertwined for temporally extended behaviors under naturalistic conditions (Lakshminarasimhan et al., 2020; Hunt et al., 2021). By analyzing gaze patterns, we find that human planning strategy during navigation is consistent with an algorithm in which both searches are coordinated and carried out simultaneously around the same set of states, a strategy we refer to as 'planning by active sensing'. The algorithm differs from traditional incremental approaches to dynamic programming in that planning (policy update) is performed in conjunction with learning (model update). It also differs from architectural frameworks such as 'Dyna' that allow for simultaneous learning and planning in that model updates are performed by visual sampling rather than physically interacting with the world which could prove too costly. Instead, the strategy is conceptually similar to adaptive RTDP and identifies closely related real time algorithms as a promising direction for achieving human-like planning in machines.