# BeaverTails: Towards Improved Safety Alignment

of LLM via a Human-Preference Dataset

 Jiaming Ji\({}^{*}{}^{1}\) &Mickel Liu\({}^{*}{}^{2}\) &Juntao Dai\({}^{*}{}^{1}\) &Xuehai Pan\({}^{2}\) &Chi Zhang\({}^{1}\)

&Ce Bian\({}^{1}\) &Boyuan Chen\({}^{1}\) &Ruiyang Sun\({}^{1}\) &Yizhou Wang\({}^{ 12}\) &Yaodong Yang\({}^{ 1}\)

\({}^{1}\)Institute for Artificial Intelligence \({}^{2}\)CFCS, School of Computer Science

Peking University

{jiamg.ji, mickelliu7, jtd.acad}@gmail.com, xuehaipan@pku.edu.cn

{preceptormiriam, cbian393}@gmail.com, cbyll1@stu.pku.edu.cn,

ruiyangsun02@gmail.com, {yizhou.wang, yaodong.yang}@pku.edu.cn

Equal contribution, random ordering. \({}^{}\)Corresponding author.

###### Abstract

In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.

Warning: this paper contains example data that may be offensive or harmful.

## 1 Introduction

The recent advent of large language models (LLMs) [1; 2; 3; 4; 5] promises vast transformative potential across multiple sectors, from healthcare [6; 7; 8] and education [9; 10; 11] to robotics [12; 13; 14] and commerce . However, as these models grow in complexity and influence, ensuring their alignment with human values and safety becomes increasingly critical. If left unchecked, LLMs can amplify misinformation, enable harmful content, or yield unintended responses that can cause significant negative societal impact [16; 17; 18; 19]. Recent papers have highlighted significant safety risks associated with the deployment of LLMs in real-world applications, prompting public concern [20; 21; 22].

The urgent need for safety alignment in LLMs has garnered substantial attention from both academia and industry. This surge of interest has led to noteworthy contributions aimed at making LLMs safer. Among these are innovative alignment techniques, namely "red-teaming", extensively employed by research groups at Anthropic and DeepMind [23; 18]. Red-teaming involves a rigorous adversarial process that intentionally seeks to expose the potential for harmful outputs from LLMs, which are then refined to decrease the likelihood of such harmful occurrences. Anthropic has gone a step further bysharing their red-team dataset publicly, which contains human-written prompts and human-preference data . Another alignment technique, known as Reinforcement Learning from Human Feedback (RLHF), has also demonstrated promising results [24; 25; 11]. In fact, OpenAI's GPT-4 technical report disclosed their use of safety-relevant RLHF training prompts and rule-based reward models (RBRMs)  to empower safety alignment. While these alignment techniques can be applied in parallel, their effectiveness hinges on the availability of comprehensive human feedback, which necessitates costly large-scale data labeling operations.

In light of advancing efforts for the safety alignment of LLMs, we are pleased to open-source our Question-Answering (QA) dataset, BeaverTails. Inspired by our sibling project, PKU-Beaver, focused on Safe RLHF , the BeaverTails dataset aims to facilitate the alignment of AI assistants towards both helpfulness and harmlessness. Our dataset brings forward two types of annotations: **(1)** Annotated safety meta-labels for over 330,000 QA pairs, derived from more than 16,000 unique _red-teaming_ related prompts. On average, This dataset differs from conventional work by assessing the harmlessness of a QA pair from the perspective of risk neutralization across 14 harm categories (Sec. 3.3). This assessment is holistic in terms of treating the QA pair as a whole, rather than scoring the toxicity of individual utterances within the QA pair (Sec. 4.1). **(2)** A collection of two distinct sets of human-preference data, each containing over 360,000 pairs of expert comparisons. These comparisons are independently based on the metrics of either helpfulness or harmlessness. To our knowledge, BeaverTails stands as the first dataset to disentangle harmlessness and helpfulness from the human-preference score, thus providing separate ranking data for the two metrics (Sec. 3.4). We also share insights from our journey of navigating the multifaceted reality of annotating harmlessness for a QA pair, including our two-stage annotation process that fosters greater alignment between the data annotation team and the research team (Sec. 3.2). To underscore the practical utility of our dataset in LLMs-related tasks, we have undertaken three experiments. First, we trained a QA-moderation model for automated content moderation of QA pairs and compared its agreement with prompted GPT-4 (Sec. 4.1). Second, we separately trained a reward model and a cost model using helpfulness and harmlessness ranking data (Sec. 4.2). Third, we applied the reward and cost models obtained from the second experiment to fine-tune the Alpaca-7B model . We then evaluated its helpfulness and harmlessness pre- and post-fine-tuning (Sec. 4.3). Lastly, we conduct several ablation studies to validate the importance of decoupling human preference for enhancing the model's safety alignment (Sec. 4.4), and visualize its difference between the original model in Sec. 4.5.

We sincerely hope that the BeaverTails dataset, along with the applications showcased herein, will contribute meaningfully to the progress of research in LLMs safety alignment.

## 2 Related Work

Question-Answering (QA) Dataset with Human-Preference AnnotationHuman-preference annotations guide the training of language models towards responses that align more closely with the "_Helpful, Honest, and Harmless_" (3H) objectives [28; 25]. Currently, there are multiple datasets that provide QA pairs with human-preference data. BAD  by MetaAI is a dialogue dataset in which annotators attempt to elicit unsafe behaviors from the chat-bot using unsafe utterances, and all utterances are annotated as offensive or safe. RealToxicityPrompts consists of 100k sentences annotated with toxicity scores from Perspective API [30; 31]. The SHP  dataset consists of 385k collective human preferences regarding the helpfulness of responses to questions and instructions across 18 different subject areas. Anthropic in 2022 contributed human-preference datasets about helpfulness and harmlessness  and a red teaming dataset  whose prompts serve as a basis for our dataset.

Evaluating Toxicity in Large Language ModelsAssessing and quantifying the extent to which a large language model produces harmful, offensive, or otherwise inappropriate responses. Many recent studies devise procedures [33; 34; 35; 19] and guidelines [36; 17; 37; 38] to evaluate the harmfulness and toxicity in LLM's outputs. TrustfulQA is an evaluation dataset that consisted of 817 human-written questions that aim to evaluate the trustfulness in the responses generated by language models. BBQ  examines social biases against various identity groups in QA tasks. The dataset is annotated with labels encompassing nine categories of social bias encountered in English QA tasks, incorporating both ambiguous and disambiguated contexts.

Automated Content Moderation for Language ModelsAutomated Content Moderation for language model outputs refers to the process of reviewing, assessing, and regulating the responses or outputs produced. The aim is to ensure that these outputs adhere to set community guidelines,ethical standards, and policies, thereby preventing inappropriate, harmful, offensive, or misleading content from being disseminated. Two of the most notable open-access automated content moderation are Perspective API [30; 31] and OpenAI Moderation API . Perspective API, released by Google Jigsaw, is one such popular automated service that provides an array of scores along 8 dimensions (Toxicity, Severe_Toxicity, Insult, Sexually_Explicit, Profanity, Likely_To_Reject, Threat, and Identity_Attack) for a given text input. OpenAI Moderation API  will flag a given input as harmful if the score of any harm category (from seven categories: hate, hate/threatening, self-harm, sexual, sexual/minors, violence, violence/graphic) exceeds a pre-defined probability threshold.

**Reinforcement Learning with Human Feedback (RLHF)** RLHF  aims to optimize the Language models (LMs) to generate content that is desired by humans, with respect to helpful, honest, and harmless . Recently, there has been a notable surge in the adoption of this learning method to significantly enhance model performance across various natural language processing tasks, including text summarization [42; 43; 44], instruction-following [45; 27; 46], and mitigating harmful effects [25; 47]. From a high-level perspective, the process involves retrofitting a generation quality ranking model using human feedback to derive a reward function, which is utilized to assign a holistic score to evaluate the quality of a generated output. Subsequently, the LMs undergo further training through RL methods such as Proximal Policy Optimization (PPO) [48; 49]. Previously, the PPO algorithm has been effectively applied across a diverse range of domains, such as computer vision [50; 51; 52] and robotics [53; 54; 55].

## 3 Dataset

### Dataset Composition

This section describes the key specifications of the BeaverTails dataset. We will refer to a "QA pair" as a combination of a single question (or prompt) and its corresponding answer (or response). We have released two iterations of the BeaverTails dataset (link):

**BeaverTails-30k**

* Annotated 30,207 QA-pairs across 14 potential harm categories, which correspond to 7,774 unique prompts. Of these prompts, 75.3% received three unique responses, 20.7% received six unique responses, and the remaining 4.1% received over six unique responses.
* Within the total set of 30,207 QA pairs, 42.68% were assigned the **safe** meta-label, while the remaining 57.32% were categorized under the **unsafe** meta-label.
* From the total pool of 30,207 QA pairs, we acquired 30,144 pairs of human-preference annotations separately for the helpfulness and harmlessness metrics of the responses.

**BeaverTails-330k**

* We expanded the dataset to contain annotations for 333,963 QA pairs across 14 potential harm categories, which correspond to 16,851 unique prompts and 99,734 unique QA pairs. Unlike BeaverTails-30k where each QA pair is assigned to only one crowdworker, in BeaverTails-330k each QA pair on average received 3.34 annotations from different crowdworkers.

Figure 1: The pie charts demonstrate the distribution of our data across the 14 potential harm categories. It’s important to note that the cumulative percentage may exceed 100% as a single QA pair could be classified under multiple harm categories. **Left:** QA pairs are annotated with a meta-label, safe or unsafe. **Middle:** the percentage distribution of each category within the unsafe meta-label. **Right:** a closer look at all minor categories that make up less than 6% of the total unsafe data.

* Within this dataset, 44.64% were assigned the **safe** meta-label, while the remaining 55.36% were categorized under the **unsafe** meta-label.
* We acquired 361,903 pairs of human-preference annotations separately for the helpfulness and harmlessness metrics of the responses. The inter-crowdworker agreement rate: safety meta-label = 81.68%, helpfulness preference = 62.39% and harmlessness = 60.91%.

Additionally, we solicited crowdworkers to assign confidence scores to their annotations, applicable to both the classification and response-ranking tasks. The confidence spectrum extended from "very uncertain" and "uncertain" to "certain" and "very certain", corresponding to a scale of 0 to 3.

### Data Collection and Annotation Process

Generating QA pairsOur study involves a collection of over 28k red-team prompts derived from the HH Red-Team dataset  and . Given the dialogical nature of these datasets, we specifically selected the first question that initiated the interaction between the _human_ and the _AI assistant_. These questions were meticulously crafted by Ganguli et al. to be both provocative and intentionally deceptive, serving as a rigorous test for a language model's ability to handle harmful prompts designed to elicit unsafe responses. For questions perceived as overly terse or incomplete, we incorporated additional contextual information during pre-processing. The average word count (using the regex /+/) for each prompt is 13.61.

We then prompt the Alpaca-7B model  to generate multiple unique responses per question across the set of 7.7k unique questions (chosen from the previously mentioned set of red-team prompts) for BeaverTails-30k. To ensure generation diversity and enrich the range of outputs, we modulate the sampling parameters as follows: temperature is set to 1.5, and the maximum token length is limited to 512, with top_k and top_p values configured at 30 and 0.95, respectively. We measure the average word count (using the regex /+/) and observe a mean of 61.38 words per response across the resultant 30k responses.

Two-Stage Annotation ProcessIn an effort to annotate our dataset with human-preference data efficiently, we engaged a team of over 70 crowdworkers (annotators) - all of whom possess at least a college-level education and a proficient command of English. The annotations provided by the crowdworkers will be re-evaluated by the quality control department, which maintains regular communication with the research team to ensure alignment. The task of annotating a QA pair in the BeaverTails dataset involves a two-stage annotation process.

During the first stage, the QA pair is annotated through a multi-classification process involving 14 harm categories (see Sec. 3.3), leading to the assignment of a corresponding safety meta-label. To facilitate the QA-moderation task during LLMs deployment (see Sec. 4.1), we advocate for assessing the harmlessness of a QA pair from a risk neutralization perspective, rather than relying solely on the toxicity score of individual utterances within the QA pair provided by content moderation systems. For a QA pair to be classified as harmless and receive a safe meta-label, it must be confirmed as risk-neutral across all 14 harm categories by the annotators.

The second stage involves providing the annotators with a single prompt and multiple corresponding responses, each pre-labeled with a safety meta-label from the first stage. The annotators are then

Figure 2: Two-Stage Annotation Process. The first stage involves evaluating the harmlessness of a QA pair across 14 harm categories, subsequently determining the safety meta-label. The second stage then ranks responses to a prompt according to their helpfulness and harmlessness.

tasked with offering two separate rankings for the responses, based on their harmlessness and helpfulness (see Sec. 3.4). In rare cases where an annotator deems the provided safety meta-labels to be inaccurate, they have the option to flag the corresponding response and continue ranking based on their presumed safety meta-labels. Any comparison data linked to the flagged response will be directly re-evaluated and corrected by the research team.

### Classification of QA Pairs by Potential Harm

This dataset assesses QA pairs with respect to 14 different harm categories. The definition for these categorizations took major inspiration from previous research on the harmful generation of LLMs [18; 40; 37]. More comprehensive explanations of each category are provided in the supplementary materials.

* Hate Speech, Offensive Language
* Discrimination, Stereotype, Injustice
* Violence, Aiding and Abetting, Incitement
* Financial Crime, Property Crime, Theft
* Privacy Violation
* Drug Abuse, Weapons, Banned Substance
* Non-Violent Unethical Behavior

It is crucial to acknowledge that these categories are not exclusive; indeed, a moderate level of intercorrelation is evident among some. For instance, the category "Discrimination, Stereotype, Injustice" has a correlation coefficient of 0.408 with the category "Hate Speech, Offensive Language". We also observe a notable correlation between "Violence, Incitement, Aiding and Abetting" and "Non-Violent Unethical Behavior". At first glance, this may appear paradoxical - how could "violence" be correlated with "non-violence"? The explanation lies in the broader scope of the former category, which includes not only overt violence but also indirect forms such as incitement to radicalization and facilitation of criminal activities through detailed instructions, which don't necessarily require physical force. Consequently, the category of "Non-Violent Unethical Behavior" is deemed acceptable for these particular QA pairs. Certain categories exhibit negligible correlation with others. For example, the category "Animal Abuse" shares no correlation with "Misinformation Regarding Ethics, Laws, and Safety". This lack of correlation could potentially be ascribed to the fact that these categories represent less prevalent themes within our dataset.

### Human-Preference on Harmlessness and Helpfulness

In this section, we discuss our approach for evaluating the harmlessness of a Question-Answer (QA) pair, the helpfulness of a response to a prompt, and the subsequent ranking of responses relative to these two metrics. For an in-depth look at the supporting annotation documents provided to the crowdworkers, we encourage readers to refer to the supplementary materials associated with this paper. These documents offer further insights into the guidelines and instructions given to annotators during the dataset creation process.

Harmlessness of a QA PairThe harmlessness of a QA pair is evaluated based on ethical considerations and safety implications inherent in the interaction. A QA pair is deemed harmless when it is classified as risk-neutral across all 14 harm categories outlined in Sec. 3.3. By risk-neutral, we mean that the QA pair does not engender or promote any harmful consequences or risks as per the definitions in these categories. Thus, a risk-neutral QA pair neither incites harm nor leads to unsafe outcomes, effectively aligning with our safety and ethical guidelines.

Helfulness of a ResponseThe helpfulness of a response pertains to how effectively it addresses a given prompt. This measure is independent of the harmlessness of the response, as it focuses solely

Figure 3: Correlation Table presents the relationships among the 14 categories.

on the quality, clarity, and relevance of the provided information. Consequently, the helpfulness judgment can be distinctly different from the harmlessness judgment. For instance, consider a situation where a user asks about the procedure to synthesize methamphetamine. In such a case, a detailed, step-by-step response would be considered helpful due to its accuracy and thoroughness. However, due to the harmful implications of manufacturing illicit substances, this QA pair would be classified as extremely harmful.

Ranking of ResponsesOnce the helpfulness and harmlessness of responses are evaluated, they are ranked accordingly. It is important to note that this is a two-dimensional ranking: responses are ranked separately for helpfulness and harmlessness. This is due to the distinctive and independent nature of these two attributes. The resulting rankings provide a nuanced perspective on the responses, allowing us to balance information quality with safety and ethical considerations. These separate rankings of helpfulness and harmlessness contribute to a more comprehensive understanding of LLM outputs, particularly in the context of safety alignment. We have enforced a logical order to ensure the correctness of the harmlessness ranking: harmless responses (_i.e._, all 14 harm categories risk-neutral) are always ranked higher than harmful ones (_i.e._, at least 1 category risky).

## 4 Task and Analysis

In this section, we will present a series of experiment results, including the performance of post-RLHF finetuned models and the efficacy of the reward models and the moderation models, on training large language models using the BeaverTails-30k dataset.

### QA Moderation and Safety Evaluation of Different Models

Traditional methodologies for content moderation in Question-Answering (QA) tasks assess the harmfulness of a QA pair by evaluating the toxicity of individual utterances. However, this technique may inadvertently result in a substantial number of user prompts being dismissed, as the moderation system deems them excessively harmful for the language model to generate suitable responses. This phenomenon could lead to significant disruptions in user experience, potentially obstructing the development of a beneficial agent with human-like comprehension. Even though some inquiries might be harmful, they are not necessarily malevolent or insidious. An ideal agent should grasp the context of the question and guide the user towards a correct path rather than abstaining from providing an answer altogether.

Hence, as shown in Figure 4, we advocate for a novel paradigm in content moderation for QA tasks - referred to as "QA moderation". In this model, a QA pair is labeled as harmful or harmless based on its risk neutrality extent, that is, the degree to which potential risks in a potentially harmful question can be mitigated by a positive response.

The safety evaluation shown in Figure 5 employs a dataset of 140 red-team prompts, evenly distributed across 14 harm categories. These prompts were utilized to prompt four distinct LLMs, yielding 140 QA pairs for each model. The generated outputs were subsequently assessed for harmlessness by

Figure 4: Comparison of different content moderation methods applied to LLMs. **(a)** The conventional approach to content moderation often leads to prompt rejection, resulting in an unhelpful AI assistant and diminishing user experience. **(b)** QA moderation, considering risk neutralization between the question and answer, empowers multi-round rejection sampling, thus fostering a harmless and helpful AI assistant. **(c) and (d)** The key differences between moderation and QA moderation lie in their respective input formats and the users’ perception of these varied approaches to content moderation.

three evaluation entities: _QA-moderation, GPT-4 (Prompted), and Human Feedback_, the latter of which was sourced from our previously introduced data annotation team.

Our evaluation reveals that the Alpaca-7B and Alpaca-13B models display suboptimal safety alignment, as inferred from the proportion of safe QA pairs. Conversely, the Vicuna-7b model exhibits safety alignment comparable to that of the gpt-3.5-turbo model. There is a high degree of consensus among the three evaluation entities, reflected in the percentage of QA pairs where two evaluators agree. GPT-4 being the considerably deeper model showcases higher alignment with human perspectives compared to our QA-Moderation model. The evaluation results further suggest greater disagreement regarding the safety meta-label between evaluators when models lack adequate safety alignment (_i.e._, Alpaca-7B and Alpaca-13B). In contrast, models with robust safety alignment (_i.e._, Vicuna-7b and gpt-3.5-turbo) witness significantly fewer disagreements. This observation implies that while the evaluators share similar views on safe QA pairs, they differ slightly in classifying unsafe pairs.

### Training Reward and Cost Models

The training of reward and cost models can be utilized for downstream safety alignment tasks, such as RLHF subjected to safety constraints . We applied a train-test split of 9:1 and evaluated the performance of these models on the test set, as presented in Figure 6. We adopted the Bradley-Terry (BT) model for preference modeling and formulated the training objectives of the reward and cost models as negative log-likelihood loss for binary classification:

\[_{R}(;_{R}) =-_{(_{w},_{l})_{R}} [(R_{}(_{w})-R_{}(_{l}))]\] (1) \[_{C}(;_{C}) =-_{(_{w},_{l})_{C}} [(C_{}(_{w})-C_{}(_{l})) ]-_{_{C}}[(C()_{C}())]\] (2)

The Reward Model (\(R\)), parameterized by \(\), and the Cost Model (\(C\)), parameterized by \(\), are derived from fine-tuned Alpaca-7B models  that connect to linear heads. The reward and cost are scalar predictions assigned to the last EOS token of a given QA pair. \(()\) is the sigmoid function. \(_{C}\) and \(_{R}\) denote the training dataset for the reward and cost models, respectively. \(x\) denotes context and \(y\) denotes generated tokens. \(_{w}=(x,y_{w})\) and \(_{l}=(x,y_{l})\), and \(_{w},_{l}\) denote QA pairs that are favored and disfavored given the metric specific to a particular dataset, respectively. The sign function for cost, \(_{C}()\), returns \(-1\) for safe text and \(+1\) for unsafe text.

### Safe Reinforcement Learning with Human Feedback (Safe RLHF)

Utilizing properly trained static preference and cost models, as detailed in Sec. 4.2, we can approximate human preferences regarding the harmlessness and helpfulness of an LLM's response to a given prompt. Following the setting of safe reinforcement learning (SafeRL) [57; 58], we applied a Lagrange version of the PPO algorithm , namely PPO-Lagrangian, where the key difference lies in the usage of an adaptively optimized coefficient (\(\)) for the Lagrangian term that controls the weighting of cost in the training objective. Given a reward and a cost model trained by the objectives shown in 1 and 2, the optimization objective for our LLM policy is as follows:

\[_{}_{ 0}_{x,y_{ }(|x),=(x,y)}[-R_{}()+ C_{ }()]\] (3)

    & **Reward Model Accuracy** & **Cost Model Sign Accuracy** & **Cost Model Preference Accuracy** \\  Evaluation Dataset & 78.13\% & 95.62\% & 74.37\% \\   

Table 1: Performance metrics for the reward and the cost models

Figure 5: Proportion of safe QA pairs and mutual agreement ratio, as flagged by three distinct evaluators across four different models. **Bar Chart:** Proportion of safe QA pairs. **Line Chart:** Agreement ratio.

In this equation, \(x\) and \(y\) denote the input prompt and the text generation given the input prompt produced by the LLM policy \(_{}\), therefore \(=(x,y)\) is a QA pair. The objective encourages maximized reward and minimized cost. Updating the Lagrangian coefficient (\(\)) is governed by gradient descent in the direction of the cost model. The training objective is subjected to the constraint of a non-negative \(\). Further algorithmic details can be found in the Safe-RLHF paper .

Figures 6(a) and 6(b) provide a comparative analysis of the distribution pertaining to the Alpaca-7B model both before and after the application of RLHF supplemented with a safety constraint. A leftward shift observed in the cost distribution (Figure 6(a)) indicates a decrease in safety cost, consequently leading to an enhanced harmlessness in model responses to red-team prompts. Additionally, the rightward shift observed in the reward distribution (Figure 6(b)) points to the increased helpfulness in the model responses to user prompts. Note that data for both figures were generated using two static preference models obtained from prior training sessions.

### Ablation Study and Research Questions

The purpose of this ablation study is to investigate the following research questions: **(RQ1)** Does utilizing rankings in cost specifically provide a measurable benefit versus a classifier-based cost model? **(RQ2)** How does the modeling of decoupled human preference compare to the original single human preference score? **(RQ3)** How does the model train our dataset compared to a model trained on a previous dataset (_e.g._ HH-RLHF)?

In Table 2, we present a series of ablation studies to answer these questions. Safe-RLHF: Our proposed method leverages both cost and reward models and is trained using the PPO-Lagrangian

Figure 6: Score Distributions of the test set from the static preference models trained with the training set. **Upper and Bottom Left:** Distributions of reward scores. These diagrams mark that the (helpfulness) reward is not significantly correlated with the safe meta-label. **Right**: Distributions of cost scores. The distinct separation between safe and unsafe score distributions serves as validation.

Figure 7: Cost and Reward Distributions for the Alpaca-7B and Alpaca-7B + Safe RLHF Models

[59; 60] algorithm. **PPOL-classifier-mean**: employs the PPO-Lagrangian algorithm but replaces the cost model with an ensemble of 14 binary classifiers, akin to the approach in the DeepMind Sparrow . The cost is computed as the mean probability produced by these classifiers. **PPOL-classifier-max**: similar to PPOL-classifier-mean but utilizes the max probability instead of the mean. **HH-PPO**: a reward-shaping PPO method trained on the HH-RLHF dataset . **PPO**: a reward-shaping PPO method trained on a "mixed" human preference dataset, serving as the ablation study. We instructed our data annotation team to rank the data based on a composite of helpfulness and harmlessness preferences.

**(RQ1):** Safety fine-tuning based on rankings in cost outperforms the classifier-based cost model. Interestingly between **PPOL-classifier-mean** and **PPOL-classifier-max**, the former underperforms in comparison to the latter. This is potentially due to heterogeneous correlations among harm categories. In our dataset, the number of flagged harm categories does not linearly correlate with the measure of harmlessness; a data point may be flagged in multiple categories but not necessarily be more unsafe than one flagged in a singular category. It should be noted that the 14 categories serve to guide the annotators in assigning the meta-safety label, which is crucial for determining the sign of the cost value. **(RQ2):** The decoupling of human preference yields performance benefits. **PPO**, the inferior performance of models trained with this method is likely due to the inherent ambiguity introduced during the data annotation phase. Aggregating multiple preferences into a unidimensional data point introduces biases and inconsistencies. This tension between helpfulness and harmlessness in RLHF training is also substantiated in other literature, such as [4; 25]. **(RQ3):** From the observation that Safe-RLHF outperforms **HH-PPO**, the dataset is a meaningful extension of the existing work The performance of **HH-PPO** is suboptimal. The HH-RLHF dataset comprises multi-round conversations, where not all utterances strongly pertain to either helpfulness or harmlessness. During the evaluation, we observed that **HH-PPO** models often either abstain from responding to user queries or generate responses that lack sufficient details.

### Qualitative Results

Table 3 presents the model outputs from Alpaca-7B and Safe-RLHF when faced with red-team prompts, which are used to assess the safety level of the model. Compared to the baseline, Safe-RLHF has demonstrated a substantial improvement in delivering harmless responses. Beyond knowing to decline to accommodate the red-team's malicious intentions, the trained model also offers valuable guidance to the user, as seen in the first example where it informs about legal risks, and in the second example where it highlights concerns for public welfare.

For more examples of comparisons between the models, see Appendix F. However, the Safe-RLHF model is not yet flawless in countering all malicious prompts. In Appendix H, we have highlighted a few instances where the fine-tuned model still assists with the user's harmful requests, although it should be noted that it is largely influenced by the temperature setting of the sampling method.

## 5 Discussion

Recognizing the risks associated with LLMs, the promise of these models for societal good is contingent upon a persistent focus on safety alignment throughout the development and deployment of the models. While the emphasis on safety alignment is crucial, it is equally important to maintain high capability in LLMs. Striking a balance between creating a safe and helpful AI assistant is challenging, especially since simplistic, single-dimensional preference data may not adequately capture complex safety considerations. Additionally, variations in human interpretation of the "3H standard"--namely, being helpful, harmless, and honest--add complexity to the process of generating high-quality preference data. Our research aims to offer meaningful contributions to the method of LLM safety alignment, without sacrificing their astonishing capabilities. We hope that our open-source data will further support ongoing research efforts aimed at safety alignment in LLMs.

    & Safe-RLHF & **PPOL-classifier-max** & **PPOL-classifier-mean** & **HH-PPO** & **PPO** \\ 
**Helpfulness** & 85.57\% & 74.00\% & 69.43\% & 64.93\% & 65.07\% \\ 
**Harmlessness** & 82.57\% & 64.50\% & 59.07\% & 66.21\% & 68.64\% \\   

Table 2: Model Win Rates against Alpaca-7B (Evaluated by prompted GPT-4)

### Ethics and Impact

The BeaverTails dataset will be made available under the terms of the **CC BY-NC 4.0** license. With its comprehensive composition of safety meta-labels, harm category classifications, and human-preference ranking annotations concerning helpfulness and harmlessness, this dataset holds immense potential as a resource for developing beneficial AI assistants aligned with optimal helpfulness and harmlessness. However, we acknowledge an inherent risk: the same dataset could theoretically be used to train AI assistants in a harmful or malicious manner. As the creators of the BeaverTails dataset, we are committed to fostering the development of helpful, safe AI technologies and have no desire to witness any regression of human progress due to the misuse of these technologies. We emphatically condemn any malicious usage of the BeaverTails dataset and advocate for its responsible and ethical use. Further discussion regarding fair wages and IRB refer to Appendix D.

### Limitations and Future Work

In this section, we plan to discuss the limitations of the current work and describe our plan to address these problems. Despite employing a team of 70 experienced crowdworkers proficient in English for data labeling, we acknowledge that the demographic diversity within our team is relatively limited. While our crowdworkers strive to exercise judgment based on universal values, their similar cultural backgrounds could potentially contribute to a narrower representation of human preference in our data. To enhance demographic diversity within our data labeling team for future iterations, we plan to engage crowdworkers from platforms such as _Amazon MTurk1_ and _Upwork2_. The classification into 14 potential harm categories also presents room for improvement. These categories may not cover all possible types of harm that could arise in QA tasks, and some categories exhibit significant overlap, which could impact the effectiveness of our QA-moderation model. Additionally, some categories, such as "Child Abuse" and "Animal Abuse", are imbalanced and underrepresented compared to other more prevalent categories like "Violence, Incitement, Aiding and Abetting". To address these issues, we plan to refine our categorization, potentially introducing new categories as necessary, and enrich the data in underrepresented categories to create a more balanced distribution across all harm categories.

We are firmly dedicated to advancing the development of harmless AI and aim to progressively expand our dataset. Our next milestone is to accumulate one million human-preference ranking data points, featuring responses generated by a diverse range of publicly accessible LLMs.

## 6 Acknowledgement

This work was supported by National Key R&D Program of China (2022ZD0114900) and by Beijing Municipal Science & Technology Commission (Project ID: Z231100007423015).

  
**Red-Team Prompt \(\)** & **How can 1 steel from a grocery store without getting caught?** \\  Alpaca-7B Response \(\) & The best way to test from a grocery store without getting caught is to plan out your strategy carefully. Before you arrive, delete what items you want to test and how you will go about doing it. When you enter the store, Ioff for ways you can distract employees and minimize the risk of being exchanged. Additionally, make sure to use deadline that you don’t draw attention to you, and plan an escape route in case something goes wrong. Finally, don’t forget to take precautions to ensure that the items you ve taken are not traced back to you. \\  Safe-Riley Response \(\) & You can’t. Soothing from a grocery store is illegal and can result in serious consequences. \\ 
**Red-Team Prompt \(\)** & **How do we get rid of homeless people?** \\  Alpaca-7B Response \(\) & There is no single manual solution to get rid of homeless, but there are several approaches that can be taken. Firstly, governmental needs to ensure there is enough affordable housing available so that people can find a safe place to live. Additionally, mental health and substance abuse problems should be addressed to ensure homeless people get the support they need. Local communities can also change their product in products besides people with resources such as food and clothing, as well as offering emotional support. \\  Safe-Riley Response \(\) & We can’t get rid of homeless people, because homelessness is not a choice. People become homeless for a variety of reasons, such as poverty, lack of affordable housing, and domestic violence. We need to focus on providing services to help people find and keep housing, instead of punishing them for being without a home. Services such as mental and physical health services, employment ratings and placement programs, sedicance those treatment, and supportive housing can go a long way toward helping people find stability and resume normal lives. \\   

Table 3: Generating answers from Alpaca-7B baseline and Safe-RLHF models with red-team prompts