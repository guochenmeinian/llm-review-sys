# FIARSE: Model-Heterogeneous Federated Learning

via Importance-Aware Submodel Extraction

 Feijie Wu\({}^{1}\), Xingchen Wang\({}^{1}\), Yaqing Wang\({}^{2}\), Tianci Liu\({}^{1}\), Lu Su\({}^{1}\), Jing Gao\({}^{1}\)

\({}^{1}\)Purdue University \({}^{2}\)Google DeepMind

{wu1977, wang2930, liu3351, lusu, jinggao}@purdue.edu

yaqingwang@google.com

###### Abstract

In federated learning (FL), accommodating clients' varied computational capacities poses a challenge, often limiting the participation of those with constrained resources in global model training. To address this issue, the concept of model heterogeneity through submodel extraction has emerged, offering a tailored solution that aligns the model's complexity with each client's computational capacity. In this work, we propose Federated Importance-Aware Submodel Extraction (FIARSE), a novel approach that dynamically adjusts submodels based on the importance of model parameters, thereby overcoming the limitations of previous static and dynamic submodel extraction methods. Compared to existing works, the proposed method offers a theoretical foundation for the submodel extraction and eliminates the need for additional information beyond the model parameters themselves to determine parameter importance, significantly reducing the overhead on clients. Extensive experiments are conducted on various datasets to showcase the superior performance of the proposed FIARSE.

## 1 Introduction

Federated learning (FL)  stands out as a promising distributed training paradigm, in which the clients enjoy mutual information without jeopardizing data privacy. Specifically, the FL server requests the clients to train a model with their local data and aggregates the models into a global one. Such a paradigm, however, may fail in a real-world FL system, where the clients usually have varying computation capacities , likely preventing the clients with insufficient computation resources from being involved in training a large global model .

To tackle the challenge, a practical solution is to enable model heterogeneity, ensuring that the model deployed on each individual client aligns with its local computation capacity. This can be done by extracting a submodel for each client from the global model, which encompasses a subset of the parameters of the global model. During the model training period, the parameters of each submodel are thereby retrieved from the counterpart of the global model. Importantly, each parameter of the global model is exclusively averaged among the submodels containing it .

Depending on whether the submodels undergo reconstruction during the model training process, existing works fall into two categories: static submodel extraction  and dynamic submodel extraction .

Static submodel extraction creates a submodel for each client prior to the model training process. As illustrated in Figure 0(a), the submodel remains unchanged throughout the training process. However, static submodel extraction suffers from certain limitations that affect both local clients and global performance. Locally, the metrics or knowledge used to extract a submodel for each client are likely to evolve during the training process. Since static submodels do not account for this evolution, they may fail to achieve optimal performance. Globally, the extractions of submodels from certain parts may incur client drift during the training, as pointed out by Alam et al.  and Liao et al. . Thisphenomenon emerges when the clients update their submodels biased to their local optima, thereby deviating from the global optimum. As a result, it either degrades the training efficiency or leads to a surrogate convergence on a global scale .

Dynamic submodel extraction updates each submodel dynamically in each training round, and thus is able to capture the evolution of the global model. For example, FedRolex  employs a rolling-based submodel extraction approach, addressing client drift by ensuring equal chances for training each parameter as shown in Figure 0(b). While the method demonstrates significant performance improvement over static submodel extraction in terms of the global model, it sacrifices the submodels' performance. This is because FedRolex treats every parameter equally, leading to a lack of clear guidance on submodel extraction.

To overcome the limitations of the above methods, in this paper, we propose Federated Importance-AwaRe Submodel Extraction, named FIARSE, for model-heterogeneous FL. Specifically, the proposed FIARSE method extracts the submodels based on the importance levels of model parameters. Here important parameters are the edges of the neural network that can induce dramatic changes in the final outputs when removed.

Figure 0(c) visually illustrates the submodels extracted by FIARSE. As demonstrated, FIARSE constructs a submodel by sequentially incorporating parameters in descending order of their importance levels (represented by the thickness of the edges in the model), from highest to lowest, until the client's maximum computation capacity is reached. In contrast to static submodel extraction, our approach enables dynamic updates of the submodels, thereby effectively capturing the evolving nature of model parameters. When compared to rolling-based submodel extraction, FIARSE adaptively identifies important parameters, ensuring outstanding performance for both the global model and local submodels. Referring back to the model shown in Figure 1, the edges connecting the leftmost neuron at the second layer are indicative of important parameters. However, the rolling-based submodel extraction method would roll these parameters out of the submodel in round \(t+1\) (as shown in Figure 0(b)), resulting in inadequate training on these crucial parameters. In contrast, our proposed method can identify and retain these important parameters in the training process, as illustrated in Figure 0(c).

Contributions.The contributions of this paper can be highlighted from the following perspectives:

* **Algorithmically**, we propose an importance-aware framework for model-heterogeneous federated learning. This framework can construct a client-specific submodel calibrated to each client's computation and storage capacity. It achieves this by representing the importance level of each model parameter with its magnitude, thereby avoiding additional storage or computational overhead needed to explicitly maintain the importance scores.
* **Theoretically**, we prove that the proposed algorithm converges at a rate of \(O(1/)\), where \(T\) is the number of communication rounds. This convergence rate is consistent with that of the state-of-the-art FL algorithms, indicating that the proposed submodel construction mechanism does not undermine the convergence properties. To the best of our knowledge, this is the first study to provide a theoretical analysis for model-heterogeneity FL under partial-client participation.
* **Empirically**, we conduct extensive experiments on image and text classification tasks, employing a training-from-scratched model ResNet, and a pretrained model RoBERTa. The results verify that FIARSE significantly outperforms existing approaches, particularly on the clients with limited capacity. The superior performance on resource-constrained devices demonstrates FIARSE's advantages in efficiently adapting submodels to meet diverse capabilities.

Figure 1: Three types of submodel extraction for model training, i.e., static, dynamic, and importance-aware (ours). The figure demonstrates the global model on the server and the local models of two consecutive rounds on a client. Note that solid lines represent the parameters preserved in the local model, while dash lines indicate the parameters excluded from the local model. In importance-aware submodel extraction, we present the importance of the parameters via the line thickness.

Related Work

This section discusses the state-of-the-art works that are most relevant to our research. Appendix A provides a more comprehensive review.

Computation Heterogeneity in FL.Computation heterogeneity in FL refers to the varying computational capacities among clients, including differences in hardware capabilities and resource availability. One typical solution is to allow faster clients to perform more local updates, while slower ones update their local models fewer times [43; 50; 53; 57; 64; 72]. However, these approaches require clients to train the full model, which becomes infeasible when some clients cannot load the full model due to limited computation resources. In our work, we extract a submodel for each client that fits within their computational capacity.

Model Customization in FL.Model customization allows the clients to build their local models that align with their local computation resources [11; 28; 46; 66; 77; 80]. To aggregate these heterogeneous models, the method often employs distillation for knowledge transfer, which requires a shared public dataset. However, this approach becomes infeasible when a shared public dataset is unavailable [3; 59]. In our work, we eliminate the need for a public dataset, broadening its applicability to a wider range of scenarios.

Model Sparsification in FL.Model sparsification, also known as model pruning, removes the unimportant model parameters from a deep learning model, reducing computation overhead and tailoring model sizes to suit clients with varying computational resources [8; 45; 81]. For example, Flado  achieves model sparsification by requiring each client to maintain the importance levels of model parameters and extracting a submodel that encompasses the most important model parameters. While effective, this method incurs considerable overhead on each client in terms of both storage and computation, posing significant challenges for resource-constrained devices. In contrast, our work implicitly represents parameter importance through their values, eliminating the need to maintain separate importance scores for each parameter.

## 3 Preliminary: Model-Heterogeneous Federated Learning

Problem Formulation.Consider there are \(N\) clients in an FL system. The computation capacity of each client \(i[N]\) is metered by the maximum ratio of a submodel extracted from the global model \(}^{d}\) and denoted by \(_{i}\), and the values of \(_{i}\) could vary among the clients. To enable submodel extraction, each client \(i\) should assign a binary mask \(^{(i)}\{0,1\}^{d}\) such that \(\|^{(i)}\|_{1}_{i}d\). Let \(\) be the collections of the clients' masks \(^{(i)}\), i.e., \(=_{i[N]}^{(i)}\{0,1\}^{N d}\). To simultaneously optimize the model parameter and the clients' masks, the model-heterogeneity FL system is formulated as

\[_{}^{d},\{0,1\}^{N d}}F( },)}{{=}}_{i[N]}[F_{i}(}^{(i)} )}{{=}}_{b_{i }}(}^{(i)};b)].\] (1)

Let \(_{i}\) be the local dataset of client \(i[N]\), \(\) be the loss function which calculates the loss for a model on a given data sample (including an input and a target). Therefore, the local objective \(F_{i}()\) in Equation (1) indicates the expected loss for client \(i\) on the local dataset. For simplicity, we consider all \(N\) clients to carry equal weights (i.e., \(1/N\)) in Equation (1), and the proposed approach can be extended to the scenario where the clients are with different weights.

A Generic Solution: Partial Averaging.There are numerous submodel extraction approaches, which are categorized into static and dynamic submodel extractions. However, these methods adopt partial averaging to aggregate clients' models into a global one, and the details are outlined as follows: At round \(t\{0,1,\}\),

* **Sampling:** The server randomly samples a subset of clients \([N]\) and distributes the global model parameters \(}_{t}\) to the selected clients.
* **Local Model Training:** The clients \(i\) performs \(K\)-times local updates via \(^{(i)}_{t,k}=^{(i)}_{t,k-1}- F_{i}(^{(i)}_{t,k-1}^{(i)}_{t})^{(i)}_{t}\), where \(k\{1,,K\}\) and \(^{(i)}_{t,0}=}_{t}\). It is noted that \(^{(i)}_{t}\) represents a binary mask for client \(i\) at \(t\)-th round, which can be either predefined [2; 13; 25; 33] or determined by the client [21; 45; 76].

* **Global Model Aggregation:** The clients collect the model updates from the participants \(\) and perform the global model aggregation via \(}_{t+1}=}_{t}-_{s}_{i} (_{t,K}^{(i)}-}_{t})\). Given a set of \(d\)-dimension vectors \(^{0},,^{||-1}^{d}\), \(_{i}()\) is defined as: (i) For the \(j\)-th index, \(_{i}(_{j})=(_{i} _{j}^{i})/(_{i}\{_{j}^{i} 0\})\); (ii) \(_{i}()=_{j[d]}_{i }(_{j})\).

Limitations.The above solution adopts a consistent mask during local model training, which cannot obtain the optimal mask \(\) for various clients as Equation (1) expects. Some recent works (e.g., Flado  and pFedGate ) have proposed to capture the importance levels of each model parameter in achieving the objective of Equation (1), where a submodel consists of the most important parameters up to the maximum capacity of a client. In these works, the clients hold the importance scores for each model parameter and extract a submodel accordingly. After the local training of the submodel, the clients take an additional step to optimize the importance scores. Despite the effectiveness of these approaches, their feasibility is compromised due to the massive costs related to storage and computation. These approaches entail a minimum training memory and storage of \(O(d)\) and \(O(d)\) on client \(i[N]\), respectively, while HeteroFL  ensures the training memory within \(O(_{i}d)\) and does not require additional storage. Moreover, it is time-consuming to separate model parameters update and mask optimization into two steps. A work  attempts to simplify the process by means of greedy pruning, where a submodel consists of the parameters selected from the largest to the smallest absolute values. Apparently, this approach avoids mask optimization while evolving the submodel architectures since they are associated with model parameters. However, this work keeps the mask consistent during local model training, which does not make sense because an update of model parameters should lead to a different mask.

## 4 Fiarse

Solution Overview.In this work, we explore the correlation between the value of model parameters and their importance levels, leveraging the insights from previous research [29; 54]. These studies reveal that the magnitude of model parameters can act as an indicator of their importance levels. This discovery offers an opportunity to simplify Equation (1). Given that our objective is to extract important parameters for submodel construction, we can approximate this goal by selecting larger parameters to build the submodel, rather than explicitly maintaining an importance score for each parameter. Though simplified, the problem is still challenging since the model parameters themselves are also variables to be optimized. To address this challenge, in Section 4.1, we will present a novel submodel construction method that can jointly select and optimize model parameters.

Finally, Section 4.2 introduces our proposed FL algorithm FIARSE that seamlessly integrates the submodel construction method and optimizes the global model. In detail, the clients optimize the model parameters \(}\) by leveraging the submodel construction method. The server subsequently aggregates these optimized models from the clients and initiates a new training round. Given that the collected model parameters inherently reflect their importance levels, the aggregated global model parameters also effectively capture their importance from a global perspective. Algorithm 1 concisely presents the pseudocode of FIARSE.

### Submodel Construction

As highlighted in the overview, the insight that the values of model parameters are correlated with their importance allows us to reframe the problem of submodel construction. Intuitively, by controlling the number of parameters included in the submodel, we can ensure the computation and/or storage costs of the submodel not exceed the budget of the clients. To achieve this, we establish a threshold for the model parameters based on each client's capacity. Only those parameters whose values exceed this threshold are included in the respective client's submodel. Thanks to the correlation between a parameter's value and its importance level, this approach ensures that the parameters included in the submodel are of greater importance than those that are excluded. This idea can be implemented by converting the mask variable in Equation (1) into a function of the parameter values:

\[_{}^{d}}F(},(}))}{{=}}_{i[N]}F_{i}( }^{(i)}(})), ^{(i)}(})=1,&|}|_{i}\\ 0,&|}|<_{i},\] (2)

where \(^{(i)}()\) represents the mask function of client \(i[N]\) and incorporates the threshold \(_{i}\) on a given model such that \(\|^{(i)}(})\|_{1}_{i}d\); and \(()\) is the collections of all local mask functions. As seen, the problem projects parameter importance to parameter values and thus can achieve parameter selection and model training through optimizing solely the parameter values.

Now the question is how to determine the threshold for Equation (2). In general, the threshold abides by the clients' local computation/storage capacity. Towards this end, we determine the value using \(_{}()\) operation, which selects the top \(\) values of the given vector. For simplicity, we discuss model-wise threshold selection strategies in this section, where the threshold \(_{i}\) is set for \(_{_{i}}(|}|)\). Our proposed method is applicable for settings where different thresholds are assigned to different model parameters. Additional threshold selection strategies will be explored in Appendix D.1.

Threshold-Controlled Biased Gradient Descent (TCB-GD).We enhance Equation (2) by integrating straight-through estimation (STE) [4; 49], where we assume the mask is labeled with 1 with a probability determined by \((}_{i}|-_{i}}{|}_{j}| +_{i}},0,1)\), where \(}_{j}\) means \(j\)-th element of a \(d\)-dimension model parameter \(}\). Therefore, the gradient calculated in the backward propagation on client \(i[N]\) process adheres to:

\[_{}}F_{i}(}^{(i)}( {}))=(}^ {(i)}(}))^{(i)}(})}_{}+}| _{i}}{(|}|+_{i})^{2}})}_{},\] (3)

A detailed derivation of the above equality is provided in Appendix B. There are two key differences in comparison with the gradient computation used by local training of partial averaging, i.e., \( F_{i}(}^{(i)}) ^{(i)}\). First, the mask shifts with the model parameters changing. Second, the backward propagation considers the importance levels of model parameters and forms a biased gradient descent. This means the second term tries to make a clear border between the important and non-important parameters.

Effectiveness.We analyze our proposed approach based on its two features, namely, threshold-controlled and biased gradient computations:

* **Threshold-controlled:** By comparing Equation (3) with Equation (2), we notice that the parameters no less than the designated threshold will be updated. In other words, this gradient descent method only updates the parameters that are greater or equal to the given threshold, and those parameters that are initially smaller than the threshold never get updated. Obviously, the computation cost at each iteration remains constant or even smaller than the cost at the previous iterations. FL clients usually update the model for multiple iterations. According to the description above, the trained submodel is shrinking because some parameters may drop behind the threshold, while no new parameters are introduced to the submodel. Therefore, the proposed gradient descent method keeps the computation cost constant or even smaller than our expectation.
* **Biased:** Biasedness accelerates the update of the parameters near the threshold to distinguish their importance. In other words, less important parameters drop below the threshold and roll out, while the important ones continue to increase until stable. This feature guarantees the model parameters reflect their importance value by minimizing the existence of ambiguous parameters close to the threshold. In other words, the clients can easily identify the unimportant model parameters, facilitating the extraction of a submodel based on parameter values ranging from large to small until it aligns with a client's maximum computation capacity.

We further integrate this submodel construction method into our proposed FL algorithm FIARSE and comprehensively discuss how threshold-controlled biased gradient descent benefits the model-heterogeneity FL in the next section.

### Algorithm Description

In FIARSE, a global model is initialized with arbitrary parameters \(}_{0}^{d}\) (a pretrained model is allowed, which can be regarded as a special case of an arbitrary model).

Partial client participation is one of the features of FL algorithms because the server is unlikely to handle all the communications from all clients, especially when the number of clients is considerably large [10; 32; 44; 71; 74]. Therefore, we present our algorithm FIARSE in support of partial client participation: At the beginning of each communication round \(t\{0,1,\}\), the server uniformly samples a group of clients from \([N]\) without replacement, denoted by \(\), which consists of \(A\) clients.

Subsequently, the server broadcasts the submodels to the selected clients and collects and merges their updates into the global model. In the rest of the section, we comprehensively discuss the details of these steps and exemplify them with \(t\)-th round.

**Submodel Extraction on Server.** Based on the set of participants \(\), the server learns their computation capacities \(\{_{i}\}_{i}\). Then, the server follows the threshold selection strategies described in Section 4.1 and extracts the submodel for all participants according to their computation capacities. Take the model-wise threshold selection as an example and select a submodel for participant \(i\). The threshold is set for \(_{i}=_{_{i}}(|}|)\). Then, the server extracts a submodel encompassing the parameters \(|}_{t}|_{i}\) and sends it to the participant. This procedure is equivalent to the expression in Line 3 of Algorithm 1, i.e., \(}_{t}_{t}^{(i)}(}_{t})\).

**Local Training on Clients \(i\).** After receiving the submodel from the server, we thereby initialize the local masking function \(_{t}^{(i)}()\) for training, which implicitly includes the threshold \(_{i}\). The threshold will remain constant during the local training. As outlined in Line 7 - 8, the client utilizes the TCB-GD to optimize the local model for \(K\) iterations. In each iteration \(k\{0,,K-1\}\), the client utilizes the masking function to sort out the parameters that drop behind the threshold. Then, the client computes the gradient \(_{_{t,k}^{(i)}}F_{i}(_{t,k}^{(i)}_{t}^ {(i)}(_{t,k}^{(i)}))\) using Equation (3) (Line 7) and updates the local model via \(_{t,k+1}^{(i)}=_{t,k}^{(i)}-_{l}_{_{t,k}^{(i )}}F_{i}(_{t,k}^{(i)}_{t}^{(i)}(_{t,k}^{ (i)}))\) (Line 8). As discussed in Section 4.1, the training memory is bounded by \(O(_{i}d)\), and there is no additional storage requirement. In contrast to other importance-aware works such as Flado, the proposed FIARSE is more feasible in practice.

Aggregation on Server.After the selected clients finish their local updates, the server aggregates the updates from the clients (Line 10 - 11). Similar to the global model aggregation of partial averaging described in Section 3, FIARSE updates the global model via \(}_{t+1}=}_{t}-_{s}_{i }(_{t}^{(i)})\) (Line 13). A detailed description of the recursive function is placed in Appendix C.1.

Given that the global model starts with randomly initialized parameters, this aggregation not only updates the global model parameters but also aligns their values with their importance levels. In the next training round \(t+1\), the FIARSE will return to Line 3, regenerating a submodel from the updated parameters, which is then sent back to the client. Since the values of the parameters represent better their importance levels than in the last training round, the newly generated submodel will contain more important parameters. The whole algorithm will then be repeated once again, resulting in a newly aggregated global model. This iterative process will progressively select important parameters and exclude unsignificant ones, steering the global model towards a state of convergence in which the importance of parameters is accurately represented. In the coming section, we theoretically analyze the convergence rate of the proposed FIARSE.

## 5 Convergence Analysis

Existing convergence analyses of FL algorithms predominantly rely on model-homogeneous settings . However, the exploration of model-heterogeneity FL remains inadequately addressed, with some studies [58; 69; 76; 81] in this domain being recently introduced but relying on full client participation. This section aims to present a thorough convergence analysis of the proposed FIARSE under non-convex objectives. Specially, our analysis is established under the scenarios of model-heterogeneity FL where not all clients actively participate in the round-by-round training process.

Before showing the convergence result, we make the following three assumptions:

**Assumption 5.1** (Masked-\(L\)-smoothness).: For all \(i[N]\), the local objectives \(F_{i}\) are \(L\)-Lipschitz smooth with a differentiable mask function \(^{(i)}\): For all \(w,v^{d}\),

\[\|_{w}F_{i}(w^{(i)}(w))- _{v}F_{i}(v^{(i)}(v))\|_{2}  L\|w-v\|_{2}.\]

**Assumption 5.2** (Bounded Global Variance).: For all \(i[N]\), the variance between local gradient \( F_{i}()\) and global gradient \( F()\) is bounded under the same model parameters: For all \(w^{d}\), there exists a constant \(_{j} 0\) for all \(j[n]\) such that

\[_{i N_{_{j}^{}}}^{}} |}\| F_{i}^{[_{j-1}^{}:_{j}^{} ]}(w)- F^{[_{j-1}^{}:_{j}^{ }]}(w)\|_{2}^{2}_{j}^{2}.\]

We further annotate \(^{2}=_{j=0}^{n}_{j}^{2}\).

**Assumption 5.3** (Masked Reduction).: For all \(i[N]\), the mask-incurred error is bounded with respect to the model parameter \(}_{t}\), \(t=0,1,\): There exists a scalar \(_{t}^{2}[0,1)\) at round \(t\) such that

\[\| F_{i}(}_{t})^{(i)}(}_{ t})-_{}_{t}}F_{i}(}_{t}^{(i)} (}_{t}))\|_{2}^{2}_{t}^{2}\| }_{t}\|_{2}^{2}.\]

Lipschitz-smooth assumption has gained widespread acceptance in machine learning research, as evidenced by its incorporation in various studies such as . Assumption 5.1 extends this assumption to a scenario where a binary mask is calculated based on the model parameters and subsequently applied to the model. The second assumption, widely made in the previous FL studies , establishes bounds between the local objectives and the global objective due to the occurrence of non-i.i.d. data. Assumption 5.3 draws inspiration from  and characterizes the masking performance by comparing gradients with and without the application of the mask. Notably, if \(_{t}^{(i)}=^{d}\) (i.e., setting the threshold for \(_{i}=0\)), then \(_{t}^{2}=0\).

With these three assumptions, we analyze the convergence rate of our proposed algorithm. Under non-convex objectives, our goal is to evaluate if the gradient norm can approach zero with respect to the model parameters \(}\) when the communication round \(t\). Theorem 5.4 presents the convergence result of the proposed FIARSE, and a detailed proof is deferred to Appendix C.

**Theorem 5.4**.: _Suppose that Assumption 5.1, 5.2 and 5.3 hold. We define \(F(})F(},^{N d})\), and \(F(}) F_{*}\) for all \(}^{d}\). Let the local learning rate satisfy_

\[_{l}(},}, }{9L},},}).\]

_Denote \(T\) as the total communication rounds. Therefore, the convergence rate of FIARSE for non-convex objectives should be_

\[_{t[T]}\| F(}_{t})\|_{2}^{2}  }_{0})-F_{*})N}{_{s}_{l} KAT}+_{s}_{l}KL^{2}+_{t[T]} _{t}^{2}\|}_{t}\|_{2}^{2}.\] (4)

The aforementioned theorem assesses the potential convergence of the global model towards a stable solution when employing the proposed algorithm (FIARSE). This implies that the submodels utilized in the analysis may differ from those employed in updates at each iteration. Consequently, our analysis takes into account clients utilizing the complete model, with the difference relative to their local submodels being constrained by Assumption 5.3. This methodology aligns with the analytical framework advocated by . Next, we set the appropriate learning rates to achieve optimal convergence properties regarding the number of communication rounds, as outlined in the corollary:

**Corollary 5.5**.: _Suppose that Assumption 5.1, 5.2 and 5.3 hold. We define \(F(})F(},^{N d})\), and \(F(}) F_{*}\) for all \(}^{d}\). Let the local learning rate \(_{l}=}\), and the global learning rate \(_{s}=1\), where \(T\) is the total communication rounds. Then, under non-convex objectives, FIARSE converges to a small neighborhood of a stationary point of standard FL as \(T\) is large enough, i.e.,_

\[_{t[T]}\| F(}_{t})\|_{2}^{2} O (}_{0})-F_{}+^{2}}{})+O(_{t[T]}_{t}^{2}\|}_ {t}\|_{2}^{2}).\] (5)

_where we treat \(L\) as constants._

_Remark 5.6_.: Regarding the first term on the right-hand side (RHS) of Equation (5), it approaches zero as \(T\) tends to infinity. However, an intriguing question arises concerning the potential for the second term to reach zero, given that the norm \(\|}_{t}\|_{2}^{2}\) cannot be zero. According to Assumption 5.3, a straightforward case where \(_{t}\) is always zero occurs when all clients use the full model - that is, when the threshold \(_{i}\) in equation Equation (3) is set to zero in the proposed FIARSE. Therefore, under model-homogeneous federated learning (FL), our theorem aligns with state-of-the-art works [34; 74] in terms of the convergence rate, which focuses solely on the number of communication rounds \(T\), i.e., \(O(1/)\).

For model-heterogeneous FL, it is noteworthy that \(_{t}\) may tend toward zero as \(t\), leading to \(_{t=1}^{T}_{t}^{2}\|}_{t}\|_{2}^ {2}\) approaching zero. As explored by [51; 69; 81], this occurs because the submodels can replicate the performance of the full model on all clients after a substantial number of communication rounds \(T\). Consequently, our proposed algorithm can achieve a convergence rate of \(O(}{A})\), as long as \(_{t}_{t}=0\).

Ignoring constant terms, the proposed FIARSE converges at a rate of \(O(^{2}/)\) under full-client participation (i.e., \(A=N\)). Recently, some works have reported their convergence rates under model-heterogeneous FL with full-client participation, such as pruning-greedy  with a rate of \(O(M^{2}/)\) and FedDSE  with \(O(K^{2}/)\). Evidently, our proposed method exhibits better theoretical performance.

## 6 Experiments

### Setup

Datasets and Models.We evaluate the proposed methodology using a combination of two computer vision (CV) datasets and one natural language processing (NLP) dataset. Specifically, we employ CIFAR-10 and CIFAR-100 datasets  for image classification, and the AGNews dataset  for text classification. For the first two datasets, we conduct training utilizing a ResNet-18 architecture , with modifications made by substituting its batch normalization (BN) layers with static BN counterparts . For the AGNews dataset, we fine-tune a pre-trained RoBERTa-base model .

Data Heterogeneity.For CIFAR-10 and CIFAR-100, we follow [22; 30] and partition the datasets into 100 clients based on a Dirichlet distribution setting \(=0.3\). As for AGNews, we partition the datasets for 200 clients with Dirichlet distribution as well, but it is with the parameter of \(=1.0\). Note that the server or clients do not use any public datasets during the training stage. In the testing phase, we refer to the superset of all clients' test datasets as a "global test dataset."

System Heterogeneity.Specifically, the parameter \(\) is defined as the ratio corresponding to the largest model that can be loaded onto a device. The experiments are conducted with four different model sizes represented by \(^{}=\{1/64,1/16,1/4,1.0\}\). The allocation of clients to each level is balanced. It's important to note that our proposed method is flexible and can accommodate varying numbers of complexity levels or client distributions.

Implementation.In this setting, we set the participation ratio to 10% by default. We perform 800-round training on the CV tasks while running for 300 rounds on the NLP task. To avoid the randomness of the results, we averaged the results from three different random seeds. In the experiments, we report the results of all the baselines based on the best hyperparameter settings. Due to the space limitation, more experimental results and analysis are deferred to Appendix D. Our code is released at https://github.com/HarliWu/FIARSE.

### Submodel Performance on Local Dataset

In this setting, we evaluate the submodels' performance on each client's test datasets. To be specific, the local models are extracted from the global models. Figure 2 comprehensively illustrates the number of clients across different test accuracies. Among the four figures presented, all exhibit a left-skewed distribution, with the exception of FedRolex. This outcome aligns with our expectations, as FedRolex employs a rolling-based approach and is unable to concentrate on optimizing submodel performance on the local dataset, while the rest three approaches can spare efforts on a specific (HeteroFL and ScaleFL) or important (FIARSE) part, effectively addressing performance on local datasets. Among these three approaches, we notice that FIARSE stands out with the best results, showcasing superior performance as more clients achieve higher accuracy compared to the other two methods. The averaged results are also reported by Table 1 under the column of CIFAR-10 and "Local" to "Model (1.0)". Specifically, "Model (1/64)" to "Model (1.0)" shows the averaged local performances classified by the model sizes, and the "Local" shows the result by averaging across these four columns. Table 1 also presents the test accuracy of CIFAR-100 and AGNews. The proposed FIARSE achieves at least 2% better than other baselines under these datasets.

### Submodel Performance on Global Dataset

In this setting, we evaluate the performance of submodels with various sizes on the global test dataset to assess the generalizability of our proposed algorithm. Table 1 presents the results of two CV datasets and one NLP dataset under the column "Global". In conjunction with Figure 3, our proposed method FIARSE constantly outperforms other baselines in all submodels with different sizes by a substantial margin. Figure 3 dives into the details of training and shows the test accuracy trend throughout the communication rounds. Consider the submodels are expected to surpass a 70% test accuracy threshold. As previously discussed, FIARSE ultimately achieves superior test accuracy compared to other baselines. Across model sizes of {1/64, 1/16, 1/4}, our proposed method requires fewer rounds to reach the targeted accuracy compared to other baselines. While the performance disparity between FIARSE and FedRolex is less discernible under the full model (Model (1.0)), both methods significantly outpace static submodel extraction approaches in achieving 70% accuracy. In summary, the proposed method stands out by attaining the desired submodels with the fewest rounds among the approaches implemented in this section.

### Unparticipated Clients Performance

The above evaluations are conducted on the clients who participated in the training process. However, a more general scenario includes clients who skip the training phase but need models to process newly arrived data. In such cases, our algorithm can enable the server to customize models from the trained global model for them. Same as the expression in Line 3 of Algorithm 1, the server extracts a submodel encompassing the parameters \(|}_{t}|_{i}\) and sends it to the client. Note that the unparticipated clients could have capacities different from that of any client involved in the training.

    &  &  &  \\   & Local & Model & Model & Model & Model & Local & Local & Model & Model & Model & Model & Global & Local & Global \\  HeterFL & 68.88 & 60.24 & 69.32 & 21.78 & 73.66 & 66.05 & 31.75 & 27.24 & 29.00 & 33.52 & 36.44 & 30.67 & 87.59 & 86.88 \\ FedRolex & 67.18 & 54.60 & 64.96 & 70.08 & 79.08 & 65.98 & 31.67 & 21.00 & 30.84 & 36.44 & 38.40 & 29.89 & 87.43 & 87.19 \\ ScaleFL & 72.10 & 69.04 & 71.64 & 70.08 & 77.64 & 67.37 & 39.69 & 36.16 & 40.48 & 42.56 & 39.56 & 37.56 & 88.02 & 87.66 \\ FIARSE & **77.04** & **73.12** & **77.20** & **77.24** & **82.04** & **73.75** & **41.76** & **39.12** & **43.34** & **43.72** & **40.96** & **38.63** & **90.03** & **89.61** \\   

Table 1: Test accuracy under four different submodel sizes. To be more specific, the columns from “Local” to “Model (1.0)” evaluate the test accuracy on the local test datasets, while “Global” evaluates the average test accuracy of the global model of four different sizes (1/64, 1/16, 1/4, 1.0) on the global test dataset.

Figure 2: Histograms of various submodel extraction methods on CIFAR-10 under four submodel sizes. Each histogram shows the number of clients achieving different levels of test accuracy.

In Figure 4, we employ our algorithm as well as the baselines to extract submodels with different sizes and compare their performance on unparticipated clients. Generally, the performances of all methods increase as the size of extracted submodels grows. Our algorithm consistently outperforms the baseline methods. In contrast, alternative approaches, particularly static ones like HeteroFL and ScaleFL, suffer from more significant drops in performance. For the case of extracting a submodel that is much smaller than the minimum size involved in the training stage, our proposed method demonstrates remarkable superiority, outperforming existing works by at least 10%.

## 7 Conclusion

This work introduces an algorithm for model-heterogeneity FL, named FIARSE, that utilizes importance-aware operation to extract various sizes of submodels. In detail, we utilize TCB-GD that is able to optimize the clients' local parameters to reflect their importance levels. Subsequently, we provide a theoretical analysis and highlight that the proposed work can converge to a neighborhood of a stationary point at the rate of \(O(1/)\), where \(T\) is the number of communication rounds. Extensive experiments are conducted on ResNet-18 and Roberta-base that demonstrate the significant superiority of our proposed method over the state-of-the-art works.

The proposed approach relies on exploiting model sparsity, which is conditionally supported by some hardware. In light of this limitation, one of the future works is to investigate neuron-wise importance-aware submodel extraction, a method that calculates the importance level of neurons without depending on additional information.

## Broader Impact

This work addresses model heterogeneity in federated learning due to varying computational capacities among clients. The proposed method enhances the efficiency of on-device training and reduces computation and energy demands, which is particularly significant for resource-constrained devices like smartphones in real-world applications. Moreover, the method facilitates the practical deployment of federated learning systems in heterogeneous environments, making them more accessible and scalable.

Figure 4: Comparison of test accuracy across different submodel sizes for different submodel extraction methods on a global test dataset of CIFAR-10.

Figure 3: Comparison of test accuracy across communication rounds for different submodel extraction strategies under four varying model sizes (1/64, 1/16, 1/4, 1.0) on global test datasets of CIFAR-10 (upper, a – d) and CIFAR-100 (lower, e – h).