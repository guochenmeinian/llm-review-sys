# Diffusion Actor-Critic with Entropy Regulator

Yinuo Wang\({}^{1}\)  Likun Wang\({}^{1}\)  Yuxuan Jiang\({}^{1}\)  Wenjun Zou\({}^{1}\)  Tong Liu\({}^{1}\)

**Xujie Song\({}^{1}\)**  Wenxuan Wang\({}^{1}\)  Liming Xiao\({}^{2}\)  Jiang Wu\({}^{2}\)

**Jingliang Duan\({}^{1,2}\)1  Shengbo Eben Li\({}^{1}\)1**

\({}^{1}\)School of Vehicle and Mobility, Tsinghua University

\({}^{2}\)School of Mechanical Engineering, University of Science and Technology Beijing

###### Abstract

Reinforcement learning (RL) has proven highly effective in addressing complex decision-making and control tasks. However, in most traditional RL algorithms, the policy is typically parameterized as a diagonal Gaussian distribution with learned mean and variance, which constrains their capability to acquire complex policies. In response to this problem, we propose an online RL algorithm termed diffusion actor-critic with entropy regulator (DACER). This algorithm conceptualizes the reverse process of the diffusion model as a novel policy function and leverages the capability of the diffusion model to fit multimodal distributions, thereby enhancing the representational capacity of the policy. Since the distribution of the diffusion policy lacks an analytical expression, its entropy cannot be determined analytically. To mitigate this, we propose a method to estimate the entropy of the diffusion policy utilizing Gaussian mixture model. Building on the estimated entropy, we can learn a parameter \(\) that modulates the degree of exploration and exploitation. Parameter \(\) will be employed to adaptively regulate the variance of the added noise, which is applied to the action output by the diffusion model. Experimental trials on MuJoCo benchmarks and a multimodal task demonstrate that the DACER algorithm achieves state-of-the-art (SOTA) performance in most MuJoCo control tasks while exhibiting a stronger representational capacity of the diffusion policy.

## 1 Introduction

Recently, deep reinforcement learning (RL) has emerged as an effective method for solving optimal control problems in the physical world [14; 29; 22; 25]. In most existing RL algorithms, the policy is parameterized as a deterministic function or a diagonal Gaussian distribution with the learned mean and variance [31; 32; 16; 10]. However, the theoretically optimal policy may exhibit strong multimodality, which cannot be well modeled by deterministic or diagonal Gaussian policies [43; 21; 45]. Restricted policy representation capabilities can make algorithms prone to local optimal solutions, damaging policy performance. For instance, in situations where two distinct actions in the same state yield approximately the same Q-value, the Gaussian policy approximates the bimodal action by maximizing the Q-value. This results in the policy displaying mode-covering behavior, concentrating high density in the intermediate region between the two patterns, which is inherently a low-density region with a lower Q-value. Consequently, modeling the policy with a unimodal Gaussian distribution is likely to significantly impair policy learning.

Lately, the diffusion model has become widely known as a generative model for its powerful ability to fit multimodal distributions [18; 35; 8]. It learns the original data distribution through the idea of stepwise addition and removal of noise and has excellent performance in the fields of image [46; 28] and video generation [9; 3]. The policy network in RL can be seen as a state-conditional generativemodel. Given the ability of diffusion models to fit complex distributions, there is increasing work on combining RL with diffusion models. Online RL learns policies by interacting with the environment [16; 32]. Offline RL, also known as batch RL, aims to effectively learn policies from previously collected data without interacting with the environment [1; 4]. In practical applications, many control problems have excellent simulators. At this time, using offline RL is not appropriate, as online RL with interaction capabilities performs better. Therefore, this paper focuses on how the diffusion model can be combined with online RL.

In this work, we propose diffusion actor-critic with entropy regulator (DACER), a generalized new approach to combine diffusion policy with online RL. Specifically, we base DACER on the denoising diffusion probabilistic model (DDPM) . A recent work by He _et al._ points out that the representational power of diffusion models stems mainly from the reverse diffusion processes, not from the forward diffusion processes. Inspired by this work, we reconceptualize the reverse process of the diffusion model as a novel policy approximator, leveraging its powerful representation capabilities to enhance the performance of RL algorithms. The optimization objective of this novel policy function is to maximize the expected Q-value. Maximizing entropy is important for policy exploration in RL, but the entropy of the diffusion policy is difficult to determine. Therefore, we choose to sample actions at fixed intervals and use a Gaussian mixture model (GMM) to fit the action distributions. Subsequently, We can calculate the approximate entropy of the policy in each state. The average of these entropies is then used as an approximation of the current diffusion policy entropy. Then, we use the estimated entropy to regulate the degree of exploration and exploitation of diffusion policy.

In summary, the key contributions of this paper are the following: 1) We propose to consider the reverse process of the diffusion model as a novel policy function. The objective function of the diffusion policy is to maximize the expected Q-value and thus achieve policy improvement. 2) We propose a method for estimating the entropy of diffusion policy. The estimated value is utilized to achieve an adaptive adjustment of the exploration level of the diffusion policy, thus improving the policy performance. 3) We evaluate the efficiency and generality of our method on the popular MuJoCo benchmarking. Compared with DDPG , TD3 , PPO , SAC , DSAC [11; 10], and TRPO , our approach achieves the SOTA performance. In addition, we demonstrate the superior representational capacity of our algorithm through a specific multi-goal task. 4) We provide the DACER code written in JAX to facilitate future researchers to follow our work 1.

Section 2 introduces and summarizes existing approaches to diffusion policy in offline RL and online RL, pointing out some of their problems. Section 3 provides an introduction to online RL and diffusion models. Our approach to combining diffusion policy with the mainstream actor-critic framework, as well as methods to enhance the performance of diffusion policy will be presented in section 4. The results of the experiments in the MuJoCo environment, the ablation experiments as well as the multimodality task will be presented in section 5. Section 6 provides the conclusions of this paper.

## 2 Related Work

Diffusion Policy in Offline RLOffline RL leverages pre-collected datasets for policy development, circumventing direct environmental interaction. Current offline RL research utilizing diffusion models as policy networks primarily adhere to the behavioral cloning framework [7; 26]. Within this framework, two main objectives emerge: performance enhancement and training efficiency improvement. For the former, Cheng _et al._ proposed a Diffusion Policy, casting the policy as a conditional denoising diffusion process within the action space to accommodate complex multimodal action distributions. Wang _et al._ introduced Diffusion-QL, which integrates behavior cloning via diffusion model loss with Q-learning for policy improvement. Ajay _et al._ created Decision Diffusion, incorporating classifier-free guidance into the diffusion model to integrate trajectory information, such as rewards and constraints. Addressing the latter, Kang _et al._ developed efficient diffusion policies (EDP), an evolution of Diffusion-QL. EDP accelerates training by utilizing initial actions from state-action pairs in the buffer and applying a one-step sample for final action derivation. Chen _et al._ proposed a consistency policy that enhances diffusion algorithm efficiency through one-step action generation from noise during training and inference. Although Diffusion Policy's powerful ability to fit multimodal policy distributions can achieve good performance in offline RL tasks, this method of policy improvement based on behavioral cloning cannot be directly transferred to online RL. In addition, the biggest challenge facing offline RL, the distribution shift problem, has not been completely solved. This paper focuses on online RL, moving away from the framework of behavioral cloning.

Diffusion Policy in Online RLOnline RL, characterized by real-time environment interaction, contrasts with offline RL's dependence on pre-existing datasets. To date, only two studies have delved into integrating online RL with diffusion models. Yang _et al._ pioneered this approach by proposing action gradient method. This approach achieves policy improvement by updating the action in the replay buffer through the \(_{a}Q\), followed by mimicry learning of the action post-update using a diffusion model. However, action gradient increased the additional training time. Furthermore, it is difficult to fully learn both the action gradient and imitation learning steps simultaneously, which also resulted in suboptimal performance of this method in MuJoCo tasks. Psenka _et al._ proposed Q-score matching (QSM), a new methodology for off-policy reinforcement learning that leverages the score-based structure of diffusion model policies to align with the \(_{a}Q\). This approach aims to overcome the limitations of simple behavior cloning in actor-critic settings by integrating the policy's score with the Q-function's action gradient. However, QSM needs to accurately learn \(_{a}Q\) in most of the action space to achieve optimal guidance. This is difficult to accomplish, resulting in suboptimal performance of QSM.

Our method is motivated to propose a diffusion policy that can be combined with most existing actor-critic frameworks. We first consider the reverse diffusion process of the diffusion model as a policy function with strong representational power. Then, we use the entropy estimation method to balance the exploration and utilization of diffusion policy and improve the performance of the policy.

Comparison with Diffusion-QLDiffusion-QL  made a successful attempt by replacing the diagonal Gaussian policy with a diffusion model. It also guides the updating of the policy by adding the normalized Q-value in the policy loss term. The main differences between our work and Diffusion-QL are as follows: 1) Diffusion-QL is still essentially an architecture for imitation learning, and policy updates are mainly motivated by the imitation learning loss term. 2) Our work adaptively regulates the standard deviation of random noise in the sampling process \(=+(0,)\), where \(\) is a learned parameter, \(\) is a hyperparameter. This method effectively balances exploration and exploitation and subsequently enhances the performance of the diffusion policy. Ablation experiments provide evidence supporting these findings.

## 3 Preliminaries

### Online Reinforcement Learning

In the conventional framework of RL, interactions between the agent and its environment occur in sequential discrete time steps. Typically, the environment is modeled as a Markov decision process (MDP) with continuous states and actions . The environment provides feedback through a bounded reward function denoted by \(r(s_{t},a_{t})\). The likelihood of transitioning to a new state based on the agent's action is expressed by the probability \(p(s_{t+1}|s_{t},a_{t})\). State-action pairs for the current and next steps are indicated as \((s,a)\) and \((s^{},a^{})\). The decision-making of an agent at any state \(s_{t}\) is guided by a stochastic policy \((a_{t}|s_{t})\), which determines the probability distribution over feasible actions at that state.

In the realm of online RL, agents engage in real-time learning and decision-making through direct interactions with their environments. Such interactions are captured within a tuple \((s_{t},a_{t},r_{t},s_{t+1})\), representing the transition during each interaction. It is common practice to store these transitions in an experience replay buffer, symbolized as \(\). Throughout the training phase, random samples drawn from \(\) produce batches of data that contribute to a more consistent training process. The fundamental aim of traditional online RL strategies is to craft a policy that optimizes the expected total reward:

\[J_{}=_{(s_{i t},a_{i t})}_{i=t}^{ }^{i-t}r(s_{i},a_{i}),\] (1)where \((0,1)\) represents the discount factor. The Q-value for a state-action pair \((s,a)\) is given by

\[Q(s,a)=_{}_{i=0}^{}^{i}r(s_{i},a_{i})|s_{0}=s,a_{0}=a.\] (2)

RL typically employs an actor-critic framework [25; 24], which includes both a policy function, symbolized by \(\), and a corresponding Q-value function, noted as \(Q^{}\). The process of policy iteration is often used to achieve the optimal policy \(^{*}\), cycling through phases of policy evaluation and enhancement. In the policy evaluation phase, the Q-value \(Q^{}\) is recalibrated according to the self-consistency requirements dictated by the Bellman equation:

\[Q^{}(s,a)=r(s,a)+_{s^{} p,a^{}}[Q^{ }(s^{},a^{})].\] (3)

In the policy improvement phase, an enhanced policy \(_{}\) is sought by optimizing current Q-value \(Q^{_{}}\):

\[_{}=_{}_{s d_{s},a}[Q^{_{ }}(s,a)].\] (4)

In practical applications, neural networks are often used to parameterize both the policy and value functions, represented by \(_{}\) and \(Q_{}\), respectively. These functions are refined through the application of gradient descent methods aimed at reducing the loss functions for both the critic, \(_{q}()=_{(s,a,s^{})}[ (r(s,a)+_{s^{} p,a^{}}[Q^{}(s^{ },a^{})]-Q_{}^{}(s,a))^{2}]\), and the actor, \(_{}()=-_{s d_{s},a}[Q^{_{ }}(s,a)]\). These loss functions are structured based on the principles outlined in (3) and (4).

### Diffusion Models

Diffusion models [34; 18; 36; 37] are highly effective generative tools. They convert data from its original distribution to a Gaussian noise distribution by gradually adding noise and then reconstruct the data by gradually removing this noise through a reverse process. This process is typically described as a continuous Markov chain: the forward process incrementally increases the noise level, while the reverse process involves a conditional generative model trained to predict the optimal reverse transitions at each denoising step. Consequently, the model reverses the diffusion sequence to generate data samples starting from pure noise.

Let us define \(p_{}(_{0}):= p_{}(_{0:T})_{1:T}\), where \(_{1},,_{T}\) denote latent variables sharing the same dimensionality as the data variable \(_{0} q(_{0})\), where \(q(_{0})\) means original data distribution. In a forward diffusion chain, the noise is incrementally introduced to the data \(_{0} q(_{0})\) across \(T\) steps, adhering to a predetermined variance sequence denoted by \(_{t}\), described as

\[q(_{1:T}|_{0})=_{t=1}^{T}q(_{t}|_{t-1}), q( _{t}|_{t-1})=(_{t};}_{t-1 },_{t}).\] (5)

When \(T\), \(_{T}\) distributes as an isotropic Gaussian distribution . The reverse diffusion process of the diffusion model can be represented as

\[p_{}(_{0:T})=p(_{T})_{t=1}^{T}p_{}(_{t-1}| _{t}), p_{}(_{t-1}|_{t})=(_{t-1} ;_{}(_{t},t),_{}(_{t},t)),\] (6)

where \(p(_{T})=(_{T};,)\) under the condition that \(_{t=1}^{T}(1-_{t}) 0\).

## 4 Method

In this section, we detail the design of our diffusion actor-critic with entropy regulator (DACER). First, we consider the reverse diffusion process of the diffusion model as a new policy approximator, serving as the policy function in RL. Second, We directly optimize the diffusion policy using gradient descent, whose objective function is to maximize expected Q-values. This feature allows it to be integrated with mainstream RL algorithms that do not require entropy. However, the diffusion policy learned this way produces overly deterministic actions with poor performance. When attempting to integrate the maximization entropy RL framework, we find the entropy of the diffusion policy is difficult to analytically determine. Therefore, we use GMM to approximate the entropy of the diffusion policy, and then learn a parameter \(\) based on it to adjust the exploration level of diffusion policy.

### Diffusion Policy Representation

We use the reverse process of a conditional diffusion model as a parametric policy:

\[_{}(|)=p_{}(_{0:T}|)=p(_{T})_ {t=1}^{T}p_{}(_{t-1}|_{t},),\] (7)

where \(p(_{T})=(0,)\), the end sample of the reverse chain, \(_{0}\), is the action used for RL evaluation. Generally, \(p_{}(_{t-1}|_{t},)\) could be modeled as a Gaussian distribution \((_{t-1};_{}(_{t},,t),_{ }(_{t},,t))\). We choose to parameterize \(_{}(|)\) like DDPM , which sets \(_{}(_{t},,t)=_{t}I\) to fixed time-dependent constants, and constructs the mean \(_{}\) from a noise prediction model as

\[_{}(_{t},,t)=}}( {a}_{t}-}{_{t}}}_{}( {a}_{t},,t)),\] (8)

where \(_{t}=1-_{t},_{t}=_{k=1}^{t}_{k}\), and \(_{}\) is a parametric model.

To obtain an action from DDPM, we need to draw samples from \(T\) different Gaussian distributions sequentially. The sampling process can be reformulated as

\[_{t-1}=}}(_{t}-}{ _{t}}}_{}(_{t},,t))+ },\] (9)

with the reparametrization trick, where \((0,)\), \(t\) is the reverse timestep from \(T\) to \(0\), \(_{T}(0,)\).

### Diffusion Policy Learning

In integrating diffusion policy with offline RL, policy improvement relies on minimizing the behavior-cloning term. However, in online RL, without a dataset to imitate, we discarded the behavior-cloning term and the imitation learning framework. In this study, the policy-learning objective is to maximize the expected Q-values of the actions generated by the diffusion network given the state:

\[_{}_{,_{0}_{}( |)}[Q_{}(,_{0})].\] (10)

Unlike the traditional reverse diffusion process, our study requires recording the gradient of the whole process. The gradient of the Q-value function with respect to the action is backpropagated through the entire diffusion chain.

Policy improvement is introduced above; next, we introduce policy evaluation. The Q-value function is learned through a conventional approach, which involves minimizing the Bellman operator [13; 25; 38] with the double Q-learning trick . We built two Q-networks \(Q_{_{1}}(,),Q_{_{2}}(,)\), and target network \(Q_{^{{}^{}}_{1}}(,),Q_{^{{}^{}}_{2}}(, )\). Then we give the objective function of policy evaluation, which is shown as

\[_{_{i}}_{(,,^{{}^{}})}[((r(,)+_{i=1,2}Q_{^{{}^{}}_ {i}}(^{{}^{}},^{{}^{}}))-Q_{_{i}}(, ))^{2}],\] (11)

where \(^{{}^{}}\) is obtained by inputting the \(^{{}^{}}\) into the diffusion policy, \(\) means replay buffer. Building on this, we employ the tricks in DSAC [11; 10] to mitigate the problem of Q-value overestimation.

The diffusion policy we construct can be directly combined with mainstream RL algorithms that do not require policy entropy. However, training with the above diffusion policy learning method suffers from overly deterministic policy actions, resulting in poor performance of the final diffusion policy. In the next section, we will propose entropy estimation to solve this problem and obtain diffusion policy with SOTA performance.

### Diffusion Policy with Entropy

The diffusion policy's distribution lacks an analytic expression, so we cannot directly determine its entropy. However, in the same state, we can use multiple samples to obtain a series of actions. By fitting these action points, we can estimate the action distribution corresponding to the state.

In this paper, we use Gaussian mixture model (GMM) to fit the policy distribution. The GMM forms a complex probability density function by combining multiple Gaussian distributions, which can be represented as

\[()=_{k=1}^{K}w_{k}(|_{k},_{k}),\] (12)

where \(K\) is the number of Gaussian distributions, and \(w_{k}\) is the mixing weight of the \(k\)-th component, satisfying \(_{k=1}^{K}w_{k}=1,w_{k} 0\). \(_{k}\), \(_{k}\) are the mean and covariance matrices of the \(k\)-th Gaussian distribution, respectively.

For each state, we use a diffusion policy to sample \(N\) actions, \(^{1},^{2},,^{N}\). The Expectation-Maximization algorithm is then used to estimate the parameters of the GMM. In the expectation step, the posterior probability that each data point \(^{i}\) belongs to each component \(k\) is computed, denoted as

\[(^{i}_{k})=(^{i}|_{k}, {}_{k})}{_{j=1}^{K}w_{j}(^{i}|_{j}, {}_{j})},\] (13)

where \((^{i}_{k})\) denotes that under the current parameter estimates, the observed data \(^{i}\) come from the \(k\)-th component of the probability. In the maximization step, the results of the Eq. (13) calculations are used to update the parameters and mixing weights for each component:

\[w_{k}=_{i=1}^{N}(^{i}_{k}),_{k}=^{N}(^{i}_{k})^{i}}{_{i=1}^{N}(^{i }_{k})},_{k}=^{N}(^{i}_{k})(^{i}- _{k})(^{i}-_{k})^{}}{_{i=1}^{N}(^ {i}_{k})}.\] (14)

Iterative optimization continues until parameter convergence. Based on our experimental experience in the MuJoCo environments, a general setting of \(K=3\) provides a better fit to the action distribution.

According to Eq. (12), we can estimate the entropy of the action distribution corresponding to the state by 

\[_{}-_{k=1}^{K}w_{k} w_{k}+_{k=1}^{K}w_{k} ((2 e)^{d}|_{k}|),\] (15)

where \(d\) is the dimension of action. Then, the mean of the entropy of the actions associated with the chosen batch of states is used as the estimated entropy \(}\) of the diffusion policy.

Similar to maximizing entropy RL, we learn a parameter \(\) based on the estimated entropy. We update this parameter using

\[-_{}[}- }],\] (16)

where \(}\) is target entropy. Finally, we use \(=+(0,)\) to adjust the diffusion policy entropy during training, where \(\) is a hyperparameter and \(\) is the output of diffusion policy. Additionally, no noise is added during the evaluation phase. We summarize our implementation in Algorithm 1.

## 5 Experiments

We evaluate the performance of our method in some control tasks of RL within MuJoCo . The benchmark tasks utilized in this study are depicted in Fig. 5, including Humanoid-v3, Ant-v3, HalfCheetah-v3, Walker2d-v3, InvertedDoublePendulum-v3, Hopper-v3, Pusher-v2, and Swimmer-v3. Moreover, we conducted experiments in a multi-goal task to demonstrate the excellent representational and exploratory capabilities of our diffusion policy. We also provide ablation studies on the critical components for better understanding. All baseline algorithms are available in GOPS , an open-source RL solver developed with PyTorch.

Baselines.Our algorithm is compared and evaluated against the six well-known model-free algorithms. These include DDPG , TD3 , PPO , SAC , DSAC [11; 10], and TRPO . These baselines have been extensively tested and applied in a series of demanding domains.

[MISSING_PAGE_FAIL:7]

\(y\)-axis represent 2D states. In this setup, the agent is represented as a 2D point mass situated on a \(7*7\) plane. The objective for the agent is to navigate towards one of four symmetrically positioned points: \((0,5)\), \((0,-5)\), \((5,0)\), and \((-5,0)\). Since the goal positions are symmetrically distributed at the four points, a policy with strong representational capacity should enable the Q-function to learn the four symmetric peaks across the entire state space. This result reflects the policy's capacity for exploration in understanding the environment.

We compare the performance of DACER with DSAC, TD3, and PPO, as shown in Fig. 2. The results show that DACER's actions are likely to point to the nearest peak in different states. DACER's value function curve shows four symmetrical peaks, aligning with the previous analysis. Compared to DSAC, our method learns a better policy representation, mainly due to using a diffusion policy instead of an MLP. In contrast, TD3 and PPO generate more random actions with poorer policy representation, lacking the symmetrical peaks in their value function curves. Overall, our method demonstrates superior representational capability.

To demonstrate the powerful multimodality of DACER, we select five points requiring multimodal policies: (0.5, 0.5), (0.5, -0.5), (-0.5, -0.5), (-0.5, 0.5), and (0, 0). For each point, we sampled 100 trajectories. The trajectories are plotted in Fig. 3. The results show that compared with DSAC, DACER exhibits strong multimodality. This also explains why only the Q-function of DACER can learn the nearly perfectly symmetrical four peaks.

### Ablation Study

In this section, we analyze why DACER outperforms all other baseline algorithms on MuJoCo tasks. We conduct ablation experiments to investigate the impact of the following three aspects on the

   Task & DACER & DSAC & SAC & TD3 & DDPG & TTRPO & PPO \\  Humanoid-v3 & **11888 \(\) 244** & 10892 \(\) 243 & 9335 \(\) 695 & 5631 \(\) 435 & 5291 \(\) 662 & 965 \(\) 555 & 6869 \(\) 1563 \\ Ant-v3 & **9196 \(\) 103** & 7086 \(\) 261 & 6427 \(\) 804 & 6184 \(\) 486 & 4549 \(\) 788 & 6203 \(\) 578 & 6156 \(\) 185 \\ HalfCheetah3 & **17177 \(\) 176** & 17025 \(\) 157 & 16573 \(\) 224 & 8623 \(\) 4041 & 13970 \(\) 2083 & 4785 \(\) 967 & 5789 \(\) 2200 \\ Walker2d-v3 & **6701 \(\) 62** & 6424 \(\) 147 & 6200 \(\) 263 & 5237 \(\) 335 & 4095 \(\) 68 & 5502 \(\) 593 & 4831 \(\) 637 \\ Inverteddoubipedulum-v3 & **9360 \(\) 0** & **9360 \(\) 0** & **9360 \(\) 0** & 9347 \(\) 15 & 9183 \(\) 9 & 6259 \(\) 2065 & 9356 \(\) 2 \\ Hopper-v3 & **4104 \(\) 49** & 3660 \(\)533 & 2483 \(\) 943 & 3569 \(\) 455 & 2644 \(\) 659 & 3474 \(\) 400 & 2647 \(\) 482 \\ Pusher-v2 & **19 \(\) 1** & **19-1** & 20 \(\) 0 & 21 \(\) 1 & -30 \(\) 6 & -23 \(\) 2 & -23 \(\) 1 \\ Swimmer-v3 & **152 \(\) 7** & 138\(\)6 & 140 \(\) 14 & 134 \(\) 5 & 146 \(\) 4 & 70 \(\) 38 & 130 \(\) 2 \\   

Table 1: Average final return. Computed as the mean of the highest return values observed in the final 10% of iteration steps per run, with an evaluation interval of 15,000 iterations. The maximum value for each task is bolded. \(\) corresponds to standard deviation five runs.

Figure 1: **Training curves on benchmarks. The solid lines represent the mean, while the shaded regions indicate the 95% confidence interval over five runs. The iteration of PPO and TRPO is measured by the number of network updates.**

performance of the diffusion policy: 1) whether adding Gaussian noise to the final output action of the diffusion policy; 2) whether the standard deviation of the added Gaussian noise can be adaptively adjusted by estimated entropy; 3) different reverse diffusion step size \(T\).

Only Q-learning.In section 4.2, we propose a method using the reverse diffusion process as a policy approximator, which can be combined with the non-maximizing entropy RL algorithm. However, the diffusion policy trained without entropy exhibits poor exploratory properties, leading to suboptimal performance. Using Walker2d-v3 as an example, we compared the training curves of this method with the DACER algorithm, as shown in Fig. 4(a).

Fixed and linear decay noise factor.In order to verify that using the estimated entropy to adaptively adjust the noise factor plays an important role in the final performance, we conducted the following two experiments in the Walker2d-v3 task: 1) Fixed noise factor to 0.1; 2) The noise factor starts

Figure 3: **Multi-goal multimodal experiments.** We selected 5 points that require multimodal policies: (0, 0), (-0.5, 0.5), (0.5, 0.5), (0.5, -0.5), (-0.5, -0.5), and sampled 100 trajectories for each point. The top row shows the experimental results of DACER, another shows the experimental results of DSAC.

Figure 2: **Policy representation comparison of different policies on a multimodal environment.** The first row exhibits the policy distribution. The length of the red arrowheads denotes the size of the action vector, and the direction of the red arrowheads denotes the direction of actions. The second row shows the value function of each state point.

from 0.27 and linearly decreases to 0.1 during the training process. These two values were chosen because the starting and ending noise factor for adaptive tuning in this setting is about in this range. As shown in Fig. 4(b), our method of adaptively adjusting the noise factor based on the estimated entropy achieves the best performance.

Diffusion steps.We further examined the performance of the diffusion policy as the number of diffusion timesteps \(T\) varied. We used the Walker2d-v3 task to plot training curves for \(T=10,20,\) and \(30\), as shown in Fig. 4(c). Experimental results indicate that a larger number of diffusion steps does not necessarily lead to better performance. Excessive diffusion steps can cause gradient explosion, significantly reducing the performance of diffusion policy. After balancing performance and computational efficiency, we selected 20 diffusion steps for all experiments.

## 6 Conclusion

In this study, we propose the diffusion actor-critic with entropy regulator (DACER) algorithm, a novel RL method designed to overcome the limitations of traditional RL methods that use diagonal Gaussian distributions for policy parameterization. By utilizing the inverse process of the diffusion model, DACER effectively handles multimodal distributions, enabling the creation of more complex policies and improving policy performance. A significant challenge arises from the lack of analytical expressions to determine the entropy of a diffusion strategy. To address this, we employ GMM to estimate entropy, thereby facilitating the learning of a key parameter, \(\), which adjusts the exploration-exploitation balance by regulating the noise variance in the action output. Empirical tests on the MuJoCo benchmark and a multimodal task show the superior performance of DACER.

## 7 Acknowledgements

This study is supported by National Key R&D Program of China with 2022YFB2502901, and Tsinghua University Initiative Scientific Research Program.

Figure 4: **Ablation experiment curves.** (a) DAC stands for not using the entropy regulator. DACERâ€™s performance on Walker2d-v3 is far better than DAC. (b) Adaptive tuning of the noise factor based on the estimated entropy achieved the best performance compared to fixing the noise factor or using the adaptive tuning method with initial, end values followed by a linear decay method. (c) The best performance was achieved with diffusion steps equal to 20, in addition to the instability of the training process when equal to 30.