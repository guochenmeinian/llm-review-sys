# PanoGRF: Generalizable Spherical Radiance Fields

for Wide-baseline Panoramas

 Zheng Chen\({}^{1}\)1, Yan-Pei Cao\({}^{2}\), Yuan-Chen Guo\({}^{1}\), Chen Wang\({}^{1}\), Ying Shan\({}^{2}\), Song-Hai Zhang\({}^{1}\)

\({}^{1}\)Tsinghua University \({}^{2}\)ARC Lab, Tencent PCG

China

{chenz20,guoyc19}@mails.tsinghua.edu.cn caoyanpei@gmail.com cw.chenwang@outlook.com yingsshan@tencent.com shz@tsinghua.edu.cn

###### Abstract

Achieving an immersive experience enabling users to explore virtual environments with six degrees of freedom (6DoF) is essential for various applications such as virtual reality (VR). Wide-baseline panoramas are commonly used in these applications to reduce network bandwidth and storage requirements. However, synthesizing novel views from these panoramas remains a key challenge. Although existing neural radiance field methods can produce photorealistic views under narrow-baseline and dense image captures, they tend to overfit the training views when dealing with _wide-baseline_ panoramas due to the difficulty in learning accurate geometry from sparse \(360^{}\) views. To address this problem, we propose PanoGRF, Generalizable Spherical Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance fields incorporating \(360^{}\) scene priors. Unlike generalizable radiance fields trained on perspective images, PanoGRF avoids the information loss from panorama-to-perspective conversion and directly aggregates geometry and appearance features of 3D sample points from each panoramic view based on spherical projection. Moreover, as some regions of the panorama are only visible from one view while invisible from others under wide baseline settings, PanoGRF incorporates \(360^{}\) monocular depth priors into spherical depth estimation to improve the geometry features. Experimental results on multiple panoramic datasets demonstrate that PanoGRF significantly outperforms state-of-the-art generalizable view synthesis methods for wide-baseline panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay). Project Page: https://thucz.github.io/PanoGRF/.

## 1 Introduction

The rise of \(360^{}\) cameras and virtual reality headsets has fueled the popularity of \(360^{}\) images among photographers and tourists. Commercial VR platforms, such as Matterport 2, enable users to experience virtual walks in \(360^{}\) scenes by interpolating between panoramas . Wide-baseline panoramas are frequently employed on these platforms for capture and network transmission to reduce storage space and bandwidth requirements. Consequently, synthesizing novel views from wide-baseline panoramas is an essential task for providing a seamless six degrees of freedom (6DoF) experience to users.

However, synthesizing novel views from a pair of wide baseline panoramas encounters two primary challenges. _First_, the input views are sparse, causing existing state-of-the-art methods such as NeRF  to struggle in learning accurate geometry due to the shape-radiance ambiguity ,leading to overfitting of the input views. _Second_, certain regions in the scene may only be visible in one view but not in another, making it challenging to provide a correct geometry prior to NeRF using only multi-view stereo. State-of-the-art generalizable NeRF methods [35; 32; 20] address the overfitting problem by incorporating scene priors into NeRF; however, they are designed for perspective images and require conversion of panoramas into perspective views when applied, resulting in information loss and suboptimal performance due to the limited field of view. Although spherical radiance fields  have been proposed to render panoramic images based on spherical projection, they still suffer from the overfitting problem without considering the \(360^{}\) scene prior. Other approaches designed for 360\({}^{}\) view synthesis, such as multi-sphere images (MSI)  and mesh-based methods , exhibit limited expressiveness and rendering quality compared to NeRF due to the use of discretized scene representations.

In this work, we present a method that addresses the overfitting issues in spherical radiance fields by incorporating \(360^{}\) scene priors. Unlike existing generalizable methods that require panorama-to-perspective conversion, our approach retains the panoramic representation. Furthermore, we incorporate \(360^{}\) monocular depth to alleviate the view occlusion problem.

To address the first challenge, we present a solution in the form of _generalizable spherical radiance fields_. To render the panorama at a new position, spherical radiance fields (Spherical NeRF) cast a ray for each pixel using spherical projection, sample points along the ray, and aggregate the colors of these points based on their density values following NeRF . In our method, named PanoGRF, we incorporate \(360^{}\) scene priors into Spherical NeRF. Specifically, we extract appearance and geometry features from input panoramas and estimated spherical depths through convolutions, respectively. PanoGRF accurately aligns the queried 3D points with the corresponding pixels in panoramas using spherical projection. This alignment strategy leverages the full field-of-view characteristic of panoramas and eliminates the information loss of panorama-to-perspective conversion. The local appearance and geometry features at these corresponding pixels are then aggregated and serve as the conditional input for Spherical NeRF.

To tackle the second challenge, we enhance the accuracy of depth estimation in \(360^{}\) multi-view stereo by integrating a \(360^{}\) monocular depth estimation network. In \(360^{}\) multi-view stereo, depth inaccuracies can arise due to view inconsistencies caused by occluded regions. Inspired by the perspective method proposed in , we sample depth candidates around the estimated \(360^{}\) monocular depth, assuming a Gaussian distribution. These depth candidates are then utilized in sphere sweeps during \(360^{}\) multi-view matching. By incorporating monocular depth candidates, we can improve the accuracy of depth estimation in regions with view inconsistencies. This improvement contributes to the robustness of the geometry features employed in PanoGRF and ultimately leads to superior view synthesis performance.

In summary, we make the following contributions: 1) We propose the design of generalizable spherical radiance fields to learn \(360^{}\) scene priors that alleviate the overfitting problem when dealing with wide-baseline panoramas. 2) We incorporate \(360^{}\) monocular depth into the spherical multi-view stereo to mitigate the view inconsistency problem caused by occluded regions, which results in more robust geometry features, ultimately improving the rendering performance. 3) We achieve the state-of-the-art novel view synthesis performance on Matterport3D , Replica  and Residential  under the wide-baseline setting.

## 2 Related Work

In this section, we briefly review the methods of perspective view synthesis and \(360^{}\) view synthesis.

### Perspective View Synthesis

Recently, NeRF (neural radiance fields ) has become the mainstream scene representation due to its photo-realistic rendering quality for perspective view synthesis. However, given sparse input views, NeRF's per-scene optimization approach is prone to overfitting the training views and fails to learn the correct geometry due to the shape-radiance ambiguity, leading to significant floaters in novel views. Several subsequent methods [12; 6; 15; 22; 5; 34] have attempted to address this issue by incorporating depth constraints from COLMAP  or other regularization terms, but they still rely on per-scene optimization and lack the ability to generalize to new scenes. In contrast,PixelNeRF , GRF  and IBRNet  condition NeRF rendering with pixel-aligned appearance features to learn scene priors from large-scale datasets, enabling direct inference with input images. NeuRay  aggregates visibility information (obtained from multi-view stereo) and appearance features from each view to generate the density and color of 3D sample points. However, when dealing with panoramas, these methods require converting the panoramas to perspective images, leading to information loss and suboptimal performance due to limited field-of-view in each perspective view. In contrast, PanoGRF operates directly on panoramas using spherical projection, eliminating the need for the panorama-to-perspective conversion.

### 360\({}^{}\) View Synthesis

Multi-sphere images (MSI) are widely adopted for 360\({}^{}\) view synthesis. It is introduced by  to represent scenes with the input of omnidirectional stereo video. SOMSI  improved the expressive ability of MSI by incorporating high-dimensional feature layers and achieved real-time performance. However, MSI's capability is limited to narrow-baseline scenarios due to the back surface rendering issues as discussed in , which attempted to expand the renderable viewpoint range of MSI through the interpolation of two MSI instances. But their pure convolutional architecture only works at low resolutions. OmniSyn  constructs two meshes for input panoramas using \(360^{}\) multi-view stereo. It warps the meshes to the target view and fuses the colors attached to the meshes with an in-painting network. This approach frequently produces ghosting artifacts due to surface-based warping and blending. \(360^{}\) Roam  divides NeRF into multiple blocks to achieve real-time rendering of panoramas, akin to KiloNeRF . But it fits a single scene without considering scene priors, making it unsuitable for wide-baseline panoramas. In contrast to the aforementioned methods, we introduce generalizable spherical radiance fields to learn both geometric and appearance priors from \(360^{}\) datasets, enabling better generalization to unseen \(360^{}\) scenes.

However, \(360^{}\) multi-view stereo alone cannot provide a robust geometric prior, as it fails to address the occlusion issue in the wide-baseline setting. Recently, numerous researchers have focused on \(360^{}\) monocular depth estimation [13; 19; 1] to mitigate spherical distortion by harnessing the fusion of equirectangular and perspective projections. Inspired by MaGNet , we employ \(360^{}\) monocular depth prior to guide the construction of \(360^{}\) cost volume, which enhances the quality of geometric features and boosts rendering performance.

Figure 1: The overview of PanoGRF. Initially, we employ convolutional neural networks (CNNs) to extract appearance features and geometry features from the input views. The geometry feature is obtained from the predicted spherical depth generated by our proposed Mono-guided \(360^{}\) Depth Estimator (Sec. 3.3). Next, we cast rays based on spherical projection to render a novel \(360^{}\) view (Sec. 3.1). Along each ray, every 3D sample point \(_{i}\) is projected onto the corresponding panoramic grid coordinate \((u,v)\). The local geometry feature \(_{i,j}\) at \((u,v)\) is decoded into \(v_{i,j}\) (the visible probability of \(_{i}\) for the \(j\)-th view). The local appearance feature \(_{i,j}\) at \((u,v)\) is aggregated into \(_{i}\) along with \(v_{i,j}\) (Sec. 3.2). Subsequently, the aggregated feature \(_{i}\) is decoded to determine the color and density of \(_{i}\). Finally, a novel \(360^{}\) view is synthesized through volume rendering (Sec. 3.1).

Method

PanoGRF facilitates novel view synthesis from wide-baseline panoramas as shown in Fig. 1. Before introducing PanoGRF, we first review NeRF and its spherical variant in Sec. 3.1. We demonstrate the two important parts of PanoGRF, the generalizable spherical radiance fields, and the mono-guided spherical depth estimator in Sec. 3.2 and Sec. 3.3 respectively.

### Spherical Radiance Fields (Spherical NeRF)

To render a pixel, NeRF  casts a ray from a given viewpoint and parameterizes the ray as \((t)=+t,t^{+}\), where \(\) is the camera center and \(\) is the ray direction. \(N\) points are sampled along the ray with increasing \(t\), we denote \(_{i}(t_{i}),i=1,...,N\). NeRF utilizes an MLP to map the position \(_{i}\) together with viewing direction \(\) into color \(_{i}\) and density \(_{i}\). The pixel color of the ray \(\) can then be computed by:

\[=_{i=1}^{N}w_{i}_{i},\] (1)

where \(^{3}\) is the rendered color of \(\), and \(_{i}^{3}\) is the color of the sampled point \(_{i}\). \(w_{i}\) is the blending weight of the point \(_{i}\), which indicates the probability that a ray travels from the origin to \(t_{i}\) without hitting any particle and terminates in the range \((t_{i},t_{i+1}\). It is computed by \(w_{i}=_{k=1}^{i-1}(1-_{k})_{i}\), where \(_{i}\) is the alpha values in the depth range \((t_{i},t_{i+1})\), \(_{i}=1-(-_{i}_{i})\) and \(_{i}=t_{i+1}-t_{i}\). The color \(\) of a ray \(\) is optimized by a photometric loss:

\[=\|-_{gt}\|^{2},\] (2)

where \(_{gt}\) represents the ground truth color.

Ray-casting based on spherical projectionThe panorama uses the panoramic pixel grid coordinate system, while the coordinate systems in NeRF are all Cartesian. By employing _spherical projection_[8; 11], a pixel \((u,v)\) in the panorama is firstly transformed into spherical polar coordinate \((,)\) and subsequently into cartesian coordinate \((x,y,z)\). Detailed transformation formulas can be seen in the supplementary material. The ray direction emitted from the pixel \((u,v)\) can be computed by \(=[x,y,z]^{T}\), where \(^{3 3}\) is the panorama's camera-to-world rotation matrix.

### Generalizable Spherical Radiance Fields

Under a wide baseline, Spherical NeRF tends to overfit training views and struggle to generate plausible novel views. To address this limitation, we draw inspiration from the generalizable NeRF approaches [35; 32; 20] designed for perspective images. In our work, we propose to incorporate \(360^{}\) scene priors into Spherical NeRF by aggregating local features from input panoramas.

Alignment based on spherical projectionPanoramas can be converted into perspective images, and then generalizable NeRF methods for perspective images can be applied to novel \(360^{}\) view synthesis. However, when projecting the 3D sample point \(_{i}\) onto the source perspective view, it may fall outside the image borders or even be located behind the source perspective camera (with a \(z\)-depth < 0) due to the limited field-of-view. This can introduce errors in the aggregation of features and result in poor rendering results. To overcome this problem, we directly align \(_{i}\) with the corresponding pixels in panoramas using spherical projection, enabling us to leverage the full field-of-view characteristic of panoramas. We first transform \(_{i}\) into the camera's Cartesian coordinate system \((x,y,z)\) of the \(j\)-th panoramic view \(I_{j}\). Subsequently, \((x,y,z)\) is converted into spherical polar coordinate \((,,t)\) (\(t^{+}\) indicates the spherical depth of \(_{i}\) for \(I_{j}\)) and finally transformed into the panoramic grid coordinate \((u,v)\). The specific formulas for these transformations can be found in the supplementary material.

Appearance and geometry feature aggregationWe follow NeuRay  to aggregate appearance and geometry features. But differently, we take panoramas and spherical depth (radial distance from the camera center) as input instead of perspective images and \(z\)-depth. PanoGRF respectively extracts appearance feature \(_{j}\) from the \(j\)-th panoramic view (\(j=1,2\)) and geometry feature \(_{j}\) from the spherical depth using convolutions. Details for getting the spherical depth will be introduced in Sec. 3.3. For a given 3D sample point \(_{i}\), we project it onto the panoramic grid coordinate \((u,v)\), and extract the local appearance feature \(_{i,j}=_{j}(u,v)\) and the local geometry feature \(_{i,j}=_{j}(u,v)\) at \((u,v)\). The local geometry feature \(_{i,j}\) is decoded into the visibility \(v_{i,j}\) (demonstrated below) using an MLP. The local appearance feature \(_{i,j}\) and visibility \(v_{i,j}\) of the \(j\)-th panoramic view are aggregated for \(_{i}\) with an aggregation network \(\):

\[_{i}=(\{_{i,j},v_{i,j}|j=1,2\}).\] (3)

According to the local geometry feature \(\) of an input panorama, we predict the visibility function \(v(t)\) to indicate the probability that a point at spherical depth \(t\) (instead of \(z\)-depth in NeuRay) is visible for the input panorama. \(v(t)\) (\(v(t)\)) is represented as \(v(t)=1-o(t)\), where \(o(t)\) is the occlusion probability. To parameterize \(o(t)\), we employ a mixture of \(N_{l}\) logistic distributions:

\[o(t;_{k},_{k},m_{k})=_{k}^{N_{l}}m_{k}S((t-_{k})/_{k}),\] (4)

where \(_{k}\), \(_{k}\) and \(m_{k}\) are the mean, standard variance, and blending weight of the \(k\)-th logistic distribution respectively. \(_{i}^{N_{l}}m_{k}=1\) and \(S()\) denotes a sigmoid function. The parameters \([_{k},_{k},m_{k}]\) are decoded from the local geometry feature vector \(\) by an MLP. For each 3D sample point \(_{i}\), we compute its spherical depth \(t_{i}\) for \(j\)-th input view based on spherical projection and obtain the visible probability \(v_{i,j}=v_{j}(t_{i})\) of \(_{i}\) for the \(j\)-th panoramic view. Then we aggregate \(v_{i,j}\) into \(_{i}\) by Eq. 3 and decode \(_{i}\) into the color and density of \(_{i}\).

### Mono-guided Spherical Depth Estimator

To obtain the visibility in Sec. 3.2, we need to predict the spherical depth of each input view. The process is illustrated in Fig. 2.

\(360^{}\) multi-view stereo (MVS)With the input of multi-view panoramas, we estimate the spherical depth of the reference view using \(360^{}\) multi-view stereo similar to previous methods [17; 14]. Firstly, we extract the image features of the reference and source views with an image encoder. For each pixel \((u,v)_{ref}\) of the reference view, assuming its depth is \(t_{i}\), we find its corresponding pixel \((u,v)_{src}\) in

Figure 2: The process of mono-guided \(360^{}\) depth estimation. We first extract the image features of reference and source views with convolutions. Using spherical projection, we determine the corresponding pixel \((u,v)_{src}\) in the source view for each pixel \((u,v)_{ref}\) of the reference view, with the depth hypothesis of \(t_{i}\). The similarity between the local features at \((u,v)_{ref}\) and \((u,v)_{src}\) is computed as the value of the cost volume at \((u,v,i)\). Depth sampling of \(t_{i}\) is guided by the \(360^{}\) monocular depth using a Gaussian distribution assumption. The cost volume is obtained after \(D\) sphere sweeps. Lastly, the cost volume is decoded into \(360^{}\) depth using convolutions.

the source view according to spherical projection. We uniformly sample \(D\) depth candidates for \(t_{i}\) (with \(i=1,2,...,D\)) without considering the monocular depth prior, which will be introduced later. Next, we compute the similarity between the local features at \((u,v)_{ref}\) and \((u,v)_{src}\), considering it as the value of the cost volume at \((u,v,i)\). Assuming the length of the local feature vector is \(F\), this process results in a 4D cost volume \(^{ D F}\) after \(D\) sphere sweeps, where \(\) and \(\) respectively represent the height and width of the feature maps for the reference view. Subsequently, the dimension of the cost volume is reduced to \( D\) through a 3D CNN. Lastly, the processed cost volume is decoded into \(360^{}\) depth using several convolution layers.

Mono-guided spherical depth samplingUnder the wide-baseline setting, some areas might be visible from one view but occluded from another, in which case \(360^{}\) MVS may struggle to produce accurate depth estimates. To address this challenge, we leverage the \(360^{}\) monocular depth  to guide the depth sampling of \(360^{}\) MVS, inspired by perspective methods [36; 31; 3].

For each pixel \((u,v)\) in the panorama, we denote its estimated spherical depth from the monocular depth network as \(_{u,v}\). We generate depth candidates in the vicinity of the monocular depth using a Gaussian distribution assumption. Specifically, a search space for each pixel in the panorama is defined as \([_{u,v}-,_{u,v}+]\) where \(\) and \(\) serve as hyperparameters and \(\) represents the standard deviation. This search space is divided into \(N_{mono}\) bins, ensuring that each bin has the same probability mass. We select the midpoint of each bin as the depth candidate. Consequently, the \(k\)-th depth candidate for pixel \((u,v)\) is defined as \(t_{u,v,k}=_{u,v}+b_{k},k=1,2,...,N_{mono}\). \(b_{k}\) represents the offset corresponding to the \(k\)-th bin. Similar to , we calculate \(b_{k}\) as the following:

\[b_{k}=[^{-1}(}P^{*}+}{2}) +^{-1}(}P^{*}+}{2})],\] (5)

where \(P^{*}=(})\) denotes the probability mass covered by the search space \([_{u,v}-,_{u,v}+]\), \(^{-1}()\) is the quantile function of standard normal distribution, \(\) is the error function. Considering errors may occur in the \(360^{}\) monocular depth, we use a mixture of \(N_{mono}\) monocular depth candidates and \(N_{uni}\) uniform depth candidates sampled from a uniform distribution. By constructing \(D=N_{uni}+N_{mono}\) sphere sweeps, we obtain a new spherical cost volume. The monocular depth guidance enables us to extract more reliable geometry features employed in PanoGRF.

## 4 Experiment

### Metrics and Datasets

PSNR, SSIM , LPIPS  and WS-PSNR  are used as evaluation metrics. We conduct experiments on Matterport3D , Replica , and Residential . For Matterport3D and Replica, we leverage HabitatAPI  to generate the \(256 256\) perspective images (representing the six sides of cube-maps) and stitch them into a \(512 1024\) panorama. We use panoramic sequences with a length of 3. The middle view is rendered based on the first and last views. We conduct comparative experiments on Matterport3D under fixed camera baselines of 1.0, 1.5, and 2.0 meters, where the camera baseline refers to the distance between the camera centers of the first and last views. The baselines used for Residential and Replica are approximately 0.3 and 1.0 meters, respectively.

### Implementation Details

We set \(N_{mono}=5\), \(N_{uni}=59\), and \(N_{l}=2\). \(D=N_{mono}+N_{uni}\) is 64. \(\) used in mono-guided depth sampling is set to 0.5 and \(\) is set to \(3\). Additional details regarding network architecture can be found in the supplementary material.

### Comparisons

We compared PanoGRF with S-NeRF (spherical variant of NeRF ), IBRNet , NeuRay , and OmniSyn . We trained S-NeRF from scratch, as it is a per-scene optimization method. Other methods are pre-trained on Matterport3D and tested on unseen testing scenes. We fed the original cube maps of panoramas (in Matterport3D and Replica) to the perspective methods (IBRNet and NeuRay). As there are only panoramas in ERP (equirectangular projection) format for Residential,cube maps were converted from panoramas. To render a panoramic view for evaluation, we cast rays in IBRNet and NeuRay with spherical projection (refer to Sec. 3.1) and aggregate features with their original perspective projection. We use NeuRay\({}^{*}\) and IBRNet\({}^{*}\) to denote variants of NeuRay and IBRNet which render panoramas. We did not compare with methods based on multi-sphere images (MSI)  as they can only render novel views within the smallest sphere of MSI, which are unsuitable for wide-baseline panoramas. In the supplementary materials, we provide additional analysis on per-scene fine-tuning of PanoGRF and also compare it with Cross Attention Renderer .

AnalysisOn Matterport3D and Replica, we quantitatively compared PanoGRF with the baseline methods. The results can be found in Table. 1 and Table. 2. Almost all the metrics show that PanoGRF outperforms other methods significantly. We show the qualitative comparisons in Fig. 3. IBRNet* and NeuRay* aggregate features based on perspective projection. Due to the limited field of view, 3D sample points are often projected somewhere behind the source perspective camera (\(z\)-depth\(<\)0) or outside the image border during pixel alignment. In this case, the aggregated features are incorrect, causing bad rendering results. As for OmniSyn, its rendering outputs exhibit ghosting artifacts, particularly notable at the boundaries of the two sofas in Sample 1 of Fig. 3. Unlike these methods, PanoGRF is a generalizable spherical NeRF which is more appropriate for panoramic images due to the use of spherical coordinates. It achieves superior accuracy in synthesizing object boundaries and demonstrates better pixel alignment, as evident from the results showcasing the sofas in Sample 1 and the desk in Sample 2 of Fig. 3). On Residential, we also compared PanoGRF with S-NeRF, IBRNet*, NeuRay*. The results of S-NeRF show severe floaters (See the ceilings of Sample 1 in Fig. 4). As

   baseline &  &  &  \\  method & WS-PSNR\(\) & SSIM\(\) & LPIPS\(\) & WS-PSNR\(\) & SSIM\(\) & LPIPS\(\) & WS-PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  S-NeRF & 15.25 & 0.579 & 0.546 & 14.16 & 0.563 & 0.580 & 13.13 & 0.523 & 0.607 \\ OmniSyn & 22.90 & 0.850 & 0.244 & 20.31 & 0.790 & 0.317 & 18.91 & 0.761 & 0.354 \\ IBRNet\({}^{*}\) & 25.72 & 0.855 & 0.258 & 21.69 & 0.751 & 0.382 & 20.04 & 0.706 & 0.431 \\ NeuRay\({}^{*}\) & 24.92 & 0.832 & 0.260 & 21.92 & 0.766 & 0.347 & 19.85 & 0.715 & 0.407 \\ PanoGRF & **27.12** & **0.876** & **0.195** & **23.38** & **0.811** & **0.282** & **20.96** & **0.761** & **0.352** \\   ^{*}\) means models are trained with cubemap projection and evaluated with equirectangular projection.} \\ 

Table 1: Quantitative comparisons with baseline methods on Matterport3D. The best results are in bold.

Figure 3: Qualitative comparison on Replica and Matterport3D between IBRNet\({}^{*}\), OmniSyn, NeuRay\({}^{*}\) and PanoGRF.

converting panoramas into perspective views brings information loss, the perspective generalization methods IBRNet* and NeuRay* have notable artifacts. In contrast, PanoGRF demonstrates excellent generalization performance on the Residential dataset, as shown in Fig. 4. Additional qualitative comparisons can be seen in the supplementary material.

### Ablation Studies

We conducted ablation studies to evaluate the effectiveness of the key components in mono-guided spherical depth estimator: \(360^{}\) monocular depth and multi-view stereo. We provide qualitative comparisons of \(360^{}\) depth estimation quality on Matterport3D in Fig. 5, and analyze the view synthesis quality on Replica in Table. 3. More results can be found in the supplementary material.

\(360^{}\) monocular depthWe removed the \(360^{}\) monocular depth guidance and kept the total number of depth candidates unchanged (\(N_{uni}=64\)) in the depth sampling. Under three camera baselines, all the metrics dropped after removing the \(360^{}\) monocular depth. Especially, under the camera baseline of 2.0 meters, WS-PSNR dropped by 1.33dB without monocular depth. This removal resulted in error-prone object boundaries, such as the bedside and wooden pillar (shown in Fig. 6). The guidance of monocular depth can give more accurate depth candidates and mitigates the view inconsistency problem, which \(360^{}\) multi-view stereo cannot handle alone.

\(360^{}\) multi-view stereoWe removed \(360^{}\) multi-view stereo and used \(360^{}\) monocular depth directly as the input of the geometry feature extraction in PanoGRF. In this case, WS-PSNR dropped

   Dataset &  &  \\  method & PSNR\(\) & WS-PSNR\(\) & SSIM\(\) & LPIPS\(\) & PSNR\(\) & WS-PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  S-NeRF & 16.91 & 16.10 & 0.723 & 0.443 & 23.06 & 22.47 & 0.741 & 0.435 \\ OmniSyn & 23.91 & 23.17 & 0.898 & 0.189 & - & - & - & - \\ IBRNet\({}^{*}\) & 23.35 & 22.65 & 0.854 & 0.291 & 22.95 & 22.47 & 0.735 & 0.498 \\ NeuRay\({}^{*}\) & 26.62 & 25.90 & 0.899 & 0.187 & 23.01 & 22.38 & 0.753 & 0.427 \\ PanoGRF & **30.08** & **29.22** & **0.937** & **0.134** & **31.64** & **31.03** & **0.909** & **0.207** \\   

Table 2: Quantitative comparison with baseline methods on Replica and Residential Dataset

Figure 4: Qualitative comparison on Residential between S-NeRF, IBRNet\({}^{*}\), NeuRay\({}^{*}\) and PanoGRF.

more than 1.0 dB under the camera baselines of 1.5 and 2.0 meters. And LPIPS became 0.165 from 0.134 under the baseline of 1.0 meters. Without multi-view stereo, multi-view consistency of geometry is not guaranteed, degrading the quality of novel view synthesis results. In Fig. 6, we observed the presence of ghosting artifacts near the left wooden pillar when relying solely on \(360^{}\) monocular depth.

## 5 Conclusion

In this paper, we propose PanoGRF, generalizable spherical radiance fields for wide-baseline panoramas. We aggregate the appearance and geometry features from input panoramas through spherical projection to avoid panorama-to-perspective conversion. To address the view inconsistency problem, we use \(360^{}\) monocular depth to guide the spherical depth sampling and obtain a more accurate geometric prior for the spherical radiance fields. Experiments on multiple datasets verify that PanoGRF can render high-quality novel views given wide-baseline panoramas.

LimitationsSimilar to other generalizable radiance fields, PanoGRF suffers from the issue of rendering speed. Additionally, since we only train PanoGRF on indoor data due to the lack of large-scale outdoor \(360^{}\) datasets, its generalization performance can be limited when applied to outdoor scenes with significantly different depth scales.

Societal impactThis method may be used to synthesize fake or deceptive panoramas, combined with generative methods.

## 6 Acknowledgements

This work was supported by the Natural Science Foundation of China (Project Number 62132012) and Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology.

Figure 5: Qualitative results of ablation studies for depth estimation on Matterport3D. _Mono only_ and _MVS only_ respectively refer to the results obtained when using only \(360^{}\) monocular depth and \(360^{}\) multi-view stereo. The use of \(360^{}\) monocular depth alone does not ensure multi-view consistency, resulting in potential discrepancies between the predicted scale and the ground truth in certain regions. Due to the occlusion problem, using only \(360^{}\) MVS results in inaccurate and less detailed depth predictions, especially at the boundaries of objects.

   baseline &  &  &  \\  method & WS-PSNR\(\) & SSIM\(\) & LPIPS \(\) & WS-PSNR\(\) & SSIM\(\) & LPIPS\(\) & WS-PSNR\(\) & SSIM\(\) & LPIPS\(\) \\  w/o Mono & 28.71 & 0.935 & 0.136 & 25.53 & 0.898 & 0.188 & 23.33 & 0.864 & 0.249 \\ w/o MVS & 28.23 & 0.925 & 0.165 & 25.15 & 0.888 & 0.227 & 23.15 & 0.855 & 0.274 \\ full & **29.22** & **0.937** & **0.134** & **26.38** & **0.903** & **0.187** & **24.48** & **0.885** & **0.223** \\   

Table 3: Ablation studies on Replica