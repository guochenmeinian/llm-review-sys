# Katakomba:

Tools and Benchmarks for Data-Driven NetHack

 Vladislav Kurenkov

Tinkoff

v.kurenkov@tinkoff.ai

&Alexander Nikulin\({}^{*}\)

Tinkoff

a.p.nikulin@tinkoff.ai

&Denis Tarasov

Tinkoff

den.tarasov@tinkoff.ai &Sergey Kolesnikov

Tinkoff

s.s.kolesnikov@tinkoff.ai

Contributed equally.

###### Abstract

NetHack is known as the frontier of reinforcement learning research where learning-based methods still need to catch up to rule-based solutions. One of the promising directions for a breakthrough is using pre-collected datasets similar to recent developments in robotics, recommender systems, and more under the umbrella of offline reinforcement learning (ORL). Recently, a large-scale NetHack dataset was released; while it was a necessary step forward, it has yet to gain wide adoption in the ORL community. In this work, we argue that there are three major obstacles for adoption: resource-wise, implementation-wise, and benchmark-wise. To address them, we develop an open-source library2 that provides workflow fundamentals familiar to the ORL community: pre-defined D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation tools with accompanying configs and logs synced to the cloud.

## 1 Introduction

Reinforcement learning (Arulkumaran et al., 2017; Levine et al., 2020) led to remarkable progress in the decision-making problems in recent years: robotics (Smith et al., 2022; Kumar et al., 2021), recommender systems (Chen et al., 2022), traffic control (Chen et al., 2020), energy management (Yu et al., 2019), combinatorial optimization (Mazyavkina et al., 2021), and videogames (Baker et al., 2022; Hafner et al., 2023). Many of those are considered solved or close to solved problems, and the arguably next untapped frontier for RL algorithms is the NetHack (Kuttler et al., 2020).

NetHack3 is considered one of the most challenging games for humans, even more to learning agents. This game requires strong generalization capabilities, as the levels are procedurally generated with each reset, and extraordinary memory capacity, as the episodes may last for 100k steps requiring to remember what happened a thousand steps before to act optimally. Moreover, the high level of uncertainty and its dependence on the initial character configuration further hardens the problem. Indeed, at the moment, the best agent, AutoAscend (Hambro et al., 2022), is essentially rule-based, and yet reinforcement learning agents cannot even come close in terms of the game score or levels reached. While there were various attempts to advance online reinforcement learning agents, this task seems out of reach for them.

Recently, a promising direction was outlined - re-using large-scale datasets of human/bot demonstrations to bootstrap RL agents, either in the pre-training phase or in the fine-tuning stage (Hambro et al., 2022; Kumar et al., 2022). This research direction holds great promise to advance the community's progress in solving the NetHack and attacking the weak sides of the RL agents. While the proposed tools and datasets are necessary, their implementations, support, and ease of adoption still need to be improved. What is essential, the use of NetHack promises the democratization of research efforts, providing way more efficient tools compared to both StarCraft (Vinyals et al., 2019) and Minecraft (Guss et al., 2019) learning environments. However, while the open-sourced datasets are a significant first step, the provided tools are very far from being readily adopted by other researchers (Section 2).

In this work, we aim to address this gap and provide a set of instruments that are conducive to future research endeavors, smooth the learning curve, and define a precise set of tasks to be evaluated against when training offline RL agents on the NetHack dataset, where contributions are as follows:

* **D4RL-Style Benchmark** A set of small-scale datasets based on the large-scale data source (Hambro et al., 2022) for faster experimentation, including several data loaders for different speed-memory tradeoffs with a familiar interface to the ORL community alike Fu et al. (2020).
* **Clean Recurrent Offline RL Baselines** Straightforward implementations of popular offline RL baselines augmented with recurrence: BC (Michie et al., 1990), CQL (Kumar et al., 2020), AWAC (Nair et al., 2020), REM (Agarwal et al., 2020), IQL (Kostrikov et al., 2021). Each implementation is separated into single files akin to Huang et al. (2021); Tarasov et al. (2022) to smooth the learning curve.
* **Evaluation Guideline** We use reliable evaluation tools for comparing offline RL agents in the natural presence of high variability. For this, we employ recent RLiable tools (Agarwal et al., 2021) that avoid comparing against mean values (which are especially deceptive in the case of the NetHack environment). In order to facilitate the comparisons, we also release the raw results we obtained in our experiments so that future work may re-use them for a fair comparison.
* **Open-Sourced Training Logs and Configs** Moreover, we keep a public track of our experiments: including configs, implementations, and learning dynamics using Weights&Biases (Biewald, 2020), so that it is transparent for inspection and reproduction.

We believe our work is a logical next step for advancing reinforcement learning in the NetHack environment, providing a conducive set of tools, algorithms, and utilities for experimenting with offline RL algorithms. Moreover, as we plan to support the benchmark, we intend to extend it with offline-to-online tools and benchmarks. By no means this work serves as a converged fixed point. We view this as a starting step in building reliable tools for solving NetHack with prior data and commit to further support and development.

## 2 What Hinders the Adoption of Data-Driven NetHack?

In this section, we describe the main obstacles and motivate our work. We outline our experience with the recently released dataset (Hambro et al., 2022), accompanying tools and divide the main issues into three categories: implementational, resources, and benchmark-wise. These are needed to illustrate that the initial release of the dataset is a significant step. However, it is only the first step, and further development is needed, which we will address in further sections.

Implementation-wiseFirst, when trying to reproduce the experiments, we found that installing the released tools is error-prone: where the provided Dockerfile was not operable and required fixing the moolib library with a CMake modification, which is probably undesirable for broader adoption of practitioners. Second, when we attempted to reproduce the scores reported in Hambro et al. (2022), the only configuration file provided was for the IQL algorithm4 and its modification with hyperparameters for other algorithms from the paper required additional effort to assign proper values. Consequently, the reproduction did not result near what was reported in the paper (note that a similar pattern was observed in Piterbarg et al. (2023)). While we do not claim that the reproduction is impossible, we could not find the proper fix in a reasonable amount of time and consider this as another obstacle in adoption - practitioners should either have access to the original training logs or be able to replicate the results reliably.

Beyond the replication frictions, another issue lies in the design of implementations. Offline and offline-to-online settings are interleaved within just one file (1500 lines of code), where the algorithms are realized using the framework for distributed reinforcement learning not typical for the ORL community (Mella et al., 2022). Consequently, understanding the underlying logic is hard, and some performance-related implementation tricks can be hard to spot. For example, q-learning-based algorithms do not use the last element of the sequence, i.e., every \(sequence\_length\) game tuple is not used for training (but only for bootstrapping). While all of these may seem like minor issues, their sum brings a significant burden upon practitioners interested in the data-driven NetHack research.

Resource-wiseThe great addition to the released datasets was an accompanying data loader that operates over compressed game episodes (we refer to it as TTYRec throughout the text). While the Hambro et al. (2022b) demonstrated that the proposed loader could operate at approximately 5 hours per epoch, we observed a slower rate when adapting it for recurrent algorithms. The main issue lies in the underlying format where the data is fetched the following way - \((s_{t},a_{t},r_{t-1})\)5. While this may seem non-significant, it results in the need to fix the tuples sequence to obtain a suitable format \((s_{t},a_{t},s_{t+1},r_{t})\). To better demonstrate the impact of this problem, consider Table 1, where we test the original data loader and the one with the fix, observing a significant increase in the loading time as both batch size and sequence length increase. While this can be avoided with the original loader by simply discarding each \(sequence\_length\) element (as done in the original work), the problem persists as the original implementations do not work without reward shaping with potentials requiring a scan over the sequence, not to mention that such data rejection is not justified beyond performance reasons. As an additional but important issue to resource-constrained practitioners, one must first download the entire 90GB AA dataset, even if they aim to work on a subset of data.

Benchmark-wiseArguably, one of the driving forces in Deep Learning and Reinforcement Learning, in general, is a well-defined set of tasks. To demonstrate how the proposed large-scale dataset could be utilized for tasks definition, Hambro et al. (2022b) described two settings: learning on the whole dataset for all role-race-alignment-sex combinations and learning on the subset of data for the Monk-Human-Neutral setting. While this is a good entry point and could be of great use to practitioners interested in large-scale data regimes, there was no detailed proposal on how one should further define tasks which is indeed an open question in the NetHack learning community (Kuttler et al., 2020). Moreover, the original raw large-scale dataset requires practitioners to manually define SQL queries for extracting the data of interest, which is flexible but could be an overkill.

More importantly, the proposed comparison metrics were mean and median statistics of average episode returns over training seeds. One may argue that the median is a robust statistic. However, in this case, it is a median of _average_ episode returns over training seeds and not of the whole set of evaluation episodes. It is well known in the RL community (Agarwal et al., 2021) that those are not reliable due to the highly-variable nature of RL agents. As of the NetHack, this further amplifies by the extremely-variable nature of the game itself as both demonstrated in Kuttler et al. (2020); Hambro et al. (2022b). Therefore, to reduce the noise in the progress of RL agents in this domain, one would need a different evaluation toolset that is better suited for it.

  
**Variants** & **TTYRec** & **TTYRec**, **Proper Tugles** & **HDF5 (Mexmap)** & **HDF5 (RAM)** & **HDF5 (Compressed)** \\  batch\_size=64, seq\_len=16 & 15ms & 17ms & 2ms & 1ms & 516ms \\ batch\_size=256, seq\_len=32 & 74ms & 132ms & 12ms & 9ms & 2.39s \\ batch\_size=256, seq\_len=64 & 255ms & 372ms & 18ms & 14ms & 2.42s \\   

Table 1: Loading times for different storage formats averaged over 500 iterations. The right part of the table, with HDF5 columns, depicts the loading time for datasets repacked within Katakomba. Note that the transitions are in the proper format for them by design.

Katakomba

Given the issues described in the previous section, we are ready to present Katakomba - a set of tools, benchmarks, and memory-based offline RL baselines. In this section, we gradually introduce the components of the library with accompanying code examples to better demonstrate the ease of use. All the numbers and performance metrics discussed in the text were measured on the same set of hardware: 14CPU, 128GB RAM, NVMe, 1xA100; for more details, please, see Appendix D.

### Benchmark

Listing 1: Usage example: Tasks are defined via the character field that is then used by the offline wrapper for dataset loading and score normalization.

```
fromkatakomba.envimportNetHackChallenge fromkatakomba.envimportOfflineNetHackChallengeWrapper
#Thetaskisspecifiedusingthecharacterfield env=NetHackChallenge( character="mon-hum-neu", observation_keys=["ttyt_chars","ttyt_colors","ttyt_cursor"] )
#Aconvenientwrapperthatprovidesinterfacesfor
#datasetloadingandscorenormalization env=OfflineNetHackChallengeWrapper(env)
```

DecompositionIn our benchmark, we re-use the dataset collected by the AutoAscend bot (Hambro et al., 2022b, a). While this bot is highly-capable, it still is considered an early-game contender because it can not descend further than two levels in half of the episodes. As the dataset becomes an early game bridgehead, we divide the tasks based on the character configurations: role, race, and alignment. In the NetHack community, these are known to be the most important (and even having a dramatic effect), requiring to utilize varying abilities of each role or race6. Overall, in opposition to merging all combinations, this decomposition allows more focus on the characters' gameplay and the size reduction one needs to download for both playing-around and committed research, as we will describe further.

CategoriesThis results in 38 datasets in total. However, it may not be possible for researchers to examine each of them as the training times can be an obstacle. To this end, we further divide the tasks into three categories: Base, Extended, and Complete. We separate each category based on the wisdom of the NetHack community, i.e., that roles have a more substantial effect on the gameplay than race, and race has more effect than alignment. The datasets and categories are listed in Table 2. Base tasks consist of all possible roles for the human race; we choose a neutral alignment where possible; otherwise, we pick the alternative one (for humans, if there is no neutral alignment, there is only one alternative). We include all other role-race combinations for Extended tasks. For Complete, we add tasks that were not included in the Base and Extended categories. Note that these three categories combined cover all of the allowed role-race-alignment combinations.

Data SelectionAs demonstrated in the previous section, the original TTYRec dataset is actually slower than expected when it is used for learning agents. Moreover, one may be unable to download the 90GB dataset. Therefore, we take a different path by subsampling and repacking the original dataset task-wise, averaging 680 episodes and 1.3GB per task. The subsampling procedure is stratified by the game score (for more details, please see the script7). This allows for more versatility: one can download the needed datasets on demand as in D4RL (Fu et al., 2020); furthermore, this permits us to address the rolling issue as we repacked the transition tuples in the way suitable to typical ORL pipeline as a part of the release. To ensure the reliable accessibility of the data, we host it on the HuggingFace Hub (Engstrom et al., 2020).

[MISSING_PAGE_FAIL:5]

```
Listing 2: Usage example: Katakomba provides different loaders that can be chosen depending on one's speed-memory tradeoff.
```
#DecompressthedatasetintofRAM dataset=env.get_dataset(mode="in_memory")
#Decompressionon-read dataset=env.get_dataset(mode="compressed")
#TheoriginaldataloaderintroducedinHambroetal,2022 dataset=env.get_dataset(scale="big")
#Decompressthedatasetondisk dataset=env.get_dataset(mode="memmap")
#Ifyouwanttodeletethedecompresseddataset
#Thiswillnotaffectthecompressedversion dataset.close() ```

HDF5, In-Disk (Compressed, Disk)This mode is the most cheap one but slow. Essentially, the dataset is read on from disk and decompressed on-the-fly. We found this useful for debugging purposes where one does not need the whole training process to be run.

TTYRec, In-Disk (Compressed, Disk)In case one finds the original approach to data loading more suitable, we also provide a convenient interface that wraps the source large-scale dataset and loader. One can also set it up in more detail using keyword arguments from the original TTYRec data loader. However, this option comes with the downsides described in Section 2, i.e., slower iteration speed and the need to download the 90GB dataset at least once.

In addition to the dataset interfaces, we also provide an implementation of a replay buffer suitable for sequential offline RL algorithms for bootstrapping practitioners with ready-to-go tools.

### Evaluation Methodology under High Variability

In Hambro et al. (2022), authors used an average episode return across seeds when comparing baselines. While this is a standard practice in the RL and ORL communities, it was recently shown to be unreliable (Agarwal et al., 2021) as the algorithms are known to be highly variable in performance. This problem amplifies even more for NetHack, where the score distribution is typically quite wide for humans and bots (Kuttler et al., 2020).

To address this, we argue that the evaluation toolbox from Agarwal et al. (2021) is more appropriate and suggest using it when comparing NetHack learning agents. We use these tools for two dimensions: in-game score and death level. The first dimension corresponds to what one typically optimizes with ORL agents but is considered a proxy metric (Kuttler et al., 2020). While the latter lower bounds the early-game progress and is more indicative of the game completion.

Furthermore, similar to Fu et al. (2020), we also suggest reporting normalized scores to capture how far one is from the AutoAscend bot. We use mean scores per dataset as a normalization factor and rescale to  range after normalization, similar to D4RL (Fu et al., 2020). This functionality is also provided as a part of the offline wrapper for the NetHackChallenge environment. Please refer to Appendix D for precise values.

## 4 Benchmarking Recurrent Offline RL Algorithms

AlgorithmsFor our benchmark, we rely on the following set of algorithms: Behavioral Cloning (BC) (Michie et al., 1990), Implicit Q-Learning (IQL) (Kostrikov et al., 2021), Conservative Q-Learning (CQL) (Kumar et al., 2020), Randomized Ensemble Mixture (REM) (Agarwal et al., 2020), and Advantage-Weighted Actor-Critic (AWAC) (Nair et al., 2020). These are known as either the most competitive in the continuous control setting (Tarasov et al., 2022) or were shown to be competitive in the discrete control (Agarwal et al., 2020; Kumar et al., 2022). Similar to Hambro et al. (2022, 2022), we build upon Chaotic-Dwarven-GPT-5 architecture that converts the try observation into an image and then feeds it into the CNN layers followed by the LSTM (Hochreiter and Schmidhuber, 1997). Ultimately, we test five common ORL algorithms augmented with recurrence. To the best of our knowledge, there are no other open-sourced alternatives beyond the Hambro et al. (2022) that also do not implement the AWAC algorithm.

Experimental SetupWe train every algorithm for 500k gradient steps, resulting in around 20 epochs per dataset. We report the scores of the last trained policy over 50 evaluation episodes as standard in the ORL community. Important to highlight that while this amount may seem small for NetHack, this number is adequate when used in conjunction with stratified datasets and RLiable evaluation tools due to the underlying bootstrapping mechanism. For specific hyperparameters used, please either see Appendix E or the configuration files provided in the code repository.

Replicability and Reproducibility StatementTo ensure that our experiments' results are replicable and can easily be reproduced and inspected, we rely on the Weights&Biases (Biewald, 2020). In the provided code repository, one can find all the logs, including configs, dependencies, code snapshots, hyperparameters, system variables, hardware specifications, and more. Moreover, to reduce the efforts of interested parties required for inspection, we structurize the results using the Weights&Biases public reports.

ResultsThe outcomes are twofold. First, the results achieved are similar to the already observed by the Piterbarg et al. (2023) and Hambro et al. (2022), but on a larger number of offline RL algorithms tested. As shown in Figure 1 and Figure 2, all algorithms were unable to replicate the scores of the AutoAscend bot, reaching normalized scores below 6.0 on average and not progressing beyond the first level on the majority of training runs. Notably, only 5% of the episodes resulted in a normalized score of around 20.0 (Figure 0(b)). Moreover, REM has not been able to achieve even the non-zero normalized score. Second, perhaps surprisingly, the only algorithm that does not rely in any way on policy constraints is also the only algorithm that completely failed. This, and also the high correlation

Figure 1: Normalized performance under the Katakomba benchmark for all 38 datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 5700 points for constructing these graphs. As one can see, there is not much improvement beyond naive behavioral cloning.

in the performance profiles (Figure 0(b)), gives us a hint that all other methods showing non-zero results actually rely primarily on behavioral cloning in one form or another, such as advantage weighted regression in IQL and AWAC or KL-divergence as in CQL. Indeed, the most successful hyperparameters in our experiments proved to be those that significantly increase the weight of losses that encourage similarity to the behavioral policy (see Table 10 in the Appendix E). Furthermore, as shown in the Figure 0(c) and Figure 0(c) Behavioral Cloning algorithm is not worse than all the other more sophisticated offline RL algorithms. Thus, NetHack remains a major challenge for offline RL algorithms, and Katakomba can serve as a starting point and testbed for offline RL research in this direction. For graphs stratified by the Base, Extended, and Complete categories, see Appendix F.

## 5 Related Work

Offline RLIn recent years, there was considerable interest of the reinforcement learning community in the offline setting, which resulted in numerous and diverse algorithms specifically tailored for this setup (Nair et al., 2020; Kumar et al., 2020; Kostrikov et al., 2021; Chen et al., 2021; Fujimoto and Gu, 2021; An et al., 2021; Nikulin et al., 2023; Tarasov et al., 2023). The core idea behind most of them is to ensure the resulting policy stays close to the behavioral one. This could be achieved via different ways: penalizing value function (Kumar et al., 2020; An et al., 2021; Nikulin et al., 2023), constraining the policy outputs (Fujimoto and Gu, 2021; Tarasov et al., 2023), or even training directly in the conservative latent space (Zhou et al., 2020; Chen et al., 2022; Akimov et al., 2022). Due to the benchmark-centricity of the RL field, most of the proposed ORL algorithms are for continuous control with a few exceptions (Agarwal et al., 2020; Kumar et al., 2020, 2022). The de-facto standard benchmark is the D4RL (Fu et al., 2020), which provides a suite of datasets focused on continuous control with proprioceptive states under different regimes, such as sparse-reward or low-data regimes. Also, few benchmarks move the focus from proprioceptive states to images or other more complex entities (Qin et al., 2022; Lu et al., 2022; Agarwal et al., 2020).

RL for NetHackNetHack as a testbed for RL agents was introduced in Kuttler et al. (2020). To further advance the RL agents in this domain, the NetHack Challenge Competition (NHC) was held

Figure 2: Death level under the Katakomba benchmark for all 38 datasets. Each algorithm was run for three seeds and evaluated over 50 episodes resulting in 570 points for constructing these graphs. As one can clearly see, there is not much improvement beyond the naive behavioral cloning.

(Hambro et al., 2022) that resulted in two of the most performant agents - learning-based Chaotic-Dwarven-GPT-5, and a rule-based AutoAscend (AA). Notably, the latter outperformed learning-based agents by a wide margin. Consequently, this solution was used to collect the large-scale NetHack Learning Dataset (Hambro et al., 2022). The closest concurrent work is by Piterbarg et al. (2023) - the authors released another AA dataset but accompanied with the hierarchical labels, which arise due to the nature of the AutoAscend bot, demonstrating their usefulness in cloning the AA bot. However, in contrast to our work, Piterbarg et al. (2023) focuses on the large-scale setup similar to Hambro et al. (2022).

## 6 Discussion, Limitations, and Future Work

In this work, we focused on building reliable tools and benchmarks for offline RL setting using the recently released AutoAscend large-scale dataset (Hambro et al., 2022). While this does not cover the whole spectrum of NetHack's community interests, such as offline-to-online regime or learning from human demonstrations, we believe our effort is helpful in establishing reliable and open instruments for data-driven NetHack. Moreover, our contributions could be of interest to the part of the ORL community studying discrete control, memory, and adaptivity (Ghosh et al., 2022).

Given our results and experience with the NetHack Learning Environment, we believe fruitful future research may lie along the following directions: finding a better state-encoder, as the current one presents a bottleneck in both efficiency (rendering is expensive) and locality (only the small part of the terminal is used). Another interesting research direction would be to assess recently appeared recurrence mechanisms such as Linear Recurrent Unit (Orvieto et al., 2023), which might also speed up the training process without hurting the performance. Also, as the interest in generalization properties will appear, it would be a great addition to include more datasets that will provide metadata on the seeds used for data generation, as it will allow to assess trained agents on both seen and unseen seeds to quantify the generalization gap more systematically.

Overall, we firmly believe that NetHack provides a nice playground for investigating how to build a next generation of reinforcement learning agents using prior data that would encompass stronger generalization and memory capabilities. To this end, we plan to continuously maintain the benchmark, accompanying tools, and curate new datasets if considered useful for further advancements.