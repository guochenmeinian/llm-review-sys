# Provably Robust Temporal Difference Learning for Heavy-Tailed Rewards

Semih Cayci

Department of Mathematics

RWTH Aachen University

Aachen, Germany

cayci@mathc.rwth-aachen.de &Atilla Eryilmaz

Department of Electrical and Computer Engineering

The Ohio State University

Columbus, OH 43210

eryilmaz.2@osu.edu

###### Abstract

In a broad class of reinforcement learning applications, stochastic rewards have heavy-tailed distributions, which lead to infinite second-order moments for stochastic (semi)gradients in policy evaluation and direct policy optimization. In such instances, the existing RL methods may fail miserably due to frequent statistical outliers. In this work, we establish that temporal difference (TD) learning with a dynamic gradient clipping mechanism, and correspondingly operated natural actor-critic (NAC), can be provably robustified against heavy-tailed reward distributions. It is shown in the framework of linear function approximation that a favorable tradeoff between bias and variability of the stochastic gradients can be achieved with this dynamic gradient clipping mechanism. In particular, we prove that robust versions of TD learning achieve sample complexities of order \((^{-})\) and \((^{-1-})\) with and without the full-rank assumption on the feature matrix, respectively, under heavy-tailed rewards with finite moments of order \((1+p)\) for some \(p(0,1]\), both in expectation and with high probability. We show that a robust variant of NAC based on Robust TD learning achieves \(}(^{-4-})\) sample complexity. We corroborate our theoretical results with numerical experiments.

## 1 Introduction

In this paper, we develop a framework for robust reinforcement learning in the presence of rewards with heavy-tailed distributions. Heavy-tailed phenomena, stemming from frequently observed statistical outliers, have been ubiquitous in decision-making applications under uncertainty. To name a few examples, waiting times in wireless communication networks [44; 24; 58], completion times of SAT solvers , numerous payoff quantities (e.g., stock prices, consumer signals) in economics and finance [22; 35; 38; 37] exhibit heavy-tailed behavior. An important characteristic of heavy-tailed random variables is the infinite order of higher moments, which stems from the frequently occurring outliers.

In reinforcement learning (RL), the goal is to maximize expected total reward in a Markov decision process (MDP) by continual interactions with the unknown and dynamic environment. Among policy optimization methods, Natural actor-critic (NAC) method and its variants [51; 30; 46; 17; 27; 5; 29] have become particularly prevalent due to their desirable stability and versatility characteristics, emanating from the use of temporal difference (TD) learning as the critic for the policy evaluation component of the NAC operation. The existing theoretical analyses for temporal difference learning [4; 54] and natural policy gradient/actor-critic methods [62; 1; 57] assume that the stochastic gradients have finite second-order moments, or even they are bounded. In particular, it is unknown whether natural actor-critic with function approximation is robust for stochastic rewards of heavy-tailed distributions with potentially infinite second-order moments. Furthermore, in practice, these methodsare prone to non-vanishing and even increasing error under heavy-tailed reward distributions (see Example 1). This motivates us for the following fundamental question in this paper:

_Can temporal difference learning with function approximation be modified to provably achieve global optimality under stochastic rewards with heavy tails?_

We provide an affirmative answer to the above question by proposing a simple modification to the TD learning algorithm, which yields robustness against heavy tails. In particular, we show that incorporating a dynamic gradient clipping mechanism with a carefully-chosen sequence of clipping radii can provably robustify TD learning and NAC with linear function approximation, leading to global near-optimality even under stochastic rewards of infinite variance.

**Example 1** (Failure of TD learning under heavy-tailed reward).: In this example, we consider a randomly-generated discounted-reward Markov reward process1\((X_{t},R_{t})_{t}\) on a state space \(\) with \(||=64\) states, with the discount factor \(=0.9\) and the reward \(R_{t}(X_{t})=r(X_{t})+N_{t}-[N_{t}]\) with \(N_{t}}{{}}(1,1.4)\) for any \(t\). In order to predict the value function, we use (projected) TD learning (see ) with linear function approximation based on Gaussian features of dimension \(d=4\) and projection radius \(=30\).

The performance results are shown in Figure 1. Since \(R_{t}\) is heavy-tailed with infinite variance, the existing convergence results for traditional TD learning, which assume that \(R_{t}\) has finite variance, do not hold. Furthermore, Figure 1 reveals that TD learning is prone to non-varnishing and even increasing error in practice despite the projection step, iterate averaging and small learning rate, due to the statistical outliers that cause extremely large error often as indicated by a non-negligible fraction of green lines in Figure 0(a). On the other hand, with the same learning rate, projection radius and state-reward realizations, our robust variant of TD learning provides resilience against outliers (see Figure 0(b)), and leads to convergence in the expected behavior as in Figure 0(c).

Stochastic rewards with heavy-tailed distributions appear in many important applications. Below, we briefly provide two motivating applications that necessitate robust RL methods to handle heavy tails.

**Application (1): Algorithm portfolios.** In solving complicated problems such as Boolean satisfiability (SAT) and complete search problems, which appear in numerous applications [19; 43], multiple algorithmic solutions with different characteristics are available. The algorithm selection problem is concerned about the minimization of total execution times to solve these problems [32; 49; 31], where different data distributions and machine characteristics, caused by recursive algorithms, are modeled as states, algorithm choices are modeled as actions, and the execution time of a selected algorithm is modeled as the cost (i.e., negative reward). It is well-known that the execution times, i.e., rewards, in the algorithm selection problem have _heavy-tailed distributions_ with infinite-variance (e.g., \((1,1+p)\) with \(0<p<1\) as in ) similar to the case in Example 1[13; 15; 49]. Thus, algorithm selection problem requires robust techniques that we consider here.

**Application (2): Scheduling for wireless networks.** The scheduling problem considers matching the users with random service demands to fading wireless channels (e.g., Gilbert-Elliot model) with stochastic transmission times so as to minimize the expected delay. A widely-adopted approach to

Figure 1: Non-convergent behavior of TD learning under heavy-tailed noise with tail index 1.4. Each faded green line is the MSE for an individual trial, and the solid lines with markers indicate the average mean squared error for TD learning and Robust TD learning.

study the scheduling problem is to use MDPs (see, e.g., [40; 21; 10; 2]). It has been observed that the transmission times follow heavy-tailed distributions of infinite variance, due to various factors including the MAC protocol used, packet size, and channel fading [18; 58; 24; 23]. As such, solving this by using RL approach necessitates robust methods to handle heavy-tailed execution times.

**Main contributions.** Our main contributions in this paper contain the following.

\(\)_Robust TD learning with dynamic clipping for heavy-tailed rewards._ We propose Robust TD learning with a dynamic gradient clipping mechanism, and prove that this TD learning variant with linear function approximation can achieve arbitrarily small estimation error that vanishes at rates \((T^{-})\) and \(}(T^{-p})\) without and with full-rank assumption on the feature matrix, respectively, even for heavy-tailed rewards with moments of order \(1+p\) for \(p(0,1]\). Our proof techniques make use of Lyapunov analysis coupled with martingale techniques for robust statistical estimation in dynamical systems, and can be of independent interest in the analysis of first-order methods.

\(\)_Robust NAC under heavy-tailed rewards._ Based on Robust TD learning and the compatible function approximation result in , we propose a robust NAC variant, and show that \((^{-4-})\) samples suffice to achieve \(>0\) error under standard concentrability coefficient assumptions.

\(\)_High-probability error bounds._ We provide high-probability (sub-Gaussian) error bounds for the robust NAC and TD learning methods in addition to the traditional expectation bounds.

From a statistical viewpoint, our analysis in this work indicates a favorable bias-variance tradeoff: by introducing a vanishing bias to the semi-gradient via particular choices of dynamic gradient clipping, one can achieve _robustness_ by eliminating the destructive impacts of statistical outliers even if the semi-gradient has infinite variance, leading to near optimality.

### Related Work

**Temporal difference learning.** Temporal difference (TD) learning was proposed in , and has been the prominent policy evaluation method. The existing theoretical analyses of TD learning consider MDPs with bounded rewards [4; 7], or rewards with finite variance , while we consider heavy-tailed rewards. Our analysis utilizes the Lyapunov techniques in .

**Policy gradient methods.** Policy gradient (PG), and its variant natural policy gradient (NPG) have attracted significant attention in RL [59; 52; 27]. Recent theoretical works investigate the local and global convergence of these methods in the exact case, or with stochastic and bounded rewards [1; 57; 34; 39; 61]. As such, heavy-tailed rewards have not been considered in these works.

**Bandits with heavy-tailed rewards.** Stochastic bandit variants with heavy-tailed payoffs were studied in multiple works [6; 48; 33; 8]. The stochastic bandit setting can be interpreted as a very simple single-state (i.e., stateless or memoryless) model-based and tabular RL problem. The model we consider in this paper is a model-free RL setting on an MDP with a large state space, which is considerably more complicated than the bandit setting.

**Stochastic gradient descent with heavy-tailed noise.** There has been an increasing interest in the analysis of SGD with heavy-tailed gradient noise recently [56; 11; 16], following the seminal work of . In our work, we consider the RL problem, which has significantly different dynamics than stochastic convex optimization.

**Robust mean and covariance estimation.** In basic statistical problems of mean and covariance estimation [42; 41; 36] and regression , the traditional methods do not yield the optimal convergence rates for heavy-tailed random variables, which led to the development of robust mean and covariance estimation techniques (for reference, see [36; 28]). Our paper utilizes tools from robust mean estimation literature (particularly, truncated mean estimator analysis in ), but considers the more complicated problem of TD learning and policy optimization in a dynamic environment rather than a static mean or covariance estimation problem with iid observations.

### Notation

For a symmetric matrix \(A^{d d}\), \(_{}(A)\) denotes its minimum eigenvalue. \(B_{2}(x,)=\{y^{d}:\|x-y\|_{2}\}\) and \(_{}\{x\}=_{y}\|x-y\|_{2}^{2}\) for any convex \(^{d}\).

Robust TD Learning for Value Prediction under Heavy Tails

First, we consider the problem of predicting the value function for a given discounted-reward Markov reward process with heavy-tailed rewards.

### Value Prediction Problem

For a finite but arbitrarily large state space \(\), let \((X_{t})_{t}\) be an \(\)-valued Markov chain with the transition kernel \(:\). We consider a Markov reward process \((X_{t},R_{t})_{t}\) such that at state \(X_{t}\), a stochastic reward \(R_{t}=R_{t}(X_{t})\) is obtained for all \(t 0\). For a discount factor \([0,1)\), the value function for the MRP \((X_{t},R_{t})_{t}\) is the following:

\[(x)=_{t=1}^{}^{t-1}R_{t}(X_{t}) X_{1}=x,\;x.\] (1)

**Objective.** The goal is to learn \(\) without knowing the transition kernel \(\) by using samples from the system. In particular, for a parameterized class of functions \(\{f_{}::^{d}\}\), the goal is to solve the following stochastic optimization problem with mean squared error:

\[_{^{d}}}_{x}|f_{}(x)- (x)|^{2}.\] (2)

In order to solve (2) under Assumption 1, next we propose a robust variant of temporal difference (TD) learning with linear function approximation [50; 54].

### Robust TD Learning Algorithm

For a given set of feature vectors \(\{(x)^{d}:x\}\) with \(_{x}\|(x)\|_{2} 1\), we use \(f_{}()=,()\) as the approximation architecture. For a given dataset \(=\{(X_{t},R_{t},X^{}_{t}) :t\}\) with \(X^{}_{t}(X_{t},)\), let the stochastic semi-gradient at \(^{d}\) be defined as

\[g_{t}()=R_{t}(X_{t})+ f_{}(X^{}_{t})-f_{} (X_{t})_{}f_{}(X_{t}).\]

Robust TD learning is summarized in Algorithm 1.

``` Inputs: number of steps \(T 1\), clipping radii \((b_{t})_{t[T]}\), projection radius \(>0\), step-size \(>0\)  Set \((1) B_{2}(0,)\)\(\)\(\)initialization for\(t=1,2,,T\)do \((t+1)=_{B_{2}(0,)}(t)+_{t} g_{t}(t ) 1\{\|g_{t}(t)\|_{2} b_{t}\}}\) endfor Output:\(f_{(T)}()=(T),()\) where \((T)=_{t=1}^{T}(t)\) ```

**Algorithm 1**Robust TD learning

In the following, we will establish finite-time bounds for Robust TD learning by specifying the sequence of dynamic gradient clipping radii \((b_{t})_{t 1}\), projection radius \(\) and step-size \(\).

### Finite-Time Bounds for Robust TD Learning

We make the following assumptions on the Markov reward process.

**Assumption 1**.: The stochastic process \((X_{t},R_{t})_{t}\) satisfies the following:

1. Ergodicity: \((X_{t})_{t}\) is an irreducible and aperiodic Markov chain with stationary distribution \(=\). Also, we assume that there are constants \(m>0,(0,1)\) such that \[_{x}\|^{t}(x,)-\|_{} m^ {t},\; t_{+}.\] (3)
2. Heavy-tailed reward: For some \(p(0,1]\) and constant \(u_{0}(0,)\), \[[|R_{t}(X_{t})|^{1+p}|X_{t}] u_{0}<,\;a.s., t .\] (4)
3. Mean reward: For any \(t\), \([R_{t}(X_{t})|X_{t}]=r(X_{t})[-1,1]\) a.s.

We note that the uniform ergodicity and bounded mean reward assumptions are standard in TD learning literature .

**Assumption 2** (Sampling).: We consider two types of sampling strategies in this work:

_2a. IID sampling:_\(X_{t}}{{}}\) and \(X^{}_{t}(X_{t},)\) for all \(t 1\).

_2b. Markovian sampling:_\(X_{1}\) and \(X^{}_{t}=X_{t+1}(X_{t},)\) for all \(t 1\).

**Assumption 3** (Realizability).: There exists \(^{} B_{2}(0,)\) such that \(()=^{},()\).

**Remark 1**.: We note that Assumption 3 holds directly in interesting realizable problem classes, e.g., linear MDPs , and allows us to obtain results on the statistical error performance of our design. In cases when it does not hold, our results will continue to hold with an additional function approximation error proportional to \(_{ B_{2}(0,)}[|(x)-, (x)|^{2}]}\), which is unavoidable due to the limitation of the linear function approximation architecture.

The following lemma is important in bounding the moments of the gradient norm under Robust TD learning in terms of the projection radius \(>0\) and the upper bound \(u_{0}\) on \(|R_{t}(X_{t})|^{1+p}X_{t}\).

**Lemma 1** (Tail bounds for \(\|g_{t}(t)\|_{2}\)).: Let \(^{+}_{t}=(1),(2),,(t),X_{t} \) for \(t_{+}\). Then, under Assumption 1, we have:

\[[\|g_{t}((t))\|_{2}^{1+p}|^{+}_{t}] u<, \ a.s.,\] (5)

for any \(t_{+}\), where \(u=\{(u_{0}^{}+2)^{1+p},u_{0}+2^{2p+3}^{1+p}\}\).

Proof.: Note that we have \([\|g_{t}((t))\|_{2}^{1+p}|^{+}_{t}][ |R_{t}(X_{t})+ f_{(t)}(X^{}_{t})-f_{(t)}(X_{t})|^{1+p }|^{+}_{t}]\) since \(_{x}\|(x)\|_{2} 1\). The upper bounds then follow by applying Minkowski's inequality and the triangle inequality for \(L^{p}\) spaces, respectively, to this inequality. 

This lemma will be useful in the analysis of both the expected (Theorems 1-2), and the high-probability bounds (Theorem 3) on the performance of Robust TD learning.

Next, we provide the main theoretical results in this paper: finite-time bounds for Robust TD learning. The proofs are mainly deferred to the appendix, while we provide a proof sketch for Theorem 3. In the following, we provide convergence bounds for the expected mean squared error under Robust TD learning with various choices of \(b_{t}\).

**Theorem 1** (Expected error under Robust TD learning - iid sampling).: Under Assumptions 1, 2a, 3, we have the following bounds for Robust TD learning:

**a)** For \(b_{t}=(ut)^{}\) for any \(t_{+}\) and \(_{t}==}}\), we have:

\[}_{(1),(2),,(T) \\ x}-1.422638pt(x)- (T),(x)^{2}}}{(1-)T^{}},\  T>1.\] (6)

**b)** Let \(=_{x}(x)(x)^{}(x)\), and \(_{p}(u,_{},,)= 4+}}\). If \(_{}()=_{}>0\), then with the diminishing step-size \(_{t}=}}\) and \(b_{t}=t\) for \(t_{+}\), for the average iterate \((T)\), we have2:

\[}_{(1),(2),,(T) \\ x}-1.422638pt(x)-(T),(x)^{2}_{p}(u,_{},,)_{p,1}T^{-p}}{1-p}+_{p,1})(eT)}{T},\] (7)

and for the last iterate \((T+1)\), we have:

\[}_{(1),,(T)}_{x}|(x)-(x),(T+1)|^{2}_{p}(u, _{},,)}{_{}}_{p,1})}{T}+_{p,1}T^{-p}}{1-p},\] (8)

for any \(T>1\), where \(_{x,y}=1\) if \(x y\) and 0 otherwise.

**Remark 2**.: The convergence rates in Theorem 1 are \((T^{-})\) and \(}(T^{-p})\) without and with the full-rank assumption \(_{}>0\), respectively. For \(p=1\), the convergence rates stated in Theorem 1 both match the existing results for TD learning with bounded rewards , up to a larger scaling factor of raw second-order moment rather than variance, due to clipping centered around 0.

**Remark 3** (Finite-time bounds in the unrealizable case).: We note that our results hold without Assumption 3 as well. In this general case, there will be an additional function approximation error proportional to \(_{ B_{2}(0,)}[|(x)-, (x)|^{2}]}\) (see Remark 1). One would use a richer function approximation scheme (e.g., larger projection radius \(\) or dimension \(d\)) to reduce this function approximation error, which will lead to an increase in the statistical error as we characterize in our bounds. For the extension of our analysis to the general case without Assumption 3, please see Appendix A.1.

In the following, we provide convergence bounds for Robust TD learning under Markovian sampling.

**Theorem 2** (Expected error under Robust TD learning - Markovian sampling).: Under Assumptions 1,2b and 3, let \(T>1\), \(>0\) be given, and define the mixing time \(=\{t_{+}:m^{t}(uT)^{-}\}\). Then, with \(_{t}==(uT)^{-}\), Robust TD learning yields the following:

\[(1),(2),,(T)\\ x}{}(x)-( T),(x)^{2}}}{(1-)T^{}}+ (1+2)(4+(1+6))}{(1-)T^{}}.\] (9)

The proof of Theorem 2 is based on a similar Lyapunov technique as Theorem 1 in conjunction with the mixing time analysis in  for Markovian sampling, and can be found in Appendix A.

The bounds in Theorem 1 involve expectation over the parameters \((t),t[T]\). In the following, we provide a high-probability error bound on the mean squared error under Robust TD learning.

**Theorem 3** (High-probability bound for Robust TD learning).: _For any \((0,1)\), let \(L_{}=(4/)\). Under Assumptions 1, 2a, 3, with step-size \(=(1-) L_{}^{}}{(uT)^{}}\) and clipping radius \(b_{t}=}^{}\),_

\[_{x}(x)(x)-(T ),(x)^{2}}}{(1-) T^{}}3L_{}^{-}+7L_{}^{},\] (10)

_holds with probability at least \(1-\)._

In the following, we give a proof sketch for Theorem 3. The full proof can be found in Appendix A.

Proof sketch.: Let \(()=\|-^{}\|_{2}^{2}\) be the Lyapunov function, and \(_{t}=1-_{t}=\{\|g_{t}\|_{2} b_{t}\}\). Then, the Lyapunov drift can be decomposed as follows:

\[((t+1))-((t)) 2_{t}[g_{t}^{ }((t)-^{})]+^{2}_{t}[\|g_{t}\|_{2}^{2} _{t}]+2 B(t)+^{2}Z(t),\] (11)

where

\[B(t)=g_{t}^{}((t)-^{})_{t}-_{t}[g_{t}^{ }((t)-^{})_{t}]-_{t}[g_{t}^{}((t)- ^{})_{t}],\]

is the bias in the stochastic semi-gradient, and

\[Z(t)=\|g_{t}\|_{2}^{2}_{t}-_{t}[\|g_{t}\|_{2}^{2}_{t}].\]

We can decompose \(B(t)\) further into a martingale difference sequence

\[B_{0}(t)=g_{t}^{}((t)-^{})_{t}-_{t}[g_{t}^ {}((t)-^{})_{t}],\]

and a bias term

\[B_{}(t)=-_{t}[g_{t}^{}((t)-^{})_ {t}].\]

By Freedman's inequality for martingales , we have \(_{t=1}^{T}B_{0}(t)}L_{}^{ }}{T^{}}\), and by Azuma inequality, we have \(_{t=1}^{t}Z(t)}T^{}}{L_{ }^{}}\), each holding with probability at least \(1-/2\).

By Holder's inequality and Lemma 1, we can bound \(B_{}(t) ub_{t}^{-p}\) and \(_{t}[\|g_{t}\|_{2}^{2}_{t}] ub_{t}^{1-p}\), both with probability 1. Finally, by Lemma 2 in , we have the negative drift term

\[_{t}[g_{t}^{}((t)-^{*})]-(1-)_{x}(x) (f_{(t)}(x)-(x))^{2}.\]

By telescoping sum of (11) and rearranging the terms, we have:

\[_{t=1}^{T}\|f_{(t)}-\|_{}^{2}((1))}{2(1-)T}+_{t=1}^{T} B(t)+_{t=1}^{T}Z(t)+_{t}[\|g_{t}\|_{2}^{2} _{t}].\]

The proof is concluded by substituting the above high probability bounds on the sample means of \(B(t)\), \(Z(t)\) and \(_{t}[\|g_{t}\|_{2}^{2}_{t}]\) (via union bound and integral upper bounds), and using Jensen's inequality on the left side of the above inequality. 

Most notably, this important theorem establishes that, by appropriately controlling the bias term of dynamic gradient clipping to yield a vanishing sample mean with high probability as the number of iterations increases, one can limit the variance of the semi-gradient, thereby resulting in the provided global near-optimality guarantee.

## 3 Robust Natural Actor-Critic for Policy Optimization under Heavy Tails

In this section, we will study a two-timescale robust natural actor-critic algorithm (Robust NAC, in short) based on Robust TD learning, and provide finite-time bounds.

### Policy Optimization Problem

We consider a discounted-reward Markov decision process (MDP) with a finite but arbitrarily large state space \(\), finite action space \(\), transition kernel \(\) and discount factor \((0,1)\). The controlled Markov chain \(\{(S_{t},A_{t}):t\}\) has the probability transition dynamics \((S_{t+1} s|S_{1}^{t},A_{1}^{t})=_{A_{t}}(S_{t},s),\) for any \(s\). Taking the action \(A_{t}\) at state \(S_{t}\) yields a random reward of \(R_{t}(S_{t},A_{t})\) at any \(t_{+}\). For a given stationary randomized policy \(=((a|s))_{(s,a)}\), the value function \(^{}\) and the state-action value function (also known as Q-function) \(^{}\) are defined as:

\[^{}(s) =^{}_{t=1}^{}^{t-1}R_{t}(S_{ t},A_{t})S_{1}=s,\;s\] (12) \[^{}(s,a) =^{}_{t=1}^{}^{t-1}R_{t}(S_{ t},A_{t})S_{1}=s,A_{1}=a,\;(s,a).\] (13)

**Remark 4** (From MDP to MRP).: Under any stationary randomized policy \(\), the process \((S_{t},A_{t})_{t>0}=:(X_{t})_{t>0}\) is a Markov chain over the state-space \(=\), thus \((X_{t},R_{t})\) with \(R_{t}(X_{t})=R_{t}(S_{t},A_{t})\) is a Markov reward process of the kind that we analyzed in Section 2. As such, we can use Robust TD learning to evaluate \((x)=^{}(x)\) for any \(x=(s,a)\).

**Heavy-tailed reward.** We assume that the process \((X_{t},R_{t})_{t>0}\) with the Markov chain \(X_{t}=(S_{t},A_{t})\) and the reward \(R_{t}=R_{t}(X_{t})\) satisfies Assumption 1. We denote the stationary distribution of \(X_{t}=(S_{t},A_{t})\) under \(\) as \(^{}\).

**Objective.** For an initial state distribution \(\), the objective in this work is to find the following:

\[^{*}_{}\ _{}^{}(s)(ds)=: ^{}(),\] (14)

over the class of stationary randomized policies.

**Policy parameterization.** In this work, we consider a finite but arbitrarily large state space \(\), and for such problems, the tabular methods do not scale [51; 3]. In order to address this scalability issue, we consider widely-used softmax parameterization with linear function approximation: for a given set of feature vectors \(\{(s,a)^{d}:s,a\}\) and policy parameter \(W^{d}\),

\[_{W}(a|s)=(s,a))}{_{a^{}} (W^{}(s,a^{}))},\;(s,a).\] (15)

In the following subsection, we will describe the robust natural actor-critic algorithm.

### Robust Natural Actor-Critic Algorithm

For any iteration \(k 1\), we denote \(_{k}:=_{W(k)}\) throughout the policy optimization iterations.

For samples \(^{(k)}=\{(S_{t,k},A_{t,k},R_{t,k},S^{}_{t,k},A^{}_{t,k}):t  1\}\), given \((b_{t,k})_{t,k_{+}}\) and \(>0\), Robust NAC Algorithm is summarized in Algorithm 2.

``` Inputs: clipping radii \((b_{t})_{t 1}\), projection radius \(>0\), learning rate \(>0\), \(L_{}>0\) for\(k=1,2,,K\)do  Set \(_{k}(1)=0\)//initialization: max-entropy policy for\(t=1,2,,T\)do  Set \(g^{(k)}_{t}(_{k}(t))=R_{t,k}+ f_{_{k}(t)}(S^{}_{ t,k},A^{}_{t,k})-f_{_{k}(t)}(S_{t,k},A_{t,k})(S_{t,k},A_{t,k})\). \(_{k}(t+1)=_{B_{2}(0,)}_{k}(t)+_{t} g^{(k) }_{t}_{k}(t)\{\|g^{(k)}_{t}_{ k}(t)\|_{2} b_{t}\}}\) endfor \(W(k+1)=W(k)+_{t=1}^{T}_{k}(t)\) endfor ```

**Algorithm 2**Robust Natural Actor-Critic Algorithm

**Remark 5**.: The optimal solution \(_{k}^{*}*{arg\,min}_{^{d}}\ \ _{x=(s,a)} ,(x)-^{_{k}}(x)^{2}\) is a good approximation of the natural policy gradient:

\[u_{k}=[G(_{k})]^{-1}_{W}^{_{k}}() *{arg\,min}_{w^{d}}\ \ _{(s,a)_{}^{_{k}}_{k}(|s)}  w,_{W}_{k}(a|s)-^{_{k}}(s,a) ^{2},\]

which follows from Jensen's inequality and leads to the Q-NPG . For a detailed discussion, refer to Appendix B.

### Finite-Time Bounds for Robust Natural Actor-Critic

In this subsection, we will provide finite-time bounds for Robust NAC.

We assume that the resulting Markov reward process under \(_{k}\) for each \(k\) satisfies Assumptions 1-3 with stationary distribution \(^{_{k}}\) and \(u_{k}[|R_{t,k}(S_{t,k},A_{t,k})|^{1+p}|S_{t,k},A_{t,k}]\). We assume that the dataset \(^{(k)}\) is obtained independently at each iteration \(k 1\) for simplicity, with \((S_{t,k},A_{t,k})^{_{k}}\) and \(S^{}_{t,k}_{A_{t,k}}(S_{t,k},)\) and \(A_{t,k}_{k}(|S_{t,k})\) according to Assumption 2a under the stationary distribution \(^{_{k}}=[^{_{k}}(s,a)]_{s S,a}\) under \(_{k}\). We make the following standard assumption for policy optimization, which is common in the policy gradient literature .

**Assumption 4** (Concentrability).: _For any \(k 1\), we assume that there exists \(C_{}<\) such that:_

\[_{(s,a)}_{}^{^{*}}( s)^{*}(a|s)}{^{_{k}}(s,a)} C_{},\] (16)

_where \(^{_{k}}\) is the stationary distribution of \((S_{t,k},A_{t,k})_{t 1}\) under \(_{k}\)._

**Theorem 4** (Finite-time bounds for Robust NAC).: Under Assumptions 1-4 for any \(k 1\), for any \((0,1)\) and \(T,K>1\), Robust NAC with \(_{k}\|_{k}^{*}\|_{2}\), \(b_{t,k}=T}{(4T/)}^{}\), learning rates \(=(1-) L_{}^{}}{(_{1  k K}u_{k}T)^{}}\) and \(=}{}\) achieves the following with probability at least \(1-\):

\[_{1 k K}\{^{^{*}}()-^{_{k}}( )\}|}}{(1-)}+u_{k})^{}C_{}}{(1- )^{3}T^{}}}3L_{}^{-}+7L_{ }^{},\]

where \(L_{}=(4T/)\).

[MISSING_PAGE_FAIL:9]

## 5 Conclusion

In this paper, we considered RL problem with heavy-tailed rewards, and proposed robust TD learning and NAC variants with a dynamic gradient clipping mechanism with provable performance guarantees, both in expectation and with high probability. Motivated by the results in this work, it would be interesting to explore single-timescale robust NAC and off-policy NAC for future work.