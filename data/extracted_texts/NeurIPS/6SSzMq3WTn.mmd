# Improved Regret of Linear Ensemble Sampling

Harin Lee

Seoul National University

Seoul, South Korea

harinboy@snu.ac.kr&Min-hwan Oh

Seoul National University

Seoul, South Korea

minoh@snu.ac.kr

###### Abstract

In this work, we close the fundamental gap of theory and practice by providing an improved regret bound for linear ensemble sampling. We prove that with an ensemble size logarithmic in \(T\), linear ensemble sampling can achieve a frequentist regret bound of \(}(d^{3/2})\), matching state-of-the-art results for randomized linear bandit algorithms, where \(d\) and \(T\) are the dimension of the parameter and the time horizon respectively. Our approach introduces a general regret analysis framework for linear bandit algorithms. Additionally, we reveal a significant relationship between linear ensemble sampling and Linear Perturbed-History Exploration (LinPHE), showing that LinPHE is a special case of linear ensemble sampling when the ensemble size equals \(T\). This insight allows us to derive a new regret bound of \(}(d^{3/2})\) for LinPHE, independent of the number of arms. Our contributions advance the theoretical foundation of ensemble sampling, bringing its regret bounds in line with the best known bounds for other randomized exploration algorithms.

## 1 Introduction

Ensemble sampling  has emerged as an empirically effective randomized exploration technique in various online decision-making problems, such as online recommendation [17; 27; 26] and deep reinforcement learning [18; 19; 20]. Despite its popularity, the theoretical understanding of ensemble sampling has lagged behind, even for the linear bandit problem, with previous results revealing sub-optimal outcomes. For instance, a prior work  demonstrated that linear ensemble sampling could achieve \(()\)_Bayesian_ regret with an ensemble size growing at least linearly with \(T\). However, the requirement for the ensemble size to be linear in \(T\) is highly unfavorable and prohibitive in many practical settings. A recent work  showed that a symmetrized version of linear ensemble sampling could provide an improvement in dependence on ensemble size of \((d T)\) and show a frequentist regret bound of \(}(d^{5/2})\). However, this regret bound clearly falls short of the existing frequentist regret achieved by standard randomized algorithms such as Thompson Sampling (TS) [4; 2] and Perturbed-History Exploration (PHE) [11; 12; 13].1

In this work, we close this fundamental gap by providing an improved regret bound for linear ensemble sampling. We prove that linear ensemble sampling with an ensemble size logarithmic in \(T\) can still attain a frequentist regret bound of \(}(d^{3/2})\), marking the first time that linear ensemble sampling achieves a state-of-the-art result for randomized linear bandit algorithms. Ourapproach not only improves upon the regret bound but also simplifies the algorithm by avoiding the use of symmetrized perturbations, making it more practical for implementation. For regret analysis, we present a general, concise framework for analyzing linear bandit algorithms, which may be of independent interest. Furthermore, we rigorously reveal the significant relationship between ensemble sampling and PHE for the first time, showing that in the regime where the ensemble size equals \(T\), linear PHE (LinPHE) is a special case of linear ensemble sampling. With this new insight, we can use the regret analysis for ensemble sampling to derive a new regret bound of \(}(d^{3/2})\) for LinPHE, which is independent of the number of arms \(K\).

Our main contributions are summarized as follows:

* We prove a \(}(d^{3/2})\) regret bound (Theorem 1) for linear ensemble sampling with an ensemble size of \(m=(K T)\), where \(K\) denotes the number of arms. Importantly, our regret bound does not depend on \(K\) or \(m\) even logarithmically. Our result is the first to establish \(}(d^{3/2})\) regret for linear ensemble sampling with an ensemble size sublinear in \(T\), improving the previous bound by the factor \(d\) while maintaining the ensemble size to be logarithmic in \(T\).
* As part of the regret analysis, we present a general regret analysis framework (Theorem 2) for linear bandit algorithms. This framework not only generalizes the regret analysis of randomized algorithms such as ensemble sampling and PHE but also applies to other optimism-based deterministic algorithms. This result can be of independent interest beyond ensemble sampling.
* We rigorously investigate the relationship between linear ensemble sampling and LinPHE. We show that in the regime of ensemble size \(m=T\), LinPHE is a special case of the linear ensemble sampling algorithm. To our best knowledge, this is the first result to show the equivalence between linear ensemble sampling and LinPHE.
* As a byproduct, with this new insight into the relationship between linear ensemble sampling and LinPHE, we provide an alternate analysis for LinPHE as an extension of the analysis for linear ensemble sampling, achieving a \(}(d^{3/2})\) regret bound with no dependence on \(K\).

### Related Work

The stochastic linear bandit problem [5; 1; 14] is a foundational sequential decision-making problem and a core model for multi-armed bandits with features. Numerous algorithms have been developed for this problem, including deterministic approaches such as UCB-based methods [5; 8; 1] and randomized algorithms such as Thompson sampling [24; 7; 4; 2] and PHE [11; 12; 13].

Thompson sampling , a classical randomized method, utilizes the posterior distribution of hidden parameters based on observed data. Initially proposed for Bayesian settings [22; 23], it has also demonstrated strong performance in frequentist settings [3; 4; 2]. For the stochastic linear bandit, Agrawal and Goyal  showed that Thompson sampling with a Gaussian prior achieves a regret bound of \(}(d^{3/2})\), which can be reduced to \(}(d)\) for small \(K\). However, applying Thompson sampling to more complex problems remains challenging, especially when posterior computation becomes intractable, though approximate methods have been proposed [16; 25].

PHE [11; 12; 13] is another class of randomized algorithms that does not rely on posterior distributions, making it potentially applicable to more complex settings. In the finite-armed linear bandit model, PHE achieves a \(}(d)\) regret bound, matching the performance of Thompson sampling for finite arms . However, the relationship between PHE and ensemble sampling remains unexplored in previous studies.

Ensemble sampling  has gained popularity as a randomized exploration method across various decision-making tasks [17; 27; 26; 18; 19; 20]. Despite its empirical success, its theoretical foundation, particularly for linear bandits, is still relatively underdeveloped. Qin et al.  showed that linear ensemble sampling achieves \(()\) Bayesian regret but requires an impractically large ensemble size that scales linearly with \(T\). More recently, Janz et al.  reduced the dependence on ensemble size to \((d T)\) and achieved a frequentist regret bound of \(}(d^{5/2})\). However, the frequentist regret bound of \(}(d^{3/2})\) for ensemble sampling has yet to be achieved.

## 2 Preliminaries

### Notations

\(\) denotes the set of natural numbers starting from \(1\). For a positive integer \(M\), \([M]\) denotes the set \(\{1,2,,M\}\). \(_{d}\) denotes the zero vector in \(^{d}\) and \(I_{d}\) denotes the identity matrix in \(^{d d}\). We define and work within a probability space \((,,)\), where \(\) is the sample space, \(\) is the event set, and \(\) is the probability measure. \((,)\) denotes the uni- or multi-variate Gaussian distribution with mean \(\) and covariance \(\). \(\) denotes logical conjunction ("and") and \(\) denotes logical disjunction ("or"). With slight abuse of notation, we write \(\{:A\}\) and \(A\) interchangeably when \(A\) is some condition, for simplicity. \(()\) denotes the asymptotic growth rate with respect to problem parameters \(d\), \(T\), and \(K\). \(}()\) further hides logarithmic factors of \(T\) and \(d\).

### Problem Setting

We consider the stochastic linear bandit problem. The learning agent is presented with a non-empty arm set \(^{d}\). For \(T\) time steps, where \(T\) is the time horizon, the agent selects an arm \(X_{t}\) and receives a real-valued reward \(Y_{t}\), where the reward is generated based on a hidden true parameter vector, \(^{*}^{d}\). Specifically, \(Y_{t}\) is defined as follows:

\[Y_{t}=X_{t}^{}^{*}+_{t}\,,\]

where \(_{t}\) is a zero-mean random noise. The objective of the agent is to maximize the cumulative reward, or equivalently, to minimize the cumulative regret \(R(T)\) defined as

\[R(T):=_{t=1}^{T}(_{x}x^{}^{*}-X_{t}^{ }^{*})\,.\]

## 3 Ensemble Sampling for Linear Bandits

Algorithm 1 describes linear ensemble sampling. The learner maintains an ensemble of \(m\) estimators, where each estimator fits perturbed rewards. For the \(j\)-th estimator, a random vector \(W^{j}^{d}\) acts as an initial perturbation on the estimator, and a random variable \(Z_{t}^{j}\) perturbs the reward at time \(t\). Specifically, \(_{i}^{j}\) is the solution of the following minimization problem:

\[^{d}}{}\,\|-W^ {j}/\|_{2}^{2}+_{i=1}^{t}(X_{i}^{}-(Y_{i }+Z_{i}^{j}))^{2}\] (1)

When selecting an arm, one of the \(m\) estimators is chosen according to an ensemble sampling distribution \(_{t}\) and acts greedily with respect to the sampled estimator. Previous ensemble samplingalgorithms [16; 21; 10] sample the estimators uniformly from the ensemble, but we allow any policy for selecting the estimator. Further distinguishing from the algorithm presented in Janz et al. , we do not sample Rademacher random variables for symmetrization, making our algorithm simpler.

Ensemble sampling is capable of being generalized to complex settings whenever solving minimization problem (1) is tractable. Especially when incremental updates of the minimization problem are cheap, for instance with neural networks or other gradient descent-based models, ensemble sampling can be an efficient exploration strategy. The algorithm may simply store an ensemble of models, sample one to select an action, and then update the models incrementally based on the observed reward and generated perturbation.

## 4 Regret Bound of Linear Ensemble Sampling

Before we present the regret bound of linear ensemble sampling (Algorithm 1), we present the following standard assumptions on the problem structure.

**Assumption 1** (Arm set and parameter).: \(\) _is closed and for all \(x\), \(\|x\|_{2} 1\). There exists \(S>0\) such that \(\|^{*}\|_{2} S\). Both bounds are known to the agent._

**Remark 1**.: Under Assumption 1, \(\) is a compact set. Therefore, we can define \(x^{*}:=*{argmax}_{x}x^{}^{*}\) and rewrite the definition of \(R(T)\) as \(_{t=1}^{T}x^{*}^{*}-X_{t}^{}^{*}\).

For a rigorous statement of the second assumption, we define several filtrations. For \(t[T]\{0\}\), let \(_{t}^{X}:=(X_{1}, X_{t})\) and \(_{t}^{}:=(_{1},,_{t})\) be the \(\)-algebras generated by \(X_{i}\) and \(_{i}\) up to time \(t\) respectively. We also define the \(\)-algebra generated by the algorithm's internal randomness up until the choice of \(X_{t}\) as \(_{t}^{A}\). Let \(_{t}:=(_{t}^{A}_{t}^{X} _{t}^{})\) be the \(\)-algebra generated by the first \(t\) iterations of the interaction between the environment and the agent. In addition, let \(_{t}^{-}:=(_{t}^{A}_{t}^{X} _{t-1}^{})\) be the \(\)-algebra generated in the same way as \(_{t}\), but excluding \(_{t}\).

**Assumption 2** (Noise).: _There exists \( 0\) such that \(_{t}\) is \(_{t}^{-}\)-conditionally \(\)-subGaussian for all \(t[T]\), i.e., \([(s_{t})|_{t}^{-}]( ^{2}s^{2}/2)\) holds almost surely for all \(s\)._

Now, we define a value \(_{t}\) to describe the variance of the generated perturbation values. Define a sequence \(\{_{t}()\}_{t=0}^{}\) as \(_{t}():=)+2 }+S\). We may omit \(\) when its value is clear from the context. The definition of \(_{t}\) comes from Abbasi-Yadkori et al.  as a confidence radius of the ridge estimator, which we later specify in Lemma 1. We now present the regret bound of linear ensemble sampling (Algorithm 1).

**Theorem 1** (Regret bound of linear ensemble sampling).: _Fix \((0,1]\). Assume \(||=K<\) and run Algorithm 1 with \( 1\), \(m C(K T+)\), \(_{t}=(_{d},_{T}^{2}I_{d}),_{R}=(0,_{T}^{2})\), and \(_{t}=(m)\), where \(C\) is a universal constant and \((m)\) denotes the uniform distribution over \([m]\). Then, with probability at least \(1-4\), the cumulative regret of Algorithm 1 is_

\[R(T)=(d T)^{}\,.\]

Discussion of Theorem 1.Theorem 1 shows that Algorithm 1 achieves a \(}(d^{3/2})\) frequentist regret bound with an ensemble size of \(m=(K T)\). Importantly, our regret bound does not depend on \(K\) or \(m\) even logarithmically. Hence, this regret bound matches the state-of-the-art frequentist regret bound of linear Thompson sampling [4; 2]. Our result is the first to establish \(}(d^{3/2})\) regret for linear ensemble sampling with an ensemble size sublinear in \(T\), improving the previous bound by the factor \(d\) compared to the existing result in Janz et al. . We conjecture that \(}(d^{3/2})\) regret is highly likely to be the best bound for linear ensemble sampling based on the negative result in Hamidi and Bayati  for LinTS.2 Comparing with the algorithm in Janz et al. , our version of linear ensemble sampling algorithm does not utilize Rademacher random variable for symmetrized perturbation. This allows our algorithm to be simpler than that of Janz et al. . Partially due to this algorithmic difference, our regret analysis is quite distinct from the analysis of Janz et al.  (see the proof in Section 5.2).

As in Lu and Van Roy  and Qin et al. , we study the finite-armed problem setting. Both studies analyze the excess regret of ensemble sampling compared to Thompson sampling through an information theoretical approach. However, direct comparisons of the regret bounds are non-trivial. The analysis by Lu and Van Roy  includes an error admitted by the authors and Qin et al.  analyze the Bayesian regret, which is a weaker notion of regret than the frequentist regret that we analyze in this work. Along with Janz et al. , our result also makes a progress in reducing the size of the ensemble compared to Lu and Van Roy  and Qin et al. . The size of the ensemble required by Janz et al.  is \((d T)\).This requirement implies that their ensemble size may be smaller than ours when \(K\) is larger than \(d\) and also allows \(K\) to be infinite. However, it is important to note that their resulting regret bound of \(}(d^{5/2})\) is clearly sub-optimal compared to regret bounds of other randomized exploration algorithms. Theorem 1 achieves the tighter regret bound while simultaneously reducing the size of the ensemble.

**Remark 2** (**Counter-intuitive dependence on ensemble size** in Janz et al. ).: The regret bound in Janz et al.  actually grows super-linearly with the ensemble size, which is counter-intuitive. Their regret bound implies that as the ensemble size increases, the performance of the algorithm deteriorates. This fails to explain the superior empirical performance observed for ensemble sampling even with a large ensemble. On the contrary, our result in Theorem 1 does not show any performance degradation as the ensemble size increases.

**Remark 3** (**Generalizability of perturbation distributions**).: We show that Gaussian distribution for perturbation is not essential. The only properties of the Gaussian distribution we utilize are its tail probability and anti-concentration property, stated as Lemma 4 and Fact 1 in Section 5.2. Therefore, any other distributions exhibiting similar behaviors can instead be adopted. In Appendix H, we rigorously demonstrate that any symmetric subGaussian distribution with lower-bounded variance can be employed, possibly at a cost of a constant factor. A large class of distributions, including uniform distribution, spherical distribution, Rademacher distribution, and centered binomial distribution with \(p=1/2\) satisfy this condition. This result can be of independent interest.

## 5 Analysis

### General Regret Analysis for Linear Bandits

We begin by presenting a general regret bound for any algorithm that selects the best arm based on an estimated parameter. This result can be of independent interest. This general bound and analysis serve as a general framework that includes the regret analysis of linear ensemble sampling (Theorem 1).

**Theorem 2** (General regret bound for linear bandit algorithm).: _Fix \(T\). Assume that at each time step \(t[T]\), the agent chooses \(X_{t}=*{argmax}_{x}x^{}_{t}\), where \(_{t}^{d}\) is chosen by the agent under some (either deterministic or random) policy. Let \(>0\) and \(V_{t}= I+_{i=1}^{t}X_{i}X_{i}^{}\). Let \(\{_{1,t}\}_{t=1}^{T}\) and \(\{_{2,t}\}_{t=1}^{T}\) be sequences of events that satisfy two conditions:_

1. _(Concentration) There exists a constant_ \(>0\) _such that_ \[\|_{t}-^{*}\|_{V_{t-1}}\{_{1,t}\}\] (2) _holds almost surely for all_ \(t[T]\)_._
2. _(Optimism)_ \(_{2,t}_{t-1}\) _holds and there exists a constant_ \(p(0,1]\) _such that_ \[(({x^{*}}^{}^{*} X_{t}^{}_{t} _{1,t})_{2,t}^{}_{t-1}) p\] (3) _holds almost surely for all_ \(t[T]\)_._

   Paper & Frequentist / Bayesian & Regret Bound & Ensemble Size \\  Lu and Van Roy  & Frequentist & Invalid & Invalid \\ Qin et al.  & Bayesian & \(}()\) & \((KT)\) \\ Janz et al.  & Frequentist & \(}(d^{5/2})\) & \((d T)\) \\
**This work** & Frequentist & \(}(d^{3/2})\) & \((K T)\) \\   

Table 1: Comparison of regret bounds for linear ensemble sampling _Take \(=_{t=1}^{T}(_{1,t}_{2,t})\) and any \((0,1]\). Then, under the event \(\) and an additional event whose probability is at least \(1-\), the cumulative regret is bounded as follows:_

\[R(T)(1+))}+}\,.\] (4)

Discussion of Theorem 2.The audience well-versed in the regret analysis of randomized algorithms such as TS and PHE would recognize that bounding the regret using the probability of being optimistic is a standard procedure, also presented in Theorem 1 of Kveton et al.  and Theorem 2 of Janz et al.  as generalizations of the results in Agrawal and Goyal  and Abeille and Lazaric  respectively. However, our regret analysis offers much more concise approach than the existing techniques, which can be of independent interest beyond the analysis of ensemble sampling.

Theorem 2 states that if the two conditions, specifically _concentration_ in (2) and _optimism_ in (3), are met, then the algorithm achieves \(\) regret. We provide the proof of Theorem 2 in Appendix B. Our proof technique generalizes the well-studied analysis of Abeille and Lazaric . While their work poses conditions on a \(d\)-dimensional perturbation vector that is added to the ridge estimator, we do not assume the use of ridge regression nor we assume that the estimator is perturbed. Instead, we only pose conditions on the final estimator the algorithm exploits. Due to this generalization, Theorem 2 is even capable of inducing the regret bound of LinUCB , which always opts for an optimistic estimator, by setting \(=2_{T}\) and \(p=1\) with appropriate concentration events assigned to \(_{1,t}\) and \(_{2,t}\). In addition, there are several improvements that simplify the proof which are worth noting. To exploit the optimism condition, we apply Markov's inequality on a well-defined random variable. The proof of Abeille and Lazaric  relies on defining a conditional distribution, conditioned on both history and the event of being optimistic. However, such distribution may not be well-defined if the probability of the event is \(0\) for given history. Janz et al.  try to solve this problem by separately handling such exceptional cases using conditional measures. However, their conditional measures depend on the random history, leading the probability \(p\) to be a random variable, which complicates the analysis. We also note that our proof does not require convex analysis studied in Abeille and Lazaric .

**Remark 4** (Role of event \(_{2,t}\)).: Previous results that utilize the probability of being optimistic [4; 2; 12; 10] do not explicitly define events \(\{_{2,t}\}_{t}\). However, their existence is crucial in our analysis of linear ensemble sampling. Since the perturbation sequences are also part of the history in ensemble sampling, the probability of \(_{t}\) being optimistic may be extremely small under some events in \(_{t-1}\) that sample unfavorable sequences. The role of \(_{2,t}\) is to confine our analysis to the case where such undesirable events do not occur.

### Proof of Regret Bound in Theorem 1

We prove the regret bound of linear ensemble sampling stated in Theorem 1. To apply Theorem 2, high-probabilities of the sequences of events, namely \(\{_{1,t},_{2,t}\}_{t=1}^{T}\), should be guaranteed with an appropriate values of \(\) and \(p\). We show that separate constraints can be imposed on the randomness of the rewards and the perturbations respectively to guarantee the probabilities of the events. We begin by decomposing the estimator into two parts: one that fits the observed rewards and the other that perturbs the estimator.

\[_{t}^{j} =V_{t}^{-1}S_{t}^{j}=V_{t}^{-1}(W^{j}+_{i=1}^{t}X_{i} (Y_{i}+Z_{i}^{j}))=V_{t}^{-1}_{i=1}^{t}X_{i}Y_{i}+V_{t}^{- 1}(W^{j}+_{i=1}^{t}X_{i}Z_{i}^{j})\] (5) \[=:_{t}+_{t}^{j}\,,\]

where we define \(_{t}:=V_{t}^{-1}_{i=1}^{t}X_{i}Y_{i}\) and \(_{t}^{j}:=V_{t}^{-1}(W^{j}+_{i=1}^{t}X_{i}Z_{i}^{j})\). \(_{t}\) is the ridge regression estimator of the observed data, and its randomness mainly comes from the noise of the rewards, \(\{_{i}\}_{i=1}^{t}\). \(_{t}^{j}\) is the perturbation added to \(_{t}\), and its randomness comes from the generated perturbation, \(W^{j}\) and \(\{Z_{i}^{j}\}_{i=1}^{t}\). The following lemma states the well-known concentration result for the ridge estimator.

**Lemma 1** (Theorem 2 of Abbasi-Yadkori et al. ).: _Fix \((0,1]\). For \(t\{0\}\), define a sequence of events with \(_{t}()=)+2 }+S\) as_

\[}_{t}:=\{:\|_{t}-^{ *}\|_{V_{t}}_{t}()\}\]_and their intersection \(}:=_{t=0}^{}}_{t}\). Then, \(} 1-\)._

Now, we address \(_{t}^{j}\). Define a _perturbation vector_ that represents the perturbation sequence for each model as follows:

\[_{t}^{j}:=(}W^{j} Z_{1}^{j}  Z_{t-1}^{j})^{}^{d+t-1}, j [m]\,.\] (6)

Let \(_{t}:=_{t}^{j_{t}}\) so that \(_{t}\) is the perturbation vector of the model chosen at time \(t\). The following lemma demonstrates that optimism condition (3) can be satisfied by an anti-concentration property of \(_{t}\) alone.

**Lemma 2** (Sufficient condition for optimism).: _For \(t[T]\), define a vector \(U_{t-1}^{d+t-1}\) by_

\[U_{t-1}^{}:=x^{*}V_{t-1}^{-1}(I_{d} X_{1}  X_{t-1})\,.\]

_Then, \(x^{*}^{*} X_{t}^{}_{t}\) holds whenever there exists a constant \(c>0\) such that \(U_{t-1}^{}_{t} c\|U_{t-1}\|_{2}\) and \(\|^{*}-_{t-1}\|_{V_{t-1}} c\) hold._

We present a straightforward proof of Lemma 2 in Appendix C. We significantly deviate from the analyses of Abeille and Lazaric  and Janz et al.  in the method of guaranteeing the optimism condition. Their analyses require a \(d\)-dimensional perturbation vector to have a constant probability of having positive component, so-called _anti-concentrated_, in "every" possible direction in \(^{d}\) since they only prove the existence of a direction that implies optimism. We observe and exploit the fact that it suffices to consider just "one" direction, specifically \(U_{t-1}\), to produce an optimistic estimator. Since \(U_{t-1}\) depends only on the sequence of selected arms, the dependency between \(U_{t-1}\) and \(_{t}\) decouples when the dependency between \(\{X_{t}\}_{t=1}^{T}\) and \(\{_{t}\}_{t=1}^{T}\) are decoupled, which we later achieve by taking the union bound in a unique way.

The following lemma shows that concentration and anti-concentration properties of the perturbation are sufficient conditions for Theorem 2. We provide a sketch of its proof and defer the remaining details to Appendix D.

**Lemma 3**.: _Suppose the agent runs Algorithm 1 with some parameters. Fix \(>0\) and \(p(0,1]\). For each \(t[T]\), define two events_

\[}_{1,t} :=\{:\|_{t-1}^{j_{t}} \|_{V_{t-1}}\},\] \[}_{2,t} :=\{:(U_{t-1}^{} _{t}_{t-1}\|U_{t-1}\|_{2}}_{1,t} _{t-1}) p\}\,,\]

_where \(U_{t-1}\) is defined as in Lemma 2. Take \(_{1,t}=}_{1,t}}_{t-1}\) and \(_{2,t}=}_{2,t}}_{t-1}\). Then, \(_{1,t}\) and \(_{2,t}\) satisfy concentration condition (2) with \(=+_{T}\) and optimism condition (3) with the same value of \(p\). Consequently, with probability at least \(1-2-(}^{})\), where \(}:=_{t=1}^{T}(}_{1,t} }_{2,t})\), Algorithm 1 achieves regret bound (4) of Theorem 2._

**Remark 5**.: Lemma 3 applies to any perturbation-based algorithm that exploits \(_{t}=_{t-1}+_{t-1}\), where \(_{t-1}\) is a linear transform of a random perturbation vector \(_{t}\). A version of linear Thompson sampling  and LinPHE also fall into this category.

Lemma 3 shifts the problem of constructing the regret bound of Algorithm 1 to lower-bounding the probabilities of the two sequences of events, \(\{}_{1,t}\}_{t=1}^{T}\) and \(\{}_{2,t}\}_{t=1}^{T}\). Note that \(}_{1,t}\) and \(}_{2,t}\) regard \(\{X_{i}\}_{i=1}^{t-1}\) and \(_{t}\) only, and are independent of further randomness of \(\{_{t}\}_{i=1}^{T}\).

Sketch of Proof of Lemma 3.: The concentration condition follows immediately by the triangle inequality. To show the optimism condition, we verify the following logical implication relationship:

\[((U_{t-1}^{}_{t}_{t-1}\|U_{t-1}\|_ {2})}_{1,t})_{2,t}^{ }((x^{*}^{*} X_{t}^{}_{t })_{1,t})_{2,t}^{}\,,\]

where Lemma 2 bridges the anti-concentration on the left hand side to the optimism on the right hand side. This implication relationship is converted to the following probability inequality:

\[(((x^{*}^{*} X_{t}^{}_{t}) _{1,t})_{2,t}^{}_{t-1 })(((U_{t-1}^{}_{t}_ {t-1}\|U_{t-1}\|_{2})}_{1,t} )_{2,t}^{}_{t-1})\,.\]

By the definition of \(}_{2,t}\), the right hand side is bounded below by \(p\), implying optimism condition (3).

The probability of failure is bounded by the union bound and Lemma 1.

Theorem 1 employs the Gaussian perturbation for a concrete instantiation of Algorithm 1. We define two values \(_{T}\) and \(_{T}\), which serve as the confidence radii of \(_{t}\) and \(_{t}\) for \(t[T]\) under the Gaussian perturbation.

\[_{T}:=_{T}( )+2}++}), _{T}:=_{T}+_{T}\,.\] (7)

Note that in terms of \(d\) and \(T\), both \(_{T}\) and \(_{T}\) are in \((d T)\). Lemma 4 illustrates the concentration result. Its proof is a simple application of Lemma 1, and is presented in Appendix E.

**Lemma 4**.: _Suppose Algorithm 1 is run with parameters specified in Theorem 1. Fix \(t[T]\) and \(j[m]\). Suppose the sequence of arms \(X_{1},,X_{t}\) is chosen arbitrarily randomly, not necessarily by the the agent. Let \(_{t}^{j}\) be defined as in Eq. (5). Then, with probability at least \(1-/T\), \(\|_{t}^{j}\|_{V_{t}}_{T}\) holds._

The following fact describes an anti-concentration property of Gaussian distribution, which follows from the fact that a linear combination of independent Gaussians is again Gaussian.

**Fact 1**.: _If \(Z(0,^{2}I_{n})\) for some \( 0\) and \(u^{n}\) is a fixed vector for some \(n\), then \((u^{}Z\|u\|_{2}) (z 1)=:p_{N}\), where \(z(0,1)\). We note that \(p_{N} 0.15\)._

All the building blocks we need to prove Theorem 1 is ready. The proof illustrates that the events specified in Lemma 3 occur with high probability.

Proof of Theorem 1.: Define \(}_{1,t}\) and \(}_{2,t}\) as in Lemma 3 with \(=_{T}\) and \(p=p_{N}/4\), where \(_{T}\) is defined in Eq. (7) and \(p_{N}\) is defined in Fact 1. We show that these events occur with high probability, and the rest follows from Lemma 3. For the sake of the analysis, assume that \(/T p_{N}/2 0.08\), which holds whenever \(T 14\) or \(<0.07\).

Assume that the perturbation values \(W^{j}\) and \(Z_{t}^{j}\) are \(_{0}^{A}\)-measurable for all \(j[m]\) and \(t[T]\). An interpretation of this assumption is that the algorithm samples all the required values in advance. Note that we still obtain an equivalent algorithm and this modification need not actually take place in the execution. Under this assumption, the uniform sampling of \(j_{t}_{t}\) is the only source of randomness regarding the choice of \(_{t}\) and \(X_{t}\) when conditioned on the history \(_{t-1}\). It may seem unintuitive, but this modification simplifies the proof because we only need to deal with \(j_{t}\).

We first lower-bound the probability of \(}_{1,t}\). When \(j[m]\) is fixed, we can apply Lemma 4 and obtain \((\|_{t-1}^{j}\|_{V_{t-1}}_{ T}) 1-/T\). Since \(j_{t}\) is sampled independently of \(_{t-1}^{j}\), it holds that

\[(}_{1,t})=_{j=1}^{m} (j_{t}=j,\|_{t-1}^{j}\|_{V_{t-1}} _{T})=_{j=1}^{m}(\| _{t-1}^{j}\|_{V_{t-1}}_{T} ) 1-\,.\] (8)

Now, we bound the probability of \(}_{2,t}\). Fix \(j[m]\). Recall that \(_{t}^{j}\) is the perturbation vector of the \(j\)-th model, defined in Eq. (6). The choice of Gaussian perturbation implies that \(_{t}^{j}(_{d+t-1},_{T}^{2}I_{d+t-1})\). Suppose that the sequence of arms \(X_{1},,X_{T}\) is fixed. Then, we can apply Fact 1, obtaining that \((U_{t-1}^{}_{t}^{j}_{t-1}\|U_{t-1}\|_{2}) p _{N}\), where the probability is measured over the randomness of the perturbation sequence \(_{t}^{j}\). Let \(I_{t}^{j}:=\{(U_{t-1}^{}_{t}^{j}_{t-1}\|U_{t -1}\|_{2})(\|_{t-1}^{j}\|_{V_{t-1}} _{T})\}\). Then, we have that

\[I_{t}^{j}=1U_{t-1}^{}_{t}^{j}_{t-1}\|U_{t-1}\|_{2}- _{t-1}^{j}_{V_{t-1}}>_{T}  p_{N}-/T p_{N}/2\,,\] (9)

where the first inequality uses that \((A B)(A)-(B^{C})\) holds for any events \(A\) and \(B\), and the last inequality holds by the assumption \(/T p_{N}/2\). However, as we assumed that the perturbation sequence is \(_{0}^{A}\)-measurable, it is \(_{t-1}\)-measurable, hence \(I_{t}^{j}\) is also \(_{t-1}\)-measurable. It means that the value of \(I_{t}^{j}\) is determined when the history up to time \(t-1\) is fixed. The only remaining source of randomness in choosing \(_{t}\) conditioned on \(_{t-1}\) is the sampling of \(j_{t}_{t}\). Therefore, it holds that

\[((U_{t-1}^{}_{t}^{j_{t}}_{t-1}\|U_{t -1}\|_{2})(\|_{t-1}^{j_{t}}\|_{V_{ t-1}}_{T})_{t-1})=_{j=1}^{m}I_{t}^{j}\,.\] (10)Since we have verified that the expectation of the right hand side is greater than \(p_{N}/2\), Azuma-Hoeffding inequality (Lemma 12) implies that \((_{j=1}^{m}I_{t}^{j}<p_{N}/4)(-p_{N}^{2}m/ 8)\). Recall that this result is obtained assuming that \(X_{1},,X_{T}\) are fixed. We take the union bound over all possible sequences of arms. However, a naive union bound multiplies \(K^{T}\) to the failure probability, which leads to an undesirable result of \(m\) scaling linearly with \(T\). We present the following proposition inspired by an observation from Lu and Van Roy  that a permutation of selected arms can be regarded as equivalent. We note that the strong result of Lemma 6 in Lu and Van Roy  is not applicable to our setting since we do not assume that \(\{_{t}\}_{t=1}^{T}\) is distributed identically nor independently. Although we present all the main ideas to support Proposition 1 in this section, there may be a few points that readers find require further justification. Due to limited space, we provide a full, rigorous justification of Proposition 1 in Appendix F, where we present a different perspective on the sampling of perturbation.

**Proposition 1**.: _There exists an event \(_{2}^{*}\) such that under \(_{2}^{*}\), \(_{j=1}^{m}I_{t}^{j} p_{N}/4\) holds for \(t=1,,T\) and \((_{2}^{*}) T^{K}(-p_{N}^{2}m/8)\)._

The key observation in Proposition 1 is that the perturbation vector consists of i.i.d. components, hence its distribution is invariant under independent permutations. Therefore, the distributions of \(_{t-1}^{j}\) and \(U_{t-1}^{}_{t}^{j}\) remain invariant under the permutation of selected arms. Although the sequence of arms and the perturbation vector are not independent as a whole, the permutation that sorts the selected arms preserves the distribution of \(_{t}\) since \(X_{t}\) and \(Z_{t}\) are independent for all \(t[T]\). The number of equivalence classes up to permutation over sequences of arms with lengths at most \(T-1\) is less than \(T^{K}\), since each arm can be selected \(0\) to \(T-1\) times inclusively. Therefore, we take the union bound over the \(T^{K}\) sequences of arms and attain \(_{2}^{*}\).

Taking \(m=^{2}}(K T+)\), we obtain that \((_{2}^{*})\). Eq. (10) implies that \((_{t=1}^{T}}_{2,t})( _{2}^{*}) 1-\). Therefore, by Lemma 3, with probability at least \(1-4\), the cumulative regret is bounded as follows:

\[R(T)_{T}(1+}))}+}{p_{N}} }=O((d T)^{})\,.\]

## 6 Ensemble Sampling and Perturbed-History Exploration

In this section, we rigorously investigate the relationship between linear ensemble sampling and LinPHE. A generalized version of LinPHE is described in Algorithm 2. Note that the perturbed estimator, \(_{t}=V_{t-1}^{-1}W_{t}+_{i=1}^{t-1}X_{i}(Y_{i}+Z_{t,i})\), resembles the estimator of linear ensemble sampling, which becomes evident when compared with Eq. (5). The main difference is that in LinPHE (Algorithm 2), the perturbation sequence is generated independently of the history at every time step, whereas in linear ensemble sampling (Algorithm 1), the sequence is not renewed but is incremented at each time step. However, we further observe that in linear ensemble sampling, as long as an estimator is not sampled for the arm selection, its perturbation sequence is independent of the selected arms and rewards. This implies that the estimator in the ensemble that is selected for the first time is equivalent to the estimator computed by the policy of LinPHE. Specifically, if the \(j\)-th estimator is selected for the first time at time step \(t\), then the perturbation values of the estimator, \(W^{j}\) and \(\{Z^{j}_{i}\}_{i=1}^{t-1}\), have had no effect on previous interactions. Therefore, newly sampling them as \(W^{j}_{t}_{I}\) and \(\{Z^{j}_{i,j}\}_{i=1}^{t-1}_{R}\), as in LinPHE, does not alter future interactions. We conclude that in the case where the ensemble size is greater than or equal to \(T\), linear ensemble sampling becomes equivalent to LinPHE by selecting the estimators in a round robin.

**Proposition 2**.: _Linear ensemble sampling (Algorithm 1) with \(m=T\) and deterministic policy of choosing a model, e.g., \(_{t} t\) for \(t=1,,T\), is equivalent to LinPHE (Algorithm 2)._

Proposition 2 shows that LinPHE is a special case of linear ensemble sampling and provides insightful consequences in both directions of the equivalence. To our best knowledge, Proposition 2 is the first result to formally demonstrate the relationship between linear ensemble sampling and LinPHE.

**Linear ensemble sampling with \(T\) models is LinPHE:** Since an ensemble of \(T\) models is equivalent to LinPHE which achieves a regret bound \(}(d)\), the ensemble size larger than \(T\) is not necessary. This implication certainly emphasizes the sub-optimal requirements of the ensemble size in Lu and Van Roy , Qin et al. . Even when \(K>T\) in our problem setting, this equivalence provides the ground for upper bounding the ensemble size by \(T\).

**LinPHE is linear ensemble sampling with \(T\) models:** Conversely, since LinPHE can be regarded as linear ensemble sampling with \(T\) models, it is possible to derive a regret bound of LinPHE by following the proof of Theorem 1. We present Corollary 1, which states a new regret bound of \(}(d^{3/2})\) for LinPHE. Note that in this case, the regret bound is independent of \(K\).

**Corollary 1** (Regret bound of LinPHE).: _Fix \((0,1]\). Algorithm 2 with \( 1\), \(_{I}=(_{d},_{T}^{2}I_{d})\) and \(_{R}=(0,_{T}^{2})\) achieves \(((d T)^{3/2})\) cumulative regret with probability at least \(1-3\)._

Discussion of Corollary 1.Kveton et al.  provide a \(}(d)\) regret bound when the number of arms is finite. Our result is the first to prove that LinPHE achieves a \(}(d^{3/2})\) regret bound that is independent of the number of arms. Hence, we view our overall results as a generalization of Kveton et al. . It is widely observed that assuming the size of the arm set to be \(K\) may lead to an interchanging of a \(\) factor with a \(\) factor in the regret bound , although attaining such reduction may not always be done in a trivial manner . Our focus is not merely on proving another regret bound for LinPHE, but rather on highlighting the close relationship between linear ensemble sampling and LinPHE, which, to our knowledge, has been overlooked in the literature.

The proof of Corollary 1 follows the proof of Theorem 1. Note that the latter part of the proof of Theorem 1 focuses on decoupling the dependency between \(\{X_{t}\}_{t=1}^{T}\) and \(_{t}\). However, in the case of LinPHE, they are already independent since the perturbation sequence is freshly sampled at every time step, enabling a more elegant and concise proof. Especially, as it skips the parts that require the number of arms to be finite, for instance the use of Proposition 1, Corollary 1 holds even when the number of arms is infinite. The whole proof is presented in Appendix G.

## 7 Conclusion

We prove that linear ensemble sampling achieves a \(}(d^{3/2})\) regret bound, marking the first such result in the frequentist setting and matching the best-known regret bound for randomized algorithms. The required ensemble size scales logarithmically with the time horizon as \((K T)\). Additionally, we expand our analysis to LinPHE, demonstrating that it is a special case of linear ensemble sampling with an ensemble of \(T\) models, achieving the same regret bound of \(}(d^{3/2})\). While our work focuses on linear bandits, ensemble sampling applications have shown superior performance in more complex settings. This suggests that theoretical extensions beyond the linear setting are worth pursuing, with our results providing an important foundation for possibly understanding these extensions. Extending the results to general contextual settings, where the arm set may change over time with potentially non-linear reward functions, represents a promising direction for future work.