# Principled Probabilistic Imaging using Diffusion

Models as Plug-and-Play Priors

 Zihui Wu\({}^{1}\) Yu Sun\({}^{4}\) Yifan Chen\({}^{5}\) Bingliang Zhang\({}^{1}\) Yisong Yue\({}^{1}\) Katherine L. Bouman\({}^{1,2,3}\)

\({}^{1}\)Department of Computing and Mathematical Sciences, Caltech

\({}^{2}\)Department of Electrical Engineering, Caltech

\({}^{3}\)Department of Astronomy, Caltech

\({}^{4}\)Department of Electrical and Computer Engineering, Johns Hopkins University

\({}^{5}\)Courant Institute of Mathematical Sciences, New York University

###### Abstract

Diffusion models (DMs) have recently shown outstanding capabilities in modeling complex image distributions, making them expressive image priors for solving Bayesian inverse problems. However, most existing DM-based methods rely on approximations in the generative process to be generic to different inverse problems, leading to inaccurate sample distributions that deviate from the target posterior defined within the Bayesian framework. To harness the generative power of DMs while avoiding such approximations, we propose a Markov chain Monte Carlo algorithm that performs posterior sampling for general inverse problems by reducing it to sampling the posterior of a Gaussian denoising problem. Crucially, we leverage a general DM formulation as a unified interface that allows for rigorously solving the denoising problem with a range of state-of-the-art DMs. We demonstrate the effectiveness of the proposed method on six inverse problems (three linear and three nonlinear), including a real-world black hole imaging problem. Experimental results indicate that our proposed method offers more accurate reconstructions and posterior estimation compared to existing DM-based imaging inverse methods.

## 1 Introduction

Inverse problems arise in many computational imaging applications, where the goal is to recover an image \(^{n}\) from a set of sparse and noisy measurements \(^{m}\). The relationship between \(\) and \(\) can be described by

\[=()+,\] (1)

where \(():^{n}^{m}\) is the forward operator (linear or nonlinear) and \(\) is the random measurement noise in \(^{m}\). Since the sparsity and noisiness of \(\) often lead to significant uncertainty in \(\), it is preferable to sample the posterior distribution \(p(|)\) over all possible solutions based on some prior distribution \(p()\), rather than finding a single deterministic solution. Traditional posterior sampling methods often rely on simple image priors that do not reflect the sophistication of real-world image distributions. On the other hand, diffusion models (DMs) have recently emerged as a powerful tool for modeling highly complex image distributions . Nevertheless, it remains a challenge to turn DMs into reliable imaging inverse solvers, which motivates us to develop a principled Bayesian method that leverages DMs as priors for posterior sampling.

Diffusion models generate samples from a distribution by reversing a diffusion process from the target distribution to a simple (usually Gaussian) distribution . In particular, it estimates a clean image \(_{0}\) from a noise image \(_{T}\) by successively denoising noisy images, where \(_{t} p_{t}\) is the intermediate noisy image at time \(t[0,T]\). Reversing diffusion requires one to estimate the time-varying gradient log density (score function) \( p_{t}(_{t})\) along the diffusion process, or \( p_{t}(_{t}|)\) in the case of sampling the posterior \(p(|)\).

To design generic DM-based inverse problem solvers, most existing methods attempt to approximate the time-varying gradient log density \( p_{t}(_{t}|)\)[17; 73; 84; 62; 39; 58; 60; 44; 15; 77; 18; 55; 8]. In particular they first apply Bayes' rule to separate the forward operator from an unconditional prior over the intermediate noisy image \(_{t}\):

\[ p_{t}(_{t}|)= p_{t}(|_{t})+  p_{t}(_{t}).\] (2)

By instead aiming to evaluate the right hand side, one can leverage the existing pre-trained DMs for the unconditional term \( p_{t}(_{t})\). However, the main challenge in this case is that \( p_{t}(|_{t})\) is intractable to compute in general, as \(p_{t}(|_{t})\) involves an integral over all possible \(_{0}\)'s that could give rise to \(_{t}\). Various methods have been proposed to circumvent the intractability and can mostly be categorized into two groups. One group of methods explicitly approximate \( p_{t}(|_{t})\) by making simplifying assumptions [62; 17; 60; 8]. However, even for arguably the finest approximation to date proposed in the recent work , it is exact only when the prior distribution \(p()\) is Gaussian. For general prior distributions beyond Gaussian, these methods do not sample the true posterior \(p(|)\). The other group of methods do not make explicit approximations but instead substitute \( p_{t}(|_{t})\) with empirically designed updates where \(\) is treated as a guidance signal [73; 84; 39; 58; 44; 15; 77; 18; 55]. Although these methods may have strong empirical performance, they have deviated from the Bayesian formulation and no longer aim to sample the target posterior. In summary, these existing DM-based inverse methods should be best viewed as _guidance methods_, where the generative process is _guided_ towards the regions where the measurement \(\) is more likely to be observed, not as posterior sampling methods . We also note that some recent work considered combing DMs with Sequential Monte Carlo to ensure asymptotic consistency in posterior sampling [11; 23], but the investigation has been limited to linear imaging inverse problems.

Our contributionsIn this work, we pursue a different path towards posterior sampling with DM priors by proposing a new Markov chain Monte Carlo (MCMC) algorithm, which we call _Plug-and-Play Diffusion Models_ (PnP-DM). It incorporates DMs in a principled way and circumvents the approximation required when taking the approach in (2). The proposed algorithm is based on the Split Gibbs Sampler  that alternates between two sampling steps that separately involve the likelihood and prior. While the likelihood step can be tackled with traditional sampling techniques, the prior step involves a Bayesian denoising problem that requires careful design. Importantly, we identify a connection between the Bayesian denoising problem and the unconditional image generation problem under a general formulation of DMs presented in  (which is referred to as the EDM formulation hereafter). This connection allows us to perform rigorous posterior sampling for denoising using DMs without approximating the generative process and enables the use of a wide range of pretrained DMs through the unified EDM formulation. We present an analysis on the non-asymptotic behavior of PnP-DM by establishing a stationarity guarantee in terms of the average Fisher information. We further demonstrate the strong empirical performance of PnP-DM by investigating three linear and three nonlinear noisy inverse problems, including a black hole interferometric imaging problem involving real data that is both nonlinear and severely ill-posed (see Figure 1). Overall, PnP-DM outperforms existing baseline methods, achieving higher accuracy in posterior estimation.

Figure 1: Demonstration of the proposed method, PnP-DM, for posterior sampling using the real data for the M87 black hole from April 6\({}^{}\), 2017 . The black hole imaging problem is non-convex and highly ill-posed due to severe noise corruption and measurement sparsity. Our method rigorously integrates measurements from a real-world imaging system with an expressive image prior in the form of a diffusion model, which was trained with images from the GRMHD black hole simulation  in this case. Besides having high visual quality, our posterior samples accurately capture key features of the M87 black hole such as the bright spot location and ring diameter.

Preliminaries

_Split Gibbs Sampler (SGS)_ is an MCMC approach developed for Bayesian inference . It is also related to the _Proximal Sampler_[42; 14; 25; 79] and serves as the backbone for the _Generative Plug-and-Play (GPnP)_ and _Diffusion Plug-and-Play (DPnP)_ frameworks in computational imaging. The goal of SGS is to sample the posterior distribution

\[p(|) p(|)p()=(-f(;)-g( ))\] (3)

where \(f(;):=- p(|)\) and \(g():=- p()\) are the potential functions of the likelihood and prior distribution, respectively. The dual dependence of (3) on both the likelihood and prior makes it nontrivial to directly sample from it in general. Instead, SGS leverages the composite structure of the posterior distribution by adopting a variable-splitting strategy and considers sampling an alternative distribution

\[(,)(-f(;)-g()-}\|-\|_{2}^{2})\] (4)

where \(^{n}\) is an augmented variable and \(>0\) is a hyperparameter that controls the strength of the coupling between \(\) and \(\). We denote the \(\)- and \(\)-marginal distributions of (4) as \(^{X}():=(,)\) and \(^{Z}():=(,)\), respectively. As \( 0\), \(^{X}\) converges to the target posterior \(p(|)\) in terms of total variation distance , so one can obtain approximate samples from the target posterior by sampling (4) instead.

SGS samples (4) via Gibbs sampling. Specifically, SGS starts from an initialization \(^{(0)}\) and, for iteration \(k=0,,K-1\), alternates between

1. **Likelihood step:** sample \(^{(k)}^{Z|X=^{(k)}}()(-f(;)-}\|^{(k)}-\|_{2}^{2})\)
2. **Prior step:** sample \(^{(k+1)}^{X|Z=^{(k)}}()(-g()- }\|-^{(k)}\|_{2}^{2})\).

Note that the two conditional distributions separately involve \(f(;)\) and \(g()\). The likelihood and prior are decoupled so that these two steps can be designed in a modular way. A similar variable-splitting strategy is also adopted in optimization methods such as the Half-Quadratic Splitting (HQS) method  and the Alternating Direction Method of Multipliers (ADMM) [30; 7]. In fact, SGS can be viewed as a sampling analogue of HQS. SGS is a principled approach to posterior sampling if the two sampling steps are rigorously implemented.

Existing works related to SGSeveral works have designed algorithms for solving imaging inverse problems based on SGS [53; 19; 6; 27; 78]. The key distinction among these methods lies in their approaches to the prior step. For instance, the works [53; 6; 27] applied Langevin-based updates for sampling \(^{X|Z=}\) such that the prior information is encoded by either traditional regularizers or off-the-shelf image denoisers. The work  tackled the prior step by heuristically customizing a diffusion model (i.e. DDPM ) for sampling \(^{X|Z=}\). A concurrent work  improved the implementation by devising two diffusion processes that rigorously solve the prior step. Our method differs from  by connecting the prior step to the EDM formulation . This connection allows us to seamlessly integrate state-of-the-art DMs as expressive image priors for Bayesian inference through a unified interface, eliminating the need for additional customization for each model and leading to better empirical performance. We also note the recent work  that adopted the optimization-based variable-splitting formulation of HQS and utilized general DMs as image priors. We instead considers the SGS formulation from a Bayesian posterior sampling standpoint. Additionally, while SGS-based methods theoretically accommodate general inverse problems, empirical evidence on real-world nonlinear inverse problems remains scarce in the literature. In this work, we demonstrate our method on three nonlinear inverse problems, including a black hole imaging problem. For a more comprehensive review of related works, see Appendix E.

## 3 Method

A schematic diagram for the proposed method is shown in Figure 2. Our method, dubbed PnP-DM, builds upon the SGS framework with rigorous implementations of the two sampling steps and an annealing schedule for the coupling parameter \(\). We start with our implementations of the first step for solving both linear and nonlinear inverse problems.

### Likelihood step: enforcing data consistency

For the likelihood step at iteration \(k\), we sample

\[^{(k)}^{Z|X=^{(k)}}()(-f(;) -}\|^{(k)}-\|_{2}^{2}).\] (5)

Linear forward model and Gaussian noiseWe first consider a simple yet common case where the forward model \(\) is linear and the noise distribution is zero-mean Gaussian, i.e. \(:=^{m n}\) and \((,)\). In this case, the potential function of the likelihood term is \(f(;)=\|-\|_{}^{2}\) (up to an additive constant that does not depend on \(\) and \(\)) where \(\|\|_{}^{2}:=,^{-1}\). It is then straightforward to show that

\[^{Z|X=}=((),^{-1})\]

where \(:=^{T}^{-1}+}\) and \(():=^{-1}(^{T}^{-1}+})\). The problem of sampling from Gaussian distributions has been systematically studied . We refer readers to Appendix C.1 for a more detailed discussion.

General caseFor general nonlinear inverse problems, the likelihood step is not sampling from a Gaussian distribution anymore. Nevertheless, since we have access to \(^{Z|X=}\) in closed form up to a multiplicative factor, we can use Monte Carlo methods based on Langevin dynamics to draw samples from it as long as the likelihood potential is differentiable. Specifically, we first set up the following Langevin SDE that admits \(^{Z|X=}\) as the stationary distribution

\[_{t}=^{Z|X=}(_{t})t+_{t}=[- f(;)-}(-)]t+_{t}.\]

We then initialize the SDE at \(_{0}=\) and run it with Euler discretization. The pseudocode is provided in Appendix C.1.

### Prior step: denoising via the EDM framework

For the prior step at iteration \(k\), we sample

\[^{(k+1)}^{X|Z=^{(k)}}()(-g()- }\|-^{(k)}\|_{2}^{2}).\] (6)

A closer examination of (6) reveals that this prior step is essentially to draw posterior samples for a Gaussian denoising problem, where the "measurement" is \(^{(k)}\), the noise level is \(\), and the prior distribution is \(p()(-g())\).

We tackle this denoising posterior sampling problem within SGS using DMs as image priors. In particular, we leverage the EDM framework , which was originally proposed to unify various

Figure 2: A schematic diagram of our method. Our method alternates between a likelihood step that enforces data consistency and a prior step that solves a denoising posterior sampling problem by leveraging the Split Gibbs Sampler . An annealing schedule controls the strength of the two steps at each iteration to facilitate efficient and accurate sampling. A crucial part of our design is the prior step, where we identify a key connection to a general diffusion model framework called the EDM . This connection allows us to easily incorporate a family of state-of-the-art diffusion models as priors to conduct posterior sampling in a principled way without additional training. Our method demonstrates strong performance on a variety of linear and nonlinear inverse problems.

formulations of DMs for unconditional image generation. To see the connection of the EDM framework to (6), consider a family of mollified distributions \(p(;)\) given by adding i.i.d Gaussian noise of standard deviation \(\) to the prior distribution \(p()\), i.e. \(+ p(;)\). The core idea of the EDM framework is that a variety of state-of-the-art DMs can be unified into the following reverse SDE:

\[_{t}=[(t)}{s(t)}_{t}-2s(t)^{2}(t)(t) p(_{t}}{s(t)};(t)) ]t+s(t)(t)(t)}}_ {t}\] (7)

where \(}_{t}\) is an \(n\)-dimensional Wiener process running backward in time, \((t)>0\) is a pre-defined noise level schedule with \((0)=0\), \(s(t)\) is a pre-defined scaling schedule, and \((t)\), \((t)\) are their time derivatives. As shown in , the defining property of (7) is that \(_{t}/s(t) p(;(t))\) for any time \(t\). Therefore, solving this SDE backward in time allows us to travel from any noise level \((t)\) to the clean image distribution at \(t=0\). This means that we can use (7) to solve (6) with arbitrary noise level \(\) as long as \(\) is within the range of \((t)\). Indeed, the distribution of \(_{0}\) conditioned on \(_{t}\) is

\[p(_{0}|_{t})  p(_{t}|_{0})p(_{0})(s(t) _{0},s(t)^{2}(t)^{2})(-g(_{0}))\] \[(-g(_{0})-}\|_ {0}-_{t}/s(t)\|_{2}^{2}).\]

We highlight that the last line exactly matches (6) when \(_{t}=s(t)^{(k)}\) and \((t)=\). Therefore, we can naturally design a practical algorithm that samples (6) by following these three steps: (1) find \(t^{*}\) such that \((t^{*})=\), (2) initialize at \(_{t^{*}}=s(t^{*})^{(k)}\), and (3) solve (7) backward from \(t^{*}\) to 0 by choosing the discretization time steps and integration scheme. Through this unified interface, any DMs, once converted to the EDM formulation, can be directly turned into a rigorous solver for (6).

Leveraging the connection with EDM, our prior step implementation comes with a large design space that encompasses a variety of existing DMs, such as DDPM (or VP-SDE) , VE-SDE , and iDDPM . In our experiments, we conduct posterior sampling with all these different models within our framework and all of them provide high-quality samples. The pseudocode of our implementation and more details on the EDM formulation for the prior step is given in Appendix C.2.

### Putting it all together

The pseudocode of PnP-DM in complete form is presented in Algorithm 1. PnP-DM alternates between the two sampling steps with an annealing schedule \(\{_{k}\}\) for the coupling parameter. We find that the annealing schedule on \(\) accelerates the mixing time of the Markov chain and prevents the algorithm from getting stuck in bad local minima for solving highly ill-posed inverse problems. This is a common practice in both Langevin-based [40; 34; 65] and SGS-based [6; 78] MCMC algorithms to improve the empirical performance in solving inverse problems.

Our work shares some similarities with PnP-SGS  but contains three main key differences. First, as demonstrated in our experiments, we investigate three nonlinear inverse problems, while nonlinear inverse problems are beyond the scope of . Our experiments show that PnP-SGS struggles with challenging nonlinear inverse problems such as Fourier phase retrieval. Second, we adopt the EDM formulation to ensure that the prior step of PnP-DM is a rigorous mapping from the image manifold with the desired noise level to the clean image manifold, aligning with the theory of SGS. In contrast, the prior step of PnP-SGS  is heuristic (which is also pointed out by ) and not rigorously designed to sample (6). Third, unlike PnP-SGS  that uses a constant \(\), we consider an annealing schedule \(\{_{k}\}\) for the coupling parameter, which is important for highly ill-posed inverse problems.

```
0: initialization \(_{0}^{n}\), total number of iterations \(K>0\), coupling strength schedule \(\{_{k}>0\}_{k=0}^{K-1}\), likelihood potential \(f(\,\,;)\) with measurements \(^{m}\), pretrained model \(D_{}(\,\,;\,\,)\) that approximates \( p(;)\) with \((D_{}(;)-)/^{2}\).
1:for\(k=0,...,K-1\)do
2:\(^{(k)}(^{(k)},_{k},f(\, \,;))\)\(\) Section 3.1
3:\(^{(k+1)}(^{(k)},_{k},D_{}(\, \,;\,\,))\)\(\) Section 3.2
4:endfor
5:return\(^{(k+1)}\) ```

**Algorithm 1** Plug-and-Play Diffusion Models (PnP-DM)

### Theoretical insights

We provide some theoretical insights on the non-asymptotic behavior of PnP-DM. We start with the following definitions. For two probability measures \(\) and \(\) such that \(\), the _Kullback-Leibler (KL) divergence_ and _Fisher information (or Fisher divergence)_ of \(\) with respect to \(\) are defined, respectively, as

\[(||):=} (||):=\| }\|_{2}^{2}.\]

Both divergences are equal to zero if and only if \(=\). KL divergence is a common metric for quantifying the difference of one distribution with respect to another. Fisher information has been used for analyzing the stationarity of sampling algorithms .

We analyze PnP-DM via a continuous-time perspective, leveraging the interpolation techniques introduced for Langevin Monte Carlo . We assume that the likelihood step (5) can be implemented exactly and the prior step (6) involves running the reverse diffusion process (7) with an approximated score function \(_{t} p_{t}:= p(\,\,;(t))\). Let \(_{0}^{X}\) be the distribution of the initialization \(^{(0)}\). Let \(_{k}^{Z}\) and \(_{k+1}^{X}\) be the distributions of \(^{(k)}\) and \(^{(k+1)}\) at the \(k^{}\) iteration. Recall that the stationary distributions are \(^{X}\) and \(^{Z}\). Our analysis is concerned with two _continuous-time_ processes: (1) the non-stationary process from \(_{0}^{X}\), a non-stationary initialization, to \(_{K}^{X}\) where (7) is run with the approximated score function \(_{t}\) and (2) the stationary process that alternates between stationary distributions \(^{X}\) and \(^{Z}\). These two processes are the interpolation PnP-DM in non-stationary and stationary states and define continuous transitions over discrete iterations. A conceptual illustration of the two processes is provided in Figure 3 with the exact formulations in Appendix A. Now we present our main result:

**Theorem 3.1**.: _Consider running \(K\) iterations of PnP-DM with \(_{k}>0\) and a score estimate \(_{t} p_{t}:= p(\,\,;(t))\). Let \(t^{*}>0\) be such that \((t^{*})=\) and \(:=_{t[0,t^{*}]}v(t)\) where \(v(t):=s(t)(t)(t)}\). Define \(_{}\) and \(_{}\) as the distributions at time \(\) of the non-stationary and stationary process, respectively. Then, for over \(K\) iterations of PnP-DM, or equivalently over \([0,T]\) with \(T:=K(t^{*}+1)\), we have_

\[_{0}^{T}(_{}||_{})(^{X}||_{0}^{X})}{K(t^{*}+1)( ,)^{2}}}_{}+}}{(t^{*}+1)^{2}}}_{},\] (8)

_where we assume that the score estimation error \(_{}:=_{1}^{t^{*}+1}v()^{2}_{_{}} \|_{}- p_{}\|_{2}^{2}<\)._

The proof is provided in Appendix A. This theorem states that the average distance (measured by Fisher information) of the non-stationary process with respect to the stationary process over \(K\) iterations of PnP-DM goes to zero at a rate of \(O(1/K)\) under certain conditions up to the score approximation error. Note that our theory only requires \(L^{2}\)-accurate score estimate under the measure \(_{}\), which is a relatively weaker condition than the common \(L^{}\)-accurate score estimate assumption in prior analysis of sampling methods involving score estimates . This result resembles the first-order stationarity for Langevin Monte Carlo . Unlike the non-asymptotic analysis in , we utilize the average Fisher information instead of the total variation distance, enabling us to obtain an explicit convergence rate. Here \(\) is the infimum of the diffusion coefficient along the reverse diffusion in (7); see further discussions on the role of \(\) in Appendix A.3. Our theory shows that the accurate implementations of the two sampling steps lead to a sampler that provably converges to the stationary process that alternates between the two target stationary distributions.

Figure 4: Results on a synthetic problem with the ground truth posterior available. PnP-DM can sample it more accurately that DPS .

Figure 3: A conceptual illustration of the non-stationary and stationary time-continuous processes as interpolations of \(K\) discretize iterations of PnP-DM.

## 4 Experiments

### Validation with ground truth posterior

We first demonstrate the accuracy of PnP-DM for posterior sampling on a simulated compressed sensing problem with a Gaussian prior where the posterior distribution can be expressed in a closed form. The mean and per-pixel standard deviation of the prior are visualized on the bottom left of Figure 4. The linear forward model \(^{m n}\) is a Gaussian matrix (\(m=n/2\)), i.e. \(_{ij}(0,1)\). A test image is randomly generated from the prior (see top left of Figure 4), and the measurement is calculated according to (1) with \((,0.01^{2})\). We compare our method with the popular DM-based method DPS . We draw 1,000 samples and visualize the empirical mean and per-pixel standard deviation for both algorithms. Compared with the true posterior (second column), we find that the both methods accurately estimate the mean. However, the standard deviation image estimated by DPS significantly deviates from the ground truth. In contrast, our standard deviation image matches the ground truth in terms of both absolute magnitude and spatial distribution. These results highlight the accuracy of our method over DPS by taking a more principled Bayesian approach.

### Benchmark experiments

Dataset and inverse problemsWe test our proposed algorithm and several baseline methods on 100 images from the validation set of the FFHQ dataset  for five inverse problems: (1) _Gaussian deblur_ with kernel size 61\(\)61 and standard deviation 3.0, (2) _Motion deblur_ with kernel size 61\(\)61 and intensity of 0.5, (3) _Super-resolution_ with 4\(\) downsampling ratio, (4) the coded diffraction patterns (CDP) reconstruction problem (nonlinear) in [10; 51] (phase retrieval with a phase mask), and (5) the Fourier phase retrieval (nonlinear) with \(4\) oversampling. We add i.i.d. Gaussian noise to all the simulated measurements \(\). In particular, i.e. \((,_{}^{2})\). For all problems except for Fourier phase retrieval, the noise standard deviation is set as \(_{}=0.05\). Due to the severe ill-posedness of Fourier phase retrieval, we consider a smaller noise standard deviation \(_{}=0.01\).

Baselines and comparison protocolsWe consider four variants of DMs as plug-in priors for our method, namely VP-SDE (VP) , VE-SDE (VE) , iDDPM , and EDM . We compare our method with various baselines, including (1) optimization-based methods: PnP-ADMM , DPIR ; (2) conditional DMs: DDRM , DPS ; and (3) SGS-based method: PnP-SGS , DPnP . For fair comparison, we use the same pre-trained score function checkpoint for all DM-based methods. Since the pre-trained score function was trained with the DDPM formu

    &  &  &  \\   & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) \\  PnP-ADMM  & 26.88 & 0.7855 & 0.3472 & 26.55 & 0.7655 & 0.3600 & 26.61 & 0.7634 & 0.3766 \\ DPP  & 28.74 & 0.8348 & 0.2677 & 29.97 & 0.8529 & 0.2404 & 28.75 & 0.8378 & 0.2577 \\ DDRM  & 27.05 & 0.7819 & 0.2570 & & & & & 29.47 & **0.8437** & 0.2322 \\ DPS  & 28.53 & 0.8212 & 0.2330 & 27.87 & 0.8035 & 0.254 & 29.45 & 0.8379 & 0.2274 \\ PnP-SGS  & 27.46 & 0.8356 & 0.2445 & 28.98 & 0.8447 & 0.2190 & 28.30 & 0.8349 & 0.2160 \\ DPP  & 29.24 & 0.8360 & 0.2098 & 30.21 & 0.8527 & 0.2010 & 29.32 & 0.8407 & 0.2127 \\  PnP-DM (VP) & 29.46 & 0.8215 & 0.2022 & 30.06 & 0.8336 & 0.2099 & 29.40 & 0.8238 & 0.2219 \\ PnP-DM (VE) & 29.65 & 0.8399 & **0.2090** & **30.38** & 0.8547 & **0.1971** & 29.57 & 0.8431 & **0.2108** \\ PnP-DM (GDDM) & 29.60 & 0.8353 & 0.2030 & 30.26 & 0.8507 & 0.2103 & 29.53 & 0.8404 & 0.2213 \\ ParP-DM (GDDM) & **29.66** & **0.8411** & 0.2170 & 30.35 & **0.8547** & 0.2062 & **29.60** & 0.8435 & 0.2191 \\   

Table 1: Quantitative comparison on three noisy linear inverse problems for 100 FFHQ color test images. **Bold: best; Underline; second best.**

Figure 5: Visual examples for the motion deblur problem (\(_{}=0.05\)). We visualize one sample generated by each sampling algorithm.

lation (VP-SDE) , we convert it to the EDM formulation by applying the VP preconditioning . We use the Peak Signal-to-Noise Ratio (PSNR), the Structural Similarity Index Measure (SSIM), and the Learned Perceptual Image Patch Similarity (LPIPS) distance for quantitative comparison. For each sampling method, we draw 20 randoms samples, calculate their mean, and report the metrics on the mean image. More experimental details are provided in Appendices B, C, D.

Results: linear problemsA quantitative comparison is provided in Table 1. PnP-DM generally outperforms the baseline methods and that the VE and EDM variants consistently outperform the other two variants on these linear problems. Figure 5 contains visual examples for the motion deblur problem (see Appendix F.2 for the other two linear problems). PnP-DM provides high-quality reconstructions that are both sharp and consistent with the ground truth image. We also provide an uncertainty quantification analysis based on pixel-wise statistics in Figure 6. In the left three columns, we visualize the absolute error (\(|}-|\)), standard deviation (std), and absolute z-score (\(|}-|/\)). In the third column, red pixels highlight locations where the ground truth pixel values are outliers of the 3-sigma credible interval (CI) under the estimated posterior uncertainty. The fourth column contains scatter plots of \(|}-|\) versus std for each pixel of the reconstructions, where red boxes show the percentages of outliers (outside of 3-sigma CI) and gray boxes indicate the percentages within the 3-sigma CI. Similar to the synthetic prior experiment, DPS tends to have larger standard deviation estimations, as shown by the less concentrated distribution of gray points around the origin. Compared with baselines, especially PnP-SGS, our approach captures a higher percentage (97.46%) of ground truth pixels than the baselines (96.20% and 88.77%). If the true posterior were truly Gaussian, 99% of the ground-truth pixels should lie within the 3-sigma CI; however, as the posterior is not Gaussian with a DM-based prior, we do not necessarily expect to reach 99% coverage.

Results: nonlinear problemsWe provide a quantitative comparison in Table 2. For the CDP reconstruction problem, PnP-DM performs on par with DPS but outperforms other SGS-based methods. We then consider the Fourier phase retrieval (FPR) problem, which is known to be a challenging nonlinear inverse problem. One challenge lies in its invariance to 180\({}^{}\) rotation, so the posterior distribution have two modes, one with upright images and another with 180\({}^{}\)-rotated images, that equally fit the measurement. To increase the chance of getting properly-oriented reconstructions, we run each algorithm with four different random initializations and report the metrics for the best run, following the practice in . We find that PnP-DM significantly outperforms the baselines on this highly ill-posed inverse problem. As shown in Figure 7 (a), our method can provide high-quality reconstructions for both orientations, while the baseline methods fail to capture at least one of the two modes. We further run our method for a test image with 100 different random initialization and collect reconstructions in both orientations that are above 28dB in PSNR (90 out of 100 runs). The percentage of upright and rotated reconstructions are visualized by the pie chart in Figure 7 (b). With a prior on upright face images, our method generate mostly samples with the upright orientation. Nevertheless, it can also find the other mode that has an equal likelihood, demonstrating its ability to capture multi-modal posterior distributions.

### Experiments on black hole imaging

Problem setupWe finally validate PnP-DM on a real-world nonlinear imaging inverse problem: black hole imaging (BHI) (see Appendix B for more details). A visual illustration of BHI is provided

Figure 6: Comparison of uncertainty quantification (UQ) for the motion deblur. Left 3 columns: absolute error (\(|}-|\)), standard deviation (std), and absolute z-score (\(|}-|/\)) with the outlier pixels in red. Right column: scatter plot of \(|}-|\) versus std. Note that PnP-DM leads to a better UQ performance than the baselines by having the lowest percentage of outliers while avoiding having overestimated per-pixel standard deviations.

in Figure 8 (a). This BHI inverse problem is severely ill-posed. Even with an Earth-sized telescope, only a small fraction of the Fourier frequencies of the target black hole can be measured (region within the red box); in reality, this region is further subsampled with a highly sparse pattern (black lines). Additionally, the atmospheric noise causes nonlinearity of this BHI problem that sometimes results in a multi-modal posterior distribution of the reconstructed image . Here we demonstrate the effectiveness of PnP-DM in capturing a multi-modal posterior distribution. For brevity, we restrict our choice of diffusion models in PnP-DM to EDM and use DPS as the baseline.

Results on simulated dataWe use the simulated data from  where the measurements are generated assuming that the ground-truth black hole image were at the location of the Sagittarius A\({}^{*}\) black hole. Figure 8 (b) visually compares the results obtained by PnP-DM and DPS. We use the t-SNE method  to cluster the generated samples (100 for each method) and identify two modes in the samples generated by PnP-DM and three modes in those generated by DPS. We visualize the mean and three samples for each image mode. A metric for quantifying the degree of data mismatch is labeled on the top right corner of each image. As illustrated by both the mean and sample images, PnP-DM successfully captures the two modes previously identified for this dataset . Note that PnP-DM generates high-fidelity samples from both modes with sharp details of the flux ring, and its samples from "Mode 1" align well with the ground truth image. In contrast, two out of the three modes sampled by DPS fail to exhibit a meaningful black hole structure and do not correspond with the observed measurements, as indicated by the significantly larger data mismatch values.

Results on real dataFinally, we apply PnP-DM to the real M87 black hole data from April 6\({}^{}\), 2017 , with the results shown in Figure 1. By leveraging an expressive DM-based image prior, PnP-DM generates high-quality samples that are both visually plausible and consistent with the ring diameters observed in the official EHT reconstruction. These results highlight the robustness and effectiveness of our method in tackling a highly ill-posed real-world inverse problem.

## 5 Conclusion

We have introduced PnP-DM, a posterior sampling method for solving imaging inverse problems. The backbone of our method is a split Gibbs sampler that iteratively alternates between two steps that separately involve the likelihood and prior. Crucially, we establish a link between the prior step and a general DM framework known as the EDM formulation. By leveraging this connection, we seamlessly integrate a diverse range of state-of-the-art DMs as priors through a unified interface. Experimental results demonstrate that our method outperforms existing DM-based methods across

    &  &  \\   & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) & PSNR (\(\)) & SSIM (\(\)) & LPIPS (\(\)) \\  HDG  & – & – & – & 20.66 & 0.4308 & 0.6469 \\ DPS  & **33.43** & 0.9049 & **0.1374** & 23.60 & 0.6804 & 0.3126 \\ PnP-SGS  & 32.19 & 0.8889 & 0.2010 & 15.36 & 0.3659 & 0.5730 \\  PnP-DM (VP) & 32.19 & 0.8846 & 0.1966 & 30.36 & 0.8553 & 0.2115 \\ PnP-DM (VE) & 33.13 & 0.8971 & 0.1663 & 29.88 & 0.8464 & 0.2186 \\ PnP-DM (DDPM) & 33.35 & **0.9083** & 0.1471 & 30.61 & 0.8718 & **0.1975** \\ PnP-DM (EDM) & 33.25 & 0.9050 & 0.1386 & **31.14** & **0.8731** & 0.2024 \\   

Table 2: Quantitative evaluation on two noisy nonlinear inverse problems for 100 FFHQ grayscale test images. **Bold:** best; Underline; second best.

Figure 7: Results of the Fourier phase retrieval problem. (a) PnP-DM provides both upright and rotated reconstructions (two modes given by the invariance of the forward model to 180\({}^{}\) rotation) with high fidelity, while the baseline methods cannot. (b) We visualize the percentages of upright and rotated reconstructions out of 90 runs for a test image with two samples for each orientation.

both linear and nonlinear inverse problems, including a nonlinear and severely ill-posed black hole interferometric imaging problem.

LimitationsPnP-DM can be further improved in the following two aspects. First, PnP-DM currently requires evaluating the likelihood and prior steps for the entire image at a time. This potentially poses computational challenges in solving large-scale inverse problems (e.g. 3D imaging) or those with expensive likelihood evaluation (e.g. PDE inverse problems). Second, the current theoretical analysis does not consider the approximation error introduced in the likelihood step for general nonlinear inverse problems when running Langevin MCMC for finite iterations. Explicit incorporation of this error would offer further insights into the empirical performance of PnP-DM.

Broader impactsWe expect this work to make a positive impact in computational imaging and related application domains. For many imaging problems, there is a need to facilitate image reconstruction with expressive image priors and quantify uncertainty, which could lead to better imaging systems that enables further understanding of the imaging target. Nonetheless, as we are introducing DMs as priors into the imaging process, it is inevitable to inherent the potential bias of these models.

## 6 Acknowledgments

The authors thank Charles Gammie, Ben Prather, Abhishek Joshi, Vedant Dhruv, and Chi-kwan Chan for providing the black hole simulations. The authors also thank the generous funding from Schmidt Sciences and the Heritage Medical Research Fellowship. Z.W. was supported by an Amazon AI4Science Fellowship. Y.S. was supported by a Computing, Data, and Society Fellowship. B.Z. was supported by a Kortschak Fellowship.

Figure 8: Results on a nonlinear and severely ill-posed black hole imaging problem. Our method, PnP-DM, is compared with the conditional diffusion model baseline DPS. A metric quantifying the mismatch with the observed measurements is labeled for each sample, which should be around 2 for ideal measurement fit. Samples generated by PnP-DM exhibit two distinct modes with sharp details and a consistent ring structure, while samples given by DPS display inconsistent ring sizes and sometimes fail to capture the black hole structure entirely with samples having poor measurement fit.