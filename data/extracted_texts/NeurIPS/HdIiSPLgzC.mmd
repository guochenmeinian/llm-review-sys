# Mint-1t:

Scaling Open-Source Multimodal Data by 10x:

A Multimodal Dataset with One Trillion Tokens

 Anas Awadalla\({}^{1,2}\)1 Le Xue\({}^{2}\) Oscar Lo\({}^{1}\) Manli Shu\({}^{2}\)

**Hannah Lee\({}^{1}\) Etash Guha\({}^{1}\) Matt Jordan\({}^{4}\) Sheng Shen\({}^{5}\) Mohamed Awadalla\({}^{1}\) Silvio Savarese\({}^{,2,3}\) Caiming Xiong\({}^{,2}\) Ran Xu\({}^{,2}\) Yejin Choi\({}^{,1}\) Ludwig Schmidt\({}^{,1}\)**

\({}^{1}\) University of Washington, \({}^{2}\) Salesforce Research, \({}^{3}\) Stanford University,

\({}^{4}\) University of Texas at Austin, \({}^{5}\) University of California, Berkeley, \({}^{}\)Senior Authors

###### Abstract

Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, open-source multimodal interleaved datasets. In response, we introduce \(\)MINT-1T, the most extensive and diverse open-source **M**utimodal **I**N**terleaved dataset to date. \(\)MINT-1T comprises of one trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS. We release our data at https://github.com/mlfoundations/MINT-1T.

Figure 1: \(\)MINT-1T is a one trillion token multimodal interleaved pre-training dataset. This is the largest dataset of its kind and is more diverse than previous open-source datasets.

## 1 Introduction

Large open-source pre-training datasets have been important artifacts for the research community in studying data engineering and training transparent, open-source models. In the text domain, we have seen how early works such as C4 (Raffel et al., 2019) and The Pile (Gao et al., 2020) were integral for the community to train the first set of open-source large language models (GPT-J (Wang and Komatsuzaki, 2021), GPT-Neo (Black et al., 2021), and others). These works also set the stage for subsequent works that improved on data filtering methods and scale. Similar trends hold in the image-text space large-scale open-source datasets led to innovation on better data curation methods such as Data filtering networks (Fang et al., 2023), T-MARS (Maini et al., 2023), and others.

We are seeing a major shift from frontier labs to train large multimodal models (LMMs) (Google, 2023; Meta, 2024; Achiam et al., 2023) which require large multimodal interleaved datasets--comprising of free-form sequences of images and texts (an example of interleaved documents can be found in Figure 2). As the capabilities of frontier models advance rapidly, there is an increasing gap in the multimodal training data between closed- and open-source models. Existing open-source multimodal interleaved datasets are smaller and less diverse compared to their text-only counterparts and are sourced only from HTML documents, limiting the breadth and variety of data. This restriction hampers the development of robust open-source LMMs and creates a disparity between the capabilities of open and closed-source models.

To bridge this gap, we built \(\)MINT-1T (**M**ultimodal **IN**Ter**leaved), the largest and most diverse open-source multimodal interleaved dataset to date. MINT-1T contains a total of one trillion text tokens and three billion images, which are sourced from diverse sources like HTML/PDFs/ArXiv. Before MINT-1T, the largest open-source dataset in this area was OBELICS (Laurencon et al., 2023), a 115 billion text token dataset with 353M images sourced only from HTML.

Our contributions with \(\)MINT-1T are as follows:

**Data Engineering** Scaling this multimodal interleaved data presents more of an engineering challenge than building either text-only or image-text pair datasets. We are handling much larger document sizes, and the original ordering of images and text must be preserved.

Figure 2: Samples of multimodal document from the HTML (Left), PDF (Middle), and ArXiv (Right) subsets of \(\)MINT-1T with each document containing a sequence of images interleaved with text. Previous work has shown that interleaved data improves question-answering performance in the context of Flamingo-style models (Laurencon et al., 2023) and for training large multimodal models with a strong performance on both text-only and multimodal benchmarks (McKinzie et al., 2024). MINT-1T is the first open-source work to scale interleaved datasets to one trillion text tokens and collect interleaved documents from PDFs and ArXiv at large scales. Samples in this figure are text truncated due to space.

[MISSING_PAGE_FAIL:3]

using PyMuPDF 2. We discard PDF documents that are more than 50MB large (as they likely contain predominantly large images) and PDFs that are over 50 pages long. We exclude pages that contain no text and extract a reading order for the remaining pages. Reading order is obtained by finding the bounding box of all text blocks on a page, clustering the blocks based on columns, and ordering the blocks from top left to bottom right. Images are anchored in the sequence based on the proximity between the image's bounding box and text blocks on the same page. We discuss the limitations of this approach in Appendix A.1.

#### 2.1.3 ArXiv pipeline

ArXiv interleaved documents are built from the LaTeX source code. We use TexSoup 3 to find figure tags and interleave the images with the paper text. For multi-file papers (i.e. where each section is written in a different Tex file), we identify the main Tex file and replace input tags with the contents of its file. We additionally clean up the the LaTex code removing imports, bibliography, tables, and citation tags. As ArXiv is already a highly curated data source, we do not perform any of the filtering and deduplication described in the rest of this section.

### Text Quality Filtering

In line with practices established by RefinedWeb (Penedo et al., 2023), Dolma (Soldaini et al., 2024), and FineWeb (Penedo et al., 2024), we avoid using model-based heuristics for text filtering. This approach has proven to scale efficiently for text-only models. Initially, we eliminate non-English documents using Fasttext's (Joulin et al., 2017) language identification model (with a confidence threshold of 0.65). Additionally, documents with URLs containing NSFW substrings were removed to exclude pornographic and undesirable content. We apply text filtering methods from RefinedWeb, specifically removing documents with excessive duplicate n-grams or those identified as low quality in using MassiveText (Rae et al., 2021) rules.

### Image Filtering

After obtaining the curated set of PDFs and HTML files, we attempt to download all image URLs in the HTML dataset, discarding any non-retrievable links and removing documents that have no valid image links. We remove images smaller than \(150\) pixels to avoid noisy images such as logos and icons and images larger than \(20,000\) pixels as those usually correspond to off-topic images. For HTML documents, we remove images with an aspect ratio greater than two to remove low-quality images such as advertisement banners. However, for PDFs, we adjust this threshold to three to preserve scientific figures and tables, which are often erroneously excluded by stricter criteria.

### Safety Filtering

NSFW Image FilteringWe apply an NSFW image detector (Laborde) to all images in our dataset. If we find that a document contains a single NSFW image, we discard the entire document.

Personally Identifiable Information RemovalTo mitigate the risk of personal data leakage, we anonymize email addresses and IP addresses in our text data. Following FineWeb, we replace emails with templates such as "email@example.com" and IPs with randomly generated non-functional IPs.

### Deduplication

To remove duplicated content, we perform paragraph and document text deduplication within each CommonCrawl snapshot and image deduplication to remove repetitive, uninformative images such as icons and logos. All deduplication steps are done separately for each data source.

#### 2.5.1 Paragraph and Document Deduplication

Following Dolma's methodology [Groeneveld, 2023], we use a Bloom Filter for efficient text deduplication. We set the false positive rate to \(0.01\) for the bloom filter and deduplicate \(13\)-gram paragraphs (indicated through double newline delimiters) from each document. If more than \(80\%\) of a document's paragraphs are duplicates, we discard the entire document.

#### 2.5.2 Removing Common Boilerplate Text

Post-paragraph deduplication, we notice that short common boilerplate sentences in HTML documents, such as "Skip to content" or "Blog Archive," remain. To remove these noisy sentences, we run exact paragraph deduplication on \(2\%\) of each CommonCrawl snapshot, in line with CCNet [Wenzek et al., 2019]; doing this at small scales ensures we mostly remove just common boilerplate text.

#### 2.5.3 Image Deduplication

Within each CommonCrawl snapshot, we remove frequently occurring images based on SHA256 hashes. Rather than strict deduplication, we follow Multimodal-C4 [Zhu et al., 2023] by only removing images that appear more than ten times within a snapshot. Consistent with OBELICS [Laurencon et al., 2023], we remove repeated images within a single document and keep only the first occurrence.

### Infrastructure

Throughout the data processing, we had access to an average of 2,350 CPU cores from a mixture of 190-processor and 90-processor nodes. In total, we used \(\)4.2M CPU hours to build this dataset.

## 3 Analysis

### Comparing Document Composition in MINT-1T with OBELICS

In assessing the composition of interleaved datasets, two key characteristics are examined: the distribution of text tokens per document and the number of images per document. For this analysis, we randomly sampled 50,000 documents from both OBELICS and each data source in MINT-1T. We use GPT-2's tokenizer to calculate the number of text tokens. We remove outliers, excluding documents that fall outside the 1.5 * interquartile range for the number of text tokens and images. As shown in Figure 4, the HTML subset of MINT-1T aligns closely with the token distribution seen in OBELICS. However, documents sourced from PDFs and ArXiv tend to be longer than HTML documents on average, highlighting the benefits of sourcing data from diverse sources. Figure 5examines the image density across all documents, revealing that PDFs and ArXiv documents contain more images compared to HTML documents, with ArXiv samples being the most image dense.

### How Do Different Data Sources Improve Document Diversity?

An important motivation for expanding the pool of multimodal documents beyond HTML is the improvement of domain coverage. To quantify the diversity and depth of this coverage, we employ a Latent Dirichlet Allocation [Campbell et al., 2003] (LDA) model trained on 100,000 documents sampled from the OBELICS dataset, the HTML subset of MINT-1T, and the PDF subset (excluding ArXiv) from MINT-1T to get 200 topics. We then use GPT-4 to classify the set of words to identify the dominant domains - such as Health & Medicine, Science, Business, Humanities, History, etc. - based on MMMU domains.

Our analysis reveals distinct trends in domain distribution:

**OBELICS:** This dataset shows a pronounced concentration in "Humanities and Social Sciences". This may be attributed to its data construction process, which involves filtering out documents that do not resemble Wikipedia articles, thus potentially altering the distribution to more general knowledge and humanities-focused content.

**MINT-1T's HTML Subset:** In contrast to OBELICS, the HTML subset of MINT-1T is not strongly biased towards any specific domain, suggesting a broader and more balanced domain representation.

**MINT-1T's PDF Subset:** There is a higher proportion of "Science and Technology" documents within the PDF documents of MINT-1T. This trend is likely due to the nature of scientific communication, where PDFs are the preferred format for sharing detailed research papers and technical reports.

## 4 Model Experiments

### Training Setup

In this section, we outline the architecture of the LMMs we train, the hyper-parameters used, and the methods to evaluate the multimodal interleaved abilities of the models.

**Modeling** Our architecture is adopted from XGen-MM [Xue et al., 2024]. We use the ViT-H vision encoder with resolution 378 from Data Filtering Networks [Fang et al., 2023] and pass the patch embeddings into a six layer perceiver resampler [Alayrac et al., 2022]. Each image is represented as 128 tokens. The pooled patch embeddings are interleaved with the text token embeddings and passed into Phi-3 mini language model [Abdin et al., 2024]. We keep the vision encoder frozen while training the resampler and the language model. We use a batch size of, on average, 1.8M multimodal tokens. For all of our training runs, we use 2,000 warmup steps with a maximum learning rate of \(5*10^{-5}\) and cosine learning rate decay. We also apply 0.05 weight decay to all trainable parameters.

Figure 6: **Document domain distribution: The percentage of documents from each domain in MMMU for OBELICS and subsets of MINT-1T. We find two interesting trends: (1) The majority of documents in OBELICS are related to the _Humanities and Social Sciences_; this trend isn’t found in MINT-1T’s HTML subset. (2) The majority of PDF documents are _Science_ related.**

All of our training is done using the OpenFlamingo (Awadalla et al., 2023) codebase. We train all of our models on 32 H100 GPUs for a total of 1,920 GPU hours per experiment.

**Training Data** For all experiments, we train our model on \(50\%\) image-text captioning batches and 50% multimodal interleaved batches. We sample a maximum of \(2048\) multimodal tokens from each interleaved document and 340 tokens from each image-text sample. As in Flamingo Alayrac et al. (2022), we add an <|endofchunk|> token to indicate the end of an adjacent image-text sequence. During training, we randomly drop 50% of single-image interleaved documents to upsample multi-image documents. For our image-text dataset, we use a mixture of internal curated caption datasets.

### Evaluation Setup

We assess a model's capability to reason about multimodal interleaved sequences through its in-context learning abilities and multi-image reasoning performance.

**In-context Learning** The models are evaluated on four-shot and eight-shot in-context learning performance on various captioning benchmarks (COCO (Karpathy test) (Lin et al., 2014) and TextCaps (validation) (Sidorov et al., 2020)) and visual question answering datasets (VQAv2 (validation) (Agrawal et al., 2015), OK-VQA (validation) (Marino et al., 2019), TextVQA (validation) (Singh et al., 2019), and VizWiz (validation) (Gurari et al., 2018)). For all evaluations, we randomly sample demonstrations from the training set. Our reported scores are averaged over multiple evaluation runs where we randomize demonstrations. We find that performance is sensitive to the chosen prompts, so we ablate through different prompts for each task and choose the prompt that performs best. The list of prompts we use and generation parameters can be found in Appendix A.2.

**Multi-image Reasoning** We additionally evaluate models on MMMU (Yue et al., 2024)(containing both single and multi-image questions) and Mantis-Eval (Jiang et al., 2024)(all multi-image questions) to probe a model's multi-image reasoning abilities beyond in-context learning evaluations.

### Experiments

**Training on HTML Documents**

We first evaluate how the HTML portion of MINT-1T compares to OBELICS, as OBELICS is the previous leading interleaved dataset and is also curated from HTML documents. We train two models on the HTML portions of MINT-1T and OBELICS for 10B multimodal tokens total and assess their 

[MISSING_PAGE_EMPTY:8]

benchmarks MINT-1T is significantly better than both baselines. We also see that MINT-1T (HTML) also outperforms OBELICS on VQA tasks.

**Performance on Different Domains** A motivation for including diverse domains in MINT-1T is to improve model generalization. In Figure 6(b), we break down performance on MMMU for each domain. With the exception of the Business domain, MINT-1T outperforms OBELICS and MINT-1T (HTML). We highlight the performance increase on Science and Technology domains for MINT-1T and speculate that this can be attributed to the prevalence of these domains in ArXiv and PDF documents.

### Impact of Model Architecture

Our experiments, presented in Section 4.3, use XGen-MM's architecture. Unlike with large language models, the design space for multimodal models is much more diverse with many architectures for aligning a vision encoder to a language model. Naturally, we were curious if our results would hold for other popular training setups.

To investigate this, we replicate our training experiments using Idefics2's architecture. Idefics2 differs from XGen-MM in that it freezes a non-instruction finetuned large language model and adds LoRA  matrices on all linear layers. For our Idefics2 reproduction we use the Mistral-7B-v0.3 language model and DFN ViT-H vision encoder with resolution 384. Unlike Idefics2, we do not experiment with flexible image resolution in training and keep the vision encoder completely frozen. We present in-context learning results for Idefics2 model experiments in Table 4. We find that MINT-1T's HTML subset performs better than OBELICS with notable gains on TextVQA, VQAv2, and TextCaps. We highlight the performance gap difference between XGen-MM and Idefics2 ablations in Figure 9. One key difference in the Idefics2 experiments is that the HTML subset of MINT-1T performs much more competitively on captioning benchmarks in comparison to OBELICS.

    &  &  &  \\   & & COCO & TextCaps & OKVQA & TextVQA & VQAv2 & VizWiz & \\   & 4 & \(110.2 0.38\) & \(83.1 1.28\) & \(52.8 0.04\) & \(46.7 0.23\) & \(63.8 0.10\) & \(28.9 0.50\) & \(64.3 0.24\) \\  & 8 & \(111.8 1.04\) & \(86.6 0.31\) & \( 0.04\) & \(46.9 0.02\) & \(64.1 0.09\) & \(36.2 0.51\) & \(66.7 0.20\) \\   & 4 & \( 0.01\) & \( 0.05\) & \(52.9 0.26\) & \( 0.31\) & \( 0.04\) & \(29.0 0.64\) & \( 0.12\) \\  & 8 & \(111.3 0.05\) & \( 0.11\) & \(54.1 0.49\) & \( 0.12\) & \(64.8 1.04\) & \(36.6 0.13\) & \( 0.19\) \\   

Table 4: **Idefics2 model results:** We compare OBELICS and MINT-1T (HTML), when training an Idefics2 LMM. Models are evaluated using four and eight in-context learning examples, with each evaluation run for two trials. We report the mean performance and standard deviation.

Figure 9: **Impact of architecture:** On in-context learning benchmarks, XGen-MM models perform marginally better when trained on OBELICS compared to MINT-1T’s HTML subset. In contrast, Idefics2 models show a slight advantage for MINT-1T (HTML) over OBELICS.

## 5 Related Work

### Multimodal Interleaved Data

Large-scale multimodal interleaved datasets were first presented in Flamingo (Alayrac et al., 2022) and CM3 (Aghajanyan et al., 2022). Kosmos (Huang et al., 2023) showed similar properties and was followed by Multimodal-C4 (Zhu et al., 2023) and OBELICS (Laurencon et al., 2023), the first open-source multimodal interleaved datasets. More recent work such as Chameleon (Meta, 2024) and MM1 (McKinzie et al., 2024) have scaled OBELICS to train state-of-the-art multimodal models. A complementary line of work, Mantis (Jiang et al., 2024) and MIMIC-IT (Li et al., 2023) builds interleaved instruction tuning datasets. Similarly, Multimodal Arxiv (Li et al., 2024a) builds high quality captioning and instruction tuning data from ArXiv papers.

### Large Open-source Pre-training Datasets

Large, high-quality pre-training datasets are the backbone of open-source research. In image-text datasets, where preliminary works (Schuhmann et al., 2021; Byeon et al., 2022; Schuhmann et al., 2022; Gadre et al., 2023) focused on scaling image-text datasets to billions of samples and has been crucial for training strong open-source multimodal models (Ilharco et al., 2021; Sun et al., 2023). Similarly in language modeling, large datasets like Pile (Gao et al., 2020), Redpajama (Computer, 2023), RefinedWeb (Penedo et al., 2023), Dolma (Soldaini et al., 2024), Datacomp-LM (Li et al., 2024b), and FineWeb (Penedo et al., 2024) have been crucial for training fully transparent open-source models.

### Large Multimodal Models

The past year has seen a large influx in strong large multimodal models (LMMs). There is a line of work that seeks to pre-train existing language models on large-scale multimodal interleaved and image-text datasets. This was first presented Flamingo (Alayrac et al., 2022) and adopted by open-source models such as OpenFlamingo (Awadalla et al., 2023), Idefics (Laurencon et al., 2023), and Emu (Sun et al., 2023). More recent works like Idefics2 (Laurencon et al., 2024), MM1 (McKinzie et al., 2024), VILA (Lin et al., 2024), and XGen-MM (Xue et al., 2024) have also adopted similar data mixtures. A separate line of work aligns large language models to vision encoders using high-quality instruction-tuning data and image-text datasets such as LLaVA (Liu et al., 2023, 2023), InstructBLIP (Dai et al., 2023), QwenVL (Bai et al., 2023), Yi-VL (Young et al., 2024), MiniCPM-V (Hu et al., 2024), and more. Moreover, the latest generation of large models such as GPT4-o (Achiam et al., 2023), Gemini (Google, 2023), and Chameleon (Meta, 2024) are trained on multimodal data from the start.

## 6 Limitations and conclusion

In this work, we present MINT-1T, the first open-source trillion token multimodal interleaved dataset and an important component for training large multimodal models. We believe MINT-1T will be a valuable resource for the research community to do open science on multimodal interleaved datasets. An important consideration when releasing large datasets is to avoid exposing harmful content from the web. We were careful about filtering out personally identifiable information and not safe for work content from MINT-1T. However, future work should explore more ways to improve the safety of multimodal internet data. Moreover, subsequent work should train models on larger subsets of MINT-1T, build filtering methods to improve data quality, and curate multimodal sequences from other sources.