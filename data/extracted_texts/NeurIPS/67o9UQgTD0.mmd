# Counterfactual Memorization

in Neural Language Models

 Chiyuan Zhang

Google Research

chiyuan@google.com &Daphne Ippolito

Carnegie Mellon University

daphnei@cmu.edu &Katherine Lee

Google DeepMind

katherinelee@google.com &Matthew Jagielski

Google DeepMind

jagielski@google.com &Florian Tramer

ETH Zurich

florian.tramer@inf.ethz.ch &Nicholas Carlini

Google DeepMind

ncarlini@google.com

###### Abstract

Modern neural language models that are widely used in various NLP tasks risk memorizing sensitive information from their training data. Understanding this memorization is important in real world applications and also from a learning-theoretical perspective. An open question in previous studies of language model memorization is how to filter out "common" memorization. In fact, most memorization criteria strongly correlate with the number of occurrences in the training set, capturing memorized familiar phrases, public knowledge, templated texts, or other repeated data. We formulate a notion of _counterfactual memorization_ which characterizes how a model's predictions change if a particular document is omitted during training. We identify and study counterfactually-memorized training examples in standard text datasets. We estimate the influence of each memorized training example on the validation set and on generated texts, showing how this can provide direct evidence of the source of memorization at test time.

## 1 Introduction

Modern neural language models (LMs) have achieved impressive results in generating high quality text [e.g. Brown et al., 2020, Zhang et al., 2022, Chowdhery et al., 2022, OpenAI, 2023] and have led to breakthroughs in many downstream natural language processing tasks [Devlin et al., 2019, Raffel et al., 2020, Bommasani et al., 2021]. The paradigm of taking a single large-scale pre-trained model and fine-tuning it for many tasks motivates the study of these models' ability to _generalize_ by avoiding _memorizing_ their training data. Moreover, memorization of sensitive user information or copyrighted materials in the training data [Carlini et al., 2020, Vyas et al., 2023, Lee et al., 2023] leads to practical concerns in real world applications.

Previous work on memorization in neural language models demonstrated the ability to extract memorized training data, including sensitive data such as phone numbers and usernames [Carlini et al., 2020, Ziegler, 2021, Carlini et al., 2019, Henderson et al., 2017, Thakkar et al., 2020, Thomas et al., 2020]. One issue with these extraction attacks is that they primarily identify "common" and frequently occurring strings in the training set. For example, as shown in the analysis of Lee et al. , near-duplicate training examples, which are very common in standard text corpora, account for a large majority of the memorized content. To filter out such commonly occurring strings from all memorized texts, previous work applied various heuristic rules to distinguish frequently-occurring sequences from memorization of isolated pieces of information.

In this paper, we propose a principled causal perspective to disentangle memorization of common vs rare data, by directly tying a model's predictions to the presence or absence of individual training examples. We define _counterfactual memorization_ as a measure of the change in a model's prediction when a particular example is excluded from the training set. Counterfactual memorization accounts for the commonality of an example as removing one instance of a text that is common across multiple documents will have a minor effect on the model's prediction on that text. The mathematical formulation of counterfactual memorization extends a prior definition of label memorization in classification models (Feldman, 2020) to the context of neural language modeling.

Formally, a training document \(x\) is considerded counterfactually memorized, when the language model predicts \(x\) accurately _if and only if_ the model was trained on \(x\). This allows us to construct a procedure to quantitatively measure the memorization of isolated pieces of text, whose sole presence in the training dataset have a large effect on the model's predictions.

Following Feldman and Zhang (2020), we further extend this definition to _counterfactual influence_, which measures the influence of a memorized training text sample on another text example. Counterfactual influence allows us to trace the source of information for a model's predictions, by locating the training example(s) which significantly contributed to it. With these tools, we study memorization across several standard text datasets. Our main contributions are as follows:

1. We define counterfactual memorization in neural LMs which gives us a principled perspective to distinguish memorization of "rare" and "common" information in neural LMs (Section 3).
2. We estimate counterfactual memorization on several standard text datasets, and confirm that rare memorized examples exist in all of them. We study common patterns across memorized text and the memorization profiles of individual internet domains. (Section 4).
3. We identify an inverse correlation between number of duplicates and counterfactual memorization as compared with previous definitions of memorization (Section 5).
4. We extend the definition of counterfactual memorization to counterfactual _influence_, and study the impact of memorized examples on the test-time prediction of the validation set examples and generated examples (Section 6).

## 2 Related Work

Previous work analyzed the memorization of large language models on sensitive information (e.g. phone numbers) in the training data (Carlini et al., 2020; Ziegler, 2021) or synthetically injected "canaries" (Carlini et al., 2019; Henderson et al., 2017; Thakar et al., 2020; Thomas et al., 2020). However, not all the memorized texts are equally interesting -- as confirmed in a later study (Lee et al., 2021), near-duplicated training examples are very common in standard text corpus, and those commonly occurring phrases contribute significantly to memorized texts. In order to distinguish "common" memorization of common phrases or public knowledge from "rare" memorization of private, rare information, various heuristics were adopted in previous investigations. Our paper proposed a principled perspective towards this problem. Our intuition comes from psychologies studies that categorize human (declarative) memory into _episodic_ memory (Tlving, 1983) of specific contents of individual events, and _semantic_ memory (Squire, 1992) about general knowledge like grammars and factual information. We would like the models to obtain semantic memory but avoid episodic memory. The capture the latter, we proposed a notion of counterfactual memorization. The mathematical formulation of counterfactual memorization is borrowed from a notion of label memorization in Feldman (2020) and adapted to the context of neural LMs in this paper. This formulation has been studied empirically in the context of computer vision in Feldman and Zhang (2020). In a follow up work, Ilyas et al. (2022) showed that it is possible to fit a _datamodel_ to predict the outcome of training a model on a specific training subset and evaluating on a specific input. However, this procedure requires training a massive number of models (e.g. 300,000 for CIFAR-10) on random subsets of the training data, thus is computationally infeasible for the scale of language models considered here.

The general idea of measuring model behavior on held-out training data is common in machine learning. In cross validation, held-out data is used to estimate the test performance for model selection; in learning theory, leave-one-out stability was shown to be deeply connected to generalization (e.g. Mukherjee et al., 2006); in differential privacy, the worst case performance difference of models trained on two "neighboring" datasets (identical except a single example being held-out or replaced)quantifies the privacy guarantee of a learning algorithm (Dwork et al., 2014; Nasr et al., 2021; Jagielski et al., 2020). Most previous work aimed for an overall measurement, while our paper focused on characterizing the behaviors of individual examples.

We estimated a _counterfactual influence_ to study how a memorized training example impact the model prediction at test time. Influence functions have been used in statistics to assess robust estimators since Hampel (1974). Previous papers adopted it to analyze neural network predictions (Koh and Liang, 2017; Koh et al., 2019). However, the estimation was found to be computational expensive and fragile (Basu et al., 2021). Pruthi et al. (2020) tracks the gradient updates during training to estimate the influence from a training example; Feldman (2020), Feldman and Zhang (2020) use aggregated statistics from multiple models independently trained on heldout data subsets to estimate the influence. Further extensions were shown to work well on detecting mislabeled data in classification problems (Wang and Jia, 2022) and characterizing hallucinations in Neural Machine Translation (Raunak et al., 2021). Alternative methods also looked at simple data statistics (e.g. co-occurrence counts) without model re-training to infer the causal effects on language models' predictions (Elazar et al., 2022). In this paper, we adapt the approach from Feldman (2020), and formulate counterfactual influence directly with subset sampling, as oppose to leave-one-out influence. We also extend the estimation to assess the influence on generated examples.

Counterfactual is an important notion in statistical causality (Pearl et al., 2000; Rubin, 2005; Pearl, 2009; Imbens and Rubin, 2015) useful for studying causal probabilistic inference under alternative conditions. Such counterfactuals may or may not be directly testable (e.g. a counterfactual treatment in medical studies). In this paper, we directly measure the counterfactual influence of a training example by comparing the behavior of the model trained with and without that example.

## 3 Counterfactual Memorization

To quantify memorization of rare details of a specific training document, we define the following notion of _counterfactual memorization_. The mathematical formulation is borrowed from Feldman (2020), where it was originally proposed to quantify label memorization in multi-class classification problems. We extend it to the context of unsupervised neural language modeling.

**Definition 3.1** (Counterfactual Memorization).: Given a training algorithm \(A\) that maps a training dataset \(D\) to a trained model \(f\), and a measure \(M(f,x)\) of the performance of \(f\) on a specific example \(x\), the counterfactual memorization of a training example \(x\) in \(D\) is given by

\[(x)_{S D,x S}[M(A(S),x )]}_{}-_{S D,x S}[M(A(S),x)]}_{$ trained with $x$}},\] (1)

where \(S\) and \(S^{}\) are subsets of training examples sampled from \(D\). The expectation is taken with respect to the random sampling of \(S\) and \(S^{}\), as well as the randomness in the training algorithm \(A\).

That is, our memorization definition compares the difference between two expected performance measures on a given example \(x\). On one side, we compute the expected performance of a model when trained on datasets that _contain_ the example \(x\), and, on the other side, we compute the expected performance of a model when trained on datasets that do _not_ contain the example \(x\). Throughout this paper we use per-token accuracy as the measure \(M\). In other words, we ask the model to predict the next token based on the groundtruth context (preceding tokens), measure the 0-1 loss of the argmax token prediction, and then average it across all predicted tokens.

The expectations in Equation (1) can be empirically estimated via sampling. Specifically, we train \(m\) different models on independently sampled subsets \(S_{1},,S_{m}\) of equal size \(|S_{i}|=r|D|\) for a fixed \(r(0,1)\). We then divide these models into two groups: the first group contains all models trained on subsets \(S\) where \(x S\); and the second group are all models trained on subsets \(S\) where \(x S\). We take the average performance on \(x\) in the two groups separately and compute the difference between the two:

\[}(x)*{mean}_{i:x S_{i}}[M(A(S _{i}),x)]-*{mean}_{i:x S_{i}}[M(A(S_{i}),x)].\] (2)

This difference quantifies how the presence or absence of the example \(x\) in a model's training set affect the model's performance on \(x\). If there is a large difference between including an example in the training set versus not including it, then we consider this example _counterfactually memorized_.

For each \(x\), we refer to models trained with \(x\) in the training set (\(\{A(S_{i}):x S_{i}\}\)) as IN models and the models \(x\) was not trained on (\(\{A(S_{i}):x S_{i}\}\)) as OUT models. Note we do not need to _retrain_ a model for each example \(x\). Instead, we train \(m\) models once on random subsets of \(D\), and compute the estimation (Equation 2) for _all_ examples using the same set of \(m\) models. Ilyas et al. (2022) recently showed that it may also be possible to directly _predict_ these scores using a regression model, yet this approach is computationally prohibitive for large language models.

## 4 Analyzing Counterfactual Memorization

We estimate and analyze counterfactual memorization of training examples in three standard text datasets: RealNews (Zellers et al., 2019), C4 (Raffel et al., 2020) and Wiki40B:en (Guo et al., 2020). Unless otherwise specified, we use Transformer-based language models (Vaswani et al., 2017) equivalent to (decoder only) T5-base (Raffel et al., 2020) with \(\)112M parameters. To save computation and enable more direct comparisons across datasets, we truncate the training set for each datasets by taking the first \(2^{21}\) documents. To estimate counterfactual memorization, we train 400 models for each dataset, each on a random \(25\%\) subset of the training examples. In practice, we use a hash-based filtering mechanism to efficiently approximate random subset sampling (details in Appendix G), as the data loading APIs for large text corpora generally support only sequential visits to examples with limited shuffling and subsampling capability within a window.

We train each model for 60 epochs1 using the Adam optimizer (Kingma and Ba, 2015) with learning rate 0.1 and weight decay \(10^{-5}\). For C4/RealNews/Wiki40B:en, respectively, our models converge to an average per-token accuracy of 44.21%/47.59%/66.35% on the subsampled training set, and 27.90%/31.09%/49.55% on the validation set. On average, the models start to overfit at around epoch 5, as indicated by the signal that the validation accuracy starting to decrease.

### Distribution of Memorization

Table 1 shows examples from the RealNews training set sampled at various memorization levels. Examples with the highest memorization are generally unconventional text such as all-capital letters, structured formats (i.e., tables or bullet list), and multilingual texts. After those artificial examples, examples with intermediate-to-high memorization are most often news reports of specific events. One of our main goals is to be able to separate memorization of such examples containing details of specific events from memorization of common facts or highly duplicated template texts. Indeed, templated documents with many near-duplicate copies in the training data generally have low counterfactual memorization. C4 and Wiki40B:en have similar trends. Interestingly, though Wikipedia articles are less likely to be auto-generated from templates than the web in general, we do observe repetitive patterns in low-scoring documents, such as "_START_ARTICLE_<place name>, Virginia _START_PARAGAPH_<place name> is an unincorporated community in <county name>, in the U.S. state of Virginia."

To visualize the distribution of memorization, we plot 2D histograms in Figure 1, where the x-axis shows the difference of IN-accuracy and OUT-accuracy (i.e. the counterfactual memorization), and the y-axis shows the sum of the two, which we term "simplicity". A simple example is one that is scored highly regardless of whether a model saw it during training. The histograms are plotted in log scale to better visualize the exponential decay in the tail for high memorization and simplicity levels.

From the 2D density plots, we find that easy examples tend to have low memorization. However, there is no simple linear correlation. Peak memorization occurs for examples of intermediate simplicity. For the hardest examples, the memorization scores are low, because even the IN-models could not learn them well. Many hard examples consist of ill formatted text or contained foreign languages. As a result, in Wiki40B:en, which contains higher quality texts, the lower bound of the histogram is higher than the other two datasets (Figure 1). Interestingly, the choice of data has a relatively minor effect on memorization: the shape of the memorization histogram is generally consistent across the three datasets; the range of memorization values is only slightly compressed for Wiki40B:en.

[MISSING_PAGE_FAIL:5]

news commentary website that frequently quotes other major news articles. This may lead to duplicate text in the dataset which we suspect contributes to its overall lower memorization distribution.

The observations are similar on C4: blogspot.com contains a large number of documents in the training set with only moderate amounts of memorization; zh.wikipedia.org and buckinghamautos.com.au have high memorization due to foreign (Chinese) or structured (car sales listings) text; and www.unitedstateszipcodes.org has very low memorization scores because common templates are re-used to generate similar pages for individual zip codes.

### Number of Models Needed

To evaluate the impact of a single training example, one may wish to train two models that differ only in that single example. In practice, the stochasticity in a single run of common training algorithms (e.g. SGD) produces too low signal-to-noise ratios to be useful for such estimation. Moreover, leave-one-out estimation means a separate pair of models needs to be trained for each training example, which is computationally costly. Therefore, we formulated our estimation in Section 3 by accumulating statistics from \(m\) models independently trained on random training subsets. In our experiments, we set \(m=400\). To understand how sensitive our results are to \(m\), we analyze the rankings produced by distinct sets of models of size \(m\). We vary \(m\) from 6 to 192, and partition our set of 400 models into up to 10 sets of \(m\) models (e.g. for \(m=192\), we construct 2 partitions, and for \(m=6\), we construct 10). We then compute the Spearman's R between these partitions to measure the agreement between the rankings produced by each partition. If the rankings are very similar (have Spearman's R close to 1), then this number of models is reliably estimating the true ranking of memorization scores. We plot these Spearman's R values in Figure 2(a). Even at 96 models, this correlation begins to plateau near 1, lending confidence that 400 models is sufficient for reliable estimation of memorization scores. See Appendix D for more analysis on the sensitivity to \(m\).

### Impact of Number of Training Epochs

As expected, the overall amount of memorization grows consistently with the number of epochs of training (Figure 2(b)). This makes sense since training for more epochs increases overfitting. As training progresses, we also see an increasingly long tail of examples with high memorization scores. On RealNews, about 59% of examples had consistently increasing memorization scores across all epochs considered. There were no examples whose memorization decreased in a significant way over training (all observed decreases can be attributed either to noise or to instability early in training). Only 0.5% of examples stayed completely un-memorized with scores which never rose above 0.1,

Figure 2: For each web domain, we plot the 95-percentile of memorization against the number of examples from that domain in **(a)** RealNews and **(b)** C4. The red dotted line indicates a threshold of a minimum of 1000 articles for RealNews and 50 articles for C4. The memorization distributions of a few representative domains are shown for **(c)** RealNews and **(d)** C4.

while 85% of examples had memorization scores which never rose above 0.2. Figure 3c shows the fraction of memorized examples as training progresses, at several thresholds of memorization. We can see that more training epochs significantly increases memorization.

## 5 Duplicate Text and Memorization

One of the goals of evaluating counterfactual memorization is to identify examples that have a low number of duplicates yet whose presence versus absence in the training data has a large effect on the model. Here, we perform a quantitative study of the (anti-)correlation between duplication and counterfactual memorization compared with the positive correlation between duplication and the "generation-time memorization" definitions of memorization used by Lee et al. (2021); Carlini et al. (2022); Kandpal et al. (2022).

Following the method from (Lee et al., 2021), we first use MinHash (Broder, 1997) to identify near-duplicate examples in RealNews train set. We consider a example a duplicate if it has an normalized edit similarity of greater than 0.7 (definition included in Appendix I). Out of 2.01 million examples, \(\)38,000 were identified as being a near-duplicate with at least one other example. Among these frequently-occurring examples, the Pearson correlation between an example's counterfactual memorization score and the number of near-duplicates for that example is -0.39; in other words, memorization does quantitatively decrease when data is repeated more often.

In Figure 3d we can see that examples with a large number of near-duplicates have smaller memorization scores. Counterfactual memorization primarily differentiates amongst examples with a few number of duplicates. This makes sense given that examples with lots of near duplicates would likely have their near duplicates in OUT-models. This is to be contrasted with "generation-time memorization" (discussed in Section A) that measures the textual overlap between model generated texts and the training documents. There, the number of occurrences strongly correlate with the measured memorization (Carlini et al., 2020; Lee et al., 2021; Kandpal et al., 2022). **Counterfactual memorization measures a fundamentally different type of memorization from simple textual matching considered in prior work, providing information about how easy or hard a training example is in the context of the rest of the training set.** In Table 1 we can see this effect qualitatively: sequences with near-duplicates in the training set tend to have low counterfactual memorization (as expected).

## 6 From Memorization to Influence

Counterfactual memorization identifies training examples that contain rare information not conveyed by other examples. A natural question to ask is whether a model would leak the information in a memorized example during inference. Previous paper studies membership inference attack (Shokri et al., 2017; Sablayrolles et al., 2019; Long et al., 2020) where an attacker tries to figure out if a particular example exists in the training set. In this paper, we consider standard model evaluation without adversarial attackers, and quantify "does seeing a particular training example strongly influence the prediction on a validation example?" Another way of asking this is if a single example in the training set has an large and over-representative impact on the prediction of a validation

Figure 3: **(a) Spearman’s R between memorization rankings from two disjoint sets of \(m\) models. The rankings are variable at low numbers of models, but starts to converge at 192 models. All of our other experiments use 400 models. Reported values are averages over up to 10 partitions, with error bars of 10 standard deviations. (b) The distribution in memorization of RealNews examples as training progresses. (c) The fraction of RealNews examples with memorization consistently above the specified threshold as training progresses. (d) For RealNews, we plot memorization scores against the number of near-duplicates an example had in the dataset.**

example. We answer these questions by measuring _counterfactual influence_ with a formulation adapted from Feldman and Zhang (2020):

**Definition 6.1** (Counterfactual Influence).: Given a training algorithm \(A\) that maps a training set \(D\) to a trained model, and a performance measure \(M\), the counterfactual influence of a training example \(x D\) on another example \(x^{}\) is

\[(x x^{})_{S D,x S }[M(A(S),x^{})]-_{S D,x S}[M(A(S),x^{})],\] (3)

where \(S\) is a subset of training examples sampled from \(D\). The expectation is taken with respect to the random sampling of \(S\), as well as the randomness in the training algorithm \(A\). Here \(x^{}\) can be an example from the validation set or test set, a generated example or a training example.

An empirical estimation of the influence can be computed similarly to counterfactual memorization by uniformly sampling \(m\) subsets \(S_{1},,S_{m}\) from \(D\), where \(|S_{i}|=r|D|\), and calculating

\[}(x x^{})*{ mean}_{i:x S_{i}}[M(A(S_{i}),x^{})]-*{mean}_{i:x  S_{i}}[M(A(S_{i}),x^{})].\] (4)

This measures how much a training sample \(x\)'s presence influences the prediction of a different example \(x^{}\). Note, \((x)=(x x)\), i.e., counterfactual memorization is self influence.

**Influence on Examples of the Validation Set**. With the same models trained for estimating memorization, we can estimate the counterfactual influence on the validation set according to Equation (4). For each example in the validation set, we can estimate the influence on it from each training example. Figure 3(a) shows the distribution of influence from all training example on three different examples from the validation set. The green example was randomly chosen and represents the behavior for most validation examples: it receive close-to-zero influence from all the (individual) training examples. The blue and orange examples were sampled to have high and intermediate maximum influence. Each of them has one (or a few) strong influencer from the training set, as indicated by the bars to the right of the histogram. They also only receive tiny influence from all the rest of the training examples, though the variance of influence is larger than for the green example.

Intuitively, most training examples will have small influence on validation set examples because the models learn distributional patterns shared across many training examples, and _individual_ training examples tend to have insignificant influence here. However, a training example \(x\) with high counterfactual memorization contains rare information that are not shared with other examples. Therefore, if a validation set example \(x^{}\) contains similar information, \((x x^{})\) could be large. Figure 3(b) shows the relationship between memorization and influence by plotting \((x)\) of each training example \(x\) against its maximum influence \(_{x^{}}(x x^{})\) on \(x^{}\) across the validation set.

Consistent with our intuition, examples with small memorization scores have small max-influence scores. Larger influence scores on the validation set generally requires larger memorization scores

Figure 4: (a) Histogram of the influence of all the training examples on a specific test example for three different test examples on RealNews. The blue and orange examples have high and intermediate influence from some training examples, as indicated by the outlier values to the right of the each histogram plot. The green one is a random example, where the influence from all individual training examples are close to zero. (b) The joint distribution of the memorization score of each training example and its maximum influence on any validation set example. The histograms are in log scale to better visualize the tail of the distributions. C4 shown in Figure 6.

[MISSING_PAGE_FAIL:9]

dataset. Comparing with the train-validation influence in Figure 3(b), the histogram (c.f. Figure 10 in Appendix.) decays faster as max-influence grows. Moreover, the value range of max-influence is also twice smaller. The reason that we did not find a lot of highly influenced generated examples are two fold: 1) there are only 24,576 generation in the public release, which is much fewer than the validation examples. As a result, the corresponding example of many memorized training examples do not get sampled in the generations. For comparison, previous work (Carlini et al., 2020, Lee et al., 2021) generated 100,000+ examples to identify memorization in generation. These approaches also count duplicates in the training set, which counterfactual memorization filters out. 2) The Grover model was trained on the full RealNews training set, while we have restricted our analysis to the first 2M training examples. There could be potentially more high influence training examples that are missed in our calculation.

## 7 Summary and Discussion

We studied memorization in neural language models. We formulated a notion of _counterfactual memorization_ as a tool that can systematically ignore "common" memorization such as general knowledge (e.g. "Paris is a city in France") and captures memorization of rare, specific information (e.g. description of a specific episode of event) present in the training examples. We conducted experiments on three commonly used text corpus in language modeling and found memorization in all of them. We further analyze the per-domain memorization profiles for Internet-crawled data, and found that different sources could have substantially different memorization profiles.

Furthermore, we analyzed how memorized training examples could impact the model predictions at test time via _counterfactual influence_. We found that for examples from both the validation set and the model generated texts, the model predictions could be drastically different depending on the presence or absence of a particular training example with high memorization.

Limitations.This study mainly focus on English datasets. While we expect the characterization of memorization would be similar when evaluated on corpus of other (natural) languages, new patterns might be observed on multilingual data or more structured domains such as programming languages.

Both the neural language models and training sets used in this work are orders of magnitude smaller than modern standards such as GPT-3 (Brown et al., 2020), GPT-4 (OpenAI, 2023) and PaLM-2 (Google, 2023). Moreover, we only conducted preliminary investigation of the dynamics of counterfactual memorization during training. Although our experiments effectively estimated and detected memorization, we suspect more interesting examples might emerge if larger, more capable models are analyzed. For example, currently when the information from a memorized training example is leaked in the prediction of a strongly influenced test example, it can usually be explained by a high text overlap between the training and test examples. For models with deeper understanding of languages, we suspect that strong influence could be observed even between documents that have no direct text overlap but that encode similar semantic information.

In order to test this, it will be necessary to scale our framework to larger models and datasets. Moreover, it will be necessary to construct datasets where semantically similar but textually different document pairs exist. One potential source to construct such datasets would be _versioned_ Wikipedia articles-two versions of the same article with large time span or edit distance may contain semantically similar (but paraphrased) information. Such a dataset of paraphrased text pairs would be more broadly useful to understand the ability of different models to disentangle text content and form--by measuring the influence of one piece of text on a paraphrased piece of text.

Counterfactual memorization enables us to identify examples that whose presence or absence has a large impact on the model and the model's ability to score and generate other text. The privacy risk for this is low since in order to perform this analysis, one would need to already have access to the dataset and the ability to train models.

Acknowledgments.The authors would like to thank Samy Bengio, Christopher A. Choquette-Choo, Ethan Dyer, Michael C. Mozer, Behnam Neyshabur, Andrew Nystrom, and Hanie Sedghi for constructive discussions and feedback. The authors would like to thank Andrew Nystrom for assistance with MinHash-based near-duplicate detection.