# TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy

Weichao Zhao\({}^{1,2,,}\)  Hao Feng\({}^{1,}\)  Qi Liu\({}^{2,*}\)  Jingqun Tang\({}^{2}\)  Shu Wei\({}^{2}\)  Binghong Wu\({}^{2}\)

Lei Liao\({}^{2}\)  Yongjie Ye\({}^{2}\)  Hao Liu\({}^{2,,}\)  Wengang Zhou\({}^{1,}\)  Houqiang Li\({}^{1}\)  Can Huang\({}^{2}\)

\({}^{1}\) University of Science and Technology of China, \({}^{2}\) ByteDance

{saruka, haof}@mail.ustc.edu.cn, {zhwg, lihq}@ustc.edu.cn

{liuqi.nero, haoliu.0128, can.huang}@bytedance.com

Equal contribution. \(\) Interns at ByteDance. \(\) Project lead.Corresponding authors: Wengang Zhou and Hao Liu.

###### Abstract

Tables contain factual and quantitative data accompanied by various structures and contents that pose challenges for machine comprehension. Previous methods generally design task-specific architectures and objectives for individual tasks, resulting in modal isolation and intricate workflows. In this paper, we present a novel large vision-language model, TabPedia, equipped with a _concept synergy_ mechanism. In this mechanism, all the involved diverse visual table understanding (VTU) tasks and multi-source visual embeddings are abstracted as concepts. This unified framework allows TabPedia to seamlessly integrate VTU tasks, such as table detection, table structure recognition, table querying, and table question answering, by leveraging the capabilities of large language models (LLMs). Moreover, the concept synergy mechanism enables table perception-related and comprehension-related tasks to work in harmony, as they can effectively leverage the needed clues from the corresponding source perception embeddings. Furthermore, to better evaluate the VTU task in real-world scenarios, we establish a new and comprehensive table VQA benchmark, ComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and qualitative experiments on both table perception and comprehension tasks, conducted across various public benchmarks, validate the effectiveness of our TabPedia. The superior performance further confirms the feasibility of using LLMs for understanding visual tables when all concepts work in synergy. The benchmark ComTQA has been open-sourced at [https://huggingface.co/datasets/ByteDance/ComTQA](https://huggingface.co/datasets/ByteDance/ComTQA). The source code and model also have been released at [https://github.com/zhaowc-ustc/TabPedia](https://github.com/zhaowc-ustc/TabPedia).

## 1 Introduction

With the rapid advancement of digital technology, numerous paper documents must be converted into electronic formats for efficient storage and utilization. Tables, as indispensable components of documents, play a vital role in summarizing facts and quantitative data . The compact yet informative nature of tables makes them advantageous for various applications, thereby attracting widespread research attention toward Visual Table Understanding (VTU). VTU generally encompasses four subtasks: _Table Detection_ (TD), which locates tables within document images; _Table Structure Recognition_ (TSR), which parses the structure of tables in table-centric images; _Table Querying_ (TQ), which recognizes the structure of a table from an entire image at a given location, a task that remains underexplored in the previous works; and _Table Question Answering_ (TQA), whichanswers questions based on table contents. These tasks pose challenges from various perspectives due to the need for representations at different visual-semantic granularities and hierarchies.

Given the success achieved, many pioneering works have mainly centered on the specific subtask with various task-specific architectures, as shown in Fig. 1 (a). For visual table perception tasks such as TD and TSR, one of most adopted approaches is in the detection manner . In contrast, generative vision-language models  are often employed to generate answers conditioned on the semantic content of tables for TQA task. Specifically, Vision Transformers (ViT)  pretrained on CLIP  or EVA-CLIP , Swin-Transformer , and similar models serve as vision encoders, while language models operate in either encoder-decoder  or decoder-only frameworks . Besides, recent fast-growing Large Vision Language Models (LVLMs)  have shown their powerful capabilities to perceive and understand visual clues by integrating instruction following of Large Language Models (LLMs) . Despite impressive progress, the _status quo_ begs for a question: "_Can we leverage the advantages of LVLMs to solve all the VTU tasks once and for all?_"

A straightforward solution would be to train the LVLM directly using all the VTU data. However, aside from the diverse table structure and the various relations of table contents, it remains a nontrivial issue due to two cruxes of table parsing and understanding: (i) discrepancy between the representation formats (two-dimensional structure VS. one-dimensional sequence); (ii) required image resolutions. Although some works  represent table structure in markup formats like HTML, XML, Markdown, or LATEX. However, they neglect spatial coordinates for cells and only encode logical relationships implicitly. The generated code contains extensive formatted information from different markup languages, increasing output length and potentially causing parsing issues with illegal grammars.

To attack above issues, we in this paper propose a novel LVLM tailored for comprehensive VTU, TabPedia, to effectively solve all VTU tasks in a unified framework, as shown in Fig. 1 (b). More concretely, we employ dual vision encoders, namely ViT-L  and Swin-B , to encode the global and fine-grained local information in the low- and high-resolution formats of the input image respectively, acquiring multi-source visual embeddings. Here, all the involved VTU tasks and multi-source visual embeddings are abstracted as _concepts_ and _concept synergy_ mechanism is implemented by introducing the _mediative tokens_ to the LLM in our model. Thanks to this mechanism, all the concepts in TabPedia can work in synergy flexibly. Quantitative and qualitative experimental results on both table perception and comprehension tasks across various public benchmarks confirm the effectiveness of our proposed TabPedia. To further investigate the potential of our model in more challenging and realistic scenarios, we establish a new and comprehensive table VQA benchmark, ComTQA, featuring round 1,500 images and 9,000 QA pairs.

Our contributions are summarized as follows,

* We propose a novel large vision-language model, TabPedia, to integrate various VTU tasks into a unified framework, including TD, TSR, TQ and TQA. Specifically, TabPedia fully leverages the comprehensive capabilities of LLMs to fertilize complex table understanding.
* We design a concept synergy mechanism to harmonize both table perception and comprehension tasks. Through introducing the meditative tokens into our framework, TabPedia

Figure 1: Comparison with previous task-specific pipelines for visual table understanding. In contrast to design different architectures for various table tasks, our TabPedia effectively performs these tasks in a unified framework through delicately leveraging the understanding capability of LLMs.

[MISSING_PAGE_FAIL:3]

inherit this spirit and design meditative tokens to enhance TabPedia's perceptive and comprehensive capability for visual tables.

## 3 Method

As shown in Fig 2, we present an overview of TabPedia. The overall training pipeline consists of two phases. Concretely, the pre-training stage aims to align the visual features to the large language model, and the fine-tuning stage focuses on visual table-aware understanding. In the following, we elaborate on the architecture of TabPedia, followed by the exposition of its two training phases.

### Model Architecture

**High-Resolution Vision Encoder.** As proved by previous methods [43; 82; 83], the high-resolution image is critical to ensuring that the LLMs could grasp rich visual information. Following Donut , we adopt Swin-B  to encode the high-resolution format of input image. Given the input RGB image \(I\), we first resize it to pre-defined high-resolution scale of \(\), denoted as \(I_{h}\). By default, both \(\) and \(\) are set to 2,560 and 1,920, respectively. Notably, we maintain the aspect ratio during the resizing process to prevent distortion of table contents and structures. Then, the resized image \(I_{h}\) is fed into the vanilla Swin Transformer initialized from  to obtain a feature map \(V_{h}\) downsampled by a factor of 1/32, each token with 1,024 dimension.

**Low-Resolution Vision Encoder.** To keep the overall layout information, the raw image is also resized to a low-resolution one denoted as \(I_{l}\). We choose the pre-trained CLIP visual encoder ViT-L/14  to encode the low-resolution image with \(224 224\), which has been pre-trained on 400 million image-text pairs sourced from the open-world data, thereby embedding extensive world knowledge into its pretrained weights. To preserve its generalization ability, we keep it frozen during the whole training procedure. The output sequence \(V_{l}\) is composed of 256 tokens, each with 1024 dimension.

**Projections.** The projections are designed to align visual tokens with the input token dimension of the subsequent large language model . For the high-resolution feature map \(V_{h}\), due to the limitation of input text length, we employ a 2D convolutional layer with a kernel size of 3 and a stride of 2, and then flatten it into \(}{64}}{64}\) tokens, denoted as \(_{h}\). For the low-resolution visual features \(V_{l}\), inspired from the paradigm of advanced LVLMs [29; 30], we adopt a linear layer to project visual tokens, denoted as \(_{l}\).

**Concept Synergy.** Given the massive visual tokens and the embedding of textual instruction \(\), we utilize Vicuna-7B  as LLM to generate its response. Taking into account the discrepancy of table perception and comprehension tasks, we introduce _meditative tokens_\(\) to implement the concept synergy for the LLM, which adaptively enable different region of visual tokens and understand the intentions of specific task question. Finally, we construct the whole input sequence as \(X\) =

Figure 2: The illustration of our proposed TabPedia. Given the input image, TabPedia feeds it into both vision encoders attached projections to extract different granular features. Then, the visual tokens are combined with instruction-derived tokens, and fed into the LLM. The LLM leverages its powerful understanding ability to generate a plausible response.

[Q, <IMG_S> ; \(_{l}\) ; <IMG_SEP> ; \(_{h}\) ; <IMG_E> ; \(\)], where \([;]\) means the concatenation operation. <IMG_S>, <IMG_E> and <IMG_SEP> are learnable special tokens, that denote the start and end of visual tokens as well as the separation of different resolution tokens, respectively.

**Objective.** Since TabPedia is trained to predict the next tokens like other LLMs, it is optimized by maximizing the likelihood of prediction loss at training time.

### Pre-training

To enable the capable of vision encoders to capture text-rich information from high-resolution images and aligning embedding space with the large language model , we first perform extensive text-aware pre-training. As shown in Fig. 2, we jointly optimize the high-resolution visual encoder with both projectors, while freezing the large language model and low-resolution vision encoder. Specifically, followed by , our pre-training procedure involves a variety of perception tasks, _i.e._, text detection , recognition , spotting , long-text reading  and image captioning . The first four tasks focuses on the various document images, while the last one targets natural scene images. These comprehensive tasks endow the vision encoders of TabPedia to effectively perceive textual and visual information from both document and natural scene images. More detailed pre-training settings about dataset and experiment could be referred to .

### Table-aware Fine-tuning

Through pre-training, TabPedia could well understand text and structure of diverse document images but cannot follow instructions to perform different table understanding tasks. In order to enhance the model capability of instruction following, we _first_ construct a large-scale dataset for visual table understanding. We will elaborate on the dataset construction in the Sec. 4. Based on this dataset, we introduce four table-related tasks, _i.e._, TD , TSR [5; 9; 65], TQ and TQA [5; 9; 88; 89] to simultaneously cultivate the perception and comprehension capabilities. In this stage, we further unfreeze the LLM and fine-tune the entire framework except the low-resolution vision encoder.

## 4 Dataset Construction

In this section, we aim to introduce the collected instruction following dataset. The entire data is derived from five public datasets, including PubTab1M , FinTabNet , PubTabNet , WikiTableQuestions (WTQ)  and TabFact . Among them, PubTab1M  contains two subsets, _i.e._, PubTab1M-Detection (PubTab1M-Det) and PubTab1M-Structure (PubTab1M-Str). Moreover, since the table images in PubTab1M-Str are cropped from PubTab1M-Det, we transform the annotations of the table structure in PubTab1M-Str into the original images and synthesize a new subset PubTab1M-Syn, which could be utilized for TQ task. The statistical data are summarized in Tab. 1. To ensure the instruction diversity, we generate multiple instructions for each task using GPT3.5 . In Tab. 2, we display one exemplar about user's question for each table task. We will provide a detailed exposition of them in the following.

**Table Detection (TD).** As a fundamental task, TD task targets to detect all table locations in a document image. Previous methods [3; 6; 9] mainly utilize DETR  or variants of R-CNN [90; 91; 92] to predict numerous overlapping bboxes, that inevitably needs complex post-processing, such as non-maximization suppression (NMS), to generate final results. In contrast, we employ LLM to directly generate the locations of instance tables in the format of "[x1, y1, x2, y2]", where x1, y1, x2,

  
**Dataset** & **Subset** & **Task** & **Num** \\   & PubTab1M-Det & TD & 460k \\  & PubTab1M-Str & TSR,TQA & 759k \\  & PubTab1M-Syn & TQ & 381k \\  FinTabNet & – & TSR,TQA & 78k \\  PubTabNet & – & TSR & 434k \\  WTQ & – & TQA & 1k \\  TabFact & – & TQA & 9k \\   

Table 1: Summary of training data statistics in the fine-tuning stage.

  
**Task** & **Example** \\  TD & “Give me the areas where table element’s locations in this picture.” \\  TSR & “Parse the structural information of the cropped table in this picture.” \\  TQ & “Parse the table structure within the region \\  & [0.095, 0.673, 0.869, 0.851] in this picture.” \\  TQA & “What was the lowest stock price in the fourth quarter of 2010” \\   

Table 2: Different task types and their instruction examples.

y2 represent the normalized coordinates of the top-left and bottom-right of the corresponding bbox. Moreover, to facilitate detection results for multiple tables, we split multiple table positions with the special symbol "n" in the output response. We adopt PubTab1M-Det  to perform TD task, where images are collected from PDF documents with different scale and rotation types of tables.

**Table Structure Recognition (TSR).** The TSR targets to parse table structure in terms of rows, columns and cells. HTML and Markdown codes are mainly two kinds of text sequences used to represent a table. HTML could represent all kinds of tables, with or without cells spanning multiple rows and grids, but they contain massive markup grammars _i.e._, "<divx>/div>" and "<td>/td>", resulting in excessively lengthy output responses. Compared with HTML, Markdown represents a table more succinctly, but it cannot represent cells spanning multiple rows or columns. By weighing the simplicity of the output and the completeness of the table parsing, we propose a canonical table structure representation based on the detection format. Inspired by , we jointly adopt five object classes to model TSR, including _table column_, _table row_, _table column header_, _table projected row header_ and _table spanning cell_. To better understanding, we display a representative sample in Appendix B. Taking into account the serialized output of the LLM, we represent the table structure with a series of "[object] [x1, y1, x2, y2]", which are also separated by "u". Notably, we standardize the order of the output objects to ensure uniqueness of the table parsing results.

We select the PubTab1M-Str , FinTabNet  and PubTabNet  to support the TSR task, where tables are collected from scientific and financial articles. These datasets contain pairs of table images and HTML annotations. We convert HTML codes into our designed annotation format using the pre-processing tool offered by .

**Table Querying (TQ).** Different from recognizing table structure from the cropped table-centric images in TSR task, the TQ task directly parses the table from the original document image based on the given table location. This task is more challenging due to the degradation of the table's resolution and the interference of other document contents around it. Moreover, this task could potentially be combined with TD task to enable automatic parsing of all table structure information in original images. Therefore, we introduce this task to fully unlock the comprehension capabilities of large language models for visual table understanding. For the annotation of table parsing, we adopt the same format as TSR. Since there is no readily available dataset, we synthesize a large amount of available data based on the annotations from PubTab1M , namely PubTab1M-Syn.

**Table Question Answering (TQA).** TQA aims to provide precise answers through table understanding and reasoning. For both public TQA datasets, _i.e._, WTQ  and TabFact , the table images are collected from wikipedia tables with pairs of content-related question and answer. Thus, we could directly apply these available data to support this task. However, the images of current TQA data are rendered from text-based tables with variations in background color and font size, resulting in poor generalization in real-world tables. In addition, the TQA data volume lags far behind other tasks. To alleviate these obstacles, we generate numerous TQA data with partial images in FinTabNet  and PubTab1M  by employing the powerful multi-modal understanding capabilities of Gemini Pro . We provide more detailed descriptions of the procedure in the Appendix A.1

To better evaluate TQA performance of various models on real-world table images, we build a complex TQA dataset (ComTQA) based on test set of FinTabNet  and PubTab1M . Compared to WTQ and TabFact, ComTQA has more challenging questions, such as multiple answers, mathematical calculations, and logical reasoning. In total, we annotate \(\)9k high-quality QA pairs from \(\)1.5k images by expert annotation. More statistics about ComTQA could be found in the Appendix A.2.

## 5 Experiment

### Implementation Details

**Parameter Settings.** For the hyper-parameters in model design, the number of meditative tokens is set to 256. The max length of text sequence is set to 4000 to satisfy task requirements. To implement TabPedia, we adopt a cosine schedule with one-cycle learning rate strategy . In the pre-training phase, the learning rate warms up in the first 2% of the training process and then decreases from the peak rate (1e-3) with batch sizes of 64. In the fine-tuning phase, we set the peak learning rate as 5e-6 with batch sizes of 16. We employ the AdamW optimizer  in both phases. All experiments are implemented by PyTorch  and trained on 16\(\) A100 GPUs.

**Datasets.** In order to comprehensively evaluate the capability of TabPedia, we employ multiple benchmarks for each task. For performance assessment, we set the temperature parameter as 0.2 in both quantitative and qualitative evaluations. For TD task, PubTab1M-Det  contains 57,125 images for testing. For TSR task, FinTabNet , PubTabNet  and PubTab1M-Str  are adopted for evaluation with 9,289, 9,115 and 93,834 testing samples, respectively. For TQ task, the synthetic dataset PubTab1M-Syn  also provides 47,186 samples for testing. For TQA task, WTQ , TabFact  and our annotated ComTQA contain 4,343, 12,722 and 9,070 QA pairs, respectively.

**Evaluation Metrics.** For TD task, we report the results with object detection metrics, including precision, recall and f1-score with IoU@0.75. For both TSR and TQ tasks, we utilize Structure Tree-EditDistance-based Similarity (S-TEDS) , which evaluates table similarity of structural aspects in HTML format. The metric represents the HTML table as a tree, and the TEDS score is computed through the tree-edit distance between the ground truth and predicted trees. In order to convert the results of TabPedia into HTML format, we employ the post-processing algorithm provided by . Moreover, we report the recently proposed GriTS metrics  for PubTab1M-Str to align its original metric. Different from S-TEDS, GriTS represents tables as matrices, better capturing the two-dimensional structure and the orders of cells in a table. Further, GriTS enables TSR to be assessed from multiple perspectives, with \(_{}\) measuring cell topology recognition, \(_{}\) measuring cell content recognition, and \(_{}\) measuring cell location recognition. For TQA task, we adopt the accuracy metric where the response generated by the model is judged correct if it contains the string present in the ground truth .

### Quantitative Results

We conduct quantitative evaluations of current state-of-the-art methods for specific tasks in perception and comprehension, comparing them to our proposed TabPedia.

**Evaluation on TD.** In Tab. 3, we compare TabPedia with the previous state-of-the-art method, TATR . TATR performs the table detection with two classic visual detection backbones, _i.e,_ DETR  and Faster R-CNN . Compared with them, TabPedia outperforms Faster R-CNN with a notable margin and achieves competitive performance with DETR. Notably, since TabPedia directly generates the independent locations of instance tables without densely overlapped bboxes,

    &  &  &  &  \\   & & & **Acc** & **Acc** & **Acc** \\  TextMonkey  & 896 & 37.9 & 53.6 & 13.9\({}^{}\) \\ Module  & 896 & 25.3\({}^{}\) & 49.8 & \({}^{}\) & – \\ Coagger  & 1,120 & 30.2\({}^{}\) & 51.7\({}^{}\) & – \\ DocCall  & 1,344 & 39.8 & **80.4** & 18.5\({}^{}\) \\ OFFIV  & 645 & 45.5\({}^{}\) & 69.3\({}^{}\) & 27.2\({}^{}\) \\ Gemini Duo  & 659 & 32.3\({}^{}\) & 67.9\({}^{}\) & 29.3\({}^{}\) \\ Koompeso2  & 511 & 28.7 & 62.3 & – \\ 
**TabPedia** & 2,560 & **47.8** & 71.3 & **58.5** \\   

Table 6: Comparison with existing LVLMs on TQA task. “\(*\)” denotes the results obtained through the open-source checkpoint or API of the closed-source model. ComTQA is our released new benchmark. The second best methods are underlined.

    &  &  &  \\   & & & **Precision** & **Recall** & **F1** \\   & Faster R-CNN & & 92.7 & 86.6 & 89.5 \\  & DETR & & **98.8** & 98.1 & **98.4** \\ 
**TabPedia** & LVLM & & 98.5 & **98.4** & **98.4** \\   

Table 3: Comparison with the existing best table detection model TATR . NMS denotes Non-Maximum Suppression.

    &  &  &  &  \\   & & & **Acc** & **Acc** & **Acc** \\  TextMonkey  & 896 & 37.9 & 53.6 & 13.9\({}^{}\) \\ Module  & 896 & 25.3\({}^{}\) & 49.8 & – \\ Coagger  & 1,120 & 30.2\({}^{}\) & 51.7\({}^{}\) & – \\ DocCall  & 1,344 & 39.8 & **80.4** & 18.5\({}^{}\) \\ OFFIV  & 645 & 45.5\({}^{}\) & 69.3\({}^{}\) & 27.2\({}^{}\) \\ Gemini Duo  & 659 & 32.3\({}^{}\) & 67.9\({}^{}\) & 29.3\({}^{}\) \\ Koompeso2  & 511 & 28.7 & 62.3 & – \\ 
**TabPedia** & 2,560 & **47.8** & 71.3 & **58.5** \\   

Table 4: Comparison with end-to-end and TSR methods on two datasets. “\(*\)” represents the results reported by .

there are no extra post-processing operations involved, _i.e._, Non-Maximum Suppression (NMS). This advantage could enable TabPedia to perform more complex table understanding, such as parsing all tables by combining TD and TQ tasks.

**Evaluation on TSR.** Tab. 4 reports the performance of TSR task compared to end-to-end TSR models on PubTabNet and FinTabNet datasets. Specifically, the OCR-free model Donut  is fine-tuned for TSR with the official default training configuration. Although OmniParser  integrates multiple visually-situated text parsing tasks into a unified framework, it adopts three isolated decoders to perform different tasks. Compared with OmniParser, TabPedia consistently surpasses it with 4.96% and 3.56% S-TEDS on both datasets, respectively. In Tab. 4(a), TATR as the task-specific method, shows high performance with the DETR architecture. Our proposed TabPedia, a generic model for tasks involving both perception and comprehension, still achieves comparable performance without the need for complex post-processing. These results highlight the exceptional capability of TabPedia.

**Evaluation on TQ.** As a new and unexplored task, the TQ task aims to parse table structures with the specific location directly from the raw image without additional cropping. In the first row of Tab. 4(b), we provide a strong baseline with 96.04% and 95.07% on \(}\) and S-TEDS, respectively, which nearly reaches the same performance as parsing from the cropped images under the interference of the document content around the table. Furthermore, we integrate both TD and TQ tasks in the form of multi-round dialogue, which endows TabPedia to directly parse all existing tables in a document image. We report the final result in the second row of Tab. 4(b). These impressive results demonstrate that TabPedia has the potential to enable more holistic table understanding.

**Evaluation on TQA.** Due to the complex structure of tables and the dense text, the understanding of the table contents remains a challenging issue. To thoroughly evaluate the performance of the understanding of table content and structure, we adopt two public benchmarks, _i.e._, WTQ  and TabFact , and our collected dataset ComTQA, as shown in Tab. 6. On the WTQ and TabFact, TabPedia achieves promising performance among the open and close sources LVLMs. In contrast to existing benchmarks, ComTQA contains real-world table images with more complex questions. It is observed that current LVLMs show poor performance due to the incomplete understanding of real-world table structures. Compared with them, TabPedia achieves the optimal result with a notable margin, which demonstrates the effectiveness of jointly learning perception and comprehension tasks.

### Qualitative Results

We further conduct qualitative evaluation on TabPedia's perception and comprehension capabilities. Firstly, we show the perception capability of TabPedia with solely TD and TSR tasks, as illustrated in the first row of Fig. 3. TabPedia accurately generates reliable and formatted results, which are rendered to the original image for better observation. Secondly, TabPedia performs a complex task to directly parsing all table structure information in a document image by integrating instructions of TD and TQ tasks within a multi-round dialogue. As shown in the second row of Fig. 3, the example indicates that TabPedia is capable of exploring more holistic visual table understanding. In the last row, we display the table comprehensive capability of TabPedia. It is observed that the response not only contains concise and reliable answer, but also provides the specific contents in the table to support its answer. Especially, TabPedia even acquires certain math calculation ability to capture the connections among table contents, as shown in the bottom right example in Fig. 3. These results demonstrate Tabpedia's powerful multimodal comprehension capabilities. We also display more visualization results in the Appendix D.

### Ablation Studies

In this section, we conduct ablation studies to validate the effectiveness of core settings and components in TabPedia. All experiments are conducted on three datasets across three tasks: PubTab1M-Det , FinTabNet  and WTQ .

**Necessity of Mediatative Tokens.** In Tab. 8, we conduct the experiment to investigate the impact of adding meditative tokens in TabPedia. It is observed that adding meditative tokens significantly improves TabPedia's capabilities of table perception and comprehension.

**What Information Matters for Mediatative Tokens?** We sample 100 test cases for each task and report the averaged numeric importance of high- and low-resolution vision tokens when they are attended by the meditative tokens for different tasks in the Tab. 9. Specifically, for the various VTUtasks, we calculate the averaged attention scores (across all layers and attention heads) from the LLM decoder, which indicates the extent to which the meditative tokens focus on either high- or low-resolution visual tokens. For the TSR and TQ tasks, the meditative tokens pay significantly more attention to the high-resolution visual encoder tokens. We attribute this to the fact that both tasks require more fine-grained visual information to be "deliberated" in order to construct the dense table structure. In contrast, for the TD and TQA tasks, the two visual encoders contribute almost equally to the information attended to by the meditative tokens, validating the importance of both vision encoders for these tasks.

**Contributions of Different Tokens.** In the Tab. 7, we calculate the averaged scores of the TabPedia-generated answers with respect to meditative tokens, high-resolution visual tokens, and low-resolution visual tokens across all the attention maps from the LLM, respectively. One can observe that the meditative tokens contribute the most information to the generation of satisfactory answers, which demonstrates that the proposed meditative tokens are indispensable and effective. We also provide a detailed analysis of the attention map of meditative tokens in Fig. D4 of Appendix. D.

**Impact of Dual Vision Encoders.** As shown in Table 11, we explore the impact of different vision encoders that capture global and local information from input images at various resolutions. The high-resolution encoder extracts intricate details from text-rich images, outperforming the low-resolution encoder, which struggles with nuanced visual representations in complex document images. Different

   Task &  Meditative \\ tokens \\  &  High-res \\ visual tokens \\  & 
 Low-res \\ visual tokens \\  \\  TD & 0.65 & 0.16 & 0.19 \\ TSR & 0.64 & 0.12 & 0.24 \\ TQ & 0.71 & 0.11 & 0.19 \\ TQA & 0.56 & 0.18 & 0.25 \\   

Table 7: Contributions of different tokens.

Figure 3: Qualitative results of TabPedia on diverse tasks. The first row shows its perception capability on both TD and TSR tasks. The second row further exhibits TabPedia’s powerful ability by employing multiple instructions of different tasks. The bottom row showcases TabPedia’s accurate responses based on intricate contents in visual tables. Zoom in for best view.

tasks may require distinct visual cues, so dual vision encoders offer flexibility. For instance, TQA tasks need detailed table information, while TSR tasks depend on global layout. The low-resolution encoder provides comprehensive layout insights, complementing the high-resolution encoder's limited receptive field. Our results demonstrate that combining both encoders enhances the extraction of structural and content-related details from tables, improving perception and comprehension tasks.

**Frozen vs. Unfrozen Low-Resolution Vision Encoder.** We further investigate different training strategies in terms of the low-resolution vision encoder. As shown in Tab. 10, it is observed that no significant performance improvement but with longer training time consumption by unfreezing it, which is in line with the conclusion in the pioneering work . Besides, we suppose the encoder frozen can serve as a regularization, facilitating the extraction of layout information and alleviating potential overfitting problems, as well as more stable training. To strike the trade-off between computational consumption and performance, we thus freeze the low-resolution vision encoder during training.

## 6 Limitation

In this section, we discuss the limitations of our TabPedia. Firstly, since we represent the table structure with regular rectangular boxes, TabPedia is currently not capable of accurately parsing structural information for twisted or distorted tables. Secondly, all images in TQA datasets, including WTQ , TabFact  and ComTQA are dominated by tables. Therefore, TabPedia still lacks the capability to directly answer the table question with original document image. In addition, compared to parallel decoding algorithms such as DETR  and Faster R-CNN , it consumes longer decoding time. Meantime, certain algorithmic designs such as KV cache, flash attention, and hardware improvements can effectively improve inference efficiency. We believe that with the iterative development of large model technology, the inference efficiency of TabPedia can be significantly improved.

## 7 Conclusion

In this paper, we propose a novel large vision-language model to unify diverse visual table understanding tasks, namely TabPedia. Specifically, we present a _concept synergy_ mechanism to seamlessly integrate diverse tasks and multi-source visual tokens embedded from dual vision encoders as _concepts_. This mechanism is implemented by introducing the _meditative tokens_ into the LLM. Then, we fully leverage the capability of LLMs to effectively understand these concepts and generate accurate and plausible responses. Extensive quantitative and qualitative experiments across various public benchmarks validate the effectiveness of our TabPedia. To further investigate the potential of TabPedia, we establish a challenging table VQA dataset, ComTQA, featuring round 9,000 QA pairs.

    Low-Res \\ Encoder \\  } & **PubTab1M-Det** & **FinTabNet** & **WTQ** \\   & **Precision** & **S-TEDS** & **Acc** \\  frozen & **98.5** & 95.11 & **47.8** \\ unfrozen & 98.4 & **95.11** & 46.4 \\   

Table 10: Impact of different training strategies on low-resolution vision encoder.

    **Task** \\  } & **High-res visual tokens** & **Low-res visual tokens** \\   & **TQ** & 0.49 & 0.51 \\   & **TQ** & 0.71 & 0.29 \\   & 0.73 & 0.27 \\   & 0.51 & 0.49 \\   

Table 9: Contribution of different visual tokens from dual vision encoders.

    **modular** \\ 
 **token** \\  \\  } & **PubTab1M-Det** & **FinTabNet** & **WTQ** \\   & **Precision** & **S-TEDS** & **Acc** \\   \(\) & 93.5 & 92.17 & 43.2 \\ \(\) & **98.5** & **95.11** & **47.8** \\   

Table 8: Impact of meditative tokens in TabPedia.