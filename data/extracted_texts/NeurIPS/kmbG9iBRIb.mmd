# Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples

Hao Sun, Alihan Huyuk, Daniel Jarrett, Mihaela van der Schaar

Department of Applied Mathematics and Theoretical Physics

University of Cambridge

hs789@cam.ac.uk

###### Abstract

Learning controllers with offline data in decision-making systems is an essential area of research due to its potential to reduce the risk of applications in real-world systems. However, in responsibility-sensitive settings such as healthcare, decision accountability is of paramount importance, yet has not been adequately addressed by the literature. This paper introduces the Accountable Offline Controller (AOC) that employs the offline dataset as the Decision Corpus and performs accountable control based on a tailored selection of examples, referred to as the Corpus Subset. AOC operates effectively in low-data scenarios, can be extended to the strictly offline imitation setting, and displays qualities of both conservation and adaptability. We assess AOC's performance in both simulated and real-world healthcare scenarios, emphasizing its capability to manage offline control tasks with high levels of performance while maintaining accountability.

## 1 Introduction

In recent years, offline control that uses pre-collected data to generate control policies has gained attention due to its potential to reduce the costs and risks associated with applying control algorithms in real-world systems , which are especially advantageous in situations where real-time feedback is challenging or expensive to obtain [2; 3; 4; 5]. However, existing literature has primarily focused on enhancing learning performance, leaving the need for accountable and reliable control policies in offline settings largely unaddressed, particularly for high-stakes, responsibility-sensitive applications.

However, in many critical real-world applications such as healthcare, the challenge is beyond enhancing policy performance. It requires the decisions made by learned policies to be transparent, traceable, and justifiable. Yet those essential properties, summarized as Accountability, are left largely unaddressed by existing literature.

In our context, we use _Accountability_ to indicate the existence of _a supportive basis for decision-making_. For instance, in tumor treatment using high-risk options like radiotherapy and chemotherapy, the treatment decisions should be based on the successful outcomes experienced by previous patients who share similar conditions and were given the same medication. Another concrete illustrative example is the allocation decisions of ventilator machines. The decision to allocate a ventilator should be accountable, in the way that it juxtaposes the potential consequences of both utilization and non-utilization and provides a reasonable decision on those bases. In those examples, the ability to refer to existing cases that support current decisions can enhance reliability and facilitate reasoning or debugging of the policy. To advance offline control towards real-world responsibility-sensitive applications, five properties are desirable: **(P1)** controllable **conservation**: this ensures the policy learning performance by avoiding aggressive extrapolation. **(P2) accountability**: as underscored by our prior examples, there is a need for a clear basis upon which decisions are made. **(P3)** suitabilityfor **low-data** regimes: given the frequent scarcity of high-stake decision data, it's essential to have methods that perform well with limited data. **(P4) adaptability** to user specification: this ensures the policy can adjust to changes, like evolving clinical guidelines, allowing for tailored solutions. **(P5) flexibility** in Strictly Offline Imitation: this property ensures a broader applicability across various scenarios and data availability.

To embody all these properties, we need to venture beyond the current scope of literature focused on conservative offline learning. In our work:

1. Methodologically, we introduce the formal definitions and necessary concepts in accountable decision-making. We propose the Accountable Offline Controller (AOC), which makes decisions according to a decomposition on the basis of the representative existing decision examples.
2. Theoretically, we prove the existence and uniqueness of the decomposition under mild conditions, guiding the design of our algorithms.
3. Practically, we introduce an efficient algorithm that takes all the aforementioned desired properties into consideration and circumvented the computational difficulty.
4. Empirically, we verify and highlight the desired properties of AOC on a variety of offline control tasks, including five simulated continuous control tasks and one real-world healthcare dataset.

## 2 Preliminaries

PomdpWe develop our work under the general Partially Observable Markov Decision Process (POMDP) setting, denoted as a tuple \((,,,,,,, _{0})\), where \(^{d_{x}}\) denotes the underlying \(d_{x}\) dimensional state space, \(:\) is the emission function that maps the underlying state into a \(d_{o}\) dimensional observation \(^{d_{o}}\); \(^{d_{a}}\) is a \(d_{a}\) dimensional action space, transition dynamics \(:\) controls the underlying transition between states given actions; and the reward function \(:\) maps state into a reward scalar; we use \(\) to denote the discount factor and \(_{0}\) the initial state distribution. Note that the MDP is the case when \(\) is an identical mapping.

Offline ControlSpecifically, our work considers the offline learning problem: a logged dataset \(=\{o_{t}^{i},a_{t}^{i},r_{t}^{i},o_{t+1}^{i}\}_{t=1,...,T}^{i=1,...,N}\) containing \(N\) trajectories with length \(T\) is collected by rolling out some behavior policies in the POMDP, where \(x_{0}^{i}_{0}\) sampled from the initial state distribution and \(o_{0}^{i}=(x_{0}^{i})\), \(a_{t}^{i}_{b}^{i}\) sampled from unknown behavior policies \(_{b}^{i}\). We denote the observational transition history until time \(t\) as \(h_{t}\), such that \(h_{t}=(o_{<t},a_{<t},r_{<t})^{(d_{o}+d_{a}+ 1)(t-1)}\).

Learning ObjectiveA policy \(:\) is a function of observational transition history. The learning objective is to find \(\) that maximizes the expected cumulative return in the POMDP:

\[_{}_{_{0},,}[^{t}r_{t}]\] (1)

where \(_{0},,\) means the trajectories \(\) is generated by sampling initial state \(x_{0}\) from \(_{0}\), and action \(a\) from \(\), such that: \(=\{x_{0},o_{0}=(x_{0}),a_{0}=(o_{0},h_{0}=),x_{1}=( x_{0},a_{0}),r_{0}=(x_{1}),...\}\). A learned belief variable \(b_{t}^{d_{b}}\) can be introduced to represent the underlying state \(x_{t}\).

## 3 Accountable Control with Decision Corpus

### Method Sketch: A Roadmap

The high-level core idea of our work is to introduce an example-based accountable framework for offline decision-making, such that the decision basis can be clear and transparent.

To achieve this, a naive approach would be to leverage the insights of Nearest Neighbors: for each action, this involves finding the most similar transitions in the offline dataset, and estimating the corresponding outcomes. Nonetheless, a pivotal hurdle arises in defining **similarity**, particularly when taking into account the intricate nature of trajectories, given both the observation space heterogeneity and the inherent temporal structure in decision-making. Compounding such a challenge, another difficulty arises in identifying the most **representative** examples and integrating the pivotal principle of **conservation**, which is widely acknowledged to be essential for the success of offline policy learning.

Our proposed method seeks to address those challenges. We start by introducing the basic definitions to support a formal discussion of accountability: in Definition 3.1, we introduce the concept of Decision Corpus.

To address the similarity challenge, we showcase that a nice linear property (Property 3.2) generally exists (Remarks 3.3 & 3.4) when working in the belief space (Definition 3.5). This subsequently leads to a theoretical bound for estimation error (Proposition 3.8);

To address the representative challenge while obeying the principle of conservation, we underscore those examples that span the convex hull (Definition 3.6) and introduce the related optimization objective (Definition 3.7 & 3.9). In a nutshell, the intuition is to use a minimal set of representative training examples to encapsulate test-time decisions. Under mild conditions, we show the solution would exist and be unique (Proposition 3.10).

### Understanding Decision-Making with a Subset of Offline Data

In order to perform accountable decision-making, we construct our approach upon the foundation of the example-based explanation framework [6; 7], which motivates us to define the offline decision dataset \(\) as the _Decision Corpus_, and introduce the concept of _Corpus Subset_ that is composed of a representative subset of the decision corpus. These corpus subsets will be utilized for comprehending control decisions. Formally:

**Definition 3.1** (Corpus Subset).: A _Corpus Subset_\(\) is defined as a subset of the decision corpus \(\), indexed by \([C]:=\{1,2,...,C\}\) -- the natural numbers between \(1\) and \(C\).

\[=\{(o^{c},a^{c},h^{c},v^{c})c[C] \}.\] (2)

In the equation above, the transition tuple \((o^{c},a^{c},h^{c},v^{c})\) can be obtained from the decision corpus. We absorb both superscripts (\(i\)) and subscripts (\(t\)) to the index (\(c\)) for the conciseness of notions. Later in our work, we use subscripts \(t\) to denote the control time step. The variable \(v^{c}\) represents a performance metric that is defined based on the user-specified decision objective. e.g., \(v^{c}\) may correspond to an ongoing cumulative return, immediate reward, or a risk-sensitive measure.

Our objective is to understand the decision-making process for control time rollouts with the decision corpus. To do this, a naive approach is to reconstruct the control time examples with the corpus subset \((o_{t},a_{t},h_{t})=_{c=1}^{C}w^{c}(o^{c},a^{c},h^{c})\), where \(w^{c}\) and \(_{c=1}^{C}w^{c}=1\). Then the corresponding performance measures \(v^{c}\) can be used to estimate \(v_{t}|a_{t}\) -- the control time return of a given decision \(a_{t}\). Nonetheless, there are critical issues associated with this straightforward method:

1. Defining a distance metric for the joint observation-action-history space presents a challenge. This is not only due to the fact that various historical observations are defined over different spaces but also because the Euclidean distance fails to capture the structural information effectively.
2. The conversion from the observation-action-history space to the performance measure does not necessarily correspond to a linear mapping: \(v_{t}_{c=1}^{C}w^{c}v^{c}\).

To address those difficulties, we propose an alternative approach called Accountable Offline Controller (AOC) that executes traceable offline control leveraging certain examples as decision corpus.

### Linear Belief Learning for Accountable Offline Control

To alleviate the above problems, we first introduce a specific type of belief function \(: ^{d_{b}}\), which maps a joint observation-action-history input onto a \(d_{b}\)-dimensional belief space.

**Property 3.2** (Belief Space Linearity).: The performance measure function \(:\) can always be decomposed as \(=\), where \(: ^{d_{b}}\) maps the joint observation-action-history space to a \(d_{b}\) dimensional belief variable \(b_{t}=(o_{t},a_{t},h_{t})\) and \(:\) is a linear function that maps the belief \(b_{t}\) to an output \((b_{t})\).

_Remark 3.3_.: This property is often the case with prevailing neural network approximators, where the belief state is the last activated layer before the final linear output layer.

_Remark 3.4_.: The belief mapping function maps a \(d_{o}+d_{a}+(d_{o}+d_{a}+1)(t-1)\) dimensional varied-length input into a fixed-length \(d_{b}\) dimensional output. Usually, we have \(1=d_{v}<d_{b} d_{o}+d_{a}+(d_{o}+d_{a}+1)(t-1)\). It is important to highlight that calculating the distance in the belief space is feasible, whereas it is intractable in the joint original space.

Based on Property 3.2, the linear relationship between the belief variable and the performance measure makes belief space a better choice for example-based decision understanding -- this is because linearly decomposing the belief space also breaks down the performance measure, hence can guide action selection towards optimizing the performance metric. To be specific, we have

\[v(a_{t}|_{0_{t}},h_{t})=(o_{t},a_{t},h_{t})= _{c=1}^{C}w^{c}(o^{c},a^{c},h^{c})= _{c=1}^{C}w^{c}(o^{c},a^{c},h^{c})= _{c=1}^{C}w^{c} v^{c}\] (3)

Although we have formally defined of the _Corpus Subset_, its composition has not been explicitly specified. We will now present the fundamental principles and techniques employed to create a practical _Corpus Subset_.

### Selection of Corpus Subset

As suggested by Equation (3), if a belief state can be expressed as a linear combination of examples from the offline dataset, the same weights applied to the linear decomposition of the belief space can be utilized to decompose and synthesize the outcome. More formally, we define the convex hull spanned by the image of the Corpus Subset under the mapping function \(\):

**Definition 3.5** (Belief Corpus Subset).: A _Belief Corpus Subset_\(()\) is defined by applying the belief function \(\) to the corpus subset \(\), with the corresponding set of values denoted as \(v()\):

\[(),v()=\{b^{c}=(o^{c},a ^{c},h^{c}),v^{c}|(o^{c},a^{c},h^{c},v^{c})\} \] (4)

**Definition 3.6** (Belief Corpus Convex Hull).: The _Belief Corpus Convex Hull_ spanned by a corpus subset \(\) with the belief corpus \(()\) is the convex set

\[()=\{_{c=1}^{C}w^{c}b^{c}|w^{c},b^{c}(), c[C],_{c=1}^{C}w^{c}=1\}\] (5)

Note that an accurate belief corpus subset decomposition for a specific control time belief value \(b_{t}\) may be unattainable when \(b_{t}()\). In such situations, the optimal course of action is to identify the element \(_{t}\) within \(()\) that provides the closest approximation to \(b_{t}\). Denoting the norm in the belief space as \(||||_{}\), this equates to minimizing a corpus residual that is defined as the distance from the control time belief to the nearest point within the belief corpus convex hull:

**Definition 3.7** (Belief Corpus Residual).: The _Belief Corpus Residual_ given a control time belief variable \(b_{t}\) and a belief corpus subset \(\) is given as

\[r_{}(b_{t})=_{_{t}()}||b_{t} -_{t}||_{}\] (6)

In summary, when \(b_{t}\) resides within \(()\), it can be decomposed using the belief corpus subset according to \(b_{t}=_{c=1}^{C}w^{c}b^{c}\). Here, the weights can be understood as representing the similarity and importance in reconstructing the control time belief variable through the belief corpus subset; otherwise, the optimizer of Equation (6) can be employed as a proxy for \(b_{t}\). Under such circumstances, the **belief corpus residual acts as a gauge for the degree of extrapolation**. Given those definitions, we use the following proposition to inform the choice of an appropriate corpus subset.

**Proposition 3.8** (Estimation Error Bound for \(v(a_{t})\), _cf._ Crabbe et al. ).: _Consider the belief variable \(b_{t}=(o_{t},a_{t},h_{t})\) and \(\) the optimizer of Equation (6), the estimated value residual between \((b_{t})\) and \(()\) is controlled by the corpus residual:_

\[||()-(b_{t})||_{}|| ||_{} r_{}(b_{t})\] (7)

_where \(||||_{}\) is a norm on \(\) and \(||||_{}=\{^{+}:|| (b)||_{}||b||_{}\}\) is the operator norm for the linear mapping._Proposition 3.8 indicates that minimizing the belief corpus residual also minimizes the estimation error. In order to approximate the belief variables in control time accurately, we propose to use the corpus subset that spans the minimum convex hull in the belief space.

**Definition 3.9** (Minimal Hull and Minimal Corpus Subset).: Denoting the corpus subsets whose belief convex hull contains a belief variable \(b_{t}\) by

\[(b_{t})=\{b_{t} ()\}\]

Among those subsets, the one that contains _maximally_\(d_{b}+1\) examples and has the smallest hypervolume forms the _minimal hull_ is called the _minimal corpus subset_ (w.r.t. \(b_{t}\)), denoted by \(}(b_{t})\):

\[}(b_{t}):=_{}_{()}dV\]

**Proposition 3.10** (Existence and Uniqueness).: _Consider the belief variable \(b_{t}=(o_{t},a_{t},h_{t})\), if \(r_{}(b_{t}) 0\) holds for \(=\), then \(}(b_{t})\) exists and the decomposition on the corresponding minimal convex hull exists and is unique._

In this work, we use the corpus subset that constructs the minimal (belief corpus convex) hull to represent the control time belief variable \(b_{t}\). Those concepts are illustrated in Figure 1.

### Accountable Offline Control with Belief Corpus

Given the linear relationship between the variables \(v\) and \(b\), for a specific action \(a_{t}\), we can estimate the corresponding \((a_{t})=()=_{c=1}^{C}w^{c}v^{c}\), where \(o_{t}\) and \(h_{t}\) are the control time observation and transition history respectively. Applying an arg-max type of operator to the above value estimator, we are able to select the action with the highest value that is supported by the belief corpus:

\[_{}(o_{t},h_{t})=_{a_{t}}(a_{t}),\] (8)

Flexibility of the Measurement Function: Plug-and-Play.As discussed earlier, the choice of measurement function \(\) depends on the objective of policy learning. For the low-dimensional tasks, we can instantiate \(\) using the Monte Carlo estimation of the cumulative return: \(v^{c}=_{t=t_{}}^{T}^{t-t_{}r_{t}}\), as the most straightforward choice in control tasks; for higher-dimensional tasks, the measurement metric can be the learned Q-value function; for other applications like when safety-constraints are also taken into consideration, the measurement function can be adapted accordingly.

### Optimization Procedure

The optimization process in our proposed AOC method involves three main steps: optimizing the belief function and linear mapping, reconstructing the belief during control time rollout, and

Figure 1: We illustrate the concepts of Corpus Residual and Minimal Hull in a \(2\)-dim example. The belief function \(\) maps trajectories in the offline dataset (colors of **blue** and **green** denote that they may come from different behavior policies) onto the belief space. The **control trajectory** with different sampled actions at time \(t\) can also be mapped to the belief space. Some of those sampled action candidates (e.g., \(a_{t}^{(1)},a_{t}^{(2)}\)) can be well-supported by a corpus subset, while in another case (\(a_{t}^{(3)}\)) the corpus residual manifests the extrapolation error. Among the three plotted beliefs generated by candidate actions, \(b_{t}^{(3)}\) is out of the convex hulls that can be spanned by any decision corpus, \(b_{t}^{(2)}\) has the minimal corpus (green shaded area), \(b_{t}^{(1)},b_{t}^{(2)}\) are also in a larger corpus hull (blue shaded area) but it is not ideal.

performing sampling-based optimization on the action space. In this section, we detail each of these steps along with corresponding optimization objectives. First, the belief function \(\) and the linear mapping \(\) are learned by minimizing the difference between the predicted performance measure and the actual value \(v\) from the dataset \(\):

\[,=_{,}_{(o,a,h,v)}( v-(o,a,h))^{2}\] (9)

Next, with the observation \(o_{t}\) and trace \(h_{t}\), any candidate action \(a_{t}\) corresponds to a belief \(b_{t}=(o_{t},a_{t},h_{t})\), which can be reconstructed by the minimal corpus subset \(}(b_{t})\), according to:

\[w^{c}=_{w^{c}}||_{c=1}^{C}w^{c}(o^{c},a^{c},h^{c})-b_{t}||, \ \ \ \ _{c=1}^{C}w^{c}=1,\ \ (o^{c},a^{c},h^{c})}(b_{t})\] (10)

Then the value of action \(a_{t}\) can be estimated by \((a_{t}|o_{t},h_{t})=_{c=1}^{C}w^{c}v^{c}\). Finally, we perform a sampling-based optimization on the action space, with the corpus residual taken into consideration for the pursuance of conservation. For each control step, we uniformly sample \(K\) actions in the action space \(a^{(k)}t U()\) and then search for the action that corresponds to the maximal performance measure while constraining the belief corpus residual to be smaller than a threshold \( 0\):

\[_{}(o_{t},h_{t})=_{a_{t}\{a_{t}^{(k)}\}}_{c=1}^ {C}w^{c}v^{c},\ \ r_{}((o_{t},a_{t},h_{t}))\] (11)

The complete optimization procedure is detailed in Algorithm 1.

```  Input \(\): \(=\{o_{t}^{i},a_{t}^{i},r_{t}^{i},o_{t+1}^{i}\}_{t=1,,T}^{i=1,,N}\), control time observation \(o_{t}\), control history \(h_{t}\). Hyper-parameters: \(K\) for the number of candidate actions, \(\) for the threshold. Output  Accountable action \(a_{t}\) for control. \(\#\) 1. Optimize belief function \(\) and linear mapping \(\) according to Equation (9). \(\#\) 2. Sample \(K\) candidate actions uniformly in the action space: \(a_{t}^{k} U(),k[K]\). \(\#\) 3. Find the minimal hulls \(}(b_{t})\) for each action, based on \(\) and \(b_{t}=(o_{t},a_{t}^{k},h_{t})\). \(\#\) 4. Minimize corpus residuals for each action candidate \(a_{t}^{k}\) according to Equation (10). \(\#\) 5. Estimate the value of action \(a_{t}\) by \((a_{t}|o_{t},h_{t})=_{c=1}^{C}w^{c}v^{c}\) \(\#\) 6. Outputs the candidate action that has the highest estimated value according to Equation (11). ```

**Algorithm 1** The Accountable Offline Controller

## 4 Related Work

We review related literature and contrast their difference in Table 1. Due to constraints on space, we have included comprehensive discussions on related work in Appendix B. Our proposed method stands out due to its unique attributes: It incorporates properties of accountability, built-in conservation for offline learning, adaptation to preferences or guidelines, is suitable for the low-data regime, and is compatible with the strictly offline imitation learning setting where the reward signal is not recorded in the dataset.

   Method / Property & Conservation & Accountable & Low-Data & Adaptive & Reward-Free & Examples \\  Q-Learning & ✓ & ✗ & ✓ & ✗ & ✗ &  \\ Episodic Control & ✗ & ✗ & ✓ & ✗ & ✗ &  \\ Nearest Neighbor & ✗ & ✓ & ✗ & ✓ & ✓ &  \\ Model-Based RL & ✗ & ✗ & ✗ & ✓ & ✗ &  \\ Behavior Clone & ✗ & ✗ & ✗ & ✗ & ✓ &  \\  AOC & ✓ & ✓ & ✓ & ✓ & ✓ & Ours \\   

Table 1: AOC is distinct as it satisfies **5 desired properties** mentioned earlier: (**P1**) Conservation: AOC seeks the best potential action in the belief convex hull, such that the estimations of decision outcomes are interpolated, avoiding aggressive extrapolation that is harmful in offline control ; (**P2**) Accountability: the decision-making process is supported by a corpus subset from the offline dataset, hence all the decisions are traceable; (**P3**) Low-Data Requirement: it works in the low-data regime; (**P4**) Adaptivity: the control behavior of AOC can be adjusted according to additional constraints as clinical guidelines without modification or re-training; (**P5**) Reward-Free: AOC can be extended to the strictly offline imitation setting where rewards are unavailable.

Experiments

Environments and Set-upsIn this section, we present empirical evidence of the properties of our proposed method in a variety of environments. These environments include (1) control benchmarks that simulate POMDP and heterogeneous outcomes in healthcare, (2) a continuous navigation task that allows for the visualization of properties, and (3) a real-world healthcare dataset from Ward  that showcases the potential of AOC for deployment in high-stakes, real-world tasks.

In Section 5.1, we benchmark the performance of AOC on the classic control benchmark, contrasting it with other algorithms and highlighting **(P1-P3)**. In Section 5.2, Section 5.3, and Section 5.4 we individually highlight properties **(P1),(P2),(P4)**. Subsequently, in Section 5.5, we apply AOC to a real-world dataset, highlighting its ability to perform accountable, high-quality decision-making under the strictly offline imitation setting **(P1-P5)**. Finally, Section 5.6 provides additional empirical evidence supporting our intuitions and extensive stress tests of AOC.

### (P1-P3): Accountable Offline Control in the Low-Data Regime

Experiment SettingOur initial experiment employs a synthetic task emulating the healthcare setting, simultaneously allowing for flexibility in stress testing the algorithms.

We adapt our environment, **Pendulum-Het**, from the classic Pendulum control task involving system stabilization. Heterogeneous outcomes in healthcare contexts are portrayed through varied dynamics governing the data generation process. Data generation process details can be found in Appendix D.4. Taking heterogeneous potential outcomes and partial observability into consideration, the Pendulum-Het task is significantly more challenging than the classical counterpart.

We compare AOC with the nearest-neighbor controller **(1NN)** and its variant that using \(k\) neighbors **(kNN)**, model-based RL with Mode Predictive Control (MPC)**, model-free RL **(MFRL)**, and behavior clone **(BC)**. Our implementation for those baseline algorithms is based on open-sourced codebases, with details elaborated in Appendix D.6. We change the size of the dataset to showcase the performance difference of various methods under different settings. The **Low-Data** denotes settings with \(100000\) transition steps, **Mid-Data** denotes settings with \(300000\) transition steps, **Rich-Data** denotes settings with \(600000\) transition steps.

ResultsWe compare AOC against baseline methods in Table 2. AOC achieves the highest average cumulative score among the accountable methods and is comparable with the best black-box controller. In experiments, we find increasing the number of offline training examples does not improve the performance of MFRL due to stability issues. Detailed discussions can be found in Appendix D.7.

Take-Away:_As an accountable algorithm, AOC performs on par with state-of-the-art black-box learning algorithms. Particularly in the POMDP task, wherein data originates from heterogeneous systems, AOC demonstrates robustness across diverse data availability conditions._

### (P1): Avoid Aggressive Extrapolation with the Minimal Hull and Minimized Residual

Experiment SettingIn Equation (11), our arg-max operator includes two adjustable hyperparameters: the number of uniformly sampled actions and the threshold. In this section, we demonstrate how these hyperparameters can be unified and easily determined.

   Task & Low-Data & Mid-Data & Rich-Data \\  AOC & \(\) & \(\) & \(\) \\ kNN & \(\) & \(\) & \(\) \\ lNN & \(\) & \(\) & \(\) \\  BC & \(\) & \(\) & \(\) \\ MFRL & \(\) & \(\) & \(\) \\ MPC & \(\) & \(\) & \(\) \\  Data-Avg-Return & \(\) & \(\) & \(\) \\   

Table 2: Results on the Heterogeneous Pendulum dataset. The cumulative reward of each method is reported. Experiments are repeated with \(\) seeds. **Higher is better.**The consideration of sample size primarily represents a balance between optimization precision and computational cost -- although such cost can be minimized by working with modern computational platforms through parallelization. On the other hand, selecting an appropriate value for \(\) might initially seem complex, as the residual's magnitude should depend on the learned belief space.

In our implementation, we employ a quantile number to filter out sampled actions whose corresponding corpus residuals exceed a certain population quantile. For instance, when \(=0.3\) with a sample size of \(100\), only the top \(30\%\) of actions with the smallest residuals are considered in the arg-max operator in Equation (11). The remaining number of actions is referred to as the _effective action size_. This section presents results for the **Pendulum-Het** environment, while additional qualitative and quantitative results on the conservation are available in Appendix F.2. Intuitively, using a larger sampling size will improve the performance, at a sub-linear cost of computational expenses, and using a smaller quantile number will make the behavior more conservative. That said, using a small quantile number may not be always good, as it also decreases the effective action size. The golden rule is to use larger sampling size and smaller quantile threshold.

ResultsTable 3 exhibits the results. Our experiments highlight the significance of this filtering mechanism: removal or weakening of this mechanism, through a larger quantile number that filters fewer actions, leads to aggressive extrapolation and subsequently, markedly deteriorated performance. This is because the decisions made by AOC in those cases cannot be well supported by the minimal convex hull, introducing substantial epistemic uncertainty in value estimation.

Conversely, using a smaller quantile number yields a reduced _effective action size_, which subsequently decreases optimization accuracy and leads to a minor performance drop. Hence when computational resources allow, utilizing a larger sample size and a smaller quantile number can stimulate more conservative behavior from AOC. In all experiments reported in Section 5.1, we find a sample size of \(100\) and \(=0.5\) generally work well.

**Take-Away:**_AOC is designed for offline control tasks by adhering to the principle of conservative estimation. The \(\) hyperparameter, introduced for conservative estimation, can be conveniently determined by incorporating it into the choice of an effective action size._

### (P2): Accountable Offline Control by Tracking Reference Examples

Experiment SettingIn healthcare, the treatments suggested by the clinician during different phases can be based on different patients, and those treatments may even be given by different clinicians according to their expertise. In this section, we highlight the accountability of AOC within a **Continuous Maze** task. Such a task involves the synthesis of decisions from multiple experts to achieve targeted goals. It resembles the healthcare example above and is ideal for visualizing the concept of accountability.

Figure 2 displays the map, tasks, and expert trajectories within a 16x16 Maze environment. This environment accommodates continuous state and action spaces. We gathered trajectories from \(4\) agents, each of which is near-optimal for a distinct navigation task:

1 agent \(_{1}\) starts at position \((0,0)\) and targets position \((8,16)\). Trajectories of \(_{1}\) are marked in blue.

2 agent \(_{2}\) starts at position \((0,0)\) and targets position \((8,8)\). Trajectories of \(_{2}\) are marked in orange.

3 agent \(_{3}\) starts at position \((8,16)\) and aims position \((16,0)\). Trajectories of \(_{3}\) are marked in green.

4 agent \(_{4}\) starts at position \((8,8)\) and targets position \((16,0)\). Trajectories of \(_{4}\) are marked in red.

We generate 250 trajectories for each of the \(4\) agents, creating the offline dataset. The control task initiates at position \((0,0)\), with the goal situated at \((16,0)\) -- requires a fusion of expertise in the dataset to finish. To complete the task, an agent could potentially learn to replicate \(_{1}\), followed by adopting \(_{3}\)'s strategy. Alternatively, it could replicate \(_{2}\) before adopting \(_{4}\)'s approach.

  \(\) (quantile) & Performance \\  \(0.3\) & \(-2.05 0.45\) \\ \(0.5\) & \(-1.25 0.40\) \\ \(0.9\) & \(-503.39 301.60\) \\ \(1.0\) & \(-853.29 138.43\) \\  

Table 3: The \(\) in Equation (11) controls the conservation tendency of the control time behaviors.

Figure 2: Left: the Maze; Right: trajectories generated by \(4\) experts for different tasks.

ResultsIn this experiment, we employ AOC to solve the task and visualize the composition of the corpus subset at each decision step in Figure 3. In the initial stages, the corpus subset may comprise a blend of trajectories from \(_{1}\) and \(_{2}\). Subsequently, AOC selects one of the two potential solutions at random, as reflected in the chosen reference corpus subsets. Upon crossing the central gates on the x-axis, AOC's decisions become reliant on either \(_{3}\) or \(_{4}\), persisting until the final steps, when the corpus subsets can again be a mixture of the overlapped trajectories.

**Take-Away:**_AOC's decision-making process exhibits strong accountability, as the corpus subset can be tracked at every decision-making step. This is evidenced by AOC's successful completion of a multi-stage Maze task, wherein it effectively learns from mixed trajectories generated by multiple policies at differing stages._

### (P4): Flexible Offline Control under User Specification

Experiment SettingGiven the tractable nature of AOC's decision-making process, the agent's behavior can be finely tuned. One practical way of modifying these decisions is to adjust the reference dataset, which potentially contributes to the construction of the corpus subset. In this section, we perform experiments in the **Continuous Maze** environment with the same offline dataset described in Section 5.3. We modify the sampling rate of trajectories generated by \(_{1}\) and experiment with both re-sampling and sub-sampling strategies, using sample rates ranging from \([ 4, 3, 2, 1, 0.75, 0.5, 0.25]\). In the re-sampling experiments (sample rates larger than \(1\)), we repeat trajectories produced by \(_{1}\) to augment their likelihood of selection as part of the corpus subset. In contrast, the sub-sampling experiments partially omit trajectories from \(_{1}\) to reduce their probability of selection. In each setting, 100 control trajectories are generated.

ResultsFigure 4 shows the results: the success rate (**Succ. Rate**) quantifies the quality of control time behaviors, while the remaining two curves denote the percentage of control time trajectories that accomplish the task by either following either \(_{1}\) - \(_{3}\) (passing the upper gate) or \(_{2}\) - \(_{4}\) (passing the lower gate). The behavior of AOC is influenced by the sampling rate. Utilizing a high sampling rate for the trajectories generated by \(_{1}\) often leads to the selection of the solution that traverses the upper gate. As the sampling rate diminishes, AOC's behavior begins to be dominated by the alternative strategy. In both re-sampling and sub-sampling strategies, the success rate of attaining the goal remains consistently high. We provide further visualizations of the learned policies under various sampling rates in Appendix E.

**Take-Away:**_The control time behavior of AOC can be manipulated by adjusting the proportions of behaviors within the offline dataset. Both re-sampling and sub-sampling strategies prove equally effective in our experiments where multiple high-performing solutions are available._

### (P1-P5): Real-World Dataset: The Strictly Batch Imitation Setting in Healthcare

Experiment SettingWe experiment with the **Ward dataset** that is publicly available with . The dataset contains treatment information for more than \(6000\) patients who received care on the general medicine floor at a large medical center in California. These patients were diagnosed with a range of ailments, such as pneumonia, sepsis, and fevers, and were generally in stable condition. The measurements recorded included standard vital signs, like blood pressure and pulse, as well as lab test results. In total, there were \(8\) static observations and \(35\) temporal observations that captured the dynamics of the patient's conditions. The action space was restricted to a choice of a binary

Figure 4: The behavior of AOC can be controlled by re-sampling or sub-sampling the offline dataset.

Figure 3: Visualize Accountability. The upper colors display the components of the corpus subset at different x-positions.

action pertaining to the use of an oxygen therapy device. In the healthcare setting, treatment decisions are given by domain experts and the objective of policy learning is to mimic those expert behaviors through imitation. However, as interaction with the environment is strictly prohibited, the learning and evaluation of policies should be conducted in a strictly offline manner. The dataset is split into \(10:1\) training and testing sets and the test set accuracy is used as the performance metric.

We benchmark the variant of AOC tailored for the strictly batch imitation setting, dubbed as **ABC** -- as an abbreviation for **Accountable **B**ehavor **C**lone -- against three baselines: (1) behavior cloning with a linear model (**BC-LR**), which serves as a transparent baseline in the offline imitation setting; (2) behavior cloning with a neural network policy class (**BC-MLP**), which represents the level of performance achievable by black-box models in such contexts; and (3) the k-nearest neighbor controller (**kNN**) as an additional accountable baseline.

ResultsThe results of kNN with various choices of \(k\) in the experiments, alongside a comparison with ABC using the same number of examples in the corpus subset, are presented in Figure 5. Shaded areas in the figure denote the variance across \(10\) runs.

Operating as an accountable policy under the offline imitation setting, ABC achieves performance similar to that of behavior cloning methods utilizing black-box models. Meanwhile, other baselines face challenges in striking a balance between transparency and performance.

Furthermore, we visualize the corpus subset supporting control time decisions in a \(2\)-dimensional belief space to illustrate the accountability. We demonstrate the differences in the decision boundary where predictions are more challenging. In such cases, the decision supports of kNN rely on extrapolation and have no idea of the potential risk of erroneous prediction, while the heterogeneity in ABC's minimal convex hull reflects the risk. We provide a more detailed discussion in Appendix E.2.

**Take-Away:**_The variant of AOC, dubbed as ABC, is well-suited for the strictly batch imitation setting, enabling accountable policy learning. While other baseline methods in this context struggle to balance transparency and policy performance, ABC achieves comparable performance with black-box models while maintaining accountability._

### Additional Empirical Studies

To further verify our intuitions and stress test AOC, we provide additional empirical analysis in Appendix F. Specifically, we analyze the **trade-off** between accountability and performance in Appendix F.1; we visualize the **controllable conservation** in Appendix F.2; we benchmark AOC on **another benchmark** for the general interests of the RL community in Appendix F.3; and show how to combine AOC with **black-box sampler** in Appendix F.4; Finally, we show AOC can detect control time **OOD examples** in Appendix F.5.

## 6 Conclusion

In conclusion, this study presents the Accountable Offline Controller (AOC) as a solution for offline control in responsibility-sensitive applications. AOC possesses five essential properties that make it particularly suitable for tasks where traceability and trust are paramount: ensures accountability through offline decision data, thrives in low-data environments, exhibits conservative behavior in offline contexts, can be easily adapted to user specifications, and demonstrates flexibility in strictly offline imitation learning settings. Results from multiple offline control tasks, including a real-world healthcare dataset, underscore the potential of AOC in enhancing trust and accountability in control systems, paving the way for its widespread adoption in real-world applications.

Figure 5: Results on the Ward dataset. ABC is able to perform accountable imitation under the strict offline setting with high performance comparable with the black-box models.