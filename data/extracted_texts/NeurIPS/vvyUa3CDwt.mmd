# Intrinsic Self-Supervision for Data Quality Audits

Fabian Groger\({}^{*}\)\({}^{1,2}\), Simone Lionetti\({}^{2}\), Philippe Gottfrois\({}^{1}\), Alvaro Gonzalez-Jimenez\({}^{1}\),

**Ludovic Amruthalingam\({}^{2}\), Labelling Consortium\({}^{}\)\({}^{3}\), Matthew Groh\({}^{4}\), Alexander A. Navarini\({}^{}\)\({}^{1,3}\), Marc Pouly\({}^{}\)\({}^{2}\)**

\({}^{1}\)University of Basel \({}^{2}\)Lucerne University of Applied Sciences and Arts

\({}^{3}\)University Hospital of Basel \({}^{4}\)Northwestern University

Correspondence: fabian.groeger@unibas.chValerie Amann, Elisabeth Gossinger, Hazem Juratli, Beda Muhleisen, Alina Muller, and Veronika Schmidt Joint last authorship

###### Abstract

Benchmark datasets in computer vision often contain off-topic images, near duplicates, and label errors, leading to inaccurate estimates of model performance. In this paper, we revisit the task of data cleaning and formalize it as either a ranking problem, which significantly reduces human inspection effort, or a scoring problem, which allows for automated decisions based on score distributions. We find that a specific combination of context-aware self-supervised representation learning and distance-based indicators is effective in finding issues without annotation biases. This methodology, which we call SelfClean, surpasses state-of-the-art performance in detecting off-topic images, near duplicates, and label errors within widely-used image datasets, such as ImageNet-1k, Food-101N, and STL-10, both for synthetic issues and real contamination. We apply the detailed method to multiple image benchmarks, identify up to 16% of issues, and confirm an improvement in evaluation reliability upon cleaning. The official implementation can be found at: https://github.com/Digital-Dermatology/SelfClean.

## 1 Introduction

In traditional machine learning (ML), data cleaning is essential since minor contamination in the dataset can significantly impact model performance and robustness . However, with the rise of deep learning (DL) and large-scale datasets, data cleaning has become less crucial as large models have shown to work relatively well even when training data has low quality . Validating and cleaning large datasets is challenging, especially for high-dimensional data, because thorough manual verification is often not feasible. Thus, a lot of research has been focusing on learning from noisy data  rather than fixing quality issues, as the overwhelming benefits of large-scale datasets are believed to exceed the drawback of diminished control. On the other hand, for many domains, the size of available datasets is still one of the main limiting factors for the progress of artificial intelligence (AI). In these low-data regimes, the importance of clean data is more pronounced since even fractional amounts of poor-quality samples can substantially hamper performance and possibly lead to wrong conclusions . This is especially relevant in high-stakes settings such as the medical domain, where high-quality data is needed to train robust models and validate their performance. However, also in these domains, many practitioners rather focus on data quantity as a key performance driver and implicitly assume a high-quality collection process . Thus, even medical datasets are known to contain varying noise levels, which can substantially undermine the progress of ML .

The necessity to report comparable results has led DL practitioners to heavily rely on benchmark datasets despite them being known for containing data quality issues. For example, an evaluation often of the most used benchmark datasets found them to have an average label error rate of 3.4% in the evaluation set . Such issues in benchmark sets, especially when used for evaluation, undermine the framework by which scientific progress is measured. Specifically, contamination in evaluation sets corrupts scores, making it unclear which methods successfully handle edge cases and obscuring their proximity to optimal performance. This is particularly relevant since many popular benchmarks are saturating, i.e., only saw minor relative changes in performance over the last few years . Data quality issues in training sets, instead, may hinder optimization and produce suboptimal models. Importantly, despite the need for correct evaluation data, cleaning evaluation sets can be problematic, as it may optimistically bias performance estimates. Ignoring known data quality issues during evaluation is, however, also incorrect, so an appropriate compromise is necessary.

In this paper, we address three types of data quality issues that illustrate these mechanisms well. _Off-topic samples_, i.e., inputs included in a dataset by mistake, add noise to evaluation metrics while slowing down and confusing training. _Near duplicates_, i.e., different views of the same object, produce arbitrary re-weighting in the evaluation set, reduce variability in the training set, and most importantly, often introduce leaks between training and evaluation sets that can lead to over-optimistic results. _Label errors_, i.e., wrongly annotated samples, result in incorrect evaluation and poison the training process. We focus on these three data quality issues because we empirically found them to be frequent in existing image benchmark datasets and challenging to detect. There are of course other types of data quality issues, including many that can be detected using ad-hoc rules, such as odd brightness, aspect ratio, resolution, sharpness, and entropy in the case of images.

In this paper, we formulate dataset cleaning as a set of ranking problems, which greatly reduce the effort for manual inspection, or alternatively as a set of scoring problems, which can be used for fully automatic decisions based on score distributions. We then find that a combination of self-supervised, dataset-specific representation learning and distance-based indicators can effectively identify multiple issues in image collections. We apply this approach to well-known benchmark datasets in computer vision and medical imaging, and discuss implications for reliability of results across these domains. The outlined method enables practitioners to audit data collections, increase evaluation reliability, and amend the training set to improve results. This work contributes to data-centric ML  and aims to bolster confidence in both existing and newly collected datasets. In summary, the main contributions are: 1) A novel data cleaning procedure called SelfClean, which can be used to find off-topic samples, near duplicates, and label errors, and relies exclusively on the dataset itself, illustrated in figure 1. 2) A detailed comparison between this cleaning method and competing approaches on synthetic and natural contamination, including validation against human experts. 3) The application of SelfClean to well-known benchmarks in computer vision and medical imaging and the identification of their issues. 4) A practical recommendation to clean training and evaluation splits of benchmark datasets as a reasonable trade-off between correctness and bias for more accurate performance estimates.

Figure 1: SelfClean first trains a self-supervised encoder on noisy data to obtain latent representations for dataset samples. It then detects off-topic samples with agglomerative clustering, near duplicates based on pairwise distances, and label errors using the intra-/extra- class distance ratio.

Related work

Data cleaning is a core component of data analytics and a topic of interest in the data management community . Recently, the data-centric AI initiative  brought it back to the attention of ML researchers, resulting in the development of data cleaning tools. For instance, Vailoppilly et al.  proposed an all-in-one "data cleansing" tool based on dimensionality reduction, a DL noise classifier, and a denoising model. Tools for data cleaning also started to appear, including CleanLab  and CleanVision , Lightly , and FastDup . Most data cleaning approaches require dimensionality reduction to work with high-dimensional data such as images. This includes traditional approaches such as PCA  or t-SNE , and feature extraction with deep encoders, which are usually trained on natural image databases such as ImageNet . In the last few years, self-supervised learning (SSL)  was shown to learn more representative latent spaces compared to supervised training [20; 21; 22]. Furthermore, Cao and Wu  demonstrated that SSL can learn meaningful latent spaces even with small datasets, low resolution, and small architectures. Inspired by these results and unlike previous works, we rely on SSL as a basis to detect three important types of data quality issues encountered in practice: off-topic samples, near duplicates, and label errors . Since these sub-problems are typically addressed separately in the literature, we briefly review them in turn.

The problem of identifying off-topic samples is closely related to generalized out-of-distribution detection  and is akin to outlier detection, which involves both normal and anomalous samples . Outlier detection can be addressed with supervised, unsupervised, and semi-supervised learning and was initially developed to fit data more smoothly . In the realm of data cleaning, where the nature of off-topic samples is generally unknown, it is most similar to the unsupervised setting. Outliers in low-density regions can be found using reconstruction errors [27; 28], classification , or probabilistic approaches . For a detailed review of these methods, see .

Near-duplicate detection is traditionally based on representation matching [31; 32]. Most DL approaches follow a similar strategy, where feature vectors are extracted by a deep network and used for content-based matching . Another option is to learn a similarity metric between samples with Siamese neural networks . A recent approach for copy detection (i.e., near-duplicate detection) uses a contrastive self-supervised objective with entropy regularization to ensure consistent separation of image descriptions . However, it requires a manually adapted threshold for each dataset .

The identification of label errors is generally focused on prediction-label agreement via confusion matrices and proceeds by removing samples with low recognition rate  or parts of the minority classes . There are exceptions, such as recent approaches based on supervised contrastive learning for label error correction [39; 40]. Another prominent method is confident learning, which identifies label errors based on noisy data pruning, using probabilistic thresholds to estimate noise and ranking examples to train with confidence .

## 3 Methodology

Let \(=\{(_{i},l_{i}) i\}\) be an image classification dataset to be cleaned, where \(=\{1,,N\}\) is the index set, \(_{i}\) is the \(i\)-th sample, and \(l_{i}\{1,,L\}\) is the \(i\)-th label. For each issue type, we construct a scoring function \(s\) that assigns values in \(\) to samples or pairs thereof, such that elements with a lower score are more likely to be problematic. Sorting samples by the value obtained from the scoring function \(s\) induces a ranking \(R\) where more likely issues appear earlier.

### Representation learning

As a first step, we train a deep feature extractor \(f\) with parameters \(\) on the dataset \(\) using self-supervised learning (SSL), which learns representations by solving auxiliary tasks. Let \(_{i}=f(_{i};)^{D}\) be the representation of sample \(_{i}\) obtained with \(f\), where \(D\) denotes the latent dimension. Note that SSL is performed on the entire dataset including data quality issues. Any SSL method can be used, as investigated in appendix F.5. Here, we consider SimCLR  and DINO , which were shown to produce meaningful latent spaces [20; 21]. SimCLR is a contrastive approach that compares different views of the same image against other randomly sampled ones. DINO is a self-distillation method which trains a student network to match a teacher network on different views of the same image. For both strategies, we rely on vision transformer (ViT) encoders, as detailed in appendix C and ablated in F.6.

As feature normalization is often built into the SSL training objective, it is natural to compare points in its latent space using cosine similarity, \((_{i},_{j})=_{i}^{}_{j}/(||_ {i}||_{2}||_{j}||_{2})\), and the associated distance scaled to \(\), \((_{i},_{j})=(1-(_{i}, _{j}))/2\). We explicitly include \(L_{2}\)-normalization during training and inference for strategies without normalization (e.g., DINO), such that their latent space is a unit hypersphere of dimension \(D-1\). In appendix F.1, we present an ablation study of this normalization and investigate the influence of different distance functions.

### Distance-based indicators

Dataset-specific representations based on inductive bias can be coupled with separate distance-based indicators to identify candidate issues. Below we introduce each issue type and the corresponding indicator function used to detect them.

**Off-topic samples.** We define samples as off-topic when they are included in the dataset by mistake. Images from extraneous modalities, affected by device malfunctions, or without any object of interest are some examples. Atypical samples, due e.g. to the phenomenon of hidden stratification , that are included intentionally, are not off-topic, and although they may be revealed in the same search, they require different treatment. We achieve off-topic sample ranking by agglomerative clustering with single linkage  in representation space. The idea is that the later a cluster is merged with a larger one, the more it can be considered an outlier . The ranking is obtained by sorting the clustering dendrogram such that, at each merge, the elements of the cluster with fewer leaves appear first. We also associate each sample with a numerical score, which takes small values for abnormal instances and is compatible with the described ranking. In appendix J, we construct such a score \(s_{}(_{i})\) starting from the idea that merges, which happen at very different distances or between clusters of very different sizes, should produce large numerical variations.

**Near duplicates.** We define near duplicates as pairs of images that contain different views of the same object. In this sense, exact duplicates are a special case of near duplicates. We rank potential near duplicates by sorting each pair of distinct samples \((i,j),i<j\) in ascending order according to the distance between their representations in the latent space, \(s_{}(_{i},_{j})=(_{i},_{ j})\).

**Label errors.** We define label errors as samples annotated with a wrong class label. We rank potential label errors by sorting samples in ascending order according to their intra-/extra- class distance ratio . For an anchor point \(_{i}\), this ratio compares the distances to the nearest representation of a different label \(m_{}(_{i})\) and the distance to the nearest representation of the same label \(m_{}(_{i})\):

\[m_{}(_{i}) =_{j,\,l_{j}=l_{i}} (_{i},_{j}), s_{}(_{i}) =^{2}(_{i})}{m_{}^{2}(_{i})+m_{ }^{2}(_{i})}.\] (1)

In all three cases, SelfClean leverages the local structure of the embedding space: Cluster distances are computed only using the closest samples during agglomeration for off-topic samples, near duplicates are identified among sample pairs with the smallest distances, and label errors are found using only the nearest examples of the same and a different class.

### Operation modes

The criteria above rank and score candidate issues, but do not specify which ones are inferred to be actual issues. This can be achieved with two operating modes: Human-in-the-loop or fully automatic.

**Human-in-the-loop.** This mode leverages candidate issue rankings to facilitate human confirmation which is often infeasible exhaustively, especially when considering pairwise relationships such as near duplicates. A human curator inspects a data sequence where issues tend to appear earlier, either confirming and correcting problems or looking for a specific rank threshold that gives the desired balance between precision and recall. In appendix H, we estimate that for a typical dataset SelfClean reduces this inspection effort by a factor between 5 and 50 depending on issue type and baseline.

**Fully-automatic.** To perform automatic cleaning, specifying a fraction of data quality issues _a priori_ is suboptimal, as contamination is not easy to estimate. The scores of section 3.2 empirically produce a smooth distribution for clean samples and relegate contaminated ones to significantly lower values. Depending on the contaminated data distribution, it may then be possible to isolate problematic samples with statistical arguments based on two robust hyperparameters, the contamination rate guess \(\)and the significance level \(q\), as detailed in appendix K. In short, we first use a logit transformation to induce a gap between scores of normal and problematic samples. We then set an upper bound for the left tail of the score distribution using a logistic functional form, and estimate its parameters using quantiles. Afterward, we identify issues based on their violation of the upper probability bound.

## 4 Experimental setup

**Datasets.** We experiment on a total of twelve datasets described in appendix D. These are four large-scale vision benchmarks: ImageNet , STL-10 , CelebA , and Food-101N , three general medical datasets of X-rays and histopathological images: CheXpert , VinDr-BodyPartXR , and PatchCamelyon , and five dermatology datasets: HAM10000 , ISIC-2019 , Fitzpatrick17k , DDI , and PAD-UFES-20 .

**Evaluation metrics.** The evaluation in this work relies on ranking metrics, as ranking constitutes the core of SelfClean independently of the operation mode. All approaches are therefore evaluated in terms of the area under the receiver operating characteristic curve (AUROC) and average precision (AP) following standard practice . AUROC measures the likelihood that a random relevant sample is ranked higher than a random irrelevant sample. AP measures precision across all values of recall, and is therefore sensitive to the proportion of positive and negative samples.

**Synthetic experiment setup.** To compare SelfClean against other methods, we create synthetic datasets by altering benchmarks of different modalities (i.e., STL-10, VinDr-BodyPartXR, and DDI), as illustrated in figure 2. These synthetic contaminations are inspired by typical issues present in the respective dataset domains. We consider 5% and 10% contamination to mimic real-world noise prevalence estimates . For each issue type, we compare against other unsupervised methods that have performed well on the given task. A detailed description of these competing approaches can be found in appendix E. Since SelfClean learns representations on the contaminated dataset, we train a separate encoder for every issue type, contamination level, and synthetic contamination strategy.

The first synthetic contamination strategy for off-topic samples, _XR_, adds images from the "other" category of VinDr-BodyPartXR , which shows scans of lower limbs and device malfunctions. The second strategy for off-topic samples, _BLUR_, corrupts images with strong Gaussian blurring to simulate badly out-of-focus pictures. The first contamination strategy for near duplicates, _AUG_, adds samples from the original dataset after augmenting them with rotation, flipping, resizing, padding, and blurring. The second approach for near duplicates, _ARTE_, adds samples from the original dataset after including artifacts such as watermarks, color bars, and rulers, followed by scaling and composition with other images to create a collage. For label errors, the first contamination strategy, _LBL_, randomly changes a fraction of the labels choosing uniformly from incorrect ones. The second strategy to evaluate label errors, _LBLC_, randomly changes a fraction of the labels choosing incorrect ones proportionally to class prevalence in the original dataset. Depending on which dataset these strategies are applied to, they produce either easy or difficult problematic samples.

Different contamination strategies can be applied sequentially to create a dataset with a more realistic constellation of artificial data quality issues, resulting in a mixed-contamination strategy. In order to consider all interactions, we start by adding off-topic samples, proceed by creating near duplicates, and finally introduce label errors. To preserve the overall contamination rate \(C\), each contamination in the sequence is added with prevalence \(C_{S}\) such that \((1+C_{S})^{S}=(1+C)\), where \(S\) is the number of contamination steps.

Figure 2: Illustration of synthetic data quality issues of all three types in STL-10, VinDR, and DDI.

[MISSING_PAGE_FAIL:6]

label error detection on STL, SelfClean performs significantly better with INet features than with DINO features, presumably because INet features are trained with supervision on data and labels similar to STL.

### Natural contamination

**Comparison with metadata.** We validate the label error ranking in a more realistic setting using annotations from the literature, such as 5,440 verified samples of ImageNet's validation set  and 57,608 of Food-101N . SelfClean achieves almost double the performance in AP for both datasets compared to other approaches, with 8.4% vs. 4.3% AP for ImageNet and 47.8% vs. 30.7% for Food-101N. We evaluate near-duplicate detection against CelebA labels that indicate images of the same celebrity. SelfClean achieves 30.9% AP, demonstrating it effectively learned facial recognition without supervision. For medical datasets, we first check how well SelfClean can find pairs of images showing the same skin lesion. We obtain good correspondence for HAM10000 and ISIC-2019, with an AP of 28.4% and 26.6%, respectively. On the other hand, for PAD-UFES-20 AP is only 10.0%, which we further investigate in appendix G.2 and is likely caused by inaccurate metadata. We also attempt to identify X-rays from the same patient within CheXpert and find only minor agreement with 7.5% AP, suggesting again that a case-by-case investigation should be performed. Overall, this shows that the rankings produced by SelfClean align with existing metadata and considerably outperform competitors. A table with detailed results can be found in appendix G.2.

**Comparison with human annotators.** We evaluate SelfClean rankings against human verification across two common vision and two medical benchmarks as described in appendix I. Human experts confirmed significantly more data quality issues in the top 50 images ranked by SelfClean compared to 50 randomly sampled images, with 95% significance in nine out of twelve tasks (table 15). We repeat the comparison for images ranked 1-25 against images ranked 26-50 and observe significance for six out of ten evaluations. Two cases in the second comparison are excluded as only containing positive samples (i.e., data quality issues) results in undefined metrics. These results indicate that SelfClean rankings align well with human assessment for these three issue types.

Figure 3: Performance of the best two approaches for each issue type to SelfClean across different representations for a mixed-contamination strategy at varying contamination rates. Gray regions indicate random performance with an AP equal to the respective contamination \(C_{S}\).

### Influence of representation learning

Table 2 examines the influence of SSL objective, dataset, and augmentation on SelfClean by measuring performance on STL. In the upper panel, we observe that dataset-specific representations (DINO STL) yield the best results for both off-topic and near duplicate detection, showcasing the strength of learning the dataset context. This is remarkable considering that STL has only 5,000 samples compared to the 1 million available for ImageNet. Label error detection seems instead to benefit from the larger data volume of ImageNet. However, this amount of data is not always available with so little domain shift, and dataset-specific representations strike a good trade-off. The lower panel investigates the influence of augmentation during pre-training. For DINO, removing color and size or multi-crop augmentations, the model loses its ability to reliably detect some issue types, in particular off-topic samples. For SimCLR, adding multi-crop substantially improves data cleaning performance. Interestingly, adding color and size augmentations alongside multi-crop seems to have a negative influence on near-duplicate detection, while isolating off-topic samples and label errors well.

In appendix F, we further demonstrate that it is important to pre-train for sufficient epochs and to either normalize embeddings or use the cosine distance. We also find that DINO works best among four SSL objectives and investigate the effectiveness of different backbones. Finally, we show that label-error detection deteriorates with label granularity, but SelfClean stays on par with other methods.

## 6 Discussion

**Application to benchmark datasets.** We apply the fully automatic mode of SelfClean to well-known image benchmark datasets and estimate the prevalence of data quality issues. For the estimation, we used conservative guesses of a contamination rate of \(=0.10\) and a significance level of \(q=0.05\). Detailed results can be found in appendix L.1. For highly curated datasets with extensive manual verification, such as DDI, PAD-UFES-20, HAM10000, CheXpert, and ImageNet-1k, we find noise levels below 1%. However, for ISIC-2019 and PatchCamelyon, we estimate 5.4% and 3.9% of near duplicates that are not accounted for in the metadata. When considering datasets with less manual curation, such as Fitzpatrick17k, CelebA, and Food-101N, we find less than 1% of off-topic samples and label errors, and approximately \(14.8\%\), \(0.4\%\), and \(1.4\%\) near duplicates, respectively. The abundance of near duplicates in these benchmarks can often be traced back to crawling data of different pages using the same illustration or thumbnail images. When data splits with near-duplicate data leaks are used, performance estimates on these datasets are optimistically biased.

**Influence of dataset cleaning.** In table 3 we examine the impact of cleaning data quality issues to better understand their relevance. We train linear and \(k\)NN classifiers based on dataset-specific SSL representations for multiple classification benchmarks and measure the performance difference in F1 score when removing the problematic samples found above, first from the evaluation set and then also from the training set. For most benchmark datasets, cleaning the evaluation set significantly alters scores. Variations are either positive or negative depending on whether wrong samples were misclassified, and larger for datasets with significant data leaks. Cleaning the training set has a significant positive impact for many benchmarks, indicating that issues in the training set hindered optimization.

   Pre-training strategy & OT (\%) & ND (\%) & LE (\%) \\  SelfClean (Sup. Net) & 1.6 & 24.6 & 63.0 \\ SelfClean (DINO INet) & 13.7 & 6.1 & **69.5** \\ SelfClean (DINO STL) & **27.4** & **47.1** & 24.8 \\   Color+Size & Multi-Crop & OT (\%) & ND (\%) & LE (\%) \\  SelfClean (DINO) & & & & \\ ✓ & ✓ & **27.4** & 47.1 & 24.8 \\ ✓ & ✗ & 2.8 & 17.5 & **39.6** \\ ✗ & ✓ & 4.2 & **67.2** & 12.6 \\  SelfClean (SimCLR) & & & & \\ ✓ & ✗ & **39.1** & 12.8 & **18.1** \\ ✓ & ✗ & 26.1 & 12.1 & 15.8 \\ ✗ & ✗ & 3.9 & **21.9** & 11.7 \\   

Table 2: Ablation of pre-training choices in SelfClean. The upper part investigates SSL objective and dataset, and the lower the influence of SSL augmentations. For the different variants (lower part), we highlight the differences from the default setting. We use a 10% mixed-containment dataset starting from STL and creating off-topic samples (OT) using XR, near duplicates (ND) using AUG, and label errors (LE) using LBLC. Performance is reported in average precision (AP).

The importance of each individual data quality issue type depends on the dataset and task, and identifying trends by domain and modality requires further investigation. For the limited number of cases in Table 3, and taking into account Table 16, data leaks caused by near duplicates across splits seem to have the highest impact, followed by label errors. However, we argue that information on off-topic samples and near duplicates within the same data split is always valuable, even if it only serves the purpose of restoring trust.

**Recommended use.** SelfClean determines context based on the dataset rather than a specific task, so the candidates it provides for correction may represent desired features (e.g., rare diseases or longitudinal data). The identification of a data quality issue should not be automatically considered a suggestion to remove it. Instead, discovering relationships among samples is always an advantage, as it can inform proper action. While undesirable behavior may occur with the automatic mode, this is similar to other cleaning methods applied without checks, and such biases can be mitigated with the human-in-the-loop approach.

The tension between correcting data quality issues and the veto against the examination of evaluation data, mentioned in the introduction, has no easy resolution. We suggest the following compromise as an improvement to the current practice. A benchmark dataset should be refined using an SSL model developed on the training set. SelfClean can be used to clean both training and evaluation sets, but for the latter the human-in-the-loop mode is required, and labels should not be altered. The number of problems found for each set separately and across them for near duplicates should be reported. Even with human confirmation and refraining from correcting label errors, the cleaning procedure introduces some degree of bias due to the sampling of the candidate issues to be confirmed. We believe that in many practical cases, the benefit of data cleaning outweighs this bias.

## 7 Conclusion and outlook

We found a data-cleaning strategy called SelfClean, based on dataset-specific self-supervised learning and local, distance-based indicator functions, to be effective for detecting off-topic samples, near duplicates, and label errors. We demonstrated this by comparing to state-of-the-art methods across multiple general vision and medical image benchmarks both with synthetic issues and with natural contamination. SelfClean outperformed competing approaches for synthetic data quality issues, and demonstrated superior correspondence to metadata and expert verification in natural settings. Notably, the detailed methodology surpassed the state-of-the-art in label-error detection, achieving a twofold increase in AP over existing approaches on known ImageNet-1k and Food-101N issues. Moreover, applying the cleaning strategy to highly curated medical datasets and general vision benchmarks revealed multiple data quality issues with significant impact on model scores. By correcting these data collections, confidence can be regained in reported benchmark performances. In the future, we plan to incorporate SelfClean during annotation to collect higher quality datasets and during inference to enhance model robustness.

    &  &  \\ 
**Dataset** & Clean Eval & Clean Train & Clean Eval & Clean Train \\  DDI & \(+1.2^{+1.9}_{-1.2}\)\({}^{***}_{-1.4}\) & \(+0.0^{+1.7}_{-1.4}\)\({}^{***}_{-1.4}\) & \(+1.0^{+11.1}_{-11.2}\) & \(-0.7^{+7.7}_{-10.8}\) \\ HAM10000 & \(+0.2^{+0.5}_{-0.4}\)\({}^{***}_{-1.4}\) & \(+0.2^{+1.3}_{-0.8}\)\({}^{**}_{-1.3}\)\({}^{**}_{-3.5}\) & \(-0.1^{+3.9}_{-3.6}\) \\ Fitzpatrick17k & \(-4.1^{+1.2}_{-1.3}\)\({}^{***}_{-1.7}\) & \(+0.1^{+2.0}_{-1.7}\) & \(-0.6^{+2.9}_{-3.6}\)\({}^{**}_{-3.6}\) & \(+0.2^{+3.9}_{-3.9}\) \\ ImageNet-1k & \(-0.4^{+0.1}_{-0.1}\)\({}^{***}_{-1.4}\) & \(+0.4^{+0.3}_{-0.2}\)\({}^{***}_{-1.4}\) & \(-0.4^{+0.6}_{-0.6}\)\({}^{***}_{-0.0}\) & \(-0.0^{+0.5}_{-0.5}\) \\ Food-101N & \(+0.1^{+0.7}_{-0.1}\)\({}^{***}_{-1.0}\) & \(+0.1^{+0.2}_{-0.2}\)\({}^{***}_{-0.5}\) & \(+0.2^{+0.6}_{-0.5}\)\({}^{***}_{-0.1}\) & \(+0.1^{+0.6}_{-0.5}\)\({}^{**}\) \\   

Table 3: Influence of removing samples detected in the automatic cleaning mode with \(=0.10\) and \(q=0.05\) on downstream tasks. We report macro-averaged F1 scores for linear and \(k\)NN classifiers on DINO features over 100 random training/evaluation splits with 80% and 20% fractions, respectively. We compute paired performance differences before and after cleaning the evaluation set, and before and after cleaning also the training set. We report the median and the intervals to the 5% (subscript) and 95% (superscript) percentiles. Additionally, we indicate significance of a paired permutation test on the difference sign with \({}^{*}p<0.05\), \({}^{**}p<0.01\), and \({}^{***}p<0.001\).