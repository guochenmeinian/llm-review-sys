# On the Benefits of Public Representations

for Private Transfer Learning under Distribution Shift

Pratiksha Thaker

Carnegie Mellon University

pthaker@andrew.cmu.edu

&Amrith Setlur

Carnegie Mellon University

asetlur@andrew.cmu.edu

&Zhiwei Steven Wu

Carnegie Mellon University

zstevenwu@andrew.cmu.edu &Virginia Smith

Carnegie Mellon University

smithv@andrew.cmu.edu

&###### Abstract

Public pretraining is a promising approach to improve differentially private model training. However, recent work has noted that many positive research results studying this paradigm only consider in-distribution tasks, and may not apply to settings where there is distribution shift between the pretraining and finetuning data--a scenario that is likely when finetuning private tasks due to the sensitive nature of the data. In this work, we show empirically across three tasks that even in settings with large distribution shift, where both zero-shot performance from public data and training from scratch with private data give unusably weak results, public features can in fact improve private training accuracy by up to 67% over private training from scratch. We provide a theoretical explanation for this phenomenon, showing that if the public and private data share a low-dimensional representation, public representations can improve the sample complexity of private training even if it is _impossible_ to learn the private task from the public data alone. Altogether, our results provide evidence that public data can indeed make private training practical in realistic settings of extreme distribution shift.

## 1 Introduction

Learning models from user data can potentially disclose sensitive user information, violating privacy constraints . Differential privacy is a standard framework that can be used when learning models from sensitive data to mitigate the risk of leaking private information . However, differentially private learning may significantly degrade accuracy, which remains a barrier to adoption . This has motivated recent works to explore the benefits of incorporating publicly available data into private training, e.g., by pretraining a model on public data and then finetuning it using private data. Empirically, this paradigm has been shown to substantially improve performance on private tasks relative to fully-private training .

While these results are encouraging, Tramer et al.  point out that much of the existing work focuses on _in-distribution_ tasks, where the public and private tasks are very similar. For example, many private vision models  use public features pretrained on ImageNet , CIFAR-10 or CIFAR-100 , but these works also simulate private _transfer_ performance by finetuning on one of these datasets. In fact, Tramer et al.  point out that "_every single_ class contained in the CIFAR-10 dataset has an identical class label in the ImageNet dataset!" This is particularly problematic when attempting to understand the utility of public pretraining for private tasks, because in practice the private task is likely to contain sensitive data that is _not_ perfectly represented by public data, such as in applications in medicine  or law . Indeed, if data is already well-represented in a public dataset, the _zero-shot_performance of a model trained only on public data should be good enough that no private "transfer" learning is required, potentially making these benchmark datasets uninformative for evaluating the benefits of transfer learning.

From a practical perspective, it is particularly important to understand transfer learning in the private setting: if a _non_-privacy-sensitive task is poorly represented by the pretrained features, one solution might be to simply add the data from that task into the public training dataset and learn a more general set of features for downstream use. But privacy-sensitive data cannot be used to train a public backbone, and individual private datasets often cannot be combined or shared. Thus, the ability to leverage public features to improve the sample dependence of private learning is critical.

Our contributions.In this work, we provide evidence to alleviate these concerns, showing theoretically and empirically that public pretraining can be helpful even in settings with realistic and possibly extreme distribution shift between public (training) and private (transfer) tasks. In particular, we focus on concept shift, where the conditional distributions \(P(Y\,|\,X)\) can vary drastically between public and private tasks. Our results are summarized as follows.

First, we conduct empirical case studies1 on three datasets to show that public features improve private training accuracy even under extreme distribution shift. In particular, we use a pretrained CLIP ViT-B vision model for public features and measure the accuracy of private transfer learning on datasets including the PatchCamelyon (PCam) , Functional Map of the World (fMoW) , and Remote Sensing Image Scene Classification (RESISC45) . On all three datasets, the pretrained model has unacceptably low zero-shot accuracy (random guessing on both PCam and fMoW), indicating that "perfect privacy" with zero-shot queries is likely hopeless. In comparison, on CIFAR-10, the CLIP ViT-B/32 model achieves 91.3% zero-shot accuracy , making transfer learning performance far less relevant as the zero-shot accuracy is already high. We observe that across all datasets, private finetuning and linear probing using public features outperform differentially training from scratch - by up to 67%. In addition, private linear probing consistently outperforms private finetuning.

Motivated by our empirical results, we provide a stylized theoretical model to understand and explain our findings. We study a simple linear transfer learning model, a common theoretical model in the non-private meta-learning literature [28; 29; 30; 31; 32; 33; 34], to show the statistical benefit of learning a shared, low-dimensional _representation_ (in our model, a low-rank linear subspace) using public data. Our transfer learning model captures an extreme form of concept shift in the sense that the target model on private data is entirely different from those on public data, even though they are all contained in the same subspace. Analogous to the paradigm of public pre-training then private linear probing, we analyze a simple two-stage algorithm that (1) first estimates the shared, low-dimensional representation (or subspace) from a diverse set of tasks in public data, and (2) performs private linear regression within the learned subspace. By leveraging the dimensionality reduction, we provide a better sample complexity that scales with the rank of the shared subspace instead of the ambient dimension of the features. To complement this sample complexity bound, we also show a novel lower bound that shows that our bound is tight among algorithms that search for regression parameters within a fixed low-rank subspace estimate.

In short, our findings provide optimistic insights regarding the concerns raised by Tramer et al. . Specifically, Tramer et al.  suggest that "current methods for large-scale pretraining may be less effective." In contrast, our results indicate that pretrained features can indeed benefit private learning, even under concept shift. Additionally, our findings address another concern from Tramer et al.  regarding the necessity of uploading private data to cloud services for finetuning large models due to high resource requirements. We demonstrate that training a linear probe privately is more effective, potentially requiring significantly fewer resources (both memory and computation) than finetuning a full model.

## 2 Related Work

Empirical studies of public pretraining for private learning.As Tramer et al.  point out, existing empirical studies on public pretraining for private learning largely focus on transfer between similar datasets. For example, [15; 16; 17; 18; 19; 35] pretrain on CIFAR-100 or ImageNet and finetune on CIFAR-10 or STL-10 (a dataset very similar to CIFAR-10).  pretrains on Places365 and finetunes on ImageNet. [18; 36] pretrain on JFT and finetune on ImageNet. Finally, [37; 38; 9; 39] pretrain and finetune on publicly available text on the Web.

All of these works build evidence that pretraining could be beneficial for private learning. Unfortunately, because the public and private tasks are so similar, these results are unlikely to be representative of real-world private training in which the private task requires learning a model on sensitive data with a very different distribution from data available on the Web.

Recent work  evaluates their algorithm on private learning tasks that are out-of-distribution for the feature extractor they use, including the PCam dataset that we also study. However, their algorithm requires access to (nearly) in-distribution _public_ data in order to learn a projection matrix into a low-dimensional space. We argue that this is a strong and unrealistic assumption considering the arguments put forth in Tramer et al.  that private data, because of its sensitive nature, will not be well-represented by public datasets. Our work instead focuses on understanding the improvements from using off-the-shelf feature extractors, with no in-distribution public data, over fully-private learning.

Transfer or meta-learning.Our results build on the framework of Tripuraneni et al.  for nonprivate transfer learning with a low-dimensional subspace. This linear, low-dimensional subspace assumption has been studied extensively in the nonprivate meta-learning literature as a tractable model for real shared representation learning [28; 29; 30; 31; 32; 33; 34]. However, none of these works consider the setting of public subspace estimation followed by private transfer learning. PILLAR  makes a shared subspace assumption in the private setting, but on the input features rather than on the models.

Private algorithms that leverage public data.A number of prior works have theoretically studied the benefits of public data in other settings, including mean estimation , query release [42; 43; 44], and optimization when _gradients_ lie in a low-rank subspace [45; 46; 47]. Kairouz et al.  in particular gives a similar analysis using the principal angle error of the subspace, but the analysis does not apply directly as we assume that models, rather than gradients, lie in a shared low-dimensional subspace. As a result, the algorithm in that work requires expensive subspace oracle calls on every iteration and would be computationally suboptimal in our setting.

Finally, as discussed earlier, pretraining has empirically been shown to be useful in a number of domains, including vision [6; 7; 8] and NLP [9; 10; 11; 12]. While our work does not model the complexities of neural networks, we can understand our results as a stylized version of finetuning in which the public network is tuned with linear regression on the last layer, potentially giving insight into these more complex models.

Theoretical analyses of pretraining for private learning.Ganesh et al.  provides a lower bound construction for a related setting in which public data is abundant and the private task is out of distribution, though does not consider the case where the public and private task explicitly share structure. In our setting, learning from the public data alone provides no guarantees on the transfer task, as we do not assume any bounded shift in the data distributions or target parameters between the public tasks to the private tasks; the key information enabling more efficient learning is the shared structure among the tasks. PILLAR  incorporates public pretraining, but their analysis focuses on the benefits of dimensionality reduction using in-distribution public data, rather than transfer from out-of-distribution public data. Finally, Ke et al.  study the tradeoffs between linear probing and finetuning in the private setting. While their empirical results focus on the in-distribution image recognition settings outlined previously, their theoretical results corroborate our findings that even under extreme distribution shift, linear probing is more effective than finetuning under differential privacy.

## 3 Preliminaries

Notation.Throughout the paper, we use lower-case \(v\) for vectors, upper-case \(V\) for matrices and calligraphic \(\) for sets. Generally, we use the "hatted" notation \(\), \(\) to refer to estimates of the underlying population variables. The use of \(,,\) is standard and \(},\) hides \(\) factors in quantities we specify separately. We use \(\|\|_{F}\) for Frobenius, \(\|\|_{}\) for operator and \(\|\|_{p}\) for \(_{p}\) norms.

### Differential Privacy

Differential privacy (DP) is a quantitative constraint on the information gained from a released statistic . Definition 3.1 restates the standard \((,)\)-differential privacy introduced in .

**Definition 3.1** ( \((,)\)-differential privacy ).: _Given \( 0\), \(\) and a neighboring relation \(\), a randomized mechanism \(M\!:\!^{n}\) from the set of datasets of size \(n\) to an output space \(\) is \((,)\)-differentially private _if for all neighboring datasets \(^{}\), and all events \(E\),_

\[[() E] e^{}[( ^{}) E]+.\]

_Here, probabilities are taken over the random coins of \(\)._

The "neighboring" relation differs according to the desired privacy guarantee. In this paper, we will study _row-level_ privacy in which neighboring datasets \(^{}\) differ in a single element.

### Problem Setting: Leveraging public samples for private transfer learning

We will study a setting in which the learner first sees \(n_{1}\) public samples \((x_{i},y_{i})\), possibly drawn from multiple different underlying tasks (i.e., sample distributions) \(P_{1},...,P_{t}\), and then sees \(n_{2}\) private samples from a new task \(P_{t+1}\). The goal is to learn a predictor \(f:^{d}\) that maps inputs \(x^{d}\) to outputs \(y\) with the constraint that \(f\) must satisfy \((,)\)-differential privacy. We aim to minimize the population loss on the private task:

\[(f)=_{(x,y) P_{t+1}}[(f(x),y)]. \]

The private learner may or may not use the public samples. We assume the samples are drawn i.i.d. conditioned on the task, but make no other assumptions on the task distribution or the number of samples drawn from each task. In Section 5, we develop a theoretical model of the relationship between the public and private tasks that allows the learner to effectively leverage information from the public tasks to improve private learning.

## 4 Public Data Improves Out-of-Distribution Private Transfer

We begin by studying three datasets and show empirically that public data can provide benefits for private transfer learning even when the public data alone gives unusable zero-shot results on the private task. Each of the tasks we evaluate on has unusably low zero-shot performance on CLIP , indicating that these are highly out-of-distribution relative to the pretraining data. This directly contrasts with existing work: the CLIP model that we use (pretrained with LAION-2B) achieves 66.6% zero-shot performance on ImageNet and 93.5% accuracy on CIFAR-10.

### Datasets

PatchCamelyon.The PatchCamelyon (PCam) medical images dataset is a binary lymph node tumor detection task highlighted by .  point out that CLIP  as well as other similar text-vision models  have notably poor zero-shot performance on PCam: CLIP ViT-B/32 achieves 51.2%, or close to random, in our evaluation. The poor zero-shot performance (relative to tasks like ImageNet or CIFAR) indicates that the task is truly "out of distribution" in comparison to the source (public) data. Moreover, being medical image data, PCam more faithfully represents a highly privacy-sensitive dataset.

While the next two datasets are not medical image datasets, they are widely studied distribution shift datasets that have poor zero-shot performance on the training data, making them suitable for understanding transfer learning performance. In particular, they are remote sensing datasets; Wang et al.  analyze LAION-2B and find that only _0.03% of samples_ are remote sensing images, another strong indication that this data is underrepresented in pretraining.

fMoW.The Functional Map of the World (fMoW) dataset  is a 62-class satellite image classification task. The pretrained CLIP ViT-B model achieves only 1.64% zero-shot accuracy, so "perfect privacy" with zero-shot classification is not possible.

RESISC45.The Remote Sensing Image Scene Classification dataset  is a 45-class satellite image classification task. The pretrained CLIP ViT-B model achieves 56.3% zero-shot accuracy.

### Experimental Setup

We train a ViT-B/32 model  on each dataset (which has output dimension 512) with a linear classification head for each task. For models trained from scratch, we use Xavier initialization on the weights, while for pretrained features, we use OpenCLIP  models initialized with weights pretrained using LAION-2B (a 2B-sample subset of LAION-5B ). We use the Opacus library  to implement private training. For each training setting we performed a hyperparameter sweep over learning rate (\(\{1e-6,\)...,\(1e-2\}\)) and number of epochs (1-10 for full training and 1000-2500 for linear probing), and for private learning, clipping norm (\(\{0.5,1.0,2.5,5.0\}\)). For both private and nonprivate models, we evaluate training from scratch, full finetuning, and linear probing. We train private models for \(\{0.3,0.4,0.5,1.0,2.0,5.0\}\) for each training setting. For PCam and RESISC45, we use SGD with momentum (parameter 0.9), while for MoW we found that Adam gave better performance . We use a cosine learning rate schedule for all experiments and a batch size of 32. Each finetuning run is performed on an A100 or A6000 GPU.

### Results

We plot our private training results in Figure 1, and also provide nonprivate training and zero-shot CLIP numbers for reference in Table 1. Zero-shot CLIP has random accuracy on PCam (binary) and fMoW (62 classes). On RESISC45, zero-shot CLIP performs better than training from scratch (nonprivately), but finetuning and linear probing have nearly 40% higher accuracy. As pointed out by Tramer et al. , if the zero-shot numbers (with no knowledge of the transfer task) matched the best performance of finetuning, then "perfect privacy" with no finetuning would be sufficient. But in each of these settings, the zero-shot performance is considerably worse than what is achievable with finetuning in both the nonprivate and private settings.

Across all datasets, we find that any type of finetuning significantly outperforms training privately from scratch. This indicates that the pretrained features are indeed contributing to training accuracy. Further, we find across all datasets that linear probing (fixing the pretrained features) outperforms full finetuning, sometimes by a large margin, as in the case of RESISC45. This finding is consistent with theoretical work  that models

    & PCam & fMoW & RESISC45 \\  Zero-shot CLIP & 51.2 & 1.64 & 56.3 \\ Full training from scratch & 78.2 & 19.7 & 41.9 \\ Full finetuning & 82.5 & 58.2 & 93.6 \\ Linear probing & 83.5 & 42.1 & 91.7 \\   

Table 1: Test accuracy of nonprivate training on each dataset that we evaluate.

Figure 2: Linear probing results for ViT-B/32 pretrained on a 14M subset of Datacomp-1B and on LAION-2B. (Solid lines are LAION results while dashed lines are Datacomp results.) While the linear probing results in both settings outperform training from scratch, the worse accuracy on the Datacomp pretrained features are reflective of the lower-quality features from the smaller pretraining set.

the benefits of linear probing over finetuning under differential privacy. This is also consistent with earlier empirical findings on (in-distribution) private finetuning .

The key takeaway is positive: that features that work well for nonprivate transfer learning also benefit private transfer learning even when the distribution shift is large. While the conclusions are similar, these results are especially important in the private setting: training models from scratch with strong privacy is simply infeasible for many tasks, resulting in only around 10% test accuracy for fMoW and RESISC at small values of \(\).

To further support our results, we additionally evaluate linear probing for all three datasets with features pretrained on a 14M subset of Datacomp-1B  in Figure 2. The trends in this setting are the same and linear probing still outperforms private training from scratch on all datasets, but the smaller pretraining dataset leads to lower-quality features that impact the final accuracy of linear probing.

## 5 Theoretical Model

Our empirical results show that even when distribution shift is extreme, public pretraining can indeed improve the accuracy of private training. In order to explain this observation, we study a simplified linear regression setting in which the goal is to estimate regression parameters privately for a single, unseen private task. This setting has been studied extensively in the nonprivate meta-learning literature as a theoretically tractable model to explain results on larger models [28; 29; 30; 31; 32; 33; 34], and we propose a novel extension to the private setting that helps explain our empirical findings.

We show that if the regression parameters for the private task lie in a low-dimensional subspace that is shared with the public tasks, the learner can use the public data to efficiently estimate the low-dimensional subspace, project the private data into the subspace, and thus achieve private estimation error rates that match optimal private linear regression rates (up to constant factors) in \(k\) dimensions (rather than \(d\) dimensions), with an additive term that accounts for the error in estimating the subspace publicly. These results hold even when we make no assumptions on the relationship between the public and private task other than that they share the same low-dimensional subspace.

We additionally provide a novel lower bound that shows that the algorithm we analyze for our upper bound achieves the optimal rate among "two-stage" algorithms that estimate the transfer parameters within a fixed low-dimensional subspace.

How realistic is the shared subspace assumption?As mentioned, the theoretical model we analyze has been previously studied to explain meta-learning results in nonprivate settings. Nevertheless, one might ask how realistic the model is for the particular settings we study, especially the assumption of a low-rank subspace shared by both the training and transfer tasks.

As a step toward understanding whether this assumption holds in practice, we plotted the eigenspectrum of the feature covariance matrix computed after extracting features of PCam images from the CLIP ViT-B-32 pretrained model (Figure 3).

From these results, we see that the pretrained features are approximately low-rank for the out-of-distribution task PCam, yet a linear probe over these features achieves good (83.5%) performance (Table 1). The fact that the representation still gives good performance when only a linear layer is trained on top suggests that the data does fundamentally lie in or near the low-rank space that is identified by the pretrained model.

In Appendix A, we plot and see similar results for the fMoW and RESISC45 datasets, where linear probing is similarly successful (relative to full finetuning).

Figure 3: Eigenspectrum of feature covariance matrix for PCam features extracted from pretrained CLIP ViT-B/32 model.

### Model and preliminaries

We first describe our model of the data distribution for the private task, learning objective, any assumptions we make and results from prior works we use.

#### 5.1.1 Shared task structure

We consider linear regression models in which every observation \((x_{i},y_{i})\) for a given task is generated according to:

\[x_{i}(0, I_{d}),(0, 1)\] \[y_{i}=x_{i}^{}B_{t(i)}+_{i}. \]

The covariates \(x_{i}\) and noise \(\) are sampled i.i.d. Here, \(B^{d k}\) is an unknown, low rank \((k d)\) feature matrix with orthonormal columns. The matrix \(B\), and consequently the subspace spanned by its columns, is shared across all tasks in our problem setting. This includes both the public tasks that may be used to derive the initial estimate of \(B\), as well as the private tasks in single-task and multi-task transfer settings.

The task vectors \(_{j}\) are all assumed to lie in the true shared subspace \(B\). \(t(i)\) indexes the task \(_{j}\) for the covariate \(x_{i}\): public tasks are in \(_{1 t}\), and the transfer task is \(_{t+1}\). Note that the tasks are not random variables and we do not make distributional assumptions on the tasks for our results. In Appendix B we provide details on the requirements for the public tasks \(_{1 t}\) (and also refer the reader to Tripuraneni et al. ), but for now we simply require that the public tasks are sufficiently "diverse" within \(B\).

The learner sees \(n_{1}\) samples from the public tasks (in total across all tasks) and \(n_{2}\) samples drawn from the private task.

We are interested in learning \(w\) that minimizes the following population risk:

\[(w)=_{(x,y)}(x^{}w-y)^{2} \]

on the private task \(B_{t+1}\).

#### 5.1.2 Oracle for public subspace estimation

In stating our main results, we first assume access to an oracle that can output an orthonormal matrix \(^{d k}\) that is "close to" \(B\). We measure the distance between subspaces in terms of the principal angle distance, denoted \(\!(B,\!)=\!(,\!B)\) (see supplement and Tripuraneni et al.  for more discussion).

The following identities on \(\) will be useful:

**Lemma 5.1** (subspace estimation errors).: _The following inequalities are satisfied for matrices with orthonormal columns \(B,^{d k}\) (and when \(B,\) are swapped): \(\|(I-^{})B\|_{F}\|(I-^{})B\|_{}=(,\!B)\!\|(I-^{})B\|_{F}/\)._

Instantiating the oracle with public data.The following corollary characterizes the error incurred from estimating the underlying subspace from public data using the _method-of-moments_ estimator from Tripuraneni et al. . We state this bound for use in subsequent results but refer the reader to the supplement for the conditions required on public data in order to achieve this bound.

**Theorem 5.2** (, Theorem 3, simplified).: _Let \(A=(_{1},...,_{t})^{}\) be the public task matrix, \(=_{k}(A}{t})\), and \(=(^{}A)}{k}\) be the average condition number. If an equal number of samples is generated from each task, and \( O(1)\) and \(()\), then the error of the method-of-moments estimator (, Algorithm 1) is_

\[\!(,\!B)\!/n_{1}}. \]

_with probability at least \(1-O(n_{1}^{-100})\)._

We will refer to \((B,)\) as an upper bound on the error of the subspace estimation oracle. We give upper bounds with respect to \(\) and also instantiate the bounds with the upper bound from Theorem 5.2.

#### 5.1.3 Private linear regression in \(d\) dimensions

We use in our analysis a known upper bound for private linear regression in \(d\)-dimensions. Theorem 5.3 states an informal result from  that upper bounds the excess risk for a variant of DP-SGD  (see Appendix B for more details). Furthermore, results from  imply that this upper bound is tight.

**Theorem 5.3** (Corollary 11 from , simplified).: _Suppose we have \(n_{2}\) i.i.d. datapoints \((x_{i},y_{i})\), where \(x_{i}(0,I_{d})\) and \(y_{i}=x_{i}^{}w+_{i}\), and \(_{i}(0,^{2})\). Given sufficient private samples \(n_{2}\), there exists an \((,)\) private estimate \(_{}\) such that, with high probability:_

\[(_{})-(w)}{ n_{2}}1+} ^{2}}. \]

#### 5.2 Private transfer learning for a single task

Algorithm.Our proposed algorithm (Algorithm 1) first projects \(x\) into the estimated subspace \(_{}\), i.e., \(x_{}^{}x\), and then runs private linear regression in the \(k\)-dimensional subspace. This is analogous to linear probing in our experiments, which first uses the public encoder to compute a low-dimensional feature representation of the data and then learns a linear model using the features. While full finetuning of the model is also a common paradigm in the transfer learning literature, we point to  which shows that when the feature representation is sufficiently informative, linear probing outperforms finetuning under differential privacy - a result that supports our empirical findings.

The following theorem states that Algorithm 1 achieves a rate that matches optimal rates for private linear regression in \(k\)-dimensions, up to the subspace estimation error \(\).

**Theorem 5.4** (single-task private transfer upper bound).: _Assume we have access to a subspace estimation oracle that solely uses public samples to provide estimate \(_{}\) for the unknown subspace \(B\) of a private task defined by the pair \((B,_{t+1})\) in (2). Further, the estimate satisfies \((_{},B)\). Given \(n_{2}\) i.i.d. samples from the distribution of this private task, Algorithm 1 outputs an estimate \(_{}_{t+1}\) that is \((,)\)-differentially private, and with high probability incurs a risk of:_

\[(_{}_{t+1})- (B_{t+1}) \] \[\,\|_{t+1}\|_{2}^{2}(^{2}+1) ^{100}}+}+ (1/)}{n_{2}^{2}^{2}}+^{2}. \]

Proof sketch.: The proof nearly follows from existing bounds on subspace estimation and private linear regression. The key difficulty is that regression on the input \(x(0,I_{d})\) projected into the estimated subspace \(_{}\) still leaves the residual that does not lie in \(_{}\), which can be treated as a noise term if we can show that the residual is independent of the projected \(x\). We can show this because \(_{}\) is orthogonal to \(_{}^{}\) (spans null space of \(_{}\)), so under the i.i.d. Gaussian assumption on \(x\), the residual is independent of the projected \(x\). As a result, we obtain the private linear regression rate in \(k\) dimensions with a variance of \(1+^{2}\) rather than 1 and an additive \(^{2}\) bias.

Discussion.From Theorem 5.4, we can break down the errors into an unavoidable bias due to the subspace estimation error (dependent only on the number of public samples) and the subsequent linear regression error due to privacy. For a subspace estimation error \(\) we require \(n_{1}}{^{2}}\). Given this inevitable error we can hope to achieve an accuracy of \(+^{2}\) where \(\) is the additional linearregression error and \((B,_{})\). This requires approximately:

\[n_{2}}+}} \]

samples. That is, if the subspace estimation error is zero then we achieve the rate of private linear regression in \(k\) dimensions, and consequently optimal non-private rates when \(\).

### Lower bound for two-phase estimator

In the previous subsection, we proved an upper bound on the single-task transfer for row-level \((,)\)-DP private algorithm, when the publicly estimated subspace \(_{}\) is \(\) accurate. In this section, we show that our upper bound is tight among algorithms for our problem that search for solutions within a fixed subspace.

In particular, we analyze the lowest possible transfer error achieved by any \((,)\)-DP algorithm that: (i) takes as input private dataset \(\) of \(n_{2}\) i.i.d. samples from task \(_{t+1}\), \(\)-accurate public estimate \(_{}\)- and (ii) outputs an estimate in the column space of \(_{}\). In Theorem 5.5, we present a lower bound on the risk suffered by any algorithm in such a class.

**Theorem 5.5** (Two-stage single-task private transfer lower bound).: _Let \(M\) be an \((,)\)-DP private algorithm where \((0,1)\), \(<}{{n^{1+}}}\), \(>0\), that takes as input: (i) publicly estimated subspace \(_{}\) from an oracle that only uses public samples; and (ii) a dataset \(\) of \(n_{2}\) private samples. For any such \(M\), there exists a private problem instance given by the pair \((B,_{t+1})\) where \(B_{k,d}(),_{t+1}^{k}\), \((B,_{})\), and \(\|B_{t+1}\|_{2} 1\), such that for \(S\) sampled i.i.d. from this instance using the model in (2), we have:_

\[_{M}_{|B,_{t+1}}_{ (x,y)|B,_{t+1}}(y-M(,_{})^{}x)^{2} \] \[=\,}{n_{2}^{2}^{2}}+ }(^{2}+^{2})+^{2}. \]

Proof Sketch.: Our proof relies mainly on tracing attacks in [59; 58], but our analysis additionally needs to handle the misspecification of the subspace \(B\) which influences the construction of the worst case problem instance. When we project inputs \(x_{}^{}x\), we can show that the projected samples can now be treated as i.i.d. samples from a \(k\)-dimensional linear regression model with independent noise. For a fixed \(_{}\), any choice of \(B,_{t+1}\) affects both the scaling of the noise (\(\|(I-_{}_{}^{})B_{t+1} \|_{2}^{2}\)), and the direction of the regression vector, based on how much of the true parameter \(B_{t+1}\) is captured in given subspace \(_{}\). To handle this, we first construct subclasses of the adversary, where each subclass fixes the norm of \(\|_{}^{}B_{t+1}\|_{2}\). Then, we lower bound the minimax risk over this subclass by via a Bayes risk which we further lower bound by constructing a _tracing adversary_.

We show that there exists a prior \(\) over \(B_{t+1}\) where the probability of the intersection of the following two events is very low: (i) small estimation error \(_{}(M(,_{}))\), and (ii) small success rate for the tracing adversary to infer the membership of some element in \(\). Since, \(M\) has to be \((,)\) private, this results in a Bayes risk lower bound.

Discussion.Our lower bound for the class of two-stage algorithms matches our upper bound in Theorem 5.4. This implies that our Algorithm 1 is optimal when \(_{}\) is the estimate given by the optimal subspace estimation oracle over public samples. When we use Algorithm 1 from , the estimation error matches lower bounds (Theorem 5 in ) upto a factor of \(\).

### Simulated results

Finally, we complement the results in this section through a simulated empirical study matching the setup described in Section 5.1.

**Setup.** We simulate \(n_{1}\) samples \((x_{i},y_{i})\) from \(t=100\) public tasks where the true dimension \(d=25\) but the underlying subspace \(B\) has rank 5. As baselines, we compare against nonprivate linear regression, DP-SGD without a subspace estimate, and DP-SGD initialized with the true subspace \(B\), and compare against DP-SGD initialized with the subspace estimated using the method-of-moments estimator . We use the Google Tensorflow implementation of DP-SGD for private learning .

We used a grid search of hyperparameters to set the clipping norm to \(0.5\), learning rate to 0.1, and used \(50\) epochs of training for DP-SGD. We use the RDP accountant to set \(=1.1\) and \(=1-5\).

Our results are shown in Figure 4. We observe that, as expected, private training from scratch has high error, and additional public data (\(n_{1}=500\) vs \(n_{1}=2000\)) improves performance, reducing the \(_{2}\) parameter error close to that of using DP-SGD with the true underlying subspace B (matching our intuition, for example, from Figure 2). However, we also see that when performing private transfer there are diminishing returns for this more precise subspace estimation, as the noise introduced via private learning becomes a dominating factor.

## 6 Discussion and Limitations

Our results answer questions posed by  positively. Empirically, we show that across three datasets with significant shift between the public and private tasks, publicly pretrained features _do_ make private learning far more effective, taking models from unusable when trained from scratch to close-to-nonprivate performance when trained privately with linear probing. In addition, we provide a theoretical model to explain our findings, based on models of nonprivate transfer learning. Our model supports our empirical findings, suggesting that public features should indeed reduce private sample complexity under even extreme distribution shift when the public and private tasks share a low-dimensional representation. Altogether, our conclusions are optimistic and provide confidence that public data can indeed support private training even for highly sensitive tasks that cannot and should not be used in public training. However, our linear subspace model has the clear limitation of being a simplified model for the neural network representations used in practice. As this is a limitation shared by literature on nonprivate transfer learning [28; 29; 30; 31; 32; 33; 34], improvements in this area would contribute to both the private and nonprivate transfer learning literature.

Acknowledgements.Thanks to Shengyuan Hu, Tian Li, Qi Pang, and Anirudh Sivaraman for helpful discussions and feedback that improved the writing. This work was supported in part by the National Science Foundation grants IIS2145670 and CCF2107024, and funding from Amazon, Apple, Google, Intel, Meta, and the CyLab Security and Privacy Institute. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of any of these funding agencies. Z.S.W. was in part supported by NSF Awards #1763786 and #2339775.