# Fine-Tuning Language Models with Just Forward Passes

Sadhika Malladi

Equal contribution and corresponding authors.

Tianyu Gao1

Eshaan Nichani

Alex Damian

Jason D. Lee

Danqi Chen

Sanjeev Arora

Princeton University

{smalladi, tianyug, eshnich, ad27, jasonlee, danqic, arora}@princeton.edu

###### Abstract

Fine-tuning language models (LMs) has yielded success on diverse downstream tasks, but as LMs grow in size, backpropagation requires a prohibitively large amount of memory. Zeroth-order (ZO) methods can in principle estimate gradients using only two forward passes but are theorized to be catastrophically slow for optimizing large models. In this work, we propose a memory-efficient zeroth-order optimizer (**MeZO**), adapting the classical ZO-SGD method to operate in-place, thereby fine-tuning LMs with _the same memory footprint as inference_. For example, with a single A100 80GB GPU, MeZO can train a 30-billion parameter model, whereas fine-tuning with backpropagation can train only a 2.7B LM with the same budget. We conduct comprehensive experiments across model types (masked and autoregressive LMs), model scales (up to 66B), and downstream tasks (classification, multiple-choice, and generation). Our results demonstrate that (1) MeZO significantly outperforms in-context learning and linear probing; (2) MeZO achieves comparable performance to fine-tuning with backpropagation across multiple tasks, with up to 12\(\) memory reduction and up to 2\(\) GPU-hour reduction in our implementation; (3) MeZO is compatible with both full-parameter and parameter-efficient tuning techniques such as LoRA and prefix tuning; (4) MeZO can effectively optimize non-differentiable objectives (e.g., maximizing accuracy or F1). We support our empirical findings with theoretical insights, highlighting how adequate pre-training and task prompts enable MeZO to fine-tune huge models, despite classical ZO analyses suggesting otherwise.1

## 1 Introduction

Fine-tuning pre-trained language models (LMs) has been the dominant methodology for solving many language tasks , adapting to specialized domains , or incorporating human instructions and preferences . However, as LMs are scaled up , computing gradients for backpropagation requires a prohibitive amount of memory - in our test, up to \(12\) the memory required for inference - because it needs to cache activations during the forward pass, gradients during the backward pass, and, in the case of Adam , also store gradient history (see Section 3.4 for a detailed analysis).

As a result, while it is possible to run inference with a 30-billion (30B) parameter LM on a single Nvidia A100 GPU (with 80GB memory), backpropagation with Adam is feasible only for a 2.7B LM. Parameter-efficient fine-tuning methods (PEFT ) update just a fraction of the networkparameters, but still need to cache many activations, because the tuned parameters are scattered throughout the model. In our tests, fine-tuning an OPT-13B model with full parameter tuning or PEFT requires \(12\) and \(6\) more memory than inference respectively.

_In-context learning_ (ICL ) has allowed solving many tasks with a single inference pass, during which the model processes labeled examples (_demonstrations_) in its context and then outputs a prediction on a test example. While this allows for quick adaptation of the model to specific use cases, current models allow a limited context size (and thus, limited demonstrations) and the performance is sensitive to the formatting and choice of demonstrations . ICL can slow with the number of demonstrations, and it often performs worse than fine-tuning of medium-sized models .

Backpropagation also cannot optimize non-differentiable criteria, which have gained popularity in fine-tuning LMs according to human preference scores or set safety standards . Typically, these adaptations involve expensive reinforcement learning from human feedback (RLHF ).

A classical zeroth-order optimization method, ZO-SGD , uses only differences of loss values to estimate the gradients. Thus, in principle, the method can update neural networks with just forward passes, though naive implementation still doubles the memory overhead and classical lower bounds  suggest that convergence slows linearly with model size. As such, ZO methods have been applied in deep learning settings to find adversarial examples or tune input embeddings  but not to directly optimize large-scale models (see Liu et al.  for a survey).

In this work, we propose a memory-efficient zeroth-order optimizer (MeZO), which adapts the classical ZO-SGD algorithm and reduces its memory consumption _to the same as inference_. We apply MeZO to fine-tune large LMs and show that, both empirically and theoretically, MeZO can successfully optimize LMs with billions of parameters. Specifically, our contributions are:

1. In MeZO, we adapt the ZO-SGD algorithm  and a number of variants to operate in-place on arbitrarily large models with almost no memory overhead (see Algorithm 1 and Section 2).
2. We conduct comprehensive experiments across model types (masked LM and autoregressive LM), model scales (from 350M to 66B), and downstream tasks (classification, multiple-choice, and generation). MeZO consistently outperforms zero-shot, ICL, and linear probing. Moreover, with RoBERTa-large, MeZO achieves performance close to standard fine-tuning within 5% gap; with OPT-13B, MeZO outperforms or performs comparably to fine-tuning on 7 out of 11 tasks, despite requiring roughly \(12\) less memory (Figure 1 and Section 3). In our implementation, MeZO requires only half as many GPU-hours as Adam fine-tuning for a 30B model (see Appendix F.6).
3. We demonstrate MeZO's compatibility with full-parameter tuning and PEFT (e.g., LoRA  and prefix-tuning ) in Section 3.
4. Further exploration showcases that MeZO can optimize non-differentiable objectives such as accuracy or F1 score, while still requiring only the same memory as inference (Section 3.3).
5. Our theory suggests that adequate pre-training ensures the per-step optimization rate (Theorem 1) and global convergence rate (Lemma 3) of MeZO depend on a certain condition number of the landscape (i.e., the local effective rank, see Assumption 1) instead of numbers of parameters. This

Figure 1: OPT-13B results with zero-shot, in-context learning (ICL), MeZO (we report the best among MeZO/MeZO (LoRA)/MeZO (prefix)), and fine-tuning with Adam (FT). MeZO demonstrates superior results over zero-shot and ICL and performs on par with FT (within 1%) on 7 out of 11 tasks, despite using only 1/12 memory. See Table 1 for detailed numbers and Figure 3 for memory profiling.

result is in sharp contrast to existing ZO lower bounds [69; 32] suggesting that the convergence rate can slow proportionally to the number of parameters (Section 4).

## 2 Zeroth-order optimization

Zeroth-order (ZO) optimizers have long been studied in the context of convex and strongly convex objectives. In the following, we first introduce a classical ZO gradient estimator, SPSA (Definition 1 ) and the corresponding SGD algorithm, ZO-SGD (Definition 2). Then we describe MeZO, our in-place implementation that requires the same memory as inference in Section 2.1 and Algorithm 1. We highlight that SPSA can also be used in more complex optimizers, such as Adam, and we provide memory-efficient implementations for those algorithms too (Section 2.2).

``` \(^{d}\), loss \(:^{d}\), step budget \(T\), perturbation scale \(\), batch size \(B\), learning rate schedule \(\{_{t}\}\) for\(t=1,...,T\)do  Sample batch \(\) and random seed \(s\) \((,,s)\) \(_{+}(;)\) \((,-2 ,s)\) \(_{-}(;)\) \((, ,s)\)\(\) Reset parameters before descent projected\(\_\)grad\((_{+}-_{-})/(2)\)  Reset random number generator with seed \(s\)\(\) For sampling \(z\) for\(_{i}\)do \(z(0,1)\) \(_{i}_{i}-_{t}**z\)  end for  end for Subroutine PerturbParameters(\(\), \(\), \(s\))  Reset random number generator with seed \(s\)\(\) For sampling \(z\) for\(_{i}\)do \(z(0,1)\) \(_{i}_{i}+ z\)\(\) Modify parameters in place  end for return\(\) ```

**Algorithm 1**MeZO

Consider a labelled dataset \(=\{(_{i},_{i})\}_{i[||]}\) and a minibatch \(\) of size \(B\), we let \((;)\) denote the loss on the minibatch. We introduce a classical ZO gradient estimate in this setting.2

**Definition 1** (Simultaneous Perturbation Stochastic Approximation or SPSA ).: _Given a model with parameters \(^{d}\) and a loss function \(\), SPSA estimates the gradient on a minibatch \(\) as_

\[(;)=(+;)-( -;)}{2} {z}^{}(;)\] (1)

_where \(^{d}\) with \((0,_{d})\) and \(\) is the perturbation scale. The \(n\)-SPSA gradient estimate averages \((;)\) over \(n\) randomly sampled \(\)._

SPSA requires only _two forward passes_ through the model to compute the gradient estimate (for \(n\)-SPSA, each estimate requires \(2n\) forward passes). As \( 0\), the SPSA estimate can be understood as a rank-1 reconstruction of the gradient. During training, \(n\) can be treated as a hyperparameter and follow a schedule [11; 15], though in cursory experiments (Appendix A), \(n=1\) is the most efficient.

We use \(n=1\) as the default. It is widely known that the SPSA estimate can be used to replace the backpropagation gradient in any optimizer such as SGD.

**Definition 2** (Zo-Sgd).: _ZO-SGD is an optimizer with learning rate \(\) that updates parameters as \(_{t+1}=_{t}-(; _{t})\) where \(_{t}\) is the minibatch at time \(t\) and \(\) is the SPSA gradient estimate._

### Memory-efficient Zo-SGD (MeZO)

The vanilla ZO-SGD algorithm costs twice the memory of inference, as it needs to store \(^{d}\). We propose a memory-efficient implementation of ZO-SGD called **MeZO**, as illustrated in Algorithm 1. At each step, we first sample a random seed \(s\), and then for each of \(\)'s four uses in Algorithm 1, we reset the random number generator by \(s\) and _resample_ the relevant entry of \(\). Using this in-place implementation, MeZO has a memory footprint equivalent to the inference memory cost.

We note that Algorithm 1 describes perturbing each parameter separately, which may be time-consuming for large models. In practice, we can save time by perturbing an entire weight matrix instead of each scalar independently. This incurs an additional memory cost as large as the largest weight matrix; usually, this is the word embedding matrix (e.g., 0.86GB for OPT-66B).

Storage Efficiency of MeZO.Parameter-efficient fine-tuning (PEFT) techniques fine-tune just a fraction of the network parameters and have thus been proposed as a way to reduce the storage costs of fine-tuned model checkpoints. Fine-tuning with MeZO reduces the storage cost of the resulting checkpoint far more than popular PEFT techniques (e.g., LoRA  and prefix tuning ). We reconstruct the MeZO trajectory using a single seed, which spawns step-wise seeds to sample \(\), and the projected_grad at each step.3 As such, for fine-tuning a 66B model, MeZO requires saving the seed plus 20,000 (steps) \(\) 2 bytes, which is less than 0.1MB. LoRA fine-tunes 19M parameters and requires 38MB storage, and prefix tuning fine-tunes 6M parameters and requires 12MB storage.

### MeZO extensions

We note that SPSA is a popular ZO gradient estimator but not the only one. Many one-point gradient estimators have been proposed in past works [34; 87; 95], and using such estimators in place of SPSA would halve the training time. However, cursory experiments with one such promising estimator  reveal that these are not as efficient as SPSA when fixing the number of forward passes (Appendix B.5). As such, we implement MeZO with the SPSA estimator.

MeZO can also be combined with other gradient-based optimizers, including SGD with momentum or Adam. Though naive implementation would require additional memory to store the gradient moment estimates, MeZO-momentum and MeZO-Adam alleviate such overhead by recomputing the moving average of the gradients using saved past losses and \(\) (see Appendix B for a full discussion).

We also note that all of the coordinates of the SPSA gradient estimate have the same scale, but deep Transformers can have gradients of different scales for each layer [59; 61]. As such, we draw inspiration from layerwise adaptive optimizers [109; 110] to design several MeZO variants. cursory experiments showed that these algorithms are not more efficient (in terms of forward passes), but we nevertheless present them as potential optimizers for more complex objectives. See Appendix B.

Forward Auto-DifferentiationNote that \(^{}(;)\) is a Jacobian-vector product (JVP), which can be computed in parallel with an inference pass with excess memory consumption equivalent to that of the largest activation in the network . In this case, \(\) must be stored on the GPU in order to construct the gradient estimate, so this procedure requires slightly more than two times the memory needed for inference. We analyze this algorithm in detail in Appendix D. Note that using a non-zero \(\) in SPSA, which is not possible through the JVP method, may boost generalization by promoting a sharpness-minimizing term. Past works (e.g., Baydin et al. ) have also studied JVP-based training but achieved limited empirical success.

## 3 Experiments

Preliminary experiments (Appendix A) show that MeZO only works when using prompts [13; 84; 35]. Past works [83; 67] have demonstrated how the inclusion of a suitable prompt ensures the fine-tuning objective is closely related to the pre-training one. In Section 4, we extend these ideas to show how using a simple prompt simplifies the fine-tuning optimization procedure, thereby enabling zeroth order methods to work efficiently. All experiments below use prompts detailed in Appendix E.2. All fine-tuning with backpropagation (FT) experiments follow convention and use Adam, though we also report results when performing FT with SGD in Appendix F.

We conduct comprehensive experiments on both medium-sized masked LMs (RoBERTa-large, 350M ) and large autoregressive LMs (OPT-13B, 30B, 66B ) in few-shot and many-shot settings with prompts. We also explore both full-parameter tuning and PEFT including LoRA  and prefix-tuning  (see Appendix E.5 for details). We compare MeZO with zero-shot, in-context learning (ICL), linear-probing (LP), and fine-tuning with Adam (FT). MeZO uses substantially less memory than FT but requires significantly more training steps.

We first show that MeZO improves substantially over zero-shot, ICL, and LP across model types, sizes, and task types. Moreover, MeZO performs comparably to FT over a number of tasks, while drastically reducing the memory cost by, for example, 12\(\) on OPT-13B. Further experiments demonstrate that MeZO can optimize non-differentiable objectives, such as accuracy and F1 score (Section 3.3). We compare the memory consumption of ICL, FT, LP, and MeZO in Figures 3 and 4.

### Medium-sized masked language models

We conduct experiments with RoBERTa-large on sentiment classification, natural language inference, and topic classification tasks. We follow past works [35; 67] in studying the few-shot and many-shot settings, sampling \(k\) examples per class for \(k=16\) and \(k=512\) (details in Appendix E). We run MeZO for \(100\)K steps and fine-tuning for \(1000\) steps, noting that one MeZO step is substantially faster than one fine-tuning step (see Appendix F.6 for a comparison). We summarize the results from Figure 2 and Table 18 below.

MeZO works significantly better than zero-shot, linear probing, and other memory-equivalent methods.On all six diverse tasks, MeZO can optimize the pre-trained model and consistently perform better than zero-shot and linear probing. We also show for several tasks that MeZO can outperform another ZO algorithm, BBTv2 , by up to \(11\%\) absolute (Appendix F.4).4

With enough data, MeZO achieves comparable performance (up to 5% gap) to FT.MeZO achieves close-to-fine-tuning performance on \(k=16\), with some tasks only having 2% gaps. When using \(k=512\) data, the gap between MeZO and FT further reduced to within 5% across all tasks.

MeZO works well on both full-parameter tuning and PEFT.Full-parameter tuning (MeZO) and PEFT (MeZO with LoRA and prefix-tuning) achieve comparable performance, while MeZO (prefix)

Figure 2: Experiments on RoBERTa-large. We report zero-shot, linear probing (LP), and MeZO and fine-tuning (FT) with full parameter, LoRA, and prefix-tuning. MeZO outperforms zero-shot and LP and approaches FT (within 5% for \(k=512\)) with much less memory. Detailed numbers in Table 18.

sometimes outperforms MeZO. We also show in Appendix F.3 that the three variants converge at similar rates, agreeing with our theory in Section 4, which shows that MeZO converges at a rate independent of the number of parameters being optimized.

We show additional results with more FT and MeZO variants in Appendix F.1. We see that (1) ZO-Adam sometimes outperforms ZO-SGD but is not consistent across tasks; (2) LP and then MeZO, as suggested for fine-tuning , can sometimes improve the performance.

### Large autoregressive language models

With the promising results from RoBERTa-large, we extend MeZO to the OPT family , on a scale of 13B (Table 1), 30B, and 66B (Table 2). We select both SuperGLUE  tasks5 (including classification and multiple-choice) and generation tasks. We randomly sample \(1000\), \(500\), and \(1000\) examples for training, validation, and test, respectively, for each datset. We run MeZO for \(20\)K steps and fine-tuning for \(5\) epochs, or \(625\) steps, noting that each step of MeZO is substantially faster than fine-tuning (see Appendix F.6 for a comparison). Please refer to Appendix E for details. Table 1 yields the following observations.

**MeZO outperforms memory-equivalent methods and closely approaches fine-tuning results.** We see that on a 13B-parameter scale, MeZO and its PEFT variants outperform zero-shot, ICL, and LP across almost all tasks. When comparing to FT, which costs 12\(\) more memory (Section 3.4), MeZO achieves comparable (within 1%) or better performance on 7 out of the 11 tasks.

**MeZO exhibits strong performance across classification, multiple-choice, and generation tasks.** We investigate MeZO on generation tasks, which are regarded as more intricate than classification or multiple-choice tasks. We evaluate on two question answering datasets, SQuAD  and DROP . We use teacher forcing for training and greedy decoding for inference (details in Appendix E).

Table 1 shows that, on all generation tasks, MeZO outperforms zero-shot, ICL, and LP, and achieves comparable performance to FT. Considering that many applications of fine-tuning LMs - including instruction tuning or domain adaptation - target generation tasks, our results underscore the potential of MeZO as a memory-efficient technique to optimize large LMs for realistic and exciting applications.

   Task & **SST-2** & **RTE** & **CB** & **BoolQ** & **WSC** & **WIC** & **MultiRc** & **COPA** & **ReCoRD** & **SQuAD** & **DROP** \\ Task type &  & & &  &  \\  Zero-shot & 58.8 & 59.6 & 46.4 & 59.0 & 38.5 & 55.0 & 46.9 & 80.0 & 81.2 & 46.2 & 14.6 \\ ICL & 87.0 & 62.1 & 57.1 & 66.9 & 39.4 & 50.5 & 53.1 & 87.0 & **82.5** & 75.9 & 29.6 \\ LP & **93.4** & 68.6 & 67.9 & 59.3 & 63.5 & 60.2 & 63.5 & 55.0 & 27.1 & 3.7 & 11.1 \\  MeZO & 91.4 & 66.1 & 67.9 & 67.6 & 63.5 & **61.1** & 60.1 & **88.0** & 81.7 & **84.7** & 30.9 \\ MeZO (LoRA) & 89.6 & 67.9 & 66.1 & **73.8** & **64.4** & 59.7 & 61.5 & 87.0 & 81.4 & 83.8 & **31.4** \\ MeZO (prefix) & 90.7 & **70.8** & **69.6** & 73.1 & 57.7 & 59.9 & **63.7** & 84.0 & 81.2 & 84.2 & 28.9 \\  FT (12s memory) & 92.0 & 70.8 & 83.9 & 77.1 & 63.5 & 70.1 & 71.1 & 79.0 & 74.1 & 84.9 & 31.3 \\   

Table 1: Experiments on OPT-13B (with \(1000\) examples). ICL: in-context learning; LP: linear probing; FT: full fine-tuning with Adam. MeZO outperforms zero-shot, ICL, and LP across the board, and achieves comparable (within 1%) or better performance than FT on 7 out of 11 tasks.

   Task & **SST-2** & **RTE** & **BoolQ** & **WSC** & **WIC** & **SQuAD** \\ 
30B zero-shot & 56.7 & 52.0 & 39.1 & 38.5 & 50.2 & 46.5 \\
30B ICL & 81.9 & 66.8 & 66.2 & 56.7 & 51.3 & 78.0 \\
30B MeZO/MeZO (prefix) & **90.6** & **72.6** & **73.5** & **63.5** & **59.1** & **85.2** \\ 
66B zero-shot & 57.5 & **67.2** & 66.8 & 43.3 & 50.6 & 48.1 \\
66B ICL & 89.3 & 65.3 & 62.8 & 52.9 & 54.9 & 81.3 \\
66B MeZO/MeZO (prefix) & **93.6** & 66.4 & **73.7** & **63.5** & **58.9** & **85.0** \\   

Table 2: Experiments on OPT-30B and OPT-66B (with \(1000\) examples). We report the best of MeZO and MeZO (prefix). See Appendix F.2 for more results. We see that on most tasks MeZO effectively optimizes up to 66B models and outperforms zero-shot and ICL.

MeZO scales up to 66 billion parameter models.We demonstrate the efficacy of MeZO on even larger models, up to 66B, in Table 2. While directly fine-tuning models at such scales is extremely costly (Section 3.4), MeZO can effectively optimize these models and outperform zero-shot and ICL.

### Training with non-differentiable objectives

We demonstrate the efficacy of MeZO for optimizing non-differentiable objectives through initial experiments. Accuracy and F1 are used as the respective objectives (details in Appendix E.6). Table 3 reveals that MeZO with accuracy/F1 successfully optimizes LMs with superior performance to zero-shot. Although minimizing cross entropy results in stronger performance, these preliminary findings highlight the promising potential of applying MeZO to optimize non-differentiable objectives without clear differentiable surrogates, such as human preferences .

### Memory usage and wall-clock time analysis

In this section we profile the memory usage of zero-shot, ICL, FT, FT (prefix), and MeZO. We test OPT models of various sizes with Nvidia A100 GPUs (80GB memory) on MultiRC (average #tokens=400), and report the peak GPU memory consumption (details in Appendix E.7).

As shown in Figure 3 (refer to Appendix F.5 for detailed numbers), MeZO exhibits the same memory consumption as zero-shot while offering memory savings of up to \(12\) times compared to standard FT and \(6\) times compared to FT (prefix). This advantage enables training larger models within a fixed hardware budget, as illustrated in Figure 4. Specifically, using a single A100 GPU, MeZO allows for tuning a model that is \(11\) times larger than what is feasible with FT.

In Appendix F.6, we compare the wall-clock time efficiencies of our implementations of MeZO and Adam fine-tuning. MeZO achieves 7.74\(\) per-step speedup and requires \(8\) fewer GPUs with a 30B model, but takes more steps to converge. Overall, MeZO only requires half as many GPU-hours to fine-tune a 30B model compared to full-parameter fine-tuning. The wall-clock benefit of MeZO is not inherent to the algorithm and is highly dependent on the implementation. We primarily provide this information as a demonstration that MeZO does not take a prohibitively long time to run.

The above measurements are dependent on the computing infrastructure. In Appendix C, we compare the theoretical time-memory tradeoff of MeZO and backpropagation and find that MeZO is always more memory-efficient than backpropagation and is often more time-efficient. The above analyses also do not consider recent advances (e.g., gradient checkpointing , FlashAttention , and quantization ). We leave investigating the how MeZO works with these methods to future work.

Figure 4: Largest OPT models that one can tune with specific hardwares and algorithms. \(\) : projected results without actual testing.

   Model &  & OPT-13B \\ Task & **SST-2** & **SST-5** & **SNLI** & **TREC** & **SQuAD** \\  Zero-shot & 79.0 & 35.5 & 50.2 & 32.0 & 46.2 \\ Cross entropy (FT) & 93.9 & 55.9 & 88.7 & 97.3 & 84.2 \\ Cross entropy (MeZO) & 93.3 & 53.2 & 83.0 & 94.3 & 84.7 \\ Accuracy/F1 (MeZO) & 92.7 & 48.9 & 82.7 & 68.6 & 78.5 \\   

Table 3: MeZO with non-differentiable objectives. For classification (\(k=512\)), we use MeZO with full-parameter and optimize accuracy; for SQuAD (1,000 examples), we use MeZO (prefix) and F1.

Figure 3: GPU memory consumption with different OPT models and tuning methods on MultiRC (400 tokens per example on average).

Theory

Our theoretical analysis highlights why MeZO can optimize large LMs, although a number of classical results  suggest that optimization should be catastrophically slow when training so many parameters. The inclusion of a simple prompt is crucial for MeZO to succeed (Appendix A). Past works  have suggested that including such a prompt ensures that the fine-tuning objective is closely related to the pre-training one. As such, here, we make the assumption that the model has already been trained for many steps on the fine-tuning objective, which implies that the loss landscape exhibits favorable conditions (Assumption 1). Then, we derive a convergence rate independent of the number of parameters. We show that the loss decreases per step at a rate independent of the parameter dimension \(d\) (Theorem 1), and that, under stronger conditions, the algorithm converges in time independent of \(d\) (Lemma 3). Together, these results imply that MeZO is not catastrophically slower than SGD when fine-tuning.6 For ease of illustration, we assume that \(\) is sampled from a sphere with radius \(\), and in Appendix G.2, we derive the rate for a general Gaussian \(\), which was used in the experiments.

We follow classical analyses of SGD and replace the minibatch gradient estimate with SPSA (Definition 1). Consider the minibatch SGD update \(_{t+1}_{t}-(; _{t})\) where \(_{t}\) is a minibatch drawn uniformly from \(^{B}\). Crucially, the SGD minibatch gradient estimate is unbiased.

**Definition 3** (Unbiased Gradient Estimate).: _Any minibatch gradient estimate \((,)\) is said to be unbiased if \([(,)]=()\)._

### Per-step analysis

The classical descent lemma uses a Taylor expansion to study how SGD reduces the loss at each optimization step. It highlights that when the gradient covariance is large, the maximum possible decrease in loss at each optimization step is small, thereby resulting in slower optimization.

**Lemma 1** (Descent Lemma).: _Let \(()\) be \(\)-smooth.7 For any unbiased gradient estimate \((,)\),_

\[[(_{t+1})_{t}]-( {}_{t})-(_{t}) ^{2}+^{2}[(, _{t})^{2}].\] (2)

The descent lemma highlights the importance of the gradient norm, which we derive for MeZO below.

**Lemma 2**.: _Let \(\) be a random minibatch of size \(B\). Then, the gradient norm of MeZO is_

\[_{x}[(; )^{2}]=[ (;)^{2}].\]

_where \(n\) is the number of \(\) sampled in \(n\)-SPSA (Definition 1) and \(d\) is the number of parameters._

Thus, in the usual case where \(n d\), MeZO has a much larger gradient norm than SGD.8 The descent lemma also shows that to guarantee loss decrease, one needs to choose the learning rate as

\[(_{t})^{2}} {[(,)^{ 2}]}}}}}_{ }=_{}\] (3)

where \(_{}\) and \(_{}\) are the maximum permissible learning rates for MeZO and SGD respectively. Thus we see that without any further assumptions, MeZO can slow optimization by decreasing the largest permissible learning rate by a factor of \(d\). Moreover, MeZO reduces the loss decrease that can be obtained at each step and, as a consequence, slows convergence by a factor of \(d\) as well.

Surprisingly, our experiments show that MeZO can quickly optimize pre-trained models with billions of parameters, and reducing the number of tuned parameters via PEFT techniques does not substantially accelerate optimization (Appendix F.3). We attribute these phenomena to the Hessian of the loss exhibiting small local effective rank. It is prohibitively expensive to directly measure the effective rank of the Hessian of a large LM on a reasonably sized dataset. However, many previous works have shown that the Hessian of the loss for deep neural networks trained by SGD has remarkably low effective rank [74; 75; 36; 107; 105; 82]. In particular, the bulk of the spectrum concentrates around \(0\) with only a small number of outliers, and the number of these outliers is an upper bound on the effective rank. In addition, prior works [4; 56] have demonstrated that LM fine-tuning can occur in a very low dimensional subspace (\(<200\) parameters), which further supports the below assumption. We formalize the assumption on the effective rank below. In particular, we require an upper bound on the Hessian in a neighborhood around the current iterate to have effective rank at most \(r\).

**Assumption 1** (Local \(r\)-effective rank).: _Let \(G(_{t})=_{(,)}\|( _{t};\{(,)\})\|\). There exists a matrix \((_{t})_{d}\) such that:_

1. _For all_ \(\) _such that_ \(\|-_{t}\| dG(_{t})\)_, we have_ \(^{2}()(_{t})\)_._
2. _The effective rank of_ \((_{t})\)_, i.e_ \(((_{t}))/\|(_{t}) \|_{op}\)_, is at most_ \(r\)_._

Under this assumption, we show that the convergence rate of ZO-SGD does not depend on the number of parameters. Instead, the slowdown factor only depends on the effective rank of the Hessian.

**Theorem 1** (Dimension-Free Rate).: _Assume the loss exhibits local \(r\)-effective rank (Assumption 1). If \(_{t+1}=_{t}-_{} (_{t};)\) is a single step of ZO-SGD using the \(n\)-SPSA estimate with a minibatch of size \(B\), then there exists a \(=(r/n)\) such that the expected loss decrease can be bounded as_

\[[(_{t+1})_{t}]- (_{t})-_{}\|( _{t})\|^{2}+_{}^{2} [\|(;)\|^{2}]\] (4)

By applying Equation (3), we can directly compare to the SGD descent lemma.

**Corollary 1**.: _Choosing the learning rate \(_{}=^{-1}_{}\), ZO-SGD obtains a loss decrease of_

\[[(_{t+1})_{t}]- (_{t})[-_{} \|(_{t})\|^{2}+_{}^{2} [\|(;)\| ^{2}]].\] (5)

Here we see that comparing to SGD, the slowdown factor of ZO-SGD scales with the local effective rank \(r\), which we argue is much smaller than the number of parameters \(d\). The above analysis focuses on how much ZO-SGD and SGD decrease the loss at each step. Below, we show that under stronger assumptions about the loss landscape, we can obtain rates for how quickly the ZO-SGD algorithm converges to an optimal value.

### Global convergence analysis

We show that the global convergence rate also slows by a factor proportional to the local effective rank under stronger assumptions about the loss landscape. We assume that the landscape obeys the classical PL inequality: the gradient norm grows quadratically with the suboptimality of the iterate.

**Definition 4** (PL Inequality).: _Let \(^{*}=_{}()\). The loss \(\) is \(\)-PL if, for all \(\), \(\|()\|^{2}(()-^{*})\)._

The PL inequality is not as strong as assuming that optimization exhibits kernel-like dynamics, but it ensures that the landscape is amenable to analysis . In addition to the PL inequality, we assume the trace of the gradient covariance is bounded, so noise does not disrupt the trajectory too drastically.

**Definition 5** (Gradient Covariance).: _The SGD gradient estimate on a minibatch of size \(B\) has covariance \(()=B([(; )(;)^{}]- ()()^{})\)._

As we show in Appendix G.1, this assumption holds for common loss functions such as square loss or binary cross entropy for several settings (e.g., kernel behavior ). With these two assumptions, we show that ZO-SGD has a slowdown proportional to the effective rank \(r\), not the parameter dimension.

**Lemma 3** (Global Convergence of ZO-SGD).: _Let \(()\) be \(\)-PL and let there exist \(\) such that \((())(()- ^{*})\) for all \(\). Then after_

\[t=(+1)+B})(_{0})-^{*}}{}}_{})\]

_iterations of ZO-SGD we have \([(_{t})]^{*}+\)._Related work

Zeroth-order optimizationMany classical lower bounds have been derived for ZO-SGD in the strongly convex and convex settings [47; 3; 79; 32; 85; 69] as well as non-convex . These bounds generally depended on the number of parameters \(d\). More recently, [100; 6; 15] showed that if the gradient has low-dimensional structure, then the query complexity scales linearly with the intrinsic dimension and logarithmically with the number of parameters, though the estimation has at least \((sd d)\) memory cost. Additional tricks such as sampling schedules  and other variance reduction methods [48; 62] can be added to ZO-SGD. ZO has inspired distributed methods [93; 43] and black-box adversarial example generation [14; 63; 17; 64] in deep learning. Ye et al. , Balasubramanian and Ghadimi  estimate the Hessian to perform ZO optimization along important directions. There are also ZO methods that optimize without estimating the gradient [38; 68; 44].

Memory-efficient backpropagationSeveral algorithms have been proposed to efficiently approximate backpropagation by sparsifying gradients [92; 102], approximating Jacobians [1; 19], and subsampling the computational graph [71; 2]. However, these methods may accrue large approximation errors for deep networks. Gradient checkpointing  reduces memory cost of backpropagation at the cost of recomputing some activations. FlashAttention  also reduces memory cost by recomputing attention matrices. Dettmers et al. [26; 27] explore quantization of large LMs' weights and optimizer states, which leads to memory reduction in both training and inference.

Gradient-free adaptation of large language modelsBBT and BBTv2 [91; 90] use evolutionary algorithms to achieve gradient-free optimization; however, due to its sensitivity to high dimensionality, BBT is limited to only optimize a low-dimension projection of prefixes and they focus on RoBERTa-large size models and few-shot settings. Other works in "black-box tuning" of LMs focus on optimizing discrete prompts without updating the model, either via reinforcement learning [16; 25; 29], ensemble , or iterative search . Concurrent work in  uses iterative forward passes to improve in-context learning performance.

## 6 Conclusion

We have shown that MeZO can effectively optimize large LMs across many tasks and scales. Further experiments suggest that MeZO can optimize non-differentiable objectives, which backpropagation usually cannot do. Our theory illustrates why MeZO is not catastrophically slow when tuning billions of parameters. As a limitation, MeZO takes many steps in order to achieve strong performance, though we show that the per-step speedup in MeZO can often make fine-tuning with MeZO run faster than a standard implementation of fine-tuning with backpropagation. We did not explore combining MeZO with other memory-efficient methods, such as FlashAttention  and quantization , though we hope to investigate this in the future.

We are excited to explore the applicability of MeZO to a number of areas, including but not limited to: pruning, distillation, saliency, interpretability, and dataset selection for fine-tuning. Non-differentiable objectives are a particularly exciting area, given recent advances in tuning large LMs to adapt to human feedback. Conducting theoretical analyses for how these efficient gradient estimates impact the performance of different applications is also of interest.