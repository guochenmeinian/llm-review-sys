# High Rank Path Development: an approach to learning the filtration of stochastic processes

Jiajie Tao

Department of Mathematics

University College London

ucahjta@ucl.ac.uk

&Hao Ni

Department of Mathematics

University College London

h.ni@ucl.ac.uk

&Chong Liu

Institute of Mathematical Sciences

ShanghaiTech University

liuchong@shanghaitech.edu.cn

Corresponding author

###### Abstract

Since the weak convergence for stochastic processes does not account for the growth of information over time which is represented by the underlying filtration, a slightly erroneous stochastic model in weak topology may cause huge loss in multi-periods decision making problems. To address such discontinuities Aldous introduced the extended weak convergence, which can fully characterise all essential properties, including the filtration, of stochastic processes; however was considered to be hard to find efficient numerical implementations. In this paper, we introduce a novel metric called High Rank PCF Distance (HRPCFD) for extended weak convergence based on the high rank path development method from rough path theory, which also defines the characteristic function for measure-valued processes. We then show that such HRPCFD admits many favourable analytic properties which allows us to design an efficient algorithm for training HRPCFD from data and construct the HRPCF-GAN by using HRPCFD as the discriminator for conditional time series generation. Our numerical experiments on both hypothesis testing and generative modelling validate the out-performance of our approach compared with several state-of-the-art methods, highlighting its potential in broad applications of synthetic time series generation and in addressing classic financial and economic challenges, such as optimal stopping or utility maximisation problems. Code is available at https://github.com/DeepIntoStreams/High-Rank-PCF-GAN.git.

## 1 Introduction

A popular criterion for measuring the differences between two stochastic processes is the weak convergence. In this framework, one views stochastic processes as path-valued random variables and then defines the convergence for their laws, which are distributions on path space. However, this viewpoint ignores the _filtration_ of stochastic processes, which models the evolution of information, and therefore such loss may have negative implications in multi-period optimisation problems. For example, for the American option pricing task, even if the two underlying processes are stochastic processes with very similar laws, the corresponding price of American options can be completely different, see a toy example A.1 in Appendix A.1. To address this shortcoming of weak convergence, D. Aldous  introduced the notion of _extended weak convergence_. The central object inthis methodology is the so-called prediction process, which consists of conditional distributions of the underlying process based on available information at different time beings, and therefore reflects how the associated information flow (i.e., filtration) affects the prediction of the future evolution of the underlying process as time varies. Instead of considering the laws of processes (i.e., distributions on path space) in weak convergence, one compares the laws of prediction processes, which are distributions on the _measure-valued_ path space, in extended weak convergence. Since the knowledge of filtration is captured through taking conditional distributions, it was shown in  the topology induced by extended weak convergence, which belongs to the so-called _adapted weak topologies2_, fully characterise essential properties of stochastic processes and endow multi-period optimisation problems with continuity, provided filtration is generated by the process itself. While the theoretical contributions to adapted weak topologies flourish in recent years (e.g., , , ), the related work on numerics is still very sparse because of the very complex nature of these topologies. In this paper, we propose a novel metric called _High Rank Path Characteristic Function (HRPCFD)_ which can metrise the extended weak convergence, and, more importantly, admits an efficient implementation algorithm. The core idea of this approach is built on top of the unitary feature of \(^{d}\)-valued paths (, ), which exploits the non-commutativity and the group structure of the unitary developments to encode information on order of paths. Based on the same consideration, Lou et al.  introduced the Path Characteristic Function (PCF) for stochastic processes, which induces a computable distance (namely, PCFD) to metrise the weak convergence. As extended weak convergence is defined in terms of laws of prediction processes which are measure-valued stochastic processes, the scheme of PCF remains valid in adapted weak topologies as long as one can construct a PCF of measure-valued paths. One of the main contributions of the present work is to give such a suitable notion via the so-called high rank path development (see Figure 1 for illustration); moreover, we can show that the induced distance (called HRPCFD) does not only characterise the more complicated extended weak convergence, but also inherits almost all favourable analytic properties of classical PCFD mentioned in . Since the measure-valued paths take values in an infinite dimensional nonlinear space, such a generalisation of the results in  from \(^{d}\)-valued paths to measure-valued paths is much technically involved and therefore significantly nontrivial.

On the numerical side, we design an efficient algorithm to train HRPCFD from data and construct the HRPCF-GAN model with HRPCFD as the discriminator for time series generation. A key computational challenge in applying distances based on extended weak topology is the accurate and efficient estimation of the conditional probability measure. To address this issue, we have implemented a sequence-to-sequence regression module that effectively resolves this bottleneck. Our work is the first of its kind to apply the adapted weak topology for generative models on time series generation. Moreover, to validate the effectiveness of our approach, we conduct experiments in (1) hypothesis testing to classify different stochastic processes, and (2) conditional time series generation to predict the future time series given the past time series. Our HRPCF-GAN can be viewed as a natural generalisation of PCF-GAN  to the setting of extended weak convergence, so that the data generated by HRPCF-GAN possesses not only a similar law but also a similar filtration with the target model. The numerical experiments validate the out-performance of this new approach based on HRPCFD compared with several state-of-the-art GAN models for time series generation in terms of various test metrics.

Figure 1: The high-level illustration of the high rank path development. Here the prediction process \(_{t}:=(X|_{t})\) for all \(t[0,T]\), \(_{_{t}}(M_{1})\) is the PCF of the prediction process and \(_{M_{1},M_{2}}()\) is the high rank development of the path \(t_{_{t}}(M_{1})\) under the linear map \(M_{2}\).

**Related work.** So far most of existing statistical and numerical methods for handling stochastic processes (e.g., [9; 16; 19]) are based on weak convergence, and the results on numerical implementation of adapted weak topologies are rather limited. The most relevant work is , whose theoretical foundation roots in . The present paper shares a similar philosophy with  in the sense that both methods for defining metrics for extended weak convergence rely on the construction of a feature of the measure-valued path by transforming it into a linear space-valued path. In contrast to , where a measure-valued path is lifted to an infinite-dimensional Hilbert space, we reduce measure-valued paths into matrix-valued paths through unitary development which allows us to apply the techniques from  to design the algorithm. Another remarkable point is that in  one has to solve a large family of PDEs to compute the distance, which can be avoided in the numerical estimation of the HRPCFD proposed here. On the other hand, as Wasserstein distances can metrise weak convergence, the so-called causal Wasserstein distances can be used to measure adapted weak topologies. One related work is  which can be seen as an improved variant of the Sinkhorn divergence tailored to sequential data. Note that the discriminator (i.e., causal Wasserstein metric) used in  is slightly weaker than the HRPCFD, as the latter is actually equivalent to the bi-causal Wasserstein distance.

## 2 Preliminaries

### Prediction Processes and Extended Weak Convergence

Let \(I=\{0,,T\}\) and \(X=(X_{t})_{t I}\) be an \(^{d}\)-valued stochastic process defined on a filtered stochastic basis \((^{X},,=(_{t})_{t I},)\) such that \(X\) is adapted to the filtration \(\), i.e., \(X_{t}\) is measurable with respect to \(_{t}\) for all \(t I\). We call the five-tuple \((^{X},,,X,)\) a _filtered process_, and denote it by \(\). Throughout this paper, we will use FP to denote the space of all (\(^{d}\)-valued) filtered processes on the discrete time interval \(I\), and assume that \(\) is the natural filtration in the sense that for every \(t I\), \(_{t}=(X_{0},,X_{t})\).

Since each discrete time path \((^{d})^{T+1}\) can be uniquely extended to a piecewise linear path on \([0,T]\) by linear interpolation, we will not distinguish the product space \((^{d})^{T+1}\) and the subspace \(:=\{:[0,T]^{d}:\}\) of \(C^{1}([0,T],^{d})\) (the space of all continuous functions in \(^{d}\) with bounded variation)3. Clearly each stochastic process \(X\) can be seen as \(\)-valued random variable, and therefore the law of \(X\), denoted by \(P_{X}= X^{-1}\), belongs to \(()\), the space of probability measures on the path space \(\). Recall that a sequence of filtered processes \(^{n}=(^{n},^{n},^{n},X^{n},^{n})\) converges to a limit \(\) weakly or in the weak topology (in notation: \(^{n}\) ) if the laws \(P_{X^{n}}=^{n}(X^{n})^{-1}\) converges to \(P_{X}\) in \(()\) weakly, i.e., for all continuous and bounded functions \(f C_{b}()\), it holds that \(_{n}_{^{n}}[f(X^{n})]=_{ }[f(X)]\).

For each \(t I\), we denote \(_{t}:=(X|_{t})\) as the (regular) conditional distribution of \(X\) given \(_{t}\), which is a random measure taking values in \(()\). We call this measure-valued process \(=(_{t})_{t I}\) the _prediction process_ of the filtered process \(\). By definition it is clear that the state space of \(\) is \(()^{T+1}\) and, again, by a routine linear interpolation4, we can embed \(()^{T+1}\) into \(}=\{:[0,T](): \}\). Thus the law of \(\), denoted by \(P_{}=^{-1}\), belongs to \((})\) (the space of probability measures on the measure-valued path space \(}\)), where \(\) is endowed with the product topology and \((})\) is equipped with the corresponding weak topology.

**Definition 2.1**.:
* _Two filtered processes_ \(=(^{X},,,X,)\) _and_ \(=(^{Y},,,Y,)\) _are called synonymous if their prediction processes_ \(\) _and_ \(\) _have the same law in_ \((})\)_, i.e.,_ \(P_{}=P_{}\)_._
* _A sequence of filtered processes_ \(^{n}=(^{n},^{n},^{n},X^{n},^{n})\)_,_ \(n\) _converges to another filtered process_ \(=(^{X},,,X,)\) _in the extended weak convergence if the law of their prediction processes_ \(^{n}\) _converges to the law of_ \(\) _in_ \((})\) _weakly, i.e., for all continuous and bounded functions_ \( C_{b}(})\)_,_ \(_{n}_{^{n}}[(^{n})]= _{}[()]\)_. In notation:_ \(^{n}\)_._If \(_{0}^{n}\) and \(_{0}\) are the trivial \(\)-algebra, then \(_{0}^{n}=P_{X^{n}}\) and \(_{0}=P_{X}\) are laws of \(X^{n}\) and \(X\) respectively, so that \(^{n}\) certainly implies that \(^{n}\). This implies that extended weak convergence is stronger than weak convergence. Moreover, the extended weak convergence induces the correct topology in multi-period decision making problems, as the next theorem (see ) shows.

**Theorem 2.2**.: _The extended weak convergence provides continuity for the value functions in multi-period optimisation problems (e.g., optimal stopping problem, utility maximisation problem), as long as the reward function is continuous and bounded._

Admittedly, the above notions related to extended weak convergence (e.g., the spaces \(}\) and \((})\), the weak convergence in \((})\) etc.) are rather abstract. Therefore, we provide some simple examples in Appendix A.1 to explain these notions in a more transparent way. We refer readers to  and  for more details on extended weak convergence.

### Path Development and Path Characteristic Function (PCF)

In this subsection, we review some important notions and properties of \(^{d}\)-valued path development and characteristic function (PCF) for \(^{d}\)-valued stochastic processes, which will be used later to construct characteristic functions for measure-valued stochastic processes. More technical details on PCF can be found in Appendix A.2. We also refer readers to  and  for a more detailed discussion on this topic.

For \(m\), let \(^{m m}\) be the space of \(m m\) complex matrices, \(I_{m}\) denote the identity matrix in \(^{m m}\), and \(*\) be conjugate transpose. Write \(U(m)\) and \((m)\) for the Lie group of \(m m\) unitary matrices and its Lie algebra, resp.:

\[U(m)=\{A^{m m}:A^{*}A=I_{m}\},(m)=\{A ^{m m}:A+A^{*}=0\}.\]

Let \((^{d},(m))\) denote the space of linear mappings from \(^{d}\) to \((m)\).

**Definition 2.3**.: _Let \( C^{1}([0,T],^{d})\) be a continuous path with bounded variation and \(M(^{d},(m))\) be a linear map. The unitary feature of \(\) under \(M\) is the solution \(:[0,T] U(m)\) to the following equation:_

\[d_{t}=_{t}M(d_{t}),_{0}=I_{m},\] (1)

_where \(_{t}M(d_{t})\) denotes the usual matrix product. We write \(_{M}():=_{T}\), i.e., the endpoint of the solution path, and by an abuse of notation, also call it the unitary feature of \(\) (under \(M\))._

The unitary feature is a special case of the _path development_, for which one may consider paths taking values in any Lie group \(G\). It is easy to see that for piecewise linear path \(=(_{0},,_{T})\), it holds \(_{M}()=_{i=1}^{T}(M(_{i}))\) for \(_{i}=_{i}-_{i-1}\) and \(\) denotes the matrix exponential. We now use the unitary feature to define the Path Characteristic Function (PCF) for \(^{d}\)-valued stochastic processes:

**Definition 2.4**.: _Let \(=(^{X},,,X,)\) be a filtered process and \(P_{X}\) be its law. The Path Characteristic Function (PCF) of \(\) is the map \(_{}:_{m}(^{d}, (m))_{m}^{m m}\) given by_

\[_{}(M):=_{}[_{M}(X)]=_{ }_{M}()P_{X}(d).\]

**Remark 2.5**.: _In the present work, we only consider the discrete-time processes defined on \(I=0,,T\), and therefore the time index \(t\) appeared in the stochastic process \(X_{t}\) and its filtration \(_{t}\) only takes values in \(0,,T\). It is just a convention in the rough path community that one views a discrete time path defined on \(I=0,,T\) as a piecewise linear path defined on the continuous time interval \([0,T]\) by a routine linear interpolation, because such identification may make some formulations and computations easier (e.g., by doing so the unitary feature of a path can be formulated as the solution of an ODE on \([0,T]\))._

To distinguish \(_{}\) from the so-called high rank PCF which will be defined in the next subsection, we also call \(_{}\) the rank \(1\) PCF. The next theorem (see [19, Theorem 3.2]) justifies why \(_{}\) defined in Definition 2.4 is called PCF for path-valued random variables.

**Theorem 2.6** (Characteristicity of laws).: _For \(\) and \(\) two filtered processes, they have the same law (i.e., \(P_{X}=P_{Y}\)) if and only if \(_{}=_{}\)._

The characteristicity of PCF allows us to define a novel distance on FP which metrises the weak convergence (locally). This metric is called the PCF-based distance (PCFD), see [19, Definition 3.3]. Moreover, such PCFD possesses many nice analytic properties including boundedness ([19, Lemma 3.5]), Maximum Mean Discrepancy (MMD, [19, Proposition B.10]) among others, see [19, Section 3.2], which ensures the feasibility of using PCFD in numerical aspect.

**Remark 2.7**.: _Rigorously speaking, we need to add an additional time component to every \(^{d}\)-valued process \(X\) (i.e., consider \(_{t}=(t,X_{t}^{1},,X_{t}^{d})\)) to guarantee Theorem 2.6 holds true. We will always implicitly use such time-augmentation throughout the whole paper and still write \(X\) instead of \(\) for simplicity of notations._

## 3 High Rank Path Development Embedding

We now want to construct a characteristic function for prediction processes and use it to metrise the extended weak convergence just like PCFD metrises the weak convergence. Since prediction processes are \(}\)-valued random variables, we first need to find a suitable notion of unitary feature/development for measure-valued paths.

### High Rank Development of Prediction Processes

Given a filtered process \(=(^{X},,,X,)\), remember that its prediction process \(\) satisfies \(_{t}=(X|_{t})\) for \(t I\). Now, for a linear operators \(M(^{d},(n))\) for \(n\), we take the conditional expectation of \(_{M}\) against \(_{t}=(X|_{t})\) to obtain a \(^{n n}\)-valued stochastic process \(_{_{t}}(M)=_{p}[_{M}(X)| _{t}],\ t I\). Then, for any \((^{n n},(m))\) with some \(m\), the unitary feature \(_{}(t_{_{t}}(M))\) of \(^{n n}\)-valued path \((t_{_{t}}(M))\) is well defined and takes values in the unitary group \(U(m)\). We call each pair \((M,)(^{d},(n)) (^{n n},(m))\) for \((n,m)^{2}\) an admissible pair of unitary representations, and the set of all admissible pairs of unitary representations is denoted by \(_{}\).

**Definition 3.1**.: _For \((M,)_{}\) with \(M(^{d},(n))\), \((^{n n},(m))\) and \(=(^{X},,,X,)\) a filtered process with its prediction process \(\), we call_

\[_{M,}():=_{}(t _{_{t}}(M))\] (2)

_the high rank development of the prediction process \(\) under \((M,)\)._

See Figure 1 for the schematic overview of the high rank development. From above we can see that the construction of \(_{M,}()\) involves with taking finite dimensional path development in Section 2.2_twice_: first use the PCF under \(M(^{d},(n))\) to transform each conditional distribution \((X|_{t})\) into a matrix \(_{_{t}}(M)\), and then apply the unitary feature \(_{}()\) to the resulting matrix-valued path \((t_{_{t}}(M))\) for \((^{n n},(m))\).

### High Rank Path Characteristic Function

With the above notion of unitary feature of measure-valued paths, following Definition 2.4, we define the high rank Path Characteristic Function (HRPCF) for filtered processes.

**Definition 3.2**.: _For a filtered process \(=(^{X},,,X,)\), the function_

\[_{}^{2}:_{}_{m=1}^{ }^{m m};(M,)_{}[ _{M,}()]=_{}[_{ }(t_{}[_{M}(X)|_{ t}])].\] (3)

_is called the High Rank Path Characteristic Function of \(\) (Abbreviation: HRPCF)5._

\(_{}^{2}\) is said to be a HRPCF for \(\) as it satisfies the following characteristicicity of synonym for filtered processes (see Definition 2.1). For a detailed proof please check the Appendix A.

**Theorem 3.3** (Characteristicity of synonym).: _Two filtered processes \(\) and \(\) are synonymous if and only if they have the same high rank PCF, that is, \(_{}^{2}(M,)=_{}^{2}(M, ),(M,)_{}\)._

### A New Distance induced by High Rank PCF

In this subsection, we will use the second rank PCF to define a distance on FP, which can (locally) characterize the extended weak convergence, as the classical PCFD introduced in subsection 2.2 can metrise the weak topology on FP.

**Definition 3.4**.: _For two filtered processes \(\) and \(\), let \((,})\) be a random admissible pair in \(_{}\) with \((^{d},(n))\) for some \(n\), and \(}(^{n n},(m))\) for some \(m\). The High Rank Path Characteristic Function-based distance, for short HRPCFD, between \(\) and \(\) with respect to \(P_{}\) and \(P_{}}\) is defined by_

\[^{2}_{,}}(,)= d ^{2}_{}(_{}^{2}(M,),_{}^{2}(M,))P_{}(dM)P_{}}(d),\]

_where \(d_{}(,)\) denotes the Hilbert-Schmidt distance6 on \(^{m m}\)._

As previously mentioned in the introduction, the so-defined HRPCFD shares the same analytic properties as the classical PCF, e.g., the separation of points, boundedness and the MMD property, whose proof can be found in Appendix A. Moreover, it metrises a much stronger topology (the extended weak convergence). as shown in the next theorem.

**Theorem 3.5**.: _Suppose \((^{i})_{i}\) and \(\) are filtered processes whose laws \(P_{X^{i}}\) and \(P_{X}\) are supported in a compact subset of \(\). Then \(^{i}\) iff \(}(^{i},) 0\), where_

\[}(^{i},):=_{j=1}^{} {\{1,_{_{j},}_{j}}(^{i}, )\}}{2^{j}}\]

_where the sequence \((_{j},}_{j})_{j}\) satisfies that for any \((n,m)^{2}\) there is a \(j\) such that \(_{j}(^{d},(n))\) and \(}_{j}(^{n n},(m))\) and \(P_{_{j}}\), \(P_{}_{j}}\) have full supports for all \(j\)._

We provide a concrete example in the last paragraph of Appendix A.1 to verify the fact that HRPCFD really reflects the differences of filtrations via an explicit computation.

## 4 Methodology

In this section, let \(\) and \(\) be two filtered processes with the law \(P_{X},P_{Y}()\), let \(=(_{i})_{i=1}^{N} P_{X}\) and \(=(_{i})_{i=1}^{N} P_{Y}\) be sample paths.

### Estimating conditional probability measure and HRPCF

A fundamental question is to estimate the conditional probability measure \(_{t}=(X|_{t})\) from the finitely many data \((_{i})_{i=1}^{N}\), in particular the random variable \(_{_{t}}(M)=_{}[_{M}(X)|_{t}]\) for any \(M(^{d},(n))\). We solve this problem by conducting a regression. Fix \(M\) we learn a sequence-to-sequence model \(F_{}^{X}:^{d(T+1)}^{n n(T+1)}\), where the input and output pairs are \((_{[0,T]},_{M}(_{[t,T]})_{t=0}^{T})\). More specifically, we optimize the model parameters of \(F_{}^{X}\) by minimizing the loss function:

\[(;,M)=_{t=0}^{T}_{}d^{2}_{ HS}(F_{}^{X}(_{[0,T]})_{t},_{M}(_{[t,T]})).\] (4)

It is worth noting that the choice of \(F_{}^{X}\) must be autoregressive models to prevent information leakage. A detailed pseudocode is shown in Algorithm 1. Then, we approximate \(_{}^{2}\) using the trained regression model \(F_{}^{X}\) following the Algorithm 2. We denote by \(}_{}^{2}\) the estimation of \(_{}^{2}\).

### Optimizing HRPCFD

In most empirical applications as we will show in Section 5, we employ HRPCFD as a discriminator under the GAN setting. That is, we optimize the loss function \(_{,}^{2}_{,}}(, )\). We would approximate the pair of random variables \((,})\) by discrete random variables \(_{K_{1}}=}_{i=1}^{K_{1}}M_{i}\) and \(}_{K_{2}}=}_{i=1}^{K_{2}}_{i}\), parametrized by \(M_{i}(^{d},(n))\) and \(_{i}(^{n n},(m))\), \(K_{1},K_{2}\) and optimize so-called Empirical HRPCFD

\[^{2}_{_{K_{1}},}_{K_{2}}}(, )=K_{2}}_{i=1}^{K_{1}}_{j=1}^{K_{2}}d^{2}_{ }(}^{2}_{}(M_{i},_{j}),}^{2}_{}(M_{i},_{j})).\] (5)

In practice, the joint training on both \(M_{K_{1}}\) and \(_{K_{2}}\) is computationally expensive and prone to overfitting. We alleviate this problem by splitting the optimization procedure in the following three steps: 1) Optimize \((M_{i})_{i=1}^{K_{1}}\) to maximize \(^{2}_{_{K_{1}}}(,)=}_{ i=1}^{K_{1}}d^{2}_{}(_{}(M_{i}),_{}(M_ {i}))\) (\(_{}(M)=_{i=1}^{n}_{M}(_{i})\))[19, Section 3.3], denote by \(_{K_{1}}^{*}=(M_{i}^{*})_{i=1}^{K_{1}}\) the optimized linear maps. 2) Train regression modules \(F^{X}_{_{i}},F^{Y}_{_{i}}\) for each \(M_{i}^{*}\) using data sampled from \(P_{X}\) and \(P_{Y}\) respectively. 3) Optimize \((_{i})_{i=1}^{K_{2}}\) to maximize \(^{2}_{_{K_{1}}^{*},}_{K_{2}}}(, )\).

The reason behind it is natural: the optimal set \((M_{i}^{*})_{i=1}^{K_{1}}\) captures the most relevant information that discriminates the distribution \(P_{X}\) from \(P_{Y}\). This difference is reflected in the design of higher rank expected path developments through regression models specifically trained for this purpose. Finally, the HRPCFD based on \((M_{i}^{*})_{i=1}^{K_{1}}\) tends to be more significant among other choices of \((M_{i})_{i=1}^{K_{1}}\), making it a stronger discriminator.

### HRPCF-GAN for conditional time series generation

Following [16; 13], we consider the task of conditional time series generation to simulate the law of the future path \(_{}:=_{(p,T]}\) given the past path \(_{}:=_{[0,p]}\) from samples of \(\). To this end, we propose the so-called HRPCF-GAN by leveraging the autoregressive generator and the trainable HRPCFD as the discriminator. See Figure 2 for the flowchart illustration.

**Conditional autoregressive generator** To simulate future time series of length \(T-p\), we construct a generator \(G_{}\) based on the step-1 conditional generator \(g_{}\) following . This generator, \(g_{}:_{}^{d}\), aims to produce a random variable approximating \((X_{t+1}|_{t})\). By applying \(g_{}\) inductively, we can simulate future paths of arbitrary length. To address the limitation of AR-RNN generator proposed in , where \((X_{t+1}|_{t})\) depends solely on \(p\)-lagged values of \(X_{t}\), we incorporate an embedding module. This module efficiently extracts past path information into a low-dimensional latent space. The output of this embedding module, along with the noise vector, serves as the input for \(g_{}\) to generate subsequent steps in the fake time series. Further details of our proposed generator are provided in Appendix B.2.

**High Rank development discriminator** To capture the conditional law, we use the HRPCFD as the discriminator of joint law of \((_{},_{})\) under true and fake measures. Here the empirical measures of \(_{K_{1}}\) and \(}_{K_{2}}\) are model parameters of the discriminator, which are optimized by the following maximization:

\[_{_{K_{1}},}_{K_{2}}}^{2}_{_{K_{1 }},}_{K_{2}}}(_{[0,T]},(_{[0,p]},G_{}( _{[0,p]},z))),\]

In principle, one can generate the fake data by the generator via Monte Carlo and apply the training procedure outlined in Section 4.2 for training the generative model. However, it would be computationally infeasible due to the need for recalibration of the regression module per generator update. To enhance the training efficiency for the regression module under the fake measure, we use the gradient descent method with efficient initialization obtained by the trained regression model under real data. For each generator, the corresponding regression model parameters are then updated to minimize the RLoss (Section 4.1) on a batch of newly generated samples by \(G_{}\). The detailed algorithm is described in Algorithm 3.

## 5 Numerical results

### Hypothesis testing

To showcase the power of EHRPCFD in discriminating laws of stochastic processes, we use it as the test statistic in the permutation test. Similar experiments have been done in [21; 15]. By regarding the permutation test as a decision rule, we assess its performance via computing its _power_ (probability of correctly rejecting the null hypothesis) and _type-I error_ (probability of falsely rejecting the null hypothesis). Similar to , we compare the law of \(3\)-dimensional Brownian motion \(B\) with the set of laws of \(3\)-dimensional fractional Brownian motion \(B^{H}\) with Hurst parameter \(H\) ranging from \([0.4,0.6]\). Details of the methodology and implementation can be found in Appendix C.1.

**Baselines** We compare the performance of HRPCFD with other test metrics including 1) the linear and RBF signature MMDs [8; 20] and its high-rank derivative, namely High Rank signature MMDs ; 2) Classical vector MMDs; 3) PCFD [15; 19].

As shown in Table 1 of the test power, HRPCFD consistently outperforms other models, especially when \(H\) is close to \(0.5\). We do see an improvement from the vanilla PCFD by considering a stronger topology. Furthermore, comparing HRPCFD and High Rank signature MMD, we observe a distinct advantage for HRPCFD. This may be due to the challenge of capturing the conditional probability measure, as High Rank signature MMD relies on linear regression for estimation, whereas we obtained a better estimation using a non-linear approach. Additional test metrics such type-I error and computational cost can be found in Appendix C.1.

### Generative modeling

To validate the effectiveness of our proposed HRPCF-GAN, we consider the task of learning the law of future time series conditional on its past time series.

**Dataset** We benchmark our model on both synthetic and empirical datasets. 1) multivariate fractional Brownian Motion (fBM) with Hurst parameter \(H=1/4\): this dataset exhibits non-Markovian properties and high oscillation. 2) Stock dataset: We collected the daily log return of 5 representative stocks in the U.S. market from 2010 to 2020, sourced from Yahoo Finance.

**Baseline** We compare the performance of HRPCF-GAN with well-known models for time-series generation such as RCGAN  and TimeGAN . Furthermore, we use PCFGAN  as a benchmarking model to showcase the significant improvement by considering the higher rank

   &  &  &  \\  \(H\) & High Rank PCFD & PCFD & Linear & RBF & High Rank & Linear & RBF \\ 
0.4 & \( 0\) & \( 0\) & \(0.09 0.06\) & \(0.97 0.03\) & \(0.22 0.07\) & \(0.05 0.04\) & \(0.97 0.04\) \\
0.425 & \( 0\) & \( 0\) & \(0.1 0.05\) & \(0.69 0.11\) & \(0.14 0.10\) & \(0.01 0.02\) & \(0.58 0.10\) \\
0.45 & \(0.97 0.04\) & \( 0.02\) & \(0.04 0.04\) & \(0.15 0.05\) & \(0.14 0.08\) & \(0.06 0.05\) & \(0.24 0.08\) \\
0.475 & \( 0.13\) & \(0.06 0.02\) & \(0.01 0.02\) & \(0.04 0.02\) & \(0.12 0.04\) & \(0.01 0.02\) & \(0.02 0.02\) \\
0.525 & \( 0.20\) & \(0.08 0.02\) & \(0.05 0.02\) & \(0.07 0.04\) & \(0.19 0.04\) & \(0.08 0.04\) & \(0.09 0.04\) \\
0.55 & \( 0.02\) & \(0.95 0.03\) & \(0.13 0.05\) & \(0.17 0.04\) & \(0.18 0.08\) & \(0.06 0.06\) & \(0.19 0.11\) \\
0.575 & \( 0\) & \( 0\) & \(0.07 0.02\) & \(0.5 0.10\) & \(0.14 0.10\) & \(0.10 0.10\) & \(0.48 0.15\) \\
0.6 & \( 0\) & \( 0\) & \(0.05 0.03\) & \(0.75 0.05\) & \(0.22 0.05\) & \(0.06 0.06\) & \(0.67 0.14\) \\  

Table 1: Test power of the distances when \(h 0.5\) in the form of mean \(\) std over 5 runs. After careful grid search, we set optimal \(=\) for the RBF signature MMD and classical RBF MMD, whereas \(_{1}=_{2}=1\) for High Rank signature MMD.

[MISSING_PAGE_FAIL:9]

Conclusion and Future work

**Conclusion:** In this paper, we apply the unitary feature from rough path theory to define the CF for measure-valued paths, which further induces a distance (HRPCFD) for metrising the extended weak convergence. Theoretically, we prove the key properties of HRPCFD, such as characteristicity, uniform boundedness, etc. Additionally, the numerical experiments validate the out-performance of the approach based on HRPCFD compared with several state-of-the-art GAN models for tasks such as hypothesis testing and synthetic time series generation.

**Limitation and Future work:** The suitable choice of network architecture for generating data is crucial in the proposed HRPCF-GAN, which merits further investigation; in particular, it will be interesting to understand how the network architecture impacts the filtration structure of the generated stochastic process. Furthermore, there is room for further improvement on the estimation method of conditional expectation in terms of accuracy and training stability. Possible routes include exploring the interplay between the regression module and the generator.

**Broader impacts:** Our approach based on the extended weak convergence has the potential in many important financial and economic applications, such as optimal stopping, utility maximisation and stochastic programming. Unlike classical methods built on top of parametric stochastic differential equations, our non-parametric and data-driven method alleviates the risk of the model mis-specification, providing better solution to complex, real-world multi-period decision making problems. However, like other synthetic data generation models, it also poses risks of misuse, e.g., misrepresenting the synthetic data as real data.