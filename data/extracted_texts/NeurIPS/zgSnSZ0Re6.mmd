# Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning

Haoyi Zhu\({}^{12}\)   Yating Wang\({}^{23}\)   Di Huang\({}^{2}\)   Weicai Ye\({}^{24}\)   Wanli Ouyang\({}^{2}\)   Tong He\({}^{2}\)\({}^{}\)

\({}^{1}\)University of Science and Technology of China  \({}^{2}\)Shanghai Artificial Intelligence Laboratory

\({}^{3}\)Northwestern Polytechnical University  \({}^{4}\)Zhejiang University

hyizhu1108@gmail.com

{wangyating,huangdi,yeweicai,ouyangwanli,hetong}@pjlab.org.cn

\({}^{}\)Corresponding Author

https://github.com/HaoyiZhu/PointCloudMatters

###### Abstract

In robot learning, the observation space is crucial due to the distinct characteristics of different modalities, which can potentially become a bottleneck alongside policy design. In this study, we explore the influence of various observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. We introduce OBSBench, a benchmark comprising two simulators and 125 tasks, along with standardized pipelines for various encoders and policy baselines. Extensive experiments on diverse contact-rich manipulation tasks reveal a notable trend: point cloud-based methods, even those with the simplest designs, frequently outperform their RGB and RGB-D counterparts. This trend persists in both scenarios: training from scratch and utilizing pre-training. Furthermore, our findings demonstrate that point cloud observations often yield better policy performance and significantly stronger generalization capabilities across various geometric and visual conditions. These outcomes suggest that the 3D point cloud is a valuable observation modality for intricate robotic tasks. We also suggest that incorporating both appearance and coordinate information can enhance the performance of point cloud methods. We hope our work provides valuable insights and guidance for designing more generalizable and robust robotic models.

## 1 Introduction

The evolution of robot learning has been profoundly influenced by the integration of visual observations, which enable robots to perceive and interact with complex environments. A key challenge faced by contemporary robot models is their limited generalization ability, particularly in dynamic and intricate settings, due to partial observability.

Researchers in robot learning primarily focus on policy design [4; 9; 68; 87]. However, these policies depend on inputs derived from estimated world states or features. Consequently, different observation spaces, such as RGB, RGB-D, or point clouds, can significantly influence robotic performance. This makes the observation space a potential bottleneck, impacting the robot's ability to generalize and perform effectively in various environments.

Commonly, robotic vision has predominantly utilized 2D images [64; 58; 51; 54; 52; 4; 87; 9] for their simplicity and the considerable advancements in 2D foundation models [33; 3; 63; 34]. Despite their ubiquity, RGB images often fall short of accurately capturing the 3D structure of environments, essential for precise action execution. These methods also exhibit vulnerable generalization to changes such as lighting conditions and camera viewpoints, owing to their reliance on appearance.

Given that robot action spaces are 3D, integrating 3D information into observation spaces appears inherently reasonable, as done in methods like CLIPort , Perveiver-Actor  and ACT3D .

The suitability of different observation modalities for robotic tasks remains unexplored due to the lack of a unified comparative framework in the existing literature. Therefore, we introduce OBSBench, a benchmark built upon two modern robot simulators: ManiSkill2  and RLBench . OBSBench comprises 125 contact-rich tasks, each with ground-truth demonstrations available in all modalities. As far as we know, this is the first extensive benchmark for assessing and comparing the effectiveness of various observation spaces in robot learning.

Based on OBSBench, we conduct extensive experiments on different observation modalities across 19 diverse representative tasks. We first evaluate each observation space under identical settings, differing only in input modality and corresponding encoders. We choose backbones with similar model sizes. Recognizing the growing prevalence and varied efficacy of pre-trained visual representations (PVRs) across different modalities, we also investigate the performance of state-of-the-art PVRs within each observation space. Additionally, we also focus on the zero-shot generalization capabilities regarding camera viewpoints, lighting conditions, visual appearance, and sampling efficiency. Finally, we explore how to better utilize point clouds with different design choices such as sampling strategies and geometric information usage.

To the best of our knowledge, our OBSBench is the first to undertake such an extensive comparison of different observation spaces. Our non-trivial and insightful findings can be summarized as follows:

* Point clouds have emerged as a promising observation modality for robot learning, showing the highest mean success rate and best mean rank, whether trained from scratch or with pre-training. Notably, point cloud PVRs utilize significantly less pre-training data. This trend is also observed across different policy implementations.
* An explicit 3D representation is crucial for optimal performance. Utilizing 2D depth maps often shows constrained model performance, whether depth alone is used, RGB-D images are stacked channel-wise, or RGB and depth images are processed separately. Furthermore, adopting a seemingly compromised pointmap format also manifests limited performance.
* Point clouds demonstrate significantly greater robustness to variations in camera views and visual changes.
* We point out several key designs for improved accuracy and robustness with point cloud, such as feature sampling strategy and incorporating both color and coordinate features.

Figure 1: **Overview of this work. We examine the impact of various observation spaces, specifically RGB, RGB-D, and point clouds, on robot learning. We develop OBSBench, a benchmark with standardized pipelines that include various encoders, PVRs, policies, simulators, evaluation settings, _etc_. Based on OBSBench, we conduct a series of empirical studies on observation spaces.**

Background

**Problem Formulation.** The fundamental aim of robot learning is to develop a policy, \((|o_{t},)\), which derives actions from visual observations. Here, \(o_{t}\) is the observation at time \(t\), and \(\) represents an optional task-specific target. The robot, guided by this policy, executes an action \(a_{t}\), generating new observations and receiving a binary reward \(r\) at each episode's end. Our goal is to maximize expected rewards, considering task distribution, initial observations, and transition dynamics.

**Observation Space.** We analyze in detail the observation spaces \(\), emphasizing that observations \(o_{t}\) are projections of the real world states \(s_{t}\) via different sensors \(h():\). Our policy, therefore, is \(_{h}(|o_{t}=h(s_{t}),)\). We explore the diversity of \(h\), focusing on three observation spaces: RGB images, RGB-D images, and point clouds.

**Behavior Cloning.** We adopt behavior cloning [60; 45; 86] for policy learning due to its simplicity and universality. This method trains \(\) on a dataset \(\) of successful demonstrations. The objective is to align the robot's actions with these demonstrations by optimizing \(\) to minimize the negative log-likelihood of the actions based on the observations and goal conditions.

## 3 OBSBench

Previous literature lacks a comprehensive and fair comparison of different observation spaces due to the varied settings, such as policy networks, datasets, augmentation methods, and training techniques. To address this, we have collected a detailed benchmark called **OBSBench**. We implemented standardized pipelines with a series of different baselines to establish a coherent framework for examining the impact of different observation spaces in robot learning. The codebase is built using Hydra  and PyTorch Lightning , making experiments easily configurable and flexible.

**Simulators and Tasks.** In recent years, many advanced simulators have emerged, facilitating reproducible and efficient robot benchmarking. Our study employs two well-known simulators, namely ManiSkill2  and RLBench . These simulators use different physics engines --SAPIEN and CoppeliaSim --making them sufficiently representative. For the ManiSkill2 simulator, we support 12 rigid body tasks and 5 soft body tasks from the official ManiSkill2 Challenge. For the RLBench simulator, we support all official 108 tasks. For all tasks in OBSBench, we provide demonstration trajectories with different observation modality replays. The advanced capabilities of these two simulators also allow for the convenient generation of more customized tasks. Detailed descriptions and examples of the tasks can be found in Appendix A.

**Encoders for different observation spaces.** We primarily consider three common visual modalities: RGB images, RGB-D images, and point clouds. For each modality, we offer standardized implementations of several commonly used encoders. For RGB images, we utilize \(\)**ResNet** and \(\)**ViT**. For RGB-D images, we use channel-wise stacked (_i.e._ the input images have 4 channels) \(\)**ResNet** and \(\)**ViT**. Additionally, we use \(\)**MultiViT**, a Vision Transformer variant designed specifically for multi-modal inputs with distinct projection layers, which can effectively integrate RGB with depth information. For point clouds, we select \(\)**PointNet** and **SparseUNet (\(\)SpUNet)**. \(\)PointNet is a popular point-based network, while \(\)SpUNet is a sparse convolutional network widely adopted in the 3D vision community for point cloud perception tasks.

**Feature Extraction.** To use any observations, we must extract features from encoders to feed into our policy networks. We adopt commonly used settings for feature extraction: \(\) ResNet uses final layer features, while \(\)ViT and \(\) MultiViT employ the [CLS] token. Regarding point cloud baselines, we use farthest point sampling (FPS) [20; 62] and K-nearest neighborhood (KNN) [19; 15]. Specifically, on the final 3D sparse convolutional feature map, we first use FPS to select \(S\) seed points, then employ KNN to form \(S\) clusters around the seeds. These clusters undergo a linear projection and pooling layer, resulting in \(S\) features as inputs for the policy network. We aim to apply simple and common methods to facilitate a fair comparison.

**Policy Networks.** We implement two state-of-the-art policy networks: Action Chunking Transformer (ACT)  and Diffusion Policy (DP) . ACT  models behavior cloning using a conditional VAE  while DP utilizes a diffusion process to model the observation and action spaces. Theyboth have demonstrated remarkable success in a variety of fine-grained manipulation tasks, both in simulated and real-world settings.

For more details, see Appendix B.

## 4 Experiments

We conduct empirical experiments on 19 selected tasks from OBSBench, considering computational resource constraints. The tasks are chosen to be diverse and representative. Details of the selected tasks are provided in Appendix A. We first introduce the evaluation metrics used in the experiments, followed by an empirical analysis of the results on OBSBench. The detailed experimental setups are described in Appendix C. Our experiments aim to address the following research questions:

**Q1:**: How do varying observation spaces influence robot learning performance?
**Q2:**: What is the performance impact of pre-trained visual representations (PVRs)?
**Q3:**: How are the zero-shot generalization capabilities across observation spaces?
**Q4:**: What is the sample efficiency across observation spaces?
**Q5:**: How do different design decisions influence point cloud performance?

### Evaluation Metrics

For each task, we employ **Success Rate (S.R.)** to evaluate the performance of each method. In addition, we adopt the evaluation methodology from VC-1  encompassing two key metrics: **Mean S.R.** and **Mean Rank**. _Mean S.R._ calculates the average success rate across all tasks, providing an overall performance indicator. _Mean Rank_, on the other hand, involves ranking each method based on their success rate for each task and then averaging these rankings across all tasks. This metric offers insights into the relative performance of methods across diverse tasks.

### Study on performance of different observations with and without pre-training (Q1, Q2)

We examine the task performance of different observation spaces across all tasks. The model parameters and the size of the pre-training data are provided in Tab. 1, with the results presented in Tab. 2. Each encoder is trained from scratch on each task using identical training data, pre-processing pipelines, policy architectures, and hyper-parameters, with the only variable being the input observation modalities, ensuring a fair comparison. To explore the impact of the depth modality, we conduct depth-only experiments. However, depth-only experiments are not feasible on RLBench, as RLBench evaluates models with varying color variations. Additionally, we analyze the effectiveness of pre-training on different observation spaces using state-of-the-art pre-trained visual representations (PVRs) for each encoder. The results of the PVRs are displayed in Tab. 3. Detailed information about each encoder and PVR can be found in Appendix B.

_Finding 1:_ We observe that using a point cloud encoder results in the highest mean success rate and the best mean rank. **Point cloud methods consistently outperform other modalities**, securing the first or second rank across all 19 tasks, whether employing ACT policy or diffusion policy. Specifically, in terms of mean success rate, \(*\) SpUNet and \(*\) PointNet outperform the best other modality **by 53.85% and 76.92%**, respectively, when using the diffusion policy. This demonstrates the robustness and superiority of point cloud representations.

_Finding 2:_ Despite providing geometric information, _the depth modality generally **degrades performance across all settings**. This includes scenarios where only depth data is used, where RGB-D images are stacked channel-wise, or when using specialized architectures like \(*\) MultiViT to process RGB and depth information separately. Although the RGB-D version of \(*\) ResNet has a slightly better mean success rate than the RGB version when using ACT, it performs significantly worse on 7

   Obs. Space & Encoder & \#Params & PVR & \#Data \\   & \(\) ResNet50 & 23.5M & \(\) R3M & 5M \\  & \(\) ViT-B & 85.8M & \(\) VC-1 & 5.6M \\ RGB-D & \(*\) MultiViT-B & 86.1M & \(\) MultiMAE 1.28M \\  & \(*\) SpUNet34 & 39.2M & \(\) PonderV2 & 4.5K \\  & \(*\) PointNet & 0.14 M & - & - \\   

Table 1: **Overview of encoders and corresponding PVRs. #Params denotes the number of model parameters while #Data represents the number of images or point clouds during pre-training.**tasks and has a lower mean rank, indicating instability. The primary issue lies in that depth data can exhibit high variability due to changes in object distance, leading to a more unstable data distribution. In robotic applications, the depth values of larger background areas often differ significantly from those of smaller foreground objects, complicating the learning process. These findings underscore the critical importance of explicit 3D representations, such as point clouds.

_Finding 3:_ Using PVRs can lead to better performance _on average_, though not for all individual tasks. Although using point cloud has a higher baseline, \(\) PonderV2 achieves a more significant performance gain compared to \(\) R3M and is comparable to \(\) VC-1 and \(\) MultiMAE. Additionally, the size of \(\) PonderV2's pre-training data is much smaller than that of the other PVRs--by orders of magnitude, from thousands (K) to millions (M). Unlike the other PVRs, \(\) PonderV2 does not utilize object-centric or human interaction data, which are more aligned with robot domains. The reason may be attributed to **the multi-view rendering pretext task during \(\) PonderV2's pre-training, which enriches it with more geometric knowledge, crucial for robot tasks.** This unexpected outcome suggests that, despite having less available data, _point cloud PVRs can still be highly efficient, and in some cases, even superior_.

    &  \\   &  &  &  &  \\   & \(\) ResNet & \(\) ViT & \(\) ResNet & \(\) ViT & \(\) MultiViT & \(\) SpUNet & \(\) PointNet & \(\) ResNet & \(\) ViT \\  PickCube & 0.60 & 0.14 & 0.75 & 0.03 & 0.04 & 0.74 & **0.84** & 0.05 & 0.01 \\ StackCube & 0.32 & 0.00 & 0.17 & 0.00 & 0.00 & 0.22 & **0.35** & 0.00 & 0.00 \\ TurnFaucet & **0.49** & 0.27 & 0.00 & 0.06 & 0.35 & 0.39 & 0.00 & 0.41 & 0.00 \\ Peg. & 0.73 & 0.36 & 0.73 & 0.03 & 0.16 & **0.81** & 0.77 & 0.07 & 0.01 \\ Insertion- & Align & 0.18 & 0.02 & 0.06 & 0.00 & 0.01 & 0.28 & **0.40** & 0.00 & 0.00 \\ Side & Insert & **0.01** & 0.00 & 0.00 & 0.00 & 0.00 & **0.01** & **0.01** & 0.00 & 0.00 \\ Excavate & 0.02 & 0.00 & 0.02 & 0.14 & 0.00 & 0.03 & 0.27 & **0.29** & 0.00 \\ Hang & **0.86** & 0.80 & 0.81 & 0.00 & 0.84 & 0.84 & 0.83 & 0.79 & 0.41 \\ Pour & 0.07 & 0.00 & 0.01 & 0.00 & 0.00 & 0.10 & **0.14** & 0.00 & 0.00 \\ Fill & 0.79 & 0.30 & 0.60 & 0.79 & 0.76 & 0.66 & **0.91** & 0.51 & 0.00 \\  open drawer & 0.00 & 0.16 & 0.08 & 0.00 & 0.20 & **0.44** & 0.00 & - & - \\ sweep to & 0.72 & 0.80 & **1.00** & 0.92 & 0.68 & 0.90 & **1.00** & - & - \\ meat off spill & 0.24 & 0.16 & 0.36 & 0.08 & 0.00 & **0.72** & 0.44 & - & - \\ turn tap & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & **0.04** & - & - \\ reach and drag & 0.32 & 0.28 & **0.60** & **0.60** & 0.04 & 0.20 & **0.60** & - & - \\ put money & 0.60 & 0.76 & **0.84** & 0.04 & 0.28 & 0.60 & 0.32 & - & - \\ push buttons & 0.12 & 0.40 & 0.28 & 0.08 & 0.14 & 0.00 & **0.52** & - & - \\ close jar & 0.04 & 0.00 & **0.16** & 0.00 & 0.00 & 0.04 & 0.00 & - & - \\ place wine & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & - & - \\  Mean S.R. \(\) & 0.32 & 0.23 & 0.34 & 0.15 & 0.18 & 0.37 & **0.39** & - & - \\ Mean Rank \(\) & 3.05 & 4.35 & 3.15 & 4.75 & 4.70 & 2.65 & **2.15** & - & - \\    
    &  &  &  &  \\   & \(\) ResNet & \(\) ViT & \(\) ResNet & \(\) ViT & \(\) MultiViT & \(\) SpUNet & \(\) PointNet & \(\) ResNet & \(\) ViT \\   \\ PickCube & 0.17 & 0.24 & 0.34 & 0.58 & 0.00 & **0.71** & 0.70 & 0.04 & 0.01 \\ StackCube & 0.03 & 0.00 & 0.59 & 0.03 & 0.00 & **0.04** & 0.00 & 0.00 & 0.00 \\ TurnFaucet & 0.08 & 0.07 & 0.24 & 0.30 & 0.00 & 0.32 & **0.36** & 0.28 & 0.00 \\ Peg. & Grasp & 0.78 & 0.45 & 0.94 & 0.68 & 0.46 & 0.82 & **0.83** & 0.06 & 0.02 \\ Insertion- & Align & 0.07 & 0.02 & 0.11 & 0.03 & 0.02 & 0.09 & **0.16** & 0.00 & 0.00 \\ Side & Insert & **0.01** & 0.00 & 0.01 & 0.00 & 0.00 & **0.01** & **0.01** & 0.00 & 0.00 \\ Excavate & 0.01 & 0.02 & 0.23 & 0.03 & 0.00 & 0.17 & **0.24** & 0.02 & 0.00 \\ Hang & 0.52 & 0.42 & 0.77 & 0.56 & 0.00 & 0.67 & **0.72** & 0.65 & 0.09 \\ Pour & 0.00 & 0.00 & 0.06 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 \\ Fill & 0.36 & 0.04 & 0.72 & 0.03 & 0.01 & 0.21 & **0.68** & 0.21 & 0.02 \\   \\ open drawer & 0.00 & 0.00 & 0.12 & 0.00 & 0.08 & **0.28** & 0.12 & - & - \\ sweep to & 0.00 & 0.04 & 0.00 & 0.00 & 0.04 & 0.08 & **0.16** & - & - \\ meat off spill & 0.00 & 0.00 & 0.00 & 0.00 & **0.12** & 0.00 & 0.00 & - & - \\ turn tap & **0.24** & 0.04 & 0.04 & 0.04 & 0.12 & 0.16 & 0.16 & 0.16 & - & - \\ reach and drag & **0.08** & 0.04 & 0.04 & 0.00 & 0.00 & 0.04 & **0.08** & - & - \\ put money & 0.08 & 0.08 & 0.16 & 0.08 & **0.20** & 0.16 & 0.08 & - & - \\ push buttons & 0.04 & 0.00 & 0.04 & 0.00 & **0.12** & 0.04 & **0.12** & - & - \\ close jar & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & - & - \\ place wine & **0.04** & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 &

[MISSING_PAGE_FAIL:6]

the camera vertically and horizontally by \(5^{}\) and \(10^{}\) respectively. The mean success rates under these varied camera views are presented in Tab. 4 and visually represented in Fig. 2.

_Finding 4:_ All methods are significantly affected by camera view changes, even with only \(5\) degrees. However, **point cloud data, both pre-trained and from scratch, shows notable resilience**. This suggests that image-based models are overly dependent on specific training views. Algorithms like  use identical training and testing camera setups, leaving true robustness untested. Inferring 3D actions from images without camera parameters is inherently ill-posed. Our findings highlight the potential of point cloud representations for more robust robot models.

#### 4.3.2 Zero-Shot Generalization to Visual Changes

We further discuss the problem of visual generalization. The StackCube task in ManiSkill2 was selected for its complexity and apparent preference for the RGB modality, as demonstrated by its higher initial accuracy. We consider 3 kinds of visual changes. **1) Lighting:** We vary lighting intensity, testing levels at \(0.03,0.05,0.15,0.6,1.8,3\), from the default intensity of \(0.3\). **2) Noise:** We introduce visual noise by switching the rendering mode from rasterization to ray tracing, disabling the denoiser, and varying the number of ray tracing samples per pixel to \(2,16,32\), and \(64\). **3) Background color:** We alter the original gray floor to red and green, varying the 'R' or 'G' values to \(0.2,0.6,1.0\) respectively. Illustrations of these visual changes are displayed in Fig. 2(a), Fig. 2(b) and Fig. 2(c). The results are plotted in Fig. 2. For comprehensive results on all our generalization experiments, please refer to Tab. 12, Tab. 13, Tab. 14, Tab. 15 and Tab. 16 in Appendix E.

_Finding 5:_ We observe that _point cloud methods generally exhibit better generalization than other observation spaces_. Specifically, we find that \(\) SpUNet demonstrates significantly greater robustness to foreground visual changes, whereas \(\) PointNet's performance drops dramatically to near zero in these scenarios. Conversely, \(\) PointNet shows superior generalization to changes in camera view. Given that point cloud inputs include both coordinate and color information, _these findings are quite noteworthy_. Sparse convolution operations maintain locality, which may contribute to robustness against noise. In contrast, point-based networks emphasize global information, which could enhance robustness to geometric changes.

_Finding 6:_ Additionally, utilizing PVRs generally improves model generalization, **especially when semantic information is incorporated during pre-training**, as seen with \(\) MultiMAE and \(\) PonderV2. This semantic knowledge provides external invariance, enhancing model robustness.

### Study on sample efficiency (Q4)

To evaluate the sample efficiency across different observation spaces, we conducted experiments on RLBench tasks with reduced training data. Specifically, each method was trained using only \(10\) and \(25\) out of the total \(100\) training trajectories. The results are presented in Tab. 5 and Appendix F.

Figure 3: **Examples of different visual changes. (a) The light intensities are \(0.03,0.6,0.15,and0.3\) from left to right and from top to down respectively. (b) The ray tracing samples per pixel are \(64,32,16,and2\) respectively. (c) The background color is denoted as ‘G0.2’, ‘G1.0’, ‘R0.2’, and ‘R1.0’ respectively, where ‘G’ and ‘R’ means green or red and the number represents the value of the green or red channel.**

_Finding 7:_ Our analysis reveals that point cloud observation spaces do not demonstrate a significant advantage in sample efficiency compared to other modalities. Notably, our results indicate that _PVRs consistently improve performance in scenarios with limited training data._ Remarkably, \(\) PonderV2, despite having significantly less pre-training data, still shows notable enhancement. This suggests that leveraging pre-trained models can be particularly beneficial in few-shot learning contexts, where extensive training datasets are not available.

### Study on design decisions on point cloud observation space (Q5)

In this section, we demonstrate that not only the point cloud itself matters but also how it is utilized is equally, if not more, important. Through extensive experimentation on ManiSkill2 tasks, we aim to provide insights into the use of point clouds in robotic learning problems. First, we investigate the importance of coordinate and color information. Next, we examine the influence of point cloud sampling strategies, particularly the widely adopted FPS sampling, used to obtain a fixed number of points for convenient input to policies, as seen in . Additionally, we explore the use of pointmap, a novel format that stacks RGB images with explicit coordinate information. The results of these experiments are detailed in Tab. 6. Full results of each task can be found in Appendix G.

_Finding 8:_ **Post-sampling**, _i.e_., FPS sampling on the feature map after the encoder, **can significantly enhance the performance of point cloud-based methods**, since it can maintain better _local information_. This finding is notable since most previous literature defaults to pre-sampling .

_Finding 9:_ Coordinate information is more critical than color information, as removing coordinate features results in a larger performance drop. Compared to the depth-only experiment results in Tab. 2, this further proves the necessity of utilizing explicit 3D structures. Moreover, using **both color and coordinate information yields the best results**.

_Finding 10:_ Pointmap, which seemingly integrates the advantages of both 2D images and 3D information, consistently outperforms RGB-only and RGB-D methods. However, it still lags behind point clouds, especially when using diffusion policies. We believe this discrepancy arises because the

    &  &  \\   & Samp. & Encoder & Color & Coord. & Mean S.R. & Samp. & Encoder & Color & Coord. & Mean S.R. \\   &  & ✓ & ✗ & 0.22\({}^{ 0.18}\) & & & ✓ & ✗ & 0.20\({}^{ 0.21}\) \\  & & & ✗ & ✓ & 0.25\({}^{ 0.15}\) & & * & SpUNet & ✗ & ✓ & 0.15\({}^{ 0.26}\) \\  & & & ✓ & ✓ & 0.21\({}^{ 0.20}\) & & & ✓ & ✓ & 0.19\({}^{ 0.22}\) \\   & & & ✓ & ✗ & 0.15\({}^{ 0.29}\) & & & ✓ & ✗ & 0.16\({}^{ 0.29}\) \\  & & & ✗ & ✓ & 0.29\({}^{ 0.15}\) & & * & PointNet & ✗ & ✓ & 0.29\({}^{ 0.17}\) \\  & & & ✓ & ✓ & 0.31\({}^{ 0.13}\) & & & ✓ & ✓ & 0.39\({}^{ 0.07}\) \\   & & & ✓ & ✗ & 0.23\({}^{ 0.18}\) & & & ✓ & ✗ & 0.26\({}^{ 0.15}\) \\  & & & ✗ & ✓ & 0.27\({}^{ 0.14}\) & & * & SpUNet & ✗ & ✓ & 0.30\({}^{ 0.11}\) \\  & Post. & & ✓ & ✓ & 0.41 & & & ✓ & ✓ & 0.41 \\   & & & ✓ & ✗ & 0.22\({}^{ 0.23}\) & & & ✓ & ✗ & 0.18\({}^{ 0.28}\) \\  & & & ✗ & ✓ & 0.38\({}^{ 0.07}\) & & * & PointNet & ✗ & ✓ & 0.37\({}^{ 0.08}\) \\  & & & ✓ & ✓ & **0.45** & & & ✓ & ✓ & **0.45** \\   &  &  & ✓ & ✓ & 0.43\({}^{ 0.02}\) & & * & ResNet & ✓ & ✓ & 0.28\({}^{ 0.08}\) \\  & & & ✓ & ✓ & 0.28\({}^{ 0.09}\) & & * & * & * & * & * & \\  RGB-D &  &  & ✓ & Depth & 0.34 & & * & ResNet & ✓ & Depth & 0.03 \\  & & & ✓ & Depth & 0.15 & & * & * & * & * & * & Depth & 0.05 \\   

Table 6: **Influence of different design choices on point cloud observations.** Results are reported for ManiSkill2 tasks. ‘Pre. Samp.’ denotes pre-sampling (FPS sampling before the encoder). ‘Post. Samp.’ indicates post-sampling (sampling after the encoder). Blue numbers show point cloud performance drop _relative to post-sampling with both color and coordinate information_ (underlined). Red numbers indicate pointmap performance gain _compared to RGB image methods_. **Here, the novel pointmap format stacks both color and coordinate information within images.**

[MISSING_PAGE_FAIL:9]

RealSense D415 RGB-D cameras. Our bimanual setups (including 2 leader arms, 2 follower arms, 2 cameras, etc.) cost about $2000 in total, making them affordable and easy to replicate for researchers. We've also open-sourced our real-world codebase  for easy reproduction by other researchers. We designed three tasks: **1) Reach Cube:** A single arm with rigid objects. The robot is required to reach towards a cube and touch it. **2) Pick Cube:** A single arm with rigid objects. The robot is required to pick up the cube and hold it in the air. **3) Fold Cloth:** Two dual arms with soft-body objects. The robot is required to simultaneously catch one side of the cloth with two grippers and then fold the cloth in half. The detailed setups, our workstation, and task visualizations are shown in Appendix D. The real-world results shown in Tab. 11 align with our simulated experiments, further supporting our conclusions.

## 7 Related Work

**Pre-trained Visual Representations (PVRs).** Recent advancements in self-supervised learning (SSL) for 2D and 3D computer vision, using methods like contrastive learning [7; 33; 8; 75; 35; 74; 63], distillation-based technique [6; 2; 55]s, reconstructive approaches [3; 34; 57; 85; 78], and differentiable rendering for pre-training [36; 90; 79] have shown promise. These techniques have been adapted for Embodied AI and robot learning, achieving notable results [58; 54; 64; 51; 42; 77; 52; 22; 80]. However, most focus on 2D RGB spaces, overlooking diverse observation spaces and 3D PVRs. Our research explores the impact of PVRs on different observation spaces.

**Observation Spaces in Robot Learning.** Estimating states from raw observations in robot learning has mainly focused on 2D RGB images. However, the importance of 3D information is gaining recognition [84; 27; 67; 68; 26; 81; 24; 47; 11; 82], as seen in methods like CLIPort  and Perceiver-Actor . Despite progress, there is a lack of systematic comparison between 2D and 3D methods, with some 3D techniques being complex [26; 47; 11] or reliant on dense voxel representations [68; 82].

**Robot Learning for Manipulation.** Modeled as (Partially-Observable) Markov Decision Processes [41; 44], robot manipulation in RL faces issues like multi-objectiveness, non-stationarity, sim-to-real gaps, and poor sample efficiency [71; 25; 31; 65]. Behavior cloning [60; 45] offers a successful alternative with diverse approaches [68; 4; 5; 40; 87; 9], _etc._ Current foundational models [56; 72] primarily use RGB images for observation. Our research examines the impact of different observation spaces to guide future advancements.

## 8 Conclusion and Limitations

In this study, we introduce OBSBench to advance research on various observation spaces in robot learning. Our findings indicate that point cloud methods consistently outperform RGB and RGB-D in terms of success rate and robustness across different conditions, regardless of whether they are trained from scratch or pre-trained. Design choices, such as post-sampling and the inclusion of coordinate and color information, further enhance their performance. However, point cloud methods face challenges with sample efficiency. Utilizing large-scale 3D datasets like RH20T  and DL3DV-10K  could improve their robustness and generalization. Future research should explore dynamic sampling techniques and multi-modal integration, including tactile sensing. Although our experiments are conducted on simulated benchmarks to ensure consistency and fairness, translating and validating these findings in real-world scenarios in a _reproducible_ and _credible_ manner remains an open question. In the short term, we do not foresee any negative societal impacts from this work. However, as our results contribute to the development of more robust robotic systems, it is crucial to study how to prevent robots from causing harm in daily life in the long run.