# MeMo: Meaningful, Modular Controllers via Noise Injection

Megan Tjandrasuwita

MIT

megantj@mit.edu

&Jie Xu

NVIDIA

jiex@nvidia.com

Armando Solar-Lezama

MIT

asolar@csail.mit.edu

&Wojciech Matusik

MIT

wojciech@mit.edu

###### Abstract

Robots are often built from standardized assemblies, (e.g. arms, legs, or fingers), but each robot must be trained from scratch to control all the actuators of all the parts together. In this paper we demonstrate a new approach that takes a single robot and its controller as input and produces a set of modular controllers for each of these assemblies such that when a new robot is built from the same parts, its control can be quickly learned by reusing the modular controllers. We achieve this with a framework called MeMo which learns (Me)aningful, (Mo)dular controllers. Specifically, we propose a novel modularity objective to learn an appropriate division of labor among the modules. We demonstrate that this objective can be optimized simultaneously with standard behavior cloning loss via noise injection. We benchmark our framework in locomotion and grasping environments on simple to complex robot morphology transfer. We also show that the modules help in task transfer. On both structure and task transfer, MeMo achieves improved training efficiency to graph neural network and Transformer baselines.1

## 1 Introduction

Consider the following scenario: A roboticist is designing a robot with 6 legs, such as the one seen in the left image of Fig. 1, and has trained a standard neural network controller with deep reinforcement learning (RL) to control the actuators circled in green. However, after more testing, they realize that the design of the robot needs to be extended with another pair of legs to support the desired amount of weight. Even though the new 8 leg robot is still composed of the same standard assemblies, the roboticist is unable to reuse any part of the 6 leg robot's controller. While many works  have studied structure transfer, or transferring neural network controllers to different robot morphologies, these works take a purely data-driven approach of training a universal controller on a dataset representative of the diversity and complexity of robots seen in testing. In contrast, we desire to learn transferable controllers from only a single robot and environment, obviating the requirement for a substantial training dataset and resources to perform multi-task RL. Our experiments demonstrate that state-of-the-art approaches for transferring control to environments with incompatible state-action spaces struggle to generalize in this highly data-scarce setting.

Motivated by the above scenario, we propose a framework, MeMo, for pretraining (Me)aningful (Mo)dular controllers that enable transfer from a single robot to variants with different dimensionalities. Learning transferable modules from a single robot trained on a single task is challenging, evenwhen we focus on transfer among robots with similar global morphologies. The key insight MeMo leverages is that a robot is built from assemblies of individual components, such as the leg of a walking robot or the arm of a claw robot. These assemblies are specified by a domain expert who is able to account for the constraints imposed by the robot's hardware implementation in their specification. Given this information, MeMo learns assembly-specific controllers, or modules, responsible for coordinating the individual actuators that comprise a given assembly, which are coordinated by a higher-level boss controller. As we are able to reuse the modules when transferring to a robot built from the same assemblies, the problem of learning a controller for a different morphology boils down to learning the coordination mechanics among assemblies, rather than having to coordinate at the granular level of individual joints. Returning to the 6 leg robot in Fig. 1, we see that the robot is comprised of multiple "leg" and "body" assemblies, circled in red and blue respectively in the right image. Module parameters are shared between assemblies of the same type, providing multiple training instances that help our modules generalize. After training the modules with MeMo, the "leg" and the "body" modules can then be reused to speed up the training of a different robot's controller, such as an 8 leg robot.

To achieve this improved training efficiency, a key challenge is to balance the labor between the boss controller and the modules. In one direction, to prevent the modules from becoming too robot-specialized, we introduce information asymmetry into our architecture, where the modules are limited to seeing the local observations of the actuators that belong in the module. In the other direction, controlling the assembly through the module must be simpler than controlling the assembly directly, since otherwise there is no benefit to this new architecture. This is achieved by a new modularity objective (Section 2) that forces the modules to capture as much of the coordination mechanics within a subassembly as possible, given limited local observations. In practice, we use noise injection (Section 3.1) to optimize the new objective simultaneously with standard behavior cloning loss.

To evaluate the transferability of the learned module, we apply MeMo in locomotion and grasping domains. We design two types of transfer: generalizing to more complex robot structures and to different tasks. When transferring model weights from a simpler agent, we show that MeMo significantly improves the sample efficiency of performing RL on more complex agents. We compare our framework with NerveNet, an alternative approach for one-shot structure transfer [4; 5], and MetaMorph , an approach for learning universal controllers. Our experiments show that MeMo either exceeds or matches NerveNet and MetaMorph's training efficiency during transfer, as the message-passing policies are prone to overfitting during pretraining.

## 2 Motivation for Modularity Objectives

Our goal is to maximize the extent to which the assembly-specific modules take responsibility for the behavior of the robot. In this section, we formalize the objectives that our training pipeline should achieve. Given an expert controller **F**, one can train a modular controller, consisting of a higher-level **B**(oss) module that sends a signal to each of the **W**(orker) modules, to mimic **F**'s behavior using the

Figure 1: **Graph Structure and Neural Network Modules of the 6 Leg Centipede. Left:** The robot’s joints are labeled numerically and circled. **Right:** The joints form the nodes and the links are the edges. The subset of joints that form each leg module are circled in red, while those that comprise each body module are circled in blue. Neural network modules are denoted as \(_{k}^{i}\), where \(k\) refers to the type, e.g. all leg modules are type 0, and \(i\) denotes different instances of the same module type.

standard behavior cloning objective. Let **B** be parameterized by \(\) and **W** be parameterized by \(\). For simplicity, this section assumes that there is a single **W** module.

**Definition 2.1**.: **Behavior Cloning Objective.** For a system with states \(s_{i}\), the modular policy whose output is \(_{}(_{}(s_{i}))\) imitates the expert monolithic policy **F**.

\[_{,}\;_{i}[(_{}( _{}(s_{i}))-(s_{i}))^{2}]\] (1)

The behavior cloning objective ensures that the composition of modules can perform the desired task, but it is not enough to ensure that the worker module is _useful_. In software engineering, a component is most useful if it can provide a narrow interface to a rich set of functionality. In the context of modularity, this is analogous to **W** giving **B** only a few degrees of freedom to control the system's outputs; otherwise, a module **W** that gives **B** full control over the actuators will leave **W** with no real responsibility for the robot's behavior. Then, when **W** is used with a new robot or for a slightly different task, **B** needs to relearn all the details of how to control the output for that new setting.

For example, consider a robot arm with 5 degrees of freedom that controls a lever in Fig. 2. An ideal worker module would take as input a signal corresponding to the desired angle of the lever and would be responsible for coordinating the signals to the five actuators to achieve the lever's desired position. This would mean that if we want to reuse **W** in controlling two arms, the new boss **B**' will only have to learn how to coordinate the two angles, and not all 10 actuators.

In practice, though, forcing the interface between **B** and **W** to be a one dimensional vector makes the optimization problem very difficult. Instead, our approach will be to use a larger vector as the interface between the two modules, but introduce an additional optimization objective. Intuitively, the interface is effectively narrow when **B**'s signal can be decomposed into a small set of dimensions that result in greater variance in **W**'s output and a much larger set of dimensions that do not cause significant perturbations in **W**'s output. In other words, the null space of **W**'s Jacobian would be higher-dimensional, meaning that **W** has a greater tolerance for error in **B**'s signal that fall in the directions of the null space. To encourage **W** to be less sensitive to perturbations in **B**'s signal, we minimize the distance between \(_{}(_{}(s_{i})+)\), where \(\) is a noise vector, and \(_{}(_{}(s_{i}))\).

**Definition 2.2**.: **Invariance to Noise Objective.** Let \(\) be a noise vector. The difference between the result of applying **W** on the distorted input and on the undistorted input is \(D(s_{i},)=_{}(_{}(s_{i})+)-_{ }(_{}(s_{i}))\). As a new distortion to \(_{}(s_{i})\) is added on each epoch, we average the difference over the added noise.

\[_{,}\;_{}[_{i}[D(s_{i },)^{2}]]\] (2)

In practice, we sample \(\) from a Gaussian distribution with \([]=0\) and \([^{T}]=^{2}\), where \(=1.0\). In Section 5.3, we demonstrate that our invariance to noise objective is the critical component in our framework that yields positive transfer benefits. In Section 5.4, we show that optimizing the noise invariance objective reduces the effective dimensionality of **B**'s signal.

Figure 2: **Effect of Modularity Objectives.** Consider a module with 5 actuators, denoted in orange, trained to push a lever clockwise. As the state of the lever is a function of its angle \(\), a module trained by MeMo represents the control signals as a one-dimensional manifold with respect to **B**’s signal. When noise is added to **B**’s signal, the outputted actions remain on the manifold. Without MeMo, perturbations to **B**’s signals cause deviations from the high reward trajectory.

Method

We describe our approach MeMo, an algorithm for learning reusable control modules. In Section 3.1, we show that the modularity objectives can be optimized with noise injection. In Section 3.2, we extend our formulation to systems with more than one module and detail our training pipeline.

### Objective

We propose to optimize both modularity objectives simultaneously with noise injection.

**Definition 3.1**.: **Noise Injection Objective.** Here, \(\) can be viewed as "injected noise."

\[_{,}\ _{}[_{i}[ (_{}(_{}(s_{i})+)-(s_{i}) )^{2}]]\] (3)

The noise injection loss can be decomposed as follows:

\[ =_{i}[(_{}(_{ }(s_{i}))-(s_{i}))^{2}]+_{}[_{i}[D(s_{i},)^{2}]]\] \[+_{i}[2(_{}(_{ }(s_{i}))-(s_{i}))_{}[D(s_{i},)]]\] (4)

See Appendix A.4 for the derivation of the decomposition. Without the last term, which we call the product term, noise injection is equivalent to the sum of Eq. 1 and 2. Analyzing the product term further, by the Mean Value Theorem, \(D(s_{i},)=_{}(_{}(s_{i})+)-_ {}(_{}(s_{i}))=}(z)^{}\) for \(z L\), where \(}\) denotes the Jacobian of \(\) with respect to \(\)'s output and \(L\) is the line segment with \(_{}(s_{i})\) and \(_{}(s_{i})+\) as endpoints. Applying the expectation over the noise:

\[_{}[D(s_{i},)]=_{}[}(z)^{}]\] (5)

Note that \(z\) depends on the value of \(\), so it cannot be pulled out of the expectation. However, in practice, we expect that \(}(z)^{}}(_{}(s_{i}) )^{}\). This implies that \(_{}[D(s_{i},)]}(_{ }(s_{i}))^{}_{}[]=0\), making the product term negligible. Empirically, in Fig. 3, we show that this product term indeed becomes much smaller than the sum of the two modularity objectives as training proceeds.

### Modular Architecture and Training Pipeline

**Modular Architecture.** Although thus far we have only a single module \(\), a robot is often comprised of multiple modules controlling physical assemblies that are common among different morphologies. Formally, we assume that we are given a partitioning \(\) of an agent's joints \(j_{0,,N-1}\). We design a modular policy composed of a boss controller \(\) that outputs intermediate signals to neural network modules that decode actions. Each element of the partition, e.g. a subset of actuators, is a module instance \(i\) of type \(k\), which we denote as \(_{k}^{i}\). In total, there are \(||\) modules. Modules of the same type \(k\) share the module parameters, yet each instance will receive a different message from \(\). We detail our architecture further in Appendix A.3.

Figure 3: **Noise Injection Error.** Over the course of training, we compute ratio = \(|_{p}|/(_{1}+_{2})\) where \(|_{p}|\) is the magnitude of the mean product term over the minibatch and \(_{1}\) and \(_{2}\) are the mean behavior cloning and invariance to noise losses. We compute training statistics over 5 runs and indicate standard deviation by shaded areas. **(Left)-(Right):** For all starting morphologies, the modularity objectives dominate the loss as the ratio is less than 1 for all updates.

**Training Pipeline.** To train our modules, inspired by previous works that combine RL and IL [6; 7; 8], we first train **F** using RL. During the RL stage, we use proximal-policy optimization  to train actor-critic controllers. The critic is a MLP, whereas the actor is a standard MLP when training the expert controller and a modular architecture when transferring pretrained modules. Once **F** is trained, we train a modular policy \(_{,}(a_{i} s_{i})\) with IL via DAgger , with noise injected into \(_{}\)'s output. At each iteration \(k\) of DAgger, we sample a trajectory \(_{k}\) from \(_{,}\). **F** provides the correct action to each \(s_{k}\), and \(_{k}\) is aggregated into the full dataset \(=\{(s_{i},a_{i})\}\). To optimize the objective defined in Section 3.1, we minimize \(=-_{s_{i}}[_{,}(a_{ i} s_{i})]\). We derive a decomposition of the negative log likelihood loss with noise injection into the modularity objectives in Appendix A.5. After transferring the modules to a new structure or task, we perform RL to retrain **B** or finetune the architecture end-to-end. Our pipeline is summarized in Fig. 4. Appendix A.6 details our RL and IL hyperparameter settings.

## 4 Related Work

Our work relates to modular controllers and structure transfer. Related works in noise injection, multi-robot RL, and hierarchical RL are discussed further in Appendix A.11.

**Modular Controllers.** Our work relates to prior works that train modular policies for robot designs.  learns neural network policies that are decomposed into "task-specific" and "robot-specific" modules and performs zero-shot transfer to unseen task and robot-specific module combinations.  coordinates modular policies shared among all actuators via message passing.  uses a GNN to internally coordinate between part-specific nodes with shared module parameters between nodes corresponding to the same part.  proposes the Dynamic Graph Network to control self-assembling agents, consisting of modules that are shared across agents.

**Structure Transfer.** In the hierarchical RL setting,  uses imitation learning to train policies that represent long-horizon behavior and improve sample efficiency when transferred from simple to complex agents.  transfers policies to robots with significantly different kinematics and morphology by defining a continuous evolution from the source to the target robot. Previous works use message-passing policy architectures to generalize across morphologies [1; 4; 5]. In the multi-task setting  proposes Transformers as policy representations that remove the need for multi-hop communication.  scales Transformer-based policies to larger and more diverse datasets of robots.

## 5 Experiments

With our experiments, we seek to answer four questions. 1) Do the modules produced by MeMo generalize when transferred to different robot morphologies and tasks? 2) When pretraining modular controllers with imitation learning, does the Gaussian noise injection help? 3) In the pretraining phase, why do we use imitation learning rather than injecting noise in reinforcement learning? 4) How does our modularity objective yield better representations of the actuator space? We answer question 1) in Sections 5.1 and 5.2, 2) and 3) in Section 5.3, and 4) in Section 5.4.

### Transfer Learning

We benchmark our framework on two types of transfer: structure and task transfer. While our framework is designed primarily for structure transfer, we use task transfer experiments as an additional means of evaluating the quality of the learned representations. For the locomotion

Figure 4: **Training Pipeline Overview.** In Phase 1, we first train an expert controller for the training robot using RL. In Phase 2, we pretrain modules with noise injection during imitation learning. In Phase 3, we transfer the modules to a different context and retrain the boss controller \(_{3}\).

experiments, we perform experiments on the tasks introduced in RoboGrammar  with training statistics computed as the average reward across 3 runs, with standard deviations indicated by shaded areas. For the grasping domain, we construct object-grasping tasks using the DiffRedMax simulator  and compute training statistics as the average reward across 5 runs. Additional details on the reward functions used are in Appendix A.9. We visualize train and test robot morphologies for structure transfer in Fig. 5 and the train and test tasks for task transfer in Fig. 18.

**Locomotion.** We design three structure transfer tasks in the locomotion domain, in which the goal is to move as far as possible while maintaining the robot's initial orientation. The starting morphologies are the 6 leg centheole robot, the 6 leg worm robot, and the 6 leg hybrid. The 6 to 12 leg centhepede transfer demonstrates scalability to transfer robots with many more modules than seen in training. The 6 to 10 leg worm shows that MeMo generalizes with only 1-2 instances of the same module seen in training. The 6 and 10 leg hybrid robots involve three types of modules, demonstrating scalability to more complex training robots. For task transfer, we transfer policy weights pretrained on a 6 leg centhepede locomoting over the Frozen Terrain to three terrains that feature obstacles or climbing.

**Grasping.** In grasping, the goal is to grasp and lift an object as high as possible. We design a grasping robot consisting of an arm that lifts a claw grasping a cube. The structure transfer is from a 4 finger to a 5 finger claw. For task transfer, we transfer policies trained to control the 4 finger claw grasping a cube to the same robot grasping a sphere of similar size and weight.

**Baselines.** We compare MeMo to MLP and modular policies trained from scratch as well as pretrained NerveNet [4; 5] and MetaMorph  baselines. NerveNet takes as input the underlying graph structure of the agent, where the nodes are actuators and edges are body parts. The graph structures of the train morphologies are detailed in Appendix A.13. For MetaMorph, a Transformer-based approach, we convert the global observations and local observations for each actuator to a 1D sequence of tokens. Full training details and state space descriptions are included in Appendix A.6 and A.7 respectively.

* **RL (MLP)**: For structure transfer, due to the change in the observation space, we train a 2 layer MLP policy from scratch with RL. In task transfer, we use a MLP pretrained with RL on the original task and finetune it on the test task. For a fair comparison, we use the same architecture size as the modular architecture's boss controller and replace the modules with a linear layer decoder.
* **RL (Modular)**: For structure transfer, we train the modular architecture, discussed in Section 3.2, from scratch with RL. In task transfer, we use the modular architecture pretrained with RL on the training task and finetune both the modules and the boss controller on the test task. The inclusion of this baseline allows us to isolate the effect of the modular architecture from the pretraining and noise injection components of MeMo.
* **Pretrained NerveNet-Conv**: We use the NerveNet network architecture proposed by , consisting of an input network \(F_{in}\) for encoding observations, a message function \(M\), an update network \(U\), and an output network \(F_{out}\) for decoding. As in , \(F_{in}\) and \(F_{out}\) are MLPs. In the convolutional  variant, \(M\) is the identity function and \(U\) is a weight matrix. During RL, we fix \(F_{out}\) in a similar manner as fixing the modules in MeMo, which improves NerveNet-Conv's performance.
* **Pretrained NerveNet-Snowflake:** Snowflake  is a state-of-the-art approach for training GNN policies that scale to high-dimensional continuous control. Their method involves fixing parts of the NerveNet architecture to prevent overfitting during PPO. Empirically, they find that fixing \(\{F_{in},M,F_{out}\}\) results in the best performance on MuJoCo tasks. We follow the same parameter fixing as Snowflake. As in Snowflake, we parameterize \(F_{in}\) and \(F_{out}\) as MLPs and the update function \(U\) as a GRU. We use a weight matrix for \(M\).

Figure 5: **Structure Transfer Tasks. Left:** Transfer “leg” and “body” modules from a 6 to a 12 leg centhepede. **Left Middle:** Transfer “body” and “head” modules from a 6 to a 10 leg worm. **Right Middle:** Transfer “leg,” “head,” and “body” modules from a 6 to a 10 leg hybrid. **Right:** Transfer “arm” and “finger” modules from a 4 to a 5 finger claw.

* **Pretrained MetaMorph:** MetaMorph  is a Transformer-based approach for learning a universal controller over a large collection of robot morphologies. We adopt MetaMorph's Transformer architecture for the policy network, which adds learned positional embeddings before processing the input sequence with a Transformer encoder. As our domains lack exteroceptive observations, we directly decode Transformer encodings to controller outputs. The Transformer policy is finetuned during RL.

### Results

The generalization ability of MeMo on structure transfer is shown in Fig. 6. On all structure transfer tasks, MeMo outperforms the message-passing baselines. On the 12 leg centipede and the 10 leg hybrid, not only is MeMo 2\(\) more sample efficient than the best baseline, but it also converges to controllers with significantly better performance than any baseline. On the 10 leg worm, MeMo outperforms all baselines in terms of training efficiency and achieves a comparable final performance as NerveNet-Conv. MeMo also outperforms all baselines on the 5 finger claw. We note that the worm transfer task is easier for GNN models, because the coordination of the shorter legs and body joints is naturally captured with multi-hop communication. MetaMorph struggles with locomotion tasks, due to the high dimensionalities of the transfer robots.

The results of MeMo on task transfer are shown in Fig. 7. As transferring from the Frozen to the Ridged, Gap, and Stepped Terrains requires the robot to overcome obstacles unseen in the Frozen Terrain, we load the pretrained boss controller and finetune MeMo end-to-end. Results (Fig. 7) show that on all test tasks, MeMo achieves improved training efficiency compared to MetaMorph and to pretrained MLP and modular architectures. MeMo achieves comparable performance on the Ridged

Figure 6: **Structure Transfer Results. Left: 6 leg centipede to 12 leg centipede transfer on the Frozen Terrain. Left Middle: 6 leg worm to 10 leg worm transfer on the Frozen Terrain. Right Middle: 6 leg hybrid to 10 leg hybrid transfer on the Frozen Terrain. Right: 4 finger claw to 5 finger claw transfer on grasping a cube. The dashed orange line shows that the final performance of the closest baseline is achieved by MeMo within half of the total number of timesteps.**

Figure 7: **Task Transfer Results. Left: The first three plots show results on transferring from the 6 leg centipede walking over the Frozen Terrain to the same centipede walking over a terrain with ridges, a terrain with gaps, and a terrain with upward steps. Right: The last plot shows the transfer from a 4-finger claw grasping a cube to the same claw grasping a sphere. MeMo either has comparable training efficiency to the strongest baseline or outperforms all baselines.**

and Gap Terrains and outperforms the NerveNet baselines on the Stepped Terrain, which requires the robot to climb up steps whereas the training terrain is flat. MeMo also has improved training efficiency and final performance in the grasping domain when transferring from grasping a cube to a sphere. The pretrained NerveNets struggle to coordinate the arm and claw components, resulting in high variance across different random seeds.

### Ablation Study

**Sum of Modularity Objectives.** We answer the question of why we choose to optimize the noise injection objective rather than the sum of the modularity objectives directly. We evaluate the sum of Eq. 1 and 2 between networks trained with the noise injection objective and those trained with the sum in Table 1. Using 100 sampled trajectories from the expert controller, we average the resulting sum of objectives over 1000 epochs, with different sampled noise on each epoch. Our results demonstrate that optimizing the noise injection objective converges to better solutions.

**Noise Injection Objective.** The key to the success of MeMo is the introduced noise injection (NI) objective which encourages proper responsibility division among the pretrained boss controller and modules, enabling the modules to improve training efficiency when reused. We conduct an ablation study to verify this technique by experimenting on a special setting, "transferring" the controller to the same robot structure and task, a 6 leg centipede traversing a Frozen Terrain. During transfer, we reuse and freeze the pretrained modules and retrain the boss controller from scratch. With the pretrained modules from MeMo, the boss controller will be retrained much more efficiently because it only needs to take partial responsibility for the control job. We compare our method to three baselines:

* **MeMo (no NI)**: We pretrain the modular architecture end-to-end without noise injection. This ablation is equivalent to MeMo without noise injection.
* **MeMo (L1):** During pretraining, we replace the injected noise with L1 regularization on **B**'s output that encourages sparsity in its signal. We weigh the regularization term by a hyperparameter \(w\) and report results with the best \(w\).
* **MeMo (L2):** During pretraining, we replace the injected noise with L2 regularization on **B**'s output and report results with the best weight on the regularization term.
* **MeMo (Jacobian):** As an alternative to noise injection, we penalize the norm of the module's Jacobian using the method described in .

In addition, we add the training curve of **RL (Modular)** as a reference. The results (Fig. 8) show that MeMo yields a significant improvement in training efficiency over all ablations.

   Morphology & Noise Injection & Dual Loss \\  Centipede & -33.518 & -33.115 \\ Worm & -39.295 & -35.896 \\ Hybrid & -30.536 & -27.849 \\ Claw & -8.215 & 0.279 \\   

Table 1: **Sum of objectives.** On all starting morphologies, optimizing the noise injection objective results in lower loss than directly optimizing the dual loss.

Figure 8: **Ablation Results. Left:** MeMo outperforms all other variants that are pretrained with IL. **Right:** MeMo outperforms all variants that pretrain modules with RL. In both settings, MeMo achieves the final performance of the closest baseline within half of the total number of timesteps.

**Imitation Learning.** We now answer the second question of whether imitation learning is necessary to pretrain modules with Gaussian noise injection. The results of using noise injection in reinforcement learning to pretrain modules is shown in Fig. 8. Note that we refer to IL ablations as experiments where modules are first pretrained with imitation learning, and subsequently, the boss controller is reinitialized and retrained with RL to test the improvement in sample efficiency. RL ablations involve the second RL phase, but the pretraining stage is done with RL as well. In addition to training the modular architecture from scratch, we experiment with two methods of injecting noise during RL. The first is naive noise injection (NNI), where we inject noise into **B**'s output when sampling rollouts and computing policy gradients. For the second, we adopt the Selective Noise Injection (SNI) technique proposed by  for applying regularization methods with stochasticity in RL. SNI stabilizes training by sampling rollouts deterministically and computing the policy gradient as a mixture of gradients from the deterministic and stochastic policies. However, even with SNI, the pretrained modules do not improve training efficiency.

### Analysis

We examine how the noise injection objective forces the modules to learn a better representation of the actuator space. As discussed in Section 2, the trajectories produced by a successful policy often lie on a much lower-dimensional manifold than the actuator space. Each dimension of the manifold can be interpreted as an individual skill that the policy has learned. We can measure the dimensionality of the modules' mapping by looking at the Jacobian matrix of the worker modules with respect to the boss's signal. The trajectories outputted by a policy can likely be captured by a few dimensions of high variance corresponding to a small set of large singular values in addition to a much larger set of dimensions of lower variance corresponding to relatively small singular values.

We visualize this effect by 1) computing the Jacobians at the trajectory input states of a successful policy and 2) normalizing the singular values of each Jacobian by its largest singular value and plotting the resulting values in the  range. We expect that a module that optimizes the invariance to noise objective will have only a small number of large singular values, with the rest being close to zero. Conversely, modules that do not produce a low-dimensional manifold would have more singular values of similar magnitude, resulting in the distribution's mass clustering close to 1. We verify this intuition by sampling 100 trajectories from an expert controller for the 6 leg centipede shown in Fig. 1. We compare the plots of the normalized singular values between MeMo and MeMo without noise injection in Fig. 9. Without noise injection, the majority of the values are close to 1. At the other extreme, with MeMo, the values are highly clustered to the left, implying that most singular values are much smaller than the biggest singular value. We plot the singular value distributions of additional MeMo ablations in Appendix A.12.

## 6 Conclusion

In this paper, we propose a modular architecture for robot controllers, in which a higher-level boss controller coordinates lower-level modules that control shared physical assemblies. We train the architecture end-to-end with noise injection, which ensures that the lower-level modules do not overrely on the boss controller's signal. In locomotion and grasping environments, we demonstrate that our pretrained modules outperform both GNN and Transformer-based methods when transferring from simple to complex morphologies and to different tasks. We ablate components of MeMo and demonstrate that the entire framework is necessary to achieve these generalization benefits.

Figure 9: **Singular Value Distributions of Actuator-Boss Jacobians.** For modular architectures trained with and without the noise injection, we plot the normalized singular values of Jacobian matrices over an expert’s trajectories. With noise injection, the mass of the distribution is much closer to 0, showing that the modules learn better representations of the actuator space.