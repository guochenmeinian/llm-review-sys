# Complexity of Derivative-Free Policy Optimization for Structured \(\mathcal{H}_{\infty}\) Control

# Complexity of Derivative-Free Policy Optimization for Structured \(_{}\) Control

Xingang Guo

ECE, CSL

UIUC

xingang2@illinois.edu &Darioush Keivan

MSE, CSL

UIUC

dk12@illinois.edu &Geir Dullerud

MSE, CSL

UIUC

dullerud@illinois.edu &Peter Seiler

EECS

UIUC

pseiler@umich.edu &Bin Hu

ECE, CSL

UIUC

binhu7@illinois.edu

###### Abstract

The applications of direct policy search in reinforcement learning and continuous control have received increasing attention. In this work, we present novel theoretical results on the complexity of derivative-free policy optimization on an important class of robust control tasks, namely the structured \(_{}\) synthesis with static output feedback. Optimal \(_{}\) synthesis under structural constraints leads to a constrained nonconvex nonsmooth problem and is typically addressed using subgradient-based policy search techniques that are built upon the concept of Goldstein subdifferential or other notions of enlarged subdifferential. In this paper, we study the complexity of finding \((,)\)-stationary points for such nonsmooth robust control design tasks using policy optimization methods which can only access the zeroth-order oracle (i.e. the \(_{}\) norm of the closed-loop system). First, we study the exact oracle setting and identify the coerciveness of the cost function to prove high-probability feasibility/complexity bounds for derivative-free policy optimization on this problem. Next, we derive a sample complexity result for the multi-input multi-output (MIMO) \(_{}\)-norm estimation. We combine this with our analysis to obtain the first sample complexity of model-free, trajectory-based, zeroth-order policy optimization on finding \((,)\)-stationary points for structured \(_{}\) control. Numerical results are also provided to demonstrate our theory.

## 1 Introduction

Policy optimization techniques have received increasing attention due to their impressive performance in reinforcement Learning (RL) and continuous control tasks . Despite the empirical successes, the theoretical properties of policy-based RL methods have not been fully understood, even on relatively simple linear control benchmarks. This has motivated a line of recent work developing sample complexity theory of model-free policy optimization on benchmark linear control problems such as linear quadratic regulator (LQR) , stabilization , linear robust/risk-sensitive control , Markov jump linear quadratic control , and distributed LQR . These existing sample complexity results heavily rely on the fact that the cost functions in these benchmark problems are differentiable over the entire feasible set, and hence cannot be applied to cover the \(_{}\) robust control setting where the objective function is nonsmooth in the first place . In this paper, we make a meaningful initial step to bridge this gap bydeveloping a novel complexity theory for an important class of \(_{}\) robust control problems, namely the structured \(_{}\) synthesis with static output feedback.

\(_{}\) synthesis is arguably the most fundamental paradigm for robust control . Therefore, it is important to understand the complexity of policy optimization on such problems. In this work, we will develop complexity theory for derivative-free policy optimization on the structured static output-feedback \(_{}\) synthesis problem . Notice that this is an important class of \(_{}\) control problems with practical importance  due to the following reasons.

* Structured control refers to fixing the structure of the controller/policy without using the order information of the plant to be controlled. In practice, the state dimension of the true system is typically unknown (e.g. the system is assumed to be rigid body dynamics, but there will always be flexible modes with unknown state order for the true plant due to elasticity or other unmodeled dynamics). It can be very challenging to learn the exact order of the plant . Therefore, it is preferred to fix the structure of the controller/policy beforehand.
* Static output feedback is one of the most important forms of structured controller used in control, and it has a long history dating back to . Static output feedback has been extensively studied and applied in various areas, including aerospace , robotics , and chemical engineering . In addition, the static output-feedback setting covers several important problems as special cases such as distributed controller design  and proportional-integral-derivative (PID) control .

Studying the complexity of policy optimization methods on structured \(_{}\) synthesis is of great importance for several reasons. First, this problem cannot be solved using convexification techniques  and hence it is natural to pose this problem as policy optimization . This is significantly different from the full-order \(_{}\) control design which yields convex reparameterizations . Specifically, if the order (i.e. state dimension) of the plant is exactly known, one can use a dynamical controller whose order is the same as the order of the plant, and the resultant full-order \(_{}\) synthesis can be lifted as a convex optimization problem. However, if the plant order is unknown and the controller structure is fixed beforehand, the resultant structured \(_{}\) problem is non-convex by nature . Second, the objective function in structured \(_{}\) synthesis can be non-differentiable over important feasible points (e.g. stationary points), and hence the structured \(_{}\) synthesis is a nonconvex nonsmooth optimization problem by nature. In the model-based setting, various advanced nonconvex nonsmooth optimization methods have been developed to solve this problem , leading to practical toolboxes such as HIFOO . Some convergence analysis has also been recently developed in . However, in the model-free setting, the sample complexity of this problem remains largely unknown. The nonsmoothness of the cost function raises new challenges in developing the complexity analysis. There is a gap between the existing sample complexity theory and the model-free policy optimization on nonsmooth \(_{}\) synthesis. Our work will bridge this gap via offering the first sample complexity result of model-free policy optimization on nonsmooth \(_{}\) robust control. Specifically, we study the complexity of derivative-free policy optimization for structured \(_{}\) static output feedback problem when we can only access the zeroth-order oracle. We consider both exact and inexact zeroth-order oracle settings. Our contributions are two-fold:

1. _Exact_ zeroth-order oracle: We establish the coerciveness of the cost function and then provide high-probability feasibility and complexity bounds by leveraging the relationship between Goldstein's subdifferential and uniform smoothing. Specifically, we show that to find a \((,)\)-stationary point, the required number of iterations is on the order of \((})\).
2. _Inexact_ zeroth-order oracle: We first derive sample complexity bounds for the MIMO \(_{}\) estimation, which are then combined to establish the high-probability sample complexity bounds on the model-free, trajectory-based zeroth-order policy optimization methods for structured \(_{}\) problem. In particular, we show that the sample complexity bounds for finding a \((,)\)-stationary point under inexact oracle setting are on the order of \((})\). To the best of our knowledge, this is the first sample complexity result for model-free policy optimization on nonsmooth \(_{}\) synthesis.

More discussions on related work.Our paper focuses on the sample complexity of model-free policy optimization. Other than sample complexity, many other results on optimization landscape and algorithm convergence of policy optimization for control have also been recently developed in the literature [9; 21; 79; 78; 75; 74; 16; 58; 39; 37; 38; 69; 66; 36]. See  for a comprehensive survey.

## 2 Preliminaries and Problem Formulation

### Notation

Let \(^{d}\) denote the \(d\)-dimensional real vectors, \(^{m n}\) denote the set of real matrices with dimension \(m n\). For a matrix \(A\), the notations \(A^{}\), \(_{}(A)(\|A\|)\), \(\|A\|_{F}\), \((A)\) denote its transpose, largest singular value (spectral norm), Frobenius norm, and spectral radius, respectively. The symbol \(_{r}(x)\) denotes a closed Euclidean ball of radius \(r\) around a point \(x\). Consider a sequence \(:=\{w_{0},w_{1},\}\) with \(w_{t}^{n_{w}}\) for all \(t\). This sequence belongs to \(_{2}^{n_{w}}\) if \(_{t=0}^{} w_{t}^{2}<\) where \( w_{t}\) is the standard 2-norm of a vector. In addition, the 2-norm for \(_{2}^{n_{w}}\) is defined as \(^{2}:=_{t=0}^{} w_{t}^{2}\). We say a function \(f(x)=O(g(x))\) if \(_{x}<\), \(f(x)=(g(x))\) if \(_{x}>0\), and \(f(x)=(g(x))\) if \(f(x)=O(g(x))\) and \(f(x)=(g(x))\). Given a matrix sequence \(\{P_{k}^{n n}\}_{k_{+}}\), let \(T(P)\) denote the infinite Toeplitz matrix \(T(P)=(P_{i-j})_{i,j=0}^{,i}\) and \(T_{N}(P)\) denote the \(N N\) block Toeplitz matrix \(T_{N}(P)=(P_{i-j})_{i,j=0}^{N-1,i}\).

### Static output feedback \(_{}\) control

Consider the following linear time-invariant (LTI) system

\[x_{t+1} =Ax_{t}+Bu_{t}+w_{t},\ x_{0}=0\] (1) \[y_{t} =Cx_{t}\]

where \(x_{t}^{n_{x}}\) is the system state, \(u_{t}^{n_{u}}\) is the control action, \(w_{t}^{n_{w}}\) is the disturbance, and \(y_{t}^{n_{y}}\) is the output measurement. We have \(A^{n_{x} n_{x}}\), \(B^{n_{x} n_{u}}\), \(C^{n_{y} n_{x}}\), and \(n_{w}=n_{x}\). The initial condition is fixed as \(x_{0}=0\). We denote \(:=\{w_{0},w_{1},\}\)1.

In this work, we consider the static output feedback defined as \(u_{t}=-Ky_{t}=-KCx_{t}\), where \(K^{n_{u} n_{y}}\) is a constant matrix. Then the closed loop system is given by

\[x_{t+1}=(A-BKC)x_{t}+w_{t},\,x_{0}=0.\] (2)

The structured \(_{}\) synthesis with static output feedback is defined as the following minimax problem

\[_{K}_{:  1}_{t=0}^{}x_{t}^{}(Q+C^{}K^{}RKC)x_{t},\] (3)

where \(\) is the set of all linear static output feedback stabilizing policies, i.e. \(=\{K^{n_{u} n_{y}}:(A-BKC)<1\}\) and we consider the standard quadratic cost function with worst case disturbance \(\) that satisfies the \(_{2}\) norm bound \( 1\). This is different than the conventional Linear Quadratic Regulator (LQR) problem where only the stochastic disturbance is considered. Throughout this paper, we also adopt the following standard assumption .

**Assumption 2.1**.: There exists a \(K^{0}\). The matrices \(Q\) and \(R\) are positive definite. The matrix \(C\) is full row rank.

In general, the problem of finding a stablizing static output feedback policy is known to be NP-hard . For the developments of policy optimization theory, it is standard to assume that such an initial stabilizing policy is available.

In the frequency domain, we can show that the above cost function (3) is equivalent to the \(_{}\) norm of the associated closed-loop transfer function . Specifically, the structured \(_{}\) static output-feedback control problem can be formulated as

\[_{K}J(K),\] (4)where \(J(K)\) is the \(_{}\) norm of the associated closed-loop transfer function that can be calculated as

\[J(K)=_{[0,2]}_{}(Q+C^{}K^{} RKC)^{}(e^{j}I-A+BKC)^{-1}.\] (5)

More details on the derivation of (5) can be found in the supplementary material. We emphasize that the policy search problem (4) is a nonconvex nonsmooth optimization problem. There are two sources of nonsmoothness for the cost (5). Specifically, computing the largest singular value and taking supremum over the frequency \([0,2]\) can both be nonsmooth. Consequently, the objective function (5) can be non-differentiable over some important feasible points [2; 3; 4; 31; 12; 18].

### Subgradient methods in the model-based setting

A key concept in the nonconvex nonsmooth optimization theory is the so-called Clarke subdifferential [15; 43]. A function \(J:\) is said to be locally Lipschitz if for any bounded set \(S\), there exists a constant \(L\) such that \(|J(K)-J(K^{})| L\|K-K^{}\|_{F}\) for all \(K,K^{} S\). It is well known that the closed-loop \(_{}\) cost (5) is locally Lipschitz over the set of stabilizing controllers . For a locally Lipschitz function, the Clarke subdifferential exists and is defined as \( J(K):=\{_{l} J(K^{l}):K^{l} K,K^ {l}( J)\}\), where \(\) stands for the convex hull . In the model-based setting, structural \(_{}\) synthesis is typically solved using advanced nonsmooth optimization algorithms that generate good descent directions via enlarging the Clarke subdifferential. Two main types of algorithms are briefly reviewed as follows.

**Goldstein's subgradient method and gradient sampling.** The main workhorse for the HIFOO toolbox is the gradient sampling method , which is developed based on the concept of Goldstein subdifferential . Specifically, the Goldstein \(\)-subdifferential for a point \(K\) is defined as

\[_{}J(K):=\{_{K^{}_{}( K)} J(K^{})\},\] (6)

which implicitly requires \(_{}(K)\). It is well known that the minimum norm element of the Goldstein subdifferential generates a good descent direction, i.e. \(J(K- H/\|H\|_{F}) J(K)-\|H\|_{F}\) for \(H\) being the minimum norm element of \(_{}J(K)\). Although calculating the exact minimum norm element from the Goldstein subdifferential can be difficult, one can still estimate a good descent direction from approximating \(_{}J(K)\) as the convex hull of randomly sampled gradients over \(_{}(K)\). This leads to the gradient sampling method.

**Frequency-domain methods.** Another popular technique is generateing the descent directions via enlarging the Clarke subdifferential in the frequency domain [17; 54]. Such an enlargement method relies on standard chain rules to exploit the fact that the closed-loop \(_{}\) cost can be rewritten as a composition of a smooth mapping and a convex norm. The Matlab function Hinfstruct from the robust control package  is developed based on such a frequency-domain technique.

### Randomized smoothing

Our analysis relies on the concept of randomized smoothing techniques, which have been widely used in convex/nonconvex optimization problems [19; 27]. The smoothed version of \(J(K)\) via uniform randomized smoothing is defined as below.

**Definition 2.2**.: Consider an \(L\)-Lipschitz function \(J\) (possibly nonconvex and nonsmooth) and a uniform distribution \(\) on \(\{U^{n_{u} n_{y}}:\|U\|_{F}=1\}\). Then the smoothed version of \(J\) is defined as \(J_{}\) is defined as

\[J_{}(K)=_{U}[J(K+ U)]\] (7)

The above definition requires both \(K\) and \(K+ U\) to belong to the feasible set \(\) for all \(U\). Very recently,  established the relationship between Goldstein subdifferential and uniform smoothing. We briefly restate such connections below.

**Lemma 2.3**.: _Suppose \(J(K)\) is \(L\)-Lipschitz over some bounded set \(\), and for any \(K\), we have \(K+ U\) for all \(U\) with \(\|U\|_{F}=1\) such that \(J_{}(K)\) is well defined (see Definition 2.2). Let \(_{}J(K)\) be the Goldstein \(\)-subdifferential at \(K\), then for any \(K\), we have: (i) \(|J(K)-J_{}(K)| L\), (ii) \(J_{}\) is differentiable and \(L\)-Lipschitz with the \(}{}\)-Lipschitz gradient, where \(d=n_{y}n_{u}\) is the problem dimension and \(c>0\) is a constant, (iii) \( J_{}(K)_{}J(K)\)._This lemma is essentially [46, Proposition 2.3, Theorem 3.1]. Based on the definition of Goldstein \(\)-subdifferential (6), we say a point \(K\) is a \((,)\)-stationary point if \((0,_{}J(K))\). Then Lemma 2.3 (iii) implies that if \(K\) is a \(\)-stationary point of \(J_{}(K)\) (i.e., \(\| J_{}(K)\|_{F}\)), then \(K\) is also a \((,)\)-stationary point of the original function \(J(K)\).

Suppose that we can compute \( J_{}(K)\), then we can simply apply gradient descent based on the gradient of \(J_{}(K)\)

\[K^{+1}=K^{t}- J_{}(K^{t})\] (8)

to find a \(\)-stationary point of \(J_{}(K)\). However, how to choose the stepsize \(\) and smooth radius \(\) to guarantee the feasibility and convergence to an \((,)\)-stationary points of the original cost function (5) is unknown. We will discuss this setup more in Section 3.2.

### Problem formulation: two zeroth-order oracle assumptions

In this paper, we aim to minimize the cost function \(J(K)\) in the policy space directly via derivative-free methods. Specifically, we consider two different zeroth-order oracle settings:

1. _Exact_ zeroth-order oracle: This oracle assumption is standard for zeroth-order optimization literature and natural for the model-based control setting. In particular, we assume that we can exactly calculate \(J(K)\) (which is the closed-loop \(_{}\) norm) for every stabilizing \(K\). When the system dynamics are known, such an oracle is available since the closed-loop \(_{}\) norm can be efficiently calculated using existing robust control packages in MATLAB (currently, the state-of-the-art techniques for model-based \(_{}\) norm calculations rely on using the relation between the singular values of the transfer function matrix and the eigenvalues of a related Hamiltonian matrix [6; 8]). Under the same oracle setting, the non-derivative sampling method has been successfully applied to state-feedback \(_{}\) control problem in  without complexity guarantee. In this work, we close this gap by showing that, for constrained policy optimization problem (4), our algorithm returns a \((,)\)-stationary point with high-probability feasibility/complexity bounds.
2. _Inexact_ zeroth-order oracle: This oracle assumption is relevant for the model-free learning-based control setting, where the system dynamics are unknown, and \(J(K)\) (the closed-loop \(_{}\) norm) can only be estimated from the input/output data of a black-box simulator of the underlying system. In particular, we use the model-free time-reversal power-iteration-based \(_{}\) estimation from  to serve as the inexact oracle for \(J(K)\). Despite the existence of such algorithms, the prior literature lacks sample complexity bounds for general MIMO systems. Therefore, we presented the first sample complexity result for \(_{}\) norm estimation for general MIMO systems. Building upon this, we obtain the first sample complexity results for model-free policy optimization of \(_{}\) control with noisy function values.

## 3 Main Results

In this section, we present our main result for the exact zeroth-order oracle case. We will start with the optimization landscape of the policy optimization problem (4).

### Optimization landscape

**Proposition 3.1**.: _The set \(=\{K:(A-BKC)<1\}\) is open and nonconvex. It can be disconnected. In general, it can be either bounded or unbounded. The cost function (5) is locally Lipschitz over \(\) and hence is continuous in \(K\)._

The proof of the above proposition can be found in [10; 22; 3]. Next, we identify the coerciveness of the cost function \(J(K)\).

**Lemma 3.2**.: _The \(_{}\) objective function \(J(K)\) defined by (5) is coercive over the set \(\) in the sense that for any sequence \(\{K^{l}\}_{l=1}^{}\) we have_

\[J(K^{l})+\]

_if either \(\|K^{l}\|_{F}+\), or \(K^{l}\) converges to an element in the boundary \(\)._It is worth mentioning that the above lemma relies on the assumption that both \(Q\) and \(R\) are positive definite. The formal proof of Lemma 3.2 is deferred to the supplementary material. The continuity and coerciveness of \(J(K)\) directly lead to the following results.

**Lemma 3.3**.: _Consider the \(_{}\) output feedback policy search problem (4) with the objective function \(J(K)\) defined in (5). Under Assumption 2.1, for any \(>J^{*}\), the sublevel set defined as \(_{}:=\{K:J(K)\}\) is compact._

_Remark 3.4_.: Based on Lemma 3.3 and the fact that the set \(\) is open, we can show that there is a strict separation between the sublevel set \(_{}\) and \(\). It is obvious that \(\) is closed. Since \(_{}\) is compact and \(_{}=\), we have \((_{},)>0\).

### Warm-up: convergence of smoothed function

In this section, we assume that the exact gradient oracle \( J_{}(K)\) is available for each \(K\) such that \(J_{}(K)\) is well defined. Then we analyze the finite-time convergence and feasibility of the vanilla gradient descent method (8). To this end, let \(K^{0}\) be an arbitrary feasible initial controller. Without loss of generality, define two feasible sets:

\[^{0}:=\{K|J(K) 50J(K^{0})\},\ ^{1}:=\{K|J(K) 1 00J(K^{0})\}.\] (9)

Denote \(_{0}=(^{0},)\) and \(_{1}=(^{1},)\). Let \(L_{0}\) and \(L_{1}\) be the Lipschitz constants of \(J(K)\) associated with the sublevel set \(^{0}\) and \(^{1}\), respectively. It is obvious that \(_{0}_{1}\) and \(L_{0} L_{1}\). In addition, we have \(=(^{0},^{1})>0\) by Remark 3.4.

_Remark 3.5_.: Since \(J(K)\) is a continuous function of \(K\) by Proposition 3.1 and the sublevel sets \(^{0}\) and \(^{1}\) are compact by Lemma 3.3, this implies that there exists a constant \(>0\) such that for any \(K^{0}\), and \(K^{}\) with \(\|K-K^{}\|_{F}\), we have \(K^{}^{1}\).

Now we are ready to state the following result.

**Theorem 3.6**.: _Let \(K^{0}\) be an arbitrary feasible initial controller and suppose that \(J_{}(K)\) defined as in (7) is \(L_{1}\)-Lipschitz with the \(}{}\)-Lipschitz gradient on the sublevel set \(^{1}\). Choose \(=\{_{1},)}{2L_{1}}\}\) and \(=\{},}\}\). Then the iterative method (8) stays in \(\) and we have:_

\[_{t=0,1,,T-1}\| J_{}(K^{t})\|_{F}^{2}(K^{0})-J^{*})}{ T}.\] (10)

_In other words, we have \(_{0 t T-1}\| J_{}(K^{t})\|_{F}\) after \(T=O(})\)._

The proof of Theorem 3.6 can be found in the supplementary material. Since we have a constrained optimization problem, it is crucial to guarantee the feasibility of the gradient descent method (8). The careful choice of smooth radius \(\) and step size \(\) ensures that \(K^{t}\) stays inside the feasible set.

### Analysis for the exact oracle case

In this section, we consider the exact zeroth-order oracle setup, where we can obtain the exact \(_{}\) norm of the closed-loop system for a given \(K\). One straightforward extension of (8) is to compute an unbiased estimation of the gradient \( J_{}(K^{t})\) via the zeroth-order oracle. Then we can perform the one-step gradient descent. In particular, we analyze the feasibility and convergence of the Algorithm 1 in the following result.

**Theorem 3.7**.: _Let \(K^{0}\) be an arbitrary feasible initial controller and suppose \(J(K)\) is \(L_{1}\)-Lipschitz with the \(}{}\)-Lipschitz gradient on the sublevel set \(^{1}\). Let \(1 80\) and suppose we choose_

\[=\{_{1},,)}{L_{1}}\},\ T=)}{50^{2}cd^{3/2}L_{1}^{3}},\ =\{)-J^{*})},}{3 500cd^{3/2}L_{1}^{3}}\}.\]

_Then the following two statements hold: (1) the controllers \(\{K^{t}\}_{t=0}^{T-1}\) generated by Algorithm 1 are all stabilizing with probability at least \(0.95-0.01\). (2) the output of Algorithm 1\(K^{R}\) satisfies_

\[\{\|H\|_{F}:H_{}J(K^{R})\}\] (11)

_with probability at least \(0.87-0.17^{-}-0.01\)._The proof of Theorem 3.7 can be found in the supplementary material. Now we provide some discussions regarding Theorem 3.7 in order:

**Probability Bounds:** Statement 1 suggests that as \(T\) increases, the probability that all the generated controllers are stabilizing will decrease. This is because our algorithm uses a zeroth-order oracle to build an estimator of the smoothed function gradient. As \(T\) increases, the biases and variance of the gradient estimation accumulate, resulting in a larger failure probability. In addition, Statement 2 suggests that as \(T\) increases, the probability of finding a \((,)\)-stationary point will first increase and then decrease. Indeed, when \(T\) is too small, more iterations will improve the performance of the generated controllers, while for large \(T\), the probability of generating unstable controllers becomes dominant. In addition, the constant factors in the probability bounds are not restrictive, they can be further improved by e.g., increasing the level of \(^{0}\), using smaller step size \(\) or using smaller smooth radius \(\) in the analysis.

**Feasibility:** Statement 1 states that all the output controllers \(\{K^{t}\}_{t=0}^{T-1}\) are feasible with high probability under the choice of algorithm parameters. It is worth mentioning that, to ensure the feasibility of the iterates \(\{K^{t}\}_{t=0}^{T-1}\), we need to show that \(\{J_{}(K^{t})\}_{t=0}^{T-1}\) are well defined so that we can apply the smoothness property of \(J_{}(K)\) in our theoretical analysis. Furthermore, this also guarantees that \(\{J(K^{t} W^{t})\}_{t=0}^{T-1}\) used in Algorithm 1 are well defined. This turns out to be a nontrivial task and one needs to design algorithm parameters carefully to resolve the feasibility issue. More discussions will be provided in the end of this section.

**Output Controller:** Since our cost function \(J(K)\) is nonconvex, we consider an output controller \(K^{R}\) uniformly sampled from \(\{K^{t}\}_{t=0}^{T-1}\). Such random output technique has been used in smooth/nonsmooth nonconvex optimization problems for theoretical analysis . In particular, Statement 2 implies that the output controller \(K^{R}\) is a \((,)\)-stationary point with high probability. In practical implementation, one can just select the iterate that gives the lowest cost \(J(K^{t})\) for \(t=1,,T\). Our numerical experiments suggest that selecting \(K^{T}\) often yields satisfactory performance (see Section 5).

**Sample Complexity:** For sufficiently small \(\), the number of iterations to guarantee (11) is given by:

\[T=(}L_{1}^{3}}{^{4}}),\] (12)

where \(d 1\) is the problem dimension, \(L_{1}\) is the Lipschtiz parameter. We also ignore the numerical constants since they are conservative and not restrictive.

Finally, we close this section by highlighting the main technical contributions of Theorem 3.7 and 3.6. In our control setup, unlike the unconstrained optimization problems , we need to ensure that the iterate \(K^{t}\) and the perturbed iterate \(K^{t} W^{t}\) stay within a non-convex feasible set \(\). Previous work on policy optimization theory of \(_{}\) control addresses this feasibility issue via using the coerciveness of \(J(K)\) and mainly relies on the fact that \(J(K)\) is a barrier function on the non-convex set of stabilizing policies . In particular, such previous results rely on model-based algorithms (such as Goldstein's subgradient method) which can decrease the value of \(J(K)\) directly. However, the zeroth-order policy search can only decrease the value of the smoothed function \(J_{}(K)\), which is not coercive over the non-convex feasible set and hence cannot be used as a barrier function. Importantly, the descent of \(J_{}(K)\) does not imply the descent of the original function value and hence cannot ensure feasibility by itself. In Theorem 3.7 and 3.6, by carefully choosing the smooth radius \(\) and step size \(\), we manage to show that the iterate \(K^{t}\) and the perturbed iterate \(K^{t} W^{t}\) stay within a non-convex feasible set with high probability.

## 4 Sample Complexity for the Model-Free Case

In this section, we assume that the system dynamics in (1) are unknown and we can only use samples to obtain an estimation of the cost function \((K)\) with some error \((K)=J(K)+(K)\). To obtain an end-to-end sample complexity result of Algorithm 1, we first derive the sample complexity of \(_{}\) norm estimation for the general MIMO systems such that \((K)\) for all \(K\).

### Complexity of \(_{}\) norm estimation for MIMO systems

The \(_{}\) norm estimation method via input/output data can be roughly categorized into two major approaches: (a) try to find the worst case \(_{2}\)-norm signal using power-iteration algorithm [71; 53]. (b) discretizing the interval \([0,2]\) and search for the maximizing frequency using multi-armed bandit . However, there are no sample complexity results of the aforementioned methods for the general MIMO system. In this section, we analyze the \(_{}\) norm estimation method proposed in  and establish its sample complexity bounds for the general MIMO systems.

To this end, let \(P(z)=(Q+C^{T}K^{T}RKC)^{1/2}(zI-A+BKC)^{-1}\) be the transfer function associated with the system (1) to the cost function (3), let \(T(P)\) denote the corresponding Toeplitz (convolution) operator and \(T_{N}(P)\) be the \(N N\) upper-left submatrix of \(T(P)\). Then it is known that \(J(K)=\|T(P)\|\) and \(\|T_{N}(P)\|\|T(P)\|\) as \(N\). With large enough \(N\), the largest singular value of the \(T_{N}(P)\) can be used as a reasonable estimation of \(J(K)\), which can be approximated by running the power iteration algorithm \(n\) steps 2. Therefore, there are two error terms based on this approach. The first term \(_{1}(K)\) comes from the difference between \(\|T_{N}(P)\|\) and \(\|T(P)\|\), and the second term \(_{2}(K)\) is induced by computing the largest singular value of \(T_{N}(P)\) via the power iteration algorithm:

\[|(K)|=|J(K)-(K)||J(K)-\|T_{N}(P)\|+\|\|T_{N}(P)\|-(K)|= _{1}(K)+_{2}(K),\] (13)

where \((K)\) is the approximated largest singular value of \(T_{N}(P)\) and hence an estimation of \(J(K)\). Therefore, if we can choose \(N\) and \(n\) large enough such that \(_{1}(K)/2\) and \(_{2}(K)/2\), we will have \((K)\) holds by (13). The finite-time condition on \(N\) has been established in  for single input single output (SISO) systems only. In the following result, we extend the result in  to the general MIMO system. Furthermore, we provide the finite-time condition on the number of power iteration iterations \(n\).

**Theorem 4.1**.: _Let \(P(z)=_{k=0}^{}P_{k}z^{-k}\) be the corresponding transfer function of system (1) to the cost function (3) with stability radius \((0,1)\). And choose \((,1)\) and \(D_{max}(P_{0})\). Let \(\|P\|_{}\) denotes the \(_{}\) norm of the system \(P^{}(z):= P( z)\). Then for \(N 3\), we have_

\[|(K)-J(K)|_{1}(K)+_{2}(K)\] (14)

_where_

\[_{1}(K) =C_{1}\|_{}(1-^{2})+\|P^{}\| _{}^{2}}{\|P\|_{}(1-)^{4}}}+C_{2}\|_{}^{2}}{\|P\|_{}(1+)(1-)^{5}}},\] (15) \[_{2}(K) =C_{3}\|P\|_{}n^{-}\] (16)

_are errors due to approximating \(T\) by \(T_{N}\) and using power iteration to estimate \(\|T_{N}\|\), respectively. Here \(C_{1}=3(2+3^{4}),C_{2}=9^{2}\) are universal constants and \(C_{3}\) depends on an angle between power iteration initialization and the eigenvector corresponding to the biggest eigenvalue of \(T_{N}^{*}(P)T_{N}(P)\)._

The proof of Theorem 4.1 can be found in the supplementary material. Theorem 4.1 implies that the approximation error \(|(K)|\) is guaranteed by choosing

\[N_{<<1}(}\|_{}^{2}}{\|P\|_{}}}),\ \ n()^{}\|P\|_{}^{}}{ ^{}}).\] (17)

### Sample complexity of zeroth-order optimization

In this section, we consider that Algorithm 1 can only access the inexact zeroth-order oracle of the cost function \(J(K)\). In this case, we do not have an unbiased estimation of \( J_{}(K^{t})\). Nevertheless, we can make the estimation as small as possible and assume that the estimation error \((K)\) is uniformly upper bounded for all \(K^{1}\):

\[|(K)|,\ \ K^{1}.\] (18)

Clearly, the smaller \(\) is, the larger power iteration number \(n\) and approximation horizon \(N\) are needed. We will specify the choice of \(\) in the next result, where we obtain the feasibility/sample complexity bounds of Algorithm 1 via inexact zeroth-order oracle on finding a \((,)\)-stationary point.

**Theorem 4.2**.: _Let \(K^{0}\) be an arbitrary feasible initial controller and suppose \(J\) is \(L_{1}\)-Lipschitz on the sublevel set \(^{1}\). Consider the Algorithm 1 with inexact zeroth-order oracle and suppose that the estimation error \(}{100dL_{1}}\). Denote \(:=cL_{1}(16dL_{1}^{2}+()^{2})\). Let \(1 25\) and suppose we choose_

\[=\{_{1},,)}{L_{1}}\},\ T=)}{2^{2}},\ =\{)-J^{*})},}{100}\}.\]

_Then the following two statements hold: (1) the controllers \(\{K^{t}\}_{t=0}^{T-1}\) generated by Algorithm 1 with inexact zeroth-order oracle are all stabilizing with probability at least \(0.95-0.03\). (2) the output of Algorithm 1\(K^{R}\) satisfies_

\[\{\|H\|_{F}:H_{}J(K^{R})\}\] (19)

_with probability at least \(0.8-0.15^{-}-0.03\)._

The proof of Theorem 4.2 can be found in the supplementary material. For sufficiently small \(\), we can obtain the following lower bound on the number of iterations to guarantee (19):

\[T=(}L_{1}^{3}}{^{4}}),\] (20)

Since we require \(}{100dL_{1}}\), we have the following lower bounds on the power iteration number \(n\) and approximation horizon \(N\) by (17):

\[N=(L_{1}^{1/2}}{^{1/2}}),\ \ n=()^{}}{^{}^{3} }).\]

Finally, by combining the above sample complexity bounds for MIMO \(_{}\) estimation with (20), the number of samples to guarantee (19) with high probability is given by:

\[nNT=(L_{1}^{5}}{^{3}^{8}}).\] (21)

In this section, we consider the sample complexity of Algorithm 1 with the inexact oracle case. This is particularly relevant for the model-free control setting. Specifically, we are using imperfect estimates of \(J(K)\) that are calculated using the model-free MIMO power iteration method. Therefore, an extra statistical error term appears in the iterations of zeroth-order policy optimization and requires special treatment. Such an extra term has not been considered in the literature on zeroth-order optimization for nonconvex nonsmooth problems. To address this extra technical difficulty, we first establish sample complexity bounds for \(_{}\) norm estimation of the general MIMO system (Theorem 4.1). Then we carefully propagate such sample complexity bounds to obtain an error bound for \( J_{}(K)\) in terms of \(\) and \(\). Built upon this, we demonstrate that Algorithm 1 remains effective even with an inexact oracle, ensuring the feasibility of the iterates while achieving finite-time sample complexity with high probability (Theorem 4.2).

Numerical Experiments

In this section, we present the numerical study to show the effectiveness of Algorithm 1. The left plot of Figure 1 displays the relative error trajectories of Algorithm 1 with an exact oracle for system dimensions \(n_{x}=\{10,50,100\}\). The system is of form (1) with parameters \((A,B,C)\), where the entries of \((A,B,C)\) are sampled from a standard normal distribution \((0,1)\). The results show that Algorithm 1 performs well even for larger-scale systems. In the middle plot, we compare the trajectories of Algorithm 1 with both exact and inexact zeroth-order oracle. It can be observed that the inexact oracle case closely tracks the performance of the exact oracle case. The right plot illustrates the zeroth-order complexity of the exact oracle case with varying \(\). As shown, the number of oracle calls increases as \(\) decreases. It is important to note that the complexity bounds derived in (12) are not tight. We conjecture that by leveraging other advanced properties of the cost function (5), further improvements in complexity can be achieved.

In addition, we conducted a comparison of our derivative-free method with the model-based methods HIFOO and Hinfstruct, using several benchmark examples from COMPL\({}_{e}\)ib . Table 1 shows the corresponding optimal closed-loop \(_{}\) norm. It can be seen that our derivative-free method yields comparable results with the model-based packages even without the knowledge of system models. More details about the numerical experiments can be found in the supplementary material.

## 6 Conclusion and Future Work

This paper presents the feasibility and complexity bounds for the derivative-free policy optimization method on the structured \(_{}\) synthesis, considering both exact and inexact zeroth-order oracles. Despite the fact that this structured \(_{}\) synthesis is a constrained nonconvex nonsmooth optimization problem, we leverage the intriguing connections between randomized smoothing and \((,)\)-stationarity, enabling the first sample analysis for model-free, trajectory-based, zeroth-order policy optimization in structured \(_{}\) synthesis. One limitation of this work is the uncertain tightness of the sample complexity bounds obtained.

For future studies, it is important to tighten the sample complexity results by exploring more properties of \(_{}\) control problems. In addition, it is interesting to explore the behavior of the structured \(_{}\) problem with a dynamic output feedback controller.

   Example & \((n_{x},n_{u},n_{y})\) & HIFOO & Hinfstruct & Algorithm 1 **(Ours)** \\  AC15 & \((4,2,3)\) & \(15.2919\) & \(15.2\) & \(15.4141\) \\ HF2D11 & \((5,2,3)\) & \(7.7237 10^{4}\) & \(7.72 10^{4}\) & \(7.7223 10^{4}\) \\ DLR2 & \((40,2,2)\) & \(4.0066 10^{3}\) & \(4.01 10^{3}\) & \(4.0094 10^{3}\) \\ HE4 & \((8,4,6)\) & \(22.8382\) & \(22.8\) & \(22.8538\) \\   

Table 1: Comparison of Algorithm 1 with model-based methods

Figure 1: Left: The relative error trajectories of Algorithm 1 with exact zeroth-order oracle and \(n_{x}=\{10,50,100\}\), the solid lines represent the mean values and the shade represents 98% confidence intervals; Middle: The trajectory of Algorithm 1 with exact and inexact zeroth-order oracle; Right: Number of zeroth-order oracle calls required to find a \((,)\)-stationary point with varying \(\).