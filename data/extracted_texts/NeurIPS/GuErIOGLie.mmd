# Unified Segment-to-Segment Framework for

Simultaneous Sequence Generation

 Shaolei Zhang\({}^{1,2}\), Yang Feng\({}^{1,2}\)

\({}^{1}\)Key Laboratory of Intelligent Information Processing,

Institute of Computing Technology, Chinese Academy of Sciences (ICT/CAS)

\({}^{2}\)University of Chinese Academy of Sciences

zhangshaolei20z@ict.ac.cn, fengyang@ict.ac.cn

Corresponding author: Yang Feng

Code is available at: https://github.com/ictnlp/Seg2Seg.

###### Abstract

Simultaneous sequence generation is a pivotal task for real-time scenarios, such as streaming speech recognition, simultaneous machine translation and simultaneous speech translation, where the target sequence is generated while receiving the source sequence. The crux of achieving high-quality generation with low latency lies in identifying the optimal moments for generating, accomplished by learning a mapping between the source and target sequences. However, existing methods often rely on task-specific heuristics for different sequence types, limiting the model's capacity to adaptively learn the source-target mapping and hindering the exploration of multi-task learning for various simultaneous tasks. In this paper, we propose a unified _segment-to-segment framework_ (_Seg2Seg_) for simultaneous sequence generation, which learns the mapping in an adaptive and unified manner. During the process of simultaneous generation, the model alternates between waiting for a source segment and generating a target segment, making the segment serve as the natural bridge between the source and target. To accomplish this, Seg2Seg introduces a latent segment as the pivot between source to target and explores all potential source-target mappings via the proposed expectation training, thereby learning the optimal moments for generating. Experiments on multiple simultaneous generation tasks demonstrate that Seg2Seg achieves state-of-the-art performance and exhibits better generality across various tasks2.

## 1 Introduction

Recently, there has been a growing interest in simultaneous sequence generation tasks [1; 2; 3; 4; 5; 6; 7; 8; 9] due to the rise of real-time scenarios, such as international conferences, live broadcasts and online subtitles. Unlike conventional sequence generation , simultaneous sequence generation receives a streaming source sequence and generates the target sequence simultaneously, in order to provide low-latency feedback . To achieve high-quality generation under such low-latency conditions, simultaneous models must learn to establish a mapping between the target sequence and the source sequence  and thereby identify the optimal moments for generating .

Directly mapping source and target sequences is non-trivial due to inherent differences between the two sequences, such as modalities or languages, resulting in significant representational and structural gaps. For instance, in streaming automatic speech recognition (Streaming ASR) [8; 9; 14; 15; 16], speech needs to be mapped to text, while simultaneous machine translation (SimulMT) [17; 11; 4; 5; 18; 13] requires the mapping from a source language to a target language (i.e., cross-lingualalignment ). Simultaneous speech translation (SimulST) [1; 2; 3; 20; 21] encounters challenges that encompass both cross-modal and cross-lingual aspects. Therefore, developing an approach to bridge source and target is critical to simultaneous sequence generation.

Existing methods for simultaneous generation often rely on task-specific heuristics to bridge the source and target sequences. For example, streaming ASR methods assume a strong correlation between the target token and local speech, employing a fixed-width window to directly predict the corresponding word [22; 9; 14]. SimulMT methods consider that the source and target sequences have similar lengths, employing fixed wait-k policies [4; 23; 18] or attention mechanisms [5; 24; 25] to establish a token-to-token mapping. Such assumptions of similar length limit their ability to handle sequences with significant length differences . SimulST methods divide the speech into multiple segments to overcome length differences [26; 3], and then apply the fixed wait-k policy [20; 27]. These task-specific heuristics not only hinder the adaptive learning of the source-target mapping but also impede the integration of various simultaneous tasks into a unified framework, restricting the potential of utilizing multi-task learning [28; 29; 30] in simultaneous generation tasks.

Under these grounds, we aim to bridge the source and target sequences in an adaptive and unified manner without any task-specific assumptions. In simultaneous generation process, the model necessarily waits for a source segment and outputs a target segment alternately, with each segment comprising one or more tokens. As such, the source sequence and target sequence should correspond in terms of the segment and ideally agree on the segment representation , enabling the segment to serve as a natural bridge between source and target. In this paper, we propose a unified _segment-to-segment framework_ (_Seg2Seg_) for simultaneous sequence generation, which introduces latent segments as pivots between source and target. As illustrated in Figure 1, given a streaming source sequence, Seg2Seg determines whether the received source tokens can be aggregated into a latent segment. Once aggregated, the latent segment starts emitting the target tokens until the latent segment can no longer emit any further target tokens. Seg2Seg repeats the above steps until finishing generating. To learn when to aggregate and emit, Seg2Seg employs expectation training to explore all possible source-target mappings and find the optimal moments for generating. Experiments on multiple simultaneous generation tasks, including streaming ASR, SimulMT and SimulST, demonstrate that Seg2Seg achieves state-of-the-art performance and exhibits better generality across various simultaneous tasks.

## 2 Related Work

Streaming ASRRecent streaming ASR methods primarily rely on two approaches: transducer and local attention. Transducer involves a speech encoder and a label predictor, which are aligned via a joiner to determine whether to generate [32; 33; 34]. Local attention approach utilizes monotonic attention to determine whether to generate based on the speech within a local window [6; 7; 35; 22]. Moreover, various methods have been proposed to reduce the latency based on these two approaches by optimizing the alignment process [36; 37; 38; 39; 40].

SimulMTRecent SimulMT methods are mainly based on pre-defined rules or alignment mechanisms. For pre-defined rules, Ma et al.  proposed wait-k policy, which waits for \(k\) source tokens before alternately waiting/generating one token. Some methods were proposed to improve the flexibility of fixed rules through training [23; 41; 42; 43], simultaneous framework [44; 45], the ensemble of wait-k [46; 18] or post-evaluation . For alignment mechanisms, previous works employ

Figure 1: Illustration of the conventional sequence-to-sequence framework for offline generation and the proposed segment-to-segment framework for simultaneous generation.

monotonic attention [5; 24; 31], Gaussian attention , binary search , non-autoregressive structure  or hidden Markov models  to learn the alignments between the source and target token-to-token, and make waiting or generating decisions accordingly.

**SimulST** Recent SimulST methods focus on the segmentation of speech [1; 50; 51; 2]. Ma et al.  proposed fixed pre-decision to divide speech into equal-length segments, and applied SimulMT methods such as wait-k  and MMA . Some other methods first use CTC results [3; 20], ASR results  or integrate-and-firing  to detect the number of words in speech, and then apply the wait-k policy. Further, Zhang and Feng  proposed ITST, which judges whether the received information is sufficient for translation. Zhang et al.  proposed MU-ST, which constructs segmentation labels based on meaning units and uses them to train a segmentation model. Zhang and Feng  proposed differentiable segmentation (DiSeg) to directly learn segmentation from the underlying translation model via an unsupervised manner.

Previous methods for simultaneous generation often involve task-specific heuristics, which hinder adaptive learning and limit their applicability to other tasks. The proposed Seg2Seg utilizes the latent segments as pivots to achieve fully adaptive learning of source-target mapping. Furthermore, Seg2Seg serves as a unified framework for various simultaneous generation tasks, making multi-task learning in simultaneous generation feasible.

## 3 Method

In this paper, we propose a segment-to-segment framework (Seg2Seg) to map the source sequence to the target sequence, using latent segments as the pivots. With the latent segments, Seg2Seg can adaptively learn to map source to target, enabling it to find the optimal moments to generate the target tokens during the simultaneous generation process. Details are introduced in the following sections.

### Mapping with Latent Segments

Seg2Seg leverages the Transformer (encoder-decoder)  as the backbone, and further converts the sequence-to-sequence framework to the segment-to-segment framework by introducing latent segments. Formally, we denote the source sequence as \(=\{x_{1},,x_{J}\}\) with length \(J\), and the target sequence as \(=\{y_{1},,y_{I}\}\) with length \(I\). In Seg2Seg, the source tokens are first aggregated into several latent segments (source tokens\(\)latent segment), and then the latent segment emits the target tokens (latent segment\(\)target tokens), as shown in Figure 2.

Source tokens\(\) Latent segmentFor aggregation, Seg2Seg produces a Bernoulli variable \(a_{j}\) for each source token \(x_{j}\) to determine whether the currently received source tokens can be aggregated into a segment. An aggregation probability \(_{j}\) is predicted as the parameter for the variable \(a_{j}\), calculated as:

\[_{j}=(((x_{j} ))), a_{j}(_{j}),\] (1)

where \(()\) is a feed-forward network, \((x_{j})\) is the representation of \(x_{j}\), and \(_{j}\) is aggregation probability at \(x_{j}\). As shown in Figure 2, if \(a_{j}=0\), Seg2Seg waits for the next input, otherwise, it aggregates the tokens received after the previous segment into a new segment. Once a latent segment is aggregated, we calculate its representation by summing the representations of all the source tokens it contains. Specifically, the representation of the \(k^{th}\) latent segment is denoted as \(_{k}\), calculated as:

\[_{k}=^{}_{x_{j} _{k}}(x_{j}),\] (2)

where \(^{}\) is the learnable projection from source to latent segment space.

Latent segment\(\) Target tokensGiven latent segment representation, Seg2Seg judges whether \(_{k}\) can emit \(y_{i}\) by producing a Bernoulli variable \(b_{ik}\) with the emission probability \(_{ik}\) as a

Figure 2: Diagram of source-target mapping with latent segments. The arrows in color and gray represent the mapping in inference and training, respectively.

parameter. Specifically, \(b_{ik}\) is calculated as a dot-product form:

\[_{ik}=(^{} (y_{i-1})_{k}^{}}{}),  b_{ik}(_{ik}),\] (3)

where \(^{}\) is the learnable projection from target to latent segment space, and \(d\) is the input dimension. During emission, if \(b_{ik}=1\), \(\) generates \(y_{i}\) based on the current received source tokens, otherwise it stops emitting and waits for the next input. Take Figure 2 as an example, \(y_{3}\) will not be emitted from \(_{1}\) as \(b_{31}=0\). After aggregating \(_{2}\), \(y_{3}\) is emitted from \(_{2}\) as \(b_{32}=1\).

Overall, Seg2Seg alternates between waiting for enough source tokens to aggregate a latent segment (i.e., wait until \(a_{j}=1\)), and outputting the target tokens until the current latent segment can no longer emit any further tokens (i.e., output until \(b_{ik}=0\)). Take the mapping in Figure 2 for instance, Seg2Seg waits for 2 source tokens and generates 2 target tokens, then waits for 3 and generates 2 tokens, then waits for 1 and generates 1 token. Figure 3(a) gives the corresponding mapping in matrix form, where the final matrix indicates whether the model receives \(x_{j}\) when generating \(y_{i}\).

### Training

During training, Seg2Seg tends to learn the aggregation and emission in an adaptive manner. However, a significant challenge arises from the use of Bernoulli variables \(a_{j}\) and \(b_{ik}\) for aggregation and emission, which prevents the back-propagation [56; 57] to the aggregation probability \(_{j}\) and emission probability \(_{ik}\). To address this issue, we propose expectation training that employs \(_{j}\) and \(_{ik}\) instead of Bernoulli variables to calculate the expected mapping, which can be jointly trained with the underlying Transformer model. As illustrated in Figure 3, in expectation training, the source tokens and target tokens are no longer forced to be associated with a single latent segment, but rather can belong to multiple latent segments by probability.

For the aggregation process from source tokens to latent segment, we introduce \(p(x_{j}_{k})\) to represent the probability that \(x_{j}\) belongs to the latent segment \(_{k}\). Since the aggregation process is monotonic with the streaming source sequence, i.e., which segment \(x_{j}\) belongs to is only related to \(x_{j-1}\), \(p(x_{j}_{k})\) can be calculated via dynamic programming:

\[p(x_{j}_{k})=p(x_{j-1}_{k-1} )_{j-1}+p(x_{j-1}_{k}) (1-_{j-1}).\] (4)

We consider all possible latent segments in the expectation training, so \(k\) ranges from \(1\) to \(J\) (i.e., aggregate at most \(J\) segments with one token in each segment), even if the source tokens may belong to the later latent segment with a small probability, as shown in Figure 3(b). With \(p(x_{j}_{k})\), we calculate the expected representation of latent segment by weighting all source tokens:

\[_{k}=^{}_{j=1}^{J}p (x_{j}_{k})(x_{j}).\] (5)

For the emission process from latent segment to target tokens, we introduce \(p(y_{i}_{k})\) to represent the probability that \(y_{i}\) can be emitted from latent segment \(_{k}\). Since the emission process is mono

Figure 3: Illustration of source-target mapping in inference and training. (a) The color indicates which latent segment the token belongs to, and the final matrix indicates whether the model receives \(x_{j}\) when generating \(y_{i}\) (i.e., the cross-attention, where the white space is masked out because those source tokens are not received yet.). (b) The shade indicates the probability that the token belongs to different latent segments, and the final matrix indicates the probability that \(y_{i}\) can pay attention to \(x_{j}\).

tonic with the simultaneous generation, \(p(y_{i}_{k})\) can be calculated via dynamic programming:

\[p(y_{i}_{k})=_{i,k}_{l=1}^{k}(p(y_{ i-1}_{l})_{m=l}^{k-1}(1-_{i,m})).\] (6)

We give a detailed introduction to the dynamic programming algorithm in Appendix A.

Learning MappingTo adaptively learn \(\) and \(\), we jointly train \(p(x_{j}_{k})\) and \(p(y_{i}_{k})\) with Transformer via the cross-entropy loss \(_{ce}\). During inference, each target token in Seg2Seg no longer focuses on all source tokens, but can only pay attention to the source token within the same latent segment or the previous segments (i.e., the current received tokens), as shown in Figure 3(a). So in training, we calculate the probability that \(y_{i}\) can pay attention to \(x_{j}\), denoted as \(_{ij}\):

\[_{ij}\!=\!_{k}p(y_{i}_{k})\!\! p(x_{j}\{_{1},,_{k}\})\!=\!_{k}p (y_{i}_{k})\!\!_{l=1}^{k}p(x_{j} _{l}).\] (7)

Then, we multiply the mapping \(_{ij}\) with the original cross-attention  and normalize it to get the final attention distribution, which is used to calculate the expected target representation. By jointly training mapping and generation via the cross-entropy loss \(_{ce}\), Seg2Seg will assign higher \(_{ij}\) between those related source and target tokens, thereby learning a reasonable mapping.

Learning LatencyBesides learning mapping for high-quality generation, we also introduce a latency loss \(_{latency}\) to encourage low latency. We utilize two commonly used latency metrics, consecutive wait (CW)  and average lagging (AL) , to calculate the expected latency of Seg2Seg, where CW measures the number of latent segments (i.e., streaming degree ), and AL measures the lagging of target token (i.e., lagging degree). Therefore, \(_{latency}\) is calculated as:

\[_{latency} =_{}()+ _{}(}),\] (8) \[_{}() =\|_{j=1}^{||}_{j}-||\|_{2}+\|(_{i}, |}{||})- ||\|_{2},\] \[_{}(}) =|}_{j=1}^{||}_{j=1}^{| |}_{ij}.\]

For the number of latent segments \(_{}()\), following Zhang and Feng , we constrain Seg2Seg via the expected segment number \(_{j=1}^{||}_{j}\) and the uniformity of aggregation, where \(()\) is the max polling operation with kernel size of \(|}{||}\). For the expected lagging \(_{}(})\), we constrain the expected lagging \(_{j=1}^{||}_{ij}\) of target token \(y_{i}\). \(\) is a hyperparameter that controls the overall latency of Seg2Seg. A larger \(\) encourages Seg2Seg to aggregate more latent segments, thereby achieving low latency. When \(\!\!0\), the number of latent segments decreases and latency becomes higher, finally degenerating into a sequence-to-sequence framework when \(\!=\!0\).

Overall, the total training objective of Seg2Seg is the trade-off between \(_{ce}\) for generation quality and \(_{latency}\) for generation latency, calculated as:

\[=_{ce}+_{latency}.\] (9)

### Inference

In inference, we set \(a_{j}=1\) when \(_{j} 0.5\) and \(b_{ik}=1\) when \(_{ik} 0.5\) without sampling . Algorithm 1 illustrates the specific inference process of Seg2Seg. Given a streaming source sequence, Seg2Seg continuously repeats the process of aggregating source tokens into a latent segment (lines 2-6) when \(a_{j}=1\) and then emitting target tokens from the latent segment (lines 8-12) while \(b_{ik}=1\), until the generation process is completed.

Owing to generating the target sequence in units of segment, it is natural for Seg2Seg to use beam search inside each target segment. Therefore, in the following experiments, we set the size of the beam search for each segment to 5.

## 4 Experiments

### Datasets

We conduct experiments on the most common benchmarks of three representative simultaneous generation tasks, including streaming ASR, SimulMT and SimulST.

Streaming ASRWe apply LibriSpeech3 benchmark , which consists of 960 hours English audio. We use dev-clean (5.4 hours) and dev-other (5.3 hours) as validation sets, and test-clean (5.4 hours) and test-other (5.1 hours) as test sets, where test-other set contains more noisy audio. For speech, we use the raw 16-bit 16kHz mono-channel audio wave. For text, we use SentencePiece  to generate a unigram vocabulary of size \(10000\).

SimulMTWe apply WMT154 German\(\)English (De\(\)En) benchmark, including 4.5M sentence pairs for training. We use newstest2013 as validation set (3000 pairs), and newstest2015 as test set (2169 pairs). 32K BPE  is applied and vocabulary is shared across languages.

SimulSTWe apply MuST-C5 English\(\)German (En\(\)De) (408 hours, 234K pairs) and English \(\) Spanish (En\(\)Es) (504 hours, 270K pairs) benchmarks . We use dev as validation set (1423 pairs for En\(\)De, 1316 pairs for En\(\)Es) and tst-COMMON as test set (2641 pairs for En\(\)De, 2502 pairs for En\(\)Es), respectively. The pre-processing is the same as streaming ASR tasks.

### Systems Settings

We conducted experiments on several strong baselines for all three tasks, described as follows.

Offline model waits for the complete source sequence before generating the target sequence. Offline model is decoded with beam 5.

# Streaming Automatic Speech Recognition (Streaming ASR)

T-T uses Transformer Transducer to determine waiting/generating via alignments from the joiner between the speech encoder and text predictor. Some methods, including ConstAlign, FastEmit and SelfAlign are proposed to further reduce the latency of the Transducer.

**MoChA** applies monotonic chunkwise attention to generate the target token based on the speech within a local window. Various training methods, such as DeCoT , MinLT  and CTC , are proposed to further constrain the latency of MoChA.

**# Simultaneous Machine Translation (SimulMT)**

**Wait-k** first waits for \(k\) source tokens, and then alternately generates and waits for one token.

**Multipath Wait-k** trains a wait-k model via randomly sampling different \(k\) between batches.

**MoE Wait-k** applies mixture-of-experts (MoE) to learn multiple wait-k policies during training.

**Adaptive Wait-k** trains a set of wait-k models (e.g., from wait-1 to wait-13), and heuristically composites these models based on their outputs during inference.

**MMA** applies monotonic multi-head attention and predicts a variable to indicate waiting or generating, which are trained through monotonic attention .

**GMA** introduces Gaussian multi-head attention and uses a Gaussian prior to learn the alignments via attention. With alignments, GMA decides when to start translating based on the aligned positions.

**GSiMT** generates waiting/generating decisions, and considers all possible decisions in training.

**HMT** proposes Hidden Markov Transformer, which uses a HMM to correspond translating moments with the target tokens, thereby learning the optimal translating moments for generating.

**# Simultaneous Speech Translation (SimulST)**

**Wait-k, MMA** for SimulMT can be applied to SimulST by making a fixed pre-decision to split the speech into 280\(ms\) durations, where each duration corresponds to one word.

**Wait-k-Stride-n** generates \(n\) tokens every \(n\!\!280ms\) to address the issue of length differences between speech and text. We set \(n\!=\!2\) following their best result.

**SimulSpeech** divides the speech based on a CTC word detector, and then applies wait-k policy.

**SH** uses the shortest hypothesis in ASR results as word number, and then applies wait-k policy.

**RealTrans** detects the word number in the streaming speech via counting the blank in CTC results, and then applies the wait-k-stride-n policy.

**MMA-CMDR** incorporates cross-modal decision regularization to MMA, which leverages the transcription of speech to improve the decision of MMA.

**MoSST** uses the integrate-and-firing method  to segment the speech based on the cumulative acoustic information, and then applies the wait-k policy.

**ITST** quantifies the transported information from source to target, and subsequently determines whether to generate output based on the accumulated received information.

**MU-ST** trains an external segmentation model based on the constructed data to detect the meaning unit, and uses it to decide whether to generate.

**DiSeg** jointly learns the speech segmentation with the underlying translation model via the proposed differentiable segmentation in an unsupervised manner.

All implementations are adapted from Fairseq Library . In Seg2Seg, we use the standard Transformer-Base (6 encoder and 6 decoder layers)  for SimulMT. For streaming ASR and SimulST, we replace the word embedding layer in Transformer-Base with a pre-trained Wav2Vec2.06 to extract the acoustic embedding, and the rest remains the same as SimulMT.

**Evaluation** We use SimulEval 7 to evaluate the quality and latency of simultaneous generation. For streaming ASR, following Inaguma and Kawahara , we use word error rate (WER) for the quality and the mean alignment delay for the latency, which considers the average difference between generating moments and ground-truth alignments. For SimulMT, following Ma et al.  and Zhang and Feng , we use BLEU  for the generation quality and average lagging (AL)  for latency, which measures the average offsets that outputs lag behind inputs (using token as the unit of AL). For SimulST, following Ma et al. , we use sacreBLEU  for the generation quality and average lagging (AL) for latency (using millisecond \(ms\) as the unit of AL). Refer to Appendix C for the detailed calculation of the latency metrics.

### Main Results

Results on Streaming ASRTable 1 reports the results on streaming ASR, where Seg2Seg exhibits a better trade-off between generation quality and latency. Compared with T-T using the transducer to align speech and text token-to-token , or MoChA generating text based on local speech , Seg2Seg adaptively learns source-target mapping through latent segments, thereby performing better.

Results on SimulMTConsistent with the previous methods [4; 24; 13], we adjust the value of \(\) (refer to Eq.(8)) to show the performance of Seg2Seg under varying latency. Figure 4 shows that Seg2Seg outperforms previous SimulMT methods at all latency. Compared to methods based on pre-defined rules, such as wait-k and MoE wait-k, Seg2Seg is more flexible in making generating/waiting decisions, achieving significant advantages. Other methods, such as MMA, GMA and HMT, align the target and source token-to-token. However, since the alignment between the two languages may not be one-to-one , some local reordering and multi-word structures can affect the performance . By mapping source to target at the segment level, Seg2Seg is more in line with the simultaneous generation process and mitigates these issues, ultimately achieving state-of-the-art performance.

Results on SimulSTFor the most challenging SimulST in Figure 5, Seg2Seg achieves state-of-the-art performance, especially at low latency. Most of the previous SimulST methods either segment the speech into fixed lengths [26; 64] or detect the number of words in the speech [3; 52; 20; 27] and then apply a wait-k policy, where both non-derivable word detection and wait-k policy hinder the adaptive learning of the model. Owing to the proposed expectation training, Seg2Seg is completely differentiable and able to jointly learn the mapping from the source to the latent segment and from the latent segment to the target, thereby finding the optimal moments for generating.

### Superiority of Unified Framework on Multi-task Learning

In the sequence-to-sequence framework, multi-task learning composed of ASR, MT and ST is shown to improve the performance on difficult tasks (e.g. speech translation) by sharing knowledge among different tasks [71; 72; 73]. However, in previous simultaneous generation methods, different tasks often involve different architectures and heuristics, leaving no room for multi-task learning. Owing to not involving any task-related heuristics, the proposed unified segment-to-segment framework provides a possibility to apply multi-task learning in simultaneous generation. In Seg2Seg, multi-task learning can include streaming ASR, SimulMT and SimulST, and these three tasks share all parameters, except that SimulMT has a text embedding and streaming ASR/SimulST have a shared speech embedding.

Figure 6 demonstrates the improvements brought by multi-task learning on the most challenging SimulST task. By employing multi-task learning in a unified framework, Seg2Seg can achieve further improvements. Specifically, jointly training with streaming ASR yields more discernible improvements, which is mainly because the monotonic properties between speech and text inherent in streaming ASR assist SimulST in learning the source-target mapping . Therefore, the unified Seg2Seg facilitates the sharing of knowledge among various simultaneous tasks through multi-task learning and is helpful for the difficult tasks, such as SimulST.

## 5 Analysis

We conducted extensive analyses to investigate the specific improvements of Seg2Seg. Unless otherwise specified, all the results are reported on SimulST with MuST-C En\(\)De test set, which is more difficult simultaneous generation task. Refer to Appendix B for more extended analyses.

### Improvements of Adaptive Learning

Seg2Seg learns the mappings from source to segment and from segment to target in an adaptive manner, without any task-specific assumptions. To verify the effect of adaptive learning, we respectively replace the source-to-segment and segment-to-target mappings with heuristic rules, such as fixed-length segment (i.e., fixed-seg)  and wait-k/wait-k-stride-n policy (i.e., fixed-emit) , and show the SimulST En\(\)De results in Figure 7.

The results show that adaptive learning significantly outperforms heuristic rules. Compared with dividing the source into fixed lengths of 200/280/360\(ms\), Seg2Seg can adaptively determine whether the received source token can be aggregated into a latent segment, bringing about 1 BLEU improvement. Compared with the rule-based wait-k policy, Seg2Seg judges whether to emit the target token based on the latent segment, thus finding more reasonable generating moments .

### Quality of Aggregation and Emission

Seg2Seg learns aggregation and emission adaptively, so we further explore the quality of aggregation and emission, respectively. We apply streaming ASR and SimulMT tasks for evaluation. The detailed calculation of the metrics for aggregation and emission quality are shown in Appendix B.1.

Aggregation QualityTo verify whether Seg2Seg can aggregate the source tokens into a latent segment at the appropriate moments in streaming ASR, following Zhang and Feng , we conduct experiments on the Buckeye dataset8, which is a speech segmentation benchmark with the annotated word boundaries. Table 2 shows the segmentation quality of Seg2Seg with some segmentation baselines, and the metrics include precision (P), recall (R) and R-value (comprehensive score) . Seg2Seg achieves better segmentation precision and higher

  
**Systems** & **P(\(\))** & **R(\(\))** & **R-value(\(\))** \\  ES K-Means  & 30.7 & 18.0 & 39.7 \\ BES GMM  & 31.7 & 13.8 & 37.9 \\ VQ-CPC  & 18.2 & 54.1 & -86.5 \\ VQ-VAE  & 16.4 & 56.8 & -126.5 \\ DSegKNN  & 30.9 & 32.0 & 40.7 \\  Fixed(280\(ms\)) & 28.1 & 16.3 & 38.4 \\ Seg2Seg & 41.1 & 18.1 & **41.2** \\   

Table 2: Segmentation accuracy of Seg2Seg.

Figure 6: SimulST results on MuST-C En\(\)De with multi-task learning.

Figure 7: Improvements brought by adaptive learning.

comprehensive score R-value, showing that Seg2Seg can perform aggregation and segment the speech at reasonable moments (i.e., token boundaries instead of breaking the continuous speech of a word ).

Emission QualityTo verify whether the model emits at reasonable moments, we follow Zhang and Feng  to evaluate the emission quality in SimulMT based on alignments. We apply RWTH9 De\(\)En alignment dataset, and calculated the proportion of the model emitting the target token after receiving its aligned source token, used as the emission accuracy. Figure 8 shows that Seg2Seg can receive more aligned source tokens before emitting under the same latency, meaning that Seg2Seg finds more favorable moments for generating and achieves high-quality generation.

### Effect of Latent Segments

To investigate whether the latent segments can act as a bridge between the source and target, we calculate the representations of the source tokens (\(_{x seg_{k}}x_{j}\)), target tokens (\(_{y seg_{k}}y_{i}\)), and latent segment \(_{k}\) within each segment during SimulST on En\(\)De. We then apply the T-SNE dimensionality reduction algorithm to project these representations into a 2D space. By doing so, we obtain a bivariate kernel density estimation of the representation distribution of source segment, target segment and latent segments, depicted in Figure 9. The visualization clearly demonstrates that the latent segment locates between the source and target sequences in the representation space, effectively serving as a bridge connecting the source and target.

Furthermore, we calculate the cosine similarity between the representations of the source, target and latent segments, as shown in Table 3. It is evident that the similarity between the source and target representations is low, posing a challenge for the model to directly map the source to the target. Conversely, the similarity between the latent segment and the source, as well as the latent segment and the target, is significantly higher. Hence, by introducing the latent segment as a pivot, the model can more easily learn the mapping from the source to the latent segment, and subsequently from the latent segment to the target, thereby finding the optimal moments for generating and achieving better performance.

## 6 Conclusion

In this paper, we propose a unified segment-to-segment framework for simultaneous sequence generation, which bridges the source and target sequences using latent segments as pivots. Unified Seg2Seg enables the handling of multiple simultaneous generation tasks and facilitates multi-task learning. Experiments and analyses show the superiority of Seg2Seg on performance and generalization.

## Limitations

The proposed Seg2Seg employs the encoder-decoder architecture as its backbone, and exhibits better generality across multiple simultaneous generation tasks. In addition to its primary application on generation tasks, the encoder (aggregation process) or decoder (emission process) of Seg2Seg can also be separately used for some real-time tasks based on encoder-only or decoder-only architecture, such as streaming tagging and online parsing. We leave this for further exploration in future work.

    & **Similarity** \\  source \(\) target & 0.53 \% \\ source \(\) segment & 20.01 \% \\ segment \(\) target & 14.66 \% \\   

Table 3: Representational similarity with the latent segment.

Figure 8: Emission accuracy of Seg2Seg in SimulMT.

Figure 9: Bivariate kernel density estimation visualization on the representations of source, target and latent segment.