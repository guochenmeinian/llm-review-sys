# Propagating Knowledge Updates to LMs

Through Distillation

 Shankar Padmanabhan, Yasumasa Onoe, Michael J.Q. Zhang, Greg Durrett, Eunsol Choi

Department of Computer Science

The University of Texas at Austin

shankarpadmanabhan@utexas.edu

###### Abstract

Modern language models have the capacity to store and use immense amounts of knowledge about real-world entities, but it remains unclear how to update such knowledge stored in model parameters. While prior methods for updating knowledge in LMs successfully inject atomic facts, updated LMs fail to make inferences based on injected facts. In this work, we demonstrate that a context distillation-based approach can both impart knowledge about entities _and_ propagate that knowledge to enable broader inferences. Our approach consists of two stages: transfer set generation and distillation on the transfer set. We first generate a transfer set by prompting a language model to generate continuations from the entity definition. Then, we update the model parameters so that the distribution of the LM (the'student') matches the distribution of the LM conditioned on the definition (the 'teacher') on the transfer set. Our experiments demonstrate that this approach is more effective at propagating knowledge updates than fine-tuning and other gradient-based knowledge-editing methods. Moreover, it does not compromise performance in other contexts, even when injecting the definitions of up to 150 entities at once.

## 1 Introduction

As large language models (LLMs) are used for a wider variety of applications, it is crucial to ensure that they contain up-to-date information about the world. One potential solution is retrieval augmentation, which prepends retrieved texts to the language model's context . However, this raises inference costs and becomes impractical when updating large amounts of information. An alternative approach, and our goal in this work, is to internalize the new knowledge into the language model via parameter updates .

Recent work on injecting LLMs with information about emerging entities  demonstrates that updating parameters effectively enables models to acquire updated facts (_Rishi Sunak is the prime minister of the UK_), but struggles to teach models how to _propagate_ this knowledge, or make inferences based on it _(what might Rishi Sunak do tomorrow?)_. This contrasts with results from retrieval augmentation  and chain-of-thought prompting , which show that LLMs can make such inferences when information is placed in the prompt.

This work aims to bridge the gap between the two approaches in knowledge injection. We use a form of knowledge distillation  called context distillation  that updates an LM to act like it is conditioned on a given context, even when that context is not shown. Our approach consists of two steps: transfer set generation and distillation on the generated transfer set. The transfer set consists of continuations of the entity definition sentence generated by prompting a language model. To distill on this transfer set, we minimize the Kullback-Leibler (KL) divergence between the model's predictionson the transfer set when it conditions on the definition (the "teacher" for distillation) and when it does not (the "student", or the language model itself). Figure 1 shows this approach.

We evaluate our approach on two knowledge propagation benchmarks: Entity Inferences and Entity Cloze by Date (ECBD) . We evaluate on three language models and find that our distillation approach outperforms fine-tuning and prior editing methods (MEND  and MEMIT ) across all models. To investigate the robustness of our approach, we present an ablation study focusing on the design choices during transfer set construction. Encouragingly, we find that distilling on transfer sets constructed from the base language model itself is competitive with those generated by a much larger model (GPT-3.5). This demonstrates that context distillation does not rely on distilling from a larger model, and that our approach can work across a range of model sizes. Finally, we show that our approach can be scaled to inject larger amounts of information at once: we can inject over 100 new entities into a language model with minimal performance degradation, suggesting that the distillation process performs relatively targeted editing even without additional objectives to ensure specificity as in past methods [22; 23].

To summarize, we present a new approach for propagating injected knowledge. We show that a knowledge distillation technique can effectively impart and propagate knowledge from entity definitions into the parameters of a pre-trained language model, compared to existing knowledge editing methods. Yet, we observe robust gap between providing information in-context and parameter updating methods, leaving ample room for future work. Our code and data are available at https://github.com/shankarp8/knowledge_distillation.

## 2 Background and Task Setup

### Motivating Example

Figure 1 shows a motivating example. An LM trained on text collected prior to November 2022 will not have specific knowledge about what ChatGPT is, as ChatGPT was introduced after that time. Past retrieval-augmented generation methods  have shown that conditioning on information about this entity can lead to lower perplexities when evaluating on sentences like _ChatGPT can respond to natural language questions_[35; 32]. For example, the model assigns a higher likelihood to tokens like _respond_ given the knowledge that ChatGPT is a chatbot.

Our approach relies on teaching a "student model" (the LM itself) to match the next-token distributions given by the model conditioned on the definition sentence _even when the definition sentence is not shown_. We do this via a distillation process on a set of _continuations_, or sampled sentences following

Figure 1: Overview of our distillation approach. Our goal is to inject the entity definition (\(_{e}\)) into the student model (\(M_{s}\)) and propagate it to make inferences based on the injected knowledge. This example uses _ChatGPT_ as a new entity. We first generate a set of continuations of the entity’s definition using a generator model (Step 1), then use these to distill the information from definition into the student model via a KL loss between the conditioned and unconditioned models (Step 2); see Section 3 for formulation.

the definition. We impose a KL penalty between the student and teacher distributions of a set of target tokens, namely all those occurring after _ChatGPT_ in the continuation. Because the distillation process does not make updates on tokens where the teacher and student have the same distribution (zero KL), only tokens that are in some way predictable from the definition drive parameter updates (see Section 7 for discussion).

### Task Setup

We refer to language models \(M\) as \(M()()\), mapping an input context \(=(x_{1},,x_{n})\) to a next-word distribution \(()=p( x_{1},,x_{n})\) over a vocabulary \(\). We will also use \(M()()_{1,,n}\) to represent the collection of distributions after each prefix of \(\), which is a standard operation used in language model training. To update knowledge in the base language model \(M_{}\), definitional information \(_{e}=(d_{1},,d_{m})\) for an entity \(e\) is provided. We will use \(e\) both as an indicator and also a reference to the entity name string (e.g., _ChatGPT_ in Figure 1). Our goal is to update \(M_{}\) to \(M_{s}\) so that it "knows" \(_{e}\), by matching \(M_{s}()\) with \(M_{t}(_{e})\) (the teacher model) as closely as possible with our distillation scheme, when \(\) is relevant to entity \(e\). We set the teacher model \(M_{t}\) to be a copy of \(M_{s}\).

We evaluate on two factors. First, **propagation success** measures how well the updated language model \(M_{s}\) acquired information about \(_{e}\) to make correct inferences in probe sentences. Crucially, our evaluation here is not just a narrow notion of whether a specific fact is injected [44; 8; 26; 22], inter alia], but captures the model's ability to make inferences on it [31; 32]. Second, **specificity** evaluates whether the predictions of the LM on other contexts are altered as in prior work [8; 26; 22; 23]. Ideally, edits should not impact inferences on examples unrelated to the edit.

### Related work

Knowledge distillationWe are not aware of prior work that uses distillation for knowledge editing. Our use of context distillation is most similar to Askell et al.'s alignment work ; however, they use it in a phase roughly analogous to RLHF and use a generic transfer set sampled from the language model training corpus. Our work is also related to prompt injection , which examines tasks like distilling a persona-conditioned language model. Unlike their work, we do not train a task-specific model to generate examples for distillation. Instead, we simply prompt existing LMs. Furthermore, while they aim to have a model memorize a particular prompt, we focus on general knowledge updates and inferences based on those. Other work has used distillation techniques for "gisting" to make shorter prompts  or to distill reasoning processes . Similar approaches as our continuation sampling have been used for example extrapolation  to generate training datasets for fine-tuning.

Efficient parametric knowledge updatesParameter updating methods such as KnowledgeEditor  and MEND  make use of standard fine-tuning to attempt to localize edits. Another line of work [7; 22; 23] attempts to locate where factual information is stored in transformers and designs edit methods based on these findings. In particular, ROME  and MEMIT  treat factual knowledge as subject-relation-object tuples, and find that new facts can be inserted into particular early and middle layer MLPs within a GPT-style transformer using specialized update rules. KILM  finds success with continual pretraining for encoder-decoder LMs using a modified pretraining objective, and  also examines continually pretraining LMs.

Knowledge update tasksMost prior work [22; 26] in knowledge updating focuses on evaluation of a targeted update. Because our goal is to test _propagation_ of knowledge, we mainly focus on two benchmarks from Onoe et al. . Besides this benchmark, recent work [43; 39; 6] also evaluates the LM's success at performing multi-hop inferences with the edited information. Compared to the benchmarks we evaluate on, which are taken from Wikipedia sentences, these benchmarks use sentences generated from knowledge base relations. Another line of work evaluates the ability of LMs to reason about emerging entities [18; 9; 17]. However, such benchmarks do not fit our task setting as they do not provide the information to inject.

## 3 Method

Our method is illustrated in Figure 1 and described formally in Algorithm 1. It consists of two steps: transfer set generation and distillation on the generated transfer set.

Transfer set generationFirst, we generate a transfer set corresponding to \(_{e}\), written as \(_{e}=\{_{1},_{2},,_{N}\}\). We do this by sampling \(N\) distinct continuations from our _generator_ model \(M_{g}\) with a prompt \(\) followed by the entity definition \(}\); we will either use GPT-3.5 or the base LM \(M_{}=M_{s}\) as the generator model \(M_{g}\).

Each continuation must contain an identifiable reference to the entity string \(e\). We describe how we ensure this in Section 5. We use \(_{i}\) to refer to the fencepost index where this entity string ends in the continuation sentence \(_{i}\); for example, in Figure 1, \(_{i}=2\) with 1-based indexing to indicate the mention string _ChatGPT_ ends before the second token. Crucially, we only want to distill losses when predicting tokens located at position \(_{i}\) or later. Tokens before do not condition on the entity name in the student and risk making broad updates to the model, which can impact specificity negatively.

DistillationWe initialize an LM \(M_{s}\) from its original pretrained checkpoint, as well as a copy of the LM, \(M_{t}\), to serve as the teacher model during the distillation process. Then, for each continuation \(_{i}\) in the transfer set, we compute the student model's distributions \(M_{s}(_{i})\) (a sequence of \(|_{i}|\) distributions) as well as the teacher model's distributions conditioned on the definition, \(M_{t}(_{i}_{e})\). We compute the KL divergence summed over the tokens after \(\) (line 8). Finally, we perform a gradient update on \(M_{s}\) based on this loss. This is done for \(K\) epochs.

Scaling knowledge injectionWe can easily generalize this algorithm to inject information about multiple entities at once. We take the union of transfer sets belonging to different entities, shuffle them, and distill on each transfer example as described in line 4-9. We evaluate this setting in Section 7.2.

## 4 Evaluating Knowledge Propagation

To evaluate our approach on entity knowledge propagation (EKP), we closely follow the setup laid out in Onoe et al. . Here, we describe two datasets and metrics for completeness. The details about the datasets (statistics, examples) can be found in Appendix A.

DataWe evaluate on two datasets. First, Entity Inferences  is a synthetic dataset designed such that the target spans in its probe sentences are easily inferable from the definition sentence. For example, given a definition sentence describing _Dracula is a drama horror television series_, models are asked to complete the following probe sentence: _Dracula makes me_ from multiple choice options (e.g., scared, atheletic, etc).

Second, Entity Cloze By Date (ECBD)  consists of cloze-style sentences from Wikipedia that probe for knowledge of specific entitites. Examples in ECBD are separated by each entity'sorigination date (e.g., when an event occured). In contrast to , which uses the 2021 subset of ECBD, we use the 2022 subset of ECBD to ensure that newer models (e.g. GPT-3.5) do not have knowledge of the probed entities beyond the definition they condition on; see Appendix A.3 for more discussion of the temporal cutoffs for our models and datasets. Each example consists of a cloze-style probe sentence prefix \(\) about an entity \(e\) followed by a target span \(\). The definition \(_{e}\) is taken from the first sentence of the entity's Wikipedia page.

Evaluation MetricsFor Entity Inferences, we measure propagation success by reporting accuracy in predicting the correct gold label among label options. We measure specificity by evaluating the model's accuracy at predicting gold spans on similar probe sentences across all other entities.

We evaluate on ECBD by computing per-token perplexity of the continuation given the probe prefix, \(PPL()\). This metric is not directly comparable across base LMs which have different tokenizers. To evaluate propagation success, we report the **decrease** in perplexity from the edit, \(PPL(;M_{})\) vs. \(PPL(;M_{s})\). To evaluate an edit's specificity, we randomly sample 40 examples from the "popular" subset of ECBD, ensuring that all 40 probes are about unique entities. We then report the change in perplexity on these sampled examples before and after the edit, using the same metric as above for evaluating on the target sentence.

## 5 Experimental Setting

Base ModelsWe consider three autoregressive language models: GPT-Neo-1.3B , GPT2-XL  (1.5B), and LLaMA-2-7B . The former two models have minimal knowledge of the entities in Entity Inferences and ECBD from their pretraining corpora as the entities in these datasets emerged after their pre-training.

Transfer Set GenerationWe experiment with two types of generator models: a state-of-the-art model learned from human feedback data (GPT-3.5, text-davinci-003), which can generate highly fluent transfer sentences from the definition sentence, and the base model itself, which presents a more realistic scenario in which we do not assume a better LM than the base LM that we are updating. For both models, we use a simple prompt to elicit a continuation of the definition sentence and sample five transfer sentences for each entity. For generation, we use nucleus sampling  with \(p=0.9\), a temperature of \(1.0\), and a max length of 40 tokens.

Table 1 summarizes the statistics of transfer sets. Upon manual inspection, we find that GPT-3.5 hallucinations substantially less than smaller models, as reflected in % of tokens in the continuations that appeared in the definition sentence. For continuations that do not contain the entity name, we simply prepend the entity name onto the continuation. We also report the number of tokens after \(l\), i.e., the number of tokens which we compute the distillation loss on. The exact prompt and example continuations can be found in Appendix C.

### Comparison Systems

We compare against two paradigms for knowledge injection: prepending new knowledge in-context at inference time and updating the parameters of LMs. For **prepending**, we report two settings: (1) prepending the correct entity definition and (2) prepending a definition of random entity, as reported in prior work . Next, we describe knowledge updating methods below.

**Finetuning** is frequently used to adapt pre-trained LMs to new domains or tasks  and is a baseline for knowledge injection. We train \(M_{}\) on \(_{e}\) with standard negative log likelihood loss on the sequence (teacher forcing). We investigate fine-tuning the full model, as well as only the last layer.

We also compare to **finetuning with the transfer set**. First, we fine-tune \(M_{s}\) on the definition. Then, for each sentence in our transfer set \(_{e}=(_{1},_{N})\), we fine-tune on \(M_{s}(_{i}_{e})\), conditioning

   \(M_{g}\) & \# Tokens & \% Token & \# Tokens \\  & & in \(E_{d}\) & after \(l\) \\  GPT-3.5 & 40.0 & 56.4 & 33.8 \\ GPT2-XL & 35.5 & 34.8 & 30.1 \\ GPT-Neo & 37.2 & 35.1 & 31.6 \\ LLaMA-2 & 32.5 & 37.4 & 26.4 \\   

Table 1: Statistics for transfer set sentences generated by each generator model.

on \(_{e}\) and only updating the model on the tokens after the entity occurrence \(\) in \(_{i}\) to make updates more comparable to our distillation setting. Here, we use the transfer set generated by GPT-3.5.

**MEND** is a hypernetwork that uses a set of smaller editing networks to make fast, local edits to a model's weights. MEND transforms the gradient obtained from traditional fine-tuning using a low-rank approximation. We train MEND editors for GPT-Neo using the WikiText-103 dataset, which utilizes generated text as altered output following the configuration used in the original paper.1

**MEMIT** treats facts as (subject, relation, object) tuples and considers each MLP within an LM as a key-value store . MEMT extends its predecessor **ROME** to be able to edit up to 10,000 facts at a time without sacrificing edit performance. Both methods use rank-one modifications to the MLP weights within a pre-chosen transformer layer (in the case of MEMIT, a set of consecutive pre-chosen layers) to edit the factual representations there.

We format the data for MEMIT as follows: For a given definition sentence \(_{e}\), the subject is the name of the entity \(e\), the relation is the part of the sentence before the masked span, and the object is the part of the sentence after the masked span, including the gold span. For details about how masked spans are defined, refer to Onoe et al. .

Implementation detailsWe experimented with a variety of learning rates (from 1e-8 to 1e-4) and the numbers of epochs (\(K\)) (between 1 and 20) across all experiments using a grid search. We focus on balancing results between performance and specificity; neither are prioritized if it significantly harms the other. The specific values used can be found in Appendix B.1.

## 6 Results

### Entity Inferences

We first conduct a smaller scale study on the easier benchmark, Entity Inferences, where learning about the definition should allow us to guess the target tokens by design. Table 2 reports the results. Our distillation approach shows promising performance in two base models we test. We find that transfer sets generated from GPT-3.5 show substantially better results than transfer sets generated from the base model itself in both datasets. This sometimes even outperforms definition prepending, which might be due to GPT3.5 introducing information about the entity beyond what can be inferred from the definition sentence. Fine-tuning on the definition and transfer set using GPT-Neo does outperform distillation, at the cost of specificity. For GPT2-XL, distillation only outperforms fine-tuning on the definition sentence when using GPT3.5 as a generator model, but still shows a substantial accuracy gain using its own generated sentences (24.3%). The drop in specificity (1.6-4.2%) is substantially less severe than fine-tuning on the definition sentence. These results indicate that context distillation teaches models to make simple inferences based on injected knowledge without significantly harming the model's distribution on unrelated concepts.

    &  &  \\  Pre-Edit Accuracy (\(\)) & 34.1 & 34.1 & 32.9 & 32.9 \\  & Target (\(\)) & Spec. (\(\)) & Target (\(\)) & Spec. (\(\)) \\   Finetuning on \(_{e}\) (full) & 57.7 (+23.6) & 18.3 (-15.9) & 62.9 (+30.0) & 24.1 (-8.8) \\ Finetuning on \(_{e}\) (last only) & 48.8 (+14.7) & 16.4 (-17.7) & 46.5 (+13.6) & **35.4 (+2.5)** \\
**Finetuning on \(_{e}+_{e}\) (full)** & **66.5 (+32.4)** & 28.8 (-5.3) & 59.4 (+26.5) & 33.8 (+0.9) \\ MEND & 41.8 (+7.7) & **34.4 (+0.3)** & - & - \\
**Distillation (\(M_{g}\) = \(M_{s}\))** & 61.8 (+27.7) & 32.6 (-1.6) & 58.2 (+25.3) & 31.4 (-1.5) \\
**Distillation (\(M_{g}\) = GPT3.5)** & 65.9 (+31.8) & 32.5 (-1.6) & **65.3 (+32.4)** & 28.7 (-4.2) \\  Prepend Def. & 60.0 (+25.9) & _34.1_ & 64.1 (+31.2) & _32.9_ \\ Prepend Random Def. & 27.7 (-6.4) & _34.1_ & 26.5 (-6.4) & _32.9_ \\   

Table 2: Results (accuracy) on Entity Inferences. Non-bolded lines are taken from prior work . Before the edit, accuracy was 34.1 for GPT-Neo and 32.9 for GPT2-XL.

### Ecbd

Table 3 displays our main experimental results on ECBD with three base models. Our context distillation method achieves high performance for all models. As established in , prepending the definition achieves the strongest performance, yet our approach recovers much of the this performance improvement. As in Entity Inferences, using a transfer set generated by GPT-3.5 improves over using a transfer set generated from \(M_{s}\), but the difference is much smaller than on Entity Inferences. These results suggest that our approach may benefit from, but does not require, access to a strong generator model. Fine-tuning the full model decreases the perplexity (2.5-4.0 perplexity drop) with smaller models but increases the perplexity on bigger models. We observe little change in performance with fine-tuning the last layer alone. We found that MEND increases the perplexity, and MEMIT for a single-edit decreases perplexity slightly.

As the dataset is moderately sized, we perform a paired bootstrap test to test for the significance of the improvements in average post-perplexity of distillation (using GPT-3.5 generated continuations) over finetuning all parameters on the definition, drawing \(N=10000\) samples . The gains of distillation over fine-tuning are significant with \(p<0.05\).

Comparing to domain adaptation: How much does the entity-specific knowledge matter?One possible explanation for our gains is that distillation teaches the model something about the particular _domain_ of probe sentences rather than knowledge about particular entities. We discuss two pieces of evidence for why this can only explain partial gains.

Existing editing methods we test do not significantly affect specificity, while our method leads to a slight decrease in specificity (improvement on unrelated sentences). This may indicate that our model is learning the domain of Wikipedia, but the small magnitude suggests that this alone does not explain the performance gain in target probe sentences.

Additionally, we compare our method to fine-tuning on the transfer set as well as the definition sentence; this can be viewed as a domain-adaptive setting . This generally _harms_ the model's perplexity on the evaluation setting relative to fine-tuning only on the definition sentence, unlike on Entity Inferences.

Ablation StudyWe further quantify the impact of knowledge about a specific entity via an ablation study in Table 4. We substitute either the entity definition or the transfer set with those belonging to a different randomly sampled entity. Similar to how prepending random definitions leads to a substantial increase in perplexity (bottom of Table 3, +11.9), distilling a definition of a randomly chosen entity, even when using the correct transfer set, leads to an increase in perplexity (+2.6). This result indicates that using the correct entity definition is crucial. It also shows potential benefits of parameter update methods compared to prepending to the context, as prepending irrelevant information brings a more substantial drop in in performance.

    &  &  &  \\  Pre-Edit PPL (\(\)) & 31.0 & 26.1 & 32.9 & 25.4 & 8.6 & 8.8 \\  & Target (\(\)) & Spec. (\(\)) & Target (\(\)) & Spec. (\(\)) & Target (\(\)) & Spec. (\(\)) \\   Finetuning on \(_{e}\) (full) & 28.5 (-2.5) & 26.0 (-0.1) & 30.0 (-2.9) & 25.4 (+0.0) & 9.0 (+0.4) & 8.7 (-0.1) \\ Finetuning on \(_{e}\) (last only) & 30.7 (-0.3) & 26.1 (+0.0) & 32.8 (-0.1) & 25.4 (+0.0) & 8.5 (-0.1) & 8.8 (+0.0) \\ Finetuning on \(_{e}+_{e}\) (full) & 28.9 (-2.1) & 26.1 (-0.0) & 30.6 (-2.3) & 25.5 (+0.1) & 8.9 (+0.3) & 8.8 (+0.0) \\ MEND & 35.2 (+4.2) & 26.4 (+0.3) & - & - & - & - \\ MEMIT & - & - & 32.6 (-0.2) & 25.4 (+0.0) & - & - \\
**Distillation** (\(M_{g}\) = \(M_{s}\)) & 26.0 (-5.0) & 25.9 (-0.2) & 27.6 (-5.3) & 25.2 (-0.2) & 8.0 (-0.6) & 8.6 (-0.2) \\
**Distillation** (\(M_{g}\) = GPT3.5) & **25.3 (-5.7)** & **25.6 (-0.5)** & **26.8 (-6.1)** & **25.1 (-0.3)** & **7.8 (-0.8)** & **8.6 (-0.2)** \\  Prepend Def. & 21.9 (-9.1) & _26.1_ & 24.0 (-8.9) & _25.4_ & 7.2 (-1.4) & _8.8_ \\ Prepend Random Def. & 42.9 (+11.9) & _26.1_ & 40.3 (+7.4) & _25.4_ & 8.6 (+0.0) & _8.8_ \\   

Table 3: Results (perplexity) on the ECBD 2022 dataset. Our distillation approach outperforms other approaches for GPT-Neo-1.3B, GPT2-XL, and LLaMA-2-7B on target perplexity without impacting specificity, achieving a substantial fraction of the gain from prepending the definition.

Next, we consider replacing the transfer set with a set of ten distinct elements from ten transfer sets of different entities (second row). We find that using the correct definition and a random transfer set _decreases_ perplexity, even outperforming fine-tuning. Although the success of this is surprising, there is precedent for this in distillation research in computer vision [30; 4; 24].

Furthermore, simply prepending the correct entity name (third row) in front of each element of the random transfer set decreases the perplexity substantially. This further shows that distillation is able to inject the definition even in the presence of a noisy transfer set. This also suggests distillation is mainly injecting information in the definition sentence, not the information in the transfer set.

## 7 Analysis

### Analyzing Distillation for ECBD

Does the distillation inject the definition itself?If distillation is teaching the model to make inferences based on the definition, how well does it teach the model about the definition itself? We measure the per-token normalized perplexity on the _definition_ sentence and report the results in Figure 2. Unsurprisingly, fine-tuning on definition sentence significantly drops its perplexity to closer to zero after 5-10 updates. While never trained to directly repeat the definition sentence, distillation also lowers the model's perplexity on the definition sentence significantly, potentially because of lexical overlap between the transfer set and the definition sentence (token overlap of 34.8-56.4% as shown in Table 1).

Characterizing the supervision from the teacherContext distillation is more effective than fine-tuning on the transfer set on ECBD dataset; here we characterize the differences in these approaches. Figure 3 shows the negative log likelihood (NLL) for GPT-Neo of the continuations on ECBD 2022 generated by GPT-3.5 without conditioning on the definition (x-axis) vs. the reduction in NLL when conditioning on the definition (y-axis). This is not the KL divergence and therefore not the actual training objective; however, by looking at how NLL values change, we can identify specific tokens whose probabilities are substantially modified, which would indicate a high KL value.

Tokens copied from the definition typically receive the highest decreases. Many tokens not in the definition are relatively unchanged in likelihood, and those in contexts that are not informed by the definition will have low KL divergence and drive small updates during learning. However, we show two examples of tokens not in the definition where conditioning _does_ reduce the NLL substantially. In the first case, _Dhaka_ is guessable given _Banglades_, and in the second, _features_ is semantically related to the definition. By contrast, _asset_ has similar NLL before and after conditioning.

Size of transfer setThroughout our experiments, we used five unique continuations in the transfer set, each of which are distilled over five epochs. Is having diverse continuations necessary for

   Definition & Transfer Set & Target (\(\)) & Specificity (\(\)) \\  Random & Correct & 33.6 (+2.6) & 25.8 (-0.3) \\ Correct & Random & 28.9 (-2.1) & 26.6 (+0.5) \\ Correct & Random + Ent. str & 26.7 (-4.3) & 25.7 (-0.4) \\ Correct & Correct & **25.3** (**5.7**) & **25.6** (**0.5**) \\   

Table 4: Distillation ablation study with GPT-Neo as the base model. We report perplexity and delta from the base model.

Figure 2: Results on GPT-Neo with varying numbers of model updates for fine-tuning and distillation approach. Left: target perplexity; right: perplexity on the definition sentence. Only distillation continues to improve in both target and definition perplexity as the number of updates increase.

successful distillation? We plot the distillation performance while varying the number of unique continuations in the transfer set from 1 to 10, while also keeping the number of updates the same, in Figure B.3 in the appendix and summarize the results here. Repeating one continuation ten times yields a target perplexity of 25.3, while using ten unique continuations once yields a target perplexity of 23.5. We see diminishing returns from introducing new continuations after 5 continuations, and most of the gains can be achieved with as few as two unique generated continuations. This is in line with prior work  which has shown distilling on more examples improves the target performance.

Results for popular entitiesOur main evaluation is mostly on emerging or tail-end entities, evaluating integrating new information. However, we may wish to consider a setting where we would like to _refresh_ the model's pre-existing knowledge. To evaluate this scenario, we use the popular split of ECBD . This has an identical format to ECBD 2022, but sentences cover popular, well-known entities (such as SpaceX and Eminem) that pre-date the training of the models we test.

We report results in Table 10 in the appendix. In this setting, our distillation approach vastly outperforms fine-tuning, suggesting that distillation can be effective in resurfacing LM knowledge.

### Scaling to multiple edits

Prior editing techniques  showed limitations in updating multiple facts at once. To evaluate how our approach scales, we perform distillation for multiple entities at once. We aggregate (entity definition, transfer sentence) for each entity and shuffle them for the entire entity set such that they are not ordered by entity during training. Figure 4 reports model performance under this setting, varying the number of entities to be updated from 10 to 150. We find that our method is largely capable of large scale updates, outperforming MEMIT, which shows increased perplexity when injecting more than 25 entities at once. For specificity, both MEMIT and distillation do not show degradation on GPT2-XL, but we observe degradation on GPT-Neo with our distillation method. Results on

Figure 4: Editing for multiple entities at once. We report the average from three runs with different random seeds for shuffling training data.

Figure 3: Per-token NLL of tokens in continuations before conditioning on definitions and after (fractional reduction). Tokens not in the definition (blue dots) are changed less but do see lower NLL when they are inferable from the definition (examples).

Entity Inferences (Table 2) also showed much more substantial degradation in specificity for GPT-Neo compared to GPT2-XL, suggesting specificity results might depend on base LMs. Overall, we observe promising results on editing multiple entities at once with distillation.

### Application to Counterfactual Knowledge Editing

Prior work  studied counterfactual knowledge editing, which injects false statements (such as "_The Eiffel Tower is located in Rome_") into the model. We evaluate our model in this setting, using a random sample of 150 entries from the CounterFact  dataset. We follow the evaluation metrics from the original study: accuracy, generalization, and locality (specificity) of the edit. For the specificity metric, we used the improved evaluation suggested in .2

Table 5 reports the experimental results. ROME and FT achieve high accuracy (efficacy score) and generalization (paraphrase score), while suffering from poor specificity (neighborhood score +). Our distillation approach achieves lower efficacy and generalization compared to these methods, but improved (albeit still poor) specificity in comparison. Additionally, we evaluate Constrained Fine-tuning (FT+L) , which imposes a \(L_{}\) norm constraint on weight changes in the parameter space. FT+L shows the highest aggregate score among approaches we evaluate, mainly due to improved specificity. However, it only yields mediocre generalization.

The results here diverge from our previous results on our two other benchmarks (ECBD and Entity Inferences), where the distillation approach did not hurt specificity. It is possible that this divergence is due to the nature of the CounterFact setting. Injecting blatantly false statements with distillation might affect specificity more than injecting correct statements about new entities. Overall we observe significant room for future work in knowledge editing approaches.

## 8 Conclusion and Limitations

We present a distillation-based method to impart entity knowledge within the parameters of a pretrained LM. Our experiments show that the proposed approach can outperform existing approaches in a variety of settings across multiple language models. Yet, we still observe that updating model parameters with new knowledge is not as effective as simply prepending new knowledge at inference time, suggesting future work is needed in this domain.

We conclude by describing the limitations of our work. Due to computational constraints, we use models that are <10B parameters. Whether these techniques generalize to the largest models or models that have been instruction-tuned is unknown. Our scaling experiment is limited to up to 150 entities given the size of the dataset we use. Further work is needed to assess whether thousands or millions of new entities can be injected in this fashion (e.g., to teach a complete set of new entities in a domain). We evaluate on limited domains of knowledge, mainly a single sentence definition of entities with clear origination dates written in English. Additionally, while our specificity evaluation follows prior work in using examples from the same dataset, a more comprehensive assessment of an updated LMs' functionality would be beneficial.

   Editor & Efficacy Score \(\) & Paraphrase Score \(\) & Neighborhood Score+ \(\) & Score \(\) \\  Base & 19.3 & 23.7 & 53.7 & 26.6 \\   FT & 100.0 & 92.0 & 10.5 & 25.8 \\ FT + L & 99.3 & 42.7 & 40.9 & 51.8 \\ ROME & 100.0 & 95.3 & 13.8 & 32.3 \\ Distillation & 79.3 & 68.0 & 22.8 & 42.2 \\   

Table 5: Results on the CounterFact benchmark for GPT2-XL. The last column reports a harmonic mean of other scores. ‘Base’ indicates the performance of the base model without any updates.