# Step 1: bound regret from optimism.

Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation

 Nikki Lijing Kuang

University of California, San Diego

likuang@ucsd.edu

&Ming Yin

Princeton University

my0049@princeton.edu

&Mengdi Wang

Princeton University

mengdiw@princeton.edu

&Yu-Xiang Wang

University of California, Santa Barbara

yuxiangw@cs.ucsb.edu

&Yi-An Ma

University of California, San Diego

yianma@ucsd.edu

Equal contribution.

###### Abstract

Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce _Delayed-PSVI_, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves \((H^{3}T}+d^{2}H^{2}[])\) worst-case regret in the presence of unknown stochastic delays. Here \([]\) is the expected delay. To further improve its computational efficiency and to expand its applicability in high-dimensional RL problems, we incorporate a gradient-based approximate sampling scheme via Langevin dynamics for _Delayed-LPSVI_, which maintains the same order-optimal regret guarantee with \((dHK)\) computational cost. Empirical evaluations are performed to demonstrate the statistical and computational efficacy of our algorithms.

## 1 Introduction

Reinforcement Learning (RL) is the main workhorse for sequential decision-making problems where an agent needs to balance the trade-off between exploitation and exploration in the unknown environment. The flexible and powerful function approximation endowed by deep neural networks greatly contributes to the empirical success of RL in domains such as Large Language Models (LLMs) [50; 59], robotics , and AI for Science . In general, collecting real-world training data from such practical systems can be expensive, which requires algorithms to be both sample efficient and computationally efficient. Recently, there have been growing efforts towards studying provably efficient RL algorithms in settings ranging from tabular Markov Decision Processes (MDPs) [29; 45; 69] to large-scale RL with function approximation [13; 35]. However, these algorithms typically rely on the availability of immediate observations of states, actions and rewards in learning no-regret policies. Unfortunately, such an assumption is rarely satisfied in real-world domains, where delayed feedback is ubiquitous and fundamental. In recommender systems and online advertisement, for instance, responses from users (e.g. click, purchase) may not be immediately observable, which can take hours or days. In healthcare and clinical trials, medical feedback from patients on the effectiveness of treatments can only be determined at a deferred time frame. More examples exist in platforms that involve human interaction and evaluation, including human-robot collaboration in teleoperating systems and multi-agent systems [15; 39], aligning LLMs with human values [50; 63], and fine-tuning generative AI models using RL with human feedback (RLHF) [11; 41].

Despite the practical importance of addressing delays in decision-making problems, theoretical understanding of delayed feedback in RL remains limited. Recent parallel works study exploration under delayed feedback via upper confidence bound (UCB) algorithms  in tabular RL [29; 45], adversarial MDPs [36; 40], and RL with low policy-switching scheme  (see Table 1). Nevertheless, posterior sampling (PS) analysis that handles delayed feedback remains untackled in both bandit and RL literature. We aim to bridge the gap in this work.

PS is a randomized Bayesian algorithm that extends Thompson sampling (TS)  to RL, which selects an action according to its posterior probability of being the best. This philosophy inspires a number of promising exploration strategies that explicitly or implicitly adopt PS to explore , including bootstrapped DQN [42; 47] and RLSVI . Compared to the popular UCB algorithms, it bears greater robustness in the presence of delays , and provides exceptional computational efficiency with competitive empirical performance [14; 65]. The fact that posteriors are often intractable in practice necessitates the use of approximate Bayesian inference such as ensemble sampling, variational inference (VI) and Markov Chain Monte Carlo (MCMC) [20; 38; 47].

In this paper, we provide the first analysis for the class of PS algorithms that handles delayed feedback in RL frameworks, in which the trajectory information is randomly delayed according to some unknown distribution. We highlight that delayed feedback model imposes new challenges that do not arise in standard RL settings. Algorithmically, it requires the computation of new posterior variance due to the weaker concentration arising from delays. Theoretically, it complicates the frequentist analysis of PS algorithms in several ways: (a) the lack of timely update in posterior learning can cause distribution shift, especially in the case of approximate sampling; (b) delays need to be carefully disentangled to quantify the penalty in regret decomposition and it prohibits the direct application of previous analysis; (c) balance between concentration and anti-concentration needs to be handled deliberately to achieve sub-linear regret.

To tackle these challenges, we introduce two novel value-based algorithms for _linear MDPs_ under unknown stochastic delayed feedback. Developed upon Bayesian linear modeling with a multi-round ensembling mechanism (\(M(H,K,d,)\) round), our algorithms achieve a sub-linear worst-case regret without requiring the knowledge of delay, thereby addressing the question raised in  that "No frequentist analysis exists for posterior sampling with delayed feedback". Empirical studies show that our algorithms outperform UCB-based methods in terms of both statistical accuracy and computational efficiency when delays are well-behaved or even long-tailed. We summarize our main contributions as follows.

* We propose the _Delayed Posterior Sampling Value Iteration_ (Delayed-PSVI, Algorithm 1) for linear MDPs. It achieves a high-probability worst-case regret of \((H^{3}T}+d^{2}H^{2}[])\)2, where \([]\) is the expected delay. * We leverage _Langevin Monte Carlo (LMC)_ for approximate inference and introduce _Delayed Langevin Posterior Sampling Value Iteration_ (Delayed-LPSVI, Algorithm 2), which maintains the same order-optimal worst-case regret of \((H^{3}T}+d^{2}H^{2}[])\). To the best of our knowledge, this is the first analysis that provably incorporates LMC in linear MDPs and jointly considers the impact of delays.
* Both algorithms achieve the optimal dependence on the parameters \(d\) and \(T\) in leading terms under the class of PS algorithms, and recover the best-available frequentist regret of \((H^{3}T})\)[31; 72] as in non-delayed linear MDPs when \([]=0\). In particular, Delayed-LPSVI reduces the computational complexity of Delayed-PSVI from \((d^{3}HK)\)to \((dHK)\), expanding the applicability in complex high-dimensional RL tasks while potentially providing a more flexible form of approximation.

### Related Work.

**Delayed feedback.** In bandit literature, delay is extensively studied in both stochastic [22; 56; 60; 77] and adversarial settings [32; 58; 78] for UCB-based methods. In comparison, while delay draws much attention in empirical RL studies [12; 17; 18], there is a lack of theoretical understanding until very recently. Parallel works focus on UCB-based methods in various RL settings [16; 28; 36; 40; 45; 68]. To provide the first analysis for PS algorithms in this context, we consider stochastic delays under linear function approximation without requiring any policy-switch scheme as in .

**Posterior sampling.** To encourage efficient exploration, PS is adopted in value-based methods to inject randomness in empirical Bellman update via Gaussian noise. From the Bayesian perspective, it is equivalent to maintaining an approximate Gaussian posterior for parameterized value function. Its sample complexity is studied in tabular settings [48; 49; 53], with the sharp worst-case regret of \((H^{2}S)\). Under linear function approximation, frequentist regret of \((H^{3}T})\)[31; 72] and Bayesian regret of \((dT})\) are established. However, in complex problem domains that require higher computational efficiency and more refined surrogates, approximate inference is the remedy. Toward this end, we resort to a gradient-based MCMC method.

**Langevin Monte Carlo.** LMC is a class of MCMC methods tailored for large-scale online learning with strong convergence guarantee by utilizing the first-order gradient information . It has been successfully applied to stochastic bandits , linear bandits  and tabular RL , In this work, we extend its usage in linear MDPs and demonstrate its convergent property under delay.

**RL with Function Approximation.** Function approximation is widely adopted to empower RL for large-scale applications. Fruitful results have been established for regret minimization in two types of MDPs under linear function approximation: linear mixture MDPs [9; 67], and linear MDPs [35; 66]. In linear mixture MDPs where transition kernel is parameterized as a linear combination of base models, provably efficient algorithms are discussed [13; 75; 76] and  provides the corresponding lower bound of \((dH)\). In contrast, linear MDPs enjoy a linear structure in value functions by assuming a low-rank representation for both transitions and reward function, where algorithms are shown to enjoy polynomial sample complexity [27; 35; 62; 73]. When it comes to general function approximation, theoretical guarantees are developed based on measures of eluder dimension [54; 61] and Bellman rank . In this work, we focus on delayed feedback in linear MDPs.

## 2 Preliminaries

We study the finite-horizon episodic MDP \((,,H,,r)\), which is time-inhomogeneous, and denote by \(\), \(\) the state and action spaces respectively, \(H\) the episode length, \(=\{_{h}\}_{h=1}^{H}\) the

 
**Algorithms** & **Setting** & **Exploration** & **Worst-case regret** & **Computation** \\ 
 & Linear Bandits & UCB & \((d+d^{3/2}[])\) & Confidence set optimization \\
 & Tabular MDPs & UCB & \((T}+S^{2}AH^{2}[])\) & Active update \\
 & Linear MDPs & UCB & \((H^{3}T}+dH^{2}[])\) & Multi-batch reduction \\
 & Adversarial MDPs & UCB & \((H^{2}S+H^{3/2}_{k=1}^{K}_{h})\) & Confidence set optimization \\  Delayed-PSVI (Thm 1) & Linear MDPs & PS & \((H^{3}T}+d^{4}H^{2}[])\) & \(O((d^{3}+M)dHK)\) \\ Delayed-LPSVI (Thm 2) & Linear MDPs & PS & \((H^{3}T}+d^{4}H^{2}[])\) & \(O((N+d)MHK)\) \\ Delayed-PSLB (Cor 2) & Linear Bandits & PS & \((T}+d^{2}[])\) & \(O((N+d)MK)\) \\  UCB Lower bound  & Linear MDPs & UCB & \((dH)\) & — \\ PS Lower bound  & Linear Bandits & PS & \((T})\) & — \\  

Table 1: Summary of regret bounds in linear bandits and episodic MDPs under stochastic delay. We denote by \(T\) the time horizon, \(K\) the number of episodes, \(H\) the episode length, \(d\) the dimension of feature space, \(M\) the number of sampling rounds, and \(N\) the total iterations in running LMC. Our choice of \(M\) and \(N\) has order of Polylog(\(H,K,d,\)), ensuring both Delayed-PSVI and Delayed-LPSVI are computationally efficient and statistically sample-efficient. We remark that the gap in the frequentist regret between PS and best UCB-based methods is unavoidable by a factor of \(\). Thus, our dependencies on \(d\) and \(T\) are optimal for the class of PS algorithms. Our results fulfill the caveat  that no worst-case analysis exists for PS with delay.

transition dynamics, and \(r=\{r_{h}\}_{h=1}^{H}\) reward function. At each step \(h[H]\), \(_{h}:_{}\) specifies the probabilities of transitioning from the current state-action pair into the next state, and \(r_{h}:\) emits a bounded reward. We adopt the prior protocol of linear MDPs as follows.

**Definition 1** (Linear MDPs ).: _Suppose there exists a known feature map \(:^{d}\) that encodes each state-action pair into a \(d\)-dimensional feature vector. An MDP is a linear MDP3 if for any time step \(h[H],\;\;(s,a)\), both the transition dynamics \(\) and reward function \(r\) are linear in \(\):_

\[_{h}(|s,a)=(s,a)^{}_{h}(),  r_{h}(s,a)=(s,a)^{}_{h},\] (1)

_where \(_{h}:^{d}\) contains \(d\) unknown probability measures over \(\), and \(_{h}^{d}\). Furthermore, we assume that \((s,a),\|(s,a)\| 1\), and \( h[H],\|_{h}\|\), \(\|_{}_{h}(s^{})\|\), where \(\|\|\) denotes the Euclidean norm._

A non-stationary policy \(=\{_{h}\}_{h=1}^{H}\) assigns the action to take at step \(h\) in state \(s_{h}\). Accordingly, we define the value functions of a policy \(\) as the expected rewards received under \(\):

\[Q_{h}^{}(s,a)=_{}[_{h^{}=h }^{H}r_{h^{}}|s_{h}=s,a_{h}=a], V_{h}^{}(s)=_{ }[_{h^{}=h}^{H}r_{h^{}}|s_{h}=s].\]

We further denote by \(^{*}\) the optimal policy whose value functions are defined as \(V_{h}^{*}(s):=V_{h}^{^{*}}(s)=_{_{h}}V_{h}^{}(s)\) and \(Q_{h}^{*}(s,a):=Q_{h}^{^{*}}(s,a)=_{}Q_{h}^{}(s,a)\). Under Definition 1, the action-value functions are always linear in the feature map, and there exists some \(w_{h}^{*}\) such that \(Q_{h}^{*}=^{}w_{h}^{*}\) (Lemma A.1). For ease of notation, \((s,a)\), denote \([_{h}V_{h+1}^{}](s,a)=_{s^{}_{h}( |s,a)}[V(s^{})]\). By Bellman equation and Bellman optimality equation,

\[Q_{h}^{}(s,a)=(r_{h}+_{h}V_{h+1}^{})(s,a), V_{h}^{}(s)=Q_{h}^{}(s,_{h}(s)),\] \[Q_{h}^{*}(s,a)=(r_{h}+_{h}V_{h+1}^{*})(s,a), V_{h}^{*}(s)=_{a}(r_{h}+_{h}V_{h+1}^{*})(s,a).\]

The goal of the agent is to maximize the cumulative episodic rewards or equivalently, minimize the regret that quantifies the difference between the value of the optimal policy \(^{*}\) and that of the executed policies. Formally, the _worse-case regret_ over \(K\) episodes is given as:

\[R(T)=_{k=1}^{K}V_{1}^{*}(s_{1}^{k})-V_{1}^{_{k}}(s_{1}^{k}).\] (2)

**Remark 1**.: _Different types of regret are used in literature to measure the performance of PS algorithms. Bayesian regret \(_{w^{*} p_{0}()}[R(T)|w^{*}]\) is often considered when assuming a prior \(p_{0}(w)\) over the true parameter \(w^{*}\). Frequentist regret \([R(T)]\) is considered when \(w^{*}\) is fixed, where the expectation is taken over all the randomness over data and algorithm. As explained in Appendix A.2, the worst-case regret that we study is stronger than the frequentist regret._

### Delayed Feedback Model

In this work, we consider stochastic delays across episodes. More specifically, the trajectory (i.e., sequence of states, actions and rewards) generated in each episode is not immediately observable in the presence of delay. The formal definition is given as follows.

**Definition 2** (Episodic Delayed Feedback).: _In each episode \(k[K]\), the execution of a fixed policy \(^{k}\) generates a trajectory \(\{s_{h}^{k},a_{h}^{k},r_{h}^{k},\)\(s_{h+1}^{k}\}_{h[H]}\). Such trajectory information is called the feedback of episode \(k\). Let \(_{k}\) represent the random delay between the rollout completion of episode \(k\) and the time point at which its feedback becomes observable._

**Remark 2**.: _Various types of delays have been independently studied in the literature, including delays in states , delays in rewards , delays in actions, and delays in trajectories . We focus on the last scheme which facilitates the delayed analysis of value-based methods in episodic linear MDPs._Episodic delays do not disrupt the policy rollout within an episode, but alter the utilization of information in subsequent episodes. More precisely, the feedback of episode \(k\) remains inaccessible for the following \(_{k}-1\) episodes, becoming observable only at the onset of the (\(k+_{k}\))-th episode. To track whether the feedback generated at episode \(k\) is revealed at episode \(k^{}\), we utilize the indicator \(_{k,k^{}}:=\{k+_{k} k^{}\}\) (where \(1\) denotes "yes" and \(0\) denotes "no"). We follow the standard assumption in literature in [28; 68] to assume delays are sub-exponential. It is crucial to note that this assumption primarily serves the purpose of theoretical analysis and is not a prerequisite for the effective functioning of our algorithms in practical settings. Without loss of generality, we discuss the performance bound under general random delays in Section 4 and empirically study the performance against different types of delays in Section 5.

**Assumption 1** (Sub-exponential Episodic Delay).: _The episodic delays \(\{_{k}\}_{k=1}^{K}\) are non-negative, integer-valued, independent and identically distributed \((v,b)\)-subexponential random variables: \(_{k}f_{}()\) with \(f_{}()\) being the probability mass function, and \([]\) being the expected value. For all \(k[K]\), the moment generating function of \(_{k}\) satisfies:_

\[[((_{k}-[] ))](v^{2}^{2}),\]

_where \(v\) and \(b\) are non-negative, and \(|| 1/b\)._

## 3 Delayed Posterior Sampling Value Iteration

In this section, we introduce a novel optimistic value-based algorithm, namely, _Delayed Posterior Sampling Value Iteration_ (Delayed-PSVI), which efficiently explores the value function space in linear MDPs by embracing several critical components: posterior sampling that injects random noise when performing the least-square value iteration, optimism via multi-round sampling to achieve the optimal worst-case regret and delayed feedback model that encodes episodic trajectory delays.

``` Input: priors \(p_{0}(w_{h}^{k})(0, I)\), scaling factor \(\), multi-round paramter \(M\), hyper parameters \(\) and \(^{2}\).
1Initialization:\( k,h,_{H+1}^{k}(,),_{H+1}(, ),_{h}(,) 0\), \(_{h}\).
2forepisode \(k=1,,K\)do
3 Sample initial state \(s_{1}^{k}\) for time step \(h=H,,1\)do
4\(y_{h}[y_{1}^{1},,y_{h}^{k-1}]\), with \(y_{h}^{}_{,k-1}[r_{h}^{}+_{h+ 1}(s_{h+1}^{})]\)
5\(_{h}[_{1}^{},^{2},,^{k-1}]\) with \(^{}=_{,k-1}(s_{h}^{},a_{h}^{})\)
6\(_{k}^{k}^{-2}_{h}_{h}^{T}+ I\), \(_{h}^{k}^{-2}(_{h}^{k})^{-1}_{h}y_{h}\)\(}^{T}\)
7\(p(w_{h}^{k}_{h},_{h})(_{h}^{k}, ^{2}(_{h}^{k})^{-1})\)
8for m = 1,...,Mdo
9 Sample \(_{h}^{k,m} p(w_{h}^{k}_{h},_{h})\)
10\(_{h}^{k,m}(,)(,)^{} _{h}^{k,m}\)
11 Update \(_{h}^{k}(,)_{m}_{h}^{k,m}\)
12\(_{h}(,)_{a}\{_{h}^{k}( ,a),H-h+1\}\)
13 Update \(_{h}^{k}()*{argmax}_{a}\{ _{h}^{k}(,a),H-h+1\}\)
14for time step \(h=1,,H\)do
15 Choose action \(a_{h}^{k}=_{h}^{k}(s_{h}^{k})\)
16 Collect trajectory observations \(_{h}_{h}\{(s_{h}^{k},a_{h}^{k},r_{h}^{k},s_ {h+1}^{k})\}\)
17 /* Feedback generated in episode \(k\) cannot be immediately observed in the presence of delay */ ```

**Algorithm 1**Delayed Posterior Sampling Value Iteration (Delayed-PSVI)

**Noisy value iteration via posterior sampling.** At the beginning of each episode, we apply PS to sample an estimated value function from the posterior, which is maintained using the observed feedback \(\) over the previous episodes. Specifically, at each time step, the \(Q\)-function is parameterized by some \(w\) such that \((s,a)=(s,a)^{}w\) is an approximation of the corresponding true optimal \(Q\)-function \(Q^{*}(s,a)\). Let \(p_{0}(w)\) be the prior of \(w\), and \(p(|w,)\) be the likelihood of the observation \(\), then the posterior of \(w\) satisfies:

\[p(w|,)(-L(w,,))p_{0}(w),\]where \(L()\) is the log-likelihood. Unlike the case of model-based RL (MBRL), where PS is utilized to maintain an exact posterior over the environment model, we aim to adopt PS to perform noisy value-iteration by injecting randomness for efficient exploration of the value function space. Specifically, at each step \(h[H]\), we consider Gaussian-noise perturbation in Delayed-PSVI by setting prior as \(p_{0}(w_{h})=(0, I_{d})\), and log-likelihood (with \(_{h}=\{s_{h}^{},a_{h}^{},r_{h}^{},s_{h+1}^{}\}_{h [H]}^{[k-1]}\)) as

\[L(w_{h},_{h},_{h})=_{=1}^{k-1}((s_{h}^{ },a_{h}^{})^{}w_{h}-y_{h}^{})^{2},\] (3)

where \(_{h}=[y_{h}^{1},,y_{h}^{k-1}]\) with \(y_{h}^{}=r_{h}^{}(s_{h}^{},a_{h}^{})+_{h+1}(s_{h +1}^{})\). Then for all step \(h[H]\) of episode \(k\), the posterior of \(w_{h}^{k}\) follows a Gaussian distribution,

\[p(w_{h}^{k}|_{h},_{h})(_{h}^ {k})^{-1}_{h}_{h}^{},(_{h}^{k})^{-1},\]

where \(_{h}^{k}:=_{h}_{h}^{}+ I_{d}\) and \(_{h}=[(s_{h}^{1},a_{h}^{1}),(s_{h}^{2},a_{h}^{2}),,(s_{h }^{k-1},a_{h}^{k-1})]\). Adding the scaling factors \(^{2}\) and \(^{2}\) yields the Line 10 of Algorithm 1. It is important to note that while the induced likelihood \((-L(w_{h}^{k},_{h}^{k},_{h}^{k}))\) from (3) is Gaussian, we do not assume \(y_{h}^{}=r_{h}^{}(s_{h}^{},a_{h}^{})\)\(+_{h+1}(s_{h+1}^{})\) follows a Gaussian distribution. Instead, the above likelihood model can be used for non-Gaussian problems as we need not sample from the exact Bayesian posterior model .

On the other hand, the \(_{h}^{k}\) computed in Line 9 of Algorithm 1 together with the greedy choice \(()_{a}(,a)\) (Line 15) approximates the solution of Bellman optimality equation via the least-square ridge regression: \(_{h}^{k}=*{argmin}_{w}_{=1}^{k-1}((s_{h}^ {},a_{h}^{})^{}w-(r+_{h}^{k}))^{2}+ I _{d}\).4 Consequently, Line 5-10 essentially performs the Posterior Sampling Value Iteration.

**Optimism via multi-round sampling scheme.** Unlike the Bayesian regret or the worst-case expected regret, the high-probability worst-case regret in (2) needs to control the sub-optimal gap with arbitrarily high probability of at least \(1-\). However, sampling once at each time step only provides a constant-probability optimistic estimation, which breaks the high probability requirement. In addition, the estimation error incurred by sampling (i.e. constant-probability pessimistic estimation) at each timestep will propagate to the previous time steps during the backward posterior sampling value iteration. This phenomenon does not appear in the \(1\)-horizon bandit problem due to a saturated-arm analysis . To remedy this issue, we design a multi-round sampling scheme that generates \(M\) estimates \(\{^{m}\}_{m[M]}\) for \(Q\)-fuction through \(M\) i.i.d. sampling procedures, and constructs an optimistic estimate by setting \(=_{m}^{m}\). Notably, our choice of \(M\) has order \((H,K,d,)\), and thus makes our algorithm sample-efficient without increasing the overall complexity dependence. As shown in Line 11-14 of Algorithm 1, this scheme guarantees the optimistic estimates \( Q^{*}\) can be achieved as desired. Lastly, ensemble sampling methods enjoy empirical success and popularity in RL , including double q-learning  and bootstraped DQN . We are among the first few works to explain its theoretical effectiveness.

**Episodic delayed feedback model.** Recall that by Definition 2, when delay \(_{k}\) takes place, the feedback \(\{s_{h}^{t},a_{h}^{t},r_{h}^{t},s_{h+1}^{t}\}_{h[H]}\) of episode \(k\) cannot be observed until the beginning of the \(k+_{k}\)-th episode. Accordingly, the delayed version of the fully observed \(y^{},_{h}^{k}\) now becomes,

\[y_{h}^{}_{,k-1}[r_{h}^{}(s_{h}^{},a_{ h}^{})+_{h+1}(s_{h+1}^{})],\ _{h}[_{1,k-1}(s_{h}^{1},a_{h}^{1}),, _{k-1,k-1}(s_{h}^{k-1},a_{h}^{k-1})].\]

As a result, episodic delays are considered during the posterior updates in subsequent episodes. This completes the design of Delayed-PSVI as presented in Algorithm 1. In the remainder of this section, we present the main theoretical guarantees of Delayed-PSVI and the proof sketch of Theorem 1.

**Theorem 1**.: _Suppose delays satisfy Assumption 1. In any episodic linear MDP with time horizon \(T=KH\), where \(K\) is the total number of episodes, for any \(0<<1\), let \(=1\), \(^{2}=1\), \(M=(4HK/)/(64/63)\) and \(=C_{/4}(})\) (\(C_{/4}\) in Lemma B.10). Then with probability at least \(1-\), there exists some absolute constants \(c,c^{},c^{}>0\) such that the regret of Delayed-PSVI (Algorithm 1) satisfies:_

\[R(T) cH^{3}T}+c^{}d^{2}H^{2}[]+c^ {}.\]

_Here \(\) is a Polylog term of \(H,d,K,\)._

**On the complexity bound.** Theorem 1 provides the first analysis for PS algorithms under delay and answers the conjecture from . Our result recovers the best-available frequentist regret of \((H^{3}T})\) for PS algorithms when there is no delay (\([]=0\)). According to , the worst-case regret of linear Thompson sampling is lower bounded by \((T})\), and this implies our regret dependencies on parameter \(d\) and \(T\) are optimal under the class of PS algorithms.5 The order \(}\) in our regret is \(\)-suboptimal to the optimal dependence in . As an initial study for posterior sampling with delayed feedback, improving the horizon dependence is beyond our pursuit and we leave it for future work. Moreover, the presence of delay incurs an additive regret term \((d^{2}H^{2}[])\). As \(T\) grows, the impact of delay will not dominate the overall regret. Furthermore, our high-probability regret bound directly implies the following worst-case expected regret.

**Corollary 1**.: _Under the setting of Theorem 1, the expected regret of Delayed-PSVI is bounded by_

\[[R(T)] O(H^{3}T})+O(d^{2}H^{2}[] )+O()\]

_Here \(\) is a Polylog of \(H,d,K\). The expectation is taken over the randomness in data and algorithm._

Proof of Corollary 1 is included in Appendix A.2. Additionally, we present the following corollary in linear bandits, whose main regret \(T}\) is optimal for PS algorithms.

**Corollary 2** (Delayed Posterior Sampling for Linear Bandits).: _For the linear bandit with \(y_{t}=x_{t}^{}_{*}+_{t}\), where \(x_{t} D_{t}^{d}\) and \(_{t}\) be a mean-zero noise with \(B\)-subgaussian. Let \(T\) be the total number of steps. Under Assumption 1, for any \(0<<1\), with probability at least \(1-\), the regret of Delayed-PSLB satisfies:_

\[R(T) O(T})+O(d^{2}[])+O().\]

_Here \(\) is a Polylog term of \(d,K,\)._

### Sketch of the analysis

Due to the space limit, we outline the key steps in our analysis and defer the complete proof of Theorem 1 in Appendix B. To bound the worst-case regret in (2), first note that

\[R(T)=_{k=1}^{K}^{*}(s_{1}^{k})-_{1}^{k}(s_ {1}^{k})}_{_{opt}^{}}+_{1}^{k}(s_{1}^{k})- V_{1}^{_{k}}(s_{1}^{k})}_{_{est}^{k}}.\]

Our goal is to attain an optimistic estimation so that \(_{opt}^{k} 0\) while controlling the estimation error \(_{est}^{k}\). For optimistic PS algorithms, Gaussian anti-concentration is the main tool  to achieve optimism with constant probability. However, the probability of optimism will diminish as the algorithm back-propagates with respect to time. In contrast, we maintain \(m[M]\) independent ensembles \(Q^{m}\) so that roughly speaking, \((Q^{m} Q^{*})\) for all valid \(m\). For any \(0<<1\), with the choice \(M=(1/m)/(64/63)\), the optimistic estimator \(Q=_{m}Q^{m}\) satisfies \((Q Q^{*}) 1-\) (Lemma B.6). We can then proceed to prove \(_{opt}^{k} 0\).

To control \(_{est}^{k}\), one key challenge is to bound the error term \(_{k=1}^{K}\|(s^{k},a^{k})\|_{(^{k} )^{-1}}\). Due to the presence of delays, we cannot directly apply the Elliptical Potential Lemma as in the non-delayed settings. Therefore, we decompose \((^{k})^{-1}\) into \((^{k})^{-1}+M_{k}\), where \(^{k}:=_{=1}^{k-1}(s_{h}^{},a_{h}^{}) (s_{h}^{},a_{h}^{})^{}+ I\) is the full information matrix, and show

\[_{k=1}^{K}\|(s^{k},a^{k})\|_{M_{k}}_{k[K]} _{k}_{k=1}^{K}\|(s^{k},a^{k})\|_{(^{k})^{-1}}^{2}.\]

By doing so, \(_{k=1}^{K}\|(s^{k},a^{k})\|_{(^{k})^{-1}}^{2}\) can be upper bounded by \((d(K))\) via the Elliptical Potential Lemma and \(_{k[K]}_{k}\) can be upper bounded by \(([])\) via the sub-exponential tail bound. Combing all these steps completes the proof.

## 4 Delayed Posterior Sampling via Langevin Dynamics

Delayed-PSVI performs noisy value iteration for linear MDPs by injecting randomness for exploration via Gaussian noise. From the Bayesian perspective, it constructs a Laplace approximation to obtain a Gaussian posterior given the observed data. However, sampling from a Gaussian distribution with a general covariance matrix \(_{h}^{k}\) can be computationally expensive in high-dimensional RL tasks. Specifically, Line 10 of Algorithm 1 is conducted via \(:=+^{-1/2}\), where \((0,I_{d})\). The complexity of computing the matrix inverse involved (_e.g._ via Cholesky decomposition) is at least \(O(d^{3})\), which is prohibitively high for large \(d\). More importantly, in complex problem domains, a flexible form of non-Gaussian noise perturbation may be desirable.

To tackle these challenges, we incorporate a gradient-based approximate sampling scheme via Langevin dynamics for PS algorithms, namely, LMC, and introduce the _Delayed-Langevin Posterior Sampling Value Iteration_ (Delayed-LPSVI) in Algorithm 2. The update rule of LMC essentially performs the following noisy gradient update:

\[w_{t} w_{t-1}-(w_{t-1})+ _{t},\]

where \(_{t}}}{{}}(0,I_{d})\). It is based on the Euler-Murayama discretization of the Langevin stochastic differential equation (SDE):

\[(t)=- L((t))t+}\;(t),\] (4)

where \((t)^{d}\) is a Brownian motion, \(>0\) and \(t>0\). Under certain regularity conditions on the drift term \( L((t))\) in (4), it can be shown that the Langevin dynamics converges to a unique stationary distribution \((d)))}\). As a result, LMC is capable of generating samples from arbitrarily complex distributions which can be intractable without closed form. With sufficient number of iterations, the posterior of \(w_{t}\) is in proportional to \((-(w))\).

In our problem, we specify \(\) to be the following delayed loss function

\[L_{h}^{k}(w):=_{=1}^{k-1}_{,k-1}((s_{h} ^{},a_{h}^{}),w-_{h}^{})^{2}+\|w\|_{2 }^{2},\] (5)

where \(_{h}^{}:=r_{h}^{}+_{h+1}^{k}(s_{h+1}^{})\). Compared to Delayed-PSVI, Algorithm 2 does not require the matrix inversion computation. Below we present the worst-case regret of Delayed-LPSVI and discuss the key insights in our analysis. The full proof is deferred to Appendix C.

``` Input:\(w_{0}\), \(_{k}\), \(N_{k}\), \(\) and rounds \(M,\). Delayed loss \(L_{h}^{k}\) as (5). Initialization:\( k[K],h[H],_{H+1}^{k}(,) 0, _{H+1}^{k}(,) 0,_{h}^{0}(,) 0\) forepisode \(k=1,,K\)do Sample initial state \(s_{1}^{k}\) fortime step \(h=H_{1},,H\)do \(_{h}^{k,m} LMC(L_{h}^{k},w_{0},_{k},N_{k},)\) //\(LMC\) is given by Algorithm 3 \(_{h}^{k,m}(,)()^{T}_{h}^ {k,m}\) Update \(_{h}^{k}(,)_{}_{h}^{k,m}\) \(_{h}^{k}(,)_{}\{_ {h}^{k}(,a),H-h+1\}\) Update policy \(_{h}^{k}()*{argmax}_{a}\{ _{h}^{k}(,a),H-h+1\}\) fortime step \(h=1,,H\)do Choose action \(a_{h}^{k}=_{h}^{k}(s_{h}^{k})\) Collect trajectory observations \(_{h}_{h}\{(s_{h}^{k},a_{h}^{k},r_{h}^{k},s_ {h+1}^{k})\}\) /* Feedback generated in episode \(k\) cannot be immediately observed in the presence of delay */ ```

**Algorithm 2**Delayed Langevin Posterior Sampling Value Iteration (Delayed-LPSVI)

**Theorem 2**.: _Suppose delays satisfy Assumption 1. In any episodic linear MDP with time horizon \(T=KH\), where \(K\) is the total number of episodes and \(H\) is the fixed episode length, for any \(0<<1\), let \(=1\), \(N_{k}=\{((K+)dk}{}+1)/[2(1/(1- }))],\)\(}))},(}{})/(1/(1-}))\}\), \(_{k}=(_{h}^{k})}\), \(=16C_{/4}^{2}(dMH^{2})\)\(w_{0}=\) and \(M=(4HK/)/(64/63)\). Then with probability at least \(1-\), there exists some absolute constants \(c,c^{},c^{}>0\) such that the regret of Algorithm 2 satisfies:_

\[R(T) cH^{3}Tt}+c^{}d^{2}H^{2}[]+c^{ }.\]

_Here \(\) is a Polylog term of \(H,d,K,\) and \(C_{}\) is defined in Lemma C.9._

Neglecting the constants and Polylog factors, Delayed-LPSVI maintains the same order regret of \((H^{3}T}+d^{2}H^{2}[])\) as Delayed-PSVI while significantly improving the computational efficiency. Precisely, LMC requires \(O(N)\) complexity to perform gradient steps in Line 6 of Algorithm 2 and an extra \(O(d)\) operations to compute \(_{h}^{k,m}\) in Line 7. Thus, the total computation complexity of LMC is \(O((N+d)MHK)\). On the other hand, sampling without LMC (Line5-8 in Algorithm 1) requires \(O(d^{3})\) operations, and the multi-round sampling (Line9-11) incurs \(O(dM)\) additional operations, which implies for a total computation complexity of \(O((d^{3}+dM)HK)\). As the choice of \(N\) in Algorithm 2 has logarithmic order, and \(M=(4HK/)/(64/63)\), the overall complexity of Delayed-LPSVI is \((dHK)\), whereas the overall computational complexity of Delayed-PSVI is \((d^{3}HK)\). Notably, Delayed-LPSVI reduces the computational overhead of Delayed-PSVI by \((d^{2})\).

**On the analysis.** The key step in the proof of Theorem 2 is to show the convergence guarantee of LMC. Indeed, by recursion, one can show

\[w_{N}=A_{h,k}^{N}w_{0}+(I-A_{h,k}^{N})_{h}^{k}+_{l=0}^{N-1}A_{h,k}^{l}_{N-l},\]

where \(A_{h,k}:=I-2_{k}_{h}^{k}\). For any \(w_{0}\), it implies \(w_{N}\) follows the Gaussian distribution \((A_{h,k}^{N_{k}}w_{0}+(I-A_{h,k}^{N_{k}})_{h}^{k},_{h}^{k})\). With the choice of \(_{k}=(_{h}^{k})}\), \(A_{h,k} I_{d}\) and \((1-(1-})^{2N_{k}})(_{h}^{k} )^{-1}_{h}^{k}(_{h}^{k})^{-1}\), which is the key to connect \(_{h}^{k}\) with \((_{h}^{k})^{-1}\) (Lemma C.2), the main analysis for Delayed-PSVI goes through by utilizing this connection.

**On arbitrary delayed feedback.** The current study considers the stochastic delays that are sub-exponential Assumption 1. What if delay has an arbitrary distribution (_e.g._ Cauchy distribution has unbounded mean)? Indeed, the regret can be (roughly) bounded by \((H^{3}T}+dH^{2}d_{}(q))\) for \(d_{}(q)\) to be the \(q\)-th quantile of delay \(\). We do not focus on this setting since there is a \(1/q\) blow-up in the main regret that many distributions (_e.g._ sub-exponential) do not need to sacrifice. We include the discussion in Appendix A.3.

## 5 Experiments

To validate whether our posterior sampling algorithms are competitive or outperform the non-sampling-based algorithms in the delayed setting, in this section, we examine their empirical performance in two simulated RL environments with different delayed feedback distributions. In particular, we consider a linear MDP environment following , and a variant of the popular River-Swim . In both environments, we benchmark Delayed-PSVI (Algorithm 1), Delayed-LPSVI (Algorithm 2) against LSVI-UCB  with delayed feedback, namely, Delayed-UCBVI. In this section, we discuss results in the first setting and defer the discussion of RiverSwim in Appendix E.

### Synthetic Linear MDP

We construct a synthetic linear MDP instance with \(||=2\), \(||=50\), \(d=10\), and \(H=20\). The linear feature mapping embeds each state-action pair with its binary representation and induces the following reward function: \(r(s,a)=0.99\) if \(s=0,a=0\); \(r(s,a)=0.01\) otherwise. The design of the environment results in the same optimal value \(V_{1}^{*}(s_{1})\) when \(d\) and \(H\) are fixed. Algorithms are examined under three types of delays that are commonly encountered in real-world phenomena, including sub-exponential delays and long-tail delays:* **Multinomial delay.** Delays follow a Multinomial distribution with three categories \(\{10,20,30\}\), with the corresponding probabilities as \(\{0.5,0.3,0.2\}\).
* **Poisson delay.** Delays follow a Poisson distribution with the expected delay as \([]=50\).
* **Long-tail delay.** Delays are discretized from a Pareto distribution 6 with the shape parameter as \(1.0\) and the scale parameter as \(500\).

To run Delayed-LPSVI, we warm start LMC by initializing \(w_{0}\) at each time step with the previous sample, and let \(M=2\), \(N=40\), \(=c_{}/_{}(_{h}^{k})\). For Delayed-PSVI, we set parameters \(M=2\), \(=H\). In the case of Delayed-UCBVI, we set the bonus coefficient as \(=c_{}/2 dH\ \). To make a fair comparison, we perform a grid search to determine the optimal hyperparameter values and fix \(c_{}=0.1\), \(c_{}=0.5\), \(=0.02\). Experiments are repeated with 10 different random seeds, and the returns are averaged over episodes in Figure 1. Further elaboration on additional metrics is available in Appendix E.2.

**Results and Discussions.** Both Delayed-PSVI and Delayed-LPSVI exhibit consistent and robust performance with resilience, not only under the well-behaved delays that decay exponentially fast, as assumed in Assumption 1, but also under the heavy-tailed delays, such as those following Pareto distributions. Notably, when confronted with the challenge of long-tail delays, our algorithms excel Delayed-UCBVI in terms of statistical accuracy (yielding higher return) and convergence rate. Specifically, the performance of Delayed-UCBVI degrades under long-tail delays, resulting from its computational inefficiency in iteratively constructing confidence intervals. In contrast, PS methods offer a higher degree of flexibility to adjust the range of exploration, owing to the inherent randomized algorithmic nature. To assess the computational advantages facilitated by LMC, we consider additional synthetic environments with varied dimensions for a more comprehensive analysis. For detailed statistics and further discussions, please refer to Appendix E.2. It is noteworthy that in practical high-dimensional RL tasks, the computational savings achieved by Delayed-LPSVI, in comparison to Delayed-PSVI, are considerably more significant.

## 6 Conclusion

In this paper, we study posterior sampling with episodic delayed feedback in linear MDPs. We introduce two novel value-based algorithms: Delayed-PSVI and Delayed-LPSVI. Both algorithms are proved to achieve \((H^{3}T}+d^{2}H^{2}[])\) worst-case regret. Notably, by incorporating LMC for approximate sampling, Delayed-LPSVI reduces the computational cost by \((d^{2})\) while maintaining the same order of regret. Our empirical experiments further validate the effectiveness of our algorithms by demonstrating their superiority over the UCB-based methods.

This work provides the first delayed-feedback analysis for posterior sampling algorithms in RL, paving the way to several promising avenues for future research. Firstly, it is interesting to extend the current results to settings with general function approximation [34; 71]. Additionally, leveraging the sharp analysis outlined in  to improve the suboptimal dependence on \(H\) for posterior sampling algorithms presents an intriguing avenue for exploration. Furthermore, addressing other types of delay (e.g. adversarial delay) that differ from stochastic one will contribute to the ongoing field of delayed feedback studies in online learning, and we leave the investigation in future works.

Figure 1: Left:(a) Multinomial delay with delay categories \(\{10,20,30\}\). (b) Poisson delay with rate \([]=50\). (c) Long-tail Pareto delay with shape 1.0, scale 500. Results are reported over 10 experiments. Delayed-PSVI and Delayed-LPSVI demonstrate robust performance under both well-behaved and long-tail delays.