# Divide-and-Conquer Meets Consensus:

Unleashing the Power of Functions in Code Generation

 Jingchang Chen

Harbin Institute of Technology

jcchen@ir.hit.edu.cn

&Hongxuan Tang

Harbin Institute of Technology

jeffswt@outlook.com

&Zheng Chu

Harbin Institute of Technology

zchu@ir.hit.edu.cn

&Qianglong Chen

Zhejiang University

chenqianglong.ai@gmail.com

&Zekun Wang

Harbin Institute of Technology

zkwang@ir.hit.edu.cn

&Ming Liu

Harbin Institute of Technology

mliu@ir.hit.edu.cn

Equal contribution.

Harbin Institute of Technology

mliu@ir.hit.edu.cn

&Bing Qin

Harbin Institute of Technology

qbin@ir.hit.edu.cn

Equal contribution.

Corresponding Authors: Ming Liu, Qianglong Chen.

###### Abstract

Despite recent progress made by large language models in code generation, they still struggle with programs that meet complex requirements. Recent work utilizes plan-and-solve decomposition to decrease the complexity and leverage self-tests to refine the generated program. Yet, planning deep-inside requirements in advance can be challenging, and the tests need to be accurate to accomplish self-improvement. To this end, we propose FunCoder, a code generation framework incorporating the divide-and-conquer strategy with functional consensus. Specifically, FunCoder recursively branches off sub-functions as smaller goals during code generation, represented by a tree hierarchy. These sub-functions are then composited to attain more complex objectives. Additionally, we designate functions via a consensus formed by identifying similarities in program behavior, mitigating error propagation. FunCoder outperforms state-of-the-art methods by +9.8% on average in HumanEval, MBPP, xCodeEval and MATH with GPT-3.5 and GPT-4. Moreover, our method demonstrates superiority on smaller models: With FunCoder, StableCode\({}_{3b}\) surpasses GPT-3.5 by +18.6% and achieves 97.7% of GPT-4's performance on HumanEval. Further analysis reveals that our proposed dynamic function decomposition is capable of handling complex requirements, and the functional consensus prevails over self-testing in correctness evaluation.

## 1 Introduction

Over the past few years, large language models have been observed to attain significant advancements in coding capabilities (OpenAI, 2023; Touvron et al., 2023). Meanwhile, models designed specifically for coding tasks have also been introduced (Roziere et al., 2023; Lozhkov et al., 2024; Pinnaparaju et al., 2024). Although LLMs can proficiently generate simple code snippets, they suffer from a decline in performance as code requirements become complicated.

Numerous efforts have been made to tackle this complexity. The two-stage methods (Jiang et al., 2023; Zelikman et al., 2023) employ the plan-and-solve strategy, which first generates a draft outlinefor the complex task and uses it as guidance for implementing the code in the second stage. Multi-agent development frameworks (Hong et al., 2024; Qian et al., 2023) mimic real-world software development workflows, assign different roles to LLMs and collaborate to solve a complex goal. Self-improvement (Shinn et al., 2023; Chen et al., 2024), on the other hand, refines the program in accordance with execution feedback from self-generated unit tests.

Despite fruitful efforts made by the previous methods in dealing with complex problems, certain challenges still remain unsolved: (1) Two-stage approaches need to design a complete plan at the beginning and lack the ability to adjust the top-level design during implementation, leading to sub-optimal decomposition. (2) Multi-agent collaboration frameworks are cumbersome and rely heavily on LLM capabilities, making them difficult to generalize to smaller open-source models. (3) Code refinement through self-tests depends on the correctness of generated unit-tests. Our preliminary study (SS3.1.3) finds that models generate unreliable self-tests in abundance. These incorrect tests may mislead self-improvement and, at worse, exacerbate program errors.

To address these issues, we propose FunCoder, a code generation framework utilizing a divide-and-conquer strategy and a novel functional consensus mechanism on functions to decompose complex problems. Starting from the main problem, FunCoder introduces new functions to cope with certain sub-problems. The new functions will be decomposed recursively, eventually forming a tree of functions. FunCoder then combines functions bottom-up to achieve increasingly complicated objectives. By dividing-and-conquering tasks into simpler sub-functions, complexity can be gradually reduced. However, errors in sub-functions may propagate to the whole program, thereby damaging overall reliability. We propose functional consensus that samples multiple functions and selects the one demonstrating consensus, measured by the aggregated similarity among candidates. By reaching a consensus, we reduce the discrepancies in code behavior and thus alleviate cascading errors.

We conduct extensive experiments on code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Khan et al., 2023) with GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023), outperforming state-of-the-art methods by \(+9.8\%\) on average. Experiments are further carried out on the mathematical competition benchmark, MATH (Hendrycks et al., 2021), achieving a \(+6.0\) improvement with GPT-4, indicating that FunCoder can also generalize to complex reasoning. Our method is observed to be equally effective on open-source models (Meta AI, 2024; Mistral AI, 2024; Pinnaparaju et al., 2024; Roziere et al., 2023; Lozhkov et al., 2024), with an average gain over baseline of \(+31.5\%\) on HumanEval and \(+47.7\%\) on MATH. Additional analysis also shows the advantage of both divide-and-conquer and functional consensus. Our code is made openly available at https://github.com/cometeme/funcoder.

Figure 1: A flowgraph illustrates FunCoder. FunCoder branches off new functions to have sub-goals tackled iteratively (left), re-composites sub-functions, and selects the best using functional consensus (right). Bottom-right figure shows how FunCoder writes functions at hierarchy-level.

## 2 FunCoder: Divide-and-Conquer Meets Consensus

### Divide-and-Conquer for Iterative Programming

A function is defined as a relation between a set of inputs and outputs where each input is assigned exactly one output (Halmos, 1998), denoted as \(y=f(x)\). In computer programming, a function is identified by its header \(h_{f}\) with its body \(b_{f}\), and is commonly accompanied by a documentation \(d_{f}\) to improve readability. Functions can be invoked from other procedures, allowing for the decomposition of large and complicated requirements into smaller structures that exhibit high comprehensibility and quality (Dahl et al., 1972). Generally, human programmers tend to decompose tasks into clearly defined sub-functions and then implement them recursively, making functions eligible for re-usage, taking advantage of the _divide-and-conquer_ principle. Inspired by this, FunCoder recursively _divides_ the requirement and _conquers_ functions to formulate a sophisticated solution, unleashing the potential of LLMs in code generation.

**Divide** is a top-down process that iteratively breaks down problems. Given a code generation problem, the process begins from the entry function \(f_{}\). We instruct the model to introduce new functions \(f_{i}(f_{})\) that solve certain sub-goals while writing the current \(f_{}\). To reduce the complexity involved in each generation, we only require the headers \(h_{f_{i}}\) and documentation \(d_{f_{i}}\) of new functions to be generated, while their implementations \(b_{f_{i}}\) can be postponed. After completing the current function, the model starts to address those unimplemented sub-functions and complete \(b_{f_{i}}\) into \(f_{i}^{}\). This process stops when the model degeners functions too simple to be further divided, finally forming a dependency tree \(T=(f_{},(f_{}))\). The _divide_ process is similar to a search starting from the entry function, gradually involving new sub-functions while writing the current, and implementing them recursively. We guide the entire process through a depth-first search.

**Conquer** is a process of achieving complex objectives through aggregating smaller functions. We notice that child functions are not yet implemented during the top-down process of writing parent functions. As a result, these parent functions may not be able to effectively utilize the child functions, or misuse them at worst. FunCoder deals with this issue by re-generating functions in inverse topological order on the dependency tree \(T\) - starting from leaves, complex goals are handled by compositing solved children as \(f_{}^{*}(f_{}^{},\{f_{1}^{*},f _{2}^{*},\}) f_{i}^{*}(f_{})\).

_Divide_ and _conquer_ naturally achieve both decomposition and composition during code generation. Unlike two-stage and agent-based methods, our approach dynamically introduces new functions

Figure 2: Left: Algorithm for FunCoder, explained in detail in Appendix A.6. Right: Comparison between decomposition by planning and our approach. FunCoder introduces new functions to describe sub-goals solely with code, achieving a more natural way of requirement decomposition.

along the process, making it less burdensome than producing a complete plan at the very beginning. Moreover, while planning or agents require chat capabilities, FunCoder represents sub-tasks through functions (Figure 2), making it more applicable to specialized code generation models.

### Functionality Similarity as a Consensus

The decomposition of complex tasks benefits from solving easier sub-goals, but might introduce the risks of cascading errors, which refers to errors in sub-functions that lead to errors in ancestral functions. To mitigate this, we introduce Functional Consensus which aims at reducing inconsistencies in program behavior. This is achieved by sampling multiple functions and selecting the one that exhibits consensus, as measured by the aggregated similarity of functionality between candidates, thus abating outlier functionalities.

Functionality SimilarityA program specifies its functionality (or behavior) through the control flow and logic defined by its code semantics. However, comparing the functionalities between two programs based on their semantics is somewhat challenging. By decomposing the requirement into functions, FunCoder is able to view the function behavior as a black box that maps arguments into return values. Considering two functions \(f\) and \(g\) with the same input domain \(D(f)=D(g)\), we define the similarity between them \(sim(f,g)\) as the identicalness of outputs when given the same input values.

\[sim(f,g)=_{x D(f)}[f(x)=g(x)]}{|D(f)|}\ \ _{x X|X D(f)}[f(x)=g(x)]}{|X|}\] (1)

The similarity becomes \(1\) if and only if two functions output consistent values for all inputs: \( x D(f):f(x)=g(x) sim(f,g)=1\). We notice that the input domain \(D(f)\) is unbounded in most cases, making its measurement barely feasible in practice. Thus, we approximate it by sampling a subset of possible inputs \(X D(f)\) with an LLM.

Consensusis reached by selecting the candidate \(f^{*}\) holding maximal similarity with others after sampling multiple function implementations \(F=\{f_{(i)}\}\) for the same requirements.

\[f^{*}=(F)=*{arg\,max}_{f_{(i)} F}_{f_ {(j)} F\{f_{(i)}\}}sim(f_{(i)},f_{(j)})\] (2)

By introducing functional consensus, FunCoder produces functions that are more consistent and common in functionality, while omitting abnormal samples. The process is applied to not just the final program, but also to every sub-tree during the bottom-up _conquering_ stage, resulting in step-by-step, thorough verification from the most fundamental functions all the way up to the whole program.

### FunCoder is a Function Coder

We design FunCoder as a procedure that takes a problem in the form of a function signature \(f(x)\), and produces a final solution \(f^{*}(x)\), as exemplified in Figure 1. Given a problem \(f(x)\), FunCoder partially implements the function as \(f^{}(x)\) referring to unimplemented sub-functions \(g(y)\) and \(h(z)\). These sub-functions are then fed into FunCoder to be recursively coped with. We then sample \(k\) implementations \(f^{}_{(i)}(x)\) based on solved children \(g^{*}(y)\) and \(h^{*}(z)\). Functional consensus is calculated by evaluating candidates on possible inputs. The function sharing maximal behavioral similarity is combined with solved children to formulate the final solution.

## 3 Experiments

We conduct experiments on competition-level code generation and mathematical reasoning benchmarks with state-of-the-art LLMs, which are covered in section SS3.1 and SS3.2, respectively. In addition to GPT models (Ouyang et al., 2022; OpenAI, 2023), we also conduct experiments with community models like Llama\(3_{bb}\)(Meta AI, 2024), StableCode\({}_{3b}\)(Pinnaparaju et al., 2024), and CodeLlama\({}_{34b}\)(Roziere et al., 2023). We use the _instruct_ variant of these models and inference on a single A100-80G under BF16 precision with vLLM (Kwon et al., 2023).

### Code Generation

We choose three benchmarks for code generation evaluation: (a) HumanEval (Chen et al., 2021) includes entry-level coding questions; (b) MBPP (Austin et al., 2021) contains questions of standard library invocation and programming basics; and (c) xCodeEval (Khan et al., 2023) consists of algorithmic challenges sourced from the competitive programming platform CodeForces.

#### 3.1.1 Experiment Setup

**Benchmarks** We adopt the full test set (164 problems) for HumanEval, and sample 200 for MBPP and 500 for xCodeEval, respectively. Following EbTech (2024), we split the xCodeEval into 4 subsets based on problem difficulty: Easy (\( 1200\)), Mid (\(1200\)-\(1599\)), Hard (\(1600\)-\(1999\)) and Expert (\( 2000\)). The evaluation metric for code generation is Pass@1 unless specified.

**Baselines** We compare FunCoder with standard prompting (Brown et al., 2020), two-stage decomposition method Parsel (Zelikman et al., 2023), self-testing method CodeT (Chen et al., 2023a), self-improvement methods Reflexion and LDB (Shinn et al., 2023; Zhong et al., 2024), and multi-agent developing framework MetaGPT (Hong et al., 2024). We implement Standard prompting with a 1-shot demonstration. CodeT samples 11 solutions with standard prompting and evaluates them on model-generated tests. The results for Reflexion are reproduced from the original code.

**Implementation Details** FunCoder uses a 2-shot prompt in the divide stage and 1-shot for conquering sub-functions. The number of sampled implementations in the functional consensus is set to 11 for code generation tasks. For further implementation details, please refer to Appendix A.1.

#### 3.1.2 Results

Table 1 shows the code generation performance on advanced proprietary models, GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023). For basic programming questions, HumanEval and MBPP, FunCoder surpass previous SOTA methods by +3.3% in Pass@1 and reduce the error rate by 18.6%. Furthermore, FunCoder demonstrates a substantial improvement on competition-level problems, outperforming others by 10.4% in GPT-4 and 35.3% with GPT-3.5. We observe that FunCoder can

    &  &  &  &  \\   & & Pass@1 & \(\) & Pass@1 & \(\) & _Easy_ & _Mid_ & _Hard_ & _Expert_ & All \\   & Standard & 68.3 & - & 72.0 & - & 44.4 & 15.2 & 4.6 & 0.0 & 20.2 \\  & CodeT & 81.1 & +12.8 & 76.0 & +4.0 & 50.6 & 16.1 & 8.0 & 0.0 & 23.2 \\  & Reflexion & 69.5 & +1.2 & 72.5 & +0.5 & 44.4 & 17.0 & 5.7 & 0.0 & 20.6 \\  & LDB & 82.9 & +14.6 & 76.0 & +4.0 & - & - & - & - & - \\  & FunCoder & **85.4** & +17.1 & **78.5** & +6.5 & **62.4** & **29.5** & **11.6** & 0.0 & **31.4** \\   & Standard & 82.9 & - & 73.5 & - & 68.5 & 39.3 & 19.5 & 1.7 & 37.4 \\  & Parsel & 85.0 & +2.1 & - & - & - & - & - & - & - \\  & CodeT & 90.9 & +8.0 & 77.0 & +3.5 & 76.4 & 51.8 & 21.8 & **3.4** & 44.0 \\  & Reflexion & 91.0 & +8.1 & 77.1 & +3.6 & 71.3 & 41.1 & 19.5 & 2.5 & 38.6 \\  & MetaGPT & 85.9 & +3.0 & - & - & - & - & - & - & - \\  & FunCoder & **94.5** & +11.6 & **79.5** & +6.0 & **83.1** & **58.0** & **26.4** & **3.4** & **48.6** \\  _{8b}^{}\)} & Standard & 61.6 & - & 60.5 & - & 9.0 & **1.8** & 0.0 & 0.0 & 3.6 \\  & CodeT & 68.9 & +7.3 & 61.5 & +1.0 & 12.4 & 0.0 & 0.0 & 0.0 & 4.4 \\  & FunCoder & **79.7** & +18.1 & **62.5** & +2.0 & **22.0** & 0.9 & 0.0 & 0.0 & **8.0** \\  _{3b}^{}\)} & Standard & 61.0 & - & 51.5 & - & 7.3 & 0.9 & 0.0 & 0.0 & 2.8 \\  & CodeT & 75.0 & +14.0 & 57.5 & +6.0 & 11.2 & 1.8 & 0.0 & 0.0 & 4.6 \\  & FunCoder & **81.0** & +20.0 & **63.5** & +12.0 & **13.5** & **4.5** & **1.1** & 0.0 & **6.2** \\  _{34b}^{}\)} & Standard & 43.9 & - & 53.5 & - & 2.3 & 0.0 & 0.0 & 0.0 & 0.8 \\  & CodeT & 55.5 & +11.6 & 56.5 & +3.0 & 10.1 & 0.0 & 0.0 & 0.0 & 3.6 \\   & FunCoder & **66.5** & +22.6 & **58.5** & +5.0 & **10.2** & 0.0 & 0.0 & 0.0 & **3.6** \\   

Table 1: Experiment results on code generation benchmarks. We report Pass@1 as evaluate metric. Results from the original paper are underlined, and the best results are bold.

enhance LLM's capability of solving more complex programming tasks, with an average accuracy improvement of 82.3% over the baseline on the _Mid_ and _Hard_ subsets of xCodeEval. _Expert_ level programs, however, still remain a colossal challenge for even the most cutting-edge LLMs.

Evaluation is also performed over community LLMs, Llama3 (Meta AI, 2024), StableCode (Pinnaparaju et al., 2024) and CodeLlama (Roziere et al., 2023) with results in Table 1. FunCoder consistently boosts the performance of smaller models in code generation, demonstrating notable improvements compared to standard prompting on HumanEval, which gained +29.4% on Llama3, +32.8% on StableCode, and even +51.5% on CodeLlama, outperforming that from the previous best method CodeT. We also supplement results on GPT-4o mini, Codestral and StarCoder2 in Table 11. Experiment results demonstrate that our method archives state-of-the-art performance on various models, ranging from basic programming to competition contests.

#### 3.1.3 Analysis

FunCoder Democratize to Smaller LLMsLimited by the LLM capabilities, the application of self-improvement or multi-agent methods on smaller models is without ease. By keeping decomposition and composition within the code generation process, our approach exhibits better generalization. As shown in Table 1, with FunCoder, StableCode\({}_{3b}\) achieves around \(118.6\%\) relative performance to standard GPT-3.5, and also aligns closely with GPT-4 by about \(97.7\%\) on HumanEval.

Preliminary Study on Self-Testing MethodWe conduct a preliminary study targeting the self-testing method on HumanEval, results are shown in Figure 3.a with further details in Appendix A.5. We first verify whether model-generated programs can also pass model-generated self-tests: (a) If a program passes self-tests, most from GPT-3.5 would also work on system tests, as much as \(19.5\%/64\% 30.5\%\) programs from StableCode are rejected, indicating that smaller models like StableCode may not effectively self-test and detect program errors on its own. (b) In the event of failed self-tests, a large portion of failures are attributed to issues in self-tests instead of the programs, on both GPT-3.5 and StableCode. These phenomena indicate that self-testing methods have limitations in generating correct and reliable unit tests. As a result, we design functional consensus to not require any assertion, but perform _mutual verification_ between solutions instead, as opposed to self-testing.

Effectiveness of Functional ConsensusFunctional consensus or self-testing may be viewed as ranking algorithms for selecting functions. To measure ranking effectiveness, we conduct an analysis on HumanEval with GPT-3.5. For each problem, 11 candidates are ranked with 3 strategies: consensus, self-test, and random shuffle (as a baseline). Effectiveness is measured via Pass@k, i.e. if any of the top-k ranked programs pass the system test. Figure 3.b shows that functional consensus achieves \(94.7\%\) upper bound (Pass@11) performance by selecting _a single_ function (Pass@1), and is close to that of self-test on Pass@4. This clearly demonstrates that functional consensus can effectively evaluate correctness and pick the most promising implementation on the first attempt.

Ablation and Token UsageTo analyze the impact of dividing, conquering, and functional consensus in FunCoder, we carry out an ablation study with different settings. Studies that replace consensus with self-testing, or with AlphaCode-like (Li et al., 2022) clustering, are also included. The ablation is constructed on HumanEval with GPT-3.5, as shown in Table 2. Note that to generate every program FunCoder costs only \(O(kN)\) tokens, where \(k\) is the number of sampled candidates,

Figure 3: (a) Preliminary study on self-testing, the programs are evaluated using unit-tests generated by LLMs. (b) The effectiveness of different ranking strategies. We compute the Pass@k over top-k programs ranked by functional consensus, self-test, and random on 11 candidates. (higher is better)

and \(N\) is the token length of the final program. This is further exemplified and explained in SSA.7. We observe that function decomposition and re-composition deliver cumulative performance improvements. Functional consistency is also shown to prevail over self-testing. Putting them all together, FunCoder received a \(+17.1\) improvement with just \(5.09\) more tokens over baseline. Compared to previous SOTA LDB (\( 23\)K tokens), we are able to gain \(+2.5\) in performance with \(76.5\%\) token usage reduction.

### Mathematical Reasoning

Code can be viewed as a tool for augmenting the reasoning capabilities of LLMs (Chen et al., 2023b). Alternative to text-based reasoning like Chain-of-Thought (Wei et al., 2022), programs can offer unique advantages in terms of iteration and calculations. To test the generalizability of FunCoder beyond algorithm challenges, we conduct an experiment on MATH (Hendrycks et al., 2021b), a competition-level mathematical reasoning benchmark.

#### 3.2.1 Experiment Setup

BenchmarkThe experiment is conducted on a subset of the MATH test set, including 500 randomly sampled problems that can be classified into 7 disjoint subjects or 5 difficulty levels. It can be noticed that labels in MATH are formatted in LaTeX, rendering exact-match verdicts impractical. We, therefore, follow previous work (Zhang et al., 2024) and adopt GPT-4 to determine the correspondence between predictions and labels, with further details provided in Appendix A.4.

BaselinesWe compare FunCoder with the text-based baselines: Standard Prompting and Chain-of-Thought (Wei et al., 2022), and program-aided baselines: Program-of-Thought (Chen et al., 2023b), Self-Refine (Madaan et al., 2023), Cumulative Reasoning (Zhang et al., 2024). The results of Cumulative reasoning are reported in the original paper. Standard prompting and chain-of-thought reasoning use 7-shot demonstrations constructed from the train set. Program-of-Thought and Self-Refine prompt the model with 1-shot demonstration to generate a solution() function that solves the problem. Additionally, self-refine iteratively refines programs based on runtime feedback. All baseline methods are run with self-consistency (Wang et al., 2023) at 5.

Implementation DetailsFunCoder adopts a program-aided reasoning setting that writes a solution() function and obtains the final prediction by running this program. The number of sampled implementations \(|F|\) in functional consensus is set to 5 to match baseline methods.

#### 3.2.2 Results

The experimental results on MATH are shown in Table 3. It shows that program-aided reasoning generally outperforms text-based reasoning. With GPT-4 as the backbone, FunCoder outperforms the strongest baseline Cumulative Reasoning (Zhang et al., 2024) by (6.0 / 8.3%) and surpasses the vanilla program-aided baseline PoT (Chen et al., 2023b) by (10.0 / 14.7%). When using GPT-3.5-turbo as the backbone, FunCoder exceeds the strongest baseline by (6.2 / 11.1%) and outperforms PoT by as much as (13.0 / 31.7%), which indicates that our approach has a strong advantage over both text-based reasoning and other program-aided reasoning methods.

  
**Setting** & Divide & Conquer & Ranking & **Pass@1** & **Avg. Tokens** \\  Standard & ✗ & ✗ & ✗ & 68.3 & **886.7** \\ One-pass & ✓ & ✗ & ✗ & 72.6 (+4.3) & 1233.7 \\ Two-pass & ✓ & ✓ & ✗ & 78.7 (+10.4) & 3343.2 \\ Two-pass + ST@11 & ✓ & ✓ & Self-Test@11 & 80.5 (+12.2) & 5408.3 \\ Two-pass + CL@11 & ✓ & ✓ & Clustering@11 & 75.0 (+6.7) & 5070.7 \\  FunCoder@5 & ✓ & ✓ & Consensus@5 & 83.5 (+15.2) & 4040.9 \\
**FunCoder@11** & ✓ & ✓ & Consensus@11 & **85.4 (+17.1)** & 5402.0 \\   

Table 2: Ablation study of FunCoder on HumanEval with GPT-3.5. The setting in our main experiment is highlighted in bold. Tokens are calculated as the sum of prompts and completions.

On open-source models, FunCoder with Llama3 outperforms PoT by (12.4 / 38.0%). It has even reached competitive performance against the state-of-the-art method based on GPT-3.5 (45.0 v.s. 48.6). When employing StableCode and CodeLLaMA as the backbone, our approach achieves significant improvements by (12.2 / 84.7%) and (9.2 / 60.5%), respectively. This improvement demonstrates that our approach can significantly boost smaller LLMs, democratizing the complex reasoning capabilities of open-source LLMs through programming.

#### 3.2.3 Analysis

FunCoderCan HandleHarderQuestionsFigure 4 compares between CoT, PoT, and FunCoder across varying difficulty levels. It illustrates that CoT performs comparatively well on the easiest questions, but suffers from a steep decline in performance as difficulty increases. This suggests that text-based reasoning is inadequate for tackling challenging mathematical reasoning problems. The same situation is also observed in PoT. In contrast, our method consistently demonstrates high performance even on challenging problems, particularly excelling on level 5 difficulty with nearly double the performance compared to PoT and CoT. This reflects that our method, with divide-and-conquer applied, can effectively cope with complex problems.

DecomposedFunctions are Domain-SpecificWe hypothesize that questions from the same subject require similar knowledge reserves, which should be reflected in the functionality of the sub-functions. To verify this hypothesis, we statisticize the common sub-functions of FunCoder in each MATH subject, as shown in Table 4. It is apparent that different subjects require different abilities, each with its own set of sub-functions closely associated with the domain knowledge. In addition, these common sub-functions are fundamentally basic and straightforward. As exemplified in Appendix B.2, our method is able to leverage and combine these basic sub-functions to achieve more complex goals, thereby reducing the complexity of reasoning and enhancing performance.

  
**Model** & **Method** & _Prealg._ & _Alg._ & _NT_ & _Prob._ & _Geo._ & _InterAlg._ & _Precalc._ & **Overall** \\   & Standard\({}^{}\) & 62.2 & 37.4 & 20.0 & 29.8 & 31.0 & 24.4 & 21.8 & 34.6 \\  & CoT\({}^{}\) & 59.8 & 51.1 & 28.9 & 29.8 & 28.6 & 26.7 & 30.9 & 40.0 \\  & PoT & 68.3 & 50.4 & 33.3 & 48.9 & 21.4 & 18.2 & 29.1 & 41.0 \\  & Self-Refine & 74.4 & 49.6 & 48.9 & 57.4 & 28.6 & 35.6 & 36.4 & 48.6 \\  & FunCoder & **76.8** & **61.2** & **55.6** & **59.6** & **34.1** & **36.0** & **41.8** & **54.0** \\   & Standard\({}^{}\) & 81.7 & 82.7 & 71.1 & 72.3 & **59.5** & 46.7 & 47.3 & 68.2 \\  & CoT\({}^{}\) & 84.1 & 87.1 & 62.2 & 68.1 & 45.2 & 48.9 & 54.5 & 68.6 \\  & PoT & 79.3 & 80.6 & 75.6 & 72.3 & 50.0 & 47.8 & 58.2 & 68.2 \\  & Self-Refine & 82.9 & 82.0 & 77.8 & 76.6 & 54.8 & 55.6 & **63.6** & 72.2 \\  & CR & 86.6 & 86.3 & **88.7** & 71.1 & 53.7 & 51.5 & 51.8 & 72.2 \\  & FunCoder & **89.0** & **92.8** & 82.2 & **83.0** & **59.5** & **63.3** & 56.4 & **78.2** \\  _{8b}\)} & CoT\({}^{}\) & 56.1 & **47.5** & 31.1 & 34.0 & **40.5** & 14.4 & **38.2** & 38.6 \\  & PoT & 67.1 & 32.4 & 24.4 & 34.0 & 16.7 & 21.1 & 18.2 & 32.6 \\  & FunCoder & **67.9** & 45.7 & **51.1** & **53.2** & 19.0 & **37.8** & 30.9 & **45.0** \\  _{3b}\)} & PoT & 20.7 & 14.4 & 17.8 & 25.5 & **4.8** & 8.9 & 9.1 & 14.4 \\  & FunCoder & **46.3** & **30.2** & **20.0** & **29.8** & **4.8** & **20.0** & **18.2** & **26.6** \\  _{4b}\)} & PoT & 35.5 & 26.1 & 15.0 & 16.7 & 0.0 & 5.5 & 33.3 & 15.2 \\  & FunCoder & **44.8** & **46.1** & **37.8** & **34.1** & **13.6** & **24.6** & **37.5** & **24.4** \\   

Table 3: Experimental results on MATH, a competition-level mathematical reasoning benchmark. Best results are in bold. Text-based reasoning methods are denoted with \({}^{}\), while others use program-aided reasoning. We report both overall results and results in seven subjects: _Prealgebra_, _Algebra_, _Number Theory_, _Counting & Probability_, _Geometry_, _Intermediate Algebra_, and _Precalculus_.

Figure 4: Average accuracy in each level with the chat model (GPT-3.5) and the code model (StableCode\({}_{3b}\)) on the MATH benchmark.

## 4 Related Work

**Large Language Model for Code** Code pre-training has received widespread attention, with early models based on small language models (SLM) (Feng et al., 2020; Lu et al., 2021; Wang et al., 2021). In recent years, with the development of large-scale pre-training techniques, code LLM has emerged, showing remarkable performance in downstream code tasks (Chen et al., 2021; Nijkamp et al., 2023; Li et al., 2022; Roziere et al., 2023; Li et al., 2023; Guo et al., 2024). Tasks between code and natural language (NL) can be generally divided into three major categories: NL2Code tasks such as code generation (Austin et al., 2021; Chen et al., 2021; Hendrycks et al., 2021; Khan et al., 2023) and code search (Husain et al., 2019); Code2Code tasks including code completion (Lu et al., 2021; Zhang et al., 2023; Liu et al., 2024), code translation (Ahmad et al., 2023; Zhu et al., 2022; Yan et al., 2023), and test generation (Siddiq et al., 2023; Schafer et al., 2024); Code2NL tasks like code summarization (Jin et al., 2023). This paper focuses on code generation tasks, ranging from basic to competition level.

**Code Refinement and Self-Testing** Code doesn't always run as expected; it could contain syntax errors, dead loops, or bugs. It's essential to debug and refine the code to ensure better quality. CodeT (Chen et al., 2023a) generates unit-tests to score the implementation. AlphaCode (Li et al., 2022) clusters programs based on whether generated program outputs were identical or not. Self-improvement methods (Madaan et al., 2023; Shinn et al., 2023; Chen et al., 2024; Zhong et al., 2024) design closed-loop procedures that repeatedly refine the code based on the feedback. Like real-life software development processes, multi-agent frameworks (Hong et al., 2024; Qian et al., 2023) construct specific LLM roles, _Tester_ or _QA_ to generate tests. These studies adopt a shared paradigm wherein self-tests are generated through LLMs. However, Olausson et al. (2024) points out the challenge that LLMs have certain shortcomings in self-repairing their code. This paper avoids these shortcomings by proposing _functional consensus_ as a reliable method of evaluation.

**Program-Aided Reasoning and Agents** Aside from code generation tasks, the program can be a tool that augments LLM to solve complex reasoning questions or interact with external environments. Program-of-Thought (Chen et al., 2023b) and PAL (Gao et al., 2023) prompt the model to generate a program that solves mathematical or symbolic problems. MathPromptter (Imani et al., 2023) and Chain-of-Code (Li et al., 2023a) fuse the text-based chain-of-thought with code-based program-of-thought prompting to complement each other in mathematical reasoning. Cumulative Reasoning (Zhang et al., 2024) conducts bottom-up reasoning to derive the final answer progressively. Numerous work (Sun et al., 2023; Wang et al., 2024; Yang et al., 2024) also use code as an intermediate component to bridge LLM agents with external environments.

**Decompose for Complex Problems** Several recent works employ decomposition to reduce the complexity of hard problems. Least-to-Most (Zhou et al., 2023) adopts a two-stage approach, which first decomposes complex problems, and then solves each sub-problem individually to tackle complex reasoning tasks. Successive Prompting (Dua et al., 2022) adopts a dynamic decomposition, iteratively breaking down problems and addressing sub-problems. Tree-of-Thought (Yao et al., 2023) breaks down complex problems into state spaces and uses tree search to solve them. Parsel (Zelikman et al., 2023) introduces decomposition to code generation tasks, taking a three-stage to break down requirements into draft and intermediate parsed programs. RepoCoder (Zhang et al., 2023) performs a retrieval in repositories to complete unfinished code one by one. Unlike these methods, FunCoder recursively decomposes problems into a tree structure, hence gradually reduces its complexity.

  
**Subject** & **Functions** \\  Prealgebra & is\_prime / factorial / gcd \\ Algebra & find\_roots / is\_perfect\_square / find\_domain \\ Number Theory & get\_divisors / mod\_inverse / gcd \\ Counting \& Probability & factorial / combinations / binomial\_coefficient \\ Geometry & distance / simplify\_fraction / calculate\_triangle\_area \\ Intermediate Algebra & find\_roots / evaluate\_polynomial / lagrange\_interpolation \\ Precalculus & cross\_product / fraction\_from\_angle / dot \\   

Table 4: Top-3 most commonly used functions in each subject of MATH, listed in descending order.

Discussion

**Limitations** Our approach unleashes the potential power of functions in programming, which is advantageous on well-defined problems such as competitive programming, or program-augmented reasoning tasks. These scenarios do not however represent all use cases, such as open-ended problems or casual software development. Nevertheless, we believe that the idea of divide-and-conquer and sub-modular consensus utilized by FunCoder can be extended to a wider range of problems, and we consider this as a future exploration.

**Broader Impact** While code generation is increasingly utilized in software development, Large Language Models (LLMs) are still prone to generating toxic, vulnerable, or malicious code. Such programs pose risks and should be used or executed with extra caution.

## 6 Conclusion

In this paper, we presented FunCoder, a novel code generation framework that integrates the divide-and-conquer strategy with functional consensus to address complex requirements. FunCoder had demonstrated superior performance compared to state-of-the-art methods on various benchmarks and models. Our findings highlighted the effectiveness of dynamic decomposition and functional consensus in writing complex code, which suggests that FunCoder may have the potential to empower further improvements in code generation and other fields.