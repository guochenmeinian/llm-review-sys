# FNP: Fourier Neural Processes for

Arbitrary-Resolution Data Assimilation

 Kun Chen \({}^{1,2}\)  Peng Ye \({}^{1,2}\)  Hao Chen \({}^{2}\)  Kang Chen \({}^{2}\)  Tao Han \({}^{2}\)

Wanli Ouyang \({}^{2}\)  Tao Chen \({}^{1,*}\)  Lei Bai \({}^{2,*}\)

\({}^{1}\) Fudan University \({}^{2}\) Shanghai Artificial Intelligence Laboratory

kunchen22@m.fudan.edu.cn eetchen@fudan.edu.cn baisanshi@gmail.com

Corresponding authors

###### Abstract

Data assimilation is a vital component in modern global medium-range weather forecasting systems to obtain the best estimation of the atmospheric state by combining the short-term forecast and observations. Recently, AI-based data assimilation approaches have attracted increasing attention for their significant advantages over traditional techniques in terms of computational consumption. However, existing AI-based data assimilation methods can only handle observations with a specific resolution, lacking the compatibility and generalization ability to assimilate observations with other resolutions. Considering that complex real-world observations often have different resolutions, we propose the **F**ourier **N**eural **P**rocesses (FNP) for _arbitrary-resolution data assimilation_ in this paper. Leveraging the efficiency of the designed modules and flexible structure of neural processes, FNP achieves state-of-the-art results in assimilating observations with varying resolutions, and also exhibits increasing advantages over the counterparts as the resolution and the amount of observations increase. Moreover, our FNP trained on a fixed resolution can directly handle the assimilation of observations with out-of-distribution resolutions and the observational information reconstruction task without additional fine-tuning, demonstrating its excellent generalization ability across data resolutions as well as across tasks. Code is available at [https://github.com/OpenEarthLab/FNP](https://github.com/OpenEarthLab/FNP).

## 1 Introduction

Accurately estimating the true state of complex and chaotic Earth systems is an important and challenging task, which can contribute to a better understanding of nature and improve forecasting by reducing the error of initial conditions. The most accurate human knowledge of the Earth's state comes from observations, which are inherently limited in their scopes due to practical constraints. Data assimilation, based on limited observational information and short-term forecasts (referred to as the _background_), serves as the primary approach for state estimation . Traditional data assimilation methods employed in operational systems include Kalman filters based on minimum variance estimation and variational methods based on maximum likelihood estimation . Taking 3D variational (3D-Var) data assimilation as an example, data assimilation is regarded as an optimization problem under given conditions, aiming to find the _analysis_\(x_{a}\) that minimizes theobjective function \(J(x)\). It can be formulated as

\[x_{a}=x^{*}=*{arg\,min}_{x}J(x) \]

\[J(x)=(x-x_{b})^{T}B^{-1}(x-x_{b})+(y-Hx)^{T}R^{-1}(y-Hx) \]

where \(B\) and \(R\) correspond to the error covariance matrix of the background \(x_{b}\) and observation \(y\), respectively, and \(H\) is observation operator that maps state variables to observational space, aligning the background and observations with different modalities (for example, satellites do not directly observe state variables such as wind speed) and resolutions.

With the significant achievements of machine learning in medium-range weather prediction [44; 6; 31; 9; 11; 5; 35; 61], data assimilation has gained increasing attention as one of the core components in building end-to-end global weather forecasting systems. Compared to traditional methods, machine learning-based data assimilation models offer the potential for competitive results with significantly reduced resource consumption and execution time [10; 28; 62], making it a promising research direction with practical applications. Chen et al.  proposed a data assimilation model for weather variables based on the idea of gated masks, and combined it with FengWu , an advanced AI-based weather prediction model, to build the first end-to-end AI-based global weather forecasting system. Subsequently, data assimilation models integrated with other AI-based weather prediction models were propsed [28; 62]. These methods have demonstrated performance and efficiency improvements through various experiments, but _all of them can only assimilate observations with the same resolution as the forecasting model_. Therefore, they need to interpolate the observations onto grids of corresponding resolution through pre-processing in advance, and the pre-trained models do not have the flexibility and out-of-domain generalization to assimilate observations with other resolutions. The pre-processing step implements part of the function of the observation operator \(H\) to perform resolution alignment, and it will introduce additional errors inevitably, thereby affecting the performance and generality of data assimilation methods.

Neural processes [17; 18; 30; 20; 46; 42] offer a promising and universal data assimilation framework for addressing the aforementioned challenges. Neural processes are a series of conditional generative models that continuously model the distribution of functions and fields based on paired coordinate-value conditions, and generate values at arbitrary target locations based on coordinate indices. Their flexible features that allow for grid or off-grid data are well-suited for assimilating observational data with diverse forms, without requiring any prior interpolation or mapping [55; 2; 53; 16; 54]. _In this context, data assimilation is defined as the process of generating the analysis given both background conditions and observational information conditions_. The network models the comprehensive functional representation based on the two conditional inputs and decodes it to obtain the posterior distribution of the target. Compared to deterministic data assimilation, the modeling of distribution by neural processes can provide uncertainty estimates and further be used for ensemble data assimilation . Moreover, data assimilation task degrades to observational information reconstruction when the background condition is missing. These two tasks can be broadly categorized as conditional generation, enabling their straightforward integration into a unified framework for direct application through simple fine-tuning.

In this paper, we propose the Fourier Neural Processes (FNP) for data assimilation with arbitrary-resolution observations. FNP is flexible to adapt to varying resolutions and can be extended to any conditional generation task. Leveraging the efficiency of the designed modules and flexible structure of neural processes, _FNP achieves state-of-the-art (SOTA) results in data assimilation experiments with different resolutions_, and demonstrates increasing advantages over other models as the resolution and amount of observational information increase. The visualization of the analysis showcases the promising performance of FNP in capturing high-frequency information. Importantly, the FNP trained at a fixed resolution can be directly applied to data assimilation with other resolutions and observational information reconstruction task without fine-tuning, highlighting its excellent out-of-domain generalization. Additionally, ablation study for different modules and experimental settings validate the effectiveness and robustness of our approach.

## 2 Related Work

Machine learning for data assimilation.There exist strong mathematical similarities between machine learning and data assimilation, enabling their integration within a unified Bayesian frame work [19; 3]. With their powerful nonlinear fitting capabilities and low computational cost, machine learning techniques can both enhance traditional data assimilation methods and provide alternative algorithms [13; 7; 22; 56]. Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are often employed as surrogate models to replace computationally expensive components in data assimilation, such as tangent-linear and adjoint models in 4D variational (4D-Var) data assimilation , localization functions in ensemble Kalman filters (EnKF) , and error covariance matrices [12; 45]. Implicit neural representations (INRs)  and various autoencoders (AEs) [47; 41; 1] can offer efficient order reduction frameworks for latent assimilation to address the challenges of high-dimensional data. More recently, algorithms based on diffusion models have also provided new solutions for data assimilation driven by the advancements and maturity of AIGC [28; 52; 48]. However, all these studies are aimed at assimilating fixed-resolution observations, and _we are the first to focus on arbitrary-resolution data assimilation_.

Machine learning for observational information reconstruction.Observational information reconstruction is the process of recovering missing values and obtaining complete field information from limited sparse observations. Traditional reconstruction methods primarily rely on kriging interpolation and principal component analysis-based infilling . As Kadow et al.  successfully applied the image inpainting techniques in computer vision to reconstruct the global temperature data, deep learning has been widely used in various reconstruction tasks [40; 59; 15; 57]. We associate the reconstruction with the data assimilation task here, and _the flexibility of our method allows the FNP pre-trained on data assimilation task to be directly applied to the observational information reconstruction without fine-tuning and achieve promising performance_.

Neural processes family and its application in geoscience.Neural processes combine the advantages of neural networks and Gaussian processes, and have demonstrated excellent performance in function regression, image completion and classification tasks [17; 18]. Attentive neural processes (ANP)  enable the network to learn location-relevant representations by introducing the attention mechanism, which improves the accuracy of predictions and broadens the scope of modeling. Convolutional conditional neural processes (ConvCNP)  model translation equivariance in the data, adding an important inductive bias to the model and enabling zero-shot generalization to out-of-domain tasks. Evidential conditional neural processes (ECNP)  replace the standard Gaussian distribution with a hierarchical Bayesian structure through evidence learning to achieve the decomposition of epistemic-aleatoric uncertainty. Neural processes and their variants, leveraging their unique advantages, have been successfully applied in geoscience tasks such as climate downscaling , sensor placement  and observational information reconstruction , and have shown promising performance. Here _we apply the neural processes to arbitrary-resolution data assimilation for the first time and achieve SOTA performance_, further demonstrating its huge application potential.

## 3 Methods

### Model Overview

The overall framework and model details of FNP are depicted in Figure 1. Initially, the background and observations undergo a unified coordinate transformation to obtain the coordinates \(x^{c}\) and values \(y^{c}\) of the conditional points when they are input into the network. This ensures spatial alignment of them, even in the presence of disparate resolutions, modalities, and data formats. Subsequently, FNP models the two components of the conditional information globally to get their respective spatial-variable functional representation. The dynamic alignment and merge (DAM) module integrates and aligns these functional representation into the target domain, resulting in a comprehensive functional representation over the target space. Finally, multi-layer perceptrons (MLPs) are employed to decode the functional representation and output the mean and variance of the analysis based on the coordinates \(x^{t}\) of the target points. In the following subsections, we provide a detailed description of the process for modeling the functional representation and the internal structure of the DAM module.

### Spatial-Variable Functional Representation

Modeling the functional representation involves two main steps: embedding the data sets into an infinite-dimensional function space and performing deep feature extraction. The former is accomplished through the _SetConv_ layer , which is a generalized form of the standard convolutional layer extended to operate on sets. It takes a set of continuous coordinate-value pairs as input and outputs a function that can be queried at continuous positions. The _SetConv_ operation is permutation-invariant and includes an additional channel to estimate the density of the conditional points. When the input coordinates are discrete, _SetConv_ essentially degenerates into a standard convolutional layer, simplifying the model into the on-the-grid version . We strongly recommend readers to refer to the theoretical proofs and derivations of ConvCNP , and the project homepage of neural processes family  for a more detailed explanation.

The original deep feature extraction module is implemented using a standard CNN with residual structures. We choose to replace the basic convolutional layer with a more efficient neural Fourier layer (NFL) in FNP. Additionally, to address the multi-variable optimization problem in weather modeling tasks, we decouple the representations in spatial and variable dimensions to reduce the difficulty of network training. Below we provide further explanations on the motivations and implementation details of these design choices.

Spatial-variable decoupled representation.Data for different weather variables are usually stacked in the channel dimension, and direct data embedding will mix the spatial auto-correlation within variables with the inter-correlation among variables. An intuitive understanding is that explicitly separating the information in the spatial and variable dimensions allows for a clearer learning objective for each block, thereby reducing the difficulty of network training and fully unleashing the network's potential. In terms of implementation, we model a spatial functional representation separately for each meteorological variable, such as geopotential and temperature (surface variables are treated together as one variable), and model a variable functional representation that encompasses all variables, which are then concatenated together. The benefits of this approach have been confirmed in our experiments. We found that the spatial-variable decoupled (SVD) representation achieves better performance with fewer parameters and faster convergence speed. The detailed comparison of performance can be seen in Table 3.

Neural Fourier layer.The smoothness of neural network outputs poses a challenging drawback in weather modeling tasks, and the background generated by AI-based forecasting models also tends to be smoother. In our experiments, we find that neural processes also struggle to overcome the issue of smoothness. To address the desire for high-frequency information, we choose to introduce the

Figure 1: Overview of the network architecture of FNP. Unified coordinate transformation ensures spatial alignment of the background and observations, and extracts the coordinates and values of the conditional points. FNP models the two components of the conditional information globally to get their respective spatial-variable functional representation through _SetConv_ for data embedding and stacking of neural Fourier layers for deep feature extraction. The dynamic alignment and merge module integrates these functional representation based on similarity to shared features and aligns them into the target domain, resulting in a comprehensive functional representation over the target space. MLPs are finally employed to decode the functional representation and output the mean and variance of the analysis based on the coordinates of the target points.

Fourier neural operator . Besides, operations in the frequency domain can also bring additional advantages in terms of global receptive fields for models based on CNN. Therefore, in addition to the convolutional operation, each neural Fourier layer consists of a branch for linear operation in the frequency domain and a branch for identity mapping to preserve high-frequency details as much as possible .

### Dynamic Alignment and Merge

The DAM module aligns functional representations from two conditional domains to the target domain for obtaining outputs at the target locations. In data assimilation tasks, the analysis typically shares the same resolution and modalities as the background, and the previous data embedding has already mapped inputs that may have different modalities into the same feature space. Therefore, it is only necessary to align the functional representation of the observation in the spatial resolution. We choose to use interpolation to adjust the spatial dimensions' size as it can accommodate inputs of arbitrary resolutions, thereby enhancing the dynamics and generality of the model. _Interpolation in the feature space differs fundamentally from that in the original observational space because the former has already extracted helpful information and contains redundancy to support dimensionality reduction, while the latter compresses valid information and missing values to the same extent._ The performance of data assimilation with different resolutions in Table 1 provides proof for this. As the amount of observational information increases, our model achieves significant improvements, while other models do not. A linear layer extracts shared features \(\) from both parts after alignment, which are then used to calculate similarities with their respective feature components. The similarity calculation is performed in the channel dimension as the spatial distribution of information differs significantly between them, with the background having a more uniform spatial distribution while the observation exhibits greater spatial variability. In our implementation, the feature similarity is represented by the Euclidean distance between the two features, i.e.,

\[Sim_{h,w}=^{k}(y_{h,w,i}-_{h,w,i})^{2}} \]

where \(h\) and \(w\) denote the indices for each grid point along the longitudinal and latitudinal directions, respectively, and \(k\) is the dimension of data embedding. The relative values of the similarity map then determines the selection of features. Specifically, features that are more similar to shared features will be retained, while features that are less similar will be discarded, that is,

\[y_{h,w}=y_{h,w}^{b},&Sim_{h,w}^{b} Sim_{h,w}^{o}\\ y_{h,w}^{o},&Sim_{h,w}^{b}<Sim_{h,w}^{o} \]

The dynamically filtered features will be spliced together with the shared features and sent to a convolutional layer for spatial smoothing, and the result will be used as the functional representation in the target domain for decoding and output.

## 4 Experiments

### Experimental Settings and Implementation

Data preparation.We demonstrate the effectiveness of our methodology on the ERA5 dataset , a global atmospheric reanalysis archive containing hourly weather variables such as geopotential, temperature, wind speed, humidity, etc. We choose to conduct experiments on a total of 69 variables, including five upper-air variables with 13 pressure levels (i.e., 50hPa, 100hPa, 150hPa, 200hPa, 250hPa, 300hPa, 400hPa, 500hPa, 600hPa, 700hPa, 850hPa, 925hPa, and 1000hPa), and four surface variables. Specifically, the upper-air variables are geopotential (z), temperature (t), specific humidity (q), zonal component of wind (u) and meridional component of wind (v), whose 13 sub-variables at different vertical level are presented by abbreviating their short name and pressure levels (e.g., z500 denotes the geopotential at a pressure level of 500 hPa), and the surface variables are 10-meter zonal component of wind (u10), 10-meter meridional component of wind (v10), 2-meter temperature (t2m) and mean sea level pressure (msl). A subset of ERA5 dataset for 40 years, from 1979 to 2018, is chosen to train and evaluate the model.

Experimental settings.The advanced AI-based weather forecasting model, FengWu , is used as the surrogate model to generate the background. The observations are simulated by adding a proportional mask to ERA5, and the default setting corresponds to 24-hour forecast lead time and 10% observations. In other words, the background used for data assimilation is produced by FengWu (with 6-hour interval) through four auto-regressive iterative predictions based on ERA5 data from one day ago. The observational space usually has higher spatial resolution than the state space in actual operation systems. Therefore, the resolution of the forecasting model and background is set to 1.40625\({}^{}\) (\(128 256\) grid points) so that we can conduct experiments using observations with different resolutions such as 0.25\({}^{}\) (\(721 1440\) grid points) to verify the assimilation performance with arbitrary resolution.

Model training and evaluation.The FNP model is implemented based on the open-source code of the neural processes family project , and trained for 20 epochs using the AdamW optimizer  with a learning rate of 1e-4. We divide the ERA5 data from 1979-2015 as the training set, 2016-2017 as validation set, and 2018 as test set. The training is run on 4 NVIDIA Tesla A100 GPUs with a global batch size of 16, and takes approximately 2.5 days. The inference only needs a few minutes to perform data assimilation for a whole year on single A100 GPU. The dimension of data embedding for default setting is 128 and the number \(N\) of NFLs is 4, and a Gaussian likelihood is used with a negative log-likelihood (NLL) loss. We evaluate the performance of models by calculating the overall mean square error (MSE), mean absolute error (MAE), and the latitude-weighted root mean square error (WRMSE) which is a statistical metric widely used in geospatial analysis and atmospheric science . Given the estimate \(_{h,w,c}\) and its ground truth \(x_{h,w,c}\) for the \(c\)-th channel, the WRMSE is defined as

\[(c)=_{h,w}H)} {_{h^{}=1}^{H}(_{h^{},w})}(x_{h,w,c}-_{h,w,c })^{2}} \]

where \(H\) and \(W\) represent the number of grid points in the longitudinal and latitudinal directions, respectively, and \(_{h,w}\) is the latitude of point \((h,w)\).

### Arbitrary-Resolution Data Assimilation

We validate the performance of models by assimilating 10% observations with resolutions of 1.40625\({}^{}\), 0.703125\({}^{}\), and 0.25\({}^{}\), respectively, onto a 24-hour forecast background with 1.40625\({}^{}\) resolution. Table 1 provides a quantitative comparison of the analysis errors between FNP and other models. The first row corresponds to the error level of the background. When assimilating observations with the same resolution as the forecasting model, _FNP achieves SOTA results (indicated in bold) in terms of overall MSE, MAE, and WRMSE metrics for the majority of variables_. Since Adas  is not flexible enough to support inputs with different resolutions, we follow its common practice to interpolate the observations and average the observations falling within the corresponding grid range when assimilating higher-resolution observations. Despite this, Adas still produces results with significantly high errors, so we only present the performance after fine-tuning to the interpolated observations. This indicates that Adas lacks the ability of out-of-domain generalization. In contrast, FNP and ConvCNP , with their flexible structures, can assimilate observations with different resolutions directly without interpolation. Therefore, the table presents the results for both cases with and without fine-tuning for FNP and ConvCNP.

In order to better understand and explain the performance of different models, we add different colors to represent the variations in results compared to that with 1.40625\({}^{}\) resolution (blue indicating worse results, i.e., increased errors, and red indicating improved results). For the q700 variable, we additionally annotate the percentage of error increase or decrease. It is worth noting that with increasing resolution, the same ratio of observations implies a greater number of absolute observations and a larger amount of information. However, during the interpolation process, averaging the observation values within a region does not guarantee a reflection of the overall conditions unless there are observations at all points within the region. Therefore, interpolation inevitably leads to information loss, and the amount of lost information is negatively correlated with the number of observations within the region. Based on the balance between these two factors, the fine-tuned Adas exhibits different trends with two different resolutions: the results generally improve when assimilating observations with 0.703125\({}^{}\) resolution, while all the results worsen when assimilating observations with 0.25\({}^{}\) resolution. This indicates that as the resolution increases gradually, the impact of information loss due to interpolation becomes more significant and surpasses the positive effect of 

[MISSING_PAGE_FAIL:7]

estimation achieved by FNP. More visualizations with different variables and resolutions are shown in Appendix B.

Data assimilation aims to improve weather forecasting results by reducing initial errors. Therefore, it is crucial to explore the impact of different methods on forecast error reduction. Figure 3 provides results on the forecast WRMSE improvement of z500 variable over the next ten days through data assimilation, where lead time 0 corresponds to the reduction of initial errors. Darker colors indicate stronger improvements, meaning a greater reduction in forecast errors compared to not using data assimilation. Similar to the results of data assimilation, FNP consistently achieves state-of-the-art results in most cases, with its advantage becoming more pronounced as the resolution increases. Moreover, FNP is the only model that strictly enhances forecast improvement with increasing resolution and observational information. Additionally, apart from the accuracy of initial values affecting forecast errors, the physical characteristics of the initial states (e.g., physical balance) also influence the rate of forecast error growth. FNP demonstrates greater improvements in forecast errors at all lead times compared to improvements in initial errors, indicating that FNP not only reduces forecast errors but also slows down the growth rate of forecast errors. Other models do not exhibit the same trend, further highlighting the superior characteristics of the initial states produced by FNP.

### Generalization to Observational Information Reconstruction

Theoretically, the functional representation learned based on observational conditions can be directly decoded through MLPs and output reconstruction results for the observational information without fine-tuning. Therefore, we evaluated the reconstruction performance of different models in the absence of the background conditions, as shown in Table 2. Similarly, Adas pre-trained on data assimilation task, cannot be directly used for information reconstruction. Hence, the table only

Figure 2: Visualization of assimilation results by different models for q700. The visualization date-time is randomly selected at 2018-04-02 06:00 UTC. The first row shows the ERA5 (ground truth), background, background error and observations with \(0.25^{}\) resolution. Other rows show the assimilation results of different models.

[MISSING_PAGE_FAIL:9]

information they can provide becomes less, so it is reasonable to observe an increase in analysis error compared to Table 1. Similarly, _FNP achieves SOTA performance in terms of overall MSE, MAE, and WRMSE for the majority of variables_.

## 5 Conclusions

In summary, we present FNP that can assimilate observations with arbitry resolution. The outstanding performance and out-of-domain generalization of FNP in data assimilation and observational information reconstruction demonstrate its significant potential and broad application prospects. It not only contributes to the field of data assimilation but also makes meaningful explorations for AI-based end-to-end weather forecasting systems. Technically and theoretically, this methodology can also be applied to more tasks and scenarios, such as downscaling , station-scale state estimation and weather prediction [36; 60; 24].

Our work has certain limitations. Firstly, the observational data used in our experiments are generated through simulations rather than real-world observations. This may lead to differences in model performance when applied in actual scenarios, thus discounting its value for practical application. In fact, due to the complex and diverse nature of real observational data, the data assimilation community lacks relevant benchmarks and large-scale datasets. The establishment of such benchmarks and datasets would be a highly meaningful endeavor, enabling fair comparisons among different models and fostering rapid advancements in the field. Secondly, FNP inherently performs 3D data assimilation without the temporal dimension. Considering the flexibility of the FNP architecture, incorporating the temporal dimension is not challenging and is expected to produce additional benefits. In the future, we will further explore the broader possibilities of data assimilation and end-to-end weather forecasting.