# WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia

Yufang Hou\({}^{1,2}\), Alessandra Pascale\({}^{1}\), Javier Carnerero-Cano\({}^{1}\), Tigran Tchrakian\({}^{1}\)

**Radu Marinescu\({}^{1}\)**, Elizabeth Daly\({}^{1}\), Inkit Padhi\({}^{3}\), Prasanna Sattigeri\({}^{3}\)

\({}^{1}\) IBM Research Europe - Ireland

\({}^{2}\) IT:U Interdisciplinary Transformation University Austria

\({}^{3}\) IBM Research, Thomas J. Watson Research Center, Yorktown Heights, USA

{yhou|apascale|tigran|radu.marinescu|elizabeth.daly}@ie.ibm.com

{javier.cano|inkpad}@ibm.com,psattig@us.ibm.com

###### Abstract

Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess the performance of LLMs in providing a complete perspective on conflicts from the retrieved documents, rather than choosing one answer over another, when augmented with retrieved passages containing real-world knowledge conflicts. We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances. To facilitate future work, we release WikiContradict at https://ibm.biz/wikicontradict.

## 1 Introduction

The advent of large language models (LLMs) [Brown et al., 2020] has revolutionized the field of Natural Language Processing (NLP), enabling unprecedented capabilities in text understanding and generation. However, static LLMs often suffer from outdated information and hallucinations. To mitigate these shortcomings, retrieval-augmented generation (RAG) techniques [Lewis et al., 2020] have been developed, which combine the strengths of LLMs with retrieved up-to-date information from external sources. While RAG frameworks have shown significant promise, it remains unclear how LLMs handle knowledge conflicts from different sources, including "_context-memory conflicts_", which refers to the retrieved context knowledge being in conflict with the parametric knowledge (memory) encapsulated within the LLM's parameters, and "_inter-context conflicts_", which refers to thecontradictions among the retrieved passages . Most prior research on LLM knowledge conflicts has concentrated on "_context-memory conflicts_" and relied on artificially generated datasets, which employ various methods to create conflicting information. These approaches span from simple entity substitution, where an entity in a passage is replaced with another entity of the same type , to more sophisticated techniques, such as instructing language models like ChatGPT to fabricate supporting evidence for counterfactual answers to a given question . However, these artificially generated datasets primarily focus on explicit, surface-level contradictions, neglecting the complexity and nuance of real-world knowledge conflicts.

In this work, we focus on investigating the behaviors of LLMs when confronted with _"real-world inter-context conflicts_", where knowledge inconsistencies arise from the same or different retrieved passages that originate from a single trusted source (Wikipedia) and are considered equally credible. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances that cover different types of contradictions identified by Wikipedia editors and validated by us. Figure 1 presents two illustrative instances that demonstrate different types of contradictions. In particular, Example 1 requires implicit reasoning to detect the contradiction between Passage 1 and Passage 2 about the number of survivors from the RMS Lusitania sinking event, which requires calculating the number of survivors by subtracting 1,195 from 1,959 based on the information provided in Passage 2. This type of instance accounts for 36% of the instances in the WikiContradict dataset.

We evaluate the performance of various LLMs on WikiContradict by employing diverse prompt templates to assess their behaviour under different question answering (QA) scenarios, including RAG with a single context passage, and RAG with two contradictory passages. Our primary focus is on evaluating the ability of LLMs to provide a comprehensive and balanced perspective on conflicts by synthesizing information from the retrieved documents, rather than simply selecting one answer over another in scenarios where contradictory information is present. We then conduct a rigorous human evaluation to assess the correctness of the models' responses. Our human evaluation dataset comprises responses from 5 LLMs to 5 prompt templates, applied to 55 instances from the WikiContradict dataset, resulting in a total of 1,375 evaluation samples. Each sample is annotated by 2 authors of this paper, yielding 2,750 human judgements. The inter-annotator agreement, measured by Cohen's \(\), ranges from 0.58 to 0.88 across different prompt templates, indicating moderate to substantial agreement. After resolving the annotation disagreements among annotators, our final human evaluation study dataset (WikiContradict_HumanEval) consists of 1,200 samples resulting from 5 LLMs' responses to 48 WikiContradict instances based on 5 prompt templates.

On WikiContradict_HumanEval, we observe that when instructing LLMs to generate answers to a given question based on the given context consisting of two contradicted passages, all models, including GPT-4, struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts that require reasoning as illustrated in Figure 1, Example 1. Interestingly, we find that prompting LLMs to pay attention to contradictory context information improves their performance to correctly answering these questions. For instance, the top-performing model, LLama-3-70b-instruct, shows a remarkable increase from 10.4% to 43.8%. Furthermore, our analysis reveals that this improvement is largely driven by instances with explicit conflicts, as illustrated in Figure 1, Example 2. Finally, to facilitate future evaluations, we have developed

Figure 1: Example instances from WikiContradict with different contradiction types.

WikiContradictEval, a simple automatic evaluation method that leverages few-shot in-context learning to teach Llama-3-70b-instruct to judge model responses, which achieves an F-score of 0.8 on WikiContradict_HumanEval for evaluating LLM responses in the RAG setting with two contradictory passages.

In summary, our proposed WikiContradict benchmark poses a significant challenge for current LLMs, highlighting substantial opportunities for future improvement. We believe WikiContradict can serve as a valuable resource for the research community, facilitating the examination and tracking of LLMs' progress in handling real-world inter-context conflicts and deepening our understanding of their capabilities in these complex settings.

## 2 Related work

Knowledge conflictsKnowledge conflicts are commonly presented to LLMs and exploring the capability of the model to understand and manage them to ensure trustworthiness of the answer is gaining increasing interest in the community. While there exist three categories of conflicts: intra-memory, context-memory and inter-context (Xu et al., 2024) we focus our attention on the inter-context conflict. This type of contradiction has become of particular interest after the advent of RAG techniques. RAG has been proven to enhance LLMs' capabilities in dealing with hallucination and enrich LLMs' responses by integrating content from retrieved documents into the context (Lewis et al., 2021). At the same time RAG can also introduce inconsistencies, as external documents may conflict with each other. In order to explore this phenomenon, previous research has relied on synthetically generated datasets containing conflicting statements (Chen et al., 2022; Wang et al., 2023; Li et al., 2024). While they are used to evaluate and fine-tune existing models, these benchmarks fail to represent the complexity of real world conflicts (Xu et al., 2024). We aim to shed light on the unexplored space of managing inter-context conflicts _in the wild_, starting from contradictions extracted and annotated from Wikipedia. Our aim is to assess how well LLMs perform in dealing with real-world scenarios, rather than with synthetically created conflicts, to better understand their behaviour and capability.

LLMs evaluation benchmarksUnderstanding adherence of LLMs to factual knowledge has gained increasing attention in recent years given the widespread use of these models. Hallucination detection and mitigation has been identified as a fundamental step to ensure transparency and trustworthiness of the models. In recent years a proliferation of benchmarks for evaluating factuality of LLMs has been observed with (Huang et al., 2023) presenting a survey of existing hallucination detection and mitigation approaches. They include many established benchmarks such as TruthfulQA (Lin et al., 2022), FreshQA (Vu et al., 2023), HaluEval (Li et al., 2023), HalluQA (Cheng et al., 2023), and FELM (Chen et al., 2023) that mainly focus on short-form answer evaluation where the knowledge of the LLM is tested in the form of a single factoid evaluated binary as true or false in adherence to the specific benchmark. More recent works (Wei et al., 2024; Min et al., 2023) cover long-form answer evaluation where the answer is decomposed into individual facts that are then independently evaluated. An ensemble metric is computed at the end to represent the overall evaluation score.

These previous works include the primary definition of truthfulness and factuality as a binary concept where the goal is to test the knowledge of the LLM in the form of a single (short-form) or multiple (long-form) factoids evaluated as **true or false** given the specific benchmark.

In this work we move away from a dualistic vision of the truth and we focus on cases where the answer to a question is not unique. We investigate how LLMs deal with real-world conflicting information where there exist different sources and possible answers considered equally trustworthy.

## 3 WikiContradict

In this section, we describe the process of leveraging contradiction tags from Wikipedia to develop WikiContradict, a QA-based benchmark consisting of 253 human-annotated instances that cover different types of real-world knowledge conflicts.

### Data collection and processing

Although Wikipedia is widely regarded as a high-quality pre-training dataset for most LLMs, its content is not without flaws, including speculation, inconsistencies, and other content issues. To address these problems, Wikipedia editors use a wide range of maintenance tags to flag problematic content for improvement. However, these maintenance tags are typically removed when creating Wikipedia datasets for LLM pre-training, which results in content with various quality issues being included in the pre-training process.

In this work, we focus on three tags that indicate content inconsistencies: _inconsistent_, _self-contradictory_, and _contradict-other_. The first two tags denote contradictory statements within the same article, whereas the third tag highlights instances where the content of one article contradicts that of another article. In total, we collect around 1,200 articles that contain these tags through the Wikipedia maintenance category "_Wikipedia articles with content issues_"1. The upper portion of Figure 2 illustrates a Wikipedia article that has been flagged by an "_inconsistent_" tag with the comment "_contradictory number of monks_". The tagged paragraphs from these articles, together with the editors' comments whenever available, serve as the starting point for our annotations.

### Data annotation

We developed an annotation interface using Label Studio2, as shown in the lower portion of Figure 2. Given a content inconsistency tag provided by Wikipedia editors, our annotators first need to verify whether the tag is valid by checking the relevant article content, the editor's comment, as well as the information in the edit history and the article's talk page if necessary.

For the verified valid tags, we instruct annotators to extract the two contradictory paragraphs/sentences from the original article(s), slightly modifying them as needed to create stand-alone passages. Such modifications normally require the annotators to resolve anaphoric references (e.g., _She_) in the first

Figure 2: WikiContradict data annotation pipeline.

sentence of each passage. Next, the annotators should provide a brief explanatory text to clarify the contradictions between the two passages and categorize them using a pre-defined taxonomy, such as _Date_, _Number_, _Explicit_, _Implicit_. Briefly speaking, our contradiction taxonomy consists of four dimensions, which provide a comprehensive framework for categorizing and analyzing contradictions:

**Semantic Type (I)**: This dimension focuses on the fine-grained semantics of the contradiction, specifically the type of entity involved. We adapt OntoNotes named entity type definitions to describe the different types of contradicted entities, such as Date/Time, Location, Number, and others.

**Modality (II)**: This dimension examines the modality of the contradiction, describing the source of the information in both passages. This includes whether the information comes from a piece of text, a table, an infobox, or other sources.

**Origin (III)**: This dimension determines whether the contradictions originate from the same Wikipedia article or from different articles.

**Reasoning Type (IV)**: This dimension focuses on the reasoning aspect of the contradiction, distinguishing between explicit contradictions (where the contradiction is clearly stated) and implicit contradictions (where the contradiction is implied or requires inference).

Finally, the annotators must craft at least one question that effectively highlights the contradictions between the two passages, eliciting different answers depending on which passage is referenced. The two examples from Figure 1 illustrate our annotation results. On average, annotators spent around 30 minutes to annotate a tag; longer times are required to annotate tags related to implicit conflicts, especially for cases in which the reasons of inconsistency are not explicitly mentioned in the comments from Wikipedia editors. More details about the pre-defined contradiction taxonomy and the full annotation guideline can be found in Appendix A.

### Data statistics

Using the annotation pipeline outlined in the previous section, the authors annotated the collected Wikipedia articles marked with inconsistency tags, yielding a total of 253 annotated instances. Each instance comprises a question, two contradictory passages, and two distinct answers, each derived from one of the passages. Table 1 shows an overview of the dataset statistics. Notably, among all annotated instances, approximately 61% of questions are categorized as wh-questions seeking specific information. Furthermore, a significant proportion of instances (36%) contain implicit contradictions.

### Evaluation

To investigate how LLMs respond to real-world inter-context conflicts, we develop five prompt templates to evaluate their performance under different question-answering (QA) scenarios. As illustrated in Figure 3, for each annotated instance from the WikiContradict dataset, we generate five question prompts based on these pre-defined templates. Specifically, template 1 evaluates a model's internal knowledge, while templates 2 and 3 examine its performance in the RAG setting with a single retrieved passage. Templates 4 and 5, on the other hand, assess a model's ability to handle QA in the RAG setting with two contradictory passages that can lead to different answers.

To evaluate LLMs' responses to these question prompts, we follow the relaxed evaluation mode from FreshLLM (Vu et al., 2023) by allowing additional hallucinated or inaccurate information as long as the primary answer is accurate and any additional information does not contradict with the primary answer. More specifically, each response is evaluated as "_correct_", "_partially correct_", or "_incorrect_":

\(\) "_Correct_" if the response accurately matches all the answers in the annotated answer list. For prompt templates 4 and 5, the response should identify and contain the contradictory answers that reflect the heterogeneous nature of the context. Additionally, the correct response should not

   Wikipedia tags & 261 \\ Verified valid tags & 130 \\  Annotated instances & 253 \\   \\  Yes/No questions & 133 (53\%) \\ Wh-questions & 120 (47\%) \\   \\  Explicit & 161 (64\%) \\ Implicit & 92 (36\%) \\   

Table 1: WikiContradict dataset.

indicate a preference for one answer over another, and it should not combine two different correct answers without indicating the contradictory nature of these answers.
* "_Partially correct_" applies to prompt templates 1, 4, and 5; it means that the response only matches one of the answers in the annotated answer list, or the response matches all the answers in the correct answer list but indicates a preference for one answer over another.
* "_Incorrect_" if the response does not match any of the annotated answers, or the response merely combines two contradictory answers from the annotated answer list and indicates that both are possible at the same time without indicating the contradictory nature of the two context passages.

Following the criteria described above, for the question from Example 2 of Figure 1, an LLM response to prompt template 4, "_According to the context, the number of monks who know the secret recipe of Chartrueise is either three or two. The first statement suggests that three monks know the recipe, while the second statement contradicts this, stating that only two monks at Grande Chartrueue know the secret recipe._" is a _correct_ response. In contrast, the LLM response to prompt template 3, "_Two monks know the secret recipe of Chartrueise._" is _partially correct_, as it only captures one aspect of the contradictory information presented in the context.

## 4 Human evaluation: LLMs are struggling on WikiContradict

### Main evaluation with inter-context conflicts

To understand LLMs' behavior when faced with real-world inter-context conflicts, we use WikiContradict to benchmark a list of LLMs with the evaluation protocol described in Section 3.43. The authors independently evaluated a subset of answers, comprising 1,375 responses from five LLMs based on the five prompt templates, as shown in Figure 3, for 55 instances. Each response is assessed by two authors of this paper, yielding a total of 2,750 human judgements. The inter-annotator agreements, as measured by Cohen's kappa \(\), were moderate to substantial, with values of 0.58 for prompt template 3, 0.67 for prompt template 4, 0.84 for prompt template 0, 0.85 for prompt template 2, and 0.88 for prompt template 1. After resolving the annotation disagreements among annotators,

Figure 3: WikiContradict evaluation.

our final human evaluation study dataset (WikiContradict_HumanEval) consists of 1,200 samples resulting from five LLMs' responses to 48 WikiContradict instances based on five prompt templates4.

Table 2 presents the results of WikiContradict_HumanEval for 5 LLMs: _Mistral-7b-instruct_, _Mistral-8x7b-instruct_, _Llama2-2-70b-chart_, _Llama3-70b-instruct_, and _GPT-4-turbo-2024-04-09_. The table provides a detailed breakdown of response accuracy for each LLM, categorized into three types: _correct_, _partially correct_ (applicable to prompt templates 1, 4, and 5), and _incorrect_. We further distinguish between instances with explicit conflicts (30 instances) and implicit conflicts (18 instances) to provide a more nuanced understanding of the LLMs' performance. Below we summarise a few key observations on WikiContradict_HumanEval.

**Prompt template 1: There is a significant overlap between the internal knowledge of LLMs and the content of Wikipedia.** As expected, the portion of correct or partially correct answers based on their internal knowledge, ranges from 37.5% (_Mistral-7b-inst_) to 60.4% (_GPT-4_).

**Prompt template 2 and 3: When tasked with answering questions based on a provided context, LLMs are generally capable of generating correct responses for the majority of instances, as long as the context does not contain conflicting information**. However, we observe that _GPT-4_ exhibits a "stubborn" behavior, particularly with prompt template 3. It often relies on its internal knowledge, which may not align with the given context, resulting in a lower accuracy of 87.8% compared to _Mistral-8x7b-inst_ and _Llama3-3-70b-inst_, which perform better in this scenario.

**Prompts template 4 and 5: LLMs often struggle to provide correct answers when the context contains conflicting information.** Typically, the models rely on a single passage to inform their response, disregarding the other passage. Notably, some models attempt to reconcile the conflicting information by providing both answers, but then proceed to explain why one of them is incorrect. This phenomenon is particularly pronounced in the _Mistral-7b-inst_ model.

**Prompts template 4 vs. 5: LLMs can improve their performance in providing correct answers when explicitly instructed to consider conflicting information within the given context.** Notably, _Llama-3-70b-inst_ exhibits the most substantial improvement, jumping from 10.4% to 43.8%. In contrast, _GPT-4_ demonstrates the smallest improvement, increasing from 6.3% to 10.4%, which is likely attributed to its previously observed stubborn behavior in prompt template 3.

   &  &  &  &  \\   & **all** & **exp** & **imp** & **all** & **exp** & **imp** & **all** & **exp** & **imp** & **all** & **exp** & **imp** & **all** & **exp** & **imp** \\   & & & & & & & & & & & & & & & \\  C & **4.2** & **6.7** & 0.0 & 2.1 & 0.0 & 5.9 & 0.0 & 0.0 & 0.0 & **4.2** & 0.0 & **11.8** & 2.1 & 0.0 & 5.9 \\  PC & 33.3 & 23.3 & 47.1 & 52.1 & 43.3 & 64.7 & 54.2 & 43.3 & 70.6 & 52.1 & 46.7 & 58.8 & 58.3 & 53.3 & 64.7 \\  IC & 62.5 & 70.0 & 52.9 & 45.8 & 56.7 & 29.4 & 45.8 & 56.7 & 29.4 & 43.8 & 53.3 & 29.4 & 39.6 & 46.7 & 29.4 \\   & & & & & & & & & & & & & & & \\  C & 92.7 & - & - & **97.6** & - & - & 87.8 & - & - & 95.1 & - & - & **97.6** & - & - \\  IC & 7.3 & - & - & 2.4 & - & - & 12.2 & - & - & 4.9 & - & - & 2.4 & - & - \\   & & & & & & & & & & & & & & & & \\  C & 82.9 & - & - & **92.7** & - & - & 90.2 & - & - & **92.7** & - & - & 87.8 & - & - \\  IC & 17.1 & - & - & 7.3 & - & - & 9.8 & - & - & 7.3 & - & - & 12.2 & - & - \\   & & & & & & & & & & & & & & & & \\  C & 2.1 & 3.3 & 0.0 & 4.2 & 3.3 & 5.9 & 4.2 & 3.3 & 5.9 & **10.4** & **13.3** & 5.9 & 6.3 & 3.3 & **11.8** \\  PC & 87.5 & 86.7 & 88.2 & 91.7 & 93.3 & 88.2 & 93.8 & 96.7 & 88.2 & 81.3 & 80.0 & 82.4 & 85.4 & 96.7 & 64.7 \\  IC & 10.4 & 10.0 & 11.8 & 4.2 & 3.3 & 5.9 & 2.1 & 0.0 & 5.9 & 8.3 & 6.7 & 11.8 & 8.3 & 0.0 & 23.5 \\   & & & & & & & & & & & & & & & \\  C & 20.8 & 26.7 & 11.8 & 14.6 & 16.7 & 11.8 & 22.9 & 26.7 & **17.6** & **43.8** & **60.0** & **17.6** & 10.4 & 10.0 & 11.8 \\  PC & 70.8 & 63.3 & 82.4 & 83.3 & 83.3 & 82.4 & 68.8 & 63.3 & 76.5 & 45.8 & 26.7 & 76.5 & 81.3 & 90.0 & 64.7 \\  IC & 8.3 & 10.0 & 5.9 & 2.1 & 0.0 & 5.9 & 8.3 & 10.0 & 5.9 & 10.4 & 13.3 & 5.9 & 8.3 & 0.0 & 23.5 \\  

Table 2: Human evaluation results on WikiContradict_HumanEval. “C”, “PC” and “IC” stand for “_Correct_”, “_Partially correct_”, “_Incorrect_”, respectively. “all”, “exp_”, and “imp” represent for instance types: all instances, instances with explicit conflicts, and instances with implicit conflicts. The numbers represent the ratio of responses from each LLM that were assessed as “_Correct_, “_Partially correct_, or “_Incorrect_ for each instance type under a prompt template. The bold numbers highlight the best models that correctly answer questions for each type and prompt template.

**Explicit conflicts vs. implicit conflicts: When explicitly instructed to consider conflicting information within the given context, LLMs' performance in providing correct answers improves in particular in cases where conflicts are explicitly stated.** For instance, for _Llama-3-70b-inst_, the performance on explicit conflicts instances jumps from 13.3% to 60.0%, while the performance on implicit conflicts instances improves from 5.9% to 17.6%.

**More insights on errors for partially correct and incorrect:** In the RAG setup (Table 2, Prompt Template 2 - 5), models' answers rarely fall into the "_incorrect_" category. When such cases happen, the model's answer acknowledge coexistence of two facts which are not logically correct, such as stating a person was born on two dates (e.g., _According to the provided context, Paul McCole was born on 1 February 1972 and 10 February 1972_). In contrast, when instructed to answer questions based on their internal knowledge without providing any context (Table 2, Prompt Template 1), the ratio of "_incorrect_" answers increases significantly. This is likely due to the fact that LLMs either memorize a different answer from another source other than Wikipedia during pre-training or hallucinate the answer. Regarding partially correct answers, most LLMs tend to produce a single answer based on only one given context, neglecting the other. However, some models, such as _Mistral-7b-inst_, attempt to reconcile the conflicting information by providing both answers and then explaining why one of them is incorrect. This phenomenon is particularly pronounced in the _Mistral-7b-inst_ model.

### Additional evaluation on the control setups

We conducted additional human evaluation studies on 48 instances from WikiContradict_HumanEval using four variations of prompt template 5: prompt template 5.1 swaps the positions of passage 1 and passage 2 from the original template 5; prompt template 5.2 instructs LLMs to identify any contradictions in the given context with respect to the question; prompt template 5.3 provides LLMs with manually revised passage 1 and passage 2, where contradictions with respect to the question were resolved; prompt template 5.4 tasks LLMs with detecting any contradictions in the revised consistent context from template 5.3 with respect to the question. In this experiment, each response is assessed by a single human annotator. Please refer to Appendix B for more details on the prompt templates 5.1 - 5.4. Table 3 presents the results of the human evaluation of five LLMs on five prompt templates (5, 5.1, 5.2, 5.3, and 5.4), yielding the following key insights:

   &  &  &  &  &  \\   & **all** & **exp** & **imp** & **all** & **exp** & **imp** & **all** & **exp** & **imp** & **all** & **exp** & **imp** & **all** & **exp** & **imp** \\   \\  C & 20.8 & 26.7 & 11.8 & 14.6 & 16.7 & 11.8 & 22.9 & 26.7 & **17.6** & **43.8** & **60.0** & **17.6** & 10.4 & 10.0 & 11.8 \\  PC & 70.8 & 63.3 & 82.4 & 83.3 & 83.3 & 82.4 & 68.8 & 63.3 & 76.5 & 45.8 & 26.7 & 76.5 & 81.3 & 90.0 & 64.7 \\  IC & 8.3 & 10.0 & 5.9 & 2.1 & 0.0 & 5.9 & 8.3 & 10.0 & 5.9 & 10.4 & 13.3 & 5.9 & 8.3 & 0.0 & 23.5 \\   \\  C & 22.9 & 30.0 & 11.8 & 14.6 & 16.7 & 5.9 & 17.0 & 27.6 & 0.0 & **54.2** & **70.0** & **23.5** & 12.5 & 16.7 & 5.9 \\  PC & 72.9 & 66.7 & 82.4 & 70.8 & 73.3 & 70.6 & 80.9 & 72.4 & 94.1 & 45.8 & 30.0 & 76.5 & 81.3 & 83.3 & 76.5 \\  IC & 4.2 & 3.3 & 5.9 & 14.6 & 10.0 & 23.5 & 2.1 & 0.0 & 5.9 & 0.0 & 0.0 & 0.0 & 6.3 & 0.0 & 17.6 \\   \\  C & 35.4 & 40.0 & 29.4 & 4.2 & 6.7 & 0.0 & 76.6 & 89.7 & 52.9 & 77.1 & 93.3 & 47.1 & **89.6** & **96.7** & **76.5** \\  PC & 14.6 & 20.0 & 0.0 & 6.3 & 6.7 & 5.9 & 17.0 & 10.3 & 29.4 & 6.3 & 3.3 & 11.8 & 2.1 & 3.3 & 0.0 \\  IC & 50.0 & 40.0 & 70.6 & 72.9 & 76.7 & 70.6 & 6.4 & 0.0 & 17.6 & 14.6 & 3.3 & 35.3 & 8.3 & 0.0 & 23.5 \\   \\  C & 87.5 & 93.3 & 82.4 & 85.4 & 83.3 & **88.2** & 87.2 & 96.6 & 76.5 & **89.6** & **96.7** & 82.4 & 87.5 & **96.7** & 70.6 \\  PC & 6.3 & 3.3 & 5.9 & 4.2 & 6.7 & 0.0 & 2.1 & 0.0 & 5.9 & 2.1 & 0.0 & 5.9 & 2.1 & 0.0 & 5.9 \\  IC & 6.3 & 3.3 & 11.8 & 10.4 & 10.0 & 11.8 & 10.6 & 3.4 & 17.6 & 8.3 & 3.3 & 11.8 & 10.4 & 3.3 & 23.5 \\   \\  C & 66.7 & 70.0 & 64.7 & **83.3** & **83.3** & **82.4** & 12.8 & 6.9 & 23.5 & 47.9 & 50.0 & 41.2 & 43.8 & 50.0 & 35.3 \\  PC & 4.2 & 6.7 & 0.0 & 2.1 & 3.3 & 0.0 & 23.4 & 24.1 & 17.6 & 4.2 & 3.3 & 5.9 & 14.6 & 10.0 & 23.5 \\  IC & 29.2 & 23.3 & 35.3 & 2.1 & 3.3 & 0.0 & 63.8 & 69.0 & 58.8 & 45.8 & 43.3 & 52.9 & 41.7 & 40.0 & 41.2 \\  

Table 3: Additional human evaluation results on WikiContradict_HumanEval_Control: “C”, “PC” and “IC” stand for “_Correct_”, “_Partially correct_”, “_Intercorrect_”, respectively. “all”, “exp”, and “imp” represent for instance types: all instances, instances with explicit conflicts, and instances with implicit conflicts. The numbers represent the ratio of responses from each LLM that were assessed as “_Correct_, “_Partially correct_, or “_Incorrect_ for each instance type under a prompt template. The bold numbers highlight the best models that correctly answer questions for each type and prompt template.

   &  &  \\   & _Correct_ & _Partially correct_ & _Incorrect_ & _Correct_ & _Partially correct_ & _Incorrect_ \\  Mistral-7b-inst & 20.8 & 70.8 & 8.3 & 39.6 & 54.2 & 6.2 \\  Mistral-8x7b-inst & 14.6 & 83.3 & 2.1 & 16.7 & 77.1 & 6.2 \\  Llama-2-70b-chat & 22.9 & 68.8 & 8.3 & 22.9 & 64.6 & 12.5 \\  Llama-3-70b-inst & 43.8 & 45.8 & 10.4 & 43.8 & 41.7 & 14.6 \\  GPT-4 & 10.4 & 81.3 & 8.3 & 8.3 & 81.2 & 10.4 \\  

Table 5: Comparing human judgement with Llama3-70b-instruct judgement for prompt template 5 for five testing LLMs on WikiContradict_HumanEval.

   &  & _Partially correct_ & _Incorrect_ \\  & **Acc** & **Macro-F** & **(P/NR)** & **(P/NR)** & **(P/R/F)** \\  Mistral-8x7b-inst & 26.2 & 19.7 & 10.5 / 22.0 / 14.2 & 66.2 / 30.0 / 41.3 & 2.9 / 5.0 / 3.7 \\  Llama-3-70b-inst & 85.4 & 74.4 & 72.6 / 90.0 / 80.4 & 96.1 / 87.6 / 91.7 & 47.8 / 55.0 / 51.2 \\  GPT-4 & **86.7** & **76.1** & **73.4 / 94.0 / 82.5** & **96.8 / 88.2 / 92.3** & **52.4 / 55.0 / 53.7** \\  GPT-4o & 83.3 & 71.3 & 74.5 / 76.0 / 75.2 & 94.9 / 88.2 / 91.5 & 38.7 / 60.0 / 47.1 \\  

Table 4: Judge LLM results for prompt template 5 on WikiContradict_HumanEval. GTP-4 and GPT-4o represent “gpt-4-turbo-2024-04-09” and “gpt-4o-2024-05-13”, respectively.

are the top performers in correctly answering questions, with correct response rates of 50.6% and 45.8%, respectively. It is noteworthy that, based on the above analysis, there is a high probability that the judge LLM overestimates the performance of _Mistral-7b-inst_, suggesting a potential bias in its evaluation. In addition, all models except Flan-ul2 have higher correct response rates for instances with explicit contradictions compared to instances with implicit contradictions, which is aligned with the observation in our human evaluation study (Section 4).

## 6 Limitation and future work

We acknowledge several limitations of the current benchmark. Firstly, the benchmark presented in this paper is restricted to English language instances, which may not generalize to other languages due to the nuanced nature of contradictory statements. Secondly, the reliance on Wikipedia contradictory tags may introduce bias towards specific types of contradictory statements, limiting the benchmark's representativeness. Future work can leverage automatic methods to detect contradictory statements from Wikipedia (e.g., ) to expand the dataset. Third, the benchmark only covers text-text contradictions, whereas contradictions can occur across modalities, such as between text and images, as demonstrated by some Wikipedia contradictory tags that capture discrepancies between textual descriptions and visual content. Future work will aim to incorporate coverage across different modalities. Lastly, the proper handling of contradictions remains a challenging question. While prior studies have utilized contradictions to detect misinformation or disinformation , our current approach outlines the contradiction in the response, assuming all retrieved passages come from the same credible resource. An alternative approach would be to cross-check each answer separately with other credible resources and provide a confidence score for the correctness of each individual answer.

## 7 Conclusion

Unlike most previous work on LLM knowledge conflicts within RAG frameworks, which focus on "_context-memory conflicts_", our focus is on "_real-world inter-context conflicts_". Within this setting we introduced the WikiContradict benchmark, which consists of 253 human-annotated instances covering different types of contradictions identified by Wikipedia editors. Our annotation of these instances, which includes isolation of relevant conflicting passages, resolution of anaphora and the creation of at least one question that highlights the contradictions, results in a benchmark that can effectively evaluate the capacity of LLMs to manage and reason over knowledge conflicts. This capacity was highlighted by the results of the evaluation of LLMs on the benchmark, in which for each contradictory instance, the LLMs were given different prompts, each one containing a different instruction and/or different conflicting information contained within the instance. Our experiments show that LLMs often struggle to correctly identify and manage real-world inter-context conflicts, indicating the usefulness of the benchmark and the need for further research in this direction.

   &  &  &  \\   & \(C\) & \(PC\) & \(IC\) & \(C\) & \(PC\) & \(IC\) & \(C\) & \(PC\) & \(IC\) \\   & 50.6 & 39.9 & 5.9 & 57.8 & 36 & 3.1 & 38.0 & 46.7 & 10.9 \\   & 45.8 & 38.7 & 13.8 & 55.9 & 28.6 & 14.3 & 28.3 & 56.5 & 13.0 \\   & 37.9 & 53 & 6.7 & 47.8 & 46.6 & 5.0 & 20.7 & 64.1 & 9.8 \\   & 37.9 & 52.2 & 6.3 & 43.5 & 48.4 & 3.7 & 28.3 & 58.7 & 10.9 \\   & 15.0 & 73.5 & 11.5 & 19.3 & 70.8 & 9.9 & 7.6 & 78.3 & 14.1 \\   & 11.5 & 73.5 & 9.5 & 12.4 & 74.5 & 7.5 & 9.8 & 71.7 & 13.0 \\   & 1.2 & 90.1 & 7.9 & 0 & 92.5 & 6.8 & 3.3 & 85.9 & 9.8 \\  

Table 6: _Llama-3-70b_ judge results on WikiContradict based on prompt template 5. “C”, “PC”, and “IC” stand for “_Correct_”, “_Partially correct_”, “_Incorrect_”, respectively.