# Self-Taught Recognizer: Toward Unsupervised Adaptation for Speech Foundation Models

Yuchen Hu\({}^{1,}\) Chen Chen\({}^{1,}\) Chao-Han Huck Yang\({}^{2}\) Chengwei Qin\({}^{1}\)

Pin-Yu Chen\({}^{3}\) Eng Siong Chng\({}^{1}\) Chao Zhang\({}^{4}\)

\({}^{1}\)Nanyang Technological University \({}^{2}\)NVIDIA Research

\({}^{3}\)IBM Research \({}^{4}\)Tsinghua University

{yuchen005, chen1436}@e.ntu.edu.sg, hucky@nvidia.com

###### Abstract

We propose an unsupervised adaptation framework, Self-TAught Recognizer (STAR), which leverages unlabeled data to enhance the robustness of automatic speech recognition (ASR) systems in diverse target domains, such as noise and accents. STAR is developed for prevalent speech foundation models based on Transformer-related architecture with auto-regressive decoding (e.g., Whisper, Canary; SeamlessM4T). Specifically, we propose a novel indicator that empirically integrates step-wise information during decoding to assess the token-level quality of pseudo labels **without** ground truth, thereby guiding model updates for effective unsupervised adaptation. Experimental results show that STAR achieves an average of 13.5% relative reduction in word error rate across 14 target domains, and it sometimes even approaches the upper-bound performance of supervised adaptation. Meanwhile, we observe that STAR prevents the adapted model from the catastrophic forgetting problem without recalling source-domain data. Furthermore, STAR exhibits high _data efficiency_ that only requires less than one-hour unlabeled data, and seamless _generality_ to alternative large speech models in recognition and translation tasks. Our code is publicly available at: https://github.com/YUCHEN005/STAR-Adapt.

+
Footnote †: dagger\) Equal Contribution.

## 1 Introduction

Human speech, characterized by its inherent acoustic nuances  and variability across speakers , is further complicated by the diverse and unpredictable environments. These factors contribute to significant domain distinctions in the speech signal, with differences in accent, speaking style, and background noise (visualized in Appendix B). Consequently, this diversity poses significant challenges in the field of automatic speech recognition (ASR), especially under diverse conditions .

In recent years, advancements in ASR technology  have been boosted, primarily by the use of deep neural models and supervised learning with high-quality datasets. In particular, end-to-end ASR models pre-trained on industry-scale datasets have been made publicly available to the research community, such as OpenAI Whisper , Meta SeamlessM4T  and NVIDIA Canary . Considering the high diversity of speech domains, even a well-trained ASR foundation model usually performs less satisfactorily when encountering a domain shift problem . This performance degradation stems from a critical dilemma: collecting and labelling sufficient training data in the target domain is immensely time-consuming and labour-intensive, thus hindering the domain adaptation process of ASR models. Some existing efforts  focus on leveraging labelled source domain and unlabeled target domain data to enhance the ASR performance, as shownin Fig. 1 (i). This solution is generally known as unsupervised domain adaptation (UDA)  and has been widely explored in both machine learning and speech processing communities.

In the context of the UDA problem in ASR, the human "self-directed" ability  when encountering an unfamiliar speech domain is first illustrated. Despite the unawareness of the ground truth labels of our heard speech, individuals can learn speech-to-text mapping from their self-directed transcriptions, particularly when they have high confidence (see Fig. 8). This learning mechanism has a parallel in machine learning, known as "self-training" , which typically involves two stages. First, a pre-trained model generates the pseudo labels on target-domain data. Then, these data with pseudo labels, along with the associated confidence levels, are used to adapt the model.

Meanwhile, unlike approaches in existing ASR literature, which often require the source data (data used to pre-train the ASR model in source domains) to achieve UDA , humans, as the gold standard of speech communication, can address UDA issues in ASR without requiring any source data. Considering the exhibited generality of the speech foundation models with Transformer-related architectures based on the attention mechanism, it is the opportune moment to centre the attention mechanism on addressing the _source-free_ UDA problem within the realm of ASR. Specifically, we study to adapt the pre-trained Whisper model using a small amount of unlabeled data from the target domain to become a domain-specific speech recognizer in different scenarios without using any source data, based on the process analogous to the human speech recognition as shown in Fig. 1 (ii). We hereby highlight the significant potential value that research on source-free UDA contributes to general ASR applications : (i) It circumvents the extensive computational resources by adapting the ASR models without using any source data. (ii) It can considerably improve ASR performance in the target domain using only a small amount of speech samples without ground-truth labels.

In this work, we propose a source-free UDA approach called Self-TAught **R**ecognizer (STAR), which aims to enhance the performance of speech foundation models in specific target domains with unlabeled data. Based on the typical self-training scheme , STAR delves deeply into a general issue: Given the absence of ground-truth labels, how do we assess the quality of pseudo labels for guiding self-training? Unlike humans who can intuitively gauge their confidence in listening, the decoding "confidence scores" from attention-based ASR models are typically approximated by the pseudo posterior probabilities from softmax function , which may be unreliable due to the well-known over-confident issue of softmax . Traditionally with HMM-based ASR, the confidence scores can be estimated based on lattice and confusion network data structures , which, however, are difficult to obtain effectively in the end-to-end ASR framework.

In pursuit of a better quality indicator, we explore the self-attention matrix obtained during auto-regressive decoding, as it is not only grounded on speech input but also focuses on linguistic acceptability . Specifically, we find the aggregated attention weights can be a more reliable indicator for measuring the quality of ASR-decoded tokens than the confidence scores. However, such an attentive score suffers from numerical instability, as recent findings  from linguistic perspectives, it is normal for equally correct words (e.g., prepositions and nouns) to receive different semantic roles in a text sentence. This leads to the sub-optimality of using attentive scores alone to guide the fine-tuning process. We first substantiate these observations experimentally and then, in our STAR method, propose a novel integration approach based on their distinct characteristics, resulting in a both stable and reliable STAR indicator. Finally, it is employed to guide the subsequent finetuning process in a re-weighting manner, making a specific form of instructive adaptation.

Figure 1: Illustration of unsupervised domain adaptation (UDA) and source-free UDA frameworks. (i) UDA problem. (ii) Source-free UDA by self-training. STAR works by selecting high-quality pseudo labels and guiding the ASR foundation model’s adaptation at the token level.

Our experiments evaluate the proposed STAR in various practical scenarios, including background noise, speaker accents, and specific scenarios (e.g., interviews and talks). Comprehensive results show the significant gains from STAR that enhances Whisper by an average of **13.5%** relative word error rate (WER) reduction across 14 target domains. On some corpora, unsupervised STAR even approaches the upper bound of supervised adaptation using real labels. We also surprisingly observe that with informed finetuning, STAR prevents the adapted models from the common catastrophic forgetting problem without recalling source-domain data. Furthermore, we demonstrate that STAR enjoys: (i) _remarkable data efficiency:_ it requires less than one hour of unlabeled data to adapt Whisper to its best performance on target domains; (ii) _seamless generality:_ it is applicable to many prevalent speech foundation models and can be easily extended to the speech translation task.

In general, our contributions are summarized as follows:

* We direct our focus on source-free UDA in ASR with as one setting closed to real-world applications, where only a pre-trained speech foundation model and unlabeled speech samples are required to adapt to specific target domains.
* We present a score-based self-training approach called STAR that includes a novel indicator to evaluate the pseudo-label quality and achieve informed finetuning, which significantly enhances the domain-specific capabilities of speech foundation models across a wide range of target domains, including noise, accent, and specific scenarios.
* Intensive experiments demonstrate that STAR effectively avoids the common catastrophic forgetting problem in adaptation. Our further analysis of data efficiency and generality shows its potential for real-world applications, such as incremental updates for voice assistant.

## 2 Related Work

**Unsupervised Domain Adaptation in ASR.** Since acquiring the ground truth speech transcriptions is often prohibitively expensive in the target domain, many existing efforts bootstrap from available out-of-domain data to build an improved target domain model [79; 59; 93]. Besides directly simulating the target domain speech [31; 7], adversarial learning is frequently utilized to learn invariant representations to mitigate domain shifts [33; 23], which is also applied for front-end speech enhancement . Meanwhile, teacher-student learning provides an alternative solution for efficient adaptation [50; 65]. These methods are also semi-supervised , since labels from the source domain are available. More recently, self-supervised pre-trained models (e.g. wav2vec2 ) have been used for pseudo-labelling to achieve unsupervised adaptation [44; 38].

**Source-free Unsupervised Domain Adaptation.** Given the potential presence of sensitive information in the source data , there is a high demand for source-free UDA methods that transfer a pre-trained source model to the unlabeled target domain without any source data [47; 66; 16]. As a long-discussed machine learning issue, the mainstream solutions include self-supervised knowledge distillation , contrastive learning , hidden structure mining , and uncertainty-guided adaptation . Considering the inherent uncertainty in ASR decoding, we focus on the latter category and briefly review some representative indicators of uncertainty. Recently, there are some works  suggesting measuring uncertainty by the predicted variance from Monte Carlo Dropout , utilizing aleatoric uncertainty by encouraging intra-domain consistency , performing pseudo-labeling denoising using soft label correction , and introducing self-entropy descent mechanism to find a threshold for pseudo-labeling . It is worth noting that, confidence estimation for ASR systems can be dated back for decades, starting by using lattices and confusion networks [18; 61] for HMM-based systems. Improved confidence estimation can be achieved by model-based approaches, such as conditional random fields , recurrent neural networks [40; 74] and graph neural networks . More recent efforts [60; 77] focus on predicting uncertainty for auto-regressive decoding of attention-based models, however, they have not been applied in the source-free UDA.

**Summary.** Given the large amount of data used to pre-train the speech foundation models, it is difficult to define the scope of its source domain and keep the source data for re-training. Therefore we believe it is necessary to directly adapt speech foundation models to target domains for UDA for speech tasks. The proposed STAR method aims to assess the quality of pseudo labels produced by the auto-regressive decoding process, which leads to an instructive and effective self-training process. Since STAR can remove the need for keeping and retraining with source data and considerably reduce the performance difference between using ground truth and pseudo labels for adaptation with target domain data samples, it has the potential to fulfil the goal of source-free UDA for the ASR task and achieve user-friendly deployment for real-world speech-based artificial intelligence products.

## 3 Methodology

### Problem Setup

**ASR Formulation.** An end-to-end ASR system relies on a neural model \(f\) to recognize the input speech \(x^{T}\) into the corresponding text transcription \(y^{L}\), where \(T\) and \(L\) denote the lengths of the input waveform and output text sequences respectively. During training, the model \(f\) is optimized by teacher-forcing  with cross-entropy loss:

\[_{}(x,y)=_{l=1}^{L}-_{}(y_{l}|y_ {l-1},,y_{1},x),\] (1)

where \(y_{1:L}\) denotes the tokens in ground-truth labels \(y\), and \(\) denotes the trainable parameters in \(f\).

**UDA Setting.** Given a source ASR model \(f^{(s)}\) trained on labelled source domain data \(\{^{(s)},^{(s)}\}^{(s)}\), domain adaption in ASR aims to transfer the learned knowledge and obtain a model \(f^{(t)}\) that performs well on target domain \(^{(t)}\), i.e., \(f^{(t)}:^{(t)}^{(t)}\). UDA is required if ground-truth labels \(^{(t)}\) are not available. Source-free UDA [19; 55] posts a more challenging but practical scenario, where the source data \(\{^{(s)},^{(s)}\}\) used to pre-train the ASR is no longer available in adaptation. That is, only speech inputs \(^{(t)}\) is available when adapting the source model \(f^{(s)}\) to the target domain \(^{(t)}\). **Self-training Strategy.** In source-free UDA, since a source model itself typically generates pseudo-labels, some previous works  have referred to this learning approach as _semi-supervised learning_. To distinguish it from _unsupervised_ domain adaptation, in this paper, we refer to the approach for addressing source-free UDA as _self-training_, consistent with the terminology used in studies . Specifically, we adopt the pipeline line of _pseudo-labeling_ and _informed finetuning_. First, \(N^{(t)}\) unlabeled speech segments \(^{(t)}=\{x_{i}^{(t)}\}_{i=1}^{N^{(t)}_{i=1}}\) are fed into source model \(f^{(s)}\) to generate the pseudo labels corresponding to each of them, which are denoted as \(}^{(t)}=\{_{i}^{(t)}\}_{i=1}^{N^{(t)}}\). Then, the paired dataset with the speech inputs and their newly-generated pseudo labels \(\{^{(t)},}^{(t)}\}\) are used to finetune the source model to the target domain based on the self-training loss \(_{}\):

\[_{}(^{(t)},}^{(t)})=_{i=1} ^{N^{(t)}}_{}(x_{i}^{(t)},_{i}^{(t)}),\] (2)

where the ASR loss \(_{}\) follows the definition in Eq. (1).

**Summary**. Since self-generated pseudo labels  do not introduce extra supervised information to the ASR source model, simply repeating this process is _unlikely_ to yield performance improvements . However, if high-quality pseudo labels are selected as domain-specific exemplars to inform the speech foundation model, it would then update in a direction beneficial to the target domain performance. Therefore, we propose a critical research question: How can we _assess the quality of pseudo labels_ using an indicator that can also _guide the model's update_? The subsequent content of this section will delve into a detailed discussion from both token and utterance levels.

### Token-level Assessment and Re-weighting

The auto-regressive decoding in ASR can provide step-wise information on predicted tokens, which can be used for token-level uncertainty assessment [60; 77]. More importantly, this information can guide the subsequent training process: assigning different weights to each token when calculating the CE loss in Eq.(2), namely _informed finetuning_.

**Why is _confidence_ not a good indicator?** The confidence score denotes the highest value among the posterior probability predicted by a neural model. In auto-regressive decoding, the \(l\)-th step of token confidence score \(C_{l}\) can be denoted as:

\[_{l}=\ (_{l}|_{l-1:1},x,^{*}).\] (3)By preserving the \(\) for each token during pseudo-labeling, we can perform informed finetuning with a re-weighting loss as follows:

\[}_{}(x,)=_{l=1}^{L}-_{ }(_{l}|_{l-1:1},x)_{l}.\] (4)

However, a substantial body of existing research  indicates that confidence does not accurately reflect predictive accuracy, especially in auto-regressive decoding . In Eq. (4), the prediction of the current token is influenced by previously predicted tokens \(_{l-1:1}\), which can easily lead to error accumulation and propagation. We further inspect this claim in Whisper by empirical observation. As shown in Fig. 2 (Right-Up), we employ a confusion matrix to visualize the relationship between confidence score and pseudo-label quality, which shows that 52% of correct tokens are assigned low confidence and 60% of wrong tokens are assigned high confidence (more discussion is in Appendix C). Therefore, confidence cannot be a reliable pseudo-label quality indicator alone, like discussed in .

**Is _attentive score_ a better indicator? We explore if the self-attention matrix \(W\) obtained during auto-regressive decoding can reflect the pseudo-label quality. Unlike \(_{l}\) defined in Eq. (3), \(W\) has a direct association with \(\) and linguistic acceptability , which means that it might be less influenced by the variability of speech input (see example in Fig. 2).

**Empirical Observation**. Starting from the fourth row and fourth column (first 3 tokens are fixed prompts: "\(||||||\)"), for the correctly decoded tokens (black), the attention weights are concentrated on the diagonal and partially fall on other pseudo tokens. However, for wrongly decoded tokens (red), the attention weights almost all fall on the second column that corresponds to the task prompt token _"\(||\)"_ (highlighted in red boxes). To quantify this finding into a numerical metric, we defined an "aggregate pattern" indicator called _attentive score_, which is highlighted in the orange box in Fig. 2 and formulated as:

\[_{l}=_{j=4}^{l}W_{l,j}+_{i=l+1}^{L}W_{i,l},\] (5)

where \(_{l}\) indicates the global semantic correlations between pseudo token \(_{l}\) with all tokens \(\{_{l}\}_{l=4}^{L}\) (first 3 tokens are task prompt). Specifically, we add the second term to also consider the attention weights with respect to future tokens, in order to capture the comprehensive global context to better assess the role of current token (see Table 7 for ablation study). We compare the values of attentive score \(_{l}\) and confidence score \(_{l}\) for this sentence in Fig. 2 (Left). As marked by black boxes, \(_{l}\) provides unreliable assessments for both _'board'_ (correct but low \(_{l}\)) and _'year'_. \(||\)'(wrong but high \(_{l}\)). In comparison, \(_{l}\) can accurately reflect the correctness of these tokens. To avoid randomness, we analyze CHiME-4 _test-real_ and plot a confusion matrix in Fig. 2 (Right-Up). It is evident that, compared to \(_{l}\), our \(_{l}\) more reliably assesses the quality of predicted tokens.

Figure 2: **(Left):** An example of pseudo label, ground-truth transcription, confidence scores, attention matrix and attentive scores. **(Right-Up):** Confusion matrix of confidence and attentive scores, where the y-axis denotes the pseudo token is correct or wrong, and the x-axis denotes the corresponding score is high or low (with 1 as the threshold, more analysis is in Fig. 6), so that the diagonal values indicate the score’s reliability in assessing the quality of pseudo-label. **(Right-Down):** Variance of the two scores of correct and wrong pseudo tokens.

Despite reliability, \(_{l}\) exhibits less numerical stability, e.g., "for" and "housing" are both correct tokens but their \(_{l}\) are distinct (1.8 vs. 0.8). The underlying reason is that their roles in the global context as prepositions and nouns are indeed different [82; 62]. However, when we try to use this \(_{l}\) to guide the ASR loss re-weighting like Eq.(4), these labels are expected to be assigned comparable weights as they are equally correct. We verify this finding with the variance of \(_{l}\) and \(_{l}\) in Fig. 2 (Right-Down). For both correct and wrong tokens, \(_{l}\) exhibits higher variance, indicating it may not be suitable to guide the finetuning in a re-weighting manner directly.

**STAR Indicator: Reliable and Stable.** To integrate the advantages of \(_{f}\) and \(_{f}\), we introduce a new indicator that balances reliability and stability. Specifically, in cases where \(_{f}\) and \(_{f}\) exhibit conflicting values toward a pseudo token, we would select \(_{f}\) as an indicator that shows higher reliability. It can be mathematically formulated as:

\[_{l}^{}=[(_{l}^{2}/_{l}- )+(_{l}^{2}/_{l}-)]_ {l},\] (6)

where \(\) denotes the sigmoid function \((x)=1/(1+e^{-x})\), and here it simulates the step function to capture the cases of conflicting scores. Our definition of conflict is \(_{l}^{2}/_{l}\) larger than a hyper-parameter threshold \(\). This criterion can be decoupled into two terms, \(_{l}\) and \(_{l}/_{l}\), which means a large attentive score as well as a large gap between attentive and confidence scores1. Similarly, \(_{l}^{2}/_{l}\) is another case of conflicting scores, and we add them up to simulate the logical "OR" operation.

On the other hand, if \(_{f}\) and \(_{f}\) present consistent assessment towards a pseudo token, \(_{f}\) would be used to scale \(_{f}\) using its stability. Specifically, we design a soft interpolation strategy inspired by focal loss  to integrate them:

\[_{l}^{}=[(-_{l}^{2}/ _{l})(-_{l}^{2}/_{l})]\;_{l} e^{(_{l}-_{l})/}.\] (7)

Similarly, we also use Sigmoid function to simulate the non-conflicting cases, where we multiply the two terms to denote logical "AND". Inspired by the smoothing technique in focal loss , we propose to leverage the gap between two scores for scaling \(_{l} e^{(_{l}-_{l})/}\), where \(\) is temperature.

During the subsequent _informed finetuning_ stage, we combine the two indicators above to guide the training process in a re-weighting manner, and Eq.(4) should be re-written as:

\[}_{}(x,)=_{l=1}^{L}- _{}(_{l}|_{l-1:1},x)_{l};_{l}=_{l}^{}+_{l}^{}.\] (8)

As a result, the STAR scores are both reliable and stable as shown in Fig. 5, which serves as a better quality indicator to guide the _informed finetuning_ (see Algorithm 1 in Appendix for details).

### Utterance-level Filtering

The utterance-level filtering aims to remove those predicted utterances with low overall quality since they are probably harmful for subsequent adaptation. We now introduce several existing approaches to assess the utterance-level quality of pseudo labels, which are often used for uncertainty estimation. Notably, high uncertainty usually implicates low quality for the generated sequence.

**Monte Carlo Sampling** conduct multiple times of stochastic forward decoding with _activated dropout_ to get a list of predictions . Then the list with a large variance is considered to have high uncertainty and should be removed from subsequent training. However, this method does not apply to Whisper as it does not use dropout in training. As an alternative, we introduce a similar method for assessing utterance-level uncertainty. Specifically, given an input speech \(x\), we first implement one forward decoding and set the result \(\) as the base transcription. Then, we randomly disturb the model weights of Whisper with Gaussian noise, and repeat the forward decoding for \(K\) times, resulting in a list of pseudo transcriptions \(\{_{k}\}_{k=1}^{K}\). Thereafter, we calculate the edit distance (ED) between pseudo transcription \(_{k}\) and the base transcription \(\), which indicates the impact of disturbance on Whisper decoding. Then, the model's robustness in transcribing speech \(x\) can be calculated as:

\[U(x,)=_{k=1}^{K}ED(,\;_{k}).\] (9)After obtaining \(k\) pseudo labels, we can examine their diversity to further assess the model's uncertainty. If there are many repetitions in the list, it indicates that the model is more confident in transcribing speech \(x\). Therefore, we utilize a scaling factor \(l\) that is equal to the utterance amount after de-duplication. The final utterance-level quality is combined by the numeric multiplication of \(l\) and \(U(x,)\), which is then used to rank the \(N_{t}\) pseudo data samples, and top \(\%\) samples are removed due to large data uncertainty. Additionally, we also implement a **beam search decoding** and a **consensus decoding** baselines as alternative utterance-level filtering approaches for comparison, where more experimental results and discussions are presented in Appendix D.

## 4 Experimental Setup

### ASR Domains

We introduce STAR in various ASR domains to verify its general effectiveness, including noisy speech, accented speech, and specific scenarios. First, for noisy speech we use the CHiME-4 , LibriSpeech-FreeSound , and RATS  datasets, which covers a wide range of noise types including bus, cafe, pedestrian area, street junctions, babble, car, airport, and the challenging radio communication noises. Second, we select four typical accents from the CommonVoice  dataset,

    &  &  & _{}\)} & _{}}}}}}}}}}}}}}\) & _{l}}\)} & _{l}}\)} & _{l}}\)} \\  & & (frozen) & (self-train.) & & & & & & & \\   \\  & _test-real_ & \(6.8\) & \(6.9\) & \(6.4\) & \(6.5\) & \(6.2\) & \(_{-11.8\%}\) & \(5.2\) \\  & _test-simu_ & \(9.9\) & \(10.1\) & \(9.7\) & \(9.8\) & \(9.5\) & \(_{-5.1\%}\) & \(8.7\) \\  & _dev-real_ & \(4.6\) & \(4.5\) & \(4.3\) & \(4.3\) & \(4.1\) & \(_{-15.2\%}\) & \(3.2\) \\  & _dev-simu_ & \(7.0\) & \(7.0\) & \(6.6\) & \(6.7\) & \(6.6\) & \(_{-8.6\%}\) & \(5.9\) \\   & _babble_ & \(40.2\) & \(37.6\) & \(35.0\) & \(33.5\) & \(31.3\) & \(_{-24.9\%}\) & \(27.2\) \\  & _airport_ & \(15.6\) & \(15.5\) & \(15.2\) & \(15.3\) & \(15.0\) & \(_{-5.1\%}\) & \(14.5\) \\  & _car_ & \(2.9\) & \(3.0\) & \(2.8\) & \(2.8\) & \(2.6\) & \(_{-13.8\%}\) & \(2.4\) \\   & _radio_ & \(46.9\) & \(47.2\) & \(46.0\) & \(45.5\) & \(44.9\) & \(_{-4.9\%}\) & \(38.6\) \\  & _Arican_ & \(6.0\) & \(5.8\) & \(5.5\) & \(5.4\) & \(5.0\) & \(_{-20.0\%}\) & \(4.6\) \\  & _Australian_ & \(5.8\) & \(5.7\) & \(5.6\) & \(5.5\) & \(5.2\) & \(_{-12.1\%}\) & \(4.3\) \\  & _Indian_ & \(6.6\) & \(6.5\) & \(6.3\) & \(6.4\) & \(6.1\) & \(_{-9.1\%}\) & \(5.7\) \\  & _Singaporean_ & \(6.5\) & \(6.2\) & \(5.8\) & \(5.8\) & \(5.4\) & \(_{-21.5\%}\) & \(4.9\) \\   \\  & _TED talks_ & \(5.2\) & \(4.9\) & \(4.7\) & \(4.8\) & \(4.3\) & \(_{-21.2\%}\) & \(3.6\) \\  & _telephone_ & \(13.3\) & \(13.0\) & \(12.7\) & \(12.3\) & \(11.9\) & \(_{-12.0\%}\) & \(9.9\) \\  & _BBC talks_ & \(8.5\) & \(8.3\) & \(7.6\) & \(7.9\) & \(7.4\) & \(_{-17.6\%}\) & \(5.6\) \\  & _airline info._ & \(3.6\) & \(3.5\) & \(3.3\) & \(3.3\) & \(3.2\) & \(_{-19.4\%}\) & \(2.0\) \\  & _interview_ & \(21.5\) & \(21.3\) & \(20.8\) & \(20.7\) & \(20.4\) & \(_{-6.5\%}\) & \(17.9\) \\   

Table 1: Main WER (%) results of the proposed STAR adaptation and baselines in various ASR domains. “Whisper (frozen)” denotes the zero-shot performance without adaptation. “Whisper (self-train.)” is the vanilla self-training scheme consisting of pseudo-labeling and finetuning. Based on that, “\(_{}\)” adds utterance-level filtering explained in §3.3, and “\(_{}}}}}}}}}\) & \(\) & \(6.5\) & \(5.2\) & \(\) & \(including African, Australian, Indian, and Singaporean accents. Finally, we also evaluate our approach under some specific scenarios, including BBC talks (LRS2 ), TED talks (TED-LIUM 3 ), telephone conversation (SwitchBoard ), interview conversation (CORAAL ), and airline information consultation (ATIS ). More details about the datasets are presented in Appendix F.

### Configurations

We use the Whisper-Large-V3 model for main experiments, which contains 1.5 billion parameters trained on 680k-hour web-scale data. It is fine-tuned using Adam optimizer  with an initial learning rate of \(1e^{-5}\) for 2 epochs. The batch size is set to 1 with 16 gradient accumulation steps. For hyper-parameters, the threshold \(\) is set to 2 and the temperature \(\) is 10. In addition, the percentile \(\) of utterance-level filtering is 20, which shows consistent effectiveness across different datasets.

## 5 Results and Analysis

### Effectiveness of STAR

To examine the effectiveness of STAR, we conduct comparative experiments across various domains and report the WER results in Table 1.

**Main Results.** From noise adaptation results on CHiME-4, LS-FreeSound, and RATS, we observe that: (i) STAR enhances Whisper in all noise scenarios, reducing the WER up to 24.9% relatively. Specifically, on the challenging RATS dataset with pseudo labels of a 46.9% WER, our STAR can still produce a 4.9% relative improvement. (ii) For some domains, e.g., "_airport_" and "_car_", STAR can even approach the upper-bound performance by supervised learning. This demonstrates that even with unlabeled data only, our method can effectively adapt Whisper to specific target domains. From results on other domains, we observe that: (i) STAR consistently improves the accented ASR to approach the supervised upper bound. (ii) Whisper does not perform well in some colloquial scenarios (_SwitchBoard_ and _CORAAL_) as the spoken language tends to be informal and less grammatically correct, which leads to poor-quality pseudo labels and then influences our adaptation performance.

**Analysis of Catastrophic Forgetting.** Table 2 analyzes the potential forgetting issue of our method by evaluating the _CHiME-4_-finetuned model on other datasets. Surprisingly, contrary to the common catastrophic forgetting issue that commonly happens in traditional source-free ASR adaptation, our STAR approach can even improve the performance in other domains. We speculate that under the self-training scheme, the pseudo label is generated by the model itself, so that it may avoid the model from over-fitting to samples with vastly different data distributions . Furthermore, compared to vanilla self-training, STAR can better highlight the high-quality pseudo tokens for _informed finetuning_, which may help improve the model's general ASR ability. More detailed analysis are in SSE.

   Model & Baseline & Self-train. & STAR & Real \\  Whisper-V3-1.5B & \(6.8\) & \(6.9\) & \(6.0_{-11.8\%}\) & \(5.2\) \\ Whisper-Med-0.8B & \(8.9\) & \(8.8\) & \(8.0_{-10.1\%}\) & \(7.1\) \\ OWSM-V3-1-1.0B & \(8.4\) & \(8.1\) & \(7.5_{-10.7\%}\) & \(6.5\) \\ Canary-1.0B & \(8.2\) & \(8.0\) & \(7.2_{-12.2\%}\) & \(6.4\) \\ Parakeet-TDT-1.1B & \(8.0\) & \(7.8\) & \(7.0_{-12.3\%}\) & \(6.2\) \\   

Table 4: WER (%) results of STAR with different speech foundation models on CHiME-4 _test-real_. More models / datasets are evaluated in Table 9 and 6.

   Metric & Content & Variance & NCE Score \\  Ground-truth & they are organised by scientific themes. & - & - \\  Pseudo label & they are organised by scientific teams. & - & - \\ \(_{1:L}\) & \([0.81,0.88,0.98,1.21,1.13,1.17,0.82]\) & \(0.023\) & \(-0.671\) \\ \(_{1:L}\) & \([1.47,1.49,0.95,1.20,0.79,0.43,0.67]\) & \(0.101\) & \(0.146\) \\ \(_{1:L}\) (ours) & \([1.39,1.40,0.91,1.14,1.03,0.41,0.73]\) & \(0.058\) & \(0.322\) \\   

Table 3: Case study of an accented speech in CV-_in_ (ID: “en_19795319”). The wrong tokens are highlighted in red. Variance indicates the stability of different scores. “NCE” denotes normalized cross-entropy, where a higher value indicates better measure quality (more results are in Fig. 5).

   X \(\) En & Baseline & Self-train. & STAR & Real \\  Ar & \(21.9\) & \(22.1\) & \(23.3_{+1.4}\) & \(24.5\) \\ De & \(33.7\) & \(34.0\) & \(35.9_{+2.2}\) & \(36.5\) \\ Es & \(23.9\) & \(24.1\) & \(24.8_{+0.9}\) & \(26.4\) \\ Fa & \(16.6\) & \(16.3\) & \(17.6_{+1.0}\) & \(19.0\) \\ Hi & \(22.4\) & \(22.5\) & \(23.4_{+1.0}\) & \(24.4\) \\ Zh & \(16.3\) & \(16.3\) & \(17.1_{+0.8}\) & \(17.9\) \\   

Table 5: BLEU results of STAR on speech translation task with FLEURS  test sets.

**Analysis of Indicators.** Table 1 also presents the performance of different indicators in the _informed finetuning_. First, we observe that the utterance-level filtering yields some effects by removing bad training samples. Then, for token-level re-weighting, the two pseudo-label quality indicators both improve the performance, where the attentive score performs better due to higher reliability. Our proposed STAR indicator achieves the best result by integrating the strengths of both scores. We also use a case in Table 3 to illustrate, where exists a wrong pseudo token "teams". The confidence score fails to reflect this error while the attentive score succeeds, but the latter suffers from less numerical stability (i.e., large variance). By integrating their strengths, our STAR score achieves both reliability and stability in assessing the pseudo-label's quality. In addition, we also calculate the NCE metric to show the better quality of our proposed STAR and attentive scores than traditional confidence scores.

### Generality of STAR

**Generalization to Different Speech Foundation Models.** To further evaluate the generalization ability of our approach, we extend STAR to different foundation models, including OWSM, Canary and Parakeet-TDT that take top places in the HuggingFace ASR leader-board 2. Consistent performance gains (i.e., over 10% relative WER reduction) on these models has verified the excellent generality of STAR. In addition, it also works well on relatively small models like Whisper-Medium.en-0.8B.

**Generalization to Speech Translation (ST) Task.** Apart from ASR, we also investigate another widely studied speech task, the ST task, to further verify the generality of STAR adaptation. As shown in Table 5, results on various FLEURS X\(\)En tracks illustrate an average of over 1.2 BLEU improvements (2.2 BLEU for De\(\)En). It shows the good potential of our STAR adaptation on other sequence-to-sequence tasks besides ASR, which could lead to more extensions for future work.

### Ablation Study

In this section, we conduct ablation studies to analyze STAR from perspectives of data (Fig. 3), model (Table 9), and finetuning approaches (Table 10), which provide a constructive reference for deploying ASR foundation models in practical scenarios using STAR. More analysis are in Appendix E.

**Data Efficiency.** We explore the requirement of unlabeled data amount (\(N_{t^{}}\)) for STAR adaptation. Fig. 3 shows the WER results on four datasets with different numbers of training utterances. Surprisingly, only 200 to 500 sentences (less than 1-hour unlabeled speech data) are required to achieve the optimal effects, which cost around 0.8-hour training time on single NVIDIA-A100-40GB GPU. This remarkable data efficiency significantly saves the labours in real-world applications: not only is there no need for manual labelling, but the collection of unlabeled data also requires less than one hour.

**Model Size.** Table 9 reports the performance on CHiME-4 _test-real_ that applies STAR to the Whisper family with different model sizes. Results show that our STAR adaptation works well on difference scales of foundation models. Specifically, the promising performance gains on light model (base.en) implicates the potential of STAR in practical resource-constrained conditions, such as mobile devices.

**Finetuning Approach.** Considering that adapting speech foundation models with a small amount of data might risk over-fitting, we explore the impact of different finetuning approaches in Table 10. We observe that both regular finetuning (full, encoder-only, decoder-only) and efficient finetuning methods (LoRA) yield similar effectiveness, which provides flexible choices under different settings.

Figure 3: WER (%) results with different numbers of unlabeled training samples. The minimum required data amount (in hours) to obtain the best performance is highlighted in the star mark.

Conclusion

We propose STAR, a source-free UDA method that effectively adapts the speech foundation models to various target domains with unlabeled data. Specifically, STAR introduces a novel indicator to assess the pseudo-label quality and then instructively guide the finentuning of the model. Our experiments verify STAR's efficacy on ASR tasks across a wide range of target domains including noise, accent, and specific scenarios, and it even approaches the upper-bound performance of supervised adaptation on some corpora. Furthermore, we observe that STAR can avoid the catastrophic forgetting problem that is often suffered by models adapted without recalling source-domain data. Furthermore, STAR only requires less than one hour of unlabeled data to achieve an average of 13.5% relative WER reduction across 14 domains, and it also shows seamless generality to speech translation tasks. This enables us to deploy speech systems in real-world scenarios rapidly and conveniently.